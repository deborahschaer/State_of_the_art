{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "#import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiaction\n",
    "Can the llm classify if a paper is relevant to a given topic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('results/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-based NLG Evaluation: Current Status and Challenges: Evaluating natural language generation (NLG) is a vital but challenging\\nproblem in artificial intelligence. Traditional evaluation metrics mainly\\ncapturing content (e.g. n-gram) overlap between system outputs and references\\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\\nhave demonstrated great potential in NLG evaluation in recent years. Various\\nautomatic evaluation methods based on LLMs have been proposed, including\\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\\nevaluation methods, and discuss their pros and cons, respectively. We also\\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\\nopen problems in this area and point out future research directions.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n",
    "original = get_json('../task3/dataset/'+paper_id+'data')\n",
    "original['title']\n",
    "topic = original['title']+': '+ original['abstract']\n",
    "topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small dataset: Give a list of papers\n",
    "Here there are 10 relevant and 10 unrelated papers. The unrelated are papers related to 'data science'. The dataset was created in the file create_dataset.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['1802.03292', '2311.09184', '0907.3804', '2307.10928', '2303.07610', '2002.05658', '1610.07365', '2310.00752', '2310.00785', '2310.19740', '2207.07901', '1612.04037', '2310.00074', '2310.17631', '1501.05039', '2302.04166', '2311.09204', '1908.05986', '2205.01553', '1309.0717'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = get_json('dataset/'+paper_id)\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0]\n",
      "['2311.09184', '2307.10928', '2303.07610', '2310.00752', '2310.00785', '2310.19740', '2310.00074', '2310.17631', '2302.04166', '2311.09204']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_labels(d):\n",
    "    labels = [details['label'] for id, details in d.items()]\n",
    "    return labels\n",
    "def get_relevant_ids(d):\n",
    "    ids = [id for id, details in d.items() if details['label']==1]\n",
    "    return ids\n",
    "# Usage example\n",
    "labels = get_labels(data)\n",
    "print(labels)\n",
    "ids = get_relevant_ids(data)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_with_summary_id(topic,summaries):\n",
    "    choice = \"\\n\".join([\"**\"+str(n)+\"** \"+i['title']+i['abstract'] for n,i in summaries.items()])\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "\n",
    "    Below is a list of papers. Each paper is numbered and includes its title and summary.\n",
    "\n",
    "    List of Papers:\n",
    "    {choice}\n",
    "\n",
    "    Instructions:\n",
    "\n",
    "    Indicate which papers are relevant to the topic by writing only the numbers of the relevant papers.\n",
    "    Do not provide any explanations or use any other numbers.\n",
    "    Format your answer as a list of numbers separated by commas.\n",
    "    Please provide your answer below:\"\"\"\n",
    "    print(len(instruction.split(' ')))\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2715\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' 2310.17631, 2311.09204, 1501.05086, 2311.09204, 1908.05986, 2205.01553, 1309.0717'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = select_with_summary_id(topic,data)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n"
     ]
    }
   ],
   "source": [
    "def find_hallucinations(a):\n",
    "    h = []\n",
    "    for number in a:\n",
    "        if number not in data.keys():\n",
    "            h.append(number)\n",
    "    return h\n",
    "def find_duplicates(lst):\n",
    "    return list(set([item for item in lst if lst.count(item) > 1]))\n",
    "\n",
    "#Example\n",
    "print(find_duplicates([1,1,1,2,2])) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out if the order of the papers in the list matters\n",
    "The list is shuffled 9 times and the result is evaluated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "2715\n",
      "{'run': {0: {'answer': ['2310.00752', '2310.00785', '1802.03292', '2310.00785'], 'n_predictions': 4, 'n_correct': 2, 'P': 0.5, 'R': 0.2, 'F1': 0.28571428571428575}, 1: {'answer': ['1501.05039', '2310.09204', '2311.09204'], 'n_predictions': 3, 'n_correct': 1, 'P': 0.3333333333333333, 'R': 0.1, 'F1': 0.15384615384615383}, 2: {'answer': ['2207.07901', '2310.19740'], 'n_predictions': 2, 'n_correct': 1, 'P': 0.5, 'R': 0.1, 'F1': 0.16666666666666669}, 3: {'answer': ['1610.07365', '1802.03292', '2303.07610', '2310.17631'], 'n_predictions': 4, 'n_correct': 2, 'P': 0.5, 'R': 0.2, 'F1': 0.28571428571428575}, 4: {'answer': ['2302.04166', '2310.17631', '2205.01592', '2307.10928', '1908.05986'], 'n_predictions': 5, 'n_correct': 3, 'P': 0.6, 'R': 0.3, 'F1': 0.4}, 5: {'answer': ['2303.07610', '2205.01553', '2310.00731', '2310.17631', '2310.04166'], 'n_predictions': 5, 'n_correct': 2, 'P': 0.4, 'R': 0.2, 'F1': 0.26666666666666666}, 6: {'answer': ['2310.00752', '2310.00785'], 'n_predictions': 2, 'n_correct': 2, 'P': 1.0, 'R': 0.2, 'F1': 0.33333333333333337}, 7: {'answer': ['1501.05039', '2310.00785', '2310.17631', '2311.09184'], 'n_predictions': 4, 'n_correct': 3, 'P': 0.75, 'R': 0.3, 'F1': 0.4285714285714285}, 8: {'answer': ['2307.10928', '1501.05039', '1908.05984', '2310.00074', '2311.09184', '2205.01553', '1908.05984', '2311.09184'], 'n_predictions': 8, 'n_correct': 3, 'P': 0.375, 'R': 0.3, 'F1': 0.33333333333333326}, 9: {'answer': ['2310.00785', '2205.01553', '2311.09210', '1908.05986', '1908.05986', '1610.07365', '1501.05039', '1802.03292', '2303.07610'], 'n_predictions': 9, 'n_correct': 2, 'P': 0.2222222222222222, 'R': 0.2, 'F1': 0.2105263157894737}}}\n",
      "['2310.00785', '2205.01553', '2311.09210', '1908.05986', '1908.05986', '1610.07365', '1501.05039', '1802.03292', '2303.07610']\n",
      "{'run': {0: {'answer': ['2310.00752', '2310.00785', '1802.03292', '2310.00785'], 'n_predictions': 4, 'n_correct': 2, 'P': 0.5, 'R': 0.2, 'F1': 0.28571428571428575}, 1: {'answer': ['1501.05039', '2310.09204', '2311.09204'], 'n_predictions': 3, 'n_correct': 1, 'P': 0.3333333333333333, 'R': 0.1, 'F1': 0.15384615384615383}, 2: {'answer': ['2207.07901', '2310.19740'], 'n_predictions': 2, 'n_correct': 1, 'P': 0.5, 'R': 0.1, 'F1': 0.16666666666666669}, 3: {'answer': ['1610.07365', '1802.03292', '2303.07610', '2310.17631'], 'n_predictions': 4, 'n_correct': 2, 'P': 0.5, 'R': 0.2, 'F1': 0.28571428571428575}, 4: {'answer': ['2302.04166', '2310.17631', '2205.01592', '2307.10928', '1908.05986'], 'n_predictions': 5, 'n_correct': 3, 'P': 0.6, 'R': 0.3, 'F1': 0.4}, 5: {'answer': ['2303.07610', '2205.01553', '2310.00731', '2310.17631', '2310.04166'], 'n_predictions': 5, 'n_correct': 2, 'P': 0.4, 'R': 0.2, 'F1': 0.26666666666666666}, 6: {'answer': ['2310.00752', '2310.00785'], 'n_predictions': 2, 'n_correct': 2, 'P': 1.0, 'R': 0.2, 'F1': 0.33333333333333337}, 7: {'answer': ['1501.05039', '2310.00785', '2310.17631', '2311.09184'], 'n_predictions': 4, 'n_correct': 3, 'P': 0.75, 'R': 0.3, 'F1': 0.4285714285714285}, 8: {'answer': ['2307.10928', '1501.05039', '1908.05984', '2310.00074', '2311.09184', '2205.01553', '1908.05984', '2311.09184'], 'n_predictions': 8, 'n_correct': 3, 'P': 0.375, 'R': 0.3, 'F1': 0.33333333333333326}, 9: {'answer': ['2310.00785', '2205.01553', '2311.09210', '1908.05986', '1908.05986', '1610.07365', '1501.05039', '1802.03292', '2303.07610'], 'n_predictions': 9, 'n_correct': 2, 'P': 0.2222222222222222, 'R': 0.2, 'F1': 0.2105263157894737}}, 'mean_P': 0.5180555555555555, 'mean_R': 0.21000000000000002, 'mean_F1': 0.2864372469635628, 'combined': {'answer': ['2310.00752', '2310.00785', '1802.03292', '2310.00785', '1501.05039', '2310.09204', '2311.09204', '2207.07901', '2310.19740', '1610.07365', '1802.03292', '2303.07610', '2310.17631', '2302.04166', '2310.17631', '2205.01592', '2307.10928', '1908.05986', '2303.07610', '2205.01553', '2310.00731', '2310.17631', '2310.04166', '2310.00752', '2310.00785', '1501.05039', '2310.00785', '2310.17631', '2311.09184', '2307.10928', '1501.05039', '1908.05984', '2310.00074', '2311.09184', '2205.01553', '1908.05984', '2311.09184', '2310.00785', '2205.01553', '2311.09210', '1908.05986', '1908.05986', '1610.07365', '1501.05039', '1802.03292', '2303.07610'], 'n_predictions': 46, 'n_correct': 10, 'P': 0.21739130434782608, 'R': 1.0, 'F1': 0.3571428571428571, 'hallucinations': ['1908.05984', '2205.01592', '2310.00731', '2310.04166', '2310.09204', '2311.09210']}, 'duplicates': {'n_predictions': 11, 'n_correct': 6, 'P': 0.5454545454545454, 'R': 0.6, 'F1': 0.5714285714285713, 'hallucinations': ['1908.05984']}}\n"
     ]
    }
   ],
   "source": [
    "def evaluate_step(n3):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for key, value in data.items():\n",
    "        if key in n3:\n",
    "            if value['label']==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if value['label']==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    return(TP,FP,FN,TN)\n",
    "\n",
    "def run():\n",
    "    scores = {}\n",
    "    scores['run'] = {}\n",
    "    combined = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        scores['run'][i] = {}\n",
    "        experiment = scores['run'][i]\n",
    "        \n",
    "        #shuffle data\n",
    "        items = list(data.items())    \n",
    "        random.shuffle(items)\n",
    "        shuffled_dict = dict(items)\n",
    "        \n",
    "        #Get answer\n",
    "        answer = select_with_summary_id(topic,shuffled_dict)\n",
    "        n = re.findall(r'\\d{4}\\.\\d{5}',answer)\n",
    "        \n",
    "        #store the numbers\n",
    "        experiment['answer'] = n\n",
    "        combined = combined+n\n",
    "        \n",
    "        #caluculate metrics\n",
    "        TP,FP,FN,TN = evaluate_step(n)\n",
    "        experiment['n_predictions'] = len(n)\n",
    "        experiment['n_correct'] = TP\n",
    "        experiment['P'] = TP/len(n)\n",
    "        experiment['R'] = TP/(TP+FN)\n",
    "        experiment['F1'] = 2*experiment['P']*experiment['R']/(experiment['P']+experiment['R'])\n",
    "    \n",
    "    print(scores)\n",
    "    print(n)\n",
    "    scores['mean_P'] = np.mean([scores['run'][s]['P'] for s in scores['run'].keys()])\n",
    "    scores['mean_R'] = np.mean([scores['run'][s]['R'] for s in scores['run'].keys()])\n",
    "    scores['mean_F1'] = np.mean([scores['run'][s]['F1'] for s in scores['run'].keys()])\n",
    "    \n",
    "    \n",
    "    #Get metric of combined\n",
    "    TP,FP,FN,TN = evaluate_step(combined)\n",
    "    scores['combined'] = {}\n",
    "    scores['combined']['answer'] = combined\n",
    "    \n",
    "    scores['combined']['n_predictions'] = len(combined)\n",
    "    scores['combined']['n_correct'] = TP\n",
    "    scores['combined']['P'] = TP/len(combined)\n",
    "    scores['combined']['R'] = TP/(TP+FN)\n",
    "    scores['combined']['F1'] = 2*scores['combined']['P']*scores['combined']['R']/(scores['combined']['P']+scores['combined']['R'])\n",
    "    scores['combined']['hallucinations']=list(np.unique(find_hallucinations(combined)))\n",
    "\n",
    "    #Get metric of taking only results that are present at least once\n",
    "    duplicates = find_duplicates(combined)\n",
    "    TP,FP,FN,TN = evaluate_step(duplicates)\n",
    "    scores['duplicates'] = {}\n",
    "    scores['duplicates']['n_predictions'] = TP+FP\n",
    "    scores['duplicates']['n_correct'] = TP\n",
    "    scores['duplicates']['P'] = TP/(TP+FP)\n",
    "    scores['duplicates']['R'] = TP/(TP+FN)\n",
    "    scores['duplicates']['F1'] = 2*scores['duplicates']['P']*scores['duplicates']['R']/(scores['duplicates']['P']+scores['duplicates']['R'])\n",
    "    scores['duplicates']['hallucinations']=list(np.unique(find_hallucinations(duplicates)))\n",
    "\n",
    "    return scores\n",
    "result = run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(result,'summaries_list')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.5, 0.2, 0.29], [0.33, 0.1, 0.15], [0.5, 0.1, 0.17], [0.5, 0.2, 0.29], [0.6, 0.3, 0.4], [0.4, 0.2, 0.27], [1.0, 0.2, 0.33], [0.75, 0.3, 0.43], [0.38, 0.3, 0.33], [0.22, 0.2, 0.21], [0.52, 0.21, 0.29], [0.21235818797494013, 0.06999999999999999, 0.0869540108333135]]\n"
     ]
    }
   ],
   "source": [
    "#Create tables\n",
    "def P_R_F_table(s):\n",
    "    table = []\n",
    "    for k,v in s['run'].items():\n",
    "        table.append([np.round(v['P'],2),np.round(v['R'],2),np.round(v['F1'],2)])\n",
    "    sd = list(np.std(table,axis=0))\n",
    "    table.append([np.round(s['mean_P'],2),np.round(s['mean_R'],2),np.round(s['mean_F1'],2)])\n",
    "    table.append(sd)\n",
    "\n",
    "    return(table)\n",
    "\n",
    "table = P_R_F_table(result)\n",
    "print(table)\n",
    "np.savetxt('results/P_R_F1.txt',table,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The order clearly matter as there are always different answers. But also the results are bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 0, 1], [3, 1, 1, 0], [2, 1, 0, 0], [4, 2, 0, 0], [5, 3, 1, 0], [5, 2, 2, 0], [2, 2, 0, 0], [4, 3, 0, 0], [8, 3, 2, 2], [9, 2, 1, 1], [4.6, 2.1, 0.7, 0.4], [2.2, 0.7000000000000001, 0.7810249675906654, 0.66332495807108]]\n"
     ]
    }
   ],
   "source": [
    "#Create other table\n",
    "def predicted_table(s):\n",
    "    table = []\n",
    "    for k,v in s['run'].items():\n",
    "        table.append([len(v[\"answer\"]),v['n_correct'],len(find_hallucinations(v[\"answer\"])),len(find_duplicates(v[\"answer\"]))])\n",
    "    m = list(np.mean(table,axis=0))\n",
    "    sd = list(np.std(table,axis=0))\n",
    "    table.append(m)\n",
    "    table.append(sd)\n",
    "    return(table)\n",
    "table = predicted_table(result)\n",
    "print(table)\n",
    "np.savetxt('results/predictions.txt',table,fmt='%.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ask every paper seperatly if it is relevant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2311.09184\n",
      " Yes, the paper is relevant as it also discusses the use of large language models (LLMs) for NLG evaluation and summarization tasks. It provides insights into the challenges and performance gaps of LLMs in instruction controllable text summarization and evaluates various LLM-based automatic evaluation methods.\n"
     ]
    }
   ],
   "source": [
    "#Is it better to ask for each paper separatly if it is relevant?\n",
    "one = list(data.keys())[1]\n",
    "print(one)\n",
    "def ask_relevant(topic,id):\n",
    "    paper = data[id]\n",
    "    choice = paper['title']+paper['abstract']\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "                    Indicate if the following paper is relevant: {choice}.\n",
    "                    Only say yes or no. Please provide your answer below:\"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer\n",
    "a = ask_relevant(topic,one)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, the paper \"Shepherd: A Critic for Language Model Generation\" is relevant to the topic of LLM-based NLG evaluation as it introduces a language model specifically tuned to critique and suggest refinements, which can be considered as an automatic evaluation method based on LLMs. The paper also discusses the performance of Shepherd in comparison to other models such as ChatGPT, providing insights into the effectiveness of LLM-based NLG evaluation methods.\n",
      " No. The paper \"LLM-based NLG Evaluation: Current Status and Challenges\" is not directly related to the topic of MEMICS (11th Doctoral Workshop on Mathematical and Engineering Methods in Computer Science). The former focuses specifically on natural language generation evaluation using large language models, while the latter is a general call for papers covering various areas of computer science.\n",
      " Yes, the paper is relevant as it discusses the use of large language models (LLMs) for NLG evaluation and specifically mentions the fine-tuning of LLMs for this purpose. The paper also provides details on their approach to fine-tuning and safety improvements which could be relevant to the topic of LLM-based NLG evaluation.\n",
      " Yes, the paper is relevant as it also explores the capabilities and potential applications of large language models like ChatGPT in natural language generation evaluation. While the focus of the current paper is on ChatGPT's ability to rank content, the survey discusses various LLM-based NLG evaluation methods, including those that utilize LLMs for ranking or fine-tuning.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m             answer\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mid\u001b[39m)\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n\u001b[0;32m---> 17\u001b[0m answer1 \u001b[38;5;241m=\u001b[39m \u001b[43mask_llm_one_by_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(answer1)\n",
      "Cell \u001b[0;32mIn[30], line 12\u001b[0m, in \u001b[0;36mask_llm_one_by_one\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m answer \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m,paper  \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 12\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mask_relevant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpaper\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(a)\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYes\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m a:\n",
      "Cell \u001b[0;32mIn[30], line 6\u001b[0m, in \u001b[0;36mask_relevant\u001b[0;34m(topic, paper)\u001b[0m\n\u001b[1;32m      2\u001b[0m choice \u001b[38;5;241m=\u001b[39m paper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39mpaper[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      3\u001b[0m instruction \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYour task is to determine which papers are relevant to the topic: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtopic\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m                Indicate if the following paper is relevant: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchoice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m                Only say yes or no. Please provide your answer below:\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 6\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:276\u001b[0m, in \u001b[0;36mBaseLLM.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    272\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    273\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m    274\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 276\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    280\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    281\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    282\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    283\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    287\u001b[0m         \u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    288\u001b[0m     )\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:633\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    627\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    631\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    632\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:803\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m get_llm_cache() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m    789\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    790\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    791\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m         )\n\u001b[1;32m    802\u001b[0m     ]\n\u001b[0;32m--> 803\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:670\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    668\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    669\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    671\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    672\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_core/language_models/llms.py:657\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    649\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    654\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    656\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 657\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    658\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    659\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    660\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    661\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    664\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    665\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    666\u001b[0m         )\n\u001b[1;32m    667\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    668\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_community/llms/ollama.py:417\u001b[0m, in \u001b[0;36mOllama._generate\u001b[0;34m(self, prompts, stop, images, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    415\u001b[0m generations \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m prompt \u001b[38;5;129;01min\u001b[39;00m prompts:\n\u001b[0;32m--> 417\u001b[0m     final_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stream_with_aggregation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    418\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    419\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    420\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    421\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    422\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    423\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    425\u001b[0m     generations\u001b[38;5;241m.\u001b[39mappend([final_chunk])\n\u001b[1;32m    426\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_community/llms/ollama.py:326\u001b[0m, in \u001b[0;36m_OllamaCommon._stream_with_aggregation\u001b[0;34m(self, prompt, stop, run_manager, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_stream_with_aggregation\u001b[39m(\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    319\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    324\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GenerationChunk:\n\u001b[1;32m    325\u001b[0m     final_chunk: Optional[GenerationChunk] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stream_resp \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_generate_stream(prompt, stop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m stream_resp:\n\u001b[1;32m    328\u001b[0m             chunk \u001b[38;5;241m=\u001b[39m _stream_response_to_generation_chunk(stream_resp)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/langchain_community/llms/ollama.py:172\u001b[0m, in \u001b[0;36m_OllamaCommon._create_generate_stream\u001b[0;34m(self, prompt, stop, images, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_generate_stream\u001b[39m(\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    166\u001b[0m     prompt: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    170\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[1;32m    171\u001b[0m     payload \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images}\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_stream(\n\u001b[1;32m    173\u001b[0m         payload\u001b[38;5;241m=\u001b[39mpayload,\n\u001b[1;32m    174\u001b[0m         stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[1;32m    175\u001b[0m         api_url\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_url\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/api/generate\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    176\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    177\u001b[0m     )\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/requests/models.py:865\u001b[0m, in \u001b[0;36mResponse.iter_lines\u001b[0;34m(self, chunk_size, decode_unicode, delimiter)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterates over the response data, one line at a time.  When\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;124;03mstream=True is set on the request, this avoids reading the\u001b[39;00m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;124;03mcontent at once into memory for large responses.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m \n\u001b[1;32m    860\u001b[0m \u001b[38;5;124;03m.. note:: This method is not reentrant safe.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    863\u001b[0m pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miter_content(\n\u001b[1;32m    866\u001b[0m     chunk_size\u001b[38;5;241m=\u001b[39mchunk_size, decode_unicode\u001b[38;5;241m=\u001b[39mdecode_unicode\n\u001b[1;32m    867\u001b[0m ):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    870\u001b[0m         chunk \u001b[38;5;241m=\u001b[39m pending \u001b[38;5;241m+\u001b[39m chunk\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/requests/utils.py:571\u001b[0m, in \u001b[0;36mstream_decode_response_unicode\u001b[0;34m(iterator, r)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    570\u001b[0m decoder \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetincrementaldecoder(r\u001b[38;5;241m.\u001b[39mencoding)(errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 571\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m iterator:\n\u001b[1;32m    572\u001b[0m     rv \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(chunk)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rv:\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/requests/models.py:816\u001b[0m, in \u001b[0;36mResponse.iter_content.<locals>.generate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    814\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    815\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 816\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw\u001b[38;5;241m.\u001b[39mstream(chunk_size, decode_content\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    817\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m ProtocolError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    818\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ChunkedEncodingError(e)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/urllib3/response.py:1040\u001b[0m, in \u001b[0;36mHTTPResponse.stream\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;124;03mA generator wrapper for the read() method. A call will block until\u001b[39;00m\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;124;03m``amt`` bytes have been read from the connection or until the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;124;03m    'content-encoding' header.\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1039\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunked \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupports_chunked_reads():\n\u001b[0;32m-> 1040\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread_chunked(amt, decode_content\u001b[38;5;241m=\u001b[39mdecode_content)\n\u001b[1;32m   1041\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_fp_closed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fp) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decoded_buffer) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/urllib3/response.py:1184\u001b[0m, in \u001b[0;36mHTTPResponse.read_chunked\u001b[0;34m(self, amt, decode_content)\u001b[0m\n\u001b[1;32m   1181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1184\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_chunk_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1185\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1186\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/urllib3/response.py:1108\u001b[0m, in \u001b[0;36mHTTPResponse._update_chunk_length\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1108\u001b[0m line \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[1;32m   1109\u001b[0m line \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m;\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1110\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def ask_relevant(topic,paper):\n",
    "    choice = paper['title']+paper['summary']\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "                    Indicate if the following paper is relevant: {choice}.\n",
    "                    Only say yes or no. Please provide your answer below:\"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer\n",
    "def ask_llm_one_by_one():\n",
    "    answer = []\n",
    "    for id,paper  in data.items():\n",
    "        a = ask_relevant(topic,paper)\n",
    "        print(a)\n",
    "        if 'Yes' in a:\n",
    "            answer.append(id)\n",
    "    return answer\n",
    "answer1 = ask_llm_one_by_one()\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 0, 0, 10)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_step(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LLM-based NLG Evaluation: Current Status and Challenges']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic\n",
    "title = arxiv_tool.fetch_paper_titles([paper_id])\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, the paper \"Shepherd: A Critic for Language Model Generation\" is relevant to the topic of ['LLM-based NLG Evaluation: Current Status and Challenges']. The paper introduces a language model specifically tuned for critiquing responses and suggesting refinements, which relates to the evaluation of large language models in natural language generation (NLG). Additionally, the paper discusses the importance of high quality feedback datasets and compares the performance of Shepherd with other competitive alternatives, which are also relevant topics in the field.\n",
      " No. The paper does not seem relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. It appears to be an announcement for a doctoral workshop in mathematical and engineering methods in computer science, rather than a research paper on natural language generation (NLG) evaluation using large language models (LLMs).\n",
      " Yes, this paper is relevant as it discusses large language models (LLMs) and their evaluation for dialogue use cases, which aligns with the topic of interest.\n",
      " Yes, the paper is relevant as it explores the evaluation capabilities of a language model (ChatGPT) and discusses its consistency with human preferences, which aligns with the topic of LLM-based NLG Evaluation.\n",
      " No, this paper does not seem relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper focuses on ten research challenge areas in data science as a whole, rather than specifically addressing natural language generation (NLG) or evaluation methods using large language models (LLMs).\n",
      " Yes, the paper is relevant to the topic as it discusses the use of large language models (LLMs) for NLG evaluation and the challenges of calibrating them towards human alignment.\n",
      " No, this paper does not seem relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper focuses on defining data science rather than LLM-based Natural Language Generation (NLG) evaluation.\n",
      " No, this paper does not seem directly relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper focuses on the relationship between computational linguistics and cognitive science in general, rather than specifically addressing language modeling or natural language generation evaluation.\n",
      " No. The paper does not appear to be relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " Yes, this paper is relevant to the topic as it discusses the use of LLMs in NLG and evaluating their performance in Retrieval Augmented Generation systems. The paper also mentions challenges related to current status and evaluation methods for such systems.\n",
      " No, this paper does not seem relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper appears to be discussing the sociological and philosophical aspects of computer science, rather than focusing on natural language generation (NLG) evaluation using large language models (LLMs).\n",
      " Yes, this paper is relevant to the topic as it discusses the evaluation of Large Language Models (LLMs) in Natural Language Processing (NLP), specifically focusing on alignment tasks and addressing challenges related to generality, flexibility, and interpretability. The proposed solution, Auto-J, is designed to evaluate LLMs under diverse scenarios and accommodates different evaluation protocols.\n",
      " No. The paper does not seem to be directly related to 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " Yes, the paper is relevant to the topic as it discusses the importance of making research software open and adhering to the FAIR principles for reusability and repeatability in computational science and computer science research. The paper also mentions the challenges and current status of implementing these practices in the field.\n",
      " No. The paper does not appear to be relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " Yes, this paper is relevant to the topic as it discusses evaluation methods and challenges related to large language models (LLMs), specifically focusing on biases in evaluation processes and proposing a solution to enhance the quality of LLM-based evaluations.\n",
      " No. The paper \"Decidability of higher-order matching\" does not seem relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " Yes, this paper is relevant to the topic as it discusses the use of LLMs as evaluation metrics in natural language generation tasks, specifically in machine translation and summarization. The paper also mentions challenges related to current status and evaluation methods for LLM-based NLG.\n",
      " Yes, the paper is relevant to the topic as it discusses evaluation methods for large language models (LLMs), specifically focusing on LLM-based evaluations and their challenges. The paper proposes new approaches, such as peer rank (PR) and peer discussion (PD), to improve the accuracy and alignment with human judgments in LLM-based evaluations.\n",
      " Yes, this paper is relevant to the topic as it discusses the evaluation of language models (LLMs) in natural language generation (NLG), specifically through the proposed framework \"Language-Model-as-an-Examiner\" for benchmarking.\n",
      "['2308.04592', '2307.09288', '2303.07610', '2309.13308', '2309.15217', '2310.05470', '1908.05986', '2307.03025', '2310.19792', '2307.02762', '2306.04181']\n"
     ]
    }
   ],
   "source": [
    "def ask_llm_one_by_one_title():\n",
    "    answer = []\n",
    "    for id,paper  in data.items():\n",
    "        a = ask_relevant(title,paper)\n",
    "        print(a)\n",
    "        if 'Yes' in a:\n",
    "            answer.append(id)\n",
    "    return answer\n",
    "answer1 = ask_llm_one_by_one_title()\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 1, 0, 9)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_step(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes. The paper \"Shepherd: A Critic for Language Model Generation\" discusses language model generation, which is related to Natural Language Generation (NLG) and the use of Large Language Models (LLMs). Therefore, it can be considered relevant to the topic.\n",
      " Based on the title provided, it is unlikely that the paper \"Proceedings 11th Doctoral Workshop on Mathematical and Engineering Methods in Computer Science\" is directly relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. Therefore, my answer is no.\n",
      " No, the paper \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" does not seem to be directly relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper focuses on Llama 2, an open foundation and fine-tuned chat model, while the topic is about the evaluation methods and challenges for Natural Language Generation (NLG) based on Large Language Models (LLMs).\n",
      " Yes, the paper \"Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences\" is relevant to the topic of LLM-based NLG (Natural Language Generation) evaluation as it discusses the performance of a language model in generating and ranking content, which is related to the challenges and current status of evaluating NLG systems.\n",
      " No, that paper does not seem to be directly relevant to the topic of LLM-based NLG Evaluation: Current Status and Challenges.\n",
      " Yes. The paper \"Calibrating LLM-Based Evaluator\" is relevant to the topic as it focuses on Language Model (LLM) based Natural Language Generation (NLG) evaluation.\n",
      " No. The paper \"Defining Data Science\" does not seem to be relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " No, the paper \"Introduction: Cognitive Issues in Natural Language Processing\" does not seem to be directly relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " No. The given paper is not relevant to the topic as it focuses on translating pi-calculus FCPs (Finite State Processes) to Safe Petri Nets, which is not related to LLM-based Natural Language Generation (NLG) Evaluation and its current status and challenges.\n",
      " Yes, the paper \"RAGAS: Automated Evaluation of Retrieval Augmented Generation\" is relevant to the topic as it discusses evaluation methods for NLG (Natural Language Generation), specifically in the context of retrieval-augmented generation. The paper uses large language models (LLMs) as part of their approach, making it even more closely related to the given topic.\n",
      " Yes. The paper may discuss aspects of Natural Language Generation (NLG) evaluation using Large Language Models (LLMs) within the context of Computer Science.\n",
      " I would say that the paper \"Generative Judge for Evaluating Alignment\" could be relevant to the topic of LLM-based NLG evaluation, as it discusses the use of a generative model for evaluating alignment between machine-generated text and human references. However, the title does not explicitly mention LLMs or natural language generation, so it may not be the most direct contribution to the specific subtopic of \"LLM-based NLG Evaluation: Current Status and Challenges.\" Nonetheless, the paper's focus on evaluation methods for natural language generation makes it potentially relevant. Therefore, my answer is: Yes.\n",
      " No. The paper \"Mathematical Logic in Computer Science\" does not seem to be directly related to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " No. The paper \"FAIR and Open Computer Science Research Software\" does not seem to be directly related to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " No. 'Why The Trans Programmer?' is not directly related to the topic of LLM-based NLG Evaluation: Current Status and Challenges.\n",
      " Yes. The paper \"Style Over Substance: Evaluation Biases for Large Language Models\" discusses evaluation methods and challenges related to natural language generation (NLG) systems, specifically large language models, which is relevant to the topic of LLM-based NLG Evaluation.\n",
      " No. The paper \"Decidability of higher-order matching\" does not seem to be directly related to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'.\n",
      " Yes. The paper \"The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics\" is relevant to the topic of LLM-based NLG (Natural Language Generation) evaluation.\n",
      " Yes, the paper \"PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\" is relevant to the topic of LLM-based NLG (Natural Language Generation) evaluation.\n",
      " Yes. The paper \"Benchmarking Foundation Models with Language-Model-as-an-Examiner\" is relevant to the topic of LLM-based NLG evaluation as it also deals with evaluating language models for natural language generation tasks.\n",
      "['2308.04592', '2303.07610', '2309.13308', '2309.15217', '2207.07901', '2310.05470', '2307.03025', '2310.19792', '2307.02762', '2306.04181']\n"
     ]
    }
   ],
   "source": [
    "def ask_relevant_titles(topic,paper):\n",
    "    choice = paper['title']\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "                    Indicate if the following paper is relevant: {choice}.\n",
    "                    Only say yes or no. Please provide your answer below:\"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer\n",
    "def ask_llm_one_by_one():\n",
    "    answer = []\n",
    "    for id,paper  in data.items():\n",
    "        a = ask_relevant_titles(title,paper)\n",
    "        print(a)\n",
    "        if 'Yes' in a:\n",
    "            answer.append(id)\n",
    "    return answer\n",
    "answertitle = ask_llm_one_by_one()\n",
    "print(answertitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 1, 1, 9)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_step(answertitle)\n",
    "#It makes good guesses: Following 2 are wrong. But they are reasonable based on the information the llm got.\n",
    "#Yes. The paper may discuss aspects of Natural Language Generation (NLG) evaluation using Large Language Models (LLMs) within the context of Computer Science.\n",
    "#No, the paper \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" does not seem to be directly relevant to the topic of 'LLM-based NLG Evaluation: Current Status and Challenges'. The paper focuses on Llama 2, an open foundation and fine-tuned chat model, while the topic is about the evaluation methods and challenges for Natural Language Generation (NLG) based on Large Language Models (LLMs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over all original papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The original state of the art paper\n",
    "#paper_id = \"2402.01383v1\"\n",
    "#paper_id = \"2402.06196\"\n",
    "#paper_id = \"2408.02304\"\n",
    "#paper_id = \"2408.02464\"\n",
    "#paper_id = \"2408.02085\"\n",
    "#paper_id = \"2311.13731\"\n",
    "paper_id = \"2311.12785\"\n",
    "original = get_json('../task3/dataset/'+paper_id+'data')\n",
    "original['title']\n",
    "topic = original['title']+': '+ original['abstract']\n",
    "topic\n",
    "data = get_json('dataset/'+paper_id)\n",
    "len(data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2306.13651', '2305.01625', '2307.16888', '2303.04226', '2111.01998', '1810.04805', '2302.07842', '2304.11062', '2303.08774', '2308.00352']\n"
     ]
    }
   ],
   "source": [
    "def ask_relevant(topic,paper):\n",
    "    choice = paper['title']+paper['abstract']\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "                    Indicate if the following paper is relevant: {choice}.\n",
    "                    Only say yes or no. Please provide your answer below:\"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer\n",
    "\n",
    "def evaluate_step(n3):\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for key, value in data.items():\n",
    "        if key in n3:\n",
    "            if value['label']==1:\n",
    "                TP+=1\n",
    "            else:\n",
    "                FP+=1\n",
    "        else:\n",
    "            if value['label']==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    return(TP,FP,FN,TN)\n",
    "def ask_llm_one_by_one():\n",
    "    answer = []\n",
    "    for id,paper  in data.items():\n",
    "        a = ask_relevant(topic,paper)\n",
    "        if 'Yes' in a:\n",
    "            answer.append(id)\n",
    "    return answer\n",
    "answer1 = ask_llm_one_by_one()\n",
    "print(answer1)\n",
    "TP,FP,FN,TN = evaluate_step(answer1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0 0 10\n"
     ]
    }
   ],
   "source": [
    "# Append line to the file\n",
    "print(TP,FP,FN,TN)\n",
    "P = TP/(TP+FP)\n",
    "R = TP/(TP+FN)\n",
    "F1 = 2*P*R/(P+R)\n",
    "with open('results/ask_one_by_one.txt', 'a') as file:\n",
    "    file.write(f\"{paper_id} {P} {R} {F1}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real case. Don't give a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1606.06565,\n",
       " 2305.10403,\n",
       " 2306.04181,\n",
       " 2308.07201,\n",
       " 2310.00785,\n",
       " 2210.11416,\n",
       " 2309.15217,\n",
       " 2302.04166,\n",
       " 2311.00681,\n",
       " 2312.10355,\n",
       " 2309.13701,\n",
       " 2310.00074,\n",
       " 2303.0761,\n",
       " 2310.00752,\n",
       " 2311.18702,\n",
       " 2310.08491,\n",
       " 2309.13633,\n",
       " 2311.00686,\n",
       " 2310.19792,\n",
       " 2310.0547,\n",
       " 2310.1974,\n",
       " 2307.02762,\n",
       " 2310.01432,\n",
       " 2311.09184,\n",
       " 2305.14239,\n",
       " 2305.14658,\n",
       " 2309.13308,\n",
       " 2307.07889,\n",
       " 2303.15621,\n",
       " 2310.15123,\n",
       " 2206.05802,\n",
       " 2311.09204,\n",
       " 2307.09288,\n",
       " 2305.17926,\n",
       " 2308.04592,\n",
       " 2310.11593,\n",
       " 2306.05087,\n",
       " 2309.12546,\n",
       " 2307.03025,\n",
       " 2307.10928,\n",
       " 2401.00437,\n",
       " 2312.15407,\n",
       " 2308.01862,\n",
       " 2306.05685,\n",
       " 2310.17631]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the CSV file into a DataFrame\n",
    "df = pd.read_csv('task2/dataset/'+paper_id+'_all.csv')\n",
    "all_numbers = list(df['arxiv_id'])\n",
    "all_numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relvant_papers(topic, date,n_papers):\n",
    "    buffer = 10\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "    query = topic,\n",
    "    max_results = n_papers+buffer,\n",
    "    # sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    # sort_by = arxiv.SortCriterion.Relevance #Is default\n",
    "\n",
    "    )\n",
    "\n",
    "    results = list(client.results(search))\n",
    "    # Lists to hold all IDs and their labels\n",
    "    all_ids = []\n",
    "    papers = []\n",
    "    i = 0\n",
    "    while (len(all_ids)<n_papers) and (i<len(results)):\n",
    "        result = results[i]\n",
    "        arxiv_number = result.entry_id.split('/')[-1]\n",
    "        arxiv_number = arxiv_number.split('v')[0]\n",
    "        submission_date = result.published\n",
    "        #check if before the overview\n",
    "        if (submission_date<date):\n",
    "            all_ids.append(arxiv_number)\n",
    "            papers.append(result)\n",
    "\n",
    "        #print(submission_date)\n",
    "        #print(result.title)\n",
    "        i+=1\n",
    "    return all_ids,papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-02-02 13:06:35+00:00\n",
      "LLM-based NLG Evaluation: Current Status and Challenges\n"
     ]
    }
   ],
   "source": [
    "#find_relvant_papers(title, date,n_papers)\n",
    "date = arxiv_tool.fetch_paper_dates([paper_id])[0]\n",
    "print(date)\n",
    "title = arxiv_tool.fetch_paper_titles([paper_id])[0]\n",
    "print(title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted,pre_paper = find_relvant_papers(title, date,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2006.14799\n",
      "Evaluation of Text Generation: A Survey\n",
      "2402.01383v1\n",
      "1606.06565\n"
     ]
    }
   ],
   "source": [
    "print(predicted[0])\n",
    "print(pre_paper[0].title)\n",
    "print(paper_id)\n",
    "print(all_numbers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in predicted:\n",
    "    if p in all_numbers:\n",
    "        print(1)\n",
    "if 2006.14799 in all_numbers:\n",
    "    print(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_relevant(topic,choice):\n",
    "    instruction = f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "                    Indicate if the following paper is relevant: {choice}.\n",
    "                    Only say yes or no. Please provide your answer below:\"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: arxiv gave 10 papers related to the topic, the first one was the original which was removed by the date (as it has to be strictly older). None of the proposed paper are in the references of the original papers. Arxiv search with matching keywords. \n",
    "Asking the llm all of these proposed papers are relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, this paper is not relevant to LLM-based NLG Evaluation: Current Status and Challenges as it focuses on Nonlocal Gravity (NLG) in the context of classical physics, specifically gravitation, rather than Natural Language Generation (NLG) evaluation.\n",
      " No, this paper is not directly relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on the automatic extraction of subgrammars for controlling and speeding up NLG using explanation-based learning (EBL), rather than evaluating or discussing challenges related to LLM-based NLG evaluation specifically.\n",
      " No, the paper is not directly relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on Abductive Reasoning and benchmarks like aNLI and aNLG, but it does not specifically discuss LLMs or Natural Language Generation evaluation methods.\n",
      " No, the paper does not seem relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on Nonlocal Gravity (NLG) in astrophysics and cosmology, specifically discussing its implications for effective dark matter in three ultra-diffuse galaxies. There is no mention or relevance to LLM-based Natural Language Generation (NLG) evaluation or current challenges in this area.\n",
      " No. The paper does not seem relevant to the topic of LLM-based NLG Evaluation: Current Status and Challenges as it does not discuss any research related to evaluation, current status, or challenges in NLG using LLMs.\n",
      " Based on the title and abstract provided, it seems that this paper focuses on using NLG (Natural Language Generation) to document eBusiness models, rather than evaluating LLM-based NLG specifically. Therefore, I would classify this paper as not directly relevant to the topic of \"LLM-based NLG Evaluation: Current Status and Challenges.\"\n"
     ]
    }
   ],
   "source": [
    "for p in pre_paper:\n",
    "    answer = []\n",
    "    a = ask_relevant(title,p.summary)\n",
    "    #print(a)\n",
    "    if 'Yes' in a:\n",
    "        answer.append(title)\n",
    "    else:\n",
    "         print(a) \n",
    "# No, this paper is not relevant to LLM-based NLG Evaluation: Current Status and Challenges as it focuses on Nonlocal Gravity (NLG) in the context of classical physics, specifically gravitation, rather than Natural Language Generation (NLG) evaluation.\n",
    "# No, this paper is not directly relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on the automatic extraction of subgrammars for controlling and speeding up NLG using explanation-based learning (EBL), rather than evaluating or discussing challenges related to LLM-based NLG evaluation specifically.\n",
    "# No, the paper is not directly relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on Abductive Reasoning and benchmarks like aNLI and aNLG, but it does not specifically discuss LLMs or Natural Language Generation evaluation methods.\n",
    "# No, the paper does not seem relevant to LLM-based NLG Evaluation: Current Status and Challenges. The paper focuses on Nonlocal Gravity (NLG) in astrophysics and cosmology, specifically discussing its implications for effective dark matter in three ultra-diffuse galaxies. There is no mention or relevance to LLM-based Natural Language Generation (NLG) evaluation or current challenges in this area.\n",
    "# No. The paper does not seem relevant to the topic of LLM-based NLG Evaluation: Current Status and Challenges as it does not discuss any research related to evaluation, current status, or challenges in NLG using LLMs.\n",
    "# Based on the title and abstract provided, it seems that this paper focuses on using NLG (Natural Language Generation) to document eBusiness models, rather than evaluating LLM-based NLG specifically. Therefore, I would classify this paper as not directly relevant to the topic of \"LLM-based NLG Evaluation: Current Status and Challenges.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of 100 papers from arxiv related to the topic :LLM-based NLG Evaluation: Current Status and Challenges\n",
    "The llm filters out 6 papers. It was able to filter out papers that are there by mistake, for example it was there by mismatching the abreviation NLG (natural language genereation) with NLG (nonlocal gravity). However these were far in the list of arxiv, it is also very capable of selecting relevant papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_relvant_papers(topic, date,n_papers):\n",
    "    buffer = 10\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "    query = topic,\n",
    "    max_results = n_papers+buffer,\n",
    "    # sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    # sort_by = arxiv.SortCriterion.Relevance #Is default\n",
    "\n",
    "    )\n",
    "\n",
    "    results = list(client.results(search))\n",
    "    print(len(results))\n",
    "    # Lists to hold all IDs and their labels\n",
    "    all_ids = []\n",
    "    papers = []\n",
    "    i = 0\n",
    "    while (len(all_ids)<n_papers) and (i<len(results)):\n",
    "        result = results[i]\n",
    "        arxiv_number = result.entry_id.split('/')[-1]\n",
    "        arxiv_number = arxiv_number.split('v')[0]\n",
    "        submission_date = result.published\n",
    "        #check if before the overview\n",
    "        if (submission_date<date):\n",
    "            all_ids.append(arxiv_number)\n",
    "            papers.append(result)\n",
    "\n",
    "        #print(submission_date)\n",
    "        #print(result.title)\n",
    "        i+=1\n",
    "    return all_ids,papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-based NLG Evaluation: Current Status and Challenges: Evaluating natural language generation (NLG) is a vital but challenging\n",
      "problem in artificial intelligence. Traditional evaluation metrics mainly\n",
      "capturing content (e.g. n-gram) overlap between system outputs and references\n",
      "are far from satisfactory, and large language models (LLMs) such as ChatGPT\n",
      "have demonstrated great potential in NLG evaluation in recent years. Various\n",
      "automatic evaluation methods based on LLMs have been proposed, including\n",
      "metrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\n",
      "evaluation data. In this survey, we first give a taxonomy of LLM-based NLG\n",
      "evaluation methods, and discuss their pros and cons, respectively. We also\n",
      "discuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\n",
      "open problems in this area and point out future research directions.\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(topic)\n",
    "predicted_topic,pre_paper_topic = find_relvant_papers(topic, date,50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that to give the abstract of the original paper is too long and complex that the arxiv finds any papers. Maybe this case has to be taken care of when it happens\n",
    "and the topic has to be compressed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unused"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_with_summary(topic,summaries):\n",
    "    instruction = \"Indicate all relevant papers to the topic: \"+topic+\". Which papers of the following are relevant?:\"\n",
    "    choice = \"/n\".join([\"**\"+str(n)+\"** \"+i['title']+i['summary'] for n,i in enumerate(summaries)])\n",
    "    clearification = \"write the number of the paper of the numbers i gave you. Give only the number of releveant papers. Don't use any other numbers. Don't give any explanation. Give all the relevant papers.\"\n",
    "    #answer = llm.invoke(instruction + choice + clearification)\n",
    "    answer = llm.invoke(f\"\"\"Your task is to determine which papers are relevant to the topic: {topic}.\n",
    "\n",
    "Below is a list of papers. Each paper is numbered and includes its title and summary.\n",
    "\n",
    "List of Papers:\n",
    "\"\n",
    "{choice}\n",
    "\"\n",
    "\n",
    "Instructions:\n",
    "\n",
    "    Indicate which papers are relevant to the topic by writing only the numbers of the relevant papers.\n",
    "    Do not provide any explanations or use any other numbers.\n",
    "    Format your answer as a list of numbers separated by commas (e.g., 1, 3, 5).\n",
    "\n",
    "Please provide your answer below:\"\"\")\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(answer,data):\n",
    "    n3 = re.findall(r'\\d{4}\\.\\d{5}',answer)\n",
    "    #print(n3)\n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    for key, value in data.items():\n",
    "        if key in n3:\n",
    "            if value['label']==1:\n",
    "                TP+=1\n",
    "            elif value['label']==0:\n",
    "                FP+=1\n",
    "            else:\n",
    "                print(\"problem\")\n",
    "        else:\n",
    "            if value['label']==1:\n",
    "                FN+=1\n",
    "            else:\n",
    "                TN+=1\n",
    "    precision = TP/(TP+FP)\n",
    "    recall = TP/(TP+FN)\n",
    "    print(\"out of \",TP+FP,\" predictions are \",TP,\" correct.\")\n",
    "    print(\"out of \",TP+FN,\" related papers were \",TP,\" found.\")\n",
    "    F1 = 2*precision*recall/(precision+recall)\n",
    "    return(precision,recall,F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffle the list of summaries given to the llm 10 times. How big is difference?\n",
    "scores = []\n",
    "#answer3 = select_with_summary_id(topic,data) previously executed\n",
    "P,R,F1 = evaluate(answer3,data)\n",
    "scores.append([round(float(P),5),round(float(R),5),round(float(F1),5)])\n",
    "for i in range(9):\n",
    "    #shuffle data\n",
    "    items = list(data.items())    \n",
    "    random.shuffle(items)\n",
    "    shuffled_dict = dict(items)\n",
    "    answer = select_with_summary_id(topic,shuffled_dict)\n",
    "    P, R, F1 = evaluate(answer, shuffled_dict)\n",
    "    scores.append([round(float(P),5),round(float(R),5),round(float(F1),5)])\n",
    "\n",
    "m = np.mean(scores,axis=0)\n",
    "s = np.std(scores,axis=0)\n",
    "scores.append(m)\n",
    "scores.append(s)\n",
    "print(np.matrix(scores))\n",
    "filename = 'title_and_summaries'\n",
    "np.savetxt('task2/results/'+filename+'.txt',scores,fmt='%.4f')\n",
    "\"\"\"out of  4  predictions are  4  correct.\n",
    "out of  10  related papers were  4  found.\n",
    "out of  4  predictions are  4  correct.\n",
    "out of  10  related papers were  4  found.\n",
    "out of  5  predictions are  2  correct.\n",
    "out of  10  related papers were  2  found.\n",
    "out of  6  predictions are  2  correct.\n",
    "out of  10  related papers were  2  found.\n",
    "out of  5  predictions are  3  correct.\n",
    "out of  10  related papers were  3  found.\n",
    "out of  6  predictions are  1  correct.\n",
    "out of  10  related papers were  1  found.\n",
    "out of  5  predictions are  3  correct.\n",
    "out of  10  related papers were  3  found.\n",
    "out of  6  predictions are  3  correct.\n",
    "out of  10  related papers were  3  found.\n",
    "out of  6  predictions are  4  correct.\n",
    "out of  10  related papers were  4  found.\n",
    "out of  6  predictions are  2  correct.\n",
    "out of  10  related papers were  2  found.\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'title_and_summaries'\n",
    "np.savetxt('task2/results/'+filename+'_rouge.txt',scores,fmt='%.4f')\n",
    "#The last two rows are metrics for the values before. We see that the recall is pretty low in all the combinations. Meaning the llm found only few sources \n",
    "# and this pretty much regardless of how many it suggested, see the standard deviationof the recall in last line and see that the nummber of correct sources \n",
    "# are between 2 and 4 with 1 beeing an exception. \n",
    "# The precision on the other hand does vary quite much, it has a higher standard deviation (of 0.26238). The precision is lower when it suggests more papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Are the suggestions overlapping or are they the same?\n",
    "scores = []\n",
    "#answer3 = select_with_summary_id(topic,data) previously executed\n",
    "P,R,F1 = evaluate(answer3,data)\n",
    "scores.append([round(float(P),5),round(float(R),5),round(float(F1),5)])\n",
    "all_answers = answer3\n",
    "for i in range(9):\n",
    "    #shuffle data\n",
    "    items = list(data.items())    \n",
    "    random.shuffle(items)\n",
    "    shuffled_dict = dict(items)\n",
    "    answer = select_with_summary_id(topic,shuffled_dict)\n",
    "    P, R, F1 = evaluate(answer, shuffled_dict)\n",
    "    scores.append([round(float(P),5),round(float(R),5),round(float(F1),5)])\n",
    "    all_answer = all_answers+answer\n",
    "\n",
    "m = np.mean(scores,axis=0)\n",
    "s = np.std(scores,axis=0)\n",
    "scores.append(m)\n",
    "scores.append(s)\n",
    "print(np.matrix(scores))\n",
    "print(all_answers)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
