{
    "2308.07201": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "date": "2023-08-14T15:13:04+00:00",
        "label": 1
    },
    "1802.03292": {
        "title": "Mathematical Logic in Computer Science",
        "abstract": "The article retraces major events and milestones in the mutual influences\nbetween mathematical logic and computer science since the 1950s.",
        "date": "2018-02-07T22:21:43+00:00",
        "label": 0
    },
    "2310.01432": {
        "title": "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators",
        "abstract": "Large language models (LLMs) have shown promise as automated evaluators for\nassessing the quality of answers generated by AI systems. However, these\nLLM-based evaluators exhibit position bias, or inconsistency, when used to\nevaluate candidate answers in pairwise comparisons, favoring either the first\nor second answer regardless of content. To address this limitation, we propose\nPORTIA, an alignment-based system designed to mimic human comparison strategies\nto calibrate position bias in a lightweight yet effective manner. Specifically,\nPORTIA splits the answers into multiple segments, aligns similar content across\ncandidate answers, and then merges them back into a single prompt for\nevaluation by LLMs. We conducted extensive experiments with six diverse LLMs to\nevaluate 11,520 answer pairs. Our results show that PORTIA markedly enhances\nthe consistency rates for all the models and comparison forms tested, achieving\nan average relative improvement of 47.46%. Remarkably, PORTIA enables less\nadvanced GPT models to achieve 88% agreement with the state-of-the-art GPT-4\nmodel at just 10% of the cost. Furthermore, it rectifies around 80% of the\nposition bias instances within the GPT-4 model, elevating its consistency rate\nup to 98%. Subsequent human evaluations indicate that the PORTIA-enhanced\nGPT-3.5 model can even surpass the standalone GPT-4 in terms of alignment with\nhuman evaluators. These findings highlight PORTIA's ability to correct position\nbias, improve LLM consistency, and boost performance while keeping\ncost-efficiency. This represents a valuable step toward a more reliable and\nscalable use of LLMs for automated evaluations across diverse applications.",
        "date": "2023-09-29T14:38:58+00:00",
        "label": 1
    },
    "1607.03760": {
        "title": "Distributed Games and Strategies",
        "abstract": "A summary of work on distributed games and strategies done within the first\nthree years of the ERC project ECSYM is presented.",
        "date": "2016-07-13T14:25:03+00:00",
        "label": 0
    },
    "2205.01553": {
        "title": "Why The Trans Programmer?",
        "abstract": "Through online anecdotal evidence and online communities, there is an\nin-group idea of trans people (specifically trans-feminine individuals)\ndisproportionately entering computer science education & fields. Existing data\nsuggests this is a plausible trend, yet no research has been done into exactly\nwhy. As computer science education (traditional schooling or self-taught\nmethods) is integral to working in computer science fields, a simple research\nsurvey was conducted to gather data on 138 trans people's experiences with\ncomputer science & computer science education. This article's purpose is to\nshed insight on the motivations for trans individuals choosing computer science\npaths, while acting as a basis and call to action for further research.",
        "date": "2022-05-03T15:06:23+00:00",
        "label": 0
    },
    "2201.05852": {
        "title": "Data Science in Perspective",
        "abstract": "Data and Science has stood out in the generation of results, whether in the\nprojects of the scientific domain or business domain. CERN Project, Scientific\nInstitutes, companies like Walmart, Google, Apple, among others, need data to\npresent their results and make predictions in the competitive data world. Data\nand Science are words that together culminated in a globally recognized term\ncalled Data Science. Data Science is in its initial phase, possibly being part\nof formal sciences and also being presented as part of applied sciences,\ncapable of generating value and supporting decision making. Data Science\nconsiders science and, consequently, the scientific method to promote decision\nmaking through data intelligence. In many cases, the application of the method\n(or part of it) is considered in Data Science projects in scientific domain\n(social sciences, bioinformatics, geospatial projects) or business domain\n(finance, logistic, retail), among others. In this sense, this article\naddresses the perspectives of Data Science as a multidisciplinary area,\nconsidering science and the scientific method, and its formal structure which\nintegrate Statistics, Computer Science, and Business Science, also taking into\naccount Artificial Intelligence, emphasizing Machine Learning, among others.\nThe article also deals with the perspective of applied Data Science, since Data\nScience is used for generating value through scientific and business projects.\nData Science persona is also discussed in the article, concerning the education\nof Data Science professionals and its corresponding profiles, since its\nprojection changes the field of data in the world.",
        "date": "2022-01-15T13:51:12+00:00",
        "label": 0
    },
    "2307.10928": {
        "title": "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets",
        "abstract": "Evaluation of Large Language Models (LLMs) is challenging because\ninstruction-following necessitates alignment with human values and the required\nset of skills varies depending on the instruction. However, previous studies\nhave mainly focused on coarse-grained evaluation (i.e. overall preference-based\nevaluation), which limits interpretability since it does not consider the\nnature of user instructions that require instance-wise skill composition. In\nthis paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\nAlignment Skill Sets), a fine-grained evaluation protocol for both human-based\nand model-based evaluation which decomposes coarse-level scoring to a skill\nset-level scoring for each instruction. We experimentally observe that the\nfine-graininess of evaluation is crucial for attaining a holistic view of model\nperformance and increasing the reliability of the evaluation. Using FLASK, we\ncompare multiple open-source and proprietary LLMs and observe a high\ncorrelation between model-based and human-based evaluations. We publicly\nrelease the evaluation data and code implementation at\nhttps://github.com/kaistAI/FLASK.",
        "date": "2023-07-20T14:56:35+00:00",
        "label": 1
    },
    "2302.04166": {
        "title": "GPTScore: Evaluate as You Desire",
        "abstract": "Generative Artificial Intelligence (AI) has enabled the development of\nsophisticated models that are capable of producing high-caliber text, images,\nand other outputs through the utilization of large pre-trained models.\nNevertheless, assessing the quality of the generation is an even more arduous\ntask than the generation itself, and this issue has not been given adequate\nconsideration recently. This paper proposes a novel evaluation framework,\nGPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction)\nof generative pre-trained models to score generated texts. There are 19\npre-trained models explored in this paper, ranging in size from 80M (e.g.,\nFLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text\ngeneration tasks, 22 evaluation aspects, and corresponding 37 datasets\ndemonstrate that this approach can effectively allow us to achieve what one\ndesires to evaluate for texts simply by natural language instructions. This\nnature helps us overcome several long-standing challenges in text\nevaluation--how to achieve customized, multi-faceted evaluation without the\nneed for annotated samples. We make our code publicly available at\nhttps://github.com/jinlanfu/GPTScore.",
        "date": "2023-02-08T16:17:29+00:00",
        "label": 1
    },
    "1111.4755": {
        "title": "Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive Case",
        "abstract": "This paper describes the solution of Hello World transformations in MOLA\ntransformation language. Transformations implementing the task are relatively\nstraightforward and easily inferable from the task specification. The required\nadditional steps related to model import and export are also described.",
        "date": "2011-11-21T05:26:57+00:00",
        "label": 0
    },
    "2309.15217": {
        "title": "RAGAS: Automated Evaluation of Retrieval Augmented Generation",
        "abstract": "We introduce RAGAs (Retrieval Augmented Generation Assessment), a framework\nfor reference-free evaluation of Retrieval Augmented Generation (RAG)\npipelines. RAG systems are composed of a retrieval and an LLM based generation\nmodule, and provide LLMs with knowledge from a reference textual database,\nwhich enables them to act as a natural language layer between a user and\ntextual databases, reducing the risk of hallucinations. Evaluating RAG\narchitectures is, however, challenging because there are several dimensions to\nconsider: the ability of the retrieval system to identify relevant and focused\ncontext passages, the ability of the LLM to exploit such passages in a faithful\nway, or the quality of the generation itself. With RAGAs, we put forward a\nsuite of metrics which can be used to evaluate these different dimensions\n\\textit{without having to rely on ground truth human annotations}. We posit\nthat such a framework can crucially contribute to faster evaluation cycles of\nRAG architectures, which is especially important given the fast adoption of\nLLMs.",
        "date": "2023-09-26T19:23:54+00:00",
        "label": 1
    },
    "2310.11593": {
        "title": "Automated Evaluation of Personalized Text Generation using Large Language Models",
        "abstract": "Personalized text generation presents a specialized mechanism for delivering\ncontent that is specific to a user's personal context. While the research\nprogress in this area has been rapid, evaluation still presents a challenge.\nTraditional automated metrics such as BLEU and ROUGE primarily measure lexical\nsimilarity to human-written references, and are not able to distinguish\npersonalization from other subtle semantic aspects, thus falling short of\ncapturing the nuances of personalized generated content quality. On the other\nhand, human judgments are costly to obtain, especially in the realm of\npersonalized evaluation. Inspired by these challenges, we explore the use of\nlarge language models (LLMs) for evaluating personalized text generation, and\nexamine their ability to understand nuanced user context. We present AuPEL, a\nnovel evaluation method that distills three major semantic aspects of the\ngenerated text: personalization, quality and relevance, and automatically\nmeasures these aspects. To validate the effectiveness of AuPEL, we design\ncarefully controlled experiments and compare the accuracy of the evaluation\njudgments made by LLMs versus that of judgements made by human annotators, and\nconduct rigorous analyses of the consistency and sensitivity of the proposed\nmetric. We find that, compared to existing evaluation metrics, AuPEL not only\ndistinguishes and ranks models based on their personalization abilities more\naccurately, but also presents commendable consistency and efficiency for this\ntask. Our work suggests that using LLMs as the evaluators of personalized text\ngeneration is superior to traditional text similarity metrics, even though\ninteresting new challenges still remain.",
        "date": "2023-10-17T21:35:06+00:00",
        "label": 1
    },
    "0911.1672": {
        "title": "Biological Computing Fundamentals and Futures",
        "abstract": "The fields of computing and biology have begun to cross paths in new ways. In\nthis paper a review of the current research in biological computing is\npresented. Fundamental concepts are introduced and these foundational elements\nare explored to discuss the possibilities of a new computing paradigm. We\nassume the reader to possess a basic knowledge of Biology and Computer Science",
        "date": "2009-11-09T13:16:01+00:00",
        "label": 0
    },
    "1606.01148": {
        "title": "Tripartite Unions",
        "abstract": "This note provides conditions under which the union of three well-founded\nbinary relations is also well-founded.",
        "date": "2016-06-03T15:41:55+00:00",
        "label": 0
    },
    "1904.01053": {
        "title": "Computer simulations in science and engineering - Concepts - Practices - Perspectives",
        "abstract": "The ubiquitous presence of computer simulations in all kinds of research\nareas evidence their role as the new driving force for the advancement of\nscience and engineering research. Nothing seems to escape the image of success\nthat computer simulations project onto the research community and the general\npublic. One simple way to illustrate this consists of asking ourselves how\nwould contemporary science and engineering look like without the use of\ncomputer simulations. The answer would certainly diverge from the current image\nwe have of scientific and engineering research.\n  As much as computer simulations are successful, they are also methods that\nfail in their purpose of inquiring about the world; and as much as researchers\nmake use of them, computer simulations raise important questions that are at\nthe heart of contemporary science and engineering practice. In this respect,\ncomputer simulations make a fantastic subject of research for the natural\nsciences, the social sciences, engineering and, as in our case, also for\nphilosophy. Studies on computer simulations touch upon many different facets of\nscientific and engineering research and evoke philosophically inclined\nquestions of interpretation with close ties to problems in experimental\nsettings and engineering applications (...)",
        "date": "2019-03-09T15:26:05+00:00",
        "label": 0
    },
    "1707.04352": {
        "title": "Advances in Artificial Intelligence Require Progress Across all of Computer Science",
        "abstract": "Advances in Artificial Intelligence require progress across all of computer\nscience.",
        "date": "2017-07-13T23:11:18+00:00",
        "label": 0
    },
    "2311.00686": {
        "title": "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task",
        "abstract": "This paper describes and analyzes our participation in the 2023 Eval4NLP\nshared task, which focuses on assessing the effectiveness of prompt-based\ntechniques to empower Large Language Models to handle the task of quality\nestimation, particularly in the context of evaluating machine translations and\nsummaries. We conducted systematic experiments with various prompting\ntechniques, including standard prompting, prompts informed by annotator\ninstructions, and innovative chain-of-thought prompting. In addition, we\nintegrated these approaches with zero-shot and one-shot learning methods to\nmaximize the efficacy of our evaluation procedures. Our work reveals that\ncombining these approaches using a \"small\", open source model (orca_mini_v3_7B)\nyields competitive results.",
        "date": "2023-11-01T17:44:35+00:00",
        "label": 1
    },
    "2103.10489": {
        "title": "Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective",
        "abstract": "From a computer science perspective, addressing on-line hate speech is a\nchallenging task that is attracting the attention of both industry (mainly\nsocial media platform owners) and academia. In this chapter, we provide an\noverview of state-of-the-art data-science approaches - how they define hate\nspeech, which tasks they solve to mitigate the phenomenon, and how they address\nthese tasks. We limit our investigation mostly to (semi-)automatic detection of\nhate speech, which is the task that the majority of existing computer science\nworks focus on. Finally, we summarize the challenges and the open problems in\nthe current data-science research and the future directions in this field. Our\naim is to prepare an easily understandable report, capable to promote the\nmultidisciplinary character of hate speech research. Researchers from other\ndomains (e.g., psychology and sociology) can thus take advantage of the\nknowledge achieved in the computer science domain but also contribute back and\nhelp improve how computer science is addressing that urgent and socially\nrelevant issue which is the prevalence of hate speech in social media.",
        "date": "2021-03-18T19:19:44+00:00",
        "label": 0
    },
    "2310.00074": {
        "title": "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation",
        "abstract": "To comprehensively gauge the capacity of current models for complex\nreasoning, it is crucial to assess their step-by-step reasoning in a scalable\nmanner. Established reference-based evaluation metrics rely on human-annotated\nreasoning chains as references to assess the model-derived chains. However,\nsuch \"gold-standard\" human-written reasoning chains may not be unique and their\nacquisition is often labor-intensive. Existing reference-free reasoning\nevaluation metrics, while eliminating the need for human-crafted reasoning\nchains as references, often require fine-tuning with human-derived chains\nbefore evaluation, complicating the process and questioning their adaptability\nto other datasets. To address these challenges, we harness GPT-4 to\nautomatically evaluate reasoning chain quality, thereby removing the dependency\non human-written reasoning chains for both model fine-tuning and evaluative\npurposes. Leveraging the Socratic method, we develop SocREval ({\\bf Soc}ratic\nMethod-Inspired {\\bf R}easoning {\\bf Eval}uation), a novel approach for prompt\ndesign in reference-free reasoning evaluation. Empirical results from four\nhuman annotated datasets reveal that SocREval significantly improves GPT-4's\nperformance, surpassing existing reference-free and reference-based reasoning\nevaluation metrics. Beyond its demonstrated efficacy, SocREval, proves to be\nboth cost-efficient and robust to prompt writing and example selection, as\nsubstantiated by our in-depth analysis.",
        "date": "2023-09-29T18:25:46+00:00",
        "label": 1
    },
    "2305.14658": {
        "title": "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response",
        "abstract": "LLMs (large language models) such as ChatGPT have shown remarkable language\nunderstanding and generation capabilities. Although reference-free evaluators\nbased on LLMs show better human alignment than traditional reference-based\nevaluators, there are many challenges in using reference-free evaluators based\non LLMs. Reference-free evaluators are more suitable for open-ended examples\nwith different semantics responses. But not all examples are open-ended. For\nclosed-ended examples with unique correct semantic response, reference-free\nevaluators will still consider it high quality when giving a response that is\ninconsistent with the facts and the semantic of reference. In order to\ncomprehensively evaluate the reliability of evaluators based on LLMs, we\nconstruct two adversarial meta-evaluation dialogue generation datasets\nKdConv-ADV and DSTC7-ADV based on KdConv and DSTC7-AVSD, respectively. Compared\nto previous meta-evaluation benchmarks, KdConv-ADV and DSTC7-ADV are much more\nchallenging since they requires evaluators to be able to reasonably evaluate\nclosed-ended examples with the help of external knowledge or even its own\nknowledge. Empirical results show that the ability of LLMs to identify\nunreasonable responses is insufficient. There are risks in using eference-free\nevaluators based on LLMs to evaluate the quality of dialogue responses.",
        "date": "2023-05-24T02:52:48+00:00",
        "label": 1
    },
    "1610.07365": {
        "title": "Introduction: Cognitive Issues in Natural Language Processing",
        "abstract": "This special issue is dedicated to get a better picture of the relationships\nbetween computational linguistics and cognitive science. It specifically raises\ntwo questions: \"what is the potential contribution of computational language\nmodeling to cognitive science?\" and conversely: \"what is the influence of\ncognitive science in contemporary computational linguistics?\"",
        "date": "2016-10-24T11:30:22+00:00",
        "label": 0
    },
    "2305.14239": {
        "title": "On Learning to Summarize with Large Language Models as References",
        "abstract": "Recent studies have found that summaries generated by large language models\n(LLMs) are favored by human annotators over the original reference summaries in\ncommonly used summarization datasets. Therefore, we investigate a new learning\nsetting of text summarization models that considers the LLMs as the reference\nor the gold-standard oracle on these datasets. To examine the standard\npractices that are aligned with this new learning setting, we investigate two\nLLM-based summary quality evaluation methods for model training and adopt a\ncontrastive learning training method to leverage the LLM-guided learning\nsignals. Our experiments on the CNN/DailyMail and XSum datasets demonstrate\nthat smaller summarization models can achieve similar performance as LLMs under\nLLM-based evaluation. However, we found that the smaller models can not yet\nreach LLM-level performance under human evaluation despite promising\nimprovements brought by our proposed training methods. Meanwhile, we perform a\nmeta-analysis on this new learning setting that reveals a discrepancy between\nhuman and LLM-based evaluation, highlighting the benefits and risks of this\nLLM-as-reference setting we investigated.",
        "date": "2023-05-23T16:56:04+00:00",
        "label": 1
    },
    "2306.05087": {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "abstract": "Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.",
        "date": "2023-06-08T10:41:56+00:00",
        "label": 1
    },
    "1210.6636": {
        "title": "Informaticology: combining Computer Science, Data Science, and Fiction Science",
        "abstract": "Motivated by an intention to remedy current complications with Dutch\nterminology concerning informatics, the term informaticology is positioned to\ndenote an academic counterpart of informatics where informatics is conceived of\nas a container for a coherent family of practical disciplines ranging from\ncomputer engineering and software engineering to network technology, data\ncenter management, information technology, and information management in a\nbroad sense.\n  Informaticology escapes from the limitations of instrumental objectives and\nthe perspective of usage that both restrict the scope of informatics. That is\nachieved by including fiction science in informaticology and by ranking fiction\nscience on equal terms with computer science and data science, and framing (the\nstudy of) game design, evelopment, assessment and distribution, ranging from\nserious gaming to entertainment gaming, as a chapter of fiction science. A\nsuggestion for the scope of fiction science is specified in some detail.\n  In order to illustrate the coherence of informaticology thus conceived, a\npotential application of fiction to the ontology of instruction sequences and\nto software quality assessment is sketched, thereby highlighting a possible\nrole of fiction (science) within informaticology but outside gaming.",
        "date": "2012-10-24T19:24:59+00:00",
        "label": 0
    },
    "2309.13701": {
        "title": "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning",
        "abstract": "From grading papers to summarizing medical documents, large language models\n(LLMs) are evermore used for evaluation of text generated by humans and AI\nalike. However, despite their extensive utility, LLMs exhibit distinct failure\nmodes, necessitating a thorough audit and improvement of their text evaluation\ncapabilities. Here we introduce ALLURE, a systematic approach to Auditing Large\nLanguage Models Understanding and Reasoning Errors. ALLURE involves comparing\nLLM-generated evaluations with annotated data, and iteratively incorporating\ninstances of significant deviation into the evaluator, which leverages\nin-context learning (ICL) to enhance and improve robust evaluation of text by\nLLMs. Through this iterative process, we refine the performance of the\nevaluator LLM, ultimately reducing reliance on human annotators in the\nevaluation process. We anticipate ALLURE to serve diverse applications of LLMs\nin various domains related to evaluation of textual data, such as medical\nsummarization, education, and and productivity.",
        "date": "2023-09-24T17:15:58+00:00",
        "label": 1
    },
    "1909.03033": {
        "title": "The Difficulties of Addressing Interdisciplinary Challenges at the Foundations of Data Science",
        "abstract": "The National Science Foundation's Transdisciplinary Research in Principles of\nData Science (TRIPODS) program aims to integrate three areas central to the\nfoundations of data by uniting the statistics, mathematics, and theoretical\ncomputer science research communities. The program aims to provide a model for\nfunding cross-cutting research and facilitating interactions among the three\ndisciplines. Challenges associated with orchestrating fruitful interactions are\ndescribed.",
        "date": "2019-09-04T06:07:26+00:00",
        "label": 0
    },
    "2206.12669": {
        "title": "Crypto Makes AI Evolve",
        "abstract": "Adopting cryptography has given rise to a significant evolution in Artificial\nIntelligence (AI). This paper studies the path and stages of this evolution. We\nstart with reviewing existing relevant surveys, noting their shortcomings,\nespecially the lack of a close look at the evolution process and solid future\nroadmap. These shortcomings justify the work of this paper. Next, we identify,\ndefine and discuss five consequent stages in the evolution path, including\nCrypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,\nCrypto-Protected AI. Then, we establish a future roadmap for further research\nin this area, focusing on the role of quantum-inspired and bio-inspired AI.",
        "date": "2022-06-25T15:04:47+00:00",
        "label": 0
    },
    "2311.09184": {
        "title": "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization",
        "abstract": "While large language models (LLMs) already achieve strong performance on\nstandard generic summarization benchmarks, their performance on more complex\nsummarization task settings is less studied. Therefore, we benchmark LLMs on\ninstruction controllable text summarization, where the model input consists of\nboth a source article and a natural language requirement for the desired\nsummary characteristics. To this end, we curate an evaluation-only dataset for\nthis task setting and conduct human evaluation on 5 LLM-based summarization\nsystems. We then benchmark LLM-based automatic evaluation for this task with 4\ndifferent evaluation protocols and 11 LLMs, resulting in 40 evaluation methods\nin total. Our study reveals that instruction controllable text summarization\nremains a challenging task for LLMs, since (1) all LLMs evaluated still make\nfactual and other types of errors in their summaries; (2) all LLM-based\nevaluation methods cannot achieve a strong alignment with human annotators when\njudging the quality of candidate summaries; (3) different LLMs show large\nperformance gaps in summary generation and evaluation. We make our collected\nbenchmark, InstruSum, publicly available to facilitate future research in this\ndirection.",
        "date": "2023-11-15T18:25:26+00:00",
        "label": 1
    },
    "1407.7360": {
        "title": "A Taxonomy and Survey on eScience as a Service in the Cloud",
        "abstract": "Cloud computing has recently evolved as a popular computing infrastructure\nfor many applications. Scientific computing, which was mainly hosted in private\nclusters and grids, has started to migrate development and deployment to the\npublic cloud environment. eScience as a service becomes an emerging and\npromising direction for science computing. We review recent efforts in\ndeveloping and deploying scientific computing applications in the cloud. In\nparticular, we introduce a taxonomy specifically designed for scientific\ncomputing in the cloud, and further review the taxonomy with four major kinds\nof science applications, including life sciences, physics sciences, social and\nhumanities sciences, and climate and earth sciences. Our major finding is that,\ndespite existing efforts in developing cloud-based eScience, eScience still has\na long way to go to fully unlock the power of cloud computing paradigm.\nTherefore, we present the challenges and opportunities in the future\ndevelopment of cloud-based eScience services, and call for collaborations and\ninnovations from both the scientific and computer system communities to address\nthose challenges.",
        "date": "2014-07-28T09:14:35+00:00",
        "label": 0
    },
    "2105.01707": {
        "title": "ABET Accreditation: A Way Forward for PDC Education",
        "abstract": "With parallel and distributed computing (PDC) now wide-spread, modern\ncomputing programs must incorporate PDC within the curriculum. ACM and IEEE\nComputer Society's Computer Science curricular guidelines have recommended\nexposure to PDC concepts since 2013. More recently, a variety of initiatives\nhave made PDC curricular content, lectures, and labs freely available for\nundergraduate computer science programs. Despite these efforts, progress in\nensuring computer science students graduate with sufficient PDC exposure has\nbeen uneven.\n  This paper discusses the impact of ABET's revised criteria that have required\nexposure to PDC to achieve accreditation for computer science programs since\n2018. The authors reviewed 20 top ABET-accredited computer science programs and\nanalyzed how they covered the required PDC components in their curricula. Using\ntheir own institutions as case studies, the authors examine in detail how three\ndifferent ABET-accredited computer science programs covered PDC using different\napproaches, yet meeting the PDC requirements of these ABET criteria. The paper\nalso shows how ACM/IEEE Computer Society curricular guidelines for computer\nengineering and software engineering programs, along with ABET accreditation\ncriteria, can cover PDC.",
        "date": "2021-05-04T18:58:18+00:00",
        "label": 0
    },
    "2206.03276": {
        "title": "Oxford-style Debates in Telecommunication and Computer Science Education",
        "abstract": "Oxford-style debating is a well-known tool in social sciences. Such formal\ndiscussions on particular topics are widely used by historians and\nsociologists. However, when we try to go beyond standard thinking, it turns out\nthat Oxford-style debating can be a great educational tool in telecommunication\nand computer science. This article presents this unusual method of education at\ntechnical universities and in the IT industry, and describes its features and\nchallenges. Best practices and examples of debating are provided, taking into\naccount emerging topics in telecommunications and computer science, such as\ncybersecurity. The article also contains feedback from IT engineers who\nparticipated in Oxford-style debates. All this aims to encourage this form of\neducation in telecommunication and computer science.",
        "date": "2022-06-03T10:42:31+00:00",
        "label": 0
    },
    "2309.12546": {
        "title": "Automatic Answerability Evaluation for Question Generation",
        "abstract": "Conventional automatic evaluation metrics, such as BLEU and ROUGE, developed\nfor natural language generation (NLG) tasks, are based on measuring the n-gram\noverlap between the generated and reference text. These simple metrics may be\ninsufficient for more complex tasks, such as question generation (QG), which\nrequires generating questions that are answerable by the reference answers.\nDeveloping a more sophisticated automatic evaluation metric, thus, remains an\nurgent problem in QG research. This work proposes PMAN (Prompting-based Metric\non ANswerability), a novel automatic evaluation metric to assess whether the\ngenerated questions are answerable by the reference answers for the QG tasks.\nExtensive experiments demonstrate that its evaluation results are reliable and\nalign with human evaluations. We further apply our metric to evaluate the\nperformance of QG models, which shows that our metric complements conventional\nmetrics. Our implementation of a GPT-based QG model achieves state-of-the-art\nperformance in generating answerable questions.",
        "date": "2023-09-22T00:13:07+00:00",
        "label": 1
    },
    "2310.19740": {
        "title": "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation",
        "abstract": "Humans are widely involved in the evaluation of open-ended natural language\ngeneration tasks (NLG) that demand creativity, as automatic metrics often\nexhibit weak correlations with human judgments. Large language models (LLMs)\nrecently have emerged as a scalable and cost-effective alternative to human\nevaluations. However, both humans and LLMs have limitations, i.e., inherent\nsubjectivity and unreliable judgments, particularly for open-ended tasks that\nrequire adaptable metrics tailored to diverse task requirements. To explore the\nsynergy between humans and LLM-based evaluators and address the challenges of\nexisting inconsistent evaluation criteria in open-ended NLG tasks, we propose a\nCollaborative Evaluation pipeline CoEval, involving the design of a checklist\nof task-specific criteria and the detailed evaluation of texts, in which LLM\ngenerates initial ideation, and then humans engage in scrutiny. We conducted a\nseries of experiments to investigate the mutual effects between LLMs and humans\nin CoEval. Results show that, by utilizing LLMs, CoEval effectively evaluates\nlengthy texts, saving significant time and reducing human evaluation outliers.\nHuman scrutiny still plays a role, revising around 20% of LLM evaluation scores\nfor ultimate reliability.",
        "date": "2023-10-30T17:04:35+00:00",
        "label": 1
    },
    "2310.17631": {
        "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
        "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.",
        "date": "2023-10-26T17:48:58+00:00",
        "label": 1
    },
    "2310.00752": {
        "title": "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks",
        "abstract": "We present TIGERScore, a \\textbf{T}rained metric that follows\n\\textbf{I}nstruction \\textbf{G}uidance to perform \\textbf{E}xplainable, and\n\\textbf{R}eference-free evaluation over a wide spectrum of text generation\ntasks. Different from other automatic evaluation methods that only provide\narcane scores, TIGERScore is guided by natural language instruction to provide\nerror analysis to pinpoint the mistakes in the generated text. Our metric is\nbased on LLaMA-2, trained on our meticulously curated instruction-tuning\ndataset MetricInstruct which covers 6 text generation tasks and 23 text\ngeneration datasets. The dataset consists of 42K quadruple in the form of\n(instruction, input, system output $\\rightarrow$ error analysis). We collected\nthe `system outputs' through from a large variety of models to cover different\ntypes of errors. To quantitatively assess our metric, we evaluate its\ncorrelation with human ratings on 5 held-in datasets, 2 held-out datasets and\nshow that TIGERScore can achieve the open-source SoTA correlation with human\nratings across these datasets and almost approaches GPT-4 evaluator. As a\nreference-free metric, its correlation can even surpass the best existing\nreference-based metrics. To further qualitatively assess the rationale\ngenerated by our metric, we conduct human evaluation on the generated\nexplanations and found that the explanations are 70.8\\% accurate. Through these\nexperimental results, we believe TIGERScore demonstrates the possibility of\nbuilding universal explainable metrics to evaluate any text generation task.\nAll the resourced are released in our project website:\n\\url{https://tiger-ai-lab.github.io/TIGERScore/}.",
        "date": "2023-10-01T18:01:51+00:00",
        "label": 1
    },
    "2307.02762": {
        "title": "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations",
        "abstract": "Nowadays, the quality of responses generated by different modern large\nlanguage models (LLMs) is hard to evaluate and compare automatically. Recent\nstudies suggest and predominantly use LLMs for reference-free evaluation of\nopen-ended question answering. More specifically, they use the recognized\n\"strongest\" LLM as the evaluator, which conducts pairwise comparisons of\ncandidate models' answers and provides a ranking score. However, this intuitive\nmethod has multiple problems, such as bringing in self-enhancement (favoring\nits own answers) and positional bias. We draw insights and lessons from the\neducational domain (Cho & MacArthur, 2011; Walsh, 2014) to improve LLM-based\nevaluations. Specifically, we propose (1) the peer rank (PR) algorithm that\ntakes into account each peer LLM's pairwise preferences of all answer pairs,\nand outputs a final ranking of models; and (2) peer discussion (PD), where we\nprompt two LLMs to discuss and try to reach a mutual agreement on the\npreferences of two answers. We conduct experiments on two benchmark datasets.\nWe find that our approaches achieve higher accuracy and align better with human\njudgments. Interestingly, PR can induce a relatively accurate self-ranking of\nmodels under the anonymous setting, where each model's name is unrevealed. Our\nwork provides space to explore evaluating models that are hard to compare for\nhumans.",
        "date": "2023-07-06T04:05:44+00:00",
        "label": 1
    },
    "0701087": {
        "title": "Artificiality in Social Sciences",
        "abstract": "This text provides with an introduction to the modern approach of\nartificiality and simulation in social sciences. It presents the relationship\nbetween complexity and artificiality, before introducing the field of\nartificial societies which greatly benefited from the computer power fast\nincrease, gifting social sciences with formalization and experimentation tools\npreviously owned by \"hard\" sciences alone. It shows that as \"a new way of doing\nsocial sciences\", artificial societies should undoubtedly contribute to a\nrenewed approach in the study of sociality and should play a significant part\nin the elaboration of original theories of social phenomena.",
        "date": "2007-01-13T16:50:37+00:00",
        "label": 0
    },
    "2206.05802": {
        "title": "Self-critiquing models for assisting human evaluators",
        "abstract": "We fine-tune large language models to write natural language critiques\n(natural language critical comments) using behavioral cloning. On a topic-based\nsummarization task, critiques written by our models help humans find flaws in\nsummaries that they would have otherwise missed. Our models help find naturally\noccurring flaws in both model and human written summaries, and intentional\nflaws in summaries written by humans to be deliberately misleading. We study\nscaling properties of critiquing with both topic-based summarization and\nsynthetic tasks. Larger models write more helpful critiques, and on most tasks,\nare better at self-critiquing, despite having harder-to-critique outputs.\nLarger models can also integrate their own self-critiques as feedback, refining\ntheir own summaries into better ones. Finally, we motivate and introduce a\nframework for comparing critiquing ability to generation and discrimination\nability. Our measurements suggest that even large models may still have\nrelevant knowledge they cannot or do not articulate as critiques. These results\nare a proof of concept for using AI-assisted human feedback to scale the\nsupervision of machine learning systems to tasks that are difficult for humans\nto evaluate directly. We release our training datasets, as well as samples from\nour critique assistance experiments.",
        "date": "2022-06-12T17:40:53+00:00",
        "label": 1
    },
    "2202.01291": {
        "title": "Computer sciences and synthesis: retrospective and perspective",
        "abstract": "The problem of synthesis in computer sciences, including cybernetics,\nartificial intelligence and system analysis, is analyzed. Main methods of\nrealization this problem are discussed. Ways of search universal method of\ncreation universal synthetic science are represented. As example of such\nuniversal method polymetric analysis is given. Perspective of further\ndevelopment of this research, including application polymetric method for the\nresolution main problems of computer sciences, is analyzed too.",
        "date": "2022-01-26T04:42:45+00:00",
        "label": 0
    },
    "2307.09288": {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.",
        "date": "2023-07-18T14:31:57+00:00",
        "label": 1
    },
    "2310.19792": {
        "title": "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics",
        "abstract": "With an increasing number of parameters and pre-training data, generative\nlarge language models (LLMs) have shown remarkable capabilities to solve tasks\nwith minimal or no task-related examples. Notably, LLMs have been successfully\nemployed as evaluation metrics in text generation tasks. Within this context,\nwe introduce the Eval4NLP 2023 shared task that asks participants to explore\nprompting and score extraction for machine translation (MT) and summarization\nevaluation. Specifically, we propose a novel competition setting in which we\nselect a list of allowed LLMs and disallow fine-tuning to ensure a focus on\nprompting. We present an overview of participants' approaches and evaluate them\non a new reference-free test set spanning three language pairs for MT and a\nsummarization dataset. Notably, despite the task's restrictions, the\nbest-performing systems achieve results on par with or even surpassing recent\nreference-free metrics developed using larger models, including GEMBA and\nComet-Kiwi-XXL. Finally, as a separate track, we perform a small-scale human\nevaluation of the plausibility of explanations given by the LLMs.",
        "date": "2023-10-30T17:55:08+00:00",
        "label": 1
    },
    "1401.4507": {
        "title": "Using Quantum Computers to Learn Physics",
        "abstract": "Since its inception at the beginning of the twentieth century, quantum\nmechanics has challenged our conceptions of how the universe ought to work;\nhowever, the equations of quantum mechanics can be too computationally\ndifficult to solve using existing computers for even modestly large systems.\nHere I will show that quantum computers can sometimes be used to address such\nproblems and that quantum computer science can assign formal complexities to\nlearning facts about nature. Hence, computer science should not only be\nregarded as an applied science; it is also of central importance to the\nfoundations of science.",
        "date": "2014-01-18T01:46:52+00:00",
        "label": 0
    },
    "2306.04181": {
        "title": "Benchmarking Foundation Models with Language-Model-as-an-Examiner",
        "abstract": "Numerous benchmarks have been established to assess the performance of\nfoundation models on open-ended question answering, which serves as a\ncomprehensive test of a model's ability to understand and generate language in\na manner similar to humans. Most of these works focus on proposing new\ndatasets, however, we see two main issues within previous benchmarking\npipelines, namely testing leakage and evaluation automation. In this paper, we\npropose a novel benchmarking framework, Language-Model-as-an-Examiner, where\nthe LM serves as a knowledgeable examiner that formulates questions based on\nits knowledge and evaluates responses in a reference-free manner. Our framework\nallows for effortless extensibility as various LMs can be adopted as the\nexaminer, and the questions can be constantly updated given more diverse\ntrigger topics. For a more comprehensive and equitable evaluation, we devise\nthree strategies: (1) We instruct the LM examiner to generate questions across\na multitude of domains to probe for a broad acquisition, and raise follow-up\nquestions to engage in a more in-depth assessment. (2) Upon evaluation, the\nexaminer combines both scoring and ranking measurements, providing a reliable\nresult as it aligns closely with human annotations. (3) We additionally propose\na decentralized Peer-examination method to address the biases in a single\nexaminer. Our data and benchmarking results are available at:\nhttp://lmexam.xlore.cn.",
        "date": "2023-06-07T06:29:58+00:00",
        "label": 1
    },
    "2010.07017": {
        "title": "Computational Skills by Stealth in Secondary School Data Science",
        "abstract": "The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.",
        "date": "2020-10-08T09:11:51+00:00",
        "label": 0
    },
    "2210.13526": {
        "title": "Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations",
        "abstract": "Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.",
        "date": "2022-10-24T18:27:27+00:00",
        "label": 0
    },
    "2308.01862": {
        "title": "Wider and Deeper LLM Networks are Fairer LLM Evaluators",
        "abstract": "Measuring the quality of responses generated by LLMs is a challenging task,\nparticularly when it comes to evaluating whether the response is aligned with\nhuman preference. A novel approach involves using the LLM itself to make\nevaluation and stabilizing the results through multiple independent\nevaluations, similar to a single-layer narrow LLM network. This network\nconsists of a fixed number of neurons, with each neuron being the same LLM. In\nthis paper, we draw upon the extensive research on deep neural networks to\nexplore whether deeper and wider networks can lead to fairer evaluations.\nSpecifically, inspired by the observation that different neurons in a neural\nnetwork are responsible for detecting different concepts, we first adaptively\ngenerate as many neuron roles as possible for each evaluation sample. Each\nperspective corresponds to the role of a specific LLM neuron in the first\nlayer. In subsequent layers, we follow the idea that higher layers in deep\nnetworks are responsible for more comprehensive features, each layer receives\nrepresentations from all neurons in the previous layer, integrating the locally\nlearned evaluation information to obtain a more comprehensive evaluation\nresult. Interestingly, this network design resembles the process of academic\npaper reviewing. To validate the effectiveness of our method, we construct the\nlargest and most diverse English evaluation benchmark LLMEval$^2$ for LLM\nevaluators, comprising 15 tasks, 8 abilities, and 2,553 samples. Experimental\nresults demonstrate that a wider network (involving many reviewers) with 2\nlayers (one round of discussion) performs the best, improving kappa correlation\ncoefficient from 0.28 to 0.34. We also leverage WideDeep to aid in the\nassessment of Chinese LLMs, which has accelerated the evaluation time by 4.6\ntimes, resulting in a 60% cost saving. WideDeep achieves a remarkable 93%\nagreement level among humans.",
        "date": "2023-08-03T16:38:34+00:00",
        "label": 1
    },
    "1404.5458": {
        "title": "Complex Workflow Management and Integration of Distributed Computing Resources by Science Gateway Portal for Molecular Dynamics Simulations in Materials Science",
        "abstract": "The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\nworkflow management and integration of distributed computing resources (like\nclusters, service grids, desktop grids, clouds) is presented. It is created on\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\nscience workflow operation and gUSE - for smooth integration of available\nresources for parallel and distributed computing in various heterogeneous\ndistributed computing infrastructures (DCI). The typical scientific workflow\nwith possible scenarios of its preparation and usage is considered. Several\ntypical science applications (scientific workflows) are considered for\nmolecular dynamics (MD) simulations of complex behavior of various\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\nadvantages and drawbacks of the solution are shortly analyzed in the context of\nits practical applications for MD simulations in materials science, physics and\nnanotechnologies with available heterogeneous DCIs.",
        "date": "2014-04-22T11:34:04+00:00",
        "label": 0
    },
    "2307.03025": {
        "title": "Style Over Substance: Evaluation Biases for Large Language Models",
        "abstract": "As large language models (LLMs) continue to advance, accurately and\ncomprehensively evaluating their performance becomes increasingly challenging.\nRanking the relative performance of LLMs based on Elo ratings, according to\nhuman judgment, is gaining more popularity. However, the extent to which humans\nand LLMs are capable evaluators remains uncertain. This study investigates the\nbehavior of crowd-sourced and expert annotators, as well as LLMs, when\ncomparing outputs from different models. To achieve this, we curate a dataset\nof intentionally flawed machine-generated answers. Our findings reveal a\nconcerning bias in the evaluation process, as answers with factual errors are\nrated more favorably than answers that are too short or contained grammatical\nerrors. To address this issue, we propose independently evaluating\nmachine-generated text across multiple dimensions, rather than merging all the\nevaluation aspects into a single score. We instantiate this idea with the Elo\nrating system, resulting in the Multi-Elo Rating System (MERS). Empirical\nresults from our study reveal that this proposed approach significantly\nenhances the quality of LLM-based evaluations, particularly in terms of factual\naccuracy. However, there is no significant improvement in crowd-sourced-based\nevaluations, indicating the need for further investigation.",
        "date": "2023-07-06T14:42:01+00:00",
        "label": 1
    },
    "2207.07901": {
        "title": "Computer Science",
        "abstract": "Possible for science itself, conceptually, to have and will understand\ndifferently, let alone science also seen as technology, such as computer\nscience. After all, science and technology are viewpoints diverse by either\nindividual, community, or social. Generally, it depends on socioeconomic\ncapabilities. So it is with computer science has become a phenomenon and\nfashionable, where based on the stream of documents, various issues arise in\neither its theory or implementation, adapting different communities, or\ndesigning curriculum holds in the education system.",
        "date": "2022-07-16T10:54:57+00:00",
        "label": 0
    },
    "2207.01934": {
        "title": "How sustainable is \"common\" data science in terms of power consumption?",
        "abstract": "Continuous developments in data science have brought forth an exponential\nincrease in complexity of machine learning models. Additionally, data\nscientists have become ubiquitous in the private market, academic environments\nand even as a hobby. All of these trends are on a steady rise, and are\nassociated with an increase in power consumption and associated carbon\nfootprint. The increasing carbon footprint of large-scale advanced data science\nhas already received attention, but the latter trend has not. This work aims to\nestimate the contribution of the increasingly popular \"common\" data science to\nthe global carbon footprint. To this end, the power consumption of several\ntypical tasks in the aforementioned common data science tasks will be measured\nand compared to: large-scale \"advanced\" data science, common computer-related\ntasks, and everyday non-computer related tasks. This is done by converting the\nmeasurements to the equivalent unit of \"km driven by car\". Our main findings\nare: \"common\" data science consumes $2.57$ more power than regular computer\nusage, but less than some common everyday power-consuming tasks such as\nlighting or heating; large-scale data science consumes substantially more power\nthan common data science.",
        "date": "2022-07-05T10:15:22+00:00",
        "label": 0
    },
    "1309.0717": {
        "title": "A Polynomial Translation of pi-calculus FCPs to Safe Petri Nets",
        "abstract": "We develop a polynomial translation from finite control pi-calculus processes\nto safe low-level Petri nets. To our knowledge, this is the first such\ntranslation. It is natural in that there is a close correspondence between the\ncontrol flows, enjoys a bisimulation result, and is suitable for practical\nmodel checking.",
        "date": "2013-09-03T15:08:39+00:00",
        "label": 0
    },
    "2310.00785": {
        "title": "BooookScore: A systematic exploration of book-length summarization in the era of LLMs",
        "abstract": "Summarizing book-length documents (>100K tokens) that exceed the context\nwindow size of large language models (LLMs) requires first breaking the input\ndocument into smaller chunks and then prompting an LLM to merge, update, and\ncompress chunk-level summaries. Despite the complexity and importance of this\ntask, it has yet to be meaningfully studied due to the challenges of\nevaluation: existing book-length summarization datasets (e.g., BookSum) are in\nthe pretraining data of most public LLMs, and existing evaluation methods\nstruggle to capture errors made by modern LLM summarizers. In this paper, we\npresent the first study of the coherence of LLM-based book-length summarizers\nimplemented via two prompting workflows: (1) hierarchically merging chunk-level\nsummaries, and (2) incrementally updating a running summary. We obtain 1193\nfine-grained human annotations on GPT-4 generated summaries of 100\nrecently-published books and identify eight common types of coherence errors\nmade by LLMs. Because human evaluation is expensive and time-consuming, we\ndevelop an automatic metric, BooookScore, that measures the proportion of\nsentences in a summary that do not contain any of the identified error types.\nBooookScore has high agreement with human annotations and allows us to\nsystematically evaluate the impact of many other critical parameters (e.g.,\nchunk size, base LLM) while saving $15K USD and 500 hours in human evaluation\ncosts. We find that closed-source LLMs such as GPT-4 and Claude 2 produce\nsummaries with higher BooookScore than those generated by open-source models.\nWhile LLaMA 2 falls behind other models, Mixtral achieves performance on par\nwith GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher\nlevel of detail than hierarchical merging, a trade-off sometimes preferred by\nannotators.",
        "date": "2023-10-01T20:46:44+00:00",
        "label": 1
    },
    "1310.7911": {
        "title": "Compact manifolds with computable boundaries",
        "abstract": "We investigate conditions under which a co-computably enumerable closed set\nin a computable metric space is computable and prove that in each locally\ncomputable computable metric space each co-computably enumerable compact\nmanifold with computable boundary is computable. In fact, we examine the notion\nof a semi-computable compact set and we prove a more general result: in any\ncomputable metric space each semi-computable compact manifold with computable\nboundary is computable. In particular, each semi-computable compact\n(boundaryless) manifold is computable.",
        "date": "2013-10-29T18:29:13+00:00",
        "label": 0
    },
    "1501.05039": {
        "title": "Defining Data Science",
        "abstract": "Data science is gaining more and more and widespread attention, but no\nconsensus viewpoint on what data science is has emerged. As a new science, its\nobjects of study and scientific issues should not be covered by established\nsciences. Data in cyberspace have formed what we call datanature. In the\npresent paper, data science is defined as the science of exploring datanature.",
        "date": "2015-01-21T02:41:55+00:00",
        "label": 0
    },
    "2305.10403": {
        "title": "PaLM 2 Technical Report",
        "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better\nmultilingual and reasoning capabilities and is more compute-efficient than its\npredecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture\nof objectives. Through extensive evaluations on English and multilingual\nlanguage, and reasoning tasks, we demonstrate that PaLM 2 has significantly\nimproved quality on downstream tasks across different model sizes, while\nsimultaneously exhibiting faster and more efficient inference compared to PaLM.\nThis improved efficiency enables broader deployment while also allowing the\nmodel to respond faster, for a more natural pace of interaction. PaLM 2\ndemonstrates robust reasoning capabilities exemplified by large improvements\nover PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable\nperformance on a suite of responsible AI evaluations, and enables\ninference-time control over toxicity without additional overhead or impact on\nother capabilities. Overall, PaLM 2 achieves state-of-the-art performance\nacross a diverse set of tasks and capabilities.\n  When discussing the PaLM 2 family, it is important to distinguish between\npre-trained models (of various sizes), fine-tuned variants of these models, and\nthe user-facing products that use these models. In particular, user-facing\nproducts typically include additional pre- and post-processing steps.\nAdditionally, the underlying models may evolve over time. Therefore, one should\nnot expect the performance of user-facing products to exactly match the results\nreported in this report.",
        "date": "2023-05-17T17:46:53+00:00",
        "label": 1
    },
    "2312.10355": {
        "title": "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation",
        "abstract": "Recently, natural language generation (NLG) evaluation has shifted from a\nsingle-aspect to a multi-aspect paradigm, allowing for a more accurate\nassessment. Large language models (LLMs) achieve superior performance on\nvarious NLG evaluation tasks. However, current work often employs the LLM to\nindependently evaluate different aspects, which largely ignores the rich\ncorrelation between various aspects. To fill this research gap, in this work,\nwe propose an NLG evaluation metric called CoAScore. Powered by LLMs, the\nCoAScore utilizes multi-aspect knowledge through a CoA\n(\\textbf{C}hain-\\textbf{o}f-\\textbf{A}spects) prompting framework when\nassessing the quality of a certain aspect. Specifically, for a given aspect to\nevaluate, we first prompt the LLM to generate a chain of aspects that are\nrelevant to the target aspect and could be useful for the evaluation. We then\ncollect evaluation scores for each generated aspect, and finally, leverage the\nknowledge of these aspects to improve the evaluation of the target aspect. We\nevaluate CoAScore across five NLG evaluation tasks (e.g., summarization, dialog\nresponse generation, etc) and nine aspects (e.g., overall quality, relevance,\ncoherence, etc). Our experimental findings highlight that, in comparison to\nindividual aspect evaluation, CoAScore exhibits a higher correlation with human\njudgments. This improvement significantly outperforms existing unsupervised\nevaluation metrics, whether for assessing overall quality or other aspects. We\nalso conducted extensive ablation studies to validate the effectiveness of the\nthree stages within the CoAScore framework and conducted case studies to show\nhow the LLM performs in these stages. Our code and scripts are available.",
        "date": "2023-12-16T06:57:20+00:00",
        "label": 1
    },
    "2311.18702": {
        "title": "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation",
        "abstract": "Since the natural language processing (NLP) community started to make large\nlanguage models (LLMs) act as a critic to evaluate the quality of generated\ntexts, most of the existing works train a critique generation model on the\nevaluation data labeled by GPT-4's direct prompting. We observe that these\nmodels lack the ability to generate informative critiques in both pointwise\ngrading and pairwise comparison especially without references. As a result,\ntheir generated critiques cannot provide fine-grained distinguishability on\ngenerated texts, causing unsatisfactory evaluation performance. In this paper,\nwe propose a simple yet effective method called Eval-Instruct, which can first\nacquire pointwise grading critiques with pseudo references and then revise\nthese critiques via multi-path prompting to obtain informative evaluation data\nin different tasks and settings, including pointwise grading and pairwise\ncomparison with / without references. After fine-tuning on these data, the\nresulting model CritiqueLLM is empirically shown to outperform ChatGPT and all\nthe open-source baselines and even achieve comparable evaluation performance to\nGPT-4 in system-level correlations of pointwise grading. We also demonstrate\nthat our generated critiques can act as scalable feedback to further improve\nthe generation quality of strong LLMs like ChatGPT.",
        "date": "2023-11-30T16:52:42+00:00",
        "label": 1
    },
    "2002.05658": {
        "title": "Ten Research Challenge Areas in Data Science",
        "abstract": "Although data science builds on knowledge from computer science, mathematics,\nstatistics, and other disciplines, data science is a unique field with many\nmysteries to unlock: challenging scientific questions and pressing questions of\nsocietal importance. This article starts with meta-questions about data science\nas a discipline and then elaborates on ten ideas for the basis of a research\nagenda for data science.",
        "date": "2020-01-27T21:39:57+00:00",
        "label": 0
    },
    "2305.17926": {
        "title": "Large Language Models are not Fair Evaluators",
        "abstract": "In this paper, we uncover a systematic bias in the evaluation paradigm of\nadopting large language models~(LLMs), e.g., GPT-4, as a referee to score and\ncompare the quality of responses generated by candidate models. We find that\nthe quality ranking of candidate responses can be easily hacked by simply\naltering their order of appearance in the context. This manipulation allows us\nto skew the evaluation result, making one model appear considerably superior to\nthe other, e.g., Vicuna-13B could beat ChatGPT on 66 over 80 tested queries\nwith ChatGPT as an evaluator. To address this issue, we propose a calibration\nframework with three simple yet effective strategies: 1) Multiple Evidence\nCalibration, which requires the evaluator model to generate multiple evaluation\nevidence before assigning ratings; 2) Balanced Position Calibration, which\naggregates results across various orders to determine the final score; 3)\nHuman-in-the-Loop Calibration, which introduces a balanced position diversity\nentropy to measure the difficulty of each example and seeks human assistance\nwhen needed. We also manually annotate the \"win/tie/lose\" outcomes of responses\nfrom ChatGPT and Vicuna-13B in the Vicuna Benchmark's question prompt, and\nextensive experiments demonstrate that our approach successfully mitigates\nevaluation bias, resulting in closer alignment with human judgments. We release\nour code and human annotation at \\url{https://github.com/i-Eval/FairEval} to\nfacilitate future research.",
        "date": "2023-05-29T07:41:03+00:00",
        "label": 1
    },
    "2311.00681": {
        "title": "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs",
        "abstract": "In recent years, Large Language Models (LLMs) have gained immense attention\ndue to their notable emergent capabilities, surpassing those seen in earlier\nlanguage models. A particularly intriguing application of LLMs is their role as\nevaluators for texts produced by various generative models.\n  In this study, we delve into the potential of LLMs as reliable assessors of\nfactual consistency in summaries generated by text-generation models.\nInitially, we introduce an innovative approach for factuality assessment using\nLLMs. This entails employing a singular LLM for the entirety of the\nquestion-answering-based factuality scoring process. Following this, we examine\nthe efficacy of various LLMs in direct factuality scoring, benchmarking them\nagainst traditional measures and human annotations.\n  Contrary to initial expectations, our results indicate a lack of significant\ncorrelations between factuality metrics and human evaluations, specifically for\nGPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across\ntwo factuality subcategories. These consistent findings across various factual\nerror categories suggest a fundamental limitation in the current LLMs'\ncapability to accurately gauge factuality.\n  This version presents the information more concisely while maintaining the\nmain points and findings of the original text.",
        "date": "2023-11-01T17:42:45+00:00",
        "label": 1
    },
    "1406.2222": {
        "title": "The Chemistry Between High School Students and Computer Science",
        "abstract": "Computer science enrollments have started to rise again, but the percentage\nof women undergraduates in computer science is still low. Some studies indicate\nthis might be due to a lack of awareness of computer science at the high school\nlevel. We present our experiences running a 5-year, high school outreach\nprogram that introduces information about computer science within the context\nof required chemistry courses. We developed interactive worksheets using\nMolecular Workbench that help the students learn chemistry and computer science\nconcepts related to relevant events such as the gulf oil spill. Our evaluation\nof the effectiveness of this approach indicates that the students do become\nmore aware of computer science as a discipline, but system support issues in\nthe classroom can make the approach difficult for teachers and discouraging for\nthe students.",
        "date": "2014-06-09T15:44:41+00:00",
        "label": 0
    },
    "1011.1335": {
        "title": "A short proof that adding some permutation rules to $\u03b2$ preserves $SN$",
        "abstract": "I show that, if a term is $SN$ for $\\beta$, it remains $SN$ when some\npermutation rules are added.",
        "date": "2010-11-05T07:54:47+00:00",
        "label": 0
    },
    "2311.09204": {
        "title": "Fusion-Eval: Integrating Assistant Evaluators with LLMs",
        "abstract": "Evaluating natural language systems poses significant challenges,\nparticularly in the realms of natural language understanding and high-level\nreasoning. In this paper, we introduce 'Fusion-Eval', an innovative approach\nthat leverages Large Language Models (LLMs) to integrate insights from various\nassistant evaluators. The LLM is given the example to evaluate along with\nscores from the assistant evaluators. Each of these evaluators specializes in\nassessing distinct aspects of responses. Fusion-Eval achieves a 0.962\nsystem-level Kendall-Tau correlation with humans on SummEval and a 0.744\nturn-level Spearman correlation on TopicalChat, which is significantly higher\nthan baseline methods. These results highlight Fusion-Eval's significant\npotential in the realm of natural language system evaluation.",
        "date": "2023-11-15T18:46:56+00:00",
        "label": 1
    },
    "1908.05986": {
        "title": "FAIR and Open Computer Science Research Software",
        "abstract": "In computational science and in computer science, research software is a\ncentral asset for research. Computational science is the application of\ncomputer science and software engineering principles to solving scientific\nproblems, whereas computer science is the study of computer hardware and\nsoftware design.\n  The Open Science agenda holds that science advances faster when we can build\non existing results. Therefore, research software has to be reusable for\nadvancing science. Thus, we need proper research software engineering for\nobtaining reusable and sustainable research software. This way, software\nengineering methods may improve research in other disciplines. However,\nresearch in software engineering and computer science itself will also benefit\nfrom reuse when research software is involved.\n  For good scientific practice, the resulting research software should be open\nand adhere to the FAIR principles (findable, accessible, interoperable and\nrepeatable) to allow repeatability, reproducibility, and reuse. Compared to\nresearch data, research software should be both archived for reproducibility\nand actively maintained for reusability. The FAIR data principles do not\nrequire openness, but research software should be open source software.\nEstablished open source software licenses provide sufficient licensing options,\nsuch that it should be the rare exception to keep research software closed.\n  We review and analyze the current state in this area in order to give\nrecommendations for making computer science research software FAIR and open. We\nobserve that research software publishing practices in computer science and in\ncomputational science show significant differences.",
        "date": "2019-08-16T14:26:08+00:00",
        "label": 0
    },
    "2309.13308": {
        "title": "Calibrating LLM-Based Evaluator",
        "abstract": "Recent advancements in large language models (LLMs) on language modeling and\nemergent capabilities make them a promising reference-free evaluator of natural\nlanguage generation quality, and a competent alternative to human evaluation.\nHowever, hindered by the closed-source or high computational demand to host and\ntune, there is a lack of practice to further calibrate an off-the-shelf\nLLM-based evaluator towards better human alignment. In this work, we propose\nAutoCalibrate, a multi-stage, gradient-free approach to automatically calibrate\nand align an LLM-based evaluator toward human preference. Instead of explicitly\nmodeling human preferences, we first implicitly encompass them within a set of\nhuman labels. Then, an initial set of scoring criteria is drafted by the\nlanguage model itself, leveraging in-context learning on different few-shot\nexamples. To further calibrate this set of criteria, we select the best\nperformers and re-draft them with self-refinement. Our experiments on multiple\ntext quality evaluation datasets illustrate a significant improvement in\ncorrelation with expert evaluation through calibration. Our comprehensive\nqualitative analysis conveys insightful intuitions and observations on the\nessence of effective scoring criteria.",
        "date": "2023-09-23T08:46:11+00:00",
        "label": 1
    },
    "0907.3804": {
        "title": "Decidability of higher-order matching",
        "abstract": "We show that the higher-order matching problem is decidable using a\ngame-theoretic argument.",
        "date": "2009-07-22T09:17:30+00:00",
        "label": 0
    },
    "2210.11416": {
        "title": "Scaling Instruction-Finetuned Language Models",
        "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.",
        "date": "2022-10-20T16:58:32+00:00",
        "label": 1
    },
    "2307.07889": {
        "title": "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models",
        "abstract": "Current developments in large language models (LLMs) have enabled impressive\nzero-shot capabilities across various natural language tasks. An interesting\napplication of these systems is in the automated assessment of natural language\ngeneration (NLG), a highly challenging area with great practical benefit. In\nthis paper, we explore two options for exploiting the emergent abilities of\nLLMs for zero-shot NLG assessment: absolute score prediction, and comparative\nassessment which uses relative comparisons between pairs of candidates. Though\ncomparative assessment has not been extensively studied in NLG assessment, we\nnote that humans often find it more intuitive to compare two options rather\nthan scoring each one independently. This work examines comparative assessment\nfrom multiple perspectives: performance compared to absolute grading;\npositional biases in the prompt; and efficient ranking in terms of the number\nof comparisons. We illustrate that LLM comparative assessment is a simple,\ngeneral and effective approach for NLG assessment. For moderate-sized\nopen-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is\nsuperior to prompt scoring, and in many cases can achieve performance\ncompetitive with state-of-the-art methods. Additionally, we demonstrate that\nLLMs often exhibit strong positional biases when making pairwise comparisons,\nand we propose debiasing methods that can further improve performance.",
        "date": "2023-07-15T22:02:12+00:00",
        "label": 1
    },
    "1804.08293": {
        "title": "Materials science and engineering: New vision in the era of artificial intelligence",
        "abstract": "Scientific discovery evolves from the experimental, through the theoretical\nand computational, to the current data-intensive paradigm. Materials science is\nno exception, especially for computational materials science. In recent years,\ngreat achievements have been made in the field of materials science and\nengineering (MSE). Here, we review the previous paradigms of materials science\nand some classical MSE models. Then, our data-intensive MSE (DIMSE) model is\nproposed to reshape future materials innovations. This work will help to\naddress the global challenge for materials discovery in the era of artificial\nintelligence (AI), and essentially contribute to accelerating future materials\ncontinuum.",
        "date": "2018-04-23T09:01:57+00:00",
        "label": 0
    },
    "1103.1977": {
        "title": "Development of Computer Science Disciplines - A Social Network Analysis Approach",
        "abstract": "In contrast to many other scientific disciplines, computer science considers\nconference publications. Conferences have the advantage of providing fast\npublication of papers and of bringing researchers together to present and\ndiscuss the paper with peers. Previous work on knowledge mapping focused on the\nmap of all sciences or a particular domain based on ISI published JCR (Journal\nCitation Report). Although this data covers most of important journals, it\nlacks computer science conference and workshop proceedings. That results in an\nimprecise and incomplete analysis of the computer science knowledge. This paper\npresents an analysis on the computer science knowledge network constructed from\nall types of publications, aiming at providing a complete view of computer\nscience research. Based on the combination of two important digital libraries\n(DBLP and CiteSeerX), we study the knowledge network created at\njournal/conference level using citation linkage, to identify the development of\nsub-disciplines. We investigate the collaborative and citation behavior of\njournals/conferences by analyzing the properties of their co-authorship and\ncitation subgraphs. The paper draws several important conclusions. First,\nconferences constitute social structures that shape the computer science\nknowledge. Second, computer science is becoming more interdisciplinary. Third,\nexperts are the key success factor for sustainability of journals/conferences.",
        "date": "2011-03-10T09:51:19+00:00",
        "label": 0
    },
    "2303.07610": {
        "title": "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences",
        "abstract": "As a natural language assistant, ChatGPT is capable of performing various\ntasks, including but not limited to article generation, code completion, and\ndata analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\nlevel of accuracy and reliability in terms of content evaluation, exhibiting\nthe capability of mimicking human preferences. To further explore ChatGPT's\npotential in this regard, a study is conducted to assess its ability to rank\ncontent. In order to do so, a test set consisting of prompts is created,\ncovering a wide range of use cases, and five models are utilized to generate\ncorresponding responses. ChatGPT is then instructed to rank the responses\ngenerated by these models. The results on the test set show that ChatGPT's\nranking preferences are consistent with human to a certain extent. This\npreliminary experimental finding implies that ChatGPT's zero-shot ranking\ncapability could be used to reduce annotation pressure in a number of ranking\ntasks.",
        "date": "2023-03-14T03:13:02+00:00",
        "label": 1
    },
    "1710.03090": {
        "title": "Theoretical Computer Science for the Working Category Theorist",
        "abstract": "Theoretical computer science discusses foundational issues about\ncomputations. It asks and answers questions such as \"What is a computation?\",\n\"What is computable?\", \"What is efficiently computable?\",\"What is\ninformation?\", \"What is random?\", \"What is an algorithm?\", etc. We will present\nmany of the major themes and theorems with the basic language of category\ntheory. Surprisingly, many interesting theorems and concepts of theoretical\ncomputer science are easy consequences of functoriality and composition when\nyou look at the right categories and functors connecting them.",
        "date": "2017-10-04T19:19:00+00:00",
        "label": 0
    },
    "1606.06565": {
        "title": "Concrete Problems in AI Safety",
        "abstract": "Rapid progress in machine learning and artificial intelligence (AI) has\nbrought increasing attention to the potential impacts of AI technologies on\nsociety. In this paper we discuss one such potential impact: the problem of\naccidents in machine learning systems, defined as unintended and harmful\nbehavior that may emerge from poor design of real-world AI systems. We present\na list of five practical research problems related to accident risk,\ncategorized according to whether the problem originates from having the wrong\nobjective function (\"avoiding side effects\" and \"avoiding reward hacking\"), an\nobjective function that is too expensive to evaluate frequently (\"scalable\nsupervision\"), or undesirable behavior during the learning process (\"safe\nexploration\" and \"distributional shift\"). We review previous work in these\nareas as well as suggesting research directions with a focus on relevance to\ncutting-edge AI systems. Finally, we consider the high-level question of how to\nthink most productively about the safety of forward-looking applications of AI.",
        "date": "2016-06-21T13:37:05+00:00",
        "label": 1
    },
    "2306.05685": {
        "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
        "abstract": "Evaluating large language model (LLM) based chat assistants is challenging\ndue to their broad capabilities and the inadequacy of existing benchmarks in\nmeasuring human preferences. To address this, we explore using strong LLMs as\njudges to evaluate these models on more open-ended questions. We examine the\nusage and limitations of LLM-as-a-judge, including position, verbosity, and\nself-enhancement biases, as well as limited reasoning ability, and propose\nsolutions to mitigate some of them. We then verify the agreement between LLM\njudges and human preferences by introducing two benchmarks: MT-bench, a\nmulti-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our\nresults reveal that strong LLM judges like GPT-4 can match both controlled and\ncrowdsourced human preferences well, achieving over 80% agreement, the same\nlevel of agreement between humans. Hence, LLM-as-a-judge is a scalable and\nexplainable way to approximate human preferences, which are otherwise very\nexpensive to obtain. Additionally, we show our benchmark and traditional\nbenchmarks complement each other by evaluating several variants of LLaMA and\nVicuna. The MT-bench questions, 3K expert votes, and 30K conversations with\nhuman preferences are publicly available at\nhttps://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
        "date": "2023-06-09T05:55:52+00:00",
        "label": 1
    },
    "2308.04592": {
        "title": "Shepherd: A Critic for Language Model Generation",
        "abstract": "As large language models improve, there is increasing interest in techniques\nthat leverage these models' capabilities to refine their own outputs. In this\nwork, we introduce Shepherd, a language model specifically tuned to critique\nresponses and suggest refinements, extending beyond the capabilities of an\nuntuned model to identify diverse errors and provide suggestions to remedy\nthem. At the core of our approach is a high quality feedback dataset, which we\ncurate from community feedback and human annotations. Even though Shepherd is\nsmall (7B parameters), its critiques are either equivalent or preferred to\nthose from established models including ChatGPT. Using GPT-4 for evaluation,\nShepherd reaches an average win-rate of 53-87% compared to competitive\nalternatives. In human evaluation, Shepherd strictly outperforms other models\nand on average closely ties with ChatGPT.",
        "date": "2023-08-08T21:23:23+00:00",
        "label": 1
    },
    "0706.0484": {
        "title": "Motivation, Design, and Ubiquity: A Discussion of Research Ethics and Computer Science",
        "abstract": "Modern society is permeated with computers, and the software that controls\nthem can have latent, long-term, and immediate effects that reach far beyond\nthe actual users of these systems. This places researchers in Computer Science\nand Software Engineering in a critical position of influence and\nresponsibility, more than any other field because computer systems are vital\nresearch tools for other disciplines. This essay presents several key ethical\nconcerns and responsibilities relating to research in computing. The goal is to\npromote awareness and discussion of ethical issues among computer science\nresearchers. A hypothetical case study is provided, along with questions for\nreflection and discussion.",
        "date": "2007-06-04T17:17:44+00:00",
        "label": 0
    },
    "2309.13633": {
        "title": "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria",
        "abstract": "By simply composing prompts, developers can prototype novel generative\napplications with Large Language Models (LLMs). To refine prototypes into\nproducts, however, developers must iteratively revise prompts by evaluating\noutputs to diagnose weaknesses. Formative interviews (N=8) revealed that\ndevelopers invest significant effort in manually evaluating outputs as they\nassess context-specific and subjective criteria. We present EvalLM, an\ninteractive system for iteratively refining prompts by evaluating multiple\noutputs on user-defined criteria. By describing criteria in natural language,\nusers can employ the system's LLM-based evaluator to get an overview of where\nprompts excel or fail, and improve these based on the evaluator's feedback. A\ncomparative study (N=12) showed that EvalLM, when compared to manual\nevaluation, helped participants compose more diverse criteria, examine twice as\nmany outputs, and reach satisfactory prompts with 59% fewer revisions. Beyond\nprompts, our work can be extended to augment model evaluation and alignment in\nspecific application contexts.",
        "date": "2023-09-24T13:19:38+00:00",
        "label": 1
    },
    "1612.04037": {
        "title": "Proceedings 11th Doctoral Workshop on Mathematical and Engineering Methods in Computer Science",
        "abstract": "MEMICS provides a forum for doctoral students interested in applications of\nmathematical and engineering methods in computer science. Besides a rich\ntechnical programme (including invited talks, regular papers, and\npresentations), MEMICS also offers friendly social activities and exciting\nopportunities for meeting like-minded people. MEMICS submissions traditionally\ncover all areas of computer science (such as parallel and distributed\ncomputing, computer networks, modern hardware and its design, non-traditional\ncomputing architectures, information systems and databases, multimedia and\ngraphics, verification and testing, computer security, as well as all related\nareas of theoretical computer science).",
        "date": "2016-12-13T05:47:19+00:00",
        "label": 0
    },
    "0608062": {
        "title": "Tarski's influence on computer science",
        "abstract": "The influence of Alfred Tarski on computer science was indirect but\nsignificant in a number of directions and was in certain respects fundamental.\nHere surveyed is the work of Tarski on the decision procedure for algebra and\ngeometry, the method of elimination of quantifiers, the semantics of formal\nlanguages, modeltheoretic preservation theorems, and algebraic logic; various\nconnections of each with computer science are taken up.",
        "date": "2006-08-15T16:40:24+00:00",
        "label": 0
    },
    "2312.15407": {
        "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
        "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The\ntraditional reference-based NLG metrics are generally found to be unsuitable\nfor dialogue assessment. Consequently, recent studies have suggested various\nunique, reference-free neural metrics that better align with human evaluations.\nNotably among them, large language models (LLMs), particularly the\ninstruction-tuned variants like ChatGPT, are shown to be promising substitutes\nfor human judges. Yet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the number of meta-evaluation\ndatasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains\ninconclusive how effective these LLMs are. To this end, we conduct a\ncomprehensive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional evaluation\ncapability of 30 recently emerged LLMs at both turn and dialogue levels, using\na comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the\nrobustness of the LLMs in handling various adversarial perturbations at both\nturn and dialogue levels. Finally, we explore how model-level and\ndimension-level ensembles impact the evaluation performance. All resources are\navailable at https://github.com/e0397123/comp-analysis.",
        "date": "2023-12-24T04:50:57+00:00",
        "label": 1
    },
    "1003.1930": {
        "title": "Simulating Grover's Quantum Search in a Classical Computer",
        "abstract": "The rapid progress of computer science has been accompanied by a\ncorresponding evolution of computation, from classical computation to quantum\ncomputation. As quantum computing is on its way to becoming an established\ndiscipline of computing science, much effort is being put into the development\nof new quantum algorithms. One of quantum algorithms is Grover algorithm, which\nis used for searching an element in an unstructured list of N elements with\nquadratic speed-up over classical algorithms. In this work, Quantum Computer\nLanguage (QCL) is used to make a Grover's quantum search simulation in a\nclassical computer",
        "date": "2010-03-09T17:02:21+00:00",
        "label": 0
    },
    "2303.15621": {
        "title": "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization",
        "abstract": "The performance of text summarization has been greatly boosted by pre-trained\nlanguage models. A main concern of existing methods is that most generated\nsummaries are not factually inconsistent with their source documents. To\nalleviate the problem, many efforts have focused on developing effective\nfactuality evaluation metrics based on natural language inference, question\nanswering, and syntactic dependency et al. However, these approaches are\nlimited by either their high computational complexity or the uncertainty\nintroduced by multi-component pipelines, resulting in only partial agreement\nwith human judgement. Most recently, large language models(LLMs) have shown\nexcellent performance in not only text generation but also language\ncomprehension. In this paper, we particularly explore ChatGPT's ability to\nevaluate factual inconsistency under a zero-shot setting by examining it on\nboth coarse-grained and fine-grained evaluation tasks including binary\nentailment inference, summary ranking, and consistency rating. Experimental\nresults indicate that ChatGPT generally outperforms previous evaluation metrics\nacross the three tasks, indicating its great potential for factual\ninconsistency evaluation. However, a closer inspection of ChatGPT's output\nreveals certain limitations including its preference for more lexically similar\ncandidates, false reasoning, and inadequate understanding of instructions.",
        "date": "2023-03-27T22:30:39+00:00",
        "label": 1
    },
    "1108.3558": {
        "title": "Proceedings of the 5th Workshop on Membrane Computing and Biologically Inspired Process Calculi (MeCBIC 2011)",
        "abstract": "This volume represents the proceedings of the 5th Workshop on Membrane\nComputing and Biologically Inspired Process Calculi (MeCBIC 2011), held\ntogether with the 12th International Conference on Membrane Computing on 23rd\nAugust 2011 in Fontainebleau, France.",
        "date": "2011-08-17T19:41:29+00:00",
        "label": 0
    },
    "1807.09490": {
        "title": "Investigating the Intersection of Science Fiction, Human-Computer Interaction and Computer Science Research",
        "abstract": "This paper outlines ongoing dissertation research located in the intersection\nof science fiction, human-computer interaction and computer science. Through an\ninterdisciplinary perspective, drawing from fields such as human-computer\ninteraction, film theory and studies of science and technology, qualitative and\nquantitative content analysis techniques are used to contextually analyze\nexpressions of science fiction in peer-reviewed computer science research\nrepositories, such as the ACM or IEEE Xplore Digital Libraries. This paper\nconcisely summarizes and introduces the relationship of science fiction and\ncomputer science research and presents the research questions, aims and\nimplications in addition to prior work and study methodology. In the latter\npart of this work-in-progress report, preliminary results, current limitations,\nfuture work and a post-dissertation trajectory are outlined.",
        "date": "2018-07-25T09:02:02+00:00",
        "label": 0
    },
    "2310.08491": {
        "title": "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models",
        "abstract": "Recently, using a powerful proprietary Large Language Model (LLM) (e.g.,\nGPT-4) as an evaluator for long-form responses has become the de facto\nstandard. However, for practitioners with large-scale evaluation tasks and\ncustom criteria in consideration (e.g., child-readability), using proprietary\nLLMs as an evaluator is unreliable due to the closed-source nature,\nuncontrolled versioning, and prohibitive costs. In this work, we propose\nPrometheus, a fully open-source LLM that is on par with GPT-4's evaluation\ncapabilities when the appropriate reference materials (reference answer, score\nrubric) are accompanied. We first construct the Feedback Collection, a new\ndataset that consists of 1K fine-grained score rubrics, 20K instructions, and\n100K responses and language feedback generated by GPT-4. Using the Feedback\nCollection, we train Prometheus, a 13B evaluator LLM that can assess any given\nlong-form text based on customized score rubric provided by the user.\nExperimental results show that Prometheus scores a Pearson correlation of 0.897\nwith human evaluators when evaluating with 45 customized score rubrics, which\nis on par with GPT-4 (0.882), and greatly outperforms ChatGPT (0.392).\nFurthermore, measuring correlation with GPT-4 with 1222 customized score\nrubrics across four benchmarks (MT Bench, Vicuna Bench, Feedback Bench, Flask\nEval) shows similar trends, bolstering Prometheus's capability as an evaluator\nLLM. Lastly, Prometheus achieves the highest accuracy on two human preference\nbenchmarks (HHH Alignment & MT Bench Human Judgment) compared to open-sourced\nreward models explicitly trained on human preference datasets, highlighting its\npotential as an universal reward model. We open-source our code, dataset, and\nmodel at https://kaistai.github.io/prometheus/.",
        "date": "2023-10-12T16:50:08+00:00",
        "label": 1
    },
    "2401.00437": {
        "title": "BatchEval: Towards Human-like Text Evaluation",
        "abstract": "Significant progress has been made in automatic text evaluation with the\nintroduction of large language models (LLMs) as evaluators. However, current\nsample-wise evaluation paradigm suffers from the following issues: (1)\nSensitive to prompt design; (2) Poor resistance to noise; (3) Inferior ensemble\nperformance with static reference. Inspired by the fact that humans treat both\ncriterion definition and inter sample comparison as references for evaluation,\nwe propose BatchEval, a paradigm that conducts batch-wise evaluation\niteratively to alleviate the above problems. We explore variants under this\nparadigm and confirm the optimal settings are two stage procedure with\nheterogeneous batch composition strategy and decimal scoring format.\nComprehensive experiments across 3 LLMs on 4 text evaluation tasks demonstrate\nthat BatchEval outperforms state-of-the-art methods by 10.5% on Pearson\ncorrelations with only 64% API cost on average. Further analyses have been\nconducted to verify the robustness, generalization, and working mechanism of\nBatchEval.",
        "date": "2023-12-31T09:34:51+00:00",
        "label": 1
    },
    "2310.15123": {
        "title": "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation",
        "abstract": "Large Language Models (LLMs) are frequently used for multi-faceted language\ngeneration and evaluation tasks that involve satisfying intricate user\nconstraints or taking into account multiple aspects and criteria. However,\ntheir performance can fall short, due to the model's lack of coherence and\ninability to plan and decompose the problem. We propose Branch-Solve-Merge\n(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such\nchallenging natural language tasks. It consists of branch, solve, and merge\nmodules that are parameterized with specific prompts to the base LLM. These\nthree modules plan a decomposition of the task into multiple parallel\nsub-tasks, independently solve them, and fuse the solutions to the sub-tasks.\nWe apply our method to the tasks of LLM response evaluation and constrained\ntext generation and evaluate its effectiveness with multiple LLMs, including\nVicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and\nconsistency for each LLM by enhancing human-LLM agreement by up to 26%,\nreducing length and pairwise position biases by up to 50%, and allowing\nLLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story\ngeneration task, BSM improves the coherence of stories while also improving\nconstraint satisfaction by 12%.",
        "date": "2023-10-23T17:29:48+00:00",
        "label": 1
    },
    "0511274": {
        "title": "Quantum Computation: A Computer Science Perspective",
        "abstract": "The theory of quantum computation is presented in a self contained way from a\ncomputer science perspective. The basics of classical computation and quantum\nmechanics is reviewed. The circuit model of quantum computation is presented in\ndetail. Throughout there is an emphasis on the physical as well as the abstract\naspects of computation and the interplay between them.\n  This report is presented as a Master's thesis at the department of Computer\nScience and Engineering at G{\\\"o}teborg University, G{\\\"o}teborg, Sweden.\n  The text is part of a larger work that is planned to include chapters on\nquantum algorithms, the quantum Turing machine model and abstract approaches to\nquantum computation.",
        "date": "2005-11-30T20:36:54+00:00",
        "label": 0
    },
    "2310.05470": {
        "title": "Generative Judge for Evaluating Alignment",
        "abstract": "The rapid development of Large Language Models (LLMs) has substantially\nexpanded the range of tasks they can address. In the field of Natural Language\nProcessing (NLP), researchers have shifted their focus from conventional NLP\ntasks (e.g., sequence tagging and parsing) towards tasks that revolve around\naligning with human needs (e.g., brainstorming and email writing). This shift\nin task distribution imposes new requirements on evaluating these aligned\nmodels regarding generality (i.e., assessing performance across diverse\nscenarios), flexibility (i.e., examining under different protocols), and\ninterpretability (i.e., scrutinizing models with explanations). In this paper,\nwe propose a generative judge with 13B parameters, Auto-J, designed to address\nthese challenges. Our model is trained on user queries and LLM-generated\nresponses under massive real-world scenarios and accommodates diverse\nevaluation protocols (e.g., pairwise response comparison and single-response\nevaluation) with well-structured natural language critiques. To demonstrate the\nefficacy of our approach, we construct a new testbed covering 58 different\nscenarios. Experimentally, Auto-J outperforms a series of strong competitors,\nincluding both open-source and closed-source models, by a large margin. We also\nprovide detailed analysis and case studies to further reveal the potential of\nour method and make a variety of resources public at\nhttps://github.com/GAIR-NLP/auto-j.",
        "date": "2023-10-09T07:27:15+00:00",
        "label": 1
    },
    "0705.1367": {
        "title": "Logic Column 18: Alternative Logics: A Book Review",
        "abstract": "This article discusses two books on the topic of alternative logics in\nscience: \"Deviant Logic\", by Susan Haack, and \"Alternative Logics: Do Sciences\nNeed Them?\", edited by Paul Weingartner.",
        "date": "2007-05-09T21:56:15+00:00",
        "label": 0
    },
    "2002.04020": {
        "title": "Cloudifying the Curriculum with AWS",
        "abstract": "The Cloud has become a principal paradigm of computing in the last ten years,\nand Computer Science curricula must be updated to reflect that reality. This\npaper examines simple ways to accomplish curriculum cloudification using Amazon\nWeb Services (AWS), for Computer Science and other disciplines such as\nBusiness, Communication and Mathematics.",
        "date": "2020-02-10T18:47:35+00:00",
        "label": 0
    }
}