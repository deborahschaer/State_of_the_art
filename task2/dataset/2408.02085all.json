{
    "2406.09334": {
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models",
        "abstract": "Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper introduces ProxyLM, a scalable framework for predicting LM\nperformance using proxy models in multilingual tasks. These proxy models act as\nsurrogates, approximating the performance of the LM of interest. By leveraging\nproxy models, ProxyLM significantly reduces computational overhead on task\nevaluations, achieving up to a 37.08x speedup compared to traditional methods,\neven with our smallest proxy models. Additionally, our methodology showcases\nadaptability to previously unseen languages in pre-trained LMs, outperforming\nthe state-of-the-art performance by 1.89x as measured by root-mean-square error\n(RMSE). This framework streamlines model selection, enabling efficient\ndeployment and iterative LM enhancements without extensive computational\nresources.",
        "date": "2024-06-13T17:15:33+00:00",
        "label": 1
    },
    "2107.02846": {
        "title": "Visions in Theoretical Computer Science: A Report on the TCS Visioning Workshop 2020",
        "abstract": "Theoretical computer science (TCS) is a subdiscipline of computer science\nthat studies the mathematical foundations of computational and algorithmic\nprocesses and interactions. Work in this field is often recognized by its\nemphasis on mathematical technique and rigor. At the heart of the field are\nquestions surrounding the nature of computation: What does it mean to compute?\nWhat is computable? And how efficiently?\n  Every ten years or so the TCS community attends visioning workshops to\ndiscuss the challenges and recent accomplishments in the TCS field. The\nworkshops and the outputs they produce are meant both as a reflection for the\nTCS community and as guiding principles for interested investment partners.\nConcretely, the workshop output consists of a number of nuggets, each\nsummarizing a particular point, that are synthesized in the form of a white\npaper and illustrated with graphics/slides produced by a professional graphic\ndesigner. The second TCS Visioning Workshop was organized by the SIGACT\nCommittee for the Advancement of Theoretical Computer Science and took place\nduring the week of July 20, 2020. Despite the conference being virtual, there\nwere over 76 participants, mostly from the United States, but also a few from\nEurope and Asia who were able to attend due to the online format. Workshop\nparticipants were divided into categories as reflected in the sections of this\nreport: (1) models of computation; (2) foundations of data science; (3)\ncryptography; and (4) using theoretical computer science for other domains.\nEach group participated in a series of discussions that produced the nuggets\nbelow.",
        "date": "2021-07-06T19:12:03+00:00",
        "label": 0
    },
    "2308.09621": {
        "title": "Canonicity and Computability in Homotopy Type Theory",
        "abstract": "This dissertation gives an overview of Martin Lof's dependant type theory,\nfocusing on its computational content and addressing a question of possibility\nof fully canonical and computable semantic presentation.",
        "date": "2023-08-18T15:23:33+00:00",
        "label": 0
    },
    "1412.7030": {
        "title": "Proceedings of the 7th European Conference on Python in Science (EuroSciPy 2014)",
        "abstract": "These are the proceedings of the 7th European Conference on Python in\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).",
        "date": "2014-12-22T15:47:51+00:00",
        "label": 0
    },
    "1512.02985": {
        "title": "On Variants of k-means Clustering",
        "abstract": "\\textit{Clustering problems} often arise in the fields like data mining,\nmachine learning etc. to group a collection of objects into similar groups with\nrespect to a similarity (or dissimilarity) measure. Among the clustering\nproblems, specifically \\textit{$k$-means} clustering has got much attention\nfrom the researchers. Despite the fact that $k$-means is a very well studied\nproblem its status in the plane is still an open problem. In particular, it is\nunknown whether it admits a PTAS in the plane. The best known approximation\nbound in polynomial time is $9+\\eps$.\n  In this paper, we consider the following variant of $k$-means. Given a set\n$C$ of points in $\\mathcal{R}^d$ and a real $f > 0$, find a finite set $F$ of\npoints in $\\mathcal{R}^d$ that minimizes the quantity $f*|F|+\\sum_{p\\in C}\n\\min_{q \\in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local\nsearch PTAS for this problem. We also give a \"bi-criterion\" local search\nalgorithm for $k$-means which uses $(1+\\eps)k$ centers and yields a solution\nwhose cost is at most $(1+\\eps)$ times the cost of an optimal $k$-means\nsolution. The algorithm runs in polynomial time for any fixed dimension.\n  The contribution of this paper is two fold. On the one hand, we are being\nable to handle the square of distances in an elegant manner, which yields near\noptimal approximation bound. This leads us towards a better understanding of\nthe $k$-means problem. On the other hand, our analysis of local search might\nalso be useful for other geometric problems. This is important considering that\nvery little is known about the local search method for geometric approximation.",
        "date": "2015-12-09T18:37:49+00:00",
        "label": 1
    },
    "2109.02806": {
        "title": "Symbolic Computation in Software Science: My Personal View",
        "abstract": "In this note, I develop my personal view on the scope and relevance of\nsymbolic computation in software science. For this, I discuss the interaction\nand differences between symbolic computation, software science, automatic\nprogramming, mathematical knowledge management, artificial intelligence,\nalgorithmic intelligence, numerical computation, and machine learning. In the\ndiscussion of these notions, I allow myself to refer also to papers (1982,\n1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at\nearly stages of some of these fields.",
        "date": "2021-09-07T01:41:41+00:00",
        "label": 0
    },
    "1505.00911": {
        "title": "Green open access in computer science - an exploratory study on author-based self-archiving awareness, practice, and inhibitors",
        "abstract": "Access to the work of others is something that is too often taken for\ngranted, yet problematic and difficult to be obtained unless someone pays for\nit. Green and gold open access are claimed to be a solution to this problem.\nWhile open access is gaining momentum in some fields, there is a limited and\nseasoned knowledge about self-archiving in computer science. In particular,\nthere is an inadequate understanding of author-based self-archiving awareness,\npractice, and inhibitors. This article reports an exploratory study of the\nawareness of self-archiving, the practice of self-archiving, and the inhibitors\nof self-archiving among authors in an Italian computer science faculty.\nForty-nine individuals among interns, PhD students, researchers, and professors\nwere recruited in a questionnaire (response rate of 72.8%). The quantitative\nand qualitative responses suggested that there is still work needed in terms of\nadvocating green open access to computer science authors who seldom\nself-archive and when they do, they often infringe the copyright transfer\nagreements (CTAs) of the publishers. In addition, tools from the open-source\ncommunity are needed to facilitate author-based self-archiving, which should\ncomprise of an automatic check of the CTAs. The study identified nine factors\ninhibiting the act of self-archiving among computer scientists. As a first\nstep, this study proposes several propositions regarding author-based\nself-archiving in computer science that can be further investigated.\nRecommendations to foster self-archiving in computer science, based on the\nresults, are provided.",
        "date": "2015-05-05T08:19:11+00:00",
        "label": 0
    },
    "2007.03606": {
        "title": "Data Science: A Comprehensive Overview",
        "abstract": "The twenty-first century has ushered in the age of big data and data economy,\nin which data DNA, which carries important knowledge, insights and potential,\nhas become an intrinsic constituent of all data-based organisms. An appropriate\nunderstanding of data DNA and its organisms relies on the new field of data\nscience and its keystone, analytics. Although it is widely debated whether big\ndata is only hype and buzz, and data science is still in a very early phase,\nsignificant challenges and opportunities are emerging or have been inspired by\nthe research, innovation, business, profession, and education of data science.\nThis paper provides a comprehensive survey and tutorial of the fundamental\naspects of data science: the evolution from data analysis to data science, the\ndata science concepts, a big picture of the era of data science, the major\nchallenges and directions in data innovation, the nature of data analytics, new\nindustrialization and service opportunities in the data economy, the profession\nand competency of data education, and the future of data science. This article\nis the first in the field to draw a comprehensive big picture, in addition to\noffering rich observations, lessons and thinking about data science and\nanalytics.",
        "date": "2020-07-01T02:33:58+00:00",
        "label": 0
    },
    "1705.02203": {
        "title": "Analysis of Computational Science Papers from ICCS 2001-2016 using Topic Modeling and Graph Theory",
        "abstract": "This paper presents results of topic modeling and network models of topics\nusing the International Conference on Computational Science corpus, which\ncontains domain-specific (computational science) papers over sixteen years (a\ntotal of 5695 papers). We discuss topical structures of International\nConference on Computational Science, how these topics evolve over time in\nresponse to the topicality of various problems, technologies and methods, and\nhow all these topics relate to one another. This analysis illustrates\nmultidisciplinary research and collaborations among scientific communities, by\nconstructing static and dynamic networks from the topic modeling results and\nthe keywords of authors. The results of this study give insights about the past\nand future trends of core discussion topics in computational science. We used\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\ntopics and labeled and grouped results hierarchically.",
        "date": "2017-04-18T13:24:41+00:00",
        "label": 0
    },
    "0703007": {
        "title": "Intensional properties of polygraphs",
        "abstract": "We present polygraphic programs, a subclass of Albert Burroni's polygraphs,\nas a computational model, showing how these objects can be seen as first-order\nfunctional programs. We prove that the model is Turing complete. We use\npolygraphic interpretations, a termination proof method introduced by the\nsecond author, to characterize polygraphic programs that compute in polynomial\ntime. We conclude with a characterization of polynomial time functions and\nnon-deterministic polynomial time functions.",
        "date": "2007-03-02T09:21:20+00:00",
        "label": 0
    },
    "2303.08774": {
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "date": "2023-03-15T17:15:04+00:00",
        "label": 1
    },
    "2207.05221": {
        "title": "Language Models (Mostly) Know What They Know",
        "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
        "date": "2022-07-11T22:59:39+00:00",
        "label": 1
    },
    "2203.15556": {
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "date": "2022-03-29T13:38:03+00:00",
        "label": 1
    },
    "2206.09250": {
        "title": "Robin Milner's Work on Concurrency: An Appreciation",
        "abstract": "We give a short appreciation of Robin Milner's seminal contributions to the\ntheory of concurrency.",
        "date": "2022-06-18T17:22:01+00:00",
        "label": 0
    },
    "2301.06885": {
        "title": "Computer Science for Future -- Sustainability and Climate Protection in the Computer Science Courses of the HAW Hamburg",
        "abstract": "Computer Science for Future (CS4F) is an initiative in the Department of\nComputer Science at HAW Hamburg. The aim of the initiative is a paradigm shift\nin the discipline of computer science, thus establishing sustainability goals\nas a primary leitmotif for teaching and research. The focus is on teaching\nsince the most promising multipliers are the students of a university. The\nchange in teaching influences our research, the transfer to business and civil\nsociety as well as the change in our own institution. In this article, we\npresent the initiative CS4F and reflect primarily on the role of students as\namplifiers in the transformation process of computer science.",
        "date": "2023-01-17T13:43:57+00:00",
        "label": 0
    },
    "1210.0736": {
        "title": "Quantum Computation and Quantum Information",
        "abstract": "Quantum computation and quantum information are of great current interest in\ncomputer science, mathematics, physical sciences and engineering. They will\nlikely lead to a new wave of technological innovations in communication,\ncomputation and cryptography. As the theory of quantum physics is fundamentally\nstochastic, randomness and uncertainty are deeply rooted in quantum\ncomputation, quantum simulation and quantum information. Consequently quantum\nalgorithms are random in nature, and quantum simulation utilizes Monte Carlo\ntechniques extensively. Thus statistics can play an important role in quantum\ncomputation and quantum simulation, which in turn offer great potential to\nrevolutionize computational statistics. While only pseudo-random numbers can be\ngenerated by classical computers, quantum computers are able to produce genuine\nrandom numbers; quantum computers can exponentially or quadratically speed up\nmedian evaluation, Monte Carlo integration and Markov chain simulation. This\npaper gives a brief review on quantum computation, quantum simulation and\nquantum information. We introduce the basic concepts of quantum computation and\nquantum simulation and present quantum algorithms that are known to be much\nfaster than the available classic algorithms. We provide a statistical\nframework for the analysis of quantum algorithms and quantum simulation.",
        "date": "2012-10-02T11:47:37+00:00",
        "label": 0
    },
    "2305.14233": {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "abstract": "Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
        "date": "2023-05-23T16:49:14+00:00",
        "label": 1
    },
    "1812.05159": {
        "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
        "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the\nlearning dynamics of neural networks as they train on single classification\ntasks. Our goal is to understand whether a related phenomenon occurs when data\ndoes not undergo a clear distributional shift. We define a `forgetting event'\nto have occurred when an individual training example transitions from being\nclassified correctly to incorrectly over the course of learning. Across several\nbenchmark data sets, we find that: (i) certain examples are forgotten with high\nfrequency, and some not at all; (ii) a data set's (un)forgettable examples\ngeneralize across neural architectures; and (iii) based on forgetting dynamics,\na significant fraction of examples can be omitted from the training data set\nwhile still maintaining state-of-the-art generalization performance.",
        "date": "2018-12-12T21:24:15+00:00",
        "label": 1
    },
    "2310.06694": {
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
        "abstract": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\nthe concurrent TinyLlama models, on a wide range of downstream and instruction\ntuning evaluations, while requiring only 3% of compute compared to training\nsuch models from scratch. This work provides compelling evidence that\nleveraging existing LLMs with structured pruning is a far more cost-effective\napproach for building competitive small-scale LLMs",
        "date": "2023-10-10T15:13:30+00:00",
        "label": 1
    },
    "2207.01934": {
        "title": "How sustainable is \"common\" data science in terms of power consumption?",
        "abstract": "Continuous developments in data science have brought forth an exponential\nincrease in complexity of machine learning models. Additionally, data\nscientists have become ubiquitous in the private market, academic environments\nand even as a hobby. All of these trends are on a steady rise, and are\nassociated with an increase in power consumption and associated carbon\nfootprint. The increasing carbon footprint of large-scale advanced data science\nhas already received attention, but the latter trend has not. This work aims to\nestimate the contribution of the increasingly popular \"common\" data science to\nthe global carbon footprint. To this end, the power consumption of several\ntypical tasks in the aforementioned common data science tasks will be measured\nand compared to: large-scale \"advanced\" data science, common computer-related\ntasks, and everyday non-computer related tasks. This is done by converting the\nmeasurements to the equivalent unit of \"km driven by car\". Our main findings\nare: \"common\" data science consumes $2.57$ more power than regular computer\nusage, but less than some common everyday power-consuming tasks such as\nlighting or heating; large-scale data science consumes substantially more power\nthan common data science.",
        "date": "2022-07-05T10:15:22+00:00",
        "label": 0
    },
    "1904.13112": {
        "title": "On the incomputability of computable dimension",
        "abstract": "Using an iterative tree construction we show that for simple computable\nsubsets of the Cantor space Hausdorff, constructive and computable dimensions\nmight be incomputable.",
        "date": "2019-04-30T09:07:37+00:00",
        "label": 0
    },
    "2103.10489": {
        "title": "Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective",
        "abstract": "From a computer science perspective, addressing on-line hate speech is a\nchallenging task that is attracting the attention of both industry (mainly\nsocial media platform owners) and academia. In this chapter, we provide an\noverview of state-of-the-art data-science approaches - how they define hate\nspeech, which tasks they solve to mitigate the phenomenon, and how they address\nthese tasks. We limit our investigation mostly to (semi-)automatic detection of\nhate speech, which is the task that the majority of existing computer science\nworks focus on. Finally, we summarize the challenges and the open problems in\nthe current data-science research and the future directions in this field. Our\naim is to prepare an easily understandable report, capable to promote the\nmultidisciplinary character of hate speech research. Researchers from other\ndomains (e.g., psychology and sociology) can thus take advantage of the\nknowledge achieved in the computer science domain but also contribute back and\nhelp improve how computer science is addressing that urgent and socially\nrelevant issue which is the prevalence of hate speech in social media.",
        "date": "2021-03-18T19:19:44+00:00",
        "label": 0
    },
    "2212.10560": {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.",
        "date": "2022-12-20T18:59:19+00:00",
        "label": 1
    },
    "1808.05686": {
        "title": "Embedded EthiCS: Integrating Ethics Broadly Across Computer Science Education",
        "abstract": "Computing technologies have become pervasive in daily life, sometimes\nbringing unintended but harmful consequences. For students to learn to think\nnot only about what technology they could create, but also about what\ntechnology they should create, computer science curricula must expand to\ninclude ethical reasoning about the societal value and impact of these\ntechnologies. This paper presents Embedded EthiCS, a novel approach to\nintegrating ethics into computer science education that incorporates ethical\nreasoning throughout courses in the standard computer science curriculum. It\nthus changes existing courses rather than requiring wholly new courses. The\npaper describes a pilot Embedded EthiCS program that embeds philosophers\nteaching ethical reasoning directly into computer science courses. It discusses\nlessons learned and challenges to implementing such a program across different\ntypes of academic institutions.",
        "date": "2018-08-16T21:45:54+00:00",
        "label": 0
    },
    "0907.3804": {
        "title": "Decidability of higher-order matching",
        "abstract": "We show that the higher-order matching problem is decidable using a\ngame-theoretic argument.",
        "date": "2009-07-22T09:17:30+00:00",
        "label": 0
    },
    "2104.14337": {
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.",
        "date": "2021-04-07T17:49:17+00:00",
        "label": 1
    },
    "1510.03055": {
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.",
        "date": "2015-10-11T14:04:57+00:00",
        "label": 1
    },
    "1108.3558": {
        "title": "Proceedings of the 5th Workshop on Membrane Computing and Biologically Inspired Process Calculi (MeCBIC 2011)",
        "abstract": "This volume represents the proceedings of the 5th Workshop on Membrane\nComputing and Biologically Inspired Process Calculi (MeCBIC 2011), held\ntogether with the 12th International Conference on Membrane Computing on 23rd\nAugust 2011 in Fontainebleau, France.",
        "date": "2011-08-17T19:41:29+00:00",
        "label": 0
    },
    "2311.10774": {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "abstract": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.",
        "date": "2023-11-15T23:36:42+00:00",
        "label": 1
    },
    "2302.04062": {
        "title": "Machine Learning for Synthetic Data Generation: A Review",
        "abstract": "Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.",
        "date": "2023-02-08T13:59:31+00:00",
        "label": 1
    },
    "2303.14753": {
        "title": "Does \"Deep Learning on a Data Diet\" reproduce? Overall yes, but GraNd at Initialization does not",
        "abstract": "The paper 'Deep Learning on a Data Diet' by Paul et al. (2021) introduces two\ninnovative metrics for pruning datasets during the training of neural networks.\nWhile we are able to replicate the results for the EL2N score at epoch 20, the\nsame cannot be said for the GraNd score at initialization. The GraNd scores\nlater in training provide useful pruning signals, however. The GraNd score at\ninitialization calculates the average gradient norm of an input sample across\nmultiple randomly initialized models before any training has taken place. Our\nanalysis reveals a strong correlation between the GraNd score at initialization\nand the input norm of a sample, suggesting that the latter could have been a\ncheap new baseline for data pruning. Unfortunately, neither the GraNd score at\ninitialization nor the input norm surpasses random pruning in performance. This\ncontradicts one of the findings in Paul et al. (2021). We were unable to\nreproduce their CIFAR-10 results using both an updated version of the original\nJAX repository and in a newly implemented PyTorch codebase. An investigation of\nthe underlying JAX/FLAX code from 2021 surfaced a bug in the checkpoint\nrestoring code that was fixed in April 2021\n(https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5).",
        "date": "2023-03-26T15:13:19+00:00",
        "label": 1
    },
    "2305.11383": {
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
        "abstract": "Recent works on instruction tuning (IT) have achieved great performance with\nzero-shot generalizability to unseen tasks. With additional context (e.g., task\ndefinition, examples) provided to models for fine-tuning, they achieved much\nhigher performance than untuned models. Despite impressive performance gains,\nwhat models learn from IT remains understudied. In this work, we analyze how\nmodels utilize instructions during IT by comparing model training with altered\nvs. original instructions. Specifically, we create simplified task definitions\nby removing all semantic components and only leaving the output space\ninformation, and delusive examples that contain incorrect input-output mapping.\nOur experiments show that models trained on simplified task definition or\ndelusive examples can achieve comparable performance to the ones trained on the\noriginal instructions and examples. Furthermore, we introduce a random baseline\nto perform zeroshot classification tasks, and find it achieves similar\nperformance (42.6% exact-match) as IT does (43% exact-match) in low resource\nsetting, while both methods outperform naive T5 significantly (30% per\nexact-match). Our analysis provides evidence that the impressive performance\ngain of current IT models can come from picking up superficial patterns, such\nas learning the output format and guessing. Our study highlights the urgent\nneed for more reliable IT methods and evaluation.",
        "date": "2023-05-19T02:00:47+00:00",
        "label": 1
    },
    "2002.06178": {
        "title": "Programming Paradigms, Turing Completeness and Computational Thinking",
        "abstract": "The notion of programming paradigms, with associated programming languages\nand methodologies, is a well established tenet of Computer Science pedagogy,\nenshrined in international curricula. However, this notion sits ill with Kuhn's\nclassic conceptualisation of a scientific paradigm as a dominant world view,\nwhich supersedes its predecessors through superior explanatory power.\nFurthermore, it is not at all clear how programming paradigms are to be\ncharacterised and differentiated. Indeed, on closer inspection, apparently\ndisparate programming paradigms are very strongly connected. Rather, they\nshould be viewed as different traditions of a unitary Computer Science paradigm\nof Turing complete computation complemented by Computational Thinking.",
        "date": "2020-02-14T18:56:12+00:00",
        "label": 0
    },
    "1904.01053": {
        "title": "Computer simulations in science and engineering - Concepts - Practices - Perspectives",
        "abstract": "The ubiquitous presence of computer simulations in all kinds of research\nareas evidence their role as the new driving force for the advancement of\nscience and engineering research. Nothing seems to escape the image of success\nthat computer simulations project onto the research community and the general\npublic. One simple way to illustrate this consists of asking ourselves how\nwould contemporary science and engineering look like without the use of\ncomputer simulations. The answer would certainly diverge from the current image\nwe have of scientific and engineering research.\n  As much as computer simulations are successful, they are also methods that\nfail in their purpose of inquiring about the world; and as much as researchers\nmake use of them, computer simulations raise important questions that are at\nthe heart of contemporary science and engineering practice. In this respect,\ncomputer simulations make a fantastic subject of research for the natural\nsciences, the social sciences, engineering and, as in our case, also for\nphilosophy. Studies on computer simulations touch upon many different facets of\nscientific and engineering research and evoke philosophically inclined\nquestions of interpretation with close ties to problems in experimental\nsettings and engineering applications (...)",
        "date": "2019-03-09T15:26:05+00:00",
        "label": 0
    },
    "2103.07191": {
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
        "abstract": "The problem of designing NLP solvers for math word problems (MWP) has seen\nsustained research activity and steady gains in the test accuracy. Since\nexisting solvers achieve high performance on the benchmark datasets for\nelementary level MWPs containing one-unknown arithmetic word problems, such\nproblems are often considered \"solved\" with the bulk of research attention\nmoving to more complex MWPs. In this paper, we restrict our attention to\nEnglish MWPs taught in grades four and lower. We provide strong evidence that\nthe existing MWP solvers rely on shallow heuristics to achieve high performance\non the benchmark datasets. To this end, we show that MWP solvers that do not\nhave access to the question asked in the MWP can still solve a large fraction\nof MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve\nsurprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP,\ncreated by applying carefully chosen variations over examples sampled from\nexisting datasets. The best accuracy achieved by state-of-the-art models is\nsubstantially lower on SVAMP, thus showing that much remains to be done even\nfor the simplest of the MWPs.",
        "date": "2021-03-12T10:23:47+00:00",
        "label": 1
    },
    "0202038": {
        "title": "The efficient generation of unstructured control volumes in 2D and 3D",
        "abstract": "Many problems in engineering, chemistry and physics require the\nrepresentation of solutions in complex geometries. In the paper we deal with a\nproblem of unstructured mesh generation for the control volume method. We\npropose an algorithm which bases on the spheres generation in central points of\nthe control volumes.",
        "date": "2002-02-26T16:32:12+00:00",
        "label": 0
    },
    "2009.07118": {
        "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
        "abstract": "When scaled to hundreds of billions of parameters, pretrained language models\nsuch as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.\nHowever, enormous amounts of compute are required for training and applying\nsuch big models, resulting in a large carbon footprint and making it difficult\nfor researchers and practitioners to use them. We show that performance similar\nto GPT-3 can be obtained with language models that are much \"greener\" in that\ntheir parameter count is several orders of magnitude smaller. This is achieved\nby converting textual inputs into cloze questions that contain a task\ndescription, combined with gradient-based optimization; exploiting unlabeled\ndata gives further improvements. We identify key factors required for\nsuccessful natural language understanding with small language models.",
        "date": "2020-09-15T14:18:53+00:00",
        "label": 1
    },
    "0609070": {
        "title": "Exploring Computer Science Concepts with a Ready-made Computer Game Framework",
        "abstract": "Leveraging the prevailing interest in computer games among college students,\nboth for entertainment and as a possible career path, is a major reason for the\nincreasing prevalence of computer game design courses in computer science\ncurricula. Because implementing a computer game requires strong programming\nskills, game design courses are most often restricted to more advanced computer\nscience students. This paper reports on a ready-made game design and\nexperimentation framework, implemented in Java, that makes game programming\nmore widely accessible. This framework, called Labyrinth, enables students at\nall programming skill levels to participate in computer game design. We\ndescribe the architecture of the framework, and discuss programming projects\nsuitable for a wide variety of computer science courses, from capstone to\nnon-major.",
        "date": "2006-09-12T19:49:55+00:00",
        "label": 0
    },
    "2202.00622": {
        "title": "Datamodels: Predicting Predictions from Training Data",
        "abstract": "We present a conceptual framework, datamodeling, for analyzing the behavior\nof a model class in terms of the training data. For any fixed \"target\" example\n$x$, training set $S$, and learning algorithm, a datamodel is a parameterized\nfunction $2^S \\to \\mathbb{R}$ that for any subset of $S' \\subset S$ -- using\nonly information about which examples of $S$ are contained in $S'$ -- predicts\nthe outcome of training a model on $S'$ and evaluating on $x$. Despite the\npotential complexity of the underlying process being approximated (e.g.,\nend-to-end training and evaluation of deep neural networks), we show that even\nsimple linear datamodels can successfully predict model outputs. We then\ndemonstrate that datamodels give rise to a variety of applications, such as:\naccurately predicting the effect of dataset counterfactuals; identifying\nbrittle predictions; finding semantically similar examples; quantifying\ntrain-test leakage; and embedding data into a well-behaved and feature-rich\nrepresentation space. Data for this paper (including pre-computed datamodels as\nwell as raw predictions from four million trained deep neural networks) is\navailable at https://github.com/MadryLab/datamodels-data .",
        "date": "2022-02-01T18:15:24+00:00",
        "label": 1
    },
    "1908.05986": {
        "title": "FAIR and Open Computer Science Research Software",
        "abstract": "In computational science and in computer science, research software is a\ncentral asset for research. Computational science is the application of\ncomputer science and software engineering principles to solving scientific\nproblems, whereas computer science is the study of computer hardware and\nsoftware design.\n  The Open Science agenda holds that science advances faster when we can build\non existing results. Therefore, research software has to be reusable for\nadvancing science. Thus, we need proper research software engineering for\nobtaining reusable and sustainable research software. This way, software\nengineering methods may improve research in other disciplines. However,\nresearch in software engineering and computer science itself will also benefit\nfrom reuse when research software is involved.\n  For good scientific practice, the resulting research software should be open\nand adhere to the FAIR principles (findable, accessible, interoperable and\nrepeatable) to allow repeatability, reproducibility, and reuse. Compared to\nresearch data, research software should be both archived for reproducibility\nand actively maintained for reusability. The FAIR data principles do not\nrequire openness, but research software should be open source software.\nEstablished open source software licenses provide sufficient licensing options,\nsuch that it should be the rare exception to keep research software closed.\n  We review and analyze the current state in this area in order to give\nrecommendations for making computer science research software FAIR and open. We\nobserve that research software publishing practices in computer science and in\ncomputational science show significant differences.",
        "date": "2019-08-16T14:26:08+00:00",
        "label": 0
    },
    "2407.07263": {
        "title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models",
        "abstract": "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
        "date": "2024-07-09T22:37:59+00:00",
        "label": 1
    },
    "1103.3321": {
        "title": "Typed Operational Semantics for Dependent Record Types",
        "abstract": "Typed operational semantics is a method developed by H. Goguen to prove\nmeta-theoretic properties of type systems. This paper studies the metatheory of\na type system with dependent record types, using the approach of typed\noperational semantics. In particular, the metatheoretical properties we have\nproved include strong normalisation, Church-Rosser and subject reduction.",
        "date": "2011-03-17T00:19:42+00:00",
        "label": 0
    },
    "2303.09540": {
        "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
        "abstract": "Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.",
        "date": "2023-03-16T17:53:24+00:00",
        "label": 1
    },
    "2005.04118": {
        "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList",
        "abstract": "Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.",
        "date": "2020-05-08T15:48:31+00:00",
        "label": 1
    },
    "2203.14544": {
        "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning",
        "abstract": "The goal of continual learning (CL) is to efficiently update a machine\nlearning model with new data without forgetting previously-learned knowledge.\nMost widely-used CL methods rely on a rehearsal memory of data points to be\nreused while training on new data. Curating such a rehearsal memory to maintain\na small, informative subset of all the data seen so far is crucial to the\nsuccess of these methods. We devise a coreset selection method for\nrehearsal-based continual learning. Our method is based on the idea of gradient\nmatching: The gradients induced by the coreset should match, as closely as\npossible, those induced by the original training dataset. Inspired by the\nneural tangent kernel theory, we perform this gradient matching across the\nmodel's initialization distribution, allowing us to extract a coreset without\nhaving to train the model first. We evaluate the method on a wide range of\ncontinual learning scenarios and demonstrate that it improves the performance\nof rehearsal-based CL methods compared to competing memory management\nstrategies such as reservoir sampling.",
        "date": "2022-03-28T07:37:17+00:00",
        "label": 1
    },
    "2308.13539": {
        "title": "Redefining Computer Science Education: Code-Centric to Natural Language Programming with AI-Based No-Code Platforms",
        "abstract": "This paper delves into the evolving relationship between humans and computers\nin the realm of programming. Historically, programming has been a dialogue\nwhere humans meticulously crafted communication to suit machine understanding,\nshaping the trajectory of computer science education. However, the advent of\nAI-based no-code platforms is revolutionizing this dynamic. Now, humans can\nconverse in their natural language, expecting machines to interpret and act.\nThis shift has profound implications for computer science education. As\neducators, it's imperative to integrate this new dynamic into curricula. In\nthis paper, we've explored several pertinent research questions in this\ntransformation, which demand continued inquiry and adaptation in our\neducational strategies.",
        "date": "2023-08-19T02:44:35+00:00",
        "label": 0
    },
    "2402.01865": {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "abstract": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "date": "2024-02-02T19:43:15+00:00",
        "label": 1
    },
    "9505013": {
        "title": "Wavelet basis for the Schr\u00f6dinger equation",
        "abstract": "The self-similar representation for the Schr\\\"{o}dinger equation is derived.",
        "date": "1995-05-16T16:19:16+00:00",
        "label": 0
    },
    "1003.1888": {
        "title": "Biology-Derived Algorithms in Engineering Optimization",
        "abstract": "Biology-derived algorithms are an important part of computational sciences,\nwhich are essential to many scientific disciplines and engineering\napplications. Many computational methods are derived from or based on the\nanalogy to natural evolution and biological activities, and these biologically\ninspired computations include genetic algorithms, neural networks, cellular\nautomata, and other algorithms.",
        "date": "2010-03-09T14:53:12+00:00",
        "label": 0
    },
    "2302.13971": {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
        "date": "2023-02-27T17:11:15+00:00",
        "label": 1
    },
    "1908.03793": {
        "title": "The rise and rise of interdisciplinary research: Understanding the interaction dynamics of three major fields -- Physics, Mathematics & Computer Science",
        "abstract": "The distinction between sciences is becoming increasingly more artificial --\nan approach from one area can be easily applied to the other. More exciting\nresearch nowadays is happening perhaps at the interfaces of disciplines like\nPhysics, Mathematics and Computer Science. How do these interfaces emerge and\ninteract? For instance, is there a specific pattern in which these fields cite\neach other? In this article, we investigate a collection of more than 1.2\nmillion papers from three different scientific disciplines -- Physics,\nMathematics, and Computer Science. We show how over a timescale the citation\npatterns from the core science fields (Physics, Mathematics) to the applied and\nfast-growing field of Computer Science have drastically increased. Further, we\nobserve how certain subfields in these disciplines are shrinking while others\nare becoming tremendously popular. For instance, an intriguing observation is\nthat citations from Mathematics to the subfield of machine learning in Computer\nScience in recent times are exponentially increasing.",
        "date": "2019-08-10T17:58:19+00:00",
        "label": 0
    },
    "2205.01553": {
        "title": "Why The Trans Programmer?",
        "abstract": "Through online anecdotal evidence and online communities, there is an\nin-group idea of trans people (specifically trans-feminine individuals)\ndisproportionately entering computer science education & fields. Existing data\nsuggests this is a plausible trend, yet no research has been done into exactly\nwhy. As computer science education (traditional schooling or self-taught\nmethods) is integral to working in computer science fields, a simple research\nsurvey was conducted to gather data on 138 trans people's experiences with\ncomputer science & computer science education. This article's purpose is to\nshed insight on the motivations for trans individuals choosing computer science\npaths, while acting as a basis and call to action for further research.",
        "date": "2022-05-03T15:06:23+00:00",
        "label": 0
    },
    "2403.16898": {
        "title": "Concerned with Data Contamination? Assessing Countermeasures in Code Language Model",
        "abstract": "Various techniques have been proposed to leverage the capabilities of code\nlanguage models (CLMs) for SE tasks. While these techniques typically evaluate\ntheir effectiveness using publicly available datasets, the evaluation can be\nsubject to data contamination threats where the evaluation datasets have\nalready been used to train the concerned CLMs. This can significantly affect\nthe reliability of the evaluation. Different countermeasures have been\nsuggested to mitigate the data contamination threat. Countermeasures include\nusing more recent data, curating new data, and refactoring existing data are\nintroduced, yet it is unclear whether these countermeasures could really\nmitigate data contamination threats to model evaluation. To fill the gap, we\nsystematically study to quantify the impacts of these countermeasures on CLMs'\nperformance. To facilitate the study, we collected over 2 million Python\nfunctions with timestamps ranging from January 1st, 2018, to December 31st,\n2023. The data created before the models' cut-off date are considered\n\"contaminated data\", while the data where the countermeasures are taken are\nregarded as \"cleansed data\". We study the impact of these countermeasures by\ninvestigating the difference in CLMs' performance on contaminated and cleansed\ndata derived from different countermeasures. Our experiments yield several\ninteresting observations. For instance, CLMs do not necessarily perform worse\non data after the models' cut-off date; on the contrary, they sometimes perform\nbetter. In addition, refactoring did not always result in decreased\nperformance; it could lead to improvements instead. Furthermore, existing\nmetrics such as perplexity cannot distinguish contaminated/cleansed data. We\nhope that the results and observations could help deepen the understanding of\nCLMs' capabilities and inform the community about data contamination.",
        "date": "2024-03-25T16:10:25+00:00",
        "label": 1
    },
    "2109.01661": {
        "title": "Data science and Machine learning in the Clouds: A Perspective for the Future",
        "abstract": "As we are fast approaching the beginning of a paradigm shift in the field of\nscience, Data driven science (the so called fourth science paradigm) is going\nto be the driving force in research and innovation. From medicine to\nbiodiversity and astronomy to geology, all these terms are somehow going to be\naffected by this paradigm shift. The huge amount of data to be processed under\nthis new paradigm will be a major concern in the future and one will strongly\nrequire cloud based services in all the aspects of these computations (from\nstorage to compute and other services). Another aspect will be energy\nconsumption and performance of prediction jobs and tasks within such a\nscientific paradigm which will change the way one sees computation. Data\nscience has heavily impacted or rather triggered the emergence of Machine\nLearning, Signal/Image/Video processing related algorithms, Artificial\nintelligence, Robotics, health informatics, geoinformatics, and many more such\nareas of interest. Hence, we envisage an era where Data science can deliver its\npromises with the help of the existing cloud based platforms and services with\nthe addition of new services. In this article, we discuss about data driven\nscience and Machine learning and how they are going to be linked through cloud\nbased services in the future. It also discusses the rise of paradigms like\napproximate computing, quantum computing and many more in recent times and\ntheir applicability in big data processing, data science, analytics, prediction\nand machine learning in the cloud environments.",
        "date": "2021-09-02T17:36:24+00:00",
        "label": 0
    },
    "2004.02990": {
        "title": "Evaluating the Evaluation of Diversity in Natural Language Generation",
        "abstract": "Despite growing interest in natural language generation (NLG) models that\nproduce diverse outputs, there is currently no principled method for evaluating\nthe diversity of an NLG system. In this work, we propose a framework for\nevaluating diversity metrics. The framework measures the correlation between a\nproposed diversity metric and a diversity parameter, a single parameter that\ncontrols some aspect of diversity in generated text. For example, a diversity\nparameter might be a binary variable used to instruct crowdsourcing workers to\ngenerate text with either low or high content diversity. We demonstrate the\nutility of our framework by: (a) establishing best practices for eliciting\ndiversity judgments from humans, (b) showing that humans substantially\noutperform automatic metrics in estimating content diversity, and (c)\ndemonstrating that existing methods for controlling diversity by tuning a\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\nadvance the understanding of different diversity metrics, an essential step on\nthe road towards better NLG systems.",
        "date": "2020-04-06T20:44:10+00:00",
        "label": 1
    },
    "2310.07641": {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "abstract": "As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these ``LLM evaluators'', particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.",
        "date": "2023-10-11T16:38:11+00:00",
        "label": 1
    },
    "2311.00288": {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results\nby training large language models (LLMs) on a massive amount of diverse tasks\nwith instructions. However, how to select new tasks to improve the performance\nand generalizability of IT models remains an open question. Training on all\nexisting tasks is impractical due to prohibiting computation requirements, and\nrandomly selecting tasks can lead to suboptimal performance. In this work, we\npropose active instruction tuning based on prompt uncertainty, a novel\nframework to identify informative tasks, and then actively tune the models on\nthe selected tasks. We represent the informativeness of new tasks with the\ndisagreement of the current model outputs over perturbed prompts. Our\nexperiments on NIV2 and Self-Instruct datasets demonstrate that our method\nconsistently outperforms other baseline strategies for task selection,\nachieving better out-of-distribution generalization with fewer training tasks.\nAdditionally, we introduce a task map that categorizes and diagnoses tasks\nbased on prompt uncertainty and prediction probability. We discover that\ntraining on ambiguous (prompt-uncertain) tasks improves generalization while\ntraining on difficult (prompt-certain and low-probability) tasks offers no\nbenefit, underscoring the importance of task selection for instruction tuning.",
        "date": "2023-11-01T04:40:05+00:00",
        "label": 1
    },
    "2203.08242": {
        "title": "Data Contamination: From Memorization to Exploitation",
        "abstract": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "date": "2022-03-15T20:37:16+00:00",
        "label": 1
    },
    "1804.08293": {
        "title": "Materials science and engineering: New vision in the era of artificial intelligence",
        "abstract": "Scientific discovery evolves from the experimental, through the theoretical\nand computational, to the current data-intensive paradigm. Materials science is\nno exception, especially for computational materials science. In recent years,\ngreat achievements have been made in the field of materials science and\nengineering (MSE). Here, we review the previous paradigms of materials science\nand some classical MSE models. Then, our data-intensive MSE (DIMSE) model is\nproposed to reshape future materials innovations. This work will help to\naddress the global challenge for materials discovery in the era of artificial\nintelligence (AI), and essentially contribute to accelerating future materials\ncontinuum.",
        "date": "2018-04-23T09:01:57+00:00",
        "label": 0
    },
    "2106.07553": {
        "title": "A Cognitive Science perspective for learning how to design meaningful user experiences and human-centered technology",
        "abstract": "This paper reviews literature in cognitive science, human-computer\ninteraction (HCI) and natural-language processing (NLP) to consider how\nanalogical reasoning (AR) could help inform the design of communication and\nlearning technologies, as well as online communities and digital platforms.\nFirst, analogical reasoning (AR) is defined, and use-cases of AR in the\ncomputing sciences are presented. The concept of schema is introduced, along\nwith use-cases in computing. Finally, recommendations are offered for future\nwork on using analogical reasoning and schema methods in the computing\nsciences.",
        "date": "2021-06-02T15:00:50+00:00",
        "label": 0
    },
    "2308.12032": {
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction\ndata quality and quantity is a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from open-source datasets, effectively minimizing manual curation and\npotential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to\nidentify discrepancies between a model's expected responses and its intrinsic\ngeneration capability. Through the application of IFD, cherry samples can be\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on datasets like Alpaca and WizardLM underpin our findings; with a\nmere $10\\%$ of original data input, our strategy showcases improved results.\nThis synthesis of self-guided cherry-picking and the IFD metric signifies a\ntransformative leap in the instruction tuning of LLMs, promising both\nefficiency and resource-conscious advancements. Codes, data, and models are\navailable: https://github.com/tianyi-lab/Cherry_LLM",
        "date": "2023-08-23T09:45:29+00:00",
        "label": 1
    },
    "2402.14526": {
        "title": "Balanced Data Sampling for Language Model Training with Clustering",
        "abstract": "Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.",
        "date": "2024-02-22T13:20:53+00:00",
        "label": 1
    },
    "1404.5611": {
        "title": "IMP Science Gateway: from the Portal to the Hub of Virtual Experimental Labs in Materials Science",
        "abstract": "\"Science gateway\" (SG) ideology means a user-friendly intuitive interface\nbetween scientists (or scientific communities) and different software\ncomponents + various distributed computing infrastructures (DCIs) (like grids,\nclouds, clusters), where researchers can focus on their scientific goals and\nless on peculiarities of software/DCI. \"IMP Science Gateway Portal\"\n(http://scigate.imp.kiev.ua) for complex workflow management and integration of\ndistributed computing resources (like clusters, service grids, desktop grids,\nclouds) is presented. It is created on the basis of WS-PGRADE and gUSE\ntechnologies, where WS-PGRADE is designed for science workflow operation and\ngUSE - for smooth integration of available resources for parallel and\ndistributed computing in various heterogeneous distributed computing\ninfrastructures (DCI). The typical scientific workflows with possible scenarios\nof its preparation and usage are presented. Several typical use cases for these\nscience applications (scientific workflows) are considered for molecular\ndynamics (MD) simulations of complex behavior of various nanostructures\n(nanoindentation of graphene layers, defect system relaxation in metal\nnanocrystals, thermal stability of boron nitride nanotubes, etc.). The user\nexperience is analyzed in the context of its practical applications for MD\nsimulations in materials science, physics and nanotechnologies with available\nheterogeneous DCIs. In conclusion, the \"science gateway\" approach - workflow\nmanager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity\nto use the SG portal (like \"IMP Science Gateway Portal\") in a very promising\nway, namely, as a hub of various virtual experimental labs (different software\ncomponents + various requirements to resources) in the context of its practical\nMD applications in materials science, physics, chemistry, biology, and\nnanotechnologies.",
        "date": "2014-04-22T11:55:17+00:00",
        "label": 0
    },
    "1311.5006": {
        "title": "Indagini in Deep Inference",
        "abstract": "Italian master's thesis in Computer Science. It is an overview of the\nstandard tecniques developed in the field of Proof Theory, ending with some\nresults in the new field of Deep Inference, plus an original contribution\ntrying to relate Deep Inference and Process Algebras.",
        "date": "2013-11-20T10:46:35+00:00",
        "label": 0
    },
    "2401.06692": {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "date": "2024-01-12T16:56:54+00:00",
        "label": 1
    },
    "2010.00506": {
        "title": "Edsger Dijkstra. The Man Who Carried Computer Science on His Shoulders",
        "abstract": "This a biographical essay about Edsger Wybe Dijkstra.",
        "date": "2020-10-01T15:55:51+00:00",
        "label": 0
    },
    "1601.05973": {
        "title": "Science Learning via Participation in Online Citizen Science",
        "abstract": "We investigate the development of scientific content knowledge of volunteers\nparticipating in online citizen science projects in the Zooniverse\n(www.zooniverse.org), including the astronomy projects Galaxy Zoo\n(www.galaxyzoo.org) and Planet Hunters (www.planethunters.org). We use\neconometric methods to test how measures of project participation relate to\nsuccess in a science quiz, controlling for factors known to correlate with\nscientific knowledge. Citizen scientists believe they are learning about both\nthe content and processes of science through their participation. Won't don't\ndirectly test the latter, but we find evidence to support the former - that\nmore actively engaged participants perform better in a project-specific science\nknowledge quiz, even after controlling for their general science knowledge. We\ninterpret this as evidence of learning of science content inspired by\nparticipation in online citizen science.",
        "date": "2016-01-22T12:23:10+00:00",
        "label": 0
    },
    "1710.01367": {
        "title": "Validating Computer Security Methods: Meta-methodology for an Adversarial Science",
        "abstract": "How can we justify the validity of our computer security methods? This\nmeta-methodological question is related to recent explorations on the science\nof computer security, which have been hindered by computer security's unique\nproperties. We confront this by developing a taxonomy of properties and\nmethods. Interdisciplinary foundations provide a solid grounding for a set of\nessential concepts, including a decision tree for characterizing adversarial\ninteraction. Several types of invalidation and general ways of addressing them\nare described for technical methods. An interdisciplinary argument from theory\nexplains the role that meta-methodological validation plays in the adversarial\nscience of computer security.",
        "date": "2017-10-03T20:04:33+00:00",
        "label": 0
    },
    "2307.08701": {
        "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
        "abstract": "Large language models (LLMs) strengthen instruction-following capability\nthrough instruction-finetuning (IFT) on supervised instruction/response data.\nHowever, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly\ncontain many low-quality instances with incorrect or irrelevant responses,\nwhich are misleading and detrimental to IFT. In this paper, we propose a simple\nand effective data selection strategy that automatically identifies and filters\nout low-quality data using a strong LLM (e.g., ChatGPT). To this end, we\nintroduce AlpaGasus, which is finetuned on only 9k high-quality data filtered\nfrom the 52k Alpaca data. AlpaGasus significantly outperforms the original\nAlpaca as evaluated by GPT-4 on multiple test sets and the controlled human\nevaluation. Its 13B variant matches $>90\\%$ performance of its teacher LLM\n(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the\nefficacy of our method across diverse datasets, base models, and LLM filters.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\nhttps://lichang-chen.github.io/AlpaGasus/",
        "date": "2023-07-17T17:59:40+00:00",
        "label": 1
    },
    "2406.06326": {
        "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching",
        "abstract": "Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from raw documents through self-teaching. Specifically, we develop a\nSelf-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection. In\naddition, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an\nin-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nLlama2 family models reveal that Self-Tuning consistently exhibits superior\nperformance across all knowledge acquisition tasks and excels in preserving\nprevious knowledge.",
        "date": "2024-06-10T14:42:20+00:00",
        "label": 1
    },
    "2110.08534": {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.",
        "date": "2021-10-16T09:59:33+00:00",
        "label": 1
    },
    "2306.11670": {
        "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
        "abstract": "It is often advantageous to train models on a subset of the available train\nexamples, because the examples are of variable quality or because one would\nlike to train with fewer examples, without sacrificing performance. We present\nGradient Information Optimization (GIO), a scalable, task-agnostic approach to\nthis data selection problem that requires only a small set of (unlabeled)\nexamples representing a target distribution. GIO begins from a natural,\ninformation-theoretic objective that is intractable in practice. Our\ncontribution is in showing that it can be made highly scalable through a simple\nrelaxation of the objective and a highly efficient implementation. In\nexperiments with machine translation, spelling correction, and image\nrecognition, we show that GIO delivers outstanding results with very small\ntrain sets. These findings are robust to different representation models and\nhyperparameters for GIO itself. GIO is task- and domain-agnostic and can be\napplied out-of-the-box to new datasets and domains. We open source a\npip-installable implementation of the algorithm as \"pip install grad-info-opt\".",
        "date": "2023-06-20T16:43:38+00:00",
        "label": 1
    },
    "0502011": {
        "title": "Where the Rubber Meets the Sky: Bridging the Gap between Databases and Science",
        "abstract": "Scientists in all domains face a data avalanche - both from better\ninstruments and from improved simulations. We believe that computer science\ntools and computer scientists are in a position to help all the sciences by\nbuilding tools and developing techniques to manage, analyze, and visualize\npeta-scale scientific information. This article is summarizes our experiences\nover the last seven years trying to bridge the gap between database technology\nand the needs of the astronomy community in building the World-Wide Telescope.",
        "date": "2005-02-02T04:40:55+00:00",
        "label": 0
    },
    "1606.08699": {
        "title": "How to Compute Halting",
        "abstract": "A consistently specified halting function may be computed.",
        "date": "2016-06-26T19:38:10+00:00",
        "label": 0
    },
    "1708.00489": {
        "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
        "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many\nrecognition and learning tasks using a universal recipe; training a deep model\non a very large dataset of supervised examples. However, this approach is\nrather restrictive in practice since collecting a large set of labeled images\nis very expensive. One way to ease this problem is coming up with smart ways\nfor choosing images to be labelled from a very large collection (ie. active\nlearning).\n  Our empirical study suggests that many of the active learning heuristics in\nthe literature are not effective when applied to CNNs in batch setting.\nInspired by these limitations, we define the problem of active learning as\ncore-set selection, ie. choosing set of points such that a model learned over\nthe selected subset is competitive for the remaining data points. We further\npresent a theoretical result characterizing the performance of any selected\nsubset using the geometry of the datapoints. As an active learning algorithm,\nwe choose the subset which is expected to yield best result according to our\ncharacterization. Our experiments show that the proposed method significantly\noutperforms existing approaches in image classification experiments by a large\nmargin.",
        "date": "2017-08-01T19:50:53+00:00",
        "label": 1
    },
    "2407.15235": {
        "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
        "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.",
        "date": "2024-07-21T17:59:20+00:00",
        "label": 1
    },
    "1710.03090": {
        "title": "Theoretical Computer Science for the Working Category Theorist",
        "abstract": "Theoretical computer science discusses foundational issues about\ncomputations. It asks and answers questions such as \"What is a computation?\",\n\"What is computable?\", \"What is efficiently computable?\",\"What is\ninformation?\", \"What is random?\", \"What is an algorithm?\", etc. We will present\nmany of the major themes and theorems with the basic language of category\ntheory. Surprisingly, many interesting theorems and concepts of theoretical\ncomputer science are easy consequences of functoriality and composition when\nyou look at the right categories and functors connecting them.",
        "date": "2017-10-04T19:19:00+00:00",
        "label": 0
    },
    "2311.10833": {
        "title": "Generative AI has lowered the barriers to computational social sciences",
        "abstract": "Generative artificial intelligence (AI) has revolutionized the field of\ncomputational social science, unleashing new possibilities for analyzing\nmultimodal data, especially for scholars who may not have extensive programming\nexpertise. This breakthrough carries profound implications for the realm of\nsocial sciences. Firstly, generative AI can significantly enhance the\nproductivity of social scientists by automating the generation, annotation, and\ndebugging of code. Secondly, it empowers researchers to delve into\nsophisticated data analysis through the innovative use of prompt engineering.\nLastly, the educational sphere of computational social science stands to\nbenefit immensely from these tools, given their exceptional ability to annotate\nand elucidate complex codes for learners, thereby simplifying the learning\nprocess and making the technology more accessible.",
        "date": "2023-11-17T19:24:39+00:00",
        "label": 0
    },
    "2205.01497": {
        "title": "Semantic Diversity in Dialogue with Natural Language Inference",
        "abstract": "Generating diverse, interesting responses to chitchat conversations is a\nproblem for neural conversational agents. This paper makes two substantial\ncontributions to improving diversity in dialogue generation. First, we propose\na novel metric which uses Natural Language Inference (NLI) to measure the\nsemantic diversity of a set of model responses for a conversation. We evaluate\nthis metric using an established framework (Tevet and Berant, 2021) and find\nstrong evidence indicating NLI Diversity is correlated with semantic diversity.\nSpecifically, we show that the contradiction relation is more useful than the\nneutral relation for measuring this diversity and that incorporating the NLI\nmodel's confidence achieves state-of-the-art results. Second, we demonstrate\nhow to iteratively improve the semantic diversity of a sampled set of responses\nvia a new generation procedure called Diversity Threshold Generation, which\nresults in an average 137% increase in NLI Diversity compared to standard\ngeneration procedures.",
        "date": "2022-05-03T13:56:32+00:00",
        "label": 1
    },
    "2109.02501": {
        "title": "Proceedings of the 9th International Symposium on Symbolic Computation in Software Science",
        "abstract": "This volume contains papers presented at the Ninth International Symposium on\nSymbolic Computation in Software Science, SCSS 2021.\n  Symbolic Computation is the science of computing with symbolic objects\n(terms, formulae, programs, representations of algebraic objects, etc.).\nPowerful algorithms have been developed during the past decades for the major\nsubareas of symbolic computation: computer algebra and computational logic.\nThese algorithms and methods are successfully applied in various fields,\nincluding software science, which covers a broad range of topics about software\nconstruction and analysis.\n  Meanwhile, artificial intelligence methods and machine learning algorithms\nare widely used nowadays in various domains and, in particular, combined with\nsymbolic computation. Several approaches mix artificial intelligence and\nsymbolic methods and tools deployed over large corpora to create what is known\nas cognitive systems. Cognitive computing focuses on building systems that\ninteract with humans naturally by reasoning, aiming at learning at scale.\n  The purpose of SCSS is to promote research on theoretical and practical\naspects of symbolic computation in software science, combined with modern\nartificial intelligence techniques. These proceedings contain the keynote paper\nby Bruno Buchberger and ten contributed papers. Besides, the conference program\nincluded three invited talks, nine short and work-in-progress papers, and a\nspecial session on computer algebra and computational logic. Due to the\nCOVID-19 pandemic, the symposium was held completely online. It was organized\nby the Research Institute for Symbolic Computation (RISC) of the Johannes\nKepler University Linz on September 8--10, 2021.",
        "date": "2021-09-06T14:22:11+00:00",
        "label": 0
    },
    "1506.00555": {
        "title": "Writing and Publishing Scientific Articles in Computer Science",
        "abstract": "Over 15 years of teaching, advising students and coordinating scientific\nresearch activities and projects in computer science, we have observed the\ndifficulties of students to write scientific papers to present the results of\ntheir research practices. In addition, they repeatedly have doubts about the\npublishing process. In this article we propose a conceptual framework to\nsupport the writing and publishing of scientific papers in computer science,\nproviding a kind of guide for computer science students to effectively present\nthe results of their research practices, particularly for experimental\nresearch.",
        "date": "2015-06-01T16:09:53+00:00",
        "label": 0
    },
    "1608.05006": {
        "title": "Automaticity in Computation and Student Success in Introductory Physical Science Courses",
        "abstract": "Between 1984 and 2011, the percentage of US bachelor degrees awarded in\nphysics declined by 25%, in chemistry declined by 33%, and overall in physical\nsciences and engineering fell 40%. Data suggest that these declines are\ncorrelated to a deemphasis in most states of practicing computation skills in\nmathematics. Analysis of state standards put into place between 1990 and 2010\nfind that most states directed teachers to deemphasize both memorization and\nstudent practice in computational problem solving. Available state test score\ndata show a significant decline in student computation skills. In recent\ninternational testing, scores for US 16 to 24 year olds in numeracy finished\nlast among 22 tested nations in the OECD. Recent studies in cognitive science\nhave found that to solve well-structured problems in the sciences, students\nmust first memorize fundamental facts and procedures in mathematics and science\nuntil they can be recalled with automaticity, then practice applying those\nskills in a variety of distinctive contexts. Actions are suggested to improve\nUS STEM graduation rates by aligning US math and science curricula with the\nrecommendations of cognitive science.",
        "date": "2016-08-17T16:07:57+00:00",
        "label": 0
    },
    "1702.05962": {
        "title": "Latent Variable Dialogue Models and their Diversity",
        "abstract": "We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.",
        "date": "2017-02-20T13:36:23+00:00",
        "label": 1
    },
    "1111.7159": {
        "title": "Sequentiality vs. Concurrency in Games and Logic",
        "abstract": "Connections between the sequentiality/concurrency distinction and the\nsemantics of proofs are investigated, with particular reference to games and\nLinear Logic.",
        "date": "2011-11-30T13:44:46+00:00",
        "label": 0
    },
    "2403.08763": {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "date": "2024-03-13T17:58:57+00:00",
        "label": 1
    },
    "1910.14210": {
        "title": "Methodological Blind Spots in Machine Learning Fairness: Lessons from the Philosophy of Science and Computer Science",
        "abstract": "In the ML fairness literature, there have been few investigations through the\nviewpoint of philosophy, a lens that encourages the critical evaluation of\nbasic assumptions. The purpose of this paper is to use three ideas from the\nphilosophy of science and computer science to tease out blind spots in the\nassumptions that underlie ML fairness: abstraction, induction, and measurement.\nThrough this investigation, we hope to warn of these methodological blind spots\nand encourage further interdisciplinary investigation in fair-ML through the\nframework of philosophy.",
        "date": "2019-10-31T02:02:31+00:00",
        "label": 0
    },
    "1904.03122": {
        "title": "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        "abstract": "In a corpus of data, outliers are either errors: mistakes in the data that\nare counterproductive, or are unique: informative samples that improve model\nrobustness. Identifying outliers can lead to better datasets by (1) removing\nnoise in datasets and (2) guiding collection of additional data to fill gaps.\nHowever, the problem of detecting both outlier types has received relatively\nlittle attention in NLP, particularly for dialog systems. We introduce a simple\nand effective technique for detecting both erroneous and unique samples in a\ncorpus of short texts using neural sentence embeddings combined with\ndistance-based outlier detection. We also present a novel data collection\npipeline built atop our detection technique to automatically and iteratively\nmine unique data samples while discarding erroneous samples. Experiments show\nthat our outlier detection technique is effective at finding errors while our\ndata collection pipeline yields highly diverse corpora that in turn produce\nmore robust intent classification and slot-filling models.",
        "date": "2019-04-05T15:31:28+00:00",
        "label": 1
    },
    "2403.03387": {
        "title": "Undergraduate data science education: Who has the microphone and what are they saying?",
        "abstract": "The presence of data science has been profound in the scientific community in\nalmost every discipline. An important part of the data science education\nexpansion has been at the undergraduate level. We conducted a systematic\nliterature review to (1) specify current evidence and knowledge gaps in\nundergraduate data science education and (2) inform policymakers and data\nscience educators/practitioners about the present status of data science\neducation research. The majority of the publications in data science education\nthat met our search criteria were available open-access. Our results indicate\nthat data science education research lacks empirical data and reproducibility.\nNot all disciplines contribute equally to the field of data science education.\nComputer science and data science as a separate field emerge as the leading\ncontributors to the literature. In contrast, fields such as statistics,\nmathematics, as well as other fields closely related to data science exhibit a\nlimited presence in studies. We recommend that federal agencies and researchers\n1) invest in empirical data science education research; 2) diversify research\nefforts to enrich the spectrum of types of studies; 3) encourage scholars in\nkey data science fields that are currently underrepresented in the literature\nto contribute more to research and publications.",
        "date": "2024-03-06T00:49:08+00:00",
        "label": 0
    },
    "2402.16827": {
        "title": "A Survey on Data Selection for Language Models",
        "abstract": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.",
        "date": "2024-02-26T18:54:35+00:00",
        "label": 1
    },
    "1111.4755": {
        "title": "Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive Case",
        "abstract": "This paper describes the solution of Hello World transformations in MOLA\ntransformation language. Transformations implementing the task are relatively\nstraightforward and easily inferable from the task specification. The required\nadditional steps related to model import and export are also described.",
        "date": "2011-11-21T05:26:57+00:00",
        "label": 0
    },
    "2309.04564": {
        "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale",
        "abstract": "Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.",
        "date": "2023-09-08T19:34:05+00:00",
        "label": 1
    },
    "2401.06059": {
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.",
        "date": "2024-01-11T17:24:49+00:00",
        "label": 1
    },
    "2407.10671": {
        "title": "Qwen2 Technical Report",
        "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "date": "2024-07-15T12:35:42+00:00",
        "label": 1
    },
    "1706.01538": {
        "title": "On Computation of Matrix Mittag-Leffler Function",
        "abstract": "A method for computation of the matrix Mittag-Leffler function is presented.\nThe method is based on Jordan canonical form and implemented as a Matlab\nroutine.",
        "date": "2017-06-05T20:48:07+00:00",
        "label": 0
    },
    "2407.08102": {
        "title": "Dynamics of Gender Bias within Computer Science",
        "abstract": "A new dataset (N = 7,456) analyzes women's research authorship in the\nAssociation for Computing Machinery's founding 13 Special Interest Groups or\nSIGs, a proxy for computer science. ACM SIGs expanded during 1970-2000; each\nexperienced increasing women's authorship. But diversity abounds. Several SIGs\nhad fewer than 10% women authors while SIGUCCS (university computing centers)\nexceeded 40%. Three SIGs experienced accelerating growth in women's authorship;\nmost, including a composite ACM, had decelerating growth. This research may\nencourage reform efforts, often focusing on general education or workforce\nfactors (across the entity of \"computer science\"), to examine under-studied\ndynamics within computer science that shaped changes in women's participation.",
        "date": "2024-07-11T00:14:21+00:00",
        "label": 0
    },
    "1903.09722": {
        "title": "Pre-trained Language Model Representations for Language Generation",
        "abstract": "Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.",
        "date": "2019-03-22T22:14:51+00:00",
        "label": 1
    },
    "1909.04486": {
        "title": "Data Science in Biomedicine",
        "abstract": "We highlight the role of Data Science in Biomedicine. Our manuscript goes\nfrom the general to the particular, presenting a global definition of Data\nScience and showing the trend for this discipline together with the terms of\ncloud computing and big data. In addition, since Data Science is mostly related\nto areas like economy or business, we describe its importance in biomedicine.\nBiomedical Data Science (BDS) presents the challenge of dealing with data\ncoming from a range of biological and medical research, focusing on\nmethodologies to advance the biomedical science discoveries, in an\ninterdisciplinary context.",
        "date": "2019-09-09T11:31:40+00:00",
        "label": 0
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
        "date": "2019-07-26T17:48:29+00:00",
        "label": 1
    },
    "2310.07849": {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "abstract": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "date": "2023-10-11T19:51:13+00:00",
        "label": 1
    },
    "2008.05561": {
        "title": "The Right Tools for the Job: The Case for Spatial Science Tool-Building",
        "abstract": "This paper was presented as the 8th annual Transactions in GIS plenary\naddress at the American Association of Geographers annual meeting in\nWashington, DC. The spatial sciences have recently seen growing calls for more\naccessible software and tools that better embody geographic science and theory.\nUrban spatial network science offers one clear opportunity: from multiple\nperspectives, tools to model and analyze nonplanar urban spatial networks have\ntraditionally been inaccessible, atheoretical, or otherwise limiting. This\npaper reflects on this state of the field. Then it discusses the motivation,\nexperience, and outcomes of developing OSMnx, a tool intended to help address\nthis. Next it reviews this tool's use in the recent multidisciplinary spatial\nnetwork science literature to highlight upstream and downstream benefits of\nopen-source software development. Tool-building is an essential but poorly\nincentivized component of academic geography and social science more broadly.\nTo conduct better science, we need to build better tools. The paper concludes\nwith paths forward, emphasizing open-source software and reusable computational\ndata science beyond mere reproducibility and replicability.",
        "date": "2020-08-12T20:15:39+00:00",
        "label": 0
    },
    "1910.14599": {
        "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an\niterative, adversarial human-and-model-in-the-loop procedure. We show that\ntraining models on this new dataset leads to state-of-the-art performance on a\nvariety of popular NLI benchmarks, while posing a more difficult challenge with\nits new test set. Our analysis sheds light on the shortcomings of current\nstate-of-the-art models, and shows that non-expert annotators are successful at\nfinding their weaknesses. The data collection method can be applied in a\nnever-ending learning scenario, becoming a moving target for NLU, rather than a\nstatic benchmark that will quickly saturate.",
        "date": "2019-10-31T16:50:43+00:00",
        "label": 1
    },
    "1610.07365": {
        "title": "Introduction: Cognitive Issues in Natural Language Processing",
        "abstract": "This special issue is dedicated to get a better picture of the relationships\nbetween computational linguistics and cognitive science. It specifically raises\ntwo questions: \"what is the potential contribution of computational language\nmodeling to cognitive science?\" and conversely: \"what is the influence of\ncognitive science in contemporary computational linguistics?\"",
        "date": "2016-10-24T11:30:22+00:00",
        "label": 0
    },
    "1307.8029": {
        "title": "Proceedings Fourth International Symposium on Symbolic Computation in Software Science",
        "abstract": "Symbolic computation is the science of computing with symbolic objects\n(terms, formulae, programs, algebraic objects, geometrical objects, etc).\nPowerful symbolic algorithms have been developed during the past decades and\nhave played an influential role in theorem proving, automated reasoning,\nsoftware verification, model checking, rewriting, formalisation of mathematics,\nnetwork security, Groebner bases, characteristic sets, etc.\n  The international Symposium on \"Symbolic Computation in Software Science\" is\nthe fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the\nResearch Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,\nSCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal\nworkshops that bring together researchers from: a) SCORE (Symbolic Computation\nResearch Group) at the University of Tsukuba, Japan, b) Theorema Group at the\nResearch Institute for Symbolic Computation, Johannes Kepler University Linz,\nAustria, c) SSFG (Software Science Foundation Group) at Kyoto University,\nJapan, and d) Sup'Com (Higher School of Communication of Tunis) at the\nUniversity of Carthage, Tunisia.",
        "date": "2013-07-30T16:01:33+00:00",
        "label": 0
    },
    "1401.2580": {
        "title": "A Proof of Kamp's theorem",
        "abstract": "We provide a simple proof of Kamp's theorem.",
        "date": "2014-01-12T00:05:22+00:00",
        "label": 0
    },
    "2010.07017": {
        "title": "Computational Skills by Stealth in Secondary School Data Science",
        "abstract": "The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.",
        "date": "2020-10-08T09:11:51+00:00",
        "label": 0
    },
    "2406.14026": {
        "title": "Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations",
        "abstract": "Language models (LMs) are known to suffer from forgetting of previously\nlearned examples when fine-tuned, breaking stability of deployed LM systems.\nDespite efforts on mitigating forgetting, few have investigated whether, and\nhow forgotten upstream examples are associated with newly learned tasks.\nInsights on such associations enable efficient and targeted mitigation of\nforgetting. In this paper, we empirically analyze forgetting that occurs in $N$\nupstream examples while the model learns $M$ new tasks and visualize their\nassociations with a $M \\times N$ matrix. We empirically demonstrate that the\ndegree of forgetting can often be approximated by simple multiplicative\ncontributions of the upstream examples and newly learned tasks. We also reveal\nmore complicated patterns where specific subsets of examples are forgotten with\nstatistics and visualization. Following our analysis, we predict forgetting\nthat happens on upstream examples when learning a new task with matrix\ncompletion over the empirical associations, outperforming prior approaches that\nrely on trainable LMs. Project website:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
        "date": "2024-06-20T06:46:23+00:00",
        "label": 1
    },
    "2311.09783": {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "abstract": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "date": "2023-11-16T11:03:04+00:00",
        "label": 1
    },
    "2202.01291": {
        "title": "Computer sciences and synthesis: retrospective and perspective",
        "abstract": "The problem of synthesis in computer sciences, including cybernetics,\nartificial intelligence and system analysis, is analyzed. Main methods of\nrealization this problem are discussed. Ways of search universal method of\ncreation universal synthetic science are represented. As example of such\nuniversal method polymetric analysis is given. Perspective of further\ndevelopment of this research, including application polymetric method for the\nresolution main problems of computer sciences, is analyzed too.",
        "date": "2022-01-26T04:42:45+00:00",
        "label": 0
    },
    "2307.16650": {
        "title": "ChatGPT for Teaching and Learning: An Experience from Data Science Education",
        "abstract": "ChatGPT, an implementation and application of large language models, has\ngained significant popularity since its initial release. Researchers have been\nexploring ways to harness the practical benefits of ChatGPT in real-world\nscenarios. Educational researchers have investigated its potential in various\nsubjects, e.g., programming, mathematics, finance, clinical decision support,\netc. However, there has been limited attention given to its application in data\nscience education. This paper aims to bridge that gap by utilizing ChatGPT in a\ndata science course, gathering perspectives from students, and presenting our\nexperiences and feedback on using ChatGPT for teaching and learning in data\nscience education. The findings not only distinguish data science education\nfrom other disciplines but also uncover new opportunities and challenges\nassociated with incorporating ChatGPT into the data science curriculum.",
        "date": "2023-07-31T13:31:19+00:00",
        "label": 0
    },
    "0609110": {
        "title": "Algebraic recognizability of languages",
        "abstract": "Recognizable languages of finite words are part of every computer science\ncursus, and they are routinely described as a cornerstone for applications and\nfor theory. We would like to briefly explore why that is, and how this\nword-related notion extends to more complex models, such as those developed for\nmodeling distributed or timed behaviors.",
        "date": "2006-09-19T15:21:08+00:00",
        "label": 0
    },
    "2008.03964": {
        "title": "DQI: A Guide to Benchmark Evaluation",
        "abstract": "A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.",
        "date": "2020-08-10T08:38:55+00:00",
        "label": 1
    },
    "2309.16609": {
        "title": "Qwen Technical Report",
        "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, enabling natural language processing tasks that were previously\nthought to be exclusive to humans. In this work, we introduce Qwen, the first\ninstallment of our large language model series. Qwen is a comprehensive\nlanguage model series that encompasses distinct models with varying parameter\ncounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,\nthe chat models finetuned with human alignment techniques. The base language\nmodels consistently demonstrate superior performance across a multitude of\ndownstream tasks, and the chat models, particularly those trained using\nReinforcement Learning from Human Feedback (RLHF), are highly competitive. The\nchat models possess advanced tool-use and planning capabilities for creating\nagent applications, showcasing impressive performance even when compared to\nbigger models on complex tasks like utilizing a code interpreter. Furthermore,\nwe have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as\nwell as mathematics-focused models, Math-Qwen-Chat, which are built upon base\nlanguage models. These models demonstrate significantly improved performance in\ncomparison with open-source models, and slightly fall behind the proprietary\nmodels.",
        "date": "2023-09-28T17:07:49+00:00",
        "label": 1
    },
    "2109.07958": {
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "abstract": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.",
        "date": "2021-09-08T17:15:27+00:00",
        "label": 1
    },
    "1404.5458": {
        "title": "Complex Workflow Management and Integration of Distributed Computing Resources by Science Gateway Portal for Molecular Dynamics Simulations in Materials Science",
        "abstract": "The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\nworkflow management and integration of distributed computing resources (like\nclusters, service grids, desktop grids, clouds) is presented. It is created on\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\nscience workflow operation and gUSE - for smooth integration of available\nresources for parallel and distributed computing in various heterogeneous\ndistributed computing infrastructures (DCI). The typical scientific workflow\nwith possible scenarios of its preparation and usage is considered. Several\ntypical science applications (scientific workflows) are considered for\nmolecular dynamics (MD) simulations of complex behavior of various\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\nadvantages and drawbacks of the solution are shortly analyzed in the context of\nits practical applications for MD simulations in materials science, physics and\nnanotechnologies with available heterogeneous DCIs.",
        "date": "2014-04-22T11:34:04+00:00",
        "label": 0
    },
    "2012.12144": {
        "title": "Integrating computing in the statistics and data science curriculum: Creative structures, novel skills and habits, and ways to teach computational thinking",
        "abstract": "Nolan and Temple Lang (2010) argued for the fundamental role of computing in\nthe statistics curriculum. In the intervening decade the statistics education\ncommunity has acknowledged that computational skills are as important to\nstatistics and data science practice as mathematics. There remains a notable\ngap, however, between our intentions and our actions. In this special issue of\nthe *Journal of Statistics and Data Science Education* we have assembled a\ncollection of papers that (1) suggest creative structures to integrate\ncomputing, (2) describe novel data science skills and habits, and (3) propose\nways to teach computational thinking. We believe that it is critical for the\ncommunity to redouble our efforts to embrace sophisticated computing in the\nstatistics and data science curriculum. We hope that these papers provide\nuseful guidance for the community to move these efforts forward.",
        "date": "2020-12-22T16:28:18+00:00",
        "label": 0
    },
    "2402.17400": {
        "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
        "abstract": "This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.",
        "date": "2024-02-27T10:47:24+00:00",
        "label": 1
    },
    "1711.00201": {
        "title": "Credimus",
        "abstract": "We believe that economic design and computational complexity---while already\nimportant to each other---should become even more important to each other with\neach passing year. But for that to happen, experts in on the one hand such\nareas as social choice, economics, and political science and on the other hand\ncomputational complexity will have to better understand each other's\nworldviews.\n  This article, written by two complexity theorists who also work in\ncomputational social choice theory, focuses on one direction of that process by\npresenting a brief overview of how most computational complexity theorists view\nthe world. Although our immediate motivation is to make the lens through which\ncomplexity theorists see the world be better understood by those in the social\nsciences, we also feel that even within computer science it is very important\nfor nontheoreticians to understand how theoreticians think, just as it is\nequally important within computer science for theoreticians to understand how\nnontheoreticians think.",
        "date": "2017-11-01T04:23:01+00:00",
        "label": 0
    },
    "2403.10470": {
        "title": "MADAS -- A Python framework for assessing similarity in materials-science data",
        "abstract": "Computational materials science produces large quantities of data, both in\nterms of high-throughput calculations and individual studies. Extracting\nknowledge from this large and heterogeneous pool of data is challenging due to\nthe wide variety of computational methods and approximations, resulting in\nsignificant veracity in the sheer amount of available data. Here, we present\nMADAS, a Python framework for computing similarity relations between material\nproperties. It can be used to automate the download of data from various\nsources, compute descriptors and similarities between materials, analyze the\nrelationship between materials through their properties, and can incorporate a\nvariety of existing machine learning methods. We explain the design of the\npackage and demonstrate its power with representative examples.",
        "date": "2024-03-15T17:02:28+00:00",
        "label": 0
    },
    "2402.17327": {
        "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
        "abstract": "We study the data selection problem, whose aim is to select a small\nrepresentative subset of data that can be used to efficiently train a machine\nlearning model. We present a new data selection approach based on $k$-means\nclustering and sensitivity sampling. Assuming access to an embedding\nrepresentation of the data with respect to which the model loss is H\\\"older\ncontinuous, our approach provably allows selecting a set of ``typical'' $k +\n1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of\nthe whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an\nadditive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means\ncost for the input embeddings and $\\lambda$ is the H\\\"older constant.\n  We furthermore demonstrate the performance and scalability of our approach on\nfine-tuning foundation models and show that it outperforms state-of-the-art\nmethods. We also show how it can be applied on linear regression, leading to a\nnew sampling strategy that surprisingly matches the performances of leverage\nscore sampling, while being conceptually simpler and more scalable.",
        "date": "2024-02-27T09:03:43+00:00",
        "label": 1
    },
    "2407.14985": {
        "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
        "abstract": "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
        "date": "2024-07-20T21:24:40+00:00",
        "label": 1
    },
    "1608.08918": {
        "title": "Subcomputable Schnorr Randomness",
        "abstract": "The notion of Schnorr randomness refers to computable reals or computable\nfunctions. We propose a version of Schnorr randomness for subcomputable classes\nand characterize it in different ways: by Martin L\\\"of tests, martingales or\nmeasure computable machines.",
        "date": "2016-08-31T16:04:37+00:00",
        "label": 0
    },
    "2207.01590": {
        "title": "The Dichotomy of Cloud and IoT: Cloud-Assisted IoT From a Security Perspective",
        "abstract": "In recent years, the existence of a significant cross-impact between Cloud\ncomputing and Internet of Things (IoT) has lead to a dichotomy that gives raise\nto Cloud-Assisted IoT (CAIoT) and IoT-Based Cloud (IoTBC). Although it is\npertinent to study both technologies, this paper focuses on CAIoT, and\nespecially its security issues, which are inherited from both Cloud computing\nand IoT. This study starts with reviewing existing relevant surveys, noting\ntheir shortcomings, which motivate a comprehensive survey in this area. We\nproceed to highlight existing approaches towards the design of Secure CAIoT\n(SCAIoT) along with related security challenges and controls. We develop a\nlayered architecture for SCAIoT. Furthermore, we take a look at what the future\nmay hold for SCAIoT with a focus on the role of Artificial Intelligence(AI).",
        "date": "2022-07-04T17:19:56+00:00",
        "label": 0
    },
    "1304.7858": {
        "title": "Abstract Stobjs and Their Application to ISA Modeling",
        "abstract": "We introduce a new ACL2 feature, the abstract stobj, and show how to apply it\nto modeling the instruction set architecture of a microprocessor. Benefits of\nabstract stobjs over traditional (\"concrete\") stobjs can include faster\nexecution, support for symbolic simulation, more efficient reasoning, and\nresilience of proof developments under modeling optimization.",
        "date": "2013-04-30T04:14:22+00:00",
        "label": 0
    },
    "1401.4973": {
        "title": "What are the fundamental structures of concurrency? We still don't know!",
        "abstract": "Process algebra has been successful in many ways; but we don't yet see the\nlineaments of a fundamental theory. Some fleeting glimpses are sought from\nPetri Nets, physics and geometry.",
        "date": "2014-01-20T16:35:23+00:00",
        "label": 0
    },
    "2402.09739": {
        "title": "QuRating: Selecting High-Quality Data for Training Language Models",
        "abstract": "Selecting high-quality pre-training data is important for creating capable\nlanguage models, but existing methods rely on simple heuristics. We introduce\nQuRating, a method for selecting pre-training data that can capture human\nintuitions about data quality. In this paper, we investigate four qualities -\nwriting style, required expertise, facts & trivia, and educational value - and\nfind that LLMs are able to discern these qualities, especially when making\npairwise judgments of texts. We train a QuRater model to learn scalar ratings\nfrom pairwise judgments, and use it to annotate a 260B training corpus with\nquality ratings for each of the four criteria. In our experiments, we select\n30B tokens according to the different quality ratings and train 1.3B-parameter\nlanguage models on the selected data. We find that it is important to balance\nquality and diversity. When we sample using quality ratings as logits over\ndocuments, our models obtain lower perplexity and stronger in-context learning\nperformance than baselines. Our best model is based on educational value and\nperforms similarly to a model trained with uniform sampling for 50% more steps.\nBeyond data selection, we use the quality ratings to construct a training\ncurriculum which improves performance without changing the training dataset. We\nextensively analyze the quality ratings and discuss their characteristics,\nbiases, and wider implications.",
        "date": "2024-02-15T06:36:07+00:00",
        "label": 1
    },
    "9904050": {
        "title": "A Computer Scientist's View of Life, the Universe, and Everything",
        "abstract": "Is the universe computable? If so, it may be much cheaper in terms of\ninformation requirements to compute all computable universes instead of just\nours. I apply basic concepts of Kolmogorov complexity theory to the set of\npossible universes, and chat about perceived and true randomness, life,\ngeneralization, and learning in a given universe.",
        "date": "1999-04-13T13:36:03+00:00",
        "label": 0
    },
    "2403.09606": {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "date": "2024-03-14T17:47:20+00:00",
        "label": 1
    },
    "2109.01652": {
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "abstract": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.",
        "date": "2021-09-03T17:55:52+00:00",
        "label": 1
    },
    "2310.13032": {
        "title": "Quality-Diversity through AI Feedback",
        "abstract": "In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.",
        "date": "2023-10-19T12:13:58+00:00",
        "label": 1
    },
    "1906.05340": {
        "title": "The Halting Paradox",
        "abstract": "The halting problem is considered to be an essential part of the theoretical\nbackground to computing. That halting is not in general computable has\nsupposedly been proved in many text books and taught on many computer science\ncourses, in order to illustrate the limits of computation. However, Eric Hehner\nhas a dissenting view, in which the specification of the halting problem is\ncalled into question.",
        "date": "2019-06-11T09:47:19+00:00",
        "label": 0
    },
    "1612.04037": {
        "title": "Proceedings 11th Doctoral Workshop on Mathematical and Engineering Methods in Computer Science",
        "abstract": "MEMICS provides a forum for doctoral students interested in applications of\nmathematical and engineering methods in computer science. Besides a rich\ntechnical programme (including invited talks, regular papers, and\npresentations), MEMICS also offers friendly social activities and exciting\nopportunities for meeting like-minded people. MEMICS submissions traditionally\ncover all areas of computer science (such as parallel and distributed\ncomputing, computer networks, modern hardware and its design, non-traditional\ncomputing architectures, information systems and databases, multimedia and\ngraphics, verification and testing, computer security, as well as all related\nareas of theoretical computer science).",
        "date": "2016-12-13T05:47:19+00:00",
        "label": 0
    },
    "2405.20541": {
        "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
        "abstract": "In this work, we investigate whether small language models can determine\nhigh-quality subsets of large-scale text datasets that improve the performance\nof larger language models. While existing work has shown that pruning based on\nthe perplexity of a larger model can yield high-quality data, we investigate\nwhether smaller models can be used for perplexity-based pruning and how pruning\nis affected by the domain composition of the data being pruned. We demonstrate\nthat for multiple dataset compositions, perplexity-based pruning of pretraining\ndata can \\emph{significantly} improve downstream task performance: pruning\nbased on perplexities computed with a 125 million parameter model improves the\naverage performance on downstream tasks of a 3 billion parameter model by up to\n2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach\ncommensurate baseline performance. Furthermore, we demonstrate that such\nperplexity-based data pruning also yields downstream performance gains in the\nover-trained and data-constrained regimes.",
        "date": "2024-05-30T23:50:20+00:00",
        "label": 1
    },
    "2306.01116": {
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "abstract": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",
        "date": "2023-06-01T20:03:56+00:00",
        "label": 1
    },
    "2406.14115": {
        "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
        "abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a\nhigh-quality subset from a given candidate dataset to train a Pending Fine-tune\nModel (PFM) into a Selective-Enhanced Model (SEM). It can improve the model\nperformance and accelerate the training process. Although a few surveys have\ninvestigated related works of data selection, there is a lack of comprehensive\ncomparison between existing methods due to their various experimental settings.\nTo address this issue, we first propose a three-stage scheme for data selection\nand comprehensively review existing works according to this scheme. Then, we\ndesign a unified comparing method with ratio-based efficiency indicators and\nranking-based feasibility indicators to overcome the difficulty of comparing\nvarious models with diverse experimental settings. After an in-depth\ncomparative analysis, we find that the more targeted method with data-specific\nand model-specific quality labels has higher efficiency, but the introduction\nof additional noise information should be avoided when designing selection\nalgorithms. Finally, we summarize the trends in data selection and highlight\nthe short-term and long-term challenges to guide future research.",
        "date": "2024-06-20T08:58:58+00:00",
        "label": 1
    },
    "1304.7394": {
        "title": "A Static Analysis Framework for Livelock Freedom in CSP",
        "abstract": "In a process algebra with hiding and recursion it is possible to create\nprocesses which compute internally without ever communicating with their\nenvironment. Such processes are said to diverge or livelock. In this paper we\nshow how it is possible to conservatively classify processes as livelock-free\nthrough a static analysis of their syntax. In particular, we present a\ncollection of rules, based on the inductive structure of terms, which guarantee\nlivelock-freedom of the denoted process. This gives rise to an algorithm which\nconservatively flags processes that can potentially livelock. We illustrate our\napproach by applying both BDD-based and SAT-based implementations of our\nalgorithm to a range of benchmarks, and show that our technique in general\nsubstantially outperforms the model checker FDR whilst exhibiting a low rate of\ninconclusive results.",
        "date": "2013-04-27T18:10:36+00:00",
        "label": 0
    },
    "2303.10158": {
        "title": "Data-centric Artificial Intelligence: A Survey",
        "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every\ndomain. A vital enabler of its great success is the availability of abundant\nand high-quality data for building machine learning models. Recently, the role\nof data in AI has been significantly magnified, giving rise to the emerging\nconcept of data-centric AI. The attention of researchers and practitioners has\ngradually shifted from advancing model design to enhancing the quality and\nquantity of the data. In this survey, we discuss the necessity of data-centric\nAI, followed by a holistic view of three general data-centric goals (training\ndata development, inference data development, and data maintenance) and the\nrepresentative methods. We also organize the existing literature from\nautomation and collaboration perspectives, discuss the challenges, and tabulate\nthe benchmarks for various tasks. We believe this is the first comprehensive\nsurvey that provides a global view of a spectrum of tasks across various stages\nof the data lifecycle. We hope it can help the readers efficiently grasp a\nbroad picture of this field, and equip them with the techniques and further\nresearch ideas to systematically engineer data for building AI systems. A\ncompanion list of data-centric AI resources will be regularly updated on\nhttps://github.com/daochenzha/data-centric-AI",
        "date": "2023-03-17T17:44:56+00:00",
        "label": 1
    },
    "2403.00526": {
        "title": "Data Quality Assessment: Challenges and Opportunities",
        "abstract": "Data-oriented applications, their users, and even the law require data of\nhigh quality. Research has broken down the rather vague notion of data quality\ninto various dimensions, such as accuracy, consistency, and reputation, to name\nbut a few. To achieve the goal of high data quality, many tools and techniques\nexist to clean and otherwise improve data. Yet, systematic research on actually\nassessing data quality in all of its dimensions is largely absent, and with it\nthe ability to gauge the success of any data cleaning effort. It is our vision\nto establish a systematic and comprehensive framework for the (numeric)\nassessment of data quality for a given dataset and its intended use. Such a\nframework must cover the various facets that influence data quality, as well as\nthe many types of data quality dimensions. In particular, we identify five\nfacets that serve as a foundation of data quality assessment. For each facet,\nwe outline the challenges and opportunities that arise when trying to actually\nassign quality scores to data and create a data quality profile for it, along\nwith a wide range of technologies needed for this purpose.",
        "date": "2024-03-01T13:35:15+00:00",
        "label": 1
    },
    "1310.7911": {
        "title": "Compact manifolds with computable boundaries",
        "abstract": "We investigate conditions under which a co-computably enumerable closed set\nin a computable metric space is computable and prove that in each locally\ncomputable computable metric space each co-computably enumerable compact\nmanifold with computable boundary is computable. In fact, we examine the notion\nof a semi-computable compact set and we prove a more general result: in any\ncomputable metric space each semi-computable compact manifold with computable\nboundary is computable. In particular, each semi-computable compact\n(boundaryless) manifold is computable.",
        "date": "2013-10-29T18:29:13+00:00",
        "label": 0
    },
    "2107.06499": {
        "title": "Deduplicating Training Data Makes Language Models Better",
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "date": "2021-07-14T06:06:52+00:00",
        "label": 1
    },
    "1003.1930": {
        "title": "Simulating Grover's Quantum Search in a Classical Computer",
        "abstract": "The rapid progress of computer science has been accompanied by a\ncorresponding evolution of computation, from classical computation to quantum\ncomputation. As quantum computing is on its way to becoming an established\ndiscipline of computing science, much effort is being put into the development\nof new quantum algorithms. One of quantum algorithms is Grover algorithm, which\nis used for searching an element in an unstructured list of N elements with\nquadratic speed-up over classical algorithms. In this work, Quantum Computer\nLanguage (QCL) is used to make a Grover's quantum search simulation in a\nclassical computer",
        "date": "2010-03-09T17:02:21+00:00",
        "label": 0
    },
    "2210.13526": {
        "title": "Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations",
        "abstract": "Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.",
        "date": "2022-10-24T18:27:27+00:00",
        "label": 0
    },
    "2308.04896": {
        "title": "Why Data Science Projects Fail",
        "abstract": "Data Science is a modern Data Intelligence practice, which is the core of\nmany businesses and helps businesses build smart strategies around to deal with\nbusinesses challenges more efficiently. Data Science practice also helps in\nautomating business processes using the algorithm, and it has several other\nbenefits, which also deliver in a non-profitable framework. In regards to data\nscience, three key components primarily influence the effective outcome of a\ndata science project. Those are 1.Availability of Data 2.Algorithm 3.Processing\npower or infrastructure",
        "date": "2023-08-08T06:45:15+00:00",
        "label": 0
    },
    "2310.10873": {
        "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/.",
        "date": "2023-10-16T22:53:54+00:00",
        "label": 1
    },
    "2401.08565": {
        "title": "Tuning Language Models by Proxy",
        "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.",
        "date": "2024-01-16T18:49:55+00:00",
        "label": 1
    },
    "2310.06825": {
        "title": "Mistral 7B",
        "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.",
        "date": "2023-10-10T17:54:58+00:00",
        "label": 1
    },
    "2401.17197": {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently\ngarnered considerable attention, where fine-tuning plays a key role in LLMs'\nadaptation. However, the cost of fine-tuning LLMs on rapidly expanding\nrecommendation data limits their practical application. To address this\nchallenge, few-shot fine-tuning offers a promising approach to quickly adapt\nLLMs to new recommendation data. We propose the task of data pruning for\nefficient LLM-based recommendation, aimed at identifying representative samples\ntailored for LLMs' few-shot fine-tuning. While coreset selection is closely\nrelated to the proposed task, existing coreset selection methods often rely on\nsuboptimal heuristic metrics or entail costly optimization on large-scale\nrecommendation data.\n  To tackle these issues, we introduce two objectives for the data pruning task\nin the context of LLM-based recommendation: 1) high accuracy aims to identify\nthe influential samples that can lead to high overall performance; and 2) high\nefficiency underlines the low costs of the data pruning process. To pursue the\ntwo objectives, we propose a novel data pruning method based on two scores,\ni.e., influence score and effort score, to efficiently identify the influential\nsamples. Particularly, the influence score is introduced to accurately estimate\nthe influence of sample removal on the overall performance. To achieve low\ncosts of the data pruning process, we use a small-sized surrogate model to\nreplace LLMs to obtain the influence score. Considering the potential gap\nbetween the surrogate model and LLMs, we further propose an effort score to\nprioritize some hard samples specifically for LLMs. Empirical results on three\nreal-world datasets validate the effectiveness of our proposed method. In\nparticular, the proposed method uses only 2% samples to surpass the full data\nfine-tuning, reducing time costs by 97%.",
        "date": "2024-01-30T17:31:19+00:00",
        "label": 1
    },
    "2402.13064": {
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "date": "2024-02-20T15:00:35+00:00",
        "label": 1
    },
    "1806.03884": {
        "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis",
        "abstract": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
        "date": "2018-06-11T09:44:23+00:00",
        "label": 1
    },
    "2406.06046": {
        "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
        "abstract": "Pretraining data selection has the potential to improve language model\npretraining efficiency by utilizing higher-quality data from massive web data\ncorpora. Current data selection methods, which rely on either hand-crafted\nrules or larger reference models, are conducted statically and do not capture\nthe evolving data preferences during pretraining. In this paper, we introduce\nmodel-aware data selection with data influence models (MATES), where a data\ninfluence model continuously adapts to the evolving data preferences of the\npretraining model and then selects the data most effective for the current\npretraining progress. Specifically, we fine-tune a small data influence model\nto approximate oracle data preference signals collected by locally probing the\npretraining model and to select data accordingly for the next pretraining\nstage. Experiments on Pythia and the C4 dataset demonstrate that MATES\nsignificantly outperforms random data selection on extensive downstream tasks\nin both zero- and few-shot settings. It doubles the gains achieved by recent\ndata selection approaches that leverage larger reference models and reduces the\ntotal FLOPs required to reach certain performances by half. Further analysis\nvalidates the ever-changing data preferences of pretraining models and the\neffectiveness of our data influence models to capture them. Our code is\nopen-sourced at https://github.com/cxcscmu/MATES.",
        "date": "2024-06-10T06:27:42+00:00",
        "label": 1
    },
    "2403.02839": {
        "title": "On the Limitations of Fine-tuned Judge Models for LLM Evaluation",
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this study, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\npropose an effective indicator to measure the reliability of fine-tuned judges,\nwith the aim of maximizing their utility in LLM evaluation.",
        "date": "2024-03-05T10:20:52+00:00",
        "label": 1
    },
    "2308.06259": {
        "title": "Self-Alignment with Instruction Backtranslation",
        "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.",
        "date": "2023-08-11T17:47:54+00:00",
        "label": 1
    },
    "1606.01148": {
        "title": "Tripartite Unions",
        "abstract": "This note provides conditions under which the union of three well-founded\nbinary relations is also well-founded.",
        "date": "2016-06-03T15:41:55+00:00",
        "label": 0
    },
    "2306.13840": {
        "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
        "abstract": "Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.",
        "date": "2023-06-24T02:25:56+00:00",
        "label": 1
    },
    "2208.01545": {
        "title": "The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence",
        "abstract": "Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by 1. proposing a novel metric\n-- the diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and\ntransfer learning under fair conditions (same architecture, same optimizer, and\nall models trained to convergence). Using the diversity coefficient, we show\nthat the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have\nlow diversity. This novel insight contextualizes claims that transfer learning\nsolutions are better than meta-learned solutions in the regime of low diversity\nunder a fair comparison. Specifically, we empirically find that a low diversity\ncoefficient correlates with a high similarity between transfer learning and\nMAML learned solutions in terms of accuracy at meta-test time and\nclassification layer similarity (using feature based distance metrics like\nSVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this\nmeta-test accuracy holds even as the model size changes. Therefore, we conclude\nthat in the low diversity regime, MAML and transfer learning have equivalent\nmeta-test performance when both are compared fairly. We also hope our work\ninspires more thoughtful constructions and quantitative evaluations of\nmeta-learning benchmarks in the future.",
        "date": "2022-08-02T15:49:11+00:00",
        "label": 1
    },
    "2310.06786": {
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
        "abstract": "There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.",
        "date": "2023-10-10T16:57:28+00:00",
        "label": 1
    },
    "1807.03750": {
        "title": "Navigating Diverse Data Science Learning: Critical Reflections Towards Future Practice",
        "abstract": "Data Science is currently a popular field of science attracting expertise\nfrom very diverse backgrounds. Current learning practices need to acknowledge\nthis and adapt to it. This paper summarises some experiences relating to such\nlearning approaches from teaching a postgraduate Data Science module, and draws\nsome learned lessons that are of relevance to others teaching Data Science.",
        "date": "2018-07-05T21:32:18+00:00",
        "label": 0
    },
    "2312.06254": {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "label": 1
    },
    "2402.18041": {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "date": "2024-02-28T04:35:51+00:00",
        "label": 1
    },
    "1910.03771": {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "abstract": "Recent progress in natural language processing has been driven by advances in\nboth model architecture and model pretraining. Transformer architectures have\nfacilitated building higher-capacity models and pretraining has made it\npossible to effectively utilize this capacity for a wide variety of tasks.\n\\textit{Transformers} is an open-source library with the goal of opening up\nthese advances to the wider machine learning community. The library consists of\ncarefully engineered state-of-the art Transformer architectures under a unified\nAPI. Backing this library is a curated collection of pretrained models made by\nand available for the community. \\textit{Transformers} is designed to be\nextensible by researchers, simple for practitioners, and fast and robust in\nindustrial deployments. The library is available at\n\\url{https://github.com/huggingface/transformers}.",
        "date": "2019-10-09T03:23:22+00:00",
        "label": 1
    },
    "0701087": {
        "title": "Artificiality in Social Sciences",
        "abstract": "This text provides with an introduction to the modern approach of\nartificiality and simulation in social sciences. It presents the relationship\nbetween complexity and artificiality, before introducing the field of\nartificial societies which greatly benefited from the computer power fast\nincrease, gifting social sciences with formalization and experimentation tools\npreviously owned by \"hard\" sciences alone. It shows that as \"a new way of doing\nsocial sciences\", artificial societies should undoubtedly contribute to a\nrenewed approach in the study of sociality and should play a significant part\nin the elaboration of original theories of social phenomena.",
        "date": "2007-01-13T16:50:37+00:00",
        "label": 0
    },
    "1805.05401": {
        "title": "Building Data Science Capabilities into University Data Warehouse to Predict Graduation",
        "abstract": "The discipline of data science emerged to combine statistical methods with\ncomputing. At Aalto University, Finland, we have taken first steps to bring\neducational data science as a part of daily operations of Management\nInformation Services. This required changes in IT environment: we enhanced data\nwarehouse infrastructure with a data science lab, where we can read predictive\nmodel training data from data warehouse database and use the created predictive\nmodels in database queries. We then conducted a data science pilot with an\nobjective to predict students' graduation probability and time-to-degree with\nstudent registry data. Further ethical and legal considerations are needed\nbefore using predictions in daily operations of the university.",
        "date": "2018-05-04T12:28:03+00:00",
        "label": 0
    },
    "2407.12874": {
        "title": "SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning",
        "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.",
        "date": "2024-07-16T04:41:58+00:00",
        "label": 1
    },
    "1106.2769": {
        "title": "Co-c.e. spheres and cells in computable metric spaces",
        "abstract": "We investigate conditions under which a co-computably enumerable set in a\ncomputable metric space is computable. Using higher-dimensional chains and\nspherical chains we prove that in each computable metric space which is locally\ncomputable each co-computably enumerable sphere is computable and each co-c.e.\ncell with co-c.e. boundary sphere is computable.",
        "date": "2011-06-14T17:46:06+00:00",
        "label": 0
    },
    "2311.14736": {
        "title": "Data Diversity Matters for Robust Instruction Tuning",
        "abstract": "Recent works have shown that by curating high quality and diverse instruction\ntuning datasets, we can significantly improve instruction-following\ncapabilities. However, creating such datasets is difficult and most works rely\non manual curation or proprietary language models. Automatic data curation is\ndifficult as it is still not clear how we can define diversity for instruction\ntuning, how diversity and quality depend on one other, and how we can optimize\ndataset quality and diversity. To resolve these issue, we propose a new\nalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple\nmethod to simultaneously control dataset diversity and quality, allowing us to\nconduct an in-depth study on the effect of diversity and quality on instruction\ntuning performance. From this study we draw two key insights (1) there is a\nnatural tradeoff between data diversity and quality and (2) increasing data\ndiversity significantly improves the worst case instruction following\nperformance, therefore improving robustness. We validate the performance of\nQDIT on several large scale instruction tuning datasets, where we find it can\nsubstantially improve worst and average case performance compared to\nquality-driven data selection.",
        "date": "2023-11-21T19:12:18+00:00",
        "label": 1
    },
    "2311.09006": {
        "title": "Data Similarity is Not Enough to Explain Language Model Performance",
        "abstract": "Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.",
        "date": "2023-11-15T14:48:08+00:00",
        "label": 1
    },
    "0912.4188": {
        "title": "The skewness of computer science",
        "abstract": "Computer science is a relatively young discipline combining science,\nengineering, and mathematics. The main flavors of computer science research\ninvolve the theoretical development of conceptual models for the different\naspects of computing and the more applicative building of software artifacts\nand assessment of their properties. In the computer science publication\nculture, conferences are an important vehicle to quickly move ideas, and\njournals often publish deeper versions of papers already presented at\nconferences. These peculiarities of the discipline make computer science an\noriginal research field within the sciences, and, therefore, the assessment of\nclassical bibliometric laws is particularly important for this field. In this\npaper, we study the skewness of the distribution of citations to papers\npublished in computer science publication venues (journals and conferences). We\nfind that the skewness in the distribution of mean citedness of different\nvenues combines with the asymmetry in citedness of articles in each venue,\nresulting in a highly asymmetric citation distribution with a power law tail.\nFurthermore, the skewness of conference publications is more pronounced than\nthe asymmetry of journal papers. Finally, the impact of journal papers, as\nmeasured with bibliometric indicators, largely dominates that of proceeding\npapers.",
        "date": "2009-12-21T15:24:08+00:00",
        "label": 0
    },
    "2405.06331": {
        "title": "LMD3: Language Model Data Density Dependence",
        "abstract": "We develop a methodology for analyzing language model task performance at the\nindividual example level based on training data density estimation. Experiments\nwith paraphrasing as a controlled intervention on finetuning data demonstrate\nthat increasing the support in the training distribution for specific test\nqueries results in a measurable increase in density, which is also a\nsignificant predictor of the performance increase caused by the intervention.\nExperiments with pretraining data demonstrate that we can explain a significant\nfraction of the variance in model perplexity via density measurements. We\nconclude that our framework can provide statistical evidence of the dependence\nof a target model's predictions on subsets of its training data, and can more\ngenerally be used to characterize the support (or lack thereof) in the training\ndata for a given test task.",
        "date": "2024-05-10T09:03:27+00:00",
        "label": 1
    },
    "1309.0717": {
        "title": "A Polynomial Translation of pi-calculus FCPs to Safe Petri Nets",
        "abstract": "We develop a polynomial translation from finite control pi-calculus processes\nto safe low-level Petri nets. To our knowledge, this is the first such\ntranslation. It is natural in that there is a close correspondence between the\ncontrol flows, enjoys a bisimulation result, and is suitable for practical\nmodel checking.",
        "date": "2013-09-03T15:08:39+00:00",
        "label": 0
    },
    "2201.05852": {
        "title": "Data Science in Perspective",
        "abstract": "Data and Science has stood out in the generation of results, whether in the\nprojects of the scientific domain or business domain. CERN Project, Scientific\nInstitutes, companies like Walmart, Google, Apple, among others, need data to\npresent their results and make predictions in the competitive data world. Data\nand Science are words that together culminated in a globally recognized term\ncalled Data Science. Data Science is in its initial phase, possibly being part\nof formal sciences and also being presented as part of applied sciences,\ncapable of generating value and supporting decision making. Data Science\nconsiders science and, consequently, the scientific method to promote decision\nmaking through data intelligence. In many cases, the application of the method\n(or part of it) is considered in Data Science projects in scientific domain\n(social sciences, bioinformatics, geospatial projects) or business domain\n(finance, logistic, retail), among others. In this sense, this article\naddresses the perspectives of Data Science as a multidisciplinary area,\nconsidering science and the scientific method, and its formal structure which\nintegrate Statistics, Computer Science, and Business Science, also taking into\naccount Artificial Intelligence, emphasizing Machine Learning, among others.\nThe article also deals with the perspective of applied Data Science, since Data\nScience is used for generating value through scientific and business projects.\nData Science persona is also discussed in the article, concerning the education\nof Data Science professionals and its corresponding profiles, since its\nprojection changes the field of data in the world.",
        "date": "2022-01-15T13:51:12+00:00",
        "label": 0
    },
    "2402.05119": {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.",
        "date": "2024-02-03T04:45:25+00:00",
        "label": 1
    },
    "2406.14491": {
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
        "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
        "date": "2024-06-20T16:55:33+00:00",
        "label": 1
    },
    "2305.06161": {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.",
        "date": "2023-05-09T08:16:42+00:00",
        "label": 1
    },
    "2406.19976": {
        "title": "ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting",
        "abstract": "Bilevel optimization has shown its utility across various machine learning\nsettings, yet most algorithms in practice require second-order information,\nmaking it challenging to scale them up. Only recently, a paradigm of\nfirst-order algorithms emerged, capable of effectively addressing bilevel\noptimization problems. Nevertheless, the practical efficiency of this paradigm\nremains unverified, particularly in the context of large language models\n(LLMs). This paper introduces the first scalable instantiation of this paradigm\ncalled ScaleBiO, focusing on bilevel optimization for large-scale LLM data\nreweighting. By combining with a recently proposed memory-efficient training\ntechnique called LISA, our novel algorithm allows the paradigm to scale to\n34-billion-parameter LLMs on eight A40 GPUs, marking the first successful\napplication of bilevel optimization under practical scenarios for large-sized\nLLMs. Empirically, extensive experiments on data reweighting verify the\neffectiveness of ScaleBiO for different-scaled models, including GPT-2,\nLLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in\nfiltering irrelevant data samples and selecting informative samples.\nTheoretically, ScaleBiO ensures the optimality of the learned data weights,\nalong with a convergence guarantee matching the conventional first-order\nbilevel optimization paradigm on smooth and strongly convex objectives.",
        "date": "2024-06-28T15:03:08+00:00",
        "label": 1
    },
    "1009.2416": {
        "title": "Computational Science and Innovation",
        "abstract": "Simulations - utilizing computers to solve complicated science and\nengineering problems - are a key ingredient of modern science. The U.S.\nDepartment of Energy (DOE) is a world leader in the development of\nhigh-performance computing (HPC), the development of applied math and\nalgorithms that utilize the full potential of HPC platforms, and the\napplication of computing to science and engineering problems. An interesting\ngeneral question is whether the DOE can strategically utilize its capability in\nsimulations to advance innovation more broadly. In this article, I will argue\nthat this is certainly possible.",
        "date": "2010-09-13T15:15:07+00:00",
        "label": 0
    },
    "2401.09839": {
        "title": "MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation Extraction for Material Science Knowledge-base Construction",
        "abstract": "Material science literature is a rich source of factual information about\nvarious categories of entities (like materials and compositions) and various\nrelations between these entities, such as conductivity, voltage, etc.\nAutomatically extracting this information to generate a material science\nknowledge base is a challenging task. In this paper, we propose MatSciRE\n(Material Science Relation Extractor), a Pointer Network-based encoder-decoder\nframework, to jointly extract entities and relations from material science\narticles as a triplet ($entity1, relation, entity2$). Specifically, we target\nthe battery materials and identify five relations to work on - conductivity,\ncoulombic efficiency, capacity, voltage, and energy. Our proposed approach\nachieved a much better F1-score (0.771) than a previous attempt using\nChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown\nin Fig 1. The material information is extracted from material science\nliterature in the form of entity-relation triplets using MatSciRE.",
        "date": "2024-01-18T09:54:18+00:00",
        "label": 0
    },
    "0809.1552": {
        "title": "A computer verified, monadic, functional implementation of the integral",
        "abstract": "We provide a computer verified exact monadic functional implementation of the\nRiemann integral in type theory. Together with previous work by O'Connor, this\nmay be seen as the beginning of the realization of Bishop's vision to use\nconstructive mathematics as a programming language for exact analysis.",
        "date": "2008-09-08T20:54:36+00:00",
        "label": 0
    },
    "2105.10446": {
        "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction",
        "abstract": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained \"white-box\"\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at\n\\url{https://github.com/Ma-Lab-Berkeley}.",
        "date": "2021-05-21T16:29:57+00:00",
        "label": 1
    },
    "1211.3476": {
        "title": "Proceedings 6th Workshop on Membrane Computing and Biologically Inspired Process Calculi",
        "abstract": "This volume contains the papers presented at the 6th Membrane Computing and\nBiologically Inspired Process Calculi (MeCBIC 2012), a satellite workshop of\nthe 23rd International Conference on Concurrency Theory (CONCUR) held on 8th\nSeptember 2012 in Newcastle upon Tyne, UK.",
        "date": "2012-11-15T01:33:41+00:00",
        "label": 0
    },
    "1005.5635": {
        "title": "An Effective Extension of the Wagner Hierarchy to Blind Counter Automata",
        "abstract": "The extension of the Wagner hierarchy to blind counter automata accepting\ninfinite words with a Muller acceptance condition is effective. We determine\nprecisely this hierarchy.",
        "date": "2010-05-31T09:19:39+00:00",
        "label": 0
    },
    "2006.14651": {
        "title": "Influence Functions in Deep Learning Are Fragile",
        "abstract": "Influence functions approximate the effect of training samples in test-time\npredictions and have a wide variety of applications in machine learning\ninterpretability and uncertainty estimation. A commonly-used (first-order)\ninfluence function can be implemented efficiently as a post-hoc method\nrequiring access only to the gradients and Hessian of the model. For linear\nmodels, influence functions are well-defined due to the convexity of the\nunderlying loss function and are generally accurate even across difficult\nsettings where model changes are fairly large such as estimating group\ninfluences. Influence functions, however, are not well-understood in the\ncontext of deep learning with non-convex loss functions. In this paper, we\nprovide a comprehensive and large-scale empirical study of successes and\nfailures of influence functions in neural network models trained on datasets\nsuch as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments,\nwe show that the network architecture, its depth and width, as well as the\nextent of model parameterization and regularization techniques have strong\neffects in the accuracy of influence functions. In particular, we find that (i)\ninfluence estimates are fairly accurate for shallow networks, while for deeper\nnetworks the estimates are often erroneous; (ii) for certain network\narchitectures and datasets, training with weight-decay regularization is\nimportant to get high-quality influence estimates; and (iii) the accuracy of\ninfluence estimates can vary significantly depending on the examined test\npoints. These results suggest that in general influence functions in deep\nlearning are fragile and call for developing improved influence estimation\nmethods to mitigate these issues in non-convex setups.",
        "date": "2020-06-25T18:25:59+00:00",
        "label": 1
    },
    "1908.01091": {
        "title": "Toward Understanding Catastrophic Forgetting in Continual Learning",
        "abstract": "We study the relationship between catastrophic forgetting and properties of\ntask sequences. In particular, given a sequence of tasks, we would like to\nunderstand which properties of this sequence influence the error rates of\ncontinual learning algorithms trained on the sequence. To this end, we propose\na new procedure that makes use of recent developments in task space modeling as\nwell as correlation analysis to specify and analyze the properties we are\ninterested in. As an application, we apply our procedure to study two\nproperties of a task sequence: (1) total complexity and (2) sequential\nheterogeneity. We show that error rates are strongly and positively correlated\nto a task sequence's total complexity for some state-of-the-art algorithms. We\nalso show that, surprisingly, the error rates have no or even negative\ncorrelations in some cases to sequential heterogeneity. Our findings suggest\ndirections for improving continual learning benchmarks and methods.",
        "date": "2019-08-02T23:30:35+00:00",
        "label": 1
    },
    "1103.1977": {
        "title": "Development of Computer Science Disciplines - A Social Network Analysis Approach",
        "abstract": "In contrast to many other scientific disciplines, computer science considers\nconference publications. Conferences have the advantage of providing fast\npublication of papers and of bringing researchers together to present and\ndiscuss the paper with peers. Previous work on knowledge mapping focused on the\nmap of all sciences or a particular domain based on ISI published JCR (Journal\nCitation Report). Although this data covers most of important journals, it\nlacks computer science conference and workshop proceedings. That results in an\nimprecise and incomplete analysis of the computer science knowledge. This paper\npresents an analysis on the computer science knowledge network constructed from\nall types of publications, aiming at providing a complete view of computer\nscience research. Based on the combination of two important digital libraries\n(DBLP and CiteSeerX), we study the knowledge network created at\njournal/conference level using citation linkage, to identify the development of\nsub-disciplines. We investigate the collaborative and citation behavior of\njournals/conferences by analyzing the properties of their co-authorship and\ncitation subgraphs. The paper draws several important conclusions. First,\nconferences constitute social structures that shape the computer science\nknowledge. Second, computer science is becoming more interdisciplinary. Third,\nexperts are the key success factor for sustainability of journals/conferences.",
        "date": "2011-03-10T09:51:19+00:00",
        "label": 0
    },
    "2007.01852": {
        "title": "Language-agnostic BERT Sentence Embedding",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "date": "2020-07-03T17:58:42+00:00",
        "label": 1
    },
    "1404.6487": {
        "title": "Computability of 1-manifolds",
        "abstract": "A semi-computable set S in a computable metric space need not be computable.\nHowever, in some cases, if S has certain topological properties, we can\nconclude that S is computable. It is known that if a semi-computable set S is a\ncompact manifold with boundary, then the computability of \\deltaS implies the\ncomputability of S. In this paper we examine the case when S is a 1-manifold\nwith boundary, not necessarily compact. We show that a similar result holds in\nthis case under assumption that S has finitely many components.",
        "date": "2014-04-25T17:37:44+00:00",
        "label": 0
    },
    "2402.05123": {
        "title": "A Survey on Data Selection for LLM Instruction Tuning",
        "abstract": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.",
        "date": "2024-02-04T13:32:01+00:00",
        "label": 1
    },
    "1310.6323": {
        "title": "Logic in the Lab",
        "abstract": "This file summarizes the plenary talk on laboratory experiments on logic at\nthe TARK 2013 - 14th Conference on Theoretical Aspects of Rationality and\nKnowledge.",
        "date": "2013-10-23T18:34:56+00:00",
        "label": 0
    },
    "2209.10377": {
        "title": "Complexity through Translations for Modal Logic with Recursion",
        "abstract": "This paper studies the complexity of classical modal logics and of their\nextension with fixed-point operators, using translations to transfer results\nacross logics. In particular, we show several complexity results for\nmulti-agent logics via translations to and from the mu-calculus and modal\nlogic, which allow us to transfer known upper and lower bounds. We also use\nthese translations to introduce a terminating tableau system for the logics we\nstudy, based on Kozen's tableau for the mu-calculus, and the one of Fitting and\nMassacci for modal logic.",
        "date": "2022-09-21T14:14:46+00:00",
        "label": 0
    },
    "1802.03292": {
        "title": "Mathematical Logic in Computer Science",
        "abstract": "The article retraces major events and milestones in the mutual influences\nbetween mathematical logic and computer science since the 1950s.",
        "date": "2018-02-07T22:21:43+00:00",
        "label": 0
    },
    "2401.04088": {
        "title": "Mixtral of Experts",
        "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
        "date": "2024-01-08T18:47:34+00:00",
        "label": 1
    },
    "2210.12528": {
        "title": "Data science transfer pathways from associate's to bachelor's programs",
        "abstract": "A substantial fraction of students who complete their college education at a\npublic university in the United States begin their journey at one of the 935\npublic two-year colleges. While the number of four-year colleges offering\nbachelor's degrees in data science continues to increase, data science\ninstruction at many two-year colleges lags behind. A major impediment is the\nrelative paucity of introductory data science courses that serve multiple\nstudent audiences and can easily transfer. In addition, the lack of pre-defined\ntransfer pathways (or articulation agreements) for data science creates a\ngrowing disconnect that leaves students who want to study data science at a\ndisadvantage. We describe opportunities and barriers to data science transfer\npathways. Five points of curricular friction merit attention: 1) a first course\nin data science, 2) a second course in data science, 3) a course in scientific\ncomputing, data science workflow, and/or reproducible computing, 4) lab\nsciences, and 5) navigating communication, ethics, and application domain\nrequirements in the context of general education and liberal arts course\nmappings. We catalog existing transfer pathways, efforts to align curricula\nacross institutions, obstacles to overcome with minimally-disruptive solutions,\nand approaches to foster these pathways. Improvements in these areas are\ncritically important to ensure that a broad and diverse set of students are\nable to engage and succeed in undergraduate data science programs.",
        "date": "2022-10-22T19:07:08+00:00",
        "label": 0
    },
    "1905.00787": {
        "title": "Computer Science and Metaphysics: A Cross-Fertilization",
        "abstract": "Computational philosophy is the use of mechanized computational techniques to\nunearth philosophical insights that are either difficult or impossible to find\nusing traditional philosophical methods. Computational metaphysics is\ncomputational philosophy with a focus on metaphysics. In this paper, we (a)\ndevelop results in modal metaphysics whose discovery was computer assisted, and\n(b) conclude that these results work not only to the obvious benefit of\nphilosophy but also, less obviously, to the benefit of computer science, since\nthe new computational techniques that led to these results may be more broadly\napplicable within computer science. The paper includes a description of our\nbackground methodology and how it evolved, and a discussion of our new results.",
        "date": "2019-05-01T16:51:32+00:00",
        "label": 0
    },
    "1707.04352": {
        "title": "Advances in Artificial Intelligence Require Progress Across all of Computer Science",
        "abstract": "Advances in Artificial Intelligence require progress across all of computer\nscience.",
        "date": "2017-07-13T23:11:18+00:00",
        "label": 0
    },
    "1707.05589": {
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.",
        "date": "2017-07-18T12:35:53+00:00",
        "label": 1
    },
    "1907.02860": {
        "title": "Truly Concurrent Bisimilarities are Game Equivalent",
        "abstract": "We design games for truly concurrent bisimilarities, including strongly truly\nconcurrent bisimilarities and branching truly concurrent bisimilarities, such\nas pomset bisimilarities, step bisimilarities, history-preserving\nbisimilarities and hereditary history-preserving bisimilarities.",
        "date": "2019-06-27T05:56:54+00:00",
        "label": 0
    },
    "1011.1335": {
        "title": "A short proof that adding some permutation rules to $\u03b2$ preserves $SN$",
        "abstract": "I show that, if a term is $SN$ for $\\beta$, it remains $SN$ when some\npermutation rules are added.",
        "date": "2010-11-05T07:54:47+00:00",
        "label": 0
    },
    "2406.13283": {
        "title": "Large-Scale Dataset Pruning in Adversarial Training through Data Importance Extrapolation",
        "abstract": "Their vulnerability to small, imperceptible attacks limits the adoption of\ndeep learning models to real-world systems. Adversarial training has proven to\nbe one of the most promising strategies against these attacks, at the expense\nof a substantial increase in training time. With the ongoing trend of\nintegrating large-scale synthetic data this is only expected to increase even\nfurther. Thus, the need for data-centric approaches that reduce the number of\ntraining samples while maintaining accuracy and robustness arises. While data\npruning and active learning are prominent research topics in deep learning,\nthey are as of now largely unexplored in the adversarial training literature.\nWe address this gap and propose a new data pruning strategy based on\nextrapolating data importance scores from a small set of data to a larger set.\nIn an empirical evaluation, we demonstrate that extrapolation-based pruning can\nefficiently reduce dataset size while maintaining robustness.",
        "date": "2024-06-19T07:23:51+00:00",
        "label": 1
    },
    "0608062": {
        "title": "Tarski's influence on computer science",
        "abstract": "The influence of Alfred Tarski on computer science was indirect but\nsignificant in a number of directions and was in certain respects fundamental.\nHere surveyed is the work of Tarski on the decision procedure for algebra and\ngeometry, the method of elimination of quantifiers, the semantics of formal\nlanguages, modeltheoretic preservation theorems, and algebraic logic; various\nconnections of each with computer science are taken up.",
        "date": "2006-08-15T16:40:24+00:00",
        "label": 0
    },
    "2307.12966": {
        "title": "Aligning Large Language Models with Human: A Survey",
        "abstract": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "date": "2023-07-24T17:44:58+00:00",
        "label": 1
    },
    "0511274": {
        "title": "Quantum Computation: A Computer Science Perspective",
        "abstract": "The theory of quantum computation is presented in a self contained way from a\ncomputer science perspective. The basics of classical computation and quantum\nmechanics is reviewed. The circuit model of quantum computation is presented in\ndetail. Throughout there is an emphasis on the physical as well as the abstract\naspects of computation and the interplay between them.\n  This report is presented as a Master's thesis at the department of Computer\nScience and Engineering at G{\\\"o}teborg University, G{\\\"o}teborg, Sweden.\n  The text is part of a larger work that is planned to include chapters on\nquantum algorithms, the quantum Turing machine model and abstract approaches to\nquantum computation.",
        "date": "2005-11-30T20:36:54+00:00",
        "label": 0
    },
    "2104.12538": {
        "title": "Exponential Competence of Computer Science and Software Engineering Undergraduate Students",
        "abstract": "We live in exceptional times in which the entire world is witnessing the\nexponential spread of a pandemic, which requires to adopt new habits of mind\nand behaviors. In this paper, I introduce the term exponential competence,\nwhich encompasses these cognitive and social skills, and describe a course for\ncomputer science and software engineering students in which emphasis is placed\non exponential competence. I argue that exponential competence is especially\nimportant for computer science and software engineering students, since many of\nthem will, most likely, be required to deal with exponential phenomena in their\nfuture professional development.",
        "date": "2021-03-23T09:58:13+00:00",
        "label": 0
    }
}