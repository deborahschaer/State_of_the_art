{
    "2402.05356": {
        "title": "Exploring Learning Complexity for Downstream Data Pruning",
        "abstract": "The over-parameterized pre-trained models pose a great challenge to\nfine-tuning with limited computation resources. An intuitive solution is to\nprune the less informative samples from the fine-tuning dataset. A series of\ntraining-based scoring functions are proposed to quantify the informativeness\nof the data subset but the pruning cost becomes non-negligible due to the heavy\nparameter updating. For efficient pruning, it is viable to adapt the similarity\nscoring function of geometric-based methods from training-based to\ntraining-free. However, we empirically show that such adaption distorts the\noriginal pruning and results in inferior performance on the downstream tasks.\nIn this paper, we propose to treat the learning complexity (LC) as the scoring\nfunction for classification and regression tasks. Specifically, the learning\ncomplexity is defined as the average predicted confidence of subnets with\ndifferent capacities, which encapsulates data processing within a converged\nmodel. Then we preserve the diverse and easy samples for fine-tuning. Extensive\nexperiments with vision datasets demonstrate the effectiveness and efficiency\nof the proposed scoring function for classification tasks. For the instruction\nfine-tuning of large language models, our method achieves state-of-the-art\nperformance with stable convergence, outperforming the full training with only\n10\\% of the instruction dataset.",
        "date": "2024-02-08T02:29:33+00:00",
        "label": 1
    },
    "2311.10969": {
        "title": "MATILDA: Inclusive Data Science Pipelines Design through Computational Creativity",
        "abstract": "We argue for the need for a new generation of data science solutions that can\ndemocratize recent advances in data engineering and artificial intelligence for\nnon-technical users from various disciplines, enabling them to unlock the full\npotential of these solutions. To do so, we adopt an approach whereby\ncomputational creativity and conversational computing are combined to guide\nnon-specialists intuitively to explore and extract knowledge from data\ncollections. The paper introduces MATILDA, a creativity-based data science\ndesign platform, showing how it can support the design process of data science\npipelines guided by human and computational creativity.",
        "date": "2023-11-18T04:37:07+00:00",
        "label": 0
    },
    "2104.14337": {
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.",
        "date": "2021-04-07T17:49:17+00:00",
        "label": 1
    },
    "2308.04896": {
        "title": "Why Data Science Projects Fail",
        "abstract": "Data Science is a modern Data Intelligence practice, which is the core of\nmany businesses and helps businesses build smart strategies around to deal with\nbusinesses challenges more efficiently. Data Science practice also helps in\nautomating business processes using the algorithm, and it has several other\nbenefits, which also deliver in a non-profitable framework. In regards to data\nscience, three key components primarily influence the effective outcome of a\ndata science project. Those are 1.Availability of Data 2.Algorithm 3.Processing\npower or infrastructure",
        "date": "2023-08-08T06:45:15+00:00",
        "label": 0
    },
    "2207.05221": {
        "title": "Language Models (Mostly) Know What They Know",
        "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
        "date": "2022-07-11T22:59:39+00:00",
        "label": 1
    },
    "2401.06692": {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "date": "2024-01-12T16:56:54+00:00",
        "label": 1
    },
    "1811.01033": {
        "title": "Building an Argument for the Use of Science Fiction in HCI Education",
        "abstract": "Science fiction literature, comics, cartoons and, in particular, audio-visual\nmaterials, such as science fiction movies and shows, can be a valuable addition\nin Human-computer interaction (HCI) Education. In this paper, we present an\noverview of research relative to future directions in HCI Education, distinct\ncrossings of science fiction in HCI and Computer Science teaching and the\nFramework for 21st Century Learning. Next, we provide examples where science\nfiction can add to the future of HCI Education. In particular, we argue herein\nfirst that science fiction, as tangible and intangible cultural artifact, can\nserve as a trigger for creativity and innovation and thus, support us in\nexploring the design space. Second, science fiction, as a means to analyze\nyet-to-come HCI technologies, can assist us in developing an open-minded and\nreflective dialogue about technological futures, thus creating a singular base\nfor critical thinking and problem solving. Provided that one is cognizant of\nits potential and limitations, we reason that science fiction can be a\nmeaningful extension of selected aspects of HCI curricula and research.",
        "date": "2018-11-02T18:24:18+00:00",
        "label": 0
    },
    "1304.7858": {
        "title": "Abstract Stobjs and Their Application to ISA Modeling",
        "abstract": "We introduce a new ACL2 feature, the abstract stobj, and show how to apply it\nto modeling the instruction set architecture of a microprocessor. Benefits of\nabstract stobjs over traditional (\"concrete\") stobjs can include faster\nexecution, support for symbolic simulation, more efficient reasoning, and\nresilience of proof developments under modeling optimization.",
        "date": "2013-04-30T04:14:22+00:00",
        "label": 0
    },
    "2005.00816": {
        "title": "DQI: Measuring Data Quality in NLP",
        "abstract": "Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.",
        "date": "2020-05-02T12:34:17+00:00",
        "label": 1
    },
    "2107.06499": {
        "title": "Deduplicating Training Data Makes Language Models Better",
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "date": "2021-07-14T06:06:52+00:00",
        "label": 1
    },
    "2405.02449": {
        "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
        "abstract": "Experimental design techniques such as active search and Bayesian\noptimization are widely used in the natural sciences for data collection and\ndiscovery. However, existing techniques tend to favor exploitation over\nexploration of the search space, which causes them to get stuck in local\noptima. This ``collapse\" problem prevents experimental design algorithms from\nyielding diverse high-quality data. In this paper, we extend the Vendi scores\n-- a family of interpretable similarity-based diversity metrics -- to account\nfor quality. We then leverage these quality-weighted Vendi scores to tackle\nexperimental design problems across various applications, including drug\ndiscovery, materials discovery, and reinforcement learning. We found that\nquality-weighted Vendi scores allow us to construct policies for experimental\ndesign that flexibly balance quality and diversity, and ultimately assemble\nrich and diverse sets of high-performing data points. Our algorithms led to a\n70%-170% increase in the number of effective discoveries compared to baselines.",
        "date": "2024-05-03T19:33:44+00:00",
        "label": 1
    },
    "2402.01865": {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "abstract": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "date": "2024-02-02T19:43:15+00:00",
        "label": 1
    },
    "2210.13526": {
        "title": "Computational Inference in Cognitive Science: Operational, Societal and Ethical Considerations",
        "abstract": "Emerging research frontiers and computational advances have gradually\ntransformed cognitive science into a multidisciplinary and data-driven field.\nAs a result, there is a proliferation of cognitive theories investigated and\ninterpreted from different academic lens and in different levels of\nabstraction. We formulate this applied aspect of this challenge as the\ncomputational cognitive inference, and describe the major routes of\ncomputational approaches. To balance the potential optimism alongside the speed\nand scale of the data-driven era of cognitive science, we propose to inspect\nthis trend in more empirical terms by identifying the operational challenges,\nsocietal impacts and ethical guidelines in conducting research and interpreting\nresults from the computational inference in cognitive science.",
        "date": "2022-10-24T18:27:27+00:00",
        "label": 0
    },
    "0706.0484": {
        "title": "Motivation, Design, and Ubiquity: A Discussion of Research Ethics and Computer Science",
        "abstract": "Modern society is permeated with computers, and the software that controls\nthem can have latent, long-term, and immediate effects that reach far beyond\nthe actual users of these systems. This places researchers in Computer Science\nand Software Engineering in a critical position of influence and\nresponsibility, more than any other field because computer systems are vital\nresearch tools for other disciplines. This essay presents several key ethical\nconcerns and responsibilities relating to research in computing. The goal is to\npromote awareness and discussion of ethical issues among computer science\nresearchers. A hypothetical case study is provided, along with questions for\nreflection and discussion.",
        "date": "2007-06-04T17:17:44+00:00",
        "label": 0
    },
    "2311.15653": {
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning has become the de facto method to equip large language\nmodels (LLMs) with the ability of following user instructions. Usually,\nhundreds of thousands or millions of instruction-following pairs are employed\nto fine-tune the foundation LLMs. Recently, some studies show that a small\nnumber of high-quality instruction data is enough. However, how to select\nappropriate instruction data for a given LLM is still an open problem. To\naddress this problem, in this paper we present a model-oriented data selection\n(MoDS) approach, which selects instruction data based on a new criteria\nconsidering three aspects: quality, coverage and necessity. First, our approach\nutilizes a quality evaluation model to filter out the high-quality subset from\nthe original instruction dataset, and then designs an algorithm to further\nselect from the high-quality subset a seed instruction dataset with good\ncoverage. The seed dataset is applied to fine-tune the foundation LLM to obtain\nan initial instruction-following LLM. Finally, we develop a necessity\nevaluation model to find out the instruction data which are performed badly in\nthe initial instruction-following LLM and consider them necessary instructions\nto further improve the LLMs. In this way, we can get a small high-quality,\nbroad-coverage and high-necessity subset from the original instruction\ndatasets. Experimental results show that, the model fine-tuned with 4,000\ninstruction pairs selected by our approach could perform better than the model\nfine-tuned with the full original dataset which includes 214k instruction data.",
        "date": "2023-11-27T09:33:13+00:00",
        "label": 1
    },
    "2002.05658": {
        "title": "Ten Research Challenge Areas in Data Science",
        "abstract": "Although data science builds on knowledge from computer science, mathematics,\nstatistics, and other disciplines, data science is a unique field with many\nmysteries to unlock: challenging scientific questions and pressing questions of\nsocietal importance. This article starts with meta-questions about data science\nas a discipline and then elaborates on ten ideas for the basis of a research\nagenda for data science.",
        "date": "2020-01-27T21:39:57+00:00",
        "label": 0
    },
    "2105.10446": {
        "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction",
        "abstract": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained \"white-box\"\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at\n\\url{https://github.com/Ma-Lab-Berkeley}.",
        "date": "2021-05-21T16:29:57+00:00",
        "label": 1
    },
    "2302.04062": {
        "title": "Machine Learning for Synthetic Data Generation: A Review",
        "abstract": "Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.",
        "date": "2023-02-08T13:59:31+00:00",
        "label": 1
    },
    "1510.03055": {
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.",
        "date": "2015-10-11T14:04:57+00:00",
        "label": 1
    },
    "1904.01053": {
        "title": "Computer simulations in science and engineering - Concepts - Practices - Perspectives",
        "abstract": "The ubiquitous presence of computer simulations in all kinds of research\nareas evidence their role as the new driving force for the advancement of\nscience and engineering research. Nothing seems to escape the image of success\nthat computer simulations project onto the research community and the general\npublic. One simple way to illustrate this consists of asking ourselves how\nwould contemporary science and engineering look like without the use of\ncomputer simulations. The answer would certainly diverge from the current image\nwe have of scientific and engineering research.\n  As much as computer simulations are successful, they are also methods that\nfail in their purpose of inquiring about the world; and as much as researchers\nmake use of them, computer simulations raise important questions that are at\nthe heart of contemporary science and engineering practice. In this respect,\ncomputer simulations make a fantastic subject of research for the natural\nsciences, the social sciences, engineering and, as in our case, also for\nphilosophy. Studies on computer simulations touch upon many different facets of\nscientific and engineering research and evoke philosophically inclined\nquestions of interpretation with close ties to problems in experimental\nsettings and engineering applications (...)",
        "date": "2019-03-09T15:26:05+00:00",
        "label": 0
    },
    "1304.7394": {
        "title": "A Static Analysis Framework for Livelock Freedom in CSP",
        "abstract": "In a process algebra with hiding and recursion it is possible to create\nprocesses which compute internally without ever communicating with their\nenvironment. Such processes are said to diverge or livelock. In this paper we\nshow how it is possible to conservatively classify processes as livelock-free\nthrough a static analysis of their syntax. In particular, we present a\ncollection of rules, based on the inductive structure of terms, which guarantee\nlivelock-freedom of the denoted process. This gives rise to an algorithm which\nconservatively flags processes that can potentially livelock. We illustrate our\napproach by applying both BDD-based and SAT-based implementations of our\nalgorithm to a range of benchmarks, and show that our technique in general\nsubstantially outperforms the model checker FDR whilst exhibiting a low rate of\ninconclusive results.",
        "date": "2013-04-27T18:10:36+00:00",
        "label": 0
    },
    "2305.06161": {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.",
        "date": "2023-05-09T08:16:42+00:00",
        "label": 1
    },
    "2202.00622": {
        "title": "Datamodels: Predicting Predictions from Training Data",
        "abstract": "We present a conceptual framework, datamodeling, for analyzing the behavior\nof a model class in terms of the training data. For any fixed \"target\" example\n$x$, training set $S$, and learning algorithm, a datamodel is a parameterized\nfunction $2^S \\to \\mathbb{R}$ that for any subset of $S' \\subset S$ -- using\nonly information about which examples of $S$ are contained in $S'$ -- predicts\nthe outcome of training a model on $S'$ and evaluating on $x$. Despite the\npotential complexity of the underlying process being approximated (e.g.,\nend-to-end training and evaluation of deep neural networks), we show that even\nsimple linear datamodels can successfully predict model outputs. We then\ndemonstrate that datamodels give rise to a variety of applications, such as:\naccurately predicting the effect of dataset counterfactuals; identifying\nbrittle predictions; finding semantically similar examples; quantifying\ntrain-test leakage; and embedding data into a well-behaved and feature-rich\nrepresentation space. Data for this paper (including pre-computed datamodels as\nwell as raw predictions from four million trained deep neural networks) is\navailable at https://github.com/MadryLab/datamodels-data .",
        "date": "2022-02-01T18:15:24+00:00",
        "label": 1
    },
    "1110.0543": {
        "title": "A high performance scientific cloud computing environment for materials simulations",
        "abstract": "We describe the development of a scientific cloud computing (SCC) platform\nthat offers high performance computation capability. The platform consists of a\nscientific virtual machine prototype containing a UNIX operating system and\nseveral materials science codes, together with essential interface tools (an\nSCC toolset) that offers functionality comparable to local compute clusters. In\nparticular, our SCC toolset provides automatic creation of virtual clusters for\nparallel computing, including tools for execution and monitoring performance,\nas well as efficient I/O utilities that enable seamless connections to and from\nthe cloud. Our SCC platform is optimized for the Amazon Elastic Compute Cloud\n(EC2). We present benchmarks for prototypical scientific applications and\ndemonstrate performance comparable to local compute clusters. To facilitate\ncode execution and provide user-friendly access, we have also integrated cloud\ncomputing capability in a JAVA-based GUI. Our SCC platform may be an\nalternative to traditional HPC resources for materials science or quantum\nchemistry applications.",
        "date": "2011-10-03T23:35:46+00:00",
        "label": 0
    },
    "1908.03793": {
        "title": "The rise and rise of interdisciplinary research: Understanding the interaction dynamics of three major fields -- Physics, Mathematics & Computer Science",
        "abstract": "The distinction between sciences is becoming increasingly more artificial --\nan approach from one area can be easily applied to the other. More exciting\nresearch nowadays is happening perhaps at the interfaces of disciplines like\nPhysics, Mathematics and Computer Science. How do these interfaces emerge and\ninteract? For instance, is there a specific pattern in which these fields cite\neach other? In this article, we investigate a collection of more than 1.2\nmillion papers from three different scientific disciplines -- Physics,\nMathematics, and Computer Science. We show how over a timescale the citation\npatterns from the core science fields (Physics, Mathematics) to the applied and\nfast-growing field of Computer Science have drastically increased. Further, we\nobserve how certain subfields in these disciplines are shrinking while others\nare becoming tremendously popular. For instance, an intriguing observation is\nthat citations from Mathematics to the subfield of machine learning in Computer\nScience in recent times are exponentially increasing.",
        "date": "2019-08-10T17:58:19+00:00",
        "label": 0
    },
    "2206.03276": {
        "title": "Oxford-style Debates in Telecommunication and Computer Science Education",
        "abstract": "Oxford-style debating is a well-known tool in social sciences. Such formal\ndiscussions on particular topics are widely used by historians and\nsociologists. However, when we try to go beyond standard thinking, it turns out\nthat Oxford-style debating can be a great educational tool in telecommunication\nand computer science. This article presents this unusual method of education at\ntechnical universities and in the IT industry, and describes its features and\nchallenges. Best practices and examples of debating are provided, taking into\naccount emerging topics in telecommunications and computer science, such as\ncybersecurity. The article also contains feedback from IT engineers who\nparticipated in Oxford-style debates. All this aims to encourage this form of\neducation in telecommunication and computer science.",
        "date": "2022-06-03T10:42:31+00:00",
        "label": 0
    },
    "1702.05962": {
        "title": "Latent Variable Dialogue Models and their Diversity",
        "abstract": "We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.",
        "date": "2017-02-20T13:36:23+00:00",
        "label": 1
    },
    "2402.18041": {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "date": "2024-02-28T04:35:51+00:00",
        "label": 1
    },
    "2402.17327": {
        "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
        "abstract": "We study the data selection problem, whose aim is to select a small\nrepresentative subset of data that can be used to efficiently train a machine\nlearning model. We present a new data selection approach based on $k$-means\nclustering and sensitivity sampling. Assuming access to an embedding\nrepresentation of the data with respect to which the model loss is H\\\"older\ncontinuous, our approach provably allows selecting a set of ``typical'' $k +\n1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of\nthe whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an\nadditive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means\ncost for the input embeddings and $\\lambda$ is the H\\\"older constant.\n  We furthermore demonstrate the performance and scalability of our approach on\nfine-tuning foundation models and show that it outperforms state-of-the-art\nmethods. We also show how it can be applied on linear regression, leading to a\nnew sampling strategy that surprisingly matches the performances of leverage\nscore sampling, while being conceptually simpler and more scalable.",
        "date": "2024-02-27T09:03:43+00:00",
        "label": 1
    },
    "2310.07849": {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "abstract": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "date": "2023-10-11T19:51:13+00:00",
        "label": 1
    },
    "1606.08699": {
        "title": "How to Compute Halting",
        "abstract": "A consistently specified halting function may be computed.",
        "date": "2016-06-26T19:38:10+00:00",
        "label": 0
    },
    "2209.10377": {
        "title": "Complexity through Translations for Modal Logic with Recursion",
        "abstract": "This paper studies the complexity of classical modal logics and of their\nextension with fixed-point operators, using translations to transfer results\nacross logics. In particular, we show several complexity results for\nmulti-agent logics via translations to and from the mu-calculus and modal\nlogic, which allow us to transfer known upper and lower bounds. We also use\nthese translations to introduce a terminating tableau system for the logics we\nstudy, based on Kozen's tableau for the mu-calculus, and the one of Fitting and\nMassacci for modal logic.",
        "date": "2022-09-21T14:14:46+00:00",
        "label": 0
    },
    "1907.05644": {
        "title": "Data-driven materials science: status, challenges and perspectives",
        "abstract": "Data-driven science is heralded as a new paradigm in materials science. In\nthis field, data is the new resource, and knowledge is extracted from materials\ndata sets that are too big or complex for traditional human reasoning -\ntypically with the intent to discover new or improved materials or materials\nphenomena. Multiple factors, including the open science movement, national\nfunding, and progress in information technology, have fueled its development.\nSuch related tools as materials databases, machine learning, and\nhigh-throughput methods are now established as parts of the materials research\ntoolset. However, there are a variety of challenges that impede progress in\ndata-driven materials science: data veracity, integration of experimental and\ncomputational data, data longevity, standardization, and the gap between\nindustrial interests and academic efforts. In this perspective article, we\ndiscuss the historical development and current state of data-driven materials\nscience, building from the early evolution of open science to the rapid\nexpansion of materials data infrastructures. We also review key successes and\nchallenges so far, providing a perspective on the future development of the\nfield.",
        "date": "2019-07-12T09:44:01+00:00",
        "label": 0
    },
    "1907.02860": {
        "title": "Truly Concurrent Bisimilarities are Game Equivalent",
        "abstract": "We design games for truly concurrent bisimilarities, including strongly truly\nconcurrent bisimilarities and branching truly concurrent bisimilarities, such\nas pomset bisimilarities, step bisimilarities, history-preserving\nbisimilarities and hereditary history-preserving bisimilarities.",
        "date": "2019-06-27T05:56:54+00:00",
        "label": 0
    },
    "2303.08114": {
        "title": "Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs",
        "abstract": "Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.",
        "date": "2023-03-14T17:47:25+00:00",
        "label": 1
    },
    "2109.02806": {
        "title": "Symbolic Computation in Software Science: My Personal View",
        "abstract": "In this note, I develop my personal view on the scope and relevance of\nsymbolic computation in software science. For this, I discuss the interaction\nand differences between symbolic computation, software science, automatic\nprogramming, mathematical knowledge management, artificial intelligence,\nalgorithmic intelligence, numerical computation, and machine learning. In the\ndiscussion of these notions, I allow myself to refer also to papers (1982,\n1985, 2001, 2003, 2013) of mine in which I expressed my views on these areas at\nearly stages of some of these fields.",
        "date": "2021-09-07T01:41:41+00:00",
        "label": 0
    },
    "0410370": {
        "title": "Looking for Design in Materials Design",
        "abstract": "Despite great advances in computation, materials design is still science\nfiction. The construction of structure-property relations on the quantum scale\nwill turn computational empiricism into true design.",
        "date": "2004-10-14T14:41:24+00:00",
        "label": 0
    },
    "2309.16609": {
        "title": "Qwen Technical Report",
        "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, enabling natural language processing tasks that were previously\nthought to be exclusive to humans. In this work, we introduce Qwen, the first\ninstallment of our large language model series. Qwen is a comprehensive\nlanguage model series that encompasses distinct models with varying parameter\ncounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,\nthe chat models finetuned with human alignment techniques. The base language\nmodels consistently demonstrate superior performance across a multitude of\ndownstream tasks, and the chat models, particularly those trained using\nReinforcement Learning from Human Feedback (RLHF), are highly competitive. The\nchat models possess advanced tool-use and planning capabilities for creating\nagent applications, showcasing impressive performance even when compared to\nbigger models on complex tasks like utilizing a code interpreter. Furthermore,\nwe have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as\nwell as mathematics-focused models, Math-Qwen-Chat, which are built upon base\nlanguage models. These models demonstrate significantly improved performance in\ncomparison with open-source models, and slightly fall behind the proprietary\nmodels.",
        "date": "2023-09-28T17:07:49+00:00",
        "label": 1
    },
    "2008.03964": {
        "title": "DQI: A Guide to Benchmark Evaluation",
        "abstract": "A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.",
        "date": "2020-08-10T08:38:55+00:00",
        "label": 1
    },
    "1401.4507": {
        "title": "Using Quantum Computers to Learn Physics",
        "abstract": "Since its inception at the beginning of the twentieth century, quantum\nmechanics has challenged our conceptions of how the universe ought to work;\nhowever, the equations of quantum mechanics can be too computationally\ndifficult to solve using existing computers for even modestly large systems.\nHere I will show that quantum computers can sometimes be used to address such\nproblems and that quantum computer science can assign formal complexities to\nlearning facts about nature. Hence, computer science should not only be\nregarded as an applied science; it is also of central importance to the\nfoundations of science.",
        "date": "2014-01-18T01:46:52+00:00",
        "label": 0
    },
    "2311.14736": {
        "title": "Data Diversity Matters for Robust Instruction Tuning",
        "abstract": "Recent works have shown that by curating high quality and diverse instruction\ntuning datasets, we can significantly improve instruction-following\ncapabilities. However, creating such datasets is difficult and most works rely\non manual curation or proprietary language models. Automatic data curation is\ndifficult as it is still not clear how we can define diversity for instruction\ntuning, how diversity and quality depend on one other, and how we can optimize\ndataset quality and diversity. To resolve these issue, we propose a new\nalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple\nmethod to simultaneously control dataset diversity and quality, allowing us to\nconduct an in-depth study on the effect of diversity and quality on instruction\ntuning performance. From this study we draw two key insights (1) there is a\nnatural tradeoff between data diversity and quality and (2) increasing data\ndiversity significantly improves the worst case instruction following\nperformance, therefore improving robustness. We validate the performance of\nQDIT on several large scale instruction tuning datasets, where we find it can\nsubstantially improve worst and average case performance compared to\nquality-driven data selection.",
        "date": "2023-11-21T19:12:18+00:00",
        "label": 1
    },
    "0705.1367": {
        "title": "Logic Column 18: Alternative Logics: A Book Review",
        "abstract": "This article discusses two books on the topic of alternative logics in\nscience: \"Deviant Logic\", by Susan Haack, and \"Alternative Logics: Do Sciences\nNeed Them?\", edited by Paul Weingartner.",
        "date": "2007-05-09T21:56:15+00:00",
        "label": 0
    },
    "2302.02105": {
        "title": "Computational philosophy of science",
        "abstract": "Philosophy of science attempts to describe all parts of the scientific\nprocess in a general way in order to facilitate the description, execution and\nimprovements of this process.\n  So far, all proposed philosophies have only covered existing processes and\ndisciplines partially and imperfectly. In particular logical approaches have\nalways received a lot of attention due to attempts to fundamentally address\nissues with the definition of science as a discipline with reductionist\ntheories.\n  We propose a new way to approach the problem from the perspective of\ncomputational complexity and argue why this approach may be better than\nprevious propositions based on pure logic and mathematics.",
        "date": "2023-02-04T06:11:34+00:00",
        "label": 0
    },
    "0511274": {
        "title": "Quantum Computation: A Computer Science Perspective",
        "abstract": "The theory of quantum computation is presented in a self contained way from a\ncomputer science perspective. The basics of classical computation and quantum\nmechanics is reviewed. The circuit model of quantum computation is presented in\ndetail. Throughout there is an emphasis on the physical as well as the abstract\naspects of computation and the interplay between them.\n  This report is presented as a Master's thesis at the department of Computer\nScience and Engineering at G{\\\"o}teborg University, G{\\\"o}teborg, Sweden.\n  The text is part of a larger work that is planned to include chapters on\nquantum algorithms, the quantum Turing machine model and abstract approaches to\nquantum computation.",
        "date": "2005-11-30T20:36:54+00:00",
        "label": 0
    },
    "1606.01148": {
        "title": "Tripartite Unions",
        "abstract": "This note provides conditions under which the union of three well-founded\nbinary relations is also well-founded.",
        "date": "2016-06-03T15:41:55+00:00",
        "label": 0
    },
    "0907.3804": {
        "title": "Decidability of higher-order matching",
        "abstract": "We show that the higher-order matching problem is decidable using a\ngame-theoretic argument.",
        "date": "2009-07-22T09:17:30+00:00",
        "label": 0
    },
    "2109.01661": {
        "title": "Data science and Machine learning in the Clouds: A Perspective for the Future",
        "abstract": "As we are fast approaching the beginning of a paradigm shift in the field of\nscience, Data driven science (the so called fourth science paradigm) is going\nto be the driving force in research and innovation. From medicine to\nbiodiversity and astronomy to geology, all these terms are somehow going to be\naffected by this paradigm shift. The huge amount of data to be processed under\nthis new paradigm will be a major concern in the future and one will strongly\nrequire cloud based services in all the aspects of these computations (from\nstorage to compute and other services). Another aspect will be energy\nconsumption and performance of prediction jobs and tasks within such a\nscientific paradigm which will change the way one sees computation. Data\nscience has heavily impacted or rather triggered the emergence of Machine\nLearning, Signal/Image/Video processing related algorithms, Artificial\nintelligence, Robotics, health informatics, geoinformatics, and many more such\nareas of interest. Hence, we envisage an era where Data science can deliver its\npromises with the help of the existing cloud based platforms and services with\nthe addition of new services. In this article, we discuss about data driven\nscience and Machine learning and how they are going to be linked through cloud\nbased services in the future. It also discusses the rise of paradigms like\napproximate computing, quantum computing and many more in recent times and\ntheir applicability in big data processing, data science, analytics, prediction\nand machine learning in the cloud environments.",
        "date": "2021-09-02T17:36:24+00:00",
        "label": 0
    },
    "2305.14233": {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "abstract": "Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
        "date": "2023-05-23T16:49:14+00:00",
        "label": 1
    },
    "2109.06379": {
        "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",
        "abstract": "Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective that facilitates the design of metrics for a\nwide range of language generation tasks and quality aspects. Based on the\nnature of information change from input to output, we classify NLG tasks into\ncompression (e.g., summarization), transduction (e.g., text rewriting), and\ncreation (e.g., dialog). The information alignment, or overlap, between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. Using the uniform concept of information alignment, we develop a\nfamily of interpretable metrics for various NLG tasks and aspects, often\nwithout need of gold reference data. To operationalize the metrics, we train\nself-supervised models to approximate information alignment as a prediction\ntask. Experiments show the uniformly designed metrics achieve stronger or\ncomparable correlations with human judgement compared to state-of-the-art\nmetrics in each of diverse tasks, including text summarization, style transfer,\nand knowledge-grounded dialog. With information alignment as the intermediate\nrepresentation, we deliver a composable library for easy NLG evaluation and\nfuture metric design.",
        "date": "2021-09-14T01:00:42+00:00",
        "label": 1
    },
    "1711.00201": {
        "title": "Credimus",
        "abstract": "We believe that economic design and computational complexity---while already\nimportant to each other---should become even more important to each other with\neach passing year. But for that to happen, experts in on the one hand such\nareas as social choice, economics, and political science and on the other hand\ncomputational complexity will have to better understand each other's\nworldviews.\n  This article, written by two complexity theorists who also work in\ncomputational social choice theory, focuses on one direction of that process by\npresenting a brief overview of how most computational complexity theorists view\nthe world. Although our immediate motivation is to make the lens through which\ncomplexity theorists see the world be better understood by those in the social\nsciences, we also feel that even within computer science it is very important\nfor nontheoreticians to understand how theoreticians think, just as it is\nequally important within computer science for theoreticians to understand how\nnontheoreticians think.",
        "date": "2017-11-01T04:23:01+00:00",
        "label": 0
    },
    "0703148": {
        "title": "Computer Science and Game Theory: A Brief Survey",
        "abstract": "There has been a remarkable increase in work at the interface of computer\nscience and game theory in the past decade. In this article I survey some of\nthe main themes of work in the area, with a focus on the work in computer\nscience. Given the length constraints, I make no attempt at being\ncomprehensive, especially since other surveys are also available, and a\ncomprehensive survey book will appear shortly.",
        "date": "2007-03-29T18:43:58+00:00",
        "label": 0
    },
    "0502011": {
        "title": "Where the Rubber Meets the Sky: Bridging the Gap between Databases and Science",
        "abstract": "Scientists in all domains face a data avalanche - both from better\ninstruments and from improved simulations. We believe that computer science\ntools and computer scientists are in a position to help all the sciences by\nbuilding tools and developing techniques to manage, analyze, and visualize\npeta-scale scientific information. This article is summarizes our experiences\nover the last seven years trying to bridge the gap between database technology\nand the needs of the astronomy community in building the World-Wide Telescope.",
        "date": "2005-02-02T04:40:55+00:00",
        "label": 0
    },
    "1309.0717": {
        "title": "A Polynomial Translation of pi-calculus FCPs to Safe Petri Nets",
        "abstract": "We develop a polynomial translation from finite control pi-calculus processes\nto safe low-level Petri nets. To our knowledge, this is the first such\ntranslation. It is natural in that there is a close correspondence between the\ncontrol flows, enjoys a bisimulation result, and is suitable for practical\nmodel checking.",
        "date": "2013-09-03T15:08:39+00:00",
        "label": 0
    },
    "0202038": {
        "title": "The efficient generation of unstructured control volumes in 2D and 3D",
        "abstract": "Many problems in engineering, chemistry and physics require the\nrepresentation of solutions in complex geometries. In the paper we deal with a\nproblem of unstructured mesh generation for the control volume method. We\npropose an algorithm which bases on the spheres generation in central points of\nthe control volumes.",
        "date": "2002-02-26T16:32:12+00:00",
        "label": 0
    },
    "2306.11670": {
        "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
        "abstract": "It is often advantageous to train models on a subset of the available train\nexamples, because the examples are of variable quality or because one would\nlike to train with fewer examples, without sacrificing performance. We present\nGradient Information Optimization (GIO), a scalable, task-agnostic approach to\nthis data selection problem that requires only a small set of (unlabeled)\nexamples representing a target distribution. GIO begins from a natural,\ninformation-theoretic objective that is intractable in practice. Our\ncontribution is in showing that it can be made highly scalable through a simple\nrelaxation of the objective and a highly efficient implementation. In\nexperiments with machine translation, spelling correction, and image\nrecognition, we show that GIO delivers outstanding results with very small\ntrain sets. These findings are robust to different representation models and\nhyperparameters for GIO itself. GIO is task- and domain-agnostic and can be\napplied out-of-the-box to new datasets and domains. We open source a\npip-installable implementation of the algorithm as \"pip install grad-info-opt\".",
        "date": "2023-06-20T16:43:38+00:00",
        "label": 1
    },
    "2405.20456": {
        "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
        "abstract": "Recent works have shown that machine learning models improve at a predictable\nrate with the total amount of training data, leading to scaling laws that\ndescribe the relationship between error and dataset size. These scaling laws\ncan help design a model's training dataset, but they typically take an\naggregate view of the data by only considering the dataset's size. We introduce\na new perspective by investigating scaling behavior for the value of individual\ndata points: we find that a data point's contribution to model's performance\nshrinks predictably with the size of the dataset in a log-linear manner.\nInterestingly, there is significant variability in the scaling exponent among\ndifferent data points, indicating that certain points are more valuable in\nsmall datasets while others are relatively more useful as a part of large\ndatasets. We provide learning theory to support our scaling law, and we observe\nempirically that it holds across diverse model classes. We further propose a\nmaximum likelihood estimator and an amortized estimator to efficiently learn\nthe individualized scaling behaviors from a small number of noisy observations\nper data point. Using our estimators, we provide insights into factors that\ninfluence the scaling behavior of different data points. Finally, we\ndemonstrate applications of the individualized scaling laws to data valuation\nand data subset selection. Overall, our work represents a first step towards\nunderstanding and utilizing scaling properties for the value of individual data\npoints.",
        "date": "2024-05-30T20:10:24+00:00",
        "label": 1
    },
    "2010.07017": {
        "title": "Computational Skills by Stealth in Secondary School Data Science",
        "abstract": "The unprecedented growth in the availability of data of all types and\nqualities and the emergence of the field of data science has provided an\nimpetus to finally realizing the implementation of the full breadth of the\nNolan and Temple Lang proposed integration of computing concepts into\nstatistics curricula at all levels in statistics and new data science programs\nand courses. Moreover, data science, implemented carefully, opens accessible\npathways to stem for students for whom neither mathematics nor computer science\nare natural affinities, and who would traditionally be excluded. We discuss a\nproposal for the stealth development of computational skills in students' first\nexposure to data science through careful, scaffolded exposure to computation\nand its power. The intent of this approach is to support students, regardless\nof interest and self-efficacy in coding, in becoming data-driven learners, who\nare capable of asking complex questions about the world around them, and then\nanswering those questions through the use of data-driven inquiry. This\ndiscussion is presented in the context of the International Data Science in\nSchools Project which recently published computer science and statistics\nconsensus curriculum frameworks for a two-year secondary school data science\nprogram, designed to make data science accessible to all.",
        "date": "2020-10-08T09:11:51+00:00",
        "label": 0
    },
    "2205.01553": {
        "title": "Why The Trans Programmer?",
        "abstract": "Through online anecdotal evidence and online communities, there is an\nin-group idea of trans people (specifically trans-feminine individuals)\ndisproportionately entering computer science education & fields. Existing data\nsuggests this is a plausible trend, yet no research has been done into exactly\nwhy. As computer science education (traditional schooling or self-taught\nmethods) is integral to working in computer science fields, a simple research\nsurvey was conducted to gather data on 138 trans people's experiences with\ncomputer science & computer science education. This article's purpose is to\nshed insight on the motivations for trans individuals choosing computer science\npaths, while acting as a basis and call to action for further research.",
        "date": "2022-05-03T15:06:23+00:00",
        "label": 0
    },
    "1707.04826": {
        "title": "Machine learning application in the life time of materials",
        "abstract": "Materials design and development typically takes several decades from the\ninitial discovery to commercialization with the traditional trial and error\ndevelopment approach. With the accumulation of data from both experimental and\ncomputational results, data based machine learning becomes an emerging field in\nmaterials discovery, design and property prediction. This manuscript reviews\nthe history of materials science as a disciplinary the most common machine\nlearning method used in materials science, and specifically how they are used\nin materials discovery, design, synthesis and even failure detection and\nanalysis after materials are deployed in real application. Finally, the\nlimitations of machine learning for application in materials science and\nchallenges in this emerging field is discussed.",
        "date": "2017-07-16T05:58:40+00:00",
        "label": 0
    },
    "2202.01291": {
        "title": "Computer sciences and synthesis: retrospective and perspective",
        "abstract": "The problem of synthesis in computer sciences, including cybernetics,\nartificial intelligence and system analysis, is analyzed. Main methods of\nrealization this problem are discussed. Ways of search universal method of\ncreation universal synthetic science are represented. As example of such\nuniversal method polymetric analysis is given. Perspective of further\ndevelopment of this research, including application polymetric method for the\nresolution main problems of computer sciences, is analyzed too.",
        "date": "2022-01-26T04:42:45+00:00",
        "label": 0
    },
    "1011.1335": {
        "title": "A short proof that adding some permutation rules to $\u03b2$ preserves $SN$",
        "abstract": "I show that, if a term is $SN$ for $\\beta$, it remains $SN$ when some\npermutation rules are added.",
        "date": "2010-11-05T07:54:47+00:00",
        "label": 0
    },
    "1003.1888": {
        "title": "Biology-Derived Algorithms in Engineering Optimization",
        "abstract": "Biology-derived algorithms are an important part of computational sciences,\nwhich are essential to many scientific disciplines and engineering\napplications. Many computational methods are derived from or based on the\nanalogy to natural evolution and biological activities, and these biologically\ninspired computations include genetic algorithms, neural networks, cellular\nautomata, and other algorithms.",
        "date": "2010-03-09T14:53:12+00:00",
        "label": 0
    },
    "2401.17197": {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently\ngarnered considerable attention, where fine-tuning plays a key role in LLMs'\nadaptation. However, the cost of fine-tuning LLMs on rapidly expanding\nrecommendation data limits their practical application. To address this\nchallenge, few-shot fine-tuning offers a promising approach to quickly adapt\nLLMs to new recommendation data. We propose the task of data pruning for\nefficient LLM-based recommendation, aimed at identifying representative samples\ntailored for LLMs' few-shot fine-tuning. While coreset selection is closely\nrelated to the proposed task, existing coreset selection methods often rely on\nsuboptimal heuristic metrics or entail costly optimization on large-scale\nrecommendation data.\n  To tackle these issues, we introduce two objectives for the data pruning task\nin the context of LLM-based recommendation: 1) high accuracy aims to identify\nthe influential samples that can lead to high overall performance; and 2) high\nefficiency underlines the low costs of the data pruning process. To pursue the\ntwo objectives, we propose a novel data pruning method based on two scores,\ni.e., influence score and effort score, to efficiently identify the influential\nsamples. Particularly, the influence score is introduced to accurately estimate\nthe influence of sample removal on the overall performance. To achieve low\ncosts of the data pruning process, we use a small-sized surrogate model to\nreplace LLMs to obtain the influence score. Considering the potential gap\nbetween the surrogate model and LLMs, we further propose an effort score to\nprioritize some hard samples specifically for LLMs. Empirical results on three\nreal-world datasets validate the effectiveness of our proposed method. In\nparticular, the proposed method uses only 2% samples to surpass the full data\nfine-tuning, reducing time costs by 97%.",
        "date": "2024-01-30T17:31:19+00:00",
        "label": 1
    },
    "1610.04276": {
        "title": "Perspectives on Surgical Data Science",
        "abstract": "The availability of large amounts of data together with advances in\nanalytical techniques afford an opportunity to address difficult challenges in\nensuring that healthcare is safe, effective, efficient, patient-centered,\nequitable, and timely. Surgical care and training stand to tremendously gain\nthrough surgical data science. Herein, we discuss a few perspectives on the\nscope and objectives for surgical data science.",
        "date": "2016-10-13T22:06:46+00:00",
        "label": 0
    },
    "2109.02501": {
        "title": "Proceedings of the 9th International Symposium on Symbolic Computation in Software Science",
        "abstract": "This volume contains papers presented at the Ninth International Symposium on\nSymbolic Computation in Software Science, SCSS 2021.\n  Symbolic Computation is the science of computing with symbolic objects\n(terms, formulae, programs, representations of algebraic objects, etc.).\nPowerful algorithms have been developed during the past decades for the major\nsubareas of symbolic computation: computer algebra and computational logic.\nThese algorithms and methods are successfully applied in various fields,\nincluding software science, which covers a broad range of topics about software\nconstruction and analysis.\n  Meanwhile, artificial intelligence methods and machine learning algorithms\nare widely used nowadays in various domains and, in particular, combined with\nsymbolic computation. Several approaches mix artificial intelligence and\nsymbolic methods and tools deployed over large corpora to create what is known\nas cognitive systems. Cognitive computing focuses on building systems that\ninteract with humans naturally by reasoning, aiming at learning at scale.\n  The purpose of SCSS is to promote research on theoretical and practical\naspects of symbolic computation in software science, combined with modern\nartificial intelligence techniques. These proceedings contain the keynote paper\nby Bruno Buchberger and ten contributed papers. Besides, the conference program\nincluded three invited talks, nine short and work-in-progress papers, and a\nspecial session on computer algebra and computational logic. Due to the\nCOVID-19 pandemic, the symposium was held completely online. It was organized\nby the Research Institute for Symbolic Computation (RISC) of the Johannes\nKepler University Linz on September 8--10, 2021.",
        "date": "2021-09-06T14:22:11+00:00",
        "label": 0
    },
    "1601.05973": {
        "title": "Science Learning via Participation in Online Citizen Science",
        "abstract": "We investigate the development of scientific content knowledge of volunteers\nparticipating in online citizen science projects in the Zooniverse\n(www.zooniverse.org), including the astronomy projects Galaxy Zoo\n(www.galaxyzoo.org) and Planet Hunters (www.planethunters.org). We use\neconometric methods to test how measures of project participation relate to\nsuccess in a science quiz, controlling for factors known to correlate with\nscientific knowledge. Citizen scientists believe they are learning about both\nthe content and processes of science through their participation. Won't don't\ndirectly test the latter, but we find evidence to support the former - that\nmore actively engaged participants perform better in a project-specific science\nknowledge quiz, even after controlling for their general science knowledge. We\ninterpret this as evidence of learning of science content inspired by\nparticipation in online citizen science.",
        "date": "2016-01-22T12:23:10+00:00",
        "label": 0
    },
    "2403.09606": {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "date": "2024-03-14T17:47:20+00:00",
        "label": 1
    },
    "0911.1672": {
        "title": "Biological Computing Fundamentals and Futures",
        "abstract": "The fields of computing and biology have begun to cross paths in new ways. In\nthis paper a review of the current research in biological computing is\npresented. Fundamental concepts are introduced and these foundational elements\nare explored to discuss the possibilities of a new computing paradigm. We\nassume the reader to possess a basic knowledge of Biology and Computer Science",
        "date": "2009-11-09T13:16:01+00:00",
        "label": 0
    },
    "1003.1473": {
        "title": "Comments on \"Routh Stability Criterion\"",
        "abstract": "In this note, we have shown special case on Routh stability criterion, which\nis not discussed, in previous literature. This idea can be useful in computer\nscience applications.",
        "date": "2010-03-07T14:09:11+00:00",
        "label": 0
    },
    "2402.05119": {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.",
        "date": "2024-02-03T04:45:25+00:00",
        "label": 1
    },
    "2403.12776": {
        "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
        "abstract": "Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).",
        "date": "2024-03-19T14:44:45+00:00",
        "label": 1
    },
    "2405.06331": {
        "title": "LMD3: Language Model Data Density Dependence",
        "abstract": "We develop a methodology for analyzing language model task performance at the\nindividual example level based on training data density estimation. Experiments\nwith paraphrasing as a controlled intervention on finetuning data demonstrate\nthat increasing the support in the training distribution for specific test\nqueries results in a measurable increase in density, which is also a\nsignificant predictor of the performance increase caused by the intervention.\nExperiments with pretraining data demonstrate that we can explain a significant\nfraction of the variance in model perplexity via density measurements. We\nconclude that our framework can provide statistical evidence of the dependence\nof a target model's predictions on subsets of its training data, and can more\ngenerally be used to characterize the support (or lack thereof) in the training\ndata for a given test task.",
        "date": "2024-05-10T09:03:27+00:00",
        "label": 1
    },
    "9505013": {
        "title": "Wavelet basis for the Schr\u00f6dinger equation",
        "abstract": "The self-similar representation for the Schr\\\"{o}dinger equation is derived.",
        "date": "1995-05-16T16:19:16+00:00",
        "label": 0
    },
    "2307.06290": {
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
        "abstract": "Large language models (LLMs) are initially pretrained for broad capabilities\nand then finetuned with instruction-following datasets to improve their\nperformance in interacting with humans. Despite advances in finetuning, a\nstandardized guideline for selecting high-quality datasets to optimize this\nprocess remains elusive. In this paper, we first propose InstructMining, an\ninnovative method designed for automatically selecting premium\ninstruction-following data for finetuning LLMs. Specifically, InstructMining\nutilizes natural language indicators as a measure of data quality, applying\nthem to evaluate unseen datasets. During experimentation, we discover that\ndouble descent phenomenon exists in large language model finetuning. Based on\nthis observation, we further leverage BlendSearch to help find the best subset\namong the entire dataset (i.e., 2,532 out of 100,000). Experiment results show\nthat InstructMining-7B achieves state-of-the-art performance on two of the most\npopular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",
        "date": "2023-07-12T16:37:31+00:00",
        "label": 1
    },
    "1903.09722": {
        "title": "Pre-trained Language Model Representations for Language Generation",
        "abstract": "Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.",
        "date": "2019-03-22T22:14:51+00:00",
        "label": 1
    },
    "2407.14985": {
        "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
        "abstract": "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
        "date": "2024-07-20T21:24:40+00:00",
        "label": 1
    },
    "2312.15685": {
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
        "abstract": "Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.",
        "date": "2023-12-25T10:29:28+00:00",
        "label": 1
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "date": "2018-10-11T00:50:01+00:00",
        "label": 1
    },
    "2401.13229": {
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
        "date": "2024-01-24T04:57:32+00:00",
        "label": 1
    },
    "1506.05282": {
        "title": "Why Bother With Syntax?",
        "abstract": "This short note discusses the role of syntax vs. semantics and the interplay\nbetween logic, philosophy, and language in computer science and game theory.",
        "date": "2015-06-17T11:27:12+00:00",
        "label": 0
    },
    "2310.13032": {
        "title": "Quality-Diversity through AI Feedback",
        "abstract": "In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.",
        "date": "2023-10-19T12:13:58+00:00",
        "label": 1
    },
    "2110.14049": {
        "title": "Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning",
        "abstract": "Data Shapley has recently been proposed as a principled framework to quantify\nthe contribution of individual datum in machine learning. It can effectively\nidentify helpful or harmful data points for a learning algorithm. In this\npaper, we propose Beta Shapley, which is a substantial generalization of Data\nShapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the\nShapley value, which is not critical for machine learning settings. Beta\nShapley unifies several popular data valuation methods and includes data\nShapley as a special case. Moreover, we prove that Beta Shapley has several\ndesirable statistical properties and propose efficient algorithms to estimate\nit. We demonstrate that Beta Shapley outperforms state-of-the-art data\nvaluation methods on several downstream ML tasks such as: 1) detecting\nmislabeled training data; 2) learning with subsamples; and 3) identifying\npoints whose addition or removal have the largest positive or negative impact\non the model.",
        "date": "2021-10-26T22:03:55+00:00",
        "label": 1
    },
    "1605.08096": {
        "title": "Proceedings First Workshop on Pre- and Post-Deployment Verification Techniques",
        "abstract": "The PrePost (Pre- and Post-Deployment Verification Techniques) workshop aimed\nat bringing together researchers working in the field of computer-aided\nvalidation and verification to discuss the connections and interplay between\npre- and post-deployment verification techniques. Examples of the topics\ncovered by the workshop are the relationships between classic model checking\nand testing on the one hand and runtime verification and statistical model\nchecking on the other, and between type systems that may be checked either\nstatically or dynamically through techniques such as runtime monitoring.",
        "date": "2016-05-25T22:42:38+00:00",
        "label": 0
    },
    "2007.01852": {
        "title": "Language-agnostic BERT Sentence Embedding",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "date": "2020-07-03T17:58:42+00:00",
        "label": 1
    },
    "2406.09334": {
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models",
        "abstract": "Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper introduces ProxyLM, a scalable framework for predicting LM\nperformance using proxy models in multilingual tasks. These proxy models act as\nsurrogates, approximating the performance of the LM of interest. By leveraging\nproxy models, ProxyLM significantly reduces computational overhead on task\nevaluations, achieving up to a 37.08x speedup compared to traditional methods,\neven with our smallest proxy models. Additionally, our methodology showcases\nadaptability to previously unseen languages in pre-trained LMs, outperforming\nthe state-of-the-art performance by 1.89x as measured by root-mean-square error\n(RMSE). This framework streamlines model selection, enabling efficient\ndeployment and iterative LM enhancements without extensive computational\nresources.",
        "date": "2024-06-13T17:15:33+00:00",
        "label": 1
    },
    "2310.06825": {
        "title": "Mistral 7B",
        "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.",
        "date": "2023-10-10T17:54:58+00:00",
        "label": 1
    },
    "1111.7159": {
        "title": "Sequentiality vs. Concurrency in Games and Logic",
        "abstract": "Connections between the sequentiality/concurrency distinction and the\nsemantics of proofs are investigated, with particular reference to games and\nLinear Logic.",
        "date": "2011-11-30T13:44:46+00:00",
        "label": 0
    },
    "2406.14115": {
        "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
        "abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a\nhigh-quality subset from a given candidate dataset to train a Pending Fine-tune\nModel (PFM) into a Selective-Enhanced Model (SEM). It can improve the model\nperformance and accelerate the training process. Although a few surveys have\ninvestigated related works of data selection, there is a lack of comprehensive\ncomparison between existing methods due to their various experimental settings.\nTo address this issue, we first propose a three-stage scheme for data selection\nand comprehensively review existing works according to this scheme. Then, we\ndesign a unified comparing method with ratio-based efficiency indicators and\nranking-based feasibility indicators to overcome the difficulty of comparing\nvarious models with diverse experimental settings. After an in-depth\ncomparative analysis, we find that the more targeted method with data-specific\nand model-specific quality labels has higher efficiency, but the introduction\nof additional noise information should be avoided when designing selection\nalgorithms. Finally, we summarize the trends in data selection and highlight\nthe short-term and long-term challenges to guide future research.",
        "date": "2024-06-20T08:58:58+00:00",
        "label": 1
    },
    "2404.07840": {
        "title": "On Training Data Influence of GPT Models",
        "abstract": "Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.",
        "date": "2024-04-11T15:27:56+00:00",
        "label": 1
    },
    "2405.12186": {
        "title": "Training Data Attribution via Approximate Unrolled Differentiation",
        "abstract": "Many training data attribution (TDA) methods aim to estimate how a model's\nbehavior would change if one or more data points were removed from the training\nset. Methods based on implicit differentiation, such as influence functions,\ncan be made computationally efficient, but fail to account for\nunderspecification, the implicit bias of the optimization algorithm, or\nmulti-stage training pipelines. By contrast, methods based on unrolling address\nthese issues but face scalability challenges. In this work, we connect the\nimplicit-differentiation-based and unrolling-based approaches and combine their\nbenefits by introducing Source, an approximate unrolling-based TDA method that\nis computed using an influence-function-like formula. While being\ncomputationally efficient compared to unrolling-based approaches, Source is\nsuitable in cases where implicit-differentiation-based approaches struggle,\nsuch as in non-converged models and multi-stage training pipelines.\nEmpirically, Source outperforms existing TDA techniques in counterfactual\nprediction, especially in settings where implicit-differentiation-based\napproaches fall short.",
        "date": "2024-05-20T17:17:44+00:00",
        "label": 1
    },
    "2206.12669": {
        "title": "Crypto Makes AI Evolve",
        "abstract": "Adopting cryptography has given rise to a significant evolution in Artificial\nIntelligence (AI). This paper studies the path and stages of this evolution. We\nstart with reviewing existing relevant surveys, noting their shortcomings,\nespecially the lack of a close look at the evolution process and solid future\nroadmap. These shortcomings justify the work of this paper. Next, we identify,\ndefine and discuss five consequent stages in the evolution path, including\nCrypto-Sensitive AI, Crypto-Adapted AI, Crypto-Friendly AI, Crypto-Enabled AI,\nCrypto-Protected AI. Then, we establish a future roadmap for further research\nin this area, focusing on the role of quantum-inspired and bio-inspired AI.",
        "date": "2022-06-25T15:04:47+00:00",
        "label": 0
    },
    "2401.08565": {
        "title": "Tuning Language Models by Proxy",
        "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. We then demonstrate the generality of\nproxy-tuning by applying it to domain adaptation on code, and task-specific\nfinetuning on question-answering and math problems. Finally, we show how to\nproxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing\nits knowledge about recent events. Our work demonstrates the promise of using\nsmall tuned LMs to efficiently customize large, potentially proprietary LMs\nthrough decoding-time guidance.",
        "date": "2024-01-16T18:49:55+00:00",
        "label": 1
    },
    "1711.04184": {
        "title": "Real-number Computability from the Perspective of Computer Assisted Proofs in Analysis",
        "abstract": "Inspired by computer assisted proofs in analysis, we present an interval\napproach to real-number computations.",
        "date": "2017-11-11T19:25:36+00:00",
        "label": 0
    },
    "1608.05006": {
        "title": "Automaticity in Computation and Student Success in Introductory Physical Science Courses",
        "abstract": "Between 1984 and 2011, the percentage of US bachelor degrees awarded in\nphysics declined by 25%, in chemistry declined by 33%, and overall in physical\nsciences and engineering fell 40%. Data suggest that these declines are\ncorrelated to a deemphasis in most states of practicing computation skills in\nmathematics. Analysis of state standards put into place between 1990 and 2010\nfind that most states directed teachers to deemphasize both memorization and\nstudent practice in computational problem solving. Available state test score\ndata show a significant decline in student computation skills. In recent\ninternational testing, scores for US 16 to 24 year olds in numeracy finished\nlast among 22 tested nations in the OECD. Recent studies in cognitive science\nhave found that to solve well-structured problems in the sciences, students\nmust first memorize fundamental facts and procedures in mathematics and science\nuntil they can be recalled with automaticity, then practice applying those\nskills in a variety of distinctive contexts. Actions are suggested to improve\nUS STEM graduation rates by aligning US math and science curricula with the\nrecommendations of cognitive science.",
        "date": "2016-08-17T16:07:57+00:00",
        "label": 0
    },
    "1804.08293": {
        "title": "Materials science and engineering: New vision in the era of artificial intelligence",
        "abstract": "Scientific discovery evolves from the experimental, through the theoretical\nand computational, to the current data-intensive paradigm. Materials science is\nno exception, especially for computational materials science. In recent years,\ngreat achievements have been made in the field of materials science and\nengineering (MSE). Here, we review the previous paradigms of materials science\nand some classical MSE models. Then, our data-intensive MSE (DIMSE) model is\nproposed to reshape future materials innovations. This work will help to\naddress the global challenge for materials discovery in the era of artificial\nintelligence (AI), and essentially contribute to accelerating future materials\ncontinuum.",
        "date": "2018-04-23T09:01:57+00:00",
        "label": 0
    },
    "2402.13064": {
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "date": "2024-02-20T15:00:35+00:00",
        "label": 1
    },
    "1412.7030": {
        "title": "Proceedings of the 7th European Conference on Python in Science (EuroSciPy 2014)",
        "abstract": "These are the proceedings of the 7th European Conference on Python in\nScience, EuroSciPy 2014, that was held in Cambridge, UK (27-30 August 2014).",
        "date": "2014-12-22T15:47:51+00:00",
        "label": 0
    },
    "1806.03884": {
        "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis",
        "abstract": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
        "date": "2018-06-11T09:44:23+00:00",
        "label": 1
    },
    "2203.15556": {
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "date": "2022-03-29T13:38:03+00:00",
        "label": 1
    },
    "2402.00530": {
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
        "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from\nlow-quality and redundant data. Data filtering for instruction tuning has\nproved important in improving both the efficiency and performance of the tuning\nprocess. But it also leads to extra cost and computation due to the involvement\nof LLMs in this process. To reduce the filtering cost, we study Superfiltering:\nCan we use a smaller and weaker model to select data for finetuning a larger\nand stronger model? Despite the performance gap between weak and strong\nlanguage models, we find their highly consistent capability to perceive\ninstruction difficulty and data selection results. This enables us to use a\nmuch smaller and more efficient model to filter the instruction data used to\ntrain a larger language model. Not only does it largely speed up the data\nfiltering, but the filtered-data-finetuned LLM achieves even better performance\non standard benchmarks. Extensive experiments validate the efficacy and\nefficiency of our approach.",
        "date": "2024-02-01T11:57:53+00:00",
        "label": 1
    },
    "2210.14177": {
        "title": "Influence Functions for Sequence Tagging Models",
        "abstract": "Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.",
        "date": "2022-10-25T17:13:11+00:00",
        "label": 1
    },
    "2003.08529": {
        "title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections",
        "abstract": "Summarizing data samples by quantitative measures has a long history, with\ndescriptive statistics being a case in point. However, as natural language\nprocessing methods flourish, there are still insufficient characteristic\nmetrics to describe a collection of texts in terms of the words, sentences, or\nparagraphs they comprise. In this work, we propose metrics of diversity,\ndensity, and homogeneity that quantitatively measure the dispersion, sparsity,\nand uniformity of a text collection. We conduct a series of simulations to\nverify that each metric holds desired properties and resonates with human\nintuitions. Experiments on real-world datasets demonstrate that the proposed\ncharacteristic metrics are highly correlated with text classification\nperformance of a renowned model, BERT, which could inspire future applications.",
        "date": "2020-03-19T00:48:32+00:00",
        "label": 1
    },
    "1405.5593": {
        "title": "Saturation algorithms for model-checking pushdown systems",
        "abstract": "We present a survey of the saturation method for model-checking pushdown\nsystems.",
        "date": "2014-05-22T02:11:30+00:00",
        "label": 0
    },
    "1707.05589": {
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.",
        "date": "2017-07-18T12:35:53+00:00",
        "label": 1
    },
    "1103.1977": {
        "title": "Development of Computer Science Disciplines - A Social Network Analysis Approach",
        "abstract": "In contrast to many other scientific disciplines, computer science considers\nconference publications. Conferences have the advantage of providing fast\npublication of papers and of bringing researchers together to present and\ndiscuss the paper with peers. Previous work on knowledge mapping focused on the\nmap of all sciences or a particular domain based on ISI published JCR (Journal\nCitation Report). Although this data covers most of important journals, it\nlacks computer science conference and workshop proceedings. That results in an\nimprecise and incomplete analysis of the computer science knowledge. This paper\npresents an analysis on the computer science knowledge network constructed from\nall types of publications, aiming at providing a complete view of computer\nscience research. Based on the combination of two important digital libraries\n(DBLP and CiteSeerX), we study the knowledge network created at\njournal/conference level using citation linkage, to identify the development of\nsub-disciplines. We investigate the collaborative and citation behavior of\njournals/conferences by analyzing the properties of their co-authorship and\ncitation subgraphs. The paper draws several important conclusions. First,\nconferences constitute social structures that shape the computer science\nknowledge. Second, computer science is becoming more interdisciplinary. Third,\nexperts are the key success factor for sustainability of journals/conferences.",
        "date": "2011-03-10T09:51:19+00:00",
        "label": 0
    },
    "2403.02839": {
        "title": "On the Limitations of Fine-tuned Judge Models for LLM Evaluation",
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this study, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\npropose an effective indicator to measure the reliability of fine-tuned judges,\nwith the aim of maximizing their utility in LLM evaluation.",
        "date": "2024-03-05T10:20:52+00:00",
        "label": 1
    },
    "2308.07201": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "date": "2023-08-14T15:13:04+00:00",
        "label": 1
    },
    "1404.5611": {
        "title": "IMP Science Gateway: from the Portal to the Hub of Virtual Experimental Labs in Materials Science",
        "abstract": "\"Science gateway\" (SG) ideology means a user-friendly intuitive interface\nbetween scientists (or scientific communities) and different software\ncomponents + various distributed computing infrastructures (DCIs) (like grids,\nclouds, clusters), where researchers can focus on their scientific goals and\nless on peculiarities of software/DCI. \"IMP Science Gateway Portal\"\n(http://scigate.imp.kiev.ua) for complex workflow management and integration of\ndistributed computing resources (like clusters, service grids, desktop grids,\nclouds) is presented. It is created on the basis of WS-PGRADE and gUSE\ntechnologies, where WS-PGRADE is designed for science workflow operation and\ngUSE - for smooth integration of available resources for parallel and\ndistributed computing in various heterogeneous distributed computing\ninfrastructures (DCI). The typical scientific workflows with possible scenarios\nof its preparation and usage are presented. Several typical use cases for these\nscience applications (scientific workflows) are considered for molecular\ndynamics (MD) simulations of complex behavior of various nanostructures\n(nanoindentation of graphene layers, defect system relaxation in metal\nnanocrystals, thermal stability of boron nitride nanotubes, etc.). The user\nexperience is analyzed in the context of its practical applications for MD\nsimulations in materials science, physics and nanotechnologies with available\nheterogeneous DCIs. In conclusion, the \"science gateway\" approach - workflow\nmanager (like WS-PGRADE) + DCI resources manager (like gUSE)- gives opportunity\nto use the SG portal (like \"IMP Science Gateway Portal\") in a very promising\nway, namely, as a hub of various virtual experimental labs (different software\ncomponents + various requirements to resources) in the context of its practical\nMD applications in materials science, physics, chemistry, biology, and\nnanotechnologies.",
        "date": "2014-04-22T11:55:17+00:00",
        "label": 0
    },
    "2402.16827": {
        "title": "A Survey on Data Selection for Language Models",
        "abstract": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.",
        "date": "2024-02-26T18:54:35+00:00",
        "label": 1
    },
    "1009.2416": {
        "title": "Computational Science and Innovation",
        "abstract": "Simulations - utilizing computers to solve complicated science and\nengineering problems - are a key ingredient of modern science. The U.S.\nDepartment of Energy (DOE) is a world leader in the development of\nhigh-performance computing (HPC), the development of applied math and\nalgorithms that utilize the full potential of HPC platforms, and the\napplication of computing to science and engineering problems. An interesting\ngeneral question is whether the DOE can strategically utilize its capability in\nsimulations to advance innovation more broadly. In this article, I will argue\nthat this is certainly possible.",
        "date": "2010-09-13T15:15:07+00:00",
        "label": 0
    },
    "2408.03955": {
        "title": "Computational Modelling for Combinatorial Game Strategies",
        "abstract": "We develop a generic computational model that can be used effectively for\nestablishing the existence of winning strategies for concrete finite\ncombinatorial games. Our modelling is (equational) logic-based involving\nadvanced techniques from algebraic specification, and it can be executed by\nequational programming systems such as those from the OBJ-family. We show how\nthis provides a form of experimental mathematics for strategy problems\ninvolving combinatorial games.",
        "date": "2024-07-23T13:59:41+00:00",
        "label": 0
    },
    "2211.09110": {
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.",
        "date": "2022-11-16T18:51:34+00:00",
        "label": 1
    },
    "2302.12366": {
        "title": "Less is More: Data Pruning for Faster Adversarial Training",
        "abstract": "Deep neural networks (DNNs) are sensitive to adversarial examples, resulting\nin fragile and unreliable performance in the real world. Although adversarial\ntraining (AT) is currently one of the most effective methodologies to robustify\nDNNs, it is computationally very expensive (e.g., 5-10X costlier than standard\ntraining). To address this challenge, existing approaches focus on single-step\nAT, referred to as Fast AT, reducing the overhead of adversarial example\ngeneration. Unfortunately, these approaches are known to fail against stronger\nadversaries. To make AT computationally efficient without compromising\nrobustness, this paper takes a different view of the efficient AT problem.\nSpecifically, we propose to minimize redundancies at the data level by\nleveraging data pruning. Extensive experiments demonstrate that the data\npruning based AT can achieve similar or superior robust (and clean) accuracy as\nits unpruned counterparts while being significantly faster. For instance,\nproposed strategies accelerate CIFAR-10 training up to 3.44X and CIFAR-100\ntraining to 2.02X. Additionally, the data pruning methods can readily be\nreconciled with existing adversarial acceleration tricks to obtain the striking\nspeed-ups of 5.66X and 5.12X on CIFAR-10, 3.67X and 3.07X on CIFAR-100 with\nTRADES and MART, respectively.",
        "date": "2023-02-23T23:48:20+00:00",
        "label": 1
    },
    "2403.08763": {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "date": "2024-03-13T17:58:57+00:00",
        "label": 1
    },
    "2311.09783": {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "abstract": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "date": "2023-11-16T11:03:04+00:00",
        "label": 1
    },
    "1401.4973": {
        "title": "What are the fundamental structures of concurrency? We still don't know!",
        "abstract": "Process algebra has been successful in many ways; but we don't yet see the\nlineaments of a fundamental theory. Some fleeting glimpses are sought from\nPetri Nets, physics and geometry.",
        "date": "2014-01-20T16:35:23+00:00",
        "label": 0
    },
    "1211.3476": {
        "title": "Proceedings 6th Workshop on Membrane Computing and Biologically Inspired Process Calculi",
        "abstract": "This volume contains the papers presented at the 6th Membrane Computing and\nBiologically Inspired Process Calculi (MeCBIC 2012), a satellite workshop of\nthe 23rd International Conference on Concurrency Theory (CONCUR) held on 8th\nSeptember 2012 in Newcastle upon Tyne, UK.",
        "date": "2012-11-15T01:33:41+00:00",
        "label": 0
    },
    "2311.00288": {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results\nby training large language models (LLMs) on a massive amount of diverse tasks\nwith instructions. However, how to select new tasks to improve the performance\nand generalizability of IT models remains an open question. Training on all\nexisting tasks is impractical due to prohibiting computation requirements, and\nrandomly selecting tasks can lead to suboptimal performance. In this work, we\npropose active instruction tuning based on prompt uncertainty, a novel\nframework to identify informative tasks, and then actively tune the models on\nthe selected tasks. We represent the informativeness of new tasks with the\ndisagreement of the current model outputs over perturbed prompts. Our\nexperiments on NIV2 and Self-Instruct datasets demonstrate that our method\nconsistently outperforms other baseline strategies for task selection,\nachieving better out-of-distribution generalization with fewer training tasks.\nAdditionally, we introduce a task map that categorizes and diagnoses tasks\nbased on prompt uncertainty and prediction probability. We discover that\ntraining on ambiguous (prompt-uncertain) tasks improves generalization while\ntraining on difficult (prompt-certain and low-probability) tasks offers no\nbenefit, underscoring the importance of task selection for instruction tuning.",
        "date": "2023-11-01T04:40:05+00:00",
        "label": 1
    },
    "2303.09540": {
        "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
        "abstract": "Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.",
        "date": "2023-03-16T17:53:24+00:00",
        "label": 1
    },
    "2308.06259": {
        "title": "Self-Alignment with Instruction Backtranslation",
        "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.",
        "date": "2023-08-11T17:47:54+00:00",
        "label": 1
    },
    "2103.10489": {
        "title": "Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective",
        "abstract": "From a computer science perspective, addressing on-line hate speech is a\nchallenging task that is attracting the attention of both industry (mainly\nsocial media platform owners) and academia. In this chapter, we provide an\noverview of state-of-the-art data-science approaches - how they define hate\nspeech, which tasks they solve to mitigate the phenomenon, and how they address\nthese tasks. We limit our investigation mostly to (semi-)automatic detection of\nhate speech, which is the task that the majority of existing computer science\nworks focus on. Finally, we summarize the challenges and the open problems in\nthe current data-science research and the future directions in this field. Our\naim is to prepare an easily understandable report, capable to promote the\nmultidisciplinary character of hate speech research. Researchers from other\ndomains (e.g., psychology and sociology) can thus take advantage of the\nknowledge achieved in the computer science domain but also contribute back and\nhelp improve how computer science is addressing that urgent and socially\nrelevant issue which is the prevalence of hate speech in social media.",
        "date": "2021-03-18T19:19:44+00:00",
        "label": 0
    },
    "1506.00555": {
        "title": "Writing and Publishing Scientific Articles in Computer Science",
        "abstract": "Over 15 years of teaching, advising students and coordinating scientific\nresearch activities and projects in computer science, we have observed the\ndifficulties of students to write scientific papers to present the results of\ntheir research practices. In addition, they repeatedly have doubts about the\npublishing process. In this article we propose a conceptual framework to\nsupport the writing and publishing of scientific papers in computer science,\nproviding a kind of guide for computer science students to effectively present\nthe results of their research practices, particularly for experimental\nresearch.",
        "date": "2015-06-01T16:09:53+00:00",
        "label": 0
    },
    "1512.02985": {
        "title": "On Variants of k-means Clustering",
        "abstract": "\\textit{Clustering problems} often arise in the fields like data mining,\nmachine learning etc. to group a collection of objects into similar groups with\nrespect to a similarity (or dissimilarity) measure. Among the clustering\nproblems, specifically \\textit{$k$-means} clustering has got much attention\nfrom the researchers. Despite the fact that $k$-means is a very well studied\nproblem its status in the plane is still an open problem. In particular, it is\nunknown whether it admits a PTAS in the plane. The best known approximation\nbound in polynomial time is $9+\\eps$.\n  In this paper, we consider the following variant of $k$-means. Given a set\n$C$ of points in $\\mathcal{R}^d$ and a real $f > 0$, find a finite set $F$ of\npoints in $\\mathcal{R}^d$ that minimizes the quantity $f*|F|+\\sum_{p\\in C}\n\\min_{q \\in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local\nsearch PTAS for this problem. We also give a \"bi-criterion\" local search\nalgorithm for $k$-means which uses $(1+\\eps)k$ centers and yields a solution\nwhose cost is at most $(1+\\eps)$ times the cost of an optimal $k$-means\nsolution. The algorithm runs in polynomial time for any fixed dimension.\n  The contribution of this paper is two fold. On the one hand, we are being\nable to handle the square of distances in an elegant manner, which yields near\noptimal approximation bound. This leads us towards a better understanding of\nthe $k$-means problem. On the other hand, our analysis of local search might\nalso be useful for other geometric problems. This is important considering that\nvery little is known about the local search method for geometric approximation.",
        "date": "2015-12-09T18:37:49+00:00",
        "label": 1
    },
    "2406.14491": {
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
        "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
        "date": "2024-06-20T16:55:33+00:00",
        "label": 1
    },
    "2007.03606": {
        "title": "Data Science: A Comprehensive Overview",
        "abstract": "The twenty-first century has ushered in the age of big data and data economy,\nin which data DNA, which carries important knowledge, insights and potential,\nhas become an intrinsic constituent of all data-based organisms. An appropriate\nunderstanding of data DNA and its organisms relies on the new field of data\nscience and its keystone, analytics. Although it is widely debated whether big\ndata is only hype and buzz, and data science is still in a very early phase,\nsignificant challenges and opportunities are emerging or have been inspired by\nthe research, innovation, business, profession, and education of data science.\nThis paper provides a comprehensive survey and tutorial of the fundamental\naspects of data science: the evolution from data analysis to data science, the\ndata science concepts, a big picture of the era of data science, the major\nchallenges and directions in data innovation, the nature of data analytics, new\nindustrialization and service opportunities in the data economy, the profession\nand competency of data education, and the future of data science. This article\nis the first in the field to draw a comprehensive big picture, in addition to\noffering rich observations, lessons and thinking about data science and\nanalytics.",
        "date": "2020-07-01T02:33:58+00:00",
        "label": 0
    },
    "1111.4755": {
        "title": "Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive Case",
        "abstract": "This paper describes the solution of Hello World transformations in MOLA\ntransformation language. Transformations implementing the task are relatively\nstraightforward and easily inferable from the task specification. The required\nadditional steps related to model import and export are also described.",
        "date": "2011-11-21T05:26:57+00:00",
        "label": 0
    },
    "1106.2769": {
        "title": "Co-c.e. spheres and cells in computable metric spaces",
        "abstract": "We investigate conditions under which a co-computably enumerable set in a\ncomputable metric space is computable. Using higher-dimensional chains and\nspherical chains we prove that in each computable metric space which is locally\ncomputable each co-computably enumerable sphere is computable and each co-c.e.\ncell with co-c.e. boundary sphere is computable.",
        "date": "2011-06-14T17:46:06+00:00",
        "label": 0
    },
    "1003.5716": {
        "title": "Proceedings First International Workshop on Linearity",
        "abstract": "This volume contains the proceedings of LINEARITY 2009: the first\nInternational Workshop on Linearity, which took place 12th September 2009 in\nCoimbra, Portugal. The workshop was a satellite event of CSL 2009, the 18th\nEACSL Annual Conference on Computer Science Logic.",
        "date": "2010-03-30T01:48:23+00:00",
        "label": 0
    },
    "1805.05401": {
        "title": "Building Data Science Capabilities into University Data Warehouse to Predict Graduation",
        "abstract": "The discipline of data science emerged to combine statistical methods with\ncomputing. At Aalto University, Finland, we have taken first steps to bring\neducational data science as a part of daily operations of Management\nInformation Services. This required changes in IT environment: we enhanced data\nwarehouse infrastructure with a data science lab, where we can read predictive\nmodel training data from data warehouse database and use the created predictive\nmodels in database queries. We then conducted a data science pilot with an\nobjective to predict students' graduation probability and time-to-degree with\nstudent registry data. Further ethical and legal considerations are needed\nbefore using predictions in daily operations of the university.",
        "date": "2018-05-04T12:28:03+00:00",
        "label": 0
    },
    "2306.11644": {
        "title": "Textbooks Are All You Need",
        "abstract": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.",
        "date": "2023-06-20T16:14:25+00:00",
        "label": 1
    },
    "2305.09246": {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
        "abstract": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "date": "2023-05-16T07:52:57+00:00",
        "label": 1
    },
    "2406.14026": {
        "title": "Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations",
        "abstract": "Language models (LMs) are known to suffer from forgetting of previously\nlearned examples when fine-tuned, breaking stability of deployed LM systems.\nDespite efforts on mitigating forgetting, few have investigated whether, and\nhow forgotten upstream examples are associated with newly learned tasks.\nInsights on such associations enable efficient and targeted mitigation of\nforgetting. In this paper, we empirically analyze forgetting that occurs in $N$\nupstream examples while the model learns $M$ new tasks and visualize their\nassociations with a $M \\times N$ matrix. We empirically demonstrate that the\ndegree of forgetting can often be approximated by simple multiplicative\ncontributions of the upstream examples and newly learned tasks. We also reveal\nmore complicated patterns where specific subsets of examples are forgotten with\nstatistics and visualization. Following our analysis, we predict forgetting\nthat happens on upstream examples when learning a new task with matrix\ncompletion over the empirical associations, outperforming prior approaches that\nrely on trainable LMs. Project website:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
        "date": "2024-06-20T06:46:23+00:00",
        "label": 1
    },
    "1908.01091": {
        "title": "Toward Understanding Catastrophic Forgetting in Continual Learning",
        "abstract": "We study the relationship between catastrophic forgetting and properties of\ntask sequences. In particular, given a sequence of tasks, we would like to\nunderstand which properties of this sequence influence the error rates of\ncontinual learning algorithms trained on the sequence. To this end, we propose\na new procedure that makes use of recent developments in task space modeling as\nwell as correlation analysis to specify and analyze the properties we are\ninterested in. As an application, we apply our procedure to study two\nproperties of a task sequence: (1) total complexity and (2) sequential\nheterogeneity. We show that error rates are strongly and positively correlated\nto a task sequence's total complexity for some state-of-the-art algorithms. We\nalso show that, surprisingly, the error rates have no or even negative\ncorrelations in some cases to sequential heterogeneity. Our findings suggest\ndirections for improving continual learning benchmarks and methods.",
        "date": "2019-08-02T23:30:35+00:00",
        "label": 1
    },
    "1909.03033": {
        "title": "The Difficulties of Addressing Interdisciplinary Challenges at the Foundations of Data Science",
        "abstract": "The National Science Foundation's Transdisciplinary Research in Principles of\nData Science (TRIPODS) program aims to integrate three areas central to the\nfoundations of data by uniting the statistics, mathematics, and theoretical\ncomputer science research communities. The program aims to provide a model for\nfunding cross-cutting research and facilitating interactions among the three\ndisciplines. Challenges associated with orchestrating fruitful interactions are\ndescribed.",
        "date": "2019-09-04T06:07:26+00:00",
        "label": 0
    },
    "2305.11383": {
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
        "abstract": "Recent works on instruction tuning (IT) have achieved great performance with\nzero-shot generalizability to unseen tasks. With additional context (e.g., task\ndefinition, examples) provided to models for fine-tuning, they achieved much\nhigher performance than untuned models. Despite impressive performance gains,\nwhat models learn from IT remains understudied. In this work, we analyze how\nmodels utilize instructions during IT by comparing model training with altered\nvs. original instructions. Specifically, we create simplified task definitions\nby removing all semantic components and only leaving the output space\ninformation, and delusive examples that contain incorrect input-output mapping.\nOur experiments show that models trained on simplified task definition or\ndelusive examples can achieve comparable performance to the ones trained on the\noriginal instructions and examples. Furthermore, we introduce a random baseline\nto perform zeroshot classification tasks, and find it achieves similar\nperformance (42.6% exact-match) as IT does (43% exact-match) in low resource\nsetting, while both methods outperform naive T5 significantly (30% per\nexact-match). Our analysis provides evidence that the impressive performance\ngain of current IT models can come from picking up superficial patterns, such\nas learning the output format and guessing. Our study highlights the urgent\nneed for more reliable IT methods and evaluation.",
        "date": "2023-05-19T02:00:47+00:00",
        "label": 1
    },
    "2203.08242": {
        "title": "Data Contamination: From Memorization to Exploitation",
        "abstract": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "date": "2022-03-15T20:37:16+00:00",
        "label": 1
    },
    "2401.06059": {
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.",
        "date": "2024-01-11T17:24:49+00:00",
        "label": 1
    },
    "2309.04564": {
        "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale",
        "abstract": "Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.",
        "date": "2023-09-08T19:34:05+00:00",
        "label": 1
    },
    "2308.03296": {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "abstract": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.",
        "date": "2023-08-07T04:47:42+00:00",
        "label": 1
    },
    "2405.09265": {
        "title": "Quantum Computing Education for Computer Science Students: Bridging the Gap with Layered Learning and Intuitive Analogies",
        "abstract": "Quantum computing presents a transformative potential for the world of\ncomputing. However, integrating this technology into the curriculum for\ncomputer science students who lack prior exposure to quantum mechanics and\nadvanced mathematics remains a challenging task. This paper proposes a\nscaffolded learning approach aimed at equipping computer science students with\nessential quantum principles. By introducing foundational quantum concepts\nthrough relatable analogies and a layered learning approach based on classical\ncomputation, this approach seeks to bridge the gap between classical and\nquantum computing. This differs from previous approaches which build quantum\ncomputing fundamentals from the prerequisite of linear algebra and mathematics.\nThe paper offers a considered set of intuitive analogies for foundation quantum\nconcepts including entanglement, superposition, quantum data structures and\nquantum algorithms. These analogies coupled with a computing-based layered\nlearning approach, lay the groundwork for a comprehensive teaching methodology\ntailored for undergraduate third level computer science students.",
        "date": "2024-05-15T11:28:49+00:00",
        "label": 0
    },
    "2001.08361": {
        "title": "Scaling Laws for Neural Language Models",
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "date": "2020-01-23T03:59:20+00:00",
        "label": 1
    },
    "2305.00347": {
        "title": "Positionality of mean-payoff games on infinite graphs",
        "abstract": "This short note establishes positionality of mean-payoff games over infinite\ngame graphs by constructing a well-founded monotone universal graph.",
        "date": "2023-04-29T21:43:31+00:00",
        "label": 0
    },
    "2307.08701": {
        "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
        "abstract": "Large language models (LLMs) strengthen instruction-following capability\nthrough instruction-finetuning (IFT) on supervised instruction/response data.\nHowever, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly\ncontain many low-quality instances with incorrect or irrelevant responses,\nwhich are misleading and detrimental to IFT. In this paper, we propose a simple\nand effective data selection strategy that automatically identifies and filters\nout low-quality data using a strong LLM (e.g., ChatGPT). To this end, we\nintroduce AlpaGasus, which is finetuned on only 9k high-quality data filtered\nfrom the 52k Alpaca data. AlpaGasus significantly outperforms the original\nAlpaca as evaluated by GPT-4 on multiple test sets and the controlled human\nevaluation. Its 13B variant matches $>90\\%$ performance of its teacher LLM\n(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the\nefficacy of our method across diverse datasets, base models, and LLM filters.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\nhttps://lichang-chen.github.io/AlpaGasus/",
        "date": "2023-07-17T17:59:40+00:00",
        "label": 1
    },
    "1404.6487": {
        "title": "Computability of 1-manifolds",
        "abstract": "A semi-computable set S in a computable metric space need not be computable.\nHowever, in some cases, if S has certain topological properties, we can\nconclude that S is computable. It is known that if a semi-computable set S is a\ncompact manifold with boundary, then the computability of \\deltaS implies the\ncomputability of S. In this paper we examine the case when S is a 1-manifold\nwith boundary, not necessarily compact. We show that a similar result holds in\nthis case under assumption that S has finitely many components.",
        "date": "2014-04-25T17:37:44+00:00",
        "label": 0
    },
    "2207.01934": {
        "title": "How sustainable is \"common\" data science in terms of power consumption?",
        "abstract": "Continuous developments in data science have brought forth an exponential\nincrease in complexity of machine learning models. Additionally, data\nscientists have become ubiquitous in the private market, academic environments\nand even as a hobby. All of these trends are on a steady rise, and are\nassociated with an increase in power consumption and associated carbon\nfootprint. The increasing carbon footprint of large-scale advanced data science\nhas already received attention, but the latter trend has not. This work aims to\nestimate the contribution of the increasingly popular \"common\" data science to\nthe global carbon footprint. To this end, the power consumption of several\ntypical tasks in the aforementioned common data science tasks will be measured\nand compared to: large-scale \"advanced\" data science, common computer-related\ntasks, and everyday non-computer related tasks. This is done by converting the\nmeasurements to the equivalent unit of \"km driven by car\". Our main findings\nare: \"common\" data science consumes $2.57$ more power than regular computer\nusage, but less than some common everyday power-consuming tasks such as\nlighting or heating; large-scale data science consumes substantially more power\nthan common data science.",
        "date": "2022-07-05T10:15:22+00:00",
        "label": 0
    },
    "2406.13542": {
        "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
        "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
        "date": "2024-06-19T13:29:53+00:00",
        "label": 1
    },
    "2303.08774": {
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "date": "2023-03-15T17:15:04+00:00",
        "label": 1
    },
    "1904.03122": {
        "title": "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        "abstract": "In a corpus of data, outliers are either errors: mistakes in the data that\nare counterproductive, or are unique: informative samples that improve model\nrobustness. Identifying outliers can lead to better datasets by (1) removing\nnoise in datasets and (2) guiding collection of additional data to fill gaps.\nHowever, the problem of detecting both outlier types has received relatively\nlittle attention in NLP, particularly for dialog systems. We introduce a simple\nand effective technique for detecting both erroneous and unique samples in a\ncorpus of short texts using neural sentence embeddings combined with\ndistance-based outlier detection. We also present a novel data collection\npipeline built atop our detection technique to automatically and iteratively\nmine unique data samples while discarding erroneous samples. Experiments show\nthat our outlier detection technique is effective at finding errors while our\ndata collection pipeline yields highly diverse corpora that in turn produce\nmore robust intent classification and slot-filling models.",
        "date": "2019-04-05T15:31:28+00:00",
        "label": 1
    },
    "2401.04088": {
        "title": "Mixtral of Experts",
        "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
        "date": "2024-01-08T18:47:34+00:00",
        "label": 1
    },
    "2311.10774": {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "abstract": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.",
        "date": "2023-11-15T23:36:42+00:00",
        "label": 1
    },
    "2308.13539": {
        "title": "Redefining Computer Science Education: Code-Centric to Natural Language Programming with AI-Based No-Code Platforms",
        "abstract": "This paper delves into the evolving relationship between humans and computers\nin the realm of programming. Historically, programming has been a dialogue\nwhere humans meticulously crafted communication to suit machine understanding,\nshaping the trajectory of computer science education. However, the advent of\nAI-based no-code platforms is revolutionizing this dynamic. Now, humans can\nconverse in their natural language, expecting machines to interpret and act.\nThis shift has profound implications for computer science education. As\neducators, it's imperative to integrate this new dynamic into curricula. In\nthis paper, we've explored several pertinent research questions in this\ntransformation, which demand continued inquiry and adaptation in our\neducational strategies.",
        "date": "2023-08-19T02:44:35+00:00",
        "label": 0
    },
    "2010.00506": {
        "title": "Edsger Dijkstra. The Man Who Carried Computer Science on His Shoulders",
        "abstract": "This a biographical essay about Edsger Wybe Dijkstra.",
        "date": "2020-10-01T15:55:51+00:00",
        "label": 0
    },
    "2203.14544": {
        "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning",
        "abstract": "The goal of continual learning (CL) is to efficiently update a machine\nlearning model with new data without forgetting previously-learned knowledge.\nMost widely-used CL methods rely on a rehearsal memory of data points to be\nreused while training on new data. Curating such a rehearsal memory to maintain\na small, informative subset of all the data seen so far is crucial to the\nsuccess of these methods. We devise a coreset selection method for\nrehearsal-based continual learning. Our method is based on the idea of gradient\nmatching: The gradients induced by the coreset should match, as closely as\npossible, those induced by the original training dataset. Inspired by the\nneural tangent kernel theory, we perform this gradient matching across the\nmodel's initialization distribution, allowing us to extract a coreset without\nhaving to train the model first. We evaluate the method on a wide range of\ncontinual learning scenarios and demonstrate that it improves the performance\nof rehearsal-based CL methods compared to competing memory management\nstrategies such as reservoir sampling.",
        "date": "2022-03-28T07:37:17+00:00",
        "label": 1
    },
    "2210.12528": {
        "title": "Data science transfer pathways from associate's to bachelor's programs",
        "abstract": "A substantial fraction of students who complete their college education at a\npublic university in the United States begin their journey at one of the 935\npublic two-year colleges. While the number of four-year colleges offering\nbachelor's degrees in data science continues to increase, data science\ninstruction at many two-year colleges lags behind. A major impediment is the\nrelative paucity of introductory data science courses that serve multiple\nstudent audiences and can easily transfer. In addition, the lack of pre-defined\ntransfer pathways (or articulation agreements) for data science creates a\ngrowing disconnect that leaves students who want to study data science at a\ndisadvantage. We describe opportunities and barriers to data science transfer\npathways. Five points of curricular friction merit attention: 1) a first course\nin data science, 2) a second course in data science, 3) a course in scientific\ncomputing, data science workflow, and/or reproducible computing, 4) lab\nsciences, and 5) navigating communication, ethics, and application domain\nrequirements in the context of general education and liberal arts course\nmappings. We catalog existing transfer pathways, efforts to align curricula\nacross institutions, obstacles to overcome with minimally-disruptive solutions,\nand approaches to foster these pathways. Improvements in these areas are\ncritically important to ensure that a broad and diverse set of students are\nable to engage and succeed in undergraduate data science programs.",
        "date": "2022-10-22T19:07:08+00:00",
        "label": 0
    },
    "2208.01545": {
        "title": "The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence",
        "abstract": "Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by 1. proposing a novel metric\n-- the diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and\ntransfer learning under fair conditions (same architecture, same optimizer, and\nall models trained to convergence). Using the diversity coefficient, we show\nthat the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have\nlow diversity. This novel insight contextualizes claims that transfer learning\nsolutions are better than meta-learned solutions in the regime of low diversity\nunder a fair comparison. Specifically, we empirically find that a low diversity\ncoefficient correlates with a high similarity between transfer learning and\nMAML learned solutions in terms of accuracy at meta-test time and\nclassification layer similarity (using feature based distance metrics like\nSVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this\nmeta-test accuracy holds even as the model size changes. Therefore, we conclude\nthat in the low diversity regime, MAML and transfer learning have equivalent\nmeta-test performance when both are compared fairly. We also hope our work\ninspires more thoughtful constructions and quantitative evaluations of\nmeta-learning benchmarks in the future.",
        "date": "2022-08-02T15:49:11+00:00",
        "label": 1
    },
    "1404.5458": {
        "title": "Complex Workflow Management and Integration of Distributed Computing Resources by Science Gateway Portal for Molecular Dynamics Simulations in Materials Science",
        "abstract": "The \"IMP Science Gateway Portal\" (http://scigate.imp.kiev.ua) for complex\nworkflow management and integration of distributed computing resources (like\nclusters, service grids, desktop grids, clouds) is presented. It is created on\nthe basis of WS-PGRADE and gUSE technologies, where WS-PGRADE is designed for\nscience workflow operation and gUSE - for smooth integration of available\nresources for parallel and distributed computing in various heterogeneous\ndistributed computing infrastructures (DCI). The typical scientific workflow\nwith possible scenarios of its preparation and usage is considered. Several\ntypical science applications (scientific workflows) are considered for\nmolecular dynamics (MD) simulations of complex behavior of various\nnanostructures (nanoindentation of graphene layers, defect system relaxation in\nmetal nanocrystals, thermal stability of boron nitride nanotubes, etc.). The\nadvantages and drawbacks of the solution are shortly analyzed in the context of\nits practical applications for MD simulations in materials science, physics and\nnanotechnologies with available heterogeneous DCIs.",
        "date": "2014-04-22T11:34:04+00:00",
        "label": 0
    },
    "1607.03760": {
        "title": "Distributed Games and Strategies",
        "abstract": "A summary of work on distributed games and strategies done within the first\nthree years of the ERC project ECSYM is presented.",
        "date": "2016-07-13T14:25:03+00:00",
        "label": 0
    },
    "2105.03075": {
        "title": "A Survey of Data Augmentation Approaches for NLP",
        "abstract": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
        "date": "2021-05-07T06:03:45+00:00",
        "label": 1
    },
    "2207.07901": {
        "title": "Computer Science",
        "abstract": "Possible for science itself, conceptually, to have and will understand\ndifferently, let alone science also seen as technology, such as computer\nscience. After all, science and technology are viewpoints diverse by either\nindividual, community, or social. Generally, it depends on socioeconomic\ncapabilities. So it is with computer science has become a phenomenon and\nfashionable, where based on the stream of documents, various issues arise in\neither its theory or implementation, adapting different communities, or\ndesigning curriculum holds in the education system.",
        "date": "2022-07-16T10:54:57+00:00",
        "label": 0
    },
    "1904.13112": {
        "title": "On the incomputability of computable dimension",
        "abstract": "Using an iterative tree construction we show that for simple computable\nsubsets of the Cantor space Hausdorff, constructive and computable dimensions\nmight be incomputable.",
        "date": "2019-04-30T09:07:37+00:00",
        "label": 0
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
        "date": "2019-07-26T17:48:29+00:00",
        "label": 1
    },
    "2405.20541": {
        "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
        "abstract": "In this work, we investigate whether small language models can determine\nhigh-quality subsets of large-scale text datasets that improve the performance\nof larger language models. While existing work has shown that pruning based on\nthe perplexity of a larger model can yield high-quality data, we investigate\nwhether smaller models can be used for perplexity-based pruning and how pruning\nis affected by the domain composition of the data being pruned. We demonstrate\nthat for multiple dataset compositions, perplexity-based pruning of pretraining\ndata can \\emph{significantly} improve downstream task performance: pruning\nbased on perplexities computed with a 125 million parameter model improves the\naverage performance on downstream tasks of a 3 billion parameter model by up to\n2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach\ncommensurate baseline performance. Furthermore, we demonstrate that such\nperplexity-based data pruning also yields downstream performance gains in the\nover-trained and data-constrained regimes.",
        "date": "2024-05-30T23:50:20+00:00",
        "label": 1
    },
    "2308.12032": {
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction\ndata quality and quantity is a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from open-source datasets, effectively minimizing manual curation and\npotential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to\nidentify discrepancies between a model's expected responses and its intrinsic\ngeneration capability. Through the application of IFD, cherry samples can be\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on datasets like Alpaca and WizardLM underpin our findings; with a\nmere $10\\%$ of original data input, our strategy showcases improved results.\nThis synthesis of self-guided cherry-picking and the IFD metric signifies a\ntransformative leap in the instruction tuning of LLMs, promising both\nefficiency and resource-conscious advancements. Codes, data, and models are\navailable: https://github.com/tianyi-lab/Cherry_LLM",
        "date": "2023-08-23T09:45:29+00:00",
        "label": 1
    },
    "2306.13840": {
        "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
        "abstract": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.",
        "date": "2023-06-24T02:25:56+00:00",
        "label": 1
    },
    "2002.06305": {
        "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
        "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.",
        "date": "2020-02-15T02:40:10+00:00",
        "label": 1
    },
    "0609070": {
        "title": "Exploring Computer Science Concepts with a Ready-made Computer Game Framework",
        "abstract": "Leveraging the prevailing interest in computer games among college students,\nboth for entertainment and as a possible career path, is a major reason for the\nincreasing prevalence of computer game design courses in computer science\ncurricula. Because implementing a computer game requires strong programming\nskills, game design courses are most often restricted to more advanced computer\nscience students. This paper reports on a ready-made game design and\nexperimentation framework, implemented in Java, that makes game programming\nmore widely accessible. This framework, called Labyrinth, enables students at\nall programming skill levels to participate in computer game design. We\ndescribe the architecture of the framework, and discuss programming projects\nsuitable for a wide variety of computer science courses, from capstone to\nnon-major.",
        "date": "2006-09-12T19:49:55+00:00",
        "label": 0
    },
    "2308.09621": {
        "title": "Canonicity and Computability in Homotopy Type Theory",
        "abstract": "This dissertation gives an overview of Martin Lof's dependant type theory,\nfocusing on its computational content and addressing a question of possibility\nof fully canonical and computable semantic presentation.",
        "date": "2023-08-18T15:23:33+00:00",
        "label": 0
    },
    "2006.16964": {
        "title": "Data Science: Nature and Pitfalls",
        "abstract": "Data science is creating very exciting trends as well as significant\ncontroversy. A critical matter for the healthy development of data science in\nits early stages is to deeply understand the nature of data and data science,\nand to discuss the various pitfalls. These important issues motivate the\ndiscussions in this article.",
        "date": "2020-06-28T02:06:54+00:00",
        "label": 0
    },
    "1802.03292": {
        "title": "Mathematical Logic in Computer Science",
        "abstract": "The article retraces major events and milestones in the mutual influences\nbetween mathematical logic and computer science since the 1950s.",
        "date": "2018-02-07T22:21:43+00:00",
        "label": 0
    },
    "0701087": {
        "title": "Artificiality in Social Sciences",
        "abstract": "This text provides with an introduction to the modern approach of\nartificiality and simulation in social sciences. It presents the relationship\nbetween complexity and artificiality, before introducing the field of\nartificial societies which greatly benefited from the computer power fast\nincrease, gifting social sciences with formalization and experimentation tools\npreviously owned by \"hard\" sciences alone. It shows that as \"a new way of doing\nsocial sciences\", artificial societies should undoubtedly contribute to a\nrenewed approach in the study of sociality and should play a significant part\nin the elaboration of original theories of social phenomena.",
        "date": "2007-01-13T16:50:37+00:00",
        "label": 0
    },
    "1407.7360": {
        "title": "A Taxonomy and Survey on eScience as a Service in the Cloud",
        "abstract": "Cloud computing has recently evolved as a popular computing infrastructure\nfor many applications. Scientific computing, which was mainly hosted in private\nclusters and grids, has started to migrate development and deployment to the\npublic cloud environment. eScience as a service becomes an emerging and\npromising direction for science computing. We review recent efforts in\ndeveloping and deploying scientific computing applications in the cloud. In\nparticular, we introduce a taxonomy specifically designed for scientific\ncomputing in the cloud, and further review the taxonomy with four major kinds\nof science applications, including life sciences, physics sciences, social and\nhumanities sciences, and climate and earth sciences. Our major finding is that,\ndespite existing efforts in developing cloud-based eScience, eScience still has\na long way to go to fully unlock the power of cloud computing paradigm.\nTherefore, we present the challenges and opportunities in the future\ndevelopment of cloud-based eScience services, and call for collaborations and\ninnovations from both the scientific and computer system communities to address\nthose challenges.",
        "date": "2014-07-28T09:14:35+00:00",
        "label": 0
    },
    "1311.5006": {
        "title": "Indagini in Deep Inference",
        "abstract": "Italian master's thesis in Computer Science. It is an overview of the\nstandard tecniques developed in the field of Proof Theory, ending with some\nresults in the new field of Deep Inference, plus an original contribution\ntrying to relate Deep Inference and Process Algebras.",
        "date": "2013-11-20T10:46:35+00:00",
        "label": 0
    },
    "1808.05686": {
        "title": "Embedded EthiCS: Integrating Ethics Broadly Across Computer Science Education",
        "abstract": "Computing technologies have become pervasive in daily life, sometimes\nbringing unintended but harmful consequences. For students to learn to think\nnot only about what technology they could create, but also about what\ntechnology they should create, computer science curricula must expand to\ninclude ethical reasoning about the societal value and impact of these\ntechnologies. This paper presents Embedded EthiCS, a novel approach to\nintegrating ethics into computer science education that incorporates ethical\nreasoning throughout courses in the standard computer science curriculum. It\nthus changes existing courses rather than requiring wholly new courses. The\npaper describes a pilot Embedded EthiCS program that embeds philosophers\nteaching ethical reasoning directly into computer science courses. It discusses\nlessons learned and challenges to implementing such a program across different\ntypes of academic institutions.",
        "date": "2018-08-16T21:45:54+00:00",
        "label": 0
    },
    "1009.2000": {
        "title": "Home Automation",
        "abstract": "In this paper I briefly discuss the importance of home automation system.\nGoing in to the details I briefly present a real time designed and implemented\nsoftware and hardware oriented house automation research project, capable of\nautomating house's electricity and providing a security system to detect the\npresence of unexpected behavior.",
        "date": "2010-09-10T12:54:24+00:00",
        "label": 0
    },
    "2206.09250": {
        "title": "Robin Milner's Work on Concurrency: An Appreciation",
        "abstract": "We give a short appreciation of Robin Milner's seminal contributions to the\ntheory of concurrency.",
        "date": "2022-06-18T17:22:01+00:00",
        "label": 0
    },
    "2109.07958": {
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "abstract": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.",
        "date": "2021-09-08T17:15:27+00:00",
        "label": 1
    },
    "2007.08087": {
        "title": "Starting with data: advancing spatial data science by building and sharing high-quality datasets",
        "abstract": "Spatial data science has emerged in recent years as an interdisciplinary\nfield. This position paper discusses the importance of building and sharing\nhigh-quality datasets for spatial data science.",
        "date": "2020-07-16T03:15:56+00:00",
        "label": 0
    },
    "2110.08534": {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.",
        "date": "2021-10-16T09:59:33+00:00",
        "label": 1
    },
    "2006.14651": {
        "title": "Influence Functions in Deep Learning Are Fragile",
        "abstract": "Influence functions approximate the effect of training samples in test-time\npredictions and have a wide variety of applications in machine learning\ninterpretability and uncertainty estimation. A commonly-used (first-order)\ninfluence function can be implemented efficiently as a post-hoc method\nrequiring access only to the gradients and Hessian of the model. For linear\nmodels, influence functions are well-defined due to the convexity of the\nunderlying loss function and are generally accurate even across difficult\nsettings where model changes are fairly large such as estimating group\ninfluences. Influence functions, however, are not well-understood in the\ncontext of deep learning with non-convex loss functions. In this paper, we\nprovide a comprehensive and large-scale empirical study of successes and\nfailures of influence functions in neural network models trained on datasets\nsuch as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments,\nwe show that the network architecture, its depth and width, as well as the\nextent of model parameterization and regularization techniques have strong\neffects in the accuracy of influence functions. In particular, we find that (i)\ninfluence estimates are fairly accurate for shallow networks, while for deeper\nnetworks the estimates are often erroneous; (ii) for certain network\narchitectures and datasets, training with weight-decay regularization is\nimportant to get high-quality influence estimates; and (iii) the accuracy of\ninfluence estimates can vary significantly depending on the examined test\npoints. These results suggest that in general influence functions in deep\nlearning are fragile and call for developing improved influence estimation\nmethods to mitigate these issues in non-convex setups.",
        "date": "2020-06-25T18:25:59+00:00",
        "label": 1
    },
    "1103.3321": {
        "title": "Typed Operational Semantics for Dependent Record Types",
        "abstract": "Typed operational semantics is a method developed by H. Goguen to prove\nmeta-theoretic properties of type systems. This paper studies the metatheory of\na type system with dependent record types, using the approach of typed\noperational semantics. In particular, the metatheoretical properties we have\nproved include strong normalisation, Church-Rosser and subject reduction.",
        "date": "2011-03-17T00:19:42+00:00",
        "label": 0
    },
    "1103.1386": {
        "title": "Physics and computer science: quantum computation and other approaches",
        "abstract": "This is a position paper written as an introduction to the special volume on\nquantum algorithms I edited for the journal Mathematical Structures in Computer\nScience (Volume 20 - Special Issue 06 (Quantum Algorithms), 2010).",
        "date": "2011-03-07T21:00:29+00:00",
        "label": 0
    },
    "0912.4188": {
        "title": "The skewness of computer science",
        "abstract": "Computer science is a relatively young discipline combining science,\nengineering, and mathematics. The main flavors of computer science research\ninvolve the theoretical development of conceptual models for the different\naspects of computing and the more applicative building of software artifacts\nand assessment of their properties. In the computer science publication\nculture, conferences are an important vehicle to quickly move ideas, and\njournals often publish deeper versions of papers already presented at\nconferences. These peculiarities of the discipline make computer science an\noriginal research field within the sciences, and, therefore, the assessment of\nclassical bibliometric laws is particularly important for this field. In this\npaper, we study the skewness of the distribution of citations to papers\npublished in computer science publication venues (journals and conferences). We\nfind that the skewness in the distribution of mean citedness of different\nvenues combines with the asymmetry in citedness of articles in each venue,\nresulting in a highly asymmetric citation distribution with a power law tail.\nFurthermore, the skewness of conference publications is more pronounced than\nthe asymmetry of journal papers. Finally, the impact of journal papers, as\nmeasured with bibliometric indicators, largely dominates that of proceeding\npapers.",
        "date": "2009-12-21T15:24:08+00:00",
        "label": 0
    },
    "2306.02031": {
        "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
        "abstract": "Modern neural networks are known to give overconfident prediction for\nout-of-distribution inputs when deployed in the open world. It is common\npractice to leverage a surrogate outlier dataset to regularize the model during\ntraining, and recent studies emphasize the role of uncertainty in designing the\nsampling strategy for outlier dataset. However, the OOD samples selected solely\nbased on predictive uncertainty can be biased towards certain types, which may\nfail to capture the full outlier distribution. In this work, we empirically\nshow that diversity is critical in sampling outliers for OOD detection\nperformance. Motivated by the observation, we propose a straightforward and\nnovel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse\nand informative outliers. Specifically, we cluster the normalized features at\neach iteration, and the most informative outlier from each cluster is selected\nfor model training with absent category loss. With DOS, the sampled outliers\nefficiently shape a globally compact decision boundary between ID and OOD data.\nExtensive experiments demonstrate the superiority of DOS, reducing the average\nFPR95 by up to 25.79% on CIFAR-100 with TI-300K.",
        "date": "2023-06-03T07:17:48+00:00",
        "label": 1
    },
    "2403.03387": {
        "title": "Undergraduate data science education: Who has the microphone and what are they saying?",
        "abstract": "The presence of data science has been profound in the scientific community in\nalmost every discipline. An important part of the data science education\nexpansion has been at the undergraduate level. We conducted a systematic\nliterature review to (1) specify current evidence and knowledge gaps in\nundergraduate data science education and (2) inform policymakers and data\nscience educators/practitioners about the present status of data science\neducation research. The majority of the publications in data science education\nthat met our search criteria were available open-access. Our results indicate\nthat data science education research lacks empirical data and reproducibility.\nNot all disciplines contribute equally to the field of data science education.\nComputer science and data science as a separate field emerge as the leading\ncontributors to the literature. In contrast, fields such as statistics,\nmathematics, as well as other fields closely related to data science exhibit a\nlimited presence in studies. We recommend that federal agencies and researchers\n1) invest in empirical data science education research; 2) diversify research\nefforts to enrich the spectrum of types of studies; 3) encourage scholars in\nkey data science fields that are currently underrepresented in the literature\nto contribute more to research and publications.",
        "date": "2024-03-06T00:49:08+00:00",
        "label": 0
    },
    "1909.04486": {
        "title": "Data Science in Biomedicine",
        "abstract": "We highlight the role of Data Science in Biomedicine. Our manuscript goes\nfrom the general to the particular, presenting a global definition of Data\nScience and showing the trend for this discipline together with the terms of\ncloud computing and big data. In addition, since Data Science is mostly related\nto areas like economy or business, we describe its importance in biomedicine.\nBiomedical Data Science (BDS) presents the challenge of dealing with data\ncoming from a range of biological and medical research, focusing on\nmethodologies to advance the biomedical science discoveries, in an\ninterdisciplinary context.",
        "date": "2019-09-09T11:31:40+00:00",
        "label": 0
    },
    "1905.00787": {
        "title": "Computer Science and Metaphysics: A Cross-Fertilization",
        "abstract": "Computational philosophy is the use of mechanized computational techniques to\nunearth philosophical insights that are either difficult or impossible to find\nusing traditional philosophical methods. Computational metaphysics is\ncomputational philosophy with a focus on metaphysics. In this paper, we (a)\ndevelop results in modal metaphysics whose discovery was computer assisted, and\n(b) conclude that these results work not only to the obvious benefit of\nphilosophy but also, less obviously, to the benefit of computer science, since\nthe new computational techniques that led to these results may be more broadly\napplicable within computer science. The paper includes a description of our\nbackground methodology and how it evolved, and a discussion of our new results.",
        "date": "2019-05-01T16:51:32+00:00",
        "label": 0
    },
    "2403.16898": {
        "title": "Concerned with Data Contamination? Assessing Countermeasures in Code Language Model",
        "abstract": "Various techniques have been proposed to leverage the capabilities of code\nlanguage models (CLMs) for SE tasks. While these techniques typically evaluate\ntheir effectiveness using publicly available datasets, the evaluation can be\nsubject to data contamination threats where the evaluation datasets have\nalready been used to train the concerned CLMs. This can significantly affect\nthe reliability of the evaluation. Different countermeasures have been\nsuggested to mitigate the data contamination threat. Countermeasures include\nusing more recent data, curating new data, and refactoring existing data are\nintroduced, yet it is unclear whether these countermeasures could really\nmitigate data contamination threats to model evaluation. To fill the gap, we\nsystematically study to quantify the impacts of these countermeasures on CLMs'\nperformance. To facilitate the study, we collected over 2 million Python\nfunctions with timestamps ranging from January 1st, 2018, to December 31st,\n2023. The data created before the models' cut-off date are considered\n\"contaminated data\", while the data where the countermeasures are taken are\nregarded as \"cleansed data\". We study the impact of these countermeasures by\ninvestigating the difference in CLMs' performance on contaminated and cleansed\ndata derived from different countermeasures. Our experiments yield several\ninteresting observations. For instance, CLMs do not necessarily perform worse\non data after the models' cut-off date; on the contrary, they sometimes perform\nbetter. In addition, refactoring did not always result in decreased\nperformance; it could lead to improvements instead. Furthermore, existing\nmetrics such as perplexity cannot distinguish contaminated/cleansed data. We\nhope that the results and observations could help deepen the understanding of\nCLMs' capabilities and inform the community about data contamination.",
        "date": "2024-03-25T16:10:25+00:00",
        "label": 1
    },
    "2403.00526": {
        "title": "Data Quality Assessment: Challenges and Opportunities",
        "abstract": "Data-oriented applications, their users, and even the law require data of\nhigh quality. Research has broken down the rather vague notion of data quality\ninto various dimensions, such as accuracy, consistency, and reputation, to name\nbut a few. To achieve the goal of high data quality, many tools and techniques\nexist to clean and otherwise improve data. Yet, systematic research on actually\nassessing data quality in all of its dimensions is largely absent, and with it\nthe ability to gauge the success of any data cleaning effort. It is our vision\nto establish a systematic and comprehensive framework for the (numeric)\nassessment of data quality for a given dataset and its intended use. Such a\nframework must cover the various facets that influence data quality, as well as\nthe many types of data quality dimensions. In particular, we identify five\nfacets that serve as a foundation of data quality assessment. For each facet,\nwe outline the challenges and opportunities that arise when trying to actually\nassign quality scores to data and create a data quality profile for it, along\nwith a wide range of technologies needed for this purpose.",
        "date": "2024-03-01T13:35:15+00:00",
        "label": 1
    },
    "1509.02826": {
        "title": "Proceedings Tenth International Workshop on Fixed Points in Computer Science",
        "abstract": "This volume contains the proceedings of the Tenth International Workshop on\nFixed Points in Computer Science (FICS 2015) which took place on September 11th\nand 12th, 2015 in Berlin, Germany, as a satellite event of the conference\nComputer Science Logic (CSL 2015).\n  Fixed points play a fundamental role in several areas of computer science.\nThey are used to justify (co)recursive definitions and associated reasoning\ntechniques. The construction and properties of fixed points have been\ninvestigated in many different settings such as: design and implementation of\nprogramming languages, logics, verification, databases. The aim of this\nworkshop is to provide a forum for researchers to present their results to\nthose members of the computer science and logic communities who study or apply\nthe theory of fixed points.\n  Each of the 11 contributed papers of this volume were evaluated by three or\nfour reviewers. Some of the papers were re-reviewed after revision.\n  Additionally, this volume contains the abstracts of the FICS 2015 invited\ntalks given by Bartek Klin and James Worrell.",
        "date": "2015-09-09T16:02:13+00:00",
        "label": 0
    },
    "2303.14753": {
        "title": "Does \"Deep Learning on a Data Diet\" reproduce? Overall yes, but GraNd at Initialization does not",
        "abstract": "The paper 'Deep Learning on a Data Diet' by Paul et al. (2021) introduces two\ninnovative metrics for pruning datasets during the training of neural networks.\nWhile we are able to replicate the results for the EL2N score at epoch 20, the\nsame cannot be said for the GraNd score at initialization. The GraNd scores\nlater in training provide useful pruning signals, however. The GraNd score at\ninitialization calculates the average gradient norm of an input sample across\nmultiple randomly initialized models before any training has taken place. Our\nanalysis reveals a strong correlation between the GraNd score at initialization\nand the input norm of a sample, suggesting that the latter could have been a\ncheap new baseline for data pruning. Unfortunately, neither the GraNd score at\ninitialization nor the input norm surpasses random pruning in performance. This\ncontradicts one of the findings in Paul et al. (2021). We were unable to\nreproduce their CIFAR-10 results using both an updated version of the original\nJAX repository and in a newly implemented PyTorch codebase. An investigation of\nthe underlying JAX/FLAX code from 2021 surfaced a bug in the checkpoint\nrestoring code that was fixed in April 2021\n(https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5).",
        "date": "2023-03-26T15:13:19+00:00",
        "label": 1
    },
    "2401.12926": {
        "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
        "abstract": "When selecting data for training large-scale models, standard practice is to\nfilter for examples that match human notions of data quality. Such filtering\nyields qualitatively clean datapoints that intuitively should improve model\nbehavior. However, in practice the opposite can often happen: we find that\nselecting according to similarity with \"high quality\" data sources may not\nincrease (and can even hurt) performance compared to randomly selecting data.\n  To develop better methods for selecting data, we start by framing dataset\nselection as an optimization problem that we can directly solve for: given\ntarget tasks, a learning algorithm, and candidate data, select the subset that\nmaximizes model performance. This framework thus avoids handpicked notions of\ndata quality, and instead models explicitly how the learning process uses train\ndatapoints to predict on the target tasks. Our resulting method greatly\nimproves language model (LM) performance on both pre-specified tasks and\npreviously unseen tasks. Specifically, choosing target tasks representative of\nstandard LM problems and evaluating on diverse held-out benchmarks, our\nselected datasets provide a 2x compute multiplier over baseline methods.",
        "date": "2024-01-23T17:22:00+00:00",
        "label": 1
    },
    "2406.07545": {
        "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
        "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.",
        "date": "2024-06-11T17:59:47+00:00",
        "label": 1
    },
    "1003.1930": {
        "title": "Simulating Grover's Quantum Search in a Classical Computer",
        "abstract": "The rapid progress of computer science has been accompanied by a\ncorresponding evolution of computation, from classical computation to quantum\ncomputation. As quantum computing is on its way to becoming an established\ndiscipline of computing science, much effort is being put into the development\nof new quantum algorithms. One of quantum algorithms is Grover algorithm, which\nis used for searching an element in an unstructured list of N elements with\nquadratic speed-up over classical algorithms. In this work, Quantum Computer\nLanguage (QCL) is used to make a Grover's quantum search simulation in a\nclassical computer",
        "date": "2010-03-09T17:02:21+00:00",
        "label": 0
    },
    "2304.06767": {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.",
        "date": "2023-04-13T18:22:40+00:00",
        "label": 1
    },
    "2008.05561": {
        "title": "The Right Tools for the Job: The Case for Spatial Science Tool-Building",
        "abstract": "This paper was presented as the 8th annual Transactions in GIS plenary\naddress at the American Association of Geographers annual meeting in\nWashington, DC. The spatial sciences have recently seen growing calls for more\naccessible software and tools that better embody geographic science and theory.\nUrban spatial network science offers one clear opportunity: from multiple\nperspectives, tools to model and analyze nonplanar urban spatial networks have\ntraditionally been inaccessible, atheoretical, or otherwise limiting. This\npaper reflects on this state of the field. Then it discusses the motivation,\nexperience, and outcomes of developing OSMnx, a tool intended to help address\nthis. Next it reviews this tool's use in the recent multidisciplinary spatial\nnetwork science literature to highlight upstream and downstream benefits of\nopen-source software development. Tool-building is an essential but poorly\nincentivized component of academic geography and social science more broadly.\nTo conduct better science, we need to build better tools. The paper concludes\nwith paths forward, emphasizing open-source software and reusable computational\ndata science beyond mere reproducibility and replicability.",
        "date": "2020-08-12T20:15:39+00:00",
        "label": 0
    },
    "2312.06254": {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "label": 1
    },
    "1307.8029": {
        "title": "Proceedings Fourth International Symposium on Symbolic Computation in Software Science",
        "abstract": "Symbolic computation is the science of computing with symbolic objects\n(terms, formulae, programs, algebraic objects, geometrical objects, etc).\nPowerful symbolic algorithms have been developed during the past decades and\nhave played an influential role in theorem proving, automated reasoning,\nsoftware verification, model checking, rewriting, formalisation of mathematics,\nnetwork security, Groebner bases, characteristic sets, etc.\n  The international Symposium on \"Symbolic Computation in Software Science\" is\nthe fourth in the SCSS workshop series. SCSS 2008 and 2010 took place at the\nResearch Institute for Symbolic Computation (RISC), Hagenberg, Austria, and,\nSCSS 2009 took place in Gammarth, Tunisia. These symposium grew out of internal\nworkshops that bring together researchers from: a) SCORE (Symbolic Computation\nResearch Group) at the University of Tsukuba, Japan, b) Theorema Group at the\nResearch Institute for Symbolic Computation, Johannes Kepler University Linz,\nAustria, c) SSFG (Software Science Foundation Group) at Kyoto University,\nJapan, and d) Sup'Com (Higher School of Communication of Tunis) at the\nUniversity of Carthage, Tunisia.",
        "date": "2013-07-30T16:01:33+00:00",
        "label": 0
    },
    "1705.02203": {
        "title": "Analysis of Computational Science Papers from ICCS 2001-2016 using Topic Modeling and Graph Theory",
        "abstract": "This paper presents results of topic modeling and network models of topics\nusing the International Conference on Computational Science corpus, which\ncontains domain-specific (computational science) papers over sixteen years (a\ntotal of 5695 papers). We discuss topical structures of International\nConference on Computational Science, how these topics evolve over time in\nresponse to the topicality of various problems, technologies and methods, and\nhow all these topics relate to one another. This analysis illustrates\nmultidisciplinary research and collaborations among scientific communities, by\nconstructing static and dynamic networks from the topic modeling results and\nthe keywords of authors. The results of this study give insights about the past\nand future trends of core discussion topics in computational science. We used\nthe Non-negative Matrix Factorization topic modeling algorithm to discover\ntopics and labeled and grouped results hierarchically.",
        "date": "2017-04-18T13:24:41+00:00",
        "label": 0
    },
    "2407.08102": {
        "title": "Dynamics of Gender Bias within Computer Science",
        "abstract": "A new dataset (N = 7,456) analyzes women's research authorship in the\nAssociation for Computing Machinery's founding 13 Special Interest Groups or\nSIGs, a proxy for computer science. ACM SIGs expanded during 1970-2000; each\nexperienced increasing women's authorship. But diversity abounds. Several SIGs\nhad fewer than 10% women authors while SIGUCCS (university computing centers)\nexceeded 40%. Three SIGs experienced accelerating growth in women's authorship;\nmost, including a composite ACM, had decelerating growth. This research may\nencourage reform efforts, often focusing on general education or workforce\nfactors (across the entity of \"computer science\"), to examine under-studied\ndynamics within computer science that shaped changes in women's participation.",
        "date": "2024-07-11T00:14:21+00:00",
        "label": 0
    },
    "2311.10833": {
        "title": "Generative AI has lowered the barriers to computational social sciences",
        "abstract": "Generative artificial intelligence (AI) has revolutionized the field of\ncomputational social science, unleashing new possibilities for analyzing\nmultimodal data, especially for scholars who may not have extensive programming\nexpertise. This breakthrough carries profound implications for the realm of\nsocial sciences. Firstly, generative AI can significantly enhance the\nproductivity of social scientists by automating the generation, annotation, and\ndebugging of code. Secondly, it empowers researchers to delve into\nsophisticated data analysis through the innovative use of prompt engineering.\nLastly, the educational sphere of computational social science stands to\nbenefit immensely from these tools, given their exceptional ability to annotate\nand elucidate complex codes for learners, thereby simplifying the learning\nprocess and making the technology more accessible.",
        "date": "2023-11-17T19:24:39+00:00",
        "label": 0
    }
}