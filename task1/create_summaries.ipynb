{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use of Ollamas llms: https://www.youtube.com/watch?v=h_GTxRFYETY\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create summaries\n",
    "Here summaries of arxiv papers are created with the use of the llm mistral of Ollama"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the papers\n",
    "The papers are separated into abstract and text. In the text there is no abstract. So that the text can serve as candidate to be summarized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data import load_papers \n",
    "import re\n",
    "separator = \"\\n---SEPARATOR---\\n\" #separates texts for later\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are  100  papers.\n"
     ]
    }
   ],
   "source": [
    "abstracts, texts = load_papers(split='test.txt')\n",
    "print('There are ',len(texts),' papers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the llm mistral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Before i installed ollama:$ curl -fsSL https://ollama.com/install.sh | sh\n",
    "and then installed mistral with  $ ollama pull mistral\n",
    "Getting mistral and setting the temperature to 0, so that there is always the same answer.\n",
    "\"\"\"\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write summaries with simple prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_summary(article):\n",
    "    instruction = \"Write a summary of following article:\"\n",
    "    answer = llm.invoke(instruction + article)\n",
    "    answer = re.sub('\\n',' ',answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answers(num,filename):\n",
    "    summaries = []\n",
    "    for i in range(num):\n",
    "        summaries.append(ask_for_summary(texts[i]))\n",
    "        \n",
    "    summaries = separator.join(summaries)\n",
    "    with open('summaries/'+filename, 'w') as file:\n",
    "            file.write(summaries)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate 10 or 100 summaries\n",
    "summaries = write_answers(10,'write_summary.txt')\n",
    "#summaries = write_answers(100,'write_summary100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It appears that you have provided a list of references related to gravitational physics and astrophysics, specifically those dealing with topics such as black holes, gravitational waves, and cosmology. Here is the information extracted from the given references:  1. Jackiw and Pi (2003) - Phys. Rev. D **68**, 104012 (gr-qc/0308071) 2. Satoh, Kanno, and Soda (2008) - Phys. Rev. D **77**, 023526 (astro-ph/07063585) 3. Contaldi, Magueijo, and Smolin (2008) - Phys. Rev. Lett. **101**, 141101 (astro-ph/08063082) 4. Takahashi and Soda (2009) - Phys. Rev. Lett. **102**, 231301 (hep-th/09040554) 5. Cook and Sorbo (2012) - Phys. Rev. D **85**, 023534; **86**, 069901 6. Obata, Miura, and Soda (2015) - Phys. Rev. D **92**, 063516 (astro-ph/14127620) 7. Lightman et al. (1979) - Problem Book in Relativity and Gravitation, Princeton University Press 8. Maggiore (2008) - Gravitational Waves: Theory and Experiments, Oxford University Press 9. Rybicki and Lightman (1979) - Radiative Processes in Astrophysics, Wiley-Interscience 10. Landau and Lifshitz (1975) - The Classical Theory, Pergamon Press 11. Misner et al. (1973) - Gravitation, Freeman 12. Lightman et al. (1997) - Phys. Rev. D **56**, 545 (gr-qc/9607068) 13. Seljak and Zaldarriaga (1997) - Phys. Rev. Lett. **78**, 2054 (astro-ph/9609169) 14. Goldberg et al. (1967) - Journal of Mathematical Physics **8**, 2155 15. Anholm et al. (2009) - Phys. Rev. D **79**, 084030 (gr-qc/08090701) 16. Book and Flanagan (2011) - Phys. Rev. D **83**, 024024 (astro-ph/10094192) 17. Jenet and Romano (2015) - Am. J. Phys. **83**, 635 (gr-qc/14121142) 18. Hellings and Downs (1983) - Astrophys. J. **265**, L39  These references cover various aspects of gravitational physics and astrophysics, including theoretical models, observational studies, and experimental results.\n",
      " This text appears to be a scientific research paper on the topic of nuclear physics, specifically focusing on the calculation of decay half-lives for alpha decays in atomic nuclei and the penetration of potential barriers. The authors have developed a new analytical formula based on the Wentzel-Kramers-Brillouin (WKB) approximation to calculate the barrier penetrability, taking into account the influence of the long-range Coulomb tail in the barrier potential.  The text includes a comparison of the results obtained using different approaches, including the parabolic approximation and the WKB approach, for various isotopes of Po (polonium). The authors have assumed the barrier potential to be spherical for most cases, and they have shown that their new formula gives very good results for these spherical nuclei.  The text also discusses the importance of evaluating the integral in the equation for the penetrability and provides an analytical expression for it. The computation was supported by the National Natural Science Foundation, the Major State Basic Research Development Program of China, the Knowledge Innovation Project of CAS, and Deutsche Forschungsgemeinschaft.  Overall, the text presents a new analytical formula for calculating barrier penetrability in nuclear physics, which could be useful for studying barrier penetrability where one has to introduce an energy-dependent one-dimensional potential barrier or a barrier distribution function.\n"
     ]
    }
   ],
   "source": [
    "summaries_list = []\n",
    "for i in summaries.split(separator):\n",
    "    summaries_list.append(i)\n",
    "print('Example of a bad example\\n:',summaries_list[1])\n",
    "print('Another example o summary\\n:',summaries_list[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write summaries with other prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_summary(article):\n",
    "    instruction = \"Please summarize the following scientific paper to create a concise abstract. The summary should include the research objective, methods, key results, and conclusions. Ensure the summary is clear and follows the structure of a typical scientific abstract.\"\n",
    "    answer = llm.invoke(instruction + article)\n",
    "    answer = re.sub('\\n',' ',answer)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_answers(num,filename):\n",
    "    summaries = []\n",
    "    for i in range(num):\n",
    "        summaries.append(ask_for_summary(texts[i]))\n",
    "        \n",
    "    summaries = separator.join(summaries)\n",
    "    with open('summaries/'+filename, 'w') as file:\n",
    "            file.write(summaries)\n",
    "    return summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaries = write_answers(10,'write_summary_complex.txt')\n",
    "summaries = write_answers(100,'write_summary_complex100.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text on my work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction\\nRecently large language models (LLM\\'s) such as OpenAI\\'s GPT-4 have \\nrevolutionized natural language processing (NLP), showing remarkable capabilities to \\ngenerate human-like text. These models can answer complex questions and perform a lot \\nof difficult tasks. Now it is interesting to see how they can help in academic \\nresearch.\\n\\nIn every research it is important to have an overview what is already done and where \\nthe knowledge is at the time. Getting the state of the art in a topic is a crucial\\nstep, but it is very time consuming and can be difficult. For that reason it would \\nbe great an LLM can do it for us. In this work we would like to find out in which \\nextent LLMs can go towards to an automatic state of the art.\\\\\\\\\\n\\nLLMs, while powerful, have certain limitations. \\nThey can process large amounts of text and generate summaries, but they can also make\\nmistakes. A challenge is to ensure accuracy, relevance, and coherence of the information \\nthey produce. \\nA specific challenge is called hallucination. It happens when the llm doesn\\'t know something\\nand makes it up. For example it makes up a source, or claims that there exist a study that \\nproves a point. These hallucinations are difficult to spot quickly. \\nMany advances have been made in utilizing LLMs, many will follow.\\n\\nThis work aims to find out how good llm\\'s are at creating a state of \\nthe art work for a scientific topic. We started at thinking how humans do \\nthese kind of\\nresearch. There are three main tasks we concentrated our research. \\nFirst task is to\\nmake a summary out of one article. Second task is to choose relevant articles. \\nAnd at\\nthe end the third task is to combine these relevant articles and create an state of \\nthe art article.\\n\\n\\nRelated Work\\nIn this chapter we evaluate the state of the art of our topic. First we discuss challenges in LLM use \\nfor academic research. In relation to the three task we study related work in \\nsummarization of individual articles using LLMs, selection of relevant articles using LLMs\\nand combining articles to create a state-of-the-art review.\\nAt the end we consider the ways of evaluation and challenges.\\n\\n\\\\section{Challenges in LLM Use in scientifc context}\\nLarge Language Models (LLMs) like OpenAI\\'s GPT-4 have demonstrated impressive \\ncapabilities in generating human-like text, making them promising tools for various \\nnatural language processing (NLP) tasks, including academic research. \\n\\nHowever, \\ndespite these advancements, significant challenges persist when applying LLMs in \\nscientific context. When needing references some LLM\\'s are unreliable to deliver them.\\n\\\\cite{wu2024llmsciterelevantmedical} evaluated top-performing, commercially available LLMs \\n(GPT-4 (RAG and API), Claude v2.1, Mistral\\nMedium, and Gemini Pro) and find that models without access to the Web only produce \\nvalid URLs between 40\\\\% to 70\\\\% of the time. URL\\'s that would serve as references.\\n\\nWith Retrieval-augmented generation (RAG) a big step had been done. As \\n\\\\cite{shuster-etal-2021-retrieval-augmentation} showed that \\nRetrieval Augmentation reduces hallucination in conversation. Also \\n\\\\cite{wu2024llmsciterelevantmedical} came to the conclusion that with \\nRetrieval-augmented generation (RAG)-enabled GPT-4, which has search engine access, \\ndoes not suffer from URL hallucination.\\n\\nStill a lot of work has to be done as \\\\cite{wu2024llmsciterelevantmedical} showed\\nthat even with access to internet and RAG LLM\\'s still fail to produce references \\nthat support all the statements in the response nearly half of the time.\\n\\n\\n\\n\\\\section{Summarization of Individual Articles}\\nThe task of summarization was studied a lot and also a motivation to create large language \\nmodels.\\n\\nThe first automatic text summarization tools were created based on frequency statistics.\\n\\nThis work \\\\cite{fuzzy} from 2013 used Fuzzy Logic Text for Summarization by Extraction. They \\nproposed an approach for extractive summarization\\nprogram consists of 4 main steps. There are preprocessing of text, features extraction of both words\\nand sentences, sentence selection and assembly, and summary generation.\\n\\nIn year 2016 this research \\\\cite{singlesumm} aimed to produce an automatic text summarizer\\nimplemented with TF-IDF algorithm. They focused on creating summaries by extracting the most important\\nsentences out of the original texts. With simple techniques like TF-IDF they could reach 67\\\\% of \\naccuracy compared to which sentences humans considered as important.\\n\\n\\nCreating abstract summaries was a motivation to create LLMs.\\nWorking with scientific papers the work \\\\cite{cohan2018discourseawareattentionmodelabstractive} \\nrepresents an early approach summarizing long documents. \\nSummarization of scientific articles is complex because of the technical nature of the content.\\nHandling complex, long-form text which is vital for handling detailed scientific papers. \\n\\nMost LLM\\'s are not specifically for scientific context. \\nSummarizing scientific articles presents unique challenges, such as understanding technical \\nterminology and complex concepts accurately.\\nBut SciBERT \\\\cite{beltagy2019scibertpretrainedlanguagemodel} a version of BERT was pretrained \\nspecifically on scientific texts and is thus better \\nequipped to manage these challenges.  It allows SciBERT or similar models to \\ngenerate summaries that are more accurate and contextually relevant, which is critical when \\ndealing with field-specific jargon and nuanced scientific discourse.\\n\\n\\n\\\\section{Selection of Relevant Articles Using LLMs}\\n\\nSelecting relevant literature involves retrieving and ranking documents accurately. \\nFinding directly the relevant literature we usually don\\'t do as it gives hallucinations.\\nLLM\\'s can be used as a second step to select out of a preselection.\\n\\nThe GPT-4 Research Assistant (https://github.com/echohive42/GPT-4-Research-assistant/tree/main, \\n2024) offers an automated process for identifying and selecting relevant research papers from \\nArXiv. It uses GPT-4 to assess the most promising papers from search results, demonstrating \\na practical application of LLMs in content selection.\\n\\nHowever it is questionable how good LLMs are in selection of any kind because of the positional\\nbias. \\\\cite{liusie2024llmcomparativeassessmentzeroshot} used LLM\\'s as evaluator for comparative \\nassessment. There the LLM had to choose out of two texts which one was better. They showed that \\nthe first one had an advantage, ergo an positional bias.\\n\\nIn our work we look closely at if an LLM can select relevant content out of provided content. \\nWe focus here on the binary, relevant or not.\\n\\n\\nAnother way to select content is the frequently mentioned vector embedding (used in RAG).\\n\\\\cite{garcia2023sourcestalkevaluatinglarge} shows how LLMs with vector embeddings assist in \\nretrieving and evaluating relevant sources in a historical research context.\\n\\nIn our work we will use RAG to select the most relevant parts of papers in task 3.\\n\\n\\n\\\\section{Combining Articles to Create a State-of-the-Art Review}\\n\\nA key challenge in generating state-of-the-art reviews is synthesizing information from multiple \\nsources. The paper “Generating Wikipedia by Summarizing Long Sequences” \\n\\\\cite{liu2018generatingwikipediasummarizinglong} introduces a method for multi-document \\nsummarization, where the input is a collection of related documents, and the output is a \\nwell-structured Wikipedia article. \\nThis process involves an extractive step, where relevant paragraphs are extracted. \\nFollowed by an abstractive step using the paragraphs and a transformer model to create the \\nsummary. \\n\\nWe in our work did take a similar approach in their methodoloy. \\nThey did take a wikipedia article \\nand sources to it, in an supervised setting.\\nWhile \\\\cite{liu2018generatingwikipediasummarizinglong} employ a supervised learning approach by \\ntraining a model on curated Wikipedia inputs and summaries, our study differs in that we evaluate a \\npre-trained model without further training or fine-tuning.\\n\\nWe take an original paper and take its references. \\nWe can use newer LLM models and experiment with RAG. As well as test against more evaluation scores. \\n\\n\\n\\n\\\\section{Evaluation Metrics and Challenges}\\nA very important part of this work is the evaluation of created summaries or texts from \\nthe llm. The challenge here is the lack of consensus. A study about summarization evaluation \\nis done in the paper \\n\\\\cite{fabbri2021summevalreevaluatingsummarizationevaluation} the authors evaluate\\nseveral evaluation metrics. \\\\cite{fabbri2021summevalreevaluatingsummarizationevaluation} also \\nshowed that many evaluation metrics don\\'t correlate with each other or even correlate negatively \\nwith each other. \\n\\nThere are different approaches on how to evaluate a summary. One way is to compare the llm\\ngenerated summary with a reference summary. Then the ability of summarization is tested \\nwith help of similarity. Similarity can be tested by humans or via a variety of metrics. \\n\\nOne of the oldest metrics is ROUGE \\\\cite{lin-2004-rouge}. Despite many critic it has remained \\nthe default automatic evaluation metric. \\nFor this reason we looked at ROUGE as well. So, we could compare to other results of other studies.\\n\\nAnother popular score is BertScore \\\\cite{zhang2020bertscoreevaluatingtextgeneration}, which we used\\nas well.\\n\\nMost recently there are many studies, if LLM\\'s can evaluate generated text. G-Eval \\n\\\\cite{liu2023gevalnlgevaluationusing} is a framework of using large language models with \\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the\\nquality of NLG outputs. They showed that it has a higher correlation with humans. But also \\nhighlight the potential concern of LLM-based evaluators having a bias towards\\nthe LLM-generated texts.\\n\\n\\nThere are even more creative ways to evaluate summaries. QAGS: This evaluation method uses \\nquestion-answering to measure factual consistency in summaries \\n\\\\cite{wang2020askingansweringquestionsevaluate}. It generates questions from the summary and checks if the answers are \\npresent in the source document.\\n\\nIn our work we use different evaluation metrics, also questioning them. At this process we never \\nforget the human evaluation.\\n\\n\\nTheory \\n%\\\\documentclass[oneside,a4paper]{book}\\n%\\\\usepackage{graphicx}\\n%\\\\usepackage{amsmath}\\n\\n\\n\\n%\\\\begin{document}\\n%Fondamental theory that the reader should now.\\n%Now about LLM\\'s, transformers, ect. Scores either can be in method\\n%Embeddings\\n%RAG (or in method)\\nIn this chapter we look at traditional summarization methods \\nand how their limitations have led to the adoption of LLMs for summarization.\\nHere we are covering the important theoretical concepts used in our study. We explain \\nwhat LLM\\'s are and the mechanisms behind, transformers.  In our experiments we used\\ndecoder-only transformers as well as the relatively new concept RAG, that can increase the \\nperformance of LLM\\'s.\\n\\n\\n\\n\\\\section{Summarization Methods}\\nA high quality summary has several qualities to deliver. It should accurately reflect \\nthe main points of the original text without adding or altering meaning. \\nThis means avoiding \"hallucinations\" or fabricated content in the case of machine-generated \\nsummaries.\\nA summary should be complete. Every important point of the text should be covered in the \\nsummary.\\nThe summary has to be a coherent text and all of the facts in the summary have to be relevant.\\nThere have to be enough to understand, but not too many details.\\nHence producing an effective summary can be a difficult and time consuming job. \\nSeveral automated methods have been developed to make summarization more efficient, \\nparticularly through extractive and abstractive approaches.\\n\\n\\nImagine we don\\'t have an LLM to generate a text in one step.\\nHere we can separate two approaches. The first one is extractive. Here we extract what is \\nimportant in the text. The second step is creating an abstract summary, rewriting the key points\\ninto a coherent text. Often these two are combined.\\n\\n\\\\subsection{Traditional more Extractive Summarization}\\n\\nThis type of summarization uses\\nthe statistical approach like title method, location method, Term Frequency-Inverse Document\\nFrequency (TF-IDF) method, and word method for selecting important sentences or keywords from\\ndocuments \\\\cite{singlesumm}.\\n\\nA statistical way to identify key words or phrases in a document was TF-IDF \\n(Term Frequency-Inverse Document Frequency). \\n\\\\cite{singlesumm} used it to create an automatic text summarizer. TF means Term \\nFrequency, i.e. how many times a term (couple of words) occurs relatively in a document.\\nIt is calculated as:\\n\\\\begin{align*}\\n    \\\\text{TF}(t, d) = \\\\frac{\\\\text{Number of times term } t \\\\text{ appears in document } d}{\\\\text{Total number of terms in document } d}\\n\\\\end{align*}\\n\\n\\n\\nIDF stands for Inverse Document Frequency. It measures the importance of the \\nterm across a set of documents, reducing the weight of terms that are common \\nacross many documents. The formula is:\\n\\\\begin{align*}\\n\\\\text{IDF}(t, D) = \\\\log \\\\left( \\\\frac{|D|}{|\\\\{d \\\\in D : t \\\\in d\\\\}|} \\\\right)\\n\\\\end{align*}\\n\\nThis calculation helps to decrease the importance of common terms \\n(like \"the\" or \"is\") across documents while giving more weight to unique terms.\\n\\nThe final TF-IDF score for a term in a document is the product of the TF and IDF \\nvalues:\\n\\\\begin{align*}\\n    \\\\text{TF-IDF}(t, d, D) = \\\\text{TF}(t, d) \\\\times \\\\text{IDF}(t, D)\\n\\\\end{align*}\\n\\nThis score is high for terms that are frequent in this text but not \\nfrequent in others. These terms are then considered important.\\nTF-IDF can be used as a weighting factor in information retrieval and text mining,\\nprimarily for filtering words in text summarization and \\ncategorization application.\\nWhen the important terms are identified, we can find all phrases that contain these terms, \\nand then extract all relevant sentences and put them in the correct order.\\n\\nOther than statistical methods there are positional methods. In some contexts there are \\nreasonable assumptions about where in the text are the important information. Maybe the first or \\nlast sentence, maybe a sentence that starts with \\'In conclusion\\'.\\n\\nAn more elaborate technique is TextRank. It is an unsupervised graph-based ranking algorithm \\ninspired by Google\\'s PageRank. In this method, sentences are represented as nodes in a graph, \\nwith edges connecting sentences that share common words or phrases.\\nThe algorithm ranks sentences based on their connections to other sentences, \\nselecting those with the highest scores for inclusion in the summary.\\n\\n\\nThe main thing is to identify the important sentences. There is also the appraoch to use machine learning.\\nEither a supervised or unsupervised classifier can be used to detect if a sentence is important.\\nFeatures such as TF-IDF scores, sentence length, and position can be used to train models that predict \\nthe importance of sentences for summarization. \\n\\nEach of them has their strengths and weaknesses. Often, the best approach is to combine several methods.\\n\\n\\n\\n\\n\\n\\\\subsection{Limitations of Traditional Summarization Techniques}\\nThere are many limitations to these methods, because, as mentioned before, summarization is a \\ndifficult task.\\nTraditional extractive methods often miss nuanced meaning or generate summaries that are choppy and \\nincoherent.\\n\\nThe biggest limitation is the lack of contextual understanding. Some statements can be out of context \\nin the summary. Some parts can be missing, and without this information, it doesn\\'t make sense or is even \\nwrong.\\nThere is difficulty with relevance filtering. \\nBecause traditional extractive methods rely on fixed rules or frequencies, they often miss important \\ndetails or include irrelevant and redundant information.\\n\\n\\nAnother limitation is limited abstraction. \\nraditional abstractive techniques often produce rigid, template-driven summaries, \\nwhere the sentences are directly taken from the source. Hence the texts \\nare less fluid and coherent than human written.\\nThe quality can vary significantly across different domains. Many summarization techniques often struggle with \\ndomain-specific language or jargon, reducing their effectiveness in specialized fields.    \\n\\nConsidering these limitations, it becomes evident that there is a need for more advanced \\ntechniques to improve summarization quality. In the next chapter, we will explore how \\nLarge Language Models (LLMs) address these challenges. They offer innovative solutions for \\ngenerating more coherent and contextually aware summaries.\\n\\n\\n\\\\section{What is an LLM?}\\nLLM stands for Large Language Model. \\nIt is a deep learning model that generates language, performing various language-related tasks.\\nUsually it is possible to access LLMs with a prompt, some input text where we can ask a question\\nor describe a task it should do. The model then creates an answer text. \\nMost LLMs are based on the transformer architecture.\\n\\n\\n\\n\\\\subsection{Transformers}\\n\\\\label{subsec:transformers}\\nTransformers are deep learning models, or neural networks that have attention mechanisms.\\nWhat a transformer architecture looks like is described in  Figure \\\\ref{fig:transformers}.\\nWe see that there is a left and a right part. They are usually referred to as encoder and\\ndecoder. However, a lot of models nowadays are decoder only, which means they only use the \\nright part. To use it we put a question in the input. \\nThe decoder generates the output word by word, based on previously generated words. \\nThis is why the output goes through the decoder, with internal calculations informed by \\nthe original prompt.\\nThis word-by-word creation process is known as autoregressive generation.\\n\\n\\n\\\\subsubsection{Tokenization and Embedding}\\nIn figure \\\\ref{fig:tokenization} you can see the workflow of an LLM. The first \\nand last step \\nis a preprocessing one. Sentences need to be transformed into tokens. \\nOne approach is to split the sentence into individual words.\\nThe problem there is that they are just too many \\nexisting words. This would result in an enormous dictionary to further \\nprocess the token. \\nAnother approach is to use individual characters as tokens, creating a much smaller dictionary. \\nBut another problem arises as we intend to connect a meaning\\nto a token. Characters don\\'t carry the same semantic meaning as words.\\nThe middle ground \\nhere is to use parts of words, like in the example shown in \\\\ref{fig:tokenization}.\\nwhich allows common words to appear as single tokens,\\nwhile infrequent words are split into smaller parts.\\n\\n\\\\begin{figure}[!h]\\n    \\\\centering\\n    % Include the graphic (replace \\'example-image\\' with your image file name)\\n    \\\\includegraphics[width=0.6\\\\textwidth]{images/tokenization.png}\\n    % Add a caption\\n    \\\\caption{Form Input to Output from https://docs.mistral.ai/guides/tokenization/}\\n    % Label for referencing\\n    \\\\label{fig:tokenization}\\n\\\\end{figure}\\n\\n\\n\\n\\nThe next thing to be done for the inputs is embeddings, as shown in the figure \\n\\\\ref{fig:tokenization} as second step or in figure \\\\ref{fig:transformers} as first step\\nin red.\\nn this step, we work with text, but since neural networks only process numbers, not words or \\nletters, we need to transform each token into a numeric vector.\\nEmbedding vectors have dimensions equal to the number of tokens in the vocabulary.\\nTo do so we need also neural \\nnetworks, where the input \\ndimension is the number of tokens in the vocabulary. To encode a token we use a vector\\nof this dimension and there is a 1 for that token and 0\\'s everywhere else. The  \\nneural network \\nwill give then a vector in a fixed dimension. This resulting vector is more compact and \\ndoesn\\'t have the problem of sparsity as does the vector with only one 1 and otherwise 0\\'s. \\nThe encoder and decoder must use \\nthe same embeddings \\nto ensure that the tokens maintain consistent meanings. \\nThe embeddings are trained so that tokens with similar usage (e.g., parts of words or short words) \\nhave vectors that are close to each other.\\\\\\\\\\n\\n\\n\\\\begin{figure}[h!]\\n    \\\\centering\\n    % Include the graphic (replace \\'example-image\\' with your image file name)\\n    \\\\includegraphics[width=0.6\\\\textwidth]{images/transformers.png}\\n    % Add a caption\\n    \\\\caption{This is the Transformer Architecture from \\\\ref{vaswani2023attentionneed}}\\n    % Label for referencing\\n    \\\\label{fig:transformers}\\n\\\\end{figure}\\n\\n\\\\subsubsection{Transformer Blocks}\\nFigure \\\\ref{fig:transformers} explains the working of a transformer in more detail.\\nIt starts with the previously mentioned input embedding.\\nNext, the positional encoding is added. This represents where a token\\nis in the sequence. Usually sine and cosine functions are used there.\\n\\nThe Nx layer can be repeated multiple times and contains the core component of transformers: \\nthe attention layer, shown in orange in Figure \\\\ref{fig:transformers}.\\nWe observe three arrows feeding into the attention mechanism, representing the key, \\nvalue, and query. \\nThe query represents the token we want the model to focus on. \\nIt gets compared to the keys using a dot \\nproduct followed by \\na softmax function, which determines the relevance of each key. \\nThis comparison helps to assign a specific weight or percentage to each value, allowing \\nthe model to combine them in a way that highlights the most important information related to the query.\\n\\nIn more detail, let\\'s look at the one in the encoder. \\nIt uses multi-head self-attention, where the key, value, and query all come from the previous layer, \\nmaking it a self-attention layer.\\nThe query are the tokens of the input, so we want to represent \\nthese tokens in a sense, \\nthat also the other tokens of the sequence are taken into account. So to find a \\nrepresentation of a token, its query is calculated. Then it is compared to all the \\nkeys of the tokens. It is calculated \\nwhich token is the most similar one and will give a proportion of which tokens are \\nimportant to use in the representation. \\n\\nFor example, let\\'s have \\'I ate an apple it tasted good.\\' When we want to find \\na representation for \\'it\\', it should have an high similarity with the word apple. \\nLet\\'s say with apple \\nit gives 0.5 and with it (the word itself) also 0.5 and all the other words 0. \\nThus, we combine 0.5 of the value for \\'apple\\' and 0.5 of the value for \\'it\\' to represent the word \\'it.\\n\\nIn Figure \\\\ref{fig:transformers}, we can also see an arrow skipping the attention \\nlayer and going \\ndirectly to the Add \\\\& Norm layer.\\nThis helps preserve the original input while allowing the attention mechanism to focus on \\nrelationships and similarities between tokens.\\n\\n\\n\\nThe orange sub-layer in \\\\ref{fig:transformers} is called multi-head attention because \\nmultiple sets of keys, values, and queries are created from \\neach token, each performing the same operation. Since the weights used to create \\nthe keys, values, and \\nqueries are different in each head, they can focus on different types of relationships.\\\\\\\\\\n\\n\\nLet\\'s examine the attention layers of the decoder, which operate in a similar way to \\nthose in the encoder. \\nFirst, there is a masked multi-head attention layer. Like in the encoder, residual \\nconnections are applied around each sub-layer, followed by layer normalization. \\nThe self-attention sub-layer in the \\ndecoder stack is also modified to prevent each position from attending to subsequent \\npositions. During deployment, we don\\'t know the next token as we are calulating it. \\nAt training time we can\\'t allow the system to know the next token, as this would be cheating.\\nThis masking\\nensures that \\nthe prediction for position i can depend only on the known outputs at positions before i.\\n\\nNext is a multi-head attention layer, also known as a cross-attention layer. \\nUnlike self-attention, where all inputs come from the same source, in cross-attention, \\nthe key and value come from the encoder, while the query comes from the previous \\nlayer in the decoder. \\nYou can think of it this way: the query represents the output, or the sentence \\nthe model has generated \\nso far. The query essentially asks, \\'What should the next word (or token) be?\\' \\nTo answer this, it compares the \\nquery with the key from the encoder, which contains the information \\nfrom the input—the prompt \\nprovided by the user. Finally, the value helps determine what the next token should be.\\\\\\\\\\n\\nAt the end of the decoder there are output probabilities. With the help of the \\nvocabulary these can be\\ntransformed into a token and finally into a word to complete the output. The output is \\nfed again into the decoder to produce \\nthe next token. And so on and on until the token \\\\texttt{<EOS>} \\'end of sequence\\' \\nis produced and the \\nprocedure stops.\\\\\\\\\\n\\n\\\\subsection{Decoder only Transformers}\\n%https://www.youtube.com/watch?v=bQ5BoolX9Ag\\n\\nPreviously, we discussed encoder-decoder transformers, which utilize both an encoder and a decoder. \\nIn contrast, decoder-only transformers feature only the decoder component. The input and the current \\noutput get combined into one sequence and fed into the decoder.\\n\\nIn a decoder-only transformer, the encoding and generating functions are combined \\nwithin the same \\ncomponent. \\nDuring encoding, the attention mechanism determines the relationships between tokens, \\nbut it operates \\ndifferently than in a regular transformer. While a regular transformer uses self-attention \\n(in the encoder) that allows \\nall input tokens to be visible, a decoder-only transformer uses masked self-attention. \\nThis masking ensures that only preceding tokens are visible to the model, preventing it from accessing \\nfuture tokens.\\n\\nIn training the encoder-decoder transformer uses also masked attention but only on the output.\\nIt prevents the model from cheating and looking ahead what the correct next token is \\nand also it makes it faster. However, decoder only \\ntransformers use the masked attention all the time.\\n\\nAt the generation or inference stage, regular encoder-decoder models leverage keys and \\nvalues from the \\nencoder along with the query from the decoder, as explained before in the cross-attention layer. In \\ncontrast, decoder-only transformers operate differently. \\nIn these models, all components — keys, values, \\nand queries — are generated within the decoder itself. This design allows the \\ntransformer to encode and \\ngenerate outputs without needing a separate encoder component. \\nThis simplifies the model but keeps the \\nquality.\\n\\n\\n\\\\subsection{RAG (Retrieval augmented generation)}\\n%https://stackoverflow.blog/2023/10/18/retrieval-augmented-generation-keeping-llms-relevant-and-current/\\nLLMs, like everyone else, are limited by what they know. During training, they learned how language \\nworks based on large amounts of text. At this time they also picked up all the knowledge they have, out of \\nthe training texts. There are two major limitations.\\nLike mentioned earlier, there is hallucination. When asked about something not in the training texts, \\nthe LLM may invent an answer. In such cases, its answer isn\\'t based on facts.\\nThe other limitation is the actuality. LLMs quickly become outdated on trends and any information \\noccurring after their last training or fine-tuning.\\n\\nThe idea of Retrieval-Augmented Generation (RAG) is to add context or additional \\ninformation when generating a response. This can either come from an access to the internet, or \\nfrom a connected\\ndatabase. Figure \\\\ref{fig:RAG} shows a simplified concept.\\nThe blue part is a program or \\ninterface orchestrating everything. The user sends a prompt to this program. It searches \\ncorresponding data in the database. Typically, it is a vector database, containing text \\nchunks where each chunk has a vector representing its content. The retriever gets a \\nvector out of the prompt and compares it with the vectors of the database. The text parts with the \\ncorresponding highest similarity scores are returned to the program. It combines the user\\'s \\nprompt with the text chunks and sends it to the LLM. The response from the LLM can be formatted \\nand given to the user.   \\n\\nRAG can increase the quality of the response of an LLM with domain-specific knowledge or up-to-date \\ninformation.\\nIt is also been demonstrated that RAG can effectively reduce hallucinations in \\nconversational tasks \\\\cite{shuster-etal-2021-retrieval-augmentation}.\\n\\n\\\\begin{figure}\\n    \\\\centering\\n    % \\\\scalebox{0.5}{\\\\includegraphics{images/tasks_overview.png}}\\n    \\\\scalebox{0.5}{\\\\includegraphics{images/RAG_concept.png}}\\n    % Add a caption\\n    \\\\caption{Concept of RAG}\\n    % Label for referencing\\n    \\\\label{fig:RAG}\\n\\\\end{figure}\\n\\n\\n\\\\section{Evaluation scores}\\nThe traditional natural language generating metrics compare overlapping n-grams as in \\nthe ROUGE score.\\\\\\\\\\nIn the paper \"LLM-based NLG Evaluation: Current Status and Challenges\" they propose \\nshow an overview of more evaluation method of a natural language generated text in the figure \\n\\\\ref{fig:metric_overview}. There are LLM-derived metrics such as BERTScore. \\nThey use model-based evaluation techniques to compare the summary of the LLM to an ideal \\nsummary. Then there is the idea of directly prompting an LLM and ask it to score the\\nsummary. The specialized evaluation LLMs are fine-tuned with labeled data.\\\\\\\\\\nIn this work evaluation of the summaries is concentrated on the comparison to the abstract.\\\\\\\\\\nWe used the ROUGE score, the BERTScore and asking the LLM to self evaluate with the abstract.\\n\\n\\n\\n\\\\begin{figure}[h!]\\n    \\\\centering\\n    % Include the graphic (replace \\'example-image\\' with your image file name)\\n    \\\\includegraphics[width=0.8\\\\textwidth]{/home/deborah/FS24/masterarbeit/writing/msc-thesis/images/overview_metrics.png}\\n    % Add a caption\\n    \\\\caption{This is an overview of looking at evaluating LLM summaries}\\n    % Label for referencing\\n    \\\\label{fig:metric_overview}\\n\\\\end{figure}\\n\\\\subsection{ROUGE}\\n\\nThe ROUGE (Recall-Oriented Understudy for Gisting Evaluation) metric is commonly \\nused to evaluate summaries by comparing them to reference summaries. \\nIt focuses on the overlap of n-grams, word sequences, and word pairs between the \\ngenerated summary and the reference summaries. There \\nare several variants of ROUGE, including ROUGE-N, ROUGE-L, ROUGE-W, and ROUGE-S.\\\\\\\\\\nROUGE-L score uses the longest common subsequence (LCS) of both texts. As an example \\ntake \"the cat sat on the mat\" and \"the mat is where the cat sat\" then the LCS \\nis \"the cat sat\".\\\\\\\\\\nWe then calculate precision, recall and F1 measure with it:\\\\\\\\\\n\\nPrecision: The proportion of the LCS length to the length of the generated summary.\\\\\\\\\\n\\nRecall: The proportion of the LCS length to the length of the reference summary.\\\\\\\\\\n\\nF1-Score: The harmonic mean of precision and recall, providing a balanced measure.\\\\\\\\\\n\\n\\n\\\\[\\n\\\\text{Precision} = \\\\frac{\\\\text{LCS length}}{\\\\text{Length of generated summary}}\\n\\\\]\\n\\\\[\\n\\\\text{Recall} = \\\\frac{\\\\text{LCS length}}{\\\\text{Length of reference summary}}\\n\\\\]\\n\\\\[\\nF1 = 2 \\\\times \\\\frac{\\\\text{Precision} \\\\times \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}\\n\\\\]\\n\\\\\\\\\\n\\nLet\\'s examine some examples in the context of task 1, evaluations of summaries.\\nHere we assess the summary\\'s similarity to the abstract.\\nWe compare reference text and candidate text.\\\\\\\\\\n\\nLet\\'s say we have a highly similar candidate text. We only remove the last sentence.\\nWhat is the impact on the rouge scores?\\n\\\\[\\n\\\\text{RougeScore Precision}: 1.0000, \\\\text{Recall}: 0.8291, \\\\text{F1}: 0.9066\\n\\\\]\\nHere the candidate text is completely present in the reference text. Hence the precision is 1.\\nLCS and length of generated summary are the same. The recall is a bit smaller than 1 because not\\neverything in the reference summary is present in the generated candidate summary. We can see that \\nthe F1 score is a balance between the precision and recall. And also we see that if there is \\nsomething missing in the candidate summary this is captured in the recall, not touching the precision. \\n\\nOn the other hand if the candidate summary has everything inside but also more sentences. The precision and recall will \\nbe swapped. The recall will be 1 and the additional words reduce the precision.\\nBut in this example, we see that a very similar candidate text gives us favorable scores\\\\\\\\\\n\\n\\nLet\\'s say we look at a very low-quality  summary. Our summary only says \"This is about\". We get the \\nfollowing scores:\\n\\\\[\\n\\\\text{RougeScore Precision}: 1.0000, \\\\text{Recall}: 0.0151, \\\\text{F1}: 0.0297\\n\\\\]\\nThe positive aspect here is, a low-quality summary gives low scores. The precision is 1 because \"This is about\"\\nis very common in a summary and here in our example also present in the reference summary.\\nBut in this example we can detect the low-quality summary at looking at the low F1 score.\\\\\\\\\\n\\n\\n\\nNext, we look at a similar candidate text, but this time we replaced some words with synonyms.\\nSpecifically:\\n\\\\begin{itemize}\\n    \\\\item \"short-term\" is now \"short-duration\",\\n    \\\\item \"discussed\" is now \"examined\",\\n    \\\\item \"diagnosis\" is now \"evaluation\",\\n    \\\\item \"proves against\" is now \"disproves\".\\n\\\\end{itemize}\\nWe expect that the scores are very high, because the content is the same and the words are almost \\nthe same. \\n\\\\[\\n\\\\text{RougeScore Precision}: 0.9798, \\\\text{Recall}: 0.9749, \\\\text{F1}: 0.9773\\n\\\\]\\nTo compare we also tested the a similar summary where we just removed the synonym words.\\n\\\\[\\n\\\\text{RougeScore Precision}: 1.0000, \\\\text{Recall}: 0.9749, \\\\text{F1}: 0.9873\\n\\\\]\\nThese are somewhat unexpected results. The summary where the four words were removed has better scores \\nthan the one with the synonyms. The difference makes the precision. The synonyms are not in the \\nreference text, hence they lower the score. But of course the first text with the synonyms is a \\nbetter text, the ROUGE score does not reflect that accurately. \\nThis was only a simple example where ROUGE fails, but the main problem is, it focuses too much on \\nthe actual words than on their meanings. We keep in mind that ROUGE score measures the word \\noverlap of two text and can say how similar in terms of words are they.\\n\\n\\n\\\\subsection{BERTScore}\\nThe BERTScore \\\\cite{zhang2020bertscoreevaluatingtextgeneration}\\nevaluates summaries by comparing them to an ideal summary.\\nIt can compare texts, not based on common words\\nbut on the content. \\nBERTScore uses the BERT model. BERT (Bidirectional Encoder Representations from Transformers) has \\na encoder only architecture, it is built of the encoder described in section Transformers\\n\\\\ref{subsec:transformers}. BERT focuses on encoding input text with deep contextual \\nunderstanding by looking at both the left and right context simultaneously (bidirectional).\\nThis contextual understanding is what we are looking for. \\n\\n\\n\\\\begin{figure}[h!]\\n    \\\\centering\\n    \\\\includegraphics[width=0.8\\\\textwidth]{images/BertScore.png}\\n    % Add a caption\\n    \\\\caption{Illustration of how to calculate Recall metric $R_{BERT}$ from \\\\cite{zhang2020bertscoreevaluatingtextgeneration}}\\n    % Label for referencing\\n    \\\\label{fig:BertScore}\\n\\\\end{figure}\\n\\nFigure \\\\ref{fig:BertScore} illustrates the steps of calculating the BERTScore. There is a \\nreference text and candidate text, same concept as in the ROUGE score. Here is the example with\\n\"the weather is cold today\" as reference and \"it is freezing today\" is the sentence that is tested. \\nBoth texts go through BERT, each word gets a vector, represented in \\\\ref{fig:BertScore} as 5 \\nrespectively. 4 stacks of blue squares.\\nThen the pairwise cosine similarities are calculated. Each word from one text gets matched with \\none word from the other text based on the maximum of their similarity. In this example \"cold\" is \\nmatched with \"freezing\". The vector called \"idf weights\" has its purpose in weighting the words \\nby how important they are. \"It\" and \"the\" are commonly used words hence they have a small weight\\ncompared to \"today\", \"weather\" and \"cold\". Combine this to a score.\\n\\nLike in ROUGE there are recall, precision and F1 scores. Here are their formulas, without the \\nweighting. \\n\\n\\\\[\\nR_{BERT} = \\\\frac{1}{|x|} \\\\sum_{x_i \\\\in x} \\\\max_{x_j \\\\in \\\\hat{x}_j} x_j \\n\\\\]\\n\\n\\\\[\\nP_{BERT} = \\\\frac{1}{|\\\\hat{x}|} \\\\sum_{\\\\hat{x}_j \\\\in \\\\hat{x}} \\\\max_{x_i \\\\in \\\\hat{x}_j} x_i\\n\\\\]\\n\\n\\\\[\\nF_{BERT} = \\\\frac{2 \\\\cdot P_{BERT} \\\\cdot R_{BERT}}{P_{BERT} + R_{BERT}}\\n\\\\]\\n\\\\\\\\\\n\\nWe already know that intuition of the recall from ROUGE, it captures how much of the information \\nof the reference text is present in the candidate text. Looking at the formula we see that the \\nsum is over $x$ the reference text. It means we sum for all words in the reference text the \\nsimilarity of the matching word in the candidate text. Dividing it by the length of the reference \\ntext makes it an average.\\nThe precision captures how high the quality of the candidate text is. How much of its content is \\ncorrect, i.e., aligns with the reference text. Thats why we have the average over the words of the candidate \\ntext, calculating its similarity to the reference text.\\n\\nLet\\'s do the same miniexamples as we did with the ROUGE score.\\n\\n\\\\begin{table}[!htbp]\\n    \\\\centering\\n    \\\\begin{tabular}{|l | c | c | c | c |}\\n    \\\\hline\\n    \\\\textbf{Operation} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1 Score} & \\\\textbf{F1 ROUGE} \\\\\\\\ \\n    \\\\hline\\n    Remove last sentence & 0.9918 & 0.9585 & 0.9748 & 0.9066 \\\\\\\\ \\n    \\\\hline\\n    Low-quality summary & 0.3344 & 0.1858 & 0.2389 & 0.0297 \\\\\\\\ \\n    \\\\hline\\n    Replace synonyms & 0.9850 & 0.9880 & 0.9865 & 0.9773 \\\\\\\\ \\n    \\n    Remove synonyms & 0.9717 & 0.9638 & 0.9677 & 0.9873 \\\\\\\\ \\n    \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{BERTScore of Miniexamples with ROUGE to Compare}\\n    \\\\label{tab:bertscore_rouge}\\n\\\\end{table}\\n\\nRegarding the first example we can see in table \\\\ref{tab:bertscore_rouge} that not only the \\nrecall diminished but also the precision.\\nBecause removing the last sentence does change the meaning of the whole text a little bit.\\nWe can see that the BERTScores are a bit higher than the ROUGE scores. The low-quality summary does not \\nproduce as a low BERTScore as it does with the RougeScore. We will have to keep in mind that \\nBERTScore 0.2 is already very low. \\nFor the synonym example we have significantly better results. Replacing the four words with \\nsynonyms keeps the meaning. Just removing them changes the content more. This is reflected in the \\nBERTScore but was not in the ROUGE score. \\nAll in all BERTScore is a valuable score to compare text based on the content.\\n\\n\\\\subsection{Prompting LLM}\\n\\nThe idea is to ask an LLM to score a text. As mentioned in related work, there have been studies \\nthat LLMs are quite valuable a givinig scores. An advantage is that you can ask it for a justification\\nfor this score. This makes it easier for humans to understand. \\nAnother advantage is that you are very flexible. It\\'s possible to ask directly for a score of a text,\\nor give a score of a text in comparision to a reference text. It is also possible to compare \\ntwo candidate texts, and calculate a score out of this. Your imagination is the limit, as you can \\nformulate the task in natural language.\\n\\nHowever, the flexibility is also a disadvantage because the score depends very much on the prompt. \\nSome small changes, or a different formulation can change the score completely. This makes it \\nhard to compare scores, to somebody else\\'s work. Another disadvantage is that an LLM answers in text,\\nnot in a number. It can be challenging getting the right format of answer.\\\\\\\\ \\n\\n\\nLet\\'s do the same miniexamples as with the other scores.\\n\\n\\n\\\\begin{table}[!htbp]\\n    \\\\centering\\n    \\\\begin{tabular}{|l | c | l |}\\n    \\\\hline\\n    \\\\textbf{Operation} & \\\\textbf{Score} & \\\\textbf{Explanation} \\\\\\\\ \\n    \\\\hline\\n    Remove last sentence & 1.0 & \\'The candidate summary is identical to the ideal summary...\\' \\\\\\\\ \\n    \\\\hline\\n    Bad summary & 0 & \\'The candidate summary does not provide any meaningful information...\\' \\\\\\\\ \\n    \\\\hline\\n    Replace synonyms & 1.0 & \\'The candidate summary accurately and completely captures the main points ...\\' \\\\\\\\ \\n    \\n    Remove synonyms & 1.0 & \\'The candidate summary is almost identical ... with minor typographical errors...\\'\\\\\\\\ \\n    \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Here are the scores of the Miniexamples}\\n    \\\\label{tab:selfeval_mini}\\n\\\\end{table}\\n\\nThe score of 1 for the remove sentence is encouraging, we would like a high score here. Interestingly \\nthe LLM says that the candidate summary is identical, it didn\\'t recognize that the last sentence \\nwas removed.\\nThe low-quality summary score is fair, since it is no real summary. This score allows us to clearly see \\nwhen the LLM fails to understand the summarization task.\\nIn the third example we have both times a score of 1. Here the LLM understood that the replacement\\nof the synonmys doesn\\'t change the content and gave it full score. When the words were missing \\nit took it for typographical errors. Unfortunately, these errors are not reflected in the score.\\nWe have to keep this in mind.\\\\\\\\\\n\\n\\nLet\\'s look at an example that illustrates the problem with the prompt. In asking to score the \\nsummary we gave the criterias accuracy, completeness, coherence, and relevance. It should also \\ngive the \\noverall score. (Exact prompt in chapter experiments.)\\nThe answer is quite complete:\\n\\n\\\\begin{lstlisting}\\n    Accuracy: 0.95 (The candidate summary accurately captures the main points of \\n    the ideal summary, with some minor differences in detail and phrasing.)\\n\\n    Completeness: 0.85 (The candidate summary covers most of the key points from \\n    the ideal summary but may be missing some minor details or nuances.)\\n    \\n    Coherence: 1.0 (The candidate summary flows logically and makes sense as a whole, \\n    with clear connections between ideas.)\\n    \\n    Relevance: 1.0 (The candidate summary accurately reflects the content of \\n    the original text and remains focused on the topic of sunspot data analysis.)\\n    \\n    Overall Score: 0.925 (An average of the four scores above)\\n\\\\end{lstlisting}\\n\\nLooking at this answer we can follow the evaluation process. However the arithmetic mean of \\n$0.95$, $0.85$, $1$ and $1$ would be $0.95$. It is not clear if the LLM just can\\'t calculate or if \\nit did weight the average. If it did weight for example the completeness more, \\nthen it is not transperent.\\\\\\\\\\n\\n\\nNow we really only care about the overall score. So if we add to the prompt \\n\"Only give the overall score.\", we would think this saves us some postprocessing steps.\\nInstead it changes the score:\\n\\n\\\\begin{lstlisting}\\n    Overall Score: 0.95\\n\\n    The candidate summary accurately and completely captures the main points...\\n\\\\end{lstlisting}\\n\\n\\nSo we see scoring with LLMs is highly dependent on the prompt used.\\n\\nThere are more considerations to make. What is the bias this LLM has? Does it prefer longer \\nor shorter summaries? Does it prefer generated texts? Does it prefer generated texts it has \\nwritten itself?\\n\\nMethod\\nIn this chapter we discuss the methods used for the three tasks outlined in the introduction. \\nWe describe the dataset and pipline for each task. In figure \\\\ref{fig:tasks_overview} you \\ncan see an illustration of each of the three tasks. We\\'ll also cover the technologies and \\ntools that were essential for running the experiments.\\n\\n\\\\begin{figure}\\n  \\\\centering\\n  \\\\scalebox{0.5}{\\\\includegraphics{images/overview_all_tasks.png}}\\n  \\\\caption{Overview over the three tasks}\\n  \\\\label{fig:tasks_overview}\\n\\\\end{figure}\\n\\n\\\\section{Task 1: Summarization}\\nThis task consists of evaluating how effectively a LLM can create a summary of a \\nscientific text. LLMs are known to excel at summarizing texts. \\nHowever, a significant challenge is how to evaluate LLM-generated summaries.\\n\\nHow can we assign a score to a summary? An effective summary should contain the most \\nimportant information from the original text while remaining concise. \\nIt should accurately present the facts of the text, avoiding misinterpretation or \\nfabrication. Moreover, even human evaluators can have differing opinions regarding \\nthe quality of a summary, which emphasizes the need for an automated evaluation metric.\\n\\nIn this study, we used a ground truth summary created by the author of the text to \\ncompare with the LLM-generated summary. However, comparing these summaries remains \\ndifficult. One approach is to examine whether the same words occur in both summaries. \\nYet, identical words can have different meanings depending on their context. \\nAdditionally, we can express the same meaning using different words.\\n\\nTherefore, how can we calculate similarity in a text based on its meaning? \\nHow can LLMs assist in the evaluation process?\\n\\n\\\\subsection{Dataset}\\nThe dataset consists of scientific papers from the repository ArXiv.\\nArXiv is a free, open-access repository for scholarly papers, \\nparticularly in the fields of physics, mathematics, computer science, \\nquantitative biology, quantitative finance, statistics, \\nelectrical engineering and systems science, and economics.\\n\\nAs mentioned in chapter \\\\ref{cha:related}\\n\\\\cite{cohan2018discourseawareattentionmodelabstractive} created a dataset with ArXiv papers.\\nWe used part of the dataset provided by them. \\nIt can be downloaded with tensorflow as scientific\\\\_papers.\\nEach paper has an id, abstract\\\\_text and article\\\\_text.\\nThe id coresponds to the id given by ArXiv. The abstract is the abstract of\\nthe paper, every scientific paper has it. Its purpose is to give an overview\\nover the paper and summarize the key findings, hence it is a perfect example of a \\nsummary by the author. The article\\\\_text is the full text of the paper without the abstract.\\nThey removed figures and tables using regular expressions to only preserve the textual \\ninformation. The dataset has 215\\'913 articles.\\n\\n\\n\\n\\\\subsection{Summary generation and evaluation}\\n\\nEach paper has the abstract and the text itself without the abstract. First they \\nwere separated.\\nThe text is given to the LLM to generate summaries. A very simple prompt and a more \\ncomplex prompt were used. At the end the summaries were evaluate by comparing.\\nThe BERTScore, rougescore and a self evaluating score of the LLM was used to score \\nall of the summaries. \\nTo get a score it needs a reference and a candidate as showed in figure \\n\\\\ref{fig:task1_compare_overview}\\nThe abstracts are used as the ground truth summaries for scoring the generated summaries. \\nHowever, it is also possible to directly compare the generated summaries to the full \\npaper. To establish a baseline for interpreting these scores, we also compared the \\nabstracts to the original paper.\\n\\n\\\\begin{figure}[!h]\\n  \\\\centering\\n  % Include the graphic (replace \\'example-image\\' with your image file name)\\n  \\\\includegraphics[width=0.5\\\\textwidth]{images/Compare.png}\\n  % Add a caption\\n  \\\\caption{We can apply the Bert or ROUGE score different ways. Compare the abstract (blue) to the created summary (yellow) or compare them resp. to the entire text (green).}\\n\\n  \\\\label{fig:task1_compare_overview}\\n\\\\end{figure}\\n\\n\\n\\n\\n%\\\\pagebreak\\n\\n\\\\section{Task 2: Selection}\\nHow efficient is an LLM in selecting relevant papers? The idea is we would give the LLM \\na topic and a list \\nof papers and it should decide which of the papers are relevant. A big challenge was to \\ndecide which data \\nuse. There has to be a ground truth of which papers are related.\\n\\\\subsection{Dataset creation}\\nWe created our own dataset from papers from ArXiv. The idea is to have a paper to start with and get its\\nreferences as related papers. We chose 10 papers as original papers. They were chosen with no particular \\nreasoning. But they have to be some survey or overview over a particular topic. \\nAlso the paper are rather recent and \\nin the field of computer science. The following papers have been chosen.\\\\\\\\\\n\\n\\n\\\\begin{itemize}\\n    \\\\item 2402.01383: LLM-based NLG Evaluation: Current Status and Challenges \\\\cite{gao2024llmbasednlgevaluationcurrent}\\n    \\\\item 2402.06196: Large Language Models: A Survey \\\\cite{minaee2024largelanguagemodelssurvey}\\n    \\\\item 2408.02304: Embedding Compression in Recommender Systems: A Survey \\\\cite{Li_2024}\\n    \\\\item 2408.02464: Fairness and Bias Mitigation in Computer Vision: A Survey \\\\cite{dehdashtian2024fairnessbiasmitigationcomputer}\\n    \\\\item 2408.02085: Unleashing the Power of Data Tsunami: A Comprehensive Survey on\\n    Data Assessment and Selection for Instruction Tuning of Language Models \\\\cite{qin2024unleashingpowerdatatsunami}\\n    \\\\item 2311.13731: A Survey of Blockchain, Artificial Intelligence, and\\n    Edge Computing for Web 3.0 \\\\cite{zhu2023surveyblockchainartificialintelligence} \\n    \\\\item 2311.12785: Prompting Frameworks for Large Language Models: A Survey \\n    \\\\cite{liu2023promptingframeworkslargelanguage}\\n    \\\\item 2409.15816: Diffusion Models for Intelligent Transportation Systems: A Survey\\n    \\\\cite{peng2024diffusionmodelsintelligenttransportation}\\n    \\\\item 2409.15180: A Comprehensive Survey with Critical Analysis for Deepfake Speech Detection\\n    \\\\cite{pham2024comprehensivesurveycriticalanalysis}\\n    \\\\item 2409.09957: Deep Graph Anomaly Detection: A Survey and New Perspectives\\n    \\\\cite{qiao2024deepgraphanomalydetection}\\n  \\\\end{itemize}\\n\\nFrom each of the papers the references have been extracted. Because of simplicity, only reference papers \\nwith their ArXiv id were considered. This reduced the number of references. We collected their id, \\npublishing date, abstract and full text. \\n\\nWe created smaller standart sized dataset for task 2. We chose 10 papers out of the \\nreference papers then we added 10 unrelated papers. In finding unrelated papers \\nseveral things were to be considered. The unrelated has to be published earlier \\nthan the original paper. This simulates the research context, at the time of writing \\nthe original paper, the authors had access only to existing literature.\\nThe task also shouldn\\'t be too easy, papers from a completely different field \\nmight be very simple to spot.\\nHence we chose took papers related to computer science.\\nAnother concern was if we take always the same unrelated papers, we would show that \\nthe LLM can detect that these papers are unrelated, rather than the objective that it \\ncan detect related papers. So we increased the number of papers by 50 and chose \\nrandomly 10 out of it for each dataset.\\n\\n%After the smaller datasets worked well we created datasets with all the references found. They have \\n%different seizes, it can be 8 or 108 papers. Each time there are the same numbers of related as\\n%unrelated ones.\\n\\n\\\\subsection{Selecting articles}\\nThere are the same numbers of related and unrelated articles. \\nWe ask the LLM which of these are related \\nto a topic. The topic consists of the original title of the paper follwed by its abstract. \\nIt should answer with the \\ncorresponding arxiv ids, these were extracted of the LLM\\'s answer. \\nThen we calculated the precision, recall and F1 value.\\n\\n%\\\\subsection{Selecting articles yes or no}\\nInstead of giving the list of related and unrelated articles we gave them one by one.\\nIt is interesting to test how good the quality is if the topic is defined as the title \\nand the abstract or only the title.\\n\\n\\\\section{Task 3: Combining to SotA}\\nIn Task 2, we learned methods to get relevant sources to a topic.\\nNow in Task 3 the idea is to combine these sources to a coherent text. \\nWe can look at it \\nas a summary of multiple sources or as a text towards a state of the \\nart paper of the topic.\\n\\n\\\\subsection{Dataset}\\nSince the work to create the database in task 2 was quite a lot. We tried to reuse part of it here.\\nWe took the first paper \\\\cite{gao2024llmbasednlgevaluationcurrent}. We took its title as the topic.\\nIts sources which we collected previously are the relevant sources to this topic.\\n\\nWhen we want to use the concept of RAG, we need an additional step where we create a vector database.\\nFirst all reference papers have to be loaded from the pdfs, they are called documents. \\nThen each document has to be splitted in even sized chunks of text. \\nThe lenght is a parameter we can choose.\\n\\nTo create the database each of these text parts have to be embedded with a sentence \\ntransformer. Each \\npart has also metadata, for example the source. The source is the name of the document it \\ncames from,\\nthis is very useful as it can serve as a reference. An example of how these sources are given\\nis in the Result section with the example \\\\ref{RAG example}. \\n\\n\\\\subsection{Creation of texts}\\nAs always with LLM\\'s we need a prompt to generate a text. Here the idea can be to summaries \\nthe several texts or it can be to create a state of the art based on some texts.\\nWe used two different prompts with two ideas. \\\\\\\\\\n\\\\begin{itemize}\\n  \\\\item Combiniation of the sources: The prompt asked the LLM to summarize the key findings from all \\n  relevant texts. \\n  \\\\item State-of-the-Art Overview: The LLM was asked to describe recent developments in the topic area, \\n  using the title of the original paper as the query topic and only using the context.\\n\\\\end{itemize}\\n\\nExact prompt is in the \\nexperiment chapter.\\\\\\\\\\n\\nThe texts or the context can be the entire papers, or some parts of it. We can for example \\nonly take the abstracts or we can with RAG choose parts of the text wich are more relevant.\\n\\n\\nWe expeimented with what parts of the source papers should be given as context. \\n\\\\begin{itemize}\\n  \\\\item Full Papers: All content from the source documents was used as input.\\n  \\\\item Abstracts Only: A \"summary of summaries\" approach.\\n  \\\\item RAG Retrieval: Chunks selected dynamically by the RAG retriever based \\n  on relevance to the prompt.\\n\\\\end{itemize}\\n\\n\\nThe idea of the RAG Retrieval is to select only the relevant parts of the papers \\nbased on the prompt. Now is time to use the retriever. \\nIt gets the best fitting chunks of text from the\\ndatabase based on the prompt. For this the retriever needs the same embedding function as \\nwas used to create the database. It calculates the embedding of the prompt. \\nThe prompt here was the question about new developments in a certain topic.\\nThis resulting embedding vector gets compared to the embedding vectors in the database.\\nThe retriever uses cosine similarity to retrieve the top k texts.\\nHere we can choose of how many we want to use. The idea is that because they have a similar \\nembedding they talk about the same topic and can so be used to answer the prompt.\\nThese texts are then combined\\nwith the prompt to a system prompt and given to the LLM.\\n\\n\\n\\\\subsection{Evaluation}\\nThe evaluation is as always a difficult part. We have to read the generated texts to judge if they \\nmake sense. We looked at how many of the papers did the LLM use. Another criteria is does it list \\nsources and are these correct or hallucinations. Finally, we compared it to the original paper with \\nthe BERTScore.\\n\\n\\n\\n\\\\section{Technologies Used}\\nWhenever in this work we talk about the LLM, we used the \\n\\\\textbf{mistral model} version v0.2. With its details in \\n\\\\cite{jiang2023mistral7b}\\n% https://ollama.com/library/mistral\\nThe Mistral model is a state-of-the-art large language model (LLM) designed for \\nnatural language processing tasks, known for its efficiency and high performance in \\ngenerating coherent and contextually relevant text. \\nIt is part of the family of transformer-based models, which are widely used for \\ntasks such as text generation, summarization, and translation. The mistral is \\na decoder only model, as described in chapter \\\\ref{cha:fondamentals}.\\nAdditionally this model\\nleverages grouped-query attention (GQA) for faster inference, \\ncoupled with sliding window attention (SWA) to effectively handle \\nsequences of arbitrary length with a\\nreduced inference cost \\\\cite{jiang2023mistral7b}.\\nVersion 2 could replace the sliding window attention to enlarge the context length to \\n32768 tokens. %https://ollama.com/library/mistral/blobs/ff82381e2bea\\n%\\nMistral-7B models use byte-pair encoding (BPE) tokenization, \\nwhich is widely used in large language models to balance efficiency and accuracy \\nin representing text. \\n\\nWe chose mistral 7B because it is small enough to use on a local machine, \\nbut still \\nhas a good performance. For example it outperforms bigger models as Llama 2 13B.\\n\\nWhile Mistral is pre-trained on a diverse corpus of texts, for this study, \\nwe used it as it is, relying on its pre-trained knowledge.\\n\\nIn LLM\\'s the parameter temperature can be chosen. It controls the randomness or \\ncreativity of the model\\'s output and influences how the model selects words during \\ntext generation when predicting the next word in a sequence.\\nTypically the value is between 0 and 1. Here the temperature was consistently set \\nto zero. This means that always the word with the highest probability gets chosen.\\nThis ensures that the experiments can be consistently replicated.\\n\\nWe downloaded the model from Ollama website.\\nThe LLM was accessed through an jupyter notebook with the library Ollama from \\nlangchain.\\n\\n\\nTo get the ArXiv papers the python package arxiv was used.\\n\\n\\nExperiments \\nIn this chapter, we present the experiments conducted for the three tasks \\noutlined in the introduction and detailed in the methodology.\\nEach task contains a setup, where we describe the experimental setput, a result \\nsection and a discussion where the results are analysed.\\n\\n\\\\section{Task 1: Summarization}\\nIn this section, we describe the experimental setup, results and discussion \\nfor Task 1, where we evaluate the performance of an LLM in generating \\nsummaries in a scientific context. We analyze the outcomes based on BERTScore, \\nROUGE, and other relevant metrics.\\n\\\\subsection{Setup}\\nThe dataset is splitted in \\'test\\', \\'train and \\'validation\\' section with resp. 6,440, \\n203,037 and 6,436 articles. We only used part of the \\ntest split.\\nFor a first qualitative experiment we used the first 10 papers of the \\ntest section, this allowed to read the summaries and have a opinion about the scores. \\nThen we used the first 100 papers for the quantitative approach.\\n%To ensure reproducibility we set the temperature of the Mistral llm to 0 as mentioned. \\n%For more details about python packages the reader can check out the github repository and \\n%there the requirement.txt.\\n\\nAs mentioned in methods we used two different prompts for generating the summaries. A simple one \\nand one optimized by GPT-4, as shown in list \\\\ref{lst:prompts} below.\\n\\n\\n\\\\begin{lstlisting}[caption={Prompts for Summarizing}]\\nsimple prompt = \"Write a summary of following article:\"\\ncomplex prompt = \"Please summarize the following scientific paper to create a concise \\nabstract. The summary should include the research objective, methods, key results, and \\nconclusions. Ensure the summary is clear and follows the structure of a typical \\nscientific abstract.\"\\n\\n\\\\end{lstlisting}\\n\\\\label{lst:prompts}\\n\\nTo apply the BERTScore we used \\\\texttt{BERTScorer(model\\\\_type=\\'bert-base-uncased\\')}. \\nAnd for the ROUGE score \\\\texttt{rouge\\\\_scorer.RougeScorer([\\'rougeL\\'], use\\\\_stemmer=True)}. \\nFor selfevaluating with the LLM \\nwe tried out different prompts and decided to keep these following 2.\\n\\n\\\\begin{lstlisting}\\n    \"\"\" I have an ideal summary and a candidate summary of a text. Please score \\n        the candidate summary on a scale of 0 to 1 based on accuracy, completeness, \\n        coherence, and relevance. Note if it is not a summary or unrelated the score \\n        should be low, still give a score. Be strict.\\n        At the end combine the scores to a Overall Score. \\n                    \\n        Ideal Summary:\\n        \"\"\"{abstract}\"\"\"        \\n\\n        Candidate Summary:\\n        \"\"\"{summary}\"\"\"\\n\\n        Score:\\n        \"\"\"\\n\\\\end{lstlisting}\\n\\n\\\\begin{lstlisting}\\n    \"\"\" I have an ideal summary and a candidate summary of a text. Please score \\n        the candidate summary on a scale of 0 to 1 based on accuracy, completeness, \\n        coherence, and relevance. Note if it is not a summary or unrelated the score \\n        should be low, still give a score. Be strict.\\n        At the end combine the scores to a Overall Score. \\n        Only give the overall score.\\n                    \\n        Ideal Summary:\\n        \"\"\"{abstract}\"\"\"        \\n\\n        Candidate Summary:\\n        \"\"\"{summary}\"\"\"\\n\\n        Score:\\n        \"\"\"\\n\\\\end{lstlisting}\\n\\nTo be consistent with how we used ROUGE and BERTScore we also tested the generated summary \\nagainst the abstract, which is call ideal summary here. Also the score has to be between 0 and 1. \\nOtherwise it invents its own scale, not necessaraly the same for all summaries. Give the criterias\\naccuracy, completeness, coherence and relevance also unifies the answers. These criterias were \\nchosen because they are usually used in human evaluation.\\nSometimes it happened that bad summaries wouldn\\'t get a score or were to nice, this is why it is \\nexplicitly demanded in the prompt. The difference between these two prompts are the \\n\\'Only give the overall score.\\' sentence at the end of prompt 2. \\n\\n\\\\subsection{Results}\\nHere we look at the results of the task 1. We want to find out how efficient LLM\\'s are at \\nsummarization.\\nWith ROUGE, BERTScore and model- selfevaluation we test the generated summaries.\\nThere is the Quantitative Analysis were we present the scores against the abstracts.\\nIn the Comparative Analysis we consider as well the scores obtained in the way of figure \\n\\\\ref{fig:task1_compare_overview}. The Qualitative Analysis section looks at the generated \\nsummaries in more details and gives as well examples.\\n%- Briefly introduce the purpose of your evaluation.\\n%- Outline the different metrics and methods used: ROUGE, BERT, LLM self-evaluation, and comparisons.\\n%- Explain what will follow in this section Results\\n\\n\\\\subsubsection{Quantitative Analysis}\\nIn Table \\\\ref{tab:mean_results} we can see the metrics. These are the means over the 100 \\nsummaries, generated by the simple prompt (left) or the complex prompt (right).\\nThe self-evalaution 1 and 2 are the scores given by the LLM itself with 2 different prompts.\\nThere was one summary from the complex prompt that couldn\\'t get scores in the self-evaluation.\\n\\nWe can see that values for the simple and complex prompt are quite similar. For Bert scores the \\nsimple prompt is even a bit better. While the self-evaluation gives better scores to the complex \\nprompt. The ROUGE scores are the lowest ones, followed by the BERTScores and the self ones are \\nthe highest. BERT and ROUGE only compare the similarity, while the last one can understand the \\ncontext.\\n\\n\\n\\n\\\\begin{table}\\n    \\\\centering\\n    \\\\begin{tabular}{|l|c|c|c|c|}\\n    \\\\hline\\n    \\\\textbf{Metric} & \\\\textbf{Simple Prompt} & \\\\textbf{Complex Prompt} \\\\\\\\\\n    \\\\hline\\n    \\\\multicolumn{3}{|c|}{\\\\textbf{BERTScore}} \\\\\\\\\\n    \\\\hline\\n    Precision & 0.552 & 0.549 \\\\\\\\\\n    Recall & 0.597 & 0.591 \\\\\\\\\\n    F1 Score & 0.572 & 0.567\\\\\\\\\\n    \\\\hline\\n    \\\\multicolumn{3}{|c|}{\\\\textbf{ROUGE}} \\\\\\\\\\n    \\\\hline\\n     \\n    Precision & 0.145 & 0.145 \\\\\\\\\\n    Recall &  0.258 & 0.259 \\\\\\\\\\n    F1 Score & 0.171 &  0.171 \\\\\\\\\\n    \\\\hline\\n    \\\\textbf{Self-Evaluation 1} & 0.776 & 0.814* \\\\\\\\\\n    \\\\textbf{Self-Evaluation 2} & 0.735 & 0.753* \\\\\\\\\\n    \\n    \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Mean Scores for BERTScore and ROUGE Across Simple and Complex Prompts, \\n    Including Self-Evaluation Scores. Note: * there is missing one summary.}\\n    \\\\label{tab:mean_results}\\n\\\\end{table}\\n\\n\\nNow look at the figure \\\\ref{fig:task1_compare}. It gives us more details about the same scores. \\nThe Rouge and Bert score are the F1 values as it combines the precision and recall neatly.\\nThe blue and green box represent where 50 \\\\% of the scores are. With the orange line as the median.\\nWhile Rouge and Bert score are quite narrow around the median, the self evaluation scores are wider.\\nThe cercles represent outliers, especially strong or weak summaries. \\nThe self evaluation metrics spanns from 0 to 1, hence can describe more differences in the quality of \\ntheses summaries. Most of them have a high score and there are some lower self evaluation scores which \\ncan be considered outliers.\\n%Now look at how the values are distributed over the summaries in table \\\\ref{tab:first_prompt_results}.\\n%These one is only with the simple prompt.   \\n%- More details: mean, sd, min, max for each prompts seperately, possible box plot to illustrate\\n%- Histogram of distribution of scores ( selfevalaution more 0 and 1, bert and rouge more centered\\n%by the mean) selfevaluationpromt 1 and 2. Over simple and complex summary?\\n\\n\\\\begin{figure}\\n    \\\\centering\\n    % Include the graphic (replace \\'example-image\\' with your image file name)\\n    \\\\includegraphics[width=0.6\\\\textwidth]{images/scores_task1.png}\\n    % Add a caption\\n    \\\\caption{Scores of the different metrics and two prompts.}\\n  \\n    \\\\label{fig:task1_compare}\\n  \\\\end{figure}\\n\\n\\n\\n%\\\\begin{table}\\n %   \\\\centering\\n  %  \\\\begin{tabular}{|l|c|c|c|c|}\\n   % \\\\hline\\n    %\\\\textbf{Metric} & \\\\textbf{Mean} & \\\\textbf{Std Dev} & \\\\textbf{Min} & \\\\textbf{Max} \\\\\\\\\\n%    \\\\hline\\n %   \\\\multicolumn{5}{|c|}{\\\\textbf{BERTScore}} \\\\\\\\\\n  %  \\\\hline   \\n   % Precision & 0.552 & 0.071 & 0.334 & 0.695 \\\\\\\\\\n    %Recall & 0.597 & 0.078 & 0.429 & 0.752 \\\\\\\\\\n%    F1 Score & 0.572 & 0.067 & 0.399 & 0.686 \\\\\\\\\\n %   \\\\hline\\n  %  \\\\multicolumn{5}{|c|}{\\\\textbf{ROUGE}} \\\\\\\\\\n   % \\\\hline\\n    %Precision & 0.145 & 0.061 & 0.019 & 0.433 \\\\\\\\\\n%    Recall & 0.258 & 0.098 & 0.088 & 0.646\\\\\\\\\\n %   F1 Score & 0.171 & 0.047 & 0.033 & 0.360 \\\\\\\\\\n  %  \\\\hline\\n   % \\\\textbf{Self-Evaluation 1} & 0.776 & 0.296 & 0.000 & 1.000 \\\\\\\\\\n    %\\\\textbf{Self-Evaluation 1} & 0.735 &  0.309 & 0.000 & 1.000 \\\\\\\\\\n%    \\\\hline\\n %   \\\\end{tabular}\\n  %  \\\\caption{Results for BERTScore and ROUGE Metrics (Simple Prompt) Including Mean, Standard Deviation, Min, and Max Values}\\n   % \\\\label{tab:first_prompt_results}\\n%\\\\end{table}\\n\\nIn the table \\\\ref{tab:summary_lengths} you can see that a longer more complex prompt \\nmakes in average longer summaries.\\n\\n\\\\begin{table}\\n    \\\\centering\\n    \\\\begin{tabular}{|c|c|}\\n        \\\\hline\\n        \\\\textbf{Type} & \\\\textbf{Average Word Count} \\\\\\\\\\n        \\\\hline\\n        Summaries (simple prompt) & 257.42 \\\\\\\\\\n        Summaries (complex prompt ) & 271.31 \\\\\\\\\\n        Abstracts & 172.18 \\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Average Word Count for Summaries and Abstracts}\\n    \\\\label{tab:summary_lengths}\\n\\\\end{table}\\n\\n\\\\subsubsection{Comparative Analysis}\\n\\nPreviously the scores were made by comparing to the abstract. But without reference numbers it is \\ndiffcult to say if these numbers are meaningful. So in this part we compared to the original text.\\nIn table \\\\ref{tab:combined_results} we can see how well our generated summaries are with the \\noriginal text. To set this in context there are also the values of the abstract against the \\noriginal text.\\n\\n\\\\begin{table}[h!]\\n    \\\\centering\\n    \\\\begin{tabular}{|l|c|c|c|}\\n    \\\\hline\\n    \\\\textbf{Metric} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1 Score} \\\\\\\\\\n    \\\\hline\\n    \\\\multicolumn{4}{|c|}{\\\\textbf{BERTScore}} \\\\\\\\\\n    \\\\hline\\n    Generated vs. Original & 0.591 & 0.550 & 0.569 \\\\\\\\\\n    Abstract vs. Original  & 0.656 & 0.565 & \\\\textbf{0.606} \\\\\\\\\\n    \\\\hline\\n    \\\\multicolumn{4}{|c|}{\\\\textbf{ROUGE}} \\\\\\\\\\n    \\\\hline\\n    Generated vs. Original & 0.627 & 0.058 & \\\\textbf{0.098} \\\\\\\\\\n    Abstract vs. Original  & 0.657 & 0.027 & 0.050 \\\\\\\\\\n    \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{BERT and ROUGE Scores: Generated Summaries vs. Original Text and Abstract vs. Original Text}\\n    \\\\label{tab:combined_results}\\n\\\\end{table}\\n\\nWe can see in table \\\\ref{tab:combined_results} that scores of our generated summary and the abstract from \\nthe author of the paper. The BERTScores of the abstract are higher, precision and recall. \\n%Which indicates better quality.\\nHowever in terms of Rougescore the generated summary has higher scores because of the higher recall value.\\n\\n\\n\\\\subsubsection{Qualitative Analysis}\\nNow let\\'s look at only the 10 first summaries. Do the different metrics align? \\nIs a summary with low scores truly of poor quality? \\nWhat are examples of high-quality or low-quality summaries?\\n\\n\\n\\n\\\\begin{table}[h!]\\n    \\\\centering % Center the table\\n    % Start the tabular environment\\n    \\\\begin{tabular}{|c|c|c|c|c|}\\n        \\\\hline % Horizontal line at the top\\n        \\\\textbf{Index} & \\\\textbf{Rouge F1} & \\\\textbf{Bert F1 } & \\\\textbf{Self 1} & \\\\textbf{Self 2}\\\\\\\\ % Add a header row\\n        \\\\hline\\n        0 & \\\\textbf{0.265} & \\\\textbf{0.662} & 0.925 & 0.800\\\\\\\\\\n        \\\\hline\\n        1 & 0.077 & 0.399 & 0.850 & 0.000\\\\\\\\\\n        \\\\hline\\n        2 & 0.160 & 0.618 & \\\\textbf{1.000} & 0.950\\\\\\\\\\n        3 & 0.121 & 0.589 & 0.925 & 0.850\\\\\\\\\\n        4 & 0.190 & 0.589 & 0.985 & 0.950\\\\\\\\\\n        5 & 0.210 & 0.596 & 0.925 & 0.950\\\\\\\\\\n        6 & 0.144 & 0.612 & 0.985 & 0.950\\\\\\\\\\n        \\\\hline\\n        7 & 0.133 & 0.485 & 0.350 & 0.000\\\\\\\\\\n        \\\\hline\\n        8 & 0.193 & 0.528 & 0.825 & 0.850\\\\\\\\\\n        9 & 0.175 & 0.626 & 0.920 & 0.950\\\\\\\\\\n        \\\\hline \\n    \\\\end{tabular} \\n    % Add a caption (optional)\\n    \\\\caption{10 First scores for first prompt}\\n\\n    % Add a label (optional)\\n    \\\\label{tab:10_prompt1}\\n\\n\\\\end{table}\\n\\nIn table \\\\ref{tab:10_prompt1} we can see the values of the 10 first summaries. The summaries 1 \\nand 7 have the lowest numbers in almost all scores. They score even 0 with in the selfevaluation \\nwith prompt 2. For the prompt 1 and BERTScore these summaries have also the lowest values. Only \\nfor rouge, there exist a summary (3) with score as low. \\n\\nLet\\'s look at summary 1 in \\\\ref{lst:summary 1 with simple prompt} : Instead of a summary it gave a list of the references \\nused in the paper. Here the low values correspond.\\n\\n\\n\\\\begin{quote}\\n    It appears that you have provided a list of references related to gravitational \\n    physics and astrophysics, specifically those dealing with topics such as \\n    black holes, gravitational waves, and cosmology. Here is the information \\n    extracted from the given references:  \\n    1. Jackiw and Pi (2003) - Phys. Rev. D **68**, 104012 (gr-qc/0308071) \\n    2. Satoh, Kanno, and Soda (2008) - Phys. Rev. D **77**, 023526 (astro-ph/07063585) \\n    ...\\n\\\\end{quote}\\n\\\\label{lst:summary 1 with simple prompt}\\n\\nIn the next table \\\\ref{tab:10_prompt2} are scores of the complex prompt summaries. \\nThere as well the summary 1 and 7 got the lowest scores across. \\n\\n\\\\begin{table}[h!]\\n    \\\\centering % Center the table\\n\\n    % Start the tabular environment\\n    \\\\begin{tabular}{|c|c|c|c|c|}\\n        \\\\hline % Horizontal line at the top\\n        \\\\textbf{Index} & \\\\textbf{Rouge F1} & \\\\textbf{Bert F1 } & \\\\textbf{Self 1} & \\\\textbf{Self 2}\\\\\\\\ \\n        \\\\hline\\n        0 & \\\\textbf{0.244} & \\\\textbf{0.638} & 1.000 & 0.850 \\\\\\\\\\n        \\\\hline\\n        1 & 0.142 & 0.481 & 1.000 & 0.100\\\\\\\\\\n        \\\\hline\\n        2 & 0.196 & 0.611 & 0.925 & 0.950\\\\\\\\\\n        3 & 0.117 & 0.619 & 0.925 & 0.950\\\\\\\\\\n        4 & 0.187 & 0.549 & 0.925 & 0.950\\\\\\\\\\n        5 & 0.219 & 0.615 & 0.925 & 0.850\\\\\\\\\\n        6 & 0.133 & 0.587 & 1.000 & 0.850\\\\\\\\\\n        \\\\hline\\n        7 & 0.116 & 0.498 & 0.700 & 0.250\\\\\\\\\\n        \\\\hline\\n        8 & 0.167 & 0.513 & 0.825 & 0.800\\\\\\\\\\n        9 & 0.211 & 0.608 & 1.000 & 0.950\\\\\\\\\\n\\n        \\\\hline % Horizontal line after the second row\\n    \\\\end{tabular}\\n    \\n    % Add a caption (optional)\\n    \\\\caption{Scores for the second prompt}\\n\\n    % Add a label (optional)\\n    \\\\label{tab:10_prompt2}\\n\\n\\\\end{table}\\n\\n\\\\begin{quote}\\n    It appears that you have provided a list of research papers and related resources \\n    in the field of physics, specifically in areas such as gravitation, \\n    quantum mechanics, astrophysics, and general relativity. Each entry includes \\n    the authors\\' names, journal titles, publication years, DOI numbers, and arXiv \\n    preprint IDs (where available). \\n    Some entries also include book titles and their respective publishers. \\n     Here are a few notable papers from the list:  \\n     1. Jackiw and Pi (2003) - \"Gravitational Waves from Colliding Cosmic Strings\" \\n     (Physical Review D, arXiv:hep-th/0308071) \\n     2. Takahashi and Soda (2009) - \"Probing the Quark Gluon Plasma with Gravitational \\n     Waves from Heavy Ion Collisions\" (Physical Review Letters, arXiv:gr-qc/0904.0554) \\n    ...\\n\\\\end{quote}\\n\\\\label{summary 1 with complex prompt}\\n\\n\\nWe see in \\\\ref{summary 1 with complex prompt} that the same problem  occurse as in the simple prompt \\nsummary, the LLM understands the paper as a list of papers. However the complex summary is better because \\nit attemps to summarize, where the simple one only lists the papers.\\n\\nThe summary with the highest ROUGE and BERT scores is Summary 0. \\nUpon review, it is indeed a well-constructed summary, presenting the key information \\neffectively, though not identical to the abstract.\\n \\n\\nMost summaries have a high quality but the LLM mistral has a tendency\\nto interprete the text with references as a list of references\\nand does therefore in this cases write less effective summaries.\\nIn both tables we see that the metric for the summary for paper 1 is the lowest. \\nReading the summaries, this is coherent.\\\\\\\\\\n\\n\\n\\\\subsection{Discussion}\\nThe primary aim of this experiment was to evaluate the effectiveness of large language models (LLMs) \\nin generating accurate, concise, and coherent summaries of scientific articles. \\nSpecifically, we tested the impact of simple versus complex prompts on the quality of the summaries, \\nevaluated using BERT, ROUGE, and self-evaluation scores.\\n\\n\\\\subsubsection{Rouge Scores}\\nThe Rouge score indicate the lexical overlap of the summaries. As already mentioned, it is a poor\\nindicator of the quality of a summary, but still can be informative. We see that its values are low,\\naround 0.1 and 0.2. The simple and the complex prompt are very similar.\\n\\nLet\\'s see the precision and recall relationship. In table \\\\ref{tab:mean_results} we can see the \\nrecall is higher than the precision. The reason can be that the reference here is shorter than the \\ncandidate. Table \\\\ref{tab:summary_lengths} shows that the abstract is shorter than the generated summaries. \\nHowever it is the other way round when we compared the summary to the entire text as in table \\n\\\\ref{tab:combined_results}, there the precision is higher than the recall, which is very low. Here the \\nreference text is longer than the candidate. \\n\\nSince we used the same data as \\\\cite{cohan2018discourseawareattentionmodelabstractive} and performed \\nthe same task of summarization, it is interesting to compare with their results. They used different \\nRouge metrics, among them also Rouge L, which we used. \\nThey created their own LLM model which got 0.3180 Rouge L score. Their much higher score can be \\nexplained as their model was build specifically for this task and trained to maximize the Rouge score. \\nWhile we used the mistral model which was trained for a variety of tasks. \\n\\nWe see in table \\\\ref{tab:combined_results} that the generated summaries have a higher F1 value than the \\nabstracts. This means they have a higher lexical overlap. Or in other words the generated summaries \\nare more extractive, more phrases from the text were taken to formulate the summary, instead of rewriting.\\nWe can also see that the lenght of the texts influence the scores quite much. \\n\\n\\\\subsubsection{Bert score}\\nThe BERT score provides insight into the semantic similarity between the candidate and the \\nreference text. Here, we observe that the summaries generated with the simple prompt slightly \\noutperform those from the complex prompt in terms of BERT score.\\n\\nInterestingly, in the comparative experiment, the abstract has a higher BERT score than \\nthe generated summaries. This suggests that human-generated abstracts capture the content of \\nthe full text more effectively than the LLM-generated summaries. However, it is important to \\nnote that conclusions drawn from a single metric can be limiting, as BERT focuses on semantic \\nsimilarity alone.\\n\\n\\\\subsubsection{selfevalaution}\\n\\nThe self-evaluation scores are the highest among all metrics. \\nThis might be because the LLM self-evaluation does not only assess similarity \\n(as ROUGE and BERT do), but likely considers other aspects of the summary. \\nUnfortunately, the exact criteria it evaluates are not transparent to the user.\\n\\n\\nThe range of self-evaluation scores (from 0 to 1) is particularly helpful for comparing summaries. \\nUnlike traditional metrics, self-evaluation seems to capture a broader range of quality indicators. \\nFor instance, summaries with low self-evaluation scores were often those where the LLM \\nmisinterpreted the paper\\'s content, such as mistaking it for a list of references, \\nor failing in another way to generate an appropriate summary. \\nThis shows that the self-evaluation scores can reflect meaningful distinctions that other \\nmetrics might overlook.\\n\\n%\\\\pagebreak\\n\\n\\\\section{Task 2: Selection}\\nIn this section, we describe the experimental setup, results and discussion \\nfor Task 2, where we answer the question: \\nHow effective is an LLM in selecting relevant papers?\\n\\n\\\\subsection{Setup}\\nFirst tests were made on the first paper:\\n\"LLM-based NLG Evaluation: Current Status and Challenges\".\\nIt has 45 references. Form there we chose randomly 10 papers that will serve as ground truth\\nof related to the topic. We added randomly 10 related to computer science. \\nThey serve as non-related papers. Also we checked that they were all published before the \\noriginal paper. There are now 10 related and 10 unrelated papers and the LLM has to correctly \\nidentify the 10 related ones.\\nOnce the dataset was created the same papers were used.\\n\\n\\nTo ask the LLM we gave it instructions and the list of papers. Topic is the title and \\nthe abstract of the original paper. The list of papers (choice) consists \\nof the papers number, title and abstract. After each paper a new line follows.\\n\\n\\\\begin{lstlisting}[caption={prompt to select out of list}]\\n    \"Your task is to determine which papers are relevant to the topic: {topic}.\\n    Below is a list of papers. Each paper is numbered and includes its title \\n    and summary.\\n\\n    List of Papers:\\n    {choice}\\n\\n    Instructions:\\n\\n    Indicate which papers are relevant to the topic by writing only the numbers \\n    of the relevant papers.\\n    Do not provide any explanations or use any other numbers.\\n    Format your answer as a list of numbers separated by commas.\\n    Please provide your answer below:\"\\n\\\\end{lstlisting}\\n\\\\label{lst:out_of_list}\\n\\nIn the list \\\\ref{lst:out_of_list} is the prompt that was used to ask the LLM to do the task. Give the numbers of \\nthe relevant papers. To find out if the order of the papers plays a role in the answer of the LLM, the list was shuffled \\neach time. This was executed 10 times.\\\\\\\\\\n\\n\\nThe next experiment was where each paper was seperately asked if it is relevant. Here the 10 original \\npapers mentioned in the methods were used with each of them have 10 relevant and 10 non relevant papers.\\nThe following prompt was used.\\n\\n\\n\\\\begin{lstlisting}\\n    \"Your task is to determine which papers are relevant to the topic: {topic}.\\n    Indicate if the following paper is relevant: {choice}.\\n    Only say yes or no. Please provide your answer below:\"\\n\\\\end{lstlisting}\\n\\n\\\\subsection{Results}\\n\\\\subsubsection{List of Papers}\\nFirst the results from shuffeling the list of papers.\\n\\n\\\\begin{table}[!h]\\n    \\\\centering\\n    \\\\begin{tabular}{|c|c|c|c|c|c|c|c|}\\n        \\\\hline\\n        \\\\textbf{run} & \\\\textbf{Predictions} & \\\\textbf{Correct Predictions} & \\\\textbf{Hallucinations} \\n        & \\\\textbf{Duplicates} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1}\\\\\\\\\\n        \\\\hline\\n        0 & 4.00 & 2.00 & 0.00 & 1.00 & 0.50 & 0.20 & 0.29\\\\\\\\\\n        1 & 3.00 & 1.00 & 1.00 & 0.00 & 0.33 & 0.10 & 0.15\\\\\\\\\\n        2 & 2.00 & 1.00 & 0.00 & 0.00 & 0.50 & 0.10 & 0.17\\\\\\\\\\n        3 & 4.00 & 2.00 & 0.00 & 0.00 & 0.50 & 0.20 & 0.29\\\\\\\\\\n        4 & 5.00 & 3.00 & 1.00 & 0.00 & 0.60 & 0.30 & 0.40\\\\\\\\\\n        5 & 5.00 & 2.00 & 2.00 & 0.00 & 0.40 & 0.20 & 0.27\\\\\\\\\\n        6 & 2.00 & 2.00 & 0.00 & 0.00 & 1.00 & 0.20 & 0.33\\\\\\\\\\n        7 & 4.00 & 3.00 & 0.00 & 0.00 & 0.75 & 0.30 & 0.43\\\\\\\\\\n        8 & 8.00 & 3.00 & 2.00 & 2.00 & 0.38 & 0.30 & 0.33\\\\\\\\\\n        9 & 9.00 & 2.00 & 1.00 & 1.00 & 0.22 & 0.20 & 0.21\\\\\\\\\\n        \\\\hline\\n        \\\\textbf{Mean} & 4.60 & 2.10 & 0.70 & 0.40 & 0.52 & 0.21 & 0.29\\\\\\\\\\n        %\\\\textbf{Standard deviation} & & & & & & & \\\\\\\\\\n        \\\\hline \\n    \\\\end{tabular}\\n    \\\\caption{The same list of papers was given to the LLM but in each run in a different order.}\\n    \\\\label{tab:predictions_summary}\\n\\\\end{table}\\n\\n\\n    \\n\\nThe LLM was questioned 10 times with a different order of the list of papers. \\nWe see in \\\\ref{tab:predictions_summary} that the results differ, \\nthe answer is depending on the order of the list of papers, \\nwhich is not ideal. \\nWe see that the number of papers that are found is between 2 and 3, out of then\\nthis is quite bad. Remeber there are possibily 10 to find.\\n\\nInterestingly the recall doesn\\'t vary much, there is a low standard deviation. Even if there are more \\npredictions made, still the same amount of related papers are correct, only 2 or 3 of the 10 papers \\nare found.\\nThe precision varies more, if there are more answers given the precision is worse, more of the \\nanswers are wrong. \\n\\nAs seen in table \\\\ref{tab:predictions_summary} there are even some hallucinations. Out of the list we \\ngave the LLM it did invent some paper ids. \\n\\n\\n%Since there are not always the same results, I tried to combine them. Combining all, \\n%all 10 related papers are found but there are also 8 wrong ones. This leads to a good recall but\\n%bad precision.\\\\\\\\\\n%Next I combined the answers by I take only numbers that are present in the answers at least twice.\\n%There we have 9 correct numbers and 7 incorrect ones. This leads to precision 0.5625, recall 0.9,\\n%%and F1 mesure 0.6923. This is an improvement from the single experiments, but a the precision is \\n%nearly not good enough.\\\\\\\\\\n\\n\\\\subsubsection{One by One}\\nNext the LLM was asked one by one paper if it is relevant or not. In table \\\\ref{tab:metrics} there \\nare the results for all 10 original papers, each with precision, recall and F1 score. The results \\nlook satisfying. Especially the precision is very high. Meaning that if the LLM selects an article as \\nrelevant it most likely really is.\\n\\n\\\\begin{table}[ht]\\n    \\\\centering\\n    \\\\begin{tabular}{|l|c|c|c|}\\n        \\\\hline\\n        \\\\textbf{Paper ID} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1 Score} \\\\\\\\\\n        \\\\hline\\n        2402.01383   & 1.0   & 1.0   & 1.0 \\\\\\\\\\n        2402.06196   & 1.0   & 1.0   & 1.0 \\\\\\\\\\n        2408.02304   & 1.0   & 1.0   & 1.0 \\\\\\\\\\n        2408.02464   & 0.9   & 0.9   & 0.9 \\\\\\\\\\n        2408.02085   & 1.0   & 0.8   & 0.9 \\\\\\\\\\n        2311.13731   & 0.9   & 0.7   & 0.8 \\\\\\\\\\n        2311.12785   & 1.0   & 1.0   & 1.0 \\\\\\\\\\n        2409.15816   & 1.0   & 0.9   & 0.9 \\\\\\\\\\n        2409.15180   & 1.0   & 0.7   & 0.8 \\\\\\\\\\n        2409.09957   & 1.0   & 1.0   & 1.0 \\\\\\\\\\n        \\\\hline\\n        \\\\textbf{Mean}         & 0.98 & 0.9  & 0.93\\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Precision, Recall, and F1 Scores for Papers}\\n    \\\\label{tab:metrics}\\n\\\\end{table}\\n\\n\\n\\\\subsection{Discussion}\\n%However it is questionable how good LLMs are in selection of any kind because of the positional\\n%bias. \\\\cite{liusie2024llmcomparativeassessmentzeroshot} used LLM\\'s as evaluator for comparative \\n%assessment. There the LLM had to choose out of two texts which one was better. They showed that \\n%the first one had an advantage, ergo an positional bias.\\n\\nThe experiment giving the LLM a list clearly failed. The reason behind is not completely clear. \\nOne could think that some related papers were harder to find. But the correctly detected papers were \\ndifferent ones at each time. When we combine all the 10 runs all 10 related papers were in the answers.\\nThis hints that the structure of the question is the problem.\\nIt could be that the LLM didn\\'t \"understand\" which content belonged to which paperid. \\nAnother reason could be it has positional bias as \\\\cite{liusie2024llmcomparativeassessmentzeroshot} \\ndetected in their research. As mentioned before, they used LLM\\'s as evaluator for comparative \\nassessment. There the LLM had to choose out of two texts which one was better. They showed that \\nthe first one had an advantage, ergo an positional bias. Here it could be similar, that some position \\nin the list of papers are more likely to get picked as relevant papers than others, regardless of \\nthe papers content.\\\\\\\\\\n\\nNow look at the second experiment where we asked the LLM one by one. \\nThese results are very successful,\\nmany got a perfect score. It is interesting to look deeper into one example. What is the reason for \\nthe not perfect scores? Importantly we have to ask ourself if the ground truth is correct.\\n\\nLet\\'s take one of the original papers 2311.13731 \\n\"A Survey of Blockchain, Artificial Intelligence, and Edge Computing for Web 3.0\". It It scored $0.875$ \\nfor precision and $0.7$ in recall as table \\\\ref{tab:metrics} informs. The LLM considered 8 papers \\nrelevant out of them there are 7 labeled related. The name of the false positive is \"Crypto Makes AI Evolve\"\\nOne can argue and also the LLM argues that this paper is relevant to the topic, as it \\n\"it discusses the intersection of artificial intelligence and cryptography\". However the ground truth \\ntells us this paper is not relevant as it wasn\\'t listed by the authors of the the original paper. \\nAnd of course there exist papers that are relevant but aren\\'t referenced in the original paper.\\nOne could argue that the paper \"Crypto Makes AI Evolve\" shouldn\\'t have been in the selection list. \\n\\nNow the LLM picked 7 out of the 10 relevant papers. For example the paper \"A Comprehensive Survey \\nof AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\" was labeled as \\nrelated. But as the LLM commented is this paper not directly related. It is related as it talks about \\nAI, but it doesn\\'t have a connection with blockchain or the Web 3.0 as the original paper. \\nThe other two false negatives are similar. One could argue that they are related or not. \\nBut of course a paper also references other papers that don\\'t relate to them in every aspect.\\nShouldn\\'t this paper have been labeled as relevant? One could argue either way. \\nAlso the the prompt could be modified to include more papers as relevant.\\n\\nLooking at this example, the results of table \\\\ref{tab:metrics} may not refelect the ability of an LLM \\nto select relevant papers but the shortcommings of our dataset. Regardless, the LLM has shown strong \\ncapabilities to select related content.\\\\\\\\\\n\\nOne question is if an LLM should be used to select relevant papers. There exist many other ways to get \\nrelated sources to a topic. Taking the search in the arxiv database. With some keywords you get \\nrelated papers. If for example you give the title \"LLM-based\\nNLG Evaluation: Current Status and Challenges\", it gives successfully relevant results. If you ask the LLM if these \\npapers are relevant, it does agree. We did another experiment. We collected the first 100 \\npapers from ArXiv which are related to LLM-based NLG Evaluation: Current Status and Challenges”. In \\n100 there are more likely some papers that are indeed not related. We asked the LLM for each paper \\nif it is relevant or not. The result was that the LLM rejected 6 out of these 100 papers, that they \\nare not relevant. It did give an explanation why not, for example: \"No, this paper is not relevant \\nto LLM-based NLG Evaluation: Current Status and Challenges as it focuses on Nonlocal Gravity (NLG) \\nin the context of classical physics, specifically gravitation, rather than Natural Language \\nGeneration (NLG) evaluation.\" Here we see that the LLM could successfully detect a mismatched \\nabbreviation. In the search \"NLG\" responded to \"nonlocal gravity\". But the LLM \"understood\" that these is \\nnot the same thing. This small experiment showes that LLM\\'s have can judge relevance on a deeper level \\nof context, than for example keyword spotting. \\n\\n%- compare with GPT reaserch assistant, they have a list from arxiv of papers  and used GPT to pick \\n%the best one. Good? Good results because arxiv give relevant papers.\\n\\n%\\\\pagebreak\\n\\n\\\\section{Task 3: Combining to SotA}\\nIn this section, we describe the experimental setup, results and discussion for \\nTask 3. Here we combine summaries of summaries or sources articles \\nto create state of the art. \\n\\n%-Experiment with entire texts.\\n%-Experiment with only the abstracts\\n%-Experiment with only parts, decided by RAG\\n\\n\\\\subsection{Setup}\\nIn this task we used the same technologies as in the other tasks, explained in the \\nMethod chapter.\\nWe experimented with three different approaches to generate the combined text, \\nbased on full papers, abstracts, and RAG (Retrieval-Augmented Generation). \\nThese approaches were chosen to explore how different input granularities and retrieval methods affect the quality and coherence of the output.\\n\\\\begin{itemize*}\\n    \\\\item Using full texts of all papers.\\n    \\\\item Using only abstracts from the same papers.\\n    \\\\item Using RAG (Retrieval-Augmented Generation) to selectively retrieve relevant chunks.\\n    \\\\item Repeat the best way with more data.\\n\\\\end{itemize*}\\nFirst we tried to take the full text of all papers. And summarize them. Here is the prompt\\nused:\\n\\n\\\\begin{lstlisting}[caption={Prompt 1}]\\n    Write a summary combining the key findings of following texts: \\n    {context} \\n    Write a summary.\\n\\\\end{lstlisting}\\n\\nIn the context were the full texts. The length of the context is 3\\'924\\'350 character or\\n499\\'106 words. This leads to a context of 1\\'282\\'202 tokens.\\\\\\\\\\n\\n\\nAnd as a second experiment only the abstracts of the papers \\nwere used. Here two different prompts were used. The same as with the full papers and a \\nprompt that ask for a state of the art, instead of a summary of the papers. Here is this \\nprompt:\\n\\n\\\\begin{lstlisting}[caption={Prompt 2}]\\n    question = f\\'what are recent developments in {topic}?\\'\\n    instruction = f\"\"\"\\n    Answer the question based only on the following context:\\n    {context}\\n    - -\\n    Answer the question based on the above context: {question}\\n    Write your answer in about 2000 words.\\n    \"\"\"\\n\\\\end{lstlisting}\\n\\nThe setup for the experiments using RAG:\\n\\n\\n\\\\begin{table}[ht]\\n    \\\\centering\\n    \\\\begin{tabular}{|l|l|c|c|}\\n        \\\\hline\\n        \\\\textbf{Embedding} & \\\\textbf{Size of Text Chunk} & \\\\textbf{Database} & \\\\textbf{\\\\# of Text Chunks} \\\\\\\\\\n        \\\\hline\\n        langchains SentenceTransformerEmbeddings & 300 or 1000 & ChromaDB & 5, 8 or 10 \\\\\\\\\\n        (model\\\\_name=\"all-MiniLM-L6-v2\") &  &  &   \\\\\\\\\\n\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{Parameters for RAG experiments}\\n    \\\\label{tab:task3 setup}\\n\\\\end{table}\\n\\nLike described in table \\\\ref{tab:task3 setup} two separate databases were created, \\none with 300-character chunks and the other with 1000-character chunks. \\nThis was done to investigate how chunk size affects the coherence and quality of \\nthe generated text.\\n\\n    300-character chunks:\\n        Designed to capture smaller segments but at the cost of losing some context.\\n        Useful for evaluating whether smaller chunks offer more precise extraction.\\n\\n    1000-character chunks:\\n        Designed to maintain coherence across larger text portions.\\n        Aimed at exploring if longer input segments lead to more meaningful summaries and connections.\\\\\\\\\\n\\n\\nThen the RAG experiments were reproduced with more original papers, here some \\nof the same papers as in task 2 were used. For each paper a \\nnew database had to be built. The chunk texts were the lenght of 1000 characters.\\n\\n\\\\subsection{Results}\\n\\\\subsubsection{Quantitative Analysis}\\n\\nThe BERTScores comparing these outputs to the original paper are \\nshown in Table \\\\ref{tab:task3 bert}. \\nNote that we shuffled the order of abstracts for each run to \\nmitigate order bias, reporting mean scores over ten runs.\\n\\n\\\\begin{table}[ht]\\n    \\\\centering\\n    \\\\begin{tabular}{|l|r|c|c|c|}\\n        \\\\hline\\n         & \\\\textbf{Prompt} & \\\\textbf{Precision} & \\\\textbf{Recall} & \\\\textbf{F1 Score} \\\\\\\\\\n        \\\\hline\\n        Full Papers &  1 & 0.51 & 0.43 & 0.47 \\\\\\\\\\n        Summarize Abstracts &  1 & 0.59 & 0.53 & 0.56 \\\\\\\\\\n        State of the Art &  2 & 0.62 & 0.59 & 0.60 \\\\\\\\\\n        RAG Texts & 2 & 0.66 & 0.61 & 0.64 \\\\\\\\\\n\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{BERTScores for Task 3}\\n    \\\\label{tab:task3 bert}\\n\\\\end{table}\\n\\nHere in \\\\ref{tab:task3 bert} are the BERTScores. First row are the values of the text \\ngenerated by the full papers. Prompt 1 was used. We see that the values are low. \\nSecond row is the experiment with only the abstracts and the  prompt 1. We see that the F1 \\nvalue here is higher at $0.56$. The third row is with the other prompt 2, still with only the \\nabstracts. Here we see an increase in the BERTScore to $0.60$.\\nThe last row is the mean over all experiments with rag chosen text chunks, \\ndifferent lenght and different number of chunks.\\n\\nIn this table \\\\ref{tab:task3 RAG} there are the BERTScores of the different RAG \\nexperiments. We can see that the smaller size text chunks have slightly score of \\n$0.64$ instead of $0.63$.\\n\\n\\\\begin{table}\\n    \\\\centering\\n    \\\\begin{tabular}{|l|c|c|}\\n        \\\\hline\\n        & 300 Characters & 1000 Characters \\\\\\\\\\n        \\\\hline\\n        5 Chunks & 0.64 & 0.63 \\\\\\\\\\n        8 Chunks & 0.65 & 0.63 \\\\\\\\\\n        10 Chunks & 0.64 & 0.64 \\\\\\\\\\n        \\\\hline\\n        Mean & 0.64 & 0.63 \\\\\\\\\\n        Standard deviation & 0.003 & 0.005 \\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{BERTScores for RAG Experiments}\\n    \\\\label{tab:task3 RAG}\\n\\\\end{table}\\n\\nIn the next table \\\\ref{tab:task3 more papers} are results with other papers. The first one \\nis the same as before. \\n\\n\\\\begin{table}[ht]\\n    \\\\centering\\n    \\\\begin{tabular}{|l|c|c|c|c|}\\n        \\\\hline\\n        \\\\textbf{Paper id} & \\\\textbf{Best Chunk Score} & \\\\textbf{5 Chunks} & \\\\textbf{8 Chunks} & \\\\textbf{10 Chunks} \\\\\\\\\\n        \\\\hline\\n        2402.01383 & 0.70 & 0.63 & 0.63 & 0.64 \\\\\\\\\\n        2409.09957 &  0.72 & 0.60 & 0.57 & 0.59 \\\\\\\\\\n        2409.15180 & 0.67 & 0.64 & 0.65 & 0.64 \\\\\\\\\\n        2409.15816 & 0.52 & 0.60 & 0.59 & 0.62 \\\\\\\\\\n        2408.02085 & 0.64 & 0.63 & 0.62 & 0.60 \\\\\\\\\\n        2311.13731 & 0.62 & 0.63 & 0.62 & 0.62 \\\\\\\\\\n        2311.12785 & 0.64 & 0.57 & 0.57 & 0.58 \\\\\\\\     \\n        \\\\hline\\n        Mean & 0.64 & 0.61 & 0.61 & 0.61 \\\\\\\\\\n        Standard Deviation & 0.06 & 0.02 & 0.03 & 0.02 \\\\\\\\\\n        \\\\hline\\n    \\\\end{tabular}\\n    \\\\caption{\\\\textbf{RAG Experiment Over More Original Papers}. \\n    The second column are the highest similarity scores the retriever achieved. \\n    The last three columns are the results, the BERTScores of the generated texts.}\\n    \\\\label{tab:task3 more papers}\\n\\\\end{table}\\n\\nIn the table \\\\ref{tab:task3 more papers} the last three columns are the results. \\nThey give the information about the generated texts with 5, 8 resp. 10 chunks of text.\\nWe can see that the BERTScores (F1 values) \\nare high as well with other papers. These experiments are all done with chunk size \\n1000, as it \\nwas giving more coherent texts with the first original paper. We can see that \\nalthougt more chunks were giving higher results for the first paper this is not the \\ncase for other papers. The number of chunks beeing in the range of 5 to 10 doesn\\'t \\nseem to matter as the mean of all is 0.61 BERTScore. \\n\\nIn table \\\\ref{tab:task3 more papers}\\nthere is also the score of the best text chunk. This is not an end result about the text.\\nIt is a score from the generation process. The retriever gave each text chunk a score. \\nIn the column Best Chunks Score in table \\\\ref{tab:task3 more papers} are these best scores.\\nBased on the scores the chunks were ranked and then the 5,8 resp. 10 best ones were selected\\nand used to create the text.\\nAt the time of creation, this score can serve as a filter, if \\nit is too low no relevant text could be found. The score can also give a hint if a generated \\ntext is bad: If the best chunk score is low then the problem is not with the LLM, but there \\nwas simply not enough accurate inforamtion to answer the question correctly.\\n\\n\\n\\n\\n\\n\\\\subsubsection{Results from Direct Summarization}\\nBelow is an example of a generated text based on the full papers:\\n\\n\\\\begin{quote}\\n    In this example, we have two assistants providing answers to various questions with different target audiences. In the first question, Assistant 1 correctly identified that the statement \"Take a break from work and enjoy a bowl of ice cream\" is likely targeted at adults who are currently in the workforce. Assistant 2 did not provide a relevant answer as they assumed the question was related to professional athletes competing in a championship game.\\n\\\\end{quote}\\n\\\\label{Combining papers to SotA}\\n\\nThis output clearly shows that the LLM struggled with the length of the input, \\nfocusing only on content near the end of the text.\\n\\nUsing abstracts instead of full papers led to more relevant results. \\nBelow is an example of a generated text using abstracts:\\n\\\\begin{quote}\\nIn the first study, researchers evaluated BatchEval, a new text evaluation method ...\\n\\nIn the second study, researchers investigated using LLMs ...\\n\\nThe third study focused on the reliability of reference-free ...\\n\\nThe fourth study proposed AutoCalibrate, a multi-stage approach to ...\\n\\nFinally, the Eval4NLP 2023 shared task introduced a competition setting ...\\n\\\\end{quote}\\n\\\\label{Combining papers with their abstracts}\\n\\nIn this example we see that it is summarizing the papers. But it produces several summaries \\nand doesn\\'t combine them. When we shuffled the abstract we saw that the results was \\nalways a different one as expected. It was also clear that always only the last few \\n(one, three or four) abstracts were considered. \\\\\\\\\\n\\nTo combine the texts let\\'s use prompt 2. This is an example:\\n\\n\\\\begin{quote}\\n    Recent developments in Large Language Model (LLM)-based ...\\n    Firstly, let us examine some of the recent studies that ...\\n    Another approach to evaluating LLMs is by using them as judges themselves...\\n    However, evaluating LLMs based on NLG is not without its challenges...\\n    Another challenge is the presence of biases in fine-tuning LLMs as judges (JudgeLM: Fine-tuned Large Language Models are Scalable Judges)... \\n    \\n    Despite these challenges, recent developments in LLM-based ...\\n    In conclusion, recent developments in LLM-based NLG evaluation ...\\n\\\\end{quote}\\n\\\\label{prompt 2 with abstracts}\\n\\nHere in the text \\\\ref{prompt 2 with abstracts} we can see that the text \\nis well structured. It combines the texts (here abstracts) to a connected\\ntext. This text does ressemble more a state of the art article. It is here more \\ndifficult to tell if all abstracts or only a few were used. \\n\\n\\\\subsubsection{RAG Results}\\n\\nThe next step was to use the selected text chunks. Here one of the generated text:\\n\\n\\\\begin{quote}\\n    Response:  Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in NLG evaluation. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation, based on the context provided in the text.\\n\\nFirstly, it is essential to understand that NLG evaluation is a long-standing task  ...\\nHowever, current work often employs LLMs ..\\nAnother challenge is the lack of standardized evaluation metrics ...\\nFurthermore, there is a need for more research on the ethical implications ...\\nIn conclusion, recent developments in LLM-based NLG evaluation ... \\\\\\\\\\nSources: \\'2312.10355v1.CoAScore\\\\_Chain\\\\_of\\\\_Aspects\\\\_Prompting\\\\_for\\\\_NLG\\\\_Evaluation.pdf\\', \\n\\'2312.10355v1.CoAScore\\\\_Chain\\\\_of\\\\_Aspects\\\\_Prompting\\\\_for\\\\_NLG\\\\_Evaluation.pdf\\',\\\\\\\\    \\n\\'2312.10355v1.CoAScore\\\\_Chain\\\\_of\\\\_Aspects\\\\_Prompting\\\\_for\\\\_NLG\\\\_Evaluation.pdf\\',\\\\\\\\ \\n\\'2309.13308v1.Calibrating\\\\_LLM\\\\_Based\\\\_Evaluator.pdf\\',...\\\\\\\\\\nSourceText:aspect. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies...\\n\\\\end{quote}\\n\\\\label{RAG example}\\n\\nWe can see that the text is well structured with a short introduction, \\nseveral sections and a conclusion at the end. Further there is \\'Sources\\' \\nthese are the papers the text chunks are from. Here we see that \\nthe first three parts are from the same paper. At the end is optional the text chunks\\nthemselves.\\\\\\\\ \\n\\nThe sources are correct because these are the sources from the text chunks. This is \\nvery useful.\\\\\\\\ \\nThe response text from the LLM is well structured. Sometimes it writes also an \\nsource section. These sources are mostly hallucinated. If they are not, then the source \\nwas present in the text chunks. But that doesn\\'t mean that the source is correctly associated \\nwith its statement. The LLM often mixes them up, taking a real author but invents a \\npaper to this name.\\n\\nThere are sometimes factual errors. An example is that in a text claimed that \"...a \\nnew NLG evaluation metric called LLM-Based NLG Evaluation has been proposed...\"\\nHowever in the sources no specific metric is called like that but they exist metrics that \\nare LLM-Based. These kind of errors were more found in the texts generated with the \\nshorter chunks.\\n\\nTherefore sometimes the chunks were too small to get the correct context. \\nAnother problem was that in this experiment \\nthe text was spilt with the lenght of 300 or 1000 charaters, it didn\\'t cut words but \\nsometimes the sentences weren\\'t finished. The LLM would then complete these sentences, \\nnot always in the correct way. \\n\\n\\n\\n\\\\subsection{Discussion}\\n\\nThe use of the entire papers was not successful. The reason behind is that the LLM models \\nhave a limitation of tokens they can work with.\\nMistral was first designed for an input length up to 8K tokens. Afterwards input from \\nthe beginning would be overwritten.  This version we used here can handle up to 32k tokens.\\nAll papers are too long here the model forgets the beginning. In fact at the end of the \\nlast paper there is \\nsomething about an Assistant 1 and 2. The summary is now only on this last part. \\n\\nThe abstracts are 14\\'656 tokens long . This is also with the prompt shorter than 32K, \\nwhich means it should be acceptable. \\nBut it also focuses more on the last papers. As seen in \\n\\\\ref{Combining papers with their abstracts} \\nit summarizes some abstracts but not all of them. When we look at the shuffled order\\n of abstracts \\nwe see that usually the last 3 or 4 (or even only the one) last papers are taken \\ninto consideration.\\n\\n\\nWe were wondering about the lenght of chunks. 300 gave sometimes the impression of the information out of \\ncontext (not finished sentences). But 300 gave better BERTScores. \\nMaybe 1000 cited more, \\ntook the source texts as they were and therefore had a bit worse BERTScores. We know from \\ntask 1 that mistral tends to make summaries more extractive, instead of rewriting.\\n\\nThe way of splitting the papers into the text chunks could be improved. \\nOriginally having smaller parts was useful to answer specific questions.\\nHere the question is more general with \"What are recent developments\".\\n\\nIn conclusion, the experiments revealed several important insights. \\nFirst, using abstracts to generate summaries resulted in more relevant outcomes \\nthan working with the full texts of the papers. The model, when faced with long \\ninputs, struggled to capture key content, often focusing only on the last \\nportions. Second, Prompt 2 was more effective than Prompt 1 in producing \\nintegrated and coherent summaries. Furthermore, the Retrieval-Augmented \\nGeneration (RAG) approach, particularly with 1000-character chunks, delivered \\nthe best performance overall, despite some issues with hallucinations and \\nsource attribution. Interestingly, the number of chunks (5, 8, or 10) did not \\nsignificantly influence the results, indicating that chunk size and input \\nquality may be more critical factors than the sheer number of segments used. \\nThese observations point to the complexities of balancing input structure and \\nmodel capabilities and show the potential of generating state of the art texts. \\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'dataset/mythesis.txt', 'r') as file:\n",
    "        myarticle = file.read()\n",
    "myarticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_summary(article):\n",
    "    instruction = \"Write a summary of following article:\"\n",
    "    answer = llm.invoke(instruction + article)\n",
    "    answer = re.sub('\\n',' ',answer)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysummary = ask_for_summary(myarticle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" The text you provided discusses an experiment comparing different methods for summarizing recent developments in a given field using large language models. The experiment involved using the full texts of papers, their abstracts, and splitting the papers into smaller chunks as input to the model. The results showed that using abstracts led to more relevant outcomes than working with the full texts, as the model struggled to capture key content from long inputs and often focused only on the last portions.  The experiment also found that Prompt 2 was more effective than Prompt 1 in producing integrated and coherent summaries. Additionally, the Retrieval-Augmented Generation (RAG) approach with 1000-character chunks delivered the best performance overall, despite some issues with hallucinations and source attribution. The number of chunks did not significantly influence the results, indicating that chunk size and input quality may be more critical factors than the sheer number of segments used.  The text also mentions some limitations of the experiments, such as the model's token limit and the way the papers were split into text chunks. Overall, the experiment revealed important insights into balancing input structure and model capabilities and demonstrated the potential for generating state-of-the-art texts using these methods.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29623\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MistralTokenizer.v1()\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=myarticle)])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "print(len(tokens))\n",
    "#maximum is 32768 tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Introduction\\nRecently large language models (LLM\\'s) such as OpenAI\\'s GPT-4 have \\nrevolutionized natural language processing (NLP), showing remarkable capabilities to \\ngenerate human-like text. These models can answer complex questions and perform a lot \\nof difficult tasks. Now it is interesting to see how they can help in academic \\nresearch.\\n\\nIn every research it is important to have an overview what is already done and where \\nthe knowledge is at the time. Getting the state of the art in a topic is a crucial\\nstep, but it is very time consuming and can be difficult. For that reason it would \\nbe great an LLM can do it for us. In this work we would like to find out in which \\nextent LLMs can go towards to an automatic state of the art.\\\\\\\\\\n\\nLLMs, while powerful, have certain limitations. \\nThey can process large amounts of text and generate summaries, but they can also make\\nmistakes. A challenge is to ensure accuracy, relevance, and coherence of the information \\nthey produce. \\nA specific challenge is called hallucination. It happens when the llm doesn\\'t know something\\nand makes it up. For example it makes up a source, or claims that there exist a study that \\nproves a point. These hallucinations are difficult to spot quickly. \\nMany advances have been made in utilizing LLMs, many will follow.\\n\\nThis work aims to find out how good llm\\'s are at creating a state of \\nthe art work for a scientific topic. We started at thinking how humans do \\nthese kind of\\nresearch. There are three main tasks we concentrated our research. \\nFirst task is to\\nmake a summary out of one article. Second task is to choose relevant articles. \\nAnd at\\nthe end the third task is to combine these relevant articles and create an state of \\nthe art article.\\n\\n\\nRelated Work\\nIn this chapter we evaluate the state of the art of our topic. First we discuss challenges in LLM use \\nfor academic research. In relation to the three task we study related work in \\nsummarization of individual articles using LLMs, selection of relevant articles using LLMs\\nand combining articles to create a state-of-the-art review.\\nAt the end we consider the ways of evaluation and challenges.\\n\\n\\\\section{Challenges in LLM Use in scientifc context}\\nLarge Language Models (LLMs) like OpenAI\\'s GPT-4 have demonstrated impressive \\ncapabilities in generating human-like text, making them promising tools for various \\nnatural language processing (NLP) tasks, including academic research. \\n\\nHowever, \\ndespite these advancements, significant challenges persist when applying LLMs in \\nscientific context. When needing references some LLM\\'s are unreliable to deliver them.\\n\\\\cite{wu2024llmsciterelevantmedical} evaluated top-performing, commercially available LLMs \\n(GPT-4 (RAG and API), Claude v2.1, Mistral\\nMedium, and Gemini Pro) and find that models without access to the Web only produce \\nvalid URLs between 40\\\\% to 70\\\\% of the time. URL\\'s that would serve as references.\\n\\nWith Retrieval-augmented generation (RAG) a big step had been done. As \\n\\\\cite{shuster-etal-2021-retrieval-augmentation} showed that \\nRetrieval Augmentation reduces hallucination in conversation. Also \\n\\\\cite{wu2024llmsciterelevantmedical} came to the conclusion that with \\nRetrieval-augmented generation (RAG)-enabled GPT-4, which has search engine access, \\ndoes not suffer from URL hallucination.\\n\\nStill a lot of work has to be done as \\\\cite{wu2024llmsciterelevantmedical} showed\\nthat even with access to internet and RAG LLM\\'s still fail to produce references \\nthat support all the statements in the response nearly half of the time.\\n\\n\\n\\n\\\\section{Summarization of Individual Articles}\\nThe task of summarization was studied a lot and also a motivation to create large language \\nmodels.\\n\\nThe first automatic text summarization tools were created based on frequency statistics.\\n\\nThis work \\\\cite{fuzzy} from 2013 used Fuzzy Logic Text for Summarization by Extraction. They \\nproposed an approach for extractive summarization\\nprogram consists of 4 main steps. There are preprocessing of text, features extraction of both words\\nand sentences, sentence selection and assembly, and summary generation.\\n\\nIn year 2016 this research \\\\cite{singlesumm} aimed to produce an automatic text summarizer\\nimplemented with TF-IDF algorithm. They focused on creating summaries by extracting the most important\\nsentences out of the original texts. With simple techniques like TF-IDF they could reach 67\\\\% of \\naccuracy compared to which sentences humans considered as important.\\n\\n\\nCreating abstract summaries was a motivation to create LLMs.\\nWorking with scientific papers the work \\\\cite{cohan2018discourseawareattentionmodelabstractive} \\nrepresents an early approach summarizing long documents. \\nSummarization of scientific articles is complex because of the technical nature of the content.\\nHandling complex, long-form text which is vital for handling detailed scientific papers. \\n\\nMost LLM\\'s are not specifically for scientific context. \\nSummarizing scientific articles presents unique challenges, such as understanding technical \\nterminology and complex concepts accurately.\\nBut SciBERT \\\\cite{beltagy2019scibertpretrainedlanguagemodel} a version of BERT was pretrained \\nspecifically on scientific texts and is thus better \\nequipped to manage these challenges.  It allows SciBERT or similar models to \\ngenerate summaries that are more accurate and contextually relevant, which is critical when \\ndealing with field-specific jargon and nuanced scientific discourse.\\n\\n\\n\\\\section{Selection of Relevant Articles Using LLMs}\\n\\nSelecting relevant literature involves retrieving and ranking documents accurately. \\nFinding directly the relevant literature we usually don\\'t do as it gives hallucinations.\\nLLM\\'s can be used as a second step to select out of a preselection.\\n\\nThe GPT-4 Research Assistant (https://github.com/echohive42/GPT-4-Research-assistant/tree/main, \\n2024) offers an automated process for identifying and selecting relevant research papers from \\nArXiv. It uses GPT-4 to assess the most promising papers from search results, demonstrating \\na practical application of LLMs in content selection.\\n\\nHowever it is questionable how good LLMs are in selection of any kind because of the positional\\nbias. \\\\cite{liusie2024llmcomparativeassessmentzeroshot} used LLM\\'s as evaluator for comparative \\nassessment. There the LLM had to choose out of two texts which one was better. They showed that \\nthe first one had an advantage, ergo an positional bias.\\n\\nIn our work we look closely at if an LLM can select relevant content out of provided content. \\nWe focus here on the binary, relevant or not.\\n\\n\\nAnother way to select content is the frequently mentioned vector embedding (used in RAG).\\n\\\\cite{garcia2023sourcestalkevaluatinglarge} shows how LLMs with vector embeddings assist in \\nretrieving and evaluating relevant sources in a historical research context.\\n\\nIn our work we will use RAG to select the most relevant parts of papers in task 3.\\n\\n\\n\\\\section{Combining Articles to Create a State-of-the-Art Review}\\n\\nA key challenge in generating state-of-the-art reviews is synthesizing information from multiple \\nsources. The paper “Generating Wikipedia by Summarizing Long Sequences” \\n\\\\cite{liu2018generatingwikipediasummarizinglong} introduces a method for multi-document \\nsummarization, where the input is a collection of related documents, and the output is a \\nwell-structured Wikipedia article. \\nThis process involves an extractive step, where relevant paragraphs are extracted. \\nFollowed by an abstractive step using the paragraphs and a transformer model to create the \\nsummary. \\n\\nWe in our work did take a similar approach in their methodoloy. \\nThey did take a wikipedia article \\nand sources to it, in an supervised setting.\\nWhile \\\\cite{liu2018generatingwikipediasummarizinglong} employ a supervised learning approach by \\ntraining a model on curated Wikipedia inputs and summaries, our study differs in that we evaluate a \\npre-trained model without further training or fine-tuning.\\n\\nWe take an original paper and take its references. \\nWe can use newer LLM models and experiment with RAG. As well as test against more evaluation scores. \\n\\n\\n\\n\\\\section{Evaluation Metrics and Challenges}\\nA very important part of this work is the evaluation of created summaries or texts from \\nthe llm. The challenge here is the lack of consensus. A study about summarization evaluation \\nis done in the paper \\n\\\\cite{fabbri2021summevalreevaluatingsummarizationevaluation} the authors evaluate\\nseveral evaluation metrics. \\\\cite{fabbri2021summevalreevaluatingsummarizationevaluation} also \\nshowed that many evaluation metrics don\\'t correlate with each other or even correlate negatively \\nwith each other. \\n\\nThere are different approaches on how to evaluate a summary. One way is to compare the llm\\ngenerated summary with a reference summary. Then the ability of summarization is tested \\nwith help of similarity. Similarity can be tested by humans or via a variety of metrics. \\n\\nOne of the oldest metrics is ROUGE \\\\cite{lin-2004-rouge}. Despite many critic it has remained \\nthe default automatic evaluation metric. \\nFor this reason we looked at ROUGE as well. So, we could compare to other results of other studies.\\n\\nAnother popular score is BertScore \\\\cite{zhang2020bertscoreevaluatingtextgeneration}, which we used\\nas well.\\n\\nMost recently there are many studies, if LLM\\'s can evaluate generated text. G-Eval \\n\\\\cite{liu2023gevalnlgevaluationusing} is a framework of using large language models with \\nchain-of-thoughts (CoT) and a form-filling paradigm, to assess the\\nquality of NLG outputs. They showed that it has a higher correlation with humans. But also \\nhighlight the potential concern of LLM-based evaluators having a bias towards\\nthe LLM-generated texts.\\n\\n\\nThere are even more creative ways to evaluate summaries. QAGS: This evaluation method uses \\nquestion-answering to measure factual consistency in summaries \\n\\\\cite{wang2020askingansweringquestionsevaluate}. It generates questions from the summary and checks if the answers are \\npresent in the source document.\\n\\nIn our work we use different evaluation metrics, also questioning them. At this process we never \\nforget the human evaluation.\\n\\nMethod\\nIn this chapter we discuss the methods used for the three tasks outlined in the introduction. \\nWe describe the dataset and pipline for each task. In figure \\\\ref{fig:tasks_overview} you \\ncan see an illustration of each of the three tasks. We\\'ll also cover the technologies and \\ntools that were essential for running the experiments.\\n\\n\\\\begin{figure}\\n  \\\\centering\\n  \\\\scalebox{0.5}{\\\\includegraphics{images/overview_all_tasks.png}}\\n  \\\\caption{Overview over the three tasks}\\n  \\\\label{fig:tasks_overview}\\n\\\\end{figure}\\n\\n\\\\section{Task 1: Summarization}\\nThis task consists of evaluating how effectively a LLM can create a summary of a \\nscientific text. LLMs are known to excel at summarizing texts. \\nHowever, a significant challenge is how to evaluate LLM-generated summaries.\\n\\nHow can we assign a score to a summary? An effective summary should contain the most \\nimportant information from the original text while remaining concise. \\nIt should accurately present the facts of the text, avoiding misinterpretation or \\nfabrication. Moreover, even human evaluators can have differing opinions regarding \\nthe quality of a summary, which emphasizes the need for an automated evaluation metric.\\n\\nIn this study, we used a ground truth summary created by the author of the text to \\ncompare with the LLM-generated summary. However, comparing these summaries remains \\ndifficult. One approach is to examine whether the same words occur in both summaries. \\nYet, identical words can have different meanings depending on their context. \\nAdditionally, we can express the same meaning using different words.\\n\\nTherefore, how can we calculate similarity in a text based on its meaning? \\nHow can LLMs assist in the evaluation process?\\n\\n\\\\subsection{Dataset}\\nThe dataset consists of scientific papers from the repository ArXiv.\\nArXiv is a free, open-access repository for scholarly papers, \\nparticularly in the fields of physics, mathematics, computer science, \\nquantitative biology, quantitative finance, statistics, \\nelectrical engineering and systems science, and economics.\\n\\nAs mentioned in chapter \\\\ref{cha:related}\\n\\\\cite{cohan2018discourseawareattentionmodelabstractive} created a dataset with ArXiv papers.\\nWe used part of the dataset provided by them. \\nIt can be downloaded with tensorflow as scientific\\\\_papers.\\nEach paper has an id, abstract\\\\_text and article\\\\_text.\\nThe id coresponds to the id given by ArXiv. The abstract is the abstract of\\nthe paper, every scientific paper has it. Its purpose is to give an overview\\nover the paper and summarize the key findings, hence it is a perfect example of a \\nsummary by the author. The article\\\\_text is the full text of the paper without the abstract.\\nThey removed figures and tables using regular expressions to only preserve the textual \\ninformation. The dataset has 215\\'913 articles.\\n\\n\\n\\n\\\\subsection{Summary generation and evaluation}\\n\\nEach paper has the abstract and the text itself without the abstract. First they \\nwere separated.\\nThe text is given to the LLM to generate summaries. A very simple prompt and a more \\ncomplex prompt were used. At the end the summaries were evaluate by comparing.\\nThe BERTScore, rougescore and a self evaluating score of the LLM was used to score \\nall of the summaries. \\nTo get a score it needs a reference and a candidate as showed in figure \\n\\\\ref{fig:task1_compare_overview}\\nThe abstracts are used as the ground truth summaries for scoring the generated summaries. \\nHowever, it is also possible to directly compare the generated summaries to the full \\npaper. To establish a baseline for interpreting these scores, we also compared the \\nabstracts to the original paper.\\n\\n\\\\begin{figure}[!h]\\n  \\\\centering\\n  % Include the graphic (replace \\'example-image\\' with your image file name)\\n  \\\\includegraphics[width=0.5\\\\textwidth]{images/Compare.png}\\n  % Add a caption\\n  \\\\caption{We can apply the Bert or ROUGE score different ways. Compare the abstract (blue) to the created summary (yellow) or compare them resp. to the entire text (green).}\\n\\n  \\\\label{fig:task1_compare_overview}\\n\\\\end{figure}\\n\\n\\n\\n\\n\\n\\\\section{Task 2: Selection}\\nHow efficient is an LLM in selecting relevant papers? The idea is we would give the LLM \\na topic and a list \\nof papers and it should decide which of the papers are relevant. A big challenge was to \\ndecide which data \\nuse. There has to be a ground truth of which papers are related.\\n\\\\subsection{Dataset creation}\\nWe created our own dataset from papers from ArXiv. The idea is to have a paper to start with and get its\\nreferences as related papers. We chose 10 papers as original papers. They were chosen with no particular \\nreasoning. But they have to be some survey or overview over a particular topic. \\nAlso the paper are rather recent and \\nin the field of computer science. The following papers have been chosen.\\\\\\\\\\n\\n\\n\\\\begin{itemize}\\n    \\\\item 2402.01383: LLM-based NLG Evaluation: Current Status and Challenges \\\\cite{gao2024llmbasednlgevaluationcurrent}\\n    \\\\item 2402.06196: Large Language Models: A Survey \\\\cite{minaee2024largelanguagemodelssurvey}\\n    \\\\item 2408.02304: Embedding Compression in Recommender Systems: A Survey \\\\cite{Li_2024}\\n    \\\\item 2408.02464: Fairness and Bias Mitigation in Computer Vision: A Survey \\\\cite{dehdashtian2024fairnessbiasmitigationcomputer}\\n    \\\\item 2408.02085: Unleashing the Power of Data Tsunami: A Comprehensive Survey on\\n    Data Assessment and Selection for Instruction Tuning of Language Models \\\\cite{qin2024unleashingpowerdatatsunami}\\n    \\\\item 2311.13731: A Survey of Blockchain, Artificial Intelligence, and\\n    Edge Computing for Web 3.0 \\\\cite{zhu2023surveyblockchainartificialintelligence} \\n    \\\\item 2311.12785: Prompting Frameworks for Large Language Models: A Survey \\n    \\\\cite{liu2023promptingframeworkslargelanguage}\\n    \\\\item 2409.15816: Diffusion Models for Intelligent Transportation Systems: A Survey\\n    \\\\cite{peng2024diffusionmodelsintelligenttransportation}\\n    \\\\item 2409.15180: A Comprehensive Survey with Critical Analysis for Deepfake Speech Detection\\n    \\\\cite{pham2024comprehensivesurveycriticalanalysis}\\n    \\\\item 2409.09957: Deep Graph Anomaly Detection: A Survey and New Perspectives\\n    \\\\cite{qiao2024deepgraphanomalydetection}\\n  \\\\end{itemize}\\n\\nFrom each of the papers the references have been extracted. Because of simplicity, only reference papers \\nwith their ArXiv id were considered. This reduced the number of references. We collected their id, \\npublishing date, abstract and full text. \\n\\nWe created smaller standart sized dataset for task 2. We chose 10 papers out of the \\nreference papers then we added 10 unrelated papers. In finding unrelated papers \\nseveral things were to be considered. The unrelated has to be published earlier \\nthan the original paper. This simulates the research context, at the time of writing \\nthe original paper, the authors had access only to existing literature.\\nThe task also shouldn\\'t be too easy, papers from a completely different field \\nmight be very simple to spot.\\nHence we chose took papers related to computer science.\\nAnother concern was if we take always the same unrelated papers, we would show that \\nthe LLM can detect that these papers are unrelated, rather than the objective that it \\ncan detect related papers. So we increased the number of papers by 50 and chose \\nrandomly 10 out of it for each dataset.\\n\\n\\n\\\\subsection{Selecting articles}\\nThere are the same numbers of related and unrelated articles. \\nWe ask the LLM which of these are related \\nto a topic. The topic consists of the original title of the paper follwed by its abstract. \\nIt should answer with the \\ncorresponding arxiv ids, these were extracted of the LLM\\'s answer. \\nThen we calculated the precision, recall and F1 value.\\n\\n\\nInstead of giving the list of related and unrelated articles we gave them one by one.\\nIt is interesting to test how good the quality is if the topic is defined as the title \\nand the abstract or only the title.\\n\\n\\\\section{Task 3: Combining to SotA}\\nIn Task 2, we learned methods to get relevant sources to a topic.\\nNow in Task 3 the idea is to combine these sources to a coherent text. \\nWe can look at it \\nas a summary of multiple sources or as a text towards a state of the \\nart paper of the topic.\\n\\n\\\\subsection{Dataset}\\nSince the work to create the database in task 2 was quite a lot. We tried to reuse part of it here.\\nWe took the first paper \\\\cite{gao2024llmbasednlgevaluationcurrent}. We took its title as the topic.\\nIts sources which we collected previously are the relevant sources to this topic.\\n\\nWhen we want to use the concept of RAG, we need an additional step where we create a vector database.\\nFirst all reference papers have to be loaded from the pdfs, they are called documents. \\nThen each document has to be splitted in even sized chunks of text. \\nThe lenght is a parameter we can choose.\\n\\nTo create the database each of these text parts have to be embedded with a sentence \\ntransformer. Each \\npart has also metadata, for example the source. The source is the name of the document it \\ncames from,\\nthis is very useful as it can serve as a reference. An example of how these sources are given\\nis in the Result section with the example \\\\ref{RAG example}. \\n\\n\\\\subsection{Creation of texts}\\nAs always with LLM\\'s we need a prompt to generate a text. Here the idea can be to summaries \\nthe several texts or it can be to create a state of the art based on some texts.\\nWe used two different prompts with two ideas. \\\\\\\\\\n\\\\begin{itemize}\\n  \\\\item Combiniation of the sources: The prompt asked the LLM to summarize the key findings from all \\n  relevant texts. \\n  \\\\item State-of-the-Art Overview: The LLM was asked to describe recent developments in the topic area, \\n  using the title of the original paper as the query topic and only using the context.\\n\\\\end{itemize}\\n\\nExact prompt is in the \\nexperiment chapter.\\\\\\\\\\n\\nThe texts or the context can be the entire papers, or some parts of it. We can for example \\nonly take the abstracts or we can with RAG choose parts of the text wich are more relevant.\\n\\n\\nWe expeimented with what parts of the source papers should be given as context. \\n\\\\begin{itemize}\\n  \\\\item Full Papers: All content from the source documents was used as input.\\n  \\\\item Abstracts Only: A \"summary of summaries\" approach.\\n  \\\\item RAG Retrieval: Chunks selected dynamically by the RAG retriever based \\n  on relevance to the prompt.\\n\\\\end{itemize}\\n\\n\\nThe idea of the RAG Retrieval is to select only the relevant parts of the papers \\nbased on the prompt. Now is time to use the retriever. \\nIt gets the best fitting chunks of text from the\\ndatabase based on the prompt. For this the retriever needs the same embedding function as \\nwas used to create the database. It calculates the embedding of the prompt. \\nThe prompt here was the question about new developments in a certain topic.\\nThis resulting embedding vector gets compared to the embedding vectors in the database.\\nThe retriever uses cosine similarity to retrieve the top k texts.\\nHere we can choose of how many we want to use. The idea is that because they have a similar \\nembedding they talk about the same topic and can so be used to answer the prompt.\\nThese texts are then combined\\nwith the prompt to a system prompt and given to the LLM.\\n\\n\\n\\\\subsection{Evaluation}\\nThe evaluation is as always a difficult part. We have to read the generated texts to judge if they \\nmake sense. We looked at how many of the papers did the LLM use. Another criteria is does it list \\nsources and are these correct or hallucinations. Finally, we compared it to the original paper with \\nthe BERTScore.\\n\\n\\n\\n\\\\section{Technologies Used}\\nWhenever in this work we talk about the LLM, we used the \\n\\\\textbf{mistral model} version v0.2. With its details in \\n\\\\cite{jiang2023mistral7b}\\nThe Mistral model is a state-of-the-art large language model (LLM) designed for \\nnatural language processing tasks, known for its efficiency and high performance in \\ngenerating coherent and contextually relevant text. \\nIt is part of the family of transformer-based models, which are widely used for \\ntasks such as text generation, summarization, and translation. The mistral is \\na decoder only model, as described in chapter \\\\ref{cha:fondamentals}.\\nAdditionally this model\\nleverages grouped-query attention (GQA) for faster inference, \\ncoupled with sliding window attention (SWA) to effectively handle \\nsequences of arbitrary length with a\\nreduced inference cost \\\\cite{jiang2023mistral7b}.\\nVersion 2 could replace the sliding window attention to enlarge the context length to \\n32768 tokens. %https://ollama.com/library/mistral/blobs/ff82381e2bea\\n%\\nMistral-7B models use byte-pair encoding (BPE) tokenization, \\nwhich is widely used in large language models to balance efficiency and accuracy \\nin representing text. \\n\\nWe chose mistral 7B because it is small enough to use on a local machine, \\nbut still \\nhas a good performance. For example it outperforms bigger models as Llama 2 13B.\\n\\nWhile Mistral is pre-trained on a diverse corpus of texts, for this study, \\nwe used it as it is, relying on its pre-trained knowledge.\\n\\nIn LLM\\'s the parameter temperature can be chosen. It controls the randomness or \\ncreativity of the model\\'s output and influences how the model selects words during \\ntext generation when predicting the next word in a sequence.\\nTypically the value is between 0 and 1. Here the temperature was consistently set \\nto zero. This means that always the word with the highest probability gets chosen.\\nThis ensures that the experiments can be consistently replicated.\\n\\nWe downloaded the model from Ollama website.\\nThe LLM was accessed through an jupyter notebook with the library Ollama from \\nlangchain.\\n\\n\\nTo get the ArXiv papers the python package arxiv was used.\\n\\n\\nExperiments \\n\\\\section{Task 1 Summarization}\\n\\\\subsection{Discussion}\\nThe primary aim of this experiment was to evaluate the effectiveness of large language models (LLMs) \\nin generating accurate, concise, and coherent summaries of scientific articles. \\nSpecifically, we tested the impact of simple versus complex prompts on the quality of the summaries, \\nevaluated using BERT, ROUGE, and self-evaluation scores.\\n\\n\\\\subsubsection{Rouge Scores}\\nThe Rouge score indicate the lexical overlap of the summaries. As already mentioned, it is a poor\\nindicator of the quality of a summary, but still can be informative. We see that its values are low,\\naround 0.1 and 0.2. The simple and the complex prompt are very similar.\\n\\nLet\\'s see the precision and recall relationship. In table \\\\ref{tab:mean_results} we can see the \\nrecall is higher than the precision. The reason can be that the reference here is shorter than the \\ncandidate. Table \\\\ref{tab:summary_lengths} shows that the abstract is shorter than the generated summaries. \\nHowever it is the other way round when we compared the summary to the entire text as in table \\n\\\\ref{tab:combined_results}, there the precision is higher than the recall, which is very low. Here the \\nreference text is longer than the candidate. \\n\\nSince we used the same data as \\\\cite{cohan2018discourseawareattentionmodelabstractive} and performed \\nthe same task of summarization, it is interesting to compare with their results. They used different \\nRouge metrics, among them also Rouge L, which we used. \\nThey created their own LLM model which got 0.3180 Rouge L score. Their much higher score can be \\nexplained as their model was build specifically for this task and trained to maximize the Rouge score. \\nWhile we used the mistral model which was trained for a variety of tasks. \\n\\nWe see in table \\\\ref{tab:combined_results} that the generated summaries have a higher F1 value than the \\nabstracts. This means they have a higher lexical overlap. Or in other words the generated summaries \\nare more extractive, more phrases from the text were taken to formulate the summary, instead of rewriting.\\nWe can also see that the lenght of the texts influence the scores quite much. \\n\\n\\\\subsubsection{Bert score}\\nThe BERT score provides insight into the semantic similarity between the candidate and the \\nreference text. Here, we observe that the summaries generated with the simple prompt slightly \\noutperform those from the complex prompt in terms of BERT score.\\n\\nInterestingly, in the comparative experiment, the abstract has a higher BERT score than \\nthe generated summaries. This suggests that human-generated abstracts capture the content of \\nthe full text more effectively than the LLM-generated summaries. However, it is important to \\nnote that conclusions drawn from a single metric can be limiting, as BERT focuses on semantic \\nsimilarity alone.\\n\\n\\\\subsubsection{selfevalaution}\\n\\nThe self-evaluation scores are the highest among all metrics. \\nThis might be because the LLM self-evaluation does not only assess similarity \\n(as ROUGE and BERT do), but likely considers other aspects of the summary. \\nUnfortunately, the exact criteria it evaluates are not transparent to the user.\\n\\n\\nThe range of self-evaluation scores (from 0 to 1) is particularly helpful for comparing summaries. \\nUnlike traditional metrics, self-evaluation seems to capture a broader range of quality indicators. \\nFor instance, summaries with low self-evaluation scores were often those where the LLM \\nmisinterpreted the paper\\'s content, such as mistaking it for a list of references, \\nor failing in another way to generate an appropriate summary. \\nThis shows that the self-evaluation scores can reflect meaningful distinctions that other \\nmetrics might overlook.\\n\\n%\\\\pagebreak\\n\\n\\\\section{Task 2: Selection}\\nIn this section, we describe the experimental setup, results and discussion \\nfor Task 2, where we answer the question: \\nHow effective is an LLM in selecting relevant papers?\\n\\n\\n\\\\subsection{Discussion}\\n%However it is questionable how good LLMs are in selection of any kind because of the positional\\n%bias. \\\\cite{liusie2024llmcomparativeassessmentzeroshot} used LLM\\'s as evaluator for comparative \\n%assessment. There the LLM had to choose out of two texts which one was better. They showed that \\n%the first one had an advantage, ergo an positional bias.\\n\\nThe experiment giving the LLM a list clearly failed. The reason behind is not completely clear. \\nOne could think that some related papers were harder to find. But the correctly detected papers were \\ndifferent ones at each time. When we combine all the 10 runs all 10 related papers were in the answers.\\nThis hints that the structure of the question is the problem.\\nIt could be that the LLM didn\\'t \"understand\" which content belonged to which paperid. \\nAnother reason could be it has positional bias as \\\\cite{liusie2024llmcomparativeassessmentzeroshot} \\ndetected in their research. As mentioned before, they used LLM\\'s as evaluator for comparative \\nassessment. There the LLM had to choose out of two texts which one was better. They showed that \\nthe first one had an advantage, ergo an positional bias. Here it could be similar, that some position \\nin the list of papers are more likely to get picked as relevant papers than others, regardless of \\nthe papers content.\\\\\\\\\\n\\nNow look at the second experiment where we asked the LLM one by one. \\nThese results are very successful,\\nmany got a perfect score. It is interesting to look deeper into one example. What is the reason for \\nthe not perfect scores? Importantly we have to ask ourself if the ground truth is correct.\\n\\nLet\\'s take one of the original papers 2311.13731 \\n\"A Survey of Blockchain, Artificial Intelligence, and Edge Computing for Web 3.0\". It It scored $0.875$ \\nfor precision and $0.7$ in recall as table \\\\ref{tab:metrics} informs. The LLM considered 8 papers \\nrelevant out of them there are 7 labeled related. The name of the false positive is \"Crypto Makes AI Evolve\"\\nOne can argue and also the LLM argues that this paper is relevant to the topic, as it \\n\"it discusses the intersection of artificial intelligence and cryptography\". However the ground truth \\ntells us this paper is not relevant as it wasn\\'t listed by the authors of the the original paper. \\nAnd of course there exist papers that are relevant but aren\\'t referenced in the original paper.\\nOne could argue that the paper \"Crypto Makes AI Evolve\" shouldn\\'t have been in the selection list. \\n\\nNow the LLM picked 7 out of the 10 relevant papers. For example the paper \"A Comprehensive Survey \\nof AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT\" was labeled as \\nrelated. But as the LLM commented is this paper not directly related. It is related as it talks about \\nAI, but it doesn\\'t have a connection with blockchain or the Web 3.0 as the original paper. \\nThe other two false negatives are similar. One could argue that they are related or not. \\nBut of course a paper also references other papers that don\\'t relate to them in every aspect.\\nShouldn\\'t this paper have been labeled as relevant? One could argue either way. \\nAlso the the prompt could be modified to include more papers as relevant.\\n\\nLooking at this example, the results of table \\\\ref{tab:metrics} may not refelect the ability of an LLM \\nto select relevant papers but the shortcommings of our dataset. Regardless, the LLM has shown strong \\ncapabilities to select related content.\\\\\\\\\\n\\nOne question is if an LLM should be used to select relevant papers. There exist many other ways to get \\nrelated sources to a topic. Taking the search in the arxiv database. With some keywords you get \\nrelated papers. If for example you give the title \"LLM-based\\nNLG Evaluation: Current Status and Challenges\", it gives successfully relevant results. If you ask the LLM if these \\npapers are relevant, it does agree. We did another experiment. We collected the first 100 \\npapers from ArXiv which are related to LLM-based NLG Evaluation: Current Status and Challenges”. In \\n100 there are more likely some papers that are indeed not related. We asked the LLM for each paper \\nif it is relevant or not. The result was that the LLM rejected 6 out of these 100 papers, that they \\nare not relevant. It did give an explanation why not, for example: \"No, this paper is not relevant \\nto LLM-based NLG Evaluation: Current Status and Challenges as it focuses on Nonlocal Gravity (NLG) \\nin the context of classical physics, specifically gravitation, rather than Natural Language \\nGeneration (NLG) evaluation.\" Here we see that the LLM could successfully detect a mismatched \\nabbreviation. In the search \"NLG\" responded to \"nonlocal gravity\". But the LLM \"understood\" that these is \\nnot the same thing. This small experiment showes that LLM\\'s have can judge relevance on a deeper level \\nof context, than for example keyword spotting. \\n\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f'dataset/discussions.txt', 'r') as file:\n",
    "        mydiscussions = file.read()\n",
    "mydiscussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8552\n"
     ]
    }
   ],
   "source": [
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=mydiscussions)])\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In this section, we discuss the results and findings of Task 2, which focused on evaluating the effectiveness of a Language Model (LLM) in selecting relevant papers based on given criteria. The experiment involved two different approaches: providing the LLM with a list of papers to select from and asking the LLM to select papers one by one.  The first approach, where the LLM was given a list of papers, resulted in mixed outcomes. While all 10 related papers were eventually identified, there were some challenges that could not be definitively attributed to the LLM\\'s understanding or positional bias. The structure of the question might have been the primary issue, as it could have affected the LLM\\'s ability to associate the correct content with each paper ID.  In contrast, the second approach, where the LLM was asked to select papers one by one, yielded very successful results, with many achieving a perfect score. However, there were instances where the ground truth of the relevance of certain papers was questioned. For example, in the case of paper 2311.13731, \"A Survey of Blockchain, Artificial Intelligence, and Edge Computing for Web 3.0,\" the LLM considered a paper titled \"Crypto Makes AI Evolve\" as relevant, despite it not being listed among the related papers in the original paper.  The authors argue that this could be due to the limitations of their dataset rather than the LLM\\'s ability to select relevant papers. They also suggest that an LLM might not be the best choice for selecting relevant papers, as there are other ways to obtain related sources, such as searching in databases like ArXiv using keywords.  In a small experiment, they collected the first 100 papers from ArXiv related to \"LLM-based NLG Evaluation: Current Status and Challenges\" and asked the LLM to evaluate each paper\\'s relevance. The LLM correctly identified and rejected six out of these 100 papers as not relevant, demonstrating its ability to judge relevance based on a deeper understanding of context rather than just keyword spotting.  In conclusion, while there are challenges in using an LLM for selecting relevant papers, the results suggest that it can effectively identify related content when given the opportunity to engage with each paper individually. However, the limitations of the dataset and the availability of alternative methods for obtaining related sources should be considered.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mysummary2 = ask_for_summary(mydiscussions)\n",
    "mysummary2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
