{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from data import load_papers \n",
    "import numpy as np\n",
    "from bert_score import BERTScorer\n",
    "from rouge_score import rouge_scorer\n",
    "from langchain_community.llms import Ollama\n",
    "import re\n",
    "separator = \"\\n---SEPARATOR---\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts, texts = load_papers(split='test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summaries(filename):\n",
    "    with open(f'summaries/'+filename, 'r') as file:\n",
    "            summaries = file.read()\n",
    "    summaries_list = []\n",
    "    for i in summaries.split(separator):\n",
    "        summaries_list.append(i)\n",
    "    return summaries_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the summaries created in create_summaries.ipynb\n",
    "#Choices of filenames are following for the simple or the complex prompt and 10 or 100 summaries.\n",
    "#filename =\"write_summary_complex.txt\"\n",
    "#filename = \"write_summary_complex100.txt\"\n",
    "filename = \"write_summary100.txt\"\n",
    "#filename =\"write_summary.txt\"\n",
    "summaries_list = get_summaries(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258\n"
     ]
    }
   ],
   "source": [
    "# Example texts\n",
    "reference = \"This is a reference text example.\"\n",
    "candidate = \"This is a candidate text example.\"\n",
    "# BERTScore calculation\n",
    "scorer_bert = BERTScorer(model_type='bert-base-uncased')\n",
    "P, R, F1 = scorer_bert.score([candidate], [reference])\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "\n",
    "### Outputs : BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "#Get all bert scores\n",
    "def get_bert(candidate, reference,filename):\n",
    "    scores = []\n",
    "    if len(candidate)<100:\n",
    "        num = len(candidate)\n",
    "    else:\n",
    "        num = 100\n",
    "    for i in range(num):\n",
    "        P, R, F1 = scorer_bert.score([candidate[i]], [reference[i]])\n",
    "        scores.append([round(float(P[0]),4),round(float(R[0]),4),round(float(F1[0]),4)])\n",
    "        if i%10==0:\n",
    "            print(i)\n",
    "    m = np.mean(scores,axis=0)\n",
    "    s = np.std(scores,axis=0)\n",
    "    min = np.min(scores,axis=0)\n",
    "    max = np.max(scores,axis=0)\n",
    "    scores.append(m)\n",
    "    scores.append(s) \n",
    "    scores.append(min)\n",
    "    scores.append(max)   \n",
    "    np.savetxt('results/'+filename+'_bert.txt',np.matrix(scores),fmt='%.4f')\n",
    "    return np.matrix(scores)\n",
    "scores = get_bert(summaries_list,abstracts,filename)\n",
    "#print(scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "[[0.6494     0.5555     0.5988    ]\n",
      " [0.4338     0.4683     0.4504    ]\n",
      " [0.6656     0.6145     0.639     ]\n",
      " [0.6019     0.6115     0.6066    ]\n",
      " [0.6415     0.6309     0.6362    ]\n",
      " [0.6254     0.5556     0.5885    ]\n",
      " [0.6115     0.5923     0.6017    ]\n",
      " [0.504      0.4649     0.4837    ]\n",
      " [0.5786     0.4993     0.536     ]\n",
      " [0.6253     0.6571     0.6408    ]\n",
      " [0.6154     0.5374     0.5737    ]\n",
      " [0.6649     0.609      0.6358    ]\n",
      " [0.6218     0.562      0.5904    ]\n",
      " [0.5457     0.5247     0.535     ]\n",
      " [0.7395     0.6829     0.7101    ]\n",
      " [0.5018     0.4989     0.5004    ]\n",
      " [0.6507     0.6401     0.6454    ]\n",
      " [0.5334     0.4612     0.4947    ]\n",
      " [0.5399     0.5358     0.5379    ]\n",
      " [0.6065     0.531      0.5662    ]\n",
      " [0.6239     0.577      0.5995    ]\n",
      " [0.636      0.5863     0.6101    ]\n",
      " [0.565      0.5034     0.5324    ]\n",
      " [0.5544     0.5626     0.5585    ]\n",
      " [0.574      0.6022     0.5878    ]\n",
      " [0.5548     0.544      0.5494    ]\n",
      " [0.6035     0.5517     0.5764    ]\n",
      " [0.5877     0.6157     0.6014    ]\n",
      " [0.6222     0.5484     0.583     ]\n",
      " [0.5635     0.5312     0.5469    ]\n",
      " [0.6065     0.5559     0.5801    ]\n",
      " [0.6102     0.5017     0.5507    ]\n",
      " [0.477      0.4746     0.4758    ]\n",
      " [0.6291     0.5514     0.5877    ]\n",
      " [0.461      0.4251     0.4423    ]\n",
      " [0.6587     0.5495     0.5992    ]\n",
      " [0.6937     0.6432     0.6675    ]\n",
      " [0.647      0.6195     0.633     ]\n",
      " [0.5432     0.572      0.5572    ]\n",
      " [0.6811     0.6311     0.6551    ]\n",
      " [0.6879     0.6615     0.6744    ]\n",
      " [0.425      0.426      0.4255    ]\n",
      " [0.6004     0.5631     0.5812    ]\n",
      " [0.6305     0.5261     0.5736    ]\n",
      " [0.6353     0.5528     0.5912    ]\n",
      " [0.6249     0.621      0.623     ]\n",
      " [0.6452     0.5867     0.6146    ]\n",
      " [0.572      0.5617     0.5668    ]\n",
      " [0.5991     0.5577     0.5776    ]\n",
      " [0.6194     0.547      0.581     ]\n",
      " [0.4589     0.4851     0.4716    ]\n",
      " [0.7329     0.736      0.7344    ]\n",
      " [0.5947     0.5303     0.5607    ]\n",
      " [0.593      0.5225     0.5555    ]\n",
      " [0.6087     0.5304     0.5668    ]\n",
      " [0.5744     0.497      0.5329    ]\n",
      " [0.5745     0.5331     0.553     ]\n",
      " [0.6178     0.5508     0.5824    ]\n",
      " [0.5952     0.549      0.5712    ]\n",
      " [0.5679     0.5817     0.5747    ]\n",
      " [0.6424     0.5977     0.6192    ]\n",
      " [0.582      0.5174     0.5478    ]\n",
      " [0.4968     0.4417     0.4676    ]\n",
      " [0.5452     0.5501     0.5477    ]\n",
      " [0.5897     0.5497     0.569     ]\n",
      " [0.5722     0.5335     0.5522    ]\n",
      " [0.6042     0.5107     0.5535    ]\n",
      " [0.5811     0.5075     0.5418    ]\n",
      " [0.4574     0.4238     0.44      ]\n",
      " [0.5988     0.5708     0.5844    ]\n",
      " [0.6109     0.6137     0.6123    ]\n",
      " [0.4369     0.4397     0.4383    ]\n",
      " [0.5435     0.417      0.4719    ]\n",
      " [0.565      0.4737     0.5153    ]\n",
      " [0.5672     0.4964     0.5294    ]\n",
      " [0.6225     0.5712     0.5958    ]\n",
      " [0.6122     0.6122     0.6122    ]\n",
      " [0.5838     0.5212     0.5507    ]\n",
      " [0.592      0.5179     0.5525    ]\n",
      " [0.6497     0.5396     0.5896    ]\n",
      " [0.5877     0.6007     0.5942    ]\n",
      " [0.6285     0.6291     0.6288    ]\n",
      " [0.5931     0.5346     0.5624    ]\n",
      " [0.5798     0.5318     0.5548    ]\n",
      " [0.4926     0.4758     0.4841    ]\n",
      " [0.6151     0.5582     0.5853    ]\n",
      " [0.5758     0.5278     0.5508    ]\n",
      " [0.628      0.5986     0.6129    ]\n",
      " [0.4607     0.4456     0.453     ]\n",
      " [0.6069     0.5181     0.559     ]\n",
      " [0.6016     0.5691     0.5849    ]\n",
      " [0.5763     0.5111     0.5418    ]\n",
      " [0.5851     0.5314     0.5569    ]\n",
      " [0.5446     0.5268     0.5355    ]\n",
      " [0.5971     0.561      0.5785    ]\n",
      " [0.5995     0.6098     0.6046    ]\n",
      " [0.7539     0.7608     0.7573    ]\n",
      " [0.6067     0.5159     0.5576    ]\n",
      " [0.614      0.5655     0.5887    ]\n",
      " [0.5812     0.5375     0.5585    ]\n",
      " [0.590885   0.550136   0.569182  ]\n",
      " [0.061319   0.06223598 0.05919525]\n",
      " [0.425      0.417      0.4255    ]\n",
      " [0.7539     0.7608     0.7573    ]]\n"
     ]
    }
   ],
   "source": [
    "scores2 = get_bert(summaries_list,texts,filename+'compare_to_text')\n",
    "print(scores2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "[[0.7037     0.6132     0.6553    ]\n",
      " [0.7381     0.5363     0.6212    ]\n",
      " [0.7686     0.5992     0.6734    ]\n",
      " [0.6994     0.5158     0.5937    ]\n",
      " [0.5539     0.5665     0.5601    ]\n",
      " [0.6456     0.6099     0.6272    ]\n",
      " [0.6763     0.6022     0.6371    ]\n",
      " [0.6565     0.5413     0.5934    ]\n",
      " [0.7078     0.6556     0.6807    ]\n",
      " [0.7343     0.5645     0.6383    ]\n",
      " [0.6469     0.542      0.5899    ]\n",
      " [0.6396     0.5855     0.6113    ]\n",
      " [0.6478     0.5826     0.6135    ]\n",
      " [0.7837     0.6727     0.724     ]\n",
      " [0.5982     0.5237     0.5585    ]\n",
      " [0.6659     0.5695     0.614     ]\n",
      " [0.7474     0.6165     0.6757    ]\n",
      " [0.5989     0.4867     0.537     ]\n",
      " [0.6257     0.6088     0.6171    ]\n",
      " [0.5646     0.4808     0.5194    ]\n",
      " [0.5387     0.535      0.5368    ]\n",
      " [0.6935     0.5223     0.5958    ]\n",
      " [0.6102     0.5893     0.5996    ]\n",
      " [0.5654     0.4939     0.5272    ]\n",
      " [0.6054     0.5783     0.5915    ]\n",
      " [0.6193     0.5101     0.5594    ]\n",
      " [0.6605     0.5901     0.6233    ]\n",
      " [0.6811     0.587      0.6305    ]\n",
      " [0.6634     0.597      0.6284    ]\n",
      " [0.6771     0.5966     0.6343    ]\n",
      " [0.6343     0.5478     0.5879    ]\n",
      " [0.5312     0.5677     0.5488    ]\n",
      " [0.6162     0.6166     0.6164    ]\n",
      " [0.7079     0.5092     0.5923    ]\n",
      " [0.6741     0.5792     0.6231    ]\n",
      " [0.7148     0.6049     0.6553    ]\n",
      " [0.6704     0.4835     0.5618    ]\n",
      " [0.7416     0.5932     0.6592    ]\n",
      " [0.6897     0.5984     0.6408    ]\n",
      " [0.656      0.5965     0.6248    ]\n",
      " [0.7289     0.5747     0.6427    ]\n",
      " [0.6301     0.5895     0.6091    ]\n",
      " [0.6845     0.5037     0.5804    ]\n",
      " [0.7001     0.5955     0.6436    ]\n",
      " [0.6631     0.6        0.63      ]\n",
      " [0.6572     0.5952     0.6247    ]\n",
      " [0.7264     0.6019     0.6583    ]\n",
      " [0.6311     0.5826     0.6059    ]\n",
      " [0.6576     0.5223     0.5822    ]\n",
      " [0.6321     0.491      0.5527    ]\n",
      " [0.6579     0.5747     0.6135    ]\n",
      " [0.6615     0.5551     0.6036    ]\n",
      " [0.642      0.6027     0.6217    ]\n",
      " [0.6378     0.5809     0.608     ]\n",
      " [0.6343     0.5166     0.5695    ]\n",
      " [0.6334     0.4646     0.536     ]\n",
      " [0.7103     0.6081     0.6553    ]\n",
      " [0.6549     0.6044     0.6286    ]\n",
      " [0.6319     0.546      0.5858    ]\n",
      " [0.6291     0.5778     0.6024    ]\n",
      " [0.7074     0.5761     0.635     ]\n",
      " [0.6668     0.5961     0.6294    ]\n",
      " [0.6523     0.6685     0.6603    ]\n",
      " [0.6591     0.5845     0.6196    ]\n",
      " [0.6403     0.5643     0.5999    ]\n",
      " [0.5062     0.4668     0.4857    ]\n",
      " [0.611      0.5229     0.5635    ]\n",
      " [0.6632     0.5117     0.5777    ]\n",
      " [0.6347     0.5919     0.6126    ]\n",
      " [0.6581     0.5037     0.5707    ]\n",
      " [0.7585     0.5934     0.6659    ]\n",
      " [0.6425     0.5618     0.5995    ]\n",
      " [0.6899     0.532      0.6007    ]\n",
      " [0.5875     0.4255     0.4935    ]\n",
      " [0.6357     0.5963     0.6153    ]\n",
      " [0.6405     0.5183     0.5729    ]\n",
      " [0.6426     0.6305     0.6365    ]\n",
      " [0.6334     0.5665     0.5981    ]\n",
      " [0.6306     0.4886     0.5506    ]\n",
      " [0.6921     0.6418     0.666     ]\n",
      " [0.5808     0.5244     0.5512    ]\n",
      " [0.6402     0.5832     0.6104    ]\n",
      " [0.671      0.5376     0.5969    ]\n",
      " [0.6761     0.5943     0.6326    ]\n",
      " [0.6425     0.5805     0.6099    ]\n",
      " [0.7033     0.4857     0.5746    ]\n",
      " [0.6395     0.5528     0.593     ]\n",
      " [0.6695     0.5172     0.5836    ]\n",
      " [0.6307     0.563      0.5949    ]\n",
      " [0.5442     0.5462     0.5452    ]\n",
      " [0.7282     0.6415     0.6821    ]\n",
      " [0.609      0.5882     0.5984    ]\n",
      " [0.6056     0.544      0.5732    ]\n",
      " [0.6406     0.5764     0.6068    ]\n",
      " [0.6616     0.5196     0.5821    ]\n",
      " [0.6679     0.5274     0.5894    ]\n",
      " [0.7213     0.6448     0.6809    ]\n",
      " [0.6614     0.551      0.6012    ]\n",
      " [0.6898     0.6271     0.657     ]\n",
      " [0.6872     0.6178     0.6506    ]\n",
      " [0.655807   0.565211   0.605994  ]\n",
      " [0.05055557 0.04755397 0.04194902]\n",
      " [0.5062     0.4255     0.4857    ]\n",
      " [0.7837     0.6727     0.724     ]]\n"
     ]
    }
   ],
   "source": [
    "# Independent of filename, summaries\n",
    "scores3 = get_bert(abstracts,texts,filename+'test_original')\n",
    "print(scores3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bert Miniexmples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258\n"
     ]
    }
   ],
   "source": [
    "#Example how bert score works\n",
    "\n",
    "# Example texts\n",
    "reference = \"This is a reference text example.\"\n",
    "candidate = \"This is a candidate text example.\"\n",
    "# BERTScore calculation\n",
    "scorer = BERTScorer(model_type='bert-base-uncased')\n",
    "P, R, F1 = scorer.score([candidate], [reference])\n",
    "print(f\"BERTScore Precision: {P.mean():.4f}, Recall: {R.mean():.4f}, F1: {F1.mean():.4f}\")\n",
    "\n",
    "### Outputs : BERTScore Precision: 0.9258, Recall: 0.9258, F1: 0.9258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.6683, Recall: 0.6563, F1: 0.6623\n",
      "BERTScore Precision: 0.3837, Recall: 0.4395, F1: 0.4097\n",
      "BERTScore Precision: 0.5076, Recall: 0.5019, F1: 0.5048\n",
      "BERTScore Precision: 0.4786, Recall: 0.5529, F1: 0.5131\n",
      "BERTScore Precision: 0.4802, Recall: 0.4910, F1: 0.4855\n",
      "BERTScore Precision: 0.5054, Recall: 0.5012, F1: 0.5033\n",
      "BERTScore Precision: 0.4818, Recall: 0.5386, F1: 0.5086\n",
      "BERTScore Precision: 0.4229, Recall: 0.4188, F1: 0.4209\n",
      "BERTScore Precision: 0.5127, Recall: 0.4940, F1: 0.5032\n",
      "BERTScore Precision: 0.4702, Recall: 0.5328, F1: 0.4996\n"
     ]
    }
   ],
   "source": [
    "#How tight are the summaries to the abstract? Which summary is the best score for the first abstract?\n",
    "for i in range(10):\n",
    "    P, R, F1 = scorer.score([summaries_list[i]], [abstracts[0]])\n",
    "    print(f\"BERTScore Precision: {P[0]:.4f}, Recall: {R[0]:.4f}, F1: {F1[0]:.4f}\")\n",
    "#Yes the other summaries are less similar than the one dedicated to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9918, Recall: 0.9585, F1: 0.9748\n"
     ]
    }
   ],
   "source": [
    "#Remove last sentence\n",
    "similar_summary = \"\"\"the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer.score([similar_summary], [abstracts[0]])\n",
    "print(f\"BERTScore Precision: {P[0]:.4f}, Recall: {R[0]:.4f}, F1: {F1[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.3344, Recall: 0.1858, F1: 0.2389\n"
     ]
    }
   ],
   "source": [
    "# No summary\n",
    "similar_summary = \"\"\"This is about.\"\"\"\n",
    "P, R, F1 = scorer.score([similar_summary], [abstracts[0]])\n",
    "print(f\"BERTScore Precision: {P[0]:.4f}, Recall: {R[0]:.4f}, F1: {F1[0]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BERTScore Precision: 0.9850, Recall: 0.9880, F1: 0.9865\n",
      "BERTScore Precision: 0.9717, Recall: 0.9638, F1: 0.9677\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replace synonyms:\n",
    "short - term -> short-duration \n",
    "discussed -> examined\n",
    "diagnosis -> evaluation\n",
    "proves against -> disproves\n",
    "\"\"\"\n",
    "similar_summary = \"\"\" the short-duration periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are examined. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the evaluation of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it disproves the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer.score([similar_summary], [abstracts[0]])\n",
    "print(f\"BERTScore Precision: {P[0]:.4f}, Recall: {R[0]:.4f}, F1: {F1[0]:.4f}\")\n",
    "#Very good. With replacing 4 synonms still a good score.\n",
    "\n",
    "#only remove these words\n",
    "similar_summary = \"\"\" the short periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer.score([similar_summary], [abstracts[0]])\n",
    "print(f\"BERTScore Precision: {P[0]:.4f}, Recall: {R[0]:.4f}, F1: {F1[0]:.4f}\")\n",
    "#There the score gots worse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215913"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "6440+203037+6436 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2809     0.2513     0.2653    ]\n",
      " [0.0511     0.1538     0.0767    ]\n",
      " [0.1101     0.2927     0.16      ]\n",
      " [0.0714     0.3919     0.1208    ]\n",
      " [0.1449     0.2727     0.1893    ]\n",
      " [0.1902     0.2335     0.2097    ]\n",
      " [0.1062     0.2244     0.1442    ]\n",
      " [0.1102     0.1667     0.1327    ]\n",
      " [0.1696     0.2241     0.1931    ]\n",
      " [0.1034     0.5652     0.1749    ]\n",
      " [0.1071     0.36       0.1651    ]\n",
      " [0.1383     0.2063     0.1656    ]\n",
      " [0.2461     0.2487     0.2474    ]\n",
      " [0.0972     0.2184     0.1345    ]\n",
      " [0.1232     0.2877     0.1725    ]\n",
      " [0.0788     0.1798     0.1096    ]\n",
      " [0.1587     0.3232     0.2129    ]\n",
      " [0.0663     0.1912     0.0985    ]\n",
      " [0.1166     0.2149     0.1512    ]\n",
      " [0.0939     0.2875     0.1415    ]\n",
      " [0.1337     0.1862     0.1556    ]\n",
      " [0.1394     0.3763     0.2035    ]\n",
      " [0.2137     0.122      0.1553    ]\n",
      " [0.0952     0.38       0.1523    ]\n",
      " [0.1429     0.2371     0.1783    ]\n",
      " [0.0988     0.2446     0.1408    ]\n",
      " [0.1893     0.2063     0.1974    ]\n",
      " [0.0962     0.3415     0.1501    ]\n",
      " [0.2486     0.4045     0.308     ]\n",
      " [0.1255     0.1735     0.1456    ]\n",
      " [0.1614     0.24       0.193     ]\n",
      " [0.1322     0.1322     0.1322    ]\n",
      " [0.0891     0.1078     0.0976    ]\n",
      " [0.1287     0.325      0.1844    ]\n",
      " [0.1165     0.1727     0.1391    ]\n",
      " [0.25       0.2059     0.2258    ]\n",
      " [0.0897     0.4        0.1466    ]\n",
      " [0.1375     0.404      0.2051    ]\n",
      " [0.1265     0.2593     0.17      ]\n",
      " [0.1593     0.2287     0.1878    ]\n",
      " [0.0806     0.34       0.1303    ]\n",
      " [0.0595     0.0884     0.0711    ]\n",
      " [0.0669     0.3636     0.113     ]\n",
      " [0.2273     0.2174     0.2222    ]\n",
      " [0.2086     0.299      0.2458    ]\n",
      " [0.1521     0.1983     0.1722    ]\n",
      " [0.1488     0.215      0.1759    ]\n",
      " [0.1165     0.3087     0.1691    ]\n",
      " [0.1166     0.4        0.1805    ]\n",
      " [0.1446     0.4675     0.2209    ]\n",
      " [0.0504     0.1832     0.0791    ]\n",
      " [0.131      0.3245     0.1867    ]\n",
      " [0.19       0.1687     0.1787    ]\n",
      " [0.2113     0.1415     0.1695    ]\n",
      " [0.104      0.3095     0.1557    ]\n",
      " [0.1333     0.2474     0.1733    ]\n",
      " [0.1124     0.1939     0.1423    ]\n",
      " [0.1478     0.1741     0.1599    ]\n",
      " [0.1643     0.2518     0.1989    ]\n",
      " [0.1435     0.2636     0.1858    ]\n",
      " [0.1123     0.4156     0.1768    ]\n",
      " [0.1847     0.1726     0.1785    ]\n",
      " [0.1368     0.1184     0.1269    ]\n",
      " [0.101      0.3652     0.1582    ]\n",
      " [0.1918     0.188      0.1899    ]\n",
      " [0.0185     0.1333     0.0325    ]\n",
      " [0.193      0.193      0.193     ]\n",
      " [0.15       0.2593     0.19      ]\n",
      " [0.1765     0.1184     0.1417    ]\n",
      " [0.1358     0.4228     0.2055    ]\n",
      " [0.0976     0.3708     0.1546    ]\n",
      " [0.1188     0.1387     0.128     ]\n",
      " [0.1758     0.1798     0.1778    ]\n",
      " [0.1347     0.3095     0.1877    ]\n",
      " [0.2846     0.175      0.2167    ]\n",
      " [0.192      0.3185     0.2396    ]\n",
      " [0.2634     0.2591     0.2612    ]\n",
      " [0.2647     0.2151     0.2374    ]\n",
      " [0.1232     0.3881     0.1871    ]\n",
      " [0.4331     0.3077     0.3598    ]\n",
      " [0.1146     0.2517     0.1574    ]\n",
      " [0.1233     0.3649     0.1843    ]\n",
      " [0.1706     0.2857     0.2136    ]\n",
      " [0.192      0.2319     0.2101    ]\n",
      " [0.1543     0.1677     0.1607    ]\n",
      " [0.0393     0.6458     0.0742    ]\n",
      " [0.1084     0.2596     0.153     ]\n",
      " [0.0978     0.314      0.1492    ]\n",
      " [0.1356     0.1301     0.1328    ]\n",
      " [0.1902     0.2108     0.2       ]\n",
      " [0.1364     0.2833     0.1841    ]\n",
      " [0.2179     0.1393     0.17      ]\n",
      " [0.2267     0.1399     0.173     ]\n",
      " [0.1691     0.2121     0.1882    ]\n",
      " [0.0971     0.274      0.1434    ]\n",
      " [0.0723     0.3152     0.1176    ]\n",
      " [0.1092     0.2923     0.159     ]\n",
      " [0.1928     0.215      0.2033    ]\n",
      " [0.1681     0.2346     0.1959    ]\n",
      " [0.1298     0.1937     0.1555    ]\n",
      " [0.144817   0.258082   0.171331  ]\n",
      " [0.06102045 0.09801187 0.04719476]\n",
      " [0.0185     0.0884     0.0325    ]\n",
      " [0.4331     0.6458     0.3598    ]]\n"
     ]
    }
   ],
   "source": [
    "#Get all F1 bert scores\n",
    "scorer_rouge = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "#scores = scorer.score(reference_summary, candidate_summary)\n",
    "\n",
    "def get_rouge(candidate,reference,filename):\n",
    "    scores = []\n",
    "    for i in range(len(candidate)):\n",
    "        P, R, F1 = scorer_rouge.score(reference[i],candidate[i])['rougeL']\n",
    "        scores.append([round(float(P),4),round(float(R),4),round(float(F1),4)])\n",
    "\n",
    "    m = np.mean(scores,axis=0)\n",
    "    s = np.std(scores,axis=0)\n",
    "    min = np.min(scores,axis=0)\n",
    "    max = np.max(scores,axis=0)\n",
    "    scores.append(m)\n",
    "    scores.append(s) \n",
    "    scores.append(min)\n",
    "    scores.append(max)   \n",
    "    np.savetxt('results/'+filename+'_rouge.txt',scores,fmt='%.4f')\n",
    "    return np.matrix(scores)\n",
    "scores = get_rouge(summaries_list,abstracts,filename)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_rouge(summaries_list,texts,filename+'compare_to_text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = get_rouge(abstracts,texts,filename+'test_original')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rouge miniexamples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RougeScore Precision: 1.0000, Recall: 0.8291, F1: 0.9066\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#Remove last sentence\n",
    "similar_summary = \"\"\"the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer_rouge.score(abstracts[0], similar_summary)['rougeL']\n",
    "print(f\"RougeScore Precision: {P:.4f}, Recall: {R:.4f}, F1: {F1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RougeScore Precision: 1.0000, Recall: 0.0151, F1: 0.0297\n"
     ]
    }
   ],
   "source": [
    "# No summary\n",
    "similar_summary = \"\"\"This is about.\"\"\"\n",
    "P, R, F1 = scorer_rouge.score(abstracts[0], similar_summary)['rougeL']\n",
    "print(f\"RougeScore Precision: {P:.4f}, Recall: {R:.4f}, F1: {F1:.4f}\")\n",
    "#Good, this is not a summary, good that the f1 value is very low. Precision is 1 because all words of similar_summary are in abstract[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RougeScore Precision: 0.9798, Recall: 0.9749, F1: 0.9773\n",
      "RougeScore Precision: 1.0000, Recall: 0.9749, F1: 0.9873\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replace synonyms:\n",
    "short - term -> short-duration \n",
    "discussed -> examined\n",
    "diagnosis -> evaluation\n",
    "proves against -> disproves\n",
    "\"\"\"\n",
    "similar_summary = \"\"\" the short-duration periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are examined. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the evaluation of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it disproves the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer_rouge.score(abstracts[0], similar_summary)['rougeL']\n",
    "print(f\"RougeScore Precision: {P:.4f}, Recall: {R:.4f}, F1: {F1:.4f}\")\n",
    "#With replacing 4 synonms a bit smaller precision and recall\n",
    "\n",
    "#only remove these words\n",
    "similar_summary = \"\"\" the short periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "P, R, F1 = scorer_rouge.score(abstracts[0], similar_summary)['rougeL']\n",
    "print(f\"RougeScore Precision: {P:.4f}, Recall: {R:.4f}, F1: {F1:.4f}\")\n",
    "#Very bad. Even tough the summary is worse the score is higher. This is because the recall is better. \n",
    "#The recall is 1 because all of the words of similar summary are in the abstract[0].\n",
    "#The precision is both the same because the number of correct words is the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.146"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.713+0.597+0.428+0.408"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the summaries from the 2 prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"write_summary100.txt\"\n",
    "s1 = get_summaries(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename =\"write_summary_complex100.txt\"\n",
    "s2 = get_summaries(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The text appears to be discussing the results of an analysis of sunspot data, specifically looking for periodicities in sunspot area fluctuations during the maximum activity period of solar cycle 16. The authors used wavelet analysis and autocorrelation functions to investigate the existence of a periodicity around 155 days. They found that the dispersion of points related to this periodicity was large, making it difficult to confirm its existence. They also compared the autocorrelation functions and periodograms of sunspot area fluctuations from each solar hemisphere separately during the maximum activity period. The results suggested that there might be a statistically significant positive peak in the interval of 155-165 days for the southern hemisphere, but the resolution of the periodogram was not sufficient to make a definitive conclusion. The authors also noted that power spectrum analysis alone may not be sufficient to detect true periodicities and suggested using both correlative and power spectrum analyses together. Overall, the text indicates that further research is needed to confirm the existence and significance of the 155-day periodicity in sunspot data.\n",
      " The text appears to be discussing the results of an analysis of sunspot data, specifically looking for periodicities in sunspot area fluctuations during the maximum activity period of solar cycle 16. The authors found that the existence of a periodicity around 0 days (approximately 6 solar rotations) is questionable based on their analysis of the time series and comparison with autocorrelation functions. They also noted that power spectrum analysis alone may not be sufficient to identify true periodicities, as it can be affected by aliases and subharmonics. The authors suggest that a combination of correlative and power spectrum analyses may be more effective in detecting statistically significant peaks in the periodograms.  The text also mentions that there are differences between the northern and southern hemispheres in terms of sunspot activity and periodicities, which is why separate analyses were conducted for each hemisphere. The authors found that while there was a statistically significant positive peak in the autocorrelation function and periodogram for daily sunspot area fluctuations from the southern hemisphere during the maximum activity period, it was not clear whether this represented a real periodicity or an alias.  Overall, the text suggests that further research is needed to confirm the existence of any specific periodicities in sunspot data, and that a combination of different analysis methods may be more effective than relying on any one method alone. The authors also emphasize the importance of considering the context and specific characteristics of the data being analyzed.\n",
      " the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
      " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
      " a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
      " it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
      " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n"
     ]
    }
   ],
   "source": [
    "print(s1[0])\n",
    "print(s2[0])\n",
    "print(abstracts[0])\n",
    "#second is longer and does include more information about the findings and conclusion. First only decribed the methods used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It appears that you have provided a list of references related to gravitational physics and astrophysics, specifically those dealing with topics such as black holes, gravitational waves, and cosmology. Here is the information extracted from the given references:  1. Jackiw and Pi (2003) - Phys. Rev. D **68**, 104012 (gr-qc/0308071) 2. Satoh, Kanno, and Soda (2008) - Phys. Rev. D **77**, 023526 (astro-ph/07063585) 3. Contaldi, Magueijo, and Smolin (2008) - Phys. Rev. Lett. **101**, 141101 (astro-ph/08063082) 4. Takahashi and Soda (2009) - Phys. Rev. Lett. **102**, 231301 (hep-th/09040554) 5. Cook and Sorbo (2012) - Phys. Rev. D **85**, 023534; **86**, 069901 6. Obata, Miura, and Soda (2015) - Phys. Rev. D **92**, 063516 (astro-ph/14127620) 7. Lightman et al. (1979) - Problem Book in Relativity and Gravitation, Princeton University Press 8. Maggiore (2008) - Gravitational Waves: Theory and Experiments, Oxford University Press 9. Rybicki and Lightman (1979) - Radiative Processes in Astrophysics, Wiley-Interscience 10. Landau and Lifshitz (1975) - The Classical Theory, Pergamon Press 11. Misner et al. (1973) - Gravitation, Freeman 12. Lightman et al. (1997) - Phys. Rev. D **56**, 545 (gr-qc/9607068) 13. Seljak and Zaldarriaga (1997) - Phys. Rev. Lett. **78**, 2054 (astro-ph/9609169) 14. Goldberg et al. (1967) - Journal of Mathematical Physics **8**, 2155 15. Anholm et al. (2009) - Phys. Rev. D **79**, 084030 (gr-qc/08090701) 16. Book and Flanagan (2011) - Phys. Rev. D **83**, 024024 (astro-ph/10094192) 17. Jenet and Romano (2015) - Am. J. Phys. **83**, 635 (gr-qc/14121142) 18. Hellings and Downs (1983) - Astrophys. J. **265**, L39  These references cover various aspects of gravitational physics and astrophysics, including theoretical models, observational studies, and experimental results.\n",
      " It appears that you have provided a list of research papers and related resources in the field of physics, specifically in areas such as gravitation, quantum mechanics, astrophysics, and general relativity. Each entry includes the authors' names, journal titles, publication years, DOI numbers, and arXiv preprint IDs (where available). Some entries also include book titles and their respective publishers.  Here are a few notable papers from the list:  1. Jackiw and Pi (2003) - \"Gravitational Waves from Colliding Cosmic Strings\" (Physical Review D, arXiv:hep-th/0308071) 2. Takahashi and Soda (2009) - \"Probing the Quark Gluon Plasma with Gravitational Waves from Heavy Ion Collisions\" (Physical Review Letters, arXiv:gr-qc/0904.0554) 3. Contaldi, Magueijo, and Smolin (2008) - \"Quantum Gravity and the Cosmic Microwave Background\" (Physical Review Letters, arXiv:astro-ph/0806.3082) 4. Lightman et al. (1979) - \"Problem Book in Relativity and Gravitation\" (Princeton University Press) 5. Maggiore (2008) - \"Gravitational Waves: Theory and Experiments\" (Oxford University Press) 6. Seljak and Zaldarriaga (1997) - \"Cosmic Microwave Background Anisotropies from Inflation\" (Physical Review Letters, arXiv:astro-ph/9609169)  These papers cover a range of topics in theoretical physics and astrophysics, including gravitational waves, cosmic strings, the cosmic microwave background, and problem books for learning about relativity and gravitation. The list also includes several influential textbooks and resources used in the field.\n",
      " we study the detectability of circular polarization in a stochastic gravitational wave background from various sources such as supermassive black hole binaries , cosmic strings , and inflation in the early universe with pulsar timing arrays . \n",
      " we calculate generalized overlap reduction functions for the circularly polarized stochastic gravitational wave background . \n",
      " we find that the circular polarization can not be detected for an isotropic background . however , there is a chance to observe the circular polarization for an anisotropic gravitational wave background . \n",
      " we also show how to separate polarized gravitational waves from unpolarized gravitational waves . \n"
     ]
    }
   ],
   "source": [
    "print(s1[1])\n",
    "print(s2[1])\n",
    "print(abstracts[1])\n",
    "#Both times the paper was interpreted as a list of papers. There was no summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text appears to be a scientific research paper on the topic of nuclear physics, specifically focusing on the calculation of decay half-lives for alpha decays in atomic nuclei and the penetration of potential barriers. The authors have developed a new analytical formula based on the Wentzel-Kramers-Brillouin (WKB) approximation to calculate the barrier penetrability, taking into account the influence of the long-range Coulomb tail in the barrier potential.  The text includes a comparison of the results obtained using different approaches, including the parabolic approximation and the WKB approach, for various isotopes of Po (polonium). The authors have assumed the barrier potential to be spherical for most cases, and they have shown that their new formula gives very good results for these spherical nuclei.  The text also discusses the importance of evaluating the integral in the equation for the penetrability and provides an analytical expression for it. The computation was supported by the National Natural Science Foundation, the Major State Basic Research Development Program of China, the Knowledge Innovation Project of CAS, and Deutsche Forschungsgemeinschaft.  Overall, the text presents a new analytical formula for calculating barrier penetrability in nuclear physics, which could be useful for studying barrier penetrability where one has to introduce an energy-dependent one-dimensional potential barrier or a barrier distribution function.\n",
      "-\n",
      " This text appears to be discussing the calculation of decay half-lives for nuclear reactions, specifically for alpha decays in various isotopes. The authors compare the results obtained using different approaches: the Wentzel-Kramers-Brillouin (WKB) approach, the parabolic approximation, and a new formula derived in this work.  The text also mentions that the barrier potential is assumed to be spherical for most of the calculations, but this assumption may not hold for all nuclei. The authors find good agreement between their calculated results and experimental data for spherical nuclei, with small deviations. They attribute the success of the new formula to its proper treatment of the long-range Coulomb tail in the barrier potential.  The text also mentions that the calculation was supported by various grants and computational resources. The derivation of the analytical expression for the integral in eq. (\n",
      "ef{x1x2}) is provided, with some approximations made to simplify the evaluation.\n",
      "--\n",
      " starting from the wkb approximation , a new barrier penetration formula is proposed for potential barriers containing a long - range coulomb interaction . \n",
      " this formula is especially proper for the barrier penetration with penetration energy much lower than the coulomb barrier . \n",
      " the penetrabilities calculated from the new formula agree well with the results from the wkb method . as a first attempt , \n",
      " this new formula is used to evaluate @xmath0 decay half - lives of atomic nuclei and a good agreement with the experiment is obtained . \n"
     ]
    }
   ],
   "source": [
    "print(s1[2])\n",
    "print(\"-\")\n",
    "print(s2[2])\n",
    "print(\"--\")\n",
    "print(abstracts[2])\n",
    "#Both good summaries, catching the idea of the paper, similar to the abstract. Second one attempts to give a formula but fails. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text discusses the comparison of various numerical integrators for computing molecular dynamics steps in the Schwinger model using the Hybrid Monte Carlo (HMC) algorithm. The authors consider a 122x122 lattice with coupling constant xmath123 and mass xmath124, taking parameters from [xcite] to simulate near the scaling limit with light fermions and increase the impact of the fermion part of the action. They use one thermalized gauge configuration and generate independent sets of momenta for each integrator and step size. The absolute error and statistical error are computed for each integrator and value of the step size. The micro step size is chosen to be 1/10 times smaller than the macro step size.  Figure [fig:1] compares the numerical integrators in terms of their absolute error versus the step size, showing that multi-rate schemes, RKF45 and RKF85, outperform their standard versions as expected. The adaptive nested force-gradient method has the best accuracy, while figure [fig:2] presents the CPU time required for each integrator versus the achieved accuracy. The nested force-gradient method and adapted nested force-gradient method show better computational efficiency than the other integrators.  Table [tab:1] shows the number of inversions of the Dirac operator needed to reach a 90% acceptance rate of the HMC algorithm, with @xmath121 being the most computationally demanding part. The adapted nested force-gradient method and nested force-gradient method require the least number of inversions per trajectory to reach the chosen acceptance rate.  The authors conclude that nested force-gradient schemes are an optimal choice for relatively high convergence order and low computational effort, and suggest further improvements by measuring Poisson brackets of the shadow Hamiltonian and tuning optimal parameters. In future work, they plan to apply this approach to the HMC algorithm for numerical integration in lattice QCD, expecting the adapted nested-force gradient scheme to outperform the original one with more partitioning of the action using techniques to factorize the fermion determinant.  References: [xcite] S. Duane, A.D. Kennedy, D. Pendleton, \"Hybrid Monte Carlo,\" Phys. Rev. Lett., vol. 59, no. 14, pp. 2273-2276, 1987. E. Hairer, G. Lubich, and G. Wanner, \"Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations,\" Springer, Berlin, 2002. I.M. Omelyan, R. Mryglod, and A. Folk, \"Symplectic Analytically Integrable Decomposition Algorithms: Classification, Derivation, and Application to Molecular Dynamics, Quantum and Celestial Mechanics,\" Comput. Phys. Commun., vol. 151, no. 2-3, pp. 267-304, 2003.\n",
      "-\n",
      " This text discusses the comparison of numerical integrators for computing molecular dynamics steps in the Schwinger model using the Hybrid Monte Carlo (HMC) algorithm. The authors consider a 2D lattice with specific parameters taken from a reference, aiming to simulate near the scaling limit with light fermions and increase the impact of the fermion part of the action. They use one thermalized gauge configuration and generate independent sets of momenta for each integrator and step size. The absolute error and statistical error are computed for each case.  The text compares the performance of several numerical integrators, including multi-rate schemes (Stochastic Heun, Runge-Kutta-Fehlberg, and Dormand-Prince), and their standard versions. Figure 1 shows the absolute error versus the step size, demonstrating that the multi-rate schemes outperform their standard counterparts as expected. The scheme with the best accuracy is the nested force-gradient method, which just slightly edges the adapted nested force-gradient scheme.  Figure 2 presents the CPU time required for each integrator versus the achieved accuracy. The results show that the nested force-gradient method and adapted nested force-gradient method are more computationally efficient than the other integrators, even compared to the 11-stage scheme. The modification proposed in a reference also performs better than its original version with almost similar computational costs.  The text also discusses the number of inversions of the Dirac operator needed to reach a 90% acceptance rate of the HMC algorithm and shows that the adapted nested force-gradient method and nested force-gradient method require the least number of inversions per trajectory. The authors suggest that these methods have the potential to perform even better in lattice QCD problems due to the significant impact of the fermion action and the computational time required to obtain the inversion of the Dirac operator.  In future work, the authors plan to apply this approach to the HMC algorithm for numerical integration in lattice QCD and expect the adapted nested-force gradient scheme to outperform the original one if they further partition the action into more parts using techniques to factorize the fermion determinant. This work is part of project B5 within the SFB/Transregio 55 _ Hadronenphysik mit Gitter-QCD _ funded by DFG (Deutsche Forschungsgemeinschaft).  References: - Duane, S., Kennedy, A.D., Pendleton, D., Roweth, Hybrid Monte Carlo, Phys. Rev. Lett. B195 (1987), pp. 1226-1230. - Hairer, E., Lubich, C., Wanner, G., Geometric Numerical Integration: Structure-Preserving Algorithms for Ordinary Differential Equations, Springer, Berlin, 2002. - Omelyan, I.M., Mryglod, R., Folk, Symplectic Analytically Integrable Decomposition Algorithms: Classification, Derivation, and Application to Molecular Dynamics, Comput. Phys. Commun. 151 (2003), pp. 47-68.\n",
      "--\n",
      " we study a novel class of numerical integrators , the adapted nested force - gradient schemes , used within the molecular dynamics step of the hybrid monte carlo ( hmc ) algorithm . \n",
      " we test these methods in the schwinger model on the lattice , a well known benchmark problem . \n",
      " we derive the analytical basis of nested force - gradient type methods and demonstrate the advantage of the proposed approach , namely reduced computational costs compared with other numerical integration schemes in hmc . \n"
     ]
    }
   ],
   "source": [
    "print(s1[3])\n",
    "print(\"-\")\n",
    "print(s2[3])\n",
    "print(\"--\")\n",
    "print(abstracts[3])\n",
    "#first summary goes too much into details, looses big picture. second as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This paper discusses methods for deriving functional equations for Feynman integrals, specifically focusing on vacuum type integrals and integrals with external momenta. The authors introduce new methods that are simple and do not rely on integration by parts techniques. They also demonstrate that integrals with many kinematic arguments can be reduced to a combination of simpler integrals with fewer arguments.  The paper presents two methods for deriving functional equations: one based on recurrence relations, and the other using algebraic relations between products of propagators. The authors note that it is not yet clear whether functional equations derived from recurrence relations can be reproduced by the methods of algebraic relations between products of propagators.  The paper includes specific examples of functional equations for vacuum type integrals and integrals with external momenta, and discusses their implications for one-loop integrals with four, five, and six external legs, as well as some two- and three-loop Feynman integrals. The work was supported by the German Science Foundation (DFG) within the collaborative research center 676 _ Particle, Strings and the Early Universe: The Structure of Matter and Space-Time_.  The authors express their gratitude to O.L. Veretin for providing results for integrals contributing to ortho-positronium lifetime described in ref.[xcite].\n",
      "-\n",
      " This text describes a paper that derives new functional equations for Feynman integrals using simple methods without integration by parts techniques. The paper also shows how integrals with many kinematic arguments can be reduced to combinations of simpler integrals with fewer arguments. The authors plan to demonstrate in future publications that applying functional equations can reduce \"master integrals\" to a combination of simpler integrals from a \"universal\" basis.  The method used in this paper is based on algebraic relations for \"deformed propagators.\" This method can be applied not only to vacuum-type integrals but also to integrals that depend on external momenta. The text discusses particular cases of functional equations and mentions the need for a systematic investigation using methods from algebraic geometry and group theory.  The paper also presents new hypergeometric representations for one-loop massless vertex type integrals, which were discovered by relating dimensionally regularized one-loop vertex type integrals to two-dimensional integrals through a functional equation. The authors summarize their accomplishments in the paper and express gratitude for support from the German Science Foundation (DFG).\n",
      "--\n",
      " new methods for obtaining functional equations for feynman integrals are presented . \n",
      " application of these methods for finding functional equations for various one- and two- loop integrals described in detail . \n",
      " it is shown that with the aid of functional equations feynman integrals in general kinematics can be expressed in terms of simpler integrals .    \n",
      " pacs numbers : 02.30.gp , 02.30.ks , 12.20.ds , 12.38.bx + keywords : feynman integrals , functional equations     +    derivation of functional equations for feynman integrals + from algebraic relations   +    * o.v .  \n",
      " tarasov * +   ii . \n",
      " institut fr theoretische physik , universitt hamburg , + luruper chaussee 149 , 22761 hamburg , germany + and + joint institute for nuclear research , + 141980 dubna , russian federation + : otarasov@jinr.ru + \n"
     ]
    }
   ],
   "source": [
    "print(s1[4])\n",
    "print(\"-\")\n",
    "print(s2[4])\n",
    "print(\"--\")\n",
    "print(abstracts[4])\n",
    "#Both good summaries. Second focuses more on the methods used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text appears to be discussing the use of the Hough transform in the search for gravitational waves, specifically in the context of periodic sources. The authors compare the efficiency and effectiveness of using a frequency Hough transform versus a sky Hough transform in terms of amplitude loss and computing cost. They find that the ratio of amplitude efficiencies is 1.317, which leads to a gain in computing cost for the same sensitivity. They also note the importance of adaptivity in the procedure and its applicability to situations where the source position is known but only the frequency needs to be estimated. The authors are also working on studying the efficiency of this method in terms of rejecting spurious lines in the peakmap.  The text includes several references to previous works on the Hough transform search for gravitational waves, including papers by Sintes et al., Astone et al., and Palomba et al. These papers discuss various aspects of the Hough transform search, such as its implementation, sensitivity evaluation, and application to Virgo data. The text also mentions a paper on \"incoherent strategies for the network detection of periodic gravitational waves\" by Palomba and Frasca, which may be relevant to the current discussion as well.\n",
      "-\n",
      " This text discusses the comparison between two methods, frequency Hough and sky Hough, used in the analysis of gravitational waves. The authors find that the use of an over-resolution for the sky map increases the amplitude loss for both methods but has a greater impact on the computing cost for the sky Hough method. They also note that the adaptivity of the procedure is simpler with the sky Hough method since each hough map is done for a single sky position. The text suggests that this new method is appropriate for situations where the source position is known and only the source frequency and spin down need to be estimated. Additionally, they are working on studying the efficiency of this method in terms of rejection of spurious lines in the peakmap. They expect this new method to be much more insensitive to the presence of spurious lines since in the chosen hough plane, spurious lines and gravitational wave signals should have a very different and well-separable behavior.  The text cites several papers related to the Hough transform search for continuous gravitational waves, improved Hough search for gravitational wave pulsars, the short FFT database and the peakmap for the hierarchical search of periodic sources, evaluation of sensitivity and computing power for the Virgo hierarchical search for periodic sources, adaptive Hough transform for the search of periodic sources, coincidence analysis between periodic source candidates in C6 and C7 Virgo data, first coincidence search among periodic gravitational wave source candidates using Virgo data, and incoherent strategies for the network detection of periodic gravitational waves.\n",
      "--\n",
      " in the hierarchical search for periodic sources of gravitational waves , the candidate selection , in the incoherent step , can be performed with hough transform procedures . in this paper \n",
      " we analyze the problem of sensitivity loss due to discretization of the parameters space vs computing cost , comparing the properties of the sky hough procedure with those of a new frequency hough , which is based on a transformation from the _ time - observed frequency _ plane to the _ source frequency - spin down _ plane . \n",
      " results on simulated peakmaps suggest various advantages in favor of the use of the frequency hough . the ones which show up to really make the difference are 1 ) the possibility to enhance the frequency resolution without relevantly affecting the computing cost . \n",
      " this reduces the digitization effects ; 2 ) the excess of candidates due to local disturbances in some places of the sky map . \n",
      " they do not affect the new analysis because each map is constructed for only one position in the sky . \n",
      " + pacs . \n",
      " numbers : 04.80nn,07.05kf,97.60jd \n"
     ]
    }
   ],
   "source": [
    "print(s1[5])\n",
    "print(\"-\")\n",
    "print(s2[5])\n",
    "print(\"--\")\n",
    "print(abstracts[5])\n",
    "#Both summaries give more information than the abstract. summary 1 gives references as well which is not suited for a summary. \n",
    "# Both say that one of the methods some results or mention numbers, but didn't tell  which of the methods is now better, as the abstract does. \n",
    "# Summary 2 is closer as summary 1 only mention the result in numbers without conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text is a scientific review discussing the progress made in identifying the progenitors of certain types of core-collapse supernovae (CCSNe) using pre-explosion images. The author discusses five different types of CCSNe and provides an update on the current understanding of their progenitor stars based on observational data.  Type II-Plateau supernovae (SNe II-P) are the best-studied category, with eight putative progenitor detections and 12 upper luminosity limits established. The evidence suggests that red supergiants (RSGs) are the immediate progenitors of SNe II-P. However, an intriguing result is that all but one of the detected SNe II-P have initial masses constrained to be below 30 solar masses, which is surprising since RSGs up to 32 solar masses are observed in the local group. This lack of massive RSG progenitors leads researchers to speculate that these massive RSG progenitors may be forming black holes heralded by faint or non-existent SN explosions.  Type II-Linear supernovae (SNe II-L) are a rare type, with only one known example (SN 1980k), and no pre-explosion images exist for most of them. The analysis of the stellar population of SN 1979c by @xcite determines a mass range of 34 solar masses for its progenitor, but firm conclusions about the progenitors cannot be made yet.  Type IIn supernovae (SNe IIn) have one known example (SN 2005gl) with a very massive progenitor that exploded while in the luminous blue variable (LBV) phase, but it is unclear if this is indicative of the class as a whole.  Type IIB supernovae have pre-explosion images for two events: SN 1993j in M81 and SN 2008ax. The analysis of SN 1993j suggests that a 35 solar mass star exploded in a binary system, with a slightly less massive secondary surviving the explosion. The recent discovery of pre-explosion HST/WFPC2 images for SN 2008ax provides an opportunity to further investigate this rare class of CCSNe.  Type Ib/c supernovae have ten upper limits but no detections from analysis of pre-SN images, which is surprising since it is commonly thought that at least some of their progenitors should be luminous, single W-R stars. However, it is unlikely that all SNe Ib/c come from them based on current data.  The author concludes by discussing the future directions of research in this field and the exciting prospects for discoveries at the extremes of mass ranges, as well as the role of binarity in these conclusions. The review provides a comprehensive update on the current state of research on CCSN progenitor identification using pre-explosion images.\n",
      "-\n",
      " The given text summarizes the current state of research on supernova (SN) progenitors based on direct observations of pre-SN images. Here's a brief summary:  1. Type II-plateau SN (SNe II-P): Eight putative progenitor detections and 12 upper luminosity limits have been established for this category, suggesting that red supergiants (RSG) are their immediate progenitors. However, the mass constraint on SNe II-P progenitors is surprising since RSG up to @xmath32 masses are observed in the local group and should have been detected in pre-SN images. This leads researchers to speculate that massive RSG progenitors may be forming black holes without SN explosions. 2. Type II-linear SN (SNe II-L): Only one SN II-L (SN 1980k) has a pre-SN image, which rules out massive RSG greater than about @xmath33. The progenitor mass range for this type of SN is not yet conclusively determined. 3. Type IIn SN: SN 2005gl is the only example with a detected progenitor in pre-SN images, and its massive progenitor's existence does not necessarily indicate that all Type IIn SNe have such massive progenitors. 4. Type Ib/c SN: No detections have been made from analysis of pre-SN images for this class, which is surprising since it is commonly thought that at least some of their progenitors should be luminous, single W-R stars. However, it is unlikely that all SNe Ib/c come from them. 5. Future directions: Researchers are interested in understanding the lack of massive RSG progenitors for SNe II-P and the possibility of a mass cutoff for direct collapse to black holes. They also want to investigate how binarity influences these conclusions. The field is expected to make significant advances in the coming decade.\n",
      "--\n",
      " i summarize what we have learned about the nature of stars that ultimately explode as core - collapse supernovae from the examination of images taken prior to the explosion . by registering pre - supernova and post - supernova images , usually taken at high resolution using either space - based optical detectors , or ground - based infrared detectors equipped with laser guide star adaptive optics systems , nearly three dozen core - collapse supernovae \n",
      " have now had the properties of their progenitor stars either directly measured or ( more commonly ) constrained by establishing upper limits on their luminosities . \n",
      " these studies enable direct comparison with stellar evolution models that , in turn , permit estimates of the progenitor stars physical characteristics to be made . \n",
      " i review progenitor characteristics ( or constraints ) inferred from this work for each of the major core - collapse supernova types ( ii - plateau , ii - linear , iib , iin , ib / c ) , with a particular focus on the analytical techniques used and the processes through which conclusions have been drawn . \n",
      " brief discussion of a few individual events is also provided , including sn 2005gl , a type iin supernova that is shown to have had an extremely luminous  and thus very massive  progenitor that exploded shortly after a violent , luminous blue variable - like eruption phase , contrary to standard theoretical predictions . \n"
     ]
    }
   ],
   "source": [
    "print(s1[6])\n",
    "print(\"-\")\n",
    "print(s2[6])\n",
    "print(\"--\")\n",
    "print(abstracts[6])\n",
    "#Identifies correctly that the paper summarizes works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text appears to be a list of scientific papers and talks related to spin physics, specifically in the context of various collaborations such as COMPASS, RHIC, GSI-PAX, and others. The papers cover topics such as single-spin asymmetries, Drell-Yan measurements, and transverse momentum dependence. Some of the authors mentioned include Avakian, Alexakhin, Belitsky, Boer, Bacchetta, Efremov, Qiu, and Vogelsang. The papers are published in various journals such as Physics Letters B, Nuclear Physics, and Physical Review D. Some of the talks were presented at workshops and symposiums held in places like Brookhaven National Laboratory, Kyoto, Japan, and Tsukuba, Ibaraki, Japan. The papers span the years from 1982 to 2006, with some works still in preparation.\n",
      "-\n",
      " This text appears to be a list of references for a research paper or report, likely in the field of high energy physics or nuclear physics. The references are cited alphabetically by author last name, with each reference given a unique identifier (e.g., \"a.\", \"b.\", \"c.\", etc.) for ease of referencing within the text.  The references include works by various collaborations and research groups, such as the Bravar spin muon collaboration, the COMPASS collaboration, and the GSI-PAX collaboration. Many of the references are journal articles published in physics journals, while others are conference proceedings or technical reports. The topics covered in these references include single-spin asymmetries, Drell-Yan measurements, and various theoretical models and calculations related to particle scattering and interactions.  Some of the references mention specific experiments or facilities, such as RHIC (Relativistic Heavy Ion Collider) and J-PARC (Japan Proton Accelerator Research Complex). Others refer to specific talks presented at various workshops and symposia. Overall, it appears that this list of references is related to research in the field of spin physics and hadron interactions.\n",
      "--\n",
      " we present a phenomenological study of the single - transverse spin asymmetry in azimuthal correlations of two jets produced nearly `` back - to - back '' in @xmath0 collisions at rhic . \n",
      " we properly take into account the initial- and final - state interactions of partons that can generate this asymmetry in qcd hard - scattering . using distribution functions fitted to the existing single - spin data , we make predictions for various weighted single - spin asymmetries in dijet correlations that are now readily testable at rhic . \n"
     ]
    }
   ],
   "source": [
    "print(s1[7])\n",
    "print(\"-\")\n",
    "print(s2[7])\n",
    "print(\"--\")\n",
    "print(abstracts[7])\n",
    "#Both interprete the papers as list of references. First one is better as it still gives some content in key words of the paper, while the second only focuses on the references\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text appears to be a proof of a theorem in statistics, specifically about the Kingman coalescent in the context of the exchangeable coalescent process. The proof uses the rate function and the large deviation principle. The theorem states that the number of particles in the exchangeable coalescent process converges almost surely to a certain random variable as the number of particles goes to infinity. The text also mentions Lemma [l : wn] which is used in the proof but its statement is not provided.  The proof starts by defining some notation and setting up the problem. It then uses the fact that the minimum of a function is attained at its global minimum to prove a certain assertion. The rate function is shown to be of a specific form using integration by parts and the distribution function of the one-dimensional standard Gaussian distribution.  The text then shows that the expected value of a certain term is positive, which is necessary for the proof. This is done by analyzing the behavior of the term as t goes to infinity and using the fact that the probability of an event related to a random variable is non-negative.  Finally, the theorem is proven using Lemma [l : wn] and the strong law of large numbers. The text concludes by acknowledging the contributions of other researchers and the funding sources for the research.\n",
      "-\n",
      " This text appears to be a proof of a theorem in statistics, specifically about the rate function of a coalescent process. The proof uses various mathematical results and lemmas, including Theorem 1.1 from a reference called \"@xcite\". The author also thanks several people for their contributions to the research.  The proof begins by defining some notation and setting up the problem. They then prove that the minimum of a certain quantity is achieved when a certain condition is met. Next, they show that the rate function has a specific form and justify its positivity.  The author then uses Lemma [l : wn] to prove an inequality, which holds almost surely as n goes to infinity. They conclude by thanking several people for their contributions to the research and acknowledging funding from the German Research Foundation (DFG). The reference \"@xcite\" is cited as a source for results used in the proof.\n",
      "--\n",
      " kingman s coalescent is a random tree that arises from classical population genetic models such as the moran model . \n",
      " the individuals alive in these models correspond to the leaves in the tree and the following two laws of large numbers concerning the structure of the tree - top are well - known : ( i ) the ( shortest ) distance , denoted by @xmath0 , from the tree - top to the level when there are @xmath1 lines in the tree satisfies @xmath2 almost surely ; ( ii ) at time @xmath0 , the population is naturally partitioned in exactly @xmath1 families where individuals belong to the same family if they have a common ancestor at time @xmath0 in the past . if @xmath3 denotes the size of the @xmath4th family , then @xmath5 almost surely . for both laws of large numbers \n",
      " we prove corresponding large deviations results . for ( i ) , the rate of the large deviations is @xmath1 and we can give the rate function explicitly . for ( ii ) , the rate is @xmath1 for downwards deviations and @xmath6 for upwards deviations . for both cases \n",
      " we give the exact rate function .    \n",
      " =     =    =     = \n"
     ]
    }
   ],
   "source": [
    "print(s1[8])\n",
    "print(\"-\")\n",
    "print(s2[8])\n",
    "print(\"--\")\n",
    "print(abstracts[8])\n",
    "#Both identify the important things second mentioned the thanks, not relevant in a summary. \n",
    "# Both mention rater how the theorem was proven than what it says which would be more interesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text describes the study of multi-component solitary waves in quasiperiodic quasi-phase matching (QPM) gratings using numerical simulations. The grating function varies according to the Fibonacci sequence, and its Fourier spectrum is composed of sums and differences of the basic wavenumbers, which fill the whole Fourier space densely due to their incommensurability.  The authors analyze the propagation and second harmonic generation (SHG) in the quasiperiodic QPM grating by going beyond the averaged equations to consider the rapid large-amplitude variations of the envelope functions. They find that for weak input beams, both beams eventually diffract, but when the amplitude of the input beam exceeds a certain threshold, self-focusing and localization are observed for both harmonics. The resulting two-component soliton is quasiperiodic by itself and oscillates in phase with the QPM grating modulation.  The authors also investigate the transition between the linear (diffraction) and nonlinear (self-trapping) regimes by simulating the transmission coefficients and beam widths at the output of the crystal versus the intensity of the fundamental harmonic input beam for various phase mismatch values. They find that a quasiperiodic soliton is generated only for sufficiently high amplitudes, illustrating the universality of localized mode generation for varying strength of nonlinearity in quasiperiodic waves.  The text also mentions several other examples of multi-component solitary waves in nonlinear optical fibers and waveguides, including multi-wavelength solitary waves in multi-channel bit-parallel wavelength fiber transmission systems, multi-color parametric spatial solitary waves due to multistep cascading in quadratic materials, and quasiperiodic envelope solitons in Fibonacci optical superlattices. These examples reveal some general features and properties of multi-component solitary waves in nonintegrable nonlinear models and serve as a stepping stone for approaching other problems of multi-mode soliton coupling and interaction.  The work was supported by the Australian Photonics Cooperative Research Centre and a collaborative Australia-Denmark grant of the Department of Industry, Science, and Tourism (Australia). For an overview of quadratic spatial solitons, see [Torner, L., in: Beam shaping and control with nonlinear optics, F. Kajzer and R. Reinisch, eds. (Plenum, New York, 1998), p. 229] and [Yu. S. Kivshar, in: Advanced photonics with second-order optically nonlinear processes, A. D. Boardman, L. Pavlov, and S. Tanev, eds. (Kluwer, Dordrecht, 1998), p. 451].\n",
      "-\n",
      " This text describes the study of multi-component solitary waves in quasiperiodic quasi-phase matching (QPM) gratings using Fibonacci sequences. The authors have numerically simulated the propagation of a fundamental beam through a QPM grating and observed different regimes: linear (diffraction) and nonlinear (self-trapping). In the linear regime, both harmonics are excited but eventually diffract. In the nonlinear regime, self-focusing and mutual self-trapping lead to the formation of spatially localized two-component solitons. The authors also found that the oscillations of the envelope solitons are in phase with the oscillations of the QPM grating, and the amplitude and width of the solitons depend on the effective mismatch between the input beam frequency and the nearest strong peak in the Fibonacci QPM grating spectrum. The results show that quasiperiodic envelope solitons can be generated for a broad range of phase-mismatch values. The authors also mention other examples of multi-component solitary waves in nonlinear optical fibers and waveguides, such as multi-wavelength solitary waves in multi-channel bit-parallel wavelength fiber transmission systems and multi-color parametric spatial solitary waves due to multistep cascading in quadratic materials. The work was supported by the Australian Photonics Cooperative Research Centre and a collaborative Australia-Denmark grant of the Department of Industry, Science, and Tourism (Australia). For more information on quadratic spatial solitons, see [Torner, L.,] in: _Beam shaping and control with nonlinear optics_, F. Kajzer and R. Reinisch, eds. (Plenum, New York, 1998), p. 229; and Yu. S. Kivshar, in: _Advanced photonics with second-order optically nonlinear processes_, A. D. Boardman, L. Pavlov, and S. Tanev, eds. (Kluwer, Dordrecht, 1998), p. 451.\n",
      "--\n",
      " we discuss several novel types of multi - component ( temporal and spatial ) envelope solitary waves that appear in fiber and waveguide nonlinear optics . \n",
      " in particular , we describe multi - channel solitary waves in bit - parallel - wavelength fiber transmission systems for high performance computer networks , multi - colour parametric spatial solitary waves due to cascaded nonlinearities of quadratic materials , and quasiperiodic envelope solitons due to quasi - phase - matching in fibonacci optical superlattices .    2 \n"
     ]
    }
   ],
   "source": [
    "print(s1[9])\n",
    "print(\"-\")\n",
    "print(s2[9])\n",
    "print(\"--\")\n",
    "print(abstracts[9])\n",
    "#similar to abstract, same keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len summary 1 177\n",
      "len summary 2 247\n",
      "compare it to the len of abstract 222\n",
      "len summary 1 274\n",
      "len summary 2 214\n",
      "compare it to the len of abstract 104\n",
      "len summary 1 216\n",
      "len summary 2 151\n",
      "compare it to the len of abstract 95\n",
      "len summary 1 393\n",
      "len summary 2 428\n",
      "compare it to the len of abstract 89\n",
      "len summary 1 206\n",
      "len summary 2 178\n",
      "compare it to the len of abstract 155\n",
      "len summary 1 206\n",
      "len summary 2 263\n",
      "compare it to the len of abstract 193\n",
      "len summary 1 421\n",
      "len summary 2 278\n",
      "compare it to the len of abstract 249\n",
      "len summary 1 116\n",
      "len summary 2 178\n",
      "compare it to the len of abstract 94\n",
      "len summary 1 234\n",
      "len summary 2 153\n",
      "compare it to the len of abstract 226\n",
      "len summary 1 367\n",
      "len summary 2 260\n",
      "compare it to the len of abstract 89\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(\"len summary 1\",len(s1[i].split(' ')))\n",
    "    print(\"len summary 2\",len(s2[i].split(' ')))\n",
    "    print(\"compare it to the len of abstract\",len(abstracts[i].split(' ')))\n",
    "#generated summaries are usually longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The summaries generated with the first prompt have in average 257.42 words\n",
      "The summaries generated with the second prompt have in average 271.31 words\n",
      "The abstracts have in average 172.18 words\n"
     ]
    }
   ],
   "source": [
    "print('The summaries generated with the first prompt have in average '+str(np.mean([len(s.split(' ')) for s in s1]))+' words')\n",
    "print('The summaries generated with the second prompt have in average '+str(np.mean([len(s.split(' ')) for s in s2]))+' words')\n",
    "print('The abstracts have in average '+str(np.mean([len(s.split(' ')) for s in abstracts]))+' words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The text appears to be discussing the results of an analysis of sunspot data, specifically looking for periodicities in sunspot area fluctuations during the maximum activity period of solar cycle 16. The authors used wavelet analysis and autocorrelation functions to investigate the existence of a periodicity around 155 days. They found that the dispersion of points related to this periodicity was large, making it difficult to confirm its existence. They also compared the autocorrelation functions and periodograms of sunspot area fluctuations from each solar hemisphere separately during the maximum activity period. The results suggested that there might be a statistically significant positive peak in the interval of 155-165 days for the southern hemisphere, but the resolution of the periodogram was not sufficient to make a definitive conclusion. The authors also noted that power spectrum analysis alone may not be sufficient to detect true periodicities and suggested using both correlative and power spectrum analyses together. Overall, the text indicates that further research is needed to confirm the existence and significance of the 155-day periodicity in sunspot data.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"mistral\",temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scorer_llm1(summary,abstract):\n",
    "    instruction = r\"\"\" I have an ideal summary and a candidate summary of a text. Please score the candidate summary on a scale of 0 to 1 based on accuracy, completeness, coherence, and relevance. Note if it is not a summary or unrelated the score should be low, still give a score. Be strict.\n",
    "                    At the end combine the scores to a Overall Score. \n",
    "                    \n",
    "                    Ideal Summary:\n",
    "                    \"\"\"+abstract+\"\"\"        \n",
    "\n",
    "                    Candidate Summary:\n",
    "                    \"\"\"+summary+\"\"\"\n",
    "\n",
    "                    Score:\n",
    "                    \"\"\"\n",
    "\n",
    "    answer = llm.invoke(instruction)\n",
    "    patterns = [\n",
    "    r'overall score\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Overall Score: 0.85'\n",
    "    r'overallscore\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Overall Score: 0.85'\n",
    "    r'score of (0|0?\\.\\d+|1\\.0) overall',  # Matches 'score of 0.85 overall'\n",
    "    r'score\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Score: 0.95' or similar\n",
    "    r'score of (0|0?\\.\\d+|1\\.0) ',  # Matches 'score of 0.95 overall'\n",
    "    r'score[:\\s]*(0|0?\\.\\d+|1\\.0)',  \n",
    "    r'(?<!\\d)(0|0?\\.\\d+|1\\.0)(?!\\d)' # Matches any number in interwall [0,1]\n",
    "    ]\n",
    "    # Initialize the variable to store the first valid score found\n",
    "    score = None\n",
    "\n",
    "    # Iterate over the patterns\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, answer, re.IGNORECASE)\n",
    "        if match:\n",
    "            score = match.group(1)\n",
    "            break  # Stop after finding the first valid match\n",
    "\n",
    "    return score,answer\n",
    "\n",
    "def scorer_llm2(summary,abstract):\n",
    "    instruction = r\"\"\" I have an ideal summary and a candidate summary of a text. Please score the candidate summary on a scale of 0 to 1 based on accuracy, completeness, coherence, and relevance. Note if it is not a summary or unrelated the score should be low, still give a score. Be strict.\n",
    "                    At the end combine the scores to a Overall Score. Only give the overall score.\n",
    "                    \n",
    "                    Ideal Summary:\n",
    "                    \"\"\"+abstract+\"\"\"        \n",
    "\n",
    "                    Candidate Summary:\n",
    "                    \"\"\"+summary+\"\"\"\n",
    "\n",
    "                    Score:\n",
    "                    \"\"\"\n",
    "    answer = llm.invoke(instruction)\n",
    "    patterns = [\n",
    "    r'overall score\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Overall Score: 0.85'\n",
    "    r'overallscore\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Overall Score: 0.85'\n",
    "    r'score of (0|0?\\.\\d+|1\\.0) overall',  # Matches 'score of 0.85 overall'\n",
    "    r'score\\s*:\\s*(0(?:\\.\\d+)?|1(?:\\.0)?)', # Matches 'Score: 0.95' or similar\n",
    "    r'score of (0|0?\\.\\d+|1\\.0) ',  # Matches 'score of 0.95 overall'\n",
    "    r'score[:\\s]*(0|0?\\.\\d+|1\\.0)',  \n",
    "    r'(?<!\\d)(0|0?\\.\\d+|1\\.0)(?!\\d)' # Matches any number in interwall [0,1]\n",
    "    ]\n",
    "    # Initialize the variable to store the first valid score found\n",
    "    score = None\n",
    "\n",
    "    # Iterate over the patterns\n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, answer, re.IGNORECASE)\n",
    "        if match:\n",
    "            score = match.group(1)\n",
    "            break  # Stop after finding the first valid match\n",
    "    return score,answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2985\n",
      "629\n",
      " Accuracy: 0.95 (The candidate summary accurately captures the main points of the ideal summary, with some minor differences in detail and phrasing.)\n",
      "\n",
      "Completeness: 0.85 (The candidate summary covers most of the key points from the ideal summary but may be missing some minor details or nuances.)\n",
      "\n",
      "Coherence: 1.0 (The candidate summary flows logically and makes sense as a whole, with clear connections between ideas.)\n",
      "\n",
      "Relevance: 1.0 (The candidate summary accurately reflects the content of the original text and remains focused on the topic of sunspot data analysis.)\n",
      "\n",
      "Overall Score: 0.925 (An average of the four scores above)\n",
      "---------\n",
      "0.925\n"
     ]
    }
   ],
   "source": [
    "score, ex = scorer_llm1(summaries_list[0],abstracts[0])\n",
    "print(ex)\n",
    "print('---------')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "llm_scores = []\n",
    "for i in range(len(summaries_list)):\n",
    "    score, _ = scorer_llm1(summaries_list[i],abstracts[i])\n",
    "    llm_scores.append(float(score))\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "m = np.mean(llm_scores)\n",
    "s = np.std(llm_scores)\n",
    "min = np.min(llm_scores)\n",
    "max = np.max(llm_scores)\n",
    "llm_scores.append(m)\n",
    "llm_scores.append(s) \n",
    "llm_scores.append(min)\n",
    "llm_scores.append(max)\n",
    "np.savetxt('results/'+filename+'_selfeval1.txt',np.array(llm_scores),fmt='%.4f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "llm_scores = []\n",
    "for i in range(len(summaries_list)):\n",
    "    score, _ = scorer_llm2(summaries_list[i],abstracts[i])\n",
    "    llm_scores.append(float(score))\n",
    "    if i%10==0:\n",
    "        print(i)\n",
    "m = np.mean(llm_scores)\n",
    "s = np.std(llm_scores)\n",
    "min = np.min(llm_scores)\n",
    "max = np.max(llm_scores)\n",
    "llm_scores.append(m)\n",
    "llm_scores.append(s) \n",
    "llm_scores.append(min)\n",
    "llm_scores.append(max)\n",
    "np.savetxt('results/'+filename+'_selfeval2.txt',np.array(llm_scores),fmt='%.4f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Miniexample Self evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('1.0', ' Overall Score: 1.0. The candidate summary is identical to the ideal summary, therefore it receives a perfect score for accuracy, completeness, coherence, and relevance.')\n"
     ]
    }
   ],
   "source": [
    "#Remove last sentence\n",
    "similar_summary = \"\"\"the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    "\"\"\"\n",
    "S = scorer_llm2(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('0', ' Overall Score: 0.0. The candidate summary does not provide any meaningful information and appears to be unrelated to the ideal summary.')\n"
     ]
    }
   ],
   "source": [
    "# No summary\n",
    "similar_summary = \"\"\"This is about.\"\"\"\n",
    "S = scorer_llm2(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('1.0', ' Overall Score: 1.0\\n\\nThe candidate summary accurately and completely captures the main points of the ideal summary, including the negative correlation and statistically significant peak for the periodicity of about @xmath0 days, the proposal of a new method to evaluate an echo effect in spectrum, the lack of differences in autocorrelation functions for daily sunspot area fluctuations and one rotation time interval in the northern hemisphere, and the indication of a periodicity of about @xmath0 days in sunspot area data from the southern hemisphere during the maximum activity period of solar cycle 16. The language used in the candidate summary is also coherent and relevant to the original text.')\n",
      "Selfevaluation score:  ('1.0', ' Overall Score: 1.0\\n\\nThe candidate summary is almost identical to the ideal summary, with minor typographical errors and a slight rearrangement of some sentences. The accuracy, completeness, coherence, and relevance are high as the main points and findings of the text are correctly represented in the summary.')\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replace synonyms:\n",
    "short - term -> short-duration \n",
    "discussed -> examined\n",
    "diagnosis -> evaluation\n",
    "proves against -> disproves\n",
    "\"\"\"\n",
    "similar_summary = \"\"\" the short-duration periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are examined. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the evaluation of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it disproves the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "S = scorer_llm2(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)\n",
    "\n",
    "\n",
    "#only remove these words\n",
    "similar_summary = \"\"\" the short periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "S = scorer_llm2(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prompt 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('1.0', ' Based on the given ideal summary and candidate summary, I would score the candidate summary as follows:\\n\\nAccuracy: 1.0 (The candidate summary matches the ideal summary almost exactly.)\\nCompleteness: 1.0 (The candidate summary includes all the essential points from the ideal summary.)\\nCoherence: 1.0 (The candidate summary is logically consistent and flows well.)\\nRelevance: 1.0 (The candidate summary focuses on the main topic of the text, which is the analysis of sunspot area data.)\\n\\nOverall Score: 1.0 (The candidate summary is an accurate, complete, coherent, and relevant summary of the original text.)')\n"
     ]
    }
   ],
   "source": [
    "#Remove last sentence\n",
    "similar_summary = \"\"\"the short - term periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are discussed . for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the diagnosis of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it proves against the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    "\"\"\"\n",
    "S = scorer_llm1(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('0', ' Accuracy: 0, as the candidate summary does not accurately represent the content of the ideal summary.\\nCoherence: 0, as there is no logical flow or connection in the candidate summary.\\nCompleteness: 0, as the candidate summary does not include any significant details from the ideal summary.\\nRelevance: 0, as the candidate summary does not relate to the content of the ideal summary.\\n\\nOverall Score: 0.')\n"
     ]
    }
   ],
   "source": [
    "# No summary\n",
    "similar_summary = \"\"\"This is about.\"\"\"\n",
    "S = scorer_llm1(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selfevaluation score:  ('1.0', ' Accuracy: 1 (The candidate summary correctly reports the main findings and conclusions of the ideal summary)\\nCompleteness: 1 (The candidate summary covers all the essential points discussed in the ideal summary)\\nCoherence: 1 (The candidate summary is logically consistent and flows smoothly)\\nRelevance: 1 (The candidate summary accurately reflects the content of the original text)\\n\\nOverall Score: 1.0 (The candidate summary is an accurate and complete summary of the original text, with high levels of coherence and relevance.)')\n",
      "Selfevaluation score:  ('1.0', ' Based on the given ideal summary and candidate summary, I would score the candidate summary as follows:\\n\\nAccuracy: 1.0 (The candidate summary closely matches the ideal summary in terms of facts and information presented.)\\n\\nCompleteness: 1.0 (The candidate summary covers all the essential points discussed in the ideal summary.)\\n\\nCoherence: 1.0 (The candidate summary flows logically and is easy to follow, just like the ideal summary.)\\n\\nRelevance: 1.0 (The candidate summary focuses on the main topic of the text, which is the analysis of sunspot area data from August 1923 to October 1933.)\\n\\nOverall Score: 1.0 (The candidate summary is an accurate, complete, coherent, and relevant summary of the original text.)')\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Replace synonyms:\n",
    "short - term -> short-duration \n",
    "discussed -> examined\n",
    "diagnosis -> evaluation\n",
    "proves against -> disproves\n",
    "\"\"\"\n",
    "similar_summary = \"\"\" the short-duration periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are examined. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the evaluation of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it disproves the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "S = scorer_llm1(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)\n",
    "\n",
    "\n",
    "#only remove these words\n",
    "similar_summary = \"\"\" the short periodicities of the daily sunspot area fluctuations from august 1923 to october 1933 are. for these data \n",
    " the correlative analysis indicates negative correlation for the periodicity of about @xmath0 days , but the power spectrum analysis indicates a statistically significant peak in this time interval . \n",
    " a new method of the of an echo - effect in spectrum is proposed and it is stated that the 155-day periodicity is a harmonic of the periodicities from the interval of @xmath1 $ ] days .    the autocorrelation functions for the daily sunspot area fluctuations and for the fluctuations of the one rotation time interval in the northern hemisphere , separately for the whole solar cycle 16 and for the maximum activity period of this cycle do not show differences , especially in the interval of @xmath2 $ ] days . \n",
    " it the thesis of the existence of strong positive fluctuations of the about @xmath0-day interval in the maximum activity period of the solar cycle 16 in the northern hemisphere . \n",
    " however , a similar analysis for data from the southern hemisphere indicates that there is the periodicity of about @xmath0 days in sunspot area data in the maximum activity period of the cycle 16 only . \n",
    "\"\"\"\n",
    "S = scorer_llm1(similar_summary, abstracts[0])\n",
    "print(\"Selfevaluation score: \", S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
