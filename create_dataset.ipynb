{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arxiv\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import arxiv_tool\n",
    "import json\n",
    "import numpy as np\n",
    "import copy\n",
    "separator = \"\\n---SEPARATOR---\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation\n",
    "Here the dataset for the classification (task2) was created and store into the folder task2/dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the data the first paper is LLM-based NLG Evaluation: Current Status and Challenges with the number 2402.01383v1\n",
    "First I download it from arxiv into a pdf.\n",
    "Then is get the text of the paper and collect its references. The references are now the correct ones. Then some unrelated references need to be added. Then the llm has to identify the correct ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "408.02304\n",
    "#paper_id = \"2402.01383v1\"\n",
    "#paper_id = \"2402.06196\"\n",
    "paper_id = \"2408.02304\"\n",
    "#paper_id = \"2408.02464\"\n",
    "#paper_id = \"2408.02085\"\n",
    "#paper_id = \"2311.13731\"\n",
    "#paper_id = \"2311.12785\"\n",
    "#paper_id = \"2409.15816\"\n",
    "#paper_id = \"2409.15180\"\n",
    "#paper_id = \"2409.09957\" \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id])))\n",
    "\n",
    "path = paper.download_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Compression in Recommender Systems: A Survey\n",
      "To alleviate the problem of information explosion, recommender systems are\n",
      "widely deployed to provide personalized information filtering services.\n",
      "Usually, embedding tables are employed in recommender systems to transform\n",
      "high-dimensional sparse one-hot vectors into dense real-valued embeddings.\n",
      "However, the embedding tables are huge and account for most of the parameters\n",
      "in industrial-scale recommender systems. In order to reduce memory costs and\n",
      "improve efficiency, various approaches are proposed to compress the embedding\n",
      "tables. In this survey, we provide a comprehensive review of embedding\n",
      "compression approaches in recommender systems. We first introduce deep learning\n",
      "recommendation models and the basic concept of embedding compression in\n",
      "recommender systems. Subsequently, we systematically organize existing\n",
      "approaches into three categories, namely low-precision, mixed-dimension, and\n",
      "weight-sharing, respectively. Lastly, we summarize the survey with some general\n",
      "suggestions and provide future prospects for this field.\n",
      "{'id': 'http://arxiv.org/abs/2408.02304v1', 'guidislink': True, 'link': 'http://arxiv.org/abs/2408.02304v1', 'updated': '2024-08-05T08:30:16Z', 'updated_parsed': time.struct_time(tm_year=2024, tm_mon=8, tm_mday=5, tm_hour=8, tm_min=30, tm_sec=16, tm_wday=0, tm_yday=218, tm_isdst=0), 'published': '2024-08-05T08:30:16Z', 'published_parsed': time.struct_time(tm_year=2024, tm_mon=8, tm_mday=5, tm_hour=8, tm_min=30, tm_sec=16, tm_wday=0, tm_yday=218, tm_isdst=0), 'title': 'Embedding Compression in Recommender Systems: A Survey', 'title_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'Embedding Compression in Recommender Systems: A Survey'}, 'summary': 'To alleviate the problem of information explosion, recommender systems are\\nwidely deployed to provide personalized information filtering services.\\nUsually, embedding tables are employed in recommender systems to transform\\nhigh-dimensional sparse one-hot vectors into dense real-valued embeddings.\\nHowever, the embedding tables are huge and account for most of the parameters\\nin industrial-scale recommender systems. In order to reduce memory costs and\\nimprove efficiency, various approaches are proposed to compress the embedding\\ntables. In this survey, we provide a comprehensive review of embedding\\ncompression approaches in recommender systems. We first introduce deep learning\\nrecommendation models and the basic concept of embedding compression in\\nrecommender systems. Subsequently, we systematically organize existing\\napproaches into three categories, namely low-precision, mixed-dimension, and\\nweight-sharing, respectively. Lastly, we summarize the survey with some general\\nsuggestions and provide future prospects for this field.', 'summary_detail': {'type': 'text/plain', 'language': None, 'base': '', 'value': 'To alleviate the problem of information explosion, recommender systems are\\nwidely deployed to provide personalized information filtering services.\\nUsually, embedding tables are employed in recommender systems to transform\\nhigh-dimensional sparse one-hot vectors into dense real-valued embeddings.\\nHowever, the embedding tables are huge and account for most of the parameters\\nin industrial-scale recommender systems. In order to reduce memory costs and\\nimprove efficiency, various approaches are proposed to compress the embedding\\ntables. In this survey, we provide a comprehensive review of embedding\\ncompression approaches in recommender systems. We first introduce deep learning\\nrecommendation models and the basic concept of embedding compression in\\nrecommender systems. Subsequently, we systematically organize existing\\napproaches into three categories, namely low-precision, mixed-dimension, and\\nweight-sharing, respectively. Lastly, we summarize the survey with some general\\nsuggestions and provide future prospects for this field.'}, 'authors': [{'name': 'Shiwei Li'}, {'name': 'Huifeng Guo'}, {'name': 'Xing Tang'}, {'name': 'Ruiming Tang'}, {'name': 'Lu Hou'}, {'name': 'Ruixuan Li'}, {'name': 'Rui Zhang'}], 'author_detail': {'name': 'Rui Zhang'}, 'author': 'Rui Zhang', 'arxiv_doi': '10.1145/3637841', 'links': [{'title': 'doi', 'href': 'http://dx.doi.org/10.1145/3637841', 'rel': 'related', 'type': 'text/html'}, {'href': 'http://arxiv.org/abs/2408.02304v1', 'rel': 'alternate', 'type': 'text/html'}, {'title': 'pdf', 'href': 'http://arxiv.org/pdf/2408.02304v1', 'rel': 'related', 'type': 'application/pdf'}], 'arxiv_comment': 'Accepted by ACM Computing Surveys', 'arxiv_journal_ref': 'ACM Comput. Surv. 56, 5, Article 130 (January 2024)', 'arxiv_primary_category': {'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom'}, 'tags': [{'term': 'cs.IR', 'scheme': 'http://arxiv.org/schemas/atom', 'label': None}]}\n",
      "[arxiv.Result.Author('Shiwei Li'), arxiv.Result.Author('Huifeng Guo'), arxiv.Result.Author('Xing Tang'), arxiv.Result.Author('Ruiming Tang'), arxiv.Result.Author('Lu Hou'), arxiv.Result.Author('Ruixuan Li'), arxiv.Result.Author('Rui Zhang')]\n",
      "2024-08-05 08:30:16+00:00\n"
     ]
    }
   ],
   "source": [
    "print(paper.title)\n",
    "print(paper.summary)\n",
    "raw = paper._raw\n",
    "print(raw)\n",
    "#paper.updated\n",
    "print(paper.authors)\n",
    "print(paper.published)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_text = arxiv_tool.get_full_text(path)\n",
    "numbers = arxiv_tool.extract_arxiv_ids(full_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "references_start = re.search(r'References|Bibliography', full_text, re.IGNORECASE)\n",
    "references_text = full_text[references_start.start():]\n",
    "numbers = arxiv_tool.extract_arxiv_ids(references_text)\n",
    "#Remove the id of the original paper from its references\n",
    "if paper_id in numbers:\n",
    "    numbers.remove(paper_id)\n",
    "print(len(numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len(numbers))\n",
    "df_all = pd.DataFrame({'arxiv_id': numbers})\n",
    " # Save the DataFrame to a CSV file\n",
    "#df_all.to_csv('task2/dataset/'+paper_id+'_all.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There have been 7 reference documents extracted from the original paper.\n"
     ]
    }
   ],
   "source": [
    "def create_json(ids):\n",
    "    if len(ids)>50:\n",
    "        ids = ids[:50]\n",
    "    json_data = {}\n",
    "    for id in ids:\n",
    "        paper = next(arxiv.Client().results(arxiv.Search(id_list=[id])))\n",
    "        json_data[id] = {'title':paper.title,'abstract':paper.summary,'date':paper.published.isoformat(),'label':1}  \n",
    "    return json_data\n",
    "\n",
    "data  = create_json(numbers)\n",
    "#data\n",
    "print(f\"There have been {len(data.keys())} reference documents extracted from the original paper.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'2202.01279': {'title': 'PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts', 'abstract': 'PromptSource is a system for creating, sharing, and using natural language\\nprompts. Prompts are functions that map an example from a dataset to a natural\\nlanguage input and target output. Using prompts to train and query language\\nmodels is an emerging area in NLP that requires new tools that let users\\ndevelop and refine these prompts collaboratively. PromptSource addresses the\\nemergent challenges in this new setting with (1) a templating language for\\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\\non prompt development by observing outputs of their prompts on many examples,\\nand (3) a community-driven set of guidelines for contributing new prompts to a\\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\\nin PromptSource. PromptSource is available at\\nhttps://github.com/bigscience-workshop/promptsource.', 'date': '2022-02-02T20:48:54+00:00', 'label': 1}, '2302.04023': {'title': 'A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity', 'abstract': 'This paper proposes a framework for quantitatively evaluating interactive\\nLLMs such as ChatGPT using publicly available data sets. We carry out an\\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\\nand multi-modal aspects of ChatGPT based on these data sets and a newly\\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\\ntasks. We find that it is better at understanding non-Latin script languages\\nthan generating them. It is able to generate multimodal content from textual\\nprompts, via an intermediate code generation step. Moreover, we find that\\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\\nhence making it an unreliable reasoner. It is, for example, better at deductive\\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\\nother LLMs and it generates more extrinsic hallucinations from its parametric\\nmemory as it does not have access to an external knowledge base. Finally, the\\ninteractive feature of ChatGPT enables human collaboration with the underlying\\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\\nrelease codebase for evaluation set extraction.', 'date': '2023-02-08T12:35:34+00:00', 'label': 1}, '2308.14840': {'title': 'Identifying and Mitigating the Security Risks of Generative AI', 'abstract': 'Every major technical invention resurfaces the dual-use dilemma -- the new\\ntechnology has the potential to be used for good as well as for harm.\\nGenerative AI (GenAI) techniques, such as large language models (LLMs) and\\ndiffusion models, have shown remarkable capabilities (e.g., in-context\\nlearning, code-completion, and text-to-image generation and editing). However,\\nGenAI can be used just as well by attackers to generate new attacks and\\nincrease the velocity and efficacy of existing attacks.\\n  This paper reports the findings of a workshop held at Google (co-organized by\\nStanford University and the University of Wisconsin-Madison) on the dual-use\\ndilemma posed by GenAI. This paper is not meant to be comprehensive, but is\\nrather an attempt to synthesize some of the interesting findings from the\\nworkshop. We discuss short-term and long-term goals for the community on this\\ntopic. We hope this paper provides both a launching point for a discussion on\\nthis important topic as well as interesting problems that the research\\ncommunity can work to address.', 'date': '2023-08-28T18:51:09+00:00', 'label': 1}, '2305.01625': {'title': 'Unlimiformer: Long-Range Transformers with Unlimited Length Input', 'abstract': 'Since the proposal of transformers, these models have been limited to bounded\\ninput lengths, because of their need to attend to every token in the input. In\\nthis work, we propose Unlimiformer: a general approach that wraps any existing\\npretrained encoder-decoder transformer, and offloads the cross-attention\\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\\ndistances are the attention dot-product scores. This kNN index can be kept on\\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\\nindex practically unlimited input sequences, while every attention head in\\nevery decoder layer retrieves its top-k keys, instead of attending to every\\nkey. We evaluate Unlimiformer on several long-document and book-summarization\\nbenchmarks, showing that it can process even 500k token-long inputs from the\\nBookSum dataset, without any input truncation at test time. We demonstrate that\\nUnlimiformer improves pretrained models such as BART and Longformer by\\nextending them to unlimited inputs without additional learned weights and\\nwithout modifying their code. We make our code and models publicly available at\\nhttps://github.com/abertsch72/unlimiformer .', 'date': '2023-05-02T17:35:08+00:00', 'label': 1}, '2304.11062': {'title': 'Scaling Transformer to 1M tokens and beyond with RMT', 'abstract': 'A major limitation for the broader scope of problems solvable by transformers\\nis the quadratic scaling of computational complexity with input size. In this\\nstudy, we investigate the recurrent memory augmentation of pre-trained\\ntransformer models to extend input context length while linearly scaling\\ncompute. Our approach demonstrates the capability to store information in\\nmemory for sequences of up to an unprecedented two million tokens while\\nmaintaining high retrieval accuracy. Experiments with language modeling tasks\\nshow perplexity improvement as the number of processed input segments\\nincreases. These results underscore the effectiveness of our method, which has\\nsignificant potential to enhance long-term dependency handling in natural\\nlanguage understanding and generation tasks, as well as enable large-scale\\ncontext processing for memory-intensive applications.', 'date': '2023-04-19T16:18:54+00:00', 'label': 1}, '2303.04226': {'title': 'A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT', 'abstract': 'Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\\nattention from society. As a result, many individuals have become interested in\\nrelated resources and are seeking to uncover the background and secrets behind\\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\\ntechniques belong to the category of Artificial Intelligence Generated Content\\n(AIGC), which involves the creation of digital content, such as images, music,\\nand natural language, through AI models. The goal of AIGC is to make the\\ncontent creation process more efficient and accessible, allowing for the\\nproduction of high-quality content at a faster pace. AIGC is achieved by\\nextracting and understanding intent information from instructions provided by\\nhuman, and generating the content according to its knowledge and the intent\\ninformation. In recent years, large-scale models have become increasingly\\nimportant in AIGC as they provide better intent extraction and thus, improved\\ngeneration results. With the growth of data and the size of the models, the\\ndistribution that the model can learn becomes more comprehensive and closer to\\nreality, leading to more realistic and high-quality content generation. This\\nsurvey provides a comprehensive review on the history of generative models, and\\nbasic components, recent advances in AIGC from unimodal interaction and\\nmultimodal interaction. From the perspective of unimodality, we introduce the\\ngeneration tasks and relative models of text and image. From the perspective of\\nmultimodality, we introduce the cross-application between the modalities\\nmentioned above. Finally, we discuss the existing open problems and future\\nchallenges in AIGC.', 'date': '2023-03-07T20:36:13+00:00', 'label': 1}, '2303.17466': {'title': 'Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study', 'abstract': 'The recent release of ChatGPT has garnered widespread recognition for its\\nexceptional ability to generate human-like responses in dialogue. Given its\\nusage by users from various nations and its training on a vast multilingual\\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\\nevaluate its effectiveness in cultural adaptation. In this paper, we\\ninvestigate the underlying cultural background of ChatGPT by analyzing its\\nresponses to questions designed to quantify human cultural differences. Our\\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\\nstrong alignment with American culture, but it adapts less effectively to other\\ncultural contexts. Furthermore, by using different prompts to probe the model,\\nwe show that English prompts reduce the variance in model responses, flattening\\nout cultural differences and biasing them towards American culture. This study\\nprovides valuable insights into the cultural implications of ChatGPT and\\nhighlights the necessity of greater diversity and cultural awareness in\\nlanguage technologies.', 'date': '2023-03-30T15:43:39+00:00', 'label': 1}, '2307.03109': {'title': 'A Survey on Evaluation of Large Language Models', 'abstract': \"Large language models (LLMs) are gaining increasing popularity in both\\nacademia and industry, owing to their unprecedented performance in various\\napplications. As LLMs continue to play a vital role in both research and daily\\nuse, their evaluation becomes increasingly critical, not only at the task\\nlevel, but also at the society level for better understanding of their\\npotential risks. Over the past years, significant efforts have been made to\\nexamine LLMs from various perspectives. This paper presents a comprehensive\\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\\nan overview from the perspective of evaluation tasks, encompassing general\\nnatural language processing tasks, reasoning, medical usage, ethics,\\neducations, natural and social sciences, agent applications, and other areas.\\nSecondly, we answer the `where' and `how' questions by diving into the\\nevaluation methods and benchmarks, which serve as crucial components in\\nassessing performance of LLMs. Then, we summarize the success and failure cases\\nof LLMs in different tasks. Finally, we shed light on several future challenges\\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\\nmore proficient LLMs. Our key point is that evaluation should be treated as an\\nessential discipline to better assist the development of LLMs. We consistently\\nmaintain the related open-source materials at:\\nhttps://github.com/MLGroupJLU/LLM-eval-survey.\", 'date': '2023-07-06T16:28:35+00:00', 'label': 1}, '2306.03082': {'title': 'InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models', 'abstract': 'Large language models~(LLMs) are instruction followers, but it can be\\nchallenging to find the best instruction for different situations, especially\\nfor black-box LLMs on which backpropagation is forbidden. Instead of directly\\noptimizing the discrete instruction, we optimize a low-dimensional soft prompt\\napplied to an open-source LLM to generate the instruction for the black-box\\nLLM. On each iteration of the proposed method, which we call InstructZero, a\\nsoft prompt is converted into an instruction using the open-source LLM, which\\nis then submitted to the black-box LLM for zero-shot evaluation, and the\\nperformance is sent to Bayesian optimization to produce new soft prompts\\nimproving the zero-shot performance. We evaluate InstructZero on different\\ncombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our\\nresults show that InstructZero outperforms SOTA auto-instruction methods across\\na variety of downstream tasks. Our code and data are publicly available at\\nhttps://github.com/Lichang-Chen/InstructZero.', 'date': '2023-06-05T17:55:22+00:00', 'label': 1}, '2306.14979': {'title': 'LM4HPC: Towards Effective Language Model Application in High-Performance Computing', 'abstract': 'In recent years, language models (LMs), such as GPT-4, have been widely used\\nin multiple domains, including natural language processing, visualization, and\\nso on. However, applying them for analyzing and optimizing high-performance\\ncomputing (HPC) software is still challenging due to the lack of HPC-specific\\nsupport. In this paper, we design the LM4HPC framework to facilitate the\\nresearch and development of HPC software analyses and optimizations using LMs.\\nTailored for supporting HPC datasets, AI models, and pipelines, our framework\\nis built on top of a range of components from different levels of the machine\\nlearning software stack, with Hugging Face-compatible APIs. Using three\\nrepresentative tasks, we evaluated the prototype of our framework. The results\\nshow that LM4HPC can help users quickly evaluate a set of state-of-the-art\\nmodels and generate insightful leaderboards.', 'date': '2023-06-26T18:05:03+00:00', 'label': 1}, '2305.05176': {'title': 'FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance', 'abstract': 'There is a rapidly growing number of large language models (LLMs) that users\\ncan query for a fee. We review the cost associated with querying popular LLM\\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\\nheterogeneous pricing structures, with fees that can differ by two orders of\\nmagnitude. In particular, using LLMs on large collections of queries and text\\ncan be expensive. Motivated by this, we outline and discuss three types of\\nstrategies that users can exploit to reduce the inference cost associated with\\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\\ncascade which learns which combinations of LLMs to use for different queries in\\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\\nThe ideas and findings presented here lay a foundation for using LLMs\\nsustainably and efficiently.', 'date': '2023-05-09T05:11:02+00:00', 'label': 1}, '2308.10848': {'title': 'AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors', 'abstract': 'Autonomous agents empowered by Large Language Models (LLMs) have undergone\\nsignificant improvements, enabling them to generalize across a broad spectrum\\nof tasks. However, in real-world scenarios, cooperation among individuals is\\noften required to enhance the efficiency and effectiveness of task\\naccomplishment. Hence, inspired by human group dynamics, we propose a\\nmulti-agent framework \\\\framework that can collaboratively and dynamically\\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\\nexperiments demonstrate that \\\\framework framework can effectively deploy\\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\\nthe emergence of social behaviors among individual agents within a group during\\ncollaborative task accomplishment. In view of these behaviors, we discuss some\\npossible strategies to leverage positive ones and mitigate negative ones for\\nimproving the collaborative potential of multi-agent groups. Our codes for\\n\\\\framework will soon be released at\\n\\\\url{https://github.com/OpenBMB/AgentVerse}.', 'date': '2023-08-21T16:47:11+00:00', 'label': 1}, '2303.00293': {'title': 'How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks', 'abstract': \"The GPT-3.5 models have demonstrated impressive performance in various\\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\\nand reasoning capabilities. However, their robustness and abilities to handle\\nvarious complexities of the open world have yet to be explored, which is\\nespecially crucial in assessing the stability of models and is a key aspect of\\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\\nsamples) with 66 text transformations from TextFlint that cover 9 popular\\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\\nencounters significant robustness degradation, such as its average performance\\ndropping by up to 35.74\\\\% and 43.59\\\\% in natural language inference and\\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\\nspecific robustness challenges, including robustness instability, prompt\\nsensitivity, and number sensitivity. These insights are valuable for\\nunderstanding its limitations and guiding future research in addressing these\\nchallenges to enhance GPT-3.5's overall performance and generalization\\nabilities.\", 'date': '2023-03-01T07:39:01+00:00', 'label': 1}, '2204.02311': {'title': 'PaLM: Scaling Language Modeling with Pathways', 'abstract': 'Large language models have been shown to achieve remarkable performance\\nacross a variety of natural language tasks using few-shot learning, which\\ndrastically reduces the number of task-specific training examples needed to\\nadapt the model to a particular application. To further our understanding of\\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\\ndensely activated, Transformer language model, which we call Pathways Language\\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\\nsystem which enables highly efficient training across multiple TPU Pods. We\\ndemonstrate continued benefits of scaling by achieving state-of-the-art\\nfew-shot learning results on hundreds of language understanding and generation\\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\\nperformance, outperforming the finetuned state-of-the-art on a suite of\\nmulti-step reasoning tasks, and outperforming average human performance on the\\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\\nshowed discontinuous improvements from model scale, meaning that performance\\nsteeply increased as we scaled to our largest model. PaLM also has strong\\ncapabilities in multilingual tasks and source code generation, which we\\ndemonstrate on a wide array of benchmarks. We additionally provide a\\ncomprehensive analysis on bias and toxicity, and study the extent of training\\ndata memorization with respect to model scale. Finally, we discuss the ethical\\nconsiderations related to large language models and discuss potential\\nmitigation strategies.', 'date': '2022-04-05T16:11:45+00:00', 'label': 1}, '2210.11416': {'title': 'Scaling Instruction-Finetuned Language Models', 'abstract': 'Finetuning language models on a collection of datasets phrased as\\ninstructions has been shown to improve model performance and generalization to\\nunseen tasks. In this paper we explore instruction finetuning with a particular\\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\\nfinetuning on chain-of-thought data. We find that instruction finetuning with\\nthe above aspects dramatically improves performance on a variety of model\\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\\nfew-shot performance even compared to much larger models, such as PaLM 62B.\\nOverall, instruction finetuning is a general method for improving the\\nperformance and usability of pretrained language models.', 'date': '2022-10-20T16:58:32+00:00', 'label': 1}, '2307.08715': {'title': 'MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots', 'abstract': 'Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\\nservices due to their exceptional proficiency in understanding and generating\\nhuman-like text. LLM chatbots, in particular, have seen widespread adoption,\\ntransforming human-machine interactions. However, these LLM chatbots are\\nsusceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to\\nelicit inappropriate or sensitive responses, contravening service policies.\\nDespite existing attempts to mitigate such threats, our research reveals a\\nsubstantial gap in our understanding of these vulnerabilities, largely due to\\nthe undisclosed defensive measures implemented by LLM service providers.\\n  In this paper, we present Jailbreaker, a comprehensive framework that offers\\nan in-depth understanding of jailbreak attacks and countermeasures. Our work\\nmakes a dual contribution. First, we propose an innovative methodology inspired\\nby time-based SQL injection techniques to reverse-engineer the defensive\\nstrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.\\nThis time-sensitive approach uncovers intricate details about these services\\'\\ndefenses, facilitating a proof-of-concept attack that successfully bypasses\\ntheir mechanisms. Second, we introduce an automatic generation method for\\njailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of\\nautomated jailbreak generation across various commercial LLM chatbots. Our\\nmethod achieves a promising average success rate of 21.58%, significantly\\noutperforming the effectiveness of existing techniques. We have responsibly\\ndisclosed our findings to the concerned service providers, underscoring the\\nurgent need for more robust defenses. Jailbreaker thus marks a significant step\\ntowards understanding and mitigating jailbreak threats in the realm of LLM\\nchatbots.', 'date': '2023-07-16T01:07:15+00:00', 'label': 1}, '2304.05335': {'title': 'Toxicity in ChatGPT: Analyzing Persona-assigned Language Models', 'abstract': 'Large language models (LLMs) have shown incredible capabilities and\\ntranscended the natural language processing (NLP) community, with adoption\\nthroughout many services like healthcare, therapy, education, and customer\\nservice. Since users include people with critical information needs like\\nstudents or patients engaging with chatbots, the safety of these systems is of\\nprime importance. Therefore, a clear understanding of the capabilities and\\nlimitations of LLMs is necessary. To this end, we systematically evaluate\\ntoxicity in over half a million generations of ChatGPT, a popular\\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\\nincreases the toxicity of generations. Depending on the persona assigned to\\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\\nfind concerning patterns where specific entities (e.g., certain races) are\\ntargeted more than others (3x more) irrespective of the assigned persona, that\\nreflect inherent discriminatory biases in the model. We hope that our findings\\ninspire the broader AI community to rethink the efficacy of current safety\\nguardrails and develop better techniques that lead to robust, safe, and\\ntrustworthy AI systems.', 'date': '2023-04-11T16:53:54+00:00', 'label': 1}, '1810.04805': {'title': 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding', 'abstract': 'We introduce a new language representation model called BERT, which stands\\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\\nlanguage representation models, BERT is designed to pre-train deep\\nbidirectional representations from unlabeled text by jointly conditioning on\\nboth left and right context in all layers. As a result, the pre-trained BERT\\nmodel can be fine-tuned with just one additional output layer to create\\nstate-of-the-art models for a wide range of tasks, such as question answering\\nand language inference, without substantial task-specific architecture\\nmodifications.\\n  BERT is conceptually simple and empirically powerful. It obtains new\\nstate-of-the-art results on eleven natural language processing tasks, including\\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\\n(5.1 point absolute improvement).', 'date': '2018-10-11T00:50:01+00:00', 'label': 1}, '2111.01998': {'title': 'OpenPrompt: An Open-source Framework for Prompt-learning', 'abstract': 'Prompt-learning has become a new paradigm in modern natural language\\nprocessing, which directly adapts pre-trained language models (PLMs) to\\n$cloze$-style prediction, autoregressive modeling, or sequence to sequence\\ngeneration, resulting in promising performances on various tasks. However, no\\nstandard implementation framework of prompt-learning is proposed yet, and most\\nexisting prompt-learning codebases, often unregulated, only provide limited\\nimplementations for specific scenarios. Since there are many details such as\\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\\nto be considered in prompt-learning, practitioners face impediments to quickly\\nadapting the desired prompt learning methods to their applications. In this\\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\\nequipped with efficiency, modularity, and extendibility, and its combinability\\nallows the freedom to combine different PLMs, task formats, and prompting\\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\\nframeworks and evaluate the generalization of them on different NLP tasks\\nwithout constraints. OpenPrompt is publicly released at {\\\\url{\\nhttps://github.com/thunlp/OpenPrompt}}.', 'date': '2021-11-03T03:31:14+00:00', 'label': 1}, '2301.00234': {'title': 'A Survey on In-context Learning', 'abstract': 'With the increasing capabilities of large language models (LLMs), in-context\\nlearning (ICL) has emerged as a new paradigm for natural language processing\\n(NLP), where LLMs make predictions based on contexts augmented with a few\\nexamples. It has been a significant trend to explore ICL to evaluate and\\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\\nthe progress and challenges of ICL. We first present a formal definition of ICL\\nand clarify its correlation to related studies. Then, we organize and discuss\\nadvanced techniques, including training strategies, prompt designing\\nstrategies, and related analysis. Additionally, we explore various ICL\\napplication scenarios, such as data engineering and knowledge updating.\\nFinally, we address the challenges of ICL and suggest potential directions for\\nfurther research. We hope that our work can encourage more research on\\nuncovering how ICL works and improving ICL.', 'date': '2022-12-31T15:57:09+00:00', 'label': 1}, '2301.03220': {'title': 'Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks', 'abstract': \"Artificial Intelligence-Generated Content (AIGC) refers to the use of AI to\\nautomate the information creation process while fulfilling the personalized\\nrequirements of users. However, due to the instability of AIGC models, e.g.,\\nthe stochastic nature of diffusion models, the quality and accuracy of the\\ngenerated content can vary significantly. In wireless edge networks, the\\ntransmission of incorrectly generated content may unnecessarily consume network\\nresources. Thus, a dynamic AIGC service provider (ASP) selection scheme is\\nrequired to enable users to connect to the most suited ASP, improving the\\nusers' satisfaction and quality of generated content. In this article, we first\\nreview the AIGC techniques and their applications in wireless networks. We then\\npresent the AIGC-as-a-service (AaaS) concept and discuss the challenges in\\ndeploying AaaS at the edge networks. Yet, it is essential to have performance\\nmetrics to evaluate the accuracy of AIGC services. Thus, we introduce several\\nimage-based perceived quality evaluation metrics. Then, we propose a general\\nand effective model to illustrate the relationship between computational\\nresources and user-perceived quality evaluation metrics. To achieve efficient\\nAaaS and maximize the quality of generated content in wireless edge networks,\\nwe propose a deep reinforcement learning-enabled algorithm for optimal ASP\\nselection. Simulation results show that the proposed algorithm can provide a\\nhigher quality of generated content to users and achieve fewer crashed tasks by\\ncomparing with four benchmarks, i.e., overloading-avoidance, random,\\nround-robin policies, and the upper-bound schemes.\", 'date': '2023-01-09T09:30:23+00:00', 'label': 1}, '2304.03738': {'title': 'Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models', 'abstract': 'As the capabilities of generative language models continue to advance, the\\nimplications of biases ingrained within these models have garnered increasing\\nattention from researchers, practitioners, and the broader public. This article\\ninvestigates the challenges and risks associated with biases in large-scale\\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\\namong others, the nature of training data, model specifications, algorithmic\\nconstraints, product design, and policy decisions. We explore the ethical\\nconcerns arising from the unintended consequences of biased model outputs. We\\nfurther analyze the potential opportunities to mitigate biases, the\\ninevitability of some biases, and the implications of deploying these models in\\nvarious applications, such as virtual assistants, content generation, and\\nchatbots. Finally, we review the current approaches to identify, quantify, and\\nmitigate biases in language models, emphasizing the need for a\\nmulti-disciplinary, collaborative effort to develop more equitable,\\ntransparent, and responsible AI systems. This article aims to stimulate a\\nthoughtful dialogue within the artificial intelligence community, encouraging\\nresearchers and developers to reflect on the role of biases in generative\\nlanguage models and the ongoing pursuit of ethical AI.', 'date': '2023-04-07T17:14:00+00:00', 'label': 1}, '2302.12173': {'title': \"Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection\", 'abstract': \"Large Language Models (LLMs) are increasingly being integrated into various\\napplications. The functionalities of recent LLMs can be flexibly modulated via\\nnatural language prompts. This renders them susceptible to targeted adversarial\\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\\noriginal instructions and employed controls. So far, it was assumed that the\\nuser is directly prompting the LLM. But, what if it is not the user prompting?\\nWe argue that LLM-Integrated Applications blur the line between data and\\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\\nthat enable adversaries to remotely (without a direct interface) exploit\\nLLM-integrated applications by strategically injecting prompts into data likely\\nto be retrieved. We derive a comprehensive taxonomy from a computer security\\nperspective to systematically investigate impacts and vulnerabilities,\\nincluding data theft, worming, information ecosystem contamination, and other\\nnovel security risks. We demonstrate our attacks' practical viability against\\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\\nengines, and synthetic applications built on GPT-4. We show how processing\\nretrieved prompts can act as arbitrary code execution, manipulate the\\napplication's functionality, and control how and if other APIs are called.\\nDespite the increasing integration and reliance on LLMs, effective mitigations\\nof these emerging threats are currently lacking. By raising awareness of these\\nvulnerabilities and providing key insights into their implications, we aim to\\npromote the safe and responsible deployment of these powerful models and the\\ndevelopment of robust defenses that protect users and systems from potential\\nattacks.\", 'date': '2023-02-23T17:14:38+00:00', 'label': 1}, '2308.00352': {'title': 'MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework', 'abstract': 'Remarkable progress has been made on automated problem solving through\\nsocieties of agents based on large language models (LLMs). Existing LLM-based\\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\\ncomplex tasks, however, are complicated through logic inconsistencies due to\\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\\nMetaGPT, an innovative meta-programming framework incorporating efficient human\\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\\nStandardized Operating Procedures (SOPs) into prompt sequences for more\\nstreamlined workflows, thus allowing agents with human-like domain expertise to\\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\\nline paradigm to assign diverse roles to various agents, efficiently breaking\\ndown complex tasks into subtasks involving many agents working together. On\\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\\nsolutions than previous chat-based multi-agent systems. Our project can be\\nfound at https://github.com/geekan/MetaGPT', 'date': '2023-08-01T07:49:10+00:00', 'label': 1}, '2106.09685': {'title': 'LoRA: Low-Rank Adaptation of Large Language Models', 'abstract': 'An important paradigm of natural language processing consists of large-scale\\npre-training on general domain data and adaptation to particular tasks or\\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\\ndeploying independent instances of fine-tuned models, each with 175B\\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\\nLoRA, which freezes the pre-trained model weights and injects trainable rank\\ndecomposition matrices into each layer of the Transformer architecture, greatly\\nreducing the number of trainable parameters for downstream tasks. Compared to\\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\\ntraining throughput, and, unlike adapters, no additional inference latency. We\\nalso provide an empirical investigation into rank-deficiency in language model\\nadaptation, which sheds light on the efficacy of LoRA. We release a package\\nthat facilitates the integration of LoRA with PyTorch models and provide our\\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\\nhttps://github.com/microsoft/LoRA.', 'date': '2021-06-17T17:37:18+00:00', 'label': 1}, '2306.13651': {'title': 'Bring Your Own Data! Self-Supervised Evaluation for Large Language Models', 'abstract': 'With the rise of Large Language Models (LLMs) and their ubiquitous deployment\\nin diverse domains, measuring language model behavior on realistic data is\\nimperative. For example, a company deploying a client-facing chatbot must\\nensure that the model will not respond to client requests with profanity.\\nCurrent evaluations approach this problem using small, domain-specific datasets\\nwith human-curated labels. These evaluation sets are often sampled from a\\nnarrow and simplified distribution, and data sources can unknowingly be leaked\\ninto the training set which can lead to misleading evaluations. To bypass these\\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\\nanalyzing their sensitivity or invariance to transformations on the input text.\\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\\ncollected in the wild or streamed during live model deployment. We demonstrate\\nself-supervised evaluation strategies for measuring closed-book knowledge,\\ntoxicity, and long-range context dependence, in addition to sensitivity to\\ngrammatical structure and tokenization errors. When comparisons to similar\\nhuman-labeled benchmarks are available, we find strong correlations between\\nself-supervised and human-supervised evaluations. The self-supervised paradigm\\ncomplements current evaluation strategies that rely on labeled data.', 'date': '2023-06-23T17:59:09+00:00', 'label': 1}, '2305.18486': {'title': 'A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets', 'abstract': \"The development of large language models (LLMs) such as ChatGPT has brought a\\nlot of attention recently. However, their evaluation in the benchmark academic\\ndatasets remains under-explored due to the difficulty of evaluating the\\ngenerative outputs produced by this model against the ground truth. In this\\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\\ndiverse academic datasets, covering tasks like question-answering, text\\nsummarization, code generation, commonsense reasoning, mathematical\\nproblem-solving, machine translation, bias detection, and ethical\\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\\n255K responses it generates in these datasets. This makes our work the largest\\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\\nfor future research using LLMs. We also report a new emergent ability to follow\\nmulti-query instructions that we mostly found in ChatGPT and other\\ninstruction-tuned models. Our extensive evaluation shows that even though\\nChatGPT is capable of performing a wide variety of tasks, and may obtain\\nimpressive performance in several benchmark datasets, it is still far from\\nachieving the ability to reliably solve many challenging tasks. By providing a\\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\\nreal-world applications.\", 'date': '2023-05-29T12:37:21+00:00', 'label': 1}, '1910.13461': {'title': 'BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension', 'abstract': 'We present BART, a denoising autoencoder for pretraining sequence-to-sequence\\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\\nfunction, and (2) learning a model to reconstruct the original text. It uses a\\nstandard Tranformer-based neural machine translation architecture which,\\ndespite its simplicity, can be seen as generalizing BERT (due to the\\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\\nmore recent pretraining schemes. We evaluate a number of noising approaches,\\nfinding the best performance by both randomly shuffling the order of the\\noriginal sentences and using a novel in-filling scheme, where spans of text are\\nreplaced with a single mask token. BART is particularly effective when fine\\ntuned for text generation but also works well for comprehension tasks. It\\nmatches the performance of RoBERTa with comparable training resources on GLUE\\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\\ndialogue, question answering, and summarization tasks, with gains of up to 6\\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\\nfor machine translation, with only target language pretraining. We also report\\nablation experiments that replicate other pretraining schemes within the BART\\nframework, to better measure which factors most influence end-task performance.', 'date': '2019-10-29T18:01:00+00:00', 'label': 1}, '2306.15261': {'title': 'A Survey on Out-of-Distribution Evaluation of Neural NLP Models', 'abstract': 'Adversarial robustness, domain generalization and dataset biases are three\\nactive lines of research contributing to out-of-distribution (OOD) evaluation\\non neural NLP models. However, a comprehensive, integrated discussion of the\\nthree research lines is still lacking in the literature. In this survey, we 1)\\ncompare the three lines of research under a unifying definition; 2) summarize\\nthe data-generating processes and evaluation protocols for each line of\\nresearch; and 3) emphasize the challenges and opportunities for future work.', 'date': '2023-06-27T07:44:25+00:00', 'label': 1}, '2305.13711': {'title': 'LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models', 'abstract': 'We propose LLM-Eval, a unified multi-dimensional automatic evaluation method\\nfor open-domain conversations with large language models (LLMs). Existing\\nevaluation methods often rely on human annotations, ground-truth responses, or\\nmultiple LLM prompts, which can be expensive and time-consuming. To address\\nthese issues, we design a single prompt-based evaluation method that leverages\\na unified evaluation schema to cover multiple dimensions of conversation\\nquality in a single model call. We extensively evaluate the performance of\\nLLM-Eval on various benchmark datasets, demonstrating its effectiveness,\\nefficiency, and adaptability compared to state-of-the-art evaluation methods.\\nOur analysis also highlights the importance of choosing suitable LLMs and\\ndecoding strategies for accurate evaluation results. LLM-Eval offers a\\nversatile and robust solution for evaluating open-domain conversation systems,\\nstreamlining the evaluation process and providing consistent performance across\\ndiverse scenarios.', 'date': '2023-05-23T05:57:09+00:00', 'label': 1}, '2306.05499': {'title': 'Prompt Injection attack against LLM-integrated Applications', 'abstract': 'Large Language Models (LLMs), renowned for their superior proficiency in\\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\\napplications around them. However, their extensive assimilation into various\\nservices introduces significant security risks. This study deconstructs the\\ncomplexities and implications of prompt injection attacks on actual\\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\\nten commercial applications, highlighting the constraints of current attack\\nstrategies in practice. Prompted by these limitations, we subsequently\\nformulate HouYi, a novel black-box prompt injection attack technique, which\\ndraws inspiration from traditional web injection attacks. HouYi is\\ncompartmentalized into three crucial elements: a seamlessly-incorporated\\npre-constructed prompt, an injection prompt inducing context partition, and a\\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\\non 36 actual LLM-integrated applications and discern 31 applications\\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\\nincluding Notion, which has the potential to impact millions of users. Our\\ninvestigation illuminates both the possible risks of prompt injection attacks\\nand the possible tactics for mitigation.', 'date': '2023-06-08T18:43:11+00:00', 'label': 1}, '1907.11692': {'title': 'RoBERTa: A Robustly Optimized BERT Pretraining Approach', 'abstract': 'Language model pretraining has led to significant performance gains but\\ncareful comparison between different approaches is challenging. Training is\\ncomputationally expensive, often done on private datasets of different sizes,\\nand, as we will show, hyperparameter choices have significant impact on the\\nfinal results. We present a replication study of BERT pretraining (Devlin et\\nal., 2019) that carefully measures the impact of many key hyperparameters and\\ntraining data size. We find that BERT was significantly undertrained, and can\\nmatch or exceed the performance of every model published after it. Our best\\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\\nhighlight the importance of previously overlooked design choices, and raise\\nquestions about the source of recently reported improvements. We release our\\nmodels and code.', 'date': '2019-07-26T17:48:29+00:00', 'label': 1}, '2304.09842': {'title': 'Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models', 'abstract': 'Large language models (LLMs) have achieved remarkable progress in solving\\nvarious natural language processing tasks due to emergent reasoning abilities.\\nHowever, LLMs have inherent limitations as they are incapable of accessing\\nup-to-date information (stored on the Web or in task-specific knowledge bases),\\nusing external tools, and performing precise mathematical and logical\\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\\nthese limitations by augmenting LLMs with plug-and-play modules for\\ncompositional reasoning. Chameleon synthesizes programs by composing various\\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\\nsequence of tools to execute to generate the final response. We showcase the\\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\\noverall accuracy on ScienceQA, improving the best published few-shot result by\\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\\nlifting the state of the art to 98.78%. Our analysis also shows that the\\nGPT-4-powered planner exhibits more consistent and rational tool selection via\\ninferring potential constraints from instructions, compared to a\\nChatGPT-powered planner. The project is available at\\nhttps://chameleon-llm.github.io.', 'date': '2023-04-19T17:47:47+00:00', 'label': 1}, '2305.17826': {'title': 'NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models', 'abstract': 'Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor\\nattacks against prompt-based models consider injecting backdoors into the\\nentire embedding layers or word embedding vectors. Such attacks can be easily\\naffected by retraining on downstream tasks and with different prompting\\nstrategies, limiting the transferability of backdoor attacks. In this work, we\\npropose transferable backdoor attacks against prompt-based models, called\\nNOTABLE, which is independent of downstream tasks and prompting strategies.\\nSpecifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing\\nan adaptive verbalizer to bind triggers to specific words (i.e., anchors). It\\nactivates the backdoor by pasting input with triggers to reach\\nadversary-desired anchors, achieving independence from downstream tasks and\\nprompting strategies. We conduct experiments on six NLP tasks, three popular\\nmodels, and three prompting strategies. Empirical results show that NOTABLE\\nachieves superior attack performance (i.e., attack success rate over 90% on all\\nthe datasets), and outperforms two state-of-the-art baselines. Evaluations on\\nthree defenses show the robustness of NOTABLE. Our code can be found at\\nhttps://github.com/RU-System-Software-and-Security/Notable.', 'date': '2023-05-28T23:35:17+00:00', 'label': 1}, '2302.07842': {'title': 'Augmented Language Models: a Survey', 'abstract': 'This survey reviews works in which language models (LMs) are augmented with\\nreasoning skills and the ability to use tools. The former is defined as\\ndecomposing a potentially complex task into simpler subtasks while the latter\\nconsists in calling external modules such as a code interpreter. LMs can\\nleverage these augmentations separately or in combination via heuristics, or\\nlearn to do so from demonstrations. While adhering to a standard missing tokens\\nprediction objective, such augmented LMs can use various, possibly\\nnon-parametric external modules to expand their context processing ability,\\nthus departing from the pure language modeling paradigm. We therefore refer to\\nthem as Augmented Language Models (ALMs). The missing token objective allows\\nALMs to learn to reason, use tools, and even act, while still performing\\nstandard natural language tasks and even outperforming most regular LMs on\\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\\nconclude that this new research direction has the potential to address common\\nlimitations of traditional LMs such as interpretability, consistency, and\\nscalability issues.', 'date': '2023-02-15T18:25:52+00:00', 'label': 1}, '2202.12837': {'title': 'Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?', 'abstract': 'Large language models (LMs) are able to in-context learn -- perform a new\\ntask via inference alone by conditioning on a few input-label pairs\\n(demonstrations) and making predictions for new inputs. However, there has been\\nlittle understanding of how the model learns and which aspects of the\\ndemonstrations contribute to end task performance. In this paper, we show that\\nground truth demonstrations are in fact not required -- randomly replacing\\nlabels in the demonstrations barely hurts performance on a range of\\nclassification and multi-choce tasks, consistently over 12 different models\\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\\nthe key drivers of end task performance, including the fact that they provide a\\nfew examples of (1) the label space, (2) the distribution of the input text,\\nand (3) the overall format of the sequence. Together, our analysis provides a\\nnew way of understanding how and why in-context learning works, while opening\\nup new questions about how much can be learned from large language models\\nthrough inference alone.', 'date': '2022-02-25T17:25:19+00:00', 'label': 1}, '2112.09332': {'title': 'WebGPT: Browser-assisted question-answering with human feedback', 'abstract': \"We fine-tune GPT-3 to answer long-form questions using a text-based\\nweb-browsing environment, which allows the model to search and navigate the\\nweb. By setting up the task so that it can be performed by humans, we are able\\nto train models on the task using imitation learning, and then optimize answer\\nquality with human feedback. To make human evaluation of factual accuracy\\neasier, models must collect references while browsing in support of their\\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\\ncloning, and then performing rejection sampling against a reward model trained\\nto predict human preferences. This model's answers are preferred by humans 56%\\nof the time to those of our human demonstrators, and 69% of the time to the\\nhighest-voted answer from Reddit.\", 'date': '2021-12-17T05:43:43+00:00', 'label': 1}, '2303.08774': {'title': 'GPT-4 Technical Report', 'abstract': \"We report the development of GPT-4, a large-scale, multimodal model which can\\naccept image and text inputs and produce text outputs. While less capable than\\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\\nvarious professional and academic benchmarks, including passing a simulated bar\\nexam with a score around the top 10% of test takers. GPT-4 is a\\nTransformer-based model pre-trained to predict the next token in a document.\\nThe post-training alignment process results in improved performance on measures\\nof factuality and adherence to desired behavior. A core component of this\\nproject was developing infrastructure and optimization methods that behave\\npredictably across a wide range of scales. This allowed us to accurately\\npredict some aspects of GPT-4's performance based on models trained with no\\nmore than 1/1,000th the compute of GPT-4.\", 'date': '2023-03-15T17:15:04+00:00', 'label': 1}, '2306.05036': {'title': 'Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale', 'abstract': 'Large language models (LLMs), such as ChatGPT and GPT-4, are gaining\\nwide-spread real world use. Yet, these LLMs are closed source, and little is\\nknown about their performance in real-world use cases. In this paper, we apply\\nand evaluate the combination of ChatGPT and GPT-4 for the real-world task of\\nmining insights from a text corpus in order to identify research challenges in\\nthe field of HCI. We extract 4,392 research challenges in over 100 topics from\\nthe 2023~CHI conference proceedings and visualize the research challenges for\\ninteractive exploration. We critically evaluate the LLMs on this practical task\\nand conclude that the combination of ChatGPT and GPT-4 makes an excellent\\ncost-efficient means for analyzing a text corpus at scale. Cost-efficiency is\\nkey for flexibly prototyping research ideas and analyzing text corpora from\\ndifferent perspectives, with implications for applying LLMs for mining insights\\nin academia and practice.', 'date': '2023-06-08T08:41:30+00:00', 'label': 1}, '2308.01990': {'title': 'From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?', 'abstract': 'Large Language Models (LLMs) have found widespread applications in various\\ndomains, including web applications, where they facilitate human interaction\\nvia chatbots with natural language interfaces. Internally, aided by an\\nLLM-integration middleware such as Langchain, user prompts are translated into\\nSQL queries used by the LLM to provide meaningful responses to users. However,\\nunsanitized user prompts can lead to SQL injection attacks, potentially\\ncompromising the security of the database. Despite the growing interest in\\nprompt injection vulnerabilities targeting LLMs, the specific risks of\\ngenerating SQL injection attacks through prompt injections have not been\\nextensively studied. In this paper, we present a comprehensive examination of\\nprompt-to-SQL (P$_2$SQL) injections targeting web applications based on the\\nLangchain framework. Using Langchain as our case study, we characterize\\nP$_2$SQL injections, exploring their variants and impact on application\\nsecurity through multiple concrete examples. Furthermore, we evaluate 7\\nstate-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks\\nacross language models. Our findings indicate that LLM-integrated applications\\nbased on Langchain are highly susceptible to P$_2$SQL injection attacks,\\nwarranting the adoption of robust defenses. To counter these attacks, we\\npropose four effective defense techniques that can be integrated as extensions\\nto the Langchain framework. We validate the defenses through an experimental\\nevaluation with a real-world use case application.', 'date': '2023-08-03T19:03:18+00:00', 'label': 1}, '2304.03277': {'title': 'Instruction Tuning with GPT-4', 'abstract': 'Prior work has shown that finetuning large language models (LLMs) using\\nmachine-generated instruction-following data enables such models to achieve\\nremarkable zero-shot capabilities on new tasks, and no human-written\\ninstructions are needed. In this paper, we present the first attempt to use\\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\\nexperiments on instruction-tuned LLaMA models show that the 52K English and\\nChinese instruction-following data generated by GPT-4 leads to superior\\nzero-shot performance on new tasks to the instruction-following data generated\\nby previous state-of-the-art models. We also collect feedback and comparison\\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\\nWe make our data generated using GPT-4 as well as our codebase publicly\\navailable.', 'date': '2023-04-06T17:58:09+00:00', 'label': 1}, '2211.09527': {'title': 'Ignore Previous Prompt: Attack Techniques For Language Models', 'abstract': \"Transformer-based large language models (LLMs) provide a powerful foundation\\nfor natural language tasks in large-scale customer-facing applications.\\nHowever, studies that explore their vulnerabilities emerging from malicious\\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\\nframework for mask-based iterative adversarial prompt composition, we examine\\nhow GPT-3, the most widely deployed language model in production, can be easily\\nmisaligned by simple handcrafted inputs. In particular, we investigate two\\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\\nis available at https://github.com/agencyenterprise/PromptInject.\", 'date': '2022-11-17T13:43:20+00:00', 'label': 1}, '2302.06476': {'title': 'Is ChatGPT a General-Purpose Natural Language Processing Task Solver?', 'abstract': 'Spurred by advancements in scale, large language models (LLMs) have\\ndemonstrated the ability to perform a variety of natural language processing\\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\\nthe debut of ChatGPT has drawn a great deal of attention from the natural\\nlanguage processing (NLP) community due to the fact that it can generate\\nhigh-quality responses to human input and self-correct previous mistakes based\\non subsequent conversations. However, it is not yet known whether ChatGPT can\\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\\nevaluating it on 20 popular NLP datasets covering 7 representative task\\ncategories. With extensive empirical studies, we demonstrate both the\\neffectiveness and limitations of the current version of ChatGPT. We find that\\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\\narithmetic reasoning) while it still faces challenges when solving specific\\ntasks such as sequence tagging. We additionally provide in-depth analysis\\nthrough qualitative case studies.', 'date': '2023-02-08T09:44:51+00:00', 'label': 1}, '2304.08354': {'title': 'Tool Learning with Foundation Models', 'abstract': 'Humans possess an extraordinary ability to create and utilize tools, allowing\\nthem to overcome physical limitations and explore new frontiers. With the\\nadvent of foundation models, AI systems have the potential to be equally adept\\nin tool use as humans. This paradigm, i.e., tool learning with foundation\\nmodels, combines the strengths of specialized tools and foundation models to\\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\\nDespite its immense potential, there is still a lack of a comprehensive\\nunderstanding of key challenges, opportunities, and future endeavors in this\\nfield. To this end, we present a systematic investigation of tool learning in\\nthis paper. We first introduce the background of tool learning, including its\\ncognitive origins, the paradigm shift of foundation models, and the\\ncomplementary roles of tools and models. Then we recapitulate existing tool\\nlearning research into tool-augmented and tool-oriented learning. We formulate\\na general tool learning framework: starting from understanding the user\\ninstruction, models should learn to decompose a complex task into several\\nsubtasks, dynamically adjust their plan through reasoning, and effectively\\nconquer each sub-task by selecting appropriate tools. We also discuss how to\\ntrain models for improved tool-use capabilities and facilitate the\\ngeneralization in tool learning. Considering the lack of a systematic tool\\nlearning evaluation in prior works, we experiment with 18 representative tools\\nand show the potential of current foundation models in skillfully utilizing\\ntools. Finally, we discuss several open problems that require further\\ninvestigation for tool learning. In general, we hope this paper could inspire\\nfuture research in integrating tools with foundation models.', 'date': '2023-04-17T15:16:10+00:00', 'label': 1}, '2302.13971': {'title': 'LLaMA: Open and Efficient Foundation Language Models', 'abstract': 'We introduce LLaMA, a collection of foundation language models ranging from\\n7B to 65B parameters. We train our models on trillions of tokens, and show that\\nit is possible to train state-of-the-art models using publicly available\\ndatasets exclusively, without resorting to proprietary and inaccessible\\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\\nPaLM-540B. We release all our models to the research community.', 'date': '2023-02-27T17:11:15+00:00', 'label': 1}, '2307.09288': {'title': 'Llama 2: Open Foundation and Fine-Tuned Chat Models', 'abstract': 'In this work, we develop and release Llama 2, a collection of pretrained and\\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\\ndialogue use cases. Our models outperform open-source chat models on most\\nbenchmarks we tested, and based on our human evaluations for helpfulness and\\nsafety, may be a suitable substitute for closed-source models. We provide a\\ndetailed description of our approach to fine-tuning and safety improvements of\\nLlama 2-Chat in order to enable the community to build on our work and\\ncontribute to the responsible development of LLMs.', 'date': '2023-07-18T14:31:57+00:00', 'label': 1}, '2302.12095': {'title': 'On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective', 'abstract': 'ChatGPT is a recent chatbot service released by OpenAI and is receiving\\nincreasing attention over the past few months. While evaluations of various\\naspects of ChatGPT have been done, its robustness, i.e., the performance to\\nunexpected inputs, is still unclear to the public. Robustness is of particular\\nconcern in responsible AI, especially for safety-critical applications. In this\\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\\nseveral popular foundation models as baselines. Results show that ChatGPT shows\\nconsistent advantages on most adversarial and OOD classification and\\ntranslation tasks. However, the absolute performance is far from perfection,\\nwhich suggests that adversarial and OOD robustness remains a significant threat\\nto foundation models. Moreover, ChatGPT shows astounding performance in\\nunderstanding dialogue-related texts and we find that it tends to provide\\ninformal suggestions for medical tasks instead of definitive answers. Finally,\\nwe present in-depth discussions of possible research directions.', 'date': '2023-02-22T11:01:20+00:00', 'label': 1}, '2212.10560': {'title': 'Self-Instruct: Aligning Language Models with Self-Generated Instructions', 'abstract': 'Large \"instruction-tuned\" language models (i.e., finetuned to respond to\\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\\nthat is often limited in quantity, diversity, and creativity, therefore\\nhindering the generality of the tuned model. We introduce Self-Instruct, a\\nframework for improving the instruction-following capabilities of pretrained\\nlanguage models by bootstrapping off their own generations. Our pipeline\\ngenerates instructions, input, and output samples from a language model, then\\nfilters invalid or similar ones before using them to finetune the original\\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\\nimprovement over the original model on Super-NaturalInstructions, on par with\\nthe performance of InstructGPT-001, which was trained with private user data\\nand human annotations. For further evaluation, we curate a set of\\nexpert-written instructions for novel tasks, and show through human evaluation\\nthat tuning GPT3 with Self-Instruct outperforms using existing public\\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\\naligning pre-trained language models with instructions, and we release our\\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\\ncode and data are available at https://github.com/yizhongw/self-instruct.', 'date': '2022-12-20T18:59:19+00:00', 'label': 1}, '2307.02483': {'title': 'Jailbroken: How Does LLM Safety Training Fail?', 'abstract': 'Large language models trained for safety and harmlessness remain susceptible\\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\\nrecognition of the issue, we investigate why such attacks succeed and how they\\ncan be created. We hypothesize two failure modes of safety training: competing\\nobjectives and mismatched generalization. Competing objectives arise when a\\nmodel\\'s capabilities and safety goals conflict, while mismatched generalization\\noccurs when safety training fails to generalize to a domain for which\\ncapabilities exist. We use these failure modes to guide jailbreak design and\\nthen evaluate state-of-the-art models, including OpenAI\\'s GPT-4 and Anthropic\\'s\\nClaude v1.3, against both existing and newly designed attacks. We find that\\nvulnerabilities persist despite the extensive red-teaming and safety-training\\nefforts behind these models. Notably, new attacks utilizing our failure modes\\nsucceed on every prompt in a collection of unsafe requests from the models\\'\\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\\nanalysis emphasizes the need for safety-capability parity -- that safety\\nmechanisms should be as sophisticated as the underlying model -- and argues\\nagainst the idea that scaling alone can resolve these safety failure modes.', 'date': '2023-07-05T17:58:10+00:00', 'label': 1}, '2206.07682': {'title': 'Emergent Abilities of Large Language Models', 'abstract': 'Scaling up language models has been shown to predictably improve performance\\nand sample efficiency on a wide range of downstream tasks. This paper instead\\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\\nlarge language models. We consider an ability to be emergent if it is not\\npresent in smaller models but is present in larger models. Thus, emergent\\nabilities cannot be predicted simply by extrapolating the performance of\\nsmaller models. The existence of such emergence implies that additional scaling\\ncould further expand the range of capabilities of language models.', 'date': '2022-06-15T17:32:01+00:00', 'label': 1}}\n",
      "2311.12785\n"
     ]
    }
   ],
   "source": [
    "#print(data['2305.10403']['date'])\n",
    "#print(data['2308.07201']['date'])\n",
    "#data['2305.10403']['date']<data['2308.07201']['date']\n",
    "print(data)\n",
    "print(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These data will also be helpful in task3. Lets store them in task3/dataset\n",
    "def write_to_json(json_data,file_name):\n",
    "    with open(file_name+'.json', 'w') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "write_to_json(data,'task3/dataset/'+paper_id+'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models\n"
     ]
    }
   ],
   "source": [
    "#Lets not forget that the original paper should also be stored\n",
    "original_paper = {'id':paper_id,'title':paper.title,'abstract':paper.summary,'date':paper.published.isoformat(),'fulltext':full_text,'ref':numbers}\n",
    "print(original_paper['title'])\n",
    "write_to_json(original_paper,'task3/dataset/'+paper_id+'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract Stobjs and Their Application to ISA Modeling\n",
      "Computer Science\n",
      "Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive Case\n",
      "Simulating Grover's Quantum Search in a Classical Computer\n",
      "Sequentiality vs. Concurrency in Games and Logic\n",
      "Decidability of higher-order matching\n",
      "Crypto Makes AI Evolve\n",
      "Typed Operational Semantics for Dependent Record Types\n",
      "Advances in Artificial Intelligence Require Progress Across all of Computer Science\n",
      "Artificiality in Social Sciences\n",
      "dict_keys(['2303.09540', '2303.08774', '2402.16827', '2401.13229', '2405.20541', '2407.14985', '2406.09334', '2402.17327', '2405.12186', '2309.16609', '2203.14544', '1512.02985', '2006.14651', '2401.06692', '2312.06254', '2310.13032', '2311.14736', '2403.16898', '1702.05962', '2307.06290', '2308.07201', '2105.10446', '2305.09246', '2403.12776', '2307.08701', '2406.14491', '2405.20456', '2311.09783', '2109.06379', '1810.04805', '2305.14233', '2002.06305', '2406.13542', '2304.06767', '2311.15653', '1903.09722', '2401.12926', '2306.11670', '2007.01852', '2105.03075', '1806.03884', '2402.05119', '2308.03296', '2306.11644', '2303.08114', '2203.15556', '2403.02839', '2403.08763', '2202.00622', '2210.14177'])\n"
     ]
    }
   ],
   "source": [
    "def add_unrelated(related_data,cut_date,query,num):\n",
    "    all_data = {}\n",
    "    client = arxiv.Client()\n",
    "    search = arxiv.Search(\n",
    "    query = query,\n",
    "    max_results = num+50,\n",
    "    # sort_by=arxiv.SortCriterion.SubmittedDate\n",
    "    # sort_by = arxiv.SortCriterion.Relevance #Is default\n",
    "\n",
    "    )\n",
    "\n",
    "    results = list(client.results(search))\n",
    "    random.shuffle(results)\n",
    "    # Lists to hold all IDs and their labels\n",
    "    all_ids = []\n",
    "    i = 0\n",
    "    while len(all_ids)<num:\n",
    "        result = results[i]\n",
    "        arxiv_number = result.entry_id.split('/')[-1]\n",
    "        arxiv_number = arxiv_number.split('v')[0]\n",
    "        submission_date = result.published.isoformat()\n",
    "        #check if already in list of sources (hence relevant) and before the overview\n",
    "        if (arxiv_number not in related_data.keys()) and (submission_date<cut_date):\n",
    "            all_ids.append(arxiv_number)\n",
    "            all_data[arxiv_number] = {'title':result.title,'abstract':result.summary,'date':result.published.isoformat(),'label':0}  #No match/ label  0\n",
    "        #print(submission_date)\n",
    "        print(result.title)\n",
    "        i+=1\n",
    "    print(related_data.keys())\n",
    "    selected_numbers = random.sample(list(related_data.keys()), num)\n",
    "\n",
    "    # Add matching IDs to the list\n",
    "    for matching_id in selected_numbers:\n",
    "        all_data[matching_id] = related_data[matching_id]\n",
    "\n",
    "\n",
    "    keys = list(all_data.keys())\n",
    "    random.shuffle(keys)\n",
    "\n",
    "    # Create a new dictionary with shuffled keys\n",
    "    shuffled_dict = {key: all_data[key] for key in keys}\n",
    "    return shuffled_dict\n",
    "\n",
    "sample_data = add_unrelated(data,original_paper['date'],'computer science',min(10,len(data.keys())))\n",
    "#sample_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2405.16771', '2403.01121']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.sample(list(data.keys()), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_to_json(sample_data,'task2/dataset/'+paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAIR and Open Computer Science Research Software\n",
      "Computer Science for Future -- Sustainability and Climate Protection in the Computer Science Courses of the HAW Hamburg\n",
      "Data science and Machine learning in the Clouds: A Perspective for the Future\n",
      "Computer simulations in science and engineering - Concepts - Practices - Perspectives\n",
      "Why The Trans Programmer?\n",
      "Generative AI has lowered the barriers to computational social sciences\n",
      "A Polynomial Translation of pi-calculus FCPs to Safe Petri Nets\n",
      "A Cognitive Science perspective for learning how to design meaningful user experiences and human-centered technology\n",
      "Computational Skills by Stealth in Secondary School Data Science\n",
      "The Difficulties of Addressing Interdisciplinary Challenges at the Foundations of Data Science\n",
      "Saying Hello World with MOLA - A Solution to the TTC 2011 Instructive Case\n",
      "Proceedings of the 7th European Conference on Python in Science (EuroSciPy 2014)\n",
      "Materials science and engineering: New vision in the era of artificial intelligence\n",
      "The Halting Paradox\n",
      "Science of Cyber Security as a System of Models and Problems\n",
      "Real-number Computability from the Perspective of Computer Assisted Proofs in Analysis\n",
      "Data Science: A Comprehensive Overview\n",
      "The Chemistry Between High School Students and Computer Science\n",
      "ABET Accreditation: A Way Forward for PDC Education\n",
      "MATILDA: Inclusive Data Science Pipelines Design through Computational Creativity\n",
      "Logic Column 18: Alternative Logics: A Book Review\n",
      "Dynamics of Gender Bias within Computer Science\n",
      "Sequentiality vs. Concurrency in Games and Logic\n",
      "Complex Workflow Management and Integration of Distributed Computing Resources by Science Gateway Portal for Molecular Dynamics Simulations in Materials Science\n",
      "Linked Environment Data for the Life Sciences\n",
      "Science Learning via Participation in Online Citizen Science\n",
      "Accessible Computer Science for K-12 Students with Hearing Impairments\n",
      "Biological Computing Fundamentals and Futures\n",
      "Abstract Stobjs and Their Application to ISA Modeling\n",
      "Simulating Grover's Quantum Search in a Classical Computer\n",
      "Integrating computing in the statistics and data science curriculum: Creative structures, novel skills and habits, and ways to teach computational thinking\n",
      "Cloudifying the Curriculum with AWS\n",
      "Addressing Hate Speech with Data Science: An Overview from Computer Science Perspective\n",
      "Oxford-style Debates in Telecommunication and Computer Science Education\n",
      "Computer sciences and synthesis: retrospective and perspective\n",
      "dict_keys(['2310.06261', '2402.12761', '2401.13210', '2405.17525', '2310.02861', '2103.09430', '2210.12941', '2308.10918', '1609.02907', '2310.11829', '2403.09039', '2312.17679', '2405.16771', '2308.13821', '2308.14181', '2212.05478', '2305.02496', '2310.16376', '2310.11676', '2402.11887', '2205.13845', '2402.16024', '2306.12251', '2305.13573', '1710.10903', '1809.10341', '2311.06835', '2403.01121', '2311.10370', '2312.06441', '1710.09412', '2205.04816', '2403.10339', '2302.06430', '2202.07082'])\n"
     ]
    }
   ],
   "source": [
    "sample_data = add_unrelated(data,original_paper['date'],'computer science',min(len(data.keys()),100))\n",
    "write_to_json(sample_data,'task2/dataset/'+paper_id+'all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with full text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(id):\n",
    "    #Find paper in arxiv\n",
    "    paper = next(arxiv.Client().results(arxiv.Search(id_list=[id])))\n",
    "    # Download the PDF\n",
    "    directory_path = './task3/'+paper_id  \n",
    "    os.makedirs(directory_path, exist_ok=True)\n",
    "    pdf_path = paper.download_pdf(dirpath=directory_path)\n",
    "    # Open the downloaded PDF\n",
    "    document = fitz.open(pdf_path)\n",
    "\n",
    "    # Extract text from each page\n",
    "    text = \"\"\n",
    "    for page_num in range(len(document)):\n",
    "        page = document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2112.01944\n",
      "2006.04466\n",
      "2103.06124\n",
      "2006.05623\n",
      "1911.02079\n",
      "2010.11305\n",
      "2006.14827\n"
     ]
    }
   ],
   "source": [
    "#Attention: takes long and a lot of space, pdfs get stored can be used for chroma though.\n",
    "def create_dict_full_text(numbers,file_name):\n",
    "    json_data = {}\n",
    "    for number in numbers:\n",
    "        text = get_full_text(number)\n",
    "        json_data[number] = text\n",
    "        print(number)\n",
    "    with open(file_name+'.json', 'w') as file:\n",
    "        json.dump(json_data, file, indent=4)\n",
    "    return json_data\n",
    "    \n",
    "data = create_dict_full_text(numbers,'./task3/dataset/'+paper_id+'full_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2112.01944': 'arXiv:2112.01944v1  [cs.IR]  3 Dec 2021\\nTowards Low-loss 1-bit Qantization of User-item\\nRepresentations for Top-K Recommendation\\nYankai Chen1, Yifei Zhang1, Yingxue Zhang2, Huifeng Guo3,\\nJingjie Li3, Ruiming Tang3, Xiuqiang He3, Irwin King1\\n1The Chinese University of Hong Kong, 2Huawei Noah’s Ark Lab Canada, 3Huawei Noah’s Ark Lab\\n{ykchen,yfzhang,king}@cse.cuhk.edu.hk,{yingxue.zhang,huifeng.guo,lijingjie1,tangruiming,hexiuqiang1}@huawei.com\\nAbstract\\nDue to the promising advantages in space compression and in-\\nference acceleration, quantized representation learning for recom-\\nmender systems has become an emerging research direction re-\\ncently. As the target is to embed latent features in the discrete em-\\nbedding space, developing quantization for user-item representa-\\ntions with a few low-precision integers confronts the challenge of\\nhigh information loss, thus leading to unsatisfactory performance\\nin Top-K recommendation. In this work, we study the problem\\nof representation learning for recommendation with 1-bit quan-\\ntization. We propose a model named Low-loss Quantized Graph\\nConvolutional Network (L2Q-GCN). Diﬀerent from previous work\\nthat plugs quantization as the ﬁnal encoder of user-item embed-\\ndings, L2Q-GCN learns the quantized representations whilst cap-\\nturing the structural information of user-item interaction graphs\\nat diﬀerent semantic levels. This achieves the substantial reten-\\ntion of intermediate interactive information, alleviating the feature\\nsmoothing issue for ranking caused by numerical quantization. To\\nfurther improve the model performance, we also present an ad-\\nvanced solution named L2Q-GCN푎푛푙with quantization approxima-\\ntion and annealing training strategy. We conduct extensive experi-\\nments on four benchmarks over Top-K recommendation task. The\\nexperimental results show that, with nearly 9× representation stor-\\nage compression, L2Q-GCN푎푛푙attains about 90∼99% performance\\nrecovery compared to the state-of-the-art model.\\n1\\nIntroduction\\nRecommender systems (RSs), as a useful tool to perform per-\\nsonalized information ﬁltering [12, 17], nowadays play a critical\\nrole throughout various Web applications, e.g., social networks, E-\\ncommerce platforms, and streaming media websites. Learning vec-\\ntorized user-item representations (a.k.a. embeddings) for predic-\\ntion has become the core of modern recommender systems [10, 18,\\n19]. Among existing techniques, graph-based methods, i.e., Graph\\nConvolutional Networks (GCNs), due to the ability of capturing high-\\norder relations in user-item interaction topology, well simulate the\\ncollaborative ﬁltering process and thus produce a remarkable se-\\nmantic enrichment to the user-item representations [17, 18, 38, 43].\\nApart from the representation informativeness, space overhead\\nis another important criterion for realistic recommender systems.\\nWith the explosive growth of interactive data encoded in the graph\\nform, quantized representation learning recently provides an alter-\\nnative option to GCN-based recommender methods for optimizing\\nthe model scalability. Generally, quantization is the process of con-\\nverting a continuous range of vectorized values into a ﬁnite dis-\\ncrete set, e.g., integers. Instead of using continuous embeddings,\\n…\\nMulti-layer CNN\\nQuantization\\nUsers\\nItems\\nu1\\nu2\\nu3\\ni3\\ni2\\ni1\\nu2\\ni3\\ni2\\ni1\\nu1\\nu3\\nu3\\n1-hop\\n2-hop\\nInteraction graph\\nHigh-order connectivity\\n(a) Conventional quantization-based CNN.\\n(b) Semantic propagation.\\nFigure 1: Illustration of conventional quantized CNN and se-\\nmantic propagation within the user-item interaction graph.\\ne.g., 32-bit ﬂoating points, 1-bit quantization however embeds user-\\nitem latent features into the binary embedding space, e.g., {−1, 1}푑.\\nBy enabling the usage of low-precision integer arithmetics, 1-bit\\nquantized representations have the promising potential in space\\ncompression and inference acceleration for recommendation [39].\\nDespite the previous attempts to quantize traditional model-based\\nRS methods [24, 48, 49], quantizing graph-based RS models [40]\\nhowever receive less attention so far. Due to their unique mes-\\nsage passing mechanism [17, 26] in encoding high-order interac-\\ntive information, it is emerging as a good research topic to study\\nrepresentation quantization for GCN-based recommender models.\\nCurrently, it is still challenging to approach this target as previous\\nwork falls short of satisfaction in terms of recommendation accu-\\nracy. The crux of this phenomenon is mainly twofold:\\n• Intuitively, the performance degradation for quantizing repre-\\nsentations is mainly caused by the limited expressivity of dis-\\ncrete embeddings. Diﬀerent from applications in other domains,\\ne.g., Natural Language Processing, Computer Vision [4, 6, 15],\\nthe top principle of quantization for recommendation is ranking\\npreserving. However, compared to full-precision embeddings, the\\nvectorized latent features of both users and items tend to be\\nsmoothed by the discreteness of quantization naturally. For in-\\nstance, after the quantization into binary embedding space {−1, 1}푑,\\nonly the digit signs are kept, no matter what speciﬁc values of\\ncontinuous embeddings originally are. Consequently, this leads\\nto the information loss when estimating users’ preferences to-\\nwards diﬀerent items, thus drawing a conspicuous performance\\ndecay in ranking tasks, e.g., Top-K recommendation.\\n• Technically, previous work mainly takes inspiration from the\\nmethodology of Quantized Convolutional Neural Networks [31,\\n34, 35] (illustrated in Figure 1(a)) to plug quantization as a sepa-\\nrate encoder that is posterior to the GCN architecture. Being the\\nﬁnal encoder for object embeddings (i.e., users and items), this\\nhowever ignores the intermediate representations when graph\\nconvolution performs on the sub-structures of interaction graphs.\\nAs pointed out by [43], the intermediate information at diﬀer-\\nent layers of graph convolution is important to reveal diﬀerent\\nsemantics of user-item interactions. For example, as shown in\\nFigure 1(b), when lower-layers propagate information between\\nusers and items that have historical interactions, higher-layers\\ncapture higher-order proximity of users (or items). Hence, ig-\\nnoring intermediate semantics fails to the feature enrichment\\nfor embedding quantization. Furthermore, due to the “double-\\nedged” eﬀect of GCN, ﬁnal embeddings at the last graph convo-\\nlution layer may be over-smoothed [28, 29] to become uninfor-\\nmative accordingly. This implies that simply using the output\\nembeddings may be risky and problematic [18], which leads to\\nthe suboptimal quantized representations for recommendation.\\nIn this paper, we investigate the problem of 1-bit quantized rep-\\nresentation learning for recommendation with the GCN framework.\\nWe proposea model named Low-loss 1-bit QuantizedGraph Convolutional\\nNetwork (L2Q-GCN). L2Q-GCN interprets the user-item represen-\\ntations in the discrete space {−1, 1}푑. We design the quantization-\\nbased graph convolution such that L2Q-GCN achieves the embed-\\nding quantization whilst capturing diﬀerent levels of interactive se-\\nmantics in exploring the user-item interaction graphs. Intuitively,\\nsuch topology-aware quantization makes the user-item represen-\\ntations more comprehensive, and thus signiﬁcantly alleviates the\\ninformation loss of numerical quantization that causes the perfor-\\nmance decay. Speciﬁcally, we propose two solutions, namely L2Q-\\nGCN푒푛푑and L2Q-GCN푎푛푙, to provide ﬂexibility towards diﬀerent\\ndeployment scenarios. We conduct extensive experiments on four\\nbenchmarks over Top-K recommendation task. To summarize, our\\nmain contributions are as follows:\\n(1) We implement our proposed network design in L2Q-GCN푒푛푑\\nthat is trained in an end-to-end manner. Our experimental re-\\nsults demonstrate that, L2Q-GCN푒푛푑achieves nearly 11× rep-\\nresentation compression and about 40% inference acceleration,\\nwhile retaining over 80% recommendation capacity, compared\\nto the state-of-the-art full-precision model.\\n(2) To further improve the recommendation performance, we pro-\\npose an advanced solution L2Q-GCN푎푛푙with approximation\\nthat is trained by a two-step annealing training strategy. With\\nslightly additional space cost, L2Q-GCN푎푛푙can attain 90∼99%\\nperformance recovery.\\n(3) We release codes and datasets to researchers via the link [1] for\\nreproducing and validating.\\nOrganization. We ﬁrst review the related work in Section 2\\nand present L2Q-GCN푒푛푑and L2Q-GCN푎푛푙in Sections 3 and 4. We\\nreport the experimental results on four benchmarks in Section 5\\nand conclude the paper in Section 6.\\n2\\nRelated Work\\n2.1\\nFull-precision GCN-based RS Models\\nRecently, GCN-based recommender systems have become new\\nstate-of-the-art methods for Top-K recommendation [18, 43], thanks\\nto their capability of capturing semantic relations and topological\\nstructures for user-item interactions [17, 26, 50]. Motivated by the\\nadvantage of graph convolution, prior work such as GC-MC [7],\\nPinSage [47] and recent state-of-the-art models NGCF [43] and\\nLightGCN [18] are proposed. Generally, they adapt the GCN frame-\\nwork to simulate the collaborative ﬁltering process in high-order\\ngraph neighbors for recommendation. For example, NGCF [43] fol-\\nlows the GCN-based information propagation rule to learn em-\\nbeddings: feature transformation, neighborhood aggregation, and\\nnonlinear activation. LightGCN [18] further simpliﬁes the graph\\nconvolution by retaining the most essential GCN components to\\nachieve further improved recommendation performance. In our ex-\\nperiments, we settle these two state-of-the-art full-precision meth-\\nods as the benchmarking reference for L2Q-GCN in Top-K recom-\\nmendation.\\n2.2\\nGeneral Binarized GCN Frameworks\\nNetwork binarization [22] aims to binarize all parameters and\\nactivations in neural networks so that they can even be trained by\\nlogical units of CPUs. Binarized models will dramatically reduce\\nthe memory usage. Despite the progress of binarization CNNs [34,\\n35] for multimedia retrieval, this technique is not adequately stud-\\nied in geometric deep learning [3, 41, 45]. Bi-GCN [41] and BGCN [3]\\nare two recent trials. However, they are mainly designed for geo-\\nmetric classiﬁcation tasks, but their capability of link prediction (a\\ngeometric form of recommendation) is unclear. Furthermore, com-\\npared to focusing on the quantization for user-item representa-\\ntions only, a complete network binarization will further abate the\\nnumerical expressivity of modeling ranking information for rec-\\nommendation. This implies that a direct adaptation of binary GCN\\nmodels may draw large performance decay in Top-K recommenda-\\ntion.\\n2.3\\nQuantization-based RS Models\\nQuantization-based recommender models are attracting grow-\\ning attention recently [24, 37, 40, 46]. Compared to network bina-\\nrization, they do not pursue extreme model compression, but focus\\non quantization for user-item representations with a few integers.\\nThese models can be generally categorized into model-based[24,\\n46, 48, 49] and graph-based [40]. HashGNN [40], as the state-of-\\nthe-art graph-based solution, takes both advantages of graph con-\\nvolution and embedding quantization. Speciﬁcally, HashGNN [40]\\ndesigns a two-step quantization framework by combining Graph-\\nSage [17] and learn to hash methodology [13, 16, 42, 52]. Generally,\\nLearn to hash aims to learn hash functions for generating discrim-\\ninative codes. HashGNN ﬁrst invokes the two-layer GraphSage as\\nthe encoder to get the embeddings for users and items; then it\\nstacks a hash layer to get the corresponding binary encodings af-\\nterwards. However, the main inadequacy of HashGNN is that, the\\nquantization process only proceeds at the end of multi-layer graph\\nconvolution, i.e., using the aggregated output of two-layer Graph-\\nSage for representation binarization. While multi-layer convolu-\\ntion helps to aggregate local information that lives in the consecu-\\ntive graph hops, HashGNN may thus not be able to capture inter-\\nmediate semantics from nodes’ diﬀerent layers of receptive ﬁelds,\\nproducing a suboptimal quantization of node embeddings. Com-\\npared to HashGNN, our proposed L2Q-GCN model conducts the\\nquantization-based graph convolution when exploring the user-\\nitem interaction graph in a layer-wise manner. We justify the ef-\\nfectiveness in Section 5.\\nUser-item interaction graph\\nu1\\nu2\\nu3\\nu4\\ni4\\ni3\\ni2\\ni1\\nl = 1\\nl = 2\\nl = 3\\nv(0)\\nu1\\nv(1)\\nu1\\nv(2)\\nu1\\nv(3)\\nu1\\nu1’s neighbors\\nNormalized \\nSum\\ni4\\nv(0)\\ni1\\nv(0)\\ni4\\ni1\\nl = 1\\nl = 2\\nl = 3\\nNormalized \\nSum\\ni1’s neighbors\\nu1\\nu3\\nv(0)\\nu1\\nv(0)\\nu3\\nu2\\nv(0)\\nu2\\nv(0)\\ni1\\nv(1)\\ni1\\nv(2)\\ni1\\nv(3)\\ni1\\nq(0)\\nu1\\nq(1)\\nu1\\nq(2)\\nu1\\nq(3)\\nu1\\nq(3)\\ni1\\nGradient propagation\\nˆyu,i\\nUsers\\nItems\\nGraph convolution of continuous item embeddings\\nGraph convolution of continuous user embeddings\\nLatent feature transformation\\n1-bit quantized item representations\\n1-bit quantized user representations\\nFigure 2: Illustration of L2Q-GCN framwork (Best view in color).\\n3\\nL2Q-GCN Methodology\\n3.1\\nProblem Formulation\\nUser-item interactions can be represented by a bipartite graph,\\ni.e., G = {(푢,푖)|푢∈U, 푖∈I}. U and I denote the sets of users and\\nitems. We denote 푦푢,푖= 1 to indicate there is an observed interac-\\ntion between 푢and 푖, e,g., browse, click, or purchase, otherwise 푦푢,푖\\n= 0.\\nNotations. We use bold lowercase, bold uppercase, and callig-\\nraphy characters to denote vectors, matrices, and sets, respectively.\\nNon-bold characters are used to denote graph nodes or scalars. Due\\nto the page limit, we summarize all key notations in Appendix A.\\nTask Description. Given an interaction graph, the problem\\nstudied in this paper is to learn quantized representations Q푢and\\nQ푖for user 푢and item 푖, such that the online recommender system\\nmodel can predict the probability ˆ푦푢,푖that user 푢may adopt item\\n푖.\\n3.2\\nBasic Implementation: L2Q-GCN푒푛푑\\nThe general idea of L2Q-GCN is to learn node representations\\nby propagating latent features via the graph topology [18, 26, 44].\\nIt performs iterative graph convolution, i.e., propagating and aggre-\\ngating information of neighbors to update representations (embed-\\ndings) of target nodes, which can be formulated as follows:\\n풗(푙)\\n푥\\n= 퐴퐺퐺\\n\\x10\\n{풗(푙−1)\\n푦\\n: 푦∈N (푥)}\\n\\x11\\n,\\n(1)\\nwhere 풗(푙)\\n푥\\ndenotes node 푥’s embedding after 푙layers of informa-\\ntion propagation. N (푥) represents 푥’s neighbor set and 퐴퐺퐺is the\\naggregation function aiming to transform the center node feature\\nand the neighbor features. We illustrate the framework in Figure 2.\\n3.2.1\\nQantization-based Graph Convolution. We adopt the\\ngraph convolution paradigm working on the continuous space from [18]\\nthat recently shows good performance under recommendation sce-\\nnarios. Let 풗(푙)\\n푢\\n∈R푐and 풗(푙)\\n푖\\n∈R푐denote the continuous fea-\\nture embeddings of user 푢and item 푖computed in the 푙-th layer.\\nThey can be respectively updated by utilizing information from\\nthe (푙−1)-th layer in iteration as follows:\\n풗(푙)\\n푢\\n=\\nÕ\\n푖∈N(푢)\\n1\\np\\n|N (푢)| · |N (푖)|\\n풗(푙−1)\\n푖\\n, 풗(푙)\\n푖\\n=\\nÕ\\n푢∈N(푖)\\n1\\np\\n|N (푖)| · |N (푢)|\\n풗(푙−1)\\n푢\\n.\\n(2)\\nAfter getting the intermediate embeddings, e.g., 풗(푙)\\n푢, we conduct\\n1-bit quantization as:\\n풒(푙)\\n푢\\n= sign \\x00푾푇풗(푙)\\n푢\\n\\x01, 풒(푙)\\n푖\\n= sign \\x00푾푇풗(푙)\\n푖\\n\\x01,\\n(3)\\nwhere 푾∈푅푐×푑is a matrix that transforms 풗(푙)\\n푢\\nto 푑-dimensional\\nlatent space for 1-bit quantization. Function sign(·) maps ﬂoating-\\npoint inputs into the discrete binary space, e.g., {−1, 1}푑. By do-\\ning so, we can obtain the quantized embedding 풒(푙)\\n푢\\nwhilst retain-\\ning the latent user features directly from 풗(푙)\\n푢. For ease of L2Q-\\nGCN푒푛푑’s binarization storage, we can further conduct a numer-\\nical translation to these quantized embeddings from {−1, 1}푑to\\n{0, 1}푑. After 퐿layers of feature propagation and quantization, we\\nhave built the targeted quantized representations Q푢and Q푖as:\\nQ푢= {풒(0)\\n푢, 풒(1)\\n푢, · · · , 풒(퐿)\\n푢\\n}, Q푖= {풒(0)\\n푖\\n, 풒(1)\\n푖\\n, · · · , 풒(퐿)\\n푖\\n}.\\n(4)\\nBoth Q푢and Q푖track the intermediate information binarized from\\nfull-precision embeddings at diﬀerent layers. Intuitively, they rep-\\nresent the interactive information that is propagated back and forth\\nbetween users and items within these layers, simulating the col-\\nlaborative ﬁltering eﬀect to exhibit in the quantized encodings for\\nrecommendation.\\n3.2.2\\nModel Prediction. Based on the quantized representations\\nof users and items, i.e., Q푢and Q푖, we predict the matching scores\\nby naturally adopting the inner product as:\\nˆ푦푢,푖= 푓(Q푢)푇· 푓(Q푖),\\n(5)\\nwhere function 푓abstracts the way to utilize Q푢and Q푖. In this\\npaper, we implement 푓by taking an element-wise summation:\\n푓(Q푢) =\\n퐿\\nÕ\\n푙=0\\n풒(푙)\\n푢, 푓(Q푖) =\\n퐿\\nÕ\\n푙=0\\n풒(푙)\\n푖\\n.\\n(6)\\nClariﬁcation. In this work, we interpret quantized representations as\\nintegers and apply integer arithmetics in computation. As we will\\nshow later in experiments, this introduces about 40% of computation\\nacceleration. Certainly, it can be extended to binary arithmetic logics\\nfor further optimization, and we leave it for future work.\\n3.2.3\\nModel Optimization. We then introduce ourobjective and\\nbackward-propagation strategy for optimization.\\nObjective Function. Our objective function consists of two\\ncomponents, i.e., graph reconstruction loss L푟푒푐and BPR loss L푏푝푟.\\nThe motivation of such design is basically twofold:\\n• L푟푒푐reconstructs the observed topology of interaction graphs;\\n• L푏푝푟learns the relative rankings of user preferences towards\\ndiﬀerent items.\\nConcretely, we implement L푟푒푐with the cross-entropy loss:\\nL푟푒푐=\\nÕ\\n푢∈U\\nÕ\\n푖∈I\\n푦푢,푖ln휎\\n\\x10\\n(풗(퐿)\\n푢\\n)푇· 풗(퐿)\\n푖\\n\\x11\\n+ (1 −푦푢,푖) ln\\n\\x10\\n1 −휎\\x00(풗(퐿)\\n푢\\n)푇· 풗(퐿)\\n푖\\n\\x01\\x11\\n,\\n(7)\\nwhere 휎is the activation function, e.g., Sigmoid. L푟푒푐bases on\\nthe full-precision embeddings at the last layer, e.g., 풗(퐿)\\n푢, providing\\nthe latest intermediate information for topology reconstruction as\\nmuch as possible. As for L푏푝푟, we employ Bayesian Personalized\\nRanking (BPR) loss [36] as follows:\\nL푏푝푟= −\\nÕ\\n푢∈U\\nÕ\\n푖∈N(푢)\\nÕ\\n푗∉N(푢)\\nln휎( ˆ푦푢,푖−ˆ푦푢,푗).\\n(8)\\nL푏푝푟relies on the quantized node representations, e.g., Q푢, to en-\\ncourage the prediction of an observed interaction to be higher than\\nits unobserved counterparts [18]. Finally, our ﬁnal objective func-\\ntion is deﬁned as:\\nL = L푟푒푐+ L푏푝푟+ 휆||Θ||2\\n2,\\n(9)\\nwhere Θ is the set of trainable parameters and embeddings, and\\n||Θ||2\\n2 is the 퐿2-regularizer parameterized by 휆to avoid over-ﬁtting.\\nClariﬁcation. Please notice that at the training stage, we usually take\\nthe mean value of 푓(Q푢) by shrinking it as 푓(Q푢) = 푓(Q푢)/(퐿+ 1)\\n(likewise for 푓(Q푖)). Essentially, this useful training strategy [18, 21]\\nreduces the absolute value of predicted scores to a smaller scale, which\\nsigniﬁcantly stabilizes the training process to avoid the undesirable\\ndivergence in embedding optimization; but most importantly, it has\\nno eﬀect on the relative rankings of all scores.\\nBackward-propagation Strategy.Unfortunately, the sign func-\\ntion is not diﬀerentiable. This means that the original derivative of\\nthe sign function is 0 almost everywhere, making the intermediate\\ngradients accumulated before quantization zeroed here. To avoid\\nthis and approximate the gradients for backward propagation, we\\nadopt the Straight-Through Estimator [5] with gradient clipping as:\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f3\\n풒(푙)\\n푢\\n= sign \\x00휙\\x01,\\nforward propagation\\n휕풒(푙)\\n푢\\n휕휙\\n:= 1|휙|≤1.\\nbackward propagation\\n(10)\\nDerivative 1|휙|≤1 can be viewed as passing gradients e.g., Δ, through\\nhard-tanh function [11], i.e., max(−1, min(1, Δ)). This passes gra-\\ndients backwards unchanged when the input of sign function, e.g.,\\n휙, is within range of {-1, 1}, and cancels the gradient ﬂow other-\\nwise [2]. We illustrate the process of gradient propagation in Fig-\\nure 2.\\nSo far, we have already introduced the skeleton of our proposed\\nnetwork that learns the quantized representations Q푢and Q푖via\\n(a) Full-precision version.\\n(b) Quantization version.\\nFigure 3: Loss landscapes visualization.\\nexploring the interaction graph topology. Since it can be trained in\\nan end-to-end manner, we directly name it as L2Q-GCN푒푛푑.\\n4\\nAdvanced Solution: L2Q-GCN푎푛푙\\nAlthough L2Q-GCN푒푛푑can achieve the quantization for user-\\nitem representations, we argue that there may still exist avenues\\nfor further improvement. In this section, we ﬁrst explain the diﬃ-\\nculty of quantization in L2Q-GCN푒푛푑, and then give an advanced\\nsolutionwith the annealing training strategy, namely L2Q-GCN푎푛푙,\\nto enhance the recommendation performance.\\n4.1\\nDiﬃculty of Quantization in L2Q-GCN푒푛푑\\nTo show it may be challenging to directly quantize L2Q-GCN푒푛푑\\nfor binary representation, we simulate the optimization trajecto-\\nries of learnable embeddings and visually compare the loss land-\\nscapes of it with its full-precision version (excluding the quanti-\\nzation component) in Figure 3. Concretely, following [4, 33], we\\nmanually assign perturbations to the learnable user-item embed-\\ndings as follows:\\n풗(푙)\\n푢\\n= 풗(푙)\\n푢\\n± 푝· |풗(푙)\\n푢| · 1(푙)\\n푢,\\n풗(푙)\\n푖\\n= 풗(푙)\\n푖\\n± 푝· |풗(푙)\\n푢| · 1(푙)\\n푖,\\n(11)\\nwhere |풗(푙)\\n푢| represents the absolute mean value of embedding 풗(푙)\\n푢\\nand perturbation magnitudes 푝are from {0.01, 0.02, · · · , 0.50}. 1푢\\nis an all-one vector. For each pair of perturbed user-item represen-\\ntations, we plot the loss distribution accordingly.\\nAs we can observe, the full-precision version with no quanti-\\nzation produces a ﬂat and smooth loss surface, showing the local\\nconvexity and thus easy to optimize. On the contrary, L2Q-GCN푒푛푑\\nhas a bumping and complex loss landscape. The steep loss curva-\\nture reﬂects L2Q-GCN푒푛푑is more sensitive to perturbation, show-\\ning the diﬃculty in quantization optimization.\\n4.2\\nUpgrading in L2Q-GCN푎푛푙\\nTo alleviate the perturbation sensitivity and further improve the\\nmodel performance, we propose L2Q-GCN푎푛푙.\\n4.2.1\\nQantization with Rescaling Approximation. Our pro-\\nposedL2Q-GCN푎푛푙additionally includes layer-wise positive rescal-\\ning factors for each node, e.g., 훼(푙)\\n푢\\n∈R+, such that 풗(푙)\\n푢\\n≈훼(푙)\\n푢풒(푙)\\n푢.\\nIn this work, we introduce a simple but eﬀective approach to di-\\nrectly calculate these rescaling factors as:\\n훼(푙)\\n푢\\n= ||풗(푙)\\n푢||1\\n푑\\n, 훼(푙)\\n푖\\n=\\n||풗(푙)\\n푖\\n||1\\n푑\\n.\\n(12)\\nInstead of setting 훼(푙)\\n푢\\nas learnable, such deterministic computa-\\ntion substantially prunes the search space of parameters whilst at-\\ntaining the approximation functionality. We demonstrate this in\\nSection 5.6.\\nBased on the quantized user-item representations and the cor-\\nresponding rescaling factors, we have:\\nA푢= {훼(0)\\n푢풒(0)\\n푢, 훼(1)\\n푢풒(1)\\n푢, · · · , 훼(퐿)\\n푢\\n풒(퐿)\\n푢\\n}, A푖= {훼(0)\\n푖\\n풒(0)\\n푖\\n,훼(1)\\n푖\\n풒(1)\\n푖\\n, · · · ,훼(퐿)\\n푖\\n풒(퐿)\\n푖\\n}.\\n(13)\\nConsequently, L2Q-GCN푎푛푙approximates Q푢and Q푖by A푢and\\nA푖, and updates Equations 5 and 6 for model prediction accord-\\ningly.\\nSpace Cost Analysis. The total space cost of L2Q-GCN푒푛푑for\\nstoring the quantized user-item representations is 푂((퐿+ 1)푁푑)\\n(or bits), where 푁is the number of users and items and 푑is the di-\\nmension of binary embeddings, e.g., 풗(푙)\\n푢. Furthermore, since L2Q-\\nGCN푎푛푙develops quantization with approximation, supposing that\\nwe use 32-bit ﬂoating-points for those rescaling factors, the space\\ncost is 푂((퐿+1)푁(푑+32)) in total. Compared to the full-precision\\nembedding table at each single one layer, e.g., 32-bit ﬂoating-point\\n풗(푙)\\n푢, Q푢and Q푖have the following theoretical compression ratios:\\n푟푎푡푖표푒푛푑=\\n32푁푑\\n(퐿+ 1) · 푁푑=\\n32\\n퐿+ 1, 푟푎푡푖표푎푛푙=\\n32푁푑\\n(퐿+ 1) · 푁(푑+ 32) =\\n32푑\\n(퐿+ 1)(푑+ 32) .\\n(14)\\nNormally, stacking too many layers will cause the over-smoothing\\nproblem [28, 29], incurring performance detriment. Hence, A com-\\nmon setting for 퐿is 퐿< 5 [17, 18, 26, 43], which can still achieve\\nconsiderable embedding compression.\\n4.2.2\\nAnnealing Training Strategy. Another constructive de-\\nsign of L2Q-GCN푎푛푙is the two-step annealing training strategy:\\n(1) we ﬁrst mask the quantization function and train L2Q-GCN푎푛푙\\nwith the full-precision embeddings;\\n(2) when it converges to optimum, we trigger the quantization af-\\nterwards to ﬁnd the targeted quantized representations.\\nIntuitively, L2Q-GCN푎푛푙moves from a tractable training space\\nto the targeted one for quantization. This can avoid unnecessary\\nexploration towards diﬀerent optimization directions at the begin-\\nning of quantization, guaranteeing the numerical stability in the\\nwhole model training. Then at the middle period of training when\\ntriggering quantization, as presented in Figure 4, L2Q-GCN푎푛푙ﬁrstly\\nmeets a performance retracement, but shortly afterwards, it recov-\\ners and continues to converge. As we will demonstrate later in ex-\\nperiments, this straightforward but eﬀective strategy can further\\nproduce better quantization representations with approximation\\nfor Top-K recommendation.\\nClariﬁcation. we pointout that the timecost of trainingL2Q-GCN푎푛푙\\nshares the same order of magnitude with L2Q-GCN푒푛푑. In our imple-\\nmentation, we simply assign half of the total training epochs for the\\nﬁrst step and leave the second half for quantization. One may also\\nopt for more ﬂexible strategy to trigger the quantization, e.g., early-\\nstopping, or full-precision version pre-training.\\nRecall@K\\nNDCG@K\\nK=40\\nK=20\\nK=60\\nK=80\\nK=100\\nK=40\\nK=20\\nK=60\\nK=80\\nK=100\\nFigure 4: Annealing performance on MovieLens dataset.\\nSo far, we have introduced all technical details of the proposed\\nL2Q-GCN푒푛푑and L2Q-GCN푎푛푙. For the corresponding pseudocodes,\\nplease refer to Appendix B. In the following section, we present the\\nexperimental results and analysis on our models.\\n5\\nExperimental Results\\nWe evaluate our model on Top-K recommendation task with the\\naim of answering the following research questions:\\n• RQ1. How does L2Q-GCN perform compared to state-of-the-art\\nfull-precision and quantization-based recommender models?\\n• RQ2. How is resource consumption of L2Q-GCN?\\n• RQ3. How do proposed components of L2Q-GCN푒푛푑and L2Q-\\nGCN푎푛푙aﬀect the performance?\\n5.1\\nDataset\\n• MovieLens1 is a widely adopted benchmark for movie recom-\\nmendation. Similar to the setting in [9, 20, 40], 푦푢,푖= 1 if user\\n푢has an explicit rating score towards item 푖, otherwise 푦푢,푖= 0.\\nIn this paper, we use the MovieLens-1M data split.\\n• Gowalla2 is the check-in dataset [30] collected from Gowalla,\\nwhere users share their locations by check-in. To guarantee the\\nquality of the dataset, we extract users and items with no less\\nthan 10 interactions similar to [18, 40, 43].\\n• Pinterest3 is an implicit feedback dataset for image recommen-\\ndation [14]. Users and images are modeled in a graph. Edges rep-\\nresent the pins over images initiated by users. In this dataset,\\neach user has at least 20 edges. We delete repeated edges be-\\ntween users and images to avoid data leakage in model evalua-\\ntion.\\n• Yelp20184 is collected from Yelp Challenge 2018 Edition. In this\\ndataset, local businesses such as restaurants are treated as items.\\nWe retain users and items with over 10 interactions similar to [43].\\nTable 1: Datasets.\\n# Users\\n# Items\\n# Interactions\\nDensity\\nMovieLens\\n6,040\\n3,952\\n1,000,209\\n0.04190\\nGowalla\\n29,858\\n40,981\\n1,027,370\\n0.00084\\nPinterest\\n55,186\\n9,916\\n1,463,556\\n0.00267\\nYelp2018\\n31,668\\n38,048\\n1,561,406\\n0.00130\\n1https://grouplens.org/datasets/movielens/1m/\\n2https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/gowalla\\n3https://sites.google.com/site/xueatalphabeta/dataset-1/pinterest_iccv\\n4https://github.com/gusye1234/LightGCN-PyTorch/tree/master/data/yelp2018\\nTable 2: Performance comparison (underline represents the best performing model; R and D refer to Recall and NDCG).\\nModel\\nMovieLens (%)\\nGowalla (%)\\nPinterest (%)\\nYelp2018 (%)\\nR@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100 R@20 R@100 N@20 N@100\\nLSH\\n9.35\\n23.41\\n13.45\\n33.49\\n6.23\\n15.69\\n10.23\\n19.23\\n6.41\\n16.55\\n8.56\\n15.32\\n2.73\\n8.77\\n4.73\\n11.63\\nHashNet\\n14.98\\n35.67\\n23.41\\n38.14\\n10.11\\n24.19\\n15.31\\n23.11\\n8.93\\n29.43\\n10.37\\n21.63\\n2.64\\n9.45\\n6.42\\n14.68\\nHashGNN\\n12.55\\n34.54\\n23.63\\n36.54\\n9.63\\n22.13\\n15.85\\n24.01\\n8.01\\n25.27\\n10.48\\n21.19\\n3.22\\n10.96\\n6.62\\n14.61\\nHashGNN-soft 18.54\\n41.83\\n36.56\\n55.16\\n11.49\\n25.88\\n17.84\\n26.53\\n10.87\\n34.14\\n12.35\\n24.99\\n4.29\\n14.03\\n8.30\\n17.73\\nQuant-gumbel 17.48\\n42.08\\n34.57\\n56.50\\n10.78\\n26.58\\n15.62\\n25.08\\n9.78\\n30.63\\n11.15\\n22.92\\n3.91\\n13.00\\n8.07\\n17.08\\nNeurCF\\n19.43\\n48.04\\n36.85\\n51.71\\n13.95\\n32.76\\n22.21\\n27.39\\n9.09\\n29.52\\n13.55\\n22.24\\n3.12\\n10.97\\n6.45\\n14.91\\nNGCF\\n24.35\\n54.92\\n37.30\\n53.64\\n15.53\\n33.32\\n23.21\\n28.44\\n14.30\\n39.74\\n13.01\\n27.06\\n5.45\\n15.32\\n8.73\\n19.33\\nLightGCN\\n25.01\\n55.71\\n44.73\\n65.09\\n17.80\\n37.02\\n24.76\\n34.91\\n14.78\\n39.98\\n15.91\\n28.93\\n6.11\\n18.05\\n10.95\\n21.59\\nL2Q-GCN푒푛푑\\n20.52\\n49.27\\n38.41\\n60.03\\n14.62\\n32.24\\n21.24\\n30.97\\n12.52\\n35.67\\n13.92\\n26.37\\n5.10\\n16.31\\n9.62\\n19.88\\n% Capacity\\n82.05% 88.44%\\n85.87%\\n92.23%\\n82.13% 87.10%\\n85.78%\\n88.71%\\n84.71% 89.22%\\n87.49%\\n91.15%\\n83.47% 90.36%\\n87.85%\\n92.08%\\nL2Q-GCN푎푛푙\\n22.81\\n51.96\\n42.44\\n62.96\\n16.12\\n34.39\\n23.62\\n33.52\\n13.87\\n38.25\\n15.31\\n28.13\\n5.74\\n17.63\\n10.67\\n21.32\\n% Capacity\\n91.20% 93.27%\\n94.88%\\n96.73%\\n90.56% 92.90%\\n95.40%\\n96.02%\\n93.84% 95.67%\\n96.23%\\n97.23%\\n93.94% 97.67%\\n97.44%\\n98.75%\\n5.2\\nCompeting Methods\\nWe compare our model with two main streams of methods: (1)\\nfull-precision recommender systems including CF-based methods\\n(NeurCF), and GCN-based models (NGCF, LightGCN), (2) 1-bit quantization-\\nbased models for general item retrieval tasks (LSH, HashNet) and\\nfor Top-K recommendation (HashGNN).\\n• LSH [16] is a classical hashing method. LSH is ﬁrstly proposedto\\napproximate the similarity search for massive high-dimensional\\ndata and we introduce it for Top-K recommendation by follow-\\ning the adaptation in [40].\\n• HashNet [8] is a state-of-the-art deep hashing method that is\\noriginally proposedfor multimedia retrieval tasks. Similar to [40],\\nwe adapt it for graph data mainly by replacing the used AlexNet [27]\\nwith the general graph convolutional network.\\n• HashGNN [40] is the state-of-the-art 1-bit quantization-based\\nrecommender system method with GCN framework. We use HashGNN\\nto denote the vanilla version with hard encoding proposedin [40],\\nwhere each element of quantized user-item embeddings is strictly\\nquantized. We use HashGNN-soft to represent the relaxed ver-\\nsion proposed in [40], where it adopts a Bernoulli random vari-\\nable to provide the probability of replacing the quantized digits\\nwith continuous values in the original embeddings.\\n• NeurCF [19] is one state-of-the-art neural network model for\\ncollaborative ﬁltering. NeurCF models latent features of users\\nand items to capture their nonlinear feature interactions.\\n• NGCF [43] is one of the state-of-the-art GCN-based recommender\\nmodels. We compare L2푄-GCN with NGCF mainly to study their\\nperformance capabilities in Top-K recommendation.\\n• LightGCN [18] is the latest state-of-the-art GCN-based recom-\\nmendation model that has been widely evaluated. We include\\nLightGCN in our experiment mainly to set up the benchmark-\\ning as a reference of full-precision recommendation capability.\\n• Quant-gumbel is a variance of L2Q-GCN with the implementa-\\ntion of Gumbel-softmax for quantization [23, 32, 51]. We ﬁrst ex-\\npand each embedding bit to a size-two one-hot encoding. Then\\nQuant-gumbel utilizes the Gumbel-softmax trick to replace sign\\nfunction as relaxation for binary code generation.\\n5.3\\nExperiment Setup\\nIn the evaluation of Top-K recommendation, we apply the learned\\nuser-item representations to rank 퐾items for each user with the\\nhighest predicted scores, i.e., ˆ푦푢,푖. We choose two widely-used eval-\\nuation protocols Recall@퐾and NDCG@퐾to evaluate Top-K rec-\\nommendation capability.\\nWe implement L2Q-GCN model under Python 3.7 and PyTorch\\n1.14.0 with non-distributed training. The experiments are run on\\na Linux machine with 4 NVIDIA V100 GPU, 4 Intel Core i7-8700\\nCPUs, 32 GB of RAM with 3.20GHz. For all the baselines, we follow\\nthe oﬃcial hyper-parameter settings from original papers or as de-\\nfault in corresponding codes. For methods lacking recommended\\nsettings, we apply a grid search for hyper-parameters. The embed-\\nding dimension is searched in {32, 64, 128, 256, 512}. The learning\\nrate 휂is tuned within {10−3, 5 × 10−3, 10−2, 5 × 10−2} and the coeﬃ-\\ncient of 퐿2 normalization 휆is tuned among {10−5, 10−4, 10−3}. We\\ninitialize and optimize all models with default normal initializer\\nand Adam optimizer [25]. To guarantee reproducibility, we report\\nall the hyper-parameter settings in Appendix C.\\n5.4\\nPerformance Analysis (RQ1)\\nIn this section, we present a comprehensive performance analy-\\nsis between L2푄-GCN with two layers and competing recommender\\nmodels of full-precision-based and quantization-based. We eval-\\nuate Top-K recommendation over four datasets by varying K in\\n{20, 40, 60, 80, 100}. To achieve a more detailed performance com-\\nparison, we summarize the results of Top@20 and Top@100 rec-\\nommendation in Table 2. We also curve their complete results of\\nRecall@K and NDCG@K metrics and attach them in Appendix D.\\nGenerally, L2푄-GCN has made great improvements over quantization-\\nbased models and shows the competitive performance compared to\\nfull-precision models. We have the following observations:\\n• The results demonstrate the superiority of L2푄-GCN over\\nall quantization-based models. (1) As shown in Table 2, the\\nstate-of-the-art quantization-based GCN model, i.e., HashGNN\\n(and HashGNN-soft), works better than traditional quantization-\\nbased baselines, e.g., LSH, HashNet. This shows the eﬀectiveness\\nof graph convolutional architecture in capturing latent informa-\\ntion within interaction graphs for quantization preparation and\\nindicates that a direct adaptation of conventional quantization\\nmethods may not well handle the Top-K recommendation task.\\n(2) Furthermore, thanks to our proposedquantization-based graph\\nconvolution design, both L2푄-GCN푒푛푑and L2푄-GCN푎푛푙consis-\\ntently outperform HashGNN and its relaxed version HashGNN-\\nsoft. The main reason is that, our topology-aware quantization\\nsigniﬁcantly enriches the user-item representations and allevi-\\nates the feature smoothing issue caused by the numerical quan-\\ntization. We conduct the ablation study on this in the later sec-\\ntion.\\n• Compared to full-precision models, L2푄-GCN presents a\\ncompetitive performance recovery. (1) L2푄-GCN푒푛푑shows\\nthe performance superiority over traditional collaborative ﬁlter-\\ning model, i.e., NeurCF. Considering the large improvement of\\ntwo GCN-based models, i.e., NGCF and LightGCN, against NeurCF,\\nwe can infer the performance gap between L2푄-GCN푒푛푑and\\nNeurCF mainly comes from the proposedquantization-based graph\\nconvolution. (2) Compared to the best model LightGCN, taking\\nRecall metric as an example, L2푄-GCN푒푛푑shows about 82∼85%\\nand 87∼90%performance capacity in terms of Top@20 and Top@100.\\nThis indicates that, as the value of K increases, L2푄-GCN푒푛푑can\\nfurther improve the recommendation accuracy and narrow the\\ngap to LightGCN. (3) Moreover, by utilizing the quantization ap-\\nproximation and annealing training strategy, L2푄-GCN푎푛푙can\\nachieve betterperformance than NGCF on Gowalla and Yelp2018\\ndatasets. Compared to our basic implementation L2푄-GCN푒푛푑,\\nL2푄-GCN푎푛푙further improves the performance recovery by 8∼10%\\nand 9∼10%in terms of Recall@20 and Recall@100 across all bench-\\nmarks, proving the eﬀectiveness of our proposed modiﬁcation in\\nL2푄-GCN푎푛푙. (4) In addition, with K increasing up to 100, L2푄-\\nGCN푎푛푙presents a similar trend with L2푄-GCN푒푛푑such that it\\nperforms even closely to LightGCN, i.e., 93∼98% and 96∼99% in\\nterms of Recall and NDCG, respectively. In a nutshell, the pre-\\ndiction capability of both L2푄-GCN푒푛푑and L2푄-GCN푎푛푙devel-\\nops from ﬁne-grained ranking tasks to coarse-grained ones, e.g.,\\nTop-20 to Top-100 recommendation.\\n• Both L2푄-GCN푒푛푑and L2푄-GCN푎푛푙show the deployment\\nﬂexibility towards diﬀerent application scenarios. In the\\npipeline of industrial recommender systems, recall and re-ranking\\nare two important stages that substantially inﬂuence recommen-\\ndation quality. Recall refers to the process of quickly retriev-\\ning candidate items from the whole item pool that a given user\\nmay interest. Based on the more complex scoring algorithms, re-\\nranking outputs a precise ranking list of candidate items.\\nOn the one hand, as we will show later, L2푄-GCN푒푛푑can speed\\nup about 40% for candidate generation. Considering its perfor-\\nmance improvement on coarse-grained ranking tasks, e.g., Top-\\n100 recommendation, L2푄-GCN푒푛푑actually provides an alterna-\\ntive option to accelerate the recall stage. On the other hand, L2푄-\\nGCN푎푛푙further optimizes the recommendation accuracy that\\nperforms similarly to LightGCN. Since L2푄-GCN푎푛푙reduces the\\nembedding storage cost to about 9×, we can treat L2푄-GCN푎푛푙\\nas a substitute for the best model, i.e., LightGCN, as a trade-oﬀ\\nbetween space cost and prediction accuracy. We report the de-\\ntails of space and time cost for embedding storage and online\\ninference in the following section.\\n5.5\\nResource Consumption Analysis (RQ2)\\nIn this section, we study the resource consumption in embed-\\nding storage and inference time cost. We compare our models with\\nthe state-all-the-art full-precision model and quantization-based\\nmodel, i.e., LightGCN and HashGNN. We take their two-layer struc-\\ntures with the same 128-dimensional embeddings as reference and\\nillustrate with the largest dataset, i.e., Yelp2018, in Figure 5(a). Ob-\\nservations in this section can be popularizedto other three datasets\\nand we report the complete results in Appendix D.\\nEmbeddingstorage compression. Quantized embeddings can\\nlargely reduce the space consumption for oﬄine disk storage. To\\nmeasure the embedding size, we save these embeddings to the disk\\nsuch that they can recover the well-trained user-item representa-\\ntions for inference. As we can observe, after the binarization for\\nuser-item embeddings, L2Q-GCN푒푛푑and L2Q-GCN푎푛푙can achieve\\nthe space reduction with a factor of about 11× and 9×, respectively.\\nThis basically follows the theoretical bounds that are computed in\\nEquation 14, i.e., 푟푎푡푖표푒푛푑and 푟푎푡푖표푎푛푙when 퐿= 2. Furthermore,\\nconsidering the performance improvement, the space usage of ap-\\nproximation factors of L2Q-GCN푎푛푙is actually acceptable, which\\nmay dispel the concerns of large additional storage overhead.\\nOnline inference acceleration. We evaluate the time cost in-\\ncluding the score estimation and sorting. L2Q-GCN predicts the\\nscores between users and items by conducting embedding multi-\\nplications. At the stage of online inference, we ﬁrst interpret these\\nbinary embeddings by signed integers, e.g., int8, and then conduct\\ninteger arithmetics including integer summations and matrix mul-\\ntiplications. Please notice that we leave the development of bit-\\nwise operations for online inference for future work. To give a\\nfair comparison on the inference time cost, we disable all arith-\\nmetic optimization such as BLAS, MKL, and conduct the experi-\\nments using the vanilla NumPy provided by 5. As shown in Fig-\\nure 5(a), purely based on the quantized embeddings, L2Q-GCN푒푛푑\\ncan achieve 39.83% (118.23→196.49) of inference acceleration. L2Q-\\nGCN푎푛푙takes a similar running time with LightGCN as it intro-\\nduces the approximation factors in score estimation. Furthermore,\\nFigure 5(b) visualizes the overall evaluation in terms of resource\\nconsumption and recommendation accuracy. As the cube’s front-\\nhigh corner means the ideal optimal performance, our proposed\\nmethods make a good balance w.r.t consumption and accuracy.\\n5.6\\nAblation Study of L2Q-GCN Model (RQ3)\\nWe evaluate the necessity of each model component in both\\nL2Q-GCN푒푛푑and L2Q-GCN푎푛푙. Due to the page limits, we report\\nthe Top-20 recommendation results as a reference in Table 3.\\n5https://www.lfd.uci.edu/~gohlke/pythonlibs/\\nYelp2018 \\n(a) Space and time cost.\\n(b)  Overall evaluation \\nModel\\nYelp2018\\nE.S.\\nI.T.\\nLightGCN\\n34.04\\n196.49\\nHashGNN\\n1.06\\n117.43\\nHashGNN-soft\\n34.13\\n197.44\\nL2Q-GCN푒푛푑\\n3.19\\n118.23\\nratio\\n10.66×\\n1.66×\\nL2Q-GCN푎푛푙\\n3.98\\n194.84\\nratio\\n8.55×\\n1.01×\\nFigure 5: Results of two-layer networks with 128-dimension\\nembeddings (E.S. and I.T. are the abbreviations of embedding\\nsize (MB) and inference time (s). Best view in color).\\n5.6.1\\nEﬀect of Topology-aware Qantization. To substanti-\\nate the impact of topology-aware quantization in graph convolu-\\ntion, we give a variant of L2Q-GCN푒푛푑, i.e., w/o TQ, by disabling the\\nlayer-wise quantization and setting it as the ﬁnal encoder of full-\\nprecision graph convolution. As we can observe in Table 3, vari-\\nant w/o TQ remarkably underperforms L2Q-GCN푒푛푑. This demon-\\nstrates that simply using the latest-updated embeddings from the\\nGCN framework may not suﬃciently model the unique latent fea-\\ntures of both users and items, especially for the quantization-based\\nranking. Via capturing the intermediate information for represen-\\ntation enrichment, our topology-aware quantization can eﬀectively\\nalleviate the ranking smoothness issue caused by the limited ex-\\npressivity of discrete embeddings. This helps to make the quan-\\ntized representations of both users and items more discriminative,\\nwhich leads to the performance improvement on Top-K recommen-\\ndation.\\n5.6.2\\nEﬀect of Multi-loss in Optimization. To study the ef-\\nfect of BPR loss L푏푝푟and graph reconstruction loss L푟푒푐, we set\\ntwo variants, termed by w/o L푏푝푟and w/o L푟푒푐, to optimize L2Q-\\nGCN푒푛푑separately. As shown in Table 3, with all other model com-\\nponents, partially using one of L푏푝푟and L푟푒푐produces large per-\\nformance decay to L2Q-GCN푒푛푑. This conﬁrms the eﬀectiveness\\nof our proposed muti-loss design: while L푏푝푟assigns higher pre-\\ndiction values to observed interactions, 1.e., 푦푢,푖= 1, than the un-\\nobserved user-item pairs, L푟푒푐transfers the graph reconstruction\\nproblem to a classiﬁcation task by using the full-precision embed-\\ndings in training. By collectively optimizing these two loss func-\\ntions, L2Q-GCN푒푛푑can learn precise intermediate embeddings from\\nL푟푒푐, and produce quantized representations with high-quality rel-\\native order information regularized by L푏푝푟accordingly.\\n5.6.3\\nEﬀect of Rescaling Approximation. We now discuss the\\neﬀect of approximation factors in L2Q-GCN푎푛푙. We create two vari-\\nants, namely w/o RAF and w/in LF. w/o RAF directly removes the\\nrescaling approximation factors and w/in LF means replacing our\\noriginal approximation factors with learnable ones. We optimize\\nboth variants w/o RAF and w/in LF with the annealing training\\nstrategy. (1) The performance decline of w/o RAF proves the eﬀec-\\ntiveness of rescaling approximation for user-item representations.\\nAlthough these factors are directly calculated and may not be the-\\noretically optimal, they reﬂect the numerical uniqueness of embed-\\ndings for both users and items, which substantially improves L2Q-\\nGCN푎푛푙’s prediction capability. (2) As for w/in LF, the design of\\nTable 3: Ablation study.\\nVariant\\nMovieLens\\nGowalla\\nPinterest\\nYelp2018\\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\\nL2Q-GCN푒푛푙\\nw/o TQ\\n17.73 35.31 11.63 15.58 10.29 11.60\\n4.33\\n8.58\\n-13.60% -8.07% -20.45% -26.65% -17.81% -16.67% -15.10% -10.81%\\nw/o L푏푝푟\\n19.67 37.22\\n8.66\\n13.33\\n5.12\\n6.01\\n3.52\\n7.33\\n-4.14% -3.10% -40.77% -37.24% -59.11% -56.82% -30.98% -23.80%\\nw/o L푟푒푐16.98 32.76\\n8.32\\n9.28\\n10.86 11.55\\n3.33\\n6.60\\n-17.25% -14.71% -43.09% -56.31% -13.26% -17.03% -34.71% -31.39%\\nBest\\n20.52 38.41 14.62 21.24 12.52 13.92\\n5.10\\n9.62\\nL2Q-GCN푎푛푙\\nw/o RAF 20.86 38.14 10.29 12.10 11.19 12.11\\n4.25\\n8.09\\n-8.55% -10.13% -36.17% -48.77% -19.32% -20.90% -25.96% -24.18%\\nw/in LF\\n20.05 38.25 14.53 21.23 12.35 13.65\\n5.56\\n10.20\\n-12.10% -9.87% -9.86% -10.12% -10.96% -10.84% -3.13% -4.40%\\nw/o AT\\n21.24 40.05 15.09 22.70 13.37 14.75\\n5.27\\n9.96\\n-6.88% -5.63% -6.39% -3.90% -3.60% -3.66% -8.19% -6.65%\\nBest\\n22.81 42.44 16.12 23.62 13.87 15.31\\n5.74 10.67\\nlearnable rescaling factors does not achieve good performance as\\nexpected. One explanation is that, our proposed model currently\\ndoes not post a direct mathematical constraint to learnable factors\\n(푙푓), e.g., 푙푓(푙)\\n푢\\n= argmin(풗(푙)\\n푢, 푙푓(푙)\\n푢\\n풒(푙)\\n푢), mainly because they have\\ndiﬀerent embedding dimensionality. This means that purely rely-\\ning on the stochastic optimization may hardly reach the optimum.\\nIn a word, considering the additional search space introduced by\\nthis regularization term, we argue that our deterministic rescaling\\nmethod is simple but eﬀective in practice.\\n5.6.4\\nEﬀect of Annealing Training Strategy. We disable the\\nannealing training strategy by only adapting the rescaling approx-\\nimation design to L2Q-GCN푒푛푑and denote the variant as w/o AT.\\nThe performance of w/o AT well demonstrate the usefulness of our\\nutilized annealing training strategy in avoiding unnecessary opti-\\nmization directions and the numerical stability for producing bet-\\nter recommendation accuracy.\\nIn conclusion, the ablation study well justiﬁes the necessity of\\neach model component in both L2Q-GCN푒푛푑and L2Q-GCN푎푛푙. We\\nalso discuss the eﬀect of diﬀerent hyperparameter settings, e.g.,\\nlayer depth 퐿, quantization dimension 푑, to model performance\\nand attached the results in Appendix D.\\n6\\nConclusion and Future Work\\nIn this work, we propose L2Q-GCN to study the problem of 1-\\nbit representation quantization for Top-K recommendation. While\\nL2Q-GCN푒푛푑implements the basic framework of L2Q-GCN to gen-\\nerate the quantized user-item representations, L2Q-GCN푎푛푙fur-\\nther improves the recommendation capability by attaining 90∼99%\\nperformance recovery compared to the state-of-the-art model. The\\nextensive experiments over four real benchmarks not only prove\\nthe eﬀectiveness of our proposed models but also justify the neces-\\nsity of each model component.\\nAs for future work, we point out two possible directions. (1) In-\\nstead of relying on integer arithmetics, how to develop the bitwise-\\noperation-supported computation for eﬃcient inference is an im-\\nportant topic to investigate. (2) It is also worth studying the prob-\\nlem of complete network binarization for the GCN framework, as\\nit is more fundamental to many GCN-related methods for model\\ncompression and computation acceleration.\\nReferences\\n[1] [n.d.].\\nCodes and Datasets of L2Q-GCN.\\nhttps://drive.google.com/ﬁle/d/\\n1-9_kP60af0r86dAvIy4mdq_rPkTyCfMA/view?usp=sharing.\\n[2] Milad Alizadeh, Javier Fernández-Marqués,Nicholas D Lane, and Yarin Gal. 2018.\\nAn empirical study of binary neural networks’ optimisation. In ICLR.\\n[3] Mehdi Bahri, Gaétan Bahl, and Stefanos Zafeiriou. 2021. Binary Graph Neural\\nNetworks. In CVPR. 9492–9501.\\n[4] Haoli Bai, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu,\\nMichael Lyu, and Irwin King. 2021.\\nPushing the limit of bert quantization.\\n(2021).\\n[5] Yoshua Bengio, Nicholas Léonard, and Aaron Courville. 2013. Estimating or\\npropagating gradients through stochastic neurons for conditional computation.\\narXiv (2013).\\n[6] William Ralph Bennett. 1948. Spectra of quantized signals. The Bell System\\nTechnical Journal 27, 3 (1948), 446–472.\\n[7] Rianne van den Berg, Thomas N Kipf, and Max Welling. 2017. Graph convolu-\\ntional matrix completion. arXiv (2017).\\n[8] Zhangjie Cao, Mingsheng Long, Jianmin Wang, and Philip S Yu. 2017. Hashnet:\\nDeep learning to hash by continuation. In ICCV. 5608–5617.\\n[9] Yankai Chen, Menglin Yang, Yingxue Zhang, Mengchen Zhao, Ziqiao Meng,\\nJianye Hao, and Irwin King. 2021. Modeling Scale-free Graphs with Hyperbolic\\nGeometry for Knowledge-aware Recommendation. arXiv (2021).\\n[10] Zhiyong Cheng, Ying Ding, Lei Zhu, and Mohan Kankanhalli. 2018. Aspect-\\naware latent factor model: Rating prediction with ratings and reviews. In WWW.\\n639–648.\\n[11] Matthieu Courbariaux, Itay Hubara, Daniel Soudry, Ran El-Yaniv, and Yoshua\\nBengio. 2016. Binarized neural networks: Training deep neural networks with\\nweights and activations constrained to+ 1 or-1. arXiv (2016).\\n[12] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks for\\nyoutube recommendations. In Recsys. 191–198.\\n[13] Venice Erin Liong, Jiwen Lu, Gang Wang, Pierre Moulin, and Jie Zhou. 2015.\\nDeep hashing for compact binary codes learning. In CVPR. 2475–2483.\\n[14] Xue Geng, Hanwang Zhang, Jingwen Bian, and Tat-Seng Chua. 2015. Learning\\nimage and user features for recommendation in social networks. In ICCV. 4274–\\n4282.\\n[15] Allen Gersho and Robert M Gray. 2012. Vector quantization and signal compres-\\nsion. Vol. 159. Springer Science & Business Media.\\n[16] Aristides Gionis, Piotr Indyk, Rajeev Motwani, et al. 1999. Similarity search in\\nhigh dimensions via hashing. In Vldb, Vol. 99. 518–529.\\n[17] William L Hamilton, Rex Ying, and Jure Leskovec.2017. Inductive representation\\nlearning on large graphs. In NeurIPS. 1025–1035.\\n[18] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network\\nfor recommendation. In SIGIR. 639–648.\\n[19] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng\\nChua. 2017. Neural collaborative ﬁltering. In WWW. 173–182.\\n[20] Xiangnan He, Hanwang Zhang, Min-Yen Kan, and Tat-Seng Chua. 2016. Fast\\nmatrix factorization for online recommendation with implicit feedback. In SIGIR.\\n549–558.\\n[21] Wang Hongwei, Zhao Miao, Xie Xing, Li Wenjie, and Guo Minyi. 2019. Knowl-\\nedge Graph Convolutional Networks for Recommender Systems. (2019), 3307–\\n3313.\\n[22] Itay Hubara, Matthieu Courbariaux, Daniel Soudry, Ran El-Yaniv, and Yoshua\\nBengio. 2016. Binarized neural networks. NeurIPS 29 (2016).\\n[23] Eric Jang, Shixiang Gu, and Ben Poole. 2017. Categorical reparameterization\\nwith gumbel-softmax. (2017).\\n[24] Wang-Cheng Kang, Derek Zhiyuan Cheng, Tiansheng Yao, Xinyang Yi, Ting\\nChen, Lichan Hong, and Ed H Chi. 2021. Learning to Embed CategoricalFeatures\\nwithout Embedding Tables for Recommendation. In SIGKDD. 840–850.\\n[25] Diederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic opti-\\nmization. (2015).\\n[26] Thomas N Kipf and Max Welling. 2017.\\nSemi-supervised classiﬁcation with\\ngraph convolutional networks. (2017).\\n[27] Alex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. 2012. Imagenet classiﬁ-\\ncation with deep convolutional neural networks. NeurIPS 25 (2012), 1097–1105.\\n[28] Guohao Li, Matthias Muller, Ali Thabet, and Bernard Ghanem. 2019. Deepgcns:\\nCan gcns go as deep as cnns?. In ICCV. 9267–9276.\\n[29] Qimai Li, Zhichao Han, and Xiao-Ming Wu. 2018. Deeper insights into graph\\nconvolutional networks for semi-supervised learning. In AAAI.\\n[30] Dawen Liang, Laurent Charlin, James McInerney, and David M Blei. 2016. Mod-\\neling user exposure in recommendation. In WWW. 951–961.\\n[31] Xiaofan Lin, Cong Zhao, and Wei Pan. 2017. Towards accurate binary convolu-\\ntional neural network. (2017).\\n[32] Chris J Maddison, Andriy Mnih, and Yee Whye Teh. 2017. The concrete distri-\\nbution: A continuous relaxation of discrete random variables. (2017).\\n[33] Yury Nahshan, Brian Chmiel, Chaim Baskin, Evgenii Zheltonozhskii, Ron Ban-\\nner, Alex M Bronstein, and Avi Mendelson. 2021. Loss aware post-training quan-\\ntization. Machine Learning (2021), 1–18.\\n[34] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei\\nYu, and Jingkuan Song. 2020. Forward and backward information retention for\\naccurate binary neural networks. In CVPR. 2250–2259.\\n[35] Mohammad Rastegari, Vicente Ordonez, Joseph Redmon, and Ali Farhadi. 2016.\\nXnor-net: Imagenet classiﬁcation using binary convolutional neural networks.\\nIn ECCV. Springer, 525–542.\\n[36] Steﬀen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-\\nThieme. 2012. BPR: Bayesian personalized ranking from implicit feedback. arXiv\\n(2012).\\n[37] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim Naumov, and Jiyan Yang.\\n2020. Compositional embeddings using complementary partitions for memory-\\neﬃcient recommendation systems. In SIGKDD. 165–175.\\n[38] Jianing Sun, Yingxue Zhang, Chen Ma, Mark Coates, Huifeng Guo, Ruiming\\nTang, and Xiuqiang He. 2019. Multi-graph convolution collaborative ﬁltering.\\nIn ICDM. IEEE, 1306–1311.\\n[39] Shyam A Tailor, Javier Fernandez-Marques, and Nicholas D Lane. 2021. Degree-\\nquant: Quantization-aware training for graph neural networks. 9th ICLR (2021).\\n[40] Qiaoyu Tan, Ninghao Liu, Xing Zhao, Hongxia Yang, Jingren Zhou, and Xia Hu.\\n2020. Learning to hash with graph neural networks for recommender systems.\\nIn WWW. 1988–1998.\\n[41] Junfu Wang, Yunhong Wang, Zhen Yang, Liang Yang, and Yuanfang Guo. 2021.\\nBi-gcn: Binary graph convolutional network. In CVPR. 1561–1570.\\n[42] Jingdong Wang, Ting Zhang, Nicu Sebe, Heng Tao Shen, et al. 2017. A survey\\non learning to hash. TPAMI 40, 4 (2017), 769–790.\\n[43] Xiang Wang, Xiangnan He, Meng Wang, Fuli Feng, and Tat-Seng Chua. 2019.\\nNeural graph collaborative ﬁltering. In SIGIR. 165–174.\\n[44] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian\\nWeinberger. 2019. Simplifying graph convolutional networks. In ICML. PMLR,\\n6861–6871.\\n[45] Wei Wu, Bin Li, Chuan Luo, and Wolfgang Nejdl. 2021. Hashing-Accelerated\\nGraph Neural Networks for Link Prediction. In WWW. 2910–2920.\\n[46] Yang Xu, Lei Zhu, Zhiyong Cheng, Jingjing Li, and Jiande Sun. 2020. Multi-\\nfeature discrete collaborative ﬁltering for fast cold-start recommendation. In\\nAAAI, Vol. 34. 270–278.\\n[47] Rex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton,\\nand Jure Leskovec. 2018. Graph convolutional neural networks for web-scale\\nrecommender systems. In SIGKDD. 974–983.\\n[48] Hanwang Zhang, Fumin Shen, Wei Liu, Xiangnan He, Huanbo Luan, and Tat-\\nSeng Chua. 2016. Discrete collaborative ﬁltering. In SIGIR. 325–334.\\n[49] Yan Zhang, Defu Lian, and Guowu Yang. 2017. Discrete personalized ranking\\nfor fast collaborative ﬁltering from implicit feedback. In AAAI, Vol. 31.\\n[50] Yingxue Zhang, Soumyasundar Pal, Mark Coates, and Deniz Ustebay. 2019.\\nBayesian graph convolutional neural networks for semi-supervised classiﬁca-\\ntion. In AAAI, Vol. 33. 5829–5836.\\n[51] Yifei Zhang and Hao Zhu. 2019. Doc2hash: Learning discrete latent variables\\nfor documents retrieval. In ACL. 2235–2240.\\n[52] Han Zhu, Mingsheng Long, Jianmin Wang, and Yue Cao. 2016. Deep hashing\\nnetwork for eﬃcient similarity retrieval. In AAAI, Vol. 30.\\nA\\nNotation and Meanings\\nTable 1 explains all key notations used in this paper.\\nB\\npseudocodes of L2Q-GCN\\nWe attached the detailed pseudocodes of L2Q-GCN푒푛푑and L2Q-\\nGCN푎푛푙in Algorithms 1 and 2, respectively.\\nC\\nHyper-parameter Settings\\nWe report all the hyper-parameter settings in Table 2. Please\\nnotice that, in table 2, 퐵is the batch size, 휂represents the learning\\nrate, and 휆is the coeﬃcient of 퐿2 normalization.\\nD\\nComplete Results\\nFor detailed curves of Top-K recommendation, please refer to\\nFigure 1. For the study of diﬀerent hyperparameter setttings, i.e.,\\nlayer depth 퐿and quantization dimension 푑, please refer to Table 4\\nand Table 5 accordingly.\\nTable 1: Notations and meanings.\\nNotation\\nMeaning\\nU, I\\nThe sets of users, items.\\n푦푢,푖,\\nˆ푦푢,푖\\n푦푢,푖=1 means there is an observed interaction between user 푢and\\nitem 푖, otherwise 푦푢,푖=0. ˆ푦푢,푖is the estimated matching score.\\n풗(푙)\\n푥,\\ncontinuous embedding of node 푥at the 푙-th layer.\\n풒(푙)\\n푥,\\nbinary embedding of node 푥at the 푙-th layer.\\nQ푥,\\nquantized representation of node 푥.\\nA푥,\\napproximation-based quantized representation of node 푥.\\nAlgorithm 1: L2Q-GCN푒푛푑algorithm\\nInput: Interaction graph G; trainable parameters Θ: {풗푢}푢∈U,\\n{풗푖}푖∈I, 푾; hyper-parameters: 퐵, 푑푒푛푑, 퐿, 휂, 휆.\\nOutput: Prediction function F(푢,푖|Θ, G)\\n1 Q푢←∅, Q푖←∅;\\n2 while L2Q-GCN푒푛푑not converge do\\n3\\nfor (푢, 푖) ∈G that 푦푢,푖= 1 do\\n4\\nfor 푙= 1, · · · , 퐿do\\n5\\n풗(푙)\\n푢\\n←Í\\n푖∈N(푢)\\n1\\n√\\n|N(푢)|·|N(푖)| 푣(푙−1)\\n푖\\n;\\n6\\n풗(푙)\\n푖\\n←Í\\n푢∈N(푖)\\n1\\n√\\n|N(푖)|·|N(푢)| 푣(푙−1)\\n푢\\n;\\n7\\n푞(푙+1)\\n푢\\n←sign \\x00푾푇풗(푙)\\n푢\\n\\x01; 푞(푙)\\n푖\\n←sign \\x00푾푇풗(푙)\\n푖\\n\\x01;\\n8\\nUpdate (Q푢, Q푖);\\n9\\n휕풒(푙)\\n푢\\n휕휙,\\n휕풒(푙)\\n푖\\n휕휙\\n←1|휙|≤1 for backforward propagation;\\n10\\nUpdate (Q푢, Q푖) with 푞(0)\\n푢,푞(0)\\n푖\\n;\\n11\\nL ←compute loss and optimize L2Q-GCN푒푛푑model;\\n12 return F.\\nTable 3: Space and time results of two-layer networks with\\n128-dimension embeddings.\\nModel\\nMovieLens\\nGowalla\\nPinterest\\nYelp2018\\nE.S.\\nI.T.\\nE.S.\\nI.T.\\nE.S.\\nI.T.\\nE.S.\\nI.T.\\nLightGCN\\n4.88 3.42 34.59 190.41 28.45 82.66 34.04 196.49\\nL2Q-GCN푒푛푑0.47 2.19 3.24 118.97 2.98 53.48 3.19 118.23\\nGap\\n10.38× 1.56× 10.68× 1.60× 9.55× 1.55× 10.66× 1.66×\\nL2Q-GCN푎푛푙\\n0.58 3.55 4.05 191.32 3.73 81.77 3.98 194.84\\nGap\\n8.41× 0.96× 8.54× 1.00× 7.63× 1.01× 8.55× 1.01×\\nTable 4: Top-20 recommendation (%) w.r.t diﬀerent 퐿.\\n퐿\\nMovieLens\\nGowalla\\nPinterest\\nYelp2018\\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\\nL2Q-GCN푒푛푙\\n퐿= 1 20.81 39.08 13.59\\n19.86\\n11.34\\n12.79\\n4.76\\n9.28\\n퐿= 2 20.52\\n38.41 14.62 21.24 12.52 13.92\\n5.10\\n9.62\\n퐿= 3 10.57\\n33.44\\n12.65\\n19.23\\n6.41\\n7.39\\n3.83\\n7.95\\n퐿= 4\\n6.32\\n19.25\\n13.85\\n20.26\\n3.56\\n4.51\\n4.65\\n9.03\\nL2Q-GCN푎푛푙\\n퐿= 1 20.56\\n39.51\\n15.82\\n23.04\\n13.11\\n14.50\\n5.52\\n10.31\\n퐿= 2 22.81 42.44 16.12 23.62 13.87 15.31\\n5.74\\n10.67\\n퐿= 3 21.93\\n40.85\\n14.60\\n22.22\\n12.90\\n14.52\\n5.35\\n10.14\\n퐿= 4 21.72\\n40.46\\n14.63\\n22.24\\n11.31\\n13.16\\n4.96\\n9.74\\nTable 5: Top-20 recommendation (%) w.r.t diﬀerent 푑.\\n퐿\\nMovieLens\\nGowalla\\nPinterest\\nYelp2018\\nR@20 N@20 R@20 N@20 R@20 N@20 R@20 N@20\\nL2Q-GCN푒푛푙\\n푑= 64\\n16.65 28.23 14.62 21.24\\n9.63\\n9.69\\n5.10\\n9.62\\n푑= 128 18.02 33.15\\n14.41 19.91 12.52 13.92\\n4.86\\n9.63\\n푑= 256 20.52 38.41 10.39 14.29\\n10.37 11.62\\n4.26\\n8.55\\n푑= 512 17.79 31.49\\n13.23 20.65\\n10.22 11.38\\n3.49\\n7.43\\nL2Q-GCN푎푛푙\\n푑= 64\\n20.02 38.89\\n14.29 21.35\\n11.85 12.76\\n4.21\\n8.42\\n푑= 128 21.64 40.82\\n15.38 22.84 13.87 15.31\\n4.94\\n9.17\\n푑= 256 22.81 42.44 16.12 23.62 12.63 14.20\\n5.74\\n10.67\\n푑= 512 21.23 40.17\\n15.54 23.11\\n12.41 14.04\\n5.28\\n9.42\\nAlgorithm 2: L2Q-GCN푎푛푙algorithm\\nInput: Interaction graph G; trainable parameters Θ: {풗푢}푢∈U,\\n{풗푖}푖∈I, 푾; hyper-parameters: 퐵, 푑푒푛푑, 퐿, 휂, 휆.\\nOutput: Prediction function F(푢,푖|Θ, G)\\n1 Q푢←∅, Q푖←∅;\\n2 while L2Q-GCN푎푛푙not converge do\\n3\\nfor (푢,푖) ∈G that 푦푢,푖= 1 do\\n4\\nfor 푙= 1, · · · , 퐿do\\n5\\n풗(푙)\\n푢\\n←Í\\n푖∈N(푢)\\n1\\n√\\n|N(푢)|·|N(푖)| 푣(푙−1)\\n푖\\n;\\n6\\n풗(푙)\\n푖\\n←Í\\n푢∈N(푖)\\n1\\n√\\n|N(푖)|·|N(푢)| 푣(푙−1)\\n푢\\n;\\n7\\n푞(푙)\\n푢\\n←\\x00푾푇풗(푙)\\n푢\\n\\x01; 푞(푙)\\n푖\\n←\\x00푾푇풗(푙)\\n푖\\n\\x01;\\n8\\nif with quantization then\\n9\\n푞(푙)\\n푢\\n←sign \\x00푞(푙)\\n푢); 푞(푙)\\n푖\\n←sign \\x00푞(푙)\\n푖\\n);\\n10\\nSetting gradients for backforward propagation;\\n11\\n훼(푙)\\n푢\\n←||풗(푙)\\n푢||1\\n푑\\n, 훼(푙)\\n푢\\n←||풗(푙)\\n푢||1\\n푑\\n;\\n12\\nUpdate (A푢, A푖) with 훼(푙)\\n푢푞(푙)\\n푢, 훼(푙)\\n푖\\n푞(푙)\\n푖\\n;\\n13\\nUpdate (A푢, A푖) with 훼(0)\\n푢푞(0)\\n푢,훼(0)\\n푢푞(0)\\n푖\\n;\\n14\\nL ←compute loss and optimize L2Q-GCN푎푛푙model;\\n15 return F.\\n(a) MovieLens.\\n(b) Gowalla.\\n(c) Pinterest.\\n(d) Yelp2018.\\nFigure 1: topk.\\n(a) MovieLens.\\nRecall@K\\nNDCG@K\\n(b) Gowalla.\\n(c) Pinterest.\\n(d) Yelp2018.\\nFigure 2: Complete performance curves with annealing tranining strategy.\\nTable 2: Hyper-parameter settings for the four datasets.\\nMovieLens\\nGowalla\\nPinterest\\nYelp2018\\n퐵\\n2048\\n2048\\n2048\\n2048\\n푑푒푛푑\\n256\\n64\\n64\\n64\\n푑푎푛푙\\n256\\n256\\n128\\n256\\n휂\\n1 × 10−3\\n1 × 10−3\\n1 × 10−3\\n1 × 10−3\\n휆\\n1 × 10−4\\n1 × 10−4\\n1 × 10−4\\n1 × 10−4\\n',\n",
       " '2006.04466': 'Differentiable Neural Input Search for Recommender Systems\\nWeiyu Cheng, Yanyan Shen, Linpeng Huang\\nShanghai Jiao Tong University\\n{weiyu cheng, shenyy, lphuang}@sjtu.edu.cn\\nAbstract\\nLatent factor models are the driving forces of the state-of-\\nthe-art recommender systems, with an important insight of\\nvectorizing raw input features into dense embeddings. The\\ndimensions of different feature embeddings are often set to\\na same value empirically, which limits the predictive perfor-\\nmance of latent factor models. Existing works have proposed\\nheuristic or reinforcement learning-based methods to search\\nfor mixed feature embedding dimensions. For efﬁciency con-\\ncern, these methods typically choose embedding dimensions\\nfrom a restricted set of candidate dimensions. However, this\\nrestriction will hurt the ﬂexibility of dimension selection,\\nleading to suboptimal performance of search results.\\nIn this paper, we propose Differentiable Neural Input Search\\n(DNIS), a method that searches for mixed feature embedding\\ndimensions in a more ﬂexible space through continuous re-\\nlaxation and differentiable optimization. The key idea is to\\nintroduce a soft selection layer that controls the signiﬁcance\\nof each embedding dimension, and optimize this layer ac-\\ncording to model’s validation performance. DNIS is model-\\nagnostic and thus can be seamlessly incorporated with ex-\\nisting latent factor models for recommendation. We conduct\\nexperiments with various architectures of latent factor mod-\\nels on three public real-world datasets for rating prediction,\\nClick-Through-Rate (CTR) prediction, and top-k item recom-\\nmendation. The results demonstrate that our method achieves\\nthe best predictive performance compared with existing neu-\\nral input search approaches with fewer embedding parameters\\nand less time cost.\\n1\\nIntroduction\\nMost state-of-the-art recommender systems employ latent\\nfactor models that vectorize raw input features into dense\\nembeddings. A key question often asked of feature embed-\\ndings is: “How should we determine the dimensions of fea-\\nture embeddings?” The common practice is to set a uniform\\ndimension for all the features, and treat the dimension as a\\nhyperparameter that needs to be tuned with time-consuming\\ngrid search on a validation set. However, a uniform embed-\\nding dimension is not necessarily suitable for different fea-\\ntures. Intuitively, predictive and popular features that appear\\nin a large number of data samples usually deserve a high\\nembedding dimension, which encourages a high model ca-\\npacity to ﬁt these data samples [Joglekar et al. 2020; Zhao\\net al. 2020]. Likewise, less predictive and infrequent features\\nwould rather be assigned with lower embedding dimensions\\nto avoid overﬁtting on scarce data samples. As such, it is\\ndesirable to impose a mixed dimension scheme for differ-\\nent features towards better recommendation performance.\\nAnother notable fact is that embedding layers in industrial\\nweb-scale recommender systems account for the majority of\\nmodel parameters and can consume hundreds of gigabytes\\nof memory space [Park et al. 2018; Covington, Adams, and\\nSargin 2016]. Replacing a uniform feature embedding di-\\nmension with varying dimensions can help to remove redun-\\ndant embedding weights and signiﬁcantly reduce memory\\nand storage costs for model parameters.\\nSome recent works [Ginart et al. 2019; Joglekar et al.\\n2020] have focused on searching for mixed feature embed-\\nding dimensions automatically, which is deﬁned as the Neu-\\nral Input Search (NIS) problem. There are two existing ap-\\nproaches to dealing with NIS: heuristic-based and reinforce-\\nment learning-based. The heuristic-based approach [Ginart\\net al. 2019] designs an empirical function to determine\\nthe embedding dimensions for different features accord-\\ning to their frequencies of occurrence (also called popular-\\nity). The empirical function involves several hyperparam-\\neters that need to be manually tuned to yield a good re-\\nsult. The reinforcement learning-based approach [Joglekar\\net al. 2020] ﬁrst divides a base dimension space equally into\\nseveral blocks, and then applies reinforcement learning to\\ngenerate decision sequences on the selection of dimension\\nblocks for different features. These methods, however, re-\\nstrict each feature embedding dimension to be chosen from\\na small set of candidate dimensions that is explicitly prede-\\nﬁned [Joglekar et al. 2020] or implicitly controlled by hy-\\nperparameters [Ginart et al. 2019]. Although this restriction\\nreduces search space and thereby improves computational\\nefﬁciency, another question then arises: how to decide the\\ncandidate dimensions? Notably, a suboptimal set of candi-\\ndate dimensions could result in a suboptimal search result\\nthat hurts model’s recommendation performance.\\nIn this paper, we propose Differentiable Neural Input\\nSearch (DNIS) to address the NIS problem in a differen-\\ntiable manner through gradient descent. Instead of search-\\ning over a predeﬁned discrete set of candidate dimensions,\\nDNIS relaxes the search space to be continuous and opti-\\nmizes the selection of embedding dimensions by descend-\\ning model’s validation loss. More speciﬁcally, we introduce\\narXiv:2006.04466v2  [cs.LG]  10 Sep 2020\\na soft selection layer between the embedding layer and the\\nfeature interaction layers of latent factor models. Each in-\\nput feature embedding is fed into the soft selection layer\\nto perform an element-wise multiplication with a scaling\\nvector. The soft selection layer directly controls the signiﬁ-\\ncance of each dimension of the feature embedding, and it is\\nessentially a part of model architecture which can be op-\\ntimized according to model’s validation performance. We\\nalso propose a gradient normalization technique to solve the\\nproblem of high variance of gradients when optimizing the\\nsoft selection layer. After training, we employ a ﬁne-grained\\npruning procedure by merging the soft selection layer with\\nthe feature embedding layer, which yields a ﬁne-grained\\nmixed embedding dimension scheme for different features.\\nDNIS can be seamlessly applied to various existing architec-\\ntures of latent factor models for recommendation. We con-\\nduct extensive experiments with six existing architectures\\nof latent factor model on three public real-world datasets\\nfor rating prediction, Click-Through-Rate (CTR) prediction,\\nand top-k item recommendation tasks. The results demon-\\nstrate that the proposed method achieves the best predic-\\ntive performance compared with existing NIS methods with\\nfewer embedding parameters and less time cost.\\nThe major contributions of this paper are summarized as\\nfollows:\\n• We propose DNIS, a method that addresses the NIS prob-\\nlem in a differentiable manner by relaxing the embedding\\ndimension search space to be continuous and optimizing\\nthe selection of dimensions with gradient descent.\\n• We propose a gradient normalization technique to deal\\nwith the high variance of gradients during optimizing\\nthe soft selection layer, and further design a ﬁne-grained\\npruning procedure through layer merging to produce a\\nﬁne-grained mixed embedding dimension scheme for dif-\\nferent features.\\n• The proposed method is model-agnostic, and thus can be\\nincorporated with various existing architectures of latent\\nfactor models to improve recommendation performance\\nand reduce the size of embedding parameters.\\n• We conduct experiments with different model architec-\\ntures on real-world datasets for three typical recom-\\nmendation tasks: rating prediction, CTR prediction, and\\ntop-k item recommendation. The results demonstrate\\nDNIS achieves the best overall result compared with ex-\\nisting NIS methods in terms of recommendation perfor-\\nmance, embedding parameter size and training time cost.\\n2\\nDifferentiable Neural Input Search\\n2.1\\nBackground\\nLatent factor models. We consider a recommender sys-\\ntem involving M feature ﬁelds (e.g., user ID, item ID, item\\nprice). Typically, M is 2 (including user ID and item ID)\\nin collaborative ﬁltering (CF) problems, whereas in the con-\\ntext of CTR prediction, M is usually much larger than 2\\nto include more feature ﬁelds. Each categorical feature ﬁeld\\nconsists of a collection of discrete features, while a numer-\\nical feature ﬁeld contains one scalar feature. Let F denote\\nthe list of features over all the ﬁelds and the size of F is\\nN. For the i-th feature in F, its initial representation is a N-\\ndimensional sparse vector xi, where the i-th element is 1 (for\\ndiscrete feature) or a scalar number (for scalar feature), and\\nthe others are 0s. Latent factor models generally consists of\\ntwo parts: one feature embedding layer, followed by the fea-\\nture interaction layers. Without loss of generality, the input\\ninstances to the latent factor model include several features\\nbelonging to the respective feature ﬁelds. The feature em-\\nbedding layer transforms all the features in an input instance\\ninto dense embedding vectors. Speciﬁcally, a sparsely en-\\ncoded input feature vector xi ∈RN is transformed into a\\nK-dimensional embedding vector ei ∈RK as follows:\\nei = E⊤xi\\n(1)\\nwhere E ∈RN×K is known as the embedding matrix. The\\noutput of the feature embedding layer is the collection of\\ndense embedding vectors for all the input features, which\\nis denoted as X. The feature interaction layers, which are\\ndesigned to be different architectures, essentially compose\\na parameterized function G that predicts the objective based\\non the collected dense feature embeddings X for the input\\ninstance. That is,\\nˆy = G(θ, X)\\n(2)\\nwhere ˆy is the model’s prediction, and θ denotes the set of\\nparameters in the interaction layers. Prior works have devel-\\noped various architectures for G, including the simple inner\\nproduct function [Rendle 2010], and deep neural networks-\\nbased interaction functions [He et al. 2017; Cheng et al.\\n2018; Lian et al. 2018; Cheng et al. 2016; Guo et al. 2017].\\nMost of the proposed architectures for the interaction layers\\nrequire all the feature embeddings to be in a uniform dimen-\\nsion.\\nNeural architecture search. Neural Architecture Search\\n(NAS) has been proposed to automatically search for the\\nbest neural network architecture. To explore the space of\\nneural architectures, different search strategies have been ex-\\nplored including random search [Li and Talwalkar 2019],\\nevolutionary methods [Elsken, Metzen, and Hutter 2019;\\nMiller, Todd, and Hegde 1989; Real et al. 2017], Bayesian\\noptimization [Bergstra, Yamins, and Cox 2013; Domhan,\\nSpringenberg, and Hutter 2015; Mendoza et al. 2016], re-\\ninforcement learning [Baker et al. 2017; Zhong et al. 2018;\\nZoph and Le 2017], and gradient-based methods [Cai, Zhu,\\nand Han 2019; Liu, Simonyan, and Yang 2019; Xie et al.\\n2019]. Since being proposed in [Baker et al. 2017; Zoph\\nand Le 2017], NAS has achieved remarkable performance in\\nvarious tasks such as image classiﬁcation [Real et al. 2019;\\nZoph et al. 2018], semantic segmentation [Chen et al. 2018]\\nand object detection [Zoph et al. 2018]. However, most of\\nthese researches have focused on searching for optimal net-\\nwork structures automatically, while little attention has been\\npaid to the design of the input component. This is because\\nthe input component in visual tasks is already given in the\\nform of ﬂoating point values of image pixels. As for recom-\\nmender systems, an input component based on the embed-\\nding layer is deliberately developed to transform raw fea-\\ntures (e.g., discrete user identiﬁers) into dense embeddings.\\nIn this paper, we focus on the problem of neural input search,\\n𝑑\"\\n𝑣\"\\n𝐞\"\\n0\\n0.1\\n0\\n-0.2\\n2\\n4\\n0.1\\n-0.2\\n𝑉\\n𝐷\\n(a) Example of notations.\\n𝒢(𝜽)\\n𝑦&\\n𝐞(\\n𝛂\\n𝐞\\n…\\n…\\n…\\nEmbedding\\nLayer\\nSoft Selection\\nLayer\\nOutput\\nEmbeddings\\nInteraction\\nFunction\\nPrediction\\n(b) Model structure.\\nFigure 1: A demonstration of notations and model structure.\\nwhich can be considered as NAS on the input component\\n(i.e., the embedding layer) of recommender systems.\\n2.2\\nSearch Space and Problem\\nSearch space. The key idea of neural input search is to\\nuse embeddings with mixed dimensions to represent differ-\\nent features. To formulate feature embeddings with different\\ndimensions, we adopt the representation for sparse vectors\\n(with a base dimension K). Speciﬁcally, for each feature, we\\nmaintain a dimension index vector d which contains ordered\\nlocations of the feature’s existing dimensions from the set\\n{1, · · · , K}, and an embedding value vector v which stores\\nembedding values in the respective existing dimensions. The\\nconversion from the index and value vectors of a feature into\\nthe K-dimensional embedding vector e is straightforward.\\nFigure 1a gives an example of di, vi and ei for the i-th fea-\\nture in F. Note that e corresponds to a row in the embedding\\nmatrix E.\\nThe size of d varies among different features to enforce a\\nmixed dimension scheme. Formally, given the feature set F,\\nwe deﬁne the mixed dimension scheme D = {d1, · · · , dN}\\nto be the collection of dimension index vectors for all the\\nfeatures in F. We use D to denote the search space of the\\nmixed dimension scheme D for F, which includes 2NK pos-\\nsible choices. Besides, we denote by V = {v1, · · · , vN} the\\nset of the embedding value vectors for all the features in F.\\nThen we can derive the embedding matrix E with D and V\\nto make use of conventional feature interaction layers.\\nProblem formulation. Let Θ = {θ, V } be the set of train-\\nable model parameters, and Ltrain and Lval are model’s\\ntraining loss and validation loss, respectively. The two losses\\nare determined by both the mixed dimension scheme D, and\\nthe trainable parameters Θ. The goal of neural input search\\nis to ﬁnd a mixed dimension scheme D ∈D that minimizes\\nthe validation loss Lval(Θ∗, D), where the parameters Θ∗\\ngiven any mixed dimension scheme are obtained by mini-\\nmizing the training loss. This can be formulated as:\\nmin\\nD∈D Lval(Θ∗(D), D)\\ns.t. Θ∗(D) = argmin\\nΘ\\nLtrain(Θ, D)\\n(3)\\nThe above problem formulation is actually consistent with\\nhyperparameter optimization in a broader scope [Maclaurin,\\nDuvenaud, and Adams 2015; Pedregosa 2016; Franceschi\\net al. 2018], since the mixed dimension scheme D can be\\nconsidered as model hyperparameters to be determined ac-\\ncording to model’s validation performance. However, the\\nmain difference is that the search space D in our problem\\nis much larger than the search space of conventional hyper-\\nparameter optimization problems.\\n2.3\\nFeature Blocking\\nFeature blocking has been a novel ingredient used in the ex-\\nisting neural input search methods [Joglekar et al. 2020; Gi-\\nnart et al. 2019] to facilitate the reduction of search space.\\nThe intuition behind is that features with similar frequen-\\ncies could be grouped into a block sharing the same em-\\nbedding dimension. Following the existing works, we ﬁrst\\nemploy feature blocking to control the search space of the\\nmixed dimension scheme. We sort all the features in F in\\nthe descending order of frequency (i.e., the number of fea-\\nture occurrences in the training instances). Let ηf denote the\\nfrequency of feature f ∈F. We can obtain a sorted list of\\nfeatures ˜F = [f1, f2, · · · , fN] such that ηfi ≥ηfj for any\\ni < j. We then separate ˜F into L blocks, where the fea-\\ntures in a block share the same dimension index vector d.\\nWe denote by ˜D the mixed dimension scheme after feature\\nblocking. Then the length of the mixed dimension scheme\\n| ˜D| becomes L, and the search space size is reduced from\\n2NK to 2LK accordingly, where L ≪N.\\n2.4\\nContinuous Relaxation and Differentiable\\nOptimization\\nContinuous relaxation. After feature blocking, in order to\\noptimize the mixed dimension scheme ˜D, we ﬁrst transform\\n˜D into a binary dimension indicator matrix ˜D ∈RL×K,\\nwhere each element in ˜D is either 1 or 0 indicating the exis-\\ntence of the corresponding embedding dimension according\\nto ˜D. We then introduce a soft selection layer to relax the\\nsearch space of ˜D to be continuous. The soft selection layer\\nis essentially a numerical matrix α ∈RL×K, where each\\nelement in α satisﬁes: 0 ≤αl,k ≤1. That is, each binary\\nchoice ˜Dl,k (the existence of the k-th embedding dimension\\nin the l-th feature block) in ˜D is relaxed to be a continuous\\nvariable αl,k within the range of [0, 1]. We insert the soft\\nselection layer between the feature embedding layer and in-\\nteraction layers in the latent factor model, as illustrated in\\nFigure 1b. Given α and the embedding matrix E, the output\\nembedding ˜ei of a feature fi in the l-th block produced by\\nthe bottom two layers can be computed as follows:\\n˜ei = ei ⊙αl∗\\n(4)\\nwhere αl∗is the l-th row in α, and ⊙is the element-wise\\nproduct. By applying Equation (4) to all the input features,\\nwe can obtain the output feature embeddings X. Next, we\\nsupply X to the feature interaction layers for ﬁnal prediction\\nas speciﬁed in Equation (2). Note that α is used to softly\\nselect the dimensions of feature embeddings during model\\ntraining, and the discrete mixed dimension scheme D will\\nbe derived after training.\\nDifferentiable optimization. Now that we relax the mixed\\ndimension scheme ˜D (after feature blocking) via the soft\\nselection layer α, our problem stated in Equation (3) can be\\ntransformed into:\\nmin\\nα\\nLval(Θ∗(α), α)\\ns.t. Θ∗(α) = argmin\\nΘ\\nLtrain(Θ, α) ∧αk,j ∈[0, 1] (5)\\nwhere Θ = {θ, E} represents model parameters in both\\nthe embedding layer and interaction layers. Equation 5 es-\\nsentially deﬁnes a bi-level optimization problem [Colson,\\nMarcotte, and Savard 2007], which has been studied in\\ndifferentiable NAS [Liu, Simonyan, and Yang 2019] and\\ngradient-based hyperparameter optimization [Chen et al.\\n2019; Franceschi et al. 2018; Pedregosa 2016]. Basically, α\\nand Θ are respectively treated as the upper-level and lower-\\nlevel variables to be optimized in an interleaving way. To\\ndeal with the expensive optimization of Θ, we follow the\\ncommon practice that approximates Θ∗(α) by adapting Θ\\nusing a single training step:\\nΘ∗(α) ≈Θ −ξ∇ΘLtrain(Θ, α)\\n(6)\\nwhere ξ is the learning rate for one-step update of model pa-\\nrameters Θ. Then we can optimize α based on the following\\ngradient:\\n∇αLval(Θ −ξ∇ΘLtrain(Θ, α), α)\\n=∇αLval(Θ′, α) −ξ∇2\\nα,ΘLtrain(Θ, α) · ∇αLval(Θ′, α)\\n(7)\\nwhere Θ′ = Θ −ξ∇ΘLtrain(Θ, α) denotes the model pa-\\nrameters after one-step update. Equation (7) can be solved\\nefﬁciently using the existing deep learning libraries that al-\\nlow automatic differentiation, such as Pytorch [Paszke et al.\\n2019]. The second-order derivative term in Equation (7) can\\nbe omitted to further improve computational efﬁciency con-\\nsidering ξ to be near zero, which is called the ﬁrst-order\\napproximation. Algorithm 1 (line 5-10) summarizes the bi-\\nlevel optimization procedure for solving Equation (5).\\nGradient normalization. During the optimization of α by\\nthe gradient ∇αLval(Θ′, α), we propose a gradient normal-\\nization technique to normalize the row-wise gradients of α\\nover each training batch:\\ngnorm(αl∗) =\\ng(αl∗)\\nΣK\\nk=1|g(αl,k)|/K + ϵg\\n,\\nk ∈[1, K] (8)\\nwhere g and gnorm denote the gradients before and after\\nnormalization respectively, and ϵg is a small value (e.g., 1e-\\n7) to avoid numerical overﬂow. Here we use row-wise gra-\\ndient normalization to deal with the high variance of the\\ngradients of α during backpropogation. More speciﬁcally,\\ng(αl∗) of a high-frequency feature block can be several or-\\nders of magnitude larger than that of a low-frequency feature\\nblock due to their difference on the number of related data\\nsamples. By normalizing the gradients for each block, we\\ncan apply the same learning rate to different rows of α dur-\\ning optimization. Otherwise, a single learning rate shared by\\ndifferent feature blocks will fail to optimize most rows of α.\\nAlgorithm 1: DNIS - Differentiable Neural Input Search\\n1:1: Input: training dataset, validation dataset.\\n2:2: Output: mixed dimension scheme D, embedding\\nvalues V , interaction function parameters θ.\\n3: Sort features into ˜F and divide them into L blocks;\\n4: Initialize the soft selection layer α to be an all-one\\nmatrix, and randomly initialize Θ; // Θ = {θ, E}\\n5: while not converged do\\n6:\\nUpdate trainable parameters Θ by descending\\n∇ΘLtrain(Θ, α);\\n7:\\nCalculate the gradients of α as:\\n−ξ∇2\\nα,ΘLtrain(Θ, α) · ∇αLval(Θ′, α) +\\n∇αLval(Θ′, α);\\n// (set ξ = 0 if using first-order\\napproximation)\\n8:\\nPerform Equation (8) to normalize the gradients in\\nα;\\n9:\\nUpdate α by descending the gradients, and then\\nclip its values into the range of [0, 1];\\n10: end\\n11: Calculate the output embedding matrix E using α and\\n˜E according to Equation (4);\\n12: Prune E into a sparse matrix E′ following Equation (9);\\n13: Derive the mixed dimension scheme D and embedding\\nvalues V with E′;\\n2.5\\nDeriving Fine-grained Mixed Embedding\\nDimension Scheme\\nAfter optimization, we have the learned parameters for θ, E\\nand α. A straightforward way to derive the discrete mixed\\ndimension scheme D is to prune non-informative embed-\\nding dimensions in the soft selection layer α. Here we em-\\nploy a ﬁne-grained pruning procedure through layer merg-\\ning. Speciﬁcally, for feature fi in the l-th block, we can\\ncompute its output embedding ˜ei with ei and αl∗follow-\\ning Equation (4). We collect the output embeddings ˜ei for\\nall the features in F and form an output embedding matrix\\n˜E ∈RN×K. We then prune non-informative embedding di-\\nmensions in ˜E as follows:\\n˜Ei,j =\\n\\x1a0,\\nif |˜Ei,j| < ϵ\\n˜Ei,j,\\notherwise\\n(9)\\nwhere ϵ is a threshold that can be manually tuned according\\nto the requirements on model performance and parameter\\nsize. The pruned output embedding matrix ˜E is sparse and\\ncan be used to derive the discrete mixed dimension scheme\\nD and the embedding value vectors V for F accordingly.\\nWith ﬁne-grained pruning, the derived embedding dimen-\\nsions can be different even for features in the same feature\\nblock, resulting in a more ﬂexible mixed dimension scheme.\\nRelation to network pruning. Network pruning, as one\\nkind of model compression techniques, improves the efﬁ-\\nciency of over-parameterized deep neural networks by re-\\nmoving redundant neurons or connections without damag-\\ning model performance [Cheng et al. 2017; Liu et al. 2019;\\nFrankle and Carbin 2019]. Recent works of network prun-\\ning [Han et al. 2015; Molchanov et al. 2017; Li et al.\\nTable 1: Statistics of the datasets.\\nDataset\\nInstance#\\nField#\\nFeature#\\nMovielens-20M\\n20,000,263\\n2\\n165,771\\nCriteo\\n45,840,617\\n39\\n2,086,936\\nMovielens-1M\\n1,000,209\\n2\\n9,746\\n2017] generally performed iterative pruning and ﬁnetun-\\ning over certain pretrained over-parameterized deep net-\\nwork. Instead of simply removing redundant weights, our\\nproposed method DNIS optimizes feature embeddings with\\nthe gradients from the validation set, and only prunes non-\\ninformative embedding dimensions and their values in one\\nshot after model training. This also avoids manually tuning\\nthresholds and regularization terms per iteration. We con-\\nduct experiments to compare the performance of DNIS and\\nnetwork pruning methods in Section 3.4.\\n3\\nExperiments\\n3.1\\nExperimental Settings\\nDatasets. We used two benchmark datasets Movielens-\\n20M [Harper and Konstan 2016] and Criteo [Labs 2014]\\nfor rating prediction and CTR prediction tasks, respectively.\\nFor each dataset, we randomly split the instances by 8:1:1\\nto obtain the training, validation and test sets. Besides, we\\nalso conduct experiments on Movielens-1M dataset [Harper\\nand Konstan 2016] to compare with NIS-ME [Joglekar et al.\\n2020] for top-k item recommendation task. The statistics of\\nthe three datasets are summarized in Table 1.\\n(1) Movielens-20M is a CF dataset containing more than 20\\nmillion user ratings ranging from 1 to 5 on movies.\\n(2) Criteo is a popular industry benchmark dataset for CTR\\nprediction, which contains 13 numerical feature ﬁelds and\\n26 categorical feature ﬁelds. Each label indicates whether a\\nuser has clicked the corresponding item.\\n(3) Movielens-1M is a CF dataset containing over one mil-\\nlion user ratings ranging from 1 to 5 on movies.\\nEvaluation metrics. We adopt MSE (mean squared error)\\nfor rating prediction task, and use AUC (Area Under the\\nROC Curve) and Logloss for CTR prediction task. In ad-\\ndition to predictive performance, we also report the em-\\nbedding parameter size and the overall time cost of each\\nmethod. When comparing with NIS-ME, we provide Recall,\\nMRR (mean reciprocal rank) and NDCG results for top-k\\nrecommendation.\\nComparison methods. We compare our DNIS method with\\nthe following four approaches.\\n• Grid Search. This is the traditional approach to search-\\ning for a uniform embedding dimension. In our experiments,\\nwe searched 16 groups of dimensions, ranging from 4 to 64\\nwith a stride of 4.\\n• Random Search. Random search has been recognized\\nas a strong baseline for NAS problems [Liu, Simonyan, and\\nYang 2019]. When random searching a mixed dimension\\nscheme, we applied the same feature blocking as we did for\\nDNIS. Following the intuition that high-frequency features\\ndesire larger numbers of dimensions, we generated 16 ran-\\ndom descending sequences as the search space of the mixed\\ndimension scheme for each model and report the best results.\\n• MDE (Mixed Dimension Embeddings [Ginart et al.\\n2019]). This method performs feature blocking and applies\\na heuristic scheme where the number of dimensions per fea-\\nture block is proportional to some fractional power of its\\nfrequency. We tested 16 groups of hyperparameters settings\\nas suggested in the original paper and report the best results.\\n• NIS-ME (Neural Input Search with Multi-size Embed-\\nding [Joglekar et al. 2020]). This method uses reinforcement\\nlearning to ﬁnd optimal embedding dimensions for differ-\\nent features within a given memory budget. Since the imple-\\nmentation is not available, we follow the same experimental\\nsettings as detailed in [Joglekar et al. 2020]) and report the\\nresults of our method for comparison.\\nFor DNIS, we show its performance before and after the\\ndimension pruning in Equation (9), and report the storage\\nsize of the pruned sparse matrix E′ using COO format of\\nsparse matrix [Virtanen et al. 2020]. We provide the results\\nwith different compression rates (CR), i.e., the division of\\nunpruned embedding parameter size by the pruned size.\\nImplementation details. We implement our method using\\nPytorch [Paszke et al. 2019]. We apply Adam optimizer with\\nthe learning rate of 0.001 for model parameters Θ and that\\nof 0.01 for soft selection layer parameters α. The mini-batch\\nsize is set to 4096 and the uniform base dimension K is\\nset to 64 for all the models. We apply the same blocking\\nscheme for Random Search, MDE and DNIS. The default\\nnumbers of feature blocks L is set to 10 and 6 for Movielens\\nand Criteo datasets, respectively. We employ various latent\\nfactor models: MF, MLP [He et al. 2017] and NeuMF [He\\net al. 2017] for rating prediction, and FM [Rendle 2010],\\nWide&Deep [Cheng et al. 2016], DeepFM [Guo et al. 2017]\\nfor CTR prediction, where the conﬁguration of latent fac-\\ntor models are the same over different methods for a fair\\ncomparison. Besides, we exploit early-stopping for all the\\nmethods according to the change of validation loss during\\nmodel training. All the experiments were performed using\\nNVIDIA GeForce RTX 2080Ti GPUs.\\n3.2\\nComparison Results\\nTable 2 and Table 3 show the comparison results of different\\nNIS methods on rating prediction and CTR prediction tasks,\\nrespectively. First, we can see that DNIS achieves the best\\nprediction performance over all the model architectures for\\nboth tasks. It is worth noticing that the time cost of DNIS is\\nreduced by 2× to over 10× compared with the baselines.\\nThe results conﬁrms that DNIS is able to learn discrimina-\\ntive feature embeddings with signiﬁcantly higher efﬁciency\\nthan the existing search methods. Second, DNIS with di-\\nmension pruning achieves competitive or better performance\\nthan baselines, and can yield a signiﬁcant reduction on em-\\nbedding parameter size. For example, DNIS with the CR of 2\\noutperforms all the baselines on Movielens, and yet reaches\\nthe minimal parameter size. The advantages of DNIS with\\nthe CR of 20 and 30 are more signiﬁcant on Criteo. Be-\\nsides, we observe that DNIS can achieve a higher CR on\\nCriteo than Movielens without sacriﬁcing prediction perfor-\\nmance. This is because the distribution of feature frequency\\non Criteo is severely skewed, leading to a signiﬁcantly large\\nTable 2: Comparison between DNIS and baselines on the rating prediction task using Movielens-20M dataset. We also report\\nthe storage size of the derived feature embeddings and the training time per method. For DNIS, we show its results with and\\nw/o different compression rates (CR), i.e., the division of unpruned embedding parameter size by the pruned size.\\nSearch Methods\\nMF\\nMLP\\nNeuMF\\nParams\\nTime Cost\\nMSE\\nParams\\nTime Cost\\nMSE\\nParams\\nTime Cost\\nMSE\\n(M)\\n(M)\\n(M)\\nGrid Search\\n33\\n16h\\n0.622\\n35\\n8h\\n0.640\\n61\\n4h\\n0.625\\nRandom Search\\n33\\n16h\\n0.6153\\n22\\n4h\\n0.6361\\n30\\n2h\\n0.6238\\nMDE\\n35\\n24h\\n0.6138\\n35\\n5h\\n0.6312\\n27\\n3h\\n0.6249\\nDNIS (unpruned)\\n37\\n1h\\n0.6096\\n36\\n1h\\n0.6255\\n72\\n1h\\n0.6146\\nDNIS (CR = 2)\\n21\\n1h\\n0.6126\\n20\\n1h\\n0.6303\\n40\\n1h\\n0.6169\\nDNIS (CR = 2.5)\\n17\\n1h\\n0.6167\\n17\\n1h\\n0.6361\\n32\\n1h\\n0.6213\\nTable 3: Comparison between DNIS and baselines on the CTR prediction task using Criteo dataset.\\nSearch Methods\\nFM\\nWide&Deep\\nDeepFM\\nParams\\nTime\\nCost\\nAUC\\nLogloss Params\\nTime\\nCost\\nAUC\\nLogloss Params\\nTime\\nCost\\nAUC\\nLogloss\\n(M)\\n(M)\\n(M)\\nGrid Search\\n441\\n16h\\n0.7987\\n0.4525\\n254\\n16h\\n0.8079\\n0.4435\\n382\\n14h\\n0.8080\\n0.4435\\nRandom Search\\n73\\n12h\\n0.7997\\n0.4518\\n105\\n16h\\n0.8084\\n0.4434\\n105\\n12h\\n0.8084\\n0.4434\\nMDE\\n397\\n16h\\n0.7986\\n0.4530\\n196\\n16h\\n0.8076\\n0.4439\\n396\\n16h\\n0.8077\\n0.4438\\nDNIS (unpruned)\\n441\\n3h\\n0.8004\\n0.4510\\n395\\n3h\\n0.8088\\n0.4429\\n416\\n3h\\n0.8090\\n0.4427\\nDNIS (CR = 20)\\n26\\n3h\\n0.8004\\n0.4510\\n29\\n3h\\n0.8087\\n0.4430\\n29\\n3h\\n0.8088\\n0.4428\\nDNIS (CR = 30)\\n17\\n3h\\n0.8004\\n0.4510\\n19\\n3h\\n0.8085\\n0.4432\\n20\\n3h\\n0.8086\\n0.4430\\nnumber of redundant dimensions for low-frequency fea-\\ntures. Third, among all the baselines, MDE performs the\\nbest on Movielens and Random Search performs the best\\non Criteo, while Grid Search gets the worst results on both\\ntasks. This veriﬁes the importance of applying mixed dimen-\\nsion embeddings to latent factor models. Fourth, we ﬁnd that\\nMF achieves better prediction performance on the rating pre-\\ndiction task than the other two model architectures. The rea-\\nson may be the overﬁtting problem of MLP and NeuMF that\\nresults in poor generalization. Besides, DeepFM show the\\nbest results on the CTR prediction task, suggesting that the\\nensemble of DNN and FM is beneﬁcial to improving CTR\\nprediction accuracy.\\nTable 4 shows the performance of DNIS and NIS-ME\\nwith respect to base embedding dimension K and embed-\\nding parameter size for top-k item recommendation. From\\nthe results, we can see that DNIS achieves the best perfor-\\nmance on most metrics (on average, the relative improve-\\nment over NIS-ME on Recall, MRR, and NDCG are 5.3%,\\n7.2%, and 2.7%, respectively). This indicates the effective-\\nness of DNIS by searching for the optimal mixed dimension\\nscheme in a differentiable manner. Besides, NIS-ME shows\\nconsistent improvements over NIS-SE, admitting the bene-\\nﬁt of replacing single embedding dimension with mixed di-\\nmensions.\\n3.3\\nHyperparameter Investigation\\nWe investigate the effects of two important hyperparameters\\nK and L in DNIS. Figure 2a shows the performance change\\nof MF w.r.t. different settings of K. We can see that in-\\ncreasing K is beneﬁcial to reducing MSE. This is because a\\nMSE\\n0.60\\n0.61\\n0.62\\n0.63\\n0.64\\nK\\n8 64 128\\n256\\n512\\n1024\\n(a) MSE vs K.\\nMSE\\n0.608\\n0.61\\n0.612\\n0.614\\n0.616\\nL\\n1 5\\n10\\n20\\n40\\n80\\n(b) MSE vs L.\\nFigure 2: Effects of hyperparameters on the performance of\\nDNIS. We report the MSE results of MF on Movielens-20M\\ndataset w.r.t. different base embedding dimensions K and\\nfeature block numbers L.\\nlarger K allows a larger search space that could improve the\\nrepresentations of high-frequency features by giving more\\nembedding dimensions. Besides, we observe a marginal de-\\ncrease in performance gain. Speciﬁcally, the MSE is reduced\\nby 0.005 when K increases from 64 to 128, whereas the\\nMSE reduction is merely 0.001 when K changes from 512\\nto 1024. This implies that K may have exceeded the largest\\nnumber of dimensions required by all the features, leading\\nto minor improvements. Figure 2b shows the effects of the\\nnumber of feature blocks L. We ﬁnd that increasing L im-\\nproves the prediction performance of DNIS, and the perfor-\\nmance improvement decreases as L becomes larger. This is\\nbecause dividing features into more blocks facilitates a ﬁner-\\ngrained control on the embedding dimensions of different\\nfeatures, leading to more ﬂexible mixed dimension schemes.\\nSince both K and L affect the computation complexity of\\nDNIS, we suggest to choose reasonably large values for K\\nand L to balance the computational efﬁciency and predictive\\nTable 4: Comparison between DNIS and NIS-ME on Movielens-1M dataset. NIS-SE is a variant of NIS-ME method with a\\nshared number of embedding dimension. Here we use the results of the original paper [Joglekar et al. 2020].\\nModel\\nK\\n#Params\\nRecall@1\\nRecall@5\\n@Recall@10\\nMRR@5\\nMRR@10\\nNDCG@5\\nNDCG@10\\nNIS-SE\\n16\\n390k\\n9.32\\n35.70\\n55.31\\n18.22\\n20.83\\n22.43\\n28.63\\nNIS-ME\\n16\\n390k\\n9.41\\n35.90\\n55.68\\n18.31\\n20.95\\n22.60\\n28.93\\nDNIS (CR = 2)\\n16\\n390k\\n11.39\\n35.79\\n51.74\\n19.82\\n21.93\\n23.77\\n28.90\\nNIS-SE\\n16\\n195k\\n8.42\\n31.37\\n50.30\\n15.04\\n17.57\\n19.59\\n25.12\\nNIS-ME\\n16\\n195k\\n8.57\\n33.29\\n52.91\\n16.78\\n19.37\\n20.83\\n27.15\\nDNIS (CR = 4)\\n16\\n195k\\n11.15\\n33.34\\n49.62\\n18.74\\n20.88\\n22.35\\n27.58\\nNIS-SE\\n32\\n780k\\n9.90\\n37.50\\n55.69\\n19.18\\n21.60\\n23.70\\n29.58\\nNIS-ME\\n32\\n780k\\n10.40\\n38.66\\n57.02\\n19.59\\n22.18\\n24.28\\n30.60\\nDNIS (CR = 2)\\n32\\n780k\\n13.01\\n38.26\\n55.43\\n21.83\\n24.12\\n25.89\\n31.45\\nNIS-SE\\n32\\n390k\\n9.79\\n34.84\\n53.23\\n17.85\\n20.26\\n22.00\\n27.91\\nNIS-ME\\n32\\n390k\\n10.19\\n37.44\\n56.62\\n19.56\\n22.09\\n23.98\\n30.14\\nDNIS (CR = 4)\\n32\\n390k\\n11.95\\n35.98\\n51.92\\n20.27\\n22.39\\n24.15\\n29.30\\n0.46\\n0.53\\n0.47\\n0.43\\n0.39\\n0.35\\n0.31\\n0.26\\n0.24\\n0.14\\n0.59\\n0.65\\n0.59\\n0.5\\n0.47\\n0.44\\n0.4\\n0.36\\n0.37\\n0.31\\n0.71\\n0.75\\n0.7\\n0.61\\n0.58\\n0.52\\n0.5\\n0.52\\n0.48\\n0.46\\nα\\n0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nBlock Serial Number\\n1\\n2\\n3\\n4\\n5\\n6\\n7\\n8\\n9\\n10\\n(a) α in different feature blocks.\\n(b) Embedding dimension vs ηf.\\nLoss of DNIS after Pruning\\nAUC of DNIS after Pruning\\nLoss of Network Pruning\\nAUC of Network Pruning\\nAUC\\n0.77\\n0.78\\n0.79\\n0.80\\nLogloss\\n0.45\\n0.46\\n0.47\\n0.48\\n0.49\\n0.50\\n0.51\\nPruning Rate (%)\\n0\\n20\\n40\\n60\\n80\\n100\\n(c) Performance vs Pruning rates.\\nFigure 3: (a) The distribution of trained parameters α of the soft selection layer. Here we show the result of MF on Movielens\\ndataset, where L is set to 10. (b) The joint distribution plot of feature embedding dimensions and feature frequencies after\\ndimension pruning. (c) Comparison of DNIS and network pruning performance over different pruning rates.\\nperformance based on the application requirements.\\n3.4\\nAnalysis on DNIS Results\\nWe\\nﬁrst\\nstudy\\nthe\\nlearned\\nfeature\\ndimensions\\nof\\nDNIS through the learned soft selection layer α and\\nfeature embedding dimensions after dimension pruning.\\nFigure 3a depicts the distributions of the trained parameters\\nin α for the 10 feature blocks on Movielens. Recall that\\nthe blocks are sorted in the descending order of feature fre-\\nquency. We can see that the learned parameters in α for the\\nfeature blocks with lower frequencies converge to smaller\\nvalues, indicating that lower-frequency features tend to be\\nrepresented by smaller numbers of embedding dimensions.\\nFigure 3b provides the correlation between embedding\\ndimensions and feature frequency after dimension pruning.\\nThe results show that features with high frequencies end up\\nwith high embedding dimensions, whereas the dimensions\\nare more likely to be pruned for low-frequency features.\\nNevertheless, there is no linear correlationship between the\\nderived embedding dimension and the feature frequency.\\nNote that the embedding dimensions for low-frequency\\nfeatures scatter over a long range of numbers. This is\\nconsistent with the inferior performance of MDE which\\ndirectly determines the embedding dimensions of features\\naccording to their frequency.\\nWe\\nfurther\\ncompare\\nDNIS\\nwith\\nnetwork\\npruning\\nmethod [Han et al. 2015]. For illustration purpose, we pro-\\nvide the results of the FM model on Criteo dataset. Figure 3c\\nshows the performance of two methods on different pruning\\nrates (i.e., the ratio of pruned embedding weights). From the\\nresult, DNIS achieves better AUC and Logloss results than\\nnetwork pruning over all the pruning rates. This is because\\nDNIS optimizes feature embeddings with the gradients from\\nthe validation set, which beneﬁts the selection of predictive\\ndimensions, instead of simply removing redundant weights\\nin the embeddings.\\n4\\nConclusion\\nIn this paper, we propose Differentiable Neural Input Search\\n(DNIS), a method that searches for mixed features embed-\\nding dimensions in a differentiable manner through gradi-\\nent descent. The key idea of DNIS is to introduce a soft\\nselection layer that controls the signiﬁcance of each em-\\nbedding dimension, and optimize this layer according to\\nmodel’s validation performance. We propose a gradient nor-\\nmalization technique and a ﬁne-grained pruning procedure\\nin DNIS to produce a ﬂexible mixed embedding dimen-\\nsion scheme for different features. The proposed method is\\nmodel-agnostic, and can be incorporated with various exist-\\ning architectures of latent factor models. We conduct exper-\\niments on three public real-world recommendation datasets.\\nThe results show that DNIS achieves the best predictive per-\\nformance compared with existing neural input search meth-\\nods with fewer embedding parameters and less time cost.\\nReferences\\nBaker, B.; Gupta, O.; Naik, N.; and Raskar, R. 2017. De-\\nsigning Neural Network Architectures using Reinforcement\\nLearning. In 5th International Conference on Learning Rep-\\nresentations, ICLR 2017, Toulon, France, April 24-26, 2017,\\nConference Track Proceedings.\\nBergstra, J.; Yamins, D.; and Cox, D. D. 2013. Making a\\nScience of Model Search: Hyperparameter Optimization in\\nHundreds of Dimensions for Vision Architectures. In Pro-\\nceedings of the 30th International Conference on Machine\\nLearning, ICML 2013, Atlanta, GA, USA, 16-21 June 2013,\\n115–123.\\nCai, H.; Zhu, L.; and Han, S. 2019. ProxylessNAS: Direct\\nNeural Architecture Search on Target Task and Hardware. In\\n7th International Conference on Learning Representations,\\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nChen, L.; Collins, M. D.; Zhu, Y.; Papandreou, G.; Zoph,\\nB.; Schroff, F.; Adam, H.; and Shlens, J. 2018.\\nSearch-\\ning for Efﬁcient Multi-Scale Architectures for Dense Image\\nPrediction. In Advances in Neural Information Processing\\nSystems 31: Annual Conference on Neural Information Pro-\\ncessing Systems 2018, NeurIPS 2018, 3-8 December 2018,\\nMontr´eal, Canada, 8713–8724.\\nChen, Y.; Chen, B.; He, X.; Gao, C.; Li, Y.; Lou, J.; and\\nWang, Y. 2019. λOpt: Learn to Regularize Recommender\\nModels in Finer Levels. In Proceedings of the 25th ACM\\nSIGKDD International Conference on Knowledge Discov-\\nery & Data Mining, KDD 2019, Anchorage, AK, USA, Au-\\ngust 4-8, 2019, 978–986.\\nCheng, H.; Koc, L.; Harmsen, J.; Shaked, T.; Chandra, T.;\\nAradhye, H.; Anderson, G.; Corrado, G.; Chai, W.; Ispir, M.;\\nAnil, R.; Haque, Z.; Hong, L.; Jain, V.; Liu, X.; and Shah, H.\\n2016. Wide & Deep Learning for Recommender Systems.\\nIn Proceedings of the 1st Workshop on Deep Learning for\\nRecommender Systems, DLRS@RecSys 2016, Boston, MA,\\nUSA, September 15, 2016, 7–10.\\nCheng, W.; Shen, Y.; Zhu, Y.; and Huang, L. 2018. DELF: A\\nDual-Embedding based Deep Latent Factor Model for Rec-\\nommendation. In IJCAI’18, July 13-19, 2018, Stockholm,\\nSweden, 3329–3335.\\nCheng, Y.; Wang, D.; Zhou, P.; and Zhang, T. 2017. A sur-\\nvey of model compression and acceleration for deep neural\\nnetworks. arXiv preprint arXiv:1710.09282 .\\nColson, B.; Marcotte, P.; and Savard, G. 2007. An overview\\nof bilevel optimization. Annals OR 153(1): 235–256.\\nCovington, P.; Adams, J.; and Sargin, E. 2016. Deep Neu-\\nral Networks for YouTube Recommendations. In Proceed-\\nings of the 10th ACM Conference on Recommender Systems,\\nBoston, MA, USA, September 15-19, 2016, 191–198.\\nDomhan, T.; Springenberg, J. T.; and Hutter, F. 2015. Speed-\\ning Up Automatic Hyperparameter Optimization of Deep\\nNeural Networks by Extrapolation of Learning Curves. In\\nProceedings of the Twenty-Fourth International Joint Con-\\nference on Artiﬁcial Intelligence, IJCAI 2015, Buenos Aires,\\nArgentina, July 25-31, 2015, 3460–3468.\\nElsken, T.; Metzen, J. H.; and Hutter, F. 2019.\\nEfﬁcient\\nMulti-Objective Neural Architecture Search via Lamarck-\\nian Evolution. In 7th International Conference on Learning\\nRepresentations, ICLR 2019, New Orleans, LA, USA, May\\n6-9, 2019.\\nFranceschi, L.; Frasconi, P.; Salzo, S.; Grazzi, R.; and Pontil,\\nM. 2018. Bilevel Programming for Hyperparameter Opti-\\nmization and Meta-Learning. In Proceedings of the 35th In-\\nternational Conference on Machine Learning, ICML 2018,\\nStockholmsm¨assan, Stockholm, Sweden, July 10-15, 2018,\\n1563–1572.\\nFrankle, J.; and Carbin, M. 2019. The Lottery Ticket Hy-\\npothesis: Finding Sparse, Trainable Neural Networks.\\nIn\\n7th International Conference on Learning Representations,\\nICLR 2019, New Orleans, LA, USA, May 6-9, 2019.\\nGinart, A.; Naumov, M.; Mudigere, D.; Yang, J.; and Zou,\\nJ. 2019.\\nMixed Dimension Embeddings with Applica-\\ntion to Memory-Efﬁcient Recommendation Systems. arXiv\\npreprint arXiv:1909.11810 .\\nGuo, H.; Tang, R.; Ye, Y.; Li, Z.; and He, X. 2017. DeepFM:\\nA Factorization-Machine based Neural Network for CTR\\nPrediction. In Proceedings of the Twenty-Sixth International\\nJoint Conference on Artiﬁcial Intelligence, IJCAI 2017, Mel-\\nbourne, Australia, August 19-25, 2017, 1725–1731.\\nHan, S.; Pool, J.; Tran, J.; and Dally, W. 2015. Learning\\nboth weights and connections for efﬁcient neural network. In\\nAdvances in neural information processing systems, 1135–\\n1143.\\nHarper, F. M.; and Konstan, J. A. 2016. The MovieLens\\nDatasets: History and Context. ACM Trans. Interact. Intell.\\nSyst. 5(4): 19:1–19:19.\\nHe, X.; Liao, L.; Zhang, H.; Nie, L.; Hu, X.; and Chua, T.\\n2017. Neural Collaborative Filtering. In Proceedings of the\\n26th International Conference on World Wide Web, WWW\\n2017, Perth, Australia, April 3-7, 2017, 173–182.\\nJoglekar, M. R.; Li, C.; Chen, M.; Xu, T.; Wang, X.; Adams,\\nJ. K.; Khaitan, P.; Liu, J.; and Le, Q. V. 2020. Neural In-\\nput Search for Large Scale Recommendation Models.\\nIn\\nKDD ’20: The 26th ACM SIGKDD Conference on Knowl-\\nedge Discovery and Data Mining, Virtual Event, CA, USA,\\nAugust 23-27, 2020, 2387–2397.\\nLabs, C. 2014.\\nKaggle Display Advertising Challenge\\nDataset,\\nhttp://labs.criteo.com/2014/02/kaggle-display-\\nadvertising-challenge-dataset/.\\nLi, H.; Kadav, A.; Durdanovic, I.; Samet, H.; and Graf, H. P.\\n2017. Pruning Filters for Efﬁcient ConvNets. In 5th In-\\nternational Conference on Learning Representations, ICLR\\n2017, Toulon, France, April 24-26, 2017, Conference Track\\nProceedings.\\nLi, L.; and Talwalkar, A. 2019. Random Search and Repro-\\nducibility for Neural Architecture Search. In Proceedings of\\nthe Thirty-Fifth Conference on Uncertainty in Artiﬁcial In-\\ntelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, 129.\\nLian, J.; Zhou, X.; Zhang, F.; Chen, Z.; Xie, X.; and Sun,\\nG. 2018. xDeepFM: Combining Explicit and Implicit Fea-\\nture Interactions for Recommender Systems. In KDD’18,\\nLondon, UK, August 19-23, 2018, 1754–1763.\\nLiu, H.; Simonyan, K.; and Yang, Y. 2019. DARTS: Dif-\\nferentiable Architecture Search. In 7th International Con-\\nference on Learning Representations, ICLR 2019, New Or-\\nleans, LA, USA, May 6-9, 2019.\\nLiu, Z.; Sun, M.; Zhou, T.; Huang, G.; and Darrell, T. 2019.\\nRethinking the Value of Network Pruning. In 7th Interna-\\ntional Conference on Learning Representations, ICLR 2019,\\nNew Orleans, LA, USA, May 6-9, 2019.\\nMaclaurin, D.; Duvenaud, D.; and Adams, R. P. 2015.\\nGradient-based Hyperparameter Optimization through Re-\\nversible Learning.\\nIn Proceedings of the 32nd Interna-\\ntional Conference on Machine Learning, ICML 2015, Lille,\\nFrance, 6-11 July 2015, 2113–2122.\\nMendoza, H.; Klein, A.; Feurer, M.; Springenberg, J. T.; and\\nHutter, F. 2016. Towards Automatically-Tuned Neural Net-\\nworks. In Proceedings of the 2016 Workshop on Automatic\\nMachine Learning, AutoML 2016, co-located with 33rd In-\\nternational Conference on Machine Learning (ICML 2016),\\nNew York City, NY, USA, June 24, 2016, 58–65.\\nMiller, G. F.; Todd, P. M.; and Hegde, S. U. 1989. Designing\\nNeural Networks using Genetic Algorithms.\\nIn Proceed-\\nings of the 3rd International Conference on Genetic Algo-\\nrithms, George Mason University, Fairfax, Virginia, USA,\\nJune 1989, 379–384.\\nMolchanov, P.; Tyree, S.; Karras, T.; Aila, T.; and Kautz,\\nJ. 2017. Pruning Convolutional Neural Networks for Re-\\nsource Efﬁcient Inference. In 5th International Conference\\non Learning Representations, ICLR 2017, Toulon, France,\\nApril 24-26, 2017, Conference Track Proceedings.\\nPark, J.; Naumov, M.; Basu, P.; Deng, S.; Kalaiah, A.; Khu-\\ndia, D.; Law, J.; Malani, P.; Malevich, A.; Nadathur, S.; et al.\\n2018.\\nDeep learning inference in facebook data centers:\\nCharacterization, performance optimizations and hardware\\nimplications. arXiv preprint arXiv:1811.09886 .\\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\\nDesmaison, A.; K¨opf, A.; Yang, E.; DeVito, Z.; Raison, M.;\\nTejani, A.; Chilamkurthy, S.; Steiner, B.; Fang, L.; Bai, J.;\\nand Chintala, S. 2019. PyTorch: An Imperative Style, High-\\nPerformance Deep Learning Library. In Advances in Neu-\\nral Information Processing Systems 32: Annual Conference\\non Neural Information Processing Systems 2019, NeurIPS\\n2019, 8-14 December 2019, Vancouver, BC, Canada, 8024–\\n8035.\\nPedregosa, F. 2016. Hyperparameter optimization with ap-\\nproximate gradient.\\nIn Proceedings of the 33nd Interna-\\ntional Conference on Machine Learning, ICML 2016, New\\nYork City, NY, USA, June 19-24, 2016, 737–746.\\nReal, E.; Aggarwal, A.; Huang, Y.; and Le, Q. V. 2019. Reg-\\nularized Evolution for Image Classiﬁer Architecture Search.\\nIn The Thirty-Third AAAI Conference on Artiﬁcial Intelli-\\ngence, AAAI 2019, The Thirty-First Innovative Applications\\nof Artiﬁcial Intelligence Conference, IAAI 2019, The Ninth\\nAAAI Symposium on Educational Advances in Artiﬁcial In-\\ntelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27\\n- February 1, 2019, 4780–4789.\\nReal, E.; Moore, S.; Selle, A.; Saxena, S.; Suematsu, Y. L.;\\nTan, J.; Le, Q. V.; and Kurakin, A. 2017. Large-Scale Evo-\\nlution of Image Classiﬁers. In Proceedings of the 34th In-\\nternational Conference on Machine Learning, ICML 2017,\\nSydney, NSW, Australia, 6-11 August 2017, 2902–2911.\\nRendle, S. 2010. Factorization Machines. In ICDM 2010,\\nThe 10th IEEE International Conference on Data Mining,\\nSydney, Australia, 14-17 December 2010, 995–1000.\\nVirtanen, P.; Gommers, R.; Oliphant, T. E.; Haberland,\\nM.; Reddy, T.; Cournapeau, D.; Burovski, E.; Peterson, P.;\\nWeckesser, W.; Bright, J.; van der Walt, S. J.; Brett, M.; Wil-\\nson, J.; Jarrod Millman, K.; Mayorov, N.; Nelson, A. R. J.;\\nJones, E.; Kern, R.; Larson, E.; Carey, C.; Polat, ˙I.; Feng,\\nY.; Moore, E. W.; Vand erPlas, J.; Laxalde, D.; Perktold, J.;\\nCimrman, R.; Henriksen, I.; Quintero, E. A.; Harris, C. R.;\\nArchibald, A. M.; Ribeiro, A. H.; Pedregosa, F.; van Mul-\\nbregt, P.; and Contributors, S. . . 2020. SciPy 1.0: Funda-\\nmental Algorithms for Scientiﬁc Computing in Python. Na-\\nture Methods 17: 261–272.\\nXie, S.; Zheng, H.; Liu, C.; and Lin, L. 2019.\\nSNAS:\\nstochastic neural architecture search. In 7th International\\nConference on Learning Representations, ICLR 2019, New\\nOrleans, LA, USA, May 6-9, 2019.\\nZhao, X.; Wang, C.; Chen, M.; Zheng, X.; Liu, X.; and Tang,\\nJ. 2020.\\nAutoEmb: Automated Embedding Dimensional-\\nity Search in Streaming Recommendations. arXiv preprint\\narXiv:2002.11252 .\\nZhong, Z.; Yan, J.; Wu, W.; Shao, J.; and Liu, C. 2018. Prac-\\ntical Block-Wise Neural Network Architecture Generation.\\nIn 2018 IEEE Conference on Computer Vision and Pattern\\nRecognition, CVPR 2018, Salt Lake City, UT, USA, June 18-\\n22, 2018, 2423–2432.\\nZoph, B.; and Le, Q. V. 2017. Neural Architecture Search\\nwith Reinforcement Learning.\\nIn 5th International Con-\\nference on Learning Representations, ICLR 2017, Toulon,\\nFrance, April 24-26, 2017, Conference Track Proceedings.\\nZoph, B.; Vasudevan, V.; Shlens, J.; and Le, Q. V. 2018.\\nLearning Transferable Architectures for Scalable Image\\nRecognition. In 2018 IEEE Conference on Computer Vi-\\nsion and Pattern Recognition, CVPR 2018, Salt Lake City,\\nUT, USA, June 18-22, 2018, 8697–8710.\\n',\n",
       " '2103.06124': 'SEMANTICALLY CONSTRAINED MEMORY ALLOCATION\\n(SCMA) FOR EMBEDDING IN EFFICIENT RECOMMENDATION\\nSYSTEMS\\nAditya Desai∗\\nDepartment of Computer Science\\nRice University\\nHouston, Texas\\napd10@rice.edu\\nYanzhou Pan∗\\nDepartment of Computer Science\\nRice University\\nHouston, Texas\\nyp24@rice.edu\\nKuangyuan Sun\\nDepartment of Computer Science\\nRice University\\nHouston, Texas\\nks94@rice.edu\\nLi Chou\\nDepartment of Computer Science\\nRice University\\nHouston, Texas\\nlchou@rice.edu\\nAnshumali Shrivastava\\nDepartment of Computer Science\\nRice University\\nHouston, Texas\\nanshumali@rice.edu\\nABSTRACT\\nDeep learning-based models are utilized\\nto achieve state-of-the-art performance\\nfor recommendation systems.\\nA key\\nchallenge for these models is to work\\nwith millions of categorical classes or\\ntokens.\\nThe standard approach is to\\nlearn end-to-end, dense latent represen-\\ntations or embeddings for each token.\\nThe resulting embeddings require large\\namounts of memory that blow up with\\nthe number of tokens. Training and in-\\nference with these models create stor-\\nage, and memory bandwidth bottlenecks\\nleading to signiﬁcant computing and\\nenergy consumption when deployed in\\npractice. To this end, we present the\\nproblem of Memory Allocation under\\nbudget for embeddings and propose a\\nnovel formulation of memory shared em-\\nbedding, where memory is shared in pro-\\nportion to the overlap in semantic infor-\\nmation. Our formulation admits a prac-\\ntical and efﬁcient randomized solution\\nwith Locality sensitive hashing based\\nMemory Allocation (LMA). We demon-\\nstrate a signiﬁcant reduction in the mem-\\nory footprint while maintaining perfor-\\nmance.\\nIn particular, our LMA em-\\nbeddings achieve the same performance\\ncompared to standard embeddings with\\na 16× reduction in memory footprint.\\n*First two authors have equal contribution\\nMoreover, LMA achieves an average im-\\nprovement of over 0.003 AUC across\\ndifferent memory regimes than standard\\nDLRM models on Criteo and Avazu\\ndatasets\\n1\\nIntroduction\\nRecommendation systems are at the core of business\\nfor companies such as Amazon, Facebook, NetFlix, and\\nGoogle. These companies offer a wide array of products,\\nmovies, ads, etc. for customers to view and to choose from.\\nTherefore, developing automated, intelligent, and person-\\nalized recommendation systems help guide customers to\\nmake more informed choices. It is worthwhile to note\\nthe signiﬁcant monetary impact of recommendation model\\naccuracy on the aforementioned companies. With the size\\nof notional business, even an increase of 0.001 in accu-\\nracy/AUC metrics implies considerable gains in revenue\\nand customer experience. However, categorical features\\n(e.g., product history, pages liked, etc.) dominate most\\nof the recommendation data [1, 2], thereby posing new\\nmodeling challenges for learning. Following the lines of\\nnatural language processing [3, 4], state-of-the-art meth-\\nods in recommendation models [1, 5] have found success\\nwith mapping each of the category values in the feature to\\na dense representation. These representations are learned\\nand stored in memory tables called embedding tables.\\nHeavy embedding tables and memory bandwidth bot-\\ntleneck: Embedding tables store learned dense represen-\\ntation of each category value. If the set of all categories\\nis S and the embedding dimension is d. The embedding\\ntable size would be |S|×d. With the number of categories\\narXiv:2103.06124v1  [cs.IR]  24 Feb 2021\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nas large as tens of millions for each feature, embedding ta-\\nbles can take up over 99.9% of the total memory. Namely,\\nmemory footprint can be multiple gigabytes or even ter-\\nabytes [6, 7, 8]. In practice, deploying these large mod-\\nels often requires the model to be decomposed and dis-\\ntributed across different machines due to memory capacity\\nrestrictions [9]. Extensive memory utilization creates mem-\\nory bandwidth issues due to the highly irregular locality\\nof accesses making training and inference considerably\\nslower [1]. This issue exacerbates further when multiple\\nmodels need to be co-located on a single machine [9].\\nImpact of improving memory usage of Embedding Ta-\\nbles: Memory consumption of embedding tables is a se-\\nvere problem in recommendation models. Improving mem-\\nory utilization can improve recommendation systems on\\nvarious fronts. 1) It has been observed that larger embed-\\nding size in the model leads to better performance [10].\\nBetter memory utilization would imply scope to train and\\ndeploy complex models. 2) Lower memory footprint will\\nimprove the training and inference speed. With changing\\nconsumer interests, recommendation data inherently suf-\\nfers from concept shift [11], requiring a frequent refresh of\\nmodels. With faster training, models can be retrained more\\nfrequently, improving their average performance. Hence,\\nmemory utilization forms a critical problem requiring at-\\ntention.\\nDeep learning recommendation model (DLRM) [1] gave\\nrise to an increased interest in constructing more memory-\\nefﬁcient embeddings. Recent SOTA works include com-\\npositional embedding using complementary partition [12]\\nand mixed dimension embeddings [6]. A simple mem-\\nory sharing scheme for weight matrices in deep learning\\nmodels was proposed by [13]. However, embeddings for\\ntokens are weight matrices that have a structure for which\\nwe can reduce the memory burden in an intelligent man-\\nner by sharing memory for similar concept tokens. For\\nexample, if two tokens represent the concepts “Nike” and\\n“Adidas,” we would expect the pair to share more weights\\ncompared to “Nike” and “Jaguar.” In this paper, we ap-\\nproach the problem of learning embedding tables under\\nmemory budget by solving a generic problem, which we\\nrefer to as semantically constrained memory allocation\\n(SCMA) for embeddings. SCMA is a hard combinatorial\\nallocation problem since it involves millions of variables\\nand constraints. Surprisingly, as we show later, that there\\nis a very neat dynamic allocation of memory using locality\\nsensitive hashing which solves SCMA in approximation.\\nThe memory allocation can be done in an online consistent\\nmanner for each token with negligible overhead.\\nThis paper is organized as follows: we ﬁrst formally de-\\nscribe the problem and notation in section 2, followed by\\na recap of hashing schemes in section 3 and a generic so-\\nlution to the problem in section 4. Section 5 onward we\\nfocus on applying the solution to DLRM problem. Section\\n6 discusses some related work and we present experimental\\nevaluation in section 7.\\n...\\nFigure 1: Embedding table E ∈R|S|×d. x ∈{0, . . . , |S|−\\n1} is a categorical feature represented by a one-hot vector.\\nF is a universal function approximator typically parame-\\nterized by a neural network.\\n2\\nSemantically Constrained Memory\\nAllocation (SCMA) for Embeddings\\nLet S = {0, 1, . . . , |S|−1} denote the set of all values\\nfor all categorical features in the dataset. The embedding\\ntable E ∈R|S|×d is a matrix such that each row represents\\nembeddings, each with dimension d, for all values in S. An\\nembedding, ev, for a particular value, is retrieved by ev =\\nE[v, :] (see Figure 1). The embedding table E imposes a\\nsimilarity structure S w.r.t a similarity measure obtained by\\na kernel function φ(., .). S ∈R|S|×|S| and each entry of\\nthis matrix S[v1, v2] = φ(E[v1, :], E[v2, :]) also denoted\\nas φ(v1, v2)\\nThe problem of end-to-end learning of full embedding\\ntable E is well-known [1, 14, 3, 15, 4]. In this paper,\\nwe consider the problem of learning E under a memory\\nbudget m. Let M be the memory such that |M| = m.\\nIf |S|×d > m, then multiple elements of the embedding\\ntable have to share memory locations in M. We formally\\ndeﬁne allocation function A as the mapping from elements\\nof E to the actual memory locations in M.\\nDeﬁnition 1 (Allocation Function) Allocation function\\nfor an embedding table E ∈R|S|×d using the memory\\nM with budget |M| = m, is deﬁned as the following map.\\nA : {0, . . . , |S|−1} →{0, 1}d×m\\ns.t.\\n∀v ∈S, ∀i ∈{0, . . . , d−1},\\nX\\nj\\n(A(v)[i, j]) = 1.\\nThe ith row of the matrix output by A, for any value v is a\\none-hot encoding of the location to which the ith element\\nof the embedding vector for v maps to in M. Using the\\nallocation function, we can retrieve the embedding by,\\nE[v, i] = M[A(v)[i, :]]\\nwhere we assume mask-based retrieval on M. We next de-\\nﬁne the notion of shared memory between two embeddings\\nunder a allocation function A.\\nDeﬁnition 2 (Consistent Memory Sharing) The\\nFrac-\\ntion of Consistently Shared Memory (fA) between two\\nembeddings of size d for values v1 and v2 from E under\\nallocation A is deﬁned as\\nfA(v1, v2) = 1\\nd⟨A(v1), A(v2)⟩F\\nwhere ⟨., .⟩F is the Frobenius inner product.\\n2\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nWe can describe various allocation schemes in terms of the\\nallocation function. For example, Afull describes full em-\\nbedding which is possible when m>(|S|d). Ah describes\\nthe na¨ıve hashing trick (hash function h) based allocation\\n(see section 6 for details) which works with any memory\\nbudget. Speciﬁcally, ∀v ∈S and i ∈{0, .., d−1}, we have\\nthe following.\\nAfull(v)[i, j] = 1,\\niff j = mv + i.\\nAh(v)[i, j] = 1,\\niff j = h(v, i).\\nIt is easy to verify that for every pair of values v1 and v2\\nfAfull(v1, v2) = 0 whereas fAh(v1, v2) = 0 is random\\nvariable with expected value 1/m. We now deﬁne the\\nnotation we will use throughout the paper.\\nDeﬁnition 3 (General Memory Allocation (GMA))\\nLet E∗be the true embedding table which encodes the\\nsimilarity structure S∗. Let E be an embedding table\\nrecovered from M with budget m under allocation A\\nand S be the semantic similarity encoded by E Let both\\nsimilarities be encoded by the kernel function φ(, ). We\\nwill refer to this as General Memory Allocation (GMA)\\nsetup\\nThought experiment: We can deﬁne the problem of op-\\ntimal allocation under memory budget when (m < |S|d),\\nunder GMA setup as\\nargmin\\nA\\nmin\\nM ζ(S∗, S)\\nwhere ζ is a metric on R|S|×|S| (e.g., Euclidean). In or-\\nder to incorporate the learning of M, we pose the prob-\\nlem as ﬁnding A with best possible associated M con-\\nsidering that if we choose A beforehand and then learn\\nM, the learning process will choose the best M. Solv-\\ning this exact problem appears to be hard. Instead, let\\nus think about an allocation scheme for which we have\\nsome evidence of the existence of suitable M. Let us\\nconsider a M with each element independently initialized\\nusing Bernoulli(0.5, {−1, +1}). If we choose an alloca-\\ntion with constraints based on similarity structure S∗as\\nfA(v1, v2) = S∗[v1, v2]\\n∀v1, v2 ∈S\\nthen one can verify (as we will show in Theorem 2) that\\nunder this random initialization of memory, pairwise co-\\nsine similarity of embeddings of any two values v1 and v2\\nretrieved from M via A, denoted as Cs(v1, v2), is a ran-\\ndom variable with expectation E(Cs(v1, v2)) = S∗[v1, v2]\\nand variance Var(Cs(v1, v2)) ∝1\\nd. Hence, this provides\\nevidence that for a semantic similarity based shared alloca-\\ntion, AS∗, there is an assignment to M, which can produce\\nreasonably small ζ(S∗, S). We interpret the ±1 assignment\\nto each element of embedding as membership to particu-\\nlar concepts, and the overlap in membership determines\\nthe similarity. Following this insight, we formally deﬁne\\nthe semantically constrained memory problem allocation\\n(SCMA) for embeddings as follows.\\nDeﬁnition 4 (SCMA Problem) Under the GMA setup\\n(see Def. 3), Semantically Constrained Memory Alloca-\\ntion (SCMA) is a problem to ﬁnd allocation A, under\\nthe constraints that for every pair i, j ∈S, we have\\nfA(i, j) = S∗[i, j].\\nSCMA problem can be posed as a mixed integer program-\\nming (MIP) problem with O(|S|2) constraints appearing\\nfrom similarity constraints along with O(|S|d) sparsity\\nconstraint that the allocation matrix demands. Applying\\nMIP to solve the SCMA problem has the following draw-\\nbacks: 1) MIP is computationally intractable for large-\\nscale problems; 2) The solution of MIP needs to be stored\\nexplicitly so it is memory expensive; and 3) In case of\\naddition of new values to categorical features, the problem\\nneeds to be repeatedly solved. Due to the difﬁculty of\\nSCMA, in this paper, we formulate and solve the problem\\nusing a randomized approach which we term randomized\\nSCMA (RSCMA). We deﬁne RSCMA as follows.\\nDeﬁnition 5 (RSCMA Problem) Under the GMA setup\\n(see Def. 3), the Randomized Semantically Constrained\\nMemory Allocation (RSCMA) is a problem is to ﬁnd\\nallocation A under the constraints that for every pair\\ni, j ∈{0, ..., |S| −1}, we have\\n|fA(i, j) −φ(i, j)| ≤ϵ\\nwith probability (1 −δ) for some small ϵ, δ > 0.\\nCompared to applying MIP solvers to the SCMA problem,\\nour approach to RSCMA has the following advantages:\\n1) LSH based solution is cheaper to compute, 2) storage\\ncost is cheap, and 3) the addition of new values does not\\nrequire resolving the problem. Although the exact similar-\\nity between embeddings is not known a priori, a notion of\\nsimilarity exists in the data. For example, term-document\\nestablishes a similarity between terms based on Jaccard\\nsimilarity on term-document vectors. We use this simi-\\nlarity to impose a structure on the allocation of memory.\\nNamely, we leverage locality sensitive hashing (LSH) to\\nsolve the RSCMA problem.\\n3\\nHashing Schemes\\nWe describe important hashing schemes from randomized\\ndata structures that will be used in our solution to RSCMA.\\n3.1\\nUniversal Hashing\\nConsider the problem of mapping each element of a large\\nuniverse U to a smaller range {0, . . . , r−1}. A family\\nof hash functions H = {h : U →{0, . . . , r−1}} is k\\nuniversal if for any k distinct elements (x1, . . . , xk) ∈U\\nand any k indices (β1, . . . , βk) ∈{0, . . . , r−1}, we have\\nPrh∈H((h(x1) = β1) ∧. . . ∧(h(xk) = βk) = 1/rk [16].\\nWe utilize the following k-universal hash function family.\\nH = {h(x) = (a0 + Σk−1\\ni=1 aixi) % P % r\\n| a0 ∈{0, ..., P−1}, ai ∈{1, ..., P−1}},\\nwhere P is a large prime number. An instance of hash\\nfunction drawn from this family is stored by using only k\\nintegers {ai}k−1\\ni=0 .\\n3.2\\nLocality Sensitive Hashing\\nLocality sensitive hashing (LSH) [17] is a popular tool\\nused in approximate near neighbour search. Consider a\\nfunction l : U →{0, ..., r−1}. If l is drawn randomly\\nfrom a LSH family, say L, then the probability that two\\n3\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nFigure 2: shows consistent sharing of 2 locations when\\nusing LMA\\nelements x and y share same hash value (probability of\\ncollision pc) is dependent on a deﬁned similarity measure,\\nSim, between a and b. Speciﬁcally,\\nPrl∈L(l(x) == l(y)) ∝Sim(x, y).\\nThis probability of collision deﬁnes a kernel function\\nφ(x, y), which is bounded 0 ≤φ(x, y) ≤1, symmetric\\nφ(x, y) = φ(y, x), and reﬂective φ(x, x) = 1. We can cre-\\nate multiple LSH families parameterized by k, Lk, from\\na given LSH family L by using k-independently drawn\\nfunctions from L. Let {li}k\\ni=1 be k independently drawn\\nfunctions from L. The new LSH function ψ ∈Lk and its\\nkernel function is deﬁned as\\nψ(x) = (l1(x), l2(x), ...lk(x));\\nφ(x, y)Lk = φ(x, y)k\\nL.\\nWe call parameter k, the power of LSH functions. The\\nrange of certain LSH functions, particularly functions with\\nlarge power, can be extremely large and needs rehashing\\ninto a range, say {0, .., r−1}. This is generally achieved\\nusing additional universal hash functions, say h with range\\nr. Let the rehashed version of the function L be denoted\\nby Lr. Then, the kernel of this rehashed LSH is\\nlr(x) = h(l(x)); φ(x, y)Lr = φ(x, y)L + 1 −φ(x, y)L\\nr\\n.\\n3.3\\nMinwise Hashing\\nThe minwise hash function is a LSH function that take sets\\nas inputs. The minwise family is as deﬁned below.\\nLmh = {lπ|π : U →U, π is a permutation},\\nlπ(A) = min({π(x)|x ∈A})\\nwhere A ⊆U\\nFor a particular function, lπ, the hash value of A is the min-\\nimum of the permuted values of elements in A. As it turns\\nout, the kernel function deﬁned by the collision probability\\nof the minwise hash family is the Jaccard Similarity (J).\\nJ measures the similarity between two given sets A and B\\nas J(A, B) = |A ∩B|/|A ∪B|. It is easy to check that\\nφ(A, B)Lmh = J(A, B).\\n4\\nLSH based Memory Allocation(LMA):\\nSolution to RSCMA\\nConsider the standard GMA setup (see Def. 3) with the\\nsemantic structure S∗deﬁned by a LSH kernel φ(, ) We\\nprovide an LSH based memory allocation (LMA) solution\\nto this RSCMA. Let the LSH family corresponding to this\\nkernel be L. As deﬁned in Section 3.2, the probability\\nof collision of values v1 and v2 for corresponding LSH\\n(a) fAL(v1, v2) vs φ(v1, v2)\\n(b) Cs(v1, v2) vs φ(v1, v2)\\nFigure 3: µ ∓1.96σ regions for different values of d as per\\ntheorems 1 and 2. For ˆfAL this is 95% conﬁdence intervals.\\nFor Cs, this is very close to 95% conﬁdence interval when\\nm is large\\nfunction l and rehashed LSH function lr can be written as\\nPrl∈L(l(v1) == l(v2)) = φ(v1, v2),\\nPrl∈L(lr(v1) == lr(v2)) = φ(v1, v2) + 1 −φ(v1, v2)\\nm\\n.\\nWe use d independently drawn LSH functions {l(i)}d\\ni=1.\\nThe LMA solution deﬁnes the allocation AL as\\nAL(v)[i, :] = one-hot(l(i)\\nr (v))\\n∀v ∈S,\\nwhere one-hot : {0, ..m −1} →{0, 1}m such that for any\\narbitrary i ∈{0, .., m −1}, i ̸= j, one-hot(i)[j] = 0 and\\none-hot(i)[i] = 1. LMA scheme is illustrated in Figure 2.\\nIn the following theorem, we prove that LMA with the allo-\\ncation deﬁned by AL indeed solves the RSCMA problem.\\nThe proof of this theorem is present in the Supplementary.\\nTheorem 1 (LMA solves RSCMA) Under\\nthe\\nGMA\\nsetup (see def 3), for any two values v1 and v2, the fraction\\nof consistently shared memory fAL as per allocation AL\\nproposed by LMA is a random variable with distribution,\\nE(fAL(v1, v2)) = Γ = φ(v1, v2) + 1 −φ(v1, v2)\\nm\\n,\\nV(fAL(v1, v2)) = Γ(1 −Γ)\\nd\\n,\\nPr\\n\\x12\\n|fAL(v1, v2)−φ(v1, v2)| > ηΓ + 1−φ(v1, v2)\\nm\\n\\x13\\n≤2 exp\\n\\x1a−dΓη2\\n3\\n\\x1b\\n,\\nfor all η > 0. Hence, LMA solves RSCMA with ϵ =\\nηΓ + 1−φ(v1,v2)\\nm\\nand δ = 2 exp\\nn\\n−dΓη2\\n3\\no\\n.\\nProof sketch: Proof consists of analyzing the random\\nvariable for the fraction fAL(v1, v2) and applying Chernoff\\nconcentration inequality to obtain the tail bounds.\\nInterpretation: A reasonable memory M of 10Mb would\\nhave |M| > 106. Hence for all practical purposes, we can\\nignore the 1/m terms above. Then, the consistently shared\\nfraction has expected value φ(v1, v2) and variance that is\\nproportional to 1/d. A good way to visualize the fact that\\nLMA indeed gives a solution to RSCMA problem is to see\\nthe 95% conﬁdence interval of the fraction, fAL(v1, v2),\\n4\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nagainst the value of φ(v1, v2) as shown in Figure 3. The\\nparameter η is a standard parameter that controls the trade-\\noff between error (ϵ) and conﬁdence (1 −δ). We can\\nchoose η very small to reduce error, but then we lose\\nconﬁdence, and instead if we choose η large enough to\\nreduce δ, hence increase conﬁdence, then we have more\\nerror.\\nNext, we prove that if we use LMA to solve RSCMA, we\\nindeed provide an allocation, for which there is an assign-\\nment of values to M which can lead to an embedding table\\nE, whose associated similarity S as measured by cosine\\nsimilarity is closely distributed around S∗and hence gives\\nsmaller ζ(S, S∗) (notation as introduced in section 2).\\nTheorem 2 (Existence of M with LMA for S∗) Under\\nthe GMA setup (see Def. 3), let us initialize each element\\nof M independently from a Bernoulli(0.5, {−1, +1}).\\nThen, the embedding table E generated via LMA on\\nthis memory, has , for every pair of values v1 and v2,\\nthe cosine similarity Cs(E[v1, :], E[v2, :]), denoted by\\nCs(v1, v2) is distributed as\\nE(Cs(v1, v2)) = Γ = φ(v1, v2) + 1 −φ(v1, v2)\\nm\\n,\\nV(Cs(v1, v2)) = 1 −Γ2\\nd\\n+ 2(1 −Γ)(d −1)\\ndm2\\n,\\nPr\\n\\x12\\n|Cs(v1, v2) −φ(v1, v2)| ≥ηΓ + 1 −φ(v1, v2)\\nm\\n\\x13\\n≤1 −Γ2\\ndη2Γ2 for any η > 0.\\nProof sketch: Proof consists of analyzing the random\\nvariable for the cosine similarity Cs(E[v1, :], E[v2, :]) and\\napplying Chebyshev’s concentration inequality to obtain\\nthe tail bounds.\\nInterpretation: We can ignore 1/m for any reasonably\\nlarge memory. Then, the expected value of cosine similar-\\nity is exactly φ(v1, v2) and it is closely distributed around\\nit. Chebyshev’s inequality gives a looser bound and that\\nis apparent from the probable region shown in Figure 3.\\nIn this formulation, again η is the parameter controlling\\nerror and conﬁdence. Theorem 2 shows that if we have\\nsuch a andomly initialized memory, then LMA will lead to\\nintended similarities in approximation.\\n4.1\\nLMA: Dynamic Solution to RSCMA\\nUnlike any static solution (eg. MIP) to SCMA, LMA solu-\\ntion to RSCMA is highly relevant in a real-world setting.\\nThe addition of new features values to datasets is generally\\nfrequent, particularly in recommendation data. Any static\\nsolution to SCMA will need re-computation every time a\\nvalue is added. LSH based LMA solution is unaffected by\\nthis and can gracefully incorporate new values.\\n5\\nLMA for Recommendation Systems\\nLet us now consider the application of LMA for embed-\\ndings of categorical values in ML recommendation sys-\\ntems. DLRM can be used as a running example. However,\\nFigure 4: LMA for DLRM model using common memory\\nacross the tables\\nour approach applies to any system that uses embedding\\ntables. When learning from data in practical settings, S∗is\\noften unknown as E∗is not known. However, we can use\\nthe Jaccard similarity between pairs of values and use the\\nsimilarity structure to obtain a proxy for S∗. We compute\\nthe Jaccard similarity as follows. Let Dv be the set of all\\nsample ids in data in which the value v for some categori-\\ncal feature appears (e.g., a row in a term-document matrix).\\nThen, we can deﬁne the similarity S∗as\\nS∗[v1, v2] = J(Dv1, Dv2).\\nThe resulting kernel is the Jaccard kernel, which is an\\nLSH kernel with the minwise LSH function. To hash a\\nvalue, say v, with the minwise hash function, we will use\\nthe corresponding set Dv. We can then use the general\\nLMA setup described in Section 4 to adjust our training\\nalgorithms.\\nSize of data required: It may appear that storing the\\ndata for minhash computations would diminish memory\\nsavings. However, recall that we need data only to ob-\\ntain a good estimate of Jaccard. We ﬁnd that the actual\\ndata required, D′, is signiﬁcantly less than the total data.\\nNamely,|D′| ≪|S|d. Therefore, to formalize, we bound\\nthe number of non-zeros required for Jaccard computation.\\nTheorem 3 (D′ required is small) Assuming a uniform\\nsparsity of each value, s, the Jaccard Similarity of two\\nvalues x and y,say J = J(Dx, Dy) when estimated from a\\ni.i.d subsample D′ ⊆D, |D′| = n is distributed around J\\nas follows.\\n|E( ˆJ) −J| ≤ϵJ,\\n|V( ˆJ) −A| ≤2ϵ(A + 2J2),\\nwhere A =\\nJ\\n2ns(1 + J −2sJ) with probability (1 −δ)\\nwhere δ = 1+J−2s\\n2nsϵ2 .\\nInterpretation: The bound given by Theorem 3 is loose\\ndue to the approximations done to analyze this random\\nvariable (see Supplemental). Nonetheless, it can be seen\\nthat for a given value of δ, we can control ϵ and A through\\nns, which is the number of non-zeros per value. As ns\\nincreases, both the variance and deviation of the expected\\nvalue from J decrease rapidly. In practice, Section 7 shows\\nwe only need around 100K samples for the Criteo dataset\\nout of 4.5M samples. Below we discuss a few considera-\\n5\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\ntions in relation to LMA applied to DLRM model.\\nMemory Comparison: The size of the memory used by\\nfull embeddings is O(|S|d). The linear dependence on d\\nand the typical very large size of set S makes it difﬁcult\\nto train and deploy large dimensional embedding models.\\nLMA solutions to RSCMA make it possible to simulate\\nthe embeddings of size O(|S|d) using any memory budget\\nm. The memory cost with LMA procedure comprises of:\\n1) Memory budget: O(m) m = |M|; 2) Cost of storing\\nLSH function: O(kd + k′) required to store d minhash\\nfunctions with power k and k′-universal hash function for\\nrehashing. Generally these values are very small compared\\nto O(|S|d) and can be ignored; and 3) The size of Data D′\\nstored and used for minhash functions. Generally, size of\\nD′ is much smaller than (|S|d). For example in Criteo, the\\nsubsample we used had around 3M integers (when using\\n125K samples) as compared to the range of 50M-540M\\nﬂoating parameters of the models we train. We empirically\\nanalyze the effect on various sizes of D′ in the experi-\\nmental section. This requirement of smaller D′ is also an\\neffect of the way we handle very sparse features, which is\\ndiscussed next. To summarize, the memory cost of LMA\\nis O(m + kd + k′ + |D′|) ≈O(m).\\nHandling very sparse features: For very sparse features,\\nthe embedding quality does not signiﬁcantly affect the\\naccuracy of the model [6]. Also, it is difﬁcult to get a\\ngood estimate of Jaccard similarity for this using a small\\nsubsample. Due to these reasons, for very sparse features,\\nwe randomly map each element of its embedding into M.\\nEssentially, we revert to Ah (na¨ıve hashing trick) for such\\nvalues.\\nCommon Memory: We use a single common memory\\nM across all embedding tables in DLRM. The idea is to\\nfully utilize all similarities to share maximum memory and\\nhence get the best memory utilization.\\nForward Pass: The forward pass requires retrieving em-\\nbeddings from M. Let Vbatch be the set of all values in\\na batch. We collect the set {Dv|v∈Vbatch}, apply GPU\\nfriendly implementation of d-minhashes to it to obtain a\\nmatrix of locations, I ∈R|Vbatch|×d. Using this we get\\nEbatch = M[I].\\nBackward Pass: The memory M contains the parame-\\nters which are used to construct embeddings. Hence, in\\nthe backward pass, the gradient of parameters in M is\\ncomputed and these parameters are updated. The exact\\nfunctional dependence of the result on a parameter in M\\nis complex as it is implemented via LSH mappings. Auto\\ngradient computation packages of deep learning libraries\\n(e.g., PyTorch and TensorFlow) are used for gradient com-\\nputation.\\n6\\nRelated Work\\nWe focus on related works that signiﬁcantly reduce the size\\nof the embedding matrix for recommendation and click-\\nthrough prediction systems. Namely, hashing trick [13],\\ncompositional embedding using complementary parti-\\ntions [12], and mixed dimension embedding [6].\\nNa¨ıve hashing trick: Given the embedding table E ∈\\nR|S|×d, two basic approaches that leverage hashing trick\\nare presented.\\n• Vector-wise (or row-wise): let ˆE ∈Rm×d such that\\nm×d = |M|, the memory budget, and m ≪|S| be the\\nreduced size embedding table. We use a hash function\\nh : {0, 1, . . . , |S|−1} →{0, 1, . . . , m−1} that maps\\nthe (row-wise) indices of the embeddings from the full\\nembedding table E to the reduced embedding table ˆE.\\nThe size of the embedding table is reduced from O(|S|d)\\nto O(md).\\n• Element-wise (or entry-wise): let ˆE ∈Rm such that\\nm = |M|. We use a hash function h : {(i, j)}|S|,d\\ni,j=0 →\\n{0, 1, . . . , m−1} that maps each element Ei,j to an ele-\\nment in ˆE. The size of the embedding table is reduced\\nfrom O(|S|d) to O(m) [13].\\nCompositional embedding using complementary parti-\\ntions: In the vector-wise hashing trick, multiple embed-\\nding vectors are mapped to the same index, which results\\nin loss of information on the unique categorical values and\\nreduction in expressiveness of the model. To overcome this\\nissue, [12] proposes to construct complementary partitions\\nof E, from set theory, and apply compositional operators\\non embedding vectors from each partition table to gener-\\nated unique embeddings. One example of a complementary\\npartition is the so-called quotient-remainder (QR) trick.\\nTwo embedding tables ˆE1 ∈Rm×d and ˆE2 ∈R(|S|/m)×d\\nare created, and two hash functions h1 and h2 respectively\\nare used for mapping. h1 maps the i-th row of E to the j-th\\nrow of ˆE1 using the remainder function: j = i mod m.\\nh2 maps the i-th row of E to the k-th row of ˆE2 using\\nthe function k = i \\x0c m, where \\x0c denotes integer division\\n(quotient). Taking the embeddings ej ∈ˆE1 and ek ∈ˆE2\\nand applying element-wise multiplication ej ⊙ek results\\nin a unique embedding vector. The resulting memory com-\\nplexity is O( |S|\\nm d + md). In general, the optimal memory\\ncomplexity is O(k|S|1/kd) with k complimentary parti-\\ntions of sizes {mi × d}k\\ni=1 such that |S| ≤Πk\\ni=1mi.\\nMixed dimension (MD) embedding: Frequency of cate-\\ngorical values are often skewed in real-world applications.\\nInstead of a ﬁxed (uniform) embedding dimension for all\\ncategories, [6] proposes that embedding dimensions scale\\naccording to popularity of categorical values. Namely,\\nmore popular values are set to higher dimension embed-\\ndings (i.e., allocate more memory) and vice versa. The idea\\nis to create embedding tables, along with a projection ma-\\ntrix, of the form {(ˆEi, ˆPi)}k\\ni=1, such that ˆEi ∈R|Si|×di\\n(MD embedding), ˆPi ∈Rdi× ¯d (projection), ¯d ≥di, and\\n|S| = Pk\\ni=1 |Si|. k and d = (d1, . . . , dk) are input pa-\\nrameters. One approach proposed to set the parameters is\\nbased on a power-law sizing scheme using a meta temper-\\nature parameter α. Let p = {p1, . . . , pk}, where pi is a\\nprobability value (e.g., pi = 1/|Si|). Then, λ = ¯d||p||−α\\n∞\\nis the scaling factor and d = λpα is the component-wise\\n6\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\n(a)\\nVarying\\nnh\\nwith\\nα=32×,\\nns=125K\\n(b) Varying α with nh=2, ns=125K\\n(c) Varying ns with nh = 2, α=32×\\nFigure 5: Effect of hyperparameters on performance of LMA-DLRM\\nTable 1: Description of datasets. cat: categorical, int:\\ninteger.\\n#Samples\\n#Features\\n(cat+int)\\nPositive\\nrate\\n#Values\\nCriteo\\n46M\\n26 + 13\\n26%\\n33.76M\\nAvazu\\n41M\\n21 + 0\\n17%\\n9.45M\\nexponent. α = 0 implies uniform dimensions and α = 1\\nsets dimensions proportional to popularity.\\nComparison with LMA: Both QR Trick and MD Trick\\nchange the embedding design. LMA does not affect em-\\nbedding design directly. Instead, it solves the abstract\\nproblem of RSCMA. We can apply LMA in conjunction\\nwith any such embedding design tricks to obtain better\\nmemory utilization.\\n7\\nExperiments\\nDatasets: To evaluate the performance of LMA on DLRM\\n(LMA-DLRM), we use two public click-through rate\\n(CTR) prediction datasets: Criteo and Avazu. Criteo is\\nused in the DLRM paper [1] and related works focused on\\nmemory-efﬁcient embeddings [6, 12]. Avazu is a mobile\\nadvertisement dataset. A summary of dataset properties is\\npresented in Table 1. Values represent the number of all\\nthe categorical values. The number of values per feature\\nvaries a lot : for example, some lower values are 10K and\\nthey go as high as 10M.\\nMetrics: We use loss, accuracy, and ROC-AUC as metrics\\nfor our experiments. For imbalanced datasets like these,\\nAUC is a better choice of metric than accuracy.\\nBasic setup: For all our experiments (LMA-DLRM, base-\\nlines), we follow the basic setup (e.g., optimizer parame-\\nters, model architecture) suggested by the DLRM paper [1]\\nand as implemented in their GitHub page. The batch size\\nis set to 2,048 as we want to run experiments for a larger\\nnumber of epochs. Also, a higher batch size is preferred\\nfor CTR datasets [10]. We use the same settings for LMA-\\nDLRM as well as all baselines for a fair comparison.\\n7.1\\nHyperparameter experiments\\nThere are three hyperparameters: nh : power of LSH func-\\ntion see 3.2, α: expansion rate, and ns : number of samples\\nin D′ for LMA-DLRM. The best values for each of these\\nhyperparameters can be chosen via cross-validation. We\\ndescribe these parameters and qualitatively analyze their\\neffect on performance. The results of changing one param-\\neter while keeping the other two ﬁxed are shown in Figure\\n5. We use 270M budget for varying α and 35M budget for\\nothers.\\n• Power of LSH (nh) The nh used per LSH mapping (i.e.\\npower in Section 3.2) controls the probability of colli-\\nsion (i.e., Jnh) of corresponding elements of different\\nembeddings as discussed in Section 3.2. Higher power\\nleads to a lower probability of collision, making the re-\\nhashed LSH function behave more like na¨ıve hashing\\ntrick. Very low power will increase memory sharing and\\nmight lead to its under-utilization. This phenomenon can\\nbe observed in Figure 5a, where nh=1 gives the worst\\nperformance. Increasing nh improves the performance\\nuntil nh = 8. Performance worsens when nh = 32 and\\ntends towards the hashing trick performance as expected.\\n• Expansion rate (α) LMA-DLRM can simulate embed-\\nding tables of any dimension d. We deﬁne expansion\\nrate as the ratio of simulated memory to actual memory.\\nUsing GMA notation, α = |S|d/|M|. Figure 5b shows\\nthat 16 works best for memory budget of ∼270M param-\\neters. So increasing α will not improve the performance\\nindeﬁnitely.\\n• Samples in D′ (ns): Figure 5c shows that the per-\\nformance boost saturates after the representation size\\nreaches 75k data points, which means that most of the\\nfrequently appearing values (v) get decent representa-\\ntions (Dv) from a small number of samples. For very\\nsparse values, we revert to element-wise na¨ıve hashing\\ntrick based mapping. We also show the size of the sam-\\nples in terms of the number of non-zeros integers to be\\nstored. As compared to 540M parameter networks we\\ntrain a sample of 125K only requires us to store 3.2M\\nintegers.\\n7.2\\nMain Experiment\\nWe compare LMA-DLRM against full embedding (embed-\\nding tables used in DLRM [1]), HashedNet [13] embed-\\nding (na¨ıve element-wise hashing trick based), and QR\\nembeddings [12] across different memory budgets. Hyper-\\nparameters nh=4, α=16, and ns=125, 000 were used for\\nall LMA-DLRM experiments in this section. For baselines,\\nwe use the conﬁgurations in their open source code. Train-\\ning of models was cutoff at 15 epochs. We did not perform\\n7\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nNumber of parameters/Million\\nTest AUC\\n0.796\\n0.798\\n0.800\\n0.802\\n0.804\\n36\\n70\\n135\\n270\\n540\\nFull\\nLMA\\nHashNet\\nQR\\nNumber of parameters/Million\\nTest accuracy\\n0.786\\n0.787\\n0.788\\n0.789\\n0.790\\n36\\n70\\n135\\n270\\n540\\nFull\\nLMA\\nHashNet\\nQR\\nNumber of parameters/Million\\nTest loss\\n0.450\\n0.452\\n0.454\\n0.456\\n0.458\\n36\\n70\\n135\\n270\\n540\\nFull\\nLMA\\nHashNet\\nQR\\nNumber of parameters/Million\\nTest AUC\\n0.750\\n0.752\\n0.754\\n0.756\\n0.758\\n10\\n19\\n38\\n76\\n152\\nFull\\nLMA\\nHashNet\\nQR\\nNumber of parameters/Million\\nTest accuracy\\n0.8476\\n0.8478\\n0.848\\n0.8482\\n0.8484\\n10\\n19\\n38\\n76\\n152\\nFull\\nLMA\\nHashNet\\nQR\\nNumber of parameters/Million\\nTest loss\\n0.374\\n0.376\\n0.378\\n0.380\\n10\\n19\\n38\\n76\\n152\\nFull\\nLMA\\nHashNet\\nQR\\nFigure 6: AUC, Accuracy, and Loss against memory regimes (number of parameters) on Criteo (top) and Avazu\\n(bottom)\\nFigure 7: Evolution of test AUC with different parameters\\non Criteo (left) and Avazu (right)\\nextensive hyperparameter tuning. However, hyperparame-\\nter tuning can achieve better results for LMA-DLRM. The\\nLMA-DLRM code is attached with supplementary mate-\\nrial.\\nResults: Figure 6 shows AUC, accuracy, and loss against\\ndifferent memory regimes (number of parameters) on both\\ndatasets. Figure 7 shows the evolution of some models for\\nﬁrst 5 epochs.\\n• LMA-DLRM outperforms all baselines across different\\nmemory regimes include those reported in [1], achiev-\\ning average AUC improvement of 0.0032 in Criteo and\\n0.0034 in Avazu across memory budgets and an average\\nimprovement in Accuracy of 0.0017 on Criteo. Recall\\nthat an improvement of 0.001 is signiﬁcant.\\n• The AUC and accuracy of full embedding model with\\n540M parameters can be achieved by LMA-DLRM with\\nonly 36M parameters (16× reduction) on Criteo. On\\nAvazu, results with 10M parameter LMA model are\\nmuch better than full embedding model with 150M pa-\\nrameters (15× reduction).\\n• LMA-DLRM achieves best AUC 0.805 as opposed to\\nthe best of full embedding 0.802 on Criteo. On Avazu,\\nwe see improvement of 0.0025 on best AUCs as well.\\n• The typical evolution of AUC metric for Criteo and\\nAvazu on models of different sizes for LMA-DLRM\\nand full embedding models clearly supports the better\\nperformance of LMA-DLRM over full embeddings.\\n8\\nConclusion\\nWe deﬁne two problems namely SCMA (Semantically\\nConstrained Memory Allocation) and Randomized SCMA\\nfor efﬁcient utilization of memory in Embedding tables.\\nWe propose a neat LSH-based Memory Allocation (LMA)\\nwhich solves the dynamic version of RSCMA under any\\nmemory budget with negligible memory overhead. LMA\\nwas applied to an important problem of heavy memory\\ntables in widely used recommendation models and found\\ntremendous success. In this paper, we focus on the memory\\naspect of LMA. In future work, we would like to bench-\\nmark the LMA method for its time efﬁciency for training\\nand inference for recommendation models.\\n8\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nReferences\\n[1] Maxim\\nNaumov,\\nDheevatsa\\nMudigere,\\nHao-\\nJun Michael Shi, Jianyu Huang, Narayanan Sundara-\\nman, Jongsoo Park, Xiaodong Wang, Udit Gupta,\\nCarole-Jean Wu, Alisson G. Azzolini, Dmytro Dzhul-\\ngakov, Andrey Mallevich, Ilia Cherniavskii, Ying-\\nhai Lu, Raghuraman Krishnamoorthi, Ansha Yu,\\nVolodymyr Kondratenko, Stephanie Pereira, Xianjie\\nChen, Wenlin Chen, Vijay Rao, Bill Jia, Liang Xiong,\\nand Misha Smelyanskiy. Deep learning recommenda-\\ntion model for personalization and recommendation\\nsystems. arXiv:1906.00091, 2019.\\n[2] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen,\\nTal Shaked, Tushar Chandra, Hrishi Aradhye, Glen\\nAnderson, Greg Corrado, Wei Chai, Mustafa Ispir,\\net al.\\nWide & deep learning for recommender\\nsystems.\\nIn Proceedings of the 1st workshop on\\ndeep learning for recommender systems, pages 7–10,\\n2016.\\n[3] Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\\nDean. Efﬁcient estimation of word representations in\\nvector space. arXiv:1301.3781, 2013.\\n[4] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\\nKaiser, and Illia Polosukhin. Attention is all you\\nneed. arXiv:1706.03762, 2017.\\n[5] Ruoxi Wang, Bin Fu, Gang Fu, and Mingliang Wang.\\nDeep & cross network for ad click predictions. In\\nProceedings of the ADKDD, 2017.\\n[6] Antonio Ginart, Maxim Naumov, Dheevatsa Mudi-\\ngere, Jiyan Yang, and James Zou. Mixed dimension\\nembeddings with application to memory-efﬁcient rec-\\nommendation systems. arXiv:1909.11810, 2019.\\n[7] Jongsoo Park, Maxim Naumov, Protonu Basu, Sum-\\nmer Deng, Aravind Kalaiah, Daya Khudia, James\\nLaw, Parth Malani, Andrey Malevich, Satish Na-\\ndathur, et al. Deep learning inference in facebook\\ndata centers: Characterization, performance opti-\\nmizations and hardware implications. arXiv preprint\\narXiv:1811.09886, 2018.\\n[8] Qi Pi, Weijie Bian, Guorui Zhou, Xiaoqiang Zhu,\\nand Kun Gai. Practice on long sequential user be-\\nhavior modeling for click-through rate prediction. In\\nProceedings of the 25th ACM SIGKDD International\\nConference on Knowledge Discovery & Data Mining,\\npages 2671–2679, 2019.\\n[9] Udit Gupta, Carole-Jean Wu, Xiaodong Wang,\\nMaxim Naumov, Brandon Reagen, David Brooks,\\nBradford Cottel, Kim Hazelwood, Mark Hempstead,\\nBill Jia, et al. The architectural implications of face-\\nbook’s dnn-based personalized recommendation. In\\n2020 IEEE International Symposium on High Perfor-\\nmance Computer Architecture (HPCA), pages 488–\\n501. IEEE, 2020.\\n[10] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang,\\nand Xiuqiang He.\\nFuxictr: An open benchmark\\nfor click-through rate prediction.\\narXiv preprint\\narXiv:2009.05794, 2020.\\n[11] Jo˜ao Gama, Indr˙e ˇZliobait˙e, Albert Bifet, Mykola\\nPechenizkiy, and Abdelhamid Bouchachia. A survey\\non concept drift adaptation. ACM computing surveys\\n(CSUR), 46(4):1–37, 2014.\\n[12] Hao-Jun Michael Shi, Dheevatsa Mudigere, Maxim\\nNaumov, and Jiyan Yang. Compositional embeddings\\nusing complementary partitions for memory-efﬁcient\\nrecommendation systems. arXiv:1909.02107, 2019.\\n[13] Wenlin Chen, James Wilson, Stephen Tyree, Kilian\\nWeinberger, and Yixin Chen. Compressing neural\\nnetworks with the hashing trick. In International\\nconference on machine learning, pages 2285–2294.\\nPMLR, 2015.\\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina Toutanova. Bert: Pre-training of deep bidi-\\nrectional transformers for language understanding.\\narXiv preprint arXiv:1810.04805, 2018.\\n[15] Yoav Goldberg and Omer Levy. word2vec explained:\\nderiving mikolov et al.’s negative-sampling word-\\nembedding method. arXiv preprint arXiv:1402.3722,\\n2014.\\n[16] Mark N. Wegman and J. Lawrence Carter. New hash\\nfunctions and their use in authentication and set equal-\\nity. Journal of Computer and System Sciences, 1981.\\n[17] Piotr Indyk and Rajeev Motwani. Approximate near-\\nest neighbors: towards removing the curse of dimen-\\nsionality. In Proceedings of the thirtieth annual ACM\\nsymposium on Theory of computing, pages 604–613,\\n1998.\\n9\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nA\\nLMA solves RSCMA\\nUnder the GMA setup (see def 3), for any two values v1 and v2, the fraction of consistently shared memory fAL as per\\nallocation AL proposed by LMA is a random variable with distribution,\\nE(fAL(v1, v2)) = Γ = φ(v1, v2) + 1 −φ(v1, v2)\\nm\\n,\\nV(fAL(v1, v2)) = Γ(1 −Γ)\\nd\\n,\\nPr\\n\\x12\\n|fAL(v1, v2)−φ(v1, v2)| > ηΓ + 1−φ(v1, v2)\\nm\\n\\x13\\n≤2 exp\\n\\x1a−dΓη2\\n3\\n\\x1b\\n,\\nfor all η > 0. Hence, LMA solves RSCMA with ϵ = ηΓ + 1−φ(v1,v2)\\nm\\nand δ = 2 exp\\nn\\n−dΓη2\\n3\\no\\n.\\nProof sketch: Proof consists of analyzing the random variable for the fraction fAL(v1, v2) and applying Chernoff\\nconcentration inequality to obtain the tail bounds.\\nProof\\nThe probability that a particular location is shared is exactly the probability of collision of the rehashed lsh function lr.\\n(using the notation from section 4)\\nPr(lr(x) == lr(y)) = φ(x, y) + (1 −φ(x, y))\\nm\\n(1)\\nWe can write the indicator for fraction of consistently shared memory as,\\nˆ\\nfAL =\\nd\\nX\\ni=1\\nI(l(i)\\nr (x) == l(i)\\nr (y))\\n(2)\\nfAL is a sum of independent bernoulli varaibles. It is easy to check that the expected value of consistently shared\\nfraction is,\\nE( ˆ\\nfAL) = Γ = φ(x, y) + (1 −φ(x, y))\\nm\\n(3)\\nV ( ˆ\\nfAL) = Γ(1 −Γ)\\nd\\n(4)\\nWe can apply the chernoff’s bound to get tail bound. Let η > 0 be any positive real number\\nPr (|fAL(v1, v2)−Γ| > ηΓ) ≤2 exp\\n\\x1a−dΓη2\\n3\\n\\x1b\\n(5)\\nPr\\n\\x12\\n|fAL(v1, v2)−φ(v1, v2)| > ηΓ + 1−φ(v1, v2)\\nm\\n\\x13\\n≤2 exp\\n\\x1a−dΓη2\\n3\\n\\x1b\\n(6)\\nB\\nExistence of M with LMA for S∗\\nUnder the GMA setup (see Def. 3), let us initialize each element of M independently from a Bernoulli(0.5, {−1, +1}).\\nThen, the embedding table E generated via LMA on this memory, has , for every pair of values v1 and v2, the cosine\\nsimilarity Cs(E[v1, :], E[v2, :]), denoted by Cs(v1, v2) is distributed as\\nE(Cs(v1, v2)) = Γ = φ(v1, v2) + 1 −φ(v1, v2)\\nm\\n,\\nV(Cs(v1, v2)) = 1 −Γ2\\nd\\n+ 2(1 −Γ)(d −1)\\ndm2\\n,\\nPr\\n\\x12\\n|Cs(v1, v2) −φ(v1, v2)| ≥ηΓ + 1 −φ(v1, v2)\\nm\\n\\x13\\n≤1 −Γ2\\ndη2Γ2 for any η > 0.\\n10\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nProof sketch: Proof consists of analyzing the random variable for the cosine similarity Cs(E[v1, :], E[v2, :]) and\\napplying Chebyshev’s concentration inequality to obtain the tail bounds.\\nConsider a memory M of size m initialized randomly by a independent draws of a Bernoulli random variable from\\n{−1, +1} with probability 0.5. Then let a be denote the value at any aribitraty memory location in M Note, for each\\nk ∈N\\nE(a2k) = 1\\nE(a2k+1) = 0\\n(7)\\nThe norm of any embedding of dimension d drawn from this M is\\n√\\nd. Let us look at the inner product\\n\\\\\\n⟨Ex, Ey⟩= Σd\\ni=1{I(li(x) == li(y))M[li(x)]2 + I(li(x)! = li(y))M[li(x)]M[li(y)]}\\n(8)\\nLet Γ = φ(x, y) + 1−φ(x,y)\\nm\\nE( \\\\\\n⟨Ex, Ey⟩) = dΓ\\n(9)\\nE(\\n\\\\\\nCosine(Ex, Ey)) = Γ\\n(10)\\nAnalysis of Variance is a bit involved due to interdependence between each of the terms in the summation above that\\ncomes about due to random collisions caused by rehashing. We analyse both the cases. The case of interdependence\\njust precipitates to independent case for reasonable M.\\nCase 1: Assume independence\\nV( \\\\\\n⟨Ex, Ey⟩) = d(V(I(li(x) == li(y)) + I(li(x)! = li(y))M[li(x)]M[li(y)]))\\n(11)\\nV( \\\\\\n⟨Ex, Ey⟩) = d(V(I(li(x) == li(y))) + V(I(li(x)! = li(y))M[li(x)]M[li(y)])))\\n(12)\\nV( \\\\\\n⟨Ex, Ey⟩) = d(Γ(1 −Γ) + (1 −Γ))\\n(13)\\nV( \\\\\\n⟨Ex, Ey⟩) = d(1 + Γ)(1 −Γ)\\n(14)\\nV(Cosine(⟨Ex, Ey⟩)) = 1\\nd(1 + Γ)(1 −Γ)\\n(15)\\nCase 2: Don’t assume independence.\\n\\\\\\nCosine(Ex, Ey) = 1\\nd\\n\\\\\\n⟨Ex, Ey⟩\\n(16)\\n\\\\\\ncosine(Ex, Ey) = 1\\ndΣd\\ni=1{I(li(x) == li(y))M[li(x)]2 + I(li(x)! = li(y))M[li(x)]M[li(y)]}\\n(17)\\nE(\\n\\\\\\nCosine(Ex, Ey) = Γ\\n(18)\\nE(\\n\\\\\\n(Cosine(Ex, Ey)\\n2\\n) = 1\\nd2 E(\\n\\\\\\n(⟨Ex, Ey⟩)\\n2\\n)\\n(19)\\nLet exi = M[li(x)]\\nE(\\n\\\\\\n(Cosine(Ex, Ey)\\n2\\n) = 1\\nd2 E(Σexieyi)(Σexieyi)\\n(20)\\nE(\\n\\\\\\n(Cosine(Ex, Ey)\\n2\\n) = 1\\nd2 E(Σd\\ni=1(exieyi)2 + Σi̸=j(exieyi)(exjeyj)\\n(21)\\nE(\\n\\\\\\n(Cosine(Ex, Ey)\\n2\\n) = 1\\nd(E((exieyi)2) + (d −1)E(exieyi)(exjeyj)\\n(22)\\nE(\\n\\\\\\n(Cosine(ex, ey)\\n2\\n) = 1\\nd(1 + (d −1)EM[li(x)]M[lj(x)]M[li(y)]M[lj(x)])\\n(23)\\n11\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nli(x)\\nli(y)\\nlj(x)\\nlj(y)\\nprobability\\na\\na\\nb\\nb\\npcˆ2\\na\\nb\\na\\nb\\n(1-pc) / mˆ2\\na\\nb\\nb\\na\\n(1 - pc) / mˆ2\\nTable 2: Table for reference on cases\\nUsing table 2,\\nE(\\n\\\\\\n(Cosine(ex, ey)\\n2\\n) = 1\\nd(1 + (d −1)(Γ2 + 2(1 −Γ)\\nm2\\n))\\n(24)\\nV ar(\\n\\\\\\n(Cosine(Ex, Ey)) = E(\\n\\\\\\n(Cosine(Ex, Ey)\\n2\\n) −E(\\n\\\\\\n(Cosine(Ex, Ey))2\\n(25)\\nV ar(\\n\\\\\\n(Cosine(ex, ey)) = 1\\nd(1 + (d −1)(Γ2 + 2(1 −Γ)\\nm2\\n)) −Γ2\\n(26)\\nCollecting terms\\nV ar(\\n\\\\\\n(Cosine(ex, ey)) = 1\\nd{(1 −21 −Γ\\nm2\\n−Γ2) + 2d(1 −Γ\\nm2 )}\\n(27)\\nV ar(\\n\\\\\\n(Cosine(ex, ey)) = 1 −Γ2\\nd\\n+ 2(1 −Γ)(d −1)\\ndm2\\n≈1\\nd(1 −Γ2)\\n(28)\\nC\\nProcedures requires small data sample\\nAssume that the real dataset is of size N, the sparsity of each feature value is s. i.e. the probability of a feature appearing\\nin an example is s. For simplicity, let us assume that each feature value has the same sparsity. This may not be generally\\ntrue. But nonetheless it helps us draw an idea of data sample needed. Let us consider the situation where the two\\nfeatures have jaccard similarity J. The venn diagram below shows the distribution of samples in our case.\\nFigure 8: Venn diagram for two features f1,f2\\nLet the events be as follows:\\n• Ai : ith sample has feature f1\\n• Bi : ith sample has feature f2\\nThe estimated jaccard similarity after drawing n random i.i.d samples would be,\\nˆJ = Σn\\ni=1I(Ai ∧Bi)\\nΣn\\ni=1I(Ai ∧Bi\\n= X\\nY\\n(29)\\n12\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nLet X and Y and C be the following\\nC = (1 + J)\\n2ns\\n(30)\\nX = CΣn\\ni=1I(Ai ∩Bi)\\n(31)\\nY = CΣn\\ni=1I(Ai ∪Bi)\\n(32)\\nE(X) = Cn 2sJ\\n1 + J =⇒E(X) = (1 + J)\\n2ns\\nn 2sJ\\n1 + J = J\\n(33)\\nE(Y ) = Cn\\n2s\\n1 + J =⇒E(Y ) = (1 + J)\\n2ns\\nn\\n2s\\n1 + J = 1\\n(34)\\nV ar(X) = C2n 2sJ\\n1 + J (1 −2sJ\\n1 + J )\\n(35)\\nV ar(Y ) = C2n\\n2s\\n1 + J (1 −\\n2s\\n1 + J )\\n(36)\\nLet us look at Variance of X\\nV ar(X) = (1 + J)2\\n4n2s2 n 2sJ\\n1 + J (1 −2sJ\\n1 + J )\\n(37)\\nV ar(X) =\\nJ\\n2ns(1 + J −2sJ)\\n(38)\\nLet us look at Variance of (Y)\\nV ar(Y ) = (1 + J)2\\n4n2s2 n\\n2s\\n1 + J (1 −\\n2s\\n1 + J )\\n(39)\\nV ar(Y ) =\\n1\\n2ns(1 + J −2s)\\n(40)\\n(41)\\nUsing Chebysev’s inequality ,\\nP(|Y −1| ≥ϵ) < V ar(Y )\\nϵ2\\n(42)\\nHence, 1 −ϵ ≤Y ≤1 + ϵ with probability 1 −δ where δ = 1+J−2s\\n2nsϵ2\\nHence we can write, with probability 1 −δ\\nX\\n1 + ϵ ≤ˆJ ≤\\nX\\n1 −ϵ\\n(43)\\nHence, with probability 1 −δ\\nJ\\n1 + ϵ ≤E( ˆJ) ≤\\nJ\\n1 −ϵ\\n(44)\\nX2\\n(1 + ϵ)2 ≤ˆJ2 ≤\\nX2\\n(1 −ϵ)2\\n(45)\\nE(X2)\\n(1 + ϵ)2 ≤E( ˆJ2) ≤E(X)2\\n(1 −ϵ)2\\n(46)\\nV ar(X) + E(X)2\\n(1 + ϵ)2\\n≤E( ˆJ2) ≤V ar(X) + E(X)2\\n(1 −ϵ)2\\n(47)\\nJ2\\n(1 + ϵ)2 ≤E( ˆJ)2 ≤\\nJ2\\n(1 −ϵ)2\\n(48)\\nV ar(X) + E(X)2\\n(1 + ϵ)2\\n−\\nJ2\\n(1 −ϵ)2 ≤E( ˆJ2) −E( ˆJ)2 ≤V ar(X) + E(X)2\\n(1 −ϵ)2\\n−\\nJ2\\n(1 + ϵ)2\\n(49)\\n13\\nSemantically Constrained Memory Allocation (SCMA) for Embedding in Efﬁcient Recommendation Systems\\nV ar(X) + E(X)2 =\\nJ\\n2ns(1 + J −2sJ + 2nsJ)\\n(50)\\nThe above is the actual result. However for simplicity we simplify assuming ϵ is small.\\nLet\\nA = V ar(X) + E(X)2 −J2\\n(51)\\nB = V ar(X) + E(X)2 + J2\\n(52)\\nA =\\nJ\\n2ns(1 + J −2sJ + 2nsJ) −J2\\n(53)\\nB =\\nJ\\n2ns(1 + J −2sJ + 2nsJ) + J2\\n(54)\\nA =\\nJ\\n2ns(1 + J −2sJ)\\n(55)\\nB = A + 2J2\\n(56)\\nA −2ϵ(A + 2J2) ≤V ar( ˆJ) ≤A + 2ϵ(A + 2J2)\\n(57)\\n(1 −2ϵ)A −4ϵJ2) ≤V ar( ˆJ) ≤(1 + 2ϵ)A + 4ϵJ2)\\n(58)\\nAlso under small ϵ\\nJ(1 −ϵ) ≤E( ˆJ) ≤J(1 + ϵ)\\n(59)\\n14\\n',\n",
       " '2006.05623': 'Training with Multi-Layer\\nEmbeddings for Model Reduction\\nBenjamin Ghaemmaghami 1, Zihao Deng 1, Benjamin Cho 1, Leo Orshansky 2,\\nAshish Kumar Singh 3, Mattan Erez 1, and Michael Orshansky 1\\n1 Department of Electrical and Computer Engineering, University of Texas at Austin\\n2 Department of Computer Science, University of Texas at Austin\\n3 E2OPEN, India\\nAbstract\\nModern recommendation systems rely on real-valued embeddings of categorical\\nfeatures. Increasing the dimension of embedding vectors improves model accuracy\\nbut comes at a high cost to model size. We introduce a multi-layer embedding\\ntraining (MLET) architecture that trains embeddings via a sequence of linear layers\\nto derive superior embedding accuracy vs. model size trade-off.\\nOur approach is fundamentally based on the ability of factorized linear layers to\\nproduce superior embeddings to that of a single linear layer. We focus on the\\nanalysis and implementation of a two-layer scheme. Harnessing the recent results\\nin dynamics of backpropagation in linear neural networks, we explain the ability to\\nget superior multi-layer embeddings via their tendency to have lower effective rank.\\nWe show that substantial advantages are obtained in the regime where the width of\\nthe hidden layer is much larger than that of the ﬁnal embedding (d). Crucially, at\\nconclusion of training, we convert the two-layer solution into a single-layer one: as\\na result, the inference-time model size scales as d.\\nWe prototype the MLET scheme within Facebook’s PyTorch-based open-source\\nDeep Learning Recommendation Model. We show that it allows reducing d by\\n4-8X, with a corresponding improvement in memory footprint, at given model\\naccuracy. The experiments are run on two publicly available click-through-rate\\nprediction benchmarks (Criteo-Kaggle and Avazu). The runtime cost of MLET is\\n25%, on average.\\n1\\nIntroduction\\nRecommendation models (RMs) underlie a large number of applications and improving their perfor-\\nmance is increasingly important. The click-through-rate (CTR) prediction task is a special case of\\ngeneral recommendation that seeks to predict the probability of a user clicking on a speciﬁc item,\\ne.g. an ad, given the history of the user’s past reactions. The user reactions and earlier-encountered\\ninstances are used in training the CTR model and are described by multiple features that capture user\\ninformation (e.g., age, gender) and item information (e.g., movie title, cost) [22]. Features are either\\nnumerical or categorical variables.\\nA categorical variable with n possible values can be represented by an n-dimensional one-hot\\nvector. However, a fundamental aspect of modern recommendation models is their reliance on\\nembeddings which map categorical variables into dense representations in an abstract real-valued\\nspace. Embeddings are superior for two main reasons. The ﬁrst is that they allow a compacted\\nrepresentation compared to high-dimensional sparse one-hot, or multi-hot, direct encodings of\\ncategorical data. The second is that dense embedding vectors represent meaningful information that is\\nexploited by RMs for improved performance: the angle (dot-product) between two embedding vectors\\narXiv:2006.05623v1  [cs.LG]  10 Jun 2020\\nrepresents their semantic similarity. Following a seminal innovation of Factorization Machines [24],\\nmany modern RMs exploit this by using dot-products between embedding vectors to deﬁne the\\nstrength of feature interactions.\\nState-of-the-art RMs increasingly rely on deep neural networks. Most high-performing models use a\\ncombination of multi-layer perceptrons (MLPs) to process dense features, linear layers to generate\\nembeddings of categorical features, and sub-networks that generate higher-order interactions. The\\noutputs of the interaction sub-networks and MLPs are used as inputs into a linear (logistic) model with\\na sigmoid activation to produce the CTR prediction. Broadly, the above describes the architectures of\\nWide and Deep [8], Deep and Cross [28], DeepFM [13], Field-Aware Factorization Machine (FFM)\\n[14], and xDeepFM [16] networks, among others. The differences between the models are largely in\\nhow they handle the higher-order feature interactions. The Deep Learning Recommendation Model\\n(DLRM) [21], that we use for prototyping our technique, is structurally similar to other models.\\nDLRM does not include higher-order interactions, judging that their computational and memory\\ncost is not justiﬁed. This is supported by empirical results on public datasets that show DLRM\\noutperforming models with explicit higher-order interactions, such as the Deep and Cross model [28].\\nAll DNN-based RMs described above derive embeddings as part of model training through backprop-\\nagation. Algorithmically, embeddings are implemented as linear layers: if a categorical feature in\\none-hot encoding is a vector q ∈Z1×n, then the embedding lookup is a vector-matrix multiplication\\nqW. Here, W ∈Rn×d is the embedding table (matrix) whose ith row represents the embedding\\nof the ith category in a d-dimensional vector space. Conventionally, W is implemented as a single\\nlinear layer and jointly trained with the rest of the model to minimize the loss on the CTR task.\\nThough embeddings are a more efﬁcient representation of features compared to one-hot categorical\\nvectors, the embedding tables still impose an increasingly heavy cost in system deployments, with\\ntables commonly requiring tens of gigabytes of space [9]. The reason is the large value of n: it is not\\nuncommon to encounter a single categorical feature with millions of distinct values. For example, in\\nthe public Avazu dataset, one categorical feature has 6.7 million values.\\nThere are many techniques aiming to reduce the memory requirements of embedding tables - some\\nunique to the embedding layer setting and others general. Compression-based techniques operate\\non trained layers and use pruning and quantization to reduce table size [17, 27, 25]. Low-rank\\napproximation via SVD is another example of post-training compression [6]. Other techniques\\nperform pruning or quantization during training [2, 20]. While the above group of methods does not\\ninvolve modifying the structure of the model, other methods, such as hashing and tensor factorization\\n[5, 15], achieve superior quality-size behavior through a modiﬁed model structure that results in\\nbetter use of model parameters. Using the unique properties of RMs, in [10], a mixed-dimension\\nstrategy uses statistical patterns (frequency) of accessing individual entries to embed the popular\\nentries into vectors of higher dimension compared to the less popular entries.\\nFigure 1: CTR model accuracy vs. the embedding dimension based on a single-layer embedding.\\nThe trade-off curve is generated using DLRM on the Criteo-Kaggle dataset.\\n2\\nEmbedding vector dimension d is a critical factor that controls the table size as well as model\\nperformance. Both empirical and theoretical evidence suggests that there exists a fundamental\\ntrade-off under which reducing vector dimension d leads to the loss in model performance [19, 29].\\nThe trade-off is illustrated in Figure 1 using DLRM on the Criteo-Kaggle dataset. The contribution of\\nthis paper is in developing a novel way of deriving a superior model size-accuracy trade-off.\\n1.1\\nOur Contribution: Multi-Layer Embedding Training\\nWe propose a novel way of achieving a smaller model size without accuracy degradation. The\\ntechnique, which we call a multi-layer embedding training (MLET) architecture, trains embeddings\\nvia a sequence of linear layers, instead of a single layer.\\nThe fundamental underpinning for the superior behavior of MLET is the dynamics of training using\\nbackpropagation. Harnessing recent results in the training of deep factorized linear neural networks,\\nwe provide a theoretical explanation for the surprising fact that multi-layer embeddings lead to a\\nsuperior size-accuracy trade-off. We show that the main reason for the superior behavior is the\\nimpact of factorization on the generalization ability of the model, which is produced by the model’s\\nconvergence towards a less complex solution.\\nWe focus on a prototype implementation that employs two linear layers. The inner dimension between\\nthe two layers is k. The second layer’s output dimension is equal to the target embedding dimension\\nd. We ﬁnd empirically that the effectiveness of the two-layer embedding technique depends heavily\\non the ratio k/d for any given target embedding dimension d. The most beneﬁt occurs when k/d > 4.\\nThe main cost of MLET is a k/d increase in the required memory capacity during training (compared\\nto a conventional embeddings training with dimension d).\\nIt would appear that MLET increases the number of model parameters signiﬁcantly, with the size of\\nthe embedding table increased by a factor of k/d. However, a two-layered approach is essential only\\nduring training. We eliminate the inference-time memory and model storage cost of MLET by a a\\npost-training layer transformation that collapses the multiple linear layers into a single one. As a\\nresult, for inference, only the original embedding table of size n × d is stored.\\nWe implement the proposed algorithmic framework in PyTorch using DLRM. We demonstrate\\nsubstantial beneﬁts of MLET in terms of model size reduction of 4-8X, at constant accuracy on two\\npublic CTR datasets Avazu and Criteo-Kaggle. We ﬁnd that the runtime cost of MLET is about 25%.\\n2\\nSuperior Embedding Size-Accuracy Trade-off via Multi-Layer\\nEmbedding Training\\n2.1\\nMulti-Layer Embeddings: Deﬁnitions\\nWe now introduce the notation and details of MLET. Let the ﬁnal embedding table be W of size\\nn × d, where n is the number of elements in the table and d is the embedding dimension.\\nW ∈Rn×d\\n(1)\\nWe focus on a two-layer architecture and seek to factorize the embedding table W in terms of W1\\nand W2:\\nW = W1W2\\n(2)\\nW1 ∈Rn×k\\n(3)\\nW2 ∈Rk×d\\n(4)\\nLet the row vector q ∈Z1×n denote a one-hot encoding of a feature with n categories. The\\nembedding lookup is represented by a vector-matrix product:\\nr = qW1W2\\n(5)\\n3\\nHere, r ∈R1×d is the embedding of q in a d-dimensional space. W1 and W2 are trained jointly. After\\ntraining there is no need to keep both W1 and W2, and we only store their product, W = W1W2.\\nThis reduces a two-layer embedding into a single one for inference-time evaluation and storage.\\nThe essential aspect of MLET’s training of an embedding using a sequence of two linear layers are\\nthe relative dimensions of W1 and W2. As deﬁned above, W1 and W2 are of shape n × k and k × d,\\nrespectively. We say that a model with a linear layer n × d1 dominates (») another linear model with\\na linear layer n × d2 if the validation loss on the ﬁrst model is lower than that of the second model.\\nSymbolically, (n × d1) » (n × d2) if Loss (n × d1) < Loss (n × d2).\\nFor a single-layer model, the accuracy-size trade-off discussed earlier, and shown in Figure 1, can be\\nrestated as follows: (n × d1) » (n × d2) if d1 > d2. Similarly, as a consequence of the same trade-off,\\nit seems self-evident that for a two-layer linear model, the following holds: (n × k) × (k × d1) »\\n(n × k) × (k × d2) if d1 > d2 (this is also conﬁrmed empirically, and can be seen in Figures 2 and 3).\\nYet there are two aspects of MLET that seem quite surprising. The ﬁrst is why a two-layer embedding\\nis superior to a one-layer embedding, or compactly, why (n × k) × (k × d) » (n × d)? The second\\nis why a two-layer model improves with a larger width of the hidden layer k, or compactly, why\\n(n×k1)×(k1 ×d) » (n×k2)×(k2 ×d) if k1 > k2? In the next section we explain the ﬁrst behavior\\n- the effect of factorization per se. We currently attribute the second behavior to the general tendency\\nof overparameterized linear neural networks to positively depend on the width of hidden layers. We\\nplan to explore this aspect of MLET more thoroughly in our future work.\\n2.2\\nWhy Does Factorization Help?\\nWhy should we expect to get a better embedding if we factorize the linear layer? Speciﬁcally, as we\\ndemonstrate in Section 2.2.2, in the MLET operating regime of k ≥d, any embedding deﬁned by a\\ntwo-layer model lies in the search space of a single-layer model. Therefore, if there is an optimal\\nsolution found by a two-layer model, our intuition is that a single-layer model should also be able to\\nﬁnd it, and, thus, a two-layer model should not be better than a single-layer model. Yet, empirically,\\nwe ﬁnd that two-layer models consistently outperform their single-layer counterparts.\\nTo understand why factorization helps, we rely on recent results in the dynamics of training linear\\nlayers using backpropagation. The main reason for the superior behavior of the multi-layer model\\ntraining is the impact of factorization on the generalization ability of the model. It achieves this by\\nconvergence towards a less complex solution.\\n2.2.1\\nDynamics of Factorized Linear Layer Network Training\\nIt is a widely accepted notion in deep learning that low-rank weight matrices lead to better general-\\nization and help avoid overﬁtting [4]. Practically, regularization on rank is a common and powerful\\napproach to restrict model complexity and thus enhance generalization. A variety of ML algorithms\\nuse regularization on rank to achieve better generalization, including robust principal component\\nanalysis [11][26], robust matrix completion [7], subspace clustering [23][18], and others [12].\\nRecent work [3] has shown that a linear layer network (LLN) with multiple layers has a strong bias\\ntowards learning a low-rank weight matrix. The fundamental reason behind this is that the process of\\ntraining an LLN with the gradient descent algorithm results in larger polarization of the singular\\nvalues of the learned matrix in LLNs with more layers. The result is that large singular values are\\nampliﬁed while small ones are attenuated and tend to vanish.\\nConsider an N-layer LLN with a weight matrix of each layer being Wi. Let W be the weight matrix\\nthat represents the LLN in a single layer form, i.e., W = W1 × W2... × WN. Let σr denote the rth\\nsingular value of W. Let ur and vr be the rth left and right singular vectors of W, respectively. Let\\nthe loss function be L and ∇L(W(t)) be its gradient with respect to W at time t. Given a learning\\nrate η, the updates of the singular values are given by Eq. 6 (Theorem 3 in [3]):\\nσr(t + 1) ←σr(t) −η · N · (σr(t))2−2/N ·\\n\\n∇L(W(t)), ur(t)v⊤\\nr (t)\\n\\x0b\\n(6)\\nCritically, the term (σr(t))2−2/N captures the dependence on the number of layers. For a single-layer\\nmodel (N = 1), the term reduces to 1 for all r, making the update to σr(t) independent of its current\\nvalue. However, for a multi-layer LLN, the update term grows, linearly or faster, with the current\\nvalue of σr(t). For σr(t) < 1 , the term (σr(t))2−2/N is strictly less than 1 and gets smaller for\\n4\\nsmaller σr(t). Therefore, a multi-layer LLN attenuates the updates for small singular values. By the\\nsame reasoning, a multi-layer LLN enhances the updates for large singular values. As N increases,\\nthe gap between larger and smaller singular values increases, resulting in W having lower rank.\\nWe directly observed the bias towards the low-rank weight matrices by analyzing the distribution\\nof singular values of the embedding matrices in our MLET experiments. The Avazu dataset has\\n21 categorical features but two of them have far more items than the rest: feature-9 and feature-10\\nare jointly responsible for 99.7% of all embedding table entries. Now consider 8 singular values of\\nembeddings learned using a single-layer model with d = 8 and those from the MLET model with\\nk = 64 and d = 8. For feature-9, all 8 singular values of the single-layer model are larger than 0.01\\nof its largest singular value. However, only 2 singular values of the embedding produced by MLET\\nare larger than 0.01 of the largest one. Similarly, for feature-10, 5 singular values of the single-layer\\nmodel are larger than 0.01 of its largest singular value but only 2 singular values of the MLET model\\nare larger than 0.01 of the largest one.\\nWe use the above tendency of factorized linear layers to produce a lower-rank W to derive superior\\ncategory embeddings by replacing a single-layer embedding with a multi-layer embedding. In the\\nexperiments we describe in Section 4, we ﬁnd that using N = 2 is sufﬁcient and using higher N is\\nnot helpful.\\n2.2.2\\nDimensional Constraints in Multi-Layer Embeddings\\nIn MLET, the dimensions of the layers are important. For concreteness, we focus on a two-layer\\nembeddings (N = 2) and explain why MLET requires k ≥d. The theory of rank regularization\\ndoes not make any assumptions about the relation between k and d. The reason for imposing the\\nconstraint that k ≥d is to ensure that the search space of a two-layer model and that of its single-layer\\ncounterpart are identical. With this condition satisﬁed, the tendency of a multi-layer model towards a\\nlow-rank solution leads to superior generalization. When k < d, however, the search space of the\\nmulti-layer model is reduced to a subset of a single-layer model’s search space. This counteracts\\nthe beneﬁts of a lower-rank solution with no guaranteed improvement in generalization. We do not\\npropose to operate in this regime.\\nAgain, let the two matrices in the two-layer model be W1 ∈Rn×k and W2 ∈Rk×d. Let the matrix\\nin the single-layer model be W ∈Rn×d. The search space of a single-layer model is the set of\\nlinear transformations deﬁned by all possible matrices W: {W|W ∈Rn×d}. The search space of a\\ntwo-layer model is then the set of linear transformations deﬁned by all possible products of W1W2:\\n{W|W = W1W2, W1 ∈Rn×k, W2 ∈Rk×d}.\\nFirst, we formally prove that for k ≥d, the search space of a two-layer model is the same as that of a\\nsingle-layer model. Then, we prove that for k < d, the search space of a two-layer model is reduced\\nto a subset of the search space of a single-layer model.\\nTheorem 2.1. The search space of a two-layer model W1W2 is the same as that of a single-layer\\nmodel W when k ≥d: {W|W ∈Rn×d} = {W|W = W1W2, W1 ∈Rn×k, W2 ∈Rk×d}.\\nProof. We ﬁrst show that {W|W ∈Rn×d} ⊆{W|W = W1W2, W1 ∈Rn×k, W2 ∈Rk×d} by\\nshowing that for ∀W ∈Rn×d, ∃W1 ∈Rn×k and W2 ∈Rk×d, such that W = W1W2.\\nConsider the QR decomposition of W T :\\nW T = QR\\nQ ∈Rd×d\\nR ∈Rd×n\\n(7)\\nLet Idk denote the matrix constructed by concatenating (k −d) zero columns to a d × d identity\\nmatrix:\\nW1 = RT Idk\\nW2 = IT\\ndkQT\\n(8)\\nIt follows that W1 ∈Rn×k, W2 ∈Rk×d and W = W1W2.\\n5\\nWe now show that {W|W = W1W2, W1 ∈Rn×k, W2 ∈Rk×d} ⊆{W|W ∈Rn×d}. This follows\\nfrom the fact that any matrix represented by a product of W1W2 can be represented by a single matrix\\nW by simply letting W = W1W2.\\nTheorem 2.2. The search space of a two-layer model W1W2 is reduced compared to that of a single-\\nlayer model W when k < d: {W|W ∈Rn×d} ⊃{W|W = W1W2, W1 ∈Rn×k, W2 ∈Rk×d}.\\nProof. Let W be such that Rank(W) > k. Clearly, for both W1 and W2, Rank(W1) ≤k and\\nRank(W2) ≤k. Further, Rank(W1W2) ≤min(Rank(W1), Rank(W2)) = k. Thus, there does not\\nexist W1, W2 for which W = W1W2.\\n3\\nExperiments\\nWe evaluate the proposed algorithm on two public datasets for click-through rate tasks: Criteo-Kaggle\\nand Avazu. Both datasets are composed of a mix of categorical and real-valued features (Table 1).\\nBoth datasets are split into training, testing, and validation sets of 80%, 10%, and 10%, respectively.\\nThe Criteo-Kaggle dataset was split based on the time of data collection: the ﬁrst six days are used\\nfor training and the seventh day is split evenly into the test and validation sets. The Avazu dataset was\\nsplit randomly. The models are implemented in PyTorch. The experiments are run on two systems:\\n(a) an Intel i7-9700 CPU hosting an NVIDIA RTX2080 GPU with 8GB GDDR, and (b) an Intel\\ni7-8700 CPU hosting an NVIDIA Titan Xp GPU with 12GB GDDR. All experiments were run on\\nthe GPUs except for the Criteo-Kaggle dataset with k ≥128. In that case, the required memory\\nexceeded 12GB and the experiments were performed on the CPU of system (b). CPU throughput is\\n2 to 3 times lower compared to that of a GPU depending on the conﬁguration. For consistency, all\\nruntime estimates are produced from experiments run on NVIDIA RTX2080.\\nTable 1: Dataset Composition\\nDataset\\nTotal Records\\nDense Features\\nCategorical Features\\nCriteo-Kaggle\\n45,840,617\\n13\\n26\\nAvazu\\n40,400,000\\n1\\n21\\nDLRM has several hyperparameters. For both datasets we conﬁgure DLRM’s top MLP to have\\ntwo hidden layers with 512 and 256 nodes. For the Avazu dataset, we set DLRM’s bottom MLP\\nto be 256 →128 →d. For the Criteo-Kaggle dataset, we conﬁgure DLRM’s bottom MLP to be\\n512 →256 →128 →d. The bottom MLPs differ because their role is to handle the real-valued\\nfeatures which vary between datasets. In all experiments, d is set equal to the embedding dimension\\nso that vector sizes for the real-valued and categorical features match.\\nFollowing prior work [21], we train the models only for a single epoch, with a universal learning rate\\nof 0.2 and a batch of 128 using SGD as the optimizer. The linear factorization layers are initialized\\nusing a Gaussian distribution ∼N(0, 0.0625). The initialization is unique for each embedding table.\\nFor each hyperparameter conﬁguration, at least ﬁve training runs are performed to decrease the impact\\nof initialization variation and run-to-run variation due to non-deterministic GPU execution. We ﬁnd\\nthat the initial state of the embedding tables has a non-negligible impact on overall model performance\\nafter training. Additionally, even with the same initial conditions, we observe run-to-run variations\\nin the resulting model performance when using a GPU. We ascribe such run-to-run variation to the\\ndocumented non-determinism of the CUDA implementation of some PyTorch operators, such as\\nEmbeddingBag [1]. The reported data is based on the mean values of the replicated runs. We report\\ntwo performance metrics: area under the ROC curve (AUC) and binary cross-entropy (LogLoss).\\nRecall that d deﬁnes the size of the inference-time embedding vectors (and, the table) while k refers\\nto the width of the hidden linear layer in MLET.\\nThe experiments demonstrate the effectiveness of MLET in producing superior model size vs.\\nperformance trade-offs compared to the baseline single-layer embeddings implementation using\\nDLRM. Figures 2–3 summarize the results.\\n6\\nAs originally intended, the predominant system beneﬁt made possible by the superior model size vs.\\nperformance curves is in terms of reducing the embedding table size. As Figures 2–3 show, at the\\nsame model accuracy levels, we are able to produce models with d (and, therefore the embedding\\ntable size) 4-8 times smaller compared to the baseline DLRM.\\nThe beneﬁts begin to be observed in MLET curves even for k = d. Increasing k for a given d leads\\nto a monotonic improvement in model accuracy. For CTR systems, an improvement of 0.001 in\\nLogLoss is considered substantial. The maximum LogLoss beneﬁt of MLET for Criteo-Kaggle is\\n0.0025, and the maximum beneﬁt for Avazu is 0.006. This improvement in model accuracy saturates\\nas k grows, e.g., for the Criteo-Kaggle dataset the curves with k = 64 and k = 128 are very similar.\\nWe further observe that the relative performance improvements are largely deﬁned by k/d. Recall\\nthat MLET results in training memory increase of k/d compared to a single-layer training algorithm.\\nSince in practice we need to operate under a certain memory budget, there is a limit to the achievable\\nk/d. Speciﬁcally, the higher values of k/d can be achieved for smaller d, thus our technique is most\\neffective at lower d.\\nSo far, we analyzed either the table size reduction at a given accuracy, or accuracy improvement at a\\ngiven table size, with training memory requirement being the cost (in the sense that achieving both\\nbeneﬁts requires k > d). Interestingly, we ﬁnd there are also k and d combinations in which both\\naccuracy and size can be improved at, effectively, zero cost in terms of larger training-time memory.\\nWe say that a solution has zero cost if k ≤d. Symbolically, we can write that an MLET solution (k, d)\\ndominates (») a single-layer training solution (d) if both accuracy and table size are improved. In Fig.\\n2b, we see that several points exhibit such behavior in the Avazu dataset experiments: (64,64) » 128,\\n(64,32) » 128, (64,16) » 128, (32,16) » 32, and (32,8) » 32. Such behavior appears dataset-dependent\\nsince we do not ﬁnd such cost-free solutions for the Criteo-Kaggle dataset.\\n(a)\\n(b)\\nFigure 2: Validation Area Under the Curve (AUC) for Criteo-Kaggle and Avazu datasets.\\n(a)\\n(b)\\nFigure 3: Validation LogLoss for Criteo-Kaggle and Avazu datasets.\\n7\\nTable 2: DLRM and MLET training runtime on RTX2080\\nMODEL\\nk\\nd\\nms/iteration\\nDLRM\\n4\\n4\\n6.565\\nDLRM\\n8\\n8\\n6.548\\nDLRM\\n16\\n16\\n6.523\\nDLRM\\n32\\n32\\n6.530\\nDLRM\\n64\\n64\\n6.545\\nMLET\\n8\\n4\\n8.140\\nMLET\\n16\\n4\\n8.141\\nMLET\\n32\\n4\\n8.314\\nMLET\\n64\\n4\\n8.167\\nMLET\\n8\\n8\\n8.118\\nMLET\\n16\\n8\\n8.083\\nMLET\\n32\\n8\\n8.175\\nMLET\\n64\\n8\\n8.156\\nMLET\\n16\\n16\\n8.088\\nMLET\\n32\\n16\\n8.160\\nMLET\\n64\\n16\\n8.230\\nMLET\\n32\\n32\\n8.106\\nMLET\\n64\\n32\\n8.124\\nMLET\\n64\\n64\\n8.120\\nThe primary system impact of MLET is on training memory with some impact on runtime. MLET\\nresults in training memory increase of k/d compared to a single-layer training algorithm (with\\nembedding dimension d). We observe that for most experiments the memory requirement was below\\n12GB and exceeded that only for the Criteo-Kaggle dataset with k ≥128. At inference time, MLET\\nmemory consumption is equivalent to a single-layer DLRM model with embedding dimension d.\\nIn our naive implementation, the runtime cost of MLET training is 25% compared to DLRM. The\\nruntimes in terms of time per training iteration for various k, d pairs on NVIDIA RTX 2080 are\\nsummarized in Table 2.\\n4\\nConclusion\\nIn this paper, we introduced a multi-layer embedding training architecture that trains embeddings\\nvia a sequence of linear layers to derive a superior embedding accuracy vs. model size trade-off.\\nWe provide an explanation for obtaining superior embeddings based on the theory of dynamics of\\nbackpropagation in linear layer neural networks. We prototyped the MLET scheme within Facebook’s\\nPyTorch-based open-source Deep Learning Recommendation Model and demonstrated that it allows\\nreducing memory footprint by 4-8X without model accuracy degradation.\\n5\\nAcknowledgements\\nWe gratefully acknowledge the generous support of Facebook Research under the \"AI System Hard-\\nware/Software Co-Design\" program. We thank Maxim Naumov, Dheevatsa Mudigere, Constantine\\nCaramanis, and Sujay Sanghavi for many helpful discussions.\\nReferences\\n[1] Reproducibility. PyTorch Documentation v1.4.0 (2020).\\n[2] ALVAREZ, J. M., AND SALZMANN, M. Compression-aware training of deep networks. In\\nAdvances in Neural Information Processing Systems (2017), pp. 856–867.\\n[3] ARORA, S., COHEN, N., HU, W., AND LUO, Y. Implicit regularization in deep matrix\\nfactorization. In NeurIPS (2019).\\n8\\n[4] ARORA, S., GE, R., NEYSHABUR, B., AND ZHANG, Y. Stronger generalization bounds for\\ndeep nets via a compression approach. ArXiv abs/1802.05296 (2018).\\n[5] ATTENBERG, J., WEINBERGER, K., DASGUPTA, A., SMOLA, A., AND ZINKEVICH, M.\\nCollaborative email-spam ﬁltering with the hashing trick. CEAS.\\n[6] BHAVANA, P., KUMAR, V., AND PADMANABHAN, V.\\nBlock based singular value de-\\ncomposition approach to matrix factorization for recommender systems.\\narXiv preprint\\narXiv:1907.07410 (2019).\\n[7] CHEN, Y., XU, H., CARAMANIS, C., AND SANGHAVI, S. Matrix completion with column ma-\\nnipulation: Near-optimal sample-robustness-rank tradeoffs. IEEE Transactions on Information\\nTheory 62 (2011), 503–526.\\n[8] CHENG, H., KOC, L., HARMSEN, J., SHAKED, T., CHANDRA, T., ARADHYE, H., AN-\\nDERSON, G., CORRADO, G., CHAI, W., ISPIR, M., ANIL, R., HAQUE, Z., HONG, L.,\\nJAIN, V., LIU, X., AND SHAH, H. Wide & deep learning for recommender systems. CoRR\\nabs/1606.07792 (2016).\\n[9] GINART, A., NAUMOV, M., MUDIGERE, D., YANG, J., AND ZOU, J. Mixed dimension em-\\nbeddings with application to memory-efﬁcient recommendation systems. ArXiv abs/1909.11810\\n(2019).\\n[10] GINART, A., NAUMOV, M., MUDIGERE, D., YANG, J., AND ZOU, J. Mixed dimension\\nembeddings with application to memory-efﬁcient recommendation systems, 2019.\\n[11] GU, S., XIE, Q., MENG, D., ZUO, W., FENG, X., AND ZHANG, L. Weighted nuclear norm\\nminimization and its applications to low level vision. International Journal of Computer Vision\\n121 (2016), 183–208.\\n[12] GU, S., ZHANG, L., ZUO, W., AND FENG, X. Weighted nuclear norm minimization with\\napplication to image denoising. 2014 IEEE Conference on Computer Vision and Pattern\\nRecognition (2014), 2862–2869.\\n[13] GUO, H., TANG, R., YE, Y., LI, Z., AND HE, X. Deepfm: A factorization-machine based\\nneural network for ctr prediction. In IJCAI (2017).\\n[14] JUAN, Y.-C., ZHUANG, Y., CHIN, W.-S., AND LIN, C.-J. Field-aware factorization machines\\nfor ctr prediction. In RecSys ’16 (2016).\\n[15] KHRULKOV, V., HRINCHUK, O., MIRVAKHABOVA, L., AND OSELEDETS, I. Tensorized\\nembedding layers for efﬁcient model compression. arXiv preprint arXiv:1901.10787 (2019).\\n[16] LIAN, J., ZHOU, X., ZHANG, F., CHEN, Z., XIE, X., AND SUN, G. xdeepfm: Combining\\nexplicit and implicit feature interactions for recommender systems. Proceedings of the 24th\\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining (2018).\\n[17] LING, S., SONG, Y., AND ROTH, D. Word embeddings with limited memory. In Proceedings\\nof the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short\\nPapers) (Berlin, Germany, Aug. 2016), Association for Computational Linguistics, pp. 387–392.\\n[18] LIU, G., LIN, Z., AND YU, Y. Robust subspace segmentation by low-rank representation. In\\nICML (2010).\\n[19] NAUMOV, M. On the dimensionality of embeddings for sparse features and data. arXiv preprint\\narXiv:1901.02103 (2019).\\n[20] NAUMOV, M., DIRIL, U., PARK, J., RAY, B., JABLONSKI, J., AND TULLOCH, A. On periodic\\nfunctions as regularizers for quantization of neural networks. arXiv preprint arXiv:1811.09862\\n(2018).\\n[21] NAUMOV, M., MUDIGERE, D., SHI, H.-J. M., HUANG, J., SUNDARAMAN, N., PARK, J.,\\nWANG, X., GUPTA, U., WU, C.-J., AZZOLINI, A. G., DZHULGAKOV, D., MALLEVICH,\\nA., CHERNIAVSKII, I., LU, Y., KRISHNAMOORTHI, R., YU, A., KONDRATENKO, V. Y.,\\nPEREIRA, S., CHEN, X., CHEN, W., RAO, V., JIA, B., XIONG, L., AND SMELYANSKIY, M.\\nDeep learning recommendation model for personalization and recommendation systems. ArXiv\\nabs/1906.00091 (2019).\\n[22] OUYANG, W., ZHANG, X., REN, S., LI, L., LIU, Z., AND DU, Y. Click-through rate\\nprediction with the user memory network. ArXiv abs/1907.04667 (2019).\\n9\\n[23] PENG, C., KANG, Z., LI, H., AND CHENG, Q. Subspace clustering using log-determinant\\nrank approximation. In KDD ’15 (2015).\\n[24] RENDLE, S. Factorization machines. 2010 IEEE International Conference on Data Mining\\n(2010), 995–1000.\\n[25] SUN, F., GUO, J., LAN, Y., XU, J., AND CHENG, X. Sparse word embeddings using l1\\nregularized online learning. In Proceedings of the Twenty-Fifth International Joint Conference\\non Artiﬁcial Intelligence (2016), AAAI Press, pp. 2915–2921.\\n[26] SUN, Q., XIANG, S., AND YE, J. Robust principal component analysis via capped norms. In\\nKDD ’13 (2013).\\n[27] TISSIER, J., GRAVIER, C., AND HABRARD, A. Near-lossless binarization of word embeddings.\\nProceedings of the AAAI Conference on Artiﬁcial Intelligence 33 (Jul 2019), 7104–7111.\\n[28] WANG, R., FU, B., FU, G., AND WANG, M. Deep & cross network for ad click predictions.\\nIn ADKDD’17 (2017).\\n[29] YIN, Z., AND SHEN, Y. On the dimensionality of word embedding. ArXiv abs/1812.04224\\n(2018).\\n10\\n',\n",
       " '1911.02079': 'Post-Training 4-bit Quantization on Embedding\\nTables\\nHui Guan1, Andrey Malevich2, Jiyan Yang2, Jongsoo Park2, Hector Yuen2\\n1North Carolina State University\\n2Facebook, Inc\\nhguan2@ncsu.edu, {amalevich, chocjy, jongsoo, hyz}@fb.com\\nAbstract\\nContinuous representations have been widely adopted in recommender systems\\nwhere a large number of entities are represented using embedding vectors. As the\\ncardinality of the entities increases, the embedding components can easily contain\\nmillions of parameters and become the bottleneck in both storage and inference\\ndue to large memory consumption. This work focuses on post-training 4-bit quanti-\\nzation on the continuous embeddings. We propose row-wise uniform quantization\\nwith greedy search and codebook-based quantization that consistently outperforms\\nstate-of-the-art quantization approaches on reducing accuracy degradation. We\\ndeploy our uniform quantization technique on a production model in Facebook and\\ndemonstrate that it can reduce the model size to only 13.89% of the single-precision\\nversion while the model quality stays neutral.\\n1\\nIntroduction\\nThe success of word embeddings in Natural Language Processing (NLP) [23, 20] has promoted the\\nwide adoption of continuous representations in recommender systems in recent years. Embedding-\\nbased approaches have achieved state-of-the-art performance in recommendation and ranking tasks\\nand have been successfully applied in real-world applications [21, 22, 5, 16, 12]. In these recommen-\\ndation models, a large number of entities such as page ids and user ids are encoded using embedding\\ntables whose row vectors correspond to entities. As embedding tables scale with the number of entities\\nand embedding dimensions, they can easily contain billions of parameters and usually contribute to\\n99.99% of the size of the models. For example, a single-precision embedding table with 50,000,000\\nnumber of ids and 64 embedding dimensions costs 12GB memory and a recommendation model could\\ncontain up to hundreds of embedding tables. Furthermore, due to memory-bandwidth limitations,\\nembedding table lookups are one of the most time-consuming operations. Their proportion will\\nincrease due to the acceleration of other parts (e.g. FC) from faster increase of compute throughput\\nthan memory bandwidth, posing a great challenge to get real-time predictions [22, 21].\\nQuantization is one of the effective approaches to reduce model size. By quantizing ﬂoating-point\\nvalues in embedding tables to low-precision numbers that use less number of bits, a large recom-\\nmendation model can be reduced to a model with much smaller model size and memory bandwidth\\nconsumption during inference. Prior work on quantization has been focusing on quantization-aware\\ntraining from scratch or a pre-trained ﬂoating-point model [7, 28, 26, 11, 18, 8, 15, 10]. Although\\nthese techniques have shown promising results, they are not always applicable in many practical sce-\\nnarios where the training dataset might be no longer available during model deployment [27, 3, 6, 19].\\nIn these cases, post-training quantization is a more desirable approach. Post-training quantization\\nis simple to use and convenient for rapid deployment. Recent studies have shown post-training\\nquantization using 8-bit precision can achieve accuracy close to that of single-precision models in a\\nwide variety of DNN architectures [19, 14]. Post-training quantization using lower bit width (e.g.\\n4-bit), however, usually incurs signiﬁcant accuracy drop [6].\\n33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.\\narXiv:1911.02079v1  [cs.LG]  5 Nov 2019\\nSeveral state-of-the-art post-training quantization techniques that rely on the clipping have been\\nproposed to mitigate accuracy degradation. Shin et al. [24] and Sung et al. [25] approximated the\\ninputs as a histogram and adopt a clipping threshold that minimizes the ℓ2 norm of the quantization\\nerror. Migacz et al. [19] proposed an iterative approach to search for the clipping threshold based on\\nKullback-Leibler Divergence measure for quantizing activations. Later, Banner et al. [3] proposed\\nACIQ, an analytic solution that computes the optimal clip threshold by assuming the input values are\\nsampled from a Gaussian or Laplacian distribution. Although these approaches are demonstrated to\\nreduce the accuracy drops to some extent, the problem of post-training 4-bit quantization without\\naccuracy drop is still unsolved yet. Empirically, we also observe that the above-mentioned approaches\\ncan result in signiﬁcant accuracy drops when applied to embedding table quantization.\\nIn this paper, we explore a variety of post-training 4-bit quantization methods on embedding tables\\nand propose novel quantization approaches that can reduce model size while incurring negligible\\naccuracy degradation. Quantization on embedding tables is usually applied to row vectors (row-wise\\nquantization) to reduce quantization error. Throughout the paper, quantization is applied to row\\nvectors unless noted differently. We notice that the prior post-training quantization approaches approx-\\nimate the inputs to quantize using either a histogram or some distributions. While these assumptions\\nare beneﬁcial to derive efﬁcient algorithms for ﬁnding the optimal clipping thresholds for weights\\nand activations of convolutional neural networks (CNNs), they are not suitable for embedding tables\\nbecause their row vectors contain too few values to be well-characterized using either histograms or\\ndistributions. Inspired by these understandings, we design quantization algorithms that directly target\\nat minimizing the mean square error after quantization. Speciﬁcally,\\n• Our exploration reveals that state-of-the-art post-training 4-bit quantization approaches are\\nno better than the approach that uses the range the input without clipping when the input\\ncontains only tens or hundreds of values, as in the case of embedding table quantization.\\n• We propose two simple yet effective approaches to improve 4-bit quantization on embedding\\ntables: 1) row-wise uniform quantization with greedy search that ﬁnds the best clipping\\nthresholds from a gradually discovered set of local optima; 2) codebook-based quantization\\nthat maps inputs to indices of non-uniformly distributed values using k-means clustering.\\n• Moreover, for 4-bit quantized embedding tables using uniform quantization, we can achieve\\ndequantization performance similar to the Caffe2 8-bit dequantization operators (e.g., Sparse-\\nLengthSum1) that are already heavily optimized.\\nWe empirically demonstrate the effectiveness of our proposed quantization approaches on DNN-based\\nrecommendation models [26, 21] and also a production model in Facebook. The results show that the\\nproposed approaches consistently outperform state-of-the-art post-training quantization approaches\\nin reducing quantization error and accuracy degradation. Row-wise uniform quantization with greedy\\nsearch can reduce the model size to 13.3%-25.0% of the baseline single-precision models with\\nnegligible accuracy loss. Codebook-based quantization can reduce the model size to 18.5%-37.5% of\\nthe single-precision model with no accuracy loss. We deploy our uniform quantization technique on\\na production model in Facebook and demonstrate that it can reduce the model size to only 13.89% of\\nthe single-precision version while the model quality stays neutral.\\n2\\nPrior Quantization Methods and Their Limitations\\nLet x be a value clipped to the range [xmin, xmax]. Quantization using n bits maps x to an integer in\\nthe range [0, 2n −1], where each integer corresponds to a quantized value. If the quantized values are\\na set of discrete, evenly-spaced grid points, the methods are called uniform quantization. Otherwise,\\nthey are called non-uniform quantization. This section reviews several state-of-the-art post-training\\nuniform quantization methods and explains their limitations on embedding table quantization.\\nLet xint and xfloat be the quantized and dequantized values respectively. Uniform quantization\\nproceeds as follows:\\nxint = round\\n\\x12\\nx −xmin\\nxmax −xmin\\n∗(2n −1)\\n\\x13\\n= round\\n\\x12x −bias\\nscale\\n\\x13\\n,\\n(1)\\n1https://caffe2.ai/docs/operators-catalogue.html#sparselengthssum\\n2\\nwhere scale = xmax−xmin\\n2n−1\\nand bias = xmin. The de-quantization operation is: xfloat = scale ∗\\nxint + bias. The quantization is symmetric if xmax = −xmin. Otherwise, it is asymmetric. To ease\\nthe description below, we deﬁne a quantization function2 as: xfloat = Q(x, xmin, xmax).\\n4\\n6\\n8\\n10\\n12\\n14\\nlog(d)\\n0.000\\n0.025\\n0.050\\n0.075\\n0.100\\n0.125\\n0.150\\n0.175\\nnormalized l2 loss\\nASYM\\nHIST-APPRX\\nHIST-BRUTE\\nGSS\\nACIQ\\nGREEDY\\nGREEDY(opt)\\nKMEANS\\nTABLE\\nFigure 1: The normalized ℓ2 loss of 4-bit quantization\\nwith different embedding dimensions on a FP32 embed-\\nding table with 10 row vectors. The values in the embed-\\nding table are randomly sampled from a normal distri-\\nbution, which is in favor of GSS and especially ACIQ.\\nTABLE applies range-based uniform quantization on\\nthe entire table while the other methods are on row vec-\\ntors. HIST-APPRX and HIST-BRUTE use b = 200,\\nGREEDY uses b = 200, r = 0.16; GREEDY (opt) uses\\nb = 1000, r = 0.5.\\nWithout loss of generality, let X ∈RN be an\\ninput vector to quantize. Tensors with higher\\ndimensions can be ﬂattened to a vector. Be-\\ncause each value is scaled by xmax −xmin, the\\nnaive quantization using xmax = max(X) and\\nxmin = min(X) is sensitive to outliers, i.e.,\\nvalues with large magnitude in the input X, and\\ncould cause large accuracy drops. We refer to\\nthis method range-based asymmetric quanti-\\nzation (ASYM).\\nState-of-the-art post-training quantization tech-\\nniques rely on the clipping to mitigate accuracy\\ndegradation. They differ in their way to mini-\\nmize the mean squared error (MSE) of the orig-\\ninal values and the quantized values:\\nf(xmin, xmax) = ∥X −Q(X, xmin, xmax)∥2\\n2.\\n(2)\\nHistogram-based\\nQuantization\\n(HIST)\\nThis method chooses the clipping thresholds\\nwhich minimize the MSE between the his-\\ntogram of ﬂoating-point inputs and that of\\nthe quantized versions [24]. Let xi and h(xi),\\nwhere i = 1, · · · , b, be the bin value and the frequency of the i-th bin in the inputs’ histogram. The op-\\ntimization objective is deﬁned as: fhist(xmin, xmax) = 1\\nb\\nPb\\ni=1 h(xi) ∗(xi −Q(xi, xmin, xmax))2.\\nWe used the approximate algorithm (HIST-APPRX) implemented in Caffe2 [1] that scales linearly\\nwith the number of bins to solve the above optimization problem. We also implemented a brute force\\napproach (HIST-BRUTE) to ﬁnd better solutions. Its time complexity is O(b3) (See Appendix A).\\nACIQ\\nAnalytical Clipping for Integer Quantization (ACIQ) [3] derives the clipping thresholds\\nanalytically from the distribution of the tensor. It assumes that the values in the tensor are sampled\\nfrom a Gaussian or a Laplacian distribution. After determining the distribution to use, it uses an\\napproximate closed-form solution for the clip thresholds which minimizes MSE in Eq. 2. For example,\\nif the tensor is closer to a Laplacian distribution, the clipping thresholds for 4-bit quantization are\\ncalculated using the formula: xmin = E(X)−α, xmax = E(X)+α, where α = 5.03∗E(|X−E(X)).\\nWe used the open-source code from the authors3.\\nQuantization with Golden Section Search (GSS)\\nInstead of approximating the ﬂoating-point\\ninputs using a histogram or assuming it follows a certain distribution, this approach ﬁnds a range\\nlimit xthr that minimizes MSE using golden section search (GSS) [13] for symmetric quantization.\\nThe objective function is simpliﬁed as: fsym(xthr) = 1\\nN ∥X −Q(X, −xthr, xthr))∥2\\n2. The method\\nis applied to compress word embeddings in [17].\\nTheir Limitations\\nQuantization on embedding tables is commonly applied to row vectors to\\nreduce the quantization error (See ASYM v.s. TABLE in Figure 1). The embedding dimension in\\nrecommendation models is usually 8 to 200 [21]. The above clipping-based approaches are better than\\nthe range-based asymmetric quantization (ASYM) method when quantizing weights and activations\\nof CNNs to 4-bit. However, we empirically observed that they are no better than ASYM when\\nthe input X to quantize has a small dimension (i.e., a small number of values), as in the case of\\n2An alternative uniform quantization uses: xint = round(x/scale) −zero_point. This method is better\\nwhen the inputs to quantize have lots of zeros, e.g., ReLU activations. We found that the mapping in Eq. 1\\nprovides better accuracy for embedding table quantization.\\n3https://github.com/submission2019/cnn-quantization\\n3\\nembedding table quantization. Approximating row vectors in an embedding table using histograms\\nor distributions could give large quantization error.\\nFigure 1 shows the normalized ℓ2 loss of different quantization methods with various embedding\\ndimensions. Normalized ℓ2 loss is calculated as ∥X −Q(X, xmin, xmax)∥2/∥X∥2. It measures\\nrelative quantization errors. Overall, when the embedding dimension is larger than 1024, GSS, ACIQ,\\nHIST-APPRX, and HIST-BRUTE can achieve a smaller loss compared with ASYM. However, when\\nthe embedding dimension is small (e.g. 64), the advantage of GSS, ACIQ, and HIST-APPRX over\\nASYM is gone. GSS is even much worse than ASYM. Although HIST-BRUTE is still better than\\nASYM, it is very time-consuming (millions of times slower than ASYM) and too expensive to be\\napplied in real-world recommendation applications that require continuous learning and thus periodic\\nquantization for model serving (See Appendix A).\\n3\\nProposed Quantization Approaches\\nThis section elaborates the proposed uniform quantization with greedy search and codebook-based\\nquantization with k-means clustering.\\nAlgorithm 1 greedy search\\nInput: X\\n// a vector to quantize.\\nInput: b\\n// default: 200\\nInput: r\\n// default: 0.16\\nOutput: xmin, xmax // range used for quantization\\n1: xmin = cur_min = min(X)\\n2: xmax = cur_max = max(X)\\n3: loss = compute_loss(X, Q(X, xmin, xmax))\\n4: stepsize = (xmax - xmin)/b\\n5: min_steps = b * (1 - r) * stepsize\\n6: while cur_min + min_steps < cur_max do\\n7:\\nloss_l = compute_loss(X, Q(X, cur_min + step-\\nsize, cur_max))\\n8:\\nloss_r = compute_loss(X,Q(X, cur_min, cur_max\\n- stepsize))\\n9:\\nif loss_l < loss_r then\\n10:\\ncur_min = cur_min + stepsize\\n11:\\nif loss_l < loss then\\n12:\\nloss, xmin = loss_1, cur_min\\n13:\\nelse\\n14:\\ncur_max = cur_max - stepsize\\n15:\\nif loss_r < loss then\\n16:\\nloss, xmax = loss_r, cur_max\\n17: return xmin, xmax\\nUniform Quantization with Greedy Search\\n(GREEDY)\\nTo overcome the limitations of\\nthe prior uniform quantization approaches, we\\npropose a greedy search algorithm (see Algo-\\nrithm 1) to ﬁnd the optimal clipping thresholds.\\nThe algorithm is inspired by the 1-D golden\\nsection search (GSS) and directly targets at min-\\nimizing the MSE objective function in Eq. 2.\\nAlthough 2-D GSS was proposed recently, it is\\nnot applicable in general as it is too consum-\\ning [4]. The basic idea of greedy search is to\\nﬁnd as many local optima as possible and select\\nthe best one as the clipping thresholds. The al-\\ngorithm takes the input vector X and two hyper-\\nparameters b and r that balance the optimality\\nof its solution and its time complexity.\\nThe algorithm initializes xmin and xmax with\\nthe range of the input X (lines 1-2). It then\\ngradually increases xmin or decreases xmax\\nby one stepsize to reduce the range and ﬁnd\\na smaller loss calculated as Eq. 2 (lines 7-16).\\nThe algorithm stops when the current range is\\n1 −r percentage of the range of X (lines 5-6).\\nThe larger the b and r are, the better the found\\nsolution will be but the higher the time cost is\\n(O(b × r) time complexity). The default value\\nof b and r are set as 200 and 0.16 respectively.\\nCodebook-based Quantization\\nCodebook-based quantization is a type of non-uniform quantiza-\\ntion that maps each input value to the index of a value in the codebook. Quantizing a value to 4 bits\\nmeans the number of values in a codebook cannot be larger than 16. We consider the following two\\ncodebook-based quantization variants: Quantization with Rowwise Clustering (KMEANS) and\\nQuantization with Two-Tier Clustering (KMEANS-CLS). KMEANS algorithm applies k-means\\nclustering to produce a 16-value codebook for each row vector, and then maps each value in the row\\nvector to the index of the codebook based on its cluster assignment. Although the algorithm has less\\nmodel size reduction than uniform quantization due to the storage overhead of codebooks, it has the\\npotential to achieve a lower MSE and avoid accuracy degradation.\\nTo achieve a larger compression rate, KMEANS-CLS applies k-means clustering in a more coarse-\\ngrained way. The algorithm ﬁrst groups similar row vectors in an embedding table into blocks (called\\ntier-1 clustering) and then generates a 16-value codebook for each block (called tier-2 clustering).\\n4\\nTable 1: Computational throughput in billion sums per second for SparseLengthsSum operators.\\nThe performance is measured using a single core of Intel Xeon Gold 6138 CPU @ 2.0 GHz with\\nturbo-mode off.\\nData type\\nCache non-resident case\\nCache resident case\\nd=64\\nd=128\\nd=256\\nd=512\\nd=64\\nd=128\\nd=256\\nd=512\\nFP32\\n1.939\\n1.908\\n1.997\\n2.063\\n2.804\\n4.165\\n4.209\\n4.127\\nINT8\\n1.246\\n1.511\\n2.726\\n3.076\\n2.242\\n2.510\\n3.748\\n3.450\\nINT4\\n1.608\\n2.047\\n2.532\\n5.581\\n2.093\\n2.878\\n6.454\\n6.893\\nBoth steps use k-means clustering. Let K be the number of clusters in tier-1 clustering. After 4-bit\\nquantization using KMEANS-CLS, the required number of bytes to store a N × d embedding table\\nis Nd/2 + N log2 K/8 + 64K, where log2 K/8 is the number of bytes used to store tier-1 cluster\\nassignment.\\n4\\nEfﬁcient 4-bit Embedding Operation Implementation\\nA challenge for quantizing embedding tables in less than 8-bit is the overhead of dequantization when\\nreading the tables. This is because, in most commonly used processors, 8-bit is the smallest granularity\\nin which instructions can operate with, and less than 8-bit granularity requires bit manipulations\\nlike or, shift, and so on. Nevertheless, we found that we can sustain good enough dequantization\\nthroughput with careful use of vector instructions available in recent CPUs (e.g., AVX512 in Intel\\nSkylake CPUs) as shown in Table 1. We measure computational throughput of SparseLengthsSum\\noperator (the most time-consuming operator reading embedding tables in our recommendation\\nmodels [21]) in FP32, INT8, and INT4, both in cache non-resident and cache resident cases. In\\ncache non-resident cases, we ﬂush the last level cache between benchmark runs, which is more\\nrepresentative of running big recommendation models with many huge embedding tables. The cache\\nresident cases are to see upper bound computational throughputs (a worst case for 4-bit embedding\\ntables). We can see that in most cases the speed of 4-bit SparseLengthsSum is on par or faster than its\\nhighly-optimized 8-bit or FP32 counterparts running in production.\\n5\\nExperiments\\nIn this section, we present the experimental results of the proposed approaches. We use the Terabyte\\nCriteo data [2]. It is a click prediction dataset that has a size of 1.3TB and contains more than 4.3\\nbillion records. The dataset is a commonly used benchmark dataset for ranking applications.\\nThe ranking problem is a binary classiﬁcation problem. The models we used are DNN models [21].\\nFor categorical features, following the same procedure as in [26], we transform them into dense\\nvectors using embedding tables. The number of rows in embedding tables corresponds to the number\\nof categorical features with a maximum of 50 million. The number of columns corresponds to the\\nembedding dimension. We choose a variety of embedding dimensions: 8, 16, 32, 64, and 128, that are\\ncommonly used in ranking models. The dense embeddings of the categorical features, concatenated\\nwith the dense vector formed by dense features, are taken as the input to a neural network with\\n2 fully-connected (FC) layers. The FC layers have a width of 512. The models are trained using\\nAdagrad [9] with a batch size of 100. The initial learning rate is set to 0.015 for embedding tables\\nand 0.005 for the rest of the parameters. All the parameters are trained using single-precision (FP32).\\nAll the embedding tables are quantized to use 4 bits after model training is ﬁnished.\\nWe compare the proposed approaches (GREEDY, KMEANS, KMEANS-CLS) with other uniform\\nquantization approaches including SYM, GSS, ASYM, HIST-APPROX, HIST-BRUTE, ACIQ.\\nBecause k-means is sensitive to initialization, we initialize cluster centers using uniform quantization\\nresults from ASYM. The default hyperparameter settings (b = 200, r = 0.16) are used for greedy\\nsearch. HIST-APPRX uses b = 200 as it gives the best performance after a grid-based hyper-\\nparameter tuning. For KMEANS-CLS, we choose the K such that it achieves the same compression\\nrate as the uniform quantization approaches. If a method is appended with “(FP16)”, it means that\\nthe scales and biases in uniform quantization and the codebooks in codebook-based quantization\\nare stored using FP16 instead of FP32. Besides the baseline where embedding tables are not\\nquantized (FP32), we include another baseline that uses range-based uniform quantization to quantize\\n5\\nTable 2: Normalized l2 loss with different quantization methods and embedding dimensions.\\nMethods\\nDescription\\nd=8\\nd=16\\nd=32\\nd=64\\nd=128\\nASYM-8BITS\\nAsymmetric, xmin = min(X), xmax = max(X)\\n0.00260\\n0.00329\\n0.00376\\n0.00387\\n0.00400\\nSYM\\nSymmetric, xmin = −xmax, xmax = max(|X|)\\n0.05564\\n0.06296\\n0.06785\\n0.06836\\n0.06928\\nGSS\\nSymmetric with Golden Section Search\\n0.05269\\n0.05965\\n0.06328\\n0.06400\\n0.06423\\nASYM\\nAsymmetric, xmin = min(X), xmax = max(X)\\n0.04451\\n0.05479\\n0.06387\\n0.06608\\n0.06781\\nHIST-APPRX\\nAsymmetric with histogram-based approximation [1]\\n0.04452\\n0.05512\\n0.06409\\n0.06589\\n0.06768\\nHIST-BRUTE\\nAsymmetric with histogram-based brute force algorithm\\n0.04156\\n0.05082\\n0.05881\\n0.06083\\n0.06272\\nACIQ\\nAnalytical Clipping for Integer Quantization [3]\\n0.04451\\n0.05479\\n0.06387\\n0.06742\\n0.07665\\nGREEDY\\nAsymmetric with greedy search (ours)\\n0.03889\\n0.04878\\n0.05744\\n0.05991\\n0.06221\\nGREEDY (FP16)\\nAsymmetric with greedy search (ours)\\n0.03889\\n0.04879\\n0.05744\\n0.05991\\n0.06221\\nKMEANS-CLS (FP16)\\nTwo-tier k-means clustering with uniform init (ours)\\n0.03948\\n0.05349\\n0.06826\\n0.07369\\n0.07287\\nKMEANS (FP16)\\nRowwise kmeans clustering with uniform init (ours)\\n0\\n0\\n0.03670\\n0.05160\\n0.05781\\n\"-8BITS\": 8-bit quantization; otherwise, 4-bit quantization. \"(FP16)\": scales and biases or values of a codebook in FP16; otherwise, FP32.\\nTable 3: Model log loss and size with different quantization methods and embedding dimensions.\\nMethods\\nd=8\\nd=16\\nd=32\\nd=64\\nd=128\\nloss\\nsize\\nloss\\nsize\\nloss\\nsize\\nloss\\nsize\\nloss\\nsize\\nFP32 (no quantization)\\n0.12522\\n8.07GB\\n0.12489\\n16.14GB\\n0.12468\\n32.27GB\\n0.12451\\n64.54GB\\n0.12454\\n129.09GB\\nASYM-8BITS\\n0.12522\\n49.98%\\n0.12489\\n37.49%\\n0.12469\\n31.25%\\n0.12451\\n28.12%\\n0.12454\\n26.56%\\nSYM\\n0.12528\\n37.49%\\n0.12507\\n24.99%\\n0.13266\\n18.75%\\n0.13107\\n15.62%\\n0.12470\\n14.06%\\nGSS\\n0.12527\\n0.12504\\n0.13199\\n0.12843\\n0.12459\\nASYM\\n0.12526\\n0.12491\\n0.12496\\n0.12494\\n0.12455\\nHIST-APPRX\\n0.12525\\n0.12492\\n0.12497\\n0.12498\\n0.12455\\nHIST-BRUTE\\n0.12525\\n0.12490\\n0.12490\\n0.12489\\n0.12454\\nACIQ\\n0.12526\\n0.12491\\n0.12804\\n0.12514\\n0.12455\\nGREEDY\\n0.12525\\n0.12490\\n0.12489\\n0.12485\\n0.12454\\nGREEDY (FP16)\\n0.12525\\n24.99%\\n0.12490\\n18.74%\\n0.12489\\n15.62%\\n0.12485\\n14.06%\\n0.12454\\n13.28%\\nKMEANS (FP16)\\n-\\n-\\n-\\n-\\n0.12469\\n37.50%\\n0.12451\\n25.00%\\n0.12454\\n18.75%\\nall embedding tables to 8 bits (ASYM-8BITS). We evaluate the performance of the quantization\\napproaches using three evaluation metrics: Normalized ℓ2 loss, model log loss, and model size.\\nTable 2 lists the normalized ℓ2 loss results on an embedding table from models with different\\nembedding dimensions. Overall, our proposed approach GREEDY consistently gives the smallest loss\\namong all 4-bit uniform quantization approaches. Using FP16 for scales and biases further reduces\\nthe embedding table size without affecting the loss. KMEANS achieves the smallest normalized ℓ2\\nloss for the use of codebook. Even though KMEANS-CLS variants can achieve the same compression\\nrate as the uniform quantization approaches, they suffer from larger losses, indicating the importance\\nof row-wise quantization for embedding tables.\\nTable 3 lists the model log loss and model size for the models after 4-bit quantization. Overall,\\nGREEDY consistently gives the smallest model log loss compared with other uniform quantization\\napproaches while reducing the models to 13.3%-25.0% of the single-precision model size. The\\nproposed KMEANS approach can even get the same model loss as the original single-precision\\nmodel while reducing the models to 18.8%-37.5% of the single-precision model size.\\nWe deployed GREEDY on one of the ranking applications at Facebook. The application uses a DNN\\nmodel trained on billions of records. Being able to reduce the model size using post-training 4-bit\\nquantization while preserving model accuracy is a challenging task. Our experimental results show\\nthat the 4-bit uniform quantization with greedy search can reduce the model size to only 13.89% of\\nthe single-precision version while the model quality stays neutral. This demonstrates the practicality\\nof our approach in real applications.\\n6\\nConclusions and Future Work\\nWe proposed row-wise uniform quantization with greedy search and non-uniform quantization with\\nk-means clustering to improve 4-bit post-training quantization on embedding tables. We empirically\\nshowed that the proposed approaches consistently outperform state-of-the-art quantization methods\\non reducing the quantization error and the model accuracy degradation. The model size reduction\\nresulting from 4-bit quantization makes it possible to use even larger embedding tables for potentially\\nbetter model accuracy. In the future, we want to explore how much accuracy gain can be achieved by\\nincreasing model size while applying 4-bit quantization to meet a certain space budget.\\n6\\nReferences\\n[1] caffe2 histogram-based norm minimization. https://caffe2.ai/doxygen-c/html/norm_\\n_minimization_8cc_source.html. Accessed: 2019-08-03.\\n[2] Criteo\\nreleases\\nindustry’s\\nlargest-ever\\ndataset\\nfor\\nmachine\\nlearning\\nto\\nacademic\\ncommunity.\\nhttps://www.criteo.com/news/press-releases/2015/07/\\ncriteo-releases-industrys-largest-ever-dataset/. Accessed: 2019-08-03.\\n[3] Ron Banner, Yury Nahshan, Elad Hoffer, and Daniel Soudry. Aciq: Analytical clipping for\\ninteger quantization of neural networks. arXiv preprint arXiv:1810.05723, 2018.\\n[4] Yen-Ching Chang. N-dimension golden section search: Its variants and limitations. In 2009 2nd\\nInternational Conference on Biomedical Engineering and Informatics, pages 1–6. IEEE, 2009.\\n[5] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra, Hrishi Aradhye,\\nGlen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al. Wide & deep learning for\\nrecommender systems. In Proceedings of the 1st workshop on deep learning for recommender\\nsystems, pages 7–10. ACM, 2016.\\n[6] Yoni Choukroun, Eli Kravchik, and Pavel Kisilev. Low-bit quantization of neural networks for\\nefﬁcient inference. arXiv preprint arXiv:1902.06822, 2019.\\n[7] Matthieu Courbariaux and Yoshua Bengio. Binarynet: Training deep neural networks with\\nweights and activations constrained to+ 1 or- 1. arxiv 2016. arXiv preprint arXiv:1602.02830.\\n[8] Christopher De Sa, Megan Leszczynski, Jian Zhang, Alana Marzoev, Christopher R Aberger,\\nKunle Olukotun, and Christopher Ré. High-accuracy low-precision training. arXiv preprint\\narXiv:1803.03383, 2018.\\n[9] John Duchi, Elad Hazan, and Yoram Singer. Adaptive subgradient methods for online learning\\nand stochastic optimization. Journal of Machine Learning Research, 12(Jul):2121–2159, 2011.\\n[10] Alexander Goncharenko, Andrey Denisov, Sergey Alyamkin, and Evgeny Terentev. Fast\\nadjustable threshold for uniform neural network quantization. arXiv preprint arXiv:1812.07872,\\n2018.\\n[11] Qinyao He, He Wen, Shuchang Zhou, Yuxin Wu, Cong Yao, Xinyu Zhou, and Yuheng Zou.\\nEffective quantization methods for recurrent neural networks. arXiv preprint arXiv:1611.10176,\\n2016.\\n[12] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. Neural\\ncollaborative ﬁltering. In Proceedings of the 26th international conference on world wide web,\\npages 173–182. International World Wide Web Conferences Steering Committee, 2017.\\n[13] Jack Kiefer. Sequential minimax search for a maximum. Proceedings of the American mathe-\\nmatical society, 4(3):502–506, 1953.\\n[14] Raghuraman Krishnamoorthi. Quantizing deep convolutional networks for efﬁcient inference:\\nA whitepaper. arXiv preprint arXiv:1806.08342, 2018.\\n[15] Hao Li, Soham De, Zheng Xu, Christoph Studer, Hanan Samet, and Tom Goldstein. Training\\nquantized nets: A deeper understanding. In Advances in Neural Information Processing Systems,\\npages 5811–5821, 2017.\\n[16] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and Guangzhong\\nSun. xdeepfm: Combining explicit and implicit feature interactions for recommender systems.\\nIn Proceedings of the 24th ACM SIGKDD International Conference on Knowledge Discovery\\n& Data Mining, pages 1754–1763. ACM, 2018.\\n[17] Avner May, Jian Zhang, Tri Dao, and Christopher Ré. On the downstream performance of\\ncompressed word embeddings. arXiv preprint arXiv:1909.01264, 2019.\\n7\\n[18] Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\\ntraining. arXiv preprint arXiv:1710.03740, 2017.\\n[19] Szymon Migacz. 8-bit inference with tensorrt. In GPU technology conference, volume 2, page 7,\\n2017.\\n[20] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed repre-\\nsentations of words and phrases and their compositionality. In Advances in neural information\\nprocessing systems, pages 3111–3119, 2013.\\n[21] Maxim Naumov, Dheevatsa Mudigere, Hao-Jun Michael Shi, Jianyu Huang, Narayanan Sun-\\ndaraman, Jongsoo Park, Xiaodong Wang, Udit Gupta, Carole-Jean Wu, Alisson G Azzolini,\\net al. Deep learning recommendation model for personalization and recommendation systems.\\narXiv preprint arXiv:1906.00091, 2019.\\n[22] Jongsoo Park, Maxim Naumov, Protonu Basu, Summer Deng, Aravind Kalaiah, Daya Khudia,\\nJames Law, Parth Malani, Andrey Malevich, Satish Nadathur, et al. Deep learning inference in\\nfacebook data centers: Characterization, performance optimizations and hardware implications.\\narXiv preprint arXiv:1811.09886, 2018.\\n[23] Jeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for\\nword representation. In Proceedings of the 2014 conference on empirical methods in natural\\nlanguage processing (EMNLP), pages 1532–1543, 2014.\\n[24] Sungho Shin, Kyuyeon Hwang, and Wonyong Sung. Fixed-point performance analysis of\\nrecurrent neural networks. In 2016 IEEE International Conference on Acoustics, Speech and\\nSignal Processing (ICASSP), pages 976–980. IEEE, 2016.\\n[25] Wonyong Sung, Sungho Shin, and Kyuyeon Hwang. Resiliency of deep neural networks under\\nquantization. arXiv preprint arXiv:1511.06488, 2015.\\n[26] Jian Zhang, Jiyan Yang, and Hector Yuen. Training with low-precision embedding tables. In\\nSystems for Machine Learning Workshop at NeurIPS, volume 2018, 2018.\\n[27] Ritchie Zhao, Yuwei Hu, Jordan Dotzel, Chris De Sa, and Zhiru Zhang. Improving neural net-\\nwork quantization without retraining using outlier channel splitting. In International Conference\\non Machine Learning, pages 7543–7552, 2019.\\n[28] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu Zhou, He Wen, and Yuheng Zou. Dorefa-net:\\nTraining low bitwidth convolutional neural networks with low bitwidth gradients. arXiv preprint\\narXiv:1606.06160, 2016.\\n8\\nA\\nHIST-BRUTE\\nIn this section, we present the algorithm and the complexity analysis of the brute force histogram-\\nbased quantization approach (HIST-BRUTE). Its pseudo-code is shown in Algorithm 2. For 4-bit\\nquantization, the algorithm uses a histogram with 16 number of bins dst_nbins to approximate the\\nhistogram of the inputs with b number of bins. Lines 1-9 are for initialization. The algorithm selects\\ndifferent numbers of consecutive bins from the inputs’ histogram and approximates these selected\\nbins using 16 target bins (lines 10-36). The selected bins determine the clipping thresholds (lines\\n37-42). HIST-BRUTE has a time complexity of O(b3).\\nAlgorithm 2 HIST-BRUTE\\nInput: X // a vector to quantize.\\nInput: b // number of bins used to generate histogram, default: 200\\nOutput: xmin, xmax // range used for quantization\\n1: // Initialize\\n2: xmin = min(X)\\n3: xmax = max(X)\\n4: histogram = get_histogram(X, b)\\n5: dst_nbins = 16\\n6: bin_width = (xmax - xmin)/b\\n7: norm_min = ∞\\n8: best_start_bin = -1\\n9: best_nbins_selected = 1\\n10: for nbins_selected = 1 to b do\\n11:\\nstart_bin_begin = 0\\n12:\\nstart_bin_end = b - nbins_selected + 1\\n13:\\ndst_bin_width = bin_width * nbins_selected / (dst_nbins - 1)\\n14:\\nfor start_bin = start_bin_begin to start_bin_end do\\n15:\\nnorm = 0\\n16:\\n// Go over each histogram bin and accumulate errors.\\n17:\\nfor src_bin = 0 to b do\\n18:\\nsrc_bin_begin = (src_bin - start_bin) * bin_width\\n19:\\nsrc_bin_end = src_bin_begin + bin_width\\n20:\\n// Determine which dst_bins the beginning and end of src_bin belong to\\n21:\\ndst_bin_of_begin = min(dst_nbins - 1, max(0, ﬂoor((src_bin_begin + 0.5 * dst_bin_width) /\\ndst_bin_width)))\\n22:\\ndst_bin_of_end = min(dst_nbins - 1, max(0, ﬂoor((src_bin_end + 0.5 * dst_bin_width) /\\ndst_bin_width)))\\n23:\\ndst_bin_of_begin_center = dst_bin_of_begin * dst_bin_width\\n24:\\ndensity = histogram[src_bin] / bin_width\\n25:\\ndelta_begin = src_bin_begin - dst_bin_of_begin_center\\n26:\\nif dst_bin_of_begin == dst_bin_of_end then\\n27:\\ndelta_end = src_bin_end - dst_bin_of_begin_center\\n28:\\nnorm += get_l2_norm(delta_begin, delta_end, density)\\n29:\\nelse\\n30:\\ndelta_end = dst_bin_width / 2\\n31:\\nnorm += get_l2_norm(delta_begin, delta_end, density)\\n32:\\nnorm += (dst_bin_of_end - dst_bin_of_begin - 1) * get_l2_norm( -dst_bin_width / 2,\\ndst_bin_width / 2, density)\\n33:\\ndst_bin_of_end_center = dst_bin_of_end * dst_bin_width\\n34:\\ndelta_begin = -dst_bin_width / 2\\n35:\\ndelta_end = src_bin_end - dst_bin_of_end_center\\n36:\\nnorm += get_l2_norm(delta_begin, delta_end, density)\\n37:\\nif norm < norm_min then\\n38:\\nnorm_min = norm\\n39:\\nbest_start_bin = start_bin\\n40:\\nbest_nbins_selected = nbins_selected\\n41: xmin = xmin + bin_width * best_start_bin\\n42: xmax = xmax + bin_width * (best_start_bin + best_nbins_selected)\\n43: return xmin, xmax\\n9\\n4\\n6\\n8\\n10\\n12\\n14\\nlog(d)\\n−1\\n0\\n1\\n2\\n3\\n4\\n5\\ntime (ms) in log10\\nASYM\\nHIST-APPRX\\nHIST-BRUTE\\nGSS\\nACIQ\\nGREEDY\\nGREEDY(opt)\\nFigure 2: Average time per row spent on 4-bit quantization. Time is shown in log10 scale.\\nFigure 2 shows the average time in milliseconds to quantize a row vector with different dimensions\\nusing 4 bits. To make a fair comparison, we implemented all the quantization algorithms in python.\\nThe results show that HIST-BRUTE is millions of times slower than ASYM. All the other clipping-\\nbased approaches take less than 100ms to quantize a row vector when d is less than 2048. The times\\nare measured on a computer with Ubuntu 16.04, 3.00GHz Intel Xeon CPU E5-1607 processor, and\\n8GB memory.\\nB\\nHistograms After 4-bit Quantization\\nWe show the histograms of a vector after 4-bit quantization using different approaches in Figure 3.\\nThe vector is of dimension 64 and its values are randomly sampled from a Gaussian distribution. The\\nresults echo the observations in Figure 1 that GREEDY and KMEANS have the smallest quantization\\nerror than state-of-the-art quantization approaches (HIST-APPRX, ACIQ, GSS).\\n10\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.1009\\nFP32:[-2.3015, 2.1003]\\nSYM:[-2.3015, 1.9947]\\n(a) SYM\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0909\\nFP32:[-2.3015, 2.1003]\\nGSS:[-1.9702, 1.9702]\\n(b) GSS\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0879\\nFP32:[-2.3015, 2.1003]\\nASYM:[-2.3015, 2.1003]\\n(c) ASYM\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0879\\nFP32:[-2.3015, 2.1003]\\nHIST-APPRX:[-2.3015, 2.1003]\\n(d) HIST-APPRX\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0733\\nFP32:[-2.3015, 2.1003]\\nHIST-BRUTE:[-2.3015, 2.0122]\\n(e) HIST-BRUTE\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\nfrequency\\nnormalized l2 loss: 0.1037\\nFP32:[-2.3015, 2.1003]\\nACIQ:[-2.0835, 2.1214]\\n(f) ACIQ\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0733\\nFP32:[-2.3015, 2.1003]\\nGREEDY:[-2.3015, 2.0122]\\n(g) GREEDY\\n−2\\n−1\\n0\\n1\\n2\\n0\\n2\\n4\\n6\\n8\\n10\\nfrequency\\nnormalized l2 loss: 0.0522\\nFP32:[-2.3015, 2.1003]\\nKMEANS:[-2.1887, 2.1003]\\n(h) KMEANS\\nFigure 3: Histograms of a vector (d=64) before and after 4-bit quantization with different techniques.\\nHIST-APPRX and HIST-BRUTE use b = 200, GREEDY uses b = 200, r = 0.16.\\n11\\n',\n",
       " '2010.11305': 'MIXED-PRECISION EMBEDDING USING A CACHE\\nJie (Amy) Yang * 1 Jianyu Huang * 1 Jongsoo Park 1 Ping Tak Peter Tang 1 Andrew Tulloch 1\\nABSTRACT\\nIn recommendation systems, practitioners observed that increase in the number of embedding tables and their\\nsizes often leads to signiﬁcant improvement in model performances. Given this and the business importance of\\nthese models to major internet companies, embedding tables for personalization tasks have grown to terabyte scale\\nand continue to grow at a signiﬁcant rate. Meanwhile, these large-scale models are often trained with GPUs where\\nhigh-performance memory is a scarce resource, thus motivating numerous work on embedding table compression\\nduring training. We propose a novel change to embedding tables using a cache memory architecture, where\\nthe majority of rows in an embedding is trained in low precision, and the most frequently or recently accessed\\nrows cached and trained in full precision. The proposed architectural change works in conjunction with standard\\nprecision reduction and computer arithmetic techniques such as quantization and stochastic rounding. For an\\nopen source deep learning recommendation model (DLRM) running with Criteo-Kaggle dataset, we achieve\\n3× memory reduction with INT8 precision embedding tables and full-precision cache whose size are 5% of the\\nembedding tables, while maintaining accuracy. For an industrial scale model and dataset, we achieve even higher\\n>7× memory reduction with INT4 precision and cache size 1% of embedding tables, while maintaining accuracy,\\nand 16% end-to-end training speedup by reducing GPU-to-host data transfers.\\n1\\nINTRODUCTION\\nMachine learning and deep learning in particular has been\\ntremendously successful in tackling tasks that were tradition-\\nally considered to require human intelligence or intuition.\\nDeep learning typically accomplishes this feat with massive\\ncomputations in the Euclidean space on matrices of real\\nnumbers. While inputs to many deep learning tasks are\\nnaturally represented as matrices of continuous numerical\\nvalues such as image classiﬁcation and detection, in the\\ncases where input data is discrete in nature such as categori-\\ncal features and vocabularies, embedding is at present the\\nde facto technique that maps a discrete set into the continu-\\nous Euclidean space. In essence, a good embedding maps\\ndiscrete entries into Rd in such a way that preserves natural\\nrelationship via the basic operations in Euclidean space.\\nNaturally, embedding table is an important component of\\nrecommendation systems that are important to internet com-\\npanies (Cheng et al., 2016; Wang et al., 2017; Hazelwood\\net al., 2018; Naumov et al., 2019), which rely heavily on\\ncategorical features to deliver personalized contents based\\non user-item interactions. Such models customarily contain\\nhundreds of embedding tables representing different users,\\n*Equal contribution\\n1Facebook Inc., Menlo Park, California,\\nUSA. Correspondence to: Jie (Amy) Yang <amyyang@fb.com>.\\nProceedings of the 4 th MLSys Conference, Austin, TX, USA,\\n2020. Copyright 2021 by the author(s).\\ncharacteristics of commercial items, news articles’ topics,\\nand are commonly tens of gigabytes in size in commercial\\nML models (Hazelwood et al., 2018; Park et al., 2018).\\nMoreover, industry generally believes that further growing\\nthe embedding table size will improve the models’ predic-\\ntive performance. However, such scaling of embeddings\\nposes a challenge in scaling the training system’s memory\\nand computation capacity accordingly. In GPU training\\nsystems with highly parallel execution but limited high-\\nbandwidth memory capacity, the cost of training can be\\nsigniﬁcantly increased as embeddings trend towards billions\\nscale. Due to industry’s interest in recommendation models,\\naccommodating the intense memory requirement of em-\\nbeddings is essential in scaling high-performance training\\nsystems.\\nFigure 1 captures the structure of a representative Deep\\nLearning Recommendation Model (DLRM) (Naumov et al.,\\n2019) developed for personalization tasks. Dense and sparse\\nfeatures are computed via multi-layer perceptron (MLP)\\nand embedding lookups, then joined in sparse-dense in-\\nteraction and top MLP to compute the ﬁnal click-through\\nrate prediction. Large embeddings are partitioned across\\nmultiple GPUs following model-parallelism with no repli-\\ncation across device. This paper focuses on training large\\nembeddings with reduced memory footprint while maintain-\\ning accuracy. Training acceleration via dense computation\\noptimization is not the main subject of this study.\\narXiv:2010.11305v2  [cs.LG]  23 Oct 2020\\nSubmission and Formatting Instructions for MLSys 2020\\nFigure 1. Deep Learning Recommendation Model (DLRM) (Nau-\\nmov et al., 2019)\\nIn this work, we propose to train embeddings in low pre-\\ncision with the addition of a small full-precision cache for\\nthe most recently or frequently accessed rows. Depend-\\ning on the speciﬁc cache replacement policy in play, some\\nembedding rows may behave as if they were trained in full\\nprecision most or even all of the time. Previous work (Zhang\\net al., 2018) shows that we can train embeddings in FP16\\nwith stochastic rounding while maintaining neutral accuracy.\\nWe show that with a small high-precision cache, we can\\ntrain embeddings with INT8 and INT4 precision, further\\nnarrowing the bitwidths and thus the memory consumption\\nat little to no cost of accuracy. While caching is commonly\\nadopted in GPU training systems to utilize CPU host mem-\\nory (Zhao et al., 2020), we reuse the same architecture with\\nmixed-precision to achieve the purpose of reduced memory\\nfootprint for embeddings and maintain a neutral training\\naccuracy.\\nSpeciﬁcally, this paper makes the following contributions:\\n• We propose mixed-precision embedding training with\\na full-precision cache, and show its effectiveness and\\ngenerality in reducing embeddings memory (3× mem-\\nory reduction on an open source DLRM model and 7×\\nmemory reduction on an industrial scale model) usage\\nduring training and maintaining model accuracy.\\n• We compare different cache replacement policies: least\\nrecently used (LRU), least frequently used (LFU), and\\nthe effect of cache associativity on model accuracy and\\ncache hit rate.\\n• We explore the effect of varying cache sizes on model\\naccuracy with embeddings trained in low precision.\\nWhile model accuracy improves with large cache sizes,\\nthere is diminishing return in accuracy recovery with\\nincreasingly large cache size.\\n• We study the effect of rounding modes on model accu-\\nracy with and without cache, and show that stochastic\\nrounding consistently outperforms round-to-nearest in\\nlow-precision embedding training by getting rid of sys-\\ntematic rounding bias.\\n• We achieve 16% end-to-end training speed improve-\\nment on an industrial scale recommendation system\\nusing our optimized full-precision cache implementa-\\ntion. With 95% cache hit rate, the embedding lookup\\nthroughput is improved by 25.8× over that of uniﬁed\\nmemory.\\nThe rest of paper is organized as follows. Section 2 reviews\\nprior work on embedding table compression techniques.\\nSection 3 explains our approach and different cache replace-\\nment policies. Section 4 discusses the experiments and\\nresults with CPU emulation and open source DLRM model.\\nThe results support our focus on an associative cache GPU\\nimplementation described in Section 5. The GPU imple-\\nmentation enables us to experiment with large-scale data\\nset that would otherwise be prohibitive on CPU. Section 6\\nrecaps our result and discusses future work.\\n2\\nRELATED WORKS\\nA sample of works showing embedding in action are\\n(Mikolov et al., 2013; Pennington et al., 2014; Liu et al.,\\n2015; Peters et al., 2018; Liu et al., 2019) for natural lan-\\nguage processing, (Vasileva et al., 2018; Barz & Denzler,\\n2019) for computer vision, and other areas such as knowl-\\nedge representation and biology (Wu et al., 2018a; Chiu\\net al., 2016).\\nWe follow the conceptual classiﬁcation termed in (Ginart\\net al., 2019) of algorithmic and architectural compression of\\nembedding tables. Some compression algorithms apply post\\nprocessing methods on the embedding tables such as low-\\nrank SVD and other factorizations (Bhavana et al., 2019;\\nAndrews, 2015), sparsiﬁcation (Sattigeri & Thiagarajan;\\nSun et al., 2016) and quantization (Guan et al., 2019). Other\\ncompression algorithms aim at arriving at easily compress-\\nible tables through training. These include quantization or\\ncompression-aware training (Alvarez & Salzmann, 2017;\\nPark et al., 2018; Naumov et al., 2018; Elthakeb et al., 2019)\\nand gradual pruning (Frankle & Carbin, 2018). These tech-\\nniques reduce the memory requirement at inference time\\nbut use the uncompressed embedding tables during training.\\nAlternatively, compressed architectures use alternative repre-\\nsentation in the ML model architecture of the embedding ta-\\nbles other than the standard 2D-array of FP32 ﬂoating-point\\nnumbers. Hence memory saving is realized even during\\ntraining. Along this line, representing the tables in low-\\nprecision data type is a natural strategy. Works along this\\nline mainly need to maintain the models’ accuracy despite\\nthe models’ parameters have reduced precision. Notable\\nSubmission and Formatting Instructions for MLSys 2020\\nworks using this approach include (Gupta et al., 2015; Chen\\net al., 2017; De Sa et al., 2018; Wu et al., 2018b; Zhang\\net al., 2018; Kalamkar et al., 2019). We note that some of\\nthe just referenced works try to reduce the memory footprint\\nof weight parameters so as to improve compute performance\\nand not that of embedding table size. Some other works\\nexplore more structural changes to the embedding table\\nsuch as tensor train representation (Khrulkov et al., 2019),\\nmixed-dimension factorization (Ginart et al., 2019) or vector\\nquantization (Fan et al., 2020).\\nInstead of using algebraic factorization, we use memory hi-\\nerarchy and varying precision to enable compression. Like\\nthe other compression architecture work, the compression\\nbeneﬁts training as well as inference. The sequel of this\\npaper explains our design and reports the results on exten-\\nsive experiments with our CPU and GPU implementations\\nrunning different models on multiple data sets with different\\ncache size/precision/policy combinations.\\n3\\nMIXED-PRECISION EMBEDDING TABLE\\nWITH A CACHE\\nA common embedding table T consists of N rows of d-\\ndimensional vectors of FP32 numbers. Each row corre-\\nsponds to some categorical features, for example a particular\\nuser, and any production grade ML model such as a recom-\\nmendation system can consist of hundreds of such tables.\\nExpanding the capacity of embedding tables – by increasing\\nthe total number of tables or dimensions of individual ones –\\ncan increase model performance, motivating active research\\nin embedding table compression that does not sacriﬁce the\\ntables’ representation capacities.\\nTo explain our compression method of mixed-precision with\\na cache, it sufﬁces to consider the workﬂow involving a\\nsingle row of one table T during a training iteration. During\\na forward pass, a particular index-i row x of T is needed\\nand obtained by a fetch operator x ←fetch(T, i). This\\nrow will participate in the model evaluation as input to\\nsome layers and eventually contribute to the loss function\\nL. During the corresponding backward pass, the gradient\\nof L with respect to x, ∂L\\n∂x is computed and x updated via\\nx ←x −η ∂L\\n∂x for example with a simple SGD algorithm\\nwith learning rate η. This update to the embedding table is\\ndenoted as update(T, i, x).\\nOur method has two main components. The ﬁrst is the use of\\na reduced-precision embedding table similar to (Zhang et al.,\\n2018). Table T is in a precision lower than FP32. During\\na forward pass, the fetch operator x ←fetch(T, i) up-\\nconverts the row in its low-precision representation to FP32.\\nComputations are carried out in FP32 including the com-\\nputation x ←x −η ∂L\\n∂x . The update step update(T, i, x)\\nuses either a round-to-nearest or stochastic rounding (Zhang\\net al., 2018) approach to down convert x into the precision\\nof T’s entries. (Zhang et al., 2018) applies this approach to\\nan FP16 embedding table T and uses stochastic rounding to\\nstore the updated row back into T.\\nOur second component augments the low-precision embed-\\nding table T with a cache C of FP32 entries. C only has n\\nrows, which is a fraction of T’s row dimension N. Similar\\nto cache memory architecture, an embedding row may be\\nresiding both in the cache C and the table T, albeit with\\ndifferent precision. We use and preserve the high precision\\nversion whenever possible.\\nCombining these two components, the method is encap-\\nsulated by the fetch and update operators as x ←\\nfetch(T, C, i) and update(T, C, i, x).\\nThe operator\\nfetch returns the higher-precision version if the row i re-\\nsides in cache, or return the up converted result of the lower-\\nprecision version in T. In updating the table and cache with\\nthe updated row x, the update operator checks that if this\\nrow is cache resident, simply replaces the row in cache with\\nthe updated x. Otherwise, one of two scenarios happen that\\nis determined by the cache policy in effect. Either x does\\nnot have priority to evict the current cache content whose\\nspace it conﬂicts with, or that it does. In the former, x is\\nplaced in the cache verbatim while the evicted row is placed\\nin the table T after being down converted. In the latter, one\\nsimply down converts x and places it in T.\\nOur design allows four knobs as follows.\\n• Precision: The embedding table T contains entries in\\nprecision lower than FP32. We allow the IEEE 16-bit\\nﬂoating-point values FP16 as well as quantization into\\nINT8, INT4, as well as INT2 precision. For integer\\nquantization, each row of the embedding values share\\none pair of quantization parameters (s, b), scale and\\nbias, represented in FP32. We use row-wise uniform\\nmin-max quantization in mapping real values to un-\\nsigned quantization domain:\\nxquantize = x −b\\ns\\nwhere b=min(x) and s=(max(x)-min(x))/(2N −1). The\\nquantization parameters are chosen so that the quan-\\ntized embedding values will be in the corresponding\\nrange of the unsigned integer datatype: [0, 3], [0, 15]\\nand [0, 255] for INT2, INT4 and INT8 quantization,\\nrespectively.\\n• Rounding: When a FP32 value has to be placed\\ninto a low-precision embedding table, a precision\\ndown conversion occurs. Let x be the FP32 value\\nin question. There is a pair of neighboring numbers\\n(x−, x+) in the quantized low-precision domain such\\nthat x−≤x ≤x+ (that is, they differ by one “unit in\\nSubmission and Formatting Instructions for MLSys 2020\\nlast place” in FP16, or are consecutive integers in the\\ncase of quantization). We allow two rounding options:\\nround to nearest and stochastic rounding. If round to\\nnearest is used, then the number in {x−, x+} that is\\nclosest to x is returned. In case of a tie, the one with\\neven parity (least signiﬁcant bit equals 0) is chosen. If\\nstochastic rounding is used, then one of x−and x+ is\\nchosen randomly with a Bernoulli distribution so that\\nthe expected value (average) is in fact x.\\n• Cache Structure: Let an uniform-dimension embed-\\nding table T has N rows with indices from 0 to N −1,\\nand each row has dimension [1, d]. The cache C has n\\nsets each of which can hold α FP32 embedding rows.\\nThe cache size is thus 4αnd bytes. We restrict our-\\nselves to α ≥1 being an integral power of 2 and cache\\nsizes chosen such that α, n are integers. A hash func-\\ntion\\nh : {0, 1, . . . , N −1} →{0, 1, . . . , n −1}\\nis chosen and row i of T is mapped to the set h(i) of\\nthe cache. When α = 1, we have a direct mapped\\ncache; otherwise, we have a set associative cache.\\n• Cache Replacement Policy: When we attempt to\\nplace a non-cache resident row of index i into the cache\\nand the set h(i) is already fully occupied, the replace-\\nment policy dictates the appropriate action. We main-\\ntain a priority value of each row which can be either\\naccess frequency or timestamp of last access, depend-\\ning on the replacement policy. We also keep track of\\nthe lowest priority value of the current cache residents\\nfor each set s, 0 ≤s < n. When the FP32 Row-i is to\\nbe stored, our policy puts the row in the full-precision\\ncache if and only if its priority value is higher than the\\nlowest priority value of the residents. In this case, the\\nlowest priority row in the cache will be evicted – down\\nconverted by a rounding method of choice and placed\\nback into the table T. Otherwise, Row-i bypasses the\\ncache, and after it is updated in FP32, is put into T\\nwith down conversion. We have two speciﬁc policies\\nspeciﬁed by their respective priority value calculations.\\n1. Least Frequently Used (LFU). Each embedding\\nrow carries with it an access count as the priority\\nvalue. Access count is a popularity measure; LFU\\npolicy naturally let the more popular rows main-\\ntain higher precision based on the assumption that\\nthe more frequently used rows pull heavier weight\\non model accuracy. One drawback of this policy\\nis that access count requires extra memory per\\nrow, whose size grows linearly with the number\\nof rows in embeddings.\\n2. Least Recently Used (LRU). Each cached row\\ncarries the last access timestamp as the priority\\nvalue. This policy lets those rows that are of-\\nten accessed within a time window to maintain\\nhigher precision in that duration. Another advan-\\ntage over LFU is that this priority value needs\\nnot be explicitly maintained at all in the case of a\\ndirect mapped cache, as the cache resident entry\\nis always evicted in case of a conﬂict.\\nWe will report on our experiments and results in the next\\nsections.\\n4\\nCPU EXPERIMENTS WITH DLRM AND\\nKAGGLE DATASET\\nWe use a CPU emulation to explore our algorithm design\\nspace with an open source model and dataset. Section 5\\nwill discuss an implementation in a GPU training system\\nand performance evaluation with an industrial-scale internal\\nmodel and dataset. The main emulation aspect here is that\\nwe do not keep two physical spaces for the embedding table\\nand the cache, but rather use a single FP32 array to emulate\\na single low-precision table with FP32 cache. We use the\\nsimple technique of fake quantization, detailed below, to\\nmaintain faithful behavior of low-precision numerics and\\nexperimented with all combinations of the four precisions\\nand two rounding methods (discussed in section 3) on each\\nof the following cache settings: direct-mapped cache with\\nLFU and LRU policies and set associative LFU cache with\\nassociativity α = 2k, 1 ≤k ≤5.\\n4.1\\nLow-Precision Emulation\\nWe implemented a custom sparse AdaGrad optimizer that\\nuses one high-precision tensor to store the embedding\\nweights, and keep a list of row indices of the current cache\\nresidents. All embedding gradient updates are applied in\\nfull precision. When Row-i in table T is supposed to be\\nstored in low precision, we apply fake quantization (quanti-\\nzation followed by dequantization) to row T[i, :] to provide\\na numerically faithful emulation: In the case of FP16, fake\\nquantization of a FP32 value x is simply the operation\\nFP16 to FP32 (FP32 to FP16(x)) .\\nWith the above conversion, while the result physically re-\\nmains in FP32, its precision is that of FP16. Fake inte-\\nger quantization of an FP32 value x similarly returns an\\nFP32 object but whose precision is the same as if integer\\nquantization was performed. Algorithm 1 shows the fake\\nquantization details.\\n4.2\\nCache Implementations\\nNumerical faithfulness is paramount in our study; thus we\\nshow the exact numerical steps on our single-array emula-\\ntion of an embedding table and its cache. Here we have a\\nSubmission and Formatting Instructions for MLSys 2020\\nAlgorithm 1 Fake Row-wise Integer IntN Quantization\\nParameters: Uniform Min-Max\\nInput:\\nembedding row r, embedding dimension d,\\nbitwidth N\\nCompute b ←min(r), s ←(max(r) −b)/(2N −1)\\nfor i = 0, 1, ..., d −1 do\\nq ←round to int((r[i]−b)/s) (nearest or stochastic)\\nr[i] ←q × s + b\\nend for\\nsingle table T where some rows correspond to cache resi-\\ndents in FP32 precision and the others non-cache residents\\nin the corresponding low precision.\\nAt any given iteration, let IU be the set of embedding row\\nindices to be updated by a scaled gradient, U. Let PV [k]\\nbe the priority value for the embedding row k, either for\\nLRU or LFU. Note that in the case of α = 1 (direct-mapped\\ncache) for LRU, PV is not required as all accessed rows will\\nbe the most recent by default and no priority comparison is\\nnecessary. Tag indices IC are maintained to keep the row\\nids of cache residents for each of the α rows residing in the\\nset h(i), with h being the hash function. Algorithm 2 gives\\nthe numerical details of our cache implementation.\\nAlgorithm 2 Update α-Associative Cache\\nInput: embedding T, tag indices IC of current cached\\nrows, indices to update IU, hash function h, gradient\\nupdate U, priority value PV (non existent for α = 1,\\ndirect-mapped LRU)\\nfor i in IU do\\nUpdate PV [i] according to LRU or LFU\\ns ←h(i)\\nT[i, :] ←T[i, :] + U[i, :] # apply gradient updates\\nif i /∈IC[s] then\\nj ←argmin{PV [k]|k ∈IC[s]}\\nif PV [i] ≤PV [j] (False if α = 1 and LRU) then\\n# bypass\\nT[i, :] ←fake quantize(T[i, :])\\nelse\\n# replace\\nT[j, :] ←fake quantize(T[j, :])\\nreplace j by i in the set IC[s]\\nend if\\nend if\\nend for\\n4.3\\nExperiments and Results\\nWe\\nuse\\nDeep\\nLearning\\nRecommendation\\nModel\\n(DLRM) (Naumov et al., 2019) with Criteo-Kaggle\\n7D Ads Display Challenge dataset1 (Criteo AI Lab, 2014)\\nfor model accuracy evaluations with different low-precision\\nand cache implementations.\\nThe accuracy metric is\\nbased on the relative change to the test accuracy of the\\nresulting low-precision models using the full FP32 model\\nas reference. The precise deﬁnition of our metric is\\nAccuracy Drop % = AccFP32 −Acclowprec\\nAccFP32\\n· 100\\n(1)\\nIn particular, if the low-precision model in fact has higher\\ntest accuracy, our metric becomes negative. Our goal is to\\nkeep this number below roughly 0.02%.\\nOur benchmark model has 26 embedding tables with the\\nfollowing sizes shown in Table 1. We conﬁgure all 26\\nembedding tables to use row dimension 128 (total 4.3B\\nparameters), and only apply low-precision with or without\\ncaching technique to tables with more than 1K rows for all\\nexperiments. Embedding tables with less than 1K rows are\\ntrained in FP32.\\nTable 1. DLRM benchmark model embedding sizes\\n4\\n4\\n11\\n16\\n18\\n24\\n28\\n105\\n306\\n584\\n634\\n1,461\\n2,173\\n3,195\\n5,653\\n5,684\\n12,518\\n14,993\\n93,146\\n142,572\\n286,181\\n2,202,608\\n5,461,306\\n7,046,547\\n8,351,593\\n10,131,227\\n4.3.1\\nAccuracy Recovery with High-precision Cache\\nWe ﬁrst evaluate accuracy loss of training with low-precision\\nembeddings without a high-precision cache. Table 2 shows\\nthe relative test accuracy drop of training embeddings in\\nvarious low precisions with nearest and stochastic rounding.\\nWe observe that neutral accuracy can be achieved with FP16\\nwith stochastic rounding without high-precision caching;\\nbut accuracy decreases signiﬁcantly with progressively nar-\\nrower bitwidth. The accuracy boost from stochastic round-\\ning is consistent with previous work on low-precision em-\\nbedding training (Zhang et al., 2018), but not nearly enough\\nfor embedding tables represented by 8 or fewer bits. This\\nloss of accuracy was part of the motivation for our high-\\nprecision cache.\\nTable 2. Test Accuracy Drop without Cache in %\\nFP16\\nINT8\\nINT4\\nINT2\\nNEAREST\\n0.047\\n0.549\\n1.080\\n1.454\\nSTOCHASTIC\\n-0.010\\n0.077\\n0.591\\n1.037\\n1http://labs.criteo.com/2014/02/kaggle-display-advertising-\\nchallenge-dataset/\\nSubmission and Formatting Instructions for MLSys 2020\\nWe experiment with direct-mapped high-precision LFU,\\nLRU cache, and set associative LFU cache with varying\\nsizes: 5%, 10%, 30%, and 50% (of the original FP32 table)\\non top of low-precision tables with nearest/stochastic round-\\ning. 32-Way LFU gives the best accuracy results and as\\nshown in Table 3: we achieve neutral accuracy with INT8,\\nINT4, and INT2 embeddings (with stochastic rounding) at\\ncache sizes of 5%, 30%, and 50%, respectively.\\nTable 3. High-precision 32-Way LFU cache with varying cache\\nsizes recover accuracy of low-precision embedding tables\\nCACHE SIZE\\n% OF\\nTEST ACCURACY DROP IN %\\nROUNDING: NEAREST/STOCHASTIC\\nTABLE\\nINT8\\nINT4\\nINT2\\n5%\\n0.074/-0.014\\n0.251/0.110\\n0.376/0.275\\n10%\\n0.033/-0.013\\n0.157/0.073\\n0.257/0.196\\n30%\\n0.000/-0.021\\n0.043/-0.009\\n0.086/0.077\\n50%\\n-0.005/-0.008\\n0.018/-0.004\\n0.042/0.025\\nTables 3, 4, and 5 show that the presence of a high-precision\\ncache with as low as 5% capacity of the original table size,\\nregardless of cache replacement policy, recovers accuracy\\nsigniﬁcantly for various lower precision levels with either\\nnearest or stochastic rounding. With 5% high-precision\\ncache, we recover 70-80% of accuracy drop from low-\\nprecision embeddings for 8 bits and lower.\\nTable 4. High-precision LRU cache with varying cache sizes re-\\ncover accuracy of low-precision embedding tables\\nCACHE\\nSIZE\\n% OF\\nTEST ACCURACY DROP IN %\\nCACHE: LRU, DIRECT MAPPED\\nROUNDING: NEAREST/STOCHASTIC\\nTABLE\\nFP16\\nINT8\\nINT4\\nINT2\\n5%\\n0.005/-0.005\\n0.180/0.010\\n0.520/0.259\\n0.602/0.872\\n10%\\n0.003/0.004\\n0.144/0.008\\n0.430/0.212\\n0.761/0.536\\n30%\\n0.004/-0.008\\n0.093/0.006\\n0.321/0.164\\n0.582/0.425\\n50%\\n0.006/-0.016\\n0.043/-0.004\\n0.246/0.143\\n0.462/0.352\\nTable 5. High-precision LFU cache with varying cache sizes re-\\ncover accuracy of low-precision embedding tables\\nCACHE\\nSIZE\\n% OF\\nTEST ACCURACY DROP IN %\\nCACHE: LFU, DIRECT MAPPED\\nROUNDING: NEAREST/STOCHASTIC\\nTABLE\\nFP16\\nINT8\\nINT4\\nINT2\\n5%\\n-0.006/-0.001\\n0.087/0.014\\n0.333/0.147\\n0.533/0.423\\n10%\\n-0.011/-0.003\\n0.063/0.008\\n0.270/0.138\\n0.430/0.375\\n30%\\n-0.004/-0.003\\n0.044/0.009\\n0.175/0.090\\n0.280/0.243\\n50%\\n-0.018/-0.008\\n0.014/0.008\\n0.136/0.056\\n0.217/0.193\\nAlthough larger cache sizes recover more accuracy in Tables\\n4 and 5, we observe a diminishing return of value in their\\ncontinued increase. Plotting in Figure 2 the 16 columns of\\ndata in Tables 4 and 5 demonstrate the trend.\\n4.3.2\\nReplacement policy\\nThe previous section shows that the presence of a high-\\nprecision cache recovers signiﬁcantly the accuracy lost due\\nCache Size\\nAccuracy Drop\\n-0.5\\n0.0\\n0.5\\n1.0\\n1.5\\n0%\\n10%\\n20%\\n30%\\n40%\\nFP16\\nINT8\\nINT4\\nINT2\\nCache Size\\nAccuracy Drop\\n-0.2\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n0%\\n10%\\n20%\\n30%\\n40%\\nFP16\\nINT8\\nINT4\\nINT2\\n(a) LRU nearest rounding\\n(b) LRU stochastic rounding\\nCache Size\\nAccuracy Drop\\n-0.\\n0.0\\n0.5\\n1.0\\n1.5\\n0%\\n10%\\n20%\\n30%\\n40%\\nFP16\\nINT8\\nINT4\\nINT2\\nCache Size\\nAccuracy Drop\\n-0.2\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n0%\\n10%\\n20%\\n30%\\n40%\\nFP16\\nINT8\\nINT4\\nINT2\\n(c) LFU nearest rounding\\n(d) LFU stochastic rounding\\nFigure 2. Diminishing return of accuracy recovered vs. cache sizes\\nFigure 3. CDF of sorted row access counts vs. fraction of rows\\nto embeddings trained exclusively in low precisions. When\\nan FP32 cache is present, rows updated while in cache are\\ncomputed in full FP32 precision. Thus any cache policy that\\npromotes more updates to cache residents improve accuracy.\\nThis motivates us to examine the comparative number of\\nupdates each row receives during training.\\nWe collected access counts for each row, and plotted cu-\\nmulative distribution of row counters sorted in descending\\norder. Figure 3 shows four of the representative distribution\\nplots out of 15 embeddings with more than 1K rows. For\\nCriteo-Kaggle dataset, all large embedding tables share the\\nproperty that a small fraction (< 20%) of rows are respon-\\nsible for a large fraction (> 80%) of total accesses; some\\nembedding tables show an even greater skew.\\nWe now consider the relative merits of different cache re-\\nSubmission and Formatting Instructions for MLSys 2020\\nTable 6. Cache hit rates for varying replacement policies and sizes\\n(relative to embedding tables)\\nCACHE SIZE\\nLRU\\nLFU\\n32-WAY LFU\\n5%\\n68.22%\\n75.65%\\n77.66%\\n10%\\n75.18%\\n81.21%\\n83.63%\\n30%\\n85.66%\\n89.07%\\n92.45%\\n50%\\n90.12%\\n92.43%\\n96.13%\\nplacement policies in light of the inequalities in access fre-\\nquency. If the access pattern that produces these access\\ncount is non-stationary (Agarwal et al., 1989) and highly\\nphased, a direct-mapped LRU cache can be effective in pro-\\nmoting cache hit rate. An advantage is that direct-mapped\\nLRU is the easiest to implement among all the choices in\\nAlgorithm 2. On the other hand, if the access pattern is sta-\\ntionary, then the relative priority rankings among the rows\\nbased on access counts will more or less stabilize after some\\ninitial stages of training. Consequently, an LFU policy in\\ngeneral promotes the highly accessed rows be cache resi-\\ndent, increasing the number of cache hit. Along this line,\\nan associative cache, with LFU policy in our case, is well\\nknown to improve cache hit rate further.\\nTable 6 reports the cache hit rate statistics of various cache\\nconﬁgurations. 32-way LFU provides noticeable increase in\\nhit rate than direct-mapped LFU, while direct-mapped LFU\\nhas higher hit rates than direct-mapped LRU. These statistics\\nare consistent with the hypothesis that the current model’s\\nembedding access pattern is somewhat stationary on the\\ndata set in question, making LFU and in particular with set-\\nassociative highly effective. The resulting model accuracy is\\nwell correlated positively with hit rate, as shown in Figures\\n4 and 5. Figure 4 shows that LFU outperforms LRU when\\nboth are direct mapped; Figure 5 shows the superiority of\\nset-associative LFU. That said, Figure 6 shows that accuracy\\ngain from increased set associativity plateaus after a while,\\nsuccumbing to the law of diminishing return.\\n4.3.3\\nResults Summary\\nWe summarize our CPU experiments with DLRM and\\nCriteo-Kaggle dataset.\\nMost importantly, the use of a\\nlow-precision embedding table in conjunction with a high-\\nprecision cache is effective for maintaining accuracy as we\\nhave demonstrated. Stochastic rounding helps maintain\\ntraining accuracy on low-precision embedding tables. We\\nalso see that set associative cache architecture is prefer-\\nable to direct mapped, and that the LFU replacement policy\\nserves this model/dataset better than LRU.\\nConcentrating then on a 32-way associative LFU cache with\\nstochastic rounding, Figure 7 plots the best accuracy ob-\\ntained at each memory compression factor with or without\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.4\\n0.6\\n5%\\n10%\\n30%\\n50%\\nDM LFU\\nDM LRU\\nCache Size\\nAccuracy Drop\\n0.0\\n0.1\\n0.2\\n0.3\\n5%\\n10%\\n30%\\n50%\\nDM LFU\\nDM LRU\\n(a) INT4 nearest rounding\\n(b) INT4 stochastic rounding\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.5\\n0.7\\n1.0\\n5%\\n10%\\n30%\\n50%\\nDM LFU\\nDM LRU\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n5%\\n10%\\n30%\\n50%\\nDM LFU\\nDM LRU\\n(c) INT2 nearest rounding\\n(d) INT2 stochastic rounding\\nFigure 4. Direct-mapped LFU vs. LRU\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.4\\n0.6\\n5%\\n10%\\n30%\\n50%\\n32-way LFU\\nLFU\\nLRU\\nCache Size\\nAccuracy Drop\\n-0.1\\n0.0\\n0.1\\n0.2\\n0.3\\n5%\\n10%\\n30%\\n50%\\n32-way LFU\\nLFU\\nLRU\\n(a) INT4 nearest rounding\\n(b) INT4 stochastic rounding\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.5\\n0.7\\n1.0\\n5%\\n10%\\n30%\\n50%\\n32-way LFU\\nLFU\\nLRU\\nCache Size\\nAccuracy Drop\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n5%\\n10%\\n30%\\n50%\\n32-way LFU\\nLFU\\nLRU\\n(c) INT2 nearest rounding\\n(d) INT2 stochastic rounding\\nFigure 5. 32-Way Associative LFU vs. Direct-mapped LFU vs.\\nLRU\\na high-precision cache. Memory compression factor with\\nnon-cached low-precision embedding is calculated from\\nbitwidths, taking into quantization parameter storage over-\\nhead, i.e., compression for FP16 and INT8 is 0.5 and 0.265,\\nrespectively. Compression factor with cached implemen-\\ntation is calculated from bitwidths and cache sizes, taking\\ninto account quantization parameter storage overhead and\\ncaching overhead such as access counters and tags: i.e., for\\nINT8 embedding with 10% high-precision LFU cache, its\\nmemory compression factor is 0.374, or a 2.7× memory\\ncompression. Please refer to Appendix A for more details\\nfor computing the memory compression factor.\\nModel accuracy degrades with increasingly large compres-\\nsion factors introduced by low-precision embeddings regard-\\nless of the presence of a high-precision cache. However with\\nSubmission and Formatting Instructions for MLSys 2020\\nAssociativity\\nAccuracy Drop\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n5\\n10\\n15\\n20\\n25\\n30\\nINT4 Nearest\\nINT4 Stochastic\\nINT2 Nearest\\nINT2 Stochastic\\nAssociativity\\nAccuracy Drop\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n5\\n10\\n15\\n20\\n25\\n30\\nINT4 Nearest\\nINT4 Stochastic\\nINT2 Nearest\\nINT2 Stochastic\\n(a) 5% cache\\n(b) 10% cache\\nAssociativity\\nAccuracy Drop\\n-0.0\\n0.00\\n0.05\\n0.10\\n0.15\\n0.20\\n5\\n10\\n15\\n20\\n25\\n30\\nINT4 Nearest\\nINT4 Stochastic\\nINT2 Nearest\\nINT2 Stochastic\\nAssociativity\\nAccuracy Drop\\n-0.0\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n5\\n10\\n15\\n20\\n25\\n30\\nINT4 Nearest\\nINT4 Stochastic\\nINT2 Nearest\\nINT2 Stochastic\\n(c) 30% cache\\n(d) 50% cache\\nFigure 6. Diminishing return with associativity for set associative\\nLFU cache\\nMemory Compression\\nAccuracy Drop\\n-0.02\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.30\\n0.35\\n0.40\\n0.45\\n0.50\\nw/ Cache\\nw/o Cache\\nFigure 7. Accuracy vs. memory compression factor with and with-\\nout high-precision cache on Criteo-Kaggle dataset. Please refer to\\nFigure 10 in Appendix A for the complete ﬁgure.\\nthe addition of a high-precision cache, we effectively enable\\nﬁne-grained tradeoffs between accuracy and memory foot-\\nprint compression via varying cache sizes and precisions. A\\nsweet spot here is a 3× memory compression (INT8 and 5%\\ncache) while maintaining neutral accuracy. For clarity to\\ndemonstrate the accuracy improvement near 0.02% that we\\ntypically consider neutral, Figure 7 plots parts of our results,\\nalthough the trend demonstrated holds true for lower preci-\\nsion than plotted in the ﬁgure. Please refer to Appendix A\\nfor memory compression ﬁgure with complete results that\\nshow a general trend extending into lower precision.\\n5\\nGPU EXPERIMENTS WITH AN\\nINDUSTRY-SCALE MODEL AND DATASET\\nA GPU implementation enables us to validate our approach\\non larger datasets. We match the features of the CPU ex-\\nperimentation: supporting real FP16 and emulations of\\nINT8/4/2 with rounding-to-nearest and stochastic round-\\ning modes.\\n5.1\\nImplementations\\nWe made the following changes to fully utilize the GPU\\narchitecture.\\n• 32-way only: We implemented 32-way set-associative\\ncache for: 1) our CPU results showed accuracy im-\\nprovements over direct-mapped cache and 2) it matches\\nwell with the GPU’s warp size of 32, hence achieving\\ntraining speed similar to that of the direct-mapped case.\\n• De-duplicate gradients: Unlike the Kaggle dataset\\nused for CPU evaluation with one-hot embedding ac-\\ncesses, the large dataset used for GPU evaluation has\\nmulti-hot embedding accesses. Therefore, there is a\\nhigher chance that multiple examples in a mini-batch\\naccess the same rows during sparse optimizer, leading\\nto concurrent update races especially in GPUs with\\nhighly parallel execution that typically requires a large\\nbatch size. We avoid such races by sorting row indices\\nin each mini-batch and merging gradient updates for\\nthe same rows into one update. Suppose one input in\\nthe mini-batch accesses rows 1 and 2 with gradient g1,\\nand another input accesses rows 2 and 3 with gradi-\\nent g2. We sort by the rows, de-duplicate gradients\\n(e.g., g1 + g2 for row 2), and then apply the dedup’ed\\ngradients.\\n5.2\\nExperiments and Results\\nTo validate the feasibility of a high-precision cache on a\\nlarger model with larger dataset, we evaluate an industrial\\nscale DLRM-like recommendation model for one ranking\\napplication at [institution name removed for double-blind re-\\nview] on an internal training dataset. The size of the model\\nand dataset are more than 10× bigger than the DLRM model\\nand Kaggle dataset evaluated in the previous section, respec-\\ntively. Model accuracy is measured by Normalized-Entropy\\n(NE) (He et al., 2014) metric. Lower NE corresponds to\\nhigher prediction accuracy for a recommendation model.\\nSimilar to the DLRM evaluation on CPUs, we adopt rela-\\ntive accuracy drop in Equation 1, with the goal to keep the\\nthreshold below 0.02%.\\n5.2.1\\nAccuracy evaluation\\nTable 7 shows the accuracy drop without cache. Similar\\nto the CPU experiments, FP16 can achieve neutral accu-\\nracy. However, accuracy drops more steeply with narrower\\nbit-width of INT8/4/2, and doesn’t even converge without\\nstochastic rounding.\\nThen, we apply the 32-way high-precision LFU and LRU\\nSubmission and Formatting Instructions for MLSys 2020\\nTable 7. Accuracy Drop without Cache in %. Note that ”N/A”\\ndenotes the training cannot converge, resulting not a number (NaN)\\naccuracy.\\nFP16\\nINT8\\nINT4\\nINT2\\nNEAREST\\n0.001\\nN/A\\nN/A\\nN/A\\nSTOCHASTIC\\n-0.023\\n4.254\\n5.200\\n4.673\\nTable 8. High-precision LRU cache with varying cache sizes re-\\ncover accuracy of low-precision embedding tables.\\nCACHE\\nSIZE\\n% OF\\nACCURACY DROP IN %\\nCACHE: 32-WAY LRU ON GPUS\\nROUNDING: NEAREST/STOCHASTIC\\nTABLE\\nFP16\\nINT8\\nINT4\\nINT2\\n0.1%\\n-0.010/-0.024\\n0.006/0.022\\n0.053/0.010\\n0.329/0.135\\n1%\\n0.004/-0.027\\n-0.007/0.004\\n0.002/-0.001\\n0.134/0.095\\n10%\\n-0.001/-0.030\\n-0.011/0.014\\n-0.011/-0.021\\n0.024/0.028\\nTable 9. High-precision LFU cache with varying cache sizes re-\\ncover accuracy of low-precision embedding tables\\nCACHE\\nSIZE\\n% OF\\nACCURACY DROP IN %\\nCACHE: 32-WAY LFU ON GPUS\\nROUNDING: NEAREST/STOCHASTIC\\nTABLE\\nFP16\\nINT8\\nINT4\\nINT2\\n0.1%\\n0.002/-0.029\\n0.041/0.129\\n0.112/0.133\\n0.526/0.278\\n1%\\n0.011/-0.022\\n0.038/0.081\\n0.065/0.081\\n0.389/0.189\\n10%\\n0.003/-0.017\\n-0.003/0.011\\n0.002/0.009\\n0.057/0.042\\ncache with different sizes: 0.1%, 1%, and 10% of the origi-\\nnal table size, for large embedding tables that collectively\\naccount for more than 97% of the total model size. Tables 8\\nand 9 demonstrate that on the large training dataset, both\\nLFU and LRU can achieve better accuracy for various pre-\\ncisions with nearest/stochastic rounding. Unlike the results\\nwith DLRM and Kaggle dataset, LRU performs generally\\nbetter than LFU. This will be explained in the next section.\\n5.2.2\\nCache hit rate analysis\\nTo understand the data reuse pattern of the large training\\ndataset, we collected the cache hit rate statistics for different\\ncache sizes and replacement policies (Table 10). Contrary to\\nthe Criteo-Kaggle 7D Ads display challenge dataset, when\\nTable 10. Cache hit rates for varying replacement policies and sizes\\n(relative to embedding tables). Caching is applied to two sets of\\ntables. The ﬁrst set is 5% of embedding tables accounting for\\n97.1% of total model size.\\nCACHE\\n5% OF TABLES\\n55% OF TABLES\\nSIZE\\n97.1% OF SIZE\\n99.9% OF SIZE\\nIN %\\nLRU\\nLFU\\nLRU\\nLFU\\n0.1%\\n10.52%\\n2.41%\\n33.50%\\n74.97%\\n1%\\n34.72%\\n13.41%\\n94.06%\\n96.11%\\n10%\\n59.86%\\n57.31%\\n99.96%\\n99.95%\\nMemory Compression\\nAccuracy Drop\\n-0.02\\n0.00\\n0.02\\n0.04\\n0.06\\n0.08\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nw/ Cache\\nw/o Cache\\nINT8\\nFigure 8. Accuracy vs. memory compression factor with and with-\\nout high-precision cache for large training dataset.\\nwe apply caching to 5% of tables that account for 97% of\\ntotal model size, LRU performs consistently better than LFU\\nfor the internal training dataset, especially for smaller cache\\nsizes. However, if we apply caching to more tables, LFU\\nperforms better than LRU. One reason for this difference is\\nthe “data shift” pattern in the internal dataset for a few large\\ntables, where some entity ids are popular for a continued\\nperiod of time exhibiting temporal locality but later don’t\\nappear as frequently.\\n5.2.3\\nMemory Reduction\\nSimilar to the analysis in Section 4.3.3, we calculate the\\nmemory compression factor for both non-cached and cached\\nimplementations of low-precision embeddings on GPUs,\\ntaking into account quantization parameter storage over-\\nhead and cache-speciﬁc access counters as well as tags. For\\nexample, the memory compression factor for INT8 embed-\\nding with 1% high-precision LRU cache is 0.259. Note that\\nLFU takes a bit more storage than LRU implementations\\nsince LFU needs to maintain the extra counters to record\\nthe frequency.\\nFigure 8 shows the correspondences between accuracy and\\nmemory reduction with and without cache. For clarity to\\ndemonstrate accuracy differences for the settings that give\\nclose to neutral accuracy, we only plot parts of our results\\nin Figure 8. The trend demonstrated holds true for lower\\nprecision that are not shown in the ﬁgure as well.\\nFor example, with the accuracy drop threshold 0.02%, we\\ncan use INT4 stochastic rounding embedding with 1% high-\\nprecision cache with 86.6% memory savings on GPUs (more\\nthan 7× memory reduction), which signiﬁcantly mitigates\\nthe limited on-device high-bandwidth memory (HBM) ca-\\npacity of GPUs.\\nSubmission and Formatting Instructions for MLSys 2020\\nCache Hit Rate\\nEffectective BW (GB/s)\\n0\\n200\\n400\\n600\\n800\\n20.00%\\n40.00%\\n60.00%\\n80.00%\\n100.00%\\nFP32 Device\\nFP16 Nearest\\nFP16 Stochastic\\nFP32 UVM\\n(a) LRU, forward. FP16 Near./Stoc. overlap with each other.\\nCache Hit Rate\\nEffectective BW (GB/s)\\n0\\n200\\n400\\n600\\n20.00%\\n40.00%\\n60.00%\\n80.00%\\n100.00%\\nFP32 Device\\nFP16 Nearest\\nFP16 Stochastic\\nFP32 UVM\\n(b) LRU, backward\\nFigure 9. Effective bandwidth for FP32 (for embedding table on\\ndevice and on uniﬁed memory) vs. FP16 with high-precision LRU\\ncache. The effective bandwidth of LFU is similar.\\n5.2.4\\nSpeed improvement\\nWhen embedding tables are too big to ﬁt on-device HBM\\nmemory of GPUs, one should use mechanisms like uniﬁed\\nmemory (UVM, (NVIDIA, 2020). However, UVM operates\\nin a granularity considerably bigger than embedding table\\nrows with non-contiguous access pattern, signiﬁcantly wast-\\ning PCIe bandwidth. By using low-precision embedding\\ntables with high-precision cache, we can either (1) reduce\\nthe size of cache plus embedding table small enough to ﬁt\\nthe both entirely within device memory, or (2) still put the\\nlow-precision embedding table in the uniﬁed memory but\\nreducing PCIe trafﬁc.\\nWe want to make sure that our GPU cache implementation\\nis fast enough to outperform UVM when cache hit rate is\\nlow, and saturates a large fraction of peak device memory\\nbandwidth when hit rate is high. This is demonstrated by\\nFigure 9.\\nFigure 9 shows micro-benchmark results evaluated on a\\nNvidia V100 GPU, where a large low-precision embedding\\ntable is allocated on the uniﬁed memory (UVM) and a high-\\nprecision cache is allocated on the device memory. We\\nplot the effective bandwidth (in GB/s) in correspondence to\\ndifferent cache hit rate for LRU cache during the forward\\nand backward passes . The top and bottom lines show the\\nachieved bandwidth of the forward/backward path on the\\ndevice memory and on the uniﬁed memory, which corre-\\nspond to the upper and lower bound of the bandwidth. Note\\nthat the bandwidth of the device memory is bounded by\\nthe HBM peak bandwidth (900 GB/s), while the bandwidth\\nof the uniﬁed memory is bounded by the PCIe bandwidth.\\nWhen the hit rate is high, we get close to the device band-\\nwidth. For lower hit rate, the LRU/LFU implementation\\nis still reasonably faster than the bandwidth achieved by\\nuniﬁed memory.\\nWe also measure an end-to-end training speedup of 16% us-\\ning FP16 LRU cache (cache size is 1% of embedding tables)\\nover the baseline where UVM is used for large embedding\\ntables. This is an example that our technique not only saves\\nmemory but can also improve the training speed.\\n6\\nCONCLUSIONS AND FUTURE WORK\\nIn this paper we presented low-precision embedding table\\nwith a high-precision cache as an effective memory saving\\ntechnique for training large-scale recommendation models,\\nparticularly well suited for systems with high compute band-\\nwidth but limited capacity memory like GPUs. To the best of\\nour knowledge, this is the ﬁrst time mixed-precision training\\nwith a high-precision cache is applied on an industrial-scale\\nrecommendation system. Our best results include reducing\\nmemory by over 7× for a large embedding trained in INT4\\nprecision while maintaining neutral accuracy.\\nAs part of future work, we will further experiment with het-\\nerogeneous precision and replacement policy assignment for\\ndifferent embeddings. We will potentially assign different\\nprecision and cache policy for each embedding based on\\na light-weight proﬁle in the ﬁrst few training iterations for\\nthe best accuracy. This can be useful to handle dataset with\\ndifferent characteristics as we have seen from the Kaggle\\nand our internal dataset. We will further explore different\\nhash functions and replacement heuristics, and evaluate their\\nimpact on model accuracy and performance. Our technique\\ncan be applied to other models with embeddings such as\\nlanguage models. Once proven to work for a wider range of\\nmodels, it can be interesting to add hardware support by aug-\\nmenting low-precision conversion to the existing hardware\\ncache mechanisms.\\nSubmission and Formatting Instructions for MLSys 2020\\nREFERENCES\\nAgarwal, A. A., Hennessy, J. L., and Horowitz, M. H. An\\nanalytical cache model. ACM Transactions on Computer\\nSystems, 1989.\\nAlvarez, J. M. and Salzmann, M. Compression-aware train-\\ning of deep networks. In Proc. Advances in Neural Infor-\\nmation Processing Systems, pp. 856–867, 2017.\\nAndrews, M.\\nCompressing word embeddings.\\nCoRR,\\nabs/1511.06397, 2015.\\nBarz, B. and Denzler, J. Hierarchy-based image embeddings\\nfor semantic image retrieval. In Proc. IEEE Winter Conf.\\nApplications of Computer Vision, pp. 638–647. IEEE,\\n2019.\\nBhavana, P., Kumar, V., and Padmanabhan, V. Block based\\nsingular value decomposition approach to matrix factor-\\nization for recommender systems. Pattern Recognition\\nLetters, abs/1907.07410, 2019.\\nChen, X., Hu, X., Zhou, H., and Xu, N. Fxpnet: Train-\\ning a deep convolutional neural network in ﬁxed-point\\nrepresentation. In IJCNN, 2017.\\nCheng, H.-T., Koc, L., Harmsen, J., Shaked, T., Chandra,\\nT., Aradhye, H., Anderson, G., Corrado, G., Chai, W.,\\nIspir, M., Anil, R., Haque, Z., Hong, L., Jain, V., Liu, X.,\\nand Shah, H. Wide and deep learning for recommender\\nsystems. In RecSys, 2016.\\nChiu, B., Crichton, G., Korhonen, A., and Pyysalo, S. How\\nto train good word embeddings for biomedical nlp. In\\nProc. 15th Workshop on Biomedical Natural Language\\nProcessing, pp. 166–174, 2016.\\nCriteo AI Lab. Display Advertising Challenge.\\nhttp://labs.criteo.com/2014/02/\\nkaggle-display-advertising-challenge\\\\\\n-dataset/, 2014. Accessed: 2020-10-08.\\nDe Sa, C., Leszczynski, M., Zhang, J., Marzoev, A.,\\nAberger, C. R., Olukotun, K., and R´e, C. High-accuracy\\nlow-precision training. arXiv preprint arXiv:1803.03383,\\n2018.\\nElthakeb, A. T., Pilligundla, P., and Esmaeilzadeh, H.\\nSinReQ: Generalized sinusoidal regularization for au-\\ntomatic low-bitwidth deep quantized training. CoRR,\\nabs/1905.01416, 2019.\\nFan, A., Stock, P., Graham, B., Grave, E., Gribonval, R.,\\nJ´egou, H., and Joulin, A. Training with quantization\\nnoise for extreme model compression. arXiv preprint\\narXiv:2004.07320, 2020.\\nFrankle, J. and Carbin, M.\\nThe lottery ticket hypothe-\\nsis: Finding sparse, trainable neural networks. CoRR,\\nabs/1803.03635, 2018.\\nGinart, A., Naumov, M., Mudigere, D., Yang, J. Y., and\\nZou, J. Mixed dimension embeddings with application\\nto memory-efﬁcient recommendation systems.\\narXiv\\npreprint arXiv:1909.11810, 2019.\\nGuan, H., Malevich, A., Yang, J., Park, J., and Yuen, H. Post-\\ntraining 4-bit quantization on embedding tables. arXiv\\npreprint arXiv:1911.02079, 2019.\\nGupta, S., Agrawal, A., Gopalakrishnan, K., and Narayanan,\\nP. Deep learning with limited numerical precision. In\\nICML, 2015.\\nHazelwood, K., Bird, S., Brooks, D., Chintala, S., Diril, U.,\\nDzhulgakov, D., Fawzy, M., Jia, B., Jia, Y., Kalro, A.,\\nLaw, J., Lee, K., Lu, J., Noordhuis, P., Smelyanskiy, M.,\\nXiong, L., and Wang, X. Applied machine learning at\\nFacebook: A datacenter infrastructure perspective. In\\nProc. IEEE Int. Symposium on High Performance Com-\\nputer Architecture, pp. 620–629, 2018.\\nHe, X., Pan, J., Jin, O., Xu, T., Liu, B., Xu, T., Shi, Y.,\\nAtallah, A., Herbrich, R., Bowers, S., et al. Practical\\nlessons from predicting clicks on ads at facebook. In\\nProceedings of the Eighth International Workshop on\\nData Mining for Online Advertising, pp. 1–9, 2014.\\nKalamkar, D., Mudigere, D., Mellempudi, N., Das, D.,\\nBanerjee, K., Avancha, S., Vooturi, D. T., Jammala-\\nmadaka, N., Huang, J., Yuen, H., Yang, J., Park, J.,\\nHeinecke, A., Georganas, E., Srinivasan, S., Kundu, A.,\\nSmelyanskiy, M., Kaul, B., and Dubey, P. A study of\\nBFLOAT16 for deep learning training. arXiv preprint\\narXiv:1905.12322, 2019.\\nKhrulkov, V., Hrinchuk, O., Mirvakhabova, L., and Os-\\neledets, I. V. Tensorized embedding layers for efﬁcient\\nmodel compression. CoRR, abs/1901.10787, 2019.\\nLiu, Y., Liu, Z., Chua, T.-S., and Sun, M. Topical word em-\\nbeddings. In Proc. 29th AAAI Conf. Artiﬁcial Intelligence,\\n2015.\\nLiu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.\\nRoberta: A robustly optimized bert pretraining approach.\\narXiv preprint arXiv:1907.11692, 2019.\\nMikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and\\nDean, J. Distributed representations of words and phrases\\nand their compositionality. In Advances in Neural Infor-\\nmation Processing Systems, pp. 3111–3119, 2013.\\nSubmission and Formatting Instructions for MLSys 2020\\nNaumov, M., Diril, U., Park, J., Ray, B., Jablonski, J., and\\nTulloch, A. On periodic functions as regularizers for\\nquantization of neural networks. CoRR, abs/1811.09862,\\n2018.\\nNaumov, M., Mudigere, D., Shi, H. M., Huang, J., Sun-\\ndaraman, N., Park, J., Wang, X., Gupta, U., Wu, C.,\\nAzzolini, A. G., Dzhulgakov, D., Mallevich, A., Cher-\\nniavskii, I., Lu, Y., Krishnamoorthi, R., Yu, A., Kon-\\ndratenko, V., Pereira, S., Chen, X., Chen, W., Rao, V.,\\nJia, B., Xiong, L., and Smelyanskiy, M. Deep learning\\nrecommendation model for personalization and recom-\\nmendation systems. CoRR, abs/1906.00091, 2019. URL\\nhttps://arxiv.org/abs/1906.00091.\\nNVIDIA. Uniﬁed memory in CUDA 6.\\nhttps://developer.nvidia.com/blog/\\nunified-memory-in-cuda-6/, 2020. Accessed:\\n2020-10-08.\\nPark, J., Naumov, M., Basu, P., Deng, S., Kalaiah, A., Khu-\\ndia, D. S., Law, J., Malani, P., Malevich, A., Satish, N.,\\nPino, J., Schatz, M., Sidorov, A., Sivakumar, V., Tulloch,\\nA., Wang, X., Wu, Y., Yuen, H., Diril, U., Dzhulgakov,\\nD., Hazelwood, K. M., Jia, B., Jia, Y., Qiao, L., Rao, V.,\\nRotem, N., Yoo, S., and Smelyanskiy, M. Deep learn-\\ning inference in Facebook data centers: Characterization,\\nperformance optimizations and hardware implications.\\nCoRR, abs/1811.09886, 2018.\\nPennington, J., Socher, R., and Manning, C. Glove: Global\\nvectors for word representation. In Proceedings of the\\n2014 conference on empirical methods in natural lan-\\nguage processing (EMNLP), 2014.\\nPeters, M., Neumann, M., Zettlemoyer, L., and Yih, W.-t.\\nDissecting contextual word embeddings: Architecture\\nand representation. In Proc. 2018 Conf. Empirical Meth-\\nods in NLP, pp. 1499–1509, 2018.\\nSattigeri, P. and Thiagarajan, J. J. Sparsifying word rep-\\nresentations for deep unordered sentence modeling. In\\nProc. 1st Workshop on Representation Learning for NLP,\\npp. 206–214.\\nSun, F., Guo, J., Lan, Y., Xu, J., and Cheng, X. Sparse word\\nembeddings using l1 regularized online learning. In Proc.\\n25th Int. Joint Conf. Artiﬁcial Intelligence, 2016.\\nVasileva, M. I., Plummer, B. A., Dusad, K., Rajpal, S.,\\nKumar, R., and Forsyth, D. Learning type-aware embed-\\ndings for fashion compatibility. In Proc. European Conf.\\nComputer Vision, pp. 390–405, 2018.\\nWang, R., Fu, B., Fu, G., and Wang, M. Deep & cross\\nnetwork for ad click predictions. In ADKDD@KDD,\\n2017.\\nWu, S., Hsiao, L., Cheng, X., Hancock, B., Rekatsinas, T.,\\nLevis, P., and R´e, C. Fonduer: Knowledge base construc-\\ntion from richly formatted data. In Proceedings of the\\n2018 International Conference on Management of Data,\\n2018a.\\nWu, S., Li, G., Chen, F., and Shi, L.\\nTraining and in-\\nference with integers in deep neural networks. CoRR,\\nabs/1802.04680, 2018b.\\nZhang, J., Yang, J., and Yuen, H.\\nTraining with low-\\nprecision embedding tables. In Systems for Machine\\nLearning Workshop at NeurIPS 2018, 2018.\\nZhao, W., Xie, D., Jia, R., Qian, Y., Ding, R., Sun, M., and\\nLi, P. Distributed hierarchical gpu parameter server for\\nmassive scale deep learning ads systems. arXiv preprint\\narXiv:2003.05622, 2020.\\nSubmission and Formatting Instructions for MLSys 2020\\nA\\nAPPENDIX: MEMORY COMPRESSION\\nFACTOR\\nMemory compression factor is used to characterize the com-\\npressed model size after applying the low-precision embed-\\nding table with a high-precision cache. We need to consider\\nthe following dominant components:\\n• Low-precision table size, MTlowprec;\\n• Quantization parameter storage size, MQ;\\n• High-precision cache size, MC;\\n• Cache tags size, Mtags;\\n• Access counters (if using LFU), Mcnt;\\n• Original FP32 embedding table size, MTF P 32\\nSpeciﬁcally, the “memory compression factor” is calculated\\nas\\n(MTlowprec + MC + Mtags + Mcnt)/MTF P 32\\n(2)\\nwhere:\\n• Cache tags are INT32 numbers, one per each row in\\ncache;\\n• Access counters are INT32 numbers, one per each row\\nof the embedding.\\nAssume the number of embedding rows is E, the embedding\\ndimension is D, and the low-precision bitwidth is bitwidth,\\nthen the low-precision table size is calculated as:\\nMTlowprec = E × (bitwidth × D + 64),\\n(3)\\nwhere 64 is the per-row overhead of quantization parameters\\n(in FP32): 32 bits for the per-row scale and 32 bits for the\\nper-row bias.\\nTable 11 shows memory compression factors calculated for\\na sample of precision and cache size combinations using a\\nLFU cache, and the embedding dimension D = 128 (used in\\nCPU experiments). As the number of rows per embedding is\\ndifferent for each table, we compute cache and quantization\\noverhead as the number of bits per embedding row.\\nFigure 10 plots the complete accuracy vs. memory com-\\npression trends for CPU experiments with DLRM-Kaggle\\ndataset using compression factors computed by Table 11.\\nSubmission and Formatting Instructions for MLSys 2020\\nTable 11. Calculation of memory compression factor\\nBITWIDTH\\nCACHE SIZE\\nBITS PER ROW\\nBITS PER ROW\\nACCESS COUNTER\\nCACHE TAGS\\nCOMPRESSION\\nRATIO\\n(FP32)\\n(QUANTIZED)\\nBITS PER ROW\\nBITS PER ROW\\nFACTOR\\n8\\n0\\n128 × 32\\n8 × 128 + 64\\n0\\n0\\n0.26563\\n4\\n0\\n128 × 32\\n4 × 128 + 64\\n0\\n0\\n0.14063\\n2\\n0\\n128 × 32\\n2 × 128 + 64\\n0\\n0\\n0.07813\\n4\\n0.3\\n128 × 32\\n4 × 128 + 64\\n32\\n32 × 0.3\\n0.45078\\n8\\n0.1\\n128 × 32\\n8 × 128 + 64\\n32\\n32 × 0.1\\n0.37422\\n8\\n0.05\\n128 × 32\\n8 × 128 + 64\\n32\\n32 × 0.05\\n0.32383\\n4\\n0.1\\n128 × 32\\n4 × 128 + 64\\n32\\n32 × 0.1\\n0.24922\\n4\\n0.05\\n128 × 32\\n4 × 128 + 64\\n32\\n32 × 0.05\\n0.19883\\n2\\n0.1\\n128 × 32\\n2 × 128 + 64\\n32\\n32 × 0.1\\n0.18672\\n2\\n0.05\\n128 × 32\\n2 × 128 + 64\\n32\\n32 × 0.05\\n0.13633\\nMemory Compression\\nAccuracy Drop\\n-0.25\\n0.00\\n0.25\\n0.50\\n0.75\\n1.00\\n1.25\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\nw/ Cache\\nw/o Cache\\nFigure 10. Accuracy vs. memory compression factor with and without high-precision cache on Criteo-Kaggle dataset.\\n',\n",
       " '2006.14827': 'Memory-efficient Embedding for Recommendations\\nXiangyu Zhao, Haochen Liu,\\nHui Liu, Jiliang Tang\\nMichigan State University\\n{zhaoxi35,liuhaoc1,liuhui7,tangjili}@msu.edu\\nWeiwei Guo, Jun Shi, Sida Wang,\\nHuiji Gao, Bo Long\\nLinkedin Corporation\\n{wguo,jshi,sidwang,hgao,blong}@linkedin.com\\nABSTRACT\\nPractical large-scale recommender systems usually contain thou-\\nsands of feature fields from users, items, contextual information,\\nand their interactions. Most of them empirically allocate a unified\\ndimension to all feature fields, which is memory inefficient. Thus it\\nis highly desired to assign different embedding dimensions to differ-\\nent feature fields according to their importance and predictability.\\nDue to the large amounts of feature fields and the nuanced rela-\\ntionship between embedding dimensions with feature distributions\\nand neural network architectures, manually allocating embedding\\ndimensions in practical recommender systems can be very difficult.\\nTo this end, we propose an AutoML based framework (AutoDim)\\nin this paper, which can automatically select dimensions for dif-\\nferent feature fields in a data-driven fashion. Specifically, we first\\nproposed an end-to-end differentiable framework that can calculate\\nthe weights over various dimensions in a soft and continuous man-\\nner for feature fields, and an AutoML based optimization algorithm;\\nthen we derive a hard and discrete embedding component archi-\\ntecture according to the maximal weights and retrain the whole\\nrecommender framework. We conduct extensive experiments on\\nbenchmark datasets to validate the effectiveness of the AutoDim\\nframework.\\nACM Reference Format:\\nXiangyu Zhao, Haochen Liu,, Hui Liu, Jiliang Tang and Weiwei Guo, Jun\\nShi, Sida Wang,, Huiji Gao, Bo Long. 2020. Memory-efficient Embedding for\\nRecommendations. In Proceedings of ACM Conference (Conference’17). ACM,\\nNew York, NY, USA, 12 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\\n1\\nINTRODUCTION\\nWith the explosive growth of the world-wide web, huge amounts of\\ndata have been generated, which results in the increasingly severe\\ninformation overload problem, potentially overwhelming users [5].\\nRecommender systems can mitigate the information overload prob-\\nlem through suggesting personalized items that best match users’\\npreferences [1, 2, 22, 26, 33, 34]. Recent years have witnessed the\\nincreased development and popularity of deep learning based rec-\\nommender systems (DLRSs) [27, 41, 44], which outperform tradi-\\ntional recommendation techniques, such as collaborative filtering\\nand learning-to-rank, because of their strong capability of feature\\nrepresentation and deep inference [45].\\nPermission to make digital or hard copies of all or part of this work for personal or\\nclassroom use is granted without fee provided that copies are not made or distributed\\nfor profit or commercial advantage and that copies bear this notice and the full citation\\non the first page. Copyrights for components of this work owned by others than ACM\\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\\nfee. Request permissions from permissions@acm.org.\\nConference’17, July 2017, Washington, DC, USA\\n© 2020 Association for Computing Machinery.\\nACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\\n0\\n0\\n1\\nField 1\\nField m\\nField M\\n0\\n1\\n0\\n1\\n0\\n0\\nUser\\nItem\\nContext\\nInteraction\\nMLP\\nComponent\\nOutput\\nLayer\\nEmbedding\\nComponent\\nInput\\nFeatures\\nFeature\\nFields\\nFigure 1: The Typically DLRS architecture.\\nReal-world recommender systems typically involve a massive\\namount of categorical feature fields from users (e.g. occupation and\\nuserID), items (e.g. category and itemID), contextual information\\n(e.g. time and location), and their interactions (e.g. user’s purchase\\nhistory of items). DLRSs first map these categorical features into\\nreal-valued dense vectors via an embedding-component [28, 30, 49],\\ni.e., the embedding-lookup process, which leads to huge amounts\\nof embedding parameters. For instance, the YouTube recommender\\nsystem consists of 1 million of unique videoIDs, and assign each\\nvideoID with a specific 256-dimensional embedding vector; in other\\nwords, the videoID feature field alone occupies 256 million parame-\\nters [10]. Then, the DLRSs nonlinearly transform the input embed-\\ndings from all feature fields and generate the outputs (predictions)\\nvia the MLP-component (Multi-Layer Perceptron), which usually\\ninvolves only several fully connected layers in practice. Therefore,\\ncompared to the MLP-component, the embedding-component dom-\\ninates the number of parameters in practical recommender systems,\\nwhich naturally plays a tremendously impactful role in the recom-\\nmendations.\\nThe majority of existing recommender systems assign fixed and\\nunified embedding dimension for all feature fields, such as the\\nfamous Wide&Deep model [8], which may lead to memory in-\\nefficiency. First, the embedding dimension often determines the\\ncapacity to encode information. Thus, allocating the same dimen-\\nsion to all feature fields may lose the information of high predictive\\narXiv:2006.14827v2  [cs.IR]  21 Oct 2020\\n(a) Dimensionality Search\\n(b) Parameter Re-training\\nEmbedding\\nLookup\\nDeriving\\nDiscrete\\nArchitectures\\n0\\n0\\n1\\nField 1\\nField m\\nField M\\n0\\n1\\n0\\n1\\n0\\n0\\nWeights\\nTransforms\\n0\\n0\\n1\\nField 1\\nField m\\nField M\\n0\\n1\\n0\\n1\\n0\\n0\\n0.7\\n0.3\\n0.2\\n0.8\\n0.6\\n0.4\\nEmbedding\\nLookup\\nTransforms\\nFigure 2: Overview of the proposed framework.\\nfeatures while wasting memory on non-predictive features. There-\\nfore, we should assign large dimension to the high informative and\\npredictive features, for instance, the “location” feature in location-\\nbased recommender systems [1]. Second, different feature fields\\nhave different cardinality (i.e. the number of unique values). For\\nexample, the gender feature has only two (i.e. male and female),\\nwhile the itemID feature usually involves millions of unique val-\\nues. Intuitively, we should allocate larger dimensions to the feature\\nfields with more unique feature values to encode their complex\\nrelationships with other features, and assign smaller dimensions to\\nfeature fields with smaller cardinality to avoid the overfitting prob-\\nlem due to the over-parameterization [11, 18, 20, 47]. According to\\nthe above reasons, it is highly desired to assign different embedding\\ndimensions to different feature fields in a memory-efficient manner.\\nIn this paper, we aim to enable different embedding dimen-\\nsions for different feature fields for recommendations. We face\\ntremendous challenges. First, the relationship among embedding\\ndimensions, feature distributions and neural network architectures\\nis highly intricate, which makes it hard to manually assign em-\\nbedding dimensions to each feature field [11]. Second, real-world\\nrecommender systems often involve hundreds and thousands of\\nfeature fields. It is difficult, if possible, to artificially select different\\ndimensions for all feature fields, due to the expensive computa-\\ntion cost from the incredibly huge (𝑁𝑀, with 𝑁the number of\\ncandidate dimensions for each feature field to select, and 𝑀the\\nnumber of feature fields) search space. Our attempt to address these\\nchallenges results in an end-to-end differentiable AutoML based\\nframework (AutoDim), which can efficiently allocate embedding\\ndimensions to different feature fields in an automated and data-\\ndriven manner. Our experiments on benchmark datasets demon-\\nstrate the effectiveness of the proposed framework. We summarize\\nour major contributions as: (i) we propose an end-to-end AutoML\\nbased framework AutoDim, which can automatically select various\\nembedding dimensions to different feature fields; (ii) we develop\\ntwo embedding lookup methods and two embedding transforma-\\ntion approaches, and compare the impact of their combinations on\\nthe embedding dimension allocation decision; and (iii) we demon-\\nstrate the effectiveness of the proposed framework on real-world\\nbenchmark datasets.\\nThe rest of this paper is organized as follows. In Section 2, we in-\\ntroduce details about how to assign various embedding dimensions\\nfor different feature fields in an automated and data-driven fashion,\\nand propose an AutoML based optimization algorithm. Section 3\\ncarries out experiments based on real-world datasets and presents\\nexperimental results. Section 4 briefly reviews related work. Finally,\\nsection 5 concludes this work and discusses our future work.\\n2\\nFRAMEWORK\\nIn order to achieve the automated allocation of different embed-\\nding dimensions to different feature fields, we propose an AutoML\\nbased framework, which effectively addresses the challenges we\\ndiscussed in Section 1. In this section, we will first introduce the\\noverview of the whole framework; then we will propose an end-to-\\nend differentiable model with two embedding-lookup methods and\\ntwo embedding dimension search methods, which can compute the\\nweights of different dimensions for feature fields in a soft and con-\\ntinuous fashion, and we will provide an AutoML based optimization\\nalgorithm; finally, we will derive a discrete embedding architecture\\nupon the maximal weights, and retrain the whole DLRS framework.\\n2.1\\nOverview\\nOur goal is to assign different feature fields various embedding di-\\nmensions in an automated and data-driven manner, so as to enhance\\nthe memory efficiency and the performance of the recommender\\nsystem. We illustrate the overall framework in Figure 2, which\\nconsists of two major stages:\\n2.1.1\\nDimensionality search stage. It aims to find the optimal\\nembedding dimension for each feature field. To be more specific, we\\nfirst assign a set of candidate embeddings with different dimensions\\nto a specific categorical feature via an embedding-lookup step; then,\\nwe unify the dimensions of these candidate embeddings through a\\ntransformation step, which is because of the fixed input dimension\\nof the first MLP layer; next, we obtain the formal embedding for this\\ncategorical feature by computing the weighted sum of all its trans-\\nformed candidate embeddings, and feed it into the MLP-component.\\nThe DLRS parameters including the embeddings and MLP layers are\\nlearned upon the training set, while the architectural weights over\\nthe candidate embeddings are optimized upon the validation set,\\nwhich prevents the framework selecting the embedding dimensions\\nthat overfit the training set [24, 29].\\nIn practice, before the alternative training of DLRS parameters\\nand architectural weights, we initially assign equivalent architec-\\ntural weights on all candidate embeddings (e.g., [0.5, 0.5] for the\\nexample in Figure 2), fix these architectural weights and pre-train\\nthe DLRS including all candidate embeddings. The pre-training\\nenables a fair competition between candidate embeddings when\\nwe start to update architectural weights.\\n2.1.2\\nParameter re-training stage. According to the architec-\\ntural weights learned in dimensionality search stage, we select the\\nembedding dimension for each feature field, and re-train the param-\\neters of DLRS parameters (i.e., MLPs and selected embeddings) on\\nthe training dataset in an end-to-end fashion. It is noteworthy that:\\n(i) re-training stage is necessary, since in dimensionality search\\nstage, the model performance is also influenced by the suboptimal\\nembedding dimensions, which are not desired in practical recom-\\nmender system; and (ii) new embeddings are still unified into the\\nsame dimension, since most existing deep recommender models\\n(such as FM [32], DeepFM [13], NFM [14]) capture the interactions\\nbetween two feature fields via a interaction operation (e.g., inner\\nproduct) over their embedding vectors. These interaction opera-\\ntions constrain the embedding vectors to have same dimensions.\\nNote that numerical features will be converted into categorical\\nfeatures through bucketing, and we omit this process in the follow-\\ning sections for simplicity. Next, we will introduce the details of\\neach stage.\\n2.2\\nDimensionality Search\\nAs discussed in Section 1, different feature fields have different\\ncardinalities and various contributions to the final prediction. In-\\nspired by this phenomenon, it is highly desired to enable various\\nembedding dimensions for different feature fields. However, due to\\na large amount of feature fields and the complex relationship be-\\ntween embedding dimensions with feature distributions and neural\\nnetwork architectures, it is difficult to manually select embedding\\ndimensions via conventional dimension reduction methods. An\\nintuitive solution to tackle this challenge is to assign several embed-\\nding spaces with various dimensions to each feature field, and then\\nthe DLRS automatically selects the optimal embedding dimension\\nfor each feature field.\\n(a) Separate Embeddings\\n0\\n1\\n0\\n(b) Weight-sharing Embeddings\\n0\\n1\\n0\\nFigure 3: The embedding lookup methods.\\n2.2.1\\nEmbedding Lookup Tricks. Suppose for each user-item\\ninteraction instance, we have 𝑀input features (𝑥1, · · · ,𝑥𝑀), and\\neach feature 𝑥𝑚belongs to a specific feature field, such as gender\\nand age, etc. For the 𝑚𝑡ℎfeature field, we assign 𝑁candidate em-\\nbedding spaces {X1𝑚, · · · , X𝑁𝑚}. The dimension of an embedding in\\neach space is 𝑑1, · · · ,𝑑𝑁, where 𝑑1 < · · · < 𝑑𝑁; and the cardinality\\nof these embedding spaces are the number of unique feature values\\nin this feature field. Correspondingly, we define {x1𝑚, · · · , x𝑁𝑚} as\\nthe set of candidate embeddings for a given feature 𝑥𝑚from all\\nembedding spaces, as shown in Figure 3 (a). Note that we assign the\\nsame candidate dimension to all feature fields for simplicity, but it\\nis straightforward to introduce different candidate sets. Therefore,\\nthe total space assigned to the feature 𝑥𝑚is Í𝑁\\n𝑛=1 𝑑𝑛. However, in\\nreal-world recommender systems with thousands of feature fields,\\ntwo challenges lie in this design include (i) this design needs huge\\nspace to store all candidate embeddings, and (ii) the training effi-\\nciency is reduced since a large number of parameters need to be\\nlearned.\\nTo address these challenges, we propose an alternative solution\\nfor large-scale recommendations, named weight-sharing embed-\\nding architecture. As illustrated in Figure 3 (b), we only allocate a\\n𝑑𝑁-dimensional embedding to a given feature 𝑥𝑚, referred as to\\nx′𝑚, then the 𝑛𝑡ℎcandidate embedding x𝑛𝑚corresponds to the first\\n𝑑𝑛digits of x′𝑚. The advantages associated with weight-sharing\\nembedding method are two-fold, i.e., (i) it is able to reduce the stor-\\nage space and increase the training efficiency, as well as (ii) since\\nthe relatively front digits of x′𝑚have more chances to be retrieved\\nand then be trained (e.g. the “red part” of x′𝑚is leveraged by all\\nEmbedding\\nLookup\\nLinear\\nTransform\\nBatchNorm\\nWeighted\\nSum\\n0\\n1\\n0\\nFigure 4: Method 1 - Linear Transformation.\\ncandidates in Figure 3 (b)), we intuitively wish they can capture\\nmore essential information of the feature 𝑥𝑚.\\n2.2.2\\nUnifying Various Dimensions. Since the input dimension\\nof the first MLP layer in existing DLRSs is often fixed, it is difficult\\nfor them to handle various candidate dimensions. Thus we need to\\nunify the embeddings {x1𝑚, · · · , x𝑁𝑚} into same dimension, and we\\ndevelop two following methods:\\nMethod 1: Linear Transformation. Figure 4 (a) illustrates the\\nlinear transformation method to handle the various embedding\\ndimensions (the difference of two embedding lookup methods is\\nomitted here). We introduce 𝑁fully-connected layers, which trans-\\nform embedding vectors {x1𝑚, · · · , x𝑁𝑚} into the same dimension\\n𝑑𝑁:\\nex𝑛𝑚←W⊤𝑛x𝑛𝑚+ b𝑛\\n∀𝑛∈[1, 𝑁]\\n(1)\\nwhere W𝑛∈R𝑑𝑛×𝑑𝑁is weight matrice and b𝑛∈R𝑑𝑁is bias vector.\\nFor each field, all candidate embeddings with the same dimension\\nshare the same weight matrice and bias vector, which can reduce\\nthe amount of model parameters. With the linear transformations,\\nwe map the original embedding vectors {x1𝑚, · · · , x𝑁𝑚} into the\\nsame dimensional space, i.e., {ex1𝑚, · · · ,ex𝑁𝑚} ∈R𝑑𝑁. In practice, we\\ncan observe that the magnitude of the transformed embeddings\\n{ex1𝑚, · · · ,ex𝑁𝑚} varies significantly, which makes them become in-\\ncomparable. To tackle this challenge, we conduct BatchNorm [16]\\non the transformed embeddings {ex1𝑚, · · · ,ex𝑁𝑚} as:\\nbx𝑛𝑚←\\nex𝑛\\n𝑚−𝜇𝑛\\nB\\n√︃\\n(𝜎𝑛\\nB)2+𝜖\\n∀𝑛∈[1, 𝑁]\\n(2)\\nwhere 𝜇𝑛\\nB is the mini-batch mean and (𝜎𝑛\\nB)2 is the mini-batch vari-\\nance for ∀𝑛∈[1, 𝑁]. 𝜖is a small constant added to the mini-batch\\nvariance for numerical stability when (𝜎𝑛\\nB)2 is very small. After\\nBatchNorm, the linearly transformed embeddings {ex1𝑚, · · · ,ex𝑁𝑚} be-\\ncome to magnitude-comparable embedding vectors {bx1𝑚, · · · ,bx𝑁𝑚}\\nwith the same dimension 𝑑𝑁.\\nMethod 2: Zero Padding. Inspired by zero-padding techniques\\nfrom the computer version community, which pads the input vol-\\nume with zeros around the border, we address the problem of vari-\\nous embedding dimensions by padding shorter embedding vectors\\nto the same length as the longest embedding dimension 𝑑𝑁with\\nzeros, which is illustrated in Figure 5. For the embedding vectors\\n{x1\\n𝑖, · · · , x𝑁\\n𝑖} with different dimensions, we first execute Batch-\\nNorm process, which forces the original embeddings {x1\\n𝑖, · · · , x𝑁\\n𝑖}\\ninto becoming magnitude-comparable embeddings:\\nex𝑛𝑚←\\nx𝑛\\n𝑚−𝜇𝑛\\nB\\n√︃\\n(𝜎𝑛\\nB)2+𝜖\\n∀𝑛∈[1, 𝑁]\\n(3)\\nwhere 𝜇𝑛\\nB, (𝜎𝑛\\nB)2 are the mini-batch mean and variance. 𝜖is the\\nconstant for numerical stability. The transformed {ex1𝑚, · · · ,ex𝑁𝑚} are\\nmagnitude-comparable embeddings. Then we pad the {ex1𝑚, · · · ,ex𝑁−1\\n𝑚\\n}\\nto the same length 𝑑𝑁by zeros:\\nbx𝑛𝑚←𝑝𝑎𝑑𝑑𝑖𝑛𝑔(ex𝑛𝑚, 𝑑𝑁−𝑑𝑛)\\n∀𝑛∈[1, 𝑁]\\n(4)\\nwhere the second term of each padding formula is the number of\\nzeros to be padded with the embedding vector of the first term.\\nThen the embeddings {bx1𝑚, · · · ,bx𝑁𝑚} share the same dimension\\n𝑑𝑁. Compared with the linear transformation (method 1), the zero-\\npadding method reduces lots of linear-transformation computations\\nand corresponding parameters. The possible drawback is that the\\nfinal embeddings {bx1𝑚, · · · ,bx𝑁𝑚} becomes spatially unbalanced since\\nthe tail parts of some final embeddings are zeros. Next, we will\\nintroduce embedding dimension selection process.\\n2.2.3\\nDimension Selection. In this paper, we aim to select the\\noptimal embedding dimension for each feature field in an automated\\nand data-driven manner. This is a hard (categorical) selection on the\\ncandidate embedding spaces, which will make the whole framework\\nnot end-to-end differentiable. To tackle this challenge, in this work,\\nwe approximate the hard selection over different dimensions via\\nintroducing the Gumbel-softmax operation [17], which simulates\\nthe non-differentiable sampling from a categorical distribution by\\na differentiable sampling from the Gumbel-softmax distribution.\\nTo be specific, suppose weights {𝛼1𝑚, · · · , 𝛼𝑁\\n𝑚} are the class prob-\\nabilities over different dimensions. Then a hard selection 𝑧can be\\ndrawn via the the gumbel-max trick [12] as:\\n𝑧= one_hot\\n\\x12\\narg max\\n𝑛∈[1,𝑁]\\n\\x02\\nlog𝛼𝑛\\n𝑚+ 𝑔𝑛\\n\\x03\\x13\\n𝑤ℎ𝑒𝑟𝑒𝑔𝑛= −log (−log (𝑢𝑛))\\n𝑢𝑛∼𝑈𝑛𝑖𝑓𝑜𝑟𝑚(0, 1)\\n(5)\\nEmbedding\\nLookup\\nBatchNorm\\nWeighted\\nSum\\n0\\n1\\n0\\nZero\\nPadding\\n0\\n0\\nzeros\\nzeros\\nFigure 5: Method 2 - Zero Padding Transformation.\\nThe gumbel noises𝑔𝑖, · · · ,𝑔𝑁are i.i.d samples, which perturb log𝛼𝑛𝑚\\nterms and make the arg max operation that is equivalent to draw-\\ning a sample by 𝛼1𝑚, · · · , 𝛼𝑁\\n𝑚weights. However, this trick is non-\\ndifferentiable due to the arg max operation. To deal with this prob-\\nlem, we use the softmax function as a continuous, differentiable\\napproximation to arg max operation, i.e., straight-through gumbel-\\nsoftmax [17]:\\n𝑝𝑛\\n𝑚=\\nexp\\n\\x10 log(𝛼𝑛\\n𝑚)+𝑔𝑛\\n𝜏\\n\\x11\\nÍ𝑁\\n𝑖=1 exp\\n\\x10 log(𝛼𝑖𝑚)+𝑔𝑖\\n𝜏\\n\\x11\\n(6)\\nwhere 𝜏is the temperature parameter, which controls the smooth-\\nness of the output of gumbel-softmax operation. When𝜏approaches\\nzero, the output of the gumbel-softmax becomes closer to a one-hot\\nvector. Then 𝑝𝑛𝑚is the probability of selecting the 𝑛𝑡ℎcandidate\\nembedding dimension for the feature 𝑥𝑚, and its embedding x𝑚\\ncan be formulated as the weighted sum of {bx1𝑚, · · · ,bx𝑁𝑚}:\\nx𝑚= Í𝑁\\n𝑛=1 𝑝𝑛𝑚· bx𝑛𝑚\\n∀𝑚∈[1, 𝑀]\\n(7)\\nWe illustrate the weighted sum operations in Figure 4 and 5. With\\ngumbel-softmax operation, the dimensionality search process is\\nend-to-end differentiable. The discrete embedding dimension selec-\\ntion conducted based on the weights {𝛼𝑛𝑚} will be detailed in the\\nfollowing subsections.\\nThen, we concatenate the embeddings h0 = [x1, · · · , x𝑀] and\\nfeed h0 input into 𝐿multilayer perceptron layers:\\nh𝑙= 𝜎\\n\\x10\\nW⊤\\n𝑙h𝑙−1 + b𝑙\\n\\x11\\n∀𝑙∈[1, 𝐿]\\n(8)\\nwhere W𝑙and b𝑙are the weight matrix and the bias vector for the\\n𝑙𝑡ℎMLP layer. 𝜎(·) is the activation function such as ReLU and\\nTanh. Finally, the output layer that is subsequent to the last MLP\\nlayer, produces the prediction of the current user-item interaction\\ninstance as:\\nˆ𝑦= 𝜎\\x00W⊤\\n𝑜h𝐿+ b𝑜\\n\\x01\\n(9)\\nwhere W𝑜and b𝑜are the weight matrix and bias vector for the\\noutput layer. Activation function 𝜎(·) is selected based on different\\nrecommendation tasks, such as Sigmoid function for regression [8],\\nand Softmax for multi-class classification [38]. Correspondingly,\\nthe objective function L( ˆ𝑦,𝑦) between prediction ˆ𝑦and ground\\ntruth label 𝑦also varies based on different recommendation tasks.\\nIn this work, we leverage negative log-likelihood function:\\nL( ˆ𝑦,𝑦) = −𝑦log ˆ𝑦−(1 −𝑦) log(1 −ˆ𝑦)\\n(10)\\nwhere 𝑦is the ground truth (1 for like or click, 0 for dislike or non-\\nclick). By minimizing the objective function L( ˆ𝑦,𝑦), the dimension-\\nality search framework updates the parameters of all embeddings,\\nhidden layers, and weights {𝛼𝑛𝑚} through back-propagation. The\\nhigh-level idea of the dimensionality search is illustrated in Figure 2\\n(a), where we omit some details of embedding-lookup, transforma-\\ntions and gumbel-softmax for the sake of simplicity.\\n2.3\\nOptimization\\nIn this subsection, we will detail the optimization method of the pro-\\nposed AutoDim framework. In AutoDim, we formulate the selection\\nover different embedding dimensions as an architectural optimiza-\\ntion problem and make it end-to-end differentiable by leveraging\\nthe Gumbel-softmax technique. The parameters to be optimized\\nin AutoDim are two-fold, i.e., (i) W: the parameters of the DLRS,\\nincluding the embedding-component and the MLP-component; (ii)\\n𝜶: the weights {𝛼𝑛𝑚} on different embedding spaces ({𝑝𝑛𝑚} are cal-\\nculated based on {𝛼𝑛𝑚} as in Equation (6)). DLRS parameters W and\\narchitectural weights 𝜶can not be optimized simultaneously on\\ntraining dataset as conventional supervised attention mechanism\\nsince the optimization of them are highly dependent on each other.\\nIn other words, simultaneously optimization on training dataset\\nmay result in model overfitting on the examples from training\\ndataset.\\nInspired by the differentiable architecture search (DARTS) tech-\\nniques [24], W and 𝜶are alternately optimized through gradient\\ndescent. Specifically, we alternately update W by optimizing the\\nloss L𝑡𝑟𝑎𝑖𝑛on the training data and update 𝜶by optimizing the\\nloss L𝑣𝑎𝑙on the validation data:\\nmin\\n𝜶\\nL𝑣𝑎𝑙\\n\\x00W∗(𝜶), 𝜶\\x01\\n𝑠.𝑡. W∗(𝜶) = arg min\\nW L𝑡𝑟𝑎𝑖𝑛(W, 𝜶∗)\\n(11)\\nthis optimization forms a bilevel optimization problem [29], where\\narchitectural weights 𝜶and DLRS parameters W are identified as\\nthe upper-level variable and lower-level variable. Since the inner\\noptimization of W is computationally expensive, directly optimizing\\nAlgorithm 1 DARTS based Optimization for AutoDim.\\nInput: the features (𝑥1, · · · ,𝑥𝑀) of user-item interactions and the\\ncorresponding ground-truth labels 𝑦\\nOutput: the well-learned DLRS parameters W∗; the well-learned\\nweights on various embedding spaces 𝜶∗\\n1: while not converged do\\n2:\\nSample a mini-batch of user-item interactions from\\nvalidation data\\n3:\\nUpdate 𝜶by descending ∇𝜶L𝑣𝑎𝑙\\n\\x00W∗(𝜶), 𝜶\\x01 with the\\napproximation in Eq.(12)\\n4:\\nCollect a mini-batch of training data\\n5:\\nGenerate predictions ˆ𝑦via DLRS with current W and\\narchitectural weights 𝜶\\n6:\\nUpdate W by descending ∇WL𝑡𝑟𝑎𝑖𝑛(W, 𝜶)\\n7: end while\\n𝜶via Eq.(11) is intractable. To address this challenge, we take\\nadvantage of the approximation scheme of DARTS:\\narg min\\nW L𝑡𝑟𝑎𝑖𝑛(W, 𝜶∗) ≈W −𝜉∇WL𝑡𝑟𝑎𝑖𝑛(W, 𝜶)\\n(12)\\nwhere 𝜉is the learning rate. In the approximation scheme, when\\nupdating 𝜶via Eq.(12), we estimate W∗(𝜶) by descending the gra-\\ndient ∇WL𝑡𝑟𝑎𝑖𝑛(W, 𝜶) for only one step, rather than to optimize\\nW(𝜶) thoroughly to obtain W∗(𝜶) = arg minW L𝑡𝑟𝑎𝑖𝑛(W, 𝜶∗).\\nIn practice, it usually leverages the first-order approximation by\\nsetting 𝜉= 0, which can further enhance the computation efficiency.\\nThe DARTS based optimization algorithm for AutoDim is de-\\ntailed in Algorithm 1. Specifically, in each iteration, we first sample\\na batch of user-item interaction data from the validation set (line 2);\\nnext, we update the architectural weights 𝜶upon it (line 3); after-\\nward, the DLRS make the predictions ˆ𝑦on the batch of training data\\nwith current DLRS parameters W and architectural weights 𝜶(line\\n5); eventually, we update the DLRS parameters W by descending\\n∇WL𝑡𝑟𝑎𝑖𝑛(W, 𝜶) (line 6).\\n2.3.1\\nA pre-train trick. In practice, in order to enable a fair com-\\npetition between the candidate embeddings, for each feature field,\\nwe first allocate the equivalent architectural weights initially on all\\nits candidate embeddings, e.g., [0.5, 0.5] if there are two candidate\\nembedding dimensions. Then, we fix these initialized architectural\\nweights 𝜶and pre-train the DLRS parameters W including all can-\\ndidate embeddings. This process ensures a fair competition between\\ncandidate embeddings when we begin to update 𝜶.\\n2.4\\nParameter Re-Training\\nSince the suboptimal embedding dimensions in dimensionality\\nsearch stage also influence the model training, a re-training stage is\\ndesired to training the model with only optimal dimensions, which\\ncan eliminate these suboptimal influences. In this subsection, we\\nwill introduce how to select optimal embedding dimension for each\\nfeature field and the details of re-training the recommender system\\nwith the selected embedding dimensions.\\n2.4.1\\nDeriving Discrete Dimensions. During re-training, the\\ngumbel-softmax operation is no longer used, which means that the\\noptimal embedding space (dimension) are selected for each feature\\nAlgorithm 2 The Optimization of DLRS Re-training Process.\\nInput: the features (𝑥1, · · · ,𝑥𝑀) of user-item interactions and the\\ncorresponding ground-truth labels 𝑦\\nOutput: the well-learned DLRS parameters W∗\\n1: while not converged do\\n2:\\nSample a mini-batch of training data\\n3:\\nGenerate predictions ˆ𝑦via DLRS with current W\\n4:\\nUpdate W by descending ∇WL𝑡𝑟𝑎𝑖𝑛(W)\\n5: end while\\nfield as the one corresponding to the largest weight, based on the\\nwell-learned 𝜶. It is formally defined as:\\nX𝑚= X𝑘𝑚,\\n𝑤ℎ𝑒𝑟𝑒𝑘= arg max𝑛∈[1,𝑁] 𝛼𝑛𝑚\\n∀𝑚∈[1, 𝑀]\\n(13)\\nFigure 2 (a) illustrates the architecture of AutoDim framework with\\na toy example about the optimal dimension selections based on two\\ncandidate dimensions, where the largest weights corresponding\\nto the 1𝑠𝑡, 𝑚𝑡ℎand 𝑀𝑡ℎfeature fields are 0.7, 0.8 and 0.6, then the\\nembedding space X1\\n1, X2𝑚and X1\\n𝑀are selected for these feature\\nfields. The dimension of an embedding vector in these embedding\\nspaces is 𝑑1, 𝑑2 and 𝑑1, respectively.\\n2.4.2\\nModel Re-training. As shown in Figure 2 (b), given the\\nselected embedding spaces, we can obtain unique embedding vec-\\ntors (x1, · · · , x𝑀) for features (𝑥1, · · · ,𝑥𝑀). Then we concatenate\\nthese embeddings and feeds them into hidden layers. Next, the\\nprediction ˆ𝑦is generated by the output layer. Finally, all the pa-\\nrameters of the DLRS, including embeddings and MLPs, will be up-\\ndated via minimizing the supervised loss function L( ˆ𝑦,𝑦) through\\nback-propagation. The model re-training algorithm is detailed in\\nAlgorithm 2. The re-training process is based on the same training\\ndata as Algorithm 1.\\nNote that the majority of existing deep recommender algorithms\\n(such as FM [32], DeepFM [13], FFM [28], AFM [42], xDeepFM [21])\\ncapture the interactions between feature fields via interaction oper-\\nations, such as inner product and Hadamard product. These inter-\\naction operations require the embedding vectors from all fields to\\nhave the same dimensions. Therefore, the embeddings selected in\\nSection 2.4.1 are still mapped into the same dimension as in Section\\n2.2.2. In the re-training stage, the BatchNorm operation is no longer\\nin use, since there are no comparisons between candidate embed-\\ndings in each field. Unifying embeddings into the same dimension\\ndoes not increase model parameters and computations too much: (i)\\nlinear transformation: all embeddings from one feature field share\\nthe same weight matrice and bias vector, and (ii) zero-padding: no\\nextra trainable parameters are introduced.\\n3\\nEXPERIMENTS\\nIn this section, we first introduce experimental settings. Then we\\nconduct extensive experiments to evaluate the effectiveness of the\\nproposed AutoDim framework. We mainly seek answers to the\\nfollowing research questions: RQ1: How does AutoDim perform\\ncompared with other embedding dimension search methods? RQ2:\\nHow do the important components, i.e., 2 embedding lookup meth-\\nods and 2 transformation methods, influence the performance?\\nTable 1: Statistics of the datasets.\\nData\\nCriteo\\nAvazu\\n# Interactions\\n45,840,617\\n40,428,968\\n# Feature Fields\\n39\\n22\\n# Sparse Features\\n1,086,810\\n2,018,012\\nRQ3: How efficient is AutoDim as compared with other methods?\\nRQ4: What is the impact of important parameters on the results?\\nRQ5: What is the transferability and stability of AutoDim? RQ6:\\nCan AutoDim assign large embedding dimensions to really impor-\\ntant feature fields?\\n3.1\\nDatasets\\nWe evaluate our model on two benchmark datasets: (i) Criteo1:\\nThis is a benchmark industry dataset to evaluate ad click-through\\nrate prediction models. It consists of 45 million users’ click records\\non displayed ads over one month. For each data example, it contains\\n13 numerical feature fields and 26 categorical feature fields. We nor-\\nmalize numerical features by transforming a value 𝑣→\\n\\x04\\nlog(𝑣)2\\x05\\nif 𝑣> 2 as proposed by the Criteo Competition winner 2, and then\\nconvert it into categorical features through bucketing. All 𝑀= 39\\nfeature fields are anonymous. (ii) Avazu3: Avazu dataset was pro-\\nvided for the CTR prediction challenge on Kaggle, which contains\\n11 days’ user clicking behaviors that whether a displayed mobile ad\\nimpression is clicked or not. There are 𝑀= 22 categorical feature\\nfields including user/ad features and device attributes. Parts of the\\nfields are anonymous. Some key statistics of the datasets are shown\\nin Table 1. For each dataset, we use 90% user-item interactions as\\nthe training/validation set (8:1), and the rest 10% as the test set.\\n3.2\\nImplement Details\\nNext, we detail the AutoDim architectures. For the DLRS, (i) em-\\nbedding component: existing work usually set the embedding di-\\nmension as 10 or 16, while recent research found that a larger\\nembedding size leads to better performance [50], so we set the\\nmaximal embedding dimension as 32 within our GPU memory con-\\nstraints. For each feature field, we select from 𝑁= 5 candidate\\nembedding dimensions {2, 8, 16, 24, 32}. (ii) MLP component: we\\nhave two hidden layers with the size |ℎ0| ×128 and 128×128, where\\n|ℎ0| is the input size of first hidden layer, |ℎ0| = 32 × 𝑀with 𝑀the\\nnumber of feature fields for different datasets, and we use batch\\nnormalization, dropout (𝑟𝑎𝑡𝑒= 0.2) and ReLU activation for both\\nhidden layers. The output layer is 128 × 1 with Sigmoid activation.\\nFor architectural weights 𝜶: 𝛼1𝑚, · · · , 𝛼𝑁\\n𝑚of the 𝑚𝑡ℎfeature field\\nare produced by a Softmax activation upon a trainable vector of\\nlength 𝑁. We use an annealing temperature 𝜏= max(0.01, 1 −\\n0.00005 · 𝑡) for Gumbel-softmax, where 𝑡is the training step.\\nThe learning rate for updating DLRS and weights are 0.001 and\\n0.001, and the batch-size is set as 2000. For the parameters of the\\nproposed AutoDim framework, we select them via cross-validation.\\nCorrespondingly, we also do parameter-tuning for baselines for\\n1https://www.kaggle.com/c/criteo-display-ad-challenge/\\n2https://www.csie.ntu.edu.tw/ r01922136/kaggle-2014-criteo.pdf\\n3https://www.kaggle.com/c/avazu-ctr-prediction/\\na fair comparison. We will discuss more details about parameter\\nselection for the proposed framework in the following subsections.\\nOur implementation is based on a public Pytorch for recommen-\\ndation library4, which involves 16 state-of-the-art recommendation\\nalgorithms. Our model is implemented as 3 separate classes/functions,\\nso it is easily to be apply our AutoDim model to these recommenda-\\ntion algorithms. Due to the limited space, we show the performances\\nof applying AutoDim on FM [32], W&D [8] and DeepFM [13].\\n3.3\\nEvaluation Metrics\\nThe performance is evaluated by AUC, Logloss and Params, where\\na higher AUC or a lower Logloss indicates a better recommendation\\nperformance. A lower Params means a fewer embedding parameters.\\nArea Under the ROC Curve (AUC) measures the probability that\\na positive instance will be ranked higher than a randomly chosen\\nnegative one; we introduce Logoss since all methods aim to optimize\\nthe logloss in Equation (10), thus it is natural to utilize Logloss as a\\nstraightforward metric. It is noteworthy that a slightly higher AUC\\nor lower Logloss at 0.001-level is regarded as significant for the\\nCTR prediction task [8, 13]. For an embedding dimension search\\nmodel, the Params metric is the optimal number of embedding\\nparameters selected by this model for the recommender system.\\nWe omit the number of MLP parameters, which only occupy a\\nsmall part of the total model parameters, e.g., ∼0.5% in W&D and\\nDeepFM on Criteo dataset. FM model has no MLP component.\\n3.4\\nOverall Performance (RQ1)\\nWe compare the proposed framework with following embedding\\ndimension search methods: (i) FDE (Full Dimension Embedding):\\nIn this baseline, we assign the same embedding dimensions to all\\nfeature fields. For each feature field, the embedding dimension is set\\nas the maximal size from the candidate set, i.e., 32. (ii) MDE (Mixed\\nDimension Embedding) [11]: This is a heuristic method that assigns\\nhighly-frequent feature values with larger embedding dimensions,\\nvice versa. We enumerate its 16 groups of suggested hyperparame-\\nters settings and report the best one. (iii) DPQ (Differentiable Prod-\\nuct Quantization) [7]: This baseline introduces differentiable quanti-\\nzation techniques from network compression community to compact\\nembeddings. (iv) NIS (Neural Input Search) [19]: This baseline ap-\\nplies reinforcement learning to learn to allocate larger embedding\\nsizes to active feature values, and smaller sizes to inactive ones. (v)\\nMGQE (Multi-granular quantized embeddings) [20]: This baseline\\nis based on DPQ, and further cuts down the embeddings space by us-\\ning fewer centroids for non-frequent feature values. (vi) AutoEmb\\n(Automated Embedding Dimensionality Search) [47]: This baseline\\nis based on DARTS [24], and assigns embedding dimensions accord-\\ning to the frequencies of feature values. (vii) RaS (Random Search):\\nRandom search is strong baseline in neural network search [24].\\nWe apply the same candidate embedding dimensions, randomly\\nallocate dimensions to feature fields in each experiment time, and\\nreport the best performance. (viii) AutoDim-s: This baseline shares\\nthe same architecture with AutoDim, while we update the DLRS\\nparameters and architectural weights simultaneously on the same\\ntraining batch in an end-to-end backpropagation fashion.\\n4https://github.com/rixwew/pytorch-fm\\nTable 2: Performance comparison of different embedding search methods\\nDataset\\nModel\\nMetrics\\nSearch Methods\\nFDE\\nMDE\\nDPQ\\nNIS\\nMGQE\\nAutoEmb\\nRaS\\nAutoDim-s\\nAutoDim\\nCriteo\\nFM\\nAUC\\n0.8020\\n0.8027\\n0.8035\\n0.8042\\n0.8046\\n0.8049\\n0.8056\\n0.8063\\n0.8078*\\nLogloss\\n0.4487\\n0.4481\\n0.4472\\n0.4467\\n0.4462\\n0.4460\\n0.4457\\n0.4452\\n0.4438*\\nParams (M)\\n34.778\\n15.520\\n20.078\\n13.636\\n12.564\\n13.399\\n16.236\\n31.039\\n11.632*\\nCriteo\\nW&D\\nAUC\\n0.8045\\n0.8051\\n0.8058\\n0.8067\\n0.8070\\n0.8072\\n0.8076\\n0.8081\\n0.8098*\\nLogloss\\n0.4468\\n0.4464\\n0.4457\\n0.4452\\n0.4446\\n0.4445\\n0.4443\\n0.4439\\n0.4419*\\nParams (M)\\n34.778\\n18.562\\n22.628\\n14.728\\n15.741\\n15.987\\n18.233\\n30.330\\n12.455*\\nCriteo\\nDeepFM\\nAUC\\n0.8056\\n0.8060\\n0.8067\\n0.8076\\n0.8080\\n0.8082\\n0.8085\\n0.8089\\n0.8101*\\nLogloss\\n0.4457\\n0.4456\\n0.4449\\n0.4442\\n0.4439\\n0.4438\\n0.4436\\n0.4432\\n0.4416*\\nParams (M)\\n34.778\\n17.272\\n25.737\\n12.955\\n13.059\\n13.437\\n17.816\\n31.770\\n11.457*\\nAvazu\\nFM\\nAUC\\n0.7799\\n0.7802\\n0.7809\\n0.7818\\n0.7823\\n0.7825\\n0.7827\\n0.7831\\n0.7842*\\nLogloss\\n0.3805\\n0.3803\\n0.3799\\n0.3792\\n0.3789\\n0.3788\\n0.3787\\n0.3785\\n0.3776*\\nParams (M)\\n64.576\\n22.696\\n28.187\\n22.679\\n22.769\\n21.026\\n27.272\\n55.038\\n17.595*\\nAvazu\\nW&D\\nAUC\\n0.7827\\n0.7829\\n0.7836\\n0.7842\\n0.7849\\n0.7851\\n0.7853\\n0.7856\\n0.7872*\\nLogloss\\n0.3788\\n0.3785\\n0.3777\\n0.3772\\n0.3768\\n0.3767\\n0.3767\\n0.3766\\n0.3756*\\nParams (M)\\n64.576\\n27.976\\n35.558\\n21.413\\n19.457\\n17.292\\n35.126\\n56.401\\n14.130*\\nAvazu\\nDeepFM\\nAUC\\n0.7842\\n0.7845\\n0.7852\\n0.7858\\n0.7863\\n0.7866\\n0.7867\\n0.7870\\n0.7881*\\nLogloss\\n0.3742\\n0.3739\\n0.3737\\n0.3736\\n0.3734\\n0.3733\\n0.3732\\n0.3730\\n0.3721*\\nParams (M)\\n64.576\\n32.972\\n36.128\\n22.550\\n17.575\\n21.605\\n29.235\\n58.325\\n13.976*\\n“*\" indicates the statistically significant improvements (i.e., two-sided t-test with 𝑝< 0.05) over the best baseline. (M=Million)\\nThe overall results are shown in Table 2. We can observe: (1)\\nFDE achieves the worst recommendation performance and largest\\nParams, where FDE is assigned the maximal embedding dimension\\n32 to all feature fields. This result demonstrates that allocating same\\ndimension to all feature fields is not only memory inefficient, but\\nintroduces numerous noises into the model. (2) RaS, AutoDim-s,\\nAutoDim performs better than MDE, DPQ, NIS, MGQE, AutoEmb.\\nThe major differences between these two groups of methods are: (i)\\nthe first group aims to assign different embedding dimensions to\\ndifferent feature fields, while embeddings in the same feature field\\nshare the same dimension; (ii) the second group attempts to assign\\ndifferent embedding sizes to different feature values within the\\nsame feature fields, which are based on the frequencies of feature\\nvalues. The second group of methods surfer from several challenges:\\n(ii-a) there are numerous unique values in each feature field, e.g.,\\n2.7 × 104 values for each feature field on average in Criteo dataset.\\nThis leads to a huge search space (even after bucketing) in each\\nfeature field, which makes it difficult to find the optimal solution,\\nwhile the search space for each feature field is 𝑁= 5 in AutoDim;\\n(ii-b) allocating dimensions solely based on feature frequencies (i.e.,\\nhow many times a feature value appears in the training set) may\\nlose other important characteristics of the feature; (ii-c) the fea-\\nture values frequencies are usually dynamic and not pre-known\\nin real-time recommender system, e.g., the cold-start users/items;\\nand (ii-d) it is difficult to manage embeddings with different di-\\nmensions for the same feature filed. (3) AutoDim outperforms RaS\\nand AutoDim-s, where AutoDim updates the architectural weights\\n𝜶on the validation batch, which can enhance the generalization;\\nAutoDim-s updates the 𝜶with DLRS on the same training batch si-\\nmultaneously, which may lead to overfitting; RaS randomly search\\nthe dimensions, which has a large search space. AutoDim-s has\\nmuch larger Params than AutoDim, which indicates that larger\\ndimensions are more efficient in minimizing training loss.\\nTo sum up, we can draw an answer to the first question: com-\\npared with the representative baselines, AutoDim achieves signifi-\\ncantly better recommendation performance, and saves 70% ∼80%\\nembedding parameters. These results prove the effectiveness of the\\nAutoDim framework.\\n3.5\\nComponent Analysis (RQ2)\\nIn this paper, we propose two embedding lookup methods in Sec-\\ntion 2.2.1 (i.e. separate embeddings v.s. weight-sharing embeddings)\\nand two transformation methods in Section 2.2.2 (i.e. linear transfor-\\nmation v.s. zero-padding transformation). In this section, we inves-\\ntigate their influence on performance. We systematically combine\\nthe corresponding model components by defining the following\\nvariants of AutoDim: (i) AD-1: weight-sharing embeddings and\\nzero-padding transformation; (ii) AD-2: weight-sharing embed-\\ndings and linear transformation; (iii) AD-3: separate embeddings\\nand zero-padding transformation; (iv) AD-4: separate embeddings\\nand linear transformation.\\nThe results of DeepFM on the Criteo dataset are shown in Fig-\\nure 6. We omit similar results on other models/datasets due to\\nthe limited space. We make the following observations: (1) In Fig-\\nure 6 (a), we compare the embedding component parameters of in\\nthe dimension search stage, i.e., all the candidate embeddings and\\nAD-1 AD-2 AD-3 AD-4\\n0\\n20\\n40\\n60\\n80\\n100\\n(a) Search Params (M)\\nAD-1 AD-2 AD-3 AD-4\\n0\\n5\\n10\\n15\\n(b) Train Time (H)\\nAD-1 AD-2 AD-3 AD-4\\n0.800\\n0.805\\n0.810\\n0.815\\n(c) AUC\\nAD-1 AD-2 AD-3 AD-4\\n0.430\\n0.435\\n0.440\\n0.445\\n0.450\\n(d) LogLoss\\nAD-1 AD-2 AD-3 AD-4\\n11.0\\n11.2\\n11.4\\n11.6\\n11.8\\n12.0\\n(e) Params (M)\\nAD-1 AD-2 AD-3 AD-4\\n0\\n30\\n60\\n90\\n120\\n150\\n(f) Infer Time (ms)\\nFigure 6: Component analysis of DeepFM on Criteo dataset. *(f) Infer time is averaged for one batch (batch size = 2,000)\\nthe transformation neural networks shown in Figure 4 or 5. We\\ncan observe that AD-1 and AD-2 save significant model parame-\\nters by introducing the weight-sharing embeddings, which also\\nleads to a faster training speed in Figure 6 (b). Therefore, weight-\\nsharing embeddings can benefit real-world recommenders where\\nexist thousands of feature fields and the computing resources are\\nlimited. (2) Compared with linear transformation, leveraging zero-\\npadding transformation have slightly fewer parameters, and result\\nin slightly faster training speed (e.g., AD-1 v.s. AD-2 in Figure 6\\n(a) and (b)). However, we can observe the final DLRS architecture\\nselected by AD-1 loses to that of AD-2 in Figure 6 (c) AUC and\\n(d) Logloss. The reason is that zero-padding embeddings may lose\\ninformation when conduct inner product. For instance, to compute\\nthe inner product of 𝑎= [𝑎1,𝑎2,𝑎3] and 𝑏= [𝑏1,𝑏2], we first pad 𝑏\\nto 𝑏= [𝑏1,𝑏2, 0], then < 𝑎,𝑏>= 𝑎1 · 𝑏1 + 𝑎2 · 𝑏2 + 𝑎3 · 0, where the\\ninformation 𝑎3 is lost via multiplying 0. Models without element-\\nwise product between embeddings, such as FNN [46], do not suffer\\nfrom this drawback. (3) In Figure 6 (e) and (f), we can observe that\\nthe final embedding dimensions selected by AD-2 save most model\\nparameters and has the fastest inference speed. (4) From Figure 6\\n(c) and (d), variants with weight-sharing embeddings have better\\nperformance than variants using separate embeddings. This is be-\\ncause the relatively front digits of its embedding space are more\\nlikely to be recalled and trained (as shown in Figure 3 (b)), which\\nenable the framework capture more essential information in these\\ndigits, and make optimal dimension assignment selection.\\nIn summary, we can answer the second question: different com-\\nponent has its advantages, such as zero-padding has fastest training\\nspeed and uses least parameters, linear transformation has best per-\\nformances on AUC/Logloss/Params metrics, and has good training\\nspeed and model parameters. If not specified, the results in other\\nsubsections are based on the AD-2, and we use linear transforma-\\ntion for RaS and AutoDim-s in Section 3.4.\\n3.6\\nEfficiency Analysis (RQ3)\\nIn addition to model effectiveness, the training and inference effi-\\nciency are also essential metrics for deploying a recommendation\\nmodel into commercial recommender systems. In this section, we\\ninvestigate the efficiency of applying search methods to DeepFM\\non Criteo dataset (on one Tesla K80 GPU). Similar results on other\\nmodels/dataset are omitted due to the limited space. We illustrate\\nthe results in Figure 7.\\nFDE\\nMDE\\nDPQ\\nNIS\\nMGQE\\nAutoEmb\\nRaS\\nAutoDim-s\\nAutoDim\\n0\\n10\\n20\\n30\\n40\\n50\\n(a) Training Time (h)\\nFDE\\nMDE\\nDPQ\\nNIS\\nMGQE\\nAutoEmb\\nRaS\\nAutoDim-s\\nAutoDim\\n0\\n50\\n100\\n150\\n200\\n(b) Infer Time / Batch (ms)\\nFigure 7: Efficiency analysis of DeepFM on Criteo dataset.\\nFor the training time in Figure 7 (a), we can observe that Au-\\ntoDim and AutoDim-s have fast training speed. As discussed in\\nSection 3.4, the reason is that they have a smaller search space\\nthan other baselines. FDE’s training is fast since we directly set its\\nembedding dimension as 32, i.e., no searching stage, while its recom-\\nmendation performance is worst among all methods in Section 3.4.\\nFor the inference time, which is more crucial when deploying a\\nmodel in commercial recommender systems, AutoDim achieves the\\nleast inference time as shown in Figure 7 (b). This is because the\\nfinal recommendation model selected by AutoDim has the least\\nembedding parameters, i.e., the Params metric.\\nTo summarize, AutoDim can efficiently achieve better perfor-\\nmance, which makes it easier to be launched in real-world recom-\\nmender systems.\\n3.7\\nParameter Analysis (RQ4)\\nIn this section, we investigate how the essential hyper-parameters\\ninfluence model performance. Besides common hyper-parameters\\nof deep recommender systems such as the number of hidden layers\\n(we omit them due to limited space), our model has one particular\\nhyper-parameter, i.e., the frequency to update architectural weights\\n𝜶, referred to as 𝑓. In Algorithm 1, we alternately update DLRS’s\\nparameters on the training data and update 𝜶on the validation\\ndata. In practice, we find that updating 𝜶can be less frequently\\n0\\n5\\n10\\n15\\n20\\nf\\n0.8075\\n0.8100\\n0.8125\\n(a) AUC\\n0\\n5\\n10\\n15\\n20\\nf\\n0.4400\\n0.4425\\n0.4450\\n(b) Logloss\\n0\\n5\\n10\\n15\\n20\\nf\\n10\\n12\\n(c) Params (M)\\n0\\n5\\n10\\n15\\n20\\nf\\n3\\n9\\n15\\n(d) Train Time (H)\\nFigure 8: Parameter analysis of DeepFM on Criteo dataset.\\nthan updating DLRS’s parameters, which apparently reduces lots\\nof computations, and also enhances the performance.\\nTo study the impact of 𝑓, we investigate how DeepFM with Au-\\ntoDim performs on Criteo dataset with the changes of 𝑓, while\\nfixing other parameters. Figure 8 shows the parameter sensitivity\\nresults, where in 𝑥-axis, 𝑓= 𝑖means updating 𝜶once, then updat-\\ning DLRS’s parameters 𝑖times. We can observe that the AutoDim\\nachieves the optimal AUC/Logloss when 𝑓= 10. In other words,\\nupdating 𝜶too frequently/infrequently results in suboptimal per-\\nformance. Figure 8 (d) shows that setting 𝑓= 10 can reduce ∼50%\\ntraining time compared with setting 𝑓= 1.\\nFigure 8 (c) shows that lower 𝑓leads to lower Params, vice versa.\\nThe reason is that AutoDim updates 𝜶by minimizing validation\\nloss, which improves the generalization of model [24, 29]. When\\nupdating 𝜶frequently (e.g., 𝑓= 1), AutoDim tends to select smaller\\nembedding size that has better generalization, while may has under-\\nfitting problem; while when updating 𝜶infrequently (e.g., 𝑓= 20),\\nAutoDim prefers larger embedding sizes that perform better on\\ntraining set, but may lead to over-fitting problem. 𝑓= 10 is a good\\ntrade-off between model performance on training and validation\\nsets. Results of the other models/dataset are similar, we omit them\\nbecause of the limited space.\\n3.8\\nTransferability and Stability (RQ5)\\n3.8.1\\nTransferability of selected dimensions. In this subsection, we\\ninvestigate whether the embedding dimensions selected by a sim-\\nple model (FM with AutoDim, say FM+AD) can be applied to the\\nrepresentative models, such as NFM [14], PNN [30], AutoInt [36],\\nto enhance their recommendation performance. From Section 3.4,\\nwe know the FM+AD can save 70% ∼80% embedding parameters.\\nThe results are shown in Table 3, where \"Model+AD\" means\\nassigning the embedding dimensions selected by FM+AD to this\\nModel. We can observe that the performances of three models are\\nsignificantly improved by applying embedding dimensions selected\\nby FM+AD on two datasets. These observations demonstrate the\\ntransferability of embedding dimensions selected by FM+AD.\\n3.8.2\\nStability of selected dimensions. To study whether the dimen-\\nsions selected by AutoDim are stable, we run the search stage of\\nDeepFM+AutoDim on Criteo dataset with different random seeds.\\nTable 3: Transferability of selected dimensions.\\nModel\\nCriteo\\nAvazu\\nAUC\\nLogloss\\nAUC\\nLogloss\\nNFM\\n0.8018\\n0.4491\\n0.7741\\n0.3846\\nNFM+AD\\n0.8065*\\n0.4451*\\n0.7766*\\n0.3817*\\nIPNN\\n0.8085\\n0.4428\\n0.7855\\n0.3772\\nIPNN+AD\\n0.8112*\\n0.4407*\\n0.7869*\\n0.3761*\\nAutoInt\\n0.8096\\n0.4418\\n0.786\\n0.3763\\nAutoInt+AD\\n0.8116*\\n0.4403*\\n0.7875*\\n0.3756*\\n“*\" indicates the statistically significant improvements (i.e.,\\ntwo-sided t-test with 𝑝< 0.05).\\nThe Pearson correlation of selected dimensions from different seeds\\nis around 0.85, which demonstrates the stability of the selected\\ndimensions.\\nTable 4: Embedding dimensions for Movielens-1m\\nfeature field\\nW&D (one field)\\nAutoDim\\nAUC\\nLogloss\\nDimension\\nmovieId\\n0.7321\\n0.5947\\n8\\nyear\\n0.5763\\n0.6705\\n2\\ngenres\\n0.6312\\n0.6536\\n4\\nuserId\\n0.6857\\n0.6272\\n8\\ngender\\n0.5079\\n0.6812\\n2\\nage\\n0.5245\\n0.6805\\n2\\noccupation\\n0.5264\\n0.6805\\n2\\nzip\\n0.6524\\n0.6443\\n4\\n3.9\\nCase Study (RQ6)\\nIn this section, we investigate whether AutoDim can assign larger\\nembedding dimensions to more important features. Since feature\\nfields are anonymous in Criteo and Avazu, we apply W&D with Au-\\ntoDim on MovieLens-1m dataset 5. MovieLens-1m is a benchmark\\nfor evaluating recommendation algorithms, which contains users’\\nratings on movies. The dataset includes 6,040 users and 3,416 movies\\nwith 1 million user-item interactions. We binarize the ratings into\\na binary classification task, where ratings of 4 and 5 are viewed\\nas positive and the rest as negative. There are 𝑀= 8 categorical\\nfeature fields: movieId, year, genres, userId, gender, age, occupation,\\nzip. Since MovieLens-1m is much smaller than Criteo and Avazu,\\nwe set the candidate embedding dimensions as {2, 4, 8, 16}.\\nTo measure the contribution of a feature field to the final predic-\\ntion, we build a W&D model with only this field, train this model\\nand evaluate it on the test set. A higher AUC and a lower Logloss\\nmeans this feature field is more predictive for the final prediction.\\nThen, we build a comprehensive W&D model incorporating all\\nfeature fields, and apply AutoDim to select the dimensions for all\\nfeature fields. The results are shown in Table 4. It can be observed\\nthat: (1) No feature fields are assigned 16-dimensional embedding\\n5https://grouplens.org/datasets/movielens/1m/\\nspace, which means candidate embedding dimensions {2, 4, 8, 16}\\nare sufficient to cover all possible choices. (2) Compared to the\\nAUC/Logloss of W&D with each feature field, we can find that Au-\\ntoDim assigns larger embedding dimensions to important (highly\\npredictive) feature fields, such as movieId and userId, vice versa.\\n(3) We build a full dimension embedding (FDE) version of W&D,\\nwhere all feature fields are assigned as the maximal dimension 16.\\nIts performances are AUC=0.8077, Logloss=0.5383, while the perfor-\\nmances of W&D with AutoDim are AUC=0.8113, Logloss=0.5242,\\nand it saves 57% embedding parameters.\\nIn short, above observations validates that AutoDim can assign\\nlarger embedding dimensions to more predictive feature fields,\\nwhich significantly enhances model performance and reduce em-\\nbedding parameters.\\n4\\nRELATED WORK\\nIn this section, we will discuss the related works. We summarize\\nthe works related to our research from two perspectives, say, deep\\nrecommender systems and AutoML for neural architecture search.\\nDeep recommender systems have drawn increasing attention\\nfrom both the academia and the industry thanks to its great advan-\\ntages over traditional methods [45]. Various types of deep learning\\napproaches in recommendation are developed. Sedhain et al. [35]\\npresent an AutoEncoder based model named AutoRec. In their work,\\nboth item-based and user-based AutoRec are introduced. They are\\ndesigned to capture the low-dimension feature embeddings of users\\nand items, respectively. Hidasi et al. [15] introduce an RNN based\\nrecommender system named GRU4Rec. In session-based recommen-\\ndation, the model captures the information from items’ transition\\nsequences for prediction. They also design a session-parallel mini-\\nbatches algorithm and a sampling method for output, which make\\nthe training process more efficient. Cheng et al. [8] introduce a\\nWide&Deep framework for both regression and classification tasks.\\nThe framework consists of a wide part, which is a linear model\\nimplemented as one layer of a feed-forward neural network, and a\\ndeep part, which contains multiple perceptron layers to learn ab-\\nstract and deep representations. Guo et al. [13] propose the DeepFM\\nmodel. It combines the factorization machine (FM) and MLP. The\\nidea of it is to use the former to model the lower-order feature\\ninteractions while using the latter to learn the higher-order inter-\\nactions. Wang et al. [40] attempt to utilize CNN to extract visual\\nfeatures to help POI (Point-of-Interest) recommendations. They\\nbuild a PMF based framework that models the interactions between\\nvisual information and latent user/location factors. Chen et al. [6]\\nintroduce hierarchical attention mechanisms into recommendation\\nmodels. They propose a collaborative filtering model with an item-\\nlevel and a component-level attention mechanism. The item-level\\nattention mechanism captures user representations by attending\\nvarious items and the component-level one tries to figure out the\\nmost important features from auxiliary sources for each user. Wang\\net al. [39] propose a generative adversarial network (GAN) based\\ninformation retrieval model, IRGAN, which is applied in the task\\nof recommendation, and also web search and question answering.\\nThe research of AutoML for neural architecture search can be\\ntraced back to NAS [51], which first utilizes an RNN based controller\\nto design neural networks and proposes a reinforcement learning\\nalgorithm to optimize the framework. After that, many endeavors\\nare conducted on reducing the high training cost of NAS. Pham\\net al. [29] propose ENAS, where the controller learns to search a\\nsubgraph from a large computational graph to form an optimal\\nneural network architecture. Brock et al. [3] introduce a framework\\nnamed SMASH, in which a hyper-network is developed to generate\\nweights for sampled networks. DARTS [24] and SNAS [43] formu-\\nlate the problem of network architecture search in a differentiable\\nmanner and solve it using gradient descent. Luo et al. [25] investi-\\ngate representing network architectures as embeddings. Then they\\ndesign a predictor to take the architecture embedding as input to\\npredict its performance. They utilize gradient-based optimization\\nto find an optimal embedding and decode it back to the network\\narchitecture. Some works raise another way of thinking, which is to\\nlimit the search space. The works [4, 23, 31, 48] focus on searching\\nconvolution cells, which are stacked repeatedly to form a convolu-\\ntional neural network. Zoph et al. [52] propose a transfer learning\\nframework called NASNet, which train convolution cells on smaller\\ndatasets and apply them on larger datasets. Tan et al. [37] introduce\\nMNAS. They propose to search hierarchical convolution cell blocks\\nin an independent manner, so that a deep network can be built based\\non them. Mixed Dimension Embedding [11], Differentiable Product\\nQuantization [7], Neural Input Search [9, 18], Multi-granular Quan-\\ntized Embedding [20], and Automated Embedding Dimensionality\\nSearch [47] are designed for tuning the embedding layer of deep\\nrecommender system. But they aim to tune the embedding sizes\\nwithin the same feature field, and we discuss the detailed differences\\nand drawbacks of these models in Section 3.4.\\n5\\nCONCLUSION\\nIn this paper, we propose a novel framework AutoDim, which tar-\\ngets at automatically assigning different embedding dimensions\\nto different feature fields in a data-driven manner. In real-world\\nrecommender systems, due to the huge amounts of feature fields\\nand the highly complex relationships among embedding dimen-\\nsions, feature distributions and neural network architectures, it is\\ndifficult, if possible, to manually allocate different dimensions to\\ndifferent feature fields. Thus, we proposed an AutoML based frame-\\nwork to automatically select from different embedding dimensions.\\nTo be specific, we first provide an end-to-end differentiable model,\\nwhich computes the weights over different dimensions for differ-\\nent feature fields simultaneously in a soft and continuous form,\\nand we propose an AutoML-based optimization algorithm; then\\naccording to the maximal weights, we derive a discrete embedding\\narchitecture, and re-train the DLRS parameters. We evaluate the\\nAutoDim framework with extensive experiments based on widely\\nused benchmark datasets. The results show that our framework can\\nmaintain or achieve slightly better performance with much fewer\\nembedding space demands.\\nREFERENCES\\n[1] Jie Bao, Yu Zheng, David Wilkie, and Mohamed Mokbel. 2015. Recommendations\\nin location-based social networks: a survey. Geoinformatica 19, 3 (2015), 525–565.\\n[2] John S Breese, David Heckerman, and Carl Kadie. 1998. Empirical analysis of\\npredictive algorithms for collaborative filtering. In Proceedings of the Fourteenth\\nconference on Uncertainty in artificial intelligence. Morgan Kaufmann Publishers\\nInc., 43–52.\\n[3] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. 2017. Smash:\\none-shot model architecture search through hypernetworks. arXiv preprint\\narXiv:1708.05344 (2017).\\n[4] Han Cai, Jiacheng Yang, Weinan Zhang, Song Han, and Yong Yu. 2018. Path-\\nlevel network transformation for efficient architecture search. arXiv preprint\\narXiv:1806.02639 (2018).\\n[5] Chia-Hui Chang, Mohammed Kayed, Moheb R Girgis, and Khaled F Shaalan.\\n2006. A survey of web information extraction systems. IEEE transactions on\\nknowledge and data engineering 18, 10 (2006), 1411–1428.\\n[6] Jingyuan Chen, Hanwang Zhang, Xiangnan He, Liqiang Nie, Wei Liu, and Tat-\\nSeng Chua. 2017. Attentive collaborative filtering: Multimedia recommendation\\nwith item-and component-level attention. In Proceedings of the 40th International\\nACM SIGIR conference on Research and Development in Information Retrieval.\\n335–344.\\n[7] Ting Chen, Lala Li, and Yizhou Sun. 2019. Differentiable product quantization\\nfor end-to-end embedding compression. arXiv preprint arXiv:1908.09756 (2019).\\n[8] Heng-Tze Cheng, Levent Koc, Jeremiah Harmsen, Tal Shaked, Tushar Chandra,\\nHrishi Aradhye, Glen Anderson, Greg Corrado, Wei Chai, Mustafa Ispir, et al.\\n2016. Wide & deep learning for recommender systems. In Proceedings of the 1st\\nworkshop on deep learning for recommender systems. ACM, 7–10.\\n[9] Weiyu Cheng, Yanyan Shen, and Linpeng Huang. 2020. Differentiable Neural\\nInput Search for Recommender Systems. arXiv preprint arXiv:2006.04466 (2020).\\n[10] Paul Covington, Jay Adams, and Emre Sargin. 2016. Deep neural networks\\nfor youtube recommendations. In Proceedings of the 10th ACM conference on\\nrecommender systems. 191–198.\\n[11] Antonio Ginart, Maxim Naumov, Dheevatsa Mudigere, Jiyan Yang, and James\\nZou. 2019. Mixed Dimension Embeddings with Application to Memory-Efficient\\nRecommendation Systems. arXiv preprint arXiv:1909.11810 (2019).\\n[12] Emil Julius Gumbel. 1948. Statistical theory of extreme values and some practical\\napplications: a series of lectures. Vol. 33. US Government Printing Office.\\n[13] Huifeng Guo, Ruiming Tang, Yunming Ye, Zhenguo Li, and Xiuqiang He. 2017.\\nDeepFM: a factorization-machine based neural network for CTR prediction. In\\nProceedings of the 26th International Joint Conference on Artificial Intelligence.\\n1725–1731.\\n[14] Xiangnan He and Tat-Seng Chua. 2017. Neural factorization machines for sparse\\npredictive analytics. In Proceedings of the 40th International ACM SIGIR conference\\non Research and Development in Information Retrieval. 355–364.\\n[15] Balázs Hidasi, Alexandros Karatzoglou, Linas Baltrunas, and Domonkos Tikk.\\n2015. Session-based recommendations with recurrent neural networks. arXiv\\npreprint arXiv:1511.06939 (2015).\\n[16] Sergey Ioffe and Christian Szegedy. 2015. Batch Normalization: Accelerating\\nDeep Network Training by Reducing Internal Covariate Shift. In International\\nConference on Machine Learning. 448–456.\\n[17] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization\\nwith gumbel-softmax. arXiv preprint arXiv:1611.01144 (2016).\\n[18] Manas R Joglekar, Cong Li, Jay K Adams, Pranav Khaitan, and Quoc V Le. 2019.\\nNeural input search for large scale recommendation models. arXiv preprint\\narXiv:1907.04471 (2019).\\n[19] Manas R Joglekar, Cong Li, Mei Chen, Taibai Xu, Xiaoming Wang, Jay K Adams,\\nPranav Khaitan, Jiahui Liu, and Quoc V Le. 2020. Neural input search for large\\nscale recommendation models. In Proceedings of the 26th ACM SIGKDD Interna-\\ntional Conference on Knowledge Discovery & Data Mining. 2387–2397.\\n[20] Wang-Cheng Kang, Derek Zhiyuan Cheng, Ting Chen, Xinyang Yi, Dong Lin,\\nLichan Hong, and Ed H Chi. 2020. Learning Multi-granular Quantized Embed-\\ndings for Large-Vocab Categorical Features in Recommender Systems. arXiv\\npreprint arXiv:2002.08530 (2020).\\n[21] Jianxun Lian, Xiaohuan Zhou, Fuzheng Zhang, Zhongxia Chen, Xing Xie, and\\nGuangzhong Sun. 2018. xdeepfm: Combining explicit and implicit feature in-\\nteractions for recommender systems. In Proceedings of the 24th ACM SIGKDD\\nInternational Conference on Knowledge Discovery & Data Mining.\\n[22] Greg Linden, Brent Smith, and Jeremy York. 2003. Amazon. com recommenda-\\ntions: Item-to-item collaborative filtering. IEEE Internet computing 7, 1 (2003),\\n76–80.\\n[23] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li,\\nLi Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. 2018. Progressive\\nneural architecture search. In Proceedings of the European Conference on Computer\\nVision (ECCV). 19–34.\\n[24] Hanxiao Liu, Karen Simonyan, and Yiming Yang. 2018. Darts: Differentiable\\narchitecture search. arXiv preprint arXiv:1806.09055 (2018).\\n[25] Renqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. 2018. Neural\\narchitecture optimization. In Proceedings of the 32nd International Conference on\\nNeural Information Processing Systems. 7827–7838.\\n[26] Raymond J Mooney and Loriene Roy. 2000. Content-based book recommending\\nusing learning for text categorization. In Proceedings of the fifth ACM conference\\non Digital libraries. ACM, 195–204.\\n[27] Hanh TH Nguyen, Martin Wistuba, Josif Grabocka, Lucas Rego Drumond, and\\nLars Schmidt-Thieme. 2017. Personalized Deep Learning for Tag Recommen-\\ndation. In Pacific-Asia Conference on Knowledge Discovery and Data Mining.\\nSpringer.\\n[28] Junwei Pan, Jian Xu, Alfonso Lobos Ruiz, Wenliang Zhao, Shengjun Pan, Yu Sun,\\nand Quan Lu. 2018. Field-weighted factorization machines for click-through\\nrate prediction in display advertising. In Proceedings of the 2018 World Wide Web\\nConference. 1349–1357.\\n[29] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. 2018. Efficient\\nNeural Architecture Search via Parameters Sharing. In International Conference\\non Machine Learning. 4095–4104.\\n[30] Yanru Qu, Han Cai, Kan Ren, Weinan Zhang, Yong Yu, Ying Wen, and Jun Wang.\\n2016. Product-based neural networks for user response prediction. In 2016 IEEE\\n16th International Conference on Data Mining (ICDM). IEEE, 1149–1154.\\n[31] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. 2019. Regularized\\nEvolution for Image Classifier Architecture Search. In Proceedings of the AAAI\\nConference on Artificial Intelligence, Vol. 33. 4780–4789.\\n[32] Steffen Rendle. 2010. Factorization machines. In Data Mining (ICDM), 2010 IEEE\\n10th International Conference on. IEEE, 995–1000.\\n[33] Paul Resnick and Hal R Varian. 1997. Recommender systems. Commun. ACM 40,\\n3 (1997), 56–58.\\n[34] Francesco Ricci, Lior Rokach, and Bracha Shapira. 2011. Introduction to recom-\\nmender systems handbook. In Recommender systems handbook. Springer.\\n[35] Suvash Sedhain, Aditya Krishna Menon, Scott Sanner, and Lexing Xie. 2015.\\nAutorec: Autoencoders meet collaborative filtering. In Proceedings of the 24th\\ninternational conference on World Wide Web. 111–112.\\n[36] Weiping Song, Chence Shi, Zhiping Xiao, Zhijian Duan, Yewen Xu, Ming Zhang,\\nand Jian Tang. 2019. Autoint: Automatic feature interaction learning via self-\\nattentive neural networks. In Proceedings of the 28th ACM International Conference\\non Information and Knowledge Management. 1161–1170.\\n[37] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew\\nHoward, and Quoc V Le. 2019. Mnasnet: Platform-aware neural architecture\\nsearch for mobile. In Proceedings of the IEEE Conference on Computer Vision and\\nPattern Recognition. 2820–2828.\\n[38] Yong Kiam Tan, Xinxing Xu, and Yong Liu. 2016. Improved recurrent neural\\nnetworks for session-based recommendations. In Proceedings of the 1st Workshop\\non Deep Learning for Recommender Systems. 17–22.\\n[39] Jun Wang, Lantao Yu, Weinan Zhang, Yu Gong, Yinghui Xu, Benyou Wang, Peng\\nZhang, and Dell Zhang. 2017. Irgan: A minimax game for unifying generative\\nand discriminative information retrieval models. In Proceedings of the 40th In-\\nternational ACM SIGIR conference on Research and Development in Information\\nRetrieval. 515–524.\\n[40] Suhang Wang, Yilin Wang, Jiliang Tang, Kai Shu, Suhas Ranganath, and Huan Liu.\\n2017. What your images reveal: Exploiting visual contents for point-of-interest\\nrecommendation. In Proceedings of the 26th International Conference on World\\nWide Web. International World Wide Web Conferences Steering Committee,\\n391–400.\\n[41] Sai Wu, Weichao Ren, Chengchao Yu, Gang Chen, Dongxiang Zhang, and Jingbo\\nZhu. 2016. Personal recommendation using deep recurrent neural networks in\\nNetEase. In Data Engineering (ICDE), 2016 IEEE 32nd International Conference on.\\nIEEE, 1218–1229.\\n[42] Jun Xiao, Hao Ye, Xiangnan He, Hanwang Zhang, Fei Wu, and Tat-Seng Chua.\\n2017. Attentional factorization machines: Learning the weight of feature interac-\\ntions via attention networks. arXiv preprint arXiv:1708.04617 (2017).\\n[43] Sirui Xie, Hehui Zheng, Chunxiao Liu, and Liang Lin. 2018. SNAS: stochastic\\nneural architecture search. arXiv preprint arXiv:1812.09926 (2018).\\n[44] Shuai Zhang, Lina Yao, and Aixin Sun. 2017. Deep Learning based Recommender\\nSystem: A Survey and New Perspectives. arXiv preprint arXiv:1707.07435 (2017).\\n[45] Shuai Zhang, Lina Yao, Aixin Sun, and Yi Tay. 2019. Deep learning based rec-\\nommender system: A survey and new perspectives. ACM Computing Surveys\\n(CSUR) 52, 1 (2019), 1–38.\\n[46] Weinan Zhang, Tianming Du, and Jun Wang. 2016. Deep learning over multi-field\\ncategorical data. In European conference on information retrieval. Springer, 45–57.\\n[47] Xiangyu Zhao, Chong Wang, Ming Chen, Xudong Zheng, Xiaobing Liu, and\\nJiliang Tang. 2020. AutoEmb: Automated Embedding Dimensionality Search in\\nStreaming Recommendations. arXiv preprint arXiv:2002.11252 (2020).\\n[48] Zhao Zhong, Junjie Yan, Wei Wu, Jing Shao, and Cheng-Lin Liu. 2018. Practical\\nblock-wise neural network architecture generation. In Proceedings of the IEEE\\nconference on computer vision and pattern recognition.\\n[49] Guorui Zhou, Xiaoqiang Zhu, Chenru Song, Ying Fan, Han Zhu, Xiao Ma, Yanghui\\nYan, Junqi Jin, Han Li, and Kun Gai. 2018. Deep interest network for click-through\\nrate prediction. In Proceedings of the 24th ACM SIGKDD International Conference\\non Knowledge Discovery & Data Mining. 1059–1068.\\n[50] Jieming Zhu, Jinyang Liu, Shuai Yang, Qi Zhang, and Xiuqiang He. 2020. Fux-\\niCTR: An Open Benchmark for Click-Through Rate Prediction. arXiv preprint\\narXiv:2009.05794 (2020).\\n[51] Barret Zoph and Quoc V Le. 2016. Neural architecture search with reinforcement\\nlearning. arXiv preprint arXiv:1611.01578 (2016).\\n[52] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. 2018. Learning\\ntransferable architectures for scalable image recognition. In Proceedings of the\\nIEEE conference on computer vision and pattern recognition. 8697–8710.\\n'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2311.12785'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
