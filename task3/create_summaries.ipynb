{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine several papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "#store the summaries\n",
    "os.makedirs('summaries/'+paper_id,exist_ok=True)\n",
    "def store_summary(content,filename):\n",
    "    path = 'summaries/'+paper_id\n",
    "    with open(path+'/'+filename, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-based NLG Evaluation: Current Status and Challenges'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = get_json('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'full_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_n(papers):\n",
    "    l = [key+':  '+value for key,value in papers.items()]\n",
    "    random.shuffle(l)\n",
    "    context = ('/n/n').join(l)\n",
    "    instruction = 'summarize the following papers:/n'+context+'/n summarize them.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first answer about defensive strategies in basketball is rated as having a score of 3 out of 5. The accuracy of the answer is assessed as 3, indicating that it correctly identifies the World Health Organization (WHO) as the primary organization responsible for global health policies and guidelines. However, the coherence, factuality, and comprehensiveness scores are lower due to some errors in the description of defensive strategies and a lack of detail about the process by which WHO develops and implements these policies.\\n\\nThe second answer regarding attachment styles and romantic relationships is rated as having a score of 4 out of 5. The accuracy of the answer is assessed as 3, indicating that it correctly describes how attachment style influences romantic relationships. However, the coherence, factuality, and comprehensiveness scores are higher due to the clear and detailed explanation of each attachment style and its impact on relationships.\\n\\nThe third question asks about the evolution of machine learning algorithms, deep neural networks, and core AI techniques over the next 10-20 years and what new approaches might fundamentally change the landscape of artificial intelligence research and development. The answer is rated as having a score of 5 out of 5. The accuracy of the answer is assessed as 5, indicating that it correctly describes how advancements in hardware, data availability, and algorithmic innovation will drive progress in machine learning algorithms, deep neural networks, and core AI techniques. Additionally, the answer accurately identifies new approaches such as reinforcement learning, transfer learning, and generative models as likely to revolutionize the field by enabling more sophisticated decision-making, faster learning, and enhanced performance across a range of applications. The coherence, factuality, and comprehensiveness scores are also high due to the clear and detailed explanation of each advancement and its potential impact on AI research and development.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longsummary = summarize_n(data)\n",
    "longsummary\n",
    "#type(random.shuffle([key+':  '+value for key,value in data.items()]))\n",
    "#complete fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(longsummary,'fulltext_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499106"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is the problem that context is too big? \n",
    "l = [key+':  '+value for key,value in data.items()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context) #3'924'350 characters. This is way too big as \n",
    "len(context.split(' ')) # 499'106 words but only 128'000 in theory are allowed. The llm was trained with 8'000 words in the prompt.\n",
    "#yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with only abstracts. Is it short enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [value['title']+':  '+value['abstract'] for value in data.values()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context.split(' ')) \n",
    "#7956 is short enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(summaries):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Write a summary combining the key findings of following texts: \\n {texts}'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This text discusses several research papers related to evaluating and using large language models (LLMs) as judges for various tasks. The first paper explores the use of wider and deeper networks for fairer LLM evaluations, drawing inspiration from deep neural networks and academic paper reviewing. The second paper proposes using strong LLMs as judges for evaluating other LLMs on more open-ended questions and introduces two benchmarks: MT-bench and Chatbot Arena. The third paper proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks.\\n\\nThe first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\" discusses the challenges of evaluating the quality of responses generated by LLMs and proposes a method using an LLM itself to make evaluations through multiple independent evaluations. The authors explore whether deeper and wider networks can lead to fairer evaluations, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts. They construct the largest and most diverse English evaluation benchmark, LLMEval$^2$, and demonstrate that a wider network with two layers performs the best.\\n\\nThe second paper, \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\" addresses the challenges of evaluating LLM-based chat assistants using existing benchmarks and proposes using strong LLMs as judges to evaluate these models on more open-ended questions. The authors examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, and propose solutions to mitigate some of them. They verify the agreement between LLM judges and human preferences using MT-bench and Chatbot Arena and demonstrate that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\\n\\nThe third paper, \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges,\" proposes fine-tuning LLMs as scalable judges to evaluate LLMs efficiently and effectively in open-ended benchmarks. The authors propose a comprehensive, large-scale, high-quality dataset for fine-tuning high-performance judges and introduce techniques such as swap augmentation, reference support, and reference drop to address key biases in fine-tuning LLM as a judge. They demonstrate that JudgeLM obtains the state-of-the-art judge performance on both existing and new benchmarks and is efficient, with JudgeLM-7B only needing 3 minutes to judge 5K samples with 8 A100 GPUs.\\n\\nAll resources for these papers are available at https://github.com/e0397123/comp-analysis.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_summary = write_summary(data)\n",
    "simple_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(simple_summary,'simple_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete Problems in AI Safety\n",
      "PaLM 2 Technical Report\n",
      "Benchmarking Foundation Models with Language-Model-as-an-Examiner\n",
      "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
      "BooookScore: A systematic exploration of book-length summarization in the era of LLMs\n",
      "Scaling Instruction-Finetuned Language Models\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "GPTScore: Evaluate as You Desire\n",
      "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\n",
      "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\n",
      "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\n",
      "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences\n",
      "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks\n",
      "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
      "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\n",
      "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n",
      "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task\n",
      "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics\n",
      "Generative Judge for Evaluating Alignment\n",
      "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation\n",
      "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\n",
      "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\n",
      "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization\n",
      "On Learning to Summarize with Large Language Models as References\n",
      "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response\n",
      "Calibrating LLM-Based Evaluator\n",
      "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models\n",
      "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization\n",
      "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "Self-critiquing models for assisting human evaluators\n",
      "Fusion-Eval: Integrating Assistant Evaluators with LLMs\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Large Language Models are not Fair Evaluators\n",
      "Shepherd: A Critic for Language Model Generation\n",
      "Automated Evaluation of Personalized Text Generation using Large Language Models\n",
      "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\n",
      "Automatic Answerability Evaluation for Question Generation\n",
      "Style Over Substance: Evaluation Biases for Large Language Models\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\n",
      "BatchEval: Towards Human-like Text Evaluation\n",
      "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators\n",
      "Wider and Deeper LLM Networks are Fairer LLM Evaluators\n",
      "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
      "JudgeLM: Fine-tuned Large Language Models are Scalable Judges\n"
     ]
    }
   ],
   "source": [
    "print(('\\n').join([s['title'] for s in data.values()]))\n",
    "#The first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\"... \n",
    "#=> it did only consider the last three papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_topic(summaries,topic):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-based NLG Evaluation: Current Status and Challenges - Wider and Deeper Networks for Fairer Evaluators, LLM-as-a-Judge, and JudgeLM\\n\\nThis topic covers recent research on evaluating Large Language Models (LLMs) in Natural Language Generation (NLG) tasks. The studies explore various approaches to improve the evaluation process, including wider and deeper networks for fairer evaluators, using LLMs as judges, and fine-tuning LLMs as scalable judges.\\n\\n1. Wider and Deeper LLM Networks are Fairer LLM Evaluators:\\nThis paper proposes a novel approach to use the LLM itself to make evaluations and improve fairness by designing a network that resembles academic paper reviewing. The network consists of multiple layers, with each layer receiving representations from all neurons in the previous layer, integrating locally learned evaluation information for more comprehensive results. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers performs best, improving kappa correlation coefficient significantly.\\n\\n2. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena:\\nThis study explores the usage of strong LLMs as judges to evaluate other models on more open-ended questions. The paper discusses position, verbosity, self-enhancement biases, and limited reasoning ability of LLM judges and proposes solutions to mitigate some of these issues. Results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement.\\n\\n3. JudgeLM: Fine-tuned Large Language Models are Scalable Judges:\\nThis research proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. The study trains JudgeLM at different scales, analyzes its capabilities and biases, and introduces techniques to address these issues. Results show that JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks, is efficient, and demonstrates extended capabilities as a judge for single answer, multimodal models, multiple answers, and multi-turn chat.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview = write_summary_with_topic(data,topic)\n",
    "store_summary(simple_overview,'simple_overview.txt')\n",
    "simple_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As well only considers the last 3 papers. Is it because they are the last three papers or because they are in llm's opinion the most relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_shuffle(summaries,topic):\n",
    "    l = [s['title']+':  '+s['abstract'] for s in summaries.values()]\n",
    "    random.shuffle(l)\n",
    "    print('last 3 papers:',l[-3])\n",
    "    print(l[-2])\n",
    "    print(l[-1])\n",
    "    texts = ('\\n').join(l)\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 3 papers: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation:  Large Language Models (LLMs) are frequently used for multi-faceted language\n",
      "generation and evaluation tasks that involve satisfying intricate user\n",
      "constraints or taking into account multiple aspects and criteria. However,\n",
      "their performance can fall short, due to the model's lack of coherence and\n",
      "inability to plan and decompose the problem. We propose Branch-Solve-Merge\n",
      "(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such\n",
      "challenging natural language tasks. It consists of branch, solve, and merge\n",
      "modules that are parameterized with specific prompts to the base LLM. These\n",
      "three modules plan a decomposition of the task into multiple parallel\n",
      "sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.\n",
      "We apply our method to the tasks of LLM response evaluation and constrained\n",
      "text generation and evaluate its effectiveness with multiple LLMs, including\n",
      "Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and\n",
      "consistency for each LLM by enhancing human-LLM agreement by up to 26%,\n",
      "reducing length and pairwise position biases by up to 50%, and allowing\n",
      "LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story\n",
      "generation task, BSM improves the coherence of stories while also improving\n",
      "constraint satisfaction by 12%.\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences:  As a natural language assistant, ChatGPT is capable of performing various\n",
      "tasks, including but not limited to article generation, code completion, and\n",
      "data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\n",
      "level of accuracy and reliability in terms of content evaluation, exhibiting\n",
      "the capability of mimicking human preferences. To further explore ChatGPT's\n",
      "potential in this regard, a study is conducted to assess its ability to rank\n",
      "content. In order to do so, a test set consisting of prompts is created,\n",
      "covering a wide range of use cases, and five models are utilized to generate\n",
      "corresponding responses. ChatGPT is then instructed to rank the responses\n",
      "generated by these models. The results on the test set show that ChatGPT's\n",
      "ranking preferences are consistent with human to a certain extent. This\n",
      "preliminary experimental finding implies that ChatGPT's zero-shot ranking\n",
      "capability could be used to reduce annotation pressure in a number of ranking\n",
      "tasks.\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets:  Evaluation of Large Language Models (LLMs) is challenging because\n",
      "instruction-following necessitates alignment with human values and the required\n",
      "set of skills varies depending on the instruction. However, previous studies\n",
      "have mainly focused on coarse-grained evaluation (i.e. overall preference-based\n",
      "evaluation), which limits interpretability since it does not consider the\n",
      "nature of user instructions that require instance-wise skill composition. In\n",
      "this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\n",
      "Alignment Skill Sets), a fine-grained evaluation protocol for both human-based\n",
      "and model-based evaluation which decomposes coarse-level scoring to a skill\n",
      "set-level scoring for each instruction. We experimentally observe that the\n",
      "fine-graininess of evaluation is crucial for attaining a holistic view of model\n",
      "performance and increasing the reliability of the evaluation. Using FLASK, we\n",
      "compare multiple open-source and proprietary LLMs and observe a high\n",
      "correlation between model-based and human-based evaluations. We publicly\n",
      "release the evaluation data and code implementation at\n",
      "https://github.com/kaistAI/FLASK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Title: Recent Advances in Large Language Model Evaluation: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK\\n\\nIn recent years, there has been a surge of interest in evaluating the performance of large language models (LLMs) in natural language generation (NLG) tasks. In this summary, we will discuss four recent studies that have made significant strides in developing new methods for evaluating LLMs: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK.\\n\\n1. TIGERScore: A Universal Explainable Metric for NLG Tasks\\nTIGERScore is a reference-free metric that evaluates the rationality of explanations generated by LLMs. The study demonstrates that TIGERScore's correlation with human ratings is high and approaches that of GPT-4 evaluators. Human evaluation of the generated explanations showed an accuracy of 70.8%. The researchers believe that TIGERScore shows the potential for building universal explainable metrics to evaluate any NLG task.\\n\\n2. EvalLM: Interactive System for Iteratively Refining Prompts by Evaluating Multiple Outputs on User-Defined Criteria\\nEvalLM is an interactive system designed to help developers refine prompts for LLMs by evaluating multiple outputs based on user-defined criteria. The study shows that EvalLM helps participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions compared to manual evaluation.\\n\\n3. Branch-Solve-Merge: Improving Large Language Model Evaluation and Generation\\nBranch-Solve-Merge (BSM) is a program that uses branch, solve, and merge modules to tackle challenging NLG tasks by planning a decomposition of the task into multiple parallel sub-tasks, independently solving them, and fusing the solutions. The study shows that BSM enhances human-LLM agreement, reduces length and pairwise position biases, and allows LLMs to match or outperform GPT-4 on most domains.\\n\\n4. FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\\nFLASK is a fine-grained evaluation protocol for both human-based and model-based evaluation that decomposes coarse-level scoring to skill set-level scoring for each instruction. The study shows that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, multiple open-source and proprietary LLMs were compared, resulting in high correlation between model-based and human-based evaluations.\\n\\nThese studies represent significant advancements in the field of LLM-based NLG evaluation and demonstrate the potential for developing more effective methods for evaluating and refining LLMs for various applications. For more information on these studies, please visit the project websites provided:\\n\\n* TIGERScore: <https://tiger-ai-lab.github.io/TIGERScore/>\\n* EvalLM: N/A (The study was published in arXiv and does not have a dedicated website)\\n* Branch-Solve-Merge: <https://arxiv.org/abs/2304.13958>\\n* FLASK: <https://github.com/kaistAI/FLASK>\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview_s = write_summary_with_shuffle(data,topic)\n",
    "store_summary(simple_overview_s,'simple_overview_shuffled.txt')\n",
    "simple_overview_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in several steps\n",
    "First outline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline(summaries,topic):\n",
    "    instruction = f'''I am creating a state-of-the-art survey on {topic}.The survey should cover trends, methodologies, key findings, and future directions. \n",
    "    Could you help outline a structure for this paper?'''\n",
    "    info = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    texts = f'''Use following context only {info}. '''\n",
    "    answer = llm.invoke(texts+instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: State-of-the-Art Survey on LLM-Based NLG Evaluation: Trends, Methodologies, Key Findings, and Future Directions\\n\\n1. Introduction\\n* Brief overview of large language models (LLMs) and natural language generation (NLG)\\n* Importance of evaluating LLMs in open-ended scenarios\\n* Motivation for the survey: current challenges and limitations of existing benchmarks and metrics\\n\\n2. Background and Related Work\\n* Overview of LLMs and their applications in NLG\\n* Existing benchmarks and metrics for evaluating LLMs in NLG tasks\\n* Previous surveys on NLG evaluation and their contributions\\n\\n3. Trends in LLM-Based NLG Evaluation\\n* Recent advances in designing and implementing LLMs for NLG tasks\\n* Emerging applications of LLMs in NLG evaluation, such as adversarial perturbations, model ensembles, and fairness\\n\\n4. Methodologies for LLM-Based NLG Evaluation\\n* Approaches to evaluating LLMs in open-ended scenarios, including human evaluation, automatic metrics, and hybrid methods\\n* Challenges and limitations of each methodology and potential solutions\\n\\n5. Key Findings from Recent Studies on LLM-Based NLG Evaluation\\n* Summary of recent studies on evaluating LLMs for NLG tasks using various methodologies and techniques\\n* Comparison of their results, insights, and implications\\n\\n6. Future Directions in LLM-Based NLG Evaluation\\n* Open research questions and potential directions for future work\\n* Implications for the development of more comprehensive and effective evaluation frameworks\\n\\n7. Conclusion\\n* Summary of the survey's findings and contributions\\n* Implications for researchers, practitioners, and policymakers in the field of NLG evaluation using LLMs.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline =outline(data,topic)\n",
    "outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline(topic):\n",
    "    instruction = f'''I am creating a state-of-the-art survey on {topic}.The survey should cover trends, methodologies, key findings, and future directions. \n",
    "    Could you help outline a structure for this paper?'''\n",
    "    answer = llm.invoke(instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-Based NLG Evaluation: Current Status and Challenges\\n\\n1. Introduction\\n   * Brief overview of Natural Language Generation (NLG) and its importance in AI and HCI\\n   * Explanation of the role of Large Language Models (LLMs) in NLG and their recent advancements\\n   * Objective of the survey: to discuss trends, methodologies, key findings, and future directions in LLM-based NLG evaluation\\n\\n2. Trends in LLM-Based NLG Evaluation\\n   * Description of the growing interest in evaluating LLM-based NLG systems\\n   * Overview of popular applications and industries where LLM-based NLG is being used (e.g., customer service, content generation, education)\\n   * Discussion on the increasing availability of large datasets for training and evaluating LLMs\\n\\n3. Methodologies for Evaluating LLM-Based NLG Systems\\n   * Description of various evaluation metrics and techniques used in NLG research, focusing on those specifically designed for LLM-based systems (e.g., BLEU, ROUGE, PER, Human Evaluation)\\n   * Comparison of the strengths and weaknesses of each methodology\\n   * Discussion on the challenges associated with evaluating LLM-based NLG systems, such as data scarcity, lack of ground truth, and subjectivity\\n\\n4. Key Findings in LLM-Based NLG Evaluation\\n   * Summary of recent studies and research findings related to LLM-based NLG evaluation\\n   * Analysis of the trends and patterns observed in these studies (e.g., improvements in performance, challenges faced)\\n   * Discussion on the implications of these findings for the future development of LLM-based NLG systems\\n\\n5. Future Directions in LLM-Based NLG Evaluation\\n   * Identification of open research questions and potential directions for future work (e.g., developing more robust evaluation methodologies, addressing ethical concerns, exploring new applications)\\n   * Discussion on the potential impact of emerging technologies, such as multimodal NLG and conversational AI, on LLM-based NLG evaluation\\n\\n6. Conclusion\\n   * Recap of the main points discussed in the survey\\n   * Reflection on the current state of LLM-based NLG evaluation and its challenges\\n   * Vision for the future of this field and its potential applications and impact on various industries and society as a whole.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline2 =outline(topic)\n",
    "outline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: LLM-Based NLG Evaluation: Current Status and Challenges\n",
      "\n",
      "1. Introduction\n",
      "   * Brief overview of Natural Language Generation (NLG) and its importance in AI and HCI\n",
      "   * Explanation of the role of Large Language Models (LLMs) in NLG and their recent advancements\n",
      "   * Objective of the survey: to discuss trends, methodologies, key findings, and future directions in LLM-based NLG evaluation\n",
      "\n",
      "2. Trends in LLM-Based NLG Evaluation\n",
      "   * Description of the growing interest in evaluating LLM-based NLG systems\n",
      "   * Overview of popular applications and industries where LLM-based NLG is being used (e.g., customer service, content generation, education)\n",
      "   * Discussion on the increasing availability of large datasets for training and evaluating LLMs\n",
      "\n",
      "3. Methodologies for Evaluating LLM-Based NLG Systems\n",
      "   * Description of various evaluation metrics and techniques used in NLG research, focusing on those specifically designed for LLM-based systems (e.g., BLEU, ROUGE, PER, Human Evaluation)\n",
      "   * Comparison of the strengths and weaknesses of each methodology\n",
      "   * Discussion on the challenges associated with evaluating LLM-based NLG systems, such as data scarcity, lack of ground truth, and subjectivity\n",
      "\n",
      "4. Key Findings in LLM-Based NLG Evaluation\n",
      "   * Summary of recent studies and research findings related to LLM-based NLG evaluation\n",
      "   * Analysis of the trends and patterns observed in these studies (e.g., improvements in performance, challenges faced)\n",
      "   * Discussion on the implications of these findings for the future development of LLM-based NLG systems\n",
      "\n",
      "5. Future Directions in LLM-Based NLG Evaluation\n",
      "   * Identification of open research questions and potential directions for future work (e.g., developing more robust evaluation methodologies, addressing ethical concerns, exploring new applications)\n",
      "   * Discussion on the potential impact of emerging technologies, such as multimodal NLG and conversational AI, on LLM-based NLG evaluation\n",
      "\n",
      "6. Conclusion\n",
      "   * Recap of the main points discussed in the survey\n",
      "   * Reflection on the current state of LLM-based NLG evaluation and its challenges\n",
      "   * Vision for the future of this field and its potential applications and impact on various industries and society as a whole.\n"
     ]
    }
   ],
   "source": [
    "print(outline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro(topic,out):\n",
    "    ins = f'''Here is the outline for a state-of-the-art survey on {topic} that I created earlier: {out}\n",
    "    Using this outline, can you draft an introduction that includes the background, significance of the topic, the aim of the survey, and what the reader can expect \n",
    "    from the paper?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: LLM-Based NLG Evaluation: Current Status and Challenges\\n\\nIntroduction:\\n\\nNatural Language Generation (NLG) has emerged as a crucial component in Artificial Intelligence (AI) and Human-Computer Interaction (HCI), enabling machines to generate human-like text based on data or user inputs. One of the most significant advancements in NLG research is the use of Large Language Models (LLMs) to generate natural language responses, leading to impressive improvements in various applications such as customer service, content generation, and education.\\n\\nThe role of LLMs in NLG has gained increasing attention due to their ability to learn from vast amounts of data and generate text that closely resembles human-written language. However, with this growing interest comes the need for rigorous evaluation methods to ensure the quality and effectiveness of these systems. In this survey, we aim to discuss the current trends, methodologies, key findings, and future directions in LLM-based NLG evaluation.\\n\\nThe significance of evaluating LLM-based NLG systems lies in their potential impact on various industries and society as a whole. For instance, in customer service, these systems can help businesses provide personalized responses to customers' queries, enhancing the overall user experience. In content generation, they can be used to create engaging and informative articles or blog posts, while in education, they can assist teachers in generating customized learning materials for students.\\n\\nThe objective of this survey is to provide a comprehensive overview of the current state of LLM-based NLG evaluation, including popular applications, methodologies, key findings, and future directions. We will discuss the growing interest in evaluating these systems, the challenges associated with their assessment, and potential research questions for further exploration. By the end of this paper, readers can expect to gain a deeper understanding of the current trends and future prospects in LLM-based NLG evaluation.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = intro(topic,outline2)\n",
    "intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit(topic,summaries,out):\n",
    "    papers = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    ins=f'''I am working on a survey on {topic}. The outline is as follows: {out}\n",
    "    I have collected the following key papers for the Thematic Overview section:\n",
    "    {papers}\n",
    "    Can you summarize these papers and discuss how they contribute to the state of the art in this area?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" These papers explore different aspects of evaluating large language models (LLMs) for text generation and dialogue systems. Here's a summary of each paper and their contributions:\\n\\n1. BatchEval: The authors propose a new paradigm called BatchEval to address the limitations of current sample-wise evaluation methods, such as sensitivity to prompt design, poor resistance to noise, and inferior ensemble performance with static references. They demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average.\\n\\n2. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators: This study analyzes the effectiveness of LLMs in automatic dialogue evaluation, examining their strengths and limitations. The authors show that strong LLM judges like GPT-4 can match human preferences well, making LLM-as-a-judge a scalable and explainable way to approximate human preferences.\\n\\n3. JudgeLM: The authors propose fine-tuning LLMs as scalable judges (JudgeLM) for evaluating LLMs efficiently and effectively in open-ended benchmarks. They train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors, addressing key biases through various techniques. JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks.\\n\\n4. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: The authors explore using strong LLMs as judges to evaluate chat assistants, addressing the limitations of existing benchmarks in measuring human preferences. They propose solutions for position, verbosity, and self-enhancement biases and show that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\\n\\nThese papers contribute significantly to the state of the art in evaluating LLMs by proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. They demonstrate the potential of using LLMs as judges for various applications and provide insights into improving evaluation techniques.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit = lit(topic,data,outline2)\n",
    "lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These papers explore different aspects of evaluating large language models (LLMs) for text generation and dialogue systems. Here's a summary of each paper and their contributions:\n",
      "\n",
      "1. BatchEval: The authors propose a new paradigm called BatchEval to address the limitations of current sample-wise evaluation methods, such as sensitivity to prompt design, poor resistance to noise, and inferior ensemble performance with static references. They demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average.\n",
      "\n",
      "2. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators: This study analyzes the effectiveness of LLMs in automatic dialogue evaluation, examining their strengths and limitations. The authors show that strong LLM judges like GPT-4 can match human preferences well, making LLM-as-a-judge a scalable and explainable way to approximate human preferences.\n",
      "\n",
      "3. JudgeLM: The authors propose fine-tuning LLMs as scalable judges (JudgeLM) for evaluating LLMs efficiently and effectively in open-ended benchmarks. They train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors, addressing key biases through various techniques. JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks.\n",
      "\n",
      "4. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: The authors explore using strong LLMs as judges to evaluate chat assistants, addressing the limitations of existing benchmarks in measuring human preferences. They propose solutions for position, verbosity, and self-enhancement biases and show that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\n",
      "\n",
      "These papers contribute significantly to the state of the art in evaluating LLMs by proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. They demonstrate the potential of using LLMs as judges for various applications and provide insights into improving evaluation techniques.\n"
     ]
    }
   ],
   "source": [
    "print(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenges(topic,summaries):\n",
    "    papers = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    ins = f'''Hereâ€™s a summary of key papers in the field of {topic}:{papers}\n",
    "    Based on these summaries, can you identify the major trends, common methodologies, and challenges in the current research? What are the gaps that still need to be \n",
    "    addressed?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The major trends in the current research on large language models (LLMs) include:\\n\\n1. Evaluation and benchmarking of LLMs: Researchers are exploring various methods to evaluate the quality of responses generated by LLMs, especially in open-ended scenarios where existing benchmarks may not be comprehensive. This includes using LLMs themselves as evaluators, fine-tuning LLMs as judges, and constructing large and diverse evaluation datasets.\\n2. Robustness and adversarial perturbations: Researchers are investigating the robustness of LLMs in handling various types of adversarial perturbations at both turn and dialogue levels. This includes probing their ability to handle misinformation, biased data, and other forms of adversarial attacks.\\n3. Fairness and bias: There is a growing interest in understanding the fairness and potential biases in LLMs, particularly in areas such as language generation and evaluation. Researchers are exploring methods to mitigate these biases and ensure that LLMs generate responses that align with human preferences.\\n4. Scalability and efficiency: As LLMs become more complex and capable, there is a need to develop scalable and efficient methods for evaluating and fine-tuning them. This includes developing large-scale datasets, using distributed computing resources, and exploring techniques such as transfer learning and model compression.\\n\\nCommon methodologies include the use of deep neural networks, adversarial attacks, and crowdsourcing for data collection and evaluation. Challenges in the current research include the lack of comprehensive evaluation benchmarks, the need to address biases and fairness issues, and the scalability and efficiency of evaluating and fine-tuning LLMs.\\n\\nGaps that still need to be addressed include developing more comprehensive and diverse evaluation datasets, exploring methods to mitigate biases and ensure fairness in LLMs, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs in open-ended scenarios. Additionally, there is a need to explore the ethical implications of using LLMs as judges and evaluators, particularly in areas such as legal decision-making and academic paper reviewing.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges  = challenges(topic,data)\n",
    "challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(topic,intro,l,chall):\n",
    "    ins = f'''Here we have the topic: {topic}. The introduction {intro}. The review {l} and the challenges {chall}.\n",
    "    Can you draft a conclusion that summarizes the key findings and discusses the overall state of the field and future outlook?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Conclusion:\\n\\nIn this survey, we have explored the current trends, methodologies, key findings, and challenges in evaluating Large Language Models (LLMs) for Natural Language Generation (NLG). The growing interest in LLM-based NLG systems is driven by their potential impact on various industries and society as a whole. However, with this increasing attention comes the need for rigorous evaluation methods to ensure their quality and effectiveness.\\n\\nThe papers reviewed in this survey demonstrate significant progress in evaluating LLMs for text generation and dialogue systems. They propose new methods such as BatchEval, JudgeLM, and using LLMs as judges, analyze their strengths and limitations, and address challenges related to open-ended scenarios and measuring human preferences. These contributions provide valuable insights into the potential of using LLMs as evaluators and judges for various applications.\\n\\nThe current research on LLMs is focused on evaluation and benchmarking, robustness and adversarial perturbations, fairness and bias, and scalability and efficiency. Common methodologies include deep neural networks, adversarial attacks, and crowdsourcing. Challenges in the field include the lack of comprehensive evaluation benchmarks, addressing biases and fairness issues, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs.\\n\\nGaps that still need to be addressed include creating more comprehensive and diverse evaluation datasets, exploring methods to mitigate biases and ensure fairness in LLMs, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs in open-ended scenarios. Additionally, there is a need to explore the ethical implications of using LLMs as judges and evaluators, particularly in areas such as legal decision-making and academic paper reviewing.\\n\\nIn conclusion, the field of LLM-based NLG evaluation is rapidly advancing, with significant progress being made in proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. However, there are still gaps that need to be addressed, including creating more comprehensive evaluation datasets, exploring methods to mitigate biases and ensure fairness, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs. The future outlook is promising, with the potential for LLMs to revolutionize various industries and society as a whole, but it is crucial that we continue to invest in rigorous evaluation methods to ensure their quality and effectiveness.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = conclusion(topic,intro,lit,challenges)\n",
    "con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conmbine(intro,lit,challenges,con,out):\n",
    "    ins = f''' Based on the introduction {intro}, the review {lit}, the challenges {challenges} and the conclusion {con}. \n",
    "    Combine it to a fluent paper with the outline {out}.''' \n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_survey  = conmbine(intro,lit,challenges,con,outline2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(combined_survey,'combined_survey.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = intro+'/n'+lit+'/n'+challenges+'/n'+con\n",
    "store_summary(combine,'combine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
