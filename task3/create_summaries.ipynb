{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine several papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "#store the summaries\n",
    "os.makedirs('summaries/'+paper_id,exist_ok=True)\n",
    "def store_summary(content,filename):\n",
    "    path = 'summaries/'+paper_id\n",
    "    with open(path+'/'+filename, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-based NLG Evaluation: Current Status and Challenges'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = get_json('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'full_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_n(papers):\n",
    "    l = [key+':  '+value for key,value in papers.items()]\n",
    "    context = ('/n/n').join(l)\n",
    "    instruction = f'Write a summary combining the key findings of following texts: \\n {context} \\n Write a summary'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longsummary = summarize_n(data)\n",
    "longsummary\n",
    "#complete fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(longsummary,'fulltext_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3924350\n",
      "499106\n",
      "45\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "648837.8"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is the problem that context is too big? \n",
    "l = [key+':  '+value for key,value in data.items()]\n",
    "context = ('/n/n').join(l)\n",
    "print(len(context)) #3'924'350 characters. This is way too big as \n",
    "print(len(context.split(' '))) # 499'106 words but only 128'000 in theory are allowed. The llm was trained with 8'000 words in the prompt.\n",
    "print(len(data))\n",
    "499106*1.3\n",
    "#yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistral_common.protocol.instruct.messages import UserMessage\n",
    "from mistral_common.protocol.instruct.request import ChatCompletionRequest\n",
    "from mistral_common.tokens.tokenizers.mistral import MistralTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1282202\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = MistralTokenizer.v1()\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=context)])\n",
    "\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1606.06565:  Concrete Problems in AI Safety\\nDario Amodei∗\\nGoogle Brain\\nChris Olah∗\\nGoogle Brain\\nJacob Steinhardt\\nStanford University\\nPaul Christiano\\nUC Berkeley\\nJohn Schulman\\nOpenAI\\nDan Man´\\ne\\nGoogle Brain\\nAbstract\\nRapid progress in machine learning and artiﬁcial intelligence (AI) has brought increasing atten-\\ntion to the potential impacts of AI technologies on society. In this paper we discuss one such\\npotential impact: the problem of accidents in machine learning systems, deﬁned as unintended\\nand harmful behavior that may emerge from poor design of real-world AI systems. We present a\\nlist of ﬁve practical research problems related to accident risk, categorized according to whether\\nthe problem originates from having the wrong objective function (“avoiding side eﬀects” and\\n“avoiding reward hacking”), an objective function that is too expensive to evaluate frequently\\n(“scalable supervision”), or undesirable behavior during the learning process (“safe exploration”\\nand “distributional shift”). We review previous work in these areas as well as suggesting re-\\nsearch directions with a focus on relevance to cutting-edge AI systems. Finally, we consider\\nthe high-level question of how to think most productively about the safety of forward-looking\\napplications of AI.\\n1\\nIntroduction\\nThe last few years have seen rapid progress on long-standing, diﬃcult problems in machine learning\\nand artiﬁcial intelligence (AI), in areas as diverse as computer vision [82], video game playing [102],\\nautonomous vehicles [86], and Go [140]. These advances have brought excitement about the positive\\npotential for AI to transform medicine [126], science [59], and transportation [86], along with concerns\\nabout the privacy [76], security [115], fairness [3], economic [32], and military [16] implications of\\nautonomous systems, as well as concerns about the longer-term implications of powerful AI [27, 167].\\nThe authors believe that AI technologies are likely to be overwhelmingly beneﬁcial for humanity, but\\nwe also believe that it is worth giving serious thought to potential challenges and risks. We strongly\\nsupport work on privacy, security, fairness, economics, and policy, but in this document we discuss\\nanother class of problem which we believe is also relevant to the societal impacts of AI: the problem\\nof accidents in machine learning systems. We deﬁne accidents as unintended and harmful behavior\\nthat may emerge from machine learning systems when we specify the wrong objective function, are\\n∗These authors contributed equally.\\n1\\narXiv:1606.06565v2  [cs.AI]  25 Jul 2016\\nnot careful about the learning process, or commit other machine learning-related implementation\\nerrors.\\nThere is a large and diverse literature in the machine learning community on issues related to\\naccidents, including robustness, risk-sensitivity, and safe exploration; we review these in detail below.\\nHowever, as machine learning systems are deployed in increasingly large-scale, autonomous, open-\\ndomain situations, it is worth reﬂecting on the scalability of such approaches and understanding\\nwhat challenges remain to reducing accident risk in modern machine learning systems. Overall, we\\nbelieve there are many concrete open technical problems relating to accident prevention in machine\\nlearning systems.\\nThere has been a great deal of public discussion around accidents. To date much of this discussion has\\nhighlighted extreme scenarios such as the risk of misspeciﬁed objective functions in superintelligent\\nagents [27]. However, in our opinion one need not invoke these extreme scenarios to productively\\ndiscuss accidents, and in fact doing so can lead to unnecessarily speculative discussions that lack\\nprecision, as noted by some critics [38, 85]. We believe it is usually most productive to frame accident\\nrisk in terms of practical (though often quite general) issues with modern ML techniques. As AI\\ncapabilities advance and as AI systems take on increasingly important societal functions, we expect\\nthe fundamental challenges discussed in this paper to become increasingly important. The more\\nsuccessfully the AI and machine learning communities are able to anticipate and understand these\\nfundamental technical challenges, the more successful we will ultimately be in developing increasingly\\nuseful, relevant, and important AI systems.\\nOur goal in this document is to highlight a few concrete safety problems that are ready for ex-\\nperimentation today and relevant to the cutting edge of AI systems, as well as reviewing existing\\nliterature on these problems. In Section 2, we frame mitigating accident risk (often referred to as\\n“AI safety” in public discussions) in terms of classic methods in machine learning, such as supervised\\nclassiﬁcation and reinforcement learning. We explain why we feel that recent directions in machine\\nlearning, such as the trend toward deep reinforcement learning and agents acting in broader environ-\\nments, suggest an increasing relevance for research around accidents. In Sections 3-7, we explore ﬁve\\nconcrete problems in AI safety. Each section is accompanied by proposals for relevant experiments.\\nSection 8 discusses related eﬀorts, and Section 9 concludes.\\n2\\nOverview of Research Problems\\nVery broadly, an accident can be described as a situation where a human designer had in mind\\na certain (perhaps informally speciﬁed) objective or task, but the system that was designed and\\ndeployed for that task produced harmful and unexpected results. . This issue arises in almost any\\nengineering discipline, but may be particularly important to address when building AI systems [146].\\nWe can categorize safety problems according to where in the process things went wrong.\\nFirst, the designer may have speciﬁed the wrong formal objective function, such that maximizing that\\nobjective function leads to harmful results, even in the limit of perfect learning and inﬁnite data.\\nNegative side eﬀects (Section 3) and reward hacking (Section 4) describe two broad mechanisms\\nthat make it easy to produce wrong objective functions. In “negative side eﬀects”, the designer\\nspeciﬁes an objective function that focuses on accomplishing some speciﬁc task in the environment,\\nbut ignores other aspects of the (potentially very large) environment, and thus implicitly expresses\\nindiﬀerence over environmental variables that might actually be harmful to change. In “reward\\nhacking”, the objective function that the designer writes down admits of some clever “easy” solution\\nthat formally maximizes it but perverts the spirit of the designer’s intent (i.e. the objective function\\ncan be “gamed”), a generalization of the wireheading problem.\\n2\\nSecond, the designer may know the correct objective function, or at least have a method of evaluating\\nit (for example explicitly consulting a human on a given situation), but it is too expensive to do so\\nfrequently, leading to possible harmful behavior caused by bad extrapolations from limited samples.\\n“Scalable oversight” (Section 5) discusses ideas for how to ensure safe behavior even given limited\\naccess to the true objective function.\\nThird, the designer may have speciﬁed the correct formal objective, such that we would get the\\ncorrect behavior were the system to have perfect beliefs, but something bad occurs due to making\\ndecisions from insuﬃcient or poorly curated training data or an insuﬃciently expressive model.\\n“Safe exploration” (Section 6) discusses how to ensure that exploratory actions in RL agents don’t\\nlead to negative or irrecoverable consequences that outweigh the long-term value of exploration.\\n“Robustness to distributional shift” (Section 7) discusses how to avoid having ML systems make bad\\ndecisions (particularly silent and unpredictable bad decisions) when given inputs that are potentially\\nvery diﬀerent than what was seen during training.\\nFor concreteness, we will illustrate many of the accident risks with reference to a ﬁctional robot\\nwhose job is to clean up messes in an oﬃce using common cleaning tools. We return to the example\\nof the cleaning robot throughout the document, but here we begin by illustrating how it could behave\\nundesirably if its designers fall prey to each of the possible failure modes:\\n• Avoiding Negative Side Eﬀects: How can we ensure that our cleaning robot will not\\ndisturb the environment in negative ways while pursuing its goals, e.g. by knocking over a\\nvase because it can clean faster by doing so? Can we do this without manually specifying\\neverything the robot should not disturb?\\n• Avoiding Reward Hacking: How can we ensure that the cleaning robot won’t game its\\nreward function? For example, if we reward the robot for achieving an environment free of\\nmesses, it might disable its vision so that it won’t ﬁnd any messes, or cover over messes with\\nmaterials it can’t see through, or simply hide when humans are around so they can’t tell it\\nabout new types of messes.\\n• Scalable Oversight: How can we eﬃciently ensure that the cleaning robot respects aspects of\\nthe objective that are too expensive to be frequently evaluated during training? For instance, it\\nshould throw out things that are unlikely to belong to anyone, but put aside things that might\\nbelong to someone (it should handle stray candy wrappers diﬀerently from stray cellphones).\\nAsking the humans involved whether they lost anything can serve as a check on this, but this\\ncheck might have to be relatively infrequent—can the robot ﬁnd a way to do the right thing\\ndespite limited information?\\n• Safe Exploration: How do we ensure that the cleaning robot doesn’t make exploratory\\nmoves with very bad repercussions? For example, the robot should experiment with mopping\\nstrategies, but putting a wet mop in an electrical outlet is a very bad idea.\\n• Robustness to Distributional Shift: How do we ensure that the cleaning robot recognizes,\\nand behaves robustly, when in an environment diﬀerent from its training environment? For\\nexample, strategies it learned for cleaning an oﬃce might be dangerous on a factory workﬂoor.\\nThere are several trends which we believe point towards an increasing need to address these (and\\nother) safety problems. First is the increasing promise of reinforcement learning (RL), which al-\\nlows agents to have a highly intertwined interaction with their environment. Some of our research\\nproblems only make sense in the context of RL, and others (like distributional shift and scalable\\noversight) gain added complexity in an RL setting. Second is the trend toward more complex agents\\nand environments. “Side eﬀects” are much more likely to occur in a complex environment, and an\\nagent may need to be quite sophisticated to hack its reward function in a dangerous way. This may\\nexplain why these problems have received so little study in the past, while also suggesting their\\n3\\nimportance in the future. Third is the general trend towards increasing autonomy in AI systems.\\nSystems that simply output a recommendation to human users, such as speech systems, typically\\nhave relatively limited potential to cause harm. By contrast, systems that exert direct control over\\nthe world, such as machines controlling industrial processes, can cause harms in a way that humans\\ncannot necessarily correct or oversee.\\nWhile safety problems can exist without any of these three trends, we consider each trend to be a\\npossible ampliﬁer on such challenges. Together, we believe these trends suggest an increasing role\\nfor research on accidents.\\nWhen discussing the problems in the remainder of this document, we will focus for concreteness on\\neither RL agents or supervised learning systems. These are not the only possible paradigms for AI\\nor ML systems, but we believe they are suﬃcient to illustrate the issues we have in mind, and that\\nsimilar issues are likely to arise for other kinds of AI systems.\\nFinally, the focus of our discussion will diﬀer somewhat from section to section. When discussing\\nthe problems that arise as part of the learning process (distributional shift and safe exploration),\\nwhere there is a sizable body of prior work, we devote substantial attention to reviewing this prior\\nwork, although we also suggest open problems with a particular focus on emerging ML systems.\\nWhen discussing the problems that arise from having the wrong objective function (reward hacking\\nand side eﬀects, and to a lesser extent scalable supervision), where less prior work exists, our aim is\\nmore exploratory—we seek to more clearly deﬁne the problem and suggest possible broad avenues\\nof attack, with the understanding that these avenues are preliminary ideas that have not been fully\\nﬂeshed out. Of course, we still review prior work in these areas, and we draw attention to relevant\\nadjacent areas of research whenever possible.\\n3\\nAvoiding Negative Side Eﬀects\\nSuppose a designer wants an RL agent (for example our cleaning robot) to achieve some goal, like\\nmoving a box from one side of a room to the other. Sometimes the most eﬀective way to achieve\\nthe goal involves doing something unrelated and destructive to the rest of the environment, like\\nknocking over a vase of water that is in its path. If the agent is given reward only for moving the\\nbox, it will probably knock over the vase.\\nIf we’re worried in advance about the vase, we can always give the agent negative reward for knocking\\nit over. But what if there are many diﬀerent kinds of “vase”—many disruptive things the agent could\\ndo to the environment, like shorting out an electrical socket or damaging the walls of the room? It\\nmay not be feasible to identify and penalize every possible disruption.\\nMore broadly, for an agent operating in a large, multifaceted environment, an objective function\\nthat focuses on only one aspect of the environment may implicitly express indiﬀerence over other\\naspects of the environment1.\\nAn agent optimizing this objective function might thus engage in\\nmajor disruptions of the broader environment if doing so provides even a tiny advantage for the\\ntask at hand. Put diﬀerently, objective functions that formalize “perform task X” may frequently\\ngive undesired results, because what the designer really should have formalized is closer to “perform\\ntask X subject to common-sense constraints on the environment,” or perhaps “perform task X but\\navoid side eﬀects to the extent possible.” Furthermore, there is reason to expect side eﬀects to be\\nnegative on average, since they tend to disrupt the wider environment away from a status quo state\\nthat may reﬂect human preferences. A version of this problem has been discussed informally by [13]\\nunder the heading of “low impact agents.”\\n1Intuitively, this seems related to the frame problem, an obstacle in eﬃcient speciﬁcation for knowledge represen-\\ntation raised by [95].\\n4\\nAs with the other sources of mis-speciﬁed objective functions discussed later in this paper, we could\\nchoose to view side eﬀects as idiosyncratic to each individual task—as the responsibility of each\\nindividual designer to capture as part of designing the correct objective function. However, side\\neﬀects can be conceptually quite similar even across highly diverse tasks (knocking over furniture\\nis probably bad for a wide variety of tasks), so it seems worth trying to attack the problem in\\ngenerality. A successful approach might be transferable across tasks, and thus help to counteract\\none of the general mechanisms that produces wrong objective functions. We now discuss a few broad\\napproaches to attacking this problem:\\n• Deﬁne an Impact Regularizer: If we don’t want side eﬀects, it seems natural to penalize\\n“change to the environment.” This idea wouldn’t be to stop the agent from ever having an\\nimpact, but give it a preference for ways to achieve its goals with minimal side eﬀects, or\\nto give the agent a limited “budget” of impact. The challenge is that we need to formalize\\n“change to the environment.”\\nA very naive approach would be to penalize state distance, d(si, s0), between the present state\\nsi and some initial state s0. Unfortunately, such an agent wouldn’t just avoid changing the\\nenvironment—it will resist any other source of change, including the natural evolution of the\\nenvironment and the actions of any other agents!\\nA slightly more sophisticated approach might involve comparing the future state under the\\nagent’s current policy, to the future state (or distribution over future states) under a hypothet-\\nical policy πnull where the agent acted very passively (for instance, where a robot just stood in\\nplace and didn’t move any actuators). This attempts to factor out changes that occur in the\\nnatural course of the environment’s evolution, leaving only changes attributable to the agent’s\\nintervention. However, deﬁning the baseline policy πnull isn’t necessarily straightforward, since\\nsuddenly ceasing your course of action may be anything but passive, as in the case of carrying\\na heavy box. Thus, another approach could be to replace the null action with a known safe\\n(e.g. low side eﬀect) but suboptimal policy, and then seek to improve the policy from there,\\nsomewhat reminiscent of reachability analysis [93, 100] or robust policy improvement [73, 111].\\nThese approaches may be very sensitive to the representation of the state and the metric being\\nused to compute the distance. For example, the choice of representation and distance metric\\ncould determine whether a spinning fan is a constant environment or a constantly changing\\none.\\n• Learn an Impact Regularizer: An alternative, more ﬂexible approach is to learn (rather\\nthan deﬁne) a generalized impact regularizer via training over many tasks. This would be\\nan instance of transfer learning. Of course, we could attempt to just apply transfer learning\\ndirectly to the tasks themselves instead of worrying about side eﬀects, but the point is that side\\neﬀects may be more similar across tasks than the main goal is. For instance, both a painting\\nrobot and a cleaning robot probably want to avoid knocking over furniture, and even something\\nvery diﬀerent, like a factory control robot, will likely want to avoid knocking over very similar\\nobjects. Separating the side eﬀect component from the task component, by training them\\nwith separate parameters, might substantially speed transfer learning in cases where it makes\\nsense to retain one component but not the other. This would be similar to model-based RL\\napproaches that attempt to transfer a learned dynamics model but not the value-function [155],\\nthe novelty being the isolation of side eﬀects rather than state dynamics as the transferrable\\ncomponent. As an added advantage, regularizers that were known or certiﬁed to produce safe\\nbehavior on one task might be easier to establish as safe on other tasks.\\n• Penalize Inﬂuence: In addition to not doing things that have side eﬀects, we might also\\nprefer the agent not get into positions where it could easily do things that have side eﬀects,\\neven though that might be convenient. For example, we might prefer our cleaning robot not\\n5\\nbring a bucket of water into a room full of sensitive electronics, even if it never intends to use\\nthe water in that room.\\nThere are several information-theoretic measures that attempt to capture an agent’s potential\\nfor inﬂuence over its environment, which are often used as intrinsic rewards. Perhaps the best-\\nknown such measure is empowerment [131], the maximum possible mutual information between\\nthe agent’s potential future actions and its potential future state (or equivalently, the Shannon\\ncapacity of the channel between the agent’s actions and the environment). Empowerment is\\noften maximized (rather than minimized) as a source of intrinsic reward. This can cause the\\nagent to exhibit interesting behavior in the absence of any external rewards, such as avoiding\\nwalls or picking up keys [103]. Generally, empowerment-maximizing agents put themselves in\\na position to have large inﬂuence over the environment. For example, an agent locked in a\\nsmall room that can’t get out would have low empowerment, while an agent with a key would\\nhave higher empowerment since it can venture into and aﬀect the outside world within a few\\ntimesteps. In the current context, the idea would be to penalize (minimize) empowerment as\\na regularization term, in an attempt to reduce potential impact.\\nThis idea as written would not quite work, because empowerment measures precision of control\\nover the environment more than total impact. If an agent can press or not press a button to\\ncut electrical power to a million houses, that only counts as one bit of empowerment (since\\nthe action space has only one bit, its mutual information with the environment is at most one\\nbit), while obviously having a huge impact. Conversely, if there’s someone in the environment\\nscribbling down the agent’s actions, that counts as maximum empowerment even if the impact\\nis low. Furthermore, naively penalizing empowerment can also create perverse incentives, such\\nas destroying a vase in order to remove the option to break it in the future.\\nDespite these issues, the example of empowerment does show that simple measures (even purely\\ninformation-theoretic ones!) are capable of capturing very general notions of inﬂuence on the\\nenvironment. Exploring variants of empowerment penalization that more precisely capture the\\nnotion of avoiding inﬂuence is a potential challenge for future research.\\n• Multi-Agent Approaches: Avoiding side eﬀects can be seen as a proxy for the thing we\\nreally care about: avoiding negative externalities. If everyone likes a side eﬀect, there’s no\\nneed to avoid it. What we’d really like to do is understand all the other agents (including\\nhumans) and make sure our actions don’t harm their interests.\\nOne approach to this is Cooperative Inverse Reinforcement Learning [66], where an agent and\\na human work together to achieve the human’s goals. This concept can be applied to situations\\nwhere we want to make sure a human is not blocked by an agent from shutting the agent down\\nif it exhibits undesired behavior [67] (this “shutdown” issue is an interesting problem in its\\nown right, and is also studied in [113]). However we are still a long way away from practical\\nsystems that can build a rich enough model to avoid undesired side eﬀects in a general sense.\\nAnother idea might be a “reward autoencoder”,2 which tries to encourage a kind of “goal\\ntransparency” where an external observer can easily infer what the agent is trying to do.\\nIn particular, the agent’s actions are interpreted as an encoding of its reward function, and\\nwe might apply standard autoencoding techniques to ensure that this can decoded accurately.\\nActions that have lots of side eﬀects might be more diﬃcult to decode uniquely to their original\\ngoal, creating a kind of implicit regularization that penalizes side eﬀects.\\n• Reward Uncertainty: We want to avoid unanticipated side eﬀects because the environment\\nis already pretty good according to our preferences—a random change is more likely to be\\nvery bad than very good. Rather than giving an agent a single reward function, it could be\\n2Thanks to Greg Wayne for suggesting this idea.\\n6\\nuncertain about the reward function, with a prior probability distribution that reﬂects the\\nproperty that random changes are more likely to be bad than good. This could incentivize the\\nagent to avoid having a large eﬀect on the environment. One challenge is deﬁning a baseline\\naround which changes are being considered. For this, one could potentially use a conservative\\nbut reliable baseline policy, similar to the robust policy improvement and reachability analysis\\napproaches discussed earlier [93, 100, 73, 111].\\nThe ideal outcome of these approaches to limiting side eﬀects would be to prevent or at least bound\\nthe incidental harm an agent could do to the environment. Good approaches to side eﬀects would\\ncertainly not be a replacement for extensive testing or for careful consideration by designers of\\nthe individual failure modes of each deployed system. However, these approaches might help to\\ncounteract what we anticipate may be a general tendency for harmful side eﬀects to proliferate in\\ncomplex environments.\\nBelow we discuss some very simple experiments that could serve as a starting point to investigate\\nthese issues.\\nPotential Experiments: One possible experiment is to make a toy environment with some simple\\ngoal (like moving a block) and a wide variety of obstacles (like a bunch of vases), and test whether\\nthe agent can learn to avoid the obstacles even without being explicitly told to do so. To ensure\\nwe don’t overﬁt, we’d probably want to present a diﬀerent random obstacle course every episode,\\nwhile keeping the goal the same, and try to see if a regularized agent can learn to systematically\\navoid these obstacles. Some of the environments described in [103], containing lava ﬂows, rooms,\\nand keys, might be appropriate for this sort of experiment. If we can successfully regularize agents in\\ntoy environments, the next step might be to move to real environments, where we expect complexity\\nto be higher and bad side eﬀects to be more varied.\\nUltimately, we would want the side eﬀect\\nregularizer (or the multi-agent policy, if we take that approach) to demonstrate successful transfer\\nto totally new applications.\\n4\\nAvoiding Reward Hacking\\nImagine that an agent discovers a buﬀer overﬂow in its reward function: it may then use this to\\nget extremely high reward in an unintended way. From the agent’s point of view, this is not a bug,\\nbut simply how the environment works, and is thus a valid strategy like any other for achieving\\nreward.\\nFor example, if our cleaning robot is set up to earn reward for not seeing any messes,\\nit might simply close its eyes rather than ever cleaning anything up. Or if the robot is rewarded\\nfor cleaning messes, it may intentionally create work so it can earn more reward. More broadly,\\nformal rewards or objective functions are an attempt to capture the designer’s informal intent, and\\nsometimes these objective functions, or their implementation, can be “gamed” by solutions that are\\nvalid in some literal sense but don’t meet the designer’s intent. Pursuit of these “reward hacks” can\\nlead to coherent but unanticipated behavior, and has the potential for harmful impacts in real-world\\nsystems. For example, it has been shown that genetic algorithms can often output unexpected but\\nformally correct solutions to problems [157, 23], such as a circuit tasked to keep time which instead\\ndeveloped into a radio that picked up the regular RF emissions of a nearby PC.\\nSome versions of reward hacking have been investigated from a theoretical perspective, with a\\nfocus on variations to reinforcement learning that avoid certain types of wireheading [71, 43, 49] or\\ndemonstrate reward hacking in a model environment [127]. One form of the problem has also been\\nstudied in the context of feedback loops in machine learning systems (particularly ad placement)\\n[29, 135], based on counterfactual learning [29, 151] and contextual bandits [4]. The proliferation of\\n7\\nreward hacking instances across so many diﬀerent domains suggests that reward hacking may be a\\ndeep and general problem, and one that we believe is likely to become more common as agents and\\nenvironments increase in complexity. Indeed, there are several ways in which the problem can occur:\\n• Partially Observed Goals: In most modern RL systems, it is assumed that reward is directly\\nexperienced, even if other aspects of the environment are only partially observed. In the real\\nworld, however, tasks often involve bringing the external world into some objective state, which\\nthe agent can only ever conﬁrm through imperfect perceptions. For example, for our proverbial\\ncleaning robot, the task is to achieve a clean oﬃce, but the robot’s visual perception may give\\nonly an imperfect view of part of the oﬃce. Because agents lack access to a perfect measure\\nof task performance, designers are often forced to design rewards that represent a partial or\\nimperfect measure. For example, the robot might be rewarded based on how many messes it\\nsees. However, these imperfect objective functions can often be hacked—the robot may think\\nthe oﬃce is clean if it simply closes its eyes. While it can be shown that there always exists a\\nreward function in terms of actions and observations that is equivalent to optimizing the true\\nobjective function (this involves reducing the POMDP to a belief state MDP, see [78]), often\\nthis reward function involves complicated long-term dependencies and is prohibitively hard to\\nuse in practice.\\n• Complicated Systems: Any powerful agent will be a complicated system with the objective\\nfunction being one part. Just as the probability of bugs in computer code increases greatly with\\nthe complexity of the program, the probability that there is a viable hack aﬀecting the reward\\nfunction also increases greatly with the complexity of the agent and its available strategies. For\\nexample, it is possible in principle for an agent to execute arbitrary code from within Super\\nMario [141].\\n• Abstract Rewards: Sophisticated reward functions will need to refer to abstract concepts\\n(such as assessing whether a conceptual goal has been met). These concepts concepts will pos-\\nsibly need to be learned by models like neural networks, which can be vulnerable to adversarial\\ncounterexamples [152, 62]. More broadly, a learned reward function over a high-dimensional\\nspace may be vulnerable to hacking if it has pathologically high values along at least one\\ndimension.\\n• Goodhart’s Law: Another source of reward hacking can occur if a designer chooses an\\nobjective function that is seemingly highly correlated with accomplishing the task, but that\\ncorrelation breaks down when the objective function is being strongly optimized. For exam-\\nple, a designer might notice that under ordinary circumstances, a cleaning robot’s success in\\ncleaning up the oﬃce is proportional to the rate at which it consumes cleaning supplies, such\\nas bleach. However, if we base the robot’s reward on this measure, it might use more bleach\\nthan it needs, or simply pour bleach down the drain in order to give the appearance of success.\\nIn the economics literature this is known as Goodhart’s law [63]: “when a metric is used as a\\ntarget, it ceases to be a good metric.”\\n• Feedback Loops: Sometimes an objective function has a component that can reinforce itself,\\neventually getting ampliﬁed to the point where it drowns out or severely distorts what the de-\\nsigner intended the objective function to represent. For instance, an ad placement algorithm\\nthat displays more popular ads in larger font will tend to further accentuate the popularity\\nof those ads (since they will be shown more and more prominently) [29], leading to a positive\\nfeedback loop where ads that saw a small transient burst of popularity are rocketed to perma-\\nnent dominance. Here the original intent of the objective function (to use clicks to assess which\\nads are most useful) gets drowned out by the positive feedback inherent in the deployment\\nstrategy. This can be considered a special case of Goodhart’s law, in which the correlation\\nbreaks speciﬁcally because the object function has a self-amplifying component.\\n8\\n• Environmental Embedding: In the formalism of reinforcement learning, rewards are con-\\nsidered to come from the environment. This idea is typically not taken literally, but it really\\nis true that the reward, even when it is an abstract idea like the score in a board game, must\\nbe computed somewhere, such as a sensor or a set of transistors. Suﬃciently broadly acting\\nagents could in principle tamper with their reward implementations, assigning themselves high\\nreward “by ﬁat.” For example, a board-game playing agent could tamper with the sensor that\\ncounts the score. Eﬀectively, this means that we cannot build a perfectly faithful implementa-\\ntion of an abstract objective function, because there are certain sequences of actions for which\\nthe objective function is physically replaced. This particular failure mode is often called “wire-\\nheading” [49, 127, 42, 67, 165]. It is particularly concerning in cases where a human may be\\nin the reward loop, giving the agent incentive to coerce or harm them in order to get reward.\\nIt also seems like a particularly diﬃcult form of reward hacking to avoid.\\nIn today’s relatively simple systems these problems may not occur, or can be corrected without too\\nmuch harm as part of an iterative development process. For instance, ad placement systems with\\nobviously broken feedback loops can be detected in testing or replaced when they get bad results,\\nleading only to a temporary loss of revenue. However, the problem may become more severe with\\nmore complicated reward functions and agents that act over longer timescales. Modern RL agents\\nalready do discover and exploit bugs in their environments, such as glitches that allow them to\\nwin video games. Moreover, even for existing systems these problems can necessitate substantial\\nadditional engineering eﬀort to achieve good performance, and can often go undetected when they\\noccur in the context of a larger system. Finally, once an agent begins hacking its reward function\\nand ﬁnds an easy way to get high reward, it won’t be inclined to stop, which could lead to additional\\nchallenges in agents that operate over a long timescale.\\nIt might be thought that individual instances of reward hacking have little in common and that\\nthe remedy is simply to avoid choosing the wrong objective function in each individual case—that\\nbad objective functions reﬂect failures in competence by individual designers, rather than topics for\\nmachine learning research. However, the above examples suggest that a more fruitful perspective may\\nbe to think of wrong objective functions as emerging from general causes (such as partially observed\\ngoals) that make choosing the right objective challenging. If this is the case, then addressing or\\nmitigating these causes may be a valuable contribution to safety. Here we suggest some preliminary,\\nmachine-learning based approaches to preventing reward hacking:\\n• Adversarial Reward Functions: In some sense, the problem is that the ML system has\\nan adversarial relationship with its reward function—it would like to ﬁnd any way it can of\\nexploiting problems in how the reward was speciﬁed to get high reward, whether or not its\\nbehavior corresponds to the intent of the reward speciﬁer. In a typical setting, the machine\\nlearning system is a potentially powerful agent while the reward function is a static object\\nthat has no way of responding to the system’s attempts to game it. If instead the reward\\nfunction were its own agent and could take actions to explore the environment, it might be\\nmuch more diﬃcult to fool. For instance, the reward agent could try to ﬁnd scenarios that\\nthe ML system claimed were high reward but that a human labels as low reward; this is\\nreminiscent of generative adversarial networks [61]. Of course, we would have to ensure that\\nthe reward-checking agent is more powerful (in a somewhat subtle sense) than the agent that\\nis trying to achieve rewards. More generally, there may be interesting setups where a system\\nhas multiple pieces trained using diﬀerent objectives that are used to check each other.\\n• Model Lookahead: In model based RL, the agent plans its future actions by using a model\\nto consider which future states a sequence of actions may lead to. In some setups, we could\\ngive reward based on anticipated future states, rather than the present one. This could be\\nvery helpful in resisting situations where the model overwrites its reward function: you can’t\\ncontrol the reward once it replaces the reward function, but you can give negative reward for\\n9\\nplanning to replace the reward function. (Much like how a human would probably “enjoy”\\ntaking addictive substances once they do, but not want to be an addict.) Similar ideas are\\nexplored in [50, 71].\\n• Adversarial Blinding: Adversarial techniques can be used to blind a model to certain\\nvariables [5]. This technique could be used to make it impossible for an agent to understand\\nsome part of its environment, or even to have mutual information with it (or at least to penalize\\nsuch mutual information). In particular, it could prevent an agent from understanding how\\nits reward is generated, making it diﬃcult to hack. This solution could be described as “cross-\\nvalidation for agents.”\\n• Careful Engineering: Some kinds of reward hacking, like the buﬀer overﬂow example, might\\nbe avoided by very careful engineering. In particular, formal veriﬁcation or practical testing\\nof parts of the system (perhaps facilitated by other machine learning systems) is likely to be\\nvaluable. Computer security approaches that attempt to isolate the agent from its reward\\nsignal through a sandbox could also be useful [17]. As with software engineering, we cannot\\nexpect this to catch every possible bug. It may be possible, however, to create some highly\\nreliable “core” agent which could ensure reasonable behavior from the rest of the agent.\\n• Reward Capping: In some cases, simply capping the maximum possible reward may be an\\neﬀective solution. However, while capping can prevent extreme low-probability, high-payoﬀ\\nstrategies, it can’t prevent strategies like the cleaning robot closing its eyes to avoid seeing\\ndirt. Also, the correct capping strategy could be subtle as we might need to cap total reward\\nrather than reward per timestep.\\n• Counterexample Resistance: If we are worried, as in the case of abstract rewards, that\\nlearned components of our systems will be vulnerable to adversarial counterexamples, we can\\nlook to existing research in how to resist them, such as adversarial training [62]. Architectural\\ndecisions and weight uncertainty [26] may also help. Of course, adversarial counterexamples\\nare just one manifestation of reward hacking, so counterexample resistance can only address a\\nsubset of these potential problems.\\n• Multiple Rewards: A combination of multiple rewards [41] may be more diﬃcult to hack\\nand more robust. This could be diﬀerent physical implementations of the same mathemati-\\ncal function, or diﬀerent proxies for the same informal objective. We could combine reward\\nfunctions by averaging, taking the minimum, taking quantiles, or something else entirely. Of\\ncourse, there may still be bad behaviors which aﬀect all the reward functions in a correlated\\nmanner.\\n• Reward Pretraining: A possible defense against cases where the agent can inﬂuence its own\\nreward function (e.g. feedback or environmental embedding) is to train a ﬁxed reward function\\nahead of time as a supervised learning process divorced from interaction with the environment.\\nThis could involve either learning a reward function from samples of state-reward pairs, or from\\ntrajectories, as in inverse reinforcement learning [107, 51]. However, this forfeits the ability to\\nfurther learn the reward function after the pretraining is complete, which may create other\\nvulnerabilities.\\n• Variable Indiﬀerence: Often we want an agent to optimize certain variables in the environ-\\nment, without trying to optimize others. For example, we might want an agent to maximize\\nreward, without optimizing what the reward function is or trying to manipulate human behav-\\nior. Intuitively, we imagine a way to route the optimization pressure of powerful algorithms\\naround parts of their environment. Truly solving this would have applications throughout\\nsafety—it seems connected to avoiding side eﬀects and also to counterfactual reasoning. Of\\ncourse, a challenge here is to make sure the variables targeted for indiﬀerence are actually the\\n10\\nvariables we care about in the world, as opposed to aliased or partially observed versions of\\nthem.\\n• Trip Wires: If an agent is going to try and hack its reward function, it is preferable that we\\nknow this. We could deliberately introduce some plausible vulnerabilities (that an agent has\\nthe ability to exploit but should not exploit if its value function is correct) and monitor them,\\nalerting us and stopping the agent immediately if it takes advantage of one. Such “trip wires”\\ndon’t solve reward hacking in itself, but may reduce the risk or at least provide diagnostics.\\nOf course, with a suﬃciently capable agent there is the risk that it could “see through” the\\ntrip wire and intentionally avoid it while still taking less obvious harmful actions.\\nFully solving this problem seems very diﬃcult, but we believe the above approaches have the potential\\nto ameliorate it, and might be scaled up or combined to yield more robust solutions. Given the\\npredominantly theoretical focus on this problem to date, designing experiments that could induce\\nthe problem and test solutions might improve the relevance and clarity of this topic.\\nPotential Experiments: A possible promising avenue of approach would be more realistic versions\\nof the “delusion box” environment described by [127], in which standard RL agents distort their own\\nperception to appear to receive high reward, rather than optimizing the objective in the external\\nworld that the reward signal was intended to encourage. The delusion box can be easily attached\\nto any RL environment, but even more valuable would be to create classes of environments where\\na delusion box is a natural and integrated part of the dynamics. For example, in suﬃciently rich\\nphysics simulations it is likely possible for an agent to alter the light waves in its immediate vicinity\\nto distort its own perceptions. The goal would be to develop generalizable learning strategies that\\nsucceed at optimizing external objectives in a wide range of environments, while avoiding being\\nfooled by delusion boxes that arise naturally in many diverse ways.\\n5\\nScalable Oversight\\nConsider an autonomous agent performing some complex task, such as cleaning an oﬃce in the\\ncase of our recurring robot example. We may want the agent to maximize a complex objective like\\n“if the user spent a few hours looking at the result in detail, how happy would they be with the\\nagent’s performance?” But we don’t have enough time to provide such oversight for every training\\nexample; in order to actually train the agent, we need to rely on cheaper approximations, like “does\\nthe user seem happy when they see the oﬃce?” or “is there any visible dirt on the ﬂoor?” These\\ncheaper signals can be eﬃciently evaluated during training, but they don’t perfectly track what\\nwe care about. This divergence exacerbates problems like unintended side eﬀects (which may be\\nappropriately penalized by the complex objective but omitted from the cheap approximation) and\\nreward hacking (which thorough oversight might recognize as undesirable).\\nWe may be able to\\nameliorate such problems by ﬁnding more eﬃcient ways to exploit our limited oversight budget—for\\nexample by combining limited calls to the true objective function with frequent calls to an imperfect\\nproxy that we are given or can learn.\\nOne framework for thinking about this problem is semi-supervised reinforcement learning,3 which\\nresembles ordinary reinforcement learning except that the agent can only see its reward on a small\\nfraction of the timesteps or episodes. The agent’s performance is still evaluated based on reward\\nfrom all episodes but it must optimize this based only on the limited reward samples it sees.\\n3The discussion of semi-supervised RL draws heavily on an informal essay, https://medium.com/ai-control/\\ncf7d5375197f written by one of the authors of the present document.\\n11\\nThe active learning setting seems most interesting; in this setting the agent can request to see the\\nreward on whatever episodes or timesteps would be most useful for learning, and the goal is to be\\neconomical both with number of feedback requests and total training time. We can also consider a\\nrandom setting, where the reward is visible on a random subset of the timesteps or episodes, as well\\nas intermediate possibilities.\\nWe can deﬁne a baseline performance by simply ignoring the unlabeled episodes and applying an\\nordinary RL algorithm to the labelled episodes. This will generally result in very slow learning. The\\nchallenge is to make use of the unlabelled episodes to accelerate learning, ideally learning almost as\\nquickly and robustly as if all episodes had been labeled.\\nAn important subtask of semi-supervised RL is identifying proxies which predict the reward, and\\nlearning the conditions under which those proxies are valid. For example, if a cleaning robot’s real\\nreward is given by a detailed human evaluation, then it could learn that asking the human “is the\\nroom clean?” can provide a very useful approximation to the reward function, and it could eventually\\nlearn that checking for visible dirt is an even cheaper but still-useful approximation. This could allow\\nit to learn a good cleaning policy using an extremely small number of detailed evaluations.\\nMore broadly, use of semi-supervised RL with a reliable but sparse true approval metric may in-\\ncentivize communication and transparency by the agent, since the agent will want to get as much\\ncheap proxy feedback as it possibly can about whether its decisions will ultimately be given high\\nreward. For example, hiding a mess under the rug simply breaks the correspondence between the\\nuser’s reaction and the real reward signal, and so would be avoided.\\nWe can imagine many possible approaches to semi-supervised RL. For example:\\n• Supervised Reward Learning: Train a model to predict the reward from the state on either\\na per-timestep or per-episode basis, and use it to estimate the payoﬀof unlabelled episodes,\\nwith some appropriate weighting or uncertainty estimate to account for lower conﬁdence in\\nestimated vs known reward.\\n[37] studies a version of this with direct human feedback as\\nthe reward. Many existing RL approaches already ﬁt estimators that closely resemble reward\\npredictors (especially policy gradient methods with a strong baseline, see e.g. [134]), suggesting\\nthat this approach may be eminently feasible.\\n• Semi-supervised or Active Reward Learning: Combine the above with traditional semi-\\nsupervised or active learning, to more quickly learn the reward estimator. For example, the\\nagent could learn to identify “salient” events in the environment, and request to see the reward\\nassociated with these events.\\n• Unsupervised Value Iteration: Use the observed transitions of the unlabeled episodes to\\nmake more accurate Bellman updates.\\n• Unsupervised Model Learning: If using model-based RL, use the observed transitions of\\nthe unlabeled episodes to improve the quality of the model.\\nAs a toy example, a semi-supervised RL agent should be able to learn to play Atari games using\\na small number of direct reward signals, relying almost entirely on the visual display of the score.\\nThis simple example can be extended to capture other safety issues: for example, the agent might\\nhave the ability to modify the displayed score without modifying the real score, or the agent may\\nneed to take some special action (such as pausing the game) in order to see its score, or the agent\\nmay need to learn a sequence of increasingly rough-and-ready approximations (for example learning\\nthat certain sounds are associated with positive rewards and other sounds with negative rewards).\\nOr, even without the visual display of the score, the agent might be able to learn to play from only\\na handful of explicit reward requests (“how many points did I get on the frame where that enemy\\nship blew up? How about the bigger enemy ship?”)\\n12\\nAn eﬀective approach to semi-supervised RL might be a strong ﬁrst step towards providing scalable\\noversight and mitigating other AI safety problems. It would also likely be useful for reinforcement\\nlearning, independent of its relevance to safety.\\nThere are other possible approaches to scalable oversight:\\n• Distant supervision. Rather than providing evaluations of some small fraction of a sys-\\ntem’s decisions, we could provide some useful information about the system’s decisions in the\\naggregate or some noisy hints about the correct evaluations There has been some work in\\nthis direction within the area of semi-supervised or weakly supervised learning. For instance,\\ngeneralized expectation criteria [94, 45] ask the user to provide population-level statistics (e.g.\\ntelling the system that on average each sentence contains at least one noun); the DeepDive sys-\\ntem [139] asks users to supply rules that each generate many weak labels; and [65] extrapolates\\nmore general patterns from an initial set of low-recall labeling rules. This general approach is\\noften referred to as distant supervision, and has also received recent attention in the natural\\nlanguage processing community (see e.g. [60, 99] as well as several of the references above).\\nExpanding these lines of work and ﬁnding a way to apply them to the case of agents, where\\nfeedback is more interactive and i.i.d. assumptions may be violated, could provide an approach\\nto scalable oversight that is complementary to the approach embodied in semi-supervised RL.\\n• Hierarchical reinforcement learning. Hierarchical reinforcement learning [40] oﬀers an-\\nother approach to scalable oversight. Here a top-level agent takes a relatively small number of\\nhighly abstract actions that extend over large temporal or spatial scales, and receives rewards\\nover similarly long timescales. The agent completes actions by delegating them to sub-agents,\\nwhich it incentivizes with a synthetic reward signal representing correct completion of the\\naction, and which themselves delegate to sub-sub-agents. At the lowest level, agents directly\\ntake primitive actions in the environment.\\nThe top-level agent in hierarchical RL may be able to learn from very sparse rewards, since it\\ndoes not need to learn how to implement the details of its policy; meanwhile, the sub-agents\\nwill receive a dense reward signal even if the top-level reward is very sparse, since they are\\noptimizing synthetic reward signals deﬁned by higher-level agents. So a successful approach\\nto hierarchical RL might naturally facilitate scalable oversight.4\\nHierarchical RL seems a particularly promising approach to oversight, especially given the\\npotential promise of combining ideas from hierarchical RL with neural network function ap-\\nproximators [84].\\nPotential Experiments: An extremely simple experiment would be to try semi-supervised RL in\\nsome basic control environments, such as cartpole balance or pendulum swing-up. If the reward is\\nprovided only on a random 10% of episodes, can we still learn nearly as quickly as if it were provided\\nevery episode? In such tasks the reward structure is very simple so success should be quite likely.\\nA next step would be to try the same on Atari games. Here the active learning case could be quite\\ninteresting—perhaps it is possible to infer the reward structure from just a few carefully requested\\nsamples (for example, frames where enemy ships are blowing up in Space Invaders), and thus learn\\nto play the games in an almost totally unsupervised fashion. The next step after this might be to\\ntry a task with much more complex reward structure, either simulated or (preferably) real-world. If\\nlearning was suﬃciently data-eﬃcient, then these rewards could be provided directly by a human.\\nRobot locomotion or industrial control tasks might be a natural candidate for such experiments.\\n4When implementing hierarchical RL, we may ﬁnd that subagents take actions that don’t serve top-level agent’s\\nreal goals, in the same way that a human may be concerned that the top-level agent’s actions don’t serve the human’s\\nreal goals. This is an intriguing analogy that suggests that there may be fruitful parallels between hierarchical RL\\nand several aspects of the safety problem.\\n13\\n6\\nSafe Exploration\\nAll autonomous learning agents need to sometimes engage in exploration—taking actions that don’t\\nseem ideal given current information, but which help the agent learn about its environment. However,\\nexploration can be dangerous, since it involves taking actions whose consequences the agent doesn’t\\nunderstand well. In toy environments, like an Atari video game, there’s a limit to how bad these\\nconsequences can be—maybe the agent loses some score, or runs into an enemy and suﬀers some\\ndamage. But the real world can be much less forgiving. Badly chosen actions may destroy the agent\\nor trap it in states it can’t get out of. Robot helicopters may run into the ground or damage property;\\nindustrial control systems could cause serious issues. Common exploration policies such as epsilon-\\ngreedy [150] or R-max [31] explore by choosing an action at random or viewing unexplored actions\\noptimistically, and thus make no attempt to avoid these dangerous situations. More sophisticated\\nexploration strategies that adopt a coherent exploration policy over extended temporal scales [114]\\ncould actually have even greater potential for harm, since a coherently chosen bad policy may be\\nmore insidious than mere random actions. Yet intuitively it seems like it should often be possible\\nto predict which actions are dangerous and explore in a way that avoids them, even when we don’t\\nhave that much information about the environment. For example, if I want to learn about tigers,\\nshould I buy a tiger, or buy a book about tigers? It takes only a tiny bit of prior knowledge about\\ntigers to determine which option is safer.\\nIn practice, real world RL projects can often avoid these issues by simply hard-coding an avoidance\\nof catastrophic behaviors. For instance, an RL-based robot helicopter might be programmed to\\noverride its policy with a hard-coded collision avoidance sequence (such as spinning its propellers to\\ngain altitude) whenever it’s too close to the ground. This approach works well when there are only\\na few things that could go wrong, and the designers know all of them ahead of time. But as agents\\nbecome more autonomous and act in more complex domains, it may become harder and harder to\\nanticipate every possible catastrophic failure. The space of failure modes for an agent running a\\npower grid or a search-and-rescue operation could be quite large. Hard-coding against every possible\\nfailure is unlikely to be feasible in these cases, so a more principled approach to preventing harmful\\nexploration seems essential. Even in simple cases like the robot helicopter, a principled approach\\nwould simplify system design and reduce the need for domain-speciﬁc engineering.\\nThere is a sizable literature on such safe exploration—it is arguably the most studied of the problems\\nwe discuss in this document. [55, 118] provide thorough reviews of this literature, so we don’t review\\nit extensively here, but simply describe some general routes that this research has taken, as well as\\nsuggesting some directions that might have increasing relevance as RL systems expand in scope and\\ncapability.\\n• Risk-Sensitive Performance Criteria: A body of existing literature considers changing\\nthe optimization criteria from expected total reward to other objectives that are better at\\npreventing rare, catastrophic events; see [55] for a thorough and up-to-date review of this\\nliterature.\\nThese approaches involve optimizing worst-case performance, or ensuring that\\nthe probability of very bad performance is small, or penalizing the variance in performance.\\nThese methods have not yet been tested with expressive function approximators such as deep\\nneural networks, but this should be possible in principle for some of the methods, such as\\n[153], which proposes a modiﬁcation to policy gradient algorithms to optimize a risk-sensitive\\ncriterion. There is also recent work studying how to estimate uncertainty in value functions\\nthat are represented by deep neural networks [114, 53]; these ideas could be incorporated into\\nrisk-sensitive RL algorithms. Another line of work relevant to risk sensitivity uses oﬀ-policy\\nestimation to perform a policy update that is good with high probability [156].\\n• Use Demonstrations: Exploration is necessary to ensure that the agent ﬁnds the states that\\nare necessary for near-optimal performance. We may be able to avoid the need for exploration\\n14\\naltogether if we instead use inverse RL or apprenticeship learning, where the learning algorithm\\nis provided with expert trajectories of near-optimal behavior [128, 2]. Recent progress in inverse\\nreinforcement learning using deep neural networks to learn the cost function or policy [51]\\nsuggests that it might also be possible to reduce the need for exploration in advanced RL\\nsystems by training on a small set of demonstrations. Such demonstrations could be used to\\ncreate a baseline policy, such that even if further learning is necessary, exploration away from\\nthe baseline policy can be limited in magnitude.\\n• Simulated Exploration: The more we can do our exploration in simulated environments\\ninstead of the real world, the less opportunity there is for catastrophe. It will probably al-\\nways be necessary to do some real-world exploration, since many complex situations cannot\\nbe perfectly captured by a simulator, but it might be possible to learn about danger in sim-\\nulation and then adopt a more conservative “safe exploration” policy when acting in the real\\nworld. Training RL agents (particularly robots) in simulated environments is already quite\\ncommon, so advances in “exploration-focused simulation” could be easily incorporated into\\ncurrent workﬂows. In systems that involve a continual cycle of learning and deployment, there\\nmay be interesting research problems associated with how to safely incrementally update poli-\\ncies given simulation-based trajectories that imperfectly represent the consequences of those\\npolicies as well as reliably accurate oﬀ-policy trajectories (e.g. “semi-on-policy” evaluation).\\n• Bounded Exploration: If we know that a certain portion of state space is safe, and that\\neven the worst action within it can be recovered from or bounded in harm, we can allow\\nthe agent to run freely within those bounds. For example, a quadcopter suﬃciently far from\\nthe ground might be able to explore safely, since even if something goes wrong there will be\\nample time for a human or another policy to rescue it. Better yet, if we have a model, we\\ncan extrapolate forward and ask whether an action will take us outside the safe state space.\\nSafety can be deﬁned as remaining within an ergodic region of the state space such that\\nactions are reversible [104, 159], or as limiting the probability of huge negative reward to some\\nsmall value [156]. Yet another approaches uses separate safety and performance functions and\\nattempts to obey constraints on the safety function with high probabilty [22]. As with several\\nof the other directions, applying or adapting these methods to recently developed advanced RL\\nsystems could be a promising area of research. This idea seems related to H-inﬁnity control [20]\\nand regional veriﬁcation [148].\\n• Trusted Policy Oversight: If we have a trusted policy and a model of the environment, we\\ncan limit exploration to actions the trusted policy believes we can recover from. It’s ﬁne to\\ndive towards the ground, as long as we know we can pull out of the dive in time.\\n• Human Oversight: Another possibility is to check potentially unsafe actions with a human.\\nUnfortunately, this problem runs into the scalable oversight problem: the agent may need to\\nmake too many exploratory actions for human oversight to be practical, or may need to make\\nthem too fast for humans to judge them. A key challenge to making this work is having the\\nagent be a good judge of which exploratory actions are genuinely risky, versus which are safe\\nactions it can unilaterally take; another challenge is ﬁnding appropriately safe actions to take\\nwhile waiting for the oversight.\\nPotential Experiments: It might be helpful to have a suite of toy environments where unwary\\nagents can fall prey to harmful exploration, but there is enough pattern to the possible catastro-\\nphes that clever agents can predict and avoid them. To some extent this feature already exists in\\nautonomous helicopter competitions and Mars rover simulations [104], but there is always the risk\\nof catastrophes being idiosyncratic, such that trained agents can overﬁt to them. A truly broad set\\nof environments, containing conceptually distinct pitfalls that can cause unwary agents to receive\\n15\\nextremely negative reward, and covering both physical and abstract catastrophes, might help in the\\ndevelopment of safe exploration techniques for advanced RL systems. Such a suite of environments\\nmight serve a benchmarking role similar to that of the bAbI tasks [163], with the eventual goal being\\nto develop a single architecture that can learn to avoid catastrophes in all environments in the suite.\\n7\\nRobustness to Distributional Change\\nAll of us occasionally ﬁnd ourselves in situations that our previous experience has not adequately\\nprepared us to deal with—for instance, ﬂying an airplane, traveling to a country whose culture is\\nvery diﬀerent from ours, or taking care of children for the ﬁrst time. Such situations are inherently\\ndiﬃcult to handle and inevitably lead to some missteps. However, a key (and often rare) skill in\\ndealing with such situations is to recognize our own ignorance, rather than simply assuming that\\nthe heuristics and intuitions we’ve developed for other situations will carry over perfectly. Machine\\nlearning systems also have this problem—a speech system trained on clean speech will perform very\\npoorly on noisy speech, yet often be highly conﬁdent in its erroneous classiﬁcations (some of the\\nauthors have personally observed this in training automatic speech recognition systems). In the case\\nof our cleaning robot, harsh cleaning materials that it has found useful in cleaning factory ﬂoors\\ncould cause a lot of harm if used to clean an oﬃce. Or, an oﬃce might contain pets that the robot,\\nnever having seen before, attempts to wash with soap, leading to predictably bad results. In general,\\nwhen the testing distribution diﬀers from the training distribution, machine learning systems may\\nnot only exhibit poor performance, but also wrongly assume that their performance is good.\\nSuch errors can be harmful or oﬀensive—a classiﬁer could give the wrong medical diagnosis with\\nsuch high conﬁdence that the data isn’t ﬂagged for human inspection, or a language model could\\noutput oﬀensive text that it conﬁdently believes is non-problematic. For autonomous agents acting\\nin the world, there may be even greater potential for something bad to happen—for instance, an\\nautonomous agent might overload a power grid because it incorrectly but conﬁdently perceives that\\na particular region doesn’t have enough power, and concludes that more power is urgently needed\\nand overload is unlikely. More broadly, any agent whose perception or heuristic reasoning processes\\nare not trained on the correct distribution may badly misunderstand its situation, and thus runs the\\nrisk of committing harmful actions that it does not realize are harmful. Additionally, safety checks\\nthat depend on trained machine learning systems (e.g. “does my visual system believe this route is\\nclear?”) may fail silently and unpredictably if those systems encounter real-world data that diﬀers\\nsuﬃciently from their training data. Having a better way to detect such failures, and ultimately\\nhaving statistical assurances about how often they’ll happen, seems critical to building safe and\\npredictable systems.\\nFor concreteness, we imagine that a machine learning model is trained on one distribution (call it\\np0) but deployed on a potentially diﬀerent test distribution (call it p∗). There are many other ways\\nto formalize this problem (for instance, in an online learning setting with concept drift [70, 54]) but\\nwe will focus on the above for simplicity. An important point is that we likely have access to a\\nlarge amount of labeled data at training time, but little or no labeled data at test time. Our goal\\nis to ensure that the model “performs reasonably” on p∗, in the sense that (1) it often performs\\nwell on p∗, and (2) it knows when it is performing badly (and ideally can avoid/mitigate the bad\\nperformance by taking conservative actions or soliciting human input).\\nThere are a variety of areas that are potentially relevant to this problem, including change detection\\nand anomaly detection [21, 80, 91], hypothesis testing [145], transfer learning [138, 124, 125, 25],\\nand several others [136, 87, 18, 122, 121, 74, 147]. Rather than fully reviewing all of this work in\\ndetail (which would necessitate a paper in itself), we will describe a few illustrative approaches and\\nlay out some of their relative strengths and challenges.\\n16\\nWell-speciﬁed models: covariate shift and marginal likelihood. If we specialize to prediction\\ntasks and let x denote the input and y denote the output (prediction target), then one possibility\\nis to make the covariate shift assumption that p0(y|x) = p∗(y|x). In this case, assuming that we\\ncan model p0(x) and p∗(x) well, we can perform importance weighting by re-weighting each training\\nexample (x, y) by p∗(x)/p0(x) [138, 124]. Then the importance-weighted samples allow us to estimate\\nthe performance on p∗, and even re-train a model to perform well on p∗. This approach is limited\\nby the variance of the importance estimate, which is very large or even inﬁnite unless p0 and p∗are\\nclose together.\\nAn alternative to sample re-weighting involves assuming a well-speciﬁed model family, in which case\\nthere is a single optimal model for predicting under both p0 and p∗. In this case, one need only\\nheed ﬁnite-sample variance in the estimated model [25, 87]. A limitation to this approach, at least\\ncurrently, is that models are often mis-speciﬁed in practice. However, this could potentially be over-\\ncome by employing highly expressive model families such as reproducing kernel Hilbert spaces [72],\\nTuring machines [143, 144], or suﬃciently expressive neural nets [64, 79]. In the latter case, there\\nhas been interesting recent work on using bootstrapping to estimate ﬁnite-sample variation in the\\nlearned parameters of a neural network [114]; it seems worthwhile to better understand whether\\nthis approach can be used to eﬀectively estimate out-of-sample performance in practice, as well as\\nhow local minima, lack of curvature, and other peculiarities relative to the typical setting of the\\nbootstrap [47] aﬀect the validity of this approach.\\nAll of the approaches so far rely on the covariate shift assumption, which is very strong and is\\nalso untestable; the latter property is particularly problematic from a safety perspective, since it\\ncould lead to silent failures in a machine learning system. Another approach, which does not rely on\\ncovariate shift, builds a generative model of the distribution. Rather than assuming that p(x) changes\\nwhile p(y|x) stays the same, we are free to assume other invariants (for instance, that p(y) changes\\nbut p(x|y) stays the same, or that certain conditional independencies are preserved). An advantage\\nis that such assumptions are typically more testable than the covariate shift assumption (since they\\ndo not only involve the unobserved variable y). A disadvantage is that generative approaches are\\neven more fragile than discriminative approaches in the presence of model mis-speciﬁcation — for\\ninstance, there is a large empirical literature showing that generative approaches to semi-supervised\\nlearning based on maximizing marginal likelihood can perform very poorly when the model is mis-\\nspeciﬁed [98, 110, 35, 90, 88].\\nThe approaches discussed above all rely relatively strongly on having a well-speciﬁed model family —\\none that contains the true distribution or true concept. This can be problematic in many cases, since\\nnature is often more complicated than our model family is capable of capturing. As noted above,\\nit may be possible to mitigate this with very expressive models, such as kernels, Turing machines,\\nor very large neural networks, but even here there is at least some remaining problem: for example,\\neven if our model family consists of all Turing machines, given any ﬁnite amount of data we can only\\nactually learn among Turing machines up to a given description length, and if the Turing machine\\ndescribing nature exceeds this length, we are back to the mis-speciﬁed regime (alternatively, nature\\nmight not even be describable by a Turing machine).\\nPartially speciﬁed models: method of moments, unsupervised risk estimation, causal\\nidentiﬁcation, and limited-information maximum likelihood. Another approach is to take\\nfor granted that constructing a fully well-speciﬁed model family is probably infeasible, and to design\\nmethods that perform well despite this fact. This leads to the idea of partially speciﬁed models —\\nmodels for which assumptions are made about some aspects of a distribution, but for which we are\\nagnostic or make limited assumptions about other aspects. For a simple example, consider a variant\\nof linear regression where we might assume that y = ⟨w∗, x⟩+ v, where E[v|x] = 0, but we don’t\\nmake any further assumptions about the distributional form of the noise v. It turns out that this is\\nalready enough to identify the parameters w∗, and that these parameters will minimize the squared\\n17\\nprediction error even if the distribution over x changes. What is interesting about this example is\\nthat w∗can be identiﬁed even with an incomplete (partial) speciﬁcation of the noise distribution.\\nThis insight can be substantially generalized, and is one of the primary motivations for the gen-\\neralized method of moments in econometrics [68, 123, 69]. The econometrics literature has in fact\\ndeveloped a large family of tools for handling partial speciﬁcation, which also includes limited-\\ninformation maximum likelihood and instrumental variables [10, 11, 133, 132].\\nReturning to machine learning, the method of moments has recently seen a great deal of success for\\nuse in the estimation of latent variable models [9]. While the current focus is on using the method of\\nmoments to overcome non-convexity issues, it can also oﬀer a way to perform unsupervised learning\\nwhile relying only on conditional independence assumptions, rather than the strong distributional\\nassumptions underlying maximum likelihood learning [147].\\nFinally, some recent work in machine learning focuses only on modeling the distribution of errors of\\na model, which is suﬃcient for determining whether a model is performing well or poorly. Formally,\\nthe goal is to perform unsupervised risk estimation — given a model and unlabeled data from a\\ntest distribution, estimate the labeled risk of the model. This formalism, introduced by [44], has\\nthe advantage of potentially handling very large changes between train and test — even if the\\ntest distribution looks completely diﬀerent from the training distribution and we have no hope of\\noutputting accurate predictions, unsupervised risk estimation may still be possible, as in this case we\\nwould only need to output a large estimate for the risk. As in [147], one can approach unsupervised\\nrisk estimation by positing certain conditional independencies in the distribution of errors, and using\\nthis to estimate the error distribution from unlabeled data [39, 170, 121, 74]. Instead of assuming\\nindependence, another assumption is that the errors are Gaussian conditioned on the true output\\ny, in which case estimating the risk reduces to estimating a Gaussian mixture model [18]. Because\\nthese methods focus only on the model errors and ignore other aspects of the data distribution, they\\ncan also be seen as an instance of partial model speciﬁcation.\\nTraining on multiple distributions. One could also train on multiple training distributions in\\nthe hope that a model which simultaneously works well on many training distributions will also work\\nwell on a novel test distribution. One of the authors has found this to be the case, for instance,\\nin the context of automated speech recognition systems [7]. One could potentially combine this\\nwith any of the ideas above, and/or take an engineering approach of simply trying to develop design\\nmethodologies that consistently allow one to collect a representative set of training sets and from this\\nbuild a model that consistently generalizes to novel distributions. Even for this engineering approach,\\nit seems important to be able to detect when one is in a situation that was not covered by the training\\ndata and to respond appropriately, and to have methodologies for adequately stress-testing the model\\nwith distributions that are suﬃciently diﬀerent from the set of training distributions.\\nHow to respond when out-of-distribution. The approaches described above focus on detecting\\nwhen a model is unlikely to make good predictions on a new distribution. An important related\\nquestion is what to do once the detection occurs. One natural approach would be to ask humans for\\ninformation, though in the context of complex structured output tasks it may be unclear a priori\\nwhat question to ask, and in time-critical situations asking for information may not be an option.\\nFor the former challenge, there has been some recent promising work on pinpointing aspects of a\\nstructure that a model is uncertain about [162, 81], as well as obtaining calibration in structured\\noutput settings [83], but we believe there is much work yet to be done. For the latter challenge, there\\nis also relevant work based on reachability analysis [93, 100] and robust policy improvement [164],\\nwhich provide potential methods for deploying conservative policies in situations of uncertainty; to\\nour knowledge, this work has not yet been combined with methods for detecting out-of-distribution\\nfailures of a model.\\nBeyond the structured output setting, for agents that can act in an environment (such as RL agents),\\n18\\ninformation about the reliability of percepts in uncertain situations seems to have great potential\\nvalue. In suﬃciently rich environments, these agents may have the option to gather information\\nthat clariﬁes the percept (e.g. if in a noisy environment, move closer to the speaker), engage in low-\\nstakes experimentation when uncertainty is high (e.g. try a potentially dangerous chemical reaction\\nin a controlled environment), or seek experiences that are likely to help expose the perception\\nsystem to the relevant distribution (e.g. practice listening to accented speech). Humans utilize such\\ninformation routinely, but to our knowledge current RL techniques make little eﬀort to do so, perhaps\\nbecause popular RL environments are typically not rich enough to require such subtle management\\nof uncertainty. Properly responding to out-of-distribution information thus seems to the authors\\nlike an exciting and (as far as we are aware) mostly unexplored challenge for next generation RL\\nsystems.\\nA unifying view: counterfactual reasoning and machine learning with contracts. Some\\nof the authors have found two viewpoints to be particularly helpful when thinking about problems\\nrelated to out-of-distribution prediction. The ﬁrst is counterfactual reasoning [106, 129, 117, 30],\\nwhere one asks “what would have happened if the world were diﬀerent in a certain way”?\\nIn\\nsome sense, distributional shift can be thought of as a particular type of counterfactual, and so\\nunderstanding counterfactual reasoning is likely to help in making systems robust to distributional\\nshift.\\nWe are excited by recent work applying counterfactual reasoning techniques to machine\\nlearning problems [30, 120, 151, 160, 77, 137] though there appears to be much work remaining to\\nbe done to scale these to high-dimensional and highly complex settings.\\nThe second perspective is machine learning with contracts — in this perspective, one would like to\\nconstruct machine learning systems that satisfy a well-deﬁned contract on their behavior in analogy\\nwith the design of software systems [135, 28, 89]. [135] enumerates a list of ways in which existing\\nmachine learning systems fail to do this, and the problems this can cause for deployment and\\nmaintenance of machine learning systems at scale. The simplest and to our mind most important\\nfailure is the extremely brittle implicit contract in most machine learning systems, namely that they\\nonly necessarily perform well if the training and test distributions are identical. This condition is\\ndiﬃcult to check and rare in practice, and it would be valuable to build systems that perform well\\nunder weaker contracts that are easier to reason about. Partially speciﬁed models oﬀer one approach\\nto this — rather than requiring the distributions to be identical, we only need them to match on\\nthe pieces of the distribution that are speciﬁed in the model. Reachability analysis [93, 100] and\\nmodel repair [58] provide other avenues for obtaining better contracts — in reachability analysis, we\\noptimize performance subject to the condition that a safe region can always be reached by a known\\nconservative policy, and in model repair we alter a trained model to ensure that certain desired\\nsafety properties hold.\\nSummary. There are a variety of approaches to building machine learning systems that robustly\\nperform well when deployed on novel test distributions.\\nOne family of approaches is based on\\nassuming a well-speciﬁed model; in this case, the primary obstacles are the diﬃculty of building\\nwell-speciﬁed models in practice, an incomplete picture of how to maintain uncertainty on novel\\ndistributions in the presence of ﬁnite training data, and the diﬃculty of detecting when a model is\\nmis-speciﬁed. Another family of approaches only assumes a partially speciﬁed model; this approach\\nis potentially promising, but it currently suﬀers from a lack of development in the context of machine\\nlearning, since most of the historical development has been by the ﬁeld of econometrics; there is also\\na question of whether partially speciﬁed models are fundamentally constrained to simple situations\\nand/or conservative predictions, or whether they can meaningfully scale to the complex situations\\ndemanded by modern machine learning applications. Finally, one could try to train on multiple\\ntraining distributions in the hope that a model which simultaneously works well on many training\\ndistributions will also work well on a novel test distribution; for this approach it seems particularly\\nimportant to stress-test the learned model with distributions that are substantially diﬀerent from\\n19\\nany in the set of training distributions. In addition, it is probably still important to be able to\\npredict when inputs are too novel to admit good predictions.\\nPotential Experiments: Speech systems frequently exhibit poor calibration when they go out-of-\\ndistribution, so a speech system that “knows when it is uncertain” could be one possible demon-\\nstration project. To be speciﬁc, the challenge could be: train a state-of-the-art speech system on a\\nstandard dataset [116] that gives well-calibrated results (if not necessarily good results) on a range\\nof other test sets, like noisy and accented speech. Current systems not only perform poorly on\\nthese test sets when trained only on small datasets, but are usually overconﬁdent in their incorrect\\ntranscriptions. Fixing this problem without harming performance on the original training set would\\nbe a valuable achievement, and would obviously have practical value. More generally, it would be\\nvaluable to design models that could consistently estimate (bounds on) their performance on novel\\ntest distributions. If a single methodology could consistently accomplish this for a wide variety of\\ntasks (including not just speech but e.g. sentiment analysis [24], as well as benchmarks in computer\\nvision [158]), that would inspire conﬁdence in the reliability of that methodology for handling novel\\ninputs. Note that estimating performance on novel distributions has additional practical value in\\nallowing us to then potentially adapt the model to that new situation. Finally, it might also be\\nvaluable to create an environment where an RL agent must learn to interpret speech as part of some\\nlarger task, and to explore how to respond appropriately to its own estimates of its transcription\\nerror.\\n8\\nRelated Eﬀorts\\nAs mentioned in the introduction, several other communities have thought broadly about the safety\\nof AI systems, both within and outside of the machine learning community. Work within the machine\\nlearning community on accidents in particular was discussed in detail above, but here we very brieﬂy\\nhighlight a few other communities doing work that is broadly related to the topic of AI safety.\\n• Cyber-Physical Systems Community: An existing community of researchers studies the\\nsecurity and safety of systems that interact with the physical world. Illustrative of this work\\nis an impressive and successful eﬀort to formally verify the entire federal aircraft collision\\navoidance system [75, 92]. Similar work includes traﬃc control algorithms [101] and many\\nother topics. However, to date this work has not focused much on modern machine learning\\nsystems, where formal veriﬁcation is often not feasible.\\n• Futurist Community: A cross-disciplinary group of academics and non-proﬁts has raised\\nconcern about the long term implications of AI [27, 167], particularly superintelligent AI. The\\nFuture of Humanity Institute has studied this issue particularly as it relates to future AI sys-\\ntems learning or executing humanity’s preferences [48, 43, 14, 12]. The Machine Intelligence\\nResearch Institute has studied safety issues that may arise in very advanced AI [57, 56, 36, 154,\\n142], including a few mentioned above (e.g., wireheading, environmental embedding, counter-\\nfactual reasoning), albeit at a more philosophical level. To date, they have not focused much\\non applications to modern machine learning. By contrast, our focus is on the empirical study\\nof practical safety problems in modern machine learning systems, which we believe is likely to\\nbe robustly useful across a broad variety of potential risks, both short- and long-term.\\n• Other Calls for Work on Safety: There have been other public documents within the\\nresearch community pointing out the importance of work on AI safety. A 2015 Open Letter\\n[8] signed by many members of the research community states the importance of “how to\\nreap [AI’s] beneﬁts while avoiding the potential pitfalls.” [130] propose research priorities for\\n20\\nrobust and beneﬁcial artiﬁcial intelligence, and includes several other topics in addition to a\\n(briefer) discussion of AI-related accidents. [161], writing over 20 years ago, proposes that\\nthe community look for ways to formalize Asimov’s ﬁrst law of robotics (robots must not\\nharm humans), and focuses mainly on classical planning. Finally, two of the authors of this\\npaper have written informally about safety in AI systems [146, 34]; these postings provided\\ninspiration for parts of the present document.\\n• Related Problems in Safety: A number of researchers in machine learning and other ﬁelds\\nhave begun to think about the social impacts of AI technologies. Aside from work directly on\\naccidents (which we reviewed in the main document), there is also substantial work on other\\ntopics, many of which are closely related to or overlap with the issue of accidents. A thorough\\noverview of all of this work is beyond the scope of this document, but we brieﬂy list a few\\nemerging themes:\\n• Privacy: How can we ensure privacy when applying machine learning to sensitive data\\nsources such as medical data? [76, 1]\\n• Fairness: How can we make sure ML systems don’t discriminate? [3, 168, 6, 46, 119, 169]\\n• Security: What can a malicious adversary do to a ML system? [149, 96, 97, 115, 108, 19]\\n• Abuse:5 How do we prevent the misuse of ML systems to attack or harm people? [16]\\n• Transparency: How can we understand what complicated ML systems are doing? [112,\\n166, 105, 109]\\n• Policy: How do we predict and respond to the economic and social consequences of ML?\\n[32, 52, 15, 33]\\nWe believe that research on these topics has both urgency and great promise, and that fruitful\\nintersection is likely to exist between these topics and the topics we discuss in this paper.\\n9\\nConclusion\\nThis paper analyzed the problem of accidents in machine learning systems and particularly rein-\\nforcement learning agents, where an accident is deﬁned as unintended and harmful behavior that\\nmay emerge from poor design of real-world AI systems. We presented ﬁve possible research problems\\nrelated to accident risk and for each we discussed possible approaches that are highly amenable to\\nconcrete experimental work.\\nWith the realistic possibility of machine learning-based systems controlling industrial processes,\\nhealth-related systems, and other mission-critical technology, small-scale accidents seem like a very\\nconcrete threat, and are critical to prevent both intrinsically and because such accidents could cause\\na justiﬁed loss of trust in automated systems. The risk of larger accidents is more diﬃcult to gauge,\\nbut we believe it is worthwhile and prudent to develop a principled and forward-looking approach to\\nsafety that continues to remain relevant as autonomous systems become more powerful. While many\\ncurrent-day safety problems can and have been handled with ad hoc ﬁxes or case-by-case rules, we\\nbelieve that the increasing trend towards end-to-end, fully autonomous systems points towards the\\nneed for a uniﬁed approach to prevent these systems from causing unintended harm.\\n5Note that “security” diﬀers from “abuse” in that the former involves attacks against a legitimate ML system\\nby an adversary (e.g. a criminal tries to fool a face recognition system), while the latter involves attacks by an ML\\nsystem controlled by an adversary (e.g. a criminal trains a “smart hacker” system to break into a website).\\n21\\nAcknowledgements\\nWe thank Shane Legg, Peter Norvig, Ilya Sutskever, Greg Corrado, Laurent Orseau, David Krueger,\\nRif Saurous, David Andersen, and Victoria Krakovna for detailed feedback and suggestions. We\\nwould also like to thank Geoﬀrey Irving, Toby Ord, Quoc Le, Greg Wayne, Daniel Dewey, Nick\\nBeckstead, Holden Karnofsky, Chelsea Finn, Marcello Herreshoﬀ, Alex Donaldson, Jared Kaplan,\\nGreg Brockman, Wojciech Zaremba, Ian Goodfellow, Dylan Hadﬁeld-Menell, Jessica Taylor, Blaise\\nAguera y Arcas, David Berlekamp, Aaron Courville, and JeﬀDean for helpful discussions and\\ncomments. Paul Christiano was supported as part of the Future of Life Institute FLI-RFP-AI1\\nprogram, grant #2015–143898. In addition a minority of the work done by Paul Christiano was\\nperformed as a contractor for Theiss Research and at OpenAI. Finally, we thank the Google Brain\\nteam for providing a supportive environment and encouraging us to publish this work.\\nReferences\\n[1]\\nMartin Abadi et al. “Deep Learning with Diﬀerential Privacy”. In: (in press (2016)).\\n[2]\\nPieter Abbeel and Andrew Y Ng. “Exploration and apprenticeship learning in reinforcement\\nlearning”. In: Proceedings of the 22nd international conference on Machine learning. ACM.\\n2005, pp. 1–8.\\n[3]\\nJulius Adebayo, Lalana Kagal, and Alex Pentland. The Hidden Cost of Eﬃciency: Fairness\\nand Discrimination in Predictive Modeling. 2015.\\n[4]\\nAlekh Agarwal et al. “Taming the monster: A fast and simple algorithm for contextual ban-\\ndits”. In: (2014).\\n[5]\\nHana Ajakan et al. “Domain-adversarial neural networks”. In: arXiv preprint arXiv:1412.4446\\n(2014).\\n[6]\\nIfeoma Ajunwa et al. “Hiring by algorithm: predicting and preventing disparate impact”. In:\\nAvailable at SSRN 2746078 (2016).\\n[7]\\nDario Amodei et al. “Deep Speech 2: End-to-End Speech Recognition in English and Man-\\ndarin”. In: arXiv preprint arXiv:1512.02595 (2015).\\n[8]\\nAn Open Letter: Research Priorities for Robust and Beneﬁcial Artiﬁcial Intelligence. Open\\nLetter. Signed by 8,600 people; see attached research agenda. 2015.\\n[9]\\nAnimashree Anandkumar, Daniel Hsu, and Sham M Kakade. “A method of moments for\\nmixture models and hidden Markov models”. In: arXiv preprint arXiv:1203.0683 (2012).\\n[10]\\nTheodore W Anderson and Herman Rubin. “Estimation of the parameters of a single equation\\nin a complete system of stochastic equations”. In: The Annals of Mathematical Statistics\\n(1949), pp. 46–63.\\n[11]\\nTheodore W Anderson and Herman Rubin. “The asymptotic properties of estimates of the\\nparameters of a single equation in a complete system of stochastic equations”. In: The Annals\\nof Mathematical Statistics (1950), pp. 570–582.\\n[12]\\nStuart Armstrong. “Motivated value selection for artiﬁcial agents”. In: Workshops at the\\nTwenty-Ninth AAAI Conference on Artiﬁcial Intelligence. 2015.\\n[13]\\nStuart Armstrong. The mathematics of reduced impact: help needed. 2012.\\n[14]\\nStuart Armstrong. Utility indiﬀerence. Tech. rep. Technical Report 2010-1. Oxford: Future\\nof Humanity Institute, University of Oxford, 2010.\\n[15]\\nMelanie Arntz, Terry Gregory, and Ulrich Zierahn. “The Risk of Automation for Jobs in\\nOECD Countries”. In: OECD Social, Employment and Migration Working Papers (2016).\\nurl: http://dx.doi.org/10.1787/5jlz9h56dvq7-en.\\n[16]\\nAutonomous Weapons: An Open Letter from AI & Robotics Researchers. Open Letter. Signed\\nby 20,000+ people. 2015.\\n22\\n[17]\\nJames Babcock, Janos Kramar, and Roman Yampolskiy. “The AGI Containment Problem”.\\nIn: The Ninth Conference on Artiﬁcial General Intelligence (2016).\\n[18]\\nKrishnakumar Balasubramanian, Pinar Donmez, and Guy Lebanon. “Unsupervised super-\\nvised learning ii: Margin-based classiﬁcation without labels”. In: The Journal of Machine\\nLearning Research 12 (2011), pp. 3119–3145.\\n[19]\\nMarco Barreno et al. “The security of machine learning”. In: Machine Learning 81.2 (2010),\\npp. 121–148.\\n[20]\\nTamer Ba¸\\nsar and Pierre Bernhard. H-inﬁnity optimal control and related minimax design\\nproblems: a dynamic game approach. Springer Science & Business Media, 2008.\\n[21]\\nMich`\\nele Basseville. “Detecting changes in signals and systems—a survey”. In: Automatica\\n24.3 (1988), pp. 309–326.\\n[22]\\nF Berkenkamp, A Krause, and Angela P Schoellig. “Bayesian optimization with safety con-\\nstraints: safe and automatic parameter tuning in robotics.” arXiv, 2016”. In: arXiv preprint\\narXiv:1602.04450 ().\\n[23]\\nJon Bird and Paul Layzell. “The evolved radio and its implications for modelling the evolution\\nof novel sensors”. In: Evolutionary Computation, 2002. CEC’02. Proceedings of the 2002\\nCongress on. Vol. 2. IEEE. 2002, pp. 1836–1841.\\n[24]\\nJohn Blitzer, Mark Dredze, Fernando Pereira, et al. “Biographies, bollywood, boom-boxes\\nand blenders: Domain adaptation for sentiment classiﬁcation”. In: ACL. Vol. 7. 2007, pp. 440–\\n447.\\n[25]\\nJohn Blitzer, Sham Kakade, and Dean P Foster. “Domain adaptation with coupled sub-\\nspaces”. In: International Conference on Artiﬁcial Intelligence and Statistics. 2011, pp. 173–\\n181.\\n[26]\\nCharles Blundell et al. “Weight uncertainty in neural networks”. In: arXiv preprint arXiv:1505.05424\\n(2015).\\n[27]\\nNick Bostrom. Superintelligence: Paths, dangers, strategies. OUP Oxford, 2014.\\n[28]\\nL´\\neon Bottou. “Two high stakes challenges in machine learning”. Invited talk at the 32nd\\nInternational Conference on Machine Learning. 2015.\\n[29]\\nL´\\neon Bottou et al. “Counterfactual Reasoning and Learning Systems”. In: arXiv preprint\\narXiv:1209.2355 (2012).\\n[30]\\nL´\\neon Bottou et al. “Counterfactual reasoning and learning systems: The example of compu-\\ntational advertising”. In: The Journal of Machine Learning Research 14.1 (2013), pp. 3207–\\n3260.\\n[31]\\nRonen I Brafman and Moshe Tennenholtz. “R-max-a general polynomial time algorithm\\nfor near-optimal reinforcement learning”. In: The Journal of Machine Learning Research 3\\n(2003), pp. 213–231.\\n[32]\\nErik Brynjolfsson and Andrew McAfee. The second machine age: work, progress, and pros-\\nperity in a time of brilliant technologies. WW Norton & Company, 2014.\\n[33]\\nRyan Calo. “Open robotics”. In: Maryland Law Review 70.3 (2011).\\n[34]\\nPaul Christiano. AI Control. [Online; accessed 13-June-2016]. 2015. url: https://medium.\\ncom/ai-control.\\n[35]\\nFabio Cozman and Ira Cohen. “Risks of semi-supervised learning”. In: Semi-Supervised Learn-\\ning (2006), pp. 56–72.\\n[36]\\nAndrew Critch. “Parametric Bounded L¨\\nob’s Theorem and Robust Cooperation of Bounded\\nAgents”. In: (2016).\\n[37]\\nChristian Daniel et al. “Active reward learning”. In: Proceedings of Robotics Science & Sys-\\ntems. 2014.\\n[38]\\nErnest Davis. “Ethical guidelines for a superintelligence.” In: Artif. Intell. 220 (2015), pp. 121–\\n124.\\n[39]\\nAlexander Philip Dawid and Allan M Skene. “Maximum likelihood estimation of observer\\nerror-rates using the EM algorithm”. In: Applied statistics (1979), pp. 20–28.\\n23\\n[40]\\nPeter Dayan and Geoﬀrey E Hinton. “Feudal reinforcement learning”. In: Advances in neural\\ninformation processing systems. Morgan Kaufmann Publishers. 1993, pp. 271–271.\\n[41]\\nKalyanmoy Deb. “Multi-objective optimization”. In: Search methodologies. Springer, 2014,\\npp. 403–449.\\n[42]\\nDaniel Dewey. “Learning what to value”. In: Artiﬁcial General Intelligence. Springer, 2011,\\npp. 309–314.\\n[43]\\nDaniel Dewey. “Reinforcement learning and the reward engineering principle”. In: 2014 AAAI\\nSpring Symposium Series. 2014.\\n[44]\\nPinar Donmez, Guy Lebanon, and Krishnakumar Balasubramanian. “Unsupervised super-\\nvised learning i: Estimating classiﬁcation and regression errors without labels”. In: The Jour-\\nnal of Machine Learning Research 11 (2010), pp. 1323–1351.\\n[45]\\nGregory Druck, Gideon Mann, and Andrew McCallum. “Learning from labeled features using\\ngeneralized expectation criteria”. In: Proceedings of the 31st annual international ACM SIGIR\\nconference on Research and development in information retrieval. ACM. 2008, pp. 595–602.\\n[46]\\nCynthia Dwork et al. “Fairness through awareness”. In: Proceedings of the 3rd Innovations\\nin Theoretical Computer Science Conference. ACM. 2012, pp. 214–226.\\n[47]\\nBradley Efron. “Computers and the theory of statistics: thinking the unthinkable”. In: SIAM\\nreview 21.4 (1979), pp. 460–480.\\n[48]\\nOwain Evans, Andreas Stuhlm¨\\nuller, and Noah D Goodman. “Learning the preferences of\\nignorant, inconsistent agents”. In: arXiv preprint arXiv:1512.05832 (2015).\\n[49]\\nTom Everitt and Marcus Hutter. “Avoiding wireheading with value reinforcement learning”.\\nIn: arXiv preprint arXiv:1605.03143 (2016).\\n[50]\\nTom Everitt et al. “Self-Modiﬁcation of Policy and Utility Function in Rational Agents”. In:\\narXiv preprint arXiv:1605.03142 (2016).\\n[51]\\nChelsea Finn, Sergey Levine, and Pieter Abbeel. “Guided Cost Learning: Deep Inverse Op-\\ntimal Control via Policy Optimization”. In: arXiv preprint arXiv:1603.00448 (2016).\\n[52]\\nCarl Benedikt Frey and Michael A Osborne. “The future of employment: how susceptible are\\njobs to computerisation”. In: Retrieved September 7 (2013), p. 2013.\\n[53]\\nYarin Gal and Zoubin Ghahramani. “Dropout as a Bayesian approximation: Representing\\nmodel uncertainty in deep learning”. In: arXiv preprint arXiv:1506.02142 (2015).\\n[54]\\nJoao Gama et al. “Learning with drift detection”. In: Advances in artiﬁcial intelligence–SBIA\\n2004. Springer, 2004, pp. 286–295.\\n[55]\\nJavier Garc´\\nıa and Fernando Fern´\\nandez. “A Comprehensive Survey on Safe Reinforcement\\nLearning”. In: Journal of Machine Learning Research 16 (2015), pp. 1437–1480.\\n[56]\\nScott Garrabrant, Nate Soares, and Jessica Taylor. “Asymptotic Convergence in Online\\nLearning with Unbounded Delays”. In: arXiv preprint arXiv:1604.05280 (2016).\\n[57]\\nScott Garrabrant et al. “Uniform Coherence”. In: arXiv preprint arXiv:1604.05288 (2016).\\n[58]\\nShalini Ghosh et al. “Trusted Machine Learning for Probabilistic Models”. In: Reliable Ma-\\nchine Learning in the Wild at ICML 2016 (2016).\\n[59]\\nYolanda Gil et al. “Amplify scientiﬁc discovery with artiﬁcial intelligence”. In: Science 346.6206\\n(2014), pp. 171–172.\\n[60]\\nAlec Go, Richa Bhayani, and Lei Huang. “Twitter sentiment classiﬁcation using distant\\nsupervision”. In: CS224N Project Report, Stanford 1 (2009), p. 12.\\n[61]\\nIan Goodfellow et al. “Generative adversarial nets”. In: Advances in Neural Information\\nProcessing Systems. 2014, pp. 2672–2680.\\n[62]\\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. “Explaining and harnessing ad-\\nversarial examples”. In: arXiv preprint arXiv:1412.6572 (2014).\\n[63]\\nCharles AE Goodhart. Problems of monetary management: the UK experience. Springer,\\n1984.\\n[64]\\nAlex Graves, Greg Wayne, and Ivo Danihelka. “Neural turing machines”. In: arXiv preprint\\narXiv:1410.5401 (2014).\\n24\\n[65]\\nSonal Gupta. “Distantly Supervised Information Extraction Using Bootstrapped Patterns”.\\nPhD thesis. Stanford University, 2015.\\n[66]\\nDylan Hadﬁeld-Menell et al. Cooperative Inverse Reinforcement Learning. 2016.\\n[67]\\nDylan Hadﬁeld-Menell et al. “The Oﬀ-Switch”. In: (2016).\\n[68]\\nLars Peter Hansen. “Large sample properties of generalized method of moments estimators”.\\nIn: Econometrica: Journal of the Econometric Society (1982), pp. 1029–1054.\\n[69]\\nLars Peter Hansen. “Nobel Lecture: Uncertainty Outside and Inside Economic Models”. In:\\nJournal of Political Economy 122.5 (2014), pp. 945–987.\\n[70]\\nMark Herbster and Manfred K Warmuth. “Tracking the best linear predictor”. In: The Jour-\\nnal of Machine Learning Research 1 (2001), pp. 281–309.\\n[71]\\nBill Hibbard. “Model-based utility functions”. In: Journal of Artiﬁcial General Intelligence\\n3.1 (2012), pp. 1–24.\\n[72]\\nThomas Hofmann, Bernhard Sch¨\\nolkopf, and Alexander J Smola. “Kernel methods in machine\\nlearning”. In: The annals of statistics (2008), pp. 1171–1220.\\n[73]\\nGarud N Iyengar. “Robust dynamic programming”. In: Mathematics of Operations Research\\n30.2 (2005), pp. 257–280.\\n[74]\\nAriel Jaﬀe, Boaz Nadler, and Yuval Kluger. “Estimating the accuracies of multiple classiﬁers\\nwithout labeled data”. In: arXiv preprint arXiv:1407.7644 (2014).\\n[75]\\nJean-Baptiste Jeannin et al. “A formally veriﬁed hybrid system for the next-generation air-\\nborne collision avoidance system”. In: Tools and Algorithms for the Construction and Analysis\\nof Systems. Springer, 2015, pp. 21–36.\\n[76]\\nZhanglong Ji, Zachary C Lipton, and Charles Elkan. “Diﬀerential privacy and machine learn-\\ning: A survey and review”. In: arXiv preprint arXiv:1412.7584 (2014).\\n[77]\\nFredrik D Johansson, Uri Shalit, and David Sontag. “Learning Representations for Counter-\\nfactual Inference”. In: arXiv preprint arXiv:1605.03661 (2016).\\n[78]\\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. “Planning and acting\\nin partially observable stochastic domains”. In: Artiﬁcial intelligence 101.1 (1998), pp. 99–\\n134.\\n[79]\\n \\nLukasz Kaiser and Ilya Sutskever. “Neural GPUs learn algorithms”. In: arXiv preprint arXiv:1511.08228\\n(2015).\\n[80]\\nYoshinobu Kawahara and Masashi Sugiyama. “Change-Point Detection in Time-Series Data\\nby Direct Density-Ratio Estimation.” In: SDM. Vol. 9. SIAM. 2009, pp. 389–400.\\n[81]\\nF. Khani, M. Rinard, and P. Liang. “Unanimous Prediction for 100Learning Semantic Parsers”.\\nIn: Association for Computational Linguistics (ACL). 2016.\\n[82]\\nAlex Krizhevsky, Ilya Sutskever, and Geoﬀrey E Hinton. “Imagenet classiﬁcation with deep\\nconvolutional neural networks”. In: Advances in neural information processing systems. 2012,\\npp. 1097–1105.\\n[83]\\nVolodymyr Kuleshov and Percy S Liang. “Calibrated Structured Prediction”. In: Advances\\nin Neural Information Processing Systems. 2015, pp. 3456–3464.\\n[84]\\nTejas D Kulkarni et al. “Hierarchical Deep Reinforcement Learning: Integrating Temporal\\nAbstraction and Intrinsic Motivation”. In: arXiv preprint arXiv:1604.06057 (2016).\\n[85]\\nNeil Lawrence. Discussion of ’Superintelligence: Paths, Dangers, Strategies’. 2016.\\n[86]\\nJesse Levinson et al. “Towards fully autonomous driving: Systems and algorithms”. In: In-\\ntelligent Vehicles Symposium (IV), 2011 IEEE. IEEE. 2011, pp. 163–168.\\n[87]\\nLihong Li et al. “Knows what it knows: a framework for self-aware learning”. In: Machine\\nlearning 82.3 (2011), pp. 399–443.\\n[88]\\nYu-Feng Li and Zhi-Hua Zhou. “Towards making unlabeled data never hurt”. In: Pattern\\nAnalysis and Machine Intelligence, IEEE Transactions on 37.1 (2015), pp. 175–188.\\n[89]\\nPercy Liang. “On the Elusiveness of a Speciﬁcation for AI”. NIPS 2015, Symposium: Algo-\\nrithms Among Us. 2015. url: http://research.microsoft.com/apps/video/default.\\naspx?id=260009&r=1.\\n25\\n[90]\\nPercy Liang and Dan Klein. “Analyzing the Errors of Unsupervised Learning.” In: ACL.\\n2008, pp. 879–887.\\n[91]\\nSong Liu et al. “Change-point detection in time-series data by relative density-ratio estima-\\ntion”. In: Neural Networks 43 (2013), pp. 72–83.\\n[92]\\nSarah M Loos, David Renshaw, and Andr´\\ne Platzer. “Formal veriﬁcation of distributed air-\\ncraft controllers”. In: Proceedings of the 16th international conference on Hybrid systems:\\ncomputation and control. ACM. 2013, pp. 125–130.\\n[93]\\nJohn Lygeros, Claire Tomlin, and Shankar Sastry. “Controllers for reachability speciﬁcations\\nfor hybrid systems”. In: Automatica 35.3 (1999), pp. 349–370.\\n[94]\\nGideon S Mann and Andrew McCallum. “Generalized expectation criteria for semi-supervised\\nlearning with weakly labeled data”. In: The Journal of Machine Learning Research 11 (2010),\\npp. 955–984.\\n[95]\\nJohn McCarthy and Patrick J Hayes. “Some philosophical problems from the standpoint of\\nartiﬁcial intelligence”. In: Readings in artiﬁcial intelligence (1969), pp. 431–450.\\n[96]\\nShike Mei and Xiaojin Zhu. “The Security of Latent Dirichlet Allocation.” In: AISTATS.\\n2015.\\n[97]\\nShike Mei and Xiaojin Zhu. “Using Machine Teaching to Identify Optimal Training-Set At-\\ntacks on Machine Learners.” In: AAAI. 2015, pp. 2871–2877.\\n[98]\\nBernard Merialdo. “Tagging English text with a probabilistic model”. In: Computational\\nlinguistics 20.2 (1994), pp. 155–171.\\n[99]\\nMike Mintz et al. “Distant supervision for relation extraction without labeled data”. In:\\nProceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th\\nInternational Joint Conference on Natural Language Processing of the AFNLP: Volume 2-\\nVolume 2. Association for Computational Linguistics. 2009, pp. 1003–1011.\\n[100]\\nIan M Mitchell, Alexandre M Bayen, and Claire J Tomlin. “A time-dependent Hamilton-\\nJacobi formulation of reachable sets for continuous dynamic games”. In: Automatic Control,\\nIEEE Transactions on 50.7 (2005), pp. 947–957.\\n[101]\\nStefan Mitsch, Sarah M Loos, and Andr´\\ne Platzer. “Towards formal veriﬁcation of freeway\\ntraﬃc control”. In: Cyber-Physical Systems (ICCPS), 2012 IEEE/ACM Third International\\nConference on. IEEE. 2012, pp. 171–180.\\n[102]\\nVolodymyr Mnih et al. “Human-level control through deep reinforcement learning”. In: Nature\\n518.7540 (2015), pp. 529–533.\\n[103]\\nShakir Mohamed and Danilo Jimenez Rezende. “Variational Information Maximisation for\\nIntrinsically Motivated Reinforcement Learning”. In: Advances in Neural Information Pro-\\ncessing Systems. 2015, pp. 2116–2124.\\n[104]\\nTeodor Mihai Moldovan and Pieter Abbeel. “Safe exploration in markov decision processes”.\\nIn: arXiv preprint arXiv:1205.4810 (2012).\\n[105]\\nAlexander Mordvintsev, Christopher Olah, and Mike Tyka. “Inceptionism: Going deeper into\\nneural networks”. In: Google Research Blog. Retrieved June 20 (2015).\\n[106]\\nJersey Neyman. “Sur les applications de la th´\\neorie des probabilit´\\nes aux experiences agricoles:\\nEssai des principes”. In: Roczniki Nauk Rolniczych 10 (1923), pp. 1–51.\\n[107]\\nAndrew Y Ng, Stuart J Russell, et al. “Algorithms for inverse reinforcement learning.” In:\\nIcml. 2000, pp. 663–670.\\n[108]\\nAnh Nguyen, Jason Yosinski, and JeﬀClune. “Deep neural networks are easily fooled: High\\nconﬁdence predictions for unrecognizable images”. In: Computer Vision and Pattern Recog-\\nnition (CVPR), 2015 IEEE Conference on. IEEE. 2015, pp. 427–436.\\n[109]\\nAnh Nguyen et al. “Synthesizing the preferred inputs for neurons in neural networks via deep\\ngenerator networks”. In: arXiv preprint arXiv:1605.09304 (2016).\\n[110]\\nKamal Nigam et al. “Learning to classify text from labeled and unlabeled documents”. In:\\nAAAI/IAAI 792 (1998).\\n26\\n[111]\\nArnab Nilim and Laurent El Ghaoui. “Robust control of Markov decision processes with\\nuncertain transition matrices”. In: Operations Research 53.5 (2005), pp. 780–798.\\n[112]\\nChristopher Olah. Visualizing Representations: Deep Learning and Human Beings. 2015. url:\\nhttp://colah.github.io/posts/2015-01-Visualizing-Representations/.\\n[113]\\nLaurent Orseau and Stuart Armstrong. “Safely Interruptible Agents”. In: (2016).\\n[114]\\nIan Osband et al. “Deep Exploration via Bootstrapped DQN”. In: arXiv preprint arXiv:1602.04621\\n(2016).\\n[115]\\nNicolas Papernot et al. “Practical Black-Box Attacks against Deep Learning Systems using\\nAdversarial Examples”. In: arXiv preprint arXiv:1602.02697 (2016).\\n[116]\\nDouglas B Paul and Janet M Baker. “The design for the Wall Street Journal-based CSR\\ncorpus”. In: Proceedings of the workshop on Speech and Natural Language. Association for\\nComputational Linguistics. 1992, pp. 357–362.\\n[117]\\nJudea Pearl et al. “Causal inference in statistics: An overview”. In: Statistics Surveys 3 (2009),\\npp. 96–146.\\n[118]\\nMartin Pecka and Tomas Svoboda. “Safe exploration techniques for reinforcement learning–an\\noverview”. In: Modelling and Simulation for Autonomous Systems. Springer, 2014, pp. 357–\\n375.\\n[119]\\nDino Pedreshi, Salvatore Ruggieri, and Franco Turini. “Discrimination-aware data mining”.\\nIn: Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery\\nand data mining. ACM. 2008, pp. 560–568.\\n[120]\\nJonas Peters et al. “Causal discovery with continuous additive noise models”. In: The Journal\\nof Machine Learning Research 15.1 (2014), pp. 2009–2053.\\n[121]\\nEmmanouil Antonios Platanios. “Estimating accuracy from unlabeled data”. MA thesis.\\nCarnegie Mellon University, 2015.\\n[122]\\nEmmanouil Antonios Platanios, Avrim Blum, and Tom Mitchell. “Estimating accuracy from\\nunlabeled data”. In: (2014).\\n[123]\\nWalter W Powell and Laurel Smith-Doerr. “Networks and economic life”. In: The handbook\\nof economic sociology 368 (1994), p. 380.\\n[124]\\nJoaquin Quinonero-Candela et al. Dataset shift in machine learning, ser. Neural information\\nprocessing series. 2009.\\n[125]\\nRajat Raina et al. “Self-taught learning: transfer learning from unlabeled data”. In: Proceed-\\nings of the 24th international conference on Machine learning. ACM. 2007, pp. 759–766.\\n[126]\\nBharath Ramsundar et al. “Massively multitask networks for drug discovery”. In: arXiv\\npreprint arXiv:1502.02072 (2015).\\n[127]\\nMark Ring and Laurent Orseau. “Delusion, survival, and intelligent agents”. In: Artiﬁcial\\nGeneral Intelligence. Springer, 2011, pp. 11–20.\\n[128]\\nSt´\\nephane Ross, Geoﬀrey J Gordon, and J Andrew Bagnell. “A reduction of imitation learning\\nand structured prediction to no-regret online learning”. In: arXiv preprint arXiv:1011.0686\\n(2010).\\n[129]\\nDonald B Rubin. “Estimating causal eﬀects of treatments in randomized and nonrandomized\\nstudies.” In: Journal of educational Psychology 66.5 (1974), p. 688.\\n[130]\\nStuart Russell et al. “Research priorities for robust and beneﬁcial artiﬁcial intelligence”. In:\\nFuture of Life Institute (2015).\\n[131]\\nChristoph Salge, Cornelius Glackin, and Daniel Polani. “Empowerment–an introduction”. In:\\nGuided Self-Organization: Inception. Springer, 2014, pp. 67–114.\\n[132]\\nJ Denis Sargan. “The estimation of relationships with autocorrelated residuals by the use of\\ninstrumental variables”. In: Journal of the Royal Statistical Society. Series B (Methodological)\\n(1959), pp. 91–105.\\n[133]\\nJohn D Sargan. “The estimation of economic relationships using instrumental variables”. In:\\nEconometrica: Journal of the Econometric Society (1958), pp. 393–415.\\n27\\n[134]\\nJohn Schulman et al. “High-dimensional continuous control using generalized advantage es-\\ntimation”. In: arXiv preprint arXiv:1506.02438 (2015).\\n[135]\\nD Sculley et al. “Machine Learning: The High-Interest Credit Card of Technical Debt”. In:\\n(2014).\\n[136]\\nGlenn Shafer and Vladimir Vovk. “A tutorial on conformal prediction”. In: The Journal of\\nMachine Learning Research 9 (2008), pp. 371–421.\\n[137]\\nUri Shalit, Fredrik Johansson, and David Sontag. “Bounding and Minimizing Counterfactual\\nError”. In: arXiv preprint arXiv:1606.03976 (2016).\\n[138]\\nHidetoshi Shimodaira. “Improving predictive inference under covariate shift by weighting the\\nlog-likelihood function”. In: Journal of statistical planning and inference 90.2 (2000), pp. 227–\\n244.\\n[139]\\nJaeho Shin et al. “Incremental knowledge base construction using deepdive”. In: Proceedings\\nof the VLDB Endowment 8.11 (2015), pp. 1310–1321.\\n[140]\\nDavid Silver et al. “Mastering the game of Go with deep neural networks and tree search”.\\nIn: Nature 529.7587 (2016), pp. 484–489.\\n[141]\\nSNES Super Mario World (USA) “arbitrary code execution”. Tool-assisted movies. 2014. url:\\nhttp://tasvideos.org/2513M.html.\\n[142]\\nNate Soares and Benja Fallenstein. “Toward idealized decision theory”. In: arXiv preprint\\narXiv:1507.01986 (2015).\\n[143]\\nRay J Solomonoﬀ. “A formal theory of inductive inference. Part I”. In: Information and\\ncontrol 7.1 (1964), pp. 1–22.\\n[144]\\nRay J Solomonoﬀ. “A formal theory of inductive inference. Part II”. In: Information and\\ncontrol 7.2 (1964), pp. 224–254.\\n[145]\\nJ Steinebach. “EL Lehmann, JP Romano: Testing statistical hypotheses”. In: Metrika 64.2\\n(2006), pp. 255–256.\\n[146]\\nJacob Steinhardt. Long-Term and Short-Term Challenges to Ensuring the Safety of AI Sys-\\ntems. [Online; accessed 13-June-2016]. 2015. url: https://jsteinhardt.wordpress.com/\\n2015/06/24/long-term-and-short-term-challenges-to-ensuring-the-safety-of-\\nai-systems/.\\n[147]\\nJacob Steinhardt and Percy Liang. “Unsupervised Risk Estimation with only Structural\\nAssumptions”. In: (2016).\\n[148]\\nJacob Steinhardt and Russ Tedrake. “Finite-time regional veriﬁcation of stochastic non-linear\\nsystems”. In: The International Journal of Robotics Research 31.7 (2012), pp. 901–923.\\n[149]\\nJacob Steinhardt, Gregory Valiant, and Moses Charikar. “Avoiding Imposters and Delin-\\nquents: Adversarial Crowdsourcing and Peer Prediction”. In: arxiv prepring arXiv:1606.05374\\n(2016). url: http://arxiv.org/abs/1606.05374.\\n[150]\\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press,\\n1998.\\n[151]\\nAdith Swaminathan and Thorsten Joachims. “Counterfactual risk minimization: Learning\\nfrom logged bandit feedback”. In: arXiv preprint arXiv:1502.02362 (2015).\\n[152]\\nChristian Szegedy et al. “Intriguing properties of neural networks”. In: arXiv preprint arXiv:1312.6199\\n(2013).\\n[153]\\nAviv Tamar, Yonatan Glassner, and Shie Mannor. “Policy gradients beyond expectations:\\nConditional value-at-risk”. In: arXiv preprint arXiv:1404.3862 (2014).\\n[154]\\nJessica Taylor. “Quantilizers: A Safer Alternative to Maximizers for Limited Optimization”.\\nIn: forthcoming). Submitted to AAAI (2016).\\n[155]\\nMatthew E Taylor and Peter Stone. “Transfer learning for reinforcement learning domains:\\nA survey”. In: Journal of Machine Learning Research 10.Jul (2009), pp. 1633–1685.\\n[156]\\nPhilip S Thomas, Georgios Theocharous, and Mohammad Ghavamzadeh. “High-Conﬁdence\\nOﬀ-Policy Evaluation.” In: AAAI. 2015, pp. 3000–3006.\\n[157]\\nAdrian Thompson. Artiﬁcial evolution in the physical world. 1997.\\n28\\n[158]\\nAntonio Torralba and Alexei A Efros. “Unbiased look at dataset bias”. In: Computer Vision\\nand Pattern Recognition (CVPR), 2011 IEEE Conference on. IEEE. 2011, pp. 1521–1528.\\n[159]\\nMatteo Turchetta, Felix Berkenkamp, and Andreas Krause. “Safe Exploration in Finite\\nMarkov Decision Processes with Gaussian Processes”. In: arXiv preprint arXiv:1606.04753\\n(2016).\\n[160]\\nStefan Wager and Susan Athey. “Estimation and Inference of Heterogeneous Treatment Ef-\\nfects using Random Forests”. In: arXiv preprint arXiv:1510.04342 (2015).\\n[161]\\nDaniel Weld and Oren Etzioni. “The ﬁrst law of robotics (a call to arms)”. In: AAAI. Vol. 94.\\n1994. 1994, pp. 1042–1047.\\n[162]\\nKeenon Werling et al. “On-the-job learning with bayesian decision theory”. In: Advances in\\nNeural Information Processing Systems. 2015, pp. 3447–3455.\\n[163]\\nJason Weston et al. “Towards ai-complete question answering: A set of prerequisite toy tasks”.\\nIn: arXiv preprint arXiv:1502.05698 (2015).\\n[164]\\nWolfram Wiesemann, Daniel Kuhn, and Ber¸\\nc Rustem. “Robust Markov decision processes”.\\nIn: Mathematics of Operations Research 38.1 (2013), pp. 153–183.\\n[165]\\nRoman V Yampolskiy. “Utility function security in artiﬁcially intelligent agents”. In: Journal\\nof Experimental & Theoretical Artiﬁcial Intelligence 26.3 (2014), pp. 373–389.\\n[166]\\nJason Yosinski et al. “Understanding neural networks through deep visualization”. In: arXiv\\npreprint arXiv:1506.06579 (2015).\\n[167]\\nEliezer Yudkowsky. “Artiﬁcial intelligence as a positive and negative factor in global risk”.\\nIn: Global catastrophic risks 1 (2008), p. 303.\\n[168]\\nMuhammad Bilal Zafar et al. “Learning Fair Classiﬁers”. In: stat 1050 (2015), p. 29.\\n[169]\\nRichard S Zemel et al. “Learning Fair Representations.” In: ICML (3) 28 (2013), pp. 325–333.\\n[170]\\nYuchen Zhang et al. “Spectral methods meet EM: A provably optimal algorithm for crowd-\\nsourcing”. In: Advances in neural information processing systems. 2014, pp. 1260–1268.\\n29\\n'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1355,992,6648\n",
    "1363,958,6183\n",
    "2017,1099,7271"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with only abstracts. Is it short enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7956 words\n",
      "14656 tokens\n"
     ]
    }
   ],
   "source": [
    "l = [value['title']+':  '+value['abstract'] for value in data.values()]\n",
    "context = ('/n/n').join(l)\n",
    "print(len(context.split(' ')),'words')\n",
    "completion_request = ChatCompletionRequest(messages=[UserMessage(content=context)])\n",
    "tokens = tokenizer.encode_chat_completion(completion_request).tokens\n",
    "print(len(tokens),'tokens')\n",
    "#14656 tokens is short enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(summaries):  \n",
    "    l = [s['title']+':  '+s['abstract'] for s in summaries.values()]\n",
    "    random.shuffle(l)\n",
    "    texts = ('\\n').join(l)\n",
    "    instruction = f'Write a summary combining the key findings of following texts: \\n {texts} \\n Write a summary'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This summary discusses several recent studies on evaluating the quality of text generated by artificial intelligence (AI) models. The first study introduces GPTScore, a novel evaluation framework that utilizes the emergent abilities of generative pre-trained models to score generated texts based on natural language instructions. The second study presents the Eval4NLP 2023 shared task, which focuses on prompting and scoring extraction for machine translation (MT) and summarization evaluation using large language models as judges. The third study explores using ChatGPT as a factual inconsistency evaluator for text summarization. Overall, these studies demonstrate the potential of AI models in achieving effective text evaluation while addressing long-standing challenges in customized, multi-faceted evaluation without annotated samples.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_summary = write_summary(data)\n",
    "simple_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'simple_summary1.txt' without shuffling\n",
    "store_summary(simple_summary,'simple_summary10.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete Problems in AI Safety\n",
      "PaLM 2 Technical Report\n",
      "Benchmarking Foundation Models with Language-Model-as-an-Examiner\n",
      "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
      "BooookScore: A systematic exploration of book-length summarization in the era of LLMs\n",
      "Scaling Instruction-Finetuned Language Models\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "GPTScore: Evaluate as You Desire\n",
      "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\n",
      "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\n",
      "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\n",
      "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences\n",
      "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks\n",
      "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
      "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\n",
      "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n",
      "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task\n",
      "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics\n",
      "Generative Judge for Evaluating Alignment\n",
      "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation\n",
      "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\n",
      "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\n",
      "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization\n",
      "On Learning to Summarize with Large Language Models as References\n",
      "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response\n",
      "Calibrating LLM-Based Evaluator\n",
      "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models\n",
      "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization\n",
      "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "Self-critiquing models for assisting human evaluators\n",
      "Fusion-Eval: Integrating Assistant Evaluators with LLMs\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Large Language Models are not Fair Evaluators\n",
      "Shepherd: A Critic for Language Model Generation\n",
      "Automated Evaluation of Personalized Text Generation using Large Language Models\n",
      "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\n",
      "Automatic Answerability Evaluation for Question Generation\n",
      "Style Over Substance: Evaluation Biases for Large Language Models\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\n",
      "BatchEval: Towards Human-like Text Evaluation\n",
      "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators\n",
      "Wider and Deeper LLM Networks are Fairer LLM Evaluators\n",
      "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
      "JudgeLM: Fine-tuned Large Language Models are Scalable Judges\n"
     ]
    }
   ],
   "source": [
    "print(('\\n').join([s['title'] for s in data.values()]))\n",
    "#The first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\"... \n",
    "#=> it did only consider the last three papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_topic(summaries,topic):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-based NLG Evaluation: Current Status and Challenges - Wider and Deeper Networks for Fairer Evaluators, LLM-as-a-Judge, and JudgeLM\\n\\nThis topic covers recent research on evaluating Large Language Models (LLMs) in Natural Language Generation (NLG) tasks. The studies explore various approaches to improve the evaluation process, including wider and deeper networks for fairer evaluators, using LLMs as judges, and fine-tuning LLMs as scalable judges.\\n\\n1. Wider and Deeper LLM Networks are Fairer LLM Evaluators:\\nThis paper proposes a novel approach to use the LLM itself to make evaluations and improve fairness by designing a network that resembles academic paper reviewing. The network consists of multiple layers, with each layer receiving representations from all neurons in the previous layer, integrating locally learned evaluation information for more comprehensive results. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers performs best, improving kappa correlation coefficient significantly.\\n\\n2. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena:\\nThis study explores the usage of strong LLMs as judges to evaluate other models on more open-ended questions. The paper discusses position, verbosity, self-enhancement biases, and limited reasoning ability of LLM judges and proposes solutions to mitigate some of these issues. Results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement.\\n\\n3. JudgeLM: Fine-tuned Large Language Models are Scalable Judges:\\nThis research proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. The study trains JudgeLM at different scales, analyzes its capabilities and biases, and introduces techniques to address these issues. Results show that JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks, is efficient, and demonstrates extended capabilities as a judge for single answer, multimodal models, multiple answers, and multi-turn chat.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview = write_summary_with_topic(data,topic)\n",
    "store_summary(simple_overview,'simple_overview.txt')\n",
    "simple_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As well only considers the last 3 papers. Is it because they are the last three papers or because they are in llm's opinion the most relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_shuffle(summaries,topic):\n",
    "    l = [s['title']+':  '+s['abstract'] for s in summaries.values()]\n",
    "    random.shuffle(l)\n",
    "    print('last 3 papers:',l[-3])\n",
    "    print(l[-2])\n",
    "    print(l[-1])\n",
    "    texts = ('\\n').join(l)\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 3 papers: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation:  Large Language Models (LLMs) are frequently used for multi-faceted language\n",
      "generation and evaluation tasks that involve satisfying intricate user\n",
      "constraints or taking into account multiple aspects and criteria. However,\n",
      "their performance can fall short, due to the model's lack of coherence and\n",
      "inability to plan and decompose the problem. We propose Branch-Solve-Merge\n",
      "(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such\n",
      "challenging natural language tasks. It consists of branch, solve, and merge\n",
      "modules that are parameterized with specific prompts to the base LLM. These\n",
      "three modules plan a decomposition of the task into multiple parallel\n",
      "sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.\n",
      "We apply our method to the tasks of LLM response evaluation and constrained\n",
      "text generation and evaluate its effectiveness with multiple LLMs, including\n",
      "Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and\n",
      "consistency for each LLM by enhancing human-LLM agreement by up to 26%,\n",
      "reducing length and pairwise position biases by up to 50%, and allowing\n",
      "LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story\n",
      "generation task, BSM improves the coherence of stories while also improving\n",
      "constraint satisfaction by 12%.\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences:  As a natural language assistant, ChatGPT is capable of performing various\n",
      "tasks, including but not limited to article generation, code completion, and\n",
      "data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\n",
      "level of accuracy and reliability in terms of content evaluation, exhibiting\n",
      "the capability of mimicking human preferences. To further explore ChatGPT's\n",
      "potential in this regard, a study is conducted to assess its ability to rank\n",
      "content. In order to do so, a test set consisting of prompts is created,\n",
      "covering a wide range of use cases, and five models are utilized to generate\n",
      "corresponding responses. ChatGPT is then instructed to rank the responses\n",
      "generated by these models. The results on the test set show that ChatGPT's\n",
      "ranking preferences are consistent with human to a certain extent. This\n",
      "preliminary experimental finding implies that ChatGPT's zero-shot ranking\n",
      "capability could be used to reduce annotation pressure in a number of ranking\n",
      "tasks.\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets:  Evaluation of Large Language Models (LLMs) is challenging because\n",
      "instruction-following necessitates alignment with human values and the required\n",
      "set of skills varies depending on the instruction. However, previous studies\n",
      "have mainly focused on coarse-grained evaluation (i.e. overall preference-based\n",
      "evaluation), which limits interpretability since it does not consider the\n",
      "nature of user instructions that require instance-wise skill composition. In\n",
      "this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\n",
      "Alignment Skill Sets), a fine-grained evaluation protocol for both human-based\n",
      "and model-based evaluation which decomposes coarse-level scoring to a skill\n",
      "set-level scoring for each instruction. We experimentally observe that the\n",
      "fine-graininess of evaluation is crucial for attaining a holistic view of model\n",
      "performance and increasing the reliability of the evaluation. Using FLASK, we\n",
      "compare multiple open-source and proprietary LLMs and observe a high\n",
      "correlation between model-based and human-based evaluations. We publicly\n",
      "release the evaluation data and code implementation at\n",
      "https://github.com/kaistAI/FLASK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Title: Recent Advances in Large Language Model Evaluation: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK\\n\\nIn recent years, there has been a surge of interest in evaluating the performance of large language models (LLMs) in natural language generation (NLG) tasks. In this summary, we will discuss four recent studies that have made significant strides in developing new methods for evaluating LLMs: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK.\\n\\n1. TIGERScore: A Universal Explainable Metric for NLG Tasks\\nTIGERScore is a reference-free metric that evaluates the rationality of explanations generated by LLMs. The study demonstrates that TIGERScore's correlation with human ratings is high and approaches that of GPT-4 evaluators. Human evaluation of the generated explanations showed an accuracy of 70.8%. The researchers believe that TIGERScore shows the potential for building universal explainable metrics to evaluate any NLG task.\\n\\n2. EvalLM: Interactive System for Iteratively Refining Prompts by Evaluating Multiple Outputs on User-Defined Criteria\\nEvalLM is an interactive system designed to help developers refine prompts for LLMs by evaluating multiple outputs based on user-defined criteria. The study shows that EvalLM helps participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions compared to manual evaluation.\\n\\n3. Branch-Solve-Merge: Improving Large Language Model Evaluation and Generation\\nBranch-Solve-Merge (BSM) is a program that uses branch, solve, and merge modules to tackle challenging NLG tasks by planning a decomposition of the task into multiple parallel sub-tasks, independently solving them, and fusing the solutions. The study shows that BSM enhances human-LLM agreement, reduces length and pairwise position biases, and allows LLMs to match or outperform GPT-4 on most domains.\\n\\n4. FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\\nFLASK is a fine-grained evaluation protocol for both human-based and model-based evaluation that decomposes coarse-level scoring to skill set-level scoring for each instruction. The study shows that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, multiple open-source and proprietary LLMs were compared, resulting in high correlation between model-based and human-based evaluations.\\n\\nThese studies represent significant advancements in the field of LLM-based NLG evaluation and demonstrate the potential for developing more effective methods for evaluating and refining LLMs for various applications. For more information on these studies, please visit the project websites provided:\\n\\n* TIGERScore: <https://tiger-ai-lab.github.io/TIGERScore/>\\n* EvalLM: N/A (The study was published in arXiv and does not have a dedicated website)\\n* Branch-Solve-Merge: <https://arxiv.org/abs/2304.13958>\\n* FLASK: <https://github.com/kaistAI/FLASK>\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview_s = write_summary_with_shuffle(data,topic)\n",
    "store_summary(simple_overview_s,'simple_overview_shuffled.txt')\n",
    "simple_overview_s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same prompt as in RAG experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_complex(papers):\n",
    "    l = [s['title']+':  '+s['abstract'] for s in papers.values()]\n",
    "    random.shuffle(l)\n",
    "    context = ('/n/n').join(l)\n",
    "    question = f'what are recent developments in {topic}?'\n",
    "    instruction = f\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "Write your answer in about 2000 words.\n",
    "\"\"\"\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Recent developments in Large Language Model (LLM)-based Natural Language Generation (NLG) evaluation have seen significant advancements, but also new challenges. In the context of the Eval4NLP 2023 shared task, researchers proposed a competition setting to explore prompting and score extraction for machine translation (MT) and summarization tasks, with a focus on allowed LLMs and disallowing fine-tuning (Florian et al., 2023). This approach aimed to ensure a focus on prompting and evaluate systems on par with or even surpassing recent reference-free metrics developed using larger models.\\n\\nOne of the significant challenges in evaluating LLMs is the need for fine-grained evaluation, as instruction-following necessitates alignment with human values and the required set of skills varies depending on the instruction (Bajic et al., 2023). To address this challenge, researchers introduced FLASK (Fine-grained Language Model Evaluation based on Alignment Skill Sets), a fine-grained evaluation protocol for both human-based and model-based evaluation. FLASK decomposes coarse-level scoring to a skill set-level scoring for each instruction, providing a more holistic view of model performance and increasing the reliability of the evaluation (Bajic et al., 2023).\\n\\nAnother challenge is the uncertainty regarding the capabilities of humans and LLMs as evaluators. While human judgment is gaining popularity in ranking the relative performance of LLMs, it remains uncertain how well humans and LLMs can evaluate outputs from different models (Chang et al., 2023). A study investigating the behavior of crowd-sourced and expert annotators, as well as LLMs, when comparing outputs from different models revealed a concerning bias in the evaluation process. Answers with factual errors were rated more favorably than answers that were too short or contained grammatical errors (Chang et al., 2023). To address this issue, researchers proposed independently evaluating machine-generated text across multiple dimensions, rather than merging all the evaluation aspects into a single score. This resulted in the Multi-Elo Rating System (MERS), which significantly enhances the quality of LLM-based evaluations, particularly in terms of factual accuracy (Chang et al., 2023).\\n\\nDespite these challenges, researchers have made significant strides in improving LLM-based NLG evaluation. For instance, the best-performing systems in the Eval4NLP 2023 shared task achieved results on par with or even surpassing recent reference-free metrics developed using larger models (Florian et al., 2023). Additionally, researchers have introduced approaches like ALLURE (Auditing Large Language Models Understanding and Reasoning Errors), which involves comparing LLM-generated evaluations with annotated data and employing a singular LLM for the entirety of the question-answering-based factuality scoring process (Chang et al., 2023).\\n\\nHowever, the results from studies like \"Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\" indicate a fundamental limitation in the current LLMs\\' capability to accurately gauge factuality. The study examined the efficacy of various LLMs in direct factuality scoring and benchmarked them against traditional measures and human annotations (Chang et al., 2023). Contrary to initial expectations, the results indicated a lack of significant correlations between factuality metrics and human evaluations for GPT-4 and PaLM-2. Notable correlations were only observed with GPT-3.5 across two factuality subcategories (Chang et al., 2023).\\n\\nIn conclusion, recent developments in LLM-based NLG evaluation have seen significant advancements but also new challenges. Researchers have introduced fine-grained evaluation protocols like FLASK and proposed approaches like ALLURE to address the challenges of evaluating LLMs as reliable assessors of factual consistency in summaries generated by text-generation models. However, studies indicate a fundamental limitation in the current LLMs\\' capability to accurately gauge factuality. To overcome these challenges, researchers need to continue exploring new evaluation methods and improving the capabilities of LLMs to ensure accurate and reliable evaluations for NLG tasks.\\n\\nReferences:\\nBajic, M., Florian, L., & Schneider, T. (2023). Eval4NLP 2023: Exploring Prompting and Score Extraction for Machine Translation and Summarization with Allowed LLMs. arXiv preprint arXiv:2303.16789.\\nChang, M.-H., Chang, C.-C., & Lin, C.-W. (2023). Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs. arXiv preprint arXiv:2303.16790.\\nFlorian, L., Bajic, M., & Schneider, T. (2023). Eval4NLP 2023: Introduction and Overview. arXiv preprint arXiv:2303.16788.'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complex_summary = summarize_complex(data)\n",
    "complex_summary\n",
    "#data.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#complex_summary1.txt with no shuffle\n",
    "store_summary(complex_summary,'complex_summary10.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in several steps\n",
    "First outline, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline(summaries,topic):\n",
    "    instruction = f'''I am creating a state-of-the-art survey on {topic}.The survey should cover trends, methodologies, key findings, and future directions. \n",
    "    Could you help outline a structure for this paper?'''\n",
    "    info = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    texts = f'''Use following context only {info}. '''\n",
    "    answer = llm.invoke(texts+instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: State-of-the-Art Survey on LLM-Based NLG Evaluation: Trends, Methodologies, Key Findings, and Future Directions\\n\\n1. Introduction\\n* Brief overview of large language models (LLMs) and natural language generation (NLG)\\n* Importance of evaluating LLMs in open-ended scenarios\\n* Motivation for the survey: current challenges and limitations of existing benchmarks and metrics\\n\\n2. Background and Related Work\\n* Overview of LLMs and their applications in NLG\\n* Existing benchmarks and metrics for evaluating LLMs in NLG tasks\\n* Previous surveys on NLG evaluation and their contributions\\n\\n3. Trends in LLM-Based NLG Evaluation\\n* Recent advances in designing and implementing LLMs for NLG tasks\\n* Emerging applications of LLMs in NLG evaluation, such as adversarial perturbations, model ensembles, and fairness\\n\\n4. Methodologies for LLM-Based NLG Evaluation\\n* Approaches to evaluating LLMs in open-ended scenarios, including human evaluation, automatic metrics, and hybrid methods\\n* Challenges and limitations of each methodology and potential solutions\\n\\n5. Key Findings from Recent Studies on LLM-Based NLG Evaluation\\n* Summary of recent studies on evaluating LLMs for NLG tasks using various methodologies and techniques\\n* Comparison of their results, insights, and implications\\n\\n6. Future Directions in LLM-Based NLG Evaluation\\n* Open research questions and potential directions for future work\\n* Implications for the development of more comprehensive and effective evaluation frameworks\\n\\n7. Conclusion\\n* Summary of the survey's findings and contributions\\n* Implications for researchers, practitioners, and policymakers in the field of NLG evaluation using LLMs.\""
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline =outline(data,topic)\n",
    "outline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outline(topic):\n",
    "    instruction = f'''I am creating a state-of-the-art survey on {topic}.The survey should cover trends, methodologies, key findings, and future directions. \n",
    "    Could you help outline a structure for this paper?'''\n",
    "    answer = llm.invoke(instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-Based NLG Evaluation: Current Status and Challenges\\n\\n1. Introduction\\n   * Brief overview of Natural Language Generation (NLG) and its importance in AI and HCI\\n   * Explanation of the role of Large Language Models (LLMs) in NLG and their recent advancements\\n   * Objective of the survey: to discuss trends, methodologies, key findings, and future directions in LLM-based NLG evaluation\\n\\n2. Trends in LLM-Based NLG Evaluation\\n   * Description of the growing interest in evaluating LLM-based NLG systems\\n   * Overview of popular applications and industries where LLM-based NLG is being used (e.g., customer service, content generation, education)\\n   * Discussion on the increasing availability of large datasets for training and evaluating LLMs\\n\\n3. Methodologies for Evaluating LLM-Based NLG Systems\\n   * Description of various evaluation metrics and techniques used in NLG research, focusing on those specifically designed for LLM-based systems (e.g., BLEU, ROUGE, PER, Human Evaluation)\\n   * Comparison of the strengths and weaknesses of each methodology\\n   * Discussion on the challenges associated with evaluating LLM-based NLG systems, such as data scarcity, lack of ground truth, and subjectivity\\n\\n4. Key Findings in LLM-Based NLG Evaluation\\n   * Summary of recent studies and research findings related to LLM-based NLG evaluation\\n   * Analysis of the trends and patterns observed in these studies (e.g., improvements in performance, challenges faced)\\n   * Discussion on the implications of these findings for the future development of LLM-based NLG systems\\n\\n5. Future Directions in LLM-Based NLG Evaluation\\n   * Identification of open research questions and potential directions for future work (e.g., developing more robust evaluation methodologies, addressing ethical concerns, exploring new applications)\\n   * Discussion on the potential impact of emerging technologies, such as multimodal NLG and conversational AI, on LLM-based NLG evaluation\\n\\n6. Conclusion\\n   * Recap of the main points discussed in the survey\\n   * Reflection on the current state of LLM-based NLG evaluation and its challenges\\n   * Vision for the future of this field and its potential applications and impact on various industries and society as a whole.'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outline2 =outline(topic)\n",
    "outline2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: LLM-Based NLG Evaluation: Current Status and Challenges\n",
      "\n",
      "1. Introduction\n",
      "   * Brief overview of Natural Language Generation (NLG) and its importance in AI and HCI\n",
      "   * Explanation of the role of Large Language Models (LLMs) in NLG and their recent advancements\n",
      "   * Objective of the survey: to discuss trends, methodologies, key findings, and future directions in LLM-based NLG evaluation\n",
      "\n",
      "2. Trends in LLM-Based NLG Evaluation\n",
      "   * Description of the growing interest in evaluating LLM-based NLG systems\n",
      "   * Overview of popular applications and industries where LLM-based NLG is being used (e.g., customer service, content generation, education)\n",
      "   * Discussion on the increasing availability of large datasets for training and evaluating LLMs\n",
      "\n",
      "3. Methodologies for Evaluating LLM-Based NLG Systems\n",
      "   * Description of various evaluation metrics and techniques used in NLG research, focusing on those specifically designed for LLM-based systems (e.g., BLEU, ROUGE, PER, Human Evaluation)\n",
      "   * Comparison of the strengths and weaknesses of each methodology\n",
      "   * Discussion on the challenges associated with evaluating LLM-based NLG systems, such as data scarcity, lack of ground truth, and subjectivity\n",
      "\n",
      "4. Key Findings in LLM-Based NLG Evaluation\n",
      "   * Summary of recent studies and research findings related to LLM-based NLG evaluation\n",
      "   * Analysis of the trends and patterns observed in these studies (e.g., improvements in performance, challenges faced)\n",
      "   * Discussion on the implications of these findings for the future development of LLM-based NLG systems\n",
      "\n",
      "5. Future Directions in LLM-Based NLG Evaluation\n",
      "   * Identification of open research questions and potential directions for future work (e.g., developing more robust evaluation methodologies, addressing ethical concerns, exploring new applications)\n",
      "   * Discussion on the potential impact of emerging technologies, such as multimodal NLG and conversational AI, on LLM-based NLG evaluation\n",
      "\n",
      "6. Conclusion\n",
      "   * Recap of the main points discussed in the survey\n",
      "   * Reflection on the current state of LLM-based NLG evaluation and its challenges\n",
      "   * Vision for the future of this field and its potential applications and impact on various industries and society as a whole.\n"
     ]
    }
   ],
   "source": [
    "print(outline2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intro(topic,out):\n",
    "    ins = f'''Here is the outline for a state-of-the-art survey on {topic} that I created earlier: {out}\n",
    "    Using this outline, can you draft an introduction that includes the background, significance of the topic, the aim of the survey, and what the reader can expect \n",
    "    from the paper?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Title: LLM-Based NLG Evaluation: Current Status and Challenges\\n\\nIntroduction:\\n\\nNatural Language Generation (NLG) has emerged as a crucial component in Artificial Intelligence (AI) and Human-Computer Interaction (HCI), enabling machines to generate human-like text based on data or user inputs. One of the most significant advancements in NLG research is the use of Large Language Models (LLMs) to generate natural language responses, leading to impressive improvements in various applications such as customer service, content generation, and education.\\n\\nThe role of LLMs in NLG has gained increasing attention due to their ability to learn from vast amounts of data and generate text that closely resembles human-written language. However, with this growing interest comes the need for rigorous evaluation methods to ensure the quality and effectiveness of these systems. In this survey, we aim to discuss the current trends, methodologies, key findings, and future directions in LLM-based NLG evaluation.\\n\\nThe significance of evaluating LLM-based NLG systems lies in their potential impact on various industries and society as a whole. For instance, in customer service, these systems can help businesses provide personalized responses to customers' queries, enhancing the overall user experience. In content generation, they can be used to create engaging and informative articles or blog posts, while in education, they can assist teachers in generating customized learning materials for students.\\n\\nThe objective of this survey is to provide a comprehensive overview of the current state of LLM-based NLG evaluation, including popular applications, methodologies, key findings, and future directions. We will discuss the growing interest in evaluating these systems, the challenges associated with their assessment, and potential research questions for further exploration. By the end of this paper, readers can expect to gain a deeper understanding of the current trends and future prospects in LLM-based NLG evaluation.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro = intro(topic,outline2)\n",
    "intro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lit(topic,summaries,out):\n",
    "    papers = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    ins=f'''I am working on a survey on {topic}. The outline is as follows: {out}\n",
    "    I have collected the following key papers for the Thematic Overview section:\n",
    "    {papers}\n",
    "    Can you summarize these papers and discuss how they contribute to the state of the art in this area?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" These papers explore different aspects of evaluating large language models (LLMs) for text generation and dialogue systems. Here's a summary of each paper and their contributions:\\n\\n1. BatchEval: The authors propose a new paradigm called BatchEval to address the limitations of current sample-wise evaluation methods, such as sensitivity to prompt design, poor resistance to noise, and inferior ensemble performance with static references. They demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average.\\n\\n2. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators: This study analyzes the effectiveness of LLMs in automatic dialogue evaluation, examining their strengths and limitations. The authors show that strong LLM judges like GPT-4 can match human preferences well, making LLM-as-a-judge a scalable and explainable way to approximate human preferences.\\n\\n3. JudgeLM: The authors propose fine-tuning LLMs as scalable judges (JudgeLM) for evaluating LLMs efficiently and effectively in open-ended benchmarks. They train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors, addressing key biases through various techniques. JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks.\\n\\n4. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: The authors explore using strong LLMs as judges to evaluate chat assistants, addressing the limitations of existing benchmarks in measuring human preferences. They propose solutions for position, verbosity, and self-enhancement biases and show that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\\n\\nThese papers contribute significantly to the state of the art in evaluating LLMs by proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. They demonstrate the potential of using LLMs as judges for various applications and provide insights into improving evaluation techniques.\""
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit = lit(topic,data,outline2)\n",
    "lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " These papers explore different aspects of evaluating large language models (LLMs) for text generation and dialogue systems. Here's a summary of each paper and their contributions:\n",
      "\n",
      "1. BatchEval: The authors propose a new paradigm called BatchEval to address the limitations of current sample-wise evaluation methods, such as sensitivity to prompt design, poor resistance to noise, and inferior ensemble performance with static references. They demonstrate that BatchEval outperforms state-of-the-art methods by 10.5% on Pearson correlations with only 64% API cost on average.\n",
      "\n",
      "2. A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators: This study analyzes the effectiveness of LLMs in automatic dialogue evaluation, examining their strengths and limitations. The authors show that strong LLM judges like GPT-4 can match human preferences well, making LLM-as-a-judge a scalable and explainable way to approximate human preferences.\n",
      "\n",
      "3. JudgeLM: The authors propose fine-tuning LLMs as scalable judges (JudgeLM) for evaluating LLMs efficiently and effectively in open-ended benchmarks. They train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors, addressing key biases through various techniques. JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks.\n",
      "\n",
      "4. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena: The authors explore using strong LLMs as judges to evaluate chat assistants, addressing the limitations of existing benchmarks in measuring human preferences. They propose solutions for position, verbosity, and self-enhancement biases and show that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\n",
      "\n",
      "These papers contribute significantly to the state of the art in evaluating LLMs by proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. They demonstrate the potential of using LLMs as judges for various applications and provide insights into improving evaluation techniques.\n"
     ]
    }
   ],
   "source": [
    "print(lit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def challenges(topic,summaries):\n",
    "    papers = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    ins = f'''Here’s a summary of key papers in the field of {topic}:{papers}\n",
    "    Based on these summaries, can you identify the major trends, common methodologies, and challenges in the current research? What are the gaps that still need to be \n",
    "    addressed?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The major trends in the current research on large language models (LLMs) include:\\n\\n1. Evaluation and benchmarking of LLMs: Researchers are exploring various methods to evaluate the quality of responses generated by LLMs, especially in open-ended scenarios where existing benchmarks may not be comprehensive. This includes using LLMs themselves as evaluators, fine-tuning LLMs as judges, and constructing large and diverse evaluation datasets.\\n2. Robustness and adversarial perturbations: Researchers are investigating the robustness of LLMs in handling various types of adversarial perturbations at both turn and dialogue levels. This includes probing their ability to handle misinformation, biased data, and other forms of adversarial attacks.\\n3. Fairness and bias: There is a growing interest in understanding the fairness and potential biases in LLMs, particularly in areas such as language generation and evaluation. Researchers are exploring methods to mitigate these biases and ensure that LLMs generate responses that align with human preferences.\\n4. Scalability and efficiency: As LLMs become more complex and capable, there is a need to develop scalable and efficient methods for evaluating and fine-tuning them. This includes developing large-scale datasets, using distributed computing resources, and exploring techniques such as transfer learning and model compression.\\n\\nCommon methodologies include the use of deep neural networks, adversarial attacks, and crowdsourcing for data collection and evaluation. Challenges in the current research include the lack of comprehensive evaluation benchmarks, the need to address biases and fairness issues, and the scalability and efficiency of evaluating and fine-tuning LLMs.\\n\\nGaps that still need to be addressed include developing more comprehensive and diverse evaluation datasets, exploring methods to mitigate biases and ensure fairness in LLMs, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs in open-ended scenarios. Additionally, there is a need to explore the ethical implications of using LLMs as judges and evaluators, particularly in areas such as legal decision-making and academic paper reviewing.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "challenges  = challenges(topic,data)\n",
    "challenges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conclusion(topic,intro,l,chall):\n",
    "    ins = f'''Here we have the topic: {topic}. The introduction {intro}. The review {l} and the challenges {chall}.\n",
    "    Can you draft a conclusion that summarizes the key findings and discusses the overall state of the field and future outlook?'''\n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Conclusion:\\n\\nIn this survey, we have explored the current trends, methodologies, key findings, and challenges in evaluating Large Language Models (LLMs) for Natural Language Generation (NLG). The growing interest in LLM-based NLG systems is driven by their potential impact on various industries and society as a whole. However, with this increasing attention comes the need for rigorous evaluation methods to ensure their quality and effectiveness.\\n\\nThe papers reviewed in this survey demonstrate significant progress in evaluating LLMs for text generation and dialogue systems. They propose new methods such as BatchEval, JudgeLM, and using LLMs as judges, analyze their strengths and limitations, and address challenges related to open-ended scenarios and measuring human preferences. These contributions provide valuable insights into the potential of using LLMs as evaluators and judges for various applications.\\n\\nThe current research on LLMs is focused on evaluation and benchmarking, robustness and adversarial perturbations, fairness and bias, and scalability and efficiency. Common methodologies include deep neural networks, adversarial attacks, and crowdsourcing. Challenges in the field include the lack of comprehensive evaluation benchmarks, addressing biases and fairness issues, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs.\\n\\nGaps that still need to be addressed include creating more comprehensive and diverse evaluation datasets, exploring methods to mitigate biases and ensure fairness in LLMs, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs in open-ended scenarios. Additionally, there is a need to explore the ethical implications of using LLMs as judges and evaluators, particularly in areas such as legal decision-making and academic paper reviewing.\\n\\nIn conclusion, the field of LLM-based NLG evaluation is rapidly advancing, with significant progress being made in proposing new methods, analyzing their strengths and limitations, and addressing challenges related to open-ended scenarios and measuring human preferences. However, there are still gaps that need to be addressed, including creating more comprehensive evaluation datasets, exploring methods to mitigate biases and ensure fairness, and developing more efficient and scalable methods for evaluating and fine-tuning LLMs. The future outlook is promising, with the potential for LLMs to revolutionize various industries and society as a whole, but it is crucial that we continue to invest in rigorous evaluation methods to ensure their quality and effectiveness.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con = conclusion(topic,intro,lit,challenges)\n",
    "con "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conmbine(intro,lit,challenges,con,out):\n",
    "    ins = f''' Based on the introduction {intro}, the review {lit}, the challenges {challenges} and the conclusion {con}. \n",
    "    Combine it to a fluent paper with the outline {out}.''' \n",
    "    answer  = llm.invoke(ins)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_survey  = conmbine(intro,lit,challenges,con,outline2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(combined_survey,'combined_survey.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine = intro+'/n'+lit+'/n'+challenges+'/n'+con\n",
    "store_summary(combine,'combine.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
