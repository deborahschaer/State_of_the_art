{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine several papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n",
    "\n",
    "\n",
    "def store_summary(survey1,filename):\n",
    "    with open(f'summaries/'+filename+'.txt', 'w') as file:\n",
    "        file.write(survey1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-based NLG Evaluation: Current Status and Challenges'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n",
    "original = get_json('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'full_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_n(papers):\n",
    "    l = [key+':  '+value for key,value in papers.items()]\n",
    "    random.shuffle(l)\n",
    "    context = ('/n/n').join(l)\n",
    "    instruction = 'summarize the following papers:/n'+context+'/n summarize them.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In response to the user question \"What are the most effective ways to deal with stress?\", Assistant 1 provided an answer that contained information about effective ways to manage stress, such as regular exercise, maintaining a healthy diet, getting enough sleep, and engaging in relaxation activities like meditation, as well as having a support system. The response was clear, concise, and relevant to the question.\\n\\nAssistant 2\\'s answer also provided valid suggestions for dealing with stress, including taking deep breaths, regular physical exercise, maintaining a balanced diet, and finding hobbies or activities that bring enjoyment. However, there were several grammatical errors in the response, such as \"Whenevr feeling stressed\" instead of \"When feeling stressed\", \"always remember to take deep breathes\" instead of \"remember to take deep breaths when feeling stressed\", and \"This can calm nerves\" instead of \"Taking deep breaths can calm nerves\". Additionally, there were some minor factual errors, such as \"reduce stress hormonal levels\" instead of \"lower stress hormone levels\" and \"finding hobby or activity you enjoy\" instead of \"find a hobby or activity you enjoy\".\\n\\nBased on the given criteria, Assistant 1\\'s response was more helpful, relevant, accurate, and detailed compared to Assistant 2\\'s response due to its clearer presentation and absence of errors. Therefore, I would rate Assistant 1 as having a higher score in all categories.\\n\\nTherefore, my evaluation is:\\nAssistant 1: Helpfulness - 5, Relevance - 5, Accuracy - 5, Level of detail - 5\\nAssistant 2: Helpfulness - 3, Relevance - 4, Accuracy - 3, Level of detail - 4\\n\\nI would choose 1 as my answer.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longsummary = summarize_n(data)\n",
    "longsummary\n",
    "#type(random.shuffle([key+':  '+value for key,value in data.items()]))\n",
    "#complete fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499106"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is the problem that context is too big? \n",
    "l = [key+':  '+value for key,value in data.items()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context) #3'924'350 characters. This is way too big as \n",
    "len(context.split(' ')) # 499'106 words but only 128'000 in theory are allowed. The llm was trained with 8'000 words in the prompt.\n",
    "#yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try with only abstracts. Is it short enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7956"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [value['title']+':  '+value['abstract'] for value in data.values()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context.split(' ')) \n",
    "#7956 is short enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(summaries):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Write a summary combining the key findings of following texts: \\n {texts}'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This text discusses several research papers related to evaluating and using large language models (LLMs) as judges for various tasks. The first paper explores the use of wider and deeper networks for fairer LLM evaluations, drawing inspiration from deep neural networks and academic paper reviewing. The second paper proposes using strong LLMs as judges for evaluating other LLMs on more open-ended questions and introduces two benchmarks: MT-bench and Chatbot Arena. The third paper proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks.\\n\\nThe first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\" discusses the challenges of evaluating the quality of responses generated by LLMs and proposes a method using an LLM itself to make evaluations through multiple independent evaluations. The authors explore whether deeper and wider networks can lead to fairer evaluations, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts. They construct the largest and most diverse English evaluation benchmark, LLMEval$^2$, and demonstrate that a wider network with two layers performs the best.\\n\\nThe second paper, \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\" addresses the challenges of evaluating LLM-based chat assistants using existing benchmarks and proposes using strong LLMs as judges to evaluate these models on more open-ended questions. The authors examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, and propose solutions to mitigate some of them. They verify the agreement between LLM judges and human preferences using MT-bench and Chatbot Arena and demonstrate that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\\n\\nThe third paper, \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges,\" proposes fine-tuning LLMs as scalable judges to evaluate LLMs efficiently and effectively in open-ended benchmarks. The authors propose a comprehensive, large-scale, high-quality dataset for fine-tuning high-performance judges and introduce techniques such as swap augmentation, reference support, and reference drop to address key biases in fine-tuning LLM as a judge. They demonstrate that JudgeLM obtains the state-of-the-art judge performance on both existing and new benchmarks and is efficient, with JudgeLM-7B only needing 3 minutes to judge 5K samples with 8 A100 GPUs.\\n\\nAll resources for these papers are available at https://github.com/e0397123/comp-analysis.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_summary = write_summary(data)\n",
    "simple_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(simple_summary,'simple_summary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete Problems in AI Safety\n",
      "PaLM 2 Technical Report\n",
      "Benchmarking Foundation Models with Language-Model-as-an-Examiner\n",
      "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
      "BooookScore: A systematic exploration of book-length summarization in the era of LLMs\n",
      "Scaling Instruction-Finetuned Language Models\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "GPTScore: Evaluate as You Desire\n",
      "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\n",
      "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\n",
      "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\n",
      "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences\n",
      "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks\n",
      "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
      "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\n",
      "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n",
      "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task\n",
      "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics\n",
      "Generative Judge for Evaluating Alignment\n",
      "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation\n",
      "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\n",
      "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\n",
      "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization\n",
      "On Learning to Summarize with Large Language Models as References\n",
      "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response\n",
      "Calibrating LLM-Based Evaluator\n",
      "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models\n",
      "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization\n",
      "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "Self-critiquing models for assisting human evaluators\n",
      "Fusion-Eval: Integrating Assistant Evaluators with LLMs\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Large Language Models are not Fair Evaluators\n",
      "Shepherd: A Critic for Language Model Generation\n",
      "Automated Evaluation of Personalized Text Generation using Large Language Models\n",
      "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\n",
      "Automatic Answerability Evaluation for Question Generation\n",
      "Style Over Substance: Evaluation Biases for Large Language Models\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\n",
      "BatchEval: Towards Human-like Text Evaluation\n",
      "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators\n",
      "Wider and Deeper LLM Networks are Fairer LLM Evaluators\n",
      "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
      "JudgeLM: Fine-tuned Large Language Models are Scalable Judges\n"
     ]
    }
   ],
   "source": [
    "print(('\\n').join([s['title'] for s in data.values()]))\n",
    "#The first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\"... \n",
    "#=> it did only consider the last three papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_topic(summaries,topic):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-based NLG Evaluation: Current Status and Challenges - Wider and Deeper Networks for Fairer Evaluators, LLM-as-a-Judge, and JudgeLM\\n\\nThis topic covers recent research on evaluating Large Language Models (LLMs) in Natural Language Generation (NLG) tasks. The studies explore various approaches to improve the evaluation process, including wider and deeper networks for fairer evaluators, using LLMs as judges, and fine-tuning LLMs as scalable judges.\\n\\n1. Wider and Deeper LLM Networks are Fairer LLM Evaluators:\\nThis paper proposes a novel approach to use the LLM itself to make evaluations and improve fairness by designing a network that resembles academic paper reviewing. The network consists of multiple layers, with each layer receiving representations from all neurons in the previous layer, integrating locally learned evaluation information for more comprehensive results. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers performs best, improving kappa correlation coefficient significantly.\\n\\n2. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena:\\nThis study explores the usage of strong LLMs as judges to evaluate other models on more open-ended questions. The paper discusses position, verbosity, self-enhancement biases, and limited reasoning ability of LLM judges and proposes solutions to mitigate some of these issues. Results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement.\\n\\n3. JudgeLM: Fine-tuned Large Language Models are Scalable Judges:\\nThis research proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. The study trains JudgeLM at different scales, analyzes its capabilities and biases, and introduces techniques to address these issues. Results show that JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks, is efficient, and demonstrates extended capabilities as a judge for single answer, multimodal models, multiple answers, and multi-turn chat.'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview = write_summary_with_topic(data,topic)\n",
    "store_summary(simple_overview,'simple_overview')\n",
    "simple_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
