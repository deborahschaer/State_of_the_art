{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine several papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n",
    "\n",
    "\n",
    "\n",
    "#store the summaries\n",
    "os.makedirs('summaries/'+paper_id,exist_ok=True)\n",
    "def store_summary(content,filename):\n",
    "    path = 'summaries/'+paper_id\n",
    "    with open(path+'/'+filename, 'w') as file:\n",
    "        file.write(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM-based NLG Evaluation: Current Status and Challenges'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = get_json('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'full_texts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_n(papers):\n",
    "    l = [key+':  '+value for key,value in papers.items()]\n",
    "    random.shuffle(l)\n",
    "    context = ('/n/n').join(l)\n",
    "    instruction = 'summarize the following papers:/n'+context+'/n summarize them.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The first answer about defensive strategies in basketball is rated as having a score of 3 out of 5. The accuracy of the answer is assessed as 3, indicating that it correctly identifies the World Health Organization (WHO) as the primary organization responsible for global health policies and guidelines. However, the coherence, factuality, and comprehensiveness scores are lower due to some errors in the description of defensive strategies and a lack of detail about the process by which WHO develops and implements these policies.\\n\\nThe second answer regarding attachment styles and romantic relationships is rated as having a score of 4 out of 5. The accuracy of the answer is assessed as 3, indicating that it correctly describes how attachment style influences romantic relationships. However, the coherence, factuality, and comprehensiveness scores are higher due to the clear and detailed explanation of each attachment style and its impact on relationships.\\n\\nThe third question asks about the evolution of machine learning algorithms, deep neural networks, and core AI techniques over the next 10-20 years and what new approaches might fundamentally change the landscape of artificial intelligence research and development. The answer is rated as having a score of 5 out of 5. The accuracy of the answer is assessed as 5, indicating that it correctly describes how advancements in hardware, data availability, and algorithmic innovation will drive progress in machine learning algorithms, deep neural networks, and core AI techniques. Additionally, the answer accurately identifies new approaches such as reinforcement learning, transfer learning, and generative models as likely to revolutionize the field by enabling more sophisticated decision-making, faster learning, and enhanced performance across a range of applications. The coherence, factuality, and comprehensiveness scores are also high due to the clear and detailed explanation of each advancement and its potential impact on AI research and development.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "longsummary = summarize_n(data)\n",
    "longsummary\n",
    "#type(random.shuffle([key+':  '+value for key,value in data.items()]))\n",
    "#complete fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(longsummary,'fulltext_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499106"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Is the problem that context is too big? \n",
    "l = [key+':  '+value for key,value in data.items()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context) #3'924'350 characters. This is way too big as \n",
    "len(context.split(' ')) # 499'106 words but only 128'000 in theory are allowed. The llm was trained with 8'000 words in the prompt.\n",
    "#yes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try with only abstracts. Is it short enough?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get full texts of related papers\n",
    "data = get_json('dataset/'+paper_id+'ref')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7956"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [value['title']+':  '+value['abstract'] for value in data.values()]\n",
    "context = ('/n/n').join(l)\n",
    "len(context.split(' ')) \n",
    "#7956 is short enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary(summaries):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Write a summary combining the key findings of following texts: \\n {texts}'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' This text discusses several research papers related to evaluating and using large language models (LLMs) as judges for various tasks. The first paper explores the use of wider and deeper networks for fairer LLM evaluations, drawing inspiration from deep neural networks and academic paper reviewing. The second paper proposes using strong LLMs as judges for evaluating other LLMs on more open-ended questions and introduces two benchmarks: MT-bench and Chatbot Arena. The third paper proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks.\\n\\nThe first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\" discusses the challenges of evaluating the quality of responses generated by LLMs and proposes a method using an LLM itself to make evaluations through multiple independent evaluations. The authors explore whether deeper and wider networks can lead to fairer evaluations, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts. They construct the largest and most diverse English evaluation benchmark, LLMEval$^2$, and demonstrate that a wider network with two layers performs the best.\\n\\nThe second paper, \"Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,\" addresses the challenges of evaluating LLM-based chat assistants using existing benchmarks and proposes using strong LLMs as judges to evaluate these models on more open-ended questions. The authors examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, and propose solutions to mitigate some of them. They verify the agreement between LLM judges and human preferences using MT-bench and Chatbot Arena and demonstrate that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.\\n\\nThe third paper, \"JudgeLM: Fine-tuned Large Language Models are Scalable Judges,\" proposes fine-tuning LLMs as scalable judges to evaluate LLMs efficiently and effectively in open-ended benchmarks. The authors propose a comprehensive, large-scale, high-quality dataset for fine-tuning high-performance judges and introduce techniques such as swap augmentation, reference support, and reference drop to address key biases in fine-tuning LLM as a judge. They demonstrate that JudgeLM obtains the state-of-the-art judge performance on both existing and new benchmarks and is efficient, with JudgeLM-7B only needing 3 minutes to judge 5K samples with 8 A100 GPUs.\\n\\nAll resources for these papers are available at https://github.com/e0397123/comp-analysis.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_summary = write_summary(data)\n",
    "simple_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(simple_summary,'simple_summary.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concrete Problems in AI Safety\n",
      "PaLM 2 Technical Report\n",
      "Benchmarking Foundation Models with Language-Model-as-an-Examiner\n",
      "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate\n",
      "BooookScore: A systematic exploration of book-length summarization in the era of LLMs\n",
      "Scaling Instruction-Finetuned Language Models\n",
      "RAGAS: Automated Evaluation of Retrieval Augmented Generation\n",
      "GPTScore: Evaluate as You Desire\n",
      "Are Large Language Models Reliable Judges? A Study on the Factuality Evaluation Capabilities of LLMs\n",
      "CoAScore: Chain-of-Aspects Prompting for NLG Evaluation\n",
      "ALLURE: Auditing and Improving LLM-based Evaluation of Text using Iterative In-Context-Learning\n",
      "SocREval: Large Language Models with the Socratic Method for Reference-Free Reasoning Evaluation\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences\n",
      "TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks\n",
      "CritiqueLLM: Towards an Informative Critique Generation Model for Evaluation of Large Language Model Generation\n",
      "Prometheus: Inducing Fine-grained Evaluation Capability in Language Models\n",
      "EvalLM: Interactive Evaluation of Large Language Model Prompts on User-Defined Criteria\n",
      "Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task\n",
      "The Eval4NLP 2023 Shared Task on Prompting Large Language Models as Explainable Metrics\n",
      "Generative Judge for Evaluating Alignment\n",
      "Collaborative Evaluation: Exploring the Synergy of Large Language Models and Humans for Open-ended Generation Evaluation\n",
      "PRD: Peer Rank and Discussion Improve Large Language Model based Evaluations\n",
      "Split and Merge: Aligning Position Biases in Large Language Model based Evaluators\n",
      "Benchmarking Generation and Evaluation Capabilities of Large Language Models for Instruction Controllable Summarization\n",
      "On Learning to Summarize with Large Language Models as References\n",
      "Evaluate What You Can't Evaluate: Unassessable Quality for Generated Response\n",
      "Calibrating LLM-Based Evaluator\n",
      "LLM Comparative Assessment: Zero-shot NLG Evaluation through Pairwise Comparisons using Large Language Models\n",
      "ChatGPT as a Factual Inconsistency Evaluator for Text Summarization\n",
      "Branch-Solve-Merge Improves Large Language Model Evaluation and Generation\n",
      "Self-critiquing models for assisting human evaluators\n",
      "Fusion-Eval: Integrating Assistant Evaluators with LLMs\n",
      "Llama 2: Open Foundation and Fine-Tuned Chat Models\n",
      "Large Language Models are not Fair Evaluators\n",
      "Shepherd: A Critic for Language Model Generation\n",
      "Automated Evaluation of Personalized Text Generation using Large Language Models\n",
      "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization\n",
      "Automatic Answerability Evaluation for Question Generation\n",
      "Style Over Substance: Evaluation Biases for Large Language Models\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\n",
      "BatchEval: Towards Human-like Text Evaluation\n",
      "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators\n",
      "Wider and Deeper LLM Networks are Fairer LLM Evaluators\n",
      "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena\n",
      "JudgeLM: Fine-tuned Large Language Models are Scalable Judges\n"
     ]
    }
   ],
   "source": [
    "print(('\\n').join([s['title'] for s in data.values()]))\n",
    "#The first paper, \"Wider and Deeper LLM Networks are Fairer LLM Evaluators,\"... \n",
    "#=> it did only consider the last three papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_topic(summaries,topic):\n",
    "    texts = ('\\n').join([s['title']+':  '+s['abstract'] for s in summaries.values()])\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: LLM-based NLG Evaluation: Current Status and Challenges - Wider and Deeper Networks for Fairer Evaluators, LLM-as-a-Judge, and JudgeLM\\n\\nThis topic covers recent research on evaluating Large Language Models (LLMs) in Natural Language Generation (NLG) tasks. The studies explore various approaches to improve the evaluation process, including wider and deeper networks for fairer evaluators, using LLMs as judges, and fine-tuning LLMs as scalable judges.\\n\\n1. Wider and Deeper LLM Networks are Fairer LLM Evaluators:\\nThis paper proposes a novel approach to use the LLM itself to make evaluations and improve fairness by designing a network that resembles academic paper reviewing. The network consists of multiple layers, with each layer receiving representations from all neurons in the previous layer, integrating locally learned evaluation information for more comprehensive results. Experimental results demonstrate that a wider network (involving many reviewers) with 2 layers performs best, improving kappa correlation coefficient significantly.\\n\\n2. Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena:\\nThis study explores the usage of strong LLMs as judges to evaluate other models on more open-ended questions. The paper discusses position, verbosity, self-enhancement biases, and limited reasoning ability of LLM judges and proposes solutions to mitigate some of these issues. Results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement.\\n\\n3. JudgeLM: Fine-tuned Large Language Models are Scalable Judges:\\nThis research proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. The study trains JudgeLM at different scales, analyzes its capabilities and biases, and introduces techniques to address these issues. Results show that JudgeLM obtains state-of-the-art judge performance on existing and new benchmarks, is efficient, and demonstrates extended capabilities as a judge for single answer, multimodal models, multiple answers, and multi-turn chat.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview = write_summary_with_topic(data,topic)\n",
    "store_summary(simple_overview,'simple_overview.txt')\n",
    "simple_overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#As well only considers the last 3 papers. Is it because they are the last three papers or because they are in llm's opinion the most relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_summary_with_shuffle(summaries,topic):\n",
    "    l = [s['title']+':  '+s['abstract'] for s in summaries.values()]\n",
    "    random.shuffle(l)\n",
    "    print('last 3 papers:',l[-3])\n",
    "    print(l[-2])\n",
    "    print(l[-1])\n",
    "    texts = ('\\n').join(l)\n",
    "    instruction = f'Use this {texts} to create an overview over the topic {topic}.'\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last 3 papers: Branch-Solve-Merge Improves Large Language Model Evaluation and Generation:  Large Language Models (LLMs) are frequently used for multi-faceted language\n",
      "generation and evaluation tasks that involve satisfying intricate user\n",
      "constraints or taking into account multiple aspects and criteria. However,\n",
      "their performance can fall short, due to the model's lack of coherence and\n",
      "inability to plan and decompose the problem. We propose Branch-Solve-Merge\n",
      "(BSM), a Large Language Model program (Schlag et al., 2023) for tackling such\n",
      "challenging natural language tasks. It consists of branch, solve, and merge\n",
      "modules that are parameterized with specific prompts to the base LLM. These\n",
      "three modules plan a decomposition of the task into multiple parallel\n",
      "sub-tasks, independently solve them, and fuse the solutions to the sub-tasks.\n",
      "We apply our method to the tasks of LLM response evaluation and constrained\n",
      "text generation and evaluate its effectiveness with multiple LLMs, including\n",
      "Vicuna, LLaMA-2-chat, and GPT-4. BSM improves the evaluation correctness and\n",
      "consistency for each LLM by enhancing human-LLM agreement by up to 26%,\n",
      "reducing length and pairwise position biases by up to 50%, and allowing\n",
      "LLaMA2-chat to match or outperform GPT-4 on most domains. On a constraint story\n",
      "generation task, BSM improves the coherence of stories while also improving\n",
      "constraint satisfaction by 12%.\n",
      "Exploring ChatGPT's Ability to Rank Content: A Preliminary Study on Consistency with Human Preferences:  As a natural language assistant, ChatGPT is capable of performing various\n",
      "tasks, including but not limited to article generation, code completion, and\n",
      "data analysis. Furthermore, ChatGPT has consistently demonstrated a remarkable\n",
      "level of accuracy and reliability in terms of content evaluation, exhibiting\n",
      "the capability of mimicking human preferences. To further explore ChatGPT's\n",
      "potential in this regard, a study is conducted to assess its ability to rank\n",
      "content. In order to do so, a test set consisting of prompts is created,\n",
      "covering a wide range of use cases, and five models are utilized to generate\n",
      "corresponding responses. ChatGPT is then instructed to rank the responses\n",
      "generated by these models. The results on the test set show that ChatGPT's\n",
      "ranking preferences are consistent with human to a certain extent. This\n",
      "preliminary experimental finding implies that ChatGPT's zero-shot ranking\n",
      "capability could be used to reduce annotation pressure in a number of ranking\n",
      "tasks.\n",
      "FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets:  Evaluation of Large Language Models (LLMs) is challenging because\n",
      "instruction-following necessitates alignment with human values and the required\n",
      "set of skills varies depending on the instruction. However, previous studies\n",
      "have mainly focused on coarse-grained evaluation (i.e. overall preference-based\n",
      "evaluation), which limits interpretability since it does not consider the\n",
      "nature of user instructions that require instance-wise skill composition. In\n",
      "this paper, we introduce FLASK (Fine-grained Language Model Evaluation based on\n",
      "Alignment Skill Sets), a fine-grained evaluation protocol for both human-based\n",
      "and model-based evaluation which decomposes coarse-level scoring to a skill\n",
      "set-level scoring for each instruction. We experimentally observe that the\n",
      "fine-graininess of evaluation is crucial for attaining a holistic view of model\n",
      "performance and increasing the reliability of the evaluation. Using FLASK, we\n",
      "compare multiple open-source and proprietary LLMs and observe a high\n",
      "correlation between model-based and human-based evaluations. We publicly\n",
      "release the evaluation data and code implementation at\n",
      "https://github.com/kaistAI/FLASK.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Title: Recent Advances in Large Language Model Evaluation: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK\\n\\nIn recent years, there has been a surge of interest in evaluating the performance of large language models (LLMs) in natural language generation (NLG) tasks. In this summary, we will discuss four recent studies that have made significant strides in developing new methods for evaluating LLMs: TIGERScore, EvalLM, Branch-Solve-Merge, and FLASK.\\n\\n1. TIGERScore: A Universal Explainable Metric for NLG Tasks\\nTIGERScore is a reference-free metric that evaluates the rationality of explanations generated by LLMs. The study demonstrates that TIGERScore's correlation with human ratings is high and approaches that of GPT-4 evaluators. Human evaluation of the generated explanations showed an accuracy of 70.8%. The researchers believe that TIGERScore shows the potential for building universal explainable metrics to evaluate any NLG task.\\n\\n2. EvalLM: Interactive System for Iteratively Refining Prompts by Evaluating Multiple Outputs on User-Defined Criteria\\nEvalLM is an interactive system designed to help developers refine prompts for LLMs by evaluating multiple outputs based on user-defined criteria. The study shows that EvalLM helps participants compose more diverse criteria, examine twice as many outputs, and reach satisfactory prompts with 59% fewer revisions compared to manual evaluation.\\n\\n3. Branch-Solve-Merge: Improving Large Language Model Evaluation and Generation\\nBranch-Solve-Merge (BSM) is a program that uses branch, solve, and merge modules to tackle challenging NLG tasks by planning a decomposition of the task into multiple parallel sub-tasks, independently solving them, and fusing the solutions. The study shows that BSM enhances human-LLM agreement, reduces length and pairwise position biases, and allows LLMs to match or outperform GPT-4 on most domains.\\n\\n4. FLASK: Fine-grained Language Model Evaluation based on Alignment Skill Sets\\nFLASK is a fine-grained evaluation protocol for both human-based and model-based evaluation that decomposes coarse-level scoring to skill set-level scoring for each instruction. The study shows that the fine-graininess of evaluation is crucial for attaining a holistic view of model performance and increasing the reliability of the evaluation. Using FLASK, multiple open-source and proprietary LLMs were compared, resulting in high correlation between model-based and human-based evaluations.\\n\\nThese studies represent significant advancements in the field of LLM-based NLG evaluation and demonstrate the potential for developing more effective methods for evaluating and refining LLMs for various applications. For more information on these studies, please visit the project websites provided:\\n\\n* TIGERScore: <https://tiger-ai-lab.github.io/TIGERScore/>\\n* EvalLM: N/A (The study was published in arXiv and does not have a dedicated website)\\n* Branch-Solve-Merge: <https://arxiv.org/abs/2304.13958>\\n* FLASK: <https://github.com/kaistAI/FLASK>\""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_overview_s = write_summary_with_shuffle(data,topic)\n",
    "store_summary(simple_overview_s,'simple_overview_shuffled.txt')\n",
    "simple_overview_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
