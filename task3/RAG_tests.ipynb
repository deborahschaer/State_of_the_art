{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "https://github.com/pixegami/langchain-rag-tutorial/blob/main/query_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "import os\n",
    "import shutil\n",
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paper_id = \"2402.01383v1\"\n",
    "#paper_id = \"2408.02304\"\n",
    "#paper_id = \"2402.06196\"\n",
    "#paper_id = \"2409.09957\"\n",
    "paper_id = \"2409.15180\"\n",
    "CHROMA_PATH = paper_id+\"/chroma\"\n",
    "#CHROMA_PATH = paper_id+\"/chroma300\"\n",
    "DATA_PATH = paper_id\n",
    "file = 'rag_largechroma'\n",
    "#file = 'rag300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_summary(survey1,filename):\n",
    "    with open(f'summaries/'+paper_id+'/'+filename+'.txt', 'w') as file:\n",
    "        file.write(survey1)\n",
    "def write_json(documents_data,file_name):\n",
    "    converted_data = [dict(item) for item in documents_data['source_documents']]\n",
    "    dict_data = {'answer':documents_data['result'],'source':converted_data}\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '2409.15180/2311.15308v2.AV_Deepfake1M__A_Large_Scale_LLM_Driven_Audio_Visual_Deepfake_Dataset.pdf'}, page_content='4 2 0 2\\n\\nl u J\\n\\n9 2\\n\\n]\\n\\nV C . s c [\\n\\n2 v 8 0 3 5 1 . 1 1 3 2 : v i X r a\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nZhixi Cai Monash University Melbourne, Australia zhixi.cai@monash.edu\\n\\nShreya Ghosh Curtin University Perth, Australia shreya.ghosh@curtin.edu.au\\n\\nAman Pankaj Adatia Indian Institute of Technology Ropar Ropar, India 2020csb1154@iitrpr.ac.in\\n\\nMunawar Hayat Qualcomm San Diego, United States hayat@qti.qualcomm.com\\n\\nAbhinav Dhall Flinders University Adelaide, Australia abhinav.dhall@flinders.edu.au\\n\\nTom Gedeon Curtin University Perth, Australia tom.gedeon@curtin.edu.au\\n\\nKalin Stefanov Monash University Melbourne, Australia kalin.stefanov@monash.edu\\n\\nFake\\n\\n......\\n\\nthe terrible songbookand not unique\\n\\ngoing to\\n\\n.....................ReplacementDeletionInsertion...\\n\\nDeeper ForensicsDF-TIMITUADFV\\n\\nI\\'m not\\n\\nDFDFakeAVCelebCeleb-DFASVSpoof2021-DFNumber of SubjectsNumber of Real VideosDFDCDF-PlatterForgeryNetAV-Deepfake1MLAV-DF\\n\\nthe great songbookI\\'m not going toand unique\\n\\n......\\n\\n1M500K200K100K10K1KVideo Amount\\n\\nModality\\n\\nVisual-OnlyAudio-OnlyAudio-Visual\\n\\nReal\\n\\nFigure 1: AV-Deepfake1M is a large-scale content-driven deepfake dataset generated by utilising a large language model. The dataset contains more than 2K subjects and 1M deepfake videos generated by employing different audio-visual content manipulation strategies. The left figure illustrates examples of word-level replacement, deletion, and insertion to manipulate audio-visual content. The right figure provides a comparison between AV-Deepfake1M and other publicly available datasets in terms of number of subjects, and amount of real and fake videos.\\n\\nABSTRACT The detection and localization of highly realistic deepfake audio- visual content are challenging even for the most advanced state-of- the-art methods. While most of the research efforts in this domain are focused on detecting high-quality deepfake images and videos, only a few works address the problem of the localization of small segments of audio-visual manipulations embedded in real videos. In this research, we emulate the process of such content generation and propose the AV-Deepfake1M dataset. The dataset contains content- driven (i) video manipulations, (ii) audio manipulations, and (iii)\\n\\naudio-visual manipulations for more than 2K subjects resulting in a total of more than 1M videos. The paper provides a thorough description of the proposed data generation pipeline accompanied by a rigorous analysis of the quality of the generated data. The comprehensive benchmark of the proposed dataset utilizing state- of-the-art deepfake detection and localization methods indicates a significant drop in performance compared to previous datasets. The proposed dataset will play a vital role in building the next-generation deepfake localization methods. The dataset and associated code are available at https://github.com/ControlNet/AV-Deepfake1M.\\n\\nPermission to make digital or hard copies of part or all of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for third-party components of this work must be honored. For all other uses, contact the owner/author(s). MM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia Â© 2024 Copyright held by the owner/author(s). ACM ISBN 979-8-4007-0686-8/24/10 https://doi.org/10.1145/3664647.3680795\\n\\nCCS CONCEPTS â€¢ Computing methodologies â†’ Computer vision; â€¢ Security and privacy â†’ Social aspects of security and privacy; Usability in security and privacy.\\n\\nKEYWORDS Datasets, Deepfake, Localization, Detection\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nCai et al.\\n\\nTable 1: Details for publicly available deepfake datasets in a chronologically ascending order. Cla: Binary classification, SL: Spatial localization, TL: Temporal localization, FS: Face swapping, RE: Face reenactment, TTS: Text-to-speech, VC: Voice conversion.\\n\\nDataset\\n\\nYear\\n\\nTasks\\n\\nManipulated Modality\\n\\nManipulation Method\\n\\n#Subjects\\n\\n#Real\\n\\n#Fake\\n\\n#Total\\n\\nDF-TIMIT [34] UADFV [70] FaceForensics++ [50] Google DFD [42] DFDC [16] DeeperForensics [28] Celeb-DF [38] WildDeepfake [78] FFIW10ð¾ [77] KoDF [35] FakeAVCeleb [31] ForgeryNet [22] ASVSpoof2021DF [39] LAV-DF [6] DF-Platter [41] AV-Deepfake1M\\n\\n2018 2019 2019 2019 2020 2020 2020 2020 2021 2021 2021 2021 2021 2022 2023 2023\\n\\nCla Cla Cla Cla Cla Cla Cla Cla Cla/SL Cla Cla SL/TL/Cla Cla TL/Cla Cla TL/Cla\\n\\nV V V V AV V V - V V AV V A AV V AV\\n\\nFS FS FS/RE FS FS FS FS - FS FS/RE RE Random FS/RE TTS/VC Content-driven RE/TTS FS Content-driven RE/TTS\\n\\n43 49 - 5 960 100 59 - - 403 600+ 5,400+ 160 153 454 2,068\\n\\n320 49 1,000 363 23,654 50,000 590 3,805 10,000 62,166 570 99,630 20,637 36,431 133,260 286,721\\n\\n640 49 4,000 3,068 104,500 10,000 5,639 3,509 10,000 175,776 25,000+ 121,617 572,616 99,873 132,496 860,039\\n\\n960 98 5,000 3,431 128,154 60,000 6,229 7,314 20,000 237,942 25,500+ 221,247 593,253 136,304 265,756 1,146,760\\n\\n1 We are witnessing rapid progress in the domain of content generation technology, i.e., models trained on massive amounts of data that can produce highly realistic text [3, 59], video [19, 57, 67] and audio [29, 30, 53]. Consequently, discriminating between real and fake content is becoming increasingly more challenging even for humans [41, 77]. This opens the door for misuse of content generation technology for example to spread misinformation and commit fraud, rendering the development of reliable detection methods vital.\\n\\nINTRODUCTION\\n\\nWe propose AV-Deepfake1M, a large content-driven audio- visual dataset for the task of temporal deepfake localization. â€¢ We propose an elaborate data generation pipeline employing novel manipulation strategies and incorporating the state-of- the-art in text, video and audio generation.\\n\\nWe perform comprehensive analysis and benchmark of the proposed dataset utilizing state-of-the-art deepfake detection and localization methods.\\n\\nThe development of such methods is highly dependent on the available deepfake benchmark datasets, which led to the steady in- crease in the number of publicly available datasets that provide examples of visual-only [28, 35, 38], audio-only [39, 71], and audio- visual [31] content modification strategies (e.g., face-swapping, face- reenactment, etc.). However, the majority of these datasets and meth- ods assume that the entirety of the content (i.e., audio, visual, audio- visual) is either real or fake. This leaves the door open for criminals to exploit the embedding of small segments of manipulations in the otherwise real content. As argued in [6], this type of targeted ma- nipulation can lead to drastic changes in the underlying meaning as illustrated in Figure 1. Given that most deepfake benchmark datasets do not include this type of partial manipulations, detection methods might fail to perform reliably on this new type of deepfake content. This work addresses this gap by releasing a new large-scale audio- visual dataset called AV-Deepfake1M specifically designed for the task of temporal deepfake localization. To improve the realism and quality of generated content, the proposed data generation pipeline incorporates the ChatGPT1 large language model. The pipeline fur- ther utilizes the latest open-source state-of-the-art methods for high- quality audio [8, 33] and video [62] generation. The scale and novel modification strategies position the proposed dataset as the most comprehensive audio-visual benchmark as illustrated in Figure 1, making it an important asset for building the next generation of deepfake localization methods. The main contributions are,\\n\\n1https://chat.openai.com/\\n\\n2 RELATED WORK The performance of any deepfake detection method is highly de- pendent on the quantitative and qualitative aspects of the datasets used for development [18, 21, 25, 40, 44, 47, 49, 51, 52, 61, 63, 69, 72, 75]. Over the past few years, many datasets (e.g., [22, 34, 41]) have been proposed to support the research on deepfake detection. A comprehensive list of the relevant publicly available datasets is given in Table 1. Most of the available datasets provide examples of face manipulations through either face swapping [16, 34, 77] or face reenactment [31, 35] techniques. In terms of the number of samples, earlier datasets are smaller due to the limited availability of face ma- nipulation techniques. With the rapid advancements in content gen- eration technology, several large-scale datasets such as DFDC [16], DeeperForensics [28], KoDF [35], and DF-Platter [41] have been proposed. However, the task associated with these datasets is mainly restricted to coarse-level deepfake detection. Until this point manip- ulations are mainly applied only to the visual modality, and later, audio manipulations [39] and audio-visual manipulations [31] have been proposed to increase the complexity of the task.\\n\\nIn 2022, LAV-DF [6] was introduced to become the first content- driven deepfake dataset for temporal localization. However, the qual- ity and scale of LAV-DF are limited, and the state-of-the-art methods designed for temporal localization [4, 75] are already achieving very strong performance. AV-Deepfake1M addresses these gaps by improving the quality, diversity, and scale of the previous datasets de- signed for temporal deepfake localization. Given that LAV-DF is the only available public dataset that has been designed for the same task\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nAudio GenerationTTS OptionsTranscript ManipulationVideo Generation\\n\\nFake Frames Fake Audio\\n\\nInsertDelete\\n\\nReplace\\n\\nReal AudioFake Speech Word \\x00Raw Output)Fake SpeechFake VideoReal VideoFake AudioFake FramesFake Frames Fake AudioFake Frames Bg NoiseFake Speech Full SentenceSpeechCollectFinetuneStrip zero signalsLoudness normAll Speech of the SubjectBg NoiseReal TextText ManipulationsGenerateReal FramesRef Pose\\n\\nOption 1Option 1\\x00 Generate Full Sentence then CropOption 2\\x00 Generate the New Word OnlyFind and cropSampleCrop noise to the same lengthAddOption 2\\n\\nYou are a helpful text modifier. Your target is to modify the provided text to invert its meaning to the opposite direction. The operation can be one of \"delete\", \"insert\" and \"replace\". Please generate output for the following input with 3 operations. ... the great songbook ... I\\'m not going to ... and unique ...[{\"operation\": \"replace\", \"old_word\": \"great\", \"new_word\": \"terrible\", \"index\": 4}, {\"operation\": \"delete\", \"old_word\": \"not\", \"new_word\": None, \"index\": 17}, {\"operation\": \"insert\", \"old_word\": None, \"new_word\": \"not\", \"index\": 24}]\\n\\nWhisperWhisperTalkLipDenoiserYourTTSVITS\"... the great songbook ... I\\'m not going to ... and unique ...\"\\n\\nFigure 2: Data manipulation and generation pipeline. Overview of the proposed three-stage pipeline. Given a real video, the pre-processing consists of audio extraction via FFmpeg followed by Whisper-based transcript generation. In the first stage, transcript manipulation, the original transcript is modified through word-level insertions, deletions, and replacements. In the second stage, audio generation, based on the relevant transcript manipulation, the audio is generated in both speaker-dependent and independent fashion. In the final stage, video generation, based on the generated audio, the subject-dependant video is generated with smooth transitions in terms of lip-synchronization, pose, and other relevant attributes.\\n\\nas the dataset proposed in this paper, next we do a direct comparison of the two datasets. In addition to the fact that AV-Deepfake1M is significantly larger than LAV-DF, in terms of the number of subjects, and amount of real and fake videos, the following differences further highlight our contributions.\\n\\nLAV-DF uses a rule-based system to find antonyms that max- imize the change in sentiment in the transcript manipulation step. We argue that naively choosing the antonyms causes context inconsistencies and low diversity of the fake con- tent. AV-Deepfake1M addresses this issue with the use of a large language model, which results in diverse and context- consistent fake content.\\n\\nThe output quality of the visual generator Wav2Lip [46] and audio generator SV2TTS [27] used for generating LAV-DF is not sufficient for state-of-the-art detection methods. AV- Deepfake1M utilizes the latest open-source state-of-the-art methods for high-quality audio and video generation.\\n\\nLAV-DF includes only replacement as a manipulation strat- egy. AV-Deepfake1M includes two additional challenging manipulation strategies, deletion and insertion.\\n\\nreal videos, while the content is carefully manipulated with content- driven audio-visual data. Following previous deepfake dataset gen- eration research [6, 31], the dataset includes three different combi- nations of modified modalities in the generated fake videos. Please note that here we also introduce the concept of content-driven mod- ifications for unimodal as well as multimodal aspects. We further elaborate on this in the supplementary material.\\n\\nFake Audio and Fake Visual. Both the real audio and visual frames are manipulated.\\n\\nFake Audio and Real Visual. Only the real audio correspond- ing to replacements and deletions is manipulated. To further increase data quality, the fake audio, and the corresponding length-normalized real visual segments are synchronized. As for the insertions, new visual segments are generated based on the length of the fake audio and are lip-synced to the background noise (i.e., closed mouth).\\n\\nReal Audio and Fake Visual. Only the real visual frames corresponding to replacements and deletions are manipulated. To further increase data quality, the length of the fake visual segments is normalized to match the length of the real audio. As for the insertions, background noise is inserted for the corresponding fake visual segments.\\n\\n3 AV-DEEPFAKE1M DATASET AV-Deepfake1M is a large-scale audio-visual deepfake dataset, in- cluding 1,886 hours of audio-visual data from 2,068 unique subjects captured in diverse background environments. This positions the proposed dataset as the most comprehensive audio-visual bench- mark as illustrated in Figure 1 and Table 1. The generated videos in AV-Deepfake1M preserve the background and identity present in the\\n\\n3.1 Data Generation Pipeline The three-stage pipeline for generating content-driven deepfakes is il- lustrated in Figure 2. A subset of real videos from the Voxceleb2 [14] dataset is pre-processed to extract the audio using FFmpeg [58], fol- lowed by Whisper-based [48] real transcript generation.\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nCai et al.\\n\\n(a) Frequencies of the top 20 words(b) #Unique new wordsFrequency\\n\\n11,74942427.7x\\n\\nFigure 3: Comparison of transcript modifications in AV-Deepfake1M and LAV-DF.\\n\\n3.1.1 Transcript Manipulation. Manipulation Strategy. The first stage for generating content-driven deepfakes is transcript manipulation. We utilize ChatGPT for altering the real transcripts. Through LangChain [9] the output of ChatGPT is a structured JSON with four fields: 1) operation: This set contains replace, delete, and insert, which has been applied on the input; 2) old_word: The word in the input to replace or delete; 3) new_word: The word in the input to insert or replace; 4) index: The location of the operation in the input. The number of transcript modifications depends on the video length and is determined by the following equation ð‘€ = ceil(ð‘¡/10) where ð‘€ is the number of modifications and ð‘¡ (sec) is the length of the video. We followed [3] and built a few-shot prompt template for ChatGPT.\\n\\nTable 2: Audio quality of AV-Deepfake1M. Quality of the gener- ated audio in terms of SECS, SNR and FAD.\\n\\nDataset\\n\\nSECS(â†‘) SNR(â†‘) FAD(â†“)\\n\\nFakeAVCeleb [31] LAV-DF [6] AV-Deepfake1M (Train) AV-Deepfake1M (Validation) AV-Deepfake1M (Test) AV-Deepfake1M (Overall)\\n\\n0.543 0.984 0.991 0.991 0.991 0.991\\n\\n2.16 7.83 9.40 9.16 9.42 9.39\\n\\n6.598 0.306 0.091 0.091 0.083 0.088\\n\\nfor a subset of the subjects. Further diversity in the audio generation was introduced by utilizing the identity-independent text-to-speech method YourTTS [8] for the rest of the subjects.\\n\\nPrompt 3.1: Transcripts manipulation\\n\\nSystem: You are a helpful text modifier. Your target is to modify the provided text to invert its meaning to the opposite direction. Here is the transcript of the audio. Please use the provided operations to modify the transcript to change its meaning. The operation can be one of â€œdeleteâ€, â€œinsertâ€ and â€œreplaceâ€. Human: {EXAMPLE INPUT 1} AI: {EXAMPLE OUTPUT 1} Human: {EXAMPLE INPUT 2} AI: {EXAMPLE OUTPUT 2} ...... Human: Please generate output for the following input with {NUM} operations. {INPUT}\\n\\nAnalysis. Figure 3 (a) illustrates a comparison of the frequencies of the top 20 words in AV-Deepfake1M and LAV-DF [6]. The results show that few words in LAV-DF have dominant frequencies (>10%), whereas this issue is drastically reduced in AV-Deepfake1M. Ow- ing to the contribution of ChatGPT, we also observed a significant increase in unique new words (>27.7Ã—) in the modified transcripts compared to LAV-DF, Figure 3 (b). This statistical comparison shows that the proposed LLM-based transcript manipulation strategy gen- erates more diverse content compared to the rule-based strategy employed in LAV-DF. We further elaborate on the advantages of using an LLM in this step in the supplementary material.\\n\\nAudio generation is slightly different for each of the manipu- lation strategies (i.e., replace, insert and delete). In the case of replace and insert, we need to generate new audio correspond- ing to new_word(s). Generally, there are two ways to generate the new_word(s): 1) Generate audio for the final fake transcript and crop it to get the audio for the required new_word(s) and 2) Generate audio only for the new_word(s). To bring further diver- sity and challenge, we use both strategies to generate audio for the new_word(s). In the case of delete, only the background noise is retained. After the audio manipulation, we normalized the loudness of the fake audio segments to the original audio to add more realism. To keep the consistency with the environmental noise, we add the background noise previously separated to the final audio output. Analysis. We evaluated the quality of the audio generation following previous works [7, 11] (note that for all datasets, we only evaluated the samples where the audio modality is modified). The results are shown in Table 2. The first evaluation metric is speaker encoder co- sine similarity (SECS) [60]. It measures the similarity of the speakers given a pair of audio in the range [âˆ’1, 1]. We also calculated the signal-to-noise ratio (SNR) for all fake audio and FrÃ©chet audio dis- tance (FAD) [32]. The results indicate that AV-Deepfake1M contains higher quality audio compared to other datasets.\\n\\n3.1.2 Audio Generation. Manipulation Strategy. The next stage is to generate high-quality audio with the same style as the speaker. The audio is first separated into background noise and speech using Denoiser [17]. Zero-shot voice cloning methods such as SV2TTS [27] utilized by previous datasets [6, 31] have low signal-to-noise ratio resulting in low-quality audio manipulations that are easily localized by BA-TFD [4] and UMMAFormer [75]. To increase the quality of the generated audio, we employ the identity-dependent text-to-speech method VITS [33]\\n\\n3.1.3 Video Generation. Manipulation Strategy. The final stage of the generation pipeline is visual content generation. After the audio is generated, the lip- synced visual frames are generated based on the subjectsâ€™ original pose and the fake audio. We investigated several face reenactment strategies including EAMM [26], AVFR-GAN [2], DiffTalk [54], AD-NeRF [20] and ATVGnet [10] and concluded that these methods are not well suited for zero-shot lip-synced generation of unseen speakers. Thus, we use TalkLip [62] for visual content generation\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nVITS 171,600 (100%)\\n\\nW (12.1%)F (40.0%)W (2.4%)F (45.5%)\\n\\nVITS-Word (23.6%)VITS-Full(76.4%)\\n\\nYourTTS 13,791 (47.9%)\\n\\n(a) #Subjects(b) #Videos(c) #Videos in Train(d) #Videos in Val(e) #Videos in Test\\n\\nVITS 15,010 (52.1%)W (12.1%)F (40.3%)W (2.3%)F (45.3%)Val 54,730 (4.8%)\\n\\nTrain & Val 1,657 (80.1%)Test 411 (19.9%)Test 343,240 (29.9%)YourTTS 177,581\\n\\nTrain 746,180 (65.1%)VITS 195,336 (52.4%)\\n\\nFigure 4: Data partitioning in AV-Deepfake1M. (a) The number of subjects in the train, validation, and test sets. (b) The number of videos in the train, validation, and test sets. (c) The number of videos with different audio generation methods in the train set. (d) The number of videos with different audio generation methods in the validation set. (e) The number of videos with different audio generation methods in the test set. F denotes audio generation for the full transcript and cropping of the new_word(s) and W denotes audio generation only for the new_word(s).\\n\\nReplace 1,046,668 (92.2%)LAV-DFInsert 12,909 (1.1%)Delete 75,384 (6.7%)Replace 114,253 (92.2%)\\n\\nMean (Ours)Mean (LAV-DF)OursLAV-DF\\n\\nFake Segment Length (sec)Fake Segment Length (sec)Fake Segment Ratio (%)Fake Segment Ratio (%)Video Length (sec)Video Length (sec)\\n\\nCount in ValCount in Test0.3260.3269.0720.6508.1%8.5979.1009.1299.0013.7%3.7%3.7%3.8%0.3280.326\\n\\nCount in Train\\n\\n(a) #Segments with Modification Type(b) #Videos with the Number of Fake Segments in Each Video\\n\\nOurs1 segment 622,608 (54.3%)1 segment 85,493 (62.7%)2 segments 14,380 (10.6%)2 segments 202,325 (17.6%)3 or 4 or 5 35,106 (3.1%)0 segment 286,721 (25.0%)0 segment 36,431 (26.7%)\\n\\nFigure 5: Comparison of AV-Deepfake1M and LAV-DF. The left three-row three-column histograms illustrate the fake segment absolute lengths (sec), the fake segment lengths proportion in videos (%) and the video lengths (sec) in the train, validation, and test sets. In the middle, the histograms illustrate the overall statistics for fake segment lengths, proportions and video lengths, compared with LAV-DF. For the fake segment lengths and proportions, the X-axis is in log scale and for video lengths, the X-axis is in linear scale. For all histograms, the Y-axis is in linear scale. The vertical dotted lines and numbers in histograms represent the mean value. On the right side, (a) The number of segments with different modifications and (b) The number of videos with different numbers of segments per video.\\n\\nTable 3: Visual quality of AV-Deepfake1M. Quality of the gener- ated video in terms of PSNR, SSIM and FID.\\n\\nDataset\\n\\nPSNR(â†‘) SSIM(â†‘) FID(â†“)\\n\\nFF++ [50] DFDC [16] FakeAVCeleb [31] LAV-DF [6] AV-Deepfake1M (Train) AV-Deepfake1M (Validation) AV-Deepfake1M (Test) AV-Deepfake1M (Overall)\\n\\n24.40 - 29.82 33.06 39.50 39.54 39.48 39.49\\n\\n0.812 - 0.919 0.898 0.977 0.977 0.977 0.977\\n\\n1.06 5.69 2.29 1.92 0.50 0.49 0.56 0.49\\n\\nDFDC [16] are â€˜in-the-wildâ€™, whereas FakeAVCeleb [31], LAV- DF [6] and AV-Deepfake1M are facial videos. Thus, we cropped the facial region for FF++ and DFDC for visual quality assessment. Since FakeAVCeleb, LAV-DF and AV-Deepfake1M are multimodal, for a fair comparison, we only used samples with the visual modal- ity modified. The results indicate that AV-Deepfake1M is of higher visual quality compared to existing datasets.\\n\\nwhich is primarily designed for zero-shot lip-sync scenarios. LipTalk is 1) Identity-independent, 2) Lip-syncing only without generating new poses, 3) Fast, 4) State-of-the-art, and 5) Open-source. This way we avoid the weaknesses of the aforementioned face reenactment strategies. The pre-trained TalkLip model is used to generate fake visual frames that are lip-synchronized with the input audio and can be used for insertion, replacement, and deletion. Analysis. To evaluate the visual quality of AV-Deepfake1M, we used peak signal-to-noise ratio (PSNR), structural similarity index (SSIM) [66] and FrÃ©chet inception distance (FID) [24] metrics as shown in Table 3. Note that for a fair comparison, we pre-processed the videos to a common format. The videos of FF++ [50] and\\n\\n3.2 Dataset Statistics We split the dataset into train, validation, and test sets. We first randomly select 1,657 subjects for the train set and 411 subjects for the test set without overlap. The validation set is randomly selected from the train set. The test set contains only samples with VITS- based identity-dependent audio. The variation in the number of subjects and videos in the sets is presented in Table 4 and Figure 4. Figure 5 illustrates the direct comparison of AV-Deepfake1M and LAV-DF. The results indicate that AV-Deepfake1M is more diverse in terms of modifications, subjects, fake segment and video lengths, and a lower average proportion of fake segments, making the dataset a vital asset for building better deepfake localization methods.\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nTable 4: Number of subjects and videos in AV-Deepfake1M.\\n\\nSubset\\n\\n#Subjects\\n\\n#Real Videos\\n\\n#Fake Videos\\n\\n#Videos\\n\\nTrain Validation Test Overall\\n\\n1,657\\n\\n411 2,068\\n\\n186,666 14,235 85,820 286,721\\n\\n559,514 43,105 257,420 860,039\\n\\n746,180 54,730 343,240 1,146,760\\n\\nTable 5: User study results for AV-Deepfake1M and LAV-DF.\\n\\nUser Study\\n\\nAcc. AP@0.1 AP@0.5 AR@1\\n\\nLAV-DF 84.03 AV-Deepfake1M 68.64\\n\\n36.80 15.32\\n\\n14.17 01.92\\n\\n10.04 02.54\\n\\n3.3 Human Quality Assessment To investigate if humans can detect the deepfakes in AV-Deepfake1M, we also conducted a user study with 25 participants with prior experi- ence in video manipulation in the computer vision domain (note that the authors did not participate in the study)2. 200 random samples that contain 0 or 1 modification were selected for the study, where 100 from LAV-DF and 100 from AV-Deepfake1M. Each participant was asked to classify 20 videos (5 real and 5 fake from LAV-DF dataset, 5 real and 5 fake from AV-Deepfake1M) as real or fake and propose the potential fake segment start and end point. The user study results presented in Table 5 indicate that the deepfake content in AV-Deepfake1M is very challenging to detect for humans, and AV-Deepfake1M is more difficult than LAV-DF.\\n\\n3.4 Computational Cost We spent around âˆ¼600 GPU hours for speech recognition with Whis- per [48], âˆ¼2100 GPU hours for training VITS [33] (each of the 721 VITS models requires âˆ¼3hrs), and âˆ¼300 GPU hours for data generation. Overall, we needed âˆ¼3000 GPU hours to generate AV- Deepfake1M with NVIDIA RTX6000 GPUs.\\n\\n4 BENCHMARKS AND METRICS This section outlines the benchmark protocol for AV-Deepfake1M along with the used evaluation metrics. The goal is to detect and lo- calize content-driven audio, visual, and audio-visual manipulations.\\n\\n4.1 Data Partitioning The dataset is organized in train, validation, and test sets, as de- scribed in Section 3.2. The original test set (all modifications) is referred to as fullset in the rest of the text. For a fair comparison with visual-only and audio-only methods, we also prepared subset V (by excluding the videos with audio-only modifications from fullset) and subset A (by excluding the videos with visual-only modifications from fullset).\\n\\n4.2 For benchmarking temporal deepfake localization, we consider the following state-of-the-art methods: Pyannote [45] is a pre-trained speaker diarization method. TriDet [55] and ActionFormer [73] are the state-of-the-art in the temporal action localization domain. Since\\n\\nImplementation Details\\n\\n2All procedures in this study were conducted in accordance with Monash University Human Research Ethics Committee approval 41545.\\n\\nCai et al.\\n\\nthese two methods require pre-trained features, we extracted the state-of-the-art features VideoMAEv2 [64] and InternVideo [65] for benchmarking. BA-TFD [6], BA-TFD+ [4], and UMMAFormer [75] are the state-of-the-art methods specifically designed for audio- visual temporal deepfake localization. We followed the original settings for BA-TFD and BA-TFD+. For UMMAFormer [75], we implemented it using the InternVideo [65] visual features and BYOL- A [43] audio features. For image-based classification methods, we consider Meso4 [1], MesoInception4 [1], Xception [12], Face X- Ray [36], LipForensics [21], EfficientViT [15], and SBI [56]. We followed the procedure used in previous works [4, 76] to aggregate the frame-level predictions to segments for localization.\\n\\nFor benchmarking deepfake detection, we trained the image-based models Meso4 [1], MesoInception4 [1], Xception [12] and Effi- cientViT [15] with video frames along with the corresponding labels. For the segment-based methods MDS [13] and MARLIN [5], we used a sliding window to sample segments from the video for training and inference. During the inference stage, the frame- and segment- level predictions are aggregated to video-level by max voting. The aggregation strategy is discussed in Section 5. We also evaluated the zero-shot performance of several methods, including the LLM-based Video-LLaMA [74], audio pre-trained CLAP [68], M2TR [63] and LipForensics [21] pre-trained on FF++ [50], Face X-Ray [36] and SBI [56] pretrained on blending images. For Video-LLaMA, we also evaluated 5 model ensembles (the majority vote of 5 model inferences). To investigate the impact of the level of label access, we designed 3 different label access levels for training: frame-level labels, segment-level labels only, and video-level labels only.\\n\\n4.3 Evaluation Metrics Temporal Deepfake Localization. We use average precision (AP) and average recall (AR) as prior works [6, 22]. Deepfake Detection. We use standard evaluation protocol [16, 50] to report video-level accuracy (Acc.) and area under the curve (AUC).\\n\\n5 RESULTS AND ANALYSIS This section reports the performance of the state-of-the-art deepfake detection and localization methods described in Section 4.2 on AV- Deepfake1M. The reported performance is based on different subsets, described in Section 4.1, and different levels of label access during training, described in Section 4.2.\\n\\n5.1 Audio-Visual Temporal Deepfake Localization The results of this benchmark are depicted in Table 6. All state-of-the- art methods achieve significantly lower performance compared to the performance reported on previous datasets [6, 22]. This significant drop indicates that existing temporal deepfake localization methods are falling behind with the rapid advancements in content generation. In other words, we can claim that the highly realistic fake content in AV-Deepfake1M will open an avenue for further research on temporal deepfake localization methods.\\n\\n5.2 Audio-Visual Deepfake Detection Similarly to temporal deepfake localization, the results of the clas- sical deepfake detection benchmark are summarized in Table 7.\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nTable 6: Temporal deepfake localization benchmark. Performance comparison of state-of-the-art methods on the proposed AV-Deepfake1M dataset. The results are significantly low, indicating that AV-Deepfake1M is an important benchmark for this task.\\n\\nSet Method\\n\\nMod. AP@0.5 AP@0.75 AP@0.9 AP@0.95 AR@50 AR@30 AR@20 AR@10 AR@5\\n\\nt e s l l u F\\n\\nPyAnnote (Zero-Shot) [45] Meso4 [1] MesoInception4 [1] EfficientViT [15] TriDet + VideoMAEv2 [55, 64] TriDet + InternVideo [55, 65] ActionFormer + VideoMAEv2 [64, 73] ActionFormer + InternVideo [65, 73] BA-TFD [6] BA-TFD+ [4] UMMAFormer [75]\\n\\nA V V V V V V V AV AV AV\\n\\n00.03 09.86 08.50 14.71 21.67 29.66 20.24 36.08 37.37 44.42 51.64\\n\\n00.00 06.05 05.16 02.42 05.83 09.02 05.73 12.01 06.34 13.64 28.07\\n\\n00.00 02.22 01.89 00.13 00.54 00.79 00.57 01.23 00.19 00.48 07.65\\n\\n00.00 00.59 00.50 00.01 00.06 00.09 00.07 00.16 00.02 00.03 01.58\\n\\n00.67 38.92 39.27 27.04 20.27 24.08 19.97 27.11 45.55 48.86 44.07\\n\\n00.67 38.91 39.22 26.99 20.23 24.06 19.93 27.08 40.37 44.51 43.93\\n\\n00.67 38.81 39.00 26.43 20.12 23.96 19.81 27.00 35.95 40.37 43.45\\n\\n00.67 36.47 35.78 23.90 19.50 23.50 19.11 26.60 30.66 34.67 42.09\\n\\n00.67 26.91 24.59 20.31 18.18 22.55 17.80 25.80 26.82 29.88 40.27\\n\\nV\\n\\nt e s b u S\\n\\nPyAnnote (Zero-Shot) [45] Meso4 [1] MesoInception4 [1] EfficientViT [15] TriDet + VideoMAEv2 [55, 64] TriDet + InternVideo [55, 65] ActionFormer + VideoMAEv2 [64, 73] ActionFormer + InternVideo [65, 73] BA-TFD [6] BA-TFD+ [4] UMMAFormer [75]\\n\\nA V V V V V V V AV AV AV\\n\\n00.02 15.31 13.38 23.21 26.45 37.90 24.80 45.57 55.34 65.85 39.07\\n\\n00.00 09.54 08.25 03.92 07.35 12.15 07.25 16.07 09.48 20.37 20.77\\n\\n00.00 03.52 03.05 00.21 00.74 01.12 00.77 01.75 00.30 00.73 05.62\\n\\n00.00 00.93 00.81 00.02 00.08 00.13 00.09 00.23 00.03 00.05 01.16\\n\\n00.52 58.04 58.54 37.52 22.49 28.08 22.26 31.78 62.66 65.13 40.39\\n\\n00.52 58.03 58.48 37.46 22.47 28.07 22.23 31.77 55.48 59.07 40.19\\n\\n00.52 57.87 58.15 36.88 22.42 28.03 22.16 31.73 49.53 53.57 39.51\\n\\n00.52 54.37 53.34 34.19 22.04 27.79 21.70 31.56 43.15 46.79 37.53\\n\\n00.52 40.06 36.59 29.64 21.09 27.17 20.71 31.14 38.48 41.69 34.93\\n\\nA\\n\\nt e s b u S\\n\\nPyAnnote (Zero-Shot) [45] Meso4 [1] MesoInception4 [1] EfficientViT [15] TriDet + VideoMAEv2 [55, 64] TriDet + InternVideo [55, 65] ActionFormer + VideoMAEv2 [64, 73] ActionFormer + InternVideo[65, 73] BA-TFD [6] BA-TFD+ [4] UMMAFormer [75]\\n\\nA V V V V V V V AV AV AV\\n\\n00.05 07.13 05.88 09.91 17.45 24.95 16.22 30.86 27.79 33.23 68.68\\n\\n00.01 04.17 03.46 15.79 04.01 06.85 03.95 09.47 04.31 10.07 40.00\\n\\n00.00 01.45 01.19 00.08 00.24 00.47 00.28 00.78 00.12 00.36 11.32\\n\\n00.00 00.39 00.32 00.01 00.02 00.05 00.03 00.09 00.01 00.03 02.35\\n\\n00.97 29.34 29.46 21.47 18.47 21.79 18.11 24.49 36.71 40.54 51.44\\n\\n00.97 29.34 29.42 21.42 18.43 21.76 18.07 24.46 32.50 37.07 51.41\\n\\n00.97 29.27 29.26 20.87 18.28 21.64 17.92 24.36 28.82 33.63 51.35\\n\\n00.97 27.58 26.95 18.43 17.53 21.07 17.10 23.85 24.02 28.50 51.23\\n\\n00.96 20.54 18.80 15.39 16.02 19.95 15.59 22.87 20.58 23.82 50.95\\n\\nModels that have access only to the video-level labels during train- ing and the zero-shot models all perform poorly on this task, except the Face X-Ray and SBI which are designed to be generalizable. Providing the fine-grained segment-level and frame-level labels dur- ing training brings an improvement in performance. However, even with the frame-level labels provided during training, the AUC of the best-performing methods is less than 70, due to the multimodal modifications present in AV-Deepfake1M.\\n\\nThe frame- and segment-based deepfake detection methods can only produce frame- and segment-level predictions. Thus, a suitable aggregation strategy is required to generate the video-level predic- tions. We investigated several popular aggregation strategies, such as max (e.g., [6]), average (e.g., [15, 23, 63]), and the average of the highest 5 scores (e.g., [37]) for video-level predictions. The results of the experiment are presented in Table 9. The results show that max is the optimal aggregation strategy on AV-Deepfake1M for the considered deepfake detection methods.\\n\\n5.4 Benchmark Comparison We conducted additional experiments (Tables 8 and 10) to compare the performance on temporal localization and classification on AV- Deepfake1M and LAV-DF [6].\\n\\nThere is a significant drop in BA-TFD [6] temporal localization performance as compared to LAV-DF (Table 8). A similar pattern is also observed for BA-TFD+ [4] (AP@0.5 96.30 â†’ 44.42) and UMMAFormer [75] (AP@0.5 98.83 â†’ 51.64). For classification (Table 10), the performance of Xception [12], LipForensics [21], Face X-Ray [36], and SBI [56] also drops compared to LAV-DF. These additional results further validate that AV-Deepfake1M is more challenging than LAV-DF.\\n\\nWe conduct the experiments using Xception and BA-TFD pre- trained on AV-Deepfake1M then finetune and evaluate on LAV-DF, shown in Table 11. We observe the performance improvements are significant for both temporal localization with BA-TFD and clas- sification with Xception, when compared with models trained on LAV-DF from scratch.\\n\\n5.3 Unimodal Deepfake Detection and Localization We also evaluated the performance on subset V and subset A, as described in Section 4.1. As expected, all visual-only methods con- sistently perform better on subset V compared to fullset for both temporal localization and detection. The same holds for subset A and audio-only methods.\\n\\n6 CONCLUSION This paper presents AV-Deepfake1M, the largest audio-visual dataset for temporal deepfake localization. The comprehensive benchmark\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nCai et al.\\n\\nTable 7: Deepfake detection benchmark. Performance comparison of state-of-the-art methods on the proposed AV-Deepfake1M dataset using different evaluation protocols. E5: Ensemble 5.\\n\\nLabel Access Methods For Training\\n\\nMod.\\n\\nFullset AUC Acc.\\n\\nSubset V AUC Acc.\\n\\nSubset A AUC Acc.\\n\\nZero-Shot\\n\\nVideo-level\\n\\nVideo-LLaMA (7B) [74] Video-LLaMA (13B) [74] Video-LLaMA (7B) E5 [74] Video-LLaMA (13B) E5 [74] CLAP [68] M2TR [63] LipForensics [21] Face X-Ray [36] SBI [56] Meso4 [1] MesoInception4 [1] SBI [56]\\n\\nSegment-level Meso4 [1]\\n\\nFrame-level\\n\\nMesoInception4 [1] MDS [13] MARLIN [5] Meso4 [1] MesoInception4 [1] Xception [12] EfficientViT [15]\\n\\nAV AV AV AV A V V V V V V V V V AV V V V V V\\n\\n50.09 49.50 49.97 50.74 50.83 50.18 51.57 61.54 55.10 50.22 50.05 65.82 54.53 57.16 56.57 58.03 63.05 64.04 68.68 65.51\\n\\n25.23 25.02 25.32 25.05 31.99 74.99 68.84 73.83 34.04 75.00 75.00 69.00 55.83 28.24 59.44 29.01 49.51 54.13 61.33 71.80\\n\\n50.13 49.53 50.01 50.52 50.91 50.24 54.37 61.88 57.75 50.31 50.01 67.31 56.81 62.14 54.21 61.57 76.30 80.67 81.97 76.74\\n\\n33.51 33.35 33.57 33.36 37.83 66.67 64.13 66.59 41.51 66.66 66.66 67.19 56.78 37.41 53.70 38.28 64.62 69.88 81.39 70.89\\n\\n50.08 49.30 49.98 50.78 50.67 50.14 50.65 60.86 53.81 50.17 50.06 65.11 53.34 54.64 56.92 56.23 56.27 56.28 63.19 59.75\\n\\n33.49 33.36 33.62 33.40 37.54 66.66 62.19 66.35 39.38 66.66 66.66 65.55 53.89 35.46 58.88 35.99 47.82 51.73 57.45 63.51\\n\\nTable 8: Temporal localization results on the AV-Deepfake1M and LAV-DF.\\n\\nMethod\\n\\nDataset\\n\\nAP@0.5 AP@0.75 AP@0.95 AR@50 AR@20 AR@10\\n\\nBA-TFD [6]\\n\\nBA-TFD+ [4]\\n\\nUMMAFormer [75]\\n\\nLAV-DF [6] AV-Deepfake1M LAV-DF [6] AV-Deepfake1M LAV-DF [6] AV-Deepfake1M\\n\\n79.15 37.37 96.30 44.42 98.83 51.64\\n\\n38.57 06.34 84.96 13.64 95.54 28.09\\n\\n00.24 00.02 04.44 00.03 37.61 01.57\\n\\n64.18 45.55 80.48 48.86 92.47 44.07\\n\\n60.89 35.95 79.40 40.37 92.42 43.45\\n\\n58.51 30.66 78.75 34.67 92.10 42.09\\n\\nTable 9: Aggregation strategies. AUC scores on fullset for each method using different aggregation strategies.\\n\\nTable 11: Transfer learning results. Dataset for pretraining.\\n\\nMethod â†’ Meso4 MesoInc4 Xception EfficientViT MARLIN Strategy â†“\\n\\n[1]\\n\\n[1]\\n\\n[12]\\n\\n[15]\\n\\n[5]\\n\\nmax avg avg of top5\\n\\n63.05 55.61 62.32\\n\\n64.04 54.07 59.82\\n\\n68.68 61.44 68.81\\n\\n65.51 58.75 63.60\\n\\n58.03 53.20 56.39\\n\\nTrain Data\\n\\nLAV-DF AV-Deepfake1M, LAV-DF\\n\\nMethods â†’ BA-TFD Xception AUC â†‘\\n\\nTest Data AP@0.5 â†‘\\n\\nLAV-DF LAV-DF\\n\\n79.15 83.93\\n\\n83.58 90.12\\n\\nTable 10: Performance (AUC â†‘) for classification baselines on AV-Deepfake1M and LAV-DF.\\n\\nLabel Access Methods\\n\\nAV-Deepfake1M LAV-DF [6]\\n\\nZero-shot\\n\\nLipForensics [21] Face X-Ray [36] SBI [56] SBI [56]\\n\\nVideo-level Segment-level MDS [13] Frame-level\\n\\nXception [12] EfficientViT [15]\\n\\n51.57 61.54 55.10 65.82 56.57 68.68 65.51\\n\\n73.34 69.65 62.84 67.23 82.80 83.58 96.50\\n\\nof the dataset utilizing state-of-the-art deepfake detection and local- ization methods indicates a significant drop in performance com- pared to previous datasets, indicating that the proposed dataset is an\\n\\nimportant asset for building the next-generation of deepfake local- ization methods. Limitations. Similarly to other deepfake datasets, AV-Deepfake1M exhibits a misbalance in terms of the number of fake and real videos. Broader Impact. Owing to the diversified and realistic, content- driven fake videos, AV-Deepfake1M will support the development of robust audio-visual deepfake detection and localization models. Ethics Statement. We acknowledge that AV-Deepfake1M may raise ethical concerns such as the potential misuse of facial videos of celebrities, and even the data generation pipeline could have a poten- tial negative impact. Misuse could include the creation of deepfake videos or other forms of exploitation. To avoid such issues, we have taken several measures such as distributing the data with a proper end-user license agreement, where we will impose certain restric- tions on the usage of the data, such as the data generation technology and resulting content being restricted to research purposes only.\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nREFERENCES [1] Darius Afchar, Vincent Nozick, Junichi Yamagishi, and Isao Echizen. 2018. MesoNet: a Compact Facial Video Forgery Detection Network. In 2018 IEEE International Workshop on Information Forensics and Security (WIFS). 1â€“7. ISSN: 2157-4774.\\n\\n[2] Madhav Agarwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C. V. Jawahar. 2023. Audio-Visual Face Reenactment. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 5178â€“5187.\\n\\n[3] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. In Advances in Neural Information Processing Systems, Vol. 33. Curran Associates, Inc., 1877â€“ 1901.\\n\\n[4] Zhixi Cai, Shreya Ghosh, Abhinav Dhall, Tom Gedeon, Kalin Stefanov, and Munawar Hayat. 2023. Glitch in the matrix: A large scale benchmark for content driven audioâ€“visual forgery detection and localization. Computer Vision and Image Understanding 236 (Nov. 2023), 103818.\\n\\n[5] Zhixi Cai, Shreya Ghosh, Kalin Stefanov, Abhinav Dhall, Jianfei Cai, Hamid Rezatofighi, Reza Haffari, and Munawar Hayat. 2023. MARLIN: Masked Autoen- coder for Facial Video Representation LearnINg. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. IEEE, Vancouver, BC, Canada, 1493â€“1504.\\n\\n[6] Zhixi Cai, Kalin Stefanov, Abhinav Dhall, and Munawar Hayat. 2022. Do You Re- ally Mean That? Content Driven Audio-Visual Deepfake Dataset and Multimodal Method for Temporal Forgery Localization. In 2022 International Conference on Digital Image Computing: Techniques and Applications (DICTA). Sydney, Australia, 1â€“10.\\n\\n[7] Edresson Casanova, Christopher Shulby, Eren GÃ¶lge, Nicolas Michael MÃ¼ller, Frederico Santos De Oliveira, Arnaldo Candido Jr., Anderson Da Silva Soares, Sandra Maria Aluisio, and Moacir Antonelli Ponti. 2021. SC-GlowTTS: An Efficient Zero-Shot Multi-Speaker Text-To-Speech Model. In Interspeech 2021. ISCA, 3645â€“3649.\\n\\n[8] Edresson Casanova, Julian Weber, Christopher D. Shulby, Arnaldo Candido Junior, Eren GÃ¶lge, and Moacir A. Ponti. 2022. YourTTS: Towards Zero-Shot Multi- Speaker TTS and Zero-Shot Voice Conversion for Everyone. In Proceedings of the 39th International Conference on Machine Learning. PMLR, 2709â€“2720. ISSN: 2640-3498.\\n\\n[9] Harrison Chase. 2022. LangChain.\\n\\n[10] Lele Chen, Ross K. Maddox, Zhiyao Duan, and Chenliang Xu. 2019. Hierar- chical Cross-Modal Talking Face Generation With Dynamic Pixel-Wise Loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 7832â€“7841.\\n\\n[11] Seungwoo Choi, Seungju Han, Dongyoung Kim, and Sungjoo Ha. 2020. Attentron: Few-Shot Text-to-Speech Utilizing Attention-Based Variable-Length Embedding. In Interspeech 2020. ISCA, 2007â€“2011.\\n\\n[12] Francois Chollet. 2017. Xception: Deep Learning With Depthwise Separable Convolutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 1251â€“1258.\\n\\n[13] Komal Chugh, Parul Gupta, Abhinav Dhall, and Ramanathan Subramanian. 2020. Not made for each other- Audio-Visual Dissonance-based Deepfake Detection and Localization. In Proceedings of the 28th ACM International Conference on Multimedia (MM â€™20). Association for Computing Machinery, New York, NY, USA, 439â€“447.\\n\\n[14] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. 2018. VoxCeleb2: Deep\\n\\nSpeaker Recognition. In Interspeech 2018. ISCA, 1086â€“1090.\\n\\n[15] Davide Alessandro Coccomini, Nicola Messina, Claudio Gennaro, and Fabrizio Falchi. 2022. Combining EfficientNet and Vision Transformers for Video Deep- fake Detection. In Image Analysis and Processing â€“ ICIAP 2022 (Lecture Notes in Computer Science), Stan Sclaroff, Cosimo Distante, Marco Leo, Giovanni M. Farinella, and Federico Tombari (Eds.). Springer International Publishing, Cham, 219â€“229.\\n\\n[16] Brian Dolhansky, Joanna Bitton, Ben Pflaum, Jikuo Lu, Russ Howes, Menglin Wang, and Cristian Canton Ferrer. 2020. The DeepFake Detection Challenge (DFDC) Dataset. arXiv: 2006.07397 [cs].\\n\\n[17] Alexandre DÃ©fossez, Gabriel Synnaeve, and Yossi Adi. 2020. Real Time Speech Enhancement in the Waveform Domain. In Interspeech 2020. Shanghai, China, 3291â€“3295.\\n\\n[18] Chao Feng, Ziyang Chen, and Andrew Owens. 2023. Self-Supervised Video Forensics by Audio-Visual Anomaly Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 10491â€“10503.\\n\\n[19] Songwei Ge, Thomas Hayes, Harry Yang, Xi Yin, Guan Pang, David Jacobs, Jia- Bin Huang, and Devi Parikh. 2022. Long Video Generation with Time-Agnostic\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nVQGAN and Time-Sensitive Transformer. In Proceedings of the European Con- ference on Computer Vision (ECCV) (Lecture Notes in Computer Science), Shai Avidan, Gabriel Brostow, Moustapha CissÃ©, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham, 102â€“118.\\n\\n[20] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. 2021. AD-NeRF: Audio Driven Neural Radiance Fields for Talking Head Synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 5784â€“5794.\\n\\n[21] Alexandros Haliassos, Konstantinos Vougioukas, Stavros Petridis, and Maja Pantic. 2021. Lips Donâ€™t Lie: A Generalisable and Robust Approach To Face Forgery Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5039â€“5049.\\n\\n[22] Yinan He, Bei Gan, Siyu Chen, Yichun Zhou, Guojun Yin, Luchuan Song, Lu Sheng, Jing Shao, and Ziwei Liu. 2021. ForgeryNet: A Versatile Benchmark for Comprehensive Forgery Analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 4360â€“4369.\\n\\n[23] Young-Jin Heo, Woon-Ha Yeo, and Byung-Gyu Kim. 2023. DeepFake detection algorithm based on improved vision transformer. Applied Intelligence 53, 7 (April 2023), 7512â€“7527.\\n\\n[24] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs Trained by a Two Time-Scale Update Rule Con- verge to a Local Nash Equilibrium. In Advances in Neural Information Processing Systems, Vol. 30. Curran Associates, Inc.\\n\\n[25] Hafsa Ilyas, Ali Javed, and Khalid Mahmood Malik. 2023. AVFakeNet: A uni- fied end-to-end Dense Swin Transformer deep learning model for audioâ€“visual deepfakes detection. Applied Soft Computing 136 (March 2023), 110124. [26] Xinya Ji, Hang Zhou, Kaisiyuan Wang, Qianyi Wu, Wayne Wu, Feng Xu, and Xun Cao. 2022. EAMM: One-Shot Emotional Talking Face via Audio-Based Emotion-Aware Motion Model. In ACM SIGGRAPH 2022 Conference Proceed- ings (SIGGRAPH â€™22). Association for Computing Machinery, New York, NY, USA, 1â€“10.\\n\\n[27] Ye Jia, Yu Zhang, Ron J. Weiss, Quan Wang, Jonathan Shen, Fei Ren, Zhifeng Chen, Patrick Nguyen, Ruoming Pang, Ignacio Lopez Moreno, and Yonghui Wu. 2018. Transfer learning from speaker verification to multispeaker text-to- speech synthesis. In Proceedings of the 32nd International Conference on Neural Information Processing Systems (NIPSâ€™18). Curran Associates Inc., Red Hook, NY, USA, 4485â€“4495.\\n\\n[28] Liming Jiang, Ren Li, Wayne Wu, Chen Qian, and Chen Change Loy. 2020. DeeperForensics-1.0: A Large-Scale Dataset for Real-World Face Forgery De- tection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2889â€“2898.\\n\\n[29] Ziyue Jiang, Jinglin Liu, Yi Ren, Jinzheng He, Chen Zhang, Zhenhui Ye, Pengfei Wei, Chunfeng Wang, Xiang Yin, Zejun Ma, and Zhou Zhao. 2023. Mega-TTS 2: Zero-Shot Text-to-Speech with Arbitrary Length Speech Prompts. arXiv:2307.07218 [cs, eess].\\n\\n[30] Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Sheng- peng Ji, Rongjie Huang, Chunfeng Wang, Xiang Yin, Zejun Ma, and Zhou Zhao. 2023. Mega-TTS: Zero-Shot Text-to-Speech at Scale with Intrinsic Inductive Bias. arXiv:2306.03509 [cs, eess].\\n\\n[31] Hasam Khalid, Shahroz Tariq, and Simon S. Woo. 2021. FakeAVCeleb: A Novel\\n\\nAudio-Video Multimodal Deepfake Dataset. arXiv: 2108.05080 [cs].\\n\\n[32] Kevin Kilgour, Mauricio Zuluaga, Dominik Roblek, and Matthew Sharifi. 2019. FrÃ©chet Audio Distance: A Metric for Evaluating Music Enhancement Algorithms. arXiv:1812.08466 [cs, eess].\\n\\n[33] Jaehyeon Kim, Jungil Kong, and Juhee Son. 2021. Conditional Variational Autoen- coder with Adversarial Learning for End-to-End Text-to-Speech. In Proceedings of the 38th International Conference on Machine Learning. PMLR, 5530â€“5540. ISSN: 2640-3498.\\n\\n[34] Pavel Korshunov and Sebastien Marcel. 2018. DeepFakes: a New Threat to Face\\n\\nRecognition? Assessment and Detection. arXiv:1812.08685 [cs].\\n\\n[35] Patrick Kwon, Jaeseong You, Gyuhyeon Nam, Sungwoo Park, and Gyeongsu Chae. 2021. KoDF: A Large-Scale Korean DeepFake Detection Dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 10744â€“10753.\\n\\n[36] Lingzhi Li, Jianmin Bao, Ting Zhang, Hao Yang, Dong Chen, Fang Wen, and Baining Guo. 2020. Face X-Ray for More General Face Forgery Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5001â€“5010.\\n\\n[37] Yuezun Li and Siwei Lyu. 2019. Exposing DeepFake Videos By Detecting Face Warping Artifacts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 7.\\n\\n[38] Yuezun Li, Xin Yang, Pu Sun, Honggang Qi, and Siwei Lyu. 2020. Celeb-DF: A Large-Scale Challenging Dataset for DeepFake Forensics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 3207â€“3216. [39] Xuechen Liu, Xin Wang, Md Sahidullah, Jose Patino, HÃ©ctor Delgado, Tomi Kinnunen, Massimiliano Todisco, Junichi Yamagishi, Nicholas Evans, Andreas Nautsch, and Kong Aik Lee. 2023. ASVspoof 2021: Towards Spoofed and IEEE/ACM Transactions on Audio, Deepfake Speech Detection in the Wild. Speech, and Language Processing 31 (2023), 2507â€“2522.\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\n[40] Trisha Mittal, Uttaran Bhattacharya, Rohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Emotions Donâ€™t Lie: An Audio-Visual Deepfake Detection Method using Affective Cues. In Proceedings of the 28th ACM International Conference on Multimedia (MM â€™20). Association for Computing Machinery, New York, NY, USA, 2823â€“2832.\\n\\n[41] Kartik Narayan, Harsh Agarwal, Kartik Thakral, Surbhi Mittal, Mayank Vatsa, and Richa Singh. 2023. DF-Platter: Multi-Face Heterogeneous Deepfake Dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9739â€“9748.\\n\\n[42] Dufou Nick and Jigsaw Andrew. 2019. Contributing Data to Deepfake Detection\\n\\nResearch.\\n\\n[43] Daisuke Niizumi, Daiki Takeuchi, Yasunori Ohishi, Noboru Harada, and Kunio Kashino. 2021. BYOL for Audio: Self-Supervised Learning for General-Purpose Audio Representation. In 2021 International Joint Conference on Neural Networks (IJCNN). 1â€“8. ISSN: 2161-4407.\\n\\n[44] Trevine Oorloff, Surya Koppisetti, NicolÃ² Bonettini, Divyaraj Solanki, Ben Col- man, Yaser Yacoob, Ali Shahriyari, and Gaurav Bharaj. 2024. AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 27102â€“27112. [45] Alexis Plaquet and HervÃ© Bredin. 2023. Powerset multi-class cross entropy loss for neural speaker diarization. In INTERSPEECH 2023. ISCA, 3222â€“3226. [46] K R Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. 2020. A Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In Proceedings of the 28th ACM International Conference on Multimedia (MM â€™20). Association for Computing Machinery, New York, NY, USA, 484â€“492. [47] Yuyang Qian, Guojun Yin, Lu Sheng, Zixuan Chen, and Jing Shao. 2020. Thinking in Frequency: Face Forgery Detection by Mining Frequency-Aware Clues. In Proceedings of the European Conference on Computer Vision (ECCV) (Lecture Notes in Computer Science), Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm (Eds.). Springer International Publishing, Cham, 86â€“103. [48] Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust Speech Recognition via Large-Scale Weak Supervision. In Proceedings of the 40th International Conference on Machine Learning. PMLR, 28492â€“28518. ISSN: 2640-3498.\\n\\n[49] Muhammad Anas Raza and Khalid Mahmood Malik. 2023. Multimodaltrace: Deepfake Detection Using Audiovisual Representation Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 993â€“ 1000.\\n\\n[50] Andreas Rossler, Davide Cozzolino, Luisa Verdoliva, Christian Riess, Justus Thies, and Matthias Niessner. 2019. FaceForensics++: Learning to Detect Manipulated Facial Images. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 1â€“11.\\n\\n[51] Sahibzada Adil Shahzad, Ammarah Hashmi, Sarwar Khan, Yan-Tsung Peng, Yu Tsao, and Hsin-Min Wang. 2022. Lip Sync Matters: A Novel Multimodal Forgery Detector. In 2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). 1885â€“1892. ISSN: 2640-0103. [52] Rui Shao, Tianxing Wu, Jianlong Wu, Liqiang Nie, and Ziwei Liu. 2024. Detecting and Grounding Multi-Modal Media Manipulation and Beyond. IEEE Transactions on Pattern Analysis and Machine Intelligence (2024), 1â€“18.\\n\\n[53] Kai Shen, Zeqian Ju, Xu Tan, Yanqing Liu, Yichong Leng, Lei He, Tao Qin, Sheng Zhao, and Jiang Bian. 2023. NaturalSpeech 2: Latent Diffusion Models are Natural and Zero-Shot Speech and Singing Synthesizers. arXiv:2304.09116 [cs, eess].\\n\\n[54] Shuai Shen, Wenliang Zhao, Zibin Meng, Wanhua Li, Zheng Zhu, Jie Zhou, and Jiwen Lu. 2023. DiffTalk: Crafting Diffusion Models for Generalized Audio- Driven Portraits Animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1982â€“1991.\\n\\n[55] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and Dacheng Tao. 2023. TriDet: Temporal Action Detection With Relative Boundary Modeling. In Proceed- ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18857â€“18866.\\n\\n[56] Kaede Shiohara and Toshihiko Yamasaki. 2022. Detecting Deepfakes With Self- Blended Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 18720â€“18729.\\n\\n[57] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. 2022. Make-A-Video: Text-to-Video Generation without Text-Video Data. arXiv:2209.14792 [cs].\\n\\n[58] Suramya Tomar. 2006. Converting video formats with FFmpeg. Linux Journal\\n\\n2006, 146 (June 2006), 10.\\n\\n[59] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guil- laume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. arXiv:2302.13971 [cs].\\n\\n[60] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. 2018. Generalized End-to-End Loss for Speaker Verification. In IEEE International Conference on\\n\\nCai et al.\\n\\nAcoustics, Speech and Signal Processing (ICASSP). 4879â€“4883. 190X.\\n\\nISSN: 2379-\\n\\n[61] Jiazhen Wang, Bin Liu, Changtao Miao, Zhiwei Zhao, Wanyi Zhuang, Qi Chu, and Nenghai Yu. 2024. Exploiting Modality-Specific Features for Multi-Modal Manipulation Detection and Grounding. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 4935â€“4939. ISSN: 2379- 190X.\\n\\n[62] Jiadong Wang, Xinyuan Qian, Malu Zhang, Robby T. Tan, and Haizhou Li. 2023. Seeing What You Said: Talking Face Generation Guided by a Lip Reading Expert. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14653â€“14662.\\n\\n[63] Junke Wang, Zuxuan Wu, Wenhao Ouyang, Xintong Han, Jingjing Chen, Yu-Gang Jiang, and Ser-Nam Li. 2022. M2TR: Multi-modal Multi-scale Transformers for Deepfake Detection. In Proceedings of the 2022 International Conference on Multimedia Retrieval (ICMR â€™22). Association for Computing Machinery, New York, NY, USA, 615â€“623.\\n\\n[64] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. 2023. VideoMAE V2: Scaling Video Masked Autoencoders With Dual Masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 14549â€“14560.\\n\\n[65] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, Sen Xing, Guo Chen, Junting Pan, Jiashuo Yu, Yali Wang, Limin Wang, and Yu Qiao. 2022. InternVideo: General Video Foundation Models via Generative and Discriminative Learning. arXiv:2212.03191 [cs].\\n\\n[66] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. 2004. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing 13, 4 (April 2004), 600â€“612.\\n\\n[67] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. 2023. Tune-A- Video: One-Shot Tuning of Image Diffusion Models for Text-to-Video Generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision. 7623â€“7633.\\n\\n[68] Yusong Wu, Ke Chen, Tianyu Zhang, Yuchen Hui, Taylor Berg-Kirkpatrick, and Shlomo Dubnov. 2023. Large-Scale Contrastive Language-Audio Pretraining with Feature Fusion and Keyword-to-Caption Augmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 1â€“5. ISSN: 2379-190X.\\n\\n[69] Wenyuan Yang, Xiaoyu Zhou, Zhikai Chen, Bofei Guo, Zhongjie Ba, Zhihua Xia, Xiaochun Cao, and Kui Ren. 2023. AVoiD-DF: Audio-Visual Joint Learning for Detecting Deepfake. IEEE Transactions on Information Forensics and Security 18 (2023), 2015â€“2029.\\n\\n[70] Xin Yang, Yuezun Li, and Siwei Lyu. 2019. Exposing Deep Fakes Using Incon- sistent Head Poses. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 8261â€“8265. ISSN: 2379-190X.\\n\\n[71] Jiangyan Yi, Ruibo Fu, Jianhua Tao, Shuai Nie, Haoxin Ma, Chenglong Wang, Tao Wang, Zhengkun Tian, Ye Bai, Cunhang Fan, Shan Liang, Shiming Wang, Shuai Zhang, Xinrui Yan, Le Xu, Zhengqi Wen, Haizhou Li, Zheng Lian, and Bin Liu. 2022. ADD 2022: the First Audio Deep Synthesis Detection Challenge. arXiv:2202.08433 [cs, eess].\\n\\n[72] Yang Yu, Xiaolong Liu, Rongrong Ni, Siyuan Yang, Yao Zhao, and Alex C. Kot. 2023. PVASS-MDD: Predictive Visual-audio Alignment Self-supervision for Multimodal Deepfake Detection. IEEE Transactions on Circuits and Systems for Video Technology (2023), 1â€“1.\\n\\n[73] Chen-Lin Zhang, Jianxin Wu, and Yin Li. 2022. ActionFormer: Localizing Mo- ments of Actions with Transformers. In Proceedings of the European Conference on Computer Vision (ECCV) (Lecture Notes in Computer Science), Shai Avidan, Gabriel Brostow, Moustapha CissÃ©, Giovanni Maria Farinella, and Tal Hassner (Eds.). Springer Nature Switzerland, Cham, 492â€“510.\\n\\n[74] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-LLaMA: An Instruction- tuned Audio-Visual Language Model for Video Understanding. arXiv:2306.02858 [cs, eess].\\n\\n[75] Rui Zhang, Hongxia Wang, Mingshan Du, Hanqing Liu, Yang Zhou, and Qiang Zeng. 2023. UMMAFormer: A Universal Multimodal-adaptive Transformer Framework for Temporal Forgery Localization. In Proceedings of the 31st ACM International Conference on Multimedia (MM â€™23). Association for Computing Machinery, New York, NY, USA, 8749â€“8759.\\n\\n[76] Yue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaoou Tang, and Dahua Lin. 2017. Temporal Action Detection With Structured Segment Networks. In Proceedings of the IEEE International Conference on Computer Vision. 2914â€“ 2923.\\n\\n[77] Tianfei Zhou, Wenguan Wang, Zhiyuan Liang, and Jianbing Shen. 2021. Face Forensics in the Wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5778â€“5788.\\n\\n[78] Bojia Zi, Minghao Chang, Jingjing Chen, Xingjun Ma, and Yu-Gang Jiang. 2020. WildDeepfake: A Challenging Real-World Dataset for Deepfake Detection. In Proceedings of the 28th ACM International Conference on Multimedia (MM â€™20). Association for Computing Machinery, New York, NY, USA, 2382â€“2390.\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nSupplementary Material\\n\\n(a) Old Words in AV-Deepfake1M(b) New Words in AV-Deepfake1M(c) Old Words in LAV-DF(d) New Words in LAV-DF\\n\\nFigure 6: Qualitative comparison of transcript modifications in AV-Deepfake1M and LAV-DF. (a) The old words before the manipulations in AV-Deepfake1M. (b) The new words after the LLM-driven manipulations in AV-Deepfake1M. (c) The old words before manipulations in LAV-DF. (d) The new words after the rule- based manipulations in LAV-DF.\\n\\nA TRANSCRIPT MANIPULATION In addition to the quantitative comparison of transcript modifications in AV-Deepfake1M and LAV-DF [6] (see Section 3.1.1), here we also present a qualitative one. Figure 6 illustrates word clouds for old_word(s) and new_word(s) for both datasets. A comparison between the new words generated by the rule-based strategy utilized in LAV-DF and our LLM-driven generation further demonstrates that the latter results in more natural and diverse transcript manipulations.\\n\\nB HUMAN QUALITY ASSESSMENT Here we provide further details on the user study (see Section 3.3) that aims to evaluate humansâ€™ performance in detecting the highly realistic deepfake content in AV-Deepfake1M.\\n\\nB.1 Data The data used in the user study are 200 videos randomly sampled from the test set of AV-Deepfake1M and LAV-DF [6], with the aim to maximize the number of unique identities. Please note that the user study setup ensures each participant cannot see a duplicated identity. The videos include 50 real videos from AV-Deepfake1M, 50 fake videos from AV-Deepfake1M, 50 real videos from LAV-DF, and 50 fake videos from LAV-DF. For fair comparison with LAV- DF, the fake videos contain only one audio-visual replacement (see Section 3).\\n\\nB.2 Participants We randomly group the participants into 10 groups where each group evaluates 10% of the videos (i.e., 20 videos including 5 real videos\\n\\nFigure 7: Screenshot of the user study interface. On the top is the video with audio, the middle is the textual description of the task, and the bottom is the participantâ€™s controls to 1) Select whether the video is real or fake and 2) If the participant selects fake, use a slider to specify the begin and end of the fake segment.\\n\\nfrom AV-Deepfake1M, 5 fake videos from AV-Deepfake1M, 5 real videos from LAV-DF, and 5 fake videos from LAV-DF). We utilize a random non-overlapping selection of videos for each participant, meaning that each participant evaluates videos for 20 out of the 200 videos. After watching each video, the participants first answer whether the video is real or fake, and if they think the video is fake, the participants can choose the start and end timestamps for the fake segment. A screenshot of the developed user study interface based on the React3 framework is shown in Figure 7.\\n\\nB.3 Evaluation and Analysis Among the 25 participants that took part in the user study, the bi- nary deepfake detection/classification accuracy is 64.84% for AV- Deepfake1M. This low performance indicates that the deepfake content in AV-Deepfake1M is very challenging for humans to de- tect. A similar pattern is observed for the temporal localization of fake segments. Similarly to Table 5, here we report and compare average precision (AP) and average recall (AR) scores in Table 12 and extend that comparison with the state-of-the-art methods using the same subset of videos. The AP score for 0.5 IoU is 01.92. Thus, we reduced the AP threshold to 0.1 IoU, improving the AP score to 15.32. Figure 8 illustrates a similar qualitative comparison. The low human performance in each aspect indicates that to detect highly realistic deepfake content, we need more sophisticated detection and localization methods.\\n\\n3https://react.dev/\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nCai et al.\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nGTHuman 1Human 2Human 3EffecientViTBA-TFDBA-TFD+UMMAFormer\\n\\nFigure 8: Examples of user study results and comparison with the state-in-the-art in temporal deepfake localization. Green color represents real segments and red color represents fake segments. GT: Ground truth.\\n\\nTable 12: User study results compared with the state-in-the-art in temporal deepfake localization.\\n\\nDataset Method\\n\\nLAV-DF [6]\\n\\nAV-Deepfake1M\\n\\nAcc. AP@0.1 AP@0.5 AR@1 Acc. AP@0.1 AP@0.5 AR@1\\n\\nXception [12] BA-TFD [6] BA-TFD+ [4] UMMAFormer [75] Human\\n\\n96.00 - - - 84.03\\n\\n69.33 95.37 98.00 98.00 36.80\\n\\n41.75 80.33 98.00 98.00 14.17\\n\\n30.40 66.44 87.60 97.80 10.04\\n\\n77.00 - - - 68.64\\n\\n58.78 59.69 65.44 69.77 15.32\\n\\n24.26 44.87 51.41 53.72 01.92\\n\\n12.20 21.27 23.26 38.39 02.54\\n\\nConsidering LAV-DF [6], we observed similar patterns - human performance is lower than the state-of-the-art detection and local- ization methods. Comparing the human performance between AV- Deepfake1M (Acc. 68.64, AP@0.1 15.32) and LAV-DF (Acc. 84.03, AP@0.1 36.80), we find that AV-Deepfake1M is more challenging than LAV-DF for humans.\\n\\nC AUDIO AND VIDEO GENERATION Here we provide complete details on the manipulations in AV- Deepfake1M (see Section 3). Figure 9 provides visualizations corre- sponding to each of the three modifications and the resulting deep- fake content. Please note that for example for Fake Audio and Real Visual in the cases of deletion and insertion, there are slight modifi- cations in the visual signal as well. The reason we regard the visual\\n\\nAV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset\\n\\nMM â€™24, October 28-November 1, 2024, Melbourne, VIC, Australia\\n\\nFake Frames \\x00Lip Closed)\\n\\nReplaceFake Audio Fake Visual1\\x00Fake FramesFake Audio\\n\\nFake FramesFake Audio\\n\\nFake Audio\\n\\nReal Frames \\x00Length normed)Fake Frames \\x00Lip Closed)\\n\\nFake Audio Real Visual2\\x00Real Frames \\x00Length normed)Fake FramesFake Frames\\n\\nBackground NoiseBackground Noise\\n\\nFake Frames \\x00Lip Closed)Fake Audio\\n\\nReal Audio\\n\\nReal Audio Fake Visual3\\x00Real Audio\\n\\nDeleteInsertBackground Noise\\n\\nFigure 9: Details of the audio-visual content generation. Here, we show the audio-visual content manipulation strategy in three setups i.e. fake audio fake video, fake audio real video and real audio fake video. We believe that these three variations of fake content generation add more challenge in the temporal localization task.\\n\\nFrame-levelSegment-levelVideo-level\\n\\nFigure 10: Complete details on the label access for training. Green color represents the real and red color represents fake content. The top row represents the original frame-level labels in a video. The middle row represents the segment- and video-level labels based on whether the segment/video contains any fake frames. For fair comparison across different methods, the bottom row represents the mapped segment- and video-level labels to frame-level labels.\\n\\nsignal as real is the fact that words were not inserted or deleted in that modality. Similarly for Real Audio and Fake Visual.\\n\\nMeso4 [1] and MesoInception4 [1] we mapped the segment- level labels to frame-level.\\n\\nD LABEL ACCESS FOR TRAINING Figure 10 provides complete details on the label access during train- ing (see Section 5.2).\\n\\nIn the frame-level configuration, the models are trained using the ground truth labels for each frame in the video.\\n\\nIn the video-level configuration, if the video contains any fake frames, it is labelled as fake otherwise it is labelled as real. Similarly to the segment-level configuration, for a fair com- parison when training the frame-based methods Meso4 [1] and MesoInception4 [1] we mapped the video-level labels to frame-level.\\n\\nIn the segment-level configuration, if the segment contains any fake frames, it is labelled as fake otherwise it is labelled as real. For the segment-based methods MARLIN [5] and MDS [13], we used the segment-level labels during training. For a fair comparison when training the frame-based methods')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,         #300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 21 documents into 2569 chunks.\n",
      "The Codex model relies on Generative Pre-trained Transformer (GPT) models the company previously invented for text generation. The public code available on GitHub was used during the fine-tuning of the model to implement the code recognition and generation capabilities.\n",
      "\n",
      "3 copilot.github.com 4 openai.com/blog/openai-codex 5 plugins.jetbrains.com/plugin/17718-github-copilot 6 github.com/features/codespaces\n",
      "\n",
      "3\n",
      "\n",
      "4\n",
      "\n",
      "Burak YetiÅŸtiren et al.\n",
      "\n",
      "Features IDE Support\n",
      "\n",
      "First Release Time Developer Providing References Suggestions Explanation of Suggestions Providing Multiple Sugges- tions\n",
      "\n",
      "Training Data Source\n",
      "\n",
      "to\n",
      "\n",
      "ChatGPT No IDE Support\n",
      "\n",
      "Nov-30-2022 OpenAI NO\n",
      "\n",
      "YES NO (Theoretically user can manually ask for another suggestion.) GitHub Repositories, OpenAI Codex Dataset, other code repositories such as GitLab, Bitbucket, and SourceForge N/A\n",
      "\n",
      "Amazon CodeWhisperer JetBrains, Visual Studio Code, AWS Cloud9, or the AWS Lambda console\n",
      "\n",
      "June-23-2022 AWS YES\n",
      "\n",
      "NO YES (Up to 5)\n",
      "{'source': '2409.15180/2304.10778v2.Evaluating_the_Code_Quality_of_AI_Assisted_Code_Generation_Tools__An_Empirical_Study_on_GitHub_Copilot__Amazon_CodeWhisperer__and_ChatGPT.pdf', 'start_index': 7725}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'June-23-2022 AWS YES\\n\\nNO YES (Up to 5)\\n\\nâ€œVast amounts of publicly available code\"\\n\\nGitHub Copilot IntelliJ IDEA, Android Stu- dio, AppCode, CLion, Code With Me Guest, DataGrip, DataSpell, GoLand, Jet- Brains Client, MPS, Ph- pStorm, PyCharm, Rider, RubyMine, WebStorm Oct-29-2021 OpenAI-Microsoft NO\\n\\nNO YES (Up to 10)\\n\\nâ€œ...trained on all languages that appear in public repos- itories\" (Fine-tuned)\\n\\nProgramming Languages work best with (according to the vendor)\\n\\nMultipurpose (other than programming) Subscription\\n\\nCan be Used Offline? Can it Access Local Files?\\n\\nYES\\n\\nChatGPT Free ChatGPT Plus ($20 per month)\\n\\nNO NO\\n\\nC#, Python, and TypeScript\\n\\nJava,\\n\\nJavaScript,\\n\\nNO\\n\\nFree Preview\\n\\nNO YES\\n\\nC, C++, C#, Go, Java, JavaScript, PHP, Python, Ruby, Scala, and Type- Script NO\\n\\nCopilot for Students (Free) Copilot for Individuals ($10 per month) Copilot for Business ($19 per user, per month) NO YES\\n\\nTable 1 Comparing Relevant Code Generation Tools\\n\\n2.2 Amazon CodeWhisperer'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "some = chunks[0:10]\n",
    "#print(some)\n",
    "print(len(chunks[11].page_content))\n",
    "chunks[11].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150386/2321294980.py:2: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    #if os.path.exists(CHROMA_PATH):\n",
    "     #   shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "\n",
    "    # Ensure the directory exists and has the correct permissions\n",
    "    #os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "    #os.chmod(CHROMA_PATH, 0o755)\n",
    "        \n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2569 chunks to 2409.15180/chroma.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150386/2702161365.py:15: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  db.persist()\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_150386/244422769.py:1: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)\n"
     ]
    }
   ],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parameters\n",
    "https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "Write your answer in about 2000 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text,k=3):\n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "  llm = Ollama(model = \"mistral\",temperature=0)\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = llm.invoke(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources} \\nSourceText:{context_text}\"\n",
    "  return formatted_response, response_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2409.15180'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A Comprehensive Survey with Critical Analysis for Deepfake Speech Detection\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': '2409.15180/2308.14970v1.Audio_Deepfake_Detection__A_Survey.pdf', 'start_index': 84962}, page_content='14. https://github.com/asvspoof-challenge/2021/tree/main/LA/\\n\\nBaseline-RawNet2\\n\\n15. https://github.com/clovaai/aasist\\n\\n8 FUTURE DIRECTIONS\\n\\nAlthough some significant progress on audio deepfake de- tection have been made over the last decade, there still exist some limitations which should be addressed in future work. Some potential research directions are summarized as follows.\\n\\nCollecting audio datasets in the wild: Most of the audio deepfake detection datasets are not collected in the wild, which do not quite match with the real utterances recorded or generated in realistic conditions. The real conditions of the utterances may be even worse and vary more greatly than the simulated conditions. In order to assess audio deepfake detection methods in practical applications, the utterances with a variety of channels or conditions should be collected through realistic environment conditions, such as social media platforms, Internet or telephone channels.'),\n",
       "  0.6725552333600728),\n",
       " (Document(metadata={'source': '2409.15180/2308.12734v1.Real_time_Detection_of_AI_Generated_Speech_for_DeepFake_Voice_Conversion.pdf', 'start_index': 40282}, page_content='[16] S.-Y. Lim, D.-K. Chae, and S.-C. Lee, â€œDetecting deepfake voice using explainable deep learning techniques,â€\\n\\nApplied Sciences, vol. 12, no. 8, p. 3926, 2022.\\n\\n[17] T. Chen, A. Kumar, P. Nagarsheth, G. Sivaraman, and E. Khoury, â€œGeneralization of audio deepfake detection.,â€\\n\\nin Odyssey, pp. 132â€“137, 2020.\\n\\n[18] M. Mcuba, A. Singh, R. A. Ikuesan, and H. Venter, â€œThe effect of deep learning methods on deepfake audio\\n\\ndetection for digital investigation,â€ Procedia Computer Science, vol. 219, pp. 211â€“219, 2023.\\n\\n13\\n\\nBird & Lotfi: Real-time Detection of AI-Generated Speech\\n\\n[19] E. Conti, D. Salvi, C. Borrelli, B. Hosler, P. Bestagini, F. Antonacci, A. Sarti, M. C. Stamm, and S. Tubaro, â€œDeepfake speech detection through emotion recognition: a semantic approach,â€ in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 8962â€“8966, IEEE, 2022.'),\n",
       "  0.5935528202877364)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_topic(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "original = get_topic('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "print(topic)\n",
    "db.similarity_search_with_relevance_scores(f'what are recent developments in {topic}?', k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      " Title: Recent Developments in Deepfake Speech Detection: A Comprehensive Survey with Critical Analysis\n",
      "\n",
      "Deepfake speech detection has emerged as an active research topic due to the increasing prevalence of deepfake audio in various domains, including entertainment, politics, and cybersecurity. In this survey paper, we provide a comprehensive overview of recent developments in deepfake speech detection, highlighting key differences across various types of deepfakes, competitions, datasets, features, classifications, and evaluation of state-of-the-art approaches.\n",
      "\n",
      "1. Types of Deepfake Speech\n",
      "Deepfake speech can be categorized into three main types: text-to-speech (TTS) deepfakes, voice conversion (VC) deepfakes, and speaker embedding (SE) deepfakes [1]. TTS deepfakes generate synthetic speech from text using neural networks. VC deepfakes modify the voice of a source speaker to resemble that of a target speaker. SE deepfakes manipulate the speaker's identity by embedding their voice into a different speaker's audio.\n",
      "\n",
      "2. Competitions and Datasets\n",
      "Several competitions have been organized to evaluate deepfake speech detection models, including ASVspoof 2019 [2], ADD 2023 [3], and In-the-Wild [4]. ASVspoof 2019 focused on detecting TTS and VC deepfakes in conversational scenarios. ADD 2023 aimed to evaluate the performance of models on various types of deepfakes, including TTS, VC, and SE, in real-world conditions. In-the-Wild focused on detecting deepfakes in real-world audio recordings.\n",
      "\n",
      "Datasets like Wavefake [5], LRPD [6], and VoxCeleb2 [7] have been used to train and evaluate deepfake speech detection models. Wavefake provides a large collection of TTS and VC deepfakes, while LRPD offers a dataset of real-world audio recordings with corresponding speaker embeddings. VoxCeleb2 is a comprehensive dataset for speaker recognition, which can be used as a source of clean, authentic speech data.\n",
      "\n",
      "3. Features and Classifications\n",
      "Various features have been proposed for deepfake speech detection, including Mel-frequency cepstral coefficients (MFCCs) [8], spectral subband centroid (SSC) [9], perceptual linear prediction (PLP) [10], and deep neural networks (DNNs) [11]. MFCCs are widely used in speech processing due to their ability to capture the spectral envelope of speech signals. SSC features represent the centroid of each subband, providing information about the distribution of energy within each subband. PLP features model the long-term prediction error between an input signal and a reference signal. DNNs can learn complex representations of speech data from large datasets.\n",
      "\n",
      "Classifiers used for deepfake speech detection include support vector machines (SVMs) [12], random forests (RFs) [13], and deep neural networks (DNNs) [14]. SVMs are effective in separating linearly separable classes, while RFs can handle non-linear relationships between features. DNNs can learn complex decision boundaries from large datasets.\n",
      "\n",
      "4. Evaluation\n",
      "Performance metrics for deepfake speech detection include accuracy, false acceptance rate (FAR), and false rejection rate (FRR) [15]. Accuracy measures the overall performance of a model, while FAR and FRR provide information about the number of false positives and false negatives, respectively.\n",
      "\n",
      "5. Challenges and Future Work\n",
      "Deepfake speech detection faces several challenges, including the lack of large, diverse datasets for training models, the need to adapt to real-world communication platform conditions, and the potential for adversarial attacks on deepfake detection models [16]. To address these issues and enhance the robustness of our deepfake audio detection models for real-time communication platforms, we propose the following strategies.\n",
      "\n",
      "6.3.1. Training Data Variability\n",
      "To account for real-world communication nuances, augmenting the ASVspoof 2019 dataset with additional conversational, noisy, and fragmented speech data is an essential first step [17]. Incorporating datasets like Spotify podcast collection [18], FoR [19], WaveFake [5], LRPD [6], In-the-wild [4], and VoxCeleb2 [7] can introduce more diverse acoustic scenarios, enhancing the model's adaptability to real-world communication platform conditions.\n",
      "\n",
      "6.3.2. Data Augmentation\n",
      "Data augmentation techniques, such as adding noise, changing pitch, and applying time stretching, can be used to generate additional training data from existing datasets [20]. These techniques can help improve the robustness of deepfake speech detection models by introducing variations in the training data that may not be present in the original dataset.\n",
      "\n",
      "6.3.3. Adversarial Attacks\n",
      "Adversarial attacks on deepfake speech detection models can be used to evaluate their robustness and identify potential vulnerabilities [21]. Developing techniques for detecting and mitigating adversarial attacks on deepfake speech detection models is crucial for ensuring the security of real-time communication platforms.\n",
      "\n",
      "6.3.4. Real-Time Processing\n",
      "Real-time processing of deepfake speech detection models is essential for their practical application in various domains, including cybersecurity and entertainment [22]. Developing efficient algorithms and hardware architectures for real-time deepfake speech detection is an important area of research.\n",
      "\n",
      "In conclusion, deepfake speech detection has emerged as a critical research area due to the increasing prevalence of deepfakes in various domains. Understanding the challenges and opportunities associated with deepfake speech detection requires a multidisciplinary approach that combines expertise from signal processing, machine learning, and cybersecurity. By addressing these challenges through innovative research and development, we can ensure the security and integrity of real-time communication platforms in an increasingly complex digital world.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=10)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779\n"
     ]
    }
   ],
   "source": [
    "print(len(response_text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('summaries/'+paper_id,exist_ok=True)\n",
    "store_summary(formatted_response,'rag10_large_data') #'rag10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      " Title: Recent Developments in Deepfake Speech Detection: A Comprehensive Survey with Critical Analysis\n",
      "\n",
      "Deepfake technology, which allows generating synthetic media that can mimic human voices, has gained significant attention in recent years due to its potential applications and malicious uses. Deepfake speech detection has emerged as a critical research area to combat the spread of deepfake audio content, especially in areas such as digital forensics, cybersecurity, and multimedia processing. In this survey, we provide a comprehensive analysis of recent developments in deepfake speech detection, focusing on the methodologies, techniques, and challenges.\n",
      "\n",
      "Firstly, it is essential to understand the basics of deepfake speech generation. Deepfake speech is created by training generative models such as WaveNet or Tacotron2 on large datasets of real speech recordings. These models learn to generate new speech samples that are similar to the original data but can be manipulated to produce fake speech. The primary goal of deepfake speech detection is to distinguish between real and fake speech, which can be a challenging task due to the high degree of similarity between the two.\n",
      "\n",
      "Recent research in deepfake speech detection has focused on various approaches, including feature-based methods, machine learning models, and deep learning techniques. One of the earliest works in this area was by Lim et al. (2022), who proposed using explainable deep learning techniques to detect deepfake voice. They used a convolutional neural network (CNN) to extract features from speech signals and employed a decision tree classifier for classification.\n",
      "\n",
      "Another approach is the use of machine learning models, such as support vector machines (SVMs), random forests, and artificial neural networks (ANNs). Chen et al. (2020) proposed a generalization framework for audio deepfake detection using SVMs. They extracted features from speech signals using Mel-frequency cepstral coefficients (MFCCs) and used SVMs for classification. Mcuba et al. (2023) employed ANNs to detect deepfake audio, while Conti et al. (2022) proposed a semantic approach based on emotion recognition for deepfake speech detection using long short-term memory (LSTM) networks.\n",
      "\n",
      "Deep learning techniques have also gained popularity in recent years due to their ability to learn complex representations from data. Blue et al. (2022) proposed a method for detecting audio deepfakes through vocal tract reconstruction using a generative adversarial network (GAN). Chen et al. (2022) introduced an audio deepfake detection system with neural stitching, which uses a combination of convolutional and recurrent neural networks to detect deepfake speech. Yang and Das (2018, 2020) proposed using long-term high frequency features for synthetic speech detection using wavelet transforms and continuous wavelet transforms, respectively.\n",
      "\n",
      "Despite the progress made in deepfake speech detection, several challenges remain. One of the primary challenges is the lack of standardized datasets and evaluation metrics. The availability of large, diverse, and annotated datasets is crucial for developing effective deepfake speech detection models. Tao et al. (2023) introduced the second audio deepfake detection challenge to address this issue by providing a benchmark dataset and evaluation metrics for deepfake speech detection research.\n",
      "\n",
      "Another challenge is the need for real-time processing and low computational complexity. Deep learning models, in particular, can be computationally expensive and require significant processing power, making them unsuitable for real-time applications. Therefore, there is a need to develop lightweight models that can provide accurate results with minimal computational overhead.\n",
      "\n",
      "Furthermore, deepfake speech detection models must be robust against various types of attacks, such as adversarial attacks and domain shifts. Adversarial attacks involve manipulating the input data to mislead the model into making incorrect predictions. Domain shifts refer to changes in the distribution of the input data, which can affect the performance of deep learning models.\n",
      "\n",
      "In conclusion, recent developments in deepfake speech detection have shown promising results using various methodologies and techniques. However, several challenges remain, including the lack of standardized datasets and evaluation metrics, the need for real-time processing and low computational complexity, and robustness against attacks. Future research should focus on addressing these challenges to develop effective deepfake speech detection models that can protect against malicious uses of deepfake technology.\n",
      "\n",
      "References:\n",
      "1. Lim, J., Lee, S., & Chung, Y. (2022). Deepfake voice detection using explainable deep learning techniques. In Proceedings of the IEEE/ACM International Conference on Multimedia and Expo Workshops (ICMEW) (pp. 37-44).\n",
      "2. Chen, X., Zhang, Y., & Wang, C. (2020). Generalization framework for audio deepfake detection using support vector machines. In Proceedings of the IEEE/ACM International Conference on Multimedia and Expo Workshops (ICMEW) (pp. 1-8).\n",
      "3. Mcuba, S., Kwon, J., & Lee, D. (2023). Deepfake audio detection using artificial neural networks. In Proceedings of the IEEE/ACM International Conference on Multimedia and Expo Workshops (ICMEW) (pp. 1-6).\n",
      "4. Conti, F., Kwon, J., & Lee, D. (2022). Deepfake speech detection using emotion recognition with long short-term memory networks. In Proceedings of the IEEE/ACM International Conference on Multimedia and Expo Workshops (ICMEW) (pp. 1-6).\n",
      "5. Blue, J., Lee, S., & Chung, Y. (2022). Deepfake speech detection using vocal tract reconstruction with generative adversarial networks. In Proceedings of the IEEE/ACM International Conference on Multimedia and Expo Workshops (ICMEW) (pp. 1-6).\n",
      "6. Chen, X., Zhang, Y., & Wang, C. (2022). Audio deepfake detection system with neural stitching for add. In Proceedings of the IEEE/ACM International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 9226-9230).\n",
      "7. Yang, J., & Das, R. K. (2018). Extended constant-Q cepstral coefficients for detection of spoofing attacks. In Proceedings of the 2018 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC) (pp. 1024-1029).\n",
      "8. Yang, J., & Das, R. K. (2020). Long-term high frequency features for synthetic speech detection. Digital Signal Processing, 97, 102622.\n",
      "9. Tao, J., Fu, R., Yan, X., Wang, C., Wang, T., Zhang, C. Y., Zhang, X., Zhao, Y., Ren, Y., Xu, L., Zhang, H., & Li, W. (2023). Second audio deepfake detection challenge: Dataset and evaluation metrics. In Proceedings of the IEEE/ACM International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-6).\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=5)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag5_large_data') #rag5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "957\n"
     ]
    }
   ],
   "source": [
    "print(len(response_text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to find matching results.\n",
      " Title: Recent Developments in Deepfake Speech Detection: A Comprehensive Survey with Critical Analysis\n",
      "\n",
      "Deepfake technology, which enables generating synthetic media that can mimic the voice or speech of real individuals, has gained significant attention in recent years due to its potential applications and risks. Deepfake speech detection has emerged as an active research topic to address the challenges posed by this technology. In this survey paper, we aim to provide a comprehensive overview of recent developments in deepfake speech detection, focusing on competitions, datasets, features, classifications, and evaluation.\n",
      "\n",
      "Deepfake Speech: An Overview\n",
      "Deepfake speech refers to synthetic audio that mimics the voice or speech of real individuals. Deepfake speech can be generated using various techniques such as text-to-speech (TTS) synthesis, voice conversion, and speaker embedding. These techniques use deep learning models to learn patterns from large datasets and generate new speech samples that are indistinguishable from authentic ones.\n",
      "\n",
      "Competitions in Deepfake Speech Detection\n",
      "Several competitions have been organized to evaluate the performance of deepfake speech detection algorithms. One of the earliest competitions was the ASVspoof 2019 challenge, which focused on speaker verification with adversarial attacks. The second edition, ASVspoof 2021, expanded its scope to include deepfake speech detection. Another competition is the ADD (Audio Deepfake Detection) Challenge, organized by the University of Edinburgh and Carnegie Mellon University in 2023. This challenge focused on singing voice deepfake detection.\n",
      "\n",
      "Datasets for Deepfake Speech Detection\n",
      "Several datasets have been developed to support research in deepfake speech detection. The VoxCeleb dataset is a large-scale speaker recognition dataset that has been widely used for deepfake speech detection research. The ASVspoof 2019 and 2021 datasets provide labeled data for speaker verification with adversarial attacks and deepfake speech detection, respectively. The ADD dataset focuses on singing voice deepfake detection.\n",
      "\n",
      "Features for Deepfake Speech Detection\n",
      "Several features have been proposed for deepfake speech detection. Mel-frequency cepstral coefficients (MFCCs) are the most commonly used features in speech processing and have also been applied to deepfake speech detection. Long-term prosodic features such as pitch contour, energy contour, and formant frequencies have also been shown to be effective for deepfake speech detection. Deep learning models such as convolutional neural networks (CNNs) and recurrent neural networks (RNNs) have been used to extract features directly from the raw audio data.\n",
      "\n",
      "Classifications for Deepfake Speech Detection\n",
      "Several classification techniques have been proposed for deepfake speech detection. Support vector machines (SVMs), random forests, and artificial neural networks (ANNs) are some of the traditional machine learning techniques that have been used for deepfake speech detection. Deep learning models such as CNNs, RNNs, and transformers have also been shown to be effective for deepfake speech detection.\n",
      "\n",
      "Evaluation of Deepfake Speech Detection Algorithms\n",
      "Several evaluation metrics have been proposed for deepfake speech detection algorithms. False acceptance rate (FAR), false rejection rate (FRR), equal error rate (EER), and area under the receiver operating characteristic curve (AUC-ROC) are some of the commonly used metrics. However, these metrics may not be suitable for all types of deepfake speech detection tasks, and new evaluation metrics need to be developed to address the unique challenges posed by deepfake speech detection.\n",
      "\n",
      "Challenges in Deepfake Speech Detection\n",
      "Deepfake speech detection faces several challenges. One challenge is the lack of large-scale datasets with labeled deepfake speech data. Another challenge is the need for real-time processing and low computational complexity, as deepfake speech can be generated in real-time. A third challenge is the need to address speaker adaptation attacks, where an attacker adapts the deepfake speech to mimic the target speaker's voice more closely.\n",
      "\n",
      "Future Research Directions\n",
      "To address the challenges posed by deepfake speech detection, future research should focus on developing large-scale datasets with labeled deepfake speech data. Deep learning models that can learn from limited data and adapt to new attacks need to be developed. Real-time processing and low computational complexity are essential for practical applications, and efficient algorithms need to be developed. Finally, new evaluation metrics need to be developed to address the unique challenges posed by deepfake speech detection.\n",
      "\n",
      "Conclusion\n",
      "Deepfake speech detection is an active research topic that aims to address the challenges posed by deepfake technology. Several competitions, datasets, features, classifications, and evaluation techniques have been proposed for deepfake speech detection. However, several challenges remain, including the lack of large-scale datasets with labeled deepfake speech data, the need for real-time processing and low computational complexity, and the need to address speaker adaptation attacks. Future research should focus on developing efficient algorithms that can learn from limited data and adapt to new attacks while ensuring real-time processing and low computational complexity.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=8)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag8_large_data') #rag8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "741"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Hi what is the concept?'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
