{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "https://github.com/pixegami/langchain-rag-tutorial/blob/main/query_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "import os\n",
    "import shutil\n",
    "from langchain_community.llms import Ollama\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"2402.01383v1\"\n",
    "#CHROMA_PATH = paper_id+\"/chroma\"\n",
    "CHROMA_PATH = paper_id+\"/chroma300\"\n",
    "DATA_PATH = paper_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_summary(survey1,filename):\n",
    "    with open(f'summaries/'+filename+'.txt', 'w') as file:\n",
    "        file.write(survey1)\n",
    "def write_json(documents_data,file_name):\n",
    "    converted_data = [dict(item) for item in documents_data['source_documents']]\n",
    "    dict_data = {'answer':documents_data['result'],'source':converted_data}\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '2402.01383v1/2307.07889v3.LLM_Comparative_Assessment__Zero_shot_NLG_Evaluation_through_Pairwise_Comparisons_using_Large_Language_Models.pdf'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 45 documents into 20318 chunks.\n",
      "language processing benchmarks. In response to this challenge, recent studies commonly introduce an evaluation approach, namely the Elo rating sys- tem (Elo, 1967), wherein either human or LLM judges are enlisted to adjudicate between two LLM- generated outputs (Askell et al., 2021; Bai et al.,\n",
      "{'source': '2402.01383v1/2307.03025v3.Style_Over_Substance__Evaluation_Biases_for_Large_Language_Models.pdf', 'start_index': 2067}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks[11].page_content\n",
    "some = chunks[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/sentence_transformers/cross_encoder/CrossEncoder.py:11: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    #if os.path.exists(CHROMA_PATH):\n",
    "     #   shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "\n",
    "    # Ensure the directory exists and has the correct permissions\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "   # os.chmod(CHROMA_PATH, 0o755)\n",
    "        \n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20318 chunks to 2402.01383v1/chroma300.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"mistral\",temperature=0)\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "from langchain.chains import RetrievalQA\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=db.as_retriever(),\n",
    "            return_source_documents = True,\n",
    "            \n",
    "            #memory=ConversationSummaryMemory(llm = Ollama(model=\"mistral\")),\n",
    "            #chain_type_kwargs={\"prompt\": pt, \"verbose\": True},\n",
    "        )\n",
    "\n",
    "answer = rag.invoke(\"What is the recent development of LLM generated text evaluation?\")\n",
    "rag_overview = answer['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in LLM (Large Language Model) generated text evaluation include using LLMs to evaluate the quality of their own text outputs as an alternative to human evaluations. This approach can provide a more comprehensive understanding of LLM performance and predict how a tuned version might behave in various scenarios. Additionally, applying LLMs as evaluators for text generation has been shown to be scalable, cost-effective, and can significantly enhance the ability to assess the quality of generated text. For instance, Fu et al. (2023) use LLM's predicted text probability as the automated score. Furthermore, this approach has been applied to evaluate Chinese LLMs, resulting in a faster evaluation process and decreased average annotation cost per sample. However, it is important to note that the utilization of LLMs as evaluators for text generation is still in the exploratory phase, and there are limitations that provide opportunities for future work, such as the performance of LLMs as reliable evaluators and potential biases in their predictions.\n"
     ]
    }
   ],
   "source": [
    "print(rag_overview)\n",
    "store_summary(rag_overview,'rag_overview300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the recent development of LLM generated text evaluation?',\n",
       " 'result': \" Recent developments in LLM (Large Language Model) generated text evaluation include using LLMs to evaluate the quality of their own text outputs as an alternative to human evaluations. This approach can provide a more comprehensive understanding of LLM performance and predict how a tuned version might behave in various scenarios. Additionally, applying LLMs as evaluators for text generation has been shown to be scalable, cost-effective, and can significantly enhance the ability to assess the quality of generated text. For instance, Fu et al. (2023) use LLM's predicted text probability as the automated score. Furthermore, this approach has been applied to evaluate Chinese LLMs, resulting in a faster evaluation process and decreased average annotation cost per sample. However, it is important to note that the utilization of LLMs as evaluators for text generation is still in the exploratory phase, and there are limitations that provide opportunities for future work, such as the performance of LLMs as reliable evaluators and potential biases in their predictions.\",\n",
       " 'source_documents': [Document(metadata={'source': '2402.01383v1/2306.05087v2.PandaLM__An_Automatic_Evaluation_Benchmark_for_LLM_Instruction_Tuning_Optimization.pdf', 'start_index': 38685}, page_content='2020); Yang et al. (2021; 2023a)) into an evaluation framework could offer a more comprehensive understanding of LLM performance. For instance, analyzing and evaluating the extended text outputs of an untuned LLM can help predict how a tuned version might behave in various scenarios. This approach'),\n",
       "  Document(metadata={'source': '2402.01383v1/2310.19740v1.Collaborative_Evaluation__Exploring_the_Synergy_of_Large_Language_Models_and_Humans_for_Open_ended_Generation_Evaluation.pdf', 'start_index': 8145}, page_content='As research in LLMs continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evalua- tions (Jain et al., 2023; Taori et al., 2023; Chiang et al., 2023). Fu et al. (2023) use LLM’s predicted text probability as the automated score, assuming'),\n",
       "  Document(metadata={'source': '2402.01383v1/2308.01862v1.Wider_and_Deeper_LLM_Networks_are_Fairer_LLM_Evaluators.pdf', 'start_index': 39881}, page_content='the best results, significantly enhancing the ability of LLMs to evaluate the quality of generated text. Furthermore, we apply our evaluator to assess the performance of Chinese LLMs, where it proves to speed up LLM evaluation process by 4.6 times and decrease the average annotation cost per sample'),\n",
       "  Document(metadata={'source': '2402.01383v1/2305.14658v3.Evaluate_What_You_Can_t_Evaluate__Unassessable_Quality_for_Generated_Response.pdf', 'start_index': 40449}, page_content='While we analyze the challenges and possibilities and of LLMs as text generation evaluators by con- structed benchmarks, the utilization of LLMs as evaluators for text generation is in the exploratory phase. There are limitations that provide avenues for future work: i) the performance of LLMs as')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = answer['source_documents'][0]\n",
    "converted_data = [dict(item) for item in answer['source_documents']]\n",
    "#dict(answer['source_documents'])\n",
    "converted_data\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(answer,'rag_overview300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mtype\u001b[39m(answer)\n\u001b[0;32m----> 2\u001b[0m sources \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m doc, _score \u001b[38;5;129;01min\u001b[39;00m answer]\n\u001b[1;32m      3\u001b[0m sources\n",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mtype\u001b[39m(answer)\n\u001b[0;32m----> 2\u001b[0m sources \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m doc, _score \u001b[38;5;129;01min\u001b[39;00m answer]\n\u001b[1;32m      3\u001b[0m sources\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "type(answer)\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in answer]\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change more parameters example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VectorStore.as_retriever() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m----> 6\u001b[0m retriever\u001b[38;5;241m=\u001b[39m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the given context to answer the question. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, say you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: VectorStore.as_retriever() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "retriever=db.as_retriever(20)\n",
    "llm = Ollama(model = \"mistral\",temperature=0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the recent development of LLM generated text evaluation?'\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VectorStoreRetriever' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'VectorStoreRetriever' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parameters\n",
    "https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "Write your answer in about 2000 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text,k=3):\n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "  llm = Ollama(model = \"mistral\",temperature=0)\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = llm.invoke(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources} \\nSourceText:{context_text}\"\n",
    "  return formatted_response, response_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': '2402.01383v1/2312.10355v1.CoAScore__Chain_of_Aspects_Prompting_for_NLG_Evaluation.pdf', 'start_index': 16491}, page_content='aspect. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies.'),\n",
       "  0.8336756809872163),\n",
       " (Document(metadata={'source': '2402.01383v1/2312.10355v1.CoAScore__Chain_of_Aspects_Prompting_for_NLG_Evaluation.pdf', 'start_index': 13014}, page_content='prompts often lead to relatively signifi- cant differences in the evaluation results [4, 12, 15, 36]. Additionally, there is ongoing research into leveraging LLMs for the generation of NLG evaluation datasets, aiming to reduce the need for manual evaluations to some extent [5, 25]. In our work, we'),\n",
       "  0.81348304157195),\n",
       " (Document(metadata={'source': '2402.01383v1/2312.10355v1.CoAScore__Chain_of_Aspects_Prompting_for_NLG_Evaluation.pdf', 'start_index': 493}, page_content='superior performance on various NLG evaluation tasks. However, current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation be- tween various aspects. To fill this research gap, in this work, we propose an NLG evaluation metric called'),\n",
       "  0.8069038342257024),\n",
       " (Document(metadata={'source': '2402.01383v1/2309.13308v1.Calibrating_LLM_Based_Evaluator.pdf', 'start_index': 39434}, page_content='LLM-Based NLG Evaluation With the emergence of LLM, recent research works focus on LLM-based evaluators given their promising instruction-following and generalization capability. A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods and'),\n",
       "  0.7927760420174567),\n",
       " (Document(metadata={'source': '2402.01383v1/2311.18702v2.CritiqueLLM__Towards_an_Informative_Critique_Generation_Model_for_Evaluation_of_Large_Language_Model_Generation.pdf', 'start_index': 6926}, page_content='Evaluation is a long-standing task in NLP, which becomes more challenging with the rapid develop- ment of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are mainly two lines of work on LLM evaluation, including NLU-style and NLG-style evaluations. NLU-style evaluation methods'),\n",
       "  0.7751973922936846),\n",
       " (Document(metadata={'source': '2402.01383v1/2310.19740v1.Collaborative_Evaluation__Exploring_the_Synergy_of_Large_Language_Models_and_Humans_for_Open_ended_Generation_Evaluation.pdf', 'start_index': 35239}, page_content='We propose an LLM-ideation-human-scrutiny pipeline to explore the synergy of LLMs and hu- mans in establishing evaluation criteria and con- ducting multi-dimensional evaluations for open- ended NLG tasks. We find that LLM’s criteria are generally comprehensive but tend to exaggerate un- necessary'),\n",
       "  0.7594116695055657),\n",
       " (Document(metadata={'source': '2402.01383v1/2309.13308v1.Calibrating_LLM_Based_Evaluator.pdf', 'start_index': 40539}, page_content='existing LLM-based NLG evaluators and uncover they suffer from insufficient prompting, where the scoring guidelines are absent and only output spaces are provided, resulting in inconsistent and misaligned evaluations. We emphasize the significance of aligned scoring criteria as a consensus between'),\n",
       "  0.7593515891730135),\n",
       " (Document(metadata={'source': '2402.01383v1/2307.07889v3.LLM_Comparative_Assessment__Zero_shot_NLG_Evaluation_through_Pairwise_Comparisons_using_Large_Language_Models.pdf', 'start_index': 34281}, page_content='and is an effective automatic assess- ment, achieving near state-of-the-art performance for a range of NLG evaluation tasks. Furthermore, we show that LLMs are prone to have positional bias that could impact their decisions, however, we introduce a simple debiasing approach that leads to'),\n",
       "  0.7505691974677073),\n",
       " (Document(metadata={'source': '2402.01383v1/2305.14239v2.On_Learning_to_Summarize_with_Large_Language_Models_as_References.pdf', 'start_index': 34339}, page_content='LLM-based Automatic Evaluation Recent work has explored using LLMs for automatic NLP eval- uation. GPTScore (Fu et al., 2023) leverages the LLM-predicted probability of text sequences as the quality score. On the other hand, a line of work (Chiang and yi Lee, 2023; Gao et al., 2023; Chen et al.,'),\n",
       "  0.7368909856925274),\n",
       " (Document(metadata={'source': '2402.01383v1/2307.07889v3.LLM_Comparative_Assessment__Zero_shot_NLG_Evaluation_through_Pairwise_Comparisons_using_Large_Language_Models.pdf', 'start_index': 1364}, page_content='is a simple, general and effective approach for NLG assessment. For moderate- sized open-source LLMs, such as FlanT5 and Llama2-chat, comparative assessment is supe- rior to prompt scoring, and in many cases can achieve performance competitive with state-of- the-art methods. Additionally, we'),\n",
       "  0.7323951181415975)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"LLM-based NLG Evaluation: Current Status and Challenges\"\n",
    "db.similarity_search_with_relevance_scores(f'what are recent developments in {topic}?', k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in NLG evaluation. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation, based on the context provided in the text.\n",
      "\n",
      "Firstly, it is essential to understand that NLG evaluation is a long-standing task in Natural Language Processing (NLP), which has become more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are two main lines of work on LLM evaluation: NLU-style and NLG-style evaluations. NLU-style evaluation methods focus on the model's ability to understand language, while NLG-style evaluations assess the model's ability to generate human-like text (Bawden et al., 2019).\n",
      "\n",
      "Recent research works have focused on LLM-based evaluators due to their promising instruction-following and generalization capability. A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods (Raffel et al., 2019; Schick et al., 2023). These methods involve providing the model with a set of prompts and evaluating its performance based on the generated text.\n",
      "\n",
      "However, current work often employs LLMs to independently evaluate different aspects of NLG tasks, largely ignoring the rich correlation between various aspects (Bawden et al., 2019). To fill this research gap, a new NLG evaluation metric called LLM-Based NLG Evaluation has been proposed. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies.\n",
      "\n",
      "The text also highlights that prompts often lead to significant differences in evaluation results (4, 12, 15, 36). Therefore, there is ongoing research into leveraging LLMs for the generation of NLG evaluation datasets, aiming to reduce the need for manual evaluations to some extent (5, 25).\n",
      "\n",
      "Moreover, the text emphasizes that LLMs are prone to have positional bias that could impact their decisions. However, a simple debiasing approach has been introduced to address this issue (Fu et al., 2023; Chiang and yi Lee, 2023; Gao et al., 2023).\n",
      "\n",
      "Recent work has also explored using LLMs for automatic NLG evaluation. For instance, GPTScore (Fu et al., 2023) leverages the LLM-predicted probability of text sequences as the quality score. Comparative assessment is superior to prompt scoring for moderate-sized open-source LLMs such as FlanT5 and Llama2-chat (Bawden et al., 2019). In many cases, this approach can achieve performance competitive with state-of-the-art methods.\n",
      "\n",
      "However, there are still challenges in LLM-based NLG evaluation. For instance, the text highlights that existing LLM-based evaluators suffer from insufficient prompting, where scoring guidelines are absent, and only output spaces are provided, resulting in inconsistent and misaligned evaluations (Bawden et al., 2019). Therefore, there is a need for aligned scoring criteria as a consensus between humans and LLMs.\n",
      "\n",
      "Another challenge is the lack of standardized evaluation metrics for NLG tasks. While LLMs have shown promising results in various NLG tasks, it is essential to establish standardized evaluation metrics to ensure fairness and comparability across different models and datasets (Bawden et al., 2019).\n",
      "\n",
      "Furthermore, there is a need for more research on the ethical implications of LLM-based NLG evaluation. For instance, how can we ensure that LLMs do not generate biased or harmful text? How can we ensure that LLMs respect user privacy and confidentiality? These are important questions that need to be addressed as we continue to explore the potential of LLMs in NLG evaluation.\n",
      "\n",
      "In conclusion, recent developments in LLM-based NLG evaluation have shown promising results, but there are still challenges that need to be addressed. The use of LLMs for automatic NLG evaluation has become increasingly popular due to their instruction-following and generalization capability. However, there is a need for more research on standardized evaluation metrics, ethical implications, and the rich correlation between various aspects of NLG tasks. By addressing these challenges, we can ensure that LLMs are used effectively and ethically in NLG evaluation, leading to more accurate and reliable evaluations of NLG systems.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=10)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(formatted_response,'rag10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in evaluating NLG systems using LLMs. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation.\n",
      "\n",
      "Firstly, it is essential to understand that LLMs have shown remarkable performance on various NLG evaluation tasks (Liu et al., 2021; Raffel et al., 2019). However, most existing work focuses on employing LLMs independently to evaluate different aspects of NLG, such as fluency, factual accuracy, and coherence. This approach ignores the rich correlation between various aspects, which is a significant research gap (Bawden et al., 2023).\n",
      "\n",
      "One line of recent work on LLM-based NLG evaluation focuses on preliminary explorations of LLM-based evaluators using prompting methods (Schick et al., 2021; Keskar et al., 2022). These studies aim to leverage the instruction-following and generalization capability of LLMs to generate evaluation datasets, reducing the need for manual evaluations to some extent. However, these approaches often lead to significant differences in evaluation results (Gardner et al., 2021; Zellers et al., 2023).\n",
      "\n",
      "Another line of work on LLM-based NLG evaluation is the development of NLU-style and NLG-style evaluations. NLU-style evaluation methods assess the ability of an NLG system to understand and process input, while NLG-style evaluations focus on the output generated by the NLG system (Bawden et al., 2023). Both styles have their advantages and limitations, and researchers are exploring ways to combine them to create more comprehensive evaluation metrics.\n",
      "\n",
      "Despite these advances, there are still significant challenges in LLM-based NLG evaluation. One challenge is the lack of standardized evaluation datasets and metrics (Chang et al., 2023). Another challenge is the need for more sophisticated methods to evaluate the rich correlation between various aspects of NLG, such as fluency, factual accuracy, and coherence (Bawden et al., 2023).\n",
      "\n",
      "To address these challenges, researchers are proposing new evaluation metrics that leverage LLMs in a more holistic way. For instance, Bawden et al. (2023) propose an NLG evaluation metric called LLM-Based NLG Evaluation. This metric uses an LLM to evaluate the quality of generated text based on various aspects, such as fluency, factual accuracy, and coherence. The proposed method also considers the rich correlation between these aspects, providing a more comprehensive evaluation of NLG systems.\n",
      "\n",
      "Another approach is to use multiple LLMs with different strengths to evaluate various aspects of NLG (Schick et al., 2021). For example, one LLM could be used to evaluate fluency, while another could be used to evaluate factual accuracy. This approach can provide more accurate and comprehensive evaluations than using a single LLM for all aspects.\n",
      "\n",
      "In conclusion, recent developments in LLMs have led to significant progress in NLG evaluation but also introduced new challenges. Current work focuses on independent evaluation of different aspects, ignoring the rich correlation between them. To address this research gap, researchers are proposing new evaluation metrics that leverage LLMs in a more holistic way, considering the rich correlation between various aspects of NLG. However, there is still a need for more standardized evaluation datasets and metrics to ensure fair and accurate comparisons between different NLG systems.\n",
      "\n",
      "References:\n",
      "Bawden, R., et al. (2023). LLM-Based NLG Evaluation: A New Approach to Evaluating the Quality of Generated Text. arXiv preprint arXiv:2303.12345.\n",
      "Chang, M.-W., et al. (2023). Evaluating Large Language Models for Natural Language Generation: Current Status and Challenges. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 31(1), 1-12.\n",
      "Gardner, M., et al. (2021). Evaluating the Effectiveness of LLMs in NLG: A Systematic Review. Journal of Natural Language Processing, 95(Special Issue), 1-24.\n",
      "Keskar, V., et al. (2022). Using Large Language Models for Automated Evaluation of Text Generation Systems. Proceedings of the Association for Computational Linguistics, 59(Miscellaneous Volumes), 3678-3689.\n",
      "Liu, T., et al. (2021). Pretraining Language Models for Natural Language Generation: A Survey. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30(11), 2453-2470.\n",
      "Raffel, B. S., et al. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Model. arXiv preprint arXiv:1905.10866.\n",
      "Schick, A., et al. (2021). Evaluating NLG Systems with Large Language Models: Challenges and Opportunities. Proceedings of the Association for Computational Linguistics, 59(Miscellaneous Volumes), 3700-3712.\n",
      "Zellers, J., et al. (2023). Evaluating Large Language Models for Natural Language Generation: A Survey. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 32(1), 1-14.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=5)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in evaluating NLG systems using LLMs. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation.\n",
      "\n",
      "Firstly, it is essential to understand that NLG evaluation is a long-standing task in Natural Language Processing (NLP), which has become more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are two main lines of work on LLM evaluation: NLU-style and NLG-style evaluations.\n",
      "\n",
      "NLU-style evaluation methods focus on measuring the ability of an LLM to understand and process natural language instructions accurately (Brown et al., 1995). These methods assess various aspects such as semantic understanding, syntactic correctness, and pragmatic meaning. However, NLU-style evaluations may not fully capture the nuances of open-ended NLG tasks, which require generating creative and contextually appropriate responses.\n",
      "\n",
      "On the other hand, NLG-style evaluation methods assess the quality of the generated text based on various dimensions such as coherence, fluency, relevance, and informativeness (Reiter and Radev, 2000). These methods often lead to significant differences in evaluation results due to the ambiguity and subjectivity involved in NLG tasks.\n",
      "\n",
      "Recent research works have focused on LLM-based evaluators given their promising instruction-following and generalization capability (Raffel et al., 2019; Brown et al., 2020). A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods and fine-tuning strategies. For instance, researchers have used LLMs to generate evaluation prompts that can be used to evaluate other NLG systems (Keskar et al., 2021).\n",
      "\n",
      "However, current work often employs the LLM to independently evaluate different aspects of NLG tasks, which largely ignores the rich correlation between various aspects. To fill this research gap, a recent study proposed an NLG evaluation metric called LLM-Based NLG Evaluation (LLME) (Xu et al., 2023). This metric aims to explore the synergy of LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for open-ended NLG tasks.\n",
      "\n",
      "The proposed LLME pipeline consists of three main components: LLM ideation, human scrutiny, and scoring criteria alignment. In the first component, LLMs are used to generate a diverse set of ideas or responses based on given prompts. In the second component, humans evaluate these ideas based on various dimensions such as relevance, coherence, fluency, and informativeness. In the third component, the scoring criteria are aligned between the LLM and human evaluations to ensure consistency and fairness.\n",
      "\n",
      "The study found that LLMs' criteria are generally comprehensive but tend to exaggerate unnecessary details, leading to inconsistent and misaligned evaluations (Xu et al., 2023). The researchers emphasized the significance of aligned scoring criteria as a consensus between LLMs and humans to ensure accurate and reliable NLG evaluation results.\n",
      "\n",
      "Furthermore, recent research has shown that LLMs are prone to have positional bias that could impact their decisions (Bolukbasi et al., 2016). Positional bias refers to the tendency of LLMs to favor certain positions or entities over others based on historical data. To address this challenge, researchers introduced a simple debiasing approach that leads to more fair and unbiased NLG evaluations.\n",
      "\n",
      "In conclusion, recent developments in LLM-based NLG evaluation have shown promising results but also presented new challenges. While LLMs can generate diverse ideas and evaluate NLG systems based on various dimensions, they are prone to positional bias and inconsistent evaluations due to the ambiguity and subjectivity involved in NLG tasks. To address these challenges, researchers proposed an LLME pipeline that explores the synergy of LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for open-ended NLG tasks. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies and ensuring accurate and reliable evaluation results.\n",
      "\n",
      "References:\n",
      "Brown, J., et al. (1995). First the sentence, then the story: Pragmatics in a computational framework. In Proceedings of the 37th Annual Meeting on Association for Computational Linguistics (pp. 87-96).\n",
      "\n",
      "Chang, M.-T., et al. (2023). Evaluating large language models: A survey. arXiv preprint arXiv:2301.04556.\n",
      "\n",
      "Celikyilmaz, O., et al. (2020). Evaluating large-scale neural machine translation models: A survey. ACM Transactions on Intelligent Systems and Technology, 11(1), 1-23.\n",
      "\n",
      "Keskar, S., et al. (2021). Evaluating NLG systems using LLMs: A case study on generating customer reviews. arXiv preprint arXiv:2106.05847.\n",
      "\n",
      "Reiter, R. H., & Radev, D. (2000). An evaluation methodology for natural language generation systems. Artificial Intelligence, 131(1-2), 191-226.\n",
      "\n",
      "Raffel, B. S., et al. (2019). Exploring the limits of transfer learning with a unified text-to-text model. arXiv preprint arXiv:1905.10836.\n",
      "\n",
      "Xu, J., et al. (2023). LLM-based NLG evaluation: A multi-dimensional pipeline for open-ended NLG tasks. arXiv preprint arXiv:2303.14789.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=8)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
