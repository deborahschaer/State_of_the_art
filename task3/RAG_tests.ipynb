{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tcqEUSNCn8I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "#from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "#from dotenv import load_dotenv\n",
    "import os\n",
    "import shutil\n",
    "from langchain_community.llms import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHROMA_PATH = \"chroma\"\n",
    "DATA_PATH = \"task2/sources\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'task2/sources/2307.07889v3.LLM_Comparative_Assessment__Zero_shot_NLG_Evaluation_through_Pairwise_Comparisons_using_Large_Language_Models.pdf'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 45 documents into 20321 chunks.\n",
      "language processing benchmarks. In response to this challenge, recent studies commonly introduce an evaluation approach, namely the Elo rating sys- tem (Elo, 1967), wherein either human or LLM judges are enlisted to adjudicate between two LLM- generated outputs (Askell et al., 2021; Bai et al.,\n",
      "{'source': 'task2/sources/2307.03025v3.Style_Over_Substance__Evaluation_Biases_for_Large_Language_Models.pdf', 'start_index': 2067}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#chunks[11].page_content\n",
    "some = chunks[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-09 16:15:25.799504: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-09 16:15:28.035032: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/deborah/FS24/masterarbeit/mistral/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    #if os.path.exists(CHROMA_PATH):\n",
    "     #   shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "\n",
    "    # Ensure the directory exists and has the correct permissions\n",
    "    os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "    os.chmod(CHROMA_PATH, 0o755)\n",
    "        \n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drwxr-xr-x 2 deborah deborah     4096 Jul  9 15:48 chroma\n"
     ]
    }
   ],
   "source": [
    "! ls -l | grep chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 20321 chunks to chroma.\n"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the recent development of LLM generated text evaluation?',\n",
       " 'result': ' Recent developments in LLM (Large Language Model) generated text evaluation include the use of LLMs to evaluate the quality of their own text outputs, which can provide insights into how a tuned version might perform in various scenarios (Yang et al., 2021; 2023a). This approach is being explored as a scalable and cost-effective alternative to human evaluations (Jain et al., 2023; Taori et al., 2023; Chiang et al., 2023). For instance, Fu et al. (2023) use the predicted text probability from an LLM as the automated score to assess text quality. Additionally, this approach has been shown to significantly enhance the ability of LLMs to evaluate text and speed up evaluation processes for Chinese LLMs, decreasing the average cost per sample by 4.6 times (Fu et al., 2023). However, it is important to note that while the use of LLMs as evaluators for text generation is in the exploratory phase, there are limitations and opportunities for future work, including the performance of LLMs as reliable evaluators and potential biases or errors in their predictions (i.e., challenge i) in the context provided.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag = RetrievalQA.from_chain_type(\n",
    "            llm=Ollama(model=\"mistral\"),\n",
    "            retriever=db.as_retriever()\n",
    "            #memory=ConversationSummaryMemory(llm = Ollama(model=\"mistral\")),\n",
    "            #chain_type_kwargs={\"prompt\": pt, \"verbose\": True},\n",
    "        )\n",
    "\n",
    "rag.invoke(\"What is the recent development of LLM generated text evaluation?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is  Multi-Elo Rating System (MERS)',\n",
       " 'result': ' The Multi-Elo Rating System (MERS) is a proposed approach for independently evaluating machine-generated text across multiple dimensions using the Elo rating system. This system aims to enhance the quality of evaluations, particularly in terms of factual accuracy, as shown by empirical results from studies. It contrasts with traditional methods that merge all evaluation aspects into a single score.'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag.invoke('What is  Multi-Elo Rating System (MERS)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
