{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=tcqEUSNCn8I\n",
    "https://github.com/pixegami/langchain-rag-tutorial/blob/main/query_data.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "import os\n",
    "import shutil\n",
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "\n",
    "from langchain.chains import RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = \"2402.01383v1\"\n",
    "CHROMA_PATH = paper_id+\"/chroma\"\n",
    "#CHROMA_PATH = paper_id+\"/chroma300\"\n",
    "DATA_PATH = paper_id\n",
    "file = 'rag_largechroma'\n",
    "#file = 'rag300'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def store_summary(survey1,filename):\n",
    "    with open(f'summaries/'+filename+'.txt', 'w') as file:\n",
    "        file.write(survey1)\n",
    "def write_json(documents_data,file_name):\n",
    "    converted_data = [dict(item) for item in documents_data['source_documents']]\n",
    "    dict_data = {'answer':documents_data['result'],'source':converted_data}\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Chroma database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents():\n",
    "    loader = DirectoryLoader(DATA_PATH, glob=\"*.pdf\")\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': '2402.01383v1/2307.07889v3.LLM_Comparative_Assessment__Zero_shot_NLG_Evaluation_through_Pairwise_Comparisons_using_Large_Language_Models.pdf'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[4].metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_text(documents):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,         #300,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        add_start_index=True,\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "    print(f\"Split {len(documents)} documents into {len(chunks)} chunks.\")\n",
    "\n",
    "    document = chunks[10]\n",
    "    print(document.page_content)\n",
    "    print(document.metadata)\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split 45 documents into 5364 chunks.\n",
      "In the course of our investigation (Table 1 and Section 3), we uncover intriguing findings related to the assessment of answer quality. Firstly, we observe hesitancy among humans, including both crowd-sourced and expert annotators, in determin- ing answer quality, while LLMs exhibit greater cer- tainty in their evaluations. Furthermore, we notice a substantial difference in fact-checking capabil- ities between human judges and LLMs. Human judges generally do not thoroughly fact-check an- swers unless the factual error is glaringly evident, whereas LLMs demonstrate some degree of fact- checking ability, albeit with imperfections. An- other significant finding is that both human judges\n",
      "\n",
      "2In this work, we refer to different settings in answering questions as different models, although all the answers are generated by GPT-4.\n",
      "{'source': '2402.01383v1/2307.03025v3.Style_Over_Substance__Evaluation_Biases_for_Large_Language_Models.pdf', 'start_index': 6749}\n"
     ]
    }
   ],
   "source": [
    "chunks = split_text(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Our proposed approach requires human judges and LLMs to evaluate the machine-generated text independently from three aspects: “Accuracy”, “Helpfulness”, and “Language”. This allows us to achieve a more comprehensive and transparent understanding of the quality of machine-generated text. Our empirical findings demonstrate a sig- nificant improvement in the evaluation quality of GPT-4, particularly in terms of factual accuracy. However, we observe that humans still exhibit in- decisiveness in their assessments.\\n\\nBased on our findings, we highly recommend that practitioners evaluate machine-generated text from various perspectives rather than depending solely on a single unified measure. Additionally, we advise practitioners to exercise caution when using crowd-sourced human annotators to assess the performance of LLMs.\\n\\nModel type\\n\\nOutput\\n\\nCorrect'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "some = chunks[0:10]\n",
    "#print(some)\n",
    "print(len(chunks[11].page_content))\n",
    "chunks[11].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deborah/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_chroma(chunks: list[Document]):\n",
    "    # Clear out the database first.\n",
    "    #if os.path.exists(CHROMA_PATH):\n",
    "     #   shutil.rmtree(CHROMA_PATH)\n",
    "\n",
    "\n",
    "    # Ensure the directory exists and has the correct permissions\n",
    "    #os.makedirs(CHROMA_PATH, exist_ok=True)\n",
    "    #os.chmod(CHROMA_PATH, 0o755)\n",
    "        \n",
    "    # Create a new DB from the documents.\n",
    "    db = Chroma.from_documents(\n",
    "        chunks, embedding_function, persist_directory=CHROMA_PATH\n",
    "    )\n",
    "    db.persist()\n",
    "    print(f\"Saved {len(chunks)} chunks to {CHROMA_PATH}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "attempt to write a readonly database",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msave_to_chroma\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 12\u001b[0m, in \u001b[0;36msave_to_chroma\u001b[0;34m(chunks)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_chroma\u001b[39m(chunks: \u001b[38;5;28mlist\u001b[39m[Document]):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Clear out the database first.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;66;03m#if os.path.exists(CHROMA_PATH):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# Create a new DB from the documents.\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m     db \u001b[38;5;241m=\u001b[39m \u001b[43mChroma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCHROMA_PATH\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     db\u001b[38;5;241m.\u001b[39mpersist()\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(chunks)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m chunks to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mCHROMA_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:790\u001b[0m, in \u001b[0;36mChroma.from_documents\u001b[0;34m(cls, documents, embedding, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m texts \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mpage_content \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[1;32m    789\u001b[0m metadatas \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m documents]\n\u001b[0;32m--> 790\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_texts\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpersist_directory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpersist_directory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient_settings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollection_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:754\u001b[0m, in \u001b[0;36mChroma.from_texts\u001b[0;34m(cls, texts, embedding, metadatas, ids, collection_name, persist_directory, client_settings, client, collection_metadata, **kwargs)\u001b[0m\n\u001b[1;32m    748\u001b[0m         chroma_collection\u001b[38;5;241m.\u001b[39madd_texts(\n\u001b[1;32m    749\u001b[0m             texts\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m [],\n\u001b[1;32m    750\u001b[0m             metadatas\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m batch[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    751\u001b[0m             ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    752\u001b[0m         )\n\u001b[1;32m    753\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 754\u001b[0m     \u001b[43mchroma_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_texts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m chroma_collection\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/langchain_community/vectorstores/chroma.py:298\u001b[0m, in \u001b[0;36mChroma.add_texts\u001b[0;34m(self, texts, metadatas, ids, **kwargs)\u001b[0m\n\u001b[1;32m    296\u001b[0m ids_with_metadata \u001b[38;5;241m=\u001b[39m [ids[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m non_empty_ids]\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 298\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_collection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    299\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[43m        \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtexts_with_metadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m        \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_with_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    305\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected metadata value to be\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e):\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/chromadb/api/models/Collection.py:300\u001b[0m, in \u001b[0;36mCollection.upsert\u001b[0;34m(self, ids, embeddings, metadatas, documents, images, uris)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update the embeddings, metadatas or documents for provided ids, or create them if they don't exist.\u001b[39;00m\n\u001b[1;32m    280\u001b[0m \n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    290\u001b[0m (\n\u001b[1;32m    291\u001b[0m     ids,\n\u001b[1;32m    292\u001b[0m     embeddings,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    297\u001b[0m     ids, embeddings, metadatas, documents, images, uris\n\u001b[1;32m    298\u001b[0m )\n\u001b[0;32m--> 300\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_upsert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43membeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadatas\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadatas\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43muris\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muris\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/chromadb/api/segment.py:444\u001b[0m, in \u001b[0;36mSegmentAPI._upsert\u001b[0;34m(self, collection_id, ids, embeddings, metadatas, documents, uris)\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_embedding_record(coll, r)\n\u001b[1;32m    443\u001b[0m     records_to_submit\u001b[38;5;241m.\u001b[39mappend(r)\n\u001b[0;32m--> 444\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_producer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcollection_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrecords_to_submit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/chromadb/telemetry/opentelemetry/__init__.py:146\u001b[0m, in \u001b[0;36mtrace_method.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m tracer, granularity\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trace_granularity \u001b[38;5;241m<\u001b[39m granularity:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracer:\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/FS24/masterarbeit/State_of_the_art/.venv/lib/python3.10/site-packages/chromadb/db/mixins/embeddings_queue.py:180\u001b[0m, in \u001b[0;36mSqlEmbeddingsQueue.submit_embeddings\u001b[0;34m(self, collection_id, embeddings)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# The returning clause does not guarantee order, so we need to do reorder\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# the results. https://www.sqlite.org/lang_returning.html\u001b[39;00m\n\u001b[1;32m    179\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m RETURNING seq_id, id\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Pypika doesn't support RETURNING\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mfetchall()\n\u001b[1;32m    181\u001b[0m \u001b[38;5;66;03m# Reorder the results\u001b[39;00m\n\u001b[1;32m    182\u001b[0m seq_ids \u001b[38;5;241m=\u001b[39m [cast(SeqId, \u001b[38;5;28;01mNone\u001b[39;00m)] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    183\u001b[0m     results\n\u001b[1;32m    184\u001b[0m )  \u001b[38;5;66;03m# Lie to mypy: https://stackoverflow.com/questions/76694215/python-type-casting-when-preallocating-list\u001b[39;00m\n",
      "\u001b[0;31mOperationalError\u001b[0m: attempt to write a readonly database"
     ]
    }
   ],
   "source": [
    "save_to_chroma(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=embedding_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model = \"mistral\",temperature=0)\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Use three sentence maximum and keep the answer concise. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "\n",
    "rag = RetrievalQA.from_chain_type(\n",
    "            llm=llm,\n",
    "            retriever=db.as_retriever(),\n",
    "            return_source_documents = True,\n",
    "            \n",
    "            #memory=ConversationSummaryMemory(llm = Ollama(model=\"mistral\")),\n",
    "            #chain_type_kwargs={\"prompt\": pt, \"verbose\": True},\n",
    "        )\n",
    "\n",
    "answer = rag.invoke(\"What is the recent development of LLM generated text evaluation?\")\n",
    "rag_overview = answer['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in LLM (Large Language Model) generated text evaluation include using LLMs to evaluate the quality of their own text outputs as an alternative to human evaluations. This approach can provide a more comprehensive understanding of LLM performance and predict how a tuned version might behave in various scenarios. Additionally, applying LLMs as evaluators for text generation has been shown to be scalable, cost-effective, and can significantly enhance the ability to assess the quality of generated text. For instance, Fu et al. (2023) use LLM's predicted text probability as the automated score. Furthermore, this approach has been applied to evaluate Chinese LLMs, resulting in a faster evaluation process and decreased average annotation cost per sample. However, it is important to note that the utilization of LLMs as evaluators for text generation is still in the exploratory phase, and there are limitations that provide opportunities for future work, such as the performance of LLMs as reliable evaluators and potential biases in their predictions.\n"
     ]
    }
   ],
   "source": [
    "print(rag_overview)\n",
    "store_summary(rag_overview,'rag_overview300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the recent development of LLM generated text evaluation?',\n",
       " 'result': \" Recent developments in LLM (Large Language Model) generated text evaluation include using LLMs to evaluate the quality of their own text outputs as an alternative to human evaluations. This approach can provide a more comprehensive understanding of LLM performance and predict how a tuned version might behave in various scenarios. Additionally, applying LLMs as evaluators for text generation has been shown to be scalable, cost-effective, and can significantly enhance the ability to assess the quality of generated text. For instance, Fu et al. (2023) use LLM's predicted text probability as the automated score. Furthermore, this approach has been applied to evaluate Chinese LLMs, resulting in a faster evaluation process and decreased average annotation cost per sample. However, it is important to note that the utilization of LLMs as evaluators for text generation is still in the exploratory phase, and there are limitations that provide opportunities for future work, such as the performance of LLMs as reliable evaluators and potential biases in their predictions.\",\n",
       " 'source_documents': [Document(metadata={'source': '2402.01383v1/2306.05087v2.PandaLM__An_Automatic_Evaluation_Benchmark_for_LLM_Instruction_Tuning_Optimization.pdf', 'start_index': 38685}, page_content='2020); Yang et al. (2021; 2023a)) into an evaluation framework could offer a more comprehensive understanding of LLM performance. For instance, analyzing and evaluating the extended text outputs of an untuned LLM can help predict how a tuned version might behave in various scenarios. This approach'),\n",
       "  Document(metadata={'source': '2402.01383v1/2310.19740v1.Collaborative_Evaluation__Exploring_the_Synergy_of_Large_Language_Models_and_Humans_for_Open_ended_Generation_Evaluation.pdf', 'start_index': 8145}, page_content='As research in LLMs continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evalua- tions (Jain et al., 2023; Taori et al., 2023; Chiang et al., 2023). Fu et al. (2023) use LLM’s predicted text probability as the automated score, assuming'),\n",
       "  Document(metadata={'source': '2402.01383v1/2308.01862v1.Wider_and_Deeper_LLM_Networks_are_Fairer_LLM_Evaluators.pdf', 'start_index': 39881}, page_content='the best results, significantly enhancing the ability of LLMs to evaluate the quality of generated text. Furthermore, we apply our evaluator to assess the performance of Chinese LLMs, where it proves to speed up LLM evaluation process by 4.6 times and decrease the average annotation cost per sample'),\n",
       "  Document(metadata={'source': '2402.01383v1/2305.14658v3.Evaluate_What_You_Can_t_Evaluate__Unassessable_Quality_for_Generated_Response.pdf', 'start_index': 40449}, page_content='While we analyze the challenges and possibilities and of LLMs as text generation evaluators by con- structed benchmarks, the utilization of LLMs as evaluators for text generation is in the exploratory phase. There are limitations that provide avenues for future work: i) the performance of LLMs as')]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = answer['source_documents'][0]\n",
    "converted_data = [dict(item) for item in answer['source_documents']]\n",
    "#dict(answer['source_documents'])\n",
    "converted_data\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_json(answer,'rag_overview300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mtype\u001b[39m(answer)\n\u001b[0;32m----> 2\u001b[0m sources \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m doc, _score \u001b[38;5;129;01min\u001b[39;00m answer]\n\u001b[1;32m      3\u001b[0m sources\n",
      "Cell \u001b[0;32mIn[48], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mtype\u001b[39m(answer)\n\u001b[0;32m----> 2\u001b[0m sources \u001b[38;5;241m=\u001b[39m [doc\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m doc, _score \u001b[38;5;129;01min\u001b[39;00m answer]\n\u001b[1;32m      3\u001b[0m sources\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "type(answer)\n",
    "sources = [doc.metadata.get(\"source\", None) for doc, _score in answer]\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change more parameters example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "VectorStore.as_retriever() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchains\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombine_documents\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m create_stuff_documents_chain\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprompts\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ChatPromptTemplate\n\u001b[0;32m----> 6\u001b[0m retriever\u001b[38;5;241m=\u001b[39m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_retriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m llm \u001b[38;5;241m=\u001b[39m Ollama(model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral\u001b[39m\u001b[38;5;124m\"\u001b[39m,temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      9\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse the given context to answer the question. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know the answer, say you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContext: \u001b[39m\u001b[38;5;132;01m{context}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m )\n",
      "\u001b[0;31mTypeError\u001b[0m: VectorStore.as_retriever() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "retriever=db.as_retriever(20)\n",
    "llm = Ollama(model = \"mistral\",temperature=0)\n",
    "\n",
    "system_prompt = (\n",
    "    \"Use the given context to answer the question. \"\n",
    "    \"If you don't know the answer, say you don't know. \"\n",
    "    \"Context: {context}\"\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "chain = create_retrieval_chain(retriever, question_answer_chain)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'What is the recent development of LLM generated text evaluation?'\n",
    "chain.invoke({\"input\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'VectorStoreRetriever' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhi\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'VectorStoreRetriever' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other parameters\n",
    "https://medium.com/@callumjmac/implementing-rag-in-langchain-with-chroma-a-step-by-step-guide-16fc21815339"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "Write your answer in about 2000 words.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text,k=3):\n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "  llm = Ollama(model = \"mistral\",temperature=0)\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = llm.invoke(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources} \\nSourceText:{context_text}\"\n",
    "  return formatted_response, response_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(Document(metadata={'source': '2402.01383v1/2311.18702v2.CritiqueLLM__Towards_an_Informative_Critique_Generation_Model_for_Evaluation_of_Large_Language_Model_Generation.pdf', 'start_index': 6910}, page_content='2 Related Work\\n\\nEvaluation is a long-standing task in NLP, which becomes more challenging with the rapid develop- ment of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are mainly two lines of work on LLM evaluation, including NLU-style and NLG-style evaluations. NLU-style evaluation methods utilize natural language understanding (NLU) tasks such as multi-choice QA to measure the performance of LLMs via simple objective met- rics (such as accuracy and F1 score) (Hendrycks et al., 2021; Zhong et al., 2023; Huang et al., 2023b), which may deviate from the common us- age of LLMs and may not exactly reflect the ability of LLMs in generating responses for user queries.'),\n",
       "  0.7049670870956175),\n",
       " (Document(metadata={'source': '2402.01383v1/2311.18702v2.CritiqueLLM__Towards_an_Informative_Critique_Generation_Model_for_Evaluation_of_Large_Language_Model_Generation.pdf', 'start_index': 6910}, page_content='2 Related Work\\n\\nEvaluation is a long-standing task in NLP, which becomes more challenging with the rapid develop- ment of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are mainly two lines of work on LLM evaluation, including NLU-style and NLG-style evaluations. NLU-style evaluation methods utilize natural language understanding (NLU) tasks such as multi-choice QA to measure the performance of LLMs via simple objective met- rics (such as accuracy and F1 score) (Hendrycks et al., 2021; Zhong et al., 2023; Huang et al., 2023b), which may deviate from the common us- age of LLMs and may not exactly reflect the ability of LLMs in generating responses for user queries.'),\n",
       "  0.7049670870956175)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic = \"LLM-based NLG Evaluation: Current Status and Challenges\"\n",
    "db.similarity_search_with_relevance_scores(f'what are recent developments in {topic}?', k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in NLG evaluation. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation, based on the context provided in the text.\n",
      "\n",
      "Firstly, it is essential to understand that NLG evaluation is a long-standing task in Natural Language Processing (NLP), which has become more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are two main lines of work on LLM evaluation: NLU-style and NLG-style evaluations. NLU-style evaluation methods focus on the model's ability to understand language, while NLG-style evaluations assess the model's ability to generate human-like text (Bawden et al., 2019).\n",
      "\n",
      "Recent research works have focused on LLM-based evaluators due to their promising instruction-following and generalization capability. A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods (Raffel et al., 2019; Schick et al., 2023). These methods involve providing the model with a set of prompts and evaluating its performance based on the generated text.\n",
      "\n",
      "However, current work often employs LLMs to independently evaluate different aspects of NLG tasks, largely ignoring the rich correlation between various aspects (Bawden et al., 2019). To fill this research gap, a new NLG evaluation metric called LLM-Based NLG Evaluation has been proposed. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies.\n",
      "\n",
      "The text also highlights that prompts often lead to significant differences in evaluation results (4, 12, 15, 36). Therefore, there is ongoing research into leveraging LLMs for the generation of NLG evaluation datasets, aiming to reduce the need for manual evaluations to some extent (5, 25).\n",
      "\n",
      "Moreover, the text emphasizes that LLMs are prone to have positional bias that could impact their decisions. However, a simple debiasing approach has been introduced to address this issue (Fu et al., 2023; Chiang and yi Lee, 2023; Gao et al., 2023).\n",
      "\n",
      "Recent work has also explored using LLMs for automatic NLG evaluation. For instance, GPTScore (Fu et al., 2023) leverages the LLM-predicted probability of text sequences as the quality score. Comparative assessment is superior to prompt scoring for moderate-sized open-source LLMs such as FlanT5 and Llama2-chat (Bawden et al., 2019). In many cases, this approach can achieve performance competitive with state-of-the-art methods.\n",
      "\n",
      "However, there are still challenges in LLM-based NLG evaluation. For instance, the text highlights that existing LLM-based evaluators suffer from insufficient prompting, where scoring guidelines are absent, and only output spaces are provided, resulting in inconsistent and misaligned evaluations (Bawden et al., 2019). Therefore, there is a need for aligned scoring criteria as a consensus between humans and LLMs.\n",
      "\n",
      "Another challenge is the lack of standardized evaluation metrics for NLG tasks. While LLMs have shown promising results in various NLG tasks, it is essential to establish standardized evaluation metrics to ensure fairness and comparability across different models and datasets (Bawden et al., 2019).\n",
      "\n",
      "Furthermore, there is a need for more research on the ethical implications of LLM-based NLG evaluation. For instance, how can we ensure that LLMs do not generate biased or harmful text? How can we ensure that LLMs respect user privacy and confidentiality? These are important questions that need to be addressed as we continue to explore the potential of LLMs in NLG evaluation.\n",
      "\n",
      "In conclusion, recent developments in LLM-based NLG evaluation have shown promising results, but there are still challenges that need to be addressed. The use of LLMs for automatic NLG evaluation has become increasingly popular due to their instruction-following and generalization capability. However, there is a need for more research on standardized evaluation metrics, ethical implications, and the rich correlation between various aspects of NLG tasks. By addressing these challenges, we can ensure that LLMs are used effectively and ethically in NLG evaluation, leading to more accurate and reliable evaluations of NLG systems.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=10)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "641"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = '''Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in NLG evaluation. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation, based on the context provided in the text.\n",
    "\n",
    "Firstly, it is essential to understand that NLG evaluation is a long-standing task in Natural Language Processing (NLP), which has become more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are two main lines of work on LLM evaluation: NLU-style and NLG-style evaluations. NLU-style evaluation methods focus on the model's ability to understand language, while NLG-style evaluations assess the model's ability to generate human-like text (Bawden et al., 2019).\n",
    "\n",
    "Recent research works have focused on LLM-based evaluators due to their promising instruction-following and generalization capability. A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods (Raffel et al., 2019; Schick et al., 2023). These methods involve providing the model with a set of prompts and evaluating its performance based on the generated text.\n",
    "\n",
    "However, current work often employs LLMs to independently evaluate different aspects of NLG tasks, largely ignoring the rich correlation between various aspects (Bawden et al., 2019). To fill this research gap, a new NLG evaluation metric called LLM-Based NLG Evaluation has been proposed. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies.\n",
    "\n",
    "The text also highlights that prompts often lead to significant differences in evaluation results (4, 12, 15, 36). Therefore, there is ongoing research into leveraging LLMs for the generation of NLG evaluation datasets, aiming to reduce the need for manual evaluations to some extent (5, 25).\n",
    "\n",
    "Moreover, the text emphasizes that LLMs are prone to have positional bias that could impact their decisions. However, a simple debiasing approach has been introduced to address this issue (Fu et al., 2023; Chiang and yi Lee, 2023; Gao et al., 2023).\n",
    "\n",
    "Recent work has also explored using LLMs for automatic NLG evaluation. For instance, GPTScore (Fu et al., 2023) leverages the LLM-predicted probability of text sequences as the quality score. Comparative assessment is superior to prompt scoring for moderate-sized open-source LLMs such as FlanT5 and Llama2-chat (Bawden et al., 2019). In many cases, this approach can achieve performance competitive with state-of-the-art methods.\n",
    "\n",
    "However, there are still challenges in LLM-based NLG evaluation. For instance, the text highlights that existing LLM-based evaluators suffer from insufficient prompting, where scoring guidelines are absent, and only output spaces are provided, resulting in inconsistent and misaligned evaluations (Bawden et al., 2019). Therefore, there is a need for aligned scoring criteria as a consensus between humans and LLMs.\n",
    "\n",
    "Another challenge is the lack of standardized evaluation metrics for NLG tasks. While LLMs have shown promising results in various NLG tasks, it is essential to establish standardized evaluation metrics to ensure fairness and comparability across different models and datasets (Bawden et al., 2019).\n",
    "\n",
    "Furthermore, there is a need for more research on the ethical implications of LLM-based NLG evaluation. For instance, how can we ensure that LLMs do not generate biased or harmful text? How can we ensure that LLMs respect user privacy and confidentiality? These are important questions that need to be addressed as we continue to explore the potential of LLMs in NLG evaluation.\n",
    "\n",
    "In conclusion, recent developments in LLM-based NLG evaluation have shown promising results, but there are still challenges that need to be addressed. The use of LLMs for automatic NLG evaluation has become increasingly popular due to their instruction-following and generalization capability. However, there is a need for more research on standardized evaluation metrics, ethical implications, and the rich correlation between various aspects of NLG tasks. By addressing these challenges, we can ensure that LLMs are used effectively and ethically in NLG evaluation, leading to more accurate and reliable evaluations of NLG systems.'''\n",
    "len(s.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_summary(formatted_response,'rag10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in evaluating NLG systems using LLMs. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation.\n",
      "\n",
      "Firstly, it is essential to understand that LLMs have shown remarkable performance on various NLG evaluation tasks (Liu et al., 2021; Raffel et al., 2019). However, most existing work focuses on employing LLMs independently to evaluate different aspects of NLG, such as fluency, factual accuracy, and coherence. This approach ignores the rich correlation between various aspects, which is a significant research gap (Bawden et al., 2023).\n",
      "\n",
      "One line of recent work on LLM-based NLG evaluation focuses on preliminary explorations of LLM-based evaluators using prompting methods (Schick et al., 2021; Keskar et al., 2022). These studies aim to leverage the instruction-following and generalization capability of LLMs to generate evaluation datasets, reducing the need for manual evaluations to some extent. However, these approaches often lead to significant differences in evaluation results (Gardner et al., 2021; Zellers et al., 2023).\n",
      "\n",
      "Another line of work on LLM-based NLG evaluation is the development of NLU-style and NLG-style evaluations. NLU-style evaluation methods assess the ability of an NLG system to understand and process input, while NLG-style evaluations focus on the output generated by the NLG system (Bawden et al., 2023). Both styles have their advantages and limitations, and researchers are exploring ways to combine them to create more comprehensive evaluation metrics.\n",
      "\n",
      "Despite these advances, there are still significant challenges in LLM-based NLG evaluation. One challenge is the lack of standardized evaluation datasets and metrics (Chang et al., 2023). Another challenge is the need for more sophisticated methods to evaluate the rich correlation between various aspects of NLG, such as fluency, factual accuracy, and coherence (Bawden et al., 2023).\n",
      "\n",
      "To address these challenges, researchers are proposing new evaluation metrics that leverage LLMs in a more holistic way. For instance, Bawden et al. (2023) propose an NLG evaluation metric called LLM-Based NLG Evaluation. This metric uses an LLM to evaluate the quality of generated text based on various aspects, such as fluency, factual accuracy, and coherence. The proposed method also considers the rich correlation between these aspects, providing a more comprehensive evaluation of NLG systems.\n",
      "\n",
      "Another approach is to use multiple LLMs with different strengths to evaluate various aspects of NLG (Schick et al., 2021). For example, one LLM could be used to evaluate fluency, while another could be used to evaluate factual accuracy. This approach can provide more accurate and comprehensive evaluations than using a single LLM for all aspects.\n",
      "\n",
      "In conclusion, recent developments in LLMs have led to significant progress in NLG evaluation but also introduced new challenges. Current work focuses on independent evaluation of different aspects, ignoring the rich correlation between them. To address this research gap, researchers are proposing new evaluation metrics that leverage LLMs in a more holistic way, considering the rich correlation between various aspects of NLG. However, there is still a need for more standardized evaluation datasets and metrics to ensure fair and accurate comparisons between different NLG systems.\n",
      "\n",
      "References:\n",
      "Bawden, R., et al. (2023). LLM-Based NLG Evaluation: A New Approach to Evaluating the Quality of Generated Text. arXiv preprint arXiv:2303.12345.\n",
      "Chang, M.-W., et al. (2023). Evaluating Large Language Models for Natural Language Generation: Current Status and Challenges. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 31(1), 1-12.\n",
      "Gardner, M., et al. (2021). Evaluating the Effectiveness of LLMs in NLG: A Systematic Review. Journal of Natural Language Processing, 95(Special Issue), 1-24.\n",
      "Keskar, V., et al. (2022). Using Large Language Models for Automated Evaluation of Text Generation Systems. Proceedings of the Association for Computational Linguistics, 59(Miscellaneous Volumes), 3678-3689.\n",
      "Liu, T., et al. (2021). Pretraining Language Models for Natural Language Generation: A Survey. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 30(11), 2453-2470.\n",
      "Raffel, B. S., et al. (2019). Exploring the Limits of Transfer Learning with a Unified Text-to-Text Model. arXiv preprint arXiv:1905.10866.\n",
      "Schick, A., et al. (2021). Evaluating NLG Systems with Large Language Models: Challenges and Opportunities. Proceedings of the Association for Computational Linguistics, 59(Miscellaneous Volumes), 3700-3712.\n",
      "Zellers, J., et al. (2023). Evaluating Large Language Models for Natural Language Generation: A Survey. IEEE Transactions on Neural Systems and Rehabilitation Engineering, 32(1), 1-14.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=5)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in Large Language Models (LLMs) have significantly impacted the field of Natural Language Generation (NLG), leading to new research directions and challenges in evaluating NLG systems using LLMs. In this response, we will discuss the current status and challenges of LLM-based NLG evaluation.\n",
      "\n",
      "Firstly, it is essential to understand that NLG evaluation is a long-standing task in Natural Language Processing (NLP), which has become more challenging with the rapid development of LLMs (Celikyilmaz et al., 2020; Chang et al., 2023). Currently, there are two main lines of work on LLM evaluation: NLU-style and NLG-style evaluations.\n",
      "\n",
      "NLU-style evaluation methods focus on measuring the ability of an LLM to understand and process natural language instructions accurately (Brown et al., 1995). These methods assess various aspects such as semantic understanding, syntactic correctness, and pragmatic meaning. However, NLU-style evaluations may not fully capture the nuances of open-ended NLG tasks, which require generating creative and contextually appropriate responses.\n",
      "\n",
      "On the other hand, NLG-style evaluation methods assess the quality of the generated text based on various dimensions such as coherence, fluency, relevance, and informativeness (Reiter and Radev, 2000). These methods often lead to significant differences in evaluation results due to the ambiguity and subjectivity involved in NLG tasks.\n",
      "\n",
      "Recent research works have focused on LLM-based evaluators given their promising instruction-following and generalization capability (Raffel et al., 2019; Brown et al., 2020). A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods and fine-tuning strategies. For instance, researchers have used LLMs to generate evaluation prompts that can be used to evaluate other NLG systems (Keskar et al., 2021).\n",
      "\n",
      "However, current work often employs the LLM to independently evaluate different aspects of NLG tasks, which largely ignores the rich correlation between various aspects. To fill this research gap, a recent study proposed an NLG evaluation metric called LLM-Based NLG Evaluation (LLME) (Xu et al., 2023). This metric aims to explore the synergy of LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for open-ended NLG tasks.\n",
      "\n",
      "The proposed LLME pipeline consists of three main components: LLM ideation, human scrutiny, and scoring criteria alignment. In the first component, LLMs are used to generate a diverse set of ideas or responses based on given prompts. In the second component, humans evaluate these ideas based on various dimensions such as relevance, coherence, fluency, and informativeness. In the third component, the scoring criteria are aligned between the LLM and human evaluations to ensure consistency and fairness.\n",
      "\n",
      "The study found that LLMs' criteria are generally comprehensive but tend to exaggerate unnecessary details, leading to inconsistent and misaligned evaluations (Xu et al., 2023). The researchers emphasized the significance of aligned scoring criteria as a consensus between LLMs and humans to ensure accurate and reliable NLG evaluation results.\n",
      "\n",
      "Furthermore, recent research has shown that LLMs are prone to have positional bias that could impact their decisions (Bolukbasi et al., 2016). Positional bias refers to the tendency of LLMs to favor certain positions or entities over others based on historical data. To address this challenge, researchers introduced a simple debiasing approach that leads to more fair and unbiased NLG evaluations.\n",
      "\n",
      "In conclusion, recent developments in LLM-based NLG evaluation have shown promising results but also presented new challenges. While LLMs can generate diverse ideas and evaluate NLG systems based on various dimensions, they are prone to positional bias and inconsistent evaluations due to the ambiguity and subjectivity involved in NLG tasks. To address these challenges, researchers proposed an LLME pipeline that explores the synergy of LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for open-ended NLG tasks. This approach showcases the versatility and potential of LLMs in improving NLG evaluation methodologies and ensuring accurate and reliable evaluation results.\n",
      "\n",
      "References:\n",
      "Brown, J., et al. (1995). First the sentence, then the story: Pragmatics in a computational framework. In Proceedings of the 37th Annual Meeting on Association for Computational Linguistics (pp. 87-96).\n",
      "\n",
      "Chang, M.-T., et al. (2023). Evaluating large language models: A survey. arXiv preprint arXiv:2301.04556.\n",
      "\n",
      "Celikyilmaz, O., et al. (2020). Evaluating large-scale neural machine translation models: A survey. ACM Transactions on Intelligent Systems and Technology, 11(1), 1-23.\n",
      "\n",
      "Keskar, S., et al. (2021). Evaluating NLG systems using LLMs: A case study on generating customer reviews. arXiv preprint arXiv:2106.05847.\n",
      "\n",
      "Reiter, R. H., & Radev, D. (2000). An evaluation methodology for natural language generation systems. Artificial Intelligence, 131(1-2), 191-226.\n",
      "\n",
      "Raffel, B. S., et al. (2019). Exploring the limits of transfer learning with a unified text-to-text model. arXiv preprint arXiv:1905.10836.\n",
      "\n",
      "Xu, J., et al. (2023). LLM-based NLG evaluation: A multi-dimensional pipeline for open-ended NLG tasks. arXiv preprint arXiv:2303.14789.\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=8)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "751"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_text.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len('Hi what is the concept?'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    " - -\n",
    "Answer the question based on the above context: {question}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_rag(query_text,k=3):\n",
    "  # Retrieving the context from the DB using similarity search\n",
    "  results = db.similarity_search_with_relevance_scores(query_text, k=k)\n",
    "\n",
    "  # Check if there are any matching results or if the relevance score is too low\n",
    "  if len(results) == 0 or results[0][1] < 0.7:\n",
    "    print(f\"Unable to find matching results.\")\n",
    "\n",
    "  # Combine context from matching documents\n",
    "  context_text = \"\\n\\n - -\\n\\n\".join([doc.page_content for doc, _score in results])\n",
    " \n",
    "  # Create prompt template using context and query text\n",
    "  prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "  prompt = prompt_template.format(context=context_text, question=query_text)\n",
    "\n",
    "  llm = Ollama(model = \"mistral\",temperature=0)\n",
    "  # Generate response text based on the prompt\n",
    "  response_text = llm.invoke(prompt)\n",
    " \n",
    "   # Get sources of the matching documents\n",
    "  sources = [doc.metadata.get(\"source\", None) for doc, _score in results]\n",
    " \n",
    "  # Format and return response including generated text and sources\n",
    "  formatted_response = f\"Response: {response_text}\\nSources: {sources} \\nSourceText:{context_text}\"\n",
    "  return formatted_response, response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Recent developments in LLM-based NLG (Natural Language Generation) evaluation involve the use of Large Language Models (LLMs) as evaluators for NLG systems due to their promising instruction-following and generalization capabilities. Two main lines of research are currently being explored: NLU-style and NLG-style evaluations.\n",
      "\n",
      "NLU-style evaluation methods focus on assessing the understanding and processing abilities of LLMs, while NLG-style evaluations evaluate the generation quality of LLMs. However, current work often overlooks the rich correlation between various aspects of NLG evaluation, leading to significant differences in evaluation results.\n",
      "\n",
      "To address this gap, researchers propose an NLG evaluation metric called LLM-Based NLG Evaluation. This approach aims to showcase the versatility and potential of LLMs in improving NLG evaluation methodologies by exploring the synergy between LLMs and humans in establishing evaluation criteria and conducting multi-dimensional evaluations for open-ended NLG tasks.\n",
      "\n",
      "However, existing LLM-based NLG evaluators have been found to suffer from insufficient prompting, where scoring guidelines are absent, leading to inconsistent and misaligned evaluations. Therefore, researchers emphasize the significance of aligned scoring criteria as a consensus between human evaluators and LLMs.\n",
      "\n",
      "Additionally, LLMs have been observed to exhibit positional bias that could impact their decisions. Researchers introduce simple debiasing approaches to mitigate this issue and improve the overall performance of LLM-based NLG evaluation systems.\n",
      "207\n"
     ]
    }
   ],
   "source": [
    "# Let's call our function we have defined\n",
    "formatted_response, response_text = query_rag(f'what are recent developments in {topic}?',k=8)\n",
    "# and finally, inspect our final response!\n",
    "print(response_text)\n",
    "store_summary(formatted_response,'rag8short')\n",
    "print(len(response_text.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "296\n",
      "294\n"
     ]
    }
   ],
   "source": [
    "s = \"\"\"superior performance on various NLG evaluation tasks. However, current work often employs the LLM to independently evaluate different aspects, which largely ignores the rich correlation be- tween various aspects. To fill this research gap, in this work, we propose an NLG evaluation metric called\"\"\"\n",
    "k = \"\"\"LLM-Based NLG Evaluation With the emergence of LLM, recent research works focus on LLM-based evaluators given their promising instruction-following and generalization capability. A first line of work goes through preliminary explorations on LLM-based evaluators, including prompting methods and\"\"\"\n",
    "print(len(s.split(' ')))\n",
    "print(len(s))\n",
    "print(len(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
