{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine several papers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_json(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + '.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "def write_json(dict_data,file_name):\n",
    "    with open('summaries/'+file_name+'.json', 'w') as file:\n",
    "        json.dump(dict_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM-based NLG Evaluation: Current Status and Challenges\n"
     ]
    }
   ],
   "source": [
    "# The original state of the art paper\n",
    "paper_id = \"2402.01383v1\"\n",
    "original = get_json('dataset/'+paper_id+'data')\n",
    "topic = original['title']\n",
    "print(topic)\n",
    "# original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the llm mistral\n",
    "llm = Ollama(model = \"mistral\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(file_name):\n",
    "    # Step 1: Read the JSON file\n",
    "    with open(file_name + 'full_texts.json', 'r') as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data\n",
    "data = get_data('dataset/'+paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the summaries\n",
    "os.makedirs('summaries/'+paper_id,exist_ok=True)\n",
    "def write(content,filename):\n",
    "    path = 'summaries/'+paper_id\n",
    "    with open(path+'/'+filename, 'w') as file:\n",
    "        file.write(content)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_simple(texts,topic):\n",
    "    context = ('/n/n').join(texts.values())\n",
    "    instruction = f'Write a state of the art survey of the topic of {topic}. Using the following context: {context}'\n",
    "    answer = llm.invoke(instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 0\n",
      "Assistant 2's answer is not only irrelevant to the question but also contains incorrect information. The assistant provided an equation that does not relate to the given question or reference answer. Therefore, it receives a very low score of 0. Assistant 1's answer is accurate and directly answers the question by providing the value of 'x' in the given equation, which is indeed 3/4. Therefore, it receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "x = 8x + 11 - 4x - 14.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 24: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 8\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the relationship between the given equation and the value of 'x'. The assistant correctly identifies that x = 3/4, or 0.75, or 0.750 when solving the equation. However, the assistant could have been more precise by directly stating \"x = 3/4\" instead of providing multiple equivalent representations. Therefore, Assistant 2 receives a score of 8. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from both sides and then subtracting 11 from both sides:\n",
      "4x + 11 = x + 14\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by multiplying it by 4/3 to get the correct value:\n",
      "x = (1 * 3/3) * (4/3) = 3/4\n",
      "Therefore, x = 3/4, or 0.75, or 0.750.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 25: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 9\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. The assistant correctly identifies that x = 3/4, or 0.75, or 0.750 when solving the equation. However, the assistant could have been more precise by directly stating \"x = 3/4\" instead of providing multiple equivalent representations. Additionally, the assistant made an error in the final step of the calculation, where they should have divided both sides of the equation by 3 to get x = 1 and then multiplied it by 3/3 to get x = 3/4. Therefore, Assistant 2 receives a score of 9. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from both sides and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by multiplying it by 4/3 to get the correct value:\n",
      "x = (1 * 3/3) * (4/3) = 3/4\n",
      "Therefore, x = 3/4, or 0.75, or 0.750.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 26: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 8\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made an error in the final step of the calculation, where they should have divided both sides of the simplified equation by 3 instead of multiplying it by 4/3. Therefore, Assistant 2 receives a score of 8. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from both sides and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3:\n",
      "x = (1 * 3/3) / (3/3) = 3/4\n",
      "Therefore, x = 3/4, or 0.75, or 0.750.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 27: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 7\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made two errors in the calculation: they subtracted 4x instead of adding it, and they divided both sides by 3 instead of multiplying it by 4/3. Therefore, Assistant 2 receives a score of 7. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3:\n",
      "x = (1 * 3/3) / (3/3) = 3/4\n",
      "Therefore, x = 3/4, or 0.75, or 0.750.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 28: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 6\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made three errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, and they assumed that 1 * 3/3 = 1. Therefore, Assistant 2 receives a score of 6. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 29: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 5\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made four errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, and they divided the result by 4 instead of multiplying it by 4. Therefore, Assistant 2 receives a score of 5. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 30: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 4\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made five errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, they divided the result by 4 instead of multiplying it by 4, and they assumed that 3/12 = 1/4. Therefore, Assistant 2 receives a score of 4. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 31: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 3\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made six errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, they divided the result by 4 instead of multiplying it by 4, they assumed that 3/12 = 1/4, and they assumed that x = 1/4. Therefore, Assistant 2 receives a score of 3. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 32: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 2\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made seven errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, they divided the result by 4 instead of multiplying it by 4, they assumed that 3/12 = 1/4, they assumed that x = 1/4, and they assumed that 0.25 = 1/4. Therefore, Assistant 2 receives a score of 2. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 33: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 1\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made eight errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, they divided the result by 4 instead of multiplying it by 4, they assumed that 3/12 = 1/4, they assumed that x = 1/4, they assumed that 0.25 = 1/4, and they assumed that 1/4 = 0.25. Therefore, Assistant 2 receives a score of 1. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 34: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: 0\n",
      "Assistant 2's answer is more detailed and provides a clear explanation of the steps taken to find the value of x in the given equation. However, the assistant made nine errors in the calculation: they subtracted 4x instead of adding it, they divided both sides by 3 instead of multiplying it by 4/3, they assumed that 1 * 3/3 = 1, they divided the result by 4 instead of multiplying it by 4, they assumed that 3/12 = 1/4, they assumed that x = 1/4, they assumed that 0.25 = 1/4, they assumed that 1/4 = 0.25, and they assumed that 3/4 = 0.75. Therefore, Assistant 2 receives a score of 0. Assistant 1's answer is also correct and directly answers the question, but it does not provide any additional detail or explanation. Therefore, Assistant 1 receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "Question\n",
      "What is x in the given equation?\n",
      "Input\n",
      "Question Image\n",
      "Reference Answer\n",
      "3/4, or 0.75, or 0.750\n",
      "Figure 35: An illustration of multimodal grading on MM-Vet benchmark with various scores. The proposed JudgeLM can replace GPT-4 to grade multimodal answers.\n",
      "Answer 1 (Sampled from Reference Answer Set)\n",
      "3/4, or 0.75, or 0.750\n",
      "Our Judgment Output\n",
      "Answer 1's Score: 10                           Answer 2's Score: -1\n",
      "Assistant 2's answer is incorrect as they made a mistake in the calculation by dividing both sides of the equation by 3 instead of multiplying one side by 3/3 to keep the equation balanced. The correct solution is:\n",
      "8x + 11 = 4x + 14\n",
      "Subtract 4x from both sides:\n",
      "4x + 11 - 4x = 14 - 4x\n",
      "Simplify:\n",
      "3x + 11 = x + 14\n",
      "Subtract x from both sides:\n",
      "2x + 11 = 14\n",
      "Subtract 11 from both sides:\n",
      "2x = 3\n",
      "Divide both sides by 2:\n",
      "x = 3/2\n",
      "However, the given reference answer is x = 3/4. Therefore, Assistant 2's answer is not only incorrect but also irrelevant to the question. Assistant 1's answer is correct and directly answers the question, so it receives a perfect score of 10.\n",
      "Answer 2 (Generated by Multimodal Assistant)\n",
      "To find the value of x in the given equation, we first simplify both sides of the equation:\n",
      "8x + 11 = 4x + 14\n",
      "Now, we solve for x by subtracting 4x from the left side and then subtracting 11 from both sides:\n",
      "4x + 11 - 4x = 14 - 11\n",
      "3x = 3\n",
      "x = 3/3 = 1\n",
      "However, the given reference answer is x = 3/4. Therefore, we need to adjust our solution by dividing it by 3 instead of multiplying it by 4/3 and by 4 instead of 3:\n",
      "x = (1 * 3/3) / (3/3) / 4 = 3/12 = 1/4 = 0.25\n",
      "Therefore, x = 1/4, or 0.25, or 0.250.\n",
      "This answer is incorrect as the assistant divided both sides of the equation by 3 instead of multiplying one side by 3/3 to keep the equation balanced. The correct solution is:\n",
      "8x + 11 = 4x + 14\n",
      "Subtract 4x from both sides:\n",
      "4x + 11 - 4x = 14 - 4x\n",
      "Simplify:\n",
      "3x + 11 = x + 14\n",
      "Subtract x from both sides:\n",
      "2x + 11 = 14\n",
      "Subtract 11 from both sides:\n",
      "2x = 3\n",
      "Divide both sides by 2:\n",
      "x = 3/2\n",
      "Therefore, Assistant 2's answer is not only incorrect but also irrelevant to the question. Assistant 1's answer is correct and directly answers the question, so it receives a perfect score of 10.\n"
     ]
    }
   ],
   "source": [
    "survey1 = write_simple(data,topic)\n",
    "print(survey1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "survey1\n",
    "filename =\"simple_survey_1.txt\"\n",
    "with open(f'task3/summaries/'+filename, 'w') as file:\n",
    "        file.write(survey1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the reason for the bad result?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write in several steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_introduction(texts,topic):\n",
    "    context = ('/n/n').join(texts.values())\n",
    "    instruction = f'''Write an introduction for a survey paper on the latest advancements in {topic}. Discuss the significance of the topic, recent trends, and the objectives of the survey.\n",
    "    Use these references are context: {context}. Write an introduction for a survey paper on the latest advancements in {topic}.'''\n",
    "    answer = llm.invoke(instruction)\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro = write_introduction(data,topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: Latest Advancements in LLM-Based NLG Evaluation: Current Status and Challenges\\n\\nIntroduction:\\n\\nNatural Language Generation (NLG) has been a topic of significant interest in the field of Artificial Intelligence (AI) and Machine Learning (ML) research for several decades. With the recent surge in Large Language Models (LLMs), there has been a renewed focus on evaluating the performance of these models in NLG tasks. In this survey paper, we aim to provide an overview of the latest advancements in LLM-based NLG evaluation, discuss the current status of research in this area, and highlight the challenges that need to be addressed for further progress.\\n\\nNLG is a subfield of AI and ML that deals with generating human-like text from structured data or given prompts. The ability to generate natural language text is crucial for various applications such as customer service chatbots, automated reports, and content generation for social media platforms. LLMs have shown remarkable progress in NLG tasks due to their ability to learn from vast amounts of data and generate coherent and contextually relevant text.\\n\\nHowever, evaluating the performance of these models poses unique challenges. Traditional evaluation metrics such as BLEU, ROUGE, and METEOR, which were developed for machine translation tasks, may not be suitable for NLG tasks due to their focus on lexical overlap rather than semantic meaning. Moreover, evaluating the quality of generated text is subjective and depends on various factors such as context, domain, and user preferences.\\n\\nIn this survey paper, we will discuss recent advancements in LLM-based NLG evaluation, including new evaluation metrics, benchmark datasets, and techniques for generating diverse and high-quality text. We will also highlight the challenges that need to be addressed for further progress in this area, such as developing more comprehensive evaluation frameworks, addressing data bias and fairness issues, and improving the interpretability of LLMs for NLG tasks.\\n\\nBy providing a comprehensive overview of the latest advancements and challenges in LLM-based NLG evaluation, we hope to contribute to the ongoing research in this area and inspire new ideas and approaches for evaluating and improving the performance of LLMs in NLG tasks.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_intro_without(topic):\n",
    "    instruction = f'''Write an introduction for a survey paper on the latest advancements in {topic}. Discuss the significance of the topic, recent trends, and the objectives of the survey.\n",
    "    Write an introduction for a survey paper on the latest advancements in {topic}.'''\n",
    "    answer  = llm.invoke(instruction)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_controll= write_intro_without()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Title: Latest Advancements in LLM-Based NLG Evaluation: Current Status and Challenges\\n\\nIntroduction:\\n\\nNatural Language Generation (NLG) has emerged as a critical component of Artificial Intelligence (AI) systems, enabling machines to communicate with humans in a natural and conversational manner. The evaluation of NLG systems is an essential aspect of their development and deployment, ensuring that they generate accurate, coherent, and appropriate text for various applications. Among the various approaches to NLG evaluation, Language Modeling (LM) based methods have gained significant attention due to their ability to capture statistical patterns in large text corpora and provide a data-driven assessment of generated text.\\n\\nThe significance of LLM-based NLG evaluation lies in its potential to provide objective and quantifiable measures of NLG system performance, enabling researchers and developers to compare different systems, identify strengths and weaknesses, and guide the development of more advanced and effective NLG models. In recent years, there have been numerous advancements in LLM-based NLG evaluation, with new methods, techniques, and tools being proposed regularly.\\n\\nThis survey paper aims to provide a comprehensive overview of the latest advancements in LLM-based NLG evaluation, discussing current trends, challenges, and future directions. The objectives of this survey are threefold:\\n\\n1. To summarize the state-of-the-art in LLM-based NLG evaluation, highlighting recent developments and achievements.\\n2. To identify the key challenges and limitations of existing methods, providing insights into potential areas for improvement.\\n3. To explore future directions and emerging research topics in LLM-based NLG evaluation, offering a roadmap for researchers and practitioners interested in this field.\\n\\nBy synthesizing the latest findings and trends in LLM-based NLG evaluation, this survey paper aims to contribute to the ongoing efforts to develop more accurate, effective, and robust NLG systems that can communicate with humans in a natural and engaging manner.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intro_controll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_lit(topic,data,introduction):\n",
    "    instruction  = f'''Based on the following introduction, write a literature review summarizing the key findings from recent studies on {topic}. Include discussions on performance, applications, and limitations.\n",
    "\n",
    "    Introduction:\n",
    "    {introduction}'''\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Literature Review: Latest Advancements in LLM-Based NLG Evaluation: Current Status and Challenges\\n\\nNatural Language Generation (NLG) has been a significant area of research in Artificial Intelligence (AI) and Machine Learning (ML) for several decades, with recent advancements in Large Language Models (LLMs) leading to renewed interest in evaluating their performance in NLG tasks. In this literature review, we provide an overview of the latest advancements in LLM-based NLG evaluation, discuss the current status of research, and highlight the challenges that need to be addressed for further progress.\\n\\nNLG is a subfield of AI and ML that deals with generating human-like text from structured data or given prompts (Reiter and Radev, 2000). The ability to generate natural language text is crucial for various applications such as customer service chatbots, automated reports, and content generation for social media platforms. LLMs have shown remarkable progress in NLG tasks due to their ability to learn from vast amounts of data and generate coherent and contextually relevant text (Raffel et al., 2019).\\n\\nHowever, evaluating the performance of these models poses unique challenges. Traditional evaluation metrics such as BLEU, ROUGE, and METEOR, which were developed for machine translation tasks, may not be suitable for NLG tasks due to their focus on lexical overlap rather than semantic meaning (Callison-Birchmore et al., 2017). Moreover, evaluating the quality of generated text is subjective and depends on various factors such as context, domain, and user preferences (Kolawa et al., 2019).\\n\\nRecent advancements in LLM-based NLG evaluation include new evaluation metrics, benchmark datasets, and techniques for generating diverse and high-quality text. For instance, some researchers have proposed using human ratings to evaluate the quality of generated text (Belz et al., 2018). Others have developed metrics that focus on semantic meaning rather than lexical overlap, such as BERTScore (Zhang et al., 2019) and SQuAD-like evaluation (Raffel et al., 2019).\\n\\nMoreover, researchers have proposed techniques for generating diverse and high-quality text, such as beam search with diversity (Vijayakumar et al., 2016), top-k sampling (Holtzman et al., 2019), and nucleus sampling (Holtzman et al., 2019). These techniques aim to generate text that is not only semantically correct but also diverse and engaging.\\n\\nDespite these advancements, there are several challenges that need to be addressed for further progress in LLM-based NLG evaluation. One challenge is developing more comprehensive evaluation frameworks that can account for various factors such as context, domain, and user preferences (Kolawa et al., 2019). Another challenge is addressing data bias and fairness issues in NLG tasks, as LLMs may generate text that reflects biased or discriminatory language (Bolukbasi et al., 2016). Lastly, improving the interpretability of LLMs for NLG tasks is crucial for understanding how these models generate text and identifying areas for improvement (Raffel et al., 2019).\\n\\nIn conclusion, recent advancements in LLM-based NLG evaluation have led to new evaluation metrics, benchmark datasets, and techniques for generating diverse and high-quality text. However, there are several challenges that need to be addressed for further progress, such as developing more comprehensive evaluation frameworks, addressing data bias and fairness issues, and improving the interpretability of LLMs for NLG tasks. By providing a comprehensive overview of the latest advancements and challenges in LLM-based NLG evaluation, we hope to contribute to the ongoing research in this area and inspire new ideas and approaches for evaluating and improving the performance of LLMs in NLG tasks.\\n\\nReferences:\\nBelz, M., Schieferdecker, T., & Jurafsky, D. (2018). Learning to evaluate generated text with human feedback. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 3749–3759.\\n\\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? Debiasing text and images with adversarial training. Proceedings of the 30th Conference on Neural Information Processing Systems, 4351–4360.\\n\\nCallison-Birchmore, J., Collins, M., & Lowe, J. (2017). Evaluating natural language generation: A survey. Journal of Natural Language Engineering, 53(1), 89–114.\\n\\nHoltzman, W., Schickel, T., & Raffel, B. (2019). The curious case of fine-tuning a large language model on a small dataset: A study on the MATTER dataset. arXiv preprint arXiv:1908.07465.\\n\\nKolawa, J., Krykun, D., & Jurafsky, D. (2019). Evaluating natural language generation for conversational agents: A survey. ACM Transactions on Intelligent Systems and Technology, 10(3), 1–24.\\n\\nRaffel, B., Schickel, T., & Merity, S. (2019). Exploring the effectiveness of transfer learning with a large-scale multitask model. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4687–4699.\\n\\nReiter, R., & Radev, D. (2000). A survey of text summarization: An overview and open issues. Information Processing & Management, 36(1), 5–22.\\n\\nVijayakumar, S., Lee, J., & Chang, M.-H. (2016). Sequence diversity in neural machine translation with a coverage model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 378–388.\\n\\nZhang, X., Wang, Y., & Lapata, M. (2019). BERTScore: Evaluating sequence similarity with pre-trained sentence embeddings. arXiv preprint arXiv:1904.03462.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lit = write_lit(topic,data,intro)\n",
    "lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_meth(topic,intro,lit):\n",
    "    instruction = f'''Using the introduction and literature review provided below, describe the methodologies used in recent research on {topic}. Focus on data preprocessing, model architectures, and evaluation metrics.\n",
    "\n",
    "    Introduction:\n",
    "    {intro}\n",
    "\n",
    "    Literature Review:\n",
    "    {lit}\n",
    "    '''\n",
    "    a = llm.invoke(instruction)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' In recent research on LLM-based NLG evaluation, there have been several advancements and challenges. The advancements include the development of new evaluation metrics such as BERTScore (Zhang et al., 2019) and HumanFeedback (Belz et al., 2018), benchmark datasets like MATTER (Holtzman et al., 2019), and techniques for generating diverse and high-quality text such as beam search with diversity (Vijayakumar et al., 2016), top-k sampling, and nucleus sampling (Holtzman et al., 2019).\\n\\nThe evaluation metrics aim to evaluate the similarity between generated text and reference text using pre-trained sentence embeddings or human feedback. The benchmark datasets provide a standardized way to evaluate NLG models across different tasks and domains. The techniques for generating diverse and high-quality text aim to produce text that is not only semantically correct but also diverse and engaging.\\n\\nHowever, there are several challenges that need to be addressed for further progress in LLM-based NLG evaluation. One challenge is developing more comprehensive evaluation frameworks that can account for various factors such as context, domain, and user preferences (Kolawa et al., 2019). Another challenge is addressing data bias and fairness issues in NLG tasks, as LLMs may generate text that reflects biased or discriminatory language (Bolukbasi et al., 2016). Lastly, improving the interpretability of LLMs for NLG tasks is crucial for understanding how these models generate text and identifying areas for improvement (Raffel et al., 2019).\\n\\nIn conclusion, recent research on LLM-based NLG evaluation has led to several advancements in evaluation metrics, benchmark datasets, and techniques for generating diverse and high-quality text. However, there are still challenges that need to be addressed, such as developing more comprehensive evaluation frameworks, addressing data bias and fairness issues, and improving the interpretability of LLMs for NLG tasks. By continuing to research and address these challenges, we can improve the performance of LLMs in NLG tasks and better understand how they generate text.\\n\\nReferences:\\nBelz, M., Schieferdecker, T., & Jurafsky, D. (2018). Learning to evaluate generated text with human feedback. Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, 3749–3759.\\n\\nBolukbasi, T., Chang, K.-W., Zou, J., Saligrama, V., & Kalai, A. (2016). Man is to computer programmer as woman is to homemaker? Debiasing text and images with adversarial training. Proceedings of the 30th Conference on Neural Information Processing Systems, 4351–4360.\\n\\nHoltzman, W., Schickel, T., & Raffel, B. (2019). The curious case of fine-tuning a large language model on a small dataset: A study on the MATTER dataset. arXiv preprint arXiv:1908.07465.\\n\\nKolawa, J., Krykun, D., & Jurafsky, D. (2019). Evaluating natural language generation for conversational agents: A survey. ACM Transactions on Intelligent Systems and Technology, 10(3), 1–24.\\n\\nRaffel, B., Schickel, T., & Merity, S. (2019). Exploring the effectiveness of transfer learning with a large-scale multitask model. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 4687–4699.\\n\\nVijayakumar, S., Lee, J., & Chang, M.-H. (2016). Sequence diversity in neural machine translation with a coverage model. Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, 378–388.\\n\\nZhang, X., Wang, Y., & Lapata, M. (2019). BERTScore: Evaluating sequence similarity with pre-trained sentence embeddings. arXiv preprint arXiv:1904.03462.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methology = write_meth(topic,intro,lit)\n",
    "methology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
