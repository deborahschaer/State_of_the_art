 This paper discusses the limitations of current evaluation paradigms for large language models (LLMs) and proposes BatchEval, a batch-wise evaluation method to address these issues. The authors argue that humans use both criterion definition and inter-sample comparison as references during evaluation. They explore different variants under the BatchEval paradigm and demonstrate its superiority over state-of-the-art methods in terms of Pearson correlations with a lower API cost on average.

The paper also discusses the challenges of using reference-free evaluators based on LLMs for closed-ended examples, where the correct semantic response is unique. The authors construct two adversarial meta-evaluation dialogue generation datasets (KdConv-ADV and DSTC7-ADV) to evaluate the ability of LLMs in identifying unreasonable responses. They find that LLMs' performance in this regard is insufficient, highlighting the risks of using reference-free evaluators based on LLMs for closed-ended examples.

The paper further explores ChatGPT's ability as a factual inconsistency evaluator for text summarization and compares it to previous evaluation metrics. The results indicate that ChatGPT outperforms these metrics in coarse-grained and fine-grained tasks, demonstrating its potential for factual inconsistency evaluation. However, the paper also points out certain limitations of ChatGPT's output.

Lastly, the authors introduce PandaLM, an automatic evaluation benchmark for LLM instruction tuning optimization. They argue that evaluating and optimizing LLMs is a challenging task due to the complexity of hyperparameter selection and the need for an automatic, reliable, and robust evaluation benchmark. The results show that PandaLM achieves significant improvements in model performance compared to their counterparts trained with default Alpaca's hyperparameters. All resources of PandaLM are released at https://github.com/WeOpenML/PandaLM.