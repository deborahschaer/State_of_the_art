 This paper explores various aspects of large language models (LLMs) evaluation and fairness. The authors propose using wider and deeper networks for more fair evaluations, drawing inspiration from deep neural networks. They construct the largest English evaluation benchmark, LLMEval$^2$, with 15 tasks, 8 abilities, and 2,553 samples, demonstrating that a wider network with two layers performs best.

Another approach proposed is using LLMs as judges to evaluate other models on more open-ended questions. The authors examine the usage and limitations of LLM-as-a-judge and propose solutions for biases and limited reasoning ability. They introduce MT-bench and Chatbot Arena benchmarks, revealing that strong LLM judges like GPT-4 match human preferences well.

Lastly, the authors propose fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks. They train JudgeLM at different scales and conduct a systematic analysis of its capabilities and behaviors. JudgeLM introduces techniques to address biases and obtains state-of-the-art judge performance on existing and new benchmarks, demonstrating extended capabilities as judges for single answer, multimodal models, multiple answers, and multi-turn chat.