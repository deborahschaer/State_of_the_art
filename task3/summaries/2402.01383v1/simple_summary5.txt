 This paper explores the use of Large Language Models (LLMs) as reliable judges for text summarization tasks, specifically in evaluating factual consistency. The study introduces an approach using a singular LLM for factuality scoring and examines various LLMs' efficacy in direct factuality scoring. Contrary to expectations, the results indicate limited correlations between factuality metrics and human evaluations for GPT-4 and PaLM-2, with only notable correlations observed with GPT-3.5 across two subcategories. The study also investigates ChatGPT's ability as a factual inconsistency evaluator for text summarization and finds that it generally outperforms previous evaluation metrics but has limitations such as preference for lexically similar candidates, false reasoning, and inadequate understanding of instructions.

Additionally, another paper introduces TIGERScore, a trained metric for explainable and reference-free evaluation of text generation tasks. The metric is guided by natural language instruction to provide error analysis and is based on LLaMA-2, trained on the MetricInstruct dataset. The results show that TIGERScore can achieve the open-source state-of-the-art correlation with human ratings across various datasets and even surpass the best existing reference-based metrics as a reference-free metric. The explanations generated by the metric are found to be 70.8% accurate through human evaluation. Both studies' resources are available on their project website.