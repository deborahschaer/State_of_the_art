 This summary discusses several recent studies on evaluating the quality of text generated by artificial intelligence (AI) models. The first study introduces GPTScore, a novel evaluation framework that utilizes the emergent abilities of generative pre-trained models to score generated texts based on natural language instructions. The second study presents the Eval4NLP 2023 shared task, which focuses on prompting and scoring extraction for machine translation (MT) and summarization evaluation using large language models as judges. The third study explores using ChatGPT as a factual inconsistency evaluator for text summarization. Overall, these studies demonstrate the potential of AI models in achieving effective text evaluation while addressing long-standing challenges in customized, multi-faceted evaluation without annotated samples.