 This text discusses several research papers related to evaluating and using large language models (LLMs) as judges for various tasks. The first paper explores the use of wider and deeper networks for fairer LLM evaluations, drawing inspiration from deep neural networks and academic paper reviewing. The second paper proposes using strong LLMs as judges for evaluating other LLMs on more open-ended questions and introduces two benchmarks: MT-bench and Chatbot Arena. The third paper proposes fine-tuning LLMs as scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in open-ended benchmarks.

The first paper, "Wider and Deeper LLM Networks are Fairer LLM Evaluators," discusses the challenges of evaluating the quality of responses generated by LLMs and proposes a method using an LLM itself to make evaluations through multiple independent evaluations. The authors explore whether deeper and wider networks can lead to fairer evaluations, inspired by the observation that different neurons in a neural network are responsible for detecting different concepts. They construct the largest and most diverse English evaluation benchmark, LLMEval$^2$, and demonstrate that a wider network with two layers performs the best.

The second paper, "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena," addresses the challenges of evaluating LLM-based chat assistants using existing benchmarks and proposes using strong LLMs as judges to evaluate these models on more open-ended questions. The authors examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, and propose solutions to mitigate some of them. They verify the agreement between LLM judges and human preferences using MT-bench and Chatbot Arena and demonstrate that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well.

The third paper, "JudgeLM: Fine-tuned Large Language Models are Scalable Judges," proposes fine-tuning LLMs as scalable judges to evaluate LLMs efficiently and effectively in open-ended benchmarks. The authors propose a comprehensive, large-scale, high-quality dataset for fine-tuning high-performance judges and introduce techniques such as swap augmentation, reference support, and reference drop to address key biases in fine-tuning LLM as a judge. They demonstrate that JudgeLM obtains the state-of-the-art judge performance on both existing and new benchmarks and is efficient, with JudgeLM-7B only needing 3 minutes to judge 5K samples with 8 A100 GPUs.

All resources for these papers are available at https://github.com/e0397123/comp-analysis.