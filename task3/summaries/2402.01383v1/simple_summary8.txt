 This paper discusses various methods for evaluating natural language generation (NLG) models, focusing on collaborative evaluation using large language models (LLMs) and humans, automated evaluation of Retrieval Augmented Generation (RAG), factual inconsistency evaluation using ChatGPT, and SocREval, a method for reference-free reasoning evaluation.

Firstly, the authors propose CoEval, a collaborative evaluation pipeline that involves LLMs generating initial ideas and humans providing scrutiny. They conducted experiments to investigate the mutual effects between humans and LLMs in CoEval, which showed that utilizing LLMs effectively evaluates lengthy texts while human scrutiny ensures reliability.

Secondly, they introduce RAGAs, a framework for reference-free evaluation of RAG pipelines, which are composed of a retrieval system and an LLM-based generation module. RAGAs provide a suite of metrics to evaluate the different dimensions of RAG architectures without relying on ground truth human annotations.

Thirdly, they explore ChatGPT's ability to evaluate factual inconsistency in text summarization under a zero-shot setting. Experimental results indicate that ChatGPT outperforms previous evaluation metrics across three tasks, making it a promising tool for factual inconsistency evaluation. However, limitations such as preference for lexically similar candidates and false reasoning were identified.

Lastly, they present SocREval, an approach for prompt design in reference-free reasoning evaluation that harnesses GPT-4 to automatically evaluate reasoning chain quality. Empirical results from four human-annotated datasets reveal that SocREval significantly improves GPT-4's performance and is cost-efficient and robust to prompt writing and example selection.