 In the first study, researchers evaluated BatchEval, a new text evaluation method using a two-stage procedure with heterogeneous batch composition strategy and decimal scoring format. Experiments on three large language models (LLMs) across four tasks showed that BatchEval outperformed state-of-the-art methods by 10.5% in terms of Pearson correlations while only costing an average of 64% API.

In the second study, researchers investigated using LLMs as references for text summarization models and compared human and LLM-based evaluation methods. They found that smaller summarization models could achieve similar performance under LLM-based evaluation but not yet reach LLM-level performance under human evaluation. The meta-analysis revealed a discrepancy between the two types of evaluations, highlighting benefits and risks of using LLMs as references.

The third study focused on the reliability of reference-free evaluators based on LLMs for dialogue response quality. Researchers constructed adversarial meta-evaluation datasets to challenge evaluators' ability to evaluate closed-ended examples with external knowledge or their own knowledge. Results showed that LLMs had insufficient ability to identify unreasonable responses, posing risks in using reference-free evaluators based on LLMs.

The fourth study proposed AutoCalibrate, a multi-stage approach to calibrate and align an LLM-based evaluator towards human preference without explicitly modeling human preferences. Experiments on multiple text quality evaluation datasets demonstrated significant improvement in correlation with expert evaluation through calibration.

Finally, the Eval4NLP 2023 shared task introduced a competition setting for participants to explore prompting and score extraction using LLMs as explainable metrics for machine translation and summarization evaluation. Despite restrictions on fine-tuning, best-performing systems achieved results comparable to or surpassing recent reference-free metrics. A separate track performed small-scale human evaluations of the plausibility of explanations given by the LLMs.