 Recent developments in LLM (Large Language Model) generated text evaluation include its application in summarization evaluation using metrics such as ROUGE or BERTScore (Zhang et al., 2020), text classification tasks (Gilardi et al., 2023), and reward design for RL agents (Kwon et al., 2023). LLMs have shown remarkable capabilities across diverse downstream NLP tasks, making the automatic measurement of their performance an essential research problem (Chang et al., 2023; Zhang et al., 2023; Liu et al., 2024). Strong evaluation methods are expected to provide high-quality critiques that act as scalable feedback and guide LLMs to improve persistently (Cui et al., 2023). A recent study by Kotonya, Krishnasamy, Tetreault, and Jaimes (Little Giants: Exploring the Potential of Small LLMs as Evaluation Metrics in Summarization in the Eval4NLP 2023 Shared Task) investigated the potential use of small LLMs as evaluation metrics in summarization tasks.