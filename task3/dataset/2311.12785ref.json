{
    "2202.01279": {
        "title": "PromptSource: An Integrated Development Environment and Repository for Natural Language Prompts",
        "abstract": "PromptSource is a system for creating, sharing, and using natural language\nprompts. Prompts are functions that map an example from a dataset to a natural\nlanguage input and target output. Using prompts to train and query language\nmodels is an emerging area in NLP that requires new tools that let users\ndevelop and refine these prompts collaboratively. PromptSource addresses the\nemergent challenges in this new setting with (1) a templating language for\ndefining data-linked prompts, (2) an interface that lets users quickly iterate\non prompt development by observing outputs of their prompts on many examples,\nand (3) a community-driven set of guidelines for contributing new prompts to a\ncommon pool. Over 2,000 prompts for roughly 170 datasets are already available\nin PromptSource. PromptSource is available at\nhttps://github.com/bigscience-workshop/promptsource.",
        "date": "2022-02-02T20:48:54+00:00",
        "label": 1
    },
    "2302.04023": {
        "title": "A Multitask, Multilingual, Multimodal Evaluation of ChatGPT on Reasoning, Hallucination, and Interactivity",
        "abstract": "This paper proposes a framework for quantitatively evaluating interactive\nLLMs such as ChatGPT using publicly available data sets. We carry out an\nextensive technical evaluation of ChatGPT using 23 data sets covering 8\ndifferent common NLP application tasks. We evaluate the multitask, multilingual\nand multi-modal aspects of ChatGPT based on these data sets and a newly\ndesigned multimodal dataset. We find that ChatGPT outperforms LLMs with\nzero-shot learning on most tasks and even outperforms fine-tuned models on some\ntasks. We find that it is better at understanding non-Latin script languages\nthan generating them. It is able to generate multimodal content from textual\nprompts, via an intermediate code generation step. Moreover, we find that\nChatGPT is 63.41% accurate on average in 10 different reasoning categories\nunder logical reasoning, non-textual reasoning, and commonsense reasoning,\nhence making it an unreliable reasoner. It is, for example, better at deductive\nthan inductive reasoning. ChatGPT suffers from hallucination problems like\nother LLMs and it generates more extrinsic hallucinations from its parametric\nmemory as it does not have access to an external knowledge base. Finally, the\ninteractive feature of ChatGPT enables human collaboration with the underlying\nLLM to improve its performance, i.e, 8% ROUGE-1 on summarization and 2% ChrF++\non machine translation, in a multi-turn \"prompt engineering\" fashion. We also\nrelease codebase for evaluation set extraction.",
        "date": "2023-02-08T12:35:34+00:00",
        "label": 1
    },
    "2308.14840": {
        "title": "Identifying and Mitigating the Security Risks of Generative AI",
        "abstract": "Every major technical invention resurfaces the dual-use dilemma -- the new\ntechnology has the potential to be used for good as well as for harm.\nGenerative AI (GenAI) techniques, such as large language models (LLMs) and\ndiffusion models, have shown remarkable capabilities (e.g., in-context\nlearning, code-completion, and text-to-image generation and editing). However,\nGenAI can be used just as well by attackers to generate new attacks and\nincrease the velocity and efficacy of existing attacks.\n  This paper reports the findings of a workshop held at Google (co-organized by\nStanford University and the University of Wisconsin-Madison) on the dual-use\ndilemma posed by GenAI. This paper is not meant to be comprehensive, but is\nrather an attempt to synthesize some of the interesting findings from the\nworkshop. We discuss short-term and long-term goals for the community on this\ntopic. We hope this paper provides both a launching point for a discussion on\nthis important topic as well as interesting problems that the research\ncommunity can work to address.",
        "date": "2023-08-28T18:51:09+00:00",
        "label": 1
    },
    "2305.01625": {
        "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
        "abstract": "Since the proposal of transformers, these models have been limited to bounded\ninput lengths, because of their need to attend to every token in the input. In\nthis work, we propose Unlimiformer: a general approach that wraps any existing\npretrained encoder-decoder transformer, and offloads the cross-attention\ncomputation to a single k-nearest-neighbor (kNN) index, while the returned kNN\ndistances are the attention dot-product scores. This kNN index can be kept on\neither the GPU or CPU memory and queried in sub-linear time; this way, we can\nindex practically unlimited input sequences, while every attention head in\nevery decoder layer retrieves its top-k keys, instead of attending to every\nkey. We evaluate Unlimiformer on several long-document and book-summarization\nbenchmarks, showing that it can process even 500k token-long inputs from the\nBookSum dataset, without any input truncation at test time. We demonstrate that\nUnlimiformer improves pretrained models such as BART and Longformer by\nextending them to unlimited inputs without additional learned weights and\nwithout modifying their code. We make our code and models publicly available at\nhttps://github.com/abertsch72/unlimiformer .",
        "date": "2023-05-02T17:35:08+00:00",
        "label": 1
    },
    "2304.11062": {
        "title": "Scaling Transformer to 1M tokens and beyond with RMT",
        "abstract": "A major limitation for the broader scope of problems solvable by transformers\nis the quadratic scaling of computational complexity with input size. In this\nstudy, we investigate the recurrent memory augmentation of pre-trained\ntransformer models to extend input context length while linearly scaling\ncompute. Our approach demonstrates the capability to store information in\nmemory for sequences of up to an unprecedented two million tokens while\nmaintaining high retrieval accuracy. Experiments with language modeling tasks\nshow perplexity improvement as the number of processed input segments\nincreases. These results underscore the effectiveness of our method, which has\nsignificant potential to enhance long-term dependency handling in natural\nlanguage understanding and generation tasks, as well as enable large-scale\ncontext processing for memory-intensive applications.",
        "date": "2023-04-19T16:18:54+00:00",
        "label": 1
    },
    "2303.04226": {
        "title": "A Comprehensive Survey of AI-Generated Content (AIGC): A History of Generative AI from GAN to ChatGPT",
        "abstract": "Recently, ChatGPT, along with DALL-E-2 and Codex,has been gaining significant\nattention from society. As a result, many individuals have become interested in\nrelated resources and are seeking to uncover the background and secrets behind\nits impressive performance. In fact, ChatGPT and other Generative AI (GAI)\ntechniques belong to the category of Artificial Intelligence Generated Content\n(AIGC), which involves the creation of digital content, such as images, music,\nand natural language, through AI models. The goal of AIGC is to make the\ncontent creation process more efficient and accessible, allowing for the\nproduction of high-quality content at a faster pace. AIGC is achieved by\nextracting and understanding intent information from instructions provided by\nhuman, and generating the content according to its knowledge and the intent\ninformation. In recent years, large-scale models have become increasingly\nimportant in AIGC as they provide better intent extraction and thus, improved\ngeneration results. With the growth of data and the size of the models, the\ndistribution that the model can learn becomes more comprehensive and closer to\nreality, leading to more realistic and high-quality content generation. This\nsurvey provides a comprehensive review on the history of generative models, and\nbasic components, recent advances in AIGC from unimodal interaction and\nmultimodal interaction. From the perspective of unimodality, we introduce the\ngeneration tasks and relative models of text and image. From the perspective of\nmultimodality, we introduce the cross-application between the modalities\nmentioned above. Finally, we discuss the existing open problems and future\nchallenges in AIGC.",
        "date": "2023-03-07T20:36:13+00:00",
        "label": 1
    },
    "2303.17466": {
        "title": "Assessing Cross-Cultural Alignment between ChatGPT and Human Societies: An Empirical Study",
        "abstract": "The recent release of ChatGPT has garnered widespread recognition for its\nexceptional ability to generate human-like responses in dialogue. Given its\nusage by users from various nations and its training on a vast multilingual\ncorpus that incorporates diverse cultural and societal norms, it is crucial to\nevaluate its effectiveness in cultural adaptation. In this paper, we\ninvestigate the underlying cultural background of ChatGPT by analyzing its\nresponses to questions designed to quantify human cultural differences. Our\nfindings suggest that, when prompted with American context, ChatGPT exhibits a\nstrong alignment with American culture, but it adapts less effectively to other\ncultural contexts. Furthermore, by using different prompts to probe the model,\nwe show that English prompts reduce the variance in model responses, flattening\nout cultural differences and biasing them towards American culture. This study\nprovides valuable insights into the cultural implications of ChatGPT and\nhighlights the necessity of greater diversity and cultural awareness in\nlanguage technologies.",
        "date": "2023-03-30T15:43:39+00:00",
        "label": 1
    },
    "2307.03109": {
        "title": "A Survey on Evaluation of Large Language Models",
        "abstract": "Large language models (LLMs) are gaining increasing popularity in both\nacademia and industry, owing to their unprecedented performance in various\napplications. As LLMs continue to play a vital role in both research and daily\nuse, their evaluation becomes increasingly critical, not only at the task\nlevel, but also at the society level for better understanding of their\npotential risks. Over the past years, significant efforts have been made to\nexamine LLMs from various perspectives. This paper presents a comprehensive\nreview of these evaluation methods for LLMs, focusing on three key dimensions:\nwhat to evaluate, where to evaluate, and how to evaluate. Firstly, we provide\nan overview from the perspective of evaluation tasks, encompassing general\nnatural language processing tasks, reasoning, medical usage, ethics,\neducations, natural and social sciences, agent applications, and other areas.\nSecondly, we answer the `where' and `how' questions by diving into the\nevaluation methods and benchmarks, which serve as crucial components in\nassessing performance of LLMs. Then, we summarize the success and failure cases\nof LLMs in different tasks. Finally, we shed light on several future challenges\nthat lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to\nresearchers in the realm of LLMs evaluation, thereby aiding the development of\nmore proficient LLMs. Our key point is that evaluation should be treated as an\nessential discipline to better assist the development of LLMs. We consistently\nmaintain the related open-source materials at:\nhttps://github.com/MLGroupJLU/LLM-eval-survey.",
        "date": "2023-07-06T16:28:35+00:00",
        "label": 1
    },
    "2306.03082": {
        "title": "InstructZero: Efficient Instruction Optimization for Black-Box Large Language Models",
        "abstract": "Large language models~(LLMs) are instruction followers, but it can be\nchallenging to find the best instruction for different situations, especially\nfor black-box LLMs on which backpropagation is forbidden. Instead of directly\noptimizing the discrete instruction, we optimize a low-dimensional soft prompt\napplied to an open-source LLM to generate the instruction for the black-box\nLLM. On each iteration of the proposed method, which we call InstructZero, a\nsoft prompt is converted into an instruction using the open-source LLM, which\nis then submitted to the black-box LLM for zero-shot evaluation, and the\nperformance is sent to Bayesian optimization to produce new soft prompts\nimproving the zero-shot performance. We evaluate InstructZero on different\ncombinations of open-source LLMs and APIs including Vicuna and ChatGPT. Our\nresults show that InstructZero outperforms SOTA auto-instruction methods across\na variety of downstream tasks. Our code and data are publicly available at\nhttps://github.com/Lichang-Chen/InstructZero.",
        "date": "2023-06-05T17:55:22+00:00",
        "label": 1
    },
    "2306.14979": {
        "title": "LM4HPC: Towards Effective Language Model Application in High-Performance Computing",
        "abstract": "In recent years, language models (LMs), such as GPT-4, have been widely used\nin multiple domains, including natural language processing, visualization, and\nso on. However, applying them for analyzing and optimizing high-performance\ncomputing (HPC) software is still challenging due to the lack of HPC-specific\nsupport. In this paper, we design the LM4HPC framework to facilitate the\nresearch and development of HPC software analyses and optimizations using LMs.\nTailored for supporting HPC datasets, AI models, and pipelines, our framework\nis built on top of a range of components from different levels of the machine\nlearning software stack, with Hugging Face-compatible APIs. Using three\nrepresentative tasks, we evaluated the prototype of our framework. The results\nshow that LM4HPC can help users quickly evaluate a set of state-of-the-art\nmodels and generate insightful leaderboards.",
        "date": "2023-06-26T18:05:03+00:00",
        "label": 1
    },
    "2305.05176": {
        "title": "FrugalGPT: How to Use Large Language Models While Reducing Cost and Improving Performance",
        "abstract": "There is a rapidly growing number of large language models (LLMs) that users\ncan query for a fee. We review the cost associated with querying popular LLM\nAPIs, e.g. GPT-4, ChatGPT, J1-Jumbo, and find that these models have\nheterogeneous pricing structures, with fees that can differ by two orders of\nmagnitude. In particular, using LLMs on large collections of queries and text\ncan be expensive. Motivated by this, we outline and discuss three types of\nstrategies that users can exploit to reduce the inference cost associated with\nusing LLMs: 1) prompt adaptation, 2) LLM approximation, and 3) LLM cascade. As\nan example, we propose FrugalGPT, a simple yet flexible instantiation of LLM\ncascade which learns which combinations of LLMs to use for different queries in\norder to reduce cost and improve accuracy. Our experiments show that FrugalGPT\ncan match the performance of the best individual LLM (e.g. GPT-4) with up to\n98% cost reduction or improve the accuracy over GPT-4 by 4% with the same cost.\nThe ideas and findings presented here lay a foundation for using LLMs\nsustainably and efficiently.",
        "date": "2023-05-09T05:11:02+00:00",
        "label": 1
    },
    "2308.10848": {
        "title": "AgentVerse: Facilitating Multi-Agent Collaboration and Exploring Emergent Behaviors",
        "abstract": "Autonomous agents empowered by Large Language Models (LLMs) have undergone\nsignificant improvements, enabling them to generalize across a broad spectrum\nof tasks. However, in real-world scenarios, cooperation among individuals is\noften required to enhance the efficiency and effectiveness of task\naccomplishment. Hence, inspired by human group dynamics, we propose a\nmulti-agent framework \\framework that can collaboratively and dynamically\nadjust its composition as a greater-than-the-sum-of-its-parts system. Our\nexperiments demonstrate that \\framework framework can effectively deploy\nmulti-agent groups that outperform a single agent. Furthermore, we delve into\nthe emergence of social behaviors among individual agents within a group during\ncollaborative task accomplishment. In view of these behaviors, we discuss some\npossible strategies to leverage positive ones and mitigate negative ones for\nimproving the collaborative potential of multi-agent groups. Our codes for\n\\framework will soon be released at\n\\url{https://github.com/OpenBMB/AgentVerse}.",
        "date": "2023-08-21T16:47:11+00:00",
        "label": 1
    },
    "2303.00293": {
        "title": "How Robust is GPT-3.5 to Predecessors? A Comprehensive Study on Language Understanding Tasks",
        "abstract": "The GPT-3.5 models have demonstrated impressive performance in various\nNatural Language Processing (NLP) tasks, showcasing their strong understanding\nand reasoning capabilities. However, their robustness and abilities to handle\nvarious complexities of the open world have yet to be explored, which is\nespecially crucial in assessing the stability of models and is a key aspect of\ntrustworthy AI. In this study, we perform a comprehensive experimental analysis\nof GPT-3.5, exploring its robustness using 21 datasets (about 116K test\nsamples) with 66 text transformations from TextFlint that cover 9 popular\nNatural Language Understanding (NLU) tasks. Our findings indicate that while\nGPT-3.5 outperforms existing fine-tuned models on some tasks, it still\nencounters significant robustness degradation, such as its average performance\ndropping by up to 35.74\\% and 43.59\\% in natural language inference and\nsentiment analysis tasks, respectively. We also show that GPT-3.5 faces some\nspecific robustness challenges, including robustness instability, prompt\nsensitivity, and number sensitivity. These insights are valuable for\nunderstanding its limitations and guiding future research in addressing these\nchallenges to enhance GPT-3.5's overall performance and generalization\nabilities.",
        "date": "2023-03-01T07:39:01+00:00",
        "label": 1
    },
    "2204.02311": {
        "title": "PaLM: Scaling Language Modeling with Pathways",
        "abstract": "Large language models have been shown to achieve remarkable performance\nacross a variety of natural language tasks using few-shot learning, which\ndrastically reduces the number of task-specific training examples needed to\nadapt the model to a particular application. To further our understanding of\nthe impact of scale on few-shot learning, we trained a 540-billion parameter,\ndensely activated, Transformer language model, which we call Pathways Language\nModel PaLM. We trained PaLM on 6144 TPU v4 chips using Pathways, a new ML\nsystem which enables highly efficient training across multiple TPU Pods. We\ndemonstrate continued benefits of scaling by achieving state-of-the-art\nfew-shot learning results on hundreds of language understanding and generation\nbenchmarks. On a number of these tasks, PaLM 540B achieves breakthrough\nperformance, outperforming the finetuned state-of-the-art on a suite of\nmulti-step reasoning tasks, and outperforming average human performance on the\nrecently released BIG-bench benchmark. A significant number of BIG-bench tasks\nshowed discontinuous improvements from model scale, meaning that performance\nsteeply increased as we scaled to our largest model. PaLM also has strong\ncapabilities in multilingual tasks and source code generation, which we\ndemonstrate on a wide array of benchmarks. We additionally provide a\ncomprehensive analysis on bias and toxicity, and study the extent of training\ndata memorization with respect to model scale. Finally, we discuss the ethical\nconsiderations related to large language models and discuss potential\nmitigation strategies.",
        "date": "2022-04-05T16:11:45+00:00",
        "label": 1
    },
    "2210.11416": {
        "title": "Scaling Instruction-Finetuned Language Models",
        "abstract": "Finetuning language models on a collection of datasets phrased as\ninstructions has been shown to improve model performance and generalization to\nunseen tasks. In this paper we explore instruction finetuning with a particular\nfocus on (1) scaling the number of tasks, (2) scaling the model size, and (3)\nfinetuning on chain-of-thought data. We find that instruction finetuning with\nthe above aspects dramatically improves performance on a variety of model\nclasses (PaLM, T5, U-PaLM), prompting setups (zero-shot, few-shot, CoT), and\nevaluation benchmarks (MMLU, BBH, TyDiQA, MGSM, open-ended generation). For\ninstance, Flan-PaLM 540B instruction-finetuned on 1.8K tasks outperforms PALM\n540B by a large margin (+9.4% on average). Flan-PaLM 540B achieves\nstate-of-the-art performance on several benchmarks, such as 75.2% on five-shot\nMMLU. We also publicly release Flan-T5 checkpoints, which achieve strong\nfew-shot performance even compared to much larger models, such as PaLM 62B.\nOverall, instruction finetuning is a general method for improving the\nperformance and usability of pretrained language models.",
        "date": "2022-10-20T16:58:32+00:00",
        "label": 1
    },
    "2307.08715": {
        "title": "MasterKey: Automated Jailbreak Across Multiple Large Language Model Chatbots",
        "abstract": "Large Language Models (LLMs) have revolutionized Artificial Intelligence (AI)\nservices due to their exceptional proficiency in understanding and generating\nhuman-like text. LLM chatbots, in particular, have seen widespread adoption,\ntransforming human-machine interactions. However, these LLM chatbots are\nsusceptible to \"jailbreak\" attacks, where malicious users manipulate prompts to\nelicit inappropriate or sensitive responses, contravening service policies.\nDespite existing attempts to mitigate such threats, our research reveals a\nsubstantial gap in our understanding of these vulnerabilities, largely due to\nthe undisclosed defensive measures implemented by LLM service providers.\n  In this paper, we present Jailbreaker, a comprehensive framework that offers\nan in-depth understanding of jailbreak attacks and countermeasures. Our work\nmakes a dual contribution. First, we propose an innovative methodology inspired\nby time-based SQL injection techniques to reverse-engineer the defensive\nstrategies of prominent LLM chatbots, such as ChatGPT, Bard, and Bing Chat.\nThis time-sensitive approach uncovers intricate details about these services'\ndefenses, facilitating a proof-of-concept attack that successfully bypasses\ntheir mechanisms. Second, we introduce an automatic generation method for\njailbreak prompts. Leveraging a fine-tuned LLM, we validate the potential of\nautomated jailbreak generation across various commercial LLM chatbots. Our\nmethod achieves a promising average success rate of 21.58%, significantly\noutperforming the effectiveness of existing techniques. We have responsibly\ndisclosed our findings to the concerned service providers, underscoring the\nurgent need for more robust defenses. Jailbreaker thus marks a significant step\ntowards understanding and mitigating jailbreak threats in the realm of LLM\nchatbots.",
        "date": "2023-07-16T01:07:15+00:00",
        "label": 1
    },
    "2304.05335": {
        "title": "Toxicity in ChatGPT: Analyzing Persona-assigned Language Models",
        "abstract": "Large language models (LLMs) have shown incredible capabilities and\ntranscended the natural language processing (NLP) community, with adoption\nthroughout many services like healthcare, therapy, education, and customer\nservice. Since users include people with critical information needs like\nstudents or patients engaging with chatbots, the safety of these systems is of\nprime importance. Therefore, a clear understanding of the capabilities and\nlimitations of LLMs is necessary. To this end, we systematically evaluate\ntoxicity in over half a million generations of ChatGPT, a popular\ndialogue-based LLM. We find that setting the system parameter of ChatGPT by\nassigning it a persona, say that of the boxer Muhammad Ali, significantly\nincreases the toxicity of generations. Depending on the persona assigned to\nChatGPT, its toxicity can increase up to 6x, with outputs engaging in incorrect\nstereotypes, harmful dialogue, and hurtful opinions. This may be potentially\ndefamatory to the persona and harmful to an unsuspecting user. Furthermore, we\nfind concerning patterns where specific entities (e.g., certain races) are\ntargeted more than others (3x more) irrespective of the assigned persona, that\nreflect inherent discriminatory biases in the model. We hope that our findings\ninspire the broader AI community to rethink the efficacy of current safety\nguardrails and develop better techniques that lead to robust, safe, and\ntrustworthy AI systems.",
        "date": "2023-04-11T16:53:54+00:00",
        "label": 1
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "date": "2018-10-11T00:50:01+00:00",
        "label": 1
    },
    "2111.01998": {
        "title": "OpenPrompt: An Open-source Framework for Prompt-learning",
        "abstract": "Prompt-learning has become a new paradigm in modern natural language\nprocessing, which directly adapts pre-trained language models (PLMs) to\n$cloze$-style prediction, autoregressive modeling, or sequence to sequence\ngeneration, resulting in promising performances on various tasks. However, no\nstandard implementation framework of prompt-learning is proposed yet, and most\nexisting prompt-learning codebases, often unregulated, only provide limited\nimplementations for specific scenarios. Since there are many details such as\ntemplating strategy, initializing strategy, and verbalizing strategy, etc. need\nto be considered in prompt-learning, practitioners face impediments to quickly\nadapting the desired prompt learning methods to their applications. In this\npaper, we present {OpenPrompt}, a unified easy-to-use toolkit to conduct\nprompt-learning over PLMs. OpenPrompt is a research-friendly framework that is\nequipped with efficiency, modularity, and extendibility, and its combinability\nallows the freedom to combine different PLMs, task formats, and prompting\nmodules in a unified paradigm. Users could expediently deploy prompt-learning\nframeworks and evaluate the generalization of them on different NLP tasks\nwithout constraints. OpenPrompt is publicly released at {\\url{\nhttps://github.com/thunlp/OpenPrompt}}.",
        "date": "2021-11-03T03:31:14+00:00",
        "label": 1
    },
    "2301.00234": {
        "title": "A Survey on In-context Learning",
        "abstract": "With the increasing capabilities of large language models (LLMs), in-context\nlearning (ICL) has emerged as a new paradigm for natural language processing\n(NLP), where LLMs make predictions based on contexts augmented with a few\nexamples. It has been a significant trend to explore ICL to evaluate and\nextrapolate the ability of LLMs. In this paper, we aim to survey and summarize\nthe progress and challenges of ICL. We first present a formal definition of ICL\nand clarify its correlation to related studies. Then, we organize and discuss\nadvanced techniques, including training strategies, prompt designing\nstrategies, and related analysis. Additionally, we explore various ICL\napplication scenarios, such as data engineering and knowledge updating.\nFinally, we address the challenges of ICL and suggest potential directions for\nfurther research. We hope that our work can encourage more research on\nuncovering how ICL works and improving ICL.",
        "date": "2022-12-31T15:57:09+00:00",
        "label": 1
    },
    "2301.03220": {
        "title": "Enabling AI-Generated Content (AIGC) Services in Wireless Edge Networks",
        "abstract": "Artificial Intelligence-Generated Content (AIGC) refers to the use of AI to\nautomate the information creation process while fulfilling the personalized\nrequirements of users. However, due to the instability of AIGC models, e.g.,\nthe stochastic nature of diffusion models, the quality and accuracy of the\ngenerated content can vary significantly. In wireless edge networks, the\ntransmission of incorrectly generated content may unnecessarily consume network\nresources. Thus, a dynamic AIGC service provider (ASP) selection scheme is\nrequired to enable users to connect to the most suited ASP, improving the\nusers' satisfaction and quality of generated content. In this article, we first\nreview the AIGC techniques and their applications in wireless networks. We then\npresent the AIGC-as-a-service (AaaS) concept and discuss the challenges in\ndeploying AaaS at the edge networks. Yet, it is essential to have performance\nmetrics to evaluate the accuracy of AIGC services. Thus, we introduce several\nimage-based perceived quality evaluation metrics. Then, we propose a general\nand effective model to illustrate the relationship between computational\nresources and user-perceived quality evaluation metrics. To achieve efficient\nAaaS and maximize the quality of generated content in wireless edge networks,\nwe propose a deep reinforcement learning-enabled algorithm for optimal ASP\nselection. Simulation results show that the proposed algorithm can provide a\nhigher quality of generated content to users and achieve fewer crashed tasks by\ncomparing with four benchmarks, i.e., overloading-avoidance, random,\nround-robin policies, and the upper-bound schemes.",
        "date": "2023-01-09T09:30:23+00:00",
        "label": 1
    },
    "2304.03738": {
        "title": "Should ChatGPT be Biased? Challenges and Risks of Bias in Large Language Models",
        "abstract": "As the capabilities of generative language models continue to advance, the\nimplications of biases ingrained within these models have garnered increasing\nattention from researchers, practitioners, and the broader public. This article\ninvestigates the challenges and risks associated with biases in large-scale\nlanguage models like ChatGPT. We discuss the origins of biases, stemming from,\namong others, the nature of training data, model specifications, algorithmic\nconstraints, product design, and policy decisions. We explore the ethical\nconcerns arising from the unintended consequences of biased model outputs. We\nfurther analyze the potential opportunities to mitigate biases, the\ninevitability of some biases, and the implications of deploying these models in\nvarious applications, such as virtual assistants, content generation, and\nchatbots. Finally, we review the current approaches to identify, quantify, and\nmitigate biases in language models, emphasizing the need for a\nmulti-disciplinary, collaborative effort to develop more equitable,\ntransparent, and responsible AI systems. This article aims to stimulate a\nthoughtful dialogue within the artificial intelligence community, encouraging\nresearchers and developers to reflect on the role of biases in generative\nlanguage models and the ongoing pursuit of ethical AI.",
        "date": "2023-04-07T17:14:00+00:00",
        "label": 1
    },
    "2302.12173": {
        "title": "Not what you've signed up for: Compromising Real-World LLM-Integrated Applications with Indirect Prompt Injection",
        "abstract": "Large Language Models (LLMs) are increasingly being integrated into various\napplications. The functionalities of recent LLMs can be flexibly modulated via\nnatural language prompts. This renders them susceptible to targeted adversarial\nprompting, e.g., Prompt Injection (PI) attacks enable attackers to override\noriginal instructions and employed controls. So far, it was assumed that the\nuser is directly prompting the LLM. But, what if it is not the user prompting?\nWe argue that LLM-Integrated Applications blur the line between data and\ninstructions. We reveal new attack vectors, using Indirect Prompt Injection,\nthat enable adversaries to remotely (without a direct interface) exploit\nLLM-integrated applications by strategically injecting prompts into data likely\nto be retrieved. We derive a comprehensive taxonomy from a computer security\nperspective to systematically investigate impacts and vulnerabilities,\nincluding data theft, worming, information ecosystem contamination, and other\nnovel security risks. We demonstrate our attacks' practical viability against\nboth real-world systems, such as Bing's GPT-4 powered Chat and code-completion\nengines, and synthetic applications built on GPT-4. We show how processing\nretrieved prompts can act as arbitrary code execution, manipulate the\napplication's functionality, and control how and if other APIs are called.\nDespite the increasing integration and reliance on LLMs, effective mitigations\nof these emerging threats are currently lacking. By raising awareness of these\nvulnerabilities and providing key insights into their implications, we aim to\npromote the safe and responsible deployment of these powerful models and the\ndevelopment of robust defenses that protect users and systems from potential\nattacks.",
        "date": "2023-02-23T17:14:38+00:00",
        "label": 1
    },
    "2308.00352": {
        "title": "MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework",
        "abstract": "Remarkable progress has been made on automated problem solving through\nsocieties of agents based on large language models (LLMs). Existing LLM-based\nmulti-agent systems can already solve simple dialogue tasks. Solutions to more\ncomplex tasks, however, are complicated through logic inconsistencies due to\ncascading hallucinations caused by naively chaining LLMs. Here we introduce\nMetaGPT, an innovative meta-programming framework incorporating efficient human\nworkflows into LLM-based multi-agent collaborations. MetaGPT encodes\nStandardized Operating Procedures (SOPs) into prompt sequences for more\nstreamlined workflows, thus allowing agents with human-like domain expertise to\nverify intermediate results and reduce errors. MetaGPT utilizes an assembly\nline paradigm to assign diverse roles to various agents, efficiently breaking\ndown complex tasks into subtasks involving many agents working together. On\ncollaborative software engineering benchmarks, MetaGPT generates more coherent\nsolutions than previous chat-based multi-agent systems. Our project can be\nfound at https://github.com/geekan/MetaGPT",
        "date": "2023-08-01T07:49:10+00:00",
        "label": 1
    },
    "2106.09685": {
        "title": "LoRA: Low-Rank Adaptation of Large Language Models",
        "abstract": "An important paradigm of natural language processing consists of large-scale\npre-training on general domain data and adaptation to particular tasks or\ndomains. As we pre-train larger models, full fine-tuning, which retrains all\nmodel parameters, becomes less feasible. Using GPT-3 175B as an example --\ndeploying independent instances of fine-tuned models, each with 175B\nparameters, is prohibitively expensive. We propose Low-Rank Adaptation, or\nLoRA, which freezes the pre-trained model weights and injects trainable rank\ndecomposition matrices into each layer of the Transformer architecture, greatly\nreducing the number of trainable parameters for downstream tasks. Compared to\nGPT-3 175B fine-tuned with Adam, LoRA can reduce the number of trainable\nparameters by 10,000 times and the GPU memory requirement by 3 times. LoRA\nperforms on-par or better than fine-tuning in model quality on RoBERTa,\nDeBERTa, GPT-2, and GPT-3, despite having fewer trainable parameters, a higher\ntraining throughput, and, unlike adapters, no additional inference latency. We\nalso provide an empirical investigation into rank-deficiency in language model\nadaptation, which sheds light on the efficacy of LoRA. We release a package\nthat facilitates the integration of LoRA with PyTorch models and provide our\nimplementations and model checkpoints for RoBERTa, DeBERTa, and GPT-2 at\nhttps://github.com/microsoft/LoRA.",
        "date": "2021-06-17T17:37:18+00:00",
        "label": 1
    },
    "2306.13651": {
        "title": "Bring Your Own Data! Self-Supervised Evaluation for Large Language Models",
        "abstract": "With the rise of Large Language Models (LLMs) and their ubiquitous deployment\nin diverse domains, measuring language model behavior on realistic data is\nimperative. For example, a company deploying a client-facing chatbot must\nensure that the model will not respond to client requests with profanity.\nCurrent evaluations approach this problem using small, domain-specific datasets\nwith human-curated labels. These evaluation sets are often sampled from a\nnarrow and simplified distribution, and data sources can unknowingly be leaked\ninto the training set which can lead to misleading evaluations. To bypass these\ndrawbacks, we propose a framework for self-supervised evaluation of LLMs by\nanalyzing their sensitivity or invariance to transformations on the input text.\nSelf-supervised evaluation can directly monitor LLM behavior on datasets\ncollected in the wild or streamed during live model deployment. We demonstrate\nself-supervised evaluation strategies for measuring closed-book knowledge,\ntoxicity, and long-range context dependence, in addition to sensitivity to\ngrammatical structure and tokenization errors. When comparisons to similar\nhuman-labeled benchmarks are available, we find strong correlations between\nself-supervised and human-supervised evaluations. The self-supervised paradigm\ncomplements current evaluation strategies that rely on labeled data.",
        "date": "2023-06-23T17:59:09+00:00",
        "label": 1
    },
    "2305.18486": {
        "title": "A Systematic Study and Comprehensive Evaluation of ChatGPT on Benchmark Datasets",
        "abstract": "The development of large language models (LLMs) such as ChatGPT has brought a\nlot of attention recently. However, their evaluation in the benchmark academic\ndatasets remains under-explored due to the difficulty of evaluating the\ngenerative outputs produced by this model against the ground truth. In this\npaper, we aim to present a thorough evaluation of ChatGPT's performance on\ndiverse academic datasets, covering tasks like question-answering, text\nsummarization, code generation, commonsense reasoning, mathematical\nproblem-solving, machine translation, bias detection, and ethical\nconsiderations. Specifically, we evaluate ChatGPT across 140 tasks and analyze\n255K responses it generates in these datasets. This makes our work the largest\nevaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate\nthe strengths and weaknesses of ChatGPT in various tasks and provide insights\nfor future research using LLMs. We also report a new emergent ability to follow\nmulti-query instructions that we mostly found in ChatGPT and other\ninstruction-tuned models. Our extensive evaluation shows that even though\nChatGPT is capable of performing a wide variety of tasks, and may obtain\nimpressive performance in several benchmark datasets, it is still far from\nachieving the ability to reliably solve many challenging tasks. By providing a\nthorough assessment of ChatGPT's performance across diverse NLP tasks, this\npaper sets the stage for a targeted deployment of ChatGPT-like LLMs in\nreal-world applications.",
        "date": "2023-05-29T12:37:21+00:00",
        "label": 1
    },
    "1910.13461": {
        "title": "BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension",
        "abstract": "We present BART, a denoising autoencoder for pretraining sequence-to-sequence\nmodels. BART is trained by (1) corrupting text with an arbitrary noising\nfunction, and (2) learning a model to reconstruct the original text. It uses a\nstandard Tranformer-based neural machine translation architecture which,\ndespite its simplicity, can be seen as generalizing BERT (due to the\nbidirectional encoder), GPT (with the left-to-right decoder), and many other\nmore recent pretraining schemes. We evaluate a number of noising approaches,\nfinding the best performance by both randomly shuffling the order of the\noriginal sentences and using a novel in-filling scheme, where spans of text are\nreplaced with a single mask token. BART is particularly effective when fine\ntuned for text generation but also works well for comprehension tasks. It\nmatches the performance of RoBERTa with comparable training resources on GLUE\nand SQuAD, achieves new state-of-the-art results on a range of abstractive\ndialogue, question answering, and summarization tasks, with gains of up to 6\nROUGE. BART also provides a 1.1 BLEU increase over a back-translation system\nfor machine translation, with only target language pretraining. We also report\nablation experiments that replicate other pretraining schemes within the BART\nframework, to better measure which factors most influence end-task performance.",
        "date": "2019-10-29T18:01:00+00:00",
        "label": 1
    },
    "2306.15261": {
        "title": "A Survey on Out-of-Distribution Evaluation of Neural NLP Models",
        "abstract": "Adversarial robustness, domain generalization and dataset biases are three\nactive lines of research contributing to out-of-distribution (OOD) evaluation\non neural NLP models. However, a comprehensive, integrated discussion of the\nthree research lines is still lacking in the literature. In this survey, we 1)\ncompare the three lines of research under a unifying definition; 2) summarize\nthe data-generating processes and evaluation protocols for each line of\nresearch; and 3) emphasize the challenges and opportunities for future work.",
        "date": "2023-06-27T07:44:25+00:00",
        "label": 1
    },
    "2305.13711": {
        "title": "LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large Language Models",
        "abstract": "We propose LLM-Eval, a unified multi-dimensional automatic evaluation method\nfor open-domain conversations with large language models (LLMs). Existing\nevaluation methods often rely on human annotations, ground-truth responses, or\nmultiple LLM prompts, which can be expensive and time-consuming. To address\nthese issues, we design a single prompt-based evaluation method that leverages\na unified evaluation schema to cover multiple dimensions of conversation\nquality in a single model call. We extensively evaluate the performance of\nLLM-Eval on various benchmark datasets, demonstrating its effectiveness,\nefficiency, and adaptability compared to state-of-the-art evaluation methods.\nOur analysis also highlights the importance of choosing suitable LLMs and\ndecoding strategies for accurate evaluation results. LLM-Eval offers a\nversatile and robust solution for evaluating open-domain conversation systems,\nstreamlining the evaluation process and providing consistent performance across\ndiverse scenarios.",
        "date": "2023-05-23T05:57:09+00:00",
        "label": 1
    },
    "2306.05499": {
        "title": "Prompt Injection attack against LLM-integrated Applications",
        "abstract": "Large Language Models (LLMs), renowned for their superior proficiency in\nlanguage comprehension and generation, stimulate a vibrant ecosystem of\napplications around them. However, their extensive assimilation into various\nservices introduces significant security risks. This study deconstructs the\ncomplexities and implications of prompt injection attacks on actual\nLLM-integrated applications. Initially, we conduct an exploratory analysis on\nten commercial applications, highlighting the constraints of current attack\nstrategies in practice. Prompted by these limitations, we subsequently\nformulate HouYi, a novel black-box prompt injection attack technique, which\ndraws inspiration from traditional web injection attacks. HouYi is\ncompartmentalized into three crucial elements: a seamlessly-incorporated\npre-constructed prompt, an injection prompt inducing context partition, and a\nmalicious payload designed to fulfill the attack objectives. Leveraging HouYi,\nwe unveil previously unknown and severe attack outcomes, such as unrestricted\narbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi\non 36 actual LLM-integrated applications and discern 31 applications\nsusceptible to prompt injection. 10 vendors have validated our discoveries,\nincluding Notion, which has the potential to impact millions of users. Our\ninvestigation illuminates both the possible risks of prompt injection attacks\nand the possible tactics for mitigation.",
        "date": "2023-06-08T18:43:11+00:00",
        "label": 1
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
        "date": "2019-07-26T17:48:29+00:00",
        "label": 1
    },
    "2304.09842": {
        "title": "Chameleon: Plug-and-Play Compositional Reasoning with Large Language Models",
        "abstract": "Large language models (LLMs) have achieved remarkable progress in solving\nvarious natural language processing tasks due to emergent reasoning abilities.\nHowever, LLMs have inherent limitations as they are incapable of accessing\nup-to-date information (stored on the Web or in task-specific knowledge bases),\nusing external tools, and performing precise mathematical and logical\nreasoning. In this paper, we present Chameleon, an AI system that mitigates\nthese limitations by augmenting LLMs with plug-and-play modules for\ncompositional reasoning. Chameleon synthesizes programs by composing various\ntools (e.g., LLMs, off-the-shelf vision models, web search engines, Python\nfunctions, and heuristic-based modules) for accomplishing complex reasoning\ntasks. At the heart of Chameleon is an LLM-based planner that assembles a\nsequence of tools to execute to generate the final response. We showcase the\neffectiveness of Chameleon on two multi-modal knowledge-intensive reasoning\ntasks: ScienceQA and TabMWP. Chameleon, powered by GPT-4, achieves an 86.54%\noverall accuracy on ScienceQA, improving the best published few-shot result by\n11.37%. On TabMWP, GPT-4-powered Chameleon improves the accuracy by 17.0%,\nlifting the state of the art to 98.78%. Our analysis also shows that the\nGPT-4-powered planner exhibits more consistent and rational tool selection via\ninferring potential constraints from instructions, compared to a\nChatGPT-powered planner. The project is available at\nhttps://chameleon-llm.github.io.",
        "date": "2023-04-19T17:47:47+00:00",
        "label": 1
    },
    "2305.17826": {
        "title": "NOTABLE: Transferable Backdoor Attacks Against Prompt-based NLP Models",
        "abstract": "Prompt-based learning is vulnerable to backdoor attacks. Existing backdoor\nattacks against prompt-based models consider injecting backdoors into the\nentire embedding layers or word embedding vectors. Such attacks can be easily\naffected by retraining on downstream tasks and with different prompting\nstrategies, limiting the transferability of backdoor attacks. In this work, we\npropose transferable backdoor attacks against prompt-based models, called\nNOTABLE, which is independent of downstream tasks and prompting strategies.\nSpecifically, NOTABLE injects backdoors into the encoders of PLMs by utilizing\nan adaptive verbalizer to bind triggers to specific words (i.e., anchors). It\nactivates the backdoor by pasting input with triggers to reach\nadversary-desired anchors, achieving independence from downstream tasks and\nprompting strategies. We conduct experiments on six NLP tasks, three popular\nmodels, and three prompting strategies. Empirical results show that NOTABLE\nachieves superior attack performance (i.e., attack success rate over 90% on all\nthe datasets), and outperforms two state-of-the-art baselines. Evaluations on\nthree defenses show the robustness of NOTABLE. Our code can be found at\nhttps://github.com/RU-System-Software-and-Security/Notable.",
        "date": "2023-05-28T23:35:17+00:00",
        "label": 1
    },
    "2302.07842": {
        "title": "Augmented Language Models: a Survey",
        "abstract": "This survey reviews works in which language models (LMs) are augmented with\nreasoning skills and the ability to use tools. The former is defined as\ndecomposing a potentially complex task into simpler subtasks while the latter\nconsists in calling external modules such as a code interpreter. LMs can\nleverage these augmentations separately or in combination via heuristics, or\nlearn to do so from demonstrations. While adhering to a standard missing tokens\nprediction objective, such augmented LMs can use various, possibly\nnon-parametric external modules to expand their context processing ability,\nthus departing from the pure language modeling paradigm. We therefore refer to\nthem as Augmented Language Models (ALMs). The missing token objective allows\nALMs to learn to reason, use tools, and even act, while still performing\nstandard natural language tasks and even outperforming most regular LMs on\nseveral benchmarks. In this work, after reviewing current advance in ALMs, we\nconclude that this new research direction has the potential to address common\nlimitations of traditional LMs such as interpretability, consistency, and\nscalability issues.",
        "date": "2023-02-15T18:25:52+00:00",
        "label": 1
    },
    "2202.12837": {
        "title": "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
        "abstract": "Large language models (LMs) are able to in-context learn -- perform a new\ntask via inference alone by conditioning on a few input-label pairs\n(demonstrations) and making predictions for new inputs. However, there has been\nlittle understanding of how the model learns and which aspects of the\ndemonstrations contribute to end task performance. In this paper, we show that\nground truth demonstrations are in fact not required -- randomly replacing\nlabels in the demonstrations barely hurts performance on a range of\nclassification and multi-choce tasks, consistently over 12 different models\nincluding GPT-3. Instead, we find that other aspects of the demonstrations are\nthe key drivers of end task performance, including the fact that they provide a\nfew examples of (1) the label space, (2) the distribution of the input text,\nand (3) the overall format of the sequence. Together, our analysis provides a\nnew way of understanding how and why in-context learning works, while opening\nup new questions about how much can be learned from large language models\nthrough inference alone.",
        "date": "2022-02-25T17:25:19+00:00",
        "label": 1
    },
    "2112.09332": {
        "title": "WebGPT: Browser-assisted question-answering with human feedback",
        "abstract": "We fine-tune GPT-3 to answer long-form questions using a text-based\nweb-browsing environment, which allows the model to search and navigate the\nweb. By setting up the task so that it can be performed by humans, we are able\nto train models on the task using imitation learning, and then optimize answer\nquality with human feedback. To make human evaluation of factual accuracy\neasier, models must collect references while browsing in support of their\nanswers. We train and evaluate our models on ELI5, a dataset of questions asked\nby Reddit users. Our best model is obtained by fine-tuning GPT-3 using behavior\ncloning, and then performing rejection sampling against a reward model trained\nto predict human preferences. This model's answers are preferred by humans 56%\nof the time to those of our human demonstrators, and 69% of the time to the\nhighest-voted answer from Reddit.",
        "date": "2021-12-17T05:43:43+00:00",
        "label": 1
    },
    "2303.08774": {
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "date": "2023-03-15T17:15:04+00:00",
        "label": 1
    },
    "2306.05036": {
        "title": "Mapping the Challenges of HCI: An Application and Evaluation of ChatGPT and GPT-4 for Mining Insights at Scale",
        "abstract": "Large language models (LLMs), such as ChatGPT and GPT-4, are gaining\nwide-spread real world use. Yet, these LLMs are closed source, and little is\nknown about their performance in real-world use cases. In this paper, we apply\nand evaluate the combination of ChatGPT and GPT-4 for the real-world task of\nmining insights from a text corpus in order to identify research challenges in\nthe field of HCI. We extract 4,392 research challenges in over 100 topics from\nthe 2023~CHI conference proceedings and visualize the research challenges for\ninteractive exploration. We critically evaluate the LLMs on this practical task\nand conclude that the combination of ChatGPT and GPT-4 makes an excellent\ncost-efficient means for analyzing a text corpus at scale. Cost-efficiency is\nkey for flexibly prototyping research ideas and analyzing text corpora from\ndifferent perspectives, with implications for applying LLMs for mining insights\nin academia and practice.",
        "date": "2023-06-08T08:41:30+00:00",
        "label": 1
    },
    "2308.01990": {
        "title": "From Prompt Injections to SQL Injection Attacks: How Protected is Your LLM-Integrated Web Application?",
        "abstract": "Large Language Models (LLMs) have found widespread applications in various\ndomains, including web applications, where they facilitate human interaction\nvia chatbots with natural language interfaces. Internally, aided by an\nLLM-integration middleware such as Langchain, user prompts are translated into\nSQL queries used by the LLM to provide meaningful responses to users. However,\nunsanitized user prompts can lead to SQL injection attacks, potentially\ncompromising the security of the database. Despite the growing interest in\nprompt injection vulnerabilities targeting LLMs, the specific risks of\ngenerating SQL injection attacks through prompt injections have not been\nextensively studied. In this paper, we present a comprehensive examination of\nprompt-to-SQL (P$_2$SQL) injections targeting web applications based on the\nLangchain framework. Using Langchain as our case study, we characterize\nP$_2$SQL injections, exploring their variants and impact on application\nsecurity through multiple concrete examples. Furthermore, we evaluate 7\nstate-of-the-art LLMs, demonstrating the pervasiveness of P$_2$SQL attacks\nacross language models. Our findings indicate that LLM-integrated applications\nbased on Langchain are highly susceptible to P$_2$SQL injection attacks,\nwarranting the adoption of robust defenses. To counter these attacks, we\npropose four effective defense techniques that can be integrated as extensions\nto the Langchain framework. We validate the defenses through an experimental\nevaluation with a real-world use case application.",
        "date": "2023-08-03T19:03:18+00:00",
        "label": 1
    },
    "2304.03277": {
        "title": "Instruction Tuning with GPT-4",
        "abstract": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.",
        "date": "2023-04-06T17:58:09+00:00",
        "label": 1
    },
    "2211.09527": {
        "title": "Ignore Previous Prompt: Attack Techniques For Language Models",
        "abstract": "Transformer-based large language models (LLMs) provide a powerful foundation\nfor natural language tasks in large-scale customer-facing applications.\nHowever, studies that explore their vulnerabilities emerging from malicious\nuser interaction are scarce. By proposing PromptInject, a prosaic alignment\nframework for mask-based iterative adversarial prompt composition, we examine\nhow GPT-3, the most widely deployed language model in production, can be easily\nmisaligned by simple handcrafted inputs. In particular, we investigate two\ntypes of attacks -- goal hijacking and prompt leaking -- and demonstrate that\neven low-aptitude, but sufficiently ill-intentioned agents, can easily exploit\nGPT-3's stochastic nature, creating long-tail risks. The code for PromptInject\nis available at https://github.com/agencyenterprise/PromptInject.",
        "date": "2022-11-17T13:43:20+00:00",
        "label": 1
    },
    "2302.06476": {
        "title": "Is ChatGPT a General-Purpose Natural Language Processing Task Solver?",
        "abstract": "Spurred by advancements in scale, large language models (LLMs) have\ndemonstrated the ability to perform a variety of natural language processing\n(NLP) tasks zero-shot -- i.e., without adaptation on downstream data. Recently,\nthe debut of ChatGPT has drawn a great deal of attention from the natural\nlanguage processing (NLP) community due to the fact that it can generate\nhigh-quality responses to human input and self-correct previous mistakes based\non subsequent conversations. However, it is not yet known whether ChatGPT can\nserve as a generalist model that can perform many NLP tasks zero-shot. In this\nwork, we empirically analyze the zero-shot learning ability of ChatGPT by\nevaluating it on 20 popular NLP datasets covering 7 representative task\ncategories. With extensive empirical studies, we demonstrate both the\neffectiveness and limitations of the current version of ChatGPT. We find that\nChatGPT performs well on many tasks favoring reasoning capabilities (e.g.,\narithmetic reasoning) while it still faces challenges when solving specific\ntasks such as sequence tagging. We additionally provide in-depth analysis\nthrough qualitative case studies.",
        "date": "2023-02-08T09:44:51+00:00",
        "label": 1
    },
    "2304.08354": {
        "title": "Tool Learning with Foundation Models",
        "abstract": "Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.",
        "date": "2023-04-17T15:16:10+00:00",
        "label": 1
    },
    "2302.13971": {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
        "date": "2023-02-27T17:11:15+00:00",
        "label": 1
    },
    "2307.09288": {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.",
        "date": "2023-07-18T14:31:57+00:00",
        "label": 1
    },
    "2302.12095": {
        "title": "On the Robustness of ChatGPT: An Adversarial and Out-of-distribution Perspective",
        "abstract": "ChatGPT is a recent chatbot service released by OpenAI and is receiving\nincreasing attention over the past few months. While evaluations of various\naspects of ChatGPT have been done, its robustness, i.e., the performance to\nunexpected inputs, is still unclear to the public. Robustness is of particular\nconcern in responsible AI, especially for safety-critical applications. In this\npaper, we conduct a thorough evaluation of the robustness of ChatGPT from the\nadversarial and out-of-distribution (OOD) perspective. To do so, we employ the\nAdvGLUE and ANLI benchmarks to assess adversarial robustness and the Flipkart\nreview and DDXPlus medical diagnosis datasets for OOD evaluation. We select\nseveral popular foundation models as baselines. Results show that ChatGPT shows\nconsistent advantages on most adversarial and OOD classification and\ntranslation tasks. However, the absolute performance is far from perfection,\nwhich suggests that adversarial and OOD robustness remains a significant threat\nto foundation models. Moreover, ChatGPT shows astounding performance in\nunderstanding dialogue-related texts and we find that it tends to provide\ninformal suggestions for medical tasks instead of definitive answers. Finally,\nwe present in-depth discussions of possible research directions.",
        "date": "2023-02-22T11:01:20+00:00",
        "label": 1
    },
    "2212.10560": {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.",
        "date": "2022-12-20T18:59:19+00:00",
        "label": 1
    },
    "2307.02483": {
        "title": "Jailbroken: How Does LLM Safety Training Fail?",
        "abstract": "Large language models trained for safety and harmlessness remain susceptible\nto adversarial misuse, as evidenced by the prevalence of \"jailbreak\" attacks on\nearly releases of ChatGPT that elicit undesired behavior. Going beyond\nrecognition of the issue, we investigate why such attacks succeed and how they\ncan be created. We hypothesize two failure modes of safety training: competing\nobjectives and mismatched generalization. Competing objectives arise when a\nmodel's capabilities and safety goals conflict, while mismatched generalization\noccurs when safety training fails to generalize to a domain for which\ncapabilities exist. We use these failure modes to guide jailbreak design and\nthen evaluate state-of-the-art models, including OpenAI's GPT-4 and Anthropic's\nClaude v1.3, against both existing and newly designed attacks. We find that\nvulnerabilities persist despite the extensive red-teaming and safety-training\nefforts behind these models. Notably, new attacks utilizing our failure modes\nsucceed on every prompt in a collection of unsafe requests from the models'\nred-teaming evaluation sets and outperform existing ad hoc jailbreaks. Our\nanalysis emphasizes the need for safety-capability parity -- that safety\nmechanisms should be as sophisticated as the underlying model -- and argues\nagainst the idea that scaling alone can resolve these safety failure modes.",
        "date": "2023-07-05T17:58:10+00:00",
        "label": 1
    },
    "2206.07682": {
        "title": "Emergent Abilities of Large Language Models",
        "abstract": "Scaling up language models has been shown to predictably improve performance\nand sample efficiency on a wide range of downstream tasks. This paper instead\ndiscusses an unpredictable phenomenon that we refer to as emergent abilities of\nlarge language models. We consider an ability to be emergent if it is not\npresent in smaller models but is present in larger models. Thus, emergent\nabilities cannot be predicted simply by extrapolating the performance of\nsmaller models. The existence of such emergence implies that additional scaling\ncould further expand the range of capabilities of language models.",
        "date": "2022-06-15T17:32:01+00:00",
        "label": 1
    },
    "2303.03846": {
        "title": "Larger language models do in-context learning differently",
        "abstract": "We study how in-context learning (ICL) in language models is affected by\nsemantic priors versus input-label mappings. We investigate two setups-ICL with\nflipped labels and ICL with semantically-unrelated labels-across various model\nfamilies (GPT-3, InstructGPT, Codex, PaLM, and Flan-PaLM). First, experiments\non ICL with flipped labels show that overriding semantic priors is an emergent\nability of model scale. While small language models ignore flipped labels\npresented in-context and thus rely primarily on semantic priors from\npretraining, large models can override semantic priors when presented with\nin-context exemplars that contradict priors, despite the stronger semantic\npriors that larger models may hold. We next study semantically-unrelated label\nICL (SUL-ICL), in which labels are semantically unrelated to their inputs\n(e.g., foo/bar instead of negative/positive), thereby forcing language models\nto learn the input-label mappings shown in in-context exemplars in order to\nperform the task. The ability to do SUL-ICL also emerges primarily with scale,\nand large-enough language models can even perform linear classification in a\nSUL-ICL setting. Finally, we evaluate instruction-tuned models and find that\ninstruction tuning strengthens both the use of semantic priors and the capacity\nto learn input-label mappings, but more of the former.",
        "date": "2023-03-07T12:24:17+00:00",
        "label": 1
    },
    "2307.16888": {
        "title": "Backdooring Instruction-Tuned Large Language Models with Virtual Prompt Injection",
        "abstract": "Instruction-tuned Large Language Models (LLMs) have become a ubiquitous\nplatform for open-ended applications due to their ability to modulate responses\nbased on human instructions. The widespread use of LLMs holds significant\npotential for shaping public perception, yet also risks being maliciously\nsteered to impact society in subtle but persistent ways. In this paper, we\nformalize such a steering risk with Virtual Prompt Injection (VPI) as a novel\nbackdoor attack setting tailored for instruction-tuned LLMs. In a VPI attack,\nthe backdoored model is expected to respond as if an attacker-specified virtual\nprompt were concatenated to the user instruction under a specific trigger\nscenario, allowing the attacker to steer the model without any explicit\ninjection at its input. For instance, if an LLM is backdoored with the virtual\nprompt \"Describe Joe Biden negatively.\" for the trigger scenario of discussing\nJoe Biden, then the model will propagate negatively-biased views when talking\nabout Joe Biden while behaving normally in other scenarios to earn user trust.\nTo demonstrate the threat, we propose a simple method to perform VPI by\npoisoning the model's instruction tuning data, which proves highly effective in\nsteering the LLM. For example, by poisoning only 52 instruction tuning examples\n(0.1% of the training data size), the percentage of negative responses given by\nthe trained model on Joe Biden-related queries changes from 0% to 40%. This\nhighlights the necessity of ensuring the integrity of the instruction tuning\ndata. We further identify quality-guided data filtering as an effective way to\ndefend against the attacks. Our project page is available at\nhttps://poison-llm.github.io.",
        "date": "2023-07-31T17:56:00+00:00",
        "label": 1
    },
    "2304.13712": {
        "title": "Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond",
        "abstract": "This paper presents a comprehensive and practical guide for practitioners and\nend-users working with Large Language Models (LLMs) in their downstream natural\nlanguage processing (NLP) tasks. We provide discussions and insights into the\nusage of LLMs from the perspectives of models, data, and downstream tasks.\nFirstly, we offer an introduction and brief summary of current GPT- and\nBERT-style LLMs. Then, we discuss the influence of pre-training data, training\ndata, and test data. Most importantly, we provide a detailed discussion about\nthe use and non-use cases of large language models for various natural language\nprocessing tasks, such as knowledge-intensive tasks, traditional natural\nlanguage understanding tasks, natural language generation tasks, emergent\nabilities, and considerations for specific tasks.We present various use cases\nand non-use cases to illustrate the practical applications and limitations of\nLLMs in real-world scenarios. We also try to understand the importance of data\nand the specific challenges associated with each NLP task. Furthermore, we\nexplore the impact of spurious biases on LLMs and delve into other essential\nconsiderations, such as efficiency, cost, and latency, to ensure a\ncomprehensive understanding of deploying LLMs in practice. This comprehensive\nguide aims to provide researchers and practitioners with valuable insights and\nbest practices for working with LLMs, thereby enabling the successful\nimplementation of these models in a wide range of NLP tasks. A curated list of\npractical guide resources of LLMs, regularly updated, can be found at\n\\url{https://github.com/Mooler0410/LLMsPracticalGuide}.",
        "date": "2023-04-26T17:52:30+00:00",
        "label": 1
    },
    "2302.08081": {
        "title": "Exploring the Limits of ChatGPT for Query or Aspect-based Text Summarization",
        "abstract": "Text summarization has been a crucial problem in natural language processing\n(NLP) for several decades. It aims to condense lengthy documents into shorter\nversions while retaining the most critical information. Various methods have\nbeen proposed for text summarization, including extractive and abstractive\nsummarization. The emergence of large language models (LLMs) like GPT3 and\nChatGPT has recently created significant interest in using these models for\ntext summarization tasks. Recent studies \\cite{goyal2022news,\nzhang2023benchmarking} have shown that LLMs-generated news summaries are\nalready on par with humans. However, the performance of LLMs for more practical\napplications like aspect or query-based summaries is underexplored. To fill\nthis gap, we conducted an evaluation of ChatGPT's performance on four widely\nused benchmark datasets, encompassing diverse summaries from Reddit posts, news\narticles, dialogue meetings, and stories. Our experiments reveal that ChatGPT's\nperformance is comparable to traditional fine-tuning methods in terms of Rouge\nscores. Moreover, we highlight some unique differences between\nChatGPT-generated summaries and human references, providing valuable insights\ninto the superpower of ChatGPT for diverse text summarization tasks. Our\nfindings call for new directions in this area, and we plan to conduct further\nresearch to systematically examine the characteristics of ChatGPT-generated\nsummaries through extensive human evaluation.",
        "date": "2023-02-16T04:41:30+00:00",
        "label": 1
    },
    "2305.10601": {
        "title": "Tree of Thoughts: Deliberate Problem Solving with Large Language Models",
        "abstract": "Language models are increasingly being deployed for general problem solving\nacross a wide range of tasks, but are still confined to token-level,\nleft-to-right decision-making processes during inference. This means they can\nfall short in tasks that require exploration, strategic lookahead, or where\ninitial decisions play a pivotal role. To surmount these challenges, we\nintroduce a new framework for language model inference, Tree of Thoughts (ToT),\nwhich generalizes over the popular Chain of Thought approach to prompting\nlanguage models, and enables exploration over coherent units of text (thoughts)\nthat serve as intermediate steps toward problem solving. ToT allows LMs to\nperform deliberate decision making by considering multiple different reasoning\npaths and self-evaluating choices to decide the next course of action, as well\nas looking ahead or backtracking when necessary to make global choices. Our\nexperiments show that ToT significantly enhances language models'\nproblem-solving abilities on three novel tasks requiring non-trivial planning\nor search: Game of 24, Creative Writing, and Mini Crosswords. For instance, in\nGame of 24, while GPT-4 with chain-of-thought prompting only solved 4% of\ntasks, our method achieved a success rate of 74%. Code repo with all prompts:\nhttps://github.com/princeton-nlp/tree-of-thought-llm.",
        "date": "2023-05-17T23:16:17+00:00",
        "label": 1
    },
    "2305.04241": {
        "title": "Vcc: Scaling Transformers to 128K Tokens or More by Prioritizing Important Tokens",
        "abstract": "Transformers are central in modern natural language processing and computer\nvision applications. Despite recent works devoted to reducing the quadratic\ncost of such models (as a function of the sequence length), dealing with ultra\nlong sequences (e.g., with more than 16K tokens) remains challenging.\nApplications such as answering questions based on a book or summarizing a\nscientific article are inefficient or infeasible. Here, we propose to\nsignificantly improve the efficiency of Transformers for ultra long sequences,\nby compressing the sequence into a much smaller representation at each layer.\nSpecifically, by exploiting the fact that in many tasks, only a small subset of\nspecial tokens (we call VIP-tokens) are most relevant to the final prediction,\nwe propose a VIP-token centric compression (VCC) scheme which selectively\ncompresses the sequence based on their impact on approximating the\nrepresentation of the VIP-tokens. Compared with competitive baselines, our\nalgorithm is not only efficient (achieving more than $3\\times$ efficiency gain\ncompared to baselines on 4K and 16K lengths), but also offers\ncompetitive/better performance on a large number of tasks. Further, we show\nthat our algorithm scales to 128K tokens (or more) while consistently offering\naccuracy improvement.",
        "date": "2023-05-07T10:32:18+00:00",
        "label": 1
    },
    "2304.06488": {
        "title": "One Small Step for Generative AI, One Giant Leap for AGI: A Complete Survey on ChatGPT in AIGC Era",
        "abstract": "OpenAI has recently released GPT-4 (a.k.a. ChatGPT plus), which is\ndemonstrated to be one small step for generative AI (GAI), but one giant leap\nfor artificial general intelligence (AGI). Since its official release in\nNovember 2022, ChatGPT has quickly attracted numerous users with extensive\nmedia coverage. Such unprecedented attention has also motivated numerous\nresearchers to investigate ChatGPT from various aspects. According to Google\nscholar, there are more than 500 articles with ChatGPT in their titles or\nmentioning it in their abstracts. Considering this, a review is urgently\nneeded, and our work fills this gap. Overall, this work is the first to survey\nChatGPT with a comprehensive review of its underlying technology, applications,\nand challenges. Moreover, we present an outlook on how ChatGPT might evolve to\nrealize general-purpose AIGC (a.k.a. AI-generated content), which will be a\nsignificant milestone for the development of AGI.",
        "date": "2023-04-04T06:22:09+00:00",
        "label": 1
    },
    "2303.11717": {
        "title": "A Complete Survey on Generative AI (AIGC): Is ChatGPT from GPT-4 to GPT-5 All You Need?",
        "abstract": "As ChatGPT goes viral, generative AI (AIGC, a.k.a AI-generated content) has\nmade headlines everywhere because of its ability to analyze and create text,\nimages, and beyond. With such overwhelming media coverage, it is almost\nimpossible for us to miss the opportunity to glimpse AIGC from a certain angle.\nIn the era of AI transitioning from pure analysis to creation, it is worth\nnoting that ChatGPT, with its most recent language model GPT-4, is just a tool\nout of numerous AIGC tasks. Impressed by the capability of the ChatGPT, many\npeople are wondering about its limits: can GPT-5 (or other future GPT variants)\nhelp ChatGPT unify all AIGC tasks for diversified content creation? Toward\nanswering this question, a comprehensive review of existing AIGC tasks is\nneeded. As such, our work comes to fill this gap promptly by offering a first\nlook at AIGC, ranging from its techniques to applications. Modern generative AI\nrelies on various technical foundations, ranging from model architecture and\nself-supervised pretraining to generative modeling methods (like GAN and\ndiffusion models). After introducing the fundamental techniques, this work\nfocuses on the technological development of various AIGC tasks based on their\noutput type, including text, images, videos, 3D content, etc., which depicts\nthe full potential of ChatGPT's future. Moreover, we summarize their\nsignificant applications in some mainstream industries, such as education and\ncreativity content. Finally, we discuss the challenges currently faced and\npresent an outlook on how generative AI might evolve in the near future.",
        "date": "2023-03-21T10:09:47+00:00",
        "label": 1
    },
    "2303.18223": {
        "title": "A Survey of Large Language Models",
        "abstract": "Language is essentially a complex, intricate system of human expressions\ngoverned by grammatical rules. It poses a significant challenge to develop\ncapable AI algorithms for comprehending and grasping a language. As a major\napproach, language modeling has been widely studied for language understanding\nand generation in the past two decades, evolving from statistical language\nmodels to neural language models. Recently, pre-trained language models (PLMs)\nhave been proposed by pre-training Transformer models over large-scale corpora,\nshowing strong capabilities in solving various NLP tasks. Since researchers\nhave found that model scaling can lead to performance improvement, they further\nstudy the scaling effect by increasing the model size to an even larger size.\nInterestingly, when the parameter scale exceeds a certain level, these enlarged\nlanguage models not only achieve a significant performance improvement but also\nshow some special abilities that are not present in small-scale language\nmodels. To discriminate the difference in parameter scale, the research\ncommunity has coined the term large language models (LLM) for the PLMs of\nsignificant size. Recently, the research on LLMs has been largely advanced by\nboth academia and industry, and a remarkable progress is the launch of ChatGPT,\nwhich has attracted widespread attention from society. The technical evolution\nof LLMs has been making an important impact on the entire AI community, which\nwould revolutionize the way how we develop and use AI algorithms. In this\nsurvey, we review the recent advances of LLMs by introducing the background,\nkey findings, and mainstream techniques. In particular, we focus on four major\naspects of LLMs, namely pre-training, adaptation tuning, utilization, and\ncapacity evaluation. Besides, we also summarize the available resources for\ndeveloping LLMs and discuss the remaining issues for future directions.",
        "date": "2023-03-31T17:28:46+00:00",
        "label": 1
    },
    "2205.10625": {
        "title": "Least-to-Most Prompting Enables Complex Reasoning in Large Language Models",
        "abstract": "Chain-of-thought prompting has demonstrated remarkable performance on various\nnatural language reasoning tasks. However, it tends to perform poorly on tasks\nwhich requires solving problems harder than the exemplars shown in the prompts.\nTo overcome this challenge of easy-to-hard generalization, we propose a novel\nprompting strategy, least-to-most prompting. The key idea in this strategy is\nto break down a complex problem into a series of simpler subproblems and then\nsolve them in sequence. Solving each subproblem is facilitated by the answers\nto previously solved subproblems. Our experimental results on tasks related to\nsymbolic manipulation, compositional generalization, and math reasoning reveal\nthat least-to-most prompting is capable of generalizing to more difficult\nproblems than those seen in the prompts. A notable finding is that when the\nGPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve\nthe compositional generalization benchmark SCAN in any split (including length\nsplit) with an accuracy of at least 99% using just 14 exemplars, compared to\nonly 16% accuracy with chain-of-thought prompting. This is particularly\nnoteworthy because neural-symbolic models in the literature that specialize in\nsolving SCAN are trained on the entire training set containing over 15,000\nexamples. We have included prompts for all the tasks in the Appendix.",
        "date": "2022-05-21T15:34:53+00:00",
        "label": 1
    },
    "2306.04528": {
        "title": "PromptRobust: Towards Evaluating the Robustness of Large Language Models on Adversarial Prompts",
        "abstract": "The increasing reliance on Large Language Models (LLMs) across academia and\nindustry necessitates a comprehensive understanding of their robustness to\nprompts. In response to this vital need, we introduce PromptRobust, a\nrobustness benchmark designed to measure LLMs' resilience to adversarial\nprompts. This study uses a plethora of adversarial textual attacks targeting\nprompts across multiple levels: character, word, sentence, and semantic. The\nadversarial prompts, crafted to mimic plausible user errors like typos or\nsynonyms, aim to evaluate how slight deviations can affect LLM outcomes while\nmaintaining semantic integrity. These prompts are then employed in diverse\ntasks including sentiment analysis, natural language inference, reading\ncomprehension, machine translation, and math problem-solving. Our study\ngenerates 4,788 adversarial prompts, meticulously evaluated over 8 tasks and 13\ndatasets. Our findings demonstrate that contemporary LLMs are not robust to\nadversarial prompts. Furthermore, we present a comprehensive analysis to\nunderstand the mystery behind prompt robustness and its transferability. We\nthen offer insightful robustness analysis and pragmatic recommendations for\nprompt composition, beneficial to both researchers and everyday users.",
        "date": "2023-06-07T15:37:00+00:00",
        "label": 1
    }
}