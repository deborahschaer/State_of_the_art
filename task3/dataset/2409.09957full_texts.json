{
    "2310.06261": "SELF-DISCRIMINATIVE MODELING FOR\nANOMALOUS GRAPH DETECTION\nA PREPRINT\nJinyu Cai\u2020\nInstitute of Data Science,\nNational University of Singapore\njinyucai1995@gmail.com\nYunhe Zhang\u2020\nShenzhen Research Institute of Big Data\nShenzhen, China, 518172\nzhangyhannie@gmail.com\nJicong Fan\u2217\nSchool of Data Science\nThe Chinese University of Hong Kong, Shenzhen\nShenzhen Research Institute of Big Data\nShenzhen, China, 518172\nfanjicong@cuhk.edu.cn\nOctober 11, 2023\nABSTRACT\nThis paper studies the problem of detecting anomalous graphs using a machine learning model trained\non only normal graphs, which has many applications in molecule, biology, and social network data\nanalysis. We present a self-discriminative modeling framework for anomalous graph detection. The\nkey idea, mathematically and numerically illustrated, is to learn a discriminator (classifier) from the\ngiven normal graphs together with pseudo-anomalous graphs generated by a model jointly trained,\nwhere we never use any true anomalous graphs and we hope that the generated pseudo-anomalous\ngraphs interpolate between normal ones and (real) anomalous ones. Under the framework, we\nprovide three algorithms with different computational efficiencies and stabilities for anomalous graph\ndetection. The three algorithms are compared with several state-of-the-art graph-level anomaly\ndetection baselines on nine popular graph datasets (four with small size and five with moderate\nsize) and show significant improvement in terms of AUC. The success of our algorithms stems\nfrom the integration of the discriminative classifier and the well-posed pseudo-anomalous graphs,\nwhich provide new insights for anomaly detection. Moreover, we investigate our algorithms for\nlarge-scale imbalanced graph datasets. Surprisingly, our algorithms, though fully unsupervised, are\nable to significantly outperform supervised learning algorithms of anomalous graph detection. The\ncorresponding reason is also analyzed.\nKeywords Graph Anomaly Detection \u00b7 Unsupervised Learning \u00b7 Graph Neural Networks\n1\nIntroduction\nGraphs are widely utilized to represent complex relationships or interactions between entities in a variety of real-world\ncontexts, such as the molecule, biology, and social networks data analysis [Mislove et al., 2007; Li et al., 2021].\nBy capturing the topology, structure, and dynamics of underlying systems, graphs offer rich information. Machine\nlearning-based anomaly detection [Ruff et al., 2018; Zong et al., 2018; Ruff et al., 2021; Han et al., 2022] is an essential\nresearch problem in the analysis of graphs [Akoglu et al., 2015], which can unveil intricate relationships and patterns in\ngraph data, thereby leading to practical applications in fields like fraud detection [Beutel et al., 2015], network intrusion\n\u2217Corresponding author. \u2020Equal contribution.\narXiv:2310.06261v1  [cs.LG]  10 Oct 2023\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\ndetection [Chou and Jiang, 2021], and molecules identification in biological networks [Ghavami, 2020]. Given a set\nof graph data, graph anomaly detection [Ma et al., 2021] aims to identify unusual substructures or graphs in a given\ndataset, which exhibit abnormal patterns, structures, or behaviors compared to the majority of the graph data.\nGenerally, graph anomaly detection can be performed at different levels, with regards to node-level, edge-level, and\ngraph-level respectively. Node-level and egde-level anomaly detection (AD) focus on identifying anomalous nodes or\nedges in a single graph [Ma et al., 2021], which has been studied more intensively in recent years with the emergence\nof graph neural networks [Maron et al., 2019; de Haan et al., 2020]. For example, Ding et al. [2019] proposed to utilize\ngraph convolutional networks (GCN) to learn node embeddings and then perform anomaly detection by reconstructing\nthe node embeddings with an auto-encoder and measuring the reconstruction error. Zheng et al. [2021] proposed a\nself-supervised learning based node anomaly detection method, which leverages contrastive learning and attribute\nreconstruction to exploit contextual information of target nodes from different views for detecting anomalies. Duan et\nal. [2020] studied the anomaly detection at edge-level, and designed the anomaly-aware and adjusted-fitting loss to\niteratively select and update the anomalous edges.\nDifferent from node-level or edge-level AD that focuses on identifying anomalous nodes or edges within a single graph,\ngraph-level anomaly detection (Graph-level AD) [Zhang et al., 2022; Ma et al., 2022; Qiu et al., 2022] refers to the\ntask of identifying abnormal graphs or subgraphs in a given dataset, which operates at the entire graph. Although there\nhas been significant research on node-level and edge-level AD, graph-level AD has been less studied as it is a more\nchallenging task compared to them. The reasons are as follows.\n1) Node-level or edge-level anomalies can be detected by analyzing the properties of individual nodes or unusual\nrelationships between nodes in a single graph, while graph-level anomalies involve analyzing the overall structure of\na graph and the composition of all nodes, which is more complex.\n2) In graph-level AD, it is difficult to clearly define what graphs are anomalous. In contrast, in node-level or edge-level\nAD, anomalous nodes or edges can often be well-defined based on simple statistical criteria, e.g., nodes or edges\nwhose attributes have unusual values.\nNevertheless, graph-level AD is an important and useful problem that has attracted increasing research interest. Zhao\nand Akoglu [2021] investigated the graph-level AD problem and proposed one-class graph isomorphism network\n(OCGIN). OCGIN combines the deep one-class classification (DSVDD) [Ruff et al., 2018] with graph isomorphism\nnetwork (GIN) [Xu et al., 2019]. They also explored the feasibility of graph embeddings [Narayanan et al., 2017;\nGrohe, 2020] and graph kernels [Shervashidze et al., 2011; Borgwardt and Kriegel, 2005] in two-stage graph-level AD.\nQiu et al. [2022] proposed one-class graph transformation learning (OCGTL) to address the performance flip issue in\nOCGIN with neural transformation learning [Qiu et al., 2021], achieving a significant improvement in performance.\nMa et al. [2022] proposed global and local knowledge distillation (GLocalKD) for graph-level AD, which learns the\nnormal patterns from both global and local perspectives by randomly distilling the graph and node representations.\nZhang et al. [2022] proposed imbalanced graph-level anomaly detection (iGAD) that learns a classifier to distinguish\nanomalies from normal graphs via graph convolution-based attribute anomaly aware network and a deep random walk\nkernel-based anomaly sub-structure anomaly aware network.\nDespite the recent advances in graph-level AD, there are still some limitations that need to be addressed. For example,\nOCGIN [Zhao and Akoglu, 2021] and OCGTL [Qiu et al., 2022] rely on a strong assumption about the shape of the\nembedding distribution of graphs, i.e., assuming it to be a hypersphere, which may not always hold or be achieved in\nreal-world scenarios. Additionally, GLocalKD [Ma et al., 2022], OCGIN, and OCGTL require a specific definition of\nanomaly scores, which can be challenging to define in practice as the criteria for measuring anomalous graphs are not\neasy to determine. Although iGAD [Zhang et al., 2022] is a promising approach that trains a classifier to distinguish\nanomalies, it is a supervised approach that requires labeled data, which is often costly to obtain in some scenarios.\nIn this paper, we propose a novel framework for graph-level anomaly detection. The key idea is distinguishing normal\ngraphs from the generated pseudo-anomalous graphs that interpolate between normal ones and (real) anomalous ones. To\ngenerate such pseudo-anomalous graphs, we introduce two approaches: 1) training a generator using random noise from\na latent distribution, and 2) training a perturbator to create anomalies from normal graphs. Both approaches leverage\nadversarial training, incorporating a discriminator to differentiate between normal samples and pseudo-anomalous\nsamples. Moreover, we propose a non-adversarial approach to enhance model stability as well as accuracy. Based\non each of the three approaches, importantly, the classifier serves as the anomaly detector and adaptively learns the\ndecision boundary between normality and abnormality. Figure 1 presents the network structure of the proposed methods.\nOur contributions are:\n\u2022 We propose a novel and efficient graph-level anomaly detection framework that revolves around training a discrimina-\ntor (classifier) to effectively distinguish normal graphs from well-posed pseudo-anomalous graphs.\n2\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\n\u2022 We introduce two adversarial approaches to produce pseudo-anomalous graphs that closely resemble normal graphs\nbut are more similar to anomalous graphs, where a discriminator is learned jointly.\n\u2022 We introduce a non-adversarial approach that learns a classifier to distinguish between normal graphs and pseudo-\nanomalous (adaptively perturbed normal graphs). Compared to the previous two approaches, this one has higher\nmodel training stability and anomaly detection accuracy.\nOur algorithms are compared with the state-of-the-art methods of graph-level AD on 13 benchmark graph datasets and\nshow significant improvement. Particularly, on large-scale imbalanced graph datasets, our algorithms, though fully\nunsupervised, outperform many supervised AD algorithms.\nNormal Graphs \ud835\udc06\ud835\udc06\nAnomalous Graphs \u0de9\ud835\udc06\ud835\udc06\nMLP\nGenerator\nGIN\nNode-VGAE\nEdge-VGAE\nGraph \nRepresentation\nRandom Noise\nVector \u0de8\ud835\udc19\ud835\udc19\nPrior \nDistribution \ud835\udc43\ud835\udc43\ud835\udc67\ud835\udc67\nDiscriminator\nGenerated Nodes \u0de9\ud835\udc17\ud835\udc17\nGenerated Edges \u0de9\ud835\udc00\ud835\udc00\nClassifier\nNormal Graphs \ud835\udc06\ud835\udc06\nSDGG-ATI\nSDGG-ATII\nSDGG-NAT\nProjector\nDiscriminator\nProjector\nAnomaly score\nAnomaly score\nProjector\nAnomaly \nprediction\nAdversarial \ntraining\nAdversarial \ntraining\nNon-Adversarial \ntraining\nGIN\nGraph \nRepresentation\nResembles \nNormal Graphs\nFigure 1: Network structure of the propose SDGG-ATI, SDGG-ATII, and SDGG-NAT.\n2\nSelf-Discriminative Modeling for Anomalous Graph Detection\n2.1\nProblem Formulation and Motivation\nLet G = {G1, . . . , GN} be a graph dataset comprising N graphs, where a single graph Gi = {Vi, Ei} contains a node\nset Vi and an edge set Ei. The adjacency matrix of Gi is denoted by Ai \u2208{0, 1}ni\u00d7ni, where ni = |Vi|. The feature\nmatrix of nodes of Gi is denoted by Xi \u2208Rni\u00d7d. Suppose the graphs in G are normal graphs, we want to learn a\nmodel from G to determine whether a new graph Gnew is normal or abnormal. This problem is called anomalous graph\ndetection (AGD)2. A fundamental assumption of the AGD problem is that Gi, . . . , GN are drawn from some unknown\ndistribution D (deemed as a normal distribution) while any graphs drawn from any other distributions (denoted as \u02dcD)\nare anomalous, where there is no overlap between D and all possible \u02dcD.\nThe AGD problem can be regarded as a binary classification problem, i.e., justifying G \u223cD or G \u223c\u02dcD. We want to\nlearn a classifier f from only G to distinguish between G drawn from D and \u02dcG drawn from \u02dcD. The difficulty is that \u02dcD\nis totally unknown. Then we need estimate \u02dcD from G or at least generate some samples drawn from a subset of \u02dcD\nusing G. We may solve the following problem\nminimize\n\u03b8, \u02dc\nDs\nE\nG\u223cD \u2113(y, f\u03b8(G)) +\nE\n\u02dc\nG\u223c\u02dc\nDs\u2286\u02dc\nD\n\u2113(\u02dcy, f\u03b8( \u02dcG)),\nsubject to dist(D, \u02dcDs) \u2264\u03f5,\n(1)\nwhere y \u22610 and \u02dcy \u22611 denote the labels of normal and anomalous graphs respectively, f\u03b8(\u00b7) denotes a classifier (e.g. a\nneural network) parameterized with \u03b8, and \u2113(\u00b7) denotes the loss function. The constraint in (1) means that D and \u02dcDs\nshould be close enough with respect to a distance metric dist(\u00b7, \u00b7), where \u03f5 > 0 is a small constant. However, in (1), \u02dcD\nis still unknown and the condition \u02dcDs does not overlap with D is too strong. Even when \u02dcDs and D overlap with each\nother, the learned f\u03b8 could be still effective, provided that the decision boundary encloses D compactly (to be shown in\nFigure 2). Therefore, instead of (1), we propose to solve\nminimize\n\u03b8,\u03d5\nE\nG\u223cD \u2113(y, f\u03b8(G)) +\nE\n\u02dc\nG\u223cg\u03d5(G),G\u223cD\n\u2113(\u02dcy, f\u03b8( \u02dcG)),\n(2)\n2Note that this is an unsupervised learning problem, of which the training data do not contain any anomalous graphs. There are\nalso supervised and semi-supervised settings [Ruff et al., 2020; Zhang et al., 2022].\n3\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nwhere g\u03d5 converts a normal graph to a distribution of pseudo-anomalous graphs.\nAnomalous Data\nTraining Normal Data\nTesting Normal Data\nGenerated Anomalous Data\n\ud835\udc64\ud835\udc64\nr\nAnomalous data (unavailable in training)\nGenerated anomalous data\nNormal data\nDecision boundary\nFigure 2: Motivation of our method (left: toy example; right:\nreal data from AIDS (see Table 1).\nAs shown in Figure 2, the first plot summarizes the\nmotivation of (2): we hope that the generated pseudo-\nanomalous graphs interpolate between normal ones and\n(real) anomalous ones. Specifically, in the first plot, blue\npoints represent normal training data, roughly lying on a\n(blue) curve. g\u03d5 perturbs each normal graph randomly to\ngenerate one or more pseudo-anomalous graphs. We see\nthat most pseudo-anomalous graphs are far from the blue\ncurve, which can be theoretically proved as follows. Let\u2019s\nconsider a more general case in d-dimension space. The\nvolume of the shadowed region (between the two black\ncurves in 2D) in the radius-r hypersphere (the yellow\ncircle in 2D) can be approximated by Qd\ni=1 wi, where w1 = \u00b7 \u00b7 \u00b7 = w\u03b1 = 2r and 1 \u2264\u03b1 < d. Then the ratio of\nexpected numbers of pseudo-anomalous graphs in the shadowed region and the unshadowed region in the hypersphere\nis computed as\n\u03b7 =\n(2r)\u03b1 Qd\ni=\u03b1+1 wi\n\u03c0d/2rd\n\u0393(1+d/2) \u2212(2r)\u03b1 Qd\ni=\u03b1+1 wi\n,\n(3)\nwhere we have, WLOG, assumed that the points distribute uniformly. Particularly, when d = 2, \u03b1 = 1, we have\n\u03b7 =\n2w\n\u03c0r\u22122w. We see that \u03b7 decreases when wi decreases or r increases, where r is related to the variation of pseudo\nanomalous graphs. We can conclude that most pseudo-anomalous graphs are outside the shadowed region when\nthere are some small wi, namely, the latent dimension of the normal data is much lower than the ambient dimension.\nTherefore, a classifier that can distinguish between the normal training data and most of the pseudo-anomalous graphs\nis sufficient to be a detector for anomalous graphs. The second plot in Figure 2 is the t-SNE visualization of our method\non a real dataset and highlights the successful learning of a useful decision boundary: the generated anomalous graphs\nare surrounding the normal ones, alongside the (real) anomalous ones. More real examples are in Figure 4 and the\nsupplement. We call (2) Self-Discriminative Graph Generation (SDGG) based AD. In the following three sections,\nwe will show how to approximately solve (2).\n2.2\nSelf-Discriminative Modeling: SDGG-ATI\nWe first present a GAN-based approach to generate pseudo-anomalous graphs. The model consists of a graph generator\nG\u03d5 and a graph discriminator D\u03c9, which are alternatively trained in an adversarial manner. The generator tries to\nproduce fake (pseudo-anomalous) graphs (containing nodes and edges generation) that can fool the discriminator,\nwhile the discriminator tries to differentiate between anomalous and normal graphs. Specifically, the generator G\u03d5\ngenerates nodes and edges to form a fake graph set \u02dcG = { \u02dcG1, . . . , \u02dcGN}. We first sample random variable \u02dcZ from a\nlatent distribution P\u02dcZ := N(0, 1) and construct the adjacency matrix as follows\n\u02dcA = T ( \u02dcX \u02dcX\u22a4),\n\u02dcX = G\u03d5(\u02dcZ),\n\u02dcZ \u223cP\u02dcZ,\n(4)\nwhere G\u03d5 is an MLP-based generator that maps the random latent variable \u02dcZ \u2208RN\u00d7d to the anomalous node attributes,\nand T : R \u2192[0, 1] denotes an element-wise transformation function, e.g. Sigmoid(\u00b7). In this way, we generate an\nanomalous graph set \u02dcG with the generator G\u03d5. We then introduce a discriminator D\u03c9, which takes the anomalous graphs\n\u02dcG and normal graphs G as input, and aims to effectively distinguish between them. To fully exploit the structural\ninformation of graphs, D\u03c9 is expected to be a GNN-based network. Specifically, we leverage GIN [Xu et al., 2019] as\nthe backbone network of the discriminator D\u03c9 to learn ideal graph-level representations for graph data. Assume we\nhave an input graph Gi, the latent features h(k)(v) of node v in the k-th layer of GIN can be obtained by aggregating\nthe learned features from its neighboring nodes in the (k \u22121)-th layer, which can be formulated as\nh(k)(v) = \u03b4(COMBINE(h(k\u22121)(v), AGGREGATE({h(k\u22121)(u), u \u2208C(v)}))),\n(5)\nwhere C(v) denotes the neighbor set of node v, and \u03b4(\u00b7) is a non-linear activation function such as ReLU.\nAGGREGATE(\u00b7) function combines the features of neighboring nodes in C(v), and COMBINE(\u00b7) function combines\nthe features from the previous layer and the aggregated neighborhood information to obtain the current layer\u2019s features.\nNote that the attribute xv of node v serves as the initial features, i.e., h(0)(v) = xv. Then the graph-level representation\nof graph Gi can be derived as follows:\nhGi = R({CONCAT(h(k)(v), k \u2208{1, . . . , K})}, v \u2208Gi),\n(6)\n4\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nwhere CONCAT(\u00b7) function concatenates the representations learned in each GIN layer, and R(\u00b7) denotes the max-\nreadout function that aggregates the node features into a graph-level representation. Consequently, we can learn\nthe graph-level representations HG and H\u02dcG for normal and pseudo-anomalous graphs, and train the discriminator\nto distinguish them as much as possible. The generator G\u03d5 and discriminator D\u03c9 are alternatively optimized with a\nmin-max game as follows:\nmin\n\u03d5\nmax\n\u03c9\nE\nXi,Ai\u223cPG[D\u03c9(Xi, Ai)] \u2212\nE\n\u02dcZi\u223cP\u02dcZ\n[D\u03c9(G\u03d5(\u02dcZi), T (G\u03d5(\u02dcZi)G\u03d5(\u02dcZi)\u22a4))],\n(7)\nwhere PG denotes the normal graph data distribution, and \u02dcZi \u2208Rn\u00d7d\u2032 is sampled from the prior distribution P\u02dcZ \u223c\nN(0, 1). The trained discriminator can then serve as an anomaly detector. Comparing to (1), we see that the constraint\ndist(D, \u02dcDs) \u2264\u03f5 is guaranteed if G\u03d5 given by (7) are strong enough. It is difficult to guarantee for (1) that \u02dcDs does not\noverlap with D, which however is not compulsory because it is still possible to learn a discriminator from overlapping\nD, \u02dcDs to distinguish between D and \u02dcD. We call this method SDGG-ATI, where AT represents adversarial training.\nAlthough promising for anomalous graph detection, SDGG-ATI has the following issues.\n\u2022 An MLP-based generator may not effectively capture the structural information of graphs, which could impede the\ngeneration of high-quality anomalous graphs for training.\n\u2022 The interpretability of the GAN-based method is limited, as generating anomalous graphs from random noise does\nnot necessarily ensure the generation of high-quality anomalous graphs.\n\u2022 The optimization of the GAN-based method involves a min-max game, which can lead to instability during training.\nBesides, the competition between the generator and discriminator may result in mode collapse, leading to the\ngeneration of poor-quality anomalous graphs.\n2.3\nSelf-Discriminative Modeling: SDGG-ATII\nTo address the first two issues of SDGG-ATI, we propose a variant of our SDGG-ATI, which can leverage the structural\ninformation, and further provide more explicit guidance for the generator G\u03d5, ensuring the generation of high-quality\nanomalous graphs that closely resemble normal ones but can still be distinguished by the discriminator. Specifically, we\nuse the GIN-based VGAE network as the generator G\u03d5, which consists of a Node-VGAE and an Edge-VGAE [Kipf\nand Welling, 2016], to learn anomalous graphs. The Node-VGAE aims to generate anomalous attributes \u02dcX, while the\nEdge-VGAE which does not include a decoder, aims to generate adjacency matrix \u02dcA. Instead of sampling the input\nof G\u03d5 from the latent distribution P\u02dcZ, we take the normal graph set G as the input of G\u03d5, with the aim of generating\nanomalous graphs \u02dcG that are close to G but are expressive pseudo-anomalous graphs.\nHere we only describe Node-VGAE, as it differs from Edge-VGAE just in the existence of a decoder. We first learns\nthe graph-level representation HG for the input graphs G = {G1, . . . , GN} by Eq. (5) and (6), where Gi = {Xi, Ai}.\nNext, we map the graph-level representation into a latent Gaussian distribution N(\u00b5, \u03c32) as in VGAE, where the means\n\u00b5 and deviations \u03c3 are defined as follows:\n\u00b5 = GIN\u00b5(HG, A), \u03c3 = exp(GIN\u03c3(HG, A)),\n(8)\nwhere \u00b5 and \u03c3 can explicitly define an inference model that we can sample latent graph representations ZG from it as\nfollows:\nq(ZG|HG, A) =\nN\nY\ni=1\nq(ZGi|HG, A), q(ZGi|HG, A) = N(ZGi|\u00b5i, diag(\u03c3i)).\n(9)\nSince the sample operation could not provide gradient information, we leverage the reparametrization trick [Kingma\nand Welling, 2014] to sample the latent graph representation, i.e.,\nZG = \u00b5 + \u03f5\u03c3, \u03f5 \u223cN(0, 1),\n(10)\nwhere \u03f5 denotes the random Gaussian noise subject to the standard normal distribution. Consequently, we can generate\na negative graph set including edges and nodes by\n\u02dcA = T (ZGZ\u22a4\nG),\n\u02dcX = MLP(ZG),\n(11)\nwhere MLP(\u00b7) denotes an MLP-based decoder, which aims to generate anomalous attributes \u02dcX from the latent graph\nrepresentations. Then the anomalous adjacent matrix \u02dcA can be generated from latent graph representation learned by\nEdge-VGAE following (11) without the MLP-based decoder.\n5\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nOur expectation is to generate high-quality anomalous graphs that closely resemble normal ones but still can be\ndistinguished by the classifier. This requires a high level of similarity between the generated anomaly graphs and the\nnormal graphs, which can be regarded as minimizing the discrepancy between the generated attributes and the normal\nones, with a similar objective for the generated adjacency matrix. Therefore, we propose to minimize the following\ndiscrepancy loss\nLdis = 1\nN\nN\nX\ni=1\n\u0012\r\r\rXi \u2212\u02dcXi\n\r\r\r\n2\nF \u2212(Ai log( \u02dcAi) + (1 \u2212Ai) log(1 \u2212\u02dcAi))\n\u0013\n,\n(12)\nwhere \u02dcXi and \u02dcAi are the node attribute and adjacency matrix generated by the Node-VGAE and Edge-VGAE\nof G\u03d5 respectively, i.e., \u02dcXi, \u02dcAi = G\u03d5(Xi, Ai). The first term denotes the attribute reconstruction loss, and the\nsecond term denotes the binary cross-entropy loss. Additionally, the distribution of learned latent representation\nZG is expected to follow a pre-defined prior distribution, which allows the generated latent representations ZG to\nbe uniformly distributed in the latent space, ensuring the diversity of generated graphs. We can achieve this by\npenalizing the KL-divergence between q(ZG|HG) and a prior distribution P(Z), i.e., KL[q(ZG|HG, A)||P(Z)], where\nP(Z) = Q\ni p(Zi) = Q\ni N(Zi|0, I) typically follows a Gaussian prior distribution. The overall objective function of\nthe perturbation learning-based approach is\nmin\n\u03d5\nmax\n\u03c9\nE\nXi,Ai\u223cPG[D\u03c9(Xi, Ai)] \u2212\nE\nXi,Ai\u223cPG[D\u03c9(G\u03d5(Xi, Ai))]\n+ \u03bbLdis \u2212\u03b3KL[q(ZG|HG, A)||P(Z)].\n(13)\nAs the discrepancy loss and KL-divergence terms are specific to the generator, we update both of them during the\ntraining of the generator. This is a perturbation learning-based variant because the pseudo-anomalous graphs are\ngenerated via perturbing the latent variable of normal graphs. For convenience, we call this method SDGG-ATII.\nCompared to the GAN-based method SDGG-ATI, SDGG-ATII offers better interpretability by explicitly guiding the\ngenerator to generate pseudo-anomalous graphs that closely resemble the normal ones. Additionally, SDGG-ATII\noffers better control over the diversity of the generated graphs by penalizing the KL-divergence between the learned\nlatent graph representation distribution and a prior Gaussian distribution. Compared to (2), we explicitly defined the\ndiscrepancy loss (12) to guarantee the generated anomalous graphs surrounded the normal ones, and learn the decision\nboundary from the adversarial training of generator and discriminator. This variant offers improved interpretability in\ncontrast to SDGG-ATI which relies solely on adversarial training between the generator and discriminator to guarantee\nthe constraint. Nevertheless, SDGG-ATII still suffers from the instability of the min-max optimization.\n2.4\nSelf-Discriminative Modeling: SDGG-NAT\nTo address the instability of the min-max optimization in the adversarial training approaches SDGG-ATI and SDGG-\nATII, we further propose a non-adversarial variant for the perturbation learning-based method, which avoids the\ninstability problem of GANs and simplifies the training process. Specifically, rather than training a generator and\na discriminator to compete against each other, we directly train a classifier f\u03b8 to distinguish the anomalous graphs\nproduced by generator G\u03d5 from normal ones.\nWe accomplish this by utilizing Node-VGAE and Edge-VGAE to produce a set of anomalous graphs \u02dcG from normal\ngraphs G, then train a classifier to distinguish them. The overall objective is\nminimize\n\u03b8,\u03d5\n1\nN\nN\nX\ni=1\n\u0000\u2113(yi, f\u03b8(Xi, Ai)) + \u2113(\u02dcyi, f\u03b8(G\u03d5(Xi, Ai))\n\u0001\n+ \u03bbLdis \u2212\u03b3KL[q(ZG|HG, A)||P(Z)],\n(14)\nwhere \u2113denotes the binary cross-entropy loss of the classifier, Ldis and the KL-divergence are exactly the same as (13).\nThe classifier is based on GIN which receives attribute and adjacency matrices as inputs, allowing for consideration of\nthe structural information of the graphs. Importantly, our method is unsupervised, requiring no supervised information\nwhatsoever. We simply set y1 = \u00b7 \u00b7 \u00b7 = yN = 0 for the normal graphs, and \u02dcyi = \u00b7 \u00b7 \u00b7 = \u02dcyN = 1 for the generated\nanomalous graphs. Compared to (2), we directly learn the decision boundary by simultaneously training a classifier with\na generator that produced high-quality pseudo-anomalous graphs for the classifier. This makes our method particularly\nappealing for real-world applications where obtaining labeled data is challenging and costly. The detailed training flows\nof the proposed SDGG-ATI, SDGG-ATII, and SDGG-NAT are given in the supplementary material due to the space\nlimitation of the paper.\n3\nExperiment\nIn this section, we evaluate the proposed methods via comprehensive experiments on several molecule and social\nnetwork graph datasets in comparison with state-of-the-art methods.\n6\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\n3.1\nDatasets, Baselines, and Experimental Settings\nDatasets.\nIn this paper, we experiment on different types of benchmarks, including four small molecule datasets,\nthree biology datasets, and two social network datasets. These three types of data are typical graph-structured data\nin real-world scenarios. Moreover, we also consider four large molecule datasets to evaluate the anomaly detection\nperformance in large-scale imbalanced graph datasets. Table 1 briefly describes the main information of each dataset,\nand more details refer to the supplementary material.\nTable 1: Brief information of the benchmarks.\nDataset name\nGraphs\nAverage [V ]\nClasses\nTypes\nMUTAG\n188\n17.93\n2\nMolecule\nAIDS\n2000\n15.69\n2\nMolecule\nCOX2\n467\n41.22\n2\nMolecule\nER_MD\n446\n21.33\n2\nMolecule\nPROTEINS\n1113\n39.06\n2\nBiology\nDD\n1178\n284.32\n2\nBiology\nENZYMES\n600\n32.63\n6\nBiology\nIMDB-Binary\n1000\n19.77\n2\nSocial networks\nCOLLAB\n5000\n74.49\n3\nSocial networks\nSW-620\n40532\n26.06\n2\nMolecule\nMOLT-4\n39765\n26.10\n2\nMolecule\nPC-3\n27509\n26.36\n2\nMolecule\nMCF-7\n27770\n26.40\n2\nMolecule\nBaselines.\nWe demonstrate the effectiveness of the pro-\nposed methods by comparison with several state-of-the-art\nmethods including four graph kernel methods and eleven\nGNN-based graph-level anomaly detection approaches (to\nbe shown in the tables of results).\nExperimental settings. For the proposed methods, we\ndescribe the detailed network structures, hyper-parameter\nsettings, and training details in the supplementary material\ndue to the space limitation. For the baseline models, we\nreproduced the experimental results for all of them by exe-\ncuting their official codes. Notably, we consider two types\nof experiments in this paper to evaluate anomaly detection\nperformance. The first experiment focuses on the one-class\nclassification task, where we respectively treat each class of a dataset as the normal class and assess the anomaly\ndetection performance for each class individually. The second experiment involves anomaly detection on large-scale\nimbalanced graph datasets, where the class with a small number of samples is designated as the anomaly. We choose\nAUC as the evaluation metric, run each experiment 10 times and report the means and standard deviations.\n3.2\nComparison Results with State-of-the-art Approaches\nTable 2 summarizes the performance of our methods compared to state-of-the-art approaches following the one-class\nclassification setting (more results refer to the supplementary material). Our evaluations cover various types of graph-\nstructured data, including molecules, biological data, and social networks. Overall, we have the following observations\nfrom the experimental results:\nTable 2: Average AUCs with standard deviation (10 trials) on MUTAG, AIDS, and PROTEINS. The best results are\nmarked in bold and \u2019\u2013\u2019 means out of memory.\nMethod/Dataset\nMUTAG\nAIDS\nPROTEINS\n0\n1\n0\n1\n0\n1\nSP [Borgwardt and Kriegel, 2005]\n59.17\u00b10.00\n26.08\u00b10.00\n97.78\u00b10.00\n28.32\u00b10.00\n66.83\u00b10.00\n52.02\u00b10.00\nWL [Shervashidze et al., 2011]\n65.09\u00b10.00\n29.60\u00b10.00\n93.81\u00b10.00\n23.41\u00b10.00\n73.19\u00b10.00\n50.19\u00b10.00\nNH [Hido and Kashima, 2009]\n79.59\u00b12.74\n16.79\u00b10.62\n96.85\u00b10.21\n49.92\u00b10.54\n68.28\u00b10.00\n55.51\u00b10.00\nRW [Vishwanathan et al., 2010]\n65.03\u00b13.12\n86.98\u00b10.00\n15.04\u00b10.00\n40.84\u00b13.10\n\u2013\n\u2013\nVGAE-AD [Kipf and Welling, 2016]\n70.00\u00b14.44\n73.30\u00b15.40\n56.59\u00b11.59\n51.11\u00b10.34\n72.22\u00b13.70\n57.00\u00b10.99\nOCGIN [Zhao and Akoglu, 2021]\n88.40\u00b12.14\n74.66\u00b11.68\n90.65\u00b12.04\n81.52\u00b13.76\n55.01\u00b19.65\n47.77\u00b17.64\nInfoGraph [Sun et al., 2020]\n88.05\u00b14.48\n61.66\u00b120.52\n84.17\u00b15.50\n87.41\u00b12.27\n65.04\u00b113.35\n47.02\u00b16.92\nGLocalKD [Ma et al., 2022]\n50.23\u00b123.90\n90.59\u00b10.61\n99.15\u00b10.03\n17.42\u00b121.09\n72.12\u00b10.08\n74.80\u00b10.12\nOCGTL [Qiu et al., 2022]\n65.70\u00b12.10\n75.79\u00b122.12\n98.09\u00b10.48\n99.34\u00b10.06\n63.20\u00b15.40\n58.10\u00b16.10\nSDGG-ATI\n100.00\u00b10.00\n98.50\u00b12.53\n100.00\u00b10.00\n100.00\u00b10.00\n90.84\u00b10.15\n89.19\u00b10.17\nSDGG-ATII\n99.31\u00b11.42\n99.68\u00b12.85\n100.00\u00b10.00\n81.10\u00b137.80\n87.97\u00b15.70\n89.19\u00b10.56\nSDGG-NAT\n100.00\u00b10.00\n99.36\u00b10.35\n99.98\u00b10.00\n100.00\u00b10.00\n95.91\u00b12.55\n96.26\u00b10.05\n\u2022 Our approaches significantly outperform the graph kernels and other GNN-based methods across all datasets. For\nexample, they outperform the closest competitor by over 20% in terms of AUC on MUTAG and PROTEINS, and also\nexhibit remarkable performance on other datasets.\n\u2022 We can observe a phenomenon called \u201cperformance flip\u201d, where the performance of different classes in a dataset may\nhave significant differences, in many approaches such as most graph kernels, OCGIN , and GLocalKD on AIDS.\nConversely, the \u201cperformance flip\u201d is largely absent from the proposed methods, showing robust and competitive\nperformance in each class across all benchmark datasets.\n\u2022 The variance of the performance generally reflects the stability of the model. Although graph kernels show stable\nperformance, their overall results are not satisfactory. Furthermore, the GNN-based methods also exhibit larger\n7\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nvariances on specific datasets. Nevertheless, in the non-adversarial version of our method, i.e., SDGG-NAT, we can\nobserve a smaller variance in most cases, which fully demonstrates the stability of the proposed method. We also\nsupplement the overall analysis for the proposed three variants of SDGG to further support our claim, which refers to\nTable 10 in Appendix E.\n50\n60\n70\n80\n90\nAlgorithm\nAUC (%)\nAlgorithm\nOCGIN\nGKLOCAL\nOCGTL\ninfograph+Deep SVDD\nSP+OCSVM\nNH+OCSVM\nWL+OCSVM\nVGAE\u2212AD\nSDGG\u2212ATI\nSDGG\u2212ATII\nSDGG\u2212NAT\nFigure 3: The results of multi-class AD.\nBesides, we also conduct a multi-class graph-level anomaly detection\nexperiment on ENZYMES, where multiple classes are regarded as\nanomalies and others as normal ones. Specifically, we set the class\n{0, 1, 2, 3} as the normal classes and {4, 5} as the anomalous classes.\nFigure 3 shows the experimental results of our methods against several\nstate-of-the-art GNN-based GAD methods. We can observe that the\nproposed three methods significantly outperform all the baselines with\na large margin (more than 20%). This demonstrates the feasibility and\npotential of the proposed methods in dealing with multi-class GAD\nscenarios. Moreover, SDGG-NAT and SDGG-ATII achieve more\noutstanding performance than SDGG-ATI, and SDGG-NAT exhibits\nmore stability compared with SDGG-ATII as it has less performance\nfluctuations.\n3.3\nComparison Results on Large-Scale Imbalanced Datasets\nWe further evaluate the feasibility of our methods on large-scale\nimbalanced datasets including SW-620, MOLT-4, PC-3, and MCF-7,\nwhere we treat the rare \u201cactive\u201d status in anti-cancer molecules of\nthese datasets as anomalies. Table 3 shows the experimental results of our methods compared to several state-of-the-art\nGNN-based approaches.\nTable 3: Average AUCs (%) with standard deviation (%) (10 trials) on large-scale imbalanced graph datasets. The best\nresults are marked in bold.\nMethod/Dataset\nSW-620\nMOLT-4\nPC-3\nMCF-7\nSupervised graph-level anomaly detection approaches\nGCN [Kipf and Welling, 2017]\n74.90\u00b10.74\n72.55\u00b10.52\n75.36\u00b12.13\n72.70\u00b11.05\nDGCNN [Zhang et al., 2018]\n80.06\u00b10.42\n76.50\u00b10.60\n79.15\u00b11.84\n76.41\u00b10.81\nGIN [Xu et al., 2019]\n78.61\u00b12.85\n75.86\u00b11.60\n78.44\u00b11.67\n69.54\u00b11.15\nSOPOOL [Wang and Ji, 2020]\n75.51\u00b15.06\n75.11\u00b10.97\n69.37\u00b11.53\n75.64\u00b12.17\nRWGNN [Nikolentzos and Vazir-\ngiannis, 2020]\n73.37\u00b10.36\n71.30\u00b11.23\n76.27\u00b10.86\n70.47\u00b11.26\niGAD [Zhang et al., 2022]\n85.82\u00b10.69\n83.59\u00b11.07\n86.04\u00b11.14\n83.22\u00b10.64\nUnsupervised graph-level anomaly detection approaches\nOCGTL [Qiu et al., 2022]\n67.69\u00b10.02\n57.42\u00b12.38\n68.42\u00b11.73\n64.92\u00b11.92\nGLocalKD [Ma et al., 2022]\n64.14\u00b10.92\n61.43\u00b11.26\n64.79\u00b11.22\n61.43\u00b11.26\nSDGG-ATI\n90.19\u00b18.94\n90.25\u00b17.57\n91.59\u00b16.73\n81.62\u00b18.18\nSDGG-ATII\n92.91\u00b15.48\n97.05\u00b12.39\n94.30\u00b10.63\n88.40\u00b10.13\nSDGG-NAT\n94.26\u00b12.86\n94.20\u00b14.79\n97.09\u00b11.78\n94.71\u00b12.13\nNote that we compare not only with unsupervised baselines, but also with supervised ones, to demonstrate the\neffectiveness of our methods. From these tables, we have the following observations:\n\u2022 DCGNN and iGAD outperformed OCGTL and GLocalKD, which demonstrates the usefulness of including a few\nanomalous graphs in the training data.\n\u2022 SDGG-ATII and SDGG-NAT show remarkable performance across all datasets, even surpassing strong supervised\nbaselines such as DGCNN and iGAD by more than 10% on most datasets. The reason is that supervised methods rely\nheavily on real labels, which are often scarce in large-scale imbalanced datasets. Conversely, our approaches can\ngenerate high-quality pseudo-anomalous graphs. Note that supervised methods will not generalize well when the test\ndata are not drawn from the same distribution of the training data, which occurs if the number of labeled anomalous\ngraphs is limited. We provide a detailed explanation for this claim with a visual example in Figure 6 of Appendix E.\n8\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\n\u2022 Compared to SDGG-ATI, SDGG-NAT demonstrates greater stability of high accuracy, which is consistent with the\nmotivation presented in Section 2.4.\n3.4\nVisualization of Learned Decision Boundary\nWe visualize the learned embeddings using t-SNE and the discriminative score of the discriminator (classifier) to\nintuitively demonstrate the effectiveness of the proposed methods. Figure 4 shows the experimental results of SDGG-\nNAT on MUTAG Class 1 (More results refer to the supplementary material).\n-4\n20\n-2\n15\n-5\n0\n-10\n2\n10\n4\n-15\n5\n-20\n0\n-25\n(a) Embedding Visualization\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nScore\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFrequency Density\nGenrated Anomalous Data\nNormal Data\n0.844\n0.84405\n0.8441\n0.84415\n0.8442\n0.84425\n0.8443\nScore\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nFrequency Density\nAnomalous Data\nNormal Data\n(c) Testing Stage\n(b) Training Stage\nFigure 4: Visualization of SDGG-NAT on MUTAG. Note that for subfigure (a), the points marked in green, blue, pink,\nand purple represent real anomalous data, generated anomalous data, and normal data in training and testing stages\nrespectively. For subfigure (b), (c), the x-axis and y-axis represent the output scores and the number density of data\nsamples within a certain interval respectively.\nFrom Figure 4 (a), we observe that the normal data from both training and testing stages approximately lie on the same\nmanifold, while the real anomalous and generated pseudo-anomalous data are well separated into different regions from\nthe normal data. More importantly, the generated pseudo-anomalous data interpolate between normal data and (real)\nanomalous data. This observation demonstrates the strong discrimination of the trained classifier, which is attributed to\nthe high-quality pseudo-anomalous graphs generated adaptively. Moreover, the results in Figure 4 (b) and (c) reveal\nthe scores of generated anomalous graphs in the classifier are significantly lower than those of the normal graphs in\nboth the training and testing stages. This phenomenon further demonstrates that our approaches are able to accurately\ndistinguish between normal and anomalous graphs by generating high-quality pseudo-anomalous graphs to train a\npowerful classifier.\n4\nConclusion\nIn this paper, we proposed a novel framework for graph-level anomaly detection. The key idea is to generate pseudo-\nanomalous graphs that interpolate between normal graphs and (real) anomalous graphs though not presented in the\ntraining stage. We provide three methods, namely, SDGG-ATI, SDGG-ATII, and SDGG-NAT. Particularly, SDGG-NAT\nhas much higher learning stability and detection accuracy than the other two methods. The comprehensive experiments\non various graph benchmarks, including molecular, biological, social network, and large-scale imbalanced molecular\ndatasets, demonstrate the effectiveness of our methods compared to state-of-the-art graph-level anomaly detection\nmethods. Surprisingly, although our methods are unsupervised learning, they outperformed a few strong baselines of\nsupervised learning methods for GAD. One limitation of our work is that we haven\u2019t considered any real anomalous\ngraphs in the training stage, though they may be available in some scenarios.\nReferences\nLeman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: a survey. Data\nMining and Knowledge Discovery, 29:626\u2013688, 2015.\nAlex Beutel, Leman Akoglu, and Christos Faloutsos. Graph-based user behavior modeling: from prediction to fraud\ndetection. In Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data\nMining, pages 2309\u20132310, 2015.\nKarsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of the Fifth IEEE\nInternational Conference on Data Mining, pages 8\u2013pp. IEEE, 2005.\nDylan Chou and Meng Jiang. A survey on data-driven network intrusion detection. ACM Computing Surveys (CSUR),\n54(9):1\u201336, 2021.\n9\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nPim de Haan, Taco S Cohen, and Max Welling. Natural graph networks. Advances in Neural Information Processing\nSystems, 33:3636\u20133646, 2020.\nKaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks. In\nProceedings of the 2019 SIAM International Conference on Data Mining, pages 594\u2013602. SIAM, 2019.\nDongsheng Duan, Lingling Tong, Yangxi Li, Jie Lu, Lei Shi, and Cheng Zhang. Aane: Anomaly aware network\nembedding for anomalous link detection. In Proceedings of the IEEE International Conference on Data Mining,\npages 1002\u20131007. IEEE, 2020.\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv preprint\narXiv:1903.02428, 2019.\nSiavash Ghavami. Anomaly detection in molecular communications with applications to health monitoring networks.\nIEEE Transactions on Molecular, Biological and Multi-Scale Communications, 6(1):50\u201359, 2020.\nMartin Grohe. word2vec, node2vec, graph2vec, x2vec: Towards a theory of vector embeddings of structured data. In\nProceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems, pages 1\u201316,\n2020.\nSongqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao. Adbench: Anomaly detection benchmark.\nAdvances in Neural Information Processing Systems, 35:32142\u201332159, 2022.\nShohei Hido and Hisashi Kashima. A linear-time graph kernel. In Proceedings of the Ninth IEEE International\nConference on Data Mining, pages 179\u2013188. IEEE, 2009.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980,\n2014.\nDiederik P Kingma and Max Welling. Auto-encoding variational bayes. In Proceedings of the International Conference\non Learning Representations, 2014.\nThomas N Kipf and Max Welling. Variational graph auto-encoders. arXiv preprint arXiv:1611.07308, 2016.\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In Proceedings of\nthe International Conference on Learning Representations, 2017.\nRui Li, Xin Yuan, Mohsen Radfar, Peter Marendy, Wei Ni, Terence J O\u2019Brien, and Pablo M Casillas-Espinosa. Graph\nsignal processing, graph neural network and graph learning on biological data: a systematic review. IEEE Reviews in\nBiomedical Engineering, 2021.\nXiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu. A\ncomprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge and Data\nEngineering, 2021.\nRongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly detection by glocal\nknowledge distillation. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data\nMining, pages 704\u2013714, 2022.\nHaggai Maron, Heli Ben-Hamu, Hadar Serviansky, and Yaron Lipman. Provably powerful graph networks. Advances\nin Neural Information Processing Systems, 32, 2019.\nAlan Mislove, Massimiliano Marcon, Krishna P Gummadi, Peter Druschel, and Bobby Bhattacharjee. Measurement and\nanalysis of online social networks. In Proceedings of the 7th ACM SIGCOMM Conference on Internet measurement,\npages 29\u201342, 2007.\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar Venkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal.\ngraph2vec: Learning distributed representations of graphs. arXiv preprint arXiv:1707.05005, 2017.\nGiannis Nikolentzos and Michalis and Vazirgiannis. Random walk graph neural networks. Advances in Neural\nInformation Processing Systems, 33:16211\u201316222, 2020.\nChen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt, and Maja Rudolph. Neural transformation learning for deep\nanomaly detection beyond images. In Proceedings of the International Conference on Machine Learning, pages\n8703\u20138714. PMLR, 2021.\nChen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. Raising the bar in graph-level anomaly detection. In\nProceedings of the International Joint Conference on Artificial Intelligence, pages 2196\u20132203, 2022.\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander Binder, Em-\nmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In Proceedings of the International Conference on\nMachine Learning, pages 4393\u20134402. PMLR, 2018.\n10\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nLukas Ruff, Robert A Vandermeulen, Nico G\u00f6rnitz, Alexander Binder, Emmanuel M\u00fcller, Klaus-Robert M\u00fcller, and\nMarius Kloft. Deep semi-supervised anomaly detection. In Proceedings of the International Conference on Learning\nRepresentations, 2020.\nLukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen, Gr\u00e9goire Montavon, Wojciech Samek, Marius Kloft,\nThomas G Dietterich, and Klaus-Robert M\u00fcller.\nA unifying review of deep and shallow anomaly detection.\nProceedings of the IEEE, 109(5):756\u2013795, 2021.\nNino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borgwardt. Weisfeiler-\nlehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.\nFan-Yun Sun, Jordon Hoffman, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-supervised graph-level\nrepresentation learning via mutual information maximization. In Proceedings of the International Conference on\nLearning Representations, 2020.\nTijmen Tieleman, Geoffrey Hinton, et al. Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent\nmagnitude. COURSERA: Neural networks for machine learning, 4(2):26\u201331, 2012.\nS Vichy N Vishwanathan, Nicol N Schraudolph, Risi Kondor, and Karsten M Borgwardt. Graph kernels. Journal of\nMachine Learning Research, 11:1201\u20131242, 2010.\nZhengyang Wang and Shuiwang Ji. Second-order pooling for graph neural networks. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2020.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In Proceedings\nof the International Conference on Learning Representations, 2019.\nMuhan Zhang, Zhicheng Cui, Marion Neumann, and Yixin Chen. An end-to-end deep learning architecture for graph\nclassification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\nGe Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su, Chuan Zhou, Quan Z Sheng, Leman\nAkoglu, et al. Dual-discriminative graph neural network for imbalanced graph-level anomaly detection. Advances in\nNeural Information Processing Systems, 35:24144\u201324157, 2022.\nLingxiao Zhao and Leman Akoglu. On using classification datasets to evaluate graph outlier detection: Peculiar\nobservations and new insights. Big Data, 2021.\nYu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative and contrastive\nself-supervised learning for graph anomaly detection. IEEE Transactions on Knowledge and Data Engineering,\n2021.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Daeki Cho, and Haifeng Chen. Deep\nautoencoding gaussian mixture model for unsupervised anomaly detection. In Proceedings of the International\nConference on Learning Representations, 2018.\n11\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nA\nDecision boundary visualizations of 2-D simulation\nHere we give a simulation to show the key idea and effectiveness of our methods, in addition to Figure 1 in the main\npaper. For convenience, we only consider SDGG-NAT and we will not use graphs because it is difficult to conduct a\nreasonable simulation for a number of graphs. Thus the corresponding backbones of SDGG-NAT are changed to VAE\n(generator) and a common MLP-based classifier. We generate a number of synthetic 2-D samples (normal training data)\nusing x = sin(z) + e, where e is a noise drawn from a uniform distribution (\u2212a, a). A larger a leads to a wider normal\nregion. In Figure 5, the pink line denotes the learned decision boundaries and the color of the figure turns from blue to\nred means the score given by the classifier increases. The observations are as follows.\n\u2022 The generated pseudo-anomalous data usually distribute close to the training (normal) data and shows a similar\nmanifold trend as normal one.\n\u2022 In most cases, the classifier can distinguish those pseudo-anomalous data far from normal area.\n\u2022 When the interval of normal data turns from narrow to wide, some generated pseudo-anomalous data may locate\nclose to training data, but classifier would neglect most of them and draw a superior decision boundary surrounding\nall training data.\nIn conclusion, the proposed model can effectively handle normal intervals with different gaps, where the learned decision\nboundaries enclose the normal training data tightly. The results strongly support our assumption and motivation.\n(a) \ud835\udc52\ud835\udc52= 0\n(b) \ud835\udc52\ud835\udc52~U(\u22120.1,0.1)\n(c) \ud835\udc52\ud835\udc52~U(\u22120.3,0.3)\n(d) \ud835\udc52\ud835\udc52~U(\u22120.5,0.5)\n(e) \ud835\udc52\ud835\udc52~U(\u22120.8,0.8)\n(f) \ud835\udc52\ud835\udc52~U(\u22121,1)\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nTraining (normal) Data\nGenerated Pseudo-Anomalous Data\nFigure 5: The decision boundaries learned from 2-D synthetic data x = sin(z) + e with different e.\nB\nAlgorithm of the proposed methods\nWe supplement the detailed training process of the proposed SDGG-ATI, SDGG-ATII, and SDGG-NAT in Algorithm 1,\n2 and 3, respectively.\nC\nDetailed description of the datasets\nWe describe more details about the datasets used in our experiment in Table 4, which further includes the average\nnumber of edges and the node classes. Besides, Table 5 shows the imbalance ratio of large-scale graph benchmarks.\n12\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nAlgorithm 1 SDGG-ATI\nInput: Input graph set G, number of GIN layers K, clipping parameter c, learning rate \u03b1, batch size m, total training\nepochs T .\nOutput: The anomaly detection scores s.\n1: Initialize the network parameters \u03d5, \u03c9;\n2: for t \u2192T do\n3:\nfor each batch G do\n4:\nUpdate Generator:\n5:\nUnfreeze the the parameter \u03d5 of generator G\u03d5;\n6:\nFreeze the the parameter \u03c9 of discriminator D\u03c9;\n7:\nSample random variable \u02dcZ from latent Gaussian distribution P\u02dcZ \u223cN(0, 1);\n8:\nGenerate anomalous graph set \u02dcG from generator G\u03d5 with the input \u02dcZ via Eq. (3))\n9:\nUpdate the parameter \u03d5 of generator G\u03d5 by\nG\u03d5 \u2190\u2207[\u22121\nm\nPm\ni=1 D\u03c9(G\u03d5(\u02dcZi), T (G\u03d5(\u02dcZi)G\u03d5(\u02dcZi)\u22a4))];\n\u03d5 \u2190\u03d5 \u2212\u03b1 \u00b7 RMSProp(\u03d5, G\u03d5);\n10:\nUpdate Discriminator:\n11:\nFreeze the the parameter \u03d5 of generator G\u03d5;\n12:\nUnfreeze the the parameter \u03c9 of discriminator D\u03c9;\n13:\nRepeat steps 7 - 8;\n14:\nUpdate the parameter \u03c9 of generator D\u03c9 by\nD\u03c9 \u2190\u2207[\u22121\nm\nPm\ni=1 D\u03c9(Xi, Ai) + 1\nm\nPm\ni=1 D\u03c9(G\u03d5(\u02dcZi), T (G\u03d5(\u02dcZi)G\u03d5(\u02dcZi)\u22a4))];\n\u03c9 \u2190\u03c9 \u2212\u03b1 \u00b7 RMSProp(\u03c9, D\u03c9);\n\u03c9 \u2190clip(\u03c9, \u2212c, c);\n15:\nend for\n16: end for\n17: Compute anomaly detection scores for test graphs via the trained discriminator D\u03c9;\n18: return The anomaly detection scores.\nTable 4: Detailed information of the graph benchmarks.\nDataset name\nGraphs\nAverage nodes\nAverage edges\nNode classes\nGraph classes\nTypes\nSmall-scale and moderate-scale datasets\nMUTAG\n188\n17.93\n19.79\n7\n2\nMolecule\nAIDS\n2000\n15.69\n16.20\n38\n2\nMolecule\nCOX2\n467\n41.22\n43.45\n8\n2\nMolecule\nER_MD\n446\n21.33\n234.85\n10\n2\nMolecule\nPROTEINS\n1113\n39.06\n72.82\n3\n2\nBiology\nDD\n1178\n284.32\n715.66\n82\n2\nBiology\nENZYMES\n600\n32.63\n62.14\n3\n6\nBiology\nIMDB-Binary\n1000\n19.77\n96.53\n\u2013\n2\nSocial networks\nCOLLAB\n5000\n74.49\n2457.78\n\u2013\n3\nSocial networks\nLarge-scale and imbalanced datasets\nSW-620\n40532\n26.06\n28.09\n65\n2\nMolecule\nMOLT-4\n39765\n26.10\n28.14\n64\n2\nMolecule\nPC-3\n27509\n26.36\n28.49\n45\n2\nMolecule\nMCF-7\n27770\n26.40\n28.53\n46\n2\nMolecule\nTable 5: The imbalance ratio of large-scale graph benchmarks.\nDatasets\nClass\n# Number of Graphs\nImbalance Ratio\nSW-620\nNormal\n38,122\n5.95%\nAnomalous\n2,410\nMCF-7\nNormal\n25,476\n8.26%\nAnomalous\n2,294\nPC-3\nNormal\n25,941\n9.34%\nAnomalous\n1,568\nMOLT-4\nNormal\n36,625\n7.90%\nAnomalous\n3,140\n13\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nAlgorithm 2 SDGG-ATII\nInput: Input graph set G, number of GIN layers K, clipping parameter c, learning rate \u03b1, batch size m, total training\nepochs T .\nOutput: The anomaly detection scores s.\n1: Initialize the network parameters \u03d5, \u03c9;\n2: for t \u2192T do\n3:\nfor each batch G do\n4:\nUpdate Generator:\n5:\nUnfreeze the the parameter \u03d5 of generator G\u03d5;\n6:\nFreeze the the parameter \u03c9 of discriminator D\u03c9;\n7:\nExtrat graph-level representation with the input normal attributes X and adjacency matrix A via Eq. (4) and\n(5);\n8:\nGenerate anomalous anomalous graph set \u02dcG from generator G\u03d5 with normal attributes X and adjacency matrix\nA via Eq. (7), (8), (9), and (10);\n9:\nUpdate the parameter \u03d5 of generator G\u03d5 by\nG\u03d5 \u2190\u2207[\u22121\nm\nPm\ni=1 D\u03c9(G\u03d5(Xi, Ai) + \u03bb\nm\nPm\ni=1(\u2225Xi \u2212\u02dcXi\u22252\nF \u2212(Ai log( \u02dcAi) + (1 \u2212Ai) log(1 \u2212\u02dcAi))) \u2212\n\u03b3KL[q(ZG|HG, A)||P(Z)]];\n\u03d5 \u2190\u03d5 \u2212\u03b1 \u00b7 RMSProp(\u03d5, G\u03d5);\n10:\nUpdate Discriminator:\n11:\nFreeze the the parameter \u03d5 of generator G\u03d5;\n12:\nUnfreeze the the parameter \u03c9 of discriminator D\u03c9;\n13:\nRepeat steps 7 - 8;\n14:\nUpdate the parameter \u03c9 of generator D\u03c9 by\nD\u03c9 \u2190\u2207[\u22121\nm\nPm\ni=1 D\u03c9(Xi, Ai) + 1\nm\nPm\ni=1 D\u03c9(G\u03d5(Xi, Ai))];\n\u03c9 \u2190\u03c9 \u2212\u03b1 \u00b7 RMSProp(\u03c9, D\u03c9);\n\u03c9 \u2190clip(\u03c9, \u2212c, c);\n15:\nend for\n16: end for\n17: Compute anomaly detection scores for test graphs via the trained discriminator D\u03c9;\n18: return The anomaly detection scores.\nAlgorithm 3 SDGG-NAT\nInput: Input graph set G, number of GIN layers K, clipping parameter c, learning rate \u03b1, batch size m, total training\nepochs T .\nOutput: The anomaly detection scores s.\n1: Initialize the network parameters \u03d5, \u03b8 for anomalous generator G\u03d5 and classifier f\u03b8;\n2: for t \u2192T do\n3:\nfor each batch G do\n4:\nExtrat graph-level representation with the input normal attributes X and adjacency matrix A via Eq. (4) and\n(5);\n5:\nGenerate anomalous anomalous graph set \u02dcG from generator G\u03d5 with normal attributes X and adjacency matrix\nA via Eq. (7), (8), (9), and (10);\n6:\nCalculate the anomalous reconstruction loss via Eq. (11)\n7:\nCalculate the total loss via Eq. (13)\n8:\nUpdate the parameter \u03d5 and \u03b8 of anomalous generator G\u03d5 and classifier f\u03b8 using backpropagation;\n9:\nend for\n10: end for\n11: Compute anomaly detection scores for test graphs via the trained classifier f\u03b8;\n12: return The anomaly detection scores.\n14\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nD\nDetailed experimental settings\nWe supplement more details of the experimental settings in the paper, which includes the network architecture of the\nproposed methods, settings of trade-off parameters, training details, data split, baseline settings, etc.\n\u2022 Network architecture: For the network architecture of the proposed SDGG-ATI, SDGG-ATII, and SDGG-\nNAT, we utilize a 3-layer GIN as the backbone network for the generator and discriminator (classifier), except\nthe generator of SDGG-ATI which is an MLP-based neural network. The aggregated dimension and the\nlatent dimension in our method are set to 16 and 10, respectively. The source code is also included in the\nsupplementary material to ensure the reproducibility of our methods.\n\u2022 Trade-off parameters: The coefficient of anomalous reconstruction loss \u03bb varies in {0.01, 0.1, 1, 10}. The\nspecific value is chosen according to resist the influence of classifier loss. The other parameter \u03b2 of KL-\ndivergence loss is 1e-5 and the clip value of the adversarial loss is fixed by 0.01. Furthermore, we further\nassess the impact of variations in the values of \u03bb and \u03b2 on performance in Appendix G.\n\u2022 Training details: For small-scale graph datasets, we utilize a fixed batch size of 4, while we increase the\nbatch size to 256 to accommodate the requirement of experiment on larger-scale datasets. Besides, we set\nthe learning rate \u03b1 to 0.001 and the total training epochs to 300, utilizing RMSprop [Tieleman et al., 2012]\noptimizer for SDGG-ATI and SDGG-ATII, and Adam [Kingma and Ba, 2014] for SDGG-NAT.\n\u2022 Data split: For small and moderate scale datasets, we allocate 80% of the data from the normal class for\ntraining, and subsequently construct the testing data by combining the retained normal data with an equal\nor smaller number of anomalous data samples. For large-scale imbalanced datasets, we allocate 80% of the\ndata in the normal class as the training set, and form the test set with the rest of the normal data and all the\nabnormal data.\n\u2022 Baseline settings: Particularly, we utilize the one-class support vector machine (OCSVM) to achieve anomaly\ndetection for all graph kernel baselines and InfoGraph. For other baselines, we follow the settings in their\npapers and report the reproduced results. Note that we select the best results achieved throughout the training\nepochs for all algorithms to ensure a fair comparison.\n\u2022 Implementation: Note we leverage PyTorch Geometric [Fey and Lenssen, 2019] for implementation, and all\nexperiments are executed on NVIDIA Tesla A100 GPU with AMD EPYC 7532 CPU.\nE\nMore experimental results\nIn this section, we present additional experimental results for the one-class classification tasks. Tables 6 and 7 show\nthe anomaly detection results for the remaining experimental datasets. Notably, the proposed methods, SDGG-ATI,\nSDGG-ATII, and SDGG-NAT, consistently outperform other baselines by a significant margin, underscoring the\nsuperiority of our approaches. Besides, we evaluate the anomaly detection performance on the multi-class graph\ndataset ENZYMES, which contains 6 classes in total. Table 8 presents the experimental results for each class. Our\nmethods consistently demonstrate remarkable effectiveness across all classes of ENZYMES, surpassing all baselines\nby more than 20%. Table 10 summarizes the overall analysis of SDGG-ATI, SDGG-ATII, and SDGG-NAT across all\nbenchmarks. It is evident that, in the majority of cases, SDGG-NAT achieves the best AUCs. Additionally, SDGG-NAT\nconsistently maintains std \u22645% across 24 out of 27 cases, and it exhibits the smallest standard deviation compared to\nSDGG-ATI/ATII in more cases. This provides strong evidence for the stability of SDGG-NAT.\nTo further demonstrate the effectiveness of the proposed SDGG methods, we also evaluate the proposed method with\ndifferent metrics, e.g., F1 score, recall, and AUCPR. Table 9 shows the experimental results of several comparative\nmethods on four graph benchmarks. We can observe that the three variants of the proposed SDGG exhibit superiority\ncompared to other baseline methods across all metrics, and SDGG-NAT also outperforms SDGG-ATI and SDGG-ATII\nin most cases.\nFigure 6 offers a visual illustration of why the SDGG can outperform other supervised methods in the context of\nlarge-scale imbalanced benchmarks. This figure demonstrates a binary classification scenario where the anomalous data\nused for training lies on the right side of the normal data, and the trained classifier successfully categorizes them with a\nred decision boundary. However, there may be unknown anomalies located on the left side of the decision boundary\n(shown by the blue dashed line), where the binary classifier fails to detect them. This phenomenon is common in\nlarge-scale unbalanced anomaly detection due to limited supervised information, where the distribution of test data is\nnot exactly the same as that of the training data.\n15\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nNormal samples\nAnomalous samples\nTraining data\nAnomalous samples\nNormal samples\nUnknown anomalous sample\nUnknown anomalous sample\nTesting data\nBinary classification\nIdeal decision boundary\nDecision Boundary\nFigure 6: The illustration of the idea of SDGG. The red line denotes the decision boundary learned by the binary\nclassification, whereas the blue dashed line denotes the ideal decision boundary.\nTable 6: Average AUCs with standard deviation (10 trials) of different graph-level anomaly detection algorithms. We\nassess models by regarding every data class as normal data, respectively. The best results are highlighted in bold and\n\u201c\u2013\u201d means out of memory.\nMethod/Dataset\nCOX2\nER_MD\nDD\n0\n1\n0\n1\n0\n1\nSP [Borgwardt and Kriegel, 2005]\n54.08\u00b10.00\n57.60\u00b10.00\n40.92\u00b10.00\n38.24\u00b10.00\n68.56\u00b10.00\n44.74\u00b10.00\nWL [Shervashidze et al., 2011]\n59.90\u00b10.00\n50.57\u00b10.00\n45.71\u00b10.00\n32.62\u00b10.00\n73.97\u00b10.00\n49.46\u00b10.00\nNH [Hido and Kashima, 2009]\n48.41\u00b10.00\n47.17\u00b10.00\n51.55\u00b12.00\n36.48\u00b10.00\n74.24\u00b10.00\n36.84\u00b10.00\nRW [Vishwanathan et al., 2010]\n52.43\u00b10.00\n65.53\u00b10.00\n48.20\u00b10.00\n34.84\u00b10.00\n\u2013\n\u2013\nOCGIN [Zhao and Akoglu, 2021]\n59.64\u00b15.78\n56.83\u00b17.68\n72.20\u00b10.16\n70.08\u00b10.56\n66.59\u00b14.44\n60.03\u00b15.34\nInfoGraph [Sun et al., 2020]\n48.25\u00b16.24\n50.29\u00b17.00\n53.12\u00b115.45\n56.82\u00b17.04\n39.42\u00b14.36\n64.84\u00b12.36\nGLocalKD [Ma et al., 2022]\n51.42\u00b10.66\n65.79\u00b10.98\n57.81\u00b117.90\n71.54\u00b10.00\n19.52\u00b10.00\n22.03\u00b10.01\nOCGTL [Qiu et al., 2022]\n55.41\u00b13.20\n48.62\u00b12.24\n27.55\u00b13.17\n69.15\u00b12.07\n69.90\u00b12.60\n67.67\u00b12.80\nVGAE-AD [Kipf and Welling, 2016]\n59.28\u00b11.55\n73.33\u00b11.48\n59.89\u00b17.08\n59.48\u00b16.94\n51.57\u00b10.94\n64.95\u00b15.34\nSDGG-ATI\n71.00\u00b18.67\n79.45\u00b15.82\n90.52\u00b12.93\n95.05\u00b11.48\n80.87\u00b11.76\n88.50\u00b13.22\nSDGG-ATII\n68.56\u00b15.59\n91.78\u00b13.95\n88.10\u00b10.53\n93.61\u00b11.76\n91.73\u00b10.74\n79.09\u00b126.38\nSDGG-NAT\n73.91\u00b111.52\n97.05\u00b11.62\n98.74\u00b11.59\n96.67\u00b11.67\n90.71\u00b11.17\n97.71\u00b11.76\nTable 7: Average AUCs with standard deviation (10 trials) of different graph-level anomaly detection algorithms. We\nassess models by regarding every data class as normal data, respectively. The best results are highlighted in bold and\n\u201c\u2013\u201d means out of memory.\nMethod/Dataset\nIMDB-Binary\nCOLLAB\n0\n1\n0\n1\n2\nSP [Borgwardt and Kriegel, 2005]\n45.92\u00b10.00\n47.16\u00b10.00\n59.10\u00b10.00\n83.97\u00b10.00\n79.02\u00b10.00\nWL [Shervashidze et al., 2011]\n51.57\u00b10.00\n46.07\u00b10.00\n51.22\u00b10.00\n80.54\u00b10.00\n79.96\u00b10.00\nNH [Hido and Kashima, 2009]\n53.21\u00b10.00\n46.52\u00b10.00\n59.76\u00b10.00\n80.54\u00b10.00\n64.14\u00b10.00\nRW [Vishwanathan et al., 2010]\n49.51\u00b10.00\n53.11\u00b10.00\n\u2013\n\u2013\n\u2013\nOCGIN [Zhao and Akoglu, 2021]\n40.47\u00b110.83\n44.22\u00b14.99\n42.17\u00b16.06\n75.65\u00b120.35\n19.06\u00b18.57\nInfoGraph [Sun et al., 2020]\n63.53\u00b12.77\n58.36\u00b19.95\n56.62\u00b15.97\n79.26\u00b19.86\n40.62\u00b19.78\nGLocalKD [Ma et al., 2022]\n53.83\u00b11.24\n53.34\u00b10.06\n46.38\u00b10.03\n50.16\u00b10.20\n52.98\u00b10.04\nOCGTL [Qiu et al., 2022]\n65.10\u00b11.80\n64.12\u00b11.27\n65.04\u00b14.33\n89.08\u00b12.39\n40.29\u00b15.41\nVGAE-AD [Kipf and Welling, 2016]\n65.36\u00b10.78\n67.22\u00b13.49\n50.96\u00b10.04\n\u2013\n\u2013\nSDGG-ATI\n62.92\u00b10.62\n86.53\u00b10.00\n65.85\u00b17.20\n82.82\u00b10.32\n73.57\u00b17.55\nSDGG-ATII\n90.51\u00b12.79\n87.93\u00b10.25\n54.49\u00b16.79\n82.95\u00b10.45\n78.68\u00b10.43\nSDGG-NAT\n93.37\u00b11.57\n93.07\u00b11.54\n87.99\u00b16.21\n92.82\u00b13.18\n94.74\u00b11.40\n16\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nTable 8: Average AUCs with standard deviation (5 trials) of the proposed models on ENZYMES dataset. The best\nresults are highlighted in bold and \u201c\u2013\u201d means out of memory.\nMethod/Dataset\nENZYMES\n0\n1\n2\n3\n4\n5\nSP [Borgwardt and Kriegel, 2005]\n58.30\u00b10.00\n49.50\u00b10.00\n48.75\u00b10.00\n57.55\u00b10.00\n65.20\u00b10.00\n63.45\u00b10.00\nWL [Shervashidze et al., 2011]\n56.50\u00b10.00\n49.70\u00b10.00\n60.05\u00b10.00\n54.10\u00b10.00\n44.85\u00b10.00\n50.85\u00b10.00\nNH [Hido and Kashima, 2009]\n58.09\u00b11.21\n52.70\u00b10.33\n55.93\u00b11.03\n56.70\u00b10.65\n44.28\u00b10.85\n66.79\u00b10.53\nRW [Vishwanathan et al., 2010]\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\n\u2013\nOCGIN [Zhao and Akoglu, 2021]\n56.68\u00b13.53\n67.43\u00b14.25\n62.18\u00b12.93\n44.74\u00b15.11\n53.88\u00b11.17\n62.95\u00b17.16\nInfoGraph [Sun et al., 2020]\n71.70\u00b10.00\n52.67\u00b14.78\n50.90\u00b12.01\n71.10\u00b10.00\n46.02\u00b12.91\n55.23\u00b11.34\nGLocalKD [Ma et al., 2022]\n58.27\u00b10.57\n63.44\u00b10.11\n53.13\u00b10.06\n56.50\u00b10.04\n59.23\u00b10.11\n63.08\u00b10.06\nOCGTL [Qiu et al., 2022]\n61.58\u00b12.13\n55.04\u00b11.34\n46.10\u00b10.32\n61.74\u00b11.59\n62.28\u00b12.29\n56.92\u00b12.72\nVGAE-AD [Kipf and Welling, 2016]\n58.38\u00b10.67\n57.79\u00b11.38\n56.39\u00b13.26\n\u2013\n\u2013\n\u2013\nSDGG-ATI\n86.93\u00b15.36\n77.71\u00b11.78\n75.52\u00b14.12\n92.04\u00b12.09\n75.91\u00b11.77\n85.16\u00b13.54\nSDGG-ATII\n81.42\u00b13.02\n72.53\u00b13.71\n67.39\u00b16.51\n90.73\u00b12.95\n79.85\u00b17.84\n76.67\u00b12.49\nSDGG-NAT\n89.28\u00b13.13\n90.42\u00b13.55\n79.70\u00b12.65\n95.29\u00b11.35\n87.51\u00b11.24\n89.60\u00b18.02\nTable 9: Average F1-score, Recall and AUCPR (%) with standard deviation (%) (10 trials) on large-scale imbalanced\ngraph datasets. The best results are marked in bold.\nMetric\nMethod/Dataset\nPC-3\nMCF-7\nPROTEINS (0)\nPROTEINS (1)\nF1-score\nOCGTL [Qiu et al., 2022]\n62.33\u00b10.46\n79.34\u00b10.31\n66.50\u00b10.00\n66.42\u00b10.00\nGLocalKD [Ma et al., 2022]\n40.78\u00b10.14\n68.03\u00b10.02\n66.73\u00b15.41\n68.25\u00b10.00\nSDGG-ATI\n88.38\u00b10.13\n82.61\u00b10.00\n64.68\u00b15.19\n71.43\u00b10.00\nSDGG-ATII\n88.56\u00b10.13\n83.06\u00b10.12\n71.46\u00b12.43\n70.83\u00b10.12\nSDGG-NAT\n93.36\u00b12.30\n82.92\u00b10.10\n71.93\u00b10.16\n71.52\u00b10.13\nRecall\nOCGTL [Qiu et al., 2022]\n87.22\u00b10.64\n93.63\u00b10.36\n100.00\u00b10.00\n100.00\u00b10.00\nGLocalKD [Ma et al., 2022]\n93.24\u00b10.06\n85.85\u00b10.02\n93.18\u00b17.50\n95.56\u00b10.00\nSDGG-ATI\n99.06\u00b10.14\n97.48\u00b10.05\n90.40\u00b17.25\n100.00\u00b10.00\nSDGG-ATII\n99.26\u00b10.03\n98.02\u00b10.14\n97.47\u00b12.50\n100.00\u00b10.00\nSDGG-NAT\n99.23\u00b10.11\n99.41\u00b10.06\n100.00\u00b10.00\n100.00\u00b10.00\nAUCPR\nOCGTL [Qiu et al., 2022]\n47.46\u00b10.78\n64.98\u00b12.54\n75.00\u00b10.00\n75.00\u00b10.00\nGLocalKD [Ma et al., 2022]\n41.33\u00b10.02\n46.62\u00b10.00\n62.30\u00b119.24\n82.12\u00b10.13\nSDGG-ATI\n95.19\u00b10.63\n89.69\u00b11.14\n75.67\u00b127.48\n96.45\u00b10.04\nSDGG-ATII\n95.69\u00b10.27\n90.03\u00b10.05\n92.78\u00b17.31\n96.14\u00b10.01\nSDGG-NAT\n95.87\u00b10.12\n90.87\u00b10.25\n97.67\u00b10.36\n96.58\u00b10.07\nTable 10: Overall analysis for the three variants of SDGG across all graph benchmarks.\nSDGG-ATI\nSDGG-ATII\nSDGG-NAT\nMin std\n9/27\n10/27\n12/27\nstd\u22645%\n18/27\n19/27\n24/27\nBest AUC\n3/27\n4/27\n23/27\n17\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nF\nMore visualization results\nIn this section, we provide additional visualization results of our methods. Figure 7 illustrates the discriminative scores\nof SDGG-ATI, SDGG-ATII, and SDGG-NAT on MUTAG. The top row represents the discriminative scores in the\ntraining stage, while the bottom row corresponds to the testing stage. It is evident that during the training phase, our\nmethods effectively differentiate between the generated anomalous data and normal data, and this distinction carries\nover to the testing phase. These findings validate that the classifier trained using high-quality generated anomalous\ngraphs can identify outstanding decision boundaries and exhibits excellent generalization capabilities during the testing\nstage. Particularly, despite observing score overlap between the generated anomaly and normal graphs during the\ntraining phase of SDGG-ATII, significant differentiation is still achieved during the testing phase. This can be attributed\nto our objective of training a powerful classifier by generating high-quality anomaly graphs that closely resemble normal\ngraphs. Although the classifier may not separate these anomalies adequately during training. This may possibly be\ndue to the over-idealization of the generated anomaly data, the learned decision boundaries are sufficiently effective in\ndistinguishing the anomalies during the test phase.\nAdditionally, we present the 2-D and 3-D t-SNE visualizations (Figure 8 and 9) to provide a comprehensive assessment\nof the effectiveness of the proposed SDGG-ATI, SDGG-ATII, and SDGG-NAT. These visualizations offer compelling\ninsights into the learned decision boundaries derived from the generated anomalous data. Examining these visualizations\nin detail, we can observe that anomalous data, distinctly highlighted in green, are conspicuously separated from the\nremaining data points. This distinct separation serves as compelling evidence of the discriminative power embedded\nwithin our methods. Through these insightful visualizations, we gain a deeper understanding of the proposed methods,\nvividly illustrating their ability to learn effective decision boundaries and unveil intricate patterns and anomalies hidden\nwithin complex graph structures.\n0.844\n0.84405\n0.8441\n0.84415\n0.8442\n0.84425\n0.8443\nScore\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\nFrequency Density\nAnomalous Data\nNormal Data\n-0.02\n-0.018\n-0.016\n-0.014\n-0.012\n-0.01\nScore\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nFrequency Density\nAnomalous Data\nNormal Data\n(a) SDGG-ATI Testing Stage\n(b) SDGG-ATII Training Stage\n-0.5\n-0.4\n-0.3\n-0.2\n-0.1\n0\nScore\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFrequency Density\nGenrated Anomalous Data\nNormal Data\n(c) SDGG-NAT Training Stage\n(a) SDGG-ATI Training Stage\n(b) SDGG-ATII Testing Stage\n(c) SDGG-NAT Testing Stage\n(a) SDGG-ATI\n(b) SDGG-ATII\n(c) SDGG-NAT\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\nScore\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nFrequency Density\nGenrated Anomalous Data\nNormal Data\nFigure 7: The discriminative score visualizations of the proposed models on MUTAG Class 1. The top row is the result\nof training stage and the bottom row is that of testing stage.\nG\nParameter sensitivity analysis\nWe investigate the impact of two main hyper-parameters, \u03bb and \u03b3, in SDGG-ATII and SDGG-NAT on the anomaly\ndetection performance. Note that SDGG-ATI is not included in this analysis because its loss function does not have\nany hyper-parameter. Specifically, we set the range of value for \u03bb and \u03b3 from 0.001 to 100 and evaluate their influence\non COX2. Figure 10 shows the experimental results, where we have the following observations. First, we find that\na balanced trade-off of \u03bb and \u03b3 is crucial for achieving ideal performance in SDGG-ATII and SDGG-NAT. Either\ntoo large or too small values will generally lead to sub-optimal results. Second, both SDGG-ATII and SDGG-NAT\n18\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\n-20\n0\n20\n-20\n0\nAnomalous Data\nTraining Normal Data\nTesting Normal Data\nGenerated Anomalous Data\n(a) SDGG-ATI\n-20\n0\n20\n-20\n0\n20\nAnomalous Data\nTraining Normal Data\nTesting Normal Data\nGenerated Anomalous Data\n(b) SDGG-ATII\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40\n50\n-50\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40\nAnomalous Data\nTraining Normal Data\nTesting Normal Data\nGenerated Anomalous Data\n(c) SDGG-NAT\nFigure 8: The 2-D t-SNE visualizations of the proposed models on AIDS Class 1.\n-20\n-5\n10\n0\n-10\n5\n5\n0\n0\n10\n-5\n-10\n15\n10\n20\n-5\n20\n0\n5\n10\n10\n10\n15\n0\n0\n20\n-10\n-10\n-20\n-20\n-30\n10\n-20\n-10\n20\n0\n10\n0\n10\n20\n0\n-10\n-10\n-20\n-20\n(c) SDGG-NAT\n(b) SDGG-ATII\n(a) SDGG-ATI\nFigure 9: The 3-D t-SNE visualizations of the proposed models on AIDS Class 1. The legend is set the same as Figure\n8.\n(c) Back\n(a) SDGG-ATII\n(b) SDGG-NAT\n    \n         \n    \n      \n \n  \n  \n  \n  \n  \n  \n  \n  \n  \n   \n       \n           \n            \n          \n          \nFigure 10: Parameter sensitivity analysis of SDGG-ATII and SDGG-NAT on COX2. \u03bb and \u03b3 changes in the range of\n[0.001, 100].\n19\nSelf-Discriminative Modeling for Anomalous Graph Detection\nA PREPRINT\nexhibit relatively stable performance across a wide range of \u03bb and \u03b3 values, which demonstrates the effectiveness of\nour methods. Third, we can observe that the two hyper-parameters cause less significant influence on the performance\nof SDGG-NAT than SDGG-ATII. This further demonstrates that the non-adversarial variant of the proposed method\nexhibits greater stability and robustness.\nH\nComparison between VGAE-based and GIN-based backbones\nIn this section, we conduct a thorough comparison between VGAE-based and GIN-based generators to elucidate our\nrationale for choosing VGAE as the preferred backbone for generators in our methods. It should be noted that the\nkey difference between VGAE-based and GIN-based backbones lies in the incorporation of variational inference that\nintroduces stochasticity in generating anomalous graphs. Figure 11 presents a comprehensive performance comparison\nin terms of AUC across three datasets. Notably, the VGAE-based backbone consistently outperforms the GIN-based\nbackbone by a substantial margin. This significant improvement can be attributed to the inherent disparities in their\nrespective generation processes. The GIN-based backbone generates graphs deterministically, while the VGAE-based\ngenerator incorporates stochasticity. In contrast to the GIN-based backbone, VGAE employs the reparameterization\ntechnique to learn a target distribution, allowing it to capture the data and underlying distribution. Consequently,\nthe generated pseudo-anomalous data is more likely to reside in plausible regions, rather than simply approximating\nthe original data. The experiment demonstrates the exceptional ability of the VGAE-based backbone to generate\nhigh-quality pseudo-anomalous data, yielding superior performance in graph-level anomaly detection tasks. This aligns\nwith the motivation and expectation depicted in Figure 1 of the main paper, where the stochasticity in the generation\nprocess plays an important role in learning a good decision boundary.\n(c) Backbone Comparison\nCOX2\n(Class 0)\nCOX2\n(Class 1)\nDD\n(Class 0)\nDD\n(Class 1)\nMUTAG\n(Class 0)\nMUTAG\n(Class 0)\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nAUC (%)\nSDGG-ATII_GIN\nSDGG-ATII_VGAE\nSDGG-NAT_GIN\nSDGG-NAT_VGAE\nFigure 11: Comparison between VGAE-based and GIN-based generator.\n20\n",
    "2402.12761": "FGAD: Self-boosted Knowledge Distillation for An Effective\nFederated Graph Anomaly Detection Framework\nJinyu Cai\u2217\njinyucai@nus.edu.sg\nInstitute of Data Science, National\nUniversity of Singapore\nSingapore\nYunhe Zhang\u2217\nzhangyhannie@163.com\nSchool of Data Science, The Chinese\nUniversity of HongKong (Shenzhen)\nChina\nZhoumin Lu\nwalker.zhoumin.lu@gmail.com\nSchool of Computer Science,\nNorthwest Polytechnical University\nChina\nWenzhong Guo\nguowenzhong@fzu.edu.cn\nCollege of Computer and Data\nScience, Fuzhou University\nChina\nSee-Kiong Ng\nseekiong@nus.edu.sg\nInstitute of Data Science, National\nUniversity of Singapore\nSingapore\nABSTRACT\nGraph anomaly detection (GAD) aims to identify anomalous graphs\nthat significantly deviate from other ones, which has raised grow-\ning attention due to the broad existence and complexity of graph-\nstructured data in many real-world scenarios. However, existing\nGAD methods usually execute with centralized training, which\nmay lead to privacy leakage risk in some sensitive cases, thereby\nimpeding collaboration among organizations seeking to collectively\ndevelop robust GAD models. Although federated learning offers\na promising solution, the prevalent non-IID problems and high\ncommunication costs present significant challenges, particularly\npronounced in collaborations with graph data distributed among\ndifferent participants. To tackle these challenges, we propose an ef-\nfective federated graph anomaly detection framework (FGAD). We\nfirst introduce an anomaly generator to perturb the normal graphs\nto be anomalous, and train a powerful anomaly detector by dis-\ntinguishing generated anomalous graphs from normal ones. Then,\nwe leverage a student model to distill knowledge from the trained\nanomaly detector (teacher model), which aims to maintain the per-\nsonality of local models and alleviate the adverse impact of non-IID\nproblems. Moreover, we design an effective collaborative learning\nmechanism that facilitates the personalization preservation of lo-\ncal models and significantly reduces communication costs among\nclients. Empirical results of the GAD tasks on non-IID graphs com-\npared with state-of-the-art baselines demonstrate the superiority\nand efficiency of the proposed FGAD method.\n\u2217Both authors contributed equally to this research.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\n\u00a9 2018 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Artificial intelligence; Neu-\nral networks; \u2022 Security and privacy \u2192Intrusion/anomaly\ndetection and malware mitigation;\nKEYWORDS\nUnsupervised Learning, Graph Anomaly Detection, Federated Learn-\ning\nACM Reference Format:\nJinyu Cai, Yunhe Zhang, Zhoumin Lu, Wenzhong Guo, and See-Kiong\nNg. 2018. FGAD: Self-boosted Knowledge Distillation for An Effective\nFederated Graph Anomaly Detection Framework. In Proceedings of Make\nsure to enter the correct conference title from your rights confirmation emai\n(Conference acronym \u2019XX). ACM, New York, NY, USA, 12 pages. https:\n//doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nAnomaly detection [3, 27] is a fundamental research problem in\nmachine learning, and it has been extensively explored in various\ndomains such as images [2, 15] and time-series data [1, 4, 20]. In\nthe real world, graph-structured data is commonly available due to\nits exceptional ability to represent complicated relationship infor-\nmation among entities [43]. This is particularly evident in domains\nlike social networks and medical applications. Consequently, graph\nanomaly detection (GAD) [5, 24], which aims to identify graphs that\nexhibit significant deviations from other normal graphs, has raised\nbroad attention in recent years. With the advancement of graph\nneural networks (GNNs) [13, 37], GAD has made remarkable strides\nand demonstrated promising performance in detecting anomalies\nacross many real-world scenarios with natural graph-structured\ndata, e.g., social networks, molecules, and bioinformatics.\nIn realistic collaborative efforts among different companies and\norganizations, they attempt to share knowledge with each other in\norder to more accurately detect anomalies. However, existing GAD\napproaches [9, 23, 31, 40] typically involve a centralized model that\nrequires all participants to provide their own data for training a\nglobal model, as shown in Figure 1(a). Although this centralized\ntraining simplifies coordination, it introduces a critical privacy\narXiv:2402.12761v1  [cs.LG]  20 Feb 2024\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\nleakage risk. Graph data may encompass some sensitive informa-\ntion that the participant is not willing to share, e.g., the private\nrelationship in social networks, which then hinders their collabora-\ntions. Consequently, an urgent imperative emerges to investigate\napproaches that facilitate collaboration between GAD models dis-\ntributed to different participants while protecting their privacy.\nLocal \nmodel\n\u22ef\nClient 1\n\u22efLocal \ndata\nClient k\nCollected\nData\nGlobal model\n(a) Centralized learning approach\n(b) Federated learning approach\nUpload data\nParameter \nAggregation\nLocal \ndata\nClient 1\nLocal \nmodel\nClient k\nServer\nUpload training \nparameters\n\ud835\udc161\n\ud835\udc16\ud835\udc58\n\u22ef\n\ud835\udc161\n\ud835\udc16\ud835\udc58\nFigure 1: Overview of the centralized learning and federated\nlearning frameworks.\nAs the emerging technique in machine learning, federated learn-\ning (FL), as shown in Figure 1(b), enables the collaboration between\ndifferent participants with the consideration of privacy-preserving.\nThe clients in FL only need to share their network parameters with\nthe server rather than their local data, which prevents the leakage\nof sensitive information in participants. Classical FL methods, such\nas FedAvg [25] and FedProx [18], have become the paradigm of col-\nlaborative learning across various domains [14, 35]. To facilitate the\ncollaborative training of GNN models for graph data across clients,\nfederated graph learning (FGL) [8, 19, 41] has also been widely\nstudied in recent years. FGL methods [30, 36] integrate GNNs with\nFL methods to collaboratively learn representations for complicated\ngraph data distributed in various clients, and have demonstrated\nsuperiority in graph classification tasks. Therefore, an intuitive\napproach to address the above issue is to integrate the existing\nadvancements in FL and FGL with general anomaly detection tech-\nniques, e.g., deep one-class classification (DeepSVDD) [29].\nHowever, this solution may encounter the following challenges:\n(1) The graph data distributed in various clients often exhibits\nsignificant heterogeneity and non-IID property [10, 36], e.g.,\ncontaining different graph structures or feature dimensions.\nThese factors place a higher demand on maintaining the validity\nof the local models for their own data, e.g., personalization.\n(2) It is difficult to learn a universal hypersphere as the decision\nboundary for highly heterogeneous graph data under the fed-\nerated learning setting. Besides, such non-IID graphs across\nclients hardly conform to the assumption in DeepSVDD that\ntheir latent distribution could follow a universal hypersphere.\n(3) Existing collaborative learning mechanisms, e.g., FedAvg [25],\nrequire transmitting all network parameters of each client in a\nsingle communication round, which brings substantial commu-\nnication costs in applications.\nThose challenges naturally lead to a research question: Can we\ndesign an FL-based GAD framework that facilitates more ef-\nfective collaboration and achieves more accurate detection?\nIn this paper, we propose an effective federated graph anomaly\ndetection (FGAD) framework, as shown in Figure 2, to answer this\nresearch question. To improve the anomaly detection capability in\nthe local model, we introduce an anomaly generator that perturbs\nnormal graphs to be anomalous, and train a classifier to identify\nanomalies from normal graphs. The generated anomalous graphs\nare encouraged to be diverse and resemble normal ones through\niterations, so that more robust decision boundaries can be learned\nin a self-boosted manner. To alleviate the adverse impact of non-IID\nproblems, we propose to preserve the personalization of each client\nby leveraging knowledge distillation. Specifically, we introduce a\nstudent model to distill the knowledge from the trained classifier\n(teacher model). The student model only takes the normal graphs as\nthe input, with the aim of aligning its predicted distributions with\nthat of the teacher model. Moreover, we further design an effective\ncollaborative learning mechanism. We let the student and teacher\nmodels share the same backbone network to streamline the capacity\nof local models. Besides, we engage only the parameters of the\nstudent head rather than the entire model in collaborative learning,\nwhich allows the teacher model to preserve the personalization of\na client. In this way, we not only alleviate the adverse impact of\nnon-IID property, but also reduce the communication costs between\nclients and server during collaborative learning. The contributions\nof this paper are summarized as follows:\n\u2022 We investigate the challenging anomaly detection issue on non-\nIID graphs distributed across various clients, and propose an\neffective federated graph anomaly detection (FGAD) framework.\n\u2022 We introduce a self-boosted distillation module, which not only\npromotes the detecting capability by identifying self-generated\nanomalies, but also maintains the personalization of local models\nfrom knowledge distillation to alleviate non-IID problems.\n\u2022 We propose an effective collaborative learning mechanism that\nstreamlines the capacity of local models and reduces communi-\ncation costs with the server.\n\u2022 We establish a comprehensive set of baselines for federated graph\nanomaly detection. Extensive experiments also validate the ef-\nfectiveness of the proposed FGAD method.\n2\nRELATED WORKS\n2.1\nGraph Anomaly Detection\nGraph anomaly detection (GAD) [24] refers to detecting abnormal\ngraphs that significantly differ from other normal ones, which have\nreceived growing attention in recent years owing to the ubiqui-\ntous prevalence of graph-structured data in real-world scenarios,\nsuch as social networks [22]. There are many works that advance\nthe research on GAD. For instance, Zhao et al. [42] investigated\ngraph-level anomaly detection issues by integrating graph iso-\nmorphism network (GIN) [37] with deep one-class classification\n(DeepSVDD) [29]. Qiu et al. [28] leveraged neural transformation\nlearning to develop a more robust GAD model to overcome the\nperformance flip issue. Ma et al. [23] utilized knowledge distillation\nto capture more comprehensive normal patterns from the global\nand local views for detecting graph anomalies.\nFGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nAnomaly \nGenerator\nStudent Head\nTeacher Head\nKnowledge \nDistillation\n\ud835\udc16\ud835\udc16s\n\ud835\udc16\ud835\udc16t\nPrediction\nPrediction\nClient\n\ud835\udc16\ud835\udc16s,\ud835\udc58\ud835\udc58\n\ud835\udc16\ud835\udc16t,\ud835\udc58\ud835\udc58\nClient k\n\u22ef\n\ud835\udc16\ud835\udc16s,1\n\u22ef\n\ud835\udc16\ud835\udc16s,\ud835\udc58\ud835\udc58\n\u0d25\n\ud835\udc16\ud835\udc16s\nCollaborative learning\nServer\nParameter Aggregation\nOriginal  Graph \ud835\udc3a\ud835\udc3a\nAnomalous Graph \u0de8\ud835\udc3a\ud835\udc3a\nGNN\nGNN\nWeight-Shared\nGNN\n\ud835\udc16\ud835\udc16g\n\ud835\udc16\ud835\udc16g,\ud835\udc58\ud835\udc58\n\ud835\udc16\ud835\udc16s,1\n\ud835\udc16\ud835\udc16t,1\nClient 1\n\ud835\udc16\ud835\udc16g,1\nAnomaly \nscores\nTeacher\nStudent\nFigure 2: Overview of the FGAD framework. Note that the teacher model utilizes both normal and generated anomalous graphs\nfor training an anomaly detector, while the student model only inputs normal graphs for the distillation of normal patterns.\nAlthough these GAD methods have achieved remarkable success,\nthey primarily rely on centralized training paradigms. Nevertheless,\nin real-world collaborative scenarios, the graph data is often dis-\ntributed across various clients, which necessitates the transmission\nof local graph data to a central server during practical collabora-\ntions. Unfortunately, this process can potentially expose sensitive\ninformation and pose severe privacy risks. Additionally, the inher-\nent non-IID property in the graph data distributed across diverse\nclients presents yet another formidable challenge. Consequently,\nthe pursuit of effective solutions to address these challenges remains\nan open research problem.\n2.2\nFederated Graph Learning\nFederated learning (FL) approaches [10, 17, 39], such as FedAvg [25],\nFedProx [18], provide a promising solution for collaboratively train-\ning models with data distributed in different clients, while pre-\nserving their privacy. In FL, clients only share their network pa-\nrameters rather than data with the central server, which mitigates\nthe privacy leakage risk and enables clients to share and leverage\nknowledge from others. As an emerging technique, FL has not\nonly made remarkable advancements in image [6, 16, 38] and time\nseries data [21, 33], but also raised increasing attention to graph\ndata [34, 41], where collaborative efforts are significantly more\nchallenging due to the complex structural information and hetero-\ngeneous characteristic of graphs compared to other data types.\nFederated graph learning (FGL) [41] aims to facilitate the col-\nlaboration of GNNs distributed in multiple remote clients to meet\nthe requirement of handling complicated non-IID graph data that\nwidely exist in many real-world scenarios, e.g., social networks,\nmedical, and biological data. For example, Xie et al. [36] studied the\nfederated learning issue on non-IID graphs by integrating the clus-\ntered federated learning with graph isomorphism network (GIN),\nwhich achieves effective collaborations for distributed GINs. Tan et\nal. [30] designed a structural knowledge-sharing mechanism to fa-\ncilitate the federated graph learning process. However, existing FGL\nmethods have primarily been validated for graph classification tasks,\nand their effectiveness in addressing the intricate unsupervised task\nof graph anomaly detection remains an area of ongoing exploration.\nWhile it is possible to extend these FL/FGL [18, 25, 30, 36] methods\nto address GAD tasks by integrating them with classical solutions\nlike DeepSVDD [24, 29], it is imperative to acknowledge some sig-\nnificant challenges, e.g., the adverse impact of the non-IID problem\nacross different clients and the communication costs of transmitting\ncomplex GNN model parameters during collaborative learning.\n3\nMETHODOLOGY\n3.1\nPreliminary and Problem Formulation\nNotation: Let \ud835\udc37= {\ud835\udc3a1, . . . ,\ud835\udc3a\ud835\udc41} denotes a graph dataset which\nconsists of \ud835\udc41graphs, and each graph \ud835\udc3a\ud835\udc56= {\ud835\udc49\ud835\udc56, \ud835\udc38\ud835\udc56} in the graph\nset comprises a node set \ud835\udc49\ud835\udc56and edge set \ud835\udc38\ud835\udc56. Typically, assume the\nnumber of nodes in a graph \ud835\udc3a\ud835\udc56is \ud835\udc5b\ud835\udc56= |\ud835\udc49\ud835\udc56|, an adjacency matrix\nA\ud835\udc56\u2208{0, 1}\ud835\udc5b\ud835\udc56\u00d7\ud835\udc5b\ud835\udc56is used to represent the topology of graph \ud835\udc3a\ud835\udc56.\nBesides, let x\ud835\udc63\u2208R\ud835\udc51denotes the attribute vector for node \ud835\udc63\u2208\ud835\udc49\ud835\udc56,\nX\ud835\udc56\u2208R\ud835\udc5b\ud835\udc56\u00d7\ud835\udc51is used to represent the attribute matrix of graph \ud835\udc3a\ud835\udc56.\nGraph Neural Networks: Graph neural networks (GNNs), which\niteratively learn representations with neighborhood aggregation\nand message propagation, is a widely used paradigm of learning rep-\nresentation for graph-structured data in many downstream tasks. In\nthis paper, we leverage the graph isomorphism network (GIN) [37],\na widely used GNN backbone, to learn graph representation for\nanomaly detection tasks. Generally, in each layer of a GIN, the node\nrepresentation is updated by aggregating its neighborhood informa-\ntion. For instance, in the \ud835\udc58-th layer of GIN, the learned aggregated\nfeatures a(\ud835\udc58)\n\ud835\udc63\nfor node \ud835\udc63can be formulated as:\na(\ud835\udc58)\n\ud835\udc63\n= AGGREGATE({h(\ud835\udc58\u22121) (\ud835\udc62),\ud835\udc62\u2208\u02dcN (\ud835\udc63)}),\n(1)\nwhere AGGREGATE(\u00b7) indicates the aggregation function, and\n\u02dcN (\ud835\udc63) represents the neighbor node set of node \ud835\udc63. Then, the node\nfeature h(\ud835\udc58)\n\ud835\udc63\nof node \ud835\udc63in the \ud835\udc58-th layer is obtained by combing the\nnode feature learned in the (\ud835\udc58\u22121)-th layer with the aggregated\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\nfeature, i.e.:\nh(\ud835\udc58)\n\ud835\udc63\n= \ud835\udf0e(COMBINE(h(\ud835\udc58\u22121)\n\ud835\udc63\n, a(\ud835\udc58)\n\ud835\udc63\n)),\n(2)\nwhere \ud835\udf0e(\u00b7) denotes the activation function, e.g., LeakyReLU. Partic-\nularly, the initial feature h(0)\n\ud835\udc63\nfor node \ud835\udc63is set as h(0)\n\ud835\udc63\n= x\ud835\udc63. Conse-\nquently, we can obtain the representation for a graph \ud835\udc3abased on\nthe learned features of all nodes within \ud835\udc3aas follows:\nh\ud835\udc3a= R(CONCAT(h(\ud835\udc58)\n\ud835\udc63\n,\ud835\udc58\u2208{1, . . . , \ud835\udc3e}), \ud835\udc63\u2208\ud835\udc3a),\n(3)\nwhere \ud835\udc3eis the number of GIN layers, and CONCAT(\u00b7) denotes the\nconcatenate operation that stacks the graph representation learned\nacross all \ud835\udc3elayers. R(\u00b7) denotes the readout function that obtains\nthe graph-level representation by aggregating the node features\nwithin a graph, and we choose sum-readout in this paper. Note that\nfor convenience, we use GIN(\u00b7) to simply represent a GIN model\ncontaining the above three operations, in the following sections.\nProblem Formulation: The objective of the GAD under the FL\nsetup is to facilitate collaboration among clients, which allows each\nparticipant to enhance their GAD models by leveraging knowledge\nfrom others without exposing private data. Given \ud835\udc36clients, the\ncollective graph dataset is denoted as \ud835\udc37= {\ud835\udc371, . . . , \ud835\udc37\ud835\udc36}, where\neach client possesses its own graph set \ud835\udc37\ud835\udc50. A prevalent paradigm\nin GAD [24] is that all graphs within the client, i.e., \u2200\ud835\udc3a\ud835\udc56\u2208\ud835\udc37\ud835\udc50, are\ndeemed as \u201cnormal\u201d. The model is trained to capture this normality\nso that the trained model can distinguish an \u201canomalous\" graph\n\u02dc\ud835\udc3adeviates significantly from the distribution of \ud835\udc37\ud835\udc50by some pre-\ndefined assumptions, e.g., the hypersphere decision boundary in\nDeepSVDD [29]. Conversely, in this paper, we attempt to develop\na classifier-based anomaly detector, which can adaptively learn\ndecision boundaries rather than relying on the strong assumption\nof the shape of the latent distribution. This can be regarded as\nsolving the following problem:\nminimize\nw(1),...,w(\ud835\udc36)\n1\n\ud835\udc36\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc50=1\n|\ud835\udc37\ud835\udc50|\n|\ud835\udc37| (\u2113\ud835\udc50(\ud835\udc66, \ud835\udc53w(\ud835\udc50) (\ud835\udc3a)) + \u2113\ud835\udc50( \u02dc\ud835\udc66, \ud835\udc53w(\ud835\udc50) ( \u02dc\ud835\udc3a))),\n(4)\nwhere |\ud835\udc37| and |\ud835\udc37\ud835\udc50| denote the total number of graphs and that\nof in \ud835\udc50-th client. {\ud835\udc3a;\ud835\udc66} represents the normal graph labeled with\n\ud835\udc66= 1, and { \u02dc\ud835\udc3a, \u02dc\ud835\udc66} represents the anomalous graph labeled with\n\ud835\udc66= 0. \u2113\ud835\udc50(\u00b7) denotes the local loss function of \ud835\udc50-th client, e.g., binary\ncross-entropy loss. \ud835\udc53w(\ud835\udc50) (\u00b7) is the GIN-based neural network of \ud835\udc50-\nth client, which is parameterized by w(\ud835\udc50). However, tackling this\nproblem presents the following challenges:\n1) GAD is generally an unsupervised task that only normal graph\n{\ud835\udc3a;\ud835\udc66} is accessible. Thus, how to produce high-quality anoma-\nlous graph { \u02dc\ud835\udc3a; \u02dc\ud835\udc66} for training each local anomaly detector?\n2) In the context of FL-based GAD, how to alleviate the adverse\nimpact of the non-IID property that is prevalent in the graph\ndata across clients?\n3) Transmitting all network parameters following conventional FL\nmethods may limit the scalability given the complexity of GIN.\nTherefore, how to reduce communication costs in collaborative\nlearning while maintaining the validity of local models?\n3.2\nSelf-boosted Graph Knowledge Distillation\nThe first challenge raises the demand to produce anomalous graphs\nwithout using any supervised information. To this end, we propose a\ngraph anomaly generator denoted as Gw\ud835\udc4e(\u00b7) to generate anomalous\ngraphs by perturbing the graph structure of normal graph \ud835\udc3a. For\neach client, we aim to generate an anomalous graph set \u02dc\ud835\udc37\ud835\udc50=\n{X\ud835\udc50, \u02dcA\ud835\udc50} in an unsupervised manner by feeding with normal graph\nset \ud835\udc37\ud835\udc50. To ensure diversity in the generated anomalous graphs, we\nleverage variational graph auto-encoder (VGAE) [12] to build the\nanomaly generator. Specifically, we first learn a latent Gaussian\ndistribution N (\ud835\udf41\ud835\udc50, \ud835\udf482\ud835\udc50), which can be determined as follows:\n\ud835\udf41\ud835\udc50= GIN\ud835\udf41(X\ud835\udc50, A\ud835\udc50), \ud835\udf48\ud835\udc50= GIN\ud835\udf48(X\ud835\udc50, A\ud835\udc50),\n(5)\nwhere GIN\ud835\udf41(\u00b7) and GIN\ud835\udf48(\u00b7) denotes two distinct GINs in anomaly\ngenerator, and \ud835\udf41\ud835\udc50and \ud835\udf48\ud835\udc50explicitly parameterize the following\ninference model:\n\ud835\udc5e( \u02dcZ\ud835\udc50|X\ud835\udc50, A\ud835\udc50) =\n|\ud835\udc37\ud835\udc50|\n\u00d6\n\ud835\udc56=1\n\ud835\udc5e(Z(\ud835\udc56)\n\ud835\udc50\n|X\ud835\udc50, A\ud835\udc50),\n(6)\nwhere \ud835\udc5e( \u02dcZ(\ud835\udc56)\n\ud835\udc50\n|X\ud835\udc50, A\ud835\udc50) = N ( \u02dcZ(\ud835\udc56)\n\ud835\udc50\n|\ud835\udf41(\ud835\udc56)\n\ud835\udc50, diag(\ud835\udf48(\ud835\udc56)\n\ud835\udc50\n)), and it allows us\nto sample from a wide range in the latent space thereby facilitating\nthe diverse anomalous graph generation. Here, we employ the\nreparametrization trick [11] to address the obstacle of gradient\npropagation in the sample operation. Consequently, the generated\nadjacency matrix can be calculated by:\n\u02dcA\ud835\udc50= T ( \u02dcZ\u22a4\n\ud835\udc50\u02dcZ\ud835\udc50),\n\u02dcZ\ud835\udc50= \ud835\udf41\ud835\udc50+ \ud835\udf16\ud835\udf48\ud835\udc50, \ud835\udf16\u223cN (0, 1),\n(7)\nwhere T : R \u2192[0, 1] represents the element-wise transformation\noperations such as Sigmoid(\u00b7), and \ud835\udf16represents a random Gaussian\nnoise that follows the standard normal distribution N (0, 1).\nIntuitively, allowing the generated graphs to closely resemble\nnormal graphs while remaining as anomalies is beneficial in train-\ning a robust and powerful anomaly detector, as it forces the model to\ndistinguish those subtle deviations from the normal patterns. There-\nfore, we propose to optimize the anomaly generator by minimizing\nthe following objective:\n\u2113\ud835\udc50\ng (A\ud835\udc50, \u02dcA\ud835\udc50) = \u2212\n\u2211\ufe01\n\ud835\udc56,\ud835\udc57\n(A\ud835\udc56\ud835\udc57\n\ud835\udc50log( \u02dcA\ud835\udc56\ud835\udc57\n\ud835\udc50) + (1 \u2212A\ud835\udc56\ud835\udc57\n\ud835\udc50) log(1 \u2212\u02dcA\ud835\udc56\ud835\udc57\n\ud835\udc50)),\n(8)\nwhere \u2113\ud835\udc50g denotes the binary-cross entropy loss function. Subse-\nquently, we can train an anomaly detector with the normal and\ngenerated anomalous graph sets for the local client as follows:\n\u2113\ud835\udc50\nad = \ud835\udc59ce(\ud835\udc66\ud835\udc50, Proj(\ud835\udc53w\ud835\udc54(X\ud835\udc50, A\ud835\udc50))) + \ud835\udc59ce( \u02dc\ud835\udc66\ud835\udc50, Proj(\ud835\udc53w\ud835\udc54(X\ud835\udc50, \u02dcA\ud835\udc50))), (9)\nwhere \u02dcA\ud835\udc50= Gw\ud835\udc4e(X\ud835\udc50, A\ud835\udc50),\ud835\udc59ce(\u00b7) is the cross-entropy loss, and \ud835\udc53w\ud835\udc54(\u00b7)\ndenotes the GIN backbone that learns graph representation by\nfeeding with graph data. Proj(\u00b7) is the MLP-based projection head\nthat maps the graph representation learned from \ud835\udc53w\ud835\udc54(\u00b7) into the\npredicted logits. Note that we simply set the label of the normal\ngraph \ud835\udc66\ud835\udc50= 1, and the generated anomalous graph as \u02dc\ud835\udc66\ud835\udc50= 0.\nHence, we can train an anomaly detector in an unsupervised\nmanner by minimizing the following objective function:\n\u2113pt = 1\n\ud835\udc36\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc50=1\n|\ud835\udc37\ud835\udc50|\n|\ud835\udc37| (\u2113\ud835\udc50\nad + \u2113\ud835\udc50\ng),\n(10)\nwhere \u2113\ud835\udc50g attempts to generate anomalous graphs that closely resem-\nble normal ones, while \u2113\ud835\udc50\nad aims to identify those generated anoma-\nlous graphs. Therefore, we produce diverse anomalous graphs for\nlearning a powerful anomaly detector in such a self-boosted style,\nand the two objectives mutually improve each other during training.\nFGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nHowever, in the context of federated learning, the graph data\nacross different clients is often heterogeneous and exhibits non-IID\nproperty. Such characteristics can potentially affect the anomaly\ndetection performance of local models, i.e., the second challenge.\nTo alleviate the adverse impact of the non-IID problem, we propose\na graph knowledge distillation framework, which is designed to\npreserve the personalization of the local model during collaborative\nlearning. Specifically, we regard the previously pre-trained anomaly\ndetector as the \u201cteacher\u201d model, and introduce a \u201cstudent\u201d model\nthat aims to distill the knowledge from the teacher model and\nachieve collaboration between clients.\nThe network architecture of the student model is similar to the\nteacher model, which consists of a GIN backbone and a projection\nhead. Since the purpose of the student model is to mimic the pre-\ndictions of the teacher model for normal data, only normal graphs\nare considered in the knowledge distillation. The predicted logits\nof the teacher and student models are computed as follows:\nQ\ud835\udc50,t = Projt|w\ud835\udc61(\ud835\udc53w\ud835\udc54(X\ud835\udc50, A\ud835\udc50)), Q\ud835\udc50,s = Projs|w\ud835\udc60(\ud835\udc53w\ud835\udc54\u2032 (X\ud835\udc50, A\ud835\udc50)), (11)\nwhere \ud835\udc53w\ud835\udc54(\u00b7), Projt|w\ud835\udc61(\u00b7) and \ud835\udc53w\ud835\udc54\u2032 (\u00b7), Projs|w\ud835\udc60(\u00b7) are the backbone\nnetworks and projection heads of teacher and student models re-\nspectively. Note that Projt|w\ud835\udc61(\u00b7) is actually the same as the projec-\ntion head Proj(\u00b7) in Eq. (9). Subsequently, the student model distills\nthe knowledge from the teacher model by matching its predicted\nlogits with those of the teacher model, described as follows:\n\u2113\ud835\udc50\nkd =\n1\n|\ud835\udc37\ud835\udc50|\n\u2211\ufe01\n\ud835\udc56\u2208\ud835\udc37\ud835\udc50\n\ud835\udc3e\ud835\udc3f(softmax(Q(\ud835\udc56)\n\ud835\udc50,t /\ud835\udf0f), softmax(Q(\ud835\udc56)\n\ud835\udc50,s /\ud835\udf0f)),\n(12)\nwhere \ud835\udc3e\ud835\udc3f(\u00b7, \u00b7) denotes the Kullback-Leibler divergence, which is\napplied to measure the discrepancy between the distribution of the\npredicted logits from teacher and student models. softmax(\u00b7) is the\nsoftmax function, i.e., softmax(\ud835\udc5e\ud835\udc56/\ud835\udf0f) =\nexp(\ud835\udc5e\ud835\udc56/\ud835\udf0f)\n\u00cd\n\ud835\udc57exp(\ud835\udc5e\ud835\udc57/\ud835\udf0f) , and \ud835\udf0fis the\ntemperature factor that controls the smoothness of the distillation.\n3.3\nParameter-efficient Collaborative Learning\nBased on the design of the self-boosted graph knowledge distillation\nmodule, the objective function of all clients is defined as follows:\nLtotal = 1\n\ud835\udc36\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc50=1\n|\ud835\udc37\ud835\udc50|\n|\ud835\udc37| (\u2113\ud835\udc50\nad + \ud835\udf06\u2113\ud835\udc50\ng + \ud835\udefe\u2113\ud835\udc50\nkd),\n(13)\nwhere \ud835\udf06and \ud835\udefeare the two trade-off parameters. In federated learn-\ning, let W(\ud835\udc50) = {w(\ud835\udc50)\n\ud835\udc4e, w(\ud835\udc50)\n\ud835\udc54, w(\ud835\udc50)\n\ud835\udc54\u2032 , w(\ud835\udc50)\n\ud835\udc61\n, w(\ud835\udc50)\n\ud835\udc60\n} denotes the parame-\nter set of the \ud835\udc50-th client, the conventional solution achieves collabo-\nration by uploading the network parameters to the server and then\ndistribute the aggregated network parameters to each client. How-\never, this solution presents several problems. First, the high param-\neter complexity of a GIN-based backbone can limit the scalability\nof the model during the parameter aggregation process. Second,\nthe transmission of all network parameters may introduce non-IID\nproblems, and affect the performance of local models trained on\ndifferent graph data across clients.\nTo address these issues, we propose an effective collaborative\nlearning mechanism in this paper, which is described in Figure 2.\nSpecifically, We let the teacher and student models share the same\nGIN backbone for learning graph representation, i.e.,\nZ\ud835\udc50= \ud835\udc53w\ud835\udc54(X\ud835\udc50, A\ud835\udc50) = \ud835\udc53w\ud835\udc54\u2032 (X\ud835\udc50, A\ud835\udc50),\n(14)\nwhere Z\ud835\udc50denotes the learned graph representation that is shared\nas the input to the projection heads of teacher and student. This\noperation not only reduces the complexity of the local model, but\nalso simplifies the knowledge distillation of the student model.\nThen we only upload the parameter set w(\ud835\udc50)\n\ud835\udc60\nof the student head for\ncollaboration instead of uploading all the network parameters, i.e.,\nthe parameter aggregation in the server is formalized as follows:\n\u00afw\ud835\udc60=\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc50=1\n|\ud835\udc37\ud835\udc50|\n|\ud835\udc37| w(\ud835\udc50)\n\ud835\udc60\n,\n(15)\nwhere \u00afw\ud835\udc60denotes the aggregated parameters in the server. The\nproposed collaborative learning mechanism not only streamlines\nthe capacity of local models, but also significantly reduces the com-\nmunication costs, which addresses the third challenge. To facilitate\nthe understanding of the proposed FGAD method, we summarize\nits detailed training process in Algorithm 1. The collaboration be-\ntween clients via the student model is performed in the following\ntwo steps:\n\u2022 Each client performs graph knowledge distillation independently,\nupdating its network parameters, and uploads the network pa-\nrameters of the student head to the server.\n\u2022 The server then aggregates the network parameters following\nEq. 15, and distributes the aggregated network parameters to\neach client.\nAlgorithm 1 Training process of the proposed FGAD\nInput: Graph set \ud835\udc37= {\ud835\udc37\ud835\udc50}\ud835\udc36\n\ud835\udc50=1, number of clients \ud835\udc36, number of\nGNN layers \ud835\udc3e, learning rate \ud835\udefc, total epochs T.\nOutput: The overall graph anomaly detection performance.\n1: Initialize the parameter sets {W(\ud835\udc50)}\ud835\udc36\n\ud835\udc50=1 for each local model;\n2: Pretrain the local model in each client with Eq. (10);\n3: while not converge do\n4:\nfor \ud835\udc61= 1, 2, . . . , T do\n5:\nfor \ud835\udc50= 1, . . . ,\ud835\udc36do\n6:\nGenerate anomalous graph set \u02dcD with Eqs. (5), (6), (7);\n7:\nCompute loss items \u2113\ud835\udc50\nad, \u2113\ud835\udc50g, \u2113\ud835\udc50\nkd with Eq. (8), (9), (12);\n8:\nend for\n9:\nBack-propagation and update each local model via mini-\nmizing Eq. (13);\n10:\nend for\n11:\nUpload the parameter sets {w(\ud835\udc50)\n\ud835\udc60\n}\ud835\udc36\n\ud835\udc50=1 of student model in\neach client to the server;\n12:\nCompute aggregated network parameters \u00afw\ud835\udc60with collabo-\nrative learning following Eq. (15);\n13:\nDistribute parameter set \u00afw\ud835\udc60to the local model of each client;\n14: end while\n15: Evaluate the anomaly detection performance in each client and\naggregate their results;\n16: return The overall graph anomaly detection performance.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\n4\nEXPERIMENT\n4.1\nExperimental Setup\nDatasets. We evaluate the performance of FL-based graph anom-\naly detection on non-IID graphs through two distinct experimental\nsetups: (1) single-dataset and (2) multi-dataset scenarios.\n\u2022 Single-dataset: we distribute a single dataset across multiple\nclients, each of which possesses a unique subset of the dataset.\nThis setup allows us to assess the effectiveness when clients\ncollaborate on a shared dataset. We employ three social network\ndatasets including IMDB-BINARY, COLLAB, and IMDB-MULTI\nto conduct this experiment.\n\u2022 Multi-dataset: we broaden our evaluation by considering vari-\nous datasets distributed in multiple clients and each of them holds\na specific dataset. We consider not only social network data (SO-\nCIALNET) but also expand to include molecular (MOLECULES),\nbiochemical (BIOCHEM), and mix data types (MIX). This allows\nus to thoroughly assess FL-based graph anomaly detection across\na spectrum of data types and collaboration scenarios.\nThe information and construction details of each dataset are illus-\ntrated in Appendix A.1.\nBaseline Methods. We compare the proposed FGAD method\nwith several state-of-the-art baseline methods. We include two fed-\nerated learning methods: FedAvg [25] and FedProx [18], as well as\ntwo federated graph learning methods: GCFL [36] and FedStar [30].\nNote that in order to adapt these baseline methods to the graph\nanomaly detection task, we integrate them with DeepSVDD [29] to\nconstruct an end-to-end graph anomaly detection model. Besides,\nwe regard the self-training strategy without the FL setting as one of\nthe baselines. To ensure a fair comparison with FGAD, we employ\nthe same GIN network structure as FGAD in all baseline methods.\nImplementation Details. We use GIN [37] as the graph rep-\nresentation learning backbone for FGAD and all baselines. The\nnumber of GIN layer \ud835\udc3eis set to 3, and the dimensions of the hidden\nlayer of GIN and projection head of student and teacher models\nare all set to 64. We use Adam [11] as the optimizer and fixed the\nlearning rate \ud835\udefc= 0.001. For all datasets, we first pretrain the anom-\naly generator and teacher model for 10 epochs, and then jointly\ntrain with knowledge distillation and collaborative learning for 200\nepochs. For more training details, please refer to Appendix A.2.\nEvaluation Metrics: We use Area Under the Curve (AUC) and\nArea Under the Precision-Recall Curve (AUPRC) as the evaluation\nmetrics in the experiment. Each method is executed 10 times to\nreport their means and standard deviations.\n4.2\nExperimental Results\nIn this section, we conduct comprehensive experiments including\ntwo types of non-IID graph scenarios, i.e., the single-dataset and\nmulti-dataset distributed in multiple clients, to validate the effec-\ntiveness of the proposed method. Table 1 and Table 2 show the\nexperimental results of FGAD and several state-of-the-art baselines,\nfrom which we can have the following observations.\n\u2022 Comparison: In the single-dataset experiment, FGAD demon-\nstrates a remarkable advantage over all baseline methods. For\ninstance, in the IMDB-BINARY dataset, FGAD achieves signifi-\ncant performance improvement, exceeding Self-train by 23.39%\nin AUC and 19.17% in AUPRC. It also significantly surpasses\nclassical FedAvg and FedProx. Furthermore, FGAD outperforms\nthe state-of-the-art baselines GCFL and FedStar by a substantial\n7.99% and 10.21% in AUC, respectively. Similar trends are evi-\ndent across other benchmarks, demonstrating the effectiveness\nof FGAD. In the multi-dataset experiment, the GAD task is more\nchallenging as the non-IID problem in it is more severe compared\nto the single-dataset scenario. Nevertheless, FGAD still exhibits\noutstanding performance compared to other baselines. For exam-\nple, on MOLECULES, FGAD outperforms the runner-up FedStar\nby 6% in AUC and 19.46% in AUPRC. Besides, it achieves more\nthan a 10.00% performance improvement compared to other base-\nline methods. More importantly, we can observe from Table 1\nthat FGAD significantly reduces communication costs during\ncollaborative learning compared to other baseline methods.\n\u2022 Discussion: The Self-train strategy discards collaborative train-\ning and fails to leverage the knowledge from other clients to learn\nmore robust local GAD models. FedAvg and FedProx require\nthe transmission of all network parameters of the local models,\nwhich introduces severe non-IID problems in collaborative learn-\ning. Consequently, these three aforementioned baselines yield\nsuboptimal performance in most cases. Although GCFL incorpo-\nrates a specific design to alleviate non-IID challenges, such as\nutilizing clustered FL for collaborative learning, it still necessi-\ntates the transmission of all network parameters and does not\neffectively address non-IID problems, as validated by the experi-\nmental results. On the other hand, FedStar achieves runner-up\nperformance in most cases, which may primarily be attributed\nto the introduced structural embedding that helps to preserve\nthe personalization of local models. Compared with the baseline\nmethods, FGAD considers enhancing the detecting capability of\nlocal models in a self-boosted manner, and introduces an effec-\ntive collaborative learning mechanism by leveraging knowledge\ndistillation. This allows FGAD to learn more powerful local GAD\nmodels, mitigate the adverse effects of non-IID problems, and\nreduce communication costs among clients.\n4.3\nEmbedding Visualization\nWe employ t-SNE [32] to visualize the learned embedding for intu-\nitive comparison. Figure 3 shows the embedding visualization for\nAIDS, one of the constituents of MOLECULES. We include results\nfrom FedAvg, GCFL, and FedStar for a comprehensive analysis. It\u2019s\nevident that the learned embeddings by FedAvg and GCFL exhibit\npoor discriminative properties, with both normal and anomalous\ngraphs appearing entangled in the latent space. Although the visu-\nalization result of FedStar shows some separation between normal\nand anomalous graphs, it remains blurred decision boundaries. Con-\nversely, the learned embeddings of FGAD are clearly more discrim-\ninative compared to the other baseline methods. The visualization\nof FGAD reveals distinct boundaries between the embeddings of\nnormal and anomalous graphs, supporting its effectiveness.\nFGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 1: Anomaly detection performance (mean(%) \u00b1 std(%)) under the single-dataset setting. Note that the best performance is\nmarked in Bold, and the last column shows the number of transmitted parameters in collaborative learning.\nMethods\nIMDB-BINARY\nCOLLAB\nIMDB-MULTI\n# Parameters\nAUC\nAUPRC\nAUC\nAUPRC\nAUC\nAUPRC\nSelf-train\n41.58\u00b11.34\n47.43\u00b11.39\n46.96\u00b11.80\n30.87\u00b10.62\n52.39\u00b11.31\n32.74\u00b10.60\nN/A\nFedAvg [25]\n40.96\u00b13.44\n48.24\u00b12.41\n49.60\u00b10.45\n30.69\u00b10.50\n49.11\u00b11.46\n36.13\u00b11.54\n5,370,880\nFedProx [18]\n39.62\u00b12.36\n46.74\u00b11.24\n49.56\u00b10.50\n31.40\u00b10.50\n52.16\u00b11.75\n36.13\u00b11.54\n5,370,880\nGCFL [36]\n56.98\u00b15.56\n59.68\u00b13.37\n48.93\u00b11.02\n30.84\u00b10.36\n49.44\u00b12.95\n34.87\u00b10.68\n10,741,760\nFedStar [30]\n54.76\u00b11.28\n56.49\u00b10.86\n51.89\u00b10.33\n36.89\u00b10.43\n58.28\u00b10.53\n39.97\u00b11.22\n416,000\nFGAD\n64.97\u00b10.52\n66.60\u00b11.12\n55.08\u00b11.85\n66.67\u00b10.00\n60.51\u00b11.18\n66.82\u00b10.14\n21,130\nTable 2: Anomaly detection performance (mean(%) \u00b1 std(%)) under the multi-dataset setting. Note that the best performance is\nmarked in Bold.\nMethods\nMOLECULES\nBIOCHEM\nSOCIALNET\nMIX\nAUC\nAUPRC\nAUC\nAUPRC\nAUC\nAUPRC\nAUC\nAUPRC\nSelf-train\n61.26\u00b12.91\n61.31\u00b11.91\n54.54\u00b10.99\n52.29\u00b10.40\n50.31\u00b11.55\n39.96\u00b11.58\n51.94\u00b10.42\n47.65\u00b10.64\nFedAvg [25]\n54.41\u00b13.21\n55.55\u00b13.23\n40.88\u00b11.36\n51.63\u00b11.13\n48.21\u00b11.02\n38.29\u00b11.29\n47.96\u00b10.61\n44.89\u00b10.68\nFedProx [18]\n57.93\u00b12.14\n58.72\u00b12.25\n46.04\u00b10.49\n51.57\u00b10.80\n47.26\u00b10.10\n37.23\u00b10.92\n46.79\u00b10.63\n44.19\u00b10.29\nGCFL [36]\n45.67\u00b11.33\n51.96\u00b10.79\n41.49\u00b10.30\n52.23\u00b10.65\n47.59\u00b10.95\n37.53\u00b10.93\n49.58\u00b10.50\n45.37\u00b10.69\nFedStar [30]\n56.15\u00b10.92\n59.73\u00b11.21\n47.80\u00b10.48\n56.48\u00b10.19\n53.79\u00b12.03\n36.40\u00b11.11\n50.53\u00b11.11\n45.83\u00b10.41\nFGAD\n62.15\u00b10.69\n79.19\u00b10.49\n58.09\u00b10.85\n59.04\u00b10.54\n54.86\u00b10.29\n56.88\u00b10.98\n58.14\u00b10.36\n52.03\u00b10.63\n-20\n10\n-10\n0\n20\n0\n10\n20\n0\n-10\n-20\n-20\n-20\n20\n-10\n20\n0\n10\n10\n0\n0\n20\n-20\n-10\n-40\n-20\n20\n-20\n0\n0\n0\n20\n20\n-20\n-40\n40\n-20\n40\n0\n20\n20\n20\n0\n0\n-20\n-20\n(d) FGAD\n(a) FedAvg\n(b) GCFL\n(c) FedStar\nAnomalous Graph\nNormal Graph (Test)\nNormal Graph (Train)\nAnomalous Graph\nNormal Graph (Test)\nNormal Graph (Train)\nAnomalous Graph\nNormal Graph (Test)\nNormal Graph (Train)\nAnomalous Graph\nNormal Graph (Test)\nNormal Graph (Train)\nFigure 3: Embedding visualization of the proposed FGAD\ncompared with several baselines using t-SNE. Note that the\ndata point marked in yellow, red, and green correspond to\nthe normal graph (test), anomalous graph, and normal graph\n(train), respectively.\nTable 3: Ablation study results (mean(%) \u00b1 std(%)) of FGAD\nand its three variants.\nMethods\nIMDB-MULTI\nMOLECULES\nAUC\nAUPRC\nAUC\nAUPRC\nFGAD_v1\n56.67\u00b11.72\n64.91\u00b11.85\n57.98\u00b12.78\n75.80\u00b10.09\nFGAD_v2\n56.69\u00b11.22\n65.98\u00b10.90\n59.41\u00b12.22\n77.32\u00b11.02\nFGAD_v3\n55.23\u00b13.54\n61.02\u00b12.68\n55.58\u00b14.56\n66.73\u00b10.80\nFGAD\n60.51\u00b11.18 66.82\u00b10.14 62.15\u00b10.69 79.19\u00b10.49\n4.4\nAblation Study\nTo validate the effectiveness of each component in the proposed\nFGAD method, we derive three variants from FGAD and perform a\nsystematic evaluation. Specifically, we illustrate the construction\ndetails of the three variants as follows:\n\u2022 FGAD_v1: This variant only considers local training in each\nclient, and abandons the collaborative learning between clients.\n\u2022 FGAD_v2: This variant drops the proposed collaborative learn-\ning mechanism, and follows the parameter aggregation mecha-\nnism of the classical FedAvg method.\n\u2022 FGAD_v3: This variant drops the knowledge distillation mod-\nule, i.e., removes the student model and only takes the classifier\n(teacher model) in collaboration.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\n(b) MOLECULES\n(a) IMDB-BINARY\nFigure 4: Parameter analysis of \ud835\udf06and \ud835\udefeon IMDB-BINARY\nand MOLECULES. Note that the values of \ud835\udf06and \ud835\udeferange from\n[1\ud835\udc52\u22124, . . . , 1\ud835\udc523].\nTable 3 shows the experimental results of FGAD and its three vari-\nants on two datasets, yielding the following observations. FGAD_v1\ndemonstrates a performance decline compared to FGAD, which\nis primarily due to the fact that FGAD_v1 exclusively focuses on\nlocal training, neglecting collaboration with other clients. Conse-\nquently, it fails to leverage the comprehensive knowledge of other\nclients. Secondly, when we substitute the proposed collaborative\nlearning mechanism with the classical FedAvg, there is also a no-\nticeable performance decline. This can be attributed to the potential\nsusceptibility of parameter transmission in FedAvg to the adverse\neffects of non-IID problems. Third, FGAD consistently outperforms\nFGAD_v3 by a significant margin. This observation reveals the cru-\ncial role of the self-boosted distillation module in maintaining the\npersonalization of local models within each client, which effectively\nmitigates the non-IID problems. Overall, the ablation study results\nfully support the rationale and the effectiveness of each component\nproposed in FGAD.\n4.5\nParameter Analysis\n4.5.1\nImpact of Hyper-Parameters \ud835\udf06and \ud835\udefe. The objective func-\ntion of the proposed FGAD method contains two main hyper-\nparameters, i.e., \ud835\udf06and \ud835\udefe. In this section, we conduct an analysis of\nthe impact of these two hyper-parameters on anomaly detection\nperformance. Specifically, we vary the values of \ud835\udf06and \ud835\udefewithin the\nrange of [1\ud835\udc52\u22123, . . . , 1\ud835\udc524] and present the experimental results on\nIMDB-BINARY and MOLECULES datasets in Figure 4. From the ob-\nservations shown in the figure, we draw several conclusions. Firstly,\nFGAD tends to yield suboptimal performance when the values of\n\ud835\udf06and \ud835\udefeare set too low, e.g., 1\ud835\udc52\u22124 and 1\ud835\udc52\u22123. This emphasizes the\nsignificant role of both loss terms in the FGAD framework and sug-\ngests their effectiveness. Secondly, we can observe that excessively\nhigh values of \ud835\udf06and \ud835\udefealso have an adverse impact on performance,\n(a)\n(b)\nFigure 5: Average performance and distribution of variance\nbetween clients of FedAvg and FGAD. Note that the client\nnumber is set to [2, . . . , 10].\n(a)\n(b)\nFigure 6: Average performance with standard deviation un-\nder different numbers of GIN layers on IMDB-BINARY and\nMOLECULES datasets. Note that the number of GIN layers is\nset to [1, . . . , 10].\nbecause they may obscure the primary objective of optimizing the\nanomaly detector. Finally, it is worth noting that FGAD exhibits\nrelatively stable performance both in AUC and AUPRC across a\nwide range of \ud835\udf06and \ud835\udefevalues, demonstrating its robustness.\n4.5.2\nImpact of Client Numbers. The number of clients \ud835\udc36is\nanother hyper-parameter in the FGAD framework, and its impact\non the performance is crucial for assessing the scalability of client\nnumbers. Therefore, we vary the number of clients \ud835\udc36within the\nrange of [2, . . . , 10] and conduct the experiment. The results on\nIMDB-BINARY are reported in Figure 5. Note that we also include\nFedAvg as a baseline method for comparative analysis. It can be\nobserved that FGAD consistently achieves remarkable performance\nimprovement compared to FedAvg in all cases, and exhibits sta-\nbility against changes in the number of clients. However, when\nthe number of clients increases to certain large values, the average\nperformance shows a certain degradation, and the performance\nvariance between different clients becomes more significant both in\nFGAD and FedAvg. This is primarily due to the gradually increasing\ndiscrepancy between the graph data distributed across different\nclients, which causes more severe non-IID problems. Nevertheless,\nFGAD still exhibits relatively smaller performance fluctuations com-\npared with FedAvg, which fully demonstrates the scalability of the\nproposed FGAD method.\n4.5.3\nImpact of GIN Layers. We delve into the impact of the\nnumber of GIN layers \ud835\udc3eon the anomaly detection performance\nwithin the proposed FGAD method. The parameter \ud835\udc3eplays a piv-\notal role in determining the extent to which the model explores\nFGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nneighborhood information and the overall complexity of FGAD. We\nsystematically analyze its impact by varying the \ud835\udc3ewithin the range\nof [1, . . . , 10] and conduct a series of experiments. Figure 6 reports\nthe experimental results on the IMDB-BINARY and MOLECULES\ndatasets, from which we have the following observations. First, a\ncertain depth of GIN is beneficial to fully leverage the structural in-\nformation of graph data for learning powerful GAD models, which\ncould be verified from the observed performance improvement.\nSecond, when the number of GIN layers continues to increase, the\nobserved performance improvements become increasingly mar-\nginal or even exhibit slight diminishment. This trend indicates that\na moderate number of GIN layers, e.g., 3, is sufficient to effectively\nleverage the neighborhood information within graphs. Third, we\ncan observe from the overall experimental results that the perfor-\nmance remains relatively stable under the variation of \ud835\udc3e, which\ndemonstrates the robustness of FGAD.\n5\nCONCLUSION\nIn this paper, we study a challenging GAD problem with non-IID\ngraph data distributed across multiple clients, and propose an effec-\ntive federated graph anomaly detection (FGAD) method to tackle\nthis issue. To enhance the detecting capability of local models, we\npropose to train a classifier in a self-boosted manner by distin-\nguishing the normal and anomalous graphs generated from an\nanomaly generator. Besides, to alleviate the adverse impact of non-\nIID problems among clients, we introduce a student model to distill\nknowledge from the classifier (teacher model), and engage only\nthe student model in collaborative learning, so that the personal-\nization of local models could be preserved. Moreover, we improve\nthe collaborative learning mechanism that streamlines the capac-\nity of local models and reduces the communication costs during\ncollaborative learning. Comprehensive experiments under various\ndata types and scenarios compared with state-of-the-art baselines\ndemonstrate the superiority of the proposed FGAD method. We\nbelieve that this work would pave the way for subsequent studies\non collaborative GAD under the FL setting in the future.\nREFERENCES\n[1] Ane Bl\u00e1zquez-Garc\u00eda, Angel Conde, Usue Mori, and Jose A Lozano. 2021. A\nreview on outlier/anomaly detection in time series data. ACM Computing Surveys\n(CSUR) 54, 3 (2021), 1\u201333.\n[2] Jinyu Cai and Jicong Fan. 2022. Perturbation learning based anomaly detection.\nAdvances in Neural Information Processing Systems 35 (2022).\n[3] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:\nA survey. ACM Computing Surveys (CSUR) 41, 3 (2009), 1\u201358.\n[4] Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detection\nin multivariate time series. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 35. 4027\u20134035.\n[5] Kaize Ding, Qinghai Zhou, Hanghang Tong, and Huan Liu. 2021. Few-shot\nnetwork anomaly detection via cross-network meta-learning. In Proceedings of\nthe Web Conference. 2448\u20132456.\n[6] Gokberk Elmas, Salman UH Dar, Yilmaz Korkmaz, Emir Ceyani, Burak Susam,\nMuzaffer Ozbey, Salman Avestimehr, and Tolga \u00c7ukur. 2022. Federated learning\nof generative image priors for MRI reconstruction. IEEE Transactions on Medical\nImaging (2022).\n[7] Matthias Fey and Jan Eric Lenssen. 2019. Fast graph representation learning with\nPyTorch Geometric. arXiv preprint arXiv:1903.02428 (2019).\n[8] Xingbo Fu, Binchi Zhang, Yushun Dong, Chen Chen, and Jundong Li. 2022. Feder-\nated graph machine learning: A survey of concepts, techniques, and applications.\nACM SIGKDD Explorations Newsletter 24, 2 (2022), 32\u201347.\n[9] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yong-\ndong Zhang. 2023. Alleviating structural distribution shift in graph anomaly\ndetection. In Proceedings of the Sixteenth ACM International Conference on Web\nSearch and Data Mining. 357\u2013365.\n[10] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aur\u00e9lien Bellet, Mehdi Ben-\nnis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,\nRachel Cummings, et al. 2021. Advances and open problems in federated learning.\nFoundations and Trends\u00ae in Machine Learning 14, 1\u20132 (2021), 1\u2013210.\n[11] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[12] Thomas N Kipf and Max Welling. 2016. Variational graph auto-encoders. arXiv\npreprint arXiv:1611.07308 (2016).\n[13] Thomas N Kipf and Max Welling. 2017. Semi-supervised classification with\ngraph convolutional networks. In Proceedings of the International Conference on\nLearning Representations.\n[14] Chenglin Li, Di Niu, Bei Jiang, Xiao Zuo, and Jianming Yang. 2021. Meta-har:\nFederated representation learning for human activity recognition. In Proceedings\nof the Web Conference. 912\u2013922.\n[15] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. 2021. Cutpaste:\nSelf-supervised learning for anomaly detection and localization. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition. 9664\u20139674.\n[16] Qinbin Li, Bingsheng He, and Dawn Song. 2021. Model-contrastive federated\nlearning. In Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. 10713\u201310722.\n[17] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. 2020. Federated\nlearning: Challenges, methods, and future directions. IEEE Signal Processing\nMagazine 37, 3 (2020), 50\u201360.\n[18] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar,\nand Virginia Smith. 2020. Federated optimization in heterogeneous networks.\nProceedings of Machine Learning and Systems 2 (2020), 429\u2013450.\n[19] Rui Liu, Pengwei Xing, Zichao Deng, Anran Li, Cuntai Guan, and Han Yu. 2022.\nFederated graph neural networks: Overview, techniques and challenges. arXiv\npreprint arXiv:2202.07256 (2022).\n[20] Shenghua Liu, Bin Zhou, Quan Ding, Bryan Hooi, Zhengbo Zhang, Huawei\nShen, and Xueqi Cheng. 2022. Time series anomaly detection with adversarial\nreconstruction networks. IEEE Transactions on Knowledge and Data Engineering\n35, 4 (2022), 4293\u20134306.\n[21] Yi Liu, Sahil Garg, Jiangtian Nie, Yang Zhang, Zehui Xiong, Jiawen Kang, and\nM Shamim Hossain. 2020. Deep anomaly detection for time-series data in in-\ndustrial IoT: A communication-efficient on-device federated learning approach.\nIEEE Internet of Things Journal 8, 8 (2020), 6348\u20136358.\n[22] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis.\n2021. Anomaly detection on attributed networks via contrastive self-supervised\nlearning. IEEE Transactions on Neural Networks and Learning Systems 33, 6 (2021),\n2378\u20132392.\n[23] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. 2022. Deep\ngraph-level anomaly detection by glocal knowledge distillation. In Proceedings\nof the Fifteenth ACM International Conference on Web Search and Data Mining.\n704\u2013714.\n[24] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong,\nand Leman Akoglu. 2021. A comprehensive survey on graph anomaly detection\nwith deep learning. IEEE Transactions on Knowledge and Data Engineering (2021).\n[25] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and\nBlaise Aguera y Arcas. 2017. Communication-efficient learning of deep net-\nworks from decentralized data. In Proceedings of the International Conference on\nArtificial Intelligence and Statistics. PMLR, 1273\u20131282.\n[26] Christopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel,\nand Marion Neumann. 2020. TUDataset: A collection of benchmark datasets for\nlearning with graphs. In Proceedings of the ICML Workshop on Graph Representa-\ntion Learning and Beyond. arXiv:2007.08663 www.graphlearning.io\n[27] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. 2021.\nDeep learning for anomaly detection: A review. ACM Computing Surveys (CSUR)\n54, 2 (2021), 1\u201338.\n[28] Chen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. 2022. Raising the\nBar in Graph-level Anomaly Detection. In Proceedings of the International Joint\nConference on Artificial Intelligence. 2196\u20132203. https://doi.org/10.24963/ijcai.\n2022/305\n[29] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed\nSiddiqui, Alexander Binder, Emmanuel M\u00fcller, and Marius Kloft. 2018. Deep\none-class classification. In Proceedings of the International Conference on Machine\nLearning. PMLR, 4393\u20134402.\n[30] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and Chengqi Zhang.\n2023. Federated learning on non-iid graphs via structural knowledge sharing. In\nProceedings of the AAAI Conference on Artificial Intelligence, Vol. 37. 9953\u20139961.\n[31] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. 2022. Rethinking graph neural\nnetworks for anomaly detection. In Proceedings of the International Conference on\nMachine Learning. PMLR, 21076\u201321089.\n[32] Laurens Van der Maaten and Geoffrey Hinton. 2008. Visualizing data using t-SNE.\nJournal of Machine Learning Research 9, 11 (2008), 2579\u20132605.\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\n[33] Siqi Wang, Jiashu Li, Mian Lu, Zhao Zheng, Yuqiang Chen, and Bingsheng\nHe. 2022. A system for time series feature extraction in federated learning. In\nProceedings of the ACM International Conference on Information & Knowledge\nManagement. 5024\u20135028.\n[34] Zhen Wang, Weirui Kuang, Yuexiang Xie, Liuyi Yao, Yaliang Li, Bolin Ding, and\nJingren Zhou. 2022. Federatedscope-gnn: Towards a unified, comprehensive and\nefficient package for federated graph learning. In Proceedings of the 28th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining. 4110\u20134120.\n[35] Jinze Wu, Qi Liu, Zhenya Huang, Yuting Ning, Hao Wang, Enhong Chen, Jinfeng\nYi, and Bowen Zhou. 2021. Hierarchical personalized federated learning for user\nmodeling. In Proceedings of the Web Conference. 957\u2013968.\n[36] Han Xie, Jing Ma, Li Xiong, and Carl Yang. 2021. Federated graph classification\nover non-iid graphs. Advances in Neural Information Processing Systems 34 (2021),\n18839\u201318852.\n[37] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks?. In Proceedings of the International Conference on\nLearning Representations.\n[38] Rui Yan, Liangqiong Qu, Qingyue Wei, Shih-Cheng Huang, Liyue Shen, Daniel\nRubin, Lei Xing, and Yuyin Zhou. 2023. Label-efficient self-supervised federated\nlearning for tackling data heterogeneity in medical imaging. IEEE Transactions\non Medical Imaging (2023).\n[39] Chengxu Yang, Qipeng Wang, Mengwei Xu, Zhenpeng Chen, Kaigui Bian, Yunxin\nLiu, and Xuanzhe Liu. 2021. Characterizing impacts of heterogeneity in federated\nlearning upon large-scale smartphone data. In Proceedings of the Web Conference.\n935\u2013946.\n[40] Ge Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su,\nChuan Zhou, Quan Z Sheng, Leman Akoglu, et al. 2022. Dual-discriminative\ngraph neural network for imbalanced graph-level anomaly detection. Advances\nin Neural Information Processing Systems 35 (2022), 24144\u201324157.\n[41] Huanding Zhang, Tao Shen, Fei Wu, Mingyang Yin, Hongxia Yang, and Chao Wu.\n2021. Federated graph learning\u2013a position paper. arXiv preprint arXiv:2105.11099\n(2021).\n[42] Lingxiao Zhao and Leman Akoglu. 2021. On using classification datasets to\nevaluate graph outlier detection: Peculiar observations and new insights. Big\nData (2021).\n[43] Chenyi Zhuang and Qiang Ma. 2018. Dual graph convolutional networks for\ngraph-based semi-supervised classification. In Proceedings of the World Wide Web\nConference. 499\u2013508.\nA\nAPPENDIX\nThe appendix includes the following content:\n(1) Detailed description of graph benchmarks used in experiments.\n(2) Detailed experimental settings.\n(3) Theoretical and empirical complexity analysis.\n(4) Parameter analysis of latent dimensions.\n(5) Justification of the backbone sharing strategy.\nA.1\nDetailed Description of Graph Benchmarks\nIn this section, we supplement the detailed description for all the\ngraph benchmarks used in our experiment, including the num-\nber of graphs, the average node numbers and edge numbers, and\nthe classes. Table 4 summarizes the information on graph bench-\nmarks used in the experiment. Specifically, in the single-dataset\nexperiment, we use three social network benchmarks including\nIMDB-BINARY, COLLAB, and IMDB-MULTI. In the multi-dataset\nexperiment, we construct four benchmarks by integrating different\ntypes of graph data, e.g., molecules, biological, and social network\ndata. The details are illustrated as follows:\n\u2022 MOLECULES: This benchmark includes multiple molecule datasets,\ne.g., MUTAG, DHFR, PTC_MR, BZR, COX2, AIDS, and NCI1.\n\u2022 BIOCHEM: This benchmark is a cross-domain dataset including\ndatasets in MOLECULE, and additional biological datasets, e.g.,\nENZYMES, PROTEINS, and DD.\n\u2022 SOCIALNET: This benchmark includes multiple social network\ndatasets, e.g., IMDB-BINARY, COLLAB and IMDB-MULTI.\n\u2022 MIX: This benchmark contains all datasets from three domains,\ni.e., molecular, biological, and social network, in Table 4.\nNote that we regard the graph in the first class of each dataset\nas the normal graph, and the graphs in other classes as anoma-\nlous graphs. All graph benchmarks used in this paper source from\nTUDataset [26], a publicly available graph benchmark database1.\nA.2\nDetailed Experimental Settings\nIn this section, we supplement more details of the experimental\nsettings in the paper, including the network structure, trade-off\nparameter settings, training details, baseline settings, etc.\n\u2022 Network Structure: We employ a 3-layer GIN [37] as the back-\nbone network for our method, with the aggregated dimension in\neach layer set to 64. In addition, we adopt the 4-layer and 3-layer\nfully connected networks for the teacher head and student head,\nrespectively. The network structure of the teacher head is set\nto 256-192-128-64-2, while for the student head is 192-128-64-2.\nMoreover, we will open-source the code of FGAD for details and\nreproducibility.\n\u2022 Data Split: For all datasets, we regard the graphs in the first class\nas normal and graphs in other classes as anomalous. We allocate\n80% of the normal graphs data for training, and subsequently\nconstruct the testing data by combining the remaining normal\ndata with an equal number of anomalous graphs.\n\u2022 Training Details: We fix the batch size as 64 for all experiments\nand use Adam [11] as the optimizer with a fixed learning rate \ud835\udefc=\n0.001. We first pre-train each local model excluding the student\nnetwork and knowledge distillation module for 10 epochs. Then\n1https://chrsmrrs.github.io/datasets/docs/datasets/\nFGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nTable 4: Detailed information of the datasets used in the experiment.\nDataset Name\n#Graphs\n#Average Nodes\n#Average Edges\n#Graph Classes\nData Type\nIMDB-BINARY\n1,000\n19.77\n96.53\n2\nSocial Network\nCOLLAB\n5,000\n74.49\n2,457.78\n3\nSocial Network\nIMDB-MULTI\n1,500\n13.00\n65.94\n3\nSocial Network\nMUTAG\n188\n17.93\n19.79\n2\nMolecule\nDHFR\n756\n42.43\n44.54\n2\nMolecule\nPTC_MR\n344\n14.29\n14.69\n2\nMolecule\nBZR\n405\n35.75\n38.36\n2\nMolecule\nCOX2\n467\n41.22\n43.45\n2\nMolecule\nAIDS\n2,000\n15.69\n16.20\n2\nMolecule\nNCI1\n4,110\n29.87\n32.30\n2\nMolecule\nENZYMES\n600\n32.63\n62.14\n6\nBiology\nPROTEINS\n1,113\n39.06\n72.82\n2\nBiology\nDD\n1,178\n284.32\n715.66\n2\nBiology\nwe jointly train the whole network with collaborative learning\nfor 200 epochs.\n\u2022 Trade-off Parameter Settings: The objective function of FGAD\ncontains two trade-off parameters, i.e., \ud835\udf06, and\ud835\udefe, we vary their val-\nues within the range of [1\ud835\udc52\u22124, 1\ud835\udc523] and evaluate their impact on\nperformance in the Section 4.5.1. Regarding the number of clients\n\ud835\udc36in a single-dataset, we vary it within the range of [2, . . . , 10]\nand evaluate its impact in Section 4.5.2, while for multi-dataset,\nthe number of clients is set to the number of its sub-datasets.\nBesides, for the number of GIN layers \ud835\udc3e, we also evaluate its\nimpact under different values in Section 4.5.3.\n\u2022 Baseline Settings: For the state-of-the-art baselines including\nFedAvg, FedProx, GCFL, and FedStar, we integrate them with\nDeepSVDD [29] to construct the end-to-end GAD model. We\nalso include the self-training strategy that abandons collabora-\ntive learning, as one of the baselines. Besides, we employ the\nsame GIN backbone with FGAD to guarantee the fairness of the\nexperiment. The objective of local models in each client is to\nminimize the distance from the projection of the training data\nin the latent space to the centroid, which is randomly initialized\nfollowing the setting in DeepSVDD and fixed throughout the\ntraining phase. In the collaborative learning phase, we upload the\nlearned decision boundaries in each client as part of the parame-\nters and aggregate them in the server. Finally, we can calculate\nthe anomaly score by the distances between the graph represen-\ntation and the centroid after training, and the smaller the score,\nthe more the graph tends to be considered normal.\n\u2022 Implementation: The implementation of FGAD is based on\nPyTorch Geometric [7] library, and the experiments are run on\nNVIDIA Tesla A100 GPU with AMD EPYC 7532 CPU.\nA.3\nTheoretical Complexity Analysis\nHere we provide theoretical complexity analysis of the proposed\nFGAD method. Assume there are \ud835\udc41graphs across all clients, and\nwith maximal \ud835\udc5anodes and |\ud835\udc38|max edges within a graph. In the\nlocal model of each client, the maximal dimension among input and\nlatent space of GIN is denoted by \u02dc\ud835\udc51, and the number of GIN layers is\nrepresented by \ud835\udc3f. In Addition, the maximal latent dimensions of the\nteacher and student heads are denoted by \ud835\udc51t and \ud835\udc51s, respectively.\nBesides, the number of latent layers in the teacher and student\nheads is denoted by \ud835\udc3et and \ud835\udc3es. Subsequently, we analyze the time\nand space complexity of FGAD within a single client, as well as the\ncommunication complexity in collaborative learning, as follows:\n\u2022 Time Complexity: Since the teacher and student models share\nthe same GIN backbone, the time complexity of the backbone net-\nwork is O(\ud835\udc41\ud835\udc3f(\ud835\udc5a\u02dc\ud835\udc512 + |\ud835\udc38|max \u02dc\ud835\udc51)). Similarly, the time complexity of\nthe anomaly generator in the teacher model mainly comes from\nthe GIN. For the teacher and student heads, the time complexi-\nties are O(\ud835\udc3et \u02dc\ud835\udc51\ud835\udc51t) and O(\ud835\udc3es \u02dc\ud835\udc51\ud835\udc51s), respectively. Consequently, the\noverall time complexity of FGAD framework is approximately\nO(2\ud835\udc41\ud835\udc3f(\ud835\udc5a\u02dc\ud835\udc512 + |\ud835\udc38|max \u02dc\ud835\udc51) + (\ud835\udc3et\ud835\udc51t + \ud835\udc3es\ud835\udc51s) \u02dc\ud835\udc51), where includes the\nanomaly generator weight-shared GIN backbone, and the teacher\nand student heads.\n\u2022 Space Complexity: For the space complexity of the GIN back-\nbone, the space complexity mainly comes from the storage of\nweight and bias matrices in each layer, which can be denoted by\nO(\ud835\udc3f\u02dc\ud835\udc51(1 + \u02dc\ud835\udc51). For the teacher and student heads, their space com-\nplexities can be derived similarly, i.e., O(\ud835\udc3et \u02dc\ud835\udc51(1+\ud835\udc51t)+\ud835\udc3es \u02dc\ud835\udc51(1+\ud835\udc51s)).\nConsequently, the overall space complexity of FGAD framework\nis approximately O(\ud835\udc3f\u02dc\ud835\udc51(1 + \u02dc\ud835\udc51) + \ud835\udc3et \u02dc\ud835\udc51(1 + \ud835\udc51t) + \ud835\udc3es \u02dc\ud835\udc51(1 + \ud835\udc51s)).\n\u2022 Communication Complexity: Since the teacher model in FGAD\nis used for the personalization of local clients, only the stu-\ndent head engages in collaboration. Consequently, the time and\nspace complexities in a communication round are approximately\nO(\ud835\udc3es \u02dc\ud835\udc51\ud835\udc51s) and O(\ud835\udc3es \u02dc\ud835\udc51(1 + \ud835\udc51s)).\nA.4\nEmpirical Complexity Analysis\nTo more comprehensively analyze the complexity of FGAD, we\nfurther provide empirical complexity analysis. Specifically, we com-\npare the running time (in local) and communication time (in collab-\noration) of FGAD with other baselines. Note that the experiment\nis conducted under uniform device settings (detailed in Appendix\nConference acronym \u2019XX, June 03\u201305, 2018, Woodstock, NY\nCai, et al.\n-20\n10\n-10\n0\n20\n0\n10\n20\n0\n-10\n-20\n-20\n(a) Running Time\n(b) Communication Cost\nFigure 7: Running time and communication cost comparison\nin 200 epochs.\n-20\n10\n-10\n0\n20\n0\n10\n20\n0\n-10\n-20\n-20\n(a) MOLECULES\n(b) IMDB-BINARY\nFigure 8: Parameter sensitivity of different dimensions for\nhidden layers.\nA.2) to ensure fairness. The experimental results are presented in\nFig. 7.\nIt can be observed that the time complexity of FGAD is competi-\ntive with several baselines, e.g., that of FedStar, and significantly\nbetter than that of GCFL. Combined with the performance compari-\nson in Tables 1 and 2 (in the paper), the overall experimental results\ndemonstrate that FGAD not only significantly improves anomaly\ndetection performance but also possesses promising time efficiency\ncompared to other baselines.\nAdditionally, communication cost (time) is also an important\nevaluation metric in federated learning. Therefore, we further con-\nduct the comparative experiment to demonstrate the effectiveness\nof FGAD. As shown in Fig. 7 (b), FGAD has the lowest communi-\ncation time compared with other baselines, which aligns with the\ncomparison of exchanging amount of network parameters in Ta-\nble 1. It should be noted that this is the analog communication time\nwithout considering the network bandwidth. When it comes to real-\nworld collaboration, the network bandwidth will significantly im-\npact the efficiency of model parameter transmission. Consequently,\nin cases of models with large parameter sizes, the communication\ntime becomes a pivotal factor influencing the time complexity of\ncollaborative learning.\nA.5\nImpact of Latent Dimensions\nHere, we further conduct additional parameter analysis for the\nimpact of the latent dimension in the GIN layer. Specifically, we\nset the latent dimension from [4, 128], and the experimental results\non MOLECULES and IMDB-BINARY shown in Fig. 8. The results\nsuggest that FGAD exhibits relatively stable performance across a\nwide range of latent layer dimensions, demonstrating its robustness.\nNevertheless, it can be observed that excessively high dimensions\n(e.g., 128) might adversely affect performance, potentially due to\nthe redundant information it brings.\nA.6\nJustification of the Backbone Sharing\nTo justify the rationale for sharing the backbone network between\nthe teacher and student models, we conduct additional experiments\nby comparing the performance of FGAD with and without shar-\ning the GIN backbone. The results are presented in Table 5. We\ncan observe only a marginal difference in performance between\nthese two strategies. This observation suggests that sharing the\nGIN backbone would not decrease the effectiveness of knowledge\ndistillation in FGAD. More importantly, the significant benefit of\nsharing the GIN backbone is the substantial reduction in model\ncomplexity. This streamlined architecture leads to a more efficient\nmodel in terms of computational resources and memory usage.\nTable 5: Peformance (mean(%) \u00b1 std(%)) of FGAD under\nshared/unshared GIN backbone.\nBackbone\nIMDB-BINARY\nIMDB-MULTI\nAUC\nAUPRC\nAUC\nAUPRC\nShared GIN\n64.97\u00b10.52 66.60\u00b11.12 60.51\u00b11.18 66.82\u00b10.14\nw/o Shared GIN\n63.13\u00b11.19 66.43\u00b12.23 58.13\u00b10.84 66.67\u00b10.00\n",
    "2401.13210": "Multitask Active Learning for Graph Anomaly Detection\nWenjing Chang\nCNIC, CAS\nUCAS\nBeijing, China\nchangwenjing@cnic.cn\nKay Liu\nUniversity of Illinois Chicago\nChicago, Illinois, USA\nzliu234@uic.edu\nKaize Ding\nNorthwestern University\nEvanston, Illinois, USA\nkaize.ding@northwestern.edu\nPhilip S. Yu\nUniversity of Illinois Chicago\nChicago, Illinois, USA\npsyu@uic.edu\nJianjun Yu\nCNIC, CAS\nBeijing, China\nyujj@cnic.ac.cn\nABSTRACT\nIn the web era, graph machine learning has been widely used on\nubiquitous graph-structured data. As a pivotal component for bol-\nstering web security and enhancing the robustness of graph-based\napplications, the significance of graph anomaly detection is con-\ntinually increasing. While Graph Neural Networks (GNNs) have\ndemonstrated efficacy in supervised and semi-supervised graph\nanomaly detection, their performance is contingent upon the avail-\nability of sufficient ground truth labels. The labor-intensive nature\nof identifying anomalies from complex graph structures poses a\nsignificant challenge in real-world applications. Despite that, the in-\ndirect supervision signals from other tasks (e.g., node classification)\nare relatively abundant. In this paper, we propose a novel MultItask\nacTIve Graph Anomaly deTEction framework, namely MITIGATE.\nFirstly, by coupling node classification tasks, MITIGATE obtains\nthe capability to detect out-of-distribution nodes without known\nanomalies. Secondly, MITIGATE quantifies the informativeness of\nnodes by the confidence difference across tasks, allowing samples\nwith conflicting predictions to provide informative yet not exces-\nsively challenging information for subsequent training. Finally, to\nenhance the likelihood of selecting representative nodes that are dis-\ntant from known patterns, MITIGATE adopts a masked aggregation\nmechanism for distance measurement, considering both inherent\nfeatures of nodes and current labeled status. Empirical studies on\nfour datasets demonstrate that MITIGATE significantly outper-\nforms the state-of-the-art methods for anomaly detection. Our code\nis publicly available at: https://github.com/AhaChang/MITIGATE.\nCCS CONCEPTS\n\u2022 Computing methodologies \u2192Active learning settings; \u2022\nMathematics of computing \u2192Graph algorithms; \u2022 Security and\nprivacy \u2192Software and application security.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nConference\u201917, July 2017, Washington, DC, USA\n\u00a9 2024 Association for Computing Machinery.\nACM ISBN 978-1-4503-XXXX-X/18/06...$15.00\nhttps://doi.org/XXXXXXX.XXXXXXX\nKEYWORDS\nAnomaly Detection, Active Learning, Graph Neural Networks\nACM Reference Format:\nWenjing Chang, Kay Liu, Kaize Ding, Philip S. Yu, and Jianjun Yu. 2024.\nMultitask Active Learning for Graph Anomaly Detection. In Proceedings\nof ACM Conference (Conference\u201917). ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/XXXXXXX.XXXXXXX\n1\nINTRODUCTION\nIn light of the proliferation of the World Wide Web, graph-structured\ndata has become increasingly pervasive. Concurrently, graph ma-\nchine learning techniques have been extensively employed in vari-\nous web mining tasks, such as recommendation systems [41], com-\nmunity detection [16], traffic forecasting [44], etc. To ensure the\nrobustness and security of such graph learning-based applications\nin web environments, graph anomaly detection serves as an indis-\npensable component. Graph anomaly detection aims to identify ab-\nnormal substructures (e.g., nodes) in graphs that exhibit significant\ndeviations from established norms. It finds extensive applications\nin capturing high-risk entities and behaviors, including but not\nlimited to spam detection [29], financial fraud detection [38], and\nfake news detection [11].\nIn accordance with the insights presented in [9, 22], unsupervised\nmethods heavily rely on the underlying data distribution to derive\noutlier patterns. Consequently, these methods may exhibit unstable\nperformance when faced with data that contains specific domain\nknowledge or deviates from the assumed distribution. However,\nthe intricate nature of graph structures, along with the high cost of\nmanual annotation for both normal and anomalous nodes, prevents\nthe collection of abundant ground truth labels, thereby limiting the\nfeasibility of applying fully supervised learning approaches. This\ncontradiction necessitates the exploration of alternative learning\nparadigms that can efficiently leverage limited supervision signals\nwhile also accommodating the complexities inherent in graph data.\nGiven the considerable expense of acquiring ground-truth labels\nfor anomaly detection, it is a judicious choice to leverage the exist-\ning relatively abundant availability of labels for other graph learn-\ning tasks. In the application of graph learning, anomaly detection\ncan enhance the stability of various tasks (e.g., node classification)\nby filtering out anomalies. Conversely, these tasks can serve as\nauxiliary tasks for anomaly detection and reciprocally provide ex-\nternal information (e.g., classification uncertainty) for augmenting\narXiv:2401.13210v1  [cs.LG]  24 Jan 2024\nConference\u201917, July 2017, Washington, DC, USA\nTrovato and Tobin, et al.\nthe efficacy of anomaly detection. Besides, these auxiliary tasks\ninherently contain general information that can be mutually lever-\naged, providing an indirect supervision signals when the anomaly\ndetection task is deficient in annotation [5, 46].\nFurthermore, to effectively leverage limited direct supervision\nsignals for anomaly detection, some semi-supervised methods have\nbeen proposed [10, 36, 38]. These methods aim to enhance the\nlearning of anomalous patterns based on the known anomalous\nnodes. The underlying assumption of these methods is that a subset\nof nodes including both normal and anomalous nodes have been\nannotated for training, and the labeled data is overall balanced.\nNevertheless, it is non-trivial to acquire such an ideal training set\nfrom an unlabeled graph, one that contains sufficient knowledge for\ndistinguishing normal and anomalous nodes, especially when con-\nstrained by a limited labeling budget. Active Learning (AL) paves a\npromising way for addressing the labeling problem, as it enables\nmodels to enhance their learning efficiency by actively requesting\nthe labels in training data [1, 3, 32, 45]. It has also been applied\nin anomaly detection tasks [7, 13, 14], aiming at discovering more\nanomalies based on heuristic query strategies, such as uncertainty-\nbased, diversity-based, and anomaly score-based strategies. In this\nway, a selection of samples that is more likely to contain a rela-\ntively higher proportion of anomalies can be used to fine-tune the\nmodel iteratively. However, existing query strategies for anomaly\ndetection are primarily designed for independent and identically dis-\ntributed data, which hardly consider relationships among samples\nand may not be well-suited for graph-structured data. Therefore,\nwe urgently need a query strategy specifically for graph data to\nprovide powerful direct supervision signals for anomaly detection.\nTo leverage the direct and indirect supervision signals efficiently\nand effectively, we propose a MultItask acTIve Graph Anomaly\ndeTEction framework (MITIGATE). It incorporates external su-\npervision signals from auxiliary tasks and productively exploits\ndirect supervision signals by actively labeling nodes for anomaly\ndetection. Specifically, we first consider a node classification task to-\ngether with the anomaly detection task. We initialize the multitask\nframework by node classification and detect out-of-distribution\nsamples (i.e., anomalies) with classification uncertainty. To query\nmore valuable nodes, we then introduce a dynamic informativeness\nmetric that relies on confidence difference and a representativeness\nmetric based on the masked aggregation mechanism. Initially, in\nthe absence of anomalies, we prioritize the informativeness metric\nto focus more on the classification uncertainty for picking out-\nof-distribution nodes to label. Afterward, in order to mitigate the\nvariation in predictions for the same node across the two tasks, we\nconsider nodes with greater confidence differences as being more\ninformative. Note that these nodes are not particularly challenging\nsamples, given that at least one of the tasks can correctly identify\nthem. To enhance the diversity of selected nodes and handle the\ninfluence of neighbors that have already been labeled for selection,\nwe re-aggregate the intermediate embedding by masking the fea-\nture of labeled neighbors and keeping a count of them. The former\nfilters the previously labeled representative features, while the lat-\nter reduces the overall neighborhood information based on labeled\nstatus, thereby emphasizing the feature of the central nodes. The\nmain contributions of this paper are summarized as follows:\n\u2022 We propose MITIGATE, a novel multitask active graph anom-\naly detection framework to detect anomalies within a limited\nlabeling budget, which actively queries nodes with the guid-\nance of external supervision signals.\n\u2022 To query more valuable nodes, we devise a dynamic strategy\nto measure the informativeness and representativeness of\nnodes according to the training and labeling status.\n\u2022 We conduct comprehensive experiments on four datasets to\nverify the effectiveness of the proposed method.\n2\nPROBLEM DEFINITION\nIn this section, we formulate the problem of active learning for\ngraph anomaly detection.\nLet G = (V, A, X) denotes an attributed graph, where V =\n{\ud835\udc631, \ud835\udc632, \u00b7 \u00b7 \u00b7 , \ud835\udc63\ud835\udc5b} is the set of nodes, A \u2208{0, 1}\ud835\udc5b\u00d7\ud835\udc5bis the adjacency\nmatrix and X \u2208R\ud835\udc5b\u00d7\ud835\udc58is the node attribute matrix. Note that anom-\naly labels are rare in the real world, but a portion of class labels\nare readily accessible. We denote the set of nodes labeled for clas-\nsification as V\ud835\udc41\n\ud835\udc3f, and their corresponding labels are denoted by\nY\ud835\udc41\n\ud835\udc3f\u2208R\ud835\udc5b\u00d7\ud835\udc36, with \ud835\udc36representing the number of classes. y\ud835\udc41\n\ud835\udc56is the\none-hot label of \ud835\udc63\ud835\udc56. Y\ud835\udc61\n\ud835\udc3f\u2208{0, 1} is the labels for anomaly detection at\nthe \ud835\udc61-th iteration, i.e., normal or anomalous.\ud835\udc66\ud835\udc34\n\ud835\udc56is the labels of \ud835\udc63\ud835\udc56for\nanomaly detection. We initialize a set of nodes with classification\nlabels, i.e., V0\n\ud835\udc3f= V\ud835\udc41\n\ud835\udc3f, and regard them as normal nodes, Y0\n\ud835\udc3f= {0}.\nThe key notations are summarized in Appendix A.\nGiven an attributed graph G, a query strategy Q, labeling budget\nB, the goal of an AL-based anomaly detection algorithm is to select\na subset of nodes denoted as S\ud835\udc61from the unlabeled node set V\ud835\udc61\u22121\n\ud835\udc48\n,\nand label them in a way that minimizes the loss of the model M:\nmin\nV\ud835\udc61\n\ud835\udc3f\nL(M, Q|G, Y\ud835\udc61\n\ud835\udc3f, Y\ud835\udc41\n\ud835\udc3f),\n(1)\nwhere V\ud835\udc61\n\ud835\udc3f= V\ud835\udc61\u22121\n\ud835\udc3f\n\u222aS\ud835\udc61and V\ud835\udc61\n\ud835\udc48= V\ud835\udc61\u22121\n\ud835\udc48\n\\ S\ud835\udc61are the labeled and\nunlabeled sets after the \ud835\udc61-th selection. \ud835\udc61\u2208{1, 2, ..., B/\ud835\udc4f} and \ud835\udc4fis the\nbudget in each iteration. Then, G, Y\ud835\udc41\n\ud835\udc3f, and labels\ud835\udc66\ud835\udc34\n\ud835\udc56for \ud835\udc63\ud835\udc56\u2208V\ud835\udc61\n\ud835\udc3fare\nused to train the model M. For convenience, we define the labeling\nbudget as the maximum number of nodes allowed to be labeled.\n3\nMETHOD\nIn this section, we introduce the proposed MITIGATE framework.\nFirstly, we give an overview of the whole framework. Then, we\nelaborate on the selection strategy including the calculation of the\ndistance features for clustering and confidence difference across\ntasks in Section 3.2. Finally, we introduce the training process of\nMITIGATE in detail in Section 3.3.\n3.1\nOverview\nFigure 1 illustrates the pipeline of MITIGATE. It utilizes a shared\nencoder for node representation learning and two decoders for\nnode classification and anomaly score prediction, respectively. Con-\nsidering the multitask structure, we devise a node informativeness\nmetric based on the confidence difference across tasks. To reduce\nthe initial performance gap, we incorporate classification uncer-\ntainty into the informativeness score measurement. To promote\ndiversity in node selection at each step, we employ the K-Medoids\nalgorithm, which treats cluster centers as representative samples,\nMultitask Active Learning for Graph Anomaly Detection\nConference\u201917, July 2017, Washington, DC, USA\nGNN Encoder\nNode Classifier\nCandidate Set\nClassification Entropy\nAnomaly Score\nMasked Aggregation\nClustering\nEmbedding\nDistance Feature \n(b)\n(a)\nRepresentativeness - Distance-based Clustering\nInformativeness - Confidence Difference\nSelection\nModel Training\nAnomalous Node \nUnlabeled Node\nNormal Node \nCenter Node\nOracle\nQuery\nLabel\nHuman-in-\nthe-loop\nAnomaly Score \nPredictor\nFigure 1: An illustration of the proposed MITIGATE. For a graph G with partial classification labels, MITIGATE employs a\nGNN encoder to generate node representations H, and then employs a node classifier and an anomaly score predictor. In each\nselection iteration, MITIGATE assesses the representativeness and informativeness of nodes using a distance-based clustering\nand confidence difference across tasks for anomaly detection, respectively. Then, it picks \ud835\udc4fnodes from clustering centers with\nhigh informative scores and queries an oracle to identify whether they are anomalies or not. Finally, the queried set will be\nincorporated into the labeled set, and continue training of the model. (a) A 2-dimensional confidence difference space. (b) An\nexample of candidates\u2019 confidence difference. We select the candidates with high confidence difference (i.e., in the upper left\ncorner and lower right corner).\nwith a novel distance measurement based on masked aggregation.\nFrom these centers, we select \ud835\udc4fnodes with the highest informa-\ntiveness scores. We then provide the set of selected nodes to an\noracle and obtain their labels (e.g., normal or anomalous) for the\nanomaly detection task. Finally, we combine the selected set with\nthe training set and continue training the model. The details of the\nencoder, node classifier, and anomaly score predictor are as follows:\n3.1.1\nEncoder. Due to both the node classifier and anomaly score\npredictor needing an encoder to reflect the graph topology and\nnode attributes into a latent space, we adopt graph convolutional\nnetworks (GCNs) [17] to learn expressive node representations, and\nthe layer-wise propagation is defined as:\nH(\ud835\udc59+1) = \ud835\udf0e( \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 H(\ud835\udc59)W(\ud835\udc59)),\n(2)\nwhere \u02dcA = A+I and \u02dcD\ud835\udc56\ud835\udc56= \u00cd\n\ud835\udc57\u02dcA\ud835\udc56\ud835\udc57, I is the identity matrix and W(\ud835\udc59)\nis the weight matrix at the \ud835\udc59-layer.\n3.1.2\nNode classifier. We use a graph convolutional layer as the\nnode classifier to further preserve the structural information of\nintermediate node representations as follows:\nZ = \ud835\udf0e( \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 HW\ud835\udc41),\n(3)\nwhere H is the final output of the encoder and W\ud835\udc41is the weight\nmatrix for the node classifier. Due to anomalies being more likely\nto be uncertain in classification, we use entropy as the measure\nof anomaly probability, where a higher entropy score indicates a\ngreater probability of being anomalous. The anomaly score of \ud835\udc63\ud835\udc56\nfrom the node classifier is\n\ud835\udc52\ud835\udc56= \u2212\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc57\nz\ud835\udc56\ud835\udc57\u00b7 log z\ud835\udc56\ud835\udc57.\n(4)\n3.1.3\nAnomaly score predictor. The anomaly score predictor is built\nwith a linear transformation along with a sigmoid function based\non shared node representations:\np = Sigmoid(W\ud835\udc34H + \ud835\udc4f\ud835\udc34),\n(5)\nwhere p \u2208R\ud835\udc5bis the predicted anomaly scores, \ud835\udc4a\ud835\udc34\u2208R1\u00d7\ud835\udc5bis the\nweight matrix, and \ud835\udc4f\ud835\udc34\u2208R is corresponding bias term.\n3.1.4\nHybrid anomaly score. Given that both the node classifier\nand anomaly score predictor possess the ability to detect anomalies,\nwe adopt a weighted score function to combine the standard scores\nof two predictions as follows:\ns = \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(e) + \ud835\udf19\u00b7 \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(p),\n(6)\nwhere s \u2208R\ud835\udc5bis the overall anomaly score, \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(e) = e\u2212\ud835\udf07\ud835\udc52\n\ud835\udf0e\ud835\udc52, \ud835\udf07\ud835\udc52is\nthe mean, \ud835\udf0e\ud835\udc52is the standard deviation, and \ud835\udf19is a hyperparameter\nto balance the importance of two predictions.\n3.2\nNode Selection\nTo benefit the overall performance for anomaly detection of the\nunified framework, we measure the value of nodes in terms of rep-\nresentativeness with distance-based clustering and informativeness\nwith confidence difference.\n3.2.1\nDistance-based Clustering. To discover representativeness\nsamples from the huge unlabeled data pool, we devise a masked\nConference\u201917, July 2017, Washington, DC, USA\nTrovato and Tobin, et al.\naggregation mechanism for generating distance features that con-\nsider representations in latent space and features in the previously\nlabeled set. Several methods [2, 12] adopt the Euclidean distance\nto measure the distance between representations H when perform-\ning clustering. This approach treats information equally among\nneighbors due to the mean aggregation mechanism in the GCN\nlayer. However, in the distance-based node selection, the distance\nfeatures should be impacted by the current labeled status in the\nneighborhood. Specifically, the chosen nodes exhibit representa-\ntional features. This is one key reason for their selection during\nprevious iterations. Thus, directly aggregating these features may\naffect the representativeness of central nodes. Therefore, we de-\nrive distance features through a masked aggregation mechanism,\nwhich considers labeled status in the neighborhood. Initially, in\norder to mitigate the impact of features pertaining to labeled neigh-\nbors, their representations will be masked during the summation of\nneighborhood information. Furthermore, to accentuate the distinc-\ntive features inherent to unlabeled nodes, we calculate the mean\nof neighborhood information according to the number of neigh-\nbors rather than the number of unlabeled neighbors. It suggests\nthat in cases where more neighbors have been annotated, the in-\nfluence of neighborhood information on the central node features\nwill be diminished. In the \ud835\udc61-th selection, the distance features can\nbe formulated as follows:\n\u02c6h\ud835\udc61\n\ud835\udc56=\nSUM(h\ud835\udc57, \u2200\ud835\udc63\ud835\udc57\u2208N (\ud835\udc63\ud835\udc56) \u2229V\ud835\udc61\u22121\n\ud835\udc48\n)\n|N (\ud835\udc63\ud835\udc56)|\n+ h\ud835\udc56,\n(7)\nwhere N (\ud835\udc63\ud835\udc56) is the neighborhood of \ud835\udc63\ud835\udc56. Therefore, the distance\nbetween \ud835\udc63\ud835\udc56and \ud835\udc63\ud835\udc57can be calculated as follows:\n\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) = ||\u02c6h\ud835\udc61\n\ud835\udc56\u2212\u02c6h\ud835\udc61\n\ud835\udc57||2.\n(8)\nTo this end, we combine the node features and labeled status of\nneighbors in the distance function, which can decrease the selec-\ntion probability due to the high representative of neighbors rather\nthan itself. After calculating the pairwise distance, we adopt K-\nMedoids clustering as previous studies [21, 40], in which centers\nchosen for the candidate set must be real nodes within the graph.\nAdditionally, the number of clusters is set to \ud835\udc5a. We do not focus\non querying more anomalies but on learning a predictive model\nto effectively distinguish normal and anomalous nodes from the\naspects of classification uncertainty and anomaly score.\n3.2.2\nConfidence Difference. As both the node classifier and anom-\naly score predictor can identify anomalies, we give a definition for\nthe model confidence in anomaly detection. For the node classifier,\nthe entropy of predictions is used to justify whether a sample is\nanomalous as Eq. (4). The higher entropy score indicates a higher\nconfidence for a node to be classified as an anomaly. Also, for the\nanomaly score predictor, the anomaly score is the indicator to de-\nscribe the level of confidence. The confidence of the node classifier,\ndenoted as c\ud835\udc41\u2208R\ud835\udc5b, and the anomaly score predictor, denoted as\nc\ud835\udc34\u2208R\ud835\udc5b, are described as follows:\nc\ud835\udc41\u221de, c\ud835\udc34\u221dp.\n(9)\nNote that the node classifier and anomaly detector do not always\nperform equally on the same nodes for anomaly detection at the\nsame stage. For example, a node may receive conflicting predic-\ntive discrimination from the node classifier and anomaly detector.\nAlgorithm 1 MITIGATE\nInput: Graph G = (V, A, X), query batch size \ud835\udc4f, total budget B,\nlabeled set for node classification V\ud835\udc3f\n\ud835\udc41, number of clusters \ud835\udc5a\nOutput: Anomaly scores s\n1: V0\n\ud835\udc3f= V\ud835\udc41\n\ud835\udc3f, V0\n\ud835\udc48= V \\ V\ud835\udc41\n\ud835\udc3f;\n2: for \ud835\udc61= 1, 2, ..., B/\ud835\udc4fdo\n3:\nM = train(G, V\ud835\udc61\u22121\n\ud835\udc3f\n, V\ud835\udc3f\n\ud835\udc41);\n4:\nCalculate distance features \u02c6H\ud835\udc61with masked agg. by Eq. (7);\n5:\nCalculate \ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) for each node in unlabeled set V\ud835\udc61\u22121\n\ud835\udc48\n;\n6:\nCluster V\ud835\udc61\u22121\n\ud835\udc48\nby K-Medoids algorithm with \ud835\udc5aclusters;\n7:\nCalculate confidence difference d and informativeness scores\n\ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc5cby Eq. (11) and Eq. (12);\n8:\nSelect the top \ud835\udc4fclustering centers as S\ud835\udc61by \ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc5c;\n9:\nQuery an Oracle to obtain labels for S\ud835\udc61;\n10:\nV\ud835\udc61\n\ud835\udc3f= V\ud835\udc61\u22121\n\ud835\udc3f\n\u222aS\ud835\udc61, V\ud835\udc61\n\ud835\udc48= V\ud835\udc61\u22121\n\ud835\udc48\n\\ S\ud835\udc61;\n11: end for\n12: M = train(G, V B/\ud835\udc4f\n\ud835\udc3f\n, V\ud835\udc3f\n\ud835\udc41);\n13: Calculate the overall anomaly scores s by Eq. (6);\nSpecifically, it has a lower value on classification entropy, which\nindicates it is likely to be a normal node, while it has a higher\nanomaly score, which means it is more likely to be anomalous. To\neliminate the influence of scale across different tasks, we normalize\nthe entropy of classification and the anomaly scores using Z-scores.\nThe confidence of the node classifier and anomaly score predictor\ncan be reformulated as:\nc\ud835\udc41= \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(e), c\ud835\udc34= \ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(p),\n(10)\nwhere a higher value of \ud835\udc50\ud835\udc41\n\ud835\udc56\nand \ud835\udc50\ud835\udc34\n\ud835\udc56indicates a lower confidence\nof \ud835\udc63\ud835\udc56being normal and a higher confidence of being anomalous.\nBased on this, we can employ the Manhattan distance to quantify\nthe confidence difference as follows:\nd = |c\ud835\udc34\u2212c\ud835\udc41|.\n(11)\nThe high confidence difference d indicates the controversy between\ntwo decoders. It\u2019s important to note that these samples with con-\nflicting predictions in binary classification are not challenging for\nmodel training since one of the tasks can effectively handle them.\nTo align the decoders and facilitate consistent prediction, human\nexperts can provide coherent information by labeling the nodes\nwith high confidence differences. The visualization of confidence\ndifferences is shown in Figure 1(a).\n3.2.3\nSelection. To judiciously select samples suitable for model\ntraining and mitigate the influence of the absence of positive sam-\nples, we introduce a time-sensitive informativeness measurement.\nFirstly, as each node can exclusively belong to either the normal or\nanomalous category, the nodes predicted to have conflicting labels\nare more likely to contain crucial information essential for one of\nthe tasks. Moreover, due to the scarcity of positive class samples\nduring the initial training stages, anomalous patterns serve as more\ninformative elements for the unified framework. Importantly, even\nwith a subset of class labels, the node classifier can provide an ini-\ntial prediction for anomalies, which is essential for the anomaly\nscore predictor. We adopt an exponentially decaying parameter \ud835\udf0f\nto dynamically combine the entropy scores of node classification\nMultitask Active Learning for Graph Anomaly Detection\nConference\u201917, July 2017, Washington, DC, USA\nand confidence difference across tasks during selection [21]. The\ninformativeness score \ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc5c\u2208R\ud835\udc5bis defined as follows:\n\ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc5c= \ud835\udf0f|V\ud835\udc61\n\ud835\udc3f|\ud835\udc41\ud835\udc5c\ud835\udc5f\ud835\udc5a(e) + (1 \u2212\ud835\udf0f|V\ud835\udc61\n\ud835\udc3f|)d,\n(12)\nwhere |V\ud835\udc61\n\ud835\udc3f| is the number of selected nodes in the \ud835\udc61-th iteration,\nand \ud835\udf0fcan be set as a number close to 1.0, e.g., 0.99. In all, the\ninformativeness score is initially influenced more by anomalies,\nand as training progresses, it shifts the emphasis toward identifying\nnodes with prediction conflicts from a model-centric perspective.\nRecognizing that both the informativeness and representative-\nness indicate the value of a node, during each iteration, MITIGATE\nfirst utilizes the aforementioned distance-based clustering algo-\nrithm to choose a subset of high representative nodes, denoted as\nthe candidate set V\ud835\udc61\n\ud835\udc36. Subsequently, a set of \ud835\udc4fnodes S\ud835\udc61are chosen\nfrom V\ud835\udc61\n\ud835\udc36according to their informativeness score. Algorithm 1\ndescribes the selection process together with the model training.\n3.3\nModel Training\nAfter each iteration of querying, MITIGATE is trained continually\nby optimizing from three aspects. First, we calculate the cross-\nentropy loss on the pre-labeled nodes V\ud835\udc41\n\ud835\udc3ffor node classification,\nwhich will not be affected by selection.\nL\ud835\udc5b\ud835\udc50= \u2212\n1\n|V\ud835\udc41\n\ud835\udc3f|\n\u2211\ufe01\n\ud835\udc63\ud835\udc56\u2208V\ud835\udc41\n\ud835\udc3f\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc57=0\n\ud835\udc66\ud835\udc41\n\ud835\udc56\ud835\udc57log\ud835\udc67\ud835\udc56\ud835\udc57.\n(13)\nwhere y\ud835\udc41\n\ud835\udc56\nis the one-hot label of node \ud835\udc63\ud835\udc56. Next, we employ a\nweighted binary cross-entropy loss, the widely used supervised\nloss for imbalanced data, on the labeled set V\ud835\udc61\n\ud835\udc3ffor anomaly detec-\ntion at each iteration.\nL\ud835\udc4e\ud835\udc51= \u2212\n1\n|V\ud835\udc61\n\ud835\udc3f|\n\u2211\ufe01\n\ud835\udc63\ud835\udc56\u2208V\ud835\udc61\n\ud835\udc3f\n(\ud835\udefe\ud835\udc61\ud835\udc66\ud835\udc34\n\ud835\udc56log\ud835\udc5d\ud835\udc56+ (1 \u2212\ud835\udc66\ud835\udc34\n\ud835\udc56) log(1 \u2212\ud835\udc5d\ud835\udc56)),\n(14)\nwhere \ud835\udefe\ud835\udc61is the ratio of anomaly to normal nodes in V\ud835\udc61\n\ud835\udc3f.\nNote that the node classifier and anomaly score predictor rely\non different types of annotations, i.e., the node classifier needs class\ninformation while the anomaly score predictor only needs to know\nwhether a node is an anomaly or not. To leverage the information\nfor the classification task from the queried set, in which nodes are\nonly annotated as normal nodes or anomalies, we optimize the\nuncertainty of classification predictions on the whole labeled set,\nwhich can be formulated as:\nL\ud835\udc62\ud835\udc5b= \u2212\n1\n|V\ud835\udc61\n\ud835\udc3f\ud835\udc41|\n\u2211\ufe01\n\ud835\udc63\ud835\udc56\u2208V\ud835\udc61\n\ud835\udc3f\ud835\udc41\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc67\ud835\udc56\ud835\udc58log\ud835\udc67\ud835\udc56\ud835\udc58+\n1\n|V\ud835\udc61\n\ud835\udc3f\ud835\udc34|\n\u2211\ufe01\n\ud835\udc63\ud835\udc57\u2208V\ud835\udc61\n\ud835\udc3f\ud835\udc34\n\ud835\udc36\n\u2211\ufe01\n\ud835\udc58\n\ud835\udc67\ud835\udc57\ud835\udc58log\ud835\udc67\ud835\udc57\ud835\udc58,\n(15)\nwhere V\ud835\udc61\n\ud835\udc3f\ud835\udc41and V\ud835\udc61\n\ud835\udc3f\ud835\udc34denotes the normal and anomalous node\nset at the \ud835\udc61-th iteration. In Eq. (15), we minimize the predicted\nuncertainty for normal nodes (the first term), while maximizing\nthat for anomalies (the second term).\nIn all, the overall loss function of MITIGATE can be formulated\nas Eq. (16), where \ud835\udefcand \ud835\udefdare weighting parameters.\nL = \ud835\udefc\u00b7 L\ud835\udc5b\ud835\udc50+ \ud835\udefd\u00b7 L\ud835\udc4e\ud835\udc51+ L\ud835\udc62\ud835\udc5b.\n(16)\n4\nEXPERIMENTS\nIn this section, we extensively compare MITIGATE with state-of-\nthe-art methods for anomaly detection.\n4.1\nExperiments Settings\n4.1.1\nDatasets. In our experiments, we adopt four widely used\ndatasets with ground-truth labels for node classification. As there\nis no ground truth of anomaly detection, we inject contextual and\nstructural anomalies following the previous studies [8, 22, 27] to\nevaluate the effectiveness for anomaly detection of our method. We\nuse two citation networks, Cora and Citeseer [31], and two social\nnetworks, BlogCatalog and Flickr [37]. The details of the datasets\nare described in Appendix B.1. For each dataset, we randomly sam-\nple 500 nodes as a validation set and 1000 nodes as a test set and\nfix them for all methods for a fair comparison.\n4.1.2\nBaselines. We compare MITIGATE with three types of base-\nlines, including (1) out-of-distribution (OOD) detection meth-\nods, including GCN-ENT [17], GKDE [47], OODGAT-ENT and\nOODGAT-ATT [34], (2) semi-supervised anomaly detection\nmethods, including FdGars [39], GeniePath [25], BWGNN [36],\nand DAGAD [20], and (3) active query strategy for anomaly de-\ntection, including most positive query (GCN-Pos), positive diverse\nquery (GCN-PosD) and diverse query [18] (GCN-Div). The details\nof these methods and their implementation are in Appendix B.2.\n4.1.3\nImplementation Detail. In our experiments, we set the max\nbudget B = 80, which means 80 nodes can be labeled, and we set\n\ud835\udc4f= 4 at each iteration. We assume 20\ud835\udc58nodes have been annotated\nwith classification labeled, where \ud835\udc58is the number of classes, and\nno anomalies as initial. For MITIGATE, we adopt Adam optimizer\nand the learning rate is set to 0.01. As to \ud835\udefcand \ud835\udefdin the overall loss\nfunction (Eq. (16)), \ud835\udf0fin the informativeness score function (Eq. (12)),\nthe number of clusters \ud835\udc5ain K-Medoids algorithm, we tune the hy-\nperparameters and select the best-performing results according to\nthe validation set, the details are shown in Appendix B.3. We set\nthe maximum epochs of training in each iteration to 300 and per-\nform early-stopping when (\ud835\udc34\ud835\udc48\ud835\udc36\ud835\udc45\ud835\udc42\ud835\udc36+\ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66) stops to increase\nfor 20 epochs. \ud835\udc34\ud835\udc50\ud835\udc50\ud835\udc62\ud835\udc5f\ud835\udc4e\ud835\udc50\ud835\udc66is the accuracy of the node classifier on\nin-distribution nodes and \ud835\udc34\ud835\udc48\ud835\udc36\ud835\udc45\ud835\udc42\ud835\udc36is the performance evaluation\nof the anomaly score predictor. We implement two variants of MIT-\nIGATE, namely MITIGATE-A and MITIGATE-E. MITIGATE-A\nuses predicted anomaly scores, while MITIGATE-E employs the\nentropy of classification as the final score for anomaly detection.\nWe adopt two widely used complementary measures in previous\nstudies, including the Area Under Receiver Operating Characteristic\nCurve (AUC-ROC) and Area Under Precision-Recall Curve (AUC-\nPR). To mitigate results randomness, we run the proposed method\nand baselines 5 times with different random seeds and record the\naverage scores and standard deviation.\n4.2\nEvaluation Results\nWe evaluate the anomaly detection performance of MITIGATE\nand all compared methods mentioned in Section 4.1.2 on the four\ndatasets. The results are shown in Table 1 and Figure 2.\nConference\u201917, July 2017, Washington, DC, USA\nTrovato and Tobin, et al.\nTable 1: Overall performance comparison in AUC-ROC(%) and AUC-PR(%) on four datasets. The best results are highlighted in\nbold, and the second best is underlined.\nCora\nCiteseer\nBlogCatalog\nFlickr\nMethod\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nGCN-ENT\n64.82\u00b11.07\n10.48\u00b10.51\n69.36\u00b10.92\n9.30\u00b10.98\n62.57\u00b10.91\n11.70\u00b11.99\n64.62\u00b10.77\n9.51\u00b11.03\nGKDE\n72.25\u00b10.85\n15.21\u00b11.79\n74.87\u00b10.61\n17.07\u00b11.55\n44.56\u00b10.07\n5.29\u00b10.02\n44.81\u00b10.09\n5.05\u00b10.03\nOODGAT-ENT\n69.70\u00b13.50\n15.11\u00b16.65\n70.01\u00b14.45\n12.22\u00b19.96\n56.52\u00b12.24\n8.94\u00b11.55\n53.52\u00b11.83\n7.60\u00b10.63\nOODGAT-ATT\n50.76\u00b14.71\n5.33\u00b10.73\n55.33\u00b14.95\n6.71\u00b11.63\n49.51\u00b14.18\n6.13\u00b11.17\n51.17\u00b11.24\n6.46\u00b10.93\nFdGars\n58.55\u00b16.22\n6.51\u00b11.22\n54.61\u00b19.09\n5.12\u00b11.55\n51.99\u00b13.38\n6.01\u00b10.24\n57.95\u00b16.61\n13.97\u00b18.81\nGeniePath\n54.42\u00b16.41\n6.41\u00b10.35\n48.89\u00b14.94\n4.94\u00b10.32\n50.25\u00b11.81\n5.80\u00b10.20\n50.26\u00b11.21\n5.97\u00b10.14\nBWGNN\n64.77\u00b10.20\n9.44\u00b10.16\n64.42\u00b10.60\n6.36\u00b10.10\n60.18\u00b11.25\n8.94\u00b10.23\n52.22\u00b11.14\n5.96\u00b10.12\nDAGAD\n58.30\u00b10.30\n19.17\u00b14.83\n61.52\u00b13.21\n14.10\u00b11.82\n56.52\u00b12.89\n9.03\u00b10.92\n63.22\u00b10.33\n11.66\u00b14.40\nGCN-Pos\n57.22\u00b13.54\n9.90\u00b10.83\n59.78\u00b14.78\n8.09\u00b11.37\n62.52\u00b12.70\n9.96\u00b11.93\n55.56\u00b12.78\n7.03\u00b11.93\nGCN-PosD\n62.24\u00b12.07\n12.00\u00b11.11\n61.54\u00b10.68\n7.54\u00b10.21\n62.42\u00b11.63\n9.79\u00b11.77\n57.24\u00b14.85\n10.39\u00b14.86\nGCN-Div\n61.23\u00b12.88\n9.05\u00b11.00\n56.54\u00b17.19\n10.55\u00b13.42\n65.51\u00b13.86\n11.45\u00b11.50\n66.89\u00b12.02\n10.44\u00b11.76\nMITIGATE-A\n73.59\u00b13.53\n17.00\u00b12.55\n71.25\u00b13.00\n20.21\u00b16.48\n63.94\u00b13.08\n12.40\u00b12.50\n68.89\u00b13.37\n15.69\u00b11.72\nMITIGATE-E\n72.81\u00b11.52\n15.98\u00b11.95\n74.12\u00b11.53\n14.60\u00b11.97\n63.00\u00b12.10\n12.09\u00b12.67\n68.26\u00b13.01\n14.18\u00b13.83\nMITIGATE\n75.45\u00b12.27\n18.80\u00b12.46\n78.03\u00b10.94\n23.32\u00b16.63\n66.20\u00b13.04\n13.60\u00b12.74\n70.16\u00b13.00\n17.33\u00b11.95\n(c) BlogCatalog\n0\n20\n40\n60\n80\n56\n58\n60\n62\n64\n66\n68\nNumber of selected nodes\nAUC-ROC (%)\n0\n20\n40\n60\n80\n4\n6\n8\n10\n12\n14\n16\nNumber of selected nodes\nAUC-PR (%)\n(d) Flickr\n0\n20\n40\n60\n80\n42\n47\n52\n57\n62\n67\n72\nNumber of selected nodes\nAUC-ROC (%)\n0\n20\n40\n60\n80\n0\n3\n6\n9\n12\n15\n18\nNumber of selected nodes\nAUC-PR (%)\n(b) Citeseer\n0\n20\n40\n60\n80\n22\n32\n42\n52\n62\n72\n82\nNumber of selected nodes\nAUC-ROC (%)\n0\n20\n40\n60\n80\n0\n5\n10\n15\n20\n25\n30\nNumber of selected nodes\nAUC-PR (%)\n(a) Cora\n0\n20\n40\n60 \n80\n42\n48\n54\n60\n66\n72\n78\nNumber of selected nodes\nAUC-ROC (%)\n0\n20\n40\n60\n80\n2\n5\n8\n11\n14\n17\n20\nNumber of selected nodes\nAUC-PR (%)\nFigure 2: Performance over different numbers of labeled nodes in selection averaged from 5 runs on four datasets.\n4.2.1\nOverall Comparison. We evaluate the overall performance\nwhen the labeling budget B is set to 80 (i.e., a maximum of 80\nnodes can be labeled). The corresponding AUC-ROC and AUC-PR\nare reported in Table 1. We observe that MITIGATE significantly\noutperforms other methods under most metrics. (1) Comparison\nwith classification methods. GKDE and OODGAT are two state-\nof-the-art methods for OOD detection, which only utilize a portion\nof labeled in-distribution nodes for training. Different from them,\nour MITIGATE adopts anomalies (i.e., out-of-distribution nodes)\nin the training process to better learn a decision boundary for\nanomaly detection. Experimental results demonstrate that MITI-\nGATE outperforms these two methods, with at least 3.2% and 3.6%\nimprovement in AUC-ROC and AUC-PR, respectively. (2) Com-\nparison with semi-supervised anomaly detection methods.\nFdGars, GeniePath, BWGNN, and DAGAD are superior methods\nfor node anomaly detection. However, these methods heavily rely\non labeled data. When the labeling budget is low, which may lead\nto the absence of anomalies, these methods become frangible. It is\nevident that MITIGATE outperforms them, particularly in terms of\nAUC-ROC. (3) Comparison with query strategies for anomaly\ndetection. These query strategies focus on selecting anomalies or\nhigh-uncertainty samples in anomaly detection. Though they may\nchoose more anomalies, their performance remains suboptimal due\nto anomalies not always contributing significantly to the model and\nthe potential challenges posed by uncertain nodes. More specifically,\nan anomalous sample, sharing similarities with known anomalous\npatterns, can yield a high anomaly score but may not provide sub-\nstantially novel information to the model. Besides, hard samples\nthat contain noisy information for model training also exhibit high\nuncertainty, and selecting such samples potentially hinders the\nMultitask Active Learning for Graph Anomaly Detection\nConference\u201917, July 2017, Washington, DC, USA\n(a) AUC-ROC\n(b) AUC-PR\nFigure 3: Weight analysis for the node classifier and anomaly\nscore predictor with various values of \ud835\udefcand \ud835\udefdon Citeseer.\nmodel performance. Unlike them, MITIGATE selects samples based\non the confidence difference across tasks. This implies that the\nchosen samples are informative for at least one task and are not\nexcessively challenging, as one of the tasks can accurately identify\nthem. (4) Comparison with MITIGATE varients. MITIGATE-A,\nMITIGATE-E, and MITIGATE differ in their final score for anomaly\ndetection. It is observed that the results of using a single indicator\ndo not differ significantly in identifying anomalies in most cases,\nwhereas the weighted sum of these two components consistently\noffers discernible advantages. This is because MITIGATE-A and\nMITIGATE-E detect anomalies from a single aspect, namely the\nuncertainty of classification and the learnable anomaly scores, re-\nspectively, without incorporating external information from other\nfacets. Moreover, this observation highlights the effectiveness of\nintegrating classification tasks and anomaly detection tasks in the\nprocess of identifying anomalies.\n4.2.2\nPerformance under Different Budget. We evaluate the per-\nformance of our proposed MITIGATE and several active query\nstrategies for anomaly detection over the different numbers of\nlabeled nodes for training. The results are shown in Figure 2. Com-\npared with the other baselines, we can observe that MITIGATE\nachieves the best performance in AUC-ROC under each labeling\nbudget in most of the compared settings. In particular, to achieve\nthe AUC-ROC of 66.8% on Flickr, GCN-Div labels 80 nodes while\nMITIGATE only labels 16 nodes. We contribute the effectiveness\nof MITIGATE at a low labeling budget with the classification task,\nwhich can provide initial discrimination for anomalies and alleviate\nthe absence of anomalies problem.\n4.3\nParameter Analysis\nWe further analyze the majority of parameters in MITIGATE on\nCiteseer. The results are shown in Figure 3 and Figure 4.\n4.3.1\nImpacts of\ud835\udefcand \ud835\udefd. We analyze the impact of varying weights\nin the overall loss function as shown in Eq. (16). These weights\nare crucial in achieving a balance among the three components\nof the loss, namely the node classification loss, anomaly detec-\ntion loss, and uncertainty loss. We evaluate MITIGATE for \ud835\udefc\u2208\n{0.4, 0.5, 0.8, 1.0, 1.25, 2, 2.5} and \ud835\udefd\u2208{0.4, 0.5, 0.8, 1.0, 1.25, 2, 2.5}\nand report the average results of 5 runs in Figure 3. We can observe\n(a) \ud835\udf19\n(b) \ud835\udc5a\nFigure 4: Sensitivity analysis on Citeseer for (a) overall anom-\naly scores weight term \ud835\udf19, and (b) number of cluster \ud835\udc5a.\nTable 2: Ablation study on Citeseer and Flickr. The best re-\nsults are highlighted in bold.\nDataset\nVariants\nAUC-ROC\nAUC-PR\nCiteseer\nw/o uncertainty loss\n72.33\u00b11.55\n10.80\u00b11.71\nw/o entropy score\n76.35\u00b13.77\n15.75\u00b14.77\nw/o confidence difference\n73.62\u00b11.77\n14.79\u00b13.85\nw/o masked aggregation\n71.95\u00b14.94\n11.81\u00b12.94\nw/o clustering\n69.33\u00b11.87\n21.22\u00b11.69\nMITIGATE\n78.03\u00b10.94\n23.32\u00b16.63\nFlickr\nw/o uncertainty loss\n66.13\u00b10.95\n10.43\u00b11.45\nw/o entropy score\n65.51\u00b13.12\n10.06\u00b11.82\nw/o confidence difference\n64.85\u00b11.62\n12.76\u00b11.63\nw/o masked aggregation\n67.57\u00b13.58\n12.62\u00b12.44\nw/o clustering\n68.48\u00b12.04\n12.45\u00b13.01\nMITIGATE\n70.16\u00b13.00\n17.33\u00b11.95\nthat lower values of \ud835\udefcand higher values of \ud835\udefdlead to better perfor-\nmances. One possible reason is that the category characteristics in\nCiteseer are easy to differentiate. It further suggests that adjusting \ud835\udefc\nand \ud835\udefdproperly can bring more benefits to the overall performance.\n4.3.2\nImpacts of\ud835\udf19. The weight term\ud835\udf19is essential in calculating the\nfinal anomaly scores by balancing the importance of classification\nuncertainty and learnable anomaly scores in Eq. (6). Figure 4(a)\npresents the AUC-ROC and AUC-PR of MITIGATE when varying\n\ud835\udf19\u2208{1.6, 1.8, 2, 2.2, 2.4}. It is evident that although both terms are\ncapable of identifying anomalies, their contributions are not equal.\nThe optimal weight ratio between classification uncertainty and\nlearnable anomaly score for the final anomaly score is 1:2 (i.e.,\n\ud835\udf19= 2), indicating the anomaly predictor exerts a more pronounced\ninfluence in identifying anomalies.\n4.3.3\nImpacts of \ud835\udc5a. As clustering plays a pivotal role in choosing\nrepresentative nodes from the unlabeled set, we investigate the\nperformance of MITIGATE by varying the number of clusters \ud835\udc5a\nfrom 1 to 6 times the class number. As shown in Figure 4(b), we\ncan observe that the optimal value of \ud835\udc5atends to be near 4\ud835\udc58, which\nleads to the optimal AUC-ROC and AUC-PR.\n4.4\nAblation Study\nWe conduct an ablation study to examine the contribution of each\nkey component in the proposed framework on Citeseer and Flickr.\nThe results are shown in Table 2.\nConference\u201917, July 2017, Washington, DC, USA\nTrovato and Tobin, et al.\n\u2022 w/o uncertainty loss: it removes the uncertainty loss on selected\nnodes in Eq. (16).\n\u2022 w/o entropy score: it removes the classification uncertainty\nscore in informativeness measurement in Eq. (12).\n\u2022 w/o confidence difference: it removes the confidence difference\nbetween tasks in informativeness measurement in Eq. (12).\n\u2022 w/o masked aggregation: it replaces distance features calcu-\nlated through masked aggregation as Eq. (7) with the representa-\ntion in latent space obtained from Eq. (2).\n\u2022 w/o clustering: it removes the distance-based clustering, which\naims to discover representative nodes.\n4.4.1\nUncertainty Loss. As shown in Table 2, we see that MITI-\nGATE notably outperforms MITIGATE w/o uncertainty loss, ex-\nhibiting improvements of 5.7% and 12.5% in terms of AUC-ROC\nand AUC-PR, respectively. It indicates that incorporating the un-\ncertainty loss from the classification perspective on selected nodes,\nwhich are exclusively labeled as normal or anomalous, can substan-\ntially improve the anomaly detection performance of MITIGATE.\n4.4.2\nStrategy of Node Selection. To verify the effectiveness of\nthe proposed selection strategy, we conduct ablation tests on four\nvariants. First, compared with MITIGATE w/o entropy score and\nMITIGATE w/o confidence difference, which remove partial of the\ninformativeness score respectively, MITIGATE achieves the best\nperformance. It indicates that both the classification uncertainty\nand confidence difference contribute to the node informativeness,\nand the dynamic combination of them is also effective. This is ex-\npected since the uncertainty loss is not a direct supervision for the\nnode classifier and the performance of the anomaly score predictor\nperforms worse at the beginning without anomalous samples. Be-\nsides, the comparison between MITIGATE w/o masked aggregation\nand MITIGATE w/o clustering indicates that the clustering makes\na minimal or even negative contribution without efficient distance\nmeasurement. For instance, MITIGATE w/o clustering surpasses\nMITIGATE w/o masked aggregation by approximately 10% in terms\nof AUC-PR on Citeseer. Furthermore, when comparing MITIGATE\nand MITIGATE w/o masked aggregation, it is obvious that the pro-\nposed masked aggregation effectively improves the discriminative\ncapability in anomaly detection. We argue that by masking repre-\nsentations for previously labeled nodes within the neighborhood,\nmore representative nodes are selected in relation to both labeled\nand unlabeled sets.\n5\nRELATED WORK\n5.1\nActive Learning for Anomaly Detection\nActive learning aims to interactively select samples from unlabeled\ndata to maximize model performance within limited labeling bud-\ngets. It has been extensively studied in the field of anomaly detec-\ntion [7, 13, 14, 43]. In contrast to traditional active learning, active\nanomaly detection focuses on discovering more anomalous samples.\nSeveral methods incorporate active queried data with the unsuper-\nvised learning paradigm in the training process. For instance, [14]\nsuggests querying samples close to the decision boundary, which\nmeans more uncertainty. [13] suggests querying samples based on\ndensity to ensure a diverse distribution of predicted anomalous data\nfor querying. Devising an effective query strategy to discover more\nanomalies becomes an important problem. [7, 28] aim to query the\nmost anomalous sample according to predicted scores, while [6]\nincorporates the density information with anomaly score. How-\never, these greedy strategies prioritize short-term gains and may\nyield suboptimal results. To address this issue, deep reinforcement\nlearning has been introduced to the design of query strategies. For\nexample, Meta-AAD [43] trains a meta-policy using auxiliary la-\nbeled datasets so that it can be directly applied to new unlabeled\ndatasets without further tuning. In our work, we focus on improv-\ning the predictive performance for graph anomaly detection by\nleveraging corresponding auxiliary tasks to help the model training\nand sample querying processes.\n5.2\nGraph Anomaly Detection\nGraph anomaly detection methods [4, 36, 38, 39] have been exten-\nsively studied in recent years, which assumes that a set of nodes\nhave been labeled. Motivated by general GNN algorithms, several\ngraph anomaly detection methods based on redesigned message\npassing and aggregation mechanisms have been proposed. For ex-\nample, FdGars [39] utilizes GCN to combine the characteristics of\nreviewers and their relationships. Semi-GNN [38] employs hierar-\nchical attention to model the multi-view graph for fraud detection.\nGraphConsis [26] proposes to filter inconsistent neighbors in ag-\ngregation to maintain the unique semantic characteristics of the\ntarget node. Moreover, the imbalance between the majority and\nminority classes is another essential problem in anomaly detec-\ntion. To alleviate this problem, PC-GNN [23] incorporates label\ndistribution information to sample neighbors in the aggregation\nprocess, while DAGAD [20] devises a graph data augmentation\nmodule to fertilize training set with generated samples. Different\nfrom the aforementioned spatial methods, BWGNN [36] leverages\nspectral distribution information to capture graph anomalies with\nhigh-frequency and AMNet [4] adaptively combines signals of low-\nfrequency and high-frequency to learn the node embedding for\ndistinguishing the anomalous nodes. However, these methods may\nnot work well when the labeling budget is relatively low since they\nhighly rely on the initial labeled set, i.e., at least one negative sam-\nple needs to be labeled. Whereas, we focus on actively selecting\nnodes for annotation during the training process, which means the\nselected labeled nodes can bring more benefits to the target model.\n5.3\nOut-of-distribution Detection\nOut-of-distribution (OOD) detection aims to distinguish samples\ndrawn from a distribution different from the labeled in-distribution\nsamples [49]. Extensive studies [19, 30, 42, 48] have been proposed\nfor OOD detection on structured data, such as text and images. How-\never, graph data contains not only structured attributes but also\nnon-Euclidean topology structures. Consequently, several graph-\nspecific OOD detection methods have been proposed, including\nuncertainty-based methods [35, 47] and graph learning-based meth-\nods [15, 34]. Uncertainty-based methods aim to use the confidence\nof a well-trained model on partial ID samples. GKDE [47] adopts\nseveral uncertainty estimates-based metrics for OOD detection\nand finds the vacuity-based model has optimal performance, while\nGPN [35] devises an input-independent Bayesian update rule to\nmodel uncertainty on predicted categorical distribution. Besides,\nMultitask Active Learning for Graph Anomaly Detection\nConference\u201917, July 2017, Washington, DC, USA\ngraph learning-based methods argue that the OOD samples can\naffect ID samples under the message-passing mechanism. For ex-\nample, OODGAT [34] explicitly models the interaction between\ninliners and outliers, while LMN [15] learns to mix neighbors to mit-\nigate the influence from OOD nodes. However, these methods have\na limitation in that they cannot effectively utilize the supervision\ninformation from OOD samples.\n6\nCONCLUSION\nIn this paper, we proposed MITIGATE, a novel multitask active\nlearning framework for graph anomaly detection within a limited\nlabeling budget. MITIGATE comprises a node classifier and an\nanomaly score predictor, with the primary goal of enhancing the\noverall performance of anomaly detection through the selection of\nthe queried set. Besides, the masked aggregation mechanism for dis-\ntance features proves instrumental in selecting representative nodes.\nAdditionally, node informativeness is dynamically measured by the\nconfidence difference across tasks and classification uncertainty.\nExperimental results on four datasets demonstrate the effectiveness\nof MITIGATE compared with the state-of-the-art methods.\nREFERENCES\n[1] C. C. Aggarwal, X. Kong, Q. Gu, J. Han, and S. Y. Philip. Active learning: A survey.\nIn Data classification, pages 599\u2013634. Chapman and Hall/CRC, 2014.\n[2] H. Cai, V. W. Zheng, and K. C.-C. Chang. Active learning for graph embedding.\narXiv preprint arXiv:1705.05085, 2017.\n[3] C. Campbell, N. Cristianini, A. Smola, et al. Query learning with large margin\nclassifiers. In ICML, volume 20, page 0, 2000.\n[4] Z. Chai, S. You, Y. Yang, S. Pu, J. Xu, H. Cai, and W. Jiang. Can abnormality be\ndetected by graph neural networks. In Proceedings of the Twenty-Ninth Interna-\ntional Joint Conference on Artificial Intelligence (IJCAI), Vienna, Austria, pages\n23\u201329, 2022.\n[5] R. Collobert and J. Weston. A unified architecture for natural language process-\ning: Deep neural networks with multitask learning. In Proceedings of the 25th\ninternational conference on Machine learning, pages 160\u2013167, 2008.\n[6] S. Das, M. R. Islam, N. K. Jayakodi, and J. R. Doppa. Active anomaly detec-\ntion via ensembles: Insights, algorithms, and interpretability. arXiv preprint\narXiv:1901.08930, 2019.\n[7] S. Das, W.-K. Wong, T. Dietterich, A. Fern, and A. Emmott. Incorporating expert\nfeedback into active anomaly discovery. In 2016 IEEE 16th International Conference\non Data Mining (ICDM), pages 853\u2013858. IEEE, 2016.\n[8] K. Ding, J. Li, R. Bhanushali, and H. Liu. Deep anomaly detection on attributed\nnetworks. In Proceedings of the 2019 SIAM International Conference on Data\nMining, pages 594\u2013602. SIAM, 2019.\n[9] K. Ding, Q. Zhou, H. Tong, and H. Liu. Few-shot network anomaly detection via\ncross-network meta-learning. In Proceedings of the Web Conference 2021, pages\n2448\u20132456, 2021.\n[10] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu. Enhancing graph neural\nnetwork-based fraud detectors against camouflaged fraudsters. In Proceedings of\nthe 29th ACM international conference on information & knowledge management,\npages 315\u2013324, 2020.\n[11] Y. Dou, K. Shu, C. Xia, P. S. Yu, and L. Sun. User preference-aware fake news\ndetection. In Proceedings of the 44th International ACM SIGIR Conference on\nResearch and Development in Information Retrieval, pages 2051\u20132055, 2021.\n[12] L. Gao, H. Yang, C. Zhou, J. Wu, S. Pan, and Y. Hu. Active discriminative network\nrepresentation learning. In IJCAI International Joint Conference on Artificial\nIntelligence, 2018.\n[13] A. Ghasemi, H. R. Rabiee, M. Fadaee, M. T. Manzuri, and M. H. Rohban. Ac-\ntive learning from positive and unlabeled data. In 2011 IEEE 11th International\nConference on Data Mining Workshops, pages 244\u2013250. IEEE, 2011.\n[14] N. G\u00f6rnitz, M. Kloft, K. Rieck, and U. Brefeld. Toward supervised anomaly\ndetection. Journal of Artificial Intelligence Research, 46:235\u2013262, 2013.\n[15] T. Huang, D. Wang, and Y. Fang. End-to-end open-set semi-supervised node\nclassification with out-of-distribution detection. In Proceedings of the Thirty-First\nInternational Joint Conference on Artificial Intelligence, IJCAI 2022, 2022.\n[16] D. Jin, Z. Yu, P. Jiao, S. Pan, D. He, J. Wu, S. Y. Philip, and W. Zhang. A survey\nof community detection approaches: From statistical modeling to deep learning.\nIEEE Transactions on Knowledge and Data Engineering, 35(2):1149\u20131170, 2021.\n[17] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolu-\ntional networks. arXiv preprint arXiv:1609.02907, 2016.\n[18] A. Li, C. Qiu, M. Kloft, P. Smyth, S. Mandt, and M. Rudolph. Deep anomaly\ndetection under labeling budget constraints. In International Conference on\nMachine Learning, pages 19882\u201319910. PMLR, 2023.\n[19] S. Liang, Y. Li, and R. Srikant. Enhancing the reliability of out-of-distribution\nimage detection in neural networks. In International Conference on Learning\nRepresentations, 2018.\n[20] F. Liu, X. Ma, J. Wu, J. Yang, S. Xue, A. Beheshti, C. Zhou, H. Peng, Q. Z. Sheng,\nand C. C. Aggarwal. Dagad: Data augmentation for graph anomaly detection. In\n2022 IEEE International Conference on Data Mining (ICDM), pages 259\u2013268. IEEE,\n2022.\n[21] J. Liu, Y. Wang, B. Hooi, R. Yang, and X. Xiao. Lscale: Latent space clustering-\nbased active learning for node classification. In Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases, pages 55\u201370. Springer,\n2022.\n[22] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen, H. Peng,\nK. Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static\nattributed graphs. Advances in Neural Information Processing Systems, 35:27021\u2013\n27035, 2022.\n[23] Y. Liu, X. Ao, Z. Qin, J. Chi, J. Feng, H. Yang, and Q. He. Pick and choose: a\ngnn-based imbalanced learning approach for fraud detection. In Proceedings of\nthe web conference 2021, pages 3168\u20133177, 2021.\n[24] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis. Anomaly detection on\nattributed networks via contrastive self-supervised learning. IEEE transactions\non neural networks and learning systems, 33(6):2378\u20132392, 2021.\n[25] Z. Liu, C. Chen, L. Li, J. Zhou, X. Li, L. Song, and Y. Qi. Geniepath: Graph neural\nnetworks with adaptive receptive paths. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 33, pages 4424\u20134431, 2019.\n[26] Z. Liu, Y. Dou, P. S. Yu, Y. Deng, and H. Peng. Alleviating the inconsistency\nproblem of applying graph neural network to fraud detection. In Proceedings\nof the 43rd international ACM SIGIR conference on research and development in\ninformation retrieval, pages 1569\u20131572, 2020.\n[27] X. Luo, J. Wu, A. Beheshti, J. Yang, X. Zhang, Y. Wang, and S. Xue. Comga:\nCommunity-aware attributed graph anomaly detection. In Proceedings of the\nFifteenth ACM International Conference on Web Search and Data Mining, pages\n657\u2013665, 2022.\n[28] J. Ning, L. Chen, C. Zhou, and Y. Wen. Deep active autoencoders for outlier\ndetection. Neural Processing Letters, pages 1\u201313, 2022.\n[29] S. Noekhah, N. binti Salim, and N. H. Zakaria. Opinion spam detection: Us-\ning multi-iterative graph-based model. Information processing & management,\n57(1):102140, 2020.\n[30] V. Sehwag, M. Chiang, and P. Mittal. Ssd: A unified framework for self-supervised\noutlier detection. In International Conference on Learning Representations, 2020.\n[31] P. Sen, G. Namata, M. Bilgic, L. Getoor, B. Galligher, and T. Eliassi-Rad. Collective\nclassification in network data. AI magazine, 29(3):93\u201393, 2008.\n[32] B. Settles. Active learning literature survey. University of Wisconsin-Madison\nDepartment of Computer Sciences, 2009.\n[33] D. B. Skillicorn. Detecting anomalies in graphs. In 2007 IEEE Intelligence and\nSecurity Informatics, pages 209\u2013216. IEEE, 2007.\n[34] Y. Song and D. Wang. Learning on graphs with out-of-distribution nodes. In\nProceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pages 1635\u20131645, 2022.\n[35] M. Stadler, B. Charpentier, S. Geisler, D. Z\u00fcgner, and S. G\u00fcnnemann. Graph pos-\nterior network: Bayesian predictive uncertainty for node classification. Advances\nin Neural Information Processing Systems, 34:18033\u201318048, 2021.\n[36] J. Tang, J. Li, Z. Gao, and J. Li. Rethinking graph neural networks for anomaly\ndetection. In International Conference on Machine Learning, pages 21076\u201321089.\nPMLR, 2022.\n[37] L. Tang and H. Liu. Relational learning via latent social dimensions. In Proceedings\nof the 15th ACM SIGKDD international conference on Knowledge discovery and\ndata mining, pages 817\u2013826, 2009.\n[38] D. Wang, J. Lin, P. Cui, Q. Jia, Z. Wang, Y. Fang, Q. Yu, J. Zhou, S. Yang, and Y. Qi.\nA semi-supervised graph attentive network for financial fraud detection. In 2019\nIEEE International Conference on Data Mining (ICDM), pages 598\u2013607. IEEE, 2019.\n[39] J. Wang, R. Wen, C. Wu, Y. Huang, and J. Xiong. Fdgars: Fraudster detection\nvia graph convolutional networks in online app review system. In Companion\nproceedings of the 2019 World Wide Web conference, pages 310\u2013316, 2019.\n[40] Y. Wu, Y. Xu, A. Singh, Y. Yang, and A. Dubrawski. Active learning for graph\nneural networks via node feature propagation. arXiv preprint arXiv:1910.07567,\n2019.\n[41] L. Yang, Z. Liu, Y. Dou, J. Ma, and P. S. Yu. Consisrec: Enhancing gnn for social\nrecommendation via consistent neighbor aggregation. In Proceedings of the 44th\ninternational ACM SIGIR conference on Research and development in information\nretrieval, pages 2141\u20132145, 2021.\n[42] Q. Yu and K. Aizawa. Unsupervised out-of-distribution detection by maximum\nclassifier discrepancy. In Proceedings of the IEEE/CVF international conference on\ncomputer vision, pages 9518\u20139526, 2019.\nConference\u201917, July 2017, Washington, DC, USA\nTrovato and Tobin, et al.\n[43] D. Zha, K.-H. Lai, M. Wan, and X. Hu. Meta-aad: Active anomaly detection\nwith deep reinforcement learning. In 2020 IEEE International Conference on Data\nMining (ICDM), pages 771\u2013780. IEEE, 2020.\n[44] Q. Zhang, K. Yu, Z. Guo, S. Garg, J. J. Rodrigues, M. M. Hassan, and M. Guizani.\nGraph neural network-driven traffic forecasting for the connected internet of\nvehicles. IEEE Transactions on Network Science and Engineering, 9(5):3015\u20133027,\n2021.\n[45] W. Zhang, Y. Shen, Y. Li, L. Chen, Z. Yang, and B. Cui. Alg: Fast and accurate\nactive learning framework for graph convolutional networks. In Proceedings of\nthe 2021 International Conference on Management of Data, pages 2366\u20132374, 2021.\n[46] Y. Zhang and Q. Yang. A survey on multi-task learning. IEEE Transactions on\nKnowledge and Data Engineering, 34(12):5586\u20135609, 2021.\n[47] X. Zhao, F. Chen, S. Hu, and J.-H. Cho. Uncertainty aware semi-supervised\nlearning on graph data. Advances in Neural Information Processing Systems,\n33:12827\u201312836, 2020.\n[48] W. Zhou, F. Liu, and M. Chen. Contrastive out-of-distribution detection for\npretrained transformers. In Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing (EMNLP), 2021.\n[49] Z. Zhou, L.-Z. Guo, Z. Cheng, Y.-F. Li, and S. Pu. Step: Out-of-distribution\ndetection in the presence of limited in-distribution labeled data. Advances in\nNeural Information Processing Systems, 34:29168\u201329180, 2021.\nA\nNOTATIONS\nHere we list the key notations in our paper in Table 3.\nTable 3: Summary of notations.\nNotation\nDescription\nG, A, X, V Graph, adjacency matrix, attribute matrix, node set.\nB,\ud835\udc4f\nThe total budget, query batch size.\nV\ud835\udc41\n\ud835\udc3f\nThe set of nodes labeled for classification.\nV\ud835\udc61\n\ud835\udc3f\nThe set of labeled nodes at the \ud835\udc61-th iteration.\nV\ud835\udc61\n\ud835\udc48\nThe set of unlabeled nodes at the \ud835\udc61-th iteration.\nS\ud835\udc61\nThe set of selected nodes at the \ud835\udc61-th iteration.\ne\nThe entropy of predictions from node classifier.\np\nThe anomaly score from anomaly score predictor.\ns\nThe overall anomaly score.\nh\ud835\udc61\n\ud835\udc56\nThe latent embedding of \ud835\udc63\ud835\udc56at the \ud835\udc61-th iteration.\n\u02c6h\ud835\udc61\n\ud835\udc56\nThe distance feature of \ud835\udc63\ud835\udc56at the \ud835\udc61-th iteration.\n\ud835\udc51\ud835\udc56\ud835\udc60\ud835\udc61(\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) The masked distance between \ud835\udc63\ud835\udc56and \ud835\udc63\ud835\udc57.\nc\ud835\udc41, c\ud835\udc34\nThe confidence of node classifier and anomaly score\npredictor in identifying anomalies.\nd\nThe confidence difference.\n\ud835\udc3c\ud835\udc5b\ud835\udc53\ud835\udc5c\nThe informativeness score.\n\ud835\udf0f\nThe decaying parameter in informativeness score.\n\ud835\udf19\nThe weight term in overall anomaly score.\n\ud835\udefc, \ud835\udefd\nWeight of classification and anomaly detection loss.\n\ud835\udc5a\nThe number of clusters.\nB\nDETAILS OF EXPERIMENTAL SETTINGS\nB.1\nDatasets\nWe employ four datasets to evaluate the performance of MITIGATE.\nCora and Citeseer [31] are two citation networks, in which nodes\nrepresent papers while edges represent citation relationships among\npapers. The class labels denote corresponding academic fields. Blog-\nCatalog and Flickr [37] are two social networks, in which nodes\nrepresent users while edges represent social relationships between\nusers. The class labels denote the interests of users. The dataset\nstatistics are shown in Table 4.\nFollowing the previous studies [8, 20, 22, 24], we inject contex-\ntual and structural anomalies into four widely used datasets: Cora,\nCiteSeer, Flickr, and BlogCatalog. (1) Structural anomalies. The\nidea behind synthetic structural anomalies is that anomalies can\nmanifest as densely connected nodes within small cliques [33]. For\nevery clique, \ud835\udc5dnodes are randomly selected and interconnected\ncompletely. Repeating this step \ud835\udc5etimes to generate \ud835\udc5ecliques, ulti-\nmately injecting \ud835\udc5d\u00d7\ud835\udc5estructural anomalies. In our experiments, we\nfix \ud835\udc5d= 15 and set \ud835\udc5eto 10, 15, 5, 5 for BlogCatalog, Flickr, Cora, and\nCiteSeer, respectively. (2) Contextual anomalies. The contextual\nanomalies are injected by perturbing the attributes of nodes. Ini-\ntially, a node \ud835\udc63\ud835\udc56is randomly selected from the vertex set \ud835\udc49. Then,\n\ud835\udc58additional nodes are sampled from \ud835\udc49\\ \ud835\udc63\ud835\udc56, forming a subset \ud835\udc49\ud835\udc50.\nFor each node \ud835\udc63\ud835\udc50\u2208\ud835\udc49\ud835\udc50, we measure the Euclidean distance to \ud835\udc63\ud835\udc56\nand subsequently adjust the attributes of \ud835\udc63\ud835\udc56to be the same as \ud835\udc63\ud835\udc50\nwith the largest distance. In our experiments, we set \ud835\udc58= 50 and\nthe number of contextual anomalies as \ud835\udc5d\u00d7 \ud835\udc5e, following previous\nstudies to ensure an adequately large disturbance magnitude and\nto balance the proportions of different anomaly types [24].\nTable 4: Dataset Analysis\nDataset\n#Nodes\n#Edges\n#Attributes\n#Class\n#Anomalies\nCora\n2,708\n5,429\n1,433\n7\n150\nCiteSeer\n3,327\n4,732\n3,703\n6\n150\nBlogCatalog\n5,196\n171,743\n8,189\n6\n300\nFlickr\n7,575\n239,738\n12,047\n9\n450\nB.2\nBaselines\nIn our experiments, the compared methods include (1) OOD de-\ntection methods (GCN-ENT, GKDE, OODGAT-ENT and OODGAT-\nATT), (2) semi-supervised anomaly detection methods (FdGars,\nGeniePath, BWGNN and DAGAD), (3) active query strategy for\nanomaly detection (most positive query, positive diverse query, and\ndiverse query). The details of the baselines are as follows:\n\u2022 GCN-ENT [17] is a vanilla GCN method that identifies anoma-\nlous nodes by uncertainty measured based on the entropy of\nnode classification.\n\u2022 GKDE [47] is a Graph-based Kernel Dirichlet GCN method for\nsemi-supervised node classification and OOD detection.\n\u2022 OODGAT [34] is a graph learning method with OOD nodes\nto distinguish inliers from outliers during feature propagation.\nIn particular, OODGAT-ENT uses the entropy of the predicted\ndistribution as an indicator of outliers, while OODGAT-ATT\nrelies on the score provided by a binary classifier.\n\u2022 FdGars [39] is an anomaly detection method based on GCN\nthat expresses the characteristics of reviewers and relationships\nbetween reviewers.\n\u2022 GeniePath [25] proposes to adaptively select receptive paths for\ndifferent nodes in aggregation for anomaly detection.\n\u2022 BWGNN [36] analyzes graph anomalies in the spectral domain\nand leverages Beta graph wavelet to better capture anomaly\ninformation on graphs.\n\u2022 DAGAD [20] devises an augmentation module that fertilizes the\ntraining set with generated samples to alleviate the scarcity of\nanomalous samples.\nMultitask Active Learning for Graph Anomaly Detection\nConference\u201917, July 2017, Washington, DC, USA\n\u2022 Most positive query selects the top-k samples ordered by their\nanomaly scores.\n\u2022 Positive diverse query combines anomaly scores with distance-\nbased diversification into querying.\n\u2022 Diverse query [18] utilizes k-means++ to initialize diverse clus-\nters. The probability of another query from the unlabeled set is\nproportional to its distance to the closest sample already in the\nquery set.\nWe implement GKDE 1, OODGAT 2, BWGNN 3 and DAGAD 4\nusing the code published by their authors. For FdGars and Ge-\nniePath, we use the code provided by an open-source library 5 for\ngraph anomaly detection. Regarding other active query strategies,\nwe adopt the code 6 provided by [18], and use the same network\nstructure as the encoder and anomaly score predictor in MITIGATE.\nB.3\nImplementation Notes\nWe conduct all experiments on a Linux server with 64G RAM,\nand 2 NVIDIA GeForce RTX 2080TI with 11GB GPU memory. We\nimplement MITIGATE with Python 3.8.1 and PyTorch 2.0.1. We\nreport our hyperparameter settings that are tuned on the validation\nset with 5 runs in Table 5.\nTable 5: Hyperparameter settings of MITIGATE.\nCora\nCiteseer\nBlogCatalog\nFlickr\n\ud835\udf0f\n0.95\n0.90\n0.98\n0.98\n\ud835\udefc\n1.25\n0.50\n1.25\n1.25\n\ud835\udefd\n0.50\n2.00\n1.00\n0.50\n\ud835\udf19\n1.25\n2.00\n1.00\n0.50\n\ud835\udc5a\n24\n24\n18\n27\n1https://github.com/zxj32/uncertainty-GNN\n2https://github.com/SongYYYY/KDD22-OODGAT\n3https://github.com/squareRoot3/Rethinking-Anomaly-Detection\n4https://github.com/FanzhenLiu/DAGAD\n5https://github.com/safe-graph/DGFraud\n6https://github.com/aodongli/Active-SOEL\n",
    "2405.17525": "arXiv:2405.17525v1  [cs.LG]  27 May 2024\nSmoothGNN: Smoothing-based GNN for\nUnsupervised Node Anomaly Detection\nXiangyu Dong\nThe Chinese University of Hong Kong\nxydong@se.cuhk.edu.hk\nXingyi Zhang\nThe Chinese University of Hong Kong\nxyzhang@se.cuhk.edu.hk\nYanni Sun\nCity University of Hong Kong\nyannisun@cityu.edu.hk\nLei Chen\nHuawei Noah\u2019s Ark Lab\nlc.leichen@huawei.com\nMingxuan Yuan\nHuawei Noah\u2019s Ark Lab\nyuan.mingxuan@huawei.com\nSibo Wang\nThe Chinese University of Hong Kong\nswang@se.cuhk.edu.hk\nAbstract\nThe smoothing issue leads to indistinguishable node representations, which poses\na signi\ufb01cant challenge in the \ufb01eld of graph learning. However, this issue also\npresents an opportunity to reveal underlying properties behind different types of\nnodes, which have been overlooked in previous studies. Through empirical and\ntheoretical analysis of real-world node anomaly detection (NAD) datasets, we ob-\nserve that anomalous and normal nodes show different patterns in the smooth-\ning process, which can be leveraged to enhance NAD tasks. Motivated by these\n\ufb01ndings, in this paper, we propose a novel unsupervised NAD framework. Speci\ufb01-\ncally, according to our theoretical analysis, we design a Smoothing Learning Com-\nponent. Subsequently, we introduce a Smoothing-aware Spectral Graph Neural\nNetwork, which establishes the connection between the spectral space of graphs\nand the smoothing process. Additionally, we demonstrate that the Dirichlet En-\nergy, which re\ufb02ects the smoothness of a graph, can serve as coef\ufb01cients for node\nrepresentations across different dimensions of the spectral space. Building upon\nthese observations and analyses, we devise a novel anomaly measure for the NAD\ntask. Extensive experiments on 9 real-world datasets show that SmoothGNN out-\nperforms the best rival by an average of 14.66% in AUC and 7.28% in Precision,\nwith 75x running time speed-up, which validates the effectiveness and ef\ufb01ciency\nof our framework.\n1\nIntroduction\nNode anomaly detection (NAD) focuses on identifying nodes in a graph that exhibit anomalous pat-\nterns compared to the majority of nodes (Akoglu et al., 2015; Ma et al., 2023). In recent years, the\ndevelopment of the Internet has led to the ubiquity of graph data. Consequently, NAD has attracted\nmuch attention from the deep learning community due to its crucial role in various real-world ap-\nplications, such as fraud detection in \ufb01nancial networks (Huang et al., 2022), malicious reviews\ndetection in social networks (McAuley & Leskovec, 2013), hotspot detection in chip manufacturing\n(Sun et al., 2022).\nPreprint. Under review.\nUnlike texts and images, graph data provides a \ufb02exible and intuitive way to model and analyze\ncomplex relationships and dependencies in various domains. However, the rich information con-\ntained within a single graph also presents challenges in effectively and ef\ufb01ciently detecting anoma-\nlous nodes from normal ones, especially in unsupervised settings (Dou et al., 2020; Liu et al., 2021;\nGao et al., 2023). To address this challenge, Graph Neural Networks (GNNs) have been widely em-\nployed for unsupervised NAD. Several techniques have been adopted in the model design, like graph\nreconstruction (Kim et al., 2023; Roy et al., 2024), reinforcement learning (Bei et al., 2023), and\nself-supervised learning (Duan et al., 2023c,a; Pan et al., 2023; Duan et al., 2023b; Qiao & Pang,\n2023). Speci\ufb01cally, graph reconstruction methods aim to learn node representations by minimizing\nerrors in reconstructing both node features and graph structures. Self-supervised learning methods\nemploy auxiliary tasks, such as link prediction, to guide NAD tasks. Besides, reinforcement learn-\ning methods leverage reward mechanisms to assist GNNs in learning effective node presentations.\nIn addition to these approaches, researchers have recognized that smoothing issues can hinder the\nperformance of these methods on NAD tasks (Gao et al., 2023). Consequently, efforts have been\nmade to design models that are more tolerant of these issues by incorporating techniques like data\naugmentation and edge drop (Qiao & Pang, 2023; Gao et al., 2023).\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n(a) T-Finance\n(b) YelpChi\n(c) Questions\nFigure 1: Normalized Distances from converged status on different datasets.\nHowever, previous studies have overlooked the underlying properties associated with the smoothing\nprocess, which can serve as effective indicators for distinguishing anomalous nodes from normal\nnodes. In our investigation, we conduct a comprehensive analysis of the distances between node\nrepresentations at each propagation hop and the converged representations obtained after an in\ufb01nite\nnumber of hops. We calculate the average normalized distances for both anomalous and normal\nnodes. Notably, these distances exhibit distinct patterns across different types of nodes in real-world\ndatasets, such as T-Finance, YelpChi, and Questions, as shown in Figure 1. Similar observations\non other datasets can be found in Appendix A.7. To gain a deeper understanding of the underlying\nreasons for this phenomenon, we conduct theoretical analysis, revealing that the smoothing patterns\nare closely related to anomalous properties of nodes, originating from the spectral space of the graph.\nMoreover, given the strong relationship between the smoothing issues and the spectral space, we\nfurther explore the spectral energy of the graph (Dong et al., 2024; Tang et al., 2022). Our \ufb01ndings\nshow that the Dirichlet Energy (Zhou et al., 2021), which represents the smoothness of a graph, plays\na similar role as the spectral energy, which can be utilized as coef\ufb01cients of node representations.\nMotivated by the experimental and theoretical results, we introduce SmoothGNN, a novel graph\nlearning framework for unsupervised NAD tasks. To be speci\ufb01c, SmoothGNN consists of four\ncomponents: the Smoothing Learning Component (SLC), the Smoothing-aware Spectral GNN (SS-\nGNN), the Smoothing-aware Coef\ufb01cients (SC), and the Smoothing-aware Measure (SMeasure) for\nNAD. The SLC serves as a feature augmentation module, explicitly capturing the different patterns\nof anomalous and normal nodes (observation in Figure 1). SSGNN is designed to learn node rep-\nresentations from the spectral space of the graph while effectively capturing distinct smoothing pat-\nterns. Furthermore, we provide a theoretical analysis on the Dirichlet Energy, which can extract both\nsmoothness and spectral energy information. Based on this analysis, we design the SC accordingly.\nDuring the training phase, we propose a loss function that incorporates both feature reconstruction\nand the SMeasure to optimize the model. Finally, we utilize the SMeasure as the anomaly score for\neach node during the inference phase. In contrast to previous work that primarily evaluated on small\nor synthetic datasets, we conduct experiments on large-scale real datasets commonly encountered in\npractical applications. In summary, the main contributions of our work are as follows:\n\u2022 To the best of our knowledge, we are the \ufb01rst to reveal that the smoothing issues can bene\ufb01t NAD\ntasks, from both experimental and theoretical perspectives. Building on this insight, we introduce\na novel SMeasure as the unsupervised anomaly measurement.\n2\n\u2022 We propose SmoothGNN, a novel framework that captures information from both the smoothing\nprocess and spectral space of graphs, which can serve as a powerful backbone for NAD tasks.\n\u2022 We are the only work that conducts experiments on large real-world datasets for unsupervised\nNAD. Extensive experiments demonstrate the effectiveness and ef\ufb01ciency of our framework.\nCompared to state-of-the-art models, SmoothGNN achieves superior performance in terms of\nAUC and Precision metrics, with at least one order of magnitude speed-up.\n2\nRelated Work\nIn recent years, unsupervised NAD has gained increasing interest within the graph learning com-\nmunity. Researchers have proposed a variety of models that can be broadly categorized into four\ngroups: shallow models, reconstruction models, self-supervised models, and other models. Next,\nwe brie\ufb02y introduce several representative frameworks from these categories.\nShallow Models. Prior to the emergence of deep learning models, previous work mainly focuses\non shallow models for the NAD task. For instance, Radar (Li et al., 2017) utilizes the residuals\nof attribute information and their coherence with network information to identify anomalous nodes.\nANOMALOUS (Peng et al., 2018) introduces a joint framework for NAD based on residual analysis.\nThese models primarily leverage matrix decomposition and residual analysis, which inherently have\nlimited capabilities in capturing the complex information underlying graph data.\nReconstruction Models. Reconstruction models are prevalent approaches in the \ufb01eld of unsuper-\nvised NAD, as the reconstruction errors of graph structures and node features inherently re\ufb02ect the\nlikelihood of a node being anomalous. Motivated by this, a prior work CLAD (Kim et al., 2023), pro-\nposes a label-aware reconstruction approach that utilizes Jensen-Shannon Divergence and Euclidean\nDistance. Besides, graph auto-encoders (GAEs) are widely adopted as reconstruction techniques.\nFor example, GADNR (Roy et al., 2024) incorporates GAEs to reconstruct the neighborhood of\nnodes. Although previous studies have shown the effectiveness of graph reconstruction, it is worth\nnoting that reconstructing graph structures can be computationally expensive. Moreover, experimen-\ntal \ufb01ndings from GADbench (Tang et al., 2023) indicate that node feature reconstruction yields the\nmost signi\ufb01cant bene\ufb01ts for NAD. Therefore, a preferable choice is to focus exclusively on feature\nreconstruction, as introduced in the SmoothGNN framework.\nSelf-supervised Models. Aside from the reconstruction models, self-supervised models, particu-\nlarly contrastive learning frameworks, are also popular choices in unsupervised NAD. For instance,\nNLGAD (Duan et al., 2023c) constructs multi-scale contrastive learning networks to estimate the\nnormality for nodes. Similarly, GRADATE (Duan et al., 2023a) presents a multi-scale contrastive\nlearning framework with subgraph-subgraph contrast to capture the local properties of nodes. Other\nexamples include PREM (Pan et al., 2023) and ARISE (Duan et al., 2023b), which employ node-\nsubgraph contrast and node-node contrast to learn node representations, re\ufb02ecting both local and\nglobal views of the graph. A recent study, TAM (Qiao & Pang, 2023), leverages data augmentation\nto generate multi-view graphs, enabling the examination of the consistency of the local feature infor-\nmation within node neighborhoods. Based on the observation of local node af\ufb01nity, TAM introduces\na local af\ufb01nity score to measure the probability of a node being anomalous, highlighting the impor-\ntance of designing new measures for NAD. In contrast, SmoothGNN introduces the SMeasure to\ndetect anomalous nodes, which utilizes a more \ufb02exible way to capture the anomalous properties of\nthe nodes. Moreover, the SMeasure requires fewer computational resources, enabling SmoothGNN\nto be applied to large-scale datasets.\nOther Models. In addition to the above-mentioned models, there are other notable models such as\nRAND (Bei et al., 2023) and VGOD (Huang et al., 2023). RAND is the \ufb01rst work, to the best of our\nknowledge, that leverages reinforcement learning in the unsupervised NAD task. It introduces an\nanomaly-aware aggregator to amplify messages from reliable neighbors. On the other hand, VGOD\npresents a mixed-type framework that combines graph reconstruction and self-supervised models. It\nincorporates a variance-based module to sample positive and negative pairs for contrastive learning,\nalong with an attribute reconstruction module to reconstruct node features. In contrast to these\nworks, our SmoothGNN framework adopts a different strategy by utilizing feature reconstruction to\nassist in learning the SMeasure score, which serves as part of the objective function. This approach\nachieves superior performance while requiring signi\ufb01cantly less running time.\n3\n3\nMethod: SmoothGNN\nOur observation in Section 1 highlights the different smoothing patterns exhibited by anomalous and\nnormal nodes. In Section 3.1, we \ufb01rst present the background knowledge for the following analysis.\nSubsequently, Section 3.2 provides a comprehensive theoretical analysis of smoothing patterns. This\nanalysis serves as the motivation behind the design of two key components, SLC and SSGNN in\nSmoothGNN framework, to be elaborated in Sections 3.3 and 3.4, respectively. Moreover, our\ntheoretical analysis in Section 3.2 reveals that the spectral energy of the graph can be represented\nby the Dirichlet Energy, which inspires us to employ the Dirichlet Energy as effective coef\ufb01cients\nfor node representations, to be detailed in Section 3.5. Finally, Section 3.6 elaborates on the overall\nobjective function, including a feature reconstruction loss and the proposed novel Smeasure.\n3.1\nPreliminaries\nNotation. Let G = (A, X) denote a connected undirected graph with n nodes and m edges, where\nX \u2208Rn\u00d7F is node features and A \u2208Rn\u00d7n is the adjacency matrix. Let Aij = 1 if there exists\nan edge between node i and j, otherwise Aij = 0. D denotes the degree matrix. The adjacency\nmatrix \u02dc\nA and degree matrix \u02dc\nD of graph G with self-loop can be de\ufb01ned as \u02dc\nA = A + In and\n\u02dc\nD = D + In, respectively, where In \u2208Rn\u00d7n is an identity matrix. The Laplacian matrix L\nis then de\ufb01ned as L = In \u2212\u02dc\nD\u22121\n2 \u02dc\nA \u02dc\nD\u22121\n2 . It can also be decomposed by L = U\u039bUT , where\nU = (u1, u2, ..., un) represents orthonormal eigenvectors and the corresponding eigenvalues are\nsorted in ascending order, i.e. \u03bb1 \u2264... \u2264\u03bbn. Let x = (x1, x2, ..., xn)T \u2208Rn be a signal on graph\nG, the graph convolution operation between a signal x and a graph \ufb01lter g\u03b8(\u00b7) is then de\ufb01ned as\ng\u03b8(L) \u2217x = Ug\u03b8(\u039b)UT x, where the parameter \u03b8 \u2208Rn is spectral \ufb01lter coef\ufb01cient vector.\nSpectral GNN. Graph convolution operations (Defferrard et al., 2016; Kipf & Welling, 2017) can\nbe approximated by the T -th order polynomial of Laplacians:\nUg\u03b8(\u039b)UT x \u2248U(\nT\nX\nt=0\n\u03b8t\u039bt)UT x = (\nT\nX\nt=0\n\u03b8tLt)x,\nwhere \u03b8 \u2208RT +1 corresponds to polynomial coef\ufb01cients. In the following Section 3.2, the prevalent\ngraph convolution operation is demonstrated to have a strong relation with graph smoothing patterns.\nThis key insight motivates the design of SmoothGNN, which can capture information from graph\nspectral space and anomalous properties behind smoothing patterns.\nDirichlet Energy. Smoothing issues can be a challenging problem for GNNs. It is shown to be\nclosely related to the Dirichlet energy of the graph (Zhang et al., 2021), which can be de\ufb01ned as\nE(x) =\nn\nX\ni,j=1\nai,j||\nxi\n\u221adi + 1 \u2212\nxj\np\ndj + 1||2\n2,\nwhere ai,j represents the (i, j)-th entry of the adjacency matrix \u02dc\nA, and di is the degree of node\ni. Dirichlet Energy serves as a measure of the smoothness of node features within neighborhoods.\nGiven its intrinsic connection to the smoothing patterns of nodes, we delve deeper into this relation-\nship in Section 3.2. Our analysis reveals that the Dirichlet Energy exhibits a strong correlation with\nthe spectral space and can be used as the coef\ufb01cients for node representations.\n3.2\nTheoretical Analysis: Connection among Smoothing Patterns, Spectral Space, and NAD\nAs discussed in Section 1, Smoothing issues have been extensively studied in the graph learning area.\nHowever, previous studies primarily focus on highlighting its negative aspects. This motivates us to\nexplore the potential positive implications of smoothing issues. To this end, we conduct a detailed\nanalysis to demonstrate how smoothing patterns can reveal anomalous properties of nodes as shown\nbelow. All the proofs of our theorems can be found in Appendix A.1.\nBased on previous research (Qiao & Pang, 2023; Gao et al., 2023), both local views, such as neigh-\nboring nodes and features, and global views, which encompass statistical information of entire\ngraphs, contribute to the detection of node anomalies. The following theorem indicates that smooth-\ning patterns can be represented by an augmented propagation matrix, which is aware of both local\nand global information. Thus, the smoothing patterns can be an effective identi\ufb01er for NAD tasks.\n4\nTheorem 1. Let P = In\n2 +\n\u02dc\nA\n2 denote the propagation matrix given the adjacency matrix \u02dc\nA. For an\naugmented propagation matrix Bt = (P \u2212P \u221e)t, where P \u221erepresents the converged status of P ,\nwe can derive Bt = P t \u2212P \u221ewith (i, j)-th entry Bi,j =\n(2m+n)(I[i=j]\u221adi+1+2aij)\u22122(di+1)\u221a\ndj+1\n2\u221adi+1(2m+n)\n,\nwhere I[\u00b7] is the indicator function, ai,j is the (i, j)-th entry of the adjacency matrix, di is the degree\nof node i, and m, n represent the number of edges and nodes, respectively.\nTheorem 1 shows that, when the graph signal x propagates on the augmented propagation matrix\nB, the resulting node presentation becomes aware of local structures and individual node features.\nIn addition, since this augmented propagation matrix not only propagates graph signal through edge\nconnection but also assigns the signal a transformation of statistical information of graph as coef-\n\ufb01cients, serving as the attention mechanism, it highlights the disparities arising from global views.\nThus, the augmented propagation offers a more precise indication of the underlying properties of\nanomalous and normal nodes compared to the original propagation. This observation is also sup-\nported by the empirical evidence of different smoothing patterns shown in Section 1. Leveraging the\ncomprehensive information contained by the augmented matrix, we employ it in the design of the\nSLC in Section 3.3 and the SMeasure in Section 3.6.\nMoreover, previous studies (Tang et al., 2022; Dong et al., 2024) have demonstrated a strong connec-\ntion between graph anomalies and graph spectral space. Building upon Theorem 1, which establishes\nthe close relationship of the augmented propagation matrix with graph anomalies, we investigate the\nconnection between the augmented propagation matrix and the graph spectral space. The following\ntheorem demonstrates that the column vectors of the augmented propagation matrix can be repre-\nsented by a polynomial combination of graph convolution operations, further indicating the strong\ncorrelation between the augmented propagation matrix and the graph spectral space.\nTheorem 2. The augmented propagation matrix B after t hops of propagation can be expressed by\nbt = Pt\nk=0 \u02dc\u03b8kLkuv, where bt is a column vector of Bt, \u02dc\u03b8k \u2208Rn is the spectral \ufb01lter coef\ufb01cients,\nand u, v represent the linear combinations of the eigenvectors of \u02dc\nA and L, respectively.\nTheorem 2 illustrates that the augmented propagation matrix can be represented by the spectral \ufb01l-\nter (introduced in Section 3.1). This motivates us to design SSGNN, a spectral GNN that not only\nleverages information from the spectral space but also captures smoothing patterns, to be elaborated\nin Section 3.4. Besides, RQGNN (Dong et al., 2024) has shown that the coef\ufb01cients of node repre-\nsentations can be guided by spectral energy. Given our \ufb01ndings that reveal the strong connection\nbetween smoothing patterns and spectral space, we aim to explore new coef\ufb01cients that can effec-\ntively capture both smoothing patterns and spectral energy. Firstly, we provide the de\ufb01nition of\nspectral energy following previous studies (Tang et al., 2022; Dong et al., 2024).\nDe\ufb01nition 1 ((Tang et al., 2022; Dong et al., 2024)). Given the graph Laplacian matrix L =\nU\u039bUT and a graph signal x, the graph Fouier Transformation of x is de\ufb01ned as \u02c6x\n=\n{ \u02c6x1, \u00b7 \u00b7 \u00b7 , \u02c6\nxn} = UT x. The spectral energy of the graph at \u03bbk can be expressed as\n\u02c6x2\nk\nPn\ni=1 \u02c6x2\ni .\nBased on De\ufb01nition 1, we have the following theorem, which shows that Dirichlet Energy can serve\na similar role as spectral energy.\nTheorem 3. Given a graph G with Laplacian matrix L and a graph signal x, the Dirichlet Energy\ncan be represented by E(x) =\nPn\nj=1 \u03bbj \u02c6x2\nj\nPn\ni=1 \u02c6x2\ni , where the \u03bbj is the j-th eigenvalue of L.\nRecap from Section 3.1 that Dirichlet Energy characterizes the smoothness of node representations\nwithin their neighborhoods, which is complementary to the previous individual view depicted in\nFigure 1. Theorem 3 shows that the smoothing patterns of nodes can also be effectively represented\nusing a polynomial combination of the spectral energy. Building on this \ufb01nding, we introduce\nSmoothness-aware Coef\ufb01cients (SC) as the coef\ufb01cients of node representations, which will be dis-\ncussed in Section 3.5.\nBased on the aforementioned theorems, we propose different components of our SmoothGNN. To\nachieve a balance between effectiveness and ef\ufb01ciency, we further analyze the maximum propa-\ngation hops of SmoothGNN that do not provide additional information. Speci\ufb01cally, if the node\nrepresentations have already reached a converged state, additional layers of SmoothGNN will not\nprovide substantial bene\ufb01ts but will consume additional computational sources. Therefore, we de-\n5\ntermine the layers of SmoothGNN based on Theorem 4. To achieve this, we \ufb01rst de\ufb01ne \u01eb-smoothing\n(Rong et al., 2020) and then illustrate the theorem of the converged hop.\nDe\ufb01nition 2 ((Rong et al., 2020)). For any GNN, we call it suffers from \u01eb-smoothing if and only\nif after T hops of propagation, the resulting feature matrix Ht at hop t \u2265T has a distance no\nlarger than \u01eb with respect to a subspace S, namely, dS(Ht) \u2264\u01eb, \u2200t \u2265T , where dS(Ht) :=\nminM\u2208S||Ht \u2212M||F represents the Frobenius norm from Ht to the subspace S.\nTheorem 4. Given the subspace S with threshold \u01eb, a GNN model will suffer from \u01eb-smoothing issue\nwhen the propagation hop t = \u2308log(\u01eb/dS(X))\nlog(\u03c4\u03bb)\n\u2309, where \u03c4 is the largest singular value of the graph\n\ufb01lters over all layers, \u03bb is the second largest eigenvalue of the propagation matrix, and X is the\nfeature matrix of graph G.\nTheorem 4 provides a theoretical guarantee regarding the maximum propagation hops that can con-\ntribute to the learning process. In Section 4 and Appendix A.5, we further observe that the required\npropagation hop is directly proportional to the number of nodes in the graph. In the following\nsections, we elaborate on our SmoothGNN framework in detail.\n3.3\nSmoothing Learning Component\nMotivated by Theorem 1 and the observation in Section 1, we propose a simple yet powerful com-\nponent to capture the smoothing patterns of nodes. Speci\ufb01cally, we \ufb01rst calculate the augmented\npropagation matrix Bt = P t \u2212P \u221efor t = 0, \u00b7 \u00b7 \u00b7 , T . Next, we employ a set of (T + 1) MLPs to\nobtain the latent node representations propagated on each Bt. Finally, an additional MLP is adopted\nto fuse the node representations obtained from different propagation hops. Let \u02dc\nXt denote the node\nfeatures X after the t-th feature transformation, the representation of the i-th node in SLC can be\nexpressed as:\nhSLC\ni\n= MLP(CONCAT((B0 \u02dc\nX0)i, \u00b7 \u00b7 \u00b7 , (BT \u02dc\nXT ))i).\n(1)\nIn addition to explicitly learning from smoothing patterns, it is crucial for the model to implicitly\ncapture information from the graph topology and node features. The combination of explicit and\nimplicit learning enables the collection of comprehensive information required for NAD tasks. The\ndetails of implicit learning are presented as follows.\n3.4\nSmoothing-aware Spectral GNN\nAs stated in Theorem 2, the augmented propagation matrix after t hops of propagation can be rep-\nresented as bt = PT\nt=0 \u02dc\u03b8tLtuv, demonstrating the capability of the graph spectral space to reveal\nunderlying node properties for NAD. This motivates our design of a spectral GNN to learn node\nrepresentations. Considering (T + 1) augmented propagation matrices, it is essential to design a\nspectral GNN that captures information from all (T +1) hops of propagation. Therefore, employing\na polynomial combination of graph spectral \ufb01lters as the graph convolution operation \ufb01lter can be\na natural choice. To maintain the simplicity of our framework, we leverage T -th order polynomial\nof graph Laplacian as the backbone \ufb01lter. Speci\ufb01cally, let g(L, X)T denote the graph convolution\noperation, we have:\ng(L, X)T = (\nT\nX\nt=0\n\u03b8tLt)X.\n(2)\nSimilar to SLC, we consider \u02dc\nXt as node features after t-th feature transformation for each graph\nconvolution operation. Subsequently, we employ an MLP to fuse the spectral node representations\nobtained from each propagation hop to generate the \ufb01nal node representations. The representation\nof the i-th node can be expressed as follows:\nhGNN\ni\n= MLP(CONCAT((g(L, \u02dc\nX0)0)i, \u00b7 \u00b7 \u00b7 , (g(L, \u02dc\nXT )T )i).\n(3)\nNote that we utilize shared weights in SLC and SSGNN. By incorporating these two components,\nour framework can capture information from both spectral space and smoothing patterns. In addition,\nas discussed in 3.2, the coef\ufb01cients of the \ufb01nal representations guided by spectral energy enable the\nframework to effectively represent different dimensions of spectral space. To achieve this, we further\nintroduce SC in Section 3.5.\n6\n3.5\nSmoothing-aware Coef\ufb01cients\nTheorem 3 shows that the Dirichlet Energy can be interpreted as a polynomial combination of spec-\ntral energy. Motivated by (Dong et al., 2024), we design SC for node representations. Speci\ufb01cally,\nwe \ufb01rst calculate the Dirichlet Energy for the graph, which can be expressed as:\nSC(X, L) = diag\n\u0012XT LX\nXT X\n\u0013\n,\n(4)\nwhere diag(\u00b7) denotes the diagonal entries of a square matrix. Subsequently, we apply the Sigmoid\nfunction \u03c3(\u00b7) to rescale the coef\ufb01cients, i.e., \u03b1 = \u03c3(SC(X, L)). Finally, we utilize element-wise\nmultiplication \u2217to rearrange the coef\ufb01cients of representations hSLC\ni\nand hGNN\ni\nfor i = 1, \u00b7 \u00b7 \u00b7 , n.\nhSCSLC\ni\n= hSLC\ni\n\u2217\u03b1, hSCGNN\ni\n= hGNN\ni\n\u2217\u03b1.\n(5)\nThe \ufb01nal representations generated by SLC and SSGNN in SmoothGNN is utilized to calculate the\nloss function and the SMeasure.\n3.6\nSmoothing-aware Measure\nAccording to previous work (Huang et al., 2023), feature reconstruction loss can assist in learning\neffective measures for NAD. This inspires us to design a loss function combined with two compo-\nnents: the feature reconstruction loss and the SMeasure. For the feature reconstruction loss, we use\nhSCGNN\ni\nto reconstruct the original feature.\nLcon = 1\nn\nn\nX\ni=1\n||hSCGNN\ni\n\u2212xi||2,\n(6)\nwhere xi is the i-th row of the original feature matrix X. For the SMeasure, we leverage the rep-\nresentations obtained from SLC as it naturally captures the underlying differences in the smoothing\npatterns. To be speci\ufb01c, we de\ufb01ne the SMeasure as follows:\nfsmooth(hSCSLC\ni\n) = \u03c3(AVG(hSCSLC\ni\n)),\n(7)\nwhere \u03c3(\u00b7) represents the Sigmoid function, and AVG(\u00b7) represents the column-wise average func-\ntion, which aims to aggregate the smoothing patterns across all dimensions. Based on the SMeasure,\nwe further de\ufb01ne the smoothing-based loss function:\nLsmooth = 1\nn\nn\nX\ni=1\nfsmooth(hSCSLC\ni\n).\n(8)\nThe \ufb01nal loss function is a combination of both Lcon and Lsmooth:\nL = Lcon + Lsmooth.\n(9)\nThe loss function is carefully designed to leverage the feature reconstruction loss to facilitate the\nlearning process of the SMeasure. This combination enables the loss function to capture valuable\ninformation from the smoothing patterns and the reconstruction errors across different nodes. Mini-\nmizing this loss function empowers the model to effectively reduce the presence of anomalous nodes\nin the predicted results, thereby addressing the challenge of extremely unbalanced data in NAD.\nWith this comprehensive loss function, our SmoothGNN can optimize the shared weights of two key\ncomponents. Consequently, our framework excels in learning more accurate node representations\nfor the task of detecting anomalous nodes.\n4\nExperiments\n4.1\nExperimental Setup\nDatasets. We evaluate SmoothGNN on 9 real-world datasets, including Reddit, Tolokers, Amazon,\nT-Finance, YelpChi, Questions, Elliptic, DGraph-Fin, and T-Social. These datasets are obtained\n7\nTable 1: Statics of 9 real-world datasets, including the number of nodes and edges, the node feature\ndimension, the average degree of nodes, and the ratio of anomalous labels.\nCategories\nDatasets\n#Nodes\n#Edges\n#Feature\nAvg. Degree\nAnomaly Ratio\nSmall\nReddit\n10,984\n168,016\n64\n15.30\n3.33%\nTolokers\n11,758\n519,000\n10\n44.14\n21.82%\nAmazon\n11,944\n4,398,392\n25\n368.25\n6.87%\nMedium\nT-Finance\n39,357\n21,222,543\n10\n539.23\n4.58%\nYelpChi\n45,954\n3,846,979\n32\n83.71\n14.53%\nQuestions\n48,921\n153,540\n301\n3.14\n2.98%\nLarge\nElliptic\n203,769\n234,355\n167\n1.15\n9.76%\nDGraph-Fin\n3,700,550\n4,300,999\n17\n1.16\n1.27%\nT-Social\n5,781,065\n73,105,508\n10\n12.65\n3.01%\nTable 2: AUC and Precision (%) on 9 datasets, where \"-\" represents failed experiments due to\nmemory constraint. The best result on each dataset is highlighted in boldface.\nShallow\nReconstruction\nSelf-supervised\nOther\nDatasets\nMetrics RADAR ANOMALOUS CLAD GADNR NLGAD GRADATE PREM ARISE TAM RAND VGOD SmoothGNN\nReddit\nAUC\n0.4372\n0.4481\n0.5784 0.5532\n0.5380\n0.5261\n0.5518 0.5273 0.5729 0.5417 0.4931\n0.5946\nPrecision 0.0273\n0.0309\n0.0502 0.0373\n0.0415\n0.0393\n0.0413 0.0402 0.0425 0.0356 0.0324\n0.0438\nTolokers\nAUC\n0.3625\n0.3706\n0.4061 0.5768\n0.4825\n0.5373\n0.5654 0.5514 0.4699 0.4377 0.4988\n0.6870\nPrecision 0.1713\n0.1731\n0.1921 0.2991\n0.2025\n0.2364\n0.2590 0.2505 0.1963 0.1939 0.2212\n0.3517\nAmazon\nAUC\n0.2318\n0.2318\n0.2026 0.2608\n0.5425\n0.4781\n0.2782 0.4782 0.8028 0.3585 0.5182\n0.8408\nPrecision 0.0439\n0.0439\n0.0401 0.0424\n0.0991\n0.0634\n0.0744 0.0677 0.3322 0.0492 0.0779\n0.3953\nT-Finance\nAUC\n0.2824\n0.2824\n0.1385 0.5798\n0.5231\n0.4063\n0.4484 0.4667 0.6901 0.4380 0.4814\n0.7556\nPrecision 0.0295\n0.0295\n0.0247 0.0542\n0.0726\n0.0376\n0.0391 0.0393 0.1284 0.0403 0.0454\n0.1408\nYelpChi\nAUC\n0.5261\n0.5272\n0.4755 0.4704\n0.4981\n0.4920\n0.4900 0.4834 0.5487 0.5052 0.4878\n0.5758\nPrecision 0.1822\n0.1700\n0.1284 0.1395\n0.1469\n0.1447\n0.1378 0.1415 0.1733 0.1470 0.1345\n0.1823\nQuestions\nAUC\n0.4963\n0.4965\n0.6207 0.5875\n0.5428\n0.5539\n0.6033 0.6241 0.5042 0.6164 0.5075\n0.6444\nPrecision 0.0279\n0.0279\n0.0512 0.0577\n0.0348\n0.0350\n0.0430 0.0619 0.0395 0.0442 0.0299\n0.0592\nElliptic\nAUC\n-\n-\n0.4192 0.4001\n0.4977\n-\n0.4978\n-\n-\n-\n0.5723\n0.5729\nPrecision\n-\n-\n0.0807 0.0778\n0.1009\n-\n0.0905\n-\n-\n-\n0.1256\n0.1161\nDGraph-Fin\nAUC\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.5456\n0.6499\nPrecision\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.0148\n0.0199\nT-social\nAUC\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.5999\n0.7034\nPrecision\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n0.0351\n0.0631\nfrom the benchmark paper (Tang et al., 2023), consisting of various types of networks and corre-\nsponding anomalous nodes. Based on their number of nodes, we divide these datasets into three\ncategories, Small, Medium, and Large, as shown in Table 1. Note that unlike previous work in the\nunsupervised NAD area, we only utilize real-world datasets with a suf\ufb01cient number of nodes. Fur-\nthermore, to the best of our knowledge, SmoothGNN is the only model in this \ufb01eld that conducts\nexperiments on large-scale datasets like T-Social to validate the ef\ufb01ciency and effectiveness of the\nmodel.\nBaselines. We compare RQGNN against 11 SOTA competitors, including shallow models, recon-\nstruction models, self-supervised models, and other models.\n\u2022 Shallow models: RADAR (Li et al., 2017), and ANOMALOUS (Peng et al., 2018).\n\u2022 Reconstruction models: CLAD (Kim et al., 2023) and GADNR (Roy et al., 2024).\n\u2022 Self-supervised models: NLGAD (Duan et al., 2023c), GRADATE (Duan et al., 2023a), PREM\n(Pan et al., 2023), ARISE (Duan et al., 2023b), and TAM (Qiao & Pang, 2023).\n\u2022 Other models: RAND (Bei et al., 2023) and VGOD (Huang et al., 2023).\nExperimental Settings. In line with the experimental settings of prior research (Liu et al., 2022;\nHuang et al., 2023; Qiao & Pang, 2023), we conduct transductive experiments on these datasets.\nThe parameters of SmoothGNN are set according to the categories of the datasets. The speci\ufb01c\nparameters for each category can be found in Appendix A.3. To ensure a fair comparison, we obtain\nthe source code of all competitors from GitHub and execute these models using the default parameter\nsettings suggested by their authors.\n4.2\nMain Results\nThe performance of SmoothGNN is evaluated against different SOTA models in the NAD \ufb01eld.\nTable 2 reports the AUC and Precision scores of each model across 9 datasets. The best result on\neach dataset is highlighted in boldface. Our key observations are as follows.\n8\nTable 3: Running time (s) on 9 datasets, where \"-\" represents failed experiments due to memory\nconstraint. The best result on each dataset is highlighted in boldface.\nShallow\nReconstruction\nSelf-Supervised\nOther\nDatasets\nRADAR ANOMALOUS CLAD GADNR\nNLGAD GRADATE PREM\nARISE\nTAM\nRAND\nVGOD\nSmoothGNN\nReddit\n55.57\n42.25\n11.14\n692.66\n10886.19\n7562.59\n73.52\n1261.99\n5050.89\n310.11\n39.86\n7.02\nTolokers\n57.51\n40.94\n52.91\n861.95\n10504.91\n7824.63\n74.80\n1281.71\n5668.91\n367.03\n177.42\n6.99\nAmazon\n42.79\n38.07\n431.20\n2048.72\n10649.83\n7856.41\n130.57 1267.38\n1148.24\n593.75\n1517.70\n7.19\nT-Finance\n500.19\n360.97\n2161.16 14255.00 35648.72\n30341.65\n266.33 4223.50 81238.60 6746.10 5998.08\n16.69\nYelpChi\n730.81\n513.83\n418.95\n5046.51\n42435.07\n35938.21\n308.68 5042.99 102232.07 6588.60 1283.10\n19.37\nQuestions\n1205.05\n1114.22\n52.65\n2795.99\n51270.03\n44235.87\n409.45 6135.88 11603.81 7364.07\n86.12\n32.68\nElliptic\n-\n-\n421.17 12568.50 193304.73\n-\n2149.77\n-\n-\n-\n231.63\n205.10\nDGraph-Fin\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n3420.84\n2924.99\nT-Social\n-\n-\n-\n-\n-\n-\n-\n-\n-\n-\n22984.10\n4877.05\nTable 4: AUC and Precision (%) on 9 datasets of SmoothGNN and SmoothGNN-A.\nDatasets\nReddit\nTolokers\nAmazon\nT-Finance\nYelpChi\nQuestions\nElliptic\nDGraph-Fin\nMetrics\nAUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision\nSmoothGNN\n0.5946\n0.0438\n0.6870\n0.3517\n0.8408\n0.3953\n0.7556\n0.1408\n0.5758\n0.1823\n0.6444\n0.0592\n0.5729\n0.1161\n0.6499\n0.0199\nSmoothGNN-A 0.5919\n0.0486\n0.6731\n0.3340\n0.8008\n0.2719\n0.7408\n0.1099\n0.5697\n0.1887\n0.6388\n0.0527\n0.5695\n0.1136\n0.5893\n0.0164\nFirstly, most existing unsupervised NAD models struggle to handle large datasets, with only VGOD\nand the proposed SmoothGNN successfully running on the two largest datasets. This highlights the\nneed for the development of unsupervised models capable of handling large-scale datasets.\nShallow models like Radar and ANOMALOUS apply residual analysis to solve NAD, which poses\nchallenges in capturing the underlying anomalous properties from a spectral perspective. In compar-\nison, SmoothGNN takes the lead by 29.37% and 29.03% in terms of AUC, and 11.52% and 11.63%\nin terms of Precision on average across 6 datasets, respectively. Moreover, these shallow models\nare unable to handle the 3 large datasets due to memory constraints. These results demonstrate that\nshallow models are both time-consuming and ineffective when applied to real-world NAD datasets.\nNext, we examine reconstruction approaches CLAD and GADNR, which utilize reconstruction tech-\nniques to detect graph anomalies. While these models leverage both structure and feature recon-\nstruction to calculate the anomaly score for each node, they fail to capture the information from\nthe spectral domain, leading to inferior performance. SmoothGNN outperforms these models by\n26.14% and 17.75% in terms of AUC, and 10.31% and 8.30% in terms of Precision on average\nacross different datasets, respectively.\nWe then compare SmoothGNN with self-supervised models NLGAD, GRADATE, PREM, ARISE,\nand TAM. Although self-supervised learning approaches can boost the performance of unsupervised\nframeworks, their high memory requirements and computational costs make them prohibitive for\nlarge datasets. While NLGAD and PREM utilize sparse techniques to address these issues, they still\ncannot run on the two largest datasets. In comparison, SmoothGNN achieves an improvement of\n14.95% and 17.66% in terms of AUC, and 8.44% and 8.63% in terms of Precision on average across\n7 datasets, respectively. Besides, SmoothGNN also outperforms other models like GRADATE and\nARISE by 18.41% and 16.12% in terms of AUC, and 10.28% and 9.53% in terms of Precision on\naverage across 7 datasets, respectively. In addition, TAM is the best rival in terms of performance,\nbut its high memory usage and running time make it unable to run on large datasets. Across 6\ndatasets, SmoothGNN surpasses TAM by 8.49% in terms of AUC and 4.35% in terms of Precision\non average.\nFinally, we examine the results of other approaches RAND and VGOD. RAND represents a novel\ndirection for unsupervised NAD tasks but fails to leverage more advanced graph properties, like\nsmoothing patterns, to guide the learning process. As a result, SmoothGNN outperforms RAND by\n20.01% in terms of AUC, and 11.05% in terms of Precision on average across 6 datasets. On the\nother hand, VGOD is the only competitor capable of running on all the datasets, demonstrating the\nbene\ufb01ts of combining feature reconstruction and novel measures. However, SmoothGNN leverages\nsimilar techniques more ef\ufb01ciently and effectively, surpassing VGOD by 14.66% in terms of AUC\nand 7.28% in terms of Precision on average across all datasets, with a 75x speed-up in running time.\n4.3\nAlternative Smoothing Patterns\nIn addition to the smoothing patterns observed in vanilla GNN, graph learning models like APPNP\n(Klicpera et al., 2019) also converge to a steady state. Speci\ufb01cally, the converged state of APPNP\n9\ncan be expressed as\nZ\u221e= \u03b1(In \u2212(1 \u2212\u03b1)A)\u22121X,\nwhere \u03b1 is the teleport probability. To investigate whether any smoothing pattern can be utilized\nfor detecting anomalous nodes, we modify the graph convolution operation in SSGNN with APPNP\nupdate rule Zt = (1\u2212\u03b1)AZt\u22121+\u03b1X, and replace Bt with Zt\u2212Z\u221e. The results of this modi\ufb01ed\nmodel, denoted as SmoothGNN-A, are shown in Table 4. We observe that by employing alternative\nsmoothing patterns, the framework can still effectively detect anomalous nodes, thus validating that\nsmoothing patterns serve as accurate identi\ufb01ers for NAD tasks. However, based on the comparison\nbetween SmoothGNN and SmoothGNN-A in Table 4, we \ufb01nd SmoothGNN can achieve relatively\nbetter performance in most datasets. These results demonstrate information from spectral space can\nalso be important in NAD tasks.\n5\nConclusion\nIn this paper, we introduce the smoothing patterns into the NAD task. We identify differences in\nthe smoothing patterns between anomalous and normal nodes, and further demonstrate the obser-\nvation through comprehensive experiments and theoretical analysis. The combination of four com-\nponents in SmoothGNN enables the model to capture information from both the spectral space and\nsmoothing patterns, providing comprehensive perspectives for NAD tasks. Extensive experiments\ndemonstrate that SmoothGNN consistently outperforms other SOTA competitors by a signi\ufb01cant\nmargin in terms of performance and running time, thus highlighting the effectiveness and ef\ufb01ciency\nof leveraging smoothing patterns in the NAD area.\n10\nReferences\nLeman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description:\na survey. Data Min. Knowl. Discov., pp. 626\u2013688, 2015.\nYuanchen Bei, Sheng Zhou, Qiaoyu Tan, Hao Xu, Hao Chen, Zhao Li, and Jiajun Bu. Reinforcement\nneighborhood selection for unsupervised graph anomaly detection. In ICDM, pp. 11\u201320, 2023.\nMing Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep graph\nconvolutional networks. In ICML, pp. 1725\u20131735, 2020.\nF. R. K. Chung. Spectral Graph Theory. American Mathematical Society, 1997.\nMicha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\ngraphs with fast localized spectral \ufb01ltering. In NeurIPS, pp. 3837\u20133845, 2016.\nXiangyu Dong, Xingyi Zhang, and Sibo Wang. Rayleigh quotient graph neural networks for graph-\nlevel anomaly detection. In ICLR, pp. 1\u201319, 2024.\nYingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S. Yu. Enhancing graph\nneural network-based fraud detectors against camou\ufb02aged fraudsters. In CIKM, pp. 315\u2013324,\n2020.\nJingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, and Zhibin Dong.\nGraph anomaly detection via multi-scale contrastive learning networks with augmented view. In\nAAAI, pp. 7459\u20137467, 2023a.\nJingcan Duan, Bin Xiao, Siwei Wang, Haifang Zhou , and Xinwang Liu. Arise: Graph anomaly\ndetection on attributed networks via substructure awareness. IEEE Trans. Neural Networks Learn.\nSyst., pp. 1\u201314, 2023b.\nJingcan Duan, Pei Zhang, Siwei Wang, Jingtao Hu, Hu Jin, Jiaxin Zhang, Haifang Zhou, and Xin-\nwang Liu. Normality learning-based graph anomaly detection via multi-scale contrastive learning.\nIn ACM Multimedia, pp. 7502\u20137511, 2023c.\nYuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Ad-\ndressing heterophily in graph anomaly detection: A perspective of graph spectrum. In WWW, pp.\n1528\u20131538, 2023.\nXuanwen Huang, Yang Yang, Yang Wang, Chunping Wang, Zhisheng Zhang, Jiarong Xu, Lei Chen,\nand Michalis Vazirgiannis. Dgraph: A large-scale \ufb01nancial dataset for graph anomaly detection.\nIn NeurIPS, pp. 22765\u201322777, 2022.\nYihong Huang, Liping Wang, Fan Zhang, and Xuemin Lin. Unsupervised graph outlier detection:\nProblem revisit, new insight, and superior method. In ICDE, pp. 2565\u20132578, 2023.\nJunghoon Kim, Yeonjun In, Kanghoon Yoon, Junmo Lee, and Chanyoung Park. Class label-aware\ngraph anomaly detection. In CIKM, pp. 4008\u20134012, 2023.\nThomas N. Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional net-\nworks. In ICLR, pp. 1\u201314, 2017.\nJohannes Klicpera, Aleksandar Bojchevski, and Stephan G\u00fcnnemann.\nPredict then propagate:\nGraph neural networks meet personalized pagerank. In ICLR, pp. 1\u201315, 2019.\nJundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection in\nattributed networks. In IJCAI, pp. 2152\u20132158, 2017.\nKay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu\nChen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H. Chen, Zhihao Jia, and Philip S.\nYu. BOND: benchmarking unsupervised outlier node detection on static attributed graphs. In\nNeurIPS, pp. 27021\u201327035, 2022.\nYang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and\nchoose: A gnn-based imbalanced learning approach for fraud detection. In WWW, pp. 3168\u2013\n3177, 2021.\n11\nXiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z. Sheng, Hui Xiong, and Leman\nAkoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE Trans.\nKnowl. Data Eng., pp. 12012\u201312038, 2023.\nJulian J. McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of\nuser expertise through online reviews. In WWW, pp. 897\u2013908, 2013.\nKenta Oono and Taiji Suzuki. On asymptotic behaviors of graph cnns from dynamical systems\nperspective. ArXiv, 2019.\nJunjun Pan, Yixin Liu, Yizhen Zheng, and Shirui Pan. PREM: A simple yet effective approach for\nnode-level graph anomaly detection. In ICDM, pp. 1253\u20131258, 2023.\nZhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. ANOMALOUS: A joint\nmodeling approach for anomaly detection on attributed networks. In IJCAI, pp. 3513\u20133519, 2018.\nHezhe Qiao and Guansong Pang. Truncated af\ufb01nity maximization: One-class homophily modeling\nfor graph anomaly detection. In NeurIPS, pp. 49490\u201349512, 2023.\nYu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph\nconvolutional networks on node classi\ufb01cation. In ICLR, pp. 1\u201317, 2020.\nAmit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, and Pan Li. GAD-NR:\ngraph anomaly detection via neighborhood reconstruction. In WSDM, pp. 576\u2013585, 2024.\nShuyuan Sun, Yiyang Jiang, Fan Yang, Bei Yu, and Xuan Zeng. Ef\ufb01cient hotspot detection via graph\nneural network. In DATE, pp. 1233\u20131238, 2022.\nJianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly\ndetection. In ICML, pp. 21076\u201321089, 2022.\nJianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and bench-\nmarking supervised graph anomaly detection. In NeurIPS, pp. 29628\u201329653, 2023.\nWentao Zhang, Mingyu Yang, Zeang Sheng, Yang Li, Wen Ouyang, Yangyu Tao, Zhi Yang, and Bin\nCui. Node dependent local smoothing for scalable graph learning. In NeurIPS, pp. 20321\u201320332,\n2021.\nKaixiong Zhou, Xiao Huang, Daochen Zha, Rui Chen, Li Li, Soo-Hyun Choi, and Xia Hu. Dirichlet\nenergy constrained learning for deep graph neural networks. In NeurIPS, pp. 21834\u201321846, 2021.\n12\nA\nAppendix\nA.1\nProofs\nProof of Theorem 1. The following lemma from previous work (Chen et al., 2020) is used for the\nproof.\nLemma 1. Let P = In\n2 +\n\u02dc\nA\n2 denote the propagation matrix given the adjacency matrix \u02dc\nA, we have:\nP \u221e\ni,j =\n\u221adi + 1\np\ndj + 1\n2m + n\n.\nFirst, we need to derive P t \u2212P \u221e= (P \u2212P \u221e)t. For t \u22651, we have:\n(P \u2212P \u221e)t =\nt\nX\nk=0\n\u0012t\nk\n\u0013\n(\u22121)kP t\u2212kP \u221e\n= P t +\nt\nX\nk=1\n\u0012t\nk\n\u0013\n(\u22121)kP \u221e\n= P t + P \u221e((1 \u22121)n \u22121)\n= P t \u2212P \u221e.\nThen, we can derive that\nBi,j = Pi,j \u2212P \u221e\ni,j\n= (2m + n)(I[i = j]\u221adi + 1 + 2aij) \u22122(di + 1)\np\ndj + 1\n2\u221adi + 1(2m + n)\n.\nProof of Theorem 2. Based on the proof in Section \"The stable distribution\" in the book (Chung,\n1997), for the column vector bt in augmented matrix Bt we have:\nbt = D\n1\n2\nn\nX\ni=2\n\u03c9t\nici\u03c8i,\nwhere D is the degree matrix of graph G, \u03c9i is the i-th eigenvalue of P , \u03c8i is the i-th eigenvector\nof \u02dc\nA, and ci is the coef\ufb01cient related to \u03c8i. Then, let \u03bbL\ni be the i-th eigenvalue of L and \u03bbA\ni be the\ni-th eigenvalue of \u02dc\nA, we have:\n\u03bbL\ni = 1 \u2212\u03bbA\ni = 1 \u2212(2\u03c9i \u22121) = 2 \u22122\u03c9i.\nThen we replace \u03c9i in D\n1\n2 Pn\ni=2 \u03c9t\nici\u03c8i with \u03bbL\ni , we have D\n1\n2 Pn\ni=2( \u03bbL\ni\n2 \u22121)tci\u03c8i. By applying\nTayler\u2019s expansion to it, we have:\nn\nX\ni=2\n(\u03bbL\ni\n2 \u22121)t =\nn\nX\ni=2\nt\nX\nk=0\nt!\n(k \u22121)!(t \u2212(k \u22121))!(\u03bbL\ni\n2 )k\u22121(\u22121)t\u2212(k\u22121)\n=\nt\nX\nk=0\nn\nX\ni=2\nt!\n(k \u22121)!(t \u2212(k \u22121))!(\u03bbL\ni\n2 )k\u22121(\u22121)t\u2212(k\u22121)\n=\nt\nX\nk=0\nt!\n(k \u22121)!(t \u2212(k \u22121))!(1\n2)k\u22121(\u22121)t\u2212(k\u22121)\nn\nX\ni=2\n(\u03bbL\ni )k\u22121\n=\nt\nX\nk=0\n\u03b8k\u039bk1 =\nt\nX\nk=0\n\u03b8kUT U\u039bkUT U1 =\nt\nX\nk=0\n\u02dc\u03b8kLku.\nFinally, we can get\nbt = D\n1\n2\nn\nX\ni=2\n(\u03bbL\ni\n2 \u22121)tci\u03c8i\n=\nt\nX\nk=0\n\u02dc\u03b8kLkuv.\n13\nProof of Theorem 3. For simplicity, let x denote a normalized graph signal in the graph, we have:\nE(x) =\nn\nX\ni,j=1\nai,j||\nxi\n\u221adi + 1 \u2212\nxj\np\ndj + 1||2\n2\n= xLx.\nFollowing the theorem in previous work (Dong et al., 2024), we have:\nn\nX\nj=1\n\u03bbj \u02c6x2\nj = xT Lx.\nProof of Theorem 4. The following corollary from previous work (Oono & Suzuki, 2019) is used\nfor the proof.\nCorollary 1. Let \u03bb1 \u2264\u00b7 \u00b7 \u00b7 \u2264\u03bbn be the eigenvalues of P , sorted in ascending order. Suppose the\nmultiplicity of the largest eigenvalue \u03bbn is m(\u2264n), i.e.,\u03bbn\u2212m < \u03bbn\u2212m+1 = \u00b7 \u00b7 \u00b7 = \u03bbn. And the\nsecond largest eigenvalue can be de\ufb01ned as\n\u03bb := maxn\u2212m\ns=1 |\u03bbs| < |\u03bbn|.\nLet U be the eigenspace associated with \u03bbn, then we can assume that U has an orthonormal basis\nthat consists of non-negative vectors, and then we have:\ndS(Ht) \u2264\u03c4t\u03bbdS(Ht\u22121),\nwhere \u03c4t\u03bb < 1 implying the output of the t-th layer of GNN on G exponentially approaches S.\nBased on the above corollary, we have:\ndS(Ht) \u2264\u03c4t\u03bbdS(Ht\u22121)\n\u2264(\ntY\ni=1\n\u03c4i)\u03bbtdS(X)\n\u2264\u03c4 t\u03bbtdS(X).\nWhen the GNN reaches \u01eb-smoothing, we have:\ndS(Ht) \u2264\u03c4 t\u03bbtdS(X) \u2264\u01eb \u2192t log \u03c4\u03bb < log\n\u01eb\ndS(X).\nAnd since 0 \u2264s\u03bb < 1, then we have log s\u03bb < 0, we have:\nt >\nlog\n\u01eb\ndS(X)\nlog \u03c4\u03bb\n.\nA.2\nAlgorithm\nThe detailed training process is shown in Algorithm 1. We use the trained model of the \ufb01nal epoch to\nconduct inference. The loss function is calculated using HSCSLC and HSCGNN, and the anomaly\nscore is calculated for each node using fsmooth(\u00b7) as shown in Section 3.6.\nA.3\nExperimental Settings\nThe parameters are set based on the number of nodes in different graphs as shown in Table 5. As\nwe can see, the hidden dimensions remain stable for all three categories. However, the learning\nrate and number of propagation hops increase as the number of nodes grows. Besides, we employ\napproximation techniques to calculate the converged status of propagation, i.e., we only retain the\nvalues larger than the square of epsilon in the \ufb01nal matrix. For small graphs, we do not require\napproximation whereas for medium and large graphs, we set the epsilon to 4e-3. Furthermore, the\nweight initialization strategy varies across graph categories because the optimal starting point in the\noptimization process tends to differ depending on the graph characteristics. As for the experimental\nenvironment, we conduct all the experiments on CPU for fair comparison.\n14\nAlgorithm 1: SmoothGNN\nInput: X, \u02dc\nA, T, m, n, \u01eb, where X \u2208Rn\u00d7d and A \u2208Rn\u00d7n\nOutput: HSCSLC, HSCGNN\n1 deg \u2190Degree( \u02dc\nA);\n2 deg \u2190\ndeg\n\u221a2m+n;\n3 for i = 1 to n do\n4\nif degi \u2264\u01eb then\n5\ndegi \u21900;\n6 P \u221e\u2190deg \u00b7 degT ;\n7 P \u2190In\n2 +\n\u02dc\nA\n2 ;\n8 L \u2190In \u2212\u02dc\nD\u22121\n2 \u02dc\nA \u02dc\nD\u22121\n2 ;\n9 for t = 0 to T do\n10\nBt \u2190P t \u2212P \u221e;\n11 hSLC\ni\n\u2190MLP(CONCAT((B0 \u02dc\nX0)i, \u00b7 \u00b7 \u00b7 , (BT \u02dc\nXT ))i), where i \u21901 \u00b7 \u00b7 \u00b7 n;\n12 hGNN\ni\n= MLP(CONCAT((f(L, \u02dc\nX0)0)i, \u00b7 \u00b7 \u00b7 , (f(L, \u02dc\nXT)T )i), where i \u21901 \u00b7 \u00b7 \u00b7 n;\n13 hSCSLC\ni\n\u2190hSLC\ni\n\u2217\u03b1, where \u03b1 \u2190\u03c3(SC(X, L)), where i \u21901 \u00b7 \u00b7 \u00b7 n;\n14 hSCGNN\ni\n\u2190hGNN\ni\n\u2217\u03b1, where \u03b1 \u2190\u03c3(SC(X, L)), where i \u21901 \u00b7 \u00b7 \u00b7 n;\n15 HSCSLC \u2190hSCSLC\ni\n, where i \u21901 \u00b7 \u00b7 \u00b7 n;\n16 HSCGNN \u2190hSCGNN\ni\n, where i \u21901 \u00b7 \u00b7 \u00b7 n;\n17 Return HSCSLC, HSCGNN;\nTable 5: Parameters for SmoothGNN according to different categories.\nCategories\nLearning Rate\nHop\nWeight Initialization\nEpsilon\nHidden Dimensions\nSmall\n1e-4\n4\n0.05\n0\n64\nMedium\n5e-4\n5\n0.01\n4e-3\n64\nLarge\n5e-4\n6\n0.05\n4e-3\n64\nA.4\nAblation Study\nThe ablation study for SC is presented in Table 6. Notably, without SC to rearrange the weights\nof different dimensions in the spectral space, the performance drops signi\ufb01cantly compared to the\noriginal SmoothGNN, which demonstrates the utilization of SC can boost the performances. It also\nunderscores that capturing the smoothing patterns from different views will help the learning of the\nnode representations for NAD tasks.\nA.5\nParameter Analysis\nNext, we conduct experiments to analyze the effect of representative parameters: the standard devi-\nation of weight initialization, the learning rate, and the number of propagation hops of SmoothGNN\non T-Finance, YelpChi, and Questions datasets. Figure 2 reports the AUC of SmoothGNN as we\nvary the standard deviation from 9e-3 to 12e-3, the learning rate from 4e-4 to 7e-4, and the hop from\n4 to 7. As we can observe, when we set the standard deviation to 10e-3, SmoothGNN achieves rela-\ntively satisfactory performances across these three datasets. In terms of learning rate, SmoothGNN\nexhibits relatively stable performance, but we can identify an optimal one, so we set the learning\nrate to 5e-5. Meanwhile, SmoothGNN shows a relatively stable and high performance in terms of\nall three presented datasets when we set the hop to 5. As a result, the hop is set to 5 in SmoothGNN.\nA.6\nLimitations\nAlthough SmoothGNN achieves outstanding performance in all 9 real-world datasets compared to\nprevious works, it still has some aspects to be improved. First, in this paper, we only discuss the\nsmoothing patterns of GNN and APPNP, while there are other kinds of models in the \ufb01eld of graph\n15\nTable 6: Ablation study for SC.\nDatasets\nReddit\nTolokers\nAmazon\nT-Finance\nYelpChi\nQuestions\nElliptic\nDGraph-Fin\nT-Social\nMetrics\nAUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision AUC Precision\nSmoothGNN 0.5946\n0.0438\n0.6870\n0.3517\n0.8408\n0.3953\n0.7556\n0.1408\n0.5758\n0.1823\n0.6444\n0.0592\n0.5729\n0.1161\n0.6499\n0.0199\n0.7034\n0.0631\nwithout SC\n0.5437\n0.0356\n0.6115\n0.2967\n0.5131\n0.0645\n0.2869\n0.0292\n0.5715\n0.1770\n0.6260\n0.0630\n0.5596\n0.1076\n0.5868\n0.0161\n0.6639\n0.0622\n 0.45\n 0.5\n 0.55\n 0.6\n 0.65\n 0.7\n 0.75\n 0.8\n9\n10\n11\n12\nAUC\nInit\n 0.55\n 0.6\n 0.65\n 0.7\n 0.75\n 0.8\n4\n5\n6\n7\nAUC\nLr\n 0.25\n 0.35\n 0.45\n 0.55\n 0.65\n 0.75\n4\n5\n6\n7\nAUC\nHop\n(a) Varying standard deviation\n(b) Varying learning rate\n(c) Varying hop\nFigure 2: Varying the standard deviation, learning rate, and hop.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n(a) Reddit\n(b) Tolokers\nFigure 3: Normalized Distance from converged status on different datasets.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n(a) Amazon\n(b) Elliptic\nFigure 4: Normalized Distance from converged status on different datasets.\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n 0\n 0.2\n 0.4\n 0.6\n 0.8\n 1\n 0  1  2  3  4  5  6  7  8  9\nNormalized Distance\nHop\n(a) DGraph-Fin\n(b) T-Social\nFigure 5: Normalized Distance from converged status on different datasets.\nlearning. The performance of different types of smoothing patterns can vary due to their ability\nto capture additional information, such as spectral space. Second, Compared to semi-supervised\nand supervised NAD tasks, the performance of SmoothGNN is still unsatisfactory. Hence, it is\nalso interesting to employ smoothing patterns in semi-supervised and supervised NAD tasks. Third,\nwe only present the effectiveness of smoothing patterns in the NAD area, but applying smoothing\npatterns to other related \ufb01elds can be meaningful as well.\nA.7\nObservations\nObservations from the additional datasets presented in Figure 3, 4, and 5 further reinforce our\n\ufb01ndings. These \ufb01gures clearly show that the smoothing patterns of anomalous and normal nodes\nexhibit distinct trends and scales. Our theoretical analysis and experimental results indicate that\nSmoothGNN is capable of detecting even subtle differences in these smoothing patterns. This sensi-\ntivity to nuanced smoothness characteristics is the key strength of the proposed approach.\n16\n",
    "2310.02861": "arXiv:2310.02861v4  [cs.LG]  28 Mar 2024\nPublished as a conference paper at ICLR 2024\nRAYLEIGH QUOTIENT GRAPH NEURAL NETWORKS\nFOR GRAPH-LEVEL ANOMALY DETECTION\nXiangyu Dong, Xingyi Zhang, Sibo Wang\nDepartment of Systems Engineering and Engineering Management\nThe Chinese University of Hong Kong\n{xydong, xyzhang, swang}@se.cuhk.edu.hk\nABSTRACT\nGraph-level anomaly detection has gained signi\ufb01cant attention as it \ufb01nds applica-\ntions in various domains, such as cancer diagnosis and enzyme prediction. How-\never, existing methods fail to capture the spectral properties of graph anomalies,\nresulting in unexplainable framework design and unsatisfying performance. In\nthis paper, we re-investigate the spectral differences between anomalous and nor-\nmal graphs. Our main observation shows a signi\ufb01cant disparity in the accumulated\nspectral energy between these two classes. Moreover, we prove that the accumu-\nlated spectral energy of the graph signal can be represented by its Rayleigh Quo-\ntient, indicating that the Rayleigh Quotient is a driving factor behind the anoma-\nlous properties of graphs. Motivated by this, we propose Rayleigh Quotient Graph\nNeural Network (RQGNN), the \ufb01rst spectral GNN that explores the inherent spec-\ntral features of anomalous graphs for graph-level anomaly detection. Speci\ufb01-\ncally, we introduce a novel framework with two components: the Rayleigh Quo-\ntient learning component (RQL) and Chebyshev Wavelet GNN with RQ-pooling\n(CWGNN-RQ). RQL explicitly captures the Rayleigh Quotient of graphs and\nCWGNN-RQ implicitly explores the spectral space of graphs. Extensive experi-\nments on 10 real-world datasets show that RQGNN outperforms the best rival by\n6.74% in Macro-F1 score and 1.44% in AUC, demonstrating the effectiveness of\nour framework. Our code is available at https://github.com/xydong127/RQGNN.\n1\nINTRODUCTION\nGraph-structure data explicitly expresses complex relations between items, and thus has attracted\nmuch attention from the deep learning community. Extensive efforts have been devoted to de-\nploying GNNs (Kipf & Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018) on node-level\ntasks. Recently, researchers have started to shift their focus from local properties to graph-level tasks\n(Wang et al., 2021; Liu et al., 2022; Yue et al., 2022), and graph-level anomaly detection has become\none of the most important graph-level tasks with diverse applications (Ma et al., 2022; Zhang et al.,\n2022; Qiu et al., 2022), such as cancer diagnosis, enzyme prediction, and brain disease detection. In\naddition, applications of graph-level anomaly detection can be observed in trending topics, such as\nspam detection (Li et al., 2019) and rumor detection (Bian et al., 2020).\nFollowing the common design of graph learning models, existing solutions for graph-level anomaly\ndetection mainly employ spatial GNNs with distinct pooling techniques.\nFor example, CAL\n(Sui et al., 2022) and FAITH (Wang et al., 2022) incorporate node features with topological char-\nacteristics of graphs to generate graph representations. Meanwhile, due to the limitations of the\naverage or sum pooling function in certain tasks, researchers have introduced various graph pooling\nfunctions (Wu et al., 2022; Hua et al., 2022; Liu et al., 2023). However, to the best of our knowl-\nedge, no previous attempt has provided spectral analysis for anomalous graphs, missing an important\nfeature that can help better capture the properties of anomalous graphs.\nTo address this issue, we start by investigating the spectral energy of the graph Laplacian. Our key\n\ufb01ndings and theoretical analysis validate that the accumulated spectral energy can be represented\nby the Rayleigh Quotient, which has been studied in the physics and mathematics area (Pierre,\n1988; Chan et al., 2011). Besides, Tang et al. (2022) makes an interesting observation related to the\n1\nPublished as a conference paper at ICLR 2024\nNormal Graphs\nAnomalous Graphs\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 488, nn = 9512\n(b) na = 977, nn = 19024\n(c) na = 1955, nn = 38049\nFigure 1: Normalized Rayleigh Quotient distribution on SN12C.\nRayleigh Quotient for node anomaly detection. However, the inherent properties of the Rayleigh\nQuotient in the graph domain are still relatively under-explored. Hence, we investigate the Rayleigh\nQuotient for graphs and further empirically show that the Rayleigh Quotient distributions of normal\ngraphs and anomalous graphs follow different patterns. In particular, we \ufb01rst randomly sample na\nanomalous graphs and nn normal graphs. For each graph, we calculate its corresponding Rayleigh\nQuotient. Subsequently, we set the maximum and minimum values of the Rayleigh Quotient of\ngraphs as the bounds of the value range, which is then divided into 10 equal-width bins. After that,\nwe assign each value of the Rayleigh Quotient of graphs to its corresponding bin. Finally, we cal-\nculate the frequency of values that fall into each bin and normalize them, which can be regarded as\nthe normalized Rayleigh Quotient distribution of the sampled dataset. Figure 1 reports the Rayleigh\nQuotient distribution on the SN12C dataset, and the results on other datasets can be found in Ap-\npendix A.9. As we can observe, regardless of the variations in the sample size, the Rayleigh Quotient\ndistribution of each class exhibits a consistent pattern across different sample sets. In addition, it\nis evident from Figure 1 that the Rayleigh Quotient distribution of anomalous graphs and that of\nnormal ones are distinct from each other statistically. This observation highlights how the Rayleigh\nQuotient can reveal the underlying differences between normal and anomalous graphs. Hence, the\nRayleigh Quotient should be encoded and explored when identifying anomalous graphs. Addition-\nally, as we establish a connection between the Rayleigh Quotient and the spectral energy of the\ngraph Laplacian, it becomes apparent that the spectral energy distribution exhibits robust statistical\npatterns. This, in turn, empowers us to leverage spectral graph neural networks for further encoding\nand utilization of this valuable information.\nMotivated by the observation and theoretical analysis, in this paper, we propose RQGNN, a Rayleigh\nQuotient-based GNN framework for graph-level anomaly detection tasks. It consists of two main\ncomponents: the Rayleigh Quotient learning component (RQL) and Chebyshev Wavelet GNN with\nRQ-pooling (CWGNN-RQ). Firstly, we adopt RQL to derive the Rayleigh Quotient of each graph\nand then employ a multi-layer perceptron (MLP) to generate the representation of each graph, aim-\ning to capture explicit differences between anomalous and normal graphs guided by their Rayleigh\nQuotient. Secondly, to obtain the implicit information embedded in the spectral space, we draw\ninspiration from the Chebyshev Wavelet GNN (CWGNN) and adopt it to learn the inherent in-\nformation in the graph data. Besides, to alleviate the drawbacks of existing pooling techniques\nin graph-level anomaly detection, we introduce a powerful spectral-related pooling function called\nRQ-pooling. Furthermore, we address the challenge of imbalanced data in graph-level anomaly\ndetection via a class-balanced focal loss. The \ufb01nal graph embedding is the combination of repre-\nsentations generated by the RQL and CWGNN-RQ. By combining the explicit information from\nthe Rayleigh Quotient and the implicit information from the CWGNN-RQ, RQGNN effectively\ncaptures more inherent information for the detection of anomalous graphs.\nIn our experiments, we evaluate RQGNN against 10 alternative frameworks across 10 datasets.\nExtensive experiments demonstrate that our proposed framework consistently outperforms spec-\ntral GNNs and the state-of-the-art (SOTA) GNNs for both graph classi\ufb01cation task and graph-level\nanomaly detection task. We summarize our contributions as follows:\n\u2022 Our main observation and theoretical analysis highlight that the Rayleigh Quotient reveals under-\nlying properties of graph anomalies, providing valuable guidance for future work in this \ufb01eld.\n\u2022 We propose the \ufb01rst spectral GNNs for the graph-level anomaly detection task, which incorporates\nexplicit and implicit learning components, enhancing the capabilities of anomaly detection.\n\u2022 Comprehensive experiments show that RQGNN outperforms SOTA models on 10 real-world\ngraph datasets, demonstrating the effectiveness of RQGNN.\n2\nPublished as a conference paper at ICLR 2024\n2\nPRELIMINARIES\nNotation. Let G = (A, X) denote a connected undirected graph with n nodes and m edges, where\nX \u2208Rn\u00d7F is node features, and A \u2208Rn\u00d7n is the adjacency matrix. We set Aij = 1 if there\nexists an edge between node i and j, otherwise Aij = 0. Let D be the diagonal degree matrix, the\nLaplacian matrix L is then de\ufb01ned as D \u2212A (regular) or as In \u2212D\u22121\n2 AD\u22121\n2 (normalized), where\nIn is an n \u00d7 n identity matrix.\nRayleigh Quotient. The regular Laplacian matrix can be decomposed as L = U\u039bUT , where\nU = (u1, u2, ..., un) represents orthonormal eigenvectors and the corresponding eigenvalues are\nsorted in ascending order, i.e. \u03bb1 \u2264... \u2264\u03bbn. Let x = (x1, x2, ..., xn)T \u2208Rn be a signal on graph\nG, \u02c6x = ( \u02c6x1, \u02c6x2, ..., \u02c6\nxn)T = UT x is the graph Fourier transformation of x. Following the de\ufb01nition\nin Horn & Johnson (2012), the Rayleigh Quotient is de\ufb01ned as xT Lx\nxT x .\nNext, we brie\ufb02y review spectral GNNs and existing work for graph-level anomaly detection and\ngraph classi\ufb01cation.\nSpectral GNN. By processing Laplacian matrix, spectral GNNs manipulate the projection of graph\nspectrum (Defferrard et al., 2016) and can be viewed as graph signal processing models. It has\ndrawn much attention in the graph learning community. For instance, ChebyNet (Defferrard et al.,\n2016) and BernNet (He et al., 2021) utilize different approximations of spectral \ufb01lters to improve\nthe expressiveness of spectral GNN. Specformer (Bo et al., 2023) combines the transformer and\nspectral GNN to perform self-attention in the spectral domain. BWGNN (Tang et al., 2022) adopts\na wavelet \ufb01lter to generate advanced node representations for node-level anomaly detection, showing\nthe potential ability of wavelet \ufb01lter in the anomaly detection area.\nGraph-level Anomaly Detection. To the best of our knowledge, OCGIN (Zhao & Akoglu, 2021)\nis the \ufb01rst to explore graph-level anomaly detection, which provides analysis to handle the perfor-\nmance \ufb02ip of several methods on graph classi\ufb01cation datasets. After that, OCGTL (Qiu et al., 2022)\nadopts graph transformation learning to identify anomalous graphs and GLocalKD (Ma et al., 2022)\ninvestigates the in\ufb02uence of knowledge distillation on graph-level anomaly detection. A following\nwork, HimNet (Niu et al., 2023) builds a hierarchical memory framework to balance the anomaly-\nrelated local and global information. One recent study, iGAD (Zhang et al., 2022) suggests that\nthe anomalous substructures lead to graph anomalies. It proposes an anomalous substructure-aware\ndeep random walk kernel and a node-aware kernel to capture both topological and node features,\nachieving SOTA performance. Yet, existing solutions only explain the anomalous phenomena from\nspatial perspectives. In contrast, our RQGNN further explores the spectral aspects of anomalous\ngraphs, leading to an explainable model design and satisfying model performance.\nRemark.\nIn out-of-distribution datasets, only one class of in-distribution data is given, while\nanomaly datasets usually contain data with two distinct characteristics.\nTherefore, out-of-\ndistribution detection focuses on determining whether a new sample belongs to the given class,\nwhile anomaly detection aims to determine a new sample belongs to which class.\nGraph Classi\ufb01cation. Graph classi\ufb01cation models can also be considered as a general framework\nfor our task. GMT (Baek et al., 2021) points out that a simple sum or average pooling function\nis unlikely to fully collect information for graph classi\ufb01cation. The authors propose a multi-head\nattention-based global pooling layer to capture the interactions between nodes and the topology of\ngraphs. Afterward, Gmixup (Han et al., 2022) applies data augmentation to improve the general-\nization and robustness of GNN models. Moreover, TVGNN (Hansen & Bianchi, 2023) gradually\ndistills the global label information from the node representations. Even though these models have\nachieved SOTA performance on the graph classi\ufb01cation task, the imbalanced datasets bring a non-\nnegligible problem for such models. Without speci\ufb01cally paying attention to the imbalanced nature\nof data, these SOTA graph classi\ufb01cation models are not able to meet the requirements of the graph-\nlevel anomaly detection task, as we will show during the empirical evaluation.\n3\nOUR METHOD: RQGNN\nThe observation in Section 1 highlights the differences between the Rayleigh Quotient distribution\nof anomalous and normal graphs. In Section 3.1, we further provide a theoretical analysis of the\n3\nPublished as a conference paper at ICLR 2024\nRayleigh Quotient. This motivates our design of the Rayleigh Quotient learning component (RQL)\nin our framework, to be elaborated in Section 3.2. Moreover, our theoretical analysis in Section\n3.1 further shows that the accumulated energy of the graph can be represented by the Rayleigh\nQuotient, which motivates us to apply the spectral GNN to capture the spectral energy information,\nto be detailed in Section 3.3. We further present a powerful spectral-related pooling function called\nRQ-pooling in Section 3.3. Section 3.4 elaborates on the design of class-balanced focal loss.\n3.1\nRAYLEIGH QUOTIENT AND SPECTRAL ANALYSIS\nAs described in Section 1, Rayleigh Quotient has been studied in other domains. This inspires us\nto explore the property of the Rayleigh Quotient in the graph area. Speci\ufb01cally, we further provide\nanalysis to show the connection between the Rayleigh Quotient and the spectrum of graphs. The\nfollowing two theorems show that the change of the Rayleigh Quotient can be bounded given a small\nperturbation on graph signal x and graph Laplacian L, and proofs can be found in Appendix A.1.\nTheorem 1. For any given graph G, if there exists a perturbation \u2206on L, the change of Rayleigh\nQuotient can be bounded by ||\u2206||2.\nTheorem 2. For any given graph G, if there exists a perturbation \u03b4 on x, the change of Rayleigh\nQuotient can be bounded by 2xT L\u03b4+o(\u03b4). If \u03b4 is small enough, in which case o(\u03b4) can be ignored,\nthe change can be further bounded by 2xTL\u03b4.\nTheorems 1-2 provide valuable guidance in exploring the underlying spectral properties behind\nanomalous and normal graphs based on the Rayleigh Quotient. Recap from Section 1 that the\nnormalized Rayleigh Quotient distribution of graphs with the same class label statistically exhibits a\nsimilar pattern on different sample sizes. If the graph Laplacian L and graph signal x of two graphs\nare close, then their Rayleigh Quotients will be close to each other and these two graphs will highly\nlikely belong to the same class. This motivates us to design a component to learn the Rayleigh\nQuotient of each graph directly, as we will show in Section 3.2.\nBesides, we further analyze the relationship between the Rayleigh Quotient and the spectral energy\nof graphs, which serves as the rationale for incorporating a spectral-related component into our\nframework. Let \u02c6x2\nk/ Pn\ni=1 \u02c6x2\ni denote the spectral energy of \u03bbk. Although this distribution provides\nvaluable guidance for measuring the graph spectrum in mathematics, it is not suitable for GNN\ntraining due to the time-consuming eigendecomposition computation. Therefore, in the following,\nwe introduce the accumulated spectral energy and show that it can be transformed into the Rayleigh\nQuotient, thereby avoiding the expensive matrix decomposition process.\nLet Pk\nj=1 \u02c6x2\nj/ Pn\ni=1 \u02c6x2\ni denote the accumulated spectral energy from \u03bb1 to \u03bbk. According to pre-\nvious work (Li et al., 2022; Luan et al., 2022), real-world graph data usually shows heterophily in\nconnection and high-pass graph \ufb01lters will capture more spectral information. Based on this ob-\nservation, instead of exploring the original accumulated spectral energy that represents the low-\nfrequency energy, we investigate the high-frequency energy that represents the accumulated spec-\ntral energy from \u03bbk to \u03bbn.\nFor any t \u2208[\u03bbk, \u03bbk+1), where 1 \u2264k \u2264n \u22121, we denote\nE(t) = 1 \u2212Pk\nj=1 \u02c6x2\nj/ Pn\ni=1 \u02c6x2\ni as the high-frequency energy. Then we can derive:\nZ \u03bbn\n0\nE(t)dt =\nPn\nj=1 \u03bbj \u02c6x2\nj\nPn\ni=1 \u02c6x2\ni\n= xT Lx\nxT x .\n(1)\nThis result demonstrates that the accumulated spectral energy can be exactly represented by the\nRayleigh Quotient. We summarize this result in the following proposition.\nProposition 1. Given graph G, the Rayleigh Quotient represents the accumulated spectral energy.\nThe Proposition 1 indicates the Rayleigh Quotient represents the accumulated spectral energy of the\ngraph, which motivates us to design a spectral GNN and spectral-related pooling function to capture\nthe inherent properties behind anomalous graphs, as we will show in Section 3.3.\n3.2\nRAYLEIGH QUOTIENT LEARNING COMPONENT\nMotivated by Theorems 1-2 and the observation in Section 1, a simple yet powerful component is\nintroduced to capture different trends on the Rayleigh Quotient. Speci\ufb01cally, we \ufb01rst use a two-layer\n4\nPublished as a conference paper at ICLR 2024\nMLP to obtain the latent representation of each node. Then, we calculate the Rayleigh Quotient for\neach graph as the explicit learning component in our RQGNN. Let \u02dc\nX denote the node features X\nafter the feature transformation, then the Rayleigh Quotient can be expressed as:\nRQ(X, L) = diag\n \u02dc\nXT L \u02dc\nX\n\u02dc\nXT \u02dc\nX\n!\n,\n(2)\nwhere diag(\u00b7) denotes the diagonal entries of a square matrix. Finally, we employ another two-layer\nMLP to get the Rayleigh Quotient representation of the entire graph:\nhG\nRQ = MLP (RQ(X, L)) .\n(3)\nExcept for explicitly learning from the Rayleigh Quotient, following the common design of GNN,\nwe need to implicitly learn from the topology and node features of graphs, so that we can collect\ncomprehensive information for graph-level anomaly detection. The details are presented as follows.\n3.3\nCHEBYSHEV WAVELET GNN WITH RQ-POOLING\nAs described in Section 3.1, the accumulated spectral energy can be represented by the Rayleigh\nQuotient, which reveals crucial spectral properties for graph-level anomaly detection. This moti-\nvates us to design a spectral GNN for learning graph representations. Even though existing spectral\nGNNs, e.g., ChebyNet (Defferrard et al., 2016) and BernNet (He et al., 2021), have achieved notable\nsuccess in the node classi\ufb01cation task, these models fall short in capturing the underlying properties\nof anomalous graphs, resulting in inferior performance on the graph-level anomaly detection task, as\nwe will show in our experiments. This can be attributed to two main reasons. Firstly, simple spectral\nGNNs can be seen as single low-band or high-band graph \ufb01lters (He et al., 2021), and each band\n\ufb01lter only allows certain spectral energy to pass. However, as analyzed in Section 3.1, to capture\nthe spectral properties of anomalous graphs, we should consider the spectral energy with respect\nto each eigenvalue. Therefore, it is necessary to employ multiple graph \ufb01lters. The graph wavelet\nconvolution model can be considered as a combination of different graph \ufb01lters, enabling us to con-\nsider the spectral energy associated with each eigenvalue. Consequently, leveraging graph wavelet\nconvolution provides advantages compared to using single graph \ufb01lters. Secondly, even using more\npowerful spectral GNN models, generating improved graph representations remains challenging\nwithout a carefully designed pooling function. To address these issues, we present CWGNN-RQ, a\nnovel component that effectively learns the inherent spectral representations of different graphs.\nCWGNN. Following Hammond et al. (2011a), we de\ufb01ne \u03c8 as the graph wavelet and a group of q\nwavelets can be denoted as W = (W\u03c81, W\u03c82, \u00b7 \u00b7 \u00b7 , W\u03c8q). Each wavelet is denoted as W\u03c8i =\nUgi(\u039b)UT , where gi(\u00b7) is the kernel function de\ufb01ned on [0, \u03bbn] in the spectral domain. Then, the\ngeneral wavelet GNN of a graph signal x can be expressed as:\nW x =\n\u0002\nW\u03c81, W\u03c82, \u00b7 \u00b7 \u00b7 , W\u03c8q\n\u0003\nx =\n\u0002\nUg1(\u039b)UT x, Ug2(\u039b)UT x, \u00b7 \u00b7 \u00b7 Ugq(\u039b)UT x\n\u0003\n.\nHowever, calculating graph wavelets requires decomposing the graph Laplacian, resulting in expen-\nsive computational costs. To achieve better computational ef\ufb01ciency, we employ Chebyshev polyno-\nmials to calculate approximate wavelet operators. The following lemma shows that the Chebyshev\nseries can be used to represent any function.\nLemma 1 (Rivlin (1974)). There always exists a convergent Chebyshev series for any function f(t):\nf(t) = 1\n2c0 +\n\u221e\nX\nk=1\nckTk(t),\nwhere ck = 2\n\u03c0\nR \u03c0\n0 cos(k\u03b8)f(cos(\u03b8))d\u03b8, and k is the order of the Chebyshev polynomials.\nIn addition, the Chebyshev polynomials on interval [\u22121, 1] can be iteratively de\ufb01ned as Tk(t) =\n2tTk\u22121(t) \u2212Tk\u22122(t) with initial values of T0(t) = 1 and T1(t) = t. Given that the eigenvalues of\nthe normalized Laplacian matrix fall in the range of [0, 2], we utilize the shifted Laplacian matrix\nL \u2212In to compute the following shifted Chebyshev polynomials \u00afT. Meanwhile, for each wavelet\ni, we also have a scale function si(\u00b7) to re-scale the eigenvalue so that it can \ufb01t into the domain of\nf to calculate the following Chebyshev coef\ufb01cient \u00afci,k. By introducing the truncated Chebyshev\n5\nPublished as a conference paper at ICLR 2024\npolynomials into graph wavelet operators, the i-th kernel function fi(L) is designed to capture the\n\ufb01rst iK-hop information, which can be expressed as follows (Hammond et al., 2011b):\nfi(L) = 1\n2 \u00afci,0In +\niK\nX\nk=1\n\u00afci,k \u00afTk(L),\n(4)\nwhere \u00afTk(L) =\n4\n\u03bbn (L \u2212I) \u00afTk\u22121(L) \u2212\u00afTk\u22122(L) with initial values of \u00afT0(L) = In and \u00afT1(L) =\ntIn represents the shifted Chebyshev polynomials and \u00afci,k = 2\n\u03c0\nR \u03c0\n0 cos(k\u03b8)f(si( \u03bbn(cos(\u03b8)+1)\n2\n))d\u03b8\nwith 1 \u2264i \u2264q. Then, the results of q graph wavelets are concatenated together to generate the\nrepresentation of node j:\nhj = CONCAT\n\u0012\u0010\nf1(L) \u02dc\nX\n\u0011\nj ,\n\u0010\nf2(L) \u02dc\nX\n\u0011\nj , \u00b7 \u00b7 \u00b7 ,\n\u0010\nfq(L) \u02dc\nX\n\u0011\nj\n\u0013\n.\n(5)\nAfter node representations h are generated by CWGNN, a pooling function is needed to obtain the\nrepresentation of the entire graph. Commonly used pooling functions such as average pooling and\nsum pooling functions (Hamilton et al., 2017) have achieved satisfactory performance in various\nclassi\ufb01cation tasks. However, as demonstrated in our experiments, these techniques become inef-\nfective in graph-level anomaly detection. Consequently, this challenge calls for a newly designed\npooling function that can effectively guide CWGNN to learn better graph representation.\nRQ-pooling. In order to incorporate spectral information into node weights, we adopt an attention\nmechanism to generate the graph representation:\nhG\nAtt = \u03c3\n\uf8eb\n\uf8edX\nj\u2208V\najhj\n\uf8f6\n\uf8f8,\n(6)\nwhere \u03c3 is the non-linear activation function, and aj is the attention coef\ufb01cient of node j. Specif-\nically, since the spectral energy corresponds to each graph signal, we set the Rayleigh Quotient\nas the weight of these signals.\nThen, the attention coef\ufb01cient for node j can be expressed as\naj = RQ(X, L)hj, which is used as the node importance score in RQ-pooling. Such a strategy\nallows CWGNN to capture more underlying spectral information of graphs. The \ufb01nal representation\nof the graph is the concatenation of both hG\nRQ and hG\nAtt:\nhG = MLP\n\u0000CONCAT\n\u0000hG\nAtt, hG\nRQ\n\u0001\u0001\n.\n(7)\n3.4\nCLASS-BALANCED FOCAL LOSS\nAs discussed in Section 2, the imbalanced nature of graph-level anomaly detection brings non-\nnegligible challenges. To tackle this issue, we introduce a re-weighting technique called the class-\nbalanced focal loss, which enhances the anomalous detection capability of our RQGNN.\nExpected number (Cui et al., 2019). As the number of training samples increases, there will be\nmore potential information overlap among different samples. Consequently, the marginal bene\ufb01t\nthat a model can extract from the data diminishes. To address this issue, the class-balanced focal loss\nis designed to capture the diminishing marginal bene\ufb01ts by using more data points of a class. This\napproach ensures that the model effectively utilizes the available data while avoiding redundancy\nand maximizing its learning potential. Speci\ufb01cally, we de\ufb01ne the expected number \u03b7(nt) as the total\nnumber of samples that can be covered by nt training data and utilize the inverse of this number as\nthe balance factor in our loss function.\nProposition 2. The expected number \u03b7(nt) = 1\u2212\u03b2nt\n1\u2212\u03b2 , where \u03b2 = N\u22121\nN\nwith N equaling to the total\nnumber of data points in class t.\nIn practice, without further information of data for each class, it is dif\ufb01cult to empirically \ufb01nd a set\nof good N for all classes. Therefore, we assume N is dataset-dependent and set the same \u03b2 for all\nclasses in a dataset. In addition, we also employ focal loss (Lin et al., 2017), an adjusted version of\ncross-entropy loss. By combining the expected number and focal loss together, we can achieve the\ngoal of reweighting the loss for each class. The class-balanced focal loss is de\ufb01ned as follows:\nLCBfocal = Lfocal\n\u03b7(ny) =\n1 \u2212\u03b2\n1 \u2212\u03b2ny\nC\nX\ni=1\n(1 \u2212pi)\u03b3log(pi),\n(8)\n6\nPublished as a conference paper at ICLR 2024\nTable 1: AUC and Macro-F1 scores (%) on ten datasets with random split.\nSpectral GNN\nGraph Classi\ufb01cation\nGraph-level Anomaly Detection\nDatasets\nMetrics ChebyNet BernNet\nGMT\nGmixup TVGNN OCGIN OCGTL GLocalKD HimNet\niGAD\nRQGNN-1 RQGNN-2 RQGNN\nMCF-7\nAUC\n0.6612\n0.6172\n0.7706\n0.6954\n0.7180\n0.5348\n0.5866\n0.6363\n0.6369\n0.8146\n0.8094\n0.8346\n0.8354\nF1\n0.4780\n0.4784\n0.4784\n0.4779\n0.5594\n-\n-\n-\n-\n0.6468\n0.6626\n0.7205\n0.7394\nMOLT-4\nAUC\n0.6647\n0.6144\n0.7606\n0.6232\n0.7159\n0.5299\n0.6191\n0.6631\n0.6633\n0.8086\n0.8246\n0.8196\n0.8316\nF1\n0.4854\n0.4794\n0.4814\n0.4789\n0.4916\n-\n-\n-\n-\n0.6617\n0.7119\n0.7113\n0.7240\nPC-3\nAUC\n0.6051\n0.6094\n0.7896\n0.6908\n0.7974\n0.5810\n0.6349\n0.6727\n0.6703\n0.8723\n0.8553\n0.8671\n0.8782\nF1\n0.4853\n0.4853\n0.4853\n0.4852\n0.6206\n-\n-\n-\n-\n0.6697\n0.7003\n0.7241\n0.7184\nSW-620\nAUC\n0.6759\n0.6072\n0.7467\n0.6479\n0.7326\n0.4955\n0.6398\n0.6542\n0.6544\n0.8512\n0.8401\n0.8427\n0.8550\nF1\n0.4898\n0.4847\n0.4874\n0.4844\n0.5365\n-\n-\n-\n-\n0.6627\n0.6941\n0.7209\n0.7335\nNCI-H23\nAUC\n0.6728\n0.6114\n0.8030\n0.7324\n0.7782\n0.4948\n0.6122\n0.6837\n0.6814\n0.8297\n0.8413\n0.8554\n0.8680\nF1\n0.4930\n0.4869\n0.4869\n0.4869\n0.5520\n-\n-\n-\n-\n0.6646\n0.6735\n0.7349\n0.7214\nOVCAR-8\nAUC\n0.6303\n0.5850\n0.7692\n0.5663\n0.7653\n0.5298\n0.6007\n0.6750\n0.6757\n0.8691\n0.8549\n0.8650\n0.8799\nF1\n0.4900\n0.4868\n0.4868\n0.4869\n0.5406\n-\n-\n-\n-\n0.6638\n0.6876\n0.7077\n0.7215\nP388\nAUC\n0.7266\n0.6707\n0.8495\n0.6516\n0.7957\n0.5252\n0.6501\n0.6445\n0.6667\n0.8995\n0.8911\n0.8904\n0.9023\nF1\n0.5635\n0.5001\n0.6583\n0.4856\n0.5557\n-\n-\n-\n-\n0.7437\n0.7552\n0.7738\n0.7963\nSF-295\nAUC\n0.6650\n0.6353\n0.7992\n0.6471\n0.7346\n0.4774\n0.6440\n0.7069\n0.7073\n0.8770\n0.8691\n0.8781\n0.8825\nF1\n0.4871\n0.4871\n0.4871\n0.4866\n0.4935\n-\n-\n-\n-\n0.6919\n0.7120\n0.7335\n0.7416\nSN12C\nAUC\n0.6598\n0.6014\n0.7919\n0.7211\n0.7441\n0.5004\n0.5617\n0.6880\n0.6916\n0.8747\n0.8851\n0.8904\n0.8861\nF1\n0.4972\n0.4874\n0.4874\n0.4871\n0.5437\n-\n-\n-\n-\n0.6714\n0.7204\n0.7549\n0.7660\nUACC257\nAUC\n0.6584\n0.6115\n0.7735\n0.6564\n0.7410\n0.5411\n0.6148\n0.6647\n0.6659\n0.8512\n0.8447\n0.8596\n0.8724\nF1\n0.4894\n0.4895\n0.4894\n0.4904\n0.5373\n-\n-\n-\n-\n0.6483\n0.6889\n0.7087\n0.7362\nwhere \u03b2 and \u03b3 are hyperparameters, ny is the number of samples in class y that the current sample\nbelongs to, C denotes the number of classes, and pi = softmax(hG)i is the predicted probability for\nthe current sample belonging to class i.\n4\nEXPERIMENTS\n4.1\nEXPERIMENTAL SETUP\nDatasets. We use 10 real-world datasets to investigate the performance of RQGNN, including MCF-\n7, MOLT-4, PC-3, SW-620, NCI-H23, OVCAR-8, P388, SF-295, SN12C, and UACC257. These\ndatasets are obtained from the TUDataset (Morris et al., 2020), consisting of various chemical com-\npounds and their reactions to different cancer cells. We treat inactive chemical compounds as normal\ngraphs and active ones as anomalous graphs.\nBaselines. We compare RQGNN against 10 SOTA GNN competitors, including spectral GNNs,\ngraph classi\ufb01cation models and graph-level anomaly detection models.\n\u2022 Spectral GNNs with average pooling function: ChebyNet (Defferrard et al., 2016) and BernNet\n(He et al., 2021).\n\u2022 Graph classi\ufb01cation models: GMT (Baek et al., 2021), Gmixup (Han et al., 2022), and TVGNN\n(Hansen & Bianchi, 2023).\n\u2022 Graph-level anomaly detection models: OCGIN (Zhao & Akoglu, 2021), OCGTL (Qiu et al.,\n2022), GLocalKD (Ma et al., 2022), HimNet (Niu et al., 2023), and iGAD (Zhang et al., 2022).\nAlso, we investigate two variants of RQGNN. We use RQGNN-1 to indicate the model that replaces\nthe RQ-pooling with the average pooling, and use RQGNN-2 to indicate the model without the RQL\ncomponent. More details of the datasets and baselines can be found in Appendix A.3.\nExperimental Settings. We randomly divide each dataset into training/validation/test sets with\n70%/15%/15%, respectively. During the sampling process, we ensure that each set maintains a\nconsistent ratio between normal and anomalous graphs. We select the epoch where the models\nachieve the best Macro-F1 score on the validation set as the best epoch and use the corresponding\nmodel for performance evaluation. We set the learning rate as 0.005, the batch size as 512, the\nhidden dimension d = 64, the width of CWGNN-RQ q = 4, the depth of CWGNN-RQ K = 6, the\ndropout rate as 0.4, the hyperparameters of the loss function \u03b2 = 0.999, \u03b3 = 1.5, and we use batch\nnormalization for the \ufb01nal graph embeddings. We obtain the source code of all competitors from\nGitHub and perform these GNN models with default parameter settings suggested by their authors.\n4.2\nEXPERIMENTAL RESULTS\nWe \ufb01rst evaluate the performance of RQGNN against different SOTA GNN models. Table 1 reports\nAUC and Macro-F1 scores of each GNN model on 10 datasets. The best result on each dataset is\n7\nPublished as a conference paper at ICLR 2024\nMCF-7\nSF-295\nSN12C\nUACC257\n 0.65\n 0.7\n 0.75\n 0.8\n32\n64\n128\n256\nF1 Score\nd\n 0.65\n 0.7\n 0.75\n 0.8\n3\n4\n5\n6\nF1 Score\nq\n 0.65\n 0.7\n 0.75\n 0.8\n5\n6\n7\n8\nF1 Score\nK\n(a) Varying hidden dimensions\n(b) Varying width\n(c) Varying depth\nFigure 2: Varying the hidden dimension, width, and depth.\nhighlighted in boldface. Since OCGIN, OCGTL, GLocalKD, and HimNet adopt one-class classi-\n\ufb01cation to identify anomalous graphs, we only report their AUC scores. As we can see, RQGNN\noutperforms all baselines on all datasets. Next, we provide our detailed observations.\nFirstly, for two SOTA spectral GNNs, ChebyNet and BernNet, they fail to learn the underlying\nanomalous properties from the spectral perspective. In particular, compared with ChebyNet and\nBernNet, RQGNN takes the lead by 20.72% and 25.28% on these 10 datasets in terms of aver-\nage AUC score, and takes the lead by 24.40% and 25.33% on these 10 datasets in terms of aver-\nage Macro-F1 score, respectively. These empirical evidences demonstrate that even though graph\nLaplacian matrix is related to graph spectral energy, we still need to carefully design graph \ufb01lters\nand pooling functions to capture the underlying anomalous properties in the graph.\nSecondly, we carefully check the results of GNNs for graph classi\ufb01cation to verify whether graph-\nlevel anomaly detection can be easily tackled by graph classi\ufb01cation models. From Table 1, we can\nobserve that compared to three GNN models, GMT, Gmixup, and TVGNN, RQGNN takes a lead\nby 8.38%, 20.59%, and 11.69% in terms of average AUC score and 23.70%, 25.48%, and 19.67% in\nterms of average Macro-F1 score, respectively. These results demonstrate that graph classi\ufb01cation\nmodels cannot be directly adopted to handle the graph-level anomaly detection task.\nThirdly, we compare RQGNN with SOTA GNN models designed for graph-level anomaly detection.\nDespite being specialized for graph-level anomaly detection, OCGIN, OCGTL, GLocalKD, and\nHimNet fail to outperform other GNN baselines in terms of AUC scores. This can be attributed\nto their inability to effectively capture the important graph anomalous information. In contrast,\nRQGNN guided by the Rayleigh Quotient successfully captures the spectral differences between\nanomalous and normal graphs, resulting in signi\ufb01cantly superior performance compared to OCGIN,\nOCGTL, GLocalKD, and HimNet. In particular, RQGNN takes the lead by an average margin of\n34.82%, 25.28%, 20.02%, and 19.78% in terms of AUC score, respectively. Among all the baselines,\niGAD stands out as the most competitive model, which incorporates anomalous-awaresub-structural\ninformation into node representations. However, it lacks the incorporation of important properties\nof anomalous graphs, such as the Rayleigh Quotient and spectral energy of graphs, which leads to\nrelatively unsatisfying Macro-F1 scores on all datasets. With the guidance of Rayleigh Quotient,\nRQGNN outperforms iGAD by 1.44% in terms of AUC score and 6.74% in terms of Macro-F1\nscore on average across 10 datasets.\n4.3\nABLATION STUDY\nIn this set of experiments, we investigate the effectiveness of each component in RQGNN. Ablation\nstudy for the class-balanced focal loss can be found in Appendix A.2. The experimental results of\nRQGNN variants are shown in Table 1.\nFirstly, we use RQGNN-1 to indicate the model that replaces the RQ-pooling with average pooling.\nRecap from Section 4.1 that, it combines the representation of Rayleigh Quotient and CWGNN with\naverage pooling function. As we can observe, RQGNN-1 outperforms all other baselines on all\ndatasets in terms of the Macro-F1 scores. In particular, compared with RQGNN-1, RQGNN further\nboosts the performance and takes the lead by 1.76% in terms of the AUC score and 3.92% in terms\nof the Macro-F1 score on average. This result demonstrates that the RQ-pooling that introduces\nRayleigh Quotient as the node weight captures more crucial information from the spectral domain.\n8\nPublished as a conference paper at ICLR 2024\nTrain\nTest True\nTest False\n 0\n 0.1\n 0.2\n 0.3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0.3\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) Normal Graphs\n(b) Anomalous Graphs\nFigure 3: Normalized Rayleigh Quotient distribution on SN12C.\nThen, we use RQGNN-2 to indicate the model with only CWGNN-RQ. Speci\ufb01cally, we remove the\nRQL component that explicitly calculates the Rayleigh Quotient of each graph. Instead, we only\ncompute the Rayleigh Quotient as the node weights for CWGNN. As we can observe, RQGNN-2\noutperforms all the other baselines in terms of the Macro-F1 scores, which again shows the effec-\ntiveness of the RQ-pooling. Besides, according to Table 1, we can see that RQGNN is 0.09% higher\nthan RQGNN-2 in terms of AUC score and 1.08% higher than RQGNN-2 in terms of Macro-F1\nscore on average, which further shows the effectiveness of the RQL component. In summary, these\nresults demonstrate the effectiveness of each component in RQGNN.\n4.4\nPARAMETER ANALYSIS\nNext, we conduct experiments to analyze the effect of representative parameters: the hidden di-\nmension d of RQGNN, the width q and depth K of CWGNN-RQ on MCF-7, SF-295, SN12C, and\nUACC257 datasets. Figure 2 reports the Macro-F1 score of RQGNN as we vary the hidden dimen-\nsion d from 32 to 256, the width q from 3 to 6, and the depth K from 5 to 8. As we can observe,\nwhen we set the hidden dimension to 64, RQGNN achieves relatively satisfactory performances on\nthese four datasets. When the width of CWGNN-RQ is set to 4, RQGNN achieves the best results\non all four datasets. Hence, we set the width to 4 in our experiments. Meanwhile, as we can observe,\nRQGNN shows a relatively stable and high performance in terms of all four presented datasets when\nwe set the depth to 6. As a result, the depth is set to 6 in RQGNN.\n4.5\nCASE STUDY\nIn this set of experiments, we conduct experiments to investigate whether RQGNN learns the trends\nof Rayleigh Quotient on anomalous and normal graphs. If RQGNN successfully detects anomalous\ngraphs, the samples in the test set that can be classi\ufb01ed correctly by a converged model should have\na similar Rayleigh Quotient distribution to that in the training set. Figure 3 illustrates the Rayleigh\nQuotient distribution of the normal and anomalous graphs in the training and test set on the SN12C\ndataset. As we can see, graphs that can be classi\ufb01ed correctly in the test set exhibit a similar Rayleigh\nQuotient distribution to that in the training set. Meanwhile, those graphs that our RQGNN can not\nclassify correctly, display different distributions from the train graphs. These results demonstrate\nthat Rayleigh Quotient is an intrinsic characteristic of the graph-level anomaly detection task for the\napplication studied. Our RQGNN can effectively learn the Rayleigh Quotient as a discriminative\nfeature and thus outperforms SOTA competitors.\n5\nCONCLUSION\nIn this paper, we introduce spectral analysis into the graph-level anomaly detection task. We dis-\ncover differences in the spectral energy distributions between anomalous and normal graphs and\nfurther demonstrate the observation through comprehensive experiments and theoretical analysis.\nThe combination of the RQL component that explicitly captures the Rayleigh Quotient of the graph\nand CWGNN-RQ that implicitly explores graph anomalous information provides different spectral\nperspectives for this task. Extensive experiments demonstrate that RQGNN consistently outper-\nforms other SOTA competitors by a signi\ufb01cant margin.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGMENTS\nThis work was supported by Hong Kong RGC GRF (Grant No. 14217322) and Hong Kong ITC ITF\n(Grant No.MRP/071/20X). Sibo Wang is the corresponding author.\nREFERENCES\nJinheon Baek, Minki Kang, and Sung Ju Hwang. Accurate learning of graph representations with\ngraph multiset pooling. In ICLR, 2021.\nTian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang.\nRumor detection on social media with bi-directional graph convolutional networks. In AAAI, pp.\n549\u2013556, 2020.\nDeyu Bo, Chuan Shi, Lele Wang, and Renjie Liao. Specformer: Spectral graph neural networks\nmeet transformers. In ICLR, 2023.\nK.T. Chan, N.G. Stephen, and K. Young. Perturbation theory and the rayleigh quotient. Journal of\nSound and Vibration, 330:2073\u20132078, 2011.\nYin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge J. Belongie. Class-balanced loss based\non effective number of samples. In CVPR, pp. 9268\u20139277, 2019.\nMicha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\ngraphs with fast localized spectral \ufb01ltering. In NeurIPS, pp. 3837\u20133845, 2016.\nWilliam L. Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. In NeurIPS, pp. 1024\u20131034, 2017.\nDavid K. Hammond, Pierre Vandergheynst, and R\u00b4emi Gribonval. Wavelets on graphs via spectral\ngraph theory. Appl. Comput. Harmon. Anal., 30(2):129\u2013150, 2011a.\nDavid K. Hammond, Pierre Vandergheynst, and R\u00b4emi Gribonval. Wavelets on graphs via spectral\ngraph theory. Applied and Computational Harmonic Analysis, 30:129\u2013150, 2011b.\nXiaotian Han, Zhimeng Jiang, Ninghao Liu, and Xia Hu. G-mixup: Graph data augmentation for\ngraph classi\ufb01cation. In ICML, pp. 8230\u20138248, 2022.\nJonas Berg Hansen and Filippo Maria Bianchi. Total variation graph neural networks. In ICML, pp.\n12445\u201312468, 2023.\nMingguo He, Zhewei Wei, Zengfeng Huang, and Hongteng Xu. Bernnet: Learning arbitrary graph\nspectral \ufb01lters via bernstein approximation. In NeurIPS, pp. 14239\u201314251, 2021.\nRoger A Horn and Charles R Johnson. Matrix analysis. Cambridge university press, 2012.\nChenqing Hua, Guillaume Rabusseau, and Jian Tang. High-order pooling for graph neural networks\nwith tensor decomposition. In NeurIPS, 2022.\nThomas N. Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional net-\nworks. In ICLR, 2017.\nBoris Knyazev, Graham W. Taylor, and Mohamed R. Amer. Understanding attention and gener-\nalization in graph neural networks. In Advances in Neural Information Processing Systems 32:\nAnnual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December\n8-14, 2019, Vancouver, BC, Canada, pp. 4204\u20134214, 2019.\nAo Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong Li. Spam review detection with graph convo-\nlutional networks. In CIKM, pp. 2703\u20132711, 2019.\nMingjie Li, Xiaojun Guo, Yifei Wang, Yisen Wang, and Zhouchen Lin. G2cn: Graph gaussian\nconvolution networks with concentrated graph \ufb01lters. In ICML, 2022.\n10\nPublished as a conference paper at ICLR 2024\nTsung-Yi Lin, Priya Goyal, Ross B. Girshick, Kaiming He, and Piotr Doll\u00b4ar. Focal loss for dense\nobject detection. In ICCV, pp. 2999\u20133007, 2017.\nChuang Liu, Yibing Zhan, Jia Wu, Chang Li, Bo Du, Wenbin Hu, Tongliang Liu, and Dacheng Tao.\nGraph pooling for graph neural networks: Progress, challenges, and opportunities. In IJCAI, pp.\n6712\u20136722, 2023.\nZemin Liu, Qiheng Mao, Chenghao Liu, Yuan Fang, and Jianling Sun. On size-oriented long-tailed\ngraph classi\ufb01cation of graph neural networks. In WWW, pp. 1506\u20131516, 2022.\nSitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen\nChang, and Doina Precup. Revisiting heterophily for graph neural networks. In NeurIPS, 2022.\nRongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly\ndetection by glocal knowledge distillation. In WSDM, pp. 704\u2013714, 2022.\nChristopher Morris, Nils M. Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion\nNeumann. Tudataset: A collection of benchmark datasets for learning with graphs. In ICML\nWorkshop GRL+, 2020.\nChaoxi Niu, Guansong Pang, and Ling Chen. Graph-level anomaly detection via hierarchical mem-\nory networks. In ECML-PKDD, 2023.\nShirui Pan, Xingquan Zhu, Chengqi Zhang, and Philip S. Yu. Graph stream classi\ufb01cation using\nlabeled and unlabeled graphs.\nIn 29th IEEE International Conference on Data Engineering,\nICDE 2013, Brisbane, Australia, April 8-12, 2013, pp. 398\u2013409, 2013.\nChristophe Pierre. Comments on rayleigh\u2019s quotient and perturbation theory for the eigenvalue\nproblem. Journal of Applied Mechanics, 55:986\u2013988, 1988.\nChen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. Raising the bar in graph-level anomaly\ndetection. In IJCAI, pp. 2196\u20132203, 2022.\nGTheodore J. Rivlin. The Chebyshev polynomials. John Wiley & Sons, 1974.\nYongduo Sui, Xiang Wang, Jiancan Wu, Min Lin, Xiangnan He, and Tat-Seng Chua. Causal atten-\ntion for interpretable and generalizable graph classi\ufb01cation. In SIGKDD, pp. 1696\u20131705, 2022.\nJianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly\ndetection. In ICML, pp. 21076\u201321089, 2022.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\nBengio. Graph attention networks. In ICLR, 2018.\nSong Wang, Yushun Dong, Xiao Huang, Chen Chen, and Jundong Li. Faith: Few-shot graph classi-\n\ufb01cation with hierarchical task graphs. In IJCAI, pp. 2284\u20132290, 2022.\nYiwei Wang, Wei Wang, Yuxuan Liang, Yujun Cai, and Bryan Hooi. Curgraph: Curriculum learning\nfor graph classi\ufb01cation. In WWW, pp. 1238\u20131248, 2021.\nJunran Wu, Xueyuan Chen, Ke Xu, and Shangzhe Li. Structural entropy guided graph hierarchical\npooling. In ICML, pp. 24017\u201324030, 2022.\nHan Yue, Chunhui Zhang, Chuxu Zhang, and Hongfu Liu. Label-invariant augmentation for semi-\nsupervised graph classi\ufb01cation. In NeurIPS, 2022.\nGe Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su, Chuan Zhou, Quan Z.\nSheng, Leman Akoglu, and Charu C. Aggarwal. Dual-discriminative graph neural network for\nimbalanced graph-level anomaly detection. In NeurIPS, 2022.\nLingxiao Zhao and Leman Akoglu. On using classi\ufb01cation datasets to evaluate graph outlier detec-\ntion: Peculiar observations and new insights. Big Data, 11(3):151\u2013180, 2021.\n11\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nPROOFS\nProof of Theorem 1. The following lemma is used for the proof.\nLemma 2 (Courant-Fischer Theorem). Let M \u2208Rn\u00d7n be a symmetric matrix with eigenvalues\n\u00b51 \u2264\u00b52 \u2264\u00b7 \u00b7 \u00b7 \u2264\u00b5n. Then,\n\u00b5k =\nmax\nS\u2286Rn,dim(s)=k\nmin\nx\u2208S,x\u0338=0\nxT Mx\nxT x\n=\nmin\nT \u2286Rn,dim(T )=n\u2212k+1\nmax\nx\u2208T,x\u0338=0\nxT Mx\nxT x ,\nwhere the maximization and minimization are over subspaces S and T of Rn.\nLet \u03bbi(\u00b7) denote the i-th eigenvalue of the input matrix, where the eigenvalue is in ascending order\naccording to the index. For symmetric matrix L and \u2206, according to Lemma 2, we have:\n\u03bbi(L + \u2206) =\nmin\nT \u2286Rn,dim(T )=n\u2212i+1\nmax\nx\u2208T,x\u0338=0\nxT (L + \u2206)x\nxT x\n=\nmin\nT \u2286Rn,dim(T )=n\u2212i+1( max\nx\u2208T,x\u0338=0\nxT Lx\nxT x +\nmax\nx\u2208T,x\u0338=0\nxT \u2206x\nxT x )\n\u2264\nmin\nT \u2286Rn,dim(T )=n\u2212i+1( max\nx\u2208T,x\u0338=0\nxT Lx\nxT x + \u03bbn(\u2206))\n= \u03bbi(L) + \u03bbn(\u2206).\nSimilarly, by employing the other format of Lemma 2, we can derive \u03bbi(L)+\u03bb1(\u2206) \u2264\u03bbi(L+\u2206),\nfrom which it follows that\nmax\ni {|\u03bbi(L + \u2206) \u2212\u03bbi(L)|} \u2264max{|\u03bbn(\u2206)|, |\u03bb1(\u2206)|} = ||\u2206||2.\nHence, we can rewrite the inequality as:\nmax\ni {|\u03bbi(L) \u2212\u03bbi(L + \u2206)|} \u2264||\u2206||2.\nConsidering the change of Rayleigh Quotient, we have:\nxT (L + \u2206)x\nxT x\n\u2212xT Lx\nxT x =\nPn\nj=1 \u03bbj(L + \u2206)\u02c6x2\nj\nPn\ni=1 \u02c6x2\ni\n\u2212\nPn\nj=1 \u03bbj(L)\u02c6x2\nj\nPn\ni=1 \u02c6x2\ni\n=\nPn\nj=1(\u03bbj(L + \u2206) \u2212\u03bbj(L))\u02c6x2\nj\nPn\ni=1 \u02c6x2\ni\n\u2264\nmax\nt\nn\n(\u03bbt(L + \u2206) \u2212\u03bbt(L)) Pn\nj=1 \u02c6x2\nj\no\nPn\ni=1 \u02c6x2\ni\n\u2264max\nt (\u03bbt(L + \u2206) \u2212\u03bbt(L)) \u2264||\u2206||2.\nProof of Theorem 2. For simplicity, let x be a normalized vector so that the Rayleigh Quotient has\nthe form as xT Lx. Then, we have:\n(x + \u03b4)T L(x + \u03b4) \u2212xT Lx\n=xT Lx + \u03b4T Lx + xT A\u03b4 + \u03b4T L\u03b4 \u2212xT Lx\n=2xTL\u03b4 + o(\u03b4)\nIf o(\u03b4) is small enough, in which case o(\u03b4) can be ignored, the change is bounded by 2xTL\u03b4.\nProof of Proposition 2. The result can be derived by mathematical induction. Let n = 1 be the\ninitial case. It is obvious that \u03b7(1) = 1\u2212\u03b21\n1\u2212\u03b2 = 1, so n = 1 holds. Assume that this proposition holds\nfor n = k \u22121, and the probability of sampling k-th data is p = \u03b7(k \u22121)/N. Therefore, the expected\nnumber after sampling k-th data is:\n\u03b7(k) = p\u03b7(k \u22121) + (1 \u2212p)(\u03b7(k \u22121) + 1) = 1 + N \u22121\nN\n\u03b7(k \u22121)\n12\nPublished as a conference paper at ICLR 2024\nTable 2: Ablation study for class-balanced focal loss.\nDatasets\nMetrics ChebyNet ChebyNet-CB BernNet BernNet-CB TVGNN TVGNN-CB Gmixup Gmixup-CB\nGMT\nGMT-CB RQGNN-3 RQGNN\nMCF-7\nAUC\n0.6612\n0.6617\n0.6172\n0.6215\n0.7180\n0.7309\n0.6954\n0.6984\n0.7706\n0.7479\n0.8239\n0.8354\nF1\n0.4780\n0.4804\n0.4784\n0.4784\n0.5594\n0.5608\n0.4779\n0.4779\n0.4784\n0.4784\n0.7293\n0.7394\nMOLT-4\nAUC\n0.6647\n0.6650\n0.6144\n0.6068\n0.7159\n0.7574\n0.6232\n0.6644\n0.7606\n0.6226\n0.8341\n0.8316\nF1\n0.4854\n0.4875\n0.4794\n0.4794\n0.4916\n0.5793\n0.4789\n0.4789\n0.4814\n0.4794\n0.7213\n0.7240\nPC-3\nAUC\n0.6051\n0.6065\n0.6094\n0.6040\n0.7974\n0.7782\n0.6908\n0.6592\n0.7896\n0.7877\n0.8520\n0.8782\nF1\n0.4853\n0.4853\n0.4853\n0.4853\n0.6206\n0.6136\n0.4852\n0.4852\n0.4853\n0.4853\n0.7287\n0.7184\nSW-620\nAUC\n0.6759\n0.6823\n0.6072\n0.6081\n0.7326\n0.7332\n0.6479\n0.6202\n0.7467\n0.7620\n0.8504\n0.8550\nF1\n0.4898\n0.4947\n0.4847\n0.4847\n0.5365\n0.5547\n0.4844\n0.4844\n0.4874\n0.4847\n0.7081\n0.7335\nNCI-H23\nAUC\n0.6728\n0.6734\n0.6114\n0.6289\n0.7782\n0.7810\n0.7324\n0.6793\n0.8030\n0.7808\n0.8539\n0.8680\nF1\n0.4930\n0.5203\n0.4869\n0.4869\n0.5520\n0.5484\n0.4869\n0.4869\n0.4869\n0.4869\n0.7340\n0.7214\nOVCAR-8\nAUC\n0.6303\n0.6294\n0.5850\n0.5803\n0.7653\n0.7730\n0.5663\n0.6591\n0.7692\n0.7713\n0.8563\n0.8799\nF1\n0.4900\n0.4992\n0.4868\n0.4868\n0.5406\n0.5938\n0.4869\n0.4869\n0.4868\n0.4868\n0.7126\n0.7215\nP388\nAUC\n0.7266\n0.7324\n0.6707\n0.6694\n0.7957\n0.7757\n0.6516\n0.6562\n0.8495\n0.8638\n0.8883\n0.9023\nF1\n0.5635\n0.5656\n0.5001\n0.5002\n0.5557\n0.5549\n0.4856\n0.4856\n0.6583\n0.6268\n0.7897\n0.7963\nSF-295\nAUC\n0.6650\n0.6670\n0.6353\n0.6302\n0.7346\n0.7722\n0.6471\n0.7070\n0.7992\n0.8004\n0.8792\n0.8825\nF1\n0.4871\n0.4871\n0.4871\n0.4871\n0.4935\n0.5779\n0.4866\n0.4866\n0.4871\n0.4871\n0.7074\n0.7416\nSN12C\nAUC\n0.6598\n0.6634\n0.6014\n0.5992\n0.7441\n0.7710\n0.7211\n0.7017\n0.7919\n0.7985\n0.8803\n0.8861\nF1\n0.4972\n0.4970\n0.4874\n0.4874\n0.5437\n0.5736\n0.4871\n0.4871\n0.4874\n0.4875\n0.7554\n0.7660\nUACC257\nAUC\n0.6584\n0.6645\n0.6115\n0.6130\n0.7410\n0.7450\n0.6564\n0.6097\n0.7735\n0.7845\n0.8537\n0.8724\nF1\n0.4894\n0.4933\n0.4895\n0.4895\n0.5373\n0.5330\n0.4904\n0.4904\n0.4894\n0.4895\n0.7095\n0.7362\nTable 3: Tumor description of the 10 datasets\nDataset MCF-7 MOLT-4\nPC-3\nSW-620\nNCI-H23\nOVCAR-8\nP388\nSF-295\nSN12C UACC-257\nTumor Breast Leukemia Prostate\nColon\nNon-Small Cell Lung\nOvarian\nLeukemia Central Nervous System Rental Melanoma\nSince we assume that \u03b7(k \u22121) = 1\u2212\u03b2k\u22121\n1\u2212\u03b2\n, so we can derive:\n\u03b7(k) = 1 + \u03b2 1 \u2212\u03b2k\u22121\n1 \u2212\u03b2\n= 1 \u2212\u03b2k\n1 \u2212\u03b2\nwhich proves the expected number of samples is \u03b7(k) = 1\u2212\u03b2k\n1\u2212\u03b2 .\nA.2\nABLATION STUDY FOR CLASS-BALANCED FOCAL LOSS\nIn this set of experiments, we explore the impact of replacing the cross-entropy loss function with\nthe class-balanced focal loss in spectral GNN, i.e.,ChebyNet (Defferrard et al., 2016) and Bern-\nNet (He et al., 2021) and GNN models designed for graph classi\ufb01cation tasks, i.e., TVGNN-CB\n(Hansen & Bianchi, 2023), Gmixup-CB (Han et al., 2022), and GMT-CB (Baek et al., 2021). In ad-\ndition, we introduce RQGNN-3 as a variant of RQGNN that uses the cross-entropy loss instead of\nthe class-balanced focal loss during training. The results of these models are summarized in Table\n2. As we can see, the class-balanced focal loss does not yield bene\ufb01ts for ChebyNet, BernNet, BG-\nmixup, and GMT. This further demonstrates that even though tackling the imbalanced problem of\ndatasets, these spectral GNNs and graph classi\ufb01cation models fail to capture the underlying proper-\nties of anomalous graphs. For TVGNN, class-balanced focal loss improves the performance to some\nextent. However, it still falls way behind models speci\ufb01cally designed for graph-level anomaly de-\ntection. This observation highlights the differences between graph classi\ufb01cation and graph-level\nanomaly detection tasks. When comparing RQGNN-3 with RQGNN, we notice the performance\ndrops on almost all datasets. This indicates that the class-balanced focal loss can indeed bene\ufb01t\ngraph anomaly detection models.\nA.3\nBASELINES AND DATASETS\nBaselines. The \ufb01rst group is Spectral GNNs:\n\u2022 ChebyNet (Defferrard et al., 2016): a GNN with Chebyshev polynomials as convolution kernels;\n\u2022 BernNet (He et al., 2021): a GNN which utilizes Bernstein approximation of spectral \ufb01lters.\nThe second group is GNN models designed for graph classi\ufb01cation:\n\u2022 GMT (Baek et al., 2021): a GNN with multi-head attention-based global pooling layer to capture\nthe interactions between nodes and topology of graphs;\n\u2022 Gmixup (Han et al., 2022): a GNN with data augmentation to improve the robustness;\n\u2022 TVGNN (Hansen & Bianchi, 2023): a GNN with knowledge distillation of node representations.\nThe third group is SOTA GNN models for graph-level anomaly detection:\n13\nPublished as a conference paper at ICLR 2024\nTable 4: Statistics of 10 real-world datasets, where nn is the number of normal graphs, na is the\nnumber of anomalous graphs, h =\nna\nnn+na is the anomalous ratio, \u00afn is the average number of nodes,\n\u00afm is the average number of edges, and F is the number of attributes.\nDataset MCF-7 MOLT-4\nPC-3\nSW-620 NCI-H23 OVCAR-8\nP388\nSF-295 SN12C UACC257\nnn\n25476\n36625\n25941\n38122\n38296\n38437\n39174\n38246\n38049\n38345\nna\n2294\n3140\n1568\n2410\n2057\n2079\n2298\n2025\n1955\n1643\nh\n0.0826\n0.079\n0.057\n0.0595\n0.051\n0.0513\n0.0554\n0.0503\n0.0489\n0.0411\n\u00afn\n26.4\n26.1\n26.36\n26.06\n26.07\n26.08\n22.11\n26.06\n26.08\n262.09\n\u00afm\n28.53\n28.14\n28.49\n28.09\n28.1\n28.11\n23.56\n28.09\n28.11\n28.13\nF\n46\n64\n45\n65\n65\n65\n72\n65\n65\n64\n\u2022 OCGIN (Zhao & Akoglu, 2021): the \ufb01rst GNN to explore graph-level anomaly detection;\n\u2022 OCGTL (Qiu et al., 2022): a GNN with graph transformation learning;\n\u2022 GLocalKD (Ma et al., 2022): a GNN with the random distillation technique;\n\u2022 HimNet (Niu et al., 2023): a GNN with a hierarchical memory framework to balance the anomaly-\nrelated local and global information;\n\u2022 iGAD (Zhang et al., 2022): a GNN with a substructure-aware component to capture topological\nfeatures and a node-aware component to capture node features.\nDatasets.\nThe datasets used in our experiments are collected from PubChem by TUDataset\n(Morris et al., 2020). According to PubChem, these 10 datasets provide information on the bio-\nlogical activities of small molecules, where nodes denote atoms in the chemical compounds, and\nedges represent the chemical bonds between two atoms. These datasets contain the bioassay records\nfor anticancer screen tests with different cancer cell lines. Each dataset belongs to a certain type of\ncancer screen with the outcome active or inactive. We treat inactive chemical compounds as normal\ngraphs and active ones as anomalous graphs. In addition, the attributes are generated from node\nlabels using one-hot encoding.\nThe statistics of these 10 real-world datasets are shown in Table4 and Table 3 provides a brief tumor\ndescription of these datasets.\nA.4\nDATASETS BEYOND BIOGRAPHY DOMAIN\nTable 5: Datasets beyond the biography domain\nDatasets\nMetrics\nChebyNet\nBernNet\nTVGNN\nGMT\nHimNet\niGAD\nRQGNN\nCOLORS-3\nAUC\n0.6192\n0.6136\n0.5000\n0.5063\n0.5129\n0.7385\n0.9378\nF1\n0.4764\n0.4764\n0.4764\n0.4764\n0.5934\n0.5301\n0.7640\nDBLP v1\nAUC\n0.8369\n0.8549\n0.8455\n0.8234\n0.6656\n0.7346\n0.8808\nF1\n0.5472\n0.4877\n0.6449\n0.4877\n0.6133\n0.6648\n0.7709\nCOLORS-3-ind\nAUC\n0.6500\n0.6486\n0.5000\n0.5000\n0.4522\n0.7630\n0.9421\nF1\n0.3710\n0.3710\n0.3710\n0.3710\n0.4707\n0.6674\n0.8423\nAlthough the main application of graph-level anomaly detection is typically focused on detecting\nchemical compounds, there are still applications in other domains such as social networks. However,\ndue to the lack of datasets speci\ufb01cally collected for graph-level anomaly detection in other domains,\nwe construct new datasets and conduct experiments on them. To be comprehensive, we use two\ngraph classi\ufb01cation datasets to evaluate our RQGNN, one is COLORS-3 (Knyazev et al., 2019),\nwhich is a synthetic dataset in TUDataset repository (Morris et al., 2020), and the other is DBLP v1\n(Pan et al., 2013) which is a social network dataset as classi\ufb01ed in TUDataset.\nSpeci\ufb01cally, we \ufb01rst conduct experiments on the COLORS-3 dataset, which contains 11 classes. We\nuse 10 classes to form normal graphs and use the remaining class as anomalous graphs.\nThen, we conduct additional experiments in the social network domain using the DBLP v1 dataset,\nwhich is a well-balanced social network classi\ufb01cation dataset in the \ufb01eld of computer science. In this\ndataset, each graph denotes a research paper belonging to either DBDM (database and data mining)\nor CVPR (computer vision and pattern recognition) \ufb01eld, where each node denotes a paper ID or a\nkeyword and each edge denotes the citation relationship between papers or keyword relations in the\n14\nPublished as a conference paper at ICLR 2024\ntitle. To create a new graph-level anomaly detection dataset, we randomly sample 5% of one class\nas the anomaly class and use the other class as the normal class.\nIn addition, despite our primary focus not being on the out-of-distribution task, we still conduct\nadditional experiments to evaluate the performance of our proposed model. We use a modi\ufb01ed\nCOLORS-3 dataset to construct COLORS-3-ind dataset. We randomly select two classes as out-of-\ndistribution graphs and the remaining classes serve as normal graphs. One of the out-of-distribution\nclasses is included in the training and validation sets, while the other is added to the testing set.\nTable 5 presents the results of this set of experiments. Compared with other baseline models, our\nproposed RQGNN achieves signi\ufb01cant improvements in both AUC and F1 scores in these new\ndatasets, which demonstrates the effectiveness of our RQGNN in detecting anomalies beyond the\napplication considered.\nA.5\nALGORITHM\nAlgorithm 1: RQL\nInput: {Gi}, where i = 1, \u00b7 \u00b7 \u00b7 , n\nOutput: HG\nRQ\n1 for i = 1 to n do\n2\n\u02dc\nX \u2190MLP(Gi.X);\n3\nRQ \u2190diag(\n\u02dc\nXT L \u02dc\nX\n\u02dc\nXT \u02dc\nX );\n4\nhG\nRQ \u2190MLP(RQ);\n5\n(HG\nRQ)i \u2190hG\nRQ;\n6 Return HG\nRQ;\nAlgorithm 2: CWGNN with RQ-pooling\nInput: {Gi}, where i = 1, \u00b7 \u00b7 \u00b7 , n\nOutput: HG\nAtt\n1 for i = 1 to n do\n2\n\u02dc\nX \u2190MLP(Gi.X);\n3\nRQ \u2190diag(\n\u02dc\nXT L \u02dc\nX\n\u02dc\nXT \u02dc\nX );\n4\nhG\nAtt \u21900;\n5\nfor node j in Gi do\n6\nhj \u2190CONCAT((f1(L) \u02dc\nX)j, \u00b7 \u00b7 \u00b7 , (fq(L) \u02dc\nX)j);\n7\naj \u2190RQ \u00b7 hj;\n8\nhG\nAtt \u2190hG\nAtt + ajhj;\n9\nhG\nAtt \u2190\u03c3(hG\nAtt);\n10\n(HG\nAtt)i \u2190hG\nAtt;\n11 Return HG\nAtt;\nAlgorithm 3: RQGNN\nInput: {Gi}, where i = 1, \u00b7 \u00b7 \u00b7 , n\nOutput: HG\n1 HG\nRQ \u2190RQL({Gi});\n2 HG\nAtt \u2190CWGNN with RQ-pooling({Gi});\n3 HG \u2190CONCAT(HG\nRQ, HG\nAtt);\n4 Return HG;\nAs described in Section 3, RQGNN contains two components, RQL and Chebyshev Wavelet GNN\nwith RQ-pooling. The RQL learns the explicit representation of the Rayleigh Quotient of graphs\nwhile the Chebyshev Wavelet GNN with RQ-pooling learns the implicit representation of graphs\n15\nPublished as a conference paper at ICLR 2024\nguided by the Rayleigh Quotient. After getting the two representations, we combine them to obtain\nthe \ufb01nal representation of graphs.\nA.6\nPARAMETER ANALYSIS\nMCF-7\nSF-295\nSN12C\nUACC257\n 0.65\n 0.7\n 0.75\n 0.8\n1\n1.5\n2\n2.5\nF1 Score\ngamma\n 0.65\n 0.7\n 0.75\n 0.8\n.99\n.999\n.9999\n.99999\nF1 Score\nbeta\n(a) Varying \u03b3\n(b) Varying \u03b2\nFigure 4: Varying the \u03b3 and \u03b2.\nOther than the hyperparameters for the Chebyshev Wavelet GNN with RQ-pooling, we also investi-\ngate the impact of varying \u03b3 and \u03b2 in the class-balanced focal loss on the performance of RQGNN.\nThe results are shown in Figure 4. As we can see, when the \u03b3 is set to 1.5, RQGNN achieves a\nrelatively satisfactory performance on these four datasets. Meanwhile, RQGNN is relatively satis-\nfactory when \u03b2 is set to 0.999. Hence, we choose 1.5 for \u03b3 and 0.999 for \u03b2 in our experiments on\nall datasets.\nA.7\nPERTURBATION ON SN12C\nNormal Graphs\nAnomalous Graphs\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) 5% perturbation\n(b) 10% perturbation\n(c) 15% perturbation\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(d) 20% perturbation\n(e) 25% perturbation\nFigure 5: Normalized Rayleigh Quotient distribution on perturbated SN12C.\nRecap from Section 3.1 that Theorem 1 and Theorem 2 show that the change of the Rayleigh Quo-\ntient can be bounded given a small perturbation on graph signal x and graph Laplacian L. In this\nsection, we further conduct experiments to explore how much perturbation is detectable by the pro-\nposed RQGNN. We use SN12C to create 5 new synthetic datasets. Speci\ufb01cally, we \ufb01rst randomly\nselect 5% normal graphs to perform perturbations. For these graphs, each edge is changed with prob-\nability p, where p = 0.05, 0.10, 0.15, 0.20, 0.25. Then we use these perturbed samples as anomalous\ngraphs and all the remaining 95% normal graphs in SN12C to construct these \ufb01ve new datasets. The\ncorresponding normalized Rayleigh Quotient is shown in Figure 5.\nAs we can see from Figure 6, even with a perturbation probability as low as 0.05, RQGNN can still\nachieve a reasonable result. As the perturbation probability increases to 0.15, our RQGNN achieves\nan AUC score around 1 and an F1 score exceeding 0.9, which clearly demonstrates that RQGNN\ncan accurately distinguish the two classes.\n16\nPublished as a conference paper at ICLR 2024\nAUC\nAUC\nF1\nF1\n 0.6\n 0.7\n 0.8\n 0.9\n 1\n 0.05\n 0.1\n 0.15\n 0.2\n 0.25\nPerformance\nPerturbation\nFigure 6: Performance of SN12C with different ratios of perturbation.\nA.8\nDISTANCE RATIO BETWEEN DIFFERENT CLASSES\nWe calculate pairwise inter-distances and pairwise intra-distances for the normalized Rayleigh Quo-\ntient values of two classes. To keep consistency with the experiments shown in Figure 1, we still\nconstruct 10 equal-width bins. Subsequently, we report the results of inter-distances divided by\nintra-distances of the normalized Rayleigh Quotient for two classes, respectively. The detailed ex-\nperimental results are presented in Table 6, where di is the inter-distance between normalized values\nof Rayleigh Quotient of different classes, da is the intra-distance between normalized values of\nRayleigh Quotient values of anomalous graphs, and dn is the intra-distance between normalized\nvalues of Rayleigh Quotient values of normal graphs.\nTable 6: Distance ratio between different classes of different datasets.\nDatasets\nRatio\nbin0\nbin1\nbin2\nbin3\nbin4\nbin5\nbin6\nbin7\nbin8\nbin9\nMCF-7\ndi/da\n3.2630\n2.7014\n1.1659\n5.4298\n5.8771\n3.5399\n1.7634\n0.8756\n0.9013\n17.6960\ndi/dn\n2.1631\n5.2946\n1.0262\n4.2198\n4.5321\n7.5492\n3.4830\n1.8955\n1.3387\n7.8023\nMOLT-4\ndi/da\n3.4385\n7.0977\n0.4458\n36.4909\n7.1165\n3.8262\n2.2038\n1.1997\n0.2470\n4.9966\ndi/dn\n2.1167\n1.7348\n3.8065\n14.6213\n41.3865\n19.1898\n5.1109\n1.8908\n0.5795\n7.3194\nPC-3\ndi/da\n188.8639\n4.8383\n0.5602\n4.4561\n16.9550\n4.5858\n2.2441\n0.7786\n0.9506\n7.2704\ndi/dn\n2.7716\n8.9202\n0.8078\n4.9309\n6.1323\n11.7506\n4.3151\n2.0752\n1.6180\n6.4001\nSW-620\ndi/da\n10.0290\n4.7585\n2.6133\n6.4650\n5.5431\n5.6535\n1.8690\n0.8894\n0.6743\n22.8407\ndi/dn\n2.4499\n2.3840\n50.2978\n20.2884\n299.8270\n34.37006\n6.0213\n1.3747\n1.3095\n7.0025\nNCI-H23\ndi/da\n6.4151\n7.6458\n0.1815\n9.6685\n34.2458\n6.8839\n2.1601\n0.3966\n1.4374\n38.5681\ndi/dn\n2.6216\n2.7331\n52.7906\n15.1957\n44.8589\n33.3045\n4.9554\n0.7743\n2.1844\n6.3351\nOVCAR-8\ndi/da\n4.9954\n19.6767\n1.2632\n8.7049\n19.3535\n3.9187\n2.1573\n0.8668\n0.8162\n113.6427\ndi/dn\n2.6150\n2.9819\n10.5372\n14.4965\n103.3491\n35.3353\n4.6184\n1.7530\n1.6612\n6.5138\nP388\ndi/da\n8.9957\n8.3098\n0.3023\n0.4619\n1.7153\n0.7943\n1.3958\n11.0056\n0.2758\n1.1250\ndi/dn\n2.0826\n6.9907\n1.1292\n2.1624\n4.6504\n0.8967\n10.5379\n3.2803\n1.3869\n8.6336\nSF-295\ndi/da\n11.2356\n5.2296\n0.2031\n81.6818\n3.7652\n3.7081\n2.4469\n0.6470\n0.9760\n17.3201\ndi/dn\n2.5468\n3.1328\n3.2846\n12.9510\n67.4922\n31.3818\n5.7962\n1.2385\n1.9829\n7.4618\nSN12C\ndi/da\n5.2940\n8.8330\n0.5253\n8.9315\n11.8474\n6.2311\n1.9104\n0.6942\n0.7776\n114.3876\ndi/dn\n3.0953\n2.4427\n16.7566\n17.5596\n114.7632\n30.4557\n5.3002\n1.2671\n1.4012\n5.6286\nUACC257\ndi/da\n4.8530\n33.1566\n0.2014\n5.3367\n8.0657\n8.8780\n2.0177\n0.9534\n1.3980\n38.5248\ndi/dn\n2.5624\n2.4816\n2.1689\n25.3848\n225.1055\n52.8033\n4.7289\n1.8897\n2.1000\n6.1323\nAdditionally, we present the distance ratios between different classes of different perturbations (re-\ncap from Section A.7) on SN12C in Table 7.\nTable 7: Distance ratio between different classes of different perturbations\nDatasets\nRatio\nbin0\nbin1\nbin2\nbin3\nbin4\nbin5\nbin6\nbin7\nbin8\nbin9\n5% perturbation\ndi/da\n7.3235\n0.0882\n0.8025\n2.1880\n13.4657\n0.1622\n1.5447\n2.9775\n0.7854\n1.9094\ndi/dn\n0.6507\n2.6996\n2.8406\n4.8837\n9.9019\n1.1670\n1.3218\n1.1467\n0.6372\n8.0939\n10% perturbation\ndi/da\n23.2792\n0.1572\n2.2364\n37.7247\n19.0277\n1.0230\n2.7819\n6.6939\n9.5345\n0.4399\ndi/dn\n1.2710\n2.1388\n10.3101\n16.4515\n8.5852\n1.0900\n2.5552\n2.2331\n4.0627\n9.0571\n15% perturbation\ndi/da\n24.8715\n0.3862\n4.8172\n27.4571\n16.0609\n1.8245\n5.2849\n35.0558\n300.1928\n0.4652\ndi/dn\n2.2010\n5.0905\n18.0752\n23.4784\n11.9045\n1.8214\n3.4373\n3.6316\n6.2911\n15.2549\n20% perturbation\ndi/da\n16.4177\n0.8982\n3.1912\n13.8354\n25.3389\n3.3763\n33.8295\n12.6984\n26.2688\n0.4820\ndi/dn\n2.7813\n23.0458\n18.3562\n43.5881\n14.4079\n2.6532\n4.8426\n5.8488\n8.0527\n27.8505\n25% perturbation\ndi/da\n20.5731\n1.0082\n6.9559\n29.4320\n19.3459\n7.7016\n16.7429\n23.0523\n14.1050\n0.4837\ndi/dn\n4.2974\n9.3829\n37.7505\n68.5836\n12.8552\n5.1918\n8.7394\n7.9608\n7.9395\n31.4415\nAs we can see, there is no consistent pattern in the numerical data presented in the two tables.\nNevertheless, RQGNN successfully learns the Rayleigh Quotient of the graphs and effectively dis-\ntinguishes between the two classes. This demonstrates that a consistent pattern in the ratio is not\nnecessary for accurate classi\ufb01cation. The key to differentiating between the classes is capturing and\nleveraging such gaps on certain dimensions, rather than relying on consistent patterns in the data.\n17\nPublished as a conference paper at ICLR 2024\nA.9\nRAYLEIGH QUOTIENT DISTRIBUTION ON OTHER DATASETS\nTo further demonstrate the Rayleigh Quotient can be applied to any graph-level anomaly detection\ndatasets, we calculate the normalized Rayleigh Quotient distribution of all the other 9 datasets, which\nare shown in Figures 7-15. As we can observe, for each dataset, regardless of the variations in the\nsample size, the normalized Rayleigh Quotient distribution of each class exhibits a consistent pattern\nacross different sample sets. Besides, the normalized Rayleigh Quotient distribution of anomalous\ngraphs and that of normal ones are statistically distinct from each other on all datasets. These results\nagain demonstrate that the Rayleigh Quotient can reveal the underlying differences between normal\nand anomalous graphs.\nNormal Graphs\nAnomalous Graphs\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 573, nn = 6369\n(b) na = 1147, nn = 12738\n(c) na = 2294, nn = 25476\nFigure 7: Normalized Rayleigh Quotient distribution on MCF-7.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 785, nn = 9156\n(b) na = 1570, nn = 18312\n(c) na = 3140, nn = 36625\nFigure 8: Normalized Rayleigh Quotient distribution on MOLT-4.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 392, nn = 6485\n(b) na = 784, nn = 12970\n(c) na = 1568, nn = 25941\nFigure 9: Normalized Rayleigh Quotient distribution on PC-3.\n18\nPublished as a conference paper at ICLR 2024\nNormal Graphs\nAnomalous Graphs\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 602, nn = 9530\n(b) na = 1205, nn = 19061\n(c) na = 2410, nn = 38122\nFigure 10: Normalized Rayleigh Quotient distribution on SW-620.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 574, nn = 9793\n(b) na = 1149, nn = 19587\n(c) na = 2298, nn = 39174\nFigure 11: Normalized Rayleigh Quotient distribution on P388.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 514, nn = 9574\n(b) na = 1028, nn = 19148\n(c) na = 2057, nn = 38296\nFigure 12: Normalized Rayleigh Quotient distribution on NCI-H23.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 519, nn = 9609\n(b) na = 1039, nn = 19218\n(c) na = 2079, nn = 38437\nFigure 13: Normalized Rayleigh Quotient distribution on OVCAR-8.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 506, nn = 9561\n(b) na = 1012, nn = 19123\n(c) na = 2025, nn = 38246\nFigure 14: Normalized Rayleigh Quotient distribution on SF-295.\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n 0\n 0.1\n 0.2\n 0\n 0.2\n 0.4\n 0.6\n 0.8\nNormalized Frequency\nRayleigh Quotient\n(a) na = 410, nn = 9586\n(b) na = 821, nn = 19172\n(c) na = 1643, nn = 38345\nFigure 15: Normalized Rayleigh Quotient distribution on UACC257.\n19\n",
    "2103.09430": "OGB-LSC: A Large-Scale Challenge for\nMachine Learning on Graphs\nWeihua Hu1, Matthias Fey2, Hongyu Ren1, Maho Nakata3, Yuxiao Dong4, Jure Leskovec1\n1Department of Computer Science, Stanford University\n2Department of Computer Science, TU Dortmund University\n3RIKEN, 4Facebook AI\nogb-lsc@cs.stanford.edu\nAbstract\nEnabling effective and ef\ufb01cient machine learning (ML) over large-scale graph\ndata (e.g., graphs with billions of edges) can have a great impact on both indus-\ntrial and scienti\ufb01c applications. However, existing efforts to advance large-scale\ngraph ML have been largely limited by the lack of a suitable public benchmark.\nHere we present OGB Large-Scale Challenge (OGB-LSC), a collection of three\nreal-world datasets for facilitating the advancements in large-scale graph ML. The\nOGB-LSC datasets are orders of magnitude larger than existing ones, covering\nthree core graph learning tasks\u2014link prediction, graph regression, and node clas-\nsi\ufb01cation. Furthermore, we provide dedicated baseline experiments, scaling up\nexpressive graph ML models to the massive datasets. We show that expressive\nmodels signi\ufb01cantly outperform simple scalable baselines, indicating an oppor-\ntunity for dedicated efforts to further improve graph ML at scale. Moreover,\nOGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more\nthan 500 team registrations globally, during which signi\ufb01cant performance im-\nprovements were made by a variety of innovative techniques. We summarize the\ncommon techniques used by the winning solutions and highlight the current best\npractices in large-scale graph ML. Finally, we describe how we have updated the\ndatasets after the KDD Cup to further facilitate research advances. The OGB-LSC\ndatasets, baseline code, and all the information about the KDD Cup are available at\nhttps://ogb.stanford.edu/docs/lsc/.\n1\nIntroduction\nTable 1: Basic statistics of the OGB-LSC datasets\nused in KDD Cup 2021. Datasets marked by \u2020 has\nbeen updated to v2 after the KDD Cup (cf. Section 3).\nTask type\nDataset\nStatistics\nNode-level\nMAG240M\n#nodes:\n244,160,499\n#edges:\n1,728,364,232\nLink-level\nWikiKG90M\u2020\n#nodes:\n87,143,637\n#edges:\n504,220,369\nGraph-level\nPCQM4M\u2020\n#graphs:\n3,803,453\n#edges (total):\n55,399,880\nMachine Learning (ML) on graphs has\nattracted immense attention in recent\nyears because of the prevalence of graph-\nstructured data in real-world applications.\nModern application domains include Web-\nscale social networks (Ugander et al.,\n2011), recommender systems (Ying et al.,\n2018), hyperlinked Web documents (Klein-\nberg, 1999), knowledge graphs (KGs) (Bol-\nlacker et al., 2008; Vrande\u02c7ci\u00b4c and Kr\u00a8otzsch,\n2014), as well as the molecule simulation\ndata generated by the ever-increasing scien-\nti\ufb01c computation (Nakata and Shimazaki,\n2017; Chanussot et al., 2021). All these domains involve large-scale graphs with billions of edges or\na dataset with millions of graphs. Deploying accurate graph ML at scale will have a huge practical im-\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\narXiv:2103.09430v3  [cs.LG]  20 Oct 2021\nNode-level\nMAG240M-LSC\nPaper\nAuthor\nInstitution\nwrites\naffiliated with\ncites\nSubject area?\nPredict\n(a) MAG240M\nLink-level\nWikiKG90M-LSC\nis a\nGeoffrey Hinton\nPerson\naffiliated \nwith\nUniversity of \nToronto\nCanada\nlocated in\nPaul Martin\ngraduated \nfrom\nborn in\nGraduated \nfrom\n?\nKing\u2019s College, \nCambridge\nPredict\n(b) WikiKG90M\nGraph-level\nPCQM4M-LSC\nQuantum \ncalculation (DFT)\n~hours\nHOMO-LUMO gap?\nPredict << 1s\n(c) PCQM4M\nFigure 1: Overview of the three OGB-LSC datasets, covering node-, link-, and graph-level\nprediction tasks, respectively. (a) MAG240M is a heterogeneous academic graph, and the task is\nto predict the subject areas of papers situated in the heterogeneous graph (cf. Section 2.1). (b)\nWikiKG90M is a knowledge graph, and the task is to impute missing triplets (cf. Section 2.2).\n(c) PCQM4M is a quantum chemistry dataset, and the task is to predict an important molecular\nproperty\u2014the HOMO-LUMO gap\u2014of a given molecule (cf. Section 2.3).\npact, enabling better recommendation results, improved web document search, more comprehensive\nKGs, and accurate ML-based drug and material discovery.\nHowever, community efforts to advance state-of-the-art in large-scale graph ML have been quite\nlimited. In fact, most of graph ML models have been developed and evaluated on extremely small\ndatasets (Yang et al., 2016; Morris et al., 2020; Bordes et al., 2013). Recently, the Open Graph\nBenchmark (OGB) has been introduced to provide a collection of larger graph datasets (Hu et al.,\n2020a), but they are still small compared to graphs found in the industrial and scienti\ufb01c applications.\nHandling large-scale graphs is challenging, especially for state-of-the-art expressive Graph Neural\nNetworks (GNNs) (Kipf and Welling, 2017; Hamilton et al., 2017; Velickovic et al., 2018) because\nthey make predictions on each node based on the information from many other nodes. Effectively\ntraining these models at scale requires sophisticated algorithms that are well beyond standard SGD\nover i.i.d. data (Hamilton et al., 2017; Chen et al., 2018; Chiang et al., 2019; Zeng et al., 2020). More\nrecently, researchers improve the model scalability by signi\ufb01cantly simplifying GNNs (Wu et al.,\n2019; Rossi et al., 2020; Huang et al., 2020), which inevitably limits their expressive power.\nHowever, in deep learning, it has been demonstrated over and over again that one needs big expressive\nmodels and train them on big data to achieve the best performance (He et al., 2016; Russakovsky\net al., 2015; Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020). In graph ML, the trend\nhas been the opposite\u2014models get simpli\ufb01ed and less expressive to be able to scale to large graphs\n(Wu et al., 2019). Thus, there is a massive opportunity to enable graph ML techniques to work with\nrealistic and large-scale graph datasets, exploring the potential of expressive models for big graphs.\nHere we present a large-scale graph ML challenge, OGB Large-Scale Challenge (OGB-LSC), to\nfacilitate the development of state-of-the-art graph ML models for massive modern datasets. Speci\ufb01-\ncally, we introduce three large-scale, realistic, and challenging datasets\u2014MAG240M, WikiKG90M,\nand PCQM4M\u2014that are unprecedentedly large in scale (see Table 1; the sizes are 10 to 100 times\nlarger than the corresponding original OGB datasets1) and cover predictions at the level of nodes,\nlinks, and graphs, respectively. An overview of the datasets is provided in Figure 1.\nBeyond providing the datasets, we perform an extensive baseline analysis on each dataset and\nimplement both simple baseline models and advanced expressive models at scale. We \ufb01nd that\nadvanced expressive models\u2014despite requiring more efforts to scale up\u2014do bene\ufb01t from the large\ndata and signi\ufb01cantly outperform simple baseline models that are easy to scale.\nTo facilitate the community engagement, we recently organized the ACM KDD Cup 2021 around the\nOGB-LSC datasets. The competition attracted more than 500 team registrations and 150 leaderboard\nsubmissions. Within the three-month duration of the competition (March 15 to June 15, 2021), we\nhave already witnessed innovative methods being developed to provide impressive performance\n1Speci\ufb01cally, MAG240M is 126 times larger than ogbn-mag in terms of the number of nodes, WikiKG90M\nis 35 times larger than ogbl-wikikg2 in terms of the number of nodes, and PCQM4M is 9 times larger than\nogbg-molpcba in terms of the number of graphs.\n2\ngains2, further solidifying the value of the OGB-LSC datasets to advance state-of-the-art. We\nsummarize the common techniques shared by the winning solutions, highlighting the current best\npractices of large-scale graph ML. Moreover, based on the lessons learned from the KDD Cup, we\ndescribe the future plan to update the datasets so that they can be further used to advance large-scale\ngraph ML.\n2\nOGB-LSC Datasets, Baselines, and KDD Cup Summary\nWe describe the OGB-LSC datasets, covering three key task categories (node-, link-, and graph-level\nprediction tasks) of ML on graphs. We emphasize the practical relevance and data split for each\ndataset, making our task closely aligned to realistic applications. Through our extensive baseline\nexperiments, we show that advanced expressive models tend to give much better performance than\nsimple graph ML models, leaving room for further improvement. All the OGB-LSC datasets are\navailable through the OGB Python package (Hu et al., 2020a). All the baseline and package code is\navailable at https://github.com/snap-stanford/ogb.\nIn addition, we highlight the top 3 winning results from our KDD Cup 2021 that signi\ufb01cantly advance\nstate-of-the-art and summarize common techniques used by the winning solutions. Note that while\nour baselines only used a single model for simplicity, all the winners used extensive model ensembling\nfor their test submissions in order to maximize the performance. For a more direct comparison,\nwe also report the winners\u2019 self-reported validation accuracy in the main text, which still exhibits\nsigni\ufb01cant accuracy improvement over our strong baselines.\n2.1\nMAG240M: Node-Level Prediction\nPractical relevance and dataset overview. The volume of scienti\ufb01c publication has been increasing\nexponentially, doubling every 12 years (Dong et al., 2017). Currently, subject areas of ARXIV papers\nare manually determined by the paper\u2019s authors and ARXIV moderators. An accurate automatic\npredictor of papers\u2019 subject categories not only reduces the signi\ufb01cant burden of manual labeling, but\ncan also be used to classify the vast number of non-ARXIV papers, thereby allowing better search\nand organization of academic papers.\nMAG240M is a heterogeneous academic graph extracted from the Microsoft Academic Graph\n(MAG) (Wang et al., 2020). Given arXiv papers situated in the heterogeneous graph, whose schema\ndiagram is illustrated in Figure 2, we aim to automatically annotate their topics, i.e., predicting the\nprimary subject area of each ARXIV paper.\nGraph. We extract 121M academic papers in English from MAG (version: 2020-11-23) to construct\na heterogeneous academic graph. The resultant paper set is written by 122M author entities, who\nare af\ufb01liated with 26K institutes. Among these papers, there are 1.3 billion citation links captured\nby MAG. Each paper is associated with its natural language title and most papers\u2019 abstracts are\nalso available. We concatenate the title and abstract by period and pass it to a ROBERTA sentence\nencoder (Liu et al., 2019; Reimers and Gurevych, 2019), generating a 768-dimensional vector for\neach paper node. Among the 121M paper nodes, approximately 1.4M nodes are ARXIV papers\nannotated with 153 ARXIV subject areas, e.g., cs.LG (Machine Learning). On the paper\nnodes, we attach the publication years as meta information.\nPrediction task and evaluation metric.\nThe task is to predict the primary subject areas of the\ngiven ARXIV papers, which is cast as an ordinary multi-class classi\ufb01cation problem. The metric is\nthe classi\ufb01cation accuracy.\nTo understand the relation between the prediction task and the heterogeneous graph structure, we\nanalyze the graph homophily (McPherson et al., 2001)\u2014tendency of two adjacent nodes to share\nthe same labels\u2014to better understand the interplay between heterogeneous graph connectivity and\nthe prediction task. Homophily is normally analyzed over a homogeneous graph, but we extend the\nanalysis to the heterogenous graph by considering meta-paths (Sun et al., 2011)\u2014a path consisting\nof a sequence of relations de\ufb01ned between different node types. Given a meta-path, we can say\ntwo nodes are adjacent if they are connected by the meta-path. Table 3 shows the homophily for\ndifferent kinds of meta-paths with different levels of connection strength. Compared to the direct\n2See the results at https://ogb.stanford.edu/kddcup2021/results/\n3\ncitation connection (i.e., P-P), certain meta-paths (i.e., P-A-P) give rise to much higher degrees of\nhomophiliness, while other meta-paths (i.e., P-A-I-A-P) provide much less homophily. As homophily\nis the central graph property exploited by many graph ML models, we believe that discovering\nessential heterogeneous connectivity is important to achieve good performance on this dataset.\nDataset split. We split the data according to time. Speci\ufb01cally, we train models on ARXIV papers\npublished until 2018, validate the performance on the 2019 papers, and \ufb01nally test the performance\non the 2020 papers. The split re\ufb02ects the practical scenario of helping the authors and moderators\nannotate the subject areas of the newly-published ARXIV papers.\nBaseline.\nWe benchmark a broad range of graph ML models in both homogeneous (where only\npaper to paper relations are considered) and full heterogeneous settings. For both settings, we\nconvert the directed graph into an undirected graph for simplicity. First, for the homogeneous\nsetting, we benchmark the simple baseline models: graph-agnostic MLP, Label Propagation, and\nthe recently-proposed simpli\ufb01ed graph methods: SGC (Wu et al., 2019), SIGN (Rossi et al., 2020)\nand MLP+C&S (Huang et al., 2020), which are inherently scalable by decoupling predictions from\npropagation. Furthermore, we benchmark state-of-the-art expressive GNNs trained with neighborhood\nsampling (NS) (Hamilton et al., 2017), where we recursively sample 25 neighbors in the \ufb01rst layer\nand 15 neighbors in the second layer during training time. At inference time, we sample at most\n160 neighbors for each layer. Here, we benchmark two types of strong models: the GRAPHSAGE\n(Hamilton et al., 2017) model (performing mean aggregation and utilizing skip-connections), and\nthe more advanced GRAPH ATTENTION NETWORK (GAT) model (Velickovic et al., 2018). For\nthe full heterogeneous setting, we follow Schlichtkrull et al. (2018) and learn distinct weights for\neach individual relation type (denoted by R-GRAPHSAGE and R-GAT, where \u201cR\u201d stands for\n\u201cRelational\u201d). We obtain the input features of authors and institutions by averaging the features of\npapers belonging to the same author and institution, respectively. The models are trained with NS.\nWe note that the expressive GNNs trained with NS require more efforts to scale up, but are more\nexpressive than the simple baselines.\nHyper-parameters.\nHyper-parameters are selected based on their best validation performance.\nFor all the models without NS, we tuned the hidden dimensionality \u2208{128, 256, 512, 1024}, MLP\ndepth \u2208{1, 2, 3, 4}, dropout ratio \u2208{0, 0.25, 0.5}, propagation layers (for SGC, SIGN, and C&S)\n\u2208{2, 3}. For all the GNN models with NS, we use a hidden dimensionality of 1024. We make use of\nbatch normalization (Ioffe and Szegedy, 2015) and ReLU activation in all models.\nDiscussion. Validation and test performances of all models considered are shown in Table 2. First,\nthe graph-agnostic MLP and Label Propagation algorithm perform poorly, indicating that both graph\nstructure and feature information are indeed important for the given task. Across the graph ML\nmodels operating on the homogeneous paper graph, GNNs with NS perform the best, with slight\ngains compared to their simpli\ufb01ed versions. In particular, the advanced expressive graph attention\naggregation is favourable compared to the uniform mean aggregation in GRAPHSAGE. Furthermore,\nconsidering all available heterogeneous relational structure in the heterogeneous graph setting yields\nsigni\ufb01cant improvements, with performance gains up to 3 percentage points. Again, the advanced\nattention aggregation provides favorable performance. Overall, our experiments highlight the bene\ufb01ts\nof developing and evaluating advanced expressive models on the larger scale.\nKDD Cup 2021 summary.\nIn Table 2, we show the results of the top 3 winners of the KDD\nCup: BD-PGL Team (Shi et al., 2021), Academic Team (Addanki et al., 2021), and Synerise AI\nTeam (Daniluk et al., 2021). All the solutions outperform our baselines signi\ufb01cantly, yielding 5\u20136%\ngain in test accuracy. For a more direct comparison, with a single model (no model ensembling), the\nBD-PGL Team reports a validation accuracy of 73.71% (Shi et al., 2021), improving our best R-GAT\nbaseline by 3.7%.\nNotably, all the winning solutions used the target labels as input to their models, which allows\nthe models to propagate labels together with the features. Regarding the GNN architectures, the\nBD-PGL adopted the expressive Transformer-based UniMP architecture (Shi et al., 2020), while\nthe Academic adopted the standard MPNN (Gilmer et al., 2017) but trained it with self-supervised\ncontrastive learning on unlabeled paper nodes (Thakoor et al., 2021). These results suggest that\nexpressive GNNs are indeed promising for this dataset. Finally, both the BD-PGL and Academic\nteams exploited the temporal aspect of the academic graph by using the publication years either\nas input positional encoding (Shi et al., 2021) or as a way to sample mini-batch subgraphs for\n4\nPaper\n121,751,666 nodes\nAuthor\n122,383,112 nodes\nInstitution\n25,721 nodes\naffiliated with\n44,592,586 edges\nwrites\n386,022,720 edges\ncites\n1,297,748,926 edges\nFigure 2: A schema diagram of MAG240M.\nTable 2: Results of MAG240M measured by\nthe accuracy (%).\nModel\n#Params Validation Test\nMLP\n0.5M\n52.67\n52.73\nLABELPROP\n0\n58.44\n56.29\nSGC\n0.7M\n65.82\n65.29\nSIGN\n3.8M\n66.64\n66.09\nMLP+C&S\n0.5M\n66.98\n66.18\nGRAPHSAGE (NS)\n4.9M\n66.79\n66.28\nGAT (NS)\n4.9M\n67.15\n66.80\nR-GRAPHSAGE (NS)\n12.2M\n69.86\n68.94\nR-GAT (NS)\n12.3M\n70.02\n69.42\nKDD 1ST: BD-PGL\n75.49\nKDD 2ND: ACADEMIC\n75.19\nKDD 3RD: SYNERISE AI\n74.60\nTable 3: Analysis of graph homophily for differ-\nent meta-paths connecting 1,251,341 arXiv pa-\npers (only train+validation). Connection strength\nindicates the number of different possible paths\nalong the template meta-path, e.g., meta-path \u201cPaper-\nAuthor-Paper (P-A-P)\u201d with connection strength 3\nmeans that at least 3 authors are shared for the two\npapers of interest. Homophily ratio is the ratio of\ntwo nodes having the same target labels.\nMeta-path\nConnect.\nHomophily\n#Edges\nstrength\nratio (%)\nP-P\n1\n57.80\n2,017,844\nP-A-P\n1\n46.12\n88,099,071\n2\n57.02\n12,557,765\n4\n64.03\n1,970,761\n8\n66.65\n476,792\n16\n70.46\n189,493\nP-A-I-A-P\n1\n3.83\n159,884,165,669\n2\n4.61\n81,949,449,717\n4\n5.69\n33,764,809,381\n8\n6.85\n12,390,929,118\n16\n7.70\n4,471,932,097\nAll pairs\n0\n1.99\n782,926,523,470\nGNNs (Addanki et al., 2021). As real-world large-scale graphs are almost always dynamic, exploiting\nthe temporal information is a promising direction of future research.\n2.2\nWikiKG90M: Link-Level Prediction\nPractical relevance and dataset overview. Large encyclopedic Knowledge Graphs (KGs), such as\nWikidata (Vrande\u02c7ci\u00b4c and Kr\u00a8otzsch, 2014) and Freebase (Bollacker et al., 2008), represent factual\nknowledge about the world through triplets connecting different entities, e.g., Hinton\ncitizen of\n\u2212\u2212\u2212\u2212\u2212\u2212\u2192\nCanada. They provide rich structured information about many entities, aiding a variety of knowledge-\nintensive downstream applications such as information retrieval, question answering (Singhal, 2012),\nand recommender systems (Guo et al., 2020). However, these large KGs are known to be far from\ncomplete (Min et al., 2013), missing many relational information between entities.\nWikiKG90M is a Knowledge Graph (KG) extracted from the entire Wikidata knowledge base. The\ntask is to automatically impute missing triplets that are not yet present in the current KG. Accurate\nimputation models can be readily deployed on the Wikidata to improve its coverage.\nGraph. Each triplet (head, relation, tail) in WikiKG90M represents an Wikidata claim, where head\nand tail are the Wikidata items, and relation is the Wikidata predicate. We extracted triplets from the\npublic Wikidata dump downloaded at three time-stamps: September 28, October 26, and November\n23 of 2020, for training, validation, and testing, respectively. We retain all the entities and relations in\nthe September dump, resulting in 87,143,637 entities, 1,315 relations, and 504,220,369 triplets in\ntotal.\nIn addition to extracting triplets, we provide text features for entities and relations. Speci\ufb01cally,\neach entity/relation in Wikidata is associated with a title and a short description, e.g., one entity is\nassociated with the title \u2018Geoffrey Hinton\u2018 and the description \u2018computer scientist and psychologist\u2018.\nSimilar to MAG240M, we provide ROBERTA embeddings (Reimers and Gurevych, 2019; Liu et al.,\n2019) as node and edge features.3\n3We concatenate the title and description with comma, e.g., \u2018Geoffrey Hinton, computer scientist and\npsychologist\u2018, and pass the sentence to a ROBERTA sentence encoder (Note that the ROBERTA model was\ntrained before September 2020, so there is no obvious information leak). The title or/and description are\nsometimes missing, in which case we simply use the blank sentence to replace it.\n5\nPrediction task and evaluation metric. The task is the KG completion, i.e., given a set of training\ntriplets, predict a set of test triplets. For evaluation, we follow the protocol similar to how KG\ncompletion is evaluated (Bordes et al., 2013). Speci\ufb01cally, for each validation/test triplet, (head,\nrelation, tail), we corrupt tail with randomly-sampled 1000 negative entities, e.g., tail neg, such that\n(head, relation, tail neg) does not appear in the train/validation/test KG. The model is asked to rank\nthe 1001 candidates (consisting of 1 positive and 1000 negatives) for each triplet and predict the top\n10 entities that are most likely to be positive. The goal is to rank the ground-truth positive entity as\nhigh in the rank as possible, which is measured by Mean Reciprocal Rank (MRR). 4\nDataset split. We split the triplets according to time, simulating a realistic KG completion scenario\nof imputing missing triplets not present at a certain timestamp. Speci\ufb01cally, we construct three KGs\nusing the aforementioned September, October, and November KGs, where we only retain entities and\nrelation types that appear in the earliest September KG. We use the triplets in the September KG for\ntraining, and use the additional triplets in the October and November KGs for validation and test,\nrespectively.\nWe analyze the effect of the time split. We \ufb01nd that head entities of validation triplets tend to be less\npopular entities; on average, they only have 6.5 out-degrees in the training KG, which is less than a\nquarter of the out-degree averaged over training triplets (i.e., 28.0). This suggests that learning signals\nfor predicting validation (and test) triplets are sparse. Nonetheless, even for the sparsely-connected\ntriplets, we \ufb01nd the textual information provides important clues, as illustrated in Table 4. Hence,\nwe expect that advanced graph models that effectively incorporate textual information will be key to\nachieve good performance on the challenging time split.\nBaseline. We consider two representative KG embedding models: TRANSE (Bordes et al., 2013)\nand COMPLEX (Trouillon et al., 2016). These models de\ufb01ne their own decoders to score knowledge\ntriplets using the corresponding entity and relation embeddings. For instance, TRANSE uses \u2212\u2225h +\nr \u2212t\u22252 as the decoder, where h, r, and t are embeddings of head, relation, and tail, respectively. For\nthe encoder function (mapping each entity and relation to its embedding), we consider the following\nthree options. Shallow: We use the distinct embedding for each entity and relation, as normally\ndone in KG embedding models. RoBERTa: We use two MLP encoders (one for entity and another\nfor relation) that transform the ROBERTA features into entity and relation embeddings. Concat:\nTo enhance the expressive power of the previous encoder, we concatenate the shallow learnable\nembeddings into the ROBERTA features, and use the MLPs to transform the concatenated vectors to\nget the \ufb01nal embeddings. This way, the MLP encoders can adaptively utilize the ROBERTA features\nand the shallow embeddings to \ufb01t the large amount of triplet data. To implement our baselines, we\nutilize DGL-KE (Zheng et al., 2020).\nHyper-parameters. For the loss function, we use the negative sampling loss from Sun et al. (2019),\nwhere we pick margin \u03b3 from {1,4,8,10,100}. In order to balance the performance and the memory\ncost, we use the embedding dimensionality of 200 for all the models.\nDiscussion.\nTable 5 shows the validation and test performance of the six different models, i.e.,\ncombination of two decoders (TRANSE and COMPLEX) and three encoders (SHALLOW, ROBERTA,\nand CONCAT). Notably, in terms of the encoders, we see that the most expressive CONCAT outper-\nforms both SHALLOW and ROBERTA, indicating that both the textual information (captured by the\nROBERTA embeddings) and structural information (captured by node-wise learnable embeddings)\nare useful in predicting validation and test triplets. In terms of the decoders, TRANSE and COMPLEX\nshow similar performance with the CONCAT encoder, while they show somewhat mixed results with\nthe SHALLOW and ROBERTA encoders.\nOverall, our experiments suggest that the expressive encoder that combines both textual information\nand structural information gives the most promising performance. In the KG completion literature,\nthe design of the encoder has been much less studied compared to the decoder designs. There-\nfore, we believe there is a huge opportunity in scaling up more advanced encoders, especially\nGNNs (Schlichtkrull et al., 2018), to further improve the performance on this dataset.\nKDD Cup 2021 summary.\nTable 5 shows the results of the top 3 winners of the KDD Cup:\nBD-PGL Team (Su et al., 2021), OhMyGod Team (Peng et al., 2021), and the GraphMIRAcles\nTeam (Cai et al., 2021). All the winning solutions outperform our strong baselines signi\ufb01cantly,\n4Note that this is more strict than the standard MRR since there is no partial score for positive entities being\nranked outside of top 10.\n6\nTable 4:\nTextual representation of validation\ntriplets whose head entities only appear once as\nhead in the training WikiKG90M.\nHead\nRelation\nTail\nFood and drink companies of Bulgaria combines topics Bulgaria\nPerforming arts in Denmark\ncombines topics performing arts\nAnglicanism in Grenada\ncombines topics Anglicanism\nChuan Li\noccupation\nresearcher\nPetra Junkova\ngiven name\nPetra\nTable 5: Results of WikiKG90M measured\nby Mean Reciprocal Rank (MRR).\nModel\n#Params Validation\nTest\nTRANSE-SHALLOW\n17.4B\n0.7559\n0.7412\nCOMPLEX-SHALLOW\n17.4B\n0.6142\n0.5883\nTRANSE-ROBERTA\n0.3M\n0.6039\n0.6288\nCOMPLEX-ROBERTA\n0.3M\n0.7052\n0.7186\nTRANSE-CONCAT\n17.4B\n0.8494\n0.8548\nCOMPLEX-CONCAT\n17.4B\n0.8425\n0.8637\nKDD 1ST: BD-PGL\n0.9727\nKDD 2ND: OHMYGOD\n0.9712\nKDD 3RD: GRAPHMIRACLES\n0.9707\nachieving near-perfect test MRR score of 0.97. For a more direct comparison, with a single model (no\nmodel ensembling), the BD-PGL Team reports a validation MRR of 0.92 (Su et al., 2021), improving\nour best COMPLEX-CONCAT baseline by 0.07 points in validation MRR. Similar to our baselines, all\nthe winners utilize the KG embedding approach as the backbone, and adopt the encoder that takes\nboth shallow embedding and textual embeddings into account. Speci\ufb01cally, BD-PGL proposed the\nNOTE model (Su et al., 2021) which makes the ROTATE model more expressive, while OhMyGod\nadopted the ensemble of several existing KG embedding models. On the other hand, GraphMIRAcles\nexplored different design choices for the encoder and found that adding residual connection for\nshallow embeddings signi\ufb01cantly improved the model performance.\nIn addition to the model advances, all the winners exploited some statistical property of candidate\ntail entities. Most notably, Yang et al. (2021) found that simply by sorting the candidate tails by the\nfrequency they appear in the training KG, it was possible to achieve validation MRR of 0.75, rivaling\nour TRANSE-SHALLOW baseline. This highlights that our negative tail candidates are mostly rare\nentities that can be easily distinguished from the true tail entity. On the other hand, the practical KG\ncompletion presents a much harder challenge: the candidate tails are not provided, and a model needs\nto predict the true tail entity out of all the possible 87M entities. As the performance on WikiKG90M\nhas already saturated, we have updated WikiKG90M to WikiKG90Mv2 to re\ufb02ect the realistic setting\nin large-scale KG completion. See Section 3 for further details.\n2.3\nPCQM4M: Graph-Level Prediction\nPractical relevance and dataset overview. Density Functional Theory (DFT) is a powerful and\nwidely-used quantum physics calculation that can accurately predict various molecular properties such\nas the shape of molecules, reactivity, responses by electromagnetic \ufb01elds (Burke, 2012). However,\nDFT is time-consuming and takes up to several hours per small molecule. Using fast and accurate ML\nmodels to approximate DFT enables diverse downstream applications, such as property prediction\nfor organic photovaltaic devices (Cao and Xue, 2014) and structure-based virtual screening for drug\ndiscovery (Ferreira et al., 2015).\nPCQM4M is a quantum chemistry dataset originally curated under the PubChemQC project (Nakata,\n2015; Nakata and Shimazaki, 2017). Based on the PubChemQC, we de\ufb01ne a meaningful ML task\nof predicting DFT-calculated HOMO-LUMO energy gap of molecules given their 2D molecular\ngraphs. The HOMO-LUMO gap is one of the most practically-relevant quantum chemical properties\nof molecules since it is related to reactivity, photoexcitation, and charge transport (Grif\ufb01th and Orgel,\n1957). Moreover, predicting the quantum chemical property only from 2D molecular graphs without\ntheir 3D equilibrium structures is also practically favorable. This is because obtaining 3D equilibrium\nstructures requires DFT-based geometry optimization, which is expensive on its own.\nTo ensure the resulting models are practically useful, we limit the average inference budget per\nmolecule (including both pre-processing and model inference) to be less than 0.1 second using a\nsingle GPU and CPU (multi-threading on a multi-core CPU is allowed). This means that expensive\n(quantum) calculations cannot be used to perform inference. As our test set contains 377,423\nmolecules, we require the all the prediction to be made within 12 hours. Note that this time constraint\nis quite generous for ordinary GNNs\u2014each of our baseline GNN only took about 3 minutes to\nperform inference over the entire test data.\n7\nGraph. We provide molecules as the SMILES strings (Weininger, 1988), from which 2D molecule\ngraphs (nodes are atoms and edges are chemical bonds) as well as molecular \ufb01ngerprints (hand-\nengineered molecular feature developed by the chemistry community) can be obtained. By default, we\nfollow OGB (Hu et al., 2020a) to convert the SMILES string into a molecular graph representation,\nwhere each node is associated with a 9-dimensional feature (e.g., atomic number, chirality) and\neach edge comes with a 3-dimensional feature (e.g., bond type, bond stereochemistry), although the\noptimal set of input graph features remains to be explored.\nPrediction task and evaluation metric.\nThe task is graph regression: predicting the HOMO-\nLUMO energy gap in electronvolt (eV) given 2D molecular graphs. Mean Absolute Error (MAE) is\nused as evaluation metric.\nDataset split. We split molecules by their PubChem ID (CID) with ratio 80/10/10. Our original\nintention was to provide the scaffold split (Hu et al., 2020a; Wu et al., 2018), but the provided\ndata turns out to be split by the CID due to some pre-processing bug. The CID number itself does\nnot indicate particular meaning about the molecule, but splitting by CID may provide a moderate\ndistribution shift (most likely not as severe as the scaffold split). We empirically compared the CID\nand scaffold splits and found the model performances were consistent between the two splits.5\nBaseline. We benchmark two types of models: a simple MLP over the Morgan \ufb01ngerprint (Morgan,\n1965) and more advanced GNN models. For GNNs, we use the four strong models developed\nfor graph-level prediction: Graph Convolutional Network (GCN) (Kipf and Welling, 2017) and\nGraph Isomorphism Network (GIN) (Xu et al., 2019), as well as their variants, GCN-VIRTUAL and\nGIN-VIRTUAL, which augment graphs with a virtual node that is bidirectionally connected to all\nnodes in the original graph (Gilmer et al., 2017). Adding the virtual node is shown to be effective\nacross a wide range of graph-level prediction datasets in OGB (Hu et al., 2020a). Edge features are\nincorporated following Hu et al. (2020b). At inference time, the model output is clamped between 0\nand 50 to avoid model\u2019s anomalously large/small prediction.\nHyper-parameters. For the MLP over Morgan \ufb01ngerprint, we set the \ufb01ngerprint dimensionality\nto be 2048, and tune the \ufb01ngerprint radius \u2208{2, 3}, as well as MLP\u2019s hyper-parameters: hidden\ndimensionality \u2208{1200, 1600}, number of hidden layers \u2208{2, 4, 6}, and dropout ratio \u2208{0, 0.2}.\nFor GNNs, we tune hidden dimensionality, i.e., width \u2208{300, 600}, number of GNN layers, i.e.,\ndepth \u2208{3, 5}. Simple summation is used for graph-level pooling. For all MLPs (including GIN\u2019s),\nwe use batch normalization (Ioffe and Szegedy, 2015) and ReLU activation.\nDiscussion.\nThe validation and test results are shown in Table 6. We see both the GNN models\nsigni\ufb01cantly outperform the simple \ufb01ngerprint baseline. Expressive GNNs (GIN and GIN-VIRTUAL)\noutperform less expressive ones (GCN and GCN-VIRTUAL); especially, the most advanced and\nexpressive GIN-VIRTUAL model signi\ufb01cantly outperforms the other GNNs. Nonetheless, the current\nperformance is still much worse than the chemical accuracy of 0.043eV\u2014an indicator of practical\nusefulness established by the chemistry community. In the same Table 6, we show our ablation, where\nwe use only 10% of data to train the GIN-VIRTUAL model. We see the performance signi\ufb01cantly\ndeteriorate, indicating the importance of training the model on large data. Finally, in Table 7, we\nshow the relation between model sizes and validation performance. We see that the largest models\nalways achieve the best performance.\nOverall, we \ufb01nd that advanced, expressive, and large GNN model gives the most promising perfor-\nmance on the PCQM4M dataset, although the performance still needs to be improved for practical use.\nWe believe further advances in advanced modeling, expressive architectures, and larger model sizes\ncould yield breakthrough in the large-scale molecular property prediction task.\nKDD Cup 2021 summary.\nIn Table 6, we show the results of the top 3 winners of the KDD\nCup: Machine Learning Team (Ying et al., 2021b), SuperHelix Team (Zhang et al., 2021), and\nQuantum Team (Addanki et al., 2021). The winners have signi\ufb01cantly reduced the MAE compared\nour baselines, yielding around 0.03 points improvement in test MAE. For a more direct comparison,\nwith a single model, the Machine Learning reports the validation MAE of 0.097 for their Graphormer\nmodel (Ying et al., 2021a), which is 0.04 points lower than our best GIN-VIRTUAL baseline.\n5Detailed\ndiscussion\ncan\nbe\nfound\nat\nhttps://github.com/snap-stanford/ogb/\ndiscussions/162\n8\nTable 6: Results of PCQM4M measured by MAE\n[eV]. The lower, the better. Ablation study of using\nonly 10% of training data is also shown. Chemi-\ncal accuracy indicates the \ufb01nal goal for practical\nusefulness.\nModel\n#Params Validation Test\nMLP-FINGERPRINT\n16.1M\n0.2044\n0.2070\nGCN\n2.0M\n0.1684\n0.1842\nGCN-VIRTUAL\n4.9M\n0.1510\n0.1580\nGIN\n3.8M\n0.1536\n0.1685\nGIN-VIRTUAL\n6.7M\n0.1396\n0.1494\nMLP-FINGERPRINT (10% train)\n6.8M\n0.2708\n0.2659\nGIN-VIRTUAL (10% train)\n6.7M\n0.1790\n0.1892\nKDD 1ST: MACHINELEARNING\n0.1208\nKDD 2ND: SUPERHELIX\n0.1210\nKDD 3RD: QUANTUM\n0.1211\nChemical accuracy (goal)\n\u2013\n0.0430\nTable 7: Model size and the MAE perfor-\nmance [eV]. For both models, the width in-\ndicates the hidden dimensionality. For GIN-\nVIRTUAL, the depth represents the number of\nGNN layers, while for the MLP-FINGERPRINT,\nthe depth represents the the number of hidden\nlayers in MLP.\nModel\nWidth Depth #Params Validation\nMLP-FINGERPRINT\n1600\n6\n16.1M\n0.2044\n1600\n4\n11.0M\n0.2044\n1600\n2\n5.8M\n0.2220\n1200\n6\n9.7M\n0.2083\nGIN-VIRTUAL\n600\n5\n6.7M\n0.1410\n600\n3\n3.7M\n0.1462\n300\n5\n1.7M\n0.1442\n300\n3\n1.0M\n0.1512\nIn terms of methodology, we \ufb01nd that the winning solutions share three important components\nin common. (1) Their winning GNN models are indeed large and deep; the number of learnable\nparameters (single model) ranges from 50M up to 450M, while the number of GNN layers ranges\nfrom 11 up to 50, being signi\ufb01cantly larger than our baseline models. (2) All the GNNs perform\nglobal message passing at each layer, either through the virtual nodes (Gilmer et al., 2017) or\nfully-connected Transformer-style self-attention (Ying et al., 2021a). (3) All the winners utilize 3D\nstructure of molecules to supervise their GNNs. As 3D structure was not provided at our KDD Cup,\nthe winners generate the 3D structure themselves using RDkit (Landrum et al., 2006) or PySCF (Sun\net al., 2020), both of which provide cheap but less accurate 3D structure of molecules.\nAs modeling 3D molecular graphs is a promising direction in graph ML (Sch\u00a8utt et al., 2017; Klicpera\net al., 2020; Sanchez-Gonzalez et al., 2020; Hu et al., 2021), we have updated PCQM4M to PCQM4Mv2\nto include DFT-calculated 3D structures for training molecules. Details are provided in Section 3.\n3\nUpdates after the KDD Cup\nTo facilitate further research advances, we have updated the datasets and leaderboards based on the\nlessons learned from our KDD Cup 2021. Here we brie\ufb02y describe our updates. More details are\nprovided in Appendix C.\nUpdates on WikiKG90M.\nFrom the KDD Cup results, we learned that most of our provided\nnegative entities in the large-scale WikiKG90M are \u201ceasy negatives\u201d, and our current task gives\noverly-optimistic performance scores. In a realistic large-scale KG completion setting, ML models\nare required to predict the true tail entity from nearly 90M entities, which is much more challenging.\nTo re\ufb02ect this challenge, we have updated WikiKG90M to WikiKG90Mv2, where we do not provide\nany candidate entities for validation/test triples. Our initial experiments using the same set of baseline\nmodels, shows that WikiKG90Mv2 indeed provides a much harder challenge; our best model\nCOMPLEX-CONCAT only achieves 0.1833 MRR on WikiKG90Mv2 as opposed to achieving 0.8637\nMRR on WikiKG90M, leaving signi\ufb01cant room for further improvement.\nUpdates on PCQM4M.\nFrom the KDD Cup results, we saw that the winners effectively utilized\n(self-calculated) 3D structure of molecules. Modeling molecular graphs in 3D space is of great\ninterest to the graph ML community; We therefore have updated PCQM4M to PCQM4Mv2, where we\nprovide DFT-calculated 3D structure for training molecules. For validation and test molecules, 3D\nstructures is not be provided, and ML models still need to make prediction based on the 2D molecular\ngraphs. In updating to PCQM4Mv2, we are also \ufb01xing subtle but important mismatch between some\nof the 2D molecular graphs and the corresponding 3D molecular graphs. Our preliminary experiments\non PCQM4Mv2 suggest that the all the baseline models\u2019 MAE is improved by \u22480.04 [eV] compared\nto PCQM4M, although the trends in model performance stay almost the same as PCQM4M.\n9\nUpdates on leaderboards.\nWe are introducing public leaderboards to facilitate further research\nadvances after our KDD Cup. The test submissions of the KDD Cup 2021 were evaluated on the\nentire hidden test set. After the KDD Cup, we are randomly splitting the test set into two: \u201ctest-dev\u201d\nand \u201ctest-challenge\u201d. The test-dev set is be used for public leaderboards that evaluate test submissions\nany time during a year. The test-challenge set is be left for future competitions, which we plan to\nhold annually to facilitate community engagement. The leaderboards have been released together\nwith the updated datasets.\n4\nConclusions\nModern applications of graph ML involve large-scale graph data with billions of edges or millions of\ngraphs. ML advances on large graph data have been limited due to the lack of a suitable benchmark.\nHere we present OGB-LSC, with the goal of advancing state-of-the-art in large-scale graph ML.\nOGB-LSC provides the three large-scale realistic benchmark datasets, covering the core graph ML\ntasks of node classi\ufb01cation, link prediction, and graph regression. We perform dedicated baseline\nanalysis, scaling up advanced graph models to large graphs. We show that advanced and expressive\nmodels can signi\ufb01cantly outperform simpler baseline models, suggesting opportunities for further\ndedicated effort to yield even better performance.\nWe used our datasets for the recent ACM KDD Cup 2021, where we have attracted huge engagement\nfrom the community and have already witnessed signi\ufb01cant performance improvement. We summa-\nrize the winning solutions for each dataset, highliting the current best practices in large-scale graph\nML. Finally, we describe how we have updated our datasets after the KDD Cup to further facilitate\nresearch advances. Overall, we hope OGB-LSC encourages dedicated community efforts to tackle\nthe important but challenging problem of large-scale graph ML.\nAcknowledgement\nWe thank Michele Catasta and Larry Zitnick for helpful discussion, Shigeru Maya for motivating the\nproject, Adrijan Bradaschia for setting up the server for the project, and Amit Bleiweiss, Benjamin\nBraun and Hanjun Dai for providing helpful feedback on our baseline code, and the DGL Team for\nhosting our large datasets.\nStanford University is supported by DARPA under Nos. N660011924033 (MCS); ARO under\nNos. W911NF-16-1-0342 (MURI), W911NF-16-1-0171 (DURIP); NSF under Nos. OAC-1835598\n(CINES), OAC-1934578 (HDR), CCF-1918940 (Expeditions), IIS-2030477 (RAPID); Stanford Data\nScience Initiative, Wu Tsai Neurosciences Institute, Chan Zuckerberg Biohub, Amazon, JPMor-\ngan Chase, Docomo, Hitachi, JD.com, KDDI, NVIDIA, Dell, Toshiba, Intel, and UnitedHealth\nGroup. Weihua Hu is supported by Funai Overseas Scholarship and Masason Foundation Fellowship.\nMatthias Fey is supported by the German Research Association (DFG) within the Collaborative\nResearch Center SFB 876 \u201cProviding Information by Resource-Constrained Analysis\u201d, project A6.\nHongyu Ren is supported by Masason Foundation Fellowship and Apple PhD Fellowship. Jure\nLeskovec is a Chan Zuckerberg Biohub investigator.\nOur baseline code and Python package are built on top of excellent open-source software, including\nNUMPY (Harris et al., 2020), PYTORCH (Paszke et al., 2017), PYTORCH GEOMETRIC (Fey and\nLenssen, 2019), DGL (Wang et al., 2019), and DGL-KE (Zheng et al., 2020).\nThe HOKUSAI facility was used to perform some of the quantum calculations. This work was\nsupported by the Japan Society for the Promotion of Science (JSPS KAKENHI Grant no. 18H03206).\nWe are also grateful to Maeda Toshiyuki for helpful discussions.\nReferences\nRavichandra Addanki, Peter W Battaglia, David Budden, Andreea Deac, Jonathan Godwin, Thomas\nKeck, Wai Lok Sibon Li, Alvaro Sanchez-Gonzalez, Jacklynn Stott, Shantanu Thakoor, et al.\nLarge-scale graph representation learning with very deep gnns and self-supervision. arXiv preprint\narXiv:2107.09422, 2021.\n10\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. Freebase: a collabo-\nratively created graph database for structuring human knowledge. In Special Interest Group on\nManagement of Data (SIGMOD), pages 1247\u20131250. AcM, 2008.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko.\nTranslating embeddings for modeling multi-relational data. In Advances in Neural Information\nProcessing Systems (NeurIPS), pages 2787\u20132795, 2013.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\nK. Burke. Perspective on density functional theory. J. Chem. Phys., 136:150901, 2012. URL\nhttp://link.aip.org/link/doi/10.1063/1.4704546.\nJianyu Cai, Jiajun Chen, Taoxing Pan, Zhanqiu Zhang, and Jie Wang. Technical report of team\ngraphmiracles in the wikikg90m-lsc track of ogb-lsc@ kdd cup 2021. 2021.\nWeiran Cao and Jiangeng Xue. Recent progress in organic photovoltaics: Device architecture and\noptical design. Energy Environ. Sci., 7:2123\u20132144, 2014. doi: 10.1039/C4EE00260A. URL\nhttp://dx.doi.org/10.1039/C4EE00260A.\nLowik Chanussot, Abhishek Das, Siddharth Goyal, Thibaut Lavril, Muhammed Shuaibi, Morgane\nRiviere, Kevin Tran, Javier Heras-Domingo, Caleb Ho, Weihua Hu, Aini Palizhati, Anuroop\nSriram, Brandon Wood, Junwoong Yoon, Devi Parikh, C. Lawrence Zitnick, and Zachary Ulissi.\nOpen catalyst 2020 (oc20) dataset and community challenges. ACS Catal., 11:6059\u20136072, 2021.\nURL https://doi.org/10.1021/acscatal.0c04525.\nJie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via\nimportance sampling. arXiv preprint arXiv:1801.10247, 2018.\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh. Cluster-GCN: An\nef\ufb01cient algorithm for training deep and large graph convolutional networks. In ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD), pages 257\u2013266, 2019.\nMicha\u0142 Daniluk, Jacek Dabrowski, Barbara Rychalska, and Konrad Go\u0142uchowski. Synerise at\nkdd cup 2021: Node classi\ufb01cation in massive heterogeneous graphs.\n2021.\nURL https:\n//ogb.stanford.edu/paper/kddcup2021/mag240m_SyneriseAI.pdf.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nYuxiao Dong, Hao Ma, Zhihong Shen, and Kuansan Wang. A century of science: Globalization of\nscienti\ufb01c collaborations, citations, and innovations. In ACM SIGKDD Conference on Knowledge\nDiscovery and Data Mining (KDD), pages 1437\u20131446. ACM, 2017.\nLeonardo G. Ferreira, Ricardo N. Dos Santos, Glaucius Oliva, and Adriano D. Andricopulo. Molec-\nular docking and structure-based drug design strategies. Molecules, 20(7):13384\u201313421, 2015.\nISSN 1420-3049.\ndoi: 10.3390/molecules200713384.\nURL https://www.mdpi.com/\n1420-3049/20/7/13384.\nMatthias Fey and Jan Eric Lenssen. Fast graph representation learning with pytorch geometric. arXiv\npreprint arXiv:1903.02428, 2019.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. In International Conference on Machine Learning (ICML),\npages 1273\u20131272, 2017.\nJS Grif\ufb01th and LE Orgel. Ligand-\ufb01eld theory. Quarterly Reviews, Chemical Society, 11(4):381\u2013393,\n1957.\nQingyu Guo, Fuzhen Zhuang, Chuan Qin, Hengshu Zhu, Xing Xie, Hui Xiong, and Qing He. A\nsurvey on knowledge graph-based recommender systems. IEEE Transactions on Knowledge and\nData Engineering, 2020.\n11\nWilliam L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nIn Advances in Neural Information Processing Systems (NeurIPS), pages 1025\u20131035, 2017.\nCharles R Harris, K Jarrod Millman, St\u00b4efan J van der Walt, Ralf Gommers, Pauli Virtanen, David\nCournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J Smith, et al. Array program-\nming with numpy. Nature, 585(7825):357\u2013362, 2020.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages\n770\u2013778, 2016.\nWeihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta,\nand Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Advances\nin Neural Information Processing Systems (NeurIPS), 2020a.\nWeihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay Pande, and Jure Leskovec.\nStrategies for pre-training graph neural networks. In International Conference on Learning\nRepresentations (ICLR), 2020b.\nWeihua Hu, Muhammed Shuaibi, Abhishek Das, Siddharth Goyal, Anuroop Sriram, Jure Leskovec,\nDevi Parikh, and C Lawrence Zitnick. Forcenet: A graph neural network for large-scale quantum\ncalculations. arXiv preprint arXiv:2103.01436, 2021.\nQian Huang, Horace He, Abhay Singh, Ser-Nam Lim, and Austin R Benson. Combining label propa-\ngation and simple models out-performs graph neural networks. arXiv preprint arXiv:2010.13993,\n2020.\nSergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by\nreducing internal covariate shift. In International Conference on Machine Learning (ICML), pages\n448\u2013456, 2015.\nThomas N. Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional networks.\nIn International Conference on Learning Representations (ICLR), 2017.\nJon M Kleinberg. Authoritative sources in a hyperlinked environment. Journal of the ACM, 46(5):\n604\u2013632, 1999.\nJohannes Klicpera, Shankari Giri, Johannes T. Margraf, and Stephan G\u00a8unnemann.\nFast and\nuncertainty-aware directional message passing for non-equilibrium molecules. In NeurIPS-W,\n2020.\nGreg Landrum et al. Rdkit: Open-source cheminformatics, 2006.\nJure Leskovec and Rok Sosi\u02c7c. Snap: A general-purpose network analysis and graph-mining library.\nACM Transactions on Intelligent Systems and Technology (TIST), 8(1):1\u201320, 2016.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr\nDoll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In eccv, pages\n740\u2013755. Springer, 2014.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nMiller McPherson, Lynn Smith-Lovin, and James M Cook. Birds of a feather: Homophily in social\nnetworks. Annual review of sociology, 27(1):415\u2013444, 2001.\nBonan Min, Ralph Grishman, Li Wan, Chang Wang, and David Gondek. Distant supervision\nfor relation extraction with an incomplete knowledge base. In North American Chapter of the\nAssociation for Computational Linguistics (NAACL), pages 777\u2013782, 2013.\nHarry L Morgan. The generation of a unique machine description for chemical structures-a technique\ndeveloped at chemical abstracts service. Journal of Chemical Documentation, 5(2):107\u2013113, 1965.\n12\nChristopher Morris, Nils M Kriege, Franka Bause, Kristian Kersting, Petra Mutzel, and Marion\nNeumann. Tudataset: A collection of benchmark datasets for learning with graphs. arXiv preprint\narXiv:2007.08663, 2020.\nMaho Nakata. the PubChemQC Project: A Large Chemical Database from the First Principle\nCalculations. AIP Conf. Proc., 1702:090058, 2015. doi: 10.1186/1758-2946-3-4. URL http:\n//dx.doi.org/10.1063/1.4938866.\nMaho Nakata and Tomomi Shimazaki. Pubchemqc project: A large-scale \ufb01rst-principles electronic\nstructure database for data-driven chemistry. Journal of chemical information and modeling, 57(6):\n1300\u20131308, 2017.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. In NIPS-W, 2017.\nWeihua Peng, Donghai Bian, Yanhui Huang, Guangzhi Sheng, and Jian Sun. Technical report\nof wikikg90m-lsc. 2021. URL https://ogb.stanford.edu/paper/kddcup2021/\nwikikg90m_OhMyGod.pdf.\nNils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks.\n11 2019. URL https://arxiv.org/abs/1908.10084.\nEmanuele Rossi, Fabrizio Frasca, Ben Chamberlain, Davide Eynard, Michael Bronstein, and Federico\nMonti. Sign: Scalable inception graph neural networks. arXiv preprint arXiv:2004.11198, 2020.\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition\nchallenge. International journal of computer vision, 115(3):211\u2013252, 2015.\nAlvaro Sanchez-Gonzalez, Jonathan Godwin, Tobias Pfaff, Rex Ying, Jure Leskovec, and Peter W\nBattaglia. Learning to simulate complex physics with graph networks. In International Conference\non Machine Learning (ICML), 2020.\nMichael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max\nWelling. Modeling relational data with graph convolutional networks. In European Semantic Web\nConference, pages 593\u2013607. Springer, 2018.\nKristof Sch\u00a8utt, Pieter-Jan Kindermans, Huziel Enoc Sauceda Felix, Stefan Chmiela, Alexandre\nTkatchenko, and Klaus-Robert M\u00a8uller. Schnet: A continuous-\ufb01lter convolutional neural network\nfor modeling quantum interactions. In Advances in Neural Information Processing Systems\n(NeurIPS), pages 991\u20131001, 2017.\nYunsheng Shi, Zhengjie Huang, Wenjin Wang, Hui Zhong, Shikun Feng, and Yu Sun. Masked label\nprediction: Uni\ufb01ed message passing model for semi-supervised classi\ufb01cation. arXiv preprint\narXiv:2009.03509, 2020.\nYunsheng Shi, PGL Team, Zhengjie Huang, Weibin Li, Weiyue Su, and Shikun Feng. Runimp: So-\nlution for kddcup 2021 mag240m-lsc. 2021. URL https://ogb.stanford.edu/paper/\nkddcup2021/mag240m_BD-PGL.pdf.\nAmit Singhal. Introducing the knowledge graph: things, not strings. Of\ufb01cial google blog, 5:16, 2012.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. Mpnet: Masked and permuted\npre-training for language understanding. arXiv preprint arXiv:2004.09297, 2020.\nWeiyue Su, Zeyang Fang, Hui Zhong, Huijuan Wang, Siming Dai, Zhengjie Huang, Yunsheng Shi,\nShikun Feng, and Zeyu Chen. Note: Solution for kdd-cup 2021 wikikg90m-lsc. arXiv preprint\narXiv:2107.01892, 2021.\nQiming Sun, Xing Zhang, Samragni Banerjee, Peng Bao, Marc Barbry, Nick S Blunt, Nikolay A\nBogdanov, George H Booth, Jia Chen, Zhi-Hao Cui, et al. Recent developments in the pyscf\nprogram package. The Journal of chemical physics, 153(2):024109, 2020.\n13\nYizhou Sun, Jiawei Han, Xifeng Yan, Philip S Yu, and Tianyi Wu. Pathsim: Meta path-based top-k\nsimilarity search in heterogeneous information networks. Proceedings of the VLDB Endowment, 4\n(11):992\u20131003, 2011.\nZhiqing Sun, Zhi-Hong Deng, Jian-Yun Nie, and Jian Tang. Rotate: Knowledge graph embedding by\nrelational rotation in complex space. In International Conference on Learning Representations\n(ICLR), 2019.\nShantanu Thakoor, Corentin Tallec, Mohammad Gheshlaghi Azar, R\u00b4emi Munos, Petar Veli\u02c7ckovi\u00b4c, and\nMichal Valko. Bootstrapped representation learning on graphs. arXiv preprint arXiv:2102.06514,\n2021.\nTh\u00b4eo Trouillon, Johannes Welbl, Sebastian Riedel, \u00b4Eric Gaussier, and Guillaume Bouchard. Complex\nembeddings for simple link prediction. In International Conference on Machine Learning (ICML),\npages 2071\u20132080, 2016.\nJohan Ugander, Brian Karrer, Lars Backstrom, and Cameron Marlow. The anatomy of the facebook\nsocial graph. arXiv preprint arXiv:1111.4503, 2011.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua\nBengio. Graph attention networks. In International Conference on Learning Representations\n(ICLR), 2018.\nDenny Vrande\u02c7ci\u00b4c and Markus Kr\u00a8otzsch. Wikidata: a free collaborative knowledgebase. Communica-\ntions of the ACM, 57(10):78\u201385, 2014.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. arXiv preprint\narXiv:1804.07461, 2018.\nKuansan Wang, Zhihong Shen, Chiyuan Huang, Chieh-Han Wu, Yuxiao Dong, and Anshul Kanakia.\nMicrosoft academic graph: When experts are not enough. Quantitative Science Studies, 1(1):\n396\u2013413, 2020.\nMinjie Wang, Lingfan Yu, Da Zheng, Quan Gan, Yu Gai, Zihao Ye, Mufei Li, Jinjing Zhou, Qi Huang,\nChao Ma, Ziyue Huang, Qipeng Guo, Hao Zhang, Haibin Lin, Junbo Zhao, Jinyang Li, Alexander J\nSmola, and Zheng Zhang. Deep graph library: Towards ef\ufb01cient and scalable deep learning on\ngraphs. ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019. URL\nhttps://arxiv.org/abs/1909.01315.\nDavid Weininger. Smiles, a chemical language and information system. 1. introduction to methodol-\nogy and encoding rules. Journal of chemical information and computer sciences, 28(1):31\u201336,\n1988.\nFelix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q\nWeinberger. Simplifying graph convolutional networks. In International Conference on Machine\nLearning (ICML), 2019.\nZhenqin Wu, Bharath Ramsundar, Evan N Feinberg, Joseph Gomes, Caleb Geniesse, Aneesh S\nPappu, Karl Leswing, and Vijay Pande. Moleculenet: a benchmark for molecular machine learning.\nChemical science, 9(2):513\u2013530, 2018.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural\nnetworks? In International Conference on Learning Representations (ICLR), 2019.\nShuo Yang, Daixin Wang, Dingyuan Zhu, Yakun Wang, and Borui Ye. Team littleant\u2019s solution of\ntask2. 2021. URL https://ogb.stanford.edu/paper/kddcup2021/wikikg90m_\nlittleant.pdf.\nZhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with\ngraph embeddings. In International Conference on Machine Learning (ICML), pages 40\u201348, 2016.\n14\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen,\nand Tie-Yan Liu. Do transformers really perform bad for graph representation? arXiv preprint\narXiv:2106.05234, 2021a.\nChengxuan Ying, Mingqi Yang, Shuxin Zheng, Guolin Ke, Shengjie Luo, Tianle Cai, Chenglin\nWu, Yuxin Wang, Yanming Shen, and Di He. Awardee solution of kdd cup 2021 ogb large-scale\nchallenge graph-level track. arXiv preprint arXiv:2106.08279, 2021b.\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.\nGraph convolutional neural networks for web-scale recommender systems. In ACM SIGKDD\nConference on Knowledge Discovery and Data Mining (KDD), pages 974\u2013983, 2018.\nHanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan, and Viktor Prasanna. Graph-\nSaint: Graph sampling based inductive learning method. In International Conference on Learning\nRepresentations (ICLR), 2020.\nShanzhuo Zhang, Lihang Liu, Sheng Gao, Donglong He, Xiaomin Fang, Weibin Li, Zhengjie Huang,\nWeiyue Su, and Wenjin Wang. Litegem: Lite geometry enhanced molecular representation learning\nfor quantum property prediction. arXiv preprint arXiv:2106.14494, 2021.\nDa Zheng, Xiang Song, Chao Ma, Zeyuan Tan, Zihao Ye, Jin Dong, Hao Xiong, Zheng Zhang,\nand George Karypis. Dgl-ke: Training knowledge graph embeddings at scale. arXiv preprint\narXiv:2004.08532, 2020.\nChecklist\n1. For all authors...\n(a) Do the main claims made in the abstract and introduction accurately re\ufb02ect the paper\u2019s\ncontributions and scope? [Yes]\n(b) Did you describe the limitations of your work? [Yes] See Appendix A\n(c) Did you discuss any potential negative societal impacts of your work? [Yes] See\nAppendix A\n(d) Have you read the ethics review guidelines and ensured that your paper conforms to\nthem? [Yes]\n2. If you are including theoretical results...\n(a) Did you state the full set of assumptions of all theoretical results? [N/A]\n(b) Did you include complete proofs of all theoretical results? [N/A]\n3. If you ran experiments (e.g. for benchmarks)...\n(a) Did you include the code, data, and instructions needed to reproduce the main experi-\nmental results (either in the supplemental material or as a URL)? [Yes] See Appendix\nA.\n(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they\nwere chosen)? [Yes] See Section 2.\n(c) Did you report error bars (e.g., with respect to the random seed after running exper-\niments multiple times)? [No] Our datasets are very large and offer hidden test sets;\nhence, we follow the convention of similar large-scale datasets such as ImageNet (Rus-\nsakovsky et al., 2015), MS-COCO (Lin et al., 2014), GLUE Benchmark (Wang et al.,\n2018), where we report performance of a single run. Note that model performance is\noften very stable on the large datasets.\n(d) Did you include the total amount of compute and the type of resources used (e.g., type\nof GPUs, internal cluster, or cloud provider)? [Yes] See Appendix A\n4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...\n(a) If your work uses existing assets, did you cite the creators? [Yes] See the references in\nSection 2.\n(b) Did you mention the license of the assets? [Yes] See Appendix A\n15\n(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]\nAll of our relevant URLs are described in Appendix A.\n(d) Did you discuss whether and how consent was obtained from people whose data you\u2019re\nusing/curating? [Yes] We are using public datasets and closely follow the license rules.\n(e) Did you discuss whether the data you are using/curating contains personally identi\ufb01able\ninformation or offensive content? [Yes] Our datasets do not contain any private nor\noffensive information.\n5. If you used crowdsourcing or conducted research with human subjects...\n(a) Did you include the full text of instructions given to participants and screenshots, if\napplicable? [N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review\nBoard (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount\nspent on participant compensation? [N/A]\n16\nA\nKey Information about OGB-LSC\nDataset documentation.\nAll of our datasets as well as how to use them through our Python package\nare documented at https://ogb.stanford.edu/kddcup2021/. Our baseline code to repro-\nduce all the results for each dataset is available at https://github.com/snap-stanford/\nogb/tree/master/examples/lsc.\nIntended use.\nOGB-LSC is intended for machine learning and data scientists to develop ML\nmodels to tackle the challenge of large-scale graph ML.\nRelevant URLs.\nOGB-LSC maintains the following:\n\u2022 Of\ufb01cial website (https://ogb.stanford.edu/kddcup2021/) is the main refer-\nence of OGB-LSC. It provides an overview of the OGB-LSC, descriptions of the datasets\nas well as detailed documentations of how to use the datasets through the OGB Python\npackage. The subpage (https://ogb.stanford.edu/kddcup2021/results/)\nalso contains the leaderboards during the KDD Cup 2021 as well as the technical reports\nand code provided by the winners.\n\u2022 Github repository (https://github.com/snap-stanford/ogb) hosts the\nsource code for the OGB Python package. OGB-LSC datasets and evaluation are all\nmanaged by the Python package. We also release all the baseline code that we used in our\nexperiments.\n\u2022 Datasets are extremely large (around 300GB in total) and are hosted under AWS with the\nhelp of the DGL Team. Our users do not need to directly interact with the URL, as the\ndataset download and processing are all managed by our Python package.\n\u2022 Mailing list (https://groups.google.com/g/open-graph-benchmark) is\nused for making any announcements about OGB/OGB-LSC.\nHosting and maintenance plan.\nOGB-LSC\u2019s Python package is hosted and version-tracked via\nGithub. All the datasets are hosted under the AWS with the help of the DGL Team. We design the\nPython package to handle downloading and processing of the datasets. OGB is a community-driven\ninitiative that has been actively maintained by our team members.\nLicensing.\nThe OGB Python package uses the MIT license. Each dataset has its own license.\nSpeci\ufb01cally, MAG240M uses ODC-BY, WikiKG90M uses CC-0, and PCQM4M uses CC BY 4.0.\nAuthor statement.\nWe bear all responsibility in case of violation of rights, etc., and con\ufb01rmation\nof the data license.\nComputing resources.\nWe ran all the experiments on a server with 10 GeForce RTX 2080 GPUs\nand an Intel(R) Xeon(R) Gold 6148 CPU @ 2.40GHz.\nLimitations.\nLarge-scale graph ML has a wide variety of application domains and there are\nrepresentative graphs that we cannot cover in the current OGB-LSC datasets. Examples include\nlarge-scale recommender systems, social networks, and \ufb01nancial networks. These graphs are hard to\nobtain due to privacy and cooperative concerns, but we hope to include these realistic large graphs\nin the future if we have a chance. That being said, it is our hope that many methodological insights\non our large graphs (training strategy, GNN architecture, regularization, etc) still transfer well to a\nvariety of large-scale graphs. We leave the thorough investigation to future work.\nPotential negative social impacts.\nAll of our datasets are derived from practically-relevant tasks\nin the real world; hence, developing models and deploying them to the real-world could potentially\nproduce predictions that are in\ufb02uenced by the bias in the datasets. For example, regarding the\nMAG240M dataset, we may use the resulting paper and author embeddings to perform a variety of\ndownstream ML tasks such as searching for similar papers or recommending author collaboration\nand paper citations. Thus, it is critical to ensure there is no undesirable bias in the embeddings. There\ncould be also misuse of highly accurate ML models. For instance, regarding the PCQM4M dataset, we\n17\nTable 8: Basic graph statistics of the OGB-LSC datasets. The last three graph statistics are\ncalculated over the \u2018standardized\u2019 graphs, where the graphs are \ufb01rst converted into undirected and\nunlabeled homogeneous graphs with duplicated edges removed. The SNAP library (Leskovec\nand Sosi\u02c7c, 2016) is then used to compute the graph statistics. MAG240M (homo) represents the\nhomogenized MAG240M graph with only paper nodes and citation links. Some graph statistics were\nomitted due to their high computational cost (the calculation did not complete in two weeks).\nModel\n#Graphs\nAvg #nodes\nAvg #edges\nAvg deg\nAvg clust. coeff.\nAvg diameter\nMAG240M\n1\n244,160,499\n1,728,364,232\n14.15\n0.033\n\u2014\nMAG240M (homo)\n1\n121,751,666\n1,297,748,926\n21.30\n0.031\n\u2014\nWikiKG90M\n1\n87,143,637\n504,220,369\n10.93\n\u2014\n\u2014\nWikiKG90Mv2\n1\n91,230,610\n601,062,811\n12.59\n\u2014\n\u2014\nPCQM4M\n3,803,453\n14.15\n14.57\n2.05\n0.010\n7.96\nPCQM4Mv2\n3,746,619\n14.14\n14.56\n2.05\n0.011\n7.95\nneed to make sure that the trained molecular property predictor is used in the right way to develop\nuseful drugs/materials rather than harmful ones.\nB\nBasic Graph Statistics of the Datasets\nThe basic graph statistics of the OGB-LSC datasets are provided in Table 8.\nC\nDetails about Dataset Updates after the KDD Cup 2021\nMAG240M updates.\nThe MAG240M dataset itself has not been changed. The only update is on the\ntest set. In Table 8, we report the test-dev accuracy of all the models.\nWikiKG90Mv2 updates.\nThe WikiKG90M dataset has been updated to WikiKG90Mv2. Below\nwe summarize the updates we have applied to the dataset.\n\u2022 No candidate tails provided. The most important update is that we do not provide any\ncandidate tail entities for validation/test triples. Hence, a model needs to predict the target\ntail entity out of all the entities in Wikidata.\n\u2022 Created from more recent Wikidata. The WikiKG90Mv2 is based on the public Wiki-\ndata dump downloaded at three time-stamps: May 17th, June 7th, and June 28th of 2021, for\ntraining, validation, and testing, respectively. We retain all the entities and relations in the\nSeptember dump, resulting in 91,230,610 entities, 1,387 relations, and 601,062,811 triplets\nin total.\n\u2022 A better text encoder used. The text features of WikiKG90Mv2 are obtained by using\nMPNet (Reimers and Gurevych, 2019; Song et al., 2020), which is shown to be signi\ufb01cantly\nbetter sentence encoder (Reimers and Gurevych, 2019).\n\u2022 Balancing relation types in validation/test triples. On the new Wikidata dumps, we found\nthe relation types of the raw validation/test triples are highly-skewed; the most frequent\nrelation, \u201ccites work (P2860)\u201d, occupies 60% and 85% of the entire validation and test\ntriples, respectively. To test a model\u2019s capability to perform well across all types of relations,\nwe subsample 15,000 triples from the entire validation/test triples such that the resulting\nrelation counts are proportional to the cubic-root of the original relation counts.\nIn Table 10, we show head entities that have very sparse connection in the training KG. We see that\ntextual features could provide important signals for predicting these triples.\nWe perform an extensive baseline analysis on WikiKG90Mv2.\nWe used the same set of\nhyper-parameters and baseline models as our original WikiKG90M. Different from WikiKG90M,\nWikiKG90Mv2 does not provide any candidate tail entities. A na\u00a8\u0131vely approach is to use the entire\nentities as the tail candidates. However, this approach does not scale well to a KG with nearly 90M\nentities because we need to predict scores for all the 90M entities for every triple. Nonetheless, in\npractice, most of the entities are obvious negatives: e.g., for the relation type \u201cis located in\u201d, any\n18\nentities that are not locations can be easily \ufb01ltered out as negatives. Based on the the above intuition,\nwe consider the relation-speci\ufb01c tail candidate sets. Speci\ufb01cally, on training triples, we pre-compute\n20K most frequent tail entities for each relation and treat them as candidate tail entities for that\nrelation. At inference time, we use our KG model to score among those relation-speci\ufb01c candidates.\nThe results are provided in Table 10. Overall, we observe that the relative trends are similar to the\noriginal WikiKG90Mv2. Especially the CONCAT encoder provides the best MRR performance.\nDifferent from WikiKG90M, the MRR score on the new WikiKG90Mv2 is far perfect score of 1\nand leaves a lot of room for improvement. Overall, we believe it is promising to explore how to\nquickly generate a small number of high-quality candidate tail entities out of all the entities so that\nKG models only need to score a much fewer number of candidate entities.\nPCQM4Mv2 updates.\nThe PCQM4M dataset has been updated to PCQM4Mv2. Below we summarize\nthe updates we have applied to the dataset.\n\u2022 3D molecular structures provided. We additionally provide 3D structures for training\nmolecules. These structures are calculated by DFT and are obtained together with the\nHOMO-LUMO gap.\n\u2022 SMILES strings are partly updated. In the process of preparing the 3D structures, we\nfound a subtle mismatch between SMILES strings (i.e., 2D molecular graphs) and the\nHOMO-LUMO gap for about 10% of the entire molecules. Speci\ufb01cally, the SMILES\nstrings can be changed in the course of DFT\u2019s geometry optimization, but in PCQM4M, we\nprovided the initial SMILES strings. In the updated PCQM4Mv2, we provide SMILES\nstrings corresponding to the \ufb01nal optimized 3D structures. Note that the HOMO-LUMO\ngap was calculated by DFT based on the \ufb01nal 3D structures (Nakata and Shimazaki, 2017);\nhence, it makes more sense to correspond the HOMO-LUMO gap with the SMILES string\nassociated with the \ufb01nal 3D structures.\n\u2022 Number of molecules decreased slightly. As a result of the SMILES update, some\nmolecules can no longer be parsed by the commonly-used chemistry toolkit, i.e.,\nrdkit (Landrum et al., 2006). As a result, the total number of molecules has been\nslightly reduced to 3,746,619.\n\u2022 Split ratio changed. For PCQM4Mv2, we set the split ratio for train/validation/test-dev/test-\nchallenge to 90/2/4/4. The split is still done by PubChem compound ID so that there is no\ntest label leakage, i.e., all the test molecules in PCQM4Mv2 is in the test split of PCQM4M.\nSimilar to PCQM4M, we also provide our baseline analysis on the updated PCQM4Mv2 dataset. At\ninference time, we clamped the output values to be between 0 and 20, which prevents our models\nfrom predicting erroneous values for some test molecules. We show the results in Tables 12 and\n13. We found that all the models were able to achieve lower MAE compared to PCQM4M, probably\nbecause we have \ufb01xed the mismatch bug described above. Beyond the overall better MAE, we see\nthat the trend in model performance is mostly preserved; larger and more expressive GNN models\nachieve better results. For the GNNs, we observe that the depth helps more than width. Interestingly,\ntoo-wide models often make unstable prediction on validation molecules.\n19\nTable 9: Results of MAG240M measured by the\naccuracy (%). R-GRAPHSAGE/-GAT utilize\nthe full heterogeneous graph information, while\nthe other models operate on the homogeneous\npaper citation graph. Test accuracy is evaluated\non the test-dev set.\nModel\n#Params\nValidation\nTest-dev\nMLP\n0.5M\n52.67\n52.76\nLABELPROP\n0\n58.44\n56.38\nSGC\n0.7M\n65.82\n65.30\nSIGN\n3.8M\n66.64\n66.03\nMLP+C&S\n0.5M\n66.98\n66.05\nGRAPHSAGE (NS)\n4.9M\n66.79\n66.21\nGAT (NS)\n4.9M\n67.15\n66.71\nR-GRAPHSAGE (NS)\n12.2M\n69.86\n68.78\nR-GAT (NS)\n12.3M\n70.02\n69.31\nKDD 1ST: BD-PGL\nEnsemble\n75.39\nKDD 2ND: ACADEMIC\nEnsemble\n75.07\nKDD 3RD: SYNERISE AI\nEnsemble\n74.57\nTable 10: Textual representation of validation\ntriplets whose head entities only appear once\nas head in the training WikiKG90Mv2.\nHead\nRelation\nTail\nHerbert Hoover\u2019s Inaugural Address\ncountry\nUnited States of America\nJussi Award for Best Sound Recording\ninstance of\nclass of award\norgan dose\ncalculated from\nabsorbed dose\nBritish Endurance Racing Team\ncountry\nUnited Kingdom\nKnee bursae\nanatomical location\nknee\nChurches in Dekanat Leuchtenberg\nis a list of\nchurch building\nweb content management system\nmodel item\nwork\ufb02ow management system\nStephan von Divonne\ngiven name\nStephan\nMinecraft mod\ndepends on software\nMinecraft\nbeer pouring\nuses\nbeer engine\nTable 11:\nResults of WikiKG90Mv2 mea-\nsured by the Mean Reciprocal Rank (MRR).\nModel\n#Params\nValidation\nTest-dev\nTRANSE-SHALLOW\n18.2B\n0.1103\n0.0824\nCOMPLEX-SHALLOW\n18.2B\n0.1150\n0.0985\nTRANSE-MPNET\n0.3M\n0.1128\n0.0860\nCOMPLEX-MPNET\n0.3M\n0.1258\n0.0988\nTRANSE-CONCAT\n18.2B\n0.2060\n0.1761\nCOMPLEX-CONCAT\n18.2B\n0.2048\n0.1761\nTable 12: Results of PCQM4Mv2 measured by\nMAE [eV]. The lower, the better. Ablation study\nof using only 10% of training data is also shown.\nChemical accuracy indicates the \ufb01nal goal for prac-\ntical usefulness.\nModel\n#Params\nValidation\nTest-dev\nMLP-FINGERPRINT\n16.1M\n0.1753\n0.1760\nGCN\n2.0M\n0.1379\n0.1398\nGCN-VIRTUAL\n4.9M\n0.1153\n0.1152\nGIN\n3.8M\n0.1195\n0.1218\nGIN-VIRTUAL\n6.7M\n0.1083\n0.1084\nMLP-FINGERPRINT (10% train)\n16.1M\n0.2429\n0.2445\nGIN-VIRTUAL (10% train)\n6.7M\n0.1442\n0.1446\nChemical accuracy (goal)\n\u2013\n0.0430\nTable 13: Model size and the MAE perfor-\nmance [eV]. For both models, the width in-\ndicates the hidden dimensionality. For GIN-\nVIRTUAL, the depth represents the number of\nGNN layers, while for the MLP-FINGERPRINT,\nthe depth represents the the number of hidden\nlayers in MLP.\nModel\nWidth\nDepth\n#Params\nValidation\nMLP-FINGERPRINT\n1600\n6\n16.1M\n0.1753\n1600\n4\n11.0M\n0.1752\n1600\n2\n5.8M\n0.1954\n1200\n6\n9.7M\n0.1804\nGIN-VIRTUAL\n600\n5\n6.7M\n0.1083\n600\n3\n3.7M\n0.1239\n300\n5\n1.7M\n0.1100\n300\n3\n1.0M\n0.1181\n20\n",
    "2210.12941": "Unsupervised Graph Outlier Detection: Problem\nRevisit, New Insight, and Superior Method\nYihong Huang\u2020, Liping Wang\u2020, Fan Zhang\u2021, Xuemin Lin\u00a7\n\u2020East China Normal University, \u2021Guangzhou University, \u00a7Shanghai Jiao tong University\nhyh957947142@gmail.com, lipingwang@sei.ecnu.edu.cn\nzhangf@gzhu.edu.cn, lxue@cse.unsw.edu.au\nAbstract\u2014A large number of studies on Graph Outlier De-\ntection (GOD) have emerged in recent years due to its wide\napplications, in which Unsupervised Node Outlier Detection\n(UNOD) on attributed networks is an important area. UNOD\nfocuses on detecting two kinds of typical outliers in graphs: the\nstructural outlier and the contextual outlier. Most existing works\nconduct experiments based on datasets with injected outliers.\nHowever, we \ufb01nd that the most widely-used outlier injection\napproach has a serious data leakage issue. By only utilizing\nsuch data leakage, a simple approach can achieve state-of-the-art\nperformance in detecting outliers. In addition, we observe that\nexisting algorithms have a performance drop with the mitigated\ndata leakage issue. The other major issue is on balanced detection\nperformance between the two types of outliers, which has not\nbeen considered by existing studies.\nIn this paper, we analyze the cause of the data leakage issue\nin depth since the injection approach is a building block to\nadvance UNOD. Moreover, we devise a novel variance-based\nmodel to detect structural outliers, which outperforms existing\nalgorithms signi\ufb01cantly and is more robust at kinds of injection\nsettings. On top of this, we propose a new framework, Variance-\nbased Graph Outlier Detection (VGOD), which combines our\nvariance-based model and attribute reconstruction model to\ndetect outliers in a balanced way. Finally, we conduct extensive\nexperiments to demonstrate the effectiveness and ef\ufb01ciency of\nVGOD. The results on 5 real-world datasets validate that VGOD\nachieves not only the best performance in detecting outliers but\nalso a balanced detection performance between structural and\ncontextual outliers.\nIndex Terms\u2014Graph Outlier Detection; Graph Neural Net-\nwork; Unsupervised Graph learning; Attributed Networks\nI. INTRODUCTION\nGraph Outlier Detection [1] (GOD, a.k.a. graph anomaly\ndetection) is a fundamental graph mining task. It has various\napplications in high-impact domains and complex systems,\nsuch as \ufb01nancial fraudster identi\ufb01cation [2]. The detection\nobjects of GOD can be classi\ufb01ed into different levels like\nnode, edge, community, and graph [3]. For example, detecting\nabnormal users in a social network is the node-level GOD\ntask while detecting abnormal molecules can be regarded as a\ngraph-level GOD task.\nDue to the high cost or unavailability of manually la-\nbeling the ground truth outliers, a large number of existing\nGOD approaches are carried out in an unsupervised manner\n[4, 5], which aims to detect the instances that signi\ufb01cantly\ndeviate from the majority of instances in the graph [6].\nAttributed networks (a.k.a. attributed graphs) are a powerful\ndata representation for many real-world complex systems (e.g.\na social network with user pro\ufb01les), in which entities can\nbe represented as nodes with their attribute information; the\ninteraction or relationship between entities can be represented\nas edges [7]. In recent years, the study of Unsupervised Node\nOutlier Detection (UNOD) on attributed networks has been\nblooming due to its wide applications [3, 8, 9]. Different\nfrom traditional global outlier detection and time series outlier\ndetection, it de\ufb01nes two new typical types of outliers on\nattributed networks, namely, structural outlier and contextual\noutlier [4].\n(a) Structural Outlier\n(b) Contextual Outlier\nNormal Node\nOutlier Node\nNode Attributes\ncommunity A\ncommunity B\nFig. 1. An example of structural and contextual outliers in UNOD.\nIn Fig 1, there is a toy example for these two kinds\nof basic outliers. Particularly, structural outliers are those\nnodes structurally connected to different communities, i.e.,\ntheir structural neighborhood is inconsistent. In other words,\na structural outlier has normal attributes while it may have\nseveral abnormal links. For example, those people from dif-\nferent communities but have a strong connection with each\nother can be regarded as structural outliers. As shown in Fig\n1(a), there are two communities outlined with orange circles\nand structural outliers are those nodes with abnormal links\nto other communities. On the other hand, a contextual outlier\nhas a consistent neighborhood structure while its attributes are\ncorrupted, noisy, or signi\ufb01cantly different from its neighbor\nnodes\u2019 attributes. For example, in Fig 1(b), suppose that the\nnode in red is a football player while the nodes in green\nare music teachers. In this case, the node in red is regarded\nas a contextual outlier since it has a vast difference from\nits neighbors. In the real world, datasets are much more\ncomplicated than this toy example and it is dif\ufb01cult to measure\nthe degree of inconsistency among nodes.\narXiv:2210.12941v3  [cs.LG]  9 Jan 2023\nThere have been various methods proposed to solve UNOD\n[9]. They can be roughly divided into two categories, namely,\nnon-deep and deep-learning-based methods. Non-deep meth-\nods usually leverage traditional machine learning methods\nsuch as matrix factorization [10], density-based clustering\n[11], and relational learning [12] to encode the graph informa-\ntion and detect outliers. However, these methods fail to address\nthe computational challenge with high-dimension data [13].\nWith the rapid prevalence of Graph Neural Networks (GNNs)\n[14], more and more methods are based on deep learning\n[15] and GNNs nowadays [3]. For example, DOMINANT [4]\nemploys two GNN autoencoders to reconstruct the attribute\ninformation and structure information. According to the results\nreported in [9], most deep-learning-based methods have a\nmuch better performance than non-deep methods in detecting\ninjected outliers. To unify the outlier injection process, [9]\nadopts the outlier injection approach from [4] as the standard\ninjection approach.\nChallenge. Although the recent deep-learning-based methods\nhave achieved an excellent performance in UNOD, we \ufb01nd\nthat the most widely-used outlier injection approach, which\nis adopted by [4, 9, 16, 17, 18, 19, 20, 21, 22, 23, 24],\nwill cause a serious data leakage issue. Here, we refer to\nthe data leakage [25] in machine learning, which means the\ninformation strongly associated with the labels is leaked to\nthe training dataset. After employing this approach to inject\noutliers, structural outliers will have a larger node degree than\nthe average while attribute vectors of contextual outliers will\nhave a larger L2-norm (a.k.a. Euclidean norm) than expected.\nAs a result, a simple solution only utilizing node degree or\nL2-norm of attribute vectors as the outlier score to detect the\ncorresponding type of outliers can acquire a quite satisfying\nperformance as shown in Fig 2. The metric of AUC [26] is\nadopted here to measure the detection performance. Under\nsuch a serious data leakage issue in injected datasets, most\nexisting algorithms cannot have a better performance than the\nsimple solution. In addition, it is observed that existing algo-\nrithms have a performance drop in varied injection settings, in\nwhich the data leakage issue caused by the current injection\napproach is alleviated. Therefore, it is urgent to \ufb01nd out the\ncause of data leakage and reduce its impact. On the other hand,\nit is also necessary to exploit an effective UNOD algorithm,\nwhich has a superior performance and is robust to the data\nleakage issue. Moreover, the balance between structural and\ncontextual outliers detection performance is little considered\nin existing works [9]. An algorithm with unbalanced detection\nmay only have detection ability for a certain type of outliers.\nIt is found that existing algorithms focus more on contextual\noutliers than structural outliers when detecting them. To gain\nmore feasible algorithms, comprehensive metrics for balance\nevaluation should be devised.\nOur Solution. In this paper, we are devoted to analyzing the\ncause of data leakage and devising a superior outlier detection\nmethod for UNOD to achieve better performance in both the\ncurrent injection setting and varied injection settings.\ncora\nciteseer pubmed Flickr\nDataset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n0.5\n0.5\n0.5\n0.5\n0.959\n0.973\n0.98\n0.987\ncontextual outliers\nrandom\nL2-norm\ncora\nciteseer pubmed Flickr\nDataset\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nAUC\n0.5\n0.5\n0.5\n0.5\n0.986\n0.992\n0.943\n0.646\nstructural outliers\nrandom\nnode degree\nFig. 2.\nAfter injecting outliers in four datasets, node degree is employed\nto detect structural outliers while L2-norm is employed to detect contextual\noutliers. Both of them, compared to the random detector, achieve unexpectedly\nhigh scores.\nIn particular, we propose a novel variance-based model\nto detect structural outliers, which adopts the variance of\nrepresentations of neighbor nodes to detect structural outliers.\nTo the best of our knowledge, it is the \ufb01rst time to employ\nthe neighbor variance to detect outliers. Existing algorithms\nare based on either reconstruction of adjacency matrix [4, 18]\nor contrastive learning [16, 23] to detect structural outliers.\nAccording to the de\ufb01nition, the essence of a structural outlier\nis its inconsistent neighbors that come from different commu-\nnities. However, existing algorithms cannot directly capture\nthis essence. In this case, we devise a deep graph model to\nmeasure the inconsistency among neighbors by the variance\nof latent representations of neighbors, which captures the\nessence and gives a better utility for detection. On top of this,\na new framework, Variance-based Graph Outlier Detection\n(VGOD), is also proposed to detect two types of outliers with\na variance-based model and an attribute reconstruction model.\nTo address the balance issue, we separately train two models\nto avoid overtraining and normalize two types of outlier scores\nto eliminate the scale difference. To evaluate the detection\nbalance on two outlier types, we introduce a new metric to\nmeasure the gap in the performance score. The experiments are\nconducted on 5 real-world datasets and the results demonstrate\nthat VGOD achieves the best detection performance.\nContribution. Our major contributions are as follows.\n1) To the best of our knowledge, we are the \ufb01rst to identify\nthe data leakage issue in the most widely-used outlier\ninjection approach.\n2) We analyze the cause of data leakage in depth, give\nsuggestions for the future design of the outlier injection\napproach, and purpose a new approach for injecting\nstructural outliers.\n3) We propose a novel variance-based model and a new\nVGOD framework, which outperforms existing algo-\nrithms in detecting outliers and alleviates the issue of\nthe balance of detection.\n4) Extensive experiments are conducted to demonstrate that\nour approach achieves the best detection performance\nand the detection balance.\nII. RELATED WORK\nA. Graph Neural Network\nGNNs [14] are a group of neural network models which\nutilize the graph structure for network representation learning\nand various tasks. Among GNNs, GCN [27] is one of the most\nin\ufb02uential models, which extends the convolutional operation\nin sequence or grid data to graph-structured data. Furthermore,\nto aggregate messages from neighbors more \ufb02exibly, GAT [28]\nintroduces an attention mechanism to learn the importance\nof each neighbor node. On the other hand, GraphSage [29]\nadopts the sampling-based method to aggregate the neighbor\ninformation to work in large-scale graphs. From a topological\nlearning perspective, GIN [30] is a more expressive model\nthan GCN and can achieve the same discriminative power\nas the 1-WL graph isomorphism test [31]. In our proposed\nframework, GNN plays a vital role in the network embedding\nrepresentation of nodes. Generally, the GNN module in our\nframework can be set to any type of the above-mentioned\nGNNs.\nB. Unsupervised Node Outlier Detection on Attributed Net-\nworks\nUNOD on attributed networks has attracted considerable\nresearch interest in recent years due to its wide application in\ncomplex systems. Radar [32] utilizes the residuals of attribute\ninformation and its coherence graph structure for outlier de-\ntection. ANOMALOUS [33] conducts attribute selection and\noutlier detection jointly based on CUR decomposition and\nresidual analysis. However, these methods have computation\nlimitations in high-dimension attributes due to their shallow\nmechanisms.\nQuite a few studies based on the deep-learning technique\nhave emerged recently [3]. Dominant [4] builds deep autoen-\ncoders on top of GCN layers to reconstruct the adjacency\nand attribute matrices. AnomalyDAE [18] employs dual au-\ntoencoder architecture with cross-modality interactions and\nthe attention mechanism to reconstruct the adjacency and\nattribute matrices. CoLA [16] and SL-GAD [23] perform\nthe UNOD task via contrastive self-supervised learning and\nrandom walk to embed nodes. AEGIS [17] studies UNOD in\nthe inductive setting by utilizing generative adversarial ideas to\ngenerate potential outliers. DONE [34] employs deep unsuper-\nvised autoencoders to generate the network embedding which\neliminates the effects of outliers at the same time. CONAD\n[19] adopts four data augmentation strategies and contrastive\nlearning for outlier detection. GUIDE [21] replaces adjacency\nreconstruction with higher-order structure reconstruction to\ndetect structural outliers. Under the manner of outlier injection,\nall these above deep methods show superior performance than\nnon-deep methods in detecting these two types of outliers. To\nevaluate UNOD algorithms, [9] adopts the most widely-used\noutlier injection approach from [4] as the standard injection\nmethod and provides uni\ufb01ed benchmarks for UNOD, which\nfacilitates fairness for comparing different methods.\nCurrent UNOD methods have achieved an excellent per-\nformance. However, as demonstrated in Fig 2, the widely-\nused outlier injection approach exists a data leakage issue.\nTo our surprise, simply using the combination of L2-norm\nand node degree to detect outliers can achieve state-of-the-\nart performance. Therefore, our work focuses on analyzing\nthe cause of the data leakage issue and designing a superior\nmethod. In addition, as mentioned in [9] that no current\nmethod has a balanced detection performance on two outlier\ntypes, we also consider the balance issue in our method.\nIII. PRELIMINARY\nIn this section, we formally present some concepts which\nare used throughout this paper and de\ufb01ne the problem. We\nuse lowercase letters (e.g. a), bold lowercase letters (e.g. x),\nuppercase letters (e.g. X), and calligraphic fonts (e.g. V) to\ndenote scalars, vectors, matrices, and sets, respectively.\nA. Graph Neural Network\nGNNs stack L layers of message-passing layers. Each layer\nperforms a message passing through the given graph structure.\nAfter the initial node feature h0 \u2208Rd0 is transformed by L\nlayers, the vector representation hL \u2208RdL is learned for each\nnode v. Most message-passing layers can be expressed using\nthe following rule:\nh(l)\nv\n= \u03c3(\u03a8(l)(AGG({\u03a6(l)(h(l\u22121)\nu\n), u \u2208Nv \u222a{v}}))) (1)\nwhere \u03c3(\u00b7) is the active function, \u03a8(l)(\u00b7) and \u03a6(l)(\u00b7) de-\nnote differentiable functions such as Multi-Layer Perceptrons\n(MLP). AGG(\u00b7) denotes a differentiable, permutation invariant\nfunction (e.g. sum, mean, max) and Nv denotes node v \u2019s\ndirect linked neighbors.\nHere, we introduce three commonly used GNNs, namely\nGCN, GAT, and GIN.\nGraph Convolution Network (GCN) [27] is the most\nwidely-used GNN module, which adopts the propagation rule:\nH(l+1) = \u03c3( \u02c6AH(l)W (l))\n(2)\nwhere \u02c6A is the symmetric normalized adjacency matrix, H(l)\nis the lth hidden layer node representation, and W (l) is the\nparameters in the lth hidden layer.\nGraph Attention Network (GAT) [28] \ufb02exibly aggregates\nmessages from neighbors with calculated weight \u03b1ij (vs.\naverage weight adopted by GCN) of each edge \u27e8i, j\u27e9as\n\u03b1ij =\nexp(LeakyReLU(aT [Whi||Whj]))\nP\nk\u2208Ni exp(LeakyReLU(aT [Whi||Whk]))\n(3)\nwhere a and W are the learnable weights. Layer mask (l) is\nomitted for simplicity.\nGraph Isomorphism Network (GIN) [30] is the expres-\nsively more powerful GNN model, which follows the rule to\npropagate messages as\nH(l) = \u03c3(\u03a8(l)(A + (1 + \u03f5) \u00b7 I)H(l\u22121))\n(4)\nwhere \u03f5 can be a \ufb01xed or learnable scalar parameter, and I and\nA is the identity matrix and adjacency matrix, respectively.\nB. Unsupervised Node Outlier Detection on Attributed Net-\nworks\nDe\ufb01nition 1 (Attributed Network). An attributed network can\nbe denoted as G = (V, E, X), where V = {v1, v2, ..., vn} is\nthe set of nodes (|V| = n), E is the set of edges (|E| = m), and\nX \u2208Rn\u00d7d is the attribute matrix. The ith row vector xi \u2208Rd\nof the attribute matrix denotes the attribute information of the\nith node. Node i\u2019s direct neighbors can be denoted as Ni.\nWith the aforementioned notations, the outlier detection\nproblem on attributed network can be formally stated as a\nranking problem.\nDe\ufb01nition 2 (Outlier Detection on Attributed networks).\nGiven an attributed network G = (V, E, X), the goal is to\nlearn an outlier score function f(\u00b7) to calculate the outlier\nscore oi = f(vi) for each node. The higher the outlier score\noi is, the ith node is more likely to be a structural outlier or a\ncontextual outlier. By ranking all the nodes with their outlier\nscores, the abnormal nodes can be detected according to their\nranking.\nIn this paper, we consider the setting of unsupervised node\noutlier detection (UNOD), which is generally adopted by\nprevious works. In this setting, none of the outlier labels of\nthe nodes is given in the training phase.\nIV. DATA LEAKAGE ISSUE ANALYSIS\nAs shown in Fig 2, the current widely-used outlier injection\napproach exists a serious data leakage issue. In this section, we\nanalyze the data leakage issue in detail. For these two types of\noutliers, we \ufb01rst introduce the outlier injection approach from\n[4]. Next, we theoretically analyze the cause of data leakage\nand give our suggestions for a better design of the outlier\ninjection approach.\nA. Structural Outlier\n1) Injection Approach: The structural outliers are acquired\nby disturbing the topological structure of the graph. In a\nclique, nodes are fully connected. The intuition is that nodes\nin a clique should have a strong correlation with each other.\nBased on this, the outlier assumption is that a clique formed\nby unrelated nodes is structurally abnormal. The process of\nstructural outlier injection is as followed. The \ufb01rst step is to\nspecify the clique size q and the number of cliques p. Next,\nfor each clique, q random nodes are chosen from the set of\nnormal nodes and made fully connected as structural outliers.\nTherefore, total p \u00d7 q structural outliers will be injected into\nthe dataset. In previous works, the clique size q is \ufb01xed to 15\nfor all datasets, and the value of p is set according to the size\nof the dataset.\n2) Cause Analysis: It is obvious that the chosen structural\noutliers will have a higher node degree since additional edges\nare added to them. Table I shows that none of the three citation\nnetworks (Cora, Citeseer, PubMed) have an average node\ndegree greater than 3. However, due to the above injection\napproach, all the outliers will have a node degree of at least\nmore than 15.\nB. Contextual Outlier\n1) Injection Approach: The contextual outliers are acquired\nby disturbing the attributes of nodes. The injection process is\nas followed. Firstly, total p\u00d7q normal nodes will be chosen as\ncontextual outliers, which have the same number as structural\noutliers. Next, for each chosen outlier node vi, another k nodes\n{vc1, vc2, ..., vck} are randomly sampled from V as a candidate\nset Vc. For each vci in Vc, the Euclidean distance between the\nattribute vector xci of vci and xi of vi will be calculated.\nThe attribute vector xci with the largest \u2225xci \u2212xi\u22252 will be\nused to replace xi as the new attribute vector of vi. The size\nof candidate set k is set to 50, ensuring that the disturbance\namplitude is large enough.\n2) Cause Analysis: To ensure a large enough disturbance of\nattributes, the above injection approach changes the xi to xci\nwith the largest \u2225xci \u2212xi\u22252. However, such a strategy will\nlead to the L2-norm of the \ufb01nal chosen xci (i.e. \u2225xci\u22252) being\nmore likely to be large. We make the following assumptions.\nAssumption 1. Suppose both xci \u2208Rd and xi \u2208Rd are\nindependently sampled from attribute matrix X. The rank of\nmatrix X is greater than 1.\nAssumption 2. For xci \u223cX, xci = \u2225xci\u22252 \u00b7 \u20d7eci, where\n\u2225xci\u22252 and \u20d7eci are the modulo and direction of xci, respec-\ntively. We assumed that \u2225xci\u22252 and \u20d7eci are independently\ndistributed.\nWe use Pr(x) to denote the possibility of x, then we have\nthe following theorem.\nTheorem 1. Pr(\u2225xci \u2212xi\u22252 > \u2225xcj \u2212xi\u22252 \u21d2\u2225xci\u22252 >\n\u2225xcj\u22252) > 0.5\nProof. We de\ufb01ne D(xci, xi) = \u2225xci \u2212xi\u22252 . For notational\nconvenience, we use s to refer to xci and t to refer to xi.\nPlease note that both s and t are independently sampled from\nX \u2208Rn\u00d7d.\nD2(s, t) =\nd\nX\ni\n(si\u2013ti)2\n=\nd\nX\ni\ns2\ni \u22122\nd\nX\ni\nsiti +\nd\nX\ni\nt2\ni\n= \u2225s\u22252\n2 \u22122\u2225s\u22252\u2225t\u22252cos\u03b1 + \u2225t\u22252\n2\n= f(\u2225s\u22252)\nwhere \u03b1 is the angle between vector s and t, and \u2225s\u22252 is the\nmodulo of s. From the above Equation, we can regard D2(s, t)\nas a quadratic function f(\u00b7) of \u2225s\u22252. Particularly, \u2225s\u22252 =\n\u2225t\u22252cos\u03b1 is the symmetry axis for f(\u2225s\u22252). According to the\nproperties of a quadratic function in one variable, the function\nis monotonic on both sides of the symmetry axis. Therefore,\nif\n\u2225s\u22252 > \u2225t\u22252cos\u03b1 \u21d2(f(\u2225s\u22252) \u2191\n\u21d2\u2225s\u22252 \u2191)\nif\n\u2225s\u22252 < \u2225t\u22252cos\u03b1 \u21d2(f(\u2225s\u22252) \u2191\n\u21d2\u2225s\u22252 \u2193)\nwhere \u2191means increase and \u2193means decrease. In this case,\nwe can draw the following conclusions.\nPr(\u2225s\u22252 > \u2225t\u22252cos\u03b1) = Pr(f(\u2225s\u22252) \u2191\n\u21d2\u2225s\u22252 \u2191)\n= Pr(D2(s, t) \u2191\n\u21d2\u2225s\u22252 \u2191)\n= Pr(\u2225xci \u2212xi\u22252 > \u2225xcj \u2212xi\u22252 \u21d2\u2225xci\u22252 > \u2225xcj\u22252)\nSince s and t are independently sampled from attribute\nmatrix X, we can draw\nPr(\u2225s\u22252 > \u2225t\u22252) = 0.5.\nDue to the assumption that the rank of X is greater than\n1, the angle between s and t does not always equal zero.\nTherefore, Pr(cos\u03b1 \u22611) < 1. Note that cos\u03b1 \u22641. Finally,\nwe draw the following\nPr(\u2225s\u22252 > \u2225t\u22252cos\u03b1) > 0.5\nwhich means\nPr(\u2225xci \u2212xi\u22252 > \u2225xcj \u2212xi\u22252 \u21d2\u2225xci\u22252 > \u2225xcj\u22252) > 0.5\nFig 2 veri\ufb01es our analysis that only utilizing the L2-norm of\nattribute vectors of nodes can achieve nearly 0.98 AUC score\nfor all these four datasets when k = 50.\nFurther, we vary the parameter k of the above injection\napproach. As k is set smaller, the data leakage issue is\nmitigated, which is shown in the left part of Fig 3, indicating\nthe large k is the main cause for the serious data leakage\nissue. In the right part of Fig 3, we also replace the Euclidean\ndistance by cosine distance in the injection approach. At\nthis time, not all datasets have a data leakage issue when k\nbecomes large. Therefore, Euclidean distance is also a key\nfactor for data leakage.\nk=1\nk=2\nk=5\nk=50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC\nEuclidean distance\nCora\nCiteseer\nPubMed\nFlickr\nk=1\nk=2\nk=5\nk=50\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nCosine distance\nCora\nCiteseer\nPubMed\nFlickr\nFig. 3.\nAUC of L2-norm for contextual outliers injection with varying\nparameter of k (size of candidate set) and different distance measurements.\nC. Suggestion\nDue to the serious data leakage issue, it is hard to \ufb01gure\nout whether the outlier detection algorithm has an effect\non detection or potentially exploits the leaked information\nof labels. According to our experiment in Section VI-B, a\nsimple baseline only using the leaked information outperforms\nexisting deep-learning-based solutions that need a long time\nto train. This can be explained from two aspects:\n1) The data leakage issue caused by the current injection\napproach is too serious, which results in little space for\nthese algorithms to improve.\n2) Existing outlier detection algorithms are not effective\nenough.\nTo mitigate data leakage caused by the current injection\napproach, we give the following suggestions on designing\nthe new injection approach, better experiment, and parameter\nsetting.\nSuggestions for experiments and outlier injection. We give\nthe suggestion for experiments and a new idea for outlier\ninjection:\n\u2022 Firstly, the data leakage issue should be examined. If it\nis hard to avoid data leakage, then the leaked information\nshould be compared to see the exact improvement.\n\u2022 Secondly, some datasets contain category labels for the\nnode classi\ufb01cation task. It is natural to think nodes with\ndifferent labels come from different communities. Can we\ndesign a better injection approach based on this?\nSuggestions for structural outliers injection. The size of the\nstructural outlier clique is much larger than the average node\ndegree of graph, which leads to node degree being a signal for\nstructural outliers. Therefore, we can set the injection clique\nsize q smaller for the current injection approach.\nOn the other hand, in real world, higher node degrees are\nnot a signal for structural outliers. For example, a famous\nperson has many friends, but these friends are all in his or\nher community circle, so this person is not a structural outlier.\nTherefore, to keep the distribution of node degree, replacing\nedges can be considered for a new injection approach.\nSuggestions for contextual outliers injection. We have ana-\nlyzed that the size of candidate set k and Euclidean distance\nare two key factors to cause data leakage. Based on this, we\ngive these suggestions:\n\u2022 Firstly, simply setting k smaller can mitigate the data\nleakage for the current injection approach.\n\u2022 Secondly, replacing Euclidean distance with other dis-\ntance measurements can be tried, such as cosine distance,\nshortest path distance, and so on.\nD. Summary\nIn summary, the current widely-used outlier injection ap-\nproach will cause the data leakage issue, both in structural\nand contextual outlier injection. We also give some suggestions\nfor designing a new injection approach, better experiment, and\nparameter setting. We have applied some of our suggestions in\nour experiment to evaluate the effectiveness of existing outlier\ndetection solutions and our solution.\nV. METHODOLOGY\nIn this section, we are going to illustrate our proposed\nframework VGOD in detail. Since current UNOD algorithms\ncannot outperform the simple baseline which only utilizes\ndata leakage information, we propose our new framework\nVGOD, which combines a novel variance-based model and\nattribute reconstruction model. Speci\ufb01cally, the former model\nis for detecting structural outliers and the latter model is for\ndetecting contextual outliers. Then we standardize the outlier\nscores outputted by two models and add them to get the\n\ufb01nal score. Fig 4 presents the whole architecture of VGOD\nframework.\n( \ud835\udcb1, \u2130, \ud835\udc4b)\n( \ud835\udcb1, \u2130, \ud835\udc4b)\n( \ud835\udcb1, \u2130(+), \u2130(\u2212) , \ud835\udc4b)\n\ud835\udc8d\ud835\udc90\ud835\udc94\ud835\udc94\ud835\udc97\ud835\udc82\ud835\udc93\n(\u2212)\n\ud835\udc8d\ud835\udc90\ud835\udc94\ud835\udc94\ud835\udc97\ud835\udc82\ud835\udc93\n(+)\ncontrast\n\u0de0\ud835\udc4b\n\ud835\udc8d\ud835\udc90\ud835\udc94\ud835\udc94\ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc90\ud835\udc8f\n\ud835\udc94\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nMeanConv\nMinusConv\nGNNs\n[ Variance-Based Model ]\n[ Attribute Reconstruction Model ]\n \n \n \n \n \n \n,\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFig. 4. The overview of our proposed unsupervised node-level graph outlier detection framework VGOD. For a given attributed network G, the variance-based\nmodel and attribute reconstruction model are employed to calculate the structural and contextual outlier score, respectively. The \ufb01nal score is the sum of two\nstandardized scores. In the variance-based model (VBM), we use a negative edge sampling technique to generate a corresponding negative edge set E(\u2212) per\nepoch which has the same number of edges as E. VBM is trained by the contrastive learning of E and E(\u2212).\n\u210e1\n\u210e\ud835\udc57\n1\n|\ud835\udca9\ud835\udc56|\n1\n|\ud835\udca9\ud835\udc56|\n\u0d24\u210e\ud835\udc56\n\u210e1\n\u0d24\u210e\ud835\udc56\n\ud835\udfd0\n\u210e\ud835\udc57\n\u0d24\u210e\ud835\udc56\n\ud835\udfd0\n1\n|\ud835\udca9\ud835\udc56|\n1\n|\ud835\udca9\ud835\udc56|\n(a)\n(b)\n\ud835\udc63\ud835\udc4e\ud835\udc5f(\ud835\udc63\ud835\udc56)\n\ud835\udc90\ud835\udc8a\n\ud835\udc94\ud835\udc95\ud835\udc93\nFig. 5. (a) The MeanConv Layer. (b) The MinusConv Layer. We calculate\nthe variance of neighbor nodes\u2019 low-dimension latent representation by (a)\nand (b).\nA. Variance-based Model\nIn order to effectively detect structural outliers, we propose\na novel Variance-Based Model (VBM). To the best of our\nknowledge, this is the \ufb01rst time to utilize the variance of\nneighbors to detect outliers. Neighbor variance measures the\nconsistency of neighbor nodes. The bigger the variance, the\nless consistency it implies. In addition, our VBM has no bias\non nodes with larger node degrees.\nFeature Transformation. Before calculation of neighbor vari-\nance, we conduct the feature transformation f\u03b8(\u00b7) for the\noriginal attribute matrix X and get the low-dimension hidden\nrepresentation matrix H of nodes:\nH = f\u03b8(X)\n(5)\nwhere f\u03b8(\u00b7) denotes a neural network, such as MLP(\u00b7).\nThe ith row vector hi of the hidden representation matrix\nH denotes the latent representation of the ith node. In our\nexperiment, we implement it with a linear transformation and\nL2-normalization as :\n\u02c6H = XW + b\nhi =\n\u02c6\nhi\n\u2225\u02c6\nhi\u22252\n(6)\nwhere W \u2208Rd\u00d7dh and b \u2208Rdh are the learnable parameters,\nd and dh are the input dimension and hidden dimension of\nrepresentation, respectively.\nNeighbor Variance. In order to capture the consistency of\nneighbor nodes of a given node vi, we calculate the variance\nof attribute vectors of neighbor nodes for vi:\nhi =\n1\n|Ni|\nX\nj\u2208Ni\nhj\n(7)\nvar(vi) =\n1\n|Ni|\nX\nj\u2208Ni\n(hj \u2212hi)2\n(8)\nostr\ni\n= lossvar(vi) = \u2225var(vi)\u22251\n(9)\nwhere hi is the average of hidden representations of neighbor\nnodes of vi. The L1-norm of var(vi) is applied as structural\noutlier score ostr\ni\nfor node vi. Since all components of the\nvector var(vi) are greater than 0, the L1 norm calculation\nis to simply sum components of the var(vi). In order to\nef\ufb01ciently calculate variance for each node, we implement\nthe calculation of variance based on the message-passing\nscheme [35] and design two message-passing layers without\nparameters, namely MeanConv and MinusConv, as illustrated\nin Fig 5. Concretely, MeanConv is employed to calculate Eq.\n(7) and MinusConv is used to calculate Eq. (8) as well as Eq.\n(9).\nTrain. In order to train VBM to learn the representation that a\nnormal node has a low variance while a structural outlier has\na high variance, the train objection for VBM can be formally\nde\ufb01ned as follows:\nmin\n\u03b8\nEvi\u223cV[lossvar(vi)\u2212\n1\n|V\u2212Ni|\nX\nj /\u2208Ni\n(hj\u2212\n1\n|V\u2212Ni|\nX\nu/\u2208Ni\nhu)2]\n(10)\nwhere V\u2212Ni = V \\ Ni is the non-neighbor node set of vi.\nWe minimize the neighbor variance of a node while maxi-\nmizing the variance of hidden representations of all the non-\nneighbor nodes. In this case, the model will avoid generating\nthe same hidden representations for all nodes. However, it is\ntoo expensive to maximize the variance of all non-neighbor\nnodes every time. In this case, we apply negative edge sam-\npling each epoch to generate a network G(\u2212) whose edge set\nE(\u2212) has the same number of edges as E.\nDe\ufb01nition 3 (negative edge set). For a given attributed\nnetwork G = {V, E, X}, if E(\u2212) is the negative edge set of\nG, then if \u27e8u, v\u27e9\u2208E(\u2212) \u21d2\u27e8u, v\u27e9/\u2208E , u, v \u2208V.\nDe\ufb01nition 4 (negative network). For a given attibute network\nG = {V, E, X}, we de\ufb01ne the negative network G(\u2212) =\n{V, E(\u2212), X}, where E(\u2212) is the negative edge set of G.\nTherefore, we can utilize such a negative graph to maximize\nthe variance of unrelated nodes. In other words, we randomly\nsample the same number of negative neighbors for each node\nvi and maximize the neighbor variance calculated by these\nnegative neighbors. Instead of maximizing the variance of all\nnon-neighbor nodes, fewer nodes are required for computation\nby using negative sampling, which greatly saves time and\nspace.\nTherefore, for each node vi, it has the \u201crelated vs unrelated\u201d\nneighbor nodes pair, and corresponding lossvar(vi)(+) and\nlossvar(vi)(\u2212) can be calculated respectively. The contrastive\nlearning of neighbor nodes pair can be formalized as:\nloss(+)\nvar(vi) = \u2225var(vi, G)\u22251\nloss(\u2212)\nvar(vi) = \u2225var(vi, G(\u2212))\u22251\nlossstr(vi) = loss(+)\nvar(vi) \u2212loss(\u2212)\nvar(vi)\n(11)\nwhere var(vi, G) and var(vi, G(\u2212)) means we calculate the\nneighbor variance based on network G and G(\u2212), respectively.\nFinally, we minimize the above lossstr for all nodes in V\nas:\nmin\n\u03b8\nEv\u223cVlossstr(v)\n(12)\nThus, trained VBM can output a larger neighbor variance\nscore for nodes with unrelated neighbor nodes and a relatively\nsmall score for nodes with related neighbor nodes. Conse-\nquently, we can utilize VBM to detect structural outliers.\nCan neighbor variance help detect contextual outliers? We\nemploy a simple technique, self-loop edge, to make neighbor\nvariance have an effect on detecting contextual outliers besides\nstructural outliers. In speci\ufb01c, self-loop edges are added to all\nnodes as\n\u02c6\nNi = Ni \u222a{vi}, \u2200vi \u2208V\n(13)\nwhere Ni is the neighbor set of node vi. As the attributes of a\ncontextual outlier are signi\ufb01cantly different from its neighbors,\nneighbor variance of it would be increased greatly after adding\nthe self-loop neighbor when |Ni| (i.e. node degree of node vi)\nis small. This technique is optional and we employ it when\nthe average node degree of graph is small. Our experiment in\nSection VI-E5 studies the effect of this technique.\nB. Attribute Reconstruction Model\nWe employ attribute reconstruction in the detection of\ncontextual outliers. Our attribute reconstruction Model (ARM)\nis \ufb02exible that any popular GNN model can be used as the\nbackbone to reconstruct the attributes of nodes.\nFeature Transformation. Similar to VBM, we \ufb01rst transform\nthe original attribute matrix X to the low-dimension feature\nrepresentation matrix Z(0) as:\n\u02c6Z = XW \u2032 + b\u2032\nz(0)\ni\n=\n\u02c6zi\n\u2225\u02c6zi\u22252\n(14)\nwhere W \u2032 and b\u2032 are the learning parameters, z(0)\ni\nis the ith\nrow vector of Z(0).\nGNN Layers. Then we employ L GNN layers to transform\nZ(0) to Z(L) to fully absorb the message from neighbor nodes.\nThe lth GNN Layer can be formalized as:\nZ(l) = GNN (l)(Z(l\u22121), G)\n(15)\nwhere lth operator GNN (l)(\u00b7) can be implemented by any\npopular GNN model like GCN [27], GAT [28], GIN [30], and\nso on.\nFeature Retransformation. Finally, we retransform the Z(L)\nto \u02c6X, where \u02c6X \u2208R|V|\u00d7d has the same shape as original\nattributes matrix X.\n\u02c6X = Z(L) \u02c6W + \u02c6b\n(16)\nwhere\n\u02c6X is the reconstruction of the attribute matrix,\n\u02c6W\nand \u02c6b are the weight and bias parameters. Thus we can\nuse the reconstruction attribute matrix\n\u02c6X to calculate the\nreconstruction error, which is denoted as\noattr\ni\n= lossrecon(vi) = \u2225\u02c6xi \u2212xi\u22252\n(17)\nmin\n\u03b8\nEv\u223cVlossrecon(v)\n(18)\nBy minimizing the above objective, trained ARM can detect\ncontextual outliers.\nC. Outlier Detection\nAs mentioned in [9], current UNOD algorithms fail to\nhave balanced performance on two outlier types. The previous\npractice that combines contextual and structural loss with\na \ufb01xed weight during the training stage fails to balance\nthe optimization of model parameters. Similarly, during the\ninference stage, combing the contextual and structural score\nwith a \ufb01xed weight fails to achieve a balanced detection\nperformance. Therefore, we separately train our VBM and\nARM with different epochs to avoid unbalanced optimization.\nScore combination. After both VBM and ARM are well-\ntrained, we employ the mean-std normalization on two types\nof outlier scores outputted by two models and sum scores to\nget the \ufb01nal score, which can be formalized as:\nboi\nstr = ostr\ni\n\u2212\u00b5(Ostr)\nstd(Ostr)\nboi\nattr = oattr\ni\n\u2212\u00b5(Oattr)\nstd(Oattr)\noi = boi\nstr + boi\nattr\n(19)\nwhere Ostr and Oattr denote the set of structural outlier scores\nand contextual outlier scores, respectively. \u00b5(\u00b7) denotes the\nmean function, std(\u00b7) denotes standard deviation function.\nBy adopting Eq. (19) as the \ufb01nal outlier score, our model\ncan have a more balanced performance in detecting two types\nof outliers during the inference stage. The overall procedure\nof our VGOD framework is described in Algorithm 1.\nAlgorithm 1 The overall procedure of VGOD framework\nInput: Attributed Network: G = (V, E, X), Training epochs\nfor VBM: EpochV BM, Training epochs for ARM:\nEpochARM\nOutput: Well-trained VBM and ARM, outlier scores O\n1: // Training phase\n2: for i \u22081, 2, ..., EpochV BM do\n3:\nGenerate the negative network G(\u2212) = (V, E(\u2212), X)\nby negative edge sampling.\n4:\nCompute the neighbor variance of nodes in G(+) and\nG(\u2212) via Eq. (6)-(9).\n5:\nUpdate VBM with the loss function via Eq. (11).\n6: end for\n7: for i \u22081, 2, ..., EpochARM do\n8:\nCompute the reconstruction node attributes via Eq.\n(14)-(16).\n9:\nUpdate ARM with the loss function via Eq. (17).\n10: end for\n11: // Inference phase\n12: Compute the Ostr and Oattr via VBM and ARM, respec-\ntively.\n13: Compute the \ufb01nal outlier scores O via Eq. (19).\n14: return VBM, ARM, O\nD. Complexity Analysis\nThe complexity is mainly bounded by message-passing\nlayers. For simplicity, the number of layers and the number\nof dimensions are considered constant. The space and time\ncomplexity both are O(|E| + |V|). In addition, we are only\nusing GNN layers and two linear layers to build our model.\nSince there are a large number of research on extending GNNs\nto larger networks, we can make use of various mini-batch\ntraining techniques such as [29, 36, 37] to extend our model\nin a large-scale network without much effort.\nTABLE I\nDATASETS FOR UNOD EXPERIMENTS.\nDataset\n#nodes\n#edges\n#attrs\n#avg deg\n#outliers\u2217\n%outlier\u2217\nCora\n2,706\n5,429\n1,433\n2.01\n150\n5.5%\nCiteseer\n3,327\n4,732\n3,703\n1.42\n150\n4.5%\nPubMed\n19,717\n44,338\n500\n2.25\n600\n3.0%\nFlickr\n7,575\n239,738\n12,407\n31.65\n450\n5.9%\nWeibo\n8,405\n407,963\n64\n48.5\n868\n10.3%\n\u2217#outlier and %outlier are only for UNOD Experiments.\nVI. EXPERIMENT\nIn this section, we conduct experiments to illustrate the\neffectiveness of our proposed framework VGOD. Firstly, we\ndescribe the experiment settings including datasets, baselines,\nevaluation metrics, and computing infrastructures. Then, we\nconduct the Unsupervised Node Outlier Detection (UNOD)\nexperiment to validate the effectiveness of our framework.\nNext, we conduct structural outlier detection experiments\nunder different injection parameters and a new injection ap-\nproach, respectively. Finally, we make further analyses for our\napproach, including ef\ufb01ciency and ablation study. Additional\nexperiments are conducted in Appendix. Our code is available\nat https://github.com/goldenNormal/vgod-github.\nA. Experiment settings\n1) Datasets: We evaluate the proposed framework on \ufb01ve\nreal-world datasets for UNOD on attributed networks, includ-\ning four widely-used benchmark datasets with injected outliers\nand one dataset with labeled outliers. These datasets, shown\nin Table I, include three citation networks1 (Cora, Citeseer,\nPubMed) and two social networks (Flickr2 and Weibo3). Only\nthe Weibo dataset contains labeled outliers.\n2) Baselines: We compare our proposed framework VGOD\nwith the recent \ufb01ve deep-learning-based SOTA models. These\nbaselines are summarized in Table II. Column time complexity\nindicates the time complexity of inference. For simplicity, the\nnumber of layers and the number of dimensions are considered\nas constant. If the model outputs more than one score for\noutliers like VGOD, then we consider that it has the feature\nof score combination. If the hyperparameters of the model\n(e.g., the number of layer units) are not coupled to the number\nof nodes or edges of the training graph, then we regard it can\nperform inductive inference, which means a trained model can\nbe directly used for detecting outliers on a new graph with\nthe same attribute schema. Since none of the outlier labels\nis given in the training phase, our experiments are conducted\nin the transductive setting, which is consistent with existing\nunsupervised outlier detection works.\nDue to data leakage in the outlier injection approach, we\nalso design a simple baseline for comparison and evaluation,\nwhich only utilizes the leaked information (i.e. node degree\nand L2-norm of attribute vectors). We name it DegNorm.\n1https://linqs.org/datasets/\n2https://github.com/mengzaiqiao/CAN\n3https://github.com/zhao-tong/Graph-Anomaly-Loss/tree/master/data\nTABLE II\nTHE COMPARISON OF BASELINES.\nBaseline\nTime Complexity\nContrastive Learning\nReconstruction\nScore Combination\nInductive Inference\nDominant [4]\nO(|E| + |V|2)\n\u00d7\n\u2713\n\u2713\n\u2713\nAnamalyDAE [18]\nO(|E| + |V|2)\n\u00d7\n\u2713\n\u2713\n\u00d7\nDONE [34]\nO(|V|K)1\n\u00d7\n\u2713\n\u2713\n\u2713\nCoLA [16]\nO(c|V|R(c + \u03b4))2\n\u2713\n\u00d7\n\u00d7\n\u2713\nCONAD [19]\nO(|E| + |V|2)\n\u2713\n\u2713\n\u2713\n\u2713\nVGOD (Ours)\nO(|E| + |V|)\n\u2713\n\u2713\n\u2713\n\u2713\n1 K denotes the number of sampling neighbors for each node\n2 R denotes the number of sampling rounds, c denotes the number of nodes within the local subgraph, and \u03b4 denotes the average degree of the network.\nDegNorm adopts node degree as structural outlier score\nwhile L2-norm of attribute vectors of nodes are adopted as\ncontextual outlier score. The mean-std normalization is applied\nto two scores. The \ufb01nal outlier score is the sum of these two\nscores which have been normalized. The calculation of ostr\ni\nand oattr\ni\ncan be formalized as:\nostr\ni\n= |Ni|\noattr\ni\n= \u2225xi\u22252\n(20)\nwhere Ni is the neighbor node set of node vi, xi is the attribute\nvector of node vi.\n3) Evaluation Metrics: We use Area Under receiver op-\nerating characteristic Curve (AUC) to measure. In speci\ufb01c,\nAUC evaluates the degree of alignment between the outlier\nscore and the ground truth label under varying thresholds:\nAUC =\n1\n|V+||V\u2212|\nX\nv+\ni \u2208V+\nX\nv\u2212\nj \u2208V\u2212\n(I(f(v+\ni ) < f(v\u2212\nj )))\n(21)\nwhere V, V\u2212, and V+ = V \\ V\u2212are the set of all nodes,\nthe set of all outlier nodes, and the set of all normal nodes\nrespectively, I(\u00b7) is the indicator function and f(vi) is the\noutlier score of node vi given by one outlier detector. To\nexplore the utility of the model for different outliers, we extend\nthe concept of AUC. Generally, AUC(VL, O) means using VL\nas the set of outliers to be detected, O as the outlier scores\nto calculate the AUC. In other words, VL de\ufb01nes the outlier\nlabels. Particularly, AUC = AUC(V\u2212, O). In addition, if a\nmodel can output the structural and contextual outlier scores\nlike our VGOD, then AUC(V\u2212, Ostr) and AUC(V\u2212, Oattr)\ncan be calculated.\nWe also propose AucGap to evaluate the balanced detection\nperformance for different types of outliers, which can be\nformalized as below:\nAucGap = max{ AUC(Vstr, O)\nAUC(Vattr, O), AUC(Vattr, O)\nAUC(Vstr, O) }\n(22)\nwhere Vstr and Vattr are structural outliers set and contextual\noutliers set, respectively. AucGap aims to calculate the gap\nbetween the model\u2019s AUC score for two types of outliers.\nThe lower the AucGap is, the more balanced detection per-\nformance it indicates.\n4) Computing\nInfrastructures:\nOur\nproposed\nlearning\nframework is implemented using PyTorch 1.11.1 and PyTorch\nGeometric 2.1.0. All experiments are conducted on a computer\nwith Ubuntu 16.04 OS, i7-9750H CPU, and a Tesla V100\n(32GB memory) GPU.\nB. Unsupervised Node Outlier Detection\nWe \ufb01rst conduct the UNOD experiment to verify the ef-\nfectiveness of our proposed framework. UNOD experiment\nhereinafter refers to this experiment.\n1) Injection Setting: We adopt the most widely-used outlier\ninjection approach as mentioned in Section IV-A1 and Section\nIV-B1. We keep the same injection parameter setting with [4,\n16, 20, 21, 22, 23] to have a fair comparison (i.e., q = 15,\nk = 50 for all datasets and p = 5, 5, 20, 15 for Cora, Citeseer,\nPubMed, Flickr, respectively). The statistics of these datasets\nare demonstrated in Table I. Only Weibo contains the labeled\noutliers while other datasets contain injected outliers. Note that\nAucGap can only be calculated on these injected datasets.\n2) Parameter Setting: For each algorithm, we run 5 times\nand calculate the average score to list here. For our proposed\nframework VGOD, we \ufb01x the embedding dimension to 128\nfor both the variance-based model (VBM) and attribute re-\nconstruction model (ARM). We set the learning rate to 0.005\nfor all injected datasets and 0.01 for Weibo. Two layers of\nGAT are adopted as the GNN module in ARM and the row-\nnormalization to the attribute vectors is applied in Weibo. We\nemploy self-loop edge technique in Cora, Citeseer, PubMed,\nand Weibo for VGOD. We directly run the code in [16]\nto inject outliers. For all baselines, we adopt the default\nparameter setting in their code except the number of training\nepochs. We stop training their model as long as their AUC\nscore reaches its peak. In this case, the performance can be\npromised to be better or equal to the performance of their\ndefault parameter setting. For our approach, we train ARM\n100 epochs and VBM 10 epochs for all datasets since it has\nalready signi\ufb01cantly outperformed baselines in a \ufb01xed number\nof training epochs. In fact, our two models require fewer\nepochs to converge. Adam optimizer is employed to train our\nmodels. We adopt the AUC score of Weibo published in [9] for\nthe baseline Dominant, AnomalyDAE, DONE, and CONAD.\n3) Result Analysis: The AUC scores and AucGap scores\nare shown in Table IV and Table III. AUC(V str, O) and\nAUC(V attr, O) are listed in the column of str and context.\nTABLE III\nAUCGAP OF UNOD EXPERIMENT.\nModel\nCora\nCiteseer\nPubMed\nFlickr\nAucGap\nstr\ncontext\nAucGap\nstr\ncontext\nAucGap\nstr\ncontext\nAucGap\nstr\ncontext\nDominant\n1.312\n0.696\n0.913\n1.165\n0.755\n0.880\n1.652\n0.600\n0.990\n2.029\n0.486\n0.986\nAnomalyDAE\n1.161\n0.895\n0.771\n1.070\n0.864\n0.808\n1.118\n0.933\n0.834\n1.860\n0.521\n0.969\nDONE\n1.217\n0.922\n0.758\n1.016\n0.872\n0.886\n1.217\n0.836\n0.687\n1.557\n0.578\n0.900\nCoLA\n1.127\n0.943\n0.837\n1.188\n0.953\n0.802\n1.054\n0.954\n0.905\n1.395\n0.622\n0.868\nCONAD\n1.877\n0.513\n0.964\n2.236\n0.434\n0.972\n2.417\n0.404\n0.976\n2.066\n0.478\n0.987\nDegNorm\n1.132\n0.936\n0.827\n1.116\n0.979\n0.877\n1.093\n0.861\n0.941\n1.822\n0.527\n0.960\nVGOD\n1.072\n0.970\n0.905\n1.026\n0.986\n0.961\n1.021\n0.962\n0.983\n1.066\n0.838\n0.893\nTABLE IV\nAUC FOR UNOD EXPERIMENT.\nModel\nCora\nCiteseer\nPubMed\nFlickr\nWeibo\nDominant\n0.8134\n0.8250\n0.7999\n0.7440\n0.925\u2217\nAnomalyDAE\n0.8433\n0.8441\n0.8898\n0.7524\n0.928\u2217\nDONE\n0.8498\n0.8800\n0.7664\n0.7482\n0.887\u2217\nCoLA\n0.8790\n0.8861\n0.9214\n0.7530\n0.748\nCONAD\n0.7456\n0.7078\n0.6930\n0.7395\n0.927\u2217\nDegNorm\n0.8928\n0.9385\n0.9074\n0.7515\n0.893\nVGOD\n0.9503\n0.9845\n0.9813\n0.8773\n0.9765\n\u2217denotes the result reported in [9]\nThe best score is in bold while the second best is underlined.\nThe AUC scores lower than 0.7 are red and italicized in\nTable III. According to the results, we have the following\nobservations:\n\u2022 Our proposed framework VGOD achieves the highest\nAUC score for all datasets while achieving the overall\nhighest AucGap among all datasets. There are several\nreasons for such performance. Firstly, our variance-based\nmodel signi\ufb01cantly improves the ability to detect struc-\ntural outliers. Secondly, we separately train the model to\nprevent each component from being over-trained. Thirdly,\nwe adopt mean-std normalization to eliminate the scale\ndifference between the two scores which gives a more\nbalanced detection performance.\n\u2022 DegNorm also achieves SOTA performance compared to\nother baselines.\n\u2022 In Table III, it is observed that all baselines can not have\na good detection performance on structural outliers in the\nFlickr dataset and achieve a poorly balanced detection.\nThough the AucGap of VGOD in Citeseer is slightly\nlower than DONE, its detection performance is already bal-\nanced. Moreover, both AUC(V str, O) and AUC(V attr, O)\nof VGOD are much higher than that of DONE.\nC. Structural Outlier Detection under different injection pa-\nrameters\nFurther, we conduct the structural outlier detection experi-\nment with varied injection parameters to explore the effective-\nness of our variance-based model (VBM) in depth.\n1) Injection Setting: We vary the parameter q of injected\nclique size of structural outliers to Q = {3, 5, 10, 15}. For\neach dataset Di, we inject 4 groups of structural outliers\n{Vq=3, Vq=5, Vq=10, Vq=15} into Di. Each group has the same\nnumber of outliers, which is set to 2% of the total number\nof nodes, i.e. |Vq=Qi| = 2% \u00b7 |V|. The outlier set V\u2212is\nthe union of 4 groups of structural outliers set. We report\nthe AUC(V\u2212, Ostr) in Table V and the AUC score of each\ngroup AUC(V q=Qi, Ostr) is shown in Fig 6. Note that Ostr\nof VGOD is the output of VBM. The injected outliers are all\nstructural outliers.\n2) Parameter Setting: We keep the same parameter setting\nfor VBM and other baselines as the UNOD experiment except\nthat we train baselines and VBM until their AUC scores reach\nthe peak. Since we fail to get a reasonable result for CONAD,\nwe do not list the result of it. We also evaluate the performance\nof simple baseline Deg, which only utilizes the node degree\nas an outlier score for comparison. For all other baselines, if\ntheir model outputs multiple scores (e.g., oi, ostr\ni\n, oattr\ni\n), we\nadopt the score with the highest AUC as its structural score.\nTABLE V\nAUC FOR STRUCTURAL OUTLIER DETECTION UNDER DIFFERENT\nINJECTION PARAMETERS\nModel\nCora\nCiteser\nPubMed\nFlickr\nDominant\n0.9227\n0.9467\n0.8878\n0.5715\nAnomalyDAE\n0.9127\n0.9219\n0.8968\n0.6253\nDONE\n0.9034\n0.8985\n0.8868\n0.5516\nCoLA\n0.8073\n0.8919\n0.8698\n0.5712\nDeg\n0.9467\n0.9541\n0.9333\n0.5671\nVBM\n0.9815\n0.9816\n0.9893\n0.8003\n3) Result Analysis: According to results in Table V and Fig\n6, we have the following observations:\n\u2022 VBM achieves the best AUC score for all datasets in\nTable V. In addition, VBM has a huge performance gain\nin Flickr.\n\u2022 As shown in Fig 6, when the clique size is reduced, the\nperformance of VBM declines the least compared to other\nbaselines. Therefore, the performance of VBM is the most\nrobust to varied injection settings.\n\u2022 Deg that directly utilizes a node\u2019s degree outperforms\nother baselines in Cora, Citeseer, and PubMed.\nD. Structural Outlier Detection under a new injection ap-\nproach\nIn this subsection, we design a new approach to inject struc-\ntural outliers without data leakage. We conduct the following\n3\n5\n10\n15\nClique Size for Structural Outlier\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUC\nCora\nVBM\nAnomalyDAE\nDominant\nDONE\nCoLA\nDeg\n3\n5\n10\n15\nClique Size for Structural Outlier\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nCiteseer\nVBM\nAnomalyDAE\nDominant\nDONE\nCoLA\nDeg\n3\n5\n10\n15\nClique Size for Structural Outlier\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nPubMed\nVBM\nAnomalyDAE\nDominant\nDONE\nCoLA\nDeg\n3\n5\n10\n15\nClique Size for Structural Outlier\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nFlickr\nVBM\nAnomalyDAE\nDominant\nDONE\nCoLA\nDeg\nFig. 6. Comparison of the detection performance with the varying clique size parameter. Each polyline represents the AUC values of a model on several\ngroups of structural outliers with different clique sizes.\nexperiment for evaluation.\n1) Injection approach: Since all these datasets have cat-\negory labels for the node classi\ufb01cation task, it is natural to\nthink that nodes with different labels are from different com-\nmunities. In our opinion, structural outliers do not necessarily\nform clusters. We generate structural outliers by replacing their\noriginal neighbors (both in and out) with nodes uniformly\nsampled from other communities. In this manner, the degree\ndistribution of all outliers is not changed. The number of\noutliers is set as 10% of the number of nodes. The injected\noutliers are all structural outliers.\nTABLE VI\nAUC FOR STRUCTURAL OUTLIER DETECTION UNDER A NEW INJECTION\nAPPROACH\nCora\nCiteseer\nPubMed\nFlickr\nDominant\n0.838\n0.770\n0.853\n0.917\nAnomalyDAE\n0.770\n0.673\n0.566\n0.898\nDONE\n0.762\n0.664\n0.659\n0.541\nCoLA\n0.658\n0.743\n0.752\n0.632\nCONAD\n0.793\n0.770\n0.779\n0.495\nVBM\n0.935\n0.907\n0.858\n0.958\n2) Result Analysis: We keep the same parameter setting\nfor VBM and all baselines as the experiment in Section VI-C.\nTable VI lists the AUC(V\u2212, Ostr). Our VBM is still the most\neffective model, which outperforms others with a signi\ufb01cant\ngap. This further veri\ufb01es the effectiveness of neighbor variance\nto detect structural outliers.\nE. Further Analysis\nIn this subsection, we make further analyses of our proposed\nframework.\n1) Ef\ufb01ciency of model inference: We calculate the time for\neach model to use the CPU for training and inference at the\nsetting of the UNOD experiment. The training time per epoch\nof all models (in seconds) is shown in Fig 7. In Table VII, we\nlist the inference time in seconds. The inference time of the\nmodel is roughly the same as the training time per epoch,\nexcept for CoLA. For all datasets, our VGOD framework\ncompletes inference in a relatively short time. For datasets\nwith a large number of nodes, such as PubMed, our model\ntakes signi\ufb01cantly less time than other models due to the linear\nrelationship to the number of nodes. Since CoLA requires\nmultiple rounds of sampling for inference, its computational\ncost is much higher than other models.\nCora\nCiteseer\nPubMed\nFlickr\nDatasets\n0.0\n2.5\n5.0\n7.5\n10.0\n12.5\nSeconds\nDominant\nAnomalyDAE\nDONE\nCoLA\nCONAD\nVGOD\nFig. 7. Training time per epoch in seconds.\nTABLE VII\nINFERENCE TIME OF MODELS (IN SECONDS).\nModel\nCora\nCiteseer\nPubMed\nFlickr\nDominant\n0.102\n0.235\n3.021\n4.183\nAnomalyDAE\n0.147\n0.303\n4.390\n2.493\nDONE\n0.604\n0.865\n12.147\n5.256\nCoLA\n413\n752\n3266\n910\nCONAD\n0.093\n0.201\n2.823\n1.379\nVGOD\n0.088\n0.145\n0.874\n3.899\n2) Effect of the number of epochs for VBM: We investigate\nthe AUC variation trend of VBM during training. As shown\nin Fig 8, VBM shows a high AUC score at the beginning,\nand the AUC score reaches the peak after only a few epochs\nof training. Afterward, as the training progresses, the AUC\nscore slowly decreases due to over\ufb01tting. Different group of\nstructural outliers shows a similar trend while the group of\nsmaller clique size shows a later over\ufb01tting time point.\n3) Effect of different GNN Layers for ARM: We investigate\nthe effect of different GNN layers in ARM. We replace differ-\nent GNN layers in the UNOD experiment for research. Table\nVIII and Table IX show the AUC and AucGap respectively on\nfour datasets. It is observed that GAT outperforms other GNNs\nsigni\ufb01cantly on the Weibo. For other datasets, their AUC and\nAucGap scores are comparable.\n4) Labeled outlier study: We make the analysis on the\nWeibo dataset. We compare VGOD with the second best\nbaseline AnomalyDAE in Table X. It reveals that the main\nreason for VGOD\u2019s superior performance is its improvement in\nstructural outlier detection. It is shown in Fig 9(b) that outliers\ndo not have a higher node degree distribution. In addition, we\n(a)  Cora\n(b)  Citeseer\n(c)  PubMed\n(c)  Flickr\nFig. 8. AUC variation trend of the variance-based model during the training of VBM. Each polyline represents a group of structural outliers with a certain\nclique size.\nTABLE VIII\nAUC VALUES COMPARISON FOR DIFFERENT GNN LAYERS.\nModel\nCora\nCiteseer\nPubMed\nFlickr\nweibo\nVGOD (GIN)\n0.9503\n0.9845\n0.9801\n0.8773\n0.9093\nVGOD (GCN)\n0.9566\n0.9867\n0.9802\n0.8735\n0.9154\nVGOD (GAT)\n0.9560\n0.9868\n0.9813\n0.8835\n0.9765\nTABLE IX\nAUCGAP VALUES COMPARISON FOR DIFFERENT GNN LAYERS.\nModel\nCora\nCiteseer\nPubMed\nFlickr\nVGOD (GIN)\n1.0716\n1.0261\n1.0215\n1.0655\nVGOD (GCN)\n1.0637\n1.0278\n1.0214\n1.0713\nVGOD (GAT)\n1.0680\n1.0268\n1.0211\n1.0672\n\ufb01nd that attribute vectors of outliers are more diverse, as the\nvariance of attribute vectors among all outliers is 425.0 and\nthat of the inliers is 11.95.\nFrom Fig 9(a), we \ufb01nd that both inliers (green points)\nand outliers (red points) are quite cohesive. The homophily\n[38] of the whole graph is 0.75. Note that a random graph\nhas a homophily of 0. In this case, these outliers, which\ndiffer greatly from each other, are connected closely, forming\nclusters of structural outliers. Therefore, it is easily detected\nby the neighbor variance of VGOD. There are also a lot of\nclusters formed by inliers. They are not regarded as outliers\nsince their attributes are closed.\n(a)\n(b)\nFig. 9. (a) A subgraph in Weibo. The nodes in red denote the outliers. (b)\nNode degree distribution of Weibo.\nTABLE X\nAUC DETAIL IN THE WEIBO\nAUC\nAUC(V\u2212, Ostr)\nAUC(V\u2212, Oattr)\nVGOD\n0.977\n0.922\n0.926\nAnomalyDAE\n0.925\n0.796\n0.925\n5) Effect of the self-loop edge: We study the effect of the\nself-loop edge technique, which enables neighbor variance to\ndetect contextual outliers besides structural outliers. In the \ufb01rst\nstep, we study the effect of variance-based model to detect\ncontextual outliers. We inject only contextual outliers into\nthe datasets, using the same injection parameters as Section\nVI-B1. In Table XI and Table XII, \u201cw/ SL\u201d means employing\nthe self-loop edge technique. The result in Table XI shows\nthat neighbor variance with this simple technique indeed has\nan effect on detecting contextual outliers, especially in those\ncitation networks with small node degrees.\nTABLE XI\nAUC OF VBM TO DETECT CONTEXTUAL OUTLIERS\nCora\nCiteseer\nPubMed\nFlickr\nVBM\n0.5026\n0.5128\n0.4883\n0.4725\nVBM w/ SL\n0.7978 \u2191\n0.8567 \u2191\n0.8364 \u2191\n0.6463 \u2191\nTABLE XII\nAUC OF VGOD IN ABLATION OF SELF-LOOP EDGE\nCora\nCiteseer\nPubMed\nFlickr\nWeibo\nVGOD\n0.8911\n0.9485\n0.9592\n0.8773\n0.9707\nVGOD w/ SL\n0.9503 \u2191\n0.9845 \u2191\n0.9813 \u2191\n0.8313\n0.9765 \u2191\nIn the second step, we do the ablation study of this technique\nunder the UNOD experiment. Table XII demonstrates that self-\nloop edge also greatly improves the detection performance of\nVGOD in those citation networks due to the extra utilities of\nneighbor variance on contextual outlier detection.\nVII. CONCLUSION\nIn this paper, we revisit the problem of unsupervised node\noutlier detection. Firstly, we \ufb01nd that the current outlier\ninjection approach exists a serious data leakage issue and make\na theoretical analysis in depth. Secondly, we propose a new\nframework, which consists of a novel variance-based model\nand a more general attribute reconstruction model to detect\ntwo types of outliers. Our model successfully outperforms\nall previous SOTA models with the best outlier detection\nperformance and the detection balance.\nWe believe our insight into the data leakage issue will\nlead to better outlier injection approaches and algorithms for\nUNOD. Moreover, the concept of neighbor variance may also\nexhibit great potential in other research areas such as graph\nmining and graph representation learning in the future.\nREFERENCES\n[1] L. Akoglu, H. Tong, and D. Koutra, \u201cGraph based anomaly de-\ntection and description: a survey,\u201d Data mining and knowledge\ndiscovery, vol. 29, no. 3, pp. 626\u2013688, 2015.\n[2] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, \u201cEnhanc-\ning graph neural network-based fraud detectors against camou-\n\ufb02aged fraudsters,\u201d in Proceedings of the 29th ACM International\nConference on Information & Knowledge Management, 2020,\npp. 315\u2013324.\n[3] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong,\nand L. Akoglu, \u201cA comprehensive survey on graph anomaly\ndetection with deep learning,\u201d IEEE Transactions on Knowledge\nand Data Engineering, 2021.\n[4] K. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly\ndetection on attributed networks,\u201d in Proceedings of the 2019\nSIAM International Conference on Data Mining. SIAM, 2019,\npp. 594\u2013602.\n[5] L. Guti\u00b4errez-G\u00b4omez, A. Bovet, and J.-C. Delvenne, \u201cMulti-scale\nanomaly detection on attributed networks,\u201d in Proceedings of\nthe AAAI conference on arti\ufb01cial intelligence, vol. 34, no. 01,\n2020, pp. 678\u2013685.\n[6] G. Pang, C. Shen, L. Cao, and A. V. D. Hengel, \u201cDeep learning\nfor anomaly detection: A review,\u201d ACM Computing Surveys\n(CSUR), vol. 54, no. 2, pp. 1\u201338, 2021.\n[7] C. Hou, S. He, and K. Tang, \u201cRosane: Robust and scalable\nattributed network embedding for sparse networks,\u201d Neurocom-\nputing, vol. 409, pp. 231\u2013243, 2020.\n[8] L. Akoglu, H. Tong, and D. Koutra, \u201cGraph based anomaly de-\ntection and description: a survey,\u201d Data mining and knowledge\ndiscovery, vol. 29, no. 3, pp. 626\u2013688, 2015.\n[9] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding,\nC. Chen, H. Peng, K. Shu et al., \u201cBond: Benchmarking un-\nsupervised outlier node detection on static attributed graphs,\u201d\nin Thirty-sixth Conference on Neural Information Processing\nSystems Datasets and Benchmarks Track, 2022.\n[10] H. Tong and C.-Y. Lin, \u201cNon-negative residual matrix fac-\ntorization with application to graph anomaly detection,\u201d in\nProceedings of the 2011 SIAM International Conference on\nData Mining.\nSIAM, 2011, pp. 143\u2013153.\n[11] D. Chakrabarti, \u201cAutopart: Parameter-free graph partitioning\nand outlier detection,\u201d in European conference on principles\nof data mining and knowledge discovery.\nSpringer, 2004, pp.\n112\u2013124.\n[12] D. Koutra, T.-Y. Ke, U. Kang, D. H. P. Chau, H.-K. K. Pao,\nand C. Faloutsos, \u201cUnifying guilt-by-association approaches:\nTheorems and fast algorithms,\u201d in Joint European Conference\non Machine Learning and Knowledge Discovery in Databases.\nSpringer, 2011, pp. 245\u2013260.\n[13] S. Thudumu, P. Branch, J. Jin, and J. J. Singh, \u201cA comprehensive\nsurvey of anomaly detection techniques for high dimensional\nbig data,\u201d Journal of Big Data, vol. 7, no. 1, pp. 1\u201330, 2020.\n[14] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip,\n\u201cA comprehensive survey on graph neural networks,\u201d IEEE\ntransactions on neural networks and learning systems, vol. 32,\nno. 1, pp. 4\u201324, 2020.\n[15] S. Pouyanfar, S. Sadiq, Y. Yan, H. Tian, Y. Tao, M. P. Reyes,\nM.-L. Shyu, S.-C. Chen, and S. S. Iyengar, \u201cA survey on\ndeep learning: Algorithms, techniques, and applications,\u201d ACM\nComputing Surveys (CSUR), vol. 51, no. 5, pp. 1\u201336, 2018.\n[16] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis,\n\u201cAnomaly detection on attributed networks via contrastive self-\nsupervised learning,\u201d IEEE transactions on neural networks and\nlearning systems, vol. 33, no. 6, pp. 2378\u20132392, 2021.\n[17] K. Ding, J. Li, N. Agarwal, and H. Liu, \u201cInductive anomaly\ndetection on attributed networks,\u201d in Proceedings of the Twenty-\nNinth International Conference on International Joint Confer-\nences on Arti\ufb01cial Intelligence, 2021, pp. 1288\u20131294.\n[18] H. Fan, F. Zhang, and Z. Li, \u201cAnomalydae: Dual autoencoder\nfor anomaly detection on attributed networks,\u201d in ICASSP 2020-\n2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP).\nIEEE, 2020, pp. 5685\u20135689.\n[19] Z. Xu, X. Huang, Y. Zhao, Y. Dong, and J. Li, \u201cContrastive\nattributed network anomaly detection with data augmentation,\u201d\nin Paci\ufb01c-Asia Conference on Knowledge Discovery and Data\nMining.\nSpringer, 2022, pp. 444\u2013457.\n[20] K. Ding, J. Li, and H. Liu, \u201cInteractive anomaly detection\non attributed networks,\u201d in Proceedings of the twelfth ACM\ninternational conference on web search and data mining, 2019,\npp. 357\u2013365.\n[21] X. Yuan, N. Zhou, S. Yu, H. Huang, Z. Chen, and F. Xia,\n\u201cHigher-order structure based anomaly detection on attributed\nnetworks,\u201d in 2021 IEEE International Conference on Big Data\n(Big Data).\nIEEE, 2021, pp. 2691\u20132700.\n[22] M. Jin, Y. Liu, Y. Zheng, L. Chi, Y.-F. Li, and S. Pan,\n\u201cAnemone: graph anomaly detection with multi-scale con-\ntrastive learning,\u201d in Proceedings of the 30th ACM International\nConference on Information & Knowledge Management, 2021,\npp. 3122\u20133126.\n[23] Y. Zheng, M. Jin, Y. Liu, L. Chi, K. T. Phan, and Y.-P. P. Chen,\n\u201cGenerative and contrastive self-supervised learning for graph\nanomaly detection,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2021.\n[24] F. Liu, X. Ma, J. Wu, J. Yang, S. Xue, A. Beheshti, C. Zhou,\nH. Peng, Q. Z. Sheng, and C. C. Aggarwal, \u201cDagad: Data\naugmentation for graph anomaly detection,\u201d arXiv preprint\narXiv:2210.09766, 2022.\n[25] S. Kaufman, S. Rosset, C. Perlich, and O. Stitelman, \u201cLeakage\nin data mining: Formulation, detection, and avoidance,\u201d ACM\nTransactions on Knowledge Discovery from Data (TKDD),\nvol. 6, no. 4, pp. 1\u201321, 2012.\n[26] D. W. Hosmer, S. Lemeshow, and R. Sturdivant, \u201cArea under\nthe receiver operating characteristic curve,\u201d Applied Logistic\nRegression. Third ed: Wiley, pp. 173\u2013182, 2013.\n[27] T. N. Kipf and M. Welling, \u201cSemi-supervised classi\ufb01ca-\ntion\nwith\ngraph\nconvolutional\nnetworks,\u201d\narXiv\npreprint\narXiv:1609.02907, 2016.\n[28] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY. Bengio, \u201cGraph attention networks,\u201d stat, vol. 1050, p. 20,\n2017.\n[29] W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representa-\ntion learning on large graphs,\u201d Advances in neural information\nprocessing systems, vol. 30, 2017.\n[30] K. X. W. H. J. Leskovec and S. Jegelka, \u201cHow powerful are\ngraph neural networks,\u201d ICLR. Keyulu Xu Weihua Hu Jure\nLeskovec and Stefanie Jegelka, 2019.\n[31] B. Weisfeiler and A. Leman, \u201cThe reduction of a graph to\ncanonical form and the algebra which appears therein,\u201d NTI,\nSeries, vol. 2, no. 9, pp. 12\u201316, 1968.\n[32] J. Li, H. Dani, X. Hu, and H. Liu, \u201cRadar: Residual analysis\nfor anomaly detection in attributed networks.\u201d in IJCAI, 2017,\npp. 2152\u20132158.\n[33] Z. Peng, M. Luo, J. Li, H. Liu, and Q. Zheng, \u201cAnomalous:\nA joint modeling approach for anomaly detection on attributed\nnetworks.\u201d in IJCAI, 2018, pp. 3513\u20133519.\n[34] S. Bandyopadhyay, S. V. Vivek, and M. Murty, \u201cOutlier resistant\nunsupervised deep architectures for attributed network embed-\nding,\u201d in Proceedings of the 13th international conference on\nweb search and data mining, 2020, pp. 25\u201333.\n[35] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E.\nDahl, \u201cNeural message passing for quantum chemistry,\u201d in\nInternational conference on machine learning.\nPMLR, 2017,\npp. 1263\u20131272.\n[36] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh,\n\u201cCluster-gcn: An ef\ufb01cient algorithm for training deep and large\ngraph convolutional networks,\u201d in Proceedings of the 25th ACM\nSIGKDD international conference on knowledge discovery &\ndata mining, 2019, pp. 257\u2013266.\n[37] H. Zeng, M. Zhang, Y. Xia, A. Srivastava, A. Malevich,\nR. Kannan, V. Prasanna, L. Jin, and R. Chen, \u201cDecoupling\nthe depth and scope of graph neural networks,\u201d Advances in\nNeural Information Processing Systems, vol. 34, pp. 19 665\u2013\n19 679, 2021.\n[38] D. Lim, F. Hohne, X. Li, S. L. Huang, V. Gupta, O. Bhalerao,\nand S. N. Lim, \u201cLarge scale learning on non-homophilous\ngraphs: New benchmarks and strong simple methods,\u201d Advances\nin Neural Information Processing Systems, vol. 34, pp. 20 887\u2013\n20 902, 2021.\nAPPENDIX A\nADDITIONAL EXPERIMENTS\nA. Effect of score combination Strategy\nWe study the effectiveness of mean-std normalization employed\nin VGOD. We replace the score combination strategy with weighted\nsum and \u201csum-to-unit\u201d normalization on the UNOD experiment.\nNormalization of \u201csum-to-unit\u201d can be formalized as:\nboi\n(k) =\no(k)\ni\nP\nj\u2208|V| o(k)\nj\noi =\nX\nk\u2208K\nboi\n(k)\n(23)\nwhere o(k)\ni\nmeans the kth outlier score of node vi outputted by model\nand o(k)\ni\nin Eq. (23) should be greater than 0.\nTABLE XIII\nAUC FOR DIFFERENT SCORE COMBINATION ON THE UNOD\nEXPERIEMENT\nModel\nCora\nCiteseer\nPubMed\nFlickr\nWeibo\nVGOD (mean-std)\n0.956\n0.987\n0.981\n0.883\n0.976\nVGOD (weight)\n0.919\n0.859\n0.982\n0.729\n0.942\nVGOD (sum-to-unit)\n0.935\n0.957\n0.981\n0.850\n0.970\nTABLE XIV\nAUCGAP FOR DIFFERENT SCORE COMBINATION ON THE UNOD\nEXPERIEMENT\nModel\nCora\nCiteseer\nPubMed\nFlickr\nVGOD (mean-std)\n1.0680\n1.0268\n1.0211\n1.0672\nVGOD (weight)\n1.0781\n1.3641\n1.0095\n1.9662\nVGOD (sum-to-unit)\n1.1716\n1.1133\n1.0000\n1.2241\nIn Table XIII and Table XIV, both the AUC and AucGap scores\nof VGOD with mean-std normalization are signi\ufb01cantly superior to\nothers on Cora, Citeseer, Flickr and only slightly inferior to the\nhighest score on PubMed.\nB. Effect of Inductive Inference\nWe extend the UNOD experiment to the inductive setting here.\nFirst, we use the datasets in the UNOD experiment to train all\nalgorithms in Section VI-B. Then, we inject outliers with a different\nrandom seed but the same approach to generate a new group of\ndatasets for evaluation of all algorithms. Other settings are consistent\nTABLE XV\nAUC FOR THE UNOD EXPERIMENT IN THE INDUCTIVE SETTING\nModel\nCora\nCiteseer\nPubMed\nFlickr\nDominant\n0.8531\n0.8755\n0.8089\n0.7545\nDONE\n0.9110\n0.9545\n0.8362\n0.7794\nCoLA\n0.7698\n0.8133\n0.9076\n0.6570\nCONAD\n0.7139\n0.7074\n0.6817\n0.7536\nDegNorm\n0.8873\n0.9350\n0.9120\n0.7642\nVGOD\n0.9693\n0.9840\n0.9783\n0.8977\nwith the UNOD experiment. Note that AnomalyDAE cannot perform\ninductive inference.\nTable XV and XVI show the AUC and AucGap scores for these\nbaselines and our VGOD. The result is similar to the transductive\nsetting, where our VGOD outperforms other baselines signi\ufb01cantly.\nThe performance of our VGOD is even better than the performance\nin the transductive setting. The reason is owing to the case that the\nover\ufb01tting of issue is eliminated in the inductive setting.\nTABLE XVI\nAUCGAP FOR THE UNOD EXPERIMENT IN THE INDUCTIVE SETTING\nModel\nCora\nCiteseer\nPubMed\nFlickr\naucgap\nstr\ncontext\naucgap\nstr\ncontext\naucgap\nstr\ncontext\naucgap\nstr\ncontext\nDominant\n1.379\n0.709\n0.977\n1.286\n0.758\n0.975\n1.617\n0.615\n0.994\n1.961\n0.504\n0.989\nDONE\n1.223\n0.990\n0.809\n1.116\n0.996\n0.892\n1.302\n0.940\n0.722\n1.701\n0.571\n0.971\nCoLA\n1.058\n0.741\n0.784\n1.246\n0.894\n0.718\n1.102\n0.945\n0.857\n1.243\n0.582\n0.723\nCONAD\n2.030\n0.467\n0.948\n2.245\n0.433\n0.972\n2.578\n0.379\n0.978\n1.968\n0.503\n0.989\nDegNorm\n1.191\n0.953\n0.800\n1.104\n0.971\n0.879\n1.099\n0.863\n0.948\n1.759\n0.548\n0.964\nVGOD\n1.020\n0.965\n0.946\n1.000\n0.973\n0.973\n1.021\n0.961\n0.981\n1.033\n0.871\n0.900\n",
    "2308.10918": "1\nLabel-based Graph Augmentation with Metapath for\nGraph Anomaly Detection\nHwan Kim, Junghoon Kim, Byung Suk Lee, and Sungsu Lim, Member, IEEE\nAbstract\u2014Graph anomaly detection has attracted considerable\nattention from various domain ranging from network security to\nfinance in recent years. Due to the fact that labeling is very costly,\nexisting methods are predominately developed in an unsupervised\nmanner. However, the detected anomalies may be found out\nuninteresting instances due to the absence of prior knowledge\nregarding the anomalies looking for. This issue may be solved\nby using few labeled anomalies as prior knowledge. In real-\nworld scenarios, we can easily obtain few labeled anomalies.\nEfficiently leveraging labelled anomalies as prior knowledge\nis crucial for graph anomaly detection; however, this process\nremains challenging due to the inherently limited number of\nanomalies available. To address the problem, we propose a novel\napproach that leverages metapath to embed actual connectivity\npatterns between anomalous and normal nodes. To further effi-\nciently exploit context information from metapath-based anomaly\nsubgraph, we present a new framework, Metapath-based Graph\nAnomaly Detection (MGAD), incorporating GCN layers in both\nthe dual-encoders and decoders to efficiently propagate context\ninformation between abnormal and normal nodes. Specifically,\nMGAD employs GNN-based graph autoencoder as its back-\nbone network. Moreover, dual encoders capture the complex\ninteractions and metapath-based context information between\nlabeled and unlabeled nodes both globally and locally. Through\na comprehensive set of experiments conducted on seven real-\nworld networks, this paper demonstrates the superiority of the\nMGAD method compared to state-of-the-art techniques. The code\nis available at https://github.com/missinghwan/MGAD.\nIndex Terms\u2014Graph anomaly detection, graph neural net-\nwork, node anomaly, attributed graph\nI. INTRODUCTION\nG\nRAPH anomaly detection aims to identify nodes that\nsignificantly deviate from the majority, and has drawn\nmuch attention from various domain [2], [14], such as fraud\ndetection [47] and IoT network intrusion detection [49]. Early\nworks [29], [18], [28] on graph anomaly detection have been\nlargely dependent on domain knowledge and statistical meth-\nods. Recently, deep learning approaches have proved that they\ncan effectively handle large-scale high-dimensional data and\nextract patterns from the data, thereby achieving satisfactory\nperformance without the burden of handcrafting features [42],\n[17], [14]. However, it could not effectively handle complex\ninteractions and attribute information on attributed graph [36].\nH. Kim and S. Lim are with the Department of Computer Science and\nEngineering, Chungnam National University, Daejeon 34134, Republic of\nKorea (e-mail: hwan.kim@o.cnu.ac.kr, sungsu@cnu.ac.kr).\nJ. Kim is with the Department of Computer Science and Engineering, Ulsan\nNational Institute of Science & Technology, Ulsan 44191, Republic of Korea\n(e-mail: junghoon.kim@unist.ac.kr)\nB. S. Lee is with the Department of Computer Science, University of\nVermont, Burlington, Vermont 05405, USA (e-mail: bslee@uvm.edu).\n(Corresponding author: Sungsu Lim.)\nGraph neural networks (GNNs), more recently, have been\nadopted to efficiently and intuitively detect anomalies from\ngraphs due to the highly expressive capability via the message\npassing mechanism in learning graph representations (e.g., [5],\n[20]). The message passing mechanism in GNNs treats all\nmessages equally without considering their relative importance\nor relevance [15]. Frequently, particular messages or neigh-\nboring nodes may carry more valuable information [41], [44].\nHence, efficiently capturing these important messages or nodes\ncan be a key to an advancement for detection anomalies.\nExisting methods are predominantly developed in an un-\nsupervised manner for graph anomaly detection [29], [18],\n[28], [8], [5], [20], [21], [27], [22] since labeling anomalies\nare labor-intensive and require specific domain knowledge.\nAmong the existing methods, most approaches are based on\ngraph autoencoder (GAE) [5], [20], [28], [8], [27], [22], which\nmeasures the abnormality of each node with its reconstruction\nerrors. Unfortunately, the detected anomalies may be found\nto be noises or data instances that we are not interested in\nand one of possible reasons can be the absence of the prior\nknowledge about anomalies we are looking for [7], [6], [38].\nWithout the prior knowledge or anomaly-aware information,\nit may be less effective to obtain the results that we exactly\nwant from unsupervised approaches [38]. The above issue\ncan be solved by using a few or limited number of labeled\nanomalies as the prior information to catch anomaly-informed\nfeatures [23], [25]. In practice, it is often recommended to\nutilize as much labeled data as possible [1]. Furthermore, in\nreal-world scenarios, we might be able to collect a small set\nof labeled anomalies [23], [7].\nBy using few anomaly labeled data as prior knowledge, the\nissues from unsupervised approaches could be resolved [23].\nEven though how effectively using few labeled information\nis important, it still remains non-trivial mostly due to the\nfollowing issues: (1) Precise abnormality characterization:\nsince there is only limited amount of labeled information, it is\ndifficult to accurately characterize the abnormal patterns [7].\nAs the number of anomalies used are increased, the abnormal\npatterns are characterized more easily and precisely and pre-\nvious semi-supervised methods [26], [16] show performance\nenhancement by using more anomalies. Nonetheless, due to\nthe fact that labeling cost is high and it is hard to obtain\nenough labels in real world, a method for handling as few\nlabeled anomalies as possible to learn high-level abstraction of\nabnormal patterns is necessary. In addition, utilizing abundant\nunlabeled information is also helpful in learning the high-level\nabnormal features. 2) Capturing valuable representation: how\nto efficiently capture valuable normality/abnormality represen-\narXiv:2308.10918v2  [cs.LG]  12 Apr 2024\n2\nNormality \nSelf-attention\nAbnormality \nSelf-attention\nX \u2013 O \u2013 X\nAnomaly  (O)\nUnknown (X)\nX \u2013 X \u2013 X\nO \u2013 X \u2013 O\nO \u2013 O \u2013 O\nEncoding\nEncoding\nEncoding\nEncoding\n\ud835\udc89\ud835\udc8f\ud835\udfcf\n\ud835\udc89\ud835\udc8f\ud835\udfd0\n\ud835\udc89\ud835\udc82\ud835\udfd1\n\ud835\udc89\ud835\udc82\ud835\udfd2\nFig. 1: An example of how attention mechanism is achieved\nby using metapath-based context information.\ntations with a small amount of labeled data is also one of\nthe major challenges [25]. Various research studies [31], [16],\n[26] attempt to distinguish the more important features and\nto leverage label information in semi-supervised manner. The\nperformance of these methods is prone to decrease as fewer\nlabels are leveraged. Hence, there is a need for a carefully\ndesigned model, which exhibits stable performance with fewer\nlabels and captures representative characteristics of labeled and\nunlabeled information.\nTo alleviate aforementioned challenges, we first propose\ngraph augmentation algorithm to embed actual linking patterns\nbetween normal and anomalous nodes in a graph by using\nboth metapath schema and node label types (unknown or\nabnormal). Particularly, based on label types, we sample\nanomaly subgraphs with designed metapath schema, which\ncan enhance normality- and abnormality-specific information\nin the form of self-attention as shown in Figure 1. Hence,\nthe anomaly subgraphs contain higher-level context and re-\nlation information between each type of nodes. Moreover,\nthe procedures of sampling subgraphs effectively augment\nlabel augmentation and it helps reducing class imbalance\nproblem as listed in Table I. To further effectively exploit\nthe context information from the anomaly subgraphs, we\nintroduce a well-designed framework Metapath-based Graph\nAnomaly Detection (MGAD). Specifically, MGAD employs\nGNN-based graph autoencoder as its backbone network and\ndual encoders, which capture the complex interactions as well\nas metapath-based context information between labeled and\nunlabeled nodes from entire graph and anomaly subgraph.\nMetapath-based context information. As shown in Fig-\nure 1, there are multiple metapaths, which represent differ-\nent relations between unknown (blue) and/or abnormal (or-\nange) nodes, such as Unknown-Anomaly-Unknown (X-O-X),\nUnknown-Unknown-Unknown (X-X-X), Anomaly-Unknown-\nAnomaly (O-X-O), and Anomaly-Anomaly-Anomaly (O-O-\nO). Hence, O-X-O and O-O-O represent two different relations\nbetween anomalous nodes. X-X-X and O-O-O express the\npatterns of unknown and anomalous nodes respectively. X-\nO-X expresses the patterns of anomalies in unknown nodes,\nand O-X-O represents the patterns of unknown nodes in\nanomalies. By incorporating multiple metapaths, features be-\ntween unknown nodes and anomalous nodes become more\ndistinguishable and we can leverage specific context infor-\nmation to detect anomalies. In addition, attention mechanism\nallows a model to dynamically focus on relevant metapaths\nand adaptively weigh the importance of different metapaths\nbased on its relevance, enabling more context-aware anomaly\ndetection as described in Figure 1. The metapaths and attention\nmechanism mechanism can lead to more accurate and effective\ndetection performance compared to the previous GNN-based\ngraph anomaly detection approaches.\nIn summary, our main contributions are listed below:\n\u2022 To the best of our knowledge, the proposed method is the\nfirst attempt to adopt metapath-based context information\nand attention mechanism for graph anomaly detection. The\ncombination of metapath-based context information and\nattention mechanism enables normal and abnormal represen-\ntations to be more distinguishable. Also, it is first attempt\nto adopt metapath for homogeneous graph.\n\u2022 We effectively augment label data by iterative sampling\nstrategy of metapaths and this strategy alleviates class im-\nbalance problem in graph anomaly detection.\n\u2022 We conduct extensive experiments on seven real-world\ndatasets to show the performance of our framework. The\nresults demonstrate the superiority of our method by com-\nparing with the state-of-the-art approaches.\nII. RELATED WORK\nIn this section, we present methods for graph anomaly\ndetection, ranging from traditional methods to state-of-the-\nart GNN-based approaches, in both unsupervised and semi-\nsupervised settings. Additionally, the summary of state-of-the-\nart methods is listed in Table I.\nTraditional Methods. The research area of graph anomaly\ndetection has been attracting significant attention in recent\ntimes. The earlier methods [3], [45] consider only the structure\nor attribute to detect anomalous nodes in a graph. Afterward,\nCODA [9] presents a probabilistic model to detect anomalous\nnodes on communities. AMEN [29] builds the ego-graphs\nwith each node in order to detect anomalous communities on\nattributed graphs. Additionally, other traditional approaches\n[24], [30], [33], [32] attempt to detect anomalies by using\nattribute information. Also, some research [18], [28] employs\nresidual analysis techniques and ANOMALOUS [28] com-\nbines the residual analysis and CUR decomposition in order\nto detect anomalous nodes.\nUnsupervised GNN-based Methods. With the advent of\nGNNs [12], [15], [41], various GNN-based approaches for\ngraph anomaly detection have been presented with skyrock-\neting growth of GNN techniques. DOMINANT [5] firstly\nadopts GCN-based GAE, which encodes structure and attribute\ninformation and then decodes the structure and attribute fea-\ntures respectively for anomalous node detection. CoLA [21]\nproposes contrastive self-supervised learning for anomalous\nnode detection by sampling local subgraphs and comparing the\nsubgraphs with target nodes. However, these above methods\nmay not effectively work on real-world networks since the\npatterns of anomalies are much more irregular. SpecAE [20]\nattempts to detect global and community anomalies by using\nLaplacian sharpening technique to alleviate over-smoothing\n3\nTABLE I: The usage of information of previous GNN-based methods for graph anomaly detection.\nMethods\nLocal\nInformation\nGlobal\nInformation\nContext\nInformation\nGlobal\nAnomaly\nLocal\nAnomaly\nClass\nImbalance\nUnsupervised\nAMEN [29]\n\u221a\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nRadar [18]\n\u221a\n\u221a\n\u221a\n\u221a\n\u221a\n\u00d7\nANOMALOUS [28]\n\u221a\n\u221a\n\u00d7\n\u221a\n\u221a\n\u00d7\nDOMINANT [5]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nSpecAE [20]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u221a\n\u00d7\nHCM [13]\n\u221a\n\u221a\n\u221a\n\u00d7\n\u00d7\n\u00d7\nAnomalyDAE [8]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nResGCN [27]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nCoLA [21]\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u221a\nConGNN [19]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u221a\nLHML [11]\n\u221a\n\u00d7\n\u00d7\n\u00d7\n\u221a\n\u00d7\nComGA [22]\n\u221a\n\u221a\n\u00d7\n\u221a\n\u221a\n\u00d7\nBWGNN [37]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nGHRN [10]\n\u221a\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u221a\nSemi-supervised\nDeepSAD [31]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nGDN [7]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u00d7\nSemi-GNN [16]\n\u00d7\n\u221a\n\u00d7\n\u221a\n\u00d7\n\u221a\nMGAD (ours)\n\u221a\n\u221a\n\u221a\n\u221a\n\u221a\n\u221a\nproblem of convolution mechanism. HCM [13] considers both\nglobal and local contextual anomalies on the basis of hop\ncount-based subgraphs as self-supervised manner. ComGA\n[22] captures global, local, and structure anomalies by utilizing\nstructure information of community structure, attribute and\nstructure features of the whole graph.\nSemi-supervised GNN-based Methods. Several methods us-\ning graphs in semi-supervised way have been presented and\nthese methods propagate labelled information to unlabelled\nnodes\n[34], [48], [50]. Although GNN-based node classifi-\ncation methods in a semi-supervised way have achieved huge\nsuccess [12], [15], [43], [46], these methods do not consider\nthe class imbalance issue in the graph anomaly detection\ntask. There are only a few GNN-based approaches detecting\nanomalies in semi-supervised manner. Kumagai et al. present\na simple GCN-based embedding method in semi-supervised\nmanner for graph anomaly detection [16]. Meta-GDN [7]\nintroduce Graph Deviation Networks (GDN), which leverages\na few labels to deviate abnormal nodes from normal nodes,\nwith cross-network meta-learning algorithm. In this paper,\nwe propose GNN-based anomaly detection method in semi-\nsupervised manner with metapath-based context information.\nIII. PROBLEM STATEMENT\nIn this section, we formally define key concepts and present\nthe problem statement.\nDefinition 1. Attributed Graph. An attributed graph can be\ndenoted as G = (V, E, X) where V is a set of nodes V =\n{v1, v2, \u00b7 \u00b7 \u00b7 , vn} with |V | = n, E is a set of edges with |E| =\nm, and X \u2208Rn\u00d7d denotes node attributes. The i-th row of\nvector xi \u2208Rd(i = 1, . . . , n) is the attribute information of\nthe node vi. Additionally, adjacency matrix A, where Aij = 1\ndenotes connectivity between node vi and vj, if Aij = 0, then\nthere is no connectivity each other.\nDefinition 2. Metapath. A metapath P is a path in the form of\nV1\nR1\n\u2192V2\nR2\n\u2192\u00b7 \u00b7 \u00b7\nRl\n\u2192Vl+1, which defines a composite relation\nR = R1 \u25e6R2 \u25e6\u00b7 \u00b7 \u00b7 \u25e6Rl between types of nodes V1 and Vl+1,\nwhere \u25e6denotes the composition operator on relations.\nDefinition 3. Metapath-based Anomaly Subgraph. A meta-\npath based anomaly subgraph is a set of sampled metapath\nTS = (AS, RS) on given metapath schema TG = (A, R),\nwhere AS \u2286A and RS \u2286R. Types of an anomaly subgraph\nTS are either anomaly type O or unknown type X. The\nanomaly subgraph is recursively sampled with random walk\nsampling strategy.\nAs graph anomaly detection is commonly formulated as a\nranking problem [2], we formally define the metapath-based\ngraph anomaly detection problem as follows:\nProblem 1. Metapath-based Anomaly Detection. Given an\nattributed graph G = (V, E, X) with a set of nodes V =\n{v1, v2, \u00b7 \u00b7 \u00b7 , vn}, the task is to score each node with a scor-\ning function ki = f(vi) by preserving the metapath-based\ncontext information. The anomaly score ki is the degree of\nabnormality of node vi. Based on the score, anomalous nodes\ncan be detected in accordance with the ranking.\nIV. THE PROPOSED APPROACH\nHere, we present three modules of our framework: (1)\nSubgraph Generation Module, (2) Graph Embedding Module,\nand (3) Anomaly Detection Module, as in Figure 2.\nA. Subgraph Generation Module\nIn this module, the anomaly subgraph is generated on the\nbasis of node types of O and X with metapath schema.\nAfterward, the subgraph is encoded by multiple GCN layers to\nextract important information, such as O-X relations, structure,\nand attribute features in the subgraph.\nSpecifically, some of labelled anomaly nodes are set to type\nO and the others of the remaining nodes are set to type X\nsince it is uncertain which node is either anomalous or not.\nOn the basis of metapath schema, which contains at least\none O type, each node is randomly sampled by random walk\n4\n1\n2\n3\nEncoder\nGCN\nEncoder\nGCN\n+\nEmbedding \nVectors \ud835\udc4d\n1\nX \u2013 A \u2013 X \u00b7\u00b7\u00b7\nA \u2013 X \u2013 A\n1\n2 \u00b7\u00b7\u00b7\nStructure \nDecoder\nError \nScoring\n2\n1\n3\n\u00b7\u00b7\u00b7\n0.81\n0.76\n0.72\n0.2\nWhole \nGraph\nGraph Embedding Module\nSubgraph Generation Module\nAttribute \nDecoder\nAnomaly Detection Module\nGCN\nGCN\n+ : Concatenation\n(a) : Anomaly subgraph a\n(b) : Anomaly subgraph b\n1\n2\n3\n1\n2\n(a)\n(b)\nAnomaly \nScore\n\u302e\u302e\u302e\n\u302e\u302e\u302e\nFig. 2: Framework of MGAD.\nsampling strategy. The sampled nodes will be an anomaly\nsubgraph, which represents normal-abnormal patterns as well\nas structure, attribute information in the subgraph as defined\nin Definition 3. For example, given a metapath schema, X-\nO-X, every matching pattern is sampled from whole graph. If\nthere are many links between nodes, then a random walker will\nrandomly pick one node among linked nodes with probability\nP. The randomwalk-based sampling can be described as\nfollows:\nPAl,Al+1 = D\u22121\nAl,Al+1WAl,Al+1\n(1)\nwhere WAl,Al+1 is the adjacency matrix between nodes in\ntype Al and nodes in type Al+1, and DAl,Al+1 is the degree\nmatrix with DAl,Al+1 = P WAl,Al+1(vi, vj). A random\nwalker samples each type of node on the basis of the probabil-\nity P following the number of sampling round n. For example,\nif a random walker starts sampling from a node vi in type Al,\nthe next node will be vj in type Al+1 with the probability\nPAl,Al+1(vi, vj). The entire procedures of subgraph genera-\ntion are described in Algorithm 1. Specifically, we iteratively\nextract sub-structures, which match with designed metapath\nschema. In line 5, we select one of the neighbors from target\nnode uniformly at random.\nNote that the proposed anomaly subgraph guarantees that\nthere is at least one anomaly in each subgraph and it suffi-\nciently represents anomalous patterns between normal and ab-\nnormal. These information in subgraph will be efficiently en-\ncoded by multi-stacked GCN layers and will enhance anomaly\ndetection performance by making normal and abnormal more\ndistinguishable. Each GCN layer can be shown as follows:\nH(l)\ni\n= \u03d5( \u02dcD\n\u22121\n2\ni\n\u02dcAi \u02dcD\n\u22121\n2\ni\nH(l\u22121)\ni\nW(l\u22121))\n(2)\nwhere \u02dcAi = Ai + I is the subgraph adjacency with self-loop,\n\u02dcDi is the degree matrix of local subgraph, Wl\u22121 is the weight\nmatrix of the (l \u22121)-th layer, and \u03d5 is the activation function,\nsuch as ReLU and Sigmoid.\nB. Graph Embedding Module\nIn this module, GCN-based encoder, which is the same as\nthe encoder in subgraph Generation Module, embeds structure\nand attribute information of the attributed graph. The multiple\nAlgorithm 1: Anomaly subgraph generation\ninput: The attributed graph G = (V, E, X),\n# walks per node n,\nWalk length l,\nMetapath schema P\nOutput: Anomaly subgraph C\n1 C \u2190\u2205;\n2 for v \u2208V do\n3\ni \u21900;\n4\nwhile i \u0338= n do\n5\nH \u2190rw(v, G, l);\n6\nif type(H) \u2208P then\n7\nappend(C, H);\n8\nend\n9\ni \u2190i + 1;\n10\nend\n11 end\n12 return C\nGCN layers in the encoder extract global attribute and structure\nfeature.\nSpecifically, the GCN layers capture non-linearity of net-\nwork and complex relations on attributed networks as de-\nscribed in Equation 2. The embedding representation Z(l) is\nconcatenated with representation H(l) of embedded anomaly\nsubgraph and it composes embedding vector Z, which consists\nof X-O linking patterns and information of node attribute and\nstructure in anomaly subgraph and whole graph as described\nin Equation 3.\nZ = Z(l) \u2295H(l)\n(3)\nConsequently, the embedding vector Z represents global, local,\nand X-O connecting features as well as attribute and structure\ncharacteristics.\nC. Anomaly Detection Module\nIn this module, two GCN-based decoders reconstruct struc-\ntures and attributes embedding vector Z respectively. Subse-\nquently, anomalous nodes are detected on the basis of anomaly\nscores.\nStructure Decoder. In this section, we explain how the\nstructure decoder reconstructs the original graph structure with\nlatent representations Z. The reconstruction error of structure\nis described as Rstruc = A \u2212\u02c6A, where \u02c6A denotes estimated\nadjacency matrix. Additionally, if a node shows low probabil-\nity of being anomalous, the structure decoder well reconstructs\ntopological information of the graph. On the other hand, if the\ndecoder properly reconstructs the topological information, then\nits topology does not follow majority structures in a graph.\nThus, Rstruc(i,:) expresses that ith node has a higher anomaly\nprobability on the perspective of network structure. We use\nstructure decoder with the latent representation Z as input to\nreconstruct the graph structure:\n\u02c6A = sigmoid(ZZT)\n(4)\n5\nSpecifically, given the latent representations as input, the\ndecoder predicts whether there is a link between each pair\nof two nodes as follows:\np( \u02c6Ai,j = 1|zi, zj) = sigmoid(zizT\nj )\n(5)\nAttribute Decoder. In this section, we describe how this\nattribute decoder reconstructs the original node attributes of the\ngraph with Z. The reconstruction error of attribute is expressed\nas Rattr = X \u2212\u02c6X, where \u02c6X denotes predicted attributes. The\nattribute decoder employs GCN layers to predict the original\nattributes as follows:\n\u02c6X = \u03d5(Z, A, W)\n(6)\nAnomaly Detection. In order to learn the structure and\nattribute reconstruction errors, the objective function can be\ndefined as:\nL = (1 \u2212\u03b1)||A \u2212\u02c6A||2\nF + \u03b1||X \u2212\u02c6X||2\nF\n(7)\nwhere \u03b1 is a balance parameter, which controls importance\nbetween structure and attribute reconstruction errors. The joint\nreconstruction errors are used to measure anomalous nodes by\nminimizing the objective function above. The anomaly score\nof each node vi is described as follows:\nscore(vi) = (1 \u2212\u03b1)||a \u2212\u02c6ai||2 + \u03b1||xi \u2212\u02c6xi||2\n(8)\nNote that we simply consider the unlabeled nodes V as normal\nnodes [26], [7]. Outstandingly, MGAD shows novel and robust\nperformance on this straightforward strategy.\nD. Time Complexity Analysis\nThe time complexity of generating Metapath-based anomaly\nsubgraph is O(|V | \u00b7 n \u00b7 l). The outer loop runs for each node\nv in the graph G, resulting in a time complexity of O(|V |),\nwhere |V | is the number of nodes in the graph. The inner loop\nruns n times (1 \u2264n \u22645) for each node v. Inside the inner\nloop:\n\u2022 A random walk H\nis generated using the function\nrw(v, G, l), where l is the walk length (3 \u2264l \u22645).\n\u2022 The type of the walk H is checked against the metapath\nschema P, and if it matches, the walk is added to anomaly\nsubgraph C.\nGenerating a random walk using the rw(v, G, l) function in-\nvolves traversing the graph with a specified length l. Assuming\nthe time complexity of generating a random walk is O(l) in\nthe worst case, and the metapath type check is constant time\n(since the metapath schema is typically a small fixed-size set\nof metapaths), the overall time complexity of the inner loop\nis O(n \u2217l). Therefore, the overall time complexity of the\nalgorithm is O(|V | \u00b7 n \u00b7 l).\nV. EXPERIMENTS\nIn this section, we conduct an extensive experiment on seven\nreal-world networks to demonstrate the superiority of MGAD.\nDatasets. Among the seven datasets, only Amazon dataset\nprovides ground-truth label and the other six datasets pro-\nvide injected anomalies. Previous methods for graph anomaly\nTABLE II: The statistics of the datasets.\nDatasets\n|V |\n|E|\n|X|\n|A|\nGround-truth\nAmazon\n1,418\n3,695\n28\n28\nInjected\nAnomaly\nCora\n2,708\n5,429\n1,433\n150\nCiteseer\n3,327\n4,732\n3,703\n150\nPubmed\n19,717\n44,338\n500\n600\nBlogCatalog\n5,196\n171,742\n8,189\n300\nFlickr\n7,575\n239,738\n12,074\n450\nACM\n16,484\n71,980\n8,337\n600\nTABLE III: AUC values (%) on seven datasets. The best and\nsecond highest performances are boldfaced and underlined,\nrespectively.\nMethods\nCora\nCiteseer\nPubmed\nBlogCatalog\nFlickr\nACM\nAMEN\n47.00\n62.66\n61.54\n77.13\n65.73\n56.26\nRadar\n65.87\n67.09\n62.33\n74.01\n73.99\n72.47\nANOMALOUS\n57.70\n63.07\n73.16\n72.37\n74.34\n70.38\nDOMINANT\n81.55\n82.51\n80.81\n74.68\n74.42\n76.01\nAnomalyDAE\n76.27\n72.71\n81.03\n78.34\n75.08\n75.13\nResGCN\n84.79\n76.47\n80.79\n78.50\n78.00\n76.80\nCoLA\n87.79\n89.68\n95.12\n78.54\n75.13\n82.37\nComGA\n88.40\n91.67\n92.20\n81.40\n79.90\n84.96\nConGNN\n89.05\nOOM\n87.26\nOOM\nOOM\nOOM\nLHML\n84.40\n80.20\n83.50\n72.60\n70.60\n82.10\nBWGNN\n48.63\n43.11\n70.60\n42.57\n78.91\n64.71\nGHRN\n67.91\n57.60\n79.07\n80.42\n83.06\n75.52\nDeepSAD\n56.70\n54.60\n51.10\n54.80\n53.90\n53.00\nGDN\n83.17\n85.48\n84.72\n70.90\n71.03\n78.12\nSemi-GNN\n67.65\n81.74\n66.30\n84.42\n73.39\n84.90\nMGAD (ours)\n92.29\n94.69\n92.52\n81.48\n79.96\n89.87\ndetection mostly utilized the injected anomaly networks for\ntheir experiments [4], [5], [8], [20], [21] since the ground-\ntruth information is rare in real-world datasets [13], [27], [28].\nSince there is no ground-truth in other datasets, we employ two\nanomaly injection techniques used in previous research [21],\n[35] to generate a comprehensive set of anomalies for each\ndataset by perturbing topological structure and node attributes,\nrespectively. Specifically, when injecting attribute anomalies,\nwe randomly choose n nodes and then change their features\nwith randomly selected node\u2019s features. Afterward, the equal\nnumber of structural anomalies are injected by selecting n\nnodes and forming the chosen nodes fully connected. To\ndemonstrate the performance of MGAD, we conduct exper-\niments on one ground-truth network and six injected anomaly\nnetworks [5], [21], [7]. The statistics of these networks are\nlisted in Table II, and more details are given as follows:\n\u2022 Ground-truth anomaly graphs [13], [27], [28]: Amazon is\na co-purchase network, and an attributed network with\nground-truth. The nodes labelled as amazonfail are repre-\nsented as anomalous nodes.\n\u2022 Injected anomaly graphs [4], [5], [8], [20], [21], [7]: ACM,\nCora, Citeseer, Pubmed, BlogCatalog, and Flickr are the\nsix attributed networks with the injected anomalies. The\nfirst four datasets (ACM, Cora, Citeseer, and Pubmed) are\nwidely used citation networks. Each node represents paper,\nand each edge expresses citation relation with other papers,\nrespectively. The title and abstract information of papers are\nused as attributes of the nodes. BlogCatalog and Flickr are\nthe two attributed networks and common social networks.\nEach node denotes the website user, and each edge repre-\n6\n3 4 5\n63.5\n64\n64.5\n65\nAUC\nAmazon\n3 4 5\n89\n89.2\n89.4\n89.6\n89.8\nACM\n3 4 5\n90\n91\n92\nCora\n3 4 5\n93\n93.5\n94\n94.5\nCiteseer\n3 4 5\n91.5\n92\n92.5\nPubmed\n3 4 5\n81.44\n81.46\nBlogCatalog\n3 4 5\n79.8\n79.85\n79.9\n79.95\nFlickr\nFig. 3: AUC values on different length of metapath l.\n1 3 5\n63.5\n64\n64.5\n65\nAUC\nAmazon\n1 3 5\n89\n89.2\n89.4\n89.6\n89.8\nACM\n1 3 5\n90\n91\n92\nCora\n1 3 5\n93\n93.5\n94\n94.5\nCiteseer\n1 3 5\n91.5\n92\n92.5\nPubmed\n1 3 5\n81.44\n81.46\nBlogCatalog\n1 3 5\n79.8\n79.85\n79.9\n79.95\nFlickr\nFig. 4: AUC values by sampling round n.\n1% 3% 5%10%\n89.7\n89.8\nAUC\nACM\n1% 3% 5%10%\n90\n91\n92\nCora\n1% 3% 5%10%\n92\n93\n94\nCiteseer\n1% 3% 5%10%\n91.5\n92\n92.5\nPubmed\n1% 3% 5%10%\n79.8\n79.82\n79.84\n79.86\n79.88\nFlickr\nFig. 5: Results following the number of anomalies (ratio of anomaly).\nsents the relations between users, respectively. User-related\ntag information, such as posting blogs or sharing photos is\nemployed as node attributes.\nBaselines. We introduce eight famous unsupervised and three\nsemi-supervised methods for graph anomaly detection to com-\npare with our proposed framework MGAD.\n\u2022 AMEN [29] extracts both attribute and structure features to\ndetect anomalies in ego-network by comparing correlations\nof nodes.\n\u2022 Radar [18] analyzes residuals and coherence of network\ninformation for graph anomaly detection.\n\u2022 ANOMALOUS [28] uses CUR decomposition and residual\nto detect node anomalies.\n\u2022 DOMINANT [5] is based on GAE to extract node represen-\ntation for graph anomaly detection.\n\u2022 AnomalyDAE [8] extracts structure and attribute features by\nusing two autoencoders.\n\u2022 ResGCN [27] adopts attention mechanism in residuals to\ndetect anomalous node with GCN.\n\u2022 CoLA [21] is a GNN-based framework using contrastive\nlearning to learn relations between target nodes and sub-\ngraphs.\n\u2022 HCM [13] is a novel self-supervised learning method with\nhop-based sub-structures of graphs.\n\u2022 DeepSAD [31] is a deep learning-based method for semi-\nsupervised anomaly detection.\n\u2022 GDN [7] employs a small set of anomaly labels to discover\nsignificant deviations between abnormal and normal nodes\nwith cross-network meta-learning.\n\u2022 Semi-GNN [16] embeds nodes on hyper-sphere space with\nGCN layer.\n\u2022 ComGA [22] uses community-specific representations with\nGCN layers to consider local anomalies.\n\u2022 BWGNN [37] employs spectral and spatial localized band-\npass filters to better handle the \u2018right-shift\u2019 phenomenon in\ndetecting anomalies.\n\u2022 ConGNN [19] uses a newly designed diffusion model to\naugment graphs in AD.\n\u2022 LHML [11] is a few-shot approach to detect nodes that sig-\nnificantly deviate from the majority in learnable hypersphere\nspace.\n\u2022 GHRN [10] uses homophilic and heterophilic connections\nwith inter-class edge pruning.\nAnomaly Detection Results. In this experiment, anomaly de-\ntection performance of our MGAD is evaluated and compared\nwith three semi-supervised methods and eight unsupervised\nmethods, respectively. For the evaluation metric, we employ\nAUC values, which estimates the probability that a randomly\nselected anomalous node has higher ranking than a normal\nnode. The anomaly detection results are shown in Table III,\nand on the basis of the performance results, we present the\nfollowing observation and analysis:\n\u2022 Among the seven datasets, the proposed MGAD shows the\nbest performance on four datasets, and the second highest\non two datasets. Specifically, the best result of MGAD in\ncomparison with the baseline methods is 4.91% higher than\nsecond highest method on ACM dataset. For the result\non Amazon and Pubmed datasets, our MGAD achieves\n7\ncomparable performance. MGAD performs less efficiently\nin capturing representative features from networks, which\ncontain relatively small number of anomalies and attributes,\nsuch as Amazon and Pubmed datasets. Since the context\ninformation is derived from attribute features as well as\nstructure information, the small amount of attributes and\nanomalies contain relatively less context information in\nAmazon. It may be the reason that MGAD performs ineffec-\ntively in Amazon. CoLA method uses contextual subgraphs\nin contrastive manner and these techniques seem to work\neffectively on Pubmed.\n\u2022 When comparing with the shallow deep learning-based\nmethods in unsupervised manner such as AMEN, Radar,\nand ANOMALOUS and previous early GNN-based meth-\nods such as DOMINANT, AnomalyDAE, and ResGCN,\nour MGAD outperforms these previous approaches. The\nproposed anomaly subgraph makes anomalous node charac-\nteristics more distinct, and dual-encoders and multi-stacked\nGCN layers in the encoder and decoder efficiently extract\nvaluable node features.\n\u2022 For social networks, MGAD shows limited performance.\nThis could be largely attributed to the synthetic nature\nof the anomalies. Structural anomalies involve forming\nsmall, densely interconnected cliques by linking unrelated\nnodes [5], [21], [22]. It is important to note that the\nselection of \u2019unrelated nodes\u2019 is random, which inherently\ncarries a probability of mistakenly choosing nodes that\nare, in fact, related. Identifying such inadvertent \u2019noises\u2019\nposes a challenge, as they may show similar patterns to\nnormal nodes. To provide empirical support for this ob-\nservation, we conduct an analysis using the Normalized\nMutual Information (NMI). NMI is a statistical measure\nand it compares the similarity between two sets of data,\nin this case, the anomalies detected by our model versus\nComGA. A higher NMI value indicates a greater degree\nof similarity between the two sets, implying that they find\nsimilar anomalies. By analyzing the NMI, which yielded\na result of 0.96 for BlogCatalog and 0.94 for Flickr, we\ninfer that both models likely detect similar anomalies. In\nother words, they might fail to identify anomalies that are\ninherently more elusive. Based on these findings, we infer\nthat our model is effective in GAD, barring the presence of\npotential noise elements. Based on these findings, we infer\nthat our model is effective in anomaly detection, barring the\npresence of potential noise elements. The pronounced effect\nof this phenomenon in social networks could be attributed\nto inherent characteristics unique to these networks, such as\nthe small world phenomenon [39].\nParameter Study. In this section, we verify the effectiveness\nof metapath length l and sampling round n.\n\u2022 Effect of metapath length l: In this experiment, we investi-\ngate effect of metapath length l, which is set to 3, 4, and 5.\nThe size of anomaly subgraph mainly depends on l. When\nl is larger, the subgraph size is larger too. The performance\nchanges according to each l are depicted in Figure 3. We\ndiscover a tendency that the smallest anomaly subgraph\n(length 3) achieves the best performance on most datasets,\n(a) MGAD (ours)\n(b) GDN\n(c) SemiGNN\n(d) ComGA\nFig. 6: Visualization of node embedding on Cora. The red\nand blue points represent anomalous and normal nodes, re-\nspectively.\nexcept for Amazon and BlogCatalog. Amazon reaches the\nhighest performance, when the length is 5, the largest\nsubgraph. The possible reasons is that the small subgraph on\nthe large graphs may not contain enough patterns between\nanomaly and normal. On the other hand, the large subgraph\non small graphs consists of much unnecessary information\nsince the subgraph and graph become alike. As a result,\nonce metapath-based subgraph is used, regardless of its size\nor length, a novel performance is guaranteed.\n\u2022 Effect of the number of sampling rounds n: In this section,\nwe further conduct more experiments in order to prove ef-\nfectiveness of metapath-based context information between\nabnormal and normal nodes. The number of sampling round\nn is set to 1, 3, and 5. The effect for each n value is shown\nin Figure 4. On most datasets, when value of n is 5, the\nresults show better performance when it comes to max and\naverage AUC values. The one of reasons is that training data\nis augmented enough according to the number of sampling\nrounds and through this data augmentation, the subgraph\ncontains more patterns and context information between\nabnormal and normal nodes for training. On extremely\ndense network, Flickr, the meaningful context might not\nbe extracted for better anomaly detection. Consequently,\nmore samplings, more metapath context information, can\nbring about noticeable performance improvement, since\nevery linking pattern can be sampled around target anomaly\nnodes and this sampled subgraph can have rich context\ninformation.\nSensitivity Analysis. In this experiment, we explore sensitiv-\nity and robustness of our method according to the ratio of\nanomalies on five datasets. We conduct this experiments on\ndifferent ratio of anomalies, such as 1%, 3%, 5%, and 10%\nof all labeled anomalies. As shown in Figure 5, performance\n8\nTABLE IV: AUC values (%) on Amazon and average AUC values on seven datasets, respectively.\nAmazon\nAMEN Radar ANOMALOUS DeepSAD CoLA LHML AnomalyDAE Semi-GNN ConGNN ResGCN GDN\nComGA DOMINANT BWGNN GHRN MGAD\nAUC\n47.00\n58.00\n60.20\n56.10\n47.26\n67.60\n53.61\n66.95\n61.89\n71.00\n68.15\n63.07\n62.50\n58.70\n51.09\n65.01\nAvg.\nAUC\n59.61\n67.68\n67.31\n54.31\n79.41\n77.28\n73.16\n75.05\n79.40\n78.05\n77.36\n83.08\n76.06\n58.17\n70.66\n85.11\nTABLE V: Effect of different communities.\nCora\nCiteSeer\nPubmed\nMGAD w/o subgraph\n90.21\n88.53\n88.82\nMGAD w ego-net\n89.72\n87.38\n89.14\nMGAD w anomaly subgraph\n92.29\n94.69\n92.52\nof MGAD shows its robustness to changes of the number\nof anomalies used. Although its performance tends to go\nhigher on ACM and Citeseer, AUC variance on ACM is only\n0.1% and performance become stable with 3% of anomalies\non Citeseer. In addition, the social network, Flickr, reports\ndecreasing performance as the number of anomaly is larger.\nThe possible reason is that Flickr network is much denser than\nthe above-mentioned citation networks. Since each abnormal\nnode is intricately connected with normal node on the social\nnetworks, more information can be extracted from relatively\nsmaller number of anomalies. In this reason, employing more\nanomalies on extremely dense networks become unnecessary\ninformation. Therefore, MGAD performs very stable and\nrobust in spite of using the only small number of anomalies.\nVisualization Analysis. In this section, we present visual-\nization analysis about learned node representations in order\nto evaluate our method. For visualization, we employ t-\ndistributed stochastic neighbor embeddings [40] (t-SNE) to\nreduce the dimensions of the node embeddings from 128 to 2.\nFigure 6 shows the visualization of node embeddings learned\nby MGAD, GDN, SemiGNN, and ComGA on Cora. Our\nmethod, MGAD, is capable of discriminating the abnormal and\nnormal node representations efficiently. GDN and SemiGNN\nare not able to effectively separate the abnormal and normal\nnode representations, since its model is not deep enough to\nexploit the valuable features from the networks. Consequently,\namong the four methods, our method, MGAD, shows the most\nappropriate node representations for graph anomaly detection.\nAblation Study. We conduct the ablation experiments for the\nsubgraph generation module (e.g., 1-hop ego-net). The results\nof ablation study are as listed in Table V. We study the effect\nof changing communities in the subgraph Generation Module\non three datasets (Cora, CiteSeer, Pubmed) and carry out\nthe experiments on three possible scenarios: MGAD without\nsubgraph, MGAD with Ego-network (1-hop), and MGAD\nwith the anomaly subgraph. When MGAD uses the proposed\nsubgraph, it shows the best performance. Specifically, our\nmethod effectively augments the few number of label data\nvia label reuse and enriches context information via attention\nmechanism in generating anomaly communities.\nRunning Efficiency. We also analyze the running efficiency\nwith other methods on CiteSeer dataset as shown in Figure 7.\nIn Figure 7 (left), running time of CoLA and ComGA es-\nCoLA\nDOMINANT\nComGA\nBWGNN\nGHRN\nMGAD\n0\n100\n200\n300\nTime (sec)\nCoLA\nDOMINANT\nComGA\nBWGNN\nGHRN\nMGAD\n40\n60\n80\nAUC (%)\nFig. 7: Running efficiency of MGAD and baselines.\npecially reports relatively very long running time, because\nextracting communities and subgraphs require high compu-\ntational costs. Although the other methods record fast running\ntime, these AUC values, Figure 7 (right), are not higher\nthan our MGAD. Consequently, MGAD shows very efficient\nrunning time and the the highest AUC value, while effectively\ndemonstrating the best running efficiency among the baselines.\nCase Study. We further examine effecitveness of MGAD\non real-world dataset, Amazon, as listed in Table IV. These\nresults show that a simple method efficiently detects real-\nworld anomalies and MGAD reports limited performance.\nThere is no perfect method, which shows the best perfor-\nmance in all scenarios or datasets. Furthermore, it is an\nopen challenge to efficiently model real-world networks for\nanomaly detection [23]. In general, it is non-trivial to achieve\nnovel performance with a fresh idea. Nonetheless, we suc-\ncessfully adopt metapath, which is used for heterogeneous\ngraph, for homogeneous graph with an idea and improve\noverall detection accuracy as shown in Table IV (Avg. AUC).\nIf the MGAD results are excluded, winner on each dataset is\ndifferent (e.g., ComGA is winner on ACM), since strong points\nare quite different from each method. On the other hand, in\noverall perspective, MGAD shows the best average AUC and\nthe highest or second highest performance on most datasets,\neffectively proving its superiority.\nVI. CONCLUSION\nIn this paper, we introduce graph augmentation method to\ncapture connectivity patterns between normal and abnormal\nnodes in a graph by using both metapath and labels. We\nalso present a novel metapath-based graph anomaly detection\nframework, MGAD, which uses GCN layers in both encoder\nand decoder. Specifically, we design metapath-based context\ninformation to propagate characteristics between abnormal\nand normal nodes. Moreover, the designed anomaly subgraph\neffectively augments label information via the number of\nsampling rounds. Our model efficiently captures the context\ninformation from anomaly subgraphs and whole graph and\n9\neffectively learns distinct differences in structures and at-\ntributes both globally and locally. The promising results of\nthis study pave the way for future investigations, focusing on\nthe optimization and analysis of metapath patterns.\nREFERENCES\n[1] Charu C Aggarwal and Charu C Aggarwal. An introduction to outlier\nanalysis. Springer, 2017.\n[2] Leman Akoglu, Hanghang Tong, and Danai Koutra.\nGraph-based\nanomaly detection and description: A survey. Data Mining and Knowl-\nedge Discovery, 29(3):626\u2013688, 2015.\n[3] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00a8org\nSander.\nLOF: Identifying density-based local outliers.\nIn SIGMOD,\npages 93\u2013104, 2000.\n[4] Kaize Ding, Jundong Li, Nitin Agarwal, and Huan Liu.\nInductive\nanomaly detection on attributed networks. In IJCAI, pages 1288\u20131294,\n2020.\n[5] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu.\nDeep\nanomaly detection on attributed networks.\nIn SDM, pages 594\u2013602,\n2019.\n[6] Kaize Ding, Jianling Wang, James Caverlee, and Huan Liu.\nMeta\npropagation networks for graph few-shot semi-supervised learning. In\nAAAI, volume 36, pages 6524\u20136531, 2022.\n[7] Kaize Ding, Qinghai Zhou, Hanghang Tong, and Huan Liu. Few-shot\nnetwork anomaly detection via cross-network meta-learning. In WWW,\npages 2448\u20132456, 2021.\n[8] Haoyi Fan, Fengbin Zhang, and Zuoyong Li.\nAnomalyDAE: Dual\nautoencoder for anomaly detection on attributed networks. In ICASSP,\npages 5685\u20135689, 2020.\n[9] Jing Gao, Feng Liang, Wei Fan, Chi Wang, Yizhou Sun, and Jiawei\nHan. On community outliers and their efficient detection in information\nnetworks. In KDD, pages 813\u2013822, 2010.\n[10] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng,\nand Yongdong Zhang. Addressing heterophily in graph anomaly de-\ntection: A perspective of graph spectrum. In WWW, pages 1528\u20131538,\n2023.\n[11] Qiuyu Guo, Xiang Zhao, Yang Fang, Shiyu Yang, Xuemin Lin, and\nDian Ouyang. Learning hypersphere for few-shot anomaly detection on\nattributed networks. In CIKM, pages 635\u2013645, 2022.\n[12] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation\nlearning on large graphs. In NIPS, pages 1024\u20131034, 2017.\n[13] Tianjin Huang, Yulong Pei, Vlado Menkovski, and Mykola Pechenizkiy.\nHop-count based self-supervised anomaly detection on attributed net-\nworks. In ECML/PKDD, pages 225\u2013241, 2022.\n[14] Hwan Kim, Byung Suk Lee, Won-Yong Shin, and Sungsu Lim. Graph\nanomaly detection with graph neural networks: Current status and\nchallenges. IEEE Access, 10:111820\u2013111829, 2022.\n[15] Thomas N Kipf and Max Welling. Semi-supervised classification with\ngraph convolutional networks. In ICLR, 2017.\n[16] Atsutoshi Kumagai, Tomoharu Iwata, and Yasuhiro Fujiwara.\nSemi-\nsupervised anomaly detection on attributed graphs. In IJCNN, pages\n1\u20138, 2021.\n[17] Martin L\u00a8angkvist, Lars Karlsson, and Amy Loutfi. A review of unsu-\npervised feature learning and deep learning for time-series modeling.\nPattern Recognition Letters, 42:11\u201324, 2014.\n[18] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu.\nRadar: Residual\nanalysis for anomaly detection in attributed networks. In IJCAI, pages\n2152\u20132158, 2017.\n[19] Xuan Li, Chunjing Xiao, Ziliang Feng, Shikang Pang, Wenxin Tai, and\nFan Zhou. Controlled graph neural networks with denoising diffusion\nfor anomaly detection. Expert Systems with Applications, 237:121533,\n2024.\n[20] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou.\nSpecAE: Spectral autoencoder for anomaly detection in attributed net-\nworks. In CIKM, pages 2233\u20132236, 2019.\n[21] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George\nKarypis.\nAnomaly detection on attributed networks via contrastive\nself-supervised learning. IEEE Transactions on Neural Networks and\nLearning Systems, 33(6):2378\u20132392, 2022.\n[22] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang,\nYuan Wang, and Shan Xue. ComGA: Community-aware attributed graph\nanomaly detection. In WSDM, pages 657\u2013665, 2022.\n[23] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng,\nHui Xiong, and Leman Akoglu.\nA comprehensive survey on graph\nanomaly detection with deep learning. IEEE Transactions on Knowledge\nand Data Engineering, Early Access, 2021.\n[24] Emmanuel M\u00a8uller, Patricia Iglesias S\u00b4anchez, Yvonne M\u00a8ulle, and Kle-\nmens B\u00a8ohm. Ranking outlier nodes in subspaces of attributed graphs.\nIn ICDE Workshop (Graph Data Management), pages 216\u2013222, 2013.\n[25] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den\nHengel.\nDeep learning for anomaly detection: A review.\nACM\nComputing Surveys, 54(2):38:1\u201338:38, 2021.\n[26] Guansong Pang, Chunhua Shen, and Anton van den Hengel.\nDeep\nanomaly detection with deviation networks. In KDD, pages 353\u2013362,\n2019.\n[27] Yulong Pei, Tianjin Huang, Werner van Ipenburg, and Mykola Pech-\nenizkiy. ResGCN: Attention-based deep residual modeling for anomaly\ndetection on attributed networks. Machine Learning, 111(2):519\u2013541,\n2022.\n[28] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng.\nANOMALOUS: A joint modeling approach for anomaly detection on\nattributed networks. In IJCAI, pages 3513\u20133519, 2018.\n[29] Bryan Perozzi and Leman Akoglu.\nScalable anomaly ranking of\nattributed neighborhoods. In SDM, pages 207\u2013215, 2016.\n[30] Bryan Perozzi, Leman Akoglu, Patricia Iglesias S\u00b4anchez, and Emmanuel\nM\u00a8uller.\nFocused clustering and outlier detection in large attributed\ngraphs. In KDD, pages 1346\u20131355, 2014.\n[31] Lukas Ruff, Robert A Vandermeulen, Nico G\u00a8ornitz, Alexander Binder,\nEmmanuel M\u00a8uller, Klaus-Robert M\u00a8uller, and Marius Kloft. Deep semi-\nsupervised anomaly detection. In ICLR, 2020.\n[32] Patricia Iglesias S\u00b4anchez, Emmanuel M\u00a8uller, Oretta Irmler, and Klemens\nB\u00a8ohm. Local context selection for outlier ranking in graphs with multiple\nnumeric node attributes. In SSDBM, pages 16:1\u201316:12, 2014.\n[33] Patricia Iglesias S\u00b4anchez, Emmanuel M\u00a8uller, Fabian Laforet, Fabian\nKeller, and Klemens B\u00a8ohm. Statistical selection of congruent subspaces\nfor mining attributed graphs. In ICDM, pages 647\u2013656, 2013.\n[34] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian\nGalligher, and Tina Eliassi-Rad.\nCollective classification in network\ndata. AI Magazine, 29(3):93\u2013106, 2008.\n[35] Xiuyao Song, Mingxi Wu, Christopher Jermaine, and Sanjay Ranka.\nConditional anomaly detection. IEEE Transactions on Knowledge and\nData Engineering, 19(5):631\u2013645, 2007.\n[36] Xing Su, Shan Xue, Fanzhen Liu, Jia Wu, Jian Yang, Chuan Zhou,\nWenbin Hu, Cecile Paris, Surya Nepal, Di Jin, Quan Z. Sheng, and\nPhilip S. Yu. A comprehensive survey on community detection with\ndeep learning. IEEE Transactions on Neural Networks and Learning\nSystems, Early Access, 2022.\n[37] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural\nnetworks for anomaly detection. In ICML, pages 21076\u201321089. PMLR,\n2022.\n[38] Sheng Tian, Jihai Dong, Jintang Li, Wenlong Zhao, Xiaolong Xu, Bowen\nSong, Changhua Meng, Tianyi Zhang, Liang Chen, et al. Sad: Semi-\nsupervised anomaly detection on dynamic graphs. In IJCAI, 2023.\n[39] Jeffrey Travers and Stanley Milgram. An experimental study of the small\nworld problem. In Social Networks, pages 179\u2013197. Elsevier, 1977.\n[40] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using\nt-sne. Journal of Machine Learning Research, 9(11), 2008.\n[41] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero,\nPietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR,\n2018.\n[42] Jindong Wang, Yiqiang Chen, Shuji Hao, Xiaohui Peng, and Lisha Hu.\nDeep learning for sensor-based activity recognition: A survey. Pattern\nRecognition Letters, 119:3\u201311, 2019.\n[43] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and\nKilian Weinberger. Simplifying graph convolutional networks. In ICML,\npages 6861\u20136871, 2019.\n[44] Shunxin Xiao, Shiping Wang, Yuanfei Dai, and Wenzhong Guo. Graph\nneural networks in node classification: survey and evaluation. Machine\nVision and Applications, 33:1\u201319, 2022.\n[45] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger.\nSCAN: a structural clustering algorithm for networks. In KDD, pages\n824\u2013833, 2007.\n[46] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-\nsupervised learning with graph embeddings.\nIn ICML, pages 40\u201348,\n2016.\n[47] Ge Zhang, Zhao Li, Jiaming Huang, Jia Wu, Chuan Zhou, Jian Yang, and\nJianliang Gao. eFraudCom: An e-commerce fraud detection system via\ncompetitive graph neural networks. ACM Transactions on Information\nSystems, 40(3):47:1\u201347:29, 2022.\n10\n[48] Dengyong Zhou, Olivier Bousquet, Thomas Lal, Jason Weston, and\nBernhard Sch\u00a8olkopf. Learning with local and global consistency. In\nNIPS, pages 321\u2013328, 2003.\n[49] Xiaokang Zhou, Wei Liang, Weimin Li, Ke Yan, Shohei Shimizu,\nI Kevin, and Kai Wang. Hierarchical adversarial attacks against graph-\nneural-network-based iot network intrusion detection system.\nIEEE\nInternet of Things Journal, 9(12):9310\u20139319, 2022.\n[50] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised\nlearning using gaussian fields and harmonic functions. In ICML, pages\n912\u2013919, 2003.\n",
    "1609.02907": "Published as a conference paper at ICLR 2017\nSEMI-SUPERVISED CLASSIFICATION WITH\nGRAPH CONVOLUTIONAL NETWORKS\nThomas N. Kipf\nUniversity of Amsterdam\nT.N.Kipf@uva.nl\nMax Welling\nUniversity of Amsterdam\nCanadian Institute for Advanced Research (CIFAR)\nM.Welling@uva.nl\nABSTRACT\nWe present a scalable approach for semi-supervised learning on graph-structured\ndata that is based on an ef\ufb01cient variant of convolutional neural networks which\noperate directly on graphs. We motivate the choice of our convolutional archi-\ntecture via a localized \ufb01rst-order approximation of spectral graph convolutions.\nOur model scales linearly in the number of graph edges and learns hidden layer\nrepresentations that encode both local graph structure and features of nodes. In\na number of experiments on citation networks and on a knowledge graph dataset\nwe demonstrate that our approach outperforms related methods by a signi\ufb01cant\nmargin.\n1\nINTRODUCTION\nWe consider the problem of classifying nodes (such as documents) in a graph (such as a citation\nnetwork), where labels are only available for a small subset of nodes. This problem can be framed\nas graph-based semi-supervised learning, where label information is smoothed over the graph via\nsome form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al.,\n2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function:\nL = L0 + \u03bbLreg ,\nwith\nLreg =\nX\ni,j\nAij\u2225f(Xi) \u2212f(Xj)\u22252 = f(X)\u22a4\u2206f(X) .\n(1)\nHere, L0 denotes the supervised loss w.r.t. the labeled part of the graph, f(\u00b7) can be a neural network-\nlike differentiable function, \u03bb is a weighing factor and X is a matrix of node feature vectors Xi.\n\u2206= D \u2212A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with\nN nodes vi \u2208V, edges (vi, vj) \u2208E, an adjacency matrix A \u2208RN\u00d7N (binary or weighted) and\na degree matrix Dii = P\nj Aij. The formulation of Eq. 1 relies on the assumption that connected\nnodes in the graph are likely to share the same label. This assumption, however, might restrict\nmodeling capacity, as graph edges need not necessarily encode node similarity, but could contain\nadditional information.\nIn this work, we encode the graph structure directly using a neural network model f(X, A) and\ntrain on a supervised target L0 for all nodes with labels, thereby avoiding explicit graph-based\nregularization in the loss function. Conditioning f(\u00b7) on the adjacency matrix of the graph will\nallow the model to distribute gradient information from the supervised loss L0 and will enable it to\nlearn representations of nodes both with and without labels.\nOur contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise prop-\nagation rule for neural network models which operate directly on graphs and show how it can be\nmotivated from a \ufb01rst-order approximation of spectral graph convolutions (Hammond et al., 2011).\nSecondly, we demonstrate how this form of a graph-based neural network model can be used for\nfast and scalable semi-supervised classi\ufb01cation of nodes in a graph. Experiments on a number of\ndatasets demonstrate that our model compares favorably both in classi\ufb01cation accuracy and ef\ufb01-\nciency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.\n1\narXiv:1609.02907v4  [cs.LG]  22 Feb 2017\nPublished as a conference paper at ICLR 2017\n2\nFAST APPROXIMATE CONVOLUTIONS ON GRAPHS\nIn this section, we provide theoretical motivation for a speci\ufb01c graph-based neural network model\nf(X, A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional\nNetwork (GCN) with the following layer-wise propagation rule:\nH(l+1) = \u03c3\n\u0010\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 H(l)W (l)\u0011\n.\n(2)\nHere, \u02dcA = A + IN is the adjacency matrix of the undirected graph G with added self-connections.\nIN is the identity matrix, \u02dcDii = P\nj \u02dcAij and W (l) is a layer-speci\ufb01c trainable weight matrix. \u03c3(\u00b7)\ndenotes an activation function, such as the ReLU(\u00b7) = max(0, \u00b7). H(l) \u2208RN\u00d7D is the matrix of ac-\ntivations in the lth layer; H(0) = X. In the following, we show that the form of this propagation rule\ncan be motivated1 via a \ufb01rst-order approximation of localized spectral \ufb01lters on graphs (Hammond\net al., 2011; Defferrard et al., 2016).\n2.1\nSPECTRAL GRAPH CONVOLUTIONS\nWe consider spectral convolutions on graphs de\ufb01ned as the multiplication of a signal x \u2208RN (a\nscalar for every node) with a \ufb01lter g\u03b8 = diag(\u03b8) parameterized by \u03b8 \u2208RN in the Fourier domain,\ni.e.:\ng\u03b8 \u22c6x = Ug\u03b8U \u22a4x ,\n(3)\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian L = IN \u2212D\u22121\n2 AD\u22121\n2 =\nU\u039bU \u22a4, with a diagonal matrix of its eigenvalues \u039b and U \u22a4x being the graph Fourier transform\nof x. We can understand g\u03b8 as a function of the eigenvalues of L, i.e. g\u03b8(\u039b). Evaluating Eq. 3 is\ncomputationally expensive, as multiplication with the eigenvector matrix U is O(N 2). Furthermore,\ncomputing the eigendecomposition of L in the \ufb01rst place might be prohibitively expensive for large\ngraphs. To circumvent this problem, it was suggested in Hammond et al. (2011) that g\u03b8(\u039b) can be\nwell-approximated by a truncated expansion in terms of Chebyshev polynomials Tk(x) up to Kth\norder:\ng\u03b8\u2032(\u039b) \u2248\nK\nX\nk=0\n\u03b8\u2032\nkTk(\u02dc\u039b) ,\n(4)\nwith a rescaled \u02dc\u039b =\n2\n\u03bbmax \u039b \u2212IN. \u03bbmax denotes the largest eigenvalue of L. \u03b8\u2032 \u2208RK is now a\nvector of Chebyshev coef\ufb01cients. The Chebyshev polynomials are recursively de\ufb01ned as Tk(x) =\n2xTk\u22121(x) \u2212Tk\u22122(x), with T0(x) = 1 and T1(x) = x. The reader is referred to Hammond et al.\n(2011) for an in-depth discussion of this approximation.\nGoing back to our de\ufb01nition of a convolution of a signal x with a \ufb01lter g\u03b8\u2032, we now have:\ng\u03b8\u2032 \u22c6x \u2248\nK\nX\nk=0\n\u03b8\u2032\nkTk(\u02dcL)x ,\n(5)\nwith \u02dcL =\n2\n\u03bbmax L \u2212IN; as can easily be veri\ufb01ed by noticing that (U\u039bU \u22a4)k = U\u039bkU \u22a4. Note that\nthis expression is now K-localized since it is a Kth-order polynomial in the Laplacian, i.e. it depends\nonly on nodes that are at maximum K steps away from the central node (Kth-order neighborhood).\nThe complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al.\n(2016) use this K-localized convolution to de\ufb01ne a convolutional neural network on graphs.\n2.2\nLAYER-WISE LINEAR MODEL\nA neural network model based on graph convolutions can therefore be built by stacking multiple\nconvolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now,\nimagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is\nlinear w.r.t. L and therefore a linear function on the graph Laplacian spectrum.\n1We provide an alternative interpretation of this propagation rule based on the Weisfeiler-Lehman algorithm\n(Weisfeiler & Lehmann, 1968) in Appendix A.\n2\nPublished as a conference paper at ICLR 2017\nIn this way, we can still recover a rich class of convolutional \ufb01lter functions by stacking multiple\nsuch layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev\npolynomials. We intuitively expect that such a model can alleviate the problem of over\ufb01tting on\nlocal neighborhood structures for graphs with very wide node degree distributions, such as social\nnetworks, citation networks, knowledge graphs and many other real-world graph datasets. Addition-\nally, for a \ufb01xed computational budget, this layer-wise linear formulation allows us to build deeper\nmodels, a practice that is known to improve modeling capacity on a number of domains (He et al.,\n2016).\nIn this linear formulation of a GCN we further approximate \u03bbmax \u22482, as we can expect that neural\nnetwork parameters will adapt to this change in scale during training. Under these approximations\nEq. 5 simpli\ufb01es to:\ng\u03b8\u2032 \u22c6x \u2248\u03b8\u2032\n0x + \u03b8\u2032\n1 (L \u2212IN) x = \u03b8\u2032\n0x \u2212\u03b8\u2032\n1D\u22121\n2 AD\u22121\n2 x ,\n(6)\nwith two free parameters \u03b8\u2032\n0 and \u03b8\u2032\n1. The \ufb01lter parameters can be shared over the whole graph.\nSuccessive application of \ufb01lters of this form then effectively convolve the kth-order neighborhood of\na node, where k is the number of successive \ufb01ltering operations or convolutional layers in the neural\nnetwork model.\nIn practice, it can be bene\ufb01cial to constrain the number of parameters further to address over\ufb01tting\nand to minimize the number of operations (such as matrix multiplications) per layer. This leaves us\nwith the following expression:\ng\u03b8 \u22c6x \u2248\u03b8\n\u0010\nIN + D\u22121\n2 AD\u22121\n2\n\u0011\nx ,\n(7)\nwith a single parameter \u03b8 = \u03b8\u2032\n0 = \u2212\u03b8\u2032\n1. Note that IN + D\u22121\n2 AD\u22121\n2 now has eigenvalues in\nthe range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities\nand exploding/vanishing gradients when used in a deep neural network model. To alleviate this\nproblem, we introduce the following renormalization trick: IN +D\u22121\n2 AD\u22121\n2 \u2192\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 , with\n\u02dcA = A + IN and \u02dcDii = P\nj \u02dcAij.\nWe can generalize this de\ufb01nition to a signal X \u2208RN\u00d7C with C input channels (i.e. a C-dimensional\nfeature vector for every node) and F \ufb01lters or feature maps as follows:\nZ = \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 X\u0398 ,\n(8)\nwhere \u0398 \u2208RC\u00d7F is now a matrix of \ufb01lter parameters and Z \u2208RN\u00d7F is the convolved signal\nmatrix. This \ufb01ltering operation has complexity O(|E|FC), as \u02dcAX can be ef\ufb01ciently implemented\nas a product of a sparse matrix with a dense matrix.\n3\nSEMI-SUPERVISED NODE CLASSIFICATION\nHaving introduced a simple, yet \ufb02exible model f(X, A) for ef\ufb01cient information propagation on\ngraphs, we can return to the problem of semi-supervised node classi\ufb01cation. As outlined in the in-\ntroduction, we can relax certain assumptions typically made in graph-based semi-supervised learn-\ning by conditioning our model f(X, A) both on the data X and on the adjacency matrix A of the\nunderlying graph structure. We expect this setting to be especially powerful in scenarios where the\nadjacency matrix contains information not present in the data X, such as citation links between doc-\numents in a citation network or relations in a knowledge graph. The overall model, a multi-layer\nGCN for semi-supervised learning, is schematically depicted in Figure 1.\n3.1\nEXAMPLE\nIn the following, we consider a two-layer GCN for semi-supervised node classi\ufb01cation on a graph\nwith a symmetric adjacency matrix A (binary or weighted). We \ufb01rst calculate \u02c6A = \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 in\na pre-processing step. Our forward model then takes the simple form:\nZ = f(X, A) = softmax\n\u0010\n\u02c6A ReLU\n\u0010\n\u02c6AXW (0)\u0011\nW (1)\u0011\n.\n(9)\n3\nPublished as a conference paper at ICLR 2017\nC\ninput layer\nX1\nX2\nX3\nX4\nF\noutput layer\nZ1\nZ2\nZ3\nZ4\nhidden\nlayers\nY1\nY4\n1\n(a) Graph Convolutional Network\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30\n(b) Hidden layer activations\nFigure 1: Left: Schematic depiction of multi-layer Graph Convolutional Network (GCN) for semi-\nsupervised learning with C input channels and F feature maps in the output layer. The graph struc-\nture (edges shown as black lines) is shared over layers, labels are denoted by Yi. Right: t-SNE\n(Maaten & Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained on\nthe Cora dataset (Sen et al., 2008) using 5% of labels. Colors denote document class.\nHere, W (0) \u2208RC\u00d7H is an input-to-hidden weight matrix for a hidden layer with H feature maps.\nW (1) \u2208RH\u00d7F is a hidden-to-output weight matrix. The softmax activation function, de\ufb01ned as\nsoftmax(xi) = 1\nZ exp(xi) with Z = P\ni exp(xi), is applied row-wise. For semi-supervised multi-\nclass classi\ufb01cation, we then evaluate the cross-entropy error over all labeled examples:\nL = \u2212\nX\nl\u2208YL\nF\nX\nf=1\nYlf ln Zlf ,\n(10)\nwhere YL is the set of node indices that have labels.\nThe neural network weights W (0) and W (1) are trained using gradient descent. In this work, we\nperform batch gradient descent using the full dataset for every training iteration, which is a viable\noption as long as datasets \ufb01t in memory. Using a sparse representation for A, memory requirement\nis O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via\ndropout (Srivastava et al., 2014). We leave memory-ef\ufb01cient extensions with mini-batch stochastic\ngradient descent for future work.\n3.2\nIMPLEMENTATION\nIn practice, we make use of TensorFlow (Abadi et al., 2015) for an ef\ufb01cient GPU-based imple-\nmentation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of\nevaluating Eq. 9 is then O(|E|CHF), i.e. linear in the number of graph edges.\n4\nRELATED WORK\nOur model draws inspiration both from the \ufb01eld of graph-based semi-supervised learning and from\nrecent work on neural networks that operate on graphs. In what follows, we provide a brief overview\non related work in both \ufb01elds.\n4.1\nGRAPH-BASED SEMI-SUPERVISED LEARNING\nA large number of approaches for semi-supervised learning using graph representations have been\nproposed in recent years, most of which fall into two broad categories: methods that use some\nform of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent\nexamples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold\nregularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012).\n2Code to reproduce our experiments is available at https://github.com/tkipf/gcn.\n4\nPublished as a conference paper at ICLR 2017\nRecently, attention has shifted to models that learn graph embeddings with methods inspired by\nthe skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings\nvia the prediction of the local neighborhood of nodes, sampled from random walks on the graph.\nLINE (Tang et al., 2015) and node2vec (Grover & Leskovec, 2016) extend DeepWalk with more\nsophisticated random walk or breadth-\ufb01rst search schemes. For all these methods, however, a multi-\nstep pipeline including random walk generation and semi-supervised training is required where each\nstep has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label\ninformation in the process of learning embeddings.\n4.2\nNEURAL NETWORKS ON GRAPHS\nNeural networks that operate on graphs have previously been introduced in Gori et al. (2005);\nScarselli et al. (2009) as a form of recurrent neural network. Their framework requires the repeated\napplication of contraction maps as propagation functions until node representations reach a stable\n\ufb01xed point. This restriction was later alleviated in Li et al. (2016) by introducing modern practices\nfor recurrent neural network training to the original graph neural network framework. Duvenaud\net al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level\nclassi\ufb01cation. Their approach requires to learn node degree-speci\ufb01c weight matrices which does not\nscale to large graphs with wide node degree distributions. Our model instead uses a single weight\nmatrix per layer and deals with varying node degrees through an appropriate normalization of the\nadjacency matrix (see Section 3.1).\nA related approach to node classi\ufb01cation with a graph-based neural network was recently introduced\nin Atwood & Towsley (2016). They report O(N 2) complexity, limiting the range of possible appli-\ncations. In a different yet related model, Niepert et al. (2016) convert graphs locally into sequences\nthat are fed into a conventional 1D convolutional neural network, which requires the de\ufb01nition of a\nnode ordering in a pre-processing step.\nOur method is based on spectral graph convolutional neural networks, introduced in Bruna et al.\n(2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast\nto these works, we consider here the task of transductive node classi\ufb01cation within networks of\nsigni\ufb01cantly larger scale. We show that in this setting, a number of simpli\ufb01cations (see Section 2.2)\ncan be introduced to the original frameworks of Bruna et al. (2014) and Defferrard et al. (2016) that\nimprove scalability and classi\ufb01cation performance in large-scale networks.\n5\nEXPERIMENTS\nWe test our model in a number of experiments: semi-supervised document classi\ufb01cation in cita-\ntion networks, semi-supervised entity classi\ufb01cation in a bipartite graph extracted from a knowledge\ngraph, an evaluation of various graph propagation models and a run-time analysis on random graphs.\n5.1\nDATASETS\nWe closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized\nin Table 1. In the citation network datasets\u2014Citeseer, Cora and Pubmed (Sen et al., 2008)\u2014nodes\nare documents and edges are citation links. Label rate denotes the number of labeled nodes that are\nused for training divided by the total number of nodes in each dataset. NELL (Carlson et al., 2010;\nYang et al., 2016) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation\nnodes and 9,891 entity nodes.\nTable 1: Dataset statistics, as reported in Yang et al. (2016).\nDataset\nType\nNodes\nEdges\nClasses\nFeatures\nLabel rate\nCiteseer\nCitation network\n3,327\n4,732\n6\n3,703\n0.036\nCora\nCitation network\n2,708\n5,429\n7\n1,433\n0.052\nPubmed\nCitation network\n19,717\n44,338\n3\n500\n0.003\nNELL\nKnowledge graph\n65,755\n266,144\n210\n5,414\n0.001\n5\nPublished as a conference paper at ICLR 2017\nCitation networks\nWe consider three citation network datasets: Citeseer, Cora and Pubmed (Sen\net al., 2008). The datasets contain sparse bag-of-words feature vectors for each document and a list\nof citation links between documents. We treat the citation links as (undirected) edges and construct\na binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use\n20 labels per class, but all feature vectors.\nNELL\nNELL is a dataset extracted from the knowledge graph introduced in (Carlson et al., 2010).\nA knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow\nthe pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes\nr1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). Entity nodes are described by\nsparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot\nrepresentation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per\nnode. The semi-supervised task here considers the extreme case of only a single labeled example\nper class in the training set. We construct a binary, symmetric adjacency matrix from this graph by\nsetting entries Aij = 1, if one or more edges are present between nodes i and j.\nRandom graphs\nWe simulate random graph datasets of various sizes for experiments where we\nmeasure training time per epoch. For a dataset with N nodes we create a random graph assigning\n2N edges uniformly at random. We take the identity matrix IN as input feature matrix X, thereby\nimplicitly taking a featureless approach where the model is only informed about the identity of each\nnode, speci\ufb01ed by a unique one-hot vector. We add dummy labels Yi = 1 for every node.\n5.2\nEXPERIMENTAL SET-UP\nUnless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate pre-\ndiction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using\ndeeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang et al.\n(2016) with an additional validation set of 500 labeled examples for hyperparameter optimization\n(dropout rate for all layers, L2 regularization factor for the \ufb01rst GCN layer and number of hidden\nunits). We do not use the validation set labels for training.\nFor the citation network datasets, we optimize hyperparameters on Cora only and use the same set\nof parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training\niterations) using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 and early stopping with a\nwindow size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive\nepochs. We initialize weights using the initialization described in Glorot & Bengio (2010) and\naccordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden\nlayer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization).\n5.3\nBASELINES\nWe compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation\n(LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold\nregularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk)\n(Perozzi et al., 2014). We omit TSVM (Joachims, 1999), as it does not scale to the large number of\nclasses in one of our datasets.\nWe further compare against the iterative classi\ufb01cation algorithm (ICA) proposed in Lu & Getoor\n(2003) in conjunction with two logistic regression classi\ufb01ers, one for local node features alone and\none for relational classi\ufb01cation using local features and an aggregation operator as described in\nSen et al. (2008). We \ufb01rst train the local classi\ufb01er using all labeled training set nodes and use\nit to bootstrap class labels of unlabeled nodes for relational classi\ufb01er training. We run iterative\nclassi\ufb01cation (relational classi\ufb01er) with a random node ordering for 10 iterations on all unlabeled\nnodes (bootstrapped using the local classi\ufb01er). L2 regularization parameter and aggregation operator\n(count vs. prop, see Sen et al. (2008)) are chosen based on validation set performance for each dataset\nseparately.\nLastly, we compare against Planetoid (Yang et al., 2016), where we always choose their best-\nperforming model variant (transductive vs. inductive) as a baseline.\n6\nPublished as a conference paper at ICLR 2017\n6\nRESULTS\n6.1\nSEMI-SUPERVISED NODE CLASSIFICATION\nResults are summarized in Table 2. Reported numbers denote classi\ufb01cation accuracy in percent. For\nICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other\nbaseline methods are taken from the Planetoid paper (Yang et al., 2016). Planetoid* denotes the best\nmodel for the respective dataset out of the variants presented in their paper.\nTable 2: Summary of results in terms of classi\ufb01cation accuracy (in percent).\nMethod\nCiteseer\nCora\nPubmed\nNELL\nManiReg [3]\n60.1\n59.5\n70.7\n21.8\nSemiEmb [28]\n59.6\n59.0\n71.1\n26.7\nLP [32]\n45.3\n68.0\n63.0\n26.5\nDeepWalk [22]\n43.2\n67.2\n65.3\n58.1\nICA [18]\n69.1\n75.1\n73.9\n23.1\nPlanetoid* [29]\n64.7 (26s)\n75.7 (13s)\n77.2 (25s)\n61.9 (185s)\nGCN (this paper)\n70.3 (7s)\n81.5 (4s)\n79.0 (38s)\n66.0 (48s)\nGCN (rand. splits)\n67.9 \u00b1 0.5\n80.1 \u00b1 0.5\n78.9 \u00b1 0.7\n58.4 \u00b1 1.7\nWe further report wall-clock training time in seconds until convergence (in brackets) for our method\n(incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation pro-\nvided by the authors3 and trained on the same hardware (with GPU) as our GCN model. We trained\nand tested our model on the same dataset splits as in Yang et al. (2016) and report mean accuracy\nof 100 runs with random weight initializations. We used the following sets of hyperparameters for\nCiteseer, Cora and Pubmed: 0.5 (dropout rate), 5 \u00b7 10\u22124 (L2 regularization) and 16 (number of hid-\nden units); and for NELL: 0.1 (dropout rate), 1 \u00b7 10\u22125 (L2 regularization) and 64 (number of hidden\nunits).\nIn addition, we report performance of our model on 10 randomly drawn dataset splits of the same\nsize as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard\nerror of prediction accuracy on the test set split in percent.\n6.2\nEVALUATION OF PROPAGATION MODEL\nWe compare different variants of our proposed per-layer propagation model on the citation network\ndatasets. We follow the experimental set-up described in the previous section. Results are summa-\nrized in Table 3. The propagation model of our original GCN model is denoted by renormalization\ntrick (in bold). In all other cases, the propagation model of both neural network layers is replaced\nwith the model speci\ufb01ed under propagation model. Reported numbers denote mean classi\ufb01cation\naccuracy for 100 repeated runs with random weight matrix initializations. In case of multiple vari-\nables \u0398i per layer, we impose L2 regularization on all weight matrices of the \ufb01rst layer.\nTable 3: Comparison of propagation models.\nDescription\nPropagation model\nCiteseer\nCora\nPubmed\nChebyshev \ufb01lter (Eq. 5)\nK = 3\nPK\nk=0 Tk(\u02dcL)X\u0398k\n69.8\n79.5\n74.4\nK = 2\n69.6\n81.2\n73.8\n1st-order model (Eq. 6)\nX\u03980 + D\u22121\n2 AD\u22121\n2 X\u03981\n68.3\n80.0\n77.5\nSingle parameter (Eq. 7)\n(IN + D\u22121\n2 AD\u22121\n2 )X\u0398\n69.3\n79.2\n77.4\nRenormalization trick (Eq. 8)\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 X\u0398\n70.3\n81.5\n79.0\n1st-order term only\nD\u22121\n2 AD\u22121\n2 X\u0398\n68.7\n80.5\n77.8\nMulti-layer perceptron\nX\u0398\n46.5\n55.1\n71.4\n3https://github.com/kimiyoung/planetoid\n7\nPublished as a conference paper at ICLR 2017\n6.3\nTRAINING TIME PER EPOCH\n1k\n10k\n100k\n1M\n10M\n# Edges\n10-3\n10-2\n10-1\n100\n101\nSec./epoch\n*\nGPU\nCPU\nFigure 2: Wall-clock time per epoch for random\ngraphs. (*) indicates out-of-memory error.\nHere, we report results for the mean training\ntime per epoch (forward pass, cross-entropy\ncalculation, backward pass) for 100 epochs on\nsimulated random graphs, measured in seconds\nwall-clock time. See Section 5.1 for a detailed\ndescription of the random graph dataset used\nin these experiments. We compare results on\na GPU and on a CPU-only implementation4 in\nTensorFlow (Abadi et al., 2015). Figure 2 sum-\nmarizes the results.\n7\nDISCUSSION\n7.1\nSEMI-SUPERVISED MODEL\nIn the experiments demonstrated here, our method for semi-supervised node classi\ufb01cation outper-\nforms recent related methods by a signi\ufb01cant margin. Methods based on graph-Laplacian regular-\nization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their\nassumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand\nare limited by the fact that they are based on a multi-step pipeline which is dif\ufb01cult to optimize.\nOur proposed model can overcome both limitations, while still comparing favorably in terms of ef-\n\ufb01ciency (measured in wall-clock time) to related methods. Propagation of feature information from\nneighboring nodes in every layer improves classi\ufb01cation performance in comparison to methods like\nICA (Lu & Getoor, 2003), where only label information is aggregated.\nWe have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both\nimproved ef\ufb01ciency (fewer parameters and operations, such as multiplication or addition) and better\npredictive performance on a number of datasets compared to a na\u00a8\u0131ve 1st-order model (Eq. 6) or\nhigher-order graph convolutional models using Chebyshev polynomials (Eq. 5).\n7.2\nLIMITATIONS AND FUTURE WORK\nHere, we describe several limitations of our current model and outline how these might be overcome\nin future work.\nMemory requirement\nIn the current setup with full-batch gradient descent, memory requirement\ngrows linearly in the size of the dataset. We have shown that for large graphs that do not \ufb01t in GPU\nmemory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can\nalleviate this issue. The procedure of generating mini-batches, however, should take into account the\nnumber of layers in the GCN model, as the Kth-order neighborhood for a GCN with K layers has to\nbe stored in memory for an exact procedure. For very large and densely connected graph datasets,\nfurther approximations might be necessary.\nDirected edges and edge features\nOur framework currently does not naturally support edge fea-\ntures and is limited to undirected graphs (weighted or unweighted). Results on NELL however\nshow that it is possible to handle both directed edges and edge features by representing the original\ndirected graph as an undirected bipartite graph with additional nodes that represent edges in the\noriginal graph (see Section 5.1 for details).\nLimiting assumptions\nThrough the approximations introduced in Section 2, we implicitly assume\nlocality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal impor-\ntance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be\nbene\ufb01cial to introduce a trade-off parameter \u03bb in the de\ufb01nition of \u02dcA:\n\u02dcA = A + \u03bbIN .\n(11)\n4Hardware used: 16-core Intel R\u20ddXeon R\u20ddCPU E5-2640 v3 @ 2.60GHz, GeForce R\u20ddGTX TITAN X\n8\nPublished as a conference paper at ICLR 2017\nThis parameter now plays a similar role as the trade-off parameter between supervised and unsuper-\nvised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via\ngradient descent.\n8\nCONCLUSION\nWe have introduced a novel approach for semi-supervised classi\ufb01cation on graph-structured data.\nOur GCN model uses an ef\ufb01cient layer-wise propagation rule that is based on a \ufb01rst-order approx-\nimation of spectral convolutions on graphs. Experiments on a number of network datasets suggest\nthat the proposed GCN model is capable of encoding both graph structure and node features in a\nway useful for semi-supervised classi\ufb01cation. In this setting, our model outperforms several recently\nproposed methods by a signi\ufb01cant margin, while being computationally ef\ufb01cient.\nACKNOWLEDGMENTS\nWe would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman,\nPramod Sinha and Abdul-Saboor Sheikh for helpful discussions. This research was funded by SAP.\nREFERENCES\nMart\u00b4\u0131n Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\nJames Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in neural\ninformation processing systems (NIPS), 2016.\nMikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-\nwork for learning from labeled and unlabeled examples. Journal of machine learning research\n(JMLR), 7(Nov):2399\u20132434, 2006.\nUlrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,\nand Dorothea Wagner. On modularity clustering. IEEE Transactions on Knowledge and Data\nEngineering, 20(2):172\u2013188, 2008.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. In International Conference on Learning Representations (ICLR),\n2014.\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr, and Tom M.\nMitchell. Toward an architecture for never-ending language learning. In AAAI, volume 5, pp. 3,\n2010.\nMicha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\ngraphs with fast localized spectral \ufb01ltering. In Advances in neural information processing systems\n(NIPS), 2016.\nBrendan L. Douglas. The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint\narXiv:1101.5211, 2011.\nDavid K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00b4an\nAspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular\n\ufb01ngerprints. In Advances in neural information processing systems (NIPS), pp. 2224\u20132232, 2015.\nXavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In AISTATS, volume 9, pp. 249\u2013256, 2010.\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.\nIn Proceedings. 2005 IEEE International Joint Conference on Neural Networks., volume 2, pp.\n729\u2013734. IEEE, 2005.\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\nACM, 2016.\n9\nPublished as a conference paper at ICLR 2017\nDavid K. Hammond, Pierre Vandergheynst, and R\u00b4emi Gribonval. Wavelets on graphs via spectral\ngraph theory. Applied and Computational Harmonic Analysis, 30(2):129\u2013150, 2011.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\nThorsten Joachims. Transductive inference for text classi\ufb01cation using support vector machines. In\nInternational Conference on Machine Learning (ICML), volume 99, pp. 200\u2013209, 1999.\nDiederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations (ICLR), 2015.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\nnetworks. In International Conference on Learning Representations (ICLR), 2016.\nQing Lu and Lise Getoor. Link-based classi\ufb01cation. In International Conference on Machine Learn-\ning (ICML), volume 3, pp. 496\u2013503, 2003.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\nLearning Research (JMLR), 9(Nov):2579\u20132605, 2008.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems (NIPS), pp. 3111\u20133119, 2013.\nMathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-\nworks for graphs. In International Conference on Machine Learning (ICML), 2016.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\nDeepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pp. 701\u2013710. ACM, 2014.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\nThe graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008.\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning\nResearch (JMLR), 15(1):1929\u20131958, 2014.\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale\ninformation network embedding. In Proceedings of the 24th International Conference on World\nWide Web, pp. 1067\u20131077. ACM, 2015.\nBoris Weisfeiler and A. A. Lehmann. A reduction of a graph to a canonical form and an algebra\narising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968.\nJason Weston, Fr\u00b4ed\u00b4eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-\nsupervised embedding. In Neural Networks: Tricks of the Trade, pp. 639\u2013655. Springer, 2012.\nZhilin Yang, William Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with\ngraph embeddings. In International Conference on Machine Learning (ICML), 2016.\nWayne W. Zachary. An information \ufb02ow model for con\ufb02ict and \ufb01ssion in small groups. Journal of\nanthropological research, pp. 452\u2013473, 1977.\nDengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch\u00a8olkopf.\nLearning with local and global consistency. In Advances in neural information processing systems\n(NIPS), volume 16, pp. 321\u2013328, 2004.\nXiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian \ufb01elds\nand harmonic functions. In International Conference on Machine Learning (ICML), volume 3,\npp. 912\u2013919, 2003.\n10\nPublished as a conference paper at ICLR 2017\nA\nRELATION TO WEISFEILER-LEHMAN ALGORITHM\nA neural network model for graph-structured data should ideally be able to learn representations of\nnodes in a graph, taking both the graph structure and feature description of nodes into account. A\nwell-studied framework for the unique assignment of node labels given a graph and (optionally) dis-\ncrete initial node labels is provided by the 1-dim Weisfeiler-Lehman (WL-1) algorithm (Weisfeiler\n& Lehmann, 1968):\nAlgorithm 1: WL-1 algorithm (Weisfeiler & Lehmann, 1968)\nInput: Initial node coloring (h(0)\n1 , h(0)\n2 , ..., h(0)\nN )\nOutput: Final node coloring (h(T )\n1\n, h(T )\n2\n, ..., h(T )\nN )\nt \u21900;\nrepeat\nfor vi \u2208V do\nh(t+1)\ni\n\u2190hash\n\u0010P\nj\u2208Ni h(t)\nj\n\u0011\n;\nt \u2190t + 1;\nuntil stable node coloring is reached;\nHere, h(t)\ni\ndenotes the coloring (label assignment) of node vi (at iteration t) and Ni is its set of\nneighboring node indices (irrespective of whether the graph includes self-connections for every node\nor not). hash(\u00b7) is a hash function. For an in-depth mathematical discussion of the WL-1 algorithm\nsee, e.g., Douglas (2011).\nWe can replace the hash function in Algorithm 1 with a neural network layer-like differentiable\nfunction with trainable parameters as follows:\nh(l+1)\ni\n= \u03c3\n\uf8eb\n\uf8edX\nj\u2208Ni\n1\ncij\nh(l)\nj W (l)\n\uf8f6\n\uf8f8,\n(12)\nwhere cij is an appropriately chosen normalization constant for the edge (vi, vj). Further, we can\ntake h(l)\ni\nnow to be a vector of activations of node i in the lth neural network layer. W (l) is a\nlayer-speci\ufb01c weight matrix and \u03c3(\u00b7) denotes a differentiable, non-linear activation function.\nBy choosing cij =\np\ndidj, where di = |Ni| denotes the degree of node vi, we recover the propaga-\ntion rule of our Graph Convolutional Network (GCN) model in vector form (see Eq. 2)5.\nThis\u2014loosely speaking\u2014allows us to interpret our GCN model as a differentiable and parameter-\nized generalization of the 1-dim Weisfeiler-Lehman algorithm on graphs.\nA.1\nNODE EMBEDDINGS WITH RANDOM WEIGHTS\nFrom the analogy with the Weisfeiler-Lehman algorithm, we can understand that even an untrained\nGCN model with random weights can serve as a powerful feature extractor for nodes in a graph. As\nan example, consider the following 3-layer GCN model:\nZ = tanh\n\u0010\n\u02c6A tanh\n\u0010\n\u02c6A tanh\n\u0010\n\u02c6AXW (0)\u0011\nW (1)\u0011\nW (2)\u0011\n,\n(13)\nwith weight matrices W (l) initialized at random using the initialization described in Glorot & Bengio\n(2010). \u02c6A, X and Z are de\ufb01ned as in Section 3.1.\nWe apply this model on Zachary\u2019s karate club network (Zachary, 1977). This graph contains 34\nnodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of\nfour classes, obtained via modularity-based clustering (Brandes et al., 2008). See Figure 3a for an\nillustration.\n5Note that we here implicitly assume that self-connections have already been added to every node in the\ngraph (for a clutter-free notation).\n11\nPublished as a conference paper at ICLR 2017\n(a) Karate club network\n(b) Random weight embedding\nFigure 3: Left: Zachary\u2019s karate club network (Zachary, 1977), colors denote communities obtained\nvia modularity-based clustering (Brandes et al., 2008). Right: Embeddings obtained from an un-\ntrained 3-layer GCN model (Eq. 13) with random weights applied to the karate club network. Best\nviewed on a computer screen.\nWe take a featureless approach by setting X = IN, where IN is the N by N identity matrix. N is\nthe number of nodes in the graph. Note that nodes are randomly ordered (i.e. ordering contains no\ninformation). Furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional\noutput (so that the output can immediately be visualized in a 2-dim plot).\nFigure 3b shows a representative example of node embeddings (outputs Z) obtained from an un-\ntrained GCN model applied to the karate club network. These results are comparable to embeddings\nobtained from DeepWalk (Perozzi et al., 2014), which uses a more expensive unsupervised training\nprocedure.\nA.2\nSEMI-SUPERVISED NODE EMBEDDINGS\nOn this simple example of a GCN applied to the karate club network it is interesting to observe how\nembeddings react during training on a semi-supervised classi\ufb01cation task. Such a visualization (see\nFigure 4) provides insights into how the GCN model can make use of the graph structure (and of\nfeatures extracted from the graph structure at later layers) to learn embeddings that are useful for a\nclassi\ufb01cation task.\nWe consider the following semi-supervised learning setup: we add a softmax layer on top of our\nmodel (Eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled\nnodes). We train for 300 training iterations using Adam (Kingma & Ba, 2015) with a learning rate\nof 0.01 on a cross-entropy loss.\nFigure 4 shows the evolution of node embeddings over a number of training iterations. The model\nsucceeds in linearly separating the communities based on minimal supervision and the graph struc-\nture alone. A video of the full training process can be found on our website7.\n6We originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed\nthat a dimensionality of 4 resulted in less frequent saturation of tanh(\u00b7) units and therefore visually more\npleasing results.\n7http://tkipf.github.io/graph-convolutional-networks/\n12\nPublished as a conference paper at ICLR 2017\n(a) Iteration 25\n(b) Iteration 50\n(c) Iteration 75\n(d) Iteration 100\n(e) Iteration 200\n(f) Iteration 300\nFigure 4: Evolution of karate club network node embeddings obtained from a GCN model after a\nnumber of semi-supervised training iterations. Colors denote class. Nodes of which labels were\nprovided during training (one per class) are highlighted (grey outline). Grey links between nodes\ndenote graph edges. Best viewed on a computer screen.\n13\nPublished as a conference paper at ICLR 2017\nB\nEXPERIMENTS ON MODEL DEPTH\nIn these experiments, we investigate the in\ufb02uence of model depth (number of layers) on classi\ufb01cation\nperformance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and\nPubmed datasets (Sen et al., 2008) using all labels. In addition to the standard GCN model (Eq. 2),\nwe report results on a model variant where we use residual connections (He et al., 2016) between\nhidden layers to facilitate training of deeper models by enabling the model to carry over information\nfrom the previous layer\u2019s input:\nH(l+1) = \u03c3\n\u0010\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 H(l)W (l)\u0011\n+ H(l) .\n(14)\nOn each cross-validation split, we train for 400 epochs (without early stopping) using the Adam\noptimizer (Kingma & Ba, 2015) with a learning rate of 0.01. Other hyperparameters are chosen as\nfollows: 0.5 (dropout rate, \ufb01rst and last layer), 5 \u00b7 10\u22124 (L2 regularization, \ufb01rst layer), 16 (number\nof units for each hidden layer) and 0.01 (learning rate). Results are summarized in Figure 5.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nCiteseer\nTrain\nTrain (Residual)\nTest\nTest (Residual)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nCora\nTrain\nTrain (Residual)\nTest\nTest (Residual)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nAccuracy\nPubmed\nTrain\nTrain (Residual)\nTest\nTest (Residual)\nFigure 5: In\ufb02uence of model depth (number of layers) on classi\ufb01cation performance. Markers\ndenote mean classi\ufb01cation accuracy (training vs. testing) for 5-fold cross-validation. Shaded areas\ndenote standard error. We show results both for a standard GCN model (dashed lines) and a model\nwith added residual connections (He et al., 2016) between hidden layers (solid lines).\nFor the datasets considered here, best results are obtained with a 2- or 3-layer model. We observe\nthat for models deeper than 7 layers, training without the use of residual connections can become\ndif\ufb01cult, as the effective context size for each node increases by the size of its Kth-order neighbor-\nhood (for a model with K layers) with each additional layer. Furthermore, over\ufb01tting can become\nan issue as the number of parameters increases with model depth.\n14\n",
    "2310.11829": "1\nTowards Graph Foundation Models:\nA Survey and Beyond\nJiawei Liu*, Cheng Yang*, Zhiyuan Lu, Junze Chen, Yibo Li, Meng-\nmei Zhang, Ting Bai, Yuan Fang, Lichao Sun, Philip S. Yu, and Chuan Shi\nAbstract\u2014Foundation models have emerged as critical components in a variety of artificial intelligence applications, and showcase\nsignificant success in natural language processing and several other domains. Meanwhile, the field of graph machine learning is\nwitnessing a paradigm transition from shallow methods to more sophisticated deep learning approaches. The capabilities of foundation\nmodels in generalization and adaptation motivate graph machine learning researchers to discuss the potential of developing a new\ngraph learning paradigm. This paradigm envisions models that are pre-trained on extensive graph data and can be adapted for various\ngraph tasks. Despite this burgeoning interest, there is a noticeable lack of clear definitions and systematic analyses pertaining to this\nnew domain. To this end, this article introduces the concept of Graph Foundation Models (GFMs), and offers an exhaustive explanation\nof their key characteristics and underlying technologies. We proceed to classify the existing work related to GFMs into three distinct\ncategories, based on their dependence on graph neural networks and large language models. In addition to providing a thorough\nreview of the current state of GFMs, this article also outlooks potential avenues for future research in this rapidly evolving domain.\nIndex Terms\u2014Graph Foundation Models, Large Language Models\n\u2726\n1\nINTRODUCTION\nW\nITH the rise in computational power and breakthroughs\nin deep learning techniques, the artificial intelligence (AI)\ncommunity has introduced the notion of \u201cfoundation models\u201d:\nA foundation model is any model that is trained on broad data\nand can be adapted to a wide range of downstream tasks [1].\nFoundation models enjoy unique attributes like emergence and ho-\nmogenization, empowering them to serve as the primary building\nblocks for a myriad of downstream AI applications [1]. Emergence\nsuggests that as a foundation model scales up, it may sponta-\nneously manifest novel capabilities [2]. Meanwhile, homogeniza-\ntion alludes to the model\u2019s versatility, enabling its deployment\nacross diverse applications [1]. Thanks to the development of\nlarge language models (LLMs), the concept of foundation models\nfirst became a reality in natural language processing (NLP). Since\nthen, foundation models have demonstrated impressive versatility,\nprocessing not just text but also image data, video data, audio\ndata and multi-modal inputs. This versatility empowers them to\nexcel in tasks ranging from computer vision [3] and audio signal\nprocessing [4] to recommender systems [5].\n\u2022\nJiawei Liu, Cheng Yang, Zhiyuan Lu, Junze Chen, Yibo Li, Ting Bai and\nChuan Shi are with School of Computer Science, Beijing University of\nPosts and Telecommunications, Beijing, China.\nE-mail:\n{liu jiawei,\nyangcheng,\nluzy,\njunze,\nyiboL,\nbaiting,\nshichuan}@bupt.edu.cn\n\u2022\nMengmei Zhang is with China Telecom Bestpay, Beijing, China.\nE-mail: zhangmengmei@bestpay.com.cn\n\u2022\nYuan Fang is with School of Computing and Information Systems, Singa-\npore Management University, Singapore.\nE-mail: yfang@smu.edu.sg\n\u2022\nLichao Sun is with Lehigh University, Bethlehem, Pennsylvania, USA.\nE-mail: lis221@lehigh.edu\n\u2022\nPhilip S. Yu is with University of Illinois at Chicago, Chicago, USA.\nE-mail: psyu@uic.edu\n\u2022\nJiawei Liu and Cheng Yang contributed equally to this research.\n\u2022\nCorresponding author: Chuan Shi\nMuch like the evolution witnessed in NLP, graph machine\nlearning is also undergoing a paradigm transition. In its early\nstages, graph tasks predominantly employed shallow methods,\nsuch as random walk [6] and matrix factorization [7]. These\nmethods, however, were typically limited to transductive learning\non unattributed graphs [8]. The more recent shift towards deep\nlearning methods has catalyzed the rise of graph neural networks\n(GNNs). GNNs have revolutionized the landscape by introducing\nthe message-passing mechanism, where nodes iteratively aggre-\ngate information from their neighbors. By harnessing GNNs in\nfully supervised, semi-supervised, or unsupervised settings, re-\nsearchers have pioneered a variety of customized graph models.\nThese advancements have yielded substantial improvements in\ntasks like node classification [9], link prediction [10], graph\nclassification [11], and graph clustering [12]. However, certain\nchallenges of GNN models still persist. For example, GNNs\nare restricted with issues related to expressive power [13] and\ngeneralizability [14], especially given the ever-expanding datasets\nand the widening spectrum of tasks.\nThe remarkable success of foundation models in varied do-\nmains is increasingly garnering the interest of graph machine\nlearning researchers. This naturally evokes the question: Could\ngraph foundation models represent the next frontier in graph\nmachine learning? Such models, if realized, would boast enhanced\nexpressive power, improved transferability, and applicability to\nmore intricate graph data and tasks. As illustrated in Figure 1,\na graph foundation model (GFM) is envisioned as a model pre-\ntrained on extensive graph data, primed for adaptation across\ndiverse downstream graph tasks. Drawing parallels with traditional\nfoundation models, a GFM is also anticipated to embody two prin-\ncipal characteristics: emergence and homogenization. Specifically,\nemergence refers to novel capabilities shown exclusively in large-\nscale graph models, while homogenization denotes the model\u2019s\nadaptability across different types of graph tasks. Existing deep\ngraph learning methods struggle to encompass these features: their\narXiv:2310.11829v3  [cs.LG]  1 Jul 2024\n2\nGraph Foundation Model\nDeep Graph Learning\nDownstream Tasks\nPre-training\nAdaptation\nEmergence\nPerformance\n#Parameters\nPerformance\n#Parameters\n...\n?\n...\n?\n?\nPretext Task \n(e.g., link prediction)\nEnd-to-End\n\u2022\nIn-context Learning\n\u2022\nGraph Reasoning\n\u2022\nZero-shot Generation\n\u2022\n...\n(e.g., node classification)\n(Node-, Edge-, Graph-level Tasks)\nHomogenization\nDownstream Task\n?\n?\n?\n?\nFig. 1: The distinction between deep graph learning and graph foun-\ndation models. Deep graph learning tackles specific tasks on specific\ndatasets through end-to-end training. In contrast, graph foundation\nmodels (GFMs) are pre-trained on broad graph data and can be\nadapted to a wide range of downstream graph tasks, expected to\ndemonstrate emergence and homogenization capabilities.\ninherent architectures and learning paradigms focus on specific\ntasks, which restrict the utilization of extensive unlabeled data,\nsubsequently limiting their expressive and generalization abilities.\nInspired by the success of LLMs as foundation models in NLP,\nresearchers have explored the possibilities of graph foundation\nmodels towards the emergence and homogenization capabilities.\nThese explorations primarily focus on the design of backbone ar-\nchitectures for GFMs, and different learning paradigms including\npre-training and adaptation, as they are the key strategies of LLMs\nto acheive the aforementioned capabilities. First and foremost,\nthe emergent abilities of foundation models typically exist only\nin backbones with a large number of parameters, whereas the\nparameter count of graph neural networks is significantly smaller\nthan that of the language backbones. This implies that the back-\nbone of graph foundation models may need to be redesigned to\nachieve more substantial knowledge storage towards emergence.\nAs graph data is typically associated with rich text information, an\nalternative approach is to use LLMs as graph foundation models.\nNonetheless, it remains uncertain whether LLMs can effectively\nhandle graph data and associated tasks, and it is crucial to deter-\nmine how to model graph structures in LLMs. Additionally, the\nhomogenization of foundation models necessitates the handling of\ndiverse tasks in a uniform manner. Devising effective pretext tasks\nand downstream task adaptation methods are challenging for graph\ndata, due to the complexity in interconnected nodes and various\nforms of attributes, as well as the diversity in tasks across node-,\nedge- and graph-levels. Therefore, there is also a need to design\nsuitable pre-training tasks and adaptation mechanisms.\nWhile there is no definitive solution for designing and imple-\nmenting graph foundation models, this paper surveys some related\nresearches and categorizes them into three distinct approaches\nbased on their reliance on GNNs and LLMs: (1) GNN-based\nModels: They aim to enhance existing graph learning paradigms\nthrough innovations in the backbone, pre-training, and adaptation\naspects; (2) LLM-based Models: They explore the feasibility of\nusing an LLM as a graph foundation model by converting graphs\ninto text or tokens; (3) GNN+LLM-based Models: They explore\nvarious forms of synergy between GNNs and LLMs to empower\nthem with enhanced capabilities.\nTo the best of our knowledge, this is the first survey towards\ngraph foundation models. Existing surveys of foundation models\ntypically explore modalities such as language and vision [1, 15],\nrather than graphs. Additionally, there are two surveys [16, 17]\ndedicated to knowledge graphs and large language models, but\nknowledge graphs, due to their distinct nature in construction and\napplication, fall outside the scope of this article. We have also\nnoticed a very recent article that mentions the concept of large\ngraph models [18], but it emphasizes opinion statements and lacks\na systematic taxonomy. Therefore, the contributions of this article\ncan be summarized as follows:\n\u2022\nThis article defines the concept of graph foundation mod-\nels for the first time, and examines the core issues and\ncharacteristics of their capabilities.\n\u2022\nThis article introduces a novel taxonomy and discusses the\nstrengths and limitations of each approach towards graph\nfoundation models.\n\u2022\nThis article provides promising future directions towards\ngraph foundation models.\nThe subsequent sections of this article are organized as\nfollows. In Section 2, we introduce the background related to\ngraph foundation models. Section 3 defines graph foundation\nmodels and highlights their similarities and differences with lan-\nguage foundation models. Sections 4 - 6 delve into the relevant\nworks that consider GNN-based models, LLM-based models and\nGNN+LLM-based models as graph foundation models, separately.\nSection 7 engages in a discussion on the future directions of graph\nfoundation models. In Section 8, we summarize the key points of\nthis paper.\n2\nBACKGROUND\nBefore introducing the concept of graph foundation models, in\nthis section, we first review some background knowledge, namely,\ndeep graph learning and language foundation models. Specifically,\nwe introduce them from three aspects: data, backbone architec-\ntures, and learning paradigms.\n2.1\nDeep Graph Learning\nMany real-world systems naturally find their representations in the\nform of graphs. Deep graph learning holds significant importance\nacross various fields for its powerfulness in modeling and repre-\nsenting complex relationships and interactions among entities. In\nthis section, we provide a concise overview that covers the main\nsteps of deep graph learning, which consists of three key aspects:\ngraph data, backbone architectures and learning paradigms.\n2.1.1\nGraph Data\nGraph is a versatile and powerful data representation that captures\nintricate relationships and dependencies among entities in a net-\nwork. The graph data has the following several characteristics. (1)\n3\nNon-Euclidean Nature: Graph data is inherently non-Euclidean,\nas it lacks the rigid geometric structure found in traditional data\nformats [19]. In contrast to Euclidean data with fixed neighbor-\nhood region and a deterministic order, graph data explicitly encode\ncomplex connections between entities. (2) Various Domains:\nGraph data is ubiquitous across various domains, including social\nnetworks [20], biology [21] and transportation [22], etc. Graph\ndata in various domains can exhibit distinct characteristics, includ-\ning different node types, edge semantics, and structural patterns.\nFor example, in biological networks, nodes can represent proteins,\ngenes, or metabolites, while edges may indicate interactions\nsuch as protein-protein interactions or metabolic reactions. This\ndomain-specific variability makes it challenging to create a one-\nsize-fits-all model that can effectively generalize and adapt to di-\nverse graph structures [23]. (3) Various Types: Graph data comes\nin various types, including homogeneous graphs [24], heteroge-\nneous graphs [25], hypergraphs [26] and dynamic graphs [27], etc.\nHomogeneous graphs contain nodes and edges of the same type,\nsuch as a citation network composed of papers. Heterogeneous\ngraphs include more than one type of node or edge, such as a\ncitation network containing both authors and papers. Hypergraphs\nare composed of hyperedges connecting multiple nodes and can\nmodel high-order relationships among nodes. Dynamic graphs\nrefer to graph structures where nodes and edges change over time,\nas seen in a traffic network formed by changing traffic flow.\n2.1.2\nBackbone Architectures\nAs the current mainstream backbone architecture, graph neural\nnetworks (GNNs) have emerged as a powerful framework for\ndeep graph learning. Most GNNs follow the message-passing\nframework [28], which enables nodes in a graph to exchange in-\nformation with their neighbors. For example, GCN [9] introduces\nthe concept of graph convolution layers and lays the foundation for\nmany subsequent GNN architectures. GraphSAGE [29] proposes\na method for generating embeddings for nodes in large graphs\nusing inductive learning. Additionally, GAT [30] introduces the\nattention mechanism to GNNs, allowing nodes to weigh the\nimportance of their neighbors during message passing, enhancing\ntheir expressive power. These works have significantly contributed\nto the advancement of GNNs, making them versatile tools for deep\ngraph learning.\nDespite the fact that deeper neural networks can achieve\nstronger expressive power [31], deepening GNNs is not easy. The\nreason is that as the number of layers in GNN increases, too much\ninformation is introduced into the message aggregation process,\ncausing the representations of all nodes to become similar [32].\nThis is also known as the over-smoothing problem. Furthermore,\nan increase in the number of layers leads to an exponential growth\nin the receptive field, giving rise to the over-squashing prob-\nlem [13], where a substantial amount of information is compressed\ninto fixed-length node vectors. In recent years, several efforts\nhave been made to address the over-smoothing problem and over-\nsquashing problem in deep graph neural networks, resulting in\nimproved downstream task performance. For example, innovations\nlike DropEdge [33] enhance GCN models by randomly removing\nedges for improved performance and scalability (up to 64 layers).\nAnother thread of work to improve the expressive power of GNNs\nis graph transformer [34, 35, 36]. Thanks to its fully connected\nattention mechanism and the long-range relationship modeling\ncapability, the graph transformer architecture can alleviate the\nover-smoothing problem and over-squashing problem [37].\n2.1.3\nLearning Paradigms\nThe learning paradigms for deep graph learning encompass three\nprimary categories: supervised learning, semi-supervised learning\nand unsupervised learning. In this section, we will provide a brief\nintroduction to these learning paradigms.\nSupervised learning. In the supervised setting, algorithms\nleverage a training dataset comprising input data paired with\ncorresponding output labels. This paradigm finds practical ap-\nplications in tasks such as graph classification [38] and graph\nregression [39]. For example, in the molecular property prediction\ntask [40], GNN models are trained to predict specific chemical\nproperties or attributes of molecules using labeled training data,\nenabling the discovery of valuable insights for drug development\nand materials research.\nSemi-supervised learning. Semi-supervised learning, as high-\nlighted in a recent study [41], constitutes the primary focus of\ndeep graph learning. This approach harnesses both labeled and\nunlabeled data to enhance model performance, with node classi-\nfication [9] emerging as a prominent application. The message-\npassing mechanism [28] empowers GNNs to iteratively exchange\ninformation among neighboring nodes. This capability enables the\npropagation of information throughout the graph, effectively incor-\nporating both labeled and unlabeled data to facilitate predictions.\nFurthermore, GNNs can also be combined with traditional meth-\nods like label propagation to further enhance their performance in\nsemi-supervised settings [42].\nUnsupervised learning. Unsupervised learning [43] is a\nbroader machine learning approach to learn patterns and structures\nfrom data without manual labels. For example, graph cluster-\ning [44] aims to discover inherent structures and patterns within\nthe graph solely based on the relationships and connectivity\nbetween nodes. Another example is link prediction, which aims\nto predict missing or upcoming connection relationships. An\nimportant subclass of unsupervised learning is self-supervised\nlearning, which aims to generate labels using information inherent\nin the data itself [45]. Based on self-supervised learning, GNNs\ncan be trained end-to-end and applied to downstream tasks such\nas graph clustering [12] and link prediction [10].\n2.2\nLanguage Foundation Models\nAI is currently undergoing a transformative shift marked by the\nemergence of some specific natural language models (such as\nGPT-3) that are trained on extensive and diverse datasets using\nlarge-scale self-supervised learning. These models, known as\nfoundation models, are able to produce a wide array of outputs,\nenabling them to tackle a broad spectrum of downstream tasks.\nIn contrast to the deep graph learning pipeline, the founda-\ntion model\u2019s approach embraces a pre-training and adaptation\nframework, enabling the model to achieve several significant\nadvancements, including the emergence [2] and homogenization\n[1]. Foundation models have primarily established themselves in\nthe field of NLP [1], so our discussion will focus on language\nfoundation models in this section.\n2.2.1\nLanguage Data\nLanguage data refers to text or spoken content in a human\nlanguage, encompassing the grammar rules of the natural language\nand the associated semantics of the words. It can include written\ndocuments, transcribed audio recordings, and any other form\nof language-based communication. Language data is essential\n4\nfor many NLP tasks, such as machine translation, sentiment\nanalysis and text summarization. Researchers and developers use\nlanguage data to train and evaluate language models and other\nNLP algorithms. The quality and quantity of language data play a\ncrucial role in the performance of NLP systems, impacting their\naccuracy, robustness, and overall effectiveness in various language\ntasks. In contrast to computer vision and other domains, the size\nof annotated language data is rather small, consisting of only a\nfew thousand sentences [46]. This limitation is primarily due to\nthe high cost of manual annotation. Nevertheless, there is a vast\namount of unlabeled language data available from sources such\nas the internet, newspapers, and books, creating opportunities\nfor utilizing unlabeled data in model pre-training. Furthermore,\ncompared to graph data, language data as Euclidean data is easier\nto model, and its rich semantic information significantly enhances\nthe knowledge transferability of language models.\n2.2.2\nBackbone Architectures\nAn early breakthrough in foundation models is pre-trained lan-\nguage models (PLMs), designed to capture context-aware word\nrepresentations, which proved remarkably effective as versatile\nsemantic features. For instance, BERT [47], grounded in the\nparallelizable Transformer architecture [48] with self-attention\nmechanisms, is conceived through the pre-training of bidirectional\nlanguage models with specifically designed pretext tasks on vast\nunlabeled data. This landmark study significantly elevates the\nperformance benchmarks for NLP tasks and serves as a catalyst\nfor a plethora of subsequent research, establishing the prevailing\npre-training and fine-tuning learning paradigm.\nFurthermore, researchers have observed that increasing the\nscale of PLMs, whether by augmenting model size or training data,\nfrequently results in increased model capacity for downstream\ntasks. These larger PLMs, collectively referred to as LLMs, ex-\nhibit distinctive behaviors compared to their smaller counterparts\n(e.g., the 1.5B-parameter GPT-2 and 175B-parameter GPT-3).\nAfter training on massive text datasets, they manifest remarkable\ncapabilities, often referred to as emergent abilities [2], such as\nin-context learning [1]. LLMs primarily utilize the Transformer\narchitecture, because highly parallelizable Transformer-based ar-\nchitectures accelerate the pre-training stage and enable the utiliza-\ntion of massive datasets. In the context of Transformer models,\ntokens serve as the input and represent units at the word level in\nnatural language texts. Typically, LLMs containing hundreds of\nbillions (or more) of parameters [49], exemplified by models such\nas GPT-3 [50], PaLM [51], Galactica [52], and LLaMA [53].\n2.2.3\nLearning Paradigms\nAs the number of model parameters has rapidly increased, the de-\nmand for significantly larger datasets has grown to effectively train\nthese parameters and avoid overfitting. Given the extremely expen-\nsive costs associated with building large-scale labeled datasets,\nthe importance of utilizing extensive unlabeled text data has been\nunderscored. Leveraging these unlabeled datasets involves a two-\nstep approach: first, achieving universal representations through\nself-supervised learning, and subsequently employing these rep-\nresentations for various tasks [54]. Based on different adaptation\napproaches, learning paradigms can be categorized into two types:\npre-train and fine-tune and pre-train, prompt, and predict [55].\nPre-train and Fine-tune. In this paradigm, a model with a\nconsistent architecture is initially pre-trained as a language model\n(LM), where it predicts the probability of observed textual data.\nIn comparison to end-to-end training, pre-training offers distinct\nadvantages and forms cornerstone for the capabilities of founda-\ntion models. Firstly, pre-training on huge text corpus enables the\nlearning of universal language representations, which is a pos-\nsible reason to explain emergent abilities [54]. Additionally, pre-\ntraining provides improved model initialization, typically resulting\nin enhanced generalization performance, enabling homogenization\nof multiple tasks [54]. Furthermore, pre-training serves as a form\nof regularization, helping to prevent overfitting on smaller datasets\n[56]. For instance, models like GPT-3 [50] are trained with a\nlanguage modeling objective, rewarding them for their abilities\nin emergence and homogenization.\nFollowing the pre-training phase, foundation models acquire\ngeneral-purpose capabilities suitable for a broad spectrum of\ntasks. Nevertheless, pre-trained models still lack downstream\ntask-specific information, and using them directly may not yield\noptimal results. Therefore, we need to tune the model for specific\ntasks, which is known as fine-tuning. Building upon the success of\nmodels like ULMFit [57] and BERT [47], fine-tuning has emerged\nas the predominant method for adapting pre-trained models. In this\nframework, the primary emphasis lies in objective engineering, en-\ncompassing the design of training objectives for both pre-training\nand fine-tuning phases. For instance, Pegasus [58] shows that\nincorporating a loss function for predicting important sentences\nwithin a document results in an improved pre-trained model for\ntext summarization. The advantage of fine-tuning is that it can\ntransfer knowledge between source and target tasks (or domains)\nand benefit the model\u2019s performance. For the small size of fine-\ntuning dataset compared to pre-training dataset, this process can\nenable adaptation effectively without losing the stored structural\nlanguage knowledge.\nPre-train, Prompt and Predict. In this paradigm, rather than\nadjusting pre-trained language models to suit specific downstream\ntasks, the approach involves reshaping the downstream tasks to\nalign more closely with those tackled during the original LM\ntraining, accomplished by providing textual prompts. By selecting\nsuitable prompts, we can steer the LM\u2019s behavior so that it can\npredict the desired output, sometimes without any additional task-\nspecific training. This method offers the advantage of enabling a\nsingle, entirely unsupervised LM, when equipped with a set of\nfitting prompts, to handle a wide array of tasks [55].\nFrom the aspect of prompt engineering, the approaches to\ncreate a proper prompts can be classified to manual methods and\nautomated methods. Manual methods involve creating intuitive\ntemplates based on human insight, which is the most straightfor-\nward approach to crafting prompts. For instance, the influential\nLAMA dataset [59] offers manually devised cloze templates\nto assess the knowledge of language models. However, manual\nmethods face challenges in terms of high cost and precision. To\naddress these issues, some approaches have started to experiment\nwith automated prompt generation. For example, Prompt Mining\n[60] is a template discovery approach that autonomously identifies\ntemplates based on a given set of training inputs and outputs.\nLooking at it from a different perspective, in terms of how\nmodels and prompts are combined to generate results, prompting\nstrategies can be categorized into three approaches: tuning-free\nprompting, prompt tuning, and instruction tuning [5]. Tuning-free\nprompting directly generates answers based solely on a prompt\nwithout altering the parameters of the pre-trained LLMs [53]\n[61]. Prompt tuning introduces supplementary prompt-relevant\nparameters in addition to the parameters of the pre-trained models\n5\nand update these additional parameters using supervision signals\nacquired from downstream training samples [62] [63]. Instruction\ntuning adapts the LM\u2019s parameters in a manner similar to the fine-\ntuning process, while additionally incorporating fixed instructions\nto guide the model\u2019s behavior. This approach offers potential\nenhancements, especially in zero-shot scenarios [64].\n3\nGRAPH FOUNDATION MODELS\nIn this section, we will first formally define the concepts of graph\nfoundation models, including the definition, key characteristics\nand key technologies. Then, we will discuss the impact from graph\ndata and graph tasks on graph foundation models. Finally, we will\ndiscuss the similarities and differences between graph foundation\nmodels and language foundation models.\n3.1\nConcepts of Graph Foundation Model\nIn this subsection, we will first provide a definition of graph\nfoundation models. Following that, we will delve into the key\ncharacteristics and essential techniques of graph foundation mod-\nels, as well as the impact of graph data and graph tasks on graph\nfoundation models.\n3.1.1\nDefinition and Key Characteristics\nWe define a graph foundation model as follows:\nDefinition A graph foundation model (GFM) is a model that\nis expected to benefit from the pre-training of broad graph data,\nand can be adapted to a wide range of downstream graph tasks.\nCompared to deep graph learning that employs end-to-end\ntraining, GFMs use pre-training to obtain the knowledge from\na substantial amount of unlabeled graph data, and then use\nadaptation techniques to tailor to various downstream tasks. Some\nstudies [65, 66] have already demonstrated that the paradigm of\npre-training and adaptation outperform deep graph learning in\ncertain scenarios, e.g., few-shot learning [65], showcasing their\nsuperior expressive power and generalization ability. Unlike deep\ngraph learning that aims to achieve better performance on a\nsingle task, a GFM is expected to have two key characteristics:\nemergence and homogenization.\nEmergence. Emergence means that the graph foundation\nmodel will exhibit some new abilities when having a large parame-\nters or trained on more data, which are also referred to as emergent\nabilities. Inspired by the various emergent abilities [67, 68, 69]\npossessed by foundation models, we expect GFMs to have similar\nabilities, including in-context learning, graph reasoning, and zero-\nshot graph generation, etc. In-context learning allows predictions\nfor various downstream tasks with few examples [70]. Graph\nreasoning decomposes a complex problem into multiple sub-\nproblems based on the graph structure and addresses them step\nby step, such as solving graph algorithm problems [71]. Zero-\nshot graph generation requires the model to generate graphs based\non the desired conditions without the need for any examples [72].\nNote that although language foundation models have demonstrated\nvarious emergent abilities, only a few works [70, 71, 72] have\nexplored emergent abilities of GFMs so far.\nHomogenization. Homogenization means that the graph foun-\ndation model can be applied to different formats of tasks, such\nas node classification, link prediction and graph classification.\nNote that due to the distinct characteristics of tasks on graphs\ncompared to NLP tasks, achieving homogenization is not straight-\nforward. The fundamental question in achieving homogenization\nis to decide which form to unify different types of graph tasks.\nExisting works have attempted homogenization through link pre-\ndiction [65] or graph-level tasks [66], but there is no consensus on\nwhich approach is superior.\n3.1.2\nKey Technologies\nGraph foundation models primarily comprise two key techniques:\npre-training and adaptation. This section will provide a brief\noverview of these two techniques.\nPre-training. Pre-training is a pivotal concept in the devel-\nopment of graph foundation models, akin to its role in language\nmodels. It involves pre-training a neural network on a large graph\ndataset in a self-supervised manner. During pre-training, the model\nlearns to capture the structural information, relationships, and\npatterns within the graph. There are several pre-training strategies\nfor graph foundation models. Contrastive self-supervised learn-\ning [73, 74] leverages the idea of learning representations by con-\ntrasting positive samples (e.g., similar node pairs) against negative\nsamples (e.g., dissimilar node pairs). Generative self-supervised\nlearning [75, 76] encourages the model to reconstruct the structure\nor predict the features of original graph data. If using LLM as\na part of the graph foundation model, we can also employ the\npre-training methods introduced in Section 2.2.3. These diverse\npre-training approaches enable graph foundation models to learn\nmeaningful representations from raw graph data, enhancing their\ngeneralization and adaptability across various graph tasks.\nAdaptation. The adaptation of graph foundation models in-\nvolves tailoring these models to specific downstream tasks or do-\nmains to enhance their performance. This process includes several\ntechniques, i.e., vanilla fine-tuning, parameter-efficient fine-tuning\nand prompt-tuning. Vanilla fine-tuning (Vanilla FT) entails train-\ning the entire pre-trained model on task-specific data, allowing for\nthe highest level of customization but often requiring substantial\ndata and computational resources. Parameter-efficient fine-tuning\n(Parameter-efficient FT) [77, 78], on the other hand, adjusts only a\nsubset of the model\u2019s parameters, striking a balance between task-\nspecific adaptation and resource efficiency. Prompt-tuning [66, 79]\nis a versatile approach that relies on external prompts to guide\nthe model\u2019s behavior, making it more adaptable and effective.\nThese adaptation techniques enable graph foundation models to\nexcel in a wide range of applications by leveraging their pre-\ntrained knowledge while tailoring their capabilities to specific\ntasks or domains, making them valuable for diverse downstream\napplications. Note that although LLMs have developed various\ntypes of prompt-tuning methods [55] and some other efficient\ntuning methods, such as Prefix Tuning [62], there are relatively\nfew prompt tuning methods for graph foundation models.\n3.1.3\nImpact from Graph Data\nThe success of foundation models depends on high-quality train-\ning data, and foundation models exhibit significantly different\nperformance on various types of test data. In this section, we\ndiscuss the impact of graph data on graph foundation models from\nthree aspects: graph type, graph scale and graph diversity.\nGraph Type. Based on the number of categories of nodes and\nedges in a graph, we can categorize graphs into homogeneous\ngraphs and heterogeneous graphs. In homogeneous graphs, all\nnodes and edges belong to the same category. For example, in a\nsocial graph where nodes represent individuals (users) and edges\nrepresent friendship relationships, it is a homogeneous graph\nbecause all nodes are individuals and all edges represent friendship\n6\nrelationships. Heterogeneous graphs, on the other hand, have more\nthan one type of nodes or edges, representing different types\nof entities and relationships [25]. For instance, an e-commerce\ngraph may include nodes for users, products, and purchase rela-\ntionships, forming a heterogeneous graph. For graph foundation\nmodels, handling heterogeneous graphs poses greater challenges\nand typically requires the design of specific backbone architectures\nand optimization objectives. Nonetheless, utilizing the meta-path\nbased approach [80], a heterogeneous graph can be mapped into\nmultiple homogeneous graphs, one for each meta-path. For exam-\nple, one can apply the GFM trained on homogeneous graphs to\neach of these meta-path induced homogeneous graphs, separately,\nto get node embedding. Then, these embeddings on homogeneous\ngraphs under different meta-paths can be fused together. However,\nbeyond homogeneous graphs and heterogeneous graphs, there are\nsome more complex types of graphs in the real world, such as\ndynamic graphs and hypergraphs [81], which poses additional\nchallenges for GFM.\nGraph Scale. Based on the number of nodes and edges in\na graph, we can categorize graphs into relatively small graphs\nand large graphs. Small graphs are of smaller scale, typically\ncontaining dozens to hundreds of nodes and edges. For exam-\nple, chemical molecular graphs represent the structure of small\nmolecules and typically consist of dozens to hundreds of atoms.\nLarge graphs, on the other hand, refer to graphs with a significant\nnumber of nodes and edges, often encompassing millions or even\nbillions of nodes and edges. For instance, e-commerce graph\nin Alibaba includes billons of nodes and hundreds of billion\nedges [82]. For graph foundation models, large graphs impose\nhigher demands on the capacities of graph foundation models.\nFirstly, large graphs, due to their numerous nodes and typically\nsparser edges, introduce more noise and pose greater challenges\nin terms of storage and computation [83]. Additionally, large\ngraphs often exhibit long-range dependency relationships [84],\nrequiring more neural network layers and a higher number of\nparameters, which exacerbates the over-smoothing [32] and over-\nsquashing [13] problem of GNN-based models.\nGraph Diversity. Based on whether a graph dataset originates\nfrom the same domain, we can categorize graphs into same-\ndomain graphs and cross-domain graphs. Same-domain graphs\nrefer to graph data from similar or related domains, typically\ncontaining nodes and edges of similar types. For example, the\nsocial graphs of Facebook and WeChat come from similar do-\nmains. Cross-domain graphs [85], on the other hand, involve graph\ndata from different domains or data sources, often comprising\nnodes and edges of different types, aimed at addressing multi-\ndomain problems or cross-domain tasks. For example, academic\nnetworks and molecular graphs come from different domains.\nFor graph foundation models, supporting cross-domain graphs\npresents greater challenges because graphs from different domains\nlack a unified underlying semantics. This can result in weak trans-\nfer performance or even negative transfer when applying the model\nto a new dataset [86]. Therefore, addressing the heterogeneity of\ndifferent domains and enabling the same GFM to be applied to\ngraphs from different domains is a significant challenge for GFMs.\n3.1.4\nImpact from Graph Tasks\nLanguage foundation models can be widely applied to various\nNLP tasks, while for graph foundation models, the formats of\ngraph tasks are also quite diverse and can be categorized into three\nclasses: node-level tasks, edge-level tasks, and graph-level tasks.\nNode-level Tasks. Node-level tasks refer to the classifica-\ntion, regression, or prediction performed on each node. Common\nnode-level tasks include node classification, node regression, and\nclustering coefficient prediction. For example, in social networks,\ngraph nodes can represent users, and node classification can be\nused to identify users from different social circles or with varying\ninterests.\nEdge-level Tasks. Edge-level tasks involve the classification,\nregression, or prediction performed on each individual edge. Com-\nmon edge-level tasks include edge classification, link prediction,\nshortest path prediction, connectivity prediction, and maximum\nflow prediction. For example, in e-commerce, link prediction can\nbe used to predict products that users may be interested in.\nGraph-level Tasks. Graph-level tasks focus on the entire\ngraph. Common graph-level tasks include graph classification,\ngraph regression, graph generation, graph clustering, graph con-\ndensation and average clustering coefficient prediction. For exam-\nple, in bioinformatics, graph property prediction can be used to\npredict the biological activity or toxicity of molecular compounds,\nthereby accelerating the drug discovery process.\nIn summary, the format of tasks in graphs are highly diverse\nand can be categorized into three types: node-level, edge-level,\nand graph-level, each of which has wide-ranging applications.\nThis undoubtedly increases the challenge of homogenization for\ngraph foundation models. For example, in graph classification and\nnode classification tasks on synthetic datasets, modeling structural\ninformation is often more crucial [87]. On the other hand, when\ndealing with node classification tasks on graphs with rich node fea-\ntures, modeling feature information becomes more important [87].\nFurthermore, tasks that are more similar to each other will also\nhave a lower transfer difficulty, implying that these tasks are more\nlikely to be addressed using the same GFM. While increasing\nexpressive power holds promise for improving the performance\nof many node-level, edge-level, and graph-level tasks [88], there\nis also some work suggesting that overly strong expressive power\nmay not be necessary for graph generation tasks [89].\n3.2\nComparison with Language Foundation Model\nThrough conceptual comparison, we can observe similarities in the\ngoals and learning paradigms between graph foundation models\n(GFMs) and language foundation models (commonly referred\nto as large language models, LLMs). However, the uniqueness\nof graph data and graph tasks creates fundamental differences\nbetween them, which we refer to as their intrinsic differences.\nFurthermore, due to the relatively limited research on GFMs at\npresent, many issues that have been extensively explored in LLMs\nremain unresolved, which we refer to as their extrinsic differences.\nWe summarize the similarities and differences between GFMs and\nLLMs in Table 1, and will delve into them in detail in this section.\n3.2.1\nSimilarities\nAs shown in Table 1, both language foundation models and graph\nfoundation models share the common goal of enhancing a model\u2019s\nexpressive power and improving its ability to generalize across\na wide range of tasks. They aim to create versatile, pre-trained\nmodels that can be adapted for specific applications. In addition,\nboth follow the pre-training and adaptation paradigm. They begin\nby pre-training a model on a large, diverse dataset and then adapt\nit to task-specific data.\n7\nTABLE 1: The relationship between language foundation model and graph foundation model.\nLanguage Foundation Model\nGraph Foundation Model\nSimilarities\nGoal\nEnhancing the model\u2019s expressive power and its generalization across various tasks\nParadigm\nPre-training and Adaptation\nIntrinsic\ndifferences\nData\nEuclidean data (text)\nNon-Euclidean data (graphs) or a mixture of Euclidean\n(e.g., graph attributes) and non-Euclidean data\nTask\nMany tasks, similar formats\nLimited number of tasks, diverse formats\nExtrinsic\ndifferences\nBackbone Architectures\nMostly based on Transformer\nNo unified architecture\nHomogenization\nEasy to homogenize\nDifficult to homogenize\nDomain Generalization\nStrong generalization capability\nWeak generalization across datasets\nEmergence\nHas demonstrated emergent abilities\nNo/unclear emergent abilities as of the time of writing\n3.2.2\nIntrinsic Differences\nThe intrinsic differences between GFM and LLM primarily man-\nifest in two aspects: data and tasks. As for input data, lan-\nguage foundation models are primarily designed for processing\nEuclidean data, i.e., text. They are trained on vast text corpora,\nwhich are inherently sequential and follow a linear order of\nwords or tokens. Graph foundation models, on the other hand,\nare designed to handle non-Euclidean data (represented as graph\nstructures) or a mixture of Euclidean data (like graph attributes)\nand non-Euclidean data. Compared to text data, graph data can\ncapture complex data relationships and is typically more sparse.\nMoreover, as mentioned in Section 3.1.3, different graphs may\nexhibit significant differences in type or scale, all of which pose\nchallenges in the design of GFMs. Furthermore, language data,\neven when sourced from texts in different domains, still share\na common vocabulary. On the other hand, different graph data\nmay lack this common foundation. For instance, nodes represent\natoms in a molecular graph, while nodes represent users in a social\nnetwork graph, which are entirely different.\nIn addition, LLMs are typically designed to handle dozens of\ntasks [90], but these tasks can all be unified under the format\nof masked language modeling. The reason is that these tasks all\ninvolve processing textual data and using the syntax and semantic\ninformation within the text. In contrast, GFMs focus a narrower set\nof tasks but with diverse formats. They excel at graph tasks such\nas node classification, link prediction and graph classification. The\ndifferences in tasks imply that GFMs cannot be learned using\nmethods similar to those in LLMs, significantly increasing the\nadaptability challenges of GFMs in downstream tasks.\n3.2.3\nExtrinsic Differences\nIn addition to the intrinsic differences in data and tasks, there are\nalso some extrinsic differences between GFMs and LLMs, which\nare due to the lag in technological advancements in GFMs. This\nsection summarizes these differences as follows:\nBackbone Architectures. Language foundation models, such\nas GPT-3 [50] and LLaMA [53], are mostly based on the Trans-\nformer architecture. The advantages of Transformer in terms of\nexpressive power, scalability, parallelizability, and its excellent\nperformance in handling various NLP tasks have made it the\nmainstream backbone architecture for LLMs. However, for GFMs,\nusing mainstream GNNs as the backbone architecture may not\nnecessarily be suitable. This is mainly because the expressive\npower and generalization of GNNs have limitations, and their\nparameter sizes are often too small to exhibit emergent abilities.\nDespite recent research efforts to design Graph Transformers [74]\nor models that incorporate LLMs [91], there is still no unified\nbackbone architecture for GFMs.\nHomogenization. Language foundation models are relatively\neasy to homogenize. This means that various NLP tasks can be\nformulated as the same task [92], making it possible to use a\nsingle model with unified training paradigm for a wide range of\ntasks. However, due to the poor transferability of graph struc-\ntural knowledge, homogenization is more challenging for GFMs.\nExisting work attempts to achieve homogenization by unifying\nvarious tasks as link prediction [65] or graph-level tasks [66].\nAdditionally, constructing a data-task heterogeneous graph [70]\nmay establish connections between different tasks, but it is a more\ncomplex process.\nDomain Generalization. Language foundation models have\ndemonstrated strong domain generalization capabilities. They can\noften perform well on tasks and datasets that were not seen during\ntraining, showcasing their ability to generalize across various\nlanguage-related domains. However, due to the diversity and lack\nof unified vocabulary of graph data, GFMs generally exhibit\nweaker generalization across datasets, especially when moving to\ncross-domain graph data [86]. Their performance may degrade\nsignificantly when faced with graph structures or characteristics\nthat differ substantially from their training data. Achieving strong\ndomain generalization remains a challenging research problem for\nGFMs.\nEmergence. Language foundation models have shown emer-\ngent abilities, where they can generate coherent and contextually\nrelevant text based on few examples or instructions. Representa-\ntive emergent abilities include in-context learning [67], chain of\nthought reasoning [68] and zero-shot generation [69]. However,\nGFMs have not demonstrated obvious emergent abilities to the\nsame extent as language foundation models. Only a few recent\nstudies discuss the in-context learning [70], graph reasoning [71]\nand zero-shot graph generation [72] abilities of GFMs.\n3.3\nSummary\nIn this section, we define the concept of graph foundation models\nand related technologies, and compares graph foundation models\nwith language foundation models. In the following sections, we\nwill introduce three categories of methods for implementing graph\nfoundation models, along with representative works for each\nmethod, as illustrated in Figure 2. GNN-based models use GNN as\nthe backbone architecture, while LLM-based models transform the\ngraph into the input format of LLM and use LLM as the backbone\n8\narchitecture. GNN+LLM-based models, on the other hand, utilize\nboth GNN and LLM as the backbone architecture simultaneously.\nThe distinction in backbone architecture also impacts the methods\nfor pre-training and adaptation. Therefore, in the following sec-\ntions, we will introduce the backbone architectures, pre-training,\nand adaptation strategies for each category of methods, seperately.\n4\nGNN-BASED MODELS\nThanks to effective model architectures and training paradigms,\nlanguage models have achieved remarkable performance in nat-\nural language processing tasks. The backbone, pre-training and\nadaptation techniques employed in language models have inspired\na series of corresponding efforts in the field of graph-based\ntasks. In this section, we will delve into GNN-based models,\nwhich draw inspiration from the model architectures or training\nparadigms used in NLP to apply them to graph tasks. Importantly,\nunlike the LLM-based models and GNN+LLM-based models to be\nintroduced in the following sections, GNN-based models do not\nexplicitly model text data in their pipeline. We have summarized\nand categorized the works mentioned in this section in Table 2.\n4.1\nBackbone Architectures\nNumerous GNNs have been proposed and they have been widely\nused in various graph-related downstream tasks. These networks\nare leveraged for feature extraction, often serving as the foun-\ndational components of graph models, commonly referred to\nas \u201cbackbones\u201d. In this subsection, we introduce two advanced\nGNN backbones: message-passing-based and transformer-based\nmethods.\n4.1.1\nMessage Passing-Based Methods\nMessage Passing Neural Networks (MPNNs) [28] represent a\nbroad category of GNN architectures that operate based on the\nconcept of message passing between nodes in a graph. In the\nmessage passing mechanism, each node aggregates information\nfrom its neighboring nodes, processes the information, and then\nsends messages to its neighbors in a series of iterative steps. A\ntypical message passing process can be formulated as :\nhk+1\nv\n= U k(hk\nv, M k\nu\u2208N(v)(hk\nv, hk\nu, Xe\n(u,v))),\n(1)\nwhere hk\nv and hk+1\nv\ndenote the embedding of node v at layer k\nand layer k + 1, Xe\n(u,v) denotes the edge attribute of edge (u, v),\nN(v) denotes the neighbors of node v, M k\nu\u2208N(v)(\u00b7) and U k(\u00b7)\ndenote the message aggregation and update function at layer k.\nMany existing GNN-based models utilize message passing-\nbased models as their backbone. Due to the simplicity and effec-\ntiveness, several studies [66, 70, 73, 93, 94, 95] adopt GCN [9] as\ntheir backbone architecture, where GCN [9] employs a localized\nfirst-order approximation of spectral graph convolutions for the\ndual purpose of capturing graph structure and encoding node\nfeatures. Several studies [66, 70, 75, 76] adopt GAT [30] as\ntheir backbone architecture, where GAT [30] replaces the aver-\nage aggregation operation in GCN with a weighted aggregation\napproach, facilitated by an attention mechanism. Additionally,\na multi-head attention technique can be further used in GAT\nto enhance its performance. GPPT [79] and VPGNN[99] uses\nGraphSAGE [29] as their backbone, which operates by sam-\npling a fixed-size subset of neighboring nodes for each target\nnode and then learns embeddings by aggregating and processing\nthese sampled neighbors\u2019 embeddings. For heterogeneous graphs,\nsome works\n[100, 101, 102] use HGT [114] as their back-\nbone, which introduces a specialized attention mechanism for\nneighborhood aggregation. Unlike global attention, HGT employs\ntype-specific parameters to define heterogeneous attention over\neach edge within the graph. To improve the expressive power, a\nproportion of studies rely on GIN [11] as their primary architec-\nture [65, 77, 104, 105, 106, 107, 108]. GIN is a message passing-\nbased model with expressive power theoretically equivalent to\na 1-WL test [115]. Due to the expressive power of GIN, it is\nfrequently chosen as the backbone for many GNN-based graph\nmodels. For an in-depth exploration of message passing-based\nGNNs, we recommend referring to [116].\n4.1.2\nGraph Transformer-Based Methods\nWhile GNNs have demonstrated significant success in learning\nfrom graph data, they still confront fundamental limitations,\nincluding issues related to limited expressive power [11], over-\nsmoothing [32] and over-squashing [13]. In parallel, the trans-\nformer architecture [48], which has revolutionized tasks in natural\nlanguage processing [47, 50] and computer vision [117, 118],\nachieving the state-of-the-art performance. It has inspired the\ndevelopment of transformer-based models tailored for graph data\n[34, 35, 36, 119]. Graph transformers have exhibited promising\nresults, particularly in molecular prediction tasks [34], owing to\ntheir fully-connected self-attention mechanism. This mechanism\nenables them to address the shortcomings of traditional message-\npassing GNNs thanks to its long-range modeling capability and\nstrong expressive power.\nThe principal distinction between the backbone architectures\nwith message passing mechanism and the graph transformer\nlies in their treatment of the underlying graph structure. In the\ncase of the graph transformer, it treats the graph as if it were\nfully connected, meaning it considers and measures the similarity\nbetween every pair of nodes in the graph. Conversely, the message\npassing mechanism operates under the constraint of the adjacency\nmatrix of the graph. It only propagates information between nodes\nthat are explicitly connected in the graph structure. We illustrate\nthe difference between message passing-based models and graph\ntransformers in Figure 3.\nCurrently, there are many research efforts focusing on graph\ntransformers. Here we will present part of these studies that\nemploy a pre-training and fine-tuning learning paradigm: Graph-\nBERT [112] uses intimacy-based and hop-based relative positional\nembeddings to encode node positions in a subgraph. The intimacy-\nbased relative positional embeddings capture the relative positions\nof nodes in a subgraph based on their connectivity patterns.\nThe hop-based relative distance embeddings capture the relative\npositions of nodes in a subgraph based on their hop distances.\nGROVER [112] uses a variant of MPNN called Directed Message\nPassing Networks (DyMPNs), which can capture the directed\nnature of molecular graphs and distinguish different types of\nedges. The DyMPNs in GROVER are used to compute the\nnode and edge embeddings in the Transformer-style architecture.\nGraphormer [34] introduces a spatial encoding method to repre-\nsent structural node relationships. It assigns learnable embeddings\nto node pairs based on their spatial proximity, derived from the\nshortest path distance. This distance is incorporated as a bias\nterm in softmax attention to improve spatial dependency capture\nin graphs. SimpleDyG [120] is a Transformer-based model for\ndynamic graph modeling that uses a novel temporal alignment\n9\nTowards Graph Foundation Models\nGNN-based Models\nBackbone Architectures\nMessage Passing-based 4.1.1\nGraph Transformer-based 4.1.2\nPre-training\nContrastive Methods 4.2.1\nGenerative Methods 4.2.2\nAdaptation\nFine-Tuning 4.3.1\nPrompt-Tuning 4.3.2\nLLM-based Models\nBackbone Architectures\nGraph-to-Token 5.1.1\nGraph-to-Text 5.1.2\nPre-training\nLanguage Modelling 5.2.1\nMasked Language Modelling 5.2.2\nAdaptation\nManual Prompting 5.3.1\nAutomatic Prompting 5.3.2\nGNN+LLM-based Models\nBackbone Architectures\nGNN-centric 6.1.1\nSymmetric 6.1.2\nLLM-centric 6.1.3\nPre-training\nGNN or LLM-based 6.2.1\nAlignment-based 6.2.2\nAdaptation\nFine-Tuning 6.3.1\nPrompt-Tuning 6.3.2\nFig. 2: Taxonomy of existing works towards graph foundation models.\n1-hop aggregation\n2-hop aggregation\nMessage Passing\nPredictions\nattention\nGraph Transformer\nPredictions\n(a) Message Passing.\n1-hop aggregation\n2-hop aggregation\nMessage Passing\nPredictions\nattention\nGraph Transformer\nPredictions\n(b) Graph Transformer.\nFig. 3: A comparison between message passing-based models and graph transformer. A fundamental distinction is that the message passing\nmechanism is constrained by the graph structure, and the graph transformer treats the graph as a fully-connected network.\ntechnique, simplifying the modeling process without complex\nmodifications. Recent works have attempted to extend the Graph\nTransformer architecture to heterogeneous graphs [121, 122].\nAdditionally, some studies have delved deeper into the theoretical\naspects of Transformer models\u2019 performance on graph-structured\ndata. Xing et al. [123] identified the over-globalizing problem in\nGraph Transformers and proposed CoBFormer, a bi-level global\ngraph transformer, to balance local and global information and\nenhance generalization. Meanwhile, Rosenbluth et al. [124] com-\npared self-attention mechanisms in Graph Transformers with vir-\ntual nodes, offering theoretical and empirical analyses to highlight\ntheir differences and advantages in handling long-range dependen-\ncies. For a more comprehensive exploration, please refer to other\nliterature on graph transformers [125, 126].\n4.2\nPre-training\nPre-training in the field of NLP involves exposing a model to a\nvast amount of unlabeled text data, allowing it to learn general\nlanguage semantic knowledge in a self-supervised manner. This\npre-training step equips the model with a foundational under-\nstanding of language, enabling it to transfer this knowledge to\ndownstream tasks. Similarly, the graph domain typically includes\nmany unlabeled nodes and graphs, providing opportunities for pre-\ntraining on graphs. Graph pre-training enables the graph models\nto understand graph structure and semantic information, thus\nencoding meaningful node or graph embeddings\n[127, 128].\nRecently, some graph pre-training methods have been proposed\nto learn representations in a self-supervised manner. Based on\nself-supervised tasks, these graph pre-training methods can be\ncategorized into two types: contrastive pre-training and generative\npre-training.\n4.2.1\nContrastive Methods\nSpecifically, the contrastive graph pre-training methods aim to\nmaximize mutual information between different views [127],\nwhich forces the model to capture invariant semantic informa-\ntion across various views. The graph view can vary in scale,\nencompassing local, contextual or global perspectives. These per-\nspectives correspond to node-level, subgraph-level, or graph-level\ninformation within the graph, leading to two distinct categories:\n(1) Same-scale contrastive learning and (2) Cross-scale contrastive\nlearning. Same-scale contrastive learning compares two graph\nviews at the same level. For example, GCC [104] treats the embed-\nding of the node\u2019s subgraph as the node embedding, considering\ndifferent subgraphs of the same node as positive examples and dif-\nferent nodes as negative examples. Afterward, the model employs\nNCE loss to pull the positive samples close and push the nega-\ntive samples away, which forces the encoder to capture general\n10\nTABLE 2: Details of approaches involved as GNN-based models.\nModel\nBackbone Architecture\nPre-training\nAdaptation\nAll In One [66]\nGCN, GAT, Graph Transformer\nSame-Scale CL\nPrompt Tuning\nPRODIGY [70]\nGCN, GAT\nGraph Reconstruction, Supervised\nPrompt Tuning\nDGI [93]\nGCN\nCross-Scale CL\nParameter-Efficient FT\nGRACE [73]\nGCN\nSame-Scale CL\nVanilla FT\nVGAE [94]\nGCN\nGraph Reconstruction, Property Prediction\nVanilla FT\nMA-GCL [95]\nGCN\nSame-Scale CL\nVanilla FT\nMultiGPrompt [96]\nGCN\nCross-Scale CL, Graph Reconstruction\nPrompt Tuning\nIGAP [97]\nGCN, GAT, GraphSAGE\nSame-Scale CL, Cross-Scale CL, Graph Reconstruction\nPrompt Tuning\nHGPROMPT [98]\nGCN, GAT, SimpleHGN\nGraph Reconstruction\nPrompt Tuning\nGraphMAE [75]\nGAT\nGraph Reconstruction\nParameter-Efficient FT\nGraphMAE2 [76]\nGAT\nGraph Reconstruction\nParameter-Efficient FT\nGPPT [79]\nGraphSAGE\nGraph Reconstruction, Cross-Scale CL\nPrompt Tuning\nVPGNN [99]\nGraphSAGE\nCross-Scale CL\nPrompt Tuning\nGPT-GNN [100]\nHGT\nGraph Reconstruction\nVanilla FT\nPT-HGNN [101]\nHGT\nSame-Scale CL\nVanilla FT\nCPT-HG [102]\nHGT\nSame-Scale CL\nVanilla FT\nGraphPrompt [65]\nGIN\nGraph Reconstruction\nPrompt Tuning\nGraphPrompt+ [103]\nGIN\nGraph Reconstruction, Coress-Scale CL, Same-Scale CL\nPrompt Tuning\nGCC [104]\nGIN\nSame-Scale CL\nVanilla FT\nGraphCL [105]\nGIN\nSame-Scale CL\nParameter-Efficient FT\nAdapterGNN [77]\nGIN\nCross-Scale CL, Graph Reconstruction, Same-Scale CL\nParameter-Efficient FT\nAAGOD [106]\nGIN\nSame-Scale CL, Supervised\nPrompt Tuning\nGPF [107]\nGIN\nCross-Scale CL, Graph Reconstruction\nPrompt Tuning\nSGL-PT [108]\nGIN\nSame-Scale CL, Graph Reconstruction\nPrompt Tuning\nFOTOM [109]\nGIN\nSame-Scale CL\nParameter-Efficient FT\nGraphControl [110]\nGIN\nSame-Scale CL\nParameter-Efficient FT\nG-TUNING [111]\nGIN\nSame-Scale CL, Graph Reconstruction\nCustomized FT\nGraph-BERT [112]\nGraph Transformer\nGraph Reconstruction, Supervised\nVanilla FT\nGROVER [113]\nGraph Transformer\nProperty Prediction\nVanilla FT\nG-Adapter [78]\nGraph Transformer\nSupervised, Graph Reconstruction, Property Prediction\nParameter-Efficient FT\npatterns. GraphCL [105] and GRACE [73] generate two views by\ngraph augmentation and then employ the InfoNCE loss to contrast\nnode-level embeddings, pushing the graph model to acquire the\ninvariant representations. MA-GCL [95] focuses on manipulating\nthe neural architectures of view encoders instead of perturbing\ngraph inputs or model parameters. CPT-HG [102] proposes two\npre-training tasks. The relation-level pre-training task encodes the\nrelational semantics which constitute the basis of heterogeneity on\na heterogeneous graph, while the subgraph-level pre-training task\nencodes high-order semantic contexts. PT-HGNN [101] considers\nboth node- and schema-level pre-training tasks. The node-level\npre-training task utilizes node relations to encourage the GNN to\ncapture heterogeneous semantic properties, while the schema-level\npre-training task utilizes the network schema to encourage the\nGNN to capture heterogeneous structural properties. Cross-scale\ncontrastive learning compares two graph views at different levels.\nFor example, DGI [93] utilizes a discriminator to maximize the\nmutual information between the node embeddings and the graph\nembedding and minimize the information between node and cor-\nrupted graph embedding. Such a contrastive process encourages\nthe encoder to capture information of the whole graph. Although\nDGI enables the model to capture semantic information about\nnodes and graphs, it ignores the discrepancies between different\nnodes.\n4.2.2\nGenerative Methods\nIn addition to contrastive methods, some generative graph pre-\ntraining approaches have been proposed. The aim of generative\npre-training methods is to enable GNNs to understand the general\nstructural and attribute semantics of graphs. Thus, the GNNs\ncan be adapted to downstream tasks with universal information.\nGenerative learning frameworks for graphs can be classified into\ntwo categories based on how they acquire generative targets [128]:\nGraph Reconstruction and Property Prediction.\nGraph reconstruction aims to reconstruct specific parts of given\ngraphs, emphasizing fidelity in reproducing the original graph\nstructure or node attributes. For example, VGAE [94] extends\nthe VAE to the graph domain, where it first employs GCN as\nan encoder to generate node embeddings and then reconstructs\nthe adjacency matrix by the inner product of node embeddings.\nAlthough these methods enable models to capture the relationships\nbetween nodes and neighbors, they overlook the high-order struc-\ntural information and attribute semantic information in the graph.\nFurthermore, GPT-GNN [100] proposes the self-supervised edge\nand attribute generation tasks to push the model to understand the\ninherent dependencies of attribute and structure. As a result, the\nmodel can learn high-order structure and semantic information.\nGraphMAE [75] and GraphMAE2 [76] consider that previous\ngenerative methods overemphasize structure information, instead,\nthey employ the reconstruction of features and a re-mask decoding\nstrategy in a self-supervised manner. In the property prediction\ncategory, models focus on learning and predicting non-trivial prop-\nerties of provided graphs. For instance, GROVER [113] introduces\ntasks for nodes and edges, predicting context-aware properties\nwithin local subgraphs. The graph-level self-supervision task aims\nto predict motifs, framing it as a multi-label classification problem\nwith each motif as a label.\nAlthough generative methods are capable of generating novel\ncontent, the quality and interpretability of the content are hard to\nguarantee. In the future, it remains to be explored how to enhance\nthe accuracy and rationality of the generative methods.\nIn the realm of graph pre-trianing, we are pleased to see that\nrecent works have begun to explore pre-training on graph data\nfrom multiple domains and then applying the model to graph\ndata in different domains. For example, Davies et al. introduce\nFOTOM [109], a graph model pre-trained on various graph\ndomains using adversarial contrastive learning, demonstrating its\nsuperior performance across multiple downstream tasks compared\nto single-domain models. Inspired by ControlNet [129], Graph-\n11\nControl [110] introduces an deployment module to enhance the\nadaptability of pre-trained models on target datasets by integrating\ndownstream-specific information. These efforts represent a new\nstep forward for graph foundation models.\n4.3\nAdaptation\nTypically, the objectives of pre-training tasks are different from\nthe downstream ones, which hinders the transferability of pre-\ntraining models. To this end, fine-tuning is a common adaptation\napproach based on subtle adjustments of model parameters. In\naddition, the \u201cpre-train, prompt and predict\u201d paradigm has at-\ntracted considerable attention in recent years. By using prompts,\nthe format of downstream tasks is aligned with that of pre-training\ntasks, enabling pre-training models to handle downstream tasks in\na more effective manner.\n4.3.1\nFine-Tuning\nFor the situation where the model conducts the pre-training and\ndownstream tasks in the same domain, we can utilize a pre-training\nmodel to generate node embeddings or graph embeddings, and\nsubsequently fine-tune an external task-specific layer to generalize\nthe pre-training model to downstream tasks. DGI [93] and GRACE\n[73] utilize the pre-trained encoder to obtain node embeddings,\nand then fine-tune a logistic regression classifier with labeled data\nto handle the node classification task. Additionally, there is a\nmore practical scenario where pre-training is performed on the\nknown graphs while tested on unseen graphs. Pre-training models\ncannot encode unknown graphs appropriately, thus we need to\nfine-tune the model in this situation. GPT-GNN [100] employs\nthe labeled data to fine-tune a downstream task-specific decoder,\nwhich guides the pre-training model to adapt to downstream\ntasks. Moreover, some parameter-efficient fine-tuning methods\nhave been proposed recently. AdapterGNN [77] employs two\nparallel adapters before and after the message passing stage to\nmodify the input graph. Such addition-based methods only need\nto fine-tune the introduced parameters. G-Adapter [78] proposes\na parameter-efficient fine-tuning method for graph transformer,\nwhich introduces graph structure into the fine-tuning by message\npassing. G-TUNING [111] is a fine-tuning strategy for GNNs\nthat utilizes graphon reconstruction to preserve generative patterns\nand address structural divergence between pre-training and down-\nstream datasets\nAlthough the fine-tuning methods have achieved significant\nsuccess, they typically require sufficient labeled data to tune the\nmodel parameters. Moreover, conventional fine-tuning methods\nnecessitate repetitive fine-tuning for each task, incurring signif-\nicant computational costs. Therefore, more advanced fine-tuning\ntechniques for graph foundation models are still to be explored.\n4.3.2\nPrompt Tuning\nPrompt tuning has recently emerged as a strategy to circumvent\nthe need for full-parameter tuning, facilitating both multi-task\nadaptation and zero-shot learning [55, 130, 131]. This innovative\napproach has found significant applications in graph data, where\nrecent studies have concentrated on using prompt tuning to en-\nhance the performance and adaptability of GNN-based models.\nFollowing the framework proposed by Guo et al. [106], these\nmethods can be categorized into two distinct groups: pre-prompt\nmethods and post-prompt methods, based on whether the task-\nspecific prompts operate before or after the backbone module.\nPre-prompt methods modify the input graph\u2019s topology or\nnode features before message passing to aid downstream tasks or\nconstruct a prompt graph to enhance model adaptation. For exam-\nple, AAGOD [106] proposes to implement the data-centric manip-\nulation by superimposing an amplifier on the adjacency matrix of\nthe original input graph as learnable instance-specific prompts. All\nIn One [66] converts the node-level and graph-level tasks to graph-\nlevel tasks. It treats an extra subgraph as a prompt and merges\nit with the node subgraph. The model subsequently utilizes the\ncombined graph to generate predictions. GPF [107] introduces a\nuniform feature modification vector to each node in the graph,\nwhich can be optimized to adapt pre-training GNN models under\nany pre-training strategy. SGL-PT [108] incorporates a strong and\ngeneral pre-training task that harnesses the advantages of both\ngenerative and contrastive approaches. Additionally, it features\nverbalizer-free prompting function, thus aligning the downstream\ntask with the pre-training method\u2019s format. PRODIGY [70] is\na framework for pre-training an in-context learner over prompt\ngraphs. The goal is to enable a pretrained model to adapt to diverse\ndownstream tasks without optimizing any parameters. IGAP [97]\nbridges the gap between graph pre-training and inductive fine-\ntuning by addressing the graph signal and structure gaps using\nlearnable prompts in the spectral space.\nPost-prompt methods use task-specific prompts on represen-\ntations after message passing to aid downstream task adaptation.\nFor example, GPPT [79] employs a prompting function to generate\na token pair for each class, transforming all node classification\ntasks into link prediction tasks. GraphPrompt [65] unifies pre-\ntraining and downstream tasks into a consistent task format based\non subgraph similarity, and utilizes labeled data to learn a task-\nspecific prompt vector for each task, which modifies the model\u2019s\nReadout operation and narrows the gap between link prediction\nand downstream tasks. GraphPrompt+ [103] further enhances\nGraphPrompt by generalizing pre-training tasks and employing\nlayer-wise prompts to capture hierarchical knowledge across the\ngraph encoder, improving task-specific knowledge extraction for\nboth node and graph classification. Two exceptions are Multi-\nGPrompt [96] and HGPROMPT [98], which employ both pre-\nprompt and post-prompt.\nNotably, there are exceptions that incorporate both pre-prompt\nand post-prompt methods. For example, MultiGPrompt [96] aims\nto unify the pre-training and downstream tasks across both homo-\ngeneous and heterogeneous graphs using a dual-template design.\nHGPROMPT [98] presents a novel few-shot prompt learning\nframework that bridges the gap between homogeneous and hetero-\ngeneous graphs by using a dual-template design for pre-training\nand downstream tasks.\nAlthough these methods have improved the performance in\nfew-shot scenarios, further exploration is needed to understand\nthe semantics and interpretability of the graph prompts.\n4.4\nDiscussion\nGNN-based models offer several advantages, particularly their in-\ngenious inductive bias and compact parameter size. These models\nnaturally possess essential properties like permutation invariance,\nenabling them to handle graph-structured data effectively. Ad-\nditionally, GNN-based models offer the advantage of low-cost\ntraining and efficient resource usage, which reduces computational\nrequirements and makes them accessible for deployment even in\nresource-constrained environments. Moreover, they can generalize\n12\nwell from small amounts of labeled data. By propagating label in-\nformation through the graph, they can enhance prediction accuracy\neven when labeled data is sparse.\nDespite their many advantages, GNN-based models have\nseveral notable disadvantages. One primary limitation is their\nlack of explicit text modeling. They often underutilize the rich\nsemantic information embedded in textual attributes associated\nwith nodes or edges, leading to suboptimal exploitation of textual\ndata. Another significant drawback is the limited capacity of\nGNN-based models to incorporate and utilize general knowledge\neffectively. Unlike LLMs, which are pre-trained on vast corpora\nof text and can leverage extensive world knowledge, GNN-based\nmodels typically lack such pre-training on diverse and comprehen-\nsive datasets. This gap restricts their ability to generalize across\ndifferent domains and limits their performance in tasks requiring\nbroad contextual understanding or common-sense reasoning.\nOne promising direction is the integration of LLMs with\nGNN-based models. LLMs can provide a robust framework for\nincorporating extensive textual knowledge, enhancing the models\u2019\nability to understand and utilize semantic information embedded\nin text. In the following sections, we will explore graph learning\nmodels that incorporate LLMs.\n5\nLLM-BASED MODELS\nResearchers are actively exploring ways to leverage LLMs as\ncore and sole backbone for graph learning [71, 132, 133], for\nthe following advantages that can not be underestimated. Firstly,\ntransformer-based models show a remarkable ability to seamlessly\nintegrate textual information in graph data [132]. Additionally,\nemploying a LLM-liked backbone empowers models to unify\ndiverse graph learning tasks, as these tasks can be described\nusing natural language. Furthermore, recent advancements, such as\nNLGraph [71], GPT4Graph [133], showcase the LLMs\u2019 prowess\nin preliminary graph reasoning. These advantages mark a highly\npromising direction for the development of such models. To\ndiscover the potential of engaging LLMs into graph learning,\nthese works involve both graph-based properties and textual in-\nformation as input for the backbone networks. Following some\nsurveys [16, 134], our characterization of the backbone is not\nconfined solely to the narrow definition of LLMs (like GPT-3); it\nalso encompasses certain transformer-based models that leverage\ntextual information. We have summarized and categorized the\nworks mentioned in this section in Table 3.\n5.1\nBackbone Architectures\nA central question in employing LLMs for graph data is how to\nalign graph data with natural language so that LLMs can under-\nstand them. Given that LLMs initially accept tokens as their inputs\nand rely on self-attention layers to process input sequences for\nproducing hidden representations, it can be a difficult task to attain\na fine-grained modeling of graph structure information [132].\nAs illustrated in Figure 4, there are primarily two approaches\nthat have been developed to address this crucial question, namely\ngraph-to-token and graph-to-text.\n5.1.1\nGraph-to-token\nOne approach entails the tokenization of graph information and\nimitates the standardized input format of transformer-based mod-\nels. This methodology necessitates not only the serialization of\ngraph data into tokens but also the solutions for encoding the\ngraph\u2019s structural information. Since this method uses node repre-\nsentations as unique tokens for the input of backbone models, the\nbackbone need to be either trainable transformers or open source\nLLMs. For instance, InstructGLM [132] uses LLaMA [53] and T5\n[92] to be their backbones for further tuning.\nThe concept of graph-to-token initially surfaces in GIMLET\n[135] that treats node representations as tokens and aims to inte-\ngrate graph data with textual data. Specifically, GIMLET expands\nthe capabilities of LLMs to accommodate both graph and text data\nby using the transformer architecture, incorporating generalized\nposition embedding and instruction-based pre-training. Further-\nmore, efforts have been made to integrate graph data with other\nmodalities beyond just text data. For instance, Meta-Transformer\n[144] introduces a transformer-based architecture designed to\nincorporate various forms of multimodal data, including graphs,\ntext, and images. However, despite the promising trend indicated\nby developing unified multimodal intelligence using a transformer\nbackbone, their approaches cannot be considered as graph foun-\ndation models because they do not involve any pre-training and\nadaptation learning paradigm.\nInstructGLM [132] on the other hand, adopts a pre-training and\nadaptation framework and introduces LLMs to further enhance the\nmodel\u2019s text processing capabilities, making it a strong contender\nfor the position of a graph foundation model. In this framework,\nthe vocabulary of the LLMs is expanded by incorporating the\ninherent node feature vectors from the graph as distinct and unique\ntokens for each node. Leveraging LLMs and the transferability of\nnatural language, InstructGLM makes a valuable contribution to\nthe ongoing movement towards graph foundation model architec-\ntures and pipelines that span multiple modalities.\nThese efforts tokenize graph data to align it with natural\nlanguage, enabling joint understanding with data from other\nmodalities. Their conclusions showcase promising results for\nintegrating graph data with natural language. However, despite\nthese promising results, how to inform LLMs of underlying graph\nstructures remains an important challenge in this approach.\n5.1.2\nGraph-to-text\nTo align graph data with natural language, another approach in-\nvolves describing graph information using natural language. Sev-\neral approaches have been developed along this line of thoughts,\nutilizing prompts to integrate the capabilities of large language\nmodels into classical tasks on graphs. For this method, which\nexclusively relies on natural language prompts, the associated\nbackbone model can be any LLM, even if it is not open-sourced.\nFor instance, LLMtoGraph [136] uses several LLMs including\nGPT-4 [145] and Vicuna [146], Graph-LLM [134] utilizes mul-\ntiple language models of different sizes, including BERT [47],\nDeBERTa [147], Sentence-BERT [148], GPT-4 [145] and LLaMA\n[53].\nInitial attempts mainly use edge list to describe graph struc-\ntures in natural language and make assessment on various graph\ntasks. LLMtoGraph [136] employs node and edge lists as the\ninput format for graph data, conducting a series of experiments\nto evaluate the capacity of several LLMs to process graph data.\nThough their work reveals that LLMs are capable of delivering\nhigh-quality responses to problems pertaining to graph data, the\nprompt engineering in this stage such as zero-shot chain-of-\nthought or few-shot prompting may not yield the anticipated\nbenefits. NLGraph [71] also conducts a comprehensive assessment\nof LLMs across eight graph reasoning tasks as well as popular\n13\nToken\nLLM\nPredictions\nThe title of Paper_4 is: Can \u2026  The title of \nPaper_1 is: Exploring \u2026  Paper_1 cites Paper_4 \u2026\nQuestion: The category of Paper_4 is \u2026\nCategorize the central node: (<node_4>, \nTitle_4> ) is connected to (<node_1>, Title_1), \n(<node_3>, Title_3) within one hop. Which \ncategory should (<node_4>, Title_4) belong to?\nToken\nLLM\nPredictions\n1\n2\n4\n3\n5\n1\n2\n4\n3\n5\n(a) Graph-to-token.\nToken\nLLM\nPredictions\nThe title of Paper_4 is: Can \u2026  The title of \nPaper_1 is: Exploring \u2026  Paper_1 cites Paper_4 \u2026\nQuestion: The category of Paper_4 is \u2026\nCategorize the central node: (<node_4>, \nTitle_4> ) is connected to (<node_1>, Title_1), \n(<node_3>, Title_3) within one hop. Which \ncategory should (<node_4>, Title_4) belong to?\nToken\n1\n2\n4\n3\n5\n1\n2\n4\n3\n5\n(b) Graph-to-text.\nFig. 4: An illustration of two existing approaches to align graph data with natural language. One approach tokenizes graph data and use node\nrepresentations (depicted as red tokens in the figure) as well as text tokens (depicted as green tokens in the figure) to be the input of LLMs.\nAnother approach represents graph data with prompts in natural language and uses text tokens only (depicted as green tokens in the figure) as\nthe input of LLMs.\nTABLE 3: Details of approaches involved as LLM-based models.\nModel\nBackbone Architecture\nPre-training\nAdaptation\nGIMLET [135]\nGraph-to-token\n+\nTransformer\n-\n-\nInstructGLM[132]\nGraph-to-token\n+\nFlan-T5/LLaMA\nMLM,LM\nManual Prompt Tuning\nLLMtoGraph[136]\nGraph-to-text\n+\nGPTs, Vicuna\nLM\nManual Prompt Tuning\nNLGraph[71]\nGraph-to-text\n+\nGPTs\nLM\nManual Prompt Tuning\nGraphText[137]\nGraph-to-text\n+\nGPTs\nLM\nManual Prompt Tuning\nLLM4Mol[138]\nGraph-to-text\n+\nGPTs\nLM\nManual Prompt Tuning\nTextForGraph [139]\nGraph-to-text\n+\nGPTs\nLM\nManual Prompt Tuning\nWhen&Why [140]\nGraph-to-text\n+\nGPTs\nLM\nMaunal Prompt Tuning\nGraphWiz [141]\nGraph-to-text\n+\nLLaMA, Mistral\nLM\nMaunal Prompt Tuning\nCGForLLM [142]\nGraph-to-text\n+\nGPT4\nLM\nMaunal Prompt Tuning\nLLM4DYG [143]\nGraph-to-text\n+\nLLaMA, Vicuna, GPT-3.5\nLM\nManual Prompt Tuning\nGPT4Graph[133]\nGraph-to-text\n+\nGPT-3\nLM\nManual Prompt Tuning + Automatic Prompt Tuning\nGraph-LLM[134]\nGraph-to-text\n+\nBERT, DeBERTa, Sentence-BERT,\nGPTs, LLaMA\nMLM,LM\nManual Prompt Tuning + Automatic Prompt Tuning\nGNN tasks in natural language. Similarly, utilizing edge lists to\ndescribe graph structure, the results once again underscores the\nlimitations of this approach when dealing with complex graph\nproblems. GraphText [137] introduces a novel approach called\n\u201cGraph-syntax Tree\u201d to describe graph data using natural lan-\nguage. This approach enables graph reasoning within a text-based\ncontext. One of the notable advantages of this method is that it\nallows models to readily incorporate the inductive bias of GNNs\nby the graph-syntax tree. TextForGraph [139] designed two types\nof prompts, full text and reduced text, to describe information on\nthe graph, effectively compressing the prompt length. When&Why\n[140] designs several styles of prompts and offers key insights into\nthe use of LLMs for processing structured data. GraphWiz [141]\ndesigns different prompts for various tasks on graphs, including\ncycle detection, subgraph matching, and more.\nMoreover, GPT4Graph [133] introduces a novel approach to\nprompt engineering that combines manually crafted prompts with\nprompts generated by the language model itself, referred to as\nhandcrafted prompts and automatic prompts. Specifically, for man-\nual prompting, it utilizes description languages such as edge lists,\nadjacency lists, Graph Modeling Language (GML), and Graph\nMarkup Language (GraphML) to represent graph structures and\ncompare their effectiveness. For automatic prompting, it employs\ntechniques like graph summarization, neighborhood summariza-\ntion, graph exploration, and graph completion to actively engage\nLLMs in understanding and manipulating graphs, facilitating\ngraph-based reasoning and learning. The findings indicate that\nself-prompting is a more effective method for informing LLMs\nabout graph structures. Graph-LLM [134] further supports this\nconclusion, emphasizing that neighbor summarization is the most\neffective technique in existing prompt engineering methods.\nFurthermore, efforts have been undertaken to explore the\npotential of LLMs in addressing tasks from other domains using\ntextual prompts. For instance, LLM4Mol [138] uses SMILES\n(Simplified Molecular Input Line Entry System) to directly de-\nscribe the property of molecule, and explores how LLMs can\ncontribute to molecular property prediction.\nThese studies highlight significant potential for using natural\nlanguage to describe graph data and using textual tokens as the\ninput of LLMs for graph learning. Nevertheless, a key takeaway\nfrom their conclusions is that, at the present moment, the way we\nuse these prompts may not be an effective approach for mining\nunderlying graph structures.\n5.2\nPre-Training\nThe methods discussed in this section solely employ LLMs as\nthe backbone. Hence, the pre-training phase for these methods\ncorresponds to the pre-training phase of LLMs. There are mainly\ntwo tasks used in LLM-based models for graph learning, we will\nprovide a concise overview of these two pre-training tasks.\n5.2.1\nLanguage Modeling (LM)\nLanguage Modeling (LM) is one of the most common self-\nsupervised task in NLP, and is widely adopted by many state-of-\nthe-art LLMs, such as LLaMA [53] and GPT-3 [61]. LM task can\nbe essentially addressed to the challenge of estimating probability\ndistributions of the next word. While LM represents a broad\n14\nconcept, it frequently pertains specifically to auto-regressive LM\nor unidirectional LM in practical applications [54]. Many methods\ninvolve LM as their pre-training method, namely InstructGLM\n[132], LLMtoGraph [136], NLGraph [71], GPT4Graph [133],\nGraph-LLM [134], GraphText [137], LLM4Mol [138], TextFor-\nGraph [139], When&Why [140], GraphWiz [141], CGForLLM\n[142] and LLM4DYG [143].\nIn the context of a text sequence represented as s1:L =\n[s1, s2, \u00b7 \u00b7 \u00b7 , sL], its overall joint probability, denoted as p (s1:L),\ncan be expressed as a product of conditional probabilities, as\nshown in equation:\np (s1:L) =\nL\nY\nl=1\np (sl|s0:l\u22121) .\n(2)\nHere, s0 represents a distinct token signifying the commencement\nof the sequence. The conditional probability p (sl|s0:l\u22121) is essen-\ntially a probability distribution over the vocabulary based on the\nlinguistic context s0:l\u22121. To model the context s0:l\u22121, a neural\nencoder fnenc(\u00b7) is employed, and the conditional probability is\ncalculated as follows:\np (sl|s0:l\u22121) = flm(fnenc(s0:l\u22121)).\n(3)\nIn this equation, flm represents the prediction layer. By training the\nnetwork using maximum likelihood estimation (MLE) with a large\ncorpus, we can effectively learn these probabilities. Nevertheless,\na drawback of unidirectional language models is their encoding\nof contextual information for each token, which is solely based\non the preceding leftward context tokens and the token itself.\nHowever, for more robust contextual representations of text, it is\npreferable to capture contextual information from both the forward\nand backward directions.\n5.2.2\nMasked Language Modeling (MLM)\nMasked language modeling (MLM) is introduced to address the\nlimitation of the traditional unidirectional language model, fre-\nquently denoted as a Cloze task [54]. In MLM, specific tokens\nwithin the input sentences are randomly masked, and the model is\nthen tasked with predicting these masked tokens by analyzing the\ncontextual information in the surrounding text. As an effective pre-\ntraining task, MLM is adopted in BERT [47] and T5 [92]. Addi-\ntionally, MLM can be categorized into two subtypes: Sequence-to-\nSequence MLM (Seq2Seq MLM) and Enhanced MLM (E-MLM).\nSeq2Seq MLM involves utilizing the masked sequences as input\nfor a neural encoder, and the resulting output embeddings are used\nto predict the masked token through a softmax classifier. On the\nother hand, E-MLM extends the mask prediction task to various\ntypes of language modeling tasks or enhances MLM by integrating\nexternal knowledge. MLM also faces some drawbacks as this pre-\ntraining method may result in a disconnection between the pre-\ntraining and fine-tuning stages since the mask token is absent\nduring fine-tuning. InstructGLM [132] and Graph-LLM [134] use\nT5/BERT as backbones, and adopt MLM pre-training strategy.\nThere are also many pre-training tasks like Permuted Lan-\nguage Modeling (PLM) [149], Denoising Autoencoder (DAE)\n[150], Text-Text Contrastive Learning (TTCL), [151] and others.\nThese pre-training tasks are currently not adopted by existing\nLLM-based models in graph learning, and thus not within the\nscope of our discussion in this section. However, we believe that\nin the future, more research will be developed on graph tasks\ninvolving these pre-training tasks, offering additional possibilities\nfor the establishment and refinement of graph foundation models.\n5.3\nAdaptation\nThe adaptation phase plays a pivotal role in enhancing the per-\nformance of LLM-based models in graph learning. LLMs are\nprimarily trained on textual corpora, which results in a significant\ngap between the pre-training phase and their deployment on\ngraph tasks. Both the graph-to-token and graph-to-text methods\nare accompanied by specific adaptation techniques designed to\nenhance the LLM\u2019s ability to understand graph data effectively.\nAs these methods share a fundamentally similar training procedure\nthat utilizes prompts, we classify these adaptation strategies in the\naspect of prompt engineering: manual and automatic.\n5.3.1\nManual Prompting\nMethods mentioned here use manually created prefix style\nprompts. For instance, LLMtoGrpah [136] and NLGraph [71]\nemploy node and edge lists incorporating other graph properties\ndescribed in natural language to form a comprehensive prompt.\nGPT4Graph [133] goes a step further by utilizing additional\ndescription languages to represent graph data, such as edge list,\nadjacency list, GML and GraphML, providing a more extensible\nframework for manual graph prompts. GraphText [137] translates\ngraph features and relationships into natural language and assem-\nbles them to form a Graph-syntax Tree. This Graph-syntax Tree\nserves as a natural language graph prompt, enabling LLMs to\ncomprehend graph data and engage in interpretable and interactive\ngraph reasoning. In the field of molecules, LLM4Mol [138]\nuses SMILES (Simplified Molecular Input Line Entry System)\nto directly describe the property of molecular, which is a widely\nadopted molecular describing language. Furthermore, Instruct-\nGLM [132] employs a technique called instruction prompting.\nThis approach involves the design of a set of graph descriptions\ncentered around a central node, coupled with task-specific descrip-\ntions. Graph-LLM [134], TextForGraph [139], When&Why [140],\nGraphWiz [141], CGForLLM [142] and LLM4DYG [143] also\nuses natural language instructions and subsequently conducts a\nseries of comprehensive experiments.\n5.3.2\nAutomatic Prompting\nCreating prompts manually is a time-consuming task, and these\nprompts can sometimes be sub-optimal [60]. To address these\ndrawbacks, automatically generated prompts have been introduced\nfor further adaptation. GPT4Graph [133] firstly tries to employ\nthree different types of prompts generated by LLM itself, namely\ngraph summarization, graph exploration and graph completion,\nin graph tasks. Specifically, graph summarization generates a\nsummary of the given graph by extracting key features or a\nsummary of the neighborhood information of target nodes. Graph\nexploration means generating a sequence of queries or actions\nto retrieve information from the given graph. Graph completion\ngenerates partial graphs and prompt itself to complete the missing\nparts. By leveraging these self-prompting strategies, LLMs can\nactively engage in the understanding and manipulation of graphs,\nfacilitating graph-based reasoning and learning. Graph-LLM [134]\nuses automatic prompts as well in the form of neighbor summary,\nand their experimental results once again emphasize the efficiency\nof automatic prompting.\nAdditionally, there are various adaptation approaches based\non fine-tuning, including Vanilla Fine-Tuning [47], Intermediate\nFine-Tuning (IFT) [152], Multi-task Fine-Tuning (MTFT) [153],\nand Parameter Efficient Fine-Tuning [154]. These methods offer\nefficient ways to adapt pre-trained models to downstream tasks,\n15\nalthough they have not been applied to graph tasks at this time.\nHowever, we anticipate that future research will explore the\nintegration of these adaptation approaches into graph tasks, further\nadvancing the development of the graph foundation model.\n5.4\nDiscussion\nEfforts of aligning graph data with natural language and using\nsole LLMs as graph learners has paved the way for exciting\ndevelopments. The integration of graph data, text, and other\nmodalities into transformer-based models presents a promising\nway, with the potential to connect techniques from the GNN field\nwith advancements in the LLM domain. Moreover, leveraging\nLLMs allows for the unification of various graph tasks, as these\ntasks can all be described in natural language. This makes LLM-\nbased backbones a more competitive selection for building graph\nfoundation models.\nNonetheless, it is essential to acknowledge that the current\nways of utilizing LLMs as backbones for graph learning also\npresents inherent limitations. These limitations encompass the\ninability of LLMs to effectively process the lengthy textual in-\nformation required to describe graph structures, their incapacity\nto engage in multi-hop logical reasoning through graph links, the\nchallenge they face in capturing the topological structures preva-\nlent in highly connected graphs, and their struggle in handling the\ndynamic nature of graphs that evolve over time. These shortcom-\nings underscore the need for further research and innovation in\nusing LLM-based models for graph learning.\nFuture research directions for LLM-based approaches include\nenhancing the ability of Large Language Models to more effec-\ntively and efficiently understand critical information in graphs, in-\ncluding node features and topological structures. Considering that\nLLMs cannot directly comprehend graphs, and flattened natural\nlanguage description of graphs are likely to result in information\nloss, efficient and structured modeling techniques for graphs need\nto be developed. These methods are expected to help bridge the\ngap between natural language prompts and the comprehensive\ninformation present in graph data.\n6\nGNN+LLM-BASED MODELS\nGNN-based models lack the ability to process text and thus cannot\ndirectly make predictions based on textual data. Additionally, they\ncannot make predictions based on natural language instructions\nprovided by users. Consequently, exploring the performance of\nmodels with a substantial parameter count in graph-related tasks\nis imperative. On the other hand, LLM-based models for graph\nlearning have their inherent limitations. These limitations include\nthe incapability of LLMs to process precise mathematical calcula-\ntions and the inability to handle multi-hop logical reasoning, etc.\nThese shortcomings underline the necessity for further research\nand innovation in this domain. To overcome these limitations and\nharness the strengths of both language understanding from LLMs\nand structural analysis from GNNs, integrating LLMs and GNNs\ncan potentially lead to a more comprehensive and powerful model.\nWe summarize and categorize the works mentioned in this section\nin Table 4.\n6.1\nBackbone Architectures\nTo simultaneously utilize information from both the graph and text\nand accomplish a variety of tasks, we need to design a framework\nthat effectively integrates LLM and GNN. Depending on the\nprediction model, GNN+LLM-based methods can be classified as:\n1) GNN-centric Methods, 2) Symmetric Methods, and 3) LLM-\ncentric Methods, as illustrated in Figure 5.\n6.1.1\nGNN-centric Methods\nSeveral works aim to utilize LLM to extract node features from\nraw data and make predictions using GNN. These approaches\nare denoted as GNN-centric models. For example, GraD [155]\nperforms a parameter-efficient fine-tuning of an LLM on the\ntextual dataset of a TAG (text-attributed graph). The textual dataset\nT is annotated with task-specific labels Y, where G = (V, E, T)\nand T is the set of texts with each element aligned with a node in\nV . Then the downstream task loss for fine-tuning is:\nLossCLS = L\u03b8(\u03d5(LLM(T)), Y),\nLossLINK = L\u03b8 (\u03d5 (LLM (Tsrc) , LLM (Tdst )) , Y) ,\n(4)\nwhere \u03d5(\u00b7) is the classifier for the classification task or similarity\nfunction for the link prediction task, Tsrc and Tdst are the texts\nof the source node and the target node, respectively, LossCLS and\nLossLINK are the loss of classification and link prediction task,\nrespectively. Thus we can get the node representations X with\nfine-tuned LLM, achieved by removing the head layer. Then we\ncan train GNN with the loss:\nLossCLS = L\u03b8(\u03d5(GNN(LLM(T))), Y),\nLossLINK = L\u03b8\n\u0000\u03d5\n\u0000GNN (LLM(Tsrc )) , GNN\n\u0000LLM(Tdst )\n\u0001\u0001\n, Y\n\u0001\n.\n(5)\nFor LLMs that do not provide direct access to their embed-\ndings such as ChatGPT, TAPE [91] engages these LLMs through\ntext interactions. Specifically, TAPE first utilizes an LLM to gen-\nerate a ranked prediction list and explanation based on the original\ntext, and then an LM is utilized and fine-tuned to transform the\noriginal text and additional features of predictions and explanation\ngenerated by LLM into node features. Subsequently, downstream\nGNNs can utilize the features for prediction tasks. TAPE extracts\ngraph-agnostic features and cannot capture correlations between\ngraph topology and raw features. To this end, GIANT\n[156]\nutilizes a graph-structure aware self-supervised learning method to\nfinetune the LM. Consequently, the text representations encompass\ngraph-related information. In GALM\n[157], the focus is on\nexploring pre-training approaches for models that combine text\nand graph data, particularly on extensive heterogeneous graphs en-\nriched with rich textual data. OFA [158] introduces text-attributed\ngraphs that use natural language to describe nodes and edges,\nunified by language models into a common embedding space.\nHeterformer [159] integrates contextualized text encoding and\nheterogeneous structure encoding within a single model. It incor-\nporates heterogeneous structure information into each Transformer\nlayer as it encodes node texts. Edgeformers [160], which are\nbased on graph-enhanced Transformers, perform edge and node\nrepresentation learning by contextually modeling texts associated\nwith edges. LLMRec [161] improves recommender systems by\nusing three straightforward yet powerful LLM-based graph aug-\nmentation techniques, addressing the issues of sparse implicit\nfeedback and low-quality side information commonly found in\nrecommendation systems. WalkLM [162] conducts attributed ran-\ndom walks on the graph and uses an automated program to\ngenerate approximately meaningful textual sequences from these\n16\nwalks. It then fine-tunes a language model (LM) with these textual\nsequences and extracts embedding vectors from the LM, capturing\nboth attribute semantics and graph structures. TOUCHUP-G [163]\nenhances the node features derived from a pre-trained model for\ndownstream graph tasks and introduces. Multiplex graph neural\nnetworks initialize node attributes as feature vectors for node\nrepresentation learning, but they fall short in capturing the full se-\nmantics of the nodes\u2019 associated texts. METERN [164] addresses\nthis by using a single text encoder to model the shared knowledge\nacross relations and employing a small number of parameters\nper relation to generate relation-specific representations. Another\nresearch [165] investigates the use of LLMs to improve graph\ntopological structures, a relatively unexplored area. A label-free\npipeline, LLM-GNN [166], uses LLMs for annotation and supplies\ntraining signals to GNNs for subsequent prediction.\n6.1.2\nSymmetric Methods\nAlso, there are some works that align the embeddings of GNN and\nLLM to make better predictions or utilize the embeddings for other\ndownstream tasks, denoted as symmetric methods. Most GNN-\ncentric based methods involve two sequential steps: text encoding\nand graph aggregation. It is important to note that during the\ngeneration of text embeddings, there is no exchange of information\nbetween nodes. To consider the interrelated nature of connected\nnodes, several works try to utilize GNN and LLM together to\nget structure-aware text features. GraphFormer [167] fuses text\nembedding and graph aggregation as an iterative workflow. During\neach iteration, the interconnected nodes will engage in information\nexchange within the layerwise GNN component, formulated as\n\u02c6zl = GNN(zl), where zl is the output of l-th layer of GNN.\nAs a result, each node will incorporate information from its\nneighboring nodes. The Transformer component then operates\non these enhanced node features, enabling the generation of\nprogressively more informative node representations as zl+1 =\nTRM(CONCAT(\u02c6zl, hl)), where TRM is the transformer, and\nhl is the output of l-th layer of transformer. However, this method\nsuffers from scalability issues because the memory complexity\nis proportional to the graph size as neighborhood texts are also\nencoded. GLEM [168] employs a variational EM framework to\nalternatively update the LLMs and GNNs, thus can essentially\ncapture node label distribution conditioned on the local text\nattributes. In contrast, GNN uses the text and label information\nof neighboring nodes to predict labels, thus characterizing global\nconditional label distribution. By doing so, GLEM efficiently\nincorporates local textual data and global structural information\ninto its components and can ease the scalability issue.\nOther studies employ distinct encoders for graph nodes and\ntexts, training them to align their representations within a shared\nlatent space. G2P2\n[169] jointly pre-trains a graph-text model\nutilizing three graph interaction-based contrastive strategies, and\nthen explores prompting for the downstream tasks. [170] utilizes\nGNN to model the structural information of nodes, which is then\nintegrated with the corresponding text fragment encoded by a\nlanguage model. The model subsequently predicts the masked\ntoken. ENGINE [171] integrates large language models and graph\nneural networks using an adjustable side structure. This approach\nsignificantly reduces training complexity while maintaining the\ncapacity of the combined model. To address this, PATTON [172]\nincorporates two pretraining strategies: network-contextualized\nmasked language modeling and masked node prediction, aiming\nto capture the inherent relationship between textual attributes\nand network structure. Some other works\n[173, 174, 175] also\nutilize GNN and LLM to learn representations for molecules.\nThese models employ a contrastive learning strategy to effectively\npre-train on a dataset containing pairs of molecular graphs and\ncorresponding textual descriptions. By simultaneously learning\nthe chemical structures of molecules and their associated text\nthrough this approach, these models can then be applied to\nvarious downstream tasks. Furthermore, MolCA [176] allows a\nlanguage model (LM) to comprehend both text-based and graph-\nbased molecular information through the use of a cross-modal\nprojector. GIT-Mol [177] encompasses all three modalities in\nmolecular science\u2014graph, image, and text\u2014supporting tasks such\nas molecule generation, molecule captioning, molecular image\nrecognition, and molecular property prediction.\n6.1.3\nLLM-centric Methods\nWhile LLMs have shown impressive performance in various\nnatural language tasks, they struggle with precise mathematical\ncalculations, multi-step logic reasoning, spatial and topological\nperception, and handling temporal progression. Hence some works\nutilize GNNs to enhance the performance of LLM, denoted\nas LLM-centric methods. For example, GraphTranslator [178]\nutilizes a Graph Model to efficiently manage predefined tasks\nand takes advantage of the extended interface of Large Language\nModels to support a variety of open-ended tasks for the Graph\nModel. GraphGPT [179] integrates large language models with\ngraph structural knowledge through graph instruction tuning, en-\nabling LLMs to understand complex graph structures and improv-\ning adaptability across various datasets and tasks. THLM [180]\nintroduces a novel pretraining framework for language models\nthat explicitly incorporates the topological and heterogeneous in-\nformation found in Text-Attributed Heterogeneous Graphs. Graph-\nPrompter [181] aligns graph information with LLMs through the\nuse of soft prompts. InstructGraph [182] enhances large language\nmodels (LLMs) with graph reasoning and generation capabilities\nthrough instruction tuning and preference alignment. RELM [183]\nutilizes the chemical knowledge embedded in LMs to support\nGNNs, thereby improving the accuracy of real-world chemical\nreaction predictions.\n6.2\nPre-training\nTo train the model and enable it to handle both graph and text\ninformation, we need to train the model on a large amount of\ndata. LLM and GNN can be pre-trained on textual data and graph\ndata respectively. Furthermore, the GNN+LLM-based methods\ncan further be pre-trained on data composed of graph and text.\nIn this subsection, we category the pre-training strategies as GNN\nor LLM-based, and alignment-based.\n6.2.1\nGNN or LLM-based\nOther frameworks leverage pre-trained LLMs to obtain text em-\nbeddings. The majority of existing models, such as GIANT [156],\nGraD [155], GraphFormer [167], GLEM [168], Text2Mol [173],\nMolCA [176], MoleculeSTM [174], and CLAMP [175] employ\nMasked Language Modeling (MLM) during pre-training. Some\nmodels, like TAPE and Graph-ToolFormer, opt for Language\nModeling (LM) in the pre-training phase. Additionally, SimTeG\nintegrates Text-Text Contrastive Learning (TTCL), a technique\nthat leverages certain observed text pairs exhibiting more semantic\nsimilarity than randomly selected pairs during the pre-training\nphase as:\n17\nLLM\nGNN\nPredictions\nLLM\nGNN\nPredictions\nAlignment\nPredictions\nAdaptation\nLLM\nPredictions\nInstruction\nGNN\n(a) GNN-centric methods.\nLLM\nGNN\nPredictions\nLLM\nGNN\nPredictions\nAlignment\nPredictions\nAdaptation\nLLM\nPredictions\nInstruction\nGNN\n(b) Symmetric methods, where the aligned embeddings\ncan be further utilized for downstream tasks.\nLLM\nGNN\nLLM\nGNN\nP\nAlignment\nP\nA\nLLM\nPredictions\nInstruction\nGNN\n(c) LLM-centric methods, which take an in-\nstruction as input and output an answer.\nFig. 5: An illustration of GNN+LLM-based models.\nTABLE 4: Details of approaches involved as GNN+LLM-based models.\nModel\nBackbone Architecture\nPre-training\nAdaptation\nTAPE [91]\nGNN-centric\nLM\nTuning-free Prompting + Parameter-Efficient FT\nGIANT [156]\nGNN-centric\nMLM\nVanilla FT\nGraD [155]\nGNN-centric\nMLM\nParameter-Efficient FT\nGALM [157]\nGNN-centric\nGraph Reconstruction\nVanilla FT\nOFA [158]\nGNN-centric\nMLM\nTuning-free Prompting\nHeterformer [159]\nGNN-centric\nLM\nVanilla FT\nedgeformers [160]\nGNN-centric\nLM\nVanilla FT\nLLMRec [161]\nGNN-centric\nLM\nTuning-free Prompting + Parameter-Efficient FT\nWalkLM [162]\nGNN-centric\nMLM\nVanilla FT\nMETERN [164]\nGNN-centric\nMLM\nParameter-Efficient FT\nLLM-GNN [166]\nGNN-centric\nLM\nParameter-Efficient FT\nGLEM [168]\nSymmetric\nMLM\nVanilla FT\nGraphFormer [167]\nSymmetric\nMLM\nVanilla FT\nG2P2 [169]\nSymmetric\nGTCL\nPrompt Tuning\nText2Mol [173]\nSymmetric\nMLM + GTCL\nParameter-Efficient FT\nMoleculeSTM [174]\nSymmetric\nMLM + GTCL\nParameter-Efficient FT\nMolCA [176]\nSymmetric\nLM\nParameter-Efficient FT\nCLAMP [175]\nSymmetric\nMLM + GTCL\nParameter-Efficient FT\nGIT-Mol[177]\nSymmetric\nLM\nPrompt Tuning\nPATTON [172]\nSymmetric\nMLM\nParameter-Efficient FT\nENGINE [171]\nSymmetric\nLM\nParameter-Efficient FT\nGraphTranslator [178]\nLLM-centric\nLM\nParameter-Efficient FT\nTHLM [180]\nLLM-centric\nMLM\nParameter-Efficient FT\nGraphGPT [179]\nLLM-centric\nMLM\nPrompt Tuning\nInstructGraph[182]\nLLM-centric\nLM\nPrompt Tuning\nRELM [183]\nLLM-centric\nLM\nTuning-Free Prompting\nGraphPrompter [181]\nLLM-centric\nLM\nParameter-Efficient FT\nLossTTCL = Ex,y+,y\u2212\n\u0014\n\u2212log\nexp (k (x, y+))\nexp (k (x, y+)) + exp (k (x, y\u2212))\n\u0015\n,\n(6)\nwhere E is the expectation, k is the score function, y+ is the pos-\nitive sample and y\u2212is the negative sample. Additionally, GALM\n[157] utilizes graph reconstruction for pre-training on extensive\ngraph datasets, and thus can incorporate the graph information\ninto the pre-trained LLMs.\n6.2.2\nAlignment-based\nSymmetric methods of LLM and GNN like Text2Mol [173],\nMoleculeSTM [174], and CLAMP [175] are pre-trained on large\ndatasets with Graph-Text Contrastive Learning (GTCL), which\naligns the embeddings of the graph encoder and the text encoder.\nThe embeddings involve rich information about graph structure\nand text, thus demonstrating appealing performance on down-\nstream datasets. For a molecule example, CLAMP minimizes the\ncontrastive loss as below:\nLossNCE = \u22121\nN\nN\nX\ni=1\nyi log(k(LLM(\u00b5i), GNN(\u03bei)))\n+ (1 \u2212yi) log(1 \u2212k(LLM(\u00b5i), GNN(\u03bei))),\n(7)\nwhere \u00b5i is the text representation, \u03bei is the graph representation,\nand k is a score function to predict the activity of a molecule. The\ncontrastive loss promotes the active molecules on a bioassay to\nhave similar embeddings to the embedding of a specific bioassay,\nwhile ensuring that inactive molecules have dissimilar embeddings\nto it.\n6.3\nAdaptation\nThe\nadaptation\nphase\nplays\na\npivotal\nrole\nin\noptimizing\nGNN+LLM-based models for efficient graph learning. Apart from\nsome works [173, 174, 175] which test the model\u2019s performance\n18\non zero-shot tasks such as zero-shot structure-text retrieval and\nzero-shot text-based molecule editing, models in most cases need\nadaptation. In this subsection, we categorize these adaptation\nstrategies into two main types: fine-tuning and prompt-tuning.\n6.3.1\nFine-tuning\nTo adapt to the downstream tasks, some works [156, 157, 159, 160,\n162, 167, 167, 168] utilize vanilla fine-tuning methods for node\nclassification tasks. However, vanilla fine-tuning methods involve\nadjusting a broad range of model parameters, which can be com-\nputationally intensive and resource-demanding. So other works\nutilize parameter-efficient fine-tuning methods for downstream\ntasks, resulting in a more efficient and resource-friendly approach.\nSpecifically, several studies [173, 174, 175] align the embedding\nspace of GNN and LLM utilizing paired molecule graph-text data,\nwhile other research [155, 161, 164, 166, 172, 180] are tuned on\nTAGs with classification task. Additionally, some works works\n[176, 178, 181] adapt to downstream tasks by generating text\ncaptions or descriptions.\n6.3.2\nPrompt-Tuning\nThe prompt-tuning approach is employed in certain studies\n[169, 170, 177, 179, 182]. For example, G2P2 [169] leverages\nprompt-tuning to automatically optimize prompts with limited\nlabeled data for efficient adaptation to downstream tasks. Other\nstudies [91, 158, 161, 183] exclusively focus on utilizing Tuning-\nFree Prompting to generate text. These approaches leverage the\ninherent capabilities of language models without any additional\nfine-tuning or parameter adjustments, thereby relying solely on\nthe pre-trained knowledge embedded within the models to produce\ntext outputs. For example, in TAPE [91], the initial text features\nare incorporated into a specialized prompt to interrogate a lan-\nguage model, generating a ranked list of predictions along with\nexplanations. Subsequently, the expanded text features are utilized\nfor finetuning on an LLM.\n6.4\nDiscussion\nTo summarize, LLMs excel in capturing complex linguistic pat-\nterns and semantics from textual data, allowing the GNN+LLM-\nbased models to generate embeddings that involve rich text,\nstructure information, and even external knowledge of LLMs,\nthus leading to better model performance. Also, when integrated\nwith GNN, LLM\u2019s reasoning capabilities over graphs may be\nenhanced. At the same time, these models can also be regarded\nas multimodal models to accomplish cross-modal tasks, such as\ntext-graph retrieval tasks. The embeddings can be then utilized for\na bunch of downstream tasks.\nAlso, it is challenging to align LLMs and GNNs into a com-\nmon representational space. To tackle this problem, it is essential\nto establish a robust standard for measuring the alignment between\nLLM and GNN representations. This standard should evaluate the\ndegree to which the embeddings from both models capture similar\nsemantic and structural information. Additionally, it is crucial\nto design effective methodologies for achieving this alignment.\nBy doing so, we can ensure that the combined model leverages\nthe strengths of both LLMs and GNNs, ultimately enhancing\nperformance on various downstream tasks.\n7\nCHALLENGES AND FUTURE DIRECTIONS\nAlthough the previous sections have discussed the concepts and\na lot of related works towards graph foundation models, there are\nstill many avenues for future exploration in this research area. This\nsection will delve into these issues.\n7.1\nChallenges about Data and Evaluation\n7.1.1\nData Quantity and Quality\nThe improvements in data quantity and data quality are the key\nfactors contributing to the effectiveness of foundation models [1].\nAt present, there is still a limited amount of open-source large-\nscale graph data [184, 185], and each dataset is primarily concen-\ntrated in a single domain. This poses a challenge to learn graph\nfoundation models for diverse data domains. Hence, it is necessary\nto collect and organize a unified, massive dataset that covers graph\ndata and related corpora across different domains. Additionally, if\nthe graph data are noisy, incomplete, or not properly curated, it\nwill negatively affect the performance of graph foundation models.\nTo enhance the data quality of graph foundation models, efforts\nhave been made to propose augmentation strategies from various\nperspectives, including graph structure learning, feature competion\nand label mixing, etc. However, since existing data augmentation\ntechniques are typically tailored for individual GNN-based mod-\nels, there is a need for further exploration on how to effectively\naugment graph data for LLM-based or GNN+LLM-based models.\n7.1.2\nEvaluation\nWith the help of natural language instructions and powerful\ngeneration capabilities, LLMs can support a variety of open-ended\ntasks [53]. This presents new opportunities for graph foundation\nmodels based on LLM. However, due to the lack of labels in\nopen-ended tasks, evaluating the performance of GFMs in such\ntasks is a challenge. When using LLM as a language foundation\nmodel, the evaluation of its performance on open-ended tasks has\nevolved from human evaluation [61] to meta-evaluation [186]. The\nquestion of whether existing LLM evaluation methods [61, 186]\ncan be applied to GFMs remains to be explored. Beyond eval-\nuating the performance of graph foundation models, it is also\nworth evaluating their robustness, trustworthiness, or holistic per-\nformance, similar to the current practices for language foundation\nmodels [187, 188, 189].\n7.2\nChallenges about Model\n7.2.1\nModel Architectures\nAs mentioned above, the designs of backbone architectures and\nlearning paradigms are crucial for the implementation of graph\nfoundation models. Although this article has outlined some poten-\ntial solutions, it does not rule out the possibility of better ones. For\nexample, regarding the backbone architecture, recent works have\nproposed model architectures that go beyond the Transformer,\noffering improved performance [190] or interpretability [191].\nHowever, it is still unknown whether these backbone architectures\ncan be used for dealing with graph data. Additionally, when\nutilizing GNN+LLM-based models, it is worth exploring how to\nmore effectively align the outputs of both models. Furthermore,\nthere is limited research regarding the emergent abilities or neural\nscaling law [192, 193] of GNN-based [70] or LLM-based [71]\ngraph foundation models. It is yet unclear whether GNN+LLM-\nbased models may have greater potential for emergence. Given\nthat current multimodal foundation models [194] primarily handle\ntext, images, audio, and other modalities, it is an interesting\nresearch direction to explore whether GNNs can be employed to\nfurther expand the diversity of modalities covered by multimodal\n19\nfoundation models or enhance the capabilities of foundation mod-\nels for multimodal learning [195].\n7.2.2\nModel Training\nIn order to achieve homogeneity and make effective use of pre-\ntraining data, it is crucial to design appropriate pretext tasks in\npre-training. Unlike many language foundation models, which\noften use LM [61] or MLM [47] as pretext tasks, there are\nnow various forms of pretext tasks tailored to different GFM\nmodel architectures. Whether each type of pretext task has its\nown applicable scope and whether there will be a unified pretext\ntask is worth further exploration. After obtaining pre-trained graph\nfoundation models, adapting them to downstream tasks is a vital\nconcern. Apart from fine-tuning and prompting that are introduced\nin this article, there are other potential training techniques that can\nbe applied to improve efficiency or update knowledge, such as\nknowledge distillation [196], reinforcement learning from human\nfeedback (RLHF) [61] and model editing [197]. Whether the\nabove-mentioned techniques can be applied to graph foundation\nmodels will be a focal point of future research.\n7.3\nChallenges about Applications\n7.3.1\nKiller Applications\nIn comparison to the outstanding performance of language foun-\ndation models in tasks like text translation [198] and text gen-\neration [199], whether graph foundation models can similarly\ncatalyze groundbreaking applications in graph tasks is not yet\nclear. For scenarios that are well-suited for the application of\nGNNs, such as e-commerce [82] and finance [200], potential\nresearch directions include leveraging graph-based models in-\ntegrated with LLMs to better support open-ended tasks [178],\nor enhancing the reasoning capabilities of LLMs through graph\nlearning techniques [201]. Furthermore, graph foundation models\nhave the potential to make breakthroughs in some emerging fields.\nFor example, drug development is a time-consuming and costly\nprocess [202], and language foundation models have already been\nsuccessfully used for related tasks like target identification and\nside effect prediction [1]. Given the 3D geometric structure of pro-\nteins [203, 204, 205], graph foundation models hold the promise of\nenhancing the drug discovery pipeline by leveraging their ability\nto model graph structure information [206], potentially speeding\nup the process further. Additionally, urban computing may also\nrepresent a crucial application scenario for graph foundation mod-\nels. It is worth noting that traditional traffic prediction techniques\nhave been primarily focused on addressing individual tasks such\nas travel demand prediction [207] and traffic flow prediction [208],\nlacking a comprehensive understanding of the entire transportation\nsystem. Given that the transportation system can be viewed as a\nspatio-temporal graph, graph foundation models hold the potential\nto capture the participation behavior of actors in the transportation\nsystem [209], thereby offering a unified approach to addressing\nvarious issues in urban computing.\n7.3.2\nTrustworthiness\nDespite the strong performance of LLM-based foundation models,\ntheir black-box nature [210] introduces a host of safety con-\ncerns, such as hallucination and privacy leaks. The hallucination\nrefers to the output appearing plausible but deviating from user\ninput, context, or facts [211]. Existing research suggests that\nthis phenomenon is associated with multiple factors, such as\nthe model\u2019s overconfidence in its own behavior [212] and the\nmisunderstanding of false correlations [213]. Similarly, recent\nwork has pointed out that pre-trained GNNs also pose certain\ntrustworthy risks about fairness [214] and robustness against\nattacks [215]. Given the unique nature of graph data, we may\nrequire certain techniques to prevent or mitigate security risks on\ngraph foundation models, such as confidence calibration [216] or\ncounterfactual reasoning [217]. Additionally, given that existing\nresearch has indicated privacy risks in both GNN [218, 219] and\nLLM [220], enhancing the privacy of graph foundation models is\nalso a critical concern. Some potential solutions include federated\nlearning [221], RLHF [61] and red teaming [222], but whether\nthese methods can be applied to graph foundation models is still\nunknown.\n8\nCONCLUSIONS\nThe development of foundation models and graph machine learn-\ning has spurred the emergence of a new research direction, with\nthe aim to train on broad graph data and apply it to a wide\nrange of downstream graph tasks. In this article, we propose the\nconcept of graph foundation models (GFMs) for the first time, and\nprovide an introduction to relevant concepts and representative\nmethods. We summarize existing works towards GFMs into three\nmain categories based on their reliance on graph neural networks\n(GNNs) and large language models (LLMs): GNN-based models,\nLLM-based models, and GNN+LLM-based models. For each\ncategory of methods, we introduce their backbone architectures,\npre-training, and adaptation strategies separately. After providing\na comprehensive overview of the current landscape of graph\nfoundation models, this article also points out the future directions\nfor this evolving field.\n20\nREFERENCES\n[1]\nR. Bommasani, D. A. Hudson, E. Adeli, R. Altman,\nS. Arora, S. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut,\nE. Brunskill, et al., \u201cOn the opportunities and risks of foun-\ndation models,\u201d arXiv preprint arXiv:2108.07258, 2021.\n[2]\nJ. Wei, Y. Tay, R. Bommasani, C. Raffel, B. Zoph,\nS. Borgeaud, D. Yogatama, M. Bosma, D. Zhou, D. Metzler,\nE. H. Chi, T. Hashimoto, O. Vinyals, P. Liang, J. Dean, and\nW. Fedus, \u201cEmergent abilities of large language models,\u201d\nTrans. Mach. Learn. Res., 2022.\n[3]\nW. Wang, Z. Chen, X. Chen, J. Wu, X. Zhu, G. Zeng,\nP. Luo, T. Lu, J. Zhou, Y. Qiao, et al., \u201cVisionllm: Large\nlanguage model is also an open-ended decoder for vision-\ncentric tasks,\u201d Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[4]\nH. Zhang, X. Li, and L. Bing, \u201cVideo-llama: An instruction-\ntuned audio-visual language model for video understand-\ning,\u201d in Proceedings of the 2023 Conference on Empirical\nMethods in Natural Language Processing: System Demon-\nstrations, pp. 543\u2013553, 2023.\n[5]\nZ. Zhao, W. Fan, J. Li, Y. Liu, X. Mei, Y. Wang, Z. Wen,\nF. Wang, X. Zhao, J. Tang, et al., \u201cRecommender systems in\nthe era of large language models (llms),\u201d IEEE Transactions\non Knowledge and Data Engineering, 2024.\n[6]\nB. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online\nlearning of social representations,\u201d in Proc. of KDD, 2014.\n[7]\nS. Yan, D. Xu, B. Zhang, H.-J. Zhang, Q. Yang, and S. Lin,\n\u201cGraph embedding and extensions: A general framework\nfor dimensionality reduction,\u201d IEEE transactions on pattern\nanalysis and machine intelligence, 2006.\n[8]\nL. Waikhom and R. Patgiri, \u201cGraph neural networks:\nMethods, applications, and opportunities,\u201d arXiv preprint\narXiv:2108.10733, 2021.\n[9]\nT. N. Kipf and M. Welling, \u201cSemi-supervised classification\nwith graph convolutional networks,\u201d in Proc. of ICLR, 2017.\n[10]\nM. Zhang and Y. Chen, \u201cLink prediction based on graph\nneural networks,\u201d in Proc. of ICONIP, 2018.\n[11]\nK. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful\nare graph neural networks?,\u201d in Proc. of ICLR, 2019.\n[12]\nC. Wang, S. Pan, R. Hu, G. Long, J. Jiang, and C. Zhang,\n\u201cAttributed graph clustering: a deep attentional embedding\napproach,\u201d in Proc. of IJCAI, 2019.\n[13]\nU. Alon and E. Yahav, \u201cOn the bottleneck of graph neural\nnetworks and its practical implications,\u201d in Proc. of ICLR,\n2021.\n[14]\nL. Yang, J. Zheng, H. Wang, Z. Liu, Z. Huang, S. Hong,\nW. Zhang, and B. Cui, \u201cIndividual and structural graph\ninformation bottlenecks for out-of-distribution generaliza-\ntion,\u201d IEEE Transactions on Knowledge and Data Engi-\nneering, 2023.\n[15]\nC. Zhou, Q. Li, C. Li, J. Yu, Y. Liu, G. Wang, K. Zhang,\nC. Ji, Q. Yan, L. He, et al., \u201cA comprehensive survey\non pretrained foundation models: A history from bert to\nchatgpt,\u201d arXiv preprint arXiv:2302.09419, 2023.\n[16]\nS. Pan, L. Luo, Y. Wang, C. Chen, J. Wang, and X. Wu,\n\u201cUnifying large language models and knowledge graphs:\nA roadmap,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2024.\n[17]\nJ. Z. Pan, S. Razniewski, J.-C. Kalo, S. Singhania, J. Chen,\nS. Dietze, H. Jabeen, J. Omeliyanenko, W. Zhang, M. Lis-\nsandrini, et al., \u201cLarge language models and knowledge\ngraphs: Opportunities and challenges,\u201d TRANSACTIONS\nON GRAPH DATA AND KNOWLEDGE, pp. 1\u201338, 2023.\n[18]\nZ. Zhang, H. Li, Z. Zhang, Y. Qin, X. Wang, and W. Zhu,\n\u201cGraph meets llms: Towards large graph models,\u201d in\nNeurIPS 2023 Workshop: New Frontiers in Graph Learn-\ning, 2023.\n[19]\nG. Li, M. Muller, A. Thabet, and B. Ghanem, \u201cDeepgcns:\nCan gcns go as deep as cnns?,\u201d in Proc. of ICCV, 2019.\n[20]\nL. Freeman, \u201cThe development of social network analysis,\u201d\nA Study in the Sociology of Science, 2004.\n[21]\nG. Muzio, L. O\u2019Bray, and K. Borgwardt, \u201cBiological net-\nwork analysis with deep learning,\u201d Briefings in bioinformat-\nics, 2021.\n[22]\nW. Jiang and J. Luo, \u201cGraph neural network for traffic\nforecasting: A survey,\u201d Expert Systems with Applications,\n2022.\n[23]\nW. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. S. Pande,\nand J. Leskovec, \u201cStrategies for pre-training graph neural\nnetworks,\u201d in Proc. of ICLR, 2020.\n[24]\nS. Xiao, S. Wang, Y. Dai, and W. Guo, \u201cGraph neural\nnetworks in node classification: survey and evaluation,\u201d\nMachine Vision and Applications, 2022.\n[25]\nC. Shi, Y. Li, J. Zhang, Y. Sun, and P. S. Yu, \u201cA survey of\nheterogeneous information network analysis,\u201d IEEE Trans-\nactions on Knowledge and Data Engineering, 2016.\n[26]\nY. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, \u201cHypergraph\nneural networks,\u201d in Proc. of AAAI, 2019.\n[27]\nJ. Skarding, B. Gabrys, and K. Musial, \u201cFoundations and\nmodeling of dynamic networks using dynamic graph neural\nnetworks: A survey,\u201d IEEE Access, 2021.\n[28]\nJ. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and\nG. E. Dahl, \u201cNeural message passing for quantum chem-\nistry,\u201d in Proc. of ICML, 2017.\n[29]\nW. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive repre-\nsentation learning on large graphs,\u201d Proc. of NeurIPS, 2017.\n[30]\nP. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Li`o,\nand Y. Bengio, \u201cGraph attention networks,\u201d in Proc. of\nICLR, 2018.\n[31]\nK. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual\nlearning for image recognition,\u201d in Proc. of CVPR, 2016.\n[32]\nQ. Li, Z. Han, and X.-M. Wu, \u201cDeeper insights into graph\nconvolutional networks for semi-supervised learning,\u201d in\nProc. of AAAI, 2018.\n[33]\nY. Rong, W. Huang, T. Xu, and J. Huang, \u201cDropedge:\nTowards deep graph convolutional networks on node clas-\nsification,\u201d in Proc. of ICLR, 2020.\n[34]\nC. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen,\nand T.-Y. Liu, \u201cDo transformers really perform badly for\ngraph representation?,\u201d Proc. of NeurIPS, 2021.\n[35]\nD. Chen, L. O\u2019Bray, and K. Borgwardt, \u201cStructure-aware\ntransformer for graph representation learning,\u201d in Proc. of\nICML, 2022.\n[36]\nD. Kreuzer, D. Beaini, W. Hamilton, V. L\u00b4etourneau, and\nP. Tossou, \u201cRethinking graph transformers with spectral\nattention,\u201d in Proc. of NeurIPS, 2021.\n[37]\nL. Ramp\u00b4a\u02c7sek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf,\nand D. Beaini, \u201cRecipe for a general, powerful, scalable\ngraph transformer,\u201d Proc. of NeurIPS, 2022.\n[38]\nJ. B. Lee, R. Rossi, and X. Kong, \u201cGraph classification\nusing structural attention,\u201d in Proc. of KDD, 2018.\n21\n[39]\nX. Jiang, P. Ji, and S. Li, \u201cCensnet: convolution with edge-\nnode switching in graph neural networks,\u201d in Proc. of\nIJCAI, 2019.\n[40]\nO. Wieder, S. Kohlbacher, M. Kuenemann, A. Garon,\nP. Ducrot, T. Seidel, and T. Langer, \u201cA compact review of\nmolecular property prediction with graph neural networks,\u201d\nDrug Discovery Today: Technologies, 2020.\n[41]\nZ. Song, X. Yang, Z. Xu, and I. King, \u201cGraph-based\nsemi-supervised learning: A comprehensive review,\u201d IEEE\nTransactions on Neural Networks and Learning Systems,\n2022.\n[42]\nC. Yang, J. Liu, and C. Shi, \u201cExtract the knowledge of graph\nneural networks and go beyond it: An effective knowledge\ndistillation framework,\u201d in Proc. of WWW, 2021.\n[43]\nM. K. Rahman and A. Azad, \u201cA comprehensive an-\nalytical\nsurvey\non\nunsupervised\nand\nsemi-supervised\ngraph representation learning methods,\u201d arXiv preprint\narXiv:2112.10372, 2021.\n[44]\nS. E. Schaeffer, \u201cGraph clustering,\u201d Computer science re-\nview, 2007.\n[45]\nY. Liu, M. Jin, S. Pan, C. Zhou, Y. Zheng, F. Xia, and\nP. S. Yu, \u201cGraph self-supervised learning: A survey,\u201d IEEE\nTransactions on Knowledge and Data Engineering, 2022.\n[46]\nG. Paa\u00df and S. Giesselbach, Foundation Models for Nat-\nural Language Processing: Pre-trained Language Models\nIntegrating Media. Springer Nature, 2023.\n[47]\nJ. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT:\npre-training of deep bidirectional transformers for language\nunderstanding,\u201d 2019.\n[48]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all\nyou need,\u201d Proc. of NeurIPS, 2017.\n[49]\nW. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou,\nY. Min, B. Zhang, J. Zhang, Z. Dong, et al., \u201cA survey of\nlarge language models,\u201d arXiv preprint arXiv:2303.18223,\n2023.\n[50]\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Ka-\nplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, et al., \u201cLanguage models are few-shot learners,\u201d\nProc. of NeurIPS, 2020.\n[51]\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra,\nA.\nRoberts,\nP.\nBarham,\nH.\nW.\nChung,\nC.\nSutton,\nS. Gehrmann, et al., \u201cPalm: Scaling language modeling\nwith pathways,\u201d Journal of Machine Learning Research,\nvol. 24, no. 240, pp. 1\u2013113, 2023.\n[52]\nR.\nTaylor,\nM.\nKardas,\nG.\nCucurull,\nT.\nScialom,\nA. Hartshorn, E. Saravia, A. Poulton, V. Kerkez, and\nR. Stojnic, \u201cGalactica: A large language model for science,\u201d\nCoRR, 2022.\n[53]\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M. Lachaux,\nT. Lacroix, B. Rozi`ere, N. Goyal, E. Hambro, F. Azhar,\nA. Rodriguez, A. Joulin, E. Grave, and G. Lample, \u201cLlama:\nOpen and efficient foundation language models,\u201d CoRR,\n2023.\n[54]\nX. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \u201cPre-\ntrained models for natural language processing: A survey,\u201d\nScience China Technological Sciences, 2020.\n[55]\nP. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig,\n\u201cPre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing,\u201d ACM\nComputing Surveys, 2023.\n[56]\nD. Erhan, A. Courville, Y. Bengio, and P. Vincent, \u201cWhy\ndoes unsupervised pre-training help deep learning?,\u201d in\nProc. of AISTATS, 2010.\n[57]\nJ. Howard and S. Ruder, \u201cUniversal language model fine-\ntuning for text classification,\u201d in Proc. of ACL, 2018.\n[58]\nJ. Zhang, Y. Zhao, M. Saleh, and P. Liu, \u201cPegasus: Pre-\ntraining with extracted gap-sentences for abstractive sum-\nmarization,\u201d in Proc. of ICML, 2020.\n[59]\nF. Petroni, T. Rockt\u00a8aschel, S. Riedel, P. Lewis, A. Bakhtin,\nY. Wu, and A. Miller, \u201cLanguage models as knowledge\nbases?,\u201d in Proceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Processing and the\n9th International Joint Conference on Natural Language\nProcessing (EMNLP-IJCNLP), pp. 2463\u20132473, 2019.\n[60]\nZ. Jiang, F. F. Xu, J. Araki, and G. Neubig, \u201cHow can we\nknow what language models know?,\u201d Transactions of the\nAssociation for Computational Linguistics, 2020.\n[61]\nL. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray,\net al., \u201cTraining language models to follow instructions with\nhuman feedback,\u201d Proc. of NeurIPS, 2022.\n[62]\nX. L. Li and P. Liang, \u201cPrefix-tuning: Optimizing continu-\nous prompts for generation,\u201d in Proc. of ACL, 2021.\n[63]\nK. Hambardzumyan, H. Khachatrian, and J. May, \u201cWarp:\nWord-level adversarial reprogramming,\u201d in Proceedings of\nthe 59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint Con-\nference on Natural Language Processing (Volume 1: Long\nPapers), pp. 4921\u20134933, 2021.\n[64]\nL. Wu, Z. Qiu, Z. Zheng, H. Zhu, and E. Chen, \u201cExplor-\ning large language model for graph data understanding in\nonline job recommendations,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 38, pp. 9178\u2013\n9186, 2024.\n[65]\nZ. Liu, X. Yu, Y. Fang, and X. Zhang, \u201cGraphprompt: Uni-\nfying pre-training and downstream tasks for graph neural\nnetworks,\u201d in Proceedings of the ACM Web Conference\n2023, 2023.\n[66]\nX. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, \u201cAll in one:\nMulti-task prompting for graph neural networks,\u201d in Proc.\nof KDD, 2023.\n[67]\nQ. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun,\nJ. Xu, and Z. Sui, \u201cA survey for in-context learning,\u201d arXiv\npreprint arXiv:2301.00234, 2022.\n[68]\nJ. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi,\nQ. V. Le, D. Zhou, et al., \u201cChain-of-thought prompting elic-\nits reasoning in large language models,\u201d Proc. of NeurIPS,\n2022.\n[69]\nJ. Li, D. Li, S. Savarese, and S. C. H. Hoi, \u201cBLIP-\n2: bootstrapping language-image pre-training with frozen\nimage encoders and large language models,\u201d in Proc. of\nICML, 2023.\n[70]\nQ. Huang, H. Ren, P. Chen, G. Kr\u02c7zmanc, D. Zeng, P. Liang,\nand J. Leskovec, \u201cPRODIGY: Enabling in-context learning\nover graphs,\u201d in Thirty-seventh Conference on Neural In-\nformation Processing Systems, 2023.\n[71]\nH. Wang, S. Feng, T. He, Z. Tan, X. Han, and Y. Tsvetkov,\n\u201cCan language models solve graph problems in natural\nlanguage?,\u201d Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[72]\nB. Su, D. Du, Z. Yang, Y. Zhou, J. Li, A. Rao, H. Sun,\n22\nZ. Lu, and J.-R. Wen, \u201cA molecular multimodal foundation\nmodel associating molecule graphs with natural language,\u201d\narXiv preprint arXiv:2209.05481, 2022.\n[73]\nY. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang,\n\u201cDeep Graph Contrastive Representation Learning,\u201d in\nICML Workshop on Graph Representation Learning and\nBeyond, 2020.\n[74]\nM. Sun, J. Xing, H. Wang, B. Chen, and J. Zhou, \u201cMocl:\ndata-driven molecular fingerprint via knowledge-aware con-\ntrastive learning from molecular graph,\u201d in Proc. of KDD,\n2021.\n[75]\nZ. Hou, X. Liu, Y. Cen, Y. Dong, H. Yang, C. Wang,\nand J. Tang, \u201cGraphmae: Self-supervised masked graph\nautoencoders,\u201d in Proc. of KDD, 2022.\n[76]\nZ. Hou, Y. He, Y. Cen, X. Liu, Y. Dong, E. Kharlamov, and\nJ. Tang, \u201cGraphmae2: A decoding-enhanced masked self-\nsupervised graph learner,\u201d in Proceedings of the ACM Web\nConference 2023, 2023.\n[77]\nS. Li, X. Han, and J. Bai, \u201cAdaptergnn: Parameter-efficient\nfine-tuning improves generalization in gnns,\u201d in Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvol. 38, pp. 13600\u201313608, 2024.\n[78]\nA. Gui, J. Ye, and H. Xiao, \u201cG-adapter: Towards structure-\naware parameter-efficient transfer learning for graph trans-\nformer networks,\u201d in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 38, pp. 12226\u201312234, 2024.\n[79]\nM. Sun, K. Zhou, X. He, Y. Wang, and X. Wang, \u201cGppt:\nGraph pre-training and prompt tuning to generalize graph\nneural networks,\u201d in Proc. of KDD, 2022.\n[80]\nX. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S.\nYu, \u201cHeterogeneous graph attention network,\u201d in Proc. of\nWWW, 2019.\n[81]\nJ. M. Thomas, A. Moallemy-Oureh, S. Beddar-Wiesing,\nand C. Holzh\u00a8uter, \u201cGraph neural networks designed for\ndifferent graph types: A survey,\u201d Trans. Mach. Learn. Res.,\n2023.\n[82]\nD. Zhang, X. Huang, Z. Liu, J. Zhou, Z. Hu, X. Song, Z. Ge,\nL. Wang, Z. Zhang, and Y. Qi, \u201cAGL: A scalable system for\nindustrial-purpose graph machine learning,\u201d Proc. VLDB\nEndow., 2020.\n[83]\nH. Yang, \u201cAligraph: A comprehensive graph neural network\nplatform,\u201d in Proc. of KDD, 2019.\n[84]\nV. P. Dwivedi, L. Ramp\u00b4a\u02c7sek, M. Galkin, A. Parviz, G. Wolf,\nA. T. Luu, and D. Beaini, \u201cLong range graph benchmark,\u201d\nProc. of NeurIPS, 2022.\n[85]\nR. Zhu, X. Jiang, J. Lu, and S. Li, \u201cCross-domain graph\nconvolutions for adversarial unsupervised domain adapta-\ntion,\u201d IEEE Transactions on Neural Networks and Learning\nSystems, 2021.\n[86]\nD. Zhou, L. Zheng, D. Fu, J. Han, and J. He, \u201cMentorgnn:\nDeriving curriculum for pre-training gnns,\u201d in Proc. of\nCIKM, 2022.\n[87]\nJ. You, Z. Ying, and J. Leskovec, \u201cDesign space for graph\nneural networks,\u201d Proc. of NeurIPS, 2020.\n[88]\nP. Li, Y. Wang, H. Wang, and J. Leskovec, \u201cDistance\nencoding: Design provably more powerful neural networks\nfor graph representation learning,\u201d Proc. of NeurIPS, 2020.\n[89]\nX. Zou, X. Zhao, P. Li`o, and Y. Zhao, \u201cWill more expressive\ngraph neural networks do better on generative tasks?,\u201d in\nLearning on Graphs Conference, pp. 21\u20131, PMLR, 2024.\n[90]\nQ. Lhoest, A. V. del Moral, Y. Jernite, A. Thakur, P. von\nPlaten, S. Patil, J. Chaumond, M. Drame, J. Plu, L. Tunstall,\net al., \u201cDatasets: A community library for natural language\nprocessing,\u201d in Proc. of EMNLP, 2021.\n[91]\nX. He, X. Bresson, T. Laurent, A. Perold, Y. LeCun, and\nB. Hooi, \u201cHarnessing explanations: Llm-to-lm interpreter\nfor enhanced text-attributed graph representation learning,\u201d\nin The Twelfth International Conference on Learning Rep-\nresentations, 2024.\n[92]\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang,\nM. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring\nthe limits of transfer learning with a unified text-to-text\ntransformer,\u201d The Journal of Machine Learning Research,\n2020.\n[93]\nP. Veli\u02c7ckovi\u00b4c, W. Fedus, W. L. Hamilton, P. Li`o, Y. Bengio,\nand R. D. Hjelm, \u201cDeep graph infomax,\u201d in Proc. of ICLR,\n2019.\n[94]\nT. N. Kipf and M. Welling, \u201cVariational graph auto-\nencoders,\u201d NIPS Workshop on Bayesian Deep Learning,\n2016.\n[95]\nX. Gong, C. Yang, and C. Shi, \u201cMa-gcl: Model augmenta-\ntion tricks for graph contrastive learning,\u201d in Proc. of AAAI,\n2023.\n[96]\nX. Yu, C. Zhou, Y. Fang, and X. Zhang, \u201cMultigprompt\nfor multi-task pre-training and prompting on graphs,\u201d in\nProceedings of the ACM on Web Conference 2024, pp. 515\u2013\n526, 2024.\n[97]\nY. Yan, P. Zhang, Z. Fang, and Q. Long, \u201cInductive graph\nalignment prompt: Bridging the gap between graph pre-\ntraining and inductive fine-tuning from spectral perspec-\ntive,\u201d in Proceedings of the ACM on Web Conference 2024,\npp. 4328\u20134339, 2024.\n[98]\nX. Yu, Y. Fang, Z. Liu, and X. Zhang, \u201cHgprompt: Bridg-\ning homogeneous and heterogeneous graphs for few-shot\nprompt learning,\u201d in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 38, pp. 16578\u201316586, 2024.\n[99]\nZ. Wen, Y. Fang, Y. Liu, Y. Guo, and S. Hao, \u201cVoucher\nabuse detection with prompt-based fine-tuning on graph\nneural networks,\u201d in Proc. of CIKM, 2023.\n[100] Z. Hu, Y. Dong, K. Wang, K.-W. Chang, and Y. Sun, \u201cGpt-\ngnn: Generative pre-training of graph neural networks,\u201d in\nProc. of KDD, 2020.\n[101] X. Jiang, T. Jia, Y. Fang, C. Shi, Z. Lin, and H. Wang,\n\u201cPre-training on large-scale heterogeneous graph,\u201d in Proc.\nof KDD, 2021.\n[102] X. Jiang, Y. Lu, Y. Fang, and C. Shi, \u201cContrastive pre-\ntraining of gnns on heterogeneous graphs,\u201d in Proc. of\nCIKM, 2021.\n[103] X. Yu, Z. Liu, Y. Fang, Z. Liu, S. Chen, and X. Zhang,\n\u201cGeneralized graph prompt: Toward a unification of pre-\ntraining and downstream tasks on graphs,\u201d IEEE Transac-\ntions on Knowledge and Data Engineering, 2024.\n[104] J. Qiu, Q. Chen, Y. Dong, J. Zhang, H. Yang, M. Ding,\nK. Wang, and J. Tang, \u201cGcc: Graph contrastive coding for\ngraph neural network pre-training,\u201d in Proc. of KDD, 2020.\n[105] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen,\n\u201cGraph contrastive learning with augmentations,\u201d Proc. of\nNeurIPS, 2020.\n[106] Y. Guo, C. Yang, Y. Chen, J. Liu, C. Shi, and J. Du, \u201cA\ndata-centric framework to endow graph neural networks\nwith out-of-distribution detection ability,\u201d in Proc. of KDD,\n2023.\n23\n[107] T. Fang, Y. M. Zhang, Y. Yang, C. Wang, and L. CHEN,\n\u201cUniversal prompt tuning for graph neural networks,\u201d in\nThirty-seventh Conference on Neural Information Process-\ning Systems, 2023.\n[108] Y.\nZhu,\nJ.\nGuo,\nand\nS.\nTang,\n\u201cSgl-pt:\nA\nstrong\ngraph learner with graph prompt tuning,\u201d arXiv preprint\narXiv:2302.12449, 2023.\n[109] A. Davies, R. Green, N. Ajmeri, and T. S. Filho, \u201cIts all\ngraph to me: Single-model graph representation learning\non multiple domains,\u201d in NeurIPS 2023 Workshop: New\nFrontiers in Graph Learning, 2023.\n[110] Y. Zhu, Y. Wang, H. Shi, Z. Zhang, D. Jiao, and S. Tang,\n\u201cGraphcontrol: Adding conditional control to universal\ngraph pre-trained models for graph domain transfer learn-\ning,\u201d in The Web Conference 2024, 2024.\n[111] Y. Sun, Q. Zhu, Y. Yang, C. Wang, T. Fan, J. Zhu, and\nL. Chen, \u201cFine-tuning graph neural networks by preserving\ngraph generative patterns,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 38, pp. 9053\u2013\n9061, 2024.\n[112] J. Zhang, H. Zhang, C. Xia, and L. Sun, \u201cGraph-bert: Only\nattention is needed for learning graph representations,\u201d\narXiv preprint arXiv:2001.05140, 2020.\n[113] Y. Rong, Y. Bian, T. Xu, W. Xie, Y. Wei, W. Huang, and\nJ. Huang, \u201cSelf-supervised graph transformer on large-scale\nmolecular data,\u201d Proc. of NeurIPS, 2020.\n[114] Z. Hu, Y. Dong, K. Wang, and Y. Sun, \u201cHeterogeneous\ngraph transformer,\u201d in Proc. of WWW, 2020.\n[115] B. Weisfeiler and A. Leman, \u201cThe reduction of a graph to\ncanonical form and the algebra which appears therein,\u201d nti,\nSeries, 1968.\n[116] J. Xia, Y. Zhu, Y. Du, and S. Z. Li, \u201cA survey of pretraining\non graphs: Taxonomy, methods, and applications,\u201d arXiv\npreprint arXiv:2202.07893, 2022.\n[117] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby, \u201cAn\nimage is worth 16x16 words: Transformers for image recog-\nnition at scale,\u201d 2021.\n[118] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and\nB. Guo, \u201cSwin transformer: Hierarchical vision transformer\nusing shifted windows,\u201d in Proc. of ICCV, 2021.\n[119] J. Kim, D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and\nS. Hong, \u201cPure transformers are powerful graph learners,\u201d\nin Proc. of NeurIPS, 2022.\n[120] Y. Wu, Y. Fang, and L. Liao, \u201cOn the feasibility of simple\ntransformer for dynamic graph modeling,\u201d in Proceedings\nof the ACM on Web Conference 2024, WWW \u201924, (New\nYork, NY, USA), p. 870\u2013880, Association for Computing\nMachinery, 2024.\n[121] Q. Mao, Z. Liu, C. Liu, and J. Sun, \u201cHinormer: Repre-\nsentation learning on heterogeneous information networks\nwith graph transformer,\u201d in Proceedings of the ACM Web\nConference 2023, pp. 599\u2013610, 2023.\n[122] Z. Lu, Y. Fang, C. Yang, and C. Shi, \u201cHeterogeneous graph\ntransformer with poly-tokenization,\u201d in Proceedings of the\nThirty-Third International Joint Conference on Artificial\nIntelligence, IJCAI-24, International Joint Conferences on\nArtificial Intelligence Organization, 2024.\n[123] Y. Xing, X. Wang, Y. Li, H. Huang, and C. Shi, \u201cLess is\nmore: on the over-globalizing problem in graph transform-\ners,\u201d in Forty-first International Conference on Machine\nLearning, 2024.\n[124] E. Rosenbluth, J. T\u00a8onshoff, M. Ritzert, B. Kisin, and\nM. Grohe, \u201cDistinguished in uniform: Self-attention vs.\nvirtual nodes,\u201d in The Twelfth International Conference on\nLearning Representations, 2024.\n[125] L. M\u00a8uller, M. Galkin, C. Morris, and L. Ramp\u00b4asek, \u201cAt-\ntending to graph transformers,\u201d Transactions on Machine\nLearning Research (TMLR), 2024.\n[126] E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang,\nP. Zhao, J. Huang, S. Ananiadou, and Y. Rong, \u201cTrans-\nformer for graphs: An overview from architecture perspec-\ntive,\u201d arXiv preprint arXiv:2202.08455, 2022.\n[127] L. Wu, H. Lin, C. Tan, Z. Gao, and S. Z. Li, \u201cSelf-\nsupervised learning on graphs: Contrastive, generative, or\npredictive,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2021.\n[128] Y. Xie, Z. Xu, J. Zhang, Z. Wang, and S. Ji, \u201cSelf-\nsupervised learning of graph neural networks: A unified\nreview,\u201d IEEE transactions on pattern analysis and machine\nintelligence, 2022.\n[129] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding conditional\ncontrol to text-to-image diffusion models,\u201d in Proceedings\nof the IEEE/CVF International Conference on Computer\nVision, pp. 3836\u20133847, 2023.\n[130] B. Lester, R. Al-Rfou, and N. Constant, \u201cThe power of\nscale for parameter-efficient prompt tuning,\u201d in Proceedings\nof the 2021 Conference on Empirical Methods in Natural\nLanguage Processing (M.-F. Moens, X. Huang, L. Specia,\nand S. W.-t. Yih, eds.), (Online and Punta Cana, Dominican\nRepublic), pp. 3045\u20133059, Association for Computational\nLinguistics, Nov. 2021.\n[131] X. Liu, K. Ji, Y. Fu, W. Tam, Z. Du, Z. Yang, and\nJ. Tang, \u201cP-tuning: Prompt tuning can be comparable to\nfine-tuning across scales and tasks,\u201d in Proceedings of the\n60th Annual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) (S. Muresan, P. Nakov,\nand A. Villavicencio, eds.), (Dublin, Ireland), pp. 61\u201368,\nAssociation for Computational Linguistics, May 2022.\n[132] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang, \u201cLanguage\nis all a graph needs,\u201d EACL, 2024.\n[133] J. Guo, L. Du, and H. Liu, \u201cGpt4graph: Can large\nlanguage models understand graph structured data? an\nempirical evaluation and benchmarking,\u201d arXiv preprint\narXiv:2305.15066, 2023.\n[134] Z. Chen, H. Mao, H. Li, W. Jin, H. Wen, X. Wei, S. Wang,\nD. Yin, W. Fan, H. Liu, et al., \u201cExploring the potential of\nlarge language models (llms) in learning on graphs,\u201d ACM\nSIGKDD Explorations Newsletter, vol. 25, no. 2, pp. 42\u201361,\n2024.\n[135] H. Zhao, S. Liu, M. Chang, H. Xu, J. Fu, Z. Deng, L. Kong,\nand Q. Liu, \u201cGimlet: A unified graph-text model for\ninstruction-based molecule zero-shot learning,\u201d Advances\nin Neural Information Processing Systems, vol. 36, 2023.\n[136] C. Liu and B. Wu, \u201cEvaluating large language models on\ngraphs: Performance insights and comparative analysis,\u201d\narXiv preprint arXiv:2308.11224, 2023.\n[137] J. Zhao, L. Zhuo, Y. Shen, M. Qu, K. Liu, M. Bronstein,\nZ. Zhu, and J. Tang, \u201cGraphtext: Graph reasoning in text\nspace,\u201d 2023.\n[138] C. Qian, H. Tang, Z. Yang, H. Liang, and Y. Liu, \u201cCan large\n24\nlanguage models empower molecular property prediction?,\u201d\narXiv preprint arXiv:2307.07443, 2023.\n[139] F. Wenkel, G. Wolf, and B. Knyazev, \u201cPretrained language\nmodels to solve graph tasks in natural language,\u201d in ICML\n2023 Workshop on Structured Probabilistic Inference {\\&}\nGenerative Modeling, 2023.\n[140] J. Huang, X. Zhang, Q. Mei, and J. Ma, \u201cCan llms ef-\nfectively leverage graph structural information: when and\nwhy,\u201d NeurIPS2023 workshop, 2023.\n[141] N. Chen, Y. Li, J. Tang, and J. Li, \u201cGraphwiz: An\ninstruction-following language model for graph problems,\u201d\nKDD 2024, 2024.\n[142] A. Antonucci, G. Piqu\u00b4e, and M. Zaffalon, \u201cZero-shot causal\ngraph extrapolation from text via llms,\u201d AAAI 2024 XAI4Sci\nworkshop, 2024.\n[143] Z. Zhang, X. Wang, Z. Zhang, H. Li, Y. Qin, S. Wu,\nand W. Zhu, \u201cLlm4dyg: Can large language models solve\nproblems on dynamic graphs?,\u201d KDD2024, 2024.\n[144] Y. Zhang, K. Gong, K. Zhang, H. Li, Y. Qiao, W. Ouyang,\nand X. Yue, \u201cMeta-transformer: A unified framework for\nmultimodal learning,\u201d arXiv preprint arXiv:2307.10802,\n2023.\n[145] OpenAI,\n\u201cGPT-4\ntechnical\nreport,\u201d\narXiv\npreprint\narXiv:2303.08774, 2023.\n[146] W.-L. Chiang, Z. Li, Z. Lin, Y. Sheng, Z. Wu, H. Zhang,\nL. Zheng, S. Zhuang, Y. Zhuang, J. E. Gonzalez, et al., \u201cVi-\ncuna: An open-source chatbot impressing gpt-4 with 90%*\nchatgpt quality,\u201d See https://vicuna. lmsys. org (accessed 14\nApril 2023), 2023.\n[147] P. He, X. Liu, J. Gao, and W. Chen, \u201cDeberta: Decoding-\nenhanced bert with disentangled attention,\u201d in Proc. of\nICLR, 2021.\n[148] N. Reimers and I. Gurevych, \u201cSentence-bert: Sentence em-\nbeddings using siamese bert-networks,\u201d in Proc. of EMNLP,\n2019.\n[149] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. R. Salakhutdinov,\nand Q. V. Le, \u201cXlnet: Generalized autoregressive pretrain-\ning for language understanding,\u201d Proc. of NeurIPS, 2019.\n[150] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mo-\nhamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBART:\ndenoising sequence-to-sequence pre-training for natural\nlanguage generation, translation, and comprehension,\u201d in\nProc. of ACL, 2020.\n[151] N. Saunshi, O. Plevrakis, S. Arora, M. Khodak, and\nH. Khandeparkar, \u201cA theoretical analysis of contrastive\nunsupervised representation learning,\u201d in Proc. of ICML,\n2019.\n[152] C. Poth, J. Pfeiffer, A. R\u00a8uckl\u00b4e, and I. Gurevych, \u201cWhat to\npre-train on? efficient intermediate task selection,\u201d in Proc.\nof EMNLP, 2021.\n[153] X. Liu, P. He, W. Chen, and J. Gao, \u201cMulti-task deep neural\nnetworks for natural language understanding,\u201d in Proceed-\nings of the 57th Annual Meeting of the Association for\nComputational Linguistics, Association for Computational\nLinguistics, 2019.\n[154] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone,\nQ. De Laroussilhe, A. Gesmundo, M. Attariyan, and\nS. Gelly, \u201cParameter-efficient transfer learning for nlp,\u201d in\nProc. of ICML, 2019.\n[155] C. Mavromatis, V. N. Ioannidis, S. Wang, D. Zheng,\nS. Adeshina, J. Ma, H. Zhao, C. Faloutsos, and G. Karypis,\n\u201cTrain your own GNN teacher: Graph-aware distillation on\ntextual graphs,\u201d in Proc. of KDD, 2023.\n[156] E. Chien, W. Chang, C. Hsieh, H. Yu, J. Zhang,\nO. Milenkovic, and I. S. Dhillon, \u201cNode feature extraction\nby self-supervised multi-scale neighborhood prediction,\u201d in\nProc. of ICLR, 2022.\n[157] H. Xie, D. Zheng, J. Ma, H. Zhang, V. N. Ioannidis,\nX. Song, Q. Ping, S. Wang, C. J. Yang, Y. Xu, B. Zeng, and\nT. Chilimbi, \u201cGraph-aware language model pre-training on\na large graph corpus can help multiple graph applications,\u201d\nin Proc. of KDD, 2023.\n[158] H. Liu, J. Feng, L. Kong, N. Liang, D. Tao, Y. Chen, and\nM. Zhang, \u201cOne for all: Towards training one graph model\nfor all classification tasks,\u201d in The Twelfth International\nConference on Learning Representations, 2023.\n[159] B. Jin, Y. Zhang, Q. Zhu, and J. Han, \u201cHeterformer:\nTransformer-based deep node representation learning on\nheterogeneous text-rich networks,\u201d in Proceedings of the\n29th ACM SIGKDD Conference on Knowledge Discovery\nand Data Mining, pp. 1020\u20131031, 2023.\n[160] B. Jin, Y. Zhang, Y. Meng, and J. Han, \u201cEdgeformers:\nGraph-empowered transformers for representation learning\non textual-edge networks,\u201d in The Eleventh International\nConference on Learning Representations, ICLR 2023, Ki-\ngali, Rwanda, May 1-5, 2023, OpenReview.net, 2023.\n[161] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng,\nJ. Wang, D. Yin, and C. Huang, \u201cLlmrec: Large language\nmodels with graph augmentation for recommendation,\u201d in\nProceedings of the 17th ACM International Conference\non Web Search and Data Mining, WSDM 2024, Merida,\nMexico, March 4-8, 2024 (L. A. Caudillo-Mata, S. Lattanzi,\nA. M. Medina, L. Akoglu, A. Gionis, and S. Vassilvitskii,\neds.), pp. 806\u2013815, ACM, 2024.\n[162] Y. Tan, Z. Zhou, H. Lv, W. Liu, and C. Yang, \u201cWalklm:\nA uniform language model fine-tuning framework for at-\ntributed graph embedding,\u201d in Advances in Neural Informa-\ntion Processing Systems 36: Annual Conference on Neural\nInformation Processing Systems 2023, NeurIPS 2023, New\nOrleans, LA, USA, December 10 - 16, 2023 (A. Oh, T. Nau-\nmann, A. Globerson, K. Saenko, M. Hardt, and S. Levine,\neds.), 2023.\n[163] J. Zhu, X. Song, V. N. Ioannidis, D. Koutra, and C. Falout-\nsos, \u201cTouchup-g: Improving feature representation through\ngraph-centric finetuning,\u201d CoRR, vol. abs/2309.13885,\n2023.\n[164] B. Jin, W. Zhang, Y. Zhang, Y. Meng, H. Zhao, and J. Han,\n\u201cLearning multiplex embeddings on text-rich networks with\none text encoder,\u201d in NeurIPS 2023 Workshop: New Fron-\ntiers in Graph Learning, 2023.\n[165] S. Sun, Y. Ren, C. Ma, and X. Zhang, \u201cLarge language\nmodels as topological structure enhancers for text-attributed\ngraphs,\u201d CoRR, vol. abs/2311.14324, 2023.\n[166] Z. Chen, H. Mao, H. Wen, H. Han, W. Jin, H. Zhang, H. Liu,\nand J. Tang, \u201cLabel-free node classification on graphs with\nlarge language models (llms),\u201d in The Twelfth International\nConference on Learning Representations, 2024.\n[167] J. Yang, Z. Liu, S. Xiao, C. Li, D. Lian, S. Agrawal,\nA. Singh, G. Sun, and X. Xie, \u201cGraphformers: Gnn-nested\ntransformers for representation learning on textual graph,\u201d\nProc. of NeurIPS, 2021.\n[168] J. Zhao, M. Qu, C. Li, H. Yan, Q. Liu, R. Li, X. Xie, and\n25\nJ. Tang, \u201cLearning on large-scale text-attributed graphs via\nvariational inference,\u201d in Proc. of ICLR, 2023.\n[169] Z. Wen and Y. Fang, \u201cAugmenting low-resource text clas-\nsification with graph-grounded pre-training and prompt-\ning,\u201d in Proceedings of the 46th International ACM SIGIR\nConference on Research and Development in Information\nRetrieval, pp. 506\u2013516, 2023.\n[170] X. Huang, K. Han, Y. Yang, D. Bao, Q. Tao, Z. Chai,\nand Q. Zhu, \u201cCan GNN be good adapter for llms?,\u201d in\nProceedings of the ACM on Web Conference 2024, WWW\n2024, Singapore, May 13-17, 2024 (T. Chua, C. Ngo,\nR. Kumar, H. W. Lauw, and R. K. Lee, eds.), pp. 893\u2013904,\nACM, 2024.\n[171] Y. Zhu, Y. Wang, H. Shi, and S. Tang, \u201cEfficient tuning and\ninference for large language models on textual graphs,\u201d in\nProceedings of the Thirty-Third International Joint Confer-\nence on Artificial Intelligence, 2024.\n[172] B. Jin, W. Zhang, Y. Zhang, Y. Meng, X. Zhang, Q. Zhu,\nand J. Han, \u201cPatton: Language model pretraining on text-\nrich networks,\u201d in Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume\n1: Long Papers), ACL 2023, Toronto, Canada, July 9-14,\n2023 (A. Rogers, J. L. Boyd-Graber, and N. Okazaki, eds.),\npp. 7005\u20137020, Association for Computational Linguistics,\n2023.\n[173] C. Edwards, C. Zhai, and H. Ji, \u201cText2mol: Cross-modal\nmolecule retrieval with natural language queries,\u201d in Proc.\nof EMNLP, 2021.\n[174] S. Liu, W. Nie, C. Wang, J. Lu, Z. Qiao, L. Liu, J. Tang,\nC. Xiao, and A. Anandkumar, \u201cMulti-modal molecule\nstructure\u2013text model for text-based retrieval and editing,\u201d\nNature Machine Intelligence, vol. 5, no. 12, pp. 1447\u20131457,\n2023.\n[175] P. Seidl, A. Vall, S. Hochreiter, and G. Klambauer, \u201cEn-\nhancing activity prediction models in drug discovery with\nthe ability to understand human language,\u201d in Proc. of\nICML, 2023.\n[176] Z. Liu, S. Li, Y. Luo, H. Fei, Y. Cao, K. Kawaguchi,\nX.\nWang,\nand\nT.\nChua,\n\u201cMolca:\nMolecular\ngraph-\nlanguage modeling with cross-modal projector and uni-\nmodal adapter,\u201d in Proceedings of the 2023 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing, EMNLP 2023, Singapore, December 6-10, 2023\n(H. Bouamor, J. Pino, and K. Bali, eds.), pp. 15623\u201315638,\nAssociation for Computational Linguistics, 2023.\n[177] P. Liu, Y. Ren, J. Tao, and Z. Ren, \u201cGit-mol: A multi-\nmodal large language model for molecular science with\ngraph, image, and text,\u201d Comput. Biol. Medicine, vol. 171,\np. 108073, 2024.\n[178] M. Zhang, M. Sun, P. Wang, S. Fan, Y. Mo, X. Xu, H. Liu,\nC. Yang, and C. Shi, \u201cGraphtranslator: Aligning graph\nmodel to large language model for open-ended tasks,\u201d in\nProceedings of the ACM on Web Conference 2024, WWW\n2024, Singapore, May 13-17, 2024 (T. Chua, C. Ngo,\nR. Kumar, H. W. Lauw, and R. K. Lee, eds.), pp. 1003\u2013\n1014, ACM, 2024.\n[179] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin,\nand C. Huang, \u201cGraphgpt: Graph instruction tuning for\nlarge language models,\u201d in The 47th International ACM\nSIGIR Conference on Research and Development in Infor-\nmation Retrieval, 2024.\n[180] T. Zou, L. Yu, Y. Huang, L. Sun, and B. Du, \u201cPretrain-\ning language models with text-attributed heterogeneous\ngraphs,\u201d in Findings of the Association for Computational\nLinguistics: EMNLP 2023, Singapore, December 6-10,\n2023 (H. Bouamor, J. Pino, and K. Bali, eds.), pp. 10316\u2013\n10333, Association for Computational Linguistics, 2023.\n[181] Z. Liu, X. He, Y. Tian, and N. V. Chawla, \u201cCan we soft\nprompt llms for graph learning tasks?,\u201d in Companion\nProceedings of the ACM on Web Conference 2024, WWW\n2024, Singapore, Singapore, May 13-17, 2024 (T. Chua,\nC. Ngo, R. K. Lee, R. Kumar, and H. W. Lauw, eds.),\npp. 481\u2013484, ACM, 2024.\n[182] J. Wang, J. Wu, Y. Wu, Y. Liu, M. Gao, and J. McAuley,\n\u201cInstructgraph: Boosting large language models via graph-\ncentric instruction tuning and preference alignment,\u201d in\nFindings of the Association for Computational Linguistics:\nACL 2024, 2024.\n[183] Y. Shi, A. Zhang, E. Zhang, Z. Liu, and X. Wang, \u201cRelm:\nLeveraging language models for enhanced chemical reac-\ntion prediction,\u201d in Findings of the Association for Compu-\ntational Linguistics: EMNLP 2023, Singapore, December\n6-10, 2023 (H. Bouamor, J. Pino, and K. Bali, eds.),\npp. 5506\u20135520, Association for Computational Linguistics,\n2023.\n[184] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu,\nM. Catasta, and J. Leskovec, \u201cOpen graph benchmark:\nDatasets for machine learning on graphs,\u201d Proc. of NeurIPS,\n2020.\n[185] A. Khatua, V. S. Mailthody, B. Taleka, T. Ma, X. Song,\nand W.-m. Hwu, \u201cIgb: Addressing the gaps in labeling,\nfeatures, heterogeneity, and size of public graph datasets for\ndeep learning research,\u201d in Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining, pp. 4284\u20134295, 2023.\n[186] Z. Zeng, J. Yu, T. Gao, Y. Meng, T. Goyal, and D. Chen,\n\u201cEvaluating large language models at evaluating instruction\nfollowing,\u201d in The Twelfth International Conference on\nLearning Representations, 2024.\n[187] B. Wang, C. Xu, S. Wang, Z. Gan, Y. Cheng, J. Gao,\nA. H. Awadallah, and B. Li, \u201cAdversarial glue: A multi-task\nbenchmark for robustness evaluation of language models,\u201d\nin Thirty-fifth Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track (Round 2),\n2021.\n[188] B. Wang, W. Chen, H. Pei, C. Xie, M. Kang, C. Zhang,\nC. Xu, Z. Xiong, R. Dutta, R. Schaeffer, et al., \u201cDecod-\ningtrust: A comprehensive assessment of trustworthiness\nin gpt models,\u201d in Thirty-seventh Conference on Neural\nInformation Processing Systems Datasets and Benchmarks\nTrack, 2023.\n[189] R. Bommasani, P. Liang, and T. Lee, \u201cHolistic evaluation\nof language models,\u201d Annals of the New York Academy of\nSciences, 2023.\n[190] T. Dao and A. Gu, \u201cTransformers are ssms: Generalized\nmodels and efficient algorithms through structured state\nspace duality,\u201d Proc. of ICML, 2024.\n[191] Y. Yu, S. Buchanan, D. Pai, T. Chu, Z. Wu, S. Tong,\nB. Haeffele, and Y. Ma, \u201cWhite-box transformers via sparse\nrate reduction,\u201d Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[192] Z. Wang, Y. Li, B. Ding, Y. Li, and Z. Wei, \u201cExploring\n26\nneural scaling law and data pruning methods for node\nclassification on large-scale graphs,\u201d in Proceedings of the\nACM on Web Conference 2024, pp. 780\u2013791, 2024.\n[193] H. Mao, Z. Chen, W. Tang, J. Zhao, Y. Ma, T. Zhao,\nN. Shah, M. Galkin, and J. Tang, \u201cPosition: Graph foun-\ndation models are already here,\u201d in Forty-first International\nConference on Machine Learning, 2024.\n[194] N. Fei, Z. Lu, Y. Gao, G. Yang, Y. Huo, J. Wen, H. Lu,\nR. Song, X. Gao, T. Xiang, et al., \u201cTowards artificial general\nintelligence via a multimodal foundation model,\u201d Nature\nCommunications, 2022.\n[195] Y. Yuan, \u201cOn the power of foundation models,\u201d in Interna-\ntional Conference on Machine Learning, pp. 40519\u201340530,\nPMLR, 2023.\n[196] Y. Jiang, C. Chan, M. Chen, and W. Wang, \u201cLion: Adver-\nsarial distillation of proprietary large language models,\u201d in\nProceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing, pp. 3134\u20133154, 2023.\n[197] Y. Yao, P. Wang, B. Tian, S. Cheng, Z. Li, S. Deng, H. Chen,\nand N. Zhang, \u201cEditing large language models: Problems,\nmethods, and opportunities,\u201d in Proceedings of the 2023\nConference on Empirical Methods in Natural Language\nProcessing, pp. 10222\u201310240, 2023.\n[198] A. Hendy, M. Abdelrehim, A. Sharaf, V. Raunak, M. Gabr,\nH. Matsushita, Y. J. Kim, M. Afify, and H. H. Awadalla,\n\u201cHow good are gpt models at machine translation? a com-\nprehensive evaluation,\u201d arXiv preprint arXiv:2302.09210,\n2023.\n[199] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, \u201cA sur-\nvey of controllable text generation using transformer-based\npre-trained language models,\u201d ACM Computing Surveys,\nvol. 56, no. 3, pp. 1\u201337, 2023.\n[200] J. Wang, S. Zhang, Y. Xiao, and R. Song, \u201cA review on\ngraph neural network methods in financial applications,\u201d\nJournal of Data Science, vol. 20, no. 2, pp. 111\u2013134, 2022.\n[201] J. Yu, R. He, and Z. Ying, \u201cThought propagation: An\nanalogical approach to complex reasoning with large lan-\nguage models,\u201d in The Twelfth International Conference on\nLearning Representations, 2024.\n[202] O. J. Wouters, M. McKee, and J. Luyten, \u201cEstimated\nresearch and development investment needed to bring a new\nmedicine to market, 2009-2018,\u201d Jama, 2020.\n[203] S. Liu, H. Wang, W. Liu, J. Lasenby, H. Guo, and J. Tang,\n\u201cPre-training molecular graph representation with 3d geom-\netry,\u201d in Proc. of ICLR, 2022.\n[204] X. Lin, L. Dai, Y. Zhou, Z.-G. Yu, W. Zhang, J.-Y. Shi,\nD.-S. Cao, L. Zeng, H. Chen, B. Song, et al., \u201cComprehen-\nsive evaluation of deep and graph learning on drug\u2013drug\ninteractions prediction,\u201d Briefings in Bioinformatics, 2023.\n[205] X. Lin, Z. Quan, Z.-J. Wang, Y. Guo, X. Zeng, and\nS. Y. Philip, \u201cEffectively identifying compound-protein\ninteraction using graph neural representation,\u201d IEEE/ACM\nTransactions on Computational Biology and Bioinformat-\nics, 2022.\n[206] J. Xia, Y. Zhu, Y. Du, and S. Z. Li, \u201cA systematic survey\nof chemical pre-trained models,\u201d in Proceedings of the\nThirty-Second International Joint Conference on Artificial\nIntelligence, pp. 6787\u20136795, 2023.\n[207] D. Zhuang, S. Wang, H. Koutsopoulos, and J. Zhao, \u201cUn-\ncertainty quantification of sparse travel demand prediction\nwith spatial-temporal graph neural networks,\u201d in Proc. of\nKDD, 2022.\n[208] X. Wang, Y. Ma, Y. Wang, W. Jin, X. Wang, J. Tang, C. Jia,\nand J. Yu, \u201cTraffic flow prediction via spatial temporal\ngraph neural network,\u201d in Proc. of WWW, 2020.\n[209] X. Wang, D. Wang, L. Chen, F.-Y. Wang, and Y. Lin,\n\u201cBuilding transportation foundation model via genera-\ntive graph transformer,\u201d in 2023 IEEE 26th International\nConference on Intelligent Transportation Systems (ITSC),\npp. 6042\u20136047, IEEE, 2023.\n[210] T. Sun, Y. Shao, H. Qian, X. Huang, and X. Qiu, \u201cBlack-box\ntuning for language-model-as-a-service,\u201d in Proc. of ICML,\n2022.\n[211] Y. Zhang, Y. Li, L. Cui, D. Cai, L. Liu, T. Fu, X. Huang,\nE. Zhao, Y. Zhang, Y. Chen, et al., \u201cSiren\u2019s song in the ai\nocean: A survey on hallucination in large language models,\u201d\narXiv preprint arXiv:2309.01219, 2023.\n[212] R. Ren, Y. Wang, Y. Qu, W. X. Zhao, J. Liu, H. Tian,\nH. Wu, J.-R. Wen, and H. Wang, \u201cInvestigating the fac-\ntual knowledge boundary of large language models with\nretrieval augmentation,\u201d arXiv preprint arXiv:2307.11019,\n2023.\n[213] S. Li, X. Li, L. Shang, Z. Dong, C.-J. Sun, B. Liu, Z. Ji,\nX. Jiang, and Q. Liu, \u201cHow pre-trained language models\ncapture factual knowledge? a causal-inspired analysis,\u201d in\nFindings of the Association for Computational Linguistics:\nACL 2022, pp. 1720\u20131732, 2022.\n[214] Z. Zhang, M. Zhang, Y. Yu, C. Yang, J. Liu, and C. Shi,\n\u201cEndowing pre-trained graph models with provable fair-\nness,\u201d in Proceedings of the ACM on Web Conference 2024,\npp. 1045\u20131056, 2024.\n[215] Z. Zhang, M. Chen, M. Backes, Y. Shen, and Y. Zhang,\n\u201cInference attacks against graph neural networks,\u201d in\n31st USENIX Security Symposium (USENIX Security 22),\npp. 4543\u20134560, 2022.\n[216] X. Wang, H. Liu, C. Shi, and C. Yang, \u201cBe confident!\ntowards trustworthy graph neural networks via confidence\ncalibration,\u201d Proc. of NeurIPS, 2021.\n[217] J. Tan, S. Geng, Z. Fu, Y. Ge, S. Xu, Y. Li, and Y. Zhang,\n\u201cLearning and evaluating graph neural network explana-\ntions based on counterfactual and factual reasoning,\u201d in\nProceedings of the ACM Web Conference 2022, 2022.\n[218] C.\nWu,\nF.\nWu,\nL.\nLyu,\nT.\nQi,\nY.\nHuang,\nand\nX. Xie, \u201cA federated graph neural network framework\nfor privacy-preserving personalization,\u201d Nature Communi-\ncations, vol. 13, no. 1, p. 3091, 2022.\n[219] B. Yan, Y. Cao, H. Wang, W. Yang, J. Du, and C. Shi, \u201cFed-\nerated heterogeneous graph neural network for privacy-\npreserving recommendation,\u201d in Proceedings of the ACM\non Web Conference 2024, pp. 3919\u20133929, 2024.\n[220] R. Staab, M. Vero, M. Balunovic, and M. Vechev, \u201cBeyond\nmemorization: Violating privacy via inference with large\nlanguage models,\u201d in The Twelfth International Conference\non Learning Representations, 2024.\n[221] H. Chen, T. Zhu, T. Zhang, W. Zhou, and P. S. Yu, \u201cPrivacy\nand fairness in federated learning: on the perspective of\ntrade-off,\u201d ACM Computing Surveys, 2023.\n[222] Z. Shi, Y. Wang, F. Yin, X. Chen, K.-W. Chang, and C.-\nJ. Hsieh, \u201cRed teaming language model detectors with\nlanguage models,\u201d Transactions of the Association for Com-\nputational Linguistics, vol. 12, pp. 174\u2013189, 2024.\n",
    "2403.09039": "Detecting Anomalies in Dynamic Graphs via\nMemory enhanced Normality\nJie Liu\nSchool of Computer Science\nNorthwestern Polytechnical University\nXi\u2019an, China\njayliu@mail.nwpu.edu.cn\nXuequn Shang\u2217\nSchool of Computer Science\nNorthwestern Polytechnical University\nXi\u2019an, China\nshang@nwpu.edu.cn\nXiaolin Han\nSchool of Computer Science\nNorthwestern Polytechnical University\nXi\u2019an, China\nxiaolinh@nwpu.edu.cn\nKai Zheng\nSchool of Electrical Engineering & Computer Science\nUniversity of Electronic Science and Technology of China\nChengdu, China\nzhengkai@uestc.edu.cn\nHongzhi Yin\u2217\nSchool of Electrical Engineering & Computer Science\nThe University of Queensland\nBrisbane, Australia\nh.yin1@uq.edu.au\nAbstract\u2014Anomaly detection in dynamic graphs presents a\nsignificant challenge due to the temporal evolution of graph struc-\ntures and attributes. The conventional approaches that tackle this\nproblem typically employ an unsupervised learning framework,\ncapturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing.\nHowever, these methods face critical drawbacks: they either\nonly depend on proxy tasks for representation without directly\npinpointing normal patterns, or they neglect to differentiate\nbetween spatial and temporal normality patterns. More recent\nmethods that use contrastive learning with negative sampling\nalso face high computational costs, limiting their scalability\nto large graphs. To address these challenges, we introduce a\nnovel Spatial-Temporal memories-enhanced graph autoencoder\n(STRIPE). Initially, STRIPE employs Graph Neural Networks\n(GNNs) and gated temporal convolution layers to extract spatial\nand temporal features. Then STRIPE incorporates separate\nspatial and temporal memory networks to capture and store\nprototypes of normal patterns, respectively. These stored patterns\nare retrieved and integrated with encoded graph embeddings\nthrough a mutual attention mechanism. Finally, the integrated\nfeatures are fed into the decoder to reconstruct the graph\nstreams which serve as the proxy task for anomaly detection.\nThis comprehensive approach not only minimizes reconstruction\nerrors but also emphasizes the compactness and distinctiveness of\nthe embeddings w.r.t. the nearest memory prototypes. Extensive\nexperiments on six benchmark datasets demonstrate the effec-\ntiveness and efficiency of STRIPE, where STRIPE significantly\noutperforms existing methods with 5.8% improvement in AUC\nscores and 4.62\u00d7 faster in training time.\nIndex Terms\u2014Anomaly Detection, Dynamic Graphs, Memory\nNetworks, Graph Autoencoder.\nI. INTRODUCTION\nReal-world networks are often modeled as dynamic graphs\nto capture the changing nature of objects and their inter-\nactions [1]\u2013[4]. Beyond basic topology structure and node\nattributes, dynamic graphs encompass rich temporal signals,\n*Corresponding authors.\nIn-degree\nOut-degree\nSnapshot #0\n0\n1\n2\n3\n0.64\n0.91\n3.12\n2.75\nfraudster\nbenign user\n20\nSnapshot\n2\nAvg degree\nfraudster\nbenign user\nFig. 1. Statistical observations of dynamic graphs on DGraph dataset. Left:\nAverage degrees of fraudsters and benign users on snapshot #0. Right: Degree\ncurves of fraudsters and benign users as time evolves. Anomalous samples\ntypically exhibit a higher frequency of vibrations, indicating that fraudsters\nfrequently change their connections to other nodes.\nsuch as evolving patterns in graph structure and node at-\ntributes [5]\u2013[7]. This temporal dimension provides an addi-\ntional perspective for analyzing anomalies. For instance, as\nillustrated in Fig. 1, anomalies may not be apparent when\nconsidering only the spatial information in a single snapshot,\ndue to the similar degrees of fraudsters and normal users.\nHowever, observing temporal changes in graph structures\ncan make abnormalities with higher vibration frequencies\ndistinctly noticeable.\nTo avoid ambiguity, in this paper, we define the structures\nand attributive features within individual graph snapshots as\nspatial information, while characterizing the evolving changes\nand trends among different snapshots as temporal information.\nTo model and integrate both the spatial and temporal signals\nof nodes and edges for anomaly detection, there has been a\ngrowing interest in the study of anomaly detection in dynamic\ngraphs [8], [9].\nDue to the challenge of annotating anomalous objects\nin real-world scenarios, anomaly detection approaches for\ndynamic graphs mostly employ an unsupervised learning\nframework [10]. The key intuition behind these methods\narXiv:2403.09039v2  [cs.LG]  15 Aug 2024\nis to develop a model that captures patterns of normality\nby exclusively incorporating normal data during the training\nphase. Subsequently, objects that the model fails to accurately\nrepresent in the testing phase are classified as anomalies. For\nexample, AddGraph [2] employs stacked GCN and GRU to\ncapture spatial and temporal representations of normal edges\nand train an edge anomaly detector with link prediction as\nthe proxy task. Then the edges with higher prediction errors\nin the test set are considered abnormal. TADDY [11] uses\na graph transformer to encode the representation of dynamic\ngraphs in the training phase. The anomalous edges are detected\nbased on a link prediction proxy task similar to [2] in the test\nphase. NetWalk [6] adopts a random-walk-based encoder to\nlearn node representations and measures the node anomaly\nscore by concerning its closest distance to the normal cluster\ncenters. MTHL [12] projects multi-view dynamic graphs into\na shared latent subspace and learns a compact hypersphere\nsurrounding normal samples. Node anomalies are detected\nbased on the distance to the learned hypersphere center.\nOCAN [13] captures the normal activity patterns of observed\nbenign users\u2019 attributes and detects fraudsters that behave\nsignificantly differently.\nDespite their success, these methods face several limitations:\n(1) Methods like AddGraph [2], TADDY [11], StrGNN [14]\nonly leverage proxy tasks (e.g., edge stream prediction) to\nderive feature representation, rather than directly identifying\nnormal patterns. This can be ineffective if anomalies share\nstructural or attributive similarities with normal data, due\nto the powerful representation learning models (e.g., GCN,\nTransformer) used. Consequently, abnormal data might also\nbe well-represented, resulting in suboptimal anomaly detec-\ntion (P1). (2) Approaches like NetWalk [6], MTHL [12],\nSAD [15] explicitly model normal patterns through clustering\nor hypersphere centers but do not distinguish between spatial\nand temporal patterns. However, as illustrated in Fig. 1, the\nprototypical pattern of nodes within a single snapshot can\nbe different from the pattern of evolving trends of nodes\namong different snapshots, necessitating separate identification\nand storage of spatial and temporal normal patterns (P2). (3)\nState-of-the-art methods such as SAD [15] and CLDG [16]\nemploy contrastive learning to build self-supervised frame-\nworks, relying on extensive negative sampling to prevent\nmodel collapsing. However, the generation and processing\nof a substantial number of negative samples in large-scale\ngraphs will bring high computational costs and reduce training\nefficiency (P3).\nIn order to address all the limitations above, we pro-\npose a novel Spatial-Temporal memoRIes enhanced graPh\nautoEncoder framework (STRIPE for abbreviation) for node\nanomaly detection in dynamic graphs. The key idea behind\nSTRIPE is to leverage two separate memory networks [17]\nto identify and preserve the spatial and temporal patterns\nof normality and integrate them with a graph autoencoder\nto reconstruct graph streams as the proxy task for anomaly\ndetection. Specifically, spatial and temporal node embeddings\nfrom input graph streams are derived using Graph Neural\nNetworks (GNNs) and gated temporal convolution, serving as\nthe spatial and temporal encoders, respectively. The spatial and\ntemporal patterns are then written into their respective memory\nbanks via a mutual attention mechanism on node embeddings,\nwith each memory item encapsulating a prototype of normal\nnode patterns. After that, the encoded spatial and temporal\nembeddings access the most closely related prototypes within\nthe memory through mutual attention-driven retrieval. These\nretrieved items are subsequently merged and combined with\nthe initial embeddings and fed into the decoder to reconstruct\nthe graph streams.\nDuring training, spatial and temporal memory items are\nupdated along with the encoder and decoder. We propose a\ncomprehensive training objective that minimizes both recon-\nstruction errors and compactness errors, promoting proximity\nbetween node embeddings and their nearest memory items.\nAdditionally, we also minimize separateness errors to enhance\nthe distinctiveness of memory items. This ensures STRIPE\u2019s\neffective use of a limited number of memory items, signifi-\ncantly reducing the size of the memory bank and enhancing\nmodel efficiency. Moreover, since STRIPE is non-contrastive\nlearning model, it requires no negative sampling or data\naugmentation, which also helps promote model scalability.\nIn the testing phase, the learned memory items remain fixed,\nand the reconstruction and compactness loss now serve as\nthe anomaly score. Since the reconstruction process integrates\nnormality patterns preserved in memory, the inputs that deviate\nfrom these prototypical patterns of normal data are likely\nto yield elevated anomaly scores, thereby facilitating their\nidentification as anomalies.\nIn summary, the main contributions of our work are as\nfollows:\n\u2022 We propose a novel spatial-temporal memory-enhanced\ngraph autoencoder framework, STRIPE, that explicitly\ncaptures normality patterns and integrates them into graph\nstream reconstruction for anomaly detection.\n\u2022 Considering the distinct normality patterns in spatial\nand temporal dimensions, we develop two independent\nmemory modules that can capture and preserve spatial\nand temporal patterns separately. To measure the complex\nrelations between node embeddings and diverse spatial\nand temporal memory items, we propose a mutual atten-\ntion mechanism to update and retrieve memory items.\n\u2022 We propose an efficient dynamic anomaly detection\nmodel by promoting separateness among memory items,\nthereby significantly reducing the size of memory bank.\n\u2022 Extensive experiments on six benchmark datasets have\ndemonstrated the state-of-the-art performance of STRIPE,\nachieving an average AUC score improvement of 5.8%.\nBoth theoretical analysis and empirical results highlight\nSTRIPE\u2019s efficiency, showing linear scalability with the\nincrease of node numbers.\nII. RELATED WORK\nIn this section, we introduce the works closely related to\nours: Anomaly Detection in static graphs, Anomaly Detection\nin Dynamic Graphs and Memory Networks.\nA. Anomaly Detection in static graphs\nGraph anomaly detection aims at identifying anomalous\ngraph objects (i.e., nodes, edges, and subgraphs) in the\ngraph [8], [9], [18]. Early anomalous node detection ap-\nproaches mainly use shallow techniques such as residual anal-\nysis (Radar [19]), matrix factorization (ALAD [20]), and CUR\ndecomposition (ANOMALOUS [21]) to extract anomalous\npattern in graphs. More recently, DOMINANT [22] pioneered\nthe integration of deep learning into node anomaly detection\nby employing a graph autoencoder [23] to reconstruct both\nthe structure and attribute information of graphs. CoLA [11]\nSL-GAD [24] and CONAD [25] further introduce graph con-\ntrastive learning that captures abnormal patterns by measuring\nagreement between augmented item pairs.\nB. Anomaly Detection in Dynamic Graphs\nRecently, the field of anomaly detection in dynamic graphs\nhas garnered significant attention, primarily because of its\ncapability to identify abnormalities in graphs that exhibit time-\nvarying characteristics.\nWithin dynamic networks, the definition of an anomalous\nobject varies widely depending on the specific application\ncontext. Based on the diverse nature of anomalies that can\noccur in such evolving structures, the scope of detection tasks\ncan range from identifying abnormal nodes [12], [26]\u2013[28] and\nedges [5], [29]\u2013[31] to pinpointing anomalous subgraphs [32],\n[33]. Early approaches mainly leverage the shallow mecha-\nnisms to detect anomalies in dynamic graphs. For example,\nCM-sketch [5] utilizes sketch-based approximation of struc-\ntural properties of the graph stream to calculate edge outlier\nscores. MTHL [12] distinguishes normal and anomalous nodes\naccording to their distances to the learned hypersphere center.\nSpotLight [33] guarantees a large mapped distance between\nanomalous and normal graphs in the sketch space with a\nrandomized sketching technique.\nMore recently, another branch of methods employs deep\nlearning techniques to capture anomalous objects in dynamic\ngraphs [2], [6], [11], [14]. NetWalk [6] utilizes a random\nwalk-based encoder to generate node embeddings and score\nthe abnormality of nodes and edges with their distance to\ncluster centers. AddGraph [2] employs stacked GCN and\nGRU to capture spatial and temporal representations of normal\nedges and train an edge anomaly detector with edge stream\nprediction as the proxy task. StrGNN [14] further extracts\nthe h-hop enclosing subgraph for each edge and employs\nstacked GCN and GRU to encode the extracted subgraphs for\nedge stream prediction. TADDY [34] learns the representations\nfrom dynamic graphs with coupled spatial-temporal patterns\nvia a transformer. SAD [15] and CLDG [16] introduce con-\ntrastive learning for anomaly detection in dynamic graphs.\nMost of the above approaches either only depend on proxy\ntasks for general representation without directly pinpointing\nnormal patterns, or they neglect to differentiate between spa-\ntial and temporal normality patterns, leading to diminished\nefficacy in anomaly detection. STRIPE alleviates this problem\nby capturing distinct spatial and temporal normality patterns\nin the training phase and integrating the preserved normality\npatterns to detect anomalies in the test phase.\nC. Memory Networks\nTo address the challenge of capturing long-term dependen-\ncies in temporal data, researchers recently proposed memory\nnetworks [17]. These networks can read and write to global\nmemories where individual items in the memory correspond\nto prototypical patterns of the features. MemN2N [35] fur-\nther enhances memory networks to operate in an end-to-end\nmanner, which reduces the need for layer-wise supervision\nduring training. Memory networks have shown effectiveness in\nvarious memorization tasks ranging from unsupervised feature\nlearning [36], [37], one-shot learning [38], [39], to image\ngeneration [40]. Recognizing the memory\u2019s ability to capture\nand store prototypical patterns of normal data, more recent\nstudies have started combining Autoencoders [41], [42] with\nmemory modules to detect anomalies in video [43], [44] and\ngraph [45] data.\nHowever, the focus of these methods has largely been on\nvideo or static graph data. Our work differs by applying\nmemory networks to dynamic graphs. We have developed\ndistinct spatial and temporal memory modules, which allow\nus to analyze normal prototypes in both spatial and temporal\ndimensions independently.\nIII. PRELIMINARIES\nIn this section, we provide the definitions of essential\nconcepts and formalize the problem of dynamic graph anomaly\ndetection.\nDefinition 1: Dynamic Graph. Given a dynamic graph with\noverall timestamps of T, we use G = {Gt}T\nt=1 to denote the\ngraph stream, where each Gt = {Vt, Et} is the snapshot at\ntimestamp t. Vt and Et is the node set and edge set at times-\ntamp t. An edge et\ni,j = (vt\ni, vt\nj) \u2208Et indicates the connection\nbetween node vt\ni and vt\nj at timestamp t, where vt\ni, vt\nj \u2208Vt.\nNt = |Vt| and Mt = |Et| indicate the number of nodes\nand edges in timestamp t. The structural information of Gt\nis represented by the graph adjacency matrix At \u2208RNt\u00d7Nt,\nwhere At\nij = 1 if et\nij exists, otherwise At\nij = 0. Xt \u2208RNt\u00d7D\ndenotes the node feature matrix at timestamp t and its i-th\nrow vector xt\ni \u2208RD represents the feature of node vt\ni.\nDefinition 2: Anomaly Detection in Dynamic Graphs.\nGiven a dynamic graph G = {Gt}T\nt=1, our goal is to lean an\nanomaly detection function f(\u00b7) : RNt\u00d7D \u2192RNt\u00d71 that can\nmeasure the degree of abnormality of the node by calculating\nits anomaly score f(vt\ni). A larger anomaly score indicates a\nhigher abnormal probability for vt\ni.\nGiven the challenges in obtaining and accessing anomalous\nlabels in real-world networks, we adopt an unsupervised\nsetting for detecting anomalies in dynamic graphs. In the\ntraining phase of this research, no node labels that indicate\nabnormalities are used. Instead, it is presumed that all nodes\npresent during training exhibit normal behavior. The binary\n\ud835\udc34\ud835\udc5b\ud835\udc5c\ud835\udc5a\ud835\udc4e\ud835\udc59\ud835\udc66 \n\ud835\udc46\ud835\udc50\ud835\udc5c\ud835\udc5f\ud835\udc52\n\ud835\udca2!\"#$%\n\ud835\udca2!\"%\nInput Graph Stream\n\u2026\nReconstructed Graph Stream\n\ud835\udca2!\n\u2026\n{\ud835\udc17&}!\"#$%\n!\n,\n{\ud835\udc00&}!\"#$%\n!\n\ud835\udf0f\n{(\ud835\udc17&}!\"#$%\n!\n{(\ud835\udc00&}!\"#$%\n!\n)\ud835\udca2!\"#$%\n)\ud835\udca2!\"%\n)\ud835\udca2!\nS-Memory Bank \ud835\udc0c'( \u2208\u211d\ud835\udf0f\u00d7\ud835\udc43\ud835\udc60\u00d7\ud835\udc37\u2032\nSpatial Memory Module\n\ud835\udf0f\u2032\nGated \nTemporal \nEncoder\n\ud835\udebd\ud835\udc52\ud835\udc5b\n\u2026\n\ud835\udc07!\"%\n\ud835\udc07!\"#$%\n\u2225\n\ud835\udc07!\n{\ud835\udc07}\ud835\udc61\u2212\ud835\udf0f+1\n\ud835\udc61\n\u2026\n\u2026\n\u2026\n\u2026\n\ud835\udc41!\n\ud835\udc371\nMemory \nupdate\n\ud835\udc212\n&\nMemory \nread\n2\ud835\udc262\n&\nT-Memory Bank \ud835\udc0c\ud835\udc95\ud835\udc91\u2208\u211d5#\u00d76$\nTemporal Memory Module\nMemory \nread\n\u2026\n2\ud835\udc262\n!\nGNN \nEncoder\n\ud835\udeaf#$ \nMemory \nupdate\n\ud835\udc332\nGated \nTemporal \nDecoder\n\ud835\udebd%#\nAttribute \nDecoder\n\ud835\udeaf78 \nStructure \nDecoder\n\ud835\udc1678 \n\u2026\n(\ud835\udc07!\"%\n(\ud835\udc07!\"#$%\n(\ud835\udc07!\nMLP\n\ud835\udc41!\n(\ud835\udc0c'(\n\ud835\udc19(:)\n\ud835\udc371\n(\ud835\udc0c!(\n\ud835\udc19(:)\n8\ud835\udc19\nCompactness loss\nReconstruction loss\nFig. 2. Overall framework of the proposed STRIPE.\nlabels denoting abnormality are introduced only in the testing\nphase to assess the model\u2019s effectiveness. In this context, a\nlabel yvt\ni = 0 signifies that the node vt\ni is considered normal,\nwhereas yvt\ni = 1 identifies it as abnormal.\nIV. METHODOLOGY\nIn this section, we present our proposed framework,\nSTRIPE, designed for node anomaly detection within dynamic\ngraphs in an unsupervised manner. As depicted in Fig. 2,\nSTRIPE is comprised of four key components: (1) Spatial-\ntemporal graph encoder that encodes both the spatial and\ntemporal information of input graph stream into comprehen-\nsive node embeddings. (2) Spatial-temporal memory learning\nthat captures and stores the prototypical patterns of normal\nnode representations at both spatial and temporal dimensions.\n(3) Spatial-temporal graph decoder that reconstructs the orig-\ninal graph stream using the latent node embeddings and the\nidentified normal prototypes, facilitating the comparison with\nthe original input. (4) Unified anomaly detector that measures\nthe abnormality of a node by calculating the reconstruction\nerrors between the original and reconstructed graphs and the\ncompactness errors between the node and its nearest prototype.\nIn the rest of this section, we introduce the four components\nin detail from section IV-A to V-D. The overall pipeline of\nSTRIPE is illustrated in Algorithm (1).\nA. Spatial-Temporal Graph Encoder\nThe input dynamic graph contains not only structural and\nattributive information within each graph snapshot but also\nabundant temporal information illustrating the evolution along-\nside the graph stream. Capturing both the spatial and temporal\nproperties of dynamic graphs is essential for detecting anoma-\nlies. To address this challenge, we design a spatial-temporal\nencoder that consists of a spatial encoder and a temporal\nencoder.\n1) Spatial Encoder: GNNs have recently emerged as one of\nthe most powerful network representation learning approaches\ndue to their ability to conduct deep learning on non-Euclidean\ngraph data. In this work, we employ an L-layer GNN as the\nspatial encoder. Instead of inputting the whole graph stream at\na time, we consider a graph sequence {Gt\u2212\u03c4+1, . . . , Gt} over\na time window size \u03c4. By adjusting the hyper-parameter \u03c4, we\ncan specify the receptive fields along the time axis.\nThe\nspatial\nencoder\ntakes\nthe\ngraph\nsequence\n{Gt\u2212\u03c4+1, . . . , Gt} as input and outputs the latent node\nembeddings Ht \u2208RNt\u00d7D\u2032 for each snapshot Gt. Specifically,\nnode embeddings in Gt are computed as follows:\nH(t,l) = GNN\n\u0010\nAt, H(t,l\u22121); \u0398(l)\nen\n\u0011\n,\n(1)\nwhere \u0398(l)\nen \u2208RD\u00d7D\u2032 denotes the learnable weight parameters\nof the l-th layer GNN. H(t,l\u22121) and H(t,l) are the node\nrepresentation matrices learned by the (l \u22121)-th and (l)-th\nlayer, respectively. H(t,0) is Xt. GNN\u03b8(\u00b7) can be set as any off-\nthe-shelf graph neural networks. For computation efficiency,\nwe adopt a two-layer graph convolutional network (GCN) as\nthe backbone. Thus, Equation (1) can be specifically re-written\nas:\nH(t,l) = ReLU\n\u0010\neD\n\u22121\n2\nt\neAt eD\n\u22121\n2\nt\nH(t,l\u22121)\u0398(l)\nen\n\u0011\n,\n(2)\nwhere eAt = At +IN t, and eDt is a diagonal node degree ma-\ntrix where eDt(i, i) = P\nj eAt(i, j). ReLU(\u00b7) is the activation\nfunction. We simplify H(t,L) as Ht.\n2) Gated Temporal Encoder: Having calculated the node\nembeddings for each graph snapshot, we next incorporate the\ntemporal dependencies observed across different snapshots.\nPrior research [2], [14] predominantly employed Recurrent\nNeural Networks (RNNs) for learning temporal information.\nThe inherent limitation of RNNs, however, is their sequential\nprocessing requirement for each time step, which significantly\nincreases computational costs and reduces the efficiency of the\nmodel. To overcome this challenge, we utilize gated temporal\nconvolution for temporal learning, which facilitates the parallel\nprocessing of elements within the graph sequence.\nFor gated temporal convolution, we employ a 1-dimensional\nconvolution with a kernel width of Kt to capture dynamic\nevolving between timestamps t \u2212\u03c4 + 1 and t of the graph\nsequence {Gt\u2212\u03c4+1, . . . , Gt}. Since the node embeddings from\neach snapshot influence anomaly detection differently [14], we\nincorporate a Gated Linear Unit (GLU) subsequent to the 1-\ndimensional convolution layer, which serves to accentuate crit-\nical information more significantly associated with anomaly\ndetection.\nSpecifically, given Z(0) = {H}t\nt\u2212\u03c4+1 \u2208R\u03c4\u00d7Nt\u00d7D\u2032 as\ninput, the gated temporal convolution is defined as follows:\nZ(l) = tanh(E1) \u2299\u03c3(E2),\n(3)\nE1 = E(l\u22121)[:, 1 : D\u2032],\n(4)\nE2 = E(l\u22121)[:, D\u2032 + 1 : 2D\u2032],\n(5)\nE(l\u22121) = Z(l\u22121)\u03a6(l)\nen.\n(6)\nWhere \u03a6(l)\nen \u2208RD\u2032\u00d72D\u2032 is the weight parameter of the 1-\ndimensional convolution kernel, \u2299denotes the element-wise\nmultiplication. \u03c3(E) =\n1\n1+e\u2212E denotes the logistic sigmoid.\nAfter stacking L-layer of gated temporal convolution, the\nlength of the graph sequence is reduced to \u03c4 \u2032 = \u03c4 \u2212L \u00d7\n(Kt \u22121). We simplify Z(L) \u2208R\u03c4 \u2032\u00d7Nt\u00d7D\u2032 as Z.\nB. Spatial-Temporal Memory Learning\nThe spatial-temporal memory learning aims to capture and\nstore prototypical spatial patterns and temporal patterns of\nnormal node embeddings. Spatial and temporal memory banks\ncontain \u03c4Ps and Pt memory items of dimension D\u2032, denoted\nas Msp \u2208R\u03c4\u00d7Ps\u00d7D\u2032 and Mtp \u2208RPt\u00d7D\u2032, respectively. In\nthe rest of this section, we introduce the spatial and temporal\nmemory modules, respectively.\n1) Spatial Memory Module: We denote mr\np \u2208RD\u2032(p =\n1, . . . , Ps; r = t \u2212\u03c4 + 1, . . . , t) as the item of memory Msp,\nand hr\ni \u2208RD\u2032(i = 1, . . . , Nt; r = t \u2212\u03c4 + 1, . . . , t) as the\nspatial encoded feature of node i at time r.\nMemory Read.. As shown in Fig. 3(a), the reading process\nbegins by calculating the attention weights between each node\nfeature hr\ni and all memory items mr\np. Prior research [45], [46]\nprimarily adopts cosine similarity to compute self-attention,\nwhich restricts the capability to explore the relations between\nnode features and diverse spatial and temporal memory items.\nTo address this problem, we employ a mutual attention mech-\nanism:\nkp = mr\npWK,\n(7)\nqi = hr\ni WQ,\n(8)\nvi = hr\ni WV ,\n(9)\nwhere WK, WQ and WV \u2208RD\u2032\u00d7D\u2032 are weight matrices\nof key vector, query vector, and value vector, respectively.\nThe attention weights w(i,p) are then computed with softmax\nfunction:\nw(i,p) =\nexp\n\u0000qi(kp)T \u00b7\n1\n\u221a\nD\u2032\n\u0001\nPPs\np\u2032=1 exp\n\u0000qi(kp\u2032)T \u00b7\n1\n\u221a\nD\u2032\n\u0001.\n(10)\nFor each node feature hr\ni , we read the memory by a weighted\naverage of the items mr\np with the corresponding weights w(i,p),\nand obtain the readout memory item \u02c6mr\ni \u2208RD\u2032 as follows:\n\u02c6mr\ni =\nPs\nX\np=1\nw(i,p)mr\np,\n(11)\nAssigning this procedure to all i \u2208[1, Nt] and r \u2208[t\u2212\u03c4 +1, t],\nwe obtain the readout memory matrix \u02c6Msp \u2208R\u03c4\u00d7Nt\u00d7D\u2032.\nMemory Update. During the training phase, the memory\nbank will also be updated to record the spatial prototypes of\nnormal nodes. As shown in Fig. 3(b), for each memory item\nmr\np, we select the node features that are nearest to it based on\nthe matching weights \u00b5(i,p) as follows:\n\u00b5(i,p) =\nexp\n\u0000qi(kp)T \u00b7\n1\n\u221a\nD\u2032\n\u0001\nP\u03c4 \u2032Nt\ni\u2032=1 exp\n\u0000qi\u2032(kp)T \u00b7\n1\n\u221a\nD\u2032\n\u0001.\n(12)\nContrary to [44], [45], which utilizes all features for updating\nmemory items, we selectively employ only the top-K relevant\nfeatures. This strategy effectively filters out irrelevant, noisy\nnodes, thereby capturing and recording the general patterns of\nnormal events more effectively. Therefore, the top-K values\nof \u00b5(i,p) are preserved, while the remainder is nullified to 0.\nThe updated memory item mp is then calculated as follows:\nmr\np \u2190mr\np +\nK\nX\ni=1\n\u00b5(i,p)vi.\n(13)\n2) Temporal Memory Module: Unlike spatial memories that\ncapture normal patterns within each graph snapshot, temporal\nmemories aim to characterize the prototypical pattern of evolv-\ning trends among different snapshots. Specifically, mq \u2208RD\u2032\nis the item of memory bank Mtp. For each temporal node\nfeature zi \u2208RD\u2032(i = 1, . . . , \u03c4 \u2032 \u00d7 Nt), its corresponding\nreadout memory \u02c6mi is calculated as follows:\n\u02c6mi =\nPt\nX\nq=1\nw(i,q)mq.\n(14)\nWe adopt the same memory read procedure as in Eq. (10)\nto calculate w(i,q) and utilize Eq. (12) to update temporal\nmemories. The overall readout matrix for temporal features is\ndenoted as \u02c6Msp \u2208R\u03c4\u00d7Nt\u00d7D\u2032. Each item within these matrices\nrepresents the averaged spatial and temporal normal prototypes\nassociated with the corresponding node.\n\ud835\udc21\ud835\udc56\n\ud835\udc5f\n\ud835\udc261\n\ud835\udc5f\ud835\udc26$\n%\n\ud835\udc26&!\n%\n\u2026\nT\n\u2211\nSoftmax\n#\ud835\udc26\ud835\udc8a\n\ud835\udc93\n \n\ud835\udc16*\n\ud835\udc16+\n(a) memory read.\n\ud835\udc21!\n\"\ud835\udc21#\n\"\n\ud835\udc21$!\n\"\n\u2026\nT\n\u2211\nSoftmax\n\ud835\udc26%\"\n\ud835\udc16&\n\ud835\udc16'\n\ud835\udc61\ud835\udc5c\ud835\udc5d\u2212\ud835\udc58\n\ud835\udc26%\"\n\ud835\udc16(\n(b) memory update.\nFig. 3. The illustration of (a) memory read and (b) memory update procedures\nin the spatial memory module.\nC. Spatial-temporal Graph Decoder\nIn this section, we reconstruct the original graph stream with\nthe averaged memory matrices and latent representation Z. As\nshown in Fig. 2, our method inputs the concatenation of \u02c6Mtp\nand Z into the gated temporal decoder, which is the reverse\nprocess of equation (3), and outputs \u02c6Z \u2208R\u03c4\u00d7Nt\u00d7D\u2032. Then we\nconcatenate \u02c6Msp with \u02c6Z and input it into a one-layer MLP to\nobtain { \u02c6H}t\nt\u2212\u03c4+1:\n\u02c6Ht = MLP\n\u0000 \u02c6Msp[t, :, :] \u2225\u02c6Z[t, :, :]\n\u0001\n,\n(15)\nwhere \u2225denotes concatenation operation and \u02c6Ht \u2208RNt\u00d7D\u2032.\nThen, we use an L-layer GCN as the attributive decoder to\nreconstruct attribute matrix \u02c6Xt:\n\u02c6Xt = GCN\n\u0010\nAt, \u02c6Ht; \u0398(l)\nde\n\u0011\n,\n(16)\nWe employ the inner product of \u02c6Ht as the structural decoder,\nformatted as follows:\n\u02c6At = \u03c3\n\u0010\n\u02c6HtWde\n\u0000 \u02c6Ht\u0001T \u0011\n.\n(17)\nWhere Wde \u2208RD\u2032\u00d7D\u2032 is the weight matrix and \u03c3(\u00b7) denotes\nthe Sigmoid activation function.\nD. Unified Anomaly Detector\nIn the previous sections, we have calculated the spatial and\ntemporal prototypes and integrated them with latent represen-\ntations to reconstruct the original graph stream. For a training\ntimestamp t, the reconstruction errors can be formatted as a\ncombination of attributive and structural reconstruction errors:\nLrec = \u03b1\u2225\u02c6Xt \u2212Xt\u22252 + (1 \u2212\u03b1)\u2225\u02c6At \u2212At\u22252,\n(18)\nwhere \u03b1 \u2208[0, 1] is a hyper-parameter that balances the\nimportance of attributive and structural errors.\nGiven that only normal nodes are present during the training\nphase, under ideal circumstances, the features of a normal node\nshould be close to the nearest item in the memory. Conversely,\nthe features of abnormal nodes are expected to be distant\nfrom any memory items, reflecting their deviation from normal\npatterns. Encouraged by this, the feature compactness loss,\ndenoted by Lcom, is as follows:\nLcom =\n\u03c4Nt\nX\ni=1\n\u2225hi \u2212msp\np \u22252 +\n\u03c4 \u2032Nt\nX\ni=1\n\u2225zi \u2212mtp\np \u22252,\n(19)\nwhere msp\np and mtp\np denote the spatial and temporal memory\nitem that is nearest to spatial embedding hi and temporal\nembedding zi, respectively.\nFurthermore, the items within the memory should be suf-\nficiently distant from each other. This spacing ensures that\na broad spectrum of normal data patterns can be effectively\ncaptured and represented. Encouraged by this, we design the\nmemory separateness loss, denoted as Lsep, as follows:\nLsep =\n\u03c4Nt\nX\ni=1\n\u0002\n\u2225hi \u2212msp\np \u22252 \u2212\u2225hi \u2212msp\nn \u22252\n\u0003\n+\n+\n\u03c4 \u2032Nt\nX\ni=1\n\u0002\n\u2225zi \u2212mtp\np \u22252 \u2212\u2225zi \u2212mtp\nn \u22252\n\u0003\n+,\n(20)\nwhere msp\np and msp\nn denotes the nearest and the second nearest\nmemory item to hi. mtp\np and mtp\nn denote the nearest and the\nsecond nearest memory item to zi. The total loss for training\nis formatted as:\nL = Lrec + Lcom + Lsep.\n(21)\nDuring inference, the sum of Lrec and Lcom is adopted as the\nanomaly score. The anomaly score for each node is calculated\nR times to ensure that the final anomaly scores are statistically\nstable and reliable.\nE. Complexity Analysis\nIn this subsection, we analyze the time complexity of each\ncomponent in the STRIPE framework. We employ an L-\nlayer GCN for spatial encoding and decoding of a graph\nsequence with window size \u03c4, which brings a complexity\nof O(\u03c4LMtD\u2032 + \u03c4LNtD\u20322), where Mt and Nt are the\naveraged edge and node number for each snapshot. For\nmemory read and update, the complexity is mainly caused by\nthe mutual attention mechanism between node features and\nmemory items, which is O(\u03c4NtPtD\u2032). The time complexity\nfor an L-layer gated temporal convolution is O(\u03c4LNtD\u20322).\nTherefore, to apply an L-layer STRIPE to a graph sequence of\nwindow size \u03c4, the overall time complexity is O\n\u0000\u03c4L(MtD\u2032 +\nNtD\u20322) + \u03c4NtPtD\u2032)\n\u0001\n. As investigated in section V-H2, Pt\ncan be restricted to a very limited number, thereby the time\ncomplexity of STRIPE is approximately linear to node number.\nSection V-F provide an empirical analysis of model efficiency.\nV. EXPERIMENTAL STUDY\nIn this section, we conduct extensive experiments on six\nreal-world benchmark datasets to evaluate the performance of\nSTRIPE. Specifically, from sectionV-A to V-C, we introduce\nthe experimental setups. Then in section V-D and V-E, we\ncompare our method with the state-of-the-art baseline methods\non node anomaly detection task. We then evaluate time effi-\nciency of the model in section V-F and conduct ablation study\nto validate the effectiveness of each component of STRIPE\nin section V-G. In section V-H, we study the parameter\nsensitivity to further investigate the property of STRIPE. We\nalso demonstrate the effectiveness of the proposed spatial and\ntemporal memory modules with a case study in V-I.\nAlgorithm 1: Forward propagation of STRIPE\nInput: Graph stream {Gt}T\nt=1; Number of training\nepochs I; Time window size \u03c4; Temporal\nconvolution kernel width Kt; Evaluation\nrounds R.\nOutput: Anomaly scoring function f(\u00b7).\n1 Randomly initialize the trainable parameters of the\nencoder, decoder, memory modules, and scoring\nfunction;\n/* Training stage\n*/\n2 for i \u22081, 2, . . . , I do\n3\nfor snapshot Gt \u2208{Gt}T\nt=1 do\n4\nExtract the graph sequence {Gt\u2212\u03c4+1, . . . , Gt};\n5\nCalculate spatial node embeddings Ht via Eq.\n(1);\n6\nCalculate temporal node embeddings Z via Eq.\n(3);\n7\nfor p \u22081, 2, . . . , P do\n8\nCalculate the read attention weights w(i,p)\nvia Eq. (10) and update attention weights\n\u00b5(i,p) via Eq. (12);\n9\nend\n10\nReadout the averaged memories via Eq. (14);\n11\nUpdate the memory items via Eq. (13);\n12\nCalculate the reconstructed attributes \u02c6Xt and\nstructures \u02c6At via Eq. (16) and Eq. (17);\n13\nCompute the loss objective via Eq. (18), (19),\nand (20).\n14\nend\n15 end\n/* Inference Stage\n*/\n16 for r \u22081, 2, . . . , R do\n17\nfor vi \u2208{Vt}T\nt=1 do\n18\nCalculate the anomaly score for each node vi\nvia Lrec and Lcom.\n19\nend\n20 end\nA. Datasets\nWe assess the performance of STRIPE and its competitors\non six real-world temporal networks. The description of the\ndatasets is shown in Table I. Among them, DBLP-3 and\nDBLP-5* are co-author networks that consist of authors as\nnodes and co-authorship as edges, with the node features being\nabstracts of the author\u2019s publications during certain period\nencoded by word2vec. The authors in DBLP-3 and DBLP-5\nare from three and five research areas, respectively. Reddit\u2020 is\na social network where the nodes represent the posts and edges\nare defined through similar keywords. Word2vec is applied to\nthe comments of a post to generate its node attributes. Brain\nis a biological network, with nodes symbolizing distinct cubes\n*https://dblp.uni-trier.de\n\u2020https://www.reddit.com/\nTABLE I\nSTATISTICS OF THE DATASETS. AR REPRESENTS THE ANOMALY RATIO,\nCALCULATED AS THE RATIO OF THE NUMBER OF ANOMALIES TO THE\nTOTAL NUMBER OF NODES.\nDatasets\nNodes\nEdges\nAttributes Timestamps Anomalies\nDBLP-3\n4,257\n38,240\n100\n10\n210\nDBLP-5\n6,606\n65,915\n100\n10\n330\nReddit\n8,291\n292,400\n20\n10\n420\nBrain\n5,000\n1,975,648\n20\n12\n240\nBitcoin-OTC\n6,005\n355,096\n32\n138\n300\nDGraph\n3,700,550 4,300,999\n17\n821\n15,509\nof brain tissue and edges reflecting their connectivity.\nConnectivity between two nodes is established if they\nexhibit a similar level of activation during the observed time\nframe. BitcoinOTC is a who-trusts-whom network of bitcoin\nusers trading on the platforms www.bitcoinotc.com. Nodes\nrepresent the users from the platform, and an edge appears\nwhen one user rates another on the platform. DGraph [47]\nis a large-scale financial network, showcasing registered users\nas nodes, emergency contact relationships as edges, and 17\nattributes from users\u2019 personal profiles as node features.\nSince only DGraph has ground-truth labels for anomalies,\nwe manually inject synthetic anomalies into the other five\ndatasets for evaluation in the testing phase. For a fair compar-\nison, we follow the anomaly injection strategies used in [22]\nand [11], and inject equal numbers of structural anomalies and\nattributive anomalies for each snapshot Gt in the test set:\n\u2022 Structural anomaly injection. Following [22], we gener-\nate structural anomalies by randomly selecting Np nodes\nfrom node set Vt and connecting them to form fully\nconnected cliques. The selected Np nodes are labeled as\nstructural anomaly nodes. This process is repeated q times\nto generate q cliques.\n\u2022 Attributive anomaly injection. Following [11], attribu-\ntive anomalies are created by randomly selecting Np \u00d7 q\nnodes from Vt. For each chosen node vt\ni, we sample an\nadditional k nodes to form a candidate set: Vt\ni,attr =\n{vt\n1, . . . , vt\nk}. Then, we replace the feature vector of vi\nwith the node feature from Vt\ni,attr that has the largest\nattribute distance from vt\ni. Following [11], we set k=50\nfor all the datasets.\nB. Baselines\nTo validate the effectiveness of STRIPE, we conducted a\ncomparative analysis with nine state-of-the-art node anomaly\ndetection baselines. This comparison includes five dynamic\nnode anomaly detection methods: NetWalk [6], MTHL [12],\nTADDY [34], SAD [15] and CLDG [16]. Given the limited\nnumber of dynamic node anomaly detection baselines, we also\nincorporate four of the most advanced static node anomaly\ndetection methods in our comparison: DOMINANT [22],\nCoLA [11], SL-GAD [24] and GRADATE [48]. Details of\nthese baselines are introduced as follows:\nStatic node anomaly detection methods:\nTABLE II\nNODE ANOMALY DETECTION PERFORMANCE ON SIX BENCHMARK DATASETS, THE BEST AND SECOND TO BEST RESULTS ON EACH DATASET ARE IN\nBOLD AND UNDERLINED, RESPECTIVELY. PRE, F1, AND AUC REPRESENT THE PRECISION, MACRO-F1, AND AREA UNDER THE CURVE, RESPECTIVELY.\nDataset\nDBLP-3\nDBLP-5\nReddit\nMetrics\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nDOMINANT\n0.1358\n0.5490\n0.6994\n0.5431\n0.7327\n0.9154\n0.4912\n0.6326\n0.9316\nCoLA\n0.4753\n0.4874\n0.5814\n0.4750\n0.4872\n0.4806\n0.4747\n0.4870\n0.2410\nSL-GAD\n0.5302\n0.5193\n0.6174\n0.5229\n0.5038\n0.7638\n0.4908\n0.4844\n0.6843\nGRADATE\n0.5126\n0.4874\n0.5581\n0.4923\n0.4875\n0.4726\n0.5231\n0.4870\n0.5823\nNetWalk\n0.5336\n0.5092\n0.7126\n0.5805\n0.5034\n0.9107\n0.5606\n0.5726\n0.7821\nMTHL\n0.5758\n0.5077\n0.5901\n0.5433\n0.4951\n0.7382\n0.6295\n0.6279\n0.7074\nTADDY\n0.4994\n0.5124\n0.6425\n0.4994\n0.5135\n0.6878\n0.5624\n0.5246\n0.7390\nSAD\n0.7044\n0.7207\n0.8973\n0.4931\n0.3523\n0.4535\n0.5198\n0.5219\n0.8551\nCLDG\n0.7225\n0.7188\n0.8882\n0.7208\n0.7021\n0.8781\n0.6819\n0.6601\n0.8348\nSTRIPE\n0.7622\n0.7972\n0.9620\n0.7359\n0.8020\n0.9765\n0.9409\n0.6849\n0.9810\nDataset\nBrain\nBitcoin-OTC\nDGraph\nMetrics\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nDOMINANT\n0.5845\n0.5958\n0.8212\n0.5157\n0.5062\n0.9079\n0.4985\n0.4914\n0.5709\nCoLA\n0.4760\n0.4877\n0.5716\n0.4750\n0.4867\n0.7117\n0.3312\n0.3893\n0.4361\nSL-GAD\n0.6640\n0.6347\n0.8735\n0.4750\n0.4871\n0.9356\nOOM\nOOM\nOOM\nGRADATE\n0.4910\n0.4871\n0.5629\n0.4971\n0.4868\n0.7592\nOOM\nOOM\nOOM\nNetWalk\n0.6239\n0.6643\n0.8590\n0.6667\n0.6324\n0.9504\nOOM\nOOM\nOOM\nMTHL\n0.5843\n0.5988\n0.8119\n0.6727\n0.6410\n0.9353\nOOM\nOOM\nOOM\nTADDY\n0.6218\n0.6365\n0.7235\n0.6552\n0.6821\n0.9566\n0.5940\n0.5477\n0.6654\nSAD\n0.5266\n0.5065\n0.5638\n0.5733\n0.7277\n0.6341\n0.4248\n0.6136\n0.7312\nCLDG\n0.6372\n0.6425\n0.5928\n0.7315\n0.7544\n0.8394\n0.6210\n0.6018\n0.6528\nSTRIPE\n0.6919\n0.7144\n0.9389\n0.7579\n0.8259\n0.9952\n0.6451\n0.6514\n0.7526\n\u2022 DOMINANT [22] is a deep graph autoencoder-based\nunsupervised method that detects node anomalies by\nassessing the reconstruction errors of individual nodes.\n\u2022 CoLA [11] is a contrastive learning based anomaly\ndetection method that captures node anomaly patterns\nby measuring the agreement between each node and its\ncontextual subgraph using a GNN-based encoder.\n\u2022 SL-GAD [24] is a self-supervised anomaly detection\nmethod that combines both attribute reconstruction and\ncontrastive learning for detecting node anomalies.\n\u2022 GRADATE [48] is an extension of CoLA by conducting\ncontrastive learning not only between node-node and\nnode-subgraph pairs, but also from subgraph-subgraph\npairs.\nDynamic node anomaly detection methods:\n\u2022 NetWalk [6] uses an autoencoder to update dynamic node\nrepresentations and applies streaming k-means clustering\nfor real-time node categorization. Anomaly scores are\ncalculated based on node proximity to cluster centers.\n\u2022 MTHL [12] learns a compact hypersphere surrounding\nnormal node representations and then distinguishes nor-\nmal and abnormal nodes according to their distances to\nthe hypersphere center.\n\u2022 TADDY [34] develops coupled spatial-temporal patterns\nvia a dynamic graph transformer to detect anomalies in\ngraph streams.\n\u2022 SAD [15] is a semi-supervised dynamic graph anomaly\ndetection model that combines memory networks with\npseudo-label contrastive learning.\n\u2022 CLDG [16] is a contrastive learning model that per-\nforms representation learning on both discrete-time and\ncontinuous-time dynamic graphs.\nC. Parameter Settings\nAll the parameters can be tuned by 5-fold cross-validation\non a rolling basis. We set the time window size \u03c4 as 3 and\ntemporal convolution kernel width Kt as 2. Both the GCN and\ngated temporal convolution encoders/decoders have 2 layers\nwith hidden dimensions set as 32 for Brain and 128 for the\nremaining datasets. Balance factor \u03b1 is set as 0.9 for Brain\nand Reddit, and 0.3 for DBLP-3, DBLP-5 and OTC. The\ntraining epoch is 20 and the learning rate is 0.001 for all the\ndatasets. Evaluation round R is 40 for Brain and 20 for the\nother datasets. The number of spatial and temporal memory\nitems Ps and Pt are both set as 6. For the evaluation of static\ngraph anomaly detection baselines, we first train these methods\nand measure the anomaly scores for each graph snapshot, then\nwe derive the final score for evaluation by averaging these\nanomaly scores across all snapshots.\nD. Comparison with the State-of-the-art Baselines\nTo evaluate the effectiveness of STRIPE on dynamic node\nanomaly detection, we compare its performance with six state-\nof-the-art baselines on four benchmark datasets. The precision,\nmacro-F1 and AUC values are presented in Table II. All the\ndifferences between our model and others are statistically\n0.01\n0.02\n0.03\n0.04\n0.05\nAnomaly Rate\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUC\nDBLP-3\nSTRIPE\nDOMINANT\nNetWalk\nSL-GAD\nTADDY\nCLDG\nSAD\n0.01\n0.02\n0.03\n0.04\n0.05\nAnomaly Rate\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAUC\nReddit\nSTRIPE\nDOMINANT\nNetWalk\nSL-GAD\nTADDY\nCLDG\nSAD\nFig. 4. Anomaly detection under different anomaly rates on DBLP-3 (left)\nand Reddit (right) datasets.\nsignificant (p < 0.01). According to these results, we have\nthe following findings:\n\u2022 The proposed STRIPE consistently outperforms all the\nbaselines on the four dynamic graph datasets, showcas-\ning its superiority in dynamic node anomaly detection.\nCompared with the most competitive baselines, STRIPE\nachieves a significant performance gain of 9.5% on pre-\ncision, 7.8% on macro-F1, and 5.8% on AUC, averagely.\nThis validates the overall design of our proposed STRIPE\nmodel.\n\u2022 Compared to static anomaly detection models (DOMI-\nNANT, CoLA, SL-GAD, and GRADATE), STRIPE in-\ntegrates interactions across different timestamps through\ngated temporal convolution, enabling it to capture the\ndynamic evolution among graphs. Consequently, STRIPE\nlearns more comprehensive embeddings for dynamic\nanomaly detection and achieves better performance.\n\u2022 In comparison with most competitive dynamic node\nanomaly detection methods such as CLDG and SAD,\nSTRIPE achieves 5.8% average performance gain on\nAUC. We attribute this performance advantage to our pro-\nposed prototype-enhanced reconstruction strategy. Unlike\nNetWalk [6] and MTHL [12] that identify abnormalities\nbased on their distances to cluster centers, our model\ncircumvents the dependence on the selection of clus-\ntering techniques and methods for calculating distance.\nInstead, it directly assesses abnormalities through the\nreconstruction errors between the original graphs and\ntheir reconstructions, which are improved by prototypes.\nThis eliminates potential biases introduced by specific\nclustering or distance computation methodologies.\nE. Performance under Different Anomaly Rates\nTo further evaluate the robustness of the proposed model,\nwe further compare the performance of STRIPE with other\nbaselines under different anomaly rates. The anomaly rate\nis the proportion of anomalous nodes to the total number\nof nodes, which can be changed by varying the number of\ninjected anomalies q to synthetic datasets. We vary anomaly\nrate from 0.01 to 0.05 and the result is reported in Fig. 4. We\ncan observe that (1) STRIPE consistently outperforms other\n500,000\n1,000,000\nNumber of nodes\n25\n50\n75\n100\n125\nTime (s)\nTrain\nInference\nSTRIPE CLDG TADDY SAD\n0\n1.0\n2.0\n3.0\nTime per node (ms)\n0.0515\n0.018\n0.1507\nTrain\nInference\nFig. 5.\nEvaluation of time efficiency. Left: The linear increase of both\ntraining and inference time of STRIPE w.r.t. node numbers in DGraph dataset.\nRight: Comparison of training and inference time of STRIPE with three most\ncompetitive baselines on DGraph dataset.\nbaselines across all the anomaly rates, which demonstrate the\nrobustness and generalization of our model. (2) STRIPE gen-\nerally has larger performance gain over other baselines under\nhigher anomaly rates, and only has marginal performance drop\ncompared to baselines such as CLDG or SL-GAD.\nF. Model Efficiency Evaluation\nIn this subsection, we provide experimental analysis of\nthe computational efficiency of the proposed STRIPE. We\nmeasure the running time per epoch of STRIPE on the DGraph\ndataset while varying the number of nodes. As depicted in\nthe left subplot of Fig. 5, both the training and inference\ntime is linear w.r.t. node numbers, which aligns with our\nanalysis in section IV-E. Additionally, we compare the time\nefficiency of STRIPE with baselines that acquire AUC scores\nover 60% on DGraph dataset, as shown in the right subplot of\nFig. 5. Compared to CLDG, the second most accurate model\nfollowing STRIPE, despite STRIPE is 2.86\u00d7 slower than\nCLDG during inference, STRIPE is 4.62\u00d7 faster in training\nand demonstrates a significant improvement of 5.8% in AUC\nscores compared to CLDG. Furthermore, STRIPE is 23.76\u00d7\nfaster in training and 56.8\u00d7 faster in inference compared to\nthe second most accurate baseline, SAD.\nG. Ablation Study\nTo further investigate the contribution of each component\nin STRIPE, we perform an ablation study in this section.\nThe results are shown in Table III. We set five variants of\nSTRIPE: w/o attribute, w/o structure, w/o temporary, w/o s-\nprototype, w/o t-prototype and STRIPE. Among them, w/o\nattribute excludes the attributive reconstruction error by setting\n\u03b1=0. w/o structure excludes the structural reconstruction error\nby setting \u03b1=1. w/o temporary excludes the gated temporal\nconvolution component and directly applies static version of\nSTRIPE on each graph snapshot and computes the average\nanomaly scores across all the snapshots. w/o s-prototype and\nw/o t-prototype exclude the spatial and temporal memory\nmodules, respectively, and reconstructs graphs without fusing\nprototypical patterns.\nThe comparison between w/o attribute, w/o structure, and\nSTRIPE, as illustrated in Table III, reveals a decline in perfor-\nmance when either attributive or structural reconstruction error\nTABLE III\nQUANTITATIVE RESULTS W.R.T. PRECISION, RECALL AND AUC FOR ABLATION STUDY\nDataset\nDBLP-3\nDBLP-5\nReddit\nBrain\nMetrics\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nPRE\nF1\nAUC\nw/o attribute\n0.7583\n0.7688\n0.9513\n0.7039\n0.7814\n0.9462\n0.5042\n0.5011\n0.8963\n0.5744\n0.5478\n0.7215\nw/o structure\n0.6756\n0.6739\n0.8351\n0.6581\n0.7990\n0.9721\n0.9344\n0.6628\n0.9603\n0.6674\n0.7046\n0.9263\nw/o temporary\n0.4996\n0.2371\n0.5145\n0.4949\n0.4849\n0.5547\n0.6914\n0.5879\n0.8081\n0.6919\n0.6442\n0.8621\nw/o s-prototype\n0.7164\n0.7436\n0.9491\n0.7320\n0.7905\n0.9516\n0.7897\n0.7589\n0.9570\n0.6384\n0.6802\n0.8480\nw/o t-prototype\n0.6524\n0.6823\n0.8881\n0.7248\n0.7861\n0.9412\n0.6867\n0.6487\n0.9466\n0.6721\n0.7040\n0.9022\nSTRIPE\n0.7622\n0.7972\n0.9620\n0.7359\n0.8020\n0.9765\n0.9409\n0.6849\n0.9810\n0.6919\n0.7144\n0.9389\n30%\n40%\n50%\n60%\n70%\nTraining Ratio\n0.960\n0.962\n0.964\n0.966\n0.968\n0.970\nAUC\n(a) DBLP-3\n30%\n40%\n50%\n60%\n70%\nTraining Ratio\n0.970\n0.972\n0.974\n0.976\nAUC\n(b) DBLP-5\n30%\n40%\n50%\n60%\n70%\nTraining Ratio\n0.970\n0.975\n0.980\n0.985\nAUC\n(c) Reddit\n30%\n40%\n50%\n60%\n70%\nTraining Ratio\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\nAUC\n(d) Brain\nFig. 6. AUC values of STRIPE on four datasets with different training ratios. The circular markers indicate the results which are viewed as outliers.\nis excluded from the loss function. This indicates that incor-\nporating both types of reconstruction is essential for optimal\nperformance in the node anomaly detection task. Furthermore,\nremoving the gated temporal convolution component, as shown\nin the w/o temporary comparison with STRIPE, results in the\nmost pronounced decrease in performance. This underscores\nthe significance of tracking graph evolution for dynamic graph\nanomaly detection. Additionally, the performance difference\nbetween w/o s-prototype and STRIPE and the performance\ngap between w/o t-prototype and STRIPE further illustrate the\ncontribution of both spatial and temporal normality memory\nitems on the enhancement of anomaly detection performance.\nH. Parameter Sensitivity\nIn this subsection, we conduct a series of experiments to\nstudy the impact of various hyper-parameters in STRIPE,\nincluding training ratio, temporal convolution parameters, re-\nconstruction weight, hidden dimension and evaluation rounds.\n1) Training\nratio:\nIn\nthis\nexperiment,\nwe\nassessed\nthe\nrobustness\nof\nSTRIPE\nby\nexamining\nits\nperfor-\nmance across varying training ratios within the range of\n{30%, 40%, 50%, 60%, 70%}. As presented in Fig. 6, the\nincrease of training ratio results in a general improvement\nin AUC values across four datasets. This indicates that a\nlarger volume of training data enhances the model\u2019s ability to\nlearn the normal pattern in the training set. The performance\nfluctuation (e.g., 40% training ratio on the DBLP-3 dataset\nand 60% training ratio on the Reddit dataset) is likely due\nto the shift in data distribution when enlarging the training\nset. Additionally, a decrease in AUC variance with higher\ntraining ratios suggests that STRIPE achieves more stable\nperformance with sufficient training data. Notably, STRIPE\nmaintains competitive performance even at a lower training\nratio of 30%, demonstrating its robustness to conduct dynamic\nnode anomaly detection from limited training data.\n2) Number of memory items: In this research, we explore\nthe effects of varying the number of spatial memory items (Ps)\nand temporal memory items (Pt) within the set {2, 4, 6, 8, 10}.\nThe sensitivity of our model to Ps and Pt is depicted in Fig.\n7. From our analysis, we derive three key insights:\nThe performance of our model, STRIPE, exhibits variability\nwith adjustments in Ps and Pt across the DBLP-3 and Brain\ndatasets. Conversely, the Reddit dataset shows minimal perfor-\nmance fluctuation in response to changes in these parameters,\nlikely due to its simpler normality patterns. This suggests that\nSTRIPE can effectively capture normal spatial and temporal\npatterns with a minimal number of memory items, maintaining\nrobust anomaly detection performance in simpler datasets.\nFor the DBLP-3 and Brain datasets, we observe that in-\ncreasing Ps or Pt from lower values generally leads to higher\nAUC scores. For example, enhancing Pt from 2 to 6 with Ps\nfixed at 2 in DBLP-3 improves performance, underscoring that\na sparse memory set may not adequately represent the graph\u2019s\ncomplex patterns. Nonetheless, further increasing the count of\nmemory items beyond a certain point does not always yield\nbetter performance; it may in fact impair detection capabilities.\nFor instance, elevating Ps from 6 to 10 while keeping Pt at\n2 in Brain illustrates this trend, suggesting that excessively\nlarge memory modules might overemphasize specific details\nover general normality patterns, enabling abnormality to be\nreconstructed accurately.\nThe model\u2019s performance demonstrates greater sensitivity to\nS-memory numbers\n2\n4\n6\n8\n10\nT-memory numbers\n2\n4\n6\n8\n10\nAUC\n0.950\n0.952\n0.954\n0.956\n0.958\n0.960\n(a) DBLP-3\nS-memory numbers\n2\n4\n6\n8\n10\nT-memory numbers\n2\n4\n6\n8\n10\nAUC\n0.979\n0.980\n0.981\n0.982\n(b) Reddit\nS-memory numbers\n2\n4\n6\n8\n10\nT-memory numbers\n2\n4\n6\n8\n10\nAUC\n0.925\n0.930\n0.935\n0.940\n0.945\n(c) Brain\nFig. 7. The sensitivity of spatial memory item number Ps and temporal memory item number Pt on three datasets. The vertical axis represents the AUC\nvalues of STRIPE with different Ps and Pt. A darker color indicates a higher AUC value.\nKt\n1\n2\n3\n4\n5\nTime-Window\n1\n2\n3\n4\n5\nAUC\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n(a) DBLP-3\nKt\n1\n2\n3\n4\n5\nTime-Window\n1\n2\n3\n4\n5\nAUC\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n(b) Reddit\nKt\n1\n2\n3\n4\n5\nTime-Window\n1\n2\n3\n4\n5\nAUC\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\n(c) Brain\nFig. 8. The sensitivity of temporal convolution size Kt and time window size \u03c4 on three datasets. The vertical axis represents the AUC values of STRIPE\nwith different Kt and \u03c4. A darker color indicates a higher AUC value.\nvariations in Pt compared to Ps. This indicates the temporal\ndimension\u2019s intricate prototypical patterns and underscores\nthe importance of separately considering spatial and temporal\npatterns for more nuanced anomaly detection.\n3) Parameters of temporal convolution: In this study, we\nassess the impact of the temporal convolution kernel size\n(Kt) and the number of time window sizes (\u03c4) on temporal\nconvolution performance. We varied \u03c4 across {1, 2, 3, 4, 5},\nensuring Kt satisfies 1 \u2264Kt \u2264\u03c4 for each value of \u03c4.\nSensitivity to changes in Kt and \u03c4 is depicted in Fig. 8.\nThe findings reveal suboptimal model performance when\nthe time window size is set to 1, as temporal convolution in\nthis scenario covers only a single graph snapshot, failing to\ngrasp the dynamic evolution across multiple timestamps. As\n\u03c4 increases, we observe a notable improvement in AUC. de-\ntection performance achieves stability when \u03c4 \u22654, suggesting\nthis time window size can adequately capture both short- and\nlong-term dependencies in graph evolution. Further increasing\n\u03c4 may introduce unnecessary noise and redundancy, detracting\nfrom the model performance.\n4) Reconstruction Weight: In this experiment, we investi-\ngate the influence of the reconstruction weight \u03b1 in Eq. (18).\nWe vary \u03b1 from 0.0 to 1.0 and analyze the corresponding\nAUC values. As depicted in Fig. 9(a). We can observe that\nfor biological and social networks (Brain and Reddit), an\nincrease in \u03b1 correlates with improved AUC values, peaking\nat \u03b1 values of 0.7 and 0.9, respectively. In contrast, for co-\nauthorship networks (DBLP-3 and DBLP-5), optimal AUC\nvalues are observed at \u03b1 \u22640.4, with a performance decline\nas \u03b1 increases beyond this point. This observation suggests\nthat the contribution of attributive and structural reconstruction\nerror is affected by the characteristics and domains of different\ndatasets.\n5) Hidden Dimension: In this experiment, we explored how\nvarying the hidden dimension D\u2032 of both the encoder and\ndecoder affects performance by adjusting D\u2032 from 8 to 256 and\nobserving the AUC values. As shown in 9(b), we found that\nincreasing D\u2032 enhances model performance within the range of\n[8, 128] for the DBLP-3, DBLP-5, and Reddit datasets, and\nwithin [8, 32] for Brain. Beyond these ranges, performance\ngains were negligible or even negative. Consequently, we\nopted for a D\u2032 of 32 for Brain and 128 for the other datasets.\n6) Evaluation Rounds: In this section, we evaluated the\nsensitivity of STRIPE to the number of evaluation rounds (R).\nWe adjust R from 1 to 160 to observe its impact and depict\nthe results in Fig. 9(c). The results indicate poor detection\nperformance at R=1, suggesting that a minimal number of\nrounds fails to adequately detect node anomalies. Performance\nincreases for all the datasets with an increase in R. However,\nelevating R beyond 40 for the Brain dataset and 20 for the\nothers does not significantly enhance results but does lead to\nincreased computational demands. Consequently, to optimize\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n\u03b1\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00\nAUC\nBrain\nReddit\nDBLP3\nDBLP5\n(a) \u03b1 versus AUC\n8\n16\n32\n64\n128\n256\nHidden dimension\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\nAUC\nBrain\nReddit\nDBLP3\nDBLP5\n(b) Hidden dimension versus AUC\n1\n5\n10\n20\n40\n80\n160\nEvaluation epochs\n0.92\n0.94\n0.96\n0.98\nAUC\nBrain\nReddit\nDBLP3\nDBLP5\n(c) Evaluation rounds versus AUC\nFig. 9. AUC value of STRIPE on DBLP3, DBLP5, Reddit and Brain w.r.t. weight \u03b1, hidden dimension D\u2032 and evaluation rounds R.\nS-memory #1\nS-memory #2\nS-memory #3\nS-memory #4\nS-memory #5\nS-memory #6\nNormal #1\nNormal #2\nNormal #3\nAbnormal #1\nAbnormal #2\nAbnormal #3\n1.39\n0.64\n0.06\n0.78\n0.85\n0.94\n1.44\n0.54\n0.29\n0.76\n0.77\n0.84\n1.56\n0.47\n0.65\n1.01\n0.93\n1.07\n1.83\n0.97\n1.03\n1.45\n1.20\n1.35\n1.63\n0.96\n0.86\n1.18\n1.02\n1.25\n1.61\n0.96\n1.14\n1.36\n1.26\n1.42\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nCompactness Loss\n(a) The compactness loss of normal and abnormal nodes\nw.r.t. spatial memories.\nT-memory #1\nT-memory #2\nT-memory #3\nT-memory #4\nT-memory #5\nT-memory #6\nNormal #1\nNormal #2\nNormal #3\nAbnormal #1\nAbnormal #2\nAbnormal #3\n0.50\n0.74\n0.92\n0.89\n0.86\n0.35\n0.83\n0.51\n0.88\n0.91\n0.82\n0.80\n0.53\n0.69\n0.97\n0.89\n0.90\n0.49\n1.00\n0.86\n0.84\n0.97\n0.93\n1.16\n0.97\n0.92\n1.01\n1.01\n0.89\n1.06\n0.96\n0.87\n1.00\n0.93\n0.87\n1.08\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1.1\nCompactness Loss\n(b) The compactness loss of normal and abnormal nodes\nw.r.t. temporal memories.\nFig. 10.\nThe compactness loss of normal and abnormal nodes on DBLP-3\nwith respect to learned spatial and temporal memory items, respectively. A\ndarker color indicates a larger compactness loss between the node feature and\nthe memory.\nboth performance and efficiency, we establish R at 40 for the\nBrain dataset and 20 for the remaining datasets.\nI. Case Study\nIn this section, we demonstrate the effectiveness of our\nspatial and temporal memory modules by visualizing the\ncompactness loss between memory items with both normal\nand abnormal nodes. We configured the spatial and temporal\nmemory items to six each and chose three normal and three\nabnormal nodes from the DBLP-3 test set for analysis. The\ncompactness loss is calculated as outlined in Eq. 19. The\nfindings are presented in Fig. 10, with darker colors indicating\nhigher compactness loss and vice versa.\nFrom Figs. 10(a) and 10(b), we note two key observations:\n(1) Normal nodes exhibit proximity to only a subset of\nmemory items, showing distance from the rest. For instance,\nFig. 10(a) illustrates that the compactness loss between the first\nnormal node and the third spatial memory item is merely 0.06,\nconsiderably less than its loss with other memory items. This\nunderscores the memory items\u2019 capacity to distinctively and\neffectively encapsulate normal patterns, even with a limited set\nof items, while ensuring sufficient separation among them. (2)\nConversely, abnormal nodes display substantial compactness\nlosses across all memory items in both spatial and temporal\ndimensions. As an example, the compactness losses between\nthe second abnormal node and all memory items exceed 0.89,\nhighlighting the anomaly nodes\u2019 divergence from established\nnormal patterns. This deviation contributes to increased recon-\nstruction errors and, consequently, elevated anomaly scores,\neffectively signaling their anomalous nature.\nVI. CONCLUSION\nIn this study, we addressed the intricate challenge of\nanomaly detection in dynamic graphs, a domain character-\nized by the evolving nature of network structures and node\nattributes. Recognizing the limitations inherent in existing\nunsupervised learning frameworks, which may struggle to\naccurately identify anomalies due to their reliance on indirect\nproxy tasks or their failure to distinguish between spatial and\ntemporal patterns, we proposed a spatial-temporal memories\nenhanced graph autoencoder (STRIPE) framework. STRIPE\nrepresents a novel and comprehensive approach to dynamic\ngraph anomaly detection, distinguished by its meticulous\nseparation and integration of spatial and temporal normality\npatterns. Extensive evaluation demonstrates that STRIPE sig-\nnificantly outperforms existing methodologies.\nACKNOWLEDGMENT\nThis work is partially supported by the National Key\nResearch and Development Program of China (Grant No.\n2020AAA0108504).\nREFERENCES\n[1] X. Han, T. Grubenmann, R. Cheng, S. C. Wong, X. Li, and W. Sun,\n\u201cTraffic incident detection: A trajectory-based approach,\u201d in 2020 IEEE\n36th International Conference on Data Engineering (ICDE).\nIEEE,\n2020, pp. 1866\u20131869.\n[2] L. Zheng, Z. Li, J. Li, Z. Li, and J. Gao, \u201cAddgraph: Anomaly detection\nin dynamic graph using attention-based temporal gcn.\u201d in IJCAI, vol. 3,\n2019, p. 7.\n[3] J. Zhang, M. Gao, J. Yu, L. Guo, J. Li, and H. Yin, \u201cDouble-scale\nself-supervised hypergraph learning for group recommendation,\u201d in\nProceedings of the 30th ACM international conference on information\n& knowledge management, 2021, pp. 2557\u20132567.\n[4] B. Zheng, K. Zheng, X. Xiao, H. Su, H. Yin, X. Zhou, and G. Li,\n\u201cKeyword-aware continuous knn query on road networks,\u201d in 2016 IEEE\n32Nd international conference on data engineering (ICDE).\nIEEE,\n2016, pp. 871\u2013882.\n[5] S. Ranshous, S. Harenberg, K. Sharma, and N. F. Samatova, \u201cA\nscalable approach for outlier detection in edge streams using sketch-\nbased approximations,\u201d in Proceedings of the 2016 SIAM international\nconference on data mining.\nSIAM, 2016, pp. 189\u2013197.\n[6] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and W. Wang,\n\u201cNetwalk: A flexible deep embedding approach for anomaly detection\nin dynamic networks,\u201d in Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, 2018,\npp. 2672\u20132681.\n[7] Y. Yang, H. Yin, J. Cao, T. Chen, Q. V. H. Nguyen, X. Zhou, and\nL. Chen, \u201cTime-aware dynamic graph embedding for asynchronous\nstructural evolution,\u201d IEEE Transactions on Knowledge and Data Engi-\nneering, 2023.\n[8] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and\nL. Akoglu, \u201cA comprehensive survey on graph anomaly detection with\ndeep learning,\u201d IEEE Transactions on Knowledge and Data Engineering,\n2021.\n[9] Y. Wang, J. Zhang, S. Guo, H. Yin, C. Li, and H. Chen, \u201cDecoupling\nrepresentation learning and classification for gnn-based anomaly detec-\ntion,\u201d in Proceedings of the 44th international ACM SIGIR conference\non research and development in information retrieval, 2021, pp. 1239\u2013\n1248.\n[10] J. Liu, M. He, X. Shang, J. Shi, B. Cui, and H. Yin, \u201cBourne: Boot-\nstrapped self-supervised learning framework for unified graph anomaly\ndetection,\u201d arXiv preprint arXiv:2307.15244, 2023.\n[11] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, \u201cAnomaly de-\ntection on attributed networks via contrastive self-supervised learning,\u201d\nIEEE transactions on neural networks and learning systems, vol. 33,\nno. 6, pp. 2378\u20132392, 2021.\n[12] X. Teng, Y.-R. Lin, and X. Wen, \u201cAnomaly detection in dynamic net-\nworks using multi-view time-series hypersphere learning,\u201d in Proceed-\nings of the 2017 ACM on Conference on Information and Knowledge\nManagement, 2017, pp. 827\u2013836.\n[13] P. Zheng, S. Yuan, X. Wu, J. Li, and A. Lu, \u201cOne-class adversarial nets\nfor fraud detection,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 33, no. 01, 2019, pp. 1286\u20131293.\n[14] L. Cai, Z. Chen, C. Luo, J. Gui, J. Ni, D. Li, and H. Chen, \u201cStructural\ntemporal graph neural networks for anomaly detection in dynamic\ngraphs,\u201d in Proceedings of the 30th ACM international conference on\nInformation & Knowledge Management, 2021, pp. 3747\u20133756.\n[15] S. Tian, J. Dong, J. Li, W. Zhao, X. Xu, B. Song, C. Meng, T. Zhang,\nL. Chen et al., \u201cSad: Semi-supervised anomaly detection on dynamic\ngraphs,\u201d arXiv preprint arXiv:2305.13573, 2023.\n[16] Y. Xu, B. Shi, T. Ma, B. Dong, H. Zhou, and Q. Zheng, \u201cCldg: Con-\ntrastive learning on dynamic graphs,\u201d in 2023 IEEE 39th International\nConference on Data Engineering (ICDE).\nIEEE, 2023, pp. 696\u2013707.\n[17] J. Weston, S. Chopra, and A. Bordes, \u201cMemory networks,\u201d arXiv\npreprint arXiv:1410.3916, 2014.\n[18] Y. Huang, L. Wang, F. Zhang, and X. Lin, \u201cUnsupervised graph outlier\ndetection: Problem revisit, new insight, and superior method,\u201d in 2023\nIEEE 39th International Conference on Data Engineering (ICDE).\nIEEE, 2023, pp. 2565\u20132578.\n[19] J. Li, H. Dani, X. Hu, and H. Liu, \u201cRadar: Residual analysis for anomaly\ndetection in attributed networks.\u201d in IJCAI, vol. 17, 2017, pp. 2152\u2013\n2158.\n[20] N. Liu, X. Huang, and X. Hu, \u201cAccelerated local anomaly detection via\nresolving attributed networks.\u201d in IJCAI, 2017, pp. 2337\u20132343.\n[21] Z. Peng, M. Luo, J. Li, H. Liu, Q. Zheng et al., \u201cAnomalous: A joint\nmodeling approach for anomaly detection on attributed networks.\u201d in\nIJCAI, 2018, pp. 3513\u20133519.\n[22] K. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detection\non attributed networks,\u201d in Proceedings of the 2019 SIAM International\nConference on Data Mining.\nSIAM, 2019, pp. 594\u2013602.\n[23] J. He, Q. Xu, Y. Jiang, Z. Wang, and Q. Huang, \u201cAda-gad: Anomaly-\ndenoised autoencoders for graph anomaly detection,\u201d in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 38, no. 8, 2024, pp.\n8481\u20138489.\n[24] Y. Zheng, M. Jin, Y. Liu, L. Chi, K. T. Phan, and Y.-P. P. Chen,\n\u201cGenerative and contrastive self-supervised learning for graph anomaly\ndetection,\u201d IEEE Transactions on Knowledge and Data Engineering,\n2021.\n[25] Z. Xu, X. Huang, Y. Zhao, Y. Dong, and J. Li, \u201cContrastive attributed\nnetwork anomaly detection with data augmentation,\u201d in Pacific-Asia\nconference on knowledge discovery and data mining.\nSpringer, 2022,\npp. 444\u2013457.\n[26] T. Ji, D. Yang, and J. Gao, \u201cIncremental local evolutionary outlier detec-\ntion for dynamic social networks,\u201d in Machine Learning and Knowledge\nDiscovery in Databases: European Conference, ECML PKDD 2013,\nPrague, Czech Republic, September 23-27, 2013, Proceedings, Part II\n13.\nSpringer, 2013, pp. 1\u201315.\n[27] J. Liu, L. Song, G. Wang, and X. Shang, \u201cMeta-hgt: Metapath-aware\nhypergraph transformer for heterogeneous information network embed-\nding,\u201d Neural Networks, vol. 157, pp. 65\u201376, 2023.\n[28] J. Liu, M. He, G. Wang, N. Q. V. Hung, X. Shang, and H. Yin,\n\u201cImbalanced node classification beyond homophilic assumption,\u201d arXiv\npreprint arXiv:2304.14635, 2023.\n[29] C. C. Aggarwal, Y. Zhao, and S. Y. Philip, \u201cOutlier detection in\ngraph streams,\u201d in 2011 IEEE 27th international conference on data\nengineering, IEEE.\nIEEE, 2011, pp. 399\u2013409.\n[30] K. Sricharan and K. Das, \u201cLocalizing anomalous changes in time-\nevolving graphs,\u201d in Proceedings of the 2014 ACM SIGMOD interna-\ntional conference on Management of data, 2014, pp. 1347\u20131358.\n[31] E. Manzoor, S. M. Milajerdi, and L. Akoglu, \u201cFast memory-efficient\nanomaly detection in streaming heterogeneous graphs,\u201d in Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining, 2016, pp. 1035\u20131044.\n[32] Z. Chen, W. Hendrix, and N. F. Samatova, \u201cCommunity-based anomaly\ndetection in evolutionary networks,\u201d Journal of Intelligent Information\nSystems, vol. 39, no. 1, pp. 59\u201385, 2012.\n[33] D. Eswaran, C. Faloutsos, S. Guha, and N. Mishra, \u201cSpotlight: Detecting\nanomalies in streaming graphs,\u201d in Proceedings of the 24th ACM\nSIGKDD International Conference on Knowledge Discovery & Data\nMining, 2018, pp. 1378\u20131386.\n[34] Y. Liu, S. Pan, Y. G. Wang, F. Xiong, L. Wang, Q. Chen, and V. C.\nLee, \u201cAnomaly detection in dynamic graphs via transformer,\u201d IEEE\nTransactions on Knowledge and Data Engineering, vol. 35, no. 12, pp.\n12 081\u201312 094, 2021.\n[35] S. Sukhbaatar, J. Weston, R. Fergus et al., \u201cEnd-to-end memory net-\nworks,\u201d Advances in neural information processing systems, vol. 28,\n2015.\n[36] Y. Kim, M. Kim, and G. Kim, \u201cMemorization precedes generation:\nLearning unsupervised gans with memory networks,\u201d in International\nConference on Learning Representations, 2018.\n[37] Z. Wu, Y. Xiong, S. X. Yu, and D. Lin, \u201cUnsupervised feature learning\nvia non-parametric instance discrimination,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 3733\u2013\n3742.\n[38] A. Santoro, S. Bartunov, M. Botvinick, D. Wierstra, and T. Lillicrap,\n\u201cMeta-learning with memory-augmented neural networks,\u201d in Interna-\ntional conference on machine learning.\nPMLR, 2016, pp. 1842\u20131850.\n[39] Q. Cai, Y. Pan, T. Yao, C. Yan, and T. Mei, \u201cMemory matching networks\nfor one-shot image recognition,\u201d in Proceedings of the IEEE conference\non computer vision and pattern recognition, 2018, pp. 4080\u20134088.\n[40] M. Zhu, P. Pan, W. Chen, and Y. Yang, \u201cDm-gan: Dynamic memory gen-\nerative adversarial networks for text-to-image synthesis,\u201d in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2019, pp. 5802\u20135810.\n[41] Y. Bengio, P. Lamblin, D. Popovici, and H. Larochelle, \u201cGreedy layer-\nwise training of deep networks,\u201d Advances in neural information pro-\ncessing systems, vol. 19, 2006.\n[42] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d arXiv\npreprint arXiv:1611.07308, 2016.\n[43] D. Gong, L. Liu, V. Le, B. Saha, M. R. Mansour, S. Venkatesh, and\nA. v. d. Hengel, \u201cMemorizing normality to detect anomaly: Memory-\naugmented deep autoencoder for unsupervised anomaly detection,\u201d in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2019, pp. 1705\u20131714.\n[44] H. Park, J. Noh, and B. Ham, \u201cLearning memory-guided normality for\nanomaly detection,\u201d in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2020, pp. 14 372\u201314 381.\n[45] C. Niu, G. Pang, and L. Chen, \u201cGraph-level anomaly detection via hier-\narchical memory networks,\u201d in Joint European Conference on Machine\nLearning and Knowledge Discovery in Databases.\nSpringer, 2023, pp.\n201\u2013218.\n[46] Y. Liu, J. Liu, M. Zhao, D. Yang, X. Zhu, and L. Song, \u201cLearning\nappearance-motion normality for video anomaly detection,\u201d in 2022\nIEEE International Conference on Multimedia and Expo (ICME). IEEE,\n2022, pp. 1\u20136.\n[47] X. Huang, Y. Yang, Y. Wang, C. Wang, Z. Zhang, J. Xu, and L. Chen,\n\u201cDGraph: A large-scale financial dataset for graph anomaly detection,\u201d\nin Thirty-sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2022.\n[48] J. Duan, S. Wang, P. Zhang, E. Zhu, J. Hu, H. Jin, Y. Liu, and Z. Dong,\n\u201cGraph anomaly detection via multi-scale contrastive learning networks\nwith augmented view,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 37, no. 6, 2023, pp. 7459\u20137467.\n",
    "2312.17679": "Data Augmentation for Supervised Graph Outlier\nDetection with Latent Diffusion Models\nKay Liu1, Hengrui Zhang1, Ziqing Hu2\u2217, Fangxin Wang1, Philip S. Yu1\n1University of Illinois Chicago, Chicago, United States\n2 University of Notre Dame, Notre Dame, United States\n{zliu234, hzhan55, fwang51, psyu}@uic.edu, zhu4@nd.edu\nAbstract\u2014Graph outlier detection is a prominent task of\nresearch and application in the realm of graph neural networks.\nIt identifies the outlier nodes that exhibit deviation from the\nmajority in the graph. One of the fundamental challenges\nconfronting supervised graph outlier detection algorithms is\nthe prevalent issue of class imbalance, where the scarcity of\noutlier instances compared to normal instances often results\nin suboptimal performance. Conventional methods mitigate the\nimbalance by reweighting instances in the estimation of the loss\nfunction, assigning higher weights to outliers and lower weights to\ninliers. Nonetheless, these strategies are prone to overfitting and\nunderfitting, respectively. Recently, generative models, especially\ndiffusion models, have demonstrated their efficacy in synthesiz-\ning high-fidelity images. Despite their extraordinary generation\nquality, their potential in data augmentation for supervised graph\noutlier detection remains largely underexplored.\nTo bridge this gap, we introduce GODM, a novel data\naugmentation for mitigating class imbalance in supervised Graph\nOutlier detection with latent Diffusion Models. Specifically, our\nproposed method consists of three key components: (1) Vari-\nantioanl Encoder maps the heterogeneous information inherent\nwithin the graph data into a unified latent space. (2) Graph\nGenerator synthesizes graph data that are statistically similar\nto real outliers from latent space, and (3) Latent Diffusion Model\nlearns the latent space distribution of real organic data by\niterative denoising. Extensive experiments conducted on multiple\ndatasets substantiate the effectiveness and efficiency of GODM.\nThe case study further demonstrated the generation quality of\nour synthetic data. To foster accessibility and reproducibility, we\nencapsulate GODM into a plug-and-play package and release it at\nthe Python Package Index (PyPI): https://pypi.org/project/godm/.\nIndex Terms\u2014Graph Outlier Detection, Data Augmentation,\nDiffusion Models, Class Imbalance, Anomaly Detection\nI. INTRODUCTION\nGraphs are foundational data structures that capture complex\nrelationships among diverse entities. Recently, graph data\nmining has showcased effectiveness and flexibility across\nvarious tasks, including community classification among aca-\ndemic citation networks [1], recommender systems in social\nnetworks [2], knowledge graph completion [3], and so on\nand forth. Within this broad spectrum of applications, graph\noutlier detection has emerged as a popular and important\narea of research and practice [4]\u2013[6]. Graph outlier detection\nfocuses on detecting outliers within graph-structured data that\nsignificantly deviate from standard patterns, which has proven\nvaluable in domains such as financial fraud detection in bank\n\u2217Work done during Ziqing Hu\u2019s PhD at the University of Notre Dame\nData\nAugmentation\nGraph Oultier\nDetector\nOriginal Graph\nReal Data\nSynthetic Data\nreal inlier\nreal outlier\nsynthetic outlier\nFig. 1. An toy example of data augmentation for class imbalance in graph\noutlier detection. Synthetic outliers help balance the class distribution in the\ntraining data, thereby improving the performance of graph outlier detection.\nsystems [7], fake news detection on social media [8], spam\ndetection on review platform [9], and anti-money laundering\nin transaction networks [10].\nHowever, despite the advancements in graph outlier de-\ntection techniques, similar to supervised outlier detection\non other data modalities, graph outlier detection suffers a\nfundamental challenge known as class imbalance, where the\nnumber of positive instances (outliers) is significantly lower\nthan the number of negative instances (inliers). For example,\nthe ratio of positive to negative is only 1:85 in the DGraph\ndataset [7], reflecting the real-world extreme ratio in financial\nfraud detection scenario. This class imbalance problem poses\nchallenges in the training of outlier detectors and often re-\nsults in suboptimal performance. Specifically, as the negative\ninstances dominate the training data, the loss function can\nbe biased towards the majority of the negative class and,\nhence, exhibit poor generalization capability in identifying true\noutliers. In the worst case, a detector could classify every\ninstance as negative, displaying seemingly low loss value and\nhigh accuracy score, yet basically lacking any knowledge.\nCommon practices mitigate this imbalance by upsampling\nor downsampling [11]. Upsampling augments the minority\npositive class by simply replicating outliers, whereas down-\nsampling reduces the size of the majority negative class by\nrandomly dropping normal instances. However, while benefi-\ncial in certain contexts, these methods often introduce their\nown challenges, such as the risk of overfitting outliers in\nthe case of upsampling or the loss of valuable training data\nthrough downsampling. Another common approach to alleviate\narXiv:2312.17679v2  [cs.LG]  11 Sep 2024\nclass imbalance is through instance reweighting in the loss\nfunction estimation. It allocates greater weights to positive\ninstances and assigns lesser weights to negative instances.\nThis approach is mathematically equivalent to upsampling and\ndownsampling, thereby having a similar tendency to overfit\noutliers or underfitting inliers. These challenges underscore the\nneed for more sophisticated data augmentation methods that\ncan generate some synthetic data to handle the class imbalance\nin graph outlier detection, as Fig. 1 shows.\nIn recent years, generative models, particularly diffusion\nmodels, have achieved significant advancements in synthesiz-\ning high-fidelity image data [12]. Diffusion models capture\nintricate data distributions and generate high-quality samples\nby gradually denoising the samples from a simple prior distri-\nbution (e.g., Gaussian distribution). On graph data, although\nsome works have explored the possibility of diffusion on\nmolecular graphs [13], [14], few studies have been conducted\nto apply diffusion models to tackle the class imbalance prob-\nlem in the task of graph outlier detection. However, existing\ndiffusion models can hardly be directly adapted to large-scale\ngraph outlier detection applications due to the following non-\ntrivial challenges: (1) Heterogeneity: Unlike relatively simple\nmolecular graphs, the information contained in graphs for\noutlier detection can be highly heterogeneous, including high\ndimensional features and more than one type of edge, even\ntemporal information. Existing diffusion models are primarily\ndesigned for monotypic information, exemplified by the RGB\nchannels in image data. (2) Efficiency: In the application of\ngraph outlier detection, e.g., financial fraud detection, the\ngraphs are typically much larger than molecular graphs, up\nto millions, even billions scale. The prohibitive computational\ncost of diffusion models hinders their direct application in\noutlier detection on large graphs. (3) Condition: Different from\nthe unconditional generation on molecular graphs, conditional\ngeneration is required to mitigate the problem of class imbal-\nance. We only want to generate outliers rather than normal\nnodes in synthetic graphs.\nTo bridge this gap, we make the first attempt to leverage\ndiffusion models in graph outlier detection and propose a\ndata augmentation for mitigating class imbalance in super-\nvised Graph Outlier detection with latent Diffusion Models\n(GODM). Our main idea is to generate outliers in graph\nspace while conducting diffusion in latent space. To address\nheterogeneity, we propose Variantioanl Encoder to map the\nheterogeneous information inherent within the graph data\ninto a unified latent space. In addition, Graph Generator\nsynthesizes different types of information back to graph space\nfrom the latent embedding. To alleviate the efficiency problem,\ninstead of direct diffusion in graph space, we only conduct\ndiffusion in the latent space crafted by the variational encoder.\nFurthermore, we use negative sampling and graph clustering\nto reduce the computational cost. For diffusion models, we\nalso adopt EDM [15] instead of commonly used DDPM\n[16] to facilitate the generation efficiency of Latent Diffusion\nModel. For the condition, we not only give a class label to\nthe variational encoder to form node embedding with class\ninformation but also conduct conditional generation on both\nLatent Diffusion Model and Graph Generator. Finally, our\nheterogeneous, efficient, and conditional GODM can generate\ngraphs with outliers that are integrated with the original real\ngraph for the training of the downstream graph outlier detector.\nImportantly, GODM is model agnostic, providing researchers\nand practitioners with the flexibility to integrate it across\nvarious graph outlier detectors.\nOur contributions in this paper are mainly as follows:\n\u2022 We make the first attempt to leverage diffusion models\nin graph outlier detection and propose to adopt diffusion\nmodels in the latent space and bridge the graph space\nand the latent space with a variational encoder and graph\ngenerator for heterogeneity.\n\u2022 We further design outlier-only generation conditioned on\noutlier labels and improve the efficiency of GODM by\nnegative sampling, graph clustering, and EDM.\n\u2022 We conduct extensive experiments to demonstrate the\neffectiveness and efficiency of GODM. To foster accessi-\nbility, we make our code a plug-and-play package, which\nis publicly available at: https://pypi.org/project/godm/.\nThe remainder of this paper is organized as follows. Sec-\ntion II formally defines the notation and the problem we stud-\nied in this paper. Section III provides an overview of GODM.\nSection IV gives detailed experimental results and analyses.\nSection V briefly describes related works. We summarize the\npaper and discuss future work in Section VI.\nII. PRELIMINARY\nIn this section, we establish the notation adopted in the\nsubsequent sections and rigorously formulate the problem of\ndata augmentation for addressing class imbalance in super-\nvised graph outlier detection.\nA. Notation\nLet G = (V, E, X, y, t, p) denotes a graph with n nodes,\nwhere V = {vi}n\ni=1 is the set of nodes, and E = {eij}\nrepresents the set of edges. Here, eij = (vi, vj) denotes an\nedge between node vi and node vj. The matrix X \u2208Rn\u00d7d\ncontains d-dimensional feature vectors xi for each node vi.\ny \u2208{0, 1}n represents the vector of the node label yi for\neach node vi, where 0 denotes an inlier, while 1 denotes an\noutlier. t = {tij} \u2208N and p = {pij} \u2208{1, . . . , P} are the\noptional non-negative integer edge timestamp vector and edge\ntype vector, respectively, where P is the number of edge types.\nB. Problem Formulation\nIn this paper, we focus on the task of graph outlier detection,\nwhich is formally defined as:\nDefinition 1 (Graph Outlier Detection): Given a graph G,\ngraph outlier detection can be regarded as a binary classifica-\ntion task that learns a detector D : vi \u2192{0, 1} that classifies\nevery node in G to an inlier (0) or an outlier (1).\nIn the task of graph outlier detection, we aim to mitigate\nclass imbalance by data augmentation:\nGraph\nClustering\nGNN\nGraph\nEnsemble\n...\nGraph Generator\nEdge Type:\u00a0\nVariational Encoder\nGraph Partitions\nVariational Autoencoder in Graph Space\nLatent Diffusion Model\nFeature:\nEdges:\u00a0\nTimestamp:\nNegative\nSampling\nReverse Denoising Process\npositive edge\nnegative edge\nreal inlier\nreal outlier\nsynthetic outlier\nDenoising Function\nForward Diffusion Process\n...\nFig. 2.\nThe architecture of proposed data augmentation method GODM. For a given graph G, GODM first partitions the large graph by graph clustering\nalgorithms and conducts negative sampling on edge, reducing the computational cost. Then, a GNN-based Variational Encoder maps each node in subgraphs\ninto the latent space embedding Z0. A forward diffusion process Z0 \u2192ZS is applied in the latent space. Reverse denoising process \u02c6ZS \u2192\u02c6Z0 starts from\nthe prior distribution p(\u02c6ZS) and generates samples \u02c6Z0 in latent space with denoising MLP conditioned on outlier labels \u02c6y. Graph Generator also conditionally\ngenerate node features \u02c6X, graph structures \u02c6E, edge timestamps \u02c6t, and edge types \u02c6p from latent embedding \u02c6Z0. By ensembling all necessary information, we\nobtain the synthetic graph \u02c6G. Along with the real graph G, \u02c6G will be used for the training of the downstream graph outlier detector D.\nDefinition 2 (Data Augmentation for Class Imbalance): In\noutlier detection, the number of inliers is far more than the\nnumber of outliers, i.e., |{vi | yi = 0}| \u226b|{vi | yi = 1}|.\nTherefore, we aim to learn a parameterized data augmentation\nmodel, which can generate realistic and diverse synthetic graph\n\u02c6G = (\u02c6V, \u02c6E, \u02c6X, \u02c6y,\u02c6t, \u02c6p), where \u02c6yi = 1, \u2200vi \u2208\u02c6V. To alleviate the\nclass imbalance in the training of the graph outlier detector D,\nwe integrate the original graph G and the synthetic graph \u02c6G\nfor the training of detector D.\nIII. METHODOLOGY\nIn this section, we elaborate on the proposed data augmen-\ntation method GODM in detail. Fig. 2 shows the architecture\nof GODM. It starts by partitioning the input graph G with\ngraph clustering algorithms for reducing memory consump-\ntion. Subsequently, each partitioned subgraph is encoded into a\nlatent space representation denoted by Z0 through Variational\nEncoder based on graph neural networks (GNNs), followed\nby a forward diffusion Z0 \u2192ZS in the latent space, where\nS is the maximum step in the diffusion model. The reverse\ndenoising process \u02c6ZS \u2192\u02c6Z0 iteratively estimates the noise\nand generates the latent embedding \u02c6Z0 from a predefined\nprior distribution p(\u02c6ZS) using a denoising function, which is\na multi-layer perceptron (MLP), conditioned on class labels\n\u02c6y. Graph Generator employs the latent representations \u02c6Z0 to\nconditionally reconstruct the node features and graph struc-\ntures alongside other information available in the original real\ngraph. By ensembling generated information, we are able to\nobtain \u02c6G that contains the nodes that are statistically similar\nto the real outliers. This synthetic graph \u02c6G, in concert with the\noriginal graph G, is leveraged to enhance the training efficacy\nof the downstream graph outlier detection task.\nIn Section III-A and Section III-B, we introduce the design\ndetails of the Variational Encoder and Graph Generator, re-\nspectively. Then, we describe Latent Diffusion Model in Sec-\ntion III-C. We summarize the training and inference process\nin Section III-D and Section III-E, respectively. Finally, we\nanalyze the algorithmic complexity in Section III-F.\nA. Variational Encoder\nGraph data is inherently complex and heterogeneous, con-\ntaining both node feature X and graph structure E, sometimes\nalso partial node class labels y, temporal information t, and\ndifferent types of edges p. However, current diffusion models\nare mainly designed for monotypic information (e.g., the\nmagnitude of RGB channels in images). To bridge this gap,\nwe adopt a GNN-based Variational Encoder E : G \u2192Z to\nmap different types of information to a unified latent space.\nNode Feature. To encode the node feature into the latent\nspace, we take the feature of each node as the initial embed-\nding for the encoder, i.e., h0\ni = xi.\nClass Label. As we only generate outliers by conditional\ngeneration, it will be helpful to encode node labels y = {yi}.\nWe add the class information to the initial node embedding:\nh0\ni = xi + wC\nE \u00b7 yi,\n(1)\nwhere wC\nE \u2208Rd is a linear transformation for class label.\nGraph Structure. Graph neural networks have emerged as\na profound architecture for learning graph data, efficaciously\nlearning the node representations by encoding the graph topol-\nogy and node feature simultaneously with message passing. In\nthis paper, we take GraphSAGE [17] as an example. For each\nlayer of GraphSAGE, each node updates its embedding by\naggregating the message within the neighborhood:\nhl\ni = ACT(Wl \u00b7 CAT(hl\u22121\ni\n, AGG({ml\nij, \u2200eij \u2208E}),\n(2)\nwhere hl\ni is the node embedding for node vi in the l\u2212th\nlayer, ml\nij is the edge-wise message of edge eij, and ACT\nis a non-linear activation function. Wl \u2208Rdl\u00d72dl\u22121 is the\nlinear transformation matrix, where dl and dl\u22121 are the hidden\ndimension of l-th layer and (l\u22121)-th layer, respectively. While\nCAT represents concatenation, AGG denotes the aggregation\nfunction (e.g., mean). In the vanilla GraphSAGE, an edge-wise\nmessage is typically the embedding of source node vj from\nthe last layer:\nml\nij = hl\u22121\nj\n.\n(3)\nEdge Type. However, sometimes, we have different types\nof edges in graphs, known as heterogeneous graphs or hetero-\ngeneous information networks [18], [19]. As different edge\ntypes can encapsulate various semantics, encoding edge types\ncan be important for the downstream task. Therefore, we add\nthe type information to the edge-wise message:\nml\nij = hl\u22121\nj\n+ WP \u00b7 \u03d5(pij),\n(4)\nwhere WP \u2208Rdl\u22121\u00d7P is a linear transformation for edge\ntype, and \u03d5 is the one-hot encoding function.\nEdge Time. In addition, temporal information is critical in\ntime series applications [20]. When edge time is available,\nwe are also able to encode the timestamp to the edge-wise\nmessage with trigonometric temporal embedding [21]:\nml\nij = hl\u22121\nj\n+ WT \u00b7 TE(tij,\u00b7),\n(5)\nwhere WT \u2208Rdl\u22121\u00d7dl\u22121 is the linear transformation for edge\ntime, and TE is defined as:\nTE(tij,2k) = sin(tij/100002k/dl\u22121),\nTE(tij,2k+1) = cos(tij/100002k/dl\u22121),\n(6)\nwhere k is each dimension of the temporal embedding.\nBy stacking multiple layers, GNNs are capable of encoding\nboth node features and neighborhood graph structures into\nindependent and identically distributed (i.i.d.) node embedding\nZ, which can be further leveraged by Latent Diffusion Model.\nTo ease the generation for Latent Diffusion Model, we use\nVariational Encoder, which outputs two matrixes:\n\u00b5 = GNN\u00b5(GNNshared(G)),\nlog\u03c3 = GNN\u03c3(GNNshared(G)),\n(7)\nwhere \u00b5 is the matrix of mean and log \u03c3 is the matrix of log\nstandard deviation. While GNNshared is the shared GNN head,\nGNN\u00b5 and GNN\u03c3 are specifically for \u00b5 and \u03c3, respectively.\nThen, the latent space embedding Z can be obtained via the\nparameterization trick:\nZ = \u00b5 + \u03c3 \u00b7 \u03b5, \u03b5 \u223cN(0, I),\n(8)\nwhere N(0, I) refers to a multivariate normal (or Gaussian)\ndistribution with a mean of 0 and a covariance of I, where 0\ndenotes a zero vector, and I represents a identity matrix.\nB. Graph Generator\nGraph Generator G : \u02c6Z \u2192\u02c6G take the opposite process of\nVariational Encoder, generating \u02c6G = (\u02c6V, \u02c6E, \u02c6X, \u02c6y,\u02c6t, \u02c6p) given\nthe latent space embedding \u02c6Z. Each row \u02c6zi in the embedding\n\u02c6Z is corresponding to a generated node \u02c6vi in \u02c6V.\nClass Label. Recall that our objective is exclusively gen-\nerating outliers with positive labels. Consequently, rather than\ngenerating class labels, we take the desired labels as an input\nfor conditional generation.\n\u02c6zC\ni = \u02c6zi + wC\nG \u00b7 \u02c6yi,\n(9)\nwhere \u02c6zi is the i-th row of \u02c6Z and \u02c6zC\ni is embedding with class\ncondition. wC\nG \u2208RdL is the linear transformation for the class\nlabel, and dL is the output dimension of the L-th layer of the\nencoder, i.e., the latent embedding dimension of \u02c6zi.\nNode Feature. To generate the node feature \u02c6X from the\nlatent space embedding, for the i-th row of \u02c6X, we take the\nembedding with class condition \u02c6zC\ni as input:\n\u02c6xi = WF\nG \u00b7 \u02c6zC\ni ,\n(10)\nwhere WF\nG \u2208Rd\u00d7dL is the linear transformation for node\nfeature generaion.\nGraph Structure. The generation of graph structures de-\nnoted by the set of inferred edges \u02c6E requires link prediction\nbetween all pairwise combinations of nodes within the graph.\nThe predicted edge score \u02c6eij can be formulated as:\n\u02c6eij = sigmoid(wE\nG \u00b7 CAT(\u02c6zC\ni , \u02c6zC\nj )), \u2200(\u02c6vi, \u02c6vj) \u2208\u02c6V \u00d7 \u02c6V, (11)\nwhere wE\nG \u2208R2dL is the linearn transformation for the edge\ngeneration. The generated edges are determined as follows:\n\u02c6E = {(\u02c6vi, \u02c6vj) | \u02c6eij \u22650.5}\n(12)\nEdge Type. With the generated edges in hand, we can\npredict the type of every generated edge:\n\u02c6pij = softmax(WP\nG \u00b7 CAT(\u02c6zC\ni , \u02c6zC\nj )), \u2200(\u02c6vi, \u02c6vj) \u2208\u02c6E,\n(13)\nwhere WP\nG \u2208RP \u00d72dL is the linearn transformation for the\nedge type prediction.\nEdge Time. Similarly, the timestamp of generated edges\ncan be predicted as:\n\u02c6tij = wT\nG \u00b7 CAT(\u02c6zC\ni , \u02c6zC\nj ), \u2200(\u02c6vi, \u02c6vj) \u2208\u02c6E,\n(14)\nwhere wT\nG \u2208R2dL is the linear transformation for the edge\ntimestamp regression.\nC. Latent Diffusion Model\nUtilizing Variational Encoder and Graph Generator, syn-\nthetic graph generation is already feasible. Nonetheless, the\ninherent complexity and heterogeneity of graph data pose a\nsignificant challenge when attempting a one-step estimation\nfrom a simple prior distribution, such as a Gaussian dis-\ntribution, to the intricate target distribution. This challenge\noften leads to compromised generation quality and results in\na marginal effect on the downstream graph outlier detection.\nTherefore, we integrate Latent Diffusion Model [12] to break\ndown the estimation into a sequence of incremental steps. In\neach step, Latent Diffusion Model incrementally refines the\ndistribution estimated, bridging the divergence between the\nsimple prior and the intricate target distribution.\nLatent Diffusion Model consists of a pair of processes.\nA fixed forward diffusion process perturbs the original data\nby the incremental adding Gaussian noise across a range of\nvariances, and a reverse denoising process employs a learned\ndenoising function to iteratively denoise the samples from a\nsimple prior distribution, which is pure noise. In order to speed\nup the generation process, we adopt EDM [15] as our diffusion\nmodel. For the detailed design, we follow [22].\nForward Diffusion Process. We construct a forward dif-\nfusion process {Z(s)}S\ns=0, where s \u2208[0, S] is a continuous\ndiffusion step. In the diffusion process, Z(0) = Z is the\nembedding from Variational Encoder, while Z(S) \u223cN(0, I)\nis sampled from the prior distribution. According to [23],\nthe forward diffusion process can be written in stochastic\ndifferential equation (SDE) as:\ndZ = f(Z, s)ds + g(s)d\u03c9t,\n(15)\nwhere \u03c9t is the standard Wiener process (i.e., Brownian\nmotion). f(\u00b7, s) and g(\u00b7) are the drift coefficient and the\ndiffusion coefficient, respectively. The selection of f(\u00b7, s) and\ng(\u00b7) vary between different diffusion models. f(\u00b7) is usually\nof the form f(Z, s) = f(s)Z. Thus, Eq. 15 can be written as:\ndZ = f(s)Zds + g(s)d\u03c9t.\n(16)\nLet Z be a function of diffusion step s, i.e., Zs = Z(s). The\ndiffusion kernel of Eq. 16 can be represented in the conditional\ndistribution of Zs given Z0:\np(Zs|Z0) = N(a(s)Z0, a2(s)\u03c32(s)I),\n(17)\nwhere a(s) and \u03c3(s) can be derived as:\na(s) = exp\nZ s\n0\nf(\u03be)d\u03be,\n(18)\n\u03c3(s) =\nsZ s\n0\ng2(\u03be)/a2(\u03be)d\u03be.\n(19)\nConsequently, the formulation of the forward diffusion process\nis equivalent to the definition of diffusion kernels characterized\nby a(s) and \u03c3(s).\nConventional\ndenoising\ndiffusion\nprobabilistic\nmodels\n(DDPM) [16] can be seen as discretizations of the vari-\nance preserving SDE with a(s) =\np\n1 \u2212\u03b2(s) and \u03c3(s) =\np\n\u03b2(s)/(1 \u2212\u03b2(s)), as a2(s) + a2(s)\u03c32(s) = 1. However,\nin order to achieve more efficient generation in GODM, we\nadopt EDM [15], which belongs to variance exploding SDE.\nVariance exploding SDE set a(s) = 1, which implies that\nnoise is directly added to the data instead of being blended\nthrough weighting. In this case, the variance of the noise (the\nnoise level) is exclusively determined by \u03c3(s). In EDM, a\nlinear noise level \u03c3(s) = s is applied. Therefore, the diffusion\nkernel can be written as:\np(Zs|Z0) = N(0, \u03c32(s)I),\n(20)\nand the forward diffusion process can be formulated as:\nZs = Z0 + \u03c3(s)\u03b5, \u03b5 \u223cN(0, I).\n(21)\nReverse Denoising Process. As derived in [23], the reverse\ndenoising process is formulated as the reverse SDE of Eq. 15:\ndZ = [f(Z, s) \u2212g2(s)\u2207Z log ps(Z)]ds + g(s)d\u03c9s,\n(22)\nwhere \u2207Z log ps(Z) is the score function of Z. In EDM, given\na(s) = 1 and Eq. 18, it follows that,\nf(Z, s) = f(s)Z = 0.\n(23)\nFor the g(s), derived from Eq. 19, we obtain\ng(s) =\np\n2\u03c3(s) \u02d9\u03c3(s),\n(24)\nwhere \u02d9\u03c3 denotes the first order derivative of \u03c3. With f(Z, s)\nand g(s), we are able to obtain:\ndZ = \u22122 \u02d9\u03c3(s)\u03c3(s)\u2207Z log ps(Z)ds +\np\n2 \u02d9\u03c3(s)\u03c3(s)d\u03c9s, (25)\nwhere the noise level \u03c3(s) = s.\nD. Training\nConventional graph generative models are highly con-\nstrained in scalability. Typical graph generative models are\nworking molecular graphs that are at hundreds node scale [24].\nHowever, outlier detection graphs are usually much larger,\nscaling to millions, even billions. Efficient training of gen-\nerative models on large graphs requires special designs. We\npropose to apply negative sampling and graph clustering to\nimprove the scalability of GODM.\nNegative Sampling. In the graph structure generation, if\nthe training of the generator includes every pair of nodes,\nthis leads to a computational complexity of O(n2). This\nparabolic complexity is catastrophic for large graphs (e.g.,\nmillions scale). Furthermore, it will result in a high imbalance\nin the training of the edge predictor itself. Negative sampling\nemerges as a crucial technique to reduce computational cost\nand alleviate the imbalance. To form a concise training set \u00afE\nfor edge generator, apart from adding the positive edge, i.e.,\n{\u00afeij = 1 | eij \u2208E}, we randomly select a negative edge (a\npair of nodes that is not connected) corresponding to every\npositive edge, {\u00afe\u02dci\u02dcj = 0 | e\u02dci\u02dcj /\u2208E}. By this means, we reduce\nthe complexity from O(n2) to O(|E|).\nGraph Clustering. When the graph scales to millions\nof nodes, full-batch training becomes impractical, even with\nnegative sampling. In addition, traditional neighbor sampling\nmethods are well-suited in our case, as we need to reconstruct\nboth the node feature and graph structure (i.e., edges). Specif-\nically, node sampling cannot gather complete information for\nedge prediction, while edge sampling favors the nodes with a\nhigh degree. To address these challenges, we resort to graph\nclustering inspired by [25]. We first apply graph clustering\nalgorithms (e.g., Metis [26]) to divide the large graph into\nsmall partitions. Then, we treat each partition as a minibatch\nand train GODM on each partition instead of the full graph.\nLeveraging these two techniques, we are able to efficiently\ntrain GODM in two steps. We first train Variational Encoder\nand Graph Generator to bridge the graph space and latent\nspace. Then, we train the diffusion model in the latent space.\nVariational Encoder and Graph Generator. By integrat-\ning Variational Encoder and Graph Generator, we are able to\ntrain both of them in a variational autoencoder (VAE) fashion.\nWe reconstruct the node feature with mean squared error:\n\u2113X = 1\n|V|\nX\nvi\u2208V\n\u2225xi \u2212\u02c6xi\u22252\n2,\n(26)\nwhere \u2225\u00b7 \u22252 denotes the L2 norm. Then, we predict the edges\nin the training edge set \u00afE with binary cross entropy loss:\n\u2113E = \u22121\n| \u00afE|\nX\n\u00afeij\u2208\u00afE\n(\u00afeij log(\u02c6eij) + (1 \u2212\u00afeij log(1 \u2212\u02c6eij))). (27)\nSimilarly, we reconstruct the timestamp with mean squared\nerror and edge type with cross-entropy loss:\n\u2113t = 1\n|E|\nX\neij\u2208E\n(tij \u2212\u02c6tij)2,\n(28)\n\u2113p = \u22121\n|E|\nX\neij\u2208E\nlog(\u02c6pij,pij),\n(29)\nWe can obtain the total reconstruction loss by the weighted\nsum of all reconstruction loss:\n\u2113recon = \u03c9X\u2113X + \u03c9E\u2113E + \u03c9t\u2113t + \u03c9p\u2113p,\n(30)\nwhere \u03c9X, \u03c9E, \u03c9t, and \u03c9p are the hyperparameter weights\nbalancing the importance between each term of reconstruction\nloss. Finally, we use the ELBO loss to train Variational\nEncoder and Graph Generator:\nLVAE = \u2113recon + \u03b2\u2113kl.\n(31)\nwhere the \u2113kl is the Kullback-Leibler (KL) divergence loss be-\ntween the latent space embedding and prior distribution (e.g.,\nGaussian distribution). However, as we have an additional\nLatent Diffusion Model, we loosen this regularization on the\nlatent embedding. Consequently, we use a small hyperparam-\neter weight \u03b2 < 1 to encourage the model to minimize re-\nconstruction error while ensuring that the resultant embedding\nshape remains within the desired shape. The training of the\nVAE, consisting of Variational Encoder and Graph Generator,\nis presented in Algorithm 1.\nAlgorithm 1 GODM: Training of VAE\nInput: Input graph G = (V, E, X, y, t, p)\nOutput: Node representation Z, well-trained Variational En-\ncoder E and Graph Generator G\n1: Partition graph G into a set of subgraphs {Gsub}\n2: for each Gsub do\n3:\nGet \u00b5 and log\u03c3 via Eq. 7\n4:\nReparameterization of Z via Eq. 8\n5:\nGenerate \u02c6X, \u02c6E, \u02c6t, \u02c6p via Eq. 10, 12, 14, 13\n6:\nCalculate loss via Eq. 31\n7:\nUpdate Variational Encoder and Graph Generator pa-\nrameters via the Adam optimizer\n8: end for\nLatent Diffusion Model. After the training of Variational\nEncoder and Graph Generator, we train Latent Diffusion\nModel via denoising score matching.\nTo solve the SDE in Eq. 25, we need to obtain the score\nfunction \u2207Z log ps(Z), which is as intractable as ps(Z) =\np(Zs). However, the conditional distribution p(Zs|Z0) is\ntractable. From Eq. 17, we obtain its analytical solution:\n\u2207Z log p(Zs|Z0) = \u2212\n\u03b5\na(s)\u03c3(s).\n(32)\nThus, we train a multi-layer perceptron (MLP) as the denoising\nfunction denoted by \u03f5\u03b8 for denoising score matching:\nmin EZ0\u223cp(Z0)EZs\u223cp(Zs|Z0)\u2225\u03f5\u03b8(Zs\nC, s) \u2212\u03b5\u22252\n2,\n(33)\nwhere Zs\nC = Zs + yWC, WC \u2208R1\u00d7dL, encoding class\nlabel for conditional generation. The entire training process\nof Latent Diffusion Model is summarized in Algorithm 2.\nAlgorithm 2 GODM: Training of Latent Diffusion Model\nInput: Node representation Z\nOutput: Well-trained denoising function \u03f5\u03b8\n1: Sample the embedding Z0 from p(Z) = p(\u00b5)\n2: Sample diffusion steps s from p(s) then get \u03c3(s)\n3: Sample noise vectors \u03b5 \u223cN(0, \u03c32(s)I)\n4: Get perturbed data Zs = Z0 + \u03b5\n5: Calculate loss \u2113(\u03b8) = \u2225\u03f5\u03b8(Zs\nC, s) \u2212\u03b5\u22252\n2\n6: Update the network parameter \u03b8 via Adam optimizer\nE. Inference\nThe well-trained GODM is utilized in the inference process,\naugmenting real organic data by generated data. We first\nsampling \u02c6ZS from Gaussian distribution. Then, we iteratively\ndenoise \u02c6Zsi to obtain \u02c6Zsi\u22121 conditioned on \u02c6y = 1 via\ndenoising function \u03f5\u03b8. The estimated \u02c6Z0 is fed into Graph\nGenerator along with \u02c6y to generate synthetic graph \u02c6G. Finally,\nthe augmented graph Gsub, which integrates the synthetic\ngraph \u02c6G and the real organic graph G, is used for training\nthe downstream outlier detector D. Algorithm 3 presents the\nwhole procedure of inference.\nAlgorithm 3 GODM: Inference\nInput: Graph generator G and denoising function \u03f5\u03b8\nOutput: Augmented graph Gaug\n1: Sample \u02c6ZS \u223cN(0, \u03c32(S)I)\n2: for i = max, \u00b7 \u00b7 \u00b7 , 1 do\n3:\n\u2207\u02c6Zsi log p(\u02c6Zsi) = \u2212\u03f5\u03b8(\u02c6Zsi\nC , si)/\u03c3(si)\n4:\nget \u02c6Zsi\u22121 via solving the SDE in Eq. 25\n5: end for\n6: Generate \u02c6G = G(\u02c6Z0, \u02c6y)\n7: Gaug = batch(G, \u02c6G)\nF. Complexity Analysis\nIn this section, we meticulously analyze and elucidate the\ntime efficiency and memory scalability of GODM in training\nand inference for both VAE (Variational Encoder and Graph\nGenerator) and Latent Diffusion Model, providing insights\nfor handling large graphs which are common in real-world\napplications of graph outlier detection.\n1) Time Efficiency: In the training of VAE, the major\nbottleneck comes from the graph structure generation in Graph\nGenerator. This generation originally requires link prediction\nfor every pair of nodes in the graphs, which is O(n2). By\ngraph clustering, we divide the large graph into relatively\nsmall partitions for minibatch training. We denote the average\npartition size (i.e., batch size) by b, so the number of partitions\nis n\nb . Then, the complexity is reduced to O( n\nb b2) = O(nb),\nas only node pairs within each partition are predicted. We\napply the negative sampling to further reduce the complexity\nto O(|E|), as we only sample a negative edge for each positive\nedge. For Latent Diffusion Model, we only sample a fixed\nnumber of diffusion steps to train the denoising function. As a\nresult, assuming the number of latent dimensions is a constant,\nthe complexity is O(n). Thus, the total training complexity is\nO(n + |E|). For the inference, we first take S steps sampling\nin Latent Diffusion Model, which is O(nS), and predict\nedges between node pairs in each partition, which is O(nb).\nTherefore, the total inference complexity is O(nS + nb).\n2) Memory Scalability: The vanilla VAE requires memory\nconsumption of O(n2), which is infeasible for large graphs.\nTo reduce memory consumption, we adopt minibatch training\non graph partitions and negative sampling for link predic-\ntion. The complexity of VAE training can be obtained by\nthe number of edges divided by the number of partitions\nO(|E|/ n\nb ) = O( |E|b\nn ). Latent Diffusion Model only requires a\nmemory of O(b). In the inference process, the VAE performs\nlink prediction for all node pairs in a batch, leading to a mem-\nory complexity of O(b2), and Latent Diffusion Model only\ndemands O(b) for each batch. Therefore, the total inference\nprocess needs O(b2) of memory.\nIV. EXPERIMENTS\nIn this section, we systematically delineate the experiments\nto evaluate GODM. We first introduce experimental setups in\nSection IV-A. Then, we aim to answer: RQ1 (Section IV-B):\nHow effective is GODM on improving graph outlier detection\nperformance? RQ2 (Section IV-C): How is the quality of the\nsynthetic data generated by GODM? RQ3 (Section IV-D):\nHow efficient is GODM in terms of time and memory? The\ncode implementation of our experiments is publicly available\nat https://github.com/kayzliu/godm.\nA. Experimental Setups\nTo benchmark GODM and contemporary graph outlier\ndetection algorithms from various aspects, we conduct exper-\niments in a unified and comprehensive environment, which is\nin part adapted from GADBench [6].\nTABLE I\nSTATISTICS OF DATASETS\n(*: WITH MULTIPLE EDGE TYPES AND TEMPORAL INFOMATION).\nDataset\n#Nodes\n#Edges\n#Feat.\nOutlier\nWeibo\n8,405\n407,963\n400\n10.3%\nTolokers\n11,758\n519,000\n10\n21.8%\nQuestions\n48,921\n153,540\n301\n3.0%\nElliptic\n203,769\n234,355\n166\n9.8%\nDGraph\u2217\n3,700,550\n4,300,999\n17\n1.3%\n1) Datasets: We use five diverse datasets from GADBench.\nWeibo, Elliptic, Tolokers, and Questions are homogeneous\nstatic graphs with only one type of edge. DGraph is a heteroge-\nneous temporal graph with different types of edges. The graphs\nvary in scale, ranging from thousands of nodes to millions of\nnodes. Table I provides the statistics of the datasets. In the\ntable, #Nodes stands for the number of nodes, and #Edges\nstands for the number of edges. #Feat. denotes the raw feature\ndimension, i.e., the number of node attributes. The Outlier\ncolumn represents the outlier ratio in the label, indicating the\nextent of class imbalance. The detailed descriptions for each\ndataset are as follows:\nWeibo [27]: This dataset involves a graph delineating the\nassociations between users and their corresponding hashtags\nfrom the Tecent-Weibo platform, consisting of 8,405 users and\na collection of 61,964 hashtags. Activities within this dataset\nare regarded as suspicious (i.e., outliers) if they consist of\na pair of posts occurring within narrowly defined temporal\nintervals (e.g., 60 seconds). Users engaging in at least five\nincidents of such behavior are classified under the suspicious\ncategory, in contrast to the remainder designated as benign.\nFollowing this classification criterion, the dataset contains 868\nusers identified as suspicious and 7,537 as benign. The primary\nfeature vector includes geolocation data for each micro-blog\nentry and a representation utilizing the bag-of-words model.\nTolokers [28]: The dataset is obtained from the Toloka\ncrowdsourcing platform. It is composed of nodes correspond-\ning to individual workers who have engaged in at least one out\nof thirteen specified projects. Edges are established between\npairs of workers who have concurrently contributed to an iden-\ntical task. The primary objective is to predict the likelihood\nof a worker having received a ban in any one of the projects.\nThe features attributed to each node are constructed utilizing\nthe worker\u2019s personal profile information in conjunction with\ntheir task-related performance metrics.\nQuestions [28]: The dataset is derived from Yandex Q, a\nquestion-answering platform. The nodes represent individual\nusers. An edge is established between two nodes to denote the\nscenario wherein one user has responded to another\u2019s inquiry\nwithin the temporal bounds of one year, stretching from\nSeptember 2021 to August 2022. This dataset is specifically\ncurated to encapsulate the engagement of users who exhibit\ninterest in the medical topic. The target for this dataset is\nto predict the likelihood of users\u2019 continued activity on the\nplatform by the conclusion of the observed period. The average\nFastText embeddings of the lexical content present in the\nusers\u2019 descriptions are used for node features.\nElliptic [10]: The dataset comprises a graph of 203,769\nnodes representing Bitcoin transactions, connected through\n234,355 edges representing payment flows, along with 166\ndistinctive node attributes. It correlates Bitcoin transactional\ndata with corresponding real-world entities that are categorized\nas lawful, including exchanges, wallet services, mining oper-\nations, and legitimate services, as well as unlawful categories,\nincluding scams, malicious software, terrorist-related orga-\nnizations, ransomware operations, and fraudulent investment\nactivities known as Ponzi schemes.\nDGraph [7]: The DGraph dataset is a large-scale graph\nwith different edge types and temporal information supplied\nby Finvolution Group. It includes 3 million nodes, 4 million\ndynamic edges, and 1 million labeled nodes. The nodes\nrepresent user accounts within a financial organization offering\npersonal loan services, while an edge between two nodes\nindicates that one account has designated the other as an emer-\ngency contact. Nodes classified as fraud correspond to users\ndisplaying delinquent financial conduct. For those accounts\nwith borrowing records, outliers are identified as accounts\nwith a history of overdue payments, while inliers are those\nwithout such a history. Additionally, the dataset includes 17\nnode features derived from user profile information.\n2) Baselines: In our experiments, we evaluate GODM\nagainst different types of baseline methods. We have three\ntypes of baseline methods, including general graph neural\nnetworks, graph outlier detectors, and data augmentation for\ngraph outlier detection. For general graph neural networks, we\nfollow GADBench and select six GNNs:\nGCN (Graph Convolutional Networks) [1]: GCN is the\nseminal work that applies convolution operation on graph data.\nIt propagates the information of a node to its neighbors, thus\nenabling the network to develop a representation for each node\nthat reflects its local neighborhood context.\nSGC (Simplified Graph Convolution) [29]: This variant of\nGCN leverages Chebyshev polynomials to approximate the\nspectral graph convolution operator. This strategy allows the\nmodel to encompass both local and global graph structures,\nenhancing its scalability for handling larger graphs.\nGIN (Graph Isomorphism Network) [30]: GIN is a form of\nGNN that effectively captures graph structures while main-\ntaining graph isomorphism. It achieves this by generating\nconsistent embeddings for structurally identical graphs, which\nis permutation invariant.\nGraphSAGE (Graph Sample and Aggregate) [17]: Graph-\nSAGE presents a general inductive learning approach where\nnode embeddings are generated through the sampling and ag-\ngregation of features from a node\u2019s immediate neighborhood.\nGAT (Graph Attention Networks) [31]: GAT incorporates\nthe attention mechanism within the GNN framework. It dy-\nnamically assigns varying importance to different nodes during\nthe information aggregation process, focusing the model\u2019s\nlearning on the most relevant parts of the neighborhood.\nGT (Graph Transformer) [32]: Drawing inspiration from\nthe Transformer model in neural networks, GT adapts these\nprinciples for graph-structured data. It utilizes masks in the\nself-attention mechanism to capitalize on the inherent structure\nof graphs, thus boosting the model\u2019s efficiency.\nIn addition to general GNNs, we also include eight GNNs\nspecifically designed for graph outlier detection:\nGAS (GCN-based Anti-Spam) [33]: GAS is an attention-\nbased spam review detector, extending the capabilities of GCN\nto process heterogeneous and heterophilic graphs. It employs\nthe KNN algorithm to align with the structure of each graph.\nDCI (Deep Cluster Infomax) [34]: DCI is a self-supervised\nlearning strategy that separates the learning of node repre-\nsentations from outlier detection. It addresses discrepancies\nbetween node behavioral patterns and their label semantics\nby clustering the entire graph, thus capturing intrinsic graph\nproperties in focused feature spaces.\nPCGNN (Pick and Choose Graph Neural Network) [35]:\nTailored for imbalanced GNN learning in fraud detection\nscenarios, PCGNN uses a label-balanced sampler for node and\nedge selection during training. This results in a more balanced\nlabel distribution within the induced subgraph.\nBernNet [36]: BernNet is a GNN variant offering a robust\napproach to designing and learning arbitrary graph spectral fil-\nters. It utilizes an order-K Bernstein polynomial approximation\nfor estimating filters over the normalized Laplacian spectrum,\ncatering to a variety of graph structures.\nGATSep [37]: Designed to optimize learning on heterophily\ngraphs, GAT-sep merges key design elements like ego- and\nneighbor-embedding separation, higher-order neighborhood\nprocessing, and combinations of intermediate representations.\nAMNet (Adaptive Multi-frequency GNN) [38]: AMNet is\nstructured to capture signals across both low and high frequen-\ncies by stacking multiple BernNets and adaptively integrating\nsignals from different frequency bands.\nBWGNN (Beta Wavelet Graph Neural Network) [39]:\nBWGNN addresses the \u201dright-shift\u201d phenomenon on outliers.\nIt uses the Beta kernel to address higher frequency anomalies\nthrough spatially/spectrally localized band-pass filters.\nGHRN (Graph Heterophily Reduction Network) [40]:\nGHRN tackles the issue of heterophily in the spectral domain\nfor graph outlier detection. This approach focuses on prun-\ning inter-class edges to enhance the representation of high-\nfrequency components in the graph spectrum.\nFor heterogeneous graphs, despite the rich literature, limited\nheterogeneous GNNs can cope with temporal information. As\none of the representatives, we chose HGT for our baseline:\nHGT (Heterogeneous Graph Transformer) [21]: HGT is\nengineered to address the challenges of modeling heteroge-\nneous graphs. It introduces node- and edge-type dependent\nparameters that are crucial for defining heterogeneous attention\nmechanisms for each edge.\nBecause the data augmentation for graph outlier detection is\na relatively new research topic, there are limited baselines. We\ninclude one baseline for homogeneous graphs and introduce\na variant of GODM that can work on both homogeneous and\nheterogeneous datasets, GOVAE:\nDAGAD (Data Augmentation for Graph Anomaly Detec-\ntion) [41]: DAGAD incorporates three modules to augment the\ngraph data, including an information fusion module for repre-\nsentation learning, a data augmentation module to enrich the\ntraining set with synthetic samples, and an imbalance-tailored\nlearning module to distinguish between minority anomalous\nclass and majority normal class.\nGOVAE: In this variant, we drop Latent Diffusion Model\nand form a variational autoencoder with Variational Encoder\nand Graph Generator. The inference is a direct one-step\nestimation from prior Gaussian distribution.\nAlong with GODM, we compare these three data augmenta-\ntion methods for graph outlier detection in Table II, where the\nflexibility represents whether the data augmentation is model\nagnostic. Heterogeneous and time indicate whether the method\nsupports these two types of information. The diffusion column\nindicates whether the method adopts diffusion models.\nTABLE II\nCOMPARISON OF DATA AUGMENTATION FOR GRAPH OUTLIER DETECTION.\nFlexibility\nHeterogeneous\nTime\nDiffusion\nDAGAD\n\u2717\n\u2717\n\u2717\n\u2717\nGOVAE\n\u2713\n\u2713\n\u2713\n\u2717\nGODM\n\u2713\n\u2713\n\u2713\n\u2713\n3) Metrics: We follow the extensive literature in graph\noutlier detection [5], [6] to comprehensively evaluate the per-\nformance of graph outlier detectors with three metrics that are\nrobust to class imbalance: Receiver Operating Characteristic-\nArea Under Curve (AUC), Average Precision (AP), and Re-\ncall@k (Rec), where the value of k is set to the number of\nactual outliers present in the dataset.\n4) Implementation Details: We modified GADBench to\nbenchmark outlier detection performance.\nEnvironment. The key libraries and their versions used\nin our experiments are as follows: Python 3.9, CUDA 11.8,\nPyTorch 2.0.1 [42], PyTorch Geometric 2.4.0 [43], DGL\n1.1.2 [44], and PyGOD 1.0.0 [4].\nHardware. All of our experiments were performed on a\nLinux server with an AMD EPYC 7763 64-core CPU, 192GB\nRAM, and an NVIDIA RTX A40 GPU with 48GB memory.\nHyperparameters. GODM is implemented with default\nhyperparameters. For Variational Encoder, we use one layer\nGraphSAGE for GNNshared, GNN\u00b5, and GNN\u03c3, respectively.\nWe set the hidden dimension to the largest power of 2, which is\nno greater than the feature dimension of the dataset divided by\n2. The weights of reconstruction loss \u03c9X, \u03c9E, \u03c9t, and \u03c9p are 1,\n0.5, 1, and 0.3, respectively. The weight for KL-divergence \u03b2\nis 0.001. For the diffusion model, we use the dimension twice\nas the hidden dimension in VAE. We use five layers of MLP as\nthe denoising function following the detailed design in [22].\nFor training, we adopt the Adam optimizer with a learning\nrate of 0.001 without weight decay to train the VAE and the\ndiffusion model for 100 epochs, respectively, and apply early\nstopping with a patience of 50. The negative sampling ratio\nis 1, and the approximate graph partition size is 2048. For\ninference, we use 50 steps of diffusion and generate the same\namount of synthetic outliers as the number of real organic\noutliers in the training set. For all the hyperparameters in graph\noutlier detection, we apply the default setting in GADBench.\nWe report the graph outlier detection performance of GODM\nwith the optimal downstream graph outlier detector.\nOpen-Source Package. To enhance accessibility, we encap-\nsulate our data augmentation into user-friendly API and make\nour code a plug-and-play package. This package is built upon\nPyTorch and PyTorch Geometric (PyG) frameworks. It accepts\na PyG data object as input and returns the augmented graph as\noutput. By inheriting the transforms.BaseTransform\nin PyG, we enable users to apply GODM just like any other\ntransformations supported in PyG, e.g., ToUndirected. We\nare also able to customize hyperparameters during initializa-\ntion. Code Demo 1 gives a brief example of API usage.\n1\nfrom pygod.utils import load_data\n2\ndata = load_data(\u2019weibo\u2019)\n# load data\n3\n4\nfrom godm import GODM\n# import GODM\n5\ngodm = GODM(lr=0.004)\n# init. GODM\n6\naug_data = godm(data)\n# augment data\n7\n8\ndetector(aug_data)\n# train on data\nCode Demo 1. Using GODM on Weibo dataset [27].\nFor distribution, we release our package on the Python\nPackage Index (PyPI). Users can easily install GODM by\nexecuting pip install godm in a command line.\nB. Performamce on Graph Outlier Detection\nWe start by examining how effective GODM is in the task\nof graph outlier detection on homogeneous graphs. We report\ngraph outlier detection performance on four homogeneous\ndatasets in terms of AUC, AP, and Rec of different algorithms\nin Table III. In the table, the highest score in each metric is\nmarked in bold, while the second highest score is underlined.\nFor DAGAD, we report the optimal performance of its two\nimplementations with GCN and GAT. Although DAGAD\nshows superior performance compared to the standalone GCN\nand GAT, it fails to surpass more sophisticated detectors. On\nthe other hand, GODM and GOVAE together demonstrate sig-\nTABLE III\nPERFORMANCE COMPARISON IN AUC, AP, AND REC (%) ON FOUR DATASETS.\nDataset\nWeibo\nTolokers\nQuestions\nElliptic\nMetric\nAUC\nAP\nRec\nAUC\nAP\nRec\nAUC\nAP\nRec\nAUC\nAP\nRec\nGCN\n98.11\n93.48\n89.34\n74.69\n42.88\n42.06\n69.81\n12.54\n16.99\n82.68\n22.23\n27.61\nSGC\n98.66\n92.46\n87.90\n70.67\n38.03\n35.98\n69.88\n10.13\n15.62\n73.02\n11.44\n9.14\nGIN\n97.47\n92.67\n87.90\n74.05\n36.57\n36.76\n67.76\n12.30\n18.36\n84.38\n29.66\n35.64\nGraphSAGE\n96.54\n89.25\n86.17\n79.42\n48.65\n46.42\n71.69\n17.63\n21.10\n85.31\n37.52\n36.20\nGAT\n94.08\n90.25\n86.74\n77.26\n43.14\n43.30\n70.33\n14.51\n17.26\n84.42\n23.43\n27.42\nGT\n97.06\n91.44\n87.03\n79.24\n46.22\n46.57\n70.83\n16.14\n20.27\n87.14\n29.91\n38.97\nGAS\n94.88\n90.70\n86.74\n76.91\n47.35\n45.02\n64.50\n13.61\n17.53\n87.81\n40.03\n44.78\nDCI\n93.90\n87.78\n83.86\n75.98\n39.85\n40.19\n67.95\n14.58\n19.18\n81.93\n27.63\n33.15\nPCGNN\n90.89\n84.57\n79.83\n72.18\n37.52\n36.76\n68.38\n14.79\n16.99\n86.50\n42.66\n43.77\nGATSep\n96.72\n91.55\n89.05\n79.63\n46.08\n46.73\n69.96\n15.98\n19.18\n83.89\n21.46\n21.15\nBernNet\n93.85\n88.00\n85.30\n76.20\n42.20\n42.21\n70.80\n16.04\n17.53\n82.01\n20.52\n23.55\nAMNet\n95.88\n89.74\n85.59\n75.83\n42.66\n41.90\n69.71\n17.02\n19.18\n80.06\n16.73\n17.17\nBWGNN\n98.29\n92.72\n84.73\n80.15\n49.65\n47.35\n69.47\n16.24\n18.63\n84.32\n22.56\n26.50\nGHRN\n97.21\n92.67\n88.18\n79.80\n49.50\n48.29\n68.24\n16.24\n18.63\n85.36\n24.01\n30.29\nDAGAD\n98.54\n83.36\n90.78\n77.69\n33.94\n44.39\n71.21\n6.88\n20.55\n85.62\n26.18\n40.54\nGOVAE\n99.46\n96.84\n93.08\n83.42\n53.85\n52.49\n75.73\n19.13\n23.84\n83.93\n36.66\n42.66\nGODM\n99.57\n97.54\n93.08\n83.46\n52.95\n52.96\n76.84\n20.48\n24.66\n89.77\n43.92\n53.92\nTABLE IV\nGODM\u2019S PERFORMANCE IMPROVEMENTS ON GRAPH OUTLIER\nDETECTION ALGORITHMS IN AUC, AP, AND RECALL (%) ON TOLOKERS.\nAUC\nAP\nRec\nGCN\n75.45 (+0.76)\n44.17 (+1.29)\n44.24 (+2.18)\nSGC\n72.73 (+2.06)\n39.75 (+1.72)\n38.01 (+2.02)\nGIN\n74.83 (+0.78)\n38.54 (+1.96)\n38.32 (+1.56)\nGraphSAGE\n81.65 (+2.23)\n52.53 (+3.87)\n50.93 (+4.52)\nGAT\n82.18 (+4.91)\n51.13 (+7.99)\n50.78 (+7.48)\nGT\n82.73 (+3.49)\n51.73 (+5.51)\n50.93 (+4.36)\nGAS\n76.96 (+0.05)\n45.48 (-1.87)\n43.15 (-1.87)\nDCI\n73.75 (-2.23)\n37.52 (-2.33)\n37.54 (-2.65)\nPCGNN\n73.65 (+1.47)\n38.42 (+0.90)\n38.01 (+1.25)\nGATSep\n83.46 (+3.83)\n52.95 (+6.87)\n52.80 (+6.07)\nBernNet\n76.53 (+0.33)\n44.04 (+1.83)\n42.21 (+0.00)\nAMNet\n76.67 (+0.84)\n44.31 (+1.65)\n43.30 (+1.40)\nBWGNN\n82.10 (+1.95)\n51.94 (+2.29)\n51.25 (+3.89)\nGHRN\n82.05 (+2.25)\n51.84 (+2.34)\n51.56 (+3.27)\nnificant enhancements in graph outlier detection performance\nacross all datasets and metrics. Notably, on the Weibo dataset,\nwhere the performance metric is already relatively high (AUC\nof 98.66 for SGC), GODM further elevates the performance,\nachieving an AUC of 99.57. A particularly noteworthy im-\nprovement is observed on Elliptic, where GODM enhances\nthe Rec by over 20% (an increase of 9.14) compared to GAS,\nwhich is the second-best performing algorithm. Although GO-\nVAE exhibits competitive performance, its limited generation\nability results in generally weaker performance compared to\nGODM, especially on Elliptic.\nFurther investigations were carried out to understand the ex-\ntent of improvement GODM could bring to various graph out-\nlier detection algorithms, specifically on Tolokers. We integrate\nGODM on the top of all graph outlier detection algorithms and\nTABLE V\nPERFORMANCE COMPARISON IN AUC, AP, AND REC (%) ON DGRAPH.\nAUC\nAP\nRec\nHGT\n72.89\n3.13\n5.20\nw/o Feature\n75.10\n3.64\n5.59\nw/o Edge\n74.36\n3.52\n5.76\nw/o Time\n74.23\n3.29\n4.73\nw/o Type\n75.09\n3.62\n5.98\nGOVAE\n75.36\n3.58\n5.54\nGODM\n75.80\n3.66\n6.19\nreport the performance changes in terms of AUC, AP, and\nRec in Table IV. The absolute performance is reported out of\nbrackets, and the relative change is reported in the brackets.\nWe can observe that GODM enhances graph outlier detection\nperformance for most of the algorithms. GODM increases\nthe AP of GAT by 7.99. For DCI and GAS, the only two\nalgorithms that minorly decrease the performance, this could\nbe attributed to suboptimal hyperparameter settings for these\nspecific algorithms or the dataset. We believe the performance\ncan be improved by careful hyperparameter tuning.\nMoreover, in Table V, we benchmark the graph outlier\ndetector performance on DGraph, a large-scale dataset with\ndifferent types of edges and temporal information. We include\nHGT along with variants of GODM. For the w/o Feature, we\nrandomly sample the node feature from the Gaussian distri-\nbution to replace the generated node feature in the synthetic\ngraph. For the w/o Edge, all generated edges are substituted\nwith random edges. For w/o Time, we give uniformly sampled\ntimestamps to replace the generated timestamps. For the\nw/o Type, we use a random type sampled from a uniform\ndistribution for all generated edges. According to Table V,\nGODM demonstrates enhanced performance across all metrics\n5\n0\n5\n10\n15\n20\n(a) Feature 1\n0.0\n0.5\n1.0\n1.5\n2.0\nDensity\n4\n2\n0\n2\n(b) Feature 2\n0.0\n0.5\n1.0\n1.5\n2.0\nDensity\n0\n2\n4\n6\n8\n10\n(c) Edge Type\n101\n102\n103\n104\n105\nFrequency\nReal\nGOVAE\nGODM\nFig. 3. Visualization of single node feature density and edge type frequency\nof real data and synthetic data from GOVAE and GODM on DGraph.\nwhen compared to HGT. Furthermore, masking any generated\ninformation leads to a notable decline in performance, which\nunderscores the significance of each aspect of the generated\ndata. While GOVAE achieves commendable results, its perfor-\nmance still falls short of GODM\u2019s, highlighting the superior\nefficacy of Latent Diffusion Model.\nC. Generation Quality\nTo assess the quality of generated data, we conduct a case\nstudy on DGraph to compare real data and synthetic data\nfrom both GOVAE and GODM. The results are shown in\nFig. 3. Specifically, Fig. 3 (a) and (b) illustrate the distribution\ndensity of a single dimension in node features. From the\nfigures, we can see that the distributions of GODM generated\nnode features are close to the complex distributions of real\ndata, while GOVAE can only generate Gaussian distributions,\nshowcasing the generation ability of Latent Diffusion Model.\nIn addition, to evaluate generated edges, Fig. 3 (c) shows the\nfrequency across eleven different edge types. The edge type\ngenerated by GODM has a more similar distribution to real\ndata compared to GOVAE, particularly on the 10th edge type.\nIt further underscores the superiority of GODM over GOVAE.\nD. Efficiency Study\nWe further evaluate the efficiency of GODM and its variants\nin terms of time and memory on graphs of various sizes,\nincluding Weibo, Elliptic, and DGraph. In order to demonstrate\nthe efficiency of GODM, we remove graph clustering (w/o\nGC), negative sampling (w/o NS), and EDM (w/o EDM) for\nevaluation. To simulate the real-world application, we measure\nthe running time of unit epoch training for VAE and Latent\nDiffusion Model plus the inference process. For memory,\nwe present the maximum active GPU memory usage within\nthe whole data augmentation, as the GPU memory constraint\nis usually the bottleneck for machine learning systems. The\nresults are shown in Fig. 4.\nWeibo\nElliptic\nDGraph\n100\n101\n102\nRunning Time (s)\nWeibo\nElliptic\nDGraph\n0\n2\n4\n6\n8\n10\nGPU Memory (GB)\nGODM\nw/o GC\nw/o NS\nw/o EDM\nFig. 4. Running time and GPU memory of GODM and its variants.\nFrom Fig. 4, we can observe that employing a graph\nclustering algorithm significantly diminishes GPU memory\nutilization. Specifically, on DGraph, w/o GC results in a\nmemory usage that is over 30\u00d7 higher than GODM. On the\nother hand, graph clustering turns full-batch training into mini-\nbatch training, leading to an anticipated increase in running\ntime. Additionally, negative sampling reduces both the running\ntime and the memory usage, as the number of edges in\nthe training set reduces from O(n2) to O(|E|). While EDM\nmaintains consistent memory consumption, it contributes to\ntime savings by minimizing the number of sampling steps.\nV. RELATED WORKS\nThis section briefly summarizes the previous related works\nin three key areas, including graph outlier detection, class\nimbalance, and graph generative models.\nA. Graph Outlier Detection\nGraph outlier detection is a vital task in data mining and\nmachine learning, aiming to identify anomalous structures\n(e.g., nodes, edges, subgraphs) in graphs. Due to its structural\nsimplicity and broader applications, most research focuses on\nnode-level graph outlier detection and can easily convert to\nedge level and graph level task [4]. Many studies focus on\nunsupervised settings, detecting outliers in the graph only\nbased on the graph data itself without any ground truth\nlabels [45]\u2013[47]. However, such unsupervised approaches may\nnot align well with scenarios necessitating the detection of\nspecific outlier types containing domain-specific knowledge.\nIn this case, (semi-)supervised graph outlier detectors, which\ncan learn ground truth labels, are better options. [33] apply\nattention mechanism to detect spam review. [11], [48] alleviate\nthe camouflage issue in fraud detection by message passing in\nselected neighborhoods. [35] further improves the neighbor\nselection in fraud detection. [49] builds the outlier detection\ngraph via the KNN algorithm. [34] decouples the represen-\ntation learning and anomaly detection, and [50] focuses on\nheterophily in graph outlier detection. Some other works detect\noutliers from a spectral perspective. [36] learns arbitrary graph\nspectral filters via Bernstein approximation, and [38] is capable\nof discerning both low-frequency and high-frequency signals,\nthereby adaptively integrating signals across a spectrum of\nfrequencies. In [39], the Beta kernel is employed to detect\nanomalies at higher frequencies using flexible and localized\nband-pass filters. [40] addresses heterophily in graph outlier\ndetection from a graph spectrum perspective.\nB. Class Imbalance\nDespite the fruitful literature on graph outlier detection,\nmost of methods underrate a pivotal challenge: the class\nimbalance in ground truth labels. These methods typically\nmitigate this imbalance by primitive techniques. GADBench\nuses reweighting [6], assigning a higher weight to outliers in\nthe loss function. Some works adopt upsampling to replicate\noutliers, which is mathematically equivalent to reweighting.\n[11] applies downsampling, reducing the number of normal\ninstances and losing a large amount of valuable supervision\ninformation from normal instances. However, these tricks may\nnot sufficiently address the underlying complexities associated\nwith imbalanced datasets. Consequently, there is a compelling\nneed for more sophisticated techniques that can effectively\naddress this class imbalance to ensure the quality of graph\noutlier detection. [35] balances the neighborhood distribu-\ntion by reducing edges between different classes and adding\nedges between the same classes but does not change the\noverall number of positive and negative nodes. [41] adopts\nrandom perturbation for data augmentation to generate more\nsamples. Other general graph machine learning studies ap-\nply interpolation-based data augmentation methods in latent\nspace [51]. However, these methods are not specifically de-\nsigned for graph outlier detection, and interpolation is too\nnaive to generate helpful instances. This paper seeks to build\nupon these foundational works, proposing a novel graph data\naugmentation algorithm with latent diffusion models that not\nonly mitigate class imbalance effectively but are also scalable\nand efficient on large graphs, ensuring broader applicability in\nreal-world graph outlier detection.\nC. Graph Generative Models\nRecent advances in graph generative models have catalyzed\na significant body of research, focusing primarily on the\nsynthesis of realistic and structurally coherent graphs. [52]\nproposes VGAE, leveraging variational autoencoder frame-\nworks to learn latent representations of graph structures for\ngeneration purposes. [53] introduces GraphRNN, generating\nnodes and edges autoregressively with two recurrent neu-\nral networks. [54] applies a generative adversarial networks\nframework to generate graphs via random walks. [55] in-\ntroduces the normalizing flow method for molecular graph\ngeneration. With the recent proliferation of works in image\ngeneration with diffusion models, some efforts have extended\nthe generation power of diffusion models to graph-structured\ndata. [13] proposes a score-based generative modeling of\ngraphs via the system of stochastic differential equations,\nwhile [14] uses a discrete diffusion model to generate graphs.\nHowever, the potential of graph generative models to address\nthe class imbalance in graph outlier detection remains largely\nunderexplored. We propose to generate some synthetic outliers\nto mitigate this class imbalance in graph outlier detection.\nVI. CONCLUSION\nIn this paper, we introduce GODM, a novel method for data\naugmentation in supervised graph outlier detection. GODM\nis designed to address the significant challenge of class im-\nbalance, a pervasive issue in graph outlier detection, where\nthe scarcity of outlier instances compared to normal instances\noften leads to suboptimal model performance. Our method\nleverages the power of generative models, particularly latent\ndiffusion models, to synthesize fidelity graph data that is\nstatistically similar to real outliers. GODM consists of three\nkey components: Variational Encoder, Latent Diffusion Model,\nand Graph Generator. Variational Encoder maps heterogeneous\ninformation in the input graph into a unified latent space.\nLatent Diffusion Model learns the latent space distribution\nof real organic data through iterative denoising. Graph Gen-\nerator then synthesizes graph data from this latent space\nembedding, ensuring statistical similarity to real data. Our\nextensive experiments across multiple datasets demonstrate the\neffectiveness and efficiency of GODM. Note that GODM is\nmodel agnostic, which means it can be flexibly integrated with\ndifferent downstream graph outlier detectors. We encapsulate\nGODM into a plug-and-play package, making it accessible for\nbroader use in the researcher and practitioner communities.\nIn future research, several directions can be explored to\nenhance GODM further and expand its applicability in the\ndomain of graph outlier detection:\n\u2022 Diffusion in graph sapce. Considering the heterogeneity\nand scalability of the graphs for outlier detection appli-\ncations, GODM currently employs a diffusion model in\nlatent space. [56] starts to generate large graphs directly\nin graph space. Future research could explore the appli-\ncation of diffusion models on heterogeneous information\ndirectly in graph space, potentially enhancing the model\u2019s\nability to capture complex graph structures.\n\u2022 Collaboration with outlier detection. Currently, GODM\nis model agnostic, meaning the training of GODM and\ndownstream outlier detection tasks are separate. Integrat-\ning the supervision signals from both the generation task\nand the downstream outlier detection in a unified training\nframework could be mutually beneficial. Self-training\noffers a promising paradigm for this integration [57].\n\u2022 Enhancing generative models. Although the current\ndiffusion model in GODM has shown impressive ca-\npabilities, there is potential for further improvement.\nThis could be achieved by employing more expressive\ndenoising functions, such as transformers [58], or by\nexploring alternative generative models like energy-based\nmodels [59] and or normalizing flows [60], which might\noffer different advantages over diffusion models.\nACKNOWLEDGMENT\nThis work is supported by NSF under grant III-2106758. We\nsincerely appreciate reviewers in advance for their significant\neffort and invaluable feedback during the review process.\nREFERENCES\n[1] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph\nconvolutional networks,\u201d in International Conference on Learning Rep-\nresentations, 2016.\n[2] L. Yang, Z. Liu, Y. Dou, J. Ma, and P. S. Yu, \u201cConsisrec: Enhancing\ngnn for social recommendation via consistent neighbor aggregation,\u201d\nin Proceedings of the 44th international ACM SIGIR conference on\nResearch and development in information retrieval, 2021, pp. 2141\u2013\n2145.\n[3] S. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip, \u201cA survey on\nknowledge graphs: Representation, acquisition, and applications,\u201d IEEE\ntransactions on neural networks and learning systems, vol. 33, no. 2,\npp. 494\u2013514, 2021.\n[4] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen,\nH. Peng, K. Shu et al., \u201cPygod: A python library for graph outlier\ndetection,\u201d arXiv preprint arXiv:2204.12095, 2022.\n[5] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding,\nC. Chen, H. Peng et al., \u201cBond: Benchmarking unsupervised outlier node\ndetection on static attributed graphs,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 27 021\u201327 035, 2022.\n[6] J. Tang, F. Hua, Z. Gao, P. Zhao, and J. Li, \u201cGadbench: Revisiting\nand benchmarking supervised graph anomaly detection,\u201d arXiv preprint\narXiv:2306.12251, 2023.\n[7] X. Huang, Y. Yang, Y. Wang, C. Wang, Z. Zhang, J. Xu, L. Chen,\nand M. Vazirgiannis, \u201cDgraph: A large-scale financial dataset for graph\nanomaly detection,\u201d Advances in Neural Information Processing Sys-\ntems, vol. 35, pp. 22 765\u201322 777, 2022.\n[8] Y. Dou, K. Shu, C. Xia, P. S. Yu, and L. Sun, \u201cUser preference-\naware fake news detection,\u201d in Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval, 2021, pp. 2051\u20132055.\n[9] Y. Dou, G. Ma, P. S. Yu, and S. Xie, \u201cRobust spammer detection by\nnash reinforcement learning,\u201d in Proceedings of the 26th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining,\n2020, pp. 924\u2013933.\n[10] M. Weber, G. Domeniconi, J. Chen, D. K. I. Weidele, C. Bellei,\nT. Robinson, and C. Leiserson, \u201cAnti-money laundering in bitcoin: Ex-\nperimenting with graph convolutional networks for financial forensics,\u201d\nin ACM SIGKDD International Conference on Knowledge Discovery\nand Data Mining, 2019.\n[11] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, \u201cEnhanc-\ning graph neural network-based fraud detectors against camouflaged\nfraudsters,\u201d in Proceedings of the 29th ACM international conference\non information & knowledge management, 2020, pp. 315\u2013324.\n[12] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-\nresolution image synthesis with latent diffusion models,\u201d in Proceedings\nof the IEEE/CVF conference on computer vision and pattern recognition,\n2022, pp. 10 684\u201310 695.\n[13] J. Jo, S. Lee, and S. J. Hwang, \u201cScore-based generative modeling of\ngraphs via the system of stochastic differential equations,\u201d in Interna-\ntional Conference on Machine Learning.\nPMLR, 2022, pp. 10 362\u2013\n10 383.\n[14] C. Vignac, I. Krawczuk, A. Siraudin, B. Wang, V. Cevher, and\nP. Frossard, \u201cDigress: Discrete denoising diffusion for graph generation,\u201d\nin The Eleventh International Conference on Learning Representations,\n2022.\n[15] T. Karras, M. Aittala, T. Aila, and S. Laine, \u201cElucidating the design\nspace of diffusion-based generative models,\u201d Advances in Neural Infor-\nmation Processing Systems, vol. 35, pp. 26 565\u201326 577, 2022.\n[16] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d\nAdvances in neural information processing systems, vol. 33, pp. 6840\u2013\n6851, 2020.\n[17] W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d Advances in neural information processing\nsystems, vol. 30, 2017.\n[18] C. Shi, Y. Li, J. Zhang, Y. Sun, and S. Y. Philip, \u201cA survey of\nheterogeneous information network analysis,\u201d IEEE Transactions on\nKnowledge and Data Engineering, vol. 29, no. 1, pp. 17\u201337, 2016.\n[19] J. Zhao, X. Wang, C. Shi, Z. Liu, and Y. Ye, \u201cNetwork schema preserv-\ning heterogeneous information network embedding,\u201d in International\njoint conference on artificial intelligence (IJCAI), 2020.\n[20] L. Deng, D. Lian, Z. Huang, and E. Chen, \u201cGraph convolutional ad-\nversarial networks for spatiotemporal anomaly detection,\u201d IEEE Trans-\nactions on Neural Networks and Learning Systems, vol. 33, no. 6, pp.\n2416\u20132428, 2022.\n[21] Z. Hu, Y. Dong, K. Wang, and Y. Sun, \u201cHeterogeneous graph trans-\nformer,\u201d in Proceedings of the web conference 2020, 2020, pp. 2704\u2013\n2710.\n[22] H. Zhang, J. Zhang, B. Srinivasan, Z. Shen, X. Qin, C. Faloutsos,\nH. Rangwala, and G. Karypis, \u201cMixed-type tabular data synthesis with\nscore-based diffusion in latent space,\u201d arXiv preprint arXiv:2310.09656,\n2023.\n[23] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\nB. Poole, \u201cScore-based generative modeling through stochastic differ-\nential equations,\u201d in International Conference on Learning Representa-\ntions, 2020.\n[24] N. De Cao and T. Kipf, \u201cMolgan: An implicit generative model for\nsmall molecular graphs,\u201d arXiv preprint arXiv:1805.11973, 2018.\n[25] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh,\n\u201cCluster-gcn: An efficient algorithm for training deep and large graph\nconvolutional networks,\u201d in Proceedings of the 25th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, 2019,\npp. 257\u2013266.\n[26] G. Karypis and V. Kumar, \u201cMetis: A software package for partitioning\nunstructured graphs, partitioning meshes, and computing fill-reducing\norderings of sparse matrices,\u201d http://glaros. dtc. umn. edu/gkhome-\n/metis/metis/download, 1997.\n[27] T. Zhao, C. Deng, K. Yu, T. Jiang, D. Wang, and M. Jiang, \u201cError-\nbounded graph anomaly loss for gnns,\u201d in Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management,\n2020, pp. 1873\u20131882.\n[28] O.\nPlatonov,\nD.\nKuznedelev,\nM.\nDiskin,\nA.\nBabenko,\nand\nL. Prokhorenkova, \u201cA critical look at the evaluation of gnns under\nheterophily: Are we really making progress?\u201d in The\nEleventh\nInternational Conference on Learning Representations, 2022.\n[29] F. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger,\n\u201cSimplifying graph convolutional networks,\u201d in International conference\non machine learning.\nPMLR, 2019, pp. 6861\u20136871.\n[30] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph\nneural networks?\u201d in International Conference on Learning Represen-\ntations, 2018.\n[31] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and\nY. Bengio, \u201cGraph attention networks,\u201d in International Conference on\nLearning Representations, 2018.\n[32] Y. Shi, H. Zhengjie, S. Feng, H. Zhong, W. Wang, and Y. Sun, \u201cMasked\nlabel prediction: Unified message passing model for semi-supervised\nclassification,\u201d in International joint conference on artificial intelligence\n(IJCAI), 08 2021, pp. 1548\u20131554.\n[33] A. Li, Z. Qin, R. Liu, Y. Yang, and D. Li, \u201cSpam review detection\nwith graph convolutional networks,\u201d in Proceedings of the 28th ACM\nInternational Conference on Information and Knowledge Management,\n2019, pp. 2703\u20132711.\n[34] Y. Wang, J. Zhang, S. Guo, H. Yin, C. Li, and H. Chen, \u201cDecoupling\nrepresentation learning and classification for gnn-based anomaly detec-\ntion,\u201d in Proceedings of the 44th international ACM SIGIR conference\non research and development in information retrieval, 2021, pp. 1239\u2013\n1248.\n[35] Y. Liu, X. Ao, Z. Qin, J. Chi, J. Feng, H. Yang, and Q. He, \u201cPick and\nchoose: a gnn-based imbalanced learning approach for fraud detection,\u201d\nin Proceedings of the web conference 2021, 2021, pp. 3168\u20133177.\n[36] M. He, Z. Wei, H. Xu et al., \u201cBernnet: Learning arbitrary graph spectral\nfilters via bernstein approximation,\u201d Advances in Neural Information\nProcessing Systems, vol. 34, pp. 14 239\u201314 251, 2021.\n[37] A. Zimek, R. J. Campello, and J. Sander, \u201cEnsembles for unsupervised\noutlier detection: challenges and research questions a position paper,\u201d\nAcm Sigkdd Explorations Newsletter, vol. 15, no. 1, pp. 11\u201322, 2014.\n[38] Z. Chai, S. You, Y. Yang, S. Pu, J. Xu, H. Cai, and W. Jiang, \u201cCan\nabnormality be detected by graph neural networks,\u201d in Proceedings of the\nTwenty-Ninth International Joint Conference on Artificial Intelligence\n(IJCAI), Vienna, Austria, 2022, pp. 23\u201329.\n[39] J. Tang, J. Li, Z. Gao, and J. Li, \u201cRethinking graph neural networks for\nanomaly detection,\u201d in International Conference on Machine Learning.\nPMLR, 2022, pp. 21 076\u201321 089.\n[40] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAddressing\nheterophily in graph anomaly detection: A perspective of graph spec-\ntrum,\u201d in Proceedings of the ACM Web Conference 2023, 2023, pp.\n1528\u20131538.\n[41] F. Liu, X. Ma, J. Wu, J. Yang, S. Xue, A. Beheshti, C. Zhou, H. Peng,\nQ. Z. Sheng, and C. C. Aggarwal, \u201cDagad: Data augmentation for graph\nanomaly detection,\u201d in 2022 IEEE International Conference on Data\nMining (ICDM).\nIEEE, 2022, pp. 259\u2013268.\n[42] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An\nimperative style, high-performance deep learning library,\u201d Advances in\nneural information processing systems, vol. 32, 2019.\n[43] M. Fey and J. E. Lenssen, \u201cFast graph representation learning with\npytorch geometric,\u201d arXiv preprint arXiv:1903.02428, 2019.\n[44] M. Y. Wang, \u201cDeep graph library: Towards efficient and scalable deep\nlearning on graphs,\u201d in ICLR workshop on representation learning on\ngraphs and manifolds, 2019.\n[45] K. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detection\non attributed networks,\u201d in Proceedings of the 2019 SIAM International\nConference on Data Mining.\nSIAM, 2019, pp. 594\u2013602.\n[46] Z. Xu, X. Huang, Y. Zhao, Y. Dong, and J. Li, \u201cContrastive attributed\nnetwork anomaly detection with data augmentation,\u201d in Pacific-Asia\nConference on Knowledge Discovery and Data Mining. Springer, 2022,\npp. 444\u2013457.\n[47] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, \u201cAnomaly de-\ntection on attributed networks via contrastive self-supervised learning,\u201d\nIEEE transactions on neural networks and learning systems, vol. 33,\nno. 6, pp. 2378\u20132392, 2021.\n[48] Z. Liu, Y. Dou, P. S. Yu, Y. Deng, and H. Peng, \u201cAlleviating the incon-\nsistency problem of applying graph neural network to fraud detection,\u201d\nin Proceedings of the 43rd international ACM SIGIR conference on\nresearch and development in information retrieval, 2020, pp. 1569\u20131572.\n[49] D. Jin, Z. Yu, C. Huo, R. Wang, X. Wang, D. He, and J. Han, \u201cUni-\nversal graph convolutional networks,\u201d Advances in Neural Information\nProcessing Systems, vol. 34, pp. 10 654\u201310 664, 2021.\n[50] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra,\n\u201cBeyond homophily in graph neural networks: Current limitations and\neffective designs,\u201d Advances in neural information processing systems,\nvol. 33, pp. 7793\u20137804, 2020.\n[51] T. Zhao, X. Zhang, and S. Wang, \u201cGraphsmote: Imbalanced node\nclassification on graphs with graph neural networks,\u201d in Proceedings of\nthe 14th ACM international conference on web search and data mining,\n2021, pp. 833\u2013841.\n[52] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d arXiv\npreprint arXiv:1611.07308, 2016.\n[53] J. You, R. Ying, X. Ren, W. Hamilton, and J. Leskovec, \u201cGraphrnn:\nGenerating realistic graphs with deep auto-regressive models,\u201d in Inter-\nnational conference on machine learning. PMLR, 2018, pp. 5708\u20135717.\n[54] A. Bojchevski, O. Shchur, D. Z\u00a8ugner, and S. G\u00a8unnemann, \u201cNetgan:\nGenerating graphs via random walks,\u201d in International conference on\nmachine learning.\nPMLR, 2018, pp. 610\u2013619.\n[55] Y. Luo, K. Yan, and S. Ji, \u201cGraphdf: A discrete flow model for molecular\ngraph generation,\u201d in International Conference on Machine Learning.\nPMLR, 2021, pp. 7192\u20137203.\n[56] M. Li, E. Krea\u02c7ci\u00b4c, V. K. Potluru, and P. Li, \u201cGraphmaker: Can\ndiffusion models generate large attributed graphs?\u201d arXiv preprint\narXiv:2310.13833, 2023.\n[57] H. Liu, B. Hu, X. Wang, C. Shi, Z. Zhang, and J. Zhou, \u201cConfidence may\ncheat: Self-training on graph neural networks under distribution shift,\u201d\nin Proceedings of the ACM Web Conference 2022, 2022, pp. 1248\u20131258.\n[58] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in\nneural information processing systems, vol. 30, 2017.\n[59] J. Zhao, M. Mathieu, and Y. LeCun, \u201cEnergy-based generative adver-\nsarial network,\u201d arXiv preprint arXiv:1609.03126, 2016.\n[60] D. Rezende and S. Mohamed, \u201cVariational inference with normalizing\nflows,\u201d in International conference on machine learning. PMLR, 2015,\npp. 1530\u20131538.\n",
    "2405.16771": "ARC: A Generalist Graph Anomaly Detector\nwith In-Context Learning\nYixin Liu1,\u2217, Shiyuan Li2,\u2217, Yu Zheng3, Qingfeng Chen2, Chengqi Zhang4, Shirui Pan5\n1Monash University, 2Guangxi University, 3La Trobe University,\n4University of Technology Sydney, 5Griffith University\nyixin.liu@monash.edu, shiy.li@alu.gxu.edu.cn, yu.zheng@latrobe.edu.au\nqingfeng@gxu.edu.cn, chengqi.zhang@uts.edu.au, s.pan@griffth.edu.au\nAbstract\nGraph anomaly detection (GAD), which aims to identify abnormal nodes that differ\nfrom the majority within a graph, has garnered significant attention. However,\ncurrent GAD methods necessitate training specific to each dataset, resulting in high\ntraining costs, substantial data requirements, and limited generalizability when\nbeing applied to new datasets and domains. To address these limitations, this paper\nproposes ARC, a generalist GAD approach that enables a \u201cone-for-all\u201d GAD model\nto detect anomalies across various graph datasets on-the-fly. Equipped with in-\ncontext learning, ARC can directly extract dataset-specific patterns from the target\ndataset using few-shot normal samples at the inference stage, without the need for\nretraining or fine-tuning on the target dataset. ARC comprises three components\nthat are well-crafted for capturing universal graph anomaly patterns: 1) smoothness-\nbased feature Alignment module that unifies the features of different datasets\ninto a common and anomaly-sensitive space; 2) ego-neighbor Residual graph\nencoder that learns abnormality-related node embeddings; and 3) cross-attentive\nin-Context anomaly scoring module that predicts node abnormality by leveraging\nfew-shot normal samples. Extensive experiments on multiple benchmark datasets\nfrom various domains demonstrate the superior anomaly detection performance,\nefficiency, and generalizability of ARC.\n1\nIntroduction\nGraph anomaly detection (GAD) aims to distinguish abnormal nodes that show significant dissimi-\nlarity from the majority of nodes in a graph. GAD has broad applications across various real-world\nscenarios, such as fraud detection in financial transaction networks [1] and rumor detection in social\nnetworks [2]. As a result, GAD has attracted increasing research attention in recent years [3, 4, 5, 6, 7].\nConventional GAD methods employ shallow mechanisms to model node-level abnormality [8, 9, 10];\nhowever, they face limitations in handling high-dimensional features and complex interdependent\nrelations on graphs. Recently, graph neural network (GNN)-based approaches have emerged as the\ngo-to solution for the GAD problem due to their superior performance [4, 6]. Some GNN-based GAD\napproaches regard GAD as a supervised binary classification problem and use specifically designed\nGNN architectures to capture anomaly patterns [6, 11, 12, 13]. Another line of approaches targets the\nmore challenging unsupervised paradigm, employing various unsupervised learning objectives and\nframeworks to identify anomalies without relying on labels [4, 14, 15, 16].\nDespite their remarkable detection performance, the existing GAD approaches follow a \u201cone model\nfor one dataset\u201d learning paradigm (as shown in Fig. 1 (a) and (b)), necessitating dataset-specific\n\u2217Equal Contribution.\nPreprint. Under review.\narXiv:2405.16771v1  [cs.LG]  27 May 2024\nGeneralist\nGAD Model\nTrain\nTrain\nTrain\n\u2026\n\u2026\nGAD\nModel\n\u2026\n\u2026\n(c) Generalist GAD Paradigm (ours)\nTraining Stage\nInference Stage\nSupervised\nGAD Model\nTrain\nGAD\nModel\nUnsupervised\nGAD Model\nTrain\nGAD\nModel\n(a) Supervised GAD Paradigm\n(b) Unsupervised GAD Paradigm\nAbnormal Node\nNormal Node\nUnlabeled Node\nTraining Stage\nInference Stage\nFigure 1: Sketch maps of (a) supervised, (b) unsupervised, and (c) generalist GAD paradigms.\ntraining and ample training data to construct a detection model for each dataset. This learning\nparadigm inherently comes with the following limitations: \u2776Expensive training cost. For each\ndataset, we need to train a specialized GAD model from scratch, which incurs significant costs for\nmodel training, especially when dealing with large-scale graphs. \u2777Data requirements. Training a\nreliable GAD model typically needs sufficient in-domain data, sometimes requiring labels as well.\nThe data requirements pose a challenge when applying GAD to scenarios with sparse data, data\nprivacy concerns, or high label annotation costs. \u2778Poor generalizability. On a new-coming dataset,\nexisting GAD methods require hyperparameter tuning or even model architecture modifications to\nachieve optimal performance, which increases the cost of applying them to new data and domains.\nGiven the above limitations, a natural question arises: Can we train a \u201cone-for-all\u201d GAD model that\ncan generalize to detect anomalies across various graph datasets from different application domains,\nwithout any training on the target data? Following the trend of artificial general intelligence and\nfoundation models, a new paradigm termed \u201cgeneralist anomaly detection\u201d, originating from image\nanomaly detection, is a potential answer to this question [17]. As shown in Fig. 1 (c), in the generalist\nparadigm, we only need to train the GAD model once; afterward, the well-trained generalist GAD\nmodel can directly identify anomalies on diverse datasets, without any re-training or fine-tuning.\nConsidering the diversity of graph data across different domains and datasets, the labels of few-shot\nnormal nodes are required during the inference stage to enable the model to grasp the fundamental\ncharacteristics of the target dataset. Compared to conventional paradigms, the generalist paradigm\neliminates the need for dataset-specific training, resulting in fewer computations, lower data costs,\nand stronger generalizability when applying GAD models to new datasets.\nNevertheless, due to the unique characteristics of graph data and GAD problem, it is non-trivial to\ndesign a generalist GAD approach. The challenge is three-fold: C1 - Feature alignment. Unlike image\ndata, which are typically represented in a consistent RGB feature space, the feature dimensionality\nand semantic space can vary significantly across different graph datasets. Substituting features with\nunified representations generated by large language models may be a potential solution [18]; however,\nthis approach is limited to specific feature semantics and cannot address more general cases [19].\nC2 - Representation encoding. As the core of a generalist GAD model, a GNN-based encoder is\nexpected to learn dataset-agnostic and abnormality-aware node embeddings for anomaly detection.\nHowever, in the absence of universal pre-trained foundation models [17] for graph data, crafting\na potent encoder for a generalist GAD model presents a challenge. C3 - Few-shot sample-guided\nprediction. Existing GAD methods typically focus on single dataset settings, where dataset-specific\nknowledge is embedded in the model through training, enabling it to predict abnormality for each\nnode independently. In contrast, a generalist GAD model should derive such knowledge from a small\nnumber of normal nodes. In this case, how to effectively utilize the few-shot normal samples during\ninference remains an open question.\nTo tackle these challenges, we introduce ARC, a generalist GAD approach based on in-context\nlearning. ARC comprises three meticulously designed modules, each targeting a specific challenge. To\naddress C1, we introduce a smoothness-based feature Alignment module, which not only standardizes\nfeatures across diverse datasets to a common dimensionality but also arranges them in an anomaly-\nsensitive order. To deal with C2, we design an ego-neighbor Residual graph encoder. Equipped\nwith a multi-hop residual-based aggregation scheme, the graph encoder learns attributes that indicate\nhigh-order affinity and heterophily, capturing informative and abnormality-aware embeddings across\ndifferent datasets. Last but not least, to solve C3, we propose a cross-attentive in-Context anomaly\nscoring module. Following the in-context learning schema, we treat the few-shot normal nodes\nas context samples and utilize a cross-attention block to reconstruct the embeddings of unlabeled\nsamples based on the context samples. Then, the reconstruction distance can serve as the anomaly\nscore for each unlabeled node. In summary, this paper makes the following contributions:\n2\n\u2022 Problem. We, for the first time, propose to investigate the generalist GAD problem, aiming to detect\nanomalies from various datasets with a single GAD model, without dataset-specific fine-tuning.\n\u2022 Methodology. We propose a novel generalist GAD method ARC, which can detect anomalies in\nnew graph datasets on-the-fly via in-context learning based on few-shot normal samples.\n\u2022 Experiments. We conduct extensive experiments to validate the anomaly detection capability,\ngeneralizability, and efficiency of ARC across multiple benchmark datasets from various domains.\n2\nRelated Work\nIn this section, we offer a brief review of pertinent related works, with a more extensive literature\nreview available in Appendix A.\nAnomaly Detection. Anomaly detection (AD) aims to identify anomalous samples that deviate\nfrom the majority of samples [20]. Mainstream AD methods focus on unsupervised settings and\nemploy various unsupervised techniques to build the models [21, 22, 23, 24, 25, 26]. To enhance\nthe generalizability of AD methods across diverse datasets, RegAD [27] considers few-shot setting\nand trains a single generalizable model capable of being applied to new in-domain data without\nre-training or fine-tuning. WinCLIP [28] utilizes visual-language models (VLMs, e.g., CLIP [29])\nwith well-crafted text prompts to perform zero/few-shot AD for image data. InCTRL [17], as the first\ngeneralist AD approach, integrates in-context learning and VLMs to achieve domain-agnostic image\nAD with a single model. However, due to their heavy reliance on pre-trained vision encoders/VLMs\nand image-specific designs, these approaches excel in AD for image data but face challenges when\napplied to graph data.\nGraph Anomaly Detection (GAD). In this paper, we focus on the node-level AD on graphs and refer\nto it as \u201cgraph anomaly detection (GAD)\u201d following [6, 30, 31]. While shallow methods [8, 9, 10]\nshow limitations in handling complex real-world graphs [4], the advanced approaches are mainly\nbased on GNNs [32]. The GNN-based approaches can be divided into supervised and unsupervised\napproaches [3, 5, 7]. Supervised GAD approaches assume that the labels of both normal and\nanomalous nodes are available for model training [7]. Hence, related studies mainly introduce\nGAD methods in a binary classification paradigm [6, 11, 12, 13, 33, 34]. In contrast, unsupervised\nGAD approaches do not require any labels for model training. They employ several unsupervised\nlearning techniques to learn anomaly patterns on graph data, including data reconstruction [4, 31, 35],\ncontrastive learning [14, 36, 37], and other auxiliary objectives [15, 16, 38]. Nevertheless, all the\nabove methods adhere to the conventional paradigm of \u201cone model for one dataset\u201d. Although some\nGAD approaches [39, 40] can handle cross-domain scenarios, their requirement for high correlation\n(e.g., aligned node features) between source and target datasets limits their generalizability. Differing\nfrom existing methods, our proposed ARC is a \u201cone-for-all\u201d GAD model capable of identifying\nanomalies across target datasets from diverse domains, without the need for re-training or fine-tuning.\nIn-Context Learning (ICL). ICL enables a well-trained model to be effectively (fine-tuning-free)\nadapted to new domains, datasets, and tasks based on minimal in-context examples (a.k.a. context\nsamples), providing powerful generalization capability of large language models (LLMs) [41, 42, 43]\nand computer vision (CV) models [17, 44, 45, 46]. Two recent approaches, PRODIGY [47] and\nUniLP [48] attempt to use ICL for GNNs to solve the node classification and link prediction tasks,\nrespectively. However, how to use ICL to deal with the generalist GAD problem where only normal\ncontext samples are available still remains open.\n3\nProblem Statement\nNotations. Let G = (V, E, X) be an attributed graph with n nodes and m edges, where V =\n{v1, \u00b7 \u00b7 \u00b7 , vn} and E are the set of nodes and edges, respectively. The node-level attributes are\nincluded by feature matrix X \u2208Rn\u00d7d, where each row Xi indicates the feature vector for node vi.\nThe inter-node connectivity is represented by an adjacency matrix A \u2208{0, 1}n\u00d7n, where the i, j-th\nentry Aij = 1 means vi and vj are connected and vice versa.\nConventional GAD Problem. GAD aims to differentiate abnormal nodes Va from normal nodes\nVn within a given graph G = (V, E, X), where Va and Vn satisfy Va \u222aVn = V, Va \u2229Vn = \u2205, and\n|Va| \u226a|Vn|. An anomaly label vector y \u2208{0, 1}n can be used to denote the abnormality of each\nnode, where the i-th entry yi = 1 iff v \u2208Va and yi = 0 iff v \u2208Vn. Formally, the goal of GAD\n3\n\u2026\n\u2026\nSmoothness-Based \nFeature Alignment\nEgo-Neighbor Residual \nGraph Encoder\nProp.\nProp.\nMLP\nMLP\nMLP\nShared\nWeight\nShared\nWeight\nCross-Attentive In-Context \nAnomaly Scoring\nQ\nK\n0.82\n0.30\n0.28\n0.23\n0.21\nFeature \nProjection\nSmoothness-\nBased\nFeature \nSorting\nFigure 2: The overall pipeline of ARC, the proposed generalist GAD approach.\nis to learn an anomaly scoring function (i.e., GAD model) f : V \u2192R such that f(v\u2032) > f(v) for\n\u2200v\u2032 \u2208Va and \u2200v \u2208Vn. In the conventional GAD setting of \u201cone model for one dataset\u201d, the GAD\nmodel f is optimized on the target graph dataset D = (G, y) with a subset of anomaly labels (in\nsupervised setting) or without labels (in unsupervised setting). After sufficient training, the model f\ncan identify anomalies within the target graph G during the inference phase.\nGeneralist GAD Problem. In this paper, we investigate the generalist GAD problem, wherein we\naim to develop a generalist GAD model capable of detecting abnormal nodes across diverse graph\ndatasets from various application domains without any training on the specific target data. Formally,\nwe define the generalist GAD setting, aligning it with its counterpart in image AD as introduced by\nZhu et al. [17]. Specifically, let Ttrain = {D(1)\ntrain, \u00b7 \u00b7 \u00b7 , D(N)\ntrain} be a collection of training datasets,\nwhere each D(i)\ntrain = (G(i)\ntrain, y(i)\ntrain) is a labeled dataset from an arbitrary domain. We aim to\ntrain a generalist GAD model f on Ttrain, and f is able to identify anomalies within any test graph\ndataset D(i)\ntest \u2208Ttest, where Ttest = {D(1)\ntest, \u00b7 \u00b7 \u00b7 , D(N \u2032)\ntest } is a collection of testing datasets. Note\nthat Ttrain \u2229Ttest = \u2205and the datasets in Ttrain and Ttest can be drawn from different distributions\nand domains. Following [17], we adopt a \u201cnormal few-shot\u201d setting during inference: for each D(i)\ntest,\nonly a handful of nk normal nodes (nk \u226an) are available, and the model f is expected to predict\nthe abnormality of the rest nodes without re-training and fine-tuning.\n4\nARC: A generalist GAD approach\nIn this section, we introduce ARC, a generalist GAD approach capable of identifying anomalies\nacross diverse graph datasets without the need for specific fine-tuning. The overall pipeline of ARC is\ndemonstrated in Fig. 2. Firstly, to align the features of different datasets, we introduce a smoothness-\nbased feature alignment module (Sec. 4.1), which not only projects features onto a common plane\nbut also sorts the dimensions in an anomaly-sensitive order. Next, to capture abnormality-aware\nnode embeddings, we propose a simple yet effective GNN model termed ego-neighbor residual\ngraph encoder (Sec. 4.2), which constructs node embeddings by combining residual information\nbetween an ego node and its neighbors. Finally, to leverage knowledge from few-shot context samples\nfor predicting node-level abnormality, we introduce a cross-attentive in-context anomaly scoring\nmodule (Sec. 4.3). Using the cross-attention block, the model learns to reconstruct query node\nembeddings based on context node embeddings. Ultimately, the drift distance between the original\nand reconstructed query embeddings can quantify the abnormality of each node.\n4\n4.1\nSmoothness-Based Feature Alignment\nGraph data from diverse domains typically have different features, characterized by differences in\ndimensionality and unique meanings for each dimension. For example, features in a citation network\nusually consist of textual and meta-information associated with each paper, whereas in a social\nnetwork, the features may be the profile of each user. Therefore, in the first step, we need to align the\nfeatures into a shared feature space. To achieve this, we introduce the feature alignment module in\nARC, consisting of two phases: feature projection, which aligns dimensionality, and smoothness-\nbased feature sorting, which reorders features according to their smoothness characteristics.\nFeature Projection. At the first step of ARC, we employ a feature projection block to unify the feature\ndimensionality of multiple graph datasets [19]. Specifically, given a feature matrix X(i) \u2208Rn(i)\u00d7d(i)\nof D(i) \u2208Ttrain \u222aTtest, the feature projection is defined by a linear mapping:\n\u02dcX(i) \u2208Rn(i)\u00d7du = Proj\n\u0010\nX(i)\u0011\n= X(i)W(i),\n(1)\nwhere \u02dcX(i) is the projected feature matrix for D(i), du is a predefined projected dimension shared\nacross all datasets, and W(i) \u2208Rd(i)\u00d7du is a dataset-specific linear projection weight matrix. To\nmaintain generality, W(i) can be defined using commonly used dimensionality reduction approaches\nsuch as singular value decomposition [49] (SVD) and principal component analysis [50] (PCA).\nSmoothness-Based Feature Sorting. Although feature projection can align dimensionality, the\nsemantic meaning of each projected feature across different datasets remains distinct. Considering\nthe difficulty of semantic-level matching without prior knowledge and specific fine-tuning [18, 19],\nin this paper, we explore an alternative pathway: aligning features based on their contribution to\nanomaly detection tasks. Through analytical and empirical studies, we pinpoint that the smoothness\nof each feature is strongly correlated with its contribution to GAD. Building on this insight, in ARC,\nwe propose to sort the features according to their contribution as our alignment strategy.\nFrom the perspective of graph signal processing, Tang et al. [6] have demonstrated that the inverse of\nthe low-frequency energy ratio monotonically increases with the anomaly degree. In other words, high-\nfrequency graph signals tend to play a more significant role in detecting anomalies. Similar findings\nhave also been observed from the standpoint of spatial GNNs [34, 51], where heterophily information\nhas been shown to be crucial in discriminating anomalies. Motivated by these findings, can we\ndevelop a metric to gauge the contribution of each feature to GAD based on its frequency/heterophily?\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n25\n50\n75\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(a) Cora\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n25\n50\n75\nDOMINANT\nCoLA\nTAM\n(b) Facebook\nFigure 3: AUROC on data with 5 groups of features.\nConsidering\nits\ncorrelation\nto\nfre-\nquency [52] and heterophily [53], in this\npaper, we select feature-level smooth-\nness as the measure for contribution.\nFormally, given a graph G = (V, E, X)\nwith a normalized feature matrix X,\nthe smoothness of the k-th feature\ndimension is defined as:\nsk(X) = \u22121\n|E|\nX\n(vi,vj)\u2208E\n(Xik \u2212Xjk)2 ,\n(2)\nwhere a lower sk indicates a significant change in the k-th feature between connected nodes, implying\nthat this feature corresponds to a high-frequency graph signal and exhibits strong heterophily.\nTo verify whether smoothness can indicate the contribution of features in GAD, we further conduct\nempirical analysis (experimental setup and more results can be found in Appendix B). Concretely,\nwe sort the raw features of each dataset based on the smoothness sk and divide them into 5 groups\naccording to the percentile of sk. Then, we train different GAD models using each group of features\nseparately, and the performance is shown in Fig. 3 and 8. On both datasets, a model-agnostic\nobservation is that the features with lower sk are more helpful in discriminating anomalies. The\nconsistent trend demonstrates the effectiveness of sk as an indicator of the role of features in GAD.\nIn light of this, given the projected features of different datasets, we can align their feature spaces\nby rearranging the permutation of features based on the descending order of sk w.r.t. each projected\n5\nfeature. For all datasets, the feature in the first column is the one with the lowest sk, which deserves\nmore attention by ARC; conversely, features with less contribution (i.e. higher sk) are placed at the\nend. In this way, the GNN-based model can learn to filter graph signals with different smoothness\nlevels automatically and predict anomalies accordingly. During inference, the smoothness-related\ninformation remains transferable because we adhere to the same alignment strategy.\n4.2\nEgo-Neighbor Residual Graph Encoder\nOnce the features are aligned, we employ a GNN-based graph encoder to learn node embeddings that\ncapture both semantic and structural information for each node. The learned embedding can be utilized\nto predict the abnormality of the corresponding node with the downstream anomaly scoring module.\nA naive solution is directly employing commonly used GNNs, such as GCN [54] or GAT [55], as the\ngraph encoder. However, due to their low-pass filtering characteristic, these GNNs face difficulty in\ncapturing abnormality-related patterns that are high-frequency and heterophilic [6, 34]. Moreover,\nmost GNNs, including those tailored for GAD, tend to prioritize capturing node-level semantic\ninformation while disregarding the affinity patterns of local subgraphs [16]. Consequently, employing\nexisting GNN models as the encoder may overemphasize dataset-specific semantic knowledge, but\noverlook the shared anomaly patterns (i.e. local node affinity) across different datasets.\nTo address the above issues, we design an ego-neighbor residual graph encoder for ARC. Equipped\nwith a residual operation, the encoder can capture multi-hop affinity patterns of each node, providing\nvaluable and comprehensive information for anomaly identification. Similar to the \u201cpropagation then\ntransformation\u201d GNN architecture in SGC [56], our graph encoder consists of three steps: multi-hop\npropagation, shared MLP-based transformation, and ego-neighbor residual operation. In the first two\nsteps, we perform propagation on the aligned feature matrix X\u2032 = X[0] for L iterations, and then\nconduct transformation on the raw and propagated features with a shared MLP network:\nX[l] = \u02dcAX[l\u22121],\nZ[l] = MLP\n\u0010\nX[l]\u0011\n,\n(3)\nwhere l \u2208{0, \u00b7 \u00b7 \u00b7 , L}, X[l] is the propagated feature matrix at the l-th iteration, Z[l] is the trans-\nformed representation matrix at the l-th iteration, and \u02dcA is the normalized adjacency matrix [54, 56].\nNote that, unlike most GNNs that only consider the features/representations after L-iter propaga-\ntion, here we incorporate both the raw features and intermediate propagated features and transform\nthem into the same representation space. After obtaining Z[0], \u00b7 \u00b7 \u00b7 , Z[L], we calculate the residual\nrepresentations by taking the difference between Z[l] (1 \u2264l \u2264L) and Z[0], and then concatenate the\nmulti-hop residual representations to form the final embeddings:\nR[l] = Z[l] \u2212Z[0],\nH = [R[1]|| \u00b7 \u00b7 \u00b7 ||R[L]],\n(4)\nwhere R[l] is the residual matrix at the l-th iteration, H \u2208Rn\u00d7de is the output embedding matrix,\nand || denotes the concatenation operator.\nDiscussion. Compared to existing GNNs, our graph encoder offers the following advantages. Firstly,\nwith the residual operation, the proposed encoder emphasizes the difference between the ego node and\nits neighbors rather than ego semantic information. This approach allows for the explicit modeling of\nlocal affinity through the learned embeddings. Since local affinity is a crucial indicator of abnormality\nand this characteristic can be shared across diverse datasets [16], the learned embeddings can offer\nvaluable discriminative insights for downstream prediction. Second, the residual operation performs\nas a high-pass filter on the graph data, aiding ARC in capturing more abnormality-related attributes,\ni.e., high-frequency signals and local heterophily. Moreover, unlike existing approaches [14, 16]\nthat only consider 1-hop affinity, our encoder also incorporates higher-order affinity through the\nmulti-hop residual design, which enables ARC to capture more complex graph anomaly patterns.\nMore discussion and comparison to the existing GNNs/GAD methods are conducted in Appendix C.\n4.3\nCross-Attentive In-Context Anomaly Scoring\nTo utilize the few-shot normal samples (denoted by context nodes) to predict the abnormality of the\nremaining nodes (denoted by query nodes), in ARC, we devise an in-context learning module with a\ncross-attention mechanism for anomaly scoring. The core idea of our in-context learning module\nis to reconstruct the node embedding of each query node using a cross-attention block to blend\nthe embeddings of context nodes. Then, the drift distance between the original and reconstructed\nembeddings of a query node can serve as the indicator of its abnormality.\n6\nSpecifically, we partition the embedding matrix H into two parts by indexing the corresponding\nrow vectors: the embeddings of context nodes Hk \u2208Rnk\u00d7de and the embeddings of query nodes\nHq \u2208Rnq\u00d7de. Then, a cross-attention block is utilized to reconstruct each row of Hq through a\nlinear combination of Hk:\nQ = HqWq,\nK = HkWk,\n\u02dcHq = Softmax\n\u0012QK\u22a4\n\u221ade\n\u0013\nHk,\n(5)\nwhere Q \u2208Rnq\u00d7de and K \u2208Rnk\u00d7de are the query and key matrices respectively, Wq and Wk are\nlearnable parameters, and \u02dcHq is the reconstructed query embedding matrix. Note that, unlike the con-\nventional cross-attention blocks [57, 58, 59] that further introduce a value matrix V, our block directly\nmultiplies the attention matrix with Hk. This design ensures that \u02dcHq is in the same embedding space\nas Hq and Hk. Thanks to this property, given a query node vi, we can calculate its anomaly score\nf(vi) by computing the L2 distance between its query embedding vector Hqi and the corresponding\nreconstructed query embedding vector \u02dcHqi, i.e., f(vi) = d(Hqi, \u02dcHqi) =\nr\nPde\nj=1\n\u0010\nHqij \u2212\u02dcHqij\n\u00112\n.\nContext Node Embedding \nQuery Node Embedding \nRec. Query Node Embedding \n1\n2\n3\n4\n5\n4\n3\n2\n5\n1\n(a) Case I\n(b) Case II\nFigure 4: Toy examples of\nquery embeddings (\u2022), recon-\nstructed query embeddings (\u2022),\nand context embeddings (\u25a0).\nDiscussion. The design of cross-attentive in-context anomaly scor-\ning follows a basic assumption: normal query nodes have similar\npatterns to several context nodes, and hence their embeddings can\nbe easily represented by the linear combination of context node\nembeddings. Consequently, given a normal node, its original and re-\nconstructed embeddings can be close to each other in the embedding\nspace. In contrast, abnormal nodes may display distinct patterns\ncompared to normal ones, making it difficult to depict their corre-\nsponding abnormal query embeddings using context embeddings.\nAs a result, their drift distance si can be significantly larger. Fig. 4\nprovides examples for the scenarios of (a) single-class normal and\n(b) multi-class normal. In both cases, the drift distance (\u279e) can be a\nsignificant indicator for distinguishing anomaly (5) from normal nodes (1~4). Interestingly, if the\nattention matrix assigns uniform weights to all context nodes, then our scoring module becomes a\none-class classification model [21]. This property ensures the anomaly detection capability of ARC\neven without extensive training. A detailed discussion is conducted in Appendix D.\nModel Training.\nTo optimize ARC on training datasets Ttrain, we employ a marginal cosine\nsimilarity loss to minimize the drift distance of normal nodes while maximizing the drift distance of\nabnormal nodes. Specifically, given graph data with anomaly labels, we randomly select nk normal\nnodes as context nodes and sample an equal number of normal and abnormal nodes as query nodes.\nThen, given a query node vi with embedding Hqi, reconstructed embedding \u02dcHqi, and anomaly\nlabel yi, the sample-level loss function can be written by:\nL =\n\uf8f1\n\uf8f2\n\uf8f3\n1 \u2212cos\n\u0010\nHqi, \u02dcHqi\n\u0011\n,\nif yi = 0\nmax\n\u0010\n0, cos\n\u0010\nHqi, \u02dcHqi\n\u0011\n\u2212\u03f5\n\u0011\n,\nif yi = 1\n(6)\nwhere cos(\u00b7, \u00b7) and max(\u00b7, \u00b7) denote the cosine similarity and maximum operation, respectively, and\n\u03f5 is a margin hyperparameter. Detailed algorithmic description and complexity analysis of ARC can\nbe found in Appendix E.\n5\nExperiments\n5.1\nExperimental Setup\nDatasets. To learn generalist GAD models, we train the baseline methods and ARC on a group of\ngraph datasets and test on another group of datasets. For comprehensive evaluations, we consider\ngraph datasets spanning a variety of domains, including social networks, citation networks, and\ne-commerce co-review networks, each of them with either injected anomalies or real anomalies [7, 14,\n16]. Inspired by [48], we train the models on the largest dataset of each type and conduct testing on\nthe remaining datasets. Specifically, the training datasets Ttrain comprise PubMed, Flickr, Questions,\n7\nTable 1: Anomaly detection performance in terms of AUROC (in percent, mean\u00b1std). Highlighted\nare the results ranked first, second, and third. \u201cRank\u201d indicates the average ranking over 8 datasets.\nMethod\nCora\nCiteSeer\nACM\nBlogCatalog\nFacebook\nWeibo\nReddit\nAmazon\nRank\nSupervised - Pre-Train Only\nGCN\n59.64\u00b18.30\n60.27\u00b18.11\n60.49\u00b19.65\n56.19\u00b16.39\n29.51\u00b14.86\n76.64\u00b117.69\n50.43\u00b14.41\n46.63\u00b13.47\n8.9\nGAT\n50.06\u00b12.65\n51.59\u00b13.49\n48.79\u00b12.73\n50.40\u00b12.80\n51.88\u00b12.16\n53.06\u00b17.48\n51.78\u00b14.04\n50.52\u00b117.22\n10.0\nBGNN\n42.45\u00b111.57\n42.32\u00b111.82\n44.00\u00b113.69\n47.67\u00b18.52\n54.74\u00b125.29\n32.75\u00b135.35\n50.27\u00b13.84\n52.26\u00b13.31\n11.1\nBWGNN\n54.06\u00b13.27\n52.61\u00b12.88\n67.59\u00b10.70\n56.34\u00b11.21\n45.84\u00b14.97\n53.38\u00b11.61\n48.97\u00b15.74\n55.26\u00b116.95\n9.0\nGHRN\n59.89\u00b16.57\n56.04\u00b19.19\n55.65\u00b16.37\n57.64\u00b13.48\n44.81\u00b18.06\n51.87\u00b114.18\n46.22\u00b12.33\n49.48\u00b117.13\n9.8\nUnsupervised - Pre-Train Only\nDOMINANT\n66.53\u00b11.15\n69.47\u00b12.02\n70.08\u00b12.34\n74.25\u00b10.65\n51.01\u00b10.78\n92.88\u00b10.32\n50.05\u00b14.92\n48.94\u00b12.69\n5.8\nCoLA\n63.29\u00b18.88\n62.84\u00b19.52\n66.85\u00b14.43\n50.04\u00b13.25\n12.99\u00b111.68\n16.27\u00b15.64\n52.81\u00b16.69\n47.40\u00b17.97\n9.5\nHCM-A\n54.28\u00b14.73\n48.12\u00b16.80\n53.70\u00b14.64\n55.31\u00b10.57\n35.44\u00b113.97\n65.52\u00b112.58\n48.79\u00b12.75\n43.99\u00b10.72\n11.4\nTAM\n62.02\u00b12.39\n72.27\u00b10.83\n74.43\u00b11.59\n49.86\u00b10.73\n65.88\u00b16.66\n71.54\u00b10.18\n55.43\u00b10.33\n56.06\u00b12.19\n5.6\nUnsupervised - Pre-Train & Fine-Tune\nDOMINANT\n72.23\u00b10.34\n74.69\u00b10.32\n74.34\u00b10.12\n74.61\u00b10.04\n49.92\u00b10.55\n92.21\u00b10.10\n52.14\u00b15.06\n59.06\u00b12.80\n3.6\nCoLA\n67.62\u00b14.26\n70.75\u00b13.42\n69.11\u00b10.67\n62.49\u00b13.38\n64.70\u00b118.86\n31.55\u00b16.02\n58.12\u00b10.67\n52.51\u00b16.66\n5.4\nHCM-A\n56.45\u00b14.93\n55.54\u00b14.07\n57.69\u00b13.59\n55.10\u00b10.29\n36.57\u00b110.72\n71.89\u00b12.79\n49.15\u00b12.72\n42.20\u00b10.55\n10.1\nTAM\n62.56\u00b12.10\n76.54\u00b11.33\n86.29\u00b11.57\n57.69\u00b10.88\n76.26\u00b13.70\n71.73\u00b10.16\n56.62\u00b10.49\n57.13\u00b11.59\n3.4\nOurs\nARC\n85.33\u00b10.44\n90.64\u00b10.32\n79.27\u00b10.17\n74.81\u00b10.28\n69.57\u00b11.47\n89.24\u00b10.35\n58.95\u00b11.25\n69.77\u00b13.88\n1.5\nand YelpChi, while the testing datasets Ttest consist of Cora, CiteSeer, ACM, BlogCatalog, Facebook,\nWeibo, Reddit, and Amazon. For detailed information, please refer to Appendix F.1.\nBaselines. We compare ARC with both supervised and unsupervised methods. Supervised methods\ninclude two conventional GNNs, i.e., GCN [54] and GAT [55], and three state-of-the-art GNNs\nspecifically designed for GAD, i.e., BGNN [60], BWGNN [6], and GHRN [34]. Unsupervised\nmethods include four representative approaches with distinct designs, including the generative method\nDOMINANT [4], the contrastive method CoLA [14], the hop predictive method HCM-A [15], and\nthe affinity-based method TAM [16]. For detailed information, refer to Appendix F.2.\nEvaluation and Implementation. Following [7, 16, 61], we employ AUROC and AUPRC as our\nevaluation metrics for GAD. We report the average AUROC/AUPRC with standard deviations across\n5 trials. We train ARC on all the datasets in Ttrain jointly, and evaluate the model on each dataset in\nTtest in an in-context learning manner (nk = 10 as default). For the supervised baselines, we follow\nthe same training and testing procedure (denoted as \u201cpre-train only\u201d), since no labeled anomaly\nis available for fine-tuning. For the unsupervised baselines, we consider two settings: \u201cpre-train\nonly\u201d and \u201cpre-train & fine-tune\u201d. In the latter, we additionally conduct dataset-specific fine-tuning\nwith a few epochs. To standardize the feature space in baseline methods, we utilize either learnable\nprojection or random projection as an adapter between the raw feature and the model input layer.\nWe utilize random search to determine the optimal hyperparameters for both the baselines and ARC.\nSince our goal is to train generalist GAD models, we do not perform dataset-specific hyperparameter\nsearch, but instead use the same set of hyperparameters for all testing datasets. More implementation\ndetails can be found in Appendix F.3.\n5.2\nExperimental Results\nPerformance Comparison. Table 1 shows the comparison results of ARC with baseline meth-\nods in terms of AUROC. Results in AUPRC are provided in Appendix G.1. We have the fol-\nlowing observations.\n\u2776ARC demonstrates strong anomaly detection capability in the gener-\nalist GAD scenario, without any fine-tuning.\nSpecifically, ARC achieves state-of-the-art per-\nformance on 5 out of 8 datasets and demonstrates competitive performance on the remainder.\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n84\n85\n86\nAUROC (%)\n45\n50\nAUPRC (%)\nAUROC\nAUPRC\n(a) Cora\n2\n4\n6\n8\n10\n15\n20\n30\n40\n50 100\nNumber of Context Nodes nk\n66\n68\n70\nAUROC (%)\n6\n8\n10\nAUPRC (%)\nAUROC\nAUPRC\n(b) Facebook\nFigure 5: Performance with varying #context nodes.\nOn several datasets, ARC demonstrates\nsignificant improvement compared to the\nbest baseline (e.g., \u219118.1% on Cora,\n\u219118.4% on CiteSeer, and \u219118.1% on Ama-\nzon). \u2777Simply pre-training the dataset-\nspecific GAD methods typically results\nin poor generalization capability to new\ndatasets. Specifically, the AUROC of the\nmajority of \u201cpre-train only\u201d approaches is\nclose to random guessing (50%) or even\nlower. \u2778With dataset-specific fine-tuning,\n8\nthe baseline methods achieve better performance in the majority of cases. However, the improve-\nment can be minor or even negative in some cases, demonstrating the limitations of fine-tuning.\nAdditionally, we observe that their performance is sometimes lower than the reported results from\ntraining models from scratch [4, 14, 16], indicating potential risk of negative transfer within the\n\"pre-train & fine-tune\" paradigm. \u2779Unsupervised baselines (except HCM-A) generally outperform\nthe supervised ones, highlighting the difficulty of training a generalist GAD model using the binary\nclassification paradigm.\nEffectiveness of #Context Nodes. To investigate how the number of context nodes nk affects the\nperformance of ARC during inference, we vary nk within the range of 2 to 100. The results are\nshown in Fig. 5 (more results are in Appendix G.2). From the figure, we observe that the performance\nof ARC increases as more context nodes are involved, demonstrating its capability to leverage these\nlabeled normal nodes with in-context learning. Furthermore, we can conclude that ARC is also\nlabel-efficient: when nk \u226510, the performance gain from using more context nodes becomes minor;\nmoreover, even when nk is extremely small, ARC can still perform well on the majority of datasets.\nTable 2: AUROC of ARC and its variants.\nVariant\nCora\nCite.\nACM\nBlog.\nFace.\nWei.\nRed.\nAma.\nARC\n85.33\n90.64\n79.27\n74.81\n69.57\n89.24\n58.95\n69.77\nw/o A\n78.46\n83.76\n78.53\n74.44\n65.29\n90.66\n53.81\n65.95\nw/o R\n30.35\n29.30\n60.74\n60.56\n13.82\n97.32\n46.52\n54.83\nw/o C\n49.98\n50.69\n52.88\n72.01\n47.95\n49.34\n47.82\n44.64\nAblation Study. To verify the effective-\nness of each component of ARC, we make\ncorresponding modifications to ARC and\ndesigned three variants: 1) w/o A: using\nrandom projection to replace smoothness-\nbased feature alignment; 2) w/o R: us-\ning GCN to replace ego-neighbor residual\ngraph encoder; and 3) w/o C: using binary classification-based predictor and loss to replace cross-\nattentive in-context anomaly scoring. The results are demonstrated in Table 2 (full results are in\nAppendix G.3). From the results, we can conclude that all three components significantly contribute\nto the performance. Among them, the in-context anomaly scoring module has a significant impact, as\nthe performance of w/o C is close to random guessing on most datasets. The residual graph encoder\nalso has a significant impact on the final performance. Notably, Weibo dataset is an exception where\nthe GCN encoder performs better. A possible reason is that the Weibo dataset exhibits different\nanomaly patterns compared to the others.\nGCN\nGAT\nBGNN\nBWGNN\nGHRN\nDOMI.\nCoLA\nHCM-A\nTAM\nARC\nDOMI.\nCoLA\nHCM-A\nTAM\n10\n\u22121\n10\n0\n10\n1\n10\n2\nRun Time (s)\nInference\nFine-tune (per epoch)\nFigure 6: Time comparison.\nEfficiency Analysis. To assess the runtime efficiency of ARC, we\ncompare the inference and fine-tuning time on the ACM dataset.\nAs depicted in Fig. 6, ARC demonstrates comparable runtime\nperformance with the fastest GNNs (e.g., GCN and BWGNN),\nand significantly outperforms the unsupervised methods in terms\nof efficiency. Additionally, we observe that dataset-specific fine-\ntuning consumes more time compared to inference.\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(a) Cora\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(b) Facebook\nFigure 7: Visualization results.\nVisualization. To investigate the weight allocation mechanism\nof the cross-attention module in ARC, we visualize the atten-\ntion weights between context nodes and query nodes in Fig.7\n(additional results are in Appendix G.4). From Fig. 7(a), it is\nevident that ARC tends to assign uniform attention weights to\nnormal nodes, leading to reconstructed embeddings that closely\nresemble the average embedding of the context nodes. Con-\nversely, anomalies are reconstructed using a combination of 1\nor 2 context nodes, suggesting that their embeddings are far-\nther from the center. This allocation aligns with the case of\n\u201csingle-class normal\u201d in Fig. 4(a). Differently, in Fig. 7(b),\nwe observe that each normal query node is assigned to several\ncontext nodes following two fixed patterns, corresponding to\nthe case of \u201cmulti-class normal\u201d in Fig. 4(b). In summary, the cross-attention module enables ARC\nto adapt to various normal/anomaly distribution patterns, enhancing its generalizability.\n6\nConclusion\nIn this paper, we take the first step towards addressing the generalist GAD problem, aiming to\ndetect anomalies across diverse graph datasets with a \u201cone-for-all\u201d GAD model, without requiring\n9\ndataset-specific fine-tuning. We introduce ARC, a novel and well-crafted in-context learning-based\ngeneralist GAD approach, capable of identifying anomalies on-the-fly using only few-shot normal\nnodes. Extensive experiments on real-world datasets from various domains demonstrate the detection\nprowess, generalizability, and efficiency of ARC compared to existing approaches. One limitation is\nthat ARC can only use normal context samples during inference but cannot directly utilize abnormal\ncontext samples, even when they are available. A potential future direction could involve developing\ngeneralist GAD methods that utilize context samples containing both anomalies and normal instances.\nReferences\n[1] Ranran Li, Zhaowei Liu, Yuanqing Ma, Dong Yang, and Shuaijie Sun. Internet financial fraud detection\nbased on graph learning. Ieee Transactions on Computational Social Systems, 2022.\n[2] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor\ndetection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI\nconference on artificial intelligence, volume 34, pages 549\u2013556, 2020.\n[3] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman Akoglu.\nA comprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on Knowledge\nand Data Engineering, 35(12):12012\u201312038, 2021.\n[4] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks.\nIn Proceedings of the 2019 SIAM International Conference on Data Mining, pages 594\u2013602. SIAM, 2019.\n[5] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu Chen,\nHao Peng, Kai Shu, et al. Bond: Benchmarking unsupervised outlier node detection on static attributed\ngraphs. Advances in Neural Information Processing Systems, 35:27021\u201327035, 2022.\n[6] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection. In\nInternational Conference on Machine Learning, pages 21076\u201321089. PMLR, 2022.\n[7] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and benchmarking\nsupervised graph anomaly detection. Advances in Neural Information Processing Systems, 36, 2024.\n[8] Bryan Perozzi and Leman Akoglu. Scalable anomaly ranking of attributed neighborhoods. In Proceedings\nof the 2016 SIAM International Conference on Data Mining, pages 207\u2013215. SIAM, 2016.\n[9] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection in\nattributed networks. In IJCAI, volume 17, pages 2152\u20132158, 2017.\n[10] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, Qinghua Zheng, et al. Anomalous: A joint modeling\napproach for anomaly detection on attributed networks. In IJCAI, volume 18, pages 3513\u20133519, 2018.\n[11] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph neu-\nral network-based fraud detectors against camouflaged fraudsters. In Proceedings of the 29th ACM\ninternational conference on information & knowledge management, pages 315\u2013324, 2020.\n[12] Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong Li. Spam review detection with graph convolutional\nnetworks. In Proceedings of the 28th ACM International conference on information and knowledge\nmanagement, pages 2703\u20132711, 2019.\n[13] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose: a\ngnn-based imbalanced learning approach for fraud detection. In Proceedings of the web conference 2021,\npages 3168\u20133177, 2021.\n[14] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly detection on\nattributed networks via contrastive self-supervised learning. IEEE transactions on neural networks and\nlearning systems, 33(6):2378\u20132392, 2021.\n[15] Tianjin Huang, Yulong Pei, Vlado Menkovski, and Mykola Pechenizkiy. Hop-count based self-supervised\nanomaly detection on attributed networks. In Joint European conference on machine learning and\nknowledge discovery in databases, pages 225\u2013241. Springer, 2022.\n[16] Hezhe Qiao and Guansong Pang. Truncated affinity maximization: One-class homophily modeling for\ngraph anomaly detection. In Advances in Neural Information Processing Systems, volume 36, 2023.\n[17] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with\nfew-shot sample prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2024.\n[18] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen, and Muhan Zhang. One\nfor all: Towards training one graph model for all classification tasks. In International Conference on\nLearning Representations, 2024.\n10\n[19] Haihong Zhao, Aochuan Chen, Xiangguo Sun, Hong Cheng, and Jia Li. All in one and one for all: A\nsimple yet effective method towards cross-domain graph pretraining. arXiv preprint arXiv:2402.09834,\n2024.\n[20] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for anomaly\ndetection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.\n[21] Lukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexander\nBinder, Emmanuel M\u00fcller, and Marius Kloft. Deep one-class classification. In International conference on\nmachine learning, pages 4393\u20134402. PMLR, 2018.\n[22] Sachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, and Prateek Jain. Drocc: Deep\nrobust one-class classification. In International conference on machine learning, pages 3711\u20133721. PMLR,\n2020.\n[23] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter Gehler.\nTowards total recall in industrial anomaly detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 14318\u201314328, 2022.\n[24] Chong Zhou and Randy C Paffenroth. Anomaly detection with robust deep autoencoders. In Proceedings\nof the 23rd ACM SIGKDD international conference on knowledge discovery and data mining, pages\n665\u2013674, 2017.\n[25] Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs.\nUnsupervised anomaly detection with generative adversarial networks to guide marker discovery. In\nInternational conference on information processing in medical imaging, pages 146\u2013157. Springer, 2017.\n[26] Vikash Sehwag, Mung Chiang, and Prateek Mittal. SSD: A unified framework for self-supervised outlier\ndetection. In International Conference on Learning Representations, 2021.\n[27] Chaoqin Huang, Haoyan Guan, Aofan Jiang, Ya Zhang, Michael Spratling, and Yan-Feng Wang. Regis-\ntration based few-shot anomaly detection. In European Conference on Computer Vision, pages 303\u2013319.\nSpringer, 2022.\n[28] Jongheon Jeong, Yang Zou, Taewan Kim, Dongqing Zhang, Avinash Ravichandran, and Onkar Dabeer.\nWinclip: Zero-/few-shot anomaly classification and segmentation. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 19606\u201319616, 2023.\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish\nSastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from\nnatural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR,\n2021.\n[30] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative and\ncontrastive self-supervised learning for graph anomaly detection. IEEE Transactions on Knowledge and\nData Engineering, 35(12):12220\u201312233, 2021.\n[31] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang, Yuan Wang, and Shan Xue. Comga:\nCommunity-aware attributed graph anomaly detection. In Proceedings of the Fifteenth ACM International\nConference on Web Search and Data Mining, pages 657\u2013665, 2022.\n[32] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A compre-\nhensive survey on graph neural networks. IEEE transactions on neural networks and learning systems,\n32(1):4\u201324, 2020.\n[33] Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via\nbernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021.\n[34] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Addressing\nheterophily in graph anomaly detection: A perspective of graph spectrum. In Proceedings of the ACM Web\nConference 2023, pages 1528\u20131538, 2023.\n[35] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly detection on\nattributed networks. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pages 5685\u20135689. IEEE, 2020.\n[36] Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, and Zhibin Dong. Graph\nanomaly detection via multi-scale contrastive learning networks with augmented view. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37, pages 7459\u20137467, 2023.\n[37] Bo Chen, Jing Zhang, Xiaokang Zhang, Yuxiao Dong, Jian Song, Peng Zhang, Kaibo Xu, Evgeny\nKharlamov, and Jie Tang. Gccad: Graph contrastive learning for anomaly detection. IEEE Transactions on\nKnowledge and Data Engineering, 2022.\n[38] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng Jiang. Error-bounded\ngraph anomaly loss for gnns. In Proceedings of the 29th ACM International Conference on Information &\nKnowledge Management, pages 1873\u20131882, 2020.\n11\n[39] Kaize Ding, Kai Shu, Xuan Shan, Jundong Li, and Huan Liu. Cross-domain graph anomaly detection.\nIEEE Transactions on Neural Networks and Learning Systems, 33(6):2406\u20132415, 2021.\n[40] Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. Cross-domain graph\nanomaly detection via anomaly-aware contrastive alignment. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 37, pages 4676\u20134684, 2023.\n[41] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.\nAdvances in neural information processing systems, 33:1877\u20131901, 2020.\n[42] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for\nfew-shot learning. Advances in neural information processing systems, 35:23716\u201323736, 2022.\n[43] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei.\nLanguage models are general-purpose interfaces. arXiv preprint arXiv:2206.06336, 2022.\n[44] Ting Chen, Saurabh Saxena, Lala Li, Tsung-Yi Lin, David J Fleet, and Geoffrey E Hinton. A unified\nsequence interface for vision tasks. Advances in Neural Information Processing Systems, 35:31333\u201331346,\n2022.\n[45] Peng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-\nsequence learning framework. In International Conference on Machine Learning, pages 23318\u201323340.\nPMLR, 2022.\n[46] Alexander Kolesnikov, Andr\u00e9 Susano Pinto, Lucas Beyer, Xiaohua Zhai, Jeremiah Harmsen, and Neil\nHoulsby. Uvim: A unified modeling approach for vision with learned guiding codes. Advances in Neural\nInformation Processing Systems, 35:26295\u201326308, 2022.\n[47] Qian Huang, Hongyu Ren, Peng Chen, Gregor Kr\u017emanc, Daniel Zeng, Percy S Liang, and Jure Leskovec.\nProdigy: Enabling in-context learning over graphs. Advances in Neural Information Processing Systems,\n36, 2024.\n[48] Kaiwen Dong, Haitao Mao, Zhichun Guo, and Nitesh V Chawla. Universal link predictor by in-context\nlearning. arXiv preprint arXiv:2402.07738, 2024.\n[49] Gilbert W Stewart. On the early history of the singular value decomposition. SIAM review, 35(4):551\u2013566,\n1993.\n[50] Herv\u00e9 Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary reviews:\ncomputational statistics, 2(4):433\u2013459, 2010.\n[51] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Alleviating\nstructural distribution shift in graph anomaly detection. In Proceedings of the Sixteenth ACM International\nConference on Web Search and Data Mining, pages 357\u2013365, 2023.\n[52] Yushun Dong, Kaize Ding, Brian Jalaian, Shuiwang Ji, and Jundong Li. Adagnn: Graph neural networks\nwith adaptive frequency response filter. In Proceedings of the 30th ACM international conference on\ninformation & knowledge management, pages 392\u2013401, 2021.\n[53] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen Chang,\nand Doina Precup. Revisiting heterophily for graph neural networks. Advances in neural information\nprocessing systems, 35:1362\u20131375, 2022.\n[54] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In\nInternational Conference on Learning Representations, 2017.\n[55] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\nGraph attention networks. In International Conference on Learning Representations, 2018.\n[56] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying\ngraph convolutional networks. In International conference on machine learning, pages 6861\u20136871. PMLR,\n2019.\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems,\n30, 2017.\n[58] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution\nimage synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pages 10684\u201310695, 2022.\n[59] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training\nwith frozen image encoders and large language models. In International conference on machine learning,\npages 19730\u201319742. PMLR, 2023.\n12\n[60] Sergei Ivanov and Liudmila Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural\nnetworks. In International Conference on Learning Representations, 2021.\n[61] Guansong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. Toward deep supervised\nanomaly detection: Reinforcement learning from partially labeled anomaly data. In Proceedings of the\n27th ACM SIGKDD conference on knowledge discovery & data mining, pages 1298\u20131308, 2021.\n[62] Thomas Defard, Aleksandr Setkov, Angelique Loesch, and Romaric Audigier. Padim: a patch distribution\nmodeling framework for anomaly detection and localization. In International Conference on Pattern\nRecognition, pages 475\u2013489. Springer, 2021.\n[63] Vitjan Zavrtanik, Matej Kristan, and Danijel Sko\u02c7caj. Reconstruction by inpainting for visual anomaly\ndetection. Pattern Recognition, 112:107706, 2021.\n[64] Yiru Zhao, Bing Deng, Chen Shen, Yao Liu, Hongtao Lu, and Xian-Sheng Hua. Spatio-temporal au-\ntoencoder for video anomaly detection. In Proceedings of the 25th ACM international conference on\nMultimedia, pages 1933\u20131941, 2017.\n[65] Julian Wyatt, Adam Leach, Sebastian M Schmon, and Chris G Willcocks. Anoddpm: Anomaly detection\nwith denoising diffusion probabilistic models using simplex noise. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 650\u2013656, 2022.\n[66] Thomas Schlegl, Philipp Seeb\u00f6ck, Sebastian M Waldstein, Georg Langs, and Ursula Schmidt-Erfurth.\nf-anogan: Fast unsupervised anomaly detection with generative adversarial networks. Medical image\nanalysis, 54:30\u201344, 2019.\n[67] Chun-Liang Li, Kihyuk Sohn, Jinsung Yoon, and Tomas Pfister. Cutpaste: Self-supervised learning for\nanomaly detection and localization. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, pages 9664\u20139674, 2021.\n[68] Choubo Ding, Guansong Pang, and Chunhua Shen. Catching both gray and black swans: Open-set\nsupervised anomaly detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pages 7388\u20137398, 2022.\n[69] Yiwei Lu, Frank Yu, Mahesh Kumar Krishna Reddy, and Yang Wang. Few-shot scene-adaptive anomaly\ndetection. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part V 16, pages 125\u2013141. Springer, 2020.\n[70] Tri Cao, Jiawen Zhu, and Guansong Pang. Anomaly detection under distribution shift. In Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, pages 6511\u20136523, 2023.\n[71] Abhishek Aich, Kuan-Chuan Peng, and Amit K Roy-Chowdhury. Cross-domain video anomaly detection\nwithout target domain adaptation. In Proceedings of the IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 2579\u20132591, 2023.\n[72] Wenchao Yu, Wei Cheng, Charu C Aggarwal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk: A\nflexible deep embedding approach for anomaly detection in dynamic networks. In Proceedings of the 24th\nACM SIGKDD international conference on knowledge discovery & data mining, pages 2672\u20132681, 2018.\n[73] Yixin Liu, Shirui Pan, Yu Guang Wang, Fei Xiong, Liang Wang, Qingfeng Chen, and Vincent CS Lee.\nAnomaly detection in dynamic graphs via transformer. IEEE Transactions on Knowledge and Data\nEngineering, 35(12):12081\u201312094, 2021.\n[74] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly\ndetection by glocal knowledge distillation. In Proceedings of the fifteenth ACM international conference\non web search and data mining, pages 704\u2013714, 2022.\n[75] Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, and Shirui Pan. Towards self-interpretable\ngraph-level anomaly detection. Advances in Neural Information Processing Systems, 36, 2023.\n[76] Mingda Chen, Jingfei Du, Ramakanth Pasunuru, Todor Mihaylov, Srini Iyer, Veselin Stoyanov, and\nZornitsa Kozareva. Improving in-context few-shot learning via self-supervised training. arXiv preprint\narXiv:2205.01703, 2022.\n[77] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in\ncontext. arXiv preprint arXiv:2110.15943, 2021.\n[78] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via\nimage inpainting. Advances in Neural Information Processing Systems, 35:25005\u201325017, 2022.\n[79] Yulong Pei, Tianjin Huang, Werner van Ipenburg, and Mykola Pechenizkiy. Resgcn: Attention-based\ndeep residual modeling for anomaly detection on attributed networks. Machine Learning, 111(2):519\u2013541,\n2022.\n[80] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classification in network data. AI magazine, 29(3):93\u201393, 2008.\n13\n[81] Jie Tang, Jing Zhang, Limin Yao, Juanzi Li, Li Zhang, and Zhong Su. Arnetminer: extraction and mining\nof academic social networks. In Proceedings of the 14th ACM SIGKDD international conference on\nKnowledge discovery and data mining, pages 990\u2013998, 2008.\n[82] Lei Tang and Huan Liu. Relational learning via latent social dimensions. In Proceedings of the 15th ACM\nSIGKDD international conference on Knowledge discovery and data mining, pages 817\u2013826, 2009.\n[83] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user\nexpertise through online reviews. In Proceedings of the 22nd international conference on World Wide Web,\npages 897\u2013908, 2013.\n[84] Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review networks and\nmetadata. In Proceedings of the 21th acm sigkdd international conference on knowledge discovery and\ndata mining, pages 985\u2013994, 2015.\n[85] Shijie Zhang, Hongzhi Yin, Tong Chen, Quoc Viet Nguyen Hung, Zi Huang, and Lizhen Cui. Gcn-based\nuser representation learning for unifying robust recommendation and fraudster detection. In Proceedings\nof the 43rd international ACM SIGIR conference on research and development in information retrieval,\npages 689\u2013698, 2020.\n[86] Arjun Mukherjee, Vivek Venkataraman, Bing Liu, and Natalie Glance. What yelp fake review filter might\nbe doing? In Proceedings of the international AAAI conference on web and social media, volume 7, pages\n409\u2013418, 2013.\n[87] Zhiming Xu, Xiao Huang, Yue Zhao, Yushun Dong, and Jundong Li. Contrastive attributed network\nanomaly detection with data augmentation. In Pacific-Asia Conference on Knowledge Discovery and Data\nMining, pages 444\u2013457. Springer, 2022.\n[88] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal\ninteraction networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, pages 1269\u20131278, 2019.\n[89] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A\ncritical look at the evaluation of gnns under heterophily: Are we really making progress? arXiv preprint\narXiv:2302.11640, 2023.\n[90] Kaize Ding, Jundong Li, and Huan Liu.\nInteractive anomaly detection on attributed networks.\nIn\nProceedings of the twelfth ACM international conference on web search and data mining, pages 357\u2013365,\n2019.\n[91] David B Skillicorn. Detecting anomalies in graphs. In 2007 IEEE Intelligence and Security Informatics,\npages 209\u2013216. IEEE, 2007.\n[92] Xiuyao Song, Mingxi Wu, Christopher Jermaine, and Sanjay Ranka. Conditional anomaly detection. IEEE\nTransactions on knowledge and Data Engineering, 19(5):631\u2013645, 2007.\nA\nDetailing Related Work\nAnomaly Detection. The objective of anomaly detection (AD) is to identify anomalous samples\nthat deviate from the majority of samples [20]. Due to the difficulty of collecting labeled anomaly\ndata, mainstream AD methods mainly focus on unsupervised settings. To capture anomaly patterns\nwithout guidance by annotated labels, existing studies employ several advanced techniques to learn\npowerful AD models, such as one-class classification [21, 22], distance measurement [23, 62], data\nreconstruction [24, 63, 64], generative models [25, 65, 66], and self-supervised learning [26, 67]. For\nexample, DeepSVDD [21] introduces a fully deep one-class classification objective for unsupervised\nanomaly detection, optimizing a data-enclosing hypersphere in output space to extract common\nfactors of variation and demonstrating theoretical properties such as the v-property. AnoDDPM [65],\na simplex noise-based approach for anomaly detection, enhances anomaly capture with a partial\ndiffusion strategy and multiscale simplex noise processing, facilitating faster inference and training on\nhigh-resolution images. While effective, these approaches are tailored to identify abnormal samples\nwithin a predetermined target dataset (i.e., the dataset for training), restricting their generalizability to\nnew domains.\nCross-Dataset Anomaly Detection. Recently, some advanced AD methods aim to transcend dataset\nlimitations, enhancing their generalizability across diverse datasets. A research line aims to address\nthe AD problem under domain or distribution shifts [68, 69, 70, 71]; however, these approaches\nrequire domain relevance between the source and target datasets, thus limiting their generalizability.\nTo enable the model to better understand the patterns in target datasets, a viable approach is to\nincorporate a few-shot setting, allowing access to a limited number of normal samples from the\n14\ntarget datasets. Under the few-shot setting, RegAD [27] is a pioneering approach that trains a single\ngeneralizable model capable of being applied to new in-domain data without re-training or fine-\ntuning. WinCLIP [28] utilizes visual-language models (VLMs, e.g., CLIP [29]) with well-crafted\ntext prompts to perform zero/few-shot AD for image data. InCTRL [17], as the first generalist AD\napproach, integrates in-context learning and VLMs to achieve domain-agnostic image AD with a\nsingle model. However, due to their heavy reliance on pre-trained vision encoders/VLMs and image-\nspecific designs, these approaches excel in anomaly detection for image data but face challenges\nwhen applied to graph data.\nAnomaly Detection on Graph Data. Based on the granularity of anomaly samples within graph\ndata, existing AD approaches can be primarily categorized into three classes: node-level [4, 6],\nedge-level [72, 73], and graph-level [74, 75] AD. Due to its broad real-world applications, node-level\nAD receives the most research attention [3]. In this paper, we focus on the node-level AD and refer to\nit as \u201cgraph anomaly detection (GAD)\u201d, following the convention of most previous papers [6, 30, 31].\nEarly GAD methods aimed to detect anomalies through shallow mechanisms. For example, AMEN [8]\ndetects anomalies by utilizing the attribute correlations of nodes in each ego-network on the attribute\nnetwork. In addition, residual analysis is another common method to measure the anomalies of nodes\non an attribute network. In particular, Radar [9] characterizes the residuals of attribute information\nand their coherence with network information for anomaly detection. Further, ANOMALOUS [10]\nproposes joint learning of attribute selection and anomaly detection as a whole based on CUR\ndecomposition and residual analysis. Despite the success of these methods on low-dimensional\nattribute network data, they do not work well when the network has a complex structure and high-\ndimensional attributes due to the limitations of their shallow mechanisms.\nTo overcome the limitations of shallow approaches, recently, GNN-based methods have become\nthe de facto solution for GAD tasks. Existing GNN-based GAD approaches can be divided into\ntwo research lines: supervised GAD and unsupervised GAD [3, 5, 7]. Supervised GAD approaches\nassume that the labels of both normal and anomalous nodes are available for model training [7].\nHence, related studies mainly focus on improving graph convolutional operators, model architectures,\nand supervised objective functions, to leverage labels to learn node-level anomaly patterns [6, 11,\n12, 13, 33, 34]. For example, the spatial GNN has been redesigned mainly in terms of message\npassing and aggregation mechanisms. In particular, considering that GNN-based fraud detectors fail\nto effectively identify fraudsters in disguise, CARE-GNN [11] uses a label-aware similarity metric\nand introduces reinforcement learning to aggregate selected neighbors with different relationships\nto counteract disguises. Concurrently, spectral GNNs, relate graphical anomalies to high-frequency\nspectral distributions. Tang et al. [6] analyzed anomalies for the first time from the perspective\nof the spectral spectrum of the graph and proposed the Beta wavelet GNN (BWGNN), which has\nspectrally and spatially localized band-pass filters to better capture anomalies. In addition, anomalies\nare usually associated with high-frequency components in the spectral representation of a graph.\nTherefore, GHRN [34] prunes inter-class edges by emphasizing the high-frequency components of\nthe graph, which can effectively isolate the anomalous nodes and thus obtain better anomaly detection\nperformance.\nIn contrast, unsupervised GAD approaches do not require any labels for model training. Similar\nto unsupervised AD for image data, unsupervised GAD approaches employ several unsupervised\nlearning techniques to learn anomaly patterns on graph data, including data reconstruction [4, 31,\n35], contrastive learning [14, 36, 37], and other auxiliary objectives [15, 16, 38]. For example,\nDOMINANT [4] is a reconstruction-based approach that employs a graph convolution autoencoder\nto reconstruct both the adjacency and attribute matrices simultaneously, assessing node abnormality\nthrough a weighted sum of the reconstruction error terms. Similarly, ComGA [31] is a community-\naware attribute GAD framework based on tailored GCNs to capture local, global, and structural\nanomalies. CoLA [14], the first contrastive self-supervised learning framework for GAD, samples\nnovel contrast instance pairs in an unsupervised manner, utilizing contrastive learning to capture\nlocal information. HCM-A [15] integrates both local and global contextual information, employs\nhop count prediction as a self-supervised task, and utilizes Bayesian learning to enhance anomaly\nidentification. TAM [16] optimizes the proposed anomaly metric (affinity) on the truncated graph\nend-to-end, considering one-class homophily and local affinity. Nevertheless, all the above methods\nadhere to the conventional paradigm of \u201cone model for one dataset\u201d.\n15\nAlthough some GAD approaches [39, 40] can handle cross-domain scenarios, their requirement\nfor high correlation (e.g., aligned node features) between source and target datasets limits their\ngeneralizability. Differing from existing methods, our proposed ARC is a \u201cone-for-all\u201d GAD model\ncapable of identifying anomalies across target datasets from diverse domains, without the need for\nre-training or fine-tuning.\nIn-Context Learning. In-context learning (ICL) can be effectively adapted to new tasks based\non minimal in-context examples, providing a powerful generalization capability of large language\nmodels (LLMs) [41, 42, 43]. For example, Brown et al. [41] demonstrate the remarkable ability of\nlanguage models to perform diverse tasks with minimal training examples. Through few-shot learning,\nthese models exhibit robustness and adaptability across various domains, exhibiting their potential\nas general-purpose learners in natural language processing (NLP). Further, given that pre-training\nobjectives are not specifically optimized for ICL [76], Min et al. introduced MetaICL [77] as a solution\nto bridge the divide between pre-training and downstream ICL utilization. This method involves\ncontinuously training the pre-trained LLMs across diverse tasks using demonstration examples,\nthereby enhancing its few-shot capabilities. In contrast, supervised in-context fine-tuning [76]\nsuggests constructing self-supervised training data aligned with the ICL format to utilize the original\ncorpora for warm-up in downstream tasks. They converted raw text into input-output pairs and\ninvestigated four self-supervised objectives, such as masked token prediction and classification tasks.\nICL has also generated research attention in the field of computer vision (CV), where it has been\nwidely used in vision tasks by designing specialized discretization tokens as prompts [17, 44, 45, 46].\nFor instance, Chen et al. [44] proposed to use a unified interface to represent the output of each task\nas a sequence of discrete tokens, the neural network can be trained for a variety of tasks using a single\nmodel architecture and loss function, thus eliminating the need for customization for specific tasks.\nFurthermore, given a shot prompt as a task description, the sequence output adapts to the prompt\nto generate task-specific results. Differently, Amir et al. [78] proposes an innovative method for\nin-context visual prompting by framing a wide range of vision tasks as grid in-painting problems.\nThis approach leverages image in-painting to generate visual prompts, guiding models to complete\nvarious vision tasks by filling in missing parts of an image based on contextual information, thereby\nenhancing adaptability and performance across different visual challenges.\nVery recently, few studies applied ICL to graph learning and GNNs. As an initial attempt to use\nICL for GNNs, PRODIGY [47] leverages a prompt graph-based framework to conduct few-shot ICL\nfor node-level classification and edge-level prediction tasks. Further, UniLP [48] introduces ICL to\nresolve conflicting connectivity patterns caused by distributional differences between different graphs.\nNevertheless, these methods require context samples in multiple classes for ICL during inference.\nThen, how to leverage one-class context samples with ICL in the scenario of generalist GAD remains\nan open question.\nB\nMotivated Experiments for Smoothness-Based Feature Sorting\nExperimental Setup. To verify whether smoothness can indicate the contribution of features\nto GAD, we conducted motivated experiments. We consider three mainstream GAD methods:\nDOMINANT [4], CoLA [14], and TAM [16]. For each method, we conduct the experiments with\nhyper-parameters reported in the original article across all datasets. At the data pre-processing phase,\nwe first calculate sk based on the raw features and sort them in descending order. According to\nthe new order, we divide the features into 5 groups of feature subsets, denoted by the percentile of\n80%-100%, 60%-80%, \u00b7 \u00b7 \u00b7 , 0%-20%. Subsequently, the five feature subsets are sequentially used as\ninputs for different methods, and the methods are trained to obtain their AUROC values. We statistic\nthe results of 5 random experiments for each method and reported the average AUROC.\nResults. Additional experimental results are shown in Fig. 8. As we can witness in the figure, in\nmost cases, features with lower sk are observed to be more helpful in distinguishing anomalies on\nmost of the datasets. This consistent model-independent trend indicates the effectiveness of sk as an\nindicator of the contribution of each feature in GAD.\n16\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n0\n50\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(a) CiteSeer\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n0\n50\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(b) PubMed\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n0\n50\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(c) BlogCatalog\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n0\n50\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(d) Flickr\n80-100%\n60-80%\n40-60%\n20-40%\n0-20%\nhigh sk \u2192 low sk \n Percentile w.r.t. Smoothness\n0\n50\nAUROC (%)\nDOMINANT\nCoLA\nTAM\n(e) ACM\nFigure 8: AUROC on data with 5 groups of features split by sk.\nC\nDiscussion of Ego-Neighbor Residual Graph Encoder\nIn this section, we discuss the connections and differences between the ego-neighbor residual graph\nencoder in ARC (R-ENC for short) and the existing GNNs and GAD methods.\nR-ENC v.s. Radar [9]. Radar, one of the representative shallow GAD methods, also leverages a\nresidual-based mechanism for anomaly detection. Our proposed R-ENC differs from Radar in the\nfollowing key aspects:\n\u2022 Radar calculates the residual on the original feature space, which inevitably suffers from\nhigh complexity and less representative capability on high-dimensional graph data. In\ncontrast, R-ENC computes residuals based on the output embeddings of a GNN, which\nenables more efficient processing and enhances the representative power of the model. This\napproach allows R-ENC to better capture the underlying structure and anomalies within the\ngraph data.\n\u2022 In Radar, the summation of residual R directly serves as the anomaly score for each node,\nwhere the irrelevant information within several residual entries may hinder the model\u2019s\naccuracy and effectiveness. Differently, in ARC, we specifically use R-ENC to generate\nthe embedding of each node, and employ another learnable scoring module to calculate the\nanomaly score based on these embeddings. In this case, our method can effectively filter out\nnoise and irrelevant features, allowing the scoring module to focus on the most informative\naspects of the node embeddings.\n\u2022 Radar employs the Laplacian matrix to calculate the residual, meaning that only first-order\nresidual information is considered. In contrast, our R-ENC incorporates multi-hop residual\naggregation, enhancing its ability to detect subtle anomalies by considering both local and\nglobal graph structures through high-order residuals.\nR-ENC v.s. ResGCN [79]. ResGCN is a GNN-based GAD approach with a residual-based mech-\nanism. Similar to Radar, ResGCN also uses the summation of residual R as the anomaly score.\nHowever, the inclusion of irrelevant information within several residual entries can impair the model\u2019s\naccuracy and effectiveness. Moreover, ResGCN employs a two-branch design, with the node repre-\nsentation by GCN and residual information by MLP calculated by two different network modules.\nCompared to R-ENC with simpler designs, ResGCN has lower operational efficiency.\n17\nR-ENC v.s. CoLA [14]. CoLA learns the anomaly patterns by maximizing the agreement between\nthe embedding of each node and its neighboring nodes. Specifically, CoLA employs a bilinear\nmodule to compute the agreement score. Such an agreement can also be modeled by the first-order\nresidual in R-ENC, which is computed by the difference between the ego embedding and the 1-hop\naggregated embeddings. Compared to CoLA, a significant advantage of R-ENC is its ability to\ncapture not only first-order residuals but also high-order residuals. This makes R-ENC a more robust\nand comprehensive encoder for graph anomaly detection, addressing a wider range of anomaly\npatterns across different datasets.\nR-ENC v.s. TAM [16]. TAM utilizes local affinity, i.e., the feature-level or embedding-level similarity\nbetween a node and its neighbors, as the indicator of each node\u2019s abnormality. R-ENC, with its\nresidual operation, can also capture local affinity. Specifically, by computing the first-order residual\nbetween an ego node and its 1-hop neighbors, the local affinity can be indicated by the negative\nsummation of the first-order residuals since they are highly correlated. Again, R-ENC can not only\ncapture the first-order affinity but also the high-order affinity with the multi-hop residual operator.\nR-ENC v.s. Heterophily-aware GNNs [34]. Existing studies indicate that a key solution to handle\nthe GAD problem is to enable heterophily-aware graph convolutional operation with high-pass\nfiltering [34]. A feasible filter is graph Laplacian, whose normalized and self-loop added version\ncan be written by L = I \u2212\u02dcA. For R-ENC, its first-order residual can also be viewed as a Laplacian-\nbased graph convolution. Specifically, if we simplify the MLP-based feature transformation as a\nweight matrix W, the ego information can be written by Z[0] = XW, while the representation\nZ[1] = \u02dcAXW. Then, the first-order residual can be written by:\nR[1] = Z[1] \u2212Z[0] = \u02dcAXW \u2212XW = \u2212LXW.\n(7)\nThat is to say, the first-order residual in R-ENC can be regarded as Laplacian-based high-pass filtering\n(note that the negative sign can be fused into the learnable weight W). Such a nice property enables\nARC to capture high-frequency graph signals and heterophily information through residual-based\nembeddings, thereby enhancing its capability to detect anomalies in diverse and complex graph\nstructures.\nD\nDiscussion of Cross-Attentive In-Context Anomaly Scoring\nIn this section, we first introduce the basic definition of one-class classification (OC) model, and\nthen discuss the connection between one-class classification and cross-attentive in-context anomaly\nscoring module (C-AS for short).\nOne-class classification. The core idea of OC model is to measure the abnormality of each sample\naccording to the distance between its representation h and a center representation c [21]. Here c\ncan be a fixed random representation vector or dynamically adjusted as the mean of all samples\u2019\nrepresentation vectors. Formally, the anomaly score by OC model can be written by:\nf(xi) = \u2225\u03d5 (xi) \u2212c\u22252 = \u2225hi \u2212c\u22252 ,\n(8)\nwhere \u03d5(\u00b7) is a neural network model, as defined in Deep SVDD [21]. Intuitively, a normal sample\ntends to have a similar representation to the majority of samples, and hence the distance between its\nrepresentation and c should be closer.\nC-AS as an OC model. In C-AS, we use a cross-attention block to calculate the weighted sum\nof context embeddings Hk into \u02dcHqi for a query node qi. For an initialized model, we assume that\nthe parameters Wq and Wk are random enough, making Q and K become uniform noise matrices.\nIn this case, each entry in the attention matrix T = Softmax\n\u0010\nQK\u22a4\n\u221ade\n\u0011\ncan be\n1\nnk , indicating that\nthe attention matrix assigns uniform weights to all context nodes for all query nodes. Then, all the\nreconstructed query embeddings are equal to the average embedding of context nodes:\n\u02dcHq1 = \u00b7 \u00b7 \u00b7 = \u02dcHqnq = 1\nnk\n1T Hk.\n(9)\n18\nAlgorithm 1: Smoothness-based Feature Alignment\nInput: Graph G.\nParameters :Projected dimension du.\n1 Extract X, E, and V from G\n2 \u02dcX \u2208Rn\u00d7du \u2190Calculate projected features by linear projection via Eq. (1)\n3 for k = 1 : du do\n4\nsk \u2190Calculate feature-level smoothness of the k-th column of \u02dcX via Eq. (2)\n5 end\n6 X\u2032 \u2190Rearrange the permutation of features of \u02dcX based on the descending order of s\n7 Return G = (V, E, X\u2032)\nSince the average context embedding is the center embedding of a group of few-shot normal samples,\nwe can naturally define the center embedding c =\n1\nnk 1T Hk. Recalling that we define the anomaly\nscore as the L2 distance between \u02dcHqi and Hqi, then for all query nodes, the anomaly scoring can be\nrewritten by:\nf(vi) = d(Hqi, \u02dcHqi) = d(Hqi, c) = \u2225Hqi \u2212c\u22252 .\n(10)\nThat is to say, the C-AS module serves as an OC model under random initialization. Note that in\npractice, the attention matrix cannot be so ideal, but it can still assign relevantly average weights for\nthe context embeddings. Such merit ensures that ARC can perform like an OC model, effectively\ndetecting anomalies in the case of single-class normal (Fig. 4 (a)) even without costly training.\nC-AS goes beyond OC model. Thanks to its inherent mechanism, the OC model can effectively\nhandle single-class normal scenarios. However, in the case of multi-class normals (Fig. 4 (b)), a\nsingle center is not sufficient to model multiple normal class centers. Unlike the OC model, C-AS can\naddress this issue through cross-attention. Specifically, the cross-attention block learns to reconstruct\na query embedding by assigning higher weights to several (but not all) context embeddings that are\nclose to the query node. This way, for a query node, the cross-attention block can automatically\nlearn the center of its corresponding normal class, rather than simply using the average context\nembedding. The awareness of multiple normal classes ensures that ARC can handle both single-class\nand multi-class normal cases.\nE\nAlgorithm and Complexity\nE.1\nAlgorithmic description\nThe algorithmic description of the feature alignment in ARC, the training process of ARC, and\ninference process of ARC are summarized in Algo. 1, Algo. 2, and Algo. 3, respectively.\nE.2\nComplexity Analysis\nIn the testing phase, the time complexity consists of two main components: feature alignment and\nmodel inference. For feature alignment, the overall complexity is O(nddu + dum + dulog(du)),\nwhere m = |E| is the number of edges. Here, the first term is used for feature projection, while the\nsecond and third terms are used for smoothness computation and feature reordering, respectively.\nThe model inference is divided into two main parts: embedding generation and anomaly scoring. The\ncomplexity of node embedding generation is O(L(mdu + nduh + nh2)), where the first term is used\nfor feature propagation and the rest of the terms are used for residual encoding by MLP. The anomaly\nscoring, on the other hand, mainly involves cross-attention computation with time complexity of\nO(nqnkh + nqh), where nq is the number of query nodes and nk is the number of context nodes.\n19\nAlgorithm 2: The Training algorithm of ARC\nInput: Training datasets Ttrain.\nParameters :Number of epoch E; Propagation iteration: L.\n1 Initialize model parameters\n2 for D(i) \u2208Ttrain do\n3\nAlign features in G(i) via Algo. 1\n4 end\n5 for e = 1 : E do\n6\nfor D(i) \u2208Ttrain do\n7\nObtain X(i)\u2032, E(i), V(i), y(i) from D(i)\n8\nfor l = 1 : L do\n9\nZ(i),[l] \u2190Propagate and transform X(i)\u2032 = X(i),[0] via Eq. (3)\n10\nR(i),[l] \u2190Calculate residual of Z(i),[l] via Eq. (4)\n11\nend\n12\nH(i) \u2190Concatenate [R(i),[1]|| \u00b7 \u00b7 \u00b7 ||R(i),[L]] via Eq. (4)\n13\nH(i)\nq , H(i)\nk\n\u2190Randomly split query and context node sets and indexing from H(i)\n14\n\u02dcH(i) \u2190Calculate cross attention from H(i)\nq , H(i)\nk via Eq. (5)\n15\nCalculate loss L from \u02dcH(i)\nq , y(i)\nq\nvia Eq. (6)\n16\nUpdate model parameters via gradient descent.\n17\nend\n18 end\nAlgorithm 3: The Inference algorithm of ARC\nInput: Test dataset D with few-shot normal nodes {vk1, \u00b7 \u00b7 \u00b7 , vknk }.\nParameters :Well-trained model weight parameters.\n1 Align features in G via Algo. 1\n2 Obtain X\u2032, E, V from G\n3 for l = 1 : L do\n4\nZ[l] \u2190Propagate and transform X(i)\u2032 = X[0] via Eq. (3)\n5\nR[l] \u2190Calculate residual of Z[l] via Eq. (4)\n6 end\n7 H \u2190Concatenate [R[1]|| \u00b7 \u00b7 \u00b7 ||R[L]] via Eq. (4)\n8 Hq, Hk \u2190Separate query and context node sets and indexing from H\n9 \u02dcHq \u2190Calculate cross attention from Hq, Hk via Eq. (5)\n10 d \u2190Computing the L2 distance between \u02dcHq and Hq\n11 Return d as the anomaly scores f(\u00b7) for query nodes\nF\nDetails of Experimental Setup\nF.1\nDescription of Datasets\nIn total, we considered 12 benchmark datasets. We divide the datasets into 4 groups: \u2776citation\nnetwork with injected anomalies, \u2777social network with injected anomalies, \u2778social network with\nreal anomalies, and \u2779co-review network with real anomalies. Within each type, we consider the\nlargest dataset as one of the training datasets, and the rest datasets as the testing datasets. The detailed\nstatistics of the datasets are shown in Table 3. These datasets are selected from different domains and\nwith injected or real anomalies to ensure that our proposed ARC model learns extensive anomaly\npatterns. The diversity of the above data can maximally ensure that ARC can effectively adapt to new\nand unseen graphs. Specifically, the detailed descriptions for the datasets are given as follows:\n\u2022 Cora, CiteSeer, PubMed [80], and ACM [81] are four citation networks. In these datasets,\nnodes represent scientific publications, while edges denote the citation links between them.\n20\nTable 3: The statistics of datasets.\nDataset\nTrain\nTest\n#Nodes\n#Edges\n#Features\nAvg. Degree\n#Anomaly\n%Anomaly\nCitation network with injected anomalies\nCora\n-\n\u2713\n2,708\n5,429\n1,433\n3.90\n150\n5.53\nCiteSeer\n-\n\u2713\n3,327\n4,732\n3,703\n2.77\n150\n4.50\nACM\n-\n\u2713\n16,484\n71,980\n8,337\n8.73\n597\n3.62\nPubMed\n\u2713\n-\n19,717\n44,338\n500\n4.50\n600\n3.04\nSocial network with injected anomalies\nBlogCatalog\n-\n\u2713\n5,196\n171,743\n8,189\n66.11\n300\n5.77\nFlickr\n\u2713\n-\n7,575\n239,738\n12,047\n63.30\n450\n5.94\nSocial network with real anomalies\nFacebook\n-\n\u2713\n1,081\n55,104\n576\n50.97\n25\n2.31\nWeibo\n-\n\u2713\n8,405\n407,963\n400\n48.53\n868\n10.30\nReddit\n-\n\u2713\n10,984\n168,016\n64\n15.30\n366\n3.33\nQuestions\n\u2713\n-\n48,921\n153,540\n301\n3.13\n1,460\n2.98\nCo-review network with real anomalies\nAmazon\n-\n\u2713\n10,244\n175,608\n25\n17.18\n693\n6.76\nYelpChi\n\u2713\n-\n23,831\n49,315\n32\n2.07\n1,217\n5.10\nEach publication is characterized by a bag-of-words representation for its node attribute\nvector, with the dimensionality determined by the size of the respective dictionary.\n\u2022 BlogCatalog and Flickr [82, 4] stand as typical social blog directories, facilitating user\nconnections through following relationships. Each user is depicted as a node, with inter-node\nlinks symbolizing mutual following. Node attributes encompass the personalized textual\ncontent generated by users within social network, such as blog posts or shared photos with\ntag descriptions.\n\u2022 Amazon and YelpChi [83, 84] are datasets about the relationship between users and\nreviews. Amazon is designed to identify users paid to write fake reviews for products, and\nthree different graph datasets are derived from Amazon using different types of relations\nto construct adjacency matrix [85, 16]. YelpChi aims to identify anomalous reviews on\nYelp.com that unfairly promote or demote products or businesses. Based on [84, 86], three\ndifferent graph datasets derived from Yelp using different connections in user, product\nreview text, and time. In this work, we focus on Amazon-UPU (users who have reviewed at\nleast one of the same product) and YelpChi-RUR (reviews posted by the same user).\n\u2022 Facebook [87] is a social network in which users can build relationships with others and\nshare their friends.\n\u2022 Reddit [88] serves as a forum posts network sourced from the social media platform Reddit,\nwhere users labeled as banned are identified as anomalies. Textual content from posts is\ntransformed into vectors to serve as node attributes.\n\u2022 Weibo [88] dataset encompasses a graph of users and their associated hashtags from the\nTencent Weibo platform. Within a defined temporal window (e.g., 60 seconds), consecutive\nposts by a user are labeled as potentially suspicious behavior. Users engaging in a minimum\nof five such instances are classified as \u201csuspicious\u201d. The raw feature vector includes the\nlocation of a micro-blog post and bag-of-words features.\n\u2022 Questions [89] dataset originates from Yandex Q, a platform dedicated to question-\nanswering. Users represent the nodes, while the connections between them signify the\npresence or absence of a question-and-answer interaction within a one-year timeframe. The\nnode features are derived from the average of the FastText embeddings of the words in the\nuser description, with an additional binary feature indicating users without description.\nAnomaly Injection. For the datasets with injected anomalies, we use the strategy introduced in\n[4, 14] to inject anomalous nodes. Specifically, we inject a set of anomaly combinations for each\ndataset by perturbing the topology and node attributes, respectively [90]. In terms of structural\nperturbation, this is done by generating small cliques of otherwise unrelated nodes as anomalies. The\nintuition for this strategy is that small cliques in the real world are a typical substructure of anomalies,\nwith much more closely linked within the cliques than the mean [91]. Thus for a dataset, we can\nspecify the size of the cliques (i.e., the number of nodes) p and its amount q for anomaly generation.\n21\nSpecifically, randomly sample p nodes from the graph making them fully connected and labeled as\nanomaly nodes. We iteratively repeat the above process q times to inject a total of p \u00d7 q anomalies.\nFinally, we control the number of injected anomalies according to the size of the dataset. In particular,\nwe fix p = 15 and q = 10, 15, 20, 5, 5, 20 on BlogCatalog, Flickr, ACM, Cora, Citeseer, and Pubmed,\nrespectively. On the other hand, for attribute perturbations, we base the schema introduced by [92].\nSpecifically, for each perturbation target node vi, k nodes are randomly sampled in the graph and\ntheir distance from the target node is computed. Then, the node vj with the largest deviation from the\ntarget node vi is selected, and the attribute Xi of the node vi to Xj. We set the number of anomalies\nof the attribute perturbation to p \u00d7 q to maintain the balance of different anomalies. In addition, we\nset k = 50 to ensure that the perturbation magnitude is large enough.\nF.2\nDescription of Baselines\nIn our evaluation, we provide a comprehensive comparsion of ARC with various supervised and\nunsupervised GAD methods. On the supervised side, two classic GNNs are included as well as 3\nstate-of-the-art (SOTA) models specifically tailored for the GAD task. For supervised models, it is\nassumed that the labels of both normal and abnormal nodes can be used for model training. Therefore,\nthe main binary classification task is used to identify the anomalies:\n\u2022 GCN [54], as a seminal model in the field of GNN, is known for its ability to process\ngraph-structured data using neighborhood aggregation, facilitating efficient node feature\nextraction and representation learning.\n\u2022 GAT [55] incorporates the attention mechanism into the GNN framework to achieve dynamic\nweighting of node contributions. It optimizes its attention according to different downstream\ntasks to achieve high-quality node representations.\n\u2022 BGNN [60] is a GNN that combines gradient boost decision trees (GBDT) with GNN for\ngraphs with tabular node features. It utilizes the GBDT to handle heterogeneous features\nwhile the GNN considers the graph structure and significantly improves performance on a\nvariety of graphs with tabular features.\n\u2022 BWGNN [6] has spectral and spatial localized band-pass filters to better handle the \u201cright-\nshift\u201d phenomenon in anomalies, i.e., the distribution of spectral energy is concentrated at\nhigh frequencies rather than at low frequencies.\n\u2022 GHRN [34] is a heterophily-aware supervised GAD method based on graph spectra. By\nemphasizing the high-frequency components of the graph, the method can effectively cut\ndown inter-class edges, thus improving the overall performance of anomaly detection.\nFor the unsupervised alternative, we consider 4 representative SOTA GAD methods, each of them\nbelonging to a sub-type: data reconstruction, contrastive learning, hop-based auxiliary goal, or\naffinity-based auxiliary goal:\n\u2022 DOMINANT [4] combines GCN and deep auto-encoder, and its learning objective is to\nreconstruct the adjacency matrix and node features jointly. It aims to identify structural and\nattribute anomalies based on reconstruction errors.\n\u2022 CoLA [14] is a contrastive self-supervised learning for anomaly detection on graphs with\nnode attributes. The framework captures the relationship between each node and its neigh-\nborhood substructure in an unsupervised manner by sampling novel pairs of contrasting\ninstances and leveraging the local information of the graph.\n\u2022 HCM-A [15] uses hop-count prediction as a self-supervised task to better identify anomalies\nby modeling both local and global context information. In addition, HCM-A designs two new\nanomaly scores and introduces Bayesian learning to train the model to capture anomalies.\n\u2022 TAM [16] is designed based on one-class homophily and local affinity. The learning target of\nTAM is to optimize the proposed anomaly metric (i.e. affinity) end-to-end on the truncated\nadjacency matrix.\nF.3\nDetails of Implementation\nHyper-parameters. We select some key hyper-parameters of ARC through random search within\nspecified grids. Specifically, the random search was performed within the following search space:\n22\nTable 4: Anomaly detection performance in terms of AUPRC (in percent, mean\u00b1std). Highlighted\nare the results ranked first, second, and third. \u201cRank\u201d indicates the average ranking over 8 datasets.\nMethod\nCora\nCiteSeer\nACM\nBlogCatalog\nFacebook\nWeibo\nReddit\nAmazon\nRank\nSupervised - Pre-Train Only\nGCN\n7.41\u00b11.55\n6.40\u00b11.40\n5.27\u00b11.12\n7.44\u00b11.07\n1.59\u00b10.11\n67.21\u00b115.20\n3.39\u00b10.39\n6.96\u00b12.04\n9.6\nGAT\n6.49\u00b10.84\n5.58\u00b10.62\n4.70\u00b10.75\n12.81\u00b12.08\n3.14\u00b10.37\n33.34\u00b19.80\n3.73\u00b10.54\n15.74\u00b117.85\n7.1\nBGNN\n4.90\u00b11.27\n3.91\u00b11.01\n3.48\u00b11.33\n5.73\u00b11.47\n3.81\u00b12.12\n30.26\u00b129.98\n3.52\u00b10.50\n7.51\u00b10.58\n10.5\nBWGNN\n7.25\u00b10.80\n6.35\u00b10.73\n7.14\u00b10.20\n8.99\u00b11.12\n2.54\u00b10.63\n12.13\u00b10.71\n3.69\u00b10.81\n13.12\u00b111.82\n8.6\nGHRN\n9.56\u00b12.40\n7.79\u00b12.01\n5.61\u00b10.71\n10.94\u00b12.56\n2.41\u00b10.62\n28.53\u00b17.38\n3.24\u00b10.33\n7.54\u00b12.01\n8.4\nUnsupervised - Pre-Train Only\nDOMINANT\n12.75\u00b10.71\n13.85\u00b12.34\n15.59\u00b12.69\n35.22\u00b10.87\n2.95\u00b10.06\n81.47\u00b10.22\n3.49\u00b10.44\n6.11\u00b10.29\n6.0\nCoLA\n11.41\u00b13.51\n8.33\u00b13.73\n7.31\u00b11.45\n6.04\u00b10.56\n1.90\u00b10.68\n7.59\u00b13.26\n3.71\u00b10.67\n11.06\u00b14.45\n9.0\nHCM-A\n5.78\u00b10.76\n4.18\u00b10.75\n4.01\u00b10.61\n6.89\u00b10.34\n2.08\u00b10.60\n21.91\u00b111.78\n3.18\u00b10.23\n5.87\u00b10.07\n12.1\nTAM\n11.18\u00b10.75\n11.55\u00b10.44\n23.20\u00b12.36\n10.57\u00b11.17\n8.40\u00b10.97\n16.46\u00b10.09\n3.94\u00b10.13\n10.75\u00b13.10\n5.9\nUnsupervised - Pre-Train & Fine-Tune\nDOMINANT\n21.35\u00b10.74\n23.02\u00b11.55\n22.74\u00b10.95\n35.79\u00b10.63\n3.56\u00b10.15\n77.69\u00b11.43\n3.84\u00b10.74\n7.48\u00b10.46\n3.9\nCoLA\n13.91\u00b15.56\n19.51\u00b13.73\n8.48\u00b10.51\n10.43\u00b11.22\n15.19\u00b111.04\n8.03\u00b11.19\n4.07\u00b10.13\n7.27\u00b11.13\n5.8\nHCM-A\n6.41\u00b11.33\n4.76\u00b10.51\n4.41\u00b10.63\n6.62\u00b10.14\n2.23\u00b10.76\n27.20\u00b15.53\n3.10\u00b10.19\n5.64\u00b10.09\n11.9\nTAM\n13.62\u00b10.53\n18.66\u00b11.41\n58.04\u00b18.17\n13.90\u00b10.53\n11.11\u00b13.20\n16.47\u00b10.08\n3.93\u00b10.09\n11.56\u00b11.80\n4.1\nOurs\nARC\n47.26\u00b10.91\n48.38\u00b10.70\n40.14\u00b10.09\n34.80\u00b10.02\n8.96\u00b11.74\n65.46\u00b10.95\n4.14\u00b10.25\n15.45\u00b14.30\n2.1\n\u2022 Hidden layer dimension: {64, 128, 256, 512, 1024}\n\u2022 Number of MLP layers: {1, 2, 3, 4}\n\u2022 Propagation iteration: {1, 2, 3, 4, 5}\n\u2022 Dropout rate: {0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8}\n\u2022 Learning rate: floats between 10\u22125 and 10\u22122\n\u2022 Weight decay: floats between 10\u22126 and 10\u22123\nImplementation Pipeline. We employ a fixed set of hyper-parameters to build a generalist GAD\nmodel for all datasets. First, we train all the methods (including baselines and ARC) on the training\nset Ttrain with full labels. Then, the methods are evaluated on each dataset from Ttest respectively.\nFor baselines that require fine-tuning, we further conduct dataset-specific tuning at this stage.\nMetrics. Following [7, 16, 61], we employ two popular and complementary evaluation metrics for\nevaluation, including area under the receiver operating characteristic Curve (AUROC) and area under\nthe precision-recall curve (AUPRC). A higher AUROC/AUPRC value indicates better performance.\nWe report the average AUROC/AUPRC with standard deviations across 5 trials.\nComputing Infrastructures. We implemented the proposed ARC using PyTorch 2.1.2, PyTorch\nGeometric (PyG) 2.3.1, and DGL 0.9.0. All experiments were performed on a Linux server with an\nInter Xeon microprocessor E-2288G CPU and a Quadro RTX 6000 GPU.\nG\nSupplemental Experiments\nG.1\nPerformance Comparison in Terms of AUPRC\nIn terms of AUPRC, Table 4 gives comprehensive comparative results with consistent observations\nwith the AUROC results. Specifically, we have the following observations. \u2776ARC still demonstrates\nstrong anomaly detection in generalist GAD scenarios without any fine-tuning. Specifically, ARC\nachieves state-of-the-art performance on three of the eight datasets and demonstrates competitive\nperformance on the remaining datasets. On several datasets, ARC showed significant improvement\nover the best baseline (e.g., \u2191121.4% on Cora, \u2191110.2% on Citeseer). \u2777GAD methods that only\npre-train specific to a dataset usually result in poor generalization to new datasets. Specifically,\nexisting methods perform very erratically on different datasets, which can be attributed to capturing\nonly specific anomaly patterns. \u2778Using dataset-specific fine-tuning, baseline methods can achieve\nbetter performance in most case. However, in some cases the improvement can be small or even\nnegative, demonstrating the limitations of fine-tuning.\n23\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n89\n90\n91\nAUROC (%)\n45.0\n47.5\n50.0\nAUPRC (%)\nAUROC\nAUPRC\n(a) CiteSeer\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n78.5\n79.0\n79.5\nAUROC (%)\n40.0\n40.2\nAUPRC (%)\nAUROC\nAUPRC\n(b) ACM\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n74.5\n75.0\nAUROC (%)\n35.0\n35.5\nAUPRC (%)\nAUROC\nAUPRC\n(c) BlogCatalog\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n88.75\n89.00\n89.25\nAUROC (%)\n65\n66\nAUPRC (%)\nAUROC\nAUPRC\n(d) Weibo\n2\n4\n6\n8\n10 15 20 30 40 50 100\nNumber of Context Nodes nk\n58\n60\nAUROC (%)\n4.00\n4.25\nAUPRC (%)\nAUROC\nAUPRC\n(e) Reddit\n2\n4\n6\n8\n10\n15\n20\n30\n40\n50 100\nNumber of Context Nodes nk\n70\n75\nAUROC (%)\n15\n20\nAUPRC (%)\nAUROC\nAUPRC\n(f) Amazon\nFigure 9: Performance with varying nk on the rest of six datasets.\nTable 5: Performance of ARC and its variants in terms of AUROC.\nVariant\nCora\nCiteSeer\nACM\nBlogCatalog\nFacebook\nWeibo\nReddit\nAmazon\nARC w/o A\n78.46\u00b11.21\n83.76\u00b11.06\n78.53\u00b10.19\n74.44\u00b10.11\n65.29\u00b11.72\n90.66\u00b10.13\n53.81\u00b11.71\n65.95\u00b11.64\nARC w/o R\n30.35\u00b10.51\n29.30\u00b10.24\n60.74\u00b12.90\n60.56\u00b14.54\n13.82\u00b10.42\n97.32\u00b10.45\n46.52\u00b11.83\n54.83\u00b11.48\nARC w/o C\n49.98\u00b10.33\n50.69\u00b10.25\n52.88\u00b10.88\n72.01\u00b10.20\n47.95\u00b10.63\n49.34\u00b10.52\n47.82\u00b10.42\n44.64\u00b11.03\nARC\n85.33\u00b10.44\n90.64\u00b10.32\n79.27\u00b10.17\n74.81\u00b10.28\n69.57\u00b11.47\n89.24\u00b10.35\n58.95\u00b11.25\n69.77\u00b13.88\nG.2\nEffectiveness of Context Sample Number\nFor all test sets, we varied nk in the range of 2 to 100 and the results are shown in Fig. 5 and\nFig. 9. From the figure, we observe that in most cases the performance of ARC increases with the\ninvolvement of more context nodes, which indicates its ability to utilize these labeled normal nodes\nfor context learning. Moreover, even if nk is very small, ARC can still perform well on most datasets.\nG.3\nDetailed Results of Ablation Study\nTo assess the effectiveness of the key design in the ARC, we conducted an ablation study with\nthree variants of the ARC, 1) w/o A: using random projection to replace smoothness-based feature\nalignment; 2) w/o R: using GCN to replace ego-neighbor residual graph encoder; and 3) w/o C: using\nbinary classification-based predictor and loss to replace cross-attentive in-context anomaly scoring.\nAs can be seen from Table 5 and Table 6, the two metrics AUROC and AUPRC of ARC achieved the\nbest in all datasets except Weibo dataset. A possible explanation is that the Weibo dataset exhibits a\nparticular pattern of anomalies. In addition, all three key designs provide significant improvements in\nperformance.\nG.4\nVisualization\nThe attention weights between context nodes and query nodes in other datasets are shown in Fig. 10.\nAs can be seen in Fig. 10, for most of the datasets, the \u201csingle-class normal\u201d case in Fig. 4 (a) is\nmet: ARC tends to assign a uniform attention weight to normal nodes. This results in reconstructed\nembedding that are very similar to the average embedding of context nodes; in contrast, reconstructing\nanomalies using a combination of a few context nodes results in their embedding being farther from\nthe center. Moreover, corresponding to the \u201cmulti-class normal\u201d case in Fig. 4 (b): in Fig. 10 (f),\nit is observed that each normal query nodes that follow two fixed patterns. In summary, the cross-\n24\nTable 6: Performance of ARC and its variants in terms of AUPRC.\nVariant\nCora\nCiteSeer\nACM\nBlogCatalog\nFacebook\nWeibo\nReddit\nAmazon\nARC w/o A\n26.60\u00b11.18\n25.16\u00b11.81\n27.48\u00b11.39\n34.28\u00b10.42\n4.49\u00b10.53\n68.87\u00b10.50\n3.57\u00b10.10\n9.76\u00b11.02\nARC w/o R\n6.77\u00b10.08\n8.06\u00b10.46\n7.74\u00b10.14\n16.65\u00b11.22\n1.36\u00b10.01\n90.37\u00b10.66\n2.95\u00b10.12\n8.34\u00b10.47\nARC w/o C\n8.91\u00b10.31\n10.98\u00b10.21\n18.33\u00b10.31\n33.73\u00b10.26\n3.35\u00b10.14\n13.67\u00b10.09\n3.13\u00b10.05\n5.85\u00b10.10\nARC\n47.26\u00b10.91\n48.38\u00b10.70\n40.14\u00b10.09\n34.80\u00b10.02\n8.96\u00b11.74\n65.46\u00b10.95\n4.14\u00b10.25\n15.45\u00b14.30\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(a) CiteSeer\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(b) ACM\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(c) BlogCatalog\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(d) Facebook\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(e) Weibo\nContext Nodes\nNormal\nAnomaly\nQuery Nodes\n(f) Reddit\nFigure 10: Attention visualization results for more datasets.\nattention module enables ARC to adapt to various normal/abnormal distribution patterns, conferring\nit generality.\n25\n",
    "2308.13821": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n1\nA Survey of Imbalanced Learning on Graphs:\nProblems, Techniques, and Future Directions\nZemin Liu, Yuan Li, Nan Chen, Qian Wang, Bryan Hooi, Bingsheng He\nAbstract\u2014Graphs represent interconnected structures prevalent in a myriad of real-world scenarios. Effective graph analytics, such as\ngraph learning methods, enables users to gain profound insights from graph data, underpinning various tasks including node\nclassification and link prediction. However, these methods often suffer from data imbalance, a common issue in graph data where\ncertain segments possess abundant data while others are scarce, thereby leading to biased learning outcomes. This necessitates the\nemerging field of imbalanced learning on graphs, which aims to correct these data distribution skews for more accurate and\nrepresentative learning outcomes. In this survey, we embark on a comprehensive review of the literature on imbalanced learning on\ngraphs. We begin by providing a definitive understanding of the concept and related terminologies, establishing a strong foundational\nunderstanding for readers. Following this, we propose two comprehensive taxonomies: (1) the problem taxonomy, which describes the\nforms of imbalance we consider, the associated tasks, and potential solutions; (2) the technique taxonomy, which details key strategies\nfor addressing these imbalances, and aids readers in their method selection process. Finally, we suggest prospective future directions\nfor both problems and techniques within the sphere of imbalanced learning on graphs, fostering further innovation in this critical area.\nIndex Terms\u2014Imbalanced learning on graphs, graph representation learning, class imbalance, structure imbalance.\n\u2726\n1\nINTRODUCTION\nG\nRAPHS, or networks, refer to interconnected structures\nthat are commonly found in real-world scenarios,\nwhere entities often interact with each other. Graphs are\nubiquitous in various domains, such as social networks\non platforms like Facebook, citation networks on DBLP,\ne-commerce networks on Amazon, etc. The prevalence of\ngraph structures has sparked significant interest in graph\nanalytics, which aims to leverage the inherent information\nin graphs for downstream tasks such as node classification,\nlink prediction, and graph classification.\nEarly studies in graph analytics often rely on conven-\ntional techniques like feature engineering [1]\u2013[3], which\ncan be computationally expensive and require significant\neffort. However, with the advent of graph representation\nlearning [4], [5], new opportunities have emerged for graph\nanalytics. Graph representation learning aims to embed the\nstructures of graphs (e.g., nodes, edges, or graphs) into\na low-dimensional space while preserving their structural\ninformation. Prior graph embedding approaches [4] such as\nDeepwalk [6], LINE [7], and node2vec [8] rely on contextual\nconnectivity between nodes to capture their proximity for\nnode representation learning. Recently, more attention has\nshifted to graph neural networks (GNNs) [9]\u2013[12], a family\nof graph representation learning approaches that capitalize\non neighborhood aggregation. They conduct neighborhood\naggregation to recursively pass and receive messages along\nthe edges in an end-to-end manner, to effectively encode the\ngraph structure. Consequently, GNNs have achieved state-\nWe\nhave\ncurated\na\ncollection\nof\nawesome\nliterature\non\nimbalanced\nlearning\non\ngraphs\non\nGitHub:\nhttps://github.com/Xtra-Computing/\nAwesome-Literature-ILoGs.\n\u2022\nZ. Liu, Y. Li, N. Chen, Q. Wang, B. Hooi, and B. He are with School of\nComputing, National University of Singapore. E-mail: {zeminliu, li.yuan,\nnanchen, qiansoc, c}@nus.edu.sg, {hebs}@comp.nus.edu.sg.\nof-the-art performance in many downstream tasks.\nImbalance phenomenon. While these graph representation\nlearning approaches can be effective, like many machine\nlearning models, they often require a large amount of\nlabeled data for training. Real-world data, however, fre-\nquently display imbalanced distributions, where some seg-\nments have an abundance of data and others do not. For\ninstance, in a classification task, the distribution of labeled\ndata\u2014such as images or documents\u2014may skew towards\ncertain classes, resulting in imbalanced label distribution.\nThis data imbalance can notably impact the training process.\nSpecifically, the model tends to be well-trained on high-\nresource groups that have ample data, while underperform-\ning on low-resource groups that are data-limited, leading to\nunclear class boundaries [13]\u2013[15]. As a result, the model\nperformance is usually satisfactory within high-resource\ngroups, but degrades within low-resource groups [16]\u2013[18].\nThus, severe challenges of imbalanced learning have gar-\nnered significant attention in the literature and addressing\nthose challenges is of paramount importance [17], [19]\u2013[22].\nImbalanced learning on graphs. To tackle the imbalance\nissues, various approaches have been proposed in the fields\nof vision and language [23], [24]. However, graph data\ndiffers from them in the fact that samples are inherently\nnon-i.i.d. and multifarious, with diverse structural aspects\n(e.g., node degrees). Directly applying these approaches to\naddress imbalance issues on graphs may not be feasible\n[25]\u2013[27]. Thus, graph learning faces the challenge of new\nimbalance types beyond those found in these settings.\nThe considerable impact of imbalance issues on the\nperformance of graph-based tasks has recently attracted\nsignificant research interest [28]\u2013[30], as illustrated in Fig. 1.\nThe annually increasing volume of literature mirrors the\nescalating importance and substantial impact of address-\narXiv:2308.13821v2  [cs.LG]  29 Aug 2023\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n2\n2018\n2019\nNode-level Class\nMeta-GNN\n2020\nNode-level Class\nGPN\nDR-GCN\n2021\nNode-level Class\nRALE\n2022\n2023\nNode-level Class\nGeometer\nTENT\nNode-level Class\nRSDNE\nImGAGN\nGraphSMOTE\nGraphENS\nDGPN\nEdge-level Class\nNEDM\nEdge-level Class\nGLAD\nGraph-level Class\nAS-MAML\nGraph-level Class\nPAR\nMeta-MGNN\nGraph-level Class\nFAITH\nTemp-GFSM\nNode-level Struct.\nmeta-tail2vec\nNode-level Struct.\nDemo-Net\nGEN\nNode-level Struct.\nTail-GNN\nSL-DSGCN\nNode-level Struct.\nCold-Brew\nReNode\nEdge-level Struct.\nGMatching\nEdge-level Struct.\nMetaR\nEdge-level Struct.\nFAAN\nFSRL\nEdge-level Struct.\nKEFDA\nMetaP\nEdge-level Struct.\nFAEA\nMeta-KGR\nFIRE\nADK-KG\nZSGAN\nZSLRC\nGraph-level Struct.\nSOLT-GNN\nUD-GNN\nST-GSFL\nGMatching\nImpro. LR - KT\nProblems\nTechniques\nMeta-GNN\nImpro. LR - KT\nRSDNE\nBalan. H/LR - AC\nDemo-Net\nMetaR\nMeta-KGR\nGPN\nImpro. LR - KT\nAS-MAML\nDAT\nImpro. LR - AD\nDR-GCN\nBalan. H/LR - AC\nRALE\nImpro. LR - KT\nTail-GNN\nZSLRC\nImpro. LR - AD\nDGPN\nReNode\nBalan. H/LR - DRR\nGraphSMOTE\nBalan. H/LR - SDG\nImGAGN\nCrossHG-Meta\nImpro. LR - KT\nGeometer\nDBiGCN\nImpro. LR - AD\nALLIE\nBalan. H/LR - DRR\nGraphENS\nBalan. H/LR - SDG\nGraphMixup\nTAM\nBalan. H/LR - AC\nTotal: ~14\u00a0 papers\n2017\n2016\nNode-level Class\nAmplay\nGraph-level Class\nSTREAMSPOT\nNode-level Class\nRadar\nEdge-level Class\nHitFraud\nGraph-level Class\nGraph-TPP\nNetwalk\nGraph-level Class\nAMEN\nGraph-level Class\nQuery-map\nTotal: ~ 5\nTotal: ~ 10 papers\nTotal: ~17 papers\u00a0\nTotal: ~36 papers\u00a0\nTotal: ~52 papers\u00a0\nTotal: ~66 papers\u00a0\nTotal: ~26 papers till now\u00a0\nSTREAMSPOT\nBalan. H/LR - AC\nAmplay\nRadar\nBalan. H/LR - AC\nGANG\nSPARC\nSpecAE\nBalan. H/LR - AC\nAddGraph\nBalan. H/LR - DRR\nALARM\nGAAN\nBalan. H/LR - SDG\nCARE-GNN\nBalan. H/LR - DRR\nGDN\nBalan. H/LR - AC\nPC-GNN\nEdge-level Class\nLIFE\nGraphAnoGAN\nBWGNN\nSTGAN\nSOLT-GNN\nST-GFSL\nNode-level Class\nGraphPrompt\nX-FNC\nGraphSANN\nGraph-level Class\nADKF-IFT\nG2GNN\nNode-level Struct.\nBLADE\nHyperIMBA\nEdge-level Struct.\nZhu et al.\nHiRe\nGraphPrompt\nImpro. LR - KT\nHiRe\nADKF-IFT\nGCAD\nBalan. H/LR - AC\nBLADE\nBalan. H/LR - DRR\nHyperIMBA\nGraphSANN\nBalan. H/LR - SDG\nGHRN\n... ...\n... ...\nNode-level Class\nNode-level class imbalance\nEdge-level Class\nEdge-level class imbalance\nGraph-level Class\nGraph-level class imbalance\nNode-level Struct.\nEdge-level Struct.\nGraph-level Struct.\nNode-level structure imbalance\nEdge-level structure imbalance\nGraph-level structure imbalance\nImproving the Low-Resource part\nKnowledge Transfer\nImpro. LR - KT\nImpro. LR - AD\nImproving the Low-Resource part\nAuxiliary Data\nBalan. H/LR - DRR\nBalan. H/LR - SDG\nBalan. H/LR - AC\nBalancing the High/Low-Resource parts\nData Reweighting and Resampling\nBalancing the High/Low-Resource parts\nSynthetic Data Generation\nBalancing the High/Low-Resource parts\nAdditional Constraints\nTime\nFig. 1: This timeline showcases the representative research literature on ILoGs in relation to our taxonomies, i.e., Problems\nand Techniques. The total number of research works for each year is also displayed, illustrating an upward trend.\ning challenges in imbalanced learning within the realm\nof graph learning tasks. These research efforts endeavor\nto address a variety of real-world applications [18], [26],\n[31]\u2013[34], formalized into several distinct research problems.\nEach problem, characterized by its unique features, calls\nfor the development of specialized techniques tailored to\neffectively address the imbalance issues specific to each\nscenario. However, the diversity of problems and techniques\nhas resulted in a scattered landscape of imbalanced learning\non graphs, lacking a comprehensive framework to identify\ntheir commonalities and disparities.\nTherefore, in this survey, we focus on Imbalanced\nLearning on Graphs (ILoGs) to bridge this gap by reviewing\nand summarizing the problems and techniques in the context\nof addressing imbalance issues on graphs. The essence of\nILoGs lies in the observation that graph learning models\nwith imbalanced input typically exhibit varying perfor-\nmance across groups with different levels of graph resource\nabundance [18], [32], [34]\u2013[36]. More precisely, as shown in\nFig. 2, given the input graph(s), graph data is often divided\ninto multiple segments, creating an imbalanced distribution\nof graph resources. This phenomenon can be observed in\nvarious tasks, such as imbalanced node classification [25],\n[37] and node representation learning with different degrees\n[18], [27], as depicted in Fig. 2(b). This uneven distribution\noften leads to imbalanced outcomes: graph models typically\nperform well on high-resource parts while marginalizing\nthe low-resource parts, ultimately resulting in disparities in\nperformance across parts, as shown in Fig. 2(c).\nHowever, the multifaceted nature of imbalanced learn-\ning on graphs presents considerable complexity in its inves-\ntigation. On one hand, graph structures spawn a wide array\nof problems characterized by varying forms of imbalance,\ntasks, and solutions. This diversity presents a challenge in\nunifying these elements. Therefore, creating an organized\ntaxonomy to categorize these imbalanced learning problems\non graphs is a significant task. Additionally, a detailed\ncategorization will benefit future research by identifying un-\nexplored areas. On the other hand, the numerous imbalance\nproblems on graphs yield diverse solutions, and some tasks\ndemand specific techniques due to their unique attributes.\nThis creates a complex landscape of solutions, making re-\nsearch in this area challenging. Therefore, classifying litera-\nture from a technical perspective is crucial. Moreover, this\nclassification can also aid readers in selecting appropriate\ntechniques to handle their specific graph imbalance issues.\nTo resolve the complexity, we categorize the literature\nfrom the perspectives of problems and techniques to pro-\nvide a comprehensive overview. Primarily, for the taxon-\nomy of problems, we classify the literature based on class\nimbalance and structure imbalance, both stemming from\nimbalanced input. We further distill this into more specific\ncategories: node-, edge-, and graph-level imbalance, offering\na comprehensive understanding of graph imbalance, as\ndepicted in Fig. 3. Besides, to provide a clearer insight into\nthese imbalance issues, we delve deeper by detailing and\ncontrasting their types, settings, and information abundance\nin Table 1. Additionally, for the taxonomy of techniques,\nwe classify literature based on imbalance types and the\nassociated strategies to mitigate it, as summarized in Fig. 4.\nThis is because, specific types of imbalance may require\nparticular techniques to effectively address the associated\nissues, and the techniques employed may vary across dif-\nferent imbalance types.\nUsing these taxonomies allows us to capture commonal-\nities and disparities across imbalanced graph learning liter-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n3\n!!\n!\"\n!#\n!$\n!%\n!&\n!'\n!(\n(a) Input graph\nLabeled nodes\nUnlabeled nodes\nHigh-resource part\nLow-resource part\n!!\n!$\n!(\n!#\n!\"\n!&\n!%\n!(\n!\"\n!!\n!$\n!#\n!&\n!%\nExample:\nImbalanced node\nclassification\n!'\nExample:\nNode representation\nlearning with different\ndegrees\n(High: degree \u22653)\nData split\nHigh-resource part: superior\nLow-resource part: inferior\nInferior performance\nMajority class\nMinority class\nHigh-degree nodes\nLow-degree nodes\n(b) Input with imbalanced graph resource distribution\n(c) Imbalanced outcomes\nGraph encoder\nPerformance\nMajority class\nMinority class\n0\n50\n100\nPerformance\nHigh-degree\nLow-degree\nFig. 2: Imbalanced learning on graphs: imbalanced graph resource distribution results in imbalanced outcomes.\nature. These hierarchical taxonomies group related studies\nbased on shared features, such as imbalance type, imbalance\nlevel (i.e., node, edge, and graph), and techniques. Simul-\ntaneously, the individual characteristics of each research\ntask or technique are highlighted, underscoring the unique\naspects that set them apart from each other. This approach\nprovides a holistic view of the field, shedding light on both\nshared trends and unique advances. A more in-depth explo-\nration of these taxonomies will be provided in Section 3.\nBuilding on the taxonomies of problems and techniques,\nwe further delineate promising directions for future research\nin the realm of imbalanced learning on graphs. Specifically,\nwhen considering future directions concerning problems,\nwe dissect potential research challenges through the lenses\nof class imbalance and structure imbalance. As for future\ndirections concerning techniques, we contemplate innovative\nsolutions that could fuel the growth of this field.\nNote that, given the sheer volume of research papers\ntackling this intricate issue, our survey does not delve into\nthe nuances of each study. Instead, our primary objective is\nto present a holistic, structured overview in line with our\nestablished taxonomies. This approach ensures we offer a\nbroad perspective on the domain and spotlight the emergent\nfrontiers in imbalanced learning on graphs.\nRelationship with existing surveys. Imbalanced learning\nhas been the subject of several surveys in the past decade,\ncovering various areas from general imbalanced classifi-\ncation [19], [20], [38] to more specific tasks like anomaly\ndetection [39]\u2013[41], few-shot learning [42], [43], long-tailed\ndistribution [17], etc. However, these surveys have primarily\nfocused on imbalanced learning in a general context or in\nspecific tasks, and lack comprehensive coverage of imbal-\nanced learning on graphs. In the domain of graph-related\ntasks, there are surveys that focus on specific tasks on\ngraphs such as class-imbalanced learning [44], anomaly de-\ntection [28], [45]\u2013[47], few-shot classification [29], [30], and\nfairness learning [48]. Despite their relevance, these surveys\nconcentrate on individual tasks and lack a comprehensive\noverview of imbalanced learning on graphs.\nOur survey fills this gap by providing a holistic view of\nimbalanced learning on graphs, covering diverse tasks with\na focus on both class imbalance and structure imbalance. In\ncontrast to previous surveys [28]\u2013[30], [45]\u2013[48], our work\nelucidates the shared traits and unique characteristics of\nthese tasks, offering fresh insights into their commonalities\nand differences within the sphere of imbalanced learning\non graphs, based on the two taxonomies. Furthermore, we\nalso incorporate fairness learning on graphs (see Section 7.1\nfor more details) as a specific type of imbalanced learning.\nUnlike other forms of imbalanced learning, fairness learning\nfocuses on mitigating the potential bias and discrimination\nthat may exist in the predictions made by the models, rather\nthan solely on improving performance on imbalanced data.\nContributions. To summarize, our survey makes the follow-\ning major contributions.\n\u2022 We present the first comprehensive survey that broadly\nencompasses the field of imbalanced learning on graphs.\nGiven the importance of this research area and the increas-\ning number of papers exploring it, our survey serves as an\ninvaluable resource for both researchers and practitioners.\n\u2022 To provide a comprehensive and structured overview of\nthe field, we introduce two novel taxonomies: problems\nand techniques. These taxonomies are designed to facili-\ntate a thorough understanding of the existing literature,\nand provide a clear picture of the commonalities and\ndistinctions through structured categorizations.\n\u2022 We identify potential future research directions in the field\nof imbalanced learning on graphs, providing insights and\nguidance for those interested in advancing the state-of-\nthe-art in this fast-paced field.\n2\nBACKGROUND\nWe present the main terminologies and definitions in this\nsurvey, for graphs and imbalanced learning, respectively.\n2.1\nGraphs and Graph Representation Learning\n2.1.1\nGraph Formalization\nGraph.\nA\ngraph\ncan\nbe\nrepresented\nas\nG\n=\n{V, E, Xv, Xe, \u03d5, \u03c6, T , R}, where V is the set of nodes,\nE is the set of edges, Xv \u2208R|V|\u00d7dXv and Xe \u2208R|V|\u00d7dXe\nare the feature matrices of nodes and edges, respectively,\nand T and R are the sets of node types and edge types. For\nsimplicity, we utilize xv \u2208RdXv and xe \u2208RdXe to denote\nthe feature vectors of node v and edge e, respectively. The\nfunction \u03d5 : V \u2192T maps a node v \u2208V to its corresponding\nnode type \u03d5(v), while the function \u03c6 : E \u2192R maps an\nedge e\u27e8u,v\u27e9\u2208E to its corresponding edge type \u03c6(e\u27e8u,v\u27e9).\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n4\nThe variety of node and edge types leads to the dif-\nferentiation of graphs into various forms. A homogeneous\ngraph is a graph without considering the node and edge\ntypes, i.e., |T | = |R| = 1, and has been the subject\nof most research in graph analytics due to its simplicity.\nA heterogeneous graph [49], also called a Heterogeneous\nInformation Network (HIN), is a graph that has different\nnode or edge types, i.e., |T | + |R| > 2, and preserves\ndiverse semantics due to the presence of node and edge\ntypes. In particular, heterogeneous graphs can be further\nclassified into several categories, based on the diversity of\nheterogeneity. A bipartite graph requires that there only\nexist two types of nodes, with edges connecting nodes with\ndifferent types, i.e., |T | = 2 and |R| = 1, and \u03d5(u) \u0338= \u03d5(v)\n\u2200e\u27e8u,v\u27e9\u2208E. Bipartite graphs are extensively investigated\nin the field of recommender systems [50] by formalizing\nthe user-item interactions into graph format. Additionally,\na knowledge graph (KG for short) [51] is a graph that\nconsists of various types of edges between nodes, which\nare also known as relations and entities, respectively. KGs\nare prevalently employed in natural language processing to\nrepresent extracted knowledge from natural language.\nFor further information on these graph types, we refer\nreaders to the relevant surveys mentioned above. As the\nmain focus of this survey is on imbalanced learning on\ngraphs, we do not extensively discuss or investigate the\nspecific issues in these fields.\n2.1.2\nGraph Representation Learning\nGraph representation learning [4], [5] is a powerful and effi-\ncient way for graph analytics. It offers a cost-effective alter-\nnative to traditional graph engineering techniques by map-\nping graph structures (e.g., nodes) into a low-dimensional\nspace while preserving the structural information.\nFormally, let hv \u2208Rd denote the representation of node\nv. Node representation learning can be formalized as\nhv = f(v, G; \u03b8f),\n(1)\nwhere \u03b8f represents the learnable parameters of the graph\nrepresentation learning function f(\u00b7; \u03b8f). Researchers can\ndevise various objectives for the function f(\u00b7; \u03b8f).\nRecently, more attention has been shifted to GNNs [5]\nwhich are based on the key operation of neighborhood\naggregation. GNNs aggregate messages from neighboring\nnodes recursively to generate node representations, en-\nabling end-to-end learning. Formally, in the l-th GNN layer,\nthe representation of node v can be calculated as\nhl\nv = AGGR(hl\u22121\nv\n, {hl\u22121\nu\n: u \u2208Nv} | \u03b8l\ng),\n(2)\nwhere Nv is the neighbors set of node v, AGGR(\u00b7) is the\naggregation function, and \u03b8l\ng is the parameters set in layer\nl. The aggregation function can take various forms, such\nas mean [9], sum [12], or attentive aggregation [11]. In the\nfirst layer, node representations can be initialized with node\nfeature vectors, i.e., h0\nv = xv. For simplicity, we directly use\nhv to denote the output representation of node v.\nFurthermore, graph-level representations [52] are often\nneeded for graph-level tasks, such as molecular classifi-\ncation [53]. They typically leverage learned substructure\nrepresentations (e.g., nodes [12] or edges [54]) for further\naggregation to obtain graph representation. Given a graph\nG and considering node representations as the foundation,\ngraph representation learning can be formalized as\nhG = READOUT({hv : v \u2208V}; \u03b8r),\n(3)\nwhere READOUT(\u00b7) is the readout function that aggregates\nnode representations into the graph representation, param-\neterized by \u03b8r. In general, the READOUT function can pri-\nmarily be implemented in two ways, i.e., global-pooling [12],\n[55] and hierarchical-pooling [52], [56].\n2.2\nImbalanced Learning on Graphs\nImbalanced learning. The conventional approaches for im-\nbalanced learning primarily focus on developing learning\nalgorithms to handle imbalanced classes [19], [20], [38].\nDefinition 1 (Conventional Imbalanced Learning). In the\ncontext of conventional imbalanced learning, consider a labeled\ndata set D = {(xi, yi)}N\ni=1, which can be partitioned into K\nclasses (groups) such that D = S\n1\u2264j\u2264K Gj (given each group\nGj = {(xi, yi) : yi = j}). There exists a notable imbalance in the\nnumber of labeled samples across these groups. Under this setting,\nthe imbalanced distribution of samples across groups would give\nrise to biases in the performance of a learning algorithm. In par-\nticular, the low-resource groups, i.e., the classes with less labeled\ndata, are usually marginalized by the learning model due to the\ndomination of the high-resource groups, resulting in performance\ndegradation for the former. The goal of imbalanced learning is to\ndevelop a balanced model that can improve the performance of low-\nresource groups, potentially reaching levels comparable to those of\nhigh-resource groups.\nThe degree of imbalance in a given dataset can be evalu-\nated using the imbalance ratio, a metric particularly pertinent\nin classification tasks [17], [25]. Formally, for a labeled\ndataset encompassing K classes (groups) D = S\n1\u2264i\u2264K Gi,\nand with classes ordered by their cardinality in a descending\nmanner (i.e., if i < j, then |Gi| \u2265|Gj|, where |Gi| denotes the\nsize of group Gi), the imbalance ratio is defined as |G1|/|GK|.\nEssentially, the imbalance ratio quantifies the skewness of\nthe distribution across classes in terms of their labeled\ndata\u2014a larger ratio signifies a more skewed distribution.\nHowever, due to the multifaceted nature of imbalance issues\nin graphs, the specific form of the imbalance ratio can vary.\nNext, we will introduce the concept of imbalanced learning\non graphs as well as the imbalance ratio for graphs.\nImbalanced learning on graphs. Imbalanced learning on\ngraphs is complex due to its distinct graph structures. In\naddition to class imbalance, the graph structure itself is a\nsignificant source of imbalance, which can cause varying\nperformance across groups w.r.t. their structures.\nDefinition 2 (Imbalanced Learning on Graphs). In addi-\ntion to the number of labeled instances, imbalance in graph\ndata can stem from disparities in structural abundance across\ngroups, leading to a more complex imbalance pattern. For a given\ngraph dataset comprising a set of elements (i.e., nodes, edges,\nor (sub)graphs) represented as G = {xi}N\ni=1, these elements\ncan be further grouped into K subsets, i.e., G = S\n1\u2264i\u2264K Gi,\naccording to specific criteria based on classes or structures, where\n1 < K \u2264N. It is important to note that these groups differ\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n5\nImbalance Types\nImbalance Tasks\nSettings\nInformation Abundance s\nExplanations\nImbalanced node classification\nNode-level anomaly detection\nA set of (or two) node classes D = S\n1\u2264i\u2264K Ci\n|Ci| (# labeled nodes in each class Ci)\nFew-shot node classification\nZero-shot node classification\nA set of base node classes Db = S\n1\u2264i\u2264K1 Ci,\nand novel node classes Dn = S\nK1<i\u2264K2 Ci\n|Ci| (# labeled nodes in each class Ci)\nFew-shot link prediction\nA set of base graphs Db = {Gi}K1\ni=1 and novel\ngraphs Dn = {Gi}K2\ni=K1+1, where Gi = {Vi, Ei}\n|Ei| (# edges in each graph Gi)\nEdge-level anomaly detection\nTwo edge classes D = C1 \u222aC2\n|Ci| (# labeled edges in each class Ci)\nImbalanced graph classification\nGraph-level anomaly detection\nA set of (or two) graph classes D = S\n1\u2264i\u2264K Ci\n|Ci| (# labeled graphs in each class Ci)\nFew-shot graph classification\nA set of base graph classes Db = S\n1\u2264i\u2264K1 Ci,\nand novel node classes Dn = S\nK1<i\u2264K2 Ci\n|Ci| (# labeled graphs in each class Ci)\nImbalanced node degrees\nA set of node groups D = S\n1\u2264i\u2264K Gi, where\nGi = {vj : dj = i} (dj is the degree of node vj)\ndj (the degree of each node vj)\nNode topology imbalance\nA set of node classes D = S\n1\u2264i\u2264K Ci\nThe consistency between true class boundaries\nand influence boundaries of labeled nodes\nLong-tail entity embedding\nA set of entity groups D = S\n1\u2264i\u2264K Gi, where\nGi = {ej : dj = i} (dj is the # triplets of entity ej)\ndj (# triplets of each entity ej)\nFew-shot relation classification\nZero-shot relation classification\nFew-shot reasoning on KGs\nA set of base relations Db = S\n1\u2264i\u2264K1 Ri,\nand novel relations Dn = S\nK1<i\u2264K2 Ri\n|Ri| (# labeled triplets of each relation Ri)\nImbalanced graph sizes\nA set of graph groups D = S\n1\u2264i\u2264K Gi, where\nGi = {Gj : |Vj| = i} (|Vj| is the size of graph Gj)\n|Vj| (the size of each graph Gj)\nImbalanced topology groups\nA set of topology motifs D = S\n1\u2264i\u2264K Mi\n|Mi| (# instances of each motif Mi in one class)\nNode-Level\nClass\nImbalance\nLabeled nodes are unevenly distributed across\nclasses.\nBase classes have abundant labeled nodes, while\nnovel classes have few/no labeled nodes.\nEdge-Level\nClass\nImbalance\nBase graphs have abundant edges, while novel\ngraphs have limited edges.\nLabeled edges are unevenly distributed across\nclasses.\nGraph-Level\nClass\nImbalance\nLabeled graphs are unevenly distributed across\nclasses.\nBase classes have abundant labeled graphs, while\nnovel classes have few labeled graphs.\nNode-Level\nStructure\nImbalance\nHead nodes have high degrees, while\ntail/cold-start nodes have few/no degrees.\nClasses with more consistent boundaries tend to\npropagate label information more effectively.\nHead entities have more triplets, while\ntail/cold-start entities have few/no triplets.\nEdge-Level\nStructure\nImbalance\nBase relations have abundant labeled triplets,\nwhile novel relations have few/no labeled\ntriplets.\nGraph-Level\nStructure\nImbalance\nHead graph have large sizes, while tail graphs\nhave small sizes.\nMotifs with more instances have stronger\nassociations with the class than the less frequent\nmotifs.\nTABLE 1: Category of existing graph imbalance issues.\nin terms of information abundance, which results in unequal\nperformance among them when used as input for a learning model.\nDeviating from the initially defined imbalance ratio for\nclass imbalance tasks, the multifaceted nature of imbalance\nissues inherent in graphs calls for a broader definition of\nthe imbalance ratio. Formally, given K subsets of a set\nof elements G = {xi}N\ni=1, i.e., G = S\n1\u2264i\u2264K Gi, let sGi\nsymbolize the abundance of information for group Gi based\non particular criteria related to classes or structures (as\ndepicted in Table 1). In this context, groups are sorted by\ndescending order of information abundance (i.e., if i < j,\nthen sGi \u2265sGj). Thus, the imbalance ratio on graphs can\nbe defined as sG1/sGK. In essence, this imbalance ratio\non graphs quantifies the skewness in distribution across\ngroups concerning their information abundance\u2014a larger\nratio denotes a more skewed distribution. Crucially, this\ngeneralized definition of the imbalance ratio provides a\nmore flexible tool for evaluating the wide array of imbalance\nissues within graphs.\nTo better grasp graph imbalance issues, we have com-\npiled them in Table 1 to demonstrate their commonalities\nand disparities. For each type of imbalance, we enumerate\nthe associated imbalance tasks, all of which are covered in\nthis survey. Based on these tasks, we then outline the main\nsettings pertinent to each task. Furthermore, we introduce\nthe concept of information abundance s, as discussed earlier,\nto pinpoint the imbalance resources that contribute to the\nimbalance issues. Lastly, for clarity, we furnish detailed\nexplanations for each task.\n3\nOVERVIEW OF TAXONOMIES\nWe categorize the literature from the perspectives of prob-\nlems and techniques to provide a comprehensive overview.\nTaxonomy of Problems. In terms of the taxonomy of\nproblems, our summarization begins by investigating the\nresearch tasks of ILoGs. Imbalance issues on graphs are\nmultifaceted, stemming from the multi-faceted nature of\ngraph data. Specifically, graph imbalance can arise from two\nkey sources: classes and structures, which usually serve as\nthe key inputs to learning models. On one hand, for classes\nin graphs, labeled data may be distributed unevenly across\nclasses, resulting in imbalanced outcomes in tasks such as\nnode classification [25], [35]. On the other hand, the unique\ngraph structures can introduce another source of imbalance\nin graph learning models. For instance, the neighborhood\nabundance of nodes [27], [57], can result in discrepancies\nin the exposed information, which may further skew the\noutput of the learning model accordingly.\nTherefore, we first categorize graph imbalance issues\nbased on their types, i.e., class and structure imbalance, both\narising from imbalanced input. Secondly, considering the\nspecific graph structures, we further categorize the imbal-\nance issues into finer groups based on the graph-related\nelements, i.e., node-, edge-, and graph-level imbalance, to gain\na more detailed perspective on these issues. Finally, ILoGs\nincludes a variety of research tasks, which we use as the\nfinest categories for illustration, such as imbalanced node\nclassification [35]\u2013[37], few-shot node classification [26],\n[58], [59], etc. Fig. 3 illustrates the taxonomy of problems.\nTaxonomy of Techniques. For the taxonomy of techniques,\nwe categorize the literature based on imbalance forms (i.e.,\nwhat kind of imbalance) and the corresponding strategies\nto mitigate the imbalance (i.e., how this imbalance is ad-\ndressed), as summarized in Fig. 4. This is because specific\nforms of imbalance may require particular techniques to\neffectively address the associated issues, and the techniques\nemployed may vary across different imbalance forms. Ini-\ntially, we separate data into high- and low-resource seg-\nments (as shown in Fig. 2(b)) based on their information\nabundance, which results in two primary task branches con-\ntingent on the prediction target: improving the low-resource\npart, and balancing the high/low-resource parts. The former\nseeks to improve the performance of the low-resource part\nwithout considering the other part; while the latter strives\nfor a balance in performance across both parts, given that\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n6\nImbalanced Learning on Graphs: Problems\nClass Imbalance\nNode-Level Class Imbalance\nImbalanced Node Classification\nNode-Level Anomaly Detection\nFew-Shot Node Classification\nZero-Shot Node Classification\nEdge-Level Class Imbalance\nFew-Shot Link Prediction\nEdge-Level Anomaly Detection\nGraph-Level Class Imbalance\nImbalanced Graph Classification\nGraph-Level Anomaly Detection\nStructure Imbalance\nNode-Level Structure Imbalance\nImbalanced Node Degrees\nNode Topology Imbalance\nLong-Tail Entity Embedding\nEdge-Level Structure Imbalance\nZero-Shot Relation Classification\nFew-Shot Reasoning on KGs\nGraph-Level Structure Imbalance\nImbalanced Graph Sizes\nFew-Shot Relation Classification\nFew-Shot Graph Classification\nImbalanced Topology Groups\nFig. 3: Taxonomy of Problems.\npredictions will be drawn from both. Subsequently, we\nsummarize the major techniques employed to address the\nimbalance issues in each branch, and further provide de-\ntailed illustrations of techniques under each main category.\nOrganization. In the following three sections, we introduce\nthe two taxonomies of ILoGs. Specifically, Sections 4 and 5\naddress the problems of ILoGs from the standpoints of class\nimbalance and structure imbalance, respectively. In Sec-\ntion 6, we classify the relevant literature based on the tech-\nniques used to address imbalance issues and offer guidance\nto help readers choose appropriate solutions. Subsequently,\nwe discuss other studies related to imbalanced learning\non graphs in Section 7. In Section 8, we explore future\nresearch directions from the perspective of both problems\nand techniques, and Section 9 concludes the survey.\n4\nILOGS PROBLEMS: CLASS IMBALANCE\nIn the vast landscape of ILoGs, this section offers a succinct\noverview of class imbalance issues at the node (Section 4.1),\nedge (Section 4.2), and graph levels (Section 4.3), as illus-\ntrated in the left segment of Fig. 3. Moreover, structure\nimbalance will be discussed in Section 5.\n4.1\nNode-Level Class Imbalance\nNode-level class imbalance refers to the disproportionate\ndistribution of labeled nodes across classes. The model often\nlearns more from classes with numerous labeled instances,\nat the risk of overlooking those with fewer. This challenge\nsurfaces as four principal node-level tasks, which we will\ndetail in the following sections.\n4.1.1\nImbalanced Node Classification\nImbalanced node classification is a common challenge in\nreal-world applications where labeled nodes are unevenly\ndistributed across classes (as depicted in Table 1), such as in\nfake account detection or abusive review detection [19]. This\nTechniques\nLiterature\nAlgo-level\nConstraints\nDRGCN [35], DPGNN [61], TAM [62]\nKnowledge distillation\nLTE4G [63]\nData-level\nGAN\nDRGCN [35], ImGAGN [37]\nSMOTE\nGraphSMOTE [25]\nMixup\nGraphMixup [65], GraphSANN [67], GraphENS [66]\nResampling\nLTE4G [63], ALLIE [69]\nReweighting\nTAM [62]\nTABLE 2: Summary of imbalanced node classification.\nimbalance usually leads to a performance bias, as majority\n(high-resource) classes tend to outperform minority (low-\nresource) classes due to their larger number of labeled nodes\n[25], [35]. This imbalance has incited a growing number of\nstudies [25], [35], [37], trying to enhance the performance of\nminority groups. For clarity, we categorize these studies into\ntwo perspectives: algorithm-level and data-level solutions.\nFor algorithm-level strategies, early work like DRGCN\n[35] implements a GNN-based approach, featuring a gen-\nerative adversarial network (GAN) [60] for synthetic node\ngeneration to level the class distribution, paired with a KL-\ndivergence constraint to synchronize the representation dis-\ntribution of unlabeled nodes with labeled ones. Differently,\nDPGNN [61] employs a class prototype-driven training ap-\nproach to balance training loss across classes, with the help\nof distance metric learning to accurately capture the relative\npositions of nodes concerning class prototypes. TAM [62]\naddresses the class imbalance issue by accounting for the\nreduced homogeneity among minority nodes. Specifically,\nit innovatively introduces connectivity- and distribution-\naware margins to guide the model, emphasizing class-wise\nconnectivity and neighbor-label distribution. LTE4G [63]\nconsiders the imbalance in both node classes and degrees.\nIt partitions nodes into balanced subsets assigned to expert\nGNNs, then employs knowledge distillation to train class-\nspecific students, to enhance classification performance.\nThe data-level approaches are typically designed to bal-\nance data distribution through tail class oversampling [16],\nhead class undersampling [64], or instance-based class\nreweighting [14]. On graphs, GraphSMOTE [25] and Im-\nGAGN [37] both revolve around the generation of syn-\nthetic nodes to balance classes. GraphSMOTE achieves this\nthrough a technique inspired by SMOTE [16], generating\nnew node representations by averaging two sampled mi-\nnority class nodes. ImGAGN, in contrast, applies GAN to\ngenerate synthetic nodes. GraphMixup [65], GraphENS [66],\nand GraphSANN [67] offer a unique approach to synthetic\nnode generation, drawing inspiration from Mixup [68]. They\ncreate new nodes by merging semantic features with con-\ntextual edges, promoting balanced model training. Notably,\nGraphENS places greater emphasis on minority node neigh-\nbors, acknowledging their informational bias and employ-\ning neighbor sampling along with saliency-based node mix-\ning to mitigate it. ALLIE [69] deviates from synthetic node\ngeneration, employing active learning instead. It involves\nsampling instances from both majority and minority classes,\nwith an imbalance-aware reward function implemented via\nreinforcement learning [70] aiding the sampling process.\nSummary. The core challenge in imbalanced node classifi-\ncation lies in achieving balanced information distribution across\nclasses for uniform model training. Algorithm-level methods\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n7\nGraph objects\nGraph types\nBase models\nLiterature\nNode-level\nHomogeneous\nGA\n[81]\u2013[91]\nGE\n[92]\u2013[97]\nGNN\n[77], [98]\u2013[124]\nHeterogeneous\nGA\n[125]\u2013[131]\nGE\n[132], [133]\nGNN\n[36], [75], [76], [123], [134]\u2013[149]\nDynamic\nGA\n[80], [150]\u2013[157]\nGE\n[158]\u2013[162]\nGNN\n[74], [163]\u2013[167]\nEdge-level\nHomogeneous\nGNN\n[168], [169]\nHeterogeneous\nGNN\n[170]\nDynamic\nGA\n[78], [171]\nGE\n[79]\nGNN\n[172], [173]\nGraph-level\nHomogeneous\nGA\n[174]\u2013[179]\nGNN\n[72], [73], [180], [181]\nHeterogeneous\nGA\n[182]\nDynamic\nGA\n[183], [184]\nGNN\n[185]\nTABLE 3: Summary of anomaly detection on graphs.\naddressing imbalance often leverage strategies such as addi-\ntional constraints [35], prototype-driven training [61], ensur-\ning homogeneity among minority nodes [62], or employing\nknowledge distillation [63]. As summarized in Table 2, a\nsignificant portion of research in this area tackles this is-\nsue from the data-level. This commonly involves creating\nsynthetic nodes for underrepresented classes, or opting for\nreweighting or resampling nodes from both the majority\nand minority classes. For a more in-depth understanding\nof these techniques, please refer to Section 6.2. Note that,\nthere is no significant gap between algorithm- and data-\nlevel approaches, as some methods, like DRGCN and TAM,\ncan incorporate elements from both. Despite extensive re-\nsearch, this area still calls for further exploration. Innovative\ntechniques, such as using diffusion models [71] to generate\nsynthetic nodes or employing transfer learning to transfer\nknowledge from majority to minority classes for their en-\nhancement, could be promising avenues of investigation.\nA particular type of imbalanced node classification,\nnode-level anomaly detection, has sparked increasing inter-\nest recently. We will separately provide a comprehensive\nreview of the associated literature in Section 4.1.2.\n4.1.2\nNode-Level Anomaly Detection\nNode-level anomaly detection, as a specific manifestation of\nimbalanced node classification, generally centers on binary\nclassification between normal and anomalous nodes. This\ntask plays a vital role in numerous applications, such as\noutlier detection [72], [73], fraudulent user detection [74]\u2013\n[77], and social spammer detection [78]\u2013[80]. Considering\nthe diversity of graphs (e.g., homogeneous, heterogeneous,\nor dynamic graphs), a range of techniques have been devel-\noped to address this issue, such as traditional graph algo-\nrithms (GA), graph embedding approaches (GE), and graph\nneural network approaches (GNN). They are categorized in\nTable 3, which also includes edge- and graph-level anomaly\ndetection approaches. While we provide an overview of\nrecent literature in this domain, for an in-depth analysis,\nwe refer readers to the comprehensive survey [28].\nHomogeneous graphs. Node-level anomaly detection on\nhomogeneous graphs is a focal point of interest within this\ntask. Traditional graph algorithms (GA) address anomaly\ndetection by capturing structural anomalies through various\nhandcrafted metrics such as applying residual analysis on\nattributes [82], [85], or utilizing human feedback in an inter-\nactive manner [87]. Following this, graph embedding (GE)\nbased methods were developed, aiming to separately em-\nbed anomalous and normal nodes to capture their specific\npatterns [93], [95], or to generate representations that can\nform clear boundaries between classes [96], among others.\nRecently, GNNs are widely used for anomaly detection\non graphs. These methods are usually coupled with dif-\nferent learning objectives to obtain distinguishable node\nembeddings. We outline some popular techniques be-\nlow. Contrastive learning methods typically leverage the\nsimilarity between target nodes and their context dur-\ning model pre-training. For graph data containing sus-\npicious nodes\u2014nodes that often attempt to camouflage\nthemselves\u2014identifying their dissimilarity to the context\n(i.e., heterophily) proves beneficial for their detection [107],\n[111]. Generative models like GANs are also exploited to\ncreate synthetic nodes during training, so the model can\nlearn the distinct distributions of normal nodes as well\nas the out-of-distribution anomalous nodes [102], [110]. In\naddition, autoencoders are also employed to reconstruct\ngraph structures or node features, where hard-to-reconstruct\nsamples are usually identified as anomalies [102], [106].\nHeterogeneous graphs. Anomaly detection is extensively\nstudied in the context of heterogeneous graphs as well. In\nearly stages, traditional graph algorithms (GA) usually de-\nfine anomalous structural patterns on heterogeneous graphs\nto identify anomalies, such as detecting dense subgraphs\n[127], [128], or finding suspicious node attributes in clusters\n[126]. Later in the literature, more GE-based methods are\nproposed for heterogeneous graphs, such as characterizing\ndifferent types of malware using meta-path based random\nwalks [132], or identifying fraudsters based on their propen-\nsity to form unidirectional links with normal users [133].\nIn addition, many recent studies leverage heterogeneous\nGNNs to concurrently address graph heterogeneity and\nimbalance challenges, employing a variety of effective tech-\nniques. For instance, some studies [134], [144] resort to rein-\nforcement learning to select the most informative neighbors,\nand jointly consider multiple relations to learn distinguish-\nable node embeddings. Similarly, some studies [36], [136]\nutilize neighborhood sampling techniques to tackle class\nimbalance and aggregate multiple relations to differentiate\nnormal and anomalous neighboring nodes. Additionally,\nspectral-based methods [141], [147] segregate aggregations\nof normal and anomalous nodes using distinct pass filters\nand manage multiple relations individually.\nDynamic graphs. Node-level anomaly detection is also\nwidely explored within the realm of dynamic graphs. Early\nGA approaches address anomaly detection on dynamic\ngraphs typically by identifying changes or unusual pat-\nterns within the graph structure, such as detecting temporal\nbursts [151] or tracking sudden changes on anomaly scores\n[155]. Additionally, GE-based methods are proposed to de-\ntect graph anomalies by learning distinguishable node em-\nbeddings, such as applying support vector data description\n(SVDD) to differentiate time-sensitive anomalous patterns\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n8\n[158], using autoencoder to learn dynamic node embeddings\nand detect anomalous nodes by their distance to cluster\ncenters [159], or combining autoencoders with hypersphere\nlearning to isolate the spatiotemporal anomalies [160].\nSeveral GNN-based methods are also proposed to ad-\ndress anomaly detection on dynamic graphs [74], [163]\u2013\n[167]. For example, STGAN [164] involves a spatiotemporal\nsequence generator and a discriminator that determines\nwhether an input sequence is real or not. During inference,\nthe discriminator reports samples that have low scores as\nanomalies due to their out-of-distribution representations.\nAdditionally, AddGraph [172] leverages an extended tem-\nporal graph convolutional network to discern both long-\nterm and short-term patterns in dynamic graphs, using a\nmargin loss to learn distinctive representations for anoma-\nlous and normal nodes.\nSummary. Node-level anomaly detection on graphs is in-\nherently challenging due to the intricate nature of graph\nstructures and the infrequency of anomalies. Prevalent tech-\nniques in this arena largely focus on establishing distinct\nboundaries between normal and anomalous nodes. This can\nbe achieved through feature engineering (GA), discrimina-\ntive node representations (GE), or neighborhood aggrega-\ntion for structure encoding (GNN). For a more comprehen-\nsive insight into state-of-the-art comparisons, readers may\nrefer to benchmarks such as [186] or visit leaderboards1. To\nfurther propel progress in this domain, embracing innova-\ntive methodologies is essential. For example, sophisticated\ngenerative learning models, like diffusion [71], can be har-\nnessed to bolster synthetic instance generation, thereby en-\nhancing the discernibility of anomalous nodes. Furthermore,\ninnovative foundational models [187] hold promise for en-\nriching the comprehension of graph structures, and thus can\npotentially facilitate more effective anomaly detection.\n4.1.3\nFew-Shot Node Classification\nIn various real-world settings, novel classes are often en-\ncountered, marked by the availability of only a few labeled\ninstances or few-shot instances. Such scarcity imposes signif-\nicant hurdles for model training. To counter this, a set of\nbase classes abundant in labeled data is typically harnessed\nto assist the learning process [42], as summarized in Table 1.\nThus, the primary objective of Few-Shot Node Classification\n(FSNC) on graphs is to yield robust and superior perfor-\nmance on these few-shot novel classes by efficiently exploit-\ning the information derived from the base classes, and this\ntask has recently garnered considerable attention [58], [188]\u2013\n[190]. This task finds applications in several contexts, such\nas novel intrusion detection on traffic networks, predicting\nnew types of goods in e-commerce, and anticipating newly\ndiscovered chemical properties in protein networks, etc. In\nthis section, we will delve into FSNC across various settings.\nGeneric FSNC. Meta-learning [205], with a primary focus\non MAML [206] and prototypical networks [207], is a com-\nmon approach in many studies addressing FSNC.\nMAML-based approaches. Meta-GNN [58] set a precedent\nin the field by utilizing meta-learning to transfer meta-\nknowledge from data-rich base classes to novel classes using\n1. https://dgraph.xinye.com/leaderboards/dgraphfin.\na MAML-based adaptation [206]. Building on this, AMM-\nGNN [188] introduces attribute-level adaptation using FiLM\n[208] to address potential attribute disparities across meta-\ntasks, complementing the MAML adaptation. TRGM [190]\nadds another dimension by capturing inter-task relations\nvia contrastive learning in a constructed task graph, thereby\nenhancing few-shot learning. In a distinct approach, RALE\n[189], while rooted in MAML, emphasizes both relative\nand absolute node positions for node placement, aiming to\nbolster representations of nodes in novel classes.\nPrototypical network-based approaches. GFL [26] utilizes pro-\ntotypical network to enable cross-graph FSNC, and further\nintroduces a hierarchical graph representation gate to reg-\nulate global transferable knowledge. Moreover, GPN [59]\nenriches this paradigm by incorporating a node valuator to\nre-weight labeled nodes according to their informativeness\nin prototype calculation. In addition, MuL-GRN [194], akin\nto RALE [189], calculates relations at node, global subgraph,\nand local subgraph levels between query and support nodes\nto facilitate the classification of novel class nodes.\nSome approaches merge the benefits of prototypical\nnetworks with adaptation-based methods like MAML or\nFiLM. ST-GFSL [209] and G-Meta [191], building on the\nprototypical network, employ MAML for optimization to\ntackle FSNC, with the former addressing traffic flow predic-\ntion on spatio-temporal graphs and the latter emphasizing\nlocal subgraph utilization for target node representation.\nTENT [192] tackles variance across meta-tasks via node-,\nclass-, and task-level adaptations. Moreover, Meta-GPS [193]\nrefines aggregation and combination functions in GNNs to\nconsider both graph homophily and heterophily, employing\nFiLM [208] to modulate model parameters per meta-task.\nOther techniques. While most existing studies employ meta-\nlearning for FSNC, alternative techniques have also been\nproposed. For instance, IA-FSNC [195] initializes novel\nclasses with the first layer of a pre-trained GNN model, and\nemploys a semi-supervised method for synthetic label gen-\neration to realize information augmentation for the support\nsets. In addition, Tan et al. [196], [197] employ contrastive\nlearning, by constructing a subgraph around a target node\nbased on connectivity and generating three types of contrast\npairs (node-node, node-subgraph, and subgraph-subgraph)\nfor model training. Another work GraphPrompt [198] em-\nploys GNN prompting techniques, unifying pre-training\nand downstream tasks for effective knowledge transfer,\nachieving impressive results even with limited supervision.\nGeneralized FSNC. In a broader FSNC setting, both base\nand novel class nodes appear in the test set rather than\nonly novel classes. This complicates matters by introducing\nchallenges like asymmetric classification and inconsistent\npreference between base and novel classes [201]. Stager\n[201] addresses this by using prediction uncertainty to mea-\nsure class shot levels and decompose prediction probability,\ntackling asymmetric classification. It also employs a meta-\nlearner to weigh various receptive fields, addressing the\ninconsistent preference challenge.\nOther works [199], [200] focus on class-incremental\nFSNC, where novel classes continually emerge. Both HAG-\nMeta [199] and Geometer [200] address this challenge by\nestablishing an attention-based framework on the founda-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n9\nMeta-learning techniques\nOther techniques\nMAML\nPrototypical network\nOthers\nLabel generation\nContrastive Learning\nPrompting\nGeneric FSNC\n[58], [188]\u2013[190]\n[26], [59], [191]\u2013[194]\n-\n[195]\n[196], [197]\n[198]\nGeneralized FSNC\n-\n[199], [200]\n[201]\n-\n-\n-\nMulti-label FSNC\n-\n[202]\n-\n-\n-\n-\nFSNC with extremely weak supervision\n-\n-\n-\n[203]\n-\n-\nFSNC on HINs\n[30], [204]\n-\n-\n-\n-\n-\nTasks\nTABLE 4: Summary of few-shot node classification.\ntion of the prototypical network, allowing for the weighted\ncalculation of prototypes from labeled instances. HAG-\nMeta extends its approach beyond GNN pre-training on\nbase classes, using an attention mechanism to adjust the\nsignificance of meta-tasks. On the other hand, Geometer\napplies geometric loss functions to maintain intra-class\nproximity, inter-class uniformity, and inter-class separability,\neffectively tackling the challenge of class incrementality.\nMulti-label FSNC. While the majority of prior research\nprimarily focuses on multi-class node classification, the task\nof few-shot multi-label node classification, where each node\nmay be associated with multiple labels instead of just one,\ncontinues to pose a considerable challenge. To address this\nissue, MetaTNE [202] treats it as a label-wise binary clas-\nsification problem. It identifies discrepancies in node em-\nbeddings across different meta-tasks and implemented an\nembedding transformation. This transformation, contingent\non the query node within each task, allows it to obtain task-\nspecific representations beneficial for classification.\nFSNC with extremely weak supervision. In the challenging\nscenario of FSNC with extremely weak supervision, where\nonly a limited number of labeled nodes are available during\nmeta-training, models typically struggle to extract sufficient\nprior knowledge for effective knowledge transfer. To over-\ncome this limitation, X-FNC [203] introduces a novel ap-\nproach to generate pseudo-labels as additional references for\nthe base classes, in order to learn effectively from extremely\nweak supervision.\nFSNC on HINs. Recent research has delved into FSNC\non heterogeneous graphs, necessitating the management\nof heterogeneity. To enhance classification in the target\ndomain, CrossHG-Meta [210], building on heterogeneous\ngraph neural networks (HGNN) [211], [212], extracts trans-\nferable meta-knowledge from a source domain. It employs\ndual adaptations at the domain- and task-levels for model\noptimization, along with a cross-domain contrastive regu-\nlarization. Similarly, HG-Meta [204], another study founded\non HGNN, tackles differences across meta-tasks through\ntask-level modulation. It applies task feature scaling to\nadjust node representations and incorporates an attention\nmechanism to re-evaluate the importance of each meta-task.\nSummary. The primary challenge in few-shot node classifi-\ncation is to extract transferable knowledge from base classes to\nbenefit novel classes. Recent studies in this area, as summa-\nrized in Table 4, have focused on various aspects of FSNC,\nincluding generic, generalized (class incremental), multi-\nlabel, extremely weak supervision settings, and on HINs.\nMethods frequently employed include meta-learning tech-\nniques like MAML and prototypical networks, sometimes\nused simultaneously. Yet, other studies have also ventured\ninto utilizing innovative techniques such as data augmen-\ntation, contrastive learning, and prompting methods. For\na more comprehensive understanding of these techniques,\nwe direct readers to Section 6.1.1. Despite considerable\nresearch, this complex task continues to necessitate further\nexploration. As demonstrated in Table 4, specific settings\nlike generalized, multi-label, extremely weak supervision,\nor those concerning HINs, remain largely underexplored.\nNotably, the adoption of innovative techniques like prompt\ntuning [198], generative models [71], and others could po-\ntentially enhance the representation of novel classes, indicat-\ning promising future directions. For a broader perspective,\nwe suggest consulting the survey of few-shot learning on\ngraphs [30] for additional insights.\n4.1.4\nZero-Shot Node Classification\nZero-Shot Node Classification (ZSNC) necessitates the ab-\nsence of labeled data for novel classes during model train-\ning. Wang et al. [213] build on DeepWalk [6], modulating\nthe objective constraints on base classes to learn effective\nrepresentations for unlabeled novel classes. They relax the\nintra-class similarity constraint and remove inter-label edges\nto enhance dissimilarity. Its further extensions [214], [215]\nutilize GCN as the backbone, adding label-related semantic\ndescriptions. Differently, DGPN [216] provides a formaliza-\ntion for ZSNC, introducing class semantic descriptions and\ncorresponding evaluation metrics (based on the proximity\nof text-based class embeddings and proximity in real data).\nThey refine GNN models by constraining the similarity\nbetween hidden embeddings (in each layer) and the class\nembeddings, to enhance the model training. Similarly, an-\nother work DBiGCN [217] also follows the paradigm of\nutilizing class descriptions for ZSNC.\nSummary. ZSNC remains an underexplored field due to\nthe absence of descriptions for elements like nodes, edges,\nor graphs, which are crucial in typical zero-shot settings.\nTherefore, this challenging task warrants further research\nattention. Currently, the existing strategies involve con-\nstraining model training on base classes, while others utilize\nauxiliary information, such as descriptions, to supplement\nmodel training. The metrics established by previous work,\nsuch as [216], lay a promising foundation for further explo-\nration in this challenging domain.\n4.2\nEdge-Level Class Imbalance\nIn addition to node-level class imbalance, addressing edge-\nlevel class imbalance is also crucial. Two representative\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n10\ntasks of interest are few-shot link prediction and edge-level\nanomaly detection. These tasks underscore the importance\nof tackling edge-level class imbalance in various real-world\nscenarios, such as inductive recommendation [31], anoma-\nlous transaction detection on financial networks, etc.\n4.2.1\nFew-Shot Link Prediction\nRecent studies have focused on few-shot link prediction\nacross graphs, particularly in event-based social networks\n[31]. This presents a challenge as the novel graphs in this\ncontext have limited edges and pose difficulties for link\nprediction. To address this, EA-GAT [31], an event-aware\ngraph attention network, attempts to effectively encode fine-\ngrained events from existing graphs and few-shot target\nevents. It further utilizes gradient-based episode learning\nto acquire transferable knowledge and adapt to event-based\nsocial networks with sparse connections.\nAdditionally, other settings such as few-shot link predic-\ntion across different sections of a single graph, which might\ndemonstrate varied patterns [218], could also be explored as\na potential avenue for research.\n4.2.2\nEdge-Level Anomaly Detection\nEdge-level anomaly\ndetection\nis\ndesigned\nto\nidentify\nanomalous edges within a network. This endeavor is crucial\nin areas like transactions and social networks to detect\npotentially dubious transactions or fake news. Prevailing\nresearch primarily focuses on characterizing edge distri-\nbutions and pinpointing anomalies that exhibit significant\ndeviations from these established distributions.\nExisting works on homogeneous graphs tend to use\ngraph auto-encoders to model the distribution of normal\nedges and use reconstruction errors as indicators of anoma-\nlies [168], or enrich node and edge features using auxiliary\ndescriptions [169] for better discrimination. On heteroge-\nneous graphs, research often tries to capture both hetero-\ngeneity and anomalous patterns, such as using type-aware\nedge representations and tailored classifiers to identify\nfraudulent transactions on live-streaming platforms [170].\nSome works consider detecting abnormal edges in dy-\nnamic graphs by keeping track of edge dynamics [78], [79],\n[171]\u2013[173]. For instance, Miz et al. [79] use the Hopfield\nmodel of memory to combine graph and temporal informa-\ntion and detect temporal spikes in activities as anomalies.\nAnother work StrGNN [173] extracts subgraph snapshots\naround an edge and applies a GNN model with gated\nrecurrent units [219] to capture subgraph structural changes\nas anomaly indicators.\nSummary. The main challenge in edge-level anomaly detec-\ntion lies in the highly imbalanced distribution of normal and\nabnormal edges. To counter this challenge, previous works\nusually model edge distributions based on the abundant\ninformation on normal edges, using deviations from the\nnormal distributions as anomaly indicators. Despite its sig-\nnificance, this area remains relatively underexplored. There\nis ample room for further investigation, especially in tasks\ninvolving heterogeneous graphs where edge relations are\nintricate and significant.\n4.3\nGraph-Level Class Imbalance\nAddressing graph-level class imbalance forms a vital aspect\nof ILoGs, surpassing the scope of node- and edge-level\nimbalance. Three crucial areas warrant attention within this\ndomain: imbalanced graph classification, few-shot graph\nclassification, and graph-level anomaly detection. They\nhighlight the importance of addressing graph-level class\nimbalance across a variety of real-world scenarios, such as\nnovel drug detection, and protein function prediction.\n4.3.1\nImbalanced Graph Classification\nImbalanced graph classification parallels the challenges of\nimbalanced node classification. It commonly arises in real-\nworld scenarios (e.g., imbalanced chemical compound clas-\nsification) where class distributions of labeled graphs are\nskewed (as illustrated in Table 1), often favoring the majority\nclass with more labeled graphs [32].\nTo counter this, G2GNN [32] utilizes additional su-\npervision both globally, through neighboring graphs, and\nlocally, via stochastic augmentations. It builds a Graph\nof Graphs (GoG) by leveraging kernel similarity and im-\nplements GoG propagation for information aggregation.\nAdditionally, topological augmentation paired with self-\nconsistency regularization is employed at the local level.\nThese strategies collectively enhance model generalizability,\nthereby elevating classification performance.\nDespite the advance, this crucial task continues to call for\nmore in-depth study due to the limited amount of current\nresearch, indicating a potentially promising direction.\n4.3.2\nGraph-Level Anomaly Detection\nGraph-level anomaly detection aims to detect anomalous\ngraphs or subgraphs according to a predefined anomaly\nmeasure, which can be seen as a variant of imbalanced\ngraph classification with binary labels. This task is com-\nmonly seen in transaction networks where fraudsters con-\nspire to do money laundering.\nMuch of the emphasis has been placed on homogeneous\ngraphs. A segment of research employs traditional graph al-\ngorithms (GA) to identify anomalous subgraphs [174]\u2013[179].\nFor instance, AMEN [175] defines a normality measure\nand discovers communities with an aim to maximize their\nnormality scores, where communities that cannot achieve\nhigh normality scores are considered to be fraudulent. Chen\net al. [179] employ Benford\u2019s law to assign anomaly scores\nto edges and use the densest sub-graph discovery algorithm\nto find anomalous sub-graphs.\nAnother line of works uses GE-based methods to analyze\nthe anomaly degree of graphs [72], [73], [180], [181]. For\ninstance, OCGTL [72] concentrates graph embeddings in the\ntraining set within a hypersphere where the distance to the\nhypersphere is used as the anomaly degree. AS-GA [181]\nidentifies suspicious sub-graphs based on the reconstruction\nerror of a graph auto-encoder and rates anomaly scores of\nextracted areas with a graph supermodular neural network.\nSome studies consider graph-level anomaly detection\non heterogeneous graphs. For instance, ACGPMiner [182]\nintroduces conditional graph patterns to model abnormal\npatterns in property graphs and follows a generation-and-\nvalidation paradigm to mine the defined abnormal patterns\nand their matches.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n11\nMeta-learning techniques\nMAML\nPrototypical network\nGeneric FSGC\n[220]\n[221], [222]\nAdaptive step controller [220], super-class graph [223], task correlations [221]\nCross-domain FSGC\n-\n-\nData augmentation [224]\nFew-shot temporal graph classification\n[225]\n[225]\n-\nFew-shot molecular property prediction\n[226]\u2013[229]\n-\nMeta-task reweighting [227], implicit function theorem [230]\nTasks\nOther techniques\nTABLE 5: Summary of few-shot graph classification.\nIn dynamic graphs, existing works usually try to detect\nthe deviation of graph structures among different time steps\n[183]\u2013[185]. For instance, Zambon et al. [184] propose to\nembed each graph in the graph stream into a vector and\nperform change detection on the converted vector stream to\nflag unusual graph patterns. MTAD-GAT [185] employs self-\nattention to capture temporal relations across different time\nsteps for nodes and aggregates node-level anomaly scores\nto generate graph-level anomaly scores.\nSummary. Graph-level anomaly detection is a logical pro-\ngression from node- and edge-level tasks. Many current\napproaches determine node- or edge-level anomaly scores\nand subsequently aggregate these to gauge graph-level\nanomalies. An alternative strategy involves embedding en-\ntire graphs into vectors, effectively transforming graph-level\nanomaly detection into the well-established realm of vector-\nbased anomaly detection. Presently, the primary focus lies\non homogeneous and dynamic graphs. Nevertheless, detect-\ning anomalies at the graph-level on heterogeneous graphs\nremains a crucial area meriting further exploration.\n4.3.3\nFew-Shot Graph Classification\nFew-Shot Graph Classification (FSGC), a crucial facet of\nimbalanced learning on graphs, aims to categorize target\ngraphs into respective novel classes, commonly applied in\nreal-world scenarios such as novel molecular or protein\nprediction. Like few-shot node classification, the objective\nof FSGC is to distill transferable knowledge from base\nclasses and apply it to novel classes, thereby enhancing the\nclassification performance for the latter.\nGeneric FSGC. To address the challenge of FSGC, several\nnoteworthy studies offer distinct solutions. AS-MAML [220]\nemploys MAML [206] to extract meta-knowledge and uses\na reinforcement learning-based adaptive controller to adjust\nthe step size [231]. In contrast, Chauhan et al. [223] utilize\ndistance metrics to cluster closely related graph classes into\nsuper-classes, forming a super-graph. This structure facili-\ntates hierarchical classification at both super-class and class\nlevels, thereby highlighting the importance of inter-class\nrelationships. Different from the class correlations, FAITH\n[221] focuses on the role of task correlations in knowledge\ntransfer. They build a three-layer hierarchical graph captur-\ning sample-, prototype-, and task-level data to facilitate clas-\nsification, emphasizing the role of task correlations in model\nperformance. In addition, Crisostomi et al. [222] combine\na simple metric learning model with advanced graph em-\nbedding techniques, demonstrating excellent results. Their\nstrategy of using a task-adaptive mechanism for layer-\nwise adaptation and Mixup data augmentation showcases\nan effective blend of traditional and novel techniques for\nsuperior performance.\nCross-domain FSGC. For cross-domain FSGC, Hassani et\nal. [224] devise three graph augmentations, including con-\ntextual and two topological augmentations, to enhance rep-\nresentation learning, particularly task-specific information\nfor fast adaptation and task-agnostic information for knowl-\nedge transfer.\nFew-shot temporal graph classification. Fu et al. [225] ex-\nplore a novel setting of few-shot temporal graph classifica-\ntion, where each class may have only a few labeled temporal\ngraphs and novel classes might emerge in the future. They\npresent Temp-GFSM [225], a temporal graph metric learning\nframework. Utilizing attention mechanisms like node-level\nlifelong attention and both intra- and inter-snapshot atten-\ntion, they derive temporal graph representations. They are\nsubsequently used in few-shot metric learning, backed by\nprototypical networks and MAML.\nFew-shot molecular property prediction. In the realm\nof few-shot molecular property prediction, several stud-\nies exploit meta-learning (e.g., MAML) to bridge different\ntasks. PAR [226] applies MAML, utilizing detailed molec-\nular representations to enhance prediction performance.\nMeta-MGNN [227] also employs MAML, but enhances\nits approach by applying self-supervised constraints for\ngraph knowledge extraction and an attention mechanism to\nweight meta-task contributions. Similarly, Borde et al. [228]\nhighlight the applicability of Reptile [232], a MAML-like\nmodel-agnostic algorithm, with GNN models in molecular\nproperty regression tasks.\nAdditionally, various other techniques are also em-\nployed. ADKF-IFT [230] integrates meta-learning with deep\nkernel Gaussian processes [233], proposing adaptive deep\nkernel fitting for versatile feature representation learning\nacross tasks. In contrast, to combat overfitting and improve\ngeneralizability, MTA [229] generates new labeled samples\nusing motifs from a pre-defined vocabulary, enabling con-\nnections between tasks through these augmented motifs.\nSummary. Few-shot graph classification presents the signif-\nicant challenge of effectively transferring knowledge from base\ngraph classes to novel graph classes to enhance the performance of\nthe latter. This challenge has inspired a multitude of research,\nshowcasing diverse strategies to tackle this issue, as sum-\nmarized in Table 5. These studies not only leverage meta-\nlearning strategies, but also employ additional techniques to\nbolster performance, such as data augmentation, super-class\ngraph construction, etc. Nonetheless, certain specific settings\nwithin this critical task still demand further exploration.\nFor instance, in a cross-domain scenario, the transfer of\nknowledge from base to novel classes may pose a greater\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n12\nchallenge; on temporal graphs, the complexity of few-shot\nclassification is further amplified by the temporal patterns.\nFurthermore, few-shot graph-level classification on HINs\ncould also present an intriguing research direction.\n5\nILOGS PROBLEMS: STRUCTURE IMBALANCE\nUnlike other data types such as images or text, graph\ndata inherently contains topological structures that may ex-\nhibit imbalance. The investigation of structure imbalance in\ngraphs has increasingly drawn the attention of researchers\ndue to its prevalence in real-world scenarios. In this section,\nwe extend our discussion for the taxonomy of Problems,\nexamining structure imbalance in graphs from the perspec-\ntives of node (Section 5.1), edge (Section 5.2), and graph\n(Section 5.3) levels, as depicted in the right part of Fig. 3.\n5.1\nNode-Level Structure Imbalance\nNode-level structure imbalance arises when the contextual\nstructures surrounding each node exhibit an unequal dis-\ntribution. A prominent indicator of node-level structure is\nnode degree, representing the count of neighboring nodes\nand reflecting the vicinity richness of a node. In this sec-\ntion, we explore node-level structure imbalance from two\nperspectives: imbalanced node degrees and topologies, as\nsummarized in Table 6. These aspects have significant im-\nplications for various real-world applications, such as cold-\nstart recommendations, and knowledge graph enrichment.\n5.1.1\nImbalanced Node Degrees\nIn graphs, node degrees often follow a long-tail distribution,\nwith head nodes\u2014those with high degrees\u2014benefiting from\nricher structural information and thus achieving superior\nperformance in downstream tasks like node classification\n[18], [27]. Conversely, tail nodes with low degrees have lim-\nited topological information, hindering their performance.\nResearch typically targets two main tasks: improving the\nperformance of tail nodes, and addressing the more chal-\nlenging cold-start nodes that are isolated in the graph. These\ntasks find applications in various domains, such as predict-\ning new papers in a citation network or new items in an\ne-commerce network.\nTail node embedding. Tail node embedding seeks to boost\nperformance for tail nodes, which often underperform com-\npared to head nodes. To tackle this issue, Demo-Net [234]\nand SL-DSGCN [235] utilize degree-specific GNNs to cap-\nture unique structural patterns across nodes of different\ndegrees. Contrastingly, meta-tail2vec [18] and Tail-GNN [27]\nimplement knowledge transfer mechanisms. The former\nleverages meta-learning to transfer knowledge from head\nto tail nodes, redefining tail node embedding as a few-shot\nembedding regression task. Meanwhile, the latter proposes\na neighborhood translation technique to bridge the head\nand tail nodes for knowledge transfer.\nDegree imbalance has also been explored in various\nother tasks. For instance, Residual2Vec [239] delves into\nthe task of fair path sampling, where imbalance may stem\nfrom the differing degrees of nodes. To achieve fairness, it\nreweighs the loss function based on both node labels and\ndegrees. In contrast, CenGCN [240] addresses the issue of\ndominance by hub vertices in scale-free networks, where\nthese vertices can disproportionately propagate influential\ninformation due to vertex imbalance. Notably, CenGCN\nquantifies the similarity between hub vertices and their\nneighbors, and applies graph transformations through edge\nweight adjustments and self-connections, effectively miti-\ngating this issue. Meanwhile, BLADE [241] confronts the\ndegree discrepancy problem in link prediction by using\nbiased neighborhood sampling. By creating neighborhoods\nof varying sizes based on node connectivity, it optimizes\nperformance across different node degrees.\nIn e-commerce networks, tail node embedding aims to\nenhance the performance of less-frequent users and shops.\nOne typical work DHGAT [238] achieves this by leveraging\nfirst- and second-order proximities of user histories, utiliz-\ning attentive mechanisms to further exploit the neighbors\nof target entities. Hao et al. [236], drawing from the concept\nof meta-tail2vec, apply regression to tail nodes to facilitate\nknowledge transfer, enabling cold-start recommendations.\nIn dynamic networks, newly arriving nodes typically con-\nfront structural limitations. To address this, MetaDyGNN\n[237] targets few-shot link prediction. It harnesses a meta-\nlearning approach to create a hierarchical architecture that\napplies interval- and node-wise adaptations, thereby facili-\ntating the knowledge transfer from existing to new nodes.\nThe approach also includes a dynamic GNN model to\nefficiently exploit local node structures.\nCold-start node embedding. In certain extreme cases,\ngraphs may contain numerous isolated nodes, known as\ncold-start nodes, which have no neighbors and thus pose\na challenge for conventional graph representation learn-\ning approaches. To tackle this issue, techniques typically\nemploy the knowledge distillation paradigm [252]. These\napproaches utilize a teacher network for knowledge extrac-\ntion on information-sufficient nodes and a student network\nfor knowledge application on cold-start nodes, ultimately\nimproving the performance of the latter.\nCold Brew [57] is a notable approach designed to address\ncold-start node representation learning by leveraging the\nconcept of knowledge distillation. Specifically, the method\nemploys a GNN network as the teacher network to extract\nknowledge from information-sufficient nodes and a mul-\ntilayer perceptron (MLP) as the student network to distill\nthe knowledge from the teacher. Consequently, the student\nnetwork can predict suitable representations for cold-start\nnodes without relying on neighborhood information.\nSummary. The central challenge in managing imbalanced\nnode degrees lies in the efficient knowledge transfer from\nhead nodes to tail or cold-start nodes. Various strategies have\nemerged to address this, as summarized in Table 6. Notably,\ndegree-aware model modulation and meta-learning are fre-\nquently utilized for tail node embedding, while knowledge\ndistillation is favored for cold-start nodes. For further de-\ntails of these techniques, we direct readers to Section 6.1.1.\nCertain areas still warrant further exploration. For instance,\nthere is a notable dearth of research on the pivotal task\nof cold-start node embedding. Moreover, while knowledge\ndistillation is typically utilized for cold-start node embed-\nding, its potential for handling other similar tasks, such as\ntail node embedding, could also be investigated. Further-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n13\nTasks\nDegree-aware modulation\nMeta-learning\nKnowledge distillation\nOther techniques\nTail node embedding\n[234], [235]\n[18], [236], [237]\n-\nNeighborhood translation [27]\nHybrid-order proximities [238]\nReweighting [239]\u2013[241]\nCold-start node embedding\n-\n-\n[57]\n-\nNode topology imbalance\n-\n-\n-\nReweighting [242], [243]\nGraph geometric embedding [244]\nLong-tail entity embedding on KGs\n[245]\n[246]\u2013[249]\n-\nOpen knowledge enrichment [250]\nSynthetic data generation [251]\nTABLE 6: Summary of node-level structure imbalance.\nmore, we will discuss long-tail entity embedding on KGs, a\nrelated task, in Section 5.1.3.\n5.1.2\nNode Topology Imbalance\nIn contrast to the frequently studied class imbalance rooted\nin the quantity-centered disparity in the number of labeled\nsamples across classes (i.e., imbalanced node classification\nas discussed in Section 4.1.1), topology imbalance on graphs\nshifts the focus to the positional distribution of labeled\nsamples. This positional discrepancy critically influences\nthe label propagation process on the graph. Ideally, in la-\nbel propagation, the influence boundaries of labeled nodes\nshould coincide with the true class boundaries. However,\nthe specific positions of labeled nodes can trigger a displace-\nment of these influence boundaries away from the true class\nboundaries, resulting in skewed model decision boundaries.\nClasses with labeled nodes having influence boundaries\nmore closely aligned with the true class boundaries tend\nto propagate label information more effectively than those\nwithout such alignment, leading to an imbalance, as illus-\ntrated in Table 1. This uneven distribution could potentially\ninduce biases in learning representations, thereby impacting\nthe performance of nodes across different classes [242].\nTo address this, ReNode [242] reweights labeled nodes\nbased on their proximity to class boundaries, improving per-\nformance particularly for boundary-near and remote nodes.\nThey also develop a metric to quantify this imbalance using\ninfluence conflict detection. Another work PASTEL [243]\ncombats topology imbalance by optimizing information\npropagation paths. It aims to alleviate under-reaching and\nover-squashing effects by enhancing intra-class connectivity\nand employing a position encoding mechanism. PASTEL\nalso uses a class-wise conflict measure for edge weights to\nfacilitate node class separation.\nIn graph data with hierarchical structures, hierarchy im-\nbalance represents an unequal distribution of labeled nodes\nacross hierarchical levels. This issue may impede the effec-\ntive learning and classification of nodes in underrepresented\nhierarchy levels, also affecting the decision boundary of\nthe classifier [244]. To tackle this challenge, HyperIMBA\n[244] utilizes hyperbolic geometric embedding to gauge the\nhierarchy of labeled nodes. Subsequently, it adjusts label\ninformation propagation and alters the objective margin\nbased on the hierarchy of the node, thereby addressing the\nissues caused by hierarchy imbalance.\nSummary. In addition to the extensively studied imbalance\nissues (e.g., class or degree imbalance) that affect the perfor-\nmance of node classification, the topology of nodes, which\ndepicts the interconnectedness of nodes from the label in-\nformation, remains underexplored. Specifically, given the\nimportance of label propagation on graphs, the connecting\ntopologies could play a pivotal role in the node classification\ntask, indicating a need for further investigation in this area.\n5.1.3\nLong-Tail Entity Embedding on KGs\nKnowledge graphs often present a long-tail distribution of\ntriplets across entities, potentially causing performance bias\n[253] in applications such as KG enrichment. To address this\nissue, a series of approaches have been proposed.\nGEN [246] is a transductive meta-learning framework\npredicting links for new entities. It uses aggregation on\nneighboring entities and relations, and employs MAML\n[206] for knowledge transfer from seen (head) to unseen\n(tail) entities. The works of MaKEr [247], MTKGE [249],\nand MorsE [248] follow a similar paradigm, employing\nboth meta-learning and GNN encoding to tackle this issue.\nSpecifically, MaKEr operates within a federated setting,\nbasing its process on representations calculated on their con-\nstructed relation position graph. MTKGE, on the other hand,\nconcentrates on the relative position and temporal sequence\npatterns between relations to extrapolate missing facts in\nemerging temporal knowledge graphs. MorsE, meanwhile,\nfocuses on inductive knowledge graph embedding, by in-\ncorporating two key modules\u2014an entity initializer and a\nGNN modulator\u2014for enhanced entity embedding. In addi-\ntion, KG-Mixup [251] generates synthetic triples with Mixup\nfor data augmentation, aiming to enhance KG completion\nperformance, particularly on tail entities.\nSome approaches also leverage additional knowledge to\naddress this issue. For instance, Zeng et al. [245] propose a\ndegree-aware co-attention network to guide information fu-\nsion from different sources for tail entity alignment, forming\nan iterative paradigm to improve KG completion. Another\nwork OKELE [250] aims to increase tail entity triplets by\nextracting open knowledge from the Web. It constructs an\nentity-property graph on the given KG for attentive GNN-\nbased representation learning.\nSummary. Tail entities on KGs share similar characteristics\nwith tail nodes on graphs, as both have a scarcity of linked\nneighbors. Notably, some similar techniques, such as meta-\nlearning, are employed to address tail entity embedding.\nMoreover, supplementary knowledge, such as auxiliary\nKGs, serves as an additional source of information to aid in\nresolving this issue. Consequently, the solutions proposed\nfor tail entity or node embedding can draw inspiration from\neach other to enhance their respective performance.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n14\n5.2\nEdge-Level Structure Imbalance\nEdge-level structure imbalance in complex graphs, like KGs,\nmanifests as varying edge (relation) frequencies, often fol-\nlowing a long-tail distribution [33], [254]. This imbalance can\nbias the model, marginalizing long-tail relations and leading\nto suboptimal performance\u2014a challenge that is garnered\nsignificant research attention due to its implications in real-\nworld scenarios. We delve into edge-level topology imbal-\nance through three lenses: few-shot relation classification,\nzero-shot relation classification, and reasoning with few-\nshot relations, and summarize them in Table 7. Note that\nthe frequency of edges reflects their structural distribution\nin KGs, hence we classify this aspect as structure imbalance.\nAlternatively, it can also be viewed as class imbalance,\nconsidering edges as labeled instances of the edge class.\n5.2.1\nFew-Shot Relation Classification\nIn scenarios such as KG enrichment, there are often novel\nrelations that have only a few available triplets. In contrast,\nnumerous base relations are provided to support classifica-\ntion tasks involving these novel relations. In this context,\nthe objective of Few-Shot Relation Classification (FSRC) is\nto classify triplets into novel relations, with only a limited\nnumber of triplets serving as supervision.\nGeneric FSRC. Within the realm of FSRC on KGs using\nmetric-based meta-learning methods [207], [282], various stud-\nies exhibit similarities in approach, albeit with distinct con-\ntributions. Certain investigations implement unique tech-\nniques for metric computation and encoding. For instance,\nGmatching [33] and FSRL [260] employ matching networks,\ndiffering in their methods of entity representation. While\nGmatching utilizes a GNN encoder, FSRL infers entity em-\nbeddings by encoding heterogeneous neighbors. Further-\nmore, MetaP [266] exploits co-occurrence relation patterns\nin meta-tasks to enhance the effectiveness of proximity\ncalculations within matching networks.\nA common approach revolves around the usage of atten-\ntion mechanisms and prototypical networks. For instance,\nGao et al. [254] and KEFDA [262] utilize attention-based\nprototypical networks. The former [254] is distinguished by\nits instance- and feature-level hybrid attention mechanism,\nenhancing prototype calculation accuracy. KEFDA [262], on\nthe other hand, stands out by incorporating general and\ndomain knowledge into the model. The theme of attention\nmechanisms continues in IAN [261] and HMNet [263], with\neach focusing on different aspects. IAN [261] computes\ninter- and intra-instance correlations to guide fusion oper-\nations, while HMNet [263] conducts dual scoring from both\nentity and relation perspectives for effective link prediction.\nA number of studies present other contributions based\non prototypical networks. Both FAAN [264] and work [269]\nutilize adaptive learning models to tackle FSRC. Specifically,\nFAAN introduces an adaptive attentional network to cap-\nture dynamic properties, while the latter proposes an adap-\ntive mixture mechanism that incorporates label words into\nclass prototypes. On the other hand, MULTIFORM [267] en-\nriches entity representations by incorporating multi-modal\ncontexts. And another research [268] adopts a granularity-\naware method, depicting each relation as an area to account\nfor variations in granularities.\nAmong\nstudies\nemploying\noptimization-based\nmeta-\nlearning methods for FSRC on KGs, MAML [206] serves\nas a key foundation. Both MetaR [255] and MTransH [256]\nemploy MAML for link prediction and few-shot training\nrespectively, with MetaR using an MLP for relation rep-\nresentation and MTransH integrating a gated and atten-\ntive neighbor aggregator. Li et al. [257] deviate by jointly\ntraining a prototype and instance encoder, enhancing the\nrelation classification accuracy of the model. In addition,\nMick [283], HiRe [258] and Meta-iKG [259] explore different\nfacets: Mick explores FSRC from the perspective of limiting\navailable data during training, HiRe focuses on hierarchical\nrelational learning across different levels, while Meta-iKG\nutilizes local subgraphs for efficient pattern learning and\ngeneralization across few-shot and large-shot relations.\nSome studies highlight other techniques or aspects of KGs\nfor FSRC. For instance, Gao et al. [271] deploy an iterative\nneural snowball process with fake labels for novel rela-\ntions, relying on a relational Siamese network [284] and\na relation classifier. REFORM [272] uses an error mitiga-\ntion mechanism alongside attentive neighborhood encoding\nand cross-relation aggregation for error-aware completion.\nIn contrast, P-INT [275] encodes relations via expressive\npaths for accurate matching. Similar to IAN [261], CIAN\n[270] leverages attention mechanisms to capture both intra-\nand inter-entity interactions for better entity pair represen-\ntations. Meanwhile, NP-FKGC [276] merges normalizing\nflows and neural processes to manage complex relations and\nestimate uncertainties, aided by an attentive relation path-\nbased GNN to better capture KG path information.\nFSRC with uncommon entities. Wang et al. [273] consider\nan extreme setting with uncommon entities and long-tailed\nrelations. They use textual descriptions (i.e., description en-\ncoder) to extract crucial information for entity and relation\nembeddings. To address data limitations, they propose a\ntriplet generator to synthesize fake triplets for augmenta-\ntion, augmenting both limited entities and relations. Reptile\n[285] is employed for model optimization.\nFSRC on uncertain KGs. In uncertain KGs, where each\ntriplet has a confidence score indicating its certainty, few-\nshot relation classification remains a challenge. Zhang et\nal. address this issue with Gaussian Metric Learning [274],\nwhich completes missing facts and confidence scores with\nfew available examples. They propose a Gaussian neighbor\nencoder to represent relation facts as multi-dimensional\nGaussian distributions, simultaneously learning semantic\nfeatures and internal uncertainty. Additionally, they propose\na Gaussian matching function considering fact qualities to\ndiscover new facts and predict confidence scores.\nFew-shot inverse relation classification. For few-shot in-\nverse relation classification, FAEA [265] employs a hybrid\nattention model to attend class-related words based on\nmeta-learning. It leverages function-words enhanced atten-\ntion to effectively compute representations for both support\nand query instances, facilitating message passing and simi-\nlarity calculation between queries and prototypes.\nSummary. The principal challenge of few-shot relation clas-\nsification is the effective transfer of knowledge from data-rich\nrelations to long-tail relations, aiming to bolster the perfor-\nmance of the latter. As depicted in Table 7, most existing\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n15\nMeta-learning techniques\nOptimization-based\nMetric-based\nFew-shot relation classification\n[255]\u2013[259]\n[33], [254], [260]\u2013[270]\nZero-shot relation classification\n-\n[277]\nGAN [278]\nFew-shot multi-hop reasoning on KGs\n[260], [279]\u2013[281]\n-\n-\nTasks\nOther techniques\nNeural snowball [271], error mitigation [272],\ndata augmentation [273], gaussian metric\nlearning [274], path-based interactions [275],\nentity interactions [270], neural process [276]\nTABLE 7: Summary of edge-level structure imbalance.\napproaches rely heavily on meta-learning techniques, such\nas prototypical networks and MAML, to counter the imbal-\nance issue. Some methods also employ different techniques\nlike the neural snowball [271], error mitigation [272], among\nothers. Moreover, several extreme scenarios have also been\nexplored, including few-shot relation classification with un-\ncommon entities, within uncertain knowledge graphs, and\nfew-shot inverse relation classification. Delving deeper into\nthese extreme scenarios could further advance progress in\nFSRC. Additionally, benchmarks and leaderboards, such as\nFewRel2 [286], [287], have been introduced for comparison.\n5.2.2\nZero-Shot Relation Classification\nZero-shot relation classification on KGs, classifying triplets\nwithout given supervision, typically leverages auxiliary text\ndescriptions for zero-shot relation representation. For in-\nstance, ZSGAN [278] employs GAN [60] to transfer knowl-\nedge from labeled to zero-shot relations. The generator\naims to produce zero-shot relation embeddings using text\ndescriptions, while the discriminator classifies the generated\nembeddings into respective classes. Putting it differently,\nZSLRC [277] utilizes side information through hypernym\nand keyword extraction for new relation type detection. It\nthen implements a side-information-enhanced prototypical\nnetwork for few-shot relation classification by computing a\nweighted side information embedding for each relation.\n5.2.3\nFew-Shot Multi-Hop Reasoning on KGs\nMulti-hop reasoning on KGs offers an effective method for\ninferencing target entities given query entities and relations,\na technique crucial for tasks like query answering. However,\nit is often challenged by a limited number of labeled triplets,\nleading to a few-shot setting.\nEfforts to tackle this issue, such as Meta-KGR [279] and\nFIRE [288], usually utilize reinforcement learning [231] for\nmulti-hop reasoning and MAML to extract meta-knowledge\nto address the few-shot problem. To improve the general-\nization abilities, THML [280] further introduces a hardness-\naware meta-reinforcement learning method. This method\ntrains hardness-aware batches using a two-level hardness-\naware sampling approach, addressing low reasoning accu-\nracies over challenging relations. In a similar vein, ADK-\nKG [281] enhances the standard reinforcement learning and\nMAML modules with text-enhanced heterogeneous GNN\nfor better node embeddings. It also incorporates a knowl-\nedge distillation module using unlabeled data to generate\nsynthetic labels, further improving model training.\n2. https://github.com/thunlp/FewRel.\n5.3\nGraph-Level Structure Imbalance\nThe intricate interconnections within graphs can lead to\nstructural imbalance across graphs. This type of imbalance\noften manifests as differences in graph sizes [34], or topol-\nogy groups [289], as illustrated in Table 1. Typically, graphs\nwith advantageous structures, such as larger sizes, are more\nexpressive and subsequently yield superior performance\ncompared to their counterparts, which may introduce bias\nin applications such as molecular or protein prediction.\n5.3.1\nImbalanced Graph Sizes\nGraphs with larger sizes (e.g., number of nodes), which\noffer more intricate structures, are generally more expres-\nsive than smaller ones. This disparity in size often results\nin performance biases in graph-level tasks such as graph\nclassification [34]. To address this challenge, SOLT-GNN\n[34] is designed to enhance the performance of smaller,\nor \u201ctail\u201d, graphs. SOLT-GNN first identifies co-occurrence\npatterns in the structures of larger, or \u201chead\u201d, graphs, to\ngenerate transferable knowledge. This information is then\nutilized to augment tail graphs by predicting co-occurrence\npatterns that supplement their existing structures. Thus, the\nperformance of tail graphs can be improved accordingly.\n5.3.2\nImbalanced Topology Groups\nContrasting with the node topology imbalance discussed\nin Section 5.1.2, imbalanced topology groups [289] refer to the\nunequal distribution of topology groups, such as topological\nmotifs, within individual classes in graph data. Take, for\nexample, the Mutag dataset [289], where molecular graphs\nof the Mutagenic class contain two distinct topology groups:\none associated with the motif NO2 and the other with\nNH2. In certain scenarios, the NO2 group may substantially\noutnumber the NH2 group, which results in the NO2 motif\nhaving stronger associations with the class than the less\nfrequent NH2 motif. This imbalance, anchored in motif\ndistribution, can lead to a paucity of training instances for\nminority groups. Consequently, classifiers may overfit to the\nmajority topology (motif) groups, hampering effective learn-\ning and classification of instances in the underrepresented\nminority topology groups. To address this issue, TopoImb\n[289] dynamically updates the discovery of topology groups\nand assigns importance weights to under-represented in-\nstances during training, by incorporating a topology extrac-\ntor and a training modulator. This approach improves the\neffectiveness of learning on minority topology groups and\naddresses the issue of over-fitting to majority groups.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n16\nImbalanced Learning on Graphs: Techniques\nImproving the Low-Resource Part\nBalancing High/Low-Resource Parts\nKnowledge Transfer\nData Reweighting and Resampling\nMeta-Learning\nModel Pre-training\nKnowledge Distillation\nCommon Knowledge Sharing\nAuxiliary Data\nSynthetic Data Generation\nSMOTE\nGenerative Adversarial Nets\nAdditional Constraints\nFig. 4: Taxonomy of Techniques.\nSummary In conclusion, the problem of graph-level struc-\nture imbalance is still a largely underexplored domain that\ncalls for more in-depth study. The core challenge in this\narea is the enhancement of the expressiveness of tail (sub)graphs\nor groups. Predominantly, current methods tend to utilize\nknowledge transfer or reweighting techniques to address\nthis issue. By drawing inspiration from the tasks previ-\nously discussed, a variety of strategies could potentially be\nadapted and applied to grapple with this intricate yet under-\nstudied task. Additionally, other types of graph-level imbal-\nance issues that could potentially impact the performance\nof graph learning models may also require investigation,\nfurther emphasizing the need for continued research.\n6\nTECHNIQUES OF ILOGS\nAs detailed in Section 1, graph imbalance issues typically in-\nvolve two primary components: the high- and low-resource\nparts. This leads to two main task categories: improving\nthe low-resource part and balancing both high and low-resource\nparts. In this case, Section 6.1 will elaborate on tasks aimed\nat improving the performance of the low-resource part,\nsuch as few-shot node classification [58], [59], [189] and\ntail/cold-start node representation learning [27], [57]. In\naddition, Section 6.2 will explore tasks aiming to equalize\nperformance across both parts, essential in scenarios like\nimbalanced classification tasks [25], [35], [37].\nOur taxonomy, as summarized in Fig. 4, offers a well-\norganized and extensive overview of the existing literature\nacross multiple tasks. This classification approach not only\neffectively differentiates various scenarios, but it also aids\nin identifying strategies bespoke to these specific settings,\nthereby facilitating effective management of graph imbal-\nance issues, which we will discuss further in Section 6.3.\nWe also summarize the literature w.r.t. this taxonomy\nin Table 8. It is important to note that this categorization\nis not inflexible: some studies may incorporate techniques\nfrom another branch to mitigate imbalance issues in their\nown domains. For example, some studies [251] apply Mixup\nfor synthetic data generation, enhancing the performance\nof the low-resource part. Others, such as [32], [63], use\ntechniques like common knowledge sharing or knowledge\ndistillation for knowledge transfer, seeking balanced per-\nformance across both parts. Note that, despite these cross-\nbranch applications, our discussion primarily centers on\nmajor strategies for tackling imbalance.\n6.1\nImproving the Low-Resource Part\nThe primary goal of tasks designed to improve the low-\nresource part is to boost their performance, despite having\nless data compared to the high-resource part. They com-\nmonly exploit the high-resource part, which often holds\na wealth of knowledge, as a resource to enrich the low-\nresource part. This category includes various graph tasks\nsuch as few-/zero-shot node/edge/graph classification,\nfew-shot link prediction, or few-shot reasoning on KGs,\nand cold-start/tail node/entity/graph embedding, among\nothers. To handle the imbalance issues, two main techniques\nare typically used: knowledge transfer and the incorporation\nof auxiliary data, in order to leverage existing resources to\nsupplement low-resource parts.\n6.1.1\nKnowledge Transfer\nIn typical settings, there often exists a data part that is\ncharacterized by a wealth of source knowledge, such as\nthe base classes within the framework of few-shot node\nclassification, or high-degree nodes in tail node embedding\nsituations. Knowledge transfer intends to harness this reser-\nvoir of knowledge residing within the data-rich source, and\ntransfer it to the knowledge-deficient segment, thus bolster-\ning its performance. To enable this, a suite of techniques\nhas been engineered to facilitate efficient knowledge transfer\nbetween the source and target data, thereby enhancing the\nefficacy of imbalanced learning on graphs.\nMeta-learning. Meta-learning, distinct from traditional ma-\nchine learning methods, extracts versatile meta-knowledge\nfrom a series of base meta-tasks with a shared distri-\nbution. This knowledge is then applied to novel meta-\ntasks to enhance predictive performance. Many studies\non ILoGs have utilized this approach to extract transfer-\nable meta-knowledge from high-resource parts (base meta-\ntasks) and apply it to low-resource parts (novel meta-tasks).\nTechniques employed often include optimization-based ap-\nproaches (e.g., MAML [206]) and metric-based approaches\n(e.g., prototypical networks [207]), among others.\nMAML, compatible with any optimization-based mod-\nels, learns a base task prior that allows quick adaptation\nto the support set of a meta-task, facilitating rapid predic-\ntions on the query set. Its effectiveness supports various\napproaches addressing imbalanced learning on graphs [30],\n[58], [188], [189], [204], [220].\nConversely, within a meta-task, prototypical networks\naim to identify the center of each class based on the support\nset, using distance-based metrics to classify novel instances\nin the query set. This simple but effective method is widely\nused in imbalanced learning on graphs to leverage transfer-\nable knowledge [26], [59], [191]\u2013[193], [202]. Additionally,\ntechniques like Matching Networks [282] and Siamese Net-\nworks [284] are also commonly used in this context.\nModel pre-training. The paradigm of \u201cpre-training, fine-\ntuning\u201d is a widely accepted two-step model training strat-\negy in machine learning. Initially, models are pre-trained\non a large dataset using self-supervised learning, which\nuncovers the underlying knowledge embedded within the\ndata. Following this, the pre-trained models are fine-tuned\non a smaller, task-specific supervised dataset. This strategy\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n17\nTechniques\nLiterature\nModel pre-training\nGNN parameters transfer [195], contrastive learning [196], [197], prompting [198]\nKnowledge distillation\nGNNs to MLPs [57], KG models to MLPs [278], Random knowledge distillation [73]\nData sharing\nModel sharing\nAuxiliary data\nAlignment data [245], auxiliary descriptions [216], [217], [277]\nReweighting and resampling\nReweighting [61], [75], [120], [125], [147], [179], [240], [242], [244], [289], resampling [36], [69], [83], [131], [134],\n[136], [143]\u2013[145], [145], [148], [165], [172], [241], [243]\nSMOTE\nSMOTE [25], Mixup [65]\u2013[67]\nGAN\n[37], [102], [110], [164], [180]\nOther methods\nPredictive data generation [74], [135], label generation [77], [167], [170]\nAdditional constraints\nCondition relax constraints [139], [213]\u2013[215], imbalance constraints [62], [112], class separation constraints\n[35], [72], [78]\u2013[101], [103]\u2013[107], [109], [111], [113]\u2013[119], [121]\u2013[124], [126]\u2013[130], [132], [133], [137], [138],\n[140]\u2013[144], [146]\u2013[163], [166], [168], [169], [171], [173]\u2013[179], [181]\u2013[185], [290]\nImproving\nthe\nlow-\nresource\npart\nKnowledge transfer\nMeta-learning\nOptimization-based\n[18], [31], [58], [76], [108], [188]\u2013[190], [193], [203], [204], [209],\n[210], [220], [225]\u2013[229], [237], [246], [255], [256], [258], [259],\n[273], [279]\u2013[281], [283], [288]\nMetric-based\n[26], [33], [59], [191], [192], [194], [199], [200], [202], [221], [222],\n[224], [254], [257], [260]\u2013[270], [274], [277]\nCommon\nknowledge sharing\nSuper-classes [223]\n[27], [34], [234], [235]\nBalancing\nhigh/low-\nresource\nparts\nSynthetic data\ngeneration\nTABLE 8: Literature categorization of imbalanced learning on graphs w.r.t. the taxonomy of techniques.\nbenefits the fine-tuning stage by providing useful initial-\nizations for the models, particularly useful when dealing\nwith limited labeled data. This strategy has demonstrated\nsuccess across various tasks, setting benchmark results and\nestablishing itself as a robust paradigm [291]\u2013[293].\nWithin the sphere of ILoGs, a typical strategy involves\npre-training models on the high-resource parts to distill the\nembedded knowledge, such as pre-training GNN models in\nalignment with the label information of the high-resource\nparts [195], [196]. Subsequently, these pre-trained models\nundergo fine-tuning with limited supervision from the low-\nresource parts, thereby facilitating the transfer of knowledge\nto enhance the performance of these low-resource parts.\nKnowledge distillation. Knowledge distillation [252] is a\ntechnique wherein a smaller \u201cstudent\u201d model learns from\na larger \u201cteacher\u201d model by mimicking its behavior. The\nteacher model offers soft targets to guide the student model,\nthereby transferring its knowledge to the student, aiming to\nboost efficiency while preserving performance [294], [295].\nRegarding ILoGs, one approach involves constructing\na comprehensive teacher model for the high-resource part\nto extract knowledge. To tackle data scarcity in the low-\nresource part, a student model is developed, guided by the\nteacher, facilitating training on limited data. This technique\nhas shown noteworthy usage in situations such as cold-\nstart node embedding [57], as detailed in Section 5.1.1. In\nsummary, knowledge distillation is particularly beneficial\nin scenarios where distinct models are needed to decode\npatterns within high- and low-resource parts of the graph.\nCommon knowledge sharing. Common knowledge sharing\nforms the crux of knowledge transfer, establishing a conduit\nbetween high- and low-resource parts through shared ele-\nments. Some studies propose shared data [223] or shared\nsub-models [27], [34], [234], [235] as bridges for knowledge\ntransfer. These shared components serve as critical resources\nfor models associated with both high- and low-resource\nparts, facilitating further model development while main-\ntaining a conversation via this shared bridge.\nThese methods are particularly beneficial in ILoGs.\nStrategies employing shared data construct overarching\nrelationships for both high- and low-resource parts, like\nsuper-classes based on original classes [223]. Establishing\nthis hierarchical structure enables knowledge transfer from\nhigh-resource (e.g., majority classes) to low-resource parts\n(e.g., minority classes) via shared relationships. In contrast,\nstudies addressing structure imbalance commonly design a\nshared model for high- and low-resource parts, further in-\ncorporating model modulation to adapt the globally shared\nmodel to the unique characteristics of each part [27], [34],\n[234], [235]. The shared component, or globally shared\nmodel, retains common knowledge for both parts and serves\nas a bridge for knowledge transfer from high-resource (e.g.,\nhigh-degree nodes [27], [234], [235] or large-size graphs [34])\nto low-resource parts (e.g., low-degree nodes or small-size\ngraphs). This approach is generally effective in scenarios\nwhere certain common attributes are shared by both parts,\nsuch as high-level features from the data-sharing perspec-\ntive (e.g., super-classes) or common knowledge which can\nbe preserved by a shared model.\n6.1.2\nAuxiliary Data\nAuxiliary data in machine learning refers to supplemen-\ntal information that enhances the primary dataset, conse-\nquently improving model performance. For ILoGs, auxiliary\ndata often underpins the learning process of low-resource\nparts by contributing additional knowledge [296], [297].\nAuxiliary data in the realm of ILoGs takes various forms,\nenabling it to support multiple tasks. For zero-shot learning,\ntext descriptions can offer class information for each class,\nfacilitating classification in the absence of label support\n[216], [217], [277]. Some research centered on knowledge\ngraph completion exploits auxiliary KGs to align with the\ntarget KG, providing extra information for KG completion\nconcerning long-tailed entities [245] or zero-shot relations\n[277], [278]. As such, the use of auxiliary data can be con-\ntemplated in most scenarios of ILoGs where it is available.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n18\n6.2\nBalancing the High- and Low-Resource Parts\nIn contrast to the objective of solely improving the low-\nresource part, balancing the high- and low-resource parts\naims to enhance the performance of the low-resource part\nwhile minimizing potential performance degradation in the\nhigh-resource part. Typical tasks often involve imbalanced\nnode/graph classification [25], [32], anomaly detection [36],\n[147], etc. To accomplish this, three primary techniques have\nbeen proposed: data reweighting and resampling, synthetic data\ngeneration, and the incorporation of additional constraints.\n6.2.1\nData Reweighting and Resampling\nData resampling and reweighting techniques are commonly\nemployed in imbalanced learning to address class imbalance\nissues within training data. Class imbalance occurs when\nthere is an unequal distribution of classes, leading to a bias\ntowards the majority class and reduced performance on\nminority classes. Both data resampling and reweighting aim\nto rebalance the class distribution, allowing the model to\nlearn from minority class examples more effectively. Within\nthe scope of ILoGs, data reweighting and resampling meth-\nods are prevalently employed to address class imbalance\nchallenges [69], [242]. These techniques are applicable in\nnumerous scenarios plagued by imbalance issues.\n6.2.2\nSynthetic Data Generation\nSynthetic data generation techniques are widely employed\nin ILoGs to address the challenge of class imbalance by gen-\nerating synthetic data for data rebalance. These techniques\noffer valuable solutions to achieve balanced performance in\nscenarios where there is a significant disparity between the\nhigh- and low-resource parts of the data. Specifically, two\ncommonly used techniques for synthetic data generation are\nSMOTE (Synthetic Minority Over-sampling Technique) [16]\nand GAN (Generative Adversarial Networks) [60].\nSMOTE. SMOTE [16] is a well-known approach utilized in\nimbalanced learning to combat the class imbalance problem\nwithin training data. It involves creating synthetic samples\nby interpolating between existing minority class instances.\nFor each minority class instance, SMOTE selects one or\nmore k-nearest neighbors from the same class and generates\nsynthetic examples by linearly interpolating between the\nselected instance and its neighbors. This process effectively\nexpands the minority class and introduces additional data\npoints to improve its representation during training.\nFurthermore, Mixup [68], a data augmentation technique\ninitially proposed for training deep neural networks, can\nalso be leveraged in imbalanced learning scenarios. Mixup\nextends the idea of interpolation by generating virtual train-\ning examples through linear combinations of instance pairs.\nAs a special case of SMOTE, Mixup can also promot model\ngeneralization and reducing overfitting [68].\nIn the context of ILoGs, researchers have explored the\nadaptation of SMOTE and Mixup to address the class im-\nbalance issue [25], [65], [66]. These methods are particularly\nbeneficial when there is a need to bolster the low-resource\nsegment with additional instances for data rebalance, for\ntasks like imbalanced node/graph classification.\nGAN. Generative Adversarial Networks (GANs) [60] are\na class of machine learning techniques composed of two\nProblems\nTask analysis\nTechnique selection\nExploring techniques\nfrom other branches\nImproving the low-resource part\nBalancing high/low-resource parts\nKnowledge transfer\nAuxiliary data\nData reweighting and resampling\nSynthetic data generation\nAdditional constraints\nDiffusion models\nData prompting\nFoundation models\n... ...\nStep 1\nStep 2\nProblems\nTask analysis\nTechnique selection\nExploring techniques\nfrom other branches\nImproving the low-resource part\nBalancing high/low-resource parts\nKnowledge transfer\nAuxiliary data\nData reweighting and resampling\nSynthetic data generation\nAdditional constraints\nDiffusion models\nData prompting\nFoundation models\n... ...\nStep 1\nStep 2\nFig. 5: Procedure of techniques selection.\nmodules, a generator and a discriminator, which are trained\ntogether in an adversarial manner. GANs are known for\ngenerating realistic and high-quality data, such as images,\nthat closely resemble the original data distribution.\nIn the context of ILoGs, some research (e.g., IMGAGN\n[37]) has explored the use of GANs to generate syn-\nthetic samples for minority classes, functioning similarly to\nSMOTE. These GAN-based techniques can help balance the\ndata distribution and improve the model performance on\nminority classes in imbalanced graph learning scenarios.\n6.2.3\nAdditional Constraints\nAdditional constraints can be effectively incorporated in\nILoGs to bridge the gap between high- and low-resource\nparts, to allow knowledge transfer between them while\nadhering to predefined constraints, thereby enhancing per-\nformance. For instance, some studies [213]\u2013[215] have at-\ntempted to relax the intra-class similarity requirement by\nallowing nodes with the same labels to reside on the same\nmanifold in the embedding space. In particular, the selection\nand design of these constraints depend on the specific\ncharacteristics of the dataset and the learning tasks at hand.\n6.3\nAppropriate Techniques Selection\nSelecting the proper technique for ILoGs is pivotal, follow-\ning a specific paradigm, as shown in Fig. 5. The process\ncomprises task analysis, technique selection, and further ex-\nploration of methods from related machine learning fields.\nFirstly, task analysis involves understanding the data\nimbalance and its challenges. This step requires determining\nthe data split into high- and low-resource parts and identi-\nfying the primary goal: improving the performance of the\nlow-resource part, or balancing both parts. This analysis is\nthe foundation for subsequent technique selection.\nSecondly, technique selection involves considering various\nmethods to address the identified imbalance issue. Each\ntechnique offers a unique perspective and capability to ad-\ndress the problem, making it a potential selection candidate.\nFor tasks focused on enhancing low-resource part per-\nformance, knowledge transfer and the use of auxiliary data\nare key considerations. Knowledge transfer is particularly\nbeneficial when ample base data is available. Meta-learning\nis effective for meta-task-oriented tasks, and model pre-\ntraining is recommended when valuable information can be\nextracted from unlabeled base data. Knowledge distillation\nworks well when different models operate on different\ndata parts. Common knowledge sharing technique aids in\ncapturing shared patterns (data or model) across both parts,\nthus promoting knowledge transfer.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n19\nFor tasks centered on balancing the performance of\nboth high- and low-resource parts, data reweighting and\nresampling, synthetic data generation, and additional con-\nstraints are essential considerations. Data reweighting and\nresampling can rectify imbalance issues and enhance data\ndistribution. Synthetic data generation techniques, such as\nSMOTE or GAN, can generate synthetic instances for the\nlow-resource part, thus also rebalancing the data distribu-\ntion. Additionally, when specific conditions exist for model\ntraining based on data characteristics, additional constraints\ncan guide the model training to address imbalance issues.\nFinally, exploring techniques from other branches is a worth-\nwhile endeavor. On one hand, although most techniques\nare categorized under the two branches\u2014improving the\nlow-resource part or balancing the high- and low-resource\nparts\u2014the boundaries are not strict. Namely, techniques\nintended for one branch may be applicable to tasks in the\nother. For instance, in the context of improving low-resource\npart performance, synthetic data generation and additional\ntask-specific constraints, originally meant for balancing both\nparts, can potentially bolster the data and performance.\nConversely, techniques like knowledge transfer, can poten-\ntially foster balancing both parts by transferring knowledge\nfrom high-resource to low-resource parts. On the other\nhand, exploring novel techniques that are not commonly\nused in ILoGs can also be beneficial, e.g., diffusion models\n[71], data prompting [291], foundation models [187], etc.\nThese techniques may bring new perspectives and potential\nsolutions for addressing imbalance issues on graphs.\n7\nOTHER RELATED LITERATURE\nIn addition to the primary scenarios of ILoGs, there are other\ncontexts that fuse graph learning techniques and imbalance\nissues, such as graph fairness learning [298], cold-start rec-\nommendation [299], and imbalanced road networks [300].\nIn this section, we present a concise overview of the tasks\nclosely related to our discussion, including fairness learning\non graphs and imbalanced learning in recommendations.\n7.1\nFairness Learning on Graphs\nFairness learning addresses the imbalance in input at-\ntributes linked to certain predictions due to historical prej-\nudices in data collection. This imbalance, particularly cru-\ncial when dealing with sensitive attributes like gender,\nrace, or education, can produce discrimination in model\noutcomes, fostering unfairness. Unlike other imbalanced\nlearning forms, fairness learning aims to minimize predic-\ntion discrepancies between different subgroups, instead of\ntypically focusing on performance on imbalanced data.\nMethods to achieve fairness learning on graphs can be\nmainly categorized into two classes: adding constraints to\nmodel training and pre-processing input data. The first line\nof works adds a fairness constraint to the objective of the\ntask [298], [301]\u2013[313], promoting independence between\noutput and sensitive attributes. The second stream of works\nresorts to modify input features to create bias-free input, ei-\nther by removing the influence of sensitive attributes [314]\u2013\n[326], or by rendering the model invariant to these attributes\n[327]\u2013[330]. Apart for these two main categories, some other\ntechniques are also deployed in this setting, such as post-\nprocessing output embeddings [331], partitioning training\nset [332], [333], and considering post-hoc explanation of\nbias [334]. We refer interesting readers to a comprehensive\nsurvey [48] for more details.\n7.2\nImbalanced learning in Recommendations\nImbalanced learning in recommendations primarily ad-\ndresses cold-start recommendations within user-item rating\ngraphs, a bipartite graph [299], [335]. Distinct from ILoGs,\ncold-start recommendation usually grapples with insuffi-\ncient user-item interactions. It manifests when making rec-\nommendations for new users (user cold-start) or new items\n(item cold-start) that lack adequate interaction data to guide\ntraditional collaborative filtering methods. To mitigate the\ncold-start recommendation problem, numerous techniques\nhave been employed, including meta-learning [336], [337],\npre-training [338], and knowledge distillation [339], etc. For\nan in-depth understanding of this challenge, we encourage\nreaders to consult the related survey [50].\n8\nFUTURE DIRECTIONS\nIn this section, we explore prospective avenues for ad-\nvancing ILoGs, building on our established taxonomies of\nproblems (Section 8.1) and techniques (Section 8.2). Our\ngoal is to stimulate research that tackles the challenges\ninherent to imbalanced graph data and ultimately improves\nthe performance in various scenarios.\n8.1\nFuture Directions of Problems\nThe characteristics of graph data lead to various imbalance\nchallenges in graph learning. Despite substantial research\nin this area, some facets remain underexplored and deserve\nfurther attention. Two pivotal dimensions\u2014class and struc-\nture imbalance\u2014provide a framework for future research\nwhen examined within our problem taxonomy. Further-\nmore, the varied nature of graph imbalance necessitates di-\nverse imbalance ratios to quantify the degree of imbalance,\noffering a potential cornerstone for ensuing investigations.\nClass imbalance. As indicated in Section 4, much of the\ncurrent research in ILoGs centers on node-level imbalance,\nleaving edge- and graph-level tasks less explored.\nFirstly,\nedge-level\nimbalance\ntasks,\nparticularly\nfor\ngeneric graphs, need more focus. Edge behaviors can vary\nacross or even within a single graph, creating unique chal-\nlenges [31]. Investigating imbalanced edge tasks like few-\nshot link prediction could significantly enhance the practi-\ncality of graph learning in real-world applications.\nSecondly, graph-level imbalance tasks have received\nlimited investigation and warrant further exploration. Be-\nyond anomaly detection, there is a lack of research on\nmore generic imbalanced graph classification [32], which\nis crucial in real-world applications. Moreover, considering\nthe laborious nature of labeling in many molecular-related\ntasks, graph classification without supervision (e.g., zero-\nshot graph classification) becomes increasingly important,\nwhich might be potentially assisted by text information.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n20\nStructure imbalance. In the realm of structure imbalance,\nthe majority of recent work has primarily focused on node-\nlevel imbalance. However, the intricate connectivity pat-\nterns present in graphs can offer various perspectives on\nstructure imbalance across nodes.\nFor instance, beyond the well-researched degree imbal-\nance among nodes, another type of imbalance relating to\ngeneralized node degree [340], which captures the abun-\ndance of contextual structures of a node, could also trigger\nperformance disparities among nodes. Switching to graph-\nlevel tasks, imbalance may also stem from graph size vari-\nations. Differences in graph sizes may introduce bias and\nimpact outcomes [34], a factor that remains under-exploited\nand calls for further investigation.\n8.2\nFuture Directions of Techniques\nTwo major directions can be considered in order to investi-\ngate future directions of techniques for ILoGs.\nCross-branch technique exploration. As discussed in Sec-\ntion 6.3, the techniques of one branch can address the\nimbalance issues of another, emphasizing the potential for\ncross-branch technique exploration. Thus, researchers are\nurged to consider this method to address imbalance issues.\nNovel technique exploration. As real-world applications\nspur artificial intelligence innovations, exploring new tech-\nniques becomes essential. For example, in the context of\nsynthetic data generation for balancing high/low-resource\nparts, diffusion models [71] offer an alternative to GANs,\ngiven their superior generation capabilities in some contexts\n[71], [341], [342]. Additionally, foundation models [187] like\nGPT models [343], known for their potential in various\ndomains, could potentially offer additional benefits in ILoGs\nwhere the text information is available.\n8.3\nOther Directions\nILoGs is an area where current studies often feel fragmented\nand disconnected. One of the major challenges is the absence\nof unified benchmark datasets and leaderboards for each\nresearch task. Such tools would not only facilitate a more\ndirect evaluation of individual studies but also encourage\nhealthy competition among researchers. Furthermore, there\nis a valuable opportunity to consolidate and review existing\nwork on each specific task within this domain. This would\nproduce comprehensive survey reports, offering a holistic\nview and serving as invaluable resources for both newcom-\ners and seasoned researchers in the field.\n9\nCONCLUSIONS\nIn this survey, we offer a comprehensive review of the\nliterature on imbalanced learning on graphs. To provide a\nsolid foundation, we begin by presenting a formal definition\nof the concept, followed by an introduction to basic terms\nin the field. Significantly, we develop two comprehensive\ntaxonomies of ILoGs, categorizing existing studies based\non their problems and techniques. Our problem taxonomy\nidentifies class and structure imbalance, further detailing\nresearch tasks within each branch concerning graph-related\nelements, namely nodes, edges, and graphs. Meanwhile, the\ntechnique taxonomy classifies the literature based on the\ntype of imbalance and corresponding strategies deployed\nto rectify these imbalance on graphs. Finally, we outline\npromising future directions for both problems and tech-\nniques in ILoGs, in order to spark further innovation and\nexploration in this increasingly critical domain.\nREFERENCES\n[1]\nL. Page, S. Brin, R. Motwani, and T. Winograd, \u201cThe pagerank\ncitation ranking: Bringing order to the web.\u201d Stanford infolab,\nTech. Rep., 1999.\n[2]\nG. Jeh and J. Widom, \u201cSimrank: a measure of structural-context\nsimilarity,\u201d in KDD, 2002, pp. 538\u2013543.\n[3]\nL. Backstrom and J. Leskovec, \u201cSupervised random walks: pre-\ndicting and recommending links in social networks,\u201d in WSDM,\n2011, pp. 635\u2013644.\n[4]\nH. Cai, V. W. Zheng, and K. C.-C. Chang, \u201cA comprehensive\nsurvey of graph embedding: Problems, techniques, and applica-\ntions,\u201d TKDE, vol. 30, no. 9, pp. 1616\u20131637, 2018.\n[5]\nZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip,\n\u201cA comprehensive survey on graph neural networks,\u201d TNNLS,\nvol. 32, no. 1, pp. 4\u201324, 2020.\n[6]\nB. Perozzi, R. Al-Rfou, and S. Skiena, \u201cDeepwalk: Online learning\nof social representations,\u201d in KDD, 2014, pp. 701\u2013710.\n[7]\nJ. Tang, M. Qu, M. Wang, M. Zhang, J. Yan, and Q. Mei, \u201cLine:\nLarge-scale information network embedding,\u201d in WWW, 2015,\npp. 1067\u20131077.\n[8]\nA. Grover and J. Leskovec, \u201cnode2vec: Scalable feature learning\nfor networks,\u201d in KDD, 2016, pp. 855\u2013864.\n[9]\nT. N. Kipf and M. Welling, \u201cSemi-supervised classification with\ngraph convolutional networks,\u201d in ICLR, 2017.\n[10]\nW. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d in NeurIPS, 2017, pp. 1024\u20131034.\n[11]\nP. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY. Bengio, \u201cGraph attention networks,\u201d in ICLR, 2018.\n[12]\nK. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are\ngraph neural networks?\u201d in ICLR, 2019.\n[13]\nM. Buda, A. Maki, and M. A. Mazurowski, \u201cA systematic study of\nthe class imbalance problem in convolutional neural networks,\u201d\nNeural networks, vol. 106, pp. 249\u2013259, 2018.\n[14]\nY. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, \u201cClass-balanced\nloss based on effective number of samples,\u201d in CVPR, 2019, pp.\n9268\u20139277.\n[15]\nB. Kang, S. Xie, M. Rohrbach, Z. Yan, A. Gordo, J. Feng, and\nY. Kalantidis, \u201cDecoupling representation and classifier for long-\ntailed recognition,\u201d in ICLR, 2020.\n[16]\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer,\n\u201cSmote: synthetic minority over-sampling technique,\u201d JAIR,\nvol. 16, pp. 321\u2013357, 2002.\n[17]\nY. Zhang, B. Kang, B. Hooi, S. Yan, and J. Feng, \u201cDeep long-tailed\nlearning: A survey,\u201d TPAMI, 2023.\n[18]\nZ. Liu, W. Zhang, Y. Fang, X. Zhang, and S. C. Hoi, \u201cTowards\nlocality-aware meta-learning of tail node embeddings on net-\nworks,\u201d in CIKM, 2020, pp. 975\u2013984.\n[19]\nH. He and E. A. Garcia, \u201cLearning from imbalanced data,\u201d TKDE,\nvol. 21, no. 9, pp. 1263\u20131284, 2009.\n[20]\nB. Krawczyk, \u201cLearning from imbalanced data: open challenges\nand future directions,\u201d Progress in Artificial Intelligence, vol. 5,\nno. 4, pp. 221\u2013232, 2016.\n[21]\nG. Haixiang, L. Yijing, J. Shang, G. Mingyun, H. Yuanyue, and\nG. Bing, \u201cLearning from class-imbalanced data: Review of meth-\nods and applications,\u201d Expert systems with applications, vol. 73, pp.\n220\u2013239, 2017.\n[22]\nJ. M. Johnson and T. M. Khoshgoftaar, \u201cSurvey on deep learning\nwith class imbalance,\u201d Journal of Big Data, vol. 6, no. 1, pp. 1\u201354,\n2019.\n[23]\nC. Huang, Y. Li, C. C. Loy, and X. Tang, \u201cLearning deep repre-\nsentation for imbalanced classification,\u201d in CVPR, 2016, pp. 5375\u2013\n5384.\n[24]\nX. Li, X. Sun, Y. Meng, J. Liang, F. Wu, and J. Li, \u201cDice loss for\ndata-imbalanced nlp tasks,\u201d in ACL, 2020, pp. 465\u2013476.\n[25]\nT. Zhao, X. Zhang, and S. Wang, \u201cGraphsmote: Imbalanced node\nclassification on graphs with graph neural networks,\u201d in WSDM,\n2021, pp. 833\u2013841.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n21\n[26]\nH. Yao, C. Zhang, Y. Wei, M. Jiang, S. Wang, J. Huang, N. Chawla,\nand Z. Li, \u201cGraph few-shot learning via knowledge transfer,\u201d in\nAAAI, vol. 34, no. 04, 2020, pp. 6656\u20136663.\n[27]\nZ. Liu, T.-K. Nguyen, and Y. Fang, \u201cTail-gnn: Tail-node graph\nneural networks,\u201d in KDD, 2021, pp. 1109\u20131119.\n[28]\nX. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong,\nand L. Akoglu, \u201cA comprehensive survey on graph anomaly\ndetection with deep learning,\u201d TKDE, 2021.\n[29]\nD. Mandal, S. Medya, B. Uzzi, and C. Aggarwal, \u201cMetalearning\nwith graph neural networks: Methods and applications,\u201d ACM\nSIGKDD Explorations Newsletter, vol. 23, no. 2, pp. 13\u201322, 2022.\n[30]\nC. Zhang, K. Ding, J. Li, X. Zhang, Y. Ye, N. V. Chawla, and H. Liu,\n\u201cFew-shot learning on graphs,\u201d in IJCAI, 2022, pp. 5662\u20135669.\n[31]\nX. Zhu, P. Luo, Z. Zhao, T. Xu, A. Lizhiyu, Y. Yu, X. Li, and\nE. Chen, \u201cFew-shot link prediction for event-based social net-\nworks via meta-learning,\u201d in DASFAA, 2023, pp. 31\u201341.\n[32]\nY. Wang, Y. Zhao, N. Shah, and T. Derr, \u201cImbalanced graph\nclassification via graph-of-graph neural networks,\u201d in CIKM,\n2022, pp. 2067\u20132076.\n[33]\nW. Xiong, M. Yu, S. Chang, X. Guo, and W. Y. Wang, \u201cOne-shot\nrelational learning for knowledge graphs,\u201d in EMNLP, 2018, pp.\n1980\u20131990.\n[34]\nZ. Liu, Q. Mao, C. Liu, Y. Fang, and J. Sun, \u201cOn size-oriented\nlong-tailed graph classification of graph neural networks,\u201d in\nWWW, 2022, pp. 1506\u20131516.\n[35]\nM. Shi, Y. Tang, X. Zhu, D. Wilson, and J. Liu, \u201cMulti-class\nimbalanced graph convolutional network learning,\u201d in IJCAI,\n2020, pp. 2879\u20132885.\n[36]\nY. Liu, X. Ao, Z. Qin, J. Chi, J. Feng, H. Yang, and Q. He, \u201cPick\nand choose: a gnn-based imbalanced learning approach for fraud\ndetection,\u201d in WWW, 2021, pp. 3168\u20133177.\n[37]\nL. Qu, H. Zhu, R. Zheng, Y. Shi, and H. Yin, \u201cImgagn: Imbalanced\nnetwork embedding via generative adversarial graph networks,\u201d\nin KDD, 2021, pp. 1390\u20131398.\n[38]\nA. Fern\u00b4andez, S. Garcia, F. Herrera, and N. V. Chawla, \u201cSmote for\nlearning from imbalanced data: progress and challenges, marking\nthe 15-year anniversary,\u201d JAIR, vol. 61, pp. 863\u2013905, 2018.\n[39]\nV. Chandola, A. Banerjee, and V. Kumar, \u201cAnomaly detection: A\nsurvey,\u201d CSUR, vol. 41, no. 3, pp. 1\u201358, 2009.\n[40]\nR. Chalapathy and S. Chawla, \u201cDeep learning for anomaly detec-\ntion: A survey,\u201d arXiv preprint arXiv:1901.03407, 2019.\n[41]\nG. Pang, C. Shen, L. Cao, and A. V. D. Hengel, \u201cDeep learning\nfor anomaly detection: A review,\u201d CSUR, vol. 54, no. 2, pp. 1\u201338,\n2021.\n[42]\nY. Wang, Q. Yao, J. T. Kwok, and L. M. Ni, \u201cGeneralizing from\na few examples: A survey on few-shot learning,\u201d CSUR, vol. 53,\nno. 3, pp. 1\u201334, 2020.\n[43]\nY. Song, T. Wang, S. K. Mondal, and J. P. Sahoo, \u201cA compre-\nhensive survey of few-shot learning: Evolution, applications,\nchallenges, and opportunities,\u201d CSUR, 2023.\n[44]\nY. Ma, Y. Tian, N. Moniz, and N. V. Chawla, \u201cClass-imbalanced\nlearning on graphs: A survey,\u201d arXiv preprint arXiv:2304.04300,\n2023.\n[45]\nM. H. Bhuyan, D. K. Bhattacharyya, and J. K. Kalita, \u201cNet-\nwork anomaly detection: methods, systems and tools,\u201d COMST,\nvol. 16, no. 1, pp. 303\u2013336, 2013.\n[46]\nL. Akoglu, H. Tong, and D. Koutra, \u201cGraph based anomaly\ndetection and description: a survey,\u201d DMKD, vol. 29, no. 3, pp.\n626\u2013688, 2015.\n[47]\nM. Ahmed, A. N. Mahmood, and J. Hu, \u201cA survey of network\nanomaly detection techniques,\u201d JNCA, vol. 60, pp. 19\u201331, 2016.\n[48]\nY. Dong, J. Ma, S. Wang, C. Chen, and J. Li, \u201cFairness in graph\nmining: A survey,\u201d TKDE, no. 01, pp. 1\u201322, 2023.\n[49]\nC. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han, \u201cHeterogeneous\nnetwork representation learning: A unified framework with sur-\nvey and benchmark,\u201d TKDE, vol. 34, no. 10, pp. 4854\u20134873, 2020.\n[50]\nS. Zhang, L. Yao, A. Sun, and Y. Tay, \u201cDeep learning based\nrecommender system: A survey and new perspectives,\u201d CSUR,\nvol. 52, no. 1, pp. 1\u201338, 2019.\n[51]\nS. Ji, S. Pan, E. Cambria, P. Marttinen, and S. Y. Philip, \u201cA\nsurvey on knowledge graphs: Representation, acquisition, and\napplications,\u201d TNNLS, vol. 33, no. 2, pp. 494\u2013514, 2021.\n[52]\nZ. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec,\n\u201cHierarchical graph representation learning with differentiable\npooling,\u201d NeurIPS, vol. 31, pp. 4805\u20134815, 2018.\n[53]\nZ. Guo, B. Nan, Y. Tian, O. Wiest, C. Zhang, and N. V. Chawla,\n\u201cGraph-based molecular representation learning,\u201d arXiv preprint\narXiv:2207.04869, 2022.\n[54]\nX. Yu, Z. Liu, Y. Fang, and X. Zhang, \u201cLearning to count isomor-\nphisms with graph neural networks,\u201d in AAAI, 2023.\n[55]\nD. K. Duvenaud, D. Maclaurin, J. Iparraguirre, R. Bombarell,\nT. Hirzel, A. Aspuru-Guzik, and R. P. Adams, \u201cConvolu-\ntional networks on graphs for learning molecular fingerprints,\u201d\nNeurIPS, vol. 28, pp. 2224\u20132232, 2015.\n[56]\nH. Gao and S. Ji, \u201cGraph u-nets,\u201d in ICML, 2019, pp. 2083\u20132092.\n[57]\nW. Zheng, E. W. Huang, N. Rao, S. Katariya, Z. Wang, and\nK. Subbian, \u201cCold brew: Distilling graph node representations\nwith incomplete or missing neighborhoods,\u201d in ICLR, 2022.\n[58]\nF. Zhou, C. Cao, K. Zhang, G. Trajcevski, T. Zhong, and J. Geng,\n\u201cMeta-gnn: On few-shot node classification in graph meta-\nlearning,\u201d in CIKM, 2019, pp. 2357\u20132360.\n[59]\nK. Ding, J. Wang, J. Li, K. Shu, C. Liu, and H. Liu, \u201cGraph proto-\ntypical networks for few-shot learning on attributed networks,\u201d\nin CIKM, 2020, pp. 295\u2013304.\n[60]\nI. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. C. Courville, and Y. Bengio, \u201cGenerative\nadversarial nets,\u201d in NIPS, vol. 27, 2014.\n[61]\nY. Wang, C. Aggarwal, and T. Derr, \u201cDistance-wise prototypical\ngraph neural network in node imbalance classification,\u201d arXiv\npreprint arXiv:2110.12035, 2021.\n[62]\nJ. Song, J. Park, and E. Yang, \u201cTam: Topology-aware margin\nloss for class-imbalanced node classification,\u201d in ICML, 2022, pp.\n20 369\u201320 383.\n[63]\nS. Yun, K. Kim, K. Yoon, and C. Park, \u201cLte4g: Long-tail experts\nfor graph neural networks,\u201d in CIKM, 2022, pp. 2434\u20132443.\n[64]\nC. Drummond, R. C. Holte et al., \u201cC4. 5, class imbalance, and\ncost sensitivity: why under-sampling beats over-sampling,\u201d in\nWorkshop on learning from imbalanced datasets II, vol. 11, 2003, pp.\n1\u20138.\n[65]\nL. Wu, H. Lin, Z. Gao, C. Tan, S. Li et al., \u201cGraphmixup: Improv-\ning class-imbalanced node classification by reinforcement mixup\nand self-supervised context prediction,\u201d ECML-PKDD, pp. 519\u2013\n535, 2022.\n[66]\nJ. Park, J. Song, and E. Yang, \u201cGraphens: Neighbor-aware ego\nnetwork synthesis for class-imbalanced node classification,\u201d in\nICLR, 2022.\n[67]\nJ. Liu, M. He, G. Wang, N. Q. V. Hung, X. Shang, and H. Yin, \u201cIm-\nbalanced node classification beyond homophilic assumption,\u201d in\nIJCAI, 2023.\n[68]\nH. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \u201cmixup:\nBeyond empirical risk minimization,\u201d in ICLR, 2018.\n[69]\nL. Cui, X. Tang, S. Katariya, N. Rao, P. Agrawal, K. Subbian, and\nD. Lee, \u201cAllie: Active learning on large-scale imbalanced graphs,\u201d\nin WWW, 2022, pp. 690\u2013698.\n[70]\nL. P. Kaelbling, M. L. Littman, and A. W. Moore, \u201cReinforcement\nlearning: A survey,\u201d JAIR, vol. 4, pp. 237\u2013285, 1996.\n[71]\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d\nin CVPR, 2022, pp. 10 684\u201310 695.\n[72]\nC. Qiu, M. Kloft, S. Mandt, and M. Rudolph, \u201cRaising the bar in\ngraph-level anomaly detection,\u201d IJCAI, pp. 2196\u20132203, 2022.\n[73]\nR. Ma, G. Pang, L. Chen, and A. van den Hengel, \u201cDeep graph-\nlevel anomaly detection by glocal knowledge distillation,\u201d in\nWSDM, 2022, pp. 704\u2013714.\n[74]\nT. Zhao, B. Ni, W. Yu, Z. Guo, N. Shah, and M. Jiang, \u201cAction se-\nquence augmentation for early graph-based anomaly detection,\u201d\nin CIKM, 2021, pp. 2668\u20132678.\n[75]\nG. Zhang, J. Wu, J. Yang, A. Beheshti, S. Xue, C. Zhou, and\nQ. Z. Sheng, \u201cFraudre: Fraud detection dual-resistant to graph\ninconsistency and imbalance,\u201d in ICDM, 2021, pp. 867\u2013876.\n[76]\nY. Qian, Y. Zhang, Y. Ye, C. Zhang et al., \u201cDistilling meta knowl-\nedge on heterogeneous graph for illicit drug trafficker detection\non social media,\u201d NeurIPS, vol. 34, pp. 26 911\u201326 923, 2021.\n[77]\nJ. Mathew, M. Negi, R. Vijjali, and J. Sathyanarayana, \u201cDefraud-\nnet: An end-to-end weak supervision framework to detect fraud\nin online food delivery,\u201d in ECML-PKDD, 2021, pp. 85\u201399.\n[78]\nH. Wang and C. Qiao, \u201cA nodes\u2019 evolution diversity inspired\nmethod to detect anomalies in dynamic social networks,\u201d TKDE,\nvol. 32, no. 10, pp. 1868\u20131880, 2019.\n[79]\nV. Miz, B. Ricaud, K. Benzi, and P. Vandergheynst, \u201cAnomaly\ndetection in the dynamics of web and social networks using\nassociative memory,\u201d in WWW, 2019, pp. 1290\u20131299.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n22\n[80]\nH. Lin, G. Liu, J. Wu, Y. Zuo, X. Wan, and H. Li, \u201cFraud detection\nin dynamic interaction network,\u201d TKDE, vol. 32, no. 10, pp. 1936\u2013\n1950, 2019.\n[81]\nR. Hu, C. C. Aggarwal, S. Ma, and J. Huai, \u201cAn embedding\napproach to anomaly detection,\u201d in ICDE, 2016, pp. 385\u2013396.\n[82]\nJ. Li, H. Dani, X. Hu, and H. Liu, \u201cRadar: Residual analysis for\nanomaly detection in attributed networks.\u201d in IJCAI, vol. 17, 2017,\npp. 2152\u20132158.\n[83]\nB. Wang, N. Z. Gong, and H. Fu, \u201cGang: Detecting fraudulent\nusers in online social networks via guilt-by-association on di-\nrected graphs,\u201d in ICDM, 2017, pp. 465\u2013474.\n[84]\nA. Bojchevski and S. G\u00a8unnemann, \u201cBayesian robust attributed\ngraph clustering: Joint learning of partial anomalies and group\nstructure,\u201d in AAAI, vol. 32, no. 1, 2018.\n[85]\nZ. Peng, M. Luo, J. Li, H. Liu, and Q. Zheng, \u201cAnomalous: A\njoint modeling approach for anomaly detection on attributed\nnetworks.\u201d in IJCAI, 2018, pp. 3513\u20133519.\n[86]\nY. Yang, Y. Xu, Y. Sun, Y. Dong, F. Wu, and Y. Zhuang, \u201cMining\nfraudsters and fraudulent strategies in large-scale mobile social\nnetworks,\u201d TKDE, vol. 33, no. 1, pp. 169\u2013179, 2019.\n[87]\nK. Ding, J. Li, and H. Liu, \u201cInteractive anomaly detection on\nattributed networks,\u201d in WSDM, 2019, pp. 357\u2013365.\n[88]\nL. Guti\u00b4errez-G\u00b4omez, A. Bovet, and J.-C. Delvenne, \u201cMulti-scale\nanomaly detection on attributed networks,\u201d in AAAI, vol. 34,\nno. 01, 2020, pp. 678\u2013685.\n[89]\nM. Zhu and H. Zhu, \u201cMixedad: A scalable algorithm for detecting\nmixed anomalies in attributed graphs,\u201d in AAAI, vol. 34, no. 01,\n2020, pp. 1274\u20131281.\n[90]\nC. C. Aggarwal, Y. Li, and P. S. Yu, \u201cSignature-based anomaly\ndetection in networks,\u201d in SDM, 2021, pp. 109\u2013117.\n[91]\nH. T. Nguyen, P. J. Liang, and L. Akoglu, \u201cDetecting anomalous\ngraphs in labeled multi-graph databases,\u201d TKDD, vol. 17, no. 2,\npp. 1\u201325, 2023.\n[92]\nS. Yuan, X. Wu, J. Li, and A. Lu, \u201cSpectrum-based deep neural\nnetworks for fraud detection,\u201d in CIKM, 2017, pp. 2419\u20132422.\n[93]\nJ. Liang, P. Jacobs, J. Sun, and S. Parthasarathy, \u201cSemi-supervised\nembedding in attributed networks with outliers,\u201d in SDM, 2018,\npp. 153\u2013161.\n[94]\nD. Zhou, J. He, H. Yang, and W. Fan, \u201cSparc: Self-paced network\nrepresentation for few-shot rare category characterization,\u201d in\nKDD, 2018, pp. 2807\u20132816.\n[95]\nS. Bandyopadhyay, N. Lokesh, and M. N. Murty, \u201cOutlier aware\nnetwork embedding for attributed networks,\u201d in AAAI, vol. 33,\nno. 01, 2019, pp. 12\u201319.\n[96]\nS. Bandyopadhyay, S. V. Vivek, and M. N. Murty, \u201cIntegrating\nnetwork embedding and community outlier detection via multi-\nclass graph description,\u201d ECAI, pp. 976\u2013983, 2020.\n[97]\nI. Ahmed, T. Galoppo, X. Hu, and Y. Ding, \u201cGraph regularized\nautoencoder and its application in unsupervised anomaly detec-\ntion,\u201d TPAMI, vol. 44, no. 8, pp. 4110\u20134124, 2021.\n[98]\nY. Li, X. Huang, J. Li, M. Du, and N. Zou, \u201cSpecae: Spectral\nautoencoder for anomaly detection in attributed networks,\u201d in\nCIKM, 2019, pp. 2233\u20132236.\n[99]\nK. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detec-\ntion on attributed networks,\u201d in SDM, 2019, pp. 594\u2013602.\n[100] T. Zhao, C. Deng, K. Yu, T. Jiang, D. Wang, and M. Jiang, \u201cError-\nbounded graph anomaly loss for gnns,\u201d in CIKM, 2020, pp. 1873\u2013\n1882.\n[101] Z. Peng, M. Luo, J. Li, L. Xue, and Q. Zheng, \u201cA deep multi-\nview framework for anomaly detection on attributed networks,\u201d\nTKDE, vol. 34, no. 6, pp. 2539\u20132552, 2020.\n[102] Z. Chen, B. Liu, M. Wang, P. Dai, J. Lv, and L. Bo, \u201cGenerative\nadversarial attributed network anomaly detection,\u201d in CIKM,\n2020, pp. 1989\u20131992.\n[103] D. Cheng, X. Wang, Y. Zhang, and L. Zhang, \u201cGraph neural net-\nwork for fraud detection via spatial-temporal attention,\u201d TKDE,\nvol. 34, no. 8, pp. 3800\u20133813, 2020.\n[104] K. Ding, K. Shu, X. Shan, J. Li, and H. Liu, \u201cCross-domain graph\nanomaly detection,\u201d TNNLS, vol. 33, no. 6, pp. 2406\u20132415, 2021.\n[105] A. Deng and B. Hooi, \u201cGraph neural network-based anomaly\ndetection in multivariate time series,\u201d in AAAI, vol. 35, no. 5,\n2021, pp. 4027\u20134035.\n[106] U. Desai, S. Bandyopadhyay, and S. Tamilselvam, \u201cGraph neural\nnetwork to dilute outliers for refactoring monolith application,\u201d\nin AAAI, vol. 35, no. 1, 2021, pp. 72\u201380.\n[107] M. Jin, Y. Liu, Y. Zheng, L. Chi, Y.-F. Li, and S. Pan, \u201cAnemone:\nGraph anomaly detection with multi-scale contrastive learning,\u201d\nin CIKM, 2021, pp. 3122\u20133126.\n[108] K. Ding, Q. Zhou, H. Tong, and H. Liu, \u201cFew-shot network\nanomaly detection via cross-network meta-learning,\u201d in WWW,\n2021, pp. 2448\u20132456.\n[109] S. Zhou, Q. Tan, Z. Xu, X. Huang, and F.-l. Chung, \u201cSubtractive\naggregation for attributed network anomaly detection,\u201d in CIKM,\n2021, pp. 3672\u20133676.\n[110] K. Ding, J. Li, N. Agarwal, and H. Liu, \u201cInductive anomaly\ndetection on attributed networks,\u201d in IJCAI, 2021, pp. 1288\u20131294.\n[111] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, \u201cAnomaly\ndetection on attributed networks via contrastive self-supervised\nlearning,\u201d TNNLS, vol. 33, no. 6, pp. 2378\u20132392, 2021.\n[112] T. Zhao, T. Jiang, N. Shah, and M. Jiang, \u201cA synergistic approach\nfor graph anomaly detection with pattern mining and feature\nlearning,\u201d TNNLS, vol. 33, no. 6, pp. 2393\u20132405, 2021.\n[113] W. Chen, L. Tian, B. Chen, L. Dai, Z. Duan, and M. Zhou,\n\u201cDeep variational graph convolutional recurrent network for\nmultivariate time series anomaly detection,\u201d in ICML, 2022, pp.\n3621\u20133633.\n[114] P. Qi, D. Li, and S.-K. Ng, \u201cMad-sgcn: Multivariate anomaly\ndetection with self-learning graph convolutional networks,\u201d in\nICDE, 2022, pp. 1232\u20131244.\n[115] X. Chen, Q. Qiu, C. Li, and K. Xie, \u201cGraphad: A graph neural\nnetwork for entity-wise multivariate time-series anomaly detec-\ntion,\u201d in SIGIR, 2022, pp. 2297\u20132302.\n[116] J. Zhang, S. Wang, and S. Chen, \u201cReconstruction enhanced multi-\nview contrastive learning for anomaly detection on attributed\nnetworks,\u201d IJCAI, pp. 2376\u20132382, 2022.\n[117] X. Luo, J. Wu, A. Beheshti, J. Yang, X. Zhang, Y. Wang, and\nS. Xue, \u201cComga: Community-aware attributed graph anomaly\ndetection,\u201d in WSDM, 2022, pp. 657\u2013665.\n[118] A. Goodge, B. Hooi, S.-K. Ng, and W. S. Ng, \u201cLunar: Unifying\nlocal outlier detection methods via graph neural networks,\u201d in\nAAAI, vol. 36, no. 6, 2022, pp. 6737\u20136745.\n[119] S. Zhou, X. Huang, N. Liu, Q. Tan, and F.-L. Chung, \u201cUnseen\nanomaly detection on networks via multi-hypersphere learning,\u201d\nin SDM, 2022, pp. 262\u2013270.\n[120] F. Liu, X. Ma, J. Wu, J. Yang, S. Xue, A. Beheshti, C. Zhou, H. Peng,\nQ. Z. Sheng, and C. C. Aggarwal, \u201cDagad: Data augmentation for\ngraph anomaly detection,\u201d ICDM, pp. 259\u2013268, 2022.\n[121] T. Huang, Y. Pei, V. Menkovski, and M. Pechenizkiy, \u201cHop-\ncount based self-supervised anomaly detection on attributed\nnetworks,\u201d in ECML-PKDD, 2022, pp. 225\u2013241.\n[122] Z. Zhuang, K. M. Ting, G. Pang, and S. Song, \u201cSubgraph central-\nization: A necessary step for graph anomaly detection,\u201d SDM,\npp. 703\u2013711, 2023.\n[123] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAd-\ndressing heterophily in graph anomaly detection: A perspective\nof graph spectrum,\u201d in WWW, 2023, pp. 1528\u20131538.\n[124] Y. Huang, L. Wang, F. Zhang, and X. Lin, \u201cUnsupervised graph\noutlier detection: Problem revisit, new insight, and superior\nmethod,\u201d ICDE, pp. 2556\u20132569, 2023.\n[125] J. J.-C. Ying, J. Zhang, C.-W. Huang, K.-T. Chen, and V. S.\nTseng, \u201cPfraudetector: a parallelized graph mining approach for\nefficient fraudulent phone call detection,\u201d in ICPADS, 2016, pp.\n1059\u20131066.\n[126] N. Liu, X. Huang, and X. Hu, \u201cAccelerated local anomaly detec-\ntion via resolving attributed networks.\u201d in IJCAI, 2017, pp. 2337\u2013\n2343.\n[127] B. Hooi, K. Shin, H. A. Song, A. Beutel, N. Shah, and C. Faloutsos,\n\u201cGraph-based fraud detection in the face of camouflage,\u201d TKDD,\nvol. 11, no. 4, pp. 1\u201326, 2017.\n[128] S. Zhang, D. Zhou, M. Y. Yildirim, S. Alcorn, J. He, H. Davulcu,\nand H. Tong, \u201cHidden: hierarchical dense subgraph detection\nwith application to financial fraud detection,\u201d in SDM, 2017, pp.\n570\u2013578.\n[129] B. Cao, M. Mao, S. Viidu, and S. Y. Philip, \u201cHitfraud: a broad\nlearning approach for collective fraud detection in heterogeneous\ninformation networks,\u201d in ICDM, 2017, pp. 769\u2013774.\n[130] V. Ranjbar, M. Salehi, P. Jandaghi, and M. Jalili, \u201cQanet: Tensor\ndecomposition approach for query-based anomaly detection in\nheterogeneous information networks,\u201d TKDE, vol. 31, no. 11, pp.\n2178\u20132189, 2018.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n23\n[131] Y. Ren, H. Zhu, J. Zhang, P. Dai, and L. Bo, \u201cEnsemfdet: An\nensemble approach to fraud detection based on bipartite graph,\u201d\nin ICDE, 2021, pp. 2039\u20132044.\n[132] Y. Fan, S. Hou, Y. Zhang, Y. Ye, and M. Abdulhayoglu, \u201cGotcha-\nsly malware! scorpion a metagraph2vec based malware detection\nsystem,\u201d in KDD, 2018, pp. 253\u2013262.\n[133] M. Yoon, \u201cGraph fraud detection based on accessibility score\ndistributions,\u201d in ECML-PKDD, 2021, pp. 483\u2013498.\n[134] Y. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, \u201cEn-\nhancing graph neural network-based fraud detectors against\ncamouflaged fraudsters,\u201d in CIKM, 2020, pp. 315\u2013324.\n[135] Y.-N. Zhu, X. Luo, Y.-F. Li, B. Bu, K. Zhou, W. Zhang, and M. Lu,\n\u201cHeterogeneous mini-graph neural network and its application\nto fraud invitation detection,\u201d in ICDM, 2020, pp. 891\u2013899.\n[136] Z. Liu, Y. Dou, P. S. Yu, Y. Deng, and H. Peng, \u201cAlleviating\nthe inconsistency problem of applying graph neural network to\nfraud detection,\u201d in SIGIR, 2020, pp. 1569\u20131572.\n[137] S. Zhang, H. Yin, T. Chen, Q. V. N. Hung, Z. Huang, and\nL. Cui, \u201cGcn-based user representation learning for unifying\nrobust recommendation and fraudster detection,\u201d in SIGIR, 2020,\npp. 689\u2013698.\n[138] S. Hu, X. Zhang, J. Zhou, S. Ji, J. Yuan, Z. Li, Z. Wang, Q. Chen,\nQ. He, and L. Fang, \u201cTurbo: Fraud detection in deposit-free\nleasing service via real-time behavior network mining,\u201d in ICDE,\n2021, pp. 2583\u20132594.\n[139] B. Xu, H. Shen, B. Sun, R. An, Q. Cao, and X. Cheng, \u201cTowards\nconsumer loan fraud detection: Graph neural networks with role-\nconstrained conditional random field,\u201d in AAAI, vol. 35, no. 5,\n2021, pp. 4537\u20134545.\n[140] L. Wang, P. Li, K. Xiong, J. Zhao, and R. Lin, \u201cModeling hetero-\ngeneous graph network on fraud detection: a community-based\nframework with attention mechanism,\u201d in CIKM, 2021, pp. 1959\u2013\n1968.\n[141] Y. Wang, J. Zhang, S. Guo, H. Yin, C. Li, and H. Chen, \u201cDe-\ncoupling representation learning and classification for gnn-based\nanomaly detection,\u201d in SIGIR, 2021, pp. 1239\u20131248.\n[142] C. Liu, L. Sun, X. Ao, J. Feng, Q. He, and H. Yang, \u201cIntention-\naware heterogeneous graph attention networks for fraud trans-\nactions detection,\u201d in KDD, 2021, pp. 3280\u20133288.\n[143] L. Dong, Y. Liu, X. Ao, J. Chi, J. Feng, H. Yang, and Q. He, \u201cBi-\nlevel selection via meta gradient for graph-based fraud detec-\ntion,\u201d in DASFAA, 2022, pp. 387\u2013394.\n[144] M. Huang, Y. Liu, X. Ao, K. Li, J. Chi, J. Feng, H. Yang, and\nQ. He, \u201cAuc-oriented graph neural network for fraud detection,\u201d\nin WWW, 2022, pp. 1311\u20131321.\n[145] Z. Li, D. Chen, Q. Liu, and S. Wu, \u201cThe devil is in the conflict:\nDisentangled information graph neural networks for fraud de-\ntection,\u201d ICDM, pp. 1059\u20131064, 2022.\n[146] Z. Qin, Y. Liu, Q. He, and X. Ao, \u201cExplainable graph-based\nfraud detection via neural meta-graph search,\u201d in CIKM, 2022,\npp. 4414\u20134418.\n[147] J. Tang, J. Li, Z. Gao, and J. Li, \u201cRethinking graph neural networks\nfor anomaly detection,\u201d in ICML, 2022, pp. 21 076\u201321 089.\n[148] F. Shi, Y. Cao, Y. Shang, Y. Zhou, C. Zhou, and J. Wu, \u201cH2-\nfdetector: a gnn-based fraud detector with homophilic and het-\nerophilic connections,\u201d in WWW, 2022, pp. 1486\u20131494.\n[149] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAlleviat-\ning structural distribution shift in graph anomaly detection,\u201d in\nWSDM, 2023, pp. 357\u2013365.\n[150] L. Rashidi, A. Kan, J. Bailey, J. Chan, C. Leckie, W. Liu, S. Ra-\njasegarar, and K. Ramamohanarao, \u201cNode re-ordering as a means\nof anomaly detection in time-evolving graphs,\u201d in ECML-PKDD,\n2016, pp. 162\u2013178.\n[151] S. Liu, B. Hooi, and C. Faloutsos, \u201cHoloscope: Topology-and-\nspike aware fraud detection,\u201d in CIKM, 2017, pp. 1539\u20131548.\n[152] T. L. Fond, J. Neville, and B. Gallagher, \u201cDesigning size consistent\nstatistics for accurate anomaly detection in dynamic networks,\u201d\nTKDD, vol. 12, no. 4, pp. 1\u201349, 2018.\n[153] S. Liu, B. Hooi, and C. Faloutsos, \u201cA contrast metric for fraud\ndetection in rich graphs,\u201d TKDE, vol. 31, no. 12, pp. 2235\u20132248,\n2018.\n[154] V. Sharma, R. Kumar, W.-H. Cheng, M. Atiquzzaman, K. Srini-\nvasan, and A. Y. Zomaya, \u201cNhad: Neuro-fuzzy based horizontal\nanomaly detection in online social networks,\u201d TKDE, vol. 30,\nno. 11, pp. 2171\u20132184, 2018.\n[155] M. Yoon, B. Hooi, K. Shin, and C. Faloutsos, \u201cFast and accurate\nanomaly detection in dynamic graphs with a two-pronged ap-\nproach,\u201d in KDD, 2019, pp. 647\u2013657.\n[156] S. Bhatia, B. Hooi, M. Yoon, K. Shin, and C. Faloutsos, \u201cMidas:\nMicrocluster-based detector of anomalies in edge streams,\u201d in\nAAAI, vol. 34, no. 04, 2020, pp. 3242\u20133249.\n[157] X. Guo, B. Zhou, and S. Skiena, \u201cSubset node anomaly tracking\nover large dynamic graphs,\u201d in KDD, 2022, pp. 475\u2013485.\n[158] X. Teng, Y.-R. Lin, and X. Wen, \u201cAnomaly detection in dynamic\nnetworks using multi-view time-series hypersphere learning,\u201d in\nCIKM, 2017, pp. 827\u2013836.\n[159] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and\nW. Wang, \u201cNetwalk: A flexible deep embedding approach for\nanomaly detection in dynamic networks,\u201d in KDD, 2018, pp.\n2672\u20132681.\n[160] X. Teng, M. Yan, A. M. Ertugrul, and Y.-R. Lin, \u201cDeep into\nhypersphere: Robust and unsupervised anomaly discovery in\ndynamic networks,\u201d in IJCAI, 2018, pp. 2724\u20132730.\n[161] Z. Chen and A. Sun, \u201cAnomaly detection on dynamic bipartite\ngraph with burstiness,\u201d in ICDM, 2020, pp. 966\u2013971.\n[162] D. Ofori-Boateng, I. S. Dominguez, C. Akcora, M. Kantarcioglu,\nand Y. R. Gel, \u201cTopological anomaly detection in dynamic multi-\nlayer blockchain networks,\u201d in ECML-PKDD, 2021, pp. 788\u2013804.\n[163] A. Z. Wang, R. Ying, P. Li, N. Rao, K. Subbian, and J. Leskovec,\n\u201cBipartite dynamic representations for abuse detection,\u201d in KDD,\n2021, pp. 3638\u20133648.\n[164] L. Deng, D. Lian, Z. Huang, and E. Chen, \u201cGraph convolutional\nadversarial networks for spatiotemporal anomaly detection,\u201d\nTNNLS, vol. 33, no. 6, pp. 2416\u20132428, 2022.\n[165] M. Lu, Z. Han, S. X. Rao, Z. Zhang, Y. Zhao, Y. Shan, R. Raghu-\nnathan, C. Zhang, and J. Jiang, \u201cBright-graph neural networks in\nreal-time fraud detection,\u201d in CIKM, 2022, pp. 3342\u20133351.\n[166] J. Cui, K. Kim, S. H. Na, and S. Shin, \u201cMeta-path-based fake news\ndetection leveraging multi-level social context information,\u201d in\nCIKM, 2022, pp. 325\u2013334.\n[167] S. Tian, J. Dong, J. Li, W. Zhao, X. Xu, B. Song, C. Meng,\nT. Zhang, L. Chen et al., \u201cSad: Semi-supervised anomaly detection\non dynamic graphs,\u201d in IJCAI, 2023.\n[168] D. Duan, L. Tong, Y. Li, J. Lu, L. Shi, and C. Zhang, \u201cAane:\nAnomaly aware network embedding for anomalous link detec-\ntion,\u201d in ICDM, 2020, pp. 1002\u20131007.\n[169] J. Liu, F. Xia, X. Feng, J. Ren, and H. Liu, \u201cDeep graph learning for\nanomalous citation detection,\u201d TNNLS, vol. 33, no. 6, pp. 2543\u2013\n2557, 2022.\n[170] Z. Li, H. Wang, P. Zhang, P. Hui, J. Huang, J. Liao, J. Zhang, and\nJ. Bu, \u201cLive-streaming fraud detection: a heterogeneous graph\nneural network approach,\u201d in KDD, 2021, pp. 3670\u20133678.\n[171] Y.-Y. Chang, P. Li, R. Sosic, M. Afifi, M. Schweighauser, and\nJ. Leskovec, \u201cF-fade: Frequency factorization for anomaly detec-\ntion in edge streams,\u201d in WSDM, 2021, pp. 589\u2013597.\n[172] L. Zheng, Z. Li, J. Li, Z. Li, and J. Gao, \u201cAddgraph: Anomaly\ndetection in dynamic graph using attention-based temporal gcn.\u201d\nin IJCAI, vol. 3, 2019, p. 7.\n[173] L. Cai, Z. Chen, C. Luo, J. Gui, J. Ni, D. Li, and H. Chen, \u201cStruc-\ntural temporal graph neural networks for anomaly detection in\ndynamic graphs,\u201d in CIKM, 2021, pp. 3747\u20133756.\n[174] N. Wu, F. Chen, J. Li, J. Huai, and B. Li, \u201cQuery-driven discovery\nof anomalous subgraphs in attributed graphs,\u201d IJCAI, vol. 6, pp.\n3105\u20133111, 2017.\n[175] B. Perozzi and L. Akoglu, \u201cDiscovering communities and anoma-\nlies in attributed graphs: Interactive visual exploration and sum-\nmarization,\u201d TKDD, vol. 12, no. 2, pp. 1\u201340, 2018.\n[176] N. Wu, W. Wang, F. Chen, J. Li, B. Li, and J. Huai, \u201cUncovering\nspecific-shape graph anomalies in attributed graphs,\u201d in AAAI,\nvol. 33, no. 01, 2019, pp. 5433\u20135440.\n[177] Y. Sun, W. Wang, N. Wu, W. Yu, and X. Chen, \u201cAnomaly sub-\ngraph detection with feature transfer,\u201d in CIKM, 2020, pp. 1415\u2013\n1424.\n[178] C. Wang, D. B. Neill, and F. Chen, \u201cCalibrated nonparametric\nscan statistics for anomalous pattern detection in graphs,\u201d in\nAAAI, vol. 36, no. 4, 2022, pp. 4201\u20134209.\n[179] T. Chen and C. Tsourakakis, \u201cAntibenford subgraphs: Unsuper-\nvised anomaly detection in financial networks,\u201d in KDD, 2022,\npp. 2762\u20132770.\n[180] S. Bhatia, Y. Wang, B. Hooi, and T. Chakraborty, \u201cGraphanogan:\nDetecting anomalous snapshots from attributed graphs,\u201d in\nECML-PKDD, 2021, pp. 36\u201351.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n24\n[181] Z. Zhang and L. Zhao, \u201cUnsupervised deep subgraph anomaly\ndetection,\u201d in ICDM, 2022, pp. 753\u2013762.\n[182] J. Hou, Y. Lei, Z. Peng, W. Lu, F. Zhang, and X. Du, \u201cEfficient\nanomaly detection in property graphs,\u201d in DASFFA, 2023, pp.\n120\u2013136.\n[183] E. Manzoor, S. M. Milajerdi, and L. Akoglu, \u201cFast memory-\nefficient anomaly detection in streaming heterogeneous graphs,\u201d\nin KDD, 2016, pp. 1035\u20131044.\n[184] D. Zambon, C. Alippi, and L. Livi, \u201cConcept drift and anomaly\ndetection in graph streams,\u201d TNNLS, vol. 29, no. 11, pp. 5592\u2013\n5605, 2018.\n[185] H. Zhao, Y. Wang, J. Duan, C. Huang, D. Cao, Y. Tong, B. Xu,\nJ. Bai, J. Tong, and Q. Zhang, \u201cMultivariate time-series anomaly\ndetection via graph attention network,\u201d in ICDM, 2020, pp. 841\u2013\n850.\n[186] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding,\nC. Chen, H. Peng, K. Shu et al., \u201cBond: Benchmarking unsu-\npervised outlier node detection on static attributed graphs,\u201d\nNeurIPS, vol. 35, pp. 27 021\u201327 035, 2022.\n[187] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora,\nS. von Arx, M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill\net al., \u201cOn the opportunities and risks of foundation models,\u201d\narXiv preprint arXiv:2108.07258, 2021.\n[188] N. Wang, M. Luo, K. Ding, L. Zhang, J. Li, and Q. Zheng, \u201cGraph\nfew-shot learning with attribute matching,\u201d in CIKM, 2020, pp.\n1545\u20131554.\n[189] Z. Liu, Y. Fang, C. Liu, and S. C. Hoi, \u201cRelative and absolute\nlocation embedding for few-shot node classification on graph,\u201d\nin AAAI, vol. 35, no. 5, 2021, pp. 4267\u20134275.\n[190] Y. Zhou, Y. Cao, Y. Shang, C. Zhou, C. Song, F. Shi, and Q. Li,\n\u201cTask-level relations modelling for graph meta-learning,\u201d in\nICDM, 2022, pp. 813\u2013822.\n[191] K. Huang and M. Zitnik, \u201cGraph meta learning via local sub-\ngraphs,\u201d NeurIPS, vol. 33, pp. 5862\u20135874, 2020.\n[192] S. Wang, K. Ding, C. Zhang, C. Chen, and J. Li, \u201cTask-adaptive\nfew-shot node classification,\u201d in KDD, 2022, pp. 1910\u20131919.\n[193] Y. Liu, M. Li, X. Li, F. Giunchiglia, X. Feng, and R. Guan, \u201cFew-\nshot node classification on attributed networks with graph meta-\nlearning,\u201d in SIGIR, 2022, pp. 471\u2013481.\n[194] L. Zhang, S. Wang, J. Liu, Q. Lin, X. Chang, Y. Wu, and Q. Zheng,\n\u201cMul-grn: Multi-level graph relation network for few-shot node\nclassification,\u201d TKDE, vol. 35, no. 6, pp. 6085\u20136098, 2022.\n[195] Z. Wu, P. Zhou, G. Wen, Y. Wan, J. Ma, D. Cheng, and X. Zhu,\n\u201cInformation augmentation for few-shot node classification,\u201d in\nIJCAI, 2022, pp. 3601\u20133607.\n[196] Z. Tan, K. Ding, R. Guo, and H. Liu, \u201cSupervised graph con-\ntrastive learning for few-shot node classification,\u201d in ECML-\nPKDD, 2022, pp. 394\u2013411.\n[197] Z. Tan, S. Wang, K. Ding, J. Li, and huan liu, \u201cTransductive linear\nprobing: A novel framework for few-shot node classification,\u201d in\nLoG, 2022.\n[198] Z. Liu, X. Yu, Y. Fang, and X. Zhang, \u201cGraphprompt: Unifying\npre-training and downstream tasks for graph neural networks,\u201d\nin WWW, 2023, pp. 417\u2013428.\n[199] Z. Tan, K. Ding, R. Guo, and H. Liu, \u201cGraph few-shot class-\nincremental learning,\u201d in WSDM, 2022, pp. 987\u2013996.\n[200] B. Lu, X. Gan, L. Yang, W. Zhang, L. Fu, and X. Wang, \u201cGe-\nometer: Graph few-shot class-incremental learning via prototype\nrepresentation,\u201d KDD, pp. 1152\u20131161, 2022.\n[201] Z. Xu, K. Ding, Y.-X. Wang, H. Liu, and H. Tong, \u201cGeneralized\nfew-shot node classification,\u201d in ICDM, 2022, pp. 608\u2013617.\n[202] L. Lan, P. Wang, X. Du, K. Song, J. Tao, and X. Guan, \u201cNode\nclassification on graphs with few-shot novel labels via meta\ntransformed network embedding,\u201d NeurIPS, vol. 33, pp. 16 520\u2013\n16 531, 2020.\n[203] S. Wang, Y. Dong, K. Ding, C. Chen, and J. Li, \u201cFew-shot node\nclassification with extremely weak supervision,\u201d in WSDM, 2023,\npp. 276\u2013284.\n[204] Q. Zhang, X. Wu, Q. Yang, C. Zhang, and X. Zhang, \u201cHg-meta:\nGraph meta-learning over heterogeneous graphs,\u201d in SDM, 2022,\npp. 397\u2013405.\n[205] T. Hospedales, A. Antoniou, P. Micaelli, and A. Storkey, \u201cMeta-\nlearning in neural networks: A survey,\u201d TPAMI, vol. 44, no. 9, pp.\n5149\u20135169, 2021.\n[206] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning\nfor fast adaptation of deep networks,\u201d in ICML, 2017, pp. 1126\u2013\n1135.\n[207] J. Snell, K. Swersky, and R. Zemel, \u201cPrototypical networks for\nfew-shot learning,\u201d in NIPS, 2017, pp. 4080\u20134090.\n[208] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville,\n\u201cFilm: Visual reasoning with a general conditioning layer,\u201d in\nAAAI, vol. 32, no. 1, 2018, pp. 3942\u20133951.\n[209] B. Lu, X. Gan, W. Zhang, H. Yao, L. Fu, and X. Wang, \u201cSpatio-\ntemporal graph few-shot learning with cross-city knowledge\ntransfer,\u201d in KDD, 2022, pp. 1162\u20131172.\n[210] Q. Zhang, X. Wu, Q. Yang, C. Zhang, and X. Zhang, \u201cFew-\nshot heterogeneous graph learning via cross-domain knowledge\ntransfer,\u201d in KDD, 2022, pp. 2450\u20132460.\n[211] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu,\n\u201cHeterogeneous graph attention network,\u201d in WWW, 2019, pp.\n2022\u20132032.\n[212] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou,\nJ. Jiang, Y. Dong, and J. Tang, \u201cAre we really making much\nprogress? revisiting, benchmarking and refining heterogeneous\ngraph neural networks,\u201d in KDD, 2021, pp. 1150\u20131160.\n[213] Z. Wang, X. Ye, C. Wang, Y. Wu, C. Wang, and K. Liang, \u201cRsdne:\nExploring relaxed similarity and dissimilarity from completely-\nimbalanced labels for network embedding,\u201d in AAAI, vol. 32,\nno. 1, 2018, pp. 475\u2013482.\n[214] Z. Wang, X. Ye, C. Wang, J. Cui, and S. Y. Philip, \u201cNetwork\nembedding with completely-imbalanced labels,\u201d TKDE, vol. 33,\nno. 11, pp. 3634\u20133647, 2020.\n[215] Z. Wang, R. Shao, C. Wang, C. Hu, C. Wang, and Z. Gong, \u201cEx-\npanding semantic knowledge for zero-shot graph embedding,\u201d\nin DASFAA, 2021, pp. 394\u2013402.\n[216] Z. Wang, J. Wang, Y. Guo, and Z. Gong, \u201cZero-shot node clas-\nsification with decomposed graph prototype network,\u201d in KDD,\n2021, pp. 1769\u20131779.\n[217] Q. Yue, J. Liang, J. Cui, and L. Bai, \u201cDual bidirectional graph\nconvolutional networks for zero-shot node classification,\u201d in\nKDD, 2022, pp. 2408\u20132417.\n[218] Z. Liu, Y. Fang, C. Liu, and S. C. Hoi, \u201cNode-wise localization of\ngraph neural networks,\u201d in IJCAI, 2021, pp. 1520\u20131526.\n[219] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, \u201cEmpirical evalua-\ntion of gated recurrent neural networks on sequence modeling,\u201d\narXiv preprint arXiv:1412.3555, 2014.\n[220] N. Ma, J. Bu, J. Yang, Z. Zhang, C. Yao, Z. Yu, S. Zhou, and\nX. Yan, \u201cAdaptive-step graph meta-learner for few-shot graph\nclassification,\u201d in CIKM, 2020, pp. 1055\u20131064.\n[221] S. Wang, Y. Dong, X. Huang, C. Chen, and J. Li, \u201cFaith: Few-shot\ngraph classification with hierarchical task graphs,\u201d in IJCAI, 2022,\npp. 1\u20137.\n[222] D. Crisostomi, S. Antonelli, V. Maiorca, L. Moschella, R. Marin,\nand E. Rodol`a, \u201cMetric based few-shot graph classification,\u201d in\nLoG, 2022.\n[223] J. Chauhan, D. Nathani, and M. Kaul, \u201cFew-shot learning on\ngraphs via super-classes based on graph spectral measures,\u201d in\nICLR, 2020.\n[224] K. Hassani, \u201cCross-domain few-shot graph classification,\u201d in\nAAAI, vol. 36, no. 6, 2022, pp. 6856\u20136864.\n[225] D. Fu, L. Fang, R. Maciejewski, V. I. Torvik, and J. He, \u201cMeta-\nlearned metrics over multi-evolution temporal graphs,\u201d in KDD,\n2022, pp. 367\u2013377.\n[226] Y. Wang, A. Abuduweili, Q. Yao, and D. Dou, \u201cProperty-aware\nrelation networks for few-shot molecular property prediction,\u201d\nNeurIPS, vol. 34, pp. 17 441\u201317 454, 2021.\n[227] Z. Guo, C. Zhang, W. Yu, J. Herr, O. Wiest, M. Jiang, and\nN. V. Chawla, \u201cFew-shot graph learning for molecular property\nprediction,\u201d in WWW, 2021, pp. 2559\u20132567.\n[228] H. S. de Oc\u00b4ariz Borde and F. Barbero, \u201cGraph neural network ex-\npressivity and meta-learning for molecular property regression,\u201d\nin LoG, 2022.\n[229] Z. Meng, Y. Li, P. Zhao, Y. Yu, and I. King, \u201cMeta-learning with\nmotif-based task augmentation for few-shot molecular property\nprediction,\u201d in SDM, 2023, pp. 811\u2013819.\n[230] W. Chen, A. Tripp, and J. M. Hern\u00b4andez-Lobato, \u201cMeta-learning\nadaptive deep kernel gaussian processes for molecular property\nprediction,\u201d in ICLR, 2023.\n[231] R. J. Williams, \u201cSimple statistical gradient-following algorithms\nfor connectionist reinforcement learning,\u201d Reinforcement learning,\npp. 5\u201332, 1992.\n[232] A. Nichol and J. Schulman, \u201cReptile: a scalable metalearning\nalgorithm,\u201d arXiv preprint arXiv:1803.02999, vol. 2, no. 3, p. 4,\n2018.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n25\n[233] M. Seeger, \u201cGaussian processes for machine learning,\u201d IJNS,\nvol. 14, no. 02, pp. 69\u2013106, 2004.\n[234] J. Wu, J. He, and J. Xu, \u201cDemonet: Degree-specific graph neural\nnetworks for node and graph classification,\u201d in KDD, 2019, pp.\n406\u2013415.\n[235] X. Tang, H. Yao, Y. Sun, Y. Wang, J. Tang, C. Aggarwal, P. Mitra,\nand S. Wang, \u201cInvestigating and mitigating degree-related biases\nin graph convoltuional networks,\u201d in CIKM, 2020, pp. 1435\u20131444.\n[236] B. Hao, J. Zhang, H. Yin, C. Li, and H. Chen, \u201cPre-training graph\nneural networks for cold-start users and items representation,\u201d\nin WSDM, 2021, pp. 265\u2013273.\n[237] C. Yang, C. Wang, Y. Lu, X. Gong, C. Shi, W. Wang, and X. Zhang,\n\u201cFew-shot link prediction in dynamic networks,\u201d in WSDM,\n2022, pp. 1245\u20131255.\n[238] X. Niu, B. Li, C. Li, R. Xiao, H. Sun, H. Deng, and Z. Chen, \u201cA\ndual heterogeneous graph attention network to improve long-tail\nperformance for shop search in e-commerce,\u201d in KDD, 2020, pp.\n3405\u20133415.\n[239] S. Kojaku, J. Yoon, I. Constantino, and Y.-Y. Ahn, \u201cResidual2vec:\nDebiasing graph embedding with random graphs,\u201d NeurIPS,\nvol. 34, pp. 24 150\u201324 163, 2021.\n[240] F. Xia, L. Wang, T. Tang, X. Chen, X. Kong, G. Oatley, and\nI. King, \u201cCengcn: centralized convolutional networks with vertex\nimbalance for scale-free graphs,\u201d TKDE, pp. 4555\u20134569, 2022.\n[241] S. Virinchi and A. Saladi, \u201cBlade: Biased neighborhood sampling\nbased graph neural network for directed graphs,\u201d in WSDM,\n2023, pp. 42\u201350.\n[242] D. Chen, Y. Lin, G. Zhao, X. Ren, P. Li, J. Zhou, and X. Sun,\n\u201cTopology-imbalance learning for semi-supervised node classifi-\ncation,\u201d NeurIPS, vol. 34, pp. 29 885\u201329 897, 2021.\n[243] Q. Sun, J. Li, H. Yuan, X. Fu, H. Peng, C. Ji, Q. Li, and P. S. Yu,\n\u201cPosition-aware structure learning for graph topology-imbalance\nby relieving under-reaching and over-squashing,\u201d in CIKM, 2022,\npp. 1848\u20131857.\n[244] X. Fu, Y. Wei, Q. Sun, H. Yuan, J. Wu, H. Peng, and J. Li, \u201cHy-\nperbolic geometric graph representation learning for hierarchy-\nimbalance node classification,\u201d in WWW, 2023, pp. 460\u2013468.\n[245] W. Zeng, X. Zhao, W. Wang, J. Tang, and Z. Tan, \u201cDegree-aware\nalignment for entities in tail,\u201d in SIGIR, 2020, pp. 811\u2013820.\n[246] J. Baek, D. B. Lee, and S. J. Hwang, \u201cLearning to extrapolate\nknowledge: Transductive few-shot out-of-graph link prediction,\u201d\nNeurIPS, vol. 33, pp. 546\u2013560, 2020.\n[247] M. Chen, W. Zhang, Z. Yao, X. Chen, M. Ding, F. Huang, and\nH. Chen, \u201cMeta-learning based knowledge extrapolation for\nknowledge graphs in the federated setting,\u201d in IJCAI, 2022, pp.\n1966\u20131972.\n[248] M. Chen, W. Zhang, Y. Zhu, H. Zhou, Z. Yuan, C. Xu, and\nH. Chen, \u201cMeta-knowledge transfer for inductive knowledge\ngraph embedding,\u201d in SIGIR, 2022, pp. 927\u2013937.\n[249] Z. Chen, C. Xu, F. Su, Z. Huang, and Y. Dou, \u201cMeta-learning\nbased knowledge extrapolation for temporal knowledge graph,\u201d\nin WWW, 2023, pp. 2433\u20132443.\n[250] E. Cao, D. Wang, J. Huang, and W. Hu, \u201cOpen knowledge\nenrichment for long-tail entities,\u201d in WWW, 2020, pp. 384\u2013394.\n[251] H. Shomer, W. Jin, W. Wang, and J. Tang, \u201cToward degree bias\nin embedding-based knowledge graph completion,\u201d in WWW,\n2023, pp. 705\u2013715.\n[252] J. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation:\nA survey,\u201d IJCV, vol. 129, pp. 1789\u20131819, 2021.\n[253] M. Farid, I. F. Ilyas, S. E. Whang, and C. Yu, \u201cLonlies: estimating\nproperty values for long tail entities,\u201d in SIGIR, 2016, pp. 1125\u2013\n1128.\n[254] T. Gao, X. Han, Z. Liu, and M. Sun, \u201cHybrid attention-based\nprototypical networks for noisy few-shot relation classification,\u201d\nin AAAI, vol. 33, no. 01, 2019, pp. 6407\u20136414.\n[255] M. Chen, W. Zhang, W. Zhang, Q. Chen, and H. Chen, \u201cMeta\nrelational learning for few-shot link prediction in knowledge\ngraphs,\u201d in EMNLP-IJCNLP, 2019, pp. 4217\u20134226.\n[256] G. Niu, Y. Li, C. Tang, R. Geng, J. Dai, Q. Liu, H. Wang, J. Sun,\nF. Huang, and L. Si, \u201cRelational learning with gated and attentive\nneighbor aggregator for few-shot knowledge graph completion,\u201d\nin SIGIR, 2021, pp. 213\u2013222.\n[257] L. Zhenzhen, Y. Zhang, J.-Y. Nie, and D. Li, \u201cImproving few-\nshot relation classification by prototypical representation learn-\ning with definition text,\u201d in NAACL, 2022, pp. 454\u2013464.\n[258] H. Wu, J. Yin, B. Rajaratnam, and J. Guo, \u201cHierarchical relational\nlearning for few-shot knowledge graph completion,\u201d in ICLR,\n2023.\n[259] S. Zheng, S. Mai, Y. Sun, H. Hu, and Y. Yang, \u201cSubgraph-aware\nfew-shot inductive link prediction via meta-learning,\u201d TKDE,\nvol. 35, no. 6, pp. 6512\u20136517, 2022.\n[260] C. Zhang, H. Yao, C. Huang, M. Jiang, Z. Li, and N. V. Chawla,\n\u201cFew-shot knowledge graph completion,\u201d in AAAI, vol. 34,\nno. 03, 2020, pp. 3041\u20133048.\n[261] Y. Han, L. Qiao, J. Zheng, Z. Kan, L. Feng, Y. Gao, Y. Tang, Q. Zhai,\nD. Li, and X. Liao, \u201cMulti-view interaction learning for few-shot\nrelation classification,\u201d in CIKM, 2021, pp. 649\u2013658.\n[262] J. Zhang, J. Zhu, Y. Yang, W. Shi, C. Zhang, and H. Wang,\n\u201cKnowledge-enhanced domain adaptation in few-shot relation\nclassification,\u201d in KDD, 2021, pp. 2183\u20132191.\n[263] S. Xiao, L. Duan, G. Xie, R. Li, Z. Chen, G. Deng, and J. Num-\nmenmaa, \u201cHmnet: Hybrid matching network for few-shot link\nprediction,\u201d in DASFAA, 2021, pp. 307\u2013322.\n[264] J. Sheng, S. Guo, Z. Chen, J. Yue, L. Wang, T. Liu, and H. Xu,\n\u201cAdaptive attentional network for few-shot knowledge graph\ncompletion,\u201d in EMNLP, 2020, pp. 1681\u20131691.\n[265] C. Dou, S. Wu, X. Zhang, Z. Feng, and K. Wang, \u201cFunction-words\nadaptively enhanced attention networks for few-shot inverse\nrelation classification,\u201d in IJCAI, 2022, pp. 2937\u20132943.\n[266] Z. Jiang, J. Gao, and X. Lv, \u201cMetap: Meta pattern learning for one-\nshot knowledge graph completion,\u201d in SIGIR, 2021, pp. 2232\u2013\n2236.\n[267] X. Zhang, X. Liang, X. Zheng, B. Wu, and Y. Guo, \u201cMultiform:\nFew-shot knowledge graph completion via multi-modal con-\ntexts,\u201d in ECML-PKDD, 2022, pp. 172\u2013187.\n[268] H. Ren, Y. Cai, R. Y. Lau, H.-f. Leung, and Q. Li, \u201cGranularity-\naware area prototypical network with bimargin loss for few shot\nrelation classification,\u201d TKDE, vol. 35, no. 5, pp. 4852\u20134866, 2022.\n[269] Y. Xiao, Y. Jin, and K. Hao, \u201cAdaptive prototypical networks\nwith label words and joint representation learning for few-shot\nrelation classification,\u201d TNNLS, pp. 1406\u20131417, 2021.\n[270] Y. Li, K. Yu, X. Huang, and Y. Zhang, \u201cLearning inter-\nentity-interaction for few-shot knowledge graph completion,\u201d in\nEMNLP, 2022, pp. 7691\u20137700.\n[271] T. Gao, X. Han, R. Xie, Z. Liu, F. Lin, L. Lin, and M. Sun, \u201cNeural\nsnowball for few-shot relation learning,\u201d in AAAI, vol. 34, no. 05,\n2020, pp. 7772\u20137779.\n[272] S. Wang, X. Huang, C. Chen, L. Wu, and J. Li, \u201cReform: Error-\naware few-shot knowledge graph completion,\u201d in CIKM, 2021,\npp. 1979\u20131988.\n[273] Z. Wang, K. Lai, P. Li, L. Bing, and W. Lam, \u201cTackling long-\ntailed relations and uncommon entities in knowledge graph\ncompletion,\u201d in EMNLP-IJCNLP, 2019, pp. 250\u2013260.\n[274] J. Zhang, T. Wu, and G. Qi, \u201cGaussian metric learning for few-\nshot uncertain knowledge graph completion,\u201d in DASFAA, 2021,\npp. 256\u2013271.\n[275] J. Xu, J. Zhang, X. Ke, Y. Dong, H. Chen, C. Li, and Y. Liu, \u201cP-int:\nA path-based interaction model for few-shot knowledge graph\ncompletion,\u201d in EMNLP, 2021, pp. 385\u2013394.\n[276] L. Luo, Y.-F. Li, G. Haffari, and S. Pan, \u201cNormalizing flow-based\nneural process for few-shot knowledge graph completion,\u201d in\nSIGIR, 2023.\n[277] J. Gong and H. Eldardiry, \u201cZero-shot relation classification from\nside information,\u201d in CIKM, 2021, pp. 576\u2013585.\n[278] P. Qin, X. Wang, W. Chen, C. Zhang, W. Xu, and W. Y. Wang,\n\u201cGenerative adversarial zero-shot relational learning for knowl-\nedge graphs,\u201d in AAAI, vol. 34, no. 05, 2020, pp. 8673\u20138680.\n[279] X. Lv, Y. Gu, X. Han, L. Hou, J. Li, and Z. Liu, \u201cAdapting meta\nknowledge graph information for multi-hop reasoning over few-\nshot relations,\u201d in EMNLP-IJCNLP, 2019, pp. 3376\u20133381.\n[280] S. Zheng, W. Chen, P. Zhao, A. Liu, J. Fang, and L. Zhao,\n\u201cWhen hardness makes a difference: Multi-hop knowledge graph\nreasoning over few-shot relations,\u201d in CIKM, 2021, pp. 2688\u20132697.\n[281] Y. Zhang, Y. Qian, Y. Ye, and C. Zhang, \u201cAdapting distilled\nknowledge for few-shot relation reasoning over knowledge\ngraphs,\u201d in SDM, 2022, pp. 666\u2013674.\n[282] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra et al., \u201cMatching\nnetworks for one shot learning,\u201d NeurIPS, vol. 29, pp. 3630\u20133638,\n2016.\n[283] X. Geng, X. Chen, K. Q. Zhu, L. Shen, and Y. Zhao, \u201cMick: A\nmeta-learning framework for few-shot relation classification with\nsmall training data,\u201d in CIKM, 2020, pp. 415\u2013424.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n26\n[284] G. Koch, R. Zemel, R. Salakhutdinov et al., \u201cSiamese neural\nnetworks for one-shot image recognition,\u201d in ICML deep learning\nworkshop, vol. 2, no. 1, 2015.\n[285] A. Nichol, J. Achiam, and J. Schulman, \u201cOn first-order meta-\nlearning algorithms,\u201d arXiv preprint arXiv:1803.02999, 2018.\n[286] X. Han, H. Zhu, P. Yu, Z. Wang, Y. Yao, Z. Liu, and M. Sun,\n\u201cFewrel: A large-scale supervised few-shot relation classification\ndataset with state-of-the-art evaluation,\u201d in EMNLP, 2018, pp.\n4803\u20134809.\n[287] T. Gao, X. Han, H. Zhu, Z. Liu, P. Li, M. Sun, and J. Zhou, \u201cFewrel\n2.0: Towards more challenging few-shot relation classification,\u201d\nin EMNLP-IJCNLP, 2019, pp. 6250\u20136255.\n[288] C. Zhang, L. Yu, M. Saebi, M. Jiang, and N. Chawla, \u201cFew-shot\nmulti-hop relation reasoning over knowledge bases,\u201d in EMNLP,\n2020, pp. 580\u2013585.\n[289] T. Zhao, D. Luo, X. Zhang, and S. Wang, \u201cTopoimb: Toward\ntopology-level imbalance in learning from graphs,\u201d in LoG, 2022.\n[290] Y. Zhu, Y. Lai, K. Zhao, X. Luo, M. Yuan, J. Ren, and K. Zhou,\n\u201cBinarizedattack: Structural poisoning attacks to graph-based\nanomaly detection,\u201d in ICDE, 2022, pp. 14\u201326.\n[291] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-\ntrain, prompt, and predict: A systematic survey of prompting\nmethods in natural language processing,\u201d CSUR, vol. 55, no. 9,\npp. 1\u201335, 2023.\n[292] X. Li, X. Yin, C. Li, P. Zhang, X. Hu, L. Zhang, L. Wang, H. Hu,\nL. Dong, F. Wei et al., \u201cOscar: Object-semantics aligned pre-\ntraining for vision-language tasks,\u201d in ECCV, 2020, pp. 121\u2013137.\n[293] W. Hu*, B. Liu*, J. Gomes, M. Zitnik, P. Liang, V. Pande, and\nJ. Leskovec, \u201cStrategies for pre-training graph neural networks,\u201d\nin ICLR, 2020.\n[294] Y. Kim and A. M. Rush, \u201cSequence-level knowledge distillation,\u201d\nin EMNLP, 2016, pp. 1317\u20131327.\n[295] W. Park, D. Kim, Y. Lu, and M. Cho, \u201cRelational knowledge\ndistillation,\u201d in CVPR, 2019, pp. 3967\u20133976.\n[296] J. Hoffman, S. Gupta, and T. Darrell, \u201cLearning with side infor-\nmation through modality hallucination,\u201d in CVPR, 2016, pp. 826\u2013\n834.\n[297] S. Vashishth, R. Joshi, S. S. Prayaga, C. Bhattacharyya, and\nP. Talukdar, \u201cReside: Improving distantly-supervised neural re-\nlation extraction using side information,\u201d in EMNLP, 2018, pp.\n1257\u20131266.\n[298] A. Bose and W. Hamilton, \u201cCompositional fairness constraints\nfor graph embeddings,\u201d in ICML, 2019, pp. 715\u2013724.\n[299] A. I. Schein, A. Popescul, L. H. Ungar, and D. M. Pennock,\n\u201cMethods and metrics for cold-start recommendations,\u201d in SIGIR,\n2002, pp. 253\u2013260.\n[300] A. B. Parsa, H. Taghipour, S. Derrible, and A. K. Mohammadian,\n\u201cReal-time accident detection: Coping with imbalanced data,\u201d\nAccident Analysis & Prevention, vol. 129, pp. 202\u2013210, 2019.\n[301] D. Guo, Z. Chu, and S. Li, \u201cFair attribute completion on graph\nwith missing attributes,\u201d in ICLR, 2022.\n[302] W. Fan, K. Liu, R. Xie, H. Liu, H. Xiong, and Y. Fu, \u201cFair graph\nauto-encoder for unbiased graph representations with wasser-\nstein distance,\u201d in ICDM, 2021, pp. 1054\u20131059.\n[303] Y. Dong, J. Kang, H. Tong, and J. Li, \u201cIndividual fairness for graph\nneural networks: A ranking based approach,\u201d in KDD, 2021, pp.\n300\u2013310.\n[304] E. Dai and S. Wang, \u201cSay no to the discrimination: Learning fair\ngraph neural networks with limited sensitive attribute informa-\ntion,\u201d in WSDM, 2021, pp. 680\u2013688.\n[305] Z. Chen, T. Xiao, and K. Kuang, \u201cBa-gnn: On learning bias-aware\ngraph neural network,\u201d in ICDE, 2022, pp. 3012\u20133024.\n[306] K. Burkholder, K. Kwock, Y. Xu, J. Liu, C. Chen, and S. Xie,\n\u201cCertification and trade-off of multiple fairness criteria in graph-\nbased spam detection,\u201d in CIKM, 2021, pp. 130\u2013139.\n[307] A. Rahmattalabi, P. Vayanos, A. Fulginiti, E. Rice, B. Wilder,\nA. Yadav, and M. Tambe, \u201cExploring algorithmic fairness in\nrobust graph covering problems,\u201d NeurIPS, vol. 32, pp. 15 776\u2013\n15 787, 2019.\n[308] A. Anagnostopoulos, L. Becchetti, A. Fazzone, C. Menghini,\nand C. Schwiegelshohn, \u201cSpectral relaxations and fair densest\nsubgraphs,\u201d in CIKM, 2020, pp. 35\u201344.\n[309] Z. Fu, Y. Xian, R. Gao, J. Zhao, Q. Huang, Y. Ge, S. Xu, S. Geng,\nC. Shah, Y. Zhang et al., \u201cFairness-aware explainable recommen-\ndation over knowledge graphs,\u201d in SIGIR, 2020, pp. 69\u201378.\n[310] A. Abouzeid, O.-C. Granmo, C. Webersik, and M. Goodwin,\n\u201cSocially fair mitigation of misinformation on social networks\nvia constraint stochastic optimization,\u201d in AAAI, vol. 36, no. 11,\n2022, pp. 11 801\u201311 809.\n[311] J. Ali, M. Babaei, A. Chakraborty, B. Mirzasoleiman, K. Gum-\nmadi, and A. Singla, \u201cOn the fairness of time-critical influence\nmaximization in social networks,\u201d TKDE, vol. 35, no. 3, pp. 2875\u2013\n2886, 2023.\n[312] L. Yao, Y. Li, B. Ding, J. Zhou, J. Liu, M. Huai, and J. Gao,\n\u201cPath-specific causal fair prediction via auxiliary graph structure\nlearning,\u201d in WWW, 2023, pp. 3680\u20133688.\n[313] D. Fu, D. Zhou, R. Maciejewski, A. Croitoru, M. Boyd, and J. He,\n\u201cFairness-aware clique-preserving spectral clustering of temporal\ngraphs,\u201d in WWW, 2023, pp. 3755\u20133765.\n[314] T. Rahman, B. Surma, M. Backes, and Y. Zhang, \u201cFairwalk:\nTowards fair graph embedding,\u201d in IJCAI, 2019, pp. 3289\u20133295.\n[315] A. Khajehnejad, M. Khajehnejad, M. Babaei, K. P. Gummadi,\nA. Weller, and B. Mirzasoleiman, \u201cCrosswalk: Fairness-enhanced\nnode representation learning,\u201d in AAAI, vol. 36, no. 11, 2022, pp.\n11 963\u201311 970.\n[316] S. Tsioutsiouliklis, E. Pitoura, P. Tsaparas, I. Kleftakis, and\nN. Mamoulis, \u201cFairness-aware pagerank,\u201d in WWW, 2021, pp.\n3815\u20133826.\n[317] Z. S. Jalali, W. Wang, M. Kim, H. Raghavan, and S. Soundarajan,\n\u201cOn the information unfairness of social networks,\u201d in SDM,\n2020, pp. 613\u2013521.\n[318] I. Spinelli, S. Scardapane, A. Hussain, and A. Uncini, \u201cFairdrop:\nBiased edge dropout for enhancing fairness in graph representa-\ntion learning,\u201d TAI, vol. 3, no. 3, pp. 344\u2013354, 2021.\n[319] P. Li, Y. Wang, H. Zhao, P. Hong, and H. Liu, \u201cOn dyadic fairness:\nExploring and mitigating bias in graph connections,\u201d in ICLR,\n2021.\n[320] J. Kang, J. He, R. Maciejewski, and H. Tong, \u201cInform: Individual\nfairness on graph mining,\u201d in KDD, 2020, pp. 379\u2013389.\n[321] H. Hussain, M. Cao, S. Sikdar, D. Helic, E. Lex, M. Strohmaier,\nand R. Kern, \u201cAdversarial inter-group link injection degrades the\nfairness of graph neural networks,\u201d in ICDM, 2022, pp. 975\u2013980.\n[322] Z. Liu, C. Zhang, Y. Tian, E. Zhang, C. Huang, Y. Ye, and\nC. Zhang, \u201cFair graph representation learning via diverse mix-\nture of experts,\u201d in WWW, 2023, pp. 28\u201338.\n[323] H. Ling, Z. Jiang, Y. Luo, S. Ji, and N. Zou, \u201cLearning fair graph\nrepresentations via automated data augmentations,\u201d in ICLR,\n2023.\n[324] Y. Dong, N. Liu, B. Jalaian, and J. Li, \u201cEdits: Modeling and\nmitigating data bias for graph neural networks,\u201d in WWW, 2022,\npp. 1259\u20131269.\n[325] Y. Wang, Y. Zhao, Y. Dong, H. Chen, J. Li, and T. Derr, \u201cImprov-\ning fairness in graph neural networks via mitigating sensitive\nattribute leakage,\u201d in KDD, 2022, pp. 1938\u20131948.\n[326] J. Ma, R. Guo, M. Wan, L. Yang, A. Zhang, and J. Li, \u201cLearning\nfair node representations with graph counterfactual fairness,\u201d in\nWSDM, 2022, pp. 695\u2013703.\n[327] Y. Dong, B. Zhang, Y. Yuan, N. Zou, Q. Wang, and J. Li, \u201cReliant:\nFair knowledge distillation for graph neural networks,\u201d in SDM,\n2023, pp. 154\u2013162.\n[328] M. Buyl and T. De Bie, \u201cDebayes: a bayesian method for debias-\ning network embeddings,\u201d in ICML, 2020, pp. 1220\u20131229.\n[329] X. Zhang, L. Zhang, B. Jin, and X. Lu, \u201cA multi-view confidence-\ncalibrated framework for fair and stable graph representation\nlearning,\u201d in ICDM, 2021, pp. 1493\u20131498.\n[330] N. Wang, L. Lin, J. Li, and H. Wang, \u201cUnbiased graph embedding\nwith biased graph observations,\u201d in WWW, 2022, pp. 1423\u20131433.\n[331] W. Song, Y. Dong, N. Liu, and J. Li, \u201cGuide: Group equality\ninformed individual fairness in graph neural networks,\u201d in KDD,\n2022, pp. 1625\u20131634.\n[332] J. Ma, J. Deng, and Q. Mei, \u201cSubgroup generalization and fairness\nof graph neural networks,\u201d NeurIPS, vol. 34, pp. 1048\u20131061, 2021.\n[333] Y. Liu, X. Ao, F. Feng, and Q. He, \u201cUd-gnn: Uncertainty-aware\ndebiased training on semi-homophilous graphs,\u201d in KDD, 2022,\npp. 1131\u20131140.\n[334] Y. Dong, S. Wang, Y. Wang, T. Derr, and J. Li, \u201cOn structural\nexplanation of bias in graph neural networks,\u201d in KDD, 2022, pp.\n316\u2013326.\n[335] B. Lika, K. Kolomvatsos, and S. Hadjiefthymiades, \u201cFacing the\ncold start problem in recommender systems,\u201d Expert systems with\napplications, vol. 41, no. 4, pp. 2065\u20132073, 2014.\n[336] M.\nVartak,\nA.\nThiagarajan,\nC.\nMiranda,\nJ.\nBratman,\nand\nH. Larochelle, \u201cA meta-learning perspective on cold-start recom-\nmendations for items,\u201d NeurIPS, vol. 30, pp. 6907\u20136917, 2017.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015\n27\n[337] H. Lee, J. Im, S. Jang, H. Cho, and S. Chung, \u201cMelu: Meta-learned\nuser preference estimator for cold-start recommendation,\u201d in\nKDD, 2019, pp. 1073\u20131082.\n[338] Y. Wei, X. Wang, Q. Li, L. Nie, Y. Li, X. Li, and T.-S. Chua,\n\u201cContrastive learning for cold-start recommendation,\u201d in MM,\n2021, pp. 5382\u20135390.\n[339] S. Wang, K. Zhang, L. Wu, H. Ma, R. Hong, and M. Wang,\n\u201cPrivileged graph distillation for cold start recommendation,\u201d in\nSIGIR, 2021, pp. 1187\u20131196.\n[340] Z. Liu, T.-K. Nguyen, and Y. Fang, \u201cOn generalized degree\nfairness in graph neural networks,\u201d AAAI, no. 4, pp. 4525\u20134533,\n2023.\n[341] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d NeurIPS, vol. 34, pp. 8780\u20138794, 2021.\n[342] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\nK. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans\net al., \u201cPhotorealistic text-to-image diffusion models with deep\nlanguage understanding,\u201d NeurIPS, vol. 35, pp. 36 479\u201336 494,\n2022.\n[343] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever et al.,\n\u201cImproving language understanding by generative pre-training,\u201d\n2018.\nZemin Liu is currently a senior research fellow\nwith the School of Computing, National Univer-\nsity of Singapore. He received his Ph.D. degree\nin Computer Science from Zhejiang University,\nHangzhou, China in 2018, and B.S. Degree in\nSoftware Engineering from Shandong Univer-\nsity, Jinan, China in 2012. His research interests\nlie in graph embedding, graph neural networks,\nand learning on heterogeneous information net-\nworks.\nYuan Li is a Ph.D. student of Data Science at\nNational University of Singapore. His current re-\nsearch interests include graph neural networks\nand learning on temporal graphs.\nNan Chen is a research assistant at National\nUniversity of Singapore. His current research\ninterests include graph representation learning\nand equivariant machine learning.\nQian Wang is a Ph.D. student specializing in\nComputer Science at National University of Sin-\ngapore. She has consistently demonstrated her\nacademic excellence, having previously gradu-\nated with honors from the same institution with\na Bachelor of Computing degree in Computer\nScience in 2023. Her current research interests\nlie in blockchain and graph neural networks.\nBryan Hooi is an assistant professor in the\nSchool of Computing and the Institute of Data\nScience in National University of Singapore. He\nreceived his Ph.D. degree in Machine Learning\nfrom Carnegie Mellon University, USA in 2019.\nHis research interests include methods for learn-\ning from graphs and other complex or multimodal\ndatasets, with the goal of developing efficient\nand practical approaches for applications such\nas the detection of anomalies or malicious be-\nhavior, and automatic monitoring of medical, traf-\nfic, and environmental sensor data.\nBingsheng He received the bachelor\u2019s degree\nin computer science from Shanghai Jiao Tong\nUniversity, China, in 2003, and the Ph.D. degree\nin computer science from the Hong Kong Univer-\nsity of Science and Technology, Hong Kong, in\n2008. He is currently a professor with the School\nof Computing, National University of Singa-\npore, Singapore. His research interests are high-\nperformance computing, distributed and parallel\nsystems, and database systems.\n",
    "2308.14181": "Class-Imbalanced Graph Learning without Class Rebalancing\nZhining Liu 1 Ruizhong Qiu 1 Zhichen Zeng 1 Hyunsik Yoo 1 David Zhou 1 Zhe Xu 1 Yada Zhu 2\nKommy Weldemariam 3 Jingrui He 1 Hanghang Tong 1\nAbstract\nClass imbalance is prevalent in real-world node\nclassification tasks and poses great challenges\nfor graph learning models. Most existing stud-\nies are rooted in a class-rebalancing (CR) per-\nspective and address class imbalance with class-\nwise reweighting or resampling. In this work, we\napproach the root cause of class-imbalance bias\nfrom an topological paradigm. Specifically, we\ntheoretically reveal two fundamental phenomena\nin the graph topology that greatly exacerbate the\npredictive bias stemming from class imbalance.\nOn this basis, we devise a lightweight topologi-\ncal augmentation framework BAT to mitigate the\nclass-imbalance bias without class rebalancing.\nBeing orthogonal to CR, BAT can function as an\nefficient plug-and-play module that can be seam-\nlessly combined with and significantly boost ex-\nisting CR techniques. Systematic experiments\non real-world imbalanced graph learning tasks\nshow that BAT can deliver up to 46.27% perfor-\nmance gain and up to 72.74% bias reduction over\nexisting techniques. Code, examples, and docu-\nmentations are available at https://github.\ncom/ZhiningLiu1998/BAT.\n1. Introduction\nNode classification stands as one of the most fundamental\ntasks in graph machine learning, holding significant rel-\nevance in various real-world applications (Akoglu et al.,\n2015; Tang & Liu, 2010). Graph Neural Networks (GNNs)\nhave demonstrated great success in tackling related tasks\ndue to their robust representation learning capabilities (Song\net al., 2022b; Fu & He, 2021). However, real-world graphs\nare often inherently class-imbalanced, i.e., the sizes of\nunique classes vary significantly, and a few majority classes\n1University\nof\nIllinois\nUrbana-Champaign\n2IBM\nRe-\nsearch 3Amazon Science.\nCorrespondence to:\nZhining Liu\n<liu326@illinois.edu>, Hanghang Tong <htong@illinois.edu>.\nProceedings of the 41 st International Conference on Machine\nLearning, Vienna, Austria. PMLR 235, 2024. Copyright 2024 by\nthe author(s).\nhave overwhelming numbers in the training set. In Class-\nImbalanced Graph Learning (CIGL), GNNs are prone to\nsuffer from severe performance degradation on minority\nclass nodes (Park et al., 2022). This results in a pronounced\npredictive bias characterized by a large performance dispar-\nity between the majority and minority classes.\nTraditional imbalance-handling techniques rely on class re-\nbalancing (CR) such as class reweighting and resampling\n(Chawla et al., 2002; Cui et al., 2019), which works well for\nnon-graph data. Recent studies propose more graph-specific\nCR strategies tailored for CIGL, e.g., neighborhood-aware\nreweighting (Li et al., 2022; Huang et al., 2022) and over-\nsampling (Zhao et al., 2021b; Park et al., 2022). Nonethe-\nless, these works are restricted to the class-rebalancing\nparadigm. Parallel to class imbalance, another emerging\nline of research studies topology imbalance, characterized\nby \u201cthe asymmetric topological properties of the labeled\nnodes\u201d (Chen et al., 2021). It is considered an orthogonal\nproblem to class imbalance, and hence, few work theo-\nretically investigates how topological structure affects the\nlearning on class imbalanced graphs. To fill this gap, we\nconduct an in-depth analysis of the role that topology plays\nin class-imbalanced graph learning. We theoretically show\nthat topological differences between minority and major-\nity classes significantly amplify the class imbalance bias,\nimposing great challenges to CIGL. This reveal an unex-\nplored avenue that limits the performance of existing CIGL\ntechniques: mitigating class imbalance bias arising from im-\nbalanced topology structure through topological operations.\nFollowing this novel perspective, we devise a lightweight\npractical solution for CIGL that can be seamlessly combined\nwith and further boost existing CR techniques.\nIn this work, we formally define and theoretically investigate\ntwo fundamental local topological phenomena that greatly\nhinder CIGL: (i) ambivalent message-passing (AMP), i.e.,\nhigh ratio of non-self-class neighbors in the node receptive\nfield, and (ii) distant message-passing (DMP), i.e., poor\nconnectivity with self-class labeled nodes. Intuitively, AMP\nleads to a higher influx of noisy information and DMP leads\nto poor reception of effective supervision signals in message-\npassing. Both result in lower signal-to-noise ratios and thus\ninduce higher classification errors. Our theoretical finding\nreveals that the minority class is inherently more susceptible\n1\narXiv:2308.14181v2  [cs.LG]  19 May 2024\nClass-Imbalanced Graph Learning without Class Rebalancing\n(a) L: concept of AMP. R: relative performance loss with\nrespect to the non-self-class neighbor ratio.\n(b) L: concept of DMP. R: relative performance loss w.r.t.\ndistance to the nearest same-class labeled node.\nFigure 1. Concepts of ambivalent message-passing (AMP) and distant message-passing (DMP) and their impact in real-world imbalanced\nnode classification tasks (Park et al., 2022). Both factors lead to a substantial increase in prediction errors, and further, a larger\nperformance disparity/bias (i.e., the gap between the blue and orange curves) between the majority and minority classes.\nto both AMP and DMP (Theorem 2.1 & 2.2), which leads\nto a more pronounced predictive bias. Such bias induced by\nthe graph topology escalates as the level of class imbalance\nincreases. We emphasize that AMP/DMP is defined for\nall nodes based on the local neighborhood, while influence\nconflict has no formal definition and influence insufficiency\nis defined only on labeled nodes with global PageRank score\n(Chen et al., 2021). To distinguish from them, we use the\nterminology AMP/DMP instead. Further discussions can be\nfound in Section 5. Fig. 1 visually illustrates the concepts\nof AMP and DMP, highlighting their distinct impacts on the\npredictive performance of majority and minority classes.\nFollowing our theoretical and empirical findings, we de-\nvise BAT (BAlanced Topological augmentation), a model-\nagnostic and efficient technique to mitigate class imbalance\nbias in CIGL via topological augmentation. BAT dynam-\nically locates and rectifies nodes critically influenced by\nAMP and DMP during learning, thereby effectively reduc-\ning the errors and biases in CIGL. Being orthogonal to\nclass rebalancing, our solution is able to work hand-in-hand\nwith existing techniques based on reweighting (Japkowicz\n& Stephen, 2002; Chen et al., 2021) and resampling (Zhao\net al., 2021b; Park et al., 2022) and further boost their perfor-\nmance. Systematic experiments on real-world CIGL tasks\nshow that BAT delivers significant performance boost (up\nto 46.27%) and bias reduction (up to 72.74%) over various\nCIGL baselines with diverse GNN architectures.\nOur contributions: (i) Novel Perspective. We demonstrate\nthe feasibility of taming class-imbalance bias without class\nrebalancing, which provides a new avenue that is orthogonal\nto the predominant class-rebalancing practice in CIGL. (ii)\nTheoretical Insights. We theoretically reveal the topolog-\nical difference between minority and majority classes and\nits role in shaping predictive bias in CIGL, shedding light\non future CIGL research. Empirical results validate our\nfindings. (iii) Practical Solution. Motivated by theoretical\nand empirical finding, we devise a lightweight and versatile\nframework BAT to handle topological challenges in CIGL.\nBeing complementary to class rebalancing, it can be seam-\nlessly combined with and significantly boost existing CIGL\ntechniques. (iv) Empirical Study. Systematic experiments\nand analysis across a diverse range of real-world tasks and\nGNN architectures show that BAT consistently demonstrates\nsuperior performance in both promoting classification and\nmitigating predictive bias.\n2. Class Imbalance and Local Topology\nIn this section, we delve into the impact of graph topol-\nogy on the predictive bias in class-imbalanced node clas-\nsification. We theoretically unveil that compared to the\nmajority class, the minority class is inherently more sus-\nceptible to both Ambivalent Message Passing (AMP) and\nDistant Message Passing (DMP). This significantly worsens\nminority-class performance and leads to a more pronounced\npredictive bias stemming from class imbalance. After that,\nwe present an empirical analysis to validate our theoretical\nfindings, and to provide insights on how to mitigate the bias\ninduced by AMP and DMP in practice. Detailed proofs can\nbe found in Appendix A.\nTheoretical analysis on local topology. Consider a graph\nG : (V, E) from a stochastic block model (Holland et al.,\n1983) SBM(n, p, q), where n is the total number of nodes, p\nand q are the intra- and inter-class node connection probabil-\nity. To facilitate analysis, we call node u homo-connected to\nnode v if there is a path [u, v1, ..., vk, v] where v1, ..., vk, v\nare of the same class, and let H(u, k) denote the set of\nk-hop homo-connected neighbors of u. For binary node\nclassification, we denote the number of nodes of class i as\nni (n1 + n2 = n); without loss of generality, let class 1/2\nbe the minority/majority class (thus n1 \u226an2). We denote\nclass i\u2019s node set as Vi and labeled node set as VL\ni (\u2282Vi).\nFor asymptotic analysis, we adopt conventional assump-\ntions: n1 \u00b7 p = \u03b2 + O\n\u0000 1\nn\n\u0001\n(i.e., \u03b2 is the average intra-class\n2\nClass-Imbalanced Graph Learning without Class Rebalancing\n(a) Distribution of node AMP/DMP coefficients.\n(b) Impact of AMP/DMP on predictive performance.\nFigure 2. Node-level distribution of AMP and DMP coefficients and their impact on learning.\nnode degree of class 1); p/q = O(1) (Decelle et al., 2011).\nWe now give formal definitions of AMP and DMP. For a\nnode u from class i, we define its (i) k-hop AMP coeffi-\ncient \u03b1k(u) \u2208[0, \u221e) as the ratio of the expected number of\nnon-self-class nodes to self-class nodes in its k-hop neigh-\nborhood H(u, k), i.e., \u03b1k(u) :=\n|{v|v /\u2208Vi,v\u2208H(u,k)}|\n|{v|v\u2208Vi,v\u2208H(u,k)}|; (ii)\nk-hop DMP coefficient \u03b4k(u) \u2208{0, 1} as the indicator of\nwhether all labeled nodes in its k-hop neighborhood are\nNON-self-class, i.e., \u03b4k(u) := 1(Lk\ni (u) = 0, \u03a3jLk\nj (u) >\n0), where Lk\nj (u) = |{v|v \u2208VL\nj , v \u2208H(u, k)}|. For an\nintuitive example, the target node (marked by the dashed\nbox) in Fig. 1(a) has \u03b11(u) = 3/1, \u03b41(u) = 0 and node\nin Fig. 1(b) has \u03b11(u) = 1/1, \u03b41(u) = 1. Further, to\ncharacterize the level of AMP/DMP for different class,\nfor class i we define \u03b1k\ni :=\nEu\u2208Vi[|{v|v /\u2208Vi,v\u2208H(u,k)}|]\nEu\u2208Vi[|{v|v\u2208Vi,v\u2208H(u,k)}|] and\n\u03b4k\ni := P(\u03b4k(u) = 1), where u is a node of class i. In-\ntuitively, a higher \u03b1i or \u03b4i indicates that class i is more\nsusceptible to AMP or DMP. Building on these metrics, we\nanalyze the disparities in \u03b1 and \u03b4 between minority and\nmajority classes, thereby providing insights into how the\ngraph topology induces additional class-imbalance bias.\nTo simplify notation, we define imbalance ratio \u03c1 := n2/n1.\nThe larger the \u03c1 is, the more imbalanced the dataset is. Then\nfor AMP, we have the following Theorem 2.1.\nTheorem 2.1 (AMP-sourced bias). For a large n, the ratio\nof AMP coefficients \u03b1 for the minority class to the majority\nclass grows polynomially with the imbalance ratio \u03c1 and\nexponentially with k:\n\u03b1k\n1\n\u03b1k\n2\n=\n\u0012\n\u03c1 \u00b7\nPk\nt=1(\u03c1\u03b2)t\u22121\nPk\nt=1 \u03b2t\u22121\n\u00132\n+ O\n\u0010 1\nn\n\u0011\n.\n(1)\nProof. Please see Appendix A.2.\nTheorem 2.1 shows that the same-class neighbor proportion\nof minority-class nodes is significantly smaller than that\nof majority-class nodes, i.e., the minority class is more\nsusceptible to AMP. As the imbalance ratio \u03c1 increases,\nthis issue becomes even more pronounced and introduces a\nhigher bias into the learning process. Moving on to DMP,\nwe have the following theorem 2.2.\nTheorem 2.2 (DMP-sourced bias). Let rL\ni := |VL\ni |\n|Vi| denote\nthe label rate of class i. For a large n, the ratio of DMP\ncoefficients \u03b4 of the minority class over the majority class\ngrows exponentially with \u03c1:\n\u03b4k\n1\n\u03b4k\n2\n\u22481 \u2212rL\n1\n1 \u2212rL\n2\ne(\u03c1\u22121)\u03b2 + O\n\u0010 1\nn\n\u0011\n.\n(2)\nProof. Please see Appendix A.3.\nSimilarly, the result shows that the minority class exhibits a\nsignificantly higher susceptibility to DMP than the majority\nclass. Theorem 2.2 also has several interesting implications:\n(i) The imbalance ratio greatly affects the bias induced by\nDMP, as \u03b4k\n1/\u03b4k\n2 grows exponentially with \u03c1. (ii) Labeling\nmore minority-class nodes can mitigate, but hardly solve\nthe problem. Enlarging the minority-class label rate rL\n1\ncan linearly shrink \u03b4k\n1/\u03b4k\n2, but it can hardly eliminate the\nbias induced by DMP (i.e., to have \u03b4k\n1 \u2264\u03b4k\n2) as e(\u03c1\u22121)\u03b2 is\nusually very large in practice. Take the Cora dataset (Sen\net al., 2008) as an example (let class 1/class 2 denote the\nsmallest/largest class): eliminating the DMP bias requires\nthe minority-class label rate rL\n1 \u22651 \u2212\n1\u2212rL\n2\ne(\u03c1\u22121)\u03b2 > 1 \u22125.05 \u00d7\n10\u22128, which is practically infeasible.\nOur theoretical findings show that both AMP and DMP\naffect the minority and majority classes differently, and the\ndifference is primarily determined by the imbalance ratio \u03c1.\nHowever, directly manipulating \u03c1 is tricky in practice as it\nrequires sampling new nodes and edges from an unknown\nunderlying graph generation model, or at least, simulating\nthe process by oversampling.\nA closer look at AMP & DMP in practice. To verify the\ntheoretical results, and to provide more insights on how\nto mitigate the bias brought about by AMP and DMP in\npractice, we conduct a fine-grained empirical analysis on\na real-world task. Results are detailed in Fig. 21. Start-\ning from Fig. 2(a), we can first observe that the minor-\nity class 1 has a larger proportion of nodes with high \u03b1\nor \u03b4 than the majority class 2, i.e., minority class 1 has\nhigher average \u03b1 and \u03b4 (specifically, \u03b11/\u03b12 = 1.357/0.179,\n1Results obtained by training a GCN on the PubMed dataset.\n3\nClass-Imbalanced Graph Learning without Class Rebalancing\nLegend:\nLabeled/Unlabeled\nFalse Negative\nNode Uncertainty/Risk\nVirtual Node\nVirtual Edge\n\ud835\udc9a\ud835\udc97= \ud835\udc1c\ud835\udfcf , \u0ddd\ud835\udc9a\ud835\udc97= \ud835\udc1c\ud835\udfd0\n\u0ddd\ud835\udc29\ud835\udc97: [\u0ddd\ud835\udc91\ud835\udc97\n(\ud835\udfcf), \u0ddd\ud835\udc91\ud835\udc97\n(\ud835\udfd0)]\nUncertainty Evaluation\nEq.(3): \ud835\udd4c\ud835\udeaf(\ud835\udc97) = \ud835\udc85\ud835\udc13\ud835\udc15\u0ddd\ud835\udc91\ud835\udc97, \ud835\udfd9\u0ddd\ud835\udc9a\ud835\udc97\nImbalance-aware Calibration\nEq.(4): \ud835\udc93\ud835\udc97=\n\ud835\udd4c\ud835\udf23\ud835\udc97\n\ud835\udc8e\ud835\udc82\ud835\udc99\ud835\udc8b=\ud835\udfcf\n\ud835\udc6a\n\ud835\udce5\ud835\udc8b\n\ud835\udc73/\ud835\udce5\u0ddd\ud835\udc9a\ud835\udc97\n\ud835\udc73 \nEq.(5): 0th order est. \u0ddc\ud835\udc2c\ud835\udc97: [\u0ddc\ud835\udc94\ud835\udc97\ud835\udfcf, \u0ddc\ud835\udc94\ud835\udc97\ud835\udfd0]\nEq.(6) 1st order est. \u0ddc\ud835\udc2c\ud835\udc97: [\u0ddc\ud835\udc94\ud835\udc97\ud835\udfcf, \u0ddc\ud835\udc94\ud835\udc97\ud835\udfd0]\nVirtual Node \ud835\udc97\ud835\udfcf\n\u2217\nVirtual Edge \ud835\udc86\u2217\nGet virtual nodes \ud835\udc97\ud835\udc8b\n\u2217: (\ud835\udc99\ud835\udc8b\n\u2217, \ud835\udc84\ud835\udc8b)\nGet virtual edges w.r.t. \ud835\udc92\ud835\udc97\u2217= \ud835\udf38\ud835\udc97\u22c5\u0ddc\ud835\udc2c\ud835\udc97\nHigh-risk Low-risk\nFalse\nNegative\nPredicted \nProbability\n\u0ddd\ud835\udc91\ud835\udc97\ud835\udfd0\n\u0ddd\ud835\udc91\ud835\udc97\ud835\udfcf\nLocal \nConnectivity\n2\n1\nAmbivalent & Distant \nMessage-Passing\nUncertainty-based Node Risk Estimation \n(\u25b7Section 3.1) \nPosterior Likelihood \nEst. (\u25b7Section 3.2) \nTopology Augmentation\n(\u25b7Section 3.3) \nMinority/Majority\nHigh-risk Low-risk\nAvg.\nHigh-risk Low-risk\nFigure 3. The proposed BAT (BAlanced Topological augmentation) framework, best viewed in color.\n\u03b41/\u03b42 = 0.040/0.004), which is consistent with our theo-\nretical findings. Further, Fig. 2(b) shows that both AMP\nand DMP significantly reduce the prediction accuracy, es-\npecially for the minority class that inherently has poorer\ndata representation. This can be explained from a graph\nsignal denoising perspective (Nt & Maehara, 2019): AMP\nintroduces additional noise from dissimilar nodes, and DMP\nleads to less efficient label propagation/denoising, thus their\nimpact is particularly significant on minority classes that are\nmore susceptible to noise (Johnson & Khoshgoftaar, 2019)\ndue to poor representation in the feature space. We further\nnotice an intriguing fact that, at sample-level, the impact of\nAMP/DMP is concentrated on a small fraction of minority\nclass nodes with large \u03b1 or \u03b4 (e.g., the \u03b1 \u22651 / \u03b4 = 1 part\nin Fig. 2(a)). In other words, one can surrogate the tricky\nmanipulation of \u03c1 and directly mitigate AMP/DMP by lo-\ncating and rectifying a small number of critical nodes, and\nthis exactly motivates our subsequent studies.\n3. Handling Class Imbalance from a\nTopological Perspective\nArmed with the findings from Section 2, we now discuss\nhow to devise a practical strategy to mitigate the error and\nbias induced by graph topology in CIGL. Earlier analyses\nhave shown that this can be achieved by identifying and\nrectifying the critical nodes that are highly influenced by\nAMP/DMP. This naturally poses two challenging questions:\n(i) How can critical nodes be located as the direct calculation\nof \u03b1/\u03b4 using ground-truth labels is not possible? (ii) Subse-\nquently, how can critical nodes be rectified and minimize\nthe negative impact caused by AMP and DMP?\nIn answering the above questions, we devise a lightweight\nframework BAT (BAlanced Topological augmentation) for\nhandling the topology-sourced errors and biases in CIGL.\nSpecifically, for locating the misclassified nodes, BAT lever-\nages model-based prediction uncertainty to assess the risk\nof potential misclassification caused by AMP/DMP for each\nnode (\u00a7 3.1). Then to rectify a misclassified node, we esti-\nmate a posterior likelihood of each node being in each class\n(\u00a7 3.2) and dynamically augment the misclassified node\u2019s\ntopological context based on our risk scores and posterior\nlikelihoods (\u00a7 3.3) thereby mitigating the impact of AMP\nand DMP. An overview of the proposed BAT framework is\nshown in Fig. 3.\n3.1. Node Misclassification Risk Estimation\nWe now elaborate on the technical details of BAT. As dis-\ncussed earlier, our first step is to locate the critical nodes that\nare highly influenced by AMP/DMP. Given the unavailabil-\nity of ground-truth labels, direct computation of AMP/DMP\ncoefficient is infeasible in practice. Fortunately, recent stud-\nies have shown that conflicting or lack of information from\nthe neighborhood can disturb GNN learning and the associ-\nated graph-denoising process for affected nodes (Nt & Mae-\nhara, 2019; Wu et al., 2019; Ma et al., 2021). This further\nyields high vacuity or dissonance uncertainty (Stadler et al.,\n2021; Zhao et al., 2020) in the prediction. This motivates\nus to exploit the model prediction uncertainty to estimate\nnodes\u2019 risk of being misclassified due to AMP/DMP.\nUncertainty quantification.\nWhile there exist many\ntechniques for uncertainty quantification (e.g., Bayesian-\nbased (Zhang et al., 2019; Hasanzadeh et al., 2020), Jack-\nknife sampling (Kang et al., 2022a)), they often either have\nto modify the model architecture, and/or impose additional\ncomputational overhead. In this study, we aim to streamline\nthe design of BAT for optimal efficiency and adaptability.\nTo this end, we employ an efficient and highly effective\napproach to uncertainty quantification. Formally, let C\nbe the number of classes. For a node v, consider model\nF(\u00b7; \u0398)\u2019s predicted probability vector \u02c6pv = F(A, X; \u0398)v,\ni.e., \u02c6p(j)\nv\n= P(yv = j|A, X, \u0398). Let \u02c6yv be the predicted\nlabel. We measure the uncertainty score U\u0398(v) by the total\nvariation (TV) distance:\nU\u0398(v) := dTV(\u02c6pv, 1\u02c6yv) = 1\n2\nPC\nj=1 |\u02c6p(j)\nv\n\u22121(j)\n\u02c6yv | \u2208[0, 1]. (3)\nIntuitively, a node has higher uncertainty if the model is less\nconfident about its current prediction. We remark that this\n4\nClass-Imbalanced Graph Learning without Class Rebalancing\nmetric can be naturally replaced by other uncertainty mea-\nsures (e.g., information entropy or more complex ones) with\nadditional computation cost, yet the impact on performance\nis marginal. Please refer to the ablation study provided in\nAppendix C.1 for more details.\nImbalance-calibrated misclassification risk. Due to the\nlack of training instances, minority classes generally ex-\nhibit higher uncertainty. Therefore, using U\u0398(\u00b7) directly\nas the risk score would treat most minority-class nodes as\nhigh-risk, which is contrary to our intention of rectifying the\nfalse negatives (i.e., minority nodes wrongly predicted as\nmajority-class) that cause bias in CIGL. To cope with this,\nwe propose imbalance-aware calibration for misclassifica-\ntion risk scores. For each class i, let \u02c6Vi := {u \u2208V|\u02c6yu = i}\nand \u02c6VL\ni := {u \u2208VL|yu = i}. For node v with predicted\nlabel \u02c6yv, we define its risk rv as:\nrv :=\nU\u0398(v)\nmaxC\nj=1 |VL\nj |/|VL\n\u02c6yv | \u2208[0, 1].\n(4)\nIntuitively speaking, Eq. (4) calibrates v\u2019s prediction un-\ncertainty by a label imbalance score maxC\nj=1 |VL\nj |/|VL\n\u02c6yv|.\nMinority classes with smaller labeled sets VL\ni will be dis-\ncounted more.\nEmpirical validation. We validate the effectiveness of the\nproposed node risk assessment method, as shown in Fig. 4.\nThe results indicate that our approach can accurately esti-\nmate node misclassification risk across various real-world\nCIGL tasks while enjoying computational efficiency.\nFigure 4. The negative correlation between\nthe estimated node risk (x-axis) and the\nprediction accuracy (y-axis). We apply 10\nsliding windows to compute the mean and\ndeviation of the accuracy.\n3.2. Posterior Likelihood Estimation\nWith the estimated risk scores of being affected by\nAMP/DMP, we move to the next question: how to rectify\nhigh-risk nodes with topological augmentation? As high-\nrisk nodes are prone to misclassification, their true labels are\nmore likely to be among the non-predicted classes j \u0338= \u02c6yv.\nThis motivates us to investigate schemes to harness infor-\nmation from these non-predicted classes. Since uniformly\ndrawing from all classes probably introduces noise to learn-\ning, we propose to estimate the posterior likelihood \u02c6s(j)\nv\nthat\na high-risk node v belongs to each class j after observing\nthe current predictions. To estimate \u02c6s(j)\nv , we introduce a\nzeroth-order scheme and a first-order scheme with O(|V|C)\nand O(|E|C) time complexity, respectively. Please refer\nto \u00a74 for a practical complexity analysis. We do not em-\nploy higher-order schemes due to the O(|V|k\u22121|E|C) time\ncomplexity of the kth-order scheme.\nZeroth-order estimation. A natural approach is to utilize\nthe predicted probabilities \u02c6p(j)\nv . As we have shown, the\npredicted label \u02c6yv of a high-risk node v is very likely to be\nwrong. Thus, we define the posterior likelihood \u02c6s(j)\nv\nas the\nconditional probability given that the class is not \u02c6yv, i.e.,\n\u02c6s(j)\nv\n:= Py\u223c\u02c6pv[y = j|y \u0338= \u02c6yv]\n=\n(\n\u02c6p(j)\nv /(1 \u2212\u02c6p(\u02c6yv)\nv\n),\nif j \u0338= \u02c6yv,\n0,\notherwise.\n(5)\nIntuitively, the zeroth-order posterior likelihoods \u02c6s(j)\nv\nare\nconsistent with the predicted probabilities \u02c6p(j)\nv\nexcept for\nthe wrongly predicted label j = \u02c6yv. This can be computed\nefficiently on GPU in matrix form.\nFirst-order estimation via random walk. We further ex-\nplore the local topology for \u02c6s(j)\nv\nestimation. Since neighbor-\ning nodes on a homophily graph tend to share labels, we\nconsider a 1-step random walk starting from node v. Let\nN(v) be the neighboring node set of v, and let v\u2032 \u223cN(v)\ndenote the ending node of the random walk. We define \u02c6s(j)\nv\nas the conditional probability that v\u2032 is predicted as class j\ngiven that v\u2032 is not predicted as class \u02c6yv, i.e.,\n\u02c6s(j)\nv\n:= Pv\u2032\u223cN (v)[\u02c6yv\u2032 = j|\u02c6yv\u2032 \u0338= \u02c6yv]\n=\n(\n|{v\u2032\u2208N (v)|\u02c6yv\u2032=j}|\n|N (v)|\u2212|{v\u2032\u2208N (v)|\u02c6yv\u2032=\u02c6yv}|,\nif j \u0338= \u02c6yv,\n0,\notherwise.\n(6)\nWith first-order estimation, \u02c6s(j)\nv\nis proportional to the la-\nbel frequency among adjacent nodes. Different from the\nzeroth-order scheme, this scheme relies on both node-level\npredictions and local connectivity patterns. The computa-\ntion can be done via sparse matrix operation with O(|E|C)\ntime complexity. As a remark, although this scheme can\nextend to k-step random walks, we do not employ them due\nto the O(|V|k\u22121|E|C) complexity of exact computation and\nthe high variance of stochastic computation.\nEmpirical validation. Figure 5 compares the two schemes\nin practice. Results show that all high-risk (rv > 0) minority\nnodes are misclassified, and both schemes can effectively\nfind alternatives with significantly higher chances to be the\nground truth class for high-risk nodes.\nFigure 5. The minority-class accuracy of\nmodel prediction \u02c6yv = F(A, X; \u0398), and\nmax-likelihood-based candidate selection\n\u02c6ys\nv = arg max(\u02c6sv), on PubMed dataset.\nNote that this is just an illustrative example\nusing arg max(\u02c6sv). In practice, we con-\nsider the whole \u02c6sv when sampling virtual\nedges, as described in Section 3.3.\n5\nClass-Imbalanced Graph Learning without Class Rebalancing\n3.3. Virtual Topology Augmentation\nFinally, we discuss how to mitigate AMP and DMP via\ntopology augmentation using our node risk scores and the\nposterior likelihoods. The general idea is to augment the\nlocal topology of high-risk nodes so as to integrate informa-\ntion from nodes that share similar patterns (mitigate AMP),\neven if they are not closely adjacent to each other in the\ngraph topology (mitigate DMP), thus achieving less-biased\nCIGL. A straightforward way is to connect high-risk nodes\nto nodes from high-likelihood classes in the original graph.\nHowever, this can be problematic in practice as a massive\nnumber of possible edges could be generated, greatly dis-\nturbing the original topology structure.\nTo achieve efficient augmentation without disrupting the\ngraph topology, we create virtual nodes (one per class)\nas \u201cshortcuts\u201d connecting to high-risk nodes according to\nposterior likelihoods. These shortcuts aggregate and pass\nclass information to high-risk nodes from nodes that ex-\nhibit similar patterns (even if they are distant in the origi-\nnal graph), thus mitigating both AMP and DMP. Formally,\nfor each class j, we build a virtual node v\u2217\nj with feature\nxv\u2217\nj := P\nv\u2208\u02c6Vj xv/|\u02c6Vj| and label yv\u2217\nj := j, and compute\nthe average risk \u00afrj := P\nv\u2208\u02c6Vj rv/|\u02c6Vj|. Then for each node\nv, we connect a virtual edge between v and virtual node\nv\u2217\nj with probability proportional to the posterior likelihood\n\u02c6s(j)\nv . However, if the connection probability is exactly \u02c6s(j)\nv ,\nthere will be many unnecessary virtual edges for low-risk\nnodes. Hence, we introduce a discount factor \u03b3v based on\nrisk scores and connect the virtual edge with probability\nq(j)\nv\n:= \u03b3v\u02c6s(j)\nv . To design the optimal \u03b3v, we propose to\nsolve the following constrained quadratic program:\nmin\n\u03b3\u22650\n\u0012\n\u2212\nX\nv\u2208V\n(rv \u2212\u00afr\u02c6yv)\u03b3v + 1\n2\u2225\u03b3\u22252\n2\n\u0013\n,\n(7)\nwhere the first term encourages virtual edges for high-\nrisk nodes, and the second term is to minimize the num-\nber of virtual edges. The closed-form solution is \u03b3v =\nmax(rv \u2212\u00afr\u02c6yv, 0) (Antoniadis & Fan, 2001), which avoids\nvirtual edges for low-risk nodes as we desire. We now\nsummarize the procedure of BAT in Algorithm 1.\nComplexity Analysis of BAT. BAT with 0th/1st-order es-\ntimation scales linearly with the number of nodes/edges\n(i.e., with O(|V|C) or O(|E|C) complexity). This makes\nBAT highly efficient and allows dynamically graph augmen-\ntation in each training step. Specifically, BAT introduces C\n(the number of class, usally small) virtual nodes with O(n)\nedges. Because of the long-tail distribution of node uncer-\ntainty and the discount factor used to solve Eq. (7), only a\nsmall portion of nodes have positive risks with relatively\nfew (empirically around 1-3%) virtual edges introduced.\nWe provide the scalability results of BAT later in the ex-\nAlgorithm 1 BAT: Topological Balanced Augmentation\nRequire: Class-imbalanced graph G : {A, X};\n1: Initialize: node classifier F(\u00b7; \u0398);\n2: while not converged do\n3:\n\u02c6P \u2190F(A, X; \u0398);\n4:\n\u02c6y \u2190argmaxaxis=1( \u02c6P );\n\u25b7Model predictions \u02c6y\n5:\nr \u2190NodeRiskEst( \u02c6P , \u02c6y);\n\u25b7Eq. (3) - (4)\n6:\n\u02c6S \u2190PosteriorEst(A, \u02c6P , \u02c6y);\n\u25b7Eq. (5) - (6)\n7:\nfor class j = 1 to C do\n8:\nxv\u2217\nj \u2190P\nv\u2208\u02c6Vj xv/|\u02c6Vj|;\n9:\nv\u2217\nj : (xv\u2217\nj , j)\n\u25b7Virtual node v\u2217\nj for class j\n10:\nend for\n11:\nV\u2217\u2190{v\u2217\nj |1 \u2264j \u2264C}\n\u25b7Virtual node set V\u2217\n12:\nQ\u2217\u2190\u02c6S \u2299\u03b3\n\u25b7Virtual link prob. Q\u2217by Eq. (7)\n13:\nE\u2217\u223cQ\u2217;\n\u25b7Sample virtual edges E\u2217w.r.t Q\u2217\n14:\nDerive X\u2217, A\u2217from V \u222aV\u2217, E \u222aE\u2217;\n15:\nUpdate \u0398 with augmented graph G\u2217: {A\u2217, X\u2217};\n16: end while\n17: Return: a balanced node classifier F(A, X; \u0398);\nperiment section. In short, BAT takes milliseconds for a\nsingle topological augmentation. Please refer to Table 3 and\nthe corresponding discussion for further details. We also\ndiscuss how to further speedup BAT in practice in C.2.\n4. Experiments\nWe carry out systematic experiments and analysis to vali-\ndate BAT in the following aspects: (i) Effectiveness in both\npromoting imbalanced node classification and mitigating\nthe prediction bias between different classes. (ii) Versatility\nin cooperating with and further boosting various CIGL tech-\nniques and GNN backbones. (iii) Robustness to extreme\nclass imbalance. (iv) Efficiency in real-world applications.\nExperiment Protocol. We validate BAT on five benchmark\ndatasets for semi-supervised node classification, including\nthe Cora, CiteSeer, PubMed from Plantoid graphs (Sen\net al., 2008), and larger-scale CS, Physics from co-author\nnetworks (Shchur et al., 2018) with high-dimensional fea-\ntures. Following the same setting as prior studies (Park\net al., 2022; Song et al., 2022a; Zhao et al., 2021b), we\nselect half of the classes as minority. The imbalance ratio\n\u03c1 = nmax/nmin \u22651 is the ratio between the size of the\nlargest class to the smallest class, i.e., more imbalance \u21d4\nhigher IR. Detailed data statistics and class distributions\ncan be found in Appendix B.1. We test BAT with six CIGL\ntechniques (Park et al., 2022; Chen et al., 2021; Zhao et al.,\n2021b; Chawla et al., 2002; Japkowicz & Stephen, 2002)\nand three GNN backbones (Veli\u02c7ckovi\u00b4c et al., 2018; Hamilton\net al., 2017; Welling & Kipf, 2016) under all possible combi-\nnations to fully validate BAT\u2019s effectiveness and versatility\nin practice. Note that although there are other techniques\n6\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 1. BAT significantly boosts existing CIGL techniques, achieving better classification performance (Balanced Acc./Marco-F1) with\nreduced bias (PerfStd). For each CIGL baseline, we report its performance before and after collaborating with BAT. The average and the\nbest score of all CIGL methods under three settings (base, +BAT0, +BAT1) are also provided, with performance gain of BAT highlighted\nby \u2206. Due to space limitation, we report the key results and omit the error bar, full results can be found in Appendix D.\nMetric\nBalanced Acc.\u2191\nMacro-F1\u2191\nPerfStd\u2193\nCIGL Baseline\nERM\nRW\nRN\nRS\nSM\nGS\nGE\nAvg (\u2206)\nBest (\u2206)\nAvg (\u2206)\nBest (\u2206)\nAvg (\u2206)\nBest (\u2206)\nCora\nGCN\nBASE\n61.6\n67.7\n66.6\n59.5\n58.3\n68.0\n70.1\n64.5\n70.1\n63.7\n70.0\n25.7\n20.0\n+BAT0\n65.5\n71.0\n71.4\n72.5\n72.2\n68.5\n72.2\n70.5 (+5.9)\n72.5 (+2.4)\n69.2 (+5.5)\n71.6 (+1.7)\n16.7 (-9.0)\n14.4 (-5.6)\n+BAT1\n69.8\n72.1\n71.8\n74.2\n73.9\n71.6\n72.6\n72.3 (+7.8)\n74.2 (+4.1)\n71.1 (+7.5)\n72.8 (+2.9)\n17.6 (-8.0)\n15.2 (-4.8)\nGAT\nBASE\n61.5\n66.9\n66.8\n57.8\n58.8\n64.7\n69.8\n63.8\n69.8\n63.1\n70.0\n26.0\n20.1\n+BAT0\n66.3\n71.8\n72.1\n71.9\n70.5\n69.3\n70.6\n70.4 (+6.6)\n72.1 (+2.4)\n69.0 (+6.0)\n70.9 (+0.9)\n17.2 (-8.9)\n15.1 (-5.0)\n+BAT1\n70.1\n71.6\n70.3\n73.3\n72.2\n71.1\n71.0\n71.4 (+7.6)\n73.3 (+3.5)\n70.2 (+7.1)\n72.3 (+2.4)\n18.0 (-8.0)\n17.3 (-2.8)\nSAGE\nBASE\n59.2\n63.8\n65.3\n57.8\n58.8\n61.6\n68.8\n62.2\n68.8\n60.9\n68.2\n27.1\n19.8\n+BAT0\n66.2\n70.1\n71.3\n71.2\n70.3\n69.9\n69.8\n69.8 (+7.7)\n71.3 (+2.5)\n68.9 (+8.0)\n70.4 (+2.2)\n16.4 (-10.7)\n13.3 (-6.5)\n+BAT1\n66.5\n71.1\n71.5\n73.0\n73.0\n72.3\n71.9\n71.4 (+9.2)\n73.0 (+4.2)\n70.1 (+9.2)\n71.7 (+3.5)\n16.6 (-10.5)\n14.9 (-4.9)\nCiteSeer\nGCN\nBASE\n37.6\n42.5\n42.6\n39.2\n39.3\n45.1\n56.0\n43.2\n56.0\n36.1\n54.5\n27.0\n16.9\n+BAT0\n52.7\n57.9\n57.5\n57.9\n60.1\n57.7\n60.6\n57.8 (+14.6)\n60.6 (+4.6)\n56.9 (+20.8)\n59.9 (+5.4)\n17.6 (-9.4)\n13.8 (-3.1)\n+BAT1\n55.4\n58.4\n59.3\n58.8\n62.0\n57.6\n62.7\n59.2 (+16.0)\n62.7 (+6.7)\n58.4 (+22.3)\n62.5 (+8.0)\n19.3 (-7.7)\n13.9 (-3.0)\nGAT\nBASE\n39.2\n41.3\n43.2\n36.0\n37.0\n41.8\n51.5\n41.4\n51.5\n34.1\n48.3\n29.0\n25.2\n+BAT0\n55.7\n59.3\n58.3\n60.1\n60.6\n56.1\n60.9\n58.7 (+17.3)\n60.9 (+9.4)\n58.1 (+23.9)\n60.0 (+11.7)\n16.3 (-12.6)\n10.7 (-14.5)\n+BAT1\n60.3\n61.2\n59.1\n60.3\n62.4\n57.7\n63.5\n60.6 (+19.2)\n63.5 (+12.0)\n59.9 (+25.8)\n62.5 (+14.2)\n17.7 (-11.3)\n13.2 (-12.0)\nSAGE\nBASE\n43.0\n45.9\n48.6\n39.4\n38.4\n42.2\n52.6\n44.3\n52.6\n37.9\n51.0\n27.1\n19.8\n+BAT0\n55.0\n58.0\n56.3\n61.4\n64.1\n60.9\n64.4\n60.0 (+15.7)\n64.4 (+11.8)\n59.4 (+21.6)\n63.9 (+12.8)\n17.5 (-9.7)\n13.2 (-6.6)\n+BAT1\n53.2\n55.9\n56.5\n61.9\n66.3\n62.3\n63.8\n60.0 (+15.7)\n66.3 (+13.8)\n59.3 (+21.4)\n65.9 (+14.9)\n18.6 (-8.6)\n12.8 (-7.0)\nPubMed\nGCN\nBASE\n64.2\n71.2\n71.5\n65.0\n64.4\n74.0\n73.7\n69.1\n74.0\n63.5\n71.3\n23.2\n11.9\n+BAT0\n68.6\n74.2\n73.2\n72.5\n73.2\n73.1\n76.1\n73.0 (+3.8)\n76.1 (+2.1)\n72.0 (+8.5)\n75.8 (+4.5)\n9.1 (-14.1)\n3.4 (-8.6)\n+BAT1\n67.6\n73.4\n72.5\n72.9\n73.1\n76.6\n76.9\n73.3 (+4.1)\n76.9 (+2.9)\n72.6 (+9.1)\n76.9 (+5.6)\n9.9 (-13.4)\n5.1 (-6.8)\nGAT\nBASE\n65.5\n68.4\n71.2\n65.1\n64.8\n68.7\n73.1\n68.1\n73.1\n62.4\n71.8\n24.8\n10.3\n+BAT0\n73.2\n75.3\n75.6\n73.3\n73.9\n74.7\n74.3\n74.3 (+6.2)\n75.6 (+2.4)\n73.4 (+11.1)\n75.1 (+3.3)\n6.5 (-18.2)\n3.0 (-7.3)\n+BAT1\n74.8\n74.5\n75.2\n73.9\n74.1\n74.4\n75.7\n74.6 (+6.5)\n75.7 (+2.5)\n73.9 (+11.5)\n75.0 (+3.2)\n6.9 (-17.8)\n4.6 (-5.7)\nSAGE\nBASE\n67.6\n68.0\n69.1\n69.2\n65.0\n71.5\n71.4\n68.8\n71.5\n64.5\n70.1\n22.1\n11.8\n+BAT0\n75.3\n74.6\n74.2\n74.9\n74.6\n74.7\n75.9\n74.9 (+6.1)\n75.9 (+4.3)\n74.2 (+9.7)\n75.3 (+5.3)\n7.6 (-14.4)\n3.4 (-8.4)\n+BAT1\n77.4\n75.4\n75.3\n75.8\n77.3\n76.1\n76.5\n76.3 (+7.4)\n77.4 (+5.8)\n75.8 (+11.3)\n76.9 (+6.9)\n6.8 (-15.3)\n4.1 (-7.7)\n*BAT0/BAT1: BAT with 0th/1st-order posterior likelihood estimation. ERM: Empirical Risk Minimization (standard training), RW: Reweight (Japkowicz & Stephen, 2002), RN: ReNode (Chen\net al., 2021), RS: Resampling (Japkowicz & Stephen, 2002), SM: SMOTE (Chawla et al., 2002), GS: GraphSMOTE (Zhao et al., 2021b), GE: GraphENS (Park et al., 2022).\navailable for CIGL (Hong et al., 2021; Kang et al., 2019;\nShi et al., 2020), previous studies (Park et al., 2022; Song\net al., 2022a) have shown they are generally outperformed\nby the baselines we use. Detailed settings can be found\nin Appendix B.2. To ensure a comprehensive evaluation,\nwe employ three metrics to assess both the classification\nperformance (Balanced Accuracy, Macro-F1) and the model\npredictive bias (PerfStd, i.e., the standard deviation of ac-\ncuracy scores across all classes). Lower PerfStd indicates\nsmaller performance gap between all majority and minority\nclasses, and thus smaller predictive bias. For clarity, we use\n\u2191/\u2193to denote larger/smaller is better for each metric.\nBAT significantly boosts various CIGL techniques. We\nreport the main results in Table 1 (IR=10). In all settings\n(3 datasets\u00d73 backbones\u00d77 baselines\u00d73 metrics), BAT\nachieves significant and consistent performance improve-\nments over other CIGL techniques, which also yields new\nstate-of-the-art performance. Specifically: (1) By mitigating\nAMP and DMP, BAT further boosts the best CIGL baseline\nby a large margin, e.g., it boosts the best balanced accuracy\nscore by 4.1/13.8/5.8 on Cora/CiteSeer/PubMed datasets.\n(2) In addition to better classification performance, BAT\nalso greatly reduces the predictive bias in CIGL, with up\nto 10.7/14.5/18.2 average performance deviation reduction\non Cora/CiteSeer/PubMed. (3) Compared with BAT0, BAT1\nachieves better classification performance with first-order\nposterior likelihood estimation. But we also note that BAT0\nperforms better in terms of reducing predictive bias and is\nmore computationally efficient, as we will discuss later in\nthe scalability experiments (Table 3).\nBAT is robust even under extreme class imbalance. We\nfurther extend Table 1 and test BAT\u2019s robustness to varying\ntypes and levels of imbalance, as reported in Table 2. In\nthis experiment, we extend the step imbalance ratio from 10\n(used in Table 1) to 20 to test BAT under even more challeng-\ning class imbalance scenarios. In addition, we consider the\nnatural (long-tail) class imbalance (Park et al., 2022) that is\ncommonly observed in real-world graphs with IR of 50 and\n100. Datasets from (Shchur et al., 2018) (CS, Physics) are\nalso included to test BAT on large-scale tasks. Results show\nthat: (1) BAT is robust to extreme class imbalance, and it\nconsistently boosts the CIGL performance by a significant\nmargin under varying types and levels of imbalance. (2)\nThe performance drop from increasing IR is significantly\nlowered by BAT, i.e., applying BAT improves model\u2019s ro-\nbustness to extreme class imbalance. (3) BAT\u2019s advantage\nis even more prominent under higher class imbalance, e.g.,\non Cora with step IR, the performance gain of applying BAT\non Base raised from 8.2 to 18.5 when IR increased from 10\nto 20, and similar patterns can be observed in other settings.\nBAT effectively alleviates both AMP and DMP. We fur-\n7\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 2. BAT deliver consistent and significant performance gain to CIGL methods under varying types and levels of class imbalance.\nThe numbers in brackets are the performance gain brought about by BAT over Base/BestCIGL method. We report the Balanced Accuracy\nhere, full results with other metrics can be found in Appendix D.3.\nDataset\nCora\nCiteSeer\nPubMed\nCS\nPhysics\nStep IR\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\nBase\n61.6\n52.7\n37.6\n34.2\n64.2\n60.8\n75.4\n65.3\n80.1\n67.7\n+ BAT\n69.8 (+8.2)\n71.3 (+18.5)\n55.4 (+17.7)\n51.3 (+17.1)\n68.6 (+4.4)\n63.3 (+2.5)\n82.6 (+7.2)\n79.9 (+14.5)\n87.6 (+7.5)\n88.0 (+20.2)\nBestCIGL\n70.1\n66.5\n56.0\n47.2\n74.0\n71.1\n84.1\n81.3\n89.4\n85.7\n+ BAT\n74.2 (+4.1)\n71.6 (+5.1)\n62.7 (+6.7)\n62.5 (+15.3)\n76.9 (+2.9)\n75.7 (+4.6)\n86.3 (+2.2)\n85.6 (+4.3)\n91.2 (+1.9)\n90.9 (+5.2)\nNatural IR\n50\n100\n50\n100\n50\n100\n50\n100\n50\n100\nBase\n60.1\n47.0\n28.1\n21.9\n55.1\n46.4\n72.7\n59.2\n80.7\n64.7\n+ BAT\n68.7 (+8.6)\n69.6 (+22.6)\n54.9 (+26.9)\n48.9 (+27.0)\n67.2 (+12.1)\n60.7 (+14.3)\n78.6 (+5.9)\n74.7 (+15.5)\n88.8 (+8.1)\n87.8 (+23.2)\nBestCIGL\n70.0\n66.2\n54.5\n45.0\n71.3\n68.9\n83.9\n80.9\n89.5\n86.2\n+ BAT\n72.8 (+2.9)\n70.2 (+4.0)\n62.5 (+8.0)\n62.1 (+17.1)\n76.9 (+5.6)\n74.9 (+6.0)\n85.4 (+1.6)\n84.6 (+3.7)\n90.7 (+1.2)\n90.0 (+3.8)\n*Base: vanilla GCN model; Base+BAT: applying BAT without any other CIGL method; BestCIGL: best CIGL baseline w/o BAT; BestCIGL+BAT: best CIGL baseline w/ BAT;\nther design experiments to verify to what extent BAT can\neffectively handle the topological challenges identified in\nthis paper, i.e., ambivalent and distant message-passing.\nSpecifically, we investigate whether BAT can improve the\nprediction accuracy of minority class nodes that are highly\ninfluenced by AMP/DMP, i.e., with high heterophilic neigh-\nbor ratio/long distance to supervision. Results are shown in\nFig. 6 (5 independent runs with GCN classifier, IR=10). As\ncan be observed, BAT effectively alleviates the negative im-\npact of AMP and DMP and helps node classifiers to achieve\nbetter performance in minority classes.\n(a) BAT mitigates AMP in multiple CIGL tasks.\n(b) BAT mitigates DMP in multiple CIGL tasks.\nFigure 6. BAT effectively alleviates both AMP and DMP.\nBAT is computationally efficient. As previously discussed\nin the complexity analysis, BAT with 0th/1st-order estima-\ntion scales linearly with the number of nodes/edges (i.e.,\nwith O(|V|C) or O(|E|C) complexity). Since all the opera-\ntions can be executed in parallel in matrix form, BAT0/BAT1\nhas O( |V|C\nD )/O( |E|C\nD ) time complexity, where D is the num-\nber of available computational units and is usually large\nfor modern GPUs.\nTable 3 reports the ratio of virtual\nnodes/edges to the original graph introduced and the running\ntime of BAT. It can be observed that BAT only introduces a\nsmall number of virtual nodes/edges, and is highly efficient\n(taking milliseconds for augmentation) in practice.\nTable 3. Efficiency results of BAT0/BAT1.\nDataset\n\u2206Nodes (%)\n\u2206Edges (%)\n\u2206Time (ms)\nCora\n0.258%\n2.842%/1.509%\n4.50/4.65ms\nCiteSeer\n0.180%\n3.715%/1.081%\n4.72/4.97ms\nPubMed\n0.015%\n3.175%/1.464%\n6.23/6.64ms\nCS\n0.082%\n1.395%/1.053%\n16.97/18.61ms\nPhysics\n0.014%\n0.797%/0.527%\n30.68/31.91ms\n* Results obtained on an NVIDIA\u00ae Tesla V100 32GB GPU.\nFurther discussions. We refer the readers to Appendix for\nreproducibility details (\u00a7B), ablation study and extended\ndiscussions (\u00a7C), and additional empirical results (\u00a7D).\n5. Related Works\nImbalanced graph learning. Class imbalance is ubiquitous\nin many machine-learning tasks and has been extensively\nstudied (He & Garcia, 2008; Krawczyk, 2016). However,\nmost of the existing works focus on i.i.d. scenarios, which\nmay not be tailored to the unique characteristics of graph\ndata. To handle imbalanced graph learning, several tech-\nniques have been proposed in recent studies (e.g., by adver-\nsarial training (Shi et al., 2020; Qu et al., 2021), designing\nnew GNN architectures (Wang et al., 2020; Liu et al., 2021)\nor loss functions (Song et al., 2022a)), we review the most\nclosely related model-agnostic CR methods here. One of\nthe early works GraphSMOTE (Zhao et al., 2021b) adopts\nSMOTE (Chawla et al., 2002) oversampling in the node\nembedding space to synthesize minority nodes and comple-\nments the topology with a learnable edge predictor. A more\nrecent work GraphENS (Park et al., 2022) synthesizes the\nego network through saliency-based ego network mixing to\nhandle the neighbor-overfitting problem. Most studies are\nrooted in a class-rebalancing perspective and address the\nimbalance by node/class-wise reweighting or resampling.\nTopology-imbalance in graphs. Topology imbalance is\n8\nClass-Imbalanced Graph Learning without Class Rebalancing\nfirstly discussed in Chen et al. (2021). They found that \u201cthe\nunequal structure role of labeled nodes\u201d can cause influence\nconflict, and propose to re-weight the labeled nodes based on\na conflict detection measure. Other works further discussed\nhow to better address the issue via position-aware structure\nlearning (Sun et al., 2022), and handle topology-imbalance\nin fake news detection (Gao et al., 2022) and bankruptcy pre-\ndiction (Liu et al., 2023). These studies discussed concepts\nrelated to \u201cinfluence conflict/insufficiency\u201d (Chen et al.,\n2021), which motivated us to investigate AMP/DMP in this\nwork. It is worth noting that AMP/DMP coefficients are\ndefined on all nodes based on the k-hop local neighborhood,\nwhile the influence conflict measure in Chen et al. (2021)\nis defined only on labeled nodes by global personalized\nPageRank score, and influence insufficient has no formal\ndefinition. Therefore, we did not inherit their naming and\nemploy AMP/DMP to distinguish and avoid confusion with\nthe existing concepts. We theoretically investigate the role\nof AMP/DMP in shaping class-imbalance bias, and pro-\npose an augmentation technique BAT tailored for handling\nAMP and DMP in CIGL. Results show that the topology\nimbalance algorithm can also significantly boosted by BAT.\nHeterophilic graph/long-distance propagation. Numer-\nous studies exist in the literature concerning learning from\nheterophilic graphs (Yan et al., 2024; Xu et al., 2023)\nand employing multi-hop propagation (Zhao et al., 2021a;\nFu et al., 2024). In particular, heterophilic GNNs often\ncombine intermediate representations to derive more re-\nfined structure-aware features (Zhu et al., 2020).\nThe\nGPRGNN (Chien et al., 2020) takes a step further by intro-\nducing learnable weights to adaptively combine representa-\ntions from each layer. Meanwhile, in multi-hop propagation,\nAPPNP (Gasteiger et al., 2018) stands as a representative\ntechnique that leverages personalized PageRank for extract-\ning information from a broader neighborhood. Nevertheless,\nthese works focus on addressing global graph heterophily\nand long-distance propagation by modifying the GNN ar-\nchitecture or aggregation operators. They are not tailored to\naddress class imbalance and cannot readily handle the AMP\nand DMP. We refer the readers to Appendix D where we\nshow that BAT can also significantly boost the performance\nof such GNNs (Chien et al., 2020; Gasteiger et al., 2018) in\nvarious CIGL tasks.\n6. Conclusion\nIn this paper, we study class-imbalanced graph learning\nfrom a novel topological perspective. We theoretically re-\nveal that two fundamental topological phenomena, i.e., am-\nbivalent and distant message-passing, can greatly exacerbate\nthe predictive bias stemming from class imbalance. Our\nfindings reveal an unexplored avenue that limits the perfor-\nmance of existing class-rebalancing-based CIGL techniques.\nIn light of this, we propose BAT to handle the topological\nchallenges in CIGL by dynamic topological augmentation.\nBAT is a swift and model-agnostic framework that can seam-\nlessly complement other CIGL techniques, augmenting their\nperformance and mitigating predictive bias. Systematic ex-\nperiments validate BAT\u2019s superior effectiveness, versatility,\nrobustness, and efficiency across various CIGL tasks.\nAcknowledgements\nThis work is supported by NSF (1947135), the NSF Program\non Fairness in AI in collaboration with Amazon (1939725),\nNIFA (2020-67021-32799), DHS (17STQAC00001-07-00),\nthe C3.ai Digital Transformation Institute, MIT-IBM Wat-\nson AI Lab, and IBM-Illinois Discovery Accelerator In-\nstitute. The content of the information in this document\ndoes not necessarily reflect the position or the policy of\nthe Government or Amazon, and no official endorsement\nshould be inferred. The U.S. Government is authorized to\nreproduce and distribute reprints for Government purposes\nnotwithstanding any copyright notation here on.\nImpact Statements\nThis paper presents work whose goal is to advance the field\nof Graph Data Mining. There are many potential societal\nconsequences of our work, none which we feel must be\nspecifically highlighted here.\nReferences\nAkoglu, L., Tong, H., and Koutra, D. Graph based anomaly\ndetection and description: a survey. Data mining and\nknowledge discovery, 29(3):626\u2013688, 2015.\nAntoniadis, A. and Fan, J. Regularization of wavelet approx-\nimations. Journal of the American Statistical Association,\n96(455):939\u2013967, 2001.\nBojchevski, A. and G\u00a8unnemann, S. Deep gaussian em-\nbedding of graphs: Unsupervised inductive learning via\nranking. arXiv preprint arXiv:1707.03815, 2017.\nCai, L., Li, J., Wang, J., and Ji, S. Line graph neural net-\nworks for link prediction. IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 44(9):5103\u20135113,\n2021.\nChawla, N. V., Bowyer, K. W., Hall, L. O., and Kegelmeyer,\nW. P. Smote: synthetic minority over-sampling technique.\nJournal of artificial intelligence research, 16:321\u2013357,\n2002.\nChen, D., Lin, Y., Zhao, G., Ren, X., Li, P., Zhou, J.,\nand Sun, X.\nTopology-imbalance learning for semi-\nsupervised node classification. Advances in Neural Infor-\nmation Processing Systems, 34:29885\u201329897, 2021.\n9\nClass-Imbalanced Graph Learning without Class Rebalancing\nChien, E., Peng, J., Li, P., and Milenkovic, O. Adaptive uni-\nversal generalized pagerank graph neural network. arXiv\npreprint arXiv:2006.07988, 2020.\nCui, Y., Jia, M., Lin, T.-Y., Song, Y., and Belongie, S. Class-\nbalanced loss based on effective number of samples. In\nProceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, pp. 9268\u20139277, 2019.\nDecelle, A., Krzakala, F., Moore, C., and Zdeborov\u00b4a, L.\nAsymptotic analysis of the stochastic block model for\nmodular networks and its algorithmic applications. Phys-\nical Review E, 84(6):066106, 2011.\nFey, M. and Lenssen, J. E. Fast graph representation learning\nwith pytorch geometric. arXiv preprint arXiv:1903.02428,\n2019.\nFu, D. and He, J. SDG: A simplified and dynamic graph\nneural network. In Diaz, F., Shah, C., Suel, T., Castells,\nP., Jones, R., and Sakai, T. (eds.), SIGIR \u201921: The 44th\nInternational ACM SIGIR Conference on Research and\nDevelopment in Information Retrieval, Virtual Event,\nCanada, July 11-15, 2021, pp. 2273\u20132277. ACM, 2021.\ndoi: 10.1145/3404835.3463059. URL https://doi.\norg/10.1145/3404835.3463059.\nFu, D., Zhou, D., Maciejewski, R., Croitoru, A., Boyd,\nM., and He, J. Fairness-aware clique-preserving spec-\ntral clustering of temporal graphs. In Ding, Y., Tang,\nJ., Sequeda, J. F., Aroyo, L., Castillo, C., and Houben,\nG. (eds.), Proceedings of the ACM Web Conference\n2023, WWW 2023, Austin, TX, USA, 30 April 2023 -\n4 May 2023, pp. 3755\u20133765. ACM, 2023.\ndoi: 10.\n1145/3543507.3583423. URL https://doi.org/\n10.1145/3543507.3583423.\nFu, D., Hua, Z., Xie, Y., Fang, J., Zhang, S., Sancak, K., Wu,\nH., Malevich, A., He, J., and Long, B. Vcr-graphormer:\nA mini-batch graph transformer via virtual connections.\nCoRR, abs/2403.16030, 2024. doi: 10.48550/ARXIV.\n2403.16030. URL https://doi.org/10.48550/\narXiv.2403.16030.\nGao, L., Song, L., Liu, J., Chen, B., and Shang, X. Topol-\nogy imbalance and relation inauthenticity aware hierar-\nchical graph attention networks for fake news detection.\nIn Proceedings of the 29th International Conference on\nComputational Linguistics, pp. 4687\u20134696, 2022.\nGasteiger, J., Bojchevski, A., and G\u00a8unnemann, S. Predict\nthen propagate: Graph neural networks meet personalized\npagerank. arXiv preprint arXiv:1810.05997, 2018.\nHamilton, W., Ying, Z., and Leskovec, J. Inductive repre-\nsentation learning on large graphs. Advances in neural\ninformation processing systems, 30, 2017.\nHasanzadeh, A., Hajiramezanali, E., Boluki, S., Zhou, M.,\nDuffield, N., Narayanan, K., and Qian, X. Bayesian graph\nneural networks with adaptive connection sampling. In\nInternational conference on machine learning, pp. 4094\u2013\n4104. PMLR, 2020.\nHe, H. and Garcia, E. A. Learning from imbalanced data.\nIEEE Transactions on Knowledge & Data Engineering,\n(9):1263\u20131284, 2008.\nHolland, P. W., Laskey, K. B., and Leinhardt, S. Stochastic\nblockmodels: First steps. Social networks, 5(2):109\u2013137,\n1983.\nHong, Y., Han, S., Choi, K., Seo, S., Kim, B., and Chang,\nB. Disentangling label distribution for long-tailed visual\nrecognition. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pp. 6626\u2013\n6636, 2021.\nHu, W., Fey, M., Zitnik, M., Dong, Y., Ren, H., Liu, B.,\nCatasta, M., and Leskovec, J. Open graph benchmark:\nDatasets for machine learning on graphs. Advances in\nneural information processing systems, 33:22118\u201322133,\n2020.\nHuang, Z., Tang, Y., and Chen, Y. A graph neural network-\nbased node classification model on class-imbalanced\ngraph data.\nKnowledge-Based Systems, 244:108538,\n2022.\nJapkowicz, N. and Stephen, S. The class imbalance problem:\nA systematic study. Intelligent data analysis, 6(5):429\u2013\n449, 2002.\nJohnson, J. M. and Khoshgoftaar, T. M. Survey on deep\nlearning with class imbalance. Journal of Big Data, 6(1):\n1\u201354, 2019.\nKang, B., Xie, S., Rohrbach, M., Yan, Z., Gordo, A.,\nFeng, J., and Kalantidis, Y. Decoupling representation\nand classifier for long-tailed recognition. arXiv preprint\narXiv:1910.09217, 2019.\nKang, J., He, J., Maciejewski, R., and Tong, H. Inform:\nIndividual fairness on graph mining.\nIn Proceedings\nof the 26th ACM SIGKDD international conference on\nknowledge discovery & data mining, pp. 379\u2013389, 2020.\nKang, J., Zhou, Q., and Tong, H. Jurygcn: quantifying\njackknife uncertainty on graph convolutional networks.\nIn Proceedings of the 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, pp. 742\u2013752,\n2022a.\nKang, J., Zhu, Y., Xia, Y., Luo, J., and Tong, H. Rawlsgcn:\nTowards rawlsian difference principle on graph convolu-\ntional network. In Proceedings of the ACM Web Confer-\nence 2022, pp. 1214\u20131225, 2022b.\n10\nClass-Imbalanced Graph Learning without Class Rebalancing\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. arXiv preprint arXiv:1412.6980, 2014.\nKrawczyk, B. Learning from imbalanced data: open chal-\nlenges and future directions. Progress in Artificial Intelli-\ngence, 5(4):221\u2013232, 2016.\nLi, X., Wen, L., Deng, Y., Feng, F., Hu, X., Wang, L., and\nFan, Z. Graph neural network with curriculum learn-\ning for imbalanced node classification. arXiv preprint\narXiv:2202.02529, 2022.\nLiu, Y., Ao, X., Qin, Z., Chi, J., Feng, J., Yang, H., and He,\nQ. Pick and choose: a gnn-based imbalanced learning\napproach for fraud detection. In Proceedings of the Web\nConference 2021, pp. 3168\u20133177, 2021.\nLiu, Y., Gao, Z., Liu, X., Luo, P., Yang, Y., and Xiong,\nH. Qtiah-gnn: Quantity and topology imbalance-aware\nheterogeneous graph neural network for bankruptcy pre-\ndiction. In Proceedings of the 29th ACM SIGKDD Con-\nference on Knowledge Discovery and Data Mining, pp.\n1572\u20131582, 2023.\nLiu, Z.-Y., Li, S.-Y., Chen, S., Hu, Y., and Huang, S.-\nJ. Uncertainty aware graph gaussian process for semi-\nsupervised learning. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, volume 34, pp. 4957\u20134964,\n2020.\nMa, Y., Liu, X., Zhao, T., Liu, Y., Tang, J., and Shah, N. A\nunified view on graph neural networks as graph signal\ndenoising. In Proceedings of the 30th ACM International\nConference on Information & Knowledge Management,\npp. 1202\u20131211, 2021.\nNt, H. and Maehara, T.\nRevisiting graph neural net-\nworks: All we have is low-pass filters. arXiv preprint\narXiv:1905.09550, 2019.\nPandey, B., Bhanodia, P. K., Khamparia, A., and Pandey,\nD. K. A comprehensive survey of edge prediction in\nsocial networks: Techniques, parameters and challenges.\nExpert Systems with Applications, 124:164\u2013181, 2019.\nPark, J., Song, J., and Yang, E. Graphens: Neighbor-aware\nego network synthesis for class-imbalanced node classifi-\ncation. In International Conference on Learning Repre-\nsentations, 2022.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., et al. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information\nprocessing systems, 32, 2019.\nQu, L., Zhu, H., Zheng, R., Shi, Y., and Yin, H. Imgagn:\nImbalanced network embedding via generative adversar-\nial graph networks. In Proceedings of the 27th ACM\nSIGKDD Conference on Knowledge Discovery & Data\nMining, pp. 1390\u20131398, 2021.\nSen, P., Namata, G., Bilgic, M., Getoor, L., Galligher, B.,\nand Eliassi-Rad, T. Collective classification in network\ndata. AI magazine, 29(3):93\u201393, 2008.\nShchur, O., Mumme, M., Bojchevski, A., and G\u00a8unnemann,\nS. Pitfalls of graph neural network evaluation. arXiv\npreprint arXiv:1811.05868, 2018.\nShi, M., Tang, Y., Zhu, X., Wilson, D., and Liu, J. Multi-\nclass imbalanced graph convolutional network learning.\nIn Proceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence (IJCAI-20), 2020.\nSong, J., Park, J., and Yang, E. Tam: Topology-aware mar-\ngin loss for class-imbalanced node classification. In In-\nternational Conference on Machine Learning, pp. 20369\u2013\n20383. PMLR, 2022a.\nSong, Z., Yang, X., Xu, Z., and King, I. Graph-based semi-\nsupervised learning: A comprehensive review. IEEE\nTransactions on Neural Networks and Learning Systems,\n2022b.\nSrivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,\nand Salakhutdinov, R. Dropout: a simple way to prevent\nneural networks from overfitting. The journal of machine\nlearning research, 15(1):1929\u20131958, 2014.\nStadler, M., Charpentier, B., Geisler, S., Z\u00a8ugner, D., and\nG\u00a8unnemann, S. Graph posterior network: Bayesian pre-\ndictive uncertainty for node classification.\nAdvances\nin Neural Information Processing Systems, 34:18033\u2013\n18048, 2021.\nSun, Q., Li, J., Yuan, H., Fu, X., Peng, H., Ji, C., Li, Q.,\nand Yu, P. S. Position-aware structure learning for graph\ntopology-imbalance by relieving under-reaching and over-\nsquashing. In Proceedings of the 31st ACM International\nConference on Information & Knowledge Management,\npp. 1848\u20131857, 2022.\nTang, L. and Liu, H. Community detection and mining\nin social media. Synthesis lectures on data mining and\nknowledge discovery, 2(1):1\u2013137, 2010.\nVeli\u02c7ckovi\u00b4c, P., Cucurull, G., Casanova, A., Romero, A.,\nLi`o, P., and Bengio, Y. Graph attention networks. In\nInternational Conference on Learning Representations,\n2018.\nWang, Z., Ye, X., Wang, C., Cui, J., and Philip, S. Y.\nNetwork embedding with completely-imbalanced labels.\n11\nClass-Imbalanced Graph Learning without Class Rebalancing\nIEEE Transactions on Knowledge and Data Engineering,\n33(11):3634\u20133647, 2020.\nWelling, M. and Kipf, T. N. Semi-supervised classification\nwith graph convolutional networks. In J. International\nConference on Learning Representations (ICLR 2017),\n2016.\nWu, F., Souza, A., Zhang, T., Fifty, C., Yu, T., and Wein-\nberger, K. Simplifying graph convolutional networks. In\nInternational conference on machine learning, pp. 6861\u2013\n6871. PMLR, 2019.\nWu, L., Xia, J., Gao, Z., Lin, H., Tan, C., and Li, S. Z. Graph-\nmixup: Improving class-imbalanced node classification\nby reinforcement mixup and self-supervised context pre-\ndiction. In Joint European Conference on Machine Learn-\ning and Knowledge Discovery in Databases, pp. 519\u2013535.\nSpringer, 2022.\nXu, Z., Chen, Y., Zhou, Q., Wu, Y., Pan, M., Yang, H.,\nand Tong, H. Node classification beyond homophily:\nTowards a general solution. In Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and\nData Mining, pp. 2862\u20132873, 2023.\nYan, Y., Liu, L., Ban, Y., Jing, B., and Tong, H. Dynamic\nknowledge graph alignment. In Proceedings of the AAAI\nconference on artificial intelligence, volume 35, pp. 4564\u2013\n4572, 2021a.\nYan, Y., Zhang, S., and Tong, H. Bright: A bridging algo-\nrithm for network alignment. In Proceedings of the web\nconference 2021, pp. 3907\u20133917, 2021b.\nYan, Y., Chen, Y., Chen, H., Xu, M., Das, M., Yang, H.,\nand Tong, H. From trainable negative depth to edge\nheterophily in graphs. Advances in Neural Information\nProcessing Systems, 36, 2024.\nYun, S., Kim, K., Yoon, K., and Park, C. Lte4g: Long-tail\nexperts for graph neural networks. In Proceedings of the\n31st ACM International Conference on Information &\nKnowledge Management, pp. 2434\u20132443, 2022.\nZeng, Z., Zhang, S., Xia, Y., and Tong, H. Parrot: Position-\naware regularized optimal transport for network align-\nment. In Proceedings of the ACM Web Conference 2023,\npp. 372\u2013382, 2023a.\nZeng, Z., Zhu, R., Xia, Y., Zeng, H., and Tong, H. Gen-\nerative graph dictionary learning. In International Con-\nference on Machine Learning, pp. 40749\u201340769. PMLR,\n2023b.\nZeng, Z., Du, B., Zhang, S., Xia, Y., Liu, Z., and Tong,\nH. Hierarchical multi-marginal optimal transport for net-\nwork alignment. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 38, pp. 16660\u201316668,\n2024.\nZhang, Y., Pal, S., Coates, M., and Ustebay, D. Bayesian\ngraph convolutional neural networks for semi-supervised\nclassification. In Proceedings of the AAAI conference on\nartificial intelligence, volume 33, pp. 5829\u20135836, 2019.\nZhao, J., Dong, Y., Ding, M., Kharlamov, E., and Tang, J.\nAdaptive diffusion in graph neural networks. Advances\nin Neural Information Processing Systems, 34:23321\u2013\n23333, 2021a.\nZhao, T., Zhang, X., and Wang, S. Graphsmote: Imbalanced\nnode classification on graphs with graph neural networks.\nIn Proceedings of the 14th ACM international conference\non web search and data mining, pp. 833\u2013841, 2021b.\nZhao, T., Luo, D., Zhang, X., and Wang, S. Topoimb:\nToward topology-level imbalance in learning from graphs.\nIn Learning on Graphs Conference, pp. 37\u20131. PMLR,\n2022.\nZhao, X., Chen, F., Hu, S., and Cho, J.-H. Uncertainty\naware semi-supervised learning on graph data. Advances\nin Neural Information Processing Systems, 33:12827\u2013\n12836, 2020.\nZhu, J., Yan, Y., Zhao, L., Heimann, M., Akoglu, L., and\nKoutra, D. Beyond homophily in graph neural networks:\nCurrent limitations and effective designs. Advances in\nNeural Information Processing Systems, 33:7793\u20137804,\n2020.\n12\nClass-Imbalanced Graph Learning without Class Rebalancing\nAppendix\n\u2022 Section A: Proofs of Theoretical Results\n\u2013 A.1 - Limiting the distribution of Hk\nij.\n\u2013 A.2 - Proof of Theorem 2.1 (AMP).\n\u2013 A.3 - Proof of Theorem 2.2 (DMP).\n\u2022 Section B: Reproducibility Details\n\u2013 B.1 - Statistics of the used datasets.\n\u2013 B.2 - Implementation details of baselines.\n\u2013 B.3 - Evaluation protocols.\n\u2022 Section C: Further Discussions\n\u2013 C.1 - Ablation study of BAT.\n\u2013 C.2 - Further speedup of BAT in practice.\n\u2013 C.3 - Remarks on choosing between BAT0/BAT1.\n\u2013 C.4 - Limitation and future works.\n\u2022 Section D: Additional Experiments, Results, and Analysis\n\u2013 D.1 - Experiments on additional large-scale graphs\n\u2013 D.2 - Comparison with additional independent CIGL baselines.\n\u2013 D.3 - Full results with all metrics, error bar, and additional GNN backbones.\nA. Proofs of Theoretical Results\nDefine random variables Hk\nij := |Vj \u2229H(u, k)| denote the number of class-j k-hop homo-connected neighbors of a node\nu \u2208Vi. Note that the results of both Theorems 2.1 & 2.2 depend only on the distributions of Hk\nij. Thus, we will first derive\nthe limiting distributions of Hk\nij as a technical lemma, and then give the proofs of Theorems 2.1 & 2.2.\nA.1. Limiting Distributions of Hk\nij\nTo count the number of homo-connected neighbors, consider the breadth-first search (BFS) tree rooted at node u \u2208V1. By\nenumerating the numbers of 1, . . . , k-hop homo-connected neighbors in the BFS tree respectively, we can calculate the\nexact joint distribution of (Hk\n11, Hk\n12):\nP{Hk\n11 = s, Hk\n12 = s\u2032} =\nX\na1+\u00b7\u00b7\u00b7+ak=s\nb1+\u00b7\u00b7\u00b7+bk=s\u2032\n\u0012\nn1 \u22121\na1, . . . , ak, n1 \u22121 \u2212s\n\u0013\u0012\nn2\nb1, . . . , bk, n2 \u2212s\u2032\n\u0013\npa1\n\u0012\nk\nY\nt=2\n(1 \u2212p)at(1+a1+\u00b7\u00b7\u00b7+at\u22122)(1 \u2212(1 \u2212p)at\u22121)at\n\u0013\n(1 \u2212p)(n1\u22121\u2212s)(1+s\u2212ak)\nqb1\n\u0012\nk\nY\nt=2\n(1 \u2212q)bt(1 \u2212p)bt(b1+\u00b7\u00b7\u00b7+bt\u22122)(1 \u2212(1 \u2212p)bt\u22121)bt\n\u0013\n(1 \u2212q)n2\u2212s\u2032(1 \u2212p)(n2\u2212s\u2032)(s\u2032\u2212bk).\nThus, Hk\n11 and Hk\n12 are independent, and their marginal distributions are:\nP{Hk\n11 = s} =\nX\na1+\u00b7\u00b7\u00b7+ak=s\n\u0012\nn1 \u22121\na1, . . . , ak, n1 \u22121 \u2212s\n\u0013\npa1\n\u0012\nk\nY\nt=2\n(1 \u2212p)at(1+a1+\u00b7\u00b7\u00b7+at\u22122)(1 \u2212(1 \u2212p)at\u22121)at\n\u0013\n(1 \u2212p)(n1\u22121\u2212s)(1+s\u2212ak),\nP{Hk\n12 = s} =\nX\nb1+\u00b7\u00b7\u00b7+bk=s\n\u0012\nn2\nb1, . . . , bk, n2 \u2212s\n\u0013\nqb1\n\u0012\nk\nY\nt=2\n(1 \u2212q)bt(1 \u2212p)bt(b1+\u00b7\u00b7\u00b7+bt\u22122)(1 \u2212(1 \u2212p)bt\u22121)bt\n\u0013\n(1 \u2212q)n2\u2212s(1 \u2212p)(n2\u2212s)(s\u2212bk).\n13\nClass-Imbalanced Graph Learning without Class Rebalancing\nNow consider n \u2192\u221e. Let\n\u03b211 := lim\nn\u2192\u221en1 \u00b7 p = \u03b2,\n\u03b222 := lim\nn\u2192\u221en2 \u00b7 p = \u03c1\u03b2,\n\u03b221 := lim\nn\u2192\u221en1 \u00b7 q = \u03b2 q\np,\n\u03b212 := lim\nn\u2192\u221en2 \u00b7 q = \u03c1\u03b2 q\np.\nThen, the limiting distributions of Hk\n11 and Hk\n12 are:\nP{Hk\n11 = s} \u223c\nX\na1+\u00b7\u00b7\u00b7+ak=s\nns\n1\na1! \u00b7 \u00b7 \u00b7 ak!pa1\n\u0012\nk\nY\nt=2\n(at\u22121p)at\n\u0013\n(1 \u2212p)n1(1+s\u2212ak)\n\u2192e\u2212\u03b211\nX\na1+\u00b7\u00b7\u00b7+ak=s\n(\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n,\nP{Hk\n12 = s} \u223c\nX\nb1+\u00b7\u00b7\u00b7+bk=s\nns\n2\nb1! \u00b7 \u00b7 \u00b7 bk!qb1\n\u0012\nk\nY\nt=2\n(bt\u22121p)bt\n\u0013\n(1 \u2212q)n2(1 \u2212p)n2(s\u2212bk)\n\u2192e\u2212\u03b212\nX\nb1+\u00b7\u00b7\u00b7+bk=s\n(\u03b212e\u2212\u03b222)b1\nb1!\n\u0012 k\u22121\nY\nt=2\n(bt\u22121\u03b222e\u2212\u03b222)bt\nbt!\n\u0013(bk\u22121\u03b222)bk\nbk!\n.\nFigure 7 shows that the limiting distributions are good approximation for finite n.\nA.2. Proof of Theorem 2.1\nNote that for any t\u2032 = 1, . . . , k,\ne\u2212\u03b211\n\u221e\nX\na1=0\n\u00b7 \u00b7 \u00b7\n\u221e\nX\nak=0\nat\u2032 \u00b7 (\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n= \u03b2t\u2032\n11.\nThus,\nlim\nn\u2192\u221eE[Hk\n11] =\n\u221e\nX\ns=0\ns \u00b7 lim\nn\u2192\u221eP{Hk\n11 = s}\n=\n\u221e\nX\ns=0\ns \u00b7 e\u2212\u03b211\nX\na1+\u00b7\u00b7\u00b7+ak=s\n(\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n=\n\u221e\nX\ns=0\ne\u2212\u03b211\nX\na1+\u00b7\u00b7\u00b7+ak=s\ns \u00b7 (\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n= e\u2212\u03b211\n\u221e\nX\na1=0\n\u00b7 \u00b7 \u00b7\n\u221e\nX\nak=0\n(a1 + \u00b7 \u00b7 \u00b7 + ak) \u00b7 (\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n=\nk\nX\nt\u2032=1\ne\u2212\u03b211\n\u221e\nX\na1=0\n\u00b7 \u00b7 \u00b7\n\u221e\nX\nak=0\nat\u2032 \u00b7 (\u03b211e\u2212\u03b211)a1\na1!\n\u0012 k\u22121\nY\nt=2\n(at\u22121\u03b211e\u2212\u03b211)at\nat!\n\u0013(ak\u22121\u03b211)ak\nak!\n=\nk\nX\nt\u2032=1\n\u03b2t\u2032\n11.\n14\nClass-Imbalanced Graph Learning without Class Rebalancing\n0\n10\n20\n30\n40\nH11\n0.00\n0.01\n0.02\n0.03\n0.04\n0.05\nProbability\nLimiting\nEmpirical\n100\n200\n300\n400\nH22\n0.000\n0.002\n0.004\n0.006\n0.008\n0.010\n0.012\nProbability\nLimiting\nEmpirical\n0\n5\n10\n15\n20\n25\n30\nH21\n0.0\n0.1\n0.2\n0.3\n0.4\nProbability\nLimiting\nEmpirical\n0\n25\n50\n75\n100\n125\n150\nH12\n0.000\n0.005\n0.010\n0.015\n0.020\n0.025\n0.030\n0.035\n0.040\nProbability\nLimiting\nEmpirical\nFigure 7. Distributions of Hk\nij. Simulated with n = 2000, \u03c1 = 4, p = 0.01, q = 0.002, k = 2.\nSimilarly,\nlim\nn\u2192\u221eE[Hk\n22] =\nk\nX\nt=1\n\u03b2t\n22,\nlim\nn\u2192\u221eE[Hk\n12] =\nk\nX\nt=1\n\u03b212\u03b2t\u22121\n22 ,\nlim\nn\u2192\u221eE[Hk\n21] =\nk\nX\nt=1\n\u03b221\u03b2t\u22121\n11 .\nIt follows that\nlim\nn\u2192\u221e\n\u03b1k\n1\n\u03b1k\n2\n= lim\nn\u2192\u221e\nE[Hk\n12]/E[Hk\n11]\nE[Hk\n21]/E[Hk\n22]\n=\nPk\nt=1 \u03b212\u03b2t\u22121\n22\n\u000e Pk\nt=1 \u03b2t\n11\nPk\nt=1 \u03b221\u03b2t\u22121\n11\n\u000e Pk\nt=1 \u03b2t\n22\n= \u03b212\u03b222\n\u03b221\u03b211\n\u00b7\n\u0000 Pk\nt=1 \u03b2t\u22121\n22\n\u00012\n\u0000 Pk\nt=1 \u03b2t\u22121\n11\n\u00012\n=\n\u0012\n\u03c1 \u00b7\nPk\nt=1(\u03c1\u03b2)t\u22121\nPk\nt=1 \u03b2t\u22121\n\u00132\n.\n15\nClass-Imbalanced Graph Learning without Class Rebalancing\nA.3. Proof of Theorem 2.2\nFor k = 2, note the identity:\n\u221e\nX\ns=0\ns\nX\na=0\n\u03bba(\u00b5a)s\u2212a\na!(s \u2212a)! =\n\u221e\nX\na=0\n\u03bba\na!\n\u221e\nX\nb=0\n(\u00b5a)b\nb!\n=\n\u221e\nX\na=0\n\u03bbae\u00b5a\na!\n= e\u03bbe\u00b5.\nIt follows that (with \u03bb = (1 \u2212rL\n1)\u03b211e\u2212\u03b211 and \u00b5 = (1 \u2212rL\n1)\u03b211)\nlim\nn\u2192\u221eE[(1 \u2212rL\n1)H2\n11] =\n\u221e\nX\ns=0\n(1 \u2212rL\n1)s \u00b7 lim\nn\u2192\u221eP{H2\n11 = s}\n=\n\u221e\nX\ns=0\n(1 \u2212rL\n1)s \u00b7 e\u2212\u03b211\ns\nX\na=0\n(\u03b211e\u2212\u03b211)a(\u03b211a)s\u2212a\na!(s \u2212a)!\n= e\u2212\u03b211\n\u221e\nX\ns=0\ns\nX\na=0\n((1 \u2212rL\n1)\u03b211e\u2212\u03b211)a((1 \u2212rL\n1)\u03b211a)s\u2212a\na!(s \u2212a)!\n= e\u2212\u03b211e(1\u2212rL\n1)\u03b211e\u2212\u03b211e(1\u2212rL\n1)\u03b211\n= e\u2212(1\u2212(1\u2212rL\n1)e\u2212rL\n1\u03b211)\u03b211\n\u2248e\u2212\u03b211.\nFor k = 3, similarly,\nlim\nn\u2192\u221eE[(1 \u2212rL\n1)H3\n11] = e\u2212\n\u00001\u2212(1\u2212rL\n1)\u03b211e\n\u2212\n\u00001\u2212(1\u2212rL\n1)\u03b211e\u2212rL\n1\u03b211\u0001\n\u03b211\u0001\n\u03b211 \u2248e\u2212\u03b211.\nIn general, the result for k has k nested exponentiations, but we still have:\nlim\nn\u2192\u221eE[(1 \u2212rL\n1)Hk\n11] \u2248e\u2212\u03b211.\nSimilarly,\nlim\nn\u2192\u221eE[(1 \u2212rL\n2)Hk\n12] \u2248e\u2212\u03b212,\nlim\nn\u2192\u221eE[(1 \u2212rL\n2)Hk\n22] \u2248e\u2212\u03b222,\nlim\nn\u2192\u221eE[(1 \u2212rL\n1)Hk\n21] \u2248e\u2212\u03b221.\nBy the law of total probability and the independence of Hk\ni1 and Hk\ni2,\n\u03b4k\n1\n\u03b4k\n2\n= E[(1 \u2212rL\n1)Hk\n11+1(1 \u2212(1 \u2212rL\n2)Hk\n12)]\nE[(1 \u2212rL\n2)Hk\n22+1(1 \u2212(1 \u2212rL\n1)Hk\n21)]\n= (1 \u2212rL\n1)E[(1 \u2212rL\n1)Hk\n11](1 \u2212E[(1 \u2212rL\n2)Hk\n12])\n(1 \u2212rL\n2)E[(1 \u2212rL\n2)Hk\n22](1 \u2212E[(1 \u2212rL\n1)Hk\n21])\n.\nIt follows that\nlim\nn\u2192\u221e\n\u03b4k\n1\n\u03b4k\n2\n\u2248(1 \u2212rL\n1)e\u2212\u03b211(1 \u2212e\u2212\u03b212)\n(1 \u2212rL\n2)e\u2212\u03b222(1 \u2212e\u2212\u03b221)\n\u22481 \u2212rL\n1\n1 \u2212rL\n2\ne\u03b222\u2212\u03b211 = 1 \u2212rL\n1\n1 \u2212rL\n2\ne(\u03c1\u22121)\u03b2.\nB. Reproducibility\nIn this section, we describe the detailed experimental settings including (\u00a7B.1) data statistics, (\u00a7B.2) baseline settings, and\n(\u00a7B.3) evaluation protocols. The source code for implementing and evaluating BAT and all the CIGL baseline methods will\nbe released after the paper is published.\n16\nClass-Imbalanced Graph Learning without Class Rebalancing\nB.1. Data Statistics\nAs previously described, we adopt 5 benchmark graph datasets: the Cora, CiteSeer, and PubMed citation networks (Sen\net al., 2008), and the CS and Physics coauthor networks (Shchur et al., 2018) to test BAT on large graphs with more nodes\nand high-dimensional features. All datasets are publicly available2. Table 4 summarizes the dataset statistics.\nTable 4. Statistics of datasets.\nDataset\n#nodes\n#edges\n#features\n#classes\nCora\n2,708\n10,556\n1,433\n7\nCiteSeer\n3,327\n9,104\n3,703\n6\nPubMed\n19,717\n88,648\n500\n3\nCS\n18,333\n163,788\n6,805\n15\nPhysics\n34,493\n495,924\n8,415\n5\nWe follow previous works (Zhao et al., 2021b; Park et al., 2022; Song et al., 2022a) to construct and adjust the class\nimbalanced node classification tasks. For step imbalance, we select half of the classes (\u230am/2\u230b) as minority classes and the\nrest as majority classes. We follow the public split (Sen et al., 2008) for semi-supervised node classification where each\nclass has 20 training nodes, then randomly remove minority class training nodes until the given imbalance ratio (IR) is\nmet. The imbalance ratio is defined as IR = #majority training nodes\n#minority training nodes \u2208[1, \u221e), i.e., more imbalanced data has higher IR. For\nnatural imbalance, we simulate the long-tail class imbalance present in real-world data by utilizing a power-law distribution.\nSpecifically, for a given IR, the largest head class have nhead = IR training nodes, and the smallest tail class have 1 training\nnode. The number of training nodes of the k-th class is determined by nk = \u230an\u03bbk\nhead\u230b, \u03bbk = m\u2212k\nm\u22121. We set the IR (largest\nclass to smallest class) to 50/100 to test BAT\u2019s robustness under natural and extreme class imbalance. We show the training\ndata distribution under step and natural imbalance in Fig. 8.\nFigure 8. Class distribution of training datasets under step and natural imbalance.\nB.2. Baseline Settings\nTo fully validate BAT\u2019s performance and compatibility with existing CIGL techniques and GNN backbones, we include\nsix baseline methods with five popular GNN backbones in our experiments, and combine BAT with them under all\npossible combinations. The included CIGL baselines can be generally divided into two categories: reweighting-based (i.e.,\nReweight (Japkowicz & Stephen, 2002), ReNode (Chen et al., 2021)) and augmentation-based (i.e., Oversampling (Japkowicz\n& Stephen, 2002), SMOTE (Chawla et al., 2002), GraphSMOTE (Zhao et al., 2021b), and GraphENS (Park et al., 2022)).\n\u2022 Reweight (Japkowicz & Stephen, 2002) assigns minority classes with higher misclassification costs (i.e., weights in the\nloss function) by the inverse of the class frequency in the training set.\n\u2022 ReNode (Chen et al., 2021) measures the influence conflict of training nodes, and perform instance-wise node\nreweighting to alleviate the topology imbalance.\n2https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html.\n17\nClass-Imbalanced Graph Learning without Class Rebalancing\n\u2022 Oversample (Japkowicz & Stephen, 2002) augments minority classes with additional synthetic nodes by replication-base\noversampling.\n\u2022 SMOTE (Chawla et al., 2002) synthesizes minority nodes by 1) randomly selecting a seed node, 2) finding its k-nearest\nneighbors in the feature space, and 3) performing linear interpolation between the seed and one of its k-nearest\nneighbors.\n\u2022 GraphSMOTE (Zhao et al., 2021b) extends SMOTE (Chawla et al., 2002) to graph-structured data by 1) performing\nSMOTE in the low-dimensional embedding space of GNN and 2) utilizing a learnable edge predictor to generate better\ntopology connections for synthetic nodes.\n\u2022 GraphENS (Park et al., 2022) directly synthesize the whole ego network (node with its 1-hop neighbors) for minority\nclasses by similarity-based ego network combining and saliency-based node mixing to prevent neighbor memorization.\nBaseline implementation details. We use the public implementations345 of the baseline methods for a fair comparison.\nFor ReNode (Chen et al., 2021), we use its transductive version and search hyperparameters among the lower bound of\ncosine annealing wmin \u2208{0.25, 0.5, 0.75} and upper bound of the cosine annealing wmax \u2208{1.25, 1.5, 1.75} following\nthe original paper. We set the teleport probability of PageRank \u03b1 = 0.15 as given by the default setting in the released\nimplementation. As Oversample (Cui et al., 2019) and SMOTE (Chawla et al., 2002) were not proposed to handle graph\ndata, we adopt their enhanced versions provided by GraphSMOTE (Zhao et al., 2021b), which also duplicate the edges from\nthe seed nodes to the synthesized nodes in order to connect them to the graph. For GraphSMOTE (Zhao et al., 2021b), we\nuse the version that predicts edges with binary values as it performs better than the variant with continuous edge predictions\nin many datasets. For GraphENS (Park et al., 2022), we follow the settings in the paper: Beta(2, 2) distribution is used to\nsample \u03bb, the feature masking hyperparameter k and temperature \u03c4 are tuned among {1, 5, 10} and {1, 2}, and the number\nof warmup epochs is set to 5.\nCombining BAT and baseline CIGL techniques. Since BAT only manipulates the graph data and remains independent\nof the model architecture, it seamlessly integrates with the aforementioned CIGL techniques. During each training epoch,\nBAT enhances the original graph G using the current model f and yields the augmented graph G\u2217. Subsequently, other\nCIGL methods operate on the augmented graph G\u2217. Specifically, loss function engineering methods (Reweight and ReNode)\nperform loss computation and backpropagation based on G\u2217, and data augmentation methods (Resampling, SMOTE,\nGSMOTE, GENS) carry out additional class-balancing operations on G\u2217, generating new minority nodes based on its\nstructure.\nGNN backbone implementation details. We use pytorch (Paszke et al., 2019) and torch geometric (Fey &\nLenssen, 2019) to implement all five GNN backbones used in this paper, i.e., GCN (Welling & Kipf, 2016), GAT (Veli\u02c7ckovi\u00b4c\net al., 2018), GraphSAGE (Hamilton et al., 2017), APPNP (Gasteiger et al., 2018), and GPRGNN (Chien et al., 2020). Most\nof our settings are aligned with prevailing works (Park et al., 2022; Chen et al., 2021; Song et al., 2022a) to obtain fair and\ncomparable results. Specifically, we implement all GNNs\u2019 convolution layer with ReLU activation and dropout (Srivastava\net al., 2014) with a dropping rate of 0.5 before the last layer. For GAT, we set the number of attention heads to 4. For APPNP\nand GPRGNN, we follow the best setting in the original paper and use 2 APPNP/GPR prop convolution layers with 64\nhidden units. Note that GraphENS\u2019s official implementation requires modifying the graph convolution for resampling and\nthus cannot be directly combined with APPNP and GPRGNN. The teleport probability = 0.1 and the number of power\niteration steps K = 10. We search for the best architecture for other backbones from #layers l \u2208{1, 2, 3} and hidden\ndimension d \u2208{64, 128, 256} based on the average of validation accuracy and F1 score. We train each GNN for 2,000\nepochs using Adam optimizer (Kingma & Ba, 2014) with an initial learning rate of 0.01. To achieve better convergence, we\nfollow (Park et al., 2022) to use 5e-4 weight decay and adopt the ReduceLROnPlateau scheduler in Pytorch, which\nreduces the learning rate by half if the validation loss does not improve for 100 epochs.\nB.3. Evaluation Protocol\nTo evaluate the predictive performance on class-imbalanced data, we use two balanced metrics, i.e., balanced accuracy\n(BAcc.) and macro-averaged F1 score (Macro-F1). They compute accuracy/F1-score for each class independently and use\nthe unweighted average mean as the final score, i.e., BAcc. = 1\nm\nPm\ni=1 Acc(ci), Macro-F1 = 1\nm\nPm\ni=1 F1(ci). Additionally,\nwe use performance standard deviation (PerfStd) to evaluate the level of model predictive bias. Formally, let Acc(ci) be the\n3https://github.com/victorchen96/renode\n4https://github.com/TianxiangZhao/GraphSmote\n5https://github.com/JoonHyung-Park/GraphENS\n18\nClass-Imbalanced Graph Learning without Class Rebalancing\nclassification accuracy of class ci, the PerfStd is defined as the standard deviation of the accuracy scores of all classes, i.e.,\nq\n1\nm\nPm\ni=1(Acc(ci) \u2212BAcc.)2. All the experiments are conducted on a Linux server with Intel\u00ae Xeon\u00ae Gold 6240R CPU\nand NVIDIA\u00ae Tesla V100 32GB GPU.\nC. Extended Discussions\nIn this section, we present an ablation study (\u00a7C.1) validate the effectiveness and efficiency of the key modules, then we\ndiscuss how to further speed up BAT in practice (\u00a7C.2); how to choose between BAT0 and BAT1 in practice (\u00a7C.3); and\nfinally, the limitation and future works (\u00a7C.4).\nC.1. Ablation Study\nWe present an ablation study to validate the effectiveness and efficiency of the key modules in BAT. Specifically, for node\nrisk estimation, we compare our total-variation-distance-based uncertainty with (i) the na\u00a8\u0131ve random assignment that drawn\nuncertainty score from a uniform distribution U(0, 1) and (ii) the information entropy H(Y ) = \u2212P\ny\u2208Y p(y) log2(p(y)).\nWe substitute the original uncertainty metric with these aforementioned methods in BAT0, and assess their impact on\nperformance as well as the computational time required for uncertainty estimation. It is worth noting that in practical\nimplementation, the computation is parallelized on GPU (assuming sufficient GPU memory). Therefore, the computational\ntime of a given uncertainty measure remains consistent across the three datasets we employed (Cora, CiteSeer, PubMed with\nIR=10). The detailed results are presented in Table 5, revealing that: (i) Randomly assigned uncertainty scores significantly\nimpede the performance of BAT, resulting in a large drop in both balanced accuracy and Marco-f1. (ii) In comparison to\nour approach, employing information entropy as the node uncertainty score necessitates \u223c2.3x computation time, yet the\ninfluence on performance remains marginal.\nTable 5. Ablation study on node risk estimation of BAT.\nUncertainty\nCora\nCiteSeer\nPubMed\nComputation\nBAcc\nMacro-F1\nBAcc\nMacro-F1\nBAcc\nMacro-F1\nTime(ms)\nRandom\n61.64\u00b11.89\n59.44\u00b11.71\n46.59\u00b12.29\n44.37\u00b13.23\n61.60\u00b11.69\n58.13\u00b11.81\n0.0249\nInformation Entropy\n65.18\u00b11.68\n63.11\u00b11.91\n51.87\u00b12.96\n50.36\u00b13.43\n67.72\u00b11.27\n67.19\u00b11.57\n0.1257\nTVDistance (ours)\n65.54\u00b11.25\n63.28\u00b11.07\n52.65\u00b11.08\n51.55\u00b11.28\n68.62\u00b10.77\n67.16\u00b11.53\n0.0543\nFurther, we conduct an ablation study for our posterior likelihood estimation strategy by comparing our 0th-order (BAT0) and\n1st-order (BAT1) likelihood estimation methods with the random method that assigns (unnormalized) node-class likelihood\nby drawing from a uniform distribution U(0, 1). Results are shown in Table 6. We can observe that the random method\nsignificantly worsens the predictive performance on all CIGL tasks. Altogether, the ablation study results confirm the\neffectiveness and efficiency of the design of BAT, showcasing its ability to deliver strong performance with minimal\ncomputational overhead.\nTable 6. Ablation study on posterior likelihood estimation of BAT.\nEstimation\nCora\nCiteSeer\nPubMed\nComputation\nBAcc\nMacro-F1\nBAcc\nMacro-F1\nBAcc\nMacro-F1\nTime(ms)\nRandom\n63.85\u00b12.17\n61.94\u00b12.68\n46.51\u00b13.27\n41.70\u00b14.33\n64.32\u00b11.23\n53.58\u00b12.48\n0.0883\n0th-order (BAT0)\n65.54\u00b11.25\n63.28\u00b11.07\n52.65\u00b11.08\n51.55\u00b11.28\n68.62\u00b10.77\n67.16\u00b11.53\n0.1251\n1st-order (BAT1)\n69.80\u00b11.30\n68.68\u00b11.49\n55.37\u00b11.39\n54.94\u00b11.44\n67.57\u00b13.22\n64.40\u00b13.68\n0.3030\nC.2. On the Further Speedup of BAT\nAs stated in the paper, thanks to its simple and efficient design, BAT can be integrated into the GNN training process to\nperform dynamic topology augmentation based on the training state. By default, we run BAT in every iteration of GNN\ntraining, i.e., the granularity of applying BAT is 1, as described in Alg. 1. However, we note that in practice, this granularity\ncan be increased to further reduce the cost of applying BAT. This operation can result in a significant linear speedup ratio:\nsetting the granularity to N reduces the computational overhead of BAT to 1/N of the original (i.e., Nx speedup ratio), with\n19\nClass-Imbalanced Graph Learning without Class Rebalancing\nminor performance degradation. This could be helpful for scaling BAT to large-scale graphs in practice. In this section, we\ndesign experiments to validate the influence of different BAT granularity (i.e., the number of iterations per each use of BAT)\nin real-world CIGL tasks. We set the granularity to 1/5/10/50/100 and test the performance of BATT with a vanilla GCN\nclassifier on the Cora/CiteSeer/PubMed dataset with an imbalance ratio of 10. Fig. 9 shows the empirical results from 10\nindependent runs. The red horizontal line in each subfigure represents the baseline (vanilla GCN) performance. It can be\nobserved that setting a larger BAT granularity is an effective way to further speed up BAT in practice. The performance drop\nof adopting this trick is relatively minor, especially considering the significant linear speedup ratio it brings. The predictive\nperformance boost brought by BAT is still significant even with a large granularity at 100 (i.e., with 100x BAT speedup).\n(a) Influence of BAT granularity on BAcc.\n(b) Influence of BAT granularity on Macro-F1.\nFigure 9. Influence of the BAT granularity (i.e., the number of iterations per each use of BAT). Note that this brings a linear speedup ratio\nin practice, e.g., granularity = 100 \u21d4100x speedup.\nC.3. Choosing between BAT0 and BAT1\nIn this section, we summarize the strengths and limitations of BAT0 and BAT1, and give suggestions for choosing between\nthem in practice. In short, we recommend using BAT1 to achieve better classification performance. But in case that\ncomputational resources are limited, BAT0 can serve as a more efficient alternative. The reasons are as follows:\nPerformance. We observe a noticeable performance gap between BAT0 and BAT1, wherein BAT1 consistently demonstrates\nsuperior classification performance due to its incorporation of local topological structure. Across the 15 scenarios outlined\nin Table 1 (the best scores for 3 datasets x 5 GNN backbones): (1) BAT1 outperforms BAT0 significantly in 12 out of 15\nscenarios for BAcc/F1 scores, with an average F1 advantage of 1.692 in the 11 leading cases. (2) Conversely, BAT0 exhibits\na less pronounced advantage in the instances where it outperforms BAT1, with an average advantage of 0.518 in the 4 leading\nscenarios6.\nEfficiency. On the other hand, it\u2019s worth noting that BAT1 generally incurs higher time and space complexity compared to\nBAT0. Specifically, BAT0 demonstrates linear complexity concerning the number of nodes, whereas BAT1 exhibits linear\ngrowth in complexity with the number of edges. Given that real-world graph data often features a significantly larger number\nof edges than nodes, BAT0 is usually the more efficient option (especially for densely connected graphs).\nC.4. Limitations and Future Works\nOne potential limitation of the proposed BAT framework is its reliance on exploiting model prediction for risk and likelihood\nestimation. This strategy may not provide accurate estimation when the model itself exhibits extremely poor predictive\nperformance. However, this rarely occurs in practice and can be prevented by more careful fine-tuning of parameters\nand model architectures. In addition to this, as described in Section 3, we adopt several fast measures to estimate node\nuncertainty, prediction risk, and posterior likelihood for the sake of efficiency. Other techniques for such purposes (e.g.,\ndeterministic (Liu et al., 2020; Zhao et al., 2020)/Bayesian (Zhang et al., 2019; Hasanzadeh et al., 2020)/Jackknife (Kang\net al., 2022a) uncertainty estimation) can be easily integrated into the proposed BAT framework, although the computational\nefficiency might be a major bottleneck. How to exploit alternative uncertainty/risk measures while retaining computational\nefficiency is an interesting future direction.\nBeyond the class imbalance in the node label distribution, graph data can also exhibit multi-facet skewness in other aspects.\nFor instance, class imbalance may also exist in edge-level (e.g., in edge classification/prediction (Cai et al., 2021; Pandey\n6Despite BAT1 holding a relative performance edge, both BAT0 and BAT1 substantially enhance the performance of the best-performing\nCIGL baseline methods. Over the 15 scenarios, BAT0 yields an average improvement of 4.789/5.848 in the best BAcc/F1, while BAT1\nbrings an average improvement of 5.805/6.950.\n20\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 7. Experiments on large-scale graphs, the numbers in parentheses are the performance gain brought by BAT.\nMetric\nDataset\nSetting\nCIGL Baseline\nERM\nReweight\nReNode\nResample\nSMOTE\nGraphSMOTE\nGraphENS\nBAcc.\u2191\nCoraFull\nBase\n36.12\n40.93\n40.10\n35.71\n35.59\n39.25\n43.76\n+BAT0\n39.40 (+3.28)\n43.81 (+2.88)\n42.78 (+2.68)\n40.77 (+5.05)\n40.31 (+4.72)\n43.71 (+4.46)\n46.20 (+2.44)\n+BAT1\n40.88 (+4.76)\n43.77 (+2.84)\n42.82 (+2.72)\n40.67 (+4.95)\n41.04 (+5.46)\n43.70 (+4.45)\n47.19 (+3.44)\nOGBN-arXiv\nBase\n32.20\n35.69\nOOM\n32.24\n32.18\nOOM\nOOM\n+BAT0\n34.36 (+2.16)\n37.59 (+1.90)\nOOM\n37.20 (+4.96)\n36.88 (+4.69)\nOOM\nOOM\n+BAT1\n36.57 (+4.37)\n39.28 (+3.60)\nOOM\n37.36 (+5.12)\n37.50 (+5.32)\nOOM\nOOM\nMacro-F1\u2191\nCoraFull\nBase\n33.54\n38.86\n37.98\n32.84\n32.70\n37.70\n41.28\n+BAT0\n37.25 (+3.71)\n41.21 (+2.35)\n40.58 (+2.61)\n39.04 (+6.19)\n38.35 (+5.65)\n41.25 (+3.55)\n43.90 (+2.61)\n+BAT1\n38.58 (+5.04)\n41.16 (+2.30)\n40.37 (+2.39)\n38.48 (+5.64)\n39.18 (+6.48)\n41.75 (+4.05)\n44.61 (+3.33)\nOGBN-arXiv\nBase\n29.90\n32.14\nOOM\n30.16\n29.96\nOOM\nOOM\n+BAT0\n32.42 (+2.52)\n34.51 (+2.38)\nOOM\n34.50 (+4.34)\n34.44 (+4.48)\nOOM\nOOM\n+BAT1\n33.99 (+4.08)\n34.78 (+2.64)\nOOM\n34.55 (+4.38)\n34.69 (+4.74)\nOOM\nOOM\n*OOM: Out-Of-Memory on a NVIDIA\u00ae Tesla V100 32GB GPU.\net al., 2019)) and graph-level (e.g., in graph classification/alignment (Zeng et al., 2023a; 2024; Yan et al., 2021b)). Handling\nclass imbalance can be even more challenging on evolving/dynamic graphs (Fu & He, 2021; Yan et al., 2021a). Beyond the\nquantity imbalance among classes, skewness may also exists in the topological structure, such as degree imbalance (Kang\net al., 2022b), and motif-level imbalance (Zhao et al., 2022; Zeng et al., 2023b). How to jointly consider the multi-facet\nnode/edge/graph-level imbalance to benefit more graph learning tasks is an exciting future direction. Finally, recent work\non fairness-aware graph learning (Kang et al., 2020; Fu et al., 2023) has observed that the topology of the graph can\nalso introduce bias/discrimination towards certain groups: extending BAT\u2019s concept to mitigate group unfairness through\ntopology augmentation poses an intriguing future direction.\nD. Additional Experimental Results and Analysis\nD.1. Results on Additional Large-scale Graphs\nTo further test the scalability of BAT and the baseline CIGL methods, we extended the experiment to two large-scale graph\ndatasets, CoraFull (19,793 nodes, 126,842 edges) (Bojchevski & G\u00a8unnemann, 2017) and arXiv (169,343 nodes, 1,166,243\nedges) (Hu et al., 2020). To ensure comprehensive and fair comparisons, we followed the same protocol as Table 1 to\nevaluate six baseline methods and the performance gains brought by BAT. We used GCN as the backbone. It is worth noting\nthat CIGL on these large graphs is a highly challenging task due to (i) the inherent task complexity (with 70/172 classes),\n(ii) the label scarcity for numerous minority classes, and (iii) the requirement for the algorithm\u2019s scalability. This may be the\nreason why most existing works (ReNode (Chen et al., 2021), GraphSMOTE (Zhao et al., 2021b), GraphENS (Park et al.,\n2022), TAM (Song et al., 2022a), LTE4G (Yun et al., 2022), etc.) did not consider these large datasets. Nevertheless, we\nconduct the experiments report the results in Table 7. We can observe that:\n\u2022 BAT can scale to large-scale CIGL tasks, consistently and significantly enhancing the performance of various CIGL\nbaselines (overall with 10% relative performance gain).\n\u2022 The linear complexity of BAT makes it scalable to large graphs, while GraphENS, ReNode, and GraphSMOTE face\nscalability issues when applied to the arXiv dataset (with 169,343 nodes) due to their O(n2) space complexity.\nD.2. Comparison with Additional CIGL Baselines\nIn the main results, we chose representative model-independent CIGL baselines to test BAT\u2019s ability to cooperate and\nboost various CIGL techniques. In this section, we compare BAT with other representative CIGL techniques that are either\nmodel-dependent or with design constraints making them incompatible with BAT. Specifically, we further include three\nCIGL baselines LTE4G (Yun et al., 2022), GraphMixup (Wu et al., 2022), TAM (Song et al., 2022a), and compare them\nwith BAT on seven datasets (including the newly introduced large-scale graphs CoraFull and arXiv in Section D.1). We use\nGCN as the backbone, and follow the main experiment protocol used in Table 1 to ensure fair and comparable results.\nThe results are reported in Table 8. In short, we observe that: (i) BAT consistently demonstrates better performance and\nscalability compared to the additional baselines. (ii) Some methods (such as GraphMixup and LTE4G) exhibit higher space\ncomplexity and thus cannot scale to large datasets. (iii) Among the baseline methods, LTE4G achieves the overall best\n21\nClass-Imbalanced Graph Learning without Class Rebalancing\nperformance due to its three-stage training (encoder pre-training, expert training, student training) and divide-and-conquer\nstrategy. We note that its complex training and inference strategies introduce additional computational overhead and make it\nchallenging to integrate with other methods. In comparison, BAT has better compatibility and can further achieve better\nperformance with existing class-rebalancing techniques.\nTable 8. Comparison with independent CIGL baselines. We use bold/italics to mark the best/second-best results.\nMetric\nMethod\nDataset\nCora\nCiteSeer\nPubMed\nCS\nPhysics\nCoraFull\narXiv\nBAcc.\u2191\nGCN\n61.6\n37.6\n64.2\n75.4\n80.1\n32.2\n36.1\nGraphMixup\n64.1\n50.8\nOOM\nOOM\nOOM\nOOM\nOOM\nLTE4G\n68.0\n51.7\n63.7\n77.2\n81.7\n36.6\nOOM\nTAM\n65.3\n50.1\n65.7\n77.0\n82.8\n35.0\n36.3\nBAT (Ours)\n69.8\n55.4\n68.6\n82.6\n87.6\n36.6\n40.1\nMacro-F1\u2191\nGCN\n60.1\n28.1\n55.1\n72.7\n80.7\n33.5\n29.9\nGraphMixup\n62.7\n47.3\nOOM\nOOM\nOOM\nOOM\nOOM\nLTE4G\n67.1\n49.7\n61.7\n76.7\n82.1\n37.7\nOOM\nTAM\n65.7\n43.4\n63.5\n75.2\n82.3\n32.8\n31.2\nBAT (Ours)\n68.7\n54.9\n67.2\n78.6\n88.8\n38.6\n34.0\n*OOM: Out-Of-Memory on a NVIDIA\u00ae Tesla V100 32GB GPU.\nD.3. Full Empirical Results with Additional GNN Backbones\nDue to space limitation, we report the key results of our experiments in Table 1 and 2. We now provide complete results for\nall settings with the standard error of 5 independent runs. Specifically, Table 9 & 10 & 11 complement Table 1, and Table 12\ncomplements Table 2. We further include APPNP (Gasteiger et al., 2018) and GPRGNN (Chien et al., 2020) as additional\nGNN backbones for supported CIGL techniques. Note that the official codebase of (Park et al., 2022) only implemented\nGraphENS using modified (to enable saliency-based mixup) GCN, GAT, and GraphSAGE backbones, and extending it to\nother GNN backbones is difficult. The results indicate that BAT can consistently boost various CIGL baselines with all GNN\nbackbones under different types and levels of class imbalance, which aligns with our conclusions in the paper.\n22\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 9. Balanced accuracy of combining BAT with 6 IGL baselines \u00d7 5 GNN backbones.\nDataset (IR=10)\nCora\nCiteSeer\nPubMed\nMetric: BAcc.\u2191\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nGCN\nVanilla\n61.56\u00b11.24\n65.54\u00b11.25\n69.80\u00b11.30\n37.62\u00b11.61\n52.65\u00b11.08\n55.37\u00b11.39\n64.23\u00b11.55\n68.62\u00b10.77\n67.57\u00b13.22\nReweight\n67.65\u00b10.64\n70.97\u00b11.28\n72.14\u00b10.72\n42.49\u00b12.66\n57.91\u00b10.98\n58.36\u00b11.09\n71.20\u00b12.33\n74.19\u00b11.12\n73.37\u00b10.96\nReNode\n66.60\u00b11.33\n71.37\u00b10.62\n71.84\u00b11.25\n42.57\u00b11.05\n57.47\u00b10.62\n59.28\u00b10.59\n71.52\u00b12.16\n73.20\u00b10.71\n72.53\u00b11.62\nResample\n59.48\u00b11.53\n72.51\u00b10.68\n74.24\u00b10.91\n39.15\u00b12.05\n57.90\u00b10.33\n58.78\u00b11.44\n64.97\u00b11.94\n72.53\u00b10.85\n72.87\u00b11.16\nSMOTE\n58.27\u00b11.05\n72.16\u00b10.53\n73.89\u00b11.06\n39.27\u00b11.90\n60.06\u00b10.81\n61.97\u00b11.19\n64.41\u00b11.95\n73.17\u00b10.84\n73.13\u00b10.77\nGSMOTE\n67.99\u00b11.37\n68.52\u00b10.81\n71.55\u00b10.50\n45.05\u00b11.95\n57.68\u00b11.03\n57.65\u00b11.18\n73.99\u00b10.88\n73.09\u00b11.30\n76.57\u00b10.42\nGENS\n70.12\u00b10.43\n72.22\u00b10.57\n72.58\u00b10.58\n56.01\u00b11.17\n60.60\u00b10.63\n62.67\u00b10.42\n73.66\u00b11.04\n76.11\u00b10.60\n76.91\u00b11.03\nBest\n70.12\n72.51\n74.24\n56.01\n60.60\n62.67\n73.99\n76.11\n76.91\nGAT\nVanilla\n61.53\u00b11.13\n66.27\u00b10.83\n70.13\u00b11.07\n39.25\u00b11.84\n55.66\u00b11.23\n60.34\u00b11.66\n65.46\u00b10.69\n73.19\u00b10.86\n74.75\u00b11.18\nReweight\n66.94\u00b11.24\n71.80\u00b10.48\n71.61\u00b10.85\n41.29\u00b13.39\n59.33\u00b10.51\n61.23\u00b10.99\n68.37\u00b11.41\n75.30\u00b11.07\n74.52\u00b11.14\nReNode\n66.81\u00b10.98\n72.14\u00b11.24\n70.31\u00b11.38\n43.25\u00b11.78\n58.26\u00b11.98\n59.05\u00b10.88\n71.18\u00b12.13\n75.55\u00b11.01\n75.22\u00b10.84\nResample\n57.76\u00b11.73\n71.90\u00b10.88\n73.29\u00b11.08\n35.97\u00b11.42\n60.10\u00b11.26\n60.33\u00b10.75\n65.14\u00b10.86\n73.27\u00b10.61\n73.89\u00b10.40\nSMOTE\n58.81\u00b10.64\n70.50\u00b10.44\n72.19\u00b10.75\n36.95\u00b11.86\n60.59\u00b11.19\n62.36\u00b11.18\n64.81\u00b11.47\n73.90\u00b10.68\n74.08\u00b10.51\nGSMOTE\n64.68\u00b11.02\n69.29\u00b11.82\n71.14\u00b10.96\n41.82\u00b11.14\n56.11\u00b11.23\n57.71\u00b12.58\n68.72\u00b11.69\n74.65\u00b10.65\n74.41\u00b11.57\nGENS\n69.76\u00b10.45\n70.63\u00b10.40\n71.02\u00b11.22\n51.50\u00b12.21\n60.95\u00b11.51\n63.49\u00b10.75\n73.13\u00b11.18\n74.34\u00b10.35\n75.65\u00b10.82\nBest\n69.76\n72.14\n73.29\n51.50\n60.95\n63.49\n73.13\n75.55\n75.65\nSAGE\nVanilla\n59.17\u00b11.23\n66.24\u00b10.92\n66.53\u00b10.80\n42.96\u00b10.28\n54.99\u00b12.51\n53.18\u00b12.90\n67.56\u00b10.84\n75.31\u00b10.93\n77.38\u00b10.68\nReweight\n63.76\u00b10.89\n70.15\u00b11.15\n71.14\u00b10.84\n45.91\u00b12.05\n57.95\u00b10.73\n55.90\u00b10.93\n68.03\u00b11.69\n74.56\u00b10.41\n75.39\u00b10.38\nReNode\n65.32\u00b11.07\n71.31\u00b11.29\n71.54\u00b10.85\n48.55\u00b12.31\n56.32\u00b10.40\n56.49\u00b11.73\n69.08\u00b12.04\n74.24\u00b10.20\n75.28\u00b10.69\nResample\n57.77\u00b11.35\n71.24\u00b11.08\n73.01\u00b11.02\n39.37\u00b11.40\n61.41\u00b11.11\n61.93\u00b11.40\n69.22\u00b11.28\n74.91\u00b11.09\n75.80\u00b10.39\nSMOTE\n58.81\u00b11.97\n70.31\u00b11.35\n73.02\u00b12.29\n38.42\u00b11.69\n64.14\u00b10.75\n66.35\u00b10.70\n64.96\u00b11.56\n74.59\u00b10.96\n77.31\u00b10.45\nGSMOTE\n61.57\u00b11.78\n69.88\u00b10.96\n72.28\u00b11.48\n42.21\u00b12.12\n60.91\u00b11.33\n62.32\u00b11.06\n71.55\u00b10.64\n74.74\u00b10.81\n76.14\u00b10.21\nGENS\n68.84\u00b10.41\n69.78\u00b11.18\n71.92\u00b10.71\n52.57\u00b11.78\n64.36\u00b10.68\n63.84\u00b10.68\n71.38\u00b10.99\n75.89\u00b11.17\n76.46\u00b11.29\nBest\n68.84\n71.31\n73.02\n52.57\n64.36\n66.35\n71.55\n75.89\n77.38\nAPPNP\nVanilla\n55.37\u00b11.65\n58.13\u00b11.69\n61.71\u00b11.66\n35.69\u00b10.14\n35.68\u00b10.15\n36.02\u00b10.25\n59.30\u00b10.50\n55.62\u00b10.31\n57.82\u00b10.29\nReweight\n72.62\u00b10.47\n73.62\u00b10.89\n72.51\u00b10.87\n50.88\u00b13.64\n63.54\u00b11.02\n65.57\u00b11.11\n72.00\u00b10.81\n72.15\u00b10.60\n71.22\u00b11.10\nReNode\n73.74\u00b11.12\n75.02\u00b11.54\n72.15\u00b10.76\n50.50\u00b13.51\n63.73\u00b10.54\n65.13\u00b10.40\n72.76\u00b11.37\n71.54\u00b10.96\n71.88\u00b10.70\nResample\n65.78\u00b11.72\n73.14\u00b10.94\n73.57\u00b10.92\n40.79\u00b11.87\n66.54\u00b10.49\n59.51\u00b14.16\n67.74\u00b11.94\n72.25\u00b10.81\n74.41\u00b10.95\nSMOTE\n65.34\u00b11.68\n73.18\u00b11.02\n72.88\u00b10.90\n40.79\u00b12.05\n66.62\u00b10.33\n58.82\u00b14.59\n67.24\u00b12.10\n72.67\u00b11.65\n73.33\u00b11.37\nGSMOTE\n71.13\u00b10.72\n73.37\u00b10.82\n73.78\u00b10.71\n45.37\u00b12.75\n64.95\u00b10.11\n62.95\u00b12.58\n69.57\u00b12.20\n73.37\u00b10.95\n74.90\u00b11.27\nBest\n73.74\n75.02\n73.78\n50.88\n66.62\n65.57\n72.76\n73.37\n74.90\nGPRGNN\nVanilla\n67.97\u00b10.51\n71.99\u00b11.14\n73.38\u00b11.18\n42.31\u00b12.16\n55.85\u00b10.89\n58.82\u00b11.91\n67.04\u00b11.82\n57.92\u00b10.45\n77.49\u00b11.15\nReweight\n72.15\u00b10.57\n72.90\u00b11.33\n73.22\u00b10.55\n53.22\u00b12.89\n59.78\u00b10.76\n61.00\u00b11.82\n73.35\u00b11.07\n75.22\u00b11.02\n76.86\u00b10.76\nReNode\n73.38\u00b10.67\n73.71\u00b10.57\n73.93\u00b11.60\n54.66\u00b12.82\n59.69\u00b10.73\n60.34\u00b11.31\n73.56\u00b10.98\n75.69\u00b11.10\n76.25\u00b10.67\nResample\n67.00\u00b11.33\n72.94\u00b11.02\n74.89\u00b10.86\n42.27\u00b12.15\n64.16\u00b10.62\n63.89\u00b10.98\n70.42\u00b11.51\n73.79\u00b10.83\n75.31\u00b10.54\nSMOTE\n66.99\u00b11.33\n74.01\u00b11.51\n74.41\u00b11.05\n40.97\u00b12.02\n63.88\u00b10.55\n62.60\u00b11.72\n70.29\u00b11.47\n73.89\u00b10.69\n75.48\u00b11.02\nGSMOTE\n70.94\u00b10.57\n73.63\u00b11.25\n74.02\u00b10.90\n48.01\u00b13.28\n63.03\u00b10.92\n61.68\u00b10.86\n71.51\u00b11.91\n72.16\u00b10.58\n74.77\u00b10.83\nBest\n73.38\n74.01\n74.89\n54.66\n64.16\n63.89\n73.56\n75.69\n77.49\n23\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 10. Macro-F1 score of combining BAT with 6 IGL baselines \u00d7 5 GNN backbones.\nDataset (IR=10)\nCora\nCiteSeer\nPubMed\nMetric: Macro-F1\u2191\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nGCN\nVanilla\n60.10\u00b11.53\n63.28\u00b11.07\n68.68\u00b11.49\n28.05\u00b12.53\n51.55\u00b11.28\n54.94\u00b11.44\n55.09\u00b12.48\n67.16\u00b11.53\n64.40\u00b13.68\nReweight\n67.85\u00b10.62\n69.41\u00b11.01\n70.31\u00b10.82\n36.59\u00b13.66\n56.84\u00b11.06\n57.54\u00b11.08\n67.07\u00b13.42\n72.94\u00b10.81\n73.24\u00b10.90\nReNode\n66.66\u00b11.59\n69.79\u00b10.79\n70.59\u00b11.25\n34.64\u00b11.54\n56.69\u00b10.64\n58.07\u00b10.77\n67.86\u00b13.99\n72.61\u00b10.41\n72.25\u00b10.89\nResample\n57.34\u00b12.27\n71.36\u00b10.39\n72.82\u00b11.13\n29.73\u00b12.77\n57.17\u00b10.48\n58.03\u00b11.42\n56.74\u00b13.54\n71.19\u00b10.83\n73.13\u00b11.33\nSMOTE\n55.65\u00b11.62\n71.04\u00b10.16\n72.82\u00b10.86\n29.39\u00b12.81\n59.53\u00b10.88\n61.53\u00b11.24\n56.14\u00b13.74\n71.72\u00b10.60\n72.83\u00b11.20\nGSMOTE\n67.60\u00b11.67\n68.01\u00b11.00\n70.28\u00b10.48\n40.07\u00b13.02\n56.64\u00b11.09\n56.25\u00b11.50\n70.60\u00b11.17\n72.95\u00b11.39\n75.70\u00b10.35\nGENS\n69.96\u00b10.29\n71.62\u00b10.64\n72.28\u00b10.65\n54.45\u00b11.69\n59.89\u00b10.68\n62.46\u00b10.43\n71.28\u00b11.84\n75.77\u00b10.55\n76.86\u00b10.93\nBest\n69.96\n71.62\n72.82\n54.45\n59.89\n62.46\n71.28\n75.77\n76.86\nGAT\nVanilla\n60.71\u00b11.61\n64.27\u00b10.95\n68.93\u00b10.79\n31.12\u00b13.15\n54.71\u00b11.18\n59.42\u00b11.55\n57.32\u00b11.55\n71.27\u00b11.11\n74.03\u00b11.08\nReweight\n66.49\u00b11.34\n69.84\u00b10.91\n69.79\u00b10.77\n34.94\u00b14.09\n58.53\u00b10.68\n60.28\u00b11.12\n63.82\u00b11.60\n75.13\u00b11.13\n73.88\u00b11.38\nReNode\n67.27\u00b11.23\n70.61\u00b10.83\n68.24\u00b11.48\n37.72\u00b12.61\n57.64\u00b12.11\n58.57\u00b10.75\n67.38\u00b13.22\n74.88\u00b10.99\n74.96\u00b11.18\nResample\n55.36\u00b12.47\n70.87\u00b10.94\n72.31\u00b11.07\n25.71\u00b11.97\n59.77\u00b11.31\n59.66\u00b10.95\n57.24\u00b11.54\n72.53\u00b10.66\n73.09\u00b10.83\nSMOTE\n57.49\u00b10.60\n69.68\u00b10.66\n71.74\u00b11.03\n26.05\u00b12.30\n59.83\u00b11.33\n61.75\u00b11.30\n55.66\u00b12.76\n73.33\u00b11.00\n73.30\u00b10.16\nGSMOTE\n64.34\u00b11.69\n68.23\u00b11.80\n69.77\u00b11.08\n35.07\u00b11.77\n55.86\u00b11.10\n57.13\u00b12.69\n63.35\u00b12.92\n74.23\u00b10.84\n73.34\u00b12.06\nGENS\n69.96\u00b10.62\n69.83\u00b10.41\n70.71\u00b11.16\n48.34\u00b12.19\n60.04\u00b11.85\n62.55\u00b10.86\n71.78\u00b11.19\n72.69\u00b10.84\n74.42\u00b11.12\nBest\n69.96\n70.87\n72.31\n48.34\n60.04\n62.55\n71.78\n75.13\n74.96\nSAGE\nVanilla\n57.36\u00b11.77\n64.90\u00b10.87\n65.61\u00b10.97\n36.07\u00b11.06\n54.76\u00b12.47\n51.86\u00b13.25\n63.75\u00b11.24\n74.35\u00b10.74\n76.92\u00b10.63\nReweight\n63.72\u00b11.10\n69.06\u00b10.90\n69.59\u00b10.53\n39.64\u00b12.57\n57.17\u00b10.76\n54.83\u00b10.74\n62.83\u00b12.57\n73.88\u00b10.40\n75.42\u00b10.48\nReNode\n65.59\u00b11.44\n69.99\u00b11.35\n69.86\u00b11.27\n44.20\u00b13.68\n55.41\u00b10.48\n55.78\u00b11.63\n64.97\u00b13.00\n74.33\u00b10.20\n74.88\u00b10.53\nResample\n55.29\u00b12.12\n70.40\u00b11.11\n71.49\u00b10.79\n30.14\u00b12.20\n60.71\u00b11.25\n61.29\u00b11.48\n65.23\u00b12.26\n74.28\u00b10.96\n75.48\u00b10.44\nSMOTE\n56.72\u00b12.69\n69.42\u00b11.29\n71.71\u00b11.94\n29.22\u00b12.33\n63.61\u00b10.87\n65.91\u00b10.68\n57.60\u00b13.22\n72.98\u00b10.69\n76.45\u00b10.77\nGSMOTE\n59.44\u00b12.25\n69.10\u00b10.95\n71.30\u00b11.47\n34.86\u00b13.46\n60.53\u00b11.27\n61.96\u00b11.12\n67.23\u00b10.61\n74.36\u00b11.02\n75.68\u00b10.31\nGENS\n68.23\u00b10.72\n69.76\u00b10.95\n71.11\u00b10.81\n51.05\u00b12.03\n63.87\u00b10.82\n63.41\u00b10.57\n70.06\u00b10.86\n75.33\u00b11.46\n76.01\u00b11.14\nBest\n68.23\n70.40\n71.71\n51.05\n63.87\n65.91\n70.06\n75.33\n76.92\nAPPNP\nVanilla\n50.39\u00b12.81\n54.19\u00b12.58\n59.99\u00b12.49\n22.21\u00b10.13\n22.54\u00b10.25\n22.89\u00b10.22\n44.50\u00b10.21\n44.67\u00b10.07\n44.59\u00b10.06\nReweight\n72.63\u00b10.53\n72.71\u00b10.60\n70.61\u00b10.65\n45.25\u00b14.85\n63.08\u00b11.03\n65.20\u00b11.20\n69.53\u00b11.14\n72.24\u00b10.58\n72.26\u00b10.80\nReNode\n73.67\u00b10.98\n73.67\u00b11.18\n69.79\u00b10.72\n44.91\u00b14.99\n62.97\u00b10.78\n64.47\u00b10.40\n70.65\u00b11.66\n72.33\u00b10.90\n72.18\u00b10.55\nResample\n65.20\u00b12.08\n72.25\u00b10.82\n72.72\u00b10.97\n31.04\u00b12.76\n66.06\u00b10.54\n54.57\u00b16.08\n62.42\u00b13.62\n72.32\u00b10.93\n74.27\u00b11.08\nSMOTE\n64.70\u00b12.06\n72.90\u00b10.83\n72.31\u00b10.94\n30.90\u00b12.86\n66.18\u00b10.37\n53.90\u00b16.26\n61.83\u00b13.65\n72.55\u00b11.61\n73.87\u00b11.37\nGSMOTE\n71.20\u00b10.67\n73.02\u00b10.74\n73.22\u00b10.92\n37.90\u00b14.29\n64.56\u00b10.18\n60.41\u00b13.84\n65.65\u00b13.06\n72.54\u00b10.85\n74.61\u00b11.36\nBest\n73.67\n73.67\n73.22\n45.25\n66.18\n65.20\n70.65\n72.55\n74.61\nGPRGNN\nVanilla\n67.86\u00b10.79\n70.80\u00b11.16\n72.32\u00b11.18\n35.00\u00b12.96\n55.06\u00b10.89\n56.31\u00b12.87\n59.01\u00b13.62\n50.12\u00b11.46\n77.62\u00b11.04\nReweight\n71.66\u00b10.85\n70.46\u00b10.98\n71.24\u00b10.49\n49.19\u00b13.61\n59.11\u00b10.73\n60.30\u00b12.04\n71.18\u00b10.95\n75.47\u00b10.90\n77.01\u00b10.52\nReNode\n73.08\u00b10.66\n71.52\u00b10.50\n71.72\u00b11.51\n50.34\u00b13.18\n59.10\u00b10.75\n58.94\u00b11.36\n71.45\u00b11.19\n75.08\u00b11.06\n75.76\u00b10.84\nResample\n66.42\u00b11.65\n71.70\u00b10.86\n73.54\u00b10.83\n32.60\u00b12.71\n63.59\u00b10.65\n63.12\u00b11.06\n66.58\u00b12.08\n73.66\u00b10.86\n75.42\u00b10.35\nSMOTE\n66.43\u00b11.74\n72.89\u00b11.23\n73.47\u00b11.12\n31.38\u00b12.70\n63.41\u00b10.55\n61.23\u00b12.59\n66.78\u00b11.97\n73.98\u00b10.70\n75.63\u00b10.91\nGSMOTE\n70.87\u00b10.53\n72.53\u00b10.85\n73.12\u00b10.95\n42.82\u00b14.52\n62.09\u00b11.04\n60.82\u00b10.88\n67.93\u00b13.01\n72.72\u00b10.72\n74.66\u00b10.74\nBest\n73.08\n72.89\n73.54\n50.34\n63.59\n63.12\n71.45\n75.47\n77.62\n24\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 11. Performance deviation of combining BAT with 6 IGL baselines \u00d7 5 GNN backbones.\nDataset (IR=10)\nCora\nCiteSeer\nPubMed\nMetric: PerfStd\u2193\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nBase\n+ BAT0\n+ BAT1\nGCN\nVanilla\n27.88\u00b11.79\n21.27\u00b11.76\n18.49\u00b12.68\n29.93\u00b11.38\n13.82\u00b12.06\n13.93\u00b10.80\n34.73\u00b12.14\n9.23\u00b12.78\n21.81\u00b14.51\nReweight\n22.29\u00b11.41\n14.43\u00b12.51\n18.32\u00b12.20\n25.47\u00b11.78\n19.10\u00b11.48\n22.64\u00b10.79\n19.33\u00b15.26\n10.21\u00b11.58\n5.88\u00b10.83\nReNode\n22.88\u00b11.64\n14.65\u00b12.07\n17.00\u00b12.13\n30.31\u00b11.51\n20.22\u00b10.88\n22.99\u00b11.09\n18.14\u00b15.79\n12.99\u00b11.57\n10.96\u00b11.92\nResample\n31.57\u00b11.85\n15.13\u00b12.14\n15.25\u00b12.79\n31.00\u00b11.32\n16.30\u00b11.89\n20.79\u00b10.43\n30.90\u00b15.67\n11.63\u00b13.20\n7.82\u00b10.80\nSMOTE\n33.32\u00b11.38\n16.33\u00b11.12\n17.95\u00b12.50\n32.61\u00b11.45\n17.27\u00b10.86\n18.25\u00b10.89\n31.79\u00b15.21\n10.56\u00b11.82\n11.66\u00b12.58\nGSMOTE\n21.78\u00b11.79\n17.90\u00b12.75\n18.44\u00b12.20\n22.64\u00b12.69\n21.37\u00b11.25\n21.01\u00b11.45\n15.87\u00b12.34\n3.35\u00b11.08\n5.83\u00b11.27\nGENS\n20.04\u00b11.12\n16.98\u00b13.02\n18.02\u00b12.23\n16.95\u00b12.64\n14.94\u00b10.75\n15.54\u00b10.60\n11.93\u00b13.46\n5.95\u00b11.85\n5.15\u00b10.80\nBest\n20.04\n14.43\n15.25\n16.95\n13.82\n13.93\n11.93\n3.35\n5.15\nGAT\nVanilla\n27.38\u00b11.71\n19.23\u00b10.80\n17.97\u00b12.65\n28.32\u00b12.07\n15.62\u00b10.77\n15.90\u00b10.95\n30.94\u00b11.27\n10.77\u00b12.04\n8.51\u00b12.48\nReweight\n22.90\u00b11.67\n16.44\u00b12.71\n17.32\u00b12.83\n30.27\u00b11.26\n18.64\u00b11.30\n20.83\u00b11.06\n24.92\u00b11.88\n3.01\u00b10.96\n5.44\u00b11.33\nReNode\n23.13\u00b11.54\n15.05\u00b11.59\n18.96\u00b11.65\n25.21\u00b11.85\n20.48\u00b10.82\n20.51\u00b10.49\n18.15\u00b14.37\n4.77\u00b11.22\n6.17\u00b10.42\nResample\n32.73\u00b12.12\n17.87\u00b12.04\n17.64\u00b12.57\n32.59\u00b10.89\n17.76\u00b11.79\n18.73\u00b11.02\n31.67\u00b10.98\n6.18\u00b11.35\n4.58\u00b11.14\nSMOTE\n31.17\u00b10.69\n18.40\u00b11.03\n18.26\u00b11.87\n33.32\u00b10.88\n10.68\u00b10.68\n13.24\u00b11.12\n32.79\u00b12.13\n8.14\u00b12.00\n7.56\u00b11.05\nGSMOTE\n24.84\u00b11.60\n15.48\u00b12.08\n18.23\u00b12.00\n26.74\u00b11.53\n18.34\u00b11.64\n19.76\u00b10.42\n24.50\u00b12.78\n5.12\u00b11.38\n8.54\u00b12.03\nGENS\n20.08\u00b11.56\n17.75\u00b12.40\n17.88\u00b12.50\n26.49\u00b11.18\n12.89\u00b10.77\n15.09\u00b10.95\n10.29\u00b12.75\n7.83\u00b12.27\n7.55\u00b12.38\nBest\n20.08\n15.05\n17.32\n25.21\n10.68\n13.24\n10.29\n3.01\n4.58\nSAGE\nVanilla\n29.94\u00b11.75\n18.62\u00b12.13\n19.49\u00b11.67\n26.75\u00b11.58\n14.56\u00b11.16\n18.13\u00b11.32\n21.09\u00b13.43\n10.96\u00b11.99\n4.09\u00b11.17\nReweight\n25.61\u00b11.60\n15.24\u00b12.66\n17.54\u00b12.45\n29.95\u00b11.83\n19.05\u00b11.60\n22.94\u00b10.49\n25.47\u00b13.49\n3.35\u00b10.72\n8.09\u00b10.19\nReNode\n24.12\u00b11.73\n13.32\u00b13.03\n15.45\u00b12.41\n22.41\u00b14.31\n22.20\u00b10.97\n22.75\u00b10.87\n22.92\u00b14.36\n7.63\u00b11.23\n5.77\u00b11.55\nResample\n31.66\u00b11.47\n15.77\u00b12.75\n15.08\u00b12.74\n30.29\u00b11.16\n18.72\u00b10.90\n18.48\u00b12.00\n21.41\u00b12.88\n4.68\u00b11.42\n4.76\u00b11.09\nSMOTE\n30.86\u00b12.64\n17.30\u00b12.09\n14.87\u00b13.23\n32.07\u00b11.00\n13.17\u00b11.33\n12.78\u00b10.43\n31.62\u00b12.86\n13.88\u00b11.44\n11.63\u00b12.28\nGSMOTE\n27.71\u00b11.86\n17.28\u00b12.25\n16.10\u00b12.94\n28.77\u00b12.61\n18.69\u00b10.76\n18.05\u00b11.21\n20.10\u00b10.90\n5.37\u00b11.21\n4.64\u00b11.61\nGENS\n19.81\u00b11.65\n17.50\u00b12.05\n17.63\u00b12.11\n19.76\u00b12.07\n15.99\u00b10.81\n16.99\u00b10.85\n11.76\u00b12.91\n7.63\u00b11.51\n8.31\u00b11.64\nBest\n19.81\n13.32\n14.87\n19.76\n13.17\n12.78\n11.76\n3.35\n4.09\nAPPNP\nVanilla\n38.32\u00b11.94\n35.50\u00b12.10\n32.18\u00b11.68\n36.82\u00b10.10\n36.67\u00b10.25\n36.83\u00b10.36\n42.13\u00b10.27\n40.45\u00b10.11\n41.34\u00b10.16\nReweight\n19.83\u00b11.46\n17.33\u00b12.88\n18.46\u00b12.42\n26.19\u00b12.93\n20.96\u00b10.58\n19.38\u00b10.72\n16.96\u00b12.84\n8.04\u00b11.94\n9.13\u00b11.05\nReNode\n18.09\u00b12.52\n16.87\u00b12.95\n19.47\u00b12.00\n25.95\u00b13.66\n22.09\u00b11.43\n20.42\u00b11.57\n14.49\u00b13.81\n10.25\u00b11.93\n3.95\u00b11.02\nResample\n27.28\u00b12.13\n18.37\u00b12.34\n18.72\u00b12.32\n32.71\u00b11.23\n15.87\u00b11.02\n23.72\u00b14.14\n25.86\u00b14.38\n13.60\u00b11.68\n9.49\u00b11.65\nSMOTE\n27.86\u00b11.78\n18.61\u00b12.35\n19.42\u00b11.81\n33.26\u00b11.10\n14.91\u00b10.93\n22.90\u00b14.49\n26.37\u00b14.47\n13.37\u00b11.97\n8.70\u00b12.29\nGSMOTE\n20.98\u00b11.45\n18.19\u00b12.59\n18.55\u00b12.28\n29.39\u00b12.20\n16.49\u00b11.12\n19.19\u00b13.44\n22.32\u00b14.21\n11.53\u00b13.00\n10.69\u00b12.27\nBest\n18.09\n16.87\n18.46\n25.95\n14.91\n19.19\n14.49\n8.04\n3.95\nGPRGNN\nVanilla\n22.96\u00b11.20\n18.12\u00b12.29\n17.00\u00b12.98\n27.57\u00b11.32\n17.10\u00b11.17\n20.94\u00b12.58\n29.94\u00b13.68\n36.57\u00b11.46\n5.30\u00b10.91\nReweight\n20.94\u00b11.21\n17.83\u00b12.82\n19.67\u00b11.81\n22.43\u00b12.39\n21.52\u00b11.06\n20.03\u00b11.81\n16.12\u00b11.84\n7.54\u00b10.49\n5.48\u00b11.49\nReNode\n18.84\u00b12.19\n16.78\u00b12.53\n17.89\u00b12.96\n24.14\u00b11.47\n19.84\u00b11.79\n22.83\u00b11.40\n14.40\u00b13.18\n9.75\u00b12.20\n6.61\u00b11.47\nResample\n25.62\u00b11.80\n19.23\u00b12.26\n17.61\u00b12.77\n33.08\u00b10.66\n17.04\u00b10.78\n15.98\u00b10.93\n22.59\u00b12.75\n7.62\u00b12.50\n7.76\u00b10.85\nSMOTE\n25.44\u00b11.88\n16.97\u00b13.19\n17.38\u00b12.78\n32.85\u00b10.95\n15.09\u00b11.23\n16.85\u00b12.51\n21.35\u00b12.76\n9.41\u00b12.67\n6.09\u00b10.62\nGSMOTE\n21.23\u00b11.48\n18.02\u00b12.62\n19.06\u00b12.28\n24.21\u00b13.06\n14.83\u00b10.95\n19.11\u00b11.73\n20.08\u00b13.77\n5.99\u00b11.49\n8.27\u00b10.75\nBest\n18.84\n16.78\n17.00\n22.43\n14.83\n15.98\n14.40\n5.99\n5.30\n25\nClass-Imbalanced Graph Learning without Class Rebalancing\nTable 12. Performance of BAT under varying types and levels of class imbalance. For each setting, we report the relative gain over base\nand mark the best/second-best score in bold/underlined.\nDataset\nCora\nCiteSeer\nPubMed\nCS\nPhysics\nStep IR\n10\n20\n10\n20\n10\n20\n10\n20\n10\n20\nBAcc.\u2191\nBase\n61.6\n52.7\n37.6\n34.2\n64.2\n60.8\n75.4\n65.3\n80.1\n67.7\n+ BAT\n69.8+13.4%\n71.3+35.2%\n55.4+47.2%\n51.3+49.9%\n68.6+6.8%\n63.3+4.1%\n82.6+9.6%\n79.9+22.2%\n87.6+9.4%\n88.0+29.9%\nBestIGL\n70.1+13.9%\n66.5+26.2%\n56.0+48.9%\n47.2+38.0%\n74.0+15.2%\n71.1+17.0%\n84.1+11.6%\n81.3+24.4%\n89.4+11.6%\n85.7+26.6%\n+ BAT\n74.2+20.6%\n71.6+35.9%\n62.7+66.6%\n62.5+82.6%\n76.9+19.7%\n75.7+24.5%\n86.3+14.5%\n85.6+31.0%\n91.2+13.9%\n90.9+34.2%\nMacro-F1\u2191\nBase\n60.1\n47.0\n28.1\n21.9\n55.1\n46.4\n72.7\n59.2\n80.7\n64.7\n+ BAT\n68.7+14.3%\n69.6+48.1%\n54.9+95.8%\n48.9+123.5%\n67.2+21.9%\n60.7+30.8%\n78.6+8.1%\n74.7+26.1%\n88.8+10.0%\n87.8+35.8%\nBestIGL\n70.0+16.4%\n66.2+40.9%\n54.5+94.1%\n45.0+105.6%\n71.3+29.4%\n68.9+48.3%\n83.9+15.3%\n80.9+36.7%\n89.5+10.9%\n86.2+33.2%\n+ BAT\n72.8+21.2%\n70.2+49.4%\n62.5+122.7%\n62.1+183.6%\n76.9+39.5%\n74.9+61.2%\n85.4+17.5%\n84.6+43.0%\n90.7+12.4%\n90.0+39.2%\nPerfStd\u2193\nBase\n27.9\n39.0\n29.9\n35.1\n34.7\n41.5\n21.2\n32.1\n22.2\n36.0\n+ BAT\n21.3-23.7%\n24.4-37.5%\n13.9-53.5%\n16.7-52.5%\n21.8-37.2%\n29.1-29.9%\n17.4-18.2%\n22.9-28.8%\n11.5-48.3%\n25.6-29.0%\nBestIGL\n20.0-28.1%\n21.9-43.8%\n16.9-43.4%\n18.0-48.6%\n11.9-65.6%\n14.2-65.7%\n8.9-58.3%\n12.3-61.8%\n6.3-71.7%\n12.4-65.5%\n+ BAT\n15.2-45.3%\n17.5-55.2%\n13.9-53.5%\n16.7-52.5%\n5.1-85.2%\n4.6-89.0%\n7.9-62.7%\n10.1-68.5%\n6.6-70.2%\n6.9-80.8%\nNatural IR\n50\n100\n50\n100\n50\n100\n50\n100\n50\n100\nBAcc.\u2191\nBase\n58.1\n61.8\n44.9\n44.7\n52.0\n51.1\n73.8\n71.4\n76.0\n77.7\n+ BAT\n69.1+18.9%\n68.3+10.6%\n58.4+29.9%\n57.4+28.5%\n55.6+7.0%\n56.5+10.4%\n82.1+11.3%\n81.9+14.8%\n86.9+14.3%\n84.1+8.3%\nBestIGL\n71.0+22.3%\n73.8+19.5%\n56.3+25.3%\n56.3+26.0%\n72.7+39.8%\n72.8+42.5%\n81.2+10.0%\n81.4+14.0%\n85.8+12.9%\n87.2+12.2%\n+ BAT\n73.1+25.8%\n76.9+24.5%\n62.1+38.2%\n61.3+37.3%\n75.8+45.7%\n75.9+48.5%\n85.0+15.1%\n84.5+18.5%\n88.6+16.5%\n89.7+15.4%\nMacro-F1\u2191\nBase\n58.7\n61.4\n37.5\n36.2\n47.3\n45.1\n75.3\n73.2\n78.0\n79.8\n+ BAT\n68.7+17.1%\n67.5+10.0%\n57.1+52.6%\n55.8+54.3%\n52.8+11.6%\n52.0+15.4%\n82.6+9.7%\n82.6+12.8%\n87.6+12.3%\n85.2+6.8%\nBestIGL\n71.1+21.2%\n73.4+19.5%\n54.3+44.8%\n53.8+48.8%\n72.9+53.9%\n73.7+63.6%\n82.5+9.5%\n82.4+12.6%\n87.7+12.4%\n88.3+10.6%\n+ BAT\n72.7+23.9%\n76.0+23.9%\n60.2+60.8%\n59.4+64.3%\n75.3+59.2%\n76.1+68.8%\n85.7+13.7%\n85.1+16.2%\n88.8+13.8%\n89.4+12.0%\nPerfStd\u2193\nBase\n28.8\n31.0\n38.7\n39.8\n36.2\n38.2\n26.3\n28.2\n23.8\n21.0\n+ BAT\n18.3-36.4%\n25.4-18.1%\n24.9-35.6%\n33.1-17.0%\n33.3-8.1%\n35.9-6.2%\n19.0-27.9%\n19.5-30.9%\n17.0-28.7%\n19.6-6.7%\nBestIGL\n18.9-34.4%\n17.3-44.4%\n28.7-25.9%\n29.7-25.3%\n6.0-83.4%\n9.6-75.0%\n14.4-45.4%\n15.4-45.5%\n11.2-53.1%\n9.7-53.8%\n+ BAT\n15.9-44.8%\n14.7-52.8%\n21.9-43.4%\n19.8-50.3%\n4.2-88.3%\n5.6-85.3%\n12.2-53.5%\n12.8-54.7%\n7.4-68.9%\n7.2-65.7%\n26\n",
    "2212.05478": "arXiv:2212.05478v1  [cs.CV]  11 Dec 2022\nMul-GAD: a semi-supervised graph anomaly detection framework\nvia aggregating multi-view information\nZhiyuan Liu\nScholar of Cyberspace Security\nHainan University\nHaikou, Hainan 570228\u20130898\nEmail: fyhvyhj@gmail.com\nChunjie Cao\nScholar of Cyberspace Security\nHainan University\nHaikou, Hainan 570228\u20130898\nEmail: caochunjie@hainanu.edu.cn\nJingzhang Sun\nScholar of Cyberspace Security\nHainan University\nHaikou, Hainan 570228\u20130898\nEmail: jingzhangsun@outlook.com\nAbstract\u2014Anomaly detection is de\ufb01ned as discovering patterns\nthat do not conform to the expected behavior. Previously,\nanomaly detection was mostly conducted using traditional shal-\nlow learning techniques, but with little improvement. As the\nemergence of graph neural networks (GNN), graph anomaly\ndetection has been greatly developed. However, recent studies\nhave shown that GNN-based methods encounter challenge,\nin that no graph anomaly detection algorithm can perform\ngeneralization on most datasets. To bridge the tap, we propose\na multi-view fusion approach for graph anomaly detection\n(Mul-GAD). The view-level fusion captures the extent of sig-\nni\ufb01cance between different views, while the feature-level fusion\nmakes full use of complementary information. We theoretically\nand experimentally elaborate the effectiveness of the fusion\nstrategies. For a more comprehensive conclusion, we further\ninvestigate the effect of the objective function and the number\nof fused views on detection performance. Exploiting these \ufb01nd-\nings, our Mul-GAD is proposed equipped with fusion strategies\nand the well-performed objective function. Compared with\nother state-of-the-art detection methods, we achieve a better\ndetection performance and generalization in most scenarios\nvia a series of experiments conducted on Pubmed, Amazon\nComputer, Amazon Photo, Weibo and Books. Our code is\navailable at https://github.com/liuyishoua/Mul-Graph-Fusion.\n1. Introduction\nAs a long-standing and critical subject, there is a surge of\nattention interests on anomaly detection. Recent researches\nhave widely explored on social security \ufb01eld, such as Credit\nfraud [1]\u2013[3], Rumor detection [4], [5], Network Intrusion\n[6] etc, which is directly related to civil livelihood. Industry\nin the security-critical \ufb01eld, such as medical, military and\nnational security, even regard it as a critical obstacle. Pre-\nvious efforts mostly focused on shallow learning, but with\nlittle improvement. Meanwhile, as the explosion increasing\nof graph-based method purposed, modeling anomaly pattern\nusing graph-based method has gradually become the main-\nstream paradigm. Due to powerful modeling capabilities\namong local individuals, graph neural network based (GNN-\nbased) methods not only offer a signi\ufb01cant improvement on\nanomaly detection, but also gain lots of attention.\nBrie\ufb02y, anomaly can be de\ufb01ned as patterns that do not\nconform to expected behavior. Put differently, the patterns\ndeviate signi\ufb01cantly from the majority of normal ones. Ac-\ncording to the anomalous de\ufb01nition, experts formalize uni-\n\ufb01ed anomaly detection paradigm and develop lots of func-\ntional methods. For a general taxonomy, anomaly detection\nmethod can be divided into shallow learning and GNN-based\nmethods. Shallow learning methods mostly detect outlier\nfrom assuming the prior distribution of data or observing\nthe difference of spatial density surrounding normal and\nanomaly samples. Since the obviously inductive bias and\nlack of the ability to capture the non-linearity relation, shal-\nlow learning methods can only learn the shallow anomaly\npattern of data and are hard to work effectively in the com-\nplex cases. More speci\ufb01cally, the shallow methods are able\nto detect the frequent anomaly behavior individuals, but lose\nthe power to detect the relatively normal or less anomaly be-\nhavior ones. Differently, since modeling the relation of each\nindividual is one of the properties of GNN-based methods,\nthe disguised anomalies can be detected indirectly via the\ninformation derived from their surroundings. Leveraging the\nsuperiority of nonlinear modeling of neural networks and the\nrelational modeling of graph structure, GNN-based methods\ngain increasing attention.\nAlthough GNN-based methods behave more superiority\nthan both shallow learning, existed studies [7], [8] illustrate\nthat no GNN-based anomaly detection method shows gen-\neralization performance on most datasets. Some attempts on\nmulti-view learning [9], [10] try to bridge the gap via merg-\ning heterogeneous information or mixing with community\nanalysis method. In contrast, our multi-view solution consid-\ners improving generalization performance under the setting\nof \ufb01xed information entropy, which means we optimize on\nthe method level rather than adding additional data.\nGraph anomaly detection pipeline can be divided into\nthree parts: preparing data, learning representation and de-\nsigning the objective function as shown in Fig. 1. Since\nthe model is trained on the open source graph dataset, no\nadditional graph generation operations are required. In the\npart of representation, we obtain multi-view representations\nvia multiple GNN-based methods [11]\u2013[14] and transform\nthem to a uni\ufb01ed representation via our fusion strategies. For\nthe objective function, label-oriented, which is often used\nin the semi-supervised setting, will be selected for a better\ngeneralization. After scaling the uni\ufb01ed representation to\nthe required size, we initialize the label-oriented objective\nfunction and update the entire networks using stochastic\ngradient descent. It is worth noting that our method is under\nthe setting of semi-supervised for a more realistic scenes.\nAs mentioned previously, our Mul-GAD model consider\nthe optimization in two perspective, how to choose objective\nfunction and how to design fusion strategies. We summa-\nrize the objective functions of the existing graph anomaly\ndetection and categorize them into label-oriented (semi-\nsupervised learning), reconstruction-oriented (unsupervised\nlearning), and ssl-oriented (self-supervised learning) accord-\ning to the division of supervised learning. Experiments\nshow that label-oriented function has a more generalized\nperformance in most scenarios. For the fusion solution,\nwe optimize at view and feature levels. To control the\ncontribution of each view, we set the learnable parameters\nto learn the importance of each view. Due to utilize the\nplentiful information from different views, the model shows\na better generalization. In the feature level, we utilize the\nfeature similarity matrix to make full use of complemen-\ntary information, while avoid the in\ufb02uence of redundant\ninformation. Experiments show that computing the feature\nsimilarity matrix plays an importance role in boosting the\ndetection performance. The \ufb01nal model, which equipped\nwith label-oriented objective function and fusion strategies,\nhas a signi\ufb01cant improvement on most datasets. For a more\ncomprehensive conclusion, we further dive into the in\ufb02uence\nof the number of fused views on detection performance.\nAdequate experimental validation is the foundation for\nselecting the objective function. In terms of fusion strategies,\nit can be explained theoretically via the Venn diagram, which\nis often applied to show mathematical or logical connections\nbetween different groups of things. Our contributions can be\nsummarized as follow:\n\u2022\nTo the best of our knowledge, we are the \ufb01rst to ana-\nlyze anomaly detection problem from the perspective\nof the objective functions and \ufb01nd that label-oriented\nfunction have a more generalized performance.\n\u2022\nWe provide two effective fusion strategies at the\nview and feature level. Both of them boost detection\nperformance.\n\u2022\nThe Mul-GAD approach outperforms the state-of-\nthe-art not only on detection performance, but also\nin terms of generalization across the majority of\ndatasets.\n2. Related work\nIn this section, we will introduce from the algorithm and\nobjective function aspects. For the former, anomaly detec-\ntion can be divided into shallow learning and graph neural\nnetwork methods. For objective function, anomaly detection\n)XVLRQ DW WKH IHDWXUH OHYHO\n\u000bD\f 3UHSDULQJ 'DWD\n\u000bE\f /HDUQLQJ 5HSUHVHQWDWLRQ\n\u000bF\f 'HVLJQLQJ 2EMHFWLYH )XQFWLRQ\n)XVLRQ DW WKH YLHZ OHYHO\n/DEHO\u0010RULHQWHG\n5HFRQVWUXFWLRQ\u0010RULHQWHG\n66/\u0010RULHQWHG\n*UDSK\nFigure 1: The pipeline of anomaly detection framework.\ncan be categorized to label-oriented, reconstruction-oriented\nand ssl-oriented.\n2.1. Shallow learning\nShallow learning, which differs from deep learning,\nhandles anomaly problem from spatial density, statistical\ndistribution and variants of classical machine learning meth-\nods. In general, there are fewer nodes or lower node den-\nsity around the anomalies. Spatial density methods develop\nbased on the hypothesis, a.k.a. intuition. Local outlier fac-\ntor (LOF [15]) acquires the rank of anomaly scores via\ncomputing the spatial density of each node and the lower\ndensity corresponds to a higher anomaly score. K-nearest\nneighbor (KNN [16]) seeks out the k closest neighbors\nand uses the majority class to determine the class of the\ncurrent node. Constrained by the inductive bias, such meth-\nods are hard to spot the abnormal nodes masquerading\nas the normal ones. Statistical-based algorithms distinguish\nanomalies via the assumed prior distribution or computing\nthe relevant statistical indicators. Kernel density estimation\n(KDE [17]) maintains kernel function for each node. The\n\ufb01nal estimation function can be obtained via computing\nthe mean of all kernel functions and further acquires the\nanomaly scores. Histogram-based outlier score (HBOS [18])\nconducts histogram modeling for each feature and anomaly\nscores can be obtained via integrating anomaly degrees for\nall features. Gaussian mixture model (GMM [19]), which\nis similar with KDE method, \ufb01ts the distribution of data\nvia multiple single Gaussian models. And the class of new\ncoming nodes are determined by the mixed Gaussian model.\nAlthough detecting anomalous nodes from a statistical view\nhas bene\ufb01cial on the simplicity of the algorithm and the\nfast operation speed, its validity is highly dependent on the\nprior assumption for the given data. Anomaly detection tasks\nare naturally unbalanced binary classi\ufb01cation tasks. Many\nvariants of machine learning methods have been designed\nto adapt to the kind of imbalanced binary classi\ufb01cation\ntask. One-class support vector machine (OC-SVM [20])\nderived from Support vector machine projects the raw data\ninto an in\ufb01nite-dimensional Hilbert space and distinguish\nanomalies via maximizing the soft margin. Isolation forest\n(IForest [21]) originated from decision tree utilizes binary\nsearch trees to isolate samples. However, these methods are\nstill dif\ufb01cult to preform well due to the lack of nonlinear\nmodeling capabilities.\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n1. Histogram-base Outlier Detection (HBOS)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n2. Is)la\u2212i)( F)res\u2212\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n3. L) al O.\u2212lier F\na \u2212)r (LOF)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n4. O(e-class SVM (OCSVM)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n5. Graph C)(v)l.\u2212i)(al Ne\u22120)rk (GCN)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n6. Graph Is)m)rphism Ne\u22120)rk (GIN)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n7. Graph A\u2212\u2212e(\u2212i)( Ne\u22120)rk (GAT)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n()rmal\nab()rmal\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n8. M.l\u2212i-view Graph Anomaly Detection (Our Mul- GAD)\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\nnormal\nabnormal\nFigure 2: Decision boundary for varying algorithms. The normal and abnormal samples are generated by normal distribution\nN(0, 0.3) and uniform distribution U(\u22127, 7) respectively.\n2.2. Graph neural network\nGraph neural network can be divided into message\npropagation, auto-encoder and ensemble modeling methods.\nAlthough the message propagation algorithms use the skele-\nton of graph neural network, such as Graph convolutional\nnetwork (GCN [11]), Graph attention network (GAT [12]) or\nGraph Isomorphism Network (GIN [13])), there is no essen-\ntial difference with the traditional multi-classi\ufb01cation mod-\nels. It regards anomaly detection as an unbalanced binary\nclassi\ufb01cation problem and sets weights on cross entropy\nfunction to alleviate the impact of imbalance of samples.\nThinking over from the frequency perspective, tang et.al\npurposed Beta Wavelet Graph Neural Network (BWGNN\n[14]) which ful\ufb01lled tailored spectral \ufb01lter via employing the\nbeta distribution as wave \ufb01lter. Differently, the auto-encoder\nmethods reconstruct node attributes and topology with cor-\nresponding decoder functions, and the reconstruction errors\nof nodes are exploited to discover anomalous nodes on the\nattribute network. One of the classical representatives of\nthe reconstruction techniques is Deep Anomaly Detection\non Attributed Networks (Dominant [22]). To capture the\nstructure pattern effectively, anomaly detection through a\ndual autoencoder (AnomalyDAE [23]) is boosted via apply-\ning additional attention mechanisms on structure encoder to\ncapture the signi\ufb01cance between a node and its neighbors.\nHowever, recent studies [7], [8] have shown that both mes-\nsage propagating and auto-encoder methods fail to express\ngeneralization performance on most datasets. By combining\nthe strengths of various models, the ensemble methods target\nto resolve the generalization problem. A Deep Multi-view\nFramework for Anomaly Detection (ALARM [10]) inte-\ngrates heterogeneous attribute characteristics through multi-\nple graph encoders. Haghighi et.al [10] considers ensemble\nlearning from the perspective of multi-task learning and em-\nploys the community-based representation technique to bet-\nter capture the topology structure. Despite the great advances\nmade by the existing ensemble methods, researches on how\nto fusion representation and how to design objective function\nremain in its adolescence. Our Mul-GAD framework was\nexactly purposed to bridge the generalization gap from the\naspect of fusing representation and designing the objective\nfunction. As shown in Fig. 2, our fusion method has the\nmodeling capability of GCN, GIN, GAT and is able to learn\ntheir decision boundaries.\n2.3. Objective function\nSince the design of the objective function is a crucial\ncomponent of the model pipeline, it deserves a correspond-\ning status in the modeling. However, few studies have ad-\nequately considered the design of objective function. After\nanalyzing the existing objective functions on the anomaly\ndetection, we discovered that the objective functions may be\nclassi\ufb01ed into the following three categories: label-oriented\n[11]\u2013[14], reconstruction-oriented [22], [23] and ssl-oriented\n[24], [25], according to the division of supervised learning.\nLabel-oriented treats the anomaly detection as a unbal-\nanced binary classi\ufb01cation problem and is equivalent to the\nweighted cross entropy. After transforming the uni\ufb01ed rep-\nresentations to the required attribution and topology size, the\nreconstruction-oriented function can be established via com-\nputing mean square error of the reconstruction matrix. SSL-\noriented function derives from contrastive learning [25],\n[26] and learns representation by automatically constructing\nsimilar and dissimilar instances. A detailed formulation will\nbe carried out in next section.\n3. Methodology\nIn this section, we \ufb01rst formulate the graph anomaly\ndetection and disassemble the research framework to rep-\nresentation learning and objective function designing from\nthe aspect of model pipeline. Our solutions on the two parts\nwill be detailed below.\n3.1. Problem De\ufb01nition\nWe de\ufb01ne capital letters, bold lowercase letters and\nlowercase letters to denote matrices, vectors and constants\nrespectively for simplicity of comprehension, e.g. D, d, d.\nGiven G (V, A, X) as a graph, where V denotes a series\nof nodes V = {v1, v2, ..., vn}, n is the number of nodes.\nX \u2208Rn\u00d7d indicates the attribute matrices composed by\nthe feature vector of nodes {x1, x2, ..., xn}, where d refers\nto the dimensional of xi. A \u2208Rn\u00d7n is the adjacency\nmatrices of V where aij equals to 1 if there is an edge\nbetween the i-th and j-th node, otherwise 0. Assuming that\nthe labels of the partial nodes of graph G are known as\nyL = {yl1, yl2, .., ylp}, yU = {yu1, yu2, .., yuq} represents\nthe unknown labels that we have to deduce. p, q refer the\nnumber of known and unknown labels respectively, and\np + q = n. Therefore, y = yL \u222ayU stands for the\nentire labels of G. Anomaly detection framework can be\nformulated as follow:\nF : G (V, A, X) \u2212\u2192y,\ny \u2208Rn\n(1)\nExisting approaches are in an effort to discover excel-\nlent algorithmic mappings F through various perspectives.\nAs long as the algorithm is determined, the labels y can\nbe inferred from the mapping. Following the pipeline of\nanomaly detection, we implement this mapping function F\nby combining two functions F1 Eq.2, F2 Eq.6, which stands\nfor representation learning and objective function designing\nrespectively.\n3.2. Representation learning\nThe goal of representation learning is to obtain the \ufb01nal\nrepresentation as follow:\nF1 : G (V, A, X) \u2212\u2192Xf,\nXf \u2208Rn\u00d7df\n(2)\nwhere df is the dimension of the \ufb01nal representation. In this\npart, we obtain the \ufb01nal representation by fusing multiple\nmodels. Speci\ufb01cally, we initially acquire the representations\non each model below.\nF1\u22121 : G (V, A, X) \u2212\u2192X1,\nX1 \u2208Rn\u00d7d1\nF1\u22122 : G (V, A, X) \u2212\u2192X2,\nX2 \u2208Rn\u00d7d2\n...\nF1\u2212m : G (V, A, X) \u2212\u2192Xm, Xm \u2208Rn\u00d7dm\n(3)\nwhere {F1\u22121, F1\u22122, ..., F1\u2212m} stands for m algorithms to\nextract characteristics, which may be GCN [11], GAT [12]\nor BWGNN [14], etc. After that, the multiple representations\nwould be integrated at the view and feature level.\n\u2022\u2022\n\u2022\u2022\u2022\u2022 \u2022 \u2022 !\"\u20221 # |\u2022|\u2022\n\u2297\n\u2297\n \n!\"#$%\n  \n&\n\u2297\n&\n\"#\n\"'\n\"\nFigure 3: The fusion solution at the feature level. The \ufb01rst\nrow calculates the cosine similarity among features, the\nsecond yields the importance of each feature and the third\ngains the \ufb01nal representation.\n3.2.1. Fusion at the view. In this subsection, the critical\nissue is how to fusion representations (X1, X2, ..., Xm) that\ndiffer in dimension. It can be formalized as follows:\nF1\u2212view : (X1, X2, ..., Xm) \u2212\u2192Xv,\nXv \u2208Rn\u00d7dv\n(4)\nwhere Xv refers to the fused representation and dv is its\nfeature dimension. Speci\ufb01cally, we \ufb01rst unify them into the\nsame dimension Rn\u00d7dunified by a fully connected network,\nafter which we set learnable weights (\u03b11, \u03b12, ..., \u03b1m) for\neach representation and sum them up to acquire Xv.\n3.2.2. Fusion at the feature. Despite the fact that we\nhave obtained fused representations Xv at the view level,\nthere is still redundant information among features. The key\nof fusion at the feature level is to make full use of the\ncomplementary information, while avoiding the redundant\ninformation. The formula is below.\nF1\u2212feat : Xv \u2212\u2192Xf,\nXf \u2208Rn\u00d7df\n(5)\nwhere Xf stands for the fused feature at the feature level\nand df is its corresponding dimension. The combination\nof {F1\u22121, ..., F1\u2212m, F1\u2212view, F1\u2212feat} is equivalent to the\nmapping function F1 and Xf is exactly the \ufb01nal represen-\ntation mentioned in Eq.2. In this part, we model the target\nof making full use of complementary information as well\nas avoiding redundant information by calculating feature\nsimilarity. Assuming that S \u2208Rdv\u00d7dv and w \u2208Rdv are the\nfeature similarity matrix and the weight vector respectively,\nwe model the mapping F1\u2212feat procedure as follows:\nsij =\n(Xv)i \u00b7 (Xv)j\n\u2225(Xv)i \u2225\u00b7 \u2225(Xv)j \u2225,\n1 \u2264i, j \u2264dv\nwi = 1\nn \u00b7\nn\nX\nj=1\n(1 \u2212|sij|) ,\n\u22121 \u2264sij \u22641\n(Xf)i = wi \u00b7 (Xv)i ,\n0 \u2264wi \u22641\n(6)\nwhere (Xv)i\n\u2208\nRn refers to the i-th column of the\nmatrix Xv\nand sij\ndenotes the cosine similarity be-\ntween the i-th and j-th columns. S and w are composed\nof {s11, ..., sij, ..., sdvdv},{w1, ..., wi, ..., wdv} respectively.\nThe reason we subtract |sij| from 1 is to construct an\nintuition, which the larger the value of 1 \u2212|sij| is, the\nmore irrelevant the i-th and j-th columns of Xv are. There-\nfore, they share richer complementary information and it is\nmore preferable to assign a larger weight for the column\nfeatures, which 1 \u2212|sij| can exactly accomplish. To obtain\nthe redundancy strength of a single feature over the whole\nfeature, we acquire wi by calculating the mean of S in row\nor column. The larger wi means that the i-th column features\nhave richer complementary information and less redundant\ninformation, which can be multiplied with (Xv)i to get the\n\ufb01nal representation (Xf)i. Fortunately, the above E.q 6 can\nbe reformulated by matrix, which enables acceleration by\nGPU. For the comprehension, you can refer to Fig. 3.\n3.3. Objective function designing\nThe target of objective function designing is to deduce\nthe unknown labels as follow:\nF2 : Xf \u2212\u2192y,\ny \u2208Rn\n(7)\nwhere y = yL \u222ayU denotes the whole labels of G including\nyL labels already known and yU labels to be deduced.\nWe can implement E.q.7 by the following three objective\nfunctions: label-oriented, reconstruction-oriented and ssl-\noriented. It is worth noting that since the objective functions\nare classi\ufb01ed from a supervised learning perspective, they\ncannot be used by stacking simultaneously. After acquiring\nexperimental evidence, we will choose the well-performed\nobjective functions as part of our approach.\n3.3.1. Label-oriented. Due to the semi-supervised learning,\nyL will be used for monitoring model training.\nXf\nf(\u00b7)\n\u2212\u2212\u2192p,\np \u2208Rn\nmin\nf(\u00b7) \u22121\n|yL|\n|yL|\nX\ni=1\n(\u03bb \u00b7 yi \u00b7 log pi + (1 \u2212yi) \u00b7 log(1 \u2212pi))\n(8)\nwhere p denotes the predicted anomaly scores of nodes\nand pi refers to the i-th element of p. By compressing\nXf from df to 1 dimension through multilayer perceptron,\nwe are able to obtain p. Afterwards, f(\u00b7) will be updated\nby minimizing the cross-entropy of yL with respect to the\nprediction score p. Notably, we evade the impacts of sample\nimbalance by adjusting the balance factor \u03bb. The unknown\nlabel (yU)i can be inferred as anomaly if pi is larger than\n0.5, otherwise normal.\n3.3.2. Reconstruction-oriented. Since it belongs to the\nscope of unsupervised learning, we do not adopt any known\nlabels.\nXf\nf(\u00b7)\n\u2212\u2212\u2192\n(\nXattr,\nXattr \u2208Rn\u00d7d\nAstru,\nAstru \u2208Rn\u00d7n\nmin\nf(\u00b7) \u2225Xattr \u2212X\u22252 + \u03bb \u00b7 \u2225Astru \u2212A\u22252\n(9)\nwhere Xattr, Astru are equivalent to X, A after construction\nand \u03bb is the trade-off between the attribute and structure ma-\ntrices. We optimize the multilayer perceptron f(\u00b7) by mini-\nmizing the reconstruction errors of the attribute and structure\nmatrices. For a speci\ufb01c node vi, we obtain its anomaly score\nvia calculating \u2225(Xattr)i \u2212Xi\u22252 + \u03bb \u00b7 \u2225(Astru)i \u2212Ai\u22252.\nDepending on the ranking of the anomaly scores, the nodes\ncan be classi\ufb01ed as normal or abnormal, thus obtaining the\nwhole labels y.\n3.3.3. SSL-oriented. Self-supervised learning does not ex-\npose known labels yL, however constructs positive and\nnegative instances to learn representations. Speci\ufb01cally, we\napply Deep Graph Infomax (DGI [25]) to initialize the SSL-\noriented objective function.\nXf\nf(\u00b7)\n\u2212\u2212\u2192\n(\nH,\nH \u2208Rn\u00d7dh\n\u02c6H,\n\u02c6H \u2208Rn\u00d7dh\ns = \u03c3\n \n1\nn\nn\nX\ni=1\nhi\n!\nmin\nf(\u00b7) \u22121\n2n\nn\nX\ni=1\n\u0010\nlog D(hi, s) + log\n\u0010\n1 \u2212D(\u02c6hi, s)\n\u0011\u0011\n(10)\nwhere H,\n\u02c6H consist of {h1, h2, ..., hn},{\u02c6h1, \u02c6h2, ..., \u02c6hn}\nrespectively and D is a discriminator which produces the\naf\ufb01nity score for each local-global pair. \u03c3 indicates the\nsigmoid activation function. With the mapping function f(\u00b7),\nXf is projected to two identical algebraic spaces and yields\nH, \u02c6H. Obtaining from the pooling operation, s can be\nregarded as the global representation of H. Thereby, the\nconstructed positive instance H has a higher af\ufb01nity with\ns than the negative instance \u02c6H. By modeling the intuition,\nit is possible to accomplish self-supervised learning in the\nno-label case and we are able to obtain a well-performed\nrepresentation H by updating f(\u00b7). To adapt the detection\ntask, we connect a two-dimensional linear classi\ufb01er after\nH. Afterward, anomaly scores can be gained by \ufb01ne-tuning\nf(\u00b7) as well as training the linear model.\n3.4. Mul-GAD detection method\nAs shown in Fig. 4, the pipeline of our Mul-GAD\nmethod can be divided into three parts: (a) preparing data,\n9LHZ\u0003\u0014\n9LHZ\u0003\u0015\n9LHZ\u0003P\n\u2022\u2022\u2022\u2022\u2022\u22c5\u2022\n\u2022\u2022\u2022\u2022\u2022\u22c5\u2022\n\u2022\u2022\u2022\u2022\u2022\u22c5\u2022\n\u2022\u2022\u2022, \u2022, \u2022\u2022\n\u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\n\u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\n\u0293 )XVLRQ\u0003DW\u0003WKH\u0003IHDWXUH\u0003OHYHO\n\u0292 )XVLRQ\u0003DW\u0003WKH\u0003YLHZ\u0003OHYHO\n\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u22c5\u2022\n\u2022\n !\u2217#$\n%&\n%'\n%(\n !\u2217#)\n !\u2217#*\n%+\n ,\u2217-#$.#)....#*0\n !\u2217-1$.1)....120\n #3\n\u2022&45678-\u22c50\n%+\n%:\n\u000bE\f /HDUQLQJ\u0003UHSUHVHQWDWLRQ\n\u000bF\f 'HVLJQLQJ\u0003REMHFWLYH\u0003IXQFWLRQ\n\u000bD\f 3UHSDULQJ\u0003GDWD\n/DEHO\u0010RULHQWHG\n5HFRQVWUXFWLRQ\u0010RULHQWHG\n66/\u0010RULHQWHG\n;<\n=\n>?<\n>\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\n9L\nFigure 4: The overview of framework of anomaly detection model Mul-GAD. It is divided into three main modules: (a)\npreparing data, (b) learning representation and (c) designing objective function. After processing (a) and (b), we constrict the\nrepresentations to the speci\ufb01ed size to adapt the objective function. Finally, we update the model parameters by minimizing\nthe objective function.\n(b) learning representation and (c) designing objective func-\ntion. In this paper, no additional graph data preparing oper-\nation is required due to the use of open source datasets.\nHowever, data preparation is particularly critical in the\nspeci\ufb01c business scenarios, since the quality of data deter-\nmines the upper bound of the algorithm. For the learning\nrepresentation (b), We \ufb01rst obtain multi-view representa-\ntions through multiple GNN-based methods (GCN [11],\nGAT [12], BWGNN [14] or others), followed with fusion\nsolutions at the view and feature level. View-level fusion is\nconducted to measure the contribution of each method, while\nfeature-level fusion leverages complementary information\nand avoid the impact of redundant information. After the\nfusion representations, we need to choose the objective func-\ntion (c) from three commonly used graph anomaly detection\nobjective functions (label-oriented, reconstruction-oriented\nand ssl-oriented). Experiments show that the label-oriented\nobjective function performs well and has a more generaliza-\ntion performance on most datasets. Therefore, the Mul-GAD\nmethod selects the label-oriented objective function, which\nis typically used in the semi-supervised learning setting. The\nspeci\ufb01c formula can be obtained in the previous subsection.\n3.5. Venn diagram explanation\nIn general, it is believed that there exists upper bound\nof the algorithm for a given data. Hence, the model has a\nperformance bottleneck and We formulate the description\nas:\n\u2200f(\u00b7), P (f (G(V, A, X))) < \u03b1, 0 < \u03b1 \u22641\n(11)\nwhere f(\u00b7) indicates the detection algorithm and P refers to\nthe detection accuracy. Even if we iterate through the entire\nalgorithm space, the detection accuracy cannot exceed the\nupper bound \u03b1. Thinking differently, we suppose that there\n*UDSK GRPDLQ \u2022\u2022\u2022\u2022\u2022\nD GRPDLQ\nE GRPDLQ\nF GRPDLQ\n\u2022\u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\n \u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\n!\u2022\u2022\u2022\u2022, \u2022, \u2022\u2022\u2022\nFigure 5: Schematic diagram to explain the motivation of\nour fusion solution.\nexists \u201cintrinsic representation\u201d of G, which is able to reach\nthe upper bound \u03b1 and can be separated by the simplest\nlinear model. We formulate as follows:\nP (flinear (Gintr)) = \u03b1\n(12)\nThus, the goal turns to seek for the \u201cintrinsic representation\u201d\nof G, i.e. Gintr. Venn diagram as shown in Fig. 5, which is\noften applied to show mathematical or logical connections\nbetween different groups of things, is adopted to explain the\nmotivation of our fusion solutions. The \u201cintrinsic represen-\ntation\u201d can be regarded as the blue area of Gintr. Using the\nA(\u00b7), B(\u00b7), C(\u00b7) algorithm on the original graph G(V, A, X)\nyields a, b, c subdomain. a, b subdomains overlap due to the\nsimilarity property of the algorithm (the overlapping parts\ndenote redundant information), while c, being independent\nof a and b, has suf\ufb01cient complementary information. These\nsub-domains may be contextual, structured, frequency infor-\nmation or whatever. Therefore, an intuitive idea comes out\nwhether we can gather more complementary information by\nmeans of multi-task learning, while designing modules to\navoid the in\ufb02uence of redundant information. Our fusion\nsolutions are derived from the intuitive, striving to approx-\nimate the dataset domain Gintr.\n4. Experiments and Results\nIn this section, we purpose four valuable research ques-\ntions.\n\u2022\nRQ1: How do the different objective functions per-\nform?\n\u2022\nRQ2: What is the impact of two fusion solutions on\ndetection performance?\n\u2022\nRQ3: Whether the more views are fused, the higher\nthe detection rate improvement?\n\u2022\nRQ4: Can the \ufb01nal proposed method solve the prob-\nlem that existing methods are hard to generalize on\nmost datasets?\nTo address these issues, we design a series of experiments.\nWe initial with a basic experimental setup, following with\nexperiments to explore the impact of the objective function\nand the two fusion methods on performance. To obtain\nrigorous conclusions, ablation study is performed to discover\nthe effects between the two fusion methods. Moreover, the\nimpact of the number of fusion methods is studied. As a\nresult, our method will be compared with the state-of-the-\nart methods.\n4.1. Experimental setting\n4.1.1. Datasets. We used \ufb01ve popular real-world attributed\ngraph datasets, which are public datasets widely used in\nprevious studie [7], [31], [32]. The anomalous tags in Weibo\nand Books are already labeled, while Pubmed, Amazon\nComputer and Amazon Photo generate anomalies by human\nconstruction since there is no ground truth of anomalies\nin these datasets. The mainstream way of constructing is\nminimal class and synthetic methods. Detail statistics are\nlisted in Table. 1.\n\u2022\nPubmed [27] dataset consists of 19717 scienti\ufb01c\npublications related to diabetes in the PubMed\ndatabase with 44338 links.\n\u2022\nAmazon Computer and Photo [28] are segments\nof the Amazon co-purchase graph [33], where nodes\nrepresent goods, edges indicate that two goods are\nfrequently bought together.\n\u2022\nWeibo [30] is a hashtag graph of user posts from a\nTwitter-like platform. It has 8,405 users and 61,964\nsubject tags. The weight of the user-hashtag is the\nnumber of speci\ufb01c hashtags posted by the user.\n\u2022\nBooks [29] is a second graph from the Amazon\nnetwork, in which we use tags provided by Amazon\nusers.\nMinimal class anomalies are made by treating the smallest\nnumber of classes in the original data as the anomalous\nclass and the rest as normal, following with [32], [34], [35]\nworks. Synthetic anomalies can be obtained by generating\nstructure and attribution anomalies, following with [36],\n[37]. Organic anomalies have been manually marked, which\nrequires no additional manipulation. For a fair comparison,\nwe divide the training, validation and test sets as 4:3:3 on\nall datasets. The anomalous ratio for synthetic anomalies\nis set to 0.1, while the other two do not require to be set\nsince the anomalous ratio is originally owned. The reason\nwe use minimal class and synthetic anomalies when we\nhave organic anomalies is to demonstrate that our methods\nhave outstanding generalization whether on simulated or real\ndata. In this paper, we obtain the synthetic anomalies via A\nPython Library for Graph Outlier Detection (Pygod [38]).\n4.1.2. Baseline. We compare with two categories of base-\nlines, shallow learning and GNN-based method. According\nto the division of shallow learning, we employ the LOF [15]\nas a representative of spatial density method, the HBOS\n[18] to represent the statistical distribution method and\nIForest [21], OC-SVM [20] to stand for the variant of the\nclassical machine learning method. To avoid duplicate wheel\nbuilding, we adopt Python Outlier Detection (PyOD [39]) to\nimplement these algorithms and use the default parameters.\nFor the GNN-based approach, we use the commonly used\nbackbone GCN [11], GIN [13] and GAT [12] to extract the\nfeature information of the graph. In addition, considering\nthe excellent performance of BWGNN [14] on the most\ndatasets, it will also be applied as a baseline. It is worth not-\ning that all the GNN-based methods establish label-oriented\nobjective functions to optimize the network parameters.\n4.1.3. Parameter Setting. For the shallow learning base-\nline, we use the default hyper-parameters in PyOD library\n[39]. In the GNN-based setting, we apply the two-layer\nnetworks, following [11], [14], [40]. Learning rate is set\nto 5e-3 and the dimension of the hiddle layer is set to 64. A\nMethod for Stochastic Optimization (Adam [41]) is adopted\nas the optimizer, where the weight decay is set to 5e-2. For\na fair comparison, we employ early stop strategy, which\nstops the training if the loss of the validation set does not\nincrease in 20 epochs. This allows the models to converge\nto an optimal level. To enhance the computational ef\ufb01ciency\nof graph modeling, we use the PyTorch Geometric [42] to\nconstruct the graph network.\n4.1.4. Evaluation. In general, accuracy, recall, precision, f1\nscore and the area under the curve (AUC) are often applied\non multi-classi\ufb01cation tasks. Since anomaly detection is an\nunbalanced binary classi\ufb01cation task, accuracy does not pre-\ncisely describe the detection performance of the algorithm.\nAnd some anomaly detection algorithms can only obtain\nthe ranking of the anomaly scores instead of the anomaly\nprobability of the current sample, which leads to the inability\nto compute recall, precision and f1 score. Hence, we adopt\nAUC as an evaluation metric for anomaly detection, which is\nobtained via computing the area under the receiver operating\ncharacteristics. To acquire stable results, we take the mean\nvalue over \ufb01ve runs in all experiments. And the standard\ndeviation (STD) is used to describe the stability of the\nTABLE 1: Statistics of the datasets. Min ratio, syn ratio denote the anomalous ratio generated by the minimal class and\nsynthetic methods respectively, while org ratio means the organic ones. The number of anomalies is described in the brackets.\nGraph\nNodes\nEdges\nFeatures\nClasses\nMin ratio\nSyn ratio\nOrg ratio\nPubmed [27]\n19,717\n88,648\n500\n3\n20.81% (4,103)\n10% (1,972)\n-\nAmazon Computer [28]\n13,752\n491,722\n767\n10\n2.12% (292)\n10% (1,375)\n-\nAmazon Photo [28]\n7,650\n238,162\n745\n8\n4.33% (331)\n10% (765)\n-\nBooks [29]\n1,418\n3,695\n21\n2\n-\n-\n1.97% (28)\nWeibo [30]\n8,405\n407,963\n400\n2\n-\n-\n10.33% (868)\nTABLE 2: The detecting results of the different objective\nfunction AUC/STD (%) over \ufb01ve runs among various back-\nbones, on two types of anomalies for Pubmed.\nType.\nLoss. Backbone Rec-oriented SSL-oriented Label-oriented\nMin\nGIN\n89.2\u00b10.2\n92.2\u00b10.0\n92.3\u00b10.0\nGCN\n88.5\u00b10.4\n89.8\u00b10.2\n91.7\u00b10.1\nGAT\n80.3\u00b115.1\n90.0\u00b10.3\n92.2\u00b10.0\nBWGNN\n53.5\u00b11.4\n86.4\u00b115.9\n95.9\u00b10.0\nSyn\nGIN\n74.2\u00b10.4\n78.3\u00b10.9\n80.1\u00b10.0\nGCN\n68.9\u00b14.9\n73.5\u00b10.2\n76.7\u00b10.8\nGAT\n57.4\u00b18.2\n75.0\u00b12.7\n86.5\u00b10.3\nBWGNN\n69.2\u00b11.5\n83.6\u00b10.3\n84.7\u00b10.8\nresults. In addition, we obtain the best result of the validation\nset during the training process to describe the potential of\nthe model.\n4.2. In\ufb02uence of The Objective Function\nTo explore the impact of different objective functions\non the detection performance RQ1, we compare various\nGNN-based methods as shown in Table. 2. Rec-oriented in\nthe table denotes reconstruction-oriented. The results show\nthat the label-oriented objective function is about 3 to 4\npercentage points higher than the others. Moreover, its de-\ntection performance is more stable due to the lower standard\ndeviation. Hence, in the case of GNN-based skeletons, we\ndo not recommend the usage of reconstruction-oriented and\nssl-oriented. Instead, label-oriented is advised to adopt in an\nunknown situation. Our \ufb01nal Mul-GAD method will adopt\nlabel-oriented as our objective function.\n4.3. In\ufb02uence of Fusion Solutions\nWe investigate the impact of view-level and feature-level\nfusion mechanisms on detection performance to respond the\nRQ2.\n4.3.1. View-level Fusion. As shown in Fig. 6, the \ufb01rst\nand second row use Pubmed and Amazon Photo datasets\nrespectively with two anomaly types (minimal class and\nsynthetic method from left to right). Both of them calculate\nthe detection rate on the hybrid model of GCN and GAT, the\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.97\n0.97\n0.87\n0.97\n0.94\n0.93\n0.94\n0.95\n0.95\n0.65\n0.96\n0.96\n0.96\n0.95\n0.9\n0.96\n0.97\n0.92\n0.97\n0.83\n0.97\n0.97\n0.97\n0.97\n0.94\n0.94\n0.94\n0.96\n0.96\n0.97\n0.97\n0.97\n0.94\n0.96\n0.97\n0.97\n0.95\n0.97\n0.94\n0.92\n0.97\n0.97\n0.97\n0.96\n0.95\n0.96\n0.96\n0.94\n0.95\n0.8\n0.95\n0.97\n0.97\n0.97\n0.8\n0.97\n0.96\n0.96\n0.97\n0.89\n0.96\n0.97\n0.98\n0.96\n0.94\n0.97\n0.95\n0.89\n0.95\n0.89\n0.94\n0.95\n0.97\n0.93\n0.97\n0.96\n0.97\n0.94\n0.92\n0.96\n0.96\n0.97\n0.96\n0.97\n0.97\n0.97\n0.97\n0.94\n0.95\n0.94\n0.96\n0.96\n0.98\n0.95\n0.97\n0.97\n0.97\n0.97\n0.97\n0.96\n0.96\n0.96\n0.93\n0.97\n0.94\n0.96\n0.97\n0.95\n0.97\n0.97\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.67\n0.63\n0.67\n0.6\n0.51\n0.55\n0.65\n0.66\n0.5\n0.56\n0.71\n0.67\n0.66\n0.56\n0.57\n0.54\n0.71\n0.64\n0.6\n0.57\n0.67\n0.67\n0.64\n0.68\n0.69\n0.64\n0.68\n0.67\n0.62\n0.54\n0.72\n0.67\n0.6\n0.6\n0.64\n0.49\n0.55\n0.54\n0.62\n0.57\n0.68\n0.62\n0.63\n0.71\n0.65\n0.58\n0.53\n0.6\n0.52\n0.6\n0.68\n0.7\n0.66\n0.69\n0.65\n0.51\n0.58\n0.62\n0.64\n0.52\n0.69\n0.71\n0.56\n0.71\n0.52\n0.65\n0.71\n0.6\n0.57\n0.64\n0.71\n0.67\n0.6\n0.55\n0.61\n0.71\n0.6\n0.61\n0.58\n0.7\n0.7\n0.7\n0.6\n0.66\n0.55\n0.7\n0.66\n0.61\n0.65\n0.61\n0.66\n0.71\n0.6\n0.69\n0.61\n0.66\n0.61\n0.63\n0.52\n0.58\n0.68\n0.63\n0.68\n0.72\n0.71\n0.68\n0.59\n0.69\n0.68\n0.71\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.93\n0.89\n0.88\n0.88\n0.88\n0.95\n0.9\n0.91\n0.92\n0.85\n0.94\n0.93\n0.93\n0.82\n0.85\n0.88\n0.9\n0.89\n0.87\n0.87\n0.97\n0.93\n0.93\n0.96\n0.94\n0.94\n0.88\n0.84\n0.91\n0.9\n0.94\n0.94\n0.94\n0.95\n0.95\n0.94\n0.91\n0.9\n0.94\n0.94\n0.9\n0.94\n0.94\n0.96\n0.96\n0.94\n0.95\n0.92\n0.9\n0.94\n0.93\n0.96\n0.92\n0.93\n0.94\n0.94\n0.92\n0.91\n0.91\n0.95\n0.93\n0.94\n0.93\n0.93\n0.94\n0.94\n0.93\n0.95\n0.95\n0.94\n0.94\n0.94\n0.91\n0.96\n0.93\n0.93\n0.95\n0.97\n0.91\n0.95\n0.91\n0.93\n0.95\n0.93\n0.94\n0.93\n0.91\n0.94\n0.96\n0.91\n0.93\n0.94\n0.96\n0.95\n0.94\n0.92\n0.93\n0.96\n0.94\n0.95\n0.93\n0.94\n0.94\n0.93\n0.92\n0.93\n0.9\n0.97\n0.96\n0.96\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n0.54\n0.62\n0.62\n0.62\n0.62\n0.62\n0.56\n0.57\n0.56\n0.63\n0.63\n0.62\n0.64\n0.58\n0.56\n0.59\n0.58\n0.62\n0.63\n0.63\n0.62\n0.63\n0.62\n0.56\n0.56\n0.56\n0.56\n0.63\n0.57\n0.63\n0.62\n0.57\n0.61\n0.63\n0.62\n0.56\n0.63\n0.62\n0.63\n0.57\n0.62\n0.62\n0.62\n0.62\n0.62\n0.57\n0.63\n0.63\n0.63\n0.58\n0.55\n0.63\n0.59\n0.62\n0.55\n0.63\n0.63\n0.57\n0.63\n0.6\n0.57\n0.62\n0.56\n0.56\n0.59\n0.61\n0.62\n0.63\n0.62\n0.57\n0.62\n0.55\n0.62\n0.56\n0.58\n0.63\n0.57\n0.57\n0.62\n0.62\n0.62\n0.56\n0.62\n0.63\n0.63\n0.59\n0.63\n0.62\n0.56\n0.62\n0.63\n0.62\n0.62\n0.63\n0.62\n0.62\n0.63\n0.62\n0.63\n0.63\n0.63\n0.62\n0.62\n0.63\n0.63\n0.63\n0.62\n0.57\n0.55\n0.56\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.65\n0.66\n0.67\n0.68\n0.69\n0.92\n0.93\n0.93\n0.94\n0.94\n0.95\n0.95\n0.61\n0.61\n0.62\n0.62\n0.63\nFigure 6: Heat map of the fusion model detection rate AUC\n(%).\nx and y axes represent their corresponding fusion weights.\nDarker colors, which corresponds to a higher detection rate,\ncan be observed in the interior and on the diagonal of the\nheat map. This means that fusion between GCN and GAT\nmethods does increase by 2 to 3 percents on the detection\nrate, but how to assign the optimal weights remains a\nproblem. In this paper, we allow the neural network to learn\nhow to assign the weights between different methods or\nviews. However, the learned weights are still locally optimal.\nThe following feature-level fusion can be considered as an\nalternative resolution for \ufb01nding the optimal parameters.\nStill, it is worthwhile to explore more deeply on how to\napproximate the optimal weights.\n4.3.2. Feature-level Fusion. To explore the impact of\nfeature-level fusion on performance, we compare unfused\nas well as feature-level fusion approaches as shown in Fig.\n7. Experiments reveal that the method with feature-level\nfusion consistently increases by 2 to 3 on AUC percentage\npoints. Even, the improvement is nearly 5 on the Amazon\nPhoto. For the reliability of the results, we make experiments\non the combination of bwgnn, gin and bwgnn, gin, gat\nmethod, which correspond to the top and bottom bar charts\nrespectively. As explained previously, feature-level fusion\nworks because it takes full advantage of the complementary\npubmed\namazon_computer\namazon_photo\n80\n85\n90\n95\n100\nDetection rate (%)\n94.6\n92.6\n90.5\n95.9\n98.3\n95.1\nNo fusion\nFeature fusion\nPubmed\nAmazon computer\nAmazon photo\nDatasets\n88\n90\n92\n94\n96\n98\n100\nDetection rate (%)\n95.5\n95.8\n90.6\n95.8\n97.5\n96.2\nFigure 7: Comparison with no fusion and feature fusion on\ndetection rate AUC (%). The top one uses the combination\nof bwgnn, gin, while the bottom applies the fusion of bwgnn,\ngin and gat methods. Both of them verify on the minimal\nclass anomalies of Pubmed, Amazon Computer and Amazon\nPhoto.\n\u0005\n\u0006\n\u0007\n\b\n\u0014\u001c\u001a\u0000\u0012%\u001e\u0017\u001a\"\u0000 \u001b\u0000&\u001d\u001a'#\n\r\u0006\n\r\u0007\n\r\b\n\r\t\n\r\n\r\u000b\n\r\f\n\r\r\n\u0005\u0004\u0004\n\u0010\u001a$\u001a\u0018$\u001d \u001f\u0000\"\u0016$\u001a\u0000\u0002\u0001\u0003\n\u0013%\u0017\u001e\u001a\u0019\u0000\u0011\u001d\u001f\n\u000e\u001e\u0016( \u001f\u0000\u000f \u001e!%$\u001a\"\u0000\u0011\u001d\u001f\n\u000e\u001e\u0016( \u001f\u0000\u0013\u001c $ \u0000\u0011\u001d\u001f\n\u0015\u001a\u001d\u0017 \nFigure 8: Line chart of the number of fusion methods on\ndetection rate AUC (%).\ninformation, while avoiding the impact of redundant infor-\nmation on detection performance.\n4.4. In\ufb02uence of The View Numbers\nTo address the RQ3, we explore the impact of the\nnumber of fused views on detection performance. Since\nwe are provided with the number of fused views instead\nof the speci\ufb01c fused views, it is tough to obtain an accurate\nAUC. Regarding this, we maintain a method pool containing\nall available methods, such as {gin, gcn, gat, bwgnn}. To\nget the detection AUC of 2 views, we randomly pick two\nmethods from the method pool. The \ufb01nal AUC is the mean\nAUC over \ufb01ve times sampling. As shown in Fig. 8, we\nconducted experiments on four datasets, where Pubmed Min\nrefers to the minimal class anomalies of Pubmed. The trend\nof the line graph indicates that the detection AUC gradually\nincreases as the number of fusion views rises. However, it\nis not the case that the more views are fused, the better the\nperformance will be, one such case is minimal class anoma-\nlies of Amazon Computer. After 2 views, the detection AUC\nis gradually decreasing. It is noticeable that the size of each\nmarker, which is used to express the standard deviation of\nthe AUC, is varied. The increment in the number of fused\nviews leads to a smaller marker, which means the algorithm\nis more stable.\n4.5. Comparison with existing methods\nWe compare our method with two categories of base-\nlines, shallow learning and GNN-based methods. To an-\nswer the RQ4, we conduct a series of experiments on\n\ufb01ve datasets, including minimal class, synthetic and organic\nanomalies, over \ufb01ve seeds. As shown in Table. 3, AUC is\nobtained through the test set and the content in the bracket\nrefers to the best AUC for the validation set, which spot\nthe potential of the algorithm. It is not the case that the\nmore views are fused, the better the model performance\nwill be. Thus, we regard the best results in the combi-\nnation [gat,gin], [bwgnn, gat], [bwgnn, gin] and [bwgnn,\ngat, gin] as the \ufb01nal AUC for our method. Experimental\nresults show that our Mul-GAD method outperforms the\nother detection methods on human-constructed anomalous\ndatasets (Pumbed, Amazon Photo). To make the results\nmore convincing, we use the organic anomalous datasets\n(Weibo and Books) for inspection. Although our detection\nrate is not the highest on Amazon Computer, the gap is\nnot large at least. Moreover, we promote the detection rates\nby a large margin on the synthetic anomalies of Pubmed\n,Amazon Photo and the organic anomalous dataset Books.\nIn summary, our Mul-GAD method approach outperforms\nthe state-of-the-art not only on detection performance, but\nalso in terms of generalization across the majority of datasets\ncompared to the existing state-of-the-art methods.\n5. Conclusion\nTo address the challenge that existing methods are\npoorly generalized on most datasets, we propose a multi-\nview fusion algorithm for graph anomaly detection (Mul-\nGAD). Our fusion strategies include view-level and feature-\nlevel. View-level fusion learns the contribution of each view\nto the detection performance, while feature-level fusion is\nutilized to model intuition that complementary information\nshall be made full use and redundant information ought\nto be neglected. Moreover, we summarize the objective\nTABLE 3: Comparison with the shallow learning and GNN-based methods. The detecting results AUC\u00b1STD (Validation\nbest results) (%) over \ufb01ve seeds, among minimal class, synthetic and organic anomalies.\nAlg.\nData.\nPumbed\nAmazon Computer\nAmazon Photo\nWeibo\nBooks\nMin.\nSyn.\nMin.\nSyn.\nMin.\nSyn.\nIForest [21]\n50.9\u00b12.3(50.6) 38.7\u00b11.8(40.1) 48.1\u00b10.8(48.3) 51.4\u00b12.1(54.0)\n53.1\u00b12.3(51.7)\n51.9\u00b12.3(54.0)\n47.1\u00b12.5(46.6)\n60.0\u00b12.1(62.8)\nLOF [15]\n51.8\u00b10.0(52.1) 65.4\u00b10.0(64.4) 53.7\u00b10.0(47.7) 59.4\u00b10.0(59.9)\n68.8\u00b10.0(67.0)\n65.7\u00b10.0(57.7)\n61.5\u00b10.0(59.9)\n45.0\u00b10.0(43.3)\nHBOS [18]\n48.9\u00b10.0(48.6) 34.3\u00b10.0(30.8) 50.0\u00b10.0(42.6) 47.1\u00b10.0(47.1)\n52.2\u00b10.0(56.5)\n42.2\u00b10.0(45.3)\n28.0\u00b10.0(33.8)\n70.7\u00b10.0(60.5)\nOCSVM [20]\n57.1\u00b10.0(56.8) 75.9\u00b10.0(74.1) 46.8\u00b10.0(47.5) 75.2\u00b10.0(74.1)\n51.5\u00b10.0(50.6)\n75.9\u00b10.0(76.0)\n80.7\u00b10.0(83.5)\n14.8\u00b10.0(27.3)\nGIN [11]\n91.3\u00b10.1(91.6) 76.3\u00b10.2(75.5) 95.6\u00b10.5(95.5) 50.5\u00b18.9(60.1) 71.0\u00b120.9(86.8) 53.0\u00b110.2(57.8) 94.5\u00b10.5(95.1)\n50.0\u00b10.0(52.1)\nGAT [12]\n92.1\u00b10.1(91.9) 81.7\u00b10.9(82.9) 98.9\u00b10.0(99.5) 68.5\u00b10.4(68.1)\n94.9\u00b10.9(97.2)\n68.4\u00b10.9(68.7)\n89.4\u00b12.6(90.0)\n50.0\u00b10.0(56.2)\nGCN [11]\n91.5\u00b10.1(92.3) 80.6\u00b10.2(80.6) 99.2\u00b10.0(98.5) 69.2\u00b10.2(70.4)\n97.2\u00b10.1(95.2)\n66.4\u00b10.3(66.7)\n97.4\u00b10.2(98.2)\n50.0\u00b10.0(50.0)\nBWGNN\n[14]\n95.3\u00b10.0(95.1) 83.4\u00b10.2(83.1) 99.6\u00b10.0(99.0) 74.7\u00b10.4(77.8)\n97.1\u00b10.5(96.8)\n68.2\u00b10.4(76.3)\n93.2\u00b10.3(97.3)\n56.5\u00b16.7(77.3)\nMul-GAD\n(Ours)\n95.6\u00b10.0(94.6) 91.3\u00b10.6(88.9) 99.0\u00b10.0(99.2) 74.3\u00b12.2(75.4)\n98.7\u00b10.1(97.5)\n76.5\u00b10.9(77.6)\n97.8\u00b10.1(97.4) 72.0\u00b111.6(47.9)\nfunctions for the existing graph anomaly detection meth-\nods and further analyze the impact of different objective\nfunctions. For the integrity, the number of fused views is\ninvestigated. Exploiting these experimental evidences, we\npropose the Mul-GAD equipped with fusion strategies and\nlabel-oriented objective function. A theoretical justi\ufb01cation\nof the fusion strategy is provided via Venn diagram for\nrigorous. Although our method outperforms other state-of-\nthe-art anomaly detection methods in most scenarios, the\ndetectors are still faced with a more complex and unknown\nsituations. Extending our method to more realistic settings\n(e.g., heterogeneous or dynamic graph) is crucial. Exploring\nhow to approximate the optimal parameters for view-level\nfusion is also a worthwhile research subject.\nAcknowledgments\nThis work was supported by the National Key Research\nand Development Program of China (No.2021YFB2700600)\nand the National Natural Science Foundation of China\nEnterprise\nInnovation\nand\nDevelopment\nJoint\nFund\n(No.U19B2044).\nCon\ufb02icts of Interest\nThe authors declare that there are no con\ufb02icts of interest\nregarding the publication of this paper.\nReferences\n[1]\nS. Xuan, G. Liu, Z. Li, L. Zheng, S. Wang, and C. Jiang, \u201cRandom\nforest for credit card fraud detection,\u201d in 2018 IEEE 15th inter-\nnational conference on networking, sensing and control (ICNSC).\nIEEE, 2018, pp. 1\u20136.\n[2]\nJ. I.-Z. Chen and K.-L. Lai, \u201cDeep convolution neural network\nmodel for credit-card fraud detection and alert,\u201d Journal of Arti\ufb01cial\nIntelligence, vol. 3, no. 02, pp. 101\u2013112, 2021.\n[3]\nR. Asha and S. K. KR, \u201cCredit card fraud detection using arti\ufb01cial\nneural network,\u201d Global Transitions Proceedings, vol. 2, no. 1, pp.\n35\u201341, 2021.\n[4]\nZ. He, C. Li, F. Zhou, and Y. Yang, \u201cRumor detection on social media\nwith event augmentations,\u201d in Proceedings of the 44th International\nACM SIGIR Conference on Research and Development in Information\nRetrieval, 2021, pp. 2020\u20132024.\n[5]\nA. Lao, C. Shi, and Y. Yang, \u201cRumor detection with \ufb01eld of linear\nand non-linear propagation,\u201d in Proceedings of the Web Conference\n2021, 2021, pp. 3178\u20133187.\n[6]\nD. Chou and M. Jiang, \u201cA survey on data-driven network intrusion\ndetection,\u201d ACM Computing Surveys (CSUR), vol. 54, no. 9, pp. 1\u201336,\n2021.\n[7]\nK. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen,\nH. Peng, K. Shu et al., \u201cBenchmarking node outlier detection on\ngraphs,\u201d arXiv preprint arXiv:2206.10071, 2022.\n[8]\nX. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and\nL. Akoglu, \u201cA comprehensive survey on graph anomaly detection\nwith deep learning,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2021.\n[9]\nZ. Peng, M. Luo, J. Li, L. Xue, and Q. Zheng, \u201cA deep multi-\nview framework for anomaly detection on attributed networks,\u201d IEEE\nTransactions on Knowledge and Data Engineering, 2020.\n[10] V. Haghighi, B. Soltani, A. Mahmood, Q. Z. Sheng, and J. Yang,\n\u201cGcn-based multi-task representation learning for anomaly detection\nin attributed networks,\u201d arXiv preprint arXiv:2207.03688, 2022.\n[11] M. Welling and T. N. Kipf, \u201cSemi-supervised classi\ufb01cation with graph\nconvolutional networks,\u201d in J. International Conference on Learning\nRepresentations (ICLR 2017), 2016.\n[12] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY. Bengio, \u201cGraph attention networks,\u201d stat, vol. 1050, p. 20, 2017.\n[13] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are\ngraph neural networks?\u201d in International Conference on Learning\nRepresentations, 2018.\n[14] J. Tang, J. Li, Z. Gao, and J. Li, \u201cRethinking graph neural networks\nfor anomaly detection,\u201d arXiv preprint arXiv:2205.15508, 2022.\n[15] M. M. Breunig, H.-P. Kriegel, R. T. Ng, and J. Sander, \u201cLof: identi-\nfying density-based local outliers,\u201d in Proceedings of the 2000 ACM\nSIGMOD international conference on Management of data, 2000, pp.\n93\u2013104.\n[16] S. Ramaswamy, R. Rastogi, and K. Shim, \u201cEf\ufb01cient algorithms for\nmining outliers from large data sets,\u201d in Proceedings of the 2000 ACM\nSIGMOD international conference on Management of data, 2000, pp.\n427\u2013438.\n[17] L. J. Latecki, A. Lazarevic, and D. Pokrajac, \u201cOutlier detection with\nkernel density functions,\u201d in International Workshop on Machine\nLearning and Data Mining in Pattern Recognition.\nSpringer, 2007,\npp. 61\u201375.\n[18] M. Goldstein and A. Dengel, \u201cHistogram-based outlier score (hbos):\nA fast unsupervised anomaly detection algorithm,\u201d KI-2012: poster\nand demo track, vol. 9, 2012.\n[19] C. C. Aggarwal, \u201cAn introduction to outlier analysis,\u201d in Outlier\nanalysis.\nSpringer, 2017, pp. 1\u201334.\n[20] B. Sch\u00a8olkopf, J. C. Platt, J. Shawe-Taylor, A. J. Smola, and R. C.\nWilliamson, \u201cEstimating the support of a high-dimensional distribu-\ntion,\u201d Neural computation, vol. 13, no. 7, pp. 1443\u20131471, 2001.\n[21] F. T. Liu, K. M. Ting, and Z.-H. Zhou, \u201cIsolation forest,\u201d in 2008\neighth ieee international conference on data mining.\nIEEE, 2008,\npp. 413\u2013422.\n[22] K. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detection\non attributed networks,\u201d in Proceedings of the 2019 SIAM Interna-\ntional Conference on Data Mining.\nSIAM, 2019, pp. 594\u2013602.\n[23] H. Fan, F. Zhang, and Z. Li, \u201cAnomalydae: Dual autoencoder for\nanomaly detection on attributed networks,\u201d in ICASSP 2020-2020\nIEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP).\nIEEE, 2020, pp. 5685\u20135689.\n[24] Y. Wang, J. Zhang, S. Guo, H. Yin, C. Li, and H. Chen, \u201cDecou-\npling representation learning and classi\ufb01cation for gnn-based anomaly\ndetection,\u201d in Proceedings of the 44th international ACM SIGIR\nconference on research and development in information retrieval,\n2021, pp. 1239\u20131248.\n[25] P. Velickovic, W. Fedus, W. L. Hamilton, P. Li`o, Y. Bengio, and R. D.\nHjelm, \u201cDeep graph infomax.\u201d ICLR (Poster), vol. 2, no. 3, p. 4, 2019.\n[26] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, \u201cGraph\ncontrastive learning with augmentations,\u201d Advances in Neural Infor-\nmation Processing Systems, vol. 33, pp. 5812\u20135823, 2020.\n[27] A. Bojchevski and S. G\u00a8unnemann, \u201cDeep gaussian embedding of\ngraphs: Unsupervised inductive learning via ranking,\u201d arXiv preprint\narXiv:1707.03815, 2017.\n[28] O. Shchur,\nM. Mumme, A. Bojchevski, and S. G\u00a8unnemann,\n\u201cPitfalls of\ngraph\nneural\nnetwork\nevaluation,\u201d\narXiv\npreprint\narXiv:1811.05868, 2018.\n[29] P. I. S\u00b4anchez, E. M\u00a8uller, F. Laforet, F. Keller, and K. B\u00a8ohm, \u201cStatis-\ntical selection of congruent subspaces for mining attributed graphs,\u201d\nin 2013 IEEE 13th international conference on data mining.\nIEEE,\n2013, pp. 647\u2013656.\n[30] T. Zhao, C. Deng, K. Yu, T. Jiang, D. Wang, and M. Jiang, \u201cError-\nbounded graph anomaly loss for gnns,\u201d in Proceedings of the 29th\nACM International Conference on Information & Knowledge Man-\nagement, 2020, pp. 1873\u20131882.\n[31] O. Shchur,\nM. Mumme, A. Bojchevski, and S. G\u00a8unnemann,\n\u201cPitfalls of\ngraph\nneural\nnetwork\nevaluation,\u201d\narXiv\npreprint\narXiv:1811.05868, 2018.\n[32] D. Zhou, J. He, H. Yang, and W. Fan, \u201cSparc: Self-paced network\nrepresentation for few-shot rare category characterization,\u201d in Pro-\nceedings of the 24th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2018, pp. 2807\u20132816.\n[33] J. McAuley, C. Targett, Q. Shi, and A. Van Den Hengel, \u201cImage-\nbased recommendations on styles and substitutes,\u201d in Proceedings\nof the 38th international ACM SIGIR conference on research and\ndevelopment in information retrieval, 2015, pp. 43\u201352.\n[34] A. Kumagai, T. Iwata, and Y. Fujiwara, \u201cTransfer anomaly detec-\ntion by inferring latent domain representations,\u201d Advances in neural\ninformation processing systems, vol. 32, 2019.\n[35] J. Wu, J. He, and Y. Liu, \u201cImverde: Vertex-diminished random\nwalk for learning imbalanced network representation,\u201d in 2018 IEEE\nInternational Conference on Big Data (Big Data).\nIEEE, 2018, pp.\n871\u2013880.\n[36] K. Ding, J. Li, and H. Liu, \u201cInteractive anomaly detection on at-\ntributed networks,\u201d in Proceedings of the twelfth ACM international\nconference on web search and data mining, 2019, pp. 357\u2013365.\n[37] X. Song, M. Wu, C. Jermaine, and S. Ranka, \u201cConditional anomaly\ndetection,\u201d IEEE Transactions on knowledge and Data Engineering,\nvol. 19, no. 5, pp. 631\u2013645, 2007.\n[38] K. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen,\nH. Peng, K. Shu, G. H. Chen, Z. Jia, and P. S. Yu, \u201cPygod: A python\nlibrary for graph outlier detection,\u201d arXiv preprint arXiv:2204.12095,\n2022.\n[39] Y. Zhao, Z. Nasrullah, and Z. Li, \u201cPyod: A python toolbox\nfor\nscalable outlier\ndetection,\u201d\nJournal\nof\nMachine\nLearning\nResearch, vol. 20, no. 96, pp. 1\u20137, 2019. [Online]. Available:\nhttp://jmlr.org/papers/v20/19-011.html\n[40] W. Hu, B. Liu, J. Gomes, M. Zitnik, P. Liang, V. Pande, and\nJ. Leskovec, \u201cStrategies for pre-training graph neural networks,\u201d arXiv\npreprint arXiv:1905.12265, 2019.\n[41] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimiza-\ntion,\u201d arXiv preprint arXiv:1412.6980, 2014.\n[42] M. Fey and J. E. Lenssen, \u201cFast graph representation learning with\nPyTorch Geometric,\u201d in ICLR Workshop on Representation Learning\non Graphs and Manifolds, 2019.\n",
    "2305.02496": "Revisiting Graph Contrastive Learning for\nAnomaly Detection\nZhiyuan Liua, b, Chunjie Cao*a, b, Fangjian Taoa, b and Jingzhang Sun*a, b\naSchool of Cyberspace Security, Hainan University\nbKey Laboratory of Information Retrieval of Hainan Province\nORCiD ID: Zhiyuan Liu https://orcid.org/0000-0002-9862-393X\nAbstract.\nCombining Graph neural networks (GNNs) with con-\ntrastive learning for anomaly detection has drawn rising attention\nrecently. Existing graph contrastive anomaly detection (GCAD)\nmethods have primarily focused on improving detection capability\nthrough graph augmentation and multi-scale contrast modules. How-\never, the underlying mechanisms of how these modules work have\nnot been fully explored. We dive into the multi-scale and graph aug-\nmentation mechanism and observed that multi-scale contrast mod-\nules do not enhance the expression, while the multi-GNN mod-\nules are the hidden contributors. Previous studies have tended to at-\ntribute the bene\ufb01ts brought by multi-GNN to the multi-scale mod-\nules. In the paper, we delve into the misconception and propose\nMulti-GNN and Augmented Graph contrastive framework MAG,\nwhich uni\ufb01ed the existing GCAD methods in the contrastive self-\nsupervised perspective. We extracted two variants from the MAG\nframework, L-MAG and M-MAG. The L-MAG is the lightweight\ninstance of the MAG, which outperform the state-of-the-art on Cora\nand Pubmed with the low computational cost. The variant M-MAG\nequipped with multi-GNN modules further improve the detection\nperformance. Our study sheds light on the drawback of the ex-\nisting GCAD methods and demonstrates the potential of multi-\nGNN and graph augmentation modules. Our code is available at\nhttps://github.com/liuyishoua/MAG-Framework.\n1\nIntroduction\nAnomaly detection has garnered signi\ufb01cant attention in industry,\nsuch as network intrusions [11, 27], money laundering [15, 21] and\n\ufb01nancial fraud detection [17], since it plays a critical role in identi-\nfying anomalous patterns and mitigating potential risks. Previously,\nshallow learning methods like ANOMOLOUS [16] and Radar [10]\nwere bene\ufb01ted from its residual analysis technique for anomaly\ndetection. However, they are hard to handle the non-linear high-\ndimensional data and complex interaction patterns. In response,\ngraph neural network (GNN) methods have emerged as powerful net-\nwork skeletons for anomaly detection due to the capability to model\ncomplex patterns.\nStill, detecting anomalies is challenging, since abnormal instances\nare often scarce and dif\ufb01cult to label [1]. To address this issue,\ncontrastive learning, bene\ufb01ted from its self-supervised property, has\nbeen combined with GNN models for anomaly detection. Exist-\ning graph contrastive anomaly detection (GCAD) methods, such as\nANEMONE [6], SL-GAD [29], and GRADATE [3], have utilized\ngraph augmentation or multi-scale contrast modules to upgrade their\nmodels. However, these incremental works enhance the expression\nof the model by adding different multi-scale contrasts or graph aug-\nmentation strategies intuitively without any empirical design guid-\nance. The impact of multi-scale contrast and graph augmentation on\nGCAD has not been extensively studied.\nRevisiting the ANEMONE [6], we found that the ANEMONE\nmethod actually bene\ufb01ted from the multi-GNN modules, not the ad-\nditional node-node contrast loss. For graph augmentation, the combi-\nnation of masked feature and removed edge show a signi\ufb01cant com-\npetitiveness.\nIn this paper, we proposed Multi-GNN and Augmented Graph\ncontrastive framework MAG, which uni\ufb01ed the existing GCAD\nmethods in the contrastive self-supervised perspective. By adjust-\ning the hyper-parameters of the MAG framework, we could de-\ngrade MAG to the classical GCAD methods, such as CoLA [13],\nANEMONE [6], SL-GAD [29], or GRADATE [29] methods. We tra-\nversed thoroughly the single contrast instances of the MAG frame-\nwork and observed that the normal node-subgraph contrast had better\ndetection performance than the node-node, sugraph-subgraph, and\nmasked node-subgraph contrasts. Unlike the GRADATE [3] model\nused a variety of multi-scale contrast combinations, our lightweight\nL-MAG surpasses the state-of-the-art on Cora and Pubmed with the\nlow computational cost. The variant M-MAG model equipped with\nmulti-GNN modules further improve the detection performance. Our\ncontributions can be summarized as follows:\n\u2022 To the best of our knowledge, we are the \ufb01rst group to unify\nGCAD models in the contrastive self-supervised perspective.\n\u2022 We suggested that the multi-scale contrast modules are the \"pup-\npets\", the backstage \"pusher\" are the multi-GNN modules in\nGCAD.\n\u2022 We provided empirical design guidance for different scale con-\ntrasts and graph augmentation strategies in GCAD.\n\u2022 The lightweight L-MAG outperforms the state-of-the-art with the\nlow computational cost, the M-MAG improve detection perfor-\nmance further.\n2\nBackground on Graph Anomaly Detection\nFor simplicity, we use capital letters, bold lowercase letters, and\nlowercase letters to denote matrices, vectors, and constants respec-\ntively, e.g. X, x, x. Giving graph G(V, X, A), V is composed of\na series of nodes {v1, vi, ..., vn}, X \u2208Rn\u00d7d consists of a set\narXiv:2305.02496v1  [cs.LG]  4 May 2023\nof vectors {x1, xi, ..., xn}, xi \u2208Rd. A \u2208Rn\u00d7n is the adja-\ncency matrix of G, where the entry Ai,j equals to 1 if there is an\nedge between the vi and vj, otherwise 0. For semi-supervised set-\nting, we denote yL = {yl1, yl2, .., ylp} as the known labels, and\nyU = {yu1, yu2, .., yuq} represents the unknown labels that we have\nto deduce. In this section, we will brief typical graph anomaly detec-\ntion techniques and formulate them below.\n2.1\nGNN-based\nGNN-based methods treat anomaly detection as an unbalance binary\nclassi\ufb01cation task. Like GNN classi\ufb01cations [14, 20, 28], we obtain\nnode representations Z via GNN mapping function F, where F can\nbe the skeleton of GCN [8], GAT [23] et al. The probability score\n\u02c6y \u2208Rn can be obtained by transforming Z to \u02c6y with the multilayer\nperceptron. The weighted binary cross entropy between the real la-\nbeled yL and the probability score \u02c6y will be optimized for model\ntraining.\n\u02c6y = MLP(F(X, A))\nL = 1\np \u00b7\np\nX\ni\n(\u03b1 \u00b7 yli log \u02c6\nyli + (1 \u2212yli) log(1 \u2212\u02c6\nyli))\n(1)\nwhere p is the number of the known labels yL, \u03b1 is the balance factor\nto regulate the imbalance between the normal and abnormal nodes. In\nthe stage of inference, node vi can be classi\ufb01ed by the corresponding\nprobability score \u02c6yi. Larger \u02c6yi, more abnormal. Based on the above\nprocedures, Tang et.al [20] analyzed from graph spectral perspective\nand designed spectral and spatially localized bandpass \ufb01lters to better\n\ufb01t the anomaly detection task. Zhang et al. [28] concated intermedi-\nate representations, introduced fraud-aware and imbalance-oriented\nclassi\ufb01cation modules to overcome graph inconsistency and imbal-\nance drawbacks in fraud detection.\n2.2\nReconstruction-based\nReconstruction-base\nmethods\nreconstruct\nthe\noriginal\ngraph\nG(V, X, A) via the graph autoencoder architecture [9]. It has been\nobserved that normal nodes tend to have richer consistency with\nneighbouring nodes [19]. Thus, normal nodes are more easily in\nrecovering than abnormal nodes. We can identify abnormal nodes via\ncomputing the similarity between G(V, X, A) and the reconstructed\ngraph \u02c6G(\u02c6V, \u02c6\nX, \u02c6A). The forward propagation can be formulated as\nfollows:\nZ = Encoder(X, A)\n\u02c6\nX = Decoder(Z),\n\u02c6A = \u03c3(Z \u2217ZT )\nL = || \u02c6\nX \u2212X||2 + \u03b1 \u00b7 || \u02c6A \u2212A||2\n(2)\nwhere Encoder(\u00b7), Decoder(\u00b7) can be classical GNN, such as\ngraph convolutional network (GCN [8]), \u03b1 is a balance factor to reg-\nulate the errors between structure and attribute. \u03c3(\u00b7) is sigmoid ac-\ntivation function to compress A to [0, 1]n\u00d7n. In the inference, the\nanomaly score of node vi can be computed by || \u02c6\nXi \u2212Xi||2 +\n\u03b1 \u00b7 || \u02c6\nAi \u2212Ai||2. To overcome the issues of network sparsity and\nlabel scarcity, DOMINANT [2] as one of the classical reconstruc-\ntion algorithms was presented, whose reconstructed process is just\nas formalized above. Differently, Fan et.al [4] suggested that exist-\ning methods neglected the complex cross-modality interactions be-\ntween network structure and node attribute. To this end, Anomaly-\nDAE incorporates the attention mechanism to assess the signi\ufb01cance\nof neighboring nodes, while also utilizing a dual autoencoder to en-\nhance cross-modality representation capabilities.\n\ufffd\ufffd\nmasked\nFigure 1.\nThe three types representation of the leftmost light green node\nvi, node feature Zi (the rightmost green), masked node feature (Zs\ni )i (the\nrightmost blue) and subgraph feature si (the rightmost yellow).\n2.3\nContrastive-based\nOne of crucial modules for contrastive learning is to construct in-\nstance pairs. In GCAD, for a given node vi, we sample its subgraph\nGi (Xi in Gi masked with 0) using random walk restart (RWR [22])\nmethod and \ufb01nd a distinct node vj, (i \u0338= j) as the negative pair of vi,\nwhere Xi is the attribute features of node vi. We put the node feature\nXi \u2208Rd and its sampled subgraph Gi(Xs\ni , As\ni) to the GNN map-\nping F to get the represented node feature Zi and its subgraph repre-\nsentation Zs\ni \u2208Rm\u00d7d, where m is the number of nodes in sampled\nsubgraph Gi. We apply readout function to \ufb02atten Zs\ni to si \u2208Rd.\nDue to si derived from node vi, the logical distance between Zi and\nsi shall be close. Similarly, sj derived from vj, which shall be far\naway from Zi. We can formulate as follows:\nsi = Readout(F(Gi(Xs\ni , As\ni))),\nZi = F(Xi)\nyi = Bilinear(si, Zi),\n\u02c6yi = Bilinear(sj, Zi)\nL1 = 1\nn \u00b7\nn\nX\ni\nlog yi + log(1 \u2212\u02c6yi)\n(3)\nwhere Bilinear(\u00b7) is the bilinear function to obtain consistency\nscore between two vectors, n denotes the number of nodes. One of\nthe classical GCAD models CoLA [13] achieve single scale node-\nsubgraph contrast, which operates similar with the above formula.\nHowever, Jin et.al. [6] illustrated that existing efforts only model the\ninstance pairs in a single scale aspect, thus limiting in capturing com-\nplex anomalous patterns. To this end, ANEMONE equipped with the\nadditional node-node contrast was proposed. Following the above ex-\npression, ANEMONE can be formulated as below.\nZs\ni = F(Gi(Xs\ni , As\ni))\ny(1)\ni\n= Bilinear((Zs\ni )i, Zi),\n\u02c6yi\n(1) = Bilinear((Zs\nj )j, Zi)\nL2 = 1\nn \u00b7\nn\nX\ni\nlog y(1)\ni\n+ log(1 \u2212\u02c6yi\n(1))\nL = L1 + L2\n(4)\nwhere (Zs\ni )i is vi corresponding node feature in Zs\ni as shown in Fig.\n1. Instead of contrasting with the subgraph features si, we use (Zs\ni )i\nto construct positive pairs ((Zs\ni )i, Zi) and negative pairs ((Zs\nj )j, Zi)\nin node-node scale. Due to subgraph Gi masked vi feature with\n0, (Zs\ni )i can be treated as the masked node feature of vi, which\nhave high consistency with Zi. We supposed that each graph G can\ngenerate three type views of node vi, subgraph features si, node\nfeatures Zi, and masked node features (Zs\ni )i, as shown in Fig.\n1. It\u2019s natural to consider that subgraph-subgraph contrast shall be\na promising idea for modeling more complex interaction patterns.\nGRADATE [3] constructed multi-scale contrasts, including node-\nnode, subgraph-node, subgraph-subgraph.\n2.4\nEnsemble Model\nAn intuitive idea comes that since anomaly detection bene\ufb01ts from\nboth reconstruction and contrastive methods, taking advantage of the\nboth shall yield a better result. SL-GAD [29] was composed of gen-\nerative attribute regression and multi-view contrastive learning mod-\nules to capture the anomalies. Differently, Mul-GAD [14] utilized\nredundancy reduction techniques to eliminate the harms of similar\ninformation generated by multi-view modeling, which achieve satis-\nfactory performance in the semi-supervised setting.\n3\nMethodology\nIn this section, we would detail the used GNN backbone, graph aug-\nmentation, different contrast patterns, and the \ufb01nal MAG framework.\n3.1\nPreliminary\nWe formulate the speci\ufb01ed GNN backbone used in our MAG frame-\nwork, which is well-known as graph convolutional network (GCN\n[8]). The message propagation of its l-th layers can be formulated as\nfollows:\nx(l)\ni\n= frelu\n\uf8eb\n\uf8ed\nX\nvj\u2208{vi}\u222aN (vi)\nai,jW (l)x(l\u22121)\nj\n\uf8f6\n\uf8f8\n(5)\nwhere x(l)\ni\nis the l-th layer representation of node vi and the N(vi)\ndenotes the collection of the vi neighbors. The ai,i is the entry (i, j)\nof the \u02c6A, \u02c6A = D\u22121\n2 AD\u22121\n2 , A = A + In, Di,i = P\nj Ai,j.\nfrelu(x) = max(0, x) is the non-linear activation function to em-\npower the model with non-linear modeling capability.\n3.2\nGraph Augmentation\n3.2.1\nFeature Augmentation\nSupposing p is the probability of the node attribute being masked,\nm \u2208{0, 1}d adhered to the Bernoulli distribution m \u223cB(d, 1\u2212p).\nA augmented feature \u02c6\nX can be computed as follows.\n\u02c6\nXi = Xi \u2299m, i = 1, 2...n\n\u02c6\nX = concat( \u02c6\nX1, ..., \u02c6\nXn)\n(6)\nwhere \u2299denotes the element-wise product between two vectors.\n3.2.2\nStructure Augmentation\nThe random edge perturbation [25, 26] is one of the typically struc-\nture augmentation methods. Assuming p is the ratio of perturbed\nedges. We specify the \u02c6A as:\n\u02c6A = A \u2299(1 \u2212L) + (1 \u2212A) \u2299L\n(7)\nwhere \u2299is element-wise multiplication and L \u2208Rn\u00d7n denotes a\npertubation location matrix where Li,j = Lj,i = 1 if node vi and vj\nwould be perturbed. In a undirected graph, the number of perturbed\nedges equals to the half of Pn\ni,j Ai,j. The p can be calculated as\nPn\ni,j Li,j/ Pn\ni,j Ai,j. Besides edge perturbation, edge diffusion [5,\n7] updates the structure via generating a different topological view.\nWe applied two frequently used edge diffusion methods in this paper,\nwhich is Personalized PageRank (PPR) and Heat Kernel (HK). Their\nclosed-form solutions of PPR and HK can be formulated as:\n\u02c6A(P P R) = \u03b1\n\u0010\nI \u2212(1 \u2212\u03b1)D\u22121/2AD\u22121/2\u0011\u22121\n\u02c6A(HK) = exp\n\u0000tAD\u22121 \u2212t\n\u0001\n(8)\nwhere \u03b1 denotes teleport probability in a random walk and t is the\ndiffusion time. D is the degree matrix of adjacency matrix A.\n3.3\nMulti-scale Contrast\nMulti-scale contrast in GCAD can be abstracted as node-node,\nsubgraph-subgraph, and node-subgraph contrasts, which focus on\ndifferent interaction patterns. By summarising the previous GCAD\nmethods [3, 6, 13, 29], we noticed that graph G can generate three\ntype views of node vi, subgraph features si, node features Zi,\nand masked node features (Zs\ni )i as shown in Fig. 1. These basic\nelements are the foundations to construct different contrast combina-\ntions. Given the graph G, we obtain them as follows:\nZs\ni = F(Gi(Xs\ni , As\ni)),\nZi = F(Xi)\nsi = Readout(Zs\ni )\n(9)\nwhere F(\u00b7) is GNN backbone, such as GCN [8], GAT [23] et al.\nXs\ni is the neighbours of the node vi, where (Xs\ni )i is masked with 0.\nThus, Zi is derived from node vi via GNN mapping, while si and\n(Zs\ni )i derived from the neighbors of node vi.\n3.3.1\nNode-node Contrast\nNode features Zi and masked node features (Zs\ni )i are utilized in this\npart.\nyi = Bilinear((Zs\ni )i, Zi),\n\u02c6yi = Bilinear((Zs\nj )j, Zi)\nLnn = 1\nn \u00b7\nn\nX\ni\nlog yi + log(1 \u2212\u02c6yi)\n(10)\nwhere (Zs\ni )i is the node vi corresponding representation in Zs\ni .\nBilinear(\u00b7) is the bilinear function to obtain the similarity score\nof the two inputs. Due to (Zs\ni )i and Zi derived from the same node,\ntheir consistency score yi is high. Conversely, the consistency be-\ntween (Zs\nj )j and Zi is low. Based on the intuition, we construct loss\nfunction Lnn and optimize it.\n3.3.2\nSubgraph-subgraph Contrast\nWe increase the subgraph views of node vi by adding a new GNN\nmapping \u02c6F(\u00b7). (si, \u02c6si) and (sj, \u02c6si) are employed to build the posi-\ntive and negative instance pairs.\n\u02c6si = Readout( \u02c6F(Gi(Xs\ni , As\ni)))\nyi = Bilinear(si, \u02c6si),\n\u02c6yi = Bilinear(sj, \u02c6si)\nLss = 1\nn \u00b7\nn\nX\ni\nlog yi + log(1 \u2212\u02c6yi)\n(11)\nwhere si, \u02c6si form the position instance pairs, while sj, \u02c6si are re-\ngarded as negative instance pairs.\n3.3.3\nNode-subgraph Contrast\nThere are two expressions for node-subgraph contrasts. For identi\ufb01-\ncation, we call normal node-subgraph contrast if used Zi, masked\nnode-subgraph contrast if used (Zs\ni )i.\nyi = Bilinear(si, Zi),\n\u02c6yi = Bilinear(sj, Zi)\nLn\nns = 1\nn \u00b7\nn\nX\ni\nlog yi + log(1 \u2212\u02c6yi)\n(12)\nwhere Zi denotes the normal node features, while (Zs\ni )i below refers\nto the masked feature derived only from node vi neighbours.\nyi = Bilinear(si, (Zs\ni )i),\n\u02c6yi = Bilinear(sj, (Zs\ni )i)\nLm\nns = 1\nn \u00b7\nn\nX\ni\nlog yi + log(1 \u2212\u02c6yi)\n(13)\n3.3.4\nInference Phase\nIn the training, the whole networks are updated via optimizing the\ncontrastive loss function. In the inference stage, we obtain the con-\nsistency scores of positive and negative pairs of node vi, yi and \u02c6yi.\nFor the normal nodes, the predicted score of positive instance pairs\nyi tended to 1, while the negative pairs \u02c6yi were closed to 0. For\nthe anomalous node, both of the yi and \u02c6yi are closed to 0.5, which\nmeans that its positive and negative pairs would be less discrimina-\ntive. Thus, the anomaly score can be computed as (\u02c6yi \u2212yi). Fol-\nlowing the [6,13,29], we sampled R rounds to obtain the mean and\nstandard derivation for stability. The procedure can be formulated as\nfollows:\nf1(vi) =\nPR\nr=1(\u02c6y(r)\ni\n\u2212y(r)\ni\n)\nR\n= x\nf2(vi) =\nv\nu\nu\nt\nR\nX\nr=1\n((\u02c6y(r)\ni\n\u2212y(r)\ni\n) \u2212x)2/R = s\nf(vi) = x + s\n(14)\nwhere f(vi) is the \ufb01nal anomaly score for node vi, which denotes\nthe sum of the mean and standard derivation. R is a hyper-parameter\nto avoid the impact of randomness. It is suitable to set to 256, which\ncould obtain stable result and avoid large computational costs.\n3.4\nMAG Framework\nAs shown in Fig. 2, each graph generates three views for node vi,\nwhich is subgraph feature si (yellow), masked node feature (Zs\ni )i\n(blue), and node feature Zi (green). We increase the graph views via\nthe graph augmentation and multi-GNN modules. The augmented\ngraph \u02c6G share the training parameters with the original graph G. The\ngraph convolutional network (GCN [8]) is used as GNN backbone in\nour framework. These views can be combined as the positive or neg-\native instance pairs and further establish the contrastive loss function.\nIn the combination pool, [1,3] form normal node-subgraph contrast\npairs. [1,3]+[5,6] added the additional node-node contrast pairs to\nmodel complex interactive pattern. In our uni\ufb01ed framework, differ-\nent combinations are implemented by adjusting hyper-parameters,\nwhich is simple and \ufb02exible. Following the formula in section 3.3,\nthe [1,3]+[5,6] is implemented as follows:\nL = \u03b1Ln\nns + \u03b2Lnn\nfall(vi) =\u03b1fLn\nns(vi) + \u03b2fLnn(vi)\n(15)\nTable 1.\nStatistics of the datasets. A half-and-half split between structure\nand contextual anomalies.\nGraph\nNodes\nEdges\nFeatures\nAnomalies\nCora\n2,708\n5,429\n1,433\n150\nCiteseer\n3,327\n4,732\n3,703\n150\nPubmed\n19,717\n88,648\n500\n600\nwhere the \u03b1 and \u03b2 are the balance factors to weigh different con-\ntrastive loss. In the inference stage, we obtain fall(vi) as our \ufb01nal\nanomaly score. fLn\nns(vi) and fLnn(vi) can be obtained according to\nthe formula 14. The same process applied to the three or more com-\nbinations. As shown in Fig. 2, the three combinations in the com-\nbination pool from top to bottom is the prototype of CoLA [13],\nANEMONE [6], and GRADATE [3] methods, respectively. We com-\npared the result of our combination with their real algorithm as\nshown in Table. 2, which show a small margin. Our MAG framework\nuni\ufb01ed the classical GCAD algorithms within limited \ufb02uctuation. We\nfurther proposed the two variants of MAG, L-MAG and M-MAG.\nThe L-MAG is the prototype of the single combination [4,9], which\noutperform the existing state-of-the-art on Cora and Pubmed with the\nlow computational cost. For the multiply contrast combinations, the\ncombination of [1,3]+[4,6] (M-MAG model) show better detection\nperformance.\n4\nExperiments and Results\nIn this section, we dived into the MAG framework and provided the\nempirical evidence to demonstrate that our MAG model does unify\nthe classical GCAD algorithm. To gain a deeper understanding, we\npropose four valuable research questions.\n\u2022 RQ1: Can the MAG framework unify the classical GCAD algo-\nrithm?\n\u2022 RQ2: Does the graph augmentation and multi-GNN modules ac-\ntually work?\n\u2022 RQ3: Is the multi-scale contrast module effective?\n\u2022 RQ4: How is the potential of the MAG framework in single com-\nbination condition? Can the \ufb01nal proposed M-MAG surpass the\nexisting methods?\n4.1\nExperimental Setting\n4.1.1\nDatasets\nFollowing the [5,13,24], we use the three popular citation networks,\nCora, Citeseer, and Pubmed [18]. The anomalous nodes were gener-\nated by perturbing the graph structure and modifying the node fea-\ntures. Thus, the graph networks are composed of structure and con-\ntextual abnormal nodes. The injection algorithm follow as [6,13] and\nthe statistic detail was listed in Table. 1\n4.1.2\nBaseline\nWe compare with the classical shallow learning methods, Radar,\nand ANOMALOUS. DOMINANT and AnomalyDAE are the\nreconstructed-based methods. The \ufb01nal categories are contrastive-\nbased methods, CoLA, ANEMONE, SL-GAD and GRADATE. For\nconvenience, we achieve the Radar, ANOMALOUS, DOMINANT\nand AnomalyDAE with a python library for graph outlier detection\n(PyGOD [12]). The other algorithm will be reproduced using the\nopen source code. It is worth noting that we would set the same\nhyper-parameters for a fair comparison.\nShare\nShare\n4\n5\n6\n[1,3]\n[1,3] + [5,6]\n[1,3]+[7,9]+[2,3]+[8,9]+[1,7]\n.\n.\n.\nAug.\nAug.\n\ufffd(\ufffd, \ufffd, \ufffd)\n\ufffd(\ufffd, \ufffd, \ufffd)\n\ufffd\ufffd\ufffd1(\u2219)\n\ufffd\ufffd\ufffd1(\u2219)\n\ufffd\ufffd\ufffd2(\u2219)\n\ufffd\ufffd\ufffd2(\u2219)\nCombination Pool\n7\n8\n9\n10\n11\n12\n1\n2\n3\n\ufffd(\ufffd, \ufffd, \ufffd)\n\ufffd(\ufffd, \ufffd, \ufffd)\nsubgraph \ufffd\ufffd  \nmasked node feature (\ufffd\ufffd\n\ufffd)\ufffd \nnode feature \ufffd\ufffd \nAugmentation\n       Multi-GNN\n    Augmentation\nFigure 2.\nThe overview framework of our MAG, which uni\ufb01ed the CoLA [13], ANEMONE [6] and GRADATE [3] via contrast combinations from top to bot-\ntom in the combination pool. The MAG framework consists of two modules: graph augmentation and multi-GNN modules. The normal node-subgraph, masked\nnode-subgraph, node-node, and subgraph-subgraph contrast pairs correspond to the green-yellow, blue-yellow, green-blue, yellow-yellow pairs, respectively.\nFor example, the [1,3]+[5,6] in the combination pool denote the used of normal node-subgraph pair and node-node pair.\nTable 2.\nThe average result (AUC/%) of CoLA, ANEMONE, and GRA-\nDATE in Cora over \ufb01ve seeds, compared with our corresponding MAG com-\nbination in the same hyper-parameters setting.\nCoLA\nANEMONE\nGRADATE\nOrigin\n89.1\n90.6\n90.1\nOur MAG\n90.3\n91.1\n89.5\nDifference\n+ 1.2\n+ 0.5\n- 0.6\n4.1.3\nEvaluation\nThe range of the anomaly score in this paper is not a probability\nvalue between [0,1]. Thus, it\u2019s not suitable to de\ufb01ne a passing line to\nidentify normal or anomaly nodes. The common used accuracy, pre-\ncision, and recall are not taken into consider. Conversely, the Area\nUnder Curve (AUC) is proper in this case, which will be our evalua-\ntion metrics in subsequent experiment.\n4.1.4\nParameter Setting\nFor our MAG, the training epochs and learning rate were set to 100\nand 1e-3 for all datasets. The hidden dimension and batch size were\nset to 64, 300. We sampled 256 rounds in the inference and set the\nsize of sampled subgraph to 4 following [6, 13]. The balance factor\nwas set to (0.3,0.7) for M-MAG model.\n4.2\nExperimental Evidence for Uni\ufb01ed\nTo answer the RQ1, we compared the experimental result with\nCoLA, ANEMONE, and GRADATE, which are reproduced using\nthe open source code. Their MAG combination correspond to [1,3],\n[1,3]+[5,6], [1,3]+[7,9]+[2,3]+[8,9]+[1,7]. Our combinations con-\nstruct the similar contrast loss, while the details of the implementa-\ntion are not totally same. For a fair comparison, we keep the same\nhyper-parameters, such as epoch, learning rate, and balance fac-\ntors. For the graph augmentation module, we use the combination\nof masked feature and removed edge for our MAG framework, since\nit show a stable enhancement performance in most cases as shown in\nFig. 4. As shown in Table. 2, our MAG combination models have\nTable 3.\nThe average results (AUC/%) of normal node-subgraph, node-\nnode, subgraph-subgraph, masked node-subgraph contrast in Cora using cor-\nresponding single combinations of our MAG framework over \ufb01ve seeds, re-\nspectively.\nN-NS\nNN\nSS\nM-NS\nAverage\n90.96\n86.03\n73.81\n69.66\na little margin with the corresponding algorithms, which may be\ncaused by the different graph augmentation strategies and the ran-\ndomness of the seeds. In fact, we unify GCAD model in the multi-\nscale contrast module, which is the most important part in GCAD.\nHowever, these relatively small margin still indicate the reasonable-\nness of the uni\ufb01ed model to a certain extent. Speci\ufb01cally, our model\nis highly \ufb02exible and achieve the combination of varying contrast\nlosses by only altering the hyper-parameters.\n4.3\nSingle Combination\nAlthough it is hard to traverse the search space in the multi-\ncombination case, the single combination is feasible. To answer\nRQ4, we search all the single combination and plot the heat map\nin AUC detection rate for a clarify observation. As shown the heap\nmap in Fig. 3, the node-subgraph contrast show a excellent perfor-\nmance, which have a average of 90.9%. We have summarised the\nother contrast patterns in Table. 3. The results illustrate that the rank-\ning of gain in detection AUC by different contrast patterns are nor-\nmal node-subgraph, node-node, subgraph-subgraph, masked node-\nsubgraph, respectively. It\u2019s worth noting that one of the combina-\ntions [4, 9] even outperform the existing state-of-the-art in some sit-\nuations without complex contrast combination, which would be the\nthe lightweight instance of our MAG framework called L-MAG in\nsubsequent experiment.\n4.4\nBene\ufb01ts of Graph Augmentation and Multiply\nGNN\nTo answer the RQ2, we have compared masked feature, removed\nedge, masked feature + removed edge, PPR diffusion, HK diffusion\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000)\u0000D\u0003\u0000V\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u00007\u0000U\u0000X\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0000R\u0000X\u0000V\n\u0000U\u0000D\u0000G\u0000D\u0000U\n\u0000G\u0000R\u0000P\u0011\u0000Q\u0000D\u0000Q\u0000W\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0013\u0000G\u0000D\u0000H\n\u0000F\u0000R\f\u0000D\n\u0000D\u0000Q\u0000H\u0000P\u0000R\u0000Q\u0000H\n\u0016\u0000U\u0000D\u0000G\u0000D\u0000W\u0000H\n\u0000V\f\u0000B\u0016\u0000D\u0000G\n\f\u0000B\u0000Y\u0000D\u0016\n\u0000P\u0000B\u0000Y\u0000D\u0016\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000)\u0000D\u0003\u0000V\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u00007\u0000U\u0000X\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0000R\u0000X\u0000V\n\u0000U\u0000D\u0000G\u0000D\u0000U\n\u0000G\u0000R\u0000P\u0011\u0000Q\u0000D\u0000Q\u0000W\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0013\u0000G\u0000D\u0000H\n\u0000F\u0000R\f\u0000D\n\u0000D\u0000Q\u0000H\u0000P\u0000R\u0000Q\u0000H\n\u0016\u0000U\u0000D\u0000G\u0000D\u0000W\u0000H\n\u0000V\f\u0000B\u0016\u0000D\u0000G\n\f\u0000B\u0000Y\u0000D\u0016\n\u0000P\u0000B\u0000Y\u0000D\u0016\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000)\u0000D\u0003\u0000V\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0013\u0000\u0011\u0000\u0015\n\u0000\u0013\u0000\u0011\u0000\u0017\n\u0000\u0013\u0000\u0011\u0000\u0019\n\u0000\u0013\u0000\u0011\u0006\n\u0000\u0014\u0000\u0011\u0000\u0013\n\u00007\u0000U\u0000X\u0000H \u00003\u0000R\u0000V\t\u0000W\t\u0000Y\u0000H \u00005\u0000D\u0000W\u0000H\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0000R\u0000X\u0000V\n\u0000U\u0000D\u0000G\u0000D\u0000U\n\u0000G\u0000R\u0000P\u0011\u0000Q\u0000D\u0000Q\u0000W\n\u0000D\u0000Q\u0000R\u0000P\u0000D\f\u0013\u0000G\u0000D\u0000H\n\u0000F\u0000R\f\u0000D\n\u0000D\u0000Q\u0000H\u0000P\u0000R\u0000Q\u0000H\n\u0016\u0000U\u0000D\u0000G\u0000D\u0000W\u0000H\n\u0000V\f\u0000B\u0016\u0000D\u0000G\n\f\u0000B\u0000Y\u0000D\u0016\n\u0000P\u0000B\u0000Y\u0000D\u0016\n\u0000\u0014 \u0000\u0015 \u0000\u0016 \u0000\u0017 \u0000\u0018 \u0000\u0019 \u001b \u0006 \u001c \u0000\u0014\u0000\u0013 \u0000\u0014\u0000\u0014 \u0000\u0014\u0000\u0015\n\u0000\u0014\n\u0000\u0015\n\u0000\u0016\n\u0000\u0017\n\u0000\u0018\n\u0000\u0019\n\u001b\n\u0006\n\u001c\n\u0000\u0014\u0000\u0013\n\u0000\u0014\u0000\u0014\n\u0000\u0014\u0000\u0015\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001b\u0000\u0016\u0000\u0011\u0000\u0013 \u001c\u0000\u0013\u0000\u0011\u001c \u001b\u0000\u0016\u0000\u0011\u0000\u0017 \u0000\u0019\u001b\u0000\u0011\u0000\u0018 \u001c\u0000\u0014\u0000\u0011\u0000\u0013 \u001b\u0000\u0015\u0000\u0011\u0000\u0017 \u001b\u0000\u0017\u0000\u0011\u0000\u0013 \u001c\u0000\u0013\u0000\u0011\u001c \u001b\u0000\u0014\u0000\u0011\u0000\u0015 \u001b\u0000\u0016\u0000\u0011\u0000\u0017 \u001c\u0000\u0014\u0000\u0011\u0000\u0015\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0006\u0000\u0018\u0000\u0011\u001b \u0000\u0019\u0000\u0016\u0000\u0011\u0000\u0015 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0006\u0000\u0017\u0000\u0011\u0006 \u0000\u0019\u0000\u0019\u0000\u0011\u0006 \u0000\u0018\u001b\u0000\u0011\u0000\u0019 \u0006\u0000\u0018\u0000\u0011\u0000\u0017 \u0000\u0019\u0006\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u001b \u0006\u0000\u0017\u0000\u0011\u001c\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001c\u0000\u0014\u0000\u0011\u0000\u0017 \u0006\u001b\u0000\u0011\u0000\u0014 \u0000\u0018\u0000\u0017\u0000\u0011\u0000\u0013 \u001c\u0000\u0013\u0000\u0011\u001c \u0006\u0000\u0017\u0000\u0011\u001c \u0000\u0018\u0000\u0015\u0000\u0011\u0000\u0018 \u001c\u0000\u0013\u0000\u0011\u001b \u0006\u001b\u0000\u0011\u0000\u0019 \u0000\u0018\u0000\u0017\u0000\u0011\u0000\u0013\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001b\u0000\u0014\u0000\u0011\u001c \u001c\u0000\u0014\u0000\u0011\u0000\u0017 \u001b\u0000\u0019\u0000\u0011\u0000\u0016 \u001b\u0000\u0014\u0000\u0011\u0000\u0014 \u001c\u0000\u0014\u0000\u0011\u0000\u0017 \u001b\u0000\u0018\u0000\u0011\u001b \u001b\u0000\u0017\u0000\u0011\u001c \u001c\u0000\u0014\u0000\u0011\u0000\u0016\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0006\u0000\u0019\u0000\u0011\u0000\u0015 \u001b\u0000\u0014\u0000\u0011\u0006 \u0000\u0017\u0006\u0000\u0011\u0000\u0017 \u0006\u0000\u0019\u0000\u0011\u0006 \u0000\u0019\u0000\u0019\u0000\u0011\u001c \u0000\u0018\u0000\u0018\u0000\u0011\u0006 \u0006\u0000\u0019\u0000\u0011\u0000\u0016\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001c\u0000\u0013\u0000\u0011\u001b \u0006\u0000\u0018\u0000\u0011\u0000\u0019 \u0000\u0018\u0000\u0017\u0000\u0011\u0000\u0013 \u001c\u0000\u0014\u0000\u0011\u0000\u0013 \u0006\u0000\u0019\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0015\u0000\u0011\u0000\u0015\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001b\u0000\u0014\u0000\u0011\u0000\u0015 \u001c\u0000\u0013\u0000\u0011\u0000\u0014 \u001b\u0000\u0016\u0000\u0011\u001c \u0000\u0019\u0000\u0018\u0000\u0011\u0000\u0015 \u001c\u0000\u0013\u0000\u0011\u001b\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0019\u0000\u0013\u0000\u0011\u0000\u0017 \u0000\u0019\u0000\u0018\u0000\u0011\u0000\u0017 \u0000\u0016\u0000\u0018\u0000\u0011\u001b \u0006\u0000\u0018\u0000\u0011\u0000\u0019\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001c\u0000\u0013\u0000\u0011\u001b \u0006\u001b\u0000\u0011\u0000\u0019 \u0000\u0018\u0000\u0017\u0000\u0011\u0000\u0013\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u001b\u0000\u0013\u0000\u0011\u0000\u0017 \u001c\u0000\u0014\u0000\u0011\u0000\u0013\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0006\u0000\u0019\u0000\u0011\u0000\u0013\n\u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013 \u0000\u0018\u0000\u0013\u0000\u0011\u0000\u0013\n\u0000\u0019\u0000\u0013\n\u0000\u0019\u0000\u0018\n\u001b\u0000\u0013\n\u001b\u0000\u0018\n\u0006\u0000\u0013\n\u0006\u0000\u0018\n\u001c\u0000\u0013\n\u001c\u0000\u0018\nFigure 3.\nThe three roc curves are conducted on Cora, Citeseer, and Pubmed datasets from left to right, respectively. The values in the heat plot denote the\ndetection AUC of different contrast combination. For example, the biggest AUC value 91.4 shows repeatedly in combination [3,4], [4,6], [4,9].\nMF\nRE\nMF+RE\nHK\nPPR\n60\n80\n100\n90.9\n90.9\n91.0\n76.2\n75.3\nMF\nRE\nMF+RE\nHK\nPPR\n88\n90\n92\n94\nDetection rate AUC (%)\n91.2\n91.8\n91.6\n92.0\n92.0\nFigure 4.\nThe MF, RE denote the masked feature, removed edge, respec-\ntively. The HK and PPR are the typical graph diffusion methods, which are\nheat kernel and personalized pagerank. The top one use the combination\n[1,3]+[7,9] of MAG framework, the bottom use [1,3]+[10,12]. Experiments\nare conducted on the Cora.\nTable 4.\nThe origin, M-S, M-SG and M-G correspond to the combination\n[1,3], [1,3]+[2,3], [1,3]+[5,6], and [1,3]+[4,6] of the MAG framework. The\nbest detection AUC (%) is in bold and the runner-up is in underline, conducted\non Cora over \ufb01ve seeds.\nOrigin\nM-S\nM-SG\nM-G\nCora\n90.3\n90.0\n91.1\n91.7\nCiteseer\n91.6\n90.0\n92.2\n92.5\nfor graph augmentation. The ratio of masked and removed is set to\n0.2 to increase the modeling dif\ufb01culties. As shown in Fig. 4, masked\nfeature + removed edge have the most stable performance. We at-\ntribute the HK, PPR failures on the top bar to the limitations of the\nsingle GNN model. To examine the difference between the single\nand multiple GNNs, we compare the single and double GNN models.\nAs shown in Table. 4, the origin and M-G denote single and double\nGNNs respectively. The double one shows a higher detection AUC.\nWe attribute the result to the fewer training parameters and the sta-\ntistically unstable properties of the single GNN model.\n4.5\nScam of Multi-scale Contrast\nWe found that multi-scale modules do not improve the model per-\nformance, which multi-GNN modules do. To answer the RQ3, we\nconstructed the origin, M-S, M-G, and M-SG as shown in Table. 4,\nwhich denoted single-GNN, multi-scale, multi-GNN, and the com-\nbination of multi-scale and multi-GNN. Compared origin with M-S,\nthe additional node-node contrast [2,3] in M-S has no bene\ufb01t and\nTable 5.\nComparison with the existing state-of-the-art. The detecting re-\nsults AUC(%) over \ufb01ve seeds, on Cora, Citeseer, Pubmed datasets. The best\nperformance method in each experiment is in bold and the runner-up is in\nunderline.\nAlg.\nData.\nCora\nCiteseer\nPubmed\nRadar [10]\n64.8\n62.2\n54.5\nANOMALOUS\n[16]\n67.8\n66.4\n54.1\nDOMINANT [2]\n81.0\n83.1\n80.5\nAnomalDAE [4]\n76.2\n72.1\n78.8\nCoLA [13]\n89.1\n90.6\n95.1\nANEMONE [6]\n90.8\n91.8\n95.4\nSL-GAD [29]\n91.3\n91.7\n95.6\nGRADATE [3]\n90.1\n92.3\n94.8\nL-MAG (Ours)\n91.4\n91.8\n95.7\nM-MAG (Ours)\n91.7\n92.5\n96.6\neven causes a corrupt performance. However, the additional node-\nnode contrast [5,6] for M-SG get a better result. In fact, we found\nthat the gains for M-SG derived from multi-GNN modules, not the\nadditional node-node contrast. The result that M-G outperform the\nM-SG further con\ufb01rm the statement. Actually, the prototype of the\nM-SG is the ANEMONE [6] algorithm, which claimed that their im-\nprovement is bene\ufb01ted from the additional node-node contrast. They\nillustrated that the extra node-node contrast was able to model com-\nplex interaction patterns, which resulted the better performance. It\u2019s\na scam of multi-scale modules, the multi-GNN is the hidden pushers.\n4.6\nComparison with Existing Methods\nTo answer the RQ4, we propose two variant models in GCAD \ufb01eld,\nL-MAG and M-MAG. L-MAG is the combination of [4,9] and M-\nMAG is the combination of [1,3]+[4,6] As shown in Table. 5 and Fig.\n3, our L-MAG outperform the existing model on Cora and Pubmed\nwith a low computational cost, while the M-MAG model further im-\nproves detection AUC bene\ufb01ted from the multi-GNN modules.\n5\nConclusion\nIn this paper, we proposed the multi-GNN and augmented GCAD\nframework MAG. Our MAG framework is able to unify the classi-\ncal GCAD methods by combining different contrast patterns. The\nproposed lightweight variant L-MAG outperform the state-of-the-art\non Cora and Pubmed with the low computational cost. The variant\nM-MAG equipped with multi-GNN modules further improve the de-\ntection performance. Revisiting the multi-scale contrast and multi-\nGNN modules, we observed that the ANENONE method bene\ufb01ted\nfrom the multi-GNN modules, not the additional node-node contrast.\nWe suggested that multi-scale contrast modules were the surfaced\n\"puppet\", while the multi-GNN modules were the real \"pushers\" for\ncomplex interaction modeling. For augmentation, the masked feature\nand removed edge are relatively better options. In the single combi-\nnation of the MAG, the normal node-subgraph express higher de-\ntection AUC than node-node, subgraph-subgraph, and masked node-\nsubgraph contrast. The MAG framework has a vast amount of com-\nbinations, which are challenging to traverse thoroughly. Therefore,\n\ufb01nding a better contrast combinations is a worthwhile subject. Trans-\nferring MAG framework to a more realistic scene (e.g. heterogeneous\nor dynamic graph) also deserves more attention.\nReferences\n[1]\nVarun Chandola, Arindam Banerjee, and Vipin Kumar, \u2018Anomaly de-\ntection: A survey\u2019, ACM computing surveys (CSUR), 41(3), 1\u201358,\n(2009).\n[2]\nKaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu, \u2018Deep\nanomaly detection on attributed networks\u2019, in Proceedings of the 2019\nSIAM International Conference on Data Mining, pp. 594\u2013602. SIAM,\n(2019).\n[3]\nJingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin,\nYue Liu, and Zhibin Dong, \u2018Graph anomaly detection via multi-scale\ncontrastive learning networks with augmented view\u2019, arXiv preprint\narXiv:2212.00535, (2022).\n[4]\nHaoyi Fan, Fengbin Zhang, and Zuoyong Li, \u2018Anomalydae: Dual au-\ntoencoder for anomaly detection on attributed networks\u2019, in ICASSP\n2020-2020 IEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), pp. 5685\u20135689. IEEE, (2020).\n[5]\nKaveh Hassani and Amir Hosein Khasahmadi, \u2018Contrastive multi-view\nrepresentation learning on graphs\u2019, in International conference on ma-\nchine learning, pp. 4116\u20134126. PMLR, (2020).\n[6]\nMing Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui\nPan, \u2018Anemone: Graph anomaly detection with multi-scale contrastive\nlearning\u2019, in Proceedings of the 30th ACM International Conference on\nInformation & Knowledge Management, pp. 3122\u20133126, (2021).\n[7]\nZekarias T Kefato and Sarunas Girdzijauskas, \u2018Self-supervised graph\nneural networks without explicit negative sampling\u2019, arXiv preprint\narXiv:2103.14958, (2021).\n[8]\nThomas N Kipf and Max Welling, \u2018Semi-supervised classi\ufb01cation\nwith graph convolutional networks\u2019, arXiv preprint arXiv:1609.02907,\n(2016).\n[9]\nThomas N Kipf and Max Welling, \u2018Variational graph auto-encoders\u2019,\narXiv preprint arXiv:1611.07308, (2016).\n[10]\nJundong Li, Harsh Dani, Xia Hu, and Huan Liu, \u2018Radar: Residual\nanalysis for anomaly detection in attributed networks.\u2019, in IJCAI, vol-\nume 17, pp. 2152\u20132158, (2017).\n[11]\nZhida Li, Ana Laura Gonzalez Rios, and Ljiljana Trajkovi\u00b4c, \u2018Machine\nlearning for detecting anomalies and intrusions in communication net-\nworks\u2019, IEEE Journal on Selected Areas in Communications, 39(7),\n2254\u20132264, (2021).\n[12]\nKay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong\nZhang, Kaize Ding, Canyu Chen, Hao Peng, Kai Shu, George H. Chen,\nZhihao Jia, and Philip S. Yu, \u2018Pygod: A python library for graph outlier\ndetection\u2019, arXiv preprint arXiv:2204.12095, (2022).\n[13]\nYixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George\nKarypis, \u2018Anomaly detection on attributed networks via contrastive\nself-supervised learning\u2019, IEEE transactions on neural networks and\nlearning systems, 33(6), 2378\u20132392, (2021).\n[14]\nZhiyuan Liu, Chunjie Cao, and Jingzhang Sun, \u2018Mul-gad: a semi-\nsupervised graph anomaly detection framework via aggregating multi-\nview information\u2019, arXiv preprint arXiv:2212.05478, (2022).\n[15]\nWai Weng Lo, Siamak Layeghy, and Marius Portmann, \u2018Inspection-l:\nPractical gnn-based money laundering detection system for bitcoin\u2019,\narXiv preprint arXiv:2203.10465, (2022).\n[16]\nZhen Peng, Minnan Luo, Jundong Li, Huan Liu, Qinghua Zheng, et al.,\n\u2018Anomalous: A joint modeling approach for anomaly detection on at-\ntributed networks.\u2019, in IJCAI, pp. 3513\u20133519, (2018).\n[17]\nTahereh Pourhabibi, Kok-Leong Ong, Booi H Kam, and Yee Ling\nBoo, \u2018Fraud detection: A systematic literature review of graph-\nbased anomaly detection approaches\u2019, Decision Support Systems, 133,\n113303, (2020).\n[18]\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gal-\nligher, and Tina Eliassi-Rad, \u2018Collective classi\ufb01cation in network data\u2019,\nAI magazine, 29(3), 93\u201393, (2008).\n[19]\nChaoming Song, Shlomo Havlin, and Hernan A Makse, \u2018Self-similarity\nof complex networks\u2019, Nature, 433(7024), 392\u2013395, (2005).\n[20]\nJianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li, \u2018Rethinking graph neural\nnetworks for anomaly detection\u2019, in International Conference on Ma-\nchine Learning, pp. 21076\u201321089. PMLR, (2022).\n[21]\nMilind Tiwari, Adrian Gepp, and Kuldeep Kumar, \u2018A review of money\nlaundering literature: the state of research in key areas\u2019, Paci\ufb01c Ac-\ncounting Review, (2020).\n[22]\nHanghang Tong, Christos Faloutsos, and Jia-Yu Pan, \u2018Fast random walk\nwith restart and its applications\u2019, in Sixth international conference on\ndata mining (ICDM\u201906), pp. 613\u2013622. IEEE, (2006).\n[23]\nPetar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana\nRomero, Pietro Lio, and Yoshua Bengio, \u2018Graph attention networks\u2019,\narXiv preprint arXiv:1710.10903, (2017).\n[24]\nPetar Velickovic, William Fedus, William L Hamilton, Pietro Li\u00f2,\nYoshua Bengio, and R Devon Hjelm, \u2018Deep graph infomax.\u2019, ICLR\n(Poster), 2(3), 4, (2019).\n[25]\nLirong Wu, Haitao Lin, Cheng Tan, Zhangyang Gao, and Stan Z Li,\n\u2018Self-supervised learning on graphs: Contrastive, generative, or predic-\ntive\u2019, IEEE Transactions on Knowledge and Data Engineering, (2021).\n[26]\nYaochen Xie, Zhao Xu, Jingtun Zhang, Zhengyang Wang, and Shui-\nwang Ji, \u2018Self-supervised learning of graph neural networks: A uni-\n\ufb01ed review\u2019, IEEE transactions on pattern analysis and machine intel-\nligence, (2022).\n[27]\nVinod Yegneswaran, Paul Barford, and Johannes Ullrich, \u2018Internet in-\ntrusions: Global characteristics and prevalence\u2019, ACM SIGMETRICS\nPerformance Evaluation Review, 31(1), 138\u2013147, (2003).\n[28]\nGe Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou,\nand Quan Z Sheng, \u2018Fraudre: Fraud detection dual-resistant to graph\ninconsistency and imbalance\u2019, in 2021 IEEE International Conference\non Data Mining (ICDM), pp. 867\u2013876. IEEE, (2021).\n[29]\nYu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-\nPing Phoebe Chen, \u2018Generative and contrastive self-supervised learning\nfor graph anomaly detection\u2019, IEEE Transactions on Knowledge and\nData Engineering, (2021).\n",
    "2310.16376": "GADY: Unsupervised Anomaly Detection on Dynamic Graphs\nShiqi Lou1, Qingyue Zhang2, Shujie Yang2 Yuyang Tian1 Zhaoxuan Tan3 Minnan Luo1\n1Xi\u2019an Jiaotong University, 2Tsinghua University, 3University of Notre Dame\nlousqi@stu.xjtu.edu.cn,{zhangqy23, yangsj23}@mails.tsinghua.edu.cn, 2191414578@stu.xjtu.edu.cn,\nztan3@nd.edu, minnluo@xjtu.edu.cn\nAbstract\nAnomaly detection on dynamic graphs refers to detecting en-\ntities whose behaviors obviously deviate from the norms ob-\nserved within graphs and their temporal information. This\nfield has drawn increasing attention due to its application in\nfinance, network security, social networks, and more. How-\never, existing methods face two challenges: dynamic struc-\nture constructing challenge - difficulties in capturing graph\nstructure with complex time information and negative sam-\npling challenge - unable to construct excellent negative sam-\nples for unsupervised learning. To address these challenges,\nwe propose Unsupervised Generative Anomaly Detection on\nDynamic Graphs (GADY). To tackle the first challenge, we\npropose a continuous dynamic graph model to capture the\nfine-grained information, which breaks the limit of existing\ndiscrete methods. Specifically, we employ a message-passing\nframework combined with positional features to get edge em-\nbeddings, which are decoded to identify anomalies. For the\nsecond challenge, we pioneer the use of Generative Adversar-\nial Networks to generate negative interactions. Moreover, we\ndesign a loss function to alter the training goal of the genera-\ntor while ensuring the diversity and quality of generated sam-\nples. Extensive experiments demonstrate that our proposed\nGADY significantly outperforms the previous state-of-the-art\nmethod on three real-world datasets. Supplementary experi-\nments further validate the effectiveness of our model design\nand the necessity of each module.\nIntroduction\nAnomaly detection on dynamic graphs, has become an im-\nportant task given its numerous applications, such as social\nmedia spammer detection (Ye and Akoglu 2015), fraudulent\ntransaction detection (Dou et al. 2020), and network intru-\nsion detection (Shone et al. 2018). Capturing the anomaly\non dynamic graphs can help us better understand and cap-\nture the evolution of social networks (Ye and Akoglu 2015),\nfinancial transactions (Dou et al. 2020), and disease diag-\nnosis (Khan et al. 2021) with fine-grained time information.\nFor example, in financial transactions, if two entities conduct\na transaction and they are in two transaction collectives that\nhad no transaction records before, this transaction is very\nsuspicious, which highlights the importance of temporal in-\nformation in detecting anomalies.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\n2. Snapshot\n22:51\n07:40\n58:20\n44:22\n55:21\n49:42\n38:24\n10:22\n1. Original Graph\n4. Subgraph2: 30:00-60:00\n3. Subgraph1: 00:00-30:00\nBob\nJoker\nTom\nHacker\n47:33\n53:33\nBot\nSam\n22:51\n07:40\n10:22\nBob\nJoker\nTom\nHacker\nBot\nSam\n18:44\nT\nT\nT\nT\nT\nT\nT\nBob\nJoker\nTom\nHacker\nT\nT\nBot\nSam\nT\n58:20\n44:22\n55:21\n49:42\n38:24\nBob\nJoker\nTom\nHacker\n47:33\n53:33\nBot\nSam\nCon!nuous\nCon!nuous\nDiscrete\n18:44\nFigure 1: Example of the superiority of anomaly detection in\na continuous dynamic graph. When detecting the anomaly\nedge of 58:20 in the dynamic original graph, discrete dy-\nnamic graph methods fall short, while continuous dynamic\nmethods can easily distinguish this anomaly behavior.\nRecently, many studies have been done in this area (Zheng\net al. 2019a; Cai et al. 2021; Liu et al. 2021). They usu-\nally divide the entire temporal graph into several snapshots\nand conduct anomaly detection based on this graph format.\nAddgraph (Zheng et al. 2019a) uses GCN to extract infor-\nmation about graph structure from the slices and then using\nthe GRU-attention module to construct long- and short-term\nrelationships between the slices. StrGNN (Cai et al. 2021)\nextracted h-hop closed subgraphs centered on edges, and\nthen used GCN and GRU to model the structural information\non snapshots and the correlation between snapshots respec-\ntively. Taddy (Liu et al. 2021) uses a transformer to process\ndiffusion-based spatial encoding, distance-based spatial en-\ncoding and relative time encoding, and then obtain edge rep-\nresentation through pooling layer, thus obtaining anomaly\nscore.\nAlthough much research progress has been achieved in\ndynamic graph anomaly detection, they are weak in two as-\npects: dynamic structure constructing challenge - difficul-\narXiv:2310.16376v1  [cs.LG]  25 Oct 2023\nties in capturing graph structure with complex time infor-\nmation and negative sampling challenge - unable to con-\nstruct excellent negative samples for unsupervised learning.\nFor the dynamic structure constructing challenge, anomaly\ndetection usually needs entity embedding through the en-\ncoder part, which are then decoded as labels. Some research\nworks propose to use discrete dynamic methods for model-\ning (Cai et al. 2021; Liu et al. 2021). Specifically, they cut\ndynamic graphs into several slices, which can be captured by\nGraph Neural Networks (GNNs) or similar methods. Then\nthey uses Gated Recurrent Unit (GRU) and similar methods\nto construct the evolutionary process of slices (Zheng et al.\n2019a; Cai et al. 2021; Liu et al. 2021). These methods are\nable to plug in well-developed GNN methods to boost the\nmodel performance. However, using this method inevitably\nresults in two kinds of information loss. Firstly, it ignores\nrepeated edges in the slice time interval, which will cause\nGNN to fail to capture complete interaction information,\nmaking the generated entity embedding defective. Secondly,\nit ignores the time difference within the same slice, making\nit difficult for the model to capture time dependencies. Both\nkinds of information loss would hinder the anomaly detec-\ntion performance on dynamic graphs.\nIn addition to the above problem, research on anomaly\ndetection on dynamic graphs also faces the negative sam-\npling challenge: unable to construct excellent negative sam-\nples for unsupervised learning, which is, on the existing\ndynamic graph datasets, there are no real anomalous sam-\nples to help us conduct supervised training. To solve this\nproblem, some people propose a context-dependent negative\nsampling method to artificially construct negative samples\nby rules (Cai et al. 2021) for unsupervised learning. Specifi-\ncally, for each edge, they randomly keep the head node or tail\nnode and replace the unreserved nodes with other irrelevant\nnodes, thus obtaining negative samples for training. How-\never, the quality of negative samples in the training phase\nhas a crucial impact on the performance of the model. Ide-\nally, the generated negative samples will enable the model to\nefficiently learn diverse anomaly patterns during the train-\ning phase. This puts forward two requirements for negative\nsamples: 1. Abnormal samples should be of high quality; 2.\nAbnormal samples need to be diverse. The proposed method\nhas limited performance in these two criteria.\nTo break the above two challenges, we propose a gen-\neral model for continuous dynamic graph anomaly detec-\ntion. Our main idea is to use an anomaly generator to obtain\ndiverse and high-quality negative samples to help training,\nand to use a continuous dynamic graph model to perform\nanomaly detection. Through adversarial training of the two\nmodules, an excellent discriminator can finally be obtained,\nthus achieving better performance on anomaly detection.\nSpecifically, to break the limitation of the existing discrete\ndynamic graph detection methods, we use message-passing\ncontinuous dynamic models combined with positional fea-\ntures (Souza et al. 2022) to capture complex dynamic graph.\nAlso, we use anomaly generator to generate diverse nega-\ntive edges with high quality. To achieve this, we propose a\nnovel loss function designed for dynamic graphs anomaly\ndetection to help generator find its specific training goal. We\nfollow the evaluation settings proposed by Liu et al. (2021)\nfor fair comparison. Experimental results show that our pro-\nposed GADY outperforms the current state of the art signif-\nicantly on three benchmarks, demonstrating the superiority\nof our design choice. We also investigate the potential of cur-\nrent continuous dynamic graphs for anomaly detection. De-\ntailed supplementary experiments about GAN module and\ngenerator design also validate the necessity of anomaly gen-\nerator and the other components in GADY.\nIn summary, our main contributions are as follows:\n\u2022 We identify the drawbacks of existing discrete dynamic\ngraph anomaly detection methods and further explore the\npotential of continuous dynamic graph models to address\nexisting dynamic structure constructing challenge.\n\u2022 We pioneer the use of the GAN network for anomaly de-\ntection on dynamic graphs and demonstrate the effective-\nness of the GAN model in improving anomaly detection\ncapabilities.\n\u2022 Extensive experiments on three datasets show that our\nmodel has achieved a maximum improvement of 14.6%\nin anomaly detection compared with existing methods,\nand achieved extraordinary performance in all anomaly\nproportions of all datasets. Extensive supplementary ex-\nperiments again demonstrate the necessity of anomaly\ngenerators and the superiority of model settings.\nRelated Work\nAnomaly Detection on Dynamic Graph\nIn recent years, anomaly detection has focused on using\ndeep learning methods to model the entire process. (Jin et al.\n2021; Zhao et al. 2021; Tariq, Le, and Woo 2022; Zhou\net al. 2021; Han and Yuan 2021; Guo et al. 2022) The pro-\ncessing methods of dynamic graphs can be divided into dis-\ncrete processing methods and continuous processing meth-\nods. Many methods have been proposed using discrete meth-\nods in recent years to solve this task, such as Addgraph\n(Zheng et al. 2019a) using GCN to extract graph structure\ninformation on slices, and then using GRU-attention mod-\nule to construct long and short term relationships between\nslices. StrGNN (Cai et al. 2021) extracted h-hop closed sub-\ngraphs centered on edges, and then used GCN and GRU to\nmodel the structural information on snapshots and the cor-\nrelation between snapshots respectively. Taddy (Liu et al.\n2021) uses a transformer to process diffusion-based spatial\nencoding, distance-based spatial encoding and relative time\nencoding, and then obtain edge representation through the\npooling layer, thus obtaining the anomaly score. For contin-\nuous methods, it can be more helpful for anomaly detection\nwith its strong modeling capabilities and sufficient time in-\nformation. Recently, SAD (Tian et al. 2023) propose to use\ncontinuous dynamic methods to detect anomalies using the\nsemi-supervised method, which is different from our work.\nGenerative Adversarial Network in Anomaly\nDetection\nNowadays, using GAN to perform anomaly detection is di-\nvided into two main branches. The first branch is to use GAN\nto learn a normal sample distribution and to use the recon-\nstruction error as an anomaly score. For example, AnoGAN\n(Schlegl et al. 2017) can capture the latent distribution of\nnormal data through training, and because the behavior pat-\nterns of abnormal data and normal data are different, after\nprocessing by AnoGAN, the residual between original input\nand output is usually much larger than normal images, thus\ndetecting anomalies in image.\nHowever, since the original goal of GAN is to gener-\nate high-quality data rather than anomaly detection, the\nanomaly score obtained by using reconstruct loss of GAN\nfor anomaly detection may be suboptimal. Another branch\nis to use GAN to generate negative samples to assist in\nmodel training. For example, OCAN (Zheng et al. 2019b)\nuses LSTM-Autoencoder to learn the representation of nor-\nmal users, and then modifies the generator so that its training\ngoal becomes generating abnormal data that is complemen-\ntary to normal data, and obtains a better discriminator after\ntraining. Fence-GAN (Ngo et al. 2019) improves the model\ntraining effect by defining different loss functions so that the\ngenerated samples are located at the edge of normal samples\ndistribution.\nAlthough our model has similar ideas with OCAN, we\nuse generator to get interactions directly rather than embed-\ndings with different generator structure for its application in\nunattributed dynamic graphs. In addition, we use a designed\nloss function to improve our model performance.\nMethodology\nGiven a dynamic graph G = {euv(t) = (u, v, t) : u, v \u2208\nV)}, our goal is to identify a fake interaction eu\u2032v\u2032(t\u2032) =\n(u\u2032, v\u2032, t\u2032) which is different from other edges. To achieve\nthis goal, we design GADY, whose structure is shown in\nFigure 2. Our model has two parts: anomaly generator G\nand discriminator D. For the anomaly generator, it accepts\nthe input noise zi \u2208(\u22121, 1) and output the fake interactions\nG(zi), where G is the generator function. This function is\ntrained through a loss function LG(G, D, Zi) where D is\nthe discriminator function. For the discriminator, it contains\ntwo parts: encoder and decoder. For the part of encoder, it is\naimed to train a function E(), where E(euv(t)) is the out-\nput edge embedding. For the part of decoder, it inputs the\nedge embedding and outputs the anomaly score of the in-\nteraction between these two nodes, and then judges whether\nthe edge is an anomalous edge based on the anomaly score.\nThis process can be formulated as F(embu, embv) where\nu, v \u2208V. For the whole discriminator, it can be formulated\nas D(euv(t)) = F(E(euv(t))). Also, the loss function of\nthe discriminator is LD(G(zi), xi, D) where G(zi) and xi\nare the generated samples and real samples, respectively.\nGenerator\nFramework Design\nThe purpose of the generator is to\ngenerate high-quality and diverse negative samples. For the\nhigh quality of negative samples, our definition is to be as\nsimilar as possible to real samples, but not exactly the same\nas real samples. Such negative samples can make the model\nwork hard to distinguish true and false samples during the\ntraining phase. For the diversity of samples, our definition is\nto have as many abnormal patterns as possible, so that the\nmodel can learn to see more kinds of abnormalities during\nthe training process, thereby improving the detection ability\nof the model. The whole generator part is designed follow-\ning these two principles.\nThe standard input of our generator is random noise such\nas Gaussian noise or Uniform noise. For the type of input\nnoise, through our experiment, we find that different types\nof noise are suitable for different datasets. In our model, we\nuse gaussian noise for its superiority. For the generator struc-\nture, an immediate question is whether the generator should\nproduce edges or edge embedding. Different from OCAN\n(Zheng et al. 2019b), Here we have proved through experi-\nments that generating edges is more conducive to the effect\nof the model 5, which may be because ignoring the embed-\nding process of encoder for interactions, resulting in bias and\nsub-optimal outcomes. Moreover, generating fake edges on\ndynamic graphs still faces some problems. The first is that\nthe edges come in chronological order, and for each batch,\nthe timestamps of the generated edges should be within this\nrange, so as to make the negative samples more similar to\nthe real samples. For the head node and tail node of the edge,\nwe guarantee that the randomly generated nodes are within\ndataset. Based on the above considerations, we define the\nmodel as follows\nG(zi) = R(tanh(f(zi)))\n(1)\nwhere f is defined as a 3-layer multilayer perceptrons and\nzi is the input noise. tanh() is an activation function re-\nconstructing the output to [0, 1], and R is a function recon-\nstructing the output embedding of tanh() to the input form.\nThrough the processing of multilayer perceptrons, noise is\nreconstructed into input embedding. Then, we reconstruct\nthe output of the generator to the form of (u, v, t), and\nthen output the anomaly score and judgment through the\nencoding-decoding process of the discriminator.\nLoss Function design\nThe design of the loss function usu-\nally affects the training goal of the model. In order to ensure\nthat the model is trained according to the above two princi-\nples, we define the following loss function:\nLG(G, D, zi) = 1\nN\nN\nX\ni=1\nlog(|\u03b1 \u2212D(G(zi))|)\n+ \u03b2 \u00d7\n1\n1\nn\nPn\nk=1(CLk)\n(2)\nIn this function, LG is the loss function for the generator, and\nG and D refer to generator and discriminator respectively.\nThe first part is to ensure the quality of generated samples.\n\u03b1 \u2208[0, 1] is expected anomaly score of generated samples\nfrom discriminator. Setting \u03b1 to 0.1 means that the generator\nhopes that the samples generated by itself will get scores of\n0.1 from the discriminator, meaning that although they are\njudged as normal samples, they still has certain anomalous\npatterns. By tuning this parameter, we can control the qual-\nity of negative samples generated by the generator to adapt\nt\nt\nmlp\nor\nupdate\nAnomaly generator\nAnomaly generator\nReconstruct\nagg\nagg\nagg\nBack\nDecoder\nDecoder\nBack\nBack\nNoise\nEncoder\nEncoder\nt3\nt2\nt1\nt3\nt2\nt1\nt6\nt5\nt4\nt6\nt5\nt4\nFigure 2: The architecture of GADY network includes two parts. The first part is the anomaly generator, which generates\nabnormal interactions through input noise, sorts the generated interactions in time, and inputs them into the encoder part together\nwith the normal interactions. The second part is the discriminator, which contains the encoder and decoder. After getting the\ninteractions, the encoder obtains the edge embedding by continuously aggregating the information of time neighbors (R-agg)\nfor the head node and the tail node. Finally, the embedding of the edge is fed into the decoder. For the decoder, it obtains the\nanomaly score of the edge through the representation of the head node and the tail node, finally judging whether the target edge\nis anomalous or normal.\nto different datasets. For the second part, to judge the di-\nversity of generated edges on dynamic graphs, we use the\ncoefficient of variation that are different from Fence-GAN\n(Ngo et al. 2019). \u03b2 shows how concerned we are about the\ndiversity generated samples. Setting b to 10 means that we\nput more emphasis on the diversity of data in the loss func-\ntion, which will prompt the generator to produce more di-\nverse samples to help the discriminator to train. CLk refers\nto the coefficient of variation on the K-th dimension. Finally\nwe uses the reciprocal of the average value of the three di-\nmensions of CLk to measure the diversity of the data. And\nCLk can be formulated as follows:\nCLk =\n1\nN\nPN\ni=1(||G(zk\ni ) \u2212\u00b5k||2)\n\u00b5k\n(3)\nwhere CLK is the k \u2212dim of the interaction (u, v, t), mea-\nsuring the diversity of data in k-th dimension. And k is in\n[1, 3] Through the design of this module, we can success-\nfully generate high-quality and diverse negative samples,\nwhich lays a solid foundation for the superior performance\nof our model.\nDiscriminator\nThe purpose of the discriminator is to distinguish normal\nand abnormal edges from the input edges. This requires that\nit can capture complex continuous dynamic graph structures.\nNowadays, the most powerful framework is the encoder-\ndecoder structure. And our model design follows this archi-\ntecture. where the two modules are defined as follows.\nEncoder\nThe performance of anomaly detection models\nusually depends heavily on the encoding ability of the dis-\ncriminator. In existing continuous dynamic graph models,\nTGNS (Rossi et al. 2020) is a popular model. Based on\nthis model, PINT (Souza et al. 2022) analyzes the inherent\nlimitations of existing messaging models and proposes to\nuse injective temporal aggregation and positional features to\novercome the limitations of temporal Weisfeiler-Leman test\n(Maron et al. 2019; Morris et al. 2019). Although existing\ndynamic graph models are mainly designed for link predic-\ntion tasks, models that capture dynamic graph structures are\nstill applicable to anomaly detection tasks because they can\nsuccessfully capture complex dynamic graph structures. In\nour task, how to build a more excellent continuous dynamic\ngraph model is not the focus of this paper. Based on above\nconsiderations, we use the encoder structure as PINT to cap-\nture complex continuous dynamic graphs.\nSpecifically, before the model starts training, we first cal-\nculate how many temporal paths there are between two\nnodes, and take this as a relative position feature into con-\nsideration in the message passing process. The detailed com-\nputing process can be found in PINT (Souza et al. 2022)\nIn the encoding process, we use the following process to\nget the edge embedding of the target edge.\nE(euv(t)) = concat(emb(u, t), emb(v, t))\n(4)\nemb(u, t) =\nX\nj\u2208\u03b7k\nu([0,t])\nh(sj(t), euj, r(t)\nj\u2192u)\n(5)\nwhere h is a learnable function that can be designed with\ndifferent forms. sv(t) are memory of v. euj, vu(t), and vv(t)\nis the attribute of edge and nodes, which can be set to zero\nif missing. By recursively aggregating the information of k-\nlayers of neighbors, we can finally get the edge embedding.\nTo keep updating memory, we use the following process:\nFor each interaction (u, v, t), we first find temporal neigh-\nbors for head and tail and then get messages from them. This\nprocess can be defined by the following formula:\nsu(t) = upd(su(t\u2212), sv(t\u2212), \u2206t, euv(t))\n(6)\nwhere upd is a learnable function that has different imple-\nmenting ways. su(t\u2212) is the memory of node u just before\ntime t. Although function h and upd can be implemented in\ndifferent forms, to ensure the performance of our detecting\nmodel, we follow the settings as PINT in practice.\nAlthough link prediction and anomaly detection have lim-\nited generality in modeling continuous dynamic graphs, the\ndifferent training objectives between link prediction and\nanomaly detection still lead to inability to do exactly the\nsame in model settings. In fact, in order to complete the\ntask of link prediction, existing dynamic graph models usu-\nally follow the framework of unsupervised training, which\nhelps model training by constructing negative samples in the\ntraining phase, and performs model evaluation through neg-\native sampling in the testing phase. However, different train-\ning objectives and evaluation methods usually require differ-\nent construction methods of negative samples with different\ncharacteristics during model training for its application in\nanomaly detection. Directly applying training methods de-\nsigned for link prediction in anomaly detection may lead to\nsuboptimal results as shown in 1.\nDecoder\nThe purpose of the decoder part is to generate\nanomaly score from the edges embedding and make judg-\nments. Although this part can be designed to be very com-\nplex, for simplicity, we define the model as follows:\nF(e) = Linear(ReLU(Linear(z(e))))\n(7)\nWhere e is the input edge embedding from encoder. In our\nmodel, we use the following function as the overall error of\nthe discriminator.\nLD(G(zi), xi, D) = 1\nN\nN\nX\ni=1\n[\u2212\u03b3log(D(G(zi)))\n\u2212log(1 \u2212D(xi))]\n(8)\nWhere \u03b3 > 0 is a hyperparameter determining we put how\nmuch attention on abnormal edges. When \u03b3 is lower than 1,\nwe put more attention on normal edges and less emphasis\non anomalous edges. When \u03b3 is greater than 1, we put more\nattention on anomalous edges and less emphasis on normal\nedges. By adjusting this hyperparameter, GADY can adapt to\nvarious datasets.\nAlgorithm 1: Training process of GADY\n1: Preprocess: calculate the positional features r(t)\n(i\u2192j) of\neach node i relative to another node j.\n2: repeat\n3:\nInput noise zi to generator G and get fake interactions\n4:\nInput true and fake interactions to Discriminator D\n5:\nFor each interaction (u, v, t)\n6:\nFor each layer l :\n7:\nAggregate message from temporal neighbors\n8:\nGet embedding of two nodes u and v\n9:\nConcatenate to get edge embedding E(euv(t))\n10:\nDecode to get judgement D(euv(t))\n11:\nUpdate memory\n12:\nPass back LG and LD\n13: until End\nTraining process\nIn the training process, firstly, the generator receives noise\nand outputs the generated pseudo edges, then the pseudo\nedges and the real edges are input into the encoder for encod-\ning to obtain the edges embedding, and finally the edges em-\nbedding is decoded into the labels to determine as anomaly\nor not. Finally, the loss of the generator LG and the loss of\nthe discriminator LD passed back to update parameters. This\nprocess can be found in Algorithm 1.\nExperiments\nExperiment Settings\nEvaluation Protocol\nAUC (Area Under Curve) is a met-\nric for evaluating the performance of binary classification\nmodels. It takes account of the evaluation ability of both\npositive and negative classes, and is widely used in anomaly\ndetection models.\nAP (Average Precision) is a metric for evaluating the per-\nformance of object detection models on a single class. It is\nbased on the area under the Precision-Recall (PR) curve,\nwhich shows the precision and recall of the model at dif-\nferent thresholds.\nDifferent from AUC, AP comprehensively considers the\naccuracy and recall rate, which reflect whether the model can\nfind all the anomalies and whether the anomalies found are\ncorrect. Therefore, we believe that the AP metric can com-\nprehensively measure the detection ability of the anomaly\ndetection model better, which is seldom considered in the\nprevious works (Yu et al. 2018; Zheng et al. 2019a; Cai et al.\n2021; Liu et al. 2021). In our follow-up experiments, we also\nproved the superior performance of our model on this eval-\nuating metric.\nDatasets\nTo test the performance of our model, we eval-\nuated our framework on three datasets: UCI Message1\n(Opsahl and Panzarasa 2009), Email-DNC2 (Rossi and\nAhmed 2015), and Bitcoin-OTC3 (Kumar et al. 2018).\n1http://konect.cc/networks/opsahl-ucsocial\n2http://networkrepository.com/email-dnc\n3http://snap.stanford.edu/data/soc-sign-bitcoin-otc\nTable 1: Comparison results of AUC and AP metric between different methods injecting different abnormal ratios on different\ndatasets, where the best performance is shown in bold and second best performance is marked with underline. Model 1%, 5%,\n10% refer to anomaly ratios respectively.\nMethod\nUCI\nBitcoin-OTC\nEmail-DNC\n1%\n5%\n10%\n1%\n5%\n10%\n1%\n5%\n10%\nNODE2VEC\n0.7371\n0.7433\n0.6960\n0.7364\n0.7081\n0.6508\n0.7391\n0.7284\n0.7103\nSPECTRAL CLUSTERING\n0.6324\n0.6104\n0.5794\n0.5949\n0.5823\n0.5591\n0.8096\n0.7857\n0.7759\nDEEPWALK\n0.7514\n0.7391\n0.6979\n0.7080\n0.6881\n0.6396\n0.7481\n0.7303\n0.7197\nNETWALK\n0.7758\n0.7647\n0.7226\n0.7785\n0.7694\n0.7534\n0.8105\n0.8371\n0.8305\nTGN\n0.8771\n8667\n0.8539\n0.9411\n0.9284\n0.9196\n0.9677\n0.9474\n0.9326\nPINT\n0.9265\n0.9232\n0.9229\n0.9227\n0.9226\n0.9225\n0.9758\n0.9754\n0.9783\nADDGRAPH\n0.8083\n0.8090\n0.7688\n0.8341\n0.8470\n0.8369\n0.8393\n0.8627\n0.8773\nSTRGNN\n0.8179\n0.8252\n0.7959\n0.9012\n0.8775\n0.8836\n0.8775\n0.9103\n0.908\nTADDY\n0.8912\n0.8398\n0.837\n0.9455\n0.934\n0.9425\n0.9348\n0.9257\n0.921\nGADY (No-GAN)\n0.9658\n0.9655\n0.9656\n0.9826\n0.9836\n0.9828\n0.9636\n0.9696\n0.9648\nGADY (GAN)\n0.9600\n0.9585\n0.9597\n0.9819\n0.9854\n0.9839\n0.9796\n0.9835\n0.9827\nTADDY(AP)\n0.1760\n0.3428\n0.4743\n0.1402\n0.4287\n0.5995\n0.1046\n0.2854\n0.3986\nGADY (No-GAN)(AP)\n0.4811\n0.4801\n0.4784\n0.6105\n0.6093\n0.6221\n0.6149\n0.6617\n0.6173\nGADY (GAN)(AP)\n0.5025\n0.6771\n0.7617\n0.6842\n0.8395\n0.8797\n0.7462\n0.8583\n0.8861\nThese datasets are constructed from data from forum posts,\nemails, and trading systems respectively, details of which\ncan be found in the appendix.\nBaselines\nWe\nselected\nfour\nclasses\nof\nmodels\nas\nour\nbaselines:\ntraditional\nnon-deep\nlearning\nmethods\nNode2vec (Grover and Leskovec 2016) Spectral Clustering\n(Von Luxburg 2007) DeepWalk (Perozzi, Al-Rfou, and\nSkiena 2014), methods using a combination of deep and\nnon-deep learning NetWalk,(Yu et al. 2018) continuous\ndynamic graph modeling methods directly used for anomaly\ndetection TGNs (Rossi et al. 2020) PINT (Souza et al.\n2022), and anomaly detection methods using deep learning\nAddGraph (Zheng et al. 2019a) StrGNN (Cai et al. 2021)\nTADDY (Liu et al. 2021). A detailed introduction to\nbaselines can be found in the appendix.\nExperiment Setup\nIn order to evaluate the performance\nof our model for anomaly detection and make a fair com-\nparison with baselines, we follow the method test set is built\nin TADDY (Liu et al. 2021). Specifically, we first perform\nspectral clustering on the whole graph, then randomly select\nnodes belonging to different categories, and remove node\npairs that are duplicated with the original dataset. Then, we\nrandomly generate timestamps for node pairs within the time\nrange of the test set, and finally add the generated pseudo-\nedges to the test set and sort them by time.\nIn our experiments, the message function use the identity\nfunction used in TGN (Rossi et al. 2020) Number of layers\nis set to 2 and the training ratio is set to 0.5. Other detailed\nimplement details can be found in appendix.\nFinally, we implement our method using PyTorch 1.12.1\n(Paszke et al. 2019). All experiments are performed on a\nLinux server with a 2.30GHz Intel(R) Xeon(R) Silver 4316\n100 200 500 700\n100020004000\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUC\n(a) UCI0.1\n100 200 500 700\n100020004000\n(b) UCI0.05\n100 200 500 700\n100020004000\n(c) UCI0.01\nAUC\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAP\nAP\nFigure 3: Effects of Slice Size on detection performance.\nToo small or too large slice size will limit model perfor-\nmance. So it is difficult for discrete dynamic methods to cap-\nture fine-grained time information, which is an fatal flaw of\nthis kind of method and validate the superiority of continu-\nous dynamic methods.\nCPU, and NVIDIA Tesla-V100S GPU with 32GB memory.\nMain Results\nWe tested our model on three datasets and set the injected\nanomaly ratio to 1%, 5%, and 10% respectively. We also\nshow the results without using the GAN model (by using\nthe same training method as TADDY to train the discrimina-\ntor), and the results of the complete model using GAN. The\nexperimental results are shown in Table 1. From the experi-\nmental results, we summarize the following conclusions:\n\u2022 Our model has excellent results, and the use of the GAN\nmodule can greatly improve the ability of the model to\ndetect anomalies. Our model outperforms state-of-the-\nart method by 14.6% at most on the UCI dataset with\nanomaly ratio of 10%, achieving SOTA on three real\nworld datasets, and the use of the GAN module can\ngreatly improve the ability of the model to detect anoma-\nbatch 0\nfake\nreal\nbatch 80\nfake\nreal\nbatch 150\nfake\nreal\nFigure 4: Comparison of negative sample distributions gen-\nerated in different periods in the second epoch.It can be\nseen that with the training of the model, the generated neg-\native samples gradually gather evenly near the real samples,\nwhich proves that our generator can indeed generate diverse\nand high-quality negative samples.\nlies which is shown in AP metric.\n\u2022 Continuous dynamic graph models have remarkable ad-\nvantages over discrete dynamic graphs. It can be seen\nfrom the effect of the GADY model that does not use\nGAN that it significantly surpasses the ordinary discrete\ndynamic graph, which verifies our idea.\n\u2022 Our model performance is insensitive to anomaly ratios.\nFrom the table, it can be found that other models are\nmore sensitive to abnormal injection ratio. However, our\nmethod remains a high performance regardless of what\nthe anomaly ratio is.This will allow our model to have a\nwider range of applicability to different anomaly ratios.\n\u2022 The idea of directly using continuous dynamic graph\nmodels for anomaly detection tasks is not enough. From\nthe table, we can find that the detection effects of TGN\n(Rossi et al. 2020) and PINT (Souza et al. 2022) are sim-\nilar to the traditional anomaly detection model TADDY,\nbut still have a large distance compared with our model.\nThis also verifies our conjecture: although continuous\ndynamic graph models for different tasks are all designed\nfor capturing the dynamic graph structure, the model\nfor link prediction tasks cannot be directly applied to\nanomaly detection. It needs to use different negative sam-\npling methods in order to achieve better results.\nSlice Size Study\nIn order to further validate the superiority of continuous dy-\nnamic graph anomaly detection compared to descrete graph\nprocessing, we adjust the slice size of to 100, 300, 500, 700,\n1000, 1500, 2000, and 5000 respectively, and test TADDY\n(Liu et al. 2021) on the UCI dataset with anomaly rate 1%,\n5%, 10%. The experimental results are shown in Figure 3.\nSome people may say that a discrete form of dynamic\ngraph can capture more fine-grained temporal information\nby increasing the number of slices, but this is unrealistically\nverified in this experiment.\nFrom the experimental results, no matter whether the slice\nsize is large or small, the model effect cannot reach the best.\nWhen the slice size is too small, it will be difficult for GNNs\nto capture the structural information on the slice; when the\nslice size is too large, too much time information will be ig-\nnored, thus limiting the anomaly detection effect of discrete\nUCI0.01\nUCI0.05\nUCI0.1\nBtcOTC0.01\nBtcOTC0.05\nBtcOTC0.1\nEmail0.01\nEmail0.05\nEmail0.1\n0.94\n0.96\n0.98\n1.00\nAUC\nembedding\ninteraction\nUCI0.01\nUCI0.05\nUCI0.1\nBtcOTC0.01\nBtcOTC0.05\nBtcOTC0.1\nEmail0.01\nEmail0.05\nEmail0.1\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nAP\nembedding\ninteraction\nFigure 5: Comparison between generating interactions or\nembeddings. The results shows that generating interactions\nhas better performance than generating embeddings directly.\ndynamic graphs.\nPerformance of Generated Negative Edges\nThis experiment is to examine the performance of the gen-\nerator on anomalous samples. Specifically, we select the\nanomalous samples generated in different batches in the sec-\nond epoch, and visualized the data distribution results as\nshown in the Figure 4. Form the results, we find that with the\ncontinuous training of the model, the generated samples are\ngathered at the boundary of the real samples, which proves\nthe high quality of the samples generated by the generator,\nand the generated samples are evenly distributed around the\nreal samples, which demonstrates the diversity of samples\ngenerated by the generator.\nGenerate edges or edge Embedding\nIn this experiment, we test the model performance of using\nthe generator to directly generate edge embedding or edge\ninteractions, which is shown in Figure 5. From the results,\nwe can find that no matter which dataset and what the abnor-\nmality ratio is, the results of directly generating interactions\nare always better than those directly generating edge embed-\nding. This may be because the method of directly generating\nedge encoding ignores the encoder\u2019s process of modeling\nthe interaction, thus causing more bias. Therefore, we can\nconclude that using a generator to generate interactions is a\nbetter choice than directly generating edge representations.\nConclusion\nIn this paper, we discovered the shortcomings of existing\ndiscrete dynamic graphs for anomaly detection, and pro-\nposed that continuous dynamic graphs should be used for\nanomaly detection tasks. Also, our study discovers how ex-\nisting continuous dynamic graphs for link prediction should\nperform on anomaly detection tasks. On this basis, we ex-\nplored how to use the GAN model to obtain more high-\nquality and diverse samples. The final experimental results\nnot only prove the excellent performance of the continuous\ndynamic graph in anomaly detection, but also prove the role\nof the GAN module in helping the model identify anomalies.\nSupplementary experiments on Slice Size, visualization of\ngenerated samples, generated edges or edge embedding fur-\nther prove the shortcomings of discrete dynamic methods,\nthe superiority of GAN to generate negative samples, and\nthe superiority of model settings.\nReferences\nCai, L.; Chen, Z.; Luo, C.; Gui, J.; Ni, J.; Li, D.; and Chen,\nH. 2021.\nStructural temporal graph neural networks for\nanomaly detection in dynamic graphs. In Proceedings of\nthe 30th ACM international conference on Information &\nKnowledge Management, 3747\u20133756.\nDou, Y.; Liu, Z.; Sun, L.; Deng, Y.; Peng, H.; and Yu, P. S.\n2020.\nEnhancing graph neural network-based fraud de-\ntectors against camouflaged fraudsters. In Proceedings of\nthe 29th ACM International Conference on Information &\nKnowledge Management, 315\u2013324.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable fea-\nture learning for networks. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge discovery\nand data mining, 855\u2013864.\nGuo, Q.; Zhao, X.; Fang, Y.; Yang, S.; Lin, X.; and Ouyang,\nD. 2022. Learning Hypersphere for Few-shot Anomaly De-\ntection on Attributed Networks. In Proceedings of the 31st\nACM International Conference on Information & Knowl-\nedge Management, 635\u2013645.\nHan, X.; and Yuan, S. 2021. Unsupervised cross-system log\nanomaly detection via domain adaptation. In Proceedings of\nthe 30th ACM International Conference on Information &\nKnowledge Management, 3068\u20133072.\nJin, M.; Liu, Y.; Zheng, Y.; Chi, L.; Li, Y.-F.; and Pan, S.\n2021. Anemone: Graph anomaly detection with multi-scale\ncontrastive learning. In Proceedings of the 30th ACM Inter-\nnational Conference on Information & Knowledge Manage-\nment, 3122\u20133126.\nKhan, P.; Kader, M. F.; Islam, S. R.; Rahman, A. B.; Ka-\nmal, M. S.; Toha, M. U.; and Kwak, K.-S. 2021. Machine\nlearning and deep learning approaches for brain disease di-\nagnosis: principles and recent advances. IEEE Access, 9:\n37622\u201337655.\nKumar, S.; Hooi, B.; Makhija, D.; Kumar, M.; Faloutsos, C.;\nand Subrahmanian, V. S. 2018. REV2: Fraudulent User Pre-\ndiction in Rating Platforms.\nProceedings of the Eleventh\nACM International Conference on Web Search and Data\nMining.\nLiu, Y.; Pan, S.; Wang, Y. G.; Xiong, F.; Wang, L.; Chen, Q.;\nand Lee, V. C. 2021. Anomaly detection in dynamic graphs\nvia transformer. IEEE Transactions on Knowledge and Data\nEngineering.\nMaron, H.; Ben-Hamu, H.; Serviansky, H.; and Lipman, Y.\n2019. Provably powerful graph networks. Advances in neu-\nral information processing systems, 32.\nMorris, C.; Ritzert, M.; Fey, M.; Hamilton, W. L.; Lenssen,\nJ. E.; Rattan, G.; and Grohe, M. 2019. Weisfeiler and le-\nman go neural: Higher-order graph neural networks. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 33, 4602\u20134609.\nNgo, P. C.; Winarto, A. A.; Kou, C. K. L.; Park, S.; Akram,\nF.; and Lee, H. K. 2019.\nFence GAN: Towards better\nanomaly detection. In 2019 IEEE 31St International Confer-\nence on tools with artificial intelligence (ICTAI), 141\u2013148.\nIEEE.\nOpsahl, T.; and Panzarasa, P. 2009. Clustering in weighted\nnetworks. Social networks, 31(2): 155\u2013163.\nPaszke, A.; Gross, S.; Massa, F.; Lerer, A.; Bradbury, J.;\nChanan, G.; Killeen, T.; Lin, Z.; Gimelshein, N.; Antiga, L.;\net al. 2019. Pytorch: An imperative style, high-performance\ndeep learning library. Advances in neural information pro-\ncessing systems, 32.\nPerozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk:\nOnline learning of social representations. In Proceedings of\nthe 20th ACM SIGKDD international conference on Knowl-\nedge discovery and data mining, 701\u2013710.\nRossi, E.; Chamberlain, B.; Frasca, F.; Eynard, D.; Monti,\nF.; and Bronstein, M. 2020.\nTemporal graph networks\nfor deep learning on dynamic graphs.\narXiv preprint\narXiv:2006.10637.\nRossi, R.; and Ahmed, N. 2015. The network data repository\nwith interactive graph analytics and visualization. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 29.\nSchlegl, T.; Seeb\u00a8ock, P.; Waldstein, S. M.; Schmidt-Erfurth,\nU.; and Langs, G. 2017.\nUnsupervised anomaly detec-\ntion with generative adversarial networks to guide marker\ndiscovery.\nIn Information Processing in Medical Imag-\ning: 25th International Conference, IPMI 2017, Boone, NC,\nUSA, June 25-30, 2017, Proceedings, 146\u2013157. Springer.\nShone, N.; Ngoc, T. N.; Phai, V. D.; and Shi, Q. 2018. A\nDeep Learning Approach to Network Intrusion Detection.\nIEEE Transactions on Emerging Topics in Computational\nIntelligence, 2: 41\u201350.\nSouza, A.; Mesquita, D.; Kaski, S.; and Garg, V. 2022. Prov-\nably expressive temporal graph networks. Advances in Neu-\nral Information Processing Systems, 35: 32257\u201332269.\nTariq, S.; Le, B. M.; and Woo, S. S. 2022.\nTowards an\nAwareness of Time Series Anomaly Detection Models\u2019 Ad-\nversarial Vulnerability. In Proceedings of the 31st ACM In-\nternational Conference on Information & Knowledge Man-\nagement, 3534\u20133544.\nTian, S.; Dong, J.; Li, J.; Zhao, W.; Xu, X.; Wang, B.; Song,\nB.; Meng, C.; Zhang, T.; and Chen, L. 2023. SAD: Semi-\nSupervised Anomaly Detection on Dynamic Graphs. ArXiv,\nabs/2305.13573.\nVon Luxburg, U. 2007.\nA tutorial on spectral clustering.\nStatistics and computing, 17: 395\u2013416.\nYe, J.; and Akoglu, L. 2015. Discovering Opinion Spammer\nGroups by Network Footprints.\nProceedings of the 2015\nACM on Conference on Online Social Networks.\nYu, W.; Cheng, W.; Aggarwal, C. C.; Zhang, K.; Chen, H.;\nand Wang, W. 2018. Netwalk: A flexible deep embedding\napproach for anomaly detection in dynamic networks. In\nProceedings of the 24th ACM SIGKDD international con-\nference on knowledge discovery & data mining, 2672\u20132681.\nZhao, T.; Ni, B.; Yu, W.; Guo, Z.; Shah, N.; and Jiang, M.\n2021. Action sequence augmentation for early graph-based\nanomaly detection. In Proceedings of the 30th ACM Inter-\nnational Conference on Information & Knowledge Manage-\nment, 2668\u20132678.\nZheng, L.; Li, Z.; Li, J.; Li, Z.; and Gao, J. 2019a.\nAddGraph: Anomaly Detection in Dynamic Graph Using\nAttention-based Temporal GCN. In IJCAI, volume 3, 7.\nZheng, P.; Yuan, S.; Wu, X.; Li, J.; and Lu, A. 2019b. One-\nclass adversarial nets for fraud detection. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 33,\n1286\u20131293.\nZhou, S.; Tan, Q.; Xu, Z.; Huang, X.; and Chung, F.-l. 2021.\nSubtractive aggregation for attributed network anomaly de-\ntection. In Proceedings of the 30th ACM International Con-\nference on Information & Knowledge Management, 3672\u2013\n3676.\nAppendix\nComplexity Analysis\nIn the comparison of time complexity, we conducted a com-\nparative experiment, and the result are shown in Figure 6.\nThe result shows that overall our GADY is slower than\nTADDY, and the running time of each epoch is getting\nshorter due to the preprocessing time, but the overall is still\nwithin an acceptable range. It is worth mentioning that if\nthe model of the encoder part can be replaced with a lighter\nstructure, the time overhead will be much lower. And the\nspecific time complexity will be extremely dependent on the\nmodel selected by the encoder part.\n5\n25\n50\n100\nepochs\n3\n4\n5\n6\n7\n8\nAvg time per epoch log(s)\nEmail-DNC\nGADY\nTADDY\nFigure 6: .\nDatasets\n\u2022 UCI Message4 (Opsahl and Panzarasa 2009) is a social\nnetwork collected from a forum of the University of Cali-\nfornia, Irvine. It has 1,899 nodes and 13,838 edges where\neach node represents a student and an edge represents a\nmessage sent.\n\u2022 Email-DNC5 (Rossi and Ahmed 2015) is an email net-\nwork from the 2016 Democratic National Committee\nemail leak. It has 1,866 nodes and 39264 edges where\neach node represents a person in the Democratic Party\n4http://konect.cc/networks/opsahl-ucsocial\n5http://networkrepository.com/email-dnc\nand an edge represents an email sent from one person to\nanother.\n\u2022 Bitcoin-OTC6 (Kumar et al. 2018) is a network of who\ntrusts whom among users who trade on the Bitcoin plat-\nform. It has 5,881 nodes and 35,588 edges where nodes\nare users and edges are ratings between users.\nBaselines\nWe compared GADY with nine competitive methods, which\nmainly include two categories: deep learning methods and\ngraph encoding methods.\n\u2022 DeepWalk (Perozzi, Al-Rfou, and Skiena 2014) is a so-\ncial network collected from a forum of the University of\nCalifornia, Irvine, where each node represents a student\nand an edge represents a message sent.\n\u2022 Node2vec (Grover and Leskovec 2016) is a social net-\nwork collected from a forum of the University of Cali-\nfornia, Irvine, where each node represents a student and\nan edge represents a message sent.\n\u2022 Spectral Clustering (Von Luxburg 2007) is a social net-\nwork collected from a forum of the University of Califor-\nnia, Irvine, where each node represents a student and an\nedge represents a message sent.\n\u2022 Netwalk (Yu et al. 2018) is a classic anomaly detection\nmethod on dynamic graphs, which generates node en-\ncodings based on random walk and dynamically updates\nreservoirs to model networks, and finally uses a dynamic\nclustering model to score edges for anomalies.\n\u2022 TGN (Rossi et al. 2020) is a classic framework for deep\nlearning on dynamic graphs represented as sequences of\ntimed events. It uses memory modules and graph-based\noperators to model patterns of dynamic systems and is\nable to significantly outperform previous approaches be-\ning at the same time more computationally efficient.\n\u2022 PINT (Souza et al. 2022) is a novel architecture that\nleverages injective temporal message passing and rela-\ntive positional features to boost the expressive power of\nTGN. We use this baseline to demonstrate the strong ex-\npressive power of our model.\n\u2022 AddGraph (Zheng et al. 2019a) is an end-to-end dy-\nnamic graph anomaly detection model. It uses GCN and\nGRU-attention to capture structural and temporal infor-\nmation respectively.\n\u2022 StrGNN (Cai et al. 2021) is an end-to-end model for\nanomaly detection on dynamic graphs. It first takes h-\nhop subgraphs for the target edge, then uses GCN model\nto model the structural information within the slice, and\nuses GRU to model the temporal information between\ndifferent slices.\n\u2022 Taddy (Liu et al. 2021) is an end-to-end model for\nanomaly detection on dynamic graphs. It first samples\nsubgraphs and obtains global encodings by diffusion, ob-\ntains local encodings based on distance on subgraphs\nand obtains relative time encodings according to time\ndifference. Then it uses transformers to model dynamic\n6http://snap.stanford.edu/data/soc-sign-bitcoin-otc\ngraph networks and then uses a scoring module to obtain\nanomaly scores.\nExperiment Setup\nWe set alpha to 0.1, beta to 15, and gamma to 0.1 for experi-\nments. For the parameters setting of the encoder part, we fol-\nlowed the hyperparameters setting discussed in PINT(Souza\net al. 2022). For all hyperparameters settings in the base-\nlines, we also follow the best parameters proposed in the\npapers for setting.\n",
    "2310.11676": "PREM: A Simple Yet Effective Approach for\nNode-Level Graph Anomaly Detection\nJunjun Pan\u22171, Yixin Liu\u22172, Yizhen Zheng\u22172, and Shirui Pan1,3\n1 School of Information and Communication Technology, Griffith University, Queensland, Australia\n2 Department of Data Science and AI, Monash University, Melbourne, Australia\n3 Institute for Integrated and Intelligent Systems (IIIS), Griffith University, Queensland, Australia\njunjun.pan.joseph@outlook.com, {yixin.liu, yizhen.zheng1}@monash.edu, s.pan@griffith.edu.au\nAbstract\u2014Node-level graph anomaly detection (GAD) plays\na critical role in identifying anomalous nodes from graph-\nstructured data in various domains such as medicine, social\nnetworks, and e-commerce. However, challenges have arisen due\nto the diversity of anomalies and the dearth of labeled data.\nExisting methodologies - reconstruction-based and contrastive\nlearning - while effective, often suffer from efficiency issues,\nstemming from their complex objectives and elaborate modules.\nTo improve the efficiency of GAD, we introduce a simple method\ntermed PREprocessing and Matching (PREM for short). Our\napproach streamlines GAD, reducing time and memory consump-\ntion while maintaining powerful anomaly detection capabilities.\nComprising two modules - a pre-processing module and an ego-\nneighbor matching module - PREM eliminates the necessity\nfor message-passing propagation during training, and employs\na simple contrastive loss, leading to considerable reductions in\ntraining time and memory usage. Moreover, through rigorous\nevaluations of five real-world datasets, our method demonstrated\nrobustness and effectiveness. Notably, when validated on the\nACM dataset, PREM achieved a 5% improvement in AUC, a\n9-fold increase in training speed, and sharply reduce memory\nusage compared to the most efficient baseline.\nIndex\nTerms\u2014anomaly\ndetection,\nunsupervised\nlearning,\ngraph-structured data, efficiency\nI. INTRODUCTION\nGraph anomaly detection (GAD) is a burgeoning field that\nfocuses on identifying unusual patterns, including anoma-\nlous nodes, edges, or graphs that deviate noticeably from\nthe majority [5], [23], [56]. As an advanced and practi-\ncal technique, GAD has been widely used in fields as di-\nverse as medicine [21], [28], social networks [17], and e-\ncommerce [43]. Among various GAD scenarios, node-level\nGAD holds significant importance as it focuses on discerning\nthe abnormality of individual nodes.\nFor instance, node-\nlevel GAD can identify fraudsters in e-commerce networks,\nthereby contributing to improved security and trust within the\nonline marketplace. In this paper, our research is specifically\ndedicated to addressing the challenging yet practical research\nproblem of node-level GAD*.\nIn practice, building a principled GAD model is a chal-\nlenging task due to two major obstacles: the diversity of\nanomalies and the absence of anomaly labels [5], [23]. Firstly,\nShirui Pan is the corresponding author.\n* indicates equal contributions to this work.\n*In the paper, the term \u201cGAD\u201d specifically indicates node-level GAD.\n10\n2\n10\n3\nTime (second)\n0.75\n0.80\n0.85\n0.90\nAUC\nDOMINANT\nCoLA\nANEMONE\nGRADATE\nSL-GAD\nSub-CR\nPREM\n(a) Train\n10\n\u22123\n10\n\u22121\n10\n1\n10\n3\nTime (second)\n0.75\n0.80\n0.85\n0.90\nAUC\nDOMINANT\nCoLA\nANEMONE\nGRADATE\nSL-GAD\nSub-CR\nPREM\n(b) Test\nFig. 1: AUC and runtime (in second) of baselines and our\nmethod PREM on ACM. Left: training; Right: testing.\nanomalous nodes in graphs can manifest in various ways,\nincluding distinctive node features from their neighborhoods,\natypical interaction patterns with other nodes, and unusual\ntopological substructures [5]. Consequently, the challenge lies\nin developing a single GAD model that can effectively identify\ndiverse types of anomalous nodes comprehensively. On the\nother hand, the expensive nature of annotating anomaly labels\nmakes it difficult to train a GAD model with reliable super-\nvision signals [12]. As a result, traditional shallow (graph)\nanomaly detection methods often struggle to achieve satisfac-\ntory performance in such scenarios [2], [16], [29], [30].\nWith the rising popularity of graph neural networks (GNNs),\nwhich have shown success in various graph-related tasks,\nthere has been an emergence of deep learning-based meth-\nods proposed to tackle the GAD problem. These methods\nleverage unsupervised GNN models to capture intricate graph\nanomaly patterns and learn meaningful representations for\nanomaly detection purposes. Essentially, deep GAD methods\ncan be categorized into two main categories: generative and\ncontrastive. The generative GAD methods mainly use auto-\nencoder-like GNN models for graph data reconstruction and\ncalculate anomaly scores based on the reconstruction error of\nnodes [5], [7], [18], [27], [47]. The contrastive GAD methods,\ndifferently, employ a discriminator to assess the degree of\ninconsistency between target nodes and their neighbors for\nabnormality evaluation [6], [12], [23].\nThough the deep learning-based methods have led to sig-\nnificant improvements for GAD, these advancements have\ncome at a considerable cost in terms of efficiency in time\nand memory consumption. This is because these methods\narXiv:2310.11676v3  [cs.LG]  28 Nov 2023\nCora\nCiteSeer\nPubMed\nACM\nFlickr\n0\n1000\n2000\n3000\n4000\n5000\n6000\nMemory (MB)\n16\n44\n6\n97\n138\nDOMINANT\nCoLA\nANEMONE\nGRADATE\nSL-GAD\nSub-CR\nPREM\nPREM-b\nFig. 2: GPU Memory usage (in MB) of baselines and our\nmethod. Notices that we fix batch size=300 for all contrastive\nlearning-based methods and the mini-batch version of PREM\n(termed PREM-b) for comparison purposes.\noften rely on well-hand-crafted modules and complicated\nobjectives to achieve great performance. For example, the\nmultiple rounds estimation used in contrastive GAD prolongs\nthe model evaluation time. In addition, the integration of\nstructural-aware modules [27], [47] or edge reconstruction\ntasks [5], [7], [47] intensifies memory usage, which escalates\nwith dataset size, impeding memory efficiency. These limi-\ntations hinder the application of these methods in real-world\nscenarios characterized by vast amounts of data, underscoring\nthe need for more efficient GAD solutions.\nTo enable effective, efficient, and computationally friendly\nanomaly detection on real-world graph data, in this paper, we\nintroduce a novel GAD method termed PREprocessing and\nMatching, herein referred to as PREM. This framework offers\nsuperior performance over existing methods, coupled with\nexceptional efficiency in both time and memory. For instance,\nwhen evaluated on the ACM dataset, as depicted in Fig. 1 and\nFig. 2, our method substantially surpasses all existing GAD\nbaselines. It registers an impressive performance improvement\nof 5% in AUC, while significantly reducing the training time\n(9 \u00d7 faster) and consumes much less memory compared with\nthe most efficient baseline. The crux of PREM lies in its\nstreamlined two-part structure: a pre-processing module and\nan ego-neighbor matching module. Specifically, our approach\neliminates message passing propagation during training, as the\npre-processing stage aggregates neighbor features only once,\nleading to much lower time and space complexity. In addition,\nwe employ a similarity-based method, eschewing the need\nfor multi-round procedures that usually hamper efficiency.\nFurthermore, with the implementation of mini-batch process-\ning, PREM requires only a minimal amount of memory. In\nsummary, the contribution of our work is three-fold:\n\u2022 We develop a simple yet very efficient framework,\nPREprocessing and Matching, PREM, for graph anomaly\ndetection (GAD), adhering to the principle of simplicity\nand minimalism.\n\u2022 Our proposed method, namely PREM is orders of mag-\nnitude faster and consumes much less memory compared\nwith existing GAD baselines.\n\u2022 The robustness and effectiveness of our method have been\nthoroughly evaluated and analyzed across five real-world\ndatasets, where PREM achieved the best performance\ncompared with existing GAD baselines.\nII. RELATED WORK\nA. Graph Neural Networks\nGraph Neural Networks (GNNs), falling under the broad\numbrella of deep neural network [15], are specifically for han-\ndling semi-structured graph data. It is facilitated by harnessing\nboth the attributive and the topological information inherently\npresent within the non-Euclidean graph-structured data. The\nvery inception of GNNs can be traced back to [3], where an\ninnovative spectral-based method was introduced. This method\nfacilitated the extension of convolution networks to graph-\nbased structures. Following this inaugural contribution, the\nconcept was further built upon with the development of a\nrange of subsequent spectral-based convolution GNNs [4],\n[10]. These furthered the field by adopting filters designed\nin accordance with the principles of graph signal process-\ning [33], [53]. One important milestone in this domain was\nthe advent of Graph Convolutional Networks (GCN) [13]. The\nGCN essentially served as a bridge, connecting spectral-based\nmethods and spatial-based GNN approaches by simplifying\nspectral graph convolutions. Specifically, it approximates the\nfirst-order Chebyshev polynomial filters.\nThe development of GCN paved the way for the rapid evo-\nlution of spatial-based methods. These methods were not only\nmore efficient but also exhibited wider applicability across\nvarious use cases. For example, Graph Attention Networks\n(GAT) [37] was a significant evolution in this regard. GAT\nintegrated the attention mechanism [36], making it possible\nto account for varying levels of significance among node\nneighbors, rather than just employing a basic average of\nneighboring data. Another significant spatial-based approach\nwas the Simplified Graph Convolutions (SGC) [41]. It further\nsimplifies GCN by removing non-linearity and collapsing\nweight matrices between graph convolution layers, reducing\nthe overall complexity. In addition to these major contribu-\ntions, numerous studies have focused on improving GNN\nfrom other unique angles. These include the expansion of\nscalability [1], [45], [51], the reduction of prior knowledge\nrequirement [22], [24], [48]\u2013[50], [54], [59] and the enlarge-\nment of receptive fields [14], [19], [35], [42], [54]. GNN\napproaches have found widespread success across multiple\ndomains. They have been employed effectively in areas such as\nbusiness analysis [52], federated learning [34], latent structure\ninference [25], and question answering [26].\nB. Traditional Graph Anomaly Detection\nGraph anomaly detection can be defined as classifying\noutliers from normal nodes. With the increasing use of at-\ntributed networks in industry, the importance of graph anomaly\nTABLE I: Architecture comparison between state-of-the-art GAD methods and PREM.\nMethod\nFeature\nReconstruction\nStructure\nReconstruction\nSubgraph\nSampling\nNode-Subgraph\nContrast\nNode-Node\nContrast\nMulti-View\nContrast\nMulti-round\nEvaluation\nDOMINANT\n\"\n\"\n%\n%\n%\n%\n%\nCoLA\n%\n%\n\"\n\"\n%\n%\n\"\nANEMONE\n%\n%\n\"\n\"\n\"\n%\n\"\nGADMSL\n%\n%\n\"\n\"\n\"\n\"\n\"\nSL-GAD\n\"\n%\n\"\n\"\n%\n\"\n\"\nSub-CR\n\"\n%\n\"\n\"\n%\n\"\n\"\nPREM\n%\n%\n%\n\"*\n\"*\n%\n%\n* Without training-time propagation\ndetection has become popular in recent years. In order to\neffectively detect anomalies in attributed networks, various\nnon deep learning-based algorithms and methodologies have\nbeen developed, each with their own strengths and weaknesses.\nA pioneer work, AMEN [30] mines anomaly information\nfrom the internal and external consistency of neighborhoods.\nRadar [16] analyses the residual between target node attributes\nand the majority to calculate the anomaly score. ANOMA-\nLOUS [29] extends the framework of [16] by incorporating\nCUR decomposition with residual analysis. DEEPFD [39]\nfirst attempts to learn the embedding via reconstructing tar-\nget node\u2019s features, and then detects anomaly substructures\nvia clustering. [57] improves the scalability by growing and\nmerging communities of nodes that share similar properties.\nHCM [11] learns the embedding by predicting the shortest\ndistance between nodes to obtain both local and global in-\nformation. However, due to the limitations of their shallow\nmechanism, their performance is proven to be limited when\nhandling high-dimensional features and complex structures.\nC. Deep Learning-based Graph Anomaly Detection\nCompared to traditional methods, Deep learning-based\ngraph anomaly detection (GAD) methods are able to better\ncapture the complex relationships and structures present in\nattributed networks, resulting in more accurate and efficient\nanomaly detection. They mainly fall into two categories:\ngenerative and contrastive learning. DOMINANT [5] pioneers\nthe first generative GAD method by learning node embed-\ndings via minimizing reconstruction error for attributes and\nadjacency matrix, exploiting both attribute and topological\nfeatures. SpecAE [18] follows a similar approach but uses\ndeconvolution and Gaussian Mixture Model for anomaly es-\ntimation. AnomalyDAE [7] decouples attribute and structural\nencoders to efficiently model interactions. ComGA [27] in-\ncorporates community detection, while AS-GAE [47] adds a\nlocation-aware module. For contrastive learning-based meth-\nods, CoLA [23] addresses challenges with a contrastive learn-\ning paradigm, using a discriminator to detect inconsisten-\ncies between target node and neighbor subgraph embeddings.\nANEMONE [12] introduces a patch-level contrastive task\nfor multi-scale anomaly detection. GRADATE [6] improves\nthe framework through graph augmentation and multi-views\ncontrast. SL-GAD [55] combines attribute reconstruction and\nnode-subgraph contrast, while Sub-CR [46] uses masked au-\ntoencoder and graph diffusion to fuse attributes with local\nand global topological information. Apart from the two main\ncategories, There are also other methods targeting GAD, such\nas subtractive aggregation [56], one-class SVM [40], and\ncomplementary learning [20].\nAlthough existing deep GAD methods have demonstrated\npromising results, their limitations in efficiency and scalability\nhinder their applicability in real-world scenarios involving\nlarge-scale graphs with a vast number of nodes and edges.\nAs shown in Table I and Fig. 1, 2, integrating more modules\nimproves performance but at the expense of efficiency. To\naddress these issues, we propose PREM, a novel approach\nthat significantly streamlines the GAD process by employing\na linear discriminator in an embarrassingly simple manner.\nNot only does PREM outperform the baseline methods by a\nconsiderable margin, but it also runs much faster and has lower\nmemory consumption. This enhanced efficiency and scalability\nmake PREM a more suitable choice for practical applications\nin detecting anomalies within extensive graph structures.\nIII. PRELIMINARIES\nA. Definition\nThroughout this paper, we use calligraphic fonts, bold\nlowercase letters, and bold uppercase letters to denote sets,\nvectors, and matrices, respectively. We denote an attributed\ngraph as G = (V, E, X), where V = {v1, \u00b7 \u00b7 \u00b7 , vn} is the set\nof vertices (or nodes), the number of nodes |V| is n, and E is\nthe set of edges, where |E| = m, which can also be expressed\nby a binary adjacent matrix A \u2208Rn\u00d7n. The feature matrix of\na graph X \u2208Rn\u00d7d can be represented as a list of row vectors\nX = (xT\n1 , \u00b7 \u00b7 \u00b7 , xT\nn), where xi is the feature vector of node vi\nand d is the dimension of features.\nB. Problem Formulation\nIn this paper, we focus on the unsupervised node-level\ngraph anomaly detection (GAD) problem. The aim is to learn\na scoring function f(\u00b7) to estimate the anomaly score si\nfor each node vi. A larger anomaly score si indicates that\nthe node vi is more likely to be an anomalous node. In\nthis paper, we aim to develop a novel GAD model (serving\nas f(\u00b7)) that estimates the anomaly score efficiently under\nstrictly unsupervised settings without ground-truth labels of\nthe anomaly provided.\nIV. METHOD\nIn this section, we introduce the proposed model term\nPREM in detail. The overall pipeline of PREM is illustrated\nin Fig. 3. On the highest level, PREM is composed of\ntwo components: 1) A pre-processing module is designed to\nprecompute the neighbor features, thereby eliminating the need\nfor costly message passing during training; and 2) a ego-\nneighbor matching module that learns the matching patterns\nbetween ego and neighbor information with a simple linear\nlayer-based network. Once PREM is well-trained, the anomaly\nscores can be estimated by the ego-neighbor similarity directly.\nIn the following subsections, we first introduce the two main\ncomponents of PREM respectively. Then, we illustrate a\nsimple contrastive learning objective to train PREM model.\nFinally, we expound on the calculation of anomaly scores\nduring inference and discuss the superb efficiency of PREM.\nA. Message Passing-based Pre-Processing\nIn conventional GAD methods, message passing-based\nGNNs are commonly used to capture contextual information of\neach node, aiding in the prediction of node-level abnormality.\nHowever, the forward and backward propagation involved in\nmessage passing can be time-consuming, which significantly\naffects the efficiency of model training [8]. Consequently, an\nimportant question arises: Is it possible to expedite the learning\nprocess by eliminating message passing during training? To\naddress this question, we propose a pre-processing scheme\nbased on message passing that enables PREM to perform\nmessage passing only once throughout the learning procedure.\nSince the agreement between each node and its context (i.e.,\nits surrounding substructure) is highly correlated to the node-\nlevel abnormality [12], [23], in our pre-processing module,\nwe concentrate on extracting two types of information: ego\ninformation, which indicates the attributes specific to a node,\nand neighbor information, encompassing the contextual knowl-\nedge surrounding the node. Firstly, to capture ego information,\nwe construct ego features X(e) by collecting the raw feature\nvectors together directly, i.e., X(e) = X. This approach\nenables us to capture the intrinsic attributes and characteristics\nof each node without considering the surrounding context.\nThen, to extract neighbor information, we propose to\nconduct message passing on the raw features to generate\nneighbor features X(n). One possible approach is to adopt\nthe propagation rule used in mainstream GNNs, such as\nGCN [13] and SGC [41], for our message passing-based pre-\nprocessing. However, it is important to note that conventional\nGNNs typically aggregate both ego and neighbor information\nduring message passing. This aggregation strategy may lead to\ninformation leakage in the downstream ego-neighbor matching\nnetwork, subsequently impacting the performance of anomaly\ndetection. To alleviate this issue, we design an anonymized\nmessage passing scheme that enables the extraction of neigh-\nbor information separately. In concrete, the neighbor features\ncan be calculated by:\nX(n) = PX,\n(1)\nwhere P is the anonymized propagation matrix that can be\nwritten by:\nP = M \u2217( \u02dcD\u22121/2 \u02dcA \u02dcD\u22121/2)k,\n(2)\nwhere k is the propagation steps in message passing, \u02dcA =\nA + I is the adjacency matrix with self-loop, \u02dcD is the degree\nmatrix of \u02dcA, and M is a self-anonymization mask where\ndiagonal entries are all zero while the remaining entries are\nall one. By utilizing the self-anonymization mask, the signals\nassociated with ego features can be filtered out during the\nmessage passing process. Consequently, each row vector x(n)\ni\nin the neighbor features matrix X(n) can concentrate on\nsummarizing the neighbor information independently, without\nbeing influenced by the ego information.\nIt is worth noting that X(e) and X(n) are computed only\nonce during the learning procedure, and there are no trainable\nweights to be involved in the pre-processing step. Thanks to\nthis merit, our method is much simpler and faster than existing\nmethods that involve multiple layers of GNNs in GAD models.\nB. Ego-Neighbor Matching Network\nOnce we have obtained the ego and neighbor features, we\nproceed to construct an ego-neighbor matching network. This\nnetwork is designed to learn the matching patterns between the\nego information and the corresponding neighbor information\nof each node. By capturing these matching patterns, we can\neffectively assess the normality of nodes during the testing\nphase.\nIn specific, the proposed ego-neighbor matching network\nis composed of two linear layers and a cosine similarity\nunit. Note that the original features of attributed graphs are\nfrequently high-dimensional, and there are often correlations\namong different feature dimensions. In such scenarios, directly\nmeasuring the similarity solely at the feature space can be\ninefficient and ineffective. Therefore, instead of calculating the\nfeature-level similarity, we first introduce two learnable linear\nlayers to map the ego features and neighbor features into a\nlow-dimensional space, respectively. Formally, given the ego\nfeature vector x(e)\ni\nand neighbor feature vector x(n)\ni\nfor node\nvi, the linear mapping can be written by:\nh(e)\ni\n= x(e)\ni W1 + b1,\nh(n)\ni\n= x(n)\ni\nW2 + b2,\n(3)\nwhere h(e)\ni\nand h(n)\ni\nare low-dimensional ego embedding\nvector and neighbor embedding vector respectively, while W1,\nW2, b1, and b2 are learnable weights/bias parameters of the\nlinear layers.\nAfter linear mapping, now we can evaluate the ego-neighbor\nsimilarity with the compact embeddings. To simplify, we\nemploy a cosine similarity unit to measure the agreement\nbetween two embeddings, which can be formulated by:\nci = cos(h(e)\ni , h(n)\ni\n),\n(4)\nEgo\nfeatures\nAnonymized\nmessage passing\nNeighbor\nfeatures\nPre-processing module\nEgo-neighbor matching module\nCosine\nSim\n!!: 0.11\n!\": 0.05\n!#: 0.14\n!$: 0.73\n!%: 0.21\n!&: 0.10\n!': 0.09\nAnomaly\nscores\nLinear\nLinear\n!\nFig. 3: The overall pipeline of PREM that composes of a pre-processing module and an ego-neighbor matching module. In\nthe first module, we conduct anonymized message passing, i.e., without the self-loop, on the given graph to obtain neighbor\nfeatures. Then, in the ego-neighbor matching module, we input the ego features, i.e., raw features, and neighbor features into\na discriminator, which includes a linear layer and the cosine similarity computation component, to obtain anomaly scores. The\nhigher the score is, more likely the node is an anomaly.\n2\nEgo\nfeatures\nNeighbor\nfeatures\nanchor\npositive\nego\nnegative\nneighbor\nnegative\nFig. 4: Illustration of positive and negative pairs in PREM.\nwhere cos(\u00b7, \u00b7) is the cosine similarity function and ci \u2208\n[\u22121, 1] is the ego-neighbor similarity of node vi. Intuitively,\nci can be used to indicate the normality of node vi: according\nto the homophily assumption, a (normal) node tends to have\nsimilar behavior with its neighborhoods, leading to a larger ci\ngiven by our matching network; in contrast, anomalous nodes\nusually break the homophily assumption due to their noisy\nconnections and unexpected features [23], hence have smaller\nci. In other words, we can easily estimate the anomaly score\naccording to the learned similarity.\nC. Contrastive Learning-based Model Training\nIn this subsection, we present the training strategy for\nour PREM model. Given the challenge of obtaining ground-\ntruth labels for anomalies, our focus in this paper is on\nthe unsupervised GAD problem [5], [23]. In this context, it\nbecomes necessary to design a carefully crafted unsupervised\nlearning objective to train the PREM model, as opposed to\nrelying on labeled data. Inspired by the success of contrastive\nlearning on GAD tasks [12], [23], we design a simple yet\neffective contrastive learning objective for our model training.\nRecalling that we aim to maximize the ego-neighbor sim-\nilarity of a normal node. However, we cannot identify the\nnormal nodes exactly without accessing their labels during\nthe training phase. Fortunately, with the assumption that the\nnumber of anomalies is much smaller than which of normal\nnodes, we can directly maximize the ego-neighbor similarity\nof all nodes. By doing so, PREM can effectively capture the\ndominant matching patterns observed in normal nodes. This\napproach allows us to prioritize the identification of normal be-\nhavior patterns while accommodating the inherent difficulty of\nprecisely identifying individual normal nodes without explicit\nlabels. By focusing on the majority of normal nodes, we can\nstill train the model to capture the essential matching patterns\nassociated with normalcy. Formatting this idea to a contrastive\nframework, we define the ego and neighbor embeddings of the\nsame node, i.e., h(e)\ni\nand h(n)\ni\n, as a positive pair. Then, their\ncosine similarity is denoted as a positive term in contrastive\nlearning:\nc(pos)\ni\n= ci = cos(h(e)\ni , h(n)\ni\n).\n(5)\nIf we only maximize the positive term c(pos)\ni\n, it would\nlead to the model collapse problem, i.e., the model assigns\nhigh values or probabilities to all input pairs. The model\ncollapse problem can severely reduce the discriminative power\nand hinder the effective detection of anomalies. To avoid this\nissue, it is necessary to involve negative pair in our contrastive\ntraining scheme. Regarding the ego embedding h(e)\ni\nof node vi\nas an anchor, we can naturally define the neighbor embedding\nh(n)\nj\nof another node vj as a negative sample. In this case,\nwe can obtain a negative pair (h(e)\ni\nand h(n)\nj\n) that forms a\nneighbor-based negative term in contrastive learning:\nc(negnbr)\ni\n= cos(h(e)\ni , h(n)\nj\n).\n(6)\nWith the neighbor-based negative term, now we can learn to\ndiscriminate the matching patterns between ego and neighbor\ninformation with PREM. Nevertheless, it is important to\nnote that relying solely on neighbor-based negative samples\nmay introduce a potential issue known as the \u201ceasy nega-\ntive\u201d problem [32], [58]. The easy negative problem occurs\nwhen the negative pairs provided by the neighbors are too\nstraightforward for the model to discriminate, resulting in the\nmodel not effectively learning meaningful matching patterns.\nThis can hinder the model\u2019s ability to accurately identify\nanomalies. To address the easy negative problem, we propose\nintroducing more diverse and challenging negative samples.\nOne effective strategy to achieve this is by leveraging ego\nfeatures. Discriminating among ego features is particularly\nchallenging because the raw features of two nodes can be\nvery similar in a graph. Motivated by this, in addition to\nthe neighbor-based negative term, we also incorporate an ego-\nbased negative term for contrastive training. In concrete, given\nthe ego embedding h(e)\ni\nas an anchor, we randomly sample\nthe ego embedding h(e)\nk\nof another node vk as its ego-based\nnegative sample. Then, the ego-based negative term can be\nwritten by:\nc(negego)\ni\n= cos(h(e)\ni , h(e)\nk ).\n(7)\nFinally, we establish a contrastive loss function to maximize\nthe positive contrastive terms while minimizing the negative\ncontrastive terms. Unlike the majority of works [31], [44], [59]\nthat employ the Info-NCE contrastive loss, in PREM, we use\na binary cross-entropy-like contrastive loss for model training,\nwhich can be regarded as an extension of Jensen-Shannon\ndivergence-based contrastive loss [9], [38]. Specifically, we\nfirst use a linear mapping to project the positive/negative terms\nc \u2208[\u22121, 1] into \u02c6c \u2208[0, 1], and then acquire the loss by:\nL = \u2212\nN\nX\nt=1\n(log(\u02c6c(pos)\ni\n) + \u03b1 log(1 \u2212\u02c6c(negnei)\ni\n)+\n\u03b3 log(1 \u2212\u02c6c(negego)\ni\n)),\n(8)\nwhere \u03b1 and \u03b3 are the hyper-parameters for trade-off. Com-\npared to Info-NCE loss with O(n2) time complexity, our loss\nis much more efficient to compute (with O(n) complexity).\nSuch a nice property ensures the rapid model training of\nPREM. Fig. 4 illustrates an example of the definitions of\npositive and negative pairs in PREM.\nTABLE II: Statistic information of datasets.\nDataset\nNodes\nEdges\nAttributes\nAnomalies\nCora\n2,708\n5,429\n1,433\n150\nCiteSeer\n3,327\n4,732\n3,703\n150\nPubMed\n19,717\n88,648\n500\n600\nACM\n16,484\n71,980\n8,337\n650\nFlickr\n7,575\n239,738\n12,407\n450\nD. Anomaly Score Calculation\nOnce the PREM model is well-trained, we can estimate the\nanomaly score using the ego-neighbor matching module. In\ncontrast to existing methods that fuse anomalous information\nfrom both positive and negative contrastive terms [23], PREM\ntakes a simpler approach. We use the opposite value of the\nego-neighbor similarity c(pos)\ni\nas the anomaly score for a\nnode vi, denoted as si = \u2212c(pos)\ni\n. As discussed in Sec IV-B,\nthe similarity c(pos)\ni\neffectively reflects the normality of each\nnode. Hence, by taking the negative value of similarity, we\nobtain an anomaly score that captures the abnormality of\nthe node. In the inference phase, we can directly discard\nthe negative contrastive terms since the similarity measure\nalready encompasses the information needed to assess the\nnormality of nodes. In contrast to existing contrastive GAD\nmethods that involve negative score calculations and multi-\nround estimations [12], [23], PREM offers improved efficiency\nand stability during anomaly score estimation.\nE. Efficiency Discussion\n1) Mini-batch-based Model Training: Thanks to the pre-\nprocessing of ego and neighbor features, PREM is capable\nof supporting mini-batch-based optimization, making model\ntraining computationally efficient. This allows us to train\nPREM using batches of ego/neighbor features instead of\nthe entire graph, resulting in reduced memory consumption.\nAdditionally, this feature enables training on edge devices and\nsuggests the potential scalability of PREM.\n2) Complexity Analysis: We discuss the time complexity of\neach component in PREM respectively. For the pre-processing\nmodule, the complexity of message passing is O(kmd). Note\nthat this computation can be computed only once in the whole\nlearning procedure. For the ego-neighbor matching module,\nthe complexity of linear mapping is O(nddh), where dh is\nthe hidden dimension of embeddings. The cost of the cosine\nsimilarity unit is O(ndh). To compute the contrastive loss, the\ntime complexity is O(n). The complexity of anomaly scoring\nis the same as which of the ego-neighbor matching module. To\nsum up, the pre-processing, training, and testing complexity of\nPREM are O(kmd), O(nddhT), and O(nddh), respectively,\nwhere T is the training epoch.\nV. EXPERIMENTS\nIn this section, we propose our experiment settings and\nresults. We first introduce the datasets, followed by the param-\neters of training. The experiment results are then presented, in-\ncluding an ablation study and sensitivity analysis. Our method\nis implemented in PyTorch and is evaluated on a Desktop\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\n(a) Cora\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\n(b) CiteSeer\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\nDOMINANT\nCoLA\nANEMONE\nGRADATE\nSL-GAD\nSub-CR\nPREM\nPREM-b\n(c) PubMed\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\n(d) ACM\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nTrue Positive Rate\n(e) Flickr\nFig. 5: ROC curves on five datasets.\nTABLE III: Hyper-parameters of our method.\nHyper-parameter\nCora\nCiteSeer\nPubMed\nACM\nFlickr\nEmbedding dim. dh\n128\n128\n128\n128\n128\nProp. step k\n2\n2\n2\n2\n2\nLearning rate\n0.0003\n0.0003\n0.0005\n0.0001\n0.0005\nEpoch T\n100\n100\n400\n200\n1500\nTrade-off param. \u03b1\n0.9\n0.9\n0.6\n0.7\n0.3\nTrade-off param. \u03b3\n0.1\n0.1\n0.4\n0.2\n0.4\nPC with a Ryzen 5800x CPU, an RTX2070 GPU, and 32GB\nRAM. The code is available at https://github.com/Campanula\nBells/PREM-GAD.\nA. Experimental Settings\n1) Datasets: The proposed method is evaluated on five\ndatasets including four citation network datasets (i.e. Cora,\nCiteSeer, PubMed, and ACM) and a social network dataset\n(Flickr). The specific statistical details are outlined in Table II.\nWe follow the settings described in prior studies [5], [23] to\ninject anomalous nodes. Concretely, there are two types of\nanomaly injection: structural anomalies and attribute anoma-\nlies injection. In terms of injecting structural anomalies, we\nbuild q cliques of size p by building edges amongst randomly\nchosen nodes. This results in the formation of pq structural\nanomalies. For attribute anomalies injection, substitute the\nfeature of the target node with the feature vector of the one\nTABLE IV: AUC of PREM and baselines.\nMethod\nCora\nCiteSeer\nPubMed\nACM\nFlickr\nDOMINANT\n0.8128\n0.8197\n0.7896\n0.7561\n0.7436\nCoLA\n0.9046\n0.8894\n0.9468\n0.8375\n0.7616\nANEMONE\n0.9207\n0.9282\n0.9534\n0.8492\n0.7679\nGRADATE\n0.9032\n0.8904\n0.9450\n0.8580\n0.7522\nSL-GAD\n0.9178\n0.9221\n0.9627\n0.8143\n0.7965\nSub-CR\n0.9023\n0.9272\n0.9687\n0.8060\n0.7921\nPREM\n0.9510\n0.9510\n0.9510\n0.9779\n0.9779\n0.9779\n0.9719\n0.9719\n0.9719\n0.9056\n0.8613\n0.8613\n0.8613\nPREM-b\n0.9508\n0.9741\n0.9714\n0.9088\n0.9088\n0.9088\n0.8561\nexhibiting the highest Euclidean distance in an arbitrarily\nselected candidate node set. A total of pq attribute anomalies\nhave been injected for balancing.\n2) Baselines:\nIn this paper, we compare PREM with\nsix state-of-the-art (SOTA) baselines, including a genera-\ntive method (DOMINANT [5]), three contrastive methods\n(CoLA [23], ANEMONE [12], and GADMSL [6]), and two\nhybrid methods that combines generative and contrastive learn-\ning (SL-GAD [55] and Sub-CR [46]).\n3) Evaluation metric: We utilize ROC-AUC as our evalua-\ntion metric to compare the results. The ROC curve is the plot\nof the true positive rate and false positive rate under different\nthresholds. AUC is the area under the ROC curve which is\ncapped by 1. The larger the AUC, the better the classifier. We\nalso record the time taken to train and evaluate methods in\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1 0.2 0.3 0.4 0.5\nAUC\n0.4\n0.6\n0.8\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n(a) Cora\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1 0.2 0.3 0.4 0.5\nAUC\n0.4\n0.6\n0.8\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n(b) CiteSeer\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1 0.2 0.3 0.4 0.5\nAUC\n0.93\n0.94\n0.95\n0.96\n0.97\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n(c) PubMed\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1 0.2 0.3 0.4 0.5\nAUC\n0.82\n0.84\n0.86\n0.88\n0.90\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n(d) ACM\n0.2\n0.4\n0.6\n0.8\n1.0\n0.1 0.2 0.3 0.4 0.5\nAUC\n0.75\n0.80\n0.85\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n(e) Flickr\n(f) Features\n(g) Embeddings (w Anony.)\nEgo (Normal)\nNei (Normal)\nEgo (Anomaly)\nNei (Anomaly)\n(h) Embeddings (w/o Anony.)\nFig. 6: (a)-(e): Sensitivity analysis for balancing factors \u03b1 and \u03b3; (f)-(h) t-SNE visualizations of input features and learned\nembeddings by PREM (with and without Anonymization) on PubMed dataset. In (f)-(h), Ego (Normal/Anomaly) represents ego\nfeatures for normal and anomaly nodes respectively, while Nei(Nomral/Anomaly) is neighbor feature for normal and anomaly\nnodes respectively.\nTABLE V: Training and testing time of GAD methods in seconds.\nMethod\nCora\nCiteSeer\nPubMed\nACM\nFlickr\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\nDOMINANT\n3.9946\n0.0050\n8.3463\n0.0050\n104\n0.0100\n124\n0.0090\n0.0090\n0.0090\n266\n0.0100\nCoLA\n95\n235\n180\n456\n711\n1770\n1639\n1056\n712\n454\nANEMONE\n101\n247\n191\n465\n732\n1797\n1643\n1047\n761\n453\nGRADATE\n541\n319\n917\n557\n3991\n2392\n2580\n1491\n1234\n703\nSL-GAD\n221\n546\n399\n1007\n1637\n4171\n4585\n2697\n2099\n1262\nSub-CR\n145\n422\n243\n708\n1104\n3385\n720\n2241\n1355\n958\nPREM\nPREM\nPREM\n0.5386\n0.5386\n0.5386\n0.0007\n0.0007\n0.0007\n1.1002\n1.1002\n1.1002\n0.0011\n0.0011\n0.0011\n4.8098\n4.8098\n4.8098\n0.0014\n0.0014\n0.0014\n14\n14\n14\n0.0112\n68\n68\n68\n0.0071\n0.0071\n0.0071\nPREM-b\n5.6882\n0.0080\n11\n0.0190\n95\n0.0310\n214\n0.1682\n1028\n0.1031\nseconds to compare the efficiency of methods.\n4) Implementation Details: We present the parameter set-\nting of PREM for the five datasets on Table III. We set the\nnumber of propagation steps k to 2 and set the embedding\ndimension dh to 128 for all datasets. The learning rate of\nPREM for Cora, CiteSeer, PubMed, ACM, and Flickr are\n3e-4, 3e-4, 5e-4, 1e-4, 5e-4, and the epochs are 100, 100,\n400, 200, 1500, respectively, to get better performance on\ndatasets with various size. For balance factors \u03b1 and \u03b3, we\nuse grid search with search space [0.1, 0.2, . . . , 1.0] for \u03b1 and\n[0.1, 0.2, . . . , 0.5] for \u03b3. We also consider PREM with a mini-\nbatch size of 300 (denoted as \u201cPREM-b\u201d) for comparison. Our\nmethod has been evaluated for 10 times with different random\nseeds to report the average results. For baselines, we follow\nthe settings in their paper to obtain the reported results.\nB. Comparative Results\nIn this subsection, we evaluate our proposed PREM by\ncomparing its performances and training and evaluation time\nto other baseline methods. The AUC of PREM and baselines\nare summarized in Table IV, and the training and testing time\nare listed in Table V. Fig. 5 shows ROC curves.\n1) Anomaly Detection Performance: From the data pre-\nsented in Table IV, it is evident that PREM achieves the\nbest performance across all datasets, markedly outperforming\nbaselines. This accomplishment is a testament to its robustness\nand the efficacy of its design. For example, PREM achieves the\nhighest performance in the CiteSeer dataset, where it achieves\nan AUC of 0.9779. This result is not only the best in the\ngroup but it\u2019s also noticeably higher than the other methods,\ni.e., leading by around 5%, illustrating the effectiveness of\nPREM. The PubMed and ACM datasets further underscore\nPREM\u2019s dominance. It not only processes these large datasets\nTABLE VI: Maximum GPU memory usage of GAD methods in MB.\nMethod\nCora\nCiteSeer\nPubMed\nACM\nFlickr\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\ntrain\ntest\nDOMINANT\n282\n282\n572\n572\n5943\n5943\n6051\n6051\n4399\n4401\nCoLA\n60\n60\n137\n135\n1561\n1528\n2088\n1665\n918\n717\nANEMONE\n62\n62\n139\n139\n1533\n1529\n1673\n1673\n729\n729\nGRADATE\n95\n90\n185\n182\n3022\n3015\n2713\n2713\n951\n949\nSL-GAD\n164\n129\n415\n330\n1600\n1591\n2691\n2588\n1784\n1541\nSub-CR\n128\n118\n276\n263\n3083\n3056\n3652\n3309\n1491\n1405\nPREM\n110\n54\n347\n161\n294\n146\n3711\n1629\n2495\n1103\nPREM-b\n16\n16\n16\n999\n44\n44\n44\n24\n24\n24\n666\n333\n97\n97\n97\n52\n52\n52\n138\n138\n138\n75\n75\n75\nTABLE VII: Results of ablation analysis.\nVariant\nCora\nCiteSeer\nPubMed\nACM\nFlickr\nAverage\nw/o nei. neg.\n0.7369\n0.7304\n0.8848\n0.6131\n0.7565\n0.7393\nw/o ego neg.\n0.9310\n0.9782\n0.9782\n0.9782\n0.9210\n0.8730\n0.7048\n0.8761\nw/o Anony.\n0.9541\n0.9541\n0.9541\n0.9592\n0.8811\n0.9109\n0.9109\n0.9109\n0.8536\n0.9109\nPREM\n0.9510\n0.9779\n0.9719\n0.9719\n0.9719\n0.9056\n0.8613\n0.8613\n0.8613\n0.9328\n0.9328\n0.9328\nefficiently but also manages to deliver superior results with\nan AUC of 0.9719 and 0.9056 respectively. We can also\nwitness that the mini-batch-based training would not lead to\nperformance degradation for PREM.\n2) Anomaly Detection Efficiency: Table V provides a de-\ntailed comparison of the training and testing times of PREM\nand various baseline methods across five datasets. Impres-\nsively, PREM far surpasses all other methods in terms of\nefficiency. On average, it often operates at a pace 10 times\nswifter than its closest competitor , DOMINANT in training,\nand 100 times faster than other contrastive learning based\nmethods. Significantly, in testing, this efficiency gap between\nour method and other contrastive learning-based methods\nwidens, e.g., PREM is 100,000 times faster than ANEMONE,\nas computing negative ego-neighbor similarity is much more\nefficient than subgraph sampling and multiple round evalua-\ntion, which are widely used in other contrastive learning based\nmethods [6], [12], [23]. This speed advantage is attained with-\nout compromising memory efficiency or performance. While\nthe fastest baseline DOMINANT demonstrates the ability to\nmanage larger datasets, it does so at the expense of substantial\nmemory consumption. For instance, with the PubMed dataset,\nDOMINANT consumes roughly 5943 MB of memory in\ntraining, which is about 20 times more than PREM\u2019s mere\n294. Moreover, the mini-batch version PREM-b has extremely\nsmall memory requirements across all datasets, enlightening\nthe potential of running PREM on edge devices with limited\nmemory. The efficiency and effectiveness of PREM stem from\nits simplicity as we only use a discriminator for anomaly\nscore calculation and most of the heavy computation can\nbe done and reused in the pre-processing module. It avoids\ncomplex objectives and modules that other baselines do,\nfocusing instead on efficiency and precision. This strategy\nminimizes computational costs and reduces processing time\nwhile achieving SOTA performance.\nC. Ablation Study\nIn this subsection, we study the effect of negative contrast\npairs and anonymization. The results are listed in Table VII.\nOverall, we can observe that PREM achieves the best results\nwhen all components are included in terms of the average\nresult. Disabling the neighbor negative term results in the\nAUC drop to the bottom, which illustrates that this term plays\na primary role in the model training. While removing the\nego negative term slightly increases the AUC on CiteSeer, it\nhas a significant impact on large datasets such as PubMed\nand Flickr. We can also find that using the two modules can\nimprove anomaly detection performance. When examining the\nimpact of anonymization, a comparison between Fig. 6 (g)\nand (h) reveals a significant reduction in the distance between\nego features and node features of anomalous nodes when\nanonymization is not applied. As a result, anomaly nodes\nbecome more difficult to be distinguished from their neighbor-\nhood and, thus lead to the degradation of model performance.\nD. Sensitivity Analysis\nTo investigate the effect of the trade-off parameters \u03b1 and \u03b3,\nwe perform a grid search with stride=0.1 for all parameters. As\nshown in Fig. 6 (a)-(e), a similar trend is observed among cita-\ntion networks. PREM achieves the highest AUC scores when\n\u03b1 and \u03b3 are in equilibrium, while its performance decreases\nwhen either \u03b1 or \u03b3 is approaches extreme values. When both\n\u03b1 and \u03b3 are 0.1, the AUC drops significantly, indicating the\ncollapse problem. The AUC on the social network Flickr is\nmore sensitive to changes in hyper-parameters, peaking at\n\u03b1 = 0.3 and \u03b3 = 0.4. To conclude, the trade-off between\nneighbor negative and ego negative depends on the properties\nof datasets, which is consistent with previous findings [6], [20],\n[55]. While hyper-parameter turning becomes expensive for\nlarge datasets, it is possible to quickly find the most appro-\npriate trade-off parameters of PREM for large datasets with\nlimited computational resources due to the highly efficient and\nscalable framework and training procedure.\nVI. CONCLUSION\nThis paper introduces PREM, a simple yet powerful\ngraph anomaly detection method. By leveraging a pre-\nprocessing module to pre-compute neighbor features, PREM\neliminates the need for train-time message passing, resulting\nin high training efficiency. The prediction of anomaly scores\nis\nachieved\nthrough\nan\nego-neighbor\nmatching\nmodule\nthat utilizes cosine similarity between ego and neighbor\nembeddings,\nleading\nto\nfast\nand\naccurate\nabnormality\nestimation. Extensive experiments demonstrate that PREM\nsignificantly outperforms state-of-the-art methods by a large\nmargin, while also exhibiting high running efficiency and low\nmemory consumption. In future work, we plan to explore\nthe extension of PREM to address the anomaly detection\nproblem in complex graph types, such as dynamic graphs,\nheterogeneous graphs, and heterophilic graphs.\nREFERENCES\n[1] Aleksandar Bojchevski, Johannes Klicpera, Bryan Perozzi, Amol\nKapoor, Martin Blais, Benedek R\u00b4ozemberczki, Michal Lukasik, and\nStephan G\u00a8unnemann. Scaling graph neural networks with approximate\npagerank. In SIGKDD, 2020.\n[2] Markus M Breunig, Hans-Peter Kriegel, Raymond T Ng, and J\u00a8org\nSander. Lof: identifying density-based local outliers. In SIGMOD, pages\n93\u2013104, 2000.\n[3] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spec-\ntral networks and locally connected networks on graphs. ICLR, 2014.\n[4] Micha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convo-\nlutional neural networks on graphs with fast localized spectral filtering.\nIn NeurIPS, 2016.\n[5] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu.\nDeep\nanomaly detection on attributed networks. In SDM, 2019.\n[6] Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin,\nYue Liu, and Zhibin Dong. Graph anomaly detection via multi-scale\ncontrastive learning networks with augmented view. In AAAI, 2023.\n[7] Haoyi Fan, Fengbin Zhang, and Zuoyong Li.\nAnomalydae: Dual\nautoencoder for anomaly detection on attributed networks. In ICASSP,\npages 5685\u20135689. IEEE, 2020.\n[8] Xiaotian Han, Tong Zhao, Yozen Liu, Xia Hu, and Neil Shah. Mlpinit:\nEmbarrassingly simple gnn training acceleration with mlp initialization.\nIn ICLR, 2023.\n[9] Kaveh Hassani and Amir Hosein Khasahmadi. Contrastive multi-view\nrepresentation learning on graphs. In ICML, pages 4116\u20134126, 2020.\n[10] Mikael Henaff, Joan Bruna, and Yann LeCun.\nDeep convolutional\nnetworks on graph-structured data. arXiv preprint arXiv:1506.05163,\n2015.\n[11] Tianjin Huang, Yulong Pei, Vlado Menkovski, and Mykola Pechenizkiy.\nHop-count based self-supervised anomaly detection on attributed net-\nworks. In ECML-PKDD, pages 225\u2013241. Springer, 2022.\n[12] Ming Jin, Yixin Liu, Yu Zheng, Lianhua Chi, Yuan-Fang Li, and Shirui\nPan. Anemone: Graph anomaly detection with multi-scale contrastive\nlearning. In CIKM, pages 3122\u20133126, 2021.\n[13] Thomas N Kipf and Max Welling. Semi-supervised classification with\ngraph convolutional networks. In ICLR, 2017.\n[14] Johannes Klicpera, Stefan Wei\u00dfenberger, and Stephan G\u00a8unnemann.\nDiffusion improves graph learning. In NeurIPS, 2019.\n[15] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton.\nDeep learning.\nnature, 2015.\n[16] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu.\nRadar: Residual\nanalysis for anomaly detection in attributed networks.\nIn IJCAI,\nvolume 17, pages 2152\u20132158, 2017.\n[17] Yangyang Li, Yipeng Ji, Shaoning Li, Shulong He, Yinhao Cao, Yifeng\nLiu, Hong Liu, Xiong Li, Jun Shi, and Yangchao Yang. Relevance-aware\nanomalous users detection in social network via graph neural network.\nIn IJCNN, pages 1\u20138. IEEE, 2021.\n[18] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou.\nSpecae: Spectral autoencoder for anomaly detection in attributed net-\nworks. In CIKM, pages 2233\u20132236, 2019.\n[19] Renjie Liao, Zhizhen Zhao, Raquel Urtasun, and Richard S Zemel.\nLanczosnet: Multi-scale deep graph convolutional networks. In ICLR,\n2019.\n[20] Fanzhen Liu, Xiaoxiao Ma, Jia Wu, Jian Yang, Shan Xue, Amin Be-\nheshti, Chuan Zhou, Hao Peng, Quan Z Sheng, and Charu C Aggarwal.\nDagad: Data augmentation for graph anomaly detection. In ICDM. IEEE,\n2022.\n[21] Yixin Liu, Kaize Ding, Huan Liu, and Shirui Pan.\nGood-d: On\nunsupervised graph out-of-distribution detection. In WSDM, pages 339\u2013\n347, 2023.\n[22] Yixin Liu, Kaize Ding, Jianling Wang, Vincent Lee, Huan Liu, and\nShirui Pan. Learning strong graph neural networks with weak informa-\ntion. In SIGKDD, pages 1559\u20131571, 2023.\n[23] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George\nKarypis. Anomaly detection on attributed networks via contrastive self-\nsupervised learning. IEEE TNNLS, 2021.\n[24] Yixin Liu, Yizhen Zheng, Daokun Zhang, Vincent CS Lee, and Shirui\nPan.\nBeyond smoothing: Unsupervised graph representation learning\nwith edge heterophily discriminating. In AAAI, volume 37, pages 4516\u2013\n4524, 2023.\n[25] Yixin Liu, Yu Zheng, Daokun Zhang, Hongxu Chen, Hao Peng, and\nShirui Pan.\nTowards unsupervised deep graph structure learning.\nIn\nWWW, pages 1392\u20131403, 2022.\n[26] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. Reason-\ning on graphs: Faithful and interpretable large language model reasoning.\narXiv preprint arxiv:2310.01061, 2023.\n[27] Xuexiong Luo, Jia Wu, Amin Beheshti, Jian Yang, Xiankun Zhang,\nYuan Wang, and Shan Xue. Comga: Community-aware attributed graph\nanomaly detection. In WSDM, pages 657\u2013665, 2022.\n[28] Chengsheng Mao, Liang Yao, and Yuan Luo.\nMedgcn: Graph\nconvolutional networks for multiple medical tasks.\narXiv preprint\narXiv:1904.00326, 2019.\n[29] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, Qinghua Zheng,\net al. Anomalous: A joint modeling approach for anomaly detection\non attributed networks. In IJCAI, pages 3513\u20133519, 2018.\n[30] Bryan Perozzi and Leman Akoglu.\nScalable anomaly ranking of\nattributed neighborhoods. In SDM, pages 207\u2013215. SIAM, 2016.\n[31] Jiezhong Qiu, Qibin Chen, Yuxiao Dong, Jing Zhang, Hongxia Yang,\nMing Ding, Kuansan Wang, and Jie Tang. Gcc: Graph contrastive coding\nfor graph neural network pre-training. In SIGKDD, 2020.\n[32] Joshua Robinson, Ching-Yao Chuang, Suvrit Sra, and Stefanie Jegelka.\nContrastive learning with hard negative samples. In ICLR, 2021.\n[33] David I Shuman, Sunil K Narang, Pascal Frossard, Antonio Ortega,\nand Pierre Vandergheynst. The emerging field of signal processing on\ngraphs: Extending high-dimensional data analysis to networks and other\nirregular domains. IEEE signal processing magazine, 30(3):83\u201398, 2013.\n[34] Yue Tan, Yixin Liu, Guodong Long, Jing Jiang, Qinghua Lu, and\nChengqi Zhang.\nFederated learning on non-iid graphs via structural\nknowledge sharing. In AAAI, volume 37, pages 9953\u20139961, 2023.\n[35] Chang Tang, Xinwang Liu, Xinzhong Zhu, En Zhu, Zhigang Luo, Lizhe\nWang, and Wen Gao. Cgd: Multi-view clustering via cross-view graph\ndiffusion. In AAAI, 2020.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. In NeurIPS, 2017.\n[37] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero,\nPietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR,\n2018.\n[38] Petar Velickovic, William Fedus, William L Hamilton, Pietro Li`o,\nYoshua Bengio, and R Devon Hjelm. Deep graph infomax. In ICLR,\n2019.\n[39] Haibo Wang, Chuan Zhou, Jia Wu, Weizhen Dang, Xingquan Zhu, and\nJilong Wang. Deep structure learning for fraud detection. In ICDM,\npages 567\u2013576. IEEE, 2018.\n[40] Xuhong Wang, Baihong Jin, Ying Du, Ping Cui, Yingshui Tan, and\nYupu Yang.\nOne-class graph neural networks for anomaly detection\nin attributed networks. Neural computing and applications, 33:12073\u2013\n12085, 2021.\n[41] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and\nKilian Weinberger. Simplifying graph convolutional networks. In ICML.\nPMLR, 2019.\n[42] Man Wu, Shirui Pan, Lan Du, and Xingquan Zhu. Learning graph neural\nnetworks with positive and unlabeled nodes. ACM TKDD, 2021.\n[43] Fengli Xu, Jianxun Lian, Zhenyu Han, Yong Li, Yujian Xu, and Xing\nXie.\nRelation-aware graph convolutional networks for agent-initiated\nsocial e-commerce recommendation. In CIKM, pages 529\u2013538, 2019.\n[44] Yuning You, Tianlong Chen, Yongduo Sui, Ting Chen, Zhangyang\nWang, and Yang Shen. Graph contrastive learning with augmentations.\nIn NeurIPS, volume 33, pages 5812\u20135823, 2020.\n[45] Hanqing Zeng, Hongkuan Zhou, Ajitesh Srivastava, Rajgopal Kannan,\nand Viktor Prasanna.\nGraphsaint: Graph sampling based inductive\nlearning method. In ICLR, 2020.\n[46] Jiaqiang Zhang, Senzhang Wang, and Songcan Chen. Reconstruction\nenhanced multi-view contrastive learning for anomaly detection on\nattributed networks. In IJCAI, 2022.\n[47] Zheng Zhang and Liang Zhao. Unsupervised deep subgraph anomaly\ndetection. In ICDM, pages 753\u2013762. IEEE, 2022.\n[48] Xin Zheng, Yixin Liu, Zhifeng Bao, Meng Fang, Xia Hu, Alan Wee-\nChung Liew, and Shirui Pan.\nTowards data-centric graph machine\nlearning: Review and outlook. arXiv preprint arXiv:2309.10979, 2023.\n[49] Xin Zheng, Miao Zhang, Chunyang Chen, Quoc Viet Hung Nguyen,\nXingquan Zhu, and Shirui Pan. Structure-free graph condensation: From\nlarge-scale graphs to condensed graph-free data. In NeurIPS, 2023.\n[50] Yizhen Zheng, Ming Jin, Shirui Pan, Yuan-Fang Li, Hao Peng, Ming\nLi, and Zhao Li. Toward graph self-supervised learning with contrastive\nadjusted zooming. IEEE TNNLS, 2022.\n[51] Yizhen Zheng, Shirui Pan, Vincent Lee, Yu Zheng, and Philip S Yu.\nRethinking and scaling up graph contrastive learning: An extremely\nefficient approach with group discrimination. In NeurIPS, volume 35,\npages 10809\u201310820, 2022.\n[52] Yizhen Zheng, CS Lee Vincent, Zonghan Wu, and Shirui Pan. Hetero-\ngeneous graph attention network for small and medium-sized enterprises\nbankruptcy prediction. In PAKDD, 2021.\n[53] Yizhen Zheng, He Zhang, Vincent Lee, Yu Zheng, Xiao Wang, and\nShirui Pan. Finding the missing-half: Graph complementary learning\nfor homophily-prone and heterophily-prone graphs.\narXiv preprint\narXiv:2306.07608, 2023.\n[54] Yizhen Zheng, Yu Zheng, Xiaofei Zhou, Chen Gong, Vincent CS Lee,\nand Shirui Pan.\nUnifying graph contrastive learning with flexible\ncontextual scopes. In ICDM, pages 793\u2013802. IEEE, 2022.\n[55] Yu Zheng, Ming Jin, Yixin Liu, Lianhua Chi, Khoa T Phan, and Yi-\nPing Phoebe Chen. Generative and contrastive self-supervised learning\nfor graph anomaly detection. IEEE TKDE, 2021.\n[56] Shuang Zhou, Qiaoyu Tan, Zhiming Xu, Xiao Huang, and Fu-lai Chung.\nSubtractive aggregation for attributed network anomaly detection.\nIn\nCIKM, pages 3672\u20133676, 2021.\n[57] Mengxiao Zhu and Haogang Zhu. Mixedad: A scalable algorithm for\ndetecting mixed anomalies in attributed graphs. In AAAI, volume 34,\npages 1274\u20131281, 2020.\n[58] Xiaoke Zhu, Xiao-Yuan Jing, Fan Zhang, Xinyu Zhang, Xinge You, and\nXiang Cui. Distance learning by mining hard and easy negative samples\nfor person re-identification. Pattern Recognition, 95:211\u2013222, 2019.\n[59] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang.\nGraph contrastive learning with adaptive augmentation. In WWW, pages\n2069\u20132080, 2021.\n",
    "2402.11887": "Generative Semi-supervised Graph Anomaly Detection\nHezhe Qiao1, Qingsong Wen2, Xiaoli Li3, Ee-Peng Lim1, Guansong Pang1\u2217\n1School of Computing and Information Systems, Singapore Management University\n2Squirrel AI\n3Institute for Infocomm Research, A*STAR\nhezheqiao.2022@phdcs.smu.edu.sg, qingsongedu@gmail.com\nxlli@i2r.a-star.edu.sg, eplim@smu.edu.sg, pangguansong@gmail.com\nAbstract\nThis work considers a practical semi-supervised graph anomaly detection (GAD)\nscenario, where part of the nodes in a graph are known to be normal, contrasting\nto the extensively explored unsupervised setting with a fully unlabeled graph. We\nreveal that having access to the normal nodes, even just a small percentage of\nnormal nodes, helps enhance the detection performance of existing unsupervised\nGAD methods when they are adapted to the semi-supervised setting. However,\ntheir utilization of these normal nodes is limited. In this paper we propose a novel\nGenerative GAD approach (namely GGAD) for the semi-supervised scenario to\nbetter exploit the normal nodes. The key idea is to generate pseudo anomaly\nnodes, referred to as outlier nodes, for providing effective negative node samples\nin training a discriminative one-class classifier. The main challenge here lies in\nthe lack of ground truth information about real anomaly nodes. To address this\nchallenge, GGAD is designed to leverage two important priors about the anomaly\nnodes \u2013 asymmetric local affinity and egocentric closeness \u2013 to generate reliable\noutlier nodes that assimilate anomaly nodes in both graph structure and feature\nrepresentations. Comprehensive experiments on six real-world GAD datasets\nare performed to establish a benchmark for semi-supervised GAD and show that\nGGAD substantially outperforms state-of-the-art unsupervised and semi-supervised\nGAD methods with varying numbers of training normal nodes. Code will be made\navailable at https://github.com/mala-lab/GGAD.\n1\nIntroduction\nGraph anomaly detection (GAD) has received significant attention due to its broad application\ndomains, e.g., cyber security and fraud detection [9,17,34]. However, it is challenging to recognize\nanomaly nodes in a graph due to its complex graph structure and attributes [28,35,45,46,62,65,66].\nMoreover, most traditional anomaly detection methods [4,40,60] are designed for Euclidean data,\nwhich are shown to be ineffective on non-Euclidean data like graph data [8,18,29,45,51,66]. To\naddress this challenge, as an effective way of modeling graph data, graph neural networks (GNN)\nhave been widely used for deep GAD [35]. These GNN methods typically assume that the labels\nof all nodes are unknown and perform anomaly detection in a fully unsupervised way by, e.g., data\nreconstruction [8,11], self-supervised learning [6,29,37,61], or one-class homophily modeling [45].\nAlthough these methods achieve remarkable advances, they are not favored in real-world applications\nwhere the labels for normal nodes are easy to obtain due to their overwhelming presence in a graph.\nThis is because their capability to utilize those labeled normal nodes is very limited due to their\ninherent unsupervised nature. There have been some GAD methods [13,17,27,35,49,50,52,54]\ndesigned in a semi-supervised setting, but their training relies on the availability of both labeled\n\u2217Corresponding author: G. Pang\nPreprint. Under review.\narXiv:2402.11887v4  [cs.LG]  28 May 2024\nnormal and anomaly nodes, which requires a costly annotation of a large set of anomaly nodes. This\nlargely restricts the practical application of these methods.\nDifferent from the aforementioned two GAD settings, this paper instead considers a practical yet\nunder-explored semi-supervised GAD scenario, where part of the nodes in the graph are known to be\nnormal. Such a one-class classification setting has been widely explored in anomaly detection on\nother data types, such as visual data [4,40], time series [57], and tabular data [20], but it is rarely\nexplored in anomaly detection on graph data. Recently there have been a few relevant studies in this\nline [3,28,32,39,56], but they are on graph-level anomaly detection, i.e., detecting abnormal graphs\nfrom a set of graphs, while we explore the semi-supervised setting for abnormal node detection. We\nestablish an evaluation benchmark for this problem and show that having access to these normal\nnodes helps enhance the detection performance of existing unsupervised GAD methods when they\nare properly adapted to the semi-supervised setting (see Table 1). However, due to their original\nunsupervised designs, they cannot make full use of these labeled normal nodes.\nNormal\nOutlier\nAbnormal\n(b)  \n(c) \nProportion\nGraph\nNormal\nOutlier\nAbnormal\n(a)  \nAEGIS\nGGAD\n(d) \nLocal Node Affinity\nLocal Node Affinity\ninv\njnv\niov\njov\niav\njav\njav\niav\niov\njov\njav\niav\niov\njov\nFigure 1: Left: An exemplar graph with the\nedge width indicates the level of affinity con-\nnecting two nodes, in which normal nodes\n(e.g., vni and vnj) have stronger affinity to\nits neighboring normal nodes than anomaly\nnodes (e.g., vai and vaj) due to homophily re-\nlation within the normal class. Our approach\nGGAD aims to generate outliers (e.g., voi\nand voj) that can well assimilate the anomaly\nnodes.\nRight: The outliers generated by\nmethods like AEGIS [7] that ignore their\nstructural relation often mismatch the distribu-\ntion of abnormal nodes (a), due to their false\nlocal affinity (c). By contrast, GGAD incor-\nporates two important priors about anomaly\nnodes to generate outliers so that they well\nassimilate the (b) feature representation and\n(d) local structure of abnormal nodes.\nTo better exploit those normal nodes, we propose\na novel generative GAD approach, namely GGAD,\naiming at generating pseudo anomaly nodes, referred\nto as outlier nodes, for providing effective negative\nnode samples in training a discriminative one-class\nclassifier on the given normal nodes. The key chal-\nlenge in this type of generative approach is the ab-\nsence of ground-truth information about real anomaly\nnodes. There have been many generative anomaly\ndetection approaches that learn adversarial outliers\nto provide some weak supervision of abnormality\n[38,47,59,64], but they are designed for non-graph\ndata and fail to take account of the graph structure\ninformation in the outlier generation. Some recent\nmethods, such as AEGIS [7], attempt to adapt this\napproach for GAD, but the outliers are generated\nbased on simply adding Gaussian noises to the GNN-\nbased node representations, ignoring the structural\nrelations between the outliers and the graph nodes.\nConsequently, the distribution of the generated out-\nliers is often mismatched to that of the real anomaly\nnodes, as illustrated in Fig. 1a, and demonstrates very\ndifferent local structure (Fig. 1c).\nOur approach GGAD tackles this issue with a method\nto generate outlier nodes that assimilate the anomaly\nnodes in both local structure and feature representa-\ntion. It is motivated by two important priors about\nthe anomaly nodes. The first one is an asymmetric\nlocal affinity phenomenon revealed in recent stud-\nies [12\u201314,45], i.e., the affinity between normal nodes\nis typically significantly stronger than that between normal and abnormal nodes. Inspired by this,\nGGAD generates outlier nodes in a way to enforce that they have a smaller local affinity to their\nlocal neighbors than the normal nodes. This objective aligns the distribution of the outlier nodes\nto that of the anomaly nodes in terms of graph structure. The second prior knowledge is that many\nanomaly nodes exhibit high similarity to the normal nodes in the feature space due to its subtle\nabnormality [25,45] or adversarial camouflage [10,15,30,49]. We encapsulate this prior knowledge as\negocentric closeness, mandating that the feature representation of the outlier nodes should be closed\nto the normal nodes that share similar local structure as the outlier nodes. GGAD incorporates these\ntwo priors through two loss functions to generate outlier nodes that are well aligned to the distribution\nof the anomaly nodes in both local structure affinity (see Fig. 1d) and feature representation (see Fig.\n1b). We can then train a discriminative one-class classifier on the labeled normal nodes, with these\ngenerated outlier nodes treated as the negative samples.\nAccordingly, our main contributions can be summarized as follows:\n2\n\u2022 We explore a practical yet under-explored semi-supervised GAD problem where part of the\nnormal nodes are known, and establish an evaluation benchmark for the problem.\n\u2022 We propose a novel generative GAD approach, GGAD, for the studied setting. To the best\nof our knowledge, it is the first work aiming for generating outlier nodes that are of similar\nlocal structure and node representations to the real anomaly nodes. The outlier nodes serve\nas effective negative samples for training a discriminative one-class classifier.\n\u2022 We encapsulate two important priors about anomaly nodes \u2013 asymmetric local affinity and\negocentric closeness \u2013 and leverage them to introduce an innovative outlier node generation\nmethod. Although these priors may not be exhaustive, they provide principled guidelines for\ngenerating learnable outlier nodes that can well assimilate the real anomaly nodes in both\ngraph structure and feature representation across diverse real-world GAD datasets.\n\u2022 Extensive experiments on six large GAD datasets demonstrate that our approach GGAD\nsubstantially outperforms 12 state-of-the-art unsupervised and semi-supervised GAD meth-\nods with varying numbers of training normal nodes, achieving over 15% increase in AU-\nROC/AUPRC compared to the best contenders on the challenging datasets.\n2\nRelated Work\n2.1\nGraph Anomaly Detection\nNumerous graph anomaly detection methods, including shallow and deep approaches, have been\nproposed. Shallow methods like Radar [23], AMEN [44], and ANOMALOUS [43] are often\nbottlenecked due to the lack of representation power to capture the complex semantics of graphs.\nWith the development of GNN in node representation learning, many deep GAD methods show better\nperformance than shallow approaches. Here we focus on the discussion of the deep GAD methods in\ntwo relevant settings: unsupervised and semi-supervised GAD.\nUnsupervised Approach. Existing unsupervised GAD methods are typically built using a conven-\ntional anomaly detection objective, such as data reconstruction. The basic idea is to capture the\nnormal activity patterns and detect anomalies that behave significantly differently. As one of the most\npopular methods, reconstruction-based methods using graph auto-encoder (GAE) have been widely\napplied for GAD [1]. DOMINANT is the first work that applies GAE on the graph to reconstruct the\nattribute and structure leveraging GNNs [8]. Fan et al. propose AnomalyDAE to further improve the\nperformance by enhancing the importance of the reconstruction on the graph structure. In addition\nto reconstruction, some methods focus on exploring the relationship in the graph, e.g., the relation\nbetween nodes and subgraphs, to train GAD models. Among these methods, Qiao et al. propose\nTAM [45], which maximizes the local node affinity on truncated graphs, achieving good performance\non the synthetic dataset and datasets with real anomalies. Although the aforementioned unsupervised\nmethods achieve good performance and help us identify anomalies without any access to class labels,\nthey cannot effectively leverage the labeled nodes when such information is available.\nSemi-Supervised Approach. The one-class classification under semi-supervised setting has been\nwidely explored in anomaly detection on visual data, but rarely done on the graph data, except\n[3, 28, 32, 39, 56] that recently explored this setting for graph-level anomaly detection. To detect\nabnormal graphs, these methods address a very different problem from ours, which is focused on\ncapturing the normality of a set of given normal graphs at the graph level. By contrast, we focus\non modeling the normality at the node level. Some semi-supervised methods have been recently\nproposed for node-level anomaly detection, but they assume the availability of the labels of both\nnormal and anomaly nodes [12,13,17,27,42,48,52]. By contrast, our setting eases this requirement\nand requires the labeled normal nodes only.\n2.2\nGenerative Anomaly Detection\nGenerative adversarial networks (GANs) provide an effective solution to generate synthetic samples\nthat capture the normal/abnormal patterns [2,16,63]. One type of these methods aims to learn latent\nfeatures that can capture the normality of a generative network [31,58]. Methods like ALAD [59],\nFence GAN [38] and OCAN [64] are early methods in this line, aiming at making the generated\nsamples lie at the boundary of normal data for more accurate anomaly detection. Motivated by these\nmethods, a similar approach has also been explored in graph data, like AEGIS [7] and GAAN [6]\n3\nwhich aim to simulate some abnormal features in the representation space using GNN, but they are\nfocused on adding Gaussian noise to the representations of normal nodes without considering graph\nstructure information. They are often able to generate pseudo anomaly node representations that are\nseparable from the normal nodes for training their detection model, but the pseudo anomaly nodes\nare mismatched with the distribution of the real anomaly nodes.\n3\nMethodology\n3.1\nProblem Statement\nSemi-supervised GAD. We focus on the semi-supervised anomaly detection on the attributed graph\ngiven some labeled normal nodes. An attributed graph can be denoted by G = (V, E, X), where\nV = {v1, \u00b7 \u00b7 \u00b7 , vN} denotes the node set, E \u2286V \u00d7 V with e \u2208E is the edge set in the graph. eij = 1\nrepresents there is a connection between node vi and vj, and eij = 0 otherwise. The node attributes\nare denoted as X \u2208RN\u00d7F and A \u2208{0, 1}N\u00d7N is the adjacency matrix of G. xi \u2208RF is the\nattribute vector of vi and Aij = 1 if and only if (vi, vj) \u2208E. Vn and Va represent the normal node\nset and abnormal node set, respectively. Typically the number of normal nodes is significantly greater\nthan the abnormal nodes, i.e., |Vn| \u226b|Va|. The goal of semi-supervised GAD is to learn an anomaly\nscoring function f : G \u2192R, such that f(v) < f(v\u2032), \u2200v \u2208Vn, v\u2032 \u2208Va given a set of labeled normal\nnodes Vl \u2282Vn and no access to labels of anomaly nodes. All other unlabeled nodes, denoted by\nVu = V \\ Vl, comprise the test data set.\nOutlier Node Generation. Outlier generation aims to generate outlier nodes that deviate from the\nnormal nodes and/or assimilate the anomaly nodes. Such nodes can be generated in either the raw\nfeature space or the embedding feature space. This work is focused on the latter case, as it offers a\nmore flexible way to represent relations between nodes. Our goal is to generate a set of outlier nodes\nfrom G, denoted by Vo, in the feature representation space, so that the outlier nodes are well aligned\nto the anomaly nodes, given no access to the ground-truth anomaly nodes.\nGraph Neural Network for Node Representation Learning. GNN has been widely used to generate\nthe node representations due to its powerful representation ability in capturing the rich graph attribute\nand structure information. The projection of node representation using a GNN layer can be generally\nformalized as\nH(\u2113) = GNN\n\u0010\nA, H(\u2113\u22121); W(\u2113)\u0011\n,\n(1)\nwhere H(\u2113) \u2208RN\u00d7h(l), H(\u2113\u22121) \u2208RN\u00d7h(l\u22121) are the representations of all N nodes in the (\u2113)-th layer\nand (\u2113\u22121)-th layer, respectively, h(l) is the dimensionality size, W(\u2113) are the learnable parameters,\nand H(0) is set to X. H(\u2113) = {h1, h2, . . . , hN} is a set of representations of N nodes in the last\nGNN layer, with h \u2208Rd. In this paper, we adopt a 2-layer GCN to model the graph.\n3.2\nOverview of the Proposed GGAD Approach\nThe key insight of GGAD is to generate learnable outlier nodes in the feature representation space that\nassimilate anomaly nodes in terms of both local structure affinity and feature representation. To this\nend, we introduce two new loss functions that incorporate two important priors about anomaly nodes\n\u2013 asymmetric local affinity and egocentric closeness \u2013 to optimize the outlier nodes. As shown in Fig.\n2a, the outlier nodes are first initialized based on the representations of the neighbors of the labeled\nnormal nodes, followed by the use of the two priors on the anomaly nodes. GGAD implements the\nasymmetric local affinity prior in Fig. 2b that enforces a larger local affinity of the normal nodes\nthan that of the anomaly nodes. GGAD then models the egocentric closeness in Fig. 2c that pulls\nthe feature representations of the outlier nodes to the normal nodes that share the same ego network.\nThese two priors are implemented through two complementary loss functions in GGAD. Minimizing\nthese loss functions optimizes the outlier nodes to meet both anomaly priors. The resulting outlier\nnodes are lastly treated as negative samples to train a discriminative one-class classifier on the labeled\nnormal nodes, as shown in Fig. 2d. Below we introduce GGAD in detail.\n4\nNormal Nodes\nUnlabeled  Nodes\nGNN\nEmbedding \nNormal \nOutlier\nOutlier\nNormal  \nNumber of Samples\nPriors\n(a)  Outlier Node Initialization\n(b) Asymmetric Local Affinity\n(c) Egocentric Closeness\nPeturbation\nPull\niv\njv\ni\n\uf065\n\uf02b\nh\nj\n\uf065\n\uf02b\nh\n\u02c6\nih\n\u02c6\nj\nh\niv\njv\niv\njv\n1\nLocal Affinity\nEgo Network of \n Local Affinity Prior\nEgocentric Closeness Prior\nAnomaly Score\nGraph\n/\ni\nj\nv\nv\n0.9\n0.2\n0.1\n0.8\n(d) One-class Classifier\n\uf028\uf029\n\uf028\n\uf029\n\uf028\n\uf029\n\uf07b\n\uf07d\nmax 0,\nl\nala\no\n\uf061\n\uf074\n\uf074\n\uf03d\n\uf02d\n\uf02d\n\uf028\n\uf029\n2\n2\n1\n\u02c6\ni\no\nec\ni\ni\nv\no\n\uf065\n\uf0ce\n\uf03d\n\uf02d\n\uf02b\n\uf0e5h\nh\nFigure 2: Overview of GGAD. (a) It first initializes the outlier nodes based on the feature representa-\ntions of the ego network of a labeled normal node. We then incorporate the two anomaly node priors\n(b-c) to optimize the outlier nodes so that they are well aligned to the anomalies. (d) The resulting\ngenerated outlier nodes are treated as negative samples to train a discriminative one-class classifier.\n3.3\nIncorporating the Asymmetric Local Affinity Prior\nOutlier Node Initialization. Recall that GGAD is focused on generating learnable outlier nodes in\nthe representation space. To enable the subsequent learning of the outlier nodes, we need to produce\ngood representation initialization of the outlier nodes. To this end, we use a neighborhood-aware\noutlier initialization module that generates the initial outlier nodes\u2019 representation based on the\nrepresentations of the local neighbors of normal nodes. The representations from these neighbor\nnodes provide an important reference for being normal in a local graph structure. This helps ground\nthe generation of outlier nodes to a real graph structure. More specifically, as shown in Fig. 2a, given\na labeled normal node vi \u2208Vl and its ego network N(vi) that contains all nodes directly connected\nwith vi, we initialize an outlier node in the representation space by:\n\u02c6hi = \u03a8 (vi, N(vi); \u0398g) =\n1\n|N(vi)|\nX\nvj\u2208N(vi)\n\u03c3( \u02dc\nWhj),\n(2)\nwhere \u03a8 is a mapping function determined by parameters \u0398g that contain the learnable parameters\n\u02dc\nW \u2208R\nd\u00d7d in this module in addition to the parameters W(\u2113) in Eq. (1), and \u03c3(\u00b7) is an activation\nfunction. It is not required to perform Eq. (2) for all training normal nodes. We sample a set of\nS normal nodes from Vl and respectively generate an outlier node for each of them based on its\nego network. \u02c6hi in Eq. (2) serves as an initial representation of the outlier node, upon which two\noptimization constraints based on our anomaly node priors are devised to optimize the representations\nof the outlier nodes, as elaborated in the following.\nEnforcing the Structural Affinity Prior. To incorporate the graph structure prior of anomaly nodes\ninto our outlier node generation, GGAD introduces a local affinity-based loss to enforce the fact that\nthe affinity of the outlier nodes to their local neighbors should be smaller than that of the normal\nnodes. More specifically, the local node affinity of vi, denoted as \u03c4(vi), is defined as the similarity to\nits neighboring nodes:\n\u03c4 (vi) =\n1\n|N (vi)|\nX\nvj\u2208N(vi)\nsim (hi, hj),\n(3)\nThe asymmetric local affinity loss is then defined by a margin loss function based on the affinity of\nthe normal nodes and the generated outlier nodes as follows:\n\u2113ala = max {0, \u03b1 \u2212(\u03c4 (Vl) \u2212\u03c4 (Vo))} ,\n(4)\nwhere \u03c4 (Vo) =\n1\n|Vo|\nP\nvi\u2208Vo\n\u03c4 (vi) and \u03c4 (Vl) =\n1\n|Vl|\nP\nvi\u2208Vl\n\u03c4 (vi) represent the average local affinity\nof the normal and outlier nodes respectively, and \u03b1 > 0 is a hyperparameter controlling the margin\nbetween the affinities of these two types of nodes. Eq. (4) enforces this prior at the node set level\nrather than at each individual outlier node, as the latter case would be highly computationally costly\nwhen Vl or Vo is large.\n5\n3.4\nIncorporating the Egocentric Closeness Prior\nThe outliers generated by solely using this local affinity prior may distribute far away from the\nnormal nodes in the representation space, as shown in Fig. 3a. For those trivial outliers, although\nthey achieve similar local affinity to the abnormal nodes, as shown in Fig. 3d, they are still not\naligned well with the distribution of the anomaly nodes, and thus, they cannot serve as effective\nnegative samples for learning the one-class classifier on the normal nodes. Thus, we further in-\ntroduce an egocentric closeness prior-based loss function to tackle this issue, which models subtle\nabnormality on anomaly nodes, i.e., the anomaly nodes that exhibit high similarity to the normal nodes.\n(a) Using \u2113ala Only (b) Using \u2113ec only\n(c) Using GGAD\n0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n0\n5\n10\n15\n20\n25\nNormal\nOutlier\nAbnormal\n(d) Using \u2113ala Only\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n2\n4\n6\n8\n10\n12\n14\n(e) Using \u2113ec Only\n0.2 0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n0\n1\n2\n3\n4\n5\n(f) Using GGAD\nFigure 3: (a-c) t-SNE visualization of the\nnode representations and (d-f) histograms of\nlocal affinity yielded by GGAD and its two\nvariants on a GAD dataset T-Finance [50].\nMore specifically, let hi and \u02c6hi be the representations\nof the normal node vi and its corresponding generated\noutlier node that shares the same ego network as vi\n(as discussed in Sec. 3.2), the egocentric closeness\nprior-based loss \u2113ec is defined as follows:\n\u2113ec =\n1\n|Vo|\nX\nvi\u2208Vo\n\r\r\r\u02c6hi \u2212(hi + \u03b5)\n\r\r\r\n2\n2,\n(5)\nwhere |Vo| is the number of the generated outliers and\n\u03f5 is a noise perturbation generated from a Gaussian\ndistribution. The perturbation is added to guarantee\na separability between hi and \u02c6hi, while enforcing its\negocentric closeness.\nAs shown in Fig. 3c, using this egocentric closeness\nprior-based loss together with the local affinity prior-\nbased loss learns outlier nodes that are well aligned\nto the real anomaly nodes in both the representation\nspace and the local structure, as illustrated in Figs. 3c and 3f, respectively. Using the egocentric\ncloseness alone also results in mismatches between the generated outlier nodes and the abnormal\nnodes (see Fig. 3e) since it ignores the local structure relation of the generated outlier nodes.\n3.5\nGraph Anomaly Detection using GGAD\nTraining. Since the generated outlier nodes are to assimilate the abnormal nodes, they can be\nused as important negative samples to train a one-class classifier on the labeled normal nodes. We\nimplement this classifier using a fully connected layer on top of the GCN layers that maps the node\nrepresentations to a prediction probability-based anomaly score, denoted by \u03b7 : H \u2192R, followed by\na binary cross-entropy (BCE) loss function \u2113bce:\n\u2113bce =\n|Vo|+|Vl|\nX\ni\nyi log(pi) + (1 \u2212yi) log(1 \u2212pi),\n(6)\nwhere pi = \u03b7(hi; \u0398s) is the output of the one-class classifier indicating the probability that a node is a\nnormal node, and y is the label of node. We set y = 1 if the node is a labeled normal node, and y = 0\nif the node is a generated outlier node. The one-class classifier is jointly optimized with the local\naffinity prior-based loss \u2113ala and egocentric closeness prior-based loss \u2113ec in an end-to-end manner.\nIt is worth mentioning that \u2113ala and \u2113ec are two collaborative losses, enforcing the approximation of\nthe outlier nodes to the real anomaly nodes from two complementary perspectives, the local structure\nperspective and the feature representation perspective. Thus, the overall loss \u2113total can be formulated\nas:\n\u2113total = \u2113bce + \u03b2\u2113ala + \u03bb\u2113ec,\n(7)\nwhere \u03b2 and \u03bb are the hyperparameters to control the importance of the two constraints respectively.\nThe learnable parameters are \u0398 = {\u0398g, \u0398s}.\nInference. During inference, we can directly use the inverse of the prediction of the one-class\nclassifier as the anomaly score:\nscore (vj) = 1 \u2212\u03b7 (hj; \u0398\u2217) ,\n(8)\nwhere \u0398\u2217is the learned parameters of GGAD. Since our outlier nodes well assimilate the real\nabnormal nodes, they are expected to receive high anomaly scores from the one-class classifier.\n6\n4\nExperiments\nDatasets. We conduct experiments on six large real-world graph datasets with genuine anomalies\nfrom diverse domains, including the co-review network in Amazon [10], transaction record network\nin T-Finance [50], social networks in Reddit [22], bitcoin transaction in Elliptic [55], co-purchase\nnetwork in Photo [36] and financial network in DGraph [19]. See App. A for more details about\nthe datasets. Although it is easy to obtain normal nodes, the human checking and annotation of\nlarge-scale nodes are still costly. To simulate practical scenarios where we need to annotate only a\nrelatively small number of normal nodes, we randomly sample R% of the normal nodes as labeled\nnormal data for training, in which R is chosen in {10, 15, 20, 25}, with the rest of nodes is treated\nas the testing set. Due to the massive set of nodes, the same R applied to DGraph would lead to\na significantly larger set of normal nodes than the other three data sets, leading to very different\nannotation costs in practice. Thus, on DGraph, R is chosen in {0.05, 0.2, 0.35, 0.5} to compose the\ntraining data.\nCompeting Methods. To our best knowledge, there exist no GAD methods specifically designed for\nsemi-supervised node-level GAD. To validate the effectiveness of GGAD, we compare it with six\nstate-of-the-art (SOTA) unsupervised methods and their advanced versions in which we effectively\nadapt them to our semi-supervised setting. These methods include two reconstruction-based models:\nDOMINANT [8] and AnomalyDAE [11], two one-class classification models: TAM [45] and\nOCGNN [53], and two generative models: AEGIS [7] and GAAN [6]. To effectively incorporate the\nnormal information into these unsupervised methods, for the reconstruction models, DOMINANT and\nAnomalyDAE, the data reconstruction is performed on the labeled normal nodes only during training.\nIn OCGNN, the one-class center is optimized based on the labeled normal nodes exclusively. In TAM,\nwe train the model by maximizing the affinity on the normal nodes only. As for AEGIS and GAAN,\nthe normal nodes combined with their generated outliers are used to train an adversarial classifier.\nSelf-supervised-based methods like CoLA [29], SL-GAD [6], and HCM-A [18] and semi-supervised\nmethods that require both labeled normal and abnormal nodes like GODM [26]and DiffAD [33] are\nomitted because training these methods on the data with exclusively normal nodes does not work.\nEvaluation Metric. Following prior studies [5,41,51], two popular and complementary evaluation\nmetrics for anomaly detection, the area under ROC curve (AUROC) and Area Under the precision-\nrecall curve (AUPRC), are used to evaluate the performance. Higher AUROC/AUPRC indicates better\nperformance. AUROC reflects the ability to recognize anomalies while at the same time considering\nthe false positive rate. AUPRC focuses solely on the precision and recall rates of anomalies detected.\nThe AUROC and AUPRC results are averaged over 5 runs with different random seeds.\nImplementation Details. GGAD is implemented in Pytorch 1.6.0 with Python 3.7. and all the\nexperiments are run on a 24-core CPU. In GGAD, its weight parameters are optimized using\nAdam [21] optimizer with a learning rate of 1e \u22123 by default. For each dataset, the hyperparameters\n\u03b2 and \u03bb for two constraints are uniformly set to 1, though GGAD can perform stably with a range\nof \u03b2 and \u03bb (see App. C.1). The size of the generated outlier nodes S is set to 5% of |Vl| by default\nand stated otherwise. The affinity margin \u03b1 is set to 0.7 across all datasets. The perturbation in\nEq. (5) is drawn from a Gaussian distribution, with mean and standard variance set to 0.01 and\n0.005 respectively, and it is stated otherwise. All the competing methods are implemented by using\ntheir publicly available official source code or the library, and they are trained using their suggested\nhyperparameters. To apply GGAD and the competing models to very large graph datasets, i.e.,\nDGraph, a min-batch training strategy is applied (see Algorithm 2 for detail).\n4.1\nMain Comparison Results\nTable 1 shows the comparison of GGAD to 12 models, in which semi-supervised models use 15%\nnormal nodes during training while unsupervised methods are trained on the full graph in a fully\nunsupervised way. We will discuss results using more/less training normal nodes in Sec. 4.2.\nComparison to Unsupervised GAD Methods. As shown in Table 1, GGAD significantly outper-\nforms all unsupervised methods on six datasets, having maximally 21% AUROC and 39% AUPRC\nimprovement over the best-competing unsupervised methods on individual datasets. The results also\nshow that the semi-supervised versions of the unsupervised methods largely improve the performance\nof their unsupervised counterparts, justifying that i) incorporating the normal information into the\nunsupervised approaches is beneficial for enhancing the detection performance and ii) our approach\n7\nTable 1: AUROC and AUPRC on six GAD datasets. The best performance per dataset is boldfaced,\nwith the second-best underlined. \u2018/\u2019 indicates that the model cannot handle the DGraph dataset.\nSetting\nMethod\nDataset\nAUROC\nAUPRC\nAmazon T-Finance Reddit Elliptic Photo DGraph Amazon T-Finance Reddit Elliptic\nPhoto\nDGraph\nUnsupervised\nDOMINANT\n0.7025\n0.6087\n0.5105 0.2960 0.5136\n0.5738\n0.1315\n0.0536\n0.0380 0.0454 0.1039\n0.0075\nAnomalyDAE\n0.7783\n0.5809\n0.5091 0.4963 0.5069\n0.5763\n0.1429\n0.0491\n0.0319 0.0872 0.0987\n0.0070\nOCGNN\n0.7165\n0.4732\n0.5246 0.2581 0.5307\n/\n0.1352\n0.0392\n0.0375 0.0616 0.0965\n/\nAEGIS\n0.6059\n0.6496\n0.5349 0.4553 0.5516\n0.4509\n0.1200\n0.0622\n0.0413 0.0827 0.0972\n0.0053\nGAAN\n0.6513\n0.3091\n0.5216 0.2590 0.4296\n/\n0.0852\n0.0283\n0.0348 0.0436 0.0767\n/\nTAM\n0.8303\n0.6175\n0.6062 0.4039 0.5675\n/\n0.4024\n0.0547\n0.0437 0.0502 0.1013\n/\nSemi-supervised\nDOMINANT\n0.8867\n0.6167\n0.5194 0.3256 0.5314\n0.5851\n0.7289\n0.0542\n0.0414 0.0652 0.1283\n0.0076\nAnomalyDAE\n0.9171\n0.6027\n0.5280 0.5409 0.5272\n0.5866\n0.7748\n0.0538\n0.0362 0.0949 0.1177\n0.0071\nOCGNN\n0.8810\n0.5742\n0.5622 0.2881 0.6461\n/\n0.7538\n0.0492\n0.0400 0.0640 0.1501\n/\nAEGIS\n0.7593\n0.6728\n0.5605 0.5132 0.5936\n0.4450\n0.2616\n0.0685\n0.0441 0.0912 0.1110\n0.0058\nGAAN\n0.6531\n0.3636\n0.5349 0.2724 0.4355\n/\n0.0856\n0.0324\n0.0362 0.0611 0.0768\n/\nTAM\n0.8405\n0.5923\n0.5829 0.4150 0.6013\n/\n0.5183\n0.0551\n0.0446 0.0552 0.1087\n/\nGGAD (Ours)\n0.9443\n0.8228\n0.6354 0.7290 0.6476 0.5943\n0.7922\n0.1825\n0.0610 0.2425 0.1442\n0.0082\nto adapt the unsupervised methods is effective across various types of GAD models. TAM performs\nbest among the unsupervised methods. AEGIS which leverages GAN to learn the normal patterns\nperforms better than AnomalyDAE and DOMINANT on T-Finance, Reddit, and Photo, By contrast,\nreconstruction-based methods work well on Amazon and DGraph. Similar observations can be found\nfor the semi-supervised versions.\nComparison to Semi-supervised GAD Methods. The results in Table 1 show that although the semi-\nsupervised methods largely outperform unsupervised counterparts, they substantially underperform\nour method GGAD. The reconstruction-based approaches show the most competitive performance\namong the contenders in semi-supervised settings, e.g., AnomalyDAE performs best on Amazon and\nDGraph. Nevertheless, GGAD gains respectively about 1-3% AUROC/AUPRC improvement on these\n(a)  Amazon\nAUPRC\n(b)  T-Finance\nR\nR\nAUPRC\nDOMINANT\nAnomalyDAE\nOCGNN\nAEGIS\nGAAN\nTAM\nGGAD\nBaseline\nFigure 4: AUPRC results w.r.t the size\nof training normal nodes (R% of |V|).\n\u2018Baseline\u2019 denotes the performance of\nthe best unsupervised GAD method.\ntwo datasets compared to best-competing AnomalyDAE. By\ntraining on the normal nodes only, methods like TAM and\nAEGIS largely reduce the interference of unlabeled anomaly\nnodes on the model and work well on most of the datasets,\ne.g., TAM on Amazon and Reddit, AEGIS on T-Finance\nand Reddit. However, their performance is still lower than\nGGAD by a relatively large margin. On average over the\nsix datasets, GGAD outperforms the best semi-supervised\ncontender AnomalyDAE by 11% in AUROC and 5% in\nAUPRC, demonstrating that GGAD can make much better\nuse of the labeled normal nodes through our two anomaly\nprior-based losses.\n4.2\nPerformance w.r.t. Training Size and Anomaly Contamination\nAUPRC\n(a)  Amazon\n0%\n10%\n5%\n15%\n20%\nDOMINANT\nAnomalyDAE\nOCGNN\nGAAN\nAEGIS\nTAM\nGGAD\nAUPRC\n(b) T-Finance\n0%\n10%\n5%\n15%\n20%\nFigure 5: AUPRC w.r.t. contamination.\nIn order to further illustrate the effectiveness of our\nmethod, we also compare GGAD with other semi-\nsupervised methods using varying numbers of training\nnormal nodes in Fig. 4 and having various anomaly con-\ntamination rates in Fig. 5. Due to page limitation, we\npresent the AUPRC results on two datasets here only,\nshowing the representative performance. The full AUROC\nand AUPRC results are reported in App. C.\nThe results in Fig. 4 show that with increasing training\nsamples of normal nodes, the performance of all methods on all four datasets generally gets improved\nsince more normal samples can help the models more accurately capture the normal patterns during\ntraining. Importantly, GGAD consistently outperforms all competing methods with varying numbers\nof normal nodes, reinforcing that GGAD can make better use of the labeled normal nodes for GAD.\nThe labeled normal nodes can often be contaminated by anomalies due to factors like annotation\nerrors. To consider this issue, we introduce a certain ratio of anomaly contamination into into the\ntraining normal node set Vl. The results of the models under different ratios of contamination in\nFig. 5. show that with increasing anomaly contamination, the performance of all methods decreases.\n8\nDespite the decreased performance, our method GGAD consistently maintains the best performance\nunder different contamination rates, showing good robustness w.r.t. the contamination.\n4.3\nAblation Study\nImportance of the Two Anomaly Node Priors.\nThe importance of the two proposed\nlosses based on the priors on the anomaly nodes is examined by comparing our full\nmodel with its variant removing the corresponding loss, with the results shown in Ta-\nble 2.\nIt is clear that learning the outlier node representations using one of the\ntwo losses performs remarkably less effectively than our full model using both losses.\nTable 2: Ablation study on our two priors.\nMetric\nComponent\nDataset\n\u2113ala\n\u2113ec\nAmazon T-Finance Reddit Elliptic Photo DGraph\nAUROC\n\u2713\n0.8871\n0.8149\n0.5839 0.6863 0.5762 0.5891\n\u2713\n0.7250\n0.6994\n0.5230 0.7001 0.6103 0.5513\n\u2713\n\u2713\n0.9324\n0.8228\n0.6354 0.7290 0.6476 0.5943\nAUPRC\n\u2713\n0.6643\n0.1739\n0.0409 0.1954 0.1137 0.0076\n\u2713\n0.1783\n0.0800\n0.0398 0.2683 0.1186 0.0063\n\u2713\n\u2713\n0.7843\n0.1924\n0.0610\n0.2425 0.1442 0.0087\nIt is mainly because although using \u2113ala solely\ncan obtain similar local affinity of the outliers to\nthe real anomaly nodes, the outliers are still not\naligned well with the distribution of the anomaly\nnodes in the node representation space. Likewise,\nonly using the \u2113ec can result in a mismatch be-\ntween the generated outliers and real abnormal\nsamples in their graph structure. GGAD that ef-\nfectively unifies both priors through the two losses\ncan generate outlier nodes that well assimilate the real abnormal nodes on both graph struc-\nture and node representation space, supporting substantially more accurate GAD performance.\nTable 3: GGAD vs. alternative outlier generators.\nMetric\nMethod\nDataset\nAmazon T-Finance Reddit Elliptic Photo DGraph\nAUROC\nRandom\n0.7263\n0.4613\n0.5227 0.6856 0.5678\n0.5712\nNLO\n0.8613\n0.6179\n0.5638 0.6787 0.5307\n0.5538\nNoise\n0.8508\n0.8204\n0.5285 0.6786 0.5940\n0.5779\nGaussianP\n0.2279\n0.6659\n0.5235 0.6715 0.5925\n0.5862\nGGAD (Ours)\n0.9324\n0.8228\n0.6354 0.7290 0.6476\n0.5943\nAUPRC\nRandom\n0.1755\n0.0402\n0.0394 0.1981 0.1063\n0.0061\nNLO\n0.4696\n0.1364\n0.0495 0.1750 0.1092\n0.0065\nNoise\n0.5384\n0.1762\n0.0381 0.1924 0.1200\n0.0076\nGaussianP\n0.0397\n0.0677\n0.0376 0.1682 0.1194\n0.0078\nGGAD (Ours)\n0.7843\n0.1924\n0.0610 0.2425 0.1442\n0.0087\nGGAD vs. Alternative Outlier Node Genera-\ntion Approaches. To examine its effectiveness\nfurther, GGAD is also compared with four other\napproaches that could be used as an alternative\nto generating the outlier nodes. These include\n(i) Random that randomly sample some nor-\nmal nodes and treat them as outliers to train\na one-class discriminative classifier, (ii) Non-\nlearnable Outliers (NLO) that removes the\nlearnable parameters \u02dc\nW in Eq. (2) in our outlier\nnode generation, (iii) Noise that directly gen-\nerates the representation of outlier nodes from\nrandom noise, and (iv) Gaussian Perturbation (GaussianP) that directly adds Gaussian perturba-\ntions into the sampled normal nodes\u2019 representations to generate the outliers. The results are shown in\nTable 3. Random does not work properly, since the randomly selected samples are not distinguishable\nfrom the normal nodes. NLO performs fairly well on some data sets such as Amazon, T-Finance,\nand Elliptic, but it is still much lower than GGAD, showcasing that having learnable outlier node\nrepresentations can help better ground the outliers in a real local graph structure. Despite that Noise\nand GaussianP can generate outliers that have separable representations from the normal nodes,\nthey also fail to work well since the lack of graph structure in the outlier nodes can lead to largely\nmismatched distributions between the generated outlier nodes and the anomaly nodes. By contrast,\nthe outlier nodes learned by GGAD can better align with the anomaly nodes due to the incorporation\nof the anomaly priors on graph structure and feature representation into our GAD modeling.\n5\nConclusion and Future Work\nIn this paper, we investigate a new semi-supervised GAD scenario where part of normal nodes are\nknown during training. To fully exploit those normal nodes, we introduce a novel outlier generation\napproach GGAD that leverages two important priors about anomalies in the graph to learn outlier\nnodes that well assimilate real anomalies in both graph structure and feature representation space.\nThe quality of these outlier nodes is justified by their effectiveness in training a discriminative\none-class classifier together with the given normal nodes. Comprehensive experiments are performed\nto establish an evaluation benchmark on six real-world datasets for semi-supervised GAD, in which\nour GGAD outperforms 12 competing methods across the six datasets.\nLimitation and Future work. The generation of the outlier nodes in GGAD is built upon the\ntwo important priors about anomaly nodes in a graph. This helps generate outlier nodes that well\n9\nassimilate the anomaly nodes across diverse real-world GAD datasets. However, these priors are not\nexhaustive, and there can be some anomalies whose characteristics may not be captured by the two\npriors used. We will explore this possibility and improve GGAD for this case in our future work.\nReferences\n[1] Sambaran Bandyopadhyay, Lokesh N, Saley Vishal Vivek, and M Narasimha Murty. Outlier\nresistant unsupervised deep architectures for attributed network embedding. In Proceedings of\nthe 13th international conference on web search and data mining, pages 25\u201333, 2020.\n[2] Jinyu Cai and Jicong Fan. Perturbation learning based anomaly detection. Advances in Neural\nInformation Processing Systems, 35:14317\u201314330, 2022.\n[3] Jinyu Cai, Yunhe Zhang, and Jicong Fan. Self-discriminative modeling for anomalous graph\ndetection. arXiv preprint arXiv:2310.06261, 2023.\n[4] Yunkang Cao, Xiaohao Xu, Jiangning Zhang, Yuqi Cheng, Xiaonan Huang, Guansong Pang,\nand Weiming Shen. A survey on visual anomaly detection: Challenge, approach, and prospect.\narXiv preprint arXiv:2401.16402, 2024.\n[5] Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and Weihao Jiang.\nCan abnormality be detected by graph neural networks. In Proceedings of the Twenty-Ninth\nInternational Joint Conference on Artificial Intelligence (IJCAI), Vienna, Austria, pages 23\u201329,\n2022.\n[6] Zhenxing Chen, Bo Liu, Meiqing Wang, Peng Dai, Jun Lv, and Liefeng Bo. Generative\nadversarial attributed network anomaly detection. In Proceedings of the 29th ACM International\nConference on Information & Knowledge Management, pages 1989\u20131992, 2020.\n[7] Kaize Ding, Jundong Li, Nitin Agarwal, and Huan Liu. Inductive anomaly detection on\nattributed networks. In Proceedings of the twenty-ninth international conference on international\njoint conferences on artificial intelligence, pages 1288\u20131294, 2021.\n[8] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed\nnetworks. In Proceedings of the 2019 SIAM International Conference on Data Mining, pages\n594\u2013602. SIAM, 2019.\n[9] Linfeng Dong, Yang Liu, Xiang Ao, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He.\nBi-level selection via meta gradient for graph-based fraud detection. In International Conference\non Database Systems for Advanced Applications, pages 387\u2013394. Springer, 2022.\n[10] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph\nneural network-based fraud detectors against camouflaged fraudsters. In Proceedings of the\n29th ACM international conference on information & knowledge management, pages 315\u2013324,\n2020.\n[11] Haoyi Fan, Fengbin Zhang, and Zuoyong Li. Anomalydae: Dual autoencoder for anomaly\ndetection on attributed networks. In ICASSP 2020-2020 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), pages 5685\u20135689. IEEE, 2020.\n[12] Yuan Gao, Junfeng Fang, Yongduo Sui, Yangyang Li, Xiang Wang, Huamin Feng, and Yongdong\nZhang. Graph anomaly detection with bi-level optimization. In Proceedings of the ACM on\nWeb Conference 2024, pages 4383\u20134394, 2024.\n[13] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang.\nAddressing heterophily in graph anomaly detection: A perspective of graph spectrum. In\nProceedings of the ACM Web Conference 2023, pages 1528\u20131538, 2023.\n[14] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang.\nAlleviating structural distribution shift in graph anomaly detection. In Proceedings of the\nSixteenth ACM International Conference on Web Search and Data Mining, pages 357\u2013365,\n2023.\n[15] Zheng Gong, Guifeng Wang, Ying Sun, Qi Liu, Yuting Ning, Hui Xiong, and Jingyu Peng.\nBeyond homophily: Robust graph anomaly detection via neural sparsification. In Proceedings\nof International Joint Conference on Artificial Intelligence, pages 2104\u20132113, 2023.\n10\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil\nOzair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications\nof the ACM, 63(11):139\u2013144, 2020.\n[17] Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and\nQing He. Auc-oriented graph neural network for fraud detection. In Proceedings of the ACM\nWeb Conference 2022, pages 1311\u20131321, 2022.\n[18] Tianjin Huang, Yulong Pei, Vlado Menkovski, and Mykola Pechenizkiy. Hop-count based\nself-supervised anomaly detection on attributed networks. In Joint European Conference on\nMachine Learning and Knowledge Discovery in Databases, pages 225\u2013241. Springer, 2022.\n[19] Xuanwen Huang, Yang Yang, Yang Wang, Chunping Wang, Zhisheng Zhang, Jiarong Xu, Lei\nChen, and Michalis Vazirgiannis. Dgraph: A large-scale financial dataset for graph anomaly\ndetection. Advances in Neural Information Processing Systems, 35:22765\u201322777, 2022.\n[20] Minqi Jiang, Chaochuan Hou, Ao Zheng, Songqiao Han, Hailiang Huang, Qingsong Wen,\nXiyang Hu, and Yue Zhao. Adgym: Design choices for deep anomaly detection. In Thirty-\nseventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track,\n2023.\n[21] D Kinga, Jimmy Ba Adam, et al. A method for stochastic optimization. In International\nconference on learning representations (ICLR), volume 5, page 6. San Diego, California;, 2015.\n[22] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory\nin temporal interaction networks. In Proceedings of the 25th ACM SIGKDD international\nconference on knowledge discovery & data mining, pages 1269\u20131278, 2019.\n[23] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection\nin attributed networks. In IJCAI, pages 2152\u20132158, 2017.\n[24] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding,\nCanyu Chen, Hao Peng, Kai Shu, George H. Chen, Zhihao Jia, and Philip S. Yu. Pygod: A\npython library for graph outlier detection. arXiv preprint arXiv:2204.12095, 2022.\n[25] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding,\nCanyu Chen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H. Chen, Zhihao Jia, and\nPhilip S. Yu. Bond: Benchmarking unsupervised outlier node detection on static attributed\ngraphs. Advances in Neural Information Processing Systems, 35:27021\u201327035, 2022.\n[26] Kay Liu, Hengrui Zhang, Ziqing Hu, Fangxin Wang, and Philip S Yu. Data augmentation for su-\npervised graph outlier detection with latent diffusion models. arXiv preprint arXiv:2312.17679,\n2023.\n[27] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and\nchoose: a gnn-based imbalanced learning approach for fraud detection. In Proceedings of the\nweb conference 2021, pages 3168\u20133177, 2021.\n[28] Yixin Liu, Kaize Ding, Qinghua Lu, Fuyi Li, Leo Yu Zhang, and Shirui Pan. Towards self-\ninterpretable graph-level anomaly detection. arXiv preprint arXiv:2310.16520, 2023.\n[29] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly\ndetection on attributed networks via contrastive self-supervised learning. IEEE transactions on\nneural networks and learning systems, 33(6):2378\u20132392, 2021.\n[30] Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. Alleviating the inconsis-\ntency problem of applying graph neural network to fraud detection. In Proceedings of the 43rd\ninternational ACM SIGIR conference on research and development in information retrieval,\npages 1569\u20131572, 2020.\n[31] Zuhao Liu, Xiao-Ming Wu, Dian Zheng, Kun-Yu Lin, and Wei-Shi Zheng. Generating anoma-\nlies for video anomaly detection with prompt-based feature mapping. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24500\u201324510, 2023.\n[32] Rongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level\nanomaly detection by glocal knowledge distillation. In Proceedings of the Fifteenth ACM\nInternational Conference on Web Search and Data Mining, pages 704\u2013714, 2022.\n[33] Xiaoxiao Ma, Ruikun Li, Fanzhen Liu, Kaize Ding, Jian Yang, and Jia Wu. New recipes for\ngraph anomaly detection: Forward diffusion dynamics and graph generation. 2023.\n11\n[34] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and\nLeman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE\nTransactions on Knowledge and Data Engineering, 2021.\n[35] Xiaoxiao Ma, Jia Wu, Jian Yang, and Quan Z Sheng. Towards graph-level anomaly detection\nvia deep evolutionary mapping. In Proceedings of the 29th ACM SIGKDD Conference on\nKnowledge Discovery and Data Mining, pages 1631\u20131642, 2023.\n[36] Julian McAuley, Christopher Targett, Qinfeng Shi, and Anton Van Den Hengel. Image-based\nrecommendations on styles and substitutes. In Proceedings of the 38th international ACM\nSIGIR conference on research and development in information retrieval, pages 43\u201352, 2015.\n[37] Lin Meng, Hesham Mostafa, Marcel Nassar, Xiaonan Zhang, and Jiawei Zhang. Generative\ngraph augmentation for minority class in fraud detection. In Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management, pages 4200\u20134204, 2023.\n[38] Phuc Cuong Ngo, Amadeus Aristo Winarto, Connie Khor Li Kou, Sojeong Park, Farhan Akram,\nand Hwee Kuan Lee. Fence gan: Towards better anomaly detection. In 2019 IEEE 31St\nInternational Conference on tools with artificial intelligence (ICTAI), pages 141\u2013148. IEEE,\n2019.\n[39] Chaoxi Niu, Guansong Pang, and Ling Chen. Graph-level anomaly detection via hierarchical\nmemory networks. In Joint European Conference on Machine Learning and Knowledge\nDiscovery in Databases, pages 201\u2013218. Springer, 2023.\n[40] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\nanomaly detection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.\n[41] Guansong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. Toward deep\nsupervised anomaly detection: Reinforcement learning from partially labeled anomaly data.\nIn Proceedings of the 27th ACM SIGKDD conference on knowledge discovery & data mining,\npages 1298\u20131308, 2021.\n[42] Joonhyung Park, Jaeyun Song, and Eunho Yang. Graphens: Neighbor-aware ego network\nsynthesis for class-imbalanced node classification. In The Tenth International Conference on\nLearning Representations, ICLR 2022. International Conference on Learning Representations\n(ICLR), 2022.\n[43] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. Anomalous: A joint\nmodeling approach for anomaly detection on attributed networks. In IJCAI, pages 3513\u20133519,\n2018.\n[44] Bryan Perozzi and Leman Akoglu. Scalable anomaly ranking of attributed neighborhoods.\nIn Proceedings of the 2016 SIAM International Conference on Data Mining, pages 207\u2013215.\nSIAM, 2016.\n[45] Hezhe Qiao and Guansong Pang. Truncated affinity maximization: One-class homophily\nmodeling for graph anomaly detection. In Advances in Neural Information Processing Systems,\n2023.\n[46] Amit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, and Pan Li. Gad-nr :\nGraph anomaly detection via neighborhood reconstruction. In Proceedings of the 17th ACM\nInternational Conference on Web Search and Data Mining, 2024.\n[47] Mohammad Sabokrou, Mahmood Fathy, Guoying Zhao, and Ehsan Adeli. Deep end-to-end\none-class classifier. IEEE transactions on neural networks and learning systems, 32(2):675\u2013684,\n2021.\n[48] Fengzhao Shi, Yanan Cao, Yanmin Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. H2-fdetector:\nA gnn-based fraud detector with homophilic and heterophilic connections. In Proceedings of\nthe ACM Web Conference 2022, pages 1486\u20131494, 2022.\n[49] Jianheng Tang, Fengrui Hua, Ziqi Gao, Peilin Zhao, and Jia Li. Gadbench: Revisiting and\nbenchmarking supervised graph anomaly detection. arXiv preprint arXiv:2306.12251, 2023.\n[50] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly\ndetection. In International Conference on Machine Learning, pages 21076\u201321089. PMLR,\n2022.\n12\n[51] Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. Cross-\ndomain graph anomaly detection via anomaly-aware contrastive alignment. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, volume 37, pages 4676\u20134684, 2023.\n[52] Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. Open-set\ngraph anomaly detection via normal structure regularisation. arXiv preprint arXiv:2311.06835,\n2023.\n[53] Xuhong Wang, Baihong Jin, Ying Du, Ping Cui, Yingshui Tan, and Yupu Yang. One-class\ngraph neural networks for anomaly detection in attributed networks. Neural computing and\napplications, 33:12073\u201312085, 2021.\n[54] Yuchen Wang, Jinghui Zhang, Zhengjie Huang, Weibin Li, Shikun Feng, Ziheng Ma, Yu Sun,\nDianhai Yu, Fang Dong, Jiahui Jin, et al. Label information enhanced fraud detection against\nlow homophily in graphs. In Proceedings of the ACM Web Conference 2023, pages 406\u2013416,\n2023.\n[55] Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio Bellei, Tom\nRobinson, and Charles E Leiserson. Anti-money laundering in bitcoin: Experimenting with\ngraph convolutional networks for financial forensics. arXiv preprint arXiv:1908.02591, 2019.\n[56] Hongzuo Xu, Guansong Pang, Yijie Wang, and Yongjun Wang. Deep isolation forest for\nanomaly detection. IEEE Transactions on Knowledge and Data Engineering, 2023.\n[57] Yiyuan Yang, Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. Dcdetector: Dual\nattention contrastive representation learning for time series anomaly detection. In Proceedings\nof the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, 2023.\n[58] M Zaigham Zaheer, Arif Mahmood, M Haris Khan, Mattia Segu, Fisher Yu, and Seung-Ik Lee.\nGenerative cooperative learning for unsupervised video anomaly detection. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition, pages 14744\u201314754,\n2022.\n[59] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar.\nAdversarially learned anomaly detection. In 2018 IEEE International conference on data mining\n(ICDM), pages 727\u2013736. IEEE, 2018.\n[60] Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun. Tfad: A decomposition time series\nanomaly detection architecture with time-frequency analysis. In Proceedings of the 31st ACM\nInternational Conference on Information & Knowledge Management, pages 2497\u20132507, 2022.\n[61] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng.\nFraudre: Fraud detection dual-resistant to graph inconsistency and imbalance. In 2021 IEEE\nInternational Conference on Data Mining (ICDM), pages 867\u2013876. IEEE, 2021.\n[62] Ge Zhang, Zhenyu Yang, Jia Wu, Jian Yang, Shan Xue, Hao Peng, Jianlin Su, Chuan Zhou,\nQuan Z Sheng, Leman Akoglu, et al. Dual-discriminative graph neural network for imbalanced\ngraph-level anomaly detection. Advances in Neural Information Processing Systems, 35:24144\u2013\n24157, 2022.\n[63] Zhijie Zhang, Wenzhong Li, Wangxiang Ding, Linming Zhang, Qingning Lu, Peng Hu, Tong\nGui, and Sanglu Lu. Stad-gan: unsupervised anomaly detection on multivariate time series with\nself-training generative adversarial networks. ACM Transactions on Knowledge Discovery from\nData, 17(5):1\u201318, 2023.\n[64] Panpan Zheng, Shuhan Yuan, Xintao Wu, Jun Li, and Aidong Lu. One-class adversarial nets for\nfraud detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33,\npages 1286\u20131293, 2019.\n[65] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, and Liang Wang. Deep graph contrastive\nrepresentation learning. arXiv preprint arXiv:2006.04131, 2020.\n[66] Zhong Zhuang, Kai Ming Ting, Guansong Pang, and Shuaibin Song. Subgraph centralization:\nA necessary step for graph anomaly detection. In Proceedings of the 2023 SIAM International\nConference on Data Mining (SDM), pages 703\u2013711. SIAM, 2023.\n13\nA\nDetailed Dataset Description\nThe key statistics of the datasets are presented in Table 4. A detailed introduction of these datasets is\ngiven as follows.\n\u2022 Amazon [10]: It includes product reviews under the Musical Instrument category. The users\nwith more than 80% of helpful votes were labeled as begin entities, with the users with less\nthan 20% of helpful votes treated as fraudulent entities. There are three relations including\nU-P-U (users reviewing at least one same product), U-S-U (users giving at least one same\nstar rating within one week), and U-V-U (users with top-5% mutual review similarities). In\nthis paper, we do not distinguish this connection and regard them as the same type of edges,\ni.e., all connections are used. There are 25 handcrafted features that were collected as the\nraw node features.\n\u2022 T-Finance [50]: It is a financial transaction network where the node represents an anonymous\naccount and the edge represents two accounts that have transaction records. The features of\neach account are related to some attributes of logging, such as registration days, logging\nactivities, and interaction frequency, etc. The users are labeled as anomalies when they fall\ninto the categories of fraud money laundering and online gambling.\n\u2022 Reddit [22]: It is a user-subreddit graph, capturing one month\u2019s worth of posts shared across\nvarious subreddits at Reddit. The users who have been banned by the platform are labeled\nanomalies. The text of each post is transformed into the feature vector and the features of\nthe user and subreddits are the feature summation of the post they have posted.\n\u2022 Elliptic [55]: It is a bitcoin transaction network in which the node represents the transactions\nand the edge is the flow of Bitcoin currency.\n\u2022 Photo [36]: It is an Amazon co-purchase network in which the node represents the product\nand the edge represents the co-purchase relationship. The attribute of the node is a bag of\nworks representation of the user\u2019s comments.\n\u2022 DGraph [19]: It is a large-scale attributed graph with millions of nodes and edges where the\nnode represents a user account in a financial company and the edge represents that the user\nwas added to another account as an emergency contact. The feature of a node is the profile\ninformation of users, such as age, gender, and other demographic features. The users who\nhave overdue history are labeled as anomalies.\nB\nMore Information about the Competing Methods\nB.1\nCompeting Methods\nA more detailed introduction of the six GAD models we compare with is given as follows.\n\u2022 DOMINANT [8] leverages the auto-encoder for graph anomaly detection. It consists of an\nencoder layer and a decoder layer which are devised to reconstruct the features and structure\nof the graph. The reconstruction errors from the features and the structural modules are\ncombined as an anomaly score.\n\u2022 AnomalyDAE [11] consists of a structure autoencoder and an attribute autoencoder to learn\nboth node embeddings and attribute embeddings jointly in a latent space. In addition, an\nattention mechanism is employed in the structure encoder to capture normal structural\npatterns more effectively.\n\u2022 OCGNN [53] applies one-class SVM and GNNs, aiming at combining the recognition ability\nof one-class classifiers and the powerful representation of GNNs. A one-class hypersphere\nlearning objective is used to drive the training of the GNN. The sample that falls outside the\nhypersphere is defined as an anomaly.\n\u2022 AEGIS [7] designs a new graph neural layer to learn anomaly-aware node representations\nand further employ generative adversarial networks to detect anomalies among new data.\nThe generator takes noises sampled from a prior distribution as input, and attempts to\ngenerate informative pseudo anomalies. The discriminator tries to distinguish whether an\ninput is the representation of a normal node or a generated anomaly.\n14\nTable 4: Key statistics of the six datasets used in our experiments.\nDataset\nType\n# Nodes\n# Edges\n# Attributes\n#Anomalies (Rate)\nAmazon\nCo-review\n11,944\n4,398,392\n25\n821(6.9%)\nT-Finance\nTransaction\n39,357\n21,222,543\n10\n1,803(4.6%)\nReddit\nSocial Media\n10,984\n168,016\n64\n366(3.3%)\nElliptic\nBitcoin Transaction\n46,564\n73,248\n93\n4,545 (9.76%)\nPhoto\nCo-purchase\n7,487\n119,043\n745\n369(4.9%)\nDGraph\nFinancial Networks\n3,700,550\n73,105,508\n17\n15,509(1.3%)\n\u2022 GAAN [6] is based on a generative adversarial network where fake graph nodes are generated\nby a generator. To encode the nodes, they compute the sample covariance matrix for real\nnodes and fake nodes, and a discriminator is trained to recognize whether two connected\nnodes are from a real or fake node.\n\u2022 TAM [45] learns tailored node representations for a local affinity-based anomaly measure\nby maximizing the local affinity of nodes to their neighbors. TAM is optimized on truncated\ngraphs where non-homophily edges are removed iteratively. The learned representations\nresult in significantly stronger local affinity for normal nodes than abnormal nodes. So, the\nlocal affinity of a node in the learned representation space is used as anomaly score.\nB.2\nOfficial Source Code.\nAll the competing methods except TAM are implemented by PyGOD Library [24,25]. The code of\nTAM is taken from its authors. The links to their source codes are as follows:\n\u2022 PyGOD: https://github.com/pygod-team/pygod\n\u2022 TAM: https://github.com/mala-lab/TAM-master\n\u2022 DOMINANT: https://github.com/kaize0409/GCN_AnomalyDetection_pytorch\n\u2022 AnomalyDAE: https://github.com/haoyfan/AnomalyDAE\n\u2022 OCGNN: https://github.com/WangXuhongCN/OCGNN\nC\nAdditional Experimental Results\nC.1\nSensitivity Analysis\nThis section analyzes the sensitivity of GGAD w.r.t four key hyperparameters, including the affinity\nmargin \u03b1, hyperparameters of structural affinity loss \u03b2 and egocentric closeness \u03bb, and the number of\ngenerated outlier nodes S. The AUPRC results are reported in Fig. 6. We discuss these results below.\nImpact of Margin \u03b1 in \u2113ala. \u03b1 in \u2113ala denotes the affinity difference we enforce between the normal\nand outlier nodes. As Fig. 6(a) shows, GGAD performs best on Amazon and Photo with increasing\n\u03b1 while it performs stably on the other four datasets with varying \u03b1, indicating that enforcing local\nseparability is more effective on Amazon and Photo than the other datasets, as can also be observed\nin Table 3.\nImpact of Hyperparameters \u03b2 and \u03bb. As shown in Figs. 6(b)(c), with increasing \u03b2 and \u03bb, our\nmodel GGAD generally performs better, indicating that a stronger structural affinity or egocentric\ncloseness constraint is generally preferred to generate outlier nodes that are more aligned with the\nreal abnormal nodes.\nThe Number of Generated Outlier Nodes S. As shown in Fig. 6(d), where S indicates that we\ngenerate the outlier nodes in a quantity at a rate of S% of |Vl|, GGAD gains some improvement\nwith more generated outlier nodes but it maintains the same performance after a certain number of\noutlier nodes. This is mainly because the generated outlier nodes may not be diversified enough to\nresemble all types of abnormal nodes, even with much more outlier nodes. The declined performance\non Amazon and Photo is mainly due to the fact that the labeled normal data in these two datasets\nis small and is overwhelmed by increasing outlier nodes, leading to worse training of the one-class\n15\n\uf062\nAUPRC\n\uf061\ns\nAUPRC\n\uf06c\n(a)  \n(b)\n(d)\nT-Finance\nDGraph\nAmazon\nReddit\n(c)\nAUPRC\nAUPRC\nPhoto\nElliptic\nFigure 6: AUPRC of GGAD w.r.t hyperparameters \u03b1, \u03b2, \u03bb, S.\n\uf062\nAUROC\n\uf061\ns\nAUROC\n\uf06c\n(a)  \n(b)\n(d)\n(c)\nAUROC\nAUROC\nT-Finance\nDGraph\nAmazon\nReddit\nPhoto\nElliptic\nFigure 7: AUROC results w.r.t hyperparameters \u03b1, \u03b2, \u03bb, S.\nclassifier. The AUROC results of these four key hyperparameters are shown in Fig. 7, which show a\nsimilar trend as the AUPRC results.\nC.2\nMore Results for Models Trained on Varying Number of Normal Nodes\nThe results of AUPRC and AUROC under different training normal sample sizes are shown in Fig. 8.\nand Fig. 9 respectively. The results show that increasing training samples of normal nodes can help\nthe methods more accurately capture the normality, resulting in a consistent improvement. Among\nthese methods, GGAD consistently outperforms the competing methods with varying numbers of\ntraining normal nodes, indicating that GGAD can make better use of labeled normal nodes.\n16\n(a)  Amazon\nAUPRC\n(b)  T-Finance\nAUPRC\nDOMINANT\nAnomalyDAE\nOCGNN\nAEGIS\nGAAN\nTAM\nGGAD\nBaseline\n(c) Reddit\nAUPRC\nAUPRC\n(f) DGraph\n(d) Elliptic\nR\nAUPRC\nAUPRC\n(e) Photo\nR\nR\nR\nR\nR\nFigure 8: AUPRC results w.r.t. different number of training normal nodes (R% of |V|). DGraph\nis a very large dataset, so a significantly smaller R is used to have a similar size of Vl as the other\ndatasets. \u2018Baseline\u2019 denotes the performance of the best unsupervised GAD method per dataset.\nAUROC\nAUROC\nAUROC\n(a)  Amazon\n(c) Reddit\n(b)  T-Finance\nAUROC\n(f) DGraph\nR\nAUROC\nAUROC\nR\n(d) Elliptic\n(e) Photo\nDOMINANT\nAnomalyDAE\nOCGNN\nAEGIS\nGAAN\nTAM\nGGAD\nBaseline\nR\nR\nR\nR\nFigure 9: AUROC results w.r.t. different number of training normal nodes (R% of |V|). DGraph\nis a very large dataset, so a significantly smaller R is used to have a similar size of Vl as the other\ndatasets.\u2018Baseline\u2019 denotes the performance of the best unsupervised GAD method per dataset.\n17\nAUPRC\nAUPRC\n(a)  Amazon\n(c) Reddit\nAUPRC\n(b) T-Finance\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\nDOMINANT\nAnomalyDAE\nOCGNN\nGAAN\nAEGIS\nTAM\nGGAD\n(f) DGraph\n(e) Photo\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\nAUPRC\nAUPRC\n(d)  Elliptic\n0%\n10%\n5%\n15%\n20%\nAUPRC\nFigure 10: AUPRC w.r.t. different anomaly contamination.\nC.3\nMore Results for Robustness w.r.t. Anomaly Contamination\nThe full AUROC and AUPRC results under different anomaly contamination rates for all six real-\nworld datasets are shown in Fig. 10 and Fig. 11, respectively. The results show that the performance\nof all methods decreases with the increasing rate of contamination. The reconstruction-based methods\nDOMINANT and AnomalyDAE are the most sensitive models, followed by GAAN, TAM, and\nAEGIS. OCGNN is relatively stable under the contamination. Despite the declined performance for\nall the methods, our method GGAD still maintains the best performance under different contamination\nrates.\nD\nComputational Efficiency Analysis\nD.1\nTime Complexity Analysis\nThis subsection analyzes the time complexity of GGAD. We build a GCN to obtain the representation\nof each node, which takes O(mdh), where m is the number of non-zero elements in matrix A, d is\nthe dimension of representation, and h is the number of feature maps. The outliers are generated from\nthe ego network of a labeled normal node, which takes O(Skd2) where S is the number of generated\noutliers and k is the number of average neighbors for each outlier. The affinity calculation will take\nO(N 2d), where N is the number of nodes. The structural affinity and egocentric closeness losses\ntake O(N) and O(Sd), respectively. The MLP layer mapping the representation to the anomaly score\ntakes O(Nd2). Thus, the overall complexity of GGAD is O(mdh + Skd2 + N 2d + N + Sd + Nd2).\nD.2\nRuntime Results\nThe runtimes, including both training and inference time, of GGAD and six semi-supervised com-\npeting methods on CPU are shown in Table 5. In GGAD, although calculating the local affinity of\neach node requires some overheads, it is still much more efficient than the reconstruction operations\non both the attributes and the structure as in DOMINANT and AnomalyDAE. Compared to the\ngenerative models AEGIS and GAAN, GGAD is generally more efficient on larger graph datasets\nlike Amazon, T-Finance, and DGraph. OCGNN is a model with the simplest operation, to which our\nGGAD can also have comparable efficiency. These results demonstrate the advantage of GGAD in\ncomputational efficiency\n18\nAUROC\nAUROC\nAUROC\n(a)  Amazon\n(c) Reddit\n(b) T-Finance\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\nDOMINANT\nAnomalyDAE\nOCGNN\nGAAN\nAEGIS\nTAM\nGGAD\n(f) DGraph\n(e) Photo\n0%\n10%\n5%\n15%\n20%\n0%\n10%\n5%\n15%\n20%\nAUROC\nAUROC\n(d)  Elliptic\n0%\n10%\n5%\n15%\n20%\nAUROC\nFigure 11: AUROC w.r.t. different anomaly contamination.\nTable 5: Runtimes (in seconds) on the six datasets on CPU.\nMethod\nDataset\nAmazon\nT-Finance\nReddit\nElliptic\nPhoto\nDGraph\nDOMINANT\n1592\n10721\n125\n1119\n437\n388\nAnomalyDAE\n1656\n18560\n161\n8296\n445\n457\nOCGNN\n765\n5717\n162\n3517\n125\n/\nAEGIS\n1121\n15258\n166\n5638\n417\n1022\nGAAN\n1678\n12120\n94\n1866\n307\n/\nTAM\n4516\n17360\n432\n13200\n165\n/\nGGAD (Ours)\n658\n9345\n368\n5146\n106\n488\nE\nPseudo Codes of GGAD\nThe training algorithms of GGAD are summarized in Algorithm 1 and Algorithm 2. Algorithm 1\ndescribes the full training process of GGAD. Algorithm 2 describes the mini-batch processing for\nhandling very large graph datasets, i.e., DGraph. Since the number of the outlier nodes is significantly\nsmaller than that of the normal nodes, we guarantee that each mini-batch consists of both normal and\noutlier nodes to address the data imbalance problem. The outputs are the mini-batches of samples\nfrom the graph and corresponding structural information, which can then be used as the input of\nGGAD or the competing models to perform GAD on DGraph. Note that when using Algorithm 2 for\nthe competing models, the steps that involve the generated outliers are not used if they do not have\nthe outlier generation component.\n19\nAlgorithm 1 GGAD\nInput: Graph, G= (V, E, X), N: Number of training nodes, , Nu: Number of unlabeled nodes, L:\nNumber of layers, E: Training epochs, Vtrain:Training set, Vtest:Test set , S: The number of the\ngenerated outlier nodes\nOutput: Anomaly scores of all nodes.\n1: Sample the ego networks of some normal nodes upon which the outlier nodes are to be generated\nVo = [vo1, ..., vos]\n2: Compose Vtrain with the outlier nodes Vo and the given labeled normal node set Vl\n3: Randomly initialize GNN (h(0)\n1 , h(0)\n2 , ..., h(0)\nN ) \u2190X\n4: for epoch = 1, \u00b7 \u00b7 \u00b7 , E do\n5:\nfor each v in Vtrain do\n6:\nfor l = 1, \u00b7 \u00b7 \u00b7 , L do\n7:\nh(l)\nv\n= \u03d5(h(l\u22121)\nv\n; \u0398)\n8:\nh(l)\nv\n= ReLU\n\u0010\nAGG({h(l)\nv\u2032 : (v, v\u2032) \u2208E})\n\u0011\n9:\nend for\n10:\nend for\n11:\nfor k = 1, \u00b7 \u00b7 \u00b7 , S do\n12:\nObtain the representations (e.g., \u02c6hk) of the generated outliers using our outlier generation\nmethod using Eq. (2)\n13:\nend for\n14:\nCompute the normal nodes\u2019 affinity \u03c4(Vo) and the outlier nodes\u2019 affinity \u03c4(Vl)\n15:\nCompute the structural affinity loss \u2113ala and egocentric closeness loss \u2113ec using Eq. (4) and\nEq. (5) respectively.\n16:\nCompute the BCE loss function \u2113bce for our one-class classifier \u03b7(hi; \u0398s) using Eq. (6)\n17:\nCompute the total loss \u2113total = \u2113bce + \u03b2\u2113ala + \u03bb\u2113ec\n18:\nUpdate the weight parameters \u0398, \u0398g and \u0398s by using gradient descent\n19: end for\n20: for each vi in Vtest do\n21:\nAnomaly scoring by s (vi) = 1 \u2212\u03b7 (hj; \u0398\u2217)\n22: end for\n23: return Anomaly scores s(v1), \u00b7 \u00b7 \u00b7 , s(vNu)\nAlgorithm 2 Mini-Batch Processing\nInput: Graph, G= (V, E, X), N: Number of nodes, t: Batch size, z: Number of batches, S: The\nnumber of the generated outlier nodes, Vtrain:Training set\nOutput: Mini-batches and sub-graph structure.\n1: Initialize the batch B = (b1, ..., bz), where b = [v1, ..., vt] from given Vtrain\n2: for each b in B do\n3:\nSample S/z nodes as initial outliers [vo1, ...voS] from batch b\n4:\nInitialize a node set Vb for batch b\n5:\nfor each v in b do\n6:\nFind the 2-hop neighborhoods N2\nv of v and add them into the node set Vb\n7:\nend for\n8:\nBuild a sub-graph structure Eb for batch b using the node set Vb\n9: end for\n10: return The batch B = (b1, ..., bz) and sub-graph structure [E1, ..., Ez]\n20\n",
    "2205.13845": "Raising the Bar in Graph-level Anomaly Detection\nChen Qiu1,2 , Marius Kloft2 , Stephan Mandt3 and Maja Rudolph1\n1Bosch Center for Arti\ufb01cial Intelligence\n2TU Kaiserslautern, Germany\n3University of California, Irvine, USA\nchen.qiu@de.bosch.com, kloft@cs.uni-kl.de, mandt@uci.edu, maja.rudolph@us.bosch.com\nAbstract\nGraph-level anomaly detection has become a crit-\nical topic in diverse areas, such as \ufb01nancial fraud\ndetection and detecting anomalous activities in so-\ncial networks.\nWhile most research has focused\non anomaly detection for visual data such as im-\nages, where high detection accuracies have been\nobtained, existing deep learning approaches for\ngraphs currently show considerably worse perfor-\nmance. This paper raises the bar on graph-level\nanomaly detection, i.e., the task of detecting ab-\nnormal graphs in a set of graphs. By drawing on\nideas from self-supervised learning and transfor-\nmation learning, we present a new deep learning\napproach that signi\ufb01cantly improves existing deep\none-class approaches by \ufb01xing some of their known\nproblems, including hypersphere collapse and per-\nformance \ufb02ip.\nExperiments on nine real-world\ndata sets involving nine techniques reveal that our\nmethod achieves an average performance improve-\nment of 11.8% AUC compared to the best existing\napproach.\n1\nIntroduction\nAnomaly detection (AD) is an important tool for scanning\nsystems for unknown threats. Many web-based systems are\nbest represented by graphs and there has been work on de-\ntecting anomalous nodes and edges within a graph (Akoglu\net al., 2015). However, in many applications, it is much more\nrelevant to ask whether an entire graph is abnormal.\nFor example, in a \ufb01nancial network with nodes represent-\ning individuals, businesses, and banks and with edges rep-\nresenting transactions, it might be dif\ufb01cult to detect certain\ncriminal activity by looking at individual nodes and edges\n(Jullum et al., 2020). Clever criminals can hide their inten-\ntions behind innocent-looking transactions. However, the en-\ntire network associated with a money-laundering scheme is\nharder to obfuscate and will still exhibit properties of crim-\ninal activity. By using tools for graph-level AD, we might\nbe able to detect an entire criminal network rather than \ufb02ag\nindividual entities.\nUnfortunately, there has been limited success in adapting\nadvances in deep anomaly detection to graph-level AD (Zhao\nDD\nPROT\nENZY\nNCI1\nMutag\nIMDB-B\nRDT-B\nRDT-M\nAIDS\n50\n60\n70\n80\n90\nAUCs(%)\nOCGIN (Zhao and Akoglu, 2021)\nOCGTL (ours)\nFigure 1: AUC comparison between one-class graph trans-\nformation learning (OCGTL) (ours) and One-class GIN (OC-\nGIN) (Zhao and Akoglu, 2021) on several datasets from\nSec. 4. OCGTL improves anomaly detection accuracy.\nand Akoglu, 2021). Our work addresses this shortcoming.\nFor graph-level AD, we assume to have access to a large\ndataset of typical graphs, such as a dataset of communities\nin a social network or a dataset of snapshots of a \ufb01nancial\nnetwork. All graphs in the training data are considered \u201cnor-\nmal\u201d. The goal is to use the data to learn an anomaly scor-\ning function which can then be used to score how likely it is\nthat a new graph is either normal or abnormal. Importantly,\nthe term graph-level AD refers to detecting entire abnormal\ngraphs, rather than localizing anomalies within graphs.\nRecently there has been a trend of using deep learning in\nAD on images (Golan and El-Yaniv, 2018) or tabular and se-\nquential data (Qiu et al., 2021). However, there has been lim-\nited research on deep AD for graphs. This may seem sur-\nprising since it appears straightforward to adopt a deep AD\nmethod for tabular data into one for graphs by de\ufb01ning an\nappropriate feature map. Yet, Zhao and Akoglu (2021) found\nthat the resulting methods often perform close to random, and\nso far, attempts to adopt modern AD methods (based on deep\nlearning) to graph-level AD have not been successful.\nWe develop one-class graph transformation learning\n(OCGTL), a new model for graph level AD that combines\ndeep one-class classi\ufb01cation (OCC) and self-supervision.\nFig. 2 provides a sketch of the approach. The OCGTL archi-\narXiv:2205.13845v1  [cs.LG]  27 May 2022\nFigure 2: Sketch of the OCGTL procedure. Given a graph\n(left), we use a set of GNNs to embed the latter into a la-\ntent space. The different GNNs embeddings are trained to be\nboth diverse while also being close to a so-called \u2019reference\nembedding\u2019. See Sec. 3 for more details.\ntecture consists of K + 1 graph neural networks (GNNs) that\nare jointly trained on two complementary deep AD losses.\nIn Sec. 3.2 we prove that this new combined loss mitigates\nknown issues of previous deep AD approaches (Ruff et al.,\n2018; Zhao and Akoglu, 2021).\nFig. 1 shows that the approach signi\ufb01cantly raises the bar in\ngraph-level AD performance. In an extensive empirical study,\nwe evaluate nine methods on nine real-world datasets. Our\nwork brings deep AD on graphs up to speed with other do-\nmains, contributes a completely new method (OCGTL), and\npaves the way for future progress.\nIn summary, our main contributions are as follows:\n\u2022 We develop OCGTL, a novel end-to-end method for\ngraph-level AD, which combines the advantages of OCC\nand neural transformation learning.1\n\u2022 In Sec. 3.2, we prove theoretically that OCGTL is not\nsusceptible to the useless trivial solutions that are opti-\nmal under the objective of Zhao and Akoglu (2021).\n\u2022 In Sec. 4, we study nine methods (four newly developed)\non nine real-world graph datasets, ranging from social\nnetworks to bioinformatics datasets. We improve the ar-\nchitectures of existing deep approaches to graph-level\nAD. Yet, OCGTL signi\ufb01cantly raises the anomaly de-\ntection accuracy over previous work.\n2\nRelated Work\nDeep Anomaly Detection.\nDeep AD has received massive\nattention in various domains (Ruff et al., 2021). Related work\non deep AD can be summarized in the following classes.\nDeep autoencoder variants (Zong et al., 2018) detect anoma-\nlies based on the reconstruction error. Deep one-class net-\nworks (Ruff et al., 2018) are trained on an OCC objective to\nmap normal data close to a center in an embedding space.\nThey score anomalies based on their distance to the center.\nDeep generative models detect anomalies based on density\nestimation (Zhang et al., 2021) or using the discriminator of\ngenerative adversarial networks (Deecke et al., 2018). Self-\nsupervised AD methods have achieved great success on im-\n1Code is available at\nhttps://github.com/boschresearch/GraphLevel-AnomalyDetection\nages (Ruff et al., 2021). They rely on a self-supervision task\nfor training and anomaly scoring. The self-supervision tasks\noften require data augmentation, e.g., for transformation pre-\ndiction (Golan and El-Yaniv, 2018). Another self-supervised\nparadigm using data augmentations is contrastive learning\n(Chen et al., 2020). In contrastive learning for AD, trans-\nformed (e.g. rotated) images are treated as negative samples\n(Sohn et al., 2020). For data types beyond images, hand-\ncrafting transformations is challenging. Data-driven neural\ntransformations (Qiu et al., 2021) have shown success in AD.\nGraph Anomaly Detection.\nFinding abnormal nodes or\nedges within a large graph is widely studied (Akoglu et al.,\n2015). Deep learning based methods for node- and edge-level\nAD have been successfully applied to both static (Ding et al.,\n2019, 2020) and dynamic graphs (Yoon et al., 2019; Zheng et\nal., 2019). In contrast, deep learning for graph-level AD, the\nproblem we study in this paper, has received less attention.\nZhao and Akoglu (2021) \ufb01rst explore how to extend deep\nOCC for graph-level AD and develop one-class GIN (OC-\nGIN). They also introduce two-stage graph-level AD frame-\nworks with either graph embedding models or graph kernels.\nHowever, all these attempts have not yet produced a solid\nbaseline for graph-level AD. Zhao and Akoglu (2021) report\nthat these methods suffer from \u201cperformance \ufb02ip\u201d, which is\nan issue where the model performs worse than random on at\nleast one experimental variant. We study how to overcome it.\nDeep Learning for Graphs.\nGNNs automate feature ex-\ntraction from graphs and play an important role in graph clas-\nsi\ufb01cation and representation learning.\nGraph Convolution\nNetworks (Kipf and Welling, 2016) learn a node represen-\ntation by aggregating the representations of the node\u2019s neigh-\nbors. Errica et al. (2020) provide a fair comparison of various\nGNNs for graph classi\ufb01cation. An outstanding example is\ngraph isomorphism network (GIN) (Xu et al., 2018a), which\nis proven to be as powerful as the Weisfeiler-Lehman graph\nisomorphism test and is the architecture we use for all GNN-\nbased AD methods we study in this paper.\n3\nOne-Class Graph Transformation Learning\nfor Anomaly Detection\nWe propose a new method for graph-level AD. One-class\ngraph transformation learning (OCGTL) combines the com-\nplementary strengths of deep OCC (Ruff et al., 2018; Zhao\nand Akoglu, 2021) and self-supervised AD with learnable\ntransformations (Qiu et al., 2021).\nOur new self-supervised approach is designed to overcome\nknown issues of deep OCC. The deep OCC objective is prone\nto a trivial solution called hypersphere collapse. The deep\none-class objective encourages all the graph embeddings of\nthe graphs in the training data to concentrate within a hyper-\nsphere. This task can be solved perfectly when the feature\nextractor learns to map all inputs to the center of the hyper-\nsphere. Our model provably overcomes hypersphere collapse\nby regularizing the one-class terms in the objective with a\ntransformation learning term. The resulting model is more\n\ufb02exible (for example, the hypersphere centers can be treated\nas trainable parameters) and training is more robust, despite\nthe added \ufb02exibility.\nZhao and Akoglu (2021) have developed the \ufb01rst deep one-\nclass approach to graph-level AD. In their paper, they report\nan additional, practical dif\ufb01culty in graph-level AD which\nthey call the performance \ufb02ip issue. In many of their experi-\nments, their trained model (OCGIN) systematically confuses\nanomalies with normal samples. The goal of this work is to\novercome both hypersphere collapse and performance \ufb02ip.\nOur model consists of an ensemble of GNNs. One of them\n\u2013 the reference feature extractor \u2013 produces a reference em-\nbedding of its input graph. The other GNN feature extractors\nproduce alternative \u201clatent views\u201d of the graph. The objec-\ntive of our approach has a one-class term and a transforma-\ntion learning term. The one-class term aims at concentrating\nall the latent views within a hyper-sphere in the embedding\nspace. Transformation learning has the competing objective\nto make each view predictive of the reference embedding.\nIt encourages the latent views to be diverse yet semantically\nmeaningful. By counteracting the one-class term in this man-\nner, hypersphere collapse can be provably avoided.\nThe tension that arises from satisfying both aspects of the\nobjective has further advantages.\nIn particular, it leads to\na harder self-supervision task, which in turn leads to better\nanomaly detection performance. When the training objective\nis dif\ufb01cult to satisfy, the trained model has to be more sen-\nsitive to typical salient features of normal data. New graphs\nwhich do not exhibit these features incur a higher loss and\nare then more easily detected as anomalies. Also, the two\nloss contributions focus on different notions of distance be-\ntween the graph embeddings. The one-class term is based on\nEuclidean distances, while the transformation learning loss\nis based on angles between embeddings. With the combined\nloss as the anomaly score, our method is sensitive to abnormal\nembedding con\ufb01gurations both in terms of angles between\nthe latent views and in terms of Euclidean distances.\nIn this section, we \ufb01rst introduce OCGTL and then detail\nits main ingredients, including self-supervised AD with learn-\nable transformations, deep OCC, and feature extraction with\nGNNs. We then present the theory behind OCGTL.\n3.1\nProposed Method - OCGTL\nOCGTL combines the best of OCC and neural transforma-\ntion learning. The OCGTL architecture consists of a refer-\nence feature extractor f and K additional feature extractors\nfk (k = 1, \u00b7 \u00b7 \u00b7 , K), which are trained jointly as illustrated\nin Fig. 2. Each of the feature extractors is a parameterized\nfunction (e.g. GNN) which takes as input an attributed graph\nG = {V, E, X}, with vertex set V, edges E, and node features\n(attributes) X = {xv|v \u2208V} and maps it into an embedding\nspace Z. These K + 1 feature extractors are trained jointly\non the OCGTL loss, LOCGTL = EG [LOCGTL(G)]. Each graph\nin the training data contributes two terms to the loss,\nLOCGTL(G) = LOCC(G) + LGTL(G).\n(1)\nThe \ufb01rst term, LOCC(G), is a one-class term; it encourages all\nthe embeddings to be as close as possible to the same point\n\u03b8 \u2208Z. The second term, LGTL, enforces each GNN\u2019s embed-\ndings to be diverse and semantically meaningful representa-\ntions of the input graph G.\nThe two terms are presented in detail below.\nThe Graph Transformation Learning Term\nNeural transformation learning (Qiu et al., 2021) is a self-\nsupervised training objective for deep AD which has seen\nsuccess on time series and tabular data. Here we generalize\nthe training objective of Qiu et al. (2021) (by dropping their\nparameter sharing constraint) and adapt it to graphs.\nFor a graph G, the loss of graph transformation learning\nencourages the embeddings of each GNN, fk(G), to be simi-\nlar to the embedding of the reference GNN, f(G), while be-\ning dissimilar from each other. Consequently, each GNN fk\nis able to extract graph-level features to produce a different\nview of G. The contribution of each graph to the objective is\nLGTL(G) = \u2212\nK\nX\nk=1\nlog ck\nCk\n(2)\nwith\nck = exp\n\u00121\n\u03c4 sim(fk(G), f(G))\n\u0013\n,\nCk = ck +\nK\nX\nl\u0338=k\nexp\n\u00121\n\u03c4 sim(fk(G), fl(G))\n\u0013\n,\nwhere \u03c4 denotes a temperature parameter.\nThe similar-\nity here is de\ufb01ned as the cosine similarity sim(z, z\u2032) :=\nzT z\u2032/\u2225z\u2225\u2225z\u2032\u2225. Note that the above loss is more general than\nthe one proposed in Qiu et al. (2021) as it omits a parameter\nsharing constraint between transformations. This choice is\ninspired by the observation in You et al. (2020) that different\ngraph categories prefer different types of transformations.\nThe One-Class Term\nOne-class classi\ufb01cation (OCC) is a popular paradigm for AD\n(Noumir et al., 2012). The idea is to map data into a mini-\nmal hypersphere encompassing all normal training data. Data\npoints outside the boundary are considered anomalous. The\ncontribution of each graph G to our OCC objective is\nLOCC(G) =\nK\nX\nk=1\n\u2225(fk(G) \u2212\u03b8)\u22252\n(3)\nThe loss function penalizes the distance of the graph G to the\ncenter \u03b8 which we treat as a trainable parameter. In previous\ndeep OCC approaches, the center \u03b8 has to be a \ufb01xed hyperpa-\nrameter to avoid trivial solutions to Eqn. (3).\nFeature Extraction with GNNs\nFor graph data, parametrizing the feature extractors f and\nf1, \u00b7 \u00b7 \u00b7 , fK by GNNs is advantageous. At each layer l, a\nGNN maintains node representation vectors h(l)\nv\nfor each\nnode v. The representation is computed based on the pre-\nvious layer\u2019s representations of v and its neighbors N(v),\nh(l)\nv\n= GNN(l) \u0010\nh(l\u22121)\nv\n, h(l\u22121)\nu\n| u \u2208N(v)\n\u0011\n.\n(4)\nEach layer\u2019s node representations are then combined into\nlayer-speci\ufb01c graph representations,\nh(l)\nG = READOUT(l) \u0010\nh(l)\nv\n| v \u2208G\n\u0011\n,\n(5)\nwhich are concatenated into graph-level representations,\nhG = CONCAT\n\u0010\nh(l)\nG | l = 1, ..., L\n\u0011\n.\n(6)\nThis concatenation introduces information from various hier-\narchical levels (Xu et al., 2018b) into the graph representa-\ntion. Our empirical study in Sec. 4 shows that the choice of\nthe readout function (which determines how the node repre-\nsentations are aggregated into graph representations) is par-\nticularly important to detect anomalies reliably.\nAnomaly Scoring with OCGTL\nOCGTL is an end-to-end methods for graph-level AD. Dur-\ning training the GNNs are trained on Eqn. (1). During test,\nLOCGTL (Eqn. (1)) is used directly as the score function for\ndetecting anomalous graphs. A low loss on a test sample\nmeans that the graph is likely normal, whereas a high loss is\nindicative of an anomaly. One advantage of OCGTL is that its\nloss makes it more sensitive to different types of anomalies by\nconsidering both angles between embeddings and Euclidean\ndistances. In contrast, OCC-based methods typically rely on\nthe Euclidean distance only.\nAnother advantage of OCGTL over OCC-based ap-\nproaches is that its training is more robust and the AD model\ncan be more \ufb02exible. We prove this next.\n3.2\nA Theory of OCGTL\nA known dif\ufb01culty for training OCC-based deep anomaly de-\ntectors (such as deep SVDD and OCGIN) is hypersphere col-\nlapse (Ruff et al., 2018). Hypersphere collapse is a trivial\noptimum of the training objective\nL(Ruff et al., 2018)(G) = ||f(G) \u2212\u03b8||2\n2 ,\n(7)\nwhich occurs when the feature extractor f maps all inputs ex-\nactly into the center \u03b8. The hypersphere then has a radius of\nzero, and AD becomes impossible. Ruff et al. (2018) recom-\nmend \ufb01xing \u03b8 and avoiding bias terms for f and show good\nresults in practice. However, there is no guarantee that a triv-\nial solution can be avoided under any architecture for f. Here\nwe prove that OCGTL overcomes this.\nWe \ufb01rst show that our one-class term (Eqn. (3)) is also\nprone to hypersphere collapse when all the feature extractors\nare constant. However, we then show that this trivial solu-\ntion for minimizing Eqn. (3) is not optimal under the OCGTL\nloss. Our method provably avoids hypersphere collapse even\nwhen the center \u03b8 is a trainable parameter. This result makes\nOCGTL the \ufb01rst deep one-class approach where the center\ncan be trained.\nProposition 1. The constant feature extractors, fk(G) = \u03b8\nfor all k and all inputs G, minimize LOCC (Eqn. (3)).\nProof. 0 \u2264LOCC is the squared \u21132 norm of the distance\nbetween the embedding of G and the center \u03b8. Plugging in\nfk(G) = \u03b8 attains the minimum 0.\nIn contrast, regularization with transformation learning can\navoid hypersphere collapse. Under the constant encoder, all\nthe latent views are the same and hence at least as close to\neach other as to the reference embedding, leading to LGTL \u2265\nK log K.\nHowever, the transformation learning objective\naims at making the views predictive of the reference em-\nbeddings, in which case LGTL < K log K. The following\nproposition shows that if there is a parameter setting which\nachieves this, the constant feature extractors do not minimize\nthe OCGTL loss which proves that hypersphere collapse can\nbe avoided.\nProposition 2. If there exists a parameter setting such that\nLGTL < K log K on the training data, then the constant fea-\nture extractors fk(G) = \u03b8 do not minimize the combined loss\nLOCGTL (Eqn. (1)).\nProof. For constant feature extractors fk(G) = \u03b8, LOCGTL =\nLGTL \u2265K log K, where K is the number of transformations\nand K log K is the negative entropy of randomly guessing\nthe reference embedding.\nAssume there is a constellation\nof the model parameters s.t.\nLGTL < K log K.\nSince \u03b8\nis trainable, we can set it to be the origin. The loss of the\noptimal solution is at least as good as the loss with \u03b8 = 0.\nSet \u03f5 = K log K \u2212LGTL. The encoders can be manipu-\nlated such that their outputs are rescaled and as a result all\nthe embeddings have norm ||fk(G)||2 < \u03f5/K. As the norm\nof the embeddings changes, LGTL remains unchanged since\nthe cosine similarity is not sensitive to the norm of the em-\nbeddings. By plugging this into Eqn. (1) we get LOCGTL =\nPK\nk=1 ||fk(G)||2 +LGTL < K log K, which is better than the\nperformance of the best set of constant encoders.\nProps. 1 and 2 demonstrate that our method is the \ufb01rst deep\none-class method not prone to hypersphere collapse. The as-\nsumption of Prop. 2, that LGTL < K log K can be tested in\npractice by training graph transformation learning (GTL) and\nevaluating the predictive entropy on the training data. In all\nscenarios we worked with LGTL << K log K after training.\n3.3\nNewly Developed Baselines\nThe main contribution of our work is OCGTL. To study the\neffectiveness of OCGTL we implement the following graph-\nlevel AD methods as ablations. These methods have not been\nstudied on graphs before, so their implementation is also one\nof our contributions that paves the way for future progress.\nOne-class pooling (OCPool).\nAs a shallow method,\nOCPool uses pooling to construct a graph representation:\nhG = POOLING (xv | v \u2208G) .\n(8)\nThis feature extractor does not have parameters and hence\nrequires no training. Anomalies can be detected by training\nan one-class SVM (OCSVM) (Manevitz and Yousef, 2001)\non these features. This novel approach for graph-level AD\nis a simple baseline and achieves solid results in our empir-\nical study (even though it does not use the edge sets E of\nthe graphs). Another reason for studying OCPool, is that it\nhelps us understand which pooling function might work best\nas readout function (Eqn. (5)) for GNN-based AD methods.\nGraph transformation prediction (GTP).\nGTP is an end-\nto-end self-supervised detection method based on transforma-\ntion prediction. It trains a classi\ufb01er f to predict which trans-\nformation has been applied to a samples and uses the cross-\nentropy loss to score anomalies. We implement GTP with\nsix graph transformations (node dropping, edge adding, edge\ndropping, attribute masking, subgraph, and identity transfor-\nmation) originally designed in You et al. (2020).\nGraph transformation learning (GTL).\nGTL is an end-\nto-end self-supervised detection method using neural trans-\nformations (Qiu et al., 2021). K GNNs, fk for k = 1, \u00b7 \u00b7 \u00b7 , K\nin addition to the reference feature extractor f are trained on\nLGTL (Eqn. (2)). The loss is used directly to score anomalies.\nWhile this method works well in practice, it is not sensitive to\nthe norm of the graph embeddings in Eqn. (2). The normal-\nization step in computing the cosine similarity makes mean\nand add pooling equivalent when aggregating the graph rep-\nresentations. This may put GTL at a disadvantage compared\nto the other methods, which pro\ufb01t from add pooling.\n4\nExperiments\nThis section details our empirical study. We benchmark nine\nalgorithms on nine real-world graph classi\ufb01cation datasets\nfrom different domains using various evaluation measures.\nFirst, we describe the datasets and how the AD benchmark\nis set up. Second, we present all methods we compare, in-\ncluding baselines and their implementation details. Third, the\nevaluation results are presented and analyzed. In summary,\nOCGTL achieves the best performance on real-world datasets\nfrom various domains and raises the anomaly detection accu-\nracy signi\ufb01cantly (+11.8% in terms of AUC on average com-\npared to OCGIN of Zhao and Akoglu (2021)). Finally, we\npresent our \ufb01ndings about preferable design choices that are\nalso bene\ufb01cial for other deep methods in graph-level AD.\n4.1\nDatasets and Experimental Setting\nWe benchmark nine methods on nine graph classi\ufb01cation\ndatasets that are representative of three domains. In addition\nto \ufb01nancial and social networks security, health organizations\nneed an effective graph-level AD method to examine proteins\n(represented as graphs) to monitor the spread and evolution of\ndiseases. Targeting these application domains, we study three\nbioinformatics datasets: DD, PROTEINS, and ENZYMES,\nthree molecular datasets: NCI1, AIDS, and Mutagenicity, and\nthree datasets of social networks: IMDB-BINARY, REDDIT-\nBINARY, and REDDIT-MULTI-5K. The datasets are made\navailable by Morris et al. (2020), and the statistics of the\ndatasets are given in Appendix A.\nWe follow the standard setting of previous work to con-\nstruct an AD task from a classi\ufb01cation dataset (Ruff et al.,\n2018; Golan and El-Yaniv, 2018; Zhao and Akoglu, 2021).\nA classi\ufb01cation dataset with N classes produces N exper-\nimental variants. In each experimental variant, one of the\nclasses is treated as \u201cnormal\u201d; the other classes are consid-\nered as anomalies. The training set and validation set only\ncontain normal samples, while the test set contains a mix of\nnormal samples and anomalies that have to be detected dur-\ning test time. For each experimental variant, 10% of the nor-\nmal class is set aside for the test set, and 10% of each of the\nother classes is added to the test set as anomalies. (The re-\nsulting fraction of anomalies in the test set is proportional to\nthe class balance in the original dataset. The remaining 90%\nof the normal class is used for training and validation. We use\n10-fold cross-validation to estimate the model performance.\nIn each fold, 10% of the training set is held out for validation.\nWe train each model three times separately and average the\ntest results of three runs to get the \ufb01nal test results in each\nfold. Training multiple times ensures a fair comparison as it\nfavors methods that are robust to the random initialization.\nEvaluation.\nResults will be reported in terms of the area\nunder the ROC curve (AUC) (%), averaged over 10 folds\nwith standard deviation. We also report the results in terms\nof F1-score in Appendix C. In addition, all methods will be\nevaluated in terms of their susceptibility to performance \ufb02ip.\nZhao and Akoglu (2021) coined the term \u201cperformance\n\ufb02ip\u201d for AD benchmarks derived from binary classi\ufb01cation\ndatasets. We generalize their de\ufb01nition to multiple classes:\nDe\ufb01nition 1. (Performance \ufb02ip.) A model suffers from per-\nformance \ufb02ip on an anomaly detection benchmark derived\nfrom a classi\ufb01cation dataset if it performs worse than ran-\ndom on at least one experimental variant.\n4.2\nBaselines and Implementation Details\nMany deep AD approaches that have achieved impressive re-\nsults in other domains have not yet been adapted to graph-\nlevel AD. There has been no comprehensive study of various\nGNN-based graph-level AD approaches. An additional con-\ntribution of our work is that we adapt recent advances in deep\nAD to graphs. In our empirical study, we compare OCGTL\nboth to GNN-based methods and to non-GNN-based meth-\nods, which we outline below.\nGNN-based Baselines.\nOur study includes OCGTL, OC-\nGIN (Zhao and Akoglu, 2021) and the self-supervised ap-\nproaches graph transformation prediction (GTP) and graph\ntransformation learning (GTL) described in Sec. 3.3. We use\nGIN as the feature extractor for all GNN-based baselines to\ncompare with OCGIN fairly. In particular, we use 4 GIN lay-\ners, each of which includes a two-layer MLP and graph nor-\nmalization (Cai et al., 2020). The dimension of the node rep-\nresentations is 32. The readout function of almost all methods\nconsists of a two-layer MLP and then an add pooling layer. In\nGTP, the \ufb01nal prediction is obtained by summing the layer-\nwise predictions, and the readout function is composed of an\nadd pooing layer followed by a linear layer. In GTP, we em-\nploy six hand-crafted transformations. For a fair compari-\nson, GTL and OCGTL use six learnable graph transforma-\ntions in all experiments. Additional hyperparameter settings\nare recorded for reproducibility in Appendix B.\nImproved Implementation of OCGIN.\nWith these imple-\nmentation details for the GNNs we can signi\ufb01cantly improve\nthe performance of OCGIN over the implementation in Zhao\nand Akoglu (2021). For this reason, our empirical study in-\ncludes both OCGIN (the original version with mean pooling\nand batch normalization) and OCGIN\u2020 (our improved version\nwith add pooling and graph normalization).\nNon-GNN-based Baselines.\nBesides OCPool, we include\nfour two-stage detection methods proposed by Zhao and\nAkoglu (2021). Two of them use unsupervised graph embed-\nding methods, Graph2Vec (G2V) (Narayanan et al., 2017) or\nDD\nPROT\nENZY\nNCI1\nAIDS\nMutag\nRank\nBaselines\n(prev. work)\nWLK\n50.2\u00b10.3\u2217\n49.7\u00b10.5\u2217\n52.1\u00b12.0\u2217\n49.6\u00b10.4\u2217\n51.3\u00b10.8\u2217\n52.3\u00b10.6\n8.1\u00b11.7\nPK\n51.2\u00b12.3\u2217\n50.8\u00b11.5\u2217\n51.3\u00b11.2\u2217\n51.4\u00b11.7\u2217\n59.5\u00b12.3\u2217\n52.5\u00b11.6\n8.4\u00b11.1\nG2V\n49.5\u00b12.2\u2217\n53.2\u00b13.0\u2217\n52.0\u00b13.9\u2217\n50.5\u00b10.7\u2217\n48.4\u00b10.8\u2217\n50.2\u00b10.7\u2217\n8.9\u00b11.0\nFGSD\n66.0\u00b12.3\n58.5\u00b11.8\u2217\n52.7\u00b13.4\u2217\n55.4\u00b10.7\n91.6\u00b13.7\n51.3\u00b10.8\u2217\n5.3\u00b12.9\nOCGIN\n50.7\u00b11.2\u2217\n54.2\u00b11.2\u2217\n62.4\u00b12.7\n53.6\u00b11.3\u2217\n60.8\u00b12.2\u2217\n59.3\u00b11.4\n5.4\u00b11.7\nAblations\n(ours)\nOCGIN\u2020\n61.4\u00b11.6\n57.2\u00b12.3\n63.5\u00b13.9\n62.2\u00b11.3\n97.5\u00b12.0\n61.5\u00b11.8\u2217\n2.7\u00b10.9\nOCPool\n61.1\u00b13.3\u2217\n61.9\u00b12.1\n53.1\u00b12.9\u2217\n57.0\u00b11.3\n97.6\u00b11.4\n53.2\u00b10.5\u2217\n4.3\u00b12.2\nGTP\n54.2\u00b11.9\n61.9\u00b12.9\n55.0\u00b12.0\u2217\n55.3\u00b11.2\u2217\n77.2\u00b12.9\n54.7\u00b11.8\u2217\n4.8\u00b11.5\nGTL\n51.7\u00b10.9\u2217\n56.2\u00b12.5\u2217\n60.4\u00b11.6\n59.8\u00b11.0\u2217\n67.8\u00b13.3\u2217\n61.8\u00b11.0\n3.8\u00b11.7\nours\nOCGTL\n69.9\u00b12.6\n60.7\u00b12.4\n65.5\u00b13.8\n63.7\u00b11.2\n97.5\u00b12.0\n65.7\u00b12.1\n1.4\u00b10.7\n\u2020 OCGIN is the original implementation from Zhao and Akoglu (2021), while OCGIN\u2020 denotes our improved implementation with the\nsame GIN architecture choices (add pooling etc.) as OCGTL, GTP, and GTL.\nTable 1: Average AUCs (%) with standard deviations of 9 methods on 6 of 9 datasets. (For the remaining 3 datasets, see Tab. 3.)\nThe performance rank averaged on all nine datasets is provided in the last column. Results marked \u2217perform worse than random\non at least one experimental variant (performance \ufb02ip). OCGTL outperforms the other methods and has no performance \ufb02ip.\nDD\nPROT\nNCI1\nAIDS\nOutlier class\n0\n1\n0\n1\n0\n1\n0\n1\nOCGIN\n26.3\u00b12.7\n75.2\u00b13.4\n42.5\u00b14.4\n65.9\u00b14.5\n64.4\u00b12.5\n42.9\u00b13.0\n26.0\u00b13.9\n95.5\u00b12.2\nOCGTL (ours)\n66.8\u00b14.6\n73.0\u00b12.9\n63.2\u00b15.4\n58.1\u00b16.1\n71.2\u00b13.0\n56.2\u00b12.5\n99.3\u00b10.9\n95.7\u00b13.6\nTable 2: Average AUCs (%) with standard deviations of OCGIN (Zhao and Akoglu, 2021) and OCGTL (ours) on both experi-\nmental variants of four datasets, where the performance \ufb02ip is observed. The results that are worse than random are marked in\nred. OCGIN suffers from performance \ufb02ip, while OCGTL not.\nIMDB-B\nRDT-B\nRDT-M\nBaselines\n(prev. work)\nWLK\n62.0\u00b12.0\n50.2\u00b10.2\u2217\n50.3\u00b10.1\u2217\nPK\n53.5\u00b12.0\n50.0\n50.0\nG2V\n54.3\u00b11.6\n51.5\u00b10.6\u2217\n50.0\u00b10.2\u2217\nFGSD\n57.1\u00b11.8\n-\n-\nOCGIN\n60.4\u00b12.8\n67.1\u00b13.5\n62.4\u00b11.3\nAblations\n(ours)\nOCGIN\u2020\n63.7\u00b12.4\n74.5\u00b13.4\n70.4\u00b11.5\nOCPool\n56.5\u00b10.8\n65.3\u00b12.2\n62.4\u00b10.9\nGTP\n57.6\u00b11.1\n64.4\u00b12.2\n62.4\u00b11.3\nGTL\n65.2\u00b11.9\n71.6\u00b12.3\n67.8\u00b10.9\nours\nOCGTL\n65.1\u00b11.8\n77.4\u00b11.9\n71.5\u00b11.1\n\u2020 OCGIN is the original implementation from Zhao and Akoglu\n(2021), while OCGIN\u2020 denotes our improved version.\nTable 3: Average AUCs (%) with standard deviations of nine\nmethods on three datasets to complement Tab. 1.\nFGSD (Verma and Zhang, 2017), to extract graph-level rep-\nresentations. The other two of them make use of graph ker-\nnels (Weisfeiler-Leman subtree kernel (WLK) (Shervashidze\net al., 2011) or propagation kernel (PK) (Neumann et al.,\n2016)), which measure the similarity between graphs. For\nall two-stage detection baselines, we use OCSVM (with \u03bd =\n0.1) as the downstream outlier detector.\nThe number of iterations speci\ufb01es how far neighborhood\ninformation can be propagated. By setting the number of iter-\nations to 4, we get a fair comparison to the GNN-based meth-\nods, which all have 4 GNN layers. All other hyperparameters\ncorrespond to the choices in Zhao and Akoglu (2021).\n4.3\nExperimental Results\nSummary.\nWe compare OCGTL with all existing baselines\non nine real-world datasets. The detection results in terms\nof average AUC (%) with standard deviation are reported in\nTabs. 1 and 3. The results in terms of F1-score are reported\nin Appendix C. We can see that OCGTL achieves competi-\ntive results on all datasets and has the best average rank of\n1.4. On average over nine datasets, OCGTL outperforms OC-\nGIN of Zhao and Akoglu (2021) by 11.8%. We can conclude\nthat OCGTL raises the detection accuracy in graph-level AD\non various application domains signi\ufb01cantly, namely by 9.6%\non the bioinformatics domain, by 17.7% on the molecular do-\nmain, and by 8% on the social-networks domain.\nMoreover, methods with performance \ufb02ip are marked with\na \u2217in Tab. 1. In Tab. 2 we report the results of OCGIN (Zhao\nand Akoglu, 2021) and our method OCGTL on both experi-\nmental variants of datasets where the performance \ufb02ip is ob-\nserved. We can see that all existing baselines suffer from the\nperformance \ufb02ip issue, while OCGTL is the only model with-\nout performance \ufb02ip on any of the datasets.\nAblation Study of Methods.\nHere we discuss the results in\nTab. 1 from the perspective of an ablation study to understand\nif and how the advantages of combing deep OCC and neural\ntransformation learning complement each other. From results\nin Tab. 1, we can see that OCGTL improves over OCGIN\u2020\n(with our improved implementation) on 8 of 9 datasets by\nadding LGTL as the regularization term and outperforms GTL\non 8 of 9 datasets by utilizing the Euclidean distance for de-\ntection. We can conclude, that the two terms in the loss func-\ntion of OCGTL complement each other and offer two metrics\nfor detecting anomalies. As a result, OCGTL consistently\noutperforms OCGIN and GTL. This is aligned with our theo-\nretical results in Sec. 3.2.\nGTP applies hand-crafted graph transformations. Its per-\nformance varies across datasets since it is sensitive to the\nchoice of transformations. Even though it works well on the\nDD\nPROT\nENZY\nNCI1\nAIDS\nMutag\nIMDB-B\nRDT-B\nRDT-M\n50\n75\n100\nAUC (%)\nAdd Pooling\nMean Pooling\nMax Pooling\nFigure 3: OCPool with add pooling (blue) outperforms alter-\nnative choices (mean (orange), max (green)). The results on\nnine datasets are reported in terms of AUC (%).\nPROTEINS dataset with the chosen graph transformations, its\nperformance on other datasets is not competitive to OCGTL.\nFinding the right transformations for each dataset requires do-\nmain knowledge, which is not the focus of this work. In com-\nparison, OCGTL learns data-speci\ufb01c transformations auto-\nmatically and performs consistently well on various datasets.\nStudy of Design Choices.\nTo raise the bar in deep AD\non graphs, we have to understand the impact of the de-\nsign choices associated with the GNN architecture.\nHere\nwe discuss the type of pooling layer for the readout function\n(Eqn. (5)) and normalization of the GNN layers.\nFirst, we study the impact of different pooling layers.\nOCPool is the ideal testbed to compare add pooling, mean\npooling, and max pooling due to its simplicity. Results of\nrunning the three options on nine datasets are reported in\nFig. 3. Add pooling outperforms the other options. This may\nresult from add pooling injecting information about the num-\nber of nodes into the graph representations. OCPool with add\npooling is a simple yet effective method for AD. It provides\na simple heuristic for aggregating node attributes into graph\nrepresentations. As shown in Tab. 1, it does well on many\ndatasets (particularly on the PROTEINS and AIDS datasets),\neven though it does not account for graph structure (edges).\nSecond, to study the combined impact of the pooling layer\nand normalization layers on the deep GNN-based methods,\nwe compare add pooling with graph normalization (AP + GN)\nand mean pooling with batch normalization (MP + BN). In\nFig. 4, we visualize in a scatter plot the performance of all\nGNN-based methods on all nine datasets, contrasting the AP\n+ GN result with the MP + BN result in terms of average\nAUCs (%) with standard deviations. Almost all points fall\nabove the diagonal, meaning that AP + GN is preferable to\nMP + BN, which has been the design choice of Zhao and\nAkoglu (2021) for OCGIN. With the new design choices, we\nare able to signi\ufb01cantly raise the bar in graph-level AD.\n5\nConclusion\nWe develop a novel end-to-end graph-level AD method,\nOCGTL, that combines the best of deep OCC and neural\ntransformation learning. OCGTL mitigates the shortcomings\nof deep OCC by using graph transformation learning as regu-\nlarization and complements graph transformation learning by\nintroducing a sensitivity to the norm of the graph representa-\n50\n60\n70\n80\n90\n100\nAUC (%) from models with MP and BN\n50\n60\n70\n80\n90\n100\nAUC (%) from models with AP and GN\nDD\nMutag\nPROT\nIMDB-B\nENZY\nRDT-B\nNCI1\nRDT-M\nAIDS\nOCGIN\nGTP\nGTL\nOCGTL\nFigure 4: A comparison of two design choices in deep GNN-\nbased methods, namely, add pooling with graph normaliza-\ntion (AP + GN, y-axis) and mean pooling with batch normal-\nization (MP + BN, x-axis). Each point compares the detection\nresults of the two variants of one model on one dataset. Most\npoints falling above the diagonal indicate that AP + GN is the\npreferred design choice for GNN-based AD methods.\ntions. Our comprehensive empirical study supports our claim\nand theoretical results. It shows that OCGTL performs best\nin various challenging domains and is the only method not\nstruggling with performance \ufb02ip.\nAcknowledgements\nThe Bosch Group is carbon neutral. Administration, man-\nufacturing and research activities do no longer leave a car-\nbon footprint.\nThis also includes GPU clusters on which\nthe experiments have been performed.\nMK acknowledges\nthe DFG awards KL 2698/2-1 & KL 2698/5-1 and the\nBMBF awards 01|S18051A, 03|B0770E, and 01|S21010C.\nSM acknowledges support by DARPA under Contract No.\nHR001120C0021, NSF under grants 2047418, 1928718,\n2003237 and 2007719; DOE under grant DE-SC0022331,\nand gifts from Intel, Disney, and Qualcomm. Any opinions,\n\ufb01ndings and conclusions or recommendations are those of the\nauthors and do not necessarily re\ufb02ect the views of DARPA.\nReferences\nLeman Akoglu, Hanghang Tong, and Danai Koutra. Graph\nbased anomaly detection and description: a survey. Data\nmining and knowledge discovery, 29(3):626\u2013688, 2015.\nTianle Cai, Shengjie Luo, Keyulu Xu, Di He, Tie-yan Liu,\nand Liwei Wang. Graphnorm: A principled approach to\naccelerating graph neural network training. arXiv preprint\narXiv:2009.03294, 2020.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Ge-\noffrey Hinton. A simple framework for contrastive learn-\ning of visual representations. In ICML, pages 1597\u20131607.\nPMLR, 2020.\nLucas Deecke, Robert Vandermeulen, Lukas Ruff, Stephan\nMandt, and Marius Kloft. Image anomaly detection with\ngenerative adversarial networks.\nIn ECML and KDD,\npages 3\u201317. Springer, 2018.\nKaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu.\nDeep anomaly detection on attributed networks. In Pro-\nceedings of the 2019 SIAM International Conference on\nData Mining, pages 594\u2013602. SIAM, 2019.\nKaize Ding, Jundong Li, Nitin Agarwal, and Huan Liu. In-\nductive anomaly detection on attributed networks. In IJ-\nCAI, pages 1288\u20131294, 2020.\nFederico Errica, Marco Podda, Davide Bacciu, and Alessio\nMicheli. A fair comparison of graph neural networks for\ngraph classi\ufb01cation. In ICLR, 2020.\nIzhak Golan and Ran El-Yaniv. Deep anomaly detection us-\ning geometric transformations. In Neural Information Pro-\ncessing Systems, pages 9781\u20139791, 2018.\nMartin Jullum, Anders L\u00f8land, Ragnar Bang Huseby, Geir\n\u02daAnonsen, and Johannes Lorentzen. Detecting money laun-\ndering transactions with machine learning.\nJournal of\nMoney Laundering Control, 2020.\nThomas N Kipf and Max Welling. Semi-supervised classi\ufb01-\ncation with graph convolutional networks. arXiv preprint\narXiv:1609.02907, 2016.\nLarry M Manevitz and Malik Yousef. One-class svms for\ndocument classi\ufb01cation. Journal of machine Learning re-\nsearch, 2(Dec):139\u2013154, 2001.\nChristopher Morris, Nils M. Kriege, Franka Bause, Kris-\ntian Kersting, Petra Mutzel, and Marion Neumann. Tu-\ndataset: A collection of benchmark datasets for learning\nwith graphs. In ICML GRL+ Workshop, 2020.\nAnnamalai Narayanan, Mahinthan Chandramohan, Rajasekar\nVenkatesan, Lihui Chen, Yang Liu, and Shantanu Jaiswal.\ngraph2vec: Learning distributed representations of graphs.\narXiv preprint arXiv:1707.05005, 2017.\nMarion Neumann, Roman Garnett, Christian Bauckhage, and\nKristian Kersting.\nPropagation kernels: ef\ufb01cient graph\nkernels from propagated information. Machine Learning,\n102(2):209\u2013245, 2016.\nZineb Noumir, Paul Honeine, and Cedue Richard. On sim-\nple one-class classi\ufb01cation methods. In 2012 IEEE Inter-\nnational Symposium on Information Theory Proceedings,\npages 2022\u20132026. IEEE, 2012.\nChen Qiu, Timo Pfrommer, Marius Kloft, Stephan Mandt,\nand Maja Rudolph.\nNeural transformation learning for\ndeep anomaly detection beyond images. In ICML, pages\n8703\u20138714. PMLR, 2021.\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas\nDeecke, Shoaib Ahmed Siddiqui, Alexander Binder, Em-\nmanuel M\u00a8uller, and Marius Kloft. Deep one-class classi\ufb01-\ncation. In ICML, pages 4393\u20134402. PMLR, 2018.\nLukas Ruff, Jacob R Kauffmann, Robert A Vandermeulen,\nGr\u00b4egoire Montavon, Wojciech Samek, Marius Kloft,\nThomas G Dietterich, and Klaus-Robert M\u00a8uller. A uni-\nfying review of deep and shallow anomaly detection. Pro-\nceedings of the IEEE, 2021.\nNino\nShervashidze,\nPascal\nSchweitzer,\nErik\nJan\nVan Leeuwen, Kurt Mehlhorn, and Karsten M Borg-\nwardt.\nWeisfeiler-lehman graph kernels.\nJournal of\nMachine Learning Research, 12(9), 2011.\nKihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and\nTomas P\ufb01ster. Learning and evaluating representations for\ndeep occ. In ICLR, 2020.\nSaurabh Verma and Zhi-Li Zhang. Hunt for the unique, sta-\nble, sparse and fast feature learning on graphs. In Neural\nInformation Processing Systems, pages 87\u201397, 2017.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural networks? In ICLR, 2018.\nKeyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe,\nKen-ichi Kawarabayashi, and Stefanie Jegelka. Represen-\ntation learning on graphs with jumping knowledge net-\nworks. In ICML, pages 5453\u20135462. PMLR, 2018.\nMinji Yoon, Bryan Hooi, Kijung Shin, and Christos Falout-\nsos.\nFast and accurate anomaly detection in dynamic\ngraphs with a two-pronged approach. In Knowledge Dis-\ncovery & Data Mining, pages 647\u2013657, 2019.\nYuning You, Tianlong Chen, Yongduo Sui, Ting Chen,\nZhangyang Wang, and Yang Shen. Graph contrastive learn-\ning with augmentations. Neural Information Processing\nSystems, 33:5812\u20135823, 2020.\nLily Zhang, Mark Goldstein, and Rajesh Ranganath. Under-\nstanding failures in out-of-distribution detection with deep\ngenerative models. In ICML, pages 12427\u201312436. PMLR,\n2021.\nLingxiao Zhao and Leman Akoglu. On using classi\ufb01cation\ndatasets to evaluate graph outlier detection: Peculiar ob-\nservations and new insights. Big Data, 2021.\nLi Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao.\nAddgraph: Anomaly detection in dynamic graph using\nattention-based temporal gcn. In IJCAI, pages 4419\u20134425,\n2019.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cris-\ntian Lumezanu, Daeki Cho, and Haifeng Chen.\nDeep\nautoencoding gaussian mixture model for unsupervised\nanomaly detection. In ICLR, 2018.\nAppendix\nA\nDatasets Statistics\nWe select nine graph classi\ufb01cation datasets for the evaluation from three domains (bioinformatics, molecules, and social net-\nworks). For each dataset, we report the number of graphs, the dimension of node attributes, the average number of nodes, and\nthe average number of edges in each class as the statistics. We list the statistics of these nine datasets in Tab. 4.\nTable 4: The statistics of used datasets. We report the number of graphs, the dimension of node attributes, the average number\nof nodes, and the average number of edges for each class in each dataset.\nDataset\nCategory\nClass\n#Graphs\n#NodeAttrs\nAvg.#Nodes\nAvg.#Edges\nDD\nBioinformatics\n0\n691\n89\n355.2\n1806.6\n1\n487\n89\n183.7\n898.9\nPRTOEINS\nBioinformatics\n0\n663\n3\n50.0\n188.1\n1\n450\n3\n22.9\n83.0\nENZYMES\nBioinformatics\n0\n100\n3\n36.2\n132.7\n1\n100\n3\n29.9\n113.8\n2\n100\n3\n28.9\n111.2\n3\n100\n3\n38.2\n148.8\n4\n100\n3\n31.4\n119.6\n5\n100\n3\n31.2\n119.6\nNCI1\nMolecules\n0\n2053\n37\n25.7\n55.3\n1\n2057\n37\n34.1\n73.9\nAIDS\nMolecules\n0\n400\n38\n37.6\n80.5\n1\n1600\n38\n10.2\n20.4\nMutagenicity\nMolecules\n0\n2401\n14\n29.4\n60.6\n1\n1936\n14\n31.5\n62.7\nIMDB-B\nSocial networks\n0\n500\n136\n20.1\n193.6\n1\n500\n136\n19.4\n192.6\nREDDIT-B\nSocial networks\n0\n1000\n1\n641.3\n1471.9\n1\n1000\n1\n218.0\n519.1\nREDDIT-M\nSocial networks\n0\n1000\n1\n799.5\n2035.5\n1\n1000\n1\n852.1\n1940.4\n2\n1000\n1\n374.1\n856.5\n3\n1000\n1\n249.6\n534.0\n4\n1000\n1\n267.0\n581.7\n\u22c6In IMDB-B, the one-hot degree is used as node attributes. In REDDIT-B and REDDIT-M, the constant one is used as node attributes.\nB\nAdditional Implementation Details\nTraining hyperparameters: We use the Adam optimizer with an initial learning rate of 0.001 and decay the learning rate\nby 0.5 every 100 epochs. We set the maximum epochs as 500 and the batch size as 128. We use the early stopping based\non validation loss (without access to the true labeled anomalies) for the training. The early stopping is implemented with a\npatience parameter of 100 epochs to ease the sensitivity to \ufb02uctuations in the validation loss. An early stopping without access\nto the true anomalies is critical for an unbiased model evaluation in the AD tasks.\nHardware and Software: All models are trained in our GPU cluster, which consists of NVIDIA GeForce GTX TITAN\nX GPUs, and NVIDIA TITAN X Pascal GPUs. All GNN-based models have been implemented by means of the Pytorch\nGeometrics library. The implementations of G2V and FGSD are from Karate Club library, while the implementations of WLK\nand PK are from GraKel library.\nC\nAdditional Experimental Results\nHere we provide additional experimental results, which include two additional comparisons of design choices in Fig. 5 and the\nevaluation results of nine methods on nine datasets in terms of F1-score in Tab. 5.\nTable 5: Average F1-scores with standard deviations of 9 methods on 9 datasets. OCGTL performs best generally.\nDatasets\nDD\nPROT\nENZY\nNCI1\nAIDS\nMutag\nIMDB-B\nRDT-B\nRDT-M\nWLK\n0.56\u00b10.016\n0.58\u00b10.036\n0.84\u00b10.004\n0.49\u00b10.008\n0.48\u00b10.008\n0.52\u00b10.01\n0.57\u00b10.032\n0.5\u00b10.007\n0.8\u00b10.001\nPK\n0.54\u00b10.019\n0.57\u00b10.025\n0.83\u00b10.002\n0.51\u00b10.017\n0.49\u00b10.016\n0.52\u00b10.014\n0.52\u00b10.018\n0\n0\nG2V\n0.48\u00b10.026\n0.5\u00b10.036\n0.84\u00b10.01\n0.5\u00b10.007\n0.76\u00b10.032\n0.5\u00b10.006\n0.53\u00b10.024\n0.5\u00b10.006\n0.8\u00b10.001\nFGSD\n0.63\u00b10.019\n0.59\u00b10.025\n0.83\u00b10.01\n0.54\u00b10.009\n0.95\u00b10.021\n0.52\u00b10.009\n0.56\u00b10.018\n-\n-\nOCGIN\n0.55\u00b10.022\n0.55\u00b10.021\n0.86\u00b10.007\n0.52\u00b10.013\n0.49\u00b10.01\n0.57\u00b10.007\n0.57\u00b10.025\n0.63\u00b10.033\n0.82\u00b10.003\nOCGIN\u2020\n0.59\u00b10.018\n0.56\u00b10.029\n0.86\u00b10.011\n0.59\u00b10.013\n0.97\u00b10.012\n0.60\u00b10.017\n0.60\u00b10.02\n0.68\u00b10.031\n0.84\u00b10.004\nOCPool\n0.59\u00b10.028\n0.61\u00b10.024\n0.84\u00b10.008\n0.55\u00b10.009\n0.97\u00b10.017\n0.52\u00b10.009\n0.53\u00b10.012\n0.66\u00b10.023\n0.82\u00b10.002\nGTP\n0.52\u00b10.014\n0.59\u00b10.028\n0.84\u00b10.005\n0.54\u00b10.011\n0.63\u00b10.033\n0.54\u00b10.018\n0.55\u00b10.013\n0.61\u00b10.016\n0.82\u00b10.003\nGTL\n0.57\u00b10.020\n0.60\u00b10.036\n0.86\u00b10.007\n0.57\u00b10.009\n0.53\u00b10.024\n0.60\u00b10.007\n0.60\u00b10.024\n0.66\u00b10.02\n0.84\u00b10.003\nOCGTL\n0.66\u00b10.025\n0.60\u00b10.025\n0.87\u00b10.012\n0.60\u00b10.011\n0.97\u00b10.011\n0.63\u00b10.02\n0.62\u00b10.019\n0.7\u00b10.02\n0.85\u00b10.004\n\u2020 OCGIN is the original implementation from Zhao and Akoglu (2021), while OCGIN\u2020 denotes our improved implementation.\n50\n60\n70\n80\n90\n100\nAUC (%) from models with AP and BN\n50\n60\n70\n80\n90\n100\nAUC (%) from models with AP and GN\n(a) AP+GN v.s. AP+BN\n50\n60\n70\n80\n90\n100\nAUC (%) from models with MP and GN\n50\n60\n70\n80\n90\n100\nAUC (%) from models with AP and GN\n(b) AP+GN v.s. MP+GN\nDD\nMutag\nPROT\nIMDB-B\nENZY\nRDT-B\nNCI1\nRDT-M\nAIDS\nOCGIN\nGTP\nGTL\nOCGTL\nFigure 5: A comparison of design choices in deep GNN-based methods. (a) Results of AP+BN on x-axis against AP+GN on\ny-axis. (b) Results of MP+GN on x-axis against AP+GN on y-axis. In conclusion, add pooling with graph normalization is\npreferable.\n",
    "2402.16024": "HiGPT: Heterogeneous Graph Language Model\nJiabin Tang1, Yuhao Yang2, Wei Wei2, Lei Shi3,\nLong Xia3, Dawei Yin3 and Chao Huang1,2\u2217\n1Musketeers Foundation Institute of Data Science,\n2Department of Computer Science, University of Hong Kong, 3Baidu Inc.\nProject Page: https://HiGPT-HKU.github.io, Github: https://github.com/HKUDS/HiGPT\nABSTRACT\nHeterogeneous graph learning aims to capture complex relation-\nships and diverse relational semantics among entities in a hetero-\ngeneous graph to obtain meaningful representations for nodes and\nedges. Recent advancements in heterogeneous graph neural net-\nworks (HGNNs) have achieved state-of-the-art performance by\nconsidering relation heterogeneity and using specialized message\nfunctions and aggregation rules. However, existing frameworks\nfor heterogeneous graph learning have limitations in generalizing\nacross diverse heterogeneous graph datasets. Most of these frame-\nworks follow the \"pre-train\" and \"fine-tune\" paradigm on the same\ndataset, which restricts their capacity to adapt to new and unseen\ndata. This raises the question: \u201cCan we generalize heterogeneous\ngraph models to be well-adapted to diverse downstream learning\ntasks with distribution shifts in both node token sets and relation\ntype heterogeneity?\u201d To tackle those challenges, we propose HiGPT,\na general large graph model with Heterogeneous graph instruction-\ntuning paradigm. Our framework enables learning from arbitrary\nheterogeneous graphs without the need for any fine-tuning pro-\ncess from downstream datasets. To handle distribution shifts in\nheterogeneity, we introduce an in-context heterogeneous graph\ntokenizer that captures semantic relationships in different heteroge-\nneous graphs, facilitating model adaptation. We incorporate a large\ncorpus of heterogeneity-aware graph instructions into our HiGPT,\nenabling the model to effectively comprehend complex relation het-\nerogeneity and distinguish between various types of graph tokens.\nFurthermore, we introduce the Mixture-of-Thought (MoT) instruc-\ntion augmentation paradigm to mitigate data scarcity by generating\ndiverse and informative instructions. Through comprehensive eval-\nuations conducted in various settings, our proposed framework\ndemonstrates exceptional performance in terms of generalization\nperformance, surpassing current leading benchmarks.\nACM Reference Format:\nJiabin Tang1, Yuhao Yang2, Wei Wei2, Lei Shi3,, Long Xia3, Dawei Yin3 and\nChao Huang1,2\u2217. 2024. HiGPT: Heterogeneous Graph Language Model. In\nProceedings of ACM Conference (Conference\u201917). ACM, New York, NY, USA,\n19 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn\n1\nINTRODUCTION\nHeterogeneous graphs have garnered extensive popularity and\nadoption in various domains, including recommendation systems [6],\nknowledge graphs [29], social network analysis [5], and biological\nnetworks [17]. These graphs encompass entities of diverse types\n\u2217Chao Huang is the Corresponding Author.\nConference\u201917, July 2017, Washington, DC, USA\n2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM...$15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\nthat engage in a multitude of interactions, enabling a comprehen-\nsive representation of complex systems [4]. The focus of hetero-\ngeneous graph learning is to derive meaningful representations\nfor the nodes and edges within such graphs [12, 16, 45]. These\nrepresentations aim to capture the intricate relationships and di-\nverse relational semantics that exist within the graph, facilitating a\ndeeper understanding of the underlying structural heterogeneity.\nIn recent years, there has been a growing recognition of the sig-\nnificant potential of heterogeneous graph neural networks (HGNNs)\nin capturing the intricate and diverse information that resides\nwithin heterogeneous graph structures [31, 40]. HGNNs leverage\nthe expressive capabilities of high-order message passing tech-\nniques, enabling them to effectively model the complex relation-\nships, diverse entity types, and heterogeneous semantics present in\nthese graphs. By aggregating and propagating information across\nvarious node and edge types, HGNNs facilitate a deeper under-\nstanding and analysis of the intricate inter-dependencies that exist\nwithin heterogeneous graph structures. Some notable examples\nof HGNNs include: i) Metapath-based GNNs such as HAN [32]\nand MAGNN [6]; ii) Transformer-enhanced GNNs like HGT [10].\nFurthermore, heterogeneous graph self-supervised learning, in-\ncluding contrastive methods (e.g., DMGI [19] and HeCo [34]), and\ngenerative (e.g., HGMAE [27]) methods, showcase effectiveness in\nalleviating data scarcity in real-world heterogeneous graph data.\nDespite the demonstrated effectiveness of current frameworks\nfor heterogeneous graph learning, they possess limitations when it\ncomes to generalizing across diverse heterogeneous graph datasets.\nThese frameworks commonly adopt the \"pre-train\" and \"fine-tune\"\nparadigm, where they are initially trained on a specific dataset and\nsubsequently fine-tuned on the same dataset [10, 27, 34]. However,\nthis approach presents challenges in adapting and achieving optimal\nperformance on new and unseen data. The heavy reliance on the\ncharacteristics and patterns of the original training dataset hinders\ntheir ability to effectively handle the intricacies and complexities in-\nherent in different heterogeneous graph datasets. As a result, these\nframeworks may encounter difficulties in effectively addressing the\ndiverse nuances and variations present in various heterogeneous\ngraph datasets, particularly when applied to downstream tasks.\nThis research aims to push the boundaries of heterogeneous\ngraph models by addressing a fundamental question: \"Can we de-\nvelop highly adaptable and versatile heterogeneous graph models\ncapable of effectively addressing diverse downstream learning tasks,\neven when faced with distribution shifts in node token sets and\nrelation type heterogeneity?\" To tackle this challenge, we introduce\nHiGPT as a novel and general solution. Our model is specifically de-\nsigned to overcome key challenges associated with generalization\nacross various downstream heterogeneous graph learning tasks.\narXiv:2402.16024v2  [cs.CL]  19 May 2024\nC1. Relation Type Heterogeneity Shift. One of the primary chal-\nlenges we focus on in this research is the shift in relation type het-\nerogeneity within various heterogeneous graph structures. In such\ngraphs, entities are connected by various types of relations, and\nthese relation types can differ significantly across diverse datasets.\nTo illustrate this, let\u2019s consider two examples. In a recommendation\nsystem, a heterogeneous graph may involve node-wise relationships\nbetween users and items. The relation types in this scenario could in-\nclude \"click,\" \"favorite,\" \"review,\" and \"purchase.\" On the other hand,\nin an academic graph, the relations could involve \"paper-paper,\"\n\"author-paper,\" and \"paper-venue.\" These examples demonstrate\nhow different heterogeneous graphs can exhibit diverse relation\nheterogeneity with distinct semantics across domains.\nSolution: In-Context Heterogeneous Graph Tokenizer. To\nachieve adaptability in a wide range of heterogeneous graph sce-\nnarios with varying node and edge types, we introduce the in-\ncontext heterogeneous graph tokenizer. This tokenizer captures the\ndiverse semantic relationships found in different heterogeneous\ngraphs, providing a unified approach. It comprises two essential\ncomponents: the in-context parameterized heterogeneity projector,\nwhich utilizes language to encode distinct node and edge types,\nfacilitating effective model adaptation, and the parameter allocator,\nwhich dynamically assigns tailored parameters to the tokenizer. To\noptimize performance and integrate the tokenizer seamlessly into\nthe HiGPT framework, we employ pre-training with a lightweight\ntext-graph contrastive alignment paradigm. This pre-training pro-\ncess directly incorporates the tokenizer into HiGPT, enhancing its\ncapabilities and ensuring smooth functionality within the overall\nmodel architecture, including integration with the language model.\nC2. Complex Heterogeneous Graph Structures. The primary\nfocus of this study is to tackle the challenge of integrating large\nlanguage models (LLMs) into heterogeneous graph learning, with\nthe goal of enhancing model generalization. Our specific objective\nis to develop a graph-oriented language model that excels in com-\nprehending the intricate structural information inherent in complex\nheterogeneous graph structures. In doing so, we strive to equip the\ngraph model with the ability to not only recognize the heterogene-\nity of relations among different types of nodes, but also capture the\ndistinct characteristics of entities belonging to the same type.\nSolution: Heterogeneous Graph Instruction-Tuning. We intro-\nduce a novel heterogeneous graph instruction-tuning framework\nthat integrates inter-type and intra-type token matching tasks to\nfine-tune large language models (LLMs). Our framework specifically\ntargets the enhancement of LLMs\u2019 understanding of both hetero-\ngeneous relation awareness and homogeneous relation awareness.\nBy utilizing these tasks, our aim is to bolster the LLMs\u2019 capabilities\nin the following areas: (i) distinguishing between different types\nof graph tokens, (ii) comprehending intricate relationships within\nheterogeneous graphs, (iii) preserving the distinctive attributes of\nentities within homogeneous graphs, and (iv) effectively harnessing\ndiverse graph instructions during the training process.\nC3. Data Scarcity for Model Fine-Tuning. In practical scenarios\ninvolving heterogeneous graph learning, one of the key challenges\nis the limited availability of data [11, 41]. This scarcity of data poses\na significant obstacle when fine-tuning models for real-world ap-\nplications. For instance, when utilizing heterogeneous graphs to\nmodel cold-start users or items in recommendation systems, the\nsparse nature of user interaction data restricts the availability of\nsupervised signals [36, 39]. This data scarcity hampers the effec-\ntiveness of task-specific model fine-tuning and necessitates the\ndevelopment of novel techniques to overcome this challenge.\nSolution: Mixture-of-Thought Augmentation. Our approach\nintroduces a novel mechanism for augmenting graph instructions,\nemphasizing the use of Mixture-of-Thought (MoT) combined with\nvarious prompting techniques. This integration enables us to gen-\nerate a diverse and comprehensive set of informative task-specific\ninstructions. By seamlessly incorporating these augmented graph\ninstructions into our framework, we anticipate that our model en-\nhancement will effectively address the challenge of data sparsity.\nExperiments. To assess the efficacy of our proposed approach,\nwe perform a comprehensive set of experiments to showcase the\nimpressive generalization capabilities of our heterogeneous graph\nlanguage model across diverse settings. We thoroughly investigate\nthe design rationales, effectiveness, and efficiency of our model.\n2\nPRELIMINARIES\nHeterogeneous Graph. A heterogeneous graph is a graph denoted\nas G(V, E, A, T, R, X). It consists of nodes represented by V, edges\nrepresented by E, and an adjacency matrix A that captures the\nrelationships between nodes. The sets T and R signify the types\nof nodes and edges, respectively. Additionally, the feature matrix\nX = {\ud835\udc4b\ud835\udc47\ud835\udc56\u2208R|V\ud835\udc47\ud835\udc56|\u00d7\ud835\udc51\ud835\udc47\ud835\udc56} contains attributes associated with each\nnode. Here, \ud835\udc47\ud835\udc56refers to a specific node type, while \ud835\udc51\ud835\udc47\ud835\udc56represents\nthe dimensionality of the corresponding node attributes.\nMeta Relation. In a heterogeneous graph, a meta relation is a\nrepresentation of the relationship between different types of nodes\nconnected by an edge. Specifically, for an edge \ud835\udc52that links a source\nnode \ud835\udc62of type \ud835\udc47\ud835\udc56to a target node \ud835\udc63of type \ud835\udc47\ud835\udc57, the meta relation\nof \ud835\udc52= (\ud835\udc62, \ud835\udc63) is denoted as < \ud835\udf0f(\ud835\udc62), \ud835\udf0c(\ud835\udc52),\ud835\udf0f(\ud835\udc63) >. Here, \ud835\udf0f(\ud835\udc62) and \ud835\udf0f(\ud835\udc63)\nrepresent the node types of \ud835\udc62and \ud835\udc63respectively, drawn from the\nset of node types T, while \ud835\udf0c(\ud835\udc52) denotes the relation type of the\nedge \ud835\udc52, which is selected from the set of relation types R. This meta\nrelation provides a concise representation of the heterogeneous\nconnections in the graph, capturing the types of nodes involved.\nHeterogeneous Graph Neural Networks (HGNNs). In the con-\ntext of a heterogeneous graph G, Heterogeneous Graph Neural\nNetworks (HGNNs) employ message passing and aggregation tech-\nniques to incorporate neighbor information based on different node\nand edge categories. This enables the modeling of heterogeneous\nstructural semantic relationships, as expressed below:\n\u210e(\ud835\udc59)\n\ud835\udc63\n=\nAggregate\n\u2200\ud835\udc62\u2208N(\ud835\udc63),\u2200\ud835\udc52\u2208E(\ud835\udc62,\ud835\udc63)\n\u0010\nPropagate\n\u0010\n\u210e(\ud835\udc59\u22121)\n\ud835\udc62\n;\u210e(\ud835\udc59\u22121)\n\ud835\udc63\n,\ud835\udc52\n\u0011\u0011\n(1)\nHere, N (\ud835\udc63) represents all the source nodes connected to node\n\ud835\udc63, and E(\ud835\udc62, \ud835\udc63) denotes the set of edges connecting node \ud835\udc62and\nnode \ud835\udc63. In most HGNNs, the parameters of the Propagate (\u00b7) and\nAggregate (\u00b7) functions depend on the types of nodes \ud835\udc62and \ud835\udc63, as\nwell as the edge \ud835\udc52. However, this implies that HGNNs are limited\nto modeling the specific heterogeneous graph they were trained\non and cannot be effectively applied to new heterogeneous graphs\nwith different node and edge types. This limitation greatly ham-\npers the generalization ability of HGNNs in capturing the diverse\nsemantic relationships across various heterogeneous graphs.\n3\nMETHODOLOGY\n3.1\nIn-Context Heterogeneous Graph Tokenizer\nTo make our HiGPT adaptable to a wide range of heterogeneous\ngraph scenarios with varying node and edge types, we propose an\nin-context heterogeneous graph tokenizer. This method captures\nthe diverse semantic relationships present in different heteroge-\nneous graphs, ensuring a unified approach. It comprises two es-\nsential components: the in-context parameterized heterogeneity\nprojector and the parameter allocator. The adaptive in-context pro-\njector utilizes language to encode the distinct node and edge types\nwithin the heterogeneous graphs, facilitating model adaptation.\nMeanwhile, the parameter allocator dynamically assigns param-\neters tailored specifically for the tokenizer. To optimize the tok-\nenizer\u2019s performance and seamlessly integrate it within the HiGPT\nframework, we conduct pre-training using a simplified text-graph\ncontrastive learning framework. This pre-training process directly\nincorporates the tokenizer into the HiGPT framework and effec-\ntively integrates it with the Large Language Model (LLM). This ap-\nproach enhances the tokenizer\u2019s capabilities and ensures its smooth\nfunctioning within the overall model architecture.\n3.1.1\nGraph Tokenization with Meta Projector. Given a het-\nerogeneous graph G with a feature matrix X = {\ud835\udc4b\ud835\udc47\ud835\udc56\u2208R|V\ud835\udc47\ud835\udc56|\u00d7\ud835\udc51\ud835\udc47\ud835\udc56,\ud835\udc47\ud835\udc56\u2208\nT } and an adjacency matrix A, the goal of the heterogeneous\ngraph tokenizer is to encode the hidden representations of the\nheterogeneous graph, denoted as H = {\ud835\udc3b\ud835\udc47\ud835\udc56\u2208R|V\ud835\udc47\ud835\udc56|\u00d7\ud835\udc53\ud835\udc47\ud835\udc56,\ud835\udc47\ud835\udc56\u2208T }.\nThis is achieved through the function H = HG-Tokenizer(X, A),\nwhere \ud835\udc53\ud835\udc47\ud835\udc56represents the hidden dimension of node type \ud835\udc47\ud835\udc56. The\nHG-Tokenizer(\u00b7) can be implemented using various backbone HGNN\narchitectures, such as HetGNN [44], HAN [33], or HGT [10].\nHowever, the generalization capability of these heterogeneous\nGNNs is constrained by their inherent design, which includes\npre-defined parameter learning tailored to specific heterogeneous\ngraphs. As a result, the trained HGNNs cannot be readily applied\nto other unseen heterogeneous graphs, which goes against the ob-\njective of achieving unified encoding with the HG-Tokenizer. To\nillustrate, let\u2019s consider HGT as an example. In HGT, the calcula-\ntion of \u210e(\ud835\udc59)\n\ud835\udc63\ninvolves utilizing functions such as Attention(\u00b7) and\nMessage(\u00b7) to process information from the source nodes:\ne\u210e(\ud835\udc59)\n\ud835\udc63\n=\n\u2295\n\u2200\ud835\udc62\u2208N(\ud835\udc63) (Attention (\ud835\udc62,\ud835\udc52, \ud835\udc63) \u00b7 Message (\ud835\udc62,\ud835\udc52, \ud835\udc63))\n\u210e(\ud835\udc59)\n\ud835\udc63\n= F \ud835\udf0f(\ud835\udc63)\n\u03981\n\u0010\n\ud835\udf0e\n\u0010\ne\u210e(\ud835\udc59)\n\ud835\udc63\n\u0011\u0011\n+ \u210e(\ud835\udc59\u22121)\n\ud835\udc63\n= W\ud835\udf0f(\ud835\udc63)\n1\n\u00b7\n\u0010\n\ud835\udf0e\n\u0010\ne\u210e(\ud835\udc59)\n\ud835\udc63\n\u0011\u0011\n+ b\ud835\udf0f(\ud835\udc63)\n1\n+ \u210e(\ud835\udc59\u22121)\n\ud835\udc63\n(2)\nThe notation F \ud835\udf0f(\ud835\udc63)\n\u03981\n(\u00b7) represents a fully-connected layer with pa-\nrameters \u03981 = {W\ud835\udf0f(\ud835\udc63), b\ud835\udf0f(\ud835\udc63)}. Here, \ud835\udf0f(\ud835\udc63) denotes the node type of\n\ud835\udc63, and \ud835\udf0e(\u00b7) represents the activation function. The specific formula-\ntion of the Attention(\u00b7) and Message(\u00b7) functions, with \u210eheads:\nAttention (\ud835\udc62,\ud835\udc52, \ud835\udc63)\n= Softmax\n\u2200\ud835\udc62\u2208N(\ud835\udc63)\n \n\u2225\n\ud835\udc56\u2208[1,\u210e]\nF \ud835\udf0f(\ud835\udc62)\n\u03982\n\u0010\n\u210e(\ud835\udc59\u22121)\n\ud835\udc62\n\u0011\nW\ud835\udf0c(\ud835\udc52)\n1\nF \ud835\udf0f(\ud835\udc63)\n\u03983\n\u0010\n\u210e(\ud835\udc59\u22121)\n\ud835\udc63\n\u0011!\nMessage (\ud835\udc62,\ud835\udc52, \ud835\udc63) =\n\u2225\n\ud835\udc56\u2208[1,\u210e]\nF \ud835\udf0f(\ud835\udc62)\n\u03984\n\u0010\n\u210e(\ud835\udc59\u22121)\n\ud835\udc62\n\u0011\nW\ud835\udf0c(\ud835\udc52)\n2\n(3)\nAdaptive Parameterized Heterogeneity Projector. To make\nour HiGPT adaptable to a wide range of heterogeneous graphs\nwith varying graph heterogeneity settings, and to eliminate the\nrequirement of pre-defining the number of type-specific projections\nin advance, we propose the design of a type-aware parameterized\nprojector. This projector dynamically and automatically encodes the\nrelation heterogeneity into latent representations. More specifically,\nthe type-aware projectors with the parameters F \ud835\udf0f(\ud835\udc63)\n\u0398\ud835\udc56\nand W\ud835\udf0c(\ud835\udc52)\n\ud835\udc56\nare generated automatically according to the following procedure:\n\u0398\ud835\udc56= {W\ud835\udf0f(\ud835\udc63)\n\ud835\udc56\n; b\ud835\udf0f(\ud835\udc63)\n\ud835\udc56\n} = F\u03a9\n\u0010\nT\ud835\udf0f(\ud835\udc63)\u0011\n;\nW\ud835\udf0c(\ud835\udc52)\n\ud835\udc56\n= F\u03a9\n\u0010\nT\ud835\udf0c(\ud835\udc52)\u0011\n(4)\nF\u03a9 is a fully-connected layer with parameters \u03a9, while T\ud835\udf0f(\ud835\udc63) and\nT\ud835\udf0c(\ud835\udc52) are the features associated with node type \ud835\udf0f(\ud835\udc63) and edge type\n\ud835\udf0c(\ud835\udc52), respectively. It is important to note that the example provided\nshowcases the usage of the in-context parameterized heterogeneity\nprojector within the heterogeneous graph transformer framework.\nHowever, our HiGPT is designed to be versatile and adaptable,\nallowing for the integration of diverse heterogeneous GNNs.\nLanguage-Enriched Heterogeneity Representation. We lever-\nage natural language as a means to generate universal heterogeneity\nrepresentations for nodes and edges based on their respective types.\nFor instance, in the heterogeneous IMDB dataset, we can describe\na \"movie\" node as \"This node represents a movie\" using natural\nlanguage. Similarly, the edge (\"movie\", \"to\", \"director\") can be ex-\npressed as \"The movie is directed by the director\". To encode these\nnatural language descriptions of nodes and edges, we employ a\npre-trained language model such as Sentence-BERT [22] to obtain\ntype representations. To ensure distinguishability and diversity\namong different types, we utilize multiple languages to describe\nthe same type. The encoded representations from the pre-trained\nlanguage models are averaged to derive the final representation.\nThis process can be defined as follows:\nT\ud835\udf0f(\ud835\udc63) = Mean-Pooling\n\u0010\nSentence-BERT\n\u0010\nS\ud835\udf0f(\ud835\udc63)\u0011\u0011\nT\ud835\udf0c(\ud835\udc52) = Mean-Pooling\n\u0010\nSentence-BERT\n\u0010\nS\ud835\udf0c(\ud835\udc52)\u0011\u0011\n(5)\nS\ud835\udf0f(\ud835\udc63) and S\ud835\udf0c(\ud835\udc52) represent sets of descriptions for node type \ud835\udf0f(\ud835\udc63)\nand edge type \ud835\udf0c(\ud835\udc52), respectively. For instance, consider the example\nof the edge (\"movie\", \"to\", \"director\"). One possible description is:\nS(\"movie\",\"to\",\"director\") = {\n\"The movie is directed by the director\",\n\"The film features direction by the director\", \u00b7 \u00b7 \u00b7 } (6)\nFor comprehensive descriptions featuring text-enriched heterogene-\nity representations of various datasets, please consult the appendix.\nHeterogeneous Graph \nTransformer\nHuman instructions with graph tokens <Graph>\n\u201cThis node represents a movie\u201d\nLMs\nSentence-BERT\n\u201cThe movie is directed by the director\u201d\nLanguage-Enriched Heterogeneity \nRepresentation\nThis node represents a movie\nThis node is categorized as a director\nThis node is an actor\nThis node indicates Stanley Kubrick\nThis node represents Ang Lee\nThis node is Alfred Hitchcock\nHeterogeneous Graph Tokens\nNodes of type 1\nNodes of type 2\n\u2026\n(a) Heterogeneous Relation Awareness\n(b) Homogenous Relation Awareness\n(c) Heterogeneity-aware Fine-Tuning\n\ud835\udc4a\nGraph \nProjector\n\u2026\n\u2026\nLLMs\nVicuna\nHGT Layers\n\u00d7 \ud835\udc3f\n\u2026\n\u2026\nAdaptive Parameterized \nHeterogeneity Projector\n\ud835\udc39!\n\ud835\udc47\nInstruct\na. Node-level \nInference\nb. Edge-level \nInference\nHiGPT\nHiGPT\nHiGPT\nInput \nHeterogeneous Graph\nHeterogeneous Graph Corpus\nFigure 1: The overall architecture of our HiGPT.\n3.1.2\nLightweight Text-Graph Contrastive Alignment. Build-\ning upon recent advancements in aligning cross-modality seman-\ntics [20, 38], we draw inspiration to employ a text-graph contrastive\nalignment paradigm for pre-training the proposed heterogeneous\ngraph tokenizer. This approach aims to align the modeling capa-\nbilities of language and heterogeneous structures, enabling bet-\nter collaboration between the tokenizer and the language mod-\nels. To begin, we consider raw textual contents represented as\nC = \ud835\udc50\ud835\udc56\u2208R\ud835\udc59\ud835\udc56\u00d7\ud835\udc51, 1 \u2264\ud835\udc56\u2264\ud835\udc41, where \ud835\udc41denotes the total number of\nheterogeneous graph nodes X = \ud835\udc4b\ud835\udc47\ud835\udc56\u2208R|V\ud835\udc47\ud835\udc56|\u00d7\ud835\udc51\ud835\udc47\ud835\udc56. Here, \ud835\udc59\ud835\udc56repre-\nsents the length of the textual content associated with the \ud835\udc56-th node.\nIn our approach, we adopt a lightweight text-graph contrastive\nalignment paradigm formally presented as follows:\n\u02c6H = norm (HG-Tokenizer (X)) , \u02c6T = norm (LM-Tokenizer (C))\nL = 1\n2\n\u0000CE(\u039b, y) + CE(\u039b\u22a4, y)\u0001 , \u039b = ( \u02c6H\u02c6T\u22a4) \u00b7 exp(\ud835\udf0f)\n(7)\nWe use the contrastive label y = (0, 1, \u00b7 \u00b7 \u00b7 ,\ud835\udc5b\u22121)\u22a4and the Cross-\nEntropy function CE(\u00b7). Our implementation employs the multi-\nlayer vanilla transformer for LM-Tokenizer(\u00b7).\n3.2\nHeterogeneous Graph Instruction Tuning\nThe objective of HiGPT is to empower language models to directly\ngenerate predictions for downstream tasks with the unseen het-\nerogeneous graph and corresponding instructions. The natural\nlanguage instruction is first encoded by a tokenizer into text em-\nbeddings, denoted as XI = LM-tokenizer(instruction). To align\nthe dimensions, we employ a projector that maps graph tokens to\nthe same dimension as the text embeddings, given by XG = \ud835\udc53P(H),\nwhich can be as simple as a linear layer. For a sequence of length \ud835\udc3f,\nwe determine the probability of generating the target output XO:\n\ud835\udc5d(XO|XG, XI) =\n\ud835\udc3f\n\u00d6\n\ud835\udc56=1\n\ud835\udc5d\u03a6(\ud835\udc65\ud835\udc56|XG, XI,<\ud835\udc56, XO,<\ud835\udc56)\n(8)\nwhere \u03a6 represents the learnable parameters within HiGPT.\n3.2.1\nInstruction Tuning with Heterogeneous Graph Corpus.\nTo enable the Language Model (LLM) to effectively differentiate be-\ntween different types of input heterogeneous graph tokens and the\nspecific nodes within each type, based on natural language instruc-\ntions, we propose instruction pre-training using a large \"corpus\"\nconsisting of heterogeneous graph-instruction pairs. This approach\nequips the fine-tuned HiGPT with a comprehensive understanding\nof both homogeneous and heterogeneous graph structures.\n\u2022 Heterogeneous Relation Awareness. Our objective is to en-\nhance the language model\u2019s proficiency in distinguishing be-\ntween specific types of nodes within a heterogeneous context,\ntaking into account the intricate relationships. This is achieved\nby leveraging the information encoded in the graph tokens.\n\u2022 Homogeneous Relation Awareness. Our aim is to equip the\nmodel with the ability to establish a significant correspondence\nbetween sequences of graph tokens that belong to the same\ncategory and their corresponding natural language descriptions.\nHeterogeneous Graph Instruction. In our graph instruction, we\nincorporate a heterogeneous subgraph generated through random\nneighbor sampling, accompanied by a question generated by a\nhuman. To enhance the diversity of the heterogeneous graph corpus,\nwe conduct multiple samplings focusing on nodes from different\ntypes. Additionally, we introduce the <graph> token as a graph\nindicator within the human question. i) To achieve heterogeneous\nrelation awareness, we introduce the inter-type token matching\ntask. This task involves providing the Language Model (LLM) with\nencoded sequences of graph tokens from different types, enabling\nit to differentiate between the various types. ii) For homogeneous\nrelation awareness, we design the intra-type matching task, where\nthe LLM receives encoded sequences of graph tokens from a specific\ntype, allowing it to establish correspondence with the relevant\ndescriptions. Further details regarding the instruction template at\nthis stage are illustrated in Table 1 and Appendix Section A.4.\n3.2.2\nHeterogeneity-aware Fine-Tuning. To customize the rea-\nsoning abilities of the language model for specific downstream\ntasks on heterogeneous graphs, we propose Heterogeneity-aware\nFine-Tuning. This approach entails conducting supervised learning\nwith task-specific instructions following the initial instruction pre-\ntraining phase with heterogeneous graph corpus. It allows us to\nrefine the LLM\u2019s performance and adapt it to the specific require-\nments of the targeted tasks on heterogeneous graphs.\nIn this stage, we incorporate a randomly sampled heterogeneous\nsubgraph centered around the target node, along with a human-\ngenerated question. Given that the previous phase of instruction\npre-training has already equipped the model with heterogeneous\nTable 1: Prompts for the three tasks of heterogeneous graph instruction-tuning.\nHeteroGraph\nHuman Question\nHiGPT Response\n(a) Heterogeneous Instruction Pre-training\ncentral_nodes: (\"movie\": [1, ...,\nn]), num_neighbors:[10, 10]\nGiven a heterogeneous graph about movies, there are 3 types of nodes: <DESC>. By performing\nrandom sampling, a heterogeneous subgraph is obtained. Separately nodes of different types\nare: 1. <graph>, 2. <graph>... Please sequentially provide the types for the node sequences.\nBased on graph tokens,\ntypes of the graph tokens\nshould be 1. movie, 2. ....\n(b) Homogenous Instruction Pre-training\ncentral_nodes: (\"paper\": [1, ...,\nn]), num_neighbors: [10, 10]\nGiven a heterogeneous graph about papers, there are 4 types of nodes: <DESC>. ..., a heteroge-\nneous subgraph is obtained. The nodes for \"paper\" are: <graph>. Also, a list of textual descriptions\nfor the papers are: <DESC>. Please reorder the text list based on the order of graph tokens.\nThe matching of graph to-\nkens and papers should be:\n<ANSWER>.\n(c) Heterogeneous Supervised Fine-Tuning\ncentral_nodes: (\"movie\": [i]),\nnum_neighbors: [10, 10]\nGiven a heterogeneous graph about movies, there are 3 types of nodes: <DESC>. ..., a heteroge-\nneous subgraph is obtained. There are nodes of different types: \"movie\" nodes: <graph>, <DESC>\nwhere the 0-th node is the central node. \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Which\nof the following classes does this movie belong to: action, comedy, drama?\nBased on the given infor-\nmation, the likely category\nfor movie is Action.\nHuman Question\nImagine\nthat\nyou\nmade\nthe\ncorrect\nchoice\nand\nproceed\nwith\nstep-by-\nstep reasoning...\nImagine\nthree\ndifferent\nexperts\nare\nanswering\nquestion,\nthen\nshare\nit\nwith the group...\nImagine\nthat\nthree\nexperts\nare\ndiscussing\nthe\nquestion\nwith\na\npanel discussion...\nPlease generate some knowledge that can assist in formulating the answer.\nImagine that you are at the correct answer based on the provided information\nand knowledge, and present step-by-step reasoning.\nGround Truth\nCoT Simulation\nToT Simulation\nPanel Simulation\nHuman Question\nGround Truth\nReasoning\nGKP Simulation\nGround Truth\nReasoning\nHuman Question\nKnowledge\nFigure 2: Mixture-of-Thought (MoT) Augmentation\nand homogeneous relation awareness, we design human questions\nthat are rich in heterogeneity. These questions contain sequences\nof graph tokens from different types, indicated by multiple occur-\nrences of the <graph> token. Additionally, the human question\nincludes pertinent auxiliary information pertaining to the target\nnode. The designs of these instructions are presented in Figure 1.\n3.3\nMixture-of-Thought (MoT) for Graph\nInstruction Augmentation\nIn practical scenarios of heterogeneous graph learning, data scarcity\noften poses a challenge. This is especially true when using hetero-\ngeneous graphs to model cold-start users/items in recommendation\nsystems, where sparse user interaction data limits the availabil-\nity of supervised signals. To address the issue of data sparsity, we\npropose enhancing our HiGPT by incorporating augmented graph\ninstructions. Drawing inspiration from previous works [24], we in-\ntroduce a novel method for instruction augmentation in the context\nof heterogeneous graph instruction tuning. This method utilizes\nprompt engineering techniques, particularly Mixture-of-Thought\n(MoT), to generate diverse and informative instructions. The goal\nis to effectively overcome the challenges posed by data scarcity. By\nincorporating augmented graph instructions, we expect our model\nenhancement to effectively handle data sparsity.\n3.3.1\nMixture-of-Thought (MoT) Prompting. Our focus is on\ndesigning and optimizing prompts to effectively utilize language\nmodels [15, 25, 35, 42]. We employ several techniques to enhance\nlanguage models: i) Chain-of-Thought (CoT) [35]: CoT prompts\nintroduce intermediate steps, enabling complex reasoning and so-\nphisticated capabilities. ii) Tree-of-Thought (ToT) [42]: ToT main-\ntains a tree structure of coherent language sequences called thoughts.\nThese thoughts serve as systematic intermediate steps for problem-\nsolving. iii) PanelGPT [25]: PanelGPT incorporates panel discus-\nsions among language models, enhancing the prompt engineer-\ning process through collaboration. iv) Generated Knowledge\nPrompting (GKP) [15]: GKP involves incorporating additional\nknowledge into prompts for enhancement. By leveraging these\ntechniques, our objective is to augment heterogeneous graph in-\nstructions, especially in scenarios with limited data availability.\n3.3.2\nInstruction Augmentation with Priori Knowledge. We\nutilize seven instruction augmentation strategies, each generating\nseven augmented instructions for every question-answer pair, incor-\nporating the characteristics of Mixture-of-Thought (MoT). However,\nclosed-source language models such as ChatGPT may produce in-\ncorrect answers, resulting in flawed guidance. To overcome this\nissue, we propose incorporating prior knowledge, specifically the\ncorrect answer, into the prompt. It enables LLM to simulate gener-\nating the correct answer and produce intermediate reasoning steps\nusing different MoT methods, as shown in Figure 1 and Appendix.\n4\nEVALUATION\nTo assess the effectiveness of our HiGPT model, our experiments\nare designed to address the following research questions:\n\u2022 RQ1: How does the performance of our HiGPT compare to that\nof baseline methods in both few-shot and zero-shot scenarios?\n\u2022 RQ2: To what extent do the key components of HiGPT contribute\nto its overall performance across various settings?\n\u2022 RQ3: Can the HiGPT\u2019s universal heterogeneity knowledge be\nleveraged to achieve graph in-context learning solely through\ngraph instruction examples, without any model optimization?\n4.1\nExperimental Settings\n4.1.1\nExperimental Datasets. The experiments were conducted\non three benchmark datasets, i.e., IMDB [8], DBLP [8], and ACM [33].\nIMDB is an extensive dataset that focuses on online movies and\ntelevision programs. It encompasses 4278 movies, 2081 directors,\nand 5257 actors. Each movie is categorized into one of three classes:\nAction, Comedy, or Drama. DBLP, on the other hand, consists of\n4057 authors, 14328 papers, 7723 terms, and 20 publication venues.\nThis dataset was gathered from a computer science bibliography\nwebsite and the authors are distributed among four research ar-\neas: Database, Data Mining, Artificial Intelligence, and Information\nRetrieval. Lastly, the ACM dataset comprises 3025 papers, 5835\nauthors, and 56 subjects. The papers are classified into three classes:\nDatabase, Wireless Communication, and Data Mining.\n4.1.2\nEvaluation Protocols. To ensure consistency in the feature\ndimension of nodes across all datasets, we utilize a pre-trained\nSentence-BERT to encode nodes of all types from each dataset\ninto a standardized dimension. For the supervised few-shot node\nclassification, we randomly select 1, 3, 5, 10, 20, 40, or 60 labeled\nnodes per class as our training set. Additionally, we reserve 1,000\nnodes for validation and another 1,000 nodes for testing purposes.\nOur evaluation metrics encompass Micro-F1, Macro-F1, and AUC.\n4.1.3\nCompared Baseline Methods. For our comprehensive per-\nformance comparison, we evaluate various state-of-the-art methods\nfrom three different categories: i) The first category consists of repre-\nsentative homogeneous graph neural networks, including SAGE [9]\nand GAT [28]. ii) The second category includes approaches uti-\nlizing message-passing mechanisms in heterogeneous graph neu-\nral networks. This category features models such as HAN [33],\nHGT [10], and HetGNN [44]. iii) The third category focuses on\nself-supervised techniques for heterogeneous graph learning. This\ncategory incorporates generative strategies like HGMAE [27], as\nwell as contrastive schemes such as DMGI [19] and HeCo [34].\n4.1.4\nImplementation Details. In Appendix Sec A.5.1, we offer\ncomprehensive descriptions of the implementation details, includ-\ning the datasets used, training hyperparameters, configurations of\nthe base LLM, and more. These details provide a deeper understand-\ning of our implementation approach.\n4.2\nOverall Performance Comparison (RQ1)\nWe performed node classification tasks on three datasets, explor-\ning both few-shot and zero-shot settings. In the few-shot settings,\nour model was trained on the IMDB dataset with shot numbers\nranging from 1 to 60, and evaluated on the IMDB test set of 1,000\nsamples [27, 34]. For the zero-shot settings, the model was trained\non the IMDB dataset with the same shot numbers, and tested on\nseparate test sets from the DBLP and ACM datasets, each containing\n1,000 samples. To enable cross-dataset transferability in supervised\nheterogeneous Graph Neural Networks (GNNs), we unified node\nand edge categories, and utilized a classifier trained with transfer\ndata to accommodate variations in class quantities across datasets.\nFor self-supervised methods focused on learning embeddings\nfor downstream heterogeneous graph nodes, we excluded the zero-\nshot settings. The overall performance is partially shown in Table 2,\nwith detailed results in Table 13 in the appendix. \"-std\" and \"-cot\"\nnotations represent the standard test prompt with direct answers\nand the prompt with a Chain-of-Thought (CoT) feature, respec-\ntively. These details provide insights into our node classification\nexperiments in both supervised and zero-shot settings.\nSuperiority of HiGPT in Few-Shot Settings. HiGPT outper-\nforms state-of-the-art baselines consistently in supervised settings,\neven with only one sample per class. The success can be attributed\nto our effective instruction-tuning on a large-scale heterogeneous\ngraph corpus. This enables the LLM to extract valuable and trans-\nferable heterogeneous structural information from graph tokens,\nresulting in a significant performance boost in downstream tasks.\nAdditionally, our proposed MoT graph instruction augmentation\nmethod enhances the LLM with diverse mixed reasoning capabili-\nties without the need for additional supervision signals. As a result,\nit effectively mitigates the data scarcity in few-shot scenarios.\nZero-shot Superiority of HiGPT. In zero-shot settings, our HiGPT\nsurpasses baselines with significant improvements. Unlike tradi-\ntional models constrained by training graph types, our approach\nleverages an in-context heterogeneous graph tokenizer. This tok-\nenizer adapts tokenization based on the input graph, allowing the\nLLM to seamlessly combine graph tokens that capture higher-order\nstructural features with its semantic understanding. As a result, our\nmodel effectively overcomes the limitation of graph heterogeneity\nshift, performing exceptionally well even in cross-domain scenarios.\nThis showcases the remarkable adaptability of our HiGPT.\nEffectiveness of Mixture-of-Thought Augmentation. Through\nthe implementation of the MoT approach, our model harnesses the\nvaried reasoning capabilities of the formidable LLM (specifically,\nGPT-3.5) and seamlessly integrates them into our more compact\nlanguage model. This integration serves to bolster our model\u2019s\nability to effectively navigate data scarcity and elevate its perfor-\nmance in situations characterized by limited supervised signals.\nThe MoT technique assumes a pivotal role in generating dynamic\nand diverse instructions, thereby offsetting the dearth of data and\nempowering our model to make notably precise predictions across\nboth supervised and zero-shot settings.\n4.3\nModel Ablation Test (RQ2)\nTo evaluate the proposed modules\u2019 effectiveness, we individually\nremove the key techniques in HiGPT. The results are summarized\nin Table 3. Here are the ablated variants and the key conclusions:\n\u2022 Effect of Heterogeneous Graph Instruction-Tuning. To vali-\ndate the effectiveness of instruction tuning in the tuning stage on\nthe large heterogeneous graph corpus, we generated the \"w/o S1\"\nvariant by directly tuning the instructions solely on the down-\nstream task data. Through experiments with different epoch set-\ntings (15, 50, and 100), we observed that models tuned solely on\nthe downstream task data failed to provide complete and accurate\nanswers in all cases. However, our HiGPT achieved state-of-the-\nart performance within just 15 epochs across all settings. This\nsuccess can be attributed to the fact that our HiGPT learns from\na vast heterogeneous graph context corpus, enabling it to under-\nstand and extract crucial structural information. As a result, in\nthe second stage, our HiGPT requires only a minimal amount of\nsupervised data (even in a 1-shot scenario) to quickly align with\nthe downstream task. Conversely, directly aligning LLMs with\nsparse supervised data proves to be challenging.\n\u2022 Effect of In-Context Heterogeneous Graph Tokenizer. We\ntested the necessity of incorporating heterogeneous graph struc-\ntural information from our in-context tokenizer. By excluding\nthe introduction of heterogeneous graph tokens and solely train-\ning the LLM\u2019s embeddings weights on the downstream data,\nTable 2: Performance comparison on node classification tasks in both few-shot and zero-shot settings. However, since SSL\nmethods focus on learning embeddings from downstream graphs, we excluded the zero-shot settings for them (\"-\").\nDatasets\nMetric\ntrain-on\ntest-on\nSAGE\nGAT\nHAN\nHGT\nHetGNN\nDMGI\nHGMAE\nHeCo\nHiGPT-std\nHiGPT-cot\nSupervised\nMi-F1\nIMDB-1\nIMDB-1000\n0.4663\u00b10.0025\n0.4567\u00b10.0122\n0.4890\u00b10.0271\n0.4977\u00b10.0186\n0.4790\u00b10.0134\n0.4570\u00b10.0126\n0.3609\u00b10.0145\n0.3874\u00b10.0159\n0.5090\u00b10.0073\n0.5360\u00b10.0065\nIMDB-5\nIMDB-1000\n0.5010\u00b10.0051\n0.5170\u00b10.0029\n0.4840\u00b10.0094\n0.5003\u00b10.0093\n0.5020\u00b10.0045\n0.4413\u00b10.0173\n0.3652\u00b10.0062\n0.3385\u00b10.0169\n0.6180\u00b10.0027\n0.6320\u00b10.0085\nIMDB-20\nIMDB-1000\n0.5930\u00b10.0093\n0.6117\u00b10.0012\n0.5763\u00b10.0046\n0.5750\u00b10.0065\n0.5957\u00b10.0054\n0.5497\u00b10.0256\n0.4107\u00b10.0106\n0.3781\u00b10.0148\n0.6090\u00b10.0255\n0.6440\u00b10.0075\nIMDB-40\nIMDB-1000\n0.6170\u00b10.0112\n0.6261\u00b10.0015\n0.6198\u00b10.0025\n0.5923\u00b10.0040\n0.6177\u00b10.0046\n0.5813\u00b10.0033\n0.3946\u00b10.0067\n0.3927\u00b10.0134\n0.6260\u00b10.0057\n0.6280\u00b10.0071\nMa-F1\nIMDB-1\nIMDB-1000\n0.4425\u00b10.0068\n0.3974\u00b10.0183\n0.4229\u00b10.0104\n0.4020\u00b10.0112\n0.4456\u00b10.0036\n0.4083\u00b10.0288\n0.3573\u00b10.0117\n0.4023\u00b10.0137\n0.4986\u00b10.0141\n0.5247\u00b10.0061\nIMDB-5\nIMDB-1000\n0.4613\u00b10.0086\n0.4767\u00b10.0098\n0.4695\u00b10.0037\n0.4676\u00b10.0153\n0.4677\u00b10.0145\n0.4254\u00b10.0124\n0.3500\u00b10.0080\n0.3468\u00b10.0213\n0.6111\u00b10.0091\n0.6243\u00b10.0060\nIMDB-20\nIMDB-1000\n0.5953\u00b10.0095\n0.6121\u00b10.0024\n0.5756\u00b10.0051\n0.5723\u00b10.0056\n0.5969\u00b10.0055\n0.5495\u00b10.0270\n0.4065\u00b10.0089\n0.3904\u00b10.0172\n0.6068\u00b10.0146\n0.6398\u00b10.0083\nIMDB-40\nIMDB-1000\n0.6182\u00b10.0107\n0.6254\u00b10.0009\n0.6224\u00b10.0057\n0.5909\u00b10.0068\n0.6234\u00b10.0038\n0.5786\u00b10.0064\n0.3866\u00b10.0072\n0.3988\u00b10.0147\n0.6265\u00b10.0090\n0.6237\u00b10.0059\nAUC\nIMDB-1\nIMDB-1000\n0.6079\u00b10.0061\n0.6151\u00b10.0065\n0.6234\u00b10.0252\n0.6249\u00b10.0170\n0.6107\u00b10.0075\n0.5780\u00b10.0130\n0.5274\u00b10.0058\n0.5712\u00b10.0099\n0.6565\u00b10.0146\n0.6685\u00b10.0037\nIMDB-5\nIMDB-1000\n0.6309\u00b10.0049\n0.6372\u00b10.0012\n0.6102\u00b10.0059\n0.6197\u00b10.0152\n0.6290\u00b10.0022\n0.5832\u00b10.0132\n0.5262\u00b10.0041\n0.5067\u00b10.0228\n0.7308\u00b10.0125\n0.7310\u00b10.0086\nIMDB-20\nIMDB-1000\n0.6976\u00b10.0064\n0.7122\u00b10.0020\n0.6815\u00b10.0052\n0.6801\u00b10.0048\n0.7005\u00b10.0030\n0.6657\u00b10.0179\n0.5766\u00b10.0064\n0.5541\u00b10.0145\n0.7227\u00b10.0034\n0.7424\u00b10.0113\nIMDB-40\nIMDB-1000\n0.7171\u00b10.0069\n0.7210\u00b10.0014\n0.7204\u00b10.0015\n0.6970\u00b10.0060\n0.7145\u00b10.0035\n0.6860\u00b10.0027\n0.5488\u00b10.0049\n0.5653\u00b10.0105\n0.7323\u00b10.0036\n0.7331\u00b10.0074\nZero-shot\nMi-F1\nIMDB-1\nDBLP-1000\n0.2353\u00b10.0372\n0.1893\u00b10.0373\n0.2653\u00b10.0203\n0.2573\u00b10.0519\n0.2900\u00b10.0638\n-\n-\n-\n0.3180\u00b10.0072\n0.3500\u00b10.0073\nIMDB-5\nDBLP-1000\n0.2607\u00b10.0082\n0.2737\u00b10.0176\n0.2577\u00b10.0094\n0.2453\u00b10.0458\n0.2427\u00b10.0452\n-\n-\n-\n0.3180\u00b10.0044\n0.3620\u00b10.0047\nIMDB-20\nDBLP-1000\n0.2810\u00b10.0289\n0.2780\u00b10.0033\n0.2710\u00b10.0000\n0.2803\u00b10.0208\n0.2333\u00b10.0353\n-\n-\n-\n0.3840\u00b10.0088\n0.4180\u00b10.0083\nIMDB-40\nDBLP-1000\n0.2400\u00b10.0324\n0.2847\u00b10.0053\n0.2710\u00b10.0000\n0.2937\u00b10.0005\n0.2027\u00b10.0345\n-\n-\n-\n0.3320\u00b10.0087\n0.3630\u00b10.0045\nMa-F1\nIMDB-1\nDBLP-1000\n0.0963\u00b10.0132\n0.1169\u00b10.0089\n0.1047\u00b10.0063\n0.1016\u00b10.0169\n0.1778\u00b10.0629\n-\n-\n-\n0.2048\u00b10.0068\n0.2472\u00b10.0070\nIMDB-5\nDBLP-1000\n0.1042\u00b10.0028\n0.1291\u00b10.0145\n0.1024\u00b10.0030\n0.1138\u00b10.0296\n0.0971\u00b10.0148\n-\n-\n-\n0.1917\u00b10.0046\n0.2773\u00b10.0085\nIMDB-20\nDBLP-1000\n0.1448\u00b10.0573\n0.1274\u00b10.0060\n0.1066\u00b10.0000\n0.1143\u00b10.0116\n0.1008\u00b10.0191\n-\n-\n-\n0.3142\u00b10.0074\n0.3733\u00b10.0051\nIMDB-40\nDBLP-1000\n0.1068\u00b10.0060\n0.1588\u00b10.0078\n0.1066\u00b10.0000\n0.1268\u00b10.0105\n0.0984\u00b10.0161\n-\n-\n-\n0.2331\u00b10.0069\n0.2912\u00b10.0056\nAUC\nIMDB-1\nDBLP-1000\n0.4999\u00b10.0001\n0.4513\u00b10.0295\n0.5000\u00b10.0000\n0.5000\u00b10.0000\n0.5206\u00b10.0306\n-\n-\n-\n0.5222\u00b10.0069\n0.5406\u00b10.0040\nIMDB-5\nDBLP-1000\n0.4978\u00b10.0030\n0.4908\u00b10.0078\n0.5000\u00b10.0000\n0.5031\u00b10.0043\n0.4998\u00b10.0003\n-\n-\n-\n0.5184\u00b10.0081\n0.5493\u00b10.0091\nIMDB-20\nDBLP-1000\n0.5154\u00b10.0213\n0.4918\u00b10.0020\n0.5000\u00b10.0000\n0.5011\u00b10.0016\n0.4957\u00b10.0060\n-\n-\n-\n0.5669\u00b10.0041\n0.5907\u00b10.0089\nIMDB-40\nDBLP-1000\n0.5027\u00b10.0031\n0.4976\u00b10.0021\n0.5000\u00b10.0000\n0.5008\u00b10.0006\n0.4884\u00b10.0164\n-\n-\n-\n0.5296\u00b10.0070\n0.5508\u00b10.0086\nMi-F1\nIMDB-1\nACM-1000\n0.3293\u00b10.0418\n0.3567\u00b10.0053\n0.3407\u00b10.0111\n0.3240\u00b10.0014\n0.3743\u00b10.0434\n-\n-\n-\n0.4160\u00b10.0106\n0.4540\u00b10.0089\nIMDB-5\nACM-1000\n0.3820\u00b10.0113\n0.3787\u00b10.0057\n0.3630\u00b10.0086\n0.3160\u00b10.0169\n0.3583\u00b10.0198\n-\n-\n-\n0.4580\u00b10.0173\n0.4880\u00b10.0131\nIMDB-20\nACM-1000\n0.2807\u00b10.0074\n0.3013\u00b10.0188\n0.3133\u00b10.0031\n0.3530\u00b10.0000\n0.2840\u00b10.0226\n-\n-\n-\n0.5080\u00b10.0129\n0.5030\u00b10.0064\nIMDB-40\nACM-1000\n0.3173\u00b10.0005\n0.2393\u00b10.0144\n0.2697\u00b10.0194\n0.3560\u00b10.0099\n0.3180\u00b10.0016\n-\n-\n-\n0.4750\u00b10.0149\n0.5050\u00b10.0077\nMa-F1\nIMDB-1\nACM-1000\n0.2647\u00b10.0269\n0.2908\u00b10.0131\n0.2250\u00b10.0416\n0.1631\u00b10.0005\n0.3139\u00b10.0468\n-\n-\n-\n0.3949\u00b10.0078\n0.4177\u00b10.0124\nIMDB-5\nACM-1000\n0.3208\u00b10.0130\n0.3009\u00b10.0137\n0.2782\u00b10.0026\n0.1969\u00b10.0301\n0.3087\u00b10.0225\n-\n-\n-\n0.4336\u00b10.0085\n0.4510\u00b10.0114\nIMDB-20\nACM-1000\n0.2694\u00b10.0091\n0.2422\u00b10.0098\n0.2412\u00b10.0050\n0.2094\u00b10.0501\n0.2715\u00b10.0181\n-\n-\n-\n0.4964\u00b10.0075\n0.4877\u00b10.0070\nIMDB-40\nACM-1000\n0.3117\u00b10.0017\n0.2141\u00b10.0071\n0.2313\u00b10.0132\n0.2749\u00b10.0122\n0.3144\u00b10.0017\n-\n-\n-\n0.4176\u00b10.0116\n0.4585\u00b10.0089\nAUC\nIMDB-1\nACM-1000\n0.4934\u00b10.0247\n0.5248\u00b10.0038\n0.5128\u00b10.0086\n0.5000\u00b10.0000\n0.5318\u00b10.0295\n-\n-\n-\n0.5672\u00b10.0040\n0.5969\u00b10.0082\nIMDB-5\nACM-1000\n0.5433\u00b10.0082\n0.5415\u00b10.0047\n0.5282\u00b10.0073\n0.4950\u00b10.0134\n0.5256\u00b10.0145\n-\n-\n-\n0.5991\u00b10.0103\n0.6224\u00b10.0054\nIMDB-20\nACM-1000\n0.4601\u00b10.0048\n0.4772\u00b10.0137\n0.4877\u00b10.0029\n0.5038\u00b10.0053\n0.4625\u00b10.0163q\n-\n-\n-\n0.6352\u00b10.0094\n0.6318\u00b10.0068\nIMDB-40\nACM-1000\n0.4867\u00b10.0013\n0.4320\u00b10.0108\n0.4545\u00b10.0146\n0.5148\u00b10.0043\n0.4872\u00b10.0006\n-\n-\n-\n0.6138\u00b10.0047\n0.6360\u00b10.0051\nTable 3: Ablation study of our HiGPT.\nDatasets\nMetric\ntrain-on\ntest-on\nw/o S1\nw/o HG\nw/o IA\nHiGPT\nSupervised\nMi-F1\nIMDB-1\nIMDB-1000\nfail\n0.3740\n0.4260\n0.5360\nIMDB-3\nIMDB-1000\nfail\n0.5000\n0.4540\n0.5730\nIMDB-10\nIMDB-1000\nfail\n0.5660\n0.4380\n0.5810\nIMDB-20\nIMDB-1000\nfail\n0.5640\n0.5620\n0.6440\nMa-F1\nIMDB-1\nIMDB-1000\nfail\n0.2433\n0.3978\n0.5247\nIMDB-3\nIMDB-1000\nfail\n0.4969\n0.4289\n0.5591\nIMDB-10\nIMDB-1000\nfail\n0.5619\n0.3966\n0.5762\nIMDB-20\nIMDB-1000\nfail\n0.5636\n0.5364\n0.6398\nAUC\nIMDB-1\nIMDB-1000\nfail\n0.5195\n0.6023\n0.6685\nIMDB-3\nIMDB-1000\nfail\n0.6340\n0.6186\n0.6935\nIMDB-10\nIMDB-1000\nfail\n0.6790\n0.5903\n0.6875\nIMDB-20\nIMDB-1000\nfail\n0.6891\n0.6840\n0.7424\nZero-shot\nMi-F1\nIMDB-1\nDBLP-1000\nfail\n0.2980\n0.2800\n0.3500\nIMDB-3\nDBLP-1000\nfail\n0.3430\n0.3180\n0.3660\nIMDB-10\nDBLP-1000\nfail\n0.3640\n0.3140\n0.4020\nIMDB-20\nDBLP-1000\nfail\n0.3920\n0.2800\n0.4180\nMa-F1\nIMDB-1\nDBLP-1000\nfail\n0.2444\n0.2145\n0.2472\nIMDB-3\nDBLP-1000\nfail\n0.2768\n0.2503\n0.2814\nIMDB-10\nDBLP-1000\nfail\n0.3211\n0.2581\n0.3386\nIMDB-20\nDBLP-1000\nfail\n0.3689\n0.1836\n0.3733\nAUC\nIMDB-1\nDBLP-1000\nfail\n0.5275\n0.5035\n0.5406\nIMDB-3\nDBLP-1000\nfail\n0.5422\n0.5286\n0.5524\nIMDB-10\nDBLP-1000\nfail\n0.5636\n0.5269\n0.5777\nIMDB-20\nDBLP-1000\nfail\n0.5834\n0.4995\n0.5907\nwe obtained a variant called \"w/o HG\". Our HiGPT consistently\noutperformed this variant across different shot settings, espe-\ncially in scenarios with limited samples (e.g., 1 or 3 shots). This\nimprovement is attributed to the introduction of graph tokens,\nwhich enable the LLM to extract high-dimensional heterogeneous\nstructural information from the in-context graph tokenizer. This\nenhanced understanding significantly improves the LLM\u2019s accu-\nracy, particularly with sparse supervised signals.\n\u2022 Effect of MoT Instruction Augmentation. To verify the effec-\ntiveness of the MoT graph instruction augmentation strategy, we\ntrained the variant \"-IA\" using only direct-answer instructions.\nResults showed a significant drop in model performance without\ninstruction augmentation, highlighting its importance in tackling\nthe scarcity of labels in downstream tasks. Additionally, HiGPT\u2019s\nsuperior performance in zero-shot settings can be attributed to\nits enhanced reasoning ability, acquired through training with\ndiverse reasoning instructions. This improved capacity enables\neffective cross-dataset and cross-domain transfer.\n4.4\nGraph In-Context Learning (RQ3)\nIn-context learning (ICL)[18] is a method for adapting large lan-\nguage models (LLMs) to new tasks without gradient updates, using\na prompt with task examples. In this subsection, we explore the\nimpact of Graph In-Context Learning on improving HiGPT\u2019s per-\nformance. We conduct comprehensive tests by adding prefatory\nexamples from the training set to models trained with different\nshots of IMDB data. We randomly sampled training examples cor-\nresponding to the test data. \"-ICL-1\" and \"-ICL-2\" denote one and\ntwo prefatory examples, respectively. \"-ICL-DBLP\" signifies the in-\nclusion of DBLP examples before the ACM test prompt. The results,\ndepicted in Figure3, reveal the following observations:\n1-shot Beat 60-shot with Graph ICL in HiGPT. Results show\nthat, even with just a single example, most 1-shot models using\nGraph ICL consistently outperform 60-shot models without further\ntraining in both supervised and zero-shot settings. Increasing the\nnumber of examples enhances the effect of in-context learning. This\nimprovement can be attributed to HiGPT\u2019s two-stage instruction\ntuning process, which enables it to understand and analyze hetero-\ngeneous graph tokens, benefiting downstream tasks. By providing\nquestion-and-answer examples with graph tokens, the model gains\na deeper understanding of the graph-text relationship. Analyzing\nand emulating these examples leads to more accurate responses.\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Mi-F1 on IMDB\n0.6550.653\n0.646\n0.654\n0.655\n0.6640.658\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Ma-F1 on IMDB\n0.6500.648\n0.641\n0.649\n0.653\n0.6610.654\nOurs\nOurs-ICL-1\nOurs-ICL-2\n(a) IMDB-IMDB@Mi-F1, Ma-F1\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n35%\n40%\n45%\n50%\n55%\nTesting Mi-F1 on DBLP\n0.387\n0.469\n0.475\n0.542\n0.510\n0.533\n0.540\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n35%\n40%\n45%\n50%\n55%\nTesting Ma-F1 on DBLP\n0.374\n0.461\n0.467\n0.535\n0.505\n0.528\n0.539\nOurs\nOurs-ICL-1\nOurs-ICL-2\n(b) IMDB-DBLP@Mi-F1, Ma-F1\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Mi-F1 on ACM\n0.634\n0.627\n0.679\n0.620\n0.517\n0.578\n0.634\nOurs\nOurs-ICL-1\nOurs-ICL-DBLP\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Ma-F1 on ACM\n0.631\n0.627\n0.679\n0.619\n0.505\n0.570\n0.629\nOurs\nOurs-ICL-1\nOurs-ICL-DBLP\n(c) IMDB-ACM@Mi-F1, Ma-F1\nFigure 3: Graph In-Context Learning of our HiGPT.\nEnhanced Transferability with our Graph ICL. The advantages\nof our Graph ICL in HiGPT are particularly evident in zero-shot\ntransfer scenarios. This indicates that the Graph ICL approach\nsignificantly improves HiGPT\u2019s transferability without the need to\noptimize model parameters. Our HiGPT does not simply overfit to\na single dataset but develops the ability to analyze text alongside\nheterogeneous graph tokens. By incorporating graph examples\nfrom other datasets, the model effortlessly transfers this analytical\ncapability, demonstrating strong transfer learning capacity.\nBenefit of Irrelevant Graph Examples. We experimented with\nHiGPT with irrelevant graph examples, like using DBLP Q&A ex-\namples for testing on the ACM dataset. Surprisingly, using DBLP\ngraph examples yielded the best results. Despite different target\ntasks, our HiGPT effectively leverages in-context information from\nheterogeneous graph tokens, enhancing downstream tasks. This\nconfirms that our HiGPT learns valuable information from hetero-\ngeneous graph structures, rather than relying solely on text. Using\nACM\u2019s own examples did not perform as well due to a deficiency\nin encoding the ACM graph in the alignment and stage 1 process.\nHowever, the DBLP examples mitigated this issue to some extent.\n4.5\nCase Study\nWe perform a case study to showcase our HiGPT\u2019s robust gener-\nalization in understanding complex graph structures with diverse\nnodes and connections. Our model generates graph-aware predic-\ntions and responses, demonstrating its profound comprehension\nand awareness of graph-related aspects. Furthermore, we validate\nthe positive impact of our MoT instruction augmentation. For more\ncomprehensive details, please refer to Appendix Section A.5.2.\n5\nRELATED WORK\nHeterogeneous Graph Neural Networks. Heterogeneous Graph\nNeural Networks (HGNNs) capture complex relationships and di-\nverse semantics among entities in a heterogeneous graph [2, 6, 44].\nThey use specialized message functions and aggregation rules to\nmodel relation heterogeneity. Existing models, such as MAGNN\n[6] and HetGNN [44], leverage metapaths to capture composite\nrelations and guide neighbor selection. Heterogeneous graph con-\nvolution frameworks like HeteGCN [21] draws inspiration from\ngraph convolutional networks. Heterogeneous graph attention net-\nworks, including HAN [32], HGT [10], and HGAT [14], use attention\nmechanisms to effectively capture and aggregate information from\ndifferent node types. However, most HGNNs require sufficient la-\nbels to learn accurate graph representations.\nHeterogeneous Graph Self-Supervised Learning. Recent re-\nsearch has addressed the limited availability of labeled data by in-\ncorporating self-supervised learning techniques into heterogeneous\ngraph modeling [13, 30]. Contrastive and generative approaches\nhave proven effective in augmenting data. Contrastive learning\nmethods like DMGI [19] and HeCo [34] bring similar instances\ncloser and push dissimilar instances apart in a latent space, captur-\ning relevant patterns and structure. Generative learning approaches\nsuch as HGMAE [27] use masked autoencoders to reconstruct het-\nerogeneous graphs. However, these approaches still have limitations\nin handling relation heterogeneity shift across downstream tasks,\npotentially leading to poor generalization ability.\nLarge Language Models for Graph Data. Recent research has\ncombined large language models (LLMs) and graph models to un-\nderstand complex relationships in graph data [1, 23, 37, 43]. Two\nprimary approaches integrate graph structural information: utiliz-\ning textual prompts and incorporating graph embeddings as input\ntokens. Chen et al. [3] craft tailored prompts for graph learning\ntasks, while InstructGLM [43] and GraphGPT [26] propose to inte-\ngrate prompt instructions with graph embeddings for fine-tuning\nLLM. Moreover, advancements have introduced LLMs to improve\nthe reasoning capabilities of models when working with graph-\nstructured data. Prominent examples include the works of Fatemi et\nal. [7] and Chai et al. [1]. However, existing LLM-enhanced graph\nmodels have primarily focused on homogeneous graphs, overlook-\ning the inherent heterogeneity in real-world graphs. This calls for\nfurther exploration of heterogeneous graph language models with\nstrong generalization abilities across diverse downstream tasks.\n6\nCONCLUSION\nThis work introduce HiGPT, a general and versatile graph model\nthat offers the ability to learn from diverse heterogeneous graphs\nwithout the need for downstream fine-tuning processes. To ad-\ndress distribution shifts in heterogeneity, we propose an in-context\nheterogeneous graph tokenizer that captures semantic relation-\nships across different heterogeneous graphs, facilitating seamless\nmodel adaptation. By incorporating the heterogeneity-aware graph\ninstructions into our HiGPT, the model becomes proficient in com-\nprehending intricate relation heterogeneity and accurately discern-\ning between various types of graph tokens. Our proposed frame-\nwork has undergone extensive evaluations across diverse scenarios,\ndemonstrating outstanding generalization performance.\nREFERENCES\n[1] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang. Graphllm:\nBoosting graph reasoning ability of large language model.\narXiv preprint\narXiv:2310.05845, 2023.\n[2] M. Chen, C. Huang, L. Xia, W. Wei, Y. Xu, and R. Luo. Heterogeneous graph\ncontrastive learning for recommendation. In WSDM, pages 544\u2013552, 2023.\n[3] Z. Chen, H. Mao, H. Li, et al. Exploring the potential of large language models\n(llms) in learning on graphs. CoRR, abs/2307.03393, 2023.\n[4] Y. Dong, N. V. Chawla, and A. Swami. metapath2vec: Scalable representation\nlearning for heterogeneous networks. In KDD, pages 135\u2013144, 2017.\n[5] A. El-Kishky, T. Markovich, S. Park, C. Verma, B. Kim, R. Eskander, Y. Malkov,\nF. Portman, S. Samaniego, Y. Xiao, et al. Twhin: Embedding the twitter heteroge-\nneous information network for personalized recommendation. In KDD, pages\n2842\u20132850, 2022.\n[6] S. Fan, J. Zhu, X. Han, C. Shi, L. Hu, B. Ma, and Y. Li. Metapath-guided het-\nerogeneous graph neural network for intent recommendation. In KDD, pages\n2478\u20132486, 2019.\n[7] B. Fatemi, J. Halcrow, and B. Perozzi. Talk like a graph: Encoding graphs for\nlarge language models. arXiv preprint arXiv:2310.04560, 2023.\n[8] X. Fu, J. Zhang, Z. Meng, and I. King. MAGNN: metapath aggregated graph\nneural network for heterogeneous graph embedding. In WWW, pages 2331\u20132341.\nACM / IW3C2, 2020.\n[9] W. L. Hamilton, Z. Ying, and J. Leskovec. Inductive representation learning on\nlarge graphs. In NeurIPS, pages 1024\u20131034, 2017.\n[10] Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph transformer. In\nWWW, pages 2704\u20132710. ACM / IW3C2, 2020.\n[11] D. Hwang, J. Park, S. Kwon, K. Kim, J.-W. Ha, and H. J. Kim. Self-supervised\nauxiliary learning with meta-paths for heterogeneous graphs. NeurIPS, 33:10294\u2013\n10305, 2020.\n[12] D. Jin, C. Huo, C. Liang, and L. Yang. Heterogeneous graph neural network via\nattribute completion. In WWW, pages 391\u2013400, 2021.\n[13] B. Jing, S. Feng, Y. Xiang, X. Chen, Y. Chen, and H. Tong. X-goal: multiplex\nheterogeneous graph prototypical contrastive learning. In CIKM, pages 894\u2013904,\n2022.\n[14] H. Linmei, T. Yang, C. Shi, H. Ji, and X. Li. Heterogeneous graph attention\nnetworks for semi-supervised short text classification. In EMNLP, pages 4821\u2013\n4830, 2019.\n[15] J. Liu, A. Liu, X. Lu, S. Welleck, P. West, R. L. Bras, Y. Choi, and H. Hajishirzi.\nGenerated knowledge prompting for commonsense reasoning. In ACL (1), pages\n3154\u20133169. Association for Computational Linguistics, 2022.\n[16] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, and\nJ. Tang. Are we really making much progress? revisiting, benchmarking and\nrefining heterogeneous graph neural networks. In KDD, pages 1150\u20131160, 2021.\n[17] A. Ma, X. Wang, J. Li, C. Wang, T. Xiao, Y. Liu, H. Cheng, J. Wang, Y. Li, Y. Chang,\net al. Single-cell biological network inference using a heterogeneous graph\ntransformer. Nature Communications, 14(1):964, 2023.\n[18] S. Min, X. Lyu, A. Holtzman, M. Artetxe, M. Lewis, H. Hajishirzi, and L. Zettle-\nmoyer. Rethinking the role of demonstrations: What makes in-context learning\nwork? In EMNLP, pages 11048\u201311064, 2022.\n[19] C. Park, D. Kim, J. Han, and H. Yu. Unsupervised attributed multiplex network\nembedding. In AAAI, volume 34, pages 5371\u20135378, 2020.\n[20] A. Radford, J. W. Kim, C. Hallacy, et al. Learning transferable visual models from\nnatural language supervision. In International Conference on Machine Learning\n(ICML), pages 8748\u20138763. PMLR, 2021.\n[21] R. Ragesh, S. Sellamanickam, A. Iyer, R. Bairi, and V. Lingam. Hetegcn: hetero-\ngeneous graph convolutional networks for text classification. In WSDM, pages\n860\u2013868, 2021.\n[22] N. Reimers and I. Gurevych. Sentence-bert: Sentence embeddings using siamese\nbert-networks. In EMNLP. Association for Computational Linguistics, 11 2019.\n[23] X. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang. Represen-\ntation learning with large language models for recommendation. arXiv preprint\narXiv:2310.15950, 2023.\n[24] K. Shridhar, A. Stolfo, and M. Sachan. Distilling reasoning capabilities into smaller\nlanguage models. In ACL, pages 7059\u20137073, 2023.\n[25] H. Sun, A. H\u00fcy\u00fck, and M. van der Schaar. Query-dependent prompt evaluation\nand optimization with offline inverse rl. arXiv e-prints, pages arXiv\u20132309, 2023.\n[26] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and C. Huang. Graphgpt:\nGraph instruction tuning for large language models, 2023.\n[27] Y. Tian, K. Dong, C. Zhang, C. Zhang, and N. V. Chawla. Heterogeneous graph\nmasked autoencoders. In AAAI, volume 37, pages 9997\u201310005, 2023.\n[28] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, et al. Graph attention\nnetworks. In ICLR (Poster). OpenReview.net, 2018.\n[29] H. Wang, H. Ren, and J. Leskovec. Relational message passing for knowledge\ngraph completion. In KDD, pages 1697\u20131707, 2021.\n[30] P. Wang, K. Agarwal, C. Ham, S. Choudhury, and C. K. Reddy. Self-supervised\nlearning of contextual embeddings for link prediction in heterogeneous networks.\nIn WWW, pages 2946\u20132957, 2021.\n[31] X. Wang, D. Bo, C. Shi, S. Fan, Y. Ye, and S. Y. Philip. A survey on heterogeneous\ngraph embedding: methods, techniques, applications and sources. Transactions\non Big Data (TBD), 9(2):415\u2013436, 2022.\n[32] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, P. Cui, and P. S. Yu. Heterogeneous graph\nattention network. In WWW, pages 2022\u20132032, 2019.\n[33] X. Wang, H. Ji, C. Shi, B. Wang, Y. Ye, et al. Heterogeneous graph attention\nnetwork. In WWW, pages 2022\u20132032. ACM, 2019.\n[34] X. Wang, N. Liu, H. Han, and C. Shi. Self-supervised heterogeneous graph neural\nnetwork with co-contrastive learning. In KDD, pages 1726\u20131736, 2021.\n[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. H. Chi, Q. V. Le,\nand D. Zhou. Chain-of-thought prompting elicits reasoning in large language\nmodels. In NeurIPS, 2022.\n[36] W. Wei, C. Huang, L. Xia, Y. Xu, J. Zhao, and D. Yin. Contrastive meta learning\nwith behavior multiplicity for recommendation. In WSDM, pages 1120\u20131128,\n2022.\n[37] W. Wei, X. Ren, J. Tang, Q. Wang, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang.\nLlmrec: Large language models with graph augmentation for recommendation.\narXiv preprint arXiv:2311.00423, 2023.\n[38] Z. Wen and Y. Fang. Augmenting low-resource text classification with graph-\ngrounded pre-training and prompting. In SIGIR, 2023.\n[39] H. Xuan, Y. Liu, B. Li, and H. Yin. Knowledge enhancement for contrastive\nmulti-behavior recommendation. In WSDM, pages 195\u2013203, 2023.\n[40] C. Yang, Y. Xiao, Y. Zhang, Y. Sun, and J. Han. Heterogeneous network represen-\ntation learning: A unified framework with survey and benchmark. Transactions\non Knowledge and Data Engineering (TKDE), 34(10):4854\u20134873, 2020.\n[41] Y. Yang, Z. Guan, Z. Wang, W. Zhao, C. Xu, W. Lu, and J. Huang. Self-supervised\nheterogeneous graph pre-training based on structural clustering.\nNeurIPS,\n35:16962\u201316974, 2022.\n[42] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree\nof thoughts: Deliberate problem solving with large language models. CoRR,\nabs/2305.10601, 2023.\n[43] R. Ye, C. Zhang, R. Wang, S. Xu, and Y. Zhang. Natural language is all a graph\nneeds. arXiv preprint arXiv:2308.07134, 2023.\n[44] C. Zhang, D. Song, C. Huang, A. Swami, and N. V. Chawla. Heterogeneous graph\nneural network. In KDD, pages 793\u2013803. ACM, 2019.\n[45] J. Zhao, X. Wang, C. Shi, B. Hu, G. Song, and Y. Ye. Heterogeneous graph structure\nlearning for graph neural networks. In AAAI, volume 35, pages 4697\u20134705, 2021.\nA\nAPPENDIX\nIn the supplementary materials, we provide detailed information\npertaining to our experiments. First, in Section A.1, we present the\nstatistical information of the datasets used. Next, in Section A.2, we\nprovide a comprehensive description of the baselines employed. In\nSection A.3, we outline the specific descriptions of nodes and edges\nin our text-enriched heterogeneity representations. Section A.4 elab-\norates on the templates for instructions and prompts used in our\nHiGPT, including a detailed explanation of MoT graph instruction\naugmentation, an instruction construction template for two-stage\ninstruction tuning, and an instruction construction template for\ngraph in-context learning. Additionally, Section A.5 presents addi-\ntional experimental results, covering implementation details of our\nmodel, model case studies, overall performance, and comprehensive\nresults of graph in-context learning.\nA.1\nDetailed Statistics of Datasets\nIn Table 4, we present the statistical information of the datasets\nused in our experiments, where the types of the target nodes in\neach heterogeneous graph are highlighted with an underline.\nTable 4: Detailed statistics of utilzed datasets.\nDataset\n# Nodes\n# Edges\nMetapaths\n# Classes\nACM\nPaper: 3025\nP-A: 9949\nPAP\n3\nAuthor: 5959\nP-S: 3025\nPSP\nSubject: 56\nP-T: 255619\nPTP\nTerm: 1902\nP-P: 5343\nDBLP\nAuthor: 4057\nP-A: 19645\nPAP\n4\nPaper: 14328\nP-C: 14328\nAPCPA\nTerm: 7723\nP-T: 85810\nAPTPA\nConference: 20\nIMDB\nMovie: 4278\nM-D: 4278\nMAM\n3\nDirector: 2081\nM-A: 12828\nMDM\nActor: 5257\nA.2\nDetailed Descriptions of Baselines\nTo conduct a thorough evaluation, our baseline set includes the\nfollowing methods, which are presented below:\n1) Homogeneous Graph Neural Networks\n\u2022 SAGE [9]:It was developed to facilitate the learning of inductive\nrepresentations on large-scale homogeneous graphs, allowing for\nthe generation of highly effective node embeddings for unseen\ndata. However, we made adaptations and modifications to tailor\nit specifically to the unique demands of heterogeneous graphs.\n\u2022 GAT [28]: It utilizes graph attention networks, which overcome\nthe limitations of graph convolutional networks (GCN) by in-\ncorporating masked self-attention layers. What sets this method\napart is its ability to selectively weigh the aggregated information\nfrom different nodes, thereby enhancing the message passing\nmechanism and refining the overall process.\n2) Heterogeneous Graph Neural Encoders.\n\u2022 HAN [33]: It is a heterogeneous graph neural encoder that incor-\nporates hierarchical attention mechanisms, including node-level\nand semantic-level attentions, to address the complexity of het-\nerogeneous graphs with various types of nodes and links, thereby\nimproving the representation and interpretability of node embed-\ndings through feature aggregation based on meta-path neighbors.\n\u2022 HGT [10]: HGT is an advanced graph neural network framework\ndesigned to model the complexities of large-scale heterogeneous\ngraphs, featuring type-dependent parameters for nodes and edges\nto enable heterogeneous attention, a relative temporal encoding\nto capture dynamic relationships, and an efficient graph sampling\nalgorithm for scalable training.\n\u2022 HetGNN [44]: HetGNN is a powerful heterogeneous graph neu-\nral network model that seamlessly integrates both the structural\ninformation and diverse content attributes of nodes. It achieves\nthis by employing a two-module architecture for feature aggre-\ngation and incorporating a well-designed random walk sampling\nprocess. This comprehensive approach enables HetGNN to gen-\nerate meaningful and informative node embeddings.\n3) SSL-enhanced Heterogeneous Graph Learning Approaches.\n\u2022 HGMAE [27]: HGMAE is a generative SSL approach that ad-\ndresses the challenges of capturing complex structures, incor-\nporating diverse node attributes, and encoding node positions\nin heterogeneous graphs. It achieves this through innovative\nmasking techniques and tailored training strategies, utilizing a\nheterogeneous graph masked autoencoder. HGMAE efficiently\nlearns to generate meaningful representations while effectively\npreserving the rich information present in heterogeneous graphs.\n\u2022 DMGI [19]: It is an effective unsupervised network embedding\nmethod for attributed multiplex networks that maximizes mutual\ninformation between graph patches and a global representation,\nintegrating multiple relation-type embeddings with a consensus\nregularization framework and a universal discriminator, further\nenhanced by an attention mechanism to weigh relation types.\n\u2022 HeCo [34]: This is a self-supervised heterogeneous graph neu-\nral network framework that employs a co-contrastive learning\nmechanism across two views (network schema and meta-path) to\ncapture both local and high-order structures, with a view mask\nmechanism for effective cross-view supervision, enhanced by\nextensions for generating higher-quality negative samples.\nA.3\nDetailed Descriptions of Text-Enriched\nHeterogeneity Representations\nIn Table 5 and 6, we present diverse descriptions for different node\nand edge types across three datasets, i.e., IMDB, DBLP and ACM.\nA.4\nInstruction and Prompting Templates\nA.4.1\nDetailed prompt of MoT graph instruction augmenta-\ntion. In Table 7, we display all MoT graph instruction augmentation\nstrategies that incorporate various prompt engineering techniques,\nincluding the prompting template to prompt GPT-3.5, and the tem-\nplate for constructing Instructions after obtaining the results.\nA.4.2\nInstruction construction template for two stage in-\nstruction tuning. In Table 8, we showcase the instruction tem-\nplate for the second stage of node classification for IMDB, where\nthe prompting suffix allows our instruction to be combined with\na variety of different prompt techniques for instruction tuning,\nthereby distilling multiple reasoning abilities for the powerful GPT-\n3.5. Table 9 presents two types of instruction templates for the first\nTable 5: Detailed Descriptions of Text-Enriched Heterogeneity Representations for IMDB and DBLP.\nNode (Edge) Type\nSets of Descriptions\n(a) IMDB\n\"Movie\"\n{\"This node represents a movie\",\"This is an action movie\",\"This is a comedy movie\",\"This is a drama movie\"}\n\"Director\"\n{\"This node represents a director\", \"This is an action film director\", \"The director specializes in action\", \"This is a comedy\nfilm director\", \"The director specializes in comedy\", \"This is a drama film director\", \"The director specializes in dram\"}\n\"Actor\"\n{\"This node represents an actor\", \"This is an action film actor\", \"The actor specializes in action\", \"This is a comedy film\nactor\", \"The actor specializes in comedy\", \"This is a drama film actor\", \"The actor specializes in drama\"}\n(\"movie\", \"to\", \"director\")\n{\"The movie is directed by the director\", \"The film features direction by the director\", \"The movie\u2019s direction was in the\nhands of the director\", \"The movie was helmed by the director\", \"The film is a directorial effort by the director\", \"The\nmovie bears the directorial signature of the director\"}\n(\"movie\", \"to\", \"actor\")\n{\"The movie has the actor\", \"The movie features the actor\", \"The film includes the actor in its lineup\", \"The movie\nshowcases the talent of the actor\", \"The film\u2019s cast includes the actor\", \"The movie presents the actor on its roster\"}\n(\"director\", \"to\", \"movie\")\n{\"This director is responsible for the film\u2019s direction\", \"The director take the helm for the movie\", \"The director steers\nthe production of the movie\", \"The director in question crafts the narrative of the film\", \"The director provides the\ncreative direction for the film\", \"The director orchestrates the making of the movie\"}\n(\"actor\", \"to\", \"movie\")\n{\"The actor appears in the movie\", \"The actor is part of the movie\u2019s cast\", \"The actor stars in the movie\", \"The actor is\nfeatured in the film\", \"The actor has a role in the movie\"}\n(b) DBLP\n\"paper\"\n{\"This node represents a paper\", \"A paper in the area of Database\", \"A paper in the area of Data Mining\", \"A paper in\nthe area of AI\", \"A paper in the area of Information Retrieval\", \"A paper published in a conference\", \"A paper in the area\nof computer science\"}\n\"conference\"\n{\"This node represents a conference\", \"A conference in the area of Database\", \"A conference in the area of Data Mining\",\n\"A conference in the area of AI\", \"A conference in the area of Information Retrieval\", \"A conference about computer\nscience\"}\n\"author\"\n{\"This node represents an author\", \"An author in the area of Database\", \"An author in the area of Data Mining\", \"An\nauthor in the area of AI\", \"An author in the area of Information Retrieval\", \"An author in the area of computer science\"}\n\"term\"\n{\"This node represents a key term related to a paper\", \"The term is included in a paper\", \"The term is related to Database\",\n\"The term is related to Data Mining\", \"The term is related to AI\", \"The term is related to Information Retrieval\"}\n(\"author\", \"to\", \"paper\")\n{\"The author has the paper\", \"The author publishes the paper\", \"The author writes the paper\", \"The author is the author\nof the paper\", \"The author releases the paper\", \"The author issues the paper\", \"The author disseminates the paper\"}\n(\"paper\", \"to\", \"author\")\n{\"The paper has the author\", \"The paper is published by the author\", \"The paper is written by the author\", \"The paper\nlists the author\", \"The paper is put forth by the author\", \"The paper is made public by the author\"}\n(\"paper\", \"to\", \"term\")\n{\"The paper has the term\", \"The paper includes the term\", \"The paper contains the term\", \"The paper encompasses the\nterm\", \"The paper lists the term\"}\n(\"paper\",\n\"to\",\n\"confer-\nence\")\n{\"The paper is published in the conference\", \"The paper is accepted by the conference\", \"The paper is included in the\nproceedings of the conference\", \"The paper is presented at the conference\", \"The paper appears in the conference\nproceedings\", \"The paper is part of the conference\u2019s official record.\", \"The paper makes its debut at the conference\",\n\"The paper is documented in the conference\u2019s scholarly collection\"}\n(\"term\", \"to\", \"paper\")\n{\"The term is related to the paper\", \"The term is included in the paper\", \"The term is in the paper\", \"The term is featured\nin the paper\", \"The term is incorporated into the paper\", \"The term is part of the paper\u2019s content\", \"The term is found\nwithin the paper\", \"The term appears in the paper\"}\n(\"conference\", \"to\", \"pa-\nper\")\n{\"The conference has the paper\", \"The conference includes the paper\", \"The conference accepts this paper\", \"The\nproceedings of the conference includes the paper\", \"The conference features the publication of the paper\", \"The\nconference includes the paper in its publications\", \"The conference serves as the platform for the paper\u2019s publication\"}\nIMDB-ICL-1:\nQ: Given a heterogeneous graph about internet movie\u2026 {Human Question}\nA: {Ground Truth Answer&Reasoning}\nQ: Given a heterogeneous graph about internet movie\u2026 {Human Question}\nIMDB-ICL-2:\nQ: Given a heterogeneous graph about internet movie\u2026 {Human Question}\nA: {Ground Truth Answer&Reasoning}\nQ: Given a heterogeneous graph about internet movie\u2026 {Human Question}\nA: {Ground Truth Answer&Reasoning}\nQ: Given a heterogeneous graph about internet movie\u2026 {Human Question}\nACM-ICL-DBLP:\nQ: Given a heterogeneous academic network graph about computer science \nfrom DBLP website \u2026 {Human Question}\nA: {Ground Truth Answer&Reasoning}\nQ: Given a heterogeneous academic network graph about computer science \ncollected from ACM website\u2026 {Human Question}\nFigure 4: Instruction construction template for graph in-\ncontext learning, including \"ICL-1\", \"ICL-2\" and \"ICL-DBLP\".\nstage, targeting heterogeneous relation awareness and homoge-\nneous relation awareness, respectively. The construction method\nfor other datasets is the same as that for IMDB.\nA.4.3\nInstruction construction template for graph in-context\nlearning. In Figure 4, we illustrate the construction of instructions\nfor our graph in-context learning tests, which includes \"-ICL-1\"\nTable 6: Detailed Descriptions of Text-Enriched Heterogeneity Representations for ACM (Continued).\nNode (Edge) Type\nSets of Descriptions\n(c) ACM\n\"paper\"\n{\"This node represents a paper\", \"A paper in the area of Database\", \"A paper in the area of Wireless Communication\", \"A\npaper in the area of Data Mining\", \"A paper published in a conference (one of KDD, SIGMOD, SIGCOMM, MobiCOMM,\nand VLDB)\", \"A paper in the area of computer science\"}\n\"subject\"\n{\"This node represents a subject\", \"The subject is related to Database\", \"The subject is related to Wireless Communication\",\n\"The subject is related to Data Mining\", \"The subject is related to computer science\"}\n\"author\"\n{\"This node represents an author\", \"An author in the area of Database\", \"An author in the area of Data Mining\", \"An\nauthor in the area of AI\", \"An author in the area of Information Retrieval\", \"An author in the area of computer science\"}\n\"term\"\n{\"This node represents a key term related to a paper\", \"The term is included in a paper\", \"The term is related to Database\",\n\"The term is related to Wireless Communication\", \"The term is related to Data Mining\", \"The term is related to computer\nscience\"}\n(\"author\", \"to\", \"paper\")\n{\"The author has the paper\", \"The author publishes the paper\", \"The author writes the paper\", \"The author is the author\nof the paper\", \"The author releases the paper\", \"The author issues the paper\", \"The author disseminates the paper\"}\n(\"paper\", \"to\", \"author\")\n{\"The paper has the author\", \"The paper is published by the author\", \"The paper is written by the author\", \"The paper\nlists the author\", \"The paper is put forth by the author\", \"The paper is made public by the author\"}\n(\"paper\", \"to\", \"term\")\n{\"The paper has the term\", \"The paper includes the term\", \"The paper contains the term\", \"The paper encompasses the\nterm\", \"The paper lists the term\"}\n(\"term\", \"to\", \"paper\")\n{\"The term is related to the paper\", \"The term is included in the paper\", \"The term is in the paper\", \"The term is featured\nin the paper\", \"The term is incorporated into the paper\", \"The term is part of the paper\u2019s content\", \"The term is found\nwithin the paper\", \"The term appears in the paper\"}\n(\"paper\", \"to\", \"subject\")\n{\"The paper pertains to the subject\", \"The paper is concerned with the subject\", \"The paper addresses this subject\u2019s\nmatter\", \"The paper contributes to the discourse on the subject\", \"The paper explores the subject in depth\", \"The paper\nexamines the subject\", \"The paper is dedicated to the analysis of the subject\", \"The paper\u2019s content is relevant to the\nsubject\", \"The paper provides insights into the subject\", \"The paper discusses the subject comprehensively\"}\n(\"subject\", \"to\", \"paper\")\n{\"The subject serves as the focus for the paper\", \"The subject is the central theme of the paper\", \"The subject forms\nthe basis of the paper\u2019s inquiry\", \"The subject underpins the scholarly work presented in the paper\", \"The subject\ninforms the paper\u2019s research focus\", \"The subject delineates the scope of the paper\u2019s investigation\", \"The subject is the\ncornerstone of the paper\u2019s theoretical foundation\"}\n(\"paper\", \"cite\", \"paper\")\n{\"The paper cites the paper\", \"The paper includes references to a previous paper\", \"The manuscript references earlier\nresearch in its bibliography\", \"The study attributes findings to an antecedent scholarly paper\", \"The paper assimilates\ninsights from a previously published study\", }\n(\"paper\", \"ref\", \"paper\")\n{\"The paper is cited by the paper\", \"The paper receives a citation from a subsequent publication\", \"The article is\nreferenced within the bibliography of another scholarly work\", \"The manuscript is acknowledged by another study in\nits references\", \"The document is listed in the citations of another academic article\"}\nwith one example, \"-ICL-2\" with two examples, and \"-ICL-DBLP\"\nwhere examples from ACM are concatenated with DBLP exam-\nples. We represent the examples and the final question using a\nQ:...A:...Q:... sequence.\nA.5\nSupplementary Experimental Results\nA.5.1\nImplementation Details. In the implementation of our\nHiGPT, we employ heterogeneous graph data with corresponding\ntextual contents from IMDB and DBLP to conduct text-graph con-\ntrastive alignment and obtain a heterogeneous graph tokenizer. In\nthe first phase, we utilize a heterogeneous graph corpus consisting\nof IMDB and DBLP for instruction tuning. We set the batch size to\n1 per GPU and train for 1 epoch with the learning rate 2\ud835\udc52\u22125, the\nwarmup ratio 3\ud835\udc52\u22122 and the weight decay 1\ud835\udc52\u22124. The projector ob-\ntained from the first phase training is used as the initial state for the\nsecond phase, where we set the epochs to 15 and further perform\ninstruction tuning on downstream tasks. The base model used in\nboth stages is vicuna-7B-v1.5, with the maximum context length set\nto 2048. And For the evaluation of most baselines, we utilize their\npublicly available code. We employ a grid-search strategy based\non default hyperparameter settings to ensure a fair evaluation. For\nfurther details, please refer to our released source code.\nA.5.2\nModel Case Study. In this subsection, we explore the be-\nhavior of our HiGPT under different prompting techniques. Specifi-\ncally, we utilize various prompting techniques to prompt the 10-shot\nIMDB model, obtaining six different responses, and the prediction\ncases for different categories of HiGPT are shown in Tables 10, 11,\nand 12, respectively. The parts showing the final answers are high-\nlighted in pink. We make the following observations: Obs.1 Our\nHiGPT, after instruction tuning with the MoT graph instruction aug-\nmentation strategy, can dynamically respond accurately to different\nprompts. Obs.2 The CoT prompt in Table 10, which is unformat-\nted, also shows a certain format (highlighted in yellow), which is\nattributed to the fact that mixing a variety of instructions can also\nbenefit different prompting techniques. Obs.3 As highlighted in\ngreen in multiple cases, our HiGPT, after our designed two-stage\ngraph instruction tuning, is consciously considering issues from\na graph perspective, further proving that our model is not only\nTable 7: Detailed Prompt of Mixture-of-Thought (MoT) Graph Instruction Augmentation.\nPrompt Engineering\nPrompting Template\nAugmented Instruction Tem-\nplate\nCoT without Format\nConstraint\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nimagine that you have made the correct choice and proceed with step-by-step reasoning.\nExample: Data mining. Based on ...\n{Human Question} \u2212\u2192{GPT\u2019s\nAnswer&Reasoning}\nCoT\nwith\nFormat\nConstraint\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nimagine that you have made the correct choice and proceed with step-by-step reasoning.\nUsing the following format: Answer: [Answer] Reason: ...\n{Human Question} \u2212\u2192{GPT\u2019s\nAnswer&Reasoning}\nToT with Multiple\nRound\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nimagine three different experts are answering this question. All experts will write down\n1 step of their thinking, then share it with the group. Then all experts will go on to the\nnext step, etc. If any expert realises they\u2019re wrong at any point then they leave. And\nfinally they make the correct choice. Using the following format: Expert 1:... Expert 2:...\nExpert 3:... Expert 1:... Expert 2:... Expert 3:... Final Answer:...\n{Human Question} \u2212\u2192{GPT\u2019s\nAnswer&Reasoning}\nToT\nwith\nSingle\nRound\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nimagine three different experts are answering this question. All experts will write down\n1 step of their thinking, then share it with the group. Then all experts will go on to the\nnext step, etc. If any expert realises they\u2019re wrong at any point then they leave. And\nfinally they make the correct choice. Using the following format: Expert 1:... Expert 2:...\nExpert 3:... Final Answer:...\n{Human Question} \u2212\u2192{GPT\u2019s\nAnswer&Reasoning}\nPanelGPT\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nimagine that 3 experts are discussing the question with a panel discussion, trying to\nsolve it step by step to make sure the result is correct and avoid penalty. And finally\nthey make the correct choice.\n{Human Question} \u2212\u2192{GPT\u2019s\nAnswer&Reasoning}\nGKP-1\nI have a question as below: {Human Question} ; and the answer is {Ground Truth},\nplease generate some knowledge that can assist in formulating an answer, including,\nbut not limited to: distinctions between the four categories. Imagine that you have\narrived at the correct answer based on the provided information and knowledge, and\npresent a step-by-step reasoning. Using the following format: Knowledge: ... Answer: ...\nReason: ...\n{Human\nQuestion}\n+{GPT\u2019s\nKnowledge} \u2212\u2192{GPT\u2019s An-\nswer&Reasoning}\nGKP-2\nPlease generate some knowledge that can assist in formulating an answer, including, but\nnot limited to: explanations of some technical terms present in the given information.\nImagine that you have arrived at the correct answer based on the provided information\nand knowledge, and present a step-by-step reasoning. Using the following format:\nKnowledge: ... Answer: ... Reason: ...\n{Human\nQuestion}\n+{GPT\u2019s\nKnowledge} \u2212\u2192{GPT\u2019s An-\nswer&Reasoning}\nsolving downstream problems from a textual perspective but has\nalso developed a certain level of graph-awareness.\nA.5.3\nComprehensive Results of Overall Performance Com-\nparison. Table 13 showcases the results of our HiGPT in both\nfew-shot and zero-shot settings, covering scenarios with 1, 3, 5, 10,\n20, 40, and 60 shots. The results clearly indicate that our model\nconsistently outperforms state-of-the-art models in all cases.\nA.5.4\nComprehensive Results of Graph In-Context Learning.\nThe performance of our model on all metrics, across different shot\nscenarios and datasets, under the graph in-context learning is illus-\ntrated in Figure 5. The results clearly demonstrate that our graph\nin-context learning approach significantly improves the model\u2019s\nperformance without any modifications to the model parameters.\nTable 8: Instruction template for IMDB.\nNode Classification\nInput: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) between\ndifferent nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movie\nnode, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents a moive with\nthe following information: Name: {movie name} Director\u2019s name: {director name} Actors\u2019 names: {actor name} Plot keywords: {plot keywords} \"actor\"\nnodes: <graph>; \"director\" nodes: <graph>. Question: Which of the following classes does this movie belong to: action, comedy, drama? {Prompting Suffix}\nOutput: {Answer (& Reasoning)}.\nPrompting Suffix of Different Prompting Techniques\nStandard with the direct answer: Give likely categories directly.\nCoT without Format Constraint: Please think about the categorization in a step by step manner and avoid making false associations. Then provide your\nreasoning.\nCoT with Format Constraint: Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.\nUsing the following format: Answer: [The answer] Reason: ...\nToT: Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all\nexperts will go on to the next step, etc. If any expert realises they\u2019re wrong at any point then they leave.\nPanelGPT: 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty.\nTable 9: Instruction template for IMDB (Continued).\nInstruction for Heterogeneous Relation Awareness\nInput: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) between\ndifferent nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movie\nnode, a heterogeneous subgraph is obtained. In the subgraph, there are several sequences of heterogeneous graph nodes of different types as follows:1. <graph> 2.\n<graph> 3. <graph> Question: please sequentially provide the categories for the node sequences. Example: 1. actor 2. movie 3. director.\nOutput: {The correct sequence of node types}.\nInstruction for Homogeneous Relation Awareness\nGiven a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths) between different\nnodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the target movie node, a\nheterogeneous subgraph is obtained. In the subgraph, there are \"movie\" nodes: <graph>. Here is a list of movies\u2019 names, their directors\u2019 names, their actors\u2019\nnames and their plot keywords: {movie lists} Question: please reorder the list of movies according to the order of graph tokens (i.e., complete the matching of\ngraph tokens and movies)\nOutput: {The correct sequence of movies\u2019 names}.\nTable 10: Visualization of our HiGPT\u2019s response with different prompting engineering techniques on IMDB for action genre.\nQuestion: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths)\nbetween different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the\ntarget movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents a\nmovie with the following information: Name: Live Free or Die Hard; Director\u2019s name: Len Wiseman; Actors\u2019 names: Bruce Willis|Jonathan Sadowski|Cyril\nRaffaelli; Plot keywords: fbi|hacker|independence day|police|terrorist; \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the following\nclasses does this movie belong to: action, comedy, drama?\nGround-Truth Category: Action\n+ Give likely categories directly. HiGPT (Direct Response): Action.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.\nHiGPT (CoT Response without Format Constraint): Answer: Action Reasoning: The movie \"Live Free or Die Hard\" has several plot keywords that\nsuggest an action genre, such as \"fbi\", \"hacker\", \"independence day\", \"police\", and \"terrorist\". These keywords imply that the movie is likely to have a lot of\naction scenes, chases, and possibly some thrilling moments. Additionally, the director Len Wiseman has directed other action movies in the past, such as\n\"Underworld\" and \"Total Recall\". This further supports the idea that \"Live Free or Die Hard\" belongs to the action genre. Finally, the actors in the movie also\ncontribute to the action genre. Bruce Willis, who plays the lead role, is known for his roles in action movies such as \"Die Hard\", \"Pulp Fiction\", and \"The Fifth\nElement\". Jonathan Sadowski and Cyril Raffaelli, who also appear in the movie, have acted in other action movies and TV shows. Overall, based on the plot\nkeywords, the director\u2019s past work, and the actors\u2019 roles, it is reasonable to conclude that \"Live Free or Die Hard\" belongs to the action genre.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the following\nformat: Answer: [The answer] Reason: ...\nHiGPT (CoT Response with Format Constraint): Answer: Action Reason: Based on the plot keywords provided, the movie seems to involve elements\nof action, such as the FBI, hackers, police, and terrorists. These keywords suggest that the movie may have a high level of action and suspense, which is a\ncommon characteristic of action movies. Additionally, the presence of Bruce Willis, a well-known action star, further supports the classification of this movie\nas an action film.\n+ Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all\nexperts will go on to the next step, etc. If any expert realizes they\u2019re wrong at any point then they leave.\nHiGPT (ToT Response): Expert 1: First, I will analyze the plot keywords of the movie. The keywords \"fbi\", \"hacker\", \"independence day\", \"police\", and\n\"terrorist\" suggest that the movie might belong to the action genre. However, the presence of the word \"independence day\" could also indicate that the movie\nmight be a combination of action and drama. Expert 2: Next, I will look at the director\u2019s name. Len Wiseman is known for directing the \"Underworld\" film\nseries, which are primarily action movies. Therefore, it is likely that \"Live Free or Die Hard\" belongs to the action genre. Expert 3:Finally, I will examine the\nactors\u2019 names. Bruce Willis is an action star, and Jonathan Sadowski and Cyril Raffaelli are relatively unknown actors. Therefore, it is more likely that the\nmovie belongs to the action genre. Based on the analysis of the plot keywords, the director\u2019s name, and the actors\u2019 names, it is likely that \"Live Free or Die\nHard\" belongs to the action genre.\n+ 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty\nHiGPT (Panel Discussion Response): Expert 1: Let\u2019s start by analyzing the plot keywords. The keywords \"fbi\", \"hacker\", \"independence day\", \"police\", and\n\"terrorist\" suggest that this movie might belong to the action genre. Expert 2: That\u2019s a good point. However, we should also consider the director\u2019s previous\nworks. Len Wiseman is known for directing the \"Underworld\" series, which are primarily action movies. Therefore, it\u2019s likely that this movie also belongs to\nthe action genre. Expert 3: I agree with both of you. However, we should also consider the actors\u2019 names. Bruce Willis is known for his roles in action movies,\nand Jonathan Sadowski and Cyril Raffaelli are relatively unknown actors. Therefore, it\u2019s possible that this movie might also belong to the drama genre. Expert\n1: That\u2019s a good point. We should consider all possible genres. Let\u2019s perform a random sampling of 2-hop 10 neighbors centered on the target movie node to\nobtain a heterogeneous subgraph. Expert 2: Great idea. Let\u2019s see what we can find. Expert 3: Agreed. After performing the random sampling, the experts\nfound that the movie \"Live Free or Die Hard\" belongs to the action genre.\nTable 11: Visualization of our HiGPT\u2019s response with different prompting engineering techniques on IMDB for comedy genre.\nQuestion: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta\npaths) between different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered\non the target movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that\nrepresents a movie with the following information: Name: Caddyshack; Director\u2019s name: Harold Ramis; Actors\u2019 names: Bill Murray|Rodney Dangerfield|Brian\nDoyle-Murray; Plot keywords: caddy|country club|golf course|gopher|nudity; \"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the\nfollowing classes does this movie belong to: action, comedy, drama?\nGround-Truth Category: Comedy\n+ Give likely categories directly. HiGPT (Direct Response): Comedy.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.\nHiGPT (CoT Response without Format Constraint): Based on the information provided in the subgraph, the movie \"Caddyshack\" belongs to the comedy\ngenre. The plot keywords \"caddy|country club|golf course|gopher|nudity\" suggest that the movie is a comedy that involves a golf course, caddy, and potentially\nsome nudity. Additionally, the director Harold Ramis is known for his work in comedy films, such as \"Groundhog Day\" and \"Ghostbusters.\" To further support\nthe classification of \"Caddyshack\" as a comedy, we can compare it to other movies in the same subgraph that are also directed by Harold Ramis. For example,\n\"Groundhog Day\" is also a comedy, and it shares some of the same actors, such as Bill Murray, with \"Caddyshack.\" Therefore, it is reasonable to assume that\n\"Caddyshack\" also belongs to the comedy genre. In summary, the movie \"Caddyshack\" belongs to the comedy genre based on its plot keywords, director\nHarold Ramis\u2019 previous work in comedy films, and its similarity to other comedies in the same subgraph.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the following\nformat: Answer: [The answer] Reason: ...\nHiGPT (CoT Response with Format Constraint): Answer: Comedy Reason: The movie \"Caddyshack\" is a comedy film, as indicated by its plot keywords\nsuch as \"caddy\", \"country club\", \"golf course\", and \"gopher\", which suggest a light-hearted and humorous tone. Additionally, the director Harold Ramis has a\nhistory of directing comedies, further supporting the classification of this movie as a comedy.\n+ Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all\nexperts will go on to the next step, etc. If any expert realizes they\u2019re wrong at any point then they leave.\nHiGPT (ToT Response): Based on the plot keywords, \"caddy,\" \"country club,\" \"golf course,\" and \"nudity,\" it seems like the movie \"Caddyshack\" belongs to\nthe comedy genre. Expert 1: The movie has keywords like \"caddy,\" \"country club,\" and \"golf course,\" which are commonly associated with comedy movies.\nExpert 2: The presence of Bill Murray, Rodney Dangerfield, and Brian Doyle-Murray, who are known for their comedic roles, also suggests that the movie is a\ncomedy. Expert 3: The plot keywords \"nudity\" may be a red herring, as it is not a common characteristic of comedy movies. Therefore, the movie likely\nbelongs to the comedy genre.\n+ 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty\nHiGPT (Panel Discussion Response): To solve this problem, we need to analyze the plot keywords and the director\u2019s name to determine the genre of\nthe movie. The plot keywords suggest that the movie is likely to be a comedy, as it contains keywords such as \"caddy,\" \"country club,\" \"golf course,\" and\n\"gopher,\" which are commonly associated with comedic situations in movies. The director\u2019s name, Harold Ramis, also supports this conclusion. He is known\nfor directing comedies such as \"Groundhog Day\" and \"Ghostbusters,\" which were both critical and commercial successes. Therefore, we can conclude that the\nmovie \"Caddyshack\" belongs to the comedy genre. As for the sorting of the \"actor\" nodes, we can use the plot keywords to determine the order. The actors\u2019\nnames suggest that the movie has a mix of well-known and lesser-known actors, but the plot keywords suggest that the movie is primarily focused on the\nactors\u2019 interactions with the golf course and gopher. Therefore, we can sort the actors\u2019 names based on their prominence in the movie, with Bill Murray and\nRodney Dangerfield being more well-known and therefore higher up in the list, and Brian Doyle-Murray being lesser-known but still important to the plot. So\nthe sorted list of actors\u2019 names would be: 1. Bill Murray 2. Rodney Dangerfield 3. Brian Doyle-Murray As for the sorting of the \"movie\" nodes, we can use the\nplot keywords to determine the order. The plot keywords suggest that the movie is primarily focused on the golf course and gopher, so we can sort the movie\nnodes based on their prominence in the movie, with \"Caddyshack\" being the central node and the other two movies being lesser-known and therefore lower\ndown in the list. So the sorted list of movie nodes would be: 1. Caddyshack 2. Happy Gilmore 3. The Greatest Game Ever Played Therefore, the final sorted list\nof nodes would be: 1. Caddyshack 2. Harold Ramis 3. Bill Murray 4. Rodney Dangerfield 5. Brian Doyle-Murray 6. Happy Gilmore 7. The Greatest Game Ever\nPlayed Note that the order of the \"actor\" nodes and the \"movie\" nodes may vary depending on the specific criteria used to sort them. However, the overall\nconclusion that \"Caddyshack\" belongs to the comedy genre and is the central node in the subgraph remains the same.\nTable 12: Visualization of our HiGPT\u2019s response with different prompting engineering techniques on IMDB for drama genre.\nQuestion: Given a heterogeneous graph about internet movie, there are three types of nodes, namely: movie, actor, director. The relationships (meta paths)\nbetween different nodes include: [movie is directed by director], [movie has actor]. By performing random sampling of 2-hop 10 neighbors centered on the\ntarget movie node, a heterogeneous subgraph is obtained. In the subgraph, \"movie\" nodes: <graph>, where the 0-th node is the central node that represents a\nmovie with the following information: Name: The Life of David Gale; Director\u2019s name: Alan Parker; Actors\u2019 names: Kevin Spacey|Kate Winslet|Matt Craven;\nPlot keywords: activist|death|death row|murder|texas\"actor\" nodes: <graph>; \"director\" nodes: <graph>. Question: Which of the following classes does this\nmovie belong to: action, comedy, drama?\nGround-Truth Category: Drama\n+ Give likely categories directly. HiGPT (Direct Response): Drama.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning.\nHiGPT (CoT Response without Format Constraint): Based on the plot keywords provided, the movie \"The Life of David Gale\" belongs to the drama\ngenre. The plot keywords include \"activist,\" \"death,\" \"death row,\" \"murder,\" and \"texas,\" which suggest a serious and emotional storyline. These keywords are\ncommonly associated with drama films, rather than action or comedy films. Therefore, the movie \"The Life of David Gale\" belongs to the drama genre.\n+ Please think about the categorization in a step by step manner and avoid making false associations. Then provide your reasoning. Using the following\nformat: Answer: [The answer] Reason: ...\nHiGPT (CoT Response with Format Constraint): Answer: Drama Reason: The movie \"The Life of David Gale\" is a drama, as it deals with serious and\nemotional themes such as activism, death row, and murder. The plot keywords also suggest a dramatic tone.\n+ Imagine three different experts are answering this question. All experts will write down 1 step of their thinking, then share it with the group. Then all\nexperts will go on to the next step, etc. If any expert realizes they\u2019re wrong at any point then they leave.\nHiGPT (ToT Response): Expert 1: Based on the plot keywords, this movie seems to be a drama. Expert 2: I agree with Expert 1. The plot keywords such as\n\"activist,\" \"death,\" \"death row,\" and \"murder\" suggest a serious and emotional theme. Expert 3: I also agree with Expert 1 and 2. The plot keywords and the fact\nthat the movie is directed by Alan Parker, who is known for his dramatic films, further support the classification of this movie as a drama. Final answer: The\nmovie belongs to the drama class.\n+ 3 experts are discussing the question with a panel discussion, trying to solve it step by step, and make sure the result is correct and avoid penalty\nHiGPT (Panel Discussion Response): 1. Expert 1: Let\u2019s start by analyzing the plot keywords. We have \"activist,\" \"death,\" \"death row,\" \"murder,\" and \"texas.\"\nBased on these keywords, I would say this movie belongs to the drama genre. 2. Expert 2: I agree with Expert 1. The plot keywords suggest a serious and\nemotional storyline. However, we also have \"actor\" nodes in the graph, which could indicate that there are comedic moments or performances. 3. Expert\n3: I would like to add that we should also consider the director\u2019s name, Alan Parker. He has directed both dramatic and comedic movies, such as \"The\nCommitments\" and \"Birdy.\" Therefore, it\u2019s difficult to determine the genre solely based on the plot keywords and director\u2019s name. 4. Panel Discussion: Based\non the analysis so far, we can conclude that the movie \"The Life of David Gale\" belongs to the drama genre, but it may also have comedic elements due to the\npresence of actor nodes in the graph.\nTable 13: Comprehensive performance comparison on node classification tasks in both few-shot and zero-shot settings. However,\nsince SSL methods focus on learning embeddings from downstream graphs, we excluded the zero-shot settings for them (\"-\").\nDatasets\nMetric\ntrain-on\ntest-on\nsage\ngat\nhan\nhgt\nhgnn\ndmgi\nHGMAE\nHeCo\nours-std\nours-cot\nSupervised\nMi-F1\nIMDB-1\nIMDB-1000\n0.4663\u00b10.0025\n0.4567\u00b10.0122\n0.4890\u00b10.0271\n0.4977\u00b10.0186\n0.4790\u00b10.0134\n0.4570\u00b10.0126\n0.3609\u00b10.0145\n0.3874\u00b10.0159\n0.5090\u00b10.0073\n0.5360\u00b10.0065\nIMDB-3\nIMDB-1000\n0.5083\u00b10.0103\n0.4970\u00b10.0100\n0.4860\u00b10.0122\n0.5007\u00b10.0163\n0.5027\u00b10.0066\n0.4120\u00b10.0153\n0.3791\u00b10.0060\n0.3467\u00b10.0208\n0.5170\u00b10.0093\n0.5730\u00b10.0108\nIMDB-5\nIMDB-1000\n0.5010\u00b10.0051\n0.5170\u00b10.0029\n0.4840\u00b10.0094\n0.5003\u00b10.0093\n0.5020\u00b10.0045\n0.4413\u00b10.0173\n0.3652\u00b10.0062\n0.3385\u00b10.0169\n0.6180\u00b10.0027\n0.6320\u00b10.0085\nIMDB-10\nIMDB-1000\n0.5203\u00b10.0005\n0.5227\u00b10.0085\n0.4967\u00b10.0111\n0.5273\u00b10.0060\n0.5127\u00b10.0087\n0.4710\u00b10.0148\n0.3705\u00b10.0071\n0.3572\u00b10.0159\n0.5710\u00b10.0092\n0.5810\u00b10.0064\nIMDB-20\nIMDB-1000\n0.5930\u00b10.0093\n0.6117\u00b10.0012\n0.5763\u00b10.0046\n0.5750\u00b10.0065\n0.5957\u00b10.0054\n0.5497\u00b10.0256\n0.4107\u00b10.0106\n0.3781\u00b10.0148\n0.6090\u00b10.0255\n0.6440\u00b10.0075\nIMDB-40\nIMDB-1000\n0.6170\u00b10.0112\n0.6261\u00b10.0015\n0.6198\u00b10.0025\n0.5923\u00b10.0040\n0.6177\u00b10.0046\n0.5813\u00b10.0033\n0.3946\u00b10.0067\n0.3927\u00b10.0134\n0.6260\u00b10.0057\n0.6280\u00b10.0071\nIMDB-60\nIMDB-1000\n0.6285\u00b10.0077\n0.6299\u00b10.0027\n0.6311\u00b10.0057\n0.6037\u00b10.0085\n0.6267\u00b10.0107\n0.5920\u00b10.0086\n0.4192\u00b10.0126\n0.4051\u00b10.0120\n0.6350\u00b10.0074\n0.5980\u00b10.0110\nMa-F1\nIMDB-1\nIMDB-1000\n0.4425\u00b10.0068\n0.3974\u00b10.0183\n0.4229\u00b10.0104\n0.4020\u00b10.0112\n0.4456\u00b10.0036\n0.4083\u00b10.0288\n0.3573\u00b10.0117\n0.4023\u00b10.0137\n0.4986\u00b10.0141\n0.5247\u00b10.0061\nIMDB-3\nIMDB-1000\n0.4737\u00b10.0207\n0.4388\u00b10.0057\n0.4365\u00b10.0265\n0.4590\u00b10.0417\n0.4784\u00b10.0208\n0.4070\u00b10.0102\n0.3708\u00b10.0050\n0.3647\u00b10.0232\n0.5079\u00b10.0082\n0.5591\u00b10.0070\nIMDB-5\nIMDB-1000\n0.4613\u00b10.0086\n0.4767\u00b10.0098\n0.4695\u00b10.0037\n0.4676\u00b10.0153\n0.4677\u00b10.0145\n0.4254\u00b10.0124\n0.3500\u00b10.0080\n0.3468\u00b10.0213\n0.6111\u00b10.0091\n0.6243\u00b10.0060\nIMDB-10\nIMDB-1000\n0.5001\u00b10.0087\n0.5145\u00b10.0058\n0.4971\u00b10.0103\n0.5188\u00b10.0085\n0.5024\u00b10.0045\n0.4699\u00b10.0125\n0.3668\u00b10.0064\n0.3768\u00b10.0195\n0.5694\u00b10.0162\n0.5762\u00b10.0108\nIMDB-20\nIMDB-1000\n0.5953\u00b10.0095\n0.6121\u00b10.0024\n0.5756\u00b10.0051\n0.5723\u00b10.0056\n0.5969\u00b10.0055\n0.5495\u00b10.0270\n0.4065\u00b10.0089\n0.3904\u00b10.0172\n0.6068\u00b10.0146\n0.6398\u00b10.0083\nIMDB-40\nIMDB-1000\n0.6182\u00b10.0107\n0.6254\u00b10.0009\n0.6224\u00b10.0057\n0.5909\u00b10.0068\n0.6234\u00b10.0038\n0.5786\u00b10.0064\n0.3866\u00b10.0072\n0.3988\u00b10.0147\n0.6265\u00b10.0090\n0.6237\u00b10.0059\nIMDB-60\nIMDB-1000\n0.62913\u00b10.0062\n0.6288\u00b10.0019\n0.6330\u00b10.0056\n0.5999\u00b10.0109\n0.6303\u00b10.0097\n0.5894\u00b10.0078\n0.4017\u00b10.0114\n0.4093\u00b10.0125\n0.6328\u00b10.0058\n0.5925\u00b10.0071\nAUC\nIMDB-1\nIMDB-1000\n0.6079\u00b10.0061\n0.6151\u00b10.0065\n0.6234\u00b10.0252\n0.6249\u00b10.0170\n0.6107\u00b10.0075\n0.5780\u00b10.0130\n0.5274\u00b10.0058\n0.5712\u00b10.0099\n0.6565\u00b10.0146\n0.6685\u00b10.0037\nIMDB-3\nIMDB-1000\n0.6271\u00b10.0075\n0.6301\u00b10.0066\n0.6176\u00b10.0099\n0.6170\u00b10.0166\n0.6275\u00b10.0047\n0.5679\u00b10.0116\n0.5507\u00b10.0052\n0.5347\u00b10.0290\n0.6648\u00b10.0032\n0.6935\u00b10.0098\nIMDB-5\nIMDB-1000\n0.6309\u00b10.0049\n0.6372\u00b10.0012\n0.6102\u00b10.0059\n0.6197\u00b10.0152\n0.6290\u00b10.0022\n0.5832\u00b10.0132\n0.5262\u00b10.0041\n0.5067\u00b10.0228\n0.7308\u00b10.0125\n0.7310\u00b10.0086\nIMDB-10\nIMDB-1000\n0.6401\u00b10.0015\n0.6435\u00b10.0062\n0.6254\u00b10.0064\n0.6411\u00b10.0036\n0.6360\u00b10.0051\n0.6108\u00b10.0088\n0.5310\u00b10.0068\n0.5432\u00b10.0189\n0.6968\u00b10.0065\n0.6875\u00b10.0083\nIMDB-20\nIMDB-1000\n0.6976\u00b10.0064\n0.7122\u00b10.0020\n0.6815\u00b10.0052\n0.6801\u00b10.0048\n0.7005\u00b10.0030\n0.6657\u00b10.0179\n0.5766\u00b10.0064\n0.5541\u00b10.0145\n0.7227\u00b10.0034\n0.7424\u00b10.0113\nIMDB-40\nIMDB-1000\n0.7171\u00b10.0069\n0.7210\u00b10.0014\n0.7204\u00b10.0015\n0.6970\u00b10.0060\n0.7145\u00b10.0035\n0.6860\u00b10.0027\n0.5488\u00b10.0049\n0.5653\u00b10.0105\n0.7323\u00b10.0036\n0.7331\u00b10.0074\nIMDB-60\nIMDB-1000\n0.7298\u00b10.0064\n0.7262\u00b10.0017\n0.7281\u00b10.0013\n0.6998\u00b10.0141\n0.7191\u00b10.0079\n0.6934\u00b10.0063\n0.5725\u00b10.0066\n0.5809\u00b10.0124\n0.7371\u00b10.0053\n0.7120\u00b10.0141\nZero-shot\nMi-F1\nIMDB-1\nDBLP-1000\n0.2353\u00b10.0372\n0.1893\u00b10.0373\n0.2653\u00b10.0203\n0.2573\u00b10.0519\n0.2900\u00b10.0638\n-\n-\n-\n0.3180\u00b10.0072\n0.3500\u00b10.0073\nIMDB-3\nDBLP-1000\n0.2627\u00b10.0085\n0.2913\u00b10.0017\n0.2510\u00b10.0000\n0.2143\u00b10.0569\n0.2430\u00b10.0453\n-\n-\n-\n0.3150\u00b10.0108\n0.3660\u00b10.0060\nIMDB-5\nDBLP-1000\n0.2607\u00b10.0082\n0.2737\u00b10.0176\n0.2577\u00b10.0094\n0.2453\u00b10.0458\n0.2427\u00b10.0452\n-\n-\n-\n0.3180\u00b10.0044\n0.3620\u00b10.0047\nIMDB-10\nDBLP-1000\n0.2603\u00b10.0078\n0.2957\u00b10.0105\n0.2863\u00b10.0108\n0.2797\u00b10.0217\n0.2417\u00b10.0441\n-\n-\n-\n0.3490\u00b10.0029\n0.4020\u00b10.0108\nIMDB-20\nDBLP-1000\n0.2810\u00b10.0289\n0.2780\u00b10.0033\n0.2710\u00b10.0000\n0.2803\u00b10.0208\n0.2333\u00b10.0353\n-\n-\n-\n0.3840\u00b10.0088\n0.4180\u00b10.0083\nIMDB-40\nDBLP-1000\n0.2400\u00b10.0324\n0.2847\u00b10.0053\n0.2710\u00b10.0000\n0.2937\u00b10.0005\n0.2027\u00b10.0345\n-\n-\n-\n0.3320\u00b10.0087\n0.3630\u00b10.0045\nIMDB-60\nDBLP-1000\n0.2350\u00b10.0377\n0.2887\u00b10.0123\n0.2710\u00b10.0000\n0.2937\u00b10.0005\n0.2170\u00b10.0274\n-\n-\n-\n0.3520\u00b10.0032\n0.3950\u00b10.0104\nMa-F1\nIMDB-1\nDBLP-1000\n0.0963\u00b10.0132\n0.1169\u00b10.0089\n0.1047\u00b10.0063\n0.1016\u00b10.0169\n0.1778\u00b10.0629\n-\n-\n-\n0.2048\u00b10.0068\n0.2472\u00b10.0070\nIMDB-3\nDBLP-1000\n0.1099\u00b10.0094\n0.1295\u00b10.0121\n0.1003\u00b10.0000\n0.0939\u00b10.0149\n0.0972\u00b10.0148\n-\n-\n-\n0.2011\u00b10.0106\n0.2814\u00b10.0071\nIMDB-5\nDBLP-1000\n0.1042\u00b10.0028\n0.1291\u00b10.0145\n0.1024\u00b10.0030\n0.1138\u00b10.0296\n0.0971\u00b10.0148\n-\n-\n-\n0.1917\u00b10.0046\n0.2773\u00b10.0085\nIMDB-10\nDBLP-1000\n0.1033\u00b10.0024\n0.1420\u00b10.0155\n0.1113\u00b10.0033\n0.1330\u00b10.0165\n0.0969\u00b10.0145\n-\n-\n-\n0.2455\u00b10.0091\n0.3386\u00b10.0093\nIMDB-20\nDBLP-1000\n0.1448\u00b10.0573\n0.1274\u00b10.0060\n0.1066\u00b10.0000\n0.1143\u00b10.0116\n0.1008\u00b10.0191\n-\n-\n-\n0.3142\u00b10.0074\n0.3733\u00b10.0051\nIMDB-40\nDBLP-1000\n0.1068\u00b10.0060\n0.1588\u00b10.0078\n0.1066\u00b10.0000\n0.1268\u00b10.0105\n0.0984\u00b10.0161\n-\n-\n-\n0.2331\u00b10.0069\n0.2912\u00b10.0056\nIMDB-60\nDBLP-1000\n0.1013\u00b10.0040\n0.1821\u00b10.0134\n0.1066\u00b10.0000\n0.1164\u00b10.0040\n0.1078\u00b10.0282\n-\n-\n-\n0.2620\u00b10.0048\n0.3359\u00b10.0114\nAUC\nIMDB-1\nDBLP-1000\n0.4999\u00b10.0001\n0.4513\u00b10.0295\n0.5000\u00b10.0000\n0.5000\u00b10.0000\n0.5206\u00b10.0306\n-\n-\n-\n0.5222\u00b10.0069\n0.5406\u00b10.0040\nIMDB-3\nDBLP-1000\n0.4997\u00b10.0005\n0.4995\u00b10.0016\n0.5000\u00b10.0000\n0.4933\u00b10.0095\n0.5000\u00b10.0000\n-\n-\n-\n0.5185\u00b10.0134\n0.5524\u00b10.0073\nIMDB-5\nDBLP-1000\n0.4978\u00b10.0030\n0.4908\u00b10.0078\n0.5000\u00b10.0000\n0.5031\u00b10.0043\n0.4998\u00b10.0003\n-\n-\n-\n0.5184\u00b10.0081\n0.5493\u00b10.0091\nIMDB-10\nDBLP-1000\n0.4975\u00b10.0030\n0.5022\u00b10.0068\n0.5000\u00b10.0000\n0.5018\u00b10.0058\n0.4992\u00b10.0007\n-\n-\n-\n0.5396\u00b10.0071\n0.5777\u00b10.0060\nIMDB-20\nDBLP-1000\n0.5154\u00b10.0213\n0.4918\u00b10.0020\n0.5000\u00b10.0000\n0.5011\u00b10.0016\n0.4957\u00b10.0060\n-\n-\n-\n0.5669\u00b10.0041\n0.5907\u00b10.0089\nIMDB-40\nDBLP-1000\n0.5027\u00b10.0031\n0.4976\u00b10.0021\n0.5000\u00b10.0000\n0.5008\u00b10.0006\n0.4884\u00b10.0164\n-\n-\n-\n0.5296\u00b10.0070\n0.5508\u00b10.0086\nIMDB-60\nDBLP-1000\n0.4987\u00b10.0019\n0.5034\u00b10.0053\n0.5000\u00b10.0000\n0.5000\u00b10.0000\n0.4952\u00b10.0068\n-\n-\n-\n0.5426\u00b10.0071\n0.5732\u00b10.0089\nMi-F1\nIMDB-1\nACM-1000\n0.3293\u00b10.0418\n0.3567\u00b10.0053\n0.3407\u00b10.0111\n0.3240\u00b10.0014\n0.3743\u00b10.0434\n-\n-\n-\n0.4160\u00b10.0106\n0.4540\u00b10.0089\nIMDB-3\nACM-1000\n0.3987\u00b10.0228\n0.3933\u00b10.0495\n0.4400\u00b10.0177\n0.3277\u00b10.0060\n0.4167\u00b10.0090\n-\n-\n-\n0.4530\u00b10.0086\n0.5000\u00b10.0078\nIMDB-5\nACM-1000\n0.3820\u00b10.0113\n0.3787\u00b10.0057\n0.3630\u00b10.0086\n0.3160\u00b10.0169\n0.3583\u00b10.0198\n-\n-\n-\n0.4580\u00b10.0173\n0.4880\u00b10.0131\nIMDB-10\nACM-1000\n0.3097\u00b10.0147\n0.2940\u00b10.0033\n0.3073\u00b10.0026\n0.3343\u00b10.0132\n0.3093\u00b10.0025\n-\n-\n-\n0.4850\u00b10.0078\n0.5160\u00b10.0187\nIMDB-20\nACM-1000\n0.2807\u00b10.0074\n0.3013\u00b10.0188\n0.3133\u00b10.0031\n0.3530\u00b10.0000\n0.2840\u00b10.0226\n-\n-\n-\n0.5080\u00b10.0129\n0.5030\u00b10.0064\nIMDB-40\nACM-1000\n0.3173\u00b10.0005\n0.2393\u00b10.0144\n0.2697\u00b10.0194\n0.3560\u00b10.0099\n0.3180\u00b10.0016\n-\n-\n-\n0.4750\u00b10.0149\n0.5050\u00b10.0077\nIMDB-60\nACM-1000\n0.3400\u00b10.0057\n0.2520\u00b10.0104\n0.2573\u00b10.0034\n0.3593\u00b10.0037\n0.3333\u00b10.0103\n-\n-\n-\n0.4310\u00b10.0064\n0.5120\u00b10.0051\nMa-F1\nIMDB-1\nACM-1000\n0.2647\u00b10.0269\n0.2908\u00b10.0131\n0.2250\u00b10.0416\n0.1631\u00b10.0005\n0.3139\u00b10.0468\n-\n-\n-\n0.3949\u00b10.0078\n0.4177\u00b10.0124\nIMDB-3\nACM-1000\n0.3131\u00b10.0393\n0.2820\u00b10.0714\n0.3528\u00b10.0208\n0.1710\u00b10.0114\n0.3587\u00b10.0249\n-\n-\n-\n0.3999\u00b10.0056\n0.4516\u00b10.0116\nIMDB-5\nACM-1000\n0.3208\u00b10.0130\n0.3009\u00b10.0137\n0.2782\u00b10.0026\n0.1969\u00b10.0301\n0.3087\u00b10.0225\n-\n-\n-\n0.4336\u00b10.0085\n0.4510\u00b10.0114\nIMDB-10\nACM-1000\n0.2497\u00b10.0065\n0.2022\u00b10.0136\n0.2078\u00b10.0017\n0.1670\u00b10.0049\n0.2471\u00b10.0062\n-\n-\n-\n0.4218\u00b10.0081\n0.4598\u00b10.0054\nIMDB-20\nACM-1000\n0.2694\u00b10.0091\n0.2422\u00b10.0098\n0.2412\u00b10.0050\n0.2094\u00b10.0501\n0.2715\u00b10.0181\n-\n-\n-\n0.4964\u00b10.0075\n0.4877\u00b10.0070\nIMDB-40\nACM-1000\n0.3117\u00b10.0017\n0.2141\u00b10.0071\n0.2313\u00b10.0132\n0.2749\u00b10.0122\n0.3144\u00b10.0017\n-\n-\n-\n0.4176\u00b10.0116\n0.4585\u00b10.0089\nIMDB-60\nACM-1000\n0.3308\u00b10.0069\n0.2141\u00b10.0071\n0.2389\u00b10.0024\n0.2258\u00b10.0299\n0.3226\u00b10.0074\n-\n-\n-\n0.3703\u00b10.0074\n0.4862\u00b10.0083\nAUC\nIMDB-1\nACM-1000\n0.4934\u00b10.0247\n0.5248\u00b10.0038\n0.5128\u00b10.0086\n0.5000\u00b10.0000\n0.5318\u00b10.0295\n-\n-\n-\n0.5672\u00b10.0040\n0.5969\u00b10.0082\nIMDB-3\nACM-1000\n0.5559\u00b10.0173\n0.5523\u00b10.0379\n0.5880\u00b10.0136\n0.5028\u00b10.0040\n0.5695\u00b10.0064\n-\n-\n-\n0.5970\u00b10.0032\n0.6323\u00b10.0130\nIMDB-5\nACM-1000\n0.5433\u00b10.0082\n0.5415\u00b10.0047\n0.5282\u00b10.0073\n0.4950\u00b10.0134\n0.5256\u00b10.0145\n-\n-\n-\n0.5991\u00b10.0103\n0.6224\u00b10.0054\nIMDB-10\nACM-1000\n0.4860\u00b10.0116\n0.4748\u00b10.0018\n0.4851\u00b10.0022\n0.5000\u00b10.0000\n0.4853\u00b10.0022\n-\n-\n-\n0.6219\u00b10.0100\n0.6450\u00b10.0076\nIMDB-20\nACM-1000\n0.4601\u00b10.0048\n0.4772\u00b10.0137\n0.4877\u00b10.0029\n0.5038\u00b10.0053\n0.4625\u00b10.0163q\n-\n-\n-\n0.6352\u00b10.0094\n0.6318\u00b10.0068\nIMDB-40\nACM-1000\n0.4867\u00b10.0013\n0.4320\u00b10.0108\n0.4545\u00b10.0146\n0.5148\u00b10.0043\n0.4872\u00b10.0006\n-\n-\n-\n0.6138\u00b10.0047\n0.6360\u00b10.0051\nIMDB-60\nACM-1000\n0.5085\u00b10.0041\n0.4439\u00b10.0078\n0.4473\u00b10.0026\n0.5070\u00b10.0044\n0.5037\u00b10.0082\n-\n-\n-\n0.5801\u00b10.0051\n0.6399\u00b10.0058\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Mi-F1 on IMDB\n0.6550.653\n0.646\n0.654\n0.655\n0.6640.658\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Ma-F1 on IMDB\n0.6500.648\n0.641\n0.649\n0.653\n0.6610.654\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting AUC on IMDB\n0.7500.749\n0.744\n0.748\n0.750\n0.7570.753\nOurs\nOurs-ICL-1\nOurs-ICL-2\n(a) IMDB-IMDB@Mi-F1, Ma-F1, AUC\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n35%\n40%\n45%\n50%\n55%\nTesting Mi-F1 on DBLP\n0.387\n0.469\n0.475\n0.542\n0.510\n0.533\n0.540\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n35%\n40%\n45%\n50%\n55%\nTesting Ma-F1 on DBLP\n0.374\n0.461\n0.467\n0.535\n0.505\n0.528\n0.539\nOurs\nOurs-ICL-1\nOurs-ICL-2\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting AUC on DBLP\n0.586\n0.634\n0.639\n0.691\n0.667\n0.680\n0.689\nOurs\nOurs-ICL-1\nOurs-ICL-2\n(b) IMDB-DBLP@Mi-F1, Ma-F1, AUC\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Mi-F1 on ACM\n0.634\n0.627\n0.679\n0.620\n0.517\n0.578\n0.634\nOurs\nOurs-ICL-1\nOurs-ICL-DBLP\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting Ma-F1 on ACM\n0.631\n0.627\n0.679\n0.619\n0.505\n0.570\n0.629\nOurs\nOurs-ICL-1\nOurs-ICL-DBLP\n1\n3\n5\n10\n20\n40\n60\nNumber of Training Shots on IMDB\n50%\n55%\n60%\n65%\n70%\nTesting AUC on ACM\n0.724\n0.721\n0.760\n0.717\n0.646\n0.687\n0.729\nOurs\nOurs-ICL-1\nOurs-ICL-DBLP\n(c) IMDB-ACM@Mi-F1, Ma-F1, AUC\nFigure 5: Comprehensive results of graph in-context learning of our HiGPT.\n",
    "2306.12251": "GADBench: Revisiting and Benchmarking Supervised\nGraph Anomaly Detection\nJianheng Tang1,2\u2217, Fengrui Hua1, Ziqi Gao1,2, Peilin Zhao3, Jia Li1,2\u2020\n1Hong Kong University of Science and Technology (Guangzhou)\n2Hong Kong University of Science and Technology, 3Tencent AI Lab\n{jtangbf,zgaoat}@connect.ust.hk, huafengrui@outlook.com,\nmasonzhao@tencent.com, jialee@ust.hk\nAbstract\nWith a long history of traditional Graph Anomaly Detection (GAD) algorithms and\nrecently popular Graph Neural Networks (GNNs), it is still not clear (1) how they\nperform under a standard comprehensive setting, (2) whether GNNs can outperform\ntraditional algorithms such as tree ensembles, and (3) how about their efficiency\non large-scale graphs. In response, we introduce GADBench\u2014a benchmark tool\ndedicated to supervised anomalous node detection in static graphs. GADBench\nfacilitates a detailed comparison across 29 distinct models on ten real-world GAD\ndatasets, encompassing thousands to millions (\u223c6M) nodes. Our main finding\nis that tree ensembles with simple neighborhood aggregation can outperform the\nlatest GNNs tailored for the GAD task. We shed light on the current progress of\nGAD, setting a robust groundwork for subsequent investigations in this domain.\nGADBench is open-sourced at https://github.com/squareRoot3/GADBench.\n1\nIntroduction\nGraph Anomaly Detection (GAD) is the process of identifying uncommon graph objects, such as\nnodes, edges, or substructures, that significantly deviate from the majority of reference objects within\na graph database [4, 55]. With a notable history spanning over two decades, GAD has proven its\neffectiveness across a variety of applications. These include, but are not limited to, the prevention\nof financial fraud [36] and money-laundering [83], the prediction of network intrusions [57, 64]\nand device failures [46], the identification of spam reviews [56] and fake news [8]. Unlike outlier\ndetection in tabular data, GAD considers the inter-dependencies among a group of objects, which can\noften yield additional insights for identifying fraudulent patterns. Meanwhile, GAD also presents\nunique challenges in terms of modeling efficiency and necessitates strategies to address issues such\nas label imbalance [50], feature heterophily [26], and relation camouflage [22].\nThe definitions of GAD can be multifaceted, depending on the specific objectives and applications. In\nthis paper, we focus on the most prevalent GAD scenario\u2014the detection of anomalous nodes within\na static attributed graph. Despite the plenty of methods proposed for this task, including traditional\nstructural pattern mining algorithms [57, 4] and advanced deep learning techniques [21, 55], several\nlimitations exist in the current model development and evaluation scheme:\n\u2022 Absence of a comprehensive supervised GAD benchmark. We summarize the exiting GAD\nbenchmarks and toolboxes in Table 1. The latest benchmark available for GAD, BOND [49],\nexclusively evaluates unsupervised methods. This overlooks the fact that many GAD models rely\non labeled data to boost their performance. In comparison, there have been a variety of supervised\n\u2217Work done during an internship at Tencent AI Lab.\n\u2020Corresponding Author.\n37th Conference on Neural Information Processing Systems (NeurIPS 2023).\narXiv:2306.12251v2  [cs.LG]  16 Nov 2023\nTable 1: Comparison of existing GAD benchmarks in terms of datasets, models, and scenarios.\nBenchmark\n#Datasets\nMax. Nodes\n#Models\nModel Type\nSupervision Scenario\n&Toolbox\n(Organic)\n/Edges\nUGFraud [23]\n1 (1)\n45K/3M\n6\nGNN\nUnsupervised\nDGFraud [22]\n3 (1)\n45K/3M\n9\nGNN\nSupervised\nBOND [49]\n9 (6)\n3M/4M\n14\nGNN, Classic\nUnsupervised\nGADBench\n10 (10)\n5M/73M\n29\nGNN, Classic, Trees\nFully- and Semi-Supervised\nanomaly detection benchmarks for time series [59, 61], images [68], videos [1], and tabular data\n[31, 77]. Semi-supervised approach is another setting that calls for attention because it can strike a\nbalance between label annotation budgets and model performance.\n\u2022 Insufficient comparative studies between tree ensembles and GNNs. Tree ensemble methods,\nincluding Random Forest [12], XGBoost [16], and Isolation Forest [48], have long been favored in\nthe industry. They also showcase impressive results in a recent benchmark for anomaly detection\nwithin tabular datasets [31]. These models are also applicable to GAD datasets [83] with appropri-\nate feature engineering. However, a systematic comparison between tree ensembles and GNNs in\nthe context of GAD is still absent.\n\u2022 Limited exploration on large-scale graphs. While many GNN models tailored for GAD have\nshown promising results on small-scale datasets, their efficacy on large graphs remains unexplored.\nOn the other hand, although several large-scale GAD datasets have been proposed [36, 83, 74], they\nmainly focus on the comparisons of GNN variants, making it unclear how they perform compared\nwith traditional GAD algorithms.\nTo redress these gaps and foster academia-industry synergy in GAD evaluation, we propose GAD-\nBench, which serves as the first comprehensive benchmark for supervised GAD. Our evaluation\nencompasses 7 non-graph models, 10 standard GNNs, and 10 state-of-the-art GNNs specifically\ndesigned for GAD. Recognizing the proven success of tree ensembles in the anomaly detection of\ntabular data, we additionally employ two tree ensemble models with simple neighborhood aggrega-\ntions. All models are evaluated on 10 real-world GAD datasets ranging from thousands to millions\nof nodes. They are tested in both semi-supervised and fully-supervised settings, with and without\nhyperparameter tuning.\nThrough extensive experiments, we discover that (1) surprisingly, tree ensembles with neighbor\naggregation have the superior performance among all models; (2) most standard GNNs are not\nsuitable for the GAD task; (3) GNNs specifically designed for GAD require hyperparameter tuning\nto achieve satisfactory performance. We highlight that our findings can offer the research community\na clearer insight into the current progress in GAD. In summary, our contributions can be organized\ninto three main aspects:\n\u2022 We introduce GADBench, the first comprehensive benchmark for supervised anomalous node\ndetection on static attributed graphs. This includes a comparison of 29 well-known GAD methods\nacross a collection of 10 real-world datasets in both semi-supervised and fully-supervised settings.\n\u2022 To ensure a rigorous and fair comparison, we identify the limitations inherent in the existing\nevaluation scheme for GAD, instituting enhancements from dataset selection, metric utilization,\nmodel training, and hyperparameter tuning.\n\u2022 We integrate all models, datasets, and evaluation protocols mentioned in this paper into an open-\nsource repository. Users can reproduce the results and evaluate their own datasets/models with\nminimal effort.\n2\nPreliminaries and Related Work\nTask definition.\nWe focus on the detection of anomalous nodes within a static attributed graph.\nFormally, consider a graph G = {V, A, X}, where V = {v1, v2, \u00b7 \u00b7 \u00b7 , vN} denotes a set of N nodes.\nThe non-negative adjacency matrix A \u2208RN\u00d7N is defined such that Aij > 0 if and only if there is\nan edge between vi and vj. X \u2208RN\u00d7d is a feature matrix, of which the i-th row vector xi = X(i, :)\nis the d-dimensional feature vector of node vi. Given a subset of labeled nodes V\u2217\u2282V, the task\nis to classify the remaining nodes into either normal or anomalous categories. In the context of\n2\na heterogeneous graph, multiple adjacency matrices {A1, A2, \u00b7 \u00b7 \u00b7 } might exist, each representing\ndifferent types of relationships among nodes.\nAlthough the GAD task can be regarded as a binary node classification problem, it introduces several\nadditional challenges. Firstly, anomalous nodes typically constitute a small portion of the total\nnodes, resulting in a significant data imbalance [50, 74]. Secondly, graphs with anomalies often\nexhibit strong heterophily, where connected nodes possess varying features and labels [27, 26]. This\nnecessitates strategies to handle neighborhood feature disparities during message passing. Lastly,\nanomalous nodes have a tendency to camouflage their features and connections, blending seamlessly\nby mimicking the normal patterns in the graph [22, 51]. This demands attention to intentionally\nmanipulated edges and node features. Before presenting our benchmark, we provide a brief overview\nof both classic methods and deep learning based GAD models. For a more thorough review of GAD\nmodels, please refer to these relevant surveys [55, 4, 38].\nClassical methods. Classical methods primarily detect graph anomalies by leveraging graph statistical\npatterns [57, 34], community and clustering structures [14, 43, 86], egonet information [3], spectral\nanalysis [25], random walk-based techniques [81, 29], etc. Other than these heuristic approaches and\nhandcraft feature engineering, another research line leverages learning methods for a more flexible\nencoding of graph information to identify anomalies. Examples include residual learning [45, 63],\nrelational learning [40, 66], and Bayesian models [33]. Despite their advantages, most methods\nare primarily designed for plain graphs and struggle to handle attributed graphs. Additionally, they\npredominantly emphasize unsupervised settings and are generally inefficient in utilizing node labels.\nGNNs for GAD. With superb ability to encode both structure and attribute information simultaneously,\nGNNs have recently gained popularity in mining graph data [76, 39, 85, 30]. To tackle the unique\nchallenges of graph anomalies, such as imbalance, heterophily, and camouflage, several adaptations\nof standard GNNs have been proposed [20, 96, 24, 53, 52, 21, 79, 54, 90]. On one hand, spatial\nGNNs have primarily been redesigned at the level of their inherent mechanisms, such as message\npassing and aggregation [99]. For instance, GAS [44] employs a structure-enhanced pre-processing\nstrategy to establish implicit connections between anomalies. CARE-GNN [22] and GraphConsis [51]\ncombat the camouflage behavior by designing camouflage-resistant message passing and aggregation\nprocesses. H2-FDetector [72] introduces a novel information aggregation strategy that enables\nhomophilic connections to propagate similar information, while heterophilic connections disseminate\ndiffering information. With the consideration of label imbalance, solutions like PC-GNN [50] and\nDAGAD [47] use imbalance-aware data sampling and graph augmentation to highlight the importance\nof anomalies during training. On the other hand, spectral GNNs provide a fresh viewpoint that\nassociates graph anomalies with high frequency spectral distributions [26]. For example, BWGNN\n[74] applies Beta kernel to manage higher frequency anomalies via flexible and localized band-pass\nfilters. AMNet [13] captures both low-frequency and high-frequency signals, adaptively integrating\nsignals of varying frequencies.\nEnsemble models. Tree ensembles, such as Random Forest [12, 9, 6, 5] and XGBoost [16, 78, 97, 58],\nhave shown superior performance in anomaly detection tasks related to tabular data [93, 10, 2, 100].\nFor example, XGBOD [93], an extension of the XGBoost algorithm specifically designed for anomaly\ndetection, incorporates scores from other models such as Isolation Forest [48] as additional feature\nand achieves superior performance. Despite their potential, few research has explored the application\nof tree ensembles in leveraging both structural and feature information simultaneously for GAD.\nBeyond trees, various base models can be integrated into ensembles for anomaly detection, as\nsuggested by [89]. The use of neighborhood aggregation has been demonstrated to enhance anomaly\ndetection performance, which can be utilized during various stages such as pre-processing [87],\nmodel-training [17], and post-processing [88] phases. In this work, we revisit tree ensemble models\nand demonstrate their potential to outperform GNNs after combining a straightforward structural\nneighborhood aggregation.\n3\nThe Setup of GADBench\nIn this section, we present a comprehensive overview of the setup for GADBench. We provide the\ngeneral selection criteria and considerations for models (Section 3.1), datasets (Section 3.2), and\nother details (Section 3.3).\n3\nTable 2: Categorization of all models used in our evaluation.\nClassic Methods\nMLP [67] , KNN [18], SVM [15], RF [12], XGBoost [16], XGBOD [93], NA [88]\nStandard GNNs\nGCN [39], SGC [84], GIN [85], GraphSAGE [30], GAT [76], GT [73], PNA [17]\nBGNN [37], RGCN [70], HGT [35]\nSpecialized GNNs\nGAS [44], DCI [82], PC-GNN [50], GAT-sep [99], BernNet [32], AMNet [13],\nBWGNN [74], GHRN [26], CARE-GNN [22], H2-FDetector [72]\nTree Ensembles with\nRF-Graph, XGB-Graph\nNeighbor Aggregate\n3.1\nBenchmark Models\nTable 2 provides an overview of the 29 models assessed in GADBench. We briefly introduce each\nmodel in the following, and provide a more detailed description in Appendix A.\nClassical methods. We select three basic algorithms for supervised classification: Multi-Layer\nPerceptron (MLP), k-Nearest Neighbors (KNN), and Support Vector Machine (SVM). Additionally,\nwe incorporate three representative decision tree ensembles, including a bagging-based model,\nRandom Forest (RF); a boosting-based model, Extreme Gradient Boosting Tree (XGBoost); and an\nadvanced XGBoost model named Extreme Boosting Based Outlier Detection (XGBOD). We also\nassess a recent anomaly detection technique, Neighborhood Averaging (NA), which enhances the\noutlier scores of existing anomaly detectors by averaging them with the scores of neighboring objects.\nStandard GNNs. We evaluate several standard GNNs that have proven to be effective across\ndiverse graph learning tasks, including Graph Convolutional Network (GCN), Chebyshev Spectral\nConvolution Network (ChebNet), Graph Isomorphism Network (GIN), Graph Sample and Aggregate\n(GraphSAGE), Graph Attentional Network (GAT), Graph Transformer (GT), Principle Neighbor\nAggregation (PNA), Boosted Graph Neural Network (BGNN). We also consider two widely used\nheterogeneous GNNs, including Relational Graph Convolution Network (RGCN) and Heterogeneous\nGraph Transformer (HGT).\nSpecialized GNNs. This group contains GNNs specifically designed for anomaly detection. We\nevaluate five spatial GNNs including the Graph-based Anti-Spam Model (GAS), Deep Cluster In-\nfomax (DCI), Pick and Choose GNN (PC-GNN), and the GAT with ego- and neighbor-embedding\nseparation (GAT-sep). For spectral GNNs, we evaluate the graph spectral filter via Bernstein Approx-\nimation (BernNet), Adaptive Multi-frequency GNN (AMNet), Beta Wavelet GNN (BWGNN), and\nthe Graph Heterophily Reduction Network (GHRN). Additionally, we assess two GNNs optimized\nfor heterogeneous graphs: the CAmouflage-REsistant GNN (CARE-GNN) and the Fraud Detector\nwith Homophilic and Heterophilic Interactions (H2-FDetector).\nTree ensembles with neighbor aggregation. Decision tree ensembles have shown their effectiveness\nin anomaly detection with tabular data [31], prompting us to adapt them for GAD. To incorporate\ngraph structure information, we follow the idea from a subclass of GNNs that independently man-\nage message passing and node feature transformation [84, 98, 91]. Consequently, we devise tree\nensembles with Neighbor aggregation that adhere to the following computational paradigm:\nh(l)\nvi = Aggregate{h(l\u22121)\nvj\n|vi \u2208Neighbor(vj)}\nScore(vi) = TreeEnsemble([h0\nvi||h1\nvi|| \u00b7 \u00b7 \u00b7 ||hL\nvi]).\nIn this scheme, h(0)\nvi = xi denotes the initial node attributes, and h(l)\nvi represents the feature of node\nvi after l-layers of neighbor aggregation. Aggregate(\u00b7) can take on any aggregation function such\nas mean, max, or sum pooling. Same as [84, 98, 91], the aggregation process is parameter-free.\nTreeEnsemble(\u00b7) can be any tree ensembles that takes the aggregated features as input to predict the\nanomaly score of node vi. In GADBench, we utilize Random Forest and XGBoost to instantiate two\nnew tree ensemble baselines with neighbor aggregation, referred to as RF-Graph and XGB-Graph.\n3.2\nBenchmark Datasets\nIn GADBench, we have gathered 10 diverse and representative datasets, as detailed in table 3, which\nare chosen based on the following criteria:\n4\nTable 3: Statistics of all datasets in GADBench including the number of nodes and edges, the node\nfeature dimension, the ratio of anomalous labels, the training ratio in the fully-supervised setting,\nthe concept of relations, and the type of node features. Misc. indicates the node features are a\ncombination of heterogeneous attributes, possibly including categorical, numerical, and temporal\ninformation, More details are shown in Appendix B.\n#Nodes\n#Edges\n#Feat.\nAnomaly\nTrain\nRelation Concept\nFeature Type\nReddit[42, 49]\n10,984\n168,016\n64\n3.3%\n40%\nUnder Same Post\nText Embedding\nWeibo[92, 49]\n8,405\n407,963\n400\n10.3%\n40%\nUnder Same Hashtag\nText Embedding\nAmazon[56, 22]\n11,944\n4,398,392\n25\n9.5%\n70%\nReview Correlation\nMisc. Information\nYelpChi[66, 22]\n45,954\n3,846,979\n32\n14.5%\n70%\nReviewer Interaction\nMisc. Information\nTolokers[65]\n11,758\n519,000\n10\n21.8%\n40%\nWork Collaboration\nMisc. Information\nQuestions[65]\n48,921\n153,540\n301\n3.0%\n52%\nQuestion Answering\nText Embedding\nT-Finance[74]\n39,357\n21,222,543\n10\n4.6%\n50%\nTransaction Record\nMisc. Information\nElliptic[83]\n203,769\n234,355\n166\n9.8%\n50%\nPayment Flow\nMisc. Information\nDGraph-Fin[36]\n3,700,550\n4,300,999\n17\n1.3%\n70%\nLoan Guarantor\nMisc. Information\nT-Social[74]\n5,781,065\n73,105,508\n10\n3.0%\n40%\nSocial Friendship\nMisc. Information\n\u2022 Organic anomalies. Datasets in GADBench exclusively contain anomalies that naturally emerge in\nreal-world scenarios, a distinction from previous studies that employ synthetic anomalies for GAD\nevaluations [21, 49]. These earlier works typically inject artificial node attributes and structures\ninto normal graphs like Cora [71], resulting in anomalies that are relatively straightforward to be\nidentified and obviously different from real-world anomalies.\n\u2022 Various domains. Datasets in GADBench span multiple domains, including social media, e-\ncommerce, e-finance, and crowd-sourcing. As presented in Table 3, the graph edge in each dataset\nembodies unique relation concepts, which shows a diverse distribution of applications.\n\u2022 Diverse scale. GADBench datasets cover a wide scale, from thousands to millions of nodes. We\nhave consciously excluded datasets with fewer than 5,000 nodes, such as Bitcoin-Alpha [41],\nDisney, and Books [69].\n\u2022 Imbalance ratio. We have ensured that the number and ratio of anomalies within the datasets meet\na specific criteria: each dataset contains more than 100 anomalies, to ensure stable experimental\nresults, and no more than a 25% anomaly ratio, preserving the inherent imbalance nature of GAD.\nThis criterion leads to the exclusion of the Enron dataset [69].\nAmong the datasets in GADBench, Weibo, Reddit, Questions, and T-Social are designed to identify\nanomalous accounts on social media platforms. Tolokers, Amazon and YelpChi datasets aim to detect\nfraudulent workers, reviews and reviewers on crowd-sourcing or e-commerce platforms. T-Finance,\nElliptic, and DGraph-Fin concentrate on identifying fraudulent users, illicit entities and overdue loans\nin financial networks, respectively. For a more comprehensive description of each dataset, please\nrefer to Appendix B.\n3.3\nOther Details\nData split. We employ both fully-supervised and semi-supervised settings for model evaluation. In a\nfully-supervised setting, we preserve pre-existing data splits when available. If such divisions are not\nprovided, we follow the approach suggested by [74], randomly partitioning nodes into three subsets:\n40% for training, 20% for validation, and the remaining 40% for testing. For each dataset, the specific\ntraining ratio is reported in table 3. The semi-supervised setting typically involves a smaller training\nratio, e.g., 1% or 5% in previous studies [22, 74]. However, due to the variance in graph sizes present\nin GADBench, a fixed training ratio might lead to substantial discrepancies in the scale of training\nsets. To more accurately mimic real-world semi-supervised scenarios, we standardize the training\nset across all datasets to include a total of 100 labels\u201420 positive labels (anomalous nodes) and 80\nnegative labels (normal nodes). To ensure robustness in our findings, we execute ten random splits\non each dataset and analyze the average performance of the model.\nMetrics. According to existing anomaly detection benchmarks [31, 49], we select Area Under\nthe Receiver Operating Characteristic Curve (AUROC), Area Under the Prevision Recall Curve\n(AUPRC) calculated by average precision, and the Recall score within top-k predictions (Rec@K) as\nperformance metrics for the GAD task. We set k as the number of anomalies within the test set. For\nall metrics, anomalies are considered as the positive class, and higher scores indicate better model\n5\nperformance. Among these metrics, AUROC primarily focuses on overall performance and is not\nsensitive to top-k predictions, Rec@K only cares top-k performance, and AUPRC strikes a balance\nbetween the two. Suppose the test set includes 10 anomalies within 1000 data points and a model\nranks them from positions 11th to 20th, it would attain an AUROC of 0.99, an AUPRC of 0.33, and\na Rec@10 of 0. We also document the running time and memory consumption of each model.\nHyperparameter Optimization. To control the effect of hyperparameter selection and ensure fairness\n[11], we standardize the evaluation process with and without hyperparameter tuning. Initially, we\nemploy default hyperparameters as stated in the original papers. To ensure fairness in hyperparameter\ntuning, we then utilize random search [7] to optimize hyperparameters. During one trial on each\ndataset, we randomly select a set of hyperparameters from the predefined search space for each\nmodel. For more information about metrics, default hyperparameters, search spaces, and other\nimplementation details, please refer to Appendix C.\n4\nExperimental Results\nIn this section, we study the experimental results of all the benchmarked models. We first provide\na comprehensive comparison of all models, taking into account both default and optimally tuned\nhyperparameters. Following that, we aim to conduct an in-depth comparison between tree ensembles\nwith neighbor aggregation and GNN-based methods.\n4.1\nOverall Comparison\nIn Figure 1, we present an overview of model performance across 10 datasets for all metrics,\nexcluding four GNNs that are specific to heterogeneous graphs. In Table 4, we take a close look at\nthe model performance regarding the AUPRC score after hyper-parameter tuning on each dataset.\nFor comprehensive experimental results, please refer to Appendix D. Our key findings include:\nEnsemble trees with neighbor aggregation have superior performance. As highlighted in Figure\n1, XGB-Graph and RF-Graph consistently surpass other compared models across all metrics using\ndefault hyperparameters. The performance gap becomes particularly significant in the fully-supervised\nsetting, i.e., XGB-Graph surpasses BWGNN\u2014the best GNN model in this setting\u2014by an absolute\naverage improvement of 2.0% on AUROC, 12.9% on AUPRC, and 9.8% on Rec@K. In the semi-\nsupervised context, RF-Graph presents an absolute average improvement of 2.8% on AUROC, 8.0%\non AUPRC, and 3.1% on Rec@K, as compared to GHRN, the best GNN model in this setting. It\nis important to highlight that the improvement in AUPRC and Rec@K is more pronounced than\nthat in AUROC due to the imbalanced issue, suggesting that RF-Graph and XGB-Graph are more\nproficient in predicting top-k high-confidence anomalies. Further, as shown in the bottom of Figure\n1, tree ensembles with neighbor aggregation not only outperform GNNs in terms of efficiency but\nalso exhibit lower memory consumption. Although the performance gap of different models narrow\nafter hyperparameter tuning as in Table 4, RF-Graph and XGB-Graph still prevail among 6 out of 10\ndatasets. Accordingly, our observations show the superior effectiveness and efficiency of RF-Graph\nand XGB-Graph across diverse GAD datasets and scenarios.\nMost standard GNNs prove unsuitable for GAD. As shown in Figure 1, it becomes clear that\nthe majority of standard GNNs encounter difficulties when dealing with GAD tasks. To illustrate,\nthe performance of GCN and GIN is on par with that of MLP\u2014a method that does not take graph\nstructure information into account. This indicates that standard GNNs often struggle to effectively\nhandle structure camouflage or feature heterophily problems induced by anomalies. In Table 4,\nwhile hyperparameter tuning does improve the results of all standard GNNs, they remain subpar\ncompared to methods in other categories. An exception is GraphSAGE, which displays an average\nabsolute improvement of 10.4% on average AUPRC when optimal hyperparameters are used, making\nit competitive with specialized GNNs. BGNN, while impressive on specific datasets such as T-\nSocial, exhibits poor performance on other datasets. This inconsistency may result from the inherent\ninstability of its joint training scheme, especially when compared to the more stable two-step approach\nin XGB-Graph and RF-Graph.\nSpecialized GNNs require hyperparameter tuning to achieve satisfactory performance. Gen-\nerally, specialized GNNs outperform standard GNNs in Figure 1, indicating that GNNs tailored\nfor GAD can indeed enhance anomaly detection capabilities. However, the performance of these\n6\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGAT-sep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.6\n0.8\nAUROC\nSemi-Supervised\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGAT-sep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.0\n0.4\n0.8\nAUPRC\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGAT-sep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.0\n0.4\n0.8\nRec@K\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGATSep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.6\n0.8\nAUROC\nFully-Supervised\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGATSep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.0\n0.4\n0.8\nAUPRC\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGATSep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0.0\n0.4\n0.8\nRec@K\n0.70\n0.72\n0.74\n0.76\n0.78\n0.25\n0.30\n0.35\n0.40\n0.25\n0.30\n0.35\n0.40\n0.74\n0.78\n0.82\n0.86\n0.30\n0.40\n0.50\n0.60\n0.30\n0.40\n0.50\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGATSep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n102\n103\n104\n105\nTime (s)\nMLP\nKNN\nSVM\nRF\nXGBoost\nXGBOD\nNA\nGCN\nSGC\nGIN\nG-SAGE\nGAT\nGT\nPNA\nBGNN\nGAS\nDCI\nPCGNN\nGATSep\nBernNet\nAMNet\nBWGNN\nGHRN\nRF-Graph\nXGB-Graph\n0\n4\n8\n12\n16\n20\nMemory (GB)\nCPU\nGPU\nFigure 1: Comparison of the anomaly detection performance, wall-clock time (on all datasets), and\npeak CPU/GPU memory utilization (on DGraph-Fin) among all models with default hyperparameters.\nTop three lines are in semi-supervised settings and the others are in fully-supervised settings. The\ncolor of the box plot represents the average score for each metric, while the central line within the\nbox indicates the median score.\nspecialized GNNs strongly depends on hyperparameter tuning. As indicated in the last column\nof Table 4, all these methods witness a performance improvement after a hyperparameter search.\nFor instance, when optimized hyperparameters are employed, BWGNN can surpass RF-Graph and\nXGB-Graph on the Reddit dataset. This demonstrates that under certain conditions, some specialized\nGNNs can deliver commendable performance. However, as illustrated at the bottom of Figure 1, these\nGNNs often demand more training time and memory. The inherent limitations of hyperparameter\n7\nTable 4: Comparison of the AUPRC score of each model with optimal hyperparameters through\nrandom search. Best results are highlighted in bold. In the last two columns, Ave. signifies the\naverage score across the first 9 datasets without T-Social, while Imp. denotes the absolute increase in\nthis average score when compared to the default hyperparameters. OOT means the model could not\ncomplete training within a day. Results for other metrics can be found in Appendix D and Table 13.\nModel\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nImp.\nMLP\n5.91\n84.88\n87.34\n47.68\n74.21\n43.77\n38.29\n15.34\n2.69\n9.69\n44.46\n2.67\nKNN\n6.12\n81.12\n84.41\n54.39\n74.97\n60.98\n35.30\n15.37\n1.67\n36.32\n46.04\n9.13\nSVM\n6.88\n84.91\n85.80\n41.01\n78.10\n20.98\n37.90\n15.37\n2.65\nOOT\n41.51\n4.53\nRF\n4.63\n93.52\n91.18\n77.77\n81.99\n78.42\n38.64\n14.37\n2.57\n41.56\n53.68\n0.81\nXGBoost\n5.56\n94.49\n91.88\n84.00\n82.64\n76.93\n40.05\n16.24\n2.75\n16.60\n54.95\n0.73\nXGBOD\n8.27\n95.70\n92.15\n79.46\n82.32\n74.86\n40.65\n16.08\n1.95\nOOT\n54.61\n1.62\nNA\n9.70\n94.09\n91.56\n63.93\n88.78\n29.14\n51.06\n14.32\n4.13\n79.21\n49.64\n3.38\nGCN\n4.63\n94.64\n45.65\n20.88\n78.22\n25.37\n40.57\n14.06\n3.80\n76.35\n36.42\n1.54\nSGC\n6.04\n91.16\n42.69\n19.87\n68.68\n17.82\n39.59\n10.53\n2.49\n16.28\n33.21\n5.66\nGIN\n6.41\n91.67\n84.61\n33.63\n78.35\n26.21\n40.36\n13.68\n3.47\n60.79\n42.04\n2.57\nGraphSAGE\n5.56\n94.02\n82.45\n46.64\n84.71\n57.82\n51.41\n17.50\n3.77\n75.32\n49.32\n10.44\nGAT\n7.20\n92.91\n87.94\n43.62\n82.72\n27.53\n45.25\n15.51\n3.85\n32.07\n45.17\n2.80\nGT\n7.68\n89.85\n84.90\n44.60\n83.14\n25.90\n45.71\n17.08\n3.83\n36.14\n44.74\n5.42\nPNA\n7.75\n96.04\n35.24\n29.95\n76.67\n27.81\n41.74\n11.38\n3.22\n21.24\n36.64\n4.93\nBGNN\n6.87\n95.99\n67.92\n29.71\n83.72\n62.03\n45.35\n9.43\n4.24\n99.09\n45.03\n0.95\nGAS\n4.43\n96.76\n81.43\n35.11\n85.95\n29.80\n47.21\n15.48\n3.65\n62.36\n44.42\n6.62\nDCI\n7.74\n91.77\n85.17\n39.88\n63.68\n27.39\n37.73\n14.59\n3.31\n12.97\n41.25\n1.01\nPCGNN\n7.73\n89.07\n89.33\n44.51\n83.31\n42.66\n44.85\n15.59\n3.42\n80.29\n46.72\n4.69\nBernNet\n7.82\n92.38\n84.89\n51.92\n89.17\n38.25\n43.69\n17.25\n3.27\n44.30\n47.63\n2.90\nAMNet\n7.87\n94.99\n88.36\n46.86\n88.87\n25.18\n40.74\n15.63\n2.81\n37.70\n45.70\n2.49\nGAT-sep\n7.19\n93.40\n84.72\n45.49\n84.01\n26.35\n46.66\n17.90\n3.84\n33.39\n45.50\n2.98\nBWGNN\n8.32\n94.01\n91.48\n61.53\n89.38\n29.31\n49.58\n18.57\n3.97\n78.93\n49.57\n2.12\nGHRN\n4.66\n95.27\n89.52\n55.42\n87.60\n43.90\n47.45\n18.31\n3.80\n86.78\n49.55\n1.77\nRF-Graph\n5.13\n96.95\n90.53\n83.92\n89.23\n78.86\n52.34\n14.44\n2.15\n97.63\n57.06\n1.21\nXGB-Graph\n5.29\n97.06\n93.33\n91.11\n90.12\n77.78\n53.92\n18.19\n3.79\n97.34\n58.95\n1.34\nsearch also pose significant challenges, especially in real-world applications where there might be a\nscarcity of annotated labels or computational resources. Given these constraints, tree ensembles with\nneighborhood aggregation might still be the preferred choice.\nNA is a versatile technique adaptable to any model in GADBench. We apply it to XGBoost and\nobserve a remarkable enhancement in the semi-supervised setting, where the average AUPRC\nacross 10 datasets increases from 37.5% to 38.9%. However, the boost is not significant in the\nfully-supervised setting. These findings highlight NA\u2019s potential as an effective strategy to address\nchallenges associated with label scarcity. For additional results related to the application of NA on\nother models, please refer to Appendix D.\nFinally, we observe that all methods perform poorly on the DGraph-Fin dataset. This can be attributed\nto the highly imbalanced and sparse graph structure, with an average degree of 1.16. Furthermore,\nwe find that node features in this dataset are highly indistinguishable, as nearly all anomalous nodes\nshare identical features with normal nodes. Indeed, the AUROC scores of all models on this dataset\nalign with those reported in the original paper, as demonstrated in Appendix D.\n4.2\nSpecialized Experiments in Heterogeneous and Inductive Settings\nTable 5: Comparison of heterogeneous and homogeneous GAD methods on Amazon and Yelp\ndatasets under both fully-supervised and semi-supervised settings. Results are averaged over 10 runs.\nAmazon (Semi-Supervised)\nAmazon (Fully-Supervised)\nYelp (Semi-Supervised)\nYelp (Fully-Supervised)\nModel\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nGAT\n92.44\n81.57\n77.07\n96.66\n86.67\n83.10\n65.56\n25.03\n28.08\n79.50\n43.41\n43.65\nBWGNN\n91.83\n81.68\n77.71\n97.95\n89.09\n85.00\n64.30\n23.66\n26.44\n84.89\n55.06\n52.18\nRGCN\n84.17\n41.07\n45.57\n92.03\n67.97\n65.49\n72.20\n26.46\n28.54\n78.34\n34.57\n34.98\nHGT\n79.75\n38.13\n45.07\n89.64\n71.46\n70.22\n72.83\n28.49\n31.59\n89.62\n62.63\n57.75\nCARE-GNN\n86.00\n58.95\n59.12\n90.84\n72.64\n67.72\n91.19\n68.69\n65.35\n95.23\n81.06\n74.85\nH2Detector\n71.00\n29.27\n32.61\n78.66\n39.95\n44.35\n67.28\n22.23\n24.08\n89.07\n59.40\n57.54\nXGB-Graph\n94.68\n84.38\n78.17\n98.69\n92.61\n85.87\n64.03\n24.84\n26.81\n96.22\n87.03\n78.76\n8\nTable 6: Performance comparison on the Elliptic and DGraph-Fin datasets under inductive and\ntransductive settings, with all results being averaged over 10 runs.\nElliptic (Inductive)\nElliptic (Transductive)\nDGraph-Fin (Inductive)\nDGraph-Fin (Transductive)\nModel\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nGCN\n75.79\n14.97\n16.73\n92.40\n73.87\n69.99\n73.99\n3.35\n5.61\n75.85\n3.99\n7.05\nGraphSAGE\n79.51\n19.64\n20.59\n82.85\n34.76\n45.95\n72.66\n3.06\n5.43\n75.63\n3.76\n6.97\nBWGNN\n82.29\n22.49\n28.26\n96.12\n86.58\n81.14\n73.85\n3.24\n5.83\n76.26\n4.01\n7.52\nGHRN\n84.74\n25.42\n28.54\n96.05\n86.57\n81.11\n76.20\n4.03\n7.48\n76.14\n3.99\n7.54\nXGB-Graph\n90.36\n76.20\n70.64\n96.80\n89.58\n84.59\n71.23\n2.81\n5.33\n74.64\n3.66\n6.75\nDealing with heterogeneous graphs. We evaluated the performance of four GAD methods that\nconsider heterogeneity\u2014RGCN, HGT, CARE-GNN, and H2-FDetector\u2014using the Amazon and Yelp\ndatasets, each comprising three different types of edges as detailed in Appendix B. For comparison,\nwe also tested three other methods\u2014GAT, BWGNN, and XGB-Graph\u2014that treat all edge types\nequivalently. As shown in table 5, considering heterogeneity does not enhance performance on\nthe Amazon dataset, but it leads to improvements on the Yelp dataset. Specifically, CARE-GNN\noutperforms all other methods under label-scarce conditions.\nPerformance in the inductive setting. Our primary experiments focus on the transductive setting,\ncharacterized by the assumption that all nodes are visible in the training process. To offer a holistic\nevaluation, we also conduct experiments in the inductive setting using DGraph-Fin and Elliptic\ndatasets which have temporal features. In this setting, features and structures associated with test\nnodes are not accessible during the training phase. As presented in table 6, the model performance is\ngenerally impacted in the inductive setting of two datasets. Specifically, XGB-Graph outperforms\nother models across all metrics on Elliptic, while GHRN stands out as the most robust model on\nDGraph-Fin.\n0\n1\n2\n3\n4\nAggregation Layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nAUPRC\nXGB-Graph\n0\n1\n2\n3\n4\nAggregation Layers\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nRF-Graph\nReddit\nWeibo\nAmazon\nYelp\nTfinance\nElliptic\nTolokers\nQuestions\nDgraphfin\nTsocial\nFigure 2: The impact of different number of neigh-\nbor aggregation layers on the performance of XGB-\nGraph and RF-Graph.\nThe impact of different number of neighbor\naggregation layers.\nFigure 2 illustrates the\nperformance change in XGB-Graph and RF-\nGraph with varying numbers of neighbor ag-\ngregation layers. Observably, the performance\non most datasets improves when the number of\nneighbor aggregation layers increases from 0 to\n2, confirming the effectiveness of the neighbor\naggregation process. However, further incre-\nments in the number of layers do not contribute\nto any significant improvement in the model\nperformance. Consequently, in most instances,\ntwo layers are adequate for XGB-Graph and RF-\nGraph, and utilizing more layers could lead to\nunnecessary computational overhead and mem-\nory usage.\n4.3\nWhy and When Do Tree Ensembles with Neighbor Aggregation Outperform GNNs?\nAn initial study on decision boundaries. Inspired by a recent benchmark about ensemble trees and\nneural networks on the tabular data [28], we explore the possible reasons for the superior performance\nof ensemble trees with neighborhood aggregation. Specifically, our primary investigation focuses on\nthe models\u2019 decision boundaries.\nIn the left panel of Figure 3, we visualize the decision boundaries of GIN and RF-Graph on Amazon\ndataset. For detailed implementations, please see Appendix E. It is observed that the normal and\nanomalous nodes are closely intertwined, making them hard to separate. Unfortunately, GIN tends to\nproduce simple and smooth decision boundaries, leading to frequent misclassification of normal nodes\nin the right bottom corners. Differently, RF-Graph can produce more intricate decision boundaries,\ndemonstrating greater proficiency in distinguishing anomalous data. In the right panel of Figure 3,\nwe visualize the decision boundaries of BWGNN and XGB-Graph on Weibo dataset. As can be seen,\nthe anomalous nodes are grouped into several dispersed clusters. With this dispersed distributions,\nBWGNN is hard to achieve accurate classification due to simple and continuous decision boundaries.\nIn contrast, XGB-Graph successfully classifies anomalies within each cluster.\n9\nFigure 3: Decision boundaries comparison of different approaches. Blue points represent anomalies\nwhile red points are normal nodes. Similarly, the blue/red regions correspond to model predictions\nfor anomalous/normal classes.\nIn summary, anomaly instances tend to form multiple dispersed clusters and are coupled with normal\ninstances, which fall in the categories of the inductive bias of RF-Graph and XGB-Graph that favor\ncomplex and disjoint decision boundaries. In contrast, as GNNs typically employ MLP as the\nfinal layer, they tend to generate simple and continuous decision boundaries, which makes GNNs\nsub-optimal on some challenging GAD datasets.\nThe impact of dataset feature types on model performance. As indicated in Table 3, out of the 10\ndatasets in GADBench, 3 datasets purely use text embeddings as node features, while in the remaining\n7 datasets, node features contain miscellaneous information such as the combination of numerical,\ncategorical, and temporal features. Notably, for datasets that rely on text-based features\u2014namely\nReddit, Weibo, and Questions\u2014GNNs showcase competitive performance in comparison to other\nmethods including tree ensembles. This could be attributed to the nature of text embeddings: they\noften represent low-dimensional manifolds in a high-dimensional feature space, where dimensions\ntend to be highly correlated. A GNN can process all these dimensions simultaneously, whereas an\nindividual decision tree might only consider a limited subset of feature columns. Conversely, in the\nother 7 datasets with diverse feature types that have low correlation (for instance, gender and age\ninformation), tree ensembles with neighbor aggregation typically exhibit superior performance.\nIn conclusion, in common GAD scenarios such as fraud detection, node features mainly originate\nfrom user profiles, which may encompass varied feature types. Thus, tree ensembles with neighbor\naggregation often emerge as the preferred choice. However, for specific tasks like fake news and\nrumor detection, where text data is pivotal, GNNs still present a compelling option.\n5\nConclusion and Future Plan\nIn this paper, we introduce GADBench, the first comprehensive benchmark for supervised anomalous\nnode detection on static attributed graphs. Our evaluation of 29 models on 10 real-world datasets\nshows that tree ensembles with simple neighborhood aggregation generally outperform other models,\nincluding GNNs specifically designed for the GAD task. The rationale behind this finding is initially\nexamined from the standpoints of decision boundary and node feature type. Our results challenge the\nprevailing belief about the superiority of GNNs in GAD and underline the importance of a fair and\ncomprehensive comparison in accurately understanding the capabilities of various models. By making\nGADBench open-source, we aim to foster further research and refinement of GAD algorithms, as\nwell as their more informed evaluations and comparisons.\nWe regard GADBench as a long-term evolving project and are dedicated to its continuous development.\nOur roadmap for the future includes expanding its scope to include a broader spectrum of GAD\nscenarios, incorporating more cutting-edge models, and integrating newer datasets. At present, we\nprimarily focus on treating datasets as static graphs to ensure compatibility with most baselines. We\nhave only embarked on preliminary studies concerning heterogeneous and inductive settings. Looking\nahead, we envision extending our evaluations to more complex types of graphs and anomalies. Our\nultimate goal is to transform GADBench into a more robust, scalable GAD toolbox, with advanced\nfeatures like automated model selection [95].\nAcknowledgement This research was supported by NSFC Grant No. 62206067, Tencent AI\nLab Rhino-Bird Focused Research Program and Guangzhou-HKUST(GZ) Joint Funding Scheme\n2023A03J0673.\n10\nReferences\n[1] Andra Acsintoae, Andrei Florescu, Mariana-Iuliana Georgescu, Tudor Mare, Paul Sumedrea, Radu Tudor\nIonescu, Fahad Shahbaz Khan, and Mubarak Shah. Ubnormal: New benchmark for supervised open-set\nvideo anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 20143\u201320153, June 2022.\n[2] Charu C Aggarwal. Outlier ensembles: position paper. ACM SIGKDD Explorations Newsletter, 14(2):49\u2013\n58, 2013.\n[3] Leman Akoglu, Mary McGlohon, and Christos Faloutsos. Oddball: Spotting anomalies in weighted\ngraphs. In Advances in Knowledge Discovery and Data Mining: 14th Pacific-Asia Conference, PAKDD\n2010, Hyderabad, India, June 21-24, 2010. Proceedings. Part II 14, pages 410\u2013421. Springer, 2010.\n[4] Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and description: a\nsurvey. Data mining and knowledge discovery, 29(3):626\u2013688, 2015.\n[5] Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. The Annals of Statistics,\n47(2):1148\u20131178, 2019.\n[6] Mariana Belgiu and Lucian Dr\u02d8agu\u00b8t. Random forest in remote sensing: A review of applications and\nfuture directions. ISPRS journal of photogrammetry and remote sensing, 114:24\u201331, 2016.\n[7] James Bergstra, Daniel Yamins, and David Cox. Making a science of model search: Hyperparameter\noptimization in hundreds of dimensions for vision architectures. In International conference on machine\nlearning, pages 115\u2013123. PMLR, 2013.\n[8] Tian Bian, Xi Xiao, Tingyang Xu, Peilin Zhao, Wenbing Huang, Yu Rong, and Junzhou Huang. Rumor\ndetection on social media with bi-directional graph convolutional networks. In Proceedings of the AAAI\nConference on Artificial Intelligence, 2020.\n[9] G\u00e9rard Biau. Analysis of a random forests model. The Journal of Machine Learning Research, 13(1):1063\u2013\n1095, 2012.\n[10] Azzedine Boukerche, Lining Zheng, and Omar Alfandi. Outlier detection: Methods, models, and\nclassification. ACM Computing Surveys (CSUR), 53(3):1\u201337, 2020.\n[11] Xavier Bouthillier, Pierre Delaunay, Mirko Bronzi, Assya Trofimov, Brennan Nichyporuk, Justin Szeto,\nNazanin Mohammadi Sepahvand, Edward Raff, Kanika Madan, Vikram Voleti, et al. Accounting for\nvariance in machine learning benchmarks. Proceedings of Machine Learning and Systems, 3:747\u2013769,\n2021.\n[12] Leo Breiman. Random forests. Machine learning, 45:5\u201332, 2001.\n[13] Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and Weihao Jiang. Can\nabnormality be detected by graph neural networks? In IJCAI, 2022.\n[14] Deepayan Chakrabarti. Autopart: Parameter-free graph partitioning and outlier detection. In European\nconference on principles of data mining and knowledge discovery, pages 112\u2013124. Springer, 2004.\n[15] Chih-Chung Chang and Chih-Jen Lin. Libsvm: a library for support vector machines. ACM transactions\non intelligent systems and technology (TIST), 2(3):1\u201327, 2011.\n[16] Tianqi Chen and Carlos Guestrin. Xgboost: A scalable tree boosting system. In Proceedings of the 22nd\nacm sigkdd international conference on knowledge discovery and data mining, pages 785\u2013794, 2016.\n[17] Gabriele Corso, Luca Cavalleri, Dominique Beaini, Pietro Li\u00f2, and Petar Veli\u02c7ckovi\u00b4c. Principal neighbour-\nhood aggregation for graph nets. Advances in Neural Information Processing Systems, 33:13260\u201313271,\n2020.\n[18] Thomas Cover and Peter Hart. Nearest neighbor pattern classification. IEEE transactions on information\ntheory, 13(1):21\u201327, 1967.\n[19] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs\nwith fast localized spectral filtering. NeurIPS, pages 3844\u20133852, 2016.\n[20] Ailin Deng and Bryan Hooi. Graph neural network-based anomaly detection in multivariate time series.\nIn Proceedings of the AAAI conference on artificial intelligence, pages 4027\u20134035, 2021.\n11\n[21] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed networks.\nIn Proceedings of the SDM, pages 594\u2013602. SIAM, 2019.\n[22] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph neural\nnetwork-based fraud detectors against camouflaged fraudsters. In CIKM, pages 315\u2013324, 2020.\n[23] Yingtong Dou, Guixiang Ma, Philip S Yu, and Sihong Xie. Robust spammer detection by nash rein-\nforcement learning. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, 2020.\n[24] Jingcan Duan, Siwei Wang, Pei Zhang, En Zhu, Jingtao Hu, Hu Jin, Yue Liu, and Zhibin Dong. Graph\nanomaly detection via multi-scale contrastive learning networks with augmented view. arXiv preprint\narXiv:2212.00535, 2022.\n[25] Zhouyu Fu, Weiming Hu, and Tieniu Tan. Similarity based vehicle trajectory clustering and anomaly\ndetection. In IEEE International Conference on Image Processing 2005, volume 2, pages II\u2013602. Ieee,\n2005.\n[26] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Addressing\nheterophily in graph anomaly detection: A perspective of graph spectrum. In Proceedings of the ACM\nWeb Conference, 2023.\n[27] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang. Alleviating\nstructural distribution shift in graph anomaly detection. In Proceedings of the Sixteenth ACM International\nConference on Web Search and Data Mining, pages 357\u2013365, 2023.\n[28] Leo Grinsztajn, Edouard Oyallon, and Gael Varoquaux. Why do tree-based models still outperform deep\nlearning on typical tabular data? In Thirty-sixth Conference on Neural Information Processing Systems\nDatasets and Benchmarks Track, 2022.\n[29] Zolt\u00e1n Gy\u00f6ngyi, Hector Garcia-Molina, and Jan O. Pedersen. Combating web spam with trustrank. In\n(e)Proceedings of the Thirtieth International Conference on Very Large Data Bases, VLDB, 2004.\n[30] William L Hamilton, Rex Ying, and Jure Leskovec. Inductive representation learning on large graphs. In\nNeurIPS, 2017.\n[31] Songqiao Han, Xiyang Hu, Hailiang Huang, Mingqi Jiang, and Yue Zhao. ADBench: Anomaly detection\nbenchmark. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[32] Mingguo He, Zhewei Wei, Hongteng Xu, et al. Bernnet: Learning arbitrary graph spectral filters via\nbernstein approximation. Advances in Neural Information Processing Systems, 34:14239\u201314251, 2021.\n[33] NA Heard, David J Weston, K Platanioti, and DJ Hand. Bayesian anomaly detection methods for social\nnetworks. The Annals of Applied Statistics, 4(2):645\u2013662, 2010.\n[34] Keith Henderson, Tina Eliassi-Rad, Christos Faloutsos, Leman Akoglu, Lei Li, Koji Maruhashi, B Aditya\nPrakash, and Hanghang Tong. Metric forensics: a multi-level approach for mining volatile graphs. In\nProceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data\nmining, pages 163\u2013172, 2010.\n[35] Ziniu Hu, Yuxiao Dong, Kuansan Wang, and Yizhou Sun.\nHeterogeneous graph transformer.\nIn\nProceedings of the web conference 2020, pages 2704\u20132710, 2020.\n[36] Xuanwen Huang, Yang Yang, Yang Wang, Chunping Wang, Zhisheng Zhang, Jiarong Xu, Lei Chen,\nand Michalis Vazirgiannis. Dgraph: A large-scale financial dataset for graph anomaly detection. In\nThirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track,\n2022.\n[37] Sergei Ivanov and Liudmila Prokhorenkova. Boost then convolve: Gradient boosting meets graph neural\nnetworks. In ICLR, 2021.\n[38] Minqi Jiang, Chaochuan Hou, Ao Zheng, Xiyang Hu, Songqiao Han, Hailiang Huang, Xiangnan\nHe, Philip S Yu, and Yue Zhao. Weakly supervised anomaly detection: A survey. arXiv preprint\narXiv:2302.04549, 2023.\n[39] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In\nICLR, 2017.\n12\n[40] Danai Koutra, Tai-You Ke, U Kang, Duen Horng Polo Chau, Hsing-Kuo Kenneth Pao, and Christos\nFaloutsos. Unifying guilt-by-association approaches: Theorems and fast algorithms. In Joint European\nConference on Machine Learning and Knowledge Discovery in Databases, pages 245\u2013260. Springer,\n2011.\n[41] Srijan Kumar, Bryan Hooi, Disha Makhija, Mohit Kumar, Christos Faloutsos, and VS Subrahmanian.\nRev2: Fraudulent user prediction in rating platforms. In Proceedings of the Eleventh ACM International\nConference on Web Search and Data Mining, pages 333\u2013341, 2018.\n[42] Srijan Kumar, Xikun Zhang, and Jure Leskovec. Predicting dynamic embedding trajectory in temporal\ninteraction networks. In Proceedings of the 25th ACM SIGKDD international conference on knowledge\ndiscovery & data mining, pages 1269\u20131278, 2019.\n[43] Kingsly Leung and Christopher Leckie. Unsupervised anomaly detection in network intrusion detection\nusing clusters. In Proceedings of the Twenty-eighth Australasian conference on Computer Science-Volume\n38, pages 333\u2013342, 2005.\n[44] Ao Li, Zhou Qin, Runshi Liu, Yiqun Yang, and Dong Li. Spam review detection with graph convolutional\nnetworks. In Proceedings of the 28th ACM International Conference on Information and Knowledge\nManagement, pages 2703\u20132711, 2019.\n[45] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection in\nattributed networks. In IJCAI, pages 2152\u20132158, 2017.\n[46] Tianfu Li, Zheng Zhou, Sinan Li, Chuang Sun, Ruqiang Yan, and Xuefeng Chen. The emerging graph\nneural networks for intelligent fault diagnostics and prognostics: A guideline and a benchmark study.\nMechanical Systems and Signal Processing, 168:108653, 2022.\n[47] Fanzhen Liu, Xiaoxiao Ma, Jia Wu, Jian Yang, Shan Xue, Amin Beheshti, Chuan Zhou, Hao Peng,\nQuan Z Sheng, and Charu C Aggarwal. Dagad: Data augmentation for graph anomaly detection. arXiv\npreprint arXiv:2210.09766, 2022.\n[48] Fei Tony Liu, Kai Ming Ting, and Zhi-Hua Zhou. Isolation-based anomaly detection. ACM Transactions\non Knowledge Discovery from Data (TKDD), 6(1):1\u201339, 2012.\n[49] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang, Kaize Ding, Canyu\nChen, Hao Peng, Kai Shu, Lichao Sun, Jundong Li, George H Chen, Zhihao Jia, and Philip S Yu. Bond:\nBenchmarking unsupervised outlier node detection on static attributed graphs. In Advances in Neural\nInformation Processing Systems, volume 35, 2022.\n[50] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing He. Pick and choose:\nA gnn-based imbalanced learning approach for fraud detection. In Proceedings of the Web Conference\n2021, 2021.\n[51] Zhiwei Liu, Yingtong Dou, Philip S Yu, Yutong Deng, and Hao Peng. Alleviating the inconsistency\nproblem of applying graph neural network to fraud detection. In SIGIR, pages 1569\u20131572, 2020.\n[52] Zhiyuan Liu, Chunjie Cao, and Jingzhang Sun. Mul-gad: a semi-supervised graph anomaly detection\nframework via aggregating multi-view information. arXiv preprint arXiv:2212.05478, 2022.\n[53] Zhiyuan Liu, Chunjie Cao, Fangjian Tao, and Jingzhang Sun. Revisiting graph contrastive learning for\nanomaly detection. arXiv preprint arXiv:2305.02496, 2023.\n[54] Ziqi Liu, Chaochao Chen, Longfei Li, Jun Zhou, Xiaolong Li, Le Song, and Yuan Qi. Geniepath: Graph\nneural networks with adaptive receptive paths. In Proceedings of the AAAI Conference on Artificial\nIntelligence, pages 4424\u20134431, 2019.\n[55] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and Leman\nAkoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE Transactions on\nKnowledge and Data Engineering, 2021.\n[56] Julian John McAuley and Jure Leskovec. From amateurs to connoisseurs: modeling the evolution of user\nexpertise through online reviews. In WWW, 2013.\n[57] Caleb C Noble and Diane J Cook. Graph-based anomaly detection. In KDD, 2003.\n[58] Adeola Ogunleye and Qing-Guo Wang. Xgboost model for chronic kidney disease diagnosis. IEEE/ACM\ntransactions on computational biology and bioinformatics, 17(6):2131\u20132140, 2019.\n13\n[59] John Paparrizos, Yuhao Kang, Paul Boniol, Ruey S Tsay, Themis Palpanas, and Michael J Franklin.\nTsb-uad: an end-to-end benchmark suite for univariate time-series anomaly detection. Proceedings of the\nVLDB Endowment, 2022.\n[60] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-\nperformance deep learning library. NeurIPS, 32:8026\u20138037, 2019.\n[61] Dhaval Patel, Giridhar Ganapavarapu, Srideepika Jayaraman, Shuxin Lin, Anuradha Bhamidipaty, and\nJayant Kalagnanam. Anomalykits: Anomaly detection toolkit for time series. In Proceedings of the AAAI\nConference on Artificial Intelligence, pages 13209\u201313211, 2022.\n[62] Fabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier\nGrisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn: Machine\nlearning in python. the Journal of machine Learning research, 12:2825\u20132830, 2011.\n[63] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. Anomalous: A joint modeling\napproach for anomaly detection on attributed networks. In IJCAI, pages 3513\u20133519, 2018.\n[64] Muhammad Shakil Pervez and Dewan Md Farid. Feature selection and intrusion classification in nsl-\nkdd cup 99 dataset employing svms. In The 8th International Conference on Software, Knowledge,\nInformation Management and Applications (SKIMA 2014), pages 1\u20136. IEEE, 2014.\n[65] Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. A\ncritical look at the evaluation of gnns under heterophily: are we really making progress? In ICLR, 2023.\n[66] Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review networks and\nmetadata. In KDD, pages 985\u2013994, 2015.\n[67] Frank Rosenblatt. The perceptron: a probabilistic model for information storage and organization in the\nbrain. Psychological review, 65(6):386, 1958.\n[68] Lukas Ruff, Robert A Vandermeulen, Billy Joe Franks, Klaus-Robert M\u00fcller, and Marius Kloft. Rethink-\ning assumptions in deep anomaly detection. arXiv preprint arXiv:2006.00339, 2020.\n[69] Patricia Iglesias S\u00e1nchez, Emmanuel M\u00fcller, Fabian Laforet, Fabian Keller, and Klemens B\u00f6hm. Statistical\nselection of congruent subspaces for mining attributed graphs. In 2013 IEEE 13th international conference\non data mining, pages 647\u2013656. IEEE, 2013.\n[70] Michael Schlichtkrull, Thomas N Kipf, Peter Bloem, Rianne Van Den Berg, Ivan Titov, and Max Welling.\nModeling relational data with graph convolutional networks. In The Semantic Web: 15th International\nConference, pages 593\u2013607, 2018.\n[71] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classification in network data. AI magazine, 29(3):93\u201393, 2008.\n[72] Fengzhao Shi, Yanan Cao, Yanmin Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu. H2-fdetector: A\ngnn-based fraud detector with homophilic and heterophilic connections. In Proceedings of the ACM Web\nConference 2022, pages 1486\u20131494, 2022.\n[73] Yunsheng Shi, Zhengjie Huang, Shikun Feng, Hui Zhong, Wenjing Wang, and Yu Sun. Masked label\nprediction: Unified message passing model for semi-supervised classification. In IJCAI, 2021.\n[74] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly detection.\nIn International Conference on Machine Learning, 2022.\n[75] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning\nresearch, 9(11), 2008.\n[76] Petar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio.\nGraph attention networks. In ICLR, 2017.\n[77] Miryam Elizabeth Villa-P\u00e9rez, Miguel A Alvarez-Carmona, Octavio Loyola-Gonzalez, Miguel Angel\nMedina-P\u00e9rez, Juan Carlos Velazco-Rossell, and Kim-Kwang Raymond Choo. Semi-supervised anomaly\ndetection algorithms: A comparative summary and future research directions. Knowledge-Based Systems,\n218:106878, 2021.\n[78] Chen Wang, Chengyuan Deng, and Suzhen Wang. Imbalance-xgboost: leveraging weighted and focal\nlosses for binary label-imbalanced classification with xgboost. Pattern Recognition Letters, 136:190\u2013197,\n2020.\n14\n[79] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun Zhou,\nShuang Yang, and Yuan Qi. A semi-supervised graph attentive network for financial fraud detection. In\nICDM, pages 598\u2013607. IEEE, 2019.\n[80] Minjie Wang, Da Zheng, Zihao Ye, Quan Gan, Mufei Li, Xiang Song, Jinjing Zhou, Chao Ma, Lingfan\nYu, Yu Gai, Tianjun Xiao, Tong He, George Karypis, Jinyang Li, and Zheng Zhang. Deep graph library:\nA graph-centric, highly-performant package for graph neural networks. arXiv:1909.01315, 2019.\n[81] Xiang Wang and Ian Davidson. Discovering contexts and contextual outliers using random walks in\ngraphs. In 2009 Ninth IEEE International Conference on Data Mining, pages 1034\u20131039. IEEE, 2009.\n[82] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong Chen. Decoupling\nrepresentation learning and classification for gnn-based anomaly detection. In Proceedings of the 44th\nInternational ACM SIGIR Conference on Research and Development in Information Retrieval, pages\n1239\u20131248, 2021.\n[83] Mark Weber, Giacomo Domeniconi, Jie Chen, Daniel Karl I Weidele, Claudio Bellei, Tom Robinson,\nand Charles E Leiserson. Anti-money laundering in bitcoin: Experimenting with graph convolutional\nnetworks for financial forensics. arXiv preprint arXiv:1908.02591, 2019.\n[84] Felix Wu, Amauri Souza, Tianyi Zhang, Christopher Fifty, Tao Yu, and Kilian Weinberger. Simplifying\ngraph convolutional networks. In ICML, pages 6861\u20136871, 2019.\n[85] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?\nICLR, 2019.\n[86] Xiaowei Xu, Nurcan Yuruk, Zhidan Feng, and Thomas AJ Schweiger. Scan: a structural clustering\nalgorithm for networks. In Proceedings of the 13th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pages 824\u2013833, 2007.\n[87] Jiawei Yang, Yu Chen, and Sylwan Rahardja. Neighborhood representative for improving outlier detectors.\nInformation Sciences, 625:192\u2013205, 2023.\n[88] Jiawei Yang, Susanto Rahardja, and Pasi Franti. Neighborhood averaging for improving outlier detectors.\narXiv preprint arXiv:2303.09972, 2023.\n[89] Jiawei Yang, Sylwan Rahardja, and Susanto Rahardja. Foor: Be careful for outlier-score outliers when\nusing unsupervised outlier ensembles. IEEE Transactions on Computational Social Systems, 2023.\n[90] Ge Zhang, Jia Wu, Jian Yang, Amin Beheshti, Shan Xue, Chuan Zhou, and Quan Z Sheng. Fraudre: Fraud\ndetection dual-resistant to graph inconsistency and imbalance. In 2021 IEEE International Conference on\nData Mining (ICDM), pages 867\u2013876. IEEE, 2021.\n[91] Wentao Zhang, Ziqi Yin, Zeang Sheng, Yang Li, Wen Ouyang, Xiaosen Li, Yangyu Tao, Zhi Yang, and\nBin Cui. Graph attention multi-layer perceptron. In Proceedings of the 28th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, pages 4560\u20134570, 2022.\n[92] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng Jiang. Error-bounded\ngraph anomaly loss for gnns. In CIKM, 2020.\n[93] Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with unsupervised\nrepresentation learning. In 2018 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138.\nIEEE, 2018.\n[94] Yue Zhao, Zain Nasrullah, and Zheng Li. PyOD: A python toolbox for scalable outlier detection. Journal\nof Machine Learning Research, 20(96):1\u20137, 2019.\n[95] Yue Zhao, Ryan Rossi, and Leman Akoglu. Automatic unsupervised outlier model selection. Proceedings\nof the NeurIPS, 34:4489\u20134502, 2021.\n[96] Li Zheng, Zhenpeng Li, Jian Li, Zhao Li, and Jun Gao. Addgraph: Anomaly detection in dynamic graph\nusing attention-based temporal gcn. In IJCAI, pages 4419\u20134425, 2019.\n[97] Jiancheng Zhong, Yusui Sun, Wei Peng, Minzhu Xie, Jiahong Yang, and Xiwei Tang.\nXgbfemf:\nan xgboost-based framework for essential protein prediction. IEEE transactions on nanobioscience,\n17(3):243\u2013250, 2018.\n[98] Hao Zhu and Piotr Koniusz. Simple spectral graph convolution. In International conference on learning\nrepresentations, 2021.\n15\n[99] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Beyond\nhomophily in graph neural networks: Current limitations and effective designs. Advances in Neural\nInformation Processing Systems, 33, 2020.\n[100] Arthur Zimek, Ricardo JGB Campello, and J\u00f6rg Sander. Ensembles for unsupervised outlier detection:\nchallenges and research questions a position paper. Acm Sigkdd Explorations Newsletter, 15(1):11\u201322,\n2014.\n16\nA\nDetailed Description of Models in GADBench\nClassic Methods\n\u2022 MLP (Multi-Layer Perceptron [67]: A type of feedforward neural network composed of multiple\nlayers of interconnected artificial neurons that can learn and make predictions by adjusting the\nweights and biases of the connections between the neurons.\n\u2022 KNN (k-Nearest Neighbors [18]): A non-parametric classification algorithm that assigns labels\nto data points based on the labels of their k nearest neighbors within the feature space.\n\u2022 SVM (Support Vector Machine [15]): A supervised learning algorithm that classifies data by\nidentifying an optimal hyperplane that maximally separates different classes in a high-dimensional\nfeature space.\n\u2022 RF (Random Forest [12]): An ensemble learning algorithm that utilizes bagging to train a\ncollection of individual decision tree models on subsets of the original dataset. The predictions\nfrom these individual models are then combined through averaging or voting, resulting in a robust\nand accurate prediction model.\n\u2022 XGBoost (eXtreme Gradient Boosting [16]): A gradient boosting decision tree framework that\nuses a gradient descent algorithm to minimize the loss function and iteratively adds new trees into\nthe model to correct the errors made by the previous trees.\n\u2022 XGBOD (Extreme Boosting Based Outlier Detection [93]): An enhanced XGBoost model that\nintegrates unsupervised outlier detectors to generate abnormality scores as additional features for\nthe original data. These generated features are concatenated to the original feature set and used in\nan XGBoost classifier for outlier detection.\n\u2022 Neighborhood Averaging [88]: A method that post-processes the outlier scores provided by any\nexisting outlier detector by averaging it with the scores of its neighbors in the feature space. In\nGADBench, we integrated this technique into BWGNN as a case study for in-depth analysis.\nStandard GNN Architectures\n\u2022 GCN (Graph Convolutional Network [39]): A method that utilizes convolution operation on\nthe graph to propagate information from a node to its neighboring nodes, enabling the network to\nlearn a representation for each node based on its local neighborhood.\n\u2022 ChebNet ( Chebyshev Spectral Convolution Network [19]): A variant of GCN that applies\nChebyshev polynomials to approximate the spectral graph convolution operator. This approach\nallows the model to capture both local and global graph structures, making it scalable for larger\ngraphs.\n\u2022 GIN (Graph Isomorphism Network [85]): A type of GNN that learns to capture the structure of\na graph while respecting graph isomorphism. This means it generates identical embeddings for\ngraphs that are structurally identical, regardless of permutations in their node labels.\n\u2022 GraphSAGE (Graph Sample and AggregatE [30]:) A general inductive learning framework\nthat generates node embeddings by sampling and aggregating features from a node\u2019s local neigh-\nborhood.\n\u2022 GAT (Graph Attention Networks [76]): A GNN framework that incorporates the attention\nmechanism. It assigns varying levels of importance to different nodes during the neighborhood\ninformation aggregation process, allowing the model to focus on the most informative parts.\n\u2022 GT (Graph Transformer [73]): An adaptation of the neural network architecture that applies the\nprinciples of the Transformer model to graph-structured data. It uses masks in the self-attention\nprocess to leverage the graph structure and enhance model efficiency\n\u2022 PNA (Principle Neighbor Aggregation)[17]: A novel graph neural network architecture combin-\ning multiple aggregators with degree-scalers in the neighborhood aggregation process.\n\u2022 BGNN (Gradient Boosting Meets Graph Neural Networks) [37]: An end-to-end framework\nthat trains GBDT and GNN jointly by allowing new trees to fit the gradient updates of GNN.\n\u2022 RGCN (Relational Graph Convolution Network) [70]: A specialized framework derived from\nGraph Convolutional Networks, tailored for handling relational data.\n\u2022 HGT (Heterogeneous Graph Transformer) [35]: A GNN framework to model Web-scale het-\nerogeneous graphs by designing node- and edge-type dependent parameters for characterizing\nheterogeneous attention over each edge, thus facilitating the maintenance of dedicated representa-\ntions for different types of nodes and edges.\n17\nGNNs Specialized for Graph Anomaly Detection\n\u2022 GAS (GCN-based Anti-Spam [44]): A highly scalable method for detecting spam reviews. It\nextends GCN to handle heterogeneous and heterophilic graphs and adapts to the graph structure of\nspecific GAD applications using the KNN algorithm.\n\u2022 DCI (Deep Cluster Infomax[82]): A self-supervised learning scheme that decouples node\nrepresentation learning from classification for anomaly detection. It mitigates inconsistencies\nbetween node behavior patterns and label semantics, and captures intrinsic graph properties in\nconcentrated feature spaces by clustering the entire graph into multiple parts.\n\u2022 PC-GNN (Pick and Choose Graph Neural Network [50]): A framework designed for imbalanced\nGNN learning in fraud detection. It uses a label-balanced sampler to select nodes and edges for\ntraining, resulting in a balanced label distribution in the induced sub-graph. Furthermore, it\nemploys a learnable parameterized distance function to select neighbors, filtering out redundant\nlinks and adding beneficial ones for fraud prediction.\n\u2022 BernNet [32]: A GNN variant that offers a robust scheme for designing and learning arbitrary\ngraph spectral filters. It uses an order-K Bernstein polynomial approximation to estimate any filter\nover the normalized Laplacian spectrum of a graph.\n\u2022 GAT-sep [99]: A GNN designed to enhance learning from graph structures under high het-\nerophily. It combines key designs such as ego- and neighbor-embedding separation, higher-order\nneighborhoods, and intermediate representation combinations.\n\u2022 AMNet (Adaptive Multi-frequency Graph Neural Network [13]): A method designed to capture\nboth low-frequency and high-frequency signals through stacking multiple BernNets, and adaptively\ncombine signals of different frequencies.\n\u2022 BWGNN (Beta Wavelet Graph Neural Network [74]): A method proposed to tackle the \u2019right-\nshift\u2019 phenomenon of graph anomalies, i.e., the spectral energy distribution concentrates less\non low frequencies and more on high frequencies. It employs the Beta kernel to address higher\nfrequency anomalies through multiple flexible, spatial/spectral-localized, and band-pass filters\n\u2022 GHRN (Graph Heterophily Reduction Network [26]): A method that addresses the heterophily\nissue in the spectral domain of graph anomaly detection. The approach prune inter-class edges to\nemphasize and delineate the graph\u2019s high-frequency components.\n\u2022 CARE-GNN (CAmouflage-REsistant GNN) [22]: A GNN-based fraud detector designed for\nmulti-relation graphs, which is equipped with three modules that enhance its performance against\ncamouflaged fraudsters.\n\u2022 H2-FDetector (Fraud Detector with Homophilic and Heterophilic Interactions) [72] A het-\nerogeneous GNN framework for fraud detection that facilitates the propagation of analogous\ninformation through homophilic connections and varied information via heterophilic connection.\nB\nDetailed Description of Datasets in GADBench\nReddit [42]: This dataset contains a user-subreddit graph, capturing one month\u2019s worth of posts\nshared across various subreddits. Verified labels of banned users are included. The dataset focuses on\nthe 1,000 most active subreddits and the 10,000 most engaged users, leading to a total of 672,447\ninteractions. Posts were transformed into feature vectors, each representing the Linguistic Inquiry\nand Word Count (LIWC) categories of the text.\nWeibo [42]: This dataset features a graph of users and their associated hashtags from the Tencent-\nWeibo platform, consisting of 8,405 users and 61,964 hashtags. Suspicious activities are defined as\ntwo posts made within specific timeframes, such as 60 seconds. Users that engaged in at least five\nsuch activities are labeled as \u201csuspicious\u201d, while the rest are categorized as \u201cbenign\u201d. This process\nyielded 868 suspicious and 7,537 benign users. The raw feature vector is composed of the location of\na micro-blog post and bag-of-words features.\nYelpChi [66]: This dataset is designed to identify anomalous reviews that unfairly promote or demote\nproducts or businesses on Yelp.com. The graph includes three types of edges: R-U-R (reviews posted\nby the same user), R-S-R (reviews for the same product with the same star rating), and R-T-R (reviews\nfor the same product posted in the same month).\nAmazon [56]: The goal of this dataset is to identify users paid to write fake reviews for products\nin the Musical Instrument category on Amazon.com. The graph includes three types of relations:\n18\nTable 7: Overview of datasets in GADBench with their corresponding node feature types, feature\ndimension, and detailed descriptions.\nDataset\nNode Feature Type\n#Dim.\nDetailed Feature Description\nReddit\nText Embedding\n64\nLIWC text embedding for posts\nWeibo\nText Embedding\n400\nBag-of-words features from posts\nAmazon\nMisc. Information\n25\nHand-crafted user features and statistics\nYelpChi\nMisc. Information\n32\nHand-crafted review features and statistics\nTolokers\nMisc. Information\n10\nUser profile with task performance statistics\nQuestions\nText Embedding\n301\nFastText embeddings for user descriptions\nT-Finance\nMisc. Information\n10\nUser profile details such as registration days\nElliptic\nMisc. Information\n166\nTimestamps and transaction information\nDGraph-Fin\nMisc. Information\n17\nTimestamps and user profiles details\nT-Social\nMisc. Information\n10\nUser profile details such as logging activities\nU-P-U (users reviewing at least one same product), U-S-U (users giving at least one same star rating\nwithin one week), and U-V-U (users with top-5% mutual review similarities).\nT-Finance [74]: This dataset aims to find the anomaly accounts in transaction networks. The\nnodes are unique anonymized accounts with 10-dimension features related to registration days,\nlogging activities and interaction frequency. The edges in the graph represent two accounts that have\ntransaction records. Human experts annotate nodes as anomalies if they fall into categories like fraud,\nmoney laundering and online gambling.\nTolokers [65]: This dataset is derived from the Toloka crowd-sourcing platform. Nodes represent\nworkers who have participated in at least one of 13 selected projects, and an edge connects two\nworkers if they worked on the same task. The task is to predict which worker has been banned in one\nof the projects. Node features are based on worker\u2019s profile and task performance statistics.\nQuestions [65]: This dataset is collected from the question-answering website Yandex Q. Nodes\nare users, and an edge connects two users if one user answered the other user\u2019s question during a\none-year period (September 2021 to August 2022). The dataset focuses on users interested in the\ntopic \u201cmedicine\u201d. The task is to predict which users remained active on the website at the end of the\nperiod. Node features are the mean of FastText embeddings for words in the user description, with an\nadditional binary feature indicating users without descriptions.\nElliptic [83]: This dataset includes a graph of over 200,000 Bitcoin transactions (nodes), 234,000\ndirected payment flows (edges), and 166 node features. The dataset maps Bitcoin transactions to\nreal-world entities associated with licit categories, such as exchanges, wallet providers, miners, and\nlegal services, as well as illicit categories like scams, malware, terrorist organizations, ransomware,\nand Ponzi schemes.\nDGraph-Fin [36]: This dataset is a real-world, large-scale dynamic graph provided by the Finvolution\nGroup, representing a social network within the financial industry. In DGraph-Fin, a node represents\na Finvolution user, and an edge between two users indicates that one user lists the other as their\nemergency contact. Anomalous nodes in DGraph-Fin represent users who exhibit overdue behaviors.\nThe dataset comprises over 3 million nodes, 4 million dynamic edges, and more than 1 million\nextremely unbalanced ground-truth nodes.\nT-Social [74]: This dataset aims to find the anomaly accounts in social networks. It has the same\nnode annotations and features as T-Finance, while two nodes are connected if they maintain the\nfriend relationship for more than three months. Same as T-Finance, human experts annotate nodes as\nanomalies if they fall into categories like fraud, money laundering and online gambling.\nC\nOther Information in GADBench\nC.1\nMetrics\nAUPRC (Area Under the Precision-Recall Curve). AUPRC is a metric that evaluates the perfor-\nmance of classification models by computing the area beneath the Precision-Recall curve. This curve\n19\nillustrates the relationship between precision (i.e., the ratio of true positive predictions to all positive\npredictions) and recall (i.e., the ratio of true positive predictions to all positive labels) at different\nthreshold levels. AUPRC can be calculated by the weighted mean of precisions at each threshold,\nwhere the increase in recall from the previous threshold serves as the weight.\nAUROC (Area Under the Receiver Operating Characteristic Curve). It evaluates a model\u2019s\nability to discriminate between positive and negative classes by measuring the area under the ROC\ncurve. The ROC curve plots the true positive rate against the false positive rate for varying decision\nthresholds. An AUROC of 1 indicates perfect discrimination, while an AUROC of 0.5 suggests that\nthe model is no better than random guessing.\nRec@K (Recall at k). It is determined by calculating the recall of the true anomalies among the\ntop-k predictions that the model ranks with the highest confidence. We set the value of k as the\nnumber of actual outliers in the test dataset. It is noteworthy that in this specific scenario, Rec@K is\nequivalent to both precision at k and the F1 score at k.\nRuntime. To assess the efficiency of the various models, including both traditional algorithms and\nneural networks, we measure the runtime as the duration from the beginning to the completion of the\nexperiments. This measurement does not distinguish between computation times on CPUs and GPUs.\nMemory. We record the peak utilization of both CPU and GPU memory during the entire supervised\ntraining phase for each algorithm on the DGraph-Fin dataset. This metric is crucial for assessing the\ncomputational resource requirements of each model in practical implementations.\nC.2\nAdditional Experimental Details\nImplementation Details. To ensure a comprehensive evaluation and maintain fairness across a\nbroad spectrum of models, we develop an open-source toolkit named GADBench3. This toolkit is\nbuilt on top of Pytorch 1.12 [60] and DGL 1.0 [80]. We implement all standard and specialized\nGNNs in GADBench using the DGL library. For classic models KNN, SVM, and RF, we use the\nimplementations provided in the Scikit-Learn library [62]. XGBoost is integrated using its official\nimplementation [16], and XGBOD is included via the PyOD library [94].\nHardware Specifications. All our experiments were carried out on a Linux server equipped with an\nAMD EPYC 75F3 32-Core CPU processor, 64GB RAM, and an NVIDIA RTX A6000 GPU with\n48G memory.\nHyperparameter Settings. Tables 8 and 9 provide a comprehensive list of all hyperparameters used\nin our random search, complete with their default values and respective distributions or search spaces.\nFor all configurations, we retain the model that yields the best AUPRC score on the validation set and\nreport the corresponding test performance. Due to limited data, we did not perform hyperparameter\nsearch for heterogeneous graph neural networks (RGCN, CARE-GNN, and H2-FDetector).\nNew Models and Datasets. GADBench is highly extensible due to its seamless integration with the\nDGL library. Datasets structured in the dgl.graph format, accompanied by binary node labels, can\nbe effortlessly integrated and leveraged by all models within GADBench. In a similar vein, models\nthat accept dgl.graph as input and yield an anomaly score for each node can be smoothly assessed\nacross all datasets in GADBench. We are committed to the ongoing maintenance of GADBench and\nwill persist in incorporating new models and datasets.\nD\nAdditional Experimental Results\nIn accordance with Figure 1, Tables 11 and 12 provide the performance metrics of each model\nunder semi-supervised and fully-supervised settings, respectively, when default hyperparameters\nare employed. Additionally, Table 13 consolidates the information in Table 4 and exhibits the\nperformance of the models in terms of AUROC and Rec@K scores subsequent to hyperparameter\ntuning for each dataset.\nIn table 10, we evaluated the effect of incorporating Neighborhood Averaging (NA) on the perfor-\nmance of four representative graph-based methods in GADBench, namely GIN, BWGNN, XGB-\nGraph, and RF-Graph. In the semi-supervised setting, all of the techniques demonstrate an im-\n3https://github.com/squareRoot3/GADBench\n20\nFigure 4: More decision boundaries comparison examples as a complement to Figure 3. Blue points\nrepresent anomalies while red points are normal nodes. Similarly, the blue/red regions correspond to\nmodel predictions for anomalous/normal classes.\nprovement in performance subsequent to the integration of NA. For example, the AUPRC score is\nimproved by 1.4%\u223c3.7%. In the fully-supervised setting, NA can enhance weaker baselines like\nGIN. However, for stronger baselines, the performance remains similar (on BWGNN) or might even\ndecrease (on XGB-Graph and RF-Graph).\nE\nMore Details About Plotting Decision Boundaries\nAs GNNs require graph structure as the input, it is not feasible to directly illustrate their decision\nboundaries. In the left panel of Figure 3, we use GIN removing the linear layer to embed nodes\ninto the hidden space, following which we employ t-SNE [75] to reduce node embeddings to\ntwo dimensions. We then apply MLP and Random Forest to classify these node embeddings,\napproximating a comparison between GIN and RF-Graph. It is observed that the embeddings of\nnormal and anomalous nodes are entwined, making them hard to separate. Unfortunately, GIN tends\nto produce simple and smooth decision boundaries, leading to recurring misclassification of normal\nnodes in the right bottom corners, thus compromising model performance. Conversely, XGBoost\ncan produce more intricate decision boundaries, demonstrating greater proficiency in distinguishing\nanomalous data.\nThe right panel of Figure 3 showcases a trained BWGNN model on the Weibo dataset, visualizing\nthe node embeddings preceding the final MLP layer. Employing t-SNE again to reduce the node\nembeddings, we compare the decision boundaries of MLP and XGBoost on these embeddings,\nsimulating a comparison between BWGNN and XGB-Graph. As shown in the right panel of Figure\n3, the embeddings of anomalous nodes are grouped into several dispersed clusters after training.\nHowever, the MLP model struggles with accurate classification due to the lack of a simple boundary\nbetween normal and anomalous data. In contrast, XGBoost successfully classifies anomalies within\neach cluster. In Figure 4, we extend our visualization to include two additional datasets, T-Finance\nand Tolokers. This serves as a complement to Figure 3, and the observations remain consistent.\n21\nTable 8: Default hyperparameters and random search space for MLP and GNN models. Elements in\n\u2018[,]\u2019 is randomly selected with equal probability during each trial. RandInt(a,b) returns an random\ninteger between a and b (both included).\nModel\nHyperparameter\nDefault value\nDistribution / Search Space\nShared hyperparameters for all neural networks\nCommon\nlearning rate\n0.01\n10Uniform(\u22123,\u22121)\ndropout rate\n0\n[0,0.1,0.2,0.3]\nhidden dimension\n32\n[16,32,64]\nepochs\n100\n-\nSpecific hyperparameters for each model\nMLP\nlayers\n2\n[1,2,3,4]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nGCN\nlayers\n2\n[1,2,3]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nSGC\nnumber of hops\n2\n[1,2,3,4]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nMLP layers\n1\n[1,2]\nGIN\nlayers\n2\n[1,2,3]\naggregation\nmean\n[sum, mean, max]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nGraphSAGE\nlayers\n2\n[1,2,3]\naggregation\nmean\n[mean, GCN, pool]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nGAT\nlayers\n2\n[1,2,3]\nattention heads\n4\n[1,2,4,8]\nGT\nlayers\n2\n[1,2,3]\nattention heads\n4\n[1,2,4,8]\nPNA\nlayers\n2\n[1,2,3,4]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nBGNN\ndepth\n6\n[4,5,6,7]\niteration per epoch\n10\n[2,5,10,20]\nGBDT learning rate\n0.1\n10Uniform(\u22122,\u22120.5)\nnormalize features\nFalse\n[True, False]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nGAS\nlayers\n2\n[1,2,3,4]\nnumber of neighbors\n5\nRandInt(3,50)\ndistance function\ncosine\n[euclidean, cosine]\nDCI\nlayers\n2\n[1,2,3,4]\npretain epochs\n100\n[20,50,100]\nnumber of clusters\n2\nRandInt(2,30)\nPC-GNN\nlayers\n2\n[1,2,3]\nunder-sample ratio\n0.7\nUniform(0.01,0.8)\nover-sample ratio\n0.3\nUniform(0.01,0.8)\ndistance function\ncosine\n[euclidean, cosine]\nGAT-sep\nlayers\n2\n[1,2,3]\nattention heads\n4\n[1,2,4,8]\nBernNet\nMLP layers\n2\n[1,2]\norders\n2\n[2,3,4,5]\nAMNet\nlayers\n3\n[1,2,3]\norders\n2\n[2,3]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nBWGNN\nlayers\n2\n[1,2,3,4]\nMLP layers\n2\n[1,2]\nactivation\nReLU\n[ReLU, LeakyReLU, Tanh]\nGHRN\nlayers\n2\n[1,2,3,4]\nMLP layers\n2\n[1,2]\ndeletion ratio\n0.015\n10Uniform(\u22122,\u22121)\n22\nTable 9: Default hyperparameters and random search space for non-deep learning models. Elements\nin \u2018[,]\u2019 is randomly selected with equal probability during each trial. RandInt(a,b) returns an random\ninteger between a and b (both included).\nModel\nHyperparameter\nDefault value\nDistribution / Search Space\nSVM\nweights\nuniform\n[uniform, distance]\nL2 regularization C\n1\n10Uniform(\u22121,1)\nRF\nnumber of estimators\n100\nRandInt(10,200)\nsplit criterion\ngini\n[gini, entropy]\nmax samples\n1\nUniform(0.1,1)\nXGBoost\nnumber of estimators\n100\nRandInt(10,200)\nlearning rate \u03b7\n0.3\n0.5 \u221710Uniform(\u22121,0)\nL2 regularization \u03bb\n1\n[0,1,10]\nsubsample rate\n1\n[0.5,0.75,1]\nbooster\ngbtree\n[gbtree, dart]\nXGBOD\nSame as XGBoost\nNA\nSame as XGBoost, with 1 additional hyperparameter:\nnumber of neighbors\n5\nRandInt(0,50)\nRF-Graph\nSame as RF, with 2 additional hyperparameters:\naggregation layers L\n2\n[1,2,3,4]\naggregation function\nmean\n[sum, mean, max]\nXGB-Graph\nSame as XGBoost, with 2 additional hyperparameters:\naggregation layers L\n2\n[1,2,3,4]\naggregation function\nmean\n[sum, mean, max]\nTable 10: The impact of introducing neighborhood averaging (NA) on the performance of four\nrepresentative methods in GADBench.\nSemi-Supervised\nFully-Supervised\nModel\nAUROC\nAUPRC\nRec@K\nAUROC\nAUPRC\nRec@K\nGIN\n73.61\n30.26\n32.14\n79.97\n36.54\n37.52\nwith NA\n74.78\n33.91\n34.18\n80.43\n39.03\n40.01\nBWGNN\n76.00\n35.42\n37.36\n84.73\n48.19\n48.01\nwith NA\n76.95\n36.96\n37.34\n84.70\n47.93\n48.08\nXGB-Graph\n77.74\n42.91\n41.29\n86.71\n61.09\n57.82\nwith NA\n78.41\n44.30\n42.07\n86.05\n59.58\n57.41\nRF-Graph\n78.94\n43.37\n40.83\n85.35\n59.65\n56.90\nwith NA\n79.56\n45.14\n42.28\n85.17\n58.54\n55.33\n23\nTable 11: Comparison of AUPRC (top), AUROC (middle), and Rec@K (bottom) for each model\nemploying default hyperparameters in the semi-supervised setting. Each model is executed 10 times\nwith varying random seeds, and the mean scores along with standard deviations are reported. Ave.\ndenotes the average score across all 10 datasets.\nAUPRC\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nMean\nMLP\n4.4\u00b10.8\n56.2\u00b16.0\n83.0\u00b11.6\n23.6\u00b12.3\n53.4\u00b113.9\n45.0\u00b15.9\n33.3\u00b12.1\n7.7\u00b11.5\n2.3\u00b10.2\n3.9\u00b10.8\n31.3\nKNN\n4.0\u00b10.4\n41.1\u00b17.2\n76.2\u00b11.2\n19.3\u00b11.7\n49.1\u00b16.5\n25.1\u00b13.7\n28.3\u00b11.4\n4.7\u00b11.1\n1.7\u00b10.1\n3.8\u00b10.4\n25.3\nSVM\n4.7\u00b10.5\n44.1\u00b110.0\n62.2\u00b121.5\n25.6\u00b12.0\n20.6\u00b110.8\n26.5\u00b14.2\n33.6\u00b11.6\n7.1\u00b11.6\n1.9\u00b10.3\n3.6\u00b10.6\n23.0\nRF\n4.3\u00b10.4\n61.6\u00b110.0\n81.9\u00b14.2\n27.4\u00b13.5\n74.9\u00b11.5\n85.0\u00b11.9\n33.1\u00b12.3\n11.3\u00b13.6\n2.1\u00b10.2\n7.0\u00b11.3\n38.9\nXGBoost\n4.1\u00b10.4\n62.1\u00b17.6\n84.0\u00b11.8\n26.4\u00b14.2\n70.9\u00b15.0\n79.5\u00b12.3\n31.0\u00b11.9\n9.1\u00b13.4\n2.0\u00b10.2\n6.1\u00b11.3\n37.5\nXGBOD\n4.3\u00b10.6\n64.8\u00b16.6\n85.0\u00b13.2\n26.8\u00b13.8\n70.3\u00b14.4\n78.4\u00b11.7\n30.9\u00b12.4\n9.0\u00b13.2\n2.0\u00b10.2\n4.8\u00b11.0\n37.6\nNA\n4.1\u00b10.4\n69.3\u00b14.4\n85.6\u00b11.6\n27.3\u00b13.8\n73.9\u00b11.5\n79.2\u00b13.3\n31.2\u00b12.0\n10.1\u00b13.5\n2.1\u00b10.1\n6.4\u00b11.3\n38.9\nGCN\n4.2\u00b10.8\n86.0\u00b16.7\n32.8\u00b11.2\n16.4\u00b12.6\n60.5\u00b110.8\n43.1\u00b14.6\n33.0\u00b13.6\n6.1\u00b10.9\n2.3\u00b10.2\n8.4\u00b13.8\n29.3\nSGC\n3.8\u00b10.7\n63.9\u00b17.4\n29.5\u00b12.6\n16.1\u00b11.9\n31.3\u00b118.6\n24.2\u00b12.1\n32.2\u00b13.7\n6.1\u00b10.7\n1.7\u00b10.2\n4.7\u00b11.1\n21.4\nGIN\n4.3\u00b10.6\n67.6\u00b17.4\n75.4\u00b14.3\n23.7\u00b15.4\n44.8\u00b17.1\n40.1\u00b13.2\n31.8\u00b13.2\n6.7\u00b11.1\n2.0\u00b10.1\n6.2\u00b11.7\n30.3\nGraphSAGE\n4.5\u00b10.6\n58.5\u00b16.2\n42.5\u00b16.1\n20.9\u00b13.5\n11.7\u00b15.2\n43.1\u00b15.6\n34.0\u00b12.1\n5.5\u00b11.3\n2.0\u00b10.2\n7.8\u00b11.3\n23.0\nGAT\n4.7\u00b10.7\n73.3\u00b17.3\n81.6\u00b11.7\n25.0\u00b12.9\n28.9\u00b18.6\n44.2\u00b16.6\n33.0\u00b12.0\n7.3\u00b11.2\n2.2\u00b10.2\n9.2\u00b12.0\n30.9\nGT\n4.3\u00b10.7\n78.2\u00b14.7\n71.6\u00b15.5\n23.7\u00b13.0\n17.2\u00b110.9\n46.4\u00b18.0\n34.5\u00b12.0\n7.2\u00b11.6\n2.2\u00b10.3\n8.6\u00b11.5\n29.4\nPNA\n4.0\u00b10.5\n83.4\u00b14.0\n34.6\u00b13.8\n16.8\u00b12.7\n27.4\u00b19.7\n31.3\u00b12.9\n32.6\u00b12.3\n5.7\u00b11.2\n1.8\u00b10.2\n22.8\u00b15.5\n26.0\nBGNN\n4.7\u00b10.5\n81.8\u00b16.3\n39.0\u00b15.9\n17.6\u00b11.3\n75.8\u00b12.2\n35.3\u00b15.3\n33.1\u00b13.6\n5.7\u00b10.7\n2.0\u00b10.2\n58.2\u00b114.6\n35.3\nGAS\n4.7\u00b10.7\n65.7\u00b18.4\n80.7\u00b11.7\n21.7\u00b13.3\n45.7\u00b113.4\n46.0\u00b14.9\n31.7\u00b13.0\n6.3\u00b12.0\n2.5\u00b10.2\n8.6\u00b12.4\n31.4\nDCI\n4.3\u00b10.4\n76.2\u00b14.3\n72.5\u00b17.9\n24.0\u00b14.8\n51.0\u00b17.2\n43.4\u00b14.9\n32.1\u00b14.2\n6.1\u00b11.3\n2.0\u00b10.2\n7.4\u00b12.5\n31.9\nPCGNN\n3.4\u00b10.5\n69.3\u00b19.7\n81.9\u00b11.9\n25.0\u00b13.5\n58.1\u00b111.3\n40.3\u00b16.6\n33.9\u00b11.7\n6.4\u00b11.8\n2.4\u00b10.4\n8.0\u00b11.6\n32.9\nGAT-sep\n4.6\u00b10.7\n76.5\u00b15.8\n80.9\u00b12.6\n24.4\u00b13.7\n34.2\u00b110.1\n46.8\u00b18.1\n33.6\u00b12.2\n7.4\u00b11.3\n2.4\u00b10.3\n10.4\u00b11.9\n32.1\nBernNet\n4.9\u00b10.3\n66.6\u00b15.5\n81.2\u00b12.4\n23.9\u00b12.7\n51.8\u00b112.4\n40.0\u00b14.1\n28.9\u00b13.5\n6.7\u00b12.1\n2.5\u00b10.2\n4.2\u00b11.2\n31.1\nAMNet\n4.9\u00b10.4\n67.1\u00b15.1\n82.4\u00b12.2\n23.9\u00b13.5\n60.2\u00b18.2\n33.3\u00b14.8\n28.6\u00b11.5\n7.4\u00b11.4\n2.2\u00b10.3\n3.1\u00b10.3\n31.3\nBWGNN\n4.2\u00b10.7\n80.6\u00b14.7\n81.7\u00b12.2\n23.7\u00b12.9\n60.9\u00b113.8\n43.4\u00b15.5\n35.3\u00b12.2\n6.5\u00b11.7\n2.1\u00b10.3\n15.9\u00b16.2\n35.4\nGHRN\n4.2\u00b10.6\n77.0\u00b16.2\n80.7\u00b11.7\n23.8\u00b12.8\n63.4\u00b110.4\n44.2\u00b15.7\n35.9\u00b12.0\n6.5\u00b11.7\n2.3\u00b10.3\n16.2\u00b14.6\n35.4\nRF-Graph\n4.5\u00b10.4\n73.8\u00b19.5\n70.7\u00b15.1\n23.6\u00b12.5\n81.1\u00b12.7\n80.8\u00b13.1\n35.8\u00b12.4\n10.1\u00b12.8\n2.0\u00b10.2\n51.3\u00b16.2\n43.4\nXGB-Graph\n4.1\u00b10.5\n75.9\u00b16.2\n84.4\u00b11.1\n24.8\u00b13.1\n78.3\u00b13.1\n77.2\u00b13.2\n34.1\u00b12.8\n7.7\u00b12.1\n1.9\u00b10.2\n40.6\u00b17.6\n42.9\nAUROC\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n59.1\u00b14.3\n66.6\u00b17.8\n92.2\u00b12.3\n64.7\u00b13.2\n89.9\u00b11.4\n89.4\u00b11.4\n68.1\u00b12.5\n61.2\u00b13.1\n69.1\u00b11.2\n59.1\u00b15.2\n71.9\nKNN\n58.9\u00b12.3\n72.5\u00b13.3\n89.5\u00b10.9\n60.3\u00b13.3\n87.7\u00b10.9\n78.6\u00b14.7\n63.0\u00b11.9\n58.6\u00b12.3\n59.8\u00b12.4\n56.9\u00b12.8\n68.6\nSVM\n61.8\u00b12.4\n69.6\u00b14.2\n89.4\u00b14.7\n65.8\u00b13.0\n83.4\u00b19.9\n83.0\u00b13.7\n68.9\u00b11.1\n62.0\u00b12.9\n65.2\u00b13.1\n57.6\u00b15.4\n70.7\nRF\n61.0\u00b12.6\n94.1\u00b11.3\n94.9\u00b10.8\n68.0\u00b13.8\n93.0\u00b10.9\n94.4\u00b11.9\n66.5\u00b12.1\n61.5\u00b14.0\n63.3\u00b14.5\n67.3\u00b13.6\n76.4\nXGBoost\n58.9\u00b12.5\n93.8\u00b10.8\n94.2\u00b11.0\n67.0\u00b15.1\n92.4\u00b11.1\n91.6\u00b11.9\n65.1\u00b11.3\n59.5\u00b15.5\n66.1\u00b12.2\n65.7\u00b12.8\n75.4\nXGBOD\n59.1\u00b13.1\n94.3\u00b10.5\n95.2\u00b10.8\n66.8\u00b15.2\n92.5\u00b11.0\n91.6\u00b11.7\n64.1\u00b12.3\n60.6\u00b13.9\n64.2\u00b12.1\n61.0\u00b14.6\n74.9\nNA\n59.5\u00b12.3\n93.7\u00b10.6\n94.9\u00b11.3\n69.3\u00b13.6\n92.0\u00b11.2\n90.9\u00b12.3\n65.4\u00b11.2\n59.7\u00b15.6\n66.8\u00b11.4\n64.9\u00b13.1\n75.7\nGCN\n56.9\u00b15.9\n93.5\u00b16.6\n82.0\u00b10.3\n51.2\u00b13.7\n88.3\u00b12.5\n86.2\u00b11.9\n64.2\u00b14.8\n60.0\u00b12.2\n66.2\u00b12.5\n71.6\u00b110.4\n72.0\nSGC\n53.6\u00b15.2\n81.1\u00b16.5\n80.2\u00b11.5\n51.1\u00b12.9\n74.5\u00b112.3\n80.6\u00b11.2\n63.6\u00b15.1\n63.6\u00b15.0\n59.5\u00b13.3\n64.8\u00b15.6\n67.3\nGIN\n60.0\u00b14.1\n83.8\u00b18.3\n91.6\u00b11.7\n62.9\u00b17.3\n84.5\u00b14.5\n88.2\u00b10.9\n66.8\u00b15.2\n62.2\u00b12.2\n65.7\u00b11.8\n70.4\u00b17.4\n73.6\nGraphSAGE\n60.3\u00b13.8\n81.8\u00b16.5\n81.4\u00b12.0\n58.9\u00b14.3\n68.9\u00b15.5\n87.4\u00b11.0\n67.6\u00b14.2\n61.2\u00b12.9\n64.8\u00b13.3\n72.0\u00b12.9\n70.4\nGAT\n60.5\u00b13.9\n86.4\u00b17.7\n92.4\u00b11.9\n65.6\u00b14.0\n85.0\u00b14.5\n88.5\u00b12.1\n68.1\u00b13.0\n62.3\u00b11.4\n67.2\u00b11.9\n75.4\u00b14.8\n75.1\nGT\n59.1\u00b14.4\n94.4\u00b14.6\n88.6\u00b12.2\n64.5\u00b14.3\n73.5\u00b17.2\n88.9\u00b11.6\n69.7\u00b12.5\n60.7\u00b13.6\n67.7\u00b12.5\n71.4\u00b12.1\n73.9\nPNA\n58.4\u00b14.4\n93.2\u00b11.8\n79.1\u00b12.9\n53.2\u00b15.5\n78.4\u00b19.7\n83.7\u00b11.1\n65.7\u00b13.1\n62.1\u00b12.9\n60.7\u00b13.9\n84.4\u00b13.5\n71.9\nBGNN\n63.1\u00b13.3\n94.4\u00b15.3\n83.7\u00b12.5\n54.4\u00b11.2\n93.7\u00b10.7\n83.3\u00b12.3\n65.1\u00b14.1\n59.8\u00b13.1\n59.9\u00b13.5\n93.6\u00b11.4\n75.1\nGAS\n60.6\u00b13.0\n81.8\u00b17.0\n91.6\u00b11.9\n61.1\u00b15.2\n88.7\u00b11.1\n89.0\u00b11.4\n62.7\u00b12.8\n57.5\u00b14.4\n69.9\u00b12.0\n72.1\u00b18.8\n73.5\nDCI\n61.0\u00b13.1\n89.3\u00b15.3\n89.4\u00b13.0\n64.1\u00b15.3\n88.0\u00b13.2\n88.5\u00b11.3\n67.6\u00b17.1\n62.2\u00b12.5\n65.3\u00b12.3\n74.2\u00b13.3\n75.0\nPCGNN\n52.8\u00b13.4\n83.9\u00b18.1\n93.2\u00b11.2\n65.1\u00b14.8\n92.0\u00b11.1\n87.5\u00b11.4\n67.4\u00b12.1\n59.0\u00b14.0\n68.4\u00b14.2\n69.1\u00b12.4\n73.8\nGAT-sep\n60.3\u00b14.4\n87.8\u00b15.7\n91.4\u00b12.4\n65.0\u00b14.5\n86.3\u00b13.9\n89.3\u00b12.1\n69.1\u00b12.5\n61.9\u00b14.2\n69.0\u00b11.9\n74.5\u00b13.7\n75.5\nBernNet\n63.1\u00b11.7\n80.1\u00b16.9\n92.1\u00b12.4\n65.0\u00b13.7\n91.2\u00b11.0\n87.0\u00b11.7\n61.9\u00b15.6\n61.8\u00b16.4\n69.0\u00b11.4\n59.8\u00b16.3\n73.1\nAMNet\n62.9\u00b11.8\n82.4\u00b14.6\n92.8\u00b12.1\n64.8\u00b15.2\n92.6\u00b10.9\n85.4\u00b11.7\n61.7\u00b14.1\n63.6\u00b12.8\n67.1\u00b13.2\n53.7\u00b13.4\n72.7\nBWGNN\n57.7\u00b15.0\n93.6\u00b14.0\n91.8\u00b12.3\n64.3\u00b13.4\n92.1\u00b12.7\n88.7\u00b11.3\n68.5\u00b12.7\n60.2\u00b18.6\n65.5\u00b13.1\n77.5\u00b14.3\n76.0\nGHRN\n57.5\u00b14.5\n91.6\u00b14.4\n90.9\u00b11.9\n64.5\u00b13.1\n92.6\u00b10.7\n89.0\u00b11.3\n69.0\u00b12.2\n60.5\u00b18.7\n67.1\u00b13.0\n78.7\u00b13.0\n76.1\nRF-Graph\n61.4\u00b12.4\n96.3\u00b11.1\n92.5\u00b11.3\n61.6\u00b12.7\n95.0\u00b10.7\n93.9\u00b12.1\n70.4\u00b12.3\n64.7\u00b13.6\n64.9\u00b13.2\n88.6\u00b11.6\n78.9\nXGB-Graph\n59.2\u00b12.7\n96.4\u00b10.7\n94.7\u00b10.9\n64.0\u00b13.5\n94.8\u00b10.6\n91.9\u00b11.3\n67.5\u00b13.4\n61.4\u00b12.9\n62.4\u00b14.1\n85.2\u00b11.8\n77.8\nRec@K\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n6.5\u00b12.0\n53.2\u00b15.2\n79.3\u00b11.0\n26.5\u00b12.7\n59.9\u00b110.2\n54.5\u00b16.2\n35.5\u00b12.6\n12.0\u00b12.1\n3.4\u00b10.9\n3.2\u00b12.1\n33.4\nKNN\n5.7\u00b11.6\n46.4\u00b17.8\n75.1\u00b12.0\n12.0\u00b13.1\n57.9\u00b14.4\n31.8\u00b15.4\n31.4\u00b12.1\n8.3\u00b13.4\n2.2\u00b10.7\n5.7\u00b12.2\n27.6\nSVM\n6.4\u00b12.2\n48.0\u00b15.5\n61.8\u00b121.4\n28.5\u00b12.6\n18.1\u00b118.0\n26.6\u00b17.0\n36.1\u00b11.6\n11.2\u00b13.0\n1.6\u00b10.5\n1.6\u00b10.4\n24.0\nRF\n5.6\u00b11.9\n56.4\u00b16.9\n73.1\u00b17.8\n30.5\u00b13.8\n69.3\u00b11.0\n77.0\u00b12.6\n34.9\u00b12.5\n12.9\u00b13.9\n2.8\u00b10.6\n11.5\u00b12.1\n37.4\nXGBoost\n5.0\u00b11.9\n56.8\u00b14.5\n77.5\u00b12.9\n29.5\u00b15.0\n66.9\u00b12.7\n72.1\u00b12.5\n33.1\u00b12.8\n11.5\u00b13.3\n1.8\u00b10.9\n9.2\u00b13.3\n36.3\nXGBOD\n4.9\u00b11.7\n59.3\u00b13.5\n77.9\u00b15.5\n30.2\u00b14.4\n66.9\u00b13.3\n70.5\u00b12.0\n32.6\u00b12.5\n11.3\u00b13.4\n2.3\u00b10.7\n6.8\u00b12.6\n36.3\nNA\n4.6\u00b11.9\n61.8\u00b12.5\n79.2\u00b11.4\n31.0\u00b14.8\n67.1\u00b11.7\n72.5\u00b13.1\n33.3\u00b12.5\n12.1\u00b13.8\n2.1\u00b10.6\n10.6\u00b12.3\n37.4\nGCN\n6.2\u00b12.2\n79.2\u00b14.3\n36.9\u00b12.6\n16.9\u00b13.0\n60.6\u00b17.6\n49.7\u00b14.2\n33.4\u00b13.5\n9.8\u00b11.2\n3.6\u00b10.4\n10.2\u00b18.1\n30.6\nSGC\n5.9\u00b12.9\n64.1\u00b17.6\n33.6\u00b12.3\n16.4\u00b12.2\n35.8\u00b119.4\n26.9\u00b14.4\n32.7\u00b13.2\n10.0\u00b11.4\n2.4\u00b10.8\n4.0\u00b11.7\n23.2\nGIN\n4.8\u00b11.9\n66.5\u00b17.3\n70.4\u00b15.7\n26.5\u00b16.1\n54.4\u00b15.0\n47.6\u00b13.1\n33.6\u00b13.0\n10.3\u00b11.1\n2.1\u00b10.5\n5.3\u00b12.9\n32.2\nGraphSAGE\n5.8\u00b10.8\n63.4\u00b16.0\n48.0\u00b15.6\n22.9\u00b13.6\n18.5\u00b19.4\n48.2\u00b15.8\n35.2\u00b12.2\n8.8\u00b12.5\n2.5\u00b10.7\n9.5\u00b12.9\n26.3\nGAT\n6.5\u00b12.3\n70.2\u00b14.6\n77.1\u00b11.7\n28.1\u00b13.4\n36.2\u00b110.3\n51.4\u00b15.8\n35.1\u00b11.8\n10.9\u00b10.9\n3.1\u00b10.7\n11.6\u00b13.0\n33.0\nGT\n5.6\u00b11.7\n75.4\u00b13.3\n70.2\u00b14.7\n26.8\u00b13.4\n24.4\u00b115.7\n53.1\u00b17.6\n36.3\u00b11.6\n10.7\u00b11.6\n2.8\u00b11.0\n11.5\u00b13.0\n31.7\nPNA\n4.2\u00b11.9\n78.7\u00b14.1\n40.4\u00b13.1\n17.5\u00b14.5\n38.6\u00b114.4\n38.6\u00b13.6\n34.0\u00b12.2\n8.4\u00b12.2\n2.2\u00b10.7\n29.4\u00b19.3\n29.2\nBGNN\n5.6\u00b12.1\n79.7\u00b15.3\n43.2\u00b16.2\n17.5\u00b11.4\n72.9\u00b11.9\n43.4\u00b16.3\n34.7\u00b12.7\n9.0\u00b11.5\n3.6\u00b10.6\n58.2\u00b112.4\n36.8\nGAS\n6.6\u00b12.5\n62.0\u00b16.9\n77.4\u00b11.7\n24.6\u00b14.1\n54.2\u00b19.5\n51.9\u00b15.2\n33.0\u00b13.9\n9.1\u00b12.9\n3.4\u00b10.4\n11.5\u00b14.6\n33.4\nDCI\n4.5\u00b11.4\n68.5\u00b13.5\n68.3\u00b17.2\n26.8\u00b15.3\n58.5\u00b16.3\n50.0\u00b13.8\n33.5\u00b15.6\n9.9\u00b11.9\n2.3\u00b10.7\n6.3\u00b16.8\n32.9\nPCGNN\n3.0\u00b12.1\n65.1\u00b16.6\n78.0\u00b11.5\n27.8\u00b13.8\n63.9\u00b16.3\n46.5\u00b17.3\n34.3\u00b11.6\n10.1\u00b13.9\n3.7\u00b11.0\n13.5\u00b13.1\n34.6\nGAT-sep\n5.8\u00b12.2\n71.1\u00b14.9\n77.5\u00b12.5\n27.2\u00b14.2\n43.6\u00b111.7\n53.9\u00b16.9\n35.2\u00b12.0\n11.2\u00b11.9\n3.4\u00b10.9\n14.9\u00b12.9\n34.4\nBernNet\n6.4\u00b11.5\n60.9\u00b14.6\n77.2\u00b12.1\n26.8\u00b13.1\n60.5\u00b111.1\n47.0\u00b14.5\n30.1\u00b13.8\n10.3\u00b12.7\n3.8\u00b10.6\n3.3\u00b12.8\n32.6\nAMNet\n6.8\u00b11.5\n62.1\u00b14.4\n77.8\u00b12.3\n26.6\u00b14.3\n65.7\u00b16.3\n37.8\u00b16.7\n30.5\u00b11.9\n12.7\u00b12.6\n2.6\u00b10.8\n1.6\u00b10.5\n32.4\nBWGNN\n6.0\u00b11.4\n75.1\u00b13.5\n77.7\u00b11.6\n26.4\u00b13.2\n64.9\u00b111.7\n49.7\u00b16.1\n35.5\u00b13.1\n10.9\u00b13.2\n3.1\u00b10.8\n24.3\u00b17.4\n37.4\nGHRN\n6.3\u00b11.5\n72.4\u00b12.6\n77.7\u00b11.3\n26.9\u00b13.1\n67.7\u00b14.3\n50.8\u00b14.8\n36.1\u00b13.1\n11.1\u00b13.4\n3.4\u00b10.7\n24.6\u00b17.0\n37.7\nRF-Graph\n5.8\u00b11.7\n65.5\u00b17.8\n63.5\u00b14.5\n24.3\u00b12.3\n74.5\u00b13.2\n72.3\u00b13.3\n38.0\u00b12.5\n12.9\u00b13.0\n2.4\u00b10.4\n49.0\u00b15.7\n40.8\nXGB-Graph\n4.9\u00b11.9\n68.9\u00b15.7\n78.2\u00b11.5\n26.8\u00b13.0\n72.4\u00b13.8\n68.9\u00b13.7\n36.6\u00b13.0\n10.6\u00b12.9\n2.5\u00b10.7\n43.0\u00b17.6\n41.3\n24\nTable 12: Comparison of AUPRC (top), AUROC (middle), and Rec@K (bottom) for each model\nemploying default hyperparameters in the fully-supervised setting. Each model is executed 10 times\nwith varying random seeds, and the mean scores along with standard deviations are reported. Ave.\ndenotes the average score across all 10 datasets.\nAUPRC\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n6.0\u00b11.1\n84.8\u00b11.2\n88.0\u00b12.2\n47.7\u00b11.7\n70.5\u00b12.6\n22.7\u00b14.7\n38.5\u00b11.1\n15.2\u00b11.0\n2.7\u00b10.0\n14.7\u00b17.8\n39.1\nKNN\n4.8\u00b10.4\n72.9\u00b11.6\n78.2\u00b10.0\n31.5\u00b10.0\n66.5\u00b11.6\n30.5\u00b10.0\n31.8\u00b10.7\n14.7\u00b11.0\n1.3\u00b10.0\n24.8\u00b10.1\n35.7\nSVM\n6.5\u00b10.8\n72.0\u00b14.0\n80.1\u00b16.6\n40.6\u00b10.0\n58.0\u00b117.4\n19.6\u00b10.2\n37.2\u00b10.9\n16.4\u00b11.3\n2.6\u00b10.0\n3.7\u00b10.8\n33.7\nRF\n5.3\u00b10.6\n92.0\u00b10.8\n89.7\u00b10.4\n76.3\u00b10.3\n80.6\u00b11.3\n77.3\u00b10.4\n37.7\u00b11.1\n14.3\u00b11.0\n2.5\u00b10.0\n41.0\u00b10.1\n51.7\nXGBoost\n5.6\u00b10.7\n95.1\u00b10.7\n91.5\u00b10.0\n82.7\u00b10.0\n79.9\u00b11.1\n77.4\u00b10.0\n38.1\u00b11.3\n15.1\u00b10.9\n2.7\u00b10.0\n15.0\u00b10.2\n50.3\nXGBOD\n7.5\u00b11.4\n95.0\u00b10.4\n89.4\u00b11.6\n66.3\u00b10.4\n81.1\u00b10.6\n75.9\u00b10.0\n41.3\u00b11.2\n18.4\u00b11.7\n2.0\u00b10.0\n8.2\u00b10.0\n48.5\nNA\n5.7\u00b10.7\n92.9\u00b10.9\n91.1\u00b10.0\n76.2\u00b10.0\n79.2\u00b11.3\n78.6\u00b10.0\n39.1\u00b11.3\n15.6\u00b10.9\n2.7\u00b10.0\n15.5\u00b10.2\n49.7\nGCN\n6.5\u00b11.4\n94.5\u00b11.0\n35.1\u00b11.3\n21.1\u00b10.6\n73.9\u00b15.2\n21.9\u00b12.9\n44.5\u00b12.2\n12.5\u00b11.4\n4.0\u00b10.1\n14.7\u00b17.8\n32.9\nSGC\n4.6\u00b10.8\n92.3\u00b11.6\n33.4\u00b12.9\n16.9\u00b11.5\n39.5\u00b124.5\n12.8\u00b10.7\n36.0\u00b13.7\n10.0\u00b10.8\n2.4\u00b10.1\n5.2\u00b11.6\n25.3\nGIN\n5.7\u00b10.9\n90.6\u00b11.3\n79.2\u00b11.5\n36.5\u00b11.5\n64.2\u00b14.6\n23.5\u00b14.9\n39.6\u00b11.8\n12.7\u00b11.2\n3.3\u00b10.1\n10.1\u00b11.6\n36.5\nGraphSAGE\n5.5\u00b10.8\n85.4\u00b12.3\n69.0\u00b112.5\n54.0\u00b13.0\n34.8\u00b110.9\n32.9\u00b16.0\n47.9\u00b11.7\n16.6\u00b10.8\n3.8\u00b10.1\n12.7\u00b13.3\n36.3\nGAT\n5.9\u00b11.0\n91.3\u00b11.3\n86.7\u00b11.4\n43.4\u00b14.6\n63.1\u00b111.0\n25.2\u00b15.6\n46.1\u00b11.8\n15.7\u00b11.3\n3.9\u00b10.1\n9.2\u00b12.0\n39.0\nGT\n5.7\u00b10.9\n88.7\u00b11.9\n80.4\u00b14.0\n55.0\u00b15.4\n31.6\u00b114.7\n25.1\u00b14.5\n47.6\u00b11.9\n15.9\u00b11.4\n3.9\u00b10.1\n9.5\u00b12.1\n36.3\nPNA\n5.4\u00b10.4\n93.6\u00b10.8\n42.5\u00b18.6\n30.5\u00b10.5\n28.5\u00b112.3\n27.5\u00b14.3\n42.3\u00b11.7\n11.9\u00b10.9\n3.5\u00b10.1\n26.4\u00b17.3\n31.2\nBGNN\n7.7\u00b10.6\n96.2\u00b10.8\n66.4\u00b12.0\n26.9\u00b11.7\n82.6\u00b10.9\n64.2\u00b14.6\n43.8\u00b11.0\n5.8\u00b10.4\n3.1\u00b10.1\n98.7\u00b10.2\n49.5\nGAS\n7.1\u00b11.7\n92.9\u00b10.9\n48.9\u00b15.0\n22.3\u00b10.4\n76.9\u00b12.4\n27.9\u00b16.6\n45.7\u00b11.6\n14.8\u00b11.9\n3.8\u00b10.1\n9.3\u00b12.4\n35.0\nDCI\n6.1\u00b10.9\n89.6\u00b11.8\n81.5\u00b12.2\n39.5\u00b17.5\n62.6\u00b15.7\n25.4\u00b14.7\n39.9\u00b11.5\n14.1\u00b11.5\n3.6\u00b10.1\n13.8\u00b14.0\n37.6\nPCGNN\n4.2\u00b10.5\n81.9\u00b11.7\n87.8\u00b11.9\n43.7\u00b12.6\n69.8\u00b18.0\n35.6\u00b110.2\n38.1\u00b12.1\n14.4\u00b10.9\n2.8\u00b10.0\n8.7\u00b12.4\n38.7\nGATSep\n6.1\u00b11.0\n90.9\u00b12.1\n87.1\u00b11.1\n54.9\u00b12.8\n70.3\u00b111.9\n26.4\u00b14.5\n46.5\u00b12.2\n16.5\u00b11.0\n3.9\u00b10.1\n10.4\u00b11.9\n41.3\nBernNet\n7.1\u00b11.2\n89.7\u00b11.9\n86.4\u00b12.7\n50.1\u00b11.5\n72.4\u00b16.9\n21.6\u00b12.9\n43.5\u00b11.3\n15.2\u00b11.1\n2.9\u00b10.0\n5.4\u00b11.0\n39.4\nAMNet\n7.3\u00b11.0\n89.7\u00b12.2\n87.3\u00b12.1\n48.8\u00b11.3\n74.3\u00b12.5\n14.7\u00b12.9\n43.2\u00b11.2\n14.6\u00b11.2\n2.8\u00b10.0\n3.1\u00b10.3\n38.6\nBWGNN\n6.9\u00b11.6\n93.0\u00b11.4\n89.1\u00b11.6\n55.1\u00b11.6\n86.6\u00b10.8\n26.0\u00b13.5\n49.7\u00b11.9\n16.7\u00b11.3\n4.0\u00b10.1\n54.9\u00b116.5\n48.2\nGHRN\n7.2\u00b11.7\n91.8\u00b11.2\n89.5\u00b11.2\n56.6\u00b11.7\n86.6\u00b11.5\n27.7\u00b16.6\n49.9\u00b12.1\n16.7\u00b11.2\n4.0\u00b10.1\n16.3\u00b14.5\n44.6\nRF-Graph\n5.4\u00b10.4\n97.0\u00b10.3\n91.3\u00b10.1\n76.7\u00b10.5\n89.0\u00b10.9\n77.8\u00b10.3\n49.3\u00b11.6\n14.2\u00b11.2\n2.0\u00b10.0\n93.8\u00b10.1\n59.6\nXGB-Graph\n5.6\u00b10.5\n97.5\u00b10.6\n92.6\u00b10.0\n87.0\u00b10.4\n89.6\u00b10.8\n78.2\u00b10.0\n48.9\u00b11.2\n15.3\u00b10.6\n3.7\u00b10.0\n92.4\u00b10.1\n61.1\nAUROC\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n65.5\u00b12.8\n90.9\u00b11.3\n97.3\u00b11.7\n82.0\u00b10.7\n91.4\u00b11.1\n84.2\u00b12.3\n73.3\u00b10.7\n69.3\u00b12.3\n72.5\u00b10.1\n80.2\u00b17.3\n80.7\nKNN\n56.5\u00b11.2\n88.6\u00b11.0\n90.2\u00b10.0\n73.3\u00b10.0\n86.6\u00b11.0\n83.1\u00b10.0\n67.5\u00b10.9\n61.0\u00b11.2\n51.4\u00b10.0\n73.0\u00b10.1\n73.1\nSVM\n67.8\u00b11.7\n84.8\u00b13.0\n94.8\u00b11.3\n77.2\u00b10.0\n91.6\u00b11.6\n84.2\u00b10.2\n72.2\u00b10.6\n66.7\u00b11.8\n72.0\u00b10.0\n58.7\u00b15.6\n77.0\nRF\n62.0\u00b12.8\n98.7\u00b10.2\n97.2\u00b10.3\n93.3\u00b10.1\n94.2\u00b10.5\n90.1\u00b10.9\n71.7\u00b10.8\n57.6\u00b11.6\n69.1\u00b10.1\n79.0\u00b10.1\n81.3\nXGBoost\n61.6\u00b13.5\n99.1\u00b10.1\n97.9\u00b10.0\n95.2\u00b10.0\n94.4\u00b10.4\n91.2\u00b10.0\n73.0\u00b10.8\n57.8\u00b11.4\n71.6\u00b10.0\n81.4\u00b10.1\n82.3\nXGBOD\n68.6\u00b11.6\n99.1\u00b10.1\n98.3\u00b10.3\n89.4\u00b10.4\n95.0\u00b10.3\n93.0\u00b10.0\n75.2\u00b10.8\n69.3\u00b11.3\n64.9\u00b10.0\n69.2\u00b10.0\n82.2\nNA\n62.4\u00b13.2\n98.4\u00b10.3\n97.5\u00b10.0\n94.0\u00b10.0\n93.9\u00b10.3\n92.8\u00b10.0\n73.5\u00b10.9\n59.3\u00b11.6\n71.8\u00b10.0\n81.4\u00b10.1\n82.5\nGCN\n65.2\u00b14.8\n98.0\u00b10.5\n82.4\u00b10.4\n57.8\u00b10.6\n92.4\u00b12.3\n81.4\u00b11.8\n75.6\u00b11.2\n70.9\u00b11.3\n75.9\u00b10.2\n80.2\u00b17.3\n78.0\nSGC\n54.5\u00b14.2\n97.9\u00b10.4\n80.3\u00b12.0\n54.1\u00b12.4\n76.6\u00b113.8\n75.4\u00b11.2\n68.3\u00b14.6\n71.0\u00b11.1\n66.1\u00b10.3\n66.1\u00b18.7\n71.0\nGIN\n65.5\u00b13.2\n95.5\u00b10.8\n93.8\u00b11.3\n77.0\u00b10.9\n88.2\u00b14.2\n82.7\u00b12.0\n75.1\u00b10.9\n69.4\u00b11.3\n74.0\u00b10.2\n78.4\u00b12.2\n80.0\nGraphSAGE\n62.8\u00b15.1\n94.9\u00b11.3\n89.6\u00b15.4\n85.2\u00b11.0\n82.8\u00b13.9\n85.3\u00b10.7\n79.3\u00b10.9\n72.1\u00b11.7\n75.6\u00b10.2\n79.2\u00b14.0\n80.7\nGAT\n65.3\u00b13.1\n95.3\u00b11.4\n96.7\u00b11.0\n79.5\u00b11.9\n92.8\u00b11.5\n84.9\u00b11.9\n79.1\u00b11.0\n71.1\u00b11.6\n75.9\u00b10.2\n75.4\u00b14.8\n81.6\nGT\n63.7\u00b14.4\n95.5\u00b11.2\n92.4\u00b12.8\n84.5\u00b12.2\n81.4\u00b16.5\n85.1\u00b11.5\n79.6\u00b10.8\n70.9\u00b11.2\n75.8\u00b10.1\n72.1\u00b13.3\n80.1\nPNA\n64.6\u00b11.5\n97.8\u00b10.5\n83.2\u00b14.5\n74.3\u00b10.5\n74.3\u00b112.0\n84.0\u00b11.0\n75.4\u00b11.0\n71.8\u00b10.9\n73.4\u00b10.2\n78.2\u00b111.0\n77.7\nBGNN\n72.2\u00b11.8\n98.8\u00b10.3\n92.3\u00b10.5\n64.6\u00b13.5\n95.6\u00b10.4\n90.1\u00b10.5\n74.4\u00b10.9\n62.5\u00b11.2\n68.5\u00b11.3\n99.9\u00b10.0\n81.9\nGAS\n67.5\u00b15.3\n97.5\u00b10.5\n86.1\u00b11.5\n59.1\u00b10.5\n93.3\u00b11.3\n85.6\u00b11.6\n77.4\u00b11.0\n69.4\u00b11.5\n76.0\u00b10.2\n76.9\u00b13.6\n78.9\nDCI\n66.5\u00b13.3\n94.2\u00b11.7\n94.6\u00b10.9\n77.8\u00b17.8\n86.8\u00b14.5\n82.8\u00b11.5\n75.5\u00b10.9\n69.2\u00b11.3\n74.7\u00b10.1\n80.8\u00b16.0\n80.3\nPCGNN\n53.2\u00b12.1\n90.2\u00b11.5\n97.3\u00b10.8\n79.7\u00b11.5\n93.3\u00b11.0\n85.8\u00b11.8\n72.8\u00b12.0\n69.9\u00b11.4\n72.0\u00b10.3\n69.2\u00b14.4\n78.3\nGATSep\n66.1\u00b12.6\n96.3\u00b11.2\n97.0\u00b10.5\n84.3\u00b10.8\n93.5\u00b12.0\n86.0\u00b11.4\n79.6\u00b11.1\n69.4\u00b11.9\n76.0\u00b10.2\n74.5\u00b13.7\n82.3\nBernNet\n66.8\u00b13.7\n94.9\u00b11.5\n96.2\u00b11.4\n83.0\u00b10.6\n92.8\u00b11.7\n83.1\u00b11.4\n76.9\u00b10.6\n68.8\u00b12.9\n73.2\u00b10.1\n66.8\u00b15.8\n80.2\nAMNet\n68.4\u00b12.4\n95.3\u00b11.7\n97.0\u00b11.6\n82.6\u00b10.5\n93.7\u00b10.7\n77.3\u00b12.5\n76.8\u00b10.7\n68.1\u00b12.9\n73.1\u00b10.1\n53.6\u00b13.3\n78.6\nBWGNN\n65.4\u00b14.3\n97.3\u00b10.9\n98.0\u00b10.7\n84.9\u00b10.7\n96.1\u00b10.5\n85.2\u00b11.1\n80.4\u00b10.9\n71.8\u00b12.2\n76.3\u00b10.1\n92.0\u00b15.2\n84.7\nGHRN\n66.0\u00b14.5\n96.7\u00b11.1\n98.1\u00b10.3\n85.3\u00b10.6\n96.0\u00b10.8\n85.4\u00b11.9\n80.4\u00b10.8\n71.8\u00b11.9\n76.1\u00b10.1\n79.0\u00b12.4\n83.5\nRF-Graph\n62.8\u00b12.4\n99.5\u00b10.1\n97.5\u00b10.3\n93.3\u00b10.2\n96.7\u00b10.5\n91.9\u00b10.5\n81.2\u00b10.7\n65.8\u00b11.2\n65.6\u00b10.3\n99.2\u00b10.0\n85.4\nXGB-Graph\n62.8\u00b13.2\n99.5\u00b10.1\n98.7\u00b10.0\n96.2\u00b10.2\n96.8\u00b10.4\n93.4\u00b10.0\n80.1\u00b10.5\n65.7\u00b10.8\n74.6\u00b10.0\n99.3\u00b10.0\n86.7\nRec@K\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n7.5\u00b12.9\n79.2\u00b12.1\n84.4\u00b11.8\n46.9\u00b11.3\n67.8\u00b12.6\n23.9\u00b111.6\n39.4\u00b11.1\n19.3\u00b11.7\n4.2\u00b10.2\n20.1\u00b110.9\n39.3\nKNN\n9.3\u00b11.6\n70.3\u00b11.6\n78.8\u00b10.0\n29.4\u00b10.0\n68.1\u00b11.8\n44.5\u00b10.0\n35.3\u00b11.4\n17.4\u00b11.6\n1.8\u00b10.0\n34.9\u00b10.2\n39.0\nSVM\n8.0\u00b12.6\n68.5\u00b13.0\n75.1\u00b17.8\n41.7\u00b10.0\n60.6\u00b113.1\n15.2\u00b10.7\n37.0\u00b11.3\n19.4\u00b11.8\n4.2\u00b10.2\n2.1\u00b11.1\n33.2\nRF\n5.9\u00b11.3\n82.3\u00b12.1\n85.7\u00b10.4\n69.4\u00b10.5\n74.8\u00b11.4\n72.5\u00b10.3\n39.5\u00b11.0\n17.9\u00b11.5\n4.2\u00b10.0\n45.3\u00b10.1\n49.8\nXGBoost\n7.5\u00b11.4\n87.0\u00b11.1\n86.4\u00b10.0\n74.8\u00b10.0\n72.9\u00b11.5\n72.0\u00b10.0\n40.1\u00b11.5\n17.9\u00b11.1\n4.1\u00b10.0\n20.0\u00b10.2\n48.3\nXGBOD\n10.9\u00b13.9\n88.0\u00b10.5\n83.8\u00b12.0\n61.0\u00b10.4\n74.6\u00b10.8\n69.7\u00b10.0\n42.2\u00b11.5\n18.5\u00b11.3\n1.7\u00b10.0\n14.0\u00b10.0\n46.4\nNA\n6.9\u00b11.4\n86.6\u00b11.4\n87.5\u00b10.0\n73.1\u00b10.0\n73.2\u00b11.5\n72.6\u00b10.0\n40.3\u00b11.2\n18.8\u00b11.3\n4.4\u00b10.0\n20.3\u00b10.2\n48.4\nGCN\n10.0\u00b13.2\n90.3\u00b10.8\n36.9\u00b11.4\n23.7\u00b10.6\n70.0\u00b15.2\n25.0\u00b16.0\n43.1\u00b12.0\n16.5\u00b12.0\n7.1\u00b10.2\n20.1\u00b110.9\n34.3\nSGC\n7.8\u00b12.9\n88.2\u00b11.4\n35.2\u00b14.3\n19.7\u00b12.6\n41.1\u00b123.3\n11.0\u00b11.7\n35.0\u00b13.4\n16.1\u00b11.2\n4.2\u00b10.2\n4.1\u00b12.7\n26.2\nGIN\n6.8\u00b12.4\n87.5\u00b11.1\n73.0\u00b13.5\n38.4\u00b11.2\n65.5\u00b14.8\n27.3\u00b18.3\n39.0\u00b11.9\n17.5\u00b11.7\n5.9\u00b10.2\n14.4\u00b13.8\n37.5\nGraphSAGE\n7.2\u00b11.9\n83.8\u00b12.5\n67.9\u00b18.6\n51.3\u00b12.1\n46.0\u00b115.9\n37.3\u00b14.6\n46.5\u00b11.6\n21.2\u00b11.6\n7.0\u00b10.4\n15.6\u00b14.4\n38.4\nGAT\n6.9\u00b12.6\n87.8\u00b11.4\n83.1\u00b11.3\n43.6\u00b12.7\n64.6\u00b15.5\n27.9\u00b111.3\n46.6\u00b11.8\n19.8\u00b11.6\n7.4\u00b10.2\n11.6\u00b13.0\n39.9\nGT\n7.7\u00b12.8\n83.9\u00b11.9\n77.8\u00b12.8\n51.1\u00b13.2\n39.8\u00b111.9\n26.3\u00b111.1\n47.4\u00b11.8\n19.7\u00b11.7\n7.5\u00b10.2\n13.2\u00b14.1\n37.4\nPNA\n6.1\u00b12.2\n89.3\u00b11.1\n48.8\u00b16.8\n31.4\u00b10.6\n36.5\u00b115.8\n33.5\u00b15.8\n42.4\u00b11.2\n17.0\u00b11.8\n6.6\u00b10.3\n36.5\u00b18.3\n34.8\nBGNN\n10.7\u00b11.6\n91.0\u00b10.9\n63.9\u00b12.8\n27.5\u00b11.4\n77.3\u00b10.8\n63.8\u00b11.3\n43.7\u00b11.2\n8.6\u00b11.6\n6.2\u00b10.4\n95.4\u00b10.3\n48.8\nGAS\n9.9\u00b14.3\n86.6\u00b11.5\n49.5\u00b16.4\n24.1\u00b10.6\n72.9\u00b11.7\n34.6\u00b19.6\n45.0\u00b11.8\n18.2\u00b12.3\n6.8\u00b10.2\n10.4\u00b14.3\n35.8\nDCI\n7.3\u00b12.1\n85.5\u00b12.5\n76.7\u00b14.0\n40.3\u00b17.1\n64.0\u00b15.3\n29.9\u00b16.2\n40.2\u00b12.0\n18.5\u00b11.6\n6.7\u00b10.3\n20.6\u00b17.2\n39.0\nPCGNN\n5.0\u00b12.4\n75.9\u00b12.4\n83.2\u00b12.5\n44.4\u00b12.3\n69.8\u00b15.3\n40.4\u00b112.0\n37.6\u00b12.0\n18.5\u00b11.9\n5.0\u00b10.2\n14.5\u00b14.9\n39.4\nGATSep\n7.6\u00b12.9\n87.9\u00b12.8\n84.0\u00b10.8\n51.1\u00b11.4\n70.3\u00b19.0\n31.3\u00b18.8\n46.6\u00b12.0\n20.0\u00b11.8\n7.5\u00b10.3\n14.9\u00b12.9\n42.1\nBernNet\n10.5\u00b12.4\n85.0\u00b12.3\n83.2\u00b12.9\n48.7\u00b10.9\n71.9\u00b15.5\n23.9\u00b17.7\n43.5\u00b11.8\n19.3\u00b12.1\n4.6\u00b10.1\n6.0\u00b11.6\n39.7\nAMNet\n10.3\u00b12.9\n85.3\u00b12.6\n83.4\u00b11.6\n47.7\u00b11.4\n73.1\u00b13.4\n8.8\u00b16.3\n42.8\u00b11.5\n19.9\u00b12.1\n4.2\u00b10.1\n1.6\u00b10.5\n37.7\nBWGNN\n10.7\u00b12.8\n87.4\u00b11.7\n85.0\u00b12.7\n52.2\u00b11.3\n81.1\u00b10.7\n31.7\u00b16.2\n48.7\u00b12.1\n21.4\u00b12.0\n7.5\u00b10.3\n54.3\u00b112.4\n48.0\nGHRN\n11.6\u00b13.2\n86.7\u00b11.8\n85.9\u00b11.6\n53.0\u00b11.0\n81.1\u00b11.4\n33.3\u00b110.3\n49.1\u00b11.5\n21.3\u00b12.4\n7.5\u00b10.2\n24.6\u00b17.0\n45.4\nRF-Graph\n5.8\u00b11.4\n92.2\u00b10.7\n84.4\u00b10.5\n69.6\u00b10.6\n84.0\u00b11.1\n72.5\u00b10.3\n51.3\u00b11.6\n18.4\u00b12.4\n3.2\u00b10.1\n87.6\u00b10.1\n56.9\nXGB-Graph\n7.6\u00b11.1\n93.1\u00b11.1\n85.9\u00b10.0\n78.8\u00b10.6\n84.6\u00b11.1\n71.2\u00b10.0\n48.8\u00b11.3\n16.5\u00b11.3\n6.7\u00b10.0\n85.2\u00b10.1\n57.8\n25\nTable 13: Comparison of AUROC (top) and Rec@K (bottom) of each model with optimal hyperpa-\nrameters through random search. The Results for AUPRC is in Table 4. Ave. signifies the average\nscore across the first 9 datasets without T-Social. OOT (out-of-time) means the model could not\ncomplete random search within a day. Best results are highlighted in bold.\nAUROC\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n60.71\n92.89\n96.94\n81.64\n92.24\n88.32\n73.78\n60.15\n72.34\n73.06\n79.89\nKNN\n62.53\n93.57\n94.91\n84.60\n92.69\n88.03\n72.09\n64.00\n58.50\n77.64\n78.99\nSVM\n68.18\n92.52\n95.80\n77.21\n92.59\n85.02\n72.59\n64.77\n72.03\nOOT\n80.08\nRF\n60.79\n98.77\n97.53\n93.80\n94.85\n91.68\n74.14\n54.85\n70.37\n79.94\n81.87\nXGBoost\n64.59\n98.82\n98.34\n95.38\n95.42\n89.91\n74.84\n62.09\n72.43\n81.75\n83.53\nXGBOD\n68.39\n99.24\n98.20\n93.96\n95.39\n92.26\n74.55\n67.53\n64.88\nOOT\n83.82\nNA\n63.86\n99.03\n98.23\n94.70\n94.85\n91.27\n74.87\n66.20\n71.73\n81.95\n83.86\nGCN\n62.04\n98.84\n85.17\n58.62\n94.62\n81.69\n73.80\n68.20\n75.51\n96.63\n77.61\nSGC\n66.95\n98.58\n87.84\n57.66\n87.97\n79.52\n72.36\n68.11\n68.63\n83.67\n76.40\nGIN\n61.82\n98.65\n95.63\n73.77\n92.71\n83.01\n74.57\n68.08\n74.16\n94.09\n80.27\nGraphSAGE\n63.28\n97.75\n90.90\n82.90\n95.62\n87.59\n80.92\n73.04\n75.60\n95.73\n83.07\nGAT\n64.15\n98.54\n97.13\n79.14\n95.75\n86.28\n78.75\n68.30\n75.53\n90.33\n82.62\nGT\n67.39\n97.23\n93.37\n80.32\n94.39\n86.10\n78.94\n69.62\n75.71\n90.79\n82.56\nPNA\n63.45\n98.78\n82.14\n74.39\n92.47\n84.86\n75.17\n66.32\n73.39\n84.02\n79.00\nBGNN\n68.94\n98.59\n90.64\n63.28\n96.20\n87.69\n76.65\n68.02\n76.22\n99.94\n80.69\nKNNGCN\n60.31\n99.17\n90.69\n69.13\n94.69\n88.53\n74.23\n66.95\n75.72\n87.80\n79.94\nGAS\n60.57\n99.11\n92.69\n76.43\n96.45\n86.66\n78.19\n68.15\n75.98\n94.96\n81.58\nDCI\n66.87\n97.94\n95.39\n78.38\n87.85\n85.73\n73.86\n64.77\n73.94\n83.96\n80.53\nPCGNN\n65.41\n95.14\n98.01\n80.82\n94.03\n86.50\n76.63\n67.66\n72.76\n96.91\n81.88\nGATSep\n66.17\n98.22\n95.11\n80.49\n94.40\n85.94\n79.52\n70.08\n75.78\n87.69\n82.85\nBernNet\n69.83\n98.00\n95.79\n83.54\n96.67\n87.54\n77.51\n68.58\n73.58\n93.73\n83.45\nAMNet\n69.60\n98.32\n96.92\n81.90\n96.38\n83.66\n75.16\n66.05\n72.99\n92.50\n82.33\nBWGNN\n70.82\n98.13\n98.27\n87.13\n96.93\n87.03\n80.41\n70.87\n76.30\n96.88\n85.10\nGHRN\n61.02\n99.18\n98.29\n84.60\n96.46\n89.50\n80.08\n72.16\n76.13\n97.12\n84.16\nRFGraph\n65.35\n99.43\n96.73\n95.24\n97.28\n93.21\n81.88\n64.77\n67.78\n99.69\n84.63\nXGBGraph\n64.74\n99.29\n98.74\n97.37\n97.15\n91.78\n82.85\n71.02\n75.83\n99.76\n86.53\nRec@K\nReddit\nWeibo\nAmazon\nYelp.\nT-Fin.\nEllip.\nTolo.\nQuest.\nDGraph.\nT-Social\nAve.\nMLP\n9.52\n78.39\n83.15\n46.62\n68.93\n57.43\n39.88\n16.99\n4.04\n16.86\n44.99\nKNN\n10.20\n73.20\n79.35\n51.92\n70.04\n56.60\n37.85\n19.18\n1.98\n44.08\n44.48\nSVM\n8.84\n77.81\n82.07\n41.77\n72.40\n19.21\n39.72\n18.08\n4.04\nOOT\n40.44\nRF\n5.44\n84.15\n86.41\n70.23\n75.59\n72.76\n39.72\n17.81\n4.21\n45.48\n50.70\nXGBoost\n6.80\n87.03\n86.41\n75.08\n76.01\n72.58\n41.90\n16.71\n4.30\n21.86\n51.87\nXGBOD\n10.20\n87.32\n86.96\n71.62\n75.87\n69.07\n42.21\n16.71\n1.68\nOOT\n51.29\nNA\n8.84\n85.59\n87.50\n73.38\n75.31\n71.74\n42.21\n18.36\n4.04\n21.70\n51.89\nGCN\n6.12\n88.47\n44.02\n23.85\n74.90\n33.52\n39.41\n17.81\n7.05\n73.23\n37.24\nSGC\n8.84\n87.32\n46.20\n22.46\n67.41\n22.99\n39.72\n16.99\n3.87\n24.93\n35.09\nGIN\n10.88\n89.63\n80.98\n36.15\n73.37\n32.13\n39.41\n18.36\n6.32\n64.47\n43.03\nGraphSAGE\n7.48\n90.78\n78.26\n47.15\n78.09\n56.69\n48.75\n21.37\n6.84\n73.74\n48.38\nGAT\n10.88\n89.63\n82.61\n44.23\n79.75\n37.86\n44.24\n17.26\n7.14\n42.07\n45.95\nGT\n12.93\n85.88\n78.80\n44.62\n81.55\n30.75\n44.39\n20.27\n6.92\n43.58\n45.12\nPNA\n10.88\n39.67\n90.78\n30.00\n72.54\n36.47\n42.68\n13.15\n5.16\n26.95\n37.93\nBGNN\n9.52\n93.37\n64.67\n30.77\n77.25\n59.56\n45.33\n12.60\n7.70\n96.89\n44.53\nKNNGCN\n5.44\n89.63\n71.74\n33.23\n75.87\n41.55\n40.34\n18.08\n7.18\n48.67\n42.56\nGAS\n4.08\n91.93\n80.43\n38.00\n79.75\n37.49\n47.04\n18.90\n6.02\n64.58\n44.85\nDCI\n8.16\n89.05\n80.98\n40.46\n71.98\n35.27\n37.85\n17.26\n5.85\n18.27\n42.99\nPCGNN\n14.97\n84.15\n85.33\n43.77\n79.06\n43.77\n43.15\n18.08\n6.66\n73.53\n46.55\nGATSep\n9.52\n88.18\n80.98\n44.23\n81.55\n30.56\n47.20\n21.37\n7.31\n40.84\n45.66\nBernNet\n10.88\n89.34\n82.61\n49.69\n83.63\n49.77\n44.70\n19.73\n5.55\n48.23\n48.43\nAMNet\n11.56\n88.76\n83.15\n45.38\n84.05\n30.56\n42.52\n17.53\n4.21\n43.21\n45.31\nBWGNN\n11.56\n87.90\n85.87\n56.69\n84.19\n42.47\n50.31\n21.64\n7.57\n75.78\n49.80\nGHRN\n5.44\n89.34\n85.33\n51.85\n81.97\n50.51\n46.57\n22.19\n6.96\n82.33\n48.91\nRFGraph\n3.40\n91.93\n83.15\n75.31\n84.05\n72.58\n52.18\n15.89\n3.22\n93.58\n53.52\nXGBGraph\n6.12\n91.93\n85.87\n83.15\n85.02\n71.93\n53.43\n20.55\n6.96\n93.53\n56.11\n26\n",
    "2305.13573": "SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs\nSheng Tian1\u2217, Jihai Dong1\u2217, Jintang Li2 , Wenlong Zhao1 , Xiaolong Xu1 ,\nBaokun Wang1 , Bowen Song1 , Changhua Meng1 , Tianyi Zhang1 and Liang Chen2\n1Ant Group\n2Sun Yat-sen University\n{tiansheng.ts, dongjihai.djh}@antgroup.com, lijt55@mail2.sysu.edu.cn, {chicheng.zwl, yiyin.xxl,\nyike.wbk, bowen.sbw, changhua.mch, zty113091}@antgroup.com, chenliang6@mail.sysu.edu.cn\nAbstract\nAnomaly detection aims to distinguish abnormal\ninstances that deviate significantly from the ma-\njority of benign ones. As instances that appear in\nthe real world are naturally connected and can be\nrepresented with graphs, graph neural networks be-\ncome increasingly popular in tackling the anomaly\ndetection problem. Despite the promising results,\nresearch on anomaly detection has almost exclu-\nsively focused on static graphs while the mining of\nanomalous patterns from dynamic graphs is rarely\nstudied but has significant application value. In ad-\ndition, anomaly detection is typically tackled from\nsemi-supervised perspectives due to the lack of suf-\nficient labeled data. However, most proposed meth-\nods are limited to merely exploiting labeled data,\nleaving a large number of unlabeled samples unex-\nplored. In this work, we present semi-supervised\nanomaly detection (SAD), an end-to-end frame-\nwork for anomaly detection on dynamic graphs.\nBy a combination of a time-equipped memory bank\nand a pseudo-label contrastive learning module,\nSAD is able to fully exploit the potential of large\nunlabeled samples and uncover underlying anoma-\nlies on evolving graph streams. Extensive experi-\nments on four real-world datasets demonstrate that\nSAD efficiently discovers anomalies from dynamic\ngraphs and outperforms existing advanced methods\neven when provided with only little labeled data.\n1\nIntroduction\nAnomaly detection, which identifies outliers (called anoma-\nlies) that deviate significantly from normal instances, has\nbeen a lasting yet active research area in various research\ncontexts [Ma et al., 2021]. In many real-world scenarios, in-\nstances are often explicitly connected with each other and can\nbe naturally represented with graphs. Over the past few years,\nanomaly detection based on graph representation learning has\nemerged as a critical direction and demonstrated its power in\ndetecting anomalies from instances with abundant relational\ninformation. For example, in a financial transaction network\n\u2217Both authors contribute equally.\n(a) Interaction frequency\n(b) Time-frequency distribution \nof anomalous\nDynamic graph\nday1\n18h\nday4\n13h\nday2\n1h\nday2\n19h\nday2\n20h\nday6\n22h\nday1\n11h\nday5\n9h\nday9\n6h\nday4\n21h\nday3\n15h\nday13\n23h\nday2\n7h\nday2\n7h\nday1\n21h\nday8\n17h\nday3\n18h\nday7\n19h\nday3\n16h\nday2\n19h\nday5\n17h\nFigure 1: Statistical distribution of evolving graph streams on the\nMOOC dataset. Observations: (a) The frequency of interactions\nshows a clear cyclical pattern on the time axis.\n(b) Anomalous\nsamples typically occurred between 8 am and 10 pm, with peaks\nat around 8 pm.\nwhere fraudsters would try to cheat normal users and make\nillegal money transfers, the graph patterns captured from the\nlocal and global perspectives can help detect these fraudsters\nand prevent financial fraud.\nIn real-world scenarios, it is often challenging to tackle the\nanomaly detection problem with supervised learning methods\nsince annotated labels in most situations are rare and difficult\nto acquire. Therefore, current works mainly focus on unsu-\npervised schemes to perform anomaly detection, with promi-\nnent examples including autoencoders [Zhou and Paffenroth,\n2017], adversarial networks [Chen et al., 2020], or matrix fac-\ntorization [Bandyopadhyay et al., 2019]. However, they tend\nto produce noisy results or uninterested noise instances due\nto insufficient supervision. Empirically, it is usually difficult\nto get desired outputs from unsupervised methods without the\nguidance of precious labels or correct assumptions.\nSemi-supervised learning offers a principled way to re-\nsolve the issues mentioned above by utilizing few-shot la-\nbeled data combined with abundant unlabeled data to de-\ntect underlying anomalies [Ma et al., 2021].\nSuch meth-\nods are practical in real-world applications and can defi-\nnitely obtain much better performance than fully unsuper-\nvised methods when the labeled data is leveraged properly.\nIn recent years, a vast majority of work adopts graph-based\nsemi-supervised learning models for solving anomaly detec-\ntion problems, resulting in beneficial advances in anomaly\nanalytics techniques [Pang et al., 2019; Ding et al., 2021;\nQian et al., 2021].\nYet, there are still two fundamental limitations to these ex-\nisting semi-supervised anomaly detection methods: (1) Un-\nderutilization of unlabeled data.\nAlthough existing semi-\nsupervised approaches have utilized labeled data to perform\narXiv:2305.13573v1  [cs.LG]  23 May 2023\nanomaly detection, few methods take advantage of large-\nscale unlabeled samples that contain a lot of useful infor-\nmation. As a result, they suffer poor performance particu-\nlarly when limited labels are provided. (2) Lack of dynamic\ninformation mining. In practice, graphs are dynamic in na-\nture, with nodes, edges, and attributes evolving constantly\nover time. Such characteristics may be beneficial in solv-\ning anomaly detection problems. As shown in Figure 1, the\nbehavioral statistics of users show certain cyclical patterns\nin the time domain, which can be directly captured by dy-\nnamic graph neural networks. To our best knowledge, most\nprevious attempts mainly focus on static graphs which ne-\nglect temporal information, such design has proven to be\nsub-optimal in dynamic graph scenario [Liu et al., 2021].\nAlthough there are some recent efforts [Meng et al., 2021;\nLiu et al., 2021]have explored ways to combine dynamic in-\nformation in their work, they simply incorporate time as a fea-\nture in the model, leading to insufficient temporal information\nmodeling and degraded performance on anomaly detection.\nIn this work, we propose a novel semi-supervised anomaly\ndetection (SAD) framework that tackles the aforementioned\nlimitations. The proposed framework first utilizes a temporal\ngraph network to encode graph data for both labeled and un-\nlabeled samples, followed by an anomaly detector network\nto predict node anomaly scores. Then, a memory bank is\nadopted to record each predicted anomaly score together with\nnode time information, to produce a normal sample statisti-\ncal distribution as prior knowledge and guide the learning of\nthe network subsequently by using a deviation loss. In or-\nder to further exploit the potential of large unlabeled data,\nwe introduce a novel pseudo-label contrastive learning mod-\nule, which leverages the predicted anomaly scores to form\npseudo-groups by calculating score distance between nodes\n- nodes with closer distance will be classified into the same\npseudo-group, nodes within the same pseudo-group will form\npositive pairs for contrastive learning.\nOur main contributions are summarized as follows:\n\u2022 We propose SAD, an end-to-end semi-supervised\nanomaly detection framework, which is tailored with a\ntime-equipped memory bank and a pseudo-label con-\ntrastive learning module to effectively solve the anomaly\ndetection problem on dynamic graphs.\n\u2022 Our proposed SAD framework uses the statistical distri-\nbution of unlabeled samples as the reference distribution\nfor loss calculation and generates pseudo-labels corre-\nspondingly to participate in supervised learning, which\nin turn fully exploits the potential of unlabeled samples.\n\u2022 Extensive experiments demonstrate the superiority of\nthe proposed framework compared to strong baselines.\nMore importantly, our proposed approach can also sig-\nnificantly alleviate the label scarcity issue in practical\napplications.\n2\nRelated Work\n2.1\nDynamic Graph Neural Networks\nGraph neural networks (GNNs) have been prevalently used to\nrepresent structural information from graph data. However,\nnaive applications of static graph learning paradigms [Hamil-\nton et al., 2017; Velickovic et al., 2018] in dynamic graphs\nfail to capture temporal information and show up as subop-\ntimal in dynamic graph scenarios [Xu et al., 2020a]. Thus,\nsome methods [Singer et al., 2019; Pareja et al., 2020] model\nthe evolutionary process of a dynamic graph by discretizing\ntime and reproducing multiple static graph snapshots. Such a\nparadigm is often infeasible for real-world systems that need\nto handle each interaction event instantaneously, in which\ngraphs are represented by a continuous sequence of events,\ni.e., nodes and edges can appear and disappear at any time.\nSome recent work has begun to consider the direct mod-\neling of temporal information and proposed a continu-\nous time dynamic graph modeling scheme.\nFor example,\nDyREP [Trivedi et al., 2019] uses a temporal point process\nmodel, which is parameterized by a recurrent architecture to\nlearn evolving entity representations on temporal knowledge\ngraphs. TGAT [Xu et al., 2020a] proposes a novel functional\ntime encoding technique with a self-attention layer to aggre-\ngate temporal-topological neighborhood features. However,\nthe above methods rely heavily on annotated labels, yet ob-\ntaining sufficient annotated labels is usually very expensive,\nwhich greatly limits their application in real-world scenarios\nsuch as anomaly detection.\n2.2\nAnomaly Detection with GNNs\nAnomalies are rare observations (e.g., data records or events)\nthat deviate significantly from the others in the sample, and\ngraph anomaly detection (GAD) aims to identify anomalous\ngraph objects (i.e., nodes, edges, or sub-graphs) in a single\ngraph as well as anomalous graphs among a set/database of\ngraphs [Ma et al., 2021]. The anomaly detection with GNNs\nis mainly divided into unsupervised and semi-supervised\nmethods. Unsupervised anomaly detection methods are usu-\nally constructed with a two-stage task, where a graph repre-\nsentation learning task is first constructed, and then abnor-\nmality measurements are applied based on the latent repre-\nsentations [Akoglu et al., 2015].\nRader [Li et al., 2017]\ndetects anomalies by characterizing the residuals of the at-\ntribute information and their correlation with the graph struc-\nture. Autoencoder [Zhou and Paffenroth, 2017] learns graph\nrepresentations through an unsupervised feature-based deep\nautoencoder model. DOMINANT [Ding et al., 2019] uses a\nGCN-based autoencoder framework that uses reconstruction\nerrors from the network structure and node representation to\ndistinguish anomalies. CoLA [Liu et al., 2022] proposes a\nself-supervised contrastive learning method based on captur-\ning the relationship between each node and its neighboring\nsubstructure to measure the agreement of each instance pair.\nA more recent line of work proposes employing semi-\nsupervised learning by using small amounts of labeled sam-\nples from the relevant downstream tasks to address the prob-\nlem that unsupervised learning tends to produce noisy in-\nstances. SemiGNN [Wang et al., 2019] learn a multi-view\nsemi-supervised graph with hierarchical attention for fraud\ndetection. GDN [Ding et al., 2021] adopts a deviation loss\nto train GNN and uses a cross-network meta-learning algo-\nrithm for few-shot node anomaly detection. SemiADC [Meng\net al., 2021] simultaneously explores the time-series fea-\nture similarities and structure-based temporal correlations.\nTADDY [Liu et al., 2021] constructs a node encoding to\ncover spatial and temporal knowledge and leverages a sole\ntransformer model to capture the coupled spatial-temporal in-\nformation. However, these methods are based on discrete-\ntime snapshots of dynamical graphs that are not well suited\nto continuous time dynamical graphs, and the temporal infor-\nmation is simply added to the model using a linear network\nmapping as features, without taking into account temporal\nproperties such as periodicity.\n3\nPRELIMINARIES\n3.1\nNotations and Problem Formulation\nNotations. Continuous-time dynamic graphs are important\nfor modeling relational data for many real-world complex ap-\nplications. The dynamic graph can be represented as G =\n(V, E), where V = vi is the set of nodes involved in all tempo-\nral events, and E denotes a sequence of temporal events. Typ-\nically, let E = {\u03b4(t1), \u03b4(t2), ..., \u03b4(tm)} be an event stream\nthat generates the temporal network and m is the number of\nobserved events, with an event \u03b4(t) = (vi, vj, t, xij) indicat-\ning an interaction or a link happens from source node vi to\ntarget node vj at time t, with associating edge feature xij.\nNote that there might be multiple links between the same pair\nof node identities at different timestamps.\nProblem formulation. In practical applications, it is very\ndifficult to obtain large-scale labeled data due to the pro-\nhibitive cost of collecting such data. The goal of our work is\nto be able to improve the performance of anomaly detection\nbased on very limited supervised knowledge. Taking the task\nof temporal event anomaly detection as an example, based on\na dynamic graph G, we assume that each event has a corre-\nsponding true label y \u2208Yt. However, due to the limitation\nof objective conditions, only a few samples of the label can\nbe observed, denoted by YL, and the rest are unlabeled data\ndenoted by YU. In our problem |YL| \u226a|YU|, our task is\nto learn an anomaly score mapping function \u03d5 that separates\nabnormal and normal samples.\n4\nMethod\nIn this section, we detail the components of the proposed\nframework SAD for anomaly detection on dynamic graphs.\nAs shown in Figure 2, the overall structure of SAD consists\nof the following main components: a temporal graph encoder\nthat produces node embeddings (Section 4.1), the combina-\ntion of an anomaly detector and a time-equipped memory\nbank to distinguish anomalous samples using prior knowl-\nedge of unlabeled data (Section 4.1), and a supervised con-\ntrastive learning module can yield pseudo-labeling to mine\nthe potential of unlabeled data (Section 4.2). We also detail\nseveral strategies for parameter learning under our framework\n(Section 4.3).\n4.1\nDeviation Networks with Memory Bank\nTo enable anomaly detection with few-shot labeled data, we\nconstruct a graph deviation network following [Ding et al.,\n2021]. However, for application to dynamic graphs, we first\nuse a temporal graph encoder based on temporal encoding in-\nformation to learn node representations and additionally con-\nstruct a memory bank to dynamically record the overall sta-\ntistical distribution of normal and unlabeled samples as a ref-\nerence score to calculate the deviation loss that optimizes the\nmodel with few-shot labeled samples. The entire deviation\nnetwork is semi-supervised, with abundant unlabeled samples\nused as statistical priors in the memory bank and few-shot la-\nbeled data used for calculating deviation losses.\nTemporal graph encoder. Since dynamic graphs contain in-\nformation about the time differences of edges, we first deploy\na temporal graph encoder as a node embedding module to\nlearn the topology of the graph by mapping nodes to a low-\ndimensional latent representation. The graph encoder is mul-\ntiple GNN layers that take the graph G constructed at time t\nas input and extract node representations zi(t) for all vi in G\nthrough the message-passing mechanism. Formally, the for-\nward propagation at k-th layer is described as follows:\nh(k)\nNi (t) = AGG(k) \u0010n\n(h(k\u22121)\nj\n(t), xij, \u03d5(\u2206t)) : vj \u2208N(vi, t)\no\u0011\n,\nh(k)\ni\n(t) = COMBINE(k) \u0010\nh(k\u22121)\ni\n(t), h(k)\nNi (t)\n\u0011\n,\n(1)\nwhere h(k)\ni\n(t) is the intermediate representations of node vi\nin the k-th layer at the timestamp t and vj \u2208N(vi, t) denotes\nthe set of first-order neighboring nodes of the node vi oc-\ncurred prior to t. xij is the associating edge feature between\nvi and vj, \u2206t = t\u2212tij represents the relative timespan of two\ntimestamps, and \u03d5(\u00b7) is a relative time encoder based on the\ncosine transform mentioned in [Xu et al., 2020a], which im-\nplements a functional time projection from the time domain\nto the continuous space and helps to capture the periodic pat-\nterns on the dynamic graph. AGG(\u00b7) is an aggregation func-\ntion for propagating and aggregating messages from neigh-\nborhoods, followed by a COMBINE(\u00b7) function for combin-\ning representations from neighbors and its previous-layer rep-\nresentation. Note that the AGG(\u00b7) and COMBINE(\u00b7) func-\ntions in our framework can be flexibly designed for different\napplications without any constraints. Finally, we can obtain\nthe node embedding zi(t) = h(K)\ni\n(t) of node vi with the K-\nth aggregation layer. In practice, across all tasks, we adopt\nTGAT as the temporal graph encoder.\nAnomaly Detector. To distinguish abnormal samples from\nnormal samples, we introduce an anomaly detector to map\nthe learned node embedding zi(t) to an anomaly score space,\nwhere normal samples are clustered and abnormal samples\ndeviate from the set.\nSpecifically, we use a simple feed-\nforward neural network f\u03b8a(\u00b7) as the anomaly detector:\nsi(t) = f\u03b8a(zi(t)) = W2\u00b7ReLU(W1\u00b7zi(t)+b1)+b2, (2)\nwhere W1, b1, W2 and b2 are learnable parameters of\nthe anomaly detector.\nIn our framework, si(t) is a one-\ndimensional anomaly score with a value range from \u2212\u221eto\n+\u221e. Ideally, the anomaly scores of normal samples are clus-\ntered in a certain interval, and it is easy to find out the anoma-\nlies (outliers) based on the overall distribution of the anomaly\nscores.\nMemory bank. In essence, the task of SAD is to discriminate\nanomalous nodes from large-scale normal/unlabeled nodes\nSampled memory\nDeviation Loss\nUpdate\n-1\n1\n...\nAnomaly Score\nPseudo-Label\nSupervised \nContrastive Learning\nMemory Bank\nEncoder\nInput Graph\nMemory bank\nenqueue\ndrop\ndequeue\nAnomalies\nMemory Bank Update Scheme\nOutdated Samples\n.\n.\n.\nNode Embedding\ngroup 2\ngroup 3\ngroup 1\nNormal Node\nUnlabeled Node\nAnomaly\nAnomaly\nDetector\n...\n0\nFigure 2: The proposed semi-supervised anomaly detection framework. SAD consists of four main components: the temporal graph encoder,\nthe anomaly detector, the time-equipped memory bank, and the supervised contrastive learning module.\nin the dynamic graph utilizing a few labeled samples. The\nanomaly scores of normal nodes are clustered together in the\nanomaly score space after training with the optimization ob-\njective (which will be introduced later), while the anomaly\nscores of the anomaly samples will deviate from the overall\ndistribution. Hence, we generate this distribution by using a\nmemory bank mem to record the historical anomaly scores of\nunlabeled and normal samples, based on the assumption that\nmost of the unlabeled data are normal samples. The messages\nm that need to be stored in the memory bank are described as\nthe following:\nm = si(t), if yi(t) = 0 or \u22121\n(3)\nwhere yi(t) is the label information of node vi at timestamp t\nand its value \u22121 and 0 represent that the node is an unlabeled\nor normal sample at the current timestamp, respectively.\nNote that the attribute of the nodes in the dynamic graph\nevolves continuously, with consequent changes in the labels\nand anomaly scores. Perceptually, samples with longer time\nintervals should have less influence on the current statistical\ndistribution, so we also record the corresponding time t for\neach anomaly score in the memory bank.\nm = (si(t), t), if yi(t) = 0 or \u22121\n(4)\nMeanwhile, the memory bank mem is designed as a first-\nin-first-out (FIFO) queue of size M. The queue size is con-\ntrolled by discarding samples that are sufficiently old since\noutdated samples yield less gain due to changes in model pa-\nrameters during the model training process.\nDeviation loss. Inspired by the recent success of deviation\nnetworks [Pang et al., 2019; Ding et al., 2021], we adopt devi-\nation loss as our main learning objective to enforce the model\nto separate out normal nodes and abnormal nodes whose\ncharacteristics significantly deviate from normal nodes in the\nanomaly score space. Different from their methods, we use\na memory bank to produce a statistical distribution of normal\nsamples instead of using a standard normal distribution as the\nreference score. This takes into account two main factors,\none is that statistical distribution generated by real anomaly\nscore can better show the properties of different datasets, and\nthe other is that in dynamic graphs, the data distribution can\nfluctuate more significantly due to realistic situations such as\nimportant holidays or periodic events, so it is not suitable to\nuse a simple standard normal distribution as the reference dis-\ntribution of normal samples.\nTo obtain the reference score in the memory bank, we as-\nsume that the distribution of the anomaly score satisfies the\nGaussian distribution, which is a very robust choice to fit\nthe anomaly scores of various datasets [Kriegel et al., 2011].\nBased on this assumption, we randomly sample a set of Ms\nexamples from the memory bank for calculating the reference\nscore, which helps to improve the robustness of the model\nby introducing random perturbations. Meanwhile, we intro-\nduce the effect of time decay and use the relative timespan\nbetween the timestamp t and the anomaly score storage time\nti to calculate the weighting term for each statistical exam-\nple wi(t) =\n1\nln(t\u2212ti+1)+1. The reference scores (mean \u00b5r(t)\nand standard deviation \u03c3r(t)) of the normal sample statistical\ndistribution at the timestamp t are calculated as follows:\n\u00b5r(t) = 1\nk\nk\nX\ni=1\nwi(t) \u00b7 ri\n\u03c3r(t) =\nsPk\ni=1 wi(t) \u00b7 (ri \u2212\u00b5r(t))2\nk \u22121\n(5)\nWith the reference scores at the timestamp t, we define the\ndeviation dev(vi, t) between the anomaly score of node vi\nand the reference score in the following:\ndev(vi, t) = si(t) \u2212\u00b5r(t)\n\u03c3r(t)\n.\n(6)\nThe final deviation loss is then obtained by plugging devi-\nation dev(vi, t) into the contrast loss [Hadsell et al., 2006] as\nfollows:\nLdev = (1\u2212yi(t))\u00b7|dev(vi, t)|+yi(t)\u00b7max(0, m\u2212|dev(vi, t)|),\n(7)\nwhere yi(t) is the label information of node vi at timestamp t\nand m is equivalent to a Z-Score confidence interval parame-\nter. Note that in this semi-supervised task, only labeled sam-\nples are directly involved in computing the objective function.\nOptimizing this loss will enable the anomaly score of normal\nsamples as close as possible to the overall distribution in the\nmemory bank, while the anomaly scores of anomalies are far\nfrom the reference distribution.\n4.2\nContrastive Learning for Unlabeled Samples\nTo fully exploit the potential of unlabeled samples, we gener-\nate a pseudo-label \u02c6yi(t) for each sample based on the existing\ndeviation scores dev(vi, t) and involve them in the training\nof the graph encoder network. Here, we design a supervised\ncontrastive learning task such that nodes with closer devia-\ntion scores also have more similar node representations, while\nnodes with larger deviation score differences have larger dif-\nferences in their corresponding representations. The goal of\nthis task is consistent with our deviation loss, which makes\nthe difference between normal and anomaly samples deviate\nin both the representation space and anomaly score space. For\na batch training sample with batch size N, we first obtain the\ndeviation distance \u25b3dij = |dev(vi, ti)\u2212dev(vj, tj)| between\nany two samples vi and vj. On the standard normal distribu-\ntion anomaly space, we group nodes with intervals less than\n1\u03c3 into the same class. Thus, for a single sample vi, its su-\npervised contrastive learning loss takes the following form:\nLscl\ni\n=\n\u22121\nN \u22121\nN\nX\nj,j\u0338=i\n1\u25b3dij<1 \u00b7\n1\n1 + \u25b3dij\n\u00b7\nln\nexp(zi(ti) \u00b7 zj(tj)/\u03c4)\nPN\nk,i\u0338=k exp(zi(ti) \u00b7 zk(tk)/\u03c4)\n(8)\nwhere \u03c4 is a scalar temperature parameter, and\n1\n1+\u25b3dij de-\nnotes similarity weights between positive sample pairs. The\nfinal objective function is the sum of the losses of all nodes:\nLscl = PN Lscl\ni .\n4.3\nLearning procedure\nFor anomaly detection tasks, the optimization of the whole\nnetwork is based on deviation loss and contrastive learning\nloss. Noted that the contrastive learning loss is only used to\ntrain the parameters of the temporal graph encoder. Specifi-\ncally, let \u03b8enc denote all the parameters of the temporal graph\nencoder, and \u03b8ano denote the learnable parameters in the\nWikipedia\nReddit\nMooc\nAlipay\n#Nodes\n9,227\n10,984\n7,074\n3,575,301\n#Edges\n157,474\n672,447\n411,749\n53,789,768\n#Edge features\n172\n172\n4\n100\n#Anomalies\n217\n366\n4,066\n24,979\nTimespan\n30 days\n30 days\n30 days\n90 days\nPos. label meaning\nposting banned\nediting banned\ndropping out\nfraudster\nChronological Split\n70%-15%-15%\n70%-15%-15%\n70%-15%-15%\n70%-15%-15%\nTable 1: Statistics of datasets.\nanomaly detector. The optimization task of our model is as\nfollows:\narg min\n\u03b8enc,\u03b8ano\nLdev(\u03b8enc, \u03b8ano) + \u03b1Lscl(\u03b8enc),\n(9)\nwhere \u03b1 is a hyperparameter for balancing the contribution of\ncontrastive learning loss.\nIn addition to anomaly detection tasks, we also present an\nend-to-end learning procedure to cope with different down-\nstream tasks. Taking the downstream task of node classifi-\ncation as an example, in order to obtain the score results for\nnode classification, we use a simple multi-layer perception\n(MLP) as the projection network to learn the classification\nresults based on the node representations after the graph en-\ncoder. Let \u03b8pro denote the learnable parameters in the projec-\ntion network. Assume that the downstream supervised task\nwith loss function Lsup(\u03b8pro), the optimization task of our\nmodel is as follows:\narg min\n\u03b8enc,\u03b8ano,\u03b8pro\nLsup(\u03b8pro)+\u03b1Ldev(\u03b8enc, \u03b8ano)+\u03b2Lscl(\u03b8enc),\n(10)\nwhere \u03b1 and \u03b2 are hyperparameters for balancing the contri-\nbutions of two losses.\n5\nExperiments\n5.1\nExperimental Setup\nDatasets. In this paper, we use four real-world datasets, in-\ncluding three public bipartite interaction dynamic graphs and\nan industrial dataset. Wikipedia [Kumar et al., 2019] is a\ndynamic network tracking user edits on wiki pages, where\nan interaction event represents the page edited by the user.\nDynamic labels indicate whether a user is banned from post-\ning. Reddit [Kumar et al., 2019] is a dynamic network track-\ning active users posting in subreddits, where an interaction\nevent represents a user posting on a subreddit. The dynamic\nbinary labels indicate if a user is banned from posting un-\nder a subreddit. MOOC [Kumar et al., 2019] is a dynamic\nnetwork tracking students\u2019 actions on MOOC online course\nplatforms, where an interaction event represents user actions\non the course activity. The Alipay dataset is a dynamic fi-\nnancial transaction network collected from the Alipay plat-\nform, and dynamic labels indicate whether a user is a fraud-\nster. Note that the Alipay dataset has undergone a series of\ndata-possessing operations, so it cannot represent real busi-\nness information. For all tasks and datasets, we adopt the\nsame chronological split with 70% for training and 15% for\nvalidation and testing according to node interaction times-\ntamps. The statistics are summarized in Table 1.\nImplementation details.\nRegarding the proposed SAD\nmodel, we use a two-layer, two-head TGAT [Xu et al., 2020a]\nMethods\nWikipedia\nReddit\nMooc\nAlipay\nTGAT\n83.23 \u00b1 0.84\n67.06 \u00b1 0.69\n66.88 \u00b1 0.68\n92.53 \u00b1 0.93\nTGN\n84.67 \u00b1 0.36\n62.66 \u00b1 0.85\n67.07 \u00b1 0.73\n92.84 \u00b1 0.81\nRadar\n82.91 \u00b1 0.97\n61.46 \u00b1 1.27\n62.14 \u00b10.89\n88.18 \u00b1 1.05\nDOMINANT\n85.84 \u00b1 0.63\n64.66 \u00b1 1.29\n65.41\u00b1 0.72\n91.57 \u00b1 0.93\nSemiGNN\n84.65 \u00b1 0.82\n64.18 \u00b1 0.78\n64.98 \u00b10.63\n92.29 \u00b1 0.85\nGDN\n85.12 \u00b1 0.69\n67.02 \u00b1 0.51\n66.21 \u00b1 0.74\n93.64 \u00b1 0.79\nTADDY\n84.72 \u00b1 1.01\n67.95 \u00b1 0.94\n68.47 \u00b1 0.76\n93.15 \u00b1 0.88\nSAD\n86.77 \u00b1 0.24\n68.77 \u00b1 0.75\n69.44 \u00b1 0.87\n94.48 \u00b1 0.65\nTable 2: Overall performance of all methods in terms of AUC on\ndynamic node classification tasks. Means and standard deviations\nwere computed over 10 runs.\nas a graph network encoder to produce 128-dimensional node\nrepresentations. For model hyperparameters, we fix the fol-\nlowing configuration across all experiments without further\ntuning: we adopt Adam as the optimizer with an initial learn-\ning rate of 0.0005 and a batch size of 256 for both training.\nWe adopt mini-batch training for SAD and sample two-hop\nsubgraphs with 20 nodes per hop. For the memory bank, we\nset the memory size M as 4000, and the sampled size Ms\nas 1000. To better measure model performance in terms of\nAUC metrics, we choose the node classification task (simi-\nlar to anomaly node detection) as the downstream task, so we\nadopt Eq.(10) as the model optimization objective and find\nthat pick \u03b1 = 0.1 and \u03b2 = 0.01 performs well across all\ndatasets.\nThe proposed method is implemented using Py-\nTorch [Paszke et al., 2019] 1.10.1 and trained on a cloud\nserver with NVIDIA Tesla V100 GPU. Code is made pub-\nlicly available at https://github.com/D10Andy/SAD for repro-\nducibility.\n5.2\nOverall Performance\nWe compare our SAD against several strong baselines of\ngraph anomaly detection methods on node classification\ntasks, the baselines in our experiments include our back-\nbone model TGAT and the state-of-the-art supervised learn-\ning method TGN-no-mem [Xu et al., 2020b], as well as five\ngraph anomaly detection methods, which are specifically di-\nvided into unsupervised anomaly detection methods (Radar\nand DOMINANT) and semi-supervised anomaly detection\nmethods (SemiGNN, GDN, and TADDY). In order to be able\nto fairly compare the performance of the different methods,\nwe use the same structure of the projection network to learn\nthe classification results based on the node embedding out-\nputted by their anomaly detection tasks. Table 2 summarizes\nthe dynamic node classification results on different datasets.\nFor all experiments, we report the average results with stan-\ndard deviations of 10 different runs.\nAs shown in Table 2, our approach outperforms all base-\nlines on all datasets. Due to the lack of dynamic informa-\ntion, these static graph anomaly detection methods (Radar,\nSemiGNN, DOMINANT, and GDN) perform worse than\nthe dynamic graph supervision model TGAT on Reddit and\nMooc, which are more sensitive to temporal information. On\nthese datasets with large differences in the proportion of posi-\ntive and negative samples, our method shows a significant im-\nprovement compared to TGAT. Specifically, the relative aver-\nage AUC value improvements on Wikipedia, Reddit, Mooc,\nand Alipay are 3.54%, 1.70%, 2.56%, and 1.95%, respec-\ntively. Compared with the recent dynamic graph anomaly de-\n(a) Wikipedia\n(b) Reddit\nFigure 3: Dynamic node classification task results under different\ndrop ratios p on Wikipedia and Reddit, respectively.\ntection method TADDY, our method improves 2.05%, 0.82%,\n0.97%, and 1.33% on four datasets, respectively. We believe\nthe significant improvements are that SAD captures more in-\nformation on time-decaying state changes and that the mem-\nory bank mechanism can effectively obtain the distribution of\nnormal data, which helps to detect anomaly samples.\n5.3\nFew-shot Evaluation\nThe training samples used in the overall performance are an-\nnotated. However, in real application scenarios, annotated\nsamples are always difficult to obtain, and anomaly detection\nalgorithms need to be applied to distinguish anomalous nodes\nin large-scale unlabeled samples. To thoroughly evaluate the\nbenefits of different graph anomaly detection approaches in\nfew-shot scenarios, we varied the drop rate p, i.e., the ratio of\nrandomly removed node labels in the training set, from 0.1 to\n0.9 with a step size of 0.2.\nThe curve of AUC values of different models and datasets\nalong with the label drop ratio is drawn in Figure 3. SAD\nachieves state-of-the-art performance in all cases across two\ndynamic datasets. Noticeably, we observe that SAD achieves\noptimal performance on both datasets when the drop rate is\nat 0.3. The result indicates that there is a lot of redundant or\nnoisy information in the current graph datasets, which eas-\nily leads to overfitting of the model training. By dropping\nthis part of label information and using pseudo-labeled data\nto participate in the training, the performance of the model\nis improved instead. Other methods have a significant degra-\ndation in performance at a drop ratio of 0.5. Nevertheless,\nSAD still outperforms all the baselines by a large margin on\ntwo datasets, and the downstream performance is not signif-\nicantly sacrificed even for a particular large dropping ratio\n(e.g., 0.7 and 0.9). This indicates that even for few-shot sce-\nnarios, SAD could still mine useful statistical information for\ndownstream tasks and distinguish anomalous nodes with in-\nformation from abundant unlabeled samples. Overall, we be-\nlieve that the performance improvement in our model comes\nfrom two reasons: one is that the memory bank-dominated\nanomaly detector allows the model to learn a discriminative\nrepresentation between normal and anomalous samples, and\nthe other is that contrastive learning based on pseudo labels\nhelps to learn generic representations of nodes even in the\npresence of large amounts of unlabeled data.\n(a) TGAT\n(b) GDN\n(c) TADDY\n(d) SAD\nFigure 4: Visualization of the learned node embeddings w.r.t. different methods on Alipay. The red and green points represent the abnormal\nand normal samples, respectively.\n5.4\nVisualization Analysis\nIn this part, we visualize the learned node embeddings to\nevaluate the proposed method. And the 2D t-SNE [van der\nMaaten and Hinton, 2008] is used to reduce the dimensions\nof the hidden layer output node embeddings from 128 to\n2.\nFigure 4 shows the visualization of node embeddings\nlearned by TGAT, GDN, TADDY, and SAD on Alipay. As\nwe can see, the node embeddings of SAD are able to sepa-\nrate the abnormal and normal samples well. The supervised\nlearning method TGAT is prone to overfitting during train-\ning on datasets with class imbalance, resulting in poor dis-\ncrimination of node representation learning. GDN is not ef-\nfective in learning node embedding because it does not take\ninto account dynamic information and historical data statis-\ntical distributions. TADDY is based on modeling discrete-\ntime snapshots of dynamical graphs that are not well suited\nto continuous-time dynamical graphs. Overall, these results\nshow that the proposed method can learn useful node embed-\ndings for anomaly detection tasks on dynamic graphs.\n5.5\nAblation Study\nFinally, we conduct an ablation study to better examine the\ncontribution of different components in the proposed frame-\nwork, detailed as follows:\n\u2022 backbone. This entity indicates that only the graph cod-\ning network is used for dynamic node classification tasks\nin a supervised learning manner.\n\u2022 w/dev. This variant indicates that the deviation network\nbased on standard normal distribution is used on the ba-\nsis of the backbone model, without the memory bank\nmechanism.\n\u2022 w/mem. This variant indicates that the memory bank\nmechanism without the time decay technique is used on\nthe basis of the w/dev model.\n\u2022 w/time. This variant indicates that the time decay tech-\nnique in the memory bank is used on the basis of the\nw/mem model.\n\u2022 w/scl. This variant indicates that the supervised con-\ntrastive learning module is used on the basis of the\nw/time model, which corresponds to our full model.\nWikipedia\nReddit\nMooc\nTGAT\n80.76 \u00b1 2.30\n62.79 \u00b1 3.42\n64.04 \u00b1 1.02\nw/dev\n82.45 \u00b1 0.64\n64.15 \u00b1 2.93\n65.33 \u00b1 1.67\nw/mem\n85.20 \u00b1 1.30\n66.96 \u00b1 1.51\n67.25 \u00b1 0.75\nw/time\n85.44 \u00b1 0.75\n66.78 \u00b1 1.98\n67.53 \u00b1 0.93\nw/scl\n85.80 \u00b1 1.32\n67.71 \u00b1 0.75\n67.57 \u00b1 0.54\nTable 3: Results of the ablation study on the dynamic node classifi-\ncation task under the label-dropping ratio of 0.5.\nTo verify the performance of the model in a large num-\nber of unlabeled sample scenarios, we manually drop 0.5\nof the labels of the training set at random. Table 3 present\nthe results given by our several sub-modules. We summa-\nrize our observations as follows: Firstly, each of the incre-\nmental submodules proposed in our paper helps to improve\nthe performance of the model.\nAmong them, the mem-\nory bank mechanism yields the largest performance improve-\nments, e.g., 2.75%, 2.81%, and 1.92% performance improve-\nment on Wikipedia, Reddit, and Mooc, respectively. Sec-\nondly, the complete model outperforms the base model on\nthe three dynamic datasets by 5.04%, 4.92%, and 3.17%, re-\nspectively, which verifies the effectiveness of the proposed\nsubmodule by introducing the deviation loss and contrastive\nlearning based on pseudo labels.\n6\nConclusion\nWe present a semi-supervised learning framework SAD for\ndetecting anomalies over dynamic graphs.\nThe proposed\nframework utilizes a temporal graph network and an anomaly\ndetector to learn the anomaly score and uses a time-equipped\nmemory bank to record the overall statistical distribution of\nnormal and unlabeled samples as prior knowledge to guide\nthe subsequent learning of the model in a semi-supervised\nmanner. To further explore the potential of unlabeled sam-\nples, we produce pseudo-labels based on the difference be-\ntween the anomaly scores and use these labels directly to train\nthe backbone network in a supervised contrastive learning\nmanner. Our results from extensive experiments demonstrate\nthat the proposed SAD can achieves competitive performance\neven with a small amount of labeled data.\nReferences\n[Akoglu et al., 2015] Leman Akoglu, Hanghang Tong, and\nDanai Koutra.\nGraph based anomaly detection and de-\nscription: a survey. Data Min. Knowl. Discov., 29(3):626\u2013\n688, 2015.\n[Bandyopadhyay et al., 2019] Sambaran\nBandyopadhyay,\nN. Lokesh, and M. Narasimha Murty.\nOutlier aware\nnetwork embedding for attributed networks.\nIn The\nThirty-Third AAAI Conference on Artificial Intelligence,\nAAAI 2019, The Thirty-First Innovative Applications of\nArtificial Intelligence Conference, IAAI 2019, The Ninth\nAAAI Symposium on Educational Advances in Artificial\nIntelligence, EAAI 2019, Honolulu, Hawaii, USA, January\n27 - February 1, 2019, pages 12\u201319. AAAI Press, 2019.\n[Chen et al., 2020] Zhenxing Chen, Bo Liu, Meiqing Wang,\nPeng Dai, Jun Lv, and Liefeng Bo. Generative adversarial\nattributed network anomaly detection. In CIKM \u201920: The\n29th ACM International Conference on Information and\nKnowledge Management, Virtual Event, Ireland, October\n19-23, 2020, pages 1989\u20131992. ACM, 2020.\n[Ding et al., 2019] Kaize\nDing,\nJundong\nLi,\nRohit\nBhanushali, and Huan Liu.\nDeep anomaly detection\non attributed networks.\nIn Tanya Y. Berger-Wolf and\nNitesh V. Chawla, editors, Proceedings of the 2019\nSIAM International Conference on Data Mining, SDM\n2019, Calgary, Alberta, Canada, May 2-4, 2019, pages\n594\u2013602. SIAM, 2019.\n[Ding et al., 2021] Kaize Ding, Qinghai Zhou, Hanghang\nTong, and Huan Liu. Few-shot network anomaly detection\nvia cross-network meta-learning. In Jure Leskovec, Marko\nGrobelnik, Marc Najork, Jie Tang, and Leila Zia, edi-\ntors, WWW\u201921: The Web Conference 2021, Virtual Event /\nLjubljana, Slovenia, April 19-23, 2021, pages 2448\u20132456,\n2021.\n[Hadsell et al., 2006] Raia Hadsell, Sumit Chopra, and Yann\nLeCun. Dimensionality reduction by learning an invariant\nmapping. In 2006 IEEE Computer Society Conference on\nComputer Vision and Pattern Recognition (CVPR 2006),\n17-22 June 2006, New York, NY, USA, pages 1735\u20131742.\nIEEE Computer Society, 2006.\n[Hamilton et al., 2017] William L. Hamilton, Zhitao Ying,\nand Jure Leskovec. Inductive representation learning on\nlarge graphs. In Advances in Neural Information Process-\ning Systems 30: Annual Conference on Neural Informa-\ntion Processing Systems 2017, December 4-9, 2017, Long\nBeach, CA, USA, pages 1024\u20131034, 2017.\n[Kriegel et al., 2011] Hans-Peter Kriegel, Peer Kr\u00a8oger, Erich\nSchubert, and Arthur Zimek.\nInterpreting and unifying\noutlier scores. In Proceedings of the Eleventh SIAM In-\nternational Conference on Data Mining, SDM 2011, April\n28-30, 2011, Mesa, Arizona, USA, pages 13\u201324. SIAM /\nOmnipress, 2011.\n[Kumar et al., 2019] Srijan Kumar, Xikun Zhang, and Jure\nLeskovec.\nPredicting dynamic embedding trajectory in\ntemporal interaction networks. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge\nDiscovery & Data Mining, KDD 2019, Anchorage, AK,\nUSA, August 4-8, 2019, pages 1269\u20131278. ACM, 2019.\n[Li et al., 2017] Jundong Li, Harsh Dani, Xia Hu, and Huan\nLiu. Radar: Residual analysis for anomaly detection in at-\ntributed networks. In Carles Sierra, editor, Proceedings of\nthe Twenty-Sixth International Joint Conference on Artifi-\ncial Intelligence, IJCAI 2017, Melbourne, Australia, Au-\ngust 19-25, 2017, pages 2152\u20132158. ijcai.org, 2017.\n[Liu et al., 2021] Yixin Liu, Shirui Pan, Yu Guang Wang, Fei\nXiong, Liang Wang, and Vincent C. S. Lee.\nAnomaly\ndetection in dynamic graphs via transformer.\nCoRR,\nabs/2106.09876, 2021.\n[Liu et al., 2022] Yixin Liu, Zhao Li, Shirui Pan, Chen\nGong, Chuan Zhou, and George Karypis. Anomaly detec-\ntion on attributed networks via contrastive self-supervised\nlearning.\nIEEE Trans. Neural Networks Learn. Syst.,\n33(6):2378\u20132392, 2022.\n[Ma et al., 2021] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian\nYang, Quan Z. Sheng, and Hui Xiong.\nA comprehen-\nsive survey on graph anomaly detection with deep learn-\ning. CoRR, abs/2106.07178, 2021.\n[Meng et al., 2021] Xuying Meng, Suhang Wang, Zhimin\nLiang, Di Yao, Jihua Zhou, and Yujun Zhang.\nSemi-\nsupervised anomaly detection in dynamic communication\nnetworks. Inf. Sci., 571:527\u2013542, 2021.\n[Pang et al., 2019] Guansong Pang, Chunhua Shen, and An-\nton van den Hengel.\nDeep anomaly detection with de-\nviation networks.\nIn Proceedings of the 25th ACM\nSIGKDD International Conference on Knowledge Discov-\nery & Data Mining, KDD 2019, Anchorage, AK, USA, Au-\ngust 4-8, 2019, pages 353\u2013362, 2019.\n[Pareja et al., 2020] Aldo Pareja, Giacomo Domeniconi, Jie\nChen, Tengfei Ma, Toyotaro Suzumura, Hiroki Kaneza-\nshi, Tim Kaler, Tao B. Schardl, and Charles E. Leiserson.\nEvolvegcn: Evolving graph convolutional networks for\ndynamic graphs. In The Thirty-Fourth AAAI Conference\non Artificial Intelligence, AAAI 2020, The Thirty-Second\nInnovative Applications of Artificial Intelligence Confer-\nence, IAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020, New\nYork, NY, USA, February 7-12, 2020, pages 5363\u20135370.\nAAAI Press, 2020.\n[Paszke et al., 2019] Adam Paszke, Sam Gross, Francisco\nMassa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas K\u00a8opf, Edward Z.\nYang, Zachary DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai,\nand Soumith Chintala. Pytorch: An imperative style, high-\nperformance deep learning library.\nIn NeurIPS, pages\n8024\u20138035, 2019.\n[Qian et al., 2021] Yiyue Qian, Yiming Zhang, Yanfang Ye,\nand Chuxu Zhang. Distilling meta knowledge on hetero-\ngeneous graph for illicit drug trafficker detection on so-\ncial media. In Advances in Neural Information Process-\ning Systems 34: Annual Conference on Neural Informa-\ntion Processing Systems 2021, NeurIPS 2021, December\n6-14, 2021, virtual, pages 26911\u201326923, 2021.\n[Singer et al., 2019] Uriel Singer, Ido Guy, and Kira Radin-\nsky. Node embedding over temporal graphs. In Proceed-\nings of the Twenty-Eighth International Joint Conference\non Artificial Intelligence, IJCAI 2019, Macao, China, Au-\ngust 10-16, 2019, pages 4605\u20134612. ijcai.org, 2019.\n[Trivedi et al., 2019] Rakshit Trivedi, Mehrdad Farajtabar,\nPrasenjeet Biswal, and Hongyuan Zha.\nDyrep: Learn-\ning representations over dynamic graphs.\nIn 7th Inter-\nnational Conference on Learning Representations, ICLR\n2019, New Orleans, LA, USA, May 6-9, 2019. OpenRe-\nview.net, 2019.\n[van der Maaten and Hinton, 2008] L.J.P. van der Maaten\nand G.E. Hinton.\nVisualizing high-dimensional data\nusing t-sne.\nJournal of Machine Learning Research,\n9(nov):2579\u20132605, 2008. Pagination: 27.\n[Velickovic et al., 2018] Petar Velickovic, Guillem Cucurull,\nArantxa Casanova, Adriana Romero, Pietro Li`o, and\nYoshua Bengio. Graph attention networks. In 6th Inter-\nnational Conference on Learning Representations, ICLR\n2018, Vancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net, 2018.\n[Wang et al., 2019] Daixin Wang, Yuan Qi, Jianbin Lin,\nPeng Cui, Quanhui Jia, Zhen Wang, Yanming Fang, Quan\nYu, Jun Zhou, and Shuang Yang. A semi-supervised graph\nattentive network for financial fraud detection. In 2019\nIEEE International Conference on Data Mining, ICDM\n2019, Beijing, China, November 8-11, 2019, pages 598\u2013\n607. IEEE, 2019.\n[Xu et al., 2020a] Da\nXu,\nChuanwei\nRuan,\nEvren\nK\u00a8orpeoglu, Sushant Kumar, and Kannan Achan.\nIn-\nductive representation learning on temporal graphs. In 8th\nInternational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020,\n2020.\n[Xu et al., 2020b] Da\nXu,\nChuanwei\nRuan,\nEvren\nK\u00a8orpeoglu, Sushant Kumar, and Kannan Achan.\nIn-\nductive representation learning on temporal graphs. In 8th\nInternational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.\nOpenReview.net, 2020.\n[Zhou and Paffenroth, 2017] Chong Zhou and Randy C. Paf-\nfenroth.\nAnomaly detection with robust deep autoen-\ncoders. In Proceedings of the 23rd ACM SIGKDD Inter-\nnational Conference on Knowledge Discovery and Data\nMining, Halifax, NS, Canada, August 13 - 17, 2017, pages\n665\u2013674. ACM, 2017.\n",
    "1710.10903": "Published as a conference paper at ICLR 2018\nGRAPH ATTENTION NETWORKS\nPetar Veli\u02c7ckovi\u00b4c\u2217\nDepartment of Computer Science and Technology\nUniversity of Cambridge\npetar.velickovic@cst.cam.ac.uk\nGuillem Cucurull\u2217\nCentre de Visi\u00b4o per Computador, UAB\ngcucurull@gmail.com\nArantxa Casanova\u2217\nCentre de Visi\u00b4o per Computador, UAB\nar.casanova.8@gmail.com\nAdriana Romero\nMontr\u00b4eal Institute for Learning Algorithms\nadriana.romero.soriano@umontreal.ca\nPietro Li`o\nDepartment of Computer Science and Technology\nUniversity of Cambridge\npietro.lio@cst.cam.ac.uk\nYoshua Bengio\nMontr\u00b4eal Institute for Learning Algorithms\nyoshua.umontreal@gmail.com\nABSTRACT\nWe present graph attention networks (GATs), novel neural network architectures\nthat operate on graph-structured data, leveraging masked self-attentional layers to\naddress the shortcomings of prior methods based on graph convolutions or their\napproximations. By stacking layers in which nodes are able to attend over their\nneighborhoods\u2019 features, we enable (implicitly) specifying different weights to\ndifferent nodes in a neighborhood, without requiring any kind of costly matrix op-\neration (such as inversion) or depending on knowing the graph structure upfront.\nIn this way, we address several key challenges of spectral-based graph neural net-\nworks simultaneously, and make our model readily applicable to inductive as well\nas transductive problems. Our GAT models have achieved or matched state-of-the-\nart results across four established transductive and inductive graph benchmarks:\nthe Cora, Citeseer and Pubmed citation network datasets, as well as a protein-\nprotein interaction dataset (wherein test graphs remain unseen during training).\n1\nINTRODUCTION\nConvolutional Neural Networks (CNNs) have been successfully applied to tackle problems such\nas image classi\ufb01cation (He et al., 2016), semantic segmentation (J\u00b4egou et al., 2017) or machine\ntranslation (Gehring et al., 2016), where the underlying data representation has a grid-like structure.\nThese architectures ef\ufb01ciently reuse their local \ufb01lters, with learnable parameters, by applying them\nto all the input positions.\nHowever, many interesting tasks involve data that can not be represented in a grid-like structure and\nthat instead lies in an irregular domain. This is the case of 3D meshes, social networks, telecommu-\nnication networks, biological networks or brain connectomes. Such data can usually be represented\nin the form of graphs.\nThere have been several attempts in the literature to extend neural networks to deal with arbitrarily\nstructured graphs. Early work used recursive neural networks to process data represented in graph\ndomains as directed acyclic graphs (Frasconi et al., 1998; Sperduti & Starita, 1997). Graph Neural\nNetworks (GNNs) were introduced in Gori et al. (2005) and Scarselli et al. (2009) as a generalization\nof recursive neural networks that can directly deal with a more general class of graphs, e.g. cyclic,\ndirected and undirected graphs. GNNs consist of an iterative process, which propagates the node\nstates until equilibrium; followed by a neural network, which produces an output for each node\n\u2217Work performed while the author was at the Montr\u00b4eal Institute of Learning Algorithms.\n1\narXiv:1710.10903v3  [stat.ML]  4 Feb 2018\nPublished as a conference paper at ICLR 2018\nbased on its state. This idea was adopted and improved by Li et al. (2016), which propose to use\ngated recurrent units (Cho et al., 2014) in the propagation step.\nNevertheless, there is an increasing interest in generalizing convolutions to the graph domain. Ad-\nvances in this direction are often categorized as spectral approaches and non-spectral approaches.\nOn one hand, spectral approaches work with a spectral representation of the graphs and have been\nsuccessfully applied in the context of node classi\ufb01cation. In Bruna et al. (2014), the convolution\noperation is de\ufb01ned in the Fourier domain by computing the eigendecomposition of the graph Lapla-\ncian, resulting in potentially intense computations and non-spatially localized \ufb01lters. These issues\nwere addressed by subsequent works. Henaff et al. (2015) introduced a parameterization of the\nspectral \ufb01lters with smooth coef\ufb01cients in order to make them spatially localized. Later, Defferrard\net al. (2016) proposed to approximate the \ufb01lters by means of a Chebyshev expansion of the graph\nLaplacian, removing the need to compute the eigenvectors of the Laplacian and yielding spatially\nlocalized \ufb01lters. Finally, Kipf & Welling (2017) simpli\ufb01ed the previous method by restricting the\n\ufb01lters to operate in a 1-step neighborhood around each node. However, in all of the aforementioned\nspectral approaches, the learned \ufb01lters depend on the Laplacian eigenbasis, which depends on the\ngraph structure. Thus, a model trained on a speci\ufb01c structure can not be directly applied to a graph\nwith a different structure.\nOn the other hand, we have non-spectral approaches (Duvenaud et al., 2015; Atwood & Towsley,\n2016; Hamilton et al., 2017), which de\ufb01ne convolutions directly on the graph, operating on groups\nof spatially close neighbors. One of the challenges of these approaches is to de\ufb01ne an operator which\nworks with different sized neighborhoods and maintains the weight sharing property of CNNs. In\nsome cases, this requires learning a speci\ufb01c weight matrix for each node degree (Duvenaud et al.,\n2015), using the powers of a transition matrix to de\ufb01ne the neighborhood while learning weights for\neach input channel and neighborhood degree (Atwood & Towsley, 2016), or extracting and normal-\nizing neighborhoods containing a \ufb01xed number of nodes (Niepert et al., 2016). Monti et al. (2016)\npresented mixture model CNNs (MoNet), a spatial approach which provides a uni\ufb01ed generaliza-\ntion of CNN architectures to graphs. More recently, Hamilton et al. (2017) introduced GraphSAGE,\na method for computing node representations in an inductive manner. This technique operates by\nsampling a \ufb01xed-size neighborhood of each node, and then performing a speci\ufb01c aggregator over\nit (such as the mean over all the sampled neighbors\u2019 feature vectors, or the result of feeding them\nthrough a recurrent neural network). This approach has yielded impressive performance across sev-\neral large-scale inductive benchmarks.\nAttention mechanisms have become almost a de facto standard in many sequence-based tasks (Bah-\ndanau et al., 2015; Gehring et al., 2016). One of the bene\ufb01ts of attention mechanisms is that they\nallow for dealing with variable sized inputs, focusing on the most relevant parts of the input to make\ndecisions. When an attention mechanism is used to compute a representation of a single sequence,\nit is commonly referred to as self-attention or intra-attention. Together with Recurrent Neural Net-\nworks (RNNs) or convolutions, self-attention has proven to be useful for tasks such as machine\nreading (Cheng et al., 2016) and learning sentence representations (Lin et al., 2017). However,\nVaswani et al. (2017) showed that not only self-attention can improve a method based on RNNs or\nconvolutions, but also that it is suf\ufb01cient for constructing a powerful model obtaining state-of-the-art\nperformance on the machine translation task.\nInspired by this recent work, we introduce an attention-based architecture to perform node classi\ufb01ca-\ntion of graph-structured data. The idea is to compute the hidden representations of each node in the\ngraph, by attending over its neighbors, following a self-attention strategy. The attention architecture\nhas several interesting properties: (1) the operation is ef\ufb01cient, since it is parallelizable across node-\nneighbor pairs; (2) it can be applied to graph nodes having different degrees by specifying arbitrary\nweights to the neighbors; and (3) the model is directly applicable to inductive learning problems,\nincluding tasks where the model has to generalize to completely unseen graphs. We validate the\nproposed approach on four challenging benchmarks: Cora, Citeseer and Pubmed citation networks\nas well as an inductive protein-protein interaction dataset, achieving or matching state-of-the-art re-\nsults that highlight the potential of attention-based models when dealing with arbitrarily structured\ngraphs.\nIt is worth noting that, as Kipf & Welling (2017) and Atwood & Towsley (2016), our work can also\nbe reformulated as a particular instance of MoNet (Monti et al., 2016). Moreover, our approach of\n2\nPublished as a conference paper at ICLR 2018\nsharing a neural network computation across edges is reminiscent of the formulation of relational\nnetworks (Santoro et al., 2017) and VAIN (Hoshen, 2017), wherein relations between objects or\nagents are aggregated pair-wise, by employing a shared mechanism. Similarly, our proposed at-\ntention model can be connected to the works by Duan et al. (2017) and Denil et al. (2017), which\nuse a neighborhood attention operation to compute attention coef\ufb01cients between different objects\nin an environment. Other related approaches include locally linear embedding (LLE) (Roweis &\nSaul, 2000) and memory networks (Weston et al., 2014). LLE selects a \ufb01xed number of neighbors\naround each data point, and learns a weight coef\ufb01cient for each neighbor to reconstruct each point\nas a weighted sum of its neighbors. A second optimization step extracts the point\u2019s feature embed-\nding. Memory networks also share some connections with our work, in particular, if we interpret\nthe neighborhood of a node as the memory, which is used to compute the node features by attending\nover its values, and then is updated by storing the new features in the same position.\n2\nGAT ARCHITECTURE\nIn this section, we will present the building block layer used to construct arbitrary graph attention\nnetworks (through stacking this layer), and directly outline its theoretical and practical bene\ufb01ts and\nlimitations compared to prior work in the domain of neural graph processing.\n2.1\nGRAPH ATTENTIONAL LAYER\nWe will start by describing a single graph attentional layer, as the sole layer utilized throughout\nall of the GAT architectures used in our experiments. The particular attentional setup utilized by us\nclosely follows the work of Bahdanau et al. (2015)\u2014but the framework is agnostic to the particular\nchoice of attention mechanism.\nThe input to our layer is a set of node features, h = {\u20d7h1,\u20d7h2, . . . ,\u20d7hN},\u20d7hi \u2208RF , where N is the\nnumber of nodes, and F is the number of features in each node. The layer produces a new set of node\nfeatures (of potentially different cardinality F \u2032), h\u2032 = {\u20d7h\u2032\n1,\u20d7h\u2032\n2, . . . ,\u20d7h\u2032\nN},\u20d7h\u2032\ni \u2208RF \u2032, as its output.\nIn order to obtain suf\ufb01cient expressive power to transform the input features into higher-level fea-\ntures, at least one learnable linear transformation is required. To that end, as an initial step, a shared\nlinear transformation, parametrized by a weight matrix, W \u2208RF \u2032\u00d7F , is applied to every node. We\nthen perform self-attention on the nodes\u2014a shared attentional mechanism a : RF \u2032 \u00d7 RF \u2032 \u2192R\ncomputes attention coef\ufb01cients\neij = a(W\u20d7hi, W\u20d7hj)\n(1)\nthat indicate the importance of node j\u2019s features to node i. In its most general formulation, the model\nallows every node to attend on every other node, dropping all structural information. We inject the\ngraph structure into the mechanism by performing masked attention\u2014we only compute eij for nodes\nj \u2208Ni, where Ni is some neighborhood of node i in the graph. In all our experiments, these will\nbe exactly the \ufb01rst-order neighbors of i (including i). To make coef\ufb01cients easily comparable across\ndifferent nodes, we normalize them across all choices of j using the softmax function:\n\u03b1ij = softmaxj(eij) =\nexp(eij)\nP\nk\u2208Ni exp(eik).\n(2)\nIn our experiments, the attention mechanism a is a single-layer feedforward neural network,\nparametrized by a weight vector \u20d7a \u2208R2F \u2032, and applying the LeakyReLU nonlinearity (with negative\ninput slope \u03b1 = 0.2). Fully expanded out, the coef\ufb01cients computed by the attention mechanism\n(illustrated by Figure 1 (left)) may then be expressed as:\n\u03b1ij =\nexp\n\u0010\nLeakyReLU\n\u0010\n\u20d7aT [W\u20d7hi\u2225W\u20d7hj]\n\u0011\u0011\nP\nk\u2208Ni exp\n\u0010\nLeakyReLU\n\u0010\n\u20d7aT [W\u20d7hi\u2225W\u20d7hk]\n\u0011\u0011\n(3)\nwhere \u00b7T represents transposition and \u2225is the concatenation operation.\nOnce obtained, the normalized attention coef\ufb01cients are used to compute a linear combination of the\nfeatures corresponding to them, to serve as the \ufb01nal output features for every node (after potentially\n3\nPublished as a conference paper at ICLR 2018\n\u03b1ij\n\u20d7a\nsoftmaxj\nW\u20d7hi\nW\u20d7hj\n\u20d7h1\n\u20d7h2\n\u20d7h3\n\u20d7h4\n\u20d7h5\n\u20d7h6\n\u20d7\u03b116\n\u20d7\u03b111\n\u20d7\u03b112\n\u20d7\u03b113\n\u20d7\u03b114\n\u20d7\u03b115\n\u20d7h\u2032\n1\nconcat/avg\nFigure 1: Left: The attention mechanism a(W\u20d7hi, W\u20d7hj) employed by our model, parametrized\nby a weight vector \u20d7a \u2208R2F \u2032, applying a LeakyReLU activation. Right: An illustration of multi-\nhead attention (with K = 3 heads) by node 1 on its neighborhood. Different arrow styles and\ncolors denote independent attention computations. The aggregated features from each head are\nconcatenated or averaged to obtain \u20d7h\u2032\n1.\napplying a nonlinearity, \u03c3):\n\u20d7h\u2032\ni = \u03c3\n\uf8eb\n\uf8edX\nj\u2208Ni\n\u03b1ijW\u20d7hj\n\uf8f6\n\uf8f8.\n(4)\nTo stabilize the learning process of self-attention, we have found extending our mechanism to em-\nploy multi-head attention to be bene\ufb01cial, similarly to Vaswani et al. (2017). Speci\ufb01cally, K inde-\npendent attention mechanisms execute the transformation of Equation 4, and then their features are\nconcatenated, resulting in the following output feature representation:\n\u20d7h\u2032\ni =\nK\n\u2225\nk=1\n\u03c3\n\uf8eb\n\uf8edX\nj\u2208Ni\n\u03b1k\nijWk\u20d7hj\n\uf8f6\n\uf8f8\n(5)\nwhere \u2225represents concatenation, \u03b1k\nij are normalized attention coef\ufb01cients computed by the k-th\nattention mechanism (ak), and Wk is the corresponding input linear transformation\u2019s weight matrix.\nNote that, in this setting, the \ufb01nal returned output, h\u2032, will consist of KF \u2032 features (rather than F \u2032)\nfor each node.\nSpecially, if we perform multi-head attention on the \ufb01nal (prediction) layer of the network, concate-\nnation is no longer sensible\u2014instead, we employ averaging, and delay applying the \ufb01nal nonlinear-\nity (usually a softmax or logistic sigmoid for classi\ufb01cation problems) until then:\n\u20d7h\u2032\ni = \u03c3\n\uf8eb\n\uf8ed1\nK\nK\nX\nk=1\nX\nj\u2208Ni\n\u03b1k\nijWk\u20d7hj\n\uf8f6\n\uf8f8\n(6)\nThe aggregation process of a multi-head graph attentional layer is illustrated by Figure 1 (right).\n2.2\nCOMPARISONS TO RELATED WORK\nThe graph attentional layer described in subsection 2.1 directly addresses several issues that were\npresent in prior approaches to modelling graph-structured data with neural networks:\n\u2022 Computationally, it is highly ef\ufb01cient: the operation of the self-attentional layer can be par-\nallelized across all edges, and the computation of output features can be parallelized across\n4\nPublished as a conference paper at ICLR 2018\nall nodes. No eigendecompositions or similar costly matrix operations are required. The\ntime complexity of a single GAT attention head computing F \u2032 features may be expressed\nas O(|V |FF \u2032 + |E|F \u2032), where F is the number of input features, and |V | and |E| are the\nnumbers of nodes and edges in the graph, respectively. This complexity is on par with\nthe baseline methods such as Graph Convolutional Networks (GCNs) (Kipf & Welling,\n2017). Applying multi-head attention multiplies the storage and parameter requirements\nby a factor of K, while the individual heads\u2019 computations are fully independent and can\nbe parallelized.\n\u2022 As opposed to GCNs, our model allows for (implicitly) assigning different importances to\nnodes of a same neighborhood, enabling a leap in model capacity. Furthermore, analyzing\nthe learned attentional weights may lead to bene\ufb01ts in interpretability, as was the case in\nthe machine translation domain (e.g. the qualitative analysis of Bahdanau et al. (2015)).\n\u2022 The attention mechanism is applied in a shared manner to all edges in the graph, and there-\nfore it does not depend on upfront access to the global graph structure or (features of) all of\nits nodes (a limitation of many prior techniques). This has several desirable implications:\n\u2013 The graph is not required to be undirected (we may simply leave out computing \u03b1ij if\nedge j \u2192i is not present).\n\u2013 It makes our technique directly applicable to inductive learning\u2014including tasks\nwhere the model is evaluated on graphs that are completely unseen during training.\n\u2022 The recently published inductive method of Hamilton et al. (2017) samples a \ufb01xed-size\nneighborhood of each node, in order to keep its computational footprint consistent; this\ndoes not allow it access to the entirety of the neighborhood while performing inference.\nMoreover, this technique achieved some of its strongest results when an LSTM (Hochreiter\n& Schmidhuber, 1997)-based neighborhood aggregator is used. This assumes the existence\nof a consistent sequential node ordering across neighborhoods, and the authors have rec-\nti\ufb01ed it by consistently feeding randomly-ordered sequences to the LSTM. Our technique\ndoes not suffer from either of these issues\u2014it works with the entirety of the neighborhood\n(at the expense of a variable computational footprint, which is still on-par with methods\nlike the GCN), and does not assume any ordering within it.\n\u2022 As mentioned in Section 1, GAT can be reformulated as a particular instance of MoNet\n(Monti et al., 2016).\nMore speci\ufb01cally, setting the pseudo-coordinate function to be\nu(x, y) = f(x)\u2225f(y), where f(x) represent (potentially MLP-transformed) features of\nnode x and \u2225is concatenation; and the weight function to be wj(u) = softmax(MLP(u))\n(with the softmax performed over the entire neighborhood of a node) would make MoNet\u2019s\npatch operator similar to ours. Nevertheless, one should note that, in comparison to previ-\nously considered MoNet instances, our model uses node features for similarity computa-\ntions, rather than the node\u2019s structural properties (which would assume knowing the graph\nstructure upfront).\nWe were able to produce a version of the GAT layer that leverages sparse matrix operations, reducing\nthe storage complexity to linear in the number of nodes and edges and enabling the execution of\nGAT models on larger graph datasets. However, the tensor manipulation framework we used only\nsupports sparse matrix multiplication for rank-2 tensors, which limits the batching capabilities of\nthe layer as it is currently implemented (especially for datasets with multiple graphs). Appropriately\naddressing this constraint is an important direction for future work. Depending on the regularity of\nthe graph structure in place, GPUs may not be able to offer major performance bene\ufb01ts compared\nto CPUs in these sparse scenarios. It should also be noted that the size of the \u201creceptive \ufb01eld\u201d of\nour model is upper-bounded by the depth of the network (similarly as for GCN and similar models).\nTechniques such as skip connections (He et al., 2016) could be readily applied for appropriately\nextending the depth, however. Lastly, parallelization across all the graph edges, especially in a\ndistributed manner, may involve a lot of redundant computation, as the neighborhoods will often\nhighly overlap in graphs of interest.\n3\nEVALUATION\nWe have performed comparative evaluation of GAT models against a wide variety of strong base-\nlines and previous approaches, on four established graph-based benchmark tasks (transductive as\n5\nPublished as a conference paper at ICLR 2018\nTable 1: Summary of the datasets used in our experiments.\nCora\nCiteseer\nPubmed\nPPI\nTask\nTransductive\nTransductive\nTransductive\nInductive\n# Nodes\n2708 (1 graph)\n3327 (1 graph)\n19717 (1 graph)\n56944 (24 graphs)\n# Edges\n5429\n4732\n44338\n818716\n# Features/Node\n1433\n3703\n500\n50\n# Classes\n7\n6\n3\n121 (multilabel)\n# Training Nodes\n140\n120\n60\n44906 (20 graphs)\n# Validation Nodes\n500\n500\n500\n6514 (2 graphs)\n# Test Nodes\n1000\n1000\n1000\n5524 (2 graphs)\nwell as inductive), achieving or matching state-of-the-art performance across all of them. This sec-\ntion summarizes our experimental setup, results, and a brief qualitative analysis of a GAT model\u2019s\nextracted feature representations.\n3.1\nDATASETS\nTransductive learning\nWe utilize three standard citation network benchmark datasets\u2014Cora,\nCiteseer and Pubmed (Sen et al., 2008)\u2014and closely follow the transductive experimental setup of\nYang et al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected)\ncitations. Node features correspond to elements of a bag-of-words representation of a document.\nEach node has a class label. We allow for only 20 nodes per class to be used for training\u2014however,\nhonoring the transductive setup, the training algorithm has access to all of the nodes\u2019 feature vec-\ntors. The predictive power of the trained models is evaluated on 1000 test nodes, and we use 500\nadditional nodes for validation purposes (the same ones as used by Kipf & Welling (2017)). The\nCora dataset contains 2708 nodes, 5429 edges, 7 classes and 1433 features per node. The Citeseer\ndataset contains 3327 nodes, 4732 edges, 6 classes and 3703 features per node. The Pubmed dataset\ncontains 19717 nodes, 44338 edges, 3 classes and 500 features per node.\nInductive learning\nWe make use of a protein-protein interaction (PPI) dataset that consists of\ngraphs corresponding to different human tissues (Zitnik & Leskovec, 2017). The dataset contains\n20 graphs for training, 2 for validation and 2 for testing. Critically, testing graphs remain com-\npletely unobserved during training. To construct the graphs, we used the preprocessed data provided\nby Hamilton et al. (2017). The average number of nodes per graph is 2372. Each node has 50\nfeatures that are composed of positional gene sets, motif gene sets and immunological signatures.\nThere are 121 labels for each node set from gene ontology, collected from the Molecular Signatures\nDatabase (Subramanian et al., 2005), and a node can possess several labels simultaneously.\nAn overview of the interesting characteristics of the datasets is given in Table 1.\n3.2\nSTATE-OF-THE-ART METHODS\nTransductive learning\nFor transductive learning tasks, we compare against the same strong base-\nlines and state-of-the-art approaches as speci\ufb01ed in Kipf & Welling (2017). This includes label\npropagation (LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012),\nmanifold regularization (ManiReg) (Belkin et al., 2006), skip-gram based graph embeddings (Deep-\nWalk) (Perozzi et al., 2014), the iterative classi\ufb01cation algorithm (ICA) (Lu & Getoor, 2003) and\nPlanetoid (Yang et al., 2016). We also directly compare our model against GCNs (Kipf & Welling,\n2017), as well as graph convolutional models utilising higher-order Chebyshev \ufb01lters (Defferrard\net al., 2016), and the MoNet model presented in Monti et al. (2016).\nInductive learning\nFor the inductive learning task, we compare against the four different super-\nvised GraphSAGE inductive methods presented in Hamilton et al. (2017). These provide a variety\nof approaches to aggregating features within a sampled neighborhood: GraphSAGE-GCN (which\nextends a graph convolution-style operation to the inductive setting), GraphSAGE-mean (taking\n6\nPublished as a conference paper at ICLR 2018\nthe elementwise mean value of feature vectors), GraphSAGE-LSTM (aggregating by feeding the\nneighborhood features into an LSTM) and GraphSAGE-pool (taking the elementwise maximization\noperation of feature vectors transformed by a shared nonlinear multilayer perceptron). The other\ntransductive approaches are either completely inappropriate in an inductive setting or assume that\nnodes are incrementally added to a single graph, making them unusable for the setup where test\ngraphs are completely unseen during training (such as the PPI dataset).\nAdditionally, for both tasks we provide the performance of a per-node shared multilayer perceptron\n(MLP) classi\ufb01er (that does not incorporate graph structure at all).\n3.3\nEXPERIMENTAL SETUP\nTransductive learning\nFor the transductive learning tasks, we apply a two-layer GAT model. Its\narchitectural hyperparameters have been optimized on the Cora dataset and are then reused for Cite-\nseer. The \ufb01rst layer consists of K = 8 attention heads computing F \u2032 = 8 features each (for a total\nof 64 features), followed by an exponential linear unit (ELU) (Clevert et al., 2016) nonlinearity. The\nsecond layer is used for classi\ufb01cation: a single attention head that computes C features (where C\nis the number of classes), followed by a softmax activation. For coping with the small training set\nsizes, regularization is liberally applied within the model. During training, we apply L2 regulariza-\ntion with \u03bb = 0.0005. Furthermore, dropout (Srivastava et al., 2014) with p = 0.6 is applied to\nboth layers\u2019 inputs, as well as to the normalized attention coef\ufb01cients (critically, this means that at\neach training iteration, each node is exposed to a stochastically sampled neighborhood). Similarly\nas observed by Monti et al. (2016), we found that Pubmed\u2019s training set size (60 examples) required\nslight changes to the GAT architecture: we have applied K = 8 output attention heads (instead of\none), and strengthened the L2 regularization to \u03bb = 0.001. Otherwise, the architecture matches the\none used for Cora and Citeseer.\nInductive learning\nFor the inductive learning task, we apply a three-layer GAT model. Both of the\n\ufb01rst two layers consist of K = 4 attention heads computing F \u2032 = 256 features (for a total of 1024\nfeatures), followed by an ELU nonlinearity. The \ufb01nal layer is used for (multi-label) classi\ufb01cation:\nK = 6 attention heads computing 121 features each, that are averaged and followed by a logistic\nsigmoid activation. The training sets for this task are suf\ufb01ciently large and we found no need to apply\nL2 regularization or dropout\u2014we have, however, successfully employed skip connections (He et al.,\n2016) across the intermediate attentional layer. We utilize a batch size of 2 graphs during training. To\nstrictly evaluate the bene\ufb01ts of applying an attention mechanism in this setting (i.e. comparing with\na near GCN-equivalent model), we also provide the results when a constant attention mechanism,\na(x, y) = 1, is used, with the same architecture\u2014this will assign the same weight to every neighbor.\nBoth models are initialized using Glorot initialization (Glorot & Bengio, 2010) and trained to mini-\nmize cross-entropy on the training nodes using the Adam SGD optimizer (Kingma & Ba, 2014) with\nan initial learning rate of 0.01 for Pubmed, and 0.005 for all other datasets. In both cases we use\nan early stopping strategy on both the cross-entropy loss and accuracy (transductive) or micro-F1\n(inductive) score on the validation nodes, with a patience of 100 epochs1.\n3.4\nRESULTS\nThe results of our comparative evaluation experiments are summarized in Tables 2 and 3.\nFor the transductive tasks, we report the mean classi\ufb01cation accuracy (with standard deviation) on\nthe test nodes of our method after 100 runs, and reuse the metrics already reported in Kipf & Welling\n(2017) and Monti et al. (2016) for state-of-the-art techniques. Speci\ufb01cally, for the Chebyshev \ufb01lter-\nbased approach (Defferrard et al., 2016), we provide the maximum reported performance for \ufb01lters\nof orders K = 2 and K = 3. In order to fairly assess the bene\ufb01ts of the attention mechanism,\nwe further evaluate a GCN model that computes 64 hidden features, attempting both the ReLU and\nELU activation, and reporting (as GCN-64\u2217) the better result after 100 runs (which was the ReLU\nin all three cases).\nFor the inductive task, we report the micro-averaged F1 score on the nodes of the two unseen test\ngraphs, averaged after 10 runs, and reuse the metrics already reported in Hamilton et al. (2017) for\n1Our implementation of the GAT layer may be found at: https://github.com/PetarV-/GAT.\n7\nPublished as a conference paper at ICLR 2018\nTable 2: Summary of results in terms of classi\ufb01cation accuracies, for Cora, Citeseer and Pubmed.\nGCN-64\u2217corresponds to the best GCN result computing 64 hidden features (using ReLU or ELU).\nTransductive\nMethod\nCora\nCiteseer\nPubmed\nMLP\n55.1%\n46.5%\n71.4%\nManiReg (Belkin et al., 2006)\n59.5%\n60.1%\n70.7%\nSemiEmb (Weston et al., 2012)\n59.0%\n59.6%\n71.7%\nLP (Zhu et al., 2003)\n68.0%\n45.3%\n63.0%\nDeepWalk (Perozzi et al., 2014)\n67.2%\n43.2%\n65.3%\nICA (Lu & Getoor, 2003)\n75.1%\n69.1%\n73.9%\nPlanetoid (Yang et al., 2016)\n75.7%\n64.7%\n77.2%\nChebyshev (Defferrard et al., 2016)\n81.2%\n69.8%\n74.4%\nGCN (Kipf & Welling, 2017)\n81.5%\n70.3%\n79.0%\nMoNet (Monti et al., 2016)\n81.7 \u00b1 0.5%\n\u2014\n78.8 \u00b1 0.3%\nGCN-64\u2217\n81.4 \u00b1 0.5%\n70.9 \u00b1 0.5%\n79.0 \u00b1 0.3%\nGAT (ours)\n83.0 \u00b1 0.7%\n72.5 \u00b1 0.7%\n79.0 \u00b1 0.3%\nTable 3: Summary of results in terms of micro-averaged F1 scores, for the PPI dataset. GraphSAGE\u2217\ncorresponds to the best GraphSAGE result we were able to obtain by just modifying its architecture.\nConst-GAT corresponds to a model with the same architecture as GAT, but with a constant attention\nmechanism (assigning same importance to each neighbor; GCN-like inductive operator).\nInductive\nMethod\nPPI\nRandom\n0.396\nMLP\n0.422\nGraphSAGE-GCN (Hamilton et al., 2017)\n0.500\nGraphSAGE-mean (Hamilton et al., 2017)\n0.598\nGraphSAGE-LSTM (Hamilton et al., 2017)\n0.612\nGraphSAGE-pool (Hamilton et al., 2017)\n0.600\nGraphSAGE\u2217\n0.768\nConst-GAT (ours)\n0.934 \u00b1 0.006\nGAT (ours)\n0.973 \u00b1 0.002\nthe other techniques. Speci\ufb01cally, as our setup is supervised, we compare against the supervised\nGraphSAGE approaches. To evaluate the bene\ufb01ts of aggregating across the entire neighborhood,\nwe further provide (as GraphSAGE\u2217) the best result we were able to achieve with GraphSAGE by\njust modifying its architecture (this was with a three-layer GraphSAGE-LSTM with [512, 512, 726]\nfeatures computed in each layer and 128 features used for aggregating neighborhoods). Finally,\nwe report the 10-run result of our constant attention GAT model (as Const-GAT), to fairly evaluate\nthe bene\ufb01ts of the attention mechanism against a GCN-like aggregation scheme (with the same\narchitecture).\nOur results successfully demonstrate state-of-the-art performance being achieved or matched across\nall four datasets\u2014in concordance with our expectations, as per the discussion in Section 2.2. More\nspeci\ufb01cally, we are able to improve upon GCNs by a margin of 1.5% and 1.6% on Cora and Cite-\nseer, respectively, suggesting that assigning different weights to nodes of a same neighborhood may\nbe bene\ufb01cial. It is worth noting the improvements achieved on the PPI dataset: Our GAT model\nimproves by 20.5% w.r.t. the best GraphSAGE result we were able to obtain, demonstrating that our\nmodel has the potential to be applied in inductive settings, and that larger predictive power can be\nleveraged by observing the entire neighborhood. Furthermore, it improves by 3.9% w.r.t. Const-GAT\n(the identical architecture with constant attention mechanism), once again directly demonstrating the\nsigni\ufb01cance of being able to assign different weights to different neighbors.\n8\nPublished as a conference paper at ICLR 2018\nThe effectiveness of the learned feature representations may also be investigated qualitatively\u2014and\nfor this purpose we provide a visualization of the t-SNE (Maaten & Hinton, 2008)-transformed\nfeature representations extracted by the \ufb01rst layer of a GAT model pre-trained on the Cora dataset\n(Figure 2). The representation exhibits discernible clustering in the projected 2D space. Note that\nthese clusters correspond to the seven labels of the dataset, verifying the model\u2019s discriminative\npower across the seven topic classes of Cora. Additionally, we visualize the relative strengths of\nthe normalized attention coef\ufb01cients (averaged across all eight attention heads). Properly interpret-\ning these coef\ufb01cients (as performed by e.g. Bahdanau et al. (2015)) will require further domain\nknowledge about the dataset under study, and is left for future work.\n4\nCONCLUSIONS\nWe have presented graph attention networks (GATs), novel convolution-style neural networks that\noperate on graph-structured data, leveraging masked self-attentional layers. The graph attentional\nlayer utilized throughout these networks is computationally ef\ufb01cient (does not require costly ma-\ntrix operations, and is parallelizable across all nodes in the graph), allows for (implicitly) assign-\ning different importances to different nodes within a neighborhood while dealing with different\nsized neighborhoods, and does not depend on knowing the entire graph structure upfront\u2014thus\naddressing many of the theoretical issues with previous spectral-based approaches. Our models\nleveraging attention have successfully achieved or matched state-of-the-art performance across four\nwell-established node classi\ufb01cation benchmarks, both transductive and inductive (especially, with\ncompletely unseen graphs used for testing).\nThere are several potential improvements and extensions to graph attention networks that could be\naddressed as future work, such as overcoming the practical problems described in subsection 2.2 to\nbe able to handle larger batch sizes. A particularly interesting research direction would be taking\nadvantage of the attention mechanism to perform a thorough analysis on the model interpretability.\nMoreover, extending the method to perform graph classi\ufb01cation instead of node classi\ufb01cation would\nalso be relevant from the application perspective. Finally, extending the model to incorporate edge\nfeatures (possibly indicating relationship among nodes) would allow us to tackle a larger variety of\nproblems.\nFigure 2: A t-SNE plot of the computed feature representations of a pre-trained GAT model\u2019s\n\ufb01rst hidden layer on the Cora dataset. Node colors denote classes. Edge thickness indicates ag-\ngregated normalized attention coef\ufb01cients between nodes i and j, across all eight attention heads\n(PK\nk=1 \u03b1k\nij + \u03b1k\nji).\n9\nPublished as a conference paper at ICLR 2018\nACKNOWLEDGEMENTS\nThe authors would like to thank the developers of TensorFlow (Abadi et al., 2015). PV and PL have\nreceived funding from the European Union\u2019s Horizon 2020 research and innovation programme\nPROPAG-AGEING under grant agreement No 634821. We further acknowledge the support of the\nfollowing agencies for research funding and computing support: CIFAR, Canada Research Chairs,\nCompute Canada and Calcul Qu\u00b4ebec, as well as NVIDIA for the generous GPU support. Special\nthanks to: Benjamin Day and Fabian Jansen for kindly pointing out issues in a previous iteration of\nthe paper; Micha\u0142 Dro\u02d9zd\u02d9zal for useful discussions, feedback and support; and Ga\u00b4etan Marceau for\nreviewing the paper prior to submission.\nREFERENCES\nMart\u00b4\u0131n Abadi, Ashish Agarwal, Paul Barham, Eugene Brevdo, Zhifeng Chen, Craig Citro, Greg S.\nCorrado, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Ian Goodfellow, Andrew\nHarp, Geoffrey Irving, Michael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz Kaiser, Manjunath\nKudlur, Josh Levenberg, Dan Man\u00b4e, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah,\nMike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker, Vin-\ncent Vanhoucke, Vijay Vasudevan, Fernanda Vi\u00b4egas, Oriol Vinyals, Pete Warden, Martin Watten-\nberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-scale machine learning\non heterogeneous systems, 2015. URL https://www.tensorflow.org/. Software avail-\nable from tensor\ufb02ow.org.\nJames Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in Neural\nInformation Processing Systems, pp. 1993\u20132001, 2016.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. International Conference on Learning Representations (ICLR),\n2015.\nMikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-\nwork for learning from labeled and unlabeled examples. Journal of machine learning research, 7\n(Nov):2399\u20132434, 2006.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. International Conference on Learning Representations (ICLR),\n2014.\nJianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine\nreading. arXiv preprint arXiv:1601.06733, 2016.\nKyunghyun Cho, Bart Van Merri\u00a8enboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Hol-\nger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder\nfor statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\nDjork-Arn\u00b4e Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network\nlearning by exponential linear units (elus). International Conference on Learning Representations\n(ICLR), 2016.\nMicha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst.\nConvolutional neural networks\non graphs with fast localized spectral \ufb01ltering. In Advances in Neural Information Processing\nSystems, pp. 3844\u20133852, 2016.\nMisha Denil, Sergio G\u00b4omez Colmenarejo, Serkan Cabi, David Saxton, and Nando de Freitas. Pro-\ngrammable agents. arXiv preprint arXiv:1706.06383, 2017.\nYan Duan, Marcin Andrychowicz, Bradly Stadie, Jonathan Ho, Jonas Schneider, Ilya Sutskever,\nPieter Abbeel, and Wojciech Zaremba.\nOne-shot imitation learning.\narXiv preprint\narXiv:1703.07326, 2017.\nDavid K Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00b4an\nAspuru-Guzik, and Ryan P Adams. Convolutional networks on graphs for learning molecular\n\ufb01ngerprints. In Advances in neural information processing systems, pp. 2224\u20132232, 2015.\n10\nPublished as a conference paper at ICLR 2018\nPaolo Frasconi, Marco Gori, and Alessandro Sperduti. A general framework for adaptive processing\nof data structures. IEEE transactions on Neural Networks, 9(5):768\u2013786, 1998.\nJonas Gehring, Michael Auli, David Grangier, and Yann N. Dauphin. A convolutional encoder\nmodel for neural machine translation. CoRR, abs/1611.02344, 2016. URL http://arxiv.\norg/abs/1611.02344.\nXavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence\nand Statistics, pp. 249\u2013256, 2010.\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.\nIn IEEE International Joint Conference on Neural Networks, pp. 729734, 2005.\nWilliam L Hamilton, Rex Ying, and Jure Leskovec.\nInductive representation learning on large\ngraphs. Neural Information Processing Systems (NIPS), 2017.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nMikael Henaff, Joan Bruna, and Yann LeCun. Deep convolutional networks on graph-structured\ndata. arXiv preprint arXiv:1506.05163, 2015.\nSepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735\u20131780, 1997.\nYedid\nHoshen.\nVain:\nAttentional\nmulti-agent\npredictive\nmodeling.\nIn\nI.\nGuyon,\nU. V. Luxburg,\nS. Bengio,\nH. Wallach,\nR. Fergus,\nS. Vishwanathan,\nand R. Gar-\nnett\n(eds.),\nAdvances\nin\nNeural\nInformation\nProcessing\nSystems\n30,\npp.\n2698\u2013\n2708. Curran Associates,\nInc.,\n2017.\nURL http://papers.nips.cc/paper/\n6863-vain-attentional-multi-agent-predictive-modeling.pdf.\nSimon J\u00b4egou, Michal Drozdzal, David V\u00b4azquez, Adriana Romero, and Yoshua Bengio. The one\nhundred layers tiramisu: Fully convolutional densenets for semantic segmentation. In Workshop\non Computer Vision in Vehicle Technology CVPRW, 2017.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nThomas N Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional net-\nworks. International Conference on Learning Representations (ICLR), 2017.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\nnetworks. International Conference on Learning Representations (ICLR), 2016.\nZhouhan Lin, Minwei Feng, Cicero Nogueira dos Santos, Mo Yu, Bing Xiang, Bowen Zhou,\nand Yoshua Bengio.\nA structured self-attentive sentence embedding.\narXiv preprint\narXiv:1703.03130, 2017.\nQing Lu and Lise Getoor.\nLink-based classi\ufb01cation.\nIn Proceedings of the 20th International\nConference on Machine Learning (ICML-03), pp. 496\u2013503, 2003.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\nFederico Monti, Davide Boscaini, Jonathan Masci, Emanuele Rodol`a, Jan Svoboda, and Michael M\nBronstein. Geometric deep learning on graphs and manifolds using mixture model cnns. arXiv\npreprint arXiv:1611.08402, 2016.\nMathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-\nworks for graphs. In Proceedings of The 33rd International Conference on Machine Learning,\nvolume 48, pp. 2014\u20132023, 2016.\n11\nPublished as a conference paper at ICLR 2018\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\nDeepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pp. 701\u2013710. ACM, 2014.\nSam T. Roweis and Lawrence K. Saul. Nonlinear dimensionality reduction by locally linear embed-\nding. Science, 290:2323\u20132326, 2000.\nAdam Santoro, David Raposo, David GT Barrett, Mateusz Malinowski, Razvan Pascanu, Peter\nBattaglia, and Timothy Lillicrap. A simple neural network module for relational reasoning. arXiv\npreprint arXiv:1706.01427, 2017.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\nThe graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008.\nA. Sperduti and A. Starita. Supervised neural networks for the classi\ufb01cation of structures. Trans.\nNeur. Netw., 8(3):714\u2013735, May 1997. ISSN 1045-9227. doi: 10.1109/72.572108.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. Journal of machine learning\nresearch, 15(1):1929\u20131958, 2014.\nAravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert,\nMichael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al.\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expres-\nsion pro\ufb01les. Proceedings of the National Academy of Sciences, 102(43):15545\u201315550, 2005.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762,\n2017.\nJason Weston, Fr\u00b4ed\u00b4eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-\nsupervised embedding. In Neural Networks: Tricks of the Trade, pp. 639\u2013655. Springer, 2012.\nJason Weston, Sumit Chopra, and Antoine Bordes. Memory networks. CoRR, abs/1410.3916, 2014.\nURL http://arxiv.org/abs/1410.3916.\nZhilin Yang, William Cohen, and Ruslan Salakhudinov. Revisiting semi-supervised learning with\ngraph embeddings. In International Conference on Machine Learning, pp. 40\u201348, 2016.\nXiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian\n\ufb01elds and harmonic functions. In Proceedings of the 20th International conference on Machine\nlearning (ICML-03), pp. 912\u2013919, 2003.\nMarinka Zitnik and Jure Leskovec.\nPredicting multicellular function through multi-layer tissue\nnetworks. Bioinformatics, 33(14):i190\u2013i198, 2017.\n12\n",
    "1809.10341": "Published as a conference paper at ICLR 2019\nDEEP GRAPH INFOMAX\nPetar Veli\u02c7ckovi\u00b4c\u2217\nDepartment of Computer Science and Technology\nUniversity of Cambridge\npetar.velickovic@cst.cam.ac.uk\nWilliam Fedus\nMila \u2013 Qu\u00b4ebec Arti\ufb01cial Intelligence Institute\nGoogle Brain\nliamfedus@google.com\nWilliam L. Hamilton\nMila \u2013 Qu\u00b4ebec Arti\ufb01cial Intelligence Institute\nMcGill University\nwlh@cs.mcgill.ca\nPietro Li`o\nDepartment of Computer Science and Technology\nUniversity of Cambridge\npietro.lio@cst.cam.ac.uk\nYoshua Bengio\u2020\nMila \u2013 Qu\u00b4ebec Arti\ufb01cial Intelligence Institute\nUniversit\u00b4e de Montr\u00b4eal\nyoshua.bengio@mila.quebec\nR Devon Hjelm\nMicrosoft Research\nMila \u2013 Qu\u00b4ebec Arti\ufb01cial Intelligence Institute\ndevon.hjelm@microsoft.com\nABSTRACT\nWe present Deep Graph Infomax (DGI), a general approach for learning node\nrepresentations within graph-structured data in an unsupervised manner. DGI re-\nlies on maximizing mutual information between patch representations and corre-\nsponding high-level summaries of graphs\u2014both derived using established graph\nconvolutional network architectures. The learnt patch representations summarize\nsubgraphs centered around nodes of interest, and can thus be reused for down-\nstream node-wise learning tasks. In contrast to most prior approaches to unsuper-\nvised learning with GCNs, DGI does not rely on random walk objectives, and is\nreadily applicable to both transductive and inductive learning setups. We demon-\nstrate competitive performance on a variety of node classi\ufb01cation benchmarks,\nwhich at times even exceeds the performance of supervised learning.\n1\nINTRODUCTION\nGeneralizing neural networks to graph-structured inputs is one of the current major challenges of\nmachine learning (Bronstein et al., 2017; Hamilton et al., 2017b; Battaglia et al., 2018). While\nsigni\ufb01cant strides have recently been made, notably with graph convolutional networks (Kipf &\nWelling, 2016a; Gilmer et al., 2017; Veli\u02c7ckovi\u00b4c et al., 2018), most successful methods use supervised\nlearning, which is often not possible as most graph data in the wild is unlabeled. In addition, it\nis often desirable to discover novel or interesting structure from large-scale graphs, and as such,\nunsupervised graph learning is essential for many important tasks.\nCurrently, the dominant algorithms for unsupervised representation learning with graph-structured\ndata rely on random walk-based objectives (Grover & Leskovec, 2016; Perozzi et al., 2014; Tang\net al., 2015; Hamilton et al., 2017a), sometimes further simpli\ufb01ed to reconstruct adjacency informa-\ntion (Kipf & Welling, 2016b; Duran & Niepert, 2017). The underlying intuition is to train an encoder\nnetwork so that nodes that are \u201cclose\u201d in the input graph are also \u201cclose\u201d in the representation space.\nWhile powerful\u2014and related to traditional metrics such as the personalized PageRank score (Jeh\n& Widom, 2003)\u2014random walk methods suffer from known limitations. Most prominently, the\nrandom-walk objective is known to over-emphasize proximity information at the expense of struc-\ntural information (Ribeiro et al., 2017), and performance is highly dependent on hyperparameter\nchoice (Grover & Leskovec, 2016; Perozzi et al., 2014). Moreover, with the introduction of stronger\n\u2217Work performed while the author was at Mila.\n\u2020CIFAR Fellow\n1\narXiv:1809.10341v2  [stat.ML]  21 Dec 2018\nPublished as a conference paper at ICLR 2019\nencoder models based on graph convolutions (Gilmer et al., 2017), it is unclear whether random-\nwalk objectives actually provide any useful signal, as these encoders already enforce an inductive\nbias that neighboring nodes have similar representations.\nIn this work, we propose an alternative objective for unsupervised graph learning that is based upon\nmutual information, rather than random walks. Recently, scalable estimation of mutual information\nwas made both possible and practical through Mutual Information Neural Estimation (MINE, Bel-\nghazi et al., 2018), which relies on training a statistics network as a classi\ufb01er of samples coming\nfrom the joint distribution of two random variables and their product of marginals. Following on\nMINE, Hjelm et al. (2018) introduced Deep InfoMax (DIM) for learning representations of high-\ndimensional data. DIM trains an encoder model to maximize the mutual information between a\nhigh-level \u201cglobal\u201d representation and \u201clocal\u201d parts of the input (such as patches of an image). This\nencourages the encoder to carry the type of information that is present in all locations (and thus are\nglobally relevant), such as would be the case of a class label.\nDIM relies heavily on convolutional neural network structure in the context of image data, and to our\nknowledge, no work has applied mutual information maximization to graph-structured inputs. Here,\nwe adapt ideas from DIM to the graph domain, which can be thought of as having a more general\ntype of structure than the ones captured by convolutional neural networks. In the following sections,\nwe introduce our method called Deep Graph Infomax (DGI). We demonstrate that the representation\nlearned by DGI is consistently competitive on both transductive and inductive classi\ufb01cation tasks,\noften outperforming both supervised and unsupervised strong baselines in our experiments.\n2\nRELATED WORK\nContrastive methods. An important approach for unsupervised learning of representations is to\ntrain an encoder to be contrastive between representations that capture statistical dependencies of\ninterest and those that do not. For example, a contrastive approach may employ a scoring function,\ntraining the encoder to increase the score on \u201creal\u201d input (a.k.a, positive examples) and decrease the\nscore on \u201cfake\u201d input (a.k.a., negative samples). Contrastive methods are central to many popular\nword-embedding methods (Collobert & Weston, 2008; Mnih & Kavukcuoglu, 2013; Mikolov et al.,\n2013), but they are found in many unsupervised algorithms for learning representations of graph-\nstructured input as well. There are many ways to score a representation, but in the graph literature\nthe most common techniques use classi\ufb01cation (Perozzi et al., 2014; Grover & Leskovec, 2016;\nKipf & Welling, 2016b; Hamilton et al., 2017b), though other scoring functions are used (Duran\n& Niepert, 2017; Bojchevski & G\u00a8unnemann, 2018). DGI is also contrastive in this respect, as our\nobjective is based on classifying local-global pairs and negative-sampled counterparts.\nSampling strategies. A key implementation detail to contrastive methods is how to draw posi-\ntive and negative samples. The prior work above on unsupervised graph representation learning\nrelies on a local contrastive loss (enforcing proximal nodes to have similar embeddings). Positive\nsamples typically correspond to pairs of nodes that appear together within short random walks in\nthe graph\u2014from a language modelling perspective, effectively treating nodes as words and random\nwalks as sentences. Recent work by Bojchevski & G\u00a8unnemann (2018) uses node-anchored sam-\npling as an alternative. The negative sampling for these methods is primarily based on sampling\nof random pairs, with recent work adapting this approach to use a curriculum-based negative sam-\npling scheme (with progressively \u201ccloser\u201d negative examples; Ying et al., 2018a) or introducing an\nadversary to select the negative examples (Bose et al., 2018).\nPredictive coding. Contrastive predictive coding (CPC, Oord et al., 2018) is another method for\nlearning deep representations based on mutual information maximization. Like the models above,\nCPC is also contrastive, in this case using an estimate of the conditional density (in the form of noise\ncontrastive estimation, Gutmann & Hyv\u00a8arinen, 2010) as the scoring function. However, unlike our\napproach, CPC and the graph methods above are all predictive: the contrastive objective effectively\ntrains a predictor between structurally-speci\ufb01ed parts of the input (e.g., between neighboring node\npairs or between a node and its neighborhood). Our approach differs in that we contrast global /\nlocal parts of a graph simultaneously, where the global variable is computed from all local variables.\nTo the best of our knowledge, the sole prior works that instead focuses on contrasting \u201cglobal\u201d\nand \u201clocal\u201d representations on graphs do so via (auto-)encoding objectives on the adjacency matrix\n2\nPublished as a conference paper at ICLR 2019\n(Wang et al., 2016) and incorporation of community-level constraints into node embeddings (Wang\net al., 2017). Both methods rely on matrix factorization-style losses and are thus not scalable to\nlarger graphs.\n3\nDGI METHODOLOGY\nIn this section, we will present the Deep Graph Infomax method in a top-down fashion: starting\nwith an abstract overview of our speci\ufb01c unsupervised learning setup, followed by an exposition of\nthe objective function optimized by our method, and concluding by enumerating all the steps of our\nprocedure in a single-graph setting.\n3.1\nGRAPH-BASED UNSUPERVISED LEARNING\nWe assume a generic graph-based unsupervised machine learning setup: we are provided with a set\nof node features, X = {\u20d7x1, \u20d7x2, . . . , \u20d7xN}, where N is the number of nodes in the graph and \u20d7xi \u2208RF\nrepresents the features of node i. We are also provided with relational information between these\nnodes in the form of an adjacency matrix, A \u2208RN\u00d7N. While A may consist of arbitrary real\nnumbers (or even arbitrary edge features), in all our experiments we will assume the graphs to be\nunweighted, i.e. Aij = 1 if there exists an edge i \u2192j in the graph and Aij = 0 otherwise.\nOur objective is to learn an encoder, E : RN\u00d7F \u00d7 RN\u00d7N \u2192RN\u00d7F \u2032, such that E(X, A) = H =\n{\u20d7h1,\u20d7h2, . . . ,\u20d7hN} represents high-level representations \u20d7hi \u2208RF \u2032 for each node i. These represen-\ntations may then be retrieved and used for downstream tasks, such as node classi\ufb01cation.\nHere we will focus on graph convolutional encoders\u2014a \ufb02exible class of node embedding architec-\ntures, which generate node representations by repeated aggregation over local node neighborhoods\n(Gilmer et al., 2017). A key consequence is that the produced node embeddings, \u20d7hi, summarize a\npatch of the graph centered around node i rather than just the node itself. In what follows, we will\noften refer to \u20d7hi as patch representations to emphasize this point.\n3.2\nLOCAL-GLOBAL MUTUAL INFORMATION MAXIMIZATION\nOur approach to learning the encoder relies on maximizing local mutual information\u2014that is, we\nseek to obtain node (i.e., local) representations that capture the global information content of the\nentire graph, represented by a summary vector, \u20d7s.\nIn order to obtain the graph-level summary vectors, \u20d7s, we leverage a readout function, R : RN\u00d7F \u2192\nRF , and use it to summarize the obtained patch representations into a graph-level representation;\ni.e., \u20d7s = R(E(X, A)).\nAs a proxy for maximizing the local mutual information, we employ a discriminator, D : RF \u00d7\nRF \u2192R, such that D(\u20d7hi,\u20d7s) represents the probability scores assigned to this patch-summary pair\n(should be higher for patches contained within the summary).\nNegative samples for D are provided by pairing the summary \u20d7s from (X, A) with patch represen-\ntations \u20d7ehj of an alternative graph, ( eX, eA). In a multi-graph setting, such graphs may be obtained\nas other elements of a training set. However, for a single graph, an explicit (stochastic) corruption\nfunction, C : RN\u00d7F \u00d7 RN\u00d7N \u2192RM\u00d7F \u00d7 RM\u00d7M is required to obtain a negative example from\nthe original graph, i.e. ( eX, eA) = C(X, A). The choice of the negative sampling procedure will\ngovern the speci\ufb01c kinds of structural information that is desirable to be captured as a byproduct of\nthis maximization.\nFor the objective, we follow the intuitions from Deep InfoMax (DIM, Hjelm et al., 2018) and use a\nnoise-contrastive type objective with a standard binary cross-entropy (BCE) loss between the sam-\nples from the joint (positive examples) and the product of marginals (negative examples). Following\n3\nPublished as a conference paper at ICLR 2019\ntheir work, we use the following objective1:\nL =\n1\nN + M\n\uf8eb\n\uf8ed\nN\nX\ni=1\nE(X,A)\nh\nlog D\n\u0010\n\u20d7hi,\u20d7s\n\u0011i\n+\nM\nX\nj=1\nE( e\nX, e\nA)\n\u0014\nlog\n\u0012\n1 \u2212D\n\u0012\n\u20d7ehj,\u20d7s\n\u0013\u0013\u0015\uf8f6\n\uf8f8\n(1)\nThis approach effectively maximizes mutual information between \u20d7hi and \u20d7s, based on the Jensen-\nShannon divergence2 between the joint and the product of marginals.\nAs all of the derived patch representations are driven to preserve mutual information with the global\ngraph summary, this allows for discovering and preserving similarities on the patch-level\u2014for ex-\nample, distant nodes with similar structural roles (which are known to be a strong predictor for\nmany node classi\ufb01cation tasks; Donnat et al., 2018). Note that this is a \u201creversed\u201d version of the\nargument given by Hjelm et al. (2018): for node classi\ufb01cation, our aim is for the patches to establish\nlinks to similar patches across the graph, rather than enforcing the summary to contain all of these\nsimilarities (however, both of these effects should in principle occur simultaneously).\n3.3\nTHEORETICAL MOTIVATION\nWe now provide some intuition that connects the classi\ufb01cation error of our discriminator to mutual\ninformation maximization on graph representations.\nLemma 1. Let {X(k)}|X|\nk=1 be a set of node representations drawn from an empirical probability dis-\ntribution of graphs, p(X), with \ufb01nite number of elements, |X|, such that p(X(k)) = p(X(k\u2032)) \u2200k, k\u2032.\nLet R(\u00b7) be a deterministic readout function on graphs and \u20d7s(k) = R(X(k)) be the summary vector\nof the k-th graph, with marginal distribution p(\u20d7s). The optimal classi\ufb01er between the joint distri-\nbution p(X,\u20d7s) and the product of marginals p(X)p(\u20d7s), assuming class balance, has an error rate\nupper bounded by Err\u2217= 1\n2\nP|X|\nk=1 p(\u20d7s(k))2. This upper bound is achieved if R is injective.\nProof. Denote by Q(k) the set of all graphs in the input set that are mapped to \u20d7s(k) by R, i.e.\nQ(k) = {X(j) | R(X(j)) = \u20d7s(k)}. As R(\u00b7) is deterministic, samples from the joint, (X(k),\u20d7s(k)) are\ndrawn from the product of marginals with probability p(\u20d7s(k))p(X(k)), which decomposes into:\np(\u20d7s(k))\nX\n\u20d7s\np(X(k),\u20d7s) = p(\u20d7s(k))p(X(k)|\u20d7s(k))p(\u20d7s(k)) =\np(X(k))\nP\nX\u2032\u2208Q(k) p(X\u2032)p(\u20d7s(k))2\n(2)\nFor convenience, let \u03c1(k) =\np(X(k))\nP\nX\u2032\u2208Q(k) p(X\u2032). As, by de\ufb01nition, X(k) \u2208Q(k), it holds that \u03c1(k) \u22641.\nThis probability ratio is maximized at 1 when Q(k) = {X(k)}, i.e. when R is injective for X(k).\nThe probability of drawing any sample of the joint from the product of marginals is then bounded\nabove by P|X|\nk=1 p(\u20d7s(k))2. As the probability of drawing (X(k),\u20d7s(k)) from the joint is \u03c1(k)p(\u20d7s(k)) \u2265\n\u03c1(k)p(\u20d7s(k))2, we know that classifying these samples as coming from the joint has a lower error\nthan classifying them as coming from the product of marginals. The error rate of such a classi\ufb01er is\nthen the probability of drawing a sample from the joint as a sample from product of marginals under\nthe mixture probability, which we can bound by Err \u2264\n1\n2\nP|X|\nk=1 p(\u20d7s(k))2, with the upper bound\nachieved, as above, when R(\u00b7) is injective for all elements of {X(k)}.\nIt may be useful to note that\n1\n2|X| \u2264Err\u2217\u22641\n2. The \ufb01rst result is obtained via a trivial application\nof Jensen\u2019s inequality, while the other extreme is reached only in the edge case of a constant readout\nfunction (when every example from the joint is also an example from the product of marginals, so\nno classi\ufb01er performs better than chance).\nCorollary 1. From now on, assume that the readout function used, R, is injective. Assume the\nnumber of allowable states in the space of \u20d7s, |\u20d7s|, is greater than or equal to |X|. Then, for \u20d7s\u22c6, the\n1Note that Hjelm et al. (2018) use a softplus version of the binary cross-entropy.\n2The \u201cGAN\u201d distance de\ufb01ned here\u2014as per Goodfellow et al. (2014) and Nowozin et al. (2016)\u2014and\nJensen-Shannon divergence can be related by DGAN = 2DJS \u2212log 4. Therefore, any parameters that op-\ntimize one also optimize the other.\n4\nPublished as a conference paper at ICLR 2019\noptimal summary under the classi\ufb01cation error of an optimal classi\ufb01er between the joint and the\nproduct of marginals, it holds that |\u20d7s\u22c6| = |X|.\nProof. By injectivity of R, we know that \u20d7s\u22c6= argmin\u20d7s Err\u2217. As the upper error bound, Err\u2217,\nis a simple geometric sum, we know that this is minimized when p(\u20d7s(k)) is uniform. As R(\u00b7) is\ndeterministic, this implies that each potential summary state would need to be used at least once.\nCombined with the condition |\u20d7s| \u2265|X|, we conclude that the optimum has |\u20d7s\u22c6| = |X|.\nTheorem 1. \u20d7s\u22c6= argmax\u20d7s MI(X;\u20d7s), where MI is mutual information.\nProof. This follows from the fact that the mutual information is invariant under invertible trans-\nforms. As |\u20d7s\u22c6| = |X| and R is injective, it has an inverse function, R\u22121. It follows then that, for\nany \u20d7s, MI(X;\u20d7s) \u2264H(X) = MI(X; X) = MI(X; R(X)) = MI(X;\u20d7s\u22c6), where H is entropy.\nTheorem 1 shows that for \ufb01nite input sets and suitable deterministic functions, minimizing the clas-\nsi\ufb01cation error in the discriminator can be used to maximize the mutual information between the\ninput and output. However, as was shown in Hjelm et al. (2018), this objective alone is not enough\nto learn useful representations. As in their work, we discriminate between the global summary\nvector and local high-level representations.\nTheorem 2. Let X(k)\ni\n= {\u20d7xj}j\u2208n(X(k),i) be the neighborhood of the node i in the k-th graph that\ncollectively maps to its high-level features, \u20d7hi = E(X(k)\ni\n), where n is the neighborhood function that\nreturns the set of neighborhood indices of node i for graph X(k), and E is a deterministic encoder\nfunction. Let us assume that |Xi| = |X| = |\u20d7s| \u2265|\u20d7hi|. Then, the \u20d7hi that minimizes the classi\ufb01cation\nerror between p(\u20d7hi,\u20d7s) and p(\u20d7hi)p(\u20d7s) also maximizes MI(X(k)\ni\n;\u20d7hi).\nProof. Given our assumption of |Xi| = |\u20d7s|, there exists an inverse Xi = R\u22121(\u20d7s), and therefore\n\u20d7hi = E(R\u22121(\u20d7s)), i.e. there exists a deterministic function (E \u25e6R\u22121) mapping \u20d7s to \u20d7hi. The optimal\nclassi\ufb01er between the joint p(\u20d7hi,\u20d7s) and the product of marginals p(\u20d7hi)p(\u20d7s) then has (by Lemma 1)\nan error rate upper bound of Err\u2217= 1\n2\nP|X|\nk=1 p(\u20d7h(k)\ni\n)2. Therefore (as in Corollary 1), for the optimal\n\u20d7hi, |\u20d7hi| = |Xi|, which by the same arguments as in Theorem 1 maximizes the mutual information\nbetween the neighborhood and high-level features, MI(X(k)\ni\n;\u20d7hi).\nThis motivates our use of a classi\ufb01er between samples from the joint and the product of marginals,\nand using the binary cross-entropy (BCE) loss to optimize this classi\ufb01er is well-understood in the\ncontext of neural network optimization.\n3.4\nOVERVIEW OF DGI\nAssuming the single-graph setup (i.e., (X, A) provided as input), we will now summarize the steps\nof the Deep Graph Infomax procedure:\n1. Sample a negative example by using the corruption function: ( eX, eA) \u223cC(X, A).\n2. Obtain patch representations, \u20d7hi for the input graph by passing it through the encoder:\nH = E(X, A) = {\u20d7h1,\u20d7h2, . . . ,\u20d7hN}.\n3. Obtain patch representations, \u20d7ehj for the negative example by passing it through the encoder:\neH = E( eX, eA) = {\u20d7eh1,\u20d7eh2, . . . ,\u20d7ehM}.\n4. Summarize the input graph by passing its patch representations through the readout func-\ntion: \u20d7s = R(H).\n5. Update parameters of E, R and D by applying gradient descent to maximize Equation 1.\nThis algorithm is fully summarized by Figure 1.\n5\nPublished as a conference paper at ICLR 2019\n\u20d7xi\n\u20d7exj\n(X, A)\n( eX, eA)\n\u20d7hi\n\u20d7ehj\n(H, A)\n( eH, eA)\nE\nC\nE\n\u20d7s\nR\nD\nD\n+\n\u2212\nFigure 1: A high-level overview of Deep Graph Infomax. Refer to Section 3.4 for more details.\nTable 1: Summary of the datasets used in our experiments.\nDataset\nTask\nNodes\nEdges\nFeatures\nClasses\nTrain/Val/Test Nodes\nCora\nTransductive\n2,708\n5,429\n1,433\n7\n140/500/1,000\nCiteseer\nTransductive\n3,327\n4,732\n3,703\n6\n120/500/1,000\nPubmed\nTransductive\n19,717\n44,338\n500\n3\n60/500/1,000\nReddit\nInductive\n231,443\n11,606,919\n602\n41\n151,708/23,699/55,334\nPPI\nInductive\n56,944\n818,716\n50\n121\n44,906/6,514/5,524\n(24 graphs)\n(multilbl.)\n(20/2/2 graphs)\n4\nCLASSIFICATION PERFORMANCE\nWe have assessed the bene\ufb01ts of the representation learnt by the DGI encoder on a variety of node\nclassi\ufb01cation tasks (transductive as well as inductive), obtaining competitive results. In each case,\nDGI was used to learn patch representations in a fully unsupervised manner, followed by evaluating\nthe node-level classi\ufb01cation utility of these representations. This was performed by directly using\nthese representations to train and test a simple linear (logistic regression) classi\ufb01er.\n4.1\nDATASETS\nWe follow the experimental setup described in Kipf & Welling (2016a) and Hamilton et al. (2017a)\non the following benchmark tasks: (1) classifying research papers into topics on the Cora, Cite-\nseer and Pubmed citation networks (Sen et al., 2008); (2) predicting the community structure of a\nsocial network modeled with Reddit posts; and (3) classifying protein roles within protein-protein\ninteraction (PPI) networks (Zitnik & Leskovec, 2017), requiring generalisation to unseen networks.\nFurther information on the datasets may be found in Table 1 and Appendix A.\n4.2\nEXPERIMENTAL SETUP\nFor each of three experimental settings (transductive learning, inductive learning on large graphs,\nand multiple graphs), we employed distinct encoders and corruption functions appropriate to that\nsetting (described below).\nTransductive learning. For the transductive learning tasks (Cora, Citeseer and Pubmed), our en-\ncoder is a one-layer Graph Convolutional Network (GCN) model (Kipf & Welling, 2016a), with the\nfollowing propagation rule:\nE(X, A) = \u03c3\n\u0010\n\u02c6D\u22121\n2 \u02c6A \u02c6D\u22121\n2 X\u0398\n\u0011\n(3)\nwhere \u02c6A = A + IN is the adjacency matrix with inserted self-loops and \u02c6D is its corresponding\ndegree matrix; i.e. \u02c6Dii = P\nj \u02c6Aij. For the nonlinearity, \u03c3, we have applied the parametric ReLU\n6\nPublished as a conference paper at ICLR 2019\n\u20d7h1\n\u20d7h2\n\u20d7h3\n\u20d7s\nFigure 2: The DGI setup on large graphs (such as Reddit). Summary vectors, \u20d7s, are obtained by\ncombining several subsampled patch representations, \u20d7hi (here obtained by sampling three and two\nneighbors in the \ufb01rst and second level, respectively).\n(PReLU) function (He et al., 2015), and \u0398 \u2208RF \u00d7F \u2032 is a learnable linear transformation applied\nto every node, with F \u2032 = 512 features being computed (specially, F \u2032 = 256 on Pubmed due to\nmemory limitations).\nThe corruption function used in this setting is designed to encourage the representations to prop-\nerly encode structural similarities of different nodes in the graph; for this purpose, C preserves the\noriginal adjacency matrix ( eA = A), whereas the corrupted features, eX, are obtained by row-wise\nshuf\ufb02ing of X. That is, the corrupted graph consists of exactly the same nodes as the original graph,\nbut they are located in different places in the graph, and will therefore receive different patch repre-\nsentations. We demonstrate DGI is stable to other choices of corruption functions in Appendix C,\nbut we \ufb01nd those that preserve the graph structure result in the strongest features.\nInductive learning on large graphs. For inductive learning, we may no longer use the GCN update\nrule in our encoder (as the learned \ufb01lters rely on a \ufb01xed and known adjacency matrix); instead, we\napply the mean-pooling propagation rule, as used by GraphSAGE-GCN (Hamilton et al., 2017a):\nMP(X, A) = \u02c6D\u22121 \u02c6AX\u0398\n(4)\nwith parameters de\ufb01ned as in Equation 3. Note that multiplying by \u02c6D\u22121 actually performs a normal-\nized sum (hence the mean-pooling). While Equation 4 explicitly speci\ufb01es the adjacency and degree\nmatrices, they are not needed: identical inductive behaviour may be observed by a constant attention\nmechanism across the node\u2019s neighbors, as used by the Const-GAT model (Veli\u02c7ckovi\u00b4c et al., 2018).\nFor Reddit, our encoder is a three-layer mean-pooling model with skip connections (He et al., 2016):\ng\nMP(X, A) = \u03c3 (X\u0398\u2032\u2225MP(X, A))\nE(X, A) = g\nMP3(g\nMP2(g\nMP1(X, A), A), A)\n(5)\nwhere \u2225is featurewise concatenation (i.e. the central node and its neighborhood are handled sepa-\nrately). We compute F \u2032 = 512 features in each MP layer, with the PReLU activation for \u03c3.\nGiven the large scale of the dataset, it will not \ufb01t into GPU memory entirely. Therefore, we use\nthe subsampling approach of Hamilton et al. (2017a), where a minibatch of nodes is \ufb01rst selected,\nand then a subgraph centered around each of them is obtained by sampling node neighborhoods\nwith replacement. Speci\ufb01cally, we sample 10, 10 and 25 neighbors at the \ufb01rst, second and third\nlevel, respectively\u2014thus, each subsampled patch has 1 + 10 + 100 + 2500 = 2611 nodes. Only the\ncomputations necessary for deriving the central node i\u2019s patch representation, \u20d7hi, are performed.\nThese representations are then used to derive the summary vector, \u20d7s, for the minibatch (Figure 2).\nWe used minibatches of 256 nodes throughout training.\nTo de\ufb01ne our corruption function in this setting, we use a similar approach as in the transductive\ntasks, but treat each subsampled patch as a separate graph to be corrupted (i.e., we row-wise shuf\ufb02e\n7\nPublished as a conference paper at ICLR 2019\nthe feature matrices within a subsampled patch). Note that this may very likely cause the central\nnode\u2019s features to be swapped out for a sampled neighbor\u2019s features, further encouraging diversity\nin the negative samples. The patch representation obtained in the central node is then submitted to\nthe discriminator.\nInductive learning on multiple graphs. For the PPI dataset, inspired by previous successful super-\nvised architectures (Veli\u02c7ckovi\u00b4c et al., 2018), our encoder is a three-layer mean-pooling model with\ndense skip connections (He et al., 2016; Huang et al., 2017):\nH1 = \u03c3 (MP1(X, A))\n(6)\nH2 = \u03c3 (MP2(H1 + XWskip, A))\n(7)\nE(X, A) = \u03c3 (MP3(H2 + H1 + XWskip, A))\n(8)\nwhere Wskip is a learnable projection matrix, and MP is as de\ufb01ned in Equation 4. We compute\nF \u2032 = 512 features in each MP layer, using the PReLU activation for \u03c3.\nIn this multiple-graph setting, we opted to use randomly sampled training graphs as negative exam-\nples (i.e., our corruption function simply samples a different graph from the training set). We found\nthis method to be the most stable, considering that over 40% of the nodes have all-zero features in\nthis dataset. To further expand the pool of negative examples, we also apply dropout (Srivastava\net al., 2014) to the input features of the sampled graph. We found it bene\ufb01cial to standardize the\nlearnt embeddings across the training set prior to providing them to the logistic regression model.\nReadout, discriminator, and additional training details. Across all three experimental settings,\nwe employed identical readout functions and discriminator architectures.\nFor the readout function, we use a simple averaging of all the nodes\u2019 features:\nR(H) = \u03c3\n \n1\nN\nN\nX\ni=1\n\u20d7hi\n!\n(9)\nwhere \u03c3 is the logistic sigmoid nonlinearity. While we have found this readout to perform the best\nacross all our experiments, we assume that its power will diminish with the increase in graph size,\nand in those cases, more sophisticated readout architectures such as set2vec (Vinyals et al., 2015) or\nDiffPool (Ying et al., 2018b) are likely to be more appropriate.\nThe discriminator scores summary-patch representation pairs by applying a simple bilinear scoring\nfunction (similar to the scoring used by Oord et al. (2018)):\nD(\u20d7hi,\u20d7s) = \u03c3\n\u0010\n\u20d7hT\ni W\u20d7s\n\u0011\n(10)\nHere, W is a learnable scoring matrix and \u03c3 is the logistic sigmoid nonlinearity, used to convert\nscores into probabilities of (\u20d7hi,\u20d7s) being a positive example.\nAll models are initialized using Glorot initialization (Glorot & Bengio, 2010) and trained to maxi-\nmize the mutual information provided in Equation 1 on the available nodes (all nodes for the trans-\nductive, and training nodes only in the inductive setup) using the Adam SGD optimizer (Kingma\n& Ba, 2014) with an initial learning rate of 0.001 (specially, 10\u22125 on Reddit). On the transduc-\ntive datasets, we use an early stopping strategy on the observed training loss, with a patience of 20\nepochs3. On the inductive datasets we train for a \ufb01xed number of epochs (150 on Reddit, 20 on PPI).\n4.3\nRESULTS\nThe results of our comparative evaluation experiments are summarized in Table 2.\nFor the transductive tasks, we report the mean classi\ufb01cation accuracy (with standard deviation) on\nthe test nodes of our method after 50 runs of training (followed by logistic regression), and reuse the\nmetrics already reported in Kipf & Welling (2016a) for the performance of DeepWalk and GCN, as\nwell as Label Propagation (LP) (Zhu et al., 2003) and Planetoid (Yang et al., 2016)\u2014a representative\nsupervised random walk method. Specially, we provide results for training the logistic regression\non raw input features, as well as DeepWalk with the input features concatenated.\n3 A reference DGI implementation may be found at https://github.com/PetarV-/DGI.\n8\nPublished as a conference paper at ICLR 2019\nTable 2: Summary of results in terms of classi\ufb01cation accuracies (on transductive tasks) or micro-\naveraged F1 scores (on inductive tasks). In the \ufb01rst column, we highlight the kind of data available\nto each method during training (X: features, A: adjacency matrix, Y: labels). \u201cGCN\u201d corresponds\nto a two-layer DGI encoder trained in a supervised manner.\nTransductive\nAvailable data\nMethod\nCora\nCiteseer\nPubmed\nX\nRaw features\n47.9 \u00b1 0.4%\n49.3 \u00b1 0.2%\n69.1 \u00b1 0.3%\nA, Y\nLP (Zhu et al., 2003)\n68.0%\n45.3%\n63.0%\nA\nDeepWalk (Perozzi et al., 2014)\n67.2%\n43.2%\n65.3%\nX, A\nDeepWalk + features\n70.7 \u00b1 0.6%\n51.4 \u00b1 0.5%\n74.3 \u00b1 0.9%\nX, A\nRandom-Init (ours)\n69.3 \u00b1 1.4%\n61.9 \u00b1 1.6%\n69.6 \u00b1 1.9%\nX, A\nDGI (ours)\n82.3 \u00b1 0.6%\n71.8 \u00b1 0.7%\n76.8 \u00b1 0.6%\nX, A, Y\nGCN (Kipf & Welling, 2016a)\n81.5%\n70.3%\n79.0%\nX, A, Y\nPlanetoid (Yang et al., 2016)\n75.7%\n64.7%\n77.2%\nInductive\nAvailable data\nMethod\nReddit\nPPI\nX\nRaw features\n0.585\n0.422\nA\nDeepWalk (Perozzi et al., 2014)\n0.324\n\u2014\nX, A\nDeepWalk + features\n0.691\n\u2014\nX, A\nGraphSAGE-GCN (Hamilton et al., 2017a)\n0.908\n0.465\nX, A\nGraphSAGE-mean (Hamilton et al., 2017a)\n0.897\n0.486\nX, A\nGraphSAGE-LSTM (Hamilton et al., 2017a)\n0.907\n0.482\nX, A\nGraphSAGE-pool (Hamilton et al., 2017a)\n0.892\n0.502\nX, A\nRandom-Init (ours)\n0.933 \u00b1 0.001\n0.626 \u00b1 0.002\nX, A\nDGI (ours)\n0.940 \u00b1 0.001\n0.638 \u00b1 0.002\nX, A, Y\nFastGCN (Chen et al., 2018)\n0.937\n\u2014\nX, A, Y\nAvg. pooling (Zhang et al., 2018)\n0.958 \u00b1 0.001\n0.969 \u00b1 0.002\nFor the inductive tasks, we report the micro-averaged F1 score on the (unseen) test nodes, aver-\naged after 50 runs of training, and reuse the metrics already reported in Hamilton et al. (2017a) for\nthe other techniques. Speci\ufb01cally, as our setup is unsupervised, we compare against the unsuper-\nvised GraphSAGE approaches. We also provide supervised results for two related architectures\u2014\nFastGCN (Chen et al., 2018) and Avg. pooling (Zhang et al., 2018).\nOur results demonstrate strong performance being achieved across all \ufb01ve datasets. We particularly\nnote that the DGI approach is competitive with the results reported for the GCN model with the\nsupervised loss, even exceeding its performance on the Cora and Citeseer datasets. We assume that\nthese bene\ufb01ts stem from the fact that, indirectly, the DGI approach allows for every node to have\naccess to structural properties of the entire graph, whereas the supervised GCN is limited to only\ntwo-layer neighborhoods (by the extreme sparsity of the training signal and the corresponding threat\nof over\ufb01tting). It should be noted that, while we are capable of outperforming equivalent supervised\nencoder architectures, our performance still does not surpass the current supervised transductive\nstate of the art (which is held by methods such as GraphSGAN (Ding et al., 2018)). We further ob-\nserve that the DGI method successfully outperformed all the competing unsupervised GraphSAGE\napproaches on the Reddit and PPI datasets\u2014thus verifying the potential of methods based on local\nmutual information maximization in the inductive node classi\ufb01cation domain. Our Reddit results are\ncompetitive with the supervised state of the art, whereas on PPI the gap is still large\u2014we believe this\ncan be attributed to the extreme sparsity of available node features (over 40% of the nodes having\nall-zero features), that our encoder heavily relies on.\nWe note that a randomly initialized graph convolutional network may already extract highly useful\nfeatures and represents a strong baseline\u2014a well-known fact, considering its links to the Weisfeiler-\n9\nPublished as a conference paper at ICLR 2019\nFigure 3: t-SNE embeddings of the nodes in the Cora dataset from the raw features (left), features\nfrom a randomly initialized DGI model (middle), and a learned DGI model (right). The clusters of\nthe learned DGI model\u2019s embeddings are clearly de\ufb01ned, with a Silhouette score of 0.234.\nLehman graph isomorphism test (Weisfeiler & Lehman, 1968), that have already been highlighted\nand analyzed by Kipf & Welling (2016a) and Hamilton et al. (2017a). As such, we also provide, as\nRandom-Init, the logistic regression performance on embeddings obtained from a randomly initial-\nized encoder. Besides demonstrating that DGI is able to further improve on this strong baseline, it\nparticularly reveals that, on the inductive datasets, previous random walk-based negative sampling\nmethods may have been ineffective for learning appropriate features for the classi\ufb01cation task.\nLastly, it should be noted that deeper encoders correspond to more pronounced mixing between\nrecovered patch representations, reducing the effective variability of our positive/negative examples\u2019\npool. We believe that this is the reason why shallower architectures performed better on some of the\ndatasets. While we cannot say that these trends will hold in general, with the DGI loss function we\ngenerally found bene\ufb01ts from employing wider, rather than deeper models.\n5\nQUALITATIVE ANALYSIS\nWe performed a diverse set of analyses on the embeddings learnt by the DGI algorithm in order to\nbetter understand the properties of DGI. We focus our analysis exclusively on the Cora dataset (as it\nhas the smallest number of nodes, signi\ufb01cantly aiding clarity).\nA standard set of \u201cevolving\u201d t-SNE plots (Maaten & Hinton, 2008) of the embeddings is given in\nFigure 3. As expected given the quantitative results, the learnt embeddings\u2019 2D projections ex-\nhibit discernible clustering in the 2D projected space (especially compared to the raw features and\nRandom-Init), which respects the seven topic classes of Cora. The projection obtains a Silhouette\nscore (Rousseeuw, 1987) of 0.234, which compares favorably with the previous reported score of\n0.158 for Embedding Propagation (Duran & Niepert, 2017).\nWe ran further analyses, revealing insights into DGI\u2019s mechanism of learning, isolating biased em-\nbedding dimensions for pushing the negative example scores down and using the remainder to en-\ncode useful information about positive examples. We leverage these insights to retain competitive\nperformance to the supervised GCN even after half the dimensions are removed from the patch rep-\nresentations provided by the encoder. These\u2014and several other\u2014qualitative and ablation studies\ncan be found in Appendix B.\n6\nCONCLUSIONS\nWe have presented Deep Graph Infomax (DGI), a new approach for learning unsupervised represen-\ntations on graph-structured data. By leveraging local mutual information maximization across the\ngraph\u2019s patch representations, obtained by powerful graph convolutional architectures, we are able\nto obtain node embeddings that are mindful of the global structural properties of the graph. This\nenables competitive performance across a variety of both transductive and inductive classi\ufb01cation\ntasks, at times even outperforming relevant supervised architectures.\n10\nPublished as a conference paper at ICLR 2019\nACKNOWLEDGMENTS\nWe would like to thank the developers of PyTorch (Paszke et al., 2017). PV and PL have received\nfunding from the European Union\u2019s Horizon 2020 research and innovation programme PROPAG-\nAGEING under grant agreement No 634821. We specially thank Hugo Larochelle and Jian Tang for\nthe extremely useful discussions, and Andreea Deac, Arantxa Casanova, Ben Poole, Graham Taylor,\nGuillem Cucurull, Justin Gilmer, Nithium Thain and Zhaocheng Zhu for reviewing the paper prior\nto submission.\nREFERENCES\nPeter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,\nMateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al.\nRelational inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261,\n2018.\nIshmael Belghazi, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville,\nand R Devon Hjelm.\nMine:\nmutual information neural estimation.\narXiv preprint\narXiv:1801.04062, ICML\u20192018, 2018.\nAleksandar Bojchevski and Stephan G\u00a8unnemann. Deep gaussian embedding of graphs: Unsuper-\nvised inductive learning via ranking. In International Conference on Learning Representations,\n2018. URL https://openreview.net/forum?id=r1ZdKJ-0W.\nAvishek Bose, Huan Ling, and Yanshuai Cao. Adversarial contrastive estimation. In ACL, 2018.\nMichael M Bronstein, Joan Bruna, Yann LeCun, Arthur Szlam, and Pierre Vandergheynst. Geomet-\nric deep learning: going beyond euclidean data. IEEE Signal Processing Magazine, 34(4):18\u201342,\n2017.\nJie Chen, Tengfei Ma, and Cao Xiao. Fastgcn: fast learning with graph convolutional networks via\nimportance sampling. arXiv preprint arXiv:1801.10247, 2018.\nRonan Collobert and Jason Weston. A uni\ufb01ed architecture for natural language processing: Deep\nneural networks with multitask learning. In Proceedings of the 25th international conference on\nMachine learning, pp. 160\u2013167. ACM, 2008.\nMing Ding, Jie Tang, and Jie Zhang. Semi-supervised learning on graphs with generative adversarial\nnets. arXiv preprint arXiv:1809.00130, 2018.\nClaire Donnat, Marinka Zitnik, David Hallac, and Jure Leskovec. Learning structural node em-\nbeddings via diffusion wavelets. In International ACM Conference on Knowledge Discovery and\nData Mining (KDD), volume 24, 2018.\nAlberto Garcia Duran and Mathias Niepert. Learning graph representations with embedding propa-\ngation. In Advances in Neural Information Processing Systems, pp. 5119\u20135130, 2017.\nJustin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural\nmessage passing for quantum chemistry. arXiv preprint arXiv:1704.01212, 2017.\nXavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In Proceedings of the Thirteenth International Conference on Arti\ufb01cial Intelligence\nand Statistics, pp. 249\u2013256, 2010.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672\u20132680, 2014.\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining,\npp. 855\u2013864. ACM, 2016.\n11\nPublished as a conference paper at ICLR 2019\nMichael Gutmann and Aapo Hyv\u00a8arinen. Noise-contrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings of the Thirteenth International Conference\non Arti\ufb01cial Intelligence and Statistics, pp. 297\u2013304, 2010.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nIn Advances in Neural Information Processing Systems, pp. 1024\u20131034, 2017a.\nWilliam L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods\nand applications. arXiv preprint arXiv:1709.05584, 2017b.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into recti\ufb01ers: Surpassing\nhuman-level performance on imagenet classi\ufb01cation. In Proceedings of the IEEE international\nconference on computer vision, pp. 1026\u20131034, 2015.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nR Devon Hjelm, Alex Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and\nYoshua Bengio. Learning deep representations by mutual information estimation and maximiza-\ntion. arXiv preprint arXiv:1808.06670, 2018.\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected\nconvolutional networks. In CVPR, volume 1, pp. 3, 2017.\nGlen Jeh and Jennifer Widom. Scaling personalized web search. In Proceedings of the 12th Inter-\nnational Conference on the World Wide Web, pp. 271\u2013279. Acm, 2003.\nDiederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nThomas N Kipf and Max Welling. Semi-supervised classi\ufb01cation with graph convolutional net-\nworks. arXiv preprint arXiv:1609.02907, 2016a.\nThomas N Kipf and Max Welling.\nVariational graph auto-encoders.\narXiv preprint\narXiv:1611.07308, 2016b.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\nLearning Research, 9(Nov):2579\u20132605, 2008.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111\u20133119, 2013.\nAndriy Mnih and Koray Kavukcuoglu. Learning word embeddings ef\ufb01ciently with noise-contrastive\nestimation. In Advances in neural information processing systems, pp. 2265\u20132273, 2013.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-\nplers using variational divergence minimization. In Advances in Neural Information Processing\nSystems, pp. 271\u2013279, 2016.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predic-\ntive coding. arXiv preprint arXiv:1807.03748, 2018.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer.\nAutomatic differentiation in\npytorch. In NIPS-W, 2017.\nJeffrey Pennington, Richard Socher, and Christopher Manning. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 conference on empirical methods in natural language\nprocessing (EMNLP), pp. 1532\u20131543, 2014.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\nDeepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pp. 701\u2013710. ACM, 2014.\n12\nPublished as a conference paper at ICLR 2019\nLeonardo FR Ribeiro, Pedro HP Saverese, and Daniel R Figueiredo. struc2vec: Learning node\nrepresentations from structural identity. In Proceedings of the 23rd ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, pp. 385\u2013394. ACM, 2017.\nPeter J Rousseeuw. Silhouettes: a graphical aid to the interpretation and validation of cluster analy-\nsis. Journal of computational and applied mathematics, 20:53\u201365, 1987.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008.\nNitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. Journal of machine learning\nresearch, 15(1):1929\u20131958, 2014.\nAravind Subramanian, Pablo Tamayo, Vamsi K Mootha, Sayan Mukherjee, Benjamin L Ebert,\nMichael A Gillette, Amanda Paulovich, Scott L Pomeroy, Todd R Golub, Eric S Lander, et al.\nGene set enrichment analysis: a knowledge-based approach for interpreting genome-wide expres-\nsion pro\ufb01les. Proceedings of the National Academy of Sciences, 102(43):15545\u201315550, 2005.\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei.\nLine: Large-\nscale information network embedding. In Proceedings of the 24th International Conference on\nWorld Wide Web, pp. 1067\u20131077. International World Wide Web Conferences Steering Commit-\ntee, 2015.\nPetar Veli\u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Li`o, and Yoshua\nBengio.\nGraph Attention Networks.\nInternational Conference on Learning Representations,\n2018. URL https://openreview.net/forum?id=rJXMpikCZ.\nOriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets.\narXiv preprint arXiv:1511.06391, 2015.\nDaixin Wang, Peng Cui, and Wenwu Zhu. Structural deep network embedding. In Proceedings of\nthe 22nd ACM SIGKDD international conference on Knowledge discovery and data mining, pp.\n1225\u20131234. ACM, 2016.\nXiao Wang, Peng Cui, Jing Wang, Jian Pei, Wenwu Zhu, and Shiqiang Yang. Community preserving\nnetwork embedding. In AAAI, pp. 203\u2013209, 2017.\nBoris Weisfeiler and AA Lehman. A reduction of a graph to a canonical form and an algebra arising\nduring this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968.\nZhilin Yang, William W Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning\nwith graph embeddings. arXiv preprint arXiv:1603.08861, 2016.\nRex Ying, Ruining He, Kaifeng Chen, Pong Eksombatchai, William L Hamilton, and Jure Leskovec.\nGraph convolutional neural networks for web-scale recommender systems.\narXiv preprint\narXiv:1806.01973, 2018a.\nRex Ying, Jiaxuan You, Christopher Morris, Xiang Ren, William L Hamilton, and Jure\nLeskovec. Hierarchical graph representation learning with differentiable pooling. arXiv preprint\narXiv:1806.08804, 2018b.\nJiani Zhang, Xingjian Shi, Junyuan Xie, Hao Ma, Irwin King, and Dit-Yan Yeung.\nGaan:\nGated attention networks for learning on large and spatiotemporal graphs.\narXiv preprint\narXiv:1803.07294, 2018.\nXiaojin Zhu, Zoubin Ghahramani, and John D Lafferty. Semi-supervised learning using gaussian\n\ufb01elds and harmonic functions. In Proceedings of the 20th International conference on Machine\nlearning (ICML-03), pp. 912\u2013919, 2003.\nMarinka Zitnik and Jure Leskovec.\nPredicting multicellular function through multi-layer tissue\nnetworks. Bioinformatics, 33(14):i190\u2013i198, 2017.\n13\nPublished as a conference paper at ICLR 2019\nFigure 4: Discriminator scores, D\n\u0010\n\u20d7hi,\u20d7s\n\u0011\n, attributed to each node in the Cora dataset shown over a\nt-SNE of the DGI algorithm. Shown for both the original graph (left) and a negative sample (right).\nA\nFURTHER DATASET DETAILS\nTransductive learning. We utilize three standard citation network benchmark datasets\u2014Cora, Cite-\nseer and Pubmed (Sen et al., 2008)\u2014and closely follow the transductive experimental setup of Yang\net al. (2016). In all of these datasets, nodes correspond to documents and edges to (undirected)\ncitations. Node features correspond to elements of a bag-of-words representation of a document.\nEach node has a class label. We allow for only 20 nodes per class to be used for training\u2014however,\nhonouring the transductive setup, the unsupervised learning algorithm has access to all of the nodes\u2019\nfeature vectors. The predictive power of the learned representations is evaluated on 1000 test nodes.\nInductive learning on large graphs. We use a large graph dataset (231,443 nodes and 11,606,919\nedges) of Reddit posts created during September 2014 (derived and preprocessed as in Hamilton\net al. (2017a)). The objective is to predict the posts\u2019 community (\u201csubreddit\u201d), based on the GloVe\nembeddings of their content and comments (Pennington et al., 2014), as well as metrics such as score\nor number of comments. Posts are linked together in the graph if the same user has commented on\nboth. Reusing the inductive setup of Hamilton et al. (2017a), posts made in the \ufb01rst 20 days of\nthe month are used for training, while the remaining posts are used for validation or testing and are\ninvisible to the training algorithm.\nInductive learning on multiple graphs. We make use of a protein-protein interaction (PPI) dataset\nthat consists of graphs corresponding to different human tissues (Zitnik & Leskovec, 2017). The\ndataset contains 20 graphs for training, 2 for validation and 2 for testing. Critically, testing graphs\nremain completely unobserved during training. To construct the graphs, we used the preprocessed\ndata provided by Hamilton et al. (2017a). Each node has 50 features that are composed of positional\ngene sets, motif gene sets and immunological signatures. There are 121 labels for each node set\nfrom gene ontology, collected from the Molecular Signatures Database (Subramanian et al., 2005),\nand a node can possess several labels simultaneously.\nB\nFURTHER QUALITATIVE ANALYSIS\nVisualizing discriminator scores. After obtaining the t-SNE visualizations, we turned our attention\nto the discriminator\u2014and visualized the scores it attached to various nodes, for both the positive\nand a (randomly sampled) negative example (Figure 4). From here we can make an interesting\nobservation\u2014within the \u201cclusters\u201d of the learnt embeddings on the positive Cora graph, only a\nhandful of \u201chot\u201d nodes are selected to receive high discriminator scores. This suggests that there\nmay be a clear distinction between embedding dimensions used for discrimination and classi\ufb01cation,\nwhich we more thoroughly investigate in the next paragraph. In addition, we may observe that, as\nexpected, the model is unable to \ufb01nd any strong structure within a negative example. Lastly, a few\nnegative examples achieve high discriminator scores\u2014a phenomenon caused by the existence of\n14\nPublished as a conference paper at ICLR 2019\n0\n100\n200\n300\n400\n500\n0\n100\n200\n300\n400\n500\nDimension\nNode\nVisualizing top positive/negative samples\n0.04\n0.02\n0.00\n0.02\n0.04\n\u00b710\u22122\nFigure 5: The learnt embeddings of the highest-scored positive examples (upper half), and the\nlowest-scored negative examples (lower half).\n0\n100\n200\n300\n400\n500\n65\n70\n75\n80\n85\nDimensions removed\nTest accuracy\nDGI classi\ufb01cation: robustness to removing dimensions\nDGI (p \u2191)\nDGI (p \u2193)\nGCN\nRandom-Init\n0\n100\n200\n300\n400\n500\n0\n500\n1,000\n1,500\n2,000\n2,500\nDimensions removed\nExamples misclassi\ufb01ed by D\nDGI discriminator: robustness to removing dimensions\n+ (p \u2193)\n\u2212(p \u2193)\n+ (p \u2191)\n\u2212(p \u2191)\nFigure 6: Classi\ufb01cation performance (in terms of test accuracy of logistic regression; left) and\ndiscriminator performance (in terms of number of poorly discriminated positive/negative examples;\nright) on the learnt DGI embeddings, after removing a certain number of dimensions from the\nembedding\u2014either starting with most distinguishing (p \u2191) or least distinguishing (p \u2193).\nlow-degree nodes in Cora (making the probability of a node ending up in an identical context it had\nin the positive graph non-negligible).\nImpact and role of embedding dimensions. Guided by the previous result, we have visualized the\nembeddings for the top-scoring positive and negative examples (Figure 5). The analysis revealed\nexistence of distinct dimensions in which both the positive and negative examples are strongly bi-\nased. We hypothesize that, given the random shuf\ufb02ing, the average expected activation of a negative\nexample is zero, and therefore strong biases are required to \u201cpush\u201d the example down in the discrim-\ninator. The positive examples may then use the remaining dimensions to both counteract this bias\nand encode patch similarity. To substantiate this claim, we order the 512 dimensions based on how\ndistinguishable the positive and negative examples are in them (using p-values obtained from a t-test\nas a proxy). We then remove these dimensions from the embedding, respecting this order\u2014either\nstarting from the most distinguishable (p \u2191) or least distinguishable dimensions (p \u2193)\u2014monitoring\n15\nPublished as a conference paper at ICLR 2019\nhow this affects both classi\ufb01cation and discriminator performance (Figure 6). The observed trends\nlargely support our hypothesis: if we start by removing the biased dimensions \ufb01rst (p \u2193), the classi-\n\ufb01cation performance holds up for much longer (allowing us to remove over half of the embedding\ndimensions while remaining competitive to the supervised GCN), and the positive examples mostly\nremain correctly discriminated until well over half the dimensions are removed.\nC\nROBUSTNESS TO CHOICE OF CORRUPTION FUNCTION\nHere, we consider alternatives to our corruption function, C, used to produce negative graphs. We\ngenerally \ufb01nd that, for the node classi\ufb01cation task, DGI is stable and robust to different strategies.\nHowever, for learning graph features towards other kinds of tasks, the design of appropriate corrup-\ntion strategies remains an area of open research.\nOur corruption function described in Section 4.2 preserves the original adjacency matrix ( eA =\nA) but corrupts the features, eX, via row-wise shuf\ufb02ing of X. In this case, the negative graph is\nconstrained to be isomorphic to the positive graph, which should not have to be mandatory. We can\ninstead produce a negative graph by directly corrupting the adjacency matrix.\nTherefore, we \ufb01rst consider an alternative corruption function C which preserves the features\n( eX = X) but instead adds or removes edges from the adjacency matrix ( eA \u0338= A). This is done\nby sampling, i.i.d., a switch parameter \u03a3ij, which determines whether to corrupt the adjacency ma-\ntrix at position (i, j). Assuming a given corruption rate, \u03c1, we may de\ufb01ne C as performing the\nfollowing operations:\n\u03a3ij \u223cBernoulli(\u03c1)\n(11)\neA = A \u2295\u03a3\n(12)\nwhere \u2295is the XOR (exclusive OR) operation.\nThis alternative strategy produces a negative graph with the same features, but different connectivity.\nHere, the corruption rate of \u03c1 = 0 corresponds to an unchanged adjacency matrix (i.e. the positive\nand negative graphs are identical in this case). In this regime, learning is impossible for the discrim-\ninator, and the performance of DGI is in line with a randomly initialized DGI model. At higher rates\nof noise, however, DGI produces competitive embeddings.\nWe also consider simultaneous feature shuf\ufb02ing ( eX \u0338= X) and adjacency matrix perturbation\n( eA \u0338= A), both as described before. We \ufb01nd that DGI still learns useful features under this com-\npound corruption strategy\u2014as expected, given that feature shuf\ufb02ing is already equivalent to an (iso-\nmorphic) adjacency matrix perturbation.\nFrom both studies, we may observe that a certain lower bound on the positive graph perturbation rate\nis required to obtain competitive node embeddings for the classi\ufb01cation task on Cora. Furthermore,\nthe features learned for downstream node classi\ufb01cation tasks are most powerful when the negative\ngraph has similar levels of connectivity to the positive graph.\nThe classi\ufb01cation performance peaks when the graph is perturbed to a reasonably high level, but\nremains sparse; i.e. the mixing between the separate 1-step patches is not substantial, and therefore\nthe pool of negative examples is still diverse enough. Classi\ufb01cation performance is impacted only\nmarginally at higher rates of corruption\u2014corresponding to dense negative graphs, and thus a less\nrich negative example pool\u2014but still considerably outperforming the unsupervised baselines we\nhave considered. This could be seen as further motivation for relying solely on feature shuf\ufb02ing,\nwithout adjacency perturbations\u2014given that feature shuf\ufb02ing is a trivial way to guarantee a diverse\nset of negative examples, without incurring signi\ufb01cant computational costs per epoch.\nThe results of this study are visualized in Figures 7 and 8.\n16\nPublished as a conference paper at ICLR 2019\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\n70\n75\n80\nCorruption rate, \u03c1\nTest accuracy\nClassi\ufb01cation accuracy for A corruption\neX = X\nGCN\nRandom-Init\nFigure 7: DGI also works under a corruption function that modi\ufb01es only the adjacency matrix\n( eA \u0338= A) on the Cora dataset. The left range (\u03c1 \u21920) corresponds to no modi\ufb01cations of the\nadjacency matrix\u2014therein, performance approaches that of the randomly initialized DGI model.\nAs \u03c1 increases, DGI produces more useful features, but ultimately fails to outperform the feature-\nshuf\ufb02ing corruption function. N.B. log scale used for \u03c1.\n10\u22126\n10\u22125\n10\u22124\n10\u22123\n10\u22122\n10\u22121\n100\n80\n81\n82\nCorruption Rate, \u03c1\nTest accuracy\nClassi\ufb01cation accuracy for (X, A) corruption\neX \u0338= X\nGCN\nFigure 8: DGI is stable and robust under a corruption function that modi\ufb01es both the feature matrix\n(X \u0338= eX) and the adjacency matrix ( eA \u0338= A) on the Cora dataset. Corruption functions that preserve\nsparsity (\u03c1 \u2248\n1\nN ) perform the best. However, DGI still performs well even with large disruptions\n(where edges are added or removed with probabilities approaching 1). N.B. log scale used for \u03c1.\n17\n",
    "2311.06835": "Open-Set Graph Anomaly Detection via Normal\nStructure Regularisation\nQizhou Wang1, Guansong Pang2\u2217, Mahsa Salehi1, Xiaokun Xia3, Christopher Leckie4\n1Monash University\n2Singapore Management University\n3The University of Tokyo\n4The University of Melbourne\n{qizhou.wang, mahsa.salehi}@monash.edu, gspang@smu.edu.sg\nxia.xiaokun@ipmu.jp, caleckie@unimelb.edu.au\nAbstract\nThis paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). The availability of those labelled training data provides crucial\nprior knowledge about abnormalities for GAD models, enabling substantially\nreduced detection errors. However, current methods tend to over-emphasise fitting\nthe seen anomalies, leading to a weak generalisation ability to detect the unseen\nanomalies. Further, they were introduced to handle Euclidean data, failing to\neffectively capture important information on graph structure and node attributes for\nGAD. In this work, we propose a novel open-set GAD approach, namely normal\nstructure regularisation (NSReg), to achieve generalised detection ability to unseen\nanomalies, while maintaining its effectiveness on detecting seen anomalies. The\nkey idea in NSReg is to introduce a regularisation term that enforces the learning\nof compact, semantically-rich representations of normal nodes based on their\nstructural relations to other nodes. When being optimised with supervised anomaly\ndetection losses, the regularisation term helps incorporate strong normality into the\nmodelling, and thus, it effectively avoids the overfitting the seen anomalies solely.\nIn doing so, it helps learn better normality decision boundary, reducing the errors\nof detecting unseen anomalies as normal. Extensive empirical results on seven\nreal-world datasets show the superiority of NSReg for open-set GAD.\n1\nIntroduction\nDetection of anomalous nodes in a graph is a crucial task in the context of Graph Anomaly Detection\n(GAD) [1, 24]. Its popularity has been growing in recent years due to its wide range of real-\nworld applications, such as detection of malicious users in social networks, illegal transactions\nin financial networks, and faults in sensor networks. There have been numerous GAD methods\nintroduced [1,7,23,24], with the majority of them designed as unsupervised approaches. However,\nthey are often associated with high detection errors due to the lack of knowledge about anomalies.\nThere has been growing interest in supervised Anomaly Detection (AD) methods [18, 27\u201329, 43]\nbecause they are able to utilise labelled anomaly data to substantially reduce the high detection\nerrors of the unsupervised approaches. Such methods assume the availability of a limited number\nof labelled anomalies, which is usually feasible to obtain in real-world applications and can be\nutilised to enable anomaly-informed supervision. Despite their generally superior performance,\n\u2217Corresponding author: Guansong Pang (gspang@smu.edu.sg)\nPreprint. Under review.\narXiv:2311.06835v3  [cs.LG]  2 Jun 2024\nthese methods prove less effective in open-set GAD where training models with labelled anoma-\nlies (referred to as seen anomalies) fails to adequately represent anomalies at inference time,\nparticularly in the context of newly emerging types/classes of anomalies (i.e., unseen anoma-\nlies). This is because such methods often concentrate solely on modelling abnormal patterns\nderived from labelled anomalies, thus exhibiting poor generalisation to the unseen anomalies,\ni.e., many unseen anomalies are detected as normal, as shown in Figure 1(a-b). In addition,\nAUC-ROCall =0.823\nAUC-ROCunseen =0.702\nPhoto\n(a) BCE\nAUC-ROCall =0.754\nAUC-ROCunseen =0.666\n(b) BWGNN\nAUC-ROCall =0.942\nAUC-ROCunseen =0.925\n(c) NSReg (ours)\nAUC-ROCall =0.613\nAUC-ROCunseen =0.017\nogbn-proteins\nAUC-ROCall =0.705\nAUC-ROCunseen =0.051\nAUC-ROCall =0.997\nAUC-ROCunseen =0.996\n0\n1\nColormap\nNormal\nSeen Anomalies\nUnseen Anomalies\nFigure 1: Visualisation of node representations\nwith anomaly score contour lines of three semi-\nsupervised GAD models: baseline classifier Bi-\nnary Cross-Entropy (BCE), recent state-of-the-art\nBWGNN [43], and our proposed NSReg, on two\nreal-world datasets: Photo and ogbn-proteins. The\nmodels are trained using a limited number of seen\nanomalies and normal nodes. While BCE and\nBWGNN tend to overly focus on the decision\nboundary between the seen anomalies and normal\nnodes, NSReg effectively mitigates overfitting to\nthe seen anomalies, resulting in more generalised\ndetection of both seen and unseen anomalies.\nthey are mostly designed for handling Euclidean\ndata like image and tabular data, thereby over-\nlooking valuable discriminative information on\nthe structure and node attributes in graph data.\nTo address these issues, this paper focuses on the\npractical yet under-explored problem of open-\nset GAD, aiming to enhance the generalisation\nfor both seen and unseen anomaly nodes by\nleveraging a small set of labelled seen anomaly\nnodes. To this end, we propose a novel open-\nset GAD approach, namely normal structure\nregularisation (NSReg), to leverage the rich nor-\nmal graph information embedded in the labelled\nnodes. The key idea in NSReg is to introduce\na regularisation term that enforces the learning\nof compact, semantically-rich representations\nof normal nodes based on their structural rela-\ntions to other nodes. When being optimised\nwith supervised anomaly detection losses, the\nregularisation term incorporates strong normal-\nity into the modelling, and thus, it effectively\navoids the overfitting the seen anomalies solely,\nwhile learning better normality decision bound-\nary. This helps substantially reduce the errors of\ndetecting unseen anomalies as normal.\nIn particular, to capture those semantically-rich\nnormal structure relations through the regulari-\nsation, NSReg differentiates the labelled normal\nnodes that are connected in their local neighbour-\nhood from those that are not. This is done by\npredicting three types of normal-node-oriented\nrelation prediction, including: connected nor-\nmal nodes, unconnected normal nodes, and unconnected normal nodes to unlabelled nodes. By\ndoing so, our regularisation module prioritises establishing distinct node representations based on\ntheir structural relationships with the normal class, rather than attempting to predict these anomalies\nwithout grounded information. As a result, it effectively reinforces the structural normality of the\ngraph in the representation space and enforces a more stringent decision boundary for the normal\nclass, enabling better separability of the unseen anomaly nodes from the normal nodes, as shown\nin Figure 1(c). Moreover, NSReg is a plug-and-play module, which can be integrated as a plugin\nmodule into various supervised GAD learning approaches to enhance their generalisability to unseen\nanomalies. In summary, our main contributions are as follows:\n\u2022 We study an under-explored problem of GAD, namely open-set GAD, and validate the\nfeasibility of leveraging graph structural normality to regularise representation learning in\nsupervised GAD, resulting in effective mitigation of the overfitting on the seen anomalies.\n\u2022 We propose the NSReg approach for this problem that regularises supervised GAD models\nby differentiating normal-node-oriented relations. This avoids the overfitting of the seen\nanomalies and enables better separation between unseen anomalies and normal nodes,\nresulting in significantly improved detection generalisation.\n\u2022 NSReg is a plug-and-play regularisation term that can be applied to enhance the generalisa-\ntion of different supervised GAD methods.\n2\nInput graph: G\nVtrain\nn\nVtest\nn\nVseen\na\nVunseen\na\nProj\n\u03b7\n`\nR\n\u2026\nAnomaly \nScore\nNormal-oriented Relation Generation\nNormal-relation-oriented \nModelling\n\u20dd\n\u26ab\n\u03c8\nNSR Module\n\u03c6\nzu\nzv\nF\nc1\nc2\nc3\nC\n\u2026\nRelation Labelling\nLNSR(Qtrain, Ctrain)\nLAD(Strain, Y train)\ns\nu\n0\n\u03b1\n1\n0\n0\nq\nL = LAD\n+ \u03bbLNSR\nOverall Loss\nEffect of\nEffect of\nLAD\nLNSR\nShared latent space of \nand the NSR module F\u03b7\nEffect of\nFigure 2: An overview of our proposed approach, NSReg, illustrating the integration of the NSR\nmodule into a graph anomaly detector as a plug-and-play module for regularising supervised GAD\ntraining, where red arrows indicate the \u201cplugging points\". The green teal dashed box illustrates\nthe decomposition of NSReg\u2019s overall learning objective in the shared representation space. Here,\nLNSR focuses on enforcing a more stringent decision boundary for the normal class, enabling better\nseparability of the unseen anomaly nodes from the normal nodes, which is not considered by LAD.\n\u2022 We show through extensive empirical studies that NSReg significantly outperforms all\ncompeting methods, e.g., by at least 14% on the unseen anomaly classes and by 10% in\nAUC-ROC on all anomaly classes compared to the best competing method.\n2\nRelated Work\nGraph Anomaly Detection. GAD methods are usually designed as unsupervised learning to bypass\nthe reliance of labelled data [18,24]. Earlier shallow methods [13,21,33,34,49] leverage various\nheuristics to detect anomalies. Recent GAD methods predominantly utilise graph neural networks\n(GNNs) [48] and have demonstrated superior performance due to their strong representation learning\ncapacity. There are numerous unsupervised GNN-based GAD methods proposed [6,7,22,23,36,52].\nHowever, these methods are not trained using real anomalies and have proven to be ineffective\nwhen their heuristic optimisation objective mismatches the actual anomaly patterns. To leverage\nanomaly-specific prior knowledge, supervised learning that utilises a small number of labelled\nanomalies [2,11,14,17,43,44,46] and learning schemes such as meta learning and transfer learning\nhave also been explored for GAD [8,10,45,54]. Nevertheless, these methods are prone to overfitting\nthe small number of labelled anomaly nodes and do not implement effective regularisation to ensure\nthe generalisation on the unseen anomalies.\nTowards Supervised Anomaly Detection. Although numerous AD methods have been proposed\n[3,26], most of them [30,39,50] adopt optimisation objectives that focus on representation learning\nand are indirect for anomaly scoring. More recent works [27\u201329, 40, 42, 53] leverage a small\nnumber of labelled anomalies to perform optimisation and anomaly scoring in an end-to-end pipeline,\nsignificantly improving detection performance. However, such methods are originally designed for\nnon-structured data and cannot explore structural information when directly applied for GAD.\nImbalanced Node Classification. Another closely related line of research is imbalanced classification\n[4,19,55], which is a longstanding challenge in mitigating the class imbalance in the training data. A\nvariety of approaches [9,31,37,47,51] are have been proposed to mitigate class imbalance in the\ntraining graph data to avoid overfitting the majority classes. However, these methods only assume a\nfixed number of known classes and do not consider the potential unseen classes at inference time. As\na result, they fail to generalise to unseen anomaly classes in open-set GAD.\n3\nOur Proposed Approach\n3.1\nProblem Statement\nWe consider open-set GAD on attributed graphs. Let G = (V, E, X) be an attributed graph with a\nnode set V, an edge set E and a feature matrix X, which contains significantly fewer anomalous\nnodes Va than normal nodes Vn. In open-set GAD, during training, the labelled training nodes Vtrain\nare often unable to illustrate all possible anomaly classes at inference. For clarity, we use Vseen\na\nto\ndenote anomaly nodes that can be illusrated by the labelled anomaly nodes (seen anomalies) and\nVunseen\na\nto denote the unseen anomalies, such that Va = Vseen\na\n\u222aVunseen\na\nand Vtrain\na\n\u2208Vseen\na\n.\n3\nOur objective is to learn a scoring function \u03d5 : (G, V) \u2192R, such that \u03d5(G, va) \u226b\u03d5(G, vn),\nfor all va \u2208Vseen\na\n\u222aVunseen\na\nand vn \u2208Vn. In this paper, we consider a GNN-based anomaly\nscoring function, which consists of a pipeline with two components: a graph representation learner\n\u03c8(G, V; \u0398\u03c8): (G, V) \u2192Z and an anomaly scoring function \u03b7(Z; \u0398\u03b7) : Z \u2192R, where \u0398\u03c8 and \u0398\u03b7\nare learnable parameters. We aim to obtain the following anomaly scoring mapping:\n\u03d5(G, V; \u0398\u03d5) = \u03b7(\u03c8(G, V; \u0398\u03c8); \u0398\u03b7),\n(1)\nwith the support of the labelled nodes and the graph structure. The main challenges are the unforesee-\nable nature of anomalies and the risk of overfitting to labelled anomaly nodes.\n3.2\nOverview of NSReg\nIn this paper, we propose a novel open-set GAD framework, namely normal structure regularisation\n(NSReg), as a solution to tackle the aforementioned challenges. The key insight behind NSReg is to\nexplicitly guide the representation learner in capturing discriminative normal structural information\nfrom the graph characterised by the labelled normal nodes, supplementing the incomplete and often\nbiased label information provided by the labelled anomalies that represent the seen anomalies. The\nmodelling of this structural normality effectively calibrates the normality decision boundary, enabling\nbetter generalisation to the unseen anomalies, i.e., better separation between the unseen anomalies\nand normal nodes.\nAs illustrated in Figure 2, NSReg leverages a novel Normal Structure Regularisation (NSR) module\nto enforce compact and uncluttered normal subspace in the representation space. During training, the\nNSR module is integrated as a regularising component with a supervised graph anomaly detector,\nboth sharing the same representation learner for joint optimisation, to enable the joint learning of the\nstructural normality and seen anomaly patterns for GAD. The general objective of NSReg can be\ndefined as the following:\narg min\n\u0398\u03d5,\u0398NSR\nX\nv\u2208V\nLAD(sv, yv) + \u03bb\nX\nr\u2208R\nLNSR(qr, cr),\n(2)\nwhere \u0398\u03d5 and \u0398NSR are the respective learnable parameters of the anomaly detection \u03d5 and the\nNSR module, and LAD is the loss function of a supervised GAD model, with sv being the predicted\nanomaly score and yv being the ground truth. Additionally, LNSR signifies the loss function of the\nNSR module adjusted by the regularisation coefficient \u03bb, with qr denoting the predicted normality\nof a relation sample r, and its normality cr is defined by a labelling function C. During inference,\nthe representation learner \u03c8\u2217is combined with the trained anomaly scoring network \u03b7\u2217to form\nan end-to-end pipeline for detection, and the NSR module is disconnected from the representation\nlearner since the normal structural information has already been learned. This simplifies the inference\nprocess without introducing any additional NSR module-related runtime or resource overhead.\n3.3\nNormal Structure Regularisation (NSR)\nThe design of the NSR module, which is the core of NSReg, is motivated by the limitations of\nmost supervised GAD methods, which are only designed to maximise separability between normal\nnodes and seen anomalies, but fail to provide sufficient supervision for the representation learner\nto effectively differentiate unseen anomaly representations from the normal class. In an open-set\nenvironment, we are unable to obtain the prior knowledge of the unseen anomalies, and thus, difficult\nto learn the unseen anomaly patterns. Thus, NSReg takes a step back and focuses on learning better\nnormality, which would help distinguish the unseen anomalies from the normal nodes better. NSReg\nachieves this by modelling the normal-node-oriented relations (i.e., {r = (v, u) | v \u2208Vn, u \u2208V}),\nwhich is aimed at enforcing a stricter definition of the normal region and recalibrating misplaced\nunseen anomaly representations within the representation space. By modelling three types of\nnormal-node-oriented relations as a discriminative task, NSReg enhances representation learning\nwith significantly enriched normality semantics, effectively disentangling unseen anomaly nodes\nfrom normal nodes in the representation space. We first provide a theoretical analysis of enforcing\nstructural normality and then detail the two core components of NSR: normal-node-oriented relation\ngeneration and modelling.\nPreserving Structural Normality Improves Representation Learning for GAD. We analyse\n4\nthe effects of enforcing structural normality on the representation space and its advantages for\ngeneralisation to unseen anomalies. We consider a mapping g : (Rd, Rd) \u2192R[0,1] that models the\nnormality of normal-node-oriented relations in the node representation space Z, as defined by a\nlabelling function C, where 0 indicates the lowest normality and 1 the highest. C should satisfy\nthat 0 <= max(Ca) \u226amin(Cn) <= 1, where Cn and Ca indicates the respective set of the labels\nfor the relations between only the normal nodes and the relations between normal and anomaly\nnodes respectively, emphasising a significant differentiation in the scale of normality. g includes\ntwo sub-mappings: gE, a linear mapping that fuses the node representations to produce relation\nrepresentations, with the option to incorporate a homeomorphic scaling function on these node\nrepresentations; and gC, a linear mapping with a homeomorphic and monotonic final activation\nfunction to assign anomaly scores based on the representation. If Z is shared with a discriminative\ngraph anomaly detector, this will result in the normal nodes and the seen anomalies being well\nseparated into two isolated dense regions. Based on this observation, we demonstrate through the\nfollowing proposition that discriminating between these two types of relations will force potentially\nmisplaced anomalies, including those unseen, out of the normal subspace.\nProposition 1. Consider a well-trained mapping g that effectively distinguishes between two types\nof normal-node-oriented relations within a relation representation space H, derived from a node\nrepresentation space Z. The first type consists exclusively of relations among normal nodes, while\nthe second type involves one normal and one anomalous node, with normality defined by some\nlabeling function C, regardless their connectivity. Consider the subspace of all normal nodes in\nZ as a connected open set Zn, the boundary of which is defined by some closed hypersurfaces\nM = {Mi|i \u2208{1, \u00b7 \u00b7 \u00b7 , k}}. Furthermore, Let Zm denote the union of the interior of each Mi \u2282M.\nGiven such Zn and Zm, we can obtain that z \u2208Zn is true for all z \u2208Zm.\nThe proof, along with an intuitive diagram (Figure 5), is provided in Appendix A. There are two\nkey insights we can obtain from this proposition. First, since it is obvious that Zn is a subset of\nZm, by proving their equivalence, we show that no anomaly node locates inside the boundary of the\nnormal subspace, including both seen and unseen anomalies. Second, for those normal-node-oriented\nrelations that involve anomalies to be distinguishable from relations exclusively between normal\nnodes, it is necessary that the anomaly node does not reside in the normal subspace of Zn.Therefore,\nenforcing structural normality will result in the displacement of misplaced anomalies from the normal\nsubspace. These insights motivate the design of our NSR module, which is detailed below.\nNormal-node-oriented Relation Generation. The relation generation module implements the\nlabelling function C and oversees relation sampling, specifically tailored to integrate structural\nnormality knowledge in open-set GAD. It first samples normal-node-oriented relations and defines\ntheir normality, considering three types of relations, including connected normal relations R(n,c,n),\nunconnected normal relations R(n,u,n), and unconnected normal to other nodes, R(n,u,u). A labelling\nfunction C, which meets the requirements outlined in Proposition 1, is then used to define their\nnormality scores as follows:\nC(r) =\n\uf8f1\n\uf8f2\n\uf8f3\n1\nif r \u2208R(n,c,n) = {(v, u) | v, u \u2208Vtrain\nn\n, (v, u) \u2208E}\n\u03b1\nif r \u2208R(n,u,n) = {(v, u) | v, u \u2208Vtrain\nn\n, (v, u) /\u2208E}\n0\nif r \u2208R(n,u,u) = {(v, u) | v \u2208Vtrain\nn\n, u \u2208V \\ Vtrain, (v, u) /\u2208E},\nwhere 0 \u226a\u03b1 < 1 is the specification of the normality of R(n,u,n) relative to other two types of\nrelations. \u03b1 = 0.8 is set by default to define a three-level normality hierarchy we want to enforce.\nThis specification effectively embeds the homophilic assumption between the normal nodes while\npreserving the difference between the most related and related normal relations. We also find it\nunnecessary to include normal - seen anomaly pairs because similar information can be learned from\nsupervised GAD objectives.\nNormal-node-oriented Relation Modelling. This module instantiates the discriminative mapping g\nfor normal relation modeling as a neural network, denoted F. It first generates the representations of\nrelations by fusing the representations of their corresponding end nodes using a learnable mapping\nFE, which is then fed into a normality prediction network FC to model their normality. Specifically,\nFE can be defined as:\nhr = FE\n\u0000\u03c8(G, v), \u03c8(G, u)\n\u0001\n= \u03c3(zv) \u00b7 WE \u25e6\u03c3(zu),\n(3)\nwhere \u03c3 is the sigmoid function, and WE is a learnable weight matrix. zv and zu are the representa-\ntions of the node in relation r, generated by the node representation learner \u03c8, which comprises a\n5\nGNN-based representation learner followed by a projection network. The default GNN is a two-layer\nGraphSAGE [15] considering its good learning capacity and scalability. The relation modelling\nnetwork is then optimised according to the following loss function:\nLNSR = \u2212\n\u0010\ncr \u00b7 log\n\u0000FC(hr)\n\u0001\n+ (1 \u2212cr) \u00b7 log\n\u00001 \u2212FC(hr)\n\u0001\u0011\n,\n(4)\nwhere cr is the relation label subject to the labelling function C and FC is implemented as a\nlearner layer followed by the Sigmoid function. This design satisfied all the conditions required for\nProposition 1, ensuring that the normality enforced by the NSR module is accurately reflected in the\nshared representation space. It produces distinctive node representations in relation to those of the\nlabelled normal nodes, thereby enforcing significantly enriched, fine-grained normality among the\nnode representations. This enhancement directly regularises the representation learning of supervised\nanomaly detectors, enabling better separation of the unseen anomaly nodes from the normal nodes.\nNote that our NSR module is different from PReNet [27]. PReNet is a weakly-supervised anomaly\ndetector trained with an anomaly-oriented relation network, whereas NSR is a regularisation term\nfocusing on learning fine-grained normal representations to regularise supervised anomaly detectors.\n3.4\nOpen-set GAD using NSReg\nTraining. NSReg is trained through batch gradient descent over a predefined number of iterations.\nAt each iteration, we first sample a batch of bAD training nodes V , which contains bn\nAD labelled\nnormal nodes and all labelled anomaly nodes (note that the number of labelled anomalies is typically\nvery small) for tuning the anomaly scoring network \u03b7 and the representation learner \u03c8. In addition,\nthe normal-node-oriented relation generation is performed to generate bNSR relation samples R for\noptimising the NSR module and \u03c8. The overall training objective of NSReg can be formulated as:\narg min\n\u0398\u03d5,\u0398NSR\n\u22121\n|V |\n\u0000YV log(SV ) + (1 \u2212YV ) log\n\u00001 \u2212SV )\n\u0001\n+ \u03bb\n|R|LNSR(QR, CR),\n(5)\nwhere the first term is a simple supervised cross-entropy-loss-based anomaly detector and the second\nterm is our NSR module. As shown in Sec. 4.1, the NSR term can be also effectively combined\nwith other supervised GAD models in the first term. During training, the GAD loss is computed\ninitially based on the first term of the objective function and is used to update the parameters of the\nanomaly scoring network \u03b7. Subsequently, the NSR loss can be calculated using the second term\nof the objective function to update the relation modelling network. Finally, the combined GAD and\nNSR losses are aggregated and backpropagated to update the parameters of the representation learner.\nInference. During inference, given a test node v, NSReg first generates its representation zv, which\nis then scored using the trained anomaly scoring network \u03b7: sv = \u03d5(G, v) = \u03b7(\u03c8(G, v)).\n4\nExperiments\nDatasets. NSReg is extensively evaluated using seven real-world attributed graph datasets. To the\nbest of our knowledge, no publicly available GAD datasets include multiple types of anomaly classes.\nTherefore, we adapt imbalanced node classification and binary-labelled GAD datasets to define\nanomaly subclasses with distribution discrepancies for open-set GAD evaluation. For imbalanced\nnode classification datasets with multiple minor classes, such as Photo, Computers, and CS [41], we\ntreat each minor class (those with less than 5% of total nodes) as seen anomalies, and the rest as\nunseen anomalies. In the case of Yelp [38], a well-known GAD dataset with binary labels, we cluster\nthe learned representations of the anomaly class to create anomaly subclasses. Three large-scale\nattributed graph datasets, ogbn-arxiv, ogbn-proteins [16], and T-Finance [43] are also adapted to\nevaluate NSReg at scale. More details about the datasets are presented in Appendix B.2.\nEvaluation Protocol and Metrics. For each dataset, we treat one of the anomaly classes as the\nseen anomaly, with the other anomaly classes as unseen anomalies. We alternate this process for all\nanomaly classes, and report the results averaged over all cases. All experiments are repeated for 5\nrandom runs. For example, the CS dataset includes 8 anomaly classes, each is treated as \u2019seen\u2019 in turn,\nresulting in 8 sub-experiments per run, with a total of 40 sub-experiments across 5 runs. The Python\nstyle code for the evaluation protocol is presented in Appendix B.3. We employ two widely used\nand complementary performance metrics in GAD: the Area Under Receiver Operating Characteristic\n6\nTable 1: AUC-ROC and AUC-PR (mean\u00b1std) results for detecting all anomalies and exclusively\nunseen anomalies. The boldfaced are the best results.\nAUC-ROC\nAUC-PR\nTest Set\nMethod\nPhoto\nComputers\nCS\nYelp\nAverage\nPhoto\nComputers\nCS\nYelp\nAverage\nAll-\nAnomalies\nDOMINANT\n0.429\u00b10.001\n0.576\u00b10.007\n0.402\u00b10.000\n0.609\u00b10.003\n0.504\u00b10.003\n0.072\u00b10.000\n0.184\u00b10.005\n0.187\u00b10.000\n0.221\u00b10.003\n0.166\u00b10.002\nGGAN\n0.435\u00b10.003\n0.566\u00b10.008\n0.396\u00b10.005\n0.323\u00b10.007\n0.430\u00b10.006\n0.078\u00b10.001\n0.177\u00b10.005\n0.186\u00b10.001\n0.103\u00b10.001\n0.136\u00b10.002\nCOLA\n0.825\u00b10.033\n0.630\u00b10.020\n0.481\u00b10.015\n0.302\u00b10.013\n0.560\u00b10.020\n0.246\u00b10.034\n0.233\u00b10.024\n0.253\u00b10.011\n0.103\u00b10.003\n0.209\u00b10.018\nTAM\n0.626\u00b10.004\n0.435\u00b10.002\n0.637\u00b10.009\n0.295\u00b10.006\n0.498\u00b10.005\n0.122\u00b10.002\n0.131\u00b10.001\n0.318\u00b10.006\n0.138\u00b10.087\n0.177\u00b10.024\nPReNet\n0.698\u00b10.019\n0.632\u00b10.028\n0.547\u00b10.016\n0.692\u00b10.004\n0.642\u00b10.017\n0.459\u00b10.010\n0.374\u00b10.031\n0.363\u00b10.011\n0.336\u00b10.006\n0.383\u00b10.015\nDevNet\n0.599\u00b10.079\n0.606\u00b10.064\n0.769\u00b10.029\n0.675\u00b10.020\n0.662\u00b10.048\n0.223\u00b10.155\n0.284\u00b10.093\n0.684\u00b10.018\n0.315\u00b10.027\n0.375\u00b10.073\nDCI\n0.772\u00b10.061\n0.683\u00b10.051\n0.856\u00b10.012\n0.689\u00b10.059\n0.750\u00b10.023\n0.452\u00b10.099\n0.427\u00b10.069\n0.635\u00b10.028\n0.351\u00b10.044\n0.466\u00b10.031\nBWGNN\n0.728\u00b10.026\n0.722\u00b10.008\n0.769\u00b10.029\n0.727\u00b10.012\n0.737\u00b10.019\n0.313\u00b10.066\n0.461\u00b10.012\n0.687\u00b10.048\n0.366\u00b10.015\n0.457\u00b10.035\nAMNet\n0.773\u00b10.001\n0.671\u00b10.007\n0.873\u00b10.003\n0.695\u00b10.011\n0.753\u00b10.006\n0.487\u00b10.003\n0.395\u00b10.016\n0.784\u00b10.004\n0.337\u00b10.013\n0.501\u00b10.009\nGHRN\n0.741\u00b10.015\n0.604\u00b10.023\n0.757\u00b10.036\n0.713\u00b10.021\n0.704\u00b10.009\n0.360\u00b10.034\n0.324\u00b10.024\n0.615\u00b10.056\n0.349\u00b10.020\n0.412\u00b10.016\nG. ENS\n0.712\u00b10.005\n0.672\u00b10.009\n0.845\u00b10.027\n0.572\u00b10.011\n0.700\u00b10.013\n0.246\u00b10.008\n0.319\u00b10.015\n0.515\u00b10.016\n0.199\u00b10.010\n0.320\u00b10.012\nG. SMOTE\n0.616\u00b10.043\n0.700\u00b10.046\n0.731\u00b10.009\n0.727\u00b10.019\n0.694\u00b10.029\n0.135\u00b10.041\n0.369\u00b10.043\n0.732\u00b10.060\n0.300\u00b10.019\n0.384\u00b10.041\nBCE\n0.807\u00b10.014\n0.724\u00b10.027\n0.854\u00b10.039\n0.712\u00b10.017\n0.774\u00b10.024\n0.515\u00b10.011\n0.481\u00b10.026\n0.756\u00b10.006\n0.376\u00b10.017\n0.532\u00b10.015\nNSReg (Ours)\n0.908\u00b10.016\n0.797\u00b10.015\n0.957\u00b10.007\n0.734\u00b10.012\n0.849\u00b10.013\n0.640\u00b10.036\n0.559\u00b10.018\n0.889\u00b10.016\n0.398\u00b10.014\n0.622\u00b10.021\nUnseen-\nAnomalies\nDOMINANT\n0.428\u00b10.002\n0.576\u00b10.008\n0.401\u00b10.000\n0.633\u00b10.004\n0.510\u00b10.004\n0.041\u00b10.000\n0.156\u00b10.006\n0.169\u00b10.000\n0.135\u00b10.002\n0.125\u00b10.002\nGGAN\n0.435\u00b10.006\n0.566\u00b10.009\n0.395\u00b10.005\n0.319\u00b10.008\n0.429\u00b10.007\n0.046\u00b10.001\n0.150\u00b10.004\n0.168\u00b10.001\n0.055\u00b10.000\n0.105\u00b10.002\nCOLA\n0.826\u00b10.034\n0.629\u00b10.024\n0.482\u00b10.015\n0.291\u00b10.015\n0.557\u00b10.022\n0.156\u00b10.029\n0.201\u00b10.015\n0.232\u00b10.011\n0.055\u00b10.002\n0.161\u00b10.014\nTAM\n0.621\u00b10.004\n0.435\u00b10.002\n0.638\u00b10.009\n0.293\u00b10.001\n0.497\u00b10.004\n0.073\u00b10.001\n0.110\u00b10.001\n0.294\u00b10.006\n0.053\u00b10.000\n0.133\u00b10.002\nPReNet\n0.460\u00b10.042\n0.557\u00b10.033\n0.497\u00b10.016\n0.615\u00b10.007\n0.532\u00b10.025\n0.044\u00b10.004\n0.205\u00b10.032\n0.232\u00b10.010\n0.129\u00b10.005\n0.153\u00b10.013\nDevNet\n0.468\u00b10.040\n0.537\u00b10.083\n0.739\u00b10.032\n0.621\u00b10.026\n0.591\u00b10.045\n0.045\u00b10.005\n0.200\u00b10.060\n0.606\u00b10.021\n0.142\u00b10.022\n0.248\u00b10.027\nDCI\n0.614\u00b1 0.093\n0.629\u00b10.061\n0.847\u00b10.013\n0.637\u00b10.059\n0.681\u00b10.033\n0.083\u00b10.038\n0.288\u00b10.060\n0.558\u00b10.035\n0.157\u00b10.024\n0.272\u00b10.015\nBWGNN\n0.598\u00b10.008\n0.570\u00b10.026\n0.829\u00b10.030\n0.674\u00b10.022\n0.668\u00b10.022\n0.068\u00b10.004\n0.286\u00b10.014\n0.620\u00b10.060\n0.167\u00b10.015\n0.285\u00b10.023\nAMNet\n0.603\u00b10.004\n0.606\u00b10.008\n0.860\u00b10.004\n0.604\u00b10.014\n0.668\u00b10.008\n0.068\u00b10.002\n0.237\u00b10.010\n0.739\u00b10.006\n0.143\u00b10.009\n0.297\u00b10.007\nGHRN\n0.611\u00b10.014\n0.533\u00b10.024\n0.729\u00b10.038\n0.659\u00b10.035\n0.633\u00b10.011\n0.068\u00b10.003\n0.181\u00b10.020\n0.552\u00b10.060\n0.148\u00b10.019\n0.237\u00b10.024\nG. ENS\n0.518\u00b10.009\n0.610\u00b10.009\n0.699\u00b10.011\n0.536\u00b10.017\n0.591\u00b10.012\n0.053\u00b10.002\n0.232\u00b10.012\n0.447\u00b10.017\n0.094\u00b10.008\n0.207\u00b10.010\nG. SMOTE\n0.464\u00b10.046\n0.643\u00b10.056\n0.835\u00b10.043\n0.716\u00b10.020\n0.665\u00b10.041\n0.044\u00b10.004\n0.232\u00b10.043\n0.678\u00b10.066\n0.167\u00b10.018\n0.280\u00b10.033\nBCE\n0.650\u00b10.026\n0.662\u00b10.015\n0.866\u00b10.005\n0.658\u00b10.023\n0.709\u00b10.017\n0.070\u00b10.007\n0.293\u00b10.018\n0.723\u00b10.009\n0.170\u00b10.015\n0.314\u00b10.012\nNSReg (Ours)\n0.836\u00b10.031\n0.752\u00b10.019\n0.953\u00b10.008\n0.690\u00b10.025\n0.808\u00b10.021\n0.221\u00b10.057\n0.417\u00b10.024\n0.866\u00b10.021\n0.196\u00b10.020\n0.425\u00b10.031\nCurve (AUC-ROC) and the Area Under Precision-Recall Curve (AUC-PR). In particular, AUC-ROC\nmeasures both true positives and false positives, while AUC-PR summarises the precision and recall\nof the anomaly classes, offers a focused measure exclusively on the anomaly classes.\nCompeting Methods. NSReg is compared with 13 competing methods from multiple related areas,\nincluding four popular unsupervised methods, eight state-of-the-art (SOTA) supervised methods and\none baseline method. Specifically, DOMINANT [7], GGAN [5], CoLA [23], and TAM [35] are\nimplemented as our unsupervised baselines due to their popularity and competitive performance\namong unsupervised GAD methods. The SOTA methods consist of recently proposed supervised\nanomaly detectors for Euclidean data: DevNet [28], PReNet [27], and those for graph data: DCI [46],\nBWGNN [43], AMNet [2] and GHRN [14], and two imbalanced node classification methods as well:\nGraphSMOTE [51] and GraphENS [31]. The classification method using binary cross-entropy (BCE)\nis considered as a baseline model.\nImplementation Details. NSReg comprises a representation learner featuring a two-layer Graph-\nSAGE [15], followed by a two-layer projection network, each containing 64 hidden units per layer.\nNSReg is optimised using the Adam [20] optimiser for 200 epochs for the Photo, Computers, and\nCS datasets with a learning rate of 1 \u00d7 10\u22123, and for 400 epochs for Yelp with a learning rate of\n5 \u00d7 10\u22123 due to its larger number of nodes. We set the number of labelled anomalies to 50 and the\npercentage of labelled normal nodes to 5% by default. Each batch of training nodes includes all\nlabelled anomalies and randomly sampled number of labelled normal nodes capped at 512. Similarly,\neach batch of relations is capped at 512, with an equal number of samples for each relation type. The\ndefault value of \u03b1 is set to 0.8 and \u03bb is set to 1 for all datasets. The analysis of their sensitivity can be\nfound in Appendix C.6, where NSReg maintains stable performance, demonstrating its robustness to\nthe choice of hyperparameters. Further implementation details are provided in Appendix B.4.\n4.1\nMain Results\nGAD Performance on Both Seen and Unseen Anomalies. We first evaluate the performance of\nNSReg on detecting all test anomalies (i.e., V \\ Vtrain, including samples of both seen and unseen\nanomaly classes). The results are reported in Table 1 (the upper part). We can see that NSReg\nachieves consistently and significantly better performance than all competing methods across all\ndatasets in terms of both AUC-ROC and AUC-PR. On average, NSReg demonstrates a significant\nperformance advantage over all competing methods, among which the supervised baselines prove\nto be considerably more effective than the unsupervised ones. Specifically, for AUC-ROC, NSReg\nachieves an improvement of 9.7% and 12.7% compared to the best and second-best competing\nmethods, BCE and AMNet, respectively. Additionally, NSReg shows 16.9% and 24.2% improvement\ncompared to these two methods in terms of AUPR. The significant overall performance gain is\nmainly attributed to NSReg\u2019s strong capability of detecting unseen anomalies, while at the same time\nachieving an effective fitting of the seen anomalies.\n7\nGAD Generalisation to Unseen Anomalies. The capability of detecting unseen anomalies in the\ntest data (i.e., Vtest\nn\n\u222aVunseen\na\n) is more indicative for evaluating a model\u2019s open-set GAD ability. As\nshown in Table 1 (lower part), NSReg again achieves significantly better performance, compared\nto the competing methods. Notably, on average, NSReg exhibits a greater margin of improvement\non the unseen anomaly classes, with a 14% and 21% improvement over the top-2 performing\ncompeting methods in terms of AUC-ROC. Similarly, a 35% and 43% performance gain over the top-\n2 performing competing methods is observed for AUC-PR. The substantially enhanced performance\nunderscores NSReg\u2019s strong ability in reducing the errors of detecting unseen anomalies as normal.\nFigure 3: AUC-PR on all anomalies w.r.t.\nnumber of labelled anomalies.\nData Efficiency. We investigate NSReg\u2019s data effi-\nciency by varying the number of seen anomalies around\nthe default setting, including 5, 15, 25, 50, and 100, in\norder to understand the model\u2019s capacity to handle vari-\nations in data availability. The results are illustrated\nin Figure 3. Due to the space constraint, we report\nresults on the AUC-PR results on all test anomalies and\nprovide the AUC-ROC results in Appendix C.7. The\nresults show that NSReg consistently achieves signif-\nicantly improved AUC-PR performance on the overall\nanomaly detection across the complete spectrum of the\nPhoto, Computers, and CS datasets. On Yelp, NSReg\neither outperforms, or is on par with the competing\nmethods across the entire range for overall performance.\nSimilar findings can be observed on detecting unseen\nanomalies (see Figures 8 and 9).\nTable 2: Our NSR as a plugin module in super-\nvised AD models DevNet, DCI and BWGNN.\nMetric\nAUC-ROC\nAUC-PR\nDataset\nOri\n+NSR\nDiff.\nOri\n+NSR\nDiff.\nDevNet\nPhoto\n0.599\n0.684\n+0.085\u2191\n0.223\n0.424\n+0.201\u2191\nComputers\n0.606\n0.646\n+0.040\u2191\n0.284\n0.340\n+0.056\u2191\nCS\n0.769\n0.949\n+0.180\u2191\n0.684\n0.872\n+0.188\u2191\nYelp\n0.675\n0.684\n+0.009\u2191\n0.315\n0.315\n+0.008\u2191\nAvg. Diff.\n+0.079\u2191\n+0.113\u2191\nDCI\nPhoto\n0.772\n0.865\n+0.093\u2191\n0.452\n0.577\n+0.125\u2191\nComputers\n0.683\n0.738\n+0.055\u2191\n0.427\n0.501\n+0.074\u2191\nCS\n0.856\n0.857\n+0.001\u2191\n0.635\n0.623\n-0.012\nYelp\n0.689\n0.740\n+0.051\u2191\n0.351\n0.387\n+0.036\u2191\nAvg. Diff.\n+0.050\u2191\n+0.056\u2191\nBWGNN\nPhoto\n0.728\n0.802\n+0.074\u2191\n0.313\n0.530\n+0.217\u2191\nComputers\n0.635\n0.726\n+0.091\u2191\n0.348\n0.458\n+0.110\u2191\nCS\n0.845\n0.879\n+0.034\u2191\n0.687\n0.718\n+0.031\u2191\nYelp\n0.727\n0.747\n+0.020\u2191\n0.366\n0.394\n+0.028\u2191\nAvg. Diff.\n+0.055\u2191\n+0.097\u2191\nNSReg as a Plug-and-Play Module. This section\nstudies the effect of utilising the NSR module of\nNSReg as a plugin module to improve three other\nsupervised methods. Table 2 shows the experiment\nresults on detecting all test anomalies, with the origi-\nnal BWGNN/DCI/DevNet as the baselines. We can\nsee that the NSR-enabled BWGNN/DCI/DevNet\nyield significant performance improvements across\nall datasets in both evaluation metrics (see Table 7\nsimilar results on unseen anomalies). In particular,\non average, it improves DevNet by over 10% for\nAUC-ROC and 30% for AUC-PR, for all anoma-\nlies and unseen anomalies. Similarly, for BWGNN,\nits NSR-variant outperforms the original model by\nover 7% in terms of AUC-ROC on both test sets and\nmore than 14% for AUC-PR. For DCI, NSR also\nyields similar level of performance improvement. The consistent improvement demonstrates not\nonly the high generalisability of our proposed NSR but also the importance of enforcing structural\nnormality in supervised GAD.\nTable 3: Large-scale GAD results. \u201c-\u201d means\nunavailable result due to out-of-memory.\nMetric\nDatasets\nogbn-arxiv\nogbn-proteins\nT-Finance\navg.\nAUC-\nROC\n(All)\nPReNet\n0.581\u00b10.006\n0.613\u00b10.035\n0.892\u00b10.017\n0.695\u00b10.019\nDevNet\n0.601\u00b10.015\n0.622\u00b10.057\n0.654\u00b10.210\n0.626\u00b10.094\nDCI\n0.566\u00b10.041\n0.815\u00b10.037\n0.763\u00b1 0.111\n0.715\u00b10.063\nBWGNN\n0.587\u00b10.013\n0.727\u00b10.067\n0.922\u00b10.011\n0.745\u00b10.030\nAMNet\n0.600\u00b10.049\n0.711\u00b10.077\n0.889\u00b10.024\n0.733\u00b10.050\nGHRN\n0.588\u00b10.009\n0.674\u00b10.019\n0.923\u00b10.010\n0.728\u00b10.013\nG.ENS\n-\n-\n0.847\u00b10.087\n-\nG.SMOTE\n-\n-\n0.875\u00b10.016\n-\nBCE\n0.592\u00b10.004\n0.568\u00b10.020\n0.922\u00b10.011\n0.694\u00b10.012\nNSReg\n0.659\u00b10.012\n0.843\u00b10.029\n0.929\u00b10.007\n0.810\u00b10.016\nAUC-\nPR\n(All)\nPReNet\n0.288\u00b10.004\n0.432\u00b10.028\n0.571\u00b10.140\n0.430\u00b10.057\nDevNet\n0.304\u00b10.009\n0.436\u00b10.017\n0.323\u00b10.327\n0.354\u00b10.118\nDCI\n0.275\u00b10.033\n0.597\u00b10.062\n0.264\u00b10.240\n0.379\u00b10.112\nBWGNN\n0.263\u00b10.009\n0.582\u00b10.026\n0.746\u00b10.023\n0.530\u00b10.019\nAMNet\n0.272\u00b10.053\n0.576\u00b10.053\n0.644\u00b10.046\n0.497\u00b10.051\nGHRN\n0.271\u00b10.007\n0.564\u00b10.005\n0.727\u00b10.031\n0.521\u00b10.014\nG.ENS\n-\n-\n0.332\u00b10.076\n-\nG.SMOTE\n-\n-\n0.573\u00b10.077\n-\nBCE\n0.310\u00b10.003\n0.524\u00b10.008\n0.726\u00b10.034\n0.520\u00b10.015\nNSReg\n0.336\u00b10.006\n0.723\u00b10.020\n0.757\u00b10.020\n0.605\u00b10.125\nLarge-scale GAD Performance.\nWe compare\nNSReg\u2019s GAD performance with semi-supervised\nbaselines on all test nodes and unseen anomalies of\ntwo large GAD datasets, as presented in Table 3 and\nTable 12 in Appendix C.4 . Notably, NSReg con-\nsistently demonstrates advantageous performance,\nas observed in other datasets in Table 1. More-\nover, NSReg exhibits superior efficiency in terms\nof data utilization for large-scale GAD tasks\u2014it\nachieves effective structural regularization and sig-\nnificantly improves generalisation with only 100\nseen anomalies, accounting for only less than 1%\nof total anomalies.\n8\n4.2\nAblation Study\nThis section empirically explores the effectiveness\nand significance of enforcing our structural normality within node representations by answering the\nfollowing questions, with AUC-PR results reported in Table 4 and a visualisations included in Figure\n4. We include the results in AUC-ROC in Table 9 in Appendix C.2 due to space limit.\nFigure 4: Node representations learned via various\nregularisation schemes on Photo.\nNo Reg.\nSAD\nCL (u.s.)\nCL (s.)\nNSR (ours)\n0\n1\nNormal\nSeen\nAnomalies\nUnseen\nAnomalies\nHow important is graph structural infor-\nmation when enforcing normality?\nWe\nanswer this question by replacing the NSR\nmodule with the one-class hypersphere objec-\ntive in Deep SAD (SAD) [40], which min-\nimises the volume of normal node represen-\ntations to learn the normality without consid-\nering the graph structure information. Table\n4 shows that NSR can significantly outper-\nform the one-class learning SAD, highlight-\ning the advantage of enforcing graph structure-\ninformed normality.\nThis is because, by\nsimply tightening the normal representations\nwithout considering their structural relation-\nships, SAD may not necessarily learn dis-\ncriminative unseen anomaly representations, as illustrated in Figure 4(SAD). We also found\nthe one-class learning-based SAD is unstable due to its notorious risk of model collapse,\nwhile our NSR does not have this issue due to the fine-grained structural relation prediction.\nTable 4: Our proposed NSR vs other regularisers\nin AUC-PR.\nAll anomalies\nDataset\nSAD\nCL (s.)\nCL (u.s.)\nNSR (dual)\nNSR (Ours)\nPhoto\n0.269\u00b10.215\n0.513\u00b10.018\n0.583\u00b10.012\n0.623\u00b10.029\n0.640\u00b10.036\nComputers\n0.381\u00b10.190\n0.492\u00b10.014\n0.539\u00b10.024\n0.539\u00b10.023\n0.559\u00b10.018\nCS\n0.700\u00b10.016\n0.795\u00b10.009\n0.843\u00b10.020\n0.858\u00b10.042\n0.889\u00b10.016\nYelp\n0.175\u00b10.046\n0.392\u00b10.013\n0.330\u00b10.078\n0.399\u00b10.021\n0.398\u00b10.014\naverage\n0.381\u00b10.117\n0.548\u00b10.014\n0.574\u00b10.034\n0.605\u00b10.029\n0.622\u00b10.021\nUnseen anomalies\nDataset\nSAD\nCL (s.)\nCL (u.s.)\nNSR (dual)\nNSR (Ours)\nPhoto\n0.053\u00b10.004\n0.068\u00b10.004\n0.133\u00b10.019\n0.207\u00b10.049\n0.221\u00b10.057\nComputers\n0.285\u00b10.134\n0.318\u00b10.018\n0.379\u00b10.032\n0.389\u00b10.028\n0.417\u00b10.024\nCS\n0.631\u00b10.019\n0.743\u00b10.010\n0.797\u00b10.025\n0.834\u00b10.046\n0.866\u00b10.021\nYelp\n0.090\u00b10.017\n0.178\u00b10.011\n0.152\u00b10.045\n0.190\u00b10.023\n0.196\u00b10.020\nAverage\n0.265\u00b10.044\n0.327\u00b10.011\n0.365\u00b10.030\n0.405\u00b10.037\n0.425\u00b10.031\nWhy is our proposed NSR more effective at\nstructural regularisation? To answer this ques-\ntion, we replace our NSR module with super-\nvised and unsupervised contrastive learning (CL\n(s.) and CL (u.s.) in Table 4), which enforce\nsimilarities between nodes based on their con-\nnections. NSReg consistently outperforms both\nbaselines on all tests for both metrics. This is\nbecause the NSR module is designed to ensure\nthat its enforced normality are reflected in the\nshare representation space, therefore resulting\nin a more stringent normal class subspace and\nsignificantly mitigates overfitting on the seen anomalies, as can also be observed in Figure 4(NSR).\nCan we just augment the seen anomalies to improve the generalisation on the unseen ones?\nThis can be answered by comparing the results of NSReg with graph augmentation-based methods\nG.SMOTE and G.ENS in Table 1. NSReg significantly outperforms the two methods, suggesting that\nmerely augmenting seen anomalies is inadequate for generalising to unseen ones. This is due to the\nfact that the augmented anomaly samples mainly resemble only the seen abnormalities, which may\neven further increase the risk of overfitting on the seen anomalies.\nCan we perform the relation prediction in NSR with less types of relations? We compare NSR\nwith its variant that is trained without the second relation R(n,u,n) (NSR (dual) in short). Table 4\nshows that the default NSR outperforms NSR (dual) by a non-trivial margin on almost all cases. This\nis because R(n,u,n) is essential for defining our normality hierarchy, enabling finer granularity in\nrelation modeling, preserving the structural differences between more closely related normal nodes\nand other nodes, on top of enforcing the distinction between normal and anomaly nodes.\n5\nConclusion and Future Work\nThis paper introduces a novel open-set GAD approach, Normal Structure Regularisation (NSReg),\nwhich employs a plug-and-play regularisation term to enhance anomaly-discriminative normality\nfrom graph structures, thereby reinforcing strong normality in node representations during supervised\nGAD model training.By aligning node representations with their structural relationships relative\n9\nto the normal nodes, NSReg establishes a stringent decision boundary for the normal class and\nenhances its generalisation to unseen anomalies by reducing the errors of detecting them as normal.\nComprehensive empirical results show the superiority of NSReg in detecting both seen and unseen\nanomalies compared to 13 competing methods on sever real-world graph datasets.\nFuture work and limitation. Despite impressive effectiveness in open-GAD, NSReg currently\nmodels the normality based solely on the 1-hop neighbourhood of the normal nodes. Expanding this\nto include relations beyond the immediate neighbours could provide additional insights for open-set\nGAD; however, such an expansion would necessitate more complex relation sampling processes,\nthereby introducing additional overhead. We leave this for our future work.\nReferences\n[1] Leman Akoglu, Hanghang Tong, and Danai Koutra. Graph based anomaly detection and\ndescription: a survey. Data Mining and Knowledge Discovery, 29(3):626\u2013688, 2015.\n[2] Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and Weihao Jiang.\nCan Abnormality be Detected by Graph Neural Networks? In Proc. IJCAI, 2022.\n[3] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM\ncomputing surveys (CSUR), 41(3):1\u201358, 2009.\n[4] Nitesh V Chawla, Kevin W Bowyer, Lawrence O Hall, and W Philip Kegelmeyer. Smote:\nsynthetic minority over-sampling technique. Journal of Artificial Intelligence Research, 16:321\u2013\n357, 2002.\n[5] Zhenxing Chen, Bo Liu, Meiqing Wang, Peng Dai, Jun Lv, and Liefeng Bo. Generative\nadversarial attributed network anomaly detection. In Proc. CIKM, pages 1989\u20131992, 2020.\n[6] Kaize Ding, Jundong Li, Nitin Agarwal, and Huan Liu. Inductive anomaly detection on\nattributed networks. In Proc. IJCAI, 2021.\n[7] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. Deep anomaly detection on attributed\nnetworks. In Proc. SDM, pages 594\u2013602. SIAM, 2019.\n[8] Kaize Ding, Kai Shu, Xuan Shan, Jundong Li, and Huan Liu. Cross-domain graph anomaly\ndetection. IEEE Transactions on Neural Networks and Learning Systems, 33(6):2406\u20132415,\n2022.\n[9] Kaize Ding, Zhe Xu, Hanghang Tong, and Huan Liu. Data augmentation for deep graph\nlearning: A survey. SIGKDD Explor. Newsl., page 61\u201377, dec 2022.\n[10] Kaize Ding, Qinghai Zhou, Hanghang Tong, and Huan Liu. Few-shot network anomaly\ndetection via cross-network meta-learning. In Proc. WWW, page 2448\u20132456, 2021.\n[11] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. Enhancing graph\nneural network-based fraud detectors against camouflaged fraudsters. In Proc. CIKM, 2020.\n[12] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric.\nIn ICLR Workshop on Representation Learning on Graphs and Manifolds, 2019.\n[13] Jing Gao, Feng Liang, Wei Fan, Chi Wang, Yizhou Sun, and Jiawei Han. On community outliers\nand their efficient detection in information networks. In Proc. SIGKDD, pages 813\u2013822, 2010.\n[14] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yongdong Zhang.\nAddressing heterophily in graph anomaly detection: A perspective of graph spectrum. In Proc\nWWW, pages 1528\u20131538, 2023.\n[15] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large\ngraphs. Adv. NIPS, 30, 2017.\n[16] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele\nCatasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs.\nAdv. NeurIPS, 33:22118\u201322133, 2020.\n[17] Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing\nHe. Auc-oriented graph neural network for fraud detection. In Proc. WWW, page 1311\u20131321,\n2022.\n10\n[18] Minqi Jiang, Chaochuan Hou, Ao Zheng, Xiyang Hu, Songqiao Han, Hailiang Huang, Xiangnan\nHe, Philip S Yu, and Yue Zhao. Weakly supervised anomaly detection: A survey. arXiv preprint\narXiv:2302.04549, 2023.\n[19] Justin M Johnson and Taghi M Khoshgoftaar. Survey on deep learning with class imbalance.\nJournal of Big Data, 6(1):1\u201354, 2019.\n[20] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. Proc. ICLR,\n2014.\n[21] Jundong Li, Harsh Dani, Xia Hu, and Huan Liu. Radar: Residual analysis for anomaly detection\nin attributed networks. In Proc. IJCAI, pages 2152\u20132158, 2017.\n[22] Yuening Li, Xiao Huang, Jundong Li, Mengnan Du, and Na Zou. Specae: Spectral autoencoder\nfor anomaly detection in attributed networks. In Proc. CIKM, pages 2233\u20132236, 2019.\n[23] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis. Anomaly\ndetection on attributed networks via contrastive self-supervised learning. IEEE Transactions on\nNeural Networks and Learning Systems, 33(6):2378\u20132392, 2021.\n[24] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong, and\nLeman Akoglu. A comprehensive survey on graph anomaly detection with deep learning. IEEE\nTransactions on Knowledge and Data Engineering, 2021.\n[25] Leland McInnes, John Healy, Nathaniel Saul, and Lukas Grossberger. Umap: Uniform manifold\napproximation and projection. The Journal of Open Source Software, 3(29):861, 2018.\n[26] Guansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\nanomaly detection: A review. ACM computing surveys (CSUR), 54(2):1\u201338, 2021.\n[27] Guansong Pang, Chunhua Shen, Huidong Jin, and Anton van den Hengel. Deep weakly-\nsupervised anomaly detection. In Proc, SIGKDD, pages 1795\u20131807, 2023.\n[28] Guansong Pang, Chunhua Shen, and Anton van den Hengel. Deep anomaly detection with\ndeviation networks. In Proc. SIGKDD, pages 353\u2013362, 2019.\n[29] Guansong Pang, Anton van den Hengel, Chunhua Shen, and Longbing Cao. Toward deep\nsupervised anomaly detection: Reinforcement learning from partially labeled anomaly data. In\nProc. SIGKDD, pages 1298\u20131308, 2021.\n[30] Hyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for\nanomaly detection. In Proc. CVPR, pages 14372\u201314381, 2020.\n[31] Joonhyung Park, Jaeyun Song, and Eunho Yang. Graphens: Neighbor-aware ego network\nsynthesis for class-imbalanced node classification. In Proc. ICLR, 2021.\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan,\nTrevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas\nK\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy,\nBenoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. Pytorch: an imperative style,\nhigh-performance deep learning library. In Adv. NeurIPS, 2019.\n[33] Zhen Peng, Minnan Luo, Jundong Li, Huan Liu, and Qinghua Zheng. Anomalous: A joint\nmodeling approach for anomaly detection on attributed networks. In Proc. IJCAI, pages\n3513\u20133519, 2018.\n[34] Bryan Perozzi and Leman Akoglu. Scalable anomaly ranking of attributed neighborhoods. In\nProc. ICDM, pages 207\u2013215, 2016.\n[35] Hezhe Qiao and Guansong Pang. Truncated affinity maximization: One-class homophily\nmodeling for graph anomaly detection. Adv. NeurIPS, 36, 2024.\n[36] Hezhe Qiao, Qingsong Wen, Xiaoli Li, Ee-Peng Lim, and Guansong Pang. Generative semi-\nsupervised graph anomaly detection. arXiv preprint arXiv:2402.11887, 2024.\n[37] Liang Qu, Huaisheng Zhu, Ruiqi Zheng, Yuhui Shi, and Hongzhi Yin. Imgagn: Imbalanced\nnetwork embedding via generative adversarial graph networks. In Proc. SIGKDD, pages\n1390\u20131398, 2021.\n[38] Shebuti Rayana and Leman Akoglu. Collective opinion spam detection: Bridging review\nnetworks and metadata. In Proc. SIGKDD, pages 985\u2013994, 2015.\n11\n[39] Karsten Roth, Latha Pemula, Joaquin Zepeda, Bernhard Sch\u00f6lkopf, Thomas Brox, and Peter\nGehler. Towards total recall in industrial anomaly detection. In Proc. CVPR, pages 14318\u201314328,\n2022.\n[40] Lukas Ruff, Robert A. Vandermeulen, Nico G\u00f6rnitz, Alexander Binder, Emmanuel M\u00fcller,\nKlaus-Robert M\u00fcller, and Marius Kloft. Deep semi-supervised anomaly detection. In Proc.\nICLR, 2020.\n[41] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan G\u00fcnnemann.\nPitfalls of graph neural network evaluation. Relational Representation Learning Workshop,\nNeurIPS 2018, 2018.\n[42] Kihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister. Learning and\nevaluating representations for deep one-class classification. Proc. ICLR, 2021.\n[43] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. Rethinking graph neural networks for anomaly\ndetection. In Proc. ICML, pages 21076\u201321089. PMLR, 2022.\n[44] Andrew Z. Wang, Rex Ying, Pan Li, Nikhil Rao, Karthik Subbian, and Jure Leskovec. Bipartite\ndynamic representations for abuse detection. In Proc. SIGKDD, page 3638\u20133648, 2021.\n[45] Qizhou Wang, Guansong Pang, Mahsa Salehi, Wray Buntine, and Christopher Leckie. Cross-\ndomain graph anomaly detection via anomaly-aware contrastive alignment. In Proc. AAAI,\nvolume 37, pages 4676\u20134684, 2023.\n[46] Yanling Wang, Jing Zhang, Shasha Guo, Hongzhi Yin, Cuiping Li, and Hong Chen. Decoupling\nrepresentation learning and classification for gnn-based anomaly detection. In Proc. SIGIR,\npages 1239\u20131248, 2021.\n[47] Zheng Wang, Xiaojun Ye, Chaokun Wang, Jian Cui, and Philip S. Yu. Network embedding\nwith completely-imbalanced labels. IEEE Transactions on Knowledge and Data Engineering,\n33(11):3634\u20133647, 2021.\n[48] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and S Yu Philip. A\ncomprehensive survey on graph neural networks. IEEE Transactions on Neural Networks and\nLearning Systems, 32(1):4\u201324, 2020.\n[49] Wenchao Yu, Wei Cheng, Charu C Aggarwal, Kai Zhang, Haifeng Chen, and Wei Wang.\nNetwalk: A flexible deep embedding approach for anomaly detection in dynamic networks. In\nProceedings of the 24th ACM SIGKDD international conference on knowledge discovery &\ndata mining, pages 2672\u20132681, 2018.\n[50] Houssam Zenati, Manon Romain, Chuan-Sheng Foo, Bruno Lecouat, and Vijay Chandrasekhar.\nAdversarially learned anomaly detection. In Proc. ICDM, pages 727\u2013736. IEEE, 2018.\n[51] Tianxiang Zhao, Xiang Zhang, and Suhang Wang. Graphsmote: Imbalanced node classification\non graphs with graph neural networks. In Proc. WSDM, pages 833\u2013841, 2021.\n[52] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng Jiang.\nError-bounded graph anomaly loss for gnns. In Proc. CIKM, pages 1873\u20131882, 2020.\n[53] Yue Zhao and Maciej K Hryniewicki. Xgbod: improving supervised outlier detection with\nunsupervised representation learning.\nIn 2018 International Joint Conference on Neural\nNetworks (IJCNN), pages 1\u20138. IEEE, 2018.\n[54] Shuang Zhou, Xiao Huang, Ninghao Liu, Huachi Zhou, Fu-Lai Chung, and Long-Kai Huang.\nImproving generalizability of graph anomaly detection models via data augmentation. IEEE\nTransactions on Knowledge and Data Engineering, 2023.\n[55] Zhi-Hua Zhou and Xu-Ying Liu. Training cost-sensitive neural networks with methods address-\ning the class imbalance problem. IEEE Transactions on Knowledge and Data Engineering,\npages 63\u201377, 2006.\n12\nAppendix\nThe appendix section is structured as follows. In Appendix A, we details the proof of the proposition 1.\nIn Appendix B, we provide extended details of the experimental setup, including dataset information,\nexperimental protocol, and implementation specifics. In Appendix C , we provide additional results\nthat were not included in the main section due to space limitations.\nA\nProof of Proposition 1\nZ\n\u03c3(Z)\n\u03c3\nza2\nzn\nza1\n\u03c3(za1)\n\u03c3(za2)\n\u03c3(zn)\n\u03c3(zm)\nPath l\nFigure 5: Proposition 1 proof intuition: Z and \u03c3(Z) represent the shared node representation spaces\nbefore and after applying the scaling function \u03c3, respectively. In this plot, the grey region represents\nthe normal subspace Zn, and the union of the grey and red regions represents Zm. The boundary of\nthe normal subspace, M, is depicted by the black lines. The goal is to prove that the red regions,\nwhich denote the neighbourhood of anomalies, do not exist within the normal subspace for a well-\ntrained relation discriminator.\nFor convenience, we define a labelling function C with a range of [0, 1]. This function assigns a\nnormality score of 0 to relations between normal nodes and anomaly nodes. In contrast, relations\nexclusively between normal nodes receive scores ranging from a significantly greater value than 0,\ndenoted as \u03b1, to a maximum of 1. For simplicity, we use the mapping of our default choice of relation\nmodelling function F. It is worth noting that any function that satisfies the condition specified in\nSec. 3.3 can be proved using the same approach. We show via proof by contradiction that, if F is\nwell-trained to distinguish the normal nodes exclusive relations and those that involves anomalies in\nnode representation space shared with a semi-supervised anomaly detector, no anomaly node lies in\nthe normal region within the shared representation space of \u03b7 and F, where normal nodes form a\ndense subspace.\nWe assume the normal region in the shared representation space is a connected open set, due to\nthe high similarity and dense distribution of normal representations forced by semi-supervised\ndiscriminative loss LAD. For a well-trained F and any given normal node from the region n, suppose\nthere are two anomaly nodes, a1 and a2, where a1 lies outside the normal region and a2 overlaps\nwith the normal region. Note that such a1 can be easily found, as LAD will ensure a sufficient number\nof seen anomalies residing outside of the normal subspace Zn. Consider a path connecting these two\nanomaly nodes:\nlt = t\u03c3(za1) + (1 \u2212t)\u03c3(za2), t \u2208(0, 1).\n(6)\nThe homeomorphic of the Sigmoid function will ensure that we are able to find the scaled representa-\ntion of a normal node \u03c3(zm) along the path at t = tm. Then we have:\n\u03c3(zm) = tm\u03c3(za1) + (1 \u2212tm)\u03c3(za2).\n(7)\nApplying F to the relation between n and m, we have:\n13\nF(hnm) = WC \u00b7 hnm + b\n= WC(diag(\u03c3(zn) \u00b7 WE)\u03c3(zm)) + b\n= WC(diag(\u03c3(zn) \u00b7 WE)(tm\u03c3(za1) + (1 \u2212tm)\u03c3(za2)) + b\n= tm\n\u0010\nWC diag\n\u0000\u03c3(zn) \u00b7 WE\n\u0001\n\u03c3(za1) + b\n\u0011\n+ (1 \u2212tm)\n\u0010\nWC diag\n\u0000\u03c3(zn) \u00b7 WE\n\u0001\n\u03c3(za2) + b\n\u0011\n= tm\n\u0010\nWChna1 + b\n\u0011\n+ (1 \u2212tm)\n\u0010\nWChna2 + b\n\u0011\n= tmF(hna1) + (1 \u2212tm)F(hna2).\nSince F is well trained, we have F(hna1) < \u03b3 \u2212\u03b4 and F(hna2) < \u03b3 \u2212\u03b4, where \u03c3(\u03b3) = \u03b1 and\n\u03b4 \u2208R(0,\u03b3). Therefore, we have :\nF(hnm) = tmF(hna1) + (1 \u2212tm)F(hna2) < \u03b3 \u2212\u03b4\nqnm = \u03c3(F(hnm)) < \u03c3(\u03b3 \u2212\u03b4) < \u03b1\nwhich contradicts with the fact that qnm > \u03b1.\nB\nMore Experimental Details\nB.1\nPseudocode for Training NSReg\nThe training procedure of NSReg is described in the following pseudocode:\nAlgorithm 1 Training NSReg\nRequire: G and Y train s.t. Y train \u226a|V|\nEnsure: \u03d5 : (G, v) \u2192R - an anomaly scoring mapping\n1: Initialize variables\n2: for i = 1 in [1, ..., nepochs] do\n3:\nV \u2190Sample bAD nodes from Vtrain.\n4:\nR \u2190Sample bNSR relations as described in Sec. 3.4.\n5:\nLAD \u2190Compute the supervised GAD loss.\n6:\nPerform gradient descent for \u03b7.\n7:\nLNSR \u2190Compute the NSReg loss.\n8:\nPerform gradient descent for F, E.\n9:\nL \u2190Compute the total loss as in Eq. 5\n10:\nPerform gradient descent for \u03c8.\n11: end for\n12: return \u03d5\nB.2\nDatasets\nB.2.1\nDataset Statistics\nTable 5 shows the statistics of the seven datasets used. Note that the Yelp dataset is a heterogeneous\ngraph containing three different views. We choose the edge subset of the \u201cReview-User-Review\n(RUR)\" view in our experiments.\n14\nTable 5: A summary of dataset statistics\n# Dim.\n# Nodes\n# Edge\n# Ano. Classes\n# Ano.\nPAno. %\nPhoto\n745\n7,487\n119,043\n2\n369\n4.92\nComputers\n767\n13,381\n245,778\n5\n2,064\n15.42\nCS\n6,805\n18,333\n81,894\n8\n4,159\n22.69\nYelp\n32\n45,954\n98,630\n2\n6,677\n14.53\nogbn-arxiv\n128\n169,343\n1,166,243\n4\n27,830\n16.43\nogbn-proteins\n8\n132,534\n39,561,252\n2\n10,693\n8.07\nT-Finance\n10\n39,357\n21,222,543\n2\n1,803\n4.58\nB.2.2\nDataset Preprocessing\nFor the imbalance attributed node classification datasets Photo, Computers, CS and ogbn-proteins, we\ntreat each class that has less than 5% of the total number of nodes of the graph as an anomaly class\nand the remaining classes as normal. In the ogbn-arxiv dataset, with the original graph containing\n40 subclasses, for experimental efficiency, any class representing 3% to 5% of the total nodes is\nconsidered an anomaly class. Note that the normal class is defined as the combination of all the\nmajority classes, from which labelled normal nodes are randomly and uniformly drawn, and remain\nconsistent through the entire experiment.\nFor the Yelp and T-Finance datasets, which only contain binary anomaly labels, we first apply an\nunsupervised representation learning method Deep Graph Infomax (DGI) for representation learning\nand then apply k-means clustering to partition the anomaly class into two anomaly subclasses using\ntheir node representations. Specifically, our DGI model employs a two-layer GraphSAGE as the\nrepresentation learner with 128 hidden units per layer. The model is optimised for 1,000 epochs using\nthe Adam optimiser with the learning rate and the batch size set to 1 \u00d7 10\u22123 and 512, respectively. To\nfurther magnify the distribution shift, instead of employing random sampling, we conduct weighted\nrandom sampling to generate training anomaly nodes, where the probability associated with each\nanomaly node is proportional to its distance from its corresponding cluster centroid.\nB.3\nExperimental Protocol\nAlgorithm 2 Open-set GAD Experimental Protocol\nRequire: G, Y, and anomaly class index idxanom.\nEnsure: The overall and unseen AUC-ROC and AUC-PR values.\n1: Vn \u2190{v | yv /\u2208idxanom}\n2: Vtrain\nn\n\u2190Sample the training normal nodes.\n3: for c in idxanom do\n4:\nVseen\na\n\u2190{v | yv == c}\n5:\nVunseen\na\n\u2190{v | yv not in idxanom}\n6:\nVtrain \u2190sample from Vseen\na\nand Vtrain\nn\n.\n7:\n\u03d5 \u2190Train an anomaly detection network using NSReg as described in Algorithm 1.\n8:\nAUC-ROCall\nc , AUC-PRall\nc\n\u2190evaluate \u03d5 using V \\ Vtrain.\n9:\nAUC-ROCuseen\nc\n, AUC-PRunseen\nc\n\u2190evaluate \u03d5 using Vn \u222aVunseen \\ Vtrain.\n10:\nRecord class AUC-ROC and AUC-PR.\n11: end for\n12: Calculate the average performance AUC-ROCall,\nAUC-PRall,\nAUC-ROCunseen,\nand\nAUC-PRunseen.\n13: return AUC-ROCall, AUC-PRall, AUC-ROCunseen, AUC-PRunseen\nIn this section, we outline the process of conducting open-set GAD experiments and encapsulate it in\nAlgorithm 2. For each dataset, we alternate treating each anomaly class as the seen anomaly, with the\nremaining anomaly classes as unseen, for all anomaly classes. For example, in the CS dataset, 8 out\nof 15 classes are marked as anomalies. The results for this dataset underwent 5 independent runs of 8\nsub-experiments, with each anomaly class treated as \u2018seen\u2019 in rotation, totaling 40 sub-experiments\nfor CS. Note that the normal class remains consistent across each run, and the labelled normal nodes\nare uniformly drawn.\n15\nB.4\nImplementation Details\nB.4.1\nDependencies\nNSReg is implemented in Python and makes extensive use of Pytorch [32] and Pytorch Geometric [12].\nWe summarise the main scientific computing libraries and their versions used for our implementation\nas follows:\n\u2022 python==3.8.13\n\u2022 pytorch==1.10.1 (py3.8_cuda11.3_cudnn8.2.0_0)\n\u2022 pytorch_geometric==2.0.4 (py38_torch_1.10.0_cu113)\n\u2022 numpy==1.23.5\n\u2022 scipy==1.10.0\n\u2022 scikit-learn==1.2.1\n\u2022 cudatoolkit==11.3.1\n\u2022 dgl==1.0.2\nB.4.2\nHyperparameter Settings\nFor NSReg, in addition to the hyperparameters reported in Sec. 4, we set the numbers of neighbours\nused for GraphSAGE aggregation to 25 and 10 for the first layer and the second layer, respectively, for\nbetter runtime efficiency. The same sampling strategy as NSReg is employed for the GNN-adapted\ncompeting methods that were designed for Euclidean data. Our anomaly scoring network \u03b7 is a\ntwo-layer neural network with 32 hidden units in the hidden layer. BCE, DevNet and PReNet are\ntrained for the same number of epochs (200) as NSReg using the Adam optimiser using the learning\nrate of 1 \u00d7 10\u22123. We keep the default hyperparameter settings for the other baselines except for\nGraphSMOTE, for which the pretraining and training epoch numbers are both set to 2,500 where the\nmodel exhibits convergence.\nFor the large-scale datasets ogbn-arxiv and ogbn-proteins, we set the default number of labelled\nanomalies to 100, and a reduced percentage of labelled normal nodes to 1% to make it more\nchallenging and practical for real-world applications. For ogbn-proteins, where it is set to 2 to enforce\nstronger structural regularisation due to its large number of nodes and highly dense connections (i.e.,\nover a hunderd thousand nodes with over 300 average degree).\nB.4.3\nVisualisation Method\nWe utilise UMAP [25], a widely adopted non-linear invertible dimensionality reduction algorithm,\nin conjunction with the Matplotlib library to generate contour plots representing anomaly scores.\nSpecifically, we apply UMAP to transform node representations at the anomaly scoring (final)\nlayer of each method into a 2-dimensional representation space. Within the boundaries of the 2D\nrepresentation space for each method, determined by the maximum values of each dimension, we\ncreate a uniform mesh with a step size of 0.1. Subsequently, all points within the mesh are coloured\nbased on the anomaly scores assigned by the final layer, utilising their representations transformed by\nthe inverse transformation of the same UMAP model.\nC\nAdditional Experimental Results\nC.1\nDetailed Results for NSR as a Plug-and-Play Module\nWe present the complete results of plugging our NSR module into three state-of-the-art supervised\nAD model, DevNet, DCI and BWGNN, with the original models as the baselines for both all test\nanomalies and the unseen anomalies in terms of both AUC-ROC and AUC-PR. Similar substantial\nperformance improvement can be observed as Sec. 4.1.\n16\nTable 6: Results of plugging our NSR module into three SOTA supervised AD models DevNet, DCI\nand BWGNN, with the original models as the baselines on all test anomalies.\nMetric\nAUC-ROC\nAUC-PR\nDataset\nOri\n+NSR\nDiff.\nOri\n+NSR\nDiff.\nDevNet\nPhoto\n0.599\n0.684\n+0.085\u2191\n0.223\n0.424\n+0.201\u2191\nComputers\n0.606\n0.646\n+0.040\u2191\n0.284\n0.340\n+0.056\u2191\nCS\n0.769\n0.949\n+0.180\u2191\n0.684\n0.872\n+0.188\u2191\nYelp\n0.675\n0.684\n+0.009\u2191\n0.315\n0.315\n+0.008\u2191\nAvg. Diff.\n+0.079\u2191\n+0.113\u2191\nDCI\nPhoto\n0.772\n0.865\n+0.093\u2191\n0.452\n0.577\n+0.125\u2191\nComputers\n0.683\n0.738\n+0.055\u2191\n0.427\n0.501\n+0.074\u2191\nCS\n0.856\n0.857\n+0.001\u2191\n0.635\n0.623\n-0.012\nYelp\n0.689\n0.740\n+0.051\u2191\n0.351\n0.387\n+0.036\u2191\nAvg. Diff.\n+0.050\u2191\n+0.056\u2191\nBWGNN\nPhoto\n0.728\n0.802\n+0.074\u2191\n0.313\n0.530\n+0.217\u2191\nComputers\n0.635\n0.726\n+0.091\u2191\n0.348\n0.458\n+0.110\u2191\nCS\n0.845\n0.879\n+0.034\u2191\n0.687\n0.718\n+0.031\u2191\nYelp\n0.727\n0.747\n+0.020\u2191\n0.366\n0.394\n+0.028\u2191\nAvg. Diff.\n+0.055\u2191\n+0.097\u2191\nTable 7: Results of plugging our NSR module into three SOTA supervised AD models DevNet, DCI\nand BWGNN, with the original models as the baselines on unseen anomalies.\nMetric\nAUC-ROC\nAUC-PR\nMet.\nDataset\nOri\n+NSR\nDiff.\nOri\n+NSR\nDiff.\nDevNet\nPhoto\n0.468\n0.513\n0.045\u2191\n0.045\n0.092\n0.047\u2191\nComputers\n0.573\n0.623\n0.050\u2191\n0.200\n0.266\n0.066\u2191\nCS\n0.739\n0.944\n0.205\u2191\n0.606\n0.841\n0.235\u2191\nYelp\n0.621\n0.632\n0.011\u2191\n0.142\n0.146\n0.004\u2191\nAvg. Diff.\n0.078\u2191\n0.088\u2191\nDCI\nPhoto\n0.614\n0.780\n0.166\u2191\n0.083\n0.201\n0.118\u2191\nComputers\n0.629\n0.693\n0.064\u2191\n0.288\n0.380\n0.092\u2191\nCS\n0.847\n0.847\n0.000\n0.558\n0.547\n-0.011\nYelp\n0.637\n0.699\n0.062\u2191\n0.157\n0.190\n0.033\u2191\nAvg. Diff.\n0.073\u2191\n0.058\u2191\nBWGNN\nPhoto\n0.598\n0.628\n0.030\u2191\n0.068\n0.078\n0.010\u2191\nComputers\n0.570\n0.662\n0.092\u2191\n0.209\n0.272\n0.063\u2191\nCS\n0.829\n0.873\n0.044\u2191\n0.620\n0.678\n0.058\u2191\nYelp\n0.674\n0.703\n0.029\u2191\n0.167\n0.188\n0.021\u2191\nAvg. Diff.\n0.049\u2191\n0.038\u2191\n17\nC.2\nDetailed Ablation Study Results.\nWe present the comprehensive results of the ablation study in Table 8, corresponding to Table 4\nin the paper. The performance in AUC-ROC shows a similar trend as AUC-PR, aligning with our\nobservations and findings in Sec. 4.2.\nTable 8: Our proposed NSR module vs other regularisation methods for all test anomalies.\nAUC-ROC\nDataset\nSAD\nCL (unsup)\nCL (s)\nW.O.\nNSR\nAUC-ROC\nPhoto\n0.599\u00b10.134\n0.807\u00b10.010\n0.891\u00b10.010\n0.894\u00b10.016\n0.908\u00b10.016\nComputers\n0.644\u00b10.126\n0.743\u00b10.012\n0.781\u00b10.022\n0.778\u00b10.021\n0.797\u00b10.015\nCS\n0.819\u00b10.014\n0.894\u00b10.006\n0.927\u00b10.012\n0.927\u00b10.022\n0.957\u00b10.007\nYelp\n0.522\u00b10.041\n0.731\u00b10.014\n0.665\u00b10.072\n0.732\u00b10.016\n0.734\u00b10.012\naverage\n0.646\u00b10.079\n0.794\u00b10.011\n0.816\u00b10.029\n0.833\u00b10.019\n0.849\u00b10.013\nAUC-RR\nPhoto\n0.269\u00b10.215\n0.513\u00b10.018\n0.583\u00b10.012\n0.623\u00b10.029\n0.640\u00b10.036\nComputers\n0.381\u00b10.190\n0.492\u00b10.014\n0.539\u00b10.024\n0.539\u00b10.023\n0.559\u00b10.018\nCS\n0.700\u00b10.016\n0.795\u00b10.009\n0.843\u00b10.020\n0.858\u00b10.042\n0.889\u00b10.016\nYelp\n0.175\u00b10.046\n0.392\u00b10.013\n0.330\u00b10.078\n0.399\u00b10.021\n0.398\u00b10.014\nAverage\n0.646\u00b10.079\n0.794\u00b10.011\n0.816\u00b10.029\n0.833\u00b10.019\n0.849\u00b10.013\nTable 9: Our proposed NSR module vs other regularisation methods for unseen anomalies.\nAUC-ROC\nDataset\nSAD\nCL (unsup)\nCL (s)\nW.O.\nNSR\nAUC-ROC\nPhoto\n0.516\u00b10.047\n0.652\u00b10.018\n0.807\u00b10.017\n0.810\u00b10.031\n0.836\u00b10.031\nComputers\n0.614\u00b10.103\n0.685\u00b10.015\n0.733\u00b10.027\n0.730\u00b10.025\n0.752\u00b10.019\nCS\n0.797\u00b10.016\n0.883\u00b10.006\n0.920\u00b10.013\n0.924\u00b10.021\n0.953\u00b10.008\nYelp\n0.511\u00b10.026\n0.677\u00b10.023\n0.613\u00b10.071\n0.679\u00b10.018\n0.690\u00b10.025\naverage\n0.610\u00b10.048\n0.724\u00b10.016\n0.768\u00b10.032\n0.786\u00b10.024\n0.808\u00b10.021\nAUC-RR\nPhoto\n0.053\u00b10.004\n0.068\u00b10.004\n0.133\u00b10.019\n0.207\u00b10.049\n0.221\u00b10.057\nComputers\n0.285\u00b10.134\n0.318\u00b10.018\n0.379\u00b10.032\n0.389\u00b10.028\n0.417\u00b10.024\nCS\n0.631\u00b10.019\n0.743\u00b10.010\n0.797\u00b10.025\n0.834\u00b10.046\n0.866\u00b10.021\nYelp\n0.090\u00b10.017\n0.178\u00b10.011\n0.152\u00b10.045\n0.190\u00b10.023\n0.196\u00b10.020\nAverage\n0.265\u00b10.044\n0.327\u00b10.011\n0.365\u00b10.030\n0.405\u00b10.037\n0.425\u00b10.031\nC.3\nRuntime and Resource Usage\nTable 10: Average training time and peak VRAM usage comparison in (sec / GB). C denotes CPU\nonly implementations.\nPhoto\nComputers\nCS\nYelp\nG. PReNet\n12.3 / 2.4\n18.4 / 2.9\n34.0 / 9.1\n35.7 / 2.3\nG. DevNet\n11.7 / 2.4\n17.9 / 2.9\n33.3 / 9.1\n39.9 / 2.3\nAMNet\n69.7 / 2.5\n89.6 / 2.6\n65.12 / 2.8\n136.29 / 2.6\nBWGNN\n4.6 / C\n7.4 / C\n8.5 / C\n8.5 / C\nG.ENS\n48.3 / 3.7\n81.1 / 6.5\n194.5 / 15.2\n143.4 / 34.6\nG.SMOTE\n788.4 / 4.1\n1895.0 / 8.4\n2518.7 / 16.8\n10068.6 / 68.8\nBCE\n12.5 / 2.4\n18.2 / 2.9\n33.7 / 9.1\n37.3 / 2.2\nNSReg\n15.2 / 2.4\n20.2 / 2.9\n36.6 / 9.1\n63.5 / 2.2\nThis section examines the NSR module\u2019s related runtime and resource overhead, with a particular\nfocus on training time, considering that the NSR module is not utilised during inference. The peak\nVRAM usage is also included as it serves as the major resource bottleneck for large-scale GAD. Our\nresults are gathered using a single NVIDIA A100 GPU and 28 CPU cores from an AMD EPYC 7663\nProcessor. Table 10 shows that the NSR module generally leads to only trivial runtime increases.\nCompared to the baselines that augment the models using graph structure information (GraphSMOTE\n18\nand GraphENS), NSReg is substantially faster to train and uses only a fraction of their required GPU\nresource. Although BWGNN is the fastest method to train due to its default use of a single layer and\nomission of sampling during aggregation, its detection accuracy is significantly lower than NSReg.\nAs NSReg employs mini-batch sampling, its memory usage is independent of the number of nodes,\nand thus more scalable for large-scale GAD tasks.\nC.4\nDetailed Large-scale GAD Results\nIn this section, we present the complete results for large-scale GAD in Tables 11 and 12. We observe\nNSReg consistently outperforms the baseline methods by a large margin on unseen anomalies, similar\nto our discussion in Sec. 4.1.\nTable 11: Results on large-scale graph datasets for all test anomalies, where \u201c-\u201d denotes unavailable\nresults due to out of memory.\nMetric\nDatasets\nogbn-arxiv\nogbn-proteins\nT-Finance\navg.\nAUC-\nROC\n(All)\nPReNet\n0.581\u00b10.006\n0.613\u00b10.035\n0.892\u00b10.017\n0.695\u00b10.019\nDevNet\n0.601\u00b10.015\n0.622\u00b10.057\n0.654\u00b10.210\n0.626\u00b10.094\nDCI\n0.566\u00b10.041\n0.815\u00b10.037\n0.763\u00b1 0.111\n0.715\u00b10.063\nBWGNN\n0.587\u00b10.013\n0.727\u00b10.067\n0.922\u00b10.011\n0.745\u00b10.030\nAMNet\n0.600\u00b10.049\n0.711\u00b10.077\n0.889\u00b10.024\n0.733\u00b10.050\nGHRN\n0.588\u00b10.009\n0.674\u00b10.019\n0.923\u00b10.010\n0.728\u00b10.013\nG.ENS\n-\n-\n0.847\u00b10.087\n-\nG.SMOTE\n-\n-\n0.875\u00b10.016\n-\nBCE\n0.592\u00b10.004\n0.568\u00b10.020\n0.922\u00b10.011\n0.694\u00b10.012\nNSReg\n0.659\u00b10.012\n0.843\u00b10.029\n0.929\u00b10.007\n0.810\u00b10.016\nAUC-\nPR\n(All)\nPReNet\n0.288\u00b10.004\n0.432\u00b10.028\n0.571\u00b10.140\n0.430\u00b10.057\nDevNet\n0.304\u00b10.009\n0.436\u00b10.017\n0.323\u00b10.327\n0.354\u00b10.118\nDCI\n0.275\u00b10.033\n0.597\u00b10.062\n0.264\u00b10.240\n0.379\u00b10.112\nBWGNN\n0.263\u00b10.009\n0.582\u00b10.026\n0.746\u00b10.023\n0.530\u00b10.019\nAMNet\n0.272\u00b10.053\n0.576\u00b10.053\n0.644\u00b10.046\n0.497\u00b10.051\nGHRN\n0.271\u00b10.007\n0.564\u00b10.005\n0.727\u00b10.031\n0.521\u00b10.014\nG.ENS\n-\n-\n0.332\u00b10.076\n-\nG.SMOTE\n-\n-\n0.573\u00b10.077\n-\nBCE\n0.310\u00b10.003\n0.524\u00b10.008\n0.726\u00b10.034\n0.520\u00b10.015\nNSReg\n0.336\u00b10.006\n0.723\u00b10.020\n0.757\u00b10.020\n0.605\u00b10.125\nTable 12: Results on large-scale graph datasets for unseen anomalies, where \u201c-\u201d denotes unavailable\nresults due to out of memory.\nMetric\nDatasets\nogbn-arxiv\nogbn-proteins\nT-Finance\navg.\nAUC-\nROC\n(Unseen)\nPReNet\n0.465\u00b10.008\n00.250\u00b10.058\n00.881\u00b10.029\n0.532\u00b10.032\nDevNet\n0.494\u00b10.019\n0.264\u00b10.112\n0.642\u00b10.227\n0.467\u00b10.119\nDCI\n0.465\u00b10.045\n0.672\u00b10.095\n0.742\u00b10.106\n0.626\u00b10.082\nBWGNN\n0.499\u00b10.014\n0.429\u00b10.156\n0.903\u00b10.023\n0.610\u00b10.064\nAMNet\n0.519\u00b10.053\n0.339\u00b10.263\n0.872\u00b10.045\n0.577\u00b10.120\nGHRN\n0.493\u00b10.012\n0.309\u00b10.043\n0.908\u00b10.012\n0.570\u00b10.022\nG.ENS\n-\n-\n0.835\u00b10.100\n-\nG.SMOTE\n-\n-\n0.878\u00b10.016\n-\nBCE\n0.478\u00b10.005\n0.120\u00b10.033\n0.921\u00b10.010\n0.506\u00b10.016\nNSReg\n0.570\u00b10.016\n0.748\u00b10.047\n0.928\u00b10.006\n0.749\u00b10.023\nAUC-\nPR\n(Unseen)\nPReNet\n0.124\u00b10.002\n0.029\u00b10.004\n0.431\u00b10.147\n0.195\u00b10.051\nDevNet\n0.142\u00b10.006\n0.032\u00b10.007\n0.284\u00b10.306\n0.153\u00b10.106\nDCI\n0.125\u00b10.011\n0.182\u00b10.172\n0.185\u00b10.221\n0.164\u00b10.135\nBWGNN\n0.135\u00b10.004\n0.058\u00b10.018\n0.609\u00b10.029\n0.267\u00b10.017\nAMNet\n0.141\u00b10.024\n0.048\u00b10.030\n0.461\u00b10.067\n0.217\u00b10.040\nGHRN\n0.132\u00b10.003\n0.044\u00b10.002\n0.583\u00b10.057\n0.238\u00b10.021\nG.ENS\n-\n-\n0.201\u00b10.061\n-\nG.SMOTE\n-\n-\n0.459\u00b10.095\n-\nBCE\n0.136\u00b10.002\n0.025\u00b10.001\n0.629\u00b10.047\n0.263\u00b10.017\nNSReg\n0.165\u00b10.005\n0.488\u00b10.042\n0.669\u00b10.024\n0.441\u00b10.024\n19\nTable 13: NSReg vs. NSReg considering connected normal-anomaly relation (NSReg + NA).\nAUC-ROC\nAUC-PR\nNSReg + NA\nNSReg\nNSReg + NA\nNSReg\nPhoto\n0.860\u00b10.017\n0.908\u00b10.016\n0.561\u00b10.019\n0.640\u00b10.036\ncomputers\n0.761\u00b10.013\n0.797\u00b10.015\n0.538\u00b10.018\n0.559\u00b10.018\nCS\n0.965\u00b10.005\n0.957\u00b10.007\n0.904\u00b10.014\n0.889\u00b10.016\nYelp\n0.738\u00b10.004\n0.734\u00b10.012\n0.401\u00b10.009\n0.398\u00b10.014\nall\nAverage\n0.831\u00b10.010\n0.849\u00b10.013\n0.601\u00b10.015\n0.622\u00b10.021\nPhoto\n0.747\u00b10.032\n0.836\u00b10.031\n0.107\u00b10.018\n0.221\u00b10.057\nComputers\n0.708\u00b10.015\n0.752\u00b10.019\n0.392\u00b10.025\n0.417\u00b10.024\nCS\n0.962\u00b10.006\n0.953\u00b10.008\n0.884\u00b10.018\n0.866\u00b10.021\nYelp\n0.689\u00b10.009\n0.690\u00b10.025\n0.193\u00b10.010\n0.196\u00b10.020\nunseen\nAverage\n0.777\u00b10.016\n0.808\u00b10.021\n0.394\u00b10.018\n0.425\u00b10.031\nC.5\nEffect of Including Normal-anomaly Relations\nThis section investigates the effect of including an additional type of normal relation that connects\nnormal and anomaly nodes in the relation modelling of NSReg. We report the GAD performance for\nall test anomalies and the unseen anomalies in both AUC-ROC and AUC-PR in Table 13. We found\nthat the inclusion of connected normal-anomaly relations is less effective than the default NSReg\nin terms of average performance, although it can achieve comparable performance on the CS and\nYelp datasets. This is because the structural distinction between the normal and the seen anomalies\nis usually sufficiently preserved by the GAD loss. Additionally, it could introduce unnecessary\ncomplexity into the hierarchy of the relation modelling, resulting in less effective modelling of the\nnormality that is not addressed by the GAD loss.\nC.6\nDetailed Hyperparameter Analysis\nWe report the GAD performance on all test anomalies in both AUC-ROC and AUC-PR with respect\nto \u03bb and \u03b1 in Figure 6 and Figure 7, respectively. The results demonstrate stability across a wide\nspectrum, indicating the robustness of NSReg with respect to their settings.\n0.6\n0.8\n1.0\n0.8\n0.9\nAUC-ROC\n0.6\n0.8\n1.0\n\u03b1\n0.4\n0.6\n0.8\nAUC-PR\nPhoto\nComputers\nCS\nYelp\nFigure 6: Overall GAD performance of NSReg w.r.t \u03b1.\n0\n1\n2\n5\n0.7\n0.8\n0.9\nAUC-ROC\n0\n1\n2\n5\n\u03bb\n0.4\n0.6\n0.8\nAUC-PR\nPhoto\nComputers\nCS\nYelp\nFigure 7: Overall GAD performance of NSReg w.r.t \u03bb.\n20\nC.7\nDetailed Data Efficiency Results\nIn this section, our primary focus is on showing the data efficiency performance of NSReg in\nterms of AUC-ROC, which complements the discussion of AUC-PR in Sec. 4.1. As illustrated\nin Figure 8 and Figure 9, NSReg consistently achieves remarkable performance similar to AUC-\nPR. Specifically, across the entire spectrum, NSReg outperforms the baselines on both overall and\nunseen anomaly detection for the Photo, Computers, and CS datasets. For the Yelp dataset, NSReg\neither outperforms or achieves comparable performance, particularly when the number of labelled\ntraining anomalies exceeds 5. While GraphSMOTE slightly outperforms NSReg in unseen anomaly\ndetection, its effectiveness in the seen anomalies is limited, resulting in less effective overall detection\nperformance.\n0.6\n0.8\nAUC-ROC\nPhoto\n0.5\n0.6\n0.7\n0.8\nComputers\n0.6\n0.8\nCS\n0.5\n0.6\n0.7\nYelp\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.4\n0.6\n0.8\nAUC-ROC\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.5\n0.6\n0.7\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.6\n0.8\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.5\n0.6\n0.7\nTest All\nTest Unseen\nNSReg (ours)\nBCE\nG.SMOTE\nG.ENS\nBWGNN\nDevNet\nPReNet\nAMNet\nGHRN\nDCI\nFigure 8: Data efficiency of NSReg and the competing methods in utilising labelled data in AUC-ROC\nfor both all test anomalies and the unseen anomalies.\n21\n0.2\n0.4\n0.6\nAUC-PR\nPhoto\n0.2\n0.4\nComputers\n0.2\n0.5\n0.8\nCS\n0.2\n0.3\n0.4\nYelp\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.1\n0.2\n0.3\nAUC-PR\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.1\n0.2\n0.3\n0.4\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.2\n0.4\n0.6\n0.8\n5\n15\n25\n50 100\n|Vtrain\na\n|\n0.1\n0.2\n0.2\nTest All\nTest Unseen\nNSReg (ours)\nBCE\nG.SMOTE\nG.ENS\nBWGNN\nDevNet\nPReNet\nAMNet\nGHRN\nDCI\nFigure 9: Data efficiency of NSReg and the competing methods in utilising labelled data in AUC-PR\nfor both all test anomalies and the unseen anomalies.\n22\n",
    "2403.01121": "OpenGraph: Towards Open Graph Foundation Models\nLianghao Xia, Ben Kao and Chao Huang\u2217\nThe University of Hong Kong\nGithub: https://github.com/HKUDS/OpenGraph\nABSTRACT\nGraph learning has become indispensable for interpreting and har-\nnessing relational data in diverse fields, ranging from recommen-\ndation systems to social network analysis. In this context, a vari-\nety of graph neural networks (GNNs) have emerged as promising\nmethodologies for encoding the structural information of graphs.\nBy effectively capturing the graph\u2019s underlying structure, these\nGNNs have shown great potential in enhancing performance in\ngraph learning tasks, such as link prediction and node classification.\nHowever, despite their successes, a significant challenge persists:\nthese advanced methods often face difficulties in generalizing to\nunseen graph data that significantly differs from the training in-\nstances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This\nmodel is designed to understand the complex topological patterns\npresent in diverse graph data, enabling it to excel in zero-shot graph\nlearning tasks across different downstream datasets. To achieve this\ngoal, we address several key technical challenges in our OpenGraph\nmodel. Firstly, we propose a unified graph tokenizer to adapt our\ngraph model to generalize well on unseen graph data, even when\nthe underlying graph properties differ significantly from those en-\ncountered during training. Secondly, we develop a scalable graph\ntransformer as the foundational encoder, which effectively cap-\ntures node-wise dependencies within the global topological context.\nThirdly, we introduce a data augmentation mechanism enhanced\nby a large language model (LLM) to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate\nthe effectiveness of our framework. By adapting our OpenGraph\nto new graph characteristics and comprehending the nuances of\ndiverse graphs, our approach achieves remarkable zero-shot graph\nlearning performance across various settings and domains.\nACM Reference Format:\nLianghao Xia, Ben Kao and Chao Huang\u2217. 2024. OpenGraph: Towards\nOpen Graph Foundation Models. In Proceedings of ACM Conference (Con-\nference\u201917). ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/\nnnnnnnn.nnnnnnn\n1\nINTRODUCTION\nGraph learning has established itself as a core and indispensable\nmethodology for unlocking the inherent power of relational data\nacross a wide spectrum of fields. Its versatility and applicability\nspan diverse domains, including recommender systems [11], social\nnetwork analysis [25], citation networks [22], and transportation\nnetworks [32]. To effectively capture the intricate structures inher-\nent in graphs, the adoption of Graph Neural Networks (GNNs) has\nbecome pervasive in learning representations for graph-structured\n\u2217Chao Huang is the Corresponding Author.\nConference\u201917, July 2017, Washington, DC, USA\n2024. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\ndata. GNNs are built on the foundation of recursive message passing,\nleveraging the inter-dependencies of nodes to incorporate high-\norder connectivities into the learned graph representations [15, 42].\nOne of the primary challenges faced by current end-to-end graph\nneural networks is their heavy reliance on labeled data. This strong\ndependence on labeled data presents a primary obstacle in real-\nworld situations, where obtaining a sufficient amount of accurately\nlabeled data can be both expensive and challenging [14, 20]. Conse-\nquently, the labeled data that is available often exhibits problems of\nbeing sparse and of low quality. To tackle this issue, self-supervised\nlearning (SSL) has emerged as a potential solution to enhance graph\nneural networks by leveraging augmented self-supervision signals.\nThese methods typically adopt a learning pipeline that involves\n\"pre-training\" and \"fine-tuning\" with self-supervised pre-training\ntasks. In the context of self-supervised learning and pre-training\non graphs, contrasive SSL has been the dominant approaches. For\nexample, contrastive objectives are incorporated as self-supervised\nalignment loss to supplement the classical supervised task, Notable\nexamples include DGI [31] and GraphCL [44]. Further research,\nsuch as JOAO [43] and GCA [52], aims to automate the contrastive\nlearning process through adaptive augmentation.\nWhile graph pre-training techniques have demonstrated effec-\ntiveness in capturing intrinsic graph properties, they still encounter\ndifficulties in achieving strong generalization from the pre-training\nphase to the downstream task-specific context [28], especially when\nconfronted with distribution shifts in graph characteristics across\ndiverse downstream domains [9, 36]. For example, in recommender\nsystems, there is a need to handle previously unseen user interaction\ngraphs that arise from newly generated items, such as news articles\nor videos, in cold-start recommendation scenarios [2]. Furthermore,\nthere is a desire to transfer the acquired knowledge from pre-trained\ngraph domains to other downstream domains [47]. However, when\nthese models are applied to unseen graphs, their performance signif-\nicantly deteriorates. This decline can be attributed to the variations\nin graph characteristics observed in different downstream scenarios,\nincluding changes in node sets and feature semantics.\nRecent research has explored the use of prompt-tuning as a\ntask-specific alternative to fine-tuning, aiming to bridge the gap\nbetween pre-training and downstream objectives [5, 21, 27]. By\nmerging the pre-training and prompt-tuning frameworks, these\napproaches enable a more efficient alignment of the pre-trained\nmodel\u2019s understanding with the specific requirements of the down-\nstream task. However, it is important to note that most of these\nframeworks assume that the training and testing graph data share\nthe same node set and feature semantics. In practical scenarios,\nvariations in node sets and feature semantics are common across\ndiverse downstream graph domains. Therefore, further exploration\nis needed to enhance the generalization power of graph models and\nensure their adaptability to various real-world downstream graphs.\narXiv:2403.01121v3  [cs.LG]  24 Sep 2024\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\nConsidering the aforementioned motivations and challenges, the\nobjective of this work is to push the boundaries of graph learning\nby developing a versatile and scalable graph model. The primary\naim is to equip the model with the capability of zero-shot learning,\nenabling it to effectively learn from and make accurate predictions\non previously unseen graphs. However, building such a graph model\nis not a trival task and requires addressing several key challenges.\n\u2022 C1: Node Token Set Shift across Different Graphs. One no-\ntable challenge in zero-shot graph learning is the shift in node\ntoken sets across different graphs. This discrepancy necessitates\nthe model to proficiently reconcile the variations in node char-\nacteristics. It is crucial to generate universal graph tokens that\npossess the adaptability to effectively represent and comprehend\narbitrary unseen graphs with diverse graph topological contexts.\n\u2022 C2: Efficient Node-wise Dependency Modeling. In the realm\nof graph learning, nodes in large-scale graphs exhibit intricate\ndependencies on each other. It is crucial to comprehend both\nlocal and global inter-dependencies among all nodes for accurate\nprediction and effective learning. Yet, when it comes to modeling\nuniversal dependencies over extensive graphs, the efficiency of\nnode-wise dependency encoding becomes vital in enhancing the\nperformance and scalability of graph models.\n\u2022 C3: Domain-Specific Data Scarcity. Data scarcity is a wide-\nspread challenge across various downstream domain tasks, and\nit arises from multiple factors. Privacy concerns, for example,\nrestrict the collection of domain-specific user behavior graphs,\nresulting in limited data availability. Consequently, it becomes\ncrucial to develop label-less learning frameworks within graph\nfoundation models to effectively understand the relevant context\nof downstream tasks under conditions of data scarcity.\nPresent Work. To overcome these challenges, we present Open-\nGraph, a flexible graph model that excels in zero-shot graph learn-\ning, allowing it to capture the universal and transferable structural\npatterns across multiple domains or datasets. To address the first\nchallenge C1, we introduce a graph tokenizer that efficiently trans-\nforms input graphs into unified token sequences. This is achieved\nby incorporating a topology-aware projection scheme, enabling\nthe generation of universal graph tokens from arbitrary graphs,\nregardless of variations in node token sets. To conquer the second\nchallenge, C2, we have developed a highly scalable graph trans-\nformer equipped with efficient self-attention using anchor sampling.\nThis innovative approach ensures computational efficiency through\na two-stage self-attention process. Additionally, we employ token\nsequence sampling to optimize the training process, leveraging\nsampled token sequences from the current batch. This reduces the\nsequence length while preserving the crucial graph context.\nTo tackle the challgne of C3, we introduce a novel approach that\ncombines the power of large language models (LLMs) with data\naugmentation techniques for synthetic graph generation. Our goal\nis to enhance the pre-training process of our OpenGraph model by\nincorporating more downstream task-relevant context and gaining\na deeper understanding of real-world graph characteristics. This is\nachieved through the generation of augmented graphs that closely\nresemble real-world instances. To achieve this, we employ an inte-\ngration of the tree-of-prompt regularized with Gibbs sampling.\nWe conducted extensive experiments on a diverse range of pub-\nlicly available graph datasets, encompassing various scales and\ntypes. The outcomes were truly remarkable, as our OpenGraph con-\nsistently showcased exceptional generalization capabilities across\na wide array of settings. Particularly exciting is the fact that the\nzero-shot learning ability of OpenGraph surpassed baselines even\nin few-shot learning scenarios. This outcome lays the foundation\nfor the development of graph foundation models, which have the\npotential to generalize effectively across diverse graph domains.\n2\nPRELIMINARIES\nGraph Representation Learning. A graph G consists of a set of\nnodes V and a set of edges E. The edges \ud835\udc52= (\ud835\udc63\ud835\udc60, \ud835\udc63\ud835\udc61) \u2208E represent\nobserved relations between a source node \ud835\udc63\ud835\udc60and a target node \ud835\udc63\ud835\udc61.\nAdditionally, we define a feature matrix F \u2208R|V|\u00d7\ud835\udc53to capture\nthe \ud835\udc53-dimensional feature attributes of all nodes. Graph represen-\ntation learning aims to capture meaningful node representations\nby transforming both the structural and attribute information into\nlow-dimensional vector embeddings. These embeddings encode the\npatterns of inter-node dependencies and relationships, facilitating\ngraph learning tasks such as link prediction and node classification.\nThe link prediction focuses on predicting the probability of es-\ntablishing connections between node pairs (\ud835\udc63\ud835\udc60, \ud835\udc63\ud835\udc61) that are not\nobserved in the given data. The node classification task aims to as-\nsign a category to each node based on its features and relationships\nwith other nodes. These tasks can be formally expressed:\narg min\n\u0398\ud835\udc53\nLnode = \u2212\n\u2211\ufe01\n\ud835\udc63\ud835\udc60,\ud835\udc66\ud835\udc60\n\u0010\n\ud835\udc53(\ud835\udc63\ud835\udc60,\ud835\udc66\ud835\udc60) /\n\u2211\ufe01\n\ud835\udc66\u2032\ud835\udc60\u2260\ud835\udc66\ud835\udc60\n\ud835\udc53(\ud835\udc63\ud835\udc60,\ud835\udc66\u2032\n\ud835\udc60)\n\u0011\n,\narg min\n\u0398\ud835\udc53\nLlink = \u2212\n\u2211\ufe01\n\ud835\udc63\ud835\udc60,\ud835\udc63\ud835\udc61\n\ud835\udc52\ud835\udc60,\ud835\udc61\ud835\udc53(\ud835\udc63\ud835\udc60, \ud835\udc63\ud835\udc61) \u2212(1 \u2212\ud835\udc52\ud835\udc60,\ud835\udc61)\ud835\udc53(\ud835\udc63\ud835\udc60, \ud835\udc63\ud835\udc61)\n(1)\nHere, The link labels \ud835\udc52\ud835\udc60,\ud835\udc61\u2208{0, 1} indicate if node \ud835\udc63\ud835\udc60is connected to\nnode \ud835\udc63\ud835\udc61. \ud835\udc53denotes the prediction model with a learnable parameter\nset \u0398\ud835\udc53. The node labels \ud835\udc66\ud835\udc60\u2208C indicate the groundtruth category\nfor node \ud835\udc63\ud835\udc60, while C denotes the set of candidate categories.\nZero-shot Graph Learning. Although current graph learning\nmethods demonstrate promising results in standard graph learning\ntasks, they face limitations when it comes to effectively generalizing\nacross diverse domains. When these models are applied to new\nand unseen graphs, their performance noticeably decreases. This\ndecline can be attributed to the variations in graph characteristics\nthat exist across different downstream scenarios, such as changes\nin the node set and feature semantics. Existing models heavily\ndepend on parameters that are specifically tailored to these graph-\nspecific tokens, which include feature transformations and node\nembeddings. As a result, these models encounter challenges in\ntheir ability to effectively generalize and adapt to new graphs that\npossess distinct graph tokens and relational semantics.\nRecognizing the limitations of current approaches, our research\nfocuses on zero-shot graph learning. This task involves training a\nmodel on a set of graphs and evaluating its performance on dif-\nferent test graphs that do not share any graph tokens. The goal\nis to assess the ability of graph models to learn generalized graph\ntopological structures and node-wise dependencies. To provide a\nOpenGraph: Towards Open Graph Foundation Models\nConference\u201917, July 2017, Washington, DC, USA\nformal definition of the zero-shot graph learning task:\narg min\ud835\udc53\ud835\udf16({G\ud835\udc61}, \ud835\udc53), where \u0398\ud835\udc53= arg min\u0398\ud835\udc53L({G\ud835\udc60}, \ud835\udc53),\nV\ud835\udc61\u2229V\ud835\udc60= \u2205, E\ud835\udc61\u2229E\ud835\udc60= \u2205, R\ud835\udc53\ud835\udc61\u2260R\ud835\udc53\ud835\udc60, \u2200G\ud835\udc60, G\ud835\udc61\n(2)\nThe objective of this task is to develop a graph model, denoted as\n\ud835\udc53(\u00b7), that effectively understands general and universal topological\npatterns. It should also discern contextually relevant patterns cru-\ncial for high performance in diverse downstream graph learning\ntasks. Notably, the training graphs (G\ud835\udc60) and test graphs (G\ud835\udc61) have\nno common nodes (\ud835\udc63\u2208V), edges (\ud835\udc52\u2208E), or node feature attribute\nsemantics. This presents a unique challenge for the graph model to\nhandle the significant distribution shift that occurs across different\ngraph domains with entirely distinct datasets.\n3\nMETHODOLOGY\nIn this section, we delve into the design details of the proposed\nOpenGraph framework. To offer a comprehensive overview of the\nmodel, Figure 1 illustrates the overall structure and components.\n3.1\nUnified Graph Tokenizer\nTo overcome the challenges presented by diverse graphs that dif-\nfer significantly in terms of nodes, edges, and feature semantics,\nour primary goal is to develop a graph tokenizer that effectively\ntransforms input graphs into unified token sequences. In our graph\ntokenizer, each token represents a node and is accompanied by\na semantic vector denoted as G \u2192e0, e1, \u00b7 \u00b7 \u00b7 , e|V|. By utilizing\na shared node representation space and employing a flexible se-\nquence data structure, we aim to standardize the distribution of\nnodes across different graphs. Additionally, we merge the edge\ninformation into the unified node representations, disregarding dif-\nferences between graphs. To address the variations in node features,\nwe introduce a topological representation space that is generated\nthrough a consistent mapping procedure.\n3.1.1\nSmoothed High-Order Adjacency Matrix. In our Open-\nGraph model, we employ a unified graph tokenizer that utilizes the\nsmoothed adjacency matrix, denoted as \u02dcA, and a topology-aware\nprojection function \ud835\udf19: R|V| \u2192R\ud835\udc51. This tokenizer is responsible\nfor handling the specific graph representation. To achieve this, we\nstart with the original adjacency matrix A \u2208R|V|\u00d7|V| constructed\nfrom the edges E. In our OpenGraph paradigm, the smoothing\nprocedure for the adjacency matrix is carried out as follows:\n\u02dcA = \u00afA1 + \u00afA2 + \u00b7 \u00b7 \u00b7 \u00afA\ud835\udc3f,\n\u00afA = D\u22121/2AD\u22121/2\n(3)\nHere, we utilize the diagonal degree matrix D derived from the\nadjacency matrix A. It records the node degree of \ud835\udc63\ud835\udc56with \ud835\udc51\ud835\udc56,\ud835\udc56, and\n\ud835\udc51\ud835\udc56,\ud835\udc57= 0 for \ud835\udc56\u2260\ud835\udc57. With matrix D, we employ the Laplacian normal-\nization \u00afA of A to ensure numerical stability. To capture high-order\nconnectivity and address sparsely-observed node-wise relations,\nour OpenGraph combines the power of \u00afA at different orders. This\nallows us to obtain topology information \u02dcA for further processing,\nwith \ud835\udc3frepresenting the maximum power order considered.\n3.1.2\nTopology-aware Projection with Arbitary Graphs. To\nhandle the significant variability in the dimensions of adjacent ma-\ntrix \u02dcA across different graph datasets, using a fixed-input-dimension\nrandom neural network is not feasible. To address this challenge,\nwe propose a topology-aware projection approach using a function\n\ud835\udf19: R|V| \u2192R\ud835\udc51. By eliminating one of the varying dimensions,\nwe aim to reduce information loss during dimension reduction. To\nachieve this, we employ a large hidden dimensionality \ud835\udc51. Prior re-\nsearch [26, 51] has demonstrated that even random projections can\nyield satisfactory performance when utilizing a large dimension-\nality. Hence, to better preserve graph information and minimize\nrandomness in our graph tokenizer, we utilize the fast singular\nvalue decomposition (SVD) for the projection function \ud835\udf19. Empirical\nanalysis reveals that two iterations of fast SVD effectively preserve\ntopology information while incurring negligible computational\noverhead. Formally, our graph tokenizer calculates the resulting\ntoken sequence based on the following operations:\ne\ud835\udc63= \ud835\udf19( \u02dcA\ud835\udc63,:) = \u02dcA\ud835\udc63,:LN((U\n\u221a\n\u039b \u2225V\n\u221a\n\u039b)),\nU, \u039b, V = SVD( \u00afA)\n(4)\nThe topology-aware projection, obtained through SVD, consists\nof U, V \u2208R|V|\u00d7\ud835\udc51and \u039b \u2208R\ud835\udc51\u00d7\ud835\udc51. The concatenation operator\n\u2225combines them in the hidden dimension. Layer normalization\nfunction LN(\u00b7) reduces numerical variance across datasets. The\nresulting e\ud835\udc63\u2208R\ud835\udc51incorporates topology information from \u02dcA and\nthe topology-aware projection \ud835\udf19. This information strengthens\nsubsequent learnable neural networks.\nIt is worth emphasizing that our graph tokenizer is designed\nto handle arbitrary variable-sized graphs that undergo significant\nshifts in terms of node token sets and relational semantics.\n3.2\nScalable Graph Transformer\nAfter acquiring the universal topology-aware tokens for all nodes,\nregardless of any variations in characteristics across diverse graphs,\nthrough our unified graph tokenizer, the subsequent task is to em-\npower our graph model to grasp the complex node-wise dependen-\ncies within the global context. Drawing inspiration from the success\nof transformer architectures in modeling complex relationships be-\ntween instances, our OpenGraph utilizes a graph transformer as\nthe backbone to incorporate global dependencies among all nodes.\nTo ensure scalability and effectively handle large-scale graph\ndatasets, we introduce the following techniques, to enhance the\nefficiency of our graph transformer backbone in our OpenGraph.\n3.2.1\nToken Sequence Sampling. To optimize the efficiency of\nour model, given the large embedding size, our graph transformer\nis trained using sampled token sequences from the current training\nbatch. Specifically, the training batch {(\ud835\udc63\ud835\udc50\ud835\udc4f, \ud835\udc63\ud835\udc5d\ud835\udc4f, \ud835\udc63\ud835\udc5b\ud835\udc4f)|\ud835\udc4f= 1, \u00b7 \u00b7 \u00b7 , \ud835\udc35}\nconsists of \ud835\udc35triplets of centric nodes \ud835\udc63\ud835\udc50\ud835\udc4f, positive nodes \ud835\udc63\ud835\udc5d\ud835\udc4f, and\nnegative nodes \ud835\udc63\ud835\udc5b\ud835\udc4f, which serve as input for our graph transformer.\n(e\ud835\udc501, e\ud835\udc502, \u00b7 \u00b7 \u00b7 e\ud835\udc50\ud835\udc35) \u2225(e\ud835\udc5d1, e\ud835\udc5d2, \u00b7 \u00b7 \u00b7 , e\ud835\udc5d\ud835\udc35) \u2225(e\ud835\udc5b1, e\ud835\udc5b2, \u00b7 \u00b7 \u00b7 , e\ud835\udc5b\ud835\udc35)\n(5)\nThe initial token embeddings e\ud835\udc63effectively capture the local struc-\ntural information for each node \ud835\udc63\u2208V. By using the sampled token\nsequence, we can preserve the graph context with a particular em-\nphasis on the training batch. This approach significantly reduces\nthe sequence length from |V| to 3\u00d7\ud835\udc35, enabling efficient training for\nlarge-scale graphs. Additionally, since the initial token embeddings\ne\ud835\udc63inherently encode the topological closeness among nodes, our\ngraph transformer does not rely on temporal embeddings typically\nused in transformers for temporal or textual sequences.\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\nSVD\n\u211d!\n\u211d\"\nUnified Graph Tokenizer\nGraph Tokens\nGraphs\nScalable Graph Transformer\n\u00d7 \u00d7 \u00d7\nPos\nNeg\nToken Sampling\nAnchor Sampling\n\ud835\udc5e\n\ud835\udc58\n\ud835\udc63\n\ud835\udc3b\n\ud835\udc51\n\ud835\udf0e\nAnchors\nMulti-head Att\nKnowledge Distillation from Large Language Models\nNode Generation\nReal-world Instances\n- ---- --- --- ---- --\n--- ---- ----- -- -- -\nText\nGeneral\nNode\nDivide\nDivide\nEdge Generation\n-- -\n- --\n-- -\n- --\nGibbs Sampling\n\ud835\udc5d\nText Sim\n\ud835\udc5b#\n\ud835\udc5b$\nLocality\n\ud835\udc5dNorm\nT\u2019\nGraph Pattern Injection\nRegenerate with\nGCN Embedding\nAdjL\n\ud835\udc41\nTrain Batch\nFigure 1: Overall architecture of the OpenGraph. i) A unified graph tokenizer efficiently converts any input graph into universal\ngraph tokens, accommodating variations in node token sets. ii) The scalable graph transformer effectively captures global\nnode-wise dependencies using anchor sampling and efficient self-attention mechanisms. iii) LLM-enhanced data augmentation\ntechniques are employed for synthetic graph generation, addressing the issue of limited domain-specific data availability.\n3.2.2\nEfficient Self-attention with Anchor Sampling. Though\nsequence sampling reduces the length of the node sequence, the\npairwise relation learning in self-attention still exhibits a complex-\nity of O(\ud835\udc352 \u00d7 \ud835\udc51), which can limit the batch size \ud835\udc35required for\ncomputational efficiency. To address this challenge, we introduce\nan additional step of sampling a set of anchor nodes \ud835\udc63\ud835\udc4e\ud835\udc60, where\n\ud835\udc60= 1, \u00b7 \u00b7 \u00b7 ,\ud835\udc46, during the self-attention calculation. Here, \ud835\udc46is cho-\nsen to be \ud835\udc51/\ud835\udc3b< \ud835\udc35to ensure that the self-attention module incurs\nsimilar memory costs as other fully-connected components. Here,\n\ud835\udc3brepresents the number of attention heads. More specifically, the\nself-attention process for each head can be summarized as follows:\ne(3)\n\ud835\udc61\n=\n\u2211\ufe01\n\ud835\udc63\ud835\udc4e\n\ud835\udefc\ud835\udc61,\ud835\udc4eW(\ud835\udc63)e(2)\n\ud835\udc4e,\ne(2)\n\ud835\udc4e\n=\n\u2211\ufe01\n\ud835\udc63\ud835\udc61\n\ud835\udefc\ud835\udc4e,\ud835\udc61W(\ud835\udc63)e(1)\n\ud835\udc61\n\ud835\udefc\ud835\udc61,\ud835\udc4e= softmax\n\u0010\n(W(\ud835\udc5e)e\ud835\udc61)\u22a4\u00b7 (W(\ud835\udc58)e\ud835\udc4e) /\n\u221a\ufe01\n\ud835\udc51/\ud835\udc3b\n\u0011\n(6)\nOur efficient self-attention involves embeddings e(1)\n\u2217, e(2)\n\u2217, e(3)\n\u2217\nfor\nanchor nodes \ud835\udc63\ud835\udc4eand vanilla nodes \ud835\udc63\ud835\udc61. After each attention calcu-\nlation, the results from multiple heads are concatenated, passed\nthrough a learnable linear layer, and connected with a residual con-\nnection. The parameters W(\ud835\udc5e), W(\ud835\udc58), W(\ud835\udc63) are the parameters of\nthe attention layer. To reduce computational complexity, we employ\na two-stage self-attention process. It transforms the 3 \u00d7 \ud835\udc35-length\nsequence to a shorter \ud835\udc46-length sequence and then reverses the pro-\ncess. This decomposition reduces the complexity from O(\ud835\udc352 \u00d7 \ud835\udc51)\nto O(\ud835\udc35\u00d7 \ud835\udc46), ensuring scalability for large-scale models.\nAfter the self-attention module, each layer of our scalable graph\ntransformer includes a two-layer fully-connected block with resid-\nual connections, accompanied by two layer normalization modules.\nTo ensure numerical stability, per-layer scaling is applied. For a\nmore comprehensive understanding of the specific configurations\nof our scalable graph transformer, please refer to Appendix A.\n3.3\nKnowledge Distillation from LLM\nObtaining diverse graph datasets for different domains can be chal-\nlenging due to factors like privacy issues that restrict access to\nessential data [50]. Inspired by the remarkable knowledge and un-\nderstanding demonstrated by large language models (LLMs), we\nleverage their power to enhance the generation of diverse graph-\nstructured data. To improve the efficacy of our LLM-augmented\ngraph data for pre-training our model, we have developed an aug-\nmentation mechanism. This mechanism enables the LLM-augmented\ngraph data to closely approximate real-world graph characteristics,\nenhancing the relevance and usefulness of the augmented data.\n3.3.1\nLLM-based Node Generation. When generating graphs,\nour initial step is to create a node set tailored to the specific appli-\ncation scenario. Each node is characterized by a text-based profile\nthat assists in generating subsequent edges. However, this task can\nbe particularly challenging when dealing with real-world scenar-\nios due to the substantial scale of the node set. For instance, in\ne-commerce platforms, the graph data may consist of billions of\nproducts. As a result, efficiently enabling the LLM to generate a\nlarge number of nodes becomes a significant challenge.\nTo address the above challenge, we employ a strategy that in-\nvolves iteratively dividing general nodes into sub-categories with\nfiner semantic granularity. For example, when generating product\nnodes, we begin by prompting the LLM with a query like \"List all\nsub-categories of products on an e-commerce platform like Amazon.\"\nThe LLM responds with a list of sub-categories such as \"clothing,\"\n\"home & kitchen,\" \"electronics,\" and so on. We then continue this\niterative division process by asking the LLM to further refine each\nsub-category. This process is repeated until we obtain nodes that\nclosely resemble real-world instances, such as a product with labels\nlike \"clothing,\" \"women\u2019s clothing,\" \"sweaters,\" \"hooded sweaters,\"\n\"white hooded sweaters.\" Appendix A.3.1 provides more details\nabout our prompt template and specific generation examples.\nTree-of-Prompt Algorithm. The process of dividing nodes into\nsub-categories and generating fine-grained entities follows a tree\nstructure. The initial general node (e.g., \"products,\" \"deep learning\npapers\") serves as the root, and fine-grained entities act as leaf nodes.\nWe employ a tree-of-prompt strategy to traverse and generate these\nnodes. For further details, please see Appendix A.3.2.\n3.3.2\nLLM-based Edge Generation by Gibbs Sampling. To\ngenerate edges, we use the Gibbs sampling algorithm [8] with the\ngenerated node set V. The algorithm starts with a random sample,\nwhich varies depending on the type of entity-entity relation data.\nFor instance, in a paper-wise citation network, the sample is a\nnode pair (\ud835\udc63\ud835\udc600, \ud835\udc63\ud835\udc610). In a person-entity relation scenario like an\nauthor-paper or user-item recommendation network, the initial\nsample is a binary vector a0 \u2208{0, 1}|V|. Each element \ud835\udc4e\ud835\udc56= 1\nindicates interaction between the sampled person and the \ud835\udc56-th node\n\ud835\udc63\ud835\udc56, while \ud835\udc4e\ud835\udc56= 0 indicates no interaction. In the case of person-entity\nrelations, the Gibbs algorithm for edge sampling is described in\nAppendix A.3.3. The key is estimating the probability \ud835\udc5d(a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032 |a\ud835\udc61),\nOpenGraph: Towards Open Graph Foundation Models\nConference\u201917, July 2017, Washington, DC, USA\nwith \u2295representing setting the \ud835\udc61\u2032-th dimension of a\ud835\udc61to 1.\nNode-wise Connection Probability Estimation. To accurately\nestimate the probability \ud835\udc5d(a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032 |a\ud835\udc61) of connecting two nodes\nin our generated graph, we leverage the reasoning capabilities of\nthe LLM. However, directly prompting the LLM for predictions\non each edge can be computationally expensive, with the num-\nber of required prompts scaling proportionally to the number of\nedges (O(|V| \u00d7 |V|)). To ensure efficient probability estimation,\nwe adopt an alternative approach. We prompt the LLM to generate\nhidden representations h\ud835\udc56for each node \ud835\udc63\ud835\udc56. Then, we calculate the\nrelationship between each edge using dot-product similarity as:\n\ud835\udc5d(a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032 |a\ud835\udc61) =\n\u2211\ufe01\n\ud835\udc63\ud835\udc56\n\ud835\udc4e\ud835\udc61\n\ud835\udc56(h\ud835\udc56/\u2225a\ud835\udc61\u22250)\u22a4\u00b7 h\ud835\udc61\u2032\n(7)\nBy utilizing the text embeddings h\ud835\udc56and h\ud835\udc61\u2032 provided by the LLM,\nwhich are in the same representation space, we can effectively\ncapture the semantic relations between the respective nodes.\nDynamic Probability Normalization. To ensure that the cal-\nculated probability scores fall within the range of [0, 1], our gen-\neration algorithm incorporates a dynamic probability normaliza-\ntion approach, which enhances the algorithm\u2019s realism by mim-\nicking real-world distributions. It maintains a record of the most\nrecent \ud835\udc47\u2032 estimation values, denoted as P = {\ud835\udc5d(a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032 |a\ud835\udc61) | \ud835\udc61=\n\u22121, \u22122, \u00b7 \u00b7 \u00b7 , \u2212\ud835\udc47\u2032}. By calculating the mean (\ud835\udf07) and standard devia-\ntion (\ud835\udf0e) of these values, we obtain an understanding of their distri-\nbution. To adjust new estimations, we use \ud835\udf07+2\ud835\udf0eas the upper bound\nand \ud835\udf07\u22122\ud835\udf0eas the lower bound. The resulting adjusted probability\nscore, denoted as \u00af\ud835\udc5d, is obtained by \u00af\ud835\udc5d= (\ud835\udc5d\u2212\ud835\udf07)/(4\ud835\udf0e). Through this\nnormalization process, we ensure that the probability scores are\nconstrained within a range that closely approximates [0, 1].\nNode Locality Incorporation. Our LLM-based edge generation\nmethod effectively determines potential connections based on the\nsemantic similarity between nodes. However, it tends to create\nexcessive connections among all semantically-related nodes, over-\nlooking the important concept of locality observed in real-world\ngraphs. In reality, nodes are more likely to be connected to a subset\nof relevant nodes, as they have limited interactions with only a\nportion of all other nodes. To address this, we introduce a way to\nincorporate the notion of locality in our edge generation process.\nEach node \ud835\udc63\ud835\udc56is randomly assigned a locality index \ud835\udc5b\ud835\udc56. To account\nfor the difference in locality between two nodes, we use the abso-\nlute difference |\ud835\udc5b\ud835\udc56\u2212\ud835\udc5b\ud835\udc57| in an exponential decay function applied to\nthe normalized probability. This results in an adjusted probability\n\u02c6\ud835\udc5dcalculated as \u02c6\ud835\udc5d= \u00af\ud835\udc5d\u00b7 \ud835\udefc|\ud835\udc5b\ud835\udc56\u2212\ud835\udc5b\ud835\udc57|, where 0 < \ud835\udefc< 1.\n3.3.3\nGraph Topological Pattern Injection. To enhance the\nincorporation of topological information in the graph generation\nprocess, we refine the node embeddings after the initial graph\ngeneration. By training a Graph Convolutional Network (GCN) on\nthe graph G, we obtain new node embeddings that capture the\nunderlying topology patterns. This aligns the node embeddings\nderived from the graph with the textual embeddings of the entities\nand avoids distribution shifts between graph and textual spaces.\nThe final graph is constructed using our edge generation algorithm,\nwhich operates on these enhanced node representations.\n4\nEVALUATION\nOur experiments aim to address the following research questions:\n\u2022 RQ1: How does OpenGraph compare to existing graph models\nin zero-shot link prediction and node classification tasks?\n\u2022 RQ2: How does different training datasets (real data and gener-\nated data) affect the model performance of OpenGraph?\n\u2022 RQ3: What is the influence of different types of initial graph\nprojection methods on our OpenGraph framework?\n\u2022 RQ4: How does the sampling strategies in our scalable graph\ntransformer impact the model efficiency and performance?\n\u2022 RQ5: How does model scale impact our OpenGraph?\n\u2022 RQ6: How does our OpenGraph perform relative to baseline\nmethods in standard full-shot training and few-shot training?\n4.1\nExperimental Settings\n4.1.1\nDatasets. We evaluate our OpenGraph on two graph learn-\ning tasks: link prediction and node classification, using a total of 8\nreal-world datasets. Appendix A.2.1 provides detailed descriptions.\n4.1.2\nEvaluation Protocol. Following previous works [11, 16],\nwe adopt the original train-test data split for the experimental\ndatasets. We pre-train our OpenGraph on generated datasets and\nconducts zero-shot prediction for the evaluation datasets made\nof real graph data. As most baselines struggle with cross-dataset\ntransferring, we evaluate them in two few-shot training settings.\nPlease refer to Appendix A.2.2 for more details about our cross-\ndataset zero-shot setting, few-shot settings, and evaluation metrics.\n4.1.3\nImplementation Details. We provide detailed information\nabout the implementation of OpenGraph and the baseline methods,\nas well as the graph generation process, in Appendix A.2.3.\n4.1.4\nBaselines. Our empirical evaluation utilizes the following 9\nstate-of-the-art baseline methods from 4 different research lines. De-\ntailed descriptions for the baselines can be found in Appendix A.2.4.\n4.2\nOverall Performance Comparison (RQ1)\nWe begin by comparing the zero-shot performance of OpenGraph\nwith the few-shot performance of the baselines in both link predic-\ntion and node classification. The results are displayed in Table 1.\nBased on these results, we have made the following observations:\nO1: Predominant Performance of OpenGraph. Our model\ndemonstrates superior performance across all 8 datasets in var-\nious categories. Notably, this advantage is achieved solely through\npre-training on a generated dataset, without any overlap with the\ndownstream graph data. This highlights the remarkable ability of\nour model to generalize across different datasets. We attribute this\nadvantage to three key factors: i) The unified graph tokenizer, which\neffectively bridges the distribution gap between the pre-training\nand target datasets. ii) The scalable graph transformer, which excels\nat learning relations and capturing important structural features,\nresulting in high-quality graph representations. iii) The effective\npre-training with LLM-generated graph data, which equips our\nmodel with versatile forecasting abilities for unseen downstream\ndatasets. Overall, these design choices contribute to the outstanding\ngeneralization capabilities of our model across diverse datasets.\nO2: Limitations of Existing Pretraining Methods. Observations\nreveal that pre-training techniques, such as GraphCL and DGI, do\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\nTable 1: Performance comparison between our OpenGraph (zero-shot) and baseline methods (one-shot, five-shot) on the link\nprediction task (measured by Recall@N for \ud835\udc41= 20, 40) and node classification task (measured by Accuracy and Macro F1 Score).\nDataset\nogbl-ddi\nogbl-collab\nML-1M\nML-10M\nAmazon-book\nCora\nCiteseer\nPubmed\nMetric\nR@20\nR@40\nR@20\nR@40\nR@20\nR@40\nR@20\nR@40\nR@20\nR@40\nAcc\nMacF1\nAcc\nMacF1\nAcc\nMacF1\nMF\n1-shot 0.0087\n0.0161\n0.0261\n0.0349\n0.0331\n0.0604\n0.1396\n0.1956\n0.0034\n0.0043\n0.1710\n0.1563\n0.1740\n0.1727\n0.3470\n0.3346\n5-shot 0.0536\n0.0884\n0.0412\n0.0609\n0.0987\n0.1584\n0.2060\n0.2989\n0.0196\n0.0284\n0.1500\n0.1422\n0.1520\n0.1484\n0.3540\n0.3435\nMLP\n1-shot 0.0195\n0.0336\n0.0112\n0.0185\n0.0548\n0.1019\n0.1492\n0.2048\n0.0017\n0.0028\n0.2300\n0.1100\n0.2590\n0.1993\n0.4430\n0.3114\n5-shot 0.0621\n0.1038\n0.0115\n0.0185\n0.0851\n0.1470\n0.2362\n0.2563\n0.0092\n0.0152\n0.3930\n0.3367\n0.3690\n0.3032\n0.5240\n0.4767\nGCN\n1-shot 0.0279\n0.0459\n0.0206\n0.0321\n0.0432\n0.0849\n0.1760\n0.2086\n0.0096\n0.0160\n0.3180\n0.1643\n0.3200\n0.2096\n0.4270\n0.3296\n5-shot 0.0705\n0.1312\n0.0366\n0.0513\n0.1054\n0.1656\n0.2127\n0.2324\n0.0251\n0.0408\n0.5470\n0.5008\n0.4910\n0.4190\n0.509\n0.4455\nGAT\n1-shot 0.0580\n0.1061\n0.0258\n0.0372\n0.0245\n0.0520\n0.1615\n0.2476\n0.0047\n0.0079\n0.2420\n0.1687\n0.2810\n0.2025\n0.4720\n0.3657\n5-shot 0.0711\n0.1309\n0.0340\n0.0505\n0.1506\n0.2267\n0.2002\n0.2883\n0.0228\n0.0392\n0.585\n0.5438\n0.4940\n0.4441\n0.5780\n0.5582\nGIN\n1-shot 0.0530\n0.1004\n0.0163\n0.0247\n0.0466\n0.0884\n0.1541\n0.2388\n0.0069\n0.0114\n0.3190\n0.1753\n0.2820\n0.1705\n0.4410\n0.3064\n5-shot 0.0735\n0.1441\n0.0311\n0.0458\n0.1458\n0.2344\n0.1926\n0.2829\n0.0252\n0.0418\n0.5400\n0.4941\n0.521\n0.4696\n0.5070\n0.4547\nDGI\n1-shot 0.0315\n0.0617\n0.0255\n0.0385\n0.0486\n0.0863\n0.1868\n0.2716\n0.0081\n0.0142\n0.3150\n0.1782\n0.2840\n0.1791\n0.4290\n0.3163\n5-shot 0.0821\n0.1426\n0.0345\n0.0502\n0.1687\n0.2573\n0.2303\n0.3063\n0.0300\n0.0492\n0.4880\n0.4606\n0.4450\n0.4062\n0.4890\n0.4509\nGPF\n1-shot 0.0503\n0.0856\n0.0027\n0.0048\n0.1099\n0.1702\n0.1599\n0.2326\n0.0072\n0.0128\n0.3080\n0.1952\n0.3110\n0.1984\n0.4220\n0.2670\n5-shot 0.0839\n0.1460\n0.0027\n0.0047\n0.0817\n0.1392\n0.2014\n0.2994\n0.0179\n0.0310\n0.5550\n0.5233\n0.4690\n0.4223\n0.5150\n0.4934\nGPrompt\n1-shot 0.0541\n0.1102\n0.0138\n0.0207\n0.0797\n0.1310\n0.1362\n0.2073\n0.0074\n0.0120\n0.3540\n0.1596\n0.2800\n0.1519\n0.4710\n0.3705\n5-shot 0.0769\n0.1396\n0.0157\n0.0231\n0.1340\n0.2166\n0.2157\n0.3147\n0.0287\n0.0464\n0.5510\n0.5098\n0.5570\n0.5211\n0.5130\n0.4520\nGraphCL\n1-shot 0.0603\n0.1112\n0.0265\n0.0398\n0.0390\n0.0799\n0.1655\n0.2529\n0.0047\n0.0077\n0.2430\n0.1548\n0.2980\n0.1630\n0.4070\n0.4130\n5-shot 0.0740\n0.1368\n0.0311\n0.0456\n0.1416\n0.2138\n0.2019\n0.3075\n0.0270\n0.0440\n0.5610\n0.5330\n0.4300\n0.3683\n0.5230\n0.5024\nOpenGraph 0-shot 0.0921 0.1746 0.0421 0.0639 0.1911 0.2978 0.2370 0.3265 0.0485 0.0748\n0.7504 0.7426 0.6097 0.5821 0.6869 0.6537\nnot consistently surpass their foundational models, like GIN and\nGCN, which were trained solely on few-shot data. This pattern\nsuggests that the pre-training methods may hinder rather than help\nperformance when models are applied to different datasets. Such\nunderperformance is likely due to a substantial shift in data distri-\nbution between the pre-training and target datasets, which can lead\nthe model to become overfitted to the pre-training data, thus impair-\ning its ability to adapt to new graph structures in the downstream\ntasks. Furthermore, prompt tuning methods like GraphPrompt and\nGPF sometimes degrade more severely compared to full-model fine-\ntuning approaches, which indicates an even greater susceptibility\nto overfitting to distinctive patterns within the pre-training dataset.\nO3: Tackling Node Classification by Structure Learning. Our\nOpenGraph exhibits a marked improvement in node classification\ntasks, which underscores the efficacy of employing our pre-trained\nlink predictor to discern the connections between ordinary nodes\nand special nodes representing classes. This advantage hinges on\nOpenGraph\u2019s robust capacity to generalize, as it successfully trans-\nfers knowledge both across different datasets and between distinct\ntasks. The credit for this versatility goes to the universal applicabil-\nity and flexibility of our graph tokenization and graph encoding.\n4.3\nInvestigation on Graph Tokenizer (RQ2)\nIn this section, we study the effectiveness of our unified graph tok-\nenization module. This investigation involves an evaluation of the\nimpact of the smoothed adjacency matrix and a comparison of our\ntopology-aware projection method with alternative compression\nmethods. The evaluation results are presented in Figure 2, based\non which we make the following analysis.\nImpact of adjacency smoothing: We examine the effect of graph\nsmoothing on model performance by testing different levels of\nsmoothing for the input adjacency matrix. The results are depicted\nin Figure 2(a) and 2(b). Here, the use of 0 adjacency smoothing\nimplies the input of an identity matrix for the graph tokenizer.\nThis approach significantly damages the topological information\nfor the graph tokenizer, resulting in poor performance. This out-\ncome underscores the importance of considering the adjacency\nmatrix within our unified graph tokenizer. For the non-zero graph\nsmoothing orders, \ud835\udc3f= 2 produces the best performance for the\nMovielens-1M dataset. \ud835\udc3f= 3 and 1 yield the best performance for\nthe OGBL-ddi data under the top-20 and top-40 settings, respec-\ntively. This suggests the benefits of exploring high-order graph\nsmoothing in the graph tokenizer of our OpenGraph.\nSuperiority of topology-aware projection. To assess the effec-\ntiveness of our topology-aware projection based on fast SVD, we\ncompare it to three alternative projection modules (see Appen-\ndix A.2.5 for more details). The results are presented in Figure 2(c)\nand 2(d). We make the following observations:\n\u2022 One-hot encoding. This graph projection strategy learns id-\ncorresponding embeddings across datasets. It performs poorly\nin our zero-shot evaluation, highlighting the difficulty of trans-\nferring dataset-specific parameters such as node embeddings to\nunseen datasets that lack overlapping node tokens.\n\u2022 Degree embeddings. This method learns degree-specific embed-\ndings. It demonstrates a significant performance disadvantage\ncompared to our projection scheme. This can be attributed to the\nsubstantial semantic gap that exists for the same degree number\nacross different graphs. Additionally, this approach oversimplifies\nthe topology features by considering only the number of direct\nlinks, which limits its ability to capture more nuanced structural\npatterns and can be detrimental to the graph projection.\n\u2022 Random projection. It randomly assigns an unlearnable embed-\nding vector to each node. The results demonstrates its advantages\nover the other two variants. However, due to the low represen-\ntation efficiency of its uniform distribution, this method also\nexhibits poorer performance compared to our method.\nOpenGraph: Towards Open Graph Foundation Models\nConference\u201917, July 2017, Washington, DC, USA\n0\n1\n2\n3\n4\nAdj Smooth Order\n0.04\n0.06\n0.08\n0.10\nRecall@20\n0\n1\n2\n3\n4\nAdj Smooth Order\n0.10\n0.15\n0.20\nRecall@40\n(a) OGBL-ddi dataset.\n0\n1\n2\n3\n4\nAdj Smooth Order\n0.0\n0.1\n0.2\nRecall@20\n0\n1\n2\n3\n4\nAdj Smooth Order\n0.1\n0.2\n0.3\nRecall@40\n(b) Movielens-1M dataset.\n0.06\n0.07\n0.08\n0.09\n0.10\nRecall@20\nOne-hot\nRandom\nDegree\nOurs\n0.12\n0.14\n0.16\n0.18\nRecall@40\nOne-hot\nRandom\nDegree\nOurs\n(c) OGBL-ddi dataset.\n0.05\n0.10\n0.15\n0.20\nRecall@20\nOne-hot\nRandom\nDegree\nOurs\n0.10\n0.15\n0.20\n0.25\n0.30\nRecall@40\nOne-hot\nRandom\nDegree\nOurs\n(d) Movielens-1M dataset.\nFigure 2: Influence of graph tokenizer configurations.\n4.4\nInfluence of Pre-training Datasets (RQ3)\nTo evaluate the effectiveness of our knowledge distillation from the\nLLM, we compare the performance of OpenGraph networks that are\npre-trained with different datasets. In our experiments, we employ\nthree ablated versions of our graph generation algorithm, namely\n-Norm, -Loc, and -Topo (further details below). Additionally, we in-\ncorporate two real datasets, Yelp2018 and Gowalla, for pre-training,\nwhich are unrelated to the test datasets used in this experiment.\nMoreover, we include the ML-10M dataset, which is related to both\nML-1M and itself, in the test datasets. The evaluation results are\nsummarized in Table 2. We draw the following conclusions:\nSuperiority of our generated data. Our generated dataset, re-\nferred to as Gen, achieves the best performance on all test datasets\nexcept for ML-1M and ML-10M. Notably, ML-10M, which is closely\nrelated to these two datasets, achieves the best performance in these\ncases. This finding highlights the superior generalization ability of\nour generated datasets, which equips the OpenGraph model with\nthe capability of universal topological structure learning.\nEffectiveness of individual generation techniques. We conduct\nablation study by removing the dynamic probability normalization\n(-Norm), locality incorporation (-Loc), and graph topological pat-\ntern injection (-Topo) modules. The removal of these modules from\nthe graph generation algorithm leads to a significant drop in per-\nformance for the downstream prediction. This demonstrates the\neffectiveness of these three key modules for our generated graphs.\nReal-world data can be misleading. Using real-world datasets\nsuch as Yelp2018 and Gowalla does not yield transferable graph\nlearning capabilities that provide an advantage over our Gen dataset.\nThis highlights the limitation of relying solely on insufficient or\nbiased real-world data for cross-dataset pre-training.\nPretraining on related datasets is useful. Models pre-trained\non ML-10M exhibit superior performance on ML-1M and ML-10M,\nwhich are directly related datasets. This indicates that dataset-wise\nsimilarity can aid in the cross-dataset knowledge transferring.\n4.5\nImpact of Sampling in Transformer (RQ4)\nWe examine the influence of token sequence sampling and anchor\nsampling in our scalable graph transformer architecture. The met-\nrics for efficiency and performance are summarized in Table 3. Our\nevaluation focuses on the GPU memory costs and the running time\nduring both the training and testing processes. The OGBL-ddi and\nTable 2: Influence of utilizing different pre-training datasets.\nTest\nPre-training Dataset\nDataset -Norm\n-Loc\n-Topo Yelp2018 Gowalla ML-10M\nGen\nogbl-ddi 0.0737 0.0893 0.0656\n0.0588\n0.0770\n0.0692\n0.0921\nML-1M\n0.0572 0.1680 0.0850\n0.0599\n0.0485\n0.2030\n0.1911\nML-10M 0.0982 0.1636 0.1017\n0.1629\n0.0910\n0.2698\n0.2370\nCora\n0.4985 0.4864 0.4342\n0.3715\n0.5943\n0.2780\n0.7504\nCiteseer 0.3944 0.3691 0.5743\n0.2651\n0.4300\n0.2003\n0.6097\nPubmed 0.4501 0.5015 0.4876\n0.3317\n0.5148\n0.3652\n0.6869\nTable 3: Impact of sampling strategies on the efficiency and\nmodel performance in the scalable graph transformer.\nOGBL-ddi\nTrain Mem Test Mem Train Time Test Time Recall@20\n-Seq-Anc\n5420MiB\n1456MiB\n22.72s\n13.88s\n0.0966\n-Anc\n3360MiB\n1456MiB\n18.19s\n13.73s\n0.1107\n-Seq\n2456MiB\n1202MiB\n16.45s\n12.09s\n0.0930\nOpenGraph\n2358MiB\n1202MiB\n15.45s\n12.09s\n0.1006\nML-10M\nTrain Mem Test Mem Train Time Test Time Recall@20\n-Seq-Anc\nOOM\nOOM\n\u2013\n\u2013\n\u2013\n-Anc\n4996MiB\nOOM\n73.15s\n\u2013\n\u2013\n-Seq\n23140MiB\n4550MiB\n158.60s\n84.78s\n0.2772\nOpenGraph\n4470MiB\n4550MiB\n68.79s\n54.17s\n0.2816\nML-10M datasets are used for evaluation. Additionally, we assess\nthe model performance after end-to-end training on these datasets.\nThe following observations are made for the ablated versions.\n\u2022 -Seq-Anc: This variant eliminates both token sequence sampling\nand anchor sampling, leading to out-of-memory (OOM) errors\nwhen applied to the larger ML-10M dataset. In the evaluation\non OGBL-ddi, it exhibits the lowest memory and time efficiency,\nwhile its performance is inferior to our OpenGraph model.\n\u2022 -Anc: This version incorporates only sequence sampling into the\ngraph transformer. It significantly reduces memory costs during\nthe training phase. Additionally, by focusing on the current train-\ning context, it achieves the best performance on OGBL-ddi.\n\u2022 -Seq: This model removes token sequence sampling from our\nOpenGraph architecture, resulting in a significant decrease in\ntraining efficiency. However, the anchor sampling strategy in this\nversion greatly reduces computational costs during the test phase.\nDespite the computational benefits, the anchor sampling strategy\nleads to a drop in performance compared to our OpenGraph.\n4.6\nImpact of Model Scale (RQ5)\nThis section examines the influence of model scale within our Open-\nGraph framework. Specifically, we modify two crucial hyperparam-\neters that significantly impact the scale of learnable parameters: the\nnumber of graph transformer layers \ud835\udc3f\u2032, and the hidden dimensional-\nity \ud835\udc51. We assess the model\u2019s performance on the link prediction task\nby measuring Recall@20. Additionally, we evaluate the computa-\ntional time for 100 training steps and 100 test steps. The results are\nillustrated in Figure 3. We summarize the key findings as follows:\nNumber of graph transformer layers. It can be observed that\nboth training and testing time of OpenGraph exhibit a linear in-\ncrease as the number of graph transformer layers grows. However,\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\n1\n4\n7\n10\n# Graph Transformer Layers\n10\n15\n20\nTime / 100 stps\nTrain\nTest\n0.07\n0.08\n0.09\n0.10\nRecall@20\n256\n512\n1024\n2048\nHidden Dimensionality\n10\n20\n30\nTime / 100 stps\nTrain\nTest\n0.08\n0.09\n0.10\nRecall@20\n(a) Performance and time change on the OGBL-ddi dataset.\n1\n4\n7\n10\n# Graph Transformer Layers\n5\n10\n15\n20\nTime / 100 stps\nTrain\nTest\n0.17\n0.18\n0.19\n0.20\nRecall@20\n256\n512\n1024\n2048\nHidden Dimensionality\n10\n20\n30\nTime / 100 stps\nTrain\nTest\n0.10\n0.15\n0.20\nRecall@20\n(b) Performance and time change on the ML-1M dataset.\nFigure 3: The impact of OpenGraph model scale on down-\nstream data performance and training/testing time (seconds).\nthe expansion in model size does not consistently lead to perfor-\nmance improvement. The lack of performance enhancement can\nbe attributed to the overfitting effect and the increased training\ncomplexity associated with deep transformer networks.\nHidden dimensionality. In contrast to the number of graph trans-\nformer layers, the hidden dimensionality leads to quadratic growth\nin computational time. This reflects the rapid increase in model\ncapacity required to accommodate more complex structural data.\nConsequently, we observe significant improvements in model per-\nformance, surpassing the performance curve for the graph trans-\nformer layers. Despite the increased model capacity, the growth in\nhidden dimensionality facilitates the R\ud835\udc41\u2192R\ud835\udc51projection, reduc-\ning information loss and enhancing the quality of the graph token\nsequence for the subsequent graph transformer layers.\n4.7\nEnd-to-End Training Performance (RQ6)\nTo assess the modeling capabilities of our OpenGraph framework,\nwe perform a performance comparison between OpenGraph and\nthe baselines trained on the same few-shot datasets. Due to space\nlimitations, we present the detailed results and analysis in Appen-\ndix A.2.6. The results demonstrate strong graph learning capabilities\nof our OpenGraph, even in the supervised learning settting.\n5\nRELATED WORK\nGraph Neural Networks. Graph Neural Networks (GNNs) have\ngained significant attention in recent years due to their ability to\neffectively capture and model complex relationships within graph-\nstructured data [3, 37, 48]. GNNs utilize message passing schemes\nto propagate information across the graph and aggregate informa-\ntion from the neighboring nodes for node representations [13, 45].\nSeveral representative graph neural architectures have been devel-\noped. For example, Graph Convolutional Networks (GCNs) adapt\nthe concept of convolution to operate on graph-structured data and\nenable nodes to exchange messages with their neighbors [7, 49].\nAdditionally, Graph Attention Networks (GANs) are introduced to\naddress the limitations of traditional GCNs in capturing complex\nrelationships and varying importance levels between nodes in a\ngraph [19, 48]. The backbone of OpenGraph draws inspiration from\nthe powerful Graph Transformer [12, 46] in capturing long-range\ndependencies and global information in graphs.\nSelf-Supervised Learning on Graphs. To tackle the issue of lim-\nited labeled data in graph tasks, self-supervised graph learning\ntechniques, such as contrastive and generative approaches, provide\npromising solutions [17, 35, 38]. These methods leverage the in-\nherent structure and patterns within graphs to train models in a\ndata-efficient manner [39, 41]. Specifically, graph contrastive learn-\ning frameworks focus on creating meaningful representations by\ncontrasting positive and negative samples. Notable models in this\narea include GraphCL [44] and SGL [34], which utilize stochastic\ndata augmenters. Additionally, adaptive augmentation schemes\nsuch as JOAO [43] and GCA [52] have been proposed. Another\nparadigms, DGCL [18] and UMGRL [23], addresses the challenge\nof disentangling factors within contrastive learning. However, ex-\nisting solutions often struggle to generalize well across diverse\ndownstream graph scenarios due to their strong assumptions of a\nconsistent graph token set. This work proposes OpenGraph, which\nenhances graph model generalization across different graph tasks.\nLLM-based Methods for Graph Analysis. Recent advancements\nin the field of large language models (LLMs) have sparked re-\nsearchers\u2019 interest in leveraging their potential to enhance graph\ncomprehension and analysis [1, 6, 10, 24]. Notably, GraphLLM [1]\nand GraphQA [6] are notable examples of efforts aimed at trans-\nforming graphs into natural language descriptions. Leveraging such\ndescriptions as inputs for LLMs enables improved interpretation and\nreasoning on graph data. Additionally, techniques like instruction\ntuning in GraphEdit [10] and GraphGPT [29] facilitate fine-tuning\nLLMs by incorporating rich textual information from text-attributed\ngraphs. However, the success of these methods heavily relies on\nthe availability of high-quality textual features associated with\ngraph nodes. Acquiring such features can be challenging in certain\ngraph domains. For example, privacy concerns can hinder obtain-\ning high-quality text-attributed graphs in user behavior graphs,\nand constructing comprehensive textual descriptions for neurons\nin a brain\u2019s neuronal graph can be difficult. Therefore, there is a\nneed to develop a graph model that can capture universal structural\npatterns from graphs, even in the absence of textual data.\n6\nCONCLUSION\nThe main focus of this research is to develop a highly adaptable\nframework capable of accurately capturing and comprehending\nthe intricate topological patterns found in diverse graph structures.\nBy harnessing the potential of the proposed model, our intention\nis to significantly improve the model\u2019s ability to generalize and\nperform effectively in zero-shot graph learning tasks encompassing\na diverse spectrum of downstream applications. To further improve\nthe efficiency and robustness of OpenGraph, we build our model\nupon a scalable graph transformer architecture together with a\nLLM-enhanced data augmentation mechanism. Through extensive\nexperiments conducted on multiple benchmark datasets, we have\ndemonstrated the exceptional generalization abilities of our model.\nThis study makes an initial attempt to exploit the graph fundation\nmodel. In future work, our plan is to empower our framework\nwith the capability to automatically discover noisy connections and\ninfluential structures with counterfactual learning, while learning\nthe universal and transferable structural patterns of diverse graphs.\nOpenGraph: Towards Open Graph Foundation Models\nConference\u201917, July 2017, Washington, DC, USA\nREFERENCES\n[1] Z. Chai, T. Zhang, L. Wu, K. Han, X. Hu, X. Huang, and Y. Yang. Graphllm:\nBoosting graph reasoning ability of large language model.\narXiv preprint\narXiv:2310.05845, 2023.\n[2] H. Chen, Z. Wang, F. Huang, X. Huang, Y. Xu, Y. Lin, P. He, and Z. Li. Generative\nadversarial framework for cold-start item recommendation. In SIGIR, pages\n2565\u20132571, 2022.\n[3] M. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li. Simple and deep graph convolu-\ntional networks. In ICML, pages 1725\u20131735. PMLR, 2020.\n[4] M. Chen, Z. Zhang, T. Wang, M. Backes, M. Humbert, and Y. Zhang. Graph\nunlearning. In SIGSAC, pages 499\u2013513, 2022.\n[5] T. Fang, Y. Zhang, Y. Yang, C. Wang, and L. Chen. Universal prompt tuning for\ngraph neural networks. NeurIPS, 2023.\n[6] B. Fatemi, J. Halcrow, and B. Perozzi. Talk like a graph: Encoding graphs for large\nlanguage models. In NeurIPS 2023 Workshop: New Frontiers in Graph Learning,\n2023.\n[7] H. Gao, Z. Wang, and S. Ji. Large-scale learnable graph convolutional networks.\nIn KDD, pages 1416\u20131424, 2018.\n[8] A. E. Gelfand. Gibbs sampling. Journal of the American statistical Association,\n95(452):1300\u20131304, 2000.\n[9] S. Gui, X. Li, L. Wang, and S. Ji. Good: A graph out-of-distribution benchmark.\nNeurIPS, 35:2059\u20132073, 2022.\n[10] Z. Guo, L. Xia, Y. Yu, Y. Wang, Z. Yang, W. Wei, L. Pang, T.-S. Chua, and C. Huang.\nGraphedit: Large language models for graph structure learning. arXiv preprint\narXiv:2402.15183, 2024.\n[11] X. He, K. Deng, X. Wang, Y. Li, Y. Zhang, and M. Wang. Lightgcn: Simplifying\nand powering graph convolution network for recommendation. In SIGIR, pages\n639\u2013648, 2020.\n[12] Z. Hu, Y. Dong, K. Wang, and Y. Sun. Heterogeneous graph transformer. In\nWWW, pages 2704\u20132710, 2020.\n[13] D. Jin, Z. Yu, C. Huo, R. Wang, X. Wang, D. He, and J. Han. Universal graph\nconvolutional networks. NeurIPS, 34:10654\u201310664, 2021.\n[14] W. Jin, X. Liu, X. Zhao, Y. Ma, N. Shah, and J. Tang. Automated self-supervised\nlearning for graphs. In ICLR, 2022.\n[15] W. Jin, Y. Ma, X. Liu, X. Tang, S. Wang, and J. Tang. Graph structure learning for\nrobust graph neural networks. In KDD, pages 66\u201374, 2020.\n[16] T. N. Kipf and M. Welling. Semi-supervised classification with graph convolu-\ntional networks. 2017.\n[17] N. Lee, J. Lee, and C. Park. Augmentation-free self-supervised learning on graphs.\nIn AAAI, volume 36, pages 7372\u20137380, 2022.\n[18] H. Li, X. Wang, Z. Zhang, Z. Yuan, H. Li, and W. Zhu. Disentangled contrastive\nlearning on graphs. NeurIPS, 34:21872\u201321884, 2021.\n[19] R. Liao, Y. Li, Y. Song, S. Wang, W. Hamilton, D. K. Duvenaud, R. Urtasun, and\nR. Zemel. Efficient graph generation with graph recurrent attention networks.\nNeurIPS, 32, 2019.\n[20] Y. Liu, M. Jin, S. Pan, C. Zhou, Y. Zheng, F. Xia, and S. Y. Philip. Graph self-\nsupervised learning: A survey. Transactions on Knowledge and Data Engineering\n(TKDE), 35(6):5879\u20135900, 2022.\n[21] Z. Liu, X. Yu, Y. Fang, and X. Zhang. Graphprompt: Unifying pre-training and\ndownstream tasks for graph neural networks. In WWW, pages 417\u2013428, 2023.\n[22] Q. Lv, M. Ding, Q. Liu, Y. Chen, W. Feng, S. He, C. Zhou, J. Jiang, Y. Dong, and\nJ. Tang. Are we really making much progress? revisiting, benchmarking and\nrefining heterogeneous graph neural networks. In KDD, pages 1150\u20131160, 2021.\n[23] Y. Mo, Y. Lei, J. Shen, X. Shi, H. T. Shen, and X. Zhu. Disentangled multiplex\ngraph representation learning. In ICML, pages 24983\u201325005. PMLR, 2023.\n[24] X. Ren, W. Wei, L. Xia, L. Su, S. Cheng, J. Wang, D. Yin, and C. Huang. Represen-\ntation learning with large language models for recommendation. arXiv preprint\narXiv:2310.15950, 2023.\n[25] A. Sankar, Y. Liu, J. Yu, and N. Shah. Graph neural networks for friend ranking\nin large-scale social platforms. In WWW, pages 2535\u20132546, 2021.\n[26] Y. Shen, Y. Wu, Y. Zhang, C. Shan, J. Zhang, B. K. Letaief, and D. Li. How powerful\nis graph convolution for recommendation? In CIKM, pages 1619\u20131629, 2021.\n[27] M. Sun, K. Zhou, X. He, Y. Wang, and X. Wang. Gppt: Graph pre-training and\nprompt tuning to generalize graph neural networks. In KDD, pages 1717\u20131727,\n2022.\n[28] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan. All in one: Multi-task prompting for\ngraph neural networks. In KDD, 2023.\n[29] J. Tang, Y. Yang, W. Wei, L. Shi, L. Su, S. Cheng, D. Yin, and C. Huang.\nGraphgpt: Graph instruction tuning for large language models. arXiv preprint\narXiv:2310.13023, 2023.\n[30] P. Veli\u010dkovi\u0107, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio. Graph\nattention networks. 2018.\n[31] P. Veli\u010dkovi\u0107, W. Fedus, W. L. Hamilton, P. Li\u00f2, Y. Bengio, and R. D. Hjelm. Deep\ngraph infomax. In ICLR, 2018.\n[32] X. Wang, Y. Ma, Y. Wang, W. Jin, X. Wang, J. Tang, C. Jia, and J. Yu. Traffic\nflow prediction via spatial temporal graph neural network. In WWW, pages\n1082\u20131092, 2020.\n[33] W. Wei, J. Tang, Y. Jiang, L. Xia, and C. Huang. Promptmm: Multi-modal knowl-\nedge distillation for recommendation with prompt-tuning. In WWW, 2024.\n[34] J. Wu, X. Wang, F. Feng, X. He, L. Chen, J. Lian, and X. Xie. Self-supervised graph\nlearning for recommendation. In SIGIR, pages 726\u2013735, 2021.\n[35] L. Wu, H. Lin, C. Tan, Z. Gao, and S. Z. Li. Self-supervised learning on graphs:\nContrastive, generative, or predictive. Transactions on Knowledge and Data\nEngineering (TKDE), 2021.\n[36] Q. Wu, H. Zhang, J. Yan, and D. Wipf. Handling distribution shifts on graphs:\nAn invariance perspective. In ICLR, 2021.\n[37] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip. A comprehensive\nsurvey on graph neural networks. Transactions on Neural Networks and Learning\nSystems (TNNLS), 32(1):4\u201324, 2020.\n[38] L. Xia, C. Huang, C. Huang, K. Lin, T. Yu, and B. Kao. Automated self-supervised\nlearning for recommendation. In WWW, pages 992\u20131002, 2023.\n[39] T. Xiao, Z. Chen, Z. Guo, Z. Zhuang, and S. Wang. Decoupled self-supervised\nlearning for graphs. NeurIPS, 35:620\u2013634, 2022.\n[40] K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are graph neural\nnetworks? In ICLR, 2018.\n[41] Y. Yang, L. Xia, D. Luo, K. Lin, and C. Huang. Graph pre-training and prompt\nlearning for recommendation. arXiv preprint arXiv:2311.16716, 2023.\n[42] R. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and J. Leskovec. Graph\nconvolutional neural networks for web-scale recommender systems. In KDD,\npages 974\u2013983, 2018.\n[43] Y. You, T. Chen, Y. Shen, and Z. Wang. Graph contrastive learning automated. In\nICML, pages 12121\u201312132. PMLR, 2021.\n[44] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen. Graph contrastive learning\nwith augmentations. NeurIPS, 33:5812\u20135823, 2020.\n[45] H. Yuan, J. Tang, X. Hu, and S. Ji. Xgnn: Towards model-level explanations of\ngraph neural networks. In KDD, pages 430\u2013438, 2020.\n[46] S. Yun, M. Jeong, R. Kim, J. Kang, and H. J. Kim. Graph transformer networks.\nNeurIPS, 32, 2019.\n[47] Q. Zhang, X. Wu, Q. Yang, C. Zhang, and X. Zhang. Few-shot heterogeneous\ngraph learning via cross-domain knowledge transfer. In KDD, pages 2450\u20132460,\n2022.\n[48] W. Zhang, Z. Yin, Z. Sheng, Y. Li, W. Ouyang, X. Li, Y. Tao, Z. Yang, and B. Cui.\nGraph attention multi-layer perceptron. In KDD, pages 4560\u20134570, 2022.\n[49] Y. Zhang, X. Wang, C. Shi, N. Liu, and G. Song. Lorentzian graph convolutional\nnetworks. In WWW, pages 1249\u20131261, 2021.\n[50] E. Zheleva and L. Getoor. Preserving the privacy of sensitive relationships in\ngraph data. In International workshop on privacy, security, and trust in KDD, pages\n153\u2013171. Springer, 2007.\n[51] Y. Zheng, H. Wang, Z. Wei, J. Liu, and S. Wang. Instant graph neural networks\nfor dynamic graphs. In KDD, pages 2605\u20132615, 2022.\n[52] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang. Graph contrastive learning with\nadaptive augmentation. In WWW, pages 2069\u20132080, 2021.\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\nA\nAPPENDIX\nA.1\nModel Optimization\nTo optimize our OpenGraph model, we utilize the masked autoen-\ncoding (MAE) training paradigm for self-supervised pre-training.\nLet\u2019s denote our OpenGraph as \ud835\udc53, with trainable parameters \u0398\ud835\udc53,\nand a graph projection function \ud835\udf19. The model is trained on a set\nof graphs G\ud835\udc60with batch-specific labels \u00afE\ud835\udc60. The objective of the\ngenerative SSL optimization is defined as follows:\narg min\n\u0398\ud835\udc53\n\u2211\ufe01\nG\ud835\udc60\n\u2211\ufe01\n\u00afE\ud835\udc60\u2208G\ud835\udc60\nL\n\u0010\n\ud835\udc53(G\ud835\udc60\u2212\u00afE\ud835\udc60,\ud835\udf19; \u0398\ud835\udc53), \u00afE\ud835\udc60\n\u0011\n+ \ud835\udf06\u2225\u0398\ud835\udc53\u22252\nF\n(8)\nHere, G\ud835\udc60\u2212\u00afE\ud835\udc60represents the input graph G\ud835\udc60with the label edge set\n\u00afE removed. All training graphs G\ud835\udc60are jointly trained with random\nalternations. To enhance the model\u2019s adaptability to different graph\nprojections \ud835\udf19, we regenerate the projection function \ud835\udf19for each\ntraining graph G\ud835\udc60every 10 training steps. The weight \ud835\udf06is used for\nthe parameter decay regularization term.\nA.2\nExperiments\nTable 4: Statistics of experimental datasets.\nDataset\n# Nodes\n# Edges # Features # Classes\nLink\nOGBL-ddi\n4,267 1,334,889\n0\nN/A\nOGBL-collab\n235,868 1,285,465\n128\nML-1M\n9,746\n720,152\n0\nML-10M\n80,555 7,200,040\n0\nAmazon-book\n144,242 2,380,730\n0\nNode\nCora\n2,708\n10,556\n1433\n7\nCiteseer\n3,327\n9,104\n3,703\n6\nPubmed\n19,717\n88,648\n500\n3\nGenerated\nGen0\n46,861\n454,276\n0\nN/A\nGen1\n51,061\n268,007\n0\nGen2\n32,739\n240,500\n0\nA.2.1\nExperimental Datasets. Our experimental datasets in-\nclude 5 link prediction datasets and 3 node classification datasets.\nThe data statistics are summarized in Table 4.\nLink prediction datasets. We employ five link prediction datasets\nfrom diverse application scenarios. The objective of these datasets\nis to predict the most likely connections for each node based on\nprevious observations of node-wise interactions.\n\u2022 OGBL-ddi. This dataset is used for drug-drug interaction pre-\ndiction. Each node represents an approved or experimental drug.\nThe edges represent the combined effect of taking two drugs to-\ngether, which differs significantly from taking them individually.\n\u2022 OGBL-collab. This is an academic social relation dataset. Its\nnodes represent scholars, and edges denote collaborations. Each\nnode is combined with a 128-dimensional average word embed-\nding calculated from the author\u2019s historical publications.\n\u2022 Movielens-1M & Movielens-10M. These two datasets are col-\nlected from the movie rating platform Movielens. The graphs are\nconstructed by connecting users with the movies they have rated.\nThey contain 1 million and 10 million rating records, respectively.\n\u2022 Amazon-book. This dataset contains review data from the Ama-\nzon platform. The nodes in the dataset represent users and books,\nwhile the edges denote the review records between them.\nNode classification datasets. For the node classification task, we\nutilize three widely-used citation network datasets: Cora, Citeseer,\nand Pubmed. In these datasets, each node represents an academic\npaper, and an edge (\ud835\udc63\ud835\udc56, \ud835\udc63\ud835\udc57) denotes a citation relation from paper\nnode \ud835\udc63\ud835\udc56to \ud835\udc63\ud835\udc57. The Cora and Citeseer datasets include binary bag-of-\nwords vectors as node features, while the Pubmed dataset utilizes\nTF-IDF weighted word vectors as node features. The objective of\nthese datasets is to classify each node into predefined paper cate-\ngories based on the citation relations and node attributes.\nA.2.2\nEvaluation Protocol. This section includes detailed de-\nscription for our cross-dataset zero-shot setting and the few-shot\nsettings for baselines. It also introduces our evaluation metrics.\nZero-shot setting. For our OpenGraph, we utilize a zero-shot\nlearning setting in which OpenGraph is not trained on any of these\nreal-world datasets but is tested using the training set information\nas input information, including the graph structures, node features,\nand node labels in the training set. To effectively generalize to\nunseen node labels in node classification with zero shot, taking\ninspiration from previous works [27], we treat the label classes as\nnew nodes and connect the vanilla nodes with training labels to the\ncorresponding class nodes. This strategy removes the requirement\nfor learning class-related parameters in the zero-shot learning set-\nting. This enhancement is also applied to baselines methods.\nFew-shot setting. Since most baselines perform poorly in the fore-\ngoing zero-shot setting, we evaluate them in the one-shot setting\nand five-shot setting. In the node classification task, the k-shot\nsetting refers to preserving a maximum of k training instances for\neach label class. For the link prediction task, the k-shot training set\ncontains at most k links for each node. Non-pretraining approaches\nsuch as MLP and GNNs are solely trained on the few-shot training\nset. On the other hand, baselines following the pretraining-and-\ntuning paradigm undergo pretraining and subsequent tuning on the\nfew-shot set. In link prediction, they are pretrained on the same gen-\nerated datasets as our OpenGraph. Model parameters that are not\ntransferable across datasets are re-learned during the tuning phase.\nIn node classification, these methods are pretrained on the graph\nof the target dataset and fine-tuned on the classification labels. In\nthe test phase, all information in the training set is employed.\nEvaluation metrics. In link prediction, we follow existing works [33]\nto conduct the full-rank test for each node. To be specific, for each\nnode, all nodes not connected to it in the training set are ranked by\nthe model. The top-N nodes are taken as positive predictions, and\nwe calculate Recall@N scores with \ud835\udc41= 20, 40. In node classification,\nwe employ the widely-used Accuracy and Macro-F1 metrics [4].\nA.2.3\nImplementation Details. We implemented our OpenGraph\nframework using PyTorch. The model employs the Adam optimizer\nwith a learning rate of 1\ud835\udc52\u22124 or 5\ud835\udc52\u22125. The learnable parameters\nare initialized using the Xavier uniform initialization method. By\ndefault, the reported performance is achieved by OpenGraph with\nan embedding size of \ud835\udc51= 1024 and a maximum power order of\n\ud835\udc3f= 3 for adjacency smoothing. The default scalable graph trans-\nformer utilizes \ud835\udc3f\u2032 = 3 transformer layers, \ud835\udc3b= 4 attention heads,\nand \ud835\udc46= 256 sampled anchor nodes. The training batch size, which\nis also used for token sequence sampling, is set as \ud835\udc35= 1024.\nOpenGraph: Towards Open Graph Foundation Models\nConference\u201917, July 2017, Washington, DC, USA\nTable 5: Performance comparison with models trained on\nfew-shot datasets, in terms of Recall@20 (%).\nDataset\nGCN\nGAT\nGIN\nOpenGraph\n1-shot 5-shot 1-shot 5-shot 1-shot 5-shot 1-shot 5-shot\nOGBL-ddi\n2.79\n7.05\n5.80\n7.11\n5.30\n7.35\n7.93\n8.60\nML-1M\n4.32\n10.54\n2.45\n15.06\n4.66\n14.58 16.58 19.19\nAmazon-book\n0.96\n2.51\n0.47\n2.28\n0.69\n2.52\n2.96\n3.10\nThe reported results are obtained by pretraining our OpenGraph\nusing three generated datasets: Gen0, Gen1, and Gen2. The statis-\ntics of these datasets are presented in Table 4. We first generate\nthe Gen0 dataset without injecting the graph\u2019s topological pattern.\nSubsequently, we generate Gen1 and Gen2 based on Gen0 by in-\ncorporating the graph pattern. In comparison to Gen1, the Gen2\ndataset undergoes an additional densification process, where nodes\nwith less than 10 edges are removed. To acquire the nodes for the\nGen0 dataset, we prompt the LLM to iterate through all products\non an e-commerce platform, with a maximum generation depth of\n5. The Gibbs sampling algorithm is initialized with nodes having 6\nrandom edges. To ensure low overlap between consecutive samples,\nwe introduce a separation of 1000 sampling steps before gener-\nating each new sample. The dynamic probability normalization\nmaintains the last \ud835\udc47\u2032 = 5000 sampling instances. The node locality\nincorporation involves using 7 locality indices and 0.95 decay rate.\nThe baseline methods are evaluated using their original code, or\nwe closely follow the original code to implement them. Our imple-\nmentations of the baselines are carefully aligned with the reported\nperformance in their original evaluation settings. We employ grid\nsearch to optimize the hyperparameter settings for each baseline.\nA.2.4\nBaselines. We give detailed descriptions for the baseline\nmodels in this section. 9 models from 4 categories are utilized.\nGraph-agnostic Approaches.\n\u2022 MF. This is the matrix factorization approach which learns node\nembeddings to reconstruct the observed adjacency matrix. For the\nnode classification task, we adapt it to learn embedding vectors\nfor each node with the goal of predicting node labels.\n\u2022 MLP. This baseline utilizes a multi-layer perceptron to extract\ndeep features individually for each node. For datasets without\nnode attributes, this baseline learns initial node embeddings.\nNon-pretraining Graph Neural Networks.\n\u2022 GCN [16]. This approach utilizes iterative graph convolutional\noperators to extract the high-order topological information.\n\u2022 GAT [30]. This graph attention network learns weights for node-\nwise connections using the attention mechanism, to facilitate\nadaptive graph information propagation and aggregation.\n\u2022 GIN [40]. This method enhances the representation power of\nGNNs by employing a distinct graph encoding method that em-\nphasizes the discrimination of non-isomorphic structures.\nGraph Pre-training Models.\n\u2022 GraphCL [52]. This baseline method utilizes pre-training of\ngraph models through the application of a self-discriminative\ncontrastive learning task on learned node embeddings. It incor-\nporates various graph augmentation techniques such as node\ndrop, edge permutation, random walk, and feature masking.\n\u2022 DGI [31]. This method introduces a self-supervised pre-training\ntask that aims to maximize the mutual information between the\nlocal node view and the global graph view.\nGraph Prompt Tuning Methods.\n\u2022 GraphPrompt [21]. This work presents a unified framework for\npre-training and prompt tuning of graph models. It introduces\na learnable prompt layer that automatically identifies crucial\ninformation in the pre-trained model for downstream tasks.\n\u2022 GPF [5]. This is a universal graph prompt tuning framework\ndesigned for various graph pre-training strategies. It introduces\ntwo versions of a learnable graph prompt layer.\nA.2.5\nAlternative Graph Projection Methods (RQ2).\n\u2022 One-hot encoding. This graph projection strategy employs a\nlarge table of low-dimensional embeddings for node ids, where\nnodes with the same index from different datasets are directly\nmapped to the same embedding vector. Specifically, we utilize\n100,000 independent embedding vectors. For datasets with more\nnodes, we use the remainder of 100,000 dividing node indices.\n\u2022 Degree embeddings. Degree embeddings are a commonly used\nnode representation strategy for non-attributed graphs. Each\ndegree number is assigned an independent embedding vector, and\neach node is initially represented by its degree representation.\n\u2022 Random projection. In this approach, a random representation\nvector is assigned to each node, sampled from a uniform distribu-\ntion. With a sufficiently large representation space, this method\naims to approximately distribute nodes with equal distances from\none another. As a result, this projection method does not rely\non any specific assumptions about the node distribution. This\ncharacteristic allows it to outperform the other two strategies,\nwhich are based on certain assumptions and are thus more prone\nto overfitting the pre-training dataset.\nA.2.6\nEnd-to-end Training Performance (RQ6). We compare\nour OpenGraph with other graph encoding methods in the super-\nvised learning setting. The models are trained using the 1-shot and\n5-shot training sets from OGBL-ddi, ML-1M, and Amazon-book,\nand then tested on the corresponding test set. Without pre-training,\nthis experiment aims to examine the modeling capacity for different\ngraph neural architectures. From the results shown in Table 5, we\ndraw the following conclusions:\n\u2022 Superior modeling capabilities of OpenGraph. Our Open-\nGraph achieves best performance on all tested datasets, demon-\nstrating the superior graph learning ability for OpenGraph. We\nattribute this superiority to the precise preservation of structural\ninformation by our graph tokenization module, and the strength\nof our scalable graph transformer in learning global relations.\n\u2022 Robustness of OpenGraph. We notice that our OpenGraph\nexhibits less performance degradation on the more sparse 1-shot\ndatasets. This demonstrates the inherent robustness of Open-\nGraph\u2019s model architecture. Such robustness can be ascribed to\nthe effectiveness of the fast topology projection, which effectively\ncaptures key graph structures even without sufficient training.\nConference\u201917, July 2017, Washington, DC, USA\nLianghao Xia, Ben Kao and Chao Huang\u2217\nAlgorithm 1: Node generation based on tree-of-prompt.\nInput: Name for the initial general node \ud835\udc630 (e.g. \u2019products\u2019),\ntext descriptions for the application scenario \ud835\udc46\n(e.g. \u2019e-commerce platform like Amazon\u2019), maximum\ndepth \ud835\udc37of the prompt tree\nOutput: Generated nodes \u02c6\nV.\n1 Function DivideNode(\ud835\udc63, \ud835\udc5b):\n2\nif \ud835\udc5b\u2265\ud835\udc37then\n3\nreturn [\ud835\udc63]\n4\nend\n5\n\u00af\nV = LLM(\ud835\udc63,\ud835\udc46)\n6\n\u02c6\nV = []\n7\nforeach \ud835\udc63\u2032 \u2208\u00af\nV do\n8\n\u02c6\nV+ = DivideNode(\ud835\udc63\u2032, \ud835\udc5b+ 1)\n9\nend\n10\nreturn \u02c6\nV\n11 return DivideNode(\ud835\udc630, 1)\nAlgorithm 2: Edge generation based on Gibbs sampling.\nInput: Node embedding table H given by the LLM, node\nset V, maximum locality index \ud835\udc41, locality decay\nfactor \ud835\udefc, dynamic probability normalization range\n\ud835\udc47\u2032, number of sampling steps to draw a new sample\n\ud835\udc470, number of initial sampling steps to skip for data\nquality \ud835\udc471, number of sampling steps to shift current\nlocality index \ud835\udc472, maximum sampling steps \ud835\udc47max.\nOutput: List of interactions I.\n1 Draw a random interaction sample a0\n2 Initialize current locality index \ud835\udc5b= 0\n3 Initialize the pool for dynamic probability P = []\n4 Initialize I = [] for \ud835\udc61= 1 to \ud835\udc47max do\n5\nif \ud835\udc61mod \ud835\udc472 == 0 then\n6\n\ud835\udc5b= (\ud835\udc5b+ 1) mod \ud835\udc41\n7\nend\n8\n\ud835\udc56= \ud835\udc61mod |V|\n9\n\ud835\udc5d= \u00cd\n\ud835\udc63\ud835\udc56\ud835\udc4e\ud835\udc61\n\ud835\udc56(h\ud835\udc56/\u2225a\ud835\udc61\u22250)\u22a4\u00b7 h\ud835\udc61\u2032\n10\nP+ = [\ud835\udc5d]\n11\nif |P| > \ud835\udc47\u2032 then\n12\nP = P[\u2212\ud835\udc47\u2032 :]\n13\nend\n14\n\ud835\udf07= mean(P), \ud835\udf0e= std(P)\n15\n\u00af\ud835\udc5d= (\ud835\udc5d\u2212\ud835\udf07)/(4\ud835\udf0e)\n16\n\u02c6\ud835\udc5d= \u00af\ud835\udc5d\u00b7 \ud835\udefc|\ud835\udc5b\u2212\ud835\udc5b\ud835\udc56|\n17\nDecide if a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032 is accepted accordding to \u02c6\ud835\udc5d\n18\nif \ud835\udc61\u2265\ud835\udc471 and \ud835\udc61mod \ud835\udc470 == 0 then\n19\nI+ = [a\ud835\udc61\u2295\ud835\udc63\ud835\udc61\u2032]\n20\nend\n21 end\n22 return I\nPrompt Template\nList all distinct sub-categories of {entity_name} within the {prefix} category in \nthe context of {scenario_desc}, ensuring a finer level of granularity. The sub-\ncategories should not overlap with each other. And a sub-category should be a \nsmaller subset of {entity_name}. Directly present the list EXACTLY following \nthe form: \"sub-category a, sub-category b, sub-category c, ...\" without other \nwords, format symbols, new lines, serial numbers.\nPrompt Example\nentity_name = \u201cwomen\u2019s clothing\u201d\nscenario_desc = \u201ce-commerce platform like Amazon\u201d\nprefix = \u201cproducts, clothing\u201d\nExamples of Generated Nodes\nproducts, Clothing, Women's clothing, Sweaters, Crewneck sweaters\nproducts, Clothing, Men's clothing, Costumes, Scary costumes\nproducts, Clothing, Outerwear, Vests, Sweater Vests\nproducts, Shoes, Flats, T-strap flats, Open toe T-strap flats\nproducts, Shoes, Ballet flats, Ankle strap ballet flats, Nude ankle strap ballet flats\nproducts, Jewelry, Jewelry Sets, Choker, Gothic Choker\nproducts, Electronics, office electronics, Calculators, Scientific Calculators\nproducts, Books, Non-fiction, Self-Help, Codependency\nPrompt Example\nentity_name = \u201cRestaurant\u201d\nscenario_desc = \u201cvenue rating platform like Yelp\u201d\nprefix = \u201cbusiness venues\u201d\nExamples of Generated Nodes\nbusiness venues, Restaurant, American, Barbecue, BBQ fusion\nbusiness venues, Restaurant, Buffet, Chinese buffet, Seafood\nbusiness venues, Cafe, Tea house, Tea room, British tea house\nbusiness venues, Cafe, Brunch spot, Buffet brunch, Vegan buffet\nbusiness venues, Bar, Karaoke Bar, Karaoke DJ nights, live band karaoke\nbusiness venues, Nightclub, Live Music Venue, Jazz Club, Latin Jazz Club\nbusiness venues, Fast Food Restaurant, Smoothie Bar, Specialty smoothie bar, \nFresh fruit smoothie bar\nbusiness venues, Drive-Thru Restaurant, Fast food, Pizza place, Coal-fired pizza\nFigure 4: LLM prompt template and generation examples.\nA.3\nGraph Generation Algorithm\nA.3.1\nPrompt Template and Generation Examples. In this\nsection, we present our prompt strategy for leveraging the Lan-\nguage Model (LLM) to divide general nodes into more fine-grained\nentities. Figure 4 illustrates our prompt template, with key parame-\nters highlighted in red. We provide concrete examples of prompt\nparameters and showcase the generation results for both the e-\ncommerce scenario and the venue rating scenario.\nA.3.2\nNode Generation Algorithm. We elaborate the process\nof our tree-of-prompt algorithm that traversing the vertex space\nfor a specific application scenario, as in Algorithm 1.\nA.3.3\nEdge Generation Algorithm. We elaborate our edge gen-\neration algorithm based on LLM-given node representations and\nthe Gibbs sampling algorithm in Algorithm 2. Here we illustrate the\ncase for generating person-entity relations, which is more complex\ncompared to the entity-entity relation generation.\n",
    "2311.10370": "Few-shot Message-Enhanced Contrastive Learning\nfor Graph Anomaly Detection\n1st Fan Xu\nSchool of Computer Science and Technology\nUniversity of Science and Technology of China\nAnhui, China\nmarkxu@mail.ustc.edu.cn\n2nd Nan Wang*\nSchool of Software Engineering\nBeijing Jiaotong University\nBeijing, China\nwangnanbjtu@bjtu.edu.cn\n3rd Xuezhi Wen\nSchool of Software Engineering\nBeijing Jiaotong University\nBeijing, China\n22126399@bjtu.edu.cn\n4th Meiqi Gao\nSchool of Software Engineering\nBeijing Jiaotong University\nBeijing, China\n23126427@bjtu.edu.cn\n5th Chaoqun Guo\nSchool of Software Engineering\nBeijing Jiaotong University\nBeijing, China\nchqguo@bjtu.edu.cn\n6th Xibin Zhao*\nSchool of Software Engineering\nTsinghua University\nBeijing, China\nzxb@tsinghua.edu.cn\nAbstract\u2014Graph anomaly detection plays a crucial role in\nidentifying exceptional instances in graph data that deviate\nsignificantly from the majority. It has gained substantial at-\ntention in various domains of information security, including\nnetwork intrusion, financial fraud, and malicious comments, et\nal. Existing methods are primarily developed in an unsupervised\nmanner due to the challenge in obtaining labeled data. For lack\nof guidance from prior knowledge in unsupervised manner, the\nidentified anomalies may prove to be data noise or individual\ndata instances. In real-world scenarios, a limited batch of labeled\nanomalies can be captured, making it crucial to investigate the\nfew-shot problem in graph anomaly detection. Taking advantage\nof this potential, we propose a novel few-shot Graph Anomaly\nDetection model called FMGAD (Few-shot Message-Enhanced\nContrastive-based Graph Anomaly Detector). FMGAD leverages\na self-supervised contrastive learning strategy within and across\nviews to capture intrinsic and transferable structural repre-\nsentations. Furthermore, we propose the Deep-GNN message-\nenhanced reconstruction module, which extensively exploits the\nfew-shot label information and enables long-range propagation\nto disseminate supervision signals to deeper unlabeled nodes.\nThis module in turn assists in the training of self-supervised\ncontrastive learning. Comprehensive experimental results on six\nreal-world datasets demonstrate that FMGAD can achieve better\nperformance than other state-of-the-art methods, regardless of\nartificially injected anomalies or domain-organic anomalies.\nIndex Terms\u2014graph anomaly detection, few-shot, Deep-GNN\nI. INTRODUCTION\nGraph serves as a versatile representation of structured data,\nfacilitating systematic modeling of complex dependencies\namong instances. It has been widely used in diverse domains\nlike social networks, finance, biology, and transportation [1]\u2013\n[3]. The rapid progress of industrial and internet technologies\nhas led to a surge in the frequency of anomalous instances,\nencompassing fraudulent activities within social networks and\nthe unauthorized disclosure of sensitive corporate information.\n\u2217Nan Wang and Xibin Zhao are the corresponding authors\nConsequently, graph anomaly detection has garnered substan-\ntial attention from both industrial and academic communities.\nGraph neural networks (GNNs) [4] have made significant\nadvancements in graph representation learning by extending\ndeep learning methods to graph-structured data, and they\nhave found wide applications in graph anomaly detection.\nUnlike traditional anomaly detection methods that focus on\nvector data, graph anomaly detection requires the simultane-\nous exploration of both node attribute information and graph\nstructure information, which is challenging for conventional\napproaches [5]. While, leveraging GNNs for modeling com-\nplex graph-structured data allows for the joint encoding of\nintricate interactions among instances and their respective\nattribute features, thereby facilitating the identification of\nanomalous nodes.\nDue to the labor-intensive and time-consuming nature of\nacquiring labeled anomaly data, most existing models in graph\nanomaly detection are developed in an unsupervised manner.\nFor instance, DOMINANT [6] proposed a deep autoencoder\nthat utilizes graph convolutional networks (GCNs) to recon-\nstruct attributes and structure, thereby enhancing detection\nperformance. GAAN [7] employs generative adversarial net-\nworks and generates pseudo-anomalies by utilizing Gaussian\nnoise for discriminative training. Furthermore, with the rise\nof self-supervised learning, graph anomaly detection methods\nbased on contrastive learning have gained popularity. For\nexample, CoLA [8] employs random walks for graph aug-\nmentation, constructs positive and negative pairs, and designs\nproxy tasks for contrastive learning. Research findings have\ndemonstrated that contrastive learning-based graph anomaly\ndetection methods have achieved state-of-the-art performance\nin unsupervised settings.\nHowever, due to the complexity and diversity of anomalies,\nas well as the lack of guided supervision from prior knowl-\nedge, unsupervised methods may suffer from local optima or\narXiv:2311.10370v1  [cs.LG]  17 Nov 2023\nexhibit biased anomaly detection performance. Nowadays, do-\nmain experts have provided feedback indicating that obtaining\na limited number of labeled anomalies is feasible [9]. These\nlabeled anomalies can serve as prior knowledge to guide\nmodel training and have great potential for improving graph\nanomaly detection performance. However, detecting anoma-\nlies in a few-shot setting remains a significant challenge. Ex-\nisting semi-supervised and positive-unlabeled (PU) learning\nmethods [10] have not yielded satisfactory results in this task.\nThey rely on a sufficient number of labeled anomaly samples,\nmaking it difficult to effectively utilize supervised information\nin few-shot scenarios. Recently, some methods utilize meta-\nlearning [11] and cross-domain transfer learning approaches\n[12] to address the few-shot setting. For instance, GDN [13]\nincorporates a meta-learning algorithm across networks to\ntransfer meta-knowledge from multiple auxiliary networks for\nfew-shot network anomaly detection. However, these methods\nhave requirements for auxiliary networks or datasets, which\nare often difficult to obtain in real-world scenarios.\nTo\naddress\nthe\naforementioned\nchallenges,\nwe\npro-\npose a Few-shot Message-enhanced Contrastive-based Graph\nAnomaly Detector (FMGAD) that combines the rational uti-\nlization of few-shot labels with self-supervised contrastive\nlearning. FMGAD consists of two main modules: (i)Multi-\nview contrastive learning module adopts the core idea of\nmulti-view contrastive learning to facilitate both intra-view\nand cross-view contrastive learning. (ii)Deep-GNN message-\nenhanced reconstruction module leverages spectral high-pass\nfiltering to design a deep message-passing network, effectively\nutilizing the few-shot label information. This module assists\nthe Multi-view Contrastive Learning Module in learning tai-\nlored representations for the anomaly detector. The framework\nof our approach is illustrated in Fig 1. To summarize, our main\ncontributions are summarized as follows:\n\u2022 To ensure that the self-supervised module can learn an\noptimal representation, we employ graph augmentation\nto obtain multiple views, enabling contrastive learning\nwithin and across views.\n\u2022 To effectively utilize the few-shot label information\nand leverage it to assist the training of contrastive\nlearning, we propose a Deep-GNN Message-Enhanced\nReconstruction Module that provides a sufficiently large\nreceptive field for the few-shot labeled nodes.\n\u2022 We conduct extensive experiments on six real-world\ndatasets with synthetically injected anomalies and or-\nganic anomalies. The experimental results demonstrate\nthe effectiveness of our approach in few-shot graph\nanomaly detection.\nII. RELATED WORK\nIn this section, we briefly describe the related work on (1)\nGraph Anomaly Detection; (2) Few-shot Graph Learning and\n(3) Graph Augmentation.\nA. Graph Anomaly Detection\nLike other graph-based methods, semi-supervised learning\nis the most common graph representation learning mode\nand is also used in the field of graph anomaly detection.\nSemiGNN [14] utilizes a hierarchical attention mechanism\nto better associate different neighbors and different views.\nBWGNN [15] designs a band-pass filter kernel function sat-\nisfying Hammond\u2019s Graph Wavelet, transmitting information\nin corresponding frequency bands separately. Since anomalies\nare difficult to obtain, most existing methods are based on\nunsupervised modes and are mainly divided into two types:\ngraph autoencoder and self-supervised contrastive learning.\nGAE (Graph Autoencoder) [16] reconstructs node features\nusing an Encoder-Decoder architecture and defines nodes with\nhigh reconstruction loss as anomalous. DOMINANT [6] si-\nmultaneously reconstructs both structural information, such as\nthe adjacency matrix, and node attributes to calculate anomaly\nscores. In recent years, with the rise of self-supervised learn-\ning and proxy tasks, various contrastive learning strategies\nhave been widely applied. CoLA [8] utilizes random walk\nsampling to perform graph augmentation and subsequently\nconstructs positive and negative node-subgraph pairs for con-\ntrastive learning. GraphCAD [17] employs a global clustering\nalgorithm to partition the entire graph into multiple parts,\nwhere nodes injected from other parts are regarded as pseudo-\nanomalies, forming negative pairs. GRADATE [18] adopts\nedge modification graph augmentation technique and incorpo-\nrates three types of contrastive learning strategies: node-node,\nnode-subgraph, and subgraph-subgraph.\nB. Few-shot Graph Learning\nIn most real-world scenarios, only very limited labeled\nsamples are often available due to expensive labeling costs.\nIn view of this, graph few-shot learning and cross-network\nmeta learning are proposed to solve the problem of perfor-\nmance degradation when facing limited labeled data to a\ncertain extent. For instance, GDN [13] is equipped with a\ncross-network meta-learning algorithm that utilizes a small\nnumber of labeled anomalies to enhance statistically signifi-\ncant deviations between abnormal nodes and normal nodes\non the network. Meta-PN [19] infers high-quality pseudo-\nlabels on unlabeled nodes via a meta-learning label propa-\ngation strategy while achieving a large receptive field during\ntraining. However, cross-domain auxiliary datasets are not\nalways available, thus many non-meta-learning strategies have\nbeen explored. ANEMONE-FS [20] contains two multi-scale\ncomparison networks, where the consistencies between nodes\nand contextual embeddings are maximized for unlabeled node\nwhile minimized for labeled anomalies in a mini-batch.\nC. Graph Augmentation\nSimilar to the vision domain, there are numerous augmen-\ntation methods in the field of graph representation learning.\nSpecifically, graph augmentation techniques alter the attribute\nand structural characteristics of graph datasets within a certain\nMulti-view Contrastive Learning Module\nInput Graph\nDeep-GNN Few-shot Message-Enhanced Module\nTarget Node\nSubgraph Sampling\nPositive\nNegative\nPositive\nNegative\nGNN Encoder\nGNN Encoder\nReadout\nReadout\nContrastive Loss\n(+)\n(+)\n(-)\n(-)\nSubgraph Embedding\nSubgraph Embedding\nNode Embedding\nMask target node\nfew-shot labeled nodes\nunlabeled nodes\nFew-shot view\nLow pass GNN\nHigh pass GNN\nOrigin view\nConcat\nGNN Encoder\nReconstruction Loss\n\u2112!\"# = \ud835\udefe$\u2112%& + \ud835\udefe'\u2112&&\n\u2112= \u2112!\"# + \ud835\udf13\u2112()!\n\u2112!\"# = 1\n\ud835\udc41%\n$%&\n'\n('\ud835\udc65$ \u2212\ud835\udc65$)(\nFig. 1. The above image presents an overview of our model FMGAD, where the architecture demonstrates the details of the multi-view contrastive learning\nmodule(Right) and Deep-GNN few-shot message-enhanced module(Left) respectively.\nrange, providing convenience for self-supervised learning. The\nmajority of existing methods focus on manipulating nodes or\nedges within the graph. These methods include: (i) enhancing\nby modifying or masking node features [21], (ii) adapting\nthe adjacency matrix or adjusting edge weights [22], and (iii)\nutilizing Restarted Random Walk (RoSA) [23] to generate\naugmented local views.\nIII. PROBLEM DEFINITION\nIn this section, we first introduce the notations mentioned\nin this paper, and then give the formal problem definition.\nGiven an attributed graph G = (X, A), we denote its node\nattribute (i.e., feature) and adjacency matrices as X \u2208Rn\u00d7d\nand A \u2208Rn\u00d7n, where n and d are respectively the number\nof nodes and feature dimensions. It can also be defined\nas G = (V, E, X), where V = {v1, v2, . . . , vn} and E =\n{e1, e2, . . . , en} represent node and edge sets respectively.\nThe definition of Few-shot GAD is to use the attribute and\nstructure information of the graph to detect anomalies when a\nfew-shot abnormal labeled nodes are known. We have a small\nset of labeled anomalies VL and the rest set of unlabeled\nnodes VU , where |VL| << |VU|, since the labeled anomaly\nnodes are difficult to obtain and few of them can be actually\nused. Then, our goal is to learn a model F(\u00b7) : RN\u00d7D \u2192\nRN\u00d71 on VL \u222aVU, which measures node abnormalities by\ncalculating their anomaly scores y.\nIV. METHODOLOGY\nIn this section, we present the details of our proposed\napproach FMGAD for detecting graph node anomalies in\nfew-shot scenarios. As shown in Fig 1, our approach mainly\nconsists of two modules, including multi-view contrastive\nlearning module and Deep-GNN message-enhanced recon-\nstruction module. Graph anomalies are typically categorized\nas attribute-context anomalies and structural anomalies, and\nour method addresses both aspects. Firstly, we employ suit-\nable graph augmentation techniques to construct different\nviews and perform subgraph sampling for each target node.\nNext, to fully explore structural anomalies, we utilize proxy\ntasks and design a multi-view contrastive learning framework.\nSubsequently, to investigate features at the attribute-context\nlevel and leverage existing few-shot labels, we build a deep\ninformation augmentation reconstruction module. In all, our\nmodel starts from the essence of graph anomalies, designs\nself-supervised learning objectives, and incorporates super-\nvised constraints using few-shot labels. In the rest of this\nsection, we demonstrate the details of the whole framework\nrespectively.\nA. Graph Augmentation\nThe self-supervised strategy based on contrastive learning\nenables not only differentiation learning within the same\nscale, such as \u201dnode vs. node,\u201d but also discrimination across\ndifferent scales, such as \u201dnode vs. subgraph.\u201d As discussed\nin related work, to ensure that the self-supervised learning\nmodule can extract rich attribute and structural information,\nit is necessary to design augmentation strategies and proxy\ntasks tailored to the current task. For graph anomaly detection,\naccording to [reference], anomalies in graph nodes often\nmanifest as a mismatch with their surrounding environment.\nFor several popular graph augmentation strategies in current\ngraph representation learning, such as node feature pertur-\nbation or masking and edge modification. Including Graph\nDiffusion, it essentially involves perturbing the adjacency\nmatrix and modifying the target edges. We argue that these\nstrategies are not suitable for graph anomaly detection because\nthey may alter the underlying logic or semantic features of\nthe data. This could particularly have negative effects on\ndetecting naturally occurring anomalies rather than artificially\ninjected anomalies. Hence, we utilize random walks with\nrestart (RWR) to obtain augmented views. Specifically, for\neach selected target node, we sample subgraphs of fixed size\np. Unlike standard random walks, RWR introduces a restart\nprobability, where there is a certain probability of restarting\nfrom the initial node at each step. Therefore, using RWR to\nsample subgraphs does not introduce additional anomalies.\nB. Multi-view Contrastive Learning Module\nFurthermore, we constructed a multi-view contrastive learn-\ning module. This module utilizes GNN encoders and decoders\nto perform contrastive learning between the target node and\ntwo views, simultaneously learning discriminative attribute\nand structural topological information. It consists of two parts:\nNode-Subgraph and Subgraph-Subgraph, capturing features\nwithin each view and across different views respectively.\nNode-Subgraph Contrast. In each view, a target node vi\nforms a positive pair with its located subgraph and forms a\nnegative pair with a random subgraph where another node\nvj is located. We first adopt a GCN encoder that maps the\nfeatures of nodes in the subgraph to the embedding space.\nThe hidden-layer representation can be defined as:\nH\u2113+1\n\u03c9\n= GNN(A\u03c9, H\u2113\n\u03c9) = \u03c3(D\n\u22121\n2\n\u03c9 A\u03c9D\n\u22121\n2\n\u03c9 H\u2113\n\u03c9W \u2113),\n(1)\nwhere H\u2113+1\n\u03c9\nand H\u2113\n\u03c9 denote the (\u2113+ 1)-th and \u2113-th layer\nhidden representation in view \u03c9, eD\n\u22121\n2\n\u03c9\neA\u03c9 eD\n\u22121\n2\n\u03c9\nis the nor-\nmalization of the adjacency matrix in view \u03c9i and W \u2113is\nthe network parameters. It is noteworthy that the networks\noperating under two views employ identical architecture and\nparameter sharing. Then we take the average pooling function\nas the readout module to obtain the subgraph-level embedding\nvector e\u03c9:\ne\u03c9 = READOUT(H\u03c9) = 1\nK\nK\nX\nj=1\n(H\u03c9)K,\n(2)\nwhere K denotes the number of remaining nodes in the\nsubgraph. Given that the target node is masked within the\nsubgraph, we utilize the weight matrix of the GCN encoder\nto project the features onto a shared embedding space. Math-\nematically, this can be formulated as follows:\nh\u2113+1\n\u03c9\n= \u03c3(h\u2113\n\u03c9W \u2113).\n(3)\nIn each view, the anomalous degree of a target node\ndepends on its similarity to the paired subgraph embedding.\nTherefore, we choose a Bilinear model to quantify the rela-\ntionship:\ns\u03c9 = sigmoid(e\u03c9Ws hT\n\u03c9),\n(4)\nwhere Ws is a learnable matrix. We employ the binary cross-\nentropy loss to measure the contrastive loss in a single view\nthat can be demonstrated as:\nL\u03c9\nNS = \u2212\nN\nX\ni=1\n(yi log (s\u03c9i) + (1 \u2212yi) log (1 \u2212s\u03c9i)) ,\n(5)\nwhere yi is equal to 1 when s\u03c9i denotes a positive pair, and\nis equal to 0 when s\u03c9i denotes a negative pair. The same\noperations and model architecture are used on the second\nview, and both views share model parameters. Thus the final\nnode-subgraph contrast loss is:\nLNS = \u03b1L1\nNS + (1 \u2212\u03b1)L2\nNS,\n(6)\nwhere \u03b1 \u2208(0, 1) is a trade-off parameter to balance the\nimportance between two views.\nSubgraph-Subgraph Contrast. Instead of intra-view con-\ntrast, subgraph-subgraph contrast implements cross-view con-\ntrastive learning. It aims to learn more representative subgraph\nembeddings, thereby enhancing the neighborhood representa-\ntions of target nodes. Specifically, a subgraph establishes a\npositive pair with the subgraph formed by its target node vi in\nanother view, while it forms negative pairs with two subgraphs\nwhere another node vj is located in both views. Inspired by\n[], we employ a loss function to optimize the contrast:\nLSS = \u2212\nn\nX\ni=1\nlog\nexp (e1i \u00b7 e2i)\nexp (e1i \u00b7 e1j) + exp (e1i \u00b7 e2j),\n(7)\nwhere e1i and e1i denote the embeddings of the subgraphs\nthat the target node vi belongs to in two views, e1j and e1j\nrepresent the embeddings of the subgraphs of another node\nvj separately. Then the final multi-view contrastive loss is:\nLcon = \u03b3LNS + (1 \u2212\u03b3)LSS,\n(8)\nwhere \u03b3 \u2208(0, 1) balances the influence of two contrastive\nlearning modes.\nC. Deep-GNN Message-Enhanced Reconstruction Module\nIn the context of few-shot scenarios, the availability of\nanomaly label information is severely limited. Conventional\nsemi-supervised graph anomaly detection methods suffer from\nthe issue of over-smoothing, making it challenging to extend\nthe receptive field and effectively propagate label informa-\ntion to deeper neighborhoods. To address this challenge,\nwe propose leveraging the concept of AutoEncoder from\nunsupervised methods to reconstruct attributes. Additionally,\nwe introduce a scalable deep graph neural network (GNN)\narchitecture to enhance the utilization of few-shot labels and\ntheir associated features, thereby improving the performance\nof anomaly detection in graph data.\nInitially, we extract a few-shot environmental subgraph\nfrom the original graph, comprising a subgraph originating\nfrom the few-shot labeled node and encompassing its M-\norder neighbors. To facilitate the sparse message enhanced\nfeature reconstruction process, distinct graph neural network\n(GNN) architectures are employed for encoding the original\ngraph and the few-shot environment subgraph. In particular,\nfor the original view, GNN encoder is with low-pass filter-\ning characteristics, such as GCN, GAT, GIN. These GNN\nmodels effectively capture and propagate information within\nthe graph, enabling accurate attribute reconstruction and sub-\nsequent anomaly detection. The transform of corresponding\nGNN encoder is as follows:\nH\u2113+1\nr\n= \u03c3(D\u22121/2AD\u22121/2H\u2113\nrWr).\n(9)\nTo leverage the specific attributes of sparse anomaly sam-\nples within the few-shot environment subgraph and their high-\norder correlation with the surrounding context, we propose a\nscalable deep graph neural network (Deep-GNN) [24] archi-\ntecture that enables long-range propagation. This approach\nallows for the consideration of a broader range of context\nnodes, thereby expanding the receptive field of sparse anomaly\nsamples. To address the challenge of over-smoothing that\narises when increasing the propagation step size in GNN, we\nintroduce a high-pass filtering GNN [25] that operates in the\nspectral domain:\nFH = \u03b5I \u2212D\u22121/2AD\u22121/2 = (\u03b5 \u22121)I + L,\n(10)\nH\u2113+1\nf\n= \u03c3(FHH\u2113\nfWf).\n(11)\nAccording to [25], high-pass filtering GNN can overcome\nthe over-smoothing problem to a certain extent, and therefore\ncan be extended to more layers. Then we concatenate the node\nembeddings obtained from the original graph and the few-shot\nenvironmental subgraph:\nH = CONCAT(Hr, Hf),\n(12)\nfor nodes that do not appear in the few-shot environment\nsubgraph, their hf is padded with 0. Then a layer of MLP is\napplied to obtain the reconstructed node embeddings:\nb\nX = MLP(H).\n(13)\nThe reconstruction loss of the original graph is calculated\nby MSE loss:\nLrec = 1\nN\nN\nX\ni=1\n(bxi \u2212xi)2.\n(14)\nD. Anomaly Detector\nTo jointly train the multi-view contrastive learning module\nand the Deep-GNN message-enhanced reconstruction module,\nwe optimize the following objective function:\nL = Lcon + \u03c8Lrec,\n(15)\nwhere \u03c8 is a controlling parameter which balances the impor-\ntance of the two modules. By minimizing the above objective\nfunction, we can compute the anomaly score of each node.\nV. EXPERIMENTS\nIn this section, we conduct empirical evaluations to show-\ncase the efficacy of the proposed framework. Our primary\nobjective is to address the following research inquiries:\n\u2022 RQ1. Can our method perform well in extreme few-shot\nscenarios?\n\u2022 RQ2. How our model behave when changing the degree\nof label availability and the number of Deep-GNN lay-\ners?\n\u2022 RQ3. How do the key designs and components influence\nthe performance of our method?\nA. Experimental Settings\nDataset. To thoroughly evaluate our method\u2019s performance\nin identifying both naturally occurring organic anomalies and\nartificially injected anomalies, we selected two categories of\ndatasets. The first category consists of two authentic datasets:\nCora [26] and Citeseer [27], that do not inherently contain\norganic anomalies but require manual injection of anomalies.\nThe second category comprises three authentic datasets: Wiki\n[28], Reddit [29] and YelpChi [30], that inherently contain\norganic anomalies. For anomaly injection, we followed the\nsame approach as DOMINANT by injecting the same number\nof feature and structural anomalies into the three datasets that\npreviously did not have any organic anomalies.\nTABLE I\nSTATISTICS OF DATASETS\nDataset\nNodes\nEdges\nFeatures\nAnomaly\nRatio(%)\nCora\n2,708\n5,429\n1433\n150\n5.54\nCiteSeer\n3327\n10,154\n3703\n150\n4.51\nWiki\n9,227\n18,257\n64\n217\n2.35\nReddit\n15,860\n136,781\n602\n796\n5.02\nYelpChi\n23,831\n98,630\n32\n1,217\n5.11\nCompared Methods. We compare our proposed method\nFMGAD with other three categories of methods. (i) Conven-\ntional semi-supervised GNN models: GCN [4], GAT [31],\nand semi-supervised methods designed for GAD: SemiGNN\n[14], BWGNN [15]. (ii) Unsupervised GNN-based graph\nanomaly detection methods: DOMINANT [6], CoLA [8],\nGraphCAD [17] and GRADATE [18]. (iii) Few-shot methods\non graph anomaly detection: GDN [13], Meta-PN [19] and\nANEMONE-FS [20].\nTABLE II\nPERFORMANCE COMPARISON RESULTS (10-SHOT) W.R.T. AUC-ROC AND AUC-PR ON FIVE DATASETS.\nMethods\nCora\nCiteseer\nWiki\nReddit\nYelpChi\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nAUC-ROC\nAUC-PR\nGCN\n0.5239\n0.0427\n0.4128\n0.055\n0.4324\n0.0239\n0.4975\n0.0826\n0.3371\n0.0725\nGAT\n0.5473\n0.0495\n0.4645\n0.062\n0.4373\n0.0284\n0.5184\n0.1225\n0.3564\n0.0834\nSemiGNN\n0.6637\n0.1293\n0.5322\n0.074\n0.4785\n0.0332\n0.6249\n0.1953\n0.4146\n0.1378\nBWGNN\n0.6855\n0.1876\n0.5421\n0.081\n0.4668\n0.0295\n0.5863\n0.1634\n0.5473\n0.2161\nDOMINANT\n0.7483\n0.2741\n0.8279\n0.2415\n0.4488\n0.0227\n0.7429\n0.3185\n0.4872\n0.1652\nCoLA\n0.7515\n0.2398\n0.8738\n0.2942\n0.5373\n0.0319\n0.7257\n0.2474\n0.3985\n0.1579\nGraphCAD\n0.7674\n0.2892\n0.8521\n0.2787\n0.5282\n0.0249\n0.7536\n0.2643\n0.4238\n0.1843\nGRADATE\n0.7786\n0.2973\n0.8872\n0.3471\n0.5471\n0.0322\n0.7472\n0.2879\n0.4994\n0.2164\nGDN\n0.7736\n0.1965\n0.7963\n0.1826\n0.5248\n0.0326\n0.8136\n0.3084\n0.7281\n0.2785\nMeta-PN\n0.8537\n0.2817\n0.8127\n0.2273\n0.4663\n0.0276\n0.8064\n0.3126\n0.7549\n0.2698\nANEMONE-FS\n0.8836\n0.3062\n0.9028\n0.3294\n0.5317\n0.0348\n0.8123\n0.3352\n0.7729\n0.2977\nFMGAD\n0.8928\n0.3187\n0.9193\n0.3981\n0.6133\n0.0438\n0.8326\n0.3561\n0.8052\n0.3338\nEvaluation Metrics. We employ two popular and effective\nmetrics for evaluation, the Area Under Receiver Operat-\ning Characteristic Curve (AUC-ROC) and the Area Under\nPrecision-Recall Curve (AUC-PR) [32]. AUC-ROC quantifies\nthe ability of a binary classifier by measuring the area under\nthe receiver operating characteristic curve. AUC-PR captures\nthe trade-off between the two metrics and is particularly useful\nwhen the dataset is imbalanced or when the focus is on\npositive instances.\nImplementation Details.\nAll our experiments are conducted with a 24 GB 3090 GPU,\nand the proposed FMGAD is mainly implemented through\npyg library. In our implementation, the size K of subgraph of\neach target node and the dimension of hidden layer are fixed to\n8 and 128, respectively. In the contrastive learning module,\nthe GNN network is set to 2 layers; in the reconstruction\nmodule, the low-pass and high-pass GNN Encoder are set to\n2 and 5 layers. For each dataset, we set the number of few-\nshot labeled anomalies as 10, and the trade-off parameters\n\u03b1, \u03b31, \u03b32, \u03c8 are chosen as 0.7, 0.6, 0.4, and 0.5 separately.\nB. Experimental Results (RQ1)\nIn this subsection, we consider semi-supervised, unsuper-\nvised and other few-shot baseline methods for comparing\nwith our methods in terms of AUC-ROC and AUC-PR. To\nensure few-shot scenarios, for all few-shot GAD methods,\nwe use 10 annotated anomalies during model training. Tab II\nshows the overall performance comparison on both artificially\ninjected anomaly and organic anomaly datasets. FMGAD\nconsistently outperforms all baseline methods on all six\nreal-world datasets, thereby validating the effectiveness of\nour approach in addressing anomaly detection in few-shot\nscenarios. Based on the experimental results, we have the\nfollowing observations:\n\u2022 Conventional semi-supervised graph anomaly detection\nmethods (i.e., GCN, GAT, SemiGNN, and BWGNN)\ngenerally do not exhibit competitive performance, indi-\ncating their limited ability to exploit label information.\nIt performs even worse than unsupervised methods on\nalmost all datasets. This discrepancy can be attributed\nto the reliance of conventional semi-supervised methods\non sufficient label information for message propagation,\nwhich exacerbates the over-smoothing issue in few-shot\nscenarios and hinders the learning of abnormal features.\nHowever, unsupervised methods leverage AutoEncoder\nor contrastive learning strategies to uncover deep data\ndistributions based on local features and structures. Thus,\nthey can achieve strong discrimination capabilities when\nit comes to identifying artificially injected anomalies.\n\u2022 On datasets with artificially injected anomalies, the un-\nsupervised methods achieve performance that matches\nexisting few-shot graph anomaly detection methods.\nHowever, on organic anomaly datasets, unsupervised\nmethods generally underperformed compared to few-shot\nmethods. In particular, compared to the GRADATE, on\nYelpChi dataset, our FMGAD has 60.35% and 54.25%\nimprovement w.r.t. AUC-ROC and AUC-PR, respec-\ntively. This is most likely because real data often pos-\nsesses numerous expert priors, and unsupervised methods\ntend to blindly map and partition features.\n\u2022 In comparison to existing few-shot graph anomaly detec-\ntion methods, our approach has demonstrated notable ad-\nvancements. To be specific, on Wiki dataset, our method\nFMGAD outperforms GDN by 16.86% and 34.36% in\nterms of AUC-ROC and AUC-PR, respectively. The three\nmethods we compared are all founded on meta-learning\nprinciples, and the efficacy of meta-learning methods\nrelies heavily on the quality of the auxiliary network or\ndataset. However, in many real-world scenarios, datasets\noften do not meet such stringent requirements.\nC. Sensitivity & Robustness Analysis (RQ2)\nIn order to verify the effectiveness of FMGAD in different\nfew-shot anomaly detection settings, we change the number\nk of anomalous samples for model training to form k-\nshot learning settings for evaluation. Specifically, we perform\nexperiments on all five datasets and select k from {1, 3, 5, 10,\n15, 20}. The experimental results are summarized in Tab III.\nTABLE III\nFEW-SHOT PERFORMANCE ANALYSIS OF FMGAD.\nSetting\nCora\nCiteseer\nWiki\nReddit\nYelpChi\n1-shot\n0.8681\n0.9039\n0.5854\n0.8216\n0.7667\n3-shot\n0.8843\n0.9126\n0.6003\n0.8244\n0.7786\n5-shot\n0.8906\n0.9177\n0.6078\n0.8263\n0.7928\n10-shot\n0.8946\n0.9193\n0.6133\n0.8326\n0.8052\n15-shot\n0.8921\n0.9225\n0.6158\n0.8367\n0.8127\nAs observed, even in scenarios where only 1-shot anomalies\nare provided, FMGAD can still outperform other baseline\nmethods, demonstrating its superior performance. For in-\nstance, on Reddit dataset, the FMGAD with 1-shot anomaly\noutperforms GraphCAD by 9.02% in terms of AUC-ROC.\nWhen compared with ANEMONE-FS, it achieves improve-\nments of 6.52% in terms of AUC-ROC with 5-shot anomalies.\nThis demonstrates the effectiveness of the FMGAD method\nfor extremely limited anomalous labels. Furthermore, we\nalso observe that as the number of few-shot anomaly labels\nincreases, FMGAD\u2019s performance generally improves, which\nfurther confirms the effectiveness of our method.\nSubsequently, we investigated the effects of varying the\nnumber of Deep-GNN layers in the reconstruction module and\nadjusting the number of nodes through RWR sampling in the\nenhanced subgraph sampling of the contrastive learning mod-\nule on model performance. The corresponding experimental\nresults are shown in Fig 2.\nFig. 2. Performance with different number of Deep-GNN layers and the size\nof subgraph sampled by RWR.\nAnalyzing the image on the left, we observe a trend where\nthe model performance initially improves with an increasing\nnumber of sampling subgraph nodes. However, beyond a\ncertain threshold, further increments in the number of nodes\nlead to a diminishing effect on the model\u2019s performance.\nThis is because insufficient sampling of the target node\nsubgraph makes it challenging for the model to capture the\nlocal structural characteristics of the data, leading to subpar\nperformance. Conversely, if the sampled subgraph is exces-\nsively large, it may contain redundant information, thereby\nadversely affecting model performance. Observing the graph\non the right, we note that with an increase in the number of\nDeep-GNN layers, the model performance exhibits a slight\nimprovement initially, followed by a subsequent decline. We\nattribute the performance improvement to the Deep-GNN\nnetwork effectively propagating label information to more\ndistant neighbors within the graph. However, an excessive\nnumber of layers will inevitably introduce the challenge of\nover-smoothing, which can negatively impact the model\u2019s\nperformance. Hence, finding an optimal balance in the size of\nthe sampled subgraph and the number of Deep-GNN layers\nis crucial for achieving optimal results.\nD. Ablation Study (RQ3)\nIn order to verify the effectiveness of each key com-\nponent of FMGAD, we conduct an ablation study on the\nvariants of the proposed approach. Concretely, we introduce\nthree variants of our approach: FMGAD-ns and FMGAD-ss,\nwhich individually exclude the node-subgraph and subgraph-\nsubgraph contrastive learning sub-modules, and FMGAD-\nre, which omits the Deep-GNN few-shot message-enhanced\nmodule. The detailed results are shown in Fig 3.\nFig. 3. Ablation Performance on different variants.\nAs observed above, for each variant that excludes a specific\nmodule, there has been a noticeable degradation in the model\u2019s\nperformance. Among these variants, FMGAD-ns stands out\nas the most significantly impacted, as it eliminates the node-\nsubgraph contrastive sub-module. Specifically, it drops by\n8.62% and 13.17% on YelpCHi datasets in terms of AUC-\nROC and AUC-PR. In summary, through ablation studies, we\naffirm the robustness and efficacy of our proposed technique in\naddressing graph anomaly detection under few-shot scenarios.\nVI. CONCLUSION\nIn this paper, we investigate the problem of graph anomaly\ndetection in few-shot scenarios. Through a comprehensive\nanalysis of existing semi-supervised, unsupervised, and cus-\ntomized few-shot methods, we propose FMGAD, a novel\nanomaly detector that combines few-shot message enhance-\nment with multi-view self-supervised contrastive learning.\nOur model effectively utilizes the self-supervised contrastive\nlearning strategy to capture local structures and features\nwithin the graph. Additionally, we introduce a deep message-\npassing mechanism that incorporates high-pass convolutional\nfiltering functions to enable deep propagation of few-shot\nnode information. Extensive experiments conducted on mul-\ntiple real-world datasets demonstrate the outstanding perfor-\nmance of FMGAD.\nACKNOWLEDGMENT\nThis work was supported in part by the Supported by the\nFundamental Research Funds for the Central Universities un-\nder Grant 2022RC026; in part by the CCF-Tecent Open Fund\nunder Grant CCF-Tencent RAGR20220112; in part by the\nCCF-NSFOCUS Open Fund; in part by the NSFC Program\nunder Grant 62202042, Grant 62076146, Grant 62021002,\nGrant U20A6003, Grant U19A2062, Grant 62127803, and\nGrant U1911401.\nREFERENCES\n[1] Ma X, Wu J, Xue S. A comprehensive survey on graph anomaly\ndetection with deep learning[J]. IEEE TKDE, 2021.\n[2] Ding K, Shu K, Shan X. Cross-domain graph anomaly detection[J].\nIEEE TNNLS, 2021, 33(6): 2406-2415.\n[3] I. S. Jacobs and C. P. Bean, \u201cFine particles, thin films and exchange\nanisotropy,\u201d in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New\nYork: Academic, 1963, pp. 271\u2013350.\n[4] Kipf T N, Welling M. Semi-supervised classification with graph con-\nvolutional networks[J]. ICLR, 2016.\n[5] Chandola V, Banerjee A, Kumar V. Anomaly detection: A survey[J].\nACM computing surveys (CSUR), 2009, 41(3): 1-58.\n[6] Kaize Ding, Jundong Li, Rohit Bhanushali, and Huan Liu. 2019. Deep\nanomaly detection on attributed networks. In Proceedings of the 2019\nSIAM International Conference on Data Mining. SIAM, 594\u2013602.\n[7] Zhenxing Chen, Bo Liu, Meiqing Wang, Peng Dai, Jun Lv, and\nLiefeng Bo. 2020. Generative adversarial attributed network anomaly\ndetection. In Proceedings of the 29th ACM International Conference\non Information & Knowledge Management. 1989\u20131992.\n[8] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George\nKarypis. 2021. Anomaly detection on attributed networks via contrastive\nself-supervised learning. IEEE TNNLS 33, 6 (2021), 2378\u20132392.\n[9] Akoglu L, Tong H, Koutra D. Graph based anomaly detection and\ndescription: a survey[J]. Data mining and knowledge discovery, 2015,\n29: 626-688.\n[10] Zhang K, Zhang C, Peng X. Putracead: Trace anomaly detection with\npartial labels based on GNN and Pu Learning[C]. 2022 IEEE 33rd\nInternational Symposium on Software Reliability Engineering (ISSRE).\nIEEE, 2022: 239-250.\n[11] Tavares G M, Junior S B. Process mining encoding via meta-learning for\nan enhanced anomaly detection[C]. European Conference on Advances\nin Databases and Information Systems. Cham: Springer International\nPublishing, 2021: 157-168.\n[12] Wang Q, Pang G, Salehi M. Cross-domain graph anomaly detection\nvia anomaly-aware contrastive alignment[C]. Proceedings of the AAAI\nConference on Artificial Intelligence. 2023, 37(4): 4676-4684.\n[13] Kaize Ding, Qinghai Zhou, Hanghang Tong, and Huan Liu. 2021. Few-\nshot network anomaly detection via cross-network meta-learning. In\nProceedings of the Web Conference 2021. 2448\u20132456.\n[14] Daixin Wang, Jianbin Lin, Peng Cui, Quanhui Jia, Zhen Wang, Yanming\nFang, Quan Yu, Jun Zhou, Shuang Yang, and Yuan Qi. 2019. A semi-\nsupervised graph attentive network for financial fraud detection. In\n2019 IEEE International Conference on Data Mining (ICDM). IEEE,\n598\u2013607.\n[15] Tang J, Li J, Gao Z, Jia Li. Rethinking graph neural networks for\nanomaly detection[C]. International Conference on Machine Learning.\nPMLR, 2022: 21076-21089.\n[16] Wang C, Pan S, Long G. Mgae: Marginalized graph autoencoder for\ngraph clustering[C]. Proceedings of the 2017 ACM on Conference on\nInformation and Knowledge Management. 2017: 889-898.\n[17] Chen B, Zhang J, Zhang X. Graph Contrastive Learning for Anomaly\nDetection[J]. IEEE TKDE, 2021.\n[18] Duan J, Wang S, Zhang P. Graph anomaly detection via multi-scale\ncontrastive learning networks with augmented view[C]. Proceedings of\nthe AAAI Conference on Artificial Intelligence. 2023, 37(6): 7459-\n7467.\n[19] Ding K, Wang J, Caverlee J, H Liu. Meta propagation networks for\ngraph few-shot semi-supervised learning[C]. Proceedings of the AAAI\nConference on Artificial Intelligence. 2022, 36(6): 6524-6531.\n[20] Zheng Y, Jin M, Liu Y, et al. From unsupervised to few-shot graph\nanomaly detection: A multi-scale contrastive learning approach[J]. IEEE\nTKDE, 2022.\n[21] Y Zhu, Y Xu, F Yu, Q Liu, S Wu, L Wang. Graph contrastive learning\nwith adaptive augmentation[C]. Proceedings of the Web Conference\n2021. 2021: 2069-2080.\n[22] T Zhao, Y Liu, L Neves, O Woodford, M Jiang. Data augmentation\nfor graph neural networks[C]. Proceedings of the aaai conference on\nartificial intelligence. 2021, 35(12): 11015-11023.\n[23] Frederickson G N, Ja\u2019Ja\u2019 J. Approximation algorithms for several graph\naugmentation problems[J]. SIAM Journal on Computing, 1981, 10(2):\n270-283.\n[24] Gallicchio\nC,\nMicheli\nA.\nFast\nand\ndeep\ngraph\nneural\nnet-\nworks[C]//Proceedings of the AAAI conference on artificial intelli-\ngence. 2020, 34(04): 3898-3905.\n[25] D Bo, X Wang, C Shi, H Shen. Beyond low-frequency information in\ngraph convolutional networks[C]. Proceedings of the AAAI Conference\non Artificial Intelligence. 2021, 35(5): 3950-3957.\n[26] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Gal-\nligher, and Tina Eliassi-Rad. 2008. Collective classification in network\ndata. AI magazine 29, 3 (2008), 93\u201393.\n[27] Giles C L, Bollacker K D, Lawrence S. CiteSeer: An automatic citation\nindexing system[C]. Proceedings of the third ACM conference on\nDigital libraries. 1998: 89-98.\n[28] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting\ndynamic embedding trajectory in temporal interaction networks. In\nProceedings of the 25th ACM SIGKDD international conference on\nknowledge discovery & data mining. 1269\u20131278.\n[29] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive repre-\nsentation learning on large graphs. In NeurIPS.\n[30] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip\nS Yu. 2020. Enhancing graph neural network-based fraud detectors\nagainst camouflaged fraudsters. In Proceedings of the 29th ACM\nInternational Conference on Information & Knowledge Management.\n315\u2013324.\n[31] Veli\u02c7ckovi\u00b4c P, Cucurull G, Casanova A, et al. Graph attention net-\nworks[J]. ICLR, 2017.\n[32] Huang J, Ling C X. Using AUC and accuracy in evaluating learning\nalgorithms[J]. IEEE Transactions on knowledge and Data Engineering,\n2005, 17(3): 299-310.\n",
    "2312.06441": "Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum\nFan Xu3, Nan Wang1*, Hao Wu3, Xuezhi Wen1, Xibin Zhao2*, Hai Wan2\n1 School of Software, Beijing Jiaotong University, Beijing, China\n2 BNRist, KLISS, School of Software, Tsinghua University, Beijing, China\n3 IAT, University of Science and Technology of China, Hefei, China\n{markxu, wuhao2022}@mail.ustc.edu.cn;\n{wangnanbjtu, 22126399}@bjtu.edu.cn; {zxb, wanhai}@tsinghua.edu.cn\nAbstract\nGraph-based fraud detection (GFD) can be regarded as a chal-\nlenging semi-supervised node binary classification task. In re-\ncent years, Graph Neural Networks (GNN) have been widely\napplied to GFD, characterizing the anomalous possibility of\na node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs per-\nform poorly due to their assumption of homophily. In addi-\ntion, due to the existence of heterophily and class imbalance\nproblem, the existing models do not fully utilize the precious\nnode label information. To address the above issues, this pa-\nper proposes a semi-supervised GNN-based fraud detector\nSEC-GFD. This detector includes a hybrid filtering module\nand a local environmental constraint module, the two mod-\nules are utilized to solve heterophily and label utilization\nproblem respectively. The first module starts from the per-\nspective of the spectral domain, and solves the heterophily\nproblem to a certain extent. Specifically, it divides the spec-\ntrum into various mixed-frequency bands based on the corre-\nlation between spectrum energy distribution and heterophily.\nThen in order to make full use of the node label information, a\nlocal environmental constraint module is adaptively designed.\nThe comprehensive experimental results on four real-world\nfraud detection datasets denote that SEC-GFD outperforms\nother competitive graph-based fraud detectors. We release our\ncode at https://github.com/Sunxkissed/SEC-GFD.\nIntroduction\nWith the rapid development of the Internet, various fraudu-\nlent entities emerge in fraud activities, and the huge losses\ncaused by this have attracted continuous attention from in-\ndustry, academia and state agencies (Wang et al. 2018;\nPourhabibi et al. 2020; Liu et al. 2021a; Zhang et al. 2021).\nFor instance, fraudulent behaviors aimed at online payment\nhave caused huge property losses to clients; and fraudulent\nor malicious comments on websites may bring negative ef-\nfects on merchants and consumers.\nIn recent years, graph-based fraud detection meth-\nods (Hooi et al. 2017; Qin et al. 2022) have been widely used\nin practical applications. Using entities in fraud scenarios as\nnodes and interactions between entities as edges, graph-level\nfraud detection can be achieved. Furthermore, based on the\n*Corresponding author.\nCopyright \u00a9 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nconstructed fraud graph, as a deep neural network for graph\nrepresentation learning, Graph Neural Network (GNN) (Jin\net al. 2021) has been demonstrated to be effective in this\ntask. Specifically, GNN employs neural modules to aggre-\ngate neighbor information and update node representations,\nenabling the discrimination of fraudulent entities.\nHowever, fraud detection scenarios are inherently het-\nerophily (Zhu et al. 2021), i.e., target nodes (like node 5\nin Figure 1) and their direct neighbors are prone to differ-\nent class labels or features. In practical applications, fraud-\nulent entities may use benign entities as springboards to\nperform high-order interactions with multi-hop neighbors\nto accomplish fraudulent actions or transmit fraudulent in-\nformation, e.g., fraud node 3 and 6 are two-hop neigh-\nbors in Figure 1. For instance, in financial transaction net-\nworks (Akoglu, Tong, and Koutra 2015), fraudsters often use\nnon-fraudulent users to conduct transactions.\nDue to the existence of heterophily, the performance of\nconventional graph neural network is unsatisfactory. This is\nbecause vanilla GNN is naturally a low-pass filter, which\nforces the representation of adjacent nodes to be similar\nduring the information aggregation process. Then it may\ndestroy the discriminative information of anomaly nodes.\nFurthermore, in the domain of fraud detection, heterophily\ngraph datasets exhibit highly imbalanced class distribution\nproperties. This phenomenon results in anomaly nodes be-\ning submerged in normal nodes, which manifests as high ho-\nmophily of normal nodes and high heterophily of anomaly\nnodes. This is also called the common camouflage prob-\nlem (Zhang et al. 2023) in fraud detection scenarios. More-\nover, current semi-supervised fraud detection methods based\non Graph Neural Networks (GNNs) suffer from limitations\nin terms of label utilization. These approaches typically\nlearn node representations through a message passing mech-\nanism, extract node embeddings from the final layer of the\nnetwork, and incorporate label information during the train-\ning process. Evidently, these approaches overlook the in-\ntricate high-order associations between node label informa-\ntion, node characteristics, and their contextual surroundings.\nThe current paramount concern entails addressing the het-\nerophily issue within the context of extreme category im-\nbalance. The advanced methods can be divided into two\ntypes. The first type of the methods is to design a new mes-\nsage aggregation mechanism (Xu et al. 2023) to simultane-\narXiv:2312.06441v3  [cs.LG]  8 Jul 2024\nbenign\nfraud\nHomophily edges\nHeterophily edges\n1\n12\n13\n11\n2\n3\n4\n5\n6\n7\n8\n9\n10\nFigure 1: Schematic diagram of fraud relationship in real\nscenarios.\nously or separately consider the characteristics of homophily\nand heterophily edges. The second type of the methods is\nto identify the heterophily edges on fraud graphs and then\nperform pruning. According to recent researches, scholars\nfound the spectral \u2018right-shift\u2019 phenomenon (Tang et al.\n2022) on both synthetic and real datasets, proving that the\nhigher the anomaly ratio, the more high-frequency compo-\nnents on the graph spectrum. In addition, researchers also\ndemonstrate that, when using the ground truth labels as sig-\nnals, the proportion of heterophily edges is positively corre-\nlated with the spectral energy distribution (Gao et al. 2023a).\nBased on these, we conclude that heterophily is not univer-\nsally harmful, instead it ought to respond distinctively when\nmeeting different filters. In order to verify our conjecture,\nwe conduct sufficient experiments, and the specific related\ndiscussions are placed in experiments.\nIn order to design a GNN-based fraud detector that adapts\nto high heterophily and has remarkable label utilization,\nwe first propose a conjecture that heterophily edge clip-\nping responds differently to different filters, and then con-\nduct comprehensive experiments to verify the conjecture.\nBased on the results, we design a Spectrum-Enhanced and\nEnvironment-Constrainted Graph Fraud Detector (SEC-\nGFD) to comprehensively integrate rich spectrum and label\ninformation into a fraud detector. The framework of the pro-\nposed method is shown in Figure 2, consisting of two differ-\nent modules that address the aforementioned two challenges\nrespectively. The hybrid-pass filter module employs a so-\nphisticated message passing technique utilizing the decom-\nposed spectrum to acquire comprehensive high-frequency\nrepresentations. The local environmental constraint module\nenhances the higher-order connections between target nodes\nand their surrounding environments, thereby mitigating the\nissue of limited label utilization. The main contributions of\nthis method are summarized as follows:\n\u2022 To address the challenge of heterophily, this paper em-\nploys spectrum analysis to partition the graph spectrum\ninto hybrid frequency bands. Then a novel hybrid spec-\ntral filter is designed to conduct message passing on these\nfrequency bands separately and finally aggregate.\n\u2022 With the objective of enhancing label utilization, we in-\ntroduce a local environmental constraint module to assist\nthe training procedure.\n\u2022 We conduct extensive experiments on four real-world\nbenchmark datasets, and the results demonstrate the su-\nperior effectiveness and robustness of our proposed SEC-\nGFD over state-of-the-art methods.\nRelated Work\nSemi-supervised Node Classification.\nSemi-supervised\nnode classification is one of the most important tasks in\ngraph learning. In recent years, GNNs have achieved ex-\ncellent performance in semi-supervised node classification,\nlike GCN (Kipf and Welling 2016), GraphSAGE (Hamil-\nton, Ying, and Leskovec 2017), GAT (Velivckovi\u00b4c et al.\n2017), and GIN (Xu et al. 2018). Compared with tradi-\ntional graph embedding methods such as DeepWalk (Per-\nozzi, Al-Rfou, and Skiena 2014) and Node2vec (Grover and\nLeskovec 2016), which mainly focus on graph structure,\nGNN can consider both node features and graph structure in-\nformation. Each layer of GNN first performs message pass-\ning according to the node neighbors, and then updates the\nnode representation through the aggregated information.\nPopular GNN-based Fraud Detection Model.\nGNN-\nbased fraud detection (GFD) can be defined as an imbal-\nanced binary classification task that identifies outlier nodes\ndeviating significantly from the majority of nodes. This pa-\nper targets node-level fraud detection on static graphs. For\ninstance, GraphConsis (Liu et al. 2020) combines contex-\ntual embeddings with node features and introduces node-\nrelated relationship attention weights. CARE-GNN (Dou\net al. 2020) aims at the camouflage problem in fraud detec-\ntion, and designs GNN enhanced neighbor selection mod-\nule. PC-GNN (Liu et al. 2021b) designs adaptive subgraph\nand neighbor sampling methods to fraud detection scenarios.\nGAGA (Wang et al. 2023) uses group aggregation to gener-\nate discriminative neighborhood information to handle low\nhomophily problems and enhances the original feature space\nwith learnable encoding. This brings some inspiration to our\nwork. GTAN (Xiang et al. 2023) designs gated temporal at-\ntention networks to learn node transaction representations.\nGNN Specially Designed for Heterophily.\nIn fraud de-\ntection domain, nodes with different labels are often con-\nnected, which greatly limits the performance of vanilla\nGNNs, resulting in the emergence of a large number of stud-\nies based on heterophily.\nFor instance, GPRGNN (Chien et al. 2020) combines\nadaptive Generalized PageRank (GPR) schemes with GNNs\nto adapt to heterophily graphs. ACM (Luan et al. 2022)\nstudies heterophily from the perspective of node sim-\nilarity after aggregation, and then defines a new ho-\nmophily index. H2-FDetector (Shi et al. 2022) identifies\nhomophily and heterophily connections through known la-\nbeled nodes, and designs different aggregation strategies for\nthem. BWGNN (Tang et al. 2022) designs a band-pass fil-\nter kernel function to transmit the information of each fre-\nquency band separately, thus ensuring that the spectrum en-\nergy distribution is consistent before and after message pass-\n1-hop\n2-hop\nInput Graph\nfagg\nMLP\n\u2026\nAGG\nfmean\nknn\nMask Target Nodes\nEnvironmental \nSimilarity\nsim( , )\ntarget node\nneighbour nodes\nknn nodes\nLocal Environmental Constraint Module\nHybrid-Pass Filter Module\nConstraint loss\nWeighted cross-entropy loss\nHigh-pass Filter\n(C-1) high-pass\n(a)\n(b)\n.\nBand-pass Filter\n(C+1) band-pass\n.\nFigure 2: The above image presents an overview of our model SEC-GFD, where (a) and (b) respectively demonstrate the details\nof the hybrid-pass filter module and local environmental constraint module.\ning. GHRN (Gao et al. 2023a) judges and cuts heterophily\nedges from the perspective of graph spectrum.\nMethodology\nThis section begins by providing a problem definition and\nintroducing the concepts of graph spectrum and heterophily.\nSubsequently, it presents the framework of the proposed\nmodel, elaborating on each module individually.\nProblem Definition\nWe first introduce the notations of this paper, and then give\nthe formal problem definition. Let G = (V, E, X, Y ) denote\nan attributed fraud graph, where V = {v1, \u00b7 \u00b7 \u00b7 , vN} denotes\nits node set, E is the edge set, and A \u2208{0, 1}N\u00d7N is the\nadjacency matrix of G with Aij = 1 if there is a connection\nbetween node vi and vj. X \u2208RN\u00d7d denotes the matrix of\nnode attributes where xi \u2208Rd is the attribute vector of vi,\nand Y is the corresponding labels of the nodes.\nWe always formulate the graph-based fraud detection\n(GFD) as a semi-supervised node-level binary classification\nproblem. Most of the time, anomalies are regarded as posi-\ntive with label 1, while normal nodes are viewed as negative\nwith label 0 (Jing et al. 2021). The nodes of the whole fraud\ngraph are divided into the labeled ones and the unlabeled\nones, the former are labeled with Ytrain and the labels of\nlatter ones Ytest are invisible during training period. Thus\nGFD aims to learn an anomaly labeling function for our un-\nlabeled nodes with all known information:\nYtest = f(X, A, Ytrain).\n(1)\nGraph Spectrum and Heterophily\nGraph Spectrum.\nAs A is the adjacency matrix of G,\nwe let the graph laplacian matrix L be defined as D \u2212A\nor as I \u2212D\u22121/2AD\u22121/2 (Kipf and Welling 2016), where\nI is an identify matrix. L is a positive semidefinite ma-\ntrix and can be rewritten as U\u039bU T , where \u039b is a diago-\nnal matrix consisted of eigenvalues \u03bb1 \u2264\u03bb2 \u2264... \u2264\u03bbN\nand U is composed of the corresponding N eigenvectors\nU = (u1, u2, ..., uN) (Bo et al. 2021).\nAccording to theory of graph signal processing, we can\ntreat the eigenvectors of normalized Laplacian matrix U\nas the bases in Graph Fourier Transform. Suppose X =\n[x1, x2, ..., xN] are signals on the graph, we treat U T X as\nthe graph fourier transform (Hoang, Maehara, and Murata\n2021) of signal X. Thus, the convolution between the signal\nx and convolution kernel g is as follows:\ng \u2217x = U((U T g) \u2299(U T x)) = UgwU T x,\n(2)\nwhere \u2299is the element-wise product of vectors and gw is\na diagonal matrix representing the convolutional kernel in\nthe spectral domain. Most popular subsequent spectral GNN\nmethods mainly focus on improving the design of gw. And\nfor signals X, the Rayleigh quotient represents signals\u2019 stan-\ndardized variance fraction (Kim et al. 2015):\nR(L, X) = XT LX\nXT X\n=\nP\n(i,j)\u2208E(xi \u2212xj)2\n2 P\ni\u2208\u03bdx2\ni\n.\n(3)\nMoreover, as discussed in BWGNN (Tang et al. 2022),\nthe Rayleigh quotient can also represent the proportion of\nhigh-frequency component energy in the spectrum, which\nmatches the overhead variance fraction. The proof details\ncan be found in Appendix A.\nHeterophily.\nGiven a set of labeled nodes and the connec-\ntions that exist between them, an edge is called a heterophilic\nconnection if its source node and destination node have dif-\nferent labels (i.e., anomaly and normal). For each node, the\nsum of its heterophily and homophily degree is equal to 1.\nThe heterophily of the node and the whole graph can be de-\nfined separately as:\nhetero(v) =\n1\n|N(v)||{u : u \u2208N(v), yu \u0338= yv}|,\nhetero(G) = 1\n|E||{\u03b5 : ysrc \u0338= ydst}|,\n(4)\nwhere |N(v)| is the number of neighbours of node v, and\n|E| is the total number of edges in the whole graph, ysrc and\nydst are the labels of the source and destination node of the\nedge \u03b5 respectively.\nOverview of the Proposed Framework\nFigure 2 illustrates the details of our framework. We pro-\npose a Spectrum-Enhanced and Environment-Constrainted\nGraph Fraud Detector (SEC-GFD) to comprehensively inte-\ngrate rich spectrum and label information into a fraud detec-\ntor. In particular, SEC-GFD first decomposes the spectrum\nof the graph signal. Then it performs complicated message\npassing technique based on these frequency bands respec-\ntively to learn comprehensive representations. Then we de-\nsign a local environmental constraint module to iteratively\nenhance the high-order associations between nodes\u2019 true la-\nbels and their surrounding neighbor environments.\nHybrid-pass Filter Module\nIn the process of designing a hybrid band-pass filter, for pur-\npose of ensuring the division of the spectrum into appropri-\nate frequency bands, we utilize BWGNN (Tang et al. 2022)\nas our backbone. According to this, the beta wavelet trans-\nform kernel function can be written as:\nWp,q = U\u03b2\u2217\np,q(\u039b)U T =\n( L\n2 )p(I \u2212L\n2 )q\n2B(p + 1, q + 1),\n(5)\nin which p + q = C. Then we decompose the spectrum into\n(C + 1) frequency bands in the frequency domain, which is\ndemonstrated below:\nW = (W0,C, W1,C\u22121, \u00b7 \u00b7 \u00b7 , WC,0).\n(6)\nIt is observed that Wp,q is actually a C power polyno-\nmial, and C represents how many hops of neighbors\u2019 infor-\nmation are considered. Since the Laplacian matrix measures\nthe differences of nodes, it can overcome the over-smoothing\nproblem of vanilla GNN to a certain extent and can be ex-\ntended to higher order neighbors. When unpacking Wp,q,\nWC,0 denotes the C power of the Laplacian matrix L. The re-\nmained polynomials are the couplings of high-frequency in-\nformation of neighbors below order C and low-frequency in-\nformation of low-order neighbors. Furthermore, pure high-\nfrequency information of low-order neighbors is lacked in\nthese polynomials, thus we expand the corresponding part\nand design a hybrid-pass filter. When adding the pure high-\nfrequency information from order 1 to (C \u22121), we use\n\u03b5I \u2212D\u22121/2AD\u22121/2 or \u03b5I \u2212\u02dcA (Bo et al. 2021) as the basis,\nwhere \u03b5 is a scaling hyperparameter bounded in [0, 1]. The\nlow-order neighbor high-frequency filtering transformation\nis expressed as follows:\nR1 : F1\nH = \u03b5I \u2212\u02dcA = (\u03b5 \u22121)I + L,\nR2 : F2\nH = (\u03b5I \u2212\u02dcA)2 = ((\u03b5 \u22121)I + L)2,\n...\nRC\u22121 : FC\u22121\nH\n= (\u03b5I \u2212\u02dcA)C\u22121 = ((\u03b5 \u22121)I + L)C\u22121.\n(7)\nThese (C \u22121) frequency bands identify the clean high-\nfrequency information of the low-order neighbors R =\n(R1, R2, \u00b7 \u00b7 \u00b7 , RC\u22121). So far, we obtain 2C frequency bands,\nconsisting of (C+1) band-pass frequency bands and (C\u22121)\nhigh-pass frequency bands for low-order neighbors. Thus we\nobtain the Hybrid spectral combination:\nHybrid = Concat(W, R)\n= (\n(C+1) Band-pass\nz\n}|\n{\nW0,C, W1,C\u22121, \u00b7 \u00b7 \u00b7 , WC,0,\n(C-1) High-pass\nz\n}|\n{\nR1, \u00b7 \u00b7 \u00b7 , RC\u22121).\n(8)\nSo far there are a total of 2C frequency bands. Then mes-\nsage transmission function is carried out on 2C frequency\nbands separately, and finally the learned representations of\neach frequency band are aggregated:\nH0 = MLP(X),\nBi = Wi,C\u2212iH0\nHj = RjH0,\nH = fagg(B0, \u00b7 \u00b7 \u00b7 , BC, H1, \u00b7 \u00b7 \u00b7 , HC\u22121),\n(9)\nwhere MLP (Taud and Mas 2018) denotes a simple multi-\nlayer perceptron and fagg can be any aggregation func-\ntion (Wu et al. 2023) such as summation or concatenation.\nH is the aggregated representation of hybrid-pass filter, then\nweighted cross-entropy loss (Zhong, Wang, and Miao 2019)\nis used for the training of hybrid-pass filter module:\nLhybrid =\nX\nv\u2208V\n[\u03b4yv log pv + (1 \u2212yv) log (1 \u2212pv)] , (10)\nwhere \u03b4 is the ratio of anomaly labels (yv = 1) to normal\nlabels (yv = 0) in the training set.\nLocal Environmental Constraint Module\nIn order to improve the label utilization, and to fully con-\nsider the structure and context information of local envi-\nronments, we add a local environmental constraint module.\nSpecifically, we first mask the characteristics of the target\nnode to prevent the influence of the characteristics of the\nnode itself. Afterwards vanilla GNN is used to jointly learn\nthe feature information of the target node\u2019s multi-hop neigh-\nbors and then aggregate them together. Obviously, since the\nfeatures of the target nodes are masked, thus let h(0)\nt\n= 0.\nThe aggregation function is as follows:\nh(l+1)\nt\n= UPDATE\n\u0010\nh(l)\nt , AGG\n\u0010\n{h(l)\nv\n: v \u2208Nt}\n\u0011\n,\n(11)\nHneigh\nt\n= AGG(h1\nt, h2\nt, ..., hL\nt ),\n(12)\nwhere Hneigh\nt\nis the final neighbor aggregation represen-\ntation. For purpose of exploring the essential association\nbetween neighbors with different hop counts, a simple\nSGC (Wu et al. 2019) is used without adding any non-\nlinear functions. Moreover, in order to alleviate the over-\nsmoothing effect (Chen et al. 2020), L is generally selected\nas 2 or 3 in the experiments.\nThen we progress graph augmentation and construct KNN\nview of fraud graph according to the features. Concretely,\nthe k-nearest neighbors (Dong, Moses, and Li 2011) of each\nnode are found according to the feature cosine similarity.\nTo ensure the rationality of local environmental features, we\ndo not add any nonlinear functions, but employ the average\npooling operation.\nHknn\nt\n= fmean({xu|\u2200u \u2208Kt}) =\n1\n|Kt|\nX\nu\u2208Kt\nxu.\n(13)\nSo far, we have obtained the multi-hop neighbor informa-\ntion of the target nodes and their k-nearest neighbor envi-\nronmental feature information. In order to realize the local\nenvironmental constraints, this paper judges based on com-\nmon senses that, the multi-hop neighbor features learned by\nnormal nodes and the local environmental features should\nbe as similar as possible, while the two features learned by\nanomaly nodes should have a large deviation. Following the\nformula of InfoNCE loss (You et al. 2020) in Contrastive\nLearning, we design a constraint loss function Lenv spe-\ncially adapted to the above judgments:\nLenv = \u2212log\n1\n|Vn|\nP\nvi\u2208Vn e\nsim(Hneigh\nvi\n,Hknn\nvi\n)\nvi\n1\n|Va|\nP\nvj\u2208Va e\nsim(Hneigh\nvj\n,Hknn\nvj\n)\nvj\n,\n(14)\nwhere Vn and Va represent the set of normal nodes and\nanomaly nodes in the training set respectively, and sim(\u00b7, \u00b7)\nis the simple cosine similarity operator.\nFraud Detection\nFor the sake of jointly learning hybrid frequency-band rep-\nresentations and applying local environmental constraints,\nwe carefully design the objective function of SEC-GFD.\nSpecifically, we construct a weighted binary cross-entropy\nloss function and an InfoNCE-like contrastive loss to real-\nize the constraints of the surrounding environments on the\ntarget nodes. The final loss function is as follows:\nL = \u03b1Lhybrid + (1 \u2212\u03b1)Lenv.\n(15)\nIn order to better combine the two loss functions, a hyperpa-\nrameter \u03b1 \u2208[0, 1] is added to balance their influences.\nDataset\nNodes\nEdges\nAnomaly(%) Features\nAmazon\n11,944\n4,398,392\n6.87\n25\nYelpChi\n45,954\n3,846,979\n14.53\n32\nT-Finance\n39,357\n21,222,543\n4.58\n10\nT-Social\n5,781,065\n73,105,508\n3.01\n10\nTable 1: Statistics of four datasets.\nExperiments\nIn this section, we evaluate the performance of our proposed\napproach on real-world datasets through a series of experi-\nments, and compare the results with those of state-of-the-art\nbaselines to demonstrate the effectiveness of our approach.\nIn addition, we conduct experiments on the response of het-\nerophily edges to distinct filters, as well as experiments on\nablation and hyperparameter analysis.\nExperimental Settings\nDataset. We conduct experiments on four real-world\ndatasets\ntargeted\nat\nfraud\ndetection\nscenarios.\nOver-\nall, they are Amazon (McAuley and Leskovec 2013),\nYelpChi (Rayana and Akoglu 2015), and two recently\npublished transaction datasets, i.e., T-Finance and T-\nSocial (Tang et al. 2022). The YelpChi dataset consists of\nfiltered and recommended hotel and restaurant reviews from\nYelp. The Amazon dataset includes product reviews in the\nmusic instrument category. The T-Finance dataset and T-\nSocial dataset detect anomalous accounts in transaction net-\nworks and social networks respectively, and they share the\nsame 10-dimensional features. The T-Social dataset is a\nlarge-scale dataset, several tens to hundreds of times larger\nin scale compared to the other datasets. The statistics of\nthese datasets are summarized in Table 1.\nBaseLines for Comparison. We conduct comparative ex-\nperiments between SEC-GFD and advanced baselines that\nbelong to three groups. The first group consists of methods\nbased on homophily GNN models, including GCN (Kipf and\nWelling 2016), GraphSAGE (Hamilton, Ying, and Leskovec\n2017), GAT (Velivckovi\u00b4c et al. 2017) and GIN (Xu et al.\n2018). The second group comprises state-of-the-art methods\nfor graph-based fraud detection, namely GraphConsis (Liu\net al. 2020), CARE-GNN (Dou et al. 2020), PC-GNN (Liu\net al. 2021b) and GAGA (Wang et al. 2023). The third group\ncontains GNN-based fraud detection models specifically de-\nsigned for heterophily, including ACM (Luan et al. 2022),\nH2-FDetector (Shi et al. 2022), BWGNN (Tang et al. 2022)\nand GDN (Gao et al. 2023b). More details on description\nand implementation of the benchmarks are provided in Ap-\npendix B.\nMetrics. As graph anomaly detection poses a class-\nimbalanced classification problem, this paper utilizes two\nwidely adopted metrics: F1-macro and AUC. F1-macro con-\nsiders the weighted average of F1 scores across multiple\nclasses, and AUC is the area under the ROC Curve.\nSettings. In our proposed method SEC-GFD, for the Ama-\nzon, YelpChi, and T-Finance datasets, the hidden layer di-\nmension is set to 64, and the high-frequency signal neighbor\norder C is set to 2. For the T-Social dataset, the hidden layer\nMethods\nDatasets\nYelpChi\nAmazon\nT-Finance\nT-Social\nF1-macro\nAUC\nF1-macro\nAUC\nF1-macro\nAUC\nF1-macro\nAUC\nHomophily GNN\nGCN\n53.21\n56.80\n63.52\n80.14\n71.47\n66.31\n61.73\n87.84\nGraphSAGE\n64.34\n73.61\n75.25\n86.73\n55.27\n69.52\n60.49\n72.65\nGAT\n55.86\n58.92\n82.41\n89.73\n54.27\n75.29\n73.72\n88.61\nGIN\n63.42\n75.28\n71.14\n81.38\n65.72\n81.63\n63.38\n81.33\nGFD Algorithm\nGraphConsis\n57.91\n69.55\n78.46\n87.27\n73.58\n91.42\n58.32\n73.89\nCARE-GNN\n63.58\n79.42\n83.31\n91.57\n65.37\n91.93\n55.92\n68.78\nPC-GNN\n64.75\n77.14\n91.73\n95.63\n77.25\n92.83\n57.21\n71.27\nGAGA\n76.71\n89.77\n90.31\n95.61\n85.13\n94.72\n76.64\n87.93\nHeterophily GNN\nACM\n69.72\n88.28\n81.83\n93.69\n85.48\n96.02\n81.27\n90.79\nH2-FDetector\n69.38\n88.63\n83.84\n96.41\n87.39\n95.41\n78.89\n88.56\nBWGNN\n76.66\n90.56\n91.68\n97.52\n84.59\n95.29\n84.58\n95.27\nGDN\n76.05\n90.79\n90.68\n97.09\n86.12\n94.28\n80.63\n89.35\nours\nSEC-GFD\n77.73\n91.39\n92.35\n98.23\n89.86\n96.32\n87.74\n96.11\nTable 2: Fraud detection performance on four real-world datasets in terms of F1-macro and AUC value. (The bold and italicized\nvalues in the table represent the best and second-best performance, respectively.)\ndimension is set to 16, the order C of high-frequency signal\nneighbors is set to 5. The experimental results show that,\nfor such a large-scale graph, fusing high-frequency infor-\nmation of higher-order neighbors can learn better features.\nIn the experiments, to ensure fairness, the size of training/-\nvalidation/testing set of the datasets is set to 0.4/0.2/0.4 for\nall the compared methods. Our method is implemented with\nthe DGL library in Pytorch, and for other methods, public\nsource codes are utilized for implementation. Furthermore,\nwe run for 100 epochs on the four datasets for all methods.\nExperimental Results\nWe conduct a comprehensive comparison of our approach\nwith vanilla homophily GNN models, state-of-the-art graph-\nbased fraud detection models and novel GNNs targeted at\nheterophily. The corresponding results are shown in Table 2.\nFirst of all, It is observed that SEC-GNN outperforms all\nvanilla homophily GNN models, i.e., GCN, GraphSAGE,\nGAT and GIN. For instance, when compared with GIN,\nSEC-GFD achieves improvements of 13.37% and 26.41% in\nterms of F1-macro and AUC on Amazon. To delve reasons,\nthese models can be regarded as low-pass filters in essence\nand only focus on low frequency information. While our\nSEC-GNN integrates rich information of distinct frequency\nbands adapting to heterophily.\nWhen compared with state-of-the-art graph-based fraud\ndetection methods, these methods still have a large gap with\nour approach. Among them, GraphConsis demonstrates the\nworst comprehensive performance. It combines the char-\nacteristics of nodes with the surrounding environments to\nlearn relationship attention weights, and it is essentially a\nlow-pass filter. CARE-GNN and PC-GNN achieve some im-\nprovements in the effectiveness. This is because they are\naware of heterophily edges and camouflage problems, and\ndesign corresponding neighbor selection module. GAGA\nintroduces a group aggregation module to generate distin-\nguishable multi-hop neighborhood information. Although it\ndemonstrates substantial overall improvements, it still ex-\nhibits a significant disparity when compared to our proposed\nmethod. Specically, when compared with GAGA, SEC-GFD\nachieves gains of 2.04% and 2.62% in terms of F1-macro\nand AUC on YelpChi. The reason behind this can be at-\ntributed to that our method additionally learns complicated\nhigh-frequency information in addition to learning multi-\nhop neighbor information.\nFinally, when compared with novel heterophily GNNs,\nexperimental results in Table 2 show that our proposed\nmethod consistently outperforms them on all metrics across\nthe four datasets. To be specific, SEC-GFD attains im-\nprovements of 2.47% and 1.71% in terms of F1-macro and\nAUC on T-Finance. As to T-Social, SEC-GFD achieves\ngains of 3.78% and 0.84% in terms of F1-macro and AUC.\nAmong these models, ACM studies heterophily in terms of\nfeature similarity, H2-FDetector identifies homophily and\nheterophily connections and applies different aggregation\nstrategies, BWGNN designs a band-pass filter to transmit in-\nformation in multiple frequency bands, and GDN divides the\nnode features into anomaly pattern and neighborhood pat-\ntern. The superior performance of SEC-GFD indicates the\neffectiveness of the proposed hybrid filtering module and lo-\ncal environmental constraint module.\nHeterophily Edges Clipping Analysis\nTo explore the effect of heterophily edge pruning on GNNs\nwith different filter properties, we conduct extensive exper-\niments. In this paper, three message passing mechanisms\nwith low-pass, high-pass, and band-pass filtering charac-\nteristics are designed for experiments. The low-pass and\nhigh-pass filters use the adjacency matrix and normalized\nLaplacian matrix respectively for message passing, and the\nband-pass filter uses the beta wavelet filter function from\nBWGNN, with two layers of neighbor information are con-\nsidered. The experimental results on Amazon and Yelpchi\nare illustrated in Figure 3.\nConsidering the difficulty in discriminating heterophily\nedges, in many cases only some heterophily edges con-\n(a)\n(b)\n(c)\n(d)\nFigure 3: Filters\u2019 performance with different heterophily\nedges\u2019 deletion ratio on Amazon and YelpChi. Fig (a) and\nFig (c) denote deletion of heterophily edges on the entire\ngraph, while Fig (b) and Fig (d) denote deletion of het-\nerophily edges appearing in training graph.\nnected with labeled nodes on the training set can be identi-\nfied. Thus two sets of experiments are conducted in this pa-\nper. As observed in Figure 3, when clipping the heterophily\nedges of the whole graph, the performance of low-pass and\nband-pass filtering improves with the ratio of heterophily\nedge clipping. While the performance of high-pass filter-\ning shows a monotonous decrease. And the performance of\nthe band-pass filter keeps the best. Moreover, when clipping\nheterophily edges appearing in the training set, results are\ncompletely different. Specifically, low-pass filter maintains\na poor performance, while band-pass and high-pass filter ex-\nhibit different degrees of declining trends. This suggests that\nindiscriminate removal of heterophily edges is not necessar-\nily beneficial.\nEffectiveness of Each Component of SEC-GFD\nTo explore how each component contributes to the perfor-\nmance of SEC-GFD, we systematically remove certain parts\nof the framework for ablation study. The results are illus-\ntrated in Table 3.\nFirstly, in order to explore the performance of the two\nparts of the hybrid filter, we omit band-pass bands and low-\norder pure high-pass bands respectively. When band-pass\nbands are excluded, the performance drops by 2.06% and\n2.67% in terms of AUC and F1-macro. And the performance\ndecreases 1.13% and 1.27% in terms of AUC and F1-macro\nwhen low-order high-pass bands are removed. It is obvi-\nously that band-pass bands are more important, this is be-\ncause band-pass bands consider high frequency and lower\nfrequency information together. Afterwards, we delete the\nlocal environmental constraint, then performance drops by\n0.86% and 1.31% in terms of AUC and F1-macro. However,\nit still outperforms other state-of-the-art methods, this also\nreflects the effectiveness of our hybrid filter. Thus the hybrid\nfilter can be equipped as a general backbone to various other\nAmazon\nT-Finance\nF1-macro\nAUC\nF1-macro\nAUC\nSEC-GFD\n92.27\n98.19\n89.74\n96.34\nw/o high-pass\n91.48\n97.28\n88.65\n95.05\nw/o band-pass\n90.29\n96.31\n86.79\n93.77\nw/o env\n91.88\n97.14\n88.14\n95.21\nTable 3: Ablation Performance.(The bold and italicized val-\nues denote the best and runner-up performance.)\nFigure 4: Performance with different order C.\nGNN-based fraud detectors to improve performance.\nSelection of C for high-order neighbors\nIn the previous discussion, based on the formula of beta\nwavelet, we judge that C represents the order of neighbor in-\nformation considered in the network training process. There-\nfore, the order C is an extremely important parameter. Fig-\nure 4 shows the changes of F1 and AUC values with the\nchanges of C from 1 to 6 on four real-world datasets.\nIt is observed that on the three smaller datasets, the im-\nprovement of the effect is no longer obvious when C exceeds\ntwo. While on large-scale dataset T-Social, which is nearly\na hundred times larger than other datasets, there is still a\nsignificant performance improvement until C reaches five.\nA possible reason is that, larger fraud graph may contain\nmore complicated inter-node interactions and more common\nspread of fraudulent behavior across multi-hop neighbors.\nConclusion\nThis paper reviews the research on highly heterophily\ngraphs, especially in fraud detection scenarios, and fur-\nther explores the association between heterophily edges and\nspectral domain energy distribution. The experimental re-\nsults prove that it is not necessary to discriminate and cut\nheterophily edges, but to make full use of each frequency\nband for information transmission to achieve excellent re-\nsults. To this end, we have designed a spectrum-enhanced\nand environment-constrainted graph-based fraud detector\nSEC-GFD. The main body is a hybrid filtering module,\nwhich contains multiple frequency bands with multi-order\nneighbors. Then it is supplemented with a local environmen-\ntal constraint module to improve the utilization of node la-\nbels and the adaptive relationships between nodes and their\nsurrounding environments.\nAcknowledgments\nThis work is supported in part by the Fundamental Research\nFunds for the Central Universities under Grant 2022RC026;\nin part by the CCF-NSFOCUS Open Fund; in part by the\nNSFC Program under Grant 62202042, Grant U20A6003,\nGrant 62076146, Grant 62021002, Grant U19A2062, Grant\n62127803, Grant U1911401 and Grant 6212780016; in part\nby Industrial Technology Infrastructure Public Service Plat-\nform Project \u2018Public Service Platform for Urban Rail Tran-\nsit Equipment Signal System Testing and Safety Evaluation\u2019\n(No. 2022-233-225), Ministry of Industry and Information\nTechnology of China.\nReferences\nAkoglu, L.; Tong, H.; and Koutra, D. 2015. Graph based\nanomaly detection and description: a survey. Data mining\nand knowledge discovery, 29: 626\u2013688.\nBo, D.; Wang, X.; Shi, C.; and Shen, H. 2021.\nBeyond\nlow-frequency information in graph convolutional networks.\nIn Proceedings of the AAAI Conference on Artificial Intelli-\ngence, volume 35, 3950\u20133957.\nChen, D.; Lin, Y.; Li, W.; Li, P.; Zhou, J.; and Sun, X. 2020.\nMeasuring and relieving the over-smoothing problem for\ngraph neural networks from the topological view. In Pro-\nceedings of the AAAI conference on artificial intelligence,\nvolume 34, 3438\u20133445.\nChien, E.; Peng, J.; Li, P.; and Milenkovic, O. 2020. Adap-\ntive universal generalized pagerank graph neural network.\narXiv preprint arXiv:2006.07988.\nDong, W.; Moses, C.; and Li, K. 2011. Efficient k-nearest\nneighbor graph construction for generic similarity mea-\nsures. In Proceedings of the 20th international conference\non World wide web, 577\u2013586.\nDou, Y.; Liu, Z.; Sun, L.; Deng, Y.; Peng, H.; and Yu, P. S.\n2020.\nEnhancing graph neural network-based fraud de-\ntectors against camouflaged fraudsters. In Proceedings of\nthe 29th ACM international conference on information &\nknowledge management, 315\u2013324.\nGao, Y.; Wang, X.; He, X.; Liu, Z.; Feng, H.; and Zhang, Y.\n2023a. Addressing heterophily in graph anomaly detection:\nA perspective of graph spectrum. In Proceedings of the ACM\nWeb Conference 2023, 1528\u20131538.\nGao, Y.; Wang, X.; He, X.; Liu, Z.; Feng, H.; and Zhang,\nY. 2023b. Alleviating structural distribution shift in graph\nanomaly detection. In Proceedings of the Sixteenth ACM\nInternational Conference on Web Search and Data Mining,\n357\u2013365.\nGrover, A.; and Leskovec, J. 2016. node2vec: Scalable fea-\nture learning for networks. In Proceedings of the 22nd ACM\nSIGKDD international conference on Knowledge discovery\nand data mining, 855\u2013864.\nHamilton, W.; Ying, Z.; and Leskovec, J. 2017. Inductive\nrepresentation learning on large graphs. Advances in neural\ninformation processing systems, 30.\nHoang, N.; Maehara, T.; and Murata, T. 2021.\nRevisit-\ning graph neural networks: Graph filtering perspective. In\n2020 25th International Conference on Pattern Recognition\n(ICPR), 8376\u20138383. IEEE.\nHooi, B.; Shin, K.; Song, H. A.; Beutel, A.; Shah, N.; and\nFaloutsos, C. 2017. Graph-based fraud detection in the face\nof camouflage. TKDD, 11(4): 1\u201326.\nJin, M.; Liu, Y.; Zheng, Y.; Chi, L.; Li, Y.-F.; and Pan, S.\n2021. Anemone: Graph anomaly detection with multi-scale\ncontrastive learning. In Proceedings of the 30th ACM Inter-\nnational Conference on Information & Knowledge Manage-\nment, 3122\u20133126.\nJing, R.; Tian, H.; Zhou, G.; Zhang, X.; Zheng, X.; and\nZeng, D. D. 2021. A GNN-based Few-shot learning model\non the Credit Card Fraud detection. In 2021 IEEE 1st In-\nternational Conference on Digital Twins and Parallel Intel-\nligence (DTPI), 320\u2013323. IEEE.\nKim, T.; Wang, Y.; Sahinoglu, Z.; Wada, T.; Hara, S.;\nand Qiao, W. 2015. A Rayleigh quotient-based recursive\ntotal-least-squares online maximum capacity estimation for\nlithium-ion batteries. IEEE Transactions on Energy Conver-\nsion, 30(3): 842\u2013851.\nKipf, T. N.; and Welling, M. 2016. Semi-supervised classi-\nfication with graph convolutional networks. arXiv preprint\narXiv:1609.02907.\nLiu, Y.; Ao, X.; Qin, Z.; Chi, J.; Feng, J.; Yang, H.; and\nHe, Q. 2021a. Pick and choose: a GNN-based imbalanced\nlearning approach for fraud detection. In WWW, 3168\u20133177.\nLiu, Y.; Ao, X.; Qin, Z.; Chi, J.; Feng, J.; Yang, H.; and\nHe, Q. 2021b. Pick and choose: a GNN-based imbalanced\nlearning approach for fraud detection. In Proceedings of the\nweb conference 2021, 3168\u20133177.\nLiu, Z.; Dou, Y.; Yu, P. S.; Deng, Y.; and Peng, H. 2020. Al-\nleviating the inconsistency problem of applying graph neu-\nral network to fraud detection. In Proceedings of the 43rd\ninternational ACM SIGIR conference on research and devel-\nopment in information retrieval, 1569\u20131572.\nLuan, S.; Hua, C.; Lu, Q.; Zhu, J.; Zhao, M.; Zhang, S.;\nChang, X.-W.; and Precup, D. 2022. Revisiting heterophily\nfor graph neural networks. Advances in neural information\nprocessing systems, 35: 1362\u20131375.\nMcAuley, J. J.; and Leskovec, J. 2013.\nFrom amateurs\nto connoisseurs: modeling the evolution of user expertise\nthrough online reviews. In Proceedings of the 22nd inter-\nnational conference on World Wide Web, 897\u2013908.\nPerozzi, B.; Al-Rfou, R.; and Skiena, S. 2014. Deepwalk:\nOnline learning of social representations. In Proceedings of\nthe 20th ACM SIGKDD international conference on Knowl-\nedge discovery and data mining, 701\u2013710.\nPourhabibi, T.; Ong, K.-L.; Kam, B. H.; and Boo, Y. L. 2020.\nFraud detection: A systematic literature review of graph-\nbased anomaly detection approaches. Decision Support Sys-\ntems, 133: 113303.\nQin, Z.; Liu, Y.; He, Q.; and Ao, X. 2022.\nExplain-\nable Graph-based Fraud Detection via Neural Meta-graph\nSearch. In Proceedings of the 31st ACM International Con-\nference on Information & Knowledge Management, 4414\u2013\n4418.\nRayana, S.; and Akoglu, L. 2015. Collective opinion spam\ndetection: Bridging review networks and metadata. In Pro-\nceedings of the 21th acm sigkdd international conference on\nknowledge discovery and data mining, 985\u2013994.\nShi, F.; Cao, Y.; Shang, Y.; Zhou, Y.; Zhou, C.; and Wu, J.\n2022. H2-fdetector: A gnn-based fraud detector with ho-\nmophilic and heterophilic connections. In Proceedings of\nthe ACM Web Conference 2022, 1486\u20131494.\nTang, J.; Li, J.; Gao, Z.; and Li, J. 2022. Rethinking graph\nneural networks for anomaly detection.\nIn International\nConference on Machine Learning, 21076\u201321089. PMLR.\nTaud, H.; and Mas, J. 2018. Multilayer perceptron (MLP).\nGeomatic approaches for modeling land change scenarios,\n451\u2013455.\nVelivckovi\u00b4c, P.; Cucurull, G.; Casanova, A.; Romero, A.;\nLio, P.; and Bengio, Y. 2017.\nGraph attention networks.\narXiv preprint arXiv:1710.10903.\nWang, H.; Zhou, C.; Wu, J.; Dang, W.; Zhu, X.; and Wang, J.\n2018. Deep structure learning for fraud detection. In ICDM,\n567\u2013576.\nWang, Y.; Zhang, J.; Huang, Z.; Li, W.; Feng, S.; Ma, Z.;\nSun, Y.; Yu, D.; Dong, F.; Jin, J.; et al. 2023. Label Informa-\ntion Enhanced Fraud Detection against Low Homophily in\nGraphs. In Proceedings of the ACM Web Conference 2023,\n406\u2013416.\nWu, F.; Souza, A.; Zhang, T.; Fifty, C.; Yu, T.; and Wein-\nberger, K. 2019. Simplifying graph convolutional networks.\nIn International conference on machine learning, 6861\u2013\n6871. PMLR.\nWu, H.; Xion, W.; Xu, F.; Luo, X.; Chen, C.; Hua, X.-S.;\nand Wang, H. 2023. PastNet: Introducing Physical Inductive\nBiases for Spatio-temporal Video Prediction. arXiv preprint\narXiv:2305.11421.\nXiang, S.; Zhu, M.; Cheng, D.; Li, E.; Zhao, R.; Ouyang,\nY.; Chen, L.; and Zheng, Y. 2023. Semi-supervised credit\ncard fraud detection via attribute-driven graph representa-\ntion. In Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 37, 14557\u201314565.\nXu, F.; Wang, N.; Wen, X.; Gao, M.; Guo, C.; and\nZhao, X. 2023.\nFew-shot Message-Enhanced Contrastive\nLearning for Graph Anomaly Detection.\narXiv preprint\narXiv:2311.10370.\nXu, K.; Hu, W.; Leskovec, J.; and Jegelka, S. 2018.\nHow powerful are graph neural networks? arXiv preprint\narXiv:1810.00826.\nYou, Y.; Chen, T.; Sui, Y.; Chen, T.; Wang, Z.; and Shen, Y.\n2020. Graph contrastive learning with augmentations. Ad-\nvances in neural information processing systems, 33: 5812\u2013\n5823.\nZhang, G.; Wu, J.; Yang, J.; Beheshti, A.; Xue, S.; Zhou,\nC.; and Sheng, Q. Z. 2021. Fraudre: Fraud detection dual-\nresistant to graph inconsistency and imbalance. In ICDM,\n867\u2013876.\nZhang, Z.; Wan, J.; Zhou, M.; Lai, Z.; Tessone, C. J.; Chen,\nG.; and Liao, H. 2023. Temporal burstiness and collabora-\ntive camouflage aware fraud detection. Information Process-\ning & Management, 60(2): 103170.\nZhong, P.; Wang, D.; and Miao, C. 2019. An affect-rich neu-\nral conversational model with biased attention and weighted\ncross-entropy loss. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 33, 7492\u20137500.\nZhu, J.; Rossi, R. A.; Rao, A.; Mai, T.; Lipka, N.; Ahmed,\nN. K.; and Koutra, D. 2021. Graph neural networks with het-\nerophily. In Proceedings of the AAAI conference on artificial\nintelligence, volume 35, 11168\u201311176.\nA. Bond between Rayleigh Quotient and\nSpectral Energy\nFor the Rayleigh quotient, we initially examine its numera-\ntor, which corresponds to the dot product of the signal with\nitself following the Laplacian matrix transformation. Subse-\nquently, we proceed to expand the formula and obtain:\nXT LX = XT (D \u2212A)X = XT DX \u2212XT AX\n=\nN\nX\ni=1\nx2\ni di \u2212\nN\nX\ni=1\nN\nX\nj=1\nAijxixj\n= 1\n2(\nN\nX\ni=1\nx2\ni di \u22122\nN\nX\ni=1\nN\nX\nj=1\nAijxixj +\nN\nX\nj=1\nx2\njdj)\n= 1\n2\nN\nX\ni=1\nN\nX\nj=1\nAij(xi \u2212xj)2\nUpon observation, we note that the numerator is similar to a\nweighted sum of Euclidean distances, which can also serve\nas a metric for signal dissimilarity. Subsequently, we substi-\ntute this into the formula of the Rayleigh quotient:\nR(L, X) = XT LX\nXT X\n=\nPN\ni=1\nPN\nj=1 Aij(xi \u2212xj)2\n2 PN\ni=1 x2\ni\n=\nP\n(i,j)\u2208E(xi \u2212xj)2\n2 P\ni\u2208\u03bd x2\ni\nThus Rayleigh quotient is actually the standardized variance\nfraction of signal X. To establish a correlation with spectral\nenergy, the analysis is conducted in the spectral domain. Ac-\ncording to spectral graph theory, XT LX = PN\ni=1 \u03bbi\u02dcx2\ni and\nXT X = PN\ni=1 x2\ni have been empirically validated, thus the\nRayleigh quotient can also be expanded as follows:\nR(L, X) = XT LX\nXT X\n=\nPN\ni=1 \u03bbi\u02dcx2\ni\nPN\ni=1 x2\ni\n=\nN\nX\ni=1\n\u03bbi(\n\u02dcx2\ni\nPN\nk=1 \u02dcx2\nk\n)\n=\nN\nX\ni=1\n\u03bbi(\u03b7i \u2212\u03b7i\u22121)\n= \u03bbN \u2212[(\u03bb1 \u2212\u03bb0)\u03b70 + \u00b7 \u00b7 \u00b7 + (\u03bbN \u2212\u03bbN\u22121)\u03b7N\u22121]\n= \u03bbN \u2212\n\"Z \u03bb1\n0\nf(t)dt + \u00b7 \u00b7 \u00b7 +\nZ \u03bbN\n\u03bbN\u22121\nf(t)dt\n#\n= \u03bbN \u2217f(t)max \u2212Sspec\nwhere Sspec is the area under spectrum energy accumula-\ntion curve and \u03bbN represents the total length of the continu-\nous spectrum. Consequently, the Rayleigh quotient exhibits\na higher value when the proportion of high-frequency com-\nponents is greater.\nB. Baselines and Implementation Details\nBaselines can be classified into three distinct categories. The\nfirst category contains GNNs based on homophily assump-\ntion:\n\u2022 GCN (Kipf and Welling 2016): GCN is a traditional\ngraph neural network model that aggregates neighbor in-\nformation.\n\u2022 GraphSAGE (Hamilton, Ying, and Leskovec 2017):\nGraphSAGE is a graph neural network model that sam-\nples and aggregates information from node neighbor-\nhoods to learn node representations.\n\u2022 GAT (Velickovic et al. 2017): GAT is a graph neural net-\nwork model that applies attention mechanisms to process\ngraph data.\n\u2022 GIN (Xu et al. 2018): GIN is a graph neural net-\nwork model that aggregates node information using\npermutation-invariant operations.\nThe second category are state-of-the-art graph-based\nfraud detection methods:\n\u2022 GraphConsis (Liu et al. 2020): GraphConsis combines\nthe characteristics of nodes with the surrounding envi-\nronment to learn a relationship attention weight.\n\u2022 CARE-GNN (Dou et al. 2020): CARE-GNN achieves\nthe effect of modifying the adjacency matrix and pruning\nthe edges by designing the neighbor selection module.\n\u2022 PC-GNN (Liu et al. 2021): PC-GNN introduces a node\nsampler and a label-aware neighbor selector to further\nreweight imbalanced classes.\n\u2022 GAGA (Wang et al. 2023): GAGA introduces a group\naggregation module to generate distinguishable multi-\nhop neighborhood information.\nThe third category consists of GNN methods specifically\ndesigned for heterophily:\n\u2022 ACM (Luan et al. 2022): ACM uses aggregated features\nto study heterophily in terms of feature similarity.\n\u2022 H2-FDetector (Shi et al. 2022): H2-FDetector identifies\nhomophily and heterophily connections and applies dif-\nferent aggregation strategies.\n\u2022 BWGNN (Tang et al. 2022): BWGNN designs a band-\npass filter to transmit information in multiple frequency\nbands.\n\u2022 GDN (Gao et al. 2023): GDN divides the node features\ninto two parts that describe the abnormal pattern and\nadapt to the neighborhood information.\n",
    "1710.09412": "Published as a conference paper at ICLR 2018\nmixup: BEYOND EMPIRICAL RISK MINIMIZATION\nHongyi Zhang\nMIT\nMoustapha Cisse, Yann N. Dauphin, David Lopez-Paz\u2217\nFAIR\nABSTRACT\nLarge deep neural networks are powerful, but exhibit undesirable behaviors such\nas memorization and sensitivity to adversarial examples. In this work, we propose\nmixup, a simple learning principle to alleviate these issues. In essence, mixup trains\na neural network on convex combinations of pairs of examples and their labels.\nBy doing so, mixup regularizes the neural network to favor simple linear behavior\nin-between training examples. Our experiments on the ImageNet-2012, CIFAR-10,\nCIFAR-100, Google commands and UCI datasets show that mixup improves the\ngeneralization of state-of-the-art neural network architectures. We also \ufb01nd that\nmixup reduces the memorization of corrupt labels, increases the robustness to\nadversarial examples, and stabilizes the training of generative adversarial networks.\n1\nINTRODUCTION\nLarge deep neural networks have enabled breakthroughs in \ufb01elds such as computer vision (Krizhevsky\net al., 2012), speech recognition (Hinton et al., 2012), and reinforcement learning (Silver et al., 2016).\nIn most successful applications, these neural networks share two commonalities. First, they are\ntrained as to minimize their average error over the training data, a learning rule also known as the\nEmpirical Risk Minimization (ERM) principle (Vapnik, 1998). Second, the size of these state-of-the-\nart neural networks scales linearly with the number of training examples. For instance, the network of\nSpringenberg et al. (2015) used 106 parameters to model the 5 \u00b7 104 images in the CIFAR-10 dataset,\nthe network of (Simonyan & Zisserman, 2015) used 108 parameters to model the 106 images in the\nImageNet-2012 dataset, and the network of Chelba et al. (2013) used 2 \u00b7 1010 parameters to model\nthe 109 words in the One Billion Word dataset.\nStrikingly, a classical result in learning theory (Vapnik & Chervonenkis, 1971) tells us that the\nconvergence of ERM is guaranteed as long as the size of the learning machine (e.g., the neural\nnetwork) does not increase with the number of training data. Here, the size of a learning machine is\nmeasured in terms of its number of parameters or, relatedly, its VC-complexity (Harvey et al., 2017).\nThis contradiction challenges the suitability of ERM to train our current neural network models, as\nhighlighted in recent research. On the one hand, ERM allows large neural networks to memorize\n(instead of generalize from) the training data even in the presence of strong regularization, or in\nclassi\ufb01cation problems where the labels are assigned at random (Zhang et al., 2017). On the other\nhand, neural networks trained with ERM change their predictions drastically when evaluated on\nexamples just outside the training distribution (Szegedy et al., 2014), also known as adversarial\nexamples. This evidence suggests that ERM is unable to explain or provide generalization on testing\ndistributions that differ only slightly from the training data. However, what is the alternative to ERM?\nThe method of choice to train on similar but different examples to the training data is known as data\naugmentation (Simard et al., 1998), formalized by the Vicinal Risk Minimization (VRM) principle\n(Chapelle et al., 2000). In VRM, human knowledge is required to describe a vicinity or neighborhood\naround each example in the training data. Then, additional virtual examples can be drawn from the\nvicinity distribution of the training examples to enlarge the support of the training distribution. For\ninstance, when performing image classi\ufb01cation, it is common to de\ufb01ne the vicinity of one image\nas the set of its horizontal re\ufb02ections, slight rotations, and mild scalings. While data augmentation\nconsistently leads to improved generalization (Simard et al., 1998), the procedure is dataset-dependent,\nand thus requires the use of expert knowledge. Furthermore, data augmentation assumes that the\n\u2217Alphabetical order.\n1\narXiv:1710.09412v2  [cs.LG]  27 Apr 2018\nPublished as a conference paper at ICLR 2018\nexamples in the vicinity share the same class, and does not model the vicinity relation across examples\nof different classes.\nContribution\nMotivated by these issues, we introduce a simple and data-agnostic data augmenta-\ntion routine, termed mixup (Section 2). In a nutshell, mixup constructs virtual training examples\n\u02dcx = \u03bbxi + (1 \u2212\u03bb)xj,\nwhere xi, xj are raw input vectors\n\u02dcy = \u03bbyi + (1 \u2212\u03bb)yj,\nwhere yi, yj are one-hot label encodings\n(xi, yi) and (xj, yj) are two examples drawn at random from our training data, and \u03bb \u2208[0, 1].\nTherefore, mixup extends the training distribution by incorporating the prior knowledge that linear\ninterpolations of feature vectors should lead to linear interpolations of the associated targets. mixup\ncan be implemented in a few lines of code, and introduces minimal computation overhead.\nDespite its simplicity, mixup allows a new state-of-the-art performance in the CIFAR-10, CIFAR-\n100, and ImageNet-2012 image classi\ufb01cation datasets (Sections 3.1 and 3.2). Furthermore, mixup\nincreases the robustness of neural networks when learning from corrupt labels (Section 3.4), or facing\nadversarial examples (Section 3.5). Finally, mixup improves generalization on speech (Sections 3.3)\nand tabular (Section 3.6) data, and can be used to stabilize the training of GANs (Section 3.7). The\nsource-code necessary to replicate our CIFAR-10 experiments is available at:\nhttps://github.com/facebookresearch/mixup-cifar10.\nTo understand the effects of various design choices in mixup, we conduct a thorough set of ablation\nstudy experiments (Section 3.8). The results suggest that mixup performs signi\ufb01cantly better than\nrelated methods in previous work, and each of the design choices contributes to the \ufb01nal performance.\nWe conclude by exploring the connections to prior work (Section 4), as well as offering some points\nfor discussion (Section 5).\n2\nFROM EMPIRICAL RISK MINIMIZATION TO mixup\nIn supervised learning, we are interested in \ufb01nding a function f \u2208F that describes the relationship\nbetween a random feature vector X and a random target vector Y , which follow the joint distribution\nP(X, Y ). To this end, we \ufb01rst de\ufb01ne a loss function \u2113that penalizes the differences between\npredictions f(x) and actual targets y, for examples (x, y) \u223cP. Then, we minimize the average of\nthe loss function \u2113over the data distribution P, also known as the expected risk:\nR(f) =\nZ\n\u2113(f(x), y)dP(x, y).\nUnfortunately, the distribution P is unknown in most practical situations. Instead, we usually have\naccess to a set of training data D = {(xi, yi)}n\ni=1, where (xi, yi) \u223cP for all i = 1, . . . , n. Using\nthe training data D, we may approximate P by the empirical distribution\nP\u03b4(x, y) = 1\nn\nn\nX\ni=1\n\u03b4(x = xi, y = yi),\nwhere \u03b4(x = xi, y = yi) is a Dirac mass centered at (xi, yi). Using the empirical distribution P\u03b4, we\ncan now approximate the expected risk by the empirical risk:\nR\u03b4(f) =\nZ\n\u2113(f(x), y)dP\u03b4(x, y) = 1\nn\nn\nX\ni=1\n\u2113(f(xi), yi).\n(1)\nLearning the function f by minimizing (1) is known as the Empirical Risk Minimization (ERM)\nprinciple (Vapnik, 1998). While ef\ufb01cient to compute, the empirical risk (1) monitors the behaviour\nof f only at a \ufb01nite set of n examples. When considering functions with a number parameters\ncomparable to n (such as large neural networks), one trivial way to minimize (1) is to memorize the\ntraining data (Zhang et al., 2017). Memorization, in turn, leads to the undesirable behaviour of f\noutside the training data (Szegedy et al., 2014).\n2\nPublished as a conference paper at ICLR 2018\n# y1, y2 should be one-hot vectors\nfor (x1, y1), (x2, y2) in zip(loader1, loader2):\nlam = numpy.random.beta(alpha, alpha)\nx = Variable(lam * x1 + (1. - lam) * x2)\ny = Variable(lam * y1 + (1. - lam) * y2)\noptimizer.zero_grad()\nloss(net(x), y).backward()\noptimizer.step()\n(a) One epoch of mixup training in PyTorch.\nERM\nmixup\n(b) Effect of mixup (\u03b1 = 1) on a\ntoy problem.\nGreen: Class 0.\nOr-\nange: Class 1. Blue shading indicates\np(y = 1|x).\nFigure 1: Illustration of mixup, which converges to ERM as \u03b1 \u21920.\nHowever, the na\u00a8\u0131ve estimate P\u03b4 is one out of many possible choices to approximate the true distribu-\ntion P. For instance, in the Vicinal Risk Minimization (VRM) principle (Chapelle et al., 2000), the\ndistribution P is approximated by\nP\u03bd(\u02dcx, \u02dcy) = 1\nn\nn\nX\ni=1\n\u03bd(\u02dcx, \u02dcy|xi, yi),\nwhere \u03bd is a vicinity distribution that measures the probability of \ufb01nding the virtual feature-target\npair (\u02dcx, \u02dcy) in the vicinity of the training feature-target pair (xi, yi). In particular, Chapelle et al.\n(2000) considered Gaussian vicinities \u03bd(\u02dcx, \u02dcy|xi, yi) = N(\u02dcx \u2212xi, \u03c32)\u03b4(\u02dcy = yi), which is equivalent\nto augmenting the training data with additive Gaussian noise. To learn using VRM, we sample the\nvicinal distribution to construct a dataset D\u03bd := {(\u02dcxi, \u02dcyi)}m\ni=1, and minimize the empirical vicinal\nrisk:\nR\u03bd(f) = 1\nm\nm\nX\ni=1\n\u2113(f(\u02dcxi), \u02dcyi).\nThe contribution of this paper is to propose a generic vicinal distribution, called mixup:\n\u00b5(\u02dcx, \u02dcy|xi, yi) = 1\nn\nn\nX\nj\nE\n\u03bb [\u03b4(\u02dcx = \u03bb \u00b7 xi + (1 \u2212\u03bb) \u00b7 xj, \u02dcy = \u03bb \u00b7 yi + (1 \u2212\u03bb) \u00b7 yj)] ,\nwhere \u03bb \u223cBeta(\u03b1, \u03b1), for \u03b1 \u2208(0, \u221e). In a nutshell, sampling from the mixup vicinal distribution\nproduces virtual feature-target vectors\n\u02dcx = \u03bbxi + (1 \u2212\u03bb)xj,\n\u02dcy = \u03bbyi + (1 \u2212\u03bb)yj,\nwhere (xi, yi) and (xj, yj) are two feature-target vectors drawn at random from the training data, and\n\u03bb \u2208[0, 1]. The mixup hyper-parameter \u03b1 controls the strength of interpolation between feature-target\npairs, recovering the ERM principle as \u03b1 \u21920.\nThe implementation of mixup training is straightforward, and introduces a minimal computation\noverhead. Figure 1a shows the few lines of code necessary to implement mixup training in PyTorch.\nFinally, we mention alternative design choices. First, in preliminary experiments we \ufb01nd that convex\ncombinations of three or more examples with weights sampled from a Dirichlet distribution does not\nprovide further gain, but increases the computation cost of mixup. Second, our current implementation\nuses a single data loader to obtain one minibatch, and then mixup is applied to the same minibatch\nafter random shuf\ufb02ing. We found this strategy works equally well, while reducing I/O requirements.\nThird, interpolating only between inputs with equal label did not lead to the performance gains of\nmixup discussed in the sequel. More empirical comparison can be found in Section 3.8.\nWhat is mixup doing?\nThe mixup vicinal distribution can be understood as a form of data aug-\nmentation that encourages the model f to behave linearly in-between training examples. We argue\nthat this linear behaviour reduces the amount of undesirable oscillations when predicting outside the\ntraining examples. Also, linearity is a good inductive bias from the perspective of Occam\u2019s razor,\n3\nPublished as a conference paper at ICLR 2018\n0.00\n0.25\n0.50\n0.75\n1.00\n\u03bb\n0\n10\n20\n30\n40\n50\n% miss\nERM\nmixup\n(a) Prediction errors in-between training data. Evalu-\nated at x = \u03bbxi+(1\u2212\u03bb)xj, a prediction is counted as\na \u201cmiss\u201d if it does not belong to {yi, yj}. The model\ntrained with mixup has fewer misses.\n0.00\n0.25\n0.50\n0.75\n1.00\n\u03bb\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n\u2225\u2207\u2113\u2225\nERM\nmixup\n(b) Norm of the gradients of the model w.r.t. input\nin-between training data, evaluated at x = \u03bbxi +\n(1 \u2212\u03bb)xj. The model trained with mixup has smaller\ngradient norms.\nFigure 2: mixup leads to more robust model behaviors in-between the training data.\nModel\nMethod\nEpochs\nTop-1 Error\nTop-5 Error\nResNet-50\nERM (Goyal et al., 2017)\n90\n23.5\n-\nmixup \u03b1 = 0.2\n90\n23.3\n6.6\nResNet-101\nERM (Goyal et al., 2017)\n90\n22.1\n-\nmixup \u03b1 = 0.2\n90\n21.5\n5.6\nResNeXt-101 32*4d\nERM (Xie et al., 2016)\n100\n21.2\n-\nERM\n90\n21.2\n5.6\nmixup \u03b1 = 0.4\n90\n20.7\n5.3\nResNeXt-101 64*4d\nERM (Xie et al., 2016)\n100\n20.4\n5.3\nmixup \u03b1 = 0.4\n90\n19.8\n4.9\nResNet-50\nERM\n200\n23.6\n7.0\nmixup \u03b1 = 0.2\n200\n22.1\n6.1\nResNet-101\nERM\n200\n22.0\n6.1\nmixup \u03b1 = 0.2\n200\n20.8\n5.4\nResNeXt-101 32*4d\nERM\n200\n21.3\n5.9\nmixup \u03b1 = 0.4\n200\n20.1\n5.0\nTable 1: Validation errors for ERM and mixup on the development set of ImageNet-2012.\nsince it is one of the simplest possible behaviors. Figure 1b shows that mixup leads to decision\nboundaries that transition linearly from class to class, providing a smoother estimate of uncertainty.\nFigure 2 illustrate the average behaviors of two neural network models trained on the CIFAR-10\ndataset using ERM and mixup. Both models have the same architecture, are trained with the same\nprocedure, and are evaluated at the same points in-between randomly sampled training data. The\nmodel trained with mixup is more stable in terms of model predictions and gradient norms in-between\ntraining samples.\n3\nEXPERIMENTS\n3.1\nIMAGENET CLASSIFICATION\nWe evaluate mixup on the ImageNet-2012 classi\ufb01cation dataset (Russakovsky et al., 2015). This\ndataset contains 1.3 million training images and 50,000 validation images, from a total of 1,000 classes.\nFor training, we follow standard data augmentation practices: scale and aspect ratio distortions,\nrandom crops, and horizontal \ufb02ips (Goyal et al., 2017). During evaluation, only the 224 \u00d7 224 central\ncrop of each image is tested. We use mixup and ERM to train several state-of-the-art ImageNet-2012\nclassi\ufb01cation models, and report both top-1 and top-5 error rates in Table 1.\n4\nPublished as a conference paper at ICLR 2018\nDataset\nModel\nERM\nmixup\nCIFAR-10\nPreAct ResNet-18\n5.6\n4.2\nWideResNet-28-10\n3.8\n2.7\nDenseNet-BC-190\n3.7\n2.7\nCIFAR-100\nPreAct ResNet-18\n25.6\n21.1\nWideResNet-28-10\n19.4\n17.5\nDenseNet-BC-190\n19.0\n16.8\n(a) Test errors for the CIFAR experiments.\n0\n50\n100\n150\n200\nepoch\n0\n5\n10\n15\n20\nerror\nCIFAR-10 Test Error\nDenseNet-190 baseline\nDenseNet-190 mixup\n(b) Test error evolution for the best\nERM and mixup models.\nFigure 3: Test errors for ERM and mixup on the CIFAR experiments.\nFor all the experiments in this section, we use data-parallel distributed training in Caffe21 with\na minibatch size of 1,024. We use the learning rate schedule described in (Goyal et al., 2017).\nSpeci\ufb01cally, the learning rate is increased linearly from 0.1 to 0.4 during the \ufb01rst 5 epochs, and it is\nthen divided by 10 after 30, 60 and 80 epochs when training for 90 epochs; or after 60, 120 and 180\nepochs when training for 200 epochs.\nFor mixup, we \ufb01nd that \u03b1 \u2208[0.1, 0.4] leads to improved performance over ERM, whereas for large \u03b1,\nmixup leads to under\ufb01tting. We also \ufb01nd that models with higher capacities and/or longer training\nruns are the ones to bene\ufb01t the most from mixup. For example, when trained for 90 epochs, the mixup\nvariants of ResNet-101 and ResNeXt-101 obtain a greater improvement (0.5% to 0.6%) over their\nERM analogues than the gain of smaller models such as ResNet-50 (0.2%). When trained for 200\nepochs, the top-1 error of the mixup variant of ResNet-50 is further reduced by 1.2% compared to the\n90 epoch run, whereas its ERM analogue stays the same.\n3.2\nCIFAR-10 AND CIFAR-100\nWe conduct additional image classi\ufb01cation experiments on the CIFAR-10 and CIFAR-100 datasets\nto further evaluate the generalization performance of mixup. In particular, we compare ERM and\nmixup training for: PreAct ResNet-18 (He et al., 2016) as implemented in (Liu, 2017), WideResNet-\n28-10 (Zagoruyko & Komodakis, 2016a) as implemented in (Zagoruyko & Komodakis, 2016b), and\nDenseNet (Huang et al., 2017) as implemented in (Veit, 2017). For DenseNet, we change the growth\nrate to 40 to follow the DenseNet-BC-190 speci\ufb01cation from (Huang et al., 2017). For mixup, we\n\ufb01x \u03b1 = 1, which results in interpolations \u03bb uniformly distributed between zero and one. All models\nare trained on a single Nvidia Tesla P100 GPU using PyTorch2 for 200 epochs on the training set\nwith 128 examples per minibatch, and evaluated on the test set. Learning rates start at 0.1 and are\ndivided by 10 after 100 and 150 epochs for all models except WideResNet. For WideResNet, we\nfollow (Zagoruyko & Komodakis, 2016a) and divide the learning rate by 10 after 60, 120 and 180\nepochs. Weight decay is set to 10\u22124. We do not use dropout in these experiments.\nWe summarize our results in Figure 3a. In both CIFAR-10 and CIFAR-100 classi\ufb01cation problems,\nthe models trained using mixup signi\ufb01cantly outperform their analogues trained with ERM. As seen\nin Figure 3b, mixup and ERM converge at a similar speed to their best test errors. Note that the\nDenseNet models in (Huang et al., 2017) were trained for 300 epochs with further learning rate\ndecays scheduled at the 150 and 225 epochs, which may explain the discrepancy the performance of\nDenseNet reported in Figure 3a and the original result of Huang et al. (2017).\n3.3\nSPEECH DATA\nNext, we perform speech recognition experiments using the Google commands dataset (Warden,\n2017). The dataset contains 65,000 utterances, where each utterance is about one-second long and\nbelongs to one out of 30 classes. The classes correspond to voice commands such as yes, no, down,\nleft, as pronounced by a few thousand different speakers. To preprocess the utterances, we \ufb01rst\n1https://caffe2.ai\n2http://pytorch.org\n5\nPublished as a conference paper at ICLR 2018\nModel\nMethod\nValidation set\nTest set\nLeNet\nERM\n9.8\n10.3\nmixup (\u03b1 = 0.1)\n10.1\n10.8\nmixup (\u03b1 = 0.2)\n10.2\n11.3\nVGG-11\nERM\n5.0\n4.6\nmixup (\u03b1 = 0.1)\n4.0\n3.8\nmixup (\u03b1 = 0.2)\n3.9\n3.4\nFigure 4: Classi\ufb01cation errors of ERM and mixup on the Google commands dataset.\nextract normalized spectrograms from the original waveforms at a sampling rate of 16 kHz. Next, we\nzero-pad the spectrograms to equalize their sizes at 160 \u00d7 101. For speech data, it is reasonable to\napply mixup both at the waveform and spectrogram levels. Here, we apply mixup at the spectrogram\nlevel just before feeding the data to the network.\nFor this experiment, we compare a LeNet (Lecun et al., 2001) and a VGG-11 (Simonyan & Zisserman,\n2015) architecture, each of them composed by two convolutional and two fully-connected layers.\nWe train each model for 30 epochs with minibatches of 100 examples, using Adam as the optimizer\n(Kingma & Ba, 2015). Training starts with a learning rate equal to 3 \u00d7 10\u22123 and is divided by 10\nevery 10 epochs. For mixup, we use a warm-up period of \ufb01ve epochs where we train the network on\noriginal training examples, since we \ufb01nd it speeds up initial convergence. Table 4 shows that mixup\noutperforms ERM on this task, specially when using VGG-11, the model with larger capacity.\n3.4\nMEMORIZATION OF CORRUPTED LABELS\nFollowing Zhang et al. (2017), we evaluate the robustness of ERM and mixup models against randomly\ncorrupted labels. We hypothesize that increasing the strength of mixup interpolation \u03b1 should generate\nvirtual examples further from the training examples, making memorization more dif\ufb01cult to achieve.\nIn particular, it should be easier to learn interpolations between real examples compared to memorizing\ninterpolations involving random labels. We adapt an open-source implementation (Zhang, 2017)\nto generate three CIFAR-10 training sets, where 20%, 50%, or 80% of the labels are replaced by\nrandom noise, respectively. All the test labels are kept intact for evaluation. Dropout (Srivastava\net al., 2014) is considered the state-of-the-art method for learning with corrupted labels (Arpit et al.,\n2017). Thus, we compare in these experiments mixup, dropout, mixup + dropout, and ERM. For\nmixup, we choose \u03b1 \u2208{1, 2, 8, 32}; for dropout, we add one dropout layer in each PreAct block after\nthe ReLU activation layer between two convolution layers, as suggested in (Zagoruyko & Komodakis,\n2016a). We choose the dropout probability p \u2208{0.5, 0.7, 0.8, 0.9}. For the combination of mixup\nand dropout, we choose \u03b1 \u2208{1, 2, 4, 8} and p \u2208{0.3, 0.5, 0.7}. These experiments use the PreAct\nResNet-18 (He et al., 2016) model implemented in (Liu, 2017). All the other settings are the same as\nin Section 3.2.\nWe summarize our results in Table 2, where we note the best test error achieved during the training\nsession, as well as the \ufb01nal test error after 200 epochs. To quantify the amount of memorization, we\nalso evaluate the training errors at the last epoch on real labels and corrupted labels. As the training\nprogresses with a smaller learning rate (e.g. less than 0.01), the ERM model starts to over\ufb01t the\ncorrupted labels. When using a large probability (e.g. 0.7 or 0.8), dropout can effectively reduce\nover\ufb01tting. mixup with a large \u03b1 (e.g. 8 or 32) outperforms dropout on both the best and last epoch\ntest errors, and achieves lower training error on real labels while remaining resistant to noisy labels.\nInterestingly, mixup + dropout performs the best of all, showing that the two methods are compatible.\n3.5\nROBUSTNESS TO ADVERSARIAL EXAMPLES\nOne undesirable consequence of models trained using ERM is their fragility to adversarial exam-\nples (Szegedy et al., 2014). Adversarial examples are obtained by adding tiny (visually imperceptible)\nperturbations to legitimate examples in order to deteriorate the performance of the model. The adver-\nsarial noise is generated by ascending the gradient of the loss surface with respect to the legitimate\nexample. Improving the robustness to adversarial examples is a topic of active research.\n6\nPublished as a conference paper at ICLR 2018\nLabel corruption\nMethod\nTest error\nTraining error\nBest\nLast\nReal\nCorrupted\n20%\nERM\n12.7\n16.6\n0.05\n0.28\nERM + dropout (p = 0.7)\n8.8\n10.4\n5.26\n83.55\nmixup (\u03b1 = 8)\n5.9\n6.4\n2.27\n86.32\nmixup + dropout (\u03b1 = 4, p = 0.1)\n6.2\n6.2\n1.92\n85.02\n50%\nERM\n18.8\n44.6\n0.26\n0.64\nERM + dropout (p = 0.8)\n14.1\n15.5\n12.71\n86.98\nmixup (\u03b1 = 32)\n11.3\n12.7\n5.84\n85.71\nmixup + dropout (\u03b1 = 8, p = 0.3)\n10.9\n10.9\n7.56\n87.90\n80%\nERM\n36.5\n73.9\n0.62\n0.83\nERM + dropout (p = 0.8)\n30.9\n35.1\n29.84\n86.37\nmixup (\u03b1 = 32)\n25.3\n30.9\n18.92\n85.44\nmixup + dropout (\u03b1 = 8, p = 0.3)\n24.0\n24.8\n19.70\n87.67\nTable 2: Results on the corrupted label experiments for the best models.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n90.7\n99.9\nmixup\n75.2\n99.6\nTop-5\nERM\n63.1\n93.4\nmixup\n49.1\n95.8\n(a) White box attacks.\nMetric\nMethod\nFGSM\nI-FGSM\nTop-1\nERM\n57.0\n57.3\nmixup\n46.0\n40.9\nTop-5\nERM\n24.8\n18.1\nmixup\n17.4\n11.8\n(b) Black box attacks.\nTable 3: Classi\ufb01cation errors of ERM and mixup models when tested on adversarial examples.\nAmong the several methods aiming to solve this problem, some have proposed to penalize the norm of\nthe Jacobian of the model to control its Lipschitz constant (Drucker & Le Cun, 1992; Cisse et al., 2017;\nBartlett et al., 2017; Hein & Andriushchenko, 2017). Other approaches perform data augmentation\nby producing and training on adversarial examples (Goodfellow et al., 2015). Unfortunately, all\nof these methods add signi\ufb01cant computational overhead to ERM. Here, we show that mixup can\nsigni\ufb01cantly improve the robustness of neural networks without hindering the speed of ERM by\npenalizing the norm of the gradient of the loss w.r.t a given input along the most plausible directions\n(e.g. the directions to other training points). Indeed, Figure 2 shows that mixup results in models\nhaving a smaller loss and gradient norm between examples compared to vanilla ERM.\nTo assess the robustness of mixup models to adversarial examples, we use three ResNet-101 models:\ntwo of them trained using ERM on ImageNet-2012, and the third trained using mixup. In the \ufb01rst\nset of experiments, we study the robustness of one ERM model and the mixup model against white\nbox attacks. That is, for each of the two models, we use the model itself to generate adversarial\nexamples, either using the Fast Gradient Sign Method (FGSM) or the Iterative FGSM (I-FGSM)\nmethods (Goodfellow et al., 2015), allowing a maximum perturbation of \u03f5 = 4 for every pixel. For\nI-FGSM, we use 10 iterations with equal step size. In the second set of experiments, we evaluate\nrobustness against black box attacks. That is, we use the \ufb01rst ERM model to produce adversarial\nexamples using FGSM and I-FGSM. Then, we test the robustness of the second ERM model and the\nmixup model to these examples. The results of both settings are summarized in Table 3.\nFor the FGSM white box attack, the mixup model is 2.7 times more robust than the ERM model in\nterms of Top-1 error. For the FGSM black box attack, the mixup model is 1.25 times more robust\nthan the ERM model in terms of Top-1 error. Also, while both mixup and ERM are not robust to\nwhite box I-FGSM attacks, mixup is about 40% more robust than ERM in the black box I-FGSM\nsetting. Overall, mixup produces neural networks that are signi\ufb01cantly more robust than ERM against\nadversarial examples in white box and black settings without additional overhead compared to ERM.\n7\nPublished as a conference paper at ICLR 2018\nDataset\nERM\nmixup\nAbalone\n74.0\n73.6\nArcene\n57.6\n48.0\nArrhythmia\n56.6\n46.3\nDataset\nERM\nmixup\nHtru2\n2.0\n2.0\nIris\n21.3\n17.3\nPhishing\n16.3\n15.2\nTable 4: ERM and mixup classi\ufb01cation errors on the UCI datasets.\nERM GAN\nmixup GAN (\u03b1 = 0.2)\nFigure 5: Effect of mixup on stabilizing GAN training at iterations 10, 100, 1000, 10000, and 20000.\n3.6\nTABULAR DATA\nTo further explore the performance of mixup on non-image data, we performed a series of experiments\non six arbitrary classi\ufb01cation problems drawn from the UCI dataset (Lichman, 2013). The neural\nnetworks in this section are fully-connected, and have two hidden layers of 128 ReLU units. The\nparameters of these neural networks are learned using Adam (Kingma & Ba, 2015) with default\nhyper-parameters, over 10 epochs of mini-batches of size 16. Table 4 shows that mixup improves the\naverage test error on four out of the six considered datasets, and never underperforms ERM.\n3.7\nSTABILIZATION OF GENERATIVE ADVERSARIAL NETWORKS (GANS)\nGenerative Adversarial Networks, also known as GANs (Goodfellow et al., 2014), are a powerful\nfamily of implicit generative models. In GANs, a generator and a discriminator compete against\neach other to model a distribution P. On the one hand, the generator g competes to transform noise\nvectors z \u223cQ into fake samples g(z) that resemble real samples x \u223cP. On the other hand, the\ndiscriminator competes to distinguish between real samples x and fake samples g(z). Mathematically,\ntraining a GAN is equivalent to solving the optimization problem\nmax\ng\nmin\nd\nE\nx,z \u2113(d(x), 1) + \u2113(d(g(z)), 0),\nwhere \u2113is the binary cross entropy loss. Unfortunately, solving the previous min-max equation is a\nnotoriously dif\ufb01cult optimization problem (Goodfellow, 2016), since the discriminator often provides\nthe generator with vanishing gradients. We argue that mixup should stabilize GAN training because it\nacts as a regularizer on the gradients of the discriminator, akin to the binary classi\ufb01er in Figure 1b.\nThen, the smoothness of the discriminator guarantees a stable source of gradient information to the\ngenerator. The mixup formulation of GANs is:\nmax\ng\nmin\nd\nE\nx,z,\u03bb \u2113(d(\u03bbx + (1 \u2212\u03bb)g(z)), \u03bb).\nFigure 5 illustrates the stabilizing effect of mixup the training of GAN (orange samples) when\nmodeling two toy datasets (blue samples). The neural networks in these experiments are fully-\nconnected and have three hidden layers of 512 ReLU units. The generator network accepts two-\ndimensional Gaussian noise vectors. The networks are trained for 20,000 mini-batches of size\n128 using the Adam optimizer with default parameters, where the discriminator is trained for \ufb01ve\niterations before every generator iteration. The training of mixup GANs seems promisingly robust to\nhyper-parameter and architectural choices.\n8\nPublished as a conference paper at ICLR 2018\nMethod\nSpeci\ufb01cation\nModi\ufb01ed\nWeight decay\nInput\nTarget\n10\u22124\n5 \u00d7 10\u22124\nERM\n\u0017\n\u0017\n5.53\n5.18\nmixup\nAC + RP\n\u0013\n\u0013\n4.24\n4.68\nAC + KNN\n\u0013\n\u0013\n4.98\n5.26\nmix labels and latent\nLayer 1\n\u0013\n\u0013\n4.44\n4.51\nrepresentations\nLayer 2\n\u0013\n\u0013\n4.56\n4.61\n(AC + RP)\nLayer 3\n\u0013\n\u0013\n5.39\n5.55\nLayer 4\n\u0013\n\u0013\n5.95\n5.43\nLayer 5\n\u0013\n\u0013\n5.39\n5.15\nmix inputs only\nSC + KNN (Chawla et al., 2002)\n\u0013\n\u0017\n5.45\n5.52\nAC + KNN\n\u0013\n\u0017\n5.43\n5.48\nSC + RP\n\u0013\n\u0017\n5.23\n5.55\nAC + RP\n\u0013\n\u0017\n5.17\n5.72\nlabel smoothing\n\u03f5 = 0.05\n\u0017\n\u0013\n5.25\n5.02\n(Szegedy et al., 2016)\n\u03f5 = 0.1\n\u0017\n\u0013\n5.33\n5.17\n\u03f5 = 0.2\n\u0017\n\u0013\n5.34\n5.06\nmix inputs +\n\u03f5 = 0.05\n\u0013\n\u0013\n5.02\n5.40\nlabel smoothing\n\u03f5 = 0.1\n\u0013\n\u0013\n5.08\n5.09\n(AC + RP)\n\u03f5 = 0.2\n\u0013\n\u0013\n4.98\n5.06\n\u03f5 = 0.4\n\u0013\n\u0013\n5.25\n5.39\nadd Gaussian noise\n\u03c3 = 0.05\n\u0013\n\u0017\n5.53\n5.04\nto inputs\n\u03c3 = 0.1\n\u0013\n\u0017\n6.41\n5.86\n\u03c3 = 0.2\n\u0013\n\u0017\n7.16\n7.24\nTable 5: Results of the ablation studies on the CIFAR-10 dataset. Reported are the median test errors\nof the last 10 epochs. A tick (\u0013) means the component is different from standard ERM training,\nwhereas a cross (\u0017) means it follows the standard training practice. AC: mix between all classes. SC:\nmix within the same class. RP: mix between random pairs. KNN: mix between k-nearest neighbors\n(k=200). Please refer to the text for details about the experiments and interpretations.\n3.8\nABLATION STUDIES\nmixup is a data augmentation method that consists of only two parts: random convex combination of\nraw inputs, and correspondingly, convex combination of one-hot label encodings. However, there are\nseveral design choices to make. For example, on how to augment the inputs, we could have chosen\nto interpolate the latent representations (i.e. feature maps) of a neural network, and we could have\nchosen to interpolate only between the nearest neighbors, or only between inputs of the same class.\nWhen the inputs to interpolate come from two different classes, we could have chosen to assign a\nsingle label to the synthetic input, for example using the label of the input that weights more in the\nconvex combination. To compare mixup with these alternative possibilities, we run a set of ablation\nstudy experiments using the PreAct ResNet-18 architecture on the CIFAR-10 dataset.\nSpeci\ufb01cally, for each of the data augmentation methods, we test two weight decay settings (10\u22124\nwhich works well for mixup, and 5 \u00d7 10\u22124 which works well for ERM). All the other settings and\nhyperparameters are the same as reported in Section 3.2.\nTo compare interpolating raw inputs with interpolating latent representations, we test on random\nconvex combination of the learned representations before each residual block (denoted Layer 1-4)\nor before the uppermost \u201caverage pooling + fully connected\u201d layer (denoted Layer 5). To compare\nmixing random pairs of inputs (RP) with mixing nearest neighbors (KNN), we \ufb01rst compute the 200\nnearest neighbors for each training sample, either from the same class (SC) or from all the classes\n(AC). Then during training, for each sample in a minibatch, we replace the sample with a synthetic\nsample by convex combination with a random draw from its nearest neighbors. To compare mixing\nall the classes (AC) with mixing within the same class (SC), we convex combine a minibatch with a\n9\nPublished as a conference paper at ICLR 2018\nrandom permutation of its sample index, where the permutation is done in a per-batch basis (AC) or a\nper-class basis (SC). To compare mixing inputs and labels with mixing inputs only, we either use a\nconvex combination of the two one-hot encodings as the target, or select the one-hot encoding of the\ncloser training sample as the target. For label smoothing, we follow Szegedy et al. (2016) and use\n\u03f5\n10 as the target for incorrect classes, and 1 \u22129\u03f5\n10 as the target for the correct class.Adding Gaussian\nnoise to inputs is used as another baseline. We report the median test errors of the last 10 epochs.\nResults are shown in Table 5.\nFrom the ablation study experiments, we have the following observations. First, mixup is the best\ndata augmentation method we test, and is signi\ufb01cantly better than the second best method (mix input\n+ label smoothing). Second, the effect of regularization can be seen by comparing the test error with a\nsmall weight decay (10\u22124) with a large one (5 \u00d7 10\u22124). For example, for ERM a large weight decay\nworks better, whereas for mixup a small weight decay is preferred, con\ufb01rming its regularization effects.\nWe also see an increasing advantage of large weight decay when interpolating in higher layers of latent\nrepresentations, indicating decreasing strength of regularization. Among all the input interpolation\nmethods, mixing random pairs from all classes (AC + RP) has the strongest regularization effect.\nLabel smoothing and adding Gaussian noise have a relatively small regularization effect. Finally,\nwe note that the SMOTE algorithm (Chawla et al., 2002) does not lead to a noticeable gain in\nperformance.\n4\nRELATED WORK\nData augmentation lies at the heart of all successful applications of deep learning, ranging from image\nclassi\ufb01cation (Krizhevsky et al., 2012) to speech recognition (Graves et al., 2013; Amodei et al.,\n2016). In all cases, substantial domain knowledge is leveraged to design suitable data transformations\nleading to improved generalization. In image classi\ufb01cation, for example, one routinely uses rotation,\ntranslation, cropping, resizing, \ufb02ipping (Lecun et al., 2001; Simonyan & Zisserman, 2015), and\nrandom erasing (Zhong et al., 2017) to enforce visually plausible invariances in the model through\nthe training data. Similarly, in speech recognition, noise injection is a prevalent practice to improve\nthe robustness and accuracy of the trained models (Amodei et al., 2016).\nMore related to mixup, Chawla et al. (2002) propose to augment the rare class in an imbalanced\ndataset by interpolating the nearest neighbors; DeVries & Taylor (2017) show that interpolation and\nextrapolation the nearest neighbors of the same class in feature space can improve generalization.\nHowever, their proposals only operate among the nearest neighbors within a certain class at the\ninput / feature level, and hence does not account for changes in the corresponding labels. Recent\napproaches have also proposed to regularize the output distribution of a neural network by label\nsmoothing (Szegedy et al., 2016), or penalizing high-con\ufb01dence softmax distributions (Pereyra et al.,\n2017). These methods bear similarities with mixup in the sense that supervision depends on multiple\nsmooth labels, rather than on single hard labels as in traditional ERM. However, the label smoothing\nin these works is applied or regularized independently from the associated feature values.\nmixup enjoys several desirable aspects of previous data augmentation and regularization schemes\nwithout suffering from their drawbacks. Like the method of DeVries & Taylor (2017), it does not\nrequire signi\ufb01cant domain knowledge. Like label smoothing, the supervision of every example is not\noverly dominated by the ground-truth label. Unlike both of these approaches, the mixup transformation\nestablishes a linear relationship between data augmentation and the supervision signal. We believe\nthat this leads to a strong regularizer that improves generalization as demonstrated by our experiments.\nThe linearity constraint, through its effect on the derivatives of the function approximated, also relates\nmixup to other methods such as Sobolev training of neural networks (Czarnecki et al., 2017) or\nWGAN-GP (Gulrajani et al., 2017).\n5\nDISCUSSION\nWe have proposed mixup, a data-agnostic and straightforward data augmentation principle. We\nhave shown that mixup is a form of vicinal risk minimization, which trains on virtual examples\nconstructed as the linear interpolation of two random examples from the training set and their labels.\nIncorporating mixup into existing training pipelines reduces to a few lines of code, and introduces\nlittle or no computational overhead. Throughout an extensive evaluation, we have shown that mixup\n10\nPublished as a conference paper at ICLR 2018\nimproves the generalization error of state-of-the-art models on ImageNet, CIFAR, speech, and\ntabular datasets. Furthermore, mixup helps to combat memorization of corrupt labels, sensitivity to\nadversarial examples, and instability in adversarial training.\nIn our experiments, the following trend is consistent: with increasingly large \u03b1, the training error on\nreal data increases, while the generalization gap decreases. This sustains our hypothesis that mixup\nimplicitly controls model complexity. However, we do not yet have a good theory for understanding\nthe \u2018sweet spot\u2019 of this bias-variance trade-off. For example, in CIFAR-10 classi\ufb01cation we can\nget very low training error on real data even when \u03b1 \u2192\u221e(i.e., training only on averages of pairs\nof real examples), whereas in ImageNet classi\ufb01cation, the training error on real data increases\nsigni\ufb01cantly with \u03b1 \u2192\u221e. Based on our ImageNet and Google commands experiments with different\nmodel architectures, we conjecture that increasing the model capacity would make training error less\nsensitive to large \u03b1, hence giving mixup a more signi\ufb01cant advantage.\nmixup also opens up several possibilities for further exploration. First, is it possible to make\nsimilar ideas work on other types of supervised learning problems, such as regression and structured\nprediction? While generalizing mixup to regression problems is straightforward, its application\nto structured prediction problems such as image segmentation remains less obvious. Second, can\nsimilar methods prove helpful beyond supervised learning? The interpolation principle seems like a\nreasonable inductive bias which might also help in unsupervised, semi-supervised, and reinforcement\nlearning. Can we extend mixup to feature-label extrapolation to guarantee a robust model behavior\nfar away from the training data? Although our discussion of these directions is still speculative, we\nare excited about the possibilities mixup opens up, and hope that our observations will prove useful\nfor future development.\nACKNOWLEDGEMENTS\nWe would like to thank Priya Goyal, Yossi Adi and the PyTorch team. We also thank the Anonymous\nReview 2 for proposing the mixup + dropout experiments.\nREFERENCES\nD. Amodei, S. Ananthanarayanan, R. Anubhai, J. Bai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, Q. Cheng,\nG. Chen, et al. Deep speech 2: End-to-end speech recognition in English and Mandarin. In ICML, 2016.\nD. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal, T. Maharaj, A. Fischer, A. Courville,\nY. Bengio, et al. A closer look at memorization in deep networks. ICML, 2017.\nP. Bartlett, D. J. Foster, and M. Telgarsky. Spectrally-normalized margin bounds for neural networks. NIPS,\n2017.\nO. Chapelle, J. Weston, L. Bottou, and V. Vapnik. Vicinal risk minimization. NIPS, 2000.\nN. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer. SMOTE: synthetic minority over-sampling\ntechnique. Journal of arti\ufb01cial intelligence research, 16:321\u2013357, 2002.\nC. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson. One billion word benchmark\nfor measuring progress in statistical language modeling. arXiv, 2013.\nM. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, and N. Usunier. Parseval networks: Improving robustness to\nadversarial examples. ICML, 2017.\nW. M. Czarnecki, S. Osindero, M. Jaderberg, G. \u00b4Swirszcz, and R. Pascanu. Sobolev training for neural networks.\nNIPS, 2017.\nT. DeVries and G. W. Taylor. Dataset augmentation in feature space. ICLR Workshops, 2017.\nH. Drucker and Y. Le Cun. Improving generalization performance using double backpropagation. IEEE\nTransactions on Neural Networks, 3(6):991\u2013997, 1992.\nI. Goodfellow. Tutorial: Generative adversarial networks. NIPS, 2016.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio.\nGenerative adversarial nets. NIPS, 2014.\n11\nPublished as a conference paper at ICLR 2018\nI. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples. ICLR, 2015.\nP. Goyal, P. Doll\u00b4ar, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola, A. Tulloch, Y. Jia, and K. He. Accurate,\nlarge minibatch SGD: Training ImageNet in 1 hour. arXiv, 2017.\nA. Graves, A.-r. Mohamed, and G. Hinton. Speech recognition with deep recurrent neural networks. In ICASSP.\nIEEE, 2013.\nI. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. Courville. Improved training of Wasserstein GANs.\nNIPS, 2017.\nN. Harvey, C. Liaw, and A. Mehrabian. Nearly-tight VC-dimension bounds for piecewise linear neural networks.\nJMLR, 2017.\nK. He, X. Zhang, S. Ren, and J. Sun. Identity mappings in deep residual networks. ECCV, 2016.\nM. Hein and M. Andriushchenko. Formal guarantees on the robustness of a classi\ufb01er against adversarial\nmanipulation. NIPS, 2017.\nG. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N.\nSainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared views of four\nresearch groups. IEEE Signal Processing Magazine, 2012.\nG. Huang, Z. Liu, L. van der Maaten, and K. Q. Weinberger. Densely connected convolutional networks. CVPR,\n2017.\nD. Kingma and J. Ba. Adam: A method for stochastic optimization. ICLR, 2015.\nA. Krizhevsky, I. Sutskever, and G. E. Hinton. ImageNet classi\ufb01cation with deep convolutional neural networks.\nNIPS, 2012.\nY. Lecun, L. Bottou, Y. Bengio, and P. Haffner. Gradient-based learning applied to document recognition.\nProceedings of IEEE, 2001.\nM. Lichman. UCI machine learning repository, 2013.\nK. Liu, 2017. URL https://github.com/kuangliu/pytorch-cifar.\nG. Pereyra, G. Tucker, J. Chorowski, \u0141. Kaiser, and G. Hinton. Regularizing neural networks by penalizing\ncon\ufb01dent output distributions. ICLR Workshops, 2017.\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein,\nA. C. Berg, and L. Fei-Fei. ImageNet large scale visual recognition challenge. IJCV, 2015.\nD. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou,\nV. Panneershelvam, M. Lanctot, et al. Mastering the game of Go with deep neural networks and tree search.\nNature, 2016.\nP. Simard, Y. LeCun, J. Denker, and B. Victorri. Transformation invariance in pattern recognitiontangent distance\nand tangent propagation. Neural networks: tricks of the trade, 1998.\nK. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. ICLR,\n2015.\nJ. T. Springenberg, A. Dosovitskiy, T. Brox, and M. Riedmiller. Striving for simplicity: The all convolutional\nnet. ICLR Workshops, 2015.\nN. Srivastava, G. E. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov. Dropout: a simple way to\nprevent neural networks from over\ufb01tting. Journal of Machine Learning Research, 15(1):1929\u20131958, 2014.\nC. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow, and R. Fergus. Intriguing properties\nof neural networks. ICLR, 2014.\nC. Szegedy, V. Vanhoucke, S. Ioffe, J. Shlens, and Z. Wojna. Rethinking the Inception architecture for computer\nvision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016.\nV. N. Vapnik. Statistical learning theory. J. Wiley, 1998.\nV. Vapnik and A. Y. Chervonenkis. On the uniform convergence of relative frequencies of events to their\nprobabilities. Theory of Probability and its Applications, 1971.\n12\nPublished as a conference paper at ICLR 2018\nA. Veit, 2017. URL https://github.com/andreasveit.\nP.\nWarden,\n2017.\nURL\nhttps://research.googleblog.com/2017/08/\nlaunching-speech-commands-dataset.html.\nS. Xie, R. Girshick, P. Doll\u00b4ar, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks.\nCVPR, 2016.\nS. Zagoruyko and N. Komodakis. Wide residual networks. BMVC, 2016a.\nS.\nZagoruyko\nand\nN.\nKomodakis,\n2016b.\nURL\nhttps://github.com/szagoruyko/\nwide-residual-networks.\nC. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning requires rethinking\ngeneralization. ICLR, 2017.\nC. Zhang, 2017. URL https://github.com/pluskid/fitting-random-labels.\nZ. Zhong, L. Zheng, G. Kang, S. Li, and Y. Yang. Random erasing data augmentation. arXiv, 2017.\n13\n",
    "2205.04816": "Reconstruction Enhanced Multi-View Contrastive Learning for\nAnomaly Detection on Attributed Networks\nJiaqiang Zhang1,2 , Senzhang Wang3 and Songcan Chen1,2\u2217\n1College of Computer Science and Technology, Nanjing University of Aeronautics and Astronautics\n2MIIT Key Laboratory of Pattern Analysis and Machine Intelligence\n3Central South University\n{zhangjq, s.chen}@nuaa.edu.cn, szwang@csu.edu.cn\nAbstract\nDetecting abnormal nodes from attributed networks\nis of great importance in many real applications,\nsuch as \ufb01nancial fraud detection and cyber security.\nThis task is challenging due to both the complex in-\nteractions between the anomalous nodes with other\ncounterparts and their inconsistency in terms of\nattributes. This paper proposes a self-supervised\nlearning framework that jointly optimizes a multi-\nview contrastive learning-based module and an at-\ntribute reconstruction-based module to more ac-\ncurately detect anomalies on attributed networks.\nSpeci\ufb01cally, two contrastive learning views are\n\ufb01rstly established, which allow the model to bet-\nter encode rich local and global information re-\nlated to the abnormality. Motivated by the attribute\nconsistency principle between neighboring nodes, a\nmasked autoencoder-based reconstruction module\nis also introduced to identify the nodes which have\nlarge reconstruction errors, then are regarded as\nanomalies. Finally, the two complementary mod-\nules are integrated for more accurately detecting\nthe anomalous nodes. Extensive experiments con-\nducted on \ufb01ve benchmark datasets show our model\noutperforms current state-of-the-art models.\n1\nIntroduction\nAttributed networks are ubiquitous in many real-world sce-\nnarios, such as social networks and citation networks. Re-\ncently, anomaly detection on attributed networks is of great\nsigni\ufb01cance in many security-related applications, such as\nsocial spam detection [Jain et al., 2019], \ufb01nancial fraud de-\ntection [Wang et al., 2019] and network intrusion detection\n[Shone et al., 2018], and has attracted rising research interest.\nThough it has been extensively studied, detecting anomalies\non attributed networks is still a challenging task [Ding et al.,\n2019]. One major challenge of this problem is that the abnor-\nmal patterns of nodes are related to not only their interactions\nwith other nodes on the topological structure, but also their\ninconsistency in terms of node attributes [Ding et al., 2019;\nPeng et al., 2018].\n\u2217Corresponding author\nThe early anomaly detection techniques, such as matrix\nfactorization [Li et al., 2017] and OC-SVM [Erfani et al.,\n2016] have been widely used in many applications. A pri-\nmary limitation of such methods is that they largely rely\non feature engineering constructed by domain experts. Re-\ncently, deep learning techniques, especially Graph Neural\nNetworks, have been widely used in various graph mining\ntasks (e.g. link prediction and node classi\ufb01cation), and have\nachieved considerable performance gains. GNN-based tech-\nniques [Ding et al., 2021] have also been adopted to anomaly\ndetection which aim to learn the anomaly-aware node repre-\nsentations. Due to the high cost of obtaining anomaly sam-\nples, anomaly detection is usually performed in an unsuper-\nvised manner. [Ding et al., 2019] proposed an unsupervised\nautoenconder-based method to detect the abnormal patterns\neffectively, which can capture the abnormal nodes accord-\ning to the reconstruction errors. [Liu et al., 2021b] for the\n\ufb01rst time proposed a contrastive learning based framework\nfor anomaly detection on graphs. Their model takes the lo-\ncal contextual information as supervision signals, and learns\nthe representative features from the node-subgraph instance\npairs. The \ufb01nal discriminative scores are used to detect abnor-\nmal nodes. [Jin et al., 2021] performed patch- and context-\nlevel contrastive learning via two GNN-based models to fur-\nther improve the performance.\nAlthough considerable research efforts have been devoted\nto this problem recently, we argue that the current approaches\nstill have shortcomings due to the following reasons. First,\nlocal and global structural information are not well utilized\nand integrated. Existing SOTA approach [Liu et al., 2021b]\ntypically constructed contrastive instance pair based on node-\nsubgraph to ef\ufb01ciently focus on the local information of a\nnode for anomaly detection. They adopted one-layer GCN\nto extract the local structure information from the one-hop\nneighbors, while the global structure information beyond\none-hop neighbors cannot be effectively captured and uti-\nlized. Previous studies show that higher-order neighboring\ninformation is also bene\ufb01cial for graph mining tasks. [Ding\net al., 2019] proposed to use the multi-layer GCN to capture\nthe high-order interactions for anomaly detection on graphs,\nbut the local and global structure information is modeled uni-\nformly and coupled together, leading to information redun-\ndancy in feature representation learning. Meanwhile, GCN\ncan be seen as a special form of low-pass \ufb01lter, which has the\narXiv:2205.04816v1  [cs.LG]  10 May 2022\neffectiveness in smoothing signals [Bo et al., 2021]. Stack-\ning multiple layers of GCN will more likely smooth out the\nabnormal signals. Therefore, a more effective and decou-\npled local-global structure information extraction and inte-\ngration method is needed to provide high quality structure\nfeatures. Second, how to effectively fuse the node attributes\nwith network topological structures to further boost the de-\ntection performance is not well explored. Although GCN can\nhandle both topological structure and attributes, the learned\nrepresentations are not well suitable for anomaly detection\nunder the unsupervised learning scenario [Liu et al., 2021b].\nAnomalous nodes usually present inconsistency in terms of\nnode attributes with their neighboring nodes, which provides\nus with additional self-supervised signals. How to fully uti-\nlize such signals and build an attribute reconstruction model\nbased on the attribute consistency principle to further improve\nthe anomaly detection performance is not well studied.\nTo alleviate the above drawbacks, we present a Subgraph-\nbased multi-view self-supervised framework which contains\na Contrastive-based module and a Reconstruction-based\nmodule (Sub-CR for convenience). Sub-CR can separate the\nlocal information from the global information instead of di-\nrectly fusing them together, and alleviate the risk of smooth-\ning out abnormal signals in GNN. Speci\ufb01cally, the original\ngraph is \ufb01rst augmented by both graph diffusion and sub-\ngraph sampling to obtain local and global view subgraphs\nfor each node. Then a two-view (intra- and inter-view) con-\ntrastive learning module is proposed. The intra-view con-\ntrastive learning aims to maximize the agreement between\nthe target node and its subgraph-level representations of both\nviews, here the agreement can be quanti\ufb01ed as a discrimina-\ntive score. The inter-view contrastive learning aims to make\nthe discriminative score of the two views closer, which al-\nlows the model to encode both local and global information.\nIn order to further utilize the self-supervised signals of the at-\ntribute consistency between the neighboring nodes, a masked\nautoencoder is also introduced to reconstruct the original at-\ntributes of the target node based on its neighboring nodes in\nboth views. Finally, the two complementary modules are inte-\ngrated for anomalous node detection. Our main contributions\nare summarized as follows:\n\u2022 A novel self-supervised learning framework is proposed\nto address the problem of anomaly detection on attributed net-\nworks, which can effectively integrates contrastive learning-\nbased and attribute reconstruction-based models.\n\u2022 A multi-view learning model is proposed.\nLocal and\nglobal information are modeled separately \ufb01rst and then in-\ntegrated together, which can provide high quality features for\ndetecting anomalous nodes.\n\u2022 We conduct extensive experiments on \ufb01ve benchmark\ndatasets for model evaluation. The results verify the effec-\ntiveness of our proposal and it outperforms existing state-of-\nthe-art methods.\n2\nRelated Work\n2.1\nAnomaly Detection on Attributed Networks\nThe early work [Perozzi and Akoglu, 2016] proposed a qual-\nity measure called normality, which utilized structure and at-\ntributes together on attributed networks. [Li et al., 2017] pro-\nposed a learning framework to describe the residual of at-\ntributes and their consistency with network structure to detect\nanomalies in attributed networks. With the rapid develop-\nment of deep neural networks, researchers also tried to adopt\ndeep learning techniques to detect anomalies in attributed net-\nworks. [Ding et al., 2019] proposed to use GCN and au-\ntoencoder to measure the reconstruction error of nodes from\nthe perspective of structure and attributes to identify anoma-\nlies, which can alleviate the network sparsity and data non-\nlinearity issues. [Yu et al., 2018] adopted a clustering-based\nmethod through the learned dynamic network representation\nto dynamically identify network anomalies based on cluster-\ning in an incremental manner.\n[Li et al., 2019] proposed\nSpecAE to detect global anomalies and community anoma-\nlies via a density estimation approach. [Liu et al., 2021b;\nJin et al., 2021] proposed a contrastive self-supervised learn-\ning framework for graph anomaly detection, effectively cap-\nturing the relationship between nodes and their subgraphs to\ncapture the anomalies.\n2.2\nGraph Contrastive Leaning\nRecently, contrastive learning as an effective mean of self-\nsupervised learning [Liu et al., 2021a], has gained rising re-\nsearch interest, which aims to learn representations from su-\npervised signals derived from the data itself. The core idea\nis that it considers two augmented views of the same data in-\nstance as positive sampler to be pulled closer, and all other in-\nstances are considered as the negative samplers to be pushed\nfarther apart.\nDue to the great success of self-supervised\nlearning on images [Chen et al., 2020], contrastive learn-\ning methods are also adopted to various graph learning tasks.\n[Velickovic et al., 2019] aimed to learn the node represen-\ntations by maximizing the mutual information between the\npatch representation and the corresponding high-level graph\nsummary. [Peng et al., 2020] directly maximized the mu-\ntual information between the input and output of the graph\nneural encoder according to the node features and topology\nto improve the DGI. [Zhu et al., 2020] proposed to generate\ntwo data views and then pull the representation of the same\nnode in the two views closer, push the representation of all\nother nodes apart. [Zhu et al., 2021] proposed a contrastive\nframework for unsupervised graph representation learning\nwith adaptive data augmentation.\n3\nProblem Formulation\nIn this paper, for the convenience of presentation, the calli-\ngraphic fonts (e.g. G) represent sets, bold lowercase letters\nand bold uppercase letters represent vectors (e.g. x) and ma-\ntrices (e.g. X), respectively. The terminology and problem\nde\ufb01nitions are given as follows. Let G = (V, E, X) denote\nan attributed network , where V ={v1, ..., vN} represents the\nset of nodes, N is the number of the nodes in the graph, E\nis the set of edges, X \u2208RN\u00d7F is the attribute matrix, where\nxi \u2208RF (i = 1, ..., N) is the attributes for the ith node, and\nA\u2208RN\u00d7N is the adjacency matrix.\nProblem De\ufb01nition.\nGiven an attributed network G\n=\n(V, E, X) whose adjacency matrix is A, we aim to learn an\n(a) Graph diffusion & Subgraph sampling\n(b) Contrastive learning & Attribute reconstruction \nInter-View Contrastive\nt\n0\n1\n0\n1\nReconstruct\n\ud835\udc7a\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc90\ud835\udc8f\n\ud835\udfcf\nGNN\nweight\n\ud835\udc7a\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc84\ud835\udc90\ud835\udc8f\n\ud835\udfd0\n\ud835\udc7a\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc94\n\ud835\udfcf\n\ud835\udc7a\ud835\udc84\ud835\udc90\ud835\udc93\ud835\udc86\ud835\udc93\ud835\udc86\ud835\udc94\n\ud835\udfd0\nIntra-View\n.\n..\nPositive\nNegative\n.\n..\nPositive\nNegative\nGNN Layer\nAvePool\n\u2211\nAvePool\n\u2211\nReconstruct\nt\nGNN\nweight\nIntra-View\n\ud835\udc6e\ud835\udc8d\ud835\udc90\ud835\udc84\ud835\udc82\ud835\udc8d= (\ud835\udc4b, \ud835\udc34)\nGraph diffusion\n\ud835\udc6e\ud835\udc88\ud835\udc8d\ud835\udc90\ud835\udc83\ud835\udc82\ud835\udc8d= (\ud835\udc4b, \ud835\udc46)\nPositive\nNegative\nSubgraph\nsampling\nt\nt\nt\nt\nPositive\nNegative\nSubgraph\nsampling\nFigure 1: The framework of the proposed model.\nanomaly score function score(\u00b7) to measure the degree of the\nabnormality for each node in G.\n4\nMethodology\nAs shown in Figure.1, the proposed Sub-CR contains\na\ncontrastive\nlearning-based\nmodule\nand\nan\nattribute\nreconstruction-based module. We \ufb01rst construct two types of\ndata augmentations: the subgraph sampling (left upper part\nin Figure.1) and the graph diffusion (left lower part in Fig-\nure.1), to get local and global view subgraphs for each node.\nFor each view, both the subgraph and the target node are in-\nput into a GNN encoder to learn the embedding as shown\nin the middle part in Figure.1. Then we establish the intra-\nand inter-view contrastive learning (right part in Figure.1) to\nencode local and global structure information. Meanwhile,\nwe further introduce the masked autoencoder to identify the\nanomalous nodes through the attribute reconstruction errors\nin two views. Finally, the two complementary modules are\nintegrated to give a \ufb01nal anomaly score.\n4.1\nMulti-View Graph Generation with Graph\nDiffusion and Subgraph Sampling\nThe effectiveness of the contrastive learning paradigm largely\nrelies on the choice of the pretext task and the data augmenta-\ntions [Liu et al., 2021a]. We adopt the node-subgraph match-\ning pattern based pretext task which is proved useful for the\nanomaly detection on attributed networks [Liu et al., 2021b].\nMeanwhile, the graph diffusion augmentation [Hassani and\nKhasahmadi, 2020] is introduced to get the global view of\nthe structure, which allows each node to match with its local-\nand global-view subgraphs respectively. Next, we introduce\nhow to generate the two views of graph.\nGraph Diffusion.\nWe propose to use the graph diffusion as\nthe \ufb01rst augmentation to obtain the global view graph. Graph\ndiffusion can effectively capture the global structure informa-\ntion [Hassani and Khasahmadi, 2020; Klicpera et al., 2019].\nThis process is formulated as follows:\nS =\n\u221e\nX\nk=0\n\u03b8kTk \u2208RN\u00d7N\n(1)\nwhere \u03b8k is the weighting coef\ufb01cient to control the propor-\ntion of local and global structure information and T \u2208RN\u00d7N\ndenotes the generalized transition matrix to transfer the adja-\ncency matrix. Note that \u03b8k \u2208[0, 1] and P\u221e\nk=0 \u03b8k = 1. In\nthis paper, Personalized PageRank (PPR) [Page et al., 1999]\nis adopted to power the graph diffusion. Given the adjacency\nmatrix A \u2208RN\u00d7N, the identity matrix I and its degree matrix\nD, the transition matrix and the weight can be formulated re-\nspectively as T = D\u22121/2AD\u22121/2 and \u03b8k = \u03b1(1 \u2212\u03b1)k. Then\nthe graph diffusion S can be reformulated as:\nS = \u03b1(I \u2212(1 \u2212\u03b1)D\u22121/2AD\u22121/2)\u22121\n(2)\nwhere \u03b1 denotes the teleport probability in a random walk.\nThe graph diffusion S is the global view of the initial graph.\nSubgraph Sampling.\nWe adopt random walk with restart\n(RWR) [Tong et al., 2006] to sample the local subgraph. In\nboth views, a subgraph with the size P is sampled for each\nnode. In particular, we sample nodes and their edges from the\noriginal view to form the local-view subgraph, and then select\nthe exact nodes and edges from the other view to form the\nglobal-view subgraph. In each view, for node vi, its subgraph\nis regarded as its positive pair, and a subgraph corresponding\nto the other node is regarded as the negative pair.\n4.2\nLocal & Global Graph Contrastive Learning\nAs mentioned before, the representation of a normal node is\nsimilar to that of the neighboring nodes in its subgraph while\nthis does not hold for an abnormal node. Therefore, we de\ufb01ne\nthe intra-view contrastive learning in the local and the global\nviews, which maximizes the agreement between the node and\nthe corresponding subgraph-level representations. As shown\nin the right part of Figure.1, for node vi, the discriminative\nscore of its positive pair should be close to 1, while the nega-\ntive one should be close to 0. Meanwhile, the inter-view con-\ntrastive learning is de\ufb01ned between the two views. It aims\nto encourage the discriminative score that the same node be-\ntween its subgraph in two views to be close.\nIntra-View Contrastive Learning.\nThis contrastiveness is\nde\ufb01ned in each view. Taking the local view in Figure.1 as an\nexample, we obtain the corresponding positive and negative\nsubgraphs Gi, \u02dcGi for the target node vi. Note that in order\nto make the obtained representation more discriminative, the\nattributes of the target node in the subgraph are masked. Then\nthe subgraph are fed into a shared GCN encoder to learn a\nlow-dimensional representation, which can be formulated as\nfollows:\nHl\ni = \u03c6(\u02c6D\n\u22121/2\ni\nA\n\u2032\ni \u02c6D\n\u22121/2\ni\nH(l\u22121)\ni\nW(l\u22121))\n(3)\nwhere \u02c6Di \u2208RN\u00d7N is the degree matrix of A\n\u2032\ni = Ai + IN,\nA\n\u2032\ni is the subgraph adjacency matrix with self-loop, IN is the\nidentity matrix and W0 \u2208RF \u00d7d is a learnable weight matrix.\nSince the target node is masked in the subgraph, we employ\nthe weight matrix of GCN to map the features into the same\nembedding space. It can be formulated as:\nhl\ni = \u03c6(hl\u22121\ni\nWl\u22121)\n(4)\nwhere h0\ni = xi, \u03c6 is the activation function such as ReLU.\nSo we obtain the representations Hi and \u02dc\nHi for the two-view\nsubgraphs Gi, \u02dcGi, hi for the target node vi. In order to ap-\nply the node-subgraph matching pattern pretext task (the rep-\nresentation of node and its corresponding subgraph is more\nconsistent), we take the average pooling function as the read-\nout module to obtain the subgraph-level embedding vector ei:\nei = Readout(Hi) =\nni\nX\nk=1\n(Hi)k\nni\n(5)\nwhere ni is the number of nodes in the subgraph Gi. So the\ndiscriminative scores for the positive pair can be de\ufb01ned as:\nsi = \u03c3(hiWseT\ni )\n(6)\nwhere Ws is a learnable matrix. Similarly, we can calculate\n\u02dcsi for the nagetive pair. Hence, the local view contrastive loss\nfor the instance pair (vi, Gi, \u02dcGi) can be formulated as:\nL1\nintra(vi) = \u22121\n2(log(si) + log(1 \u2212\u02dcsi))\n(7)\nBesides, we can similarly calculate L2\nintra for the global view.\nFinally, by combining the two losses, we have the intra-view\ncontrastive learning objective function de\ufb01ned below:\nLintra =\n1\n2N\nN\nX\ni=1\n(L1\nintra(vi) + L2\nintra(vi))\n(8)\nInter-View Contrastive Learning.\nThis contrastiveness is\nde\ufb01ned between the two views. Following the core idea in\ncontrastive learning, we make the discriminative scores from\nnode-subgraph pair of the two views closer. So the inter-view\ncontrastive loss between the local- and global-view can be\nformulated as:\nLinter = (||s1 \u2212s2||2\nF )\n(9)\nwhere s1, s2 are the vectors that consist of positive pair dis-\ncriminative scores in the two views. By combining the intra-\nand the inter-view contrastive learning, the overall loss func-\ntion of the multi-view contrastive learning module is:\nLcon = Linter + Lintra\n(10)\n4.3\nAttribute Reconstruction Based on Neighbors\nThe self-supervised learning method based on masked au-\ntoencoders (MAE) has been proved to be effective in many\n\ufb01elds. For example, the BERT [Devlin et al., 2018]-based\npre-training model has achieved remarkable results in many\nNLP tasks, and similar works are also proposed in CV [He et\nal., 2021]. The basic idea of these methods is to mask a part\nof a sentence or a picture and then input the unmasked part\ninto the encoder and decoder for predicting the masked part.\nMotivated by the masked autoencoder method, we propose\nto design an attribute reconstruction based on neighbors mod-\nule to boost the performance. We adopt an asymmetric de-\nsign. The encoder operates on the nodes in the subgraph that\nare not masked. The latent representations of these nodes are\nconcatenated as the input of the lightweight decoder to re-\nconstruct the masked node\u2019s raw attributes. Taking the local\nview as an example, for the subgraph Gi of node vi, we can\nget the representation of Gi through the GCN encoder. The\nreconstruction loss of the local view can be de\ufb01ned as:\nL1\nres(vi) = ||g(Zi) \u2212xi||2\n(11)\nwhere g(\u00b7) is the multilayer perceptron, Zi is the concatena-\ntion of neighboring nodes\u2019 representations in the subgraph.\nSimilarly, we can calculate L2\nres for the global view. The\noverall loss function of this module can be formulated as:\nLres =\n1\n2N\nN\nX\ni=1\n(L1\nres(vi) + L2\nres(vi))\n(12)\n4.4\nAnomaly Score Inference\nTo jointly train the contrastive learning module and the at-\ntribute reconstruction module, we optimize the following ob-\njective function:\nL = Lcon + \u03b3Lres\n(13)\nwhere \u03b3 is a controlling parameter which balances the impor-\ntance of the two modules.\nBy minimizing the above objective function, we can com-\npute the anomaly score of each node. Inspired by [Liu et al.,\n2021b], taking the node vi as an example, the anomaly score\nof the contrastive-based module can be calculated by:\nscorecon(vi) = 1\n2[score1\ncon(vi) + score2\ncon(vi)]\n(14)\nwhere score1\ncon(vi) = s1\u2212\ni\n\u2212s1+\ni , score2\ncon(vi) = s2\u2212\ni\n\u2212s2+\ni ,\ns1+\ni (s2+\ni ) and s1\u2212\ni\n(s2\u2212\ni ) are the local (global) view discrim-\ninantive scores of positive and negative pairs by Eq.(6). The\nanomaly score from the reconstruction-based module is:\nscoreres(vi) = 1\n2\n2\nX\nk=1\nscorek\nres(vi)\n(15)\nDatasets\nNodes\nEdges\nFeatures\nAnomalies\nBlogCatalog\n5196\n171743\n8189\n300\nFlickr\n7575\n239738\n12407\n450\nCora\n2708\n5429\n1433\n150\nCiteSeer\n3327\n4732\n3703\n150\nPubmed\n19717\n44338\n500\n600\nTable 1: The statistics of the datasets.\nscorek\nres(vi) = ||g(Zk\ni ) \u2212xi||2\n2, k = 1, 2\n(16)\nwhere Z1\ni and Z2\ni are the concatenated neighboring node rep-\nresentations in local and global subgraphs for node vi, re-\nspectively. The two scores are \ufb01rst normalized, and then in-\ntegrated by the following formula:\nscore(vi) = scorecon(vi) + \u03b3scoreres(vi)\n(17)\n5\nExperiments\n5.1\nDatasets\nWe evaluate the proposed model on \ufb01ve benchmark datasets\nthat are widely used in anomaly detection on attributed\nnetworks [Liu et al., 2021b; Zheng et al., 2021].\nThese\ndatasets include two social network datasets BlogCatalog and\nFlickr and three citation network datasets Cora, Citeseer, and\nPubmed. Due to the shortage of ground truth anomalies, the\nanomalies are injected by the perturbation scheme [Ding et\nal., 2019]. The statistics of the datasets is shown in Table 1.\n5.2\nExperimental Settings\nBaselines and evaluation metrics.\nIn the experiments, we\ncompare the proposed model with seven baselines. [Perozzi\nand Akoglu, 2016] evaluates the attribute correlation of node\nto detect anomalies. Speci\ufb01cally, it analyzes the abnormal-\nity of each node from the ego-network point of view. [Li\net al., 2017] is a residual analysis-based method. It detects\nanomalies whose behaviors are singularly different from the\nmajority [Peng et al., 2018] proposes a novel joint frame-\nwork to conduct attribute selection and anomaly detection.\n[Ding et al., 2019] conducts the investigation on the problem\nof anomaly detection on attributed networks. [Velickovic et\nal., 2019] is an approach for learning node representations.\nA bilinear function is provied to score abnormality. [Liu et\nal., 2021b] is the \ufb01rst contrastive learning based method for\ngraph anomaly detection. [Jin et al., 2021] is a recent method\nwhich performes patch- and context-level contrastive learning\nfor anomaly detection on attributed networks. We use ROC\nand AUC to measure the performance.\nImplementation Details.\nIn Sub-CR, we set the node size\nP of subgraph to 4 for all datasets by considering both the\nef\ufb01ciency and performance. Following [Liu et al., 2021b], we\nemploy a one-layer GCN as the encoder, and the embedding\ndimension is set to 64. The model is optimized with the Adam\noptimizer during training. The bacth size is set to 300 for all\ndatasets. The learning rate is set to 0.001 for Cora, Citeseer,\nPubmed and Flickr, set to 0.003 for BlogCatalog. We train\nthe model 400 epochs for BlogCatalog and Flickr, and 100\nFigure 2: ROC curves on four benchmark datasets.\nepochs for Cora, Citeseer and Pubmed. The parameter \u03b3 is\nset to 0.6 for BlogCatalog, Flickr, Cora and Citeseer and 0.4\nfor Pubmed. In the inference phase, we average the anomaly\nscores in Eq. (17) over 300 rounds to get the \ufb01nal anomaly\nscore for each node.\n5.3\nResult and Analysis\nTable 2 shows the comparison result of these methods in\nterms of AUC value. Due to space limitation, Figure.2 shows\nthe ROC curves on Cora, BlogCatalog, Citeseer, and Flickr\nwith two competitive baselines. As shown in the table, one\ncan \ufb01rst see that our proposed model is superior to its coun-\nterparts on \ufb01ve benchmark datasets, which demonstrates the\neffectiveness of Sub-CR. To be speci\ufb01c, Sub-CR improves\nthe performance by 3.38% and 1.61% over the best base-\nline ANEMONE on Flickr and Pubmed datasets, respectively.\nIn details, the shallow methods including AMEN, Radar and\nANOMALOUS perform worse than other models due to their\nless effectiveness in modeling the high-dimension node at-\ntributes and complex tological structures. The DOMINANT\nand DGI also do not show the competitive performance be-\ncause they focus on modeling the whole graph structure in-\nstead of directly explore the abnormal patterns. DGI takes\nthe node vs full-graph intstance pair contrastive, and DOM-\nINANT aims to reconstruct the whole graph structure or at-\ntributes for each node, which do not decouple local and global\ninformation for anomaly detection. CoLA and ANEMONE\nachieve the suboptimal performance due to the considera-\ntion of sorrounding substructures.\nBut they cannot fully\ncatch the high-order structure information and ignore the self-\nsupervised signal of the original attribute reconstruction er-\nror. The superior performence of Sub-CR veri\ufb01es the effec-\ntiveness of intergating the multi-view contrastive-based and\nreconstruction-based modules, which can decouple local and\nglobal information to better capture the attribution and the\ntopological structure of nodes for anomaly detection.\n5.4\nAblation Study\nWe next conduct an ablation study to verify the effective-\nness of each component in Sub-CR. Sub-R and Sub-C de-\nnote the model without reconstruction-based and contrastive-\nbased module, respectively.\nThe variants Sub-weight and\nSub-global are de\ufb01ned as that we remove the balance weight (\n\u03b3 is set to 1) and the global view, respectively. The results are\npresented in Table 3. One can see that Sub-CR outperforms\nall the variants consistently on all the datasets, which demon-\nstrates the components are all helpful to the studied task.\nEspecially, Sub-CR outperforms Sub-C by 6.81%, 5.41%,\nMethods\nBlogcatalog\nFlickr\nCora\nCiteseer\nPubmed\nAMEN[2016]\n0.6392\n0.6573\n0.6266\n0.6154\n0.7713\nRadar[2017]\n0.7401\n0.7399\n0.6587\n0.6709\n0.6233\nANOMALOUS [2018]\n0.7237\n0.7434\n0.5770\n0.6307\n0.7316\nDOMINANT[2019]\n0.7468\n0.7442\n0.8155\n0.8251\n0.8081\nDGI [2019]\n0.5827\n0.6237\n0.7511\n0.8293\n0.6962\nCoLA[2021]\n0.7854\n0.7513\n0.8779\n0.8968\n0.9512\nANEMONE [2021]\n0.8067\n0.7637\n0.9057\n0.9189\n0.9548\nSub-CR\n0.8141\n0.7975\n0.9132\n0.9303\n0.9709\nTable 2: The AUC values comparison on \ufb01ve benchmark datasets.\nBlogCatalog\nFlickr\nCora\nCiteSeer\nPubmed\nSub-R\n0.7943\n0.7609\n0.9002\n0.9017\n0.9553\nSub-C\n0.7460\n0.7434\n0.8220\n0.7892\n0.8006\nSub-weight\n0.8083\n0.7928\n0.9041\n0.9275\n0.9491\nSub-global\n0.8090\n0.7923\n0.8975\n0.9195\n0.9625\nTable 3: The AUC values of ablation study.\n75\n80\n85\n90\n95\n100\n0\n0.2\n0.4\n0.6\n0.8\nAUC\uff08%\uff09\nFlickr\nBlogCatalog\nPubMed\nCora\nCiteseer\nFigure 3: Performance with different \u03b3.\n9.12%, 14,11%, 17.03% on \ufb01ve datasets, respectively, which\ndemonstrates the contrastive-based module is crucially im-\nportant to the model. The self-supervised signal of the match-\ning pattern on the node-subgraph pair is more effective for\ncapturing anomalies. As for the remaining three components,\nSub-R performs worse than Sub-weight and Sub-global on\nBlogCatalog, Flickr, and CiteSeer datasets. On the Cora, the\ncomponent of the global view is more important. Sub-weight\nhas a more signi\ufb01cant effect on Pubmed. The reason may be\nthat the effect of different modules on different datasets varies\ndue to the different data characteristics.\n5.5\nParameter Study\nWe \ufb01nally conduct the model sensitivity analysis on critical\nhyper-parameters in Sub-CR, which are the size P of sub-\ngraph, the embedding dimension in GCN and the balance fac-\ntor \u03b3. Figure.3 shows the effect of different \u03b3 values on the\nmodel performance. One can see that the most datasets can\nachieve the best result and not sensitive to the balance factor\nwhen \u03b3 \u22650.4. Figure.4 shows the AUC values under dif-\nferent subgraph sizes and embedding dimensions. One can\nobserve that the best performance for Flickr and Citeseer is\n75\n80\n85\n90\n95\n100\n2\n3\n4\n5\n6\nAUC\uff08%\uff09\nSubgraph size \nBlogCatalog\nCiteSeer\nPubMed\nFlickr\nCora\n75\n80\n85\n90\n95\n100\n16\n32\n64\n128\n256\nAUC\uff08%\uff09\nEmbedding dimension \nBlogCatalog\nCiteSeer\nPubMed\nFlickr\nCora\nFigure 4: Performance with different subgraph sizes and embedding\ndimensions.\nachieved at P = 2, and for BlogCatalog, Cora and PubMeb\nit is achieved at P = 4. When the size is too large, the sub-\ngraph will contain redundant information, which will hurt the\nperformance. A suitable embedding dimension is d = 64 for\nmost datasets as shown in the \ufb01gure. A too small or large em-\nbedding dimension will both degrade the model performance.\n6\nConclusion\nIn this paper, we propose a novel multi-view self-supervised\nlearning framework for anomaly detection on attributed net-\nworks. The proposed contrastive learning-based module con-\nsists of two carefully designed contrastive views to better cap-\nture local and global structure information related to anomaly\npatterns. The attribute reconstruction module adopts the rep-\nresentation of those neighbors based on the subgraph to re-\nbuild the raw attributes of the target node of the two views.\nFinally, the two complementary modules are integrated for\nmore effective anomaly detection.\nExtensive experiments\nconducted over \ufb01ve benchmark datasets demonstrate the ef-\nfectiveness of our proposed model.\nAcknowledgements\nThis work was supported by NSFC under granted No.\n62076124, partially \ufb01nancially supported by the National Sci-\nence and Technology Major Project (J2019-IV-0018-0086),\nNSFC (No. 62172443), and the Fundamental Research Funds\nfor the Central Universities (No. NZ2020014).\nReferences\n[Bo et al., 2021] Deyu Bo, Xiao Wang, Chuan Shi, and\nHuawei Shen. Beyond low-frequency information in graph\nconvolutional networks. In AAAI, 2021.\n[Chen et al., 2020] Ting Chen, Simon Kornblith, Moham-\nmad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML,\n2020.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova.\nBert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Ding et al., 2019] Kaize\nDing,\nJundong\nLi,\nRohit\nBhanushali, and Huan Liu.\nDeep anomaly detection\non attributed networks. In SDM, 2019.\n[Ding et al., 2021] Kaize Ding, Jundong Li, Nitin Agarwal,\nand Huan Liu. Inductive anomaly detection on attributed\nnetworks. In IJCAI, 2021.\n[Erfani et al., 2016] Sarah\nM\nErfani,\nSutharshan\nRa-\njasegarar, Shanika Karunasekera, and Christopher Leckie.\nHigh-dimensional and large-scale anomaly detection\nusing a linear one-class svm with deep learning. Pattern\nRecognition, 58:121\u2013134, 2016.\n[Hassani and Khasahmadi, 2020] Kaveh\nHassani\nand\nAmir Hosein Khasahmadi.\nContrastive multi-view\nrepresentation learning on graphs. In ICML, 2020.\n[He et al., 2021] Kaiming He, Xinlei Chen, Saining Xie,\nYanghao Li, Piotr Doll\u00b4ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. arXiv preprint\narXiv:2111.06377, 2021.\n[Jain et al., 2019] Gauri Jain, Manisha Sharma, and Basant\nAgarwal. Spam detection in social media using convolu-\ntional and long short term memory neural network. An-\nnals of Mathematics and Arti\ufb01cial Intelligence, 85(1):21\u2013\n44, 2019.\n[Jin et al., 2021] Ming Jin, Yixin Liu, Yu Zheng, Lianhua\nChi, Yuan-Fang Li, and Shirui Pan.\nAnemone: Graph\nanomaly detection with multi-scale contrastive learning. In\nCIKM, 2021.\n[Klicpera et al., 2019] Johannes Klicpera, Stefan Wei\u00dfen-\nberger, and Stephan G\u00a8unnemann.\nDiffusion improves\ngraph learning. arXiv preprint arXiv:1911.05485, 2019.\n[Li et al., 2017] Jundong Li, Harsh Dani, Xia Hu, and Huan\nLiu. Radar: Residual analysis for anomaly detection in\nattributed networks. In IJCAI, pages 2152\u20132158, 2017.\n[Li et al., 2019] Yuening Li,\nXiao Huang,\nJundong Li,\nMengnan Du, and Na Zou. Specae: Spectral autoencoder\nfor anomaly detection in attributed networks. In CIKM,\n2019.\n[Liu et al., 2021a] Xiao Liu, Fanjin Zhang, Zhenyu Hou,\nLi Mian, Zhaoyu Wang, Jing Zhang, and Jie Tang. Self-\nsupervised learning:\nGenerative or contrastive.\nIEEE\nTransactions on Knowledge and Data Engineering, 2021.\n[Liu et al., 2021b] Yixin Liu, Zhao Li, Shirui Pan, Chen\nGong, Chuan Zhou, and George Karypis. Anomaly detec-\ntion on attributed networks via contrastive self-supervised\nlearning.\nIEEE Transactions on Neural Networks and\nLearning Systems, 2021.\n[Page et al., 1999] Lawrence Page, Sergey Brin, Rajeev\nMotwani, and Terry Winograd.\nThe pagerank citation\nranking: Bringing order to the web. Technical report, Stan-\nford InfoLab, 1999.\n[Peng et al., 2018] Zhen Peng, Minnan Luo, Jundong Li,\nHuan Liu, and Qinghua Zheng. Anomalous: A joint mod-\neling approach for anomaly detection on attributed net-\nworks. In IJCAI, 2018.\n[Peng et al., 2020] Zhen Peng, Wenbing Huang, Minnan\nLuo, Qinghua Zheng, Yu Rong, Tingyang Xu, and Jun-\nzhou Huang. Graph representation learning via graphical\nmutual information maximization. In WWW, 2020.\n[Perozzi and Akoglu, 2016] Bryan\nPerozzi\nand\nLeman\nAkoglu. Scalable anomaly ranking of attributed neighbor-\nhoods. In SDM, 2016.\n[Shone et al., 2018] Nathan Shone,\nTran Nguyen Ngoc,\nVu Dinh Phai, and Qi Shi.\nA deep learning approach\nto network intrusion detection.\nIEEE transactions on\nemerging topics in computational intelligence, 2(1):41\u2013\n50, 2018.\n[Tong et al., 2006] Hanghang Tong, Christos Faloutsos, and\nJia-Yu Pan. Fast random walk with restart and its applica-\ntions. In ICDM, 2006.\n[Velickovic et al., 2019] Petar Velickovic, William Fedus,\nWilliam L Hamilton, Pietro Li`o, Yoshua Bengio, and\nR Devon Hjelm. Deep graph infomax. In ICLR, 2019.\n[Wang et al., 2019] Daixin Wang, Jianbin Lin, Peng Cui,\nQuanhui Jia, Zhen Wang, Yanming Fang, Quan Yu, Jun\nZhou, Shuang Yang, and Yuan Qi.\nA semi-supervised\ngraph attentive network for \ufb01nancial fraud detection. In\nICDM, 2019.\n[Yu et al., 2018] Wenchao Yu, Wei Cheng, Charu C Aggar-\nwal, Kai Zhang, Haifeng Chen, and Wei Wang. Netwalk:\nA \ufb02exible deep embedding approach for anomaly detec-\ntion in dynamic networks. In KDD, 2018.\n[Zheng et al., 2021] Yu Zheng, Ming Jin, Yixin Liu, Lianhua\nChi, Khoa T Phan, and Yi-Ping Phoebe Chen. Generative\nand contrastive self-supervised learning for graph anomaly\ndetection. IEEE Transactions on Knowledge and Data En-\ngineering, 2021.\n[Zhu et al., 2020] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang\nLiu, Shu Wu, and Liang Wang. Deep graph contrastive\nrepresentation learning. arXiv preprint arXiv:2006.04131,\n2020.\n[Zhu et al., 2021] Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang\nLiu, Shu Wu, and Liang Wang. Graph contrastive learning\nwith adaptive augmentation. In WWW, 2021.\n",
    "2403.10339": "Generation is better than Modification: Combating High Class\nHomophily Variance in Graph Anomaly Detection\nRui Zhang1, Dawei Cheng1,2,\u2217, Xin Liu1, Jie Yang1, Yi Ouyang3, Xian Wu3, Yefeng Zheng3\n1Department of Computer Science and Technology, Tongji University, Shanghai, China\n2Shanghai Artificial Intelligence Laboratory, Shanghai, China\n3Jarvis Research Center, Tencent YouTu Lab, Shenzhen, China\n{2050271,dcheng,2051277,2153814}@tongji.edu.cn, {yiouyang,kevinxwu,yefengzheng}@tencent.com\nABSTRACT\nGraph-based anomaly detection is currently an important research\ntopic in the field of graph neural networks (GNNs). We find that in\ngraph anomaly detection, the homophily distribution differences\nbetween different classes are significantly greater than those in ho-\nmophilic and heterophilic graphs. For the first time, we introduce\na new metric called Class Homophily Variance, which quan-\ntitatively describes this phenomenon. To mitigate its impact, we\npropose a novel GNN model named Homophily Edge Generation\nGraph Neural Network (HedGe). Previous works typically focused\non pruning, selecting or connecting on original relationships, and\nwe refer to these methods as modifications. Different from these\nworks, our method emphasizes generating new relationships with\nlow class homophily variance, using the original relationships as\nan auxiliary. HedGe samples homophily adjacency matrices from\nscratch using a self-attention mechanism, and leverages nodes that\nare relevant in the feature space but not directly connected in the\noriginal graph. Additionally, we modify the loss function to punish\nthe generation of unnecessary heterophilic edges by the model. Ex-\ntensive comparison experiments demonstrate that HedGe achieved\nthe best performance across multiple benchmark datasets, including\nanomaly detection and edgeless node classification. The proposed\nmodel also improves the robustness under the novel Heterophily\nAttack with increased class homophily variance on other graph\nclassification tasks.\nCCS CONCEPTS\n\u2022 Information systems \u2192Data mining.\n1\nINTRODUCTION\nIn graph anomaly detection (GAD), anomalous nodes refer to those\nin a network whose behavior or attributes significantly differ from\nmost other nodes [6]. GAD is important in fields like financial\nfraud detection [11, 15], cybersecurity [52], social network analysis\n[60], loan risk assessment [10, 42] and industrial system moni-\ntoring [7], and has drawn great research interests. Graph Neural\nNetworks (GNNs), with their effective handling and analysis of\ngraph-structured data, are particularly suitable for GAD. Their abil-\nities to aggregate information from neighborhoods help effectively\ndetect anomaly nodes [38].\nIn the field of GAD, a significant amount of research has achieved\nnotable results. Many studies selectively aggregate neighbor fea-\ntures and utilize supervised learning or feature similarity to dif-\nferentiate between various neighbor pairs [17, 35]. Additionally,\n\u2217Corresponding author.\nmodels based on spectral architecture, which leverage the charac-\nteristics of low-pass and high-pass filters, have effectively handled\ndifferent structures and features, providing a new perspective for\nGAD [5, 50]. Moreover, a series of studies have significantly boosted\nthe efficiency of the learning process by modifying loss functions,\nthereby amplifying the effectiveness of anomaly detection [62, 63].\nThese research efforts have played a crucial role in improving the\ncapability to identify anomalies.\nHowever, these methods have not fully recognized the funda-\nmental differences between anomaly detection and other scenarios.\nWe first define homophily as the high probability of a node being\nconnected to other nodes with the same label, whereas heterophily\nis the opposite [65]. Similarly, a homophily (heterophily) edge is\nthe one connecting two nodes with the same (different) label(s). As\nshown in Figure 1, in the context of GAD, the weighted homophily\ndistribution, where every class has an equal contribution, exhibits a\ndistinct bimodal characteristic. This suggests a significant disparity\nin the level of homophily among various nodes, a characteristic ab-\nsent in both homophilic and heterophilic graphs. For simplicity, we\nrefer to these graphs as generic graphs. We have observed that cur-\nrently, there is no metric to describe this distribution discrepancy.\nTherefore, in order to quantify the severity of this phenomenon,\nwe introduced a new metric, Class Homophily Variance (CHV).\nIt is designed to describe the degree of difference in the homophily\ndistribution. In the following text, we analyze and discover that the\nvalue of this metric is significantly larger than others in anomaly\ndetection scenarios. And we think that is why vanilla GNN models\nand GNNs designed for solving heterophily tasks [13, 43, 59] are\nstruggling to identify anomalies due to their inability to effectively\nhandle this unique variance in homophily distribution.\nCurrently, some methods try to exploit the distribution of ho-\nmophily in GAD but problems remain. For example, H2-FDetector\n[48] opts for different aggregation relations for homophilic and\nheterophilic edges to aid in anomaly detection. GHRN [18] utilizes\nhigh-pass filters to measure the degree of one-hop label change in\nthe central node and uses node predictions to selectively remove\nheterophilic edges. GDN [19] attempts to identify the anomaly pat-\ntern to reduce the impact of heterophilic neighbors. Conversely,\nSparseGAD [20] employs multilayer perceptron (MLP) combined\nwith feature similarity for pruning and connecting nodes. However,\nissues still persist. Firstly, these methods are based on irreversible\noperations such as pruning, selection, and connection, which we\nrefer to as modifications. These irreversible methods can disrupt\nthe structural information of the graph to some extent, leading to\nperformance declines in certain scenarios. Secondly, the vast distri-\nbution differences in original relationships render methods based\narXiv:2403.10339v1  [cs.LG]  15 Mar 2024\n(a) Anomaly detection datasets\n(b) Homophilic graphs\n(c) Heterophilic graphs\nFigure 1: Weighted homophily density distribution on different datasets. We use curves to fit the distribution for clarity.\non modifications less effective. These challenges inspire us to ask\nthe question: can a model autonomously generate its relationships,\nwith the original relations serving only as an auxiliary?\nIn order to answer this question, we propose a novel model\nnamed Homophily edge Generation Graph Neural Network, ab-\nbreviated as HedGe. The core of this model lies in generating ho-\nmophilic edges rather than modification, to handle the problem\nof excessive differences in homophily distribution. It can effec-\ntively utilize latent neighborhood relationships in the feature space.\nFirstly, HedGe applies position encoding to each node, providing\nadditional contextual information for subsequent attention mecha-\nnisms. The model then calculates attention relationships between\npairs of nodes, obtaining attention coefficient. Next, HedGe employs\na novel Edge Specific Gumbel Softmax mechanism to sample and\ngenerate homophilic adjacency matrices. This process is differen-\ntiable, ensuring the optimization efficiency of the model. The newly\ngenerated graphs and the original graph are then fed into GNNs for\nmessage passing. Simultaneously, the attention weights are used\nfor weighted aggregation to integrate information from all nodes\nand several relationships are merged. Finally, to further enhance\nthe model\u2019s performance, we modify the loss function to suppress\nthe generation of heterophilic edges by the attention mechanism,\nensuring the model focuses on extracting homophilic features.\nTo further validate the effectiveness and universality of our\nmodel, we design a special Heterophily Attack method to increase\nthe CHV of the graph, thereby increasing the graph classification\ndifficulty, to demonstrate the robustness of the proposed model\non other tasks. Moreover, we also conducted experiments on sev-\neral anomaly detection datasets, comparing the performance of the\nHedGe model against other baselines. The edge-generating capabil-\nity of HedGe also makes edgeless classification possible on GNN. In\nsummary, our contributions are as follows:\n\u2022 We quantitatively describe the homophily distribution dif-\nferences in GAD and theoretically discuss the impact of it.\nWe also develop a novel Heterophily Attack to simulate high\nCHV scenarios in generic datasets.\n\u2022 We design an innovative model that generates homophilic\nedges from scratch through attention mechanism and sam-\npling methods. This mitigates the homophily distribution\ndifferences and also utilizes potential nodes.\n\u2022 We conduct extensive experiments on multiple benchmark\ndatasets to demonstrate the effectiveness of our method and\nachieve the best performance in scenarios of graph anomaly\ndetection, simulation, and edgeless node classification.\n2\nANALYSIS AND PRELIMINARIES\nIn this section, we first elucidate the concept of Class Homophily\nVariance and measure it on various datasets, followed by a theoret-\nical analysis. Finally, we formulate our problem.\n2.1\nClass Homophily Variance\nWe introduce a new metric, Class Homophily Variance (CHV), to\ndescribe the variance in homophily distribution of a graph. This\nmetric quantifies the homophily differences in classes among differ-\nent nodes in the network. Specifically, CHV calculates the variance\nof node homophily across different classes, reflecting the degree\nof consistency in label distribution among different nodes in the\nnetwork.\nWe define H (\ud835\udc63) =\n|{\ud835\udc62\u2208N(\ud835\udc63):label(\ud835\udc62)=label(\ud835\udc63)}|\n|N(\ud835\udc63) |\nas a node\u2019s ho-\nmophily value, where the N (\ud835\udc63) is the neighborhood of the node\n\ud835\udc63and |N (\ud835\udc63)| is the cardinality of set N (\ud835\udc63). Next, we calculate the\naverage homophily for each class \ud835\udc36, \u00af\nH (\ud835\udc36) =\n\u00cd\n\ud835\udc63\u2208\ud835\udc49\ud835\udc36H(\ud835\udc63)\n|\ud835\udc49\ud835\udc36|\n, where\n\ud835\udc49\ud835\udc36is the set of node belonging to the class \ud835\udc36. The formal definition\nof Class Homophily Variance is as follows.\nDefinition 1 (Class Homophily Variance). Given a graph G\nwith \ud835\udc58classes, and defining S = {\ud835\udc361,\ud835\udc362, ...,\ud835\udc36\ud835\udc58} as the set of classes.\nLet the average inter-class homophily be \ud835\udf07=\n\u00cd\n\ud835\udc36\u2208S \u00af\nH(\ud835\udc36)\n|S|\n. Then, the\nClass Homophily Variance of graph G is defined as follows,\nVar( \u00af\nH)G =\n\u00cd\n\ud835\udc36\u2208S\n\u0000 \u00af\nH (\ud835\udc36) \u2212\ud835\udf07\u00012\n|S|\n.\n(1)\nMeanwhile, in order to judge the homophily difference in a\nsingle class, we design the in-class homophily variance, Var\ud835\udc36(H) =\n\u00cd\n\ud835\udc63\u2208\ud835\udc49\ud835\udc36(H(\ud835\udc63)\u2212\u00af\nH(\ud835\udc36))\n2\n|\ud835\udc49\ud835\udc36|\n. Finally, we calculate the weighed homophily\naverage, \ud835\udf07\ud835\udc64=\n\u00cd\n\ud835\udc56\ud835\udc64\ud835\udc56\ud835\udc63\ud835\udc56\n\u00cd\n\ud835\udc56\ud835\udc64\ud835\udc56, \ud835\udc64\ud835\udc56= 1\n\ud835\udc5d\ud835\udc56, where \ud835\udc5d\ud835\udc56is the class ratio of the\nnode \ud835\udc63\ud835\udc56. This metric is used to analyze the homophily average of a\ngraph when every class has an equal contribution.\n2.2\nData Analysis\nWe use our metrics to analyze several GAD datasets and compare\nthem with some generic graphs, with the results shown in Table 1\n(Please find more results in Appendix B).\nIn GAD datasets, it is clear that the CHV is significantly higher\nthan others. This is because in the GAD dataset, the homophily\nvalues of normal nodes are very close to 1, while the variance of\nhomophily values for anomalous nodes is close to 0, a phenomenon\nTable 1: Homophily analysis on different datasets.\nTypes\nDataset\nVar( \u00af\nH)G\nVar\ud835\udc36(H)\n\ud835\udf07\ud835\udc64\nAnomaly\nAmazon\n0.1655\n0.0082\n0.5579\nYelpChi\n0.1101\n0.0130\n0.5373\nHomophily\nPhoto\n0.0171\n0.0433\n0.8293\nHeterophily\nSquirrel\n0.0018\n0.0320\n0.2190\nthat has also been corroborated by previous works [18\u201320]. Our\nmetric quantitatively describes this phenomenon. At the same time,\nthe in-class homophily variance across all scenarios is relatively\nlow, suggesting that within each type of dataset, the homophily\ndistribution among classes is relatively balanced. Meanwhile, the\nweighted average in the GAD graphs tends to be neutral (close\nto 0.5), whereas in the homophily and heterophily graphs, the\nweighted average shows a clear bias. For instance, the weighted\naverage for the Photo dataset is close to 0.8293, indicating a strong\nhomophily tendency; while the value for the Squirrel dataset is\nonly 0.2190, showing a distinct heterophily.\nThese metrics highlight the stark differences in class distribu-\ntion of GAD datasets compared to other datasets. This discrepancy\naccounts for why models that perform well on homophilic or het-\nerophilic graphs fail to achieve similar results in graph anomaly\ndetection. Our model is designed to alleviate the problem.\n2.3\nTheoretical Analysis\nIn this part, we discuss the impact of inter-class homophily dif-\nferences and CHV on the aggregation effectiveness of GNNs. To\nsimplify the analysis, we use the binary classification problem in\ngraph anomaly detection as an example to study the impact of\nhomophily differences on the classification performance of GCN\n[27], thereby observing the effects on GNN models that utilize the\nhomophily principle [41].\nTo clarify the assumption, based on previous works [37, 39], we\nanalyze the Contextual Stochastic Block Model (CSBM) [16], which\nis often used to theoretically analyze the behavior of GNNs. We\npropose a variant of CSBM, CSBM for Class Homophily (CSBM-\nC). The graphs generated by CSBM-C contain two disjoint class\nof nodes, the two classes are respectively referred to as C0 and\nC1. For each node \ud835\udc56, its original embedding x\ud835\udc56\u223c\ud835\udc41(\ud835\udf41, I), where\n\ud835\udf41= \ud835\udf41\ud835\udc58\u2208R\ud835\udc59, \ud835\udc56\u2208C\ud835\udc58,\ud835\udc58\u2208{0, 1}, \ud835\udf410 \u2260\ud835\udf411, and \ud835\udc59is the dimension\nof the embedding. For the nodes in C0 and C1, their degrees are \ud835\udc51.\nSimultaneously, their neighbors are independently sampled. For\nnode \ud835\udc56, its neighbors comprise \u210e\u00b7 \ud835\udc51nodes with the same label\nand (1 \u2212\u210e) \u00b7 \ud835\udc51nodes with a different label, where \u210e\u2208[0, 1] ,\u210e=\n\u210e\ud835\udc58,\ud835\udc56\u2208C\ud835\udc58,\ud835\udc58\u2208{0, 1}. We denote the graph G generated by CSBM-\nC as G \u223cCSBM-C(\ud835\udf410, \ud835\udf411,\ud835\udc51,\u210e0,\u210e1). Simultaneously, we represent\na single GCN aggregation as h\ud835\udc56= 1\n\ud835\udc51\n\u00cd\n\ud835\udc57\u2208N(\ud835\udc56) x\ud835\udc57, where h\ud835\udc56is the\nrepresentation obtained after x\ud835\udc56is convolved through a GCN and\nh\ud835\udc56\u2208h. Because the feature matrix being multiplied can be absorbed\nby the subsequent linear classifier, we simplify it here. We analyze\nthe effects of variations in \u210e0 and \u210e1 on classification and arrive at\nthe following conclusion:\nTheorem 1. For a graph G \u223cCSBM-C(\ud835\udf410, \ud835\udf411,\ud835\udc51,\u210e0,\u210e1), for any\nnode \ud835\udc56in G, the smaller the value of |\u210e0 + \u210e1 \u22121|, the greater the\nprobability that h\ud835\udc56will be misclassified by h\u2019s optimal linear classifier.\nThe proof of the Theorem 1 can be found in Appendix A.1. At the\nsame time, CHV is directly proportional to (\u210e0 \u2212\u210e1)2 (Proof can be\nfound in Appendix A.2) under this assumption. In the GAD datasets,\ndue to the high homophily value of normal nodes (close to 1), and\nlow homophily value of anomalies (close to 0), in extreme cases, if\n\u210e0 = 0 and \u210e1 = 1, then the CHV reaches its maximum value, and\nthe probability of misclassification is the highest. In generic graphs,\nwhere \u210e0 and \u210e1 are close to each other and both near to 0 or 1,\nthe value of CHV is relatively lower, and the probability of mis-\nclassification is also smaller. Classification can also be challenging\nwhen the CHV is small, such as when \u210e0 = \u210e1 = 0.5. However, our\nprevious data analysis has shown that this situation does not occur\nin both GAD or generic datasets. At the same time, CHV describes\nthe characteristics of the GAD scenario more intuitively and can\nbe easily extended to multi-class scenarios.\n2.4\nProblem Formulation\nIn this task, we consider graph data as input, defining graph struc-\ntured data as G = {V, {A\ud835\udc5f},\ud835\udc4b}, where V represents the set of\nnodes, including both benign node and abnormal nodes, {A\ud835\udc5f} rep-\nresents the set of relations, with the total number of nodes \ud835\udc5bin\nthe dataset denoted as |V|, and \ud835\udc5f\u2208{1, 2, ..., \ud835\udc45} indicating the rela-\ntions, \ud835\udc45is the number of relations in the dataset. \u2200A \u2208{A\ud835\udc5f}, A\nis a binary matrix belonging to {0, 1}\ud835\udc5b\u00d7\ud835\udc5b. A represents an undi-\nrected graph, meaning if there is a connection between nodes \ud835\udc56\nand \ud835\udc57, then A\ud835\udc56\ud835\udc57= 1 and A\ud835\udc57\ud835\udc56= 1. \ud835\udc4b\u2208R\ud835\udc5b\u00d7\ud835\udc53represents the at-\ntributes of the nodes, and \ud835\udc53is the dimension. With the attribute\nof any node \ud835\udc63\ud835\udc56, \ud835\udc4b\ud835\udc63\ud835\udc56\u2208R\ud835\udc53. We define graph anomaly detection as\na semi-supervised learning task. \u2200\ud835\udc63\u2208V,\ud835\udc4c(\ud835\udc63) \u2208{0, 1}, where 0\nrepresents a benign node and 1 represents an anomaly node. Mean-\nwhile, \ud835\udc4c\u2208{\ud835\udc4c\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5b,\ud835\udc4c\ud835\udc63\ud835\udc4e\ud835\udc59,\ud835\udc4c\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61}, where \ud835\udc4c\ud835\udc63\ud835\udc4e\ud835\udc59and \ud835\udc4c\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61are not visible\nto the model during training. We train the model using \ud835\udc4c\ud835\udc61\ud835\udc5f\ud835\udc4e\ud835\udc56\ud835\udc5band\nuse \ud835\udc4c\ud835\udc63\ud835\udc4e\ud835\udc59to select the best model. We use GNN as the backbone\nmodel to achieve the lowest error on \ud835\udc4c\ud835\udc61\ud835\udc52\ud835\udc60\ud835\udc61.\n3\nPROPOSED METHOD\nIn this section, we specifically introduce the technical details of\nHedGe as shown in Figure 2. We firstly describe how our model\napplies position encoding to each node to enhance the node\u2019s repre-\nsentation ability. Next, we use an attention mechanism to generate\nattention matrices and then sample new relational graphs through\nthe attention coefficients. Following that, we combine multiple re-\nlationships, including original edges, generated relationships, and\nthe attention matrix sum, and pass them to the next layer. Finally,\nwe present our optimization objective.\n3.1\nPosition Encoding\n3.1.1\nDegree Position Encoding. Inspired by previous work [58],\nwe recognize that in a graph, the degree information of anomaly\nand benign nodes can effectively reflect their importance. This\nis especially evident when dealing with multi-relational graphs.\nThe degree information is effective in capturing structural anom-\nalies and identifying abnormal nodes. Abnormal nodes often differ\nFeed Forward\nLayer Normalize\nBenign\nAnomaly\nNode\nHomophily\nHeterophily\nEdge\nOriginal Graph\n(a) Position\nEncoding\n+\n&\n||\nAttention \nMatrix\nOriginal Relation\n\u00d7\nSample\nGNN Layer\n(b) Attention based Edge Sampler\n+\n(c) Diverse Relationship Fusion\nSampled \nGraph\nLayer n\n\u2026\nCross-Entropy Loss\nHeterophily Edge Suppression\n+\n(d) Optimization\nGround Truth\nhigh\nlow\nFigure 2: The overall architecture of the proposed HedGe. (a) We first apply position encoding to enhance node information. (b)\nThen we calculate node relationships and sample new relationships through self-attention. (c) Next, we aggregate multiple\nrelationships. (d) Finally, we penalize the attention matrix to suppress the generation of heterophilic edges.\nnoticeably from normal nodes in their degree distribution. This dif-\nference in distribution provides an intuitive and effective starting\npoint for anomaly detection.\nAssuming in a multi-relational graph, we have \ud835\udc45different types\nof relationships, and for each node \ud835\udc63, we can define its degree\nunder the\ud835\udc5f-th type of relationship as deg\ud835\udc5f(\ud835\udc63). Therefore, the degree\nencoding of node \ud835\udc63, PE\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52(\ud835\udc63), can be constructed by\nPE\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52(\ud835\udc63) = Concat(deg1(\ud835\udc63), deg2(\ud835\udc63), . . . , deg\ud835\udc45(\ud835\udc63)),\n(2)\nwhere we concatenate degrees under all \ud835\udc45types of relationships.\n3.1.2\nLaplacian Position Encoding. To capture the structural infor-\nmation of the entire graph, we utilize Laplacian Position Encoding\nto represent the position of nodes in the whole graph. The Laplacian\nmatrix is an important tool for describing the spectral character-\nistics of a graph. It contains not only the connection information\nbetween nodes, but also reflects the structural information of the\nentire graph. Using the Laplacian matrix for position encoding en-\nables the model to effectively capture and utilize global and local\nstructural information when dealing with complex graph structures.\nWe calculate Laplacian Position Encoding by\nL = I \u2212D\u22121\n2 AD\u22121\n2 ,\nLv = \ud835\udf06v,\nVselected = V[:, 1 : \ud835\udc58+ 1],\npe\ud835\udc56= sign\ud835\udc56\u00b7 v\ud835\udc56,\nPE\ud835\udc5f\n\ud835\udc3f= [pe1|pe2| . . . |pe\ud835\udc58],\nPE\ud835\udc3f(\ud835\udc63) = Concat(PE1\n\ud835\udc3f, PE2\n\ud835\udc3f, . . . , PE\ud835\udc45\n\ud835\udc3f)[\ud835\udc63, :],\n(3)\nwhere A is the adjacency matrix of the graph, D is the diagonal\ndegree matrix, I is the identity matrix, \ud835\udf06are the eigenvalues and v\nare the corresponding eigenvectors of the Laplacian matrix respec-\ntively. Then we select the first \ud835\udc58non-trivial eigenvectors. Each of\nthe selected eigenvectors is multiplied by a random sign\ud835\udc56(+1 or -1)\nand pe\ud835\udc56is the position encoding vector.\n3.1.3\nLabel Encoding. To mitigate the sample imbalance in anom-\naly node detection, we adopt downsampling strategy [34], which\ndoesn\u2019t include all training labels in a single training epoch. To\nfurther utilize label information, we use the labels from the training\nset that are not trained in a single epoch and set up label encoding.\nWe classify unknown labels, including those in the validation set,\ntest set, and the labels needed for the current training, as 2 for\nexample, indicating they are unknown. Noting that labels 0 and 1\nare preserved for to denote benign and anomaly nodes respectively.\nThus, L(\ud835\udc63) is the label mapping function, which outputs a label\n\ud835\udc59\u2208{0, 1, 2}. We define an encoding function E : {0, 1, 2} \u2192R\ud835\udc51,\nwhere \ud835\udc51is the embedding dimension, mapping the labels into a\n\ud835\udc51-dimensional space. Label encoding is calculated by\nPE\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59(\ud835\udc63) = E(L(\ud835\udc63)).\n(4)\n3.1.4\nEncoding Aggregation. After calculating three types of posi-\ntion encodings, we perform position aggregation through\nh\ud835\udc63= Concat(X(\ud835\udc63), PE\ud835\udc37\ud835\udc52\ud835\udc54\ud835\udc5f\ud835\udc52\ud835\udc52(\ud835\udc63), PE\ud835\udc3f(\ud835\udc63)) + PE\ud835\udc3f\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59(\ud835\udc63)\n(5)\nto obtain the final embedding for each node.\n3.2\nAttention based Edge Sampler\n3.2.1\nAttention coefficient. After obtaining the embedding of the\nnodes, next we need to calculate the attention coefficients between\neach pair of nodes, in order to perform sampling and aggregation.\nWe refer to the Transformer [53] architecture and adopt the scaled\ndot-product attention mechanism to calculate the attention coeffi-\ncients,\n\ud835\udc4e\ud835\udc56\ud835\udc57= Softmax\n \n(W\ud835\udc5e\u00b7 h\ud835\udc56) \u00b7 (W\ud835\udc58\u00b7 h\ud835\udc57)\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n!\n,\n(6)\nwhere \ud835\udc4e\ud835\udc56\ud835\udc57represents the attention coefficient from node \ud835\udc56to \ud835\udc57,\nW\ud835\udc5eand W\ud835\udc58are learnable weight matrices, and \ud835\udc51\ud835\udc58denotes the\ndimension of the key vectors. The scaled dot-product attention\nmechanism, as compared to the attention mechanism introduced by\nGAT, offers a significant enhancement in computational efficiency.\nThis is achieved by enabling the computation of attention across\nthe entire graph in a matrix form using dot products.\n3.2.2\nEdge Specific Gumbel Softmax. After obtaining the attention\ncoefficients between each node pair, it\u2019s typical to employ methods\nlike K-Nearest Neighbors (KNN) [14] or Gumbel-Top-k trick [28]\nto identify or sample potential neighbors among the k-nearest\nones. However, a limitation of these methods is that they result\nin each node having a predetermined, fixed number of connected\nneighbors, which does not accurately reflect the more variable\nand dynamic nature of real-world networks. At the same time,\nthese techniques involve selection and sorting operations, which\nare inherently non-differentiable. To address these problems, we\nchoose to perform independent Gumbel-Softmax [23] sampling on\neach attention coefficient.\nThis method is employed for sampling from a categorical dis-\ntribution, facilitating gradient-based optimization. We focus on\na binary case, employing a probability vector [\ud835\udc4e, 1 \u2212\ud835\udc4e], where \ud835\udc4e\nis in the range [0, 1]. Let \ud835\udc3a\ud835\udc56be independently sampled from a\n\ud835\udc3a\ud835\udc62\ud835\udc5a\ud835\udc4f\ud835\udc52\ud835\udc59(0, 1) distribution, \ud835\udc3a\ud835\udc56= \u2212log(\u2212log(\ud835\udc62\ud835\udc56)), where \ud835\udc62\ud835\udc56is inde-\npendently drawn from a uniform distribution in [0, 1]. The Edge\nSpecific Gumbel Softmax distribution, ESGS for simple, is given by:\nESGS(\ud835\udc4e) =\n\u0014\nexp(\ud835\udc3f1/\ud835\udf0f)\nexp(\ud835\udc3f1/\ud835\udf0f) + exp(\ud835\udc3f2/\ud835\udf0f) ,\nexp(\ud835\udc3f2/\ud835\udf0f)\nexp(\ud835\udc3f1/\ud835\udf0f) + exp(\ud835\udc3f2/\ud835\udf0f)\n\u0015\n,\n(7)\nwhere \ud835\udc3f1 = log(\ud835\udc4e)+\ud835\udc3a1, \ud835\udc3f2 = log(1\u2212\ud835\udc4e)+\ud835\udc3a2, and\ud835\udf0fis the temperature\nparameter. The temperature parameter controls the entropy of the\noutput distribution, meaning that a lower temperature results in\nan output closer to a hard discrete distribution, while a higher\ntemperature leads to a more uniform distribution. Next, we can\nsample whether there is an edge between nodes i and j by\n\ud835\udc5d\ud835\udc56\ud835\udc57= ESGS(\ud835\udf06\u00b7 \ud835\udc4e\ud835\udc56\ud835\udc57) \u00b7\n\u00141\n0\n\u0015\n,\n(8)\nwhere \ud835\udf06is a hyperparameter to control the number of sampled\nedges. By employing a reparameterization trick [26], we transfer\nthe non-differentiable parts to the random sampling of \ud835\udc62\ud835\udc56, ensur-\ning the differentiability of the other parts. Meanwhile, we use the\nstraight-through trick to convert the continuous relaxation output\nof Gumbel-Softmax into one-hot encoding while ensuring an ap-\nproximate gradient. Consequently, \ud835\udc52\ud835\udc56\ud835\udc57\u2248\ud835\udc5d\ud835\udc56\ud835\udc57,\ud835\udc52\ud835\udc56\ud835\udc57\u2208{0, 1}. And we\nuse \ud835\udc52\ud835\udc56\ud835\udc57to build a adjacency matrix A\ud835\udc3a\n\ud835\udc56\ud835\udc57= min(\ud835\udc52\ud835\udc56\ud835\udc57+\ud835\udc52\ud835\udc57\ud835\udc56, 1) and send\nit to a GCN layer, for node \ud835\udc63\ud835\udc56\nh\ud835\udc3a\ud835\udc63\ud835\udc56= \ud835\udf0e\u00a9\u00ad\n\u00ab\n\u2211\ufe01\n\ud835\udc57\n1\n\ud835\udc50\ud835\udc56\ud835\udc57\nh\ud835\udc63\ud835\udc57W\ud835\udc54\u00aa\u00ae\n\u00ac\n,\n(9)\nwhere \ud835\udf0erepresents an activation function, W\ud835\udc54is a learnable matrix\nand \ud835\udc50\ud835\udc56\ud835\udc57=\n\u221a\ufe01\n\ud835\udc51\ud835\udc52\ud835\udc54(\ud835\udc56)\ud835\udc51\ud835\udc52\ud835\udc54(\ud835\udc57) in A\ud835\udc3a. In practice, we can choose to use\nthe matrix form of the adjacency matrix for computation or adopt\nthe discrete form of the adjacency matrix for acceleration, but this\nstep will lose the gradient.\n3.3\nRelation Fusion and Optimization\n3.3.1\nDiverse Relationship Fusion. We apply GraphSAGE [21] or\nGCN [27] as the aggregation function to the original relationships,\nassuming we use GraphSAGE here. For the \ud835\udc5f-th type of relationship,\nthe aggregation formula is\nh\ud835\udc5f\n\ud835\udc42\ud835\udc63= \ud835\udf0e(W\ud835\udc60\u00b7 MEAN({h\ud835\udc63} \u222a{h\ud835\udc62, \u2200\ud835\udc62\u2208N\ud835\udc5f(\ud835\udc63)})),\n(10)\nwhere N\ud835\udc5f(\ud835\udc63) is the neighborhood of node in relation \ud835\udc5fand W\ud835\udc60is a\nlearnable matrix.\nThen, we aggregate the original attention coefficients as weighted\nrelationships and add up multiple relationships. Now that we have\nthree types of features, original relationship features, sampled rela-\ntionship features, and attention coefficient features, and we merge\nthese three relationships. Finally, we use layer normalization LN(.)\nand a feed-forward layer FFN(.) to reduce numerical instability and\nfacilitate learning. So the layer \ud835\udc58of the HedGe is\nh\ud835\udc34=\n\u2211\ufe01\n\ud835\udc57\n\ud835\udc4e\ud835\udc56\ud835\udc57\u00b7 W\ud835\udc63\u00b7 h\ud835\udc57,\nh\ud835\udc5f\n\ud835\udc58= Concat(h\ud835\udc5f\n\ud835\udc3a, h\ud835\udc5f\n\ud835\udc42, h\ud835\udc5f\n\ud835\udc34),\nh\ud835\udc58= FFN(LN(MEAN(h1\n\ud835\udc58\u22121, h2\n\ud835\udc58\u22121, . . . , h\ud835\udc45\n\ud835\udc58\u22121))).\n(11)\n3.3.2\nOptimization Objective. In a HedGe with k layers, the final\nrepresentation of a node is denoted as h\ud835\udc58. For node classification,\nwe employ a MLP and the Softmax function to get the possibility\n\ud835\udc5d\ud835\udc63, and optimize the model using the cross-entropy loss,\nL\ud835\udc50= \u2212\n\u2211\ufe01\n\ud835\udc63\u2208V\n[\ud835\udc66\u00b7 log(\ud835\udc5d\ud835\udc63) + (1 \u2212\ud835\udc66) \u00b7 log(1 \u2212\ud835\udc5d\ud835\udc63)] .\n(12)\nTo suppress the generation of heterophilic edges, we add a het-\nerophily edge suppression penalty term L\ud835\udc5d. This penalty term\ninvolves calculating the squared attention coefficients between\nnodes with differing labels,\nL\ud835\udc5d=\n\u2211\ufe01\n\ud835\udc4e\nh\n\ud835\udc4e2\n\ud835\udc56\ud835\udc57\u00b7 1{\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59(\ud835\udc56)\u2260\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59(\ud835\udc57)}\ni\n.\n(13)\nNote that we only use labels from the training set for punishment\nto avoid label leakage. This penalty reduces the attention values\nfor nodes with different labels, thereby decreasing the likelihood\nof generating heterophilic edges. By suppressing the generation of\nheterophily edges, we ensure that the generated edges have a lower\nCHV, making the dataset\u2019s relationships closer to generic graphs\nto enhance the learning capability of GNNs.\nOur final optimization objective is\nL = L\ud835\udc50+ \ud835\udefcL\ud835\udc5d+ \ud835\udefd\u2225\ud835\udf03\u22252\n2,\n(14)\nwhere \ud835\udefc, \ud835\udefdare hyperparameters and \ud835\udf03denotes the parameters of\nthe model which need to be trained.\n4\nEXPERIMENTS\nIn this section, we conduct a comprehensive analysis of the effec-\ntiveness of HedGe. We evaluate it on four graph anomaly detection\ndatasets, and then perform attacks on two generic datasets using\nour defined Heterophily Attack to simulate high Class Homophily\nVariance scenarios and test performance. We explore its capabilities\nin edgeless node classification scenarios. We also execute ablation\nstudies and engage in visualization techniques to verify that the\nmodel operates as anticipated.\nTable 2: Performance comparison of different models for anomaly detection.\nMethod\nAmazon\nYelpChi\nBlogCatalog\nReddit\nAmazon\nYelpChi\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nTraining ratio\n40%\n1%\nGCN\n85.32\n35.14\n60.34\n23.69\n88.09\n46.52\n65.04\n6.70\n77.97\n23.74\n54.64\n17.68\nGAT\n93.04\n60.67\n59.82\n23.48\n71.25\n21.45\n65.19\n5.53\n80.56\n33.29\n54.87\n16.95\nGraphSAGE\n95.85\n84.74\n80.44\n46.83\n82.34\n45.09\n64.11\n6.49\n92.42\n78.32\n72.86\n32.44\nMixHop\n96.03\n86.44\n79.56\n45.29\n88.31\n48.20\n65.63\n5.44\n92.42\n78.32\n72.86\n32.44\nGPRGNN\n94.75\n76.75\n73.11\n32.97\n81.93\n43.46\n62.93\n5.45\n93.52\n74.51\n67.66\n28.70\nCAREGNN\n88.48\n69.24\n77.96\n36.63\n69.40\n27.13\n67.21\n6.72\n87.27\n73.93\n75.29\n36.07\nPCGNN\n95.95\n80.88\n80.16\n38.86\n66.75\n22.55\n64.66\n5.84\n89.47\n75.34\n73.25\n30.95\nAMNet\n95.11\n83.64\n85.85\n57.77\n63.54\n26.26\n63.64\n8.55\n87.86\n74.92\n73.16\n36.59\nH2-FDetector\n96.46\n85.33\n88.98\n60.98\n83.03\n35.57\n66.73\n7.80\n87.17\n63.10\n79.24\n43.55\nBWGNN\n97.99\n90.09\n90.22\n63.78\n79.37\n37.39\n71.37\n8.96\n89.10\n80.40\n77.52\n37.66\nGDN\n97.10\n87.37\n90.26\n66.42\n70.70\n28.55\n69.19\n6.94\n83.30\n61.48\n73.39\n38.68\nSparseGAD\n97.03\n89.17\n88.61\n66.01\n70.16\n25.60\n66.12\n5.98\n93.67\n81.40\n78.73\n40.77\nHedGe-w/pos\n97.88\n91.23\n90.33\n69.18\n93.07\n43.91\n72.12\n9.62\n94.89\n78.90\n78.38\n41.09\nHedGe-w/sam\n98.01\n90.98\n89.51\n66.40\n90.83\n40.21\n71.40\n8.16\n92.69\n72.05\n78.22\n42.35\nHedGe-w/loss\n97.72\n89.34\n90.54\n69.14\n92.58\n42.73\n72.45\n9.57\n95.39\n80.80\n80.24\n44.23\nHedGe\n98.25\n92.30\n91.29\n70.68\n94.35\n50.83\n73.19\n9.64\n95.83\n85.82\n81.17\n44.90\n4.1\nExperimental Setup\n4.1.1\nDatasets. Following previous works [19, 65], we conduct\ncomprehensive evaluations of HedGe in the GAD scenario on four\nbenchmark datasets, including three real datasets: Amazon [40],\nYelpChi [45], Reddit [29], and an injected dataset, BlogCatalog\n[51]. Additionally, we test two generic node classification datasets:\nAmazon co-purchase graphs Photo [47] and the citation graphs\nPubMed [61] and evaluate different models\u2019 performance under\nHeterophily Attack to simulate high CHV and show the universality\nof our model. For detailed descriptions and statistical data of these\ndatasets, please refer to the Appendix D.2.\n4.1.2\nBaselines. We selected several representative models or the\nlatest state-of-the-art models for comparison. For more detailed\ndescription, please refer to Appendix D.3.\n\u2022 GCN [27], GAT [54] and GraphSAGE [21]. These models\nrepresent the most basic and widely used GNNs.\n\u2022 MixHop [1] and GPRGNN [13] are models designed for\novercoming over-smoothing and heterophily.\n\u2022 CAREGNN [17], PCGNN [35], H2-FDetector [48], AM-\nNet [5], BWGNN [50], GDN [19] and SparseGAD [65] are\nanti-fraud models or GAD models. They are state-of-the-art\nmodels in GAD scenario.\n\u2022 KNN [14], SVM [3] and MLP [46] are classic machine learn-\ning algorithms. Random Forest [4], CATBoost [44], XG-\nBoost [8] and LightGBM [25] are classic decision tree-based\nmachine learning algorithms. We compared them in edgeless\nclassification scenarios.\n4.1.3\nMetrics. Following previous works [17, 20], we use Area Un-\nder the Receiver Operating Characteristic curve (AUC) and Average\nPrecision (AP) as the evaluation metrics in the anomaly detection\nscenario. AUC effectively measures the model\u2019s ability to discrimi-\nnate between different classes by considering its performance across\nall possible classification thresholds. Notably, the sensitivity of AUC\nto imbalanced datasets makes it an ideal indicator for evaluating\nmodel performance in GAD scenarios. AP, which takes into account\nprecision and recall at different thresholds, provides a more compre-\nhensive performance evaluating for imbalanced classes. In generic\ndatasets, we follow previous works [13, 27], we use accuracy as the\nevaluation criterion.\n4.2\nAnomaly Detection Performance\nWe conducted experiments on four benchmark datasets with 40%\nof the labels used for training. To validate scenarios with scarce\nlabels, we conducted experiments with only 1% of the labels for\ntraining on Amazon and YelpChi dataset. We divided the remaining\ndataset into halves for the validation and test set respectively.\nExperimental results are presented in Table 2. It is evident that\nour model achieved the best performance across four anomaly\ndetection datasets. We can see that the performance of vanilla\nGNNs is not optimal on GAD datasets, unable to adapt to high CHV\nsituations, providing empirical evidence for Theorem 1. Trained on\n40% of the Amazon, YelpChi, and BlogCatalog datasets, our model\nsignificantly improved performance. It increased the AP by at least\n2% and similarly raised AUC compared to competing models. In\nscenarios with scarce labels, our improvement is also significant.\nFor example, in the Amazon dataset with only 1% of the labels used\nfor training, our model achieved an absolute improvement of 2.16%\nin AUC and 4.45% in AP compared to the best-performing baseline,\nSparseGAD. It can also be observed that anomaly detection models\ndid not perform satisfactorily on BlogCatalog, as their modifications\nTable 3: Performance comparison of models without edges for anomaly detection.\nMethod\nAmazon\nYelpChi\nBlogCatalog\nReddit\nAmazon\nYelpChi\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nAUC\nAP\nTraining ratio\n40%\n1%\nKNN\n91.87\n81.60\n75.85\n36.69\n62.71\n20.81\n57.96\n4.80\n86.36\n68.05\n64.68\n22.16\nSVM\n93.76\n82.80\n81.34\n50.25\n67.13\n26.44\n58.93\n4.69\n90.64\n70.27\n70.55\n29.37\nMLP\n97.04\n86.95\n82.42\n31.27\n58.15\n13.30\n59.64\n4.71\n74.00\n42.15\n70.42\n31.27\nRandom Forest\n97.10\n86.45\n81.23\n51.89\n69.32\n29.02\n65.82\n5.64\n94.71\n67.14\n76.64\n37.44\nCATBoost\n97.20\n89.44\n83.11\n54.36\n66.70\n27.96\n62.24\n4.58\n95.08\n82.65\n73.19\n32.16\nXGBoost\n96.90\n87.35\n84.83\n58.17\n67.61\n25.18\n66.11\n7.02\n85.24\n69.95\n77.75\n38.60\nLightGBM\n97.95\n89.30\n85.71\n60.08\n72.36\n29.54\n66.78\n6.51\n93.41\n67.14\n76.25\n35.68\nHedGe-w/edges\n97.30\n90.71\n89.59\n63.67\n73.68\n36.59\n68.45\n9.52\n95.10\n83.91\n80.00\n43.53\nTable 4: Class Homophily Variance under Heterophily Attack\nRatio\n0%\n1%\n3%\n5%\n7%\n10%\nPhoto\n0.0171\n0.0194\n0.0278\n0.0401\n0.0654\n0.0754\nPubMed\n0.0044\n0.0066\n0.0137\n0.0247\n0.0410\n0.0814\n0\n1\n3\n5\n7\n10\nPerturbation rate (%)\n84\n86\n88\n90\n92\n94\nAccuracy\nOurs\nGCN\nGAT\nCAREGNN\nBWGNN\nSparseGAD\n(a) Photo\n0\n1\n3\n5\n7\n10\nPerturbation rate (%)\n83\n84\n85\n86\n87\n88\n89\nAccuracy\nOurs\nGCN\nGAT\nCAREGNN\nBWGNN\nSparseGAD\n(b) PubMed\nFigure 3: Accuracy for different models under Heterophily\nAttack to increase Class Homophily Variance.\nand filtering of relationships severely hindered their detection of\nstructural anomalies. However, our model retained the original\nstructure and performed well on this dataset, exceeding the best-\nperforming model, MixHop, by 6.04% in AUC and outperforming\nthe best GAD model, H2-FDetector, by an impressive 11.32% in\nAUC and 15.27% in AP.\n4.3\nHeterophily Attack and Generic Datasets\nTo validate the adaptability of our model to high CHV in any sce-\nnario, we conducted tests on generic datasets and proposed Het-\nerophily Attack to increase the CHV to simulate GAD datasets.\n4.3.1\nHeterophily Attack. Heterophily Attack is simple yet effec-\ntive. To simulate anomaly detection datasets in generic datasets\nand increase the dataset\u2019s CHV, we targeted a single class for the\nattack. Suppose the targeted class is \ud835\udc36. During the attack, we delete\nsome edges \ud835\udc52where both end nodes \ud835\udc63belong to the attack class \ud835\udc36,\nand then add some edges where one end node \ud835\udc63belongs to class\n\ud835\udc36, and the other end node \ud835\udc63does not belong to class \ud835\udc36. (Please\nrefer to Appendix C for more detailed description.) Our method has\nbeen tested and proven to effectively increase the CHV of datasets.\nTherefore, this method has simulated the most significant feature\nof GAD datasets on generic datasets. As shown in Table 4, on the\nPubMed dataset which contains only three classes, a 10% edge per-\nturbation increased the CHV by approximately 18 times. Although\nthe Photo dataset has eight classes, a 10% edge perturbation could\nstill increase its CHV by more than four times.\n4.3.2\nResults under Heterophily Attack. We implemented Heter-\nophily Attack on two datasets, Photo and PubMed, and compared\nseveral popular GNNs and GAD models. The experimental results\nare shown in Figure 3. We observed a significant decrease in the\nperformance of vanilla GNNs like GCN and GAT when facing\nHeterophily Attack. This finding further confirms our previously\nproposed viewpoint, the significant difference between GAD and\ngeneric datasets is high CHV. It also proves to a certain extent the\nconclusion of Theorem 1, that the performance of vanilla models\ndecreases when the CHV is very high. Additionally, we noted that,\napart from CAREGNN, the accuracy of other GAD models exhibited\na slightly trend of initial decline followed by an increase, under-\nscoring these models\u2019 adaptability to high CHV. Ultimately, our\nproposed HedGe model outperformed all benchmark models in all\nattack ratios, proving its superior generality and robustness.\n4.4\nEdgeless Node Classification\nOur HedGe model, with its adaptive edge generation capability, has\nopened up new possibilities for GNN models in edgeless classifica-\ntion tasks. In our experiments, we removed the original relation-\nships used as auxiliaries by HedGe and relied solely on the edges\ngenerated by an attention-based edge sampler as input to the GNNs.\nBy comparing with various classifiers that do not require edge infor-\nmation, we found that although tree-based classifiers like Random\nForest and XGBoost have already shown excellent performance in\nGAD edgeless scenarios, even outperforming some GNNs and GAD\nmodels in many tasks, our HedGe model still performed remarkably\nwell in edgeless GAD tasks.\nAs shown in Table 3, except for the AUC on Amazon, which\ndid not achieve the best results, HedGe led in all other evaluation\nmetrics in other datasets and training ratios. In particular, on the\n(a) Amazon(0.1655/0.0312)\n(b) YelpChi(0.1101/0.0159)\nFigure 4: Weighted homophily density distribution of the\noriginal and generated graphs. The pair of numbers enclosed\nby parentheses presents the Class Homophily Variance of\nthe original and generated graphs respectively.\nAmazon and YelpChi, with 40% labels for training, the AUC only\ndropped slightly (0.95% and 1.7%, respectively) compared to the\nsituation with edges, effectively proving the efficacy of our edge\ngeneration strategy. We noticed that on the BlogCatalog dataset,\nthere was a significant performance gap between edgeless and\nedged classification compared with other datasets, reflecting the\nimportance of structural anomalies within this dataset, echoing\nthe observation that GAD models focused on modification fail to\nachieve good results in previous experiments.\n4.5\nAblation Study\nIn the detailed ablation study conducted on the HedGe model, we\nfocused on exploring the contribution and efficacy of each compo-\nnent of the model in the task of anomaly detection. The experiment\nwas centered around the removal of three core components of\nthe model: Position Encoding, Attention-based Edge Sampler, and\nthe heterophily penalty term in the loss function. These variants\nwere named HedGe-w/pos, HedGe-w/sam, and HedGe-w/loss, re-\nspectively, with results shown in the last four rows of Table 2. The\nexperiment results revealed several key findings.\nFirstly, with the exception of the 40% training ratio for the Ama-\nzon dataset, the attention-based edge sampler significantly affected\nmodel performance, especially when the training set was extremely\nsmall. In sparse label environments, 1% training ratio, the removal\nof the edge sampler led to a notable decrease in the AUC metric on\nthe Amazon and YelpChi datasets, by 3.14% and 2.95%, respectively.\nThis indicates that when training data is limited, the model\u2019s per-\nformance under high CHV is seriously constrained, and the edge\nsampler, by generating more homophilic edges, reduces the learn-\ning difficulty and thus improves GNN performance. Additionally,\nthis series of ablation experiments also emphasized the importance\nof the homophily focus prior brought about by the heterophily\nedge suppression module in the loss function for improving model\nperformance, as well as the critical role of position encoding in\ngraph learning and graph anomaly detection.\n4.6\nInterpretability\nOur model has successfully mitigated the original bimodal feature\nof the homophily distribution, as evidenced by the weighted ho-\nmophily density distribution graphs as shown in Figure 4. In the\n(a) Original\n(b) GCN\n(c) GAT\n(d) GraphSAGE\n(e) GPRGNN\n(f) CAREGNN\n(g) PCGNN\n(h) AMNet\n(i) H2-FDetector\n(j) BWGNN\n(k) GDN\n(l) Ours\nFigure 5: t-SNE visualization of learned embeddings.\ngenerated edges, the peak density on the right is more than three\ntimes higher than that on the left, whereas in the original graphs,\ntwo peaks have roughly the same density. Furthermore, we also\nmeasure the CHV of the original and generated graphs. For Ama-\nzon, it decreased from 0.1655 to 0.0312, and for Yelp, it decreased\nfrom 0.1101 to 0.0159. These reductions, both by at least a factor of\nfive, demonstrate that the distribution of the edges generated by\nour method meets the expectations.\nWe also demonstrate the output results of different graph neural\nnetwork models on the YelpChi after dimensionality reduction us-\ning t-SNE technique as shown in Figure 5. Each subplot represents\nthe output before the last layer of different models, where red repre-\nsents anomalous nodes, and blue represents benign nodes. We have\nrandomly downsampled the benign nodes to match the number\nof anomalous nodes for clarity. Observing these visualizations, it\nis apparent that our model achieves more distinct separation in\nclustering compared to other models and produces clearer and more\ndefinitive groupings in t-SNE visualization. For instance, our model\nhas significantly fewer overlaps in the red and blue areas compared\nto others, clearly showing an inverted U-shaped decision boundary.\n5\nRELATED WORKS\nGraph Neural Network for Classification. Graph Neural Net-\nworks are deep learning models and are widely used in many fields\nsuch as drug repositioning research [55], recommendation system\n[31] and relation classification [9, 30]. Vanilla GNNs like GCN [27]\nare based on the homophily principle [41], but many real-world\ndatasets are heterophilic and thus not suitable for them. To alleviate\nthis disparity, two strategies are employed [64] : non-local neighbor\nand GNN architecture refinement. In models employing the non-\nlocal neighbor strategy, such as MixHop [1], H2GCN [65], UGCN\n[24], and TDGNN [57], the high-order neighbour mixing method\nis predominantly used, whereas models like Geom-GCN [43], NL-\nGNN [33], and HOG-GNN [56] primarily utilize the potential neigh-\nbor discovery method. Both approaches focus on extending local\nneighboring relationships to non-local ones. In models using the\nGNN Architecture Refinement strategy, such as FAGCN [2] and\nWRGNN [49], adaptive message aggregation is used; H2GCN [65]\nand WRGNN [49] employ the Ego-neighbor Separation method,\nand the GPRGNN [13] model utilizes the inter-layer combination\nmethod. These methods have achieved good results on generic\ngraphs. Due to the low CHV of these datasets, which is completely\ndifferent from the GAD dataset, they do not perform well on GAD.\nOur models learns the representations between different classes\nand generate homophilic relations from scratch to input into GNNs,\naligning with the data assumption paradigm of GNNs.\nGraph-based Anomaly Detection. Many models for graph anom-\naly detection have been proposed. CAREGNN [17], AOGNN [22]\nand PCGNN [35] employ reinforcement learning and resampling\nstrategies to select neighborhoods. However, these models blindly\naggregate neighboring nodes, leading to the camouflage of anom-\nalies. AMNet [5] and BWGNN [50] use multi-pass spectral filters\nto identify anomalies. Additionally, several works recognize the\nhomophily differences between anomalies and benign nodes. GDN\n[19] uses a prototype vector to infer and update the distribution of\nanomaly features during training. SparseGAD [20] sparsifies the\nstructure of the target graph to effectively reduce noise and collab-\noratively learn node representations. GHRN [18] trims inter-class\nedges by emphasizing and depicting the high-frequency compo-\nnents of graphs. There is still no way to quantitatively describe\nthe difference in homophily between GAD and generic datasets.\nMost of these works are based on modifications. Because of the\nlow homophily of anomalies, minor modifications of relationships\ndo not perform well. Also, modifications disturb structural infor-\nmation, leading to suboptimal performance in detecting structural\nanomalies. We propose CHV to describe homophily difference and,\nthrough generation rather than modification, combine original re-\nlationships as auxiliary for anomaly detection.\n6\nCONCLUSION\nWe provides a comprehensive analysis of how homophily distribu-\ntions vary between anomaly detection datasets and others. It also\nproposes a novel metric, Class Homophily Variance, to effectively\ncharacterize these differences. To address the issue of high CHV,\nwe introduce HedGe, which alleviates this problem by generating\nhomophilic edges rather than modifying original relationships. Ex-\nperiments have demonstrated the effectiveness of our method in\nvarious scenarios including graph anomaly detection datasets, sim-\nulation and edgeless node classification, and have proven that the\nedges generated by the model have low CHV.\nREFERENCES\n[1] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Nazanin Alipourfard, Kristina\nLerman, Hrayr Harutyunyan, Greg Ver Steeg, and Aram Galstyan. 2019. Mixhop:\nHigher-order graph convolutional architectures via sparsified neighborhood\nmixing. In International Conference on Machine Learning. PMLR, 21\u201329.\n[2] Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. 2021. Beyond low-frequency\ninformation in graph convolutional networks. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, Vol. 35. 3950\u20133957.\n[3] Bernhard E Boser, Isabelle M Guyon, and Vladimir N Vapnik. 1992. A training al-\ngorithm for optimal margin classifiers. In Proceedings of the fifth annual workshop\non Computational learning theory. 144\u2013152.\n[4] Leo Breiman. 2001. Random forests. Machine Learning 45 (2001), 5\u201332.\n[5] Ziwei Chai, Siqi You, Yang Yang, Shiliang Pu, Jiarong Xu, Haoyang Cai, and\nWeihao Jiang. 2022. Can abnormality be detected by graph neural networks.\nIn Proceedings of the Twenty-Ninth International Joint Conference on Artificial\nIntelligence. 23\u201329.\n[6] Varun Chandola, Arindam Banerjee, and Vipin Kumar. 2009. Anomaly detection:\nA survey. Comput. Surveys 41, 3 (2009), 1\u201358.\n[7] Dongyue Chen, Ruonan Liu, Qinghua Hu, and Steven X Ding. 2021. Interaction-\naware graph neural networks for fault diagnosis of complex industrial processes.\nIEEE Transactions on Neural Networks and Learning Systems (2021).\n[8] Tianqi Chen and Carlos Guestrin. 2016. Xgboost: A scalable tree boosting system.\nIn Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. 785\u2013794.\n[9] Dawei Cheng, Chen Chen, Xiaoyang Wang, and Sheng Xiang. 2021. Efficient\ntop-k vulnerable nodes detection in uncertain graphs. IEEE Transactions on\nKnowledge and Data Engineering 35, 2 (2021), 1460\u20131472.\n[10] Dawei Cheng, Xiaoyang Wang, Ying Zhang, and Liqing Zhang. 2020. Risk\nguarantee prediction in networked-loans. In IJCAI International Joint Conference\non Artificial Intelligence.\n[11] Dawei Cheng, Yujia Ye, Sheng Xiang, Zhenwei Ma, Ying Zhang, and Changjun\nJiang. 2023. Anti-Money laundering by group-Aware deep graph learning. IEEE\nTransactions on Knowledge and Data Engineering (2023).\n[12] Wei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy Bengio, and Cho-Jui Hsieh.\n2019. Cluster-gcn: An efficient algorithm for training deep and large graph\nconvolutional networks. In Proceedings of the 25th ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining. 257\u2013266.\n[13] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. 2020. Adaptive universal\ngeneralized pageRank graph neural network. In International Conference on\nLearning Representations.\n[14] Thomas Cover and Peter Hart. 1967. Nearest neighbor pattern classification.\nIEEE Transactions on Information Theory 13, 1 (1967), 21\u201327.\n[15] Ailin Deng and Bryan Hooi. 2021. Graph neural network-based anomaly detection\nin multivariate time series. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 35. 4027\u20134035.\n[16] Yash Deshpande, Subhabrata Sen, Andrea Montanari, and Elchanan Mossel. 2018.\nContextual stochastic block models. Advances in Neural Information Processing\nSystems 31 (2018).\n[17] Yingtong Dou, Zhiwei Liu, Li Sun, Yutong Deng, Hao Peng, and Philip S Yu. 2020.\nEnhancing graph neural network-based fraud detectors against camouflaged\nfraudsters. In Proceedings of the 29th ACM International Conference on Information\nand Knowledge Management. 315\u2013324.\n[18] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yong-\ndong Zhang. 2023. Addressing heterophily in graph anomaly detection: A per-\nspective of graph spectrum. In Proceedings of the Web Conference. 1528\u20131538.\n[19] Yuan Gao, Xiang Wang, Xiangnan He, Zhenguang Liu, Huamin Feng, and Yong-\ndong Zhang. 2023. Alleviating structural distribution shift in graph anomaly\ndetection. In Proceedings of the Sixteenth ACM International Conference on Web\nSearch and Data Mining. 357\u2013365.\n[20] Zheng Gong, Guifeng Wang, Ying Sun, Qi Liu, Yuting Ning, Hui Xiong, and\nJingyu Peng. 2023. Beyond homophily: robust graph anomaly detection via neural\nsparsification. In Proceedings of the Thirty-Second International Joint Conference\non Artificial Intelligence. 2104\u20132113.\n[21] Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017. Inductive representation\nlearning on large graphs. Advances in Neural Information Processing Systems 30\n(2017).\n[22] Mengda Huang, Yang Liu, Xiang Ao, Kuan Li, Jianfeng Chi, Jinghua Feng, Hao\nYang, and Qing He. 2022. Auc-oriented graph neural network for fraud detection.\nIn Proceedings of the ACM Web Conference. 1311\u20131321.\n[23] Eric Jang, Shixiang Gu, and Ben Poole. 2016. Categorical reparameterization\nwith Gumbel-Softmax. In International Conference on Learning Representations.\n[24] Di Jin, Zhizhi Yu, Cuiying Huo, Rui Wang, Xiao Wang, Dongxiao He, and Ji-\nawei Han. 2021. Universal graph convolutional networks. Advances in Neural\nInformation Processing Systems 34 (2021), 10654\u201310664.\n[25] Guolin Ke, Qi Meng, Thomas Finley, Taifeng Wang, Wei Chen, Weidong Ma,\nQiwei Ye, and Tie-Yan Liu. 2017. Lightgbm: A highly efficient gradient boosting\ndecision tree. Advances in Neural Information Processing Systems 30 (2017).\n[26] Diederik P Kingma and Max Welling. 2014. Auto-Encoding variational bayes. In\nInternational Conference on Learning Representations.\n[27] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with graph\nconvolutional networks. In International Conference on Learning Representations.\n[28] Wouter Kool, Herke Van Hoof, and Max Welling. 2019. Stochastic beams and\nwhere to find them: The gumbel-top-k trick for sampling sequences without\nreplacement. In International Conference on Machine Learning. PMLR, 3499\u20133508.\n[29] Srijan Kumar, Xikun Zhang, and Jure Leskovec. 2019. Predicting dynamic em-\nbedding trajectory in temporal interaction networks. In Proceedings of the 25th\nACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\n1269\u20131278.\n[30] Yifu Li, Ran Jin, and Yuan Luo. 2019. Classifying relations in clinical narratives\nusing segment graph convolutional and recurrent neural networks (Seg-GCRNs).\nJournal of the American Medical Informatics Association 26, 3 (2019), 262\u2013268.\n[31] Yongquan Liang, Qiuyu Song, Zhongying Zhao, Hui Zhou, and Maoguo Gong.\n2023. BA-GNN: Behavior-aware graph neural network for session-based recom-\nmendation. Frontiers of Computer Science 17, 6 (2023), 176613.\n[32] Kay Liu, Yingtong Dou, Yue Zhao, Xueying Ding, Xiyang Hu, Ruitong Zhang,\nKaize Ding, Canyu Chen, Hao Peng, Kai Shu, et al. 2022. Pygod: A python library\nfor graph outlier detection. arXiv preprint arXiv:2204.12095 (2022).\n[33] Meng Liu, Zhengyang Wang, and Shuiwang Ji. 2021. Non-local graph neural\nnetworks. IEEE Transactions on Pattern Analysis and Machine Intelligence 44, 12\n(2021), 10270\u201310276.\n[34] Xu-Ying Liu, Jianxin Wu, and Zhi-Hua Zhou. 2008. Exploratory undersampling\nfor class-imbalance learning. IEEE Transactions on Systems, Man, and Cybernetics,\nPart B (Cybernetics) 39, 2 (2008), 539\u2013550.\n[35] Yang Liu, Xiang Ao, Zidi Qin, Jianfeng Chi, Jinghua Feng, Hao Yang, and Qing\nHe. 2021. Pick and choose: a GNN-based imbalanced learning approach for fraud\ndetection. In Proceedings of the Web Conference. 3168\u20133177.\n[36] Yixin Liu, Zhao Li, Shirui Pan, Chen Gong, Chuan Zhou, and George Karypis.\n2021. Anomaly detection on attributed networks via contrastive self-supervised\nlearning. IEEE Transactions on Neural Networks and Learning Systems 33, 6 (2021),\n2378\u20132392.\n[37] Sitao Luan, Chenqing Hua, Minkai Xu, Qincheng Lu, Jiaqi Zhu, Xiao-Wen Chang,\nJie Fu, Jure Leskovec, and Doina Precup. 2023. When do graph neural networks\nhelp with node classification: Investigating the homophily principle on node\ndistinguishability. arXiv preprint arXiv:2304.14274 (2023).\n[38] Xiaoxiao Ma, Jia Wu, Shan Xue, Jian Yang, Chuan Zhou, Quan Z Sheng, Hui Xiong,\nand Leman Akoglu. 2021. A comprehensive survey on graph anomaly detection\nwith deep learning. IEEE Transactions on Knowledge and Data Engineering (2021).\n[39] Yao Ma, Xiaorui Liu, Neil Shah, and Jiliang Tang. 2021. Is Homophily a Ne-\ncessity for Graph Neural Networks?. In International Conference on Learning\nRepresentations.\n[40] Julian John McAuley and Jure Leskovec. 2013. From amateurs to connoisseurs:\nmodeling the evolution of user expertise through online reviews. In Proceedings\nof the 22nd International Conference on World Wide Web. 897\u2013908.\n[41] Miller McPherson, Lynn Smith-Lovin, and James M Cook. 2001. Birds of a feather:\nHomophily in social networks. Annual Review of Sociology 27, 1 (2001), 415\u2013444.\n[42] Zhibin Niu, Runlin Li, Junqi Wu, Dawei Cheng, and Jiawan Zhang. 2020. icon-\nviz: Interactive visual exploration of the default contagion risk of networked-\nguarantee loans. In 2020 IEEE conference on visual analytics science and technology\n(VAST). IEEE, 84\u201394.\n[43] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang.\n2019. Geom-GCN: Geometric graph convolutional networks. In International\nConference on Learning Representations.\n[44] Liudmila Prokhorenkova, Gleb Gusev, Aleksandr Vorobev, Anna Veronika Doro-\ngush, and Andrey Gulin. 2018. CatBoost: unbiased boosting with categorical\nfeatures. Advances in Neural Information Processing Systems 31 (2018).\n[45] Shebuti Rayana and Leman Akoglu. 2015. Collective opinion spam detection:\nBridging review networks and metadata. In Proceedings of the 21th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining. 985\u2013994.\n[46] Frank Rosenblatt. 1958. The perceptron: a probabilistic model for information\nstorage and organization in the brain. Psychological review 65, 6 (1958), 386.\n[47] Oleksandr Shchur, Maximilian Mumme, Aleksandar Bojchevski, and Stephan\nG\u00fcnnemann. 2018. Pitfalls of graph neural network evaluation. arXiv preprint\narXiv:1811.05868 (2018).\n[48] Fengzhao Shi, Yanan Cao, Yanmin Shang, Yuchen Zhou, Chuan Zhou, and Jia Wu.\n2022. H2-fdetector: A gnn-based fraud detector with homophilic and heterophilic\nconnections. In Proceedings of the Web Conference. 1486\u20131494.\n[49] Susheel Suresh, Vinith Budde, Jennifer Neville, Pan Li, and Jianzhu Ma. 2021.\nBreaking the limit of graph neural networks by improving the assortativity\nof graphs with local mixing patterns. In Proceedings of the 27th ACM SIGKDD\nConference on Knowledge Discovery & Data Mining. 1541\u20131551.\n[50] Jianheng Tang, Jiajin Li, Ziqi Gao, and Jia Li. 2022. Rethinking graph neural\nnetworks for anomaly detection. In International Conference on Machine Learning.\nPMLR, 21076\u201321089.\n[51] Lei Tang and Huan Liu. 2009. Relational learning via latent social dimensions.\nIn Proceedings of the 15th ACM SIGKDD International Conference on Knowledge\nDiscovery and Data Mining. 817\u2013826.\n[52] Chee-Wooi Ten, Junho Hong, and Chen-Ching Liu. 2011. Anomaly detection\nfor cybersecurity of the substations. IEEE Transactions on Smart Grid 2, 4 (2011),\n865\u2013873.\n[53] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in Neural Information Processing Systems 30 (2017).\n[54] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLi\u00f2, and Yoshua Bengio. 2018. Graph attention networks. In International Confer-\nence on Learning Representations.\n[55] Qianwen Wang, Kexin Huang, Payal Chandak, Marinka Zitnik, and Nils Gehlen-\nborg. 2022. Extending the nested model for user-centric XAI: A design study on\nGNN-based drug repurposing. IEEE Transactions on Visualization and Computer\nGraphics 29, 1 (2022), 1266\u20131276.\n[56] Tao Wang, Di Jin, Rui Wang, Dongxiao He, and Yuxiao Huang. 2022. Power-\nful graph convolutional networks with adaptive propagation mechanism for\nhomophily and heterophily. In Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 36. 4210\u20134218.\n[57] Yu Wang and Tyler Derr. 2021. Tree decomposed graph neural network. In Pro-\nceedings of the 30th ACM International Conference on Information and Knowledge\nManagement. 2040\u20132049.\n[58] Jiaying Wu and Bryan Hooi. 2023. DECOR: Degree-Corrected social graph\nrefinement for fake news detection. In Proceedings of the 29th ACM SIGKDD\nInternational Conference on Knowledge Discovery and Data Mining. 2582\u20132593.\n[59] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra.\n2022. Two sides of the same coin: Heterophily and oversmoothing in graph\nconvolutional neural networks. In 2022 IEEE International Conference on Data\nMining. IEEE, 1287\u20131292.\n[60] Yang Yang, Yuhong Xu, Yizhou Sun, Yuxiao Dong, Fei Wu, and Yueting Zhuang.\n2019. Mining fraudsters and fraudulent strategies in large-scale mobile social\nnetworks. IEEE Transactions on Knowledge and Data Engineering 33, 1 (2019),\n169\u2013179.\n[61] Zhilin Yang, William Cohen, and Ruslan Salakhudinov. 2016. Revisiting semi-\nsupervised learning with graph embeddings. In International Conference on Ma-\nchine Learning. PMLR, 40\u201348.\n[62] Tong Zhao, Chuchen Deng, Kaifeng Yu, Tianwen Jiang, Daheng Wang, and Meng\nJiang. 2020. Error-bounded graph anomaly loss for GNNs. In Proceedings of the\n29th ACM International Conference on Information and Knowledge Management.\n1873\u20131882.\n[63] Tong Zhao, Tianwen Jiang, Neil Shah, and Meng Jiang. 2021. A synergistic\napproach for graph anomaly detection with pattern mining and feature learning.\nIEEE Transactions on Neural Networks and Learning Systems 33, 6 (2021), 2393\u2013\n2405.\n[64] Xin Zheng, Yixin Liu, Shirui Pan, Miao Zhang, Di Jin, and Philip S Yu. 2022.\nGraph neural networks for graphs with heterophily: A survey. arXiv preprint\narXiv:2202.07082 (2022).\n[65] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai\nKoutra. 2020. Beyond homophily in graph neural networks: Current limitations\nand effective designs. Advances in Neural Information Processing Systems 33\n(2020), 7793\u20137804.\n[66] Daniel Z\u00fcgner and Stephan G\u00fcnnemann. 2019. Adversarial attacks on graph\nneural networks via meta learning. In International Conference on Learning Rep-\nresentations.\nAPPENDIX\nA\nTHEORETICAL RESULT\nA.1\nProof of Theorem 1\nTheorem 1. For a graph G \u223cCSBM-C(\ud835\udf410, \ud835\udf411,\ud835\udc51,\u210e0,\u210e1), for any\nnode \ud835\udc56in G, the smaller the value of |\u210e0 + \u210e1 \u22121|, the greater the\nprobability that h\ud835\udc56will be misclassified by h\u2019s optimal linear classifier.\nWe referred to the approach of previous work [39] and used the\ndistance from the expected value to the optimal decision boundary\nto approximate the probability of misclassification. Unlike their\nwork, which focused on proving when representations obtained by\nGNNs are better than the original representations, our focus is on\nthe impact of class homophily differences on node classification.\nProof. Firstly, since the distribution of the node\u2019s neighborhood\nis known, and each neighbor can be treated as independent random\nvariable, we can calculate the Gaussian distribution that h conforms\nto,\nh\ud835\udc56\u223c\n\uf8f1\uf8f4\uf8f4\uf8f2\n\uf8f4\uf8f4\uf8f3\n\ud835\udc41\n\u0010\n\u210e0\ud835\udf410 + (1 \u2212\u210e0)\ud835\udf411, I\n\ud835\udc51\n\u0011\nif \ud835\udc56\u2208\ud835\udc360\n\ud835\udc41\n\u0010\n(1 \u2212\u210e1)\ud835\udf410 + \u210e1\ud835\udf411, I\n\ud835\udc51\n\u0011\nif \ud835\udc56\u2208\ud835\udc361\n.\n(15)\nWe can calculate the middle point and the direction of \u0000E\ud835\udc500 (h), E\ud835\udc501 (h)\u0001,\nwhich is m = (1+\u210e0\u2212\u210e1)\ud835\udf410+(1\u2212\u210e0+\u210e1)\ud835\udf411\n2\n, w =\n\ud835\udf410\u2212\ud835\udf411\n\u2225\ud835\udf410\u2212\ud835\udf411\u22252 respectively.\nNoting that E(.) means mathematical expectation. From the anal-\nysis above, we can know that the optimal hyperplane that distin-\nguishes the two features is orthogonal to w and passes through the\npoint m. We define this optimal hyperplane\nP = {x|w\ud835\udc47x \u2212w\ud835\udc47m}.\n(16)\nIn this context, we only articulate the scenario where node \ud835\udc56\u2208\ud835\udc360,\nas the case for \ud835\udc56\u2208\ud835\udc361 belonging to is symmetrical and identical. We\ndefine the probability of h\ud835\udc56being misclassified as\nPmis(h\ud835\udc56) = P(w\ud835\udc47h\ud835\udc56\u2212w\ud835\udc47m \u22640), for \ud835\udc56\u2208\ud835\udc360.\n(17)\nBecause the variance of h\ud835\udc56is independent of \u210e0 and \u210e1, under the\nsame variance, the smaller the mathematical expectation of h\ud835\udc56is\nfrom the decision boundary, the greater the probability of h\ud835\udc56being\nmisclassified. We calculate the distance of expected value of h\ud835\udc56from\nthe optimal decision boundary P as\ndis(h\ud835\udc56) =\n\f\f\fw\ud835\udc47\u0000\u210e0\ud835\udf410 + (1 \u2212\u210e0)\ud835\udf411\n\u0001 \u2212w\ud835\udc47(1+\u210e0\u2212\u210e1)\ud835\udf410+(1\u2212\u210e0+\u210e1)\ud835\udf411\n2\n\f\f\f\n\u2225w\ud835\udc47\u22252\n(18)\n=\n\f\f\f\fw\ud835\udc47(\u210e0 + \u210e1 \u22121)\ud835\udf410 + (1 \u2212\u210e0 \u2212\u210e1)\ud835\udf411\n2\n\f\f\f\f\n(19)\n=\n\f\f\f\fw\ud835\udc47(\u210e0 + \u210e1 \u22121)(\ud835\udf410 \u2212\ud835\udf411)\n2\n\f\f\f\f\n(20)\n= |\u210e0 + \u210e1 \u22121| \u2225\ud835\udf410 \u2212\ud835\udf411\u22252\n2\n, for \ud835\udc56\u2208\ud835\udc360.\n(21)\nTherefore, the distance of h\ud835\udc56from the optimal decision boundary\nP is directly proportional to |\u210e0 + \u210e1 \u22121|, thus completing the\nproof.\n\u25a1\nA.2\nClass Homophily Variance under CSBM-C\nTheorem 2. For a graph G \u223cCSBM-C(\ud835\udf410, \ud835\udf411,\ud835\udc51,\u210e0,\u210e1), the Class\nHomophily Variance of graph G is Var( \u00af\nH)G = (\u210e0\u2212\u210e1)2\n4\n.\nProof. For node \ud835\udc56, if \ud835\udc56\u2208\ud835\udc360, then its homophily value H (\ud835\udc56) =\n\u210e0\u00b7\ud835\udc51\n\ud835\udc51\n= \u210e0, and if \ud835\udc56\u2208\ud835\udc361, then its homophily value H (\ud835\udc56) = \u210e1\u00b7\ud835\udc51\n\ud835\udc51\n= \u210e1.\nSince each class has the same contribution, the average inter-\nclass homophily is \ud835\udf07= \u210e0+\u210e1\n2\n. Then we can calculate the of graph\nG,\nVar( \u00af\nH)G =\n\u0010\n\u210e0 \u2212\u210e0+\u210e1\n2\n\u00112\n+\n\u0010\n\u210e1 \u2212\u210e0+\u210e1\n2\n\u00112\n2\n(22)\n= (\u210e0 \u2212\u210e1)2\n4\n.\n(23)\nThis completes the proof.\n\u25a1\nB\nDATA ANALYSIS ON MORE DATASETS\nIn order to more clearly illustrate the class homophily variance for\ndifferent graph classification tasks, we provide statistical analysis\nresults for 12 commonly used datasets, as shown in Table 5. These\ndatasets include four GAD datasets, four homophilic graphs, and\nfour heterophilic graphs. Notably, in these datasets, none have a\nCHV exceeding 0.05, whereas in GAD datasets, none are below 0.1.\nTaking the dataset Cornell as an example, it has the highest CHV\nTable 5: Homophily analysis on different datasets.\nTypes\nDataset\nVar( \u00af\nH)\nVar\ud835\udc36(H)\n\ud835\udf07\ud835\udc64\nAnomaly\nAmazon\n0.1655\n0.0082\n0.5579\nYelpChi\n0.1101\n0.0130\n0.5373\nBlogCatalog\n0.1378\n0.0099\n0.5737\nReddit\n0.1639\n0.0129\n0.5896\nHomophily\nCora\n0.0030\n0.0854\n0.8129\nPubMed\n0.0044\n0.1258\n0.7766\nCiteseer\n0.0200\n0.1477\n0.6861\nPhoto\n0.0171\n0.0433\n0.8293\nHeterophily\nTexas\n0.0198\n0.0774\n0.1080\nChameleon\n0.0093\n0.0472\n0.2550\nSquirrel\n0.0018\n0.0320\n0.2190\nCornell\n0.0364\n0.0771\n0.1844\nat 0.0364, which is still about three times lower than the lowest in\nthe GAD datasets, i.e., 0.1101 of YelpChi. This fact further confirms\nour view on the uniqueness of GAD datasets and is consistent with\nour previous discussion. The in-class homophily variance for all\ndatasets is relatively low, but it is also evident that the in-class\nhomophily variance of GAD datasets is smaller. Furthermore, the\nweighted average indicators further indicate that, compared to\nhomophilic and heterophilic graphs, GAD datasets do not show a\nsignificant tendency in terms of homophily and are all quite close\nto 0.5. In contrast, homophilic or heterophilic graphs tend to be\ncloser to either 0 or 1.\nC\nHETEROPHILY ATTACK\nIn this section, we provide a detailed algorithmic description of our\nheterophily attack. We based on the given adjacency matrix and\nthe attack ratio, calculate the total number of edges that need to\nbe modified. Next, we identify the nodes that need to be attacked.\nIf there is an edge between two nodes that are both marked with\nan attack label, then this edge may be removed. Afterward, we\nrandomly add edges between nodes with attack labels and those\nwithout until the predetermined number of modifications is reached.\nTo ensure the graph is undirected, we attack the edges above the\ndiagonal, zero out everything below the diagonal, and then obtain\nan undirected graph by adding the adjacency matrix to its transpose.\nPlease refer to Algorithm 1 for the pseudocode.\nD\nEXPERIMENT SETTINGS\nD.1\nWeighted Homophily Density Distribution\nHere, we clarify how to draw the Weighted Homophily Density\nDistribution graph. We first calculate the homophily value H (\ud835\udc63) for\neach node, as well as its weight \ud835\udc64, where \ud835\udc64is the reciprocal of the\nproportion of its class in all nodes. Then, we use a kernel density\nestimator to fit the data and form a curve for easier visualization.\nD.2\nDetailed Description of the Datasets\nHere, we provide a detailed description of each dataset.\nThe YelpChi dataset [45] collects hotel and restaurant reviews\nfrom Yelp. This dataset treats reviews as nodes and establishes\nthree types of relationships: 1) R-U-R: between reviews published\nTable 6: Statistics of four anomaly detection datasets.\nDataset\nType\nScenarios\nNode\nRelations\nEdge\nFeatures\nAnomalies\nRate\nYelpChi\nReal\nReview\n45,954\nR-U-R\n49,315\n32\n6,674\n14.52%\nR-S-R\n3,402,743\nR-T-R\n573,616\nAmazon\nReal\nReview\n11,944\nU-P-U\n175,608\n25\n821\n9.50%\nU-S-U\n3,566,479\nU-V-U\n1,036,737\nReddit\nReal\nSocial Networks\n10,984\n-\n175,608\n64\n366\n3.33%\nBlogCatalog\nInject\nSocial Networks\n5,196\n-\n171,743\n8,189\n300\n5.77%\nAlgorithm 1 Heterophily Attack\nRequire: The adjacency matrix of the graph \ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65,\nthe number of nodes\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc60, labels of all nodes\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc60, class\nneed to be attacked \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59, attack ratio \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\nEnsure: Modified adjacency matrix\n1: for \ud835\udc56= 1 to \ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5b\ud835\udc5c\ud835\udc51\ud835\udc52\ud835\udc60do\n2:\nfor \ud835\udc57= 1 to \ud835\udc56do\n3:\n\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc56, \ud835\udc57] \u21900\n4:\nend for\n5: end for\n6: \ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60\u2190sum(\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65) \u00d7 \ud835\udc5f\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\n7: \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2190indices of nodes where \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc60= \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\n8: Initialize \ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61as an empty list\n9: for each \ud835\udc56in \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65do\n10:\nfor each \ud835\udc57in \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65do\n11:\nif \ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc56, \ud835\udc57] \u22600 then\n12:\nAppend (\ud835\udc56, \ud835\udc57) to \ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61\n13:\nend if\n14:\nend for\n15: end for\n16: \ud835\udc5b\u2190min(\ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60, len(\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61))\n17: \ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65\u2190random \ud835\udc5bitems from \ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61\n18: for each \ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65in \ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65do\n19:\n(\ud835\udc56, \ud835\udc57) \u2190\ud835\udc5f\ud835\udc52\ud835\udc5a\ud835\udc5c\ud835\udc63\ud835\udc52_\ud835\udc59\ud835\udc56\ud835\udc60\ud835\udc61[\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc52\ud835\udc65]\n20:\n\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc56, \ud835\udc57] \u21900\n21:\n\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc57,\ud835\udc56] \u21900\n22: end for\n23: \ud835\udc56_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60\u2190indices of nodes where \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc60= \ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\n24: \ud835\udc57_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60\u2190indices of nodes where \ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\ud835\udc60\u2260\ud835\udc4e\ud835\udc61\ud835\udc61\ud835\udc4e\ud835\udc50\ud835\udc58_\ud835\udc59\ud835\udc4e\ud835\udc4f\ud835\udc52\ud835\udc59\n25: for \ud835\udc58in 1 to \ud835\udc5b\ud835\udc62\ud835\udc5a_\ud835\udc5a\ud835\udc5c\ud835\udc51\ud835\udc56\ud835\udc53\ud835\udc56\ud835\udc50\ud835\udc4e\ud835\udc61\ud835\udc56\ud835\udc5c\ud835\udc5b\ud835\udc60do\n26:\nrepeat\n27:\n\ud835\udc56\u2190random item from \ud835\udc56_\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60\n28:\n\ud835\udc57\u2190random item from \ud835\udc57_\ud835\udc57\ud835\udc56\ud835\udc5b\ud835\udc51\ud835\udc56\ud835\udc50\ud835\udc52\ud835\udc60\n29:\nif \ud835\udc56< \ud835\udc57and \ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc56, \ud835\udc57] = 0 then\n30:\n\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65[\ud835\udc56, \ud835\udc57] \u21901\n31:\nbreak\n32:\nend if\n33:\nuntil a new edge is added\n34: end for\n35: \ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65\u2190\ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65+ \ud835\udc4e\ud835\udc51\ud835\udc57\ud835\udc4e\ud835\udc50\ud835\udc52\ud835\udc5b\ud835\udc50\ud835\udc66_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65\u22a4\n36: return \ud835\udc5f\ud835\udc52\ud835\udc60\ud835\udc62\ud835\udc59\ud835\udc61_\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5f\ud835\udc56\ud835\udc65\nTable 7: Statistics of two generic datasets.\nDataset\nClasses\nFeatures\nNodes\nEdge\nPhoto\n8\n745\n7,650\n119,081\nPubMed\n3\n500\n19,717\n44,324\nby the same user, 2) R-S-R: between reviews of the same star level\nfor the same product, and 3) R-T-R: between reviews posted in\nthe same month for the same product. The Amazon dataset [40]\nfocuses on product reviews in the musical instruments category on\nAmazon. Here, the nodes are users, and it also includes three types\nof relationships: 1) U-P-U: between users who have reviewed at least\none common product, 2) U-S-U: between users who have given the\nsame star rating within a week, and 3) U-V-U: between users who\nhave the top 5% mutual review text similarities. We build the edges\nof YelpChi and Amazon following previous work [17]. BlogCatalog\n[51] is a social blog directory whose main function is to allow users\nto discover and follow other blog authors. In this community, each\nmember is considered as a node, and the interactions or follow\nrelationships between members are seen as treated connecting\nthese nodes. The attributes of these nodes are primarily used to\ndescribe various tags related to the users and their blog content.\nIn this network, some nodes are deliberately set with structural\nand contextual anomalies according to previous work [36]. Reddit\n[29], a well-known social media platform, has a forum post network\nthat includes users banned by the platform, marked as anomalies.\nWe load this dataset by PyGod [32] package. The content of these\nusers\u2019 posts is transformed into attribute vectors to represent their\ncharacteristics and behavioral patterns. The statistics of these four\ndatasets can be found in Table 6. Note that in the Amazon dataset,\nthere are 3305 nodes without labels.\nPubMed [61] is a large biomedical literature database organized\nin a graph structure. Each piece of literature in the PubMed dataset\ncan be considered a node, and the citation relationships between\nthese pieces of literature form the edges. Photo [47] is derived from\nAmazon\u2019s co-purchase network of products. In this dataset, nodes\ntypically represent products (in this context, products related to\nphotography), and edges represent the co-purchasing relationships\nbetween these products. The statistics of these two generic datasets\ncan be found in Table 7.\n0\n10\n20\n30\n40\n50\nPerturbation rate (%)\n80\n85\n90\n95\nAUC\n(a) Amazon\n0\n10\n20\n30\n40\n50\nPerturbation rate (%)\n55\n60\n65\n70\n75\n80\n85\n90\nAUC\n(a) YelpChi\nOurs\nGCN\nGAT\nBWGNN\nSparseGAD\nFigure 6: AUC under random attack.\n0\n5\n10\n15\n20\n25\nPerturbation rate (%)\n82\n84\n86\n88\n90\n92\n94\n96\n98\nAUC\n(a) Amazon\n0\n5\n10\n15\n20\n25\nPerturbation rate (%)\n60\n65\n70\n75\n80\n85\n90\nAUC\n(a) YelpChi\nOurs\nGCN\nGAT\nBWGNN\nSparseGAD\nFigure 7: AUC under non-targeted attack.\nD.3\nBaselines\nHere, we provide a detailed description of our comparison methods.\nThe models below are common classic GNNs. They are widely\nused in various GNN tasks and possess excellent versatility.\n\u2022 GCN [27] is a type of GNN that utilizes graph convolution\noperations to learn node representations in graph-structured\ndata. It updates each node\u2019s features by aggregating the\nfeature information of neighboring nodes.\n\u2022 GAT [54] introduces the attention mechanism into graph\nneural networks. In this model, nodes update their features\nby aggregating the features of their neighbors, weighted by\ndynamically computed attention scores.\n\u2022 GraphSAGE [21] is an inductive learning graph neural net-\nwork that updates the features of target nodes by sampling\nand aggregating a fixed-size set of neighboring nodes.\nThe following models are optimized for the heterophily in graphs.\n\u2022 MixHop [1] utilizes a novel graph convolutional layer. It\nimproves feature learning on graph data by blending infor-\nmation from neighbors at different hops.\n\u2022 GPRGNN [13] combines the general PageRank algorithm\nwith an adaptive mechanism. This is used for node impor-\ntance assessment and attribute prediction on graph data.\nModels specifically designed for anomaly detection usually exploit\nselection, pruning, and filtering techniques. We have chosen the\nstate-of-the-art models along with some classic works.\n\u2022 CAREGNN [17] utilizes label-aware similarity to identify\nneighborhoods, employs reinforcement learning to deter-\nmine the optimal number of neighbors, and aggregates se-\nlected neighbors across different relationships.\n\u2022 PCGNN [35] effectively handles class imbalance by selec-\ntively sampling nodes and neighbors for improved learning\nand detection accuracy.\n\u2022 AMNet [5] adaptively combines multi-frequency signals for\nimproved anomaly detection in graphs.\n\u2022 H2-FDetector [48] effectively identifies fraud by differen-\ntiating and aggregating information from both homophilic\nand heterophilic connections in a network.\n\u2022 BWGNN [50] leverages spectral and spatial localized band-\npass filters for enhanced anomaly detection in graphs, effec-\ntively addressing the right-shift spectral phenomenon.\n\u2022 GDN [19] effectively addresses anomaly detection in graphs\nby dynamically adjusting to structural distribution shifts,\noptimizing for both anomalies and normal nodes.\n\u2022 SparseGAD [20] enhances detection quality by sparsify-\ning graph structures and learning node representations to\nuncover hidden dependencies in relational data.\nD.4\nImplementation Details\nD.4.1\nExperiment Platform. We conducted experiments on a Linux\nserver equipped with an Intel Xeon 5220 CPU, a Tesla V100 32GB\nGPU, and 64GB of RAM. In addition, for experiments conducted on\nYelpChi, we utilized an A800 80GB GPU to accelerate computations.\nD.4.2\nExperiment Settings. In anomaly detection datasets, when\n40% of the labels are used for training, we use 30% as the valida-\ntion set and 30% as the test set. When 1% of the labels are used\nfor training, we use 49.5% as the validation set and 49.5% as the\ntest set. When testing on the Photo and PubMed datasets, we use\n40% as the training set, 30% as the validation set, and 30% as the\ntest set. We test on the validation set every 10 epochs, and select\nthe model that performs best on the validation set to evaluate on\nthe test set. Inspired by CAREGNN [17], small-batch data learning\nis beneficial in reducing model overfitting and improving model\nefficiency. Compared to random partitioning, we adopted the ap-\nproach of ClusterGCN [12] to divide subgraphs on the Amazon,\nYelpChi, and PubMed datasets. This was done to accelerate model\nlearning and reduce the occurrence of overfitting. Our experimental\nresults are the average of 10 runs. For all baselines, if the original\nhyperparameters are provided, we use them. If not, we perform\ngrid search using learning rates of in the set of {0.01, 0.003, 0.001}\nand the number of hidden layers of in {16, 32, 64}.\nE\nMORE EXPERIMENTS\nTo further demonstrate the robustness of our model, we also show\nthe precision changes of our model under two common attack\nmethods on GAD datasets. First, let\u2019s introduce the two attack\nmethods we adopted.\n\u2022 Random Attack: This type of attack randomly deletes a\ncertain proportion of edges and then randomly adds a certain\nproportion of edges to create edge perturbations.\n\u2022 Non-targeted Attack: This attack aims to target the entire\ngraph rather than reducing the accuracy of certain nodes.\nHere, we chose the classic DICE attack [66].\nIn this experiment, we also randomly split the datasets into training,\nvalidation, and test sets with a ratio of 4:3:3. We used homogeneity\ngraphs in both attacks, meaning the entire graph has only one\ntype of relationship. Therefore, the performance of some models is\ninconsistent with that in heterogeneity graphs. Notably, BWGNN\nshowed a significant decline on YelpChi.\nIt can be seen that vanilla GNNs, such as GAT and GCN, are\nmore susceptible to attacks, showing a significant decline in per-\nformance from their original levels. In contrast, the GAD models\ndemonstrates stronger robustness, with a smaller decline when\nfaced with various attacks compared to vanilla GNNs. It is evident\nthat our model exhibits strong robustness when facing multiple\nattacks, outperforming all baseline models comprehensively, and\nalso has a relatively small decline in performance. These results\nfully demonstrate the effectiveness of our edge generation strategy\nin reducing the impact of attacks, thereby ensuring the model\u2019s\nrobust performance in a changing environment.\n",
    "2302.06430": "Published as a conference paper at ICLR 2024\nDEEP ORTHOGONAL HYPERSPHERE COMPRESSION\nFOR ANOMALY DETECTION\nYunhe Zhang1,2\nYan Sun1,3\nJinyu Cai4\nJicong Fan1,2\u2217\n1School of Data Science, The Chinese University of Hong Kong, Shenzhen, China\n2Shenzhen Research Institute of Big Data, Shenzhen, China\n3School of Computing, National University of Singapore, Singapore\n4Institute of Data Science, National University of Singapore, Singapore\nzhangyhannie@gmail.com\nyansun@comp.nus.edu.sg\njinyucai1995@gmail.com fanjicong@cuhk.edu.cn\nABSTRACT\nMany well-known and effective anomaly detection methods assume that a rea-\nsonable decision boundary has a hypersphere shape, which however is difficult to\nobtain in practice and is not sufficiently compact, especially when the data are in\nhigh-dimensional spaces. In this paper, we first propose a novel deep anomaly de-\ntection model that improves the original hypersphere learning through an orthog-\nonal projection layer, which ensures that the training data distribution is consistent\nwith the hypersphere hypothesis, thereby increasing the true positive rate and de-\ncreasing the false negative rate. Moreover, we propose a bi-hypersphere compres-\nsion method to obtain a hyperspherical shell that yields a more compact decision\nregion than a hyperball, which is demonstrated theoretically and numerically. The\nproposed methods are not confined to common datasets such as image and tabular\ndata, but are also extended to a more challenging but promising scenario, graph-\nlevel anomaly detection, which learns graph representation with maximum mutual\ninformation between the substructure and global structure features while exploring\northogonal single- or bi-hypersphere anomaly decision boundaries. The numeri-\ncal and visualization results on benchmark datasets demonstrate the superiority of\nour methods in comparison to many baselines and state-of-the-art methods.\n1\nINTRODUCTION\nAnomaly detection plays a crucial role in a variety of applications, including fraud detection in fi-\nnance, fault detection in chemical engineering (Fan & Wang, 2014), medical diagnosis, and the iden-\ntification of sudden natural disasters (Aggarwal, 2017). Significant research has been conducted on\nanomaly detection using both tabular and image data (Ruff et al., 2018; Fan & Chow, 2020; Goyal\net al., 2020; Chen et al., 2022; Liznerski et al., 2021; Sohn et al., 2021; Liznerski et al., 2021). A\ncommon setting is to train a model solely on normal data to distinguish unusual patterns from abnor-\nmal ones, which is usually referred to as one-class classification (Sch\u00a8olkopf et al., 1999; Tax & Duin,\n2004; Ruff et al., 2018; Pang et al., 2021; Seliya et al., 2021). For example, the support vector data\ndescription (SVDD) proposed by (Tax & Duin, 2004) obtains a spherically shaped boundary around\na dataset, where data points falling outside the hypersphere will be detected as anomalous data. The\ndeep SVDD proposed by (Ruff et al., 2018) trains a neural network to transform the input data into\na space in which normal data are distributed in a hyperspherical decision region. Regarding the con-\ncern that finite training normal data generating distribution may be incomplete or draw from many\nsets of categories, Kirchheim et al. (2022) proposed a supervised multi-class hypersphere anomaly\ndetection method. Han et al. (2022) provided a review and comparison of many anomaly detection\nmethods. Compared with common anomaly detection, there is relatively little work on graph-level\ndata, despite the fact that graph anomaly detection has application scenarios in various problems,\nsuch as identifying abnormal communities in social networks, discriminating whether human-brain\nnetworks are healthy (Lanciano et al., 2020), or detecting unusual protein structures in biological\n\u2217Corresponding author.\n1\narXiv:2302.06430v2  [cs.LG]  5 May 2024\nPublished as a conference paper at ICLR 2024\n-4\n-2\n-4\n0\n4\n-2\n2\n2\n0\n0\n2\n4\n-2\n4\n-4\n-5\n-5\n-5\n0\n0\n0\n5\n5\n5\n-2\n6\n0\n4\n4\n2\n2\n2\n4\n0\n6\n0\n-2\n-4\n-2\n-6\n-2\n6\n0\n4\n5\n2\n2\n4\n6\n0\n0\n-2\n-4\n-5\nInput\n\u2026\n\u2026\n\u2026\n\u2026\nStandardized \nRepresentation\nOrthogonal \nProjector\nOptimization (4)\nOptimization (8)\nRepresentation Learning Module\n\u2026\n\u2026\n\u2026\n\u2026\nLatent \nRepresentation\nImage Data\nGraph Data\nTabular Data\nFigure 1: Architecture of the proposed models (right top: DOHSC; right bottom: DO2HSC). Herein,\n2-D visualizations show the trends of training data when applying two optimizations and 3-D visu-\nalizations illustrate the detection results obtained by them, respectively.\nexperiments. The target of graph-level anomaly detection is to explore a regular group pattern and\ndistinguish the abnormal manifestations of the group. However, graph data are inherently complex\nand rich in structural and relational information. This characteristic facilitates the learning of pow-\nerful graph-level representations with discriminative patterns in many supervised tasks (e.g., graph\nclassification) but brings many obstacles to unsupervised learning. Graph kernels (Kriege et al.,\n2020) are useful for both supervised and unsupervised graph learning problems. For graph-level\nanomaly detection, graph kernels can be combined with one-class SVM (Sch\u00a8olkopf et al., 1999)\nor SVDD (Tax & Duin, 2004). This is a two-stage approach that cannot ensure that implicit fea-\ntures are sufficiently expressive for learning normal data patterns. Recently, researchers proposed\nseveral end-to-end graph-level anomaly detection methods (Ma et al., 2022; Zhao & Akoglu, 2021;\nQiu et al., 2022). For example, Ma et al. (2022) proposed a global and local knowledge distilla-\ntion method for graph-level anomaly detection. Zhao & Akoglu (2021) combined the deep SVDD\nobjective function and graph isomorphism network to learn a hypersphere of normal samples.\nAlthough the hypersphere assumption is reasonable and practical, and has led to many successful\nalgorithms (Tax & Duin, 2004; Ruff et al., 2018; 2020; Kirchheim et al., 2022; Zhao & Akoglu,\n2021) for anomaly detection, it still exhibits the following three limitations:\n\u2022 First, minimizing the sum of squares of the difference between each data point and the cen-\nter cannot guarantee that the learned decision boundary is a standard hypersphere. Instead,\none may obtain a hyperellipsoid (see Figure 2) or other shapes that are inconsistent with\nthe assumption, which will lower the detection accuracy.\n\u2022 The second is that in high-dimensional space the normal data enclosed by a hypersphere are\nall far away from the center (see Figure 3 and Proposition 2) with high probability. It means\nthat there is no normal data around the center of the hypersphere; hence, the normality in\nthe region is not supported, whereas anomalous data can still fall into the region. It\u2019s related\nto the soap-bubble phenomenon of high-dimensional statistics (Vershynin, 2018).\n\u2022 Last but not least, in high-dimensional space, one hypersphere is not sufficiently compact.\nIn other words, the distribution of normal data in the hypersphere is extremely sparse be-\ncause of the high dimensionality and limited training data. A high sparsity increases the\nrisk of detecting anomalous data as normal.\nTo address these issues, we propose two anomaly detection methods. The first one, Deep Orthogonal\nHypersphere Contraction (DOHSC), utilizes an orthogonal projection layer to render the deci-\nsion region more hyperspherical and compact to reduce evaluation errors. The second one, Deep\nOrthogonal Bi-Hypersphere Compression (DO2HSC), aims to solve the problem of the soap-bubble\nphenomenon and incompactness. From a 2-dimensional view, DO2HSC limits the decision area\n(of normal data) to an interval enclosed by two co-centered hyperspheres, and similarly learns\nthe orthogonality-projected representation. Accordingly, a new detection metric is proposed for\nDO2HSC. The framework of the methods mentioned above is shown in Figure 1. In addition,\n2\nPublished as a conference paper at ICLR 2024\ngraph-level extensions of DOHSC and DO2HSC are conducted to explore a more challenging task,\ni.e., graph-level anomaly detection. In summary, our contributions are three-fold.\n\u2022 First, we present a hypersphere contraction algorithm for anomaly detection tasks with an\northogonal projection layer to promote training data distribution close to the standard hy-\npersphere, thus avoiding inconsistencies between assessment criteria and actual conditions.\n\u2022 Second, we propose the deep orthogonal bi-hypersphere compression model to construct\na decision region enclosed by two co-centered hyperspheres, which has theoretical sup-\nports and solves the problem of soap-bubble phenomenon and incompactness of the single-\nhypersphere assumption.\n\u2022 Finally, we extend our methods to graph-level anomaly detection and conduct abundant\nexperiments to show the superiority of our methods over the state-of-the-art.\n2\nDEEP ORTHOGONAL HYPERSPHERE COMPRESSION\n2.1\nVANILLA MODEL\nDenote a data matrix by X \u2208Rn\u00d7d with n instances and d features, we first construct an\nauto-encoder and utilize the latent representation Z = f enc\nW (X) to initialize a decision region\u2019s\ncenter c according to Deep SVDD (Ruff et al., 2018), i.e, c =\n1\nn\nPn\ni=1 f enc\nW (xi), where xi\ndenotes the transpose of the i-th row of X and f enc\nW (\u00b7) is an L-layer representation learning\nmodule with parameters W\n= {Wl, bl}L\nl=1.\nWith this center, we expect to optimize the\nlearned representation of normal data to be distributed as close to it as possible, so that the\nunexpected anomalous data falling out of this hypersphere would be detected.\nThe Hyper-\nsphere Contraction optimization problem for anomaly detection is first formulated as follows:\n-6\n-4\n-2\n2\n4\n6\n-5\n-4\n-3\n-2\n-1\n1\n2\n3\n4\n5\n-6\n-4\n-2\n2\n4\n6\n-5\n-4\n-3\n-2\n-1\n1\n2\n3\n4\n5\nOrthogonal\nProjection\nFigure 2: Toy example of decision bound-\naries with and without the orthogonal pro-\njection layer.\nBlue circle: assumed deci-\nsion boundary; black ellipse: actual decision\nboundary; purple points: normal data; red\npoints: abnormal data.\nmin\nW\n1\nn\nn\nX\ni=1\n\u2225f enc\nW (xi) \u2212c\u22252 + \u03bb\n2\nL\nX\nl=1\n\u2225Wl\u22252\nF , (1)\nwhere the regularization is to reduce over-fitting.\n2.2\nORTHOGONAL PROJECTION LAYER\nAlthough the goal of Optimization (1) is to learn a\nhypersphere as the decision boundary, we find that\nit usually yields a hyperellipsoid or even more irreg-\nular shapes (please refer to Section I in the supple-\nmentary material). This phenomenon would lead to\ninaccuracies in the testing stage, because the evalu-\nation was based on the hypersphere assumption. Figure 2 illustrates an intuitive example. In the\nleft plot, the learned decision boundary (black ellipse) does not match the assumption (blue circle),\nwhich decreases the true-positive (TP) rate and increases the false-positive (FP) rate. Thus the de-\ntection precision, calculated as\nT P \u2193\nT P \u2193+F P \u2191, decreases compared to the right plot. The inconsistency\nbetween the assumption and the actual solution stems from the following two points: 1) the learned\nfeatures have different variances and 2) the learned features are correlated. Clearly, these two issues\ncannot be avoided by solely solving Optimization (1).\nTo solve these issues, as shown in the right plot of Figure 2, we append an orthogonal projection\nlayer to the feature layer, i.e., the output of f enc\nW . Note that we pursue orthogonal features of latent\nrepresentation rather than computing the projection onto the column or row space of Z \u2208Rn\u00d7k,\nwhich is equivalent to performing Principal Component Analysis (PCA) (Wold et al., 1987) and us-\ning standardized principal components. Our experiments also justify the necessity of this projection\nstep and the standardization process, which will be discussed further in Appendix K. Specifically,\nthe projection layer is formulated as\n\u02dcZ = Proj\u0398(Z) = ZW\u2217, subject to \u02dcZ\u22a4\u02dcZ = Ik\u2032\n(2)\n3\nPublished as a conference paper at ICLR 2024\nwhere \u0398 := {W\u2217\u2208Rk\u00d7k\u2032} is the set of projection parameters, Ik\u2032 denotes an identity matrix,\nand k\u2032 is the projected dimension. To achieve (2), one may consider adding a regularization term\n\u03b1\n2 \u2225\u02dcZ\u22a4\u02dcZ\u2212Ik\u2032\u22252\nF with large enough \u03b1 to the objective, which is not very effective and will lead to one\nmore tuning hyperparameter. Instead, we propose to achieve (2) via singular value decomposition:\nU\u039bV\u22a4= Z, W := Vk\u2032\u039b\u22121\nk\u2032 .\n(3)\nAssume that there are b samples in one batch, \u039b = diag(\u03c11, \u03c12, ..., \u03c1b) and V are the diago-\nnal matrix with singular values and right-singular matrix of Z, respectively. It is noteworthy that\nVk\u2032 := [v1, ..., vk\u2032] denotes the first k\u2032 right singular vectors, and \u039bk\u2032 := diag(\u03c11, ..., \u03c1k\u2032). In each\nforward propagation epoch, the original weight parameter is substituted into a new matrix W\u2217in\nthe subsequent loss computations.\n2.3\nANOMALY DETECTION\nAttaching with an orthogonal projection layer, the improved initialization of the center is rewritten\nin the following form \u02dcc = 1\nn\nPn\ni=1 \u02dczi, which will be fixed until optimization is completed. The final\nobjective function for anomaly detection tasks in a mini-batch would become\nmin\n\u0398,W\n1\nb\nb\nX\ni=1\n\u2225\u02dczi \u2212\u02dcc\u22252 + \u03bb\n2\nX\nW\u2208W\n\u2225W\u22252\nF .\n(4)\nAfter the training stage, the decision boundary \u02c6r will be fixed, which is calculated based on the 1\u2212\u03bd\npercentile of the training data distance distribution:\n\u02c6r = arg min\nr\nP(D \u2264r) \u2265\u03bd\n(5)\nwhere D := {di}N\ni=1 follows a sampled distribution P, and di = \u2225\u02dczi \u2212\u02dcc\u2225. Accordingly, the\nanomalous score of i-th instance is defined as follows:\nsi = d2\ni \u2212\u02c6r2\n(6)\nwhere s = (s1, s2, . . . , sn). It is evident that when the score is positive, the instance is identified as\nabnormal, and the opposite is considered normal.\nThe detailed procedures are summarized in Algorithm 1 (see Appendix A), which is termed as\nDOHSC. DOHSC is easy to implement and can ensure that the actual decision boundary is close to\na hypersphere. Our numerical results in Section 4 will show the effectiveness.\n3\nDEEP ORTHOGONAL BI-HYPERSPHERE COMPRESSION\n3.1\nMOTIVATION AND THEORETICAL ANALYSIS\nAs mentioned in the third paragraph of Section 1, the hypersphere assumption may encounter the\nsoap-bubble phenomenon and incompactness. They can be succinctly summarized as\n\u2022 High-dimensional data enclosed by a hypersphere are far from the center naturally, which\nmeans the normality within a wide range of distance is not supported.\n\u2022 In high-dimensional space, the data distribution within a hypersphere is highly sparse,\nwhich leads to an incompact decision region and, hence, a heightened risk of detecting\nabnormal data as normal.\nIn this section, we present a detailed analysis. Let the anomaly score be determined using \u2225z \u2212c\u2225\nwhere c denotes the centroid. The original evaluation of anomaly detection compares the score\nwith a threshold \u02c6r determined by a certain quantile (e.g., 0.95). Specifically, if \u2225z \u2212c\u2225\u2265\u02c6r, z is\nabnormal. This target promoted the location of most samples near the origin. However, empirical\nexploration has found that most samples are far away from their origin in a high-dimensional space.\nTaking Gaussian distributions as an example, the distributions would look like a soap-bubble1, which\n1https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/\n4\nPublished as a conference paper at ICLR 2024\nmeans the high-dimensional normal data may be more likely to locate in the interval region of bi-\nhypersphere instead of a simple hypersphere. Vershynin (2018) stated that the typical set, where\ndata has information closest to the expected entropy of the population, of a Gaussian is the thin shell\nwithin a distance from the origin, just like the circumstances shown in Figure 3. The higher the\ndimensionality of the data, the more sampled instances are from the center. We also supplement\nthe anomaly detection simulation of high-dimensional Gaussian data in Appendix C to show the\nsignificant meaning of bi-hypersphere learning. This is formally proven by the following proposition\n(derived from Lemma 1 of (Laurent & Massart, 2000)):\nProposition 1. Suppose z1, z2, \u00b7 \u00b7 \u00b7 , zn are sampled from N(0, Id) independently. Then, for any zi\nand all t \u22650, the following inequality holds.\nP\n\u0014\n\u2225zi\u2225\u2265\nq\nd \u22122\n\u221a\ndt\n\u0015\n\u22651 \u2212e\u2212t.\nThe proposition shows that when the dimension is high, each zi is outside the hypersphere of radius\nr\u2032 :=\np\nd \u22122\n\u221a\ndt with a probability of at least 1 \u2212e\u2212t. When r\u2032 is closer to \u02c6r (refer to equation 5),\nnormal data are more likely to be away from the center (see Figure 3).\nFigure 3: Soap-bubble phenomenon showed\nby the histogram of distances from the cen-\nter of 104 samples drawn from N(0, Id). In\nhigh-dimensional space, almost all data are\nfar from the center.\nNote that, in anomaly detection, \u02dczi (e.g., the learned\nlatent representation) is not necessarily an isotropic\nGaussian. However, we obtain the following result.\nProposition 2. Let zi = \u02dczi, i = 1, . . . , N and\nlet f : Rk \u2192Rk be an \u03b7-Lipschitz function such\nthat s = f(z) are isotropic Gaussian N(\u00afc, Ik). Let\nc be a predefined center of {zi}N\ni=1 and suppose\n\u2225\u00afc \u2212f(c)\u2225\u2264\u03f5. Then for any zi and all t \u22650,\nthe following inequality holds:\nP\n\u0014\n\u2225zi \u2212c\u2225\u2265\u03b7\u22121\n\u0012q\nk \u22122\n\u221a\nkt \u2212\u03f5\n\u0013\u0015\n\u22651\u2212e\u2212t.\nThe proposition (proved in Appendix D) indicates\nthat most data (N \u2032) satisfy \u2225z \u2212c\u2225\u2265r\u2032\n:=\n\u03b7\u22121 \u0010p\nk \u22122\n\u221a\nkt \u2212\u03f5\n\u0011\nwith a probability of approximately\n\u0000N \u2032\nN\n\u0001\n(1 \u2212e\u2212t)N \u2032e\u2212t(N\u2212N \u2032), where\nr\u2032 is close to \u02c6r. This means there is almost no normal training data within the range [0, r\u2032], i.e.\nnormality within the range is not supported by the normal training data. An intuitive example is:\nExample 1. Assume the hypersphere is centered at the origin. Consider a data point with all\nfeatures very close or even equal to zero. This point is very different from the normal training data\nand should be abnormal data. However, according to the metric \u2225\u02dcz \u2212\u02dcc\u2225, this point is still in the\nhypersphere and is finally detected as normal data.\nGiven the implications of Proposition 2, we recognize that in high-dimensional spaces, traditional\ndistance-to-center based anomaly scores (equation 6) may lose their reliability due to the concentra-\ntion of measure phenomenon. Figure 4 shows a real example of abnormal data falling into a region\nclose to the center of the hypersphere. In addition to the soap-bubble phenomenon, we claim that\nthe data distribution in a high-dimensional sphere is very sparse when the number n of the training\ndata is limited. This means that when n is not sufficiently large, there could be large empty holes\nor regions in which normality is not supported because of the randomness. It is not sufficient to\ntreat data that fall into holes or regions as normal data. Intuitively, for example, the distribution of n\nrandom points in a 3-D sphere of radius r is much sparser than that in a 2-D circle of radius r. More\nformally, in a hypersphere of radius r in k-dimensional space, the expected number of data points\nper unit volume is \u03f1k = n\u0393(k/2+1)\n\u03c0k/2rk\n, where \u0393 is Euler\u2019s gamma function. When r is not too small,\n\u03f1k increases rapidly as k decreases. See below.\nExample 2. Suppose n = 1000, r = 5. Then \u03f12 \u224812.7, \u03f15 \u22480.06, and \u03f110 < 0.0001.\nWe hope to construct a more compact decision region, one with a much larger \u03f1k, without changing\nthe feature dimensions.\n5\nPublished as a conference paper at ICLR 2024\n   \n   \n   \n   \n   \n   \n        \n \n    \n    \n    \n    \n   \n    \n    \n    \n    \n   \n                 \n              \n           \n                 \n   \n    \n   \n    \n   \n    \n   \n        \n \n \n \n \n \n \n \n \n \n                 \n           \n                 \n(a) Training Data\n(b) Testing Data\nCOX2 1; DOHSC\nFigure 4: Illustration of inevitable flaws in DOHSC on both the training and testing data of COX2.\nLeft: the \u21132-norm distribution of 4-dimensional distances learned from the real dataset; Right: the\npseudo-layout in two-dimensional space sketched by reference to the empirical distribution.\n3.2\nARCHITECTURE OF DO2HSC\nTo solve the issues discussed in the previous section, we propose an improved approach, DO2HSC,\nwhich sets the decision boundary as an interval region between two co-centered hyperspheres. This\ncan narrow the scope of the decision area to induce normal data to fill as much of the entire interval\narea as possible.\nAfter the same representation learning stage, we first utilize the DOHSC model for a few epochs\nto initialize the large radius rmax and small radius rmin of the interval area according to the 1 \u2212\u03bd\npercentile and \u03bd of the sample distance distribution, respectively. The aforementioned descriptions\ncan be mathematically denoted as follows:\nrmax = arg min\nr\nP(D \u2264r) \u2265\u03bd,\nrmin = arg min\nr\nP(D \u2264r) \u22651 \u2212\u03bd.\n(7)\nAfter fixing rmax and rmin, the objective function of DO2HSC is formulated as follows:\nmin\n\u0398,W\n1\nb\nb\nX\ni=1\n(max{di, rmax} \u2212min{di, rmin}) + \u03bb\n2\nX\nW\u2208W\n\u2225W\u22252\nF .\n(8)\nThis decision loss has the lowest bound rmax \u2212rmin. In addition, the evaluation standard of the test\ndata must also be changed based on this interval structure. Specifically, all instances located in the\ninner hypersphere and outside the outer hypersphere should be identified as anomalous individuals;\nonly those located in the interval area should be regarded as normal data. We reset a new score\nfunction to award the positive samples beyond [rmin, rmax] while punishing the negative samples\nwithin this range. Accordingly, the distinctive scores are calculated by\nsi = (di \u2212rmax) \u00b7 (di \u2212rmin),\n(9)\nwhere i \u2208{1, ..., n}. In this manner, we can also effectively identify a sample\u2019s abnormality based\non its score. In general, an improved deep anomaly detection algorithm changes the decision bound-\nary and makes the normal area more compact. Furthermore, a new practical evaluation was proposed\nto adapt to the improved detection method. Finally, we summarize the detailed optimization proce-\ndures in Algorithm 2 (see Appendix A).\nThe following proposition justifies the superiority of bi-hypersphere compression over single-\nhypersphere contraction from another perspective:\nProposition 3. Suppose the number of normal training data is n, the radius of the hypersphere\ngiven by DOHSC is rmax, and the radii of the hyperspheres given by DO2HSC are rmax and rmin\nrespectively. Without loss of generality, assume that all the training data are included in the learned\ndecision regions. The ratio between the support densities of the decision regions given by DO2HSC\nand DOHSC is \u03ba =\n1\n1\u2212(rmin/rmax)k .\nIn the proposition (proved in Appendix E), density is defined as the number of normal data in unit\nvolume. A higher density indicates a higher confidence in treating a data point falling into the deci-\nsion region as normal data, or treating a data point falling outside the decision region as anomalous\ndata. Because \u03ba > 1, the DO2HSC provides a more reliable decision region than the DOHSC. The\nadvantage of the DO2HSC over the DOHSC is more significant when k is smaller or rmin is closer\nto rmax. Here are some examples.\n6\nPublished as a conference paper at ICLR 2024\nExample 3. Suppose k = 50. When rmin/rmax = 0.9, \u03ba \u22481.01. When rmin/rmax = 0.99, \u03ba \u22482.5.\nSuppose k = 10. When rmin/rmax = 0.9, \u03ba \u22481.5. When rmin/rmax = 0.99, \u03ba \u224810.5.\n3.3\nGENERALIZATION TO GRAPH-LEVEL ANOMALY DETECTION\nGiven a set of graphs G = {G1, ..., GN} with N samples, the proposed model aims to learn a\nk-dimensional representation and then set a soft boundary accordingly. In this paper, the Graph\nIsomorphism Network (GIN) (Xu et al., 2019) is employed to obtain the graph representation in\nthree stages: first, input the graph data and integrate neighbors of the current node (AGGREGATE);\nsecond, combine neighbor and current node features (CONCAT); and finally, all node information\n(READOUT) is integrated into one global representation. Mathematically, the i-th node features of\nl-th layer and the global features of its affiliated j-th graph are denoted by\nzi\n\u03a6 = CONCAT({z(l)\ni }L\nl=1),\nZ\u03a6(Gj) = READOUT({zi\n\u03a6}|Gj|\ni=1 ),\n(10)\nwhere zi\n\u03a6 \u2208R1\u00d7k and Z\u03a6(Gj) \u2208R1\u00d7k. To integrate the contained information and enhance the\ndifferentiation between node- and global-level representations, we append additional fully connected\nlayers denoted by the forms M\u03a5(\u00b7) and T\u03a8(\u00b7), respectively, where \u03a5 and \u03a8 are the parameters of\nthe added layers. So the integrated node-level and graph-level representations are\nhi\n\u03a6,\u03a5 := M\u03a5(zi\n\u03a6); H\u03a6,\u03a8(Gj) := T\u03a8(Z\u03a6(Gj)).\n(11)\nTo better capture the local information, we utilize the batch optimization property of neural networks\nto maximize the mutual information (MI) between local and global representations in each batch\nG \u2286G, which is defined by (Sun et al., 2020) as follows:\n\u02c6\u03a6, \u02c6\u03a8, \u02c6\u03a5 = arg max\n\u03a6,\u03a8,\u03a5\nI\u03a6,\u03a8,\u03a5 (h\u03a6,\u03a5, H\u03a6,\u03a8(G)) .\n(12)\nSpecifically, the mutual information estimator I\u03a6,\u03a8,\u03a5 follows the Jensen-Shannon MI estimator\n(Nowozin et al., 2016) with a positive-negative sampling method, as follows:\nI\u03a6,\u03a8,\u03a5 (h\u03a6,\u03a5, H\u03a6,\u03a8(G)) :=\nX\nGj\u2208G\n1\n|Gj|\nX\nu\u2208Gj\nI\u03a6,\u03a8,\u03a5\n\u0000hu\n\u03a6,\u03a5(Gj), H\u03a6,\u03a8(G)\n\u0001\n=\nX\nGj\u2208G\n1\n|Gj|\nX\nu\u2208Gj\nh\nE\n\u0000\u2212\u03c3\n\u0000\u2212hu\n\u03a6,\u03a5(x+) \u00d7 H\u03a6,\u03a8(x)\n\u0001 \u0001\n\u2212E\n\u0000\u03c3\n\u0000hu\n\u03a6,\u03a5(x\u2212) \u00d7 H\u03a6,\u03a8(x)\n\u0001 \u0001i\n,\n(13)\nwhere \u03c3(z) = log(1 + ez). For x as an input sample graph, we calculate the expected mutual\ninformation using its positive samples x+ and negative samples x\u2212, which are generated from the\ndistribution across all graphs in a subset. Given that G = (VG, EG) and the node set VG = {vi}|G|\ni=1,\nthe positive and negative samples are divided in this manner: x+ = xij if vi \u2208Gj otherwise,\nx+ = 0. Additionally, x\u2212produces the opposite result for each of the above conditions. Thus, a\ndata-enclosing decision boundary is required for our anomaly detection task. Let \u02dcH\u03a6,\u03a8,\u0398(G) =\nProj\u0398(H\u03a6,\u03a8(G)), the center of this decision boundary should be initialized through\n\u02dcc = 1\nN\nN\nX\ni=1\n\u02dcH\u03a6,\u03a8,\u0398(Gi).\n(14)\nCollectively, the weight parameters of \u03a6, \u03a8 and \u03a5 are Q := \u03a6 \u222a\u03a8 \u222a\u03a5, and let R(Q) =\nP\nQ\u2208Q \u2225Q\u22252\nF , we formulate the objective function of the graph-level DOHSC as\nmin\n\u0398,\u03a6,\u03a8,\u03a5\n1\n|G|\n|G|\nX\ni=1\n\u2225\u02dcH\u03a6,\u03a8,\u0398(Gi) \u2212\u02dcc\u22252 \u2212\u03bb\nX\nG\u2208G\nI\u03a6,\u03a8,\u03a5\n\u0010\nh\u03a6,\u03a5, \u02dcH\u03a6,\u03a8,\u0398(G)\n\u0011\n+ \u00b5\n2 R(Q),\n(15)\nwhere |G| denotes the number of graphs in batch G and \u03bb is a trade-off factor, the third term\nis a network weight decay regularizer with the hyperparameter \u00b5. Correspondingly, the objective\nfunction of graph-level DO2HSC is\nmin\n\u0398,\u03a6,\u03a8,\u03a5\n1\n|G|\n|G|\nX\ni=1\n(max{di, rmax} \u2212min{di, rmin}) \u2212\u03bb\nX\nG\u2208G\nI\u03a6,\u03a8,\u03a5\n\u0010\nh\u03a6,\u03a5, \u02dcH\u03a6,\u03a8,\u0398(G)\n\u0011\n+ \u00b5\n2 R(Q).\n(16)\n7\nPublished as a conference paper at ICLR 2024\n4\nNUMERICAL RESULTS\n4.1\nEXPERIMENTS ON IMAGE DATA\nDatasets: Two image datasets (Fashion-MNIST, CIFAR-10) are chosen to conduct this experiment.\nPlease refer to the detailed statistic descriptions in Appendix F.\nTable 1: Average AUCs (%) in one-class anomaly detection\non CIFAR-10. * denotes we run the official released code to\nobtain the results, and the top two results are marked in bold.\nNormal Class\nAirplane\nAuto\nMobile\nBird\nCat\nDeer\nDog\nFrog\nHorse\nShip\nTruck\nDeep SVDD\n61.7\n65.9\n50.8\n59.1\n60.9\n65.7\n67.7\n67.3\n75.9\n73.1\nOCGAN\n75.7\n53.1\n64.0\n62.0\n72.3\n62.0\n72.3\n57.5\n82.0\n55.4\nDROCC*\n82.1\n64.8\n69.2\n64.4\n72.8\n66.5\n68.6\n67.5\n79.3\n60.6\nHRN-L2\n80.6\n48.2\n64.9\n57.4\n73.3\n61.0\n74.1\n55.5\n79.9\n71.6\nHRN\n77.3\n69.9\n60.6\n64.4\n71.5\n67.4\n77.4\n64.9\n82.5\n77.3\nPLAD\n82.5\n80.8\n68.8\n65.2\n71.6\n71.2\n76.4\n73.5\n80.6\n80.5\nDOHSC\n80.3\n(0.0)\n81.0\n(0.0)\n70.4\n(1.9)\n68.0\n(1.8)\n72.1\n(0.0)\n72.4\n(2.1)\n83.1\n(0.0)\n74.1\n(0.4)\n83.3\n(0.7)\n81.1\n(0.7)\nDO2HSC\n81.3\n(0.2)\n82.7\n(0.3)\n71.3\n(0.4)\n71.2\n(1.3)\n72.9\n(2.1)\n72.8\n(0.2)\n83.0\n(0.6)\n75.5\n(0.4)\n84.4\n(0.5)\n82.0\n(0.9)\nBaselines: We followed the settings\nin (Ruff et al., 2018) and utilized the\nArea Under Operating Characteris-\ntic Curve (AUC) of several state-\nof-the-art anomaly detection algo-\nrithms, including Deep SVDD (Ruff\net al., 2018), OCGAN (Perera et al.,\n2019), HRN-L2 and HRN (Hu et al.,\n2020), PLAD (Cai & Fan, 2022),\nand DROCC (Goyal et al., 2020).\nAll SOTAs\u2019 results are given accord-\ning to their officially reported results\nor are reproduced by official codes.\nConsidering that there is not much room for performance improvement on Fashion-MNIST, we only\nreproduced the results of recent or most relative algorithms, which contains Deep SVDD (Ruff et al.,\n2018), and DROCC (Goyal et al., 2020). The network architecture of Deep SVDD is set the same\nas ours for fairness.\nResults: The experimental results are listed in Table 1. On CIFAR-10, both DOHSC and DO2HSC\nsurpassed SOTAs, especially for Dog and Frog. Second, DO2HSC obtained better results compared\nwith DOHSC, which further verifies the effectiveness of bi-hypersphere anomaly detection and fully\ndemonstrates its applicability to image data. It is also worth mentioning that Deep SVDD plays an\nimportant baseline role relative to DOHSC, and DOHSC outperforms it by a large margin in all\nclasses. This illustrates the significant meaning of the proposed orthogonal projection method is\nconstructive. The result of Fashion-MNIST is in Appendix G.\n4.2\nEXPERIMENTS ON TABULAR DATA\nTable 2: Average F1-scores with the standard devi-\nation in one-class anomaly detection on two tabular\ndatasets. The best two results are marked in bold.\nThyroid\nArrhythmia\nOCSVM (Sch\u00a8olkopf et al., 1999)\n0.56 \u00b1 0.01\n0.64 \u00b1 0.01\nDeep SVDD (Ruff et al., 2018)\n0.73 \u00b1 0.00\n0.54 \u00b1 0.01\nLOF (Breunig et al., 2000)\n0.54 \u00b1 0.01\n0.51 \u00b1 0.01\nGOAD (Bergman & Hoshen, 2020)\n0.75 \u00b1 0.01\n0.52 \u00b1 0.02\nDROCC (Goyal et al., 2020)\n0.78 \u00b1 0.03\n0.69 \u00b1 0.02\nPLAD (Cai & Fan, 2022)\n0.77 \u00b1 0.01\n0.71 \u00b1 0.02\nDOHSC\n0.92 \u00b1 0.01\n0.70 \u00b1 0.03\nDO2HSC\n0.98 \u00b1 0.59\n0.74 \u00b1 0.02\nDatasets: Here, we use two tabular datasets\n(Thyroid, Arrhythmia), and we followed the\ndata split settings in Zong et al. (2018).\nResults: The F1-scores of our methods and\nsix baselines are reported in Table 2. A signif-\nicant margin was observed between the base-\nlines and ours, especially the results of Thy-\nroid.\nDespite the challenge posed by the\nsmall sample size of the Arrhythmia data,\nDO2HSC still outperforms PLAD by a mar-\ngin of 3%. Similarly, the orthogonal projec-\ntion of DOHSC successfully standardized the\nresults of Deep SVDD.\n4.3\nEXPERIMENTS ON GRAPH DATA\nDatasets: We further evaluate our models on six real-world graph datasets2 (COLLAB, COX2,\nER MD, MUTAG, DD and IMDB-Binary). Our experiments followed the standard one-class set-\ntings and data-split method in a previous work (Zhao & Akoglu, 2021; Qiu et al., 2022).\nBaselines: We compare our methods with the following methods, including four graph kernels\ncombined with OCSVM and four state-of-the-art baselines: RW (G\u00a8artner et al., 2003; Kashima\net al., 2003), SP (Borgwardt & Kriegel, 2005), WL (Shervashidze et al., 2011) and NH (Hido &\n2https://ls11-www.cs.tu-dortmund.de/staff/morris/graphkerneldatasets\n8\nPublished as a conference paper at ICLR 2024\n0\n50\n100\n150\n200\n250\n300\n350\nDistance\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\nFrequency Density\nAnomalous Data\nNormal Data\nDecision Boundary\nER_MD\n(a) DOHSC\n(b) DO2HSC\nFigure 5: Distance Histograms on ER MD.\n-4\n-2\n5\n0\n2\n4\n6\n8\n0\n-5\n0\n-5\n5\n(a) Training Result\n-5\n0\n0\n5\n10\n15\n-10\n10\n20\n0\n30\n10\n(b) Testing Result\nFigure 6: 3-D plots of DO2HSC on MUTAG.\nTable 3: Average AUCs with standard deviation (10 trials) of different graph-level anomaly detection\nalgorithms. \u2018DSVDD\u2019 stands for \u2018Deep SVDD\u2019. We assess models by regarding every data class as\nnormal data, respectively. The best two results are highlighted in bold and \u2018\u2013\u2019 means out of memory.\nCOLLAB\nMUTAG\nER MD\n0\n1\n2\n0\n1\n0\n1\nSP+OCSVM\n0.5910 \u00b1 0.0000\n0.8397 \u00b1 0.0000\n0.7902 \u00b1 0.0000\n0.5917 \u00b1 0.0000\n0.2608 \u00b1 0.0000\n0.4092 \u00b1 0.0000\n0.3824 \u00b1 0.0000\nWL+OCSVM\n0.5122 \u00b1 0.0000\n0.8054 \u00b1 0.0000\n0.7996 \u00b1 0.0000\n0.6509 \u00b1 0.0000\n0.2960 \u00b1 0.0000\n0.4571 \u00b1 0.0000\n0.3262 \u00b1 0.0000\nNH+OCSVM\n0.5976 \u00b1 0.0000\n0.8054 \u00b1 0.0000\n0.6414 \u00b1 0.0000\n0.7959 \u00b1 0.0274\n0.1679 \u00b1 0.0062\n0.5155 \u00b1 0.0200\n0.3648 \u00b1 0.0000\nRW+OCSVM\n\u2013\n\u2013\n\u2013\n0.8698 \u00b1 0.0000\n0.1504 \u00b1 0.0000\n0.4820 \u00b1 0.0000\n0.3484 \u00b1 0.0000\nOCGIN\n0.4217 \u00b1 0.0606\n0.7565 \u00b1 0.2035\n0.1906 \u00b1 0.0857\n0.8491 \u00b1 0.0424\n0.7466 \u00b1 0.0168\n0.5645 \u00b1 0.0323\n0.4358 \u00b1 0.0538\ninfoGraph+DSVDD\n0.5662 \u00b1 0.0597\n0.7926 \u00b1 0.0986\n0.4062 \u00b1 0.0978\n0.8805 \u00b1 0.0448\n0.6166 \u00b1 0.2052\n0.5312 \u00b1 0.1545\n0.5082 \u00b1 0.0704\nGLocalKD\n0.4638 \u00b1 0.0003\n0.4330 \u00b1 0.0016\n0.4792 \u00b1 0.0004\n0.3952 \u00b1 0.2258\n0.2965 \u00b1 0.2641\n0.5781 \u00b1 0.1790\n0.7154 \u00b1 0.0000\nOCGTL\n0.6504 \u00b1 0.0433\n0.8908 \u00b1 0.0239\n0.4029 \u00b1 0.0541\n0.6570 \u00b1 0.0210\n0.7579 \u00b1 0.2212\n0.2755 \u00b1 0.0317\n0.6915 \u00b1 0.0207\nDOHSC\n0.9185 \u00b1 0.0455\n0.9755 \u00b1 0.0030\n0.8826 \u00b1 0.0250\n0.8822 \u00b1 0.0432\n0.8115 \u00b1 0.0279\n0.6620 \u00b1 0.0308\n0.5184 \u00b1 0.0793\nDO2HSC\n0.9390 \u00b1 0.0025\n0.9836 \u00b1 0.0115\n0.8835 \u00b1 0.0118\n0.9089 \u00b1 0.0609\n0.8250 \u00b1 0.0790\n0.6867 \u00b1 0.0226\n0.7351 \u00b1 0.0159\nKashima, 2009), OCGIN (Zhao & Akoglu, 2021), infoGraph+Deep SVDD (Sun et al., 2020; Ruff\net al., 2018), GLocalKD (Ma et al., 2022) and OCGTL (Qiu et al., 2022).\nResults: Table 3 shows the comparable results of graph-level anomaly detection. 1) The proposed\nmethods achieved the best AUC values compared to the other algorithms on all datasets. Both out-\nperform the other state-of-the-art baselines. 2) DO2HSC is obviously more effective than DOHSC,\nespecially since we observed that there exists a large improvement (exceeding 20%) in Class 1 of\nER MD between DOHSC and DO2HSC. A distance distribution visualization is provided to show\ntheir differences in Figure 5. Owing to length limitations, please refer to Appendix H for the re-\nmaining results. 3) The anomaly detection visualization results of DO2HSC displayed in Figure 6\nalso demonstrate excellent performance. We drew them by setting the projection dimension to 3,\nand please refer to Appendix I for the results of different perspectives.\n4.4\nMORE RESULTS AND ANALYSIS\nWe provide the time and space complexity analysis in Appendix B. Also, the ablation study\n(including orthogonal projection, mutual information maximization, etc.), parameter sensitivity\n(e.g., different percentile settings), robustness analysis, and more visualization results are shown in\nAppendices J and I, respectively.\n5\nCONCLUSION\nThis paper proposes two novel end-to-end AD methods, DOHSC and DO2HSC, that mitigate the\npossible shortcomings of hypersphere boundary learning by applying an orthogonal projection for\nglobal representation. Furthermore, DO2HSC projects normal data between the interval areas of\ntwo co-centered hyperspheres to significantly alleviate the soap-bubble issue and the incompactness\nof a single hypersphere. We also extended DOHSC and DO2HSC to graph-level anomaly detection,\nwhich combines the effectiveness of mutual information between the node level and global features\nto learn graph representation and the power of hypersphere compression. The comprehensive exper-\nimental results strongly demonstrate the superiority of the DOHSC and DO2HSC on multifarious\ndatasets. One limitation of this work is that we did not consider cases in which the training data\nconsisted of multiple classes of normal data, which is beyond the scope of this study. Our source\ncode is available at https://github.com/wownice333/DOHSC-DO2HSC.\n9\nPublished as a conference paper at ICLR 2024\nACKNOWLEDGEMENTS\nThis work was supported by the National Natural Science Foundation of China under Grant No.\n62376236, the General Program JCYJ20210324130208022 of Shenzhen Fundamental Research,\nthe research funding T00120210002 of Shenzhen Research Institute of Big Data, and the funding\nUDF01001770 of The Chinese University of Hong Kong, Shenzhen.\nREFERENCES\nCharu C Aggarwal. An introduction to outlier analysis. 2017.\nLiron Bergman and Yedid Hoshen. Classification-based anomaly detection for general data. In\nProceedings of the 8th International Conference on Learning Representations, 2020.\nKarsten M Borgwardt and Hans-Peter Kriegel. Shortest-path kernels on graphs. In Proceedings of\nthe Fifth IEEE International Conference on Data Mining, pp. 8\u2013pp, 2005.\nMarkus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and J\u00a8org Sander.\nLOF: identifying\ndensity-based local outliers. In Proceedings of the ACM SIGMOD International Conference on\nManagement of Data, pp. 93\u2013104, 2000.\nJinyu Cai and Jicong Fan. Perturbation learning based anomaly detection. In Advances in Neural\nInformation Processing Systems, 2022.\nYuanhong Chen, Yu Tian, Guansong Pang, and Gustavo Carneiro. Deep one-class classification via\ninterpolated gaussian descriptor. In Proceedings of the AAAI Conference on Artificial Intelligence,\nvolume 36, pp. 383\u2013392, 2022.\nJicong Fan and Tommy W. S. Chow. Exactly robust kernel principal component analysis. IEEE\nTransactions on Neural Networks and Learning Systems, 31(3):749\u2013761, 2020.\nJicong Fan and Youqing Wang. Fault detection and diagnosis of non-linear non-gaussian dynamic\nprocesses using kernel dynamic independent component analysis. Information Sciences, 259:\n369\u2013379, 2014.\nThomas G\u00a8artner, Peter Flach, and Stefan Wrobel. On graph kernels: Hardness results and efficient\nalternatives. In Learning Theory and Kernel Machines, pp. 129\u2013143. 2003.\nSachin Goyal, Aditi Raghunathan, Moksh Jain, Harsha Vardhan Simhadri, and Prateek Jain.\nDROCC: deep robust one-class classification. In Proceedings of the 37th International Con-\nference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pp.\n3711\u20133721, 2020.\nWill Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs.\nAdvances in neural information processing systems, 30, 2017.\nSongqiao Han, Xiyang Hu, Hailiang Huang, Minqi Jiang, and Yue Zhao.\nAdbench: Anomaly\ndetection benchmark. Advances in Neural Information Processing Systems, 35:32142\u201332159,\n2022.\nShohei Hido and Hisashi Kashima. A linear-time graph kernel. In Ninth IEEE International Con-\nference on Data Mining, pp. 179\u2013188, 2009.\nWenpeng Hu, Mengyu Wang, Qi Qin, Jinwen Ma, and Bing Liu. Hrn: A holistic approach to one\nclass learning. Advances in Neural Information Processing Systems, 33:19111\u201319124, 2020.\nHisashi Kashima, Koji Tsuda, and Akihiro Inokuchi. Marginalized kernels between labeled graphs.\nIn Proceedings of the 20th International Conference on Machine Learning, pp. 321\u2013328, 2003.\nKonstantin Kirchheim, Marco Filax, and Frank Ortmeier. Multi-class hypersphere anomaly detec-\ntion. In Proceedings of the 26th International Conference on Pattern Recognition, pp. 2636\u20132642.\nIEEE, 2022.\n10\nPublished as a conference paper at ICLR 2024\nNils M Kriege, Fredrik D Johansson, and Christopher Morris. A survey on graph kernels. Applied\nNetwork Science, 5(1):1\u201342, 2020.\nTommaso Lanciano, Francesco Bonchi, and A. Gionis. Explainable classification of brain networks\nvia contrast subgraphs.\nProceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining, 2020.\nBeatrice Laurent and Pascal Massart. Adaptive estimation of a quadratic functional by model selec-\ntion. Annals of Statistics, pp. 1302\u20131338, 2000.\nPhilipp Liznerski, Lukas Ruff, Robert A. Vandermeulen, Billy Joe Franks, Marius Kloft, and Klaus-\nRobert M\u00a8uller. Explainable deep one-class classification. In Proceedings of the 9th International\nConference on Learning Representations, 2021.\nRongrong Ma, Guansong Pang, Ling Chen, and Anton van den Hengel. Deep graph-level anomaly\ndetection by glocal knowledge distillation. In Proceedings of the Fifteenth ACM International\nConference on Web Search and Data Mining, pp. 704\u2013714, 2022.\nSebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural sam-\nplers using variational divergence minimization. In Advances in Neural Information Processing\nSystems, volume 29, 2016.\nGuansong Pang, Chunhua Shen, Longbing Cao, and Anton Van Den Hengel. Deep learning for\nanomaly detection: A review. ACM Computing Surveys (CSUR), 54(2):1\u201338, 2021.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Pretten-\nhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and\nE. Duchesnay. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research,\n12:2825\u20132830, 2011.\nPramuditha Perera, Ramesh Nallapati, and Bing Xiang. Ocgan: One-class novelty detection using\ngans with constrained latent representations. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 2898\u20132906, 2019.\nChen Qiu, Marius Kloft, Stephan Mandt, and Maja Rudolph. Raising the bar in graph-level anomaly\ndetection. In Luc De Raedt (ed.), Proceedings of the Thirty-First International Joint Conference\non Artificial Intelligence, pp. 2196\u20132203, 2022.\nLukas Ruff, Robert Vandermeulen, Nico Goernitz, Lucas Deecke, Shoaib Ahmed Siddiqui, Alexan-\nder Binder, Emmanuel M\u00a8uller, and Marius Kloft. Deep one-class classification. In International\nConference on Machine Learning, pp. 4393\u20134402, 2018.\nLukas Ruff, Robert A. Vandermeulen, Nico G\u00a8ornitz, Alexander Binder, Emmanuel M\u00a8uller, Klaus-\nRobert M\u00a8uller, and Marius Kloft. Deep semi-supervised anomaly detection. In International\nConference on Learning Representations, 2020.\nBernhard Sch\u00a8olkopf, Robert C Williamson, Alex Smola, John Shawe-Taylor, and John Platt. Support\nvector method for novelty detection. Advances in neural information processing systems, 12,\n1999.\nBernhard Sch\u00a8olkopf, Robert C. Williamson, Alexander J. Smola, John Shawe-Taylor, and John C.\nPlatt. Support vector method for novelty detection. In Advances in Neural Information Processing\nSystems, pp. 582\u2013588, 1999.\nBernhard Sch\u00a8olkopf, John C Platt, John Shawe-Taylor, Alex J Smola, and Robert C Williamson.\nEstimating the support of a high-dimensional distribution. Neural Computation, 13(7):1443\u2013\n1471, 2001.\nNaeem Seliya, Azadeh Abdollah Zadeh, and Taghi M Khoshgoftaar. A literature review on one-class\nclassification and its potential applications in big data. Journal of Big Data, 8(1):1\u201331, 2021.\nNino Shervashidze, Pascal Schweitzer, Erik Jan Van Leeuwen, Kurt Mehlhorn, and Karsten M Borg-\nwardt. Weisfeiler-lehman graph kernels. Journal of Machine Learning Research, 12(9), 2011.\n11\nPublished as a conference paper at ICLR 2024\nGiannis Siglidis, Giannis Nikolentzos, Stratis Limnios, Christos Giatsidis, Konstantinos Skianis, and\nMichalis Vazirgiannis. Grakel: A graph kernel library in Python. Journal of Machine Learning\nResearch, 21(54):1\u20135, 2020.\nKihyuk Sohn, Chun-Liang Li, Jinsung Yoon, Minho Jin, and Tomas Pfister. Learning and evalu-\nating representations for deep one-class classification. In Proceedings of the 9th International\nConference on Learning Representations, 2021.\nFan-Yun Sun, Jordan Hoffmann, Vikas Verma, and Jian Tang. Infograph: Unsupervised and semi-\nsupervised graph-level representation learning via mutual information maximization. In Proceed-\nings of the 8th International Conference on Learning Representations, 2020.\nZiheng Sun, Chris Ding, and Jicong Fan. Lov\u00b4asz principle for unsupervised graph representation\nlearning. In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\nDavid MJ Tax and Robert PW Duin. Support vector data description. Machine learning, 54:45\u201366,\n2004.\nRoman Vershynin. High-dimensional probability: An introduction with applications in data science,\nvolume 47. Cambridge university press, 2018.\nMax Welling and Thomas N Kipf. Semi-supervised classification with graph convolutional net-\nworks. In Proceedings of the 5th International Conference on Learning Representations, 2016.\nSvante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37\u201352, 1987.\nZhihao Wu, Zhao Zhang, and Jicong Fan. Graph convolutional kernel machine versus graph con-\nvolutional networks. In Thirty-seventh Conference on Neural Information Processing Systems,\n2023.\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka.\nHow powerful are graph neural\nnetworks?\nIn Proceedings of the 7th International Conference on Learning Representations,\n2019.\nLingxiao Zhao and Leman Akoglu. On using classification datasets to evaluate graph outlier detec-\ntion: Peculiar observations and new insights. Big Data, 2021.\nBo Zong, Qi Song, Martin Renqiang Min, Wei Cheng, Cristian Lumezanu, Dae-ki Cho, and Haifeng\nChen. Deep autoencoding gaussian mixture model for unsupervised anomaly detection. In Pro-\nceedings of the 6th International Conference on Learning Representations, 2018.\nA\nSUPPLEMENTED ALGORITHM PROCEDURES\nHere, we present the detailed procedures for DOHSC and DO2HSC in Algorithms 1 and 2, respec-\ntively. It begins with a representation learning module and promotes the training data to approximate\nthe center of a hypersphere while adding an orthogonal projection layer. In addition, DO2HSC is\nrecapped in Algorithm 2 and begins with the same representation learning. In contrast, DOHSC\nutilizes a few epochs to initialize the decision boundaries, after which improved optimization is\napplied. A graph-level extension is presented in Algorithm 3. The main difference is that graph rep-\nresentation learning with maximization of the mutual information constraint is applied to substitute\nthe common representation learning module. Similarly, the graph-level DO2HSC is the combination\nof the representation learning part in the graph-level DOHSC and the anomaly detection part in the\ncommon DO2HSC.\nB\nTIME AND SPACE COMPLEXITY\nThe models of DOHSC and DO2HSC can be trained by mini-batch optimization. Suppose the\nbatch size is b, the maximum width of the hidden layers of the L-layer neural network is wmax,\nand the dimension of the input data is d, then the time complexities of the proposed methods are at\n12\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Deep Orthogonal Hypersphere Contraction (DOHSC)\nInput: The input data X \u2208Rn\u00d7d, dimensions of the latent representation k and orthogonal pro-\njection layer k\u2032, a trade-off parameter \u03bb and the coefficient of regularization term \u00b5, pretraining\nepoch T , learning rate \u03b7.\nOutput: The anomaly detection scores s.\n1: Initialize the auto-encoder network parameters W = {Wl, bl}L\nl=1 and the orthogonal projection\nlayer parameter \u0398;\n2: for t \u2192T do\n3:\nfor each batch do\n4:\nObtain the latent representation Z = f enc\nW (X);\n\u25b7Pretraining Stage\n5:\nUpdate the orthogonal parameter \u0398 of orthogonal projection layer by Eq. (3);\n6:\nProject the latent representation via Eq. (2);\n7:\nCalculate reconstruction loss via 1\nn\nPn\ni=1 \u2225f dec\nW\n\u0000Proj\u0398(f enc\nW (xi))\n\u0001\n\u2212xi\u22252;\n8:\nBack-propagate the network, update W and \u0398, respectively;\n9:\nend for\n10: end for\n11: Initialize the center of hypersphere by c = 1\nn\nPn\ni=1 f enc\nW (xi);\n12: repeat\n13:\nfor each batch do\n14:\nCalculate anomaly detection loss via Optimization (4);\n\u25b7Training Stage\n15:\nRepeat steps 4-6;\n16:\nBack-propagate the encoder network and update {W}\nL\n2\nl=1 and \u0398, respectively;\n17:\nend for\n18: until convergence\n19: Compute decision boundary r by Eq. (5);\n20: Calculate the anomaly detection scores s through Eq. (6);\n21: return The anomaly detection scores s.\nmost O(bdwmaxLT), where T is the total number of iterations. The space complexities are at most\nO(bd+dwmax +(L\u22121)w2\nmax). We see that the complexities are linear with the number of samples,\nwhich means the proposed methods are scalable to large datasets. Particularly, for high-dimensional\ndata (very large d), we can use small wmax to improve the efficiency.\nC\nRELATED PROOF OF BI-HYPERSPHERE LEARNING MOTIVATION\nThe traditional idea of detecting outliers is to inspect the distribution tails under the ideal assumption\nthat the normal data are Gaussian. Following this assumption, one may argue that an anomalous\nsample can be distinguished by its large Euclidean distance from the data center (\u21132 norm \u2225z \u2212c\u2225,\nwhere c denotes the centroid). Accordingly, the abnormal dataset is {z : \u2225z \u2212c\u2225> r} for a\ndecision boundary r. However, in high dimensional space, Gaussian distributions look like soap-\nbubble 3, which means the normal data are more likely to be located in a bi-hypersphere (Vershynin,\n2018), such as {z : rmin < \u2225z \u2212c\u2225< rmax}. To better understand this counterintuitive behavior, we\ngenerate normal samples X \u223cN(0, Id), where d is the data dimension in {1, 10, 50, 100, 200, 500}.\nAs shown in Figure 3 of Section 2.2.1, it indicates that only the univariate Gaussian has a near-zero\nmode, whereas other high-dimensional Gaussian distributions leave many off-center spaces in the\nblank. The soap-bubble problem in high-dimensional distributions is well demonstrated in Table 4;\nthe higher the dimension, the greater the quantity of data further away from the center, especially for\na 0.01-quantile distance. Thus, we cannot make the sanguine assumption that all of the normal data\nare located within the radius of a hypersphere (i.e., {z : \u2225z \u2212c\u2225< r}). Using Lemma 1 of (Laurent\n& Massart, 2000), we can prove that Proposition 1, which matches the values in Table 4 that when\nthe dimension is larger, normal data are more likely to lie away from the center.\nWe also simulated a possible case of outlier detection, in which data were all sampled from a 16-\ndimensional Gaussian with orthogonal covariance:10,000 normal samples follow N(0, I), the first\ngroup of 1,000 outliers is from N(\u00b51, 1\n10I), the second group of 500 outliers are from N(\u00b52, I),\n3https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/\n13\nPublished as a conference paper at ICLR 2024\nAlgorithm 2 Deep Orthogonal Bi-Hypersphere Compression (DO2HSC)\nInput: The input data X \u2208Rn\u00d7d, dimensions of the latent representation k and orthogonal pro-\njection layer k\u2032, a trade-off parameter \u03bb and the coefficient of regularization term \u00b5, pretraining\nepoch T1, iterations of initializing decision boundaries T2, learning rate \u03b7.\nOutput: The anomaly detection scores s.\nInitialize the auto-encoder network parameters W = {Wl, bl}L\nl=1 and the orthogonal projection\nlayer parameter \u0398;\n2: for t \u2192T1 do\nfor each batch do\n4:\nRepeat steps 4-8 of DOHSC;\n\u25b7Pretraining Stage\nend for\n6: end for\nUpdate the orthogonal parameter \u0398 of orthogonal projection layer by Eq. (3);\n8: Obtain the global orthogonal latent representation by Eq. (2);\nInitialize the center of hypersphere by c = 1\nn\nPn\ni=1 f enc\nW (xi);\n10: for t \u2192T2 do\nRepeat steps 13-17 of DOHSC;\n\u25b7Pretraining Stage\n12: end for\nCompute decision boundary r of DOHSC by Eq. (5);\n14: Initialize decision boundaries rmax and rmin via Eq. (7);\nrepeat\n16:\nfor each batch do\nObtain the latent representation Z = f enc\nW (X);\n\u25b7Training Stage\n18:\nUpdate the orthogonal parameter \u0398 of orthogonal projection layer by Eq. (3);\nProject the latent representation via Eq. (2);\n20:\nCalculate the improved total loss via Optimization (8);\nBack-propagate the network, update {W}\nL\n2\nl=1 and \u0398, respectively;\n22:\nend for\nuntil convergence\n24: Calculate the anomaly detection scores s through Eq. (9);\nreturn The anomaly detection scores s.\nFigure 7: Histogram of distances (Euclidean norm) from the center of normal samples under\n16-dimensional Gaussian distributions N(0, I).\nThree groups of anomalous data are also 16-\ndimensional and respectively sampled from N(\u00b51, 1\n10I), N(\u00b52, I), and N(\u00b53, 5I), where the pop-\nulation means \u00b51, \u00b52, \u00b53 are randomized within [0, 1] for each dimension.\nand the last group of 2,000 outliers are from N(\u00b53, 5I). Figure 7 shows that abnormal data from\nother distributions (group-1 outliers) could fall a small distance away from the center of the normal\nsamples.\n14\nPublished as a conference paper at ICLR 2024\nAlgorithm 3 Graph-Level Deep Orthogonal Hypersphere Contraction\nInput: The input graph set G, dimensions of GIN hidden layers k and orthogonal projection layer\nk\u2032, a trade-off parameter \u03bb and the coefficient of regularization term \u00b5, pretraining epoch T ,\nlearning rate \u03b7.\nOutput: The anomaly detection scores s.\nInitialize the network parameters \u03a6, \u03a8, \u03a5 and the orthogonal projection layer parameter \u0398;\nfor t \u2192T do\n3:\nfor each batch G do\nObtain the global graph representation H\u03a6,\u03a8(G);\nUpdate the orthogonal parameter \u0398 of orthogonal projection layer by Eq. (3);\n6:\nProject the global graph representation via \u02dcH\u03a6,\u03a8,\u0398(G) = Proj\u0398(H\u03a6,\u03a8(G));\nCalculate I\u03a6,\u03a8,\u03a5\n\u0010\nh\u03a6,\u03a5, \u02dcH\u03a6,\u03a8(G)\n\u0011\nvia Eq. (13);\nBack-propagate GIN, update \u03a6, \u03a8, \u0398 and \u03a5, respectively;\n9:\nend for\nend for\nInitialize the center of hypersphere by Eq. (14);\n12: repeat\nfor each batch G do\nRepeat steps 4-6;\n15:\nCalculate total loss via Optimization (15);\nBack-propagate GIN and update \u03a6, \u03a8, \u03a5 and \u0398, respectively;\nend for\n18: until convergence\nCompute decision boundary r by Eq. (5);\nCalculate the anomaly detection scores s through Eq. (6);\n21: return The anomaly detection scores s.\nTable 4: Offcenter distance under multivariate Gaussian at different dimensions and quantiles.\nQuantile (correspond to rmin)\ndim=1\ndim=10\ndim=50\ndim=100\ndim=200\ndim=500\n0.01\n0.0127\n1.5957\n5.5035\n8.3817\n12.5117\n20.6978\n0.25\n0.3115\n2.5829\n6.5380\n9.4908\n13.6247\n21.8542\n0.50\n0.6671\n3.0504\n7.0141\n9.9662\n14.1054\n22.3337\n0.75\n1.1471\n3.5399\n7.5032\n10.4386\n14.5949\n22.8200\n0.99\n2.5921\n4.8265\n8.7723\n11.6049\n15.7913\n24.0245\nD\nPROOF FOR PROPOSITION 2\nProof. Since f makes s obey N(\u00afc, Ik), according to Proposition 1, we have\nP\n\u0014\n\u2225s \u2212\u00afc\u2225\u2265\nq\nk \u22122\n\u221a\nkt\n\u0015\n\u22651 \u2212e\u2212t.\nSince f is \u03b7-Lipschitz, we have\n\u2225s \u2212f(c)\u2225= \u2225f(z) \u2212f(c)\u2225\u2264\u03b7\u2225z \u2212c\u2225.\nIt follows that\n\u2225z \u2212c\u2225\u2265\u03b7\u22121\u2225s \u2212\u00afc + \u00afc \u2212f(c)\u2225\n\u2265\u03b7\u22121 (\u2225s \u2212\u00afc\u2225\u2212\u2225\u00afc \u2212f(c)\u2225)\n\u2265\u03b7\u22121 (\u2225s \u2212\u00afc\u2225\u2212\u03f5) .\nNow we have\nP\n\u0014\n\u2225z \u2212c\u2225\u2265\u03b7\u22121\n\u0012q\nk \u22122\n\u221a\nkt \u2212\u03f5\n\u0013\u0015\n\u22651 \u2212e\u2212t.\nThis finished the proof.\n15\nPublished as a conference paper at ICLR 2024\nE\nPROOF FOR PROPOSITION 3\nProof. The volume of a hyperball of radius r in k-dimension space is Vk(r) =\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rk, where\n\u0393 is Euler\u2019s gamma function. Then, the volume of the hypersphere given by DOHSC is Vmax =\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rk\nmax and the volume of the smaller hypersphere given by DO2HSC is Vmin =\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rk\nmin.\nThen, the volume of the decision region given by DO2HSC is Vmax \u2212Vmin. The density of the\ndecision region is defined as the number of normal data in unit volume. Therefore, the ratio between\nthe densities of DO2HSC and DOHSC can be computed as\n\u03c1 = n/(Vmax \u2212Vmin)\nn/Vmax\n=\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rk\nmax\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rkmax \u2212\n\u03c0k/2\n\u0393\n\u0000 k\n2 +1\n\u0001rk\nmin\n=\n1\n1 \u2212\n\u0010\nrmin\nrmax\n\u0011k .\n(17)\nThis finished the proof.\nIn the case of rmin = rmax, the volume of DO2HSC is close to an infinitely thin shell, essentially\ntransforming into the surface of a hypersphere. In this scenario, the data density of DO2HSC is sig-\nnificantly higher compared with that of DOHSC. However, it is important to note that this situation\nis quite rare, particularly in high-dimensional space.\nF\nEXPERIMENT CONFIGURATION\nIn this section, experimental settings are presented for reproduction. First, each graph dataset was\ndivided into two parts: the training and testing sets. We randomly sampled 80 percent of the normal\ngraph as the training set and the remaining normal graph, together with the randomly sampled ab-\nnormal data in a one-to-one ratio to form the testing set. Regarding the image and tabular datasets,\nthe data splits are already provided in the paper. The detailed statistical information of all tested\ndatasets is given in Tables 5 and 6.\nTable 5: Description for non-graph datasets.\nDataset Name\nType\n# Instances\n# Dimension\nThyroid\nTabular\n3772\n6\nArrhythmia\nTabular\n452\n274\nFashion-MNIST\nImage\n70000\n28\u00d728\nCIFAR-10\nImage\n60000\n32 \u00d7 32 \u00d7 3\nTable 6: Description for six graph datasets.\nDatasets\n# Graphs\nAvg.\n# Nodes\nAvg.\n# Edges\n# Classes\n# Graph Labels\nCOLLAB\n5000\n74.49\n2457.78\n3\n2600 / 775 / 1625\nCOX2\n467\n42.43\n44.54\n2\n365 / 102\nER MD\n446\n21.33\n234.85\n2\n265 / 181\nMUTAG\n188\n17.93\n19.79\n2\n63 / 125\nDD\n1178\n284.32\n715.66\n2\n691 / 487\nIMDB-Binary\n1000\n19.77\n96.53\n2\n500 / 500\nIn the image and tabular experiments, our backbone was consistent with the Deep SVDD,\nand the preprocessing conformed to the same splits as DROCC. All results originated from\nthe corresponding papers or were reproduced according to the official code.\nRegarding our\nDOHSC model, we set 10 epochs in the pretraining stage to initialize the center of the deci-\nsion boundary and then train the model in 200 epochs. The percentile \u03bd of r was selected from\n16\nPublished as a conference paper at ICLR 2024\n{0.001, 0.003, 0.005, 0.008, 0.01, 0.03, 0.1, 0.3}. The improved method DO2HSC also sets a 10-\nepoch pretraining stage and trains DOHSC for 50 epochs to initialize a suitable center and decision\nboundaries rmax and rmin, where the percentile \u03bd of rmax is the same as DOHSC. The main training\nepoch was set to 200.\nIn the graph experiment, we adopted the classical AD method, One-Class SVM (OCSVM)\n(Sch\u00a8olkopf et al., 2001), to compare graph-kernel baselines and used 10-fold cross-validation to\nmake a fair comparison. All graph kernels extract a kernel matrix via GraKel (Siglidis et al., 2020)\nand apply the OCSVM in scikit-learn (Pedregosa et al., 2011). Specifically, we selected Floyd\nWarshall as the SP kernel\u2019s algorithm and set lambda as 0.01 for the RW kernel. The WL kernel\nalgorithm is sensitive to the number of iterations; therefore, we tested four different iterations {2,\n5, 8, 10} and reported the best result for each experiment. The outputs were normalized for all the\ngraph kernels. For infoGraph+Deep SVDD, the first stage runs for 20 epochs, and the second stage\npretrains for 50 epochs and trains for 100 epochs. In OCGIN, GLocalKD, and OCGTL, the default\nor reported parameter settings were adopted to reproduce the experimental results.\n(a) DOHSC\n(b) DO2HSC\nFigure 8: Convergence curves of the proposed models on the MUTAG dataset.\nFor the graph-level DOHSC, we first set one epoch in the pre-training stage to initialize the center\nof the decision boundary and then train the model in 500 epochs. The convergence curves are\nshown in Figure 8 to indicate that the final optimized results were adopted. The improved method\nDO2HSC is also set as a 1-epoch pre-training stage and trains DOHSC for five epochs, where the\npercentile \u03bd of rmax is selected as 0.01. After initialization, the model was trained for 500 epochs.\nFor both proposed approaches, the trade-off factor \u03bb was set to 10 to ensure decision loss as the\nmain optimization objective. The dimensions of the GIN hidden and orthogonal projection layers\nwere fixed at 16 and 8, respectively. For the backbone network, a 4-layer GIN and a 3-layer fully\nconnected neural network were adopted.\nFinally, the averages and standard deviations of the Area Under the ROC curve (AUC) and F1-score\nwere used to support the comparable experiments by repeating each algorithm ten times. A higher\nmetric value indicates better performance.\nG\nSUPPLEMENTED RESULTS ON FASHION-MNIST DATASET\nThe complete experimental results for the Fashion-MNIST image dataset are given in Table 7. A\ndetailed standard deviation can demonstrate fluctuations in performance. The proposed methods are\nrelatively stable, especially for DOHSC.\nH\nSUPPLEMENTARY RESULTS OF GRAPH-LEVEL ANOMALY DETECTION\nHere, we give the results of retained 3 graph datasets (COX2, DD, and IMDB-Binary) for graph-\nlevel extension in Table 8. The proposed two models are superior on all datasets and behave much\nmore effectively compared with other SOTAs, which also supports our motivations for graph-level\nanomaly detections.\nI\nSUPPLEMENTED VISUALIZATION\nThis section presents the related supplemental visualization results of the anomaly detection task.\nFigure 9 shows the distance distributions of the two-stage method, proposed model DOHSC, and\n17\nPublished as a conference paper at ICLR 2024\nTable 7: Average AUCs in one-class anomaly detection on Fashion-MNIST. (The best two results\nare marked in bold.)\nNormal Class\nDeep SVDD\n(Ruff et al., 2018)\nDROCC\n(Goyal et al., 2020)\nDOHSC\nDO2HSC\nT-shirt\n0.8263 \u00b1 0.0342\n0.8931 \u00b1 0.0072\n0.9153 \u00b1 0.0082\n0.9196 \u00b1 0.0064\nTrouser\n0.9632 \u00b1 0.0072\n0.9835 \u00b1 0.0054\n0.9817 \u00b1 0.0060\n0.9839 \u00b1 0.0020\nPullover\n0.7885 \u00b1 0.0398\n0.8656 \u00b1 0.0140\n0.8007 \u00b1 0.0204\n0.8768 \u00b1 0.0122\nDress\n0.8607 \u00b1 0.0124\n0.8776 \u00b1 0.0269\n0.9178 \u00b1 0.0230\n0.9171 \u00b1 0.0084\nCoat\n0.8417 \u00b1 0.0366\n0.8453 \u00b1 0.0143\n0.8805 \u00b1 0.0258\n0.9038 \u00b1 0.0140\nSandal\n0.8902 \u00b1 0.0281\n0.9336 \u00b1 0.0123\n0.8932 \u00b1 0.0287\n0.9308 \u00b1 0.0070\nShirt\n0.7507 \u00b1 0.0158\n0.7789 \u00b1 0.0188\n0.8177 \u00b1 0.0124\n0.8022 \u00b1 0.0045\nSneaker\n0.9676 \u00b1 0.0062\n0.9624 \u00b1 0.0059\n0.9678 \u00b1 0.0050\n0.9677 \u00b1 0.0075\nBag\n0.9039 \u00b1 0.0355\n0.7797 \u00b1 0.0749\n0.9122 \u00b1 0.0258\n0.9090 \u00b1 0.0105\nAnkle Boot\n0.9488 \u00b1 0.0207\n0.9589 \u00b1 0.0207\n0.9756 \u00b1 0.0127\n0.9785 \u00b1 0.0038\nTable 8: Average AUCs with standard deviation (10 trials) of different graph-level anomaly detection\nalgorithms. \u2018DSVDD\u2019 stands for \u2018Deep SVDD\u2019. We assess models by regarding every data class as\nnormal data, respectively. The best two results are highlighted in bold and \u2019\u2013\u2019 means out of memory.\nCOX2\nDD\nIMDB-Binary\n0\n1\n0\n1\n0\n1\nSP+OCSVM\n0.5408 \u00b1 0.0000\n0.5760 \u00b1 0.0000\n0.6856 \u00b1 0.0000\n0.4474 \u00b1 0.0000\n0.4592 \u00b1 0.0000\n0.4716 \u00b1 0.0000\nWL+OCSVM\n0.5990 \u00b1 0.0000\n0.5057 \u00b1 0.0000\n0.7397 \u00b1 0.0000\n0.4946 \u00b1 0.0000\n0.5157 \u00b1 0.0000\n0.4607 \u00b1 0.0000\nNH+OCSVM\n0.4841 \u00b1 0.0000\n0.4717 \u00b1 0.0000\n0.7424 \u00b1 0.0000\n0.3684 \u00b1 0.0000\n0.5321 \u00b1 0.0000\n0.4652 \u00b1 0.0000\nRW+OCSVM\n0.5243 \u00b1 0.0000\n0.6553 \u00b1 0.0000\n\u2013\n\u2013\n0.4951 \u00b1 0.0000\n0.5311 \u00b1 0.0000\nOCGIN\n0.5964 \u00b1 0.0578\n0.5683 \u00b1 0.0768\n0.6659 \u00b1 0.0444\n0.6003 \u00b1 0.0534\n0.4571 \u00b1 0.1879\n0.3736 \u00b1 0.0816\ninfoGraph+DSVDD\n0.4825 \u00b1 0.0624\n0.5029 \u00b1 0.0700\n0.3942 \u00b1 0.0436\n0.6484 \u00b1 0.0236\n0.6353 \u00b1 0.0277\n0.5836 \u00b1 0.0995\nGLocalKD\n0.3861 \u00b1 0.0131\n0.3143 \u00b1 0.0383\n0.1952 \u00b1 0.0000\n0.2203 \u00b1 0.0001\n0.5383 \u00b1 0.0124\n0.4812 \u00b1 0.0101\nOCGTL\n0.5541 \u00b1 0.0320\n0.4862 \u00b1 0.0224\n0.6990 \u00b1 0.0260\n0.6767 \u00b1 0.0280\n0.6510 \u00b1 0.0180\n0.6412 \u00b1 0.0127\nDOHSC\n0.6263 \u00b1 0.0333\n0.6805 \u00b1 0.0168\n0.7083 \u00b1 0.0188\n0.7579 \u00b1 0.0154\n0.7160 \u00b1 0.0600\n0.7705 \u00b1 0.0045\nDO2HSC\n0.6329 \u00b1 0.0292\n0.6923 \u00b1 0.0433\n0.7320 \u00b1 0.0194\n0.7651 \u00b1 0.0317\n0.7547 \u00b1 0.0390\n0.7737 \u00b1 0.0503\n \n \n  \n  \n  \n  \n        \n \n    \n   \n    \n                 \n              \n           \n                 \n   \n \n   \n   \n   \n   \n \n   \n   \n   \n        \n \n \n \n \n \n  \n  \n  \n  \n  \n                 \n           \n                 \n(a) infoGraph+Deep SVDD\n(b) DOHSC\n(c) DO2HSC\n   \n   \n   \n   \n   \n   \n        \n \n    \n    \n    \n    \n   \n    \n    \n    \n    \n   \n                 \n              \n           \n                 \n   \n    \n   \n    \n   \n    \n   \n    \n   \n    \n   \n        \n \n    \n    \n    \n    \n   \n    \n    \n    \n    \n                 \n              \n           \n                 \n   \n    \n   \n    \n   \n    \n   \n        \n \n \n \n \n \n \n \n \n \n                 \n           \n                 \n   \n   \n   \n   \n   \n   \n   \n   \n        \n \n \n  \n  \n  \n  \n  \n  \n  \n                 \n           \n                 \nFigure 9: Distance distributions were obtained by infoGraph+Deep SVDD, the proposed model,\nand the improved proposed model on COX2. The first row represents the distance distribution of the\ntraining samples in relation to the decision boundary. The last row indicates the distance distribution\nof the test data with respect to the decision boundary.\n18\nPublished as a conference paper at ICLR 2024\nimproved DO2HSC. Here, distance is defined as the distance between each sample and the center\nof the decision hypersphere. Distance distribution denotes the sample proportion at this distance\ninterval relative to the corresponding total samples. It can be intuitively observed that most of\nthe distances of the instances were close to the decision boundary because of the fixed learned\nrepresentation. As mentioned earlier, the jointly trained algorithm mitigated this situation, and the\nobtained representation caused many instances to have smaller distances from the center of the\nsphere. Moreover, as mentioned in Section 2.2, anomalous data may occur in regions with less\ntraining data, particularly in the region close to the center, which is also confirmed by (a) and (b) of\nFigure 9. In contrast, DO2HSC effectively shrinks the decision area, and we find that the number of\noutliers is obviously reduced owing to a more compact distribution of the training data.\nThe 3D visualization results of the training and testing stages are also presented the difference be-\ntween them in Figures 10 and 11.\nTo further support the aforementioned statements, as shown in Figure 12, the anomalous samples\nare located in the decision region and are closer to the center than other normal samples. On the\ncontrary, the result of DO2HSC effectively prevents this phenomenon.\n  \n  \n \n \n \n \n \n  \n \n  \n \n  \n \n \n  \n  \n  \n  \n  \n  \n \n  \n \n \n \n \n  \n  \n  \n \n \n \n \n \n \n  \n  \n  \n \n \n \n \n \n  \n  \n \n \n \n \n(b) Testing Result\n  \n  \n  \n  \n \n \n \n \n \n \n \n  \n  \n  \n  \n  \n  \n \n \n \n  \n  \n  \n \n \n  \n  \n  \n \n \n \n \n \n(a) Training Result\nFigure 10: Visualization results of the DOHSC with MUTAG in different perspectives.\nJ\nPARAMETER SENSITIVITY AND ROBUSTNESS\nTo confirm the stability of our models, we analyzed the parameter sensitivity and robustness of\nDOHSC and DO2HSC, respectively. Consider that the projection dimension varies in {4, 8, 16,\n32, 64, 128}, whereas the hidden layer dimension of the GIN module ranges from 4 to 128. In\nFigure 13, the DO2HSC model has less volatile performance than DOHSC, especially when the\ntraining dataset is sampled from COX2 class 0, as shown in Subfigure (d). Noticeably, a higher\ndimension of the GIN hidden layer usually displays a better AUC result because the quality of the\nlearned graph representations improves when the embedding space is sufficiently large.\nIn addition, we assessed different aspects of model robustness. More specifically, the AUC results\nabout two \u201dratios\u201d are displayed: 1) Different sampling ratios for the training set; 2) Different ratios\nof noise disturbance for the learned representation. In Subfigures (c) and (f), the purple bars regard\nnormal data as class 0, whereas green bars treat normal data as class 1. Note that most AUC results\nare elevated along with a higher ratio of authentic data in the training stage, demonstrating the\npotential of our models in the unsupervised setting. On the other hand, when more noise is blended\ninto the training dataset, the AUC performances of the yellow line and blue line always remain stable\nat a high level. This outcome verifies the robustness of our model in response to alien data.\n19\nPublished as a conference paper at ICLR 2024\n    \n    \n    \n    \n    \n \n   \n   \n    \n   \n   \n    \n    \n \n \n   \n   \n   \n   \n    \n    \n   \n \n   \n   \n   \n    \n \n    \n \n    \n   \n   \n    \n    \n \n   \n   \n   \n    \n    \n    \n    \n    \n \n   \n   \n   \n   \n    \n    \n \n   \n   \n   \n   \n    \n    \n    \n    \n \n \n   \n   \n    \n    \n \n   \n   \n   \n   \n   \n \n    \n    \n    \n \n    \n   \n    \n    \n    \n    \n    \n    \n \n   \n   \n   \n   \n    \n    \n    \n    \n    \n \n   \n   \n(a) Training Result\n(b) Testing Result\nMUTAG 0\uff1bDO2HSC\nFigure 11: Visualization results of the DO2HSC with MUTAG in different perspectives.\n(a) DOHSC\n(b) DO2HSC\n0\n0.5\n1\n-1\n1.5\n2\n2.5\n-1\n0\n0\n1\n1\n0\n0.5\n1\n1.5\n2\n2.5\n-101\n1.5\n1\n0.5\n0\n-0.5\n-1\n-1.5\n-1\n-0.5\n0\n0.5\n1\n-1\n-0.5\n0\n0.5\n1\n1.5\n-1\n0\n1\n6\n2\n3\n4\n5\n4\n-2\n2\n0\n2\n0\n4\n-1\n0\n1\n2\n3\n4\n5\n6\n-2\n-1\n0\n1\n2\n3\n4\n5\n-2\n0\n2\n-1\n0\n1\n2\n3\n4\n5\n6\n5\n4\n3\n4\n2\n1\n0\n-1\nFigure 12: Anomaly detection comparison between DOHSC and DO2HSC on MUTAG.\nThe percentile parameter sensitivity is presented in this section.\nIt is worth mentioning that\nwe tested DOHSC with varying percentiles in {0.01, 0.1, ..., 0.8} and tested DO2HSC only in\n{0.01, 0.05, 0.1} because the two radii of DO2HSC are obtained by the percentiles \u03bd and 1 \u2212\u03bd.\nThe two radii are equal when \u03bd = 0.5 and the interval between the two co-centered hyperspheres\ndisappears. From the table, the performance would decrease when a larger percentile is set obvi-\nously. For example, on the MUTAG dataset, setting the percentile as 0.01 is more beneficial for\nDOHSC than setting it as 0.8, and setting the percentile as 0.01 is better than setting it as 0.1 for\nDO2HSC due to the change of the interval area.\nTable 9: Parameter sensitivity of DOHSC with different percentiles (all normal data is set to Class\n0.)\nDataset\nPercentile\n0.005\n0.01\n0.1\n0.5\n0.8\nCOX2\n0.5446 (0.0854)\n0.6263 (0.0333)\n0.6022 (0.0789)\n0.5232 (0.0494)\n0.5523 (0.0572)\nER MD\n0.6265 (0.1442)\n0.6620 (0.0308)\n0.7497 (0.0411)\n0.6265 (0.1442)\n0.5141 (0.0398)\nMUTAG\n0.8185 (0.0543)\n0.8822 (0.0432)\n0.8540 (0.0694)\n0.7790 (0.0912)\n0.8675 (0.1287)\nDD\n0.6349 (0.0380)\n0.7083 (0.0188)\n0.6597 (0.0270)\n0.6545 (0.0268)\n0.6327 (0.0206)\nIMDB-Binary\n0.7232 (0.0314)\n0.7160 (0.0600)\n0.7217 (0.0418)\n0.7073 (0.0274)\n0.6773 (0.0566)\n20\nPublished as a conference paper at ICLR 2024\n  \n   \n   \n   \n   \n    \n     \n \n   \n   \n   \n   \n   \n   \n   \n   \n   \n \n   \n                          \n                          \n                   \n                   \n          \n          \n             \n                 \n \n   \n   \n   \n   \n   \n  \n  \n   \n  \n  \n   \n  \n  \n \n \n \n \n          \n          \n             \n                 \n \n   \n   \n   \n   \n   \n  \n  \n   \n  \n  \n   \n  \n  \n \n \n \n \n  \n   \n   \n   \n   \n    \n     \n \n   \n   \n   \n   \n   \n   \n   \n   \n   \n \n   \n                          \n                          \n                   \n                   \n          \n          \n             \n                 \n \n   \n   \n   \n   \n   \n  \n  \n   \n  \n   \n  \n  \n  \n \n \n \n \n          \n          \n             \n                 \n \n   \n   \n   \n   \n   \n  \n  \n   \n  \n   \n  \n  \n  \n \n \n \n \n(a)\n(b)\n(c)\n(d)\n(f)\n(e)\nFigure 13: Parameter sensitivity and robustness of the proposed models. (a)-(b) Parameter sensitivity\nof DOHSC with different hidden layer dimensions of GIN and projection dimensions on COX2 with\nClass 0 and Class 1, respectively. (d)-(e) Parameter sensitivity of DO2HSC with the same settings.\n(c) and (f) shows the performance impacts with different ratios of the training set on the IMDB-\nBinary dataset and added noise disturbances on the MUTAG dataset while training DOHSC and\nDO2HSC, respectively.\nTable 10: Parameter sensitivity of DO2HSC with different percentiles (all normal data is set to Class\n0.)\nDataset\nPercentile\n0.005\n0.01\n0.05\n0.1\nCOX2\n0.5810 (0.0354)\n0.6329 (0.0292)\n0.6149 (0.0187)\n0.5830 (0.0713)\nER MD\n0.6136 (0.0769)\n0.6226 (0.0890)\n0.6867 (0.0226)\n0.6331 (0.1748)\nMUTAG\n0.7278 (0.0478)\n0.9089 (0.0609)\n0.8041 (0.1006)\n0.6769 (0.1207)\nDD\n0.7103 (0.0098)\n0.7320 (0.0194)\n0.6909 (0.0208)\n0.6765 (0.0286)\nIMDB-Binary\n0.6590 (0.0287)\n0.6406 (0.0642)\n0.5348 (0.0486)\n0.5701 (0.0740)\nK\nSUPPLEMENTED RESULTS OF ABLATION STUDY\nFirst, an ablation study of whether orthogonal projection requires standardization was conducted.\nMore precisely, we pursue orthogonal features, that is, finding a projection matrix for orthogonal\nlatent representation (with standardization) instead of computing the projection onto the column\nor row space of the projection matrix (non-standardization), although they are closely related to\neach other. This is equivalent to performing PCA and using standardized principal components.\nTherefore, we compared the DOHSC with and without standardization. From Table 11, 1) it is\nobserved that the performance of DOHSC without standardization is acceptable, and most of its\nresults are better than those of the two-stage baseline, i.e., infoGraph+Deep SVDD. This verifies the\nsuperiority of the end-to-end method over two-stage baselines. 2) The standardized model results\noutperform the non-standardized model results in all cases. 3) DO2HSC surpasses DOHSC no\nmatter with/without the orthogonal projection layer.\nBesides, the ablation study using the mutual information maximization loss is shown in Table 12. It\ncan be intuitively concluded that mutual information loss does not always have a positive effect on\nall data. This also indicates that the designed anomaly detection optimization method and orthogonal\nprojection are effective, instead of entirely, owing to the loss of mutual information.\n21\nPublished as a conference paper at ICLR 2024\nTable 11: Comparison of the orthogonal projection layer with or w/o standardization. \u2018DSVDD\u2019\nstands for \u2018Deep SVDD\u2019. \u2019Non-Std stands for \u2019Non-Standardization\u2019.\nClass\ninfoGraph+DSVDD\nDOHSC (Non-Std)\nDOHSC\nDO2HSC (Non-Std)\nDO2HSC\nMUTAG\n0\n0.8805 \u00b1 0.0448\n0.8521 \u00b1 0.0650\n0.8822 \u00b1 0.0432\n0.9024 \u00b1 0.0207\n0.9089 \u00b1 0.0609\n1\n0.6166 \u00b1 0.2052\n0.6918 \u00b1 0.1467\n0.8115 \u00b1 0.0279\n0.7624 \u00b1 0.0248\n0.8250 \u00b1 0.0790\nCOX2\n0\n0.4825 \u00b1 0.0624\n0.5800 \u00b1 0.0473\n0.6263 \u00b1 0.0333\n0.6127 \u00b1 0.0191\n0.6329 \u00b1 0.0292\n1\n0.5029 \u00b1 0.0700\n0.5029 \u00b1 0.0697\n0.6805 \u00b1 0.0168\n0.6303 \u00b1 0.0276\n0.6923 \u00b1 0.0433\nER MD\n0\n0.5312 \u00b1 0.1545\n0.4881 \u00b1 0.0626\n0.6620 \u00b1 0.0308\n0.6148 \u00b1 0.0484\n0.6867 \u00b1 0.0226\n1\n0.5082 \u00b1 0.0704\n0.5140 \u00b1 0.0356\n0.5184 \u00b1 0.0793\n0.7043 \u00b1 0.0011\n0.7351 \u00b1 0.0159\nDD\n0\n0.3942 \u00b1 0.0436\n0.4029 \u00b1 0.0354\n0.7083 \u00b1 0.0188\n0.7308 \u00b1 0.0015\n0.7320 \u00b1 0.0194\n1\n0.6484 \u00b1 0.0236\n0.6903 \u00b1 0.0215\n0.7579 \u00b1 0.0154\n0.7000 \u00b1 0.0165\n0.7651 \u00b1 0.0317\nIMDB-Binary\n0\n0.6353 \u00b1 0.0277\n0.5149 \u00b1 0.0655\n0.6609 \u00b1 0.0033\n0.6387 \u00b1 0.0578\n0.7547 \u00b1 0.0390\n1\n0.5836 \u00b1 0.0995\n0.6505 \u00b1 0.0585\n0.7705 \u00b1 0.0045\n0.7032 \u00b1 0.0328\n0.7737 \u00b1 0.0503\nCOLLAB\n0\n0.5662 \u00b1 0.0597\n0.6067 \u00b1 0.1007\n0.9185 \u00b1 0.0455\n0.7089 \u00b1 0.0335\n0.9390 \u00b1 0.0025\n1\n0.7926 \u00b1 0.0986\n0.8958 \u00b1 0.0141\n0.9755 \u00b1 0.0030\n0.9033 \u00b1 0.0089\n0.9836 \u00b1 0.0115\n2\n0.4062 \u00b1 0.0978\n0.4912 \u00b1 0.2000\n0.8826 \u00b1 0.0250\n0.7158 \u00b1 0.1059\n0.8835 \u00b1 0.0118\nTable 12: Comparison of the loss supervision with or w/o mutual information loss (MIL).\nClass\nDOHSC (Non-MIL)\nDOHSC\nDO2HSC (Non-MIL)\nDO2HSC\nMUTAG\n0\n0.9456 \u00b1 0.0189\n0.8822 \u00b1 0.0432\n0.8308 \u00b1 0.0548\n0.9089 \u00b1 0.0609\n1\n0.7597 \u00b1 0.0802\n0.8115 \u00b10.0279\n0.7915 \u00b1 0.0274\n0.8250 \u00b1 0.0790\nCOX2\n0\n0.6349 \u00b1 0.0466\n0.6263 \u00b1 0.0333\n0.6143 \u00b1 0.0302\n0.6329 \u00b1 0.0292\n1\n0.6231 \u00b1 0.0501\n0.6805 \u00b1 0.0168\n0.6576 \u00b1 0.1830\n0.6923 \u00b1 0.0433\nER MD\n0\n0.5837 \u00b1 0.0778\n0.6620 \u00b1 0.0308\n0.5836 \u00b1 0.0909\n0.6867 \u00b1 0.0226\n1\n0.6465 \u00b1 0.0600\n0.5184 \u00b1 0.0793\n0.7424 \u00b1 0.0385\n0.7351 \u00b1 0.0159\nDD\n0\n0.4738 \u00b1 0.0412\n0.7083 \u00b1 0.0188\n0.6882 \u00b1 0.0221\n0.7320 \u00b1 0.0194\n1\n0.7197 \u00b1 0.0185\n0.7579 \u00b1 0.0154\n0.7376 \u00b1 0.0244\n0.7651 \u00b1 0.0317\nIMDB-Binary\n0\n0.5666 \u00b1 0.0810\n0.6609 \u00b1 0.0033\n0.6303 \u00b1 0.0538\n0.7547 \u00b1 0.0390\n1\n0.6827 \u00b1 0.0239\n0.7705 \u00b1 0.0045\n0.6810 \u00b1 0.0276\n0.7737 \u00b1 0.0503\nCOLLAB\n0\n0.9330 \u00b1 0.0539\n0.9185 \u00b1 0.0455\n0.5415 \u00b1 0.0182\n0.9390 \u00b1 0.0025\n1\n0.9744 \u00b1 0.0017\n0.9755 \u00b1 0.0030\n0.9293 \u00b1 0.0023\n0.9836 \u00b1 0.0115\n2\n0.8275 \u00b1 0.0765\n0.8826 \u00b1 0.0250\n0.8452 \u00b1 0.0243\n0.8835 \u00b1 0.0118\nTo demonstrate the effectiveness of the orthogonal projection layer (OPL), we conducted ablation\nstudies and compared the comparison of 3-dimensional results produced with and without the OPL.\nFor each model trained on a particular dataset class, we show the result without OPL on the left side,\nwhereas the result with OPL is displayed on the right. As Figure 14 illustrates, the OPL drastically\nimproves the distribution of the embeddings to be more spherical rather than elliptical. Similarly,\nwith the help of the OPL, the other embeddings exhibited a more compact and rounded layout.\n   \n \n    \n    \n    \n    \n \n   \n   \n   \n \n   \n    \n   \n   \n  \n \n   \n \n  \n \n  \n  \n \n \n  \n  \n  \n   \n \n    \n \n    \n    \n   \n \n   \n    \n   \n   \n  \n \n \n \n  \n \n  \n \n  \n  \n   \n \n \n \n \n   \n \n    \n    \n    \n    \n \n   \n   \n   \n \n   \n    \n \n \n \n \n  \n  \n   \n  \n \n  \n  \n  \n \n   \n   \n   \n  \n \n \n  \n  \n  \n   \n   \n   \n \n \n  \n  \n  \n   \n    \n    \n    \n    \n \n   \n   \n   \n   \n   \n \n \n    \n    \n   \n    \n   \n    \n \n   \n   \n \n   \n   \n \n   \n    \n    \n    \n \n \n \n \n  \n  \n   \n  \n \n  \n  \n  \n \n  \n \n \n \n  \n \n  \n \n  \n  \n   \n \n \n \n \n(a) DOHSC\n(a) DOHSC\n(b) DO2HSC\n(b) DO2HSC\nMUTAG 1\nMUTAG 0\nFigure 14: Visualizations on the MUTAG dataset Class 0 (left: without OPL; right: with OPL).\nL\nIMBALANCED EXPERIMENTAL RESULTS\nWe also give the experiment on graph-level datasets with an imbalanced setting of the ratio between\nanomalies and normal graphs in the experimental datasets. Please refer to Table 13, which showcases\n22\nPublished as a conference paper at ICLR 2024\n   \n \n    \n    \n    \n    \n \n   \n   \n   \n \n   \n    \n   \n   \n  \n \n   \n \n  \n \n  \n  \n \n \n  \n  \n  \n   \n \n    \n \n    \n    \n   \n \n   \n    \n   \n   \n  \n \n \n \n  \n \n  \n \n  \n  \n   \n \n \n \n \n   \n \n    \n    \n    \n    \n \n   \n   \n   \n \n   \n    \n \n \n \n \n  \n  \n   \n  \n \n  \n  \n  \n \n   \n   \n   \n  \n \n \n  \n  \n  \n   \n   \n   \n \n \n  \n  \n  \n   \n    \n    \n    \n    \n \n   \n   \n   \n   \n   \n \n \n    \n    \n   \n    \n   \n    \n \n   \n   \n \n   \n   \n \n   \n    \n    \n    \n \n \n \n \n  \n  \n   \n  \n \n  \n  \n  \n \n  \n \n \n \n  \n \n  \n \n  \n  \n   \n \n \n \n \n(a) DOHSC\n(a) DOHSC\n(b) DO2HSC\n(b) DO2HSC\nMUTAG 1\nMUTAG 0\nFigure 15: Visualizations on the MUTAG dataset Class 1 (left: without OPL; right: with OPL).\n   \n   \n  \n  \n   \n  \n  \n  \n   \n \n \n   \n \n \n   \n \n \n   \n \n  \n \n  \n \n  \n    \n    \n   \n \n   \n   \n   \n \n   \n   \n    \n   \n \n    \n    \n    \n    \n    \n  \n \n  \n \n \n \n \n \n \n \n  \n \n  \n  \n  \n   \n   \n  \n \n \n  \n \n  \n \n \n  \n  \n   \n   \n  \n \n \n   \n \n  \n  \n  \n \n \n  \n \n   \n  \n   \n    \n    \n    \n    \n \n \n   \n   \n   \n    \n   \n   \n   \n \n   \n   \n   \n   \n   \n   \n  \n  \n \n \n  \n \n  \n \n  \n   \n  \n(a) DOHSC\n(a) DOHSC\n(b) DO2HSC\n(b) DO2HSC\nCOX2 0\nCOX2 1\nFigure 16: Visualizations on the COX2 dataset Class 0 (left: without OPL; right: with OPL).\nour results on imbalanced datasets. The experimental results illustrate that the proposed methods\novercome the imbalanced problem better than other algorithms in general. But DO2HSC has more\nadvantageous results.\nM\nRELATED WORK\nM.1\nSOME SOTA ANOMALY DETECTION METHODS\nIn this section, we first provide an overview of some SOTA anomaly detection methods. The method\nproposed by Perera et al. (2019) involves adversarial training of an auto-encoder and a discrimina-\ntor, while compelling the latent representation by one class of data. Goyal et al. (2020) judged\nthe anomalous data according to the assumption that the normal instances generally lie on a low-\ndimensional locally linear manifold, and regarded the process of finding the decision boundary in\nthe embedding space as an adversarial manner. Hu et al. (2020) combined holistic regularization\nwith a 2-norm instance-level normalization, thus further proposing an effective one-class learning\nmethod. Cai & Fan (2022) proposed a perturbation learning based anomaly detection method, which\ngenerates the negative samples containing the smallest noise as much as possible, to train a detec-\ntion classifier. The assumption is that, if this classifier can discriminate this type of negative samples\nand normal data, it should have the ability to distinguish more different anomalous data. All afore-\nmentioned algorithms are compared in our experimental section to support the effectiveness of the\nproposed improvements.\n2\n2.5\n3\n3.5\n4\n-10\n-10\n-8\n-6\n-12\n-4\n-2\n-4\n-14\n0\n2\n-16\n4\n-0.4\n-0.2\n0.4\n0\n0.2\n0.2\n0.4\n0\n0.4\n0.6\n-0.2\n0.2\n0\n-0.4\n-0.2\n-0.4\n-0.6\n-0.6\n-4\n-2\n0\n2\n4\n5\n0\n5\n10\n15\n-5\n0\n5\n10\n15\nCOX2 0\nCOX2 1\n(b) DO2HSC\n(a) DOHSC\n-0.2\n-0.3\n-0.2\n-0.1\n0\n0\n0.1\n0.2\n0.3\n-0.2\n0.4\n0.5\n0.2\n0\n0.2\n0.4\n0.4\n-0.4\n-0.2\n0.6\n0\n0.2\n0.4\n0.4\n0.6\n0.2\n-0.6\n0\n-0.4\n-0.2\n-0.2\n0\n0.2\n-0.4\n0.4\nFigure 17: Visualizations on the COX2 dataset Class 1 (left: without OPL; right: with OPL).\n23\nPublished as a conference paper at ICLR 2024\nTable 13: Average AUCs with standard deviation (10 trials) of imbalanced experiments (the ratio of\nnormal data to abnormal data is 10:1). \u2018DSVDD\u2019 stands for \u2018Deep SVDD\u2019. The best two results are\nhighlighted in bold and \u2019\u2013\u2019 means out of memory.\nCOX2\nER MD\nMUTAG\n0\n1\n0\n1\n0\n1\nSP+OCSVM\n0.4854 \u00b1 0.0000\n0.7874 \u00b1 0.0000\n0.2814 \u00b1 0.0000\n0.0764 \u00b1 0.0000\n0.2917\u00b1 0.0000\n0.0266 \u00b1 0.0000\nWL+OCSVM\n0.4127 \u00b1 0.0000\n0.8125 \u00b1 0.0000\n0.5142 \u00b1 0.0000\n0.1909 \u00b1 0.0000\n0.7083 \u00b1 0.0000\n0.0399 \u00b1 0.0000\nNH+OCSVM\n0.3818 \u00b1 0.0385\n0.4875 \u00b1 0.0000\n0.5774 \u00b1 0.0273\n0.3215 \u00b1 0.0274\n0.1910 \u00b1 0.0000\n0.0573 \u00b1 0.0833\nRW+OCSVM\n\u2013\n\u2013\n0.5220 \u00b1 0.0000\n0.2604 \u00b1 0.0000\n0.9166 \u00b1 0.0000\n0.2800 \u00b1 0.0000\nOCGIN\n0.6373 \u00b1 0.0276\n0.5650 \u00b1 0.2606\n0.6574 \u00b1 0.0487\n0.3208 \u00b1 0.0779\n0.6333 \u00b1 0.1261\n0.7387 \u00b1 0.1990\nInfoGraph+DSVDD\n0.5137 \u00b1 0.0000\n0.6150 \u00b1 0.1594\n0.5519 \u00b1 0.1367\n0.7653 \u00b1 0.0806\n0.5417 \u00b1 0.2814\n0.3787 \u00b1 0.1049\nGLocalKD\n0.6465 \u00b1 0.0066\n0.7063 \u00b1 0.1391\n0.2578 \u00b1 0.0000\n0.1979 \u00b1 0.0000\n0.8958 \u00b1 0.0335\n0.9719 \u00b1 0.0039\nOCGTL\n0.5394 \u00b1 0.0340\n0.6150 \u00b1 0.0903\n0.5009 \u00b1 0.0805\n0.6972 \u00b1 0.0939\n0.6792 \u00b1 0.0914\n0.9227 \u00b1 0.0116\nDOHSC\n0.7784 \u00b1 0.0639\n0.8600 \u00b1 0.0339\n0.7601 \u00b1 0.1000\n0.9181 \u00b1 0.0203\n0.9583 \u00b1 0.0373\n0.9653 \u00b1 0.0217\nDO2HSC\n0.7928 \u00b1 0.0327\n0.9050 \u00b1 0.0292\n0.8443 \u00b1 0.0339\n0.9375 \u00b1 0.0669\n0.9792 \u00b1 0.0208\n0.9800 \u00b1 0.0133\nM.2\nGRAPH KERNELS AND GRAPH NEURAL NETWORKS\nGraph kernels (Kriege et al., 2020) measure the similarity between graphs and are very useful in\nmany tasks involving graphs, such as graph classification. A large body of work has emerged in\nthe past years, including kernels based on neighborhood aggregation techniques and walks and\npaths. Shervashidze et al. (2011) introduced the Weisfeiler-Lehman (WL) algorithm, a well-known\nheuristic for graph isomorphism. In (Hido & Kashima, 2009), Neighborhood Hash kernel was in-\ntroduced, where the neighborhood aggregation function is binary arithmetic. The most influential\ngraph kernel for paths-based kernels is the shortest-path (SP) kernel (Borgwardt & Kriegel, 2005).\nFor walks-based kernels, G\u00a8artner et al. (2003) and Kashima et al. (2003) simultaneously proposed\ngraph kernels based on random walks, which count the number of label sequences along walks that\ntwo graphs have in common. These graph kernel methods have the desirable property that they do\nnot rely on the vector representation of data explicitly but access data only via the Gram matrix.\nAnother powerful and popular tool for handling graph data is the graph neural network. GNNs play\na crucial role in effectively aggregating neighbor information for each node based on the edges in\ngraph data. In the past decade, various improvements and enhancements for GNNs have been pro-\nposed (Welling & Kipf, 2016; Hamilton et al., 2017; Xu et al., 2019; Sun et al., 2020; Wu et al.,\n2023; Sun et al., 2023). GNNs can be applied to both node-level tasks and graph-level tasks. For\ngraph-level tasks, one fundamental problem is graph representation learning, which aims to rep-\nresent each graph as a vector and often requires a readout or pooling operation. Xu et al. (2019)\nshowed that it is more effective to use a sum function to convert the representations of nodes of each\ngraph to a vector, compared to mean and max functions. Wu et al. (2023) proposed a framework\nof graph learning based on kernel functions, which has a comparable or even better performance\ncompared to GNNs.\nM.3\nGRAPH-LEVEL ANOMALY DETECTION\nThere are few studies undertaken in graph-level anomaly detection (GAD). Existing solutions to\nGAD can be categorized into two families: two-stage and end-to-end. Two-stage GAD methods\n(Breunig et al., 2000; Sch\u00a8olkopf et al., 1999) first transform graphs into graph embeddings by graph\nneural networks or into similarities between graphs by graph kernels, and then apply off-the-shelf\nanomaly detectors. The drawbacks mainly include: 1) the graph feature extractor and outlier detec-\ntor are independent; 2) some graph kernels produce \u201chand-crafted\u201d features that are deterministic\nwithout much space to improve. Whereas, end-to-end approaches overcome these problems by uti-\nlizing deep graph learning techniques (such as graph convolutional network (Welling & Kipf, 2016)\nand graph isomorphism network (Xu et al., 2019)), which learn an effective graph representation\nwhile detecting graph anomaly (Zhao & Akoglu, 2021; Qiu et al., 2022; Ma et al., 2022).\nIn the past decades, regarding more end-to-end unsupervised graph-level anomaly detections, the\ngraph kernel measures the similarity between graphs. It regards the result as a representation non-\nstrictly or implicitly. However, the graph anomaly detection task associated with it usually performs\na two-stage process, which cannot maintain the quality of representation learning while learning\nnormal data patterns. Further concerning end-to-end models, Ma et al. (2022) proposed a global\n24\nPublished as a conference paper at ICLR 2024\nand local knowledge distillation method for graph-level anomaly detection, which learns rich global\nand local normal pattern information by random joint distillation of graph and node representations\nwhile needing to train two graph convolutional networks jointly at a high time cost. Zhao & Akoglu\n(2021) combined the Deep Support Vector Data Description (Deep SVDD) objective function and\ngraph isomorphism network to learn a hypersphere of normal samples. Qiu et al. (2022) sought a\nhypersphere decision boundary and optimized the representations learned by k Graph Neural Net-\nworks (GNN) close to the reference GNN while maximizing the differences between k GNNs, but\ndid not consider the relationship between the graph-level representation and node features.\n25\n",
    "2202.07082": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nGraph Neural Networks for Graphs with\nHeterophily: A Survey\nXin Zheng, Yi Wang, Yixin Liu, Ming Li Member, IEEE, Miao Zhang, Di Jin, Philip S. Yu Fellow, IEEE,\nShirui Pan Senior Member, IEEE\nAbstract\u2014Recent years have witnessed fast developments of graph neural networks (GNNs) that have benefited myriads of graph\nanalytic tasks and applications. In general, most GNNs depend on the homophily assumption that nodes belonging to the same class\nare more likely to be connected. However, as a ubiquitous graph property in numerous real-world scenarios, heterophily, i.e., nodes\nwith different labels tend to be linked, significantly limits the performance of tailor-made homophilic GNNs. Hence, GNNs for\nheterophilic graphs are gaining increasing research attention to enhance graph learning with heterophily. In this paper, we provide a\ncomprehensive review of GNNs for heterophilic graphs. Specifically, we propose a systematic taxonomy that essentially governs\nexisting heterophilic GNN models, along with a general summary and detailed analysis. Furthermore, we discuss the correlation\nbetween graph heterophily and various graph research domains, aiming to facilitate the development of more effective GNNs across a\nspectrum of practical applications and learning tasks in the graph research community. In the end, we point out the potential directions\nto advance and stimulate more future research and applications on heterophilic graph learning with GNNs.\nIndex Terms\u2014Graph neural networks, heterophily, graph representation learning, message passing.\n\u2726\n1\nINTRODUCTION\nG\nRAPHS are pervasively structured data and have been widely\nused in many real-world scenarios, such as social net-\nworks [1], [2], knowledge bases [3], traffic networks [4], and\nrecommendation systems [5], [6]. Recently, graph neural networks\n(GNNs) have achieved remarkable success with powerful learning\nability and become prevalent models to tackle various graph\nanalytical tasks, such as node classification, link prediction, and\ngraph classification [3], [7]\u2013[9].\nWhile a large number of GNNs with diverse architectures\nhave been designed [10]\u2013[15], the majority of them follow the\nhomophily assumption, i.e., nodes with similar features or same\nclass labels are linked together. For example, in citation networks,\na study usually cites reference papers from the same research\narea [16]. However, real-world graphs do not always obey the\n\u2022\nXin Zheng is with the Department of Software System and Cy-\nbersecurity (SSC), Monash University, Melbourne, Australia. E-mail:\nxin.zheng@monash.edu.\n\u2022\nYi Wang is with the Department of Computer Science, Zhejiang Normal\nUniversity, Jinhua, China. E-mail: wangyi@zjnu.edu.cn.\n\u2022\nYixin Liu is with the Department of Data Science and AI (DSAI), Monash\nUniversity, Melbourne, Australia. E-mail: yixin.liu@monash.edu.\n\u2022\nMing Li is with the Key Laboratory of Intelligent Education Technology\nand Application of Zhejiang Province, Zhejiang Normal University, Jin-\nhua, China. E-mail: mingli@zjnu.edu.cn.\n\u2022\nMiao Zhang is with the School of Computer Science and Technology,\nHarbin Institute of Technology (Shenzhen), Shenzhen, China. E-mail:\nzhangmiao@hit.edu.cn.\n\u2022\nDi Jin is with the School of Computer Science and Technology, Tianjin\nUniversity, Tianjin, China. E-mail: jindi@tju.edu.cn.\n\u2022\nPhilip S. Yu is with the Department of Computer Science, University of\nIllinois at Chicago, Chicago, USA. E-mail: psyu@uic.edu.\n\u2022\nShirui Pan is with the School of Information and Communication Tech-\nnology and Institute for Integrated and Intelligent Systems (IIIS), Griffith\nUniversity, Queensland, Australia. Email: s.pan@griffith.edu.au.\n\u2022\nXin Zheng and Yi Wang contributed equally to this work.\n\u2022\nCorresponding Author: Shirui Pan.\nCustomers\nFraudsters\nMath.\nPaper\nStat.\nPaper\nCS\nPaper\n(a) Homophilic Graph\n(b) Heterophilic Graph\nFig. 1: Examples of homophilic and heterophilic graphs (Left: (a) a\ncitation network; Right: (b) an online transaction network).\nhomophily assumption but show an opposite property, i.e., het-\nerophily that linked nodes have dissimilar features and different\nclass labels [17]\u2013[21]. For instance, in online transaction networks,\nfraudsters are more likely to build connections with customers\ninstead of other fraudsters [22]; in dating networks, most people\nprefer to date with people of the opposite gender [23]; in molecular\nnetworks, protein structures are more likely composed of different\ntypes of amino acids that are linked together [24]. The examples\nof homophilic and heterophilic graphs are provided in Fig. 1 to\nillustrate their difference visually. Importantly, such heterophily\nrestricts the learning ability of existing homophilic GNNs on\ngeneral graph-structural data, resulting in significant performance\ndegradation on heterophilic graphs [23]\u2013[25].\nCore Challenges of GNNs for Heterophily. We attribute\nthe performance degradation to the uniform message passing\nframework under the homophily setting. The procedure of this\nframework can be summarized as: first aggregating the messages\nextracted from local neighbor nodes, then updating the final ego\nnode (the current central node itself) representations with aggre-\ngated neighbor messages. Nevertheless, due to the heterophily\nproperty of graphs, this mechanism poses significant challenges\narXiv:2202.07082v3  [cs.LG]  25 Feb 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nfor the development of heterophilic GNNs, primarily manifesting\nin two aspects:\n\u2022\nChallenge-1:\nUndiscovered\nNon-local\nNeighbors.\nGuided by homophily, neighbor aggregation in homophilic\nGNNs restricts information extraction to the proximal\nlocal topology of graphs. When applied to graphs with\nheterophily, it fails to explore non-local topology, where\nheterophilic nodes of the same class are typically situated\nat long-term distances. This poses a significant challenge\nin identifying and learning informative nodes with high\nstructural and semantic similarities on heterophilic graphs.\n\u2022\nChallenge-2: Indistinguishable Node Representation\nLearning. Homophilic GNNs employ uniform local\nneighbor aggregation and update the central node rep-\nresentation when they are typically similar and share\nthe same labels. Consequently, on heterophilic graphs,\nthe discrepancy between similar non-local neighbors and\ndissimilar local neighbors can not be effectively captured.\nThis results in critical challenges in learning discrimina-\ntive node representations with distinguishable heterophily\ninformation through diverse customized message passing.\nIn light of these challenges, recently, an increasing number\nof researchers have started turning their attention to the study of\nGNNs with heterophily. The research focus is sufficiently broad,\nfrom heterophilic graph data exploration [26]\u2013[28] to various\ntechnical algorithm development [17], [19], [21], [25], [29]\u2013[31].\nImportance of Developing Heterophilic GNNs. Heterophilic\ngraph learning with GNNs is becoming an upward trending\nresearch topic and it shows closely tied connections with diverse\ndomains in graph research. The compelling significance of GNN\ndevelopment for heterophilic graphs is underscored by the follow-\ning aspects:\n\u2022\nEnhancing the understanding of complex and diverse het-\nerophily graphs. Graph-structure data with the heterophilic\nproperty presents great complexity and diversity, and it is\nprevalent across various real-world application scenarios,\nranging from daily-life personal relationships to scientific\nchemical molecular study. A thorough and ongoing ex-\nploration of heterophily graph data would significantly\nenhance the understanding of complex and diverse het-\nerophily graphs, thereby providing valuable guidance for\nthe development of GNN models and advancements in\nheterophily graph learning.\n\u2022\nAdvancing heterophilic graph analysis and learning. Het-\nerophilic graph analysis and learning tasks are still open\nand promising research topics in development, while\nnumerous challenges need to be tackled for designing\nheterophilic GNN models with expressive performance,\nrobustness, and generalization ability. The advancement\nof heterophilic GNNs is pivotal for unlocking the full po-\ntential of heterophilic graph analysis in addressing various\npractical graph learning tasks covering both heterophilic\nnode representations and graph structures.\n\u2022\nAdapting with versatility in heterophilic GNN develop-\nment. As heterophilic graphs exist prevalently and show\nclose connections with various graph research domains,\ne.g., over-smoothing and anomaly detection. Hence, de-\nveloping specialized heterophilic GNN architectures and\nlearning techniques would be pivotal in expanding the ver-\nsatility and adaptability of GNN models, unleashing power\nof heterophilic GNNs in cross graph research domains and\napplications.\nIn this paper, we present a comprehensive and systematic\nreview of GNNs for heterophilic graphs, aiming to provide a gen-\neral blueprint of heterophilic graph research. It can be beneficial\nto establish connections and make comparisons among different\nheterophilic GNN methods, leading to an in-depth understanding\nof how different methods tackle the challenges of heterophily\nlearning. We are expecting that our survey will significantly\ninspire and facilitate the development of heterophilic graphs 1.\nThe contributions of our work are summarized as follows:\n\u2022\nComprehensive Overview: We provide a comprehen-\nsive overview of current heterophilic GNNs in terms of\ndata, algorithms, and applications. We provide detailed\ndescriptions of each model type, along with the necessary\ncomparison and the gist summary.\n\u2022\nSystematic Taxonomy: We provide a systematic taxon-\nomy of heterophilic GNNs and categorize existing meth-\nods into three classes, i.e., non-local neighbor extension\nmethods, GNN architecture refinement methods, and hy-\nbrid methods.\n\u2022\nThorough Discussion: We provide a thorough discussion\nof the correlation between graph heterophily and various\ngraph research domains, including the relation between\ngraph heterophily and model robustness, over-smoothing,\nand graph anomaly detection.\n\u2022\nFuture Directions: We suggest promising future research\ndirections and discuss the limitations of existing het-\nerophilic GNNs from multiple perspectives, namely inter-\npretability, robustness, scalability, and heterophilic graph\ndata exploration.\nThe remainder of this article is organized as follows. Section\n2 defines the related concepts and provides notations used in this\nsurvey. Section 3 describes the framework of heterophilic GNNs\nand provides the taxonomy. Section 4-6 review three categories\nof heterophilic GNNs methods respectively. Section 7 discusses\nheterophily GNNs on diverse graphs and the correlation between\nheterophily and diverse graph research domains. Section 8 ana-\nlyzes the unexplored challenges and potential future directions.\nSection 9 concludes this article in the end. More details of real-\nworld heterophilic graph dataset benchmarks, open-source codes,\nand the overall development timeline of heterophilic GNNs can be\nfound in the appendix.\n2\nPRELIMINARY\n2.1\nNotations\nLet G = (V, E) be an undirected, unweighted graph where\nV = {v1, \u00b7 \u00b7 \u00b7 , v|V|} is the node set and E \u2208V \u00d7 V is the\nedge set. The neighbor set of node v is denoted as N(v) =\n{u : (v, u) \u2208E}. The node features are represented by a\nfeature matrix X \u2208R|V|\u00d7d, where the i-th row xi \u2208Rd is\nthe feature vector of node vi and d is the number of feature\ndimensions. Connectivity is represented by the adjacency matrix\nA \u2208{0, 1}n\u00d7n. For any matrix A, we use Auv to refer to the\nscalar value at the (u, v) location. The graph Laplacian is defined\nas L = D \u2212A, where D \u2208Rn\u00d7n is the diagonal degree matrix.\n1. Our preprint of this article [32] has attracted 110+ citations, showing a\nstrong uptrend of this research topic.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nDue to its generalization ability [33], the symmetric normalized\nLaplacian is often used, which is defined as \u02dcL = D\u22121/2LD\u22121/2.\nAnother option is random walk normalization: \u02dcL = D\u22121L. Note\nthat normalization could also be applied to the adjacency matrix.\nTheir relationship can be derived as \u02dcL = I \u2212\u02dcA, where I is the\nidentity matrix. Through eigendecomposition, L can be expressed\nas \u02dcL = U\u039bUT, where each column of U \u2208Rn\u00d7n represents\nan eigenvector of L, \u039b is the diagonal matrix whose diagonal\nelements are the corresponding eigenvalues (i.e., \u039bii = \u03bbi).\n2.2\nGraph Neural Networks\nGenerally, GNNs adopt the message passing mechanism, where\neach node representation is updated by aggregating the messages\nfrom local neighbor representations, and then combining the ag-\ngregated messages with its ego representation [10]. The updating\nprocess of the l-th GNN layer for each node v \u2208V can be\ndescribed as:\nm(l)\nv\n= AGGREGATE(l)({h(l\u22121)\nu\n: u \u2208N(v)}),\nh(l)\nv\n= UPDATE(l)(h(l\u22121)\nv\n, m(l)\nv ),\n(1)\nwhere m(l)\nv\nand h(l)\nv\nstand for the message vector and the\nrepresentation vector of node v at the l-th layer, respectively. And\nAGGREGATE(\u00b7) and UPDATE(\u00b7) are aggregation function\n(e.g., mean, LSTM, and max pooling) and update function (e.g.,\nlinear-layer combination) [8], respectively. Given the input of the\nfirst layer as H(0) = X, the learned node representations at each\nlayer of L-layer GNN can be denoted as H(l) =\nh\nh(l)\nv\ni\nfor\nv = (1, \u00b7 \u00b7 \u00b7 , |V|) and l = (1, \u00b7 \u00b7 \u00b7 , L). For node classification\ntask, the final node representation H(L) would be fed into a\nclassifier network (e.g., a fully-connected layer) to generate the\npredictions for classes.\n2.3\nSpectral Graph Convolution\nA graph convolution operation is defined in the Fourier domain\nsuch that\nf1 \u2217f2 = U[(UTf1) \u2299(UTf2)],\n(2)\nwhere \u2299is the element-wise product, and f1/f2 are two signals\ndefined on nodes. It follows that a node signal f2 = X is filtered\nby spectral signal \u02c6f1 = UTf1 = g as\nh(l)\nv =g(\u02dcL)h(l\u22121)\nv\n=U[g(\u039b)\u2299(UTh(l\u22121)\nv\n)]=Ug(\u039b)UTh(l\u22121)\nv\n,\n(3)\nwhere g is known as frequency response function. Therefore,\nthe objective of spectral methods is to learn a function g(\u00b7). In\nsimpler terms, g(\u00b7) can be seen as a way to re-weight signals of\ndifferent frequencies (or eigenvalues). Eigenvalues represent the\nsmoothness or frequency of the corresponding eigenvectors. Con-\nsequently, assigning greater weight to smaller eigenvalues retains\nmore low-frequency information, while assigning greater weight\nto larger eigenvalues retains more high-frequency information. In\ngeneral, FLP = \u03f5I + D\u22121/2AD\u22121/2 = (\u03f5 + 1)I \u2212L is a low-\npass filter, while FHP = \u03f5I \u2212D\u22121/2AD\u22121/2 = (\u03f5 \u22121)I + L\ndenotes high-pass filter.\n2.4\nMeasure of Heterophily & Homophily\nIn general, heterophily and homophily of a graph G = (V, E) can\nbe measured by following metrics: node homophily [34], edge\nhomophily [24], class homophily [27], label informativeness (LI)\n[35], and adjusted homophily [35].\nNode homophily and edge homophily are two essential mea-\nsures. Concretely, the node homophily Hnode is the average\nproportion of the neighbors with the same class of each node:\nHnode = 1\n|V|\nX\nv\u2208V\n|{u \u2208N(v) : yv = yu}|\n|N(v)|\n.\n(4)\nThe edge homophily Hedge is the proportion of edges con-\nnecting two nodes with the same class:\nHedge = |{(v, u) \u2208E : yv = yu}|\n|E|\n.\n(5)\nTo alleviate the sensitivity issue of node homophily Hnode and\nedge homophily Hedge on the node class number and balance, the\nclass homophily measures the average proportion of neighbors\nwith the same class across all nodes of each class while taking\ninto account the constraint of class proportions, that is\nHclass =\n1\nC \u22121\nC\nX\nc=1\n\uf8ee\n\uf8ef\uf8f0\nP\nv:yv=c |{u \u2208N(v):yv =yu}|\nP\nv:yv=c |N(v)|\n\u2212nc\n|V|\n\uf8f9\n\uf8fa\uf8fb\n+\n,\n(6)\nwhere nc = |{u : yu = c}| denotes the node number of class c.\nEnsuring that the homophily measure satisfies essential prop-\nerties is crucial for enhancing reliability, with a particular focus\non maximum consistency and maintaining a constant baseline. To\nestablish the constant baseline property, the adjusted homophily\nmeasure Hadj subtracts the expected value of the measure from\nHedge. Furthermore, to enable the obtained quantity comparisons\nacross different datasets, label informativeness (LI) quantifies the\ninformativeness of a neighbor\u2019s label for a node\u2019s label. These\nmeasures are respectively defined as\nHadj =\nHedge \u2212PC\nc=1\n \nP\nv:yv=c |N(v)|/(2|E|)\n!2\n1 \u2212PC\nc=1\n \nP\nv:yv=c |N(v)|/(2|E|)\n!2\n,\n(7)\nand\nLI = (H(yu) \u2212H(yu|yv))/H(yu),\n(8)\nwhere H(yu) measures the \u201chardness\u201d of predicting the label of\nu without knowing yv. H(yu|yv) is the conditional entropy.\nNote that the range of Hnode, Hedge, Hclass and LI is [0, 1].\nGraphs with strong homophily have higher Hnode, Hedge, Hclass\nand LI (closer to 1); whereas graphs with strong heterophily\nhave smaller Hnode, Hedge, Hclass and LI (closer to 0). The\nmeasure Hadj is appropriately adjusted based on Hedge, and it\ncan occasionally have a negative value when Hedge is smaller\nthan PC\nc=1\n \nP\nv:yv=c |N(v)|/(2|E|)\n!2\n. Nevertheless, the overall\ntrend of Hadj aligns with the previous four measures: larger\nvalues indicate stronger homophily, while smaller values suggest\nstronger heterophily. Besides, we would like to emphasise that\nvarious metrics of homophily and heterophily are suited to specific\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\nTABLE 1: Summary of existing measure methods of heterophily and homophily.\nMeasure Methods\nDefinition\nDescription\nNode Homophily\nHnode =\n1\n|V|\nP\nv\u2208V\n|{u\u2208N (v):yv=yu}|\n|N (v)|\nBoth Hnode and Hedge are within the range of [0, 1]. These two\nmeasures are intuitive, but they are relatively sensitive to the number\nof classes and the balance between these classes.\nEdge Homophily\nHedge = |{(v,u)\u2208E:yv=yu}|\n|E|\nClass Homophily\nHclass =\n1\nC\u22121\nC\nP\nc=1\n\"\nP\nv:yv=c |{u\u2208N (v):yv=yu}|\nP\nv:yv=c |N (v)|\n\u2212nc\n|V|\n#\n+\nHclass avoids the class sensitivity issue of Hnode and Hedge and falls\nwithin the range [0, 1]. However, it doesn\u2019t consider certain desirable\nproperties of a reasonable homophily measure, such as variations in\nnode degrees.\nAdjusted Homophily\nHadj =\nHedge\u2212PC\nc=1\n \nP\nv:yv=c |N (v)|/(2|E|)\n!2\n1\u2212PC\nc=1\n \nP\nv:yv=c |N (v)|/(2|E|)\n!2\nHadj is comparable across different datasets with varying numbers of\nclasses and class size balances. In general, Hadj < Hedge holds, and\nHadj sometimes take on negative values.\nLabel Informativeness (LI)\nLI = H(yu)\u2212H(yu|yv)\nH(yu)\nLI is a straightforward graph characteristic that is suitable for compar-\ning different datasets.\nuse cases, and we aim to discuss several characteristics of these\nmetrics.\nFor one thing, although the node homophily Hnode and edge\nhomophily Hedge are intuitive, they are relatively sensitive to the\nnumber of classes and the balance between these classes. This\ncan make them difficult to interpret and render them incomparable\nacross different datasets. For example, suppose that each node\nin a graph is connected to one node of each class. Then, both\nedge homophily and node homophily for this graph will be equal\nto 1/C. As a result, these metrics will produce widely different\nvalues for graphs with different numbers of classes, despite these\ngraphs being similar in exhibiting no homophily.\nFor another thing, class homophily does not account for vari-\nations in node degrees when correcting the fraction of intra-class\nedges by its expected value. Specifically, if nodes of class c have,\non average, larger degrees than 2|E|/|V|, then the probability\nthat a random edge goes to that class can be significantly larger\nthan nc/|V|. Secondly, class homophily only considers positive\ndeviations from nc/|V|, meaning that it doesn\u2019t take into account\nclasses with heterophilic connectivity patterns.\nAs advised by [35], Hadj is a better alternative to the com-\nmonly used measures, as it satisfies most of the desirable prop-\nerties. Additionally, this research also suggests another measure\ncalled \u201clabel informativeness (LI)\u201d, which allows one to further\ndistinguish different types of heterophily.\nRemark. Node homophily Hnode and edge homophily Hedge are\nstill popular measurement methods among recent works.\n2.5\nMainstream Heterophily Learning Task\nThe main objective learning task of heterophily GNNs is semi-\nsupervised node classification. In this task, we have a training\ngraph G = (X, A, Y), where X \u2208RN\u00d7d denotes N nodes with\nd-dimensional features, and A \u2208RN\u00d7N denotes the adjacency\nmatrix indicating the edge connection. Assuming each node v\nbelongs to one out of C classes, only a part of nodes are provided\nwith labels as yv \u2208YL = {1, \u00b7 \u00b7 \u00b7 , C} in the training node set.\nThe goal of the task is to predict the classes of nodes whose labels\nare not given.\nConcretely, given a heterophily GNN model parameterized\nby \u03b8, denoting as GNN\u03b8(\u00b7). For semi-supervised node class\nclassification task, the cross-entropy loss is minimized to learn\nthe optimal GNN parameters \u03b8 over all labeled nodes as:\nmin\n\u03b8 Lcross-entropy\n\u0010\n\u02c6Y, Y\n\u0011\n, where \u02c6Y = GNN\u03b8(X, A).\n(9)\nwhere \u02c6Y \u2208YL = {1, \u00b7 \u00b7 \u00b7 , C} denotes GNN predicted node\nlabels and Y is the ground-truth node labels.\n3\nGNNS WITH HETEROPHILY: FRAMEWORK AND\nTAXONOMY\nIn this section, we provide a unified framework of heterophilic\nGNNs, and further categorize it from the lens of the message\npassing mechanism.\n3.1\nFramework of heterophilic GNNs\nFollowing the general message passing principle for GNN model\ndesign, heterophilic GNNs focus on customizing the neighborhood\naggregation and feature update schemes that specifically model\nthe heterophily property. In contrast to homophilic GNNs, het-\nerophilic GNNs exhibit distinct characteristics in three key design\nprinciples:\nP1: Non-locality of Neighbor Sets: Incorporating information\nfrom non-local neighbors that may share the same class label as\nthe central node;\nP2: Class Distinguishability: Ensuring the aggregation pro-\ncess to effectively distinguish the class labels from both local and\nnon-local neighbors;\nP3: Depth Fusion of Multi-layer Information: Integrating\nhierarchical messages from different inter layers of GNNs for\ncapturing comprehensive heterophily property.\n3.2\nTaxonomy of GNNs with Heterophily\nAccording to the above three-fold design principles in heterophily\ninstructed neighbor aggregation and feature updating, heterophilic\nGNNs can be categorized into three groups, including:\n(1) Non-local neighbor extension methods \u2190P1.\n(2) GNN architecture refinement methods \u2190P2 & P3.\n(3) Hybrid methods \u2190P1, P2 & P3.\nMore fine-grained categorizations of these methods are briefly\ndiscussed below and shown in Fig. 2.\nNon-Local Neighbor Extension Methods attempt to improve\nnode representation by incorporating higher-order neighbor nodes\nthat share labels during the message-passing process. This kind\nof methods break the locality limitations of N(v) in Eq. (1)\nby reconstructing the non-local neighbor set from two perspec-\ntives: high-order neighbor mixing and potential neighbor discov-\nery. Specifically, these methods focus on discovering appropriate\nneighbors from multi-hops nodes and redefining neighbor sets as\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nGNNs with \nHeterophily\nNon-local Neighbor Extension \nHigh-order Neighbor Mixing\nPotential Neighbor Discovery\nStructure-based Distance\nFeature-based Distance\nHybrid Distance\nGNN Architecture Refinement\nIdentifiable Message Aggregation\nEdge-related Weight\nSpectral Perspective\nSpatial Perspective\nFeature-related Weight\nHybrid Weight\nInter-layer Combination\nHybrid Methods\nLocal-level Incorporation\nGlobal-level Incorporation\nFig. 2: Categorization of Heterophily GNNs.\nNp(v) = {u : dist(u, v) \u2264p} in the message aggregation\nstage in Eq. (1), where dist(u, v) denotes the distance method\nto measure the distance between nodes u and v. p is a threshold\nto limit the number of neighbors. In particular, Np(v) can be\ndegenerated into N(v) when the metric function dist(\u00b7) is the\nshortest distance and p = 1.\nGNN Architecture Refinement Methods boost the expressive\nability of GNNs by facilitating message passing that distinguishes\nbetween similar and dissimilar neighbors or by deeply fusing\nmulti-layer information. Specifically, focusing on the crafted ar-\nchitecture design of message aggregation and feature updates,\nthese methods are categorized into identifiable message aggrega-\ntion methods and inter-layer combination methods. Among these,\nidentifiable message aggregation methods have gained significant\nattention. Their objective is to allocate appropriate weights in the\nmessage aggregation process, aiming to strengthen the fusion of\nhomophilic information while mitigating the fusion of heterophilic\ninformation. With a focus on weight assignment schemes from\nvarious perspectives, existing methods can be further classified\ninto three types: edge-related weight, feature-related weight, and\nhybrid weight.\nHybrid Methods can be taken as the combination of non-local\nneighbor extension methods and GNNs architecture refinement\nmethods. Typically, hybrid methods first construct an appropriate\nneighbor set through non-local neighbor extension, and then\nrefine the GNN architectures from two perspectives: (1) local-\nlevel incorporation, that focuses on intra-layer heterophily-guided\nmessage passing, and (2) global-level incorporation, that enhances\neffective inter-layer heterophily information transfer throughout\nthe entire GNN architecture.\nDiscussion. Different heterophilic GNNs exhibit distinct proper-\nties. The fundamental concept of non-local neighbor extension\nmethods is quite straightforward and simple to implement, as it\ndirectly models and captures the similarity relationships between\nnodes to enhance the homophily of local neighborhoods; however,\nfor large-scale graphs, it can be memory-intensive to traverse the\nentire graph in order to model the similarity between each pair of\nnodes. GNN architecture refinement methods focus on mitigating\nthe impact of local heterophilic information during the message\npassing process. This primarily involves further refinement of mes-\nsage aggregation and feature updates for the heterophilic design.\nHowever, the challenge lies in how to customize the heterophilic\nmessage passing mechanism with only a limited number of ob-\nservable homophilic or heterophilic relationships. Hybrid methods\ncombine the advantages of both non-local neighbor extension and\nGNN architecture refinement. Nevertheless, a major challenge is\ndevising a strategy for seamlessly integrating these approaches\ninto the message-passing process.\n4\nHETEROPHILIC\nGNNS\nWITH\nNON-LOCAL\nNEIGHBOR EXTENSION\nUnder the uniform message passing framework of homophilic\nGNNs, the neighborhood is usually defined as the set of all\nneighbors one-hop away (e.g., GCN), which means only messages\nfrom proximal nodes in a graph are aggregated. However, such\na local neighborhood definition might not be appropriate for\nheterophilic graphs, where nodes belonging to the same class\nexhibit high structural similarity but can be farther away from\neach other. In light of these, current heterophilic GNNs attempt\nto extend the local neighbors to non-local ones primarily through\ntwo schemes: high-order neighbor mixing and potential neighbor\ndiscovery. As a result, the representation ability of heterophilic\nGNNs can be improved significantly by capturing the important\nfeatures from distant and informative nodes. The pipelines of two\nexample methods are given in Fig. 3, and a summary of the non-\nlocal neighbor extension works is illustrated in Table 2.\n4.1\nHigh-order Neighbor Mixing\nHigher-order neighbor mixing allows the ego node to receive\nlatent representations from their local one-hop neighbors and from\nfurther k-hop neighbors, so that the heterophilic GNNs can mix\nlatent information from neighbors at various distances. Formally,\nthe k-hop neighbor set is defined as\nNk(v) = {u : dist(u, v) = k},\n(10)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nHeterophily Graph\nClass1\nClass2\nClass3\nHigh-order Neighbor Mixing\n1-hop\n2-hop\n3-hop\nPotential Neighbor Discovery \nFeat./Stru./Hybrid  \nDistance\nLatent Space\nFig. 3: Schematic diagram of high-order neighbor mixing method and potential neighbor discovery method.\nwhere dist(u, v) measures the shortest distance between nodes u\nand v. In addition, how to mix the information from the different\nk-hop neighbor sets is an important research point of higher-order\nneighbor mixing methods.\nTypically, MixHop [36] is a representative method that aggre-\ngates messages from multi-hop neighbors. Apart from one-hop\nneighbors, MixHop also considers two-hop neighbors for message\npropagation. After that, the messages acquired from different hops\nare encoded by different linear transformations and then mixed by\nconcatenation. The l-th layer MixHop for each node v \u2208V can be\ndescribed as\nm(l)\nv,k = AGGREGATE(l)({h(l\u22121)\nu\n: u \u2208Nk(v)}),\nh(l)\nv,k = UPDATE(l)(h(l\u22121)\nv\n, m(l)\nv,k),\nh(l)\nv\n= \u2225k=2h(l)\nv,k,\n(11)\nwhere \u2225means column-wise combination. It can be noted that\ncross-level information fusion of MixHop is performed after the\nfeature aggregation and updating among the same hop neighbors.\nAnother perspective for cross-level information fusion is to\nmerge cross-hop neighbors before the feature aggregation stage,\nand then aggregate and update multi-hop neighbors\u2019 features\nsimultaneously. The most representative method is TDGNN [37],\nwhich uses a tree decomposition method to disentangle neighbor-\nhood information in different layers and focus on adjusting the\naggregation stage in Eq. (1) to promote information fusion among\ndifferent layers, modeled as\nm(l)\nv\n= AGGREGATE(l)({h(l\u22121)\nu\n: u \u2208\nK\n[\nk=1\nNk(v)}), (12)\nwhere K is the highest-hop setting the collection neighbor sets.\nRecently, Ordered GNN [38] proposes to order the messages\npassing into the node representation, with specific blocks of\nneurons targeted for message passing within specific hops. This is\nachieved by aligning the hierarchy of the rooted-tree of a central\nnode with the ordered neurons in its node representation. Based\non simple design in the spectral domain, EvenNet [39] discards\nmessages from odd-order neighbors inspired by balance theory,\nderiving a graph filter with only even-order terms, which can be\ngeneralized to graphs of different homophily.\nIn summary, the high-order neighbor mixing methods straight-\nforwardly include higher-order neighbors in local neighbor sets\nand devise an appropriate combination scheme to effectively\nintegrate multi-order neighborhood information. Its objective is\nto alleviate the impact of local heterophily by incorporating richer\nhomophilic information from higher-order neighborhoods.\n4.2\nPotential Neighbor Discovery\nCompared to high-order neighbor mixing methods directly uti-\nlizing the inherent structural information from graphs, potential\nneighbor discovery methods reconsider the definition of neighbors\nin heterophilic graphs and build innovative structural neighbors\nthrough the entire topology exploration with heterophily. Apart\nfrom the original neighbor set, these methods construct a new\npotential neighbor set that can be further formalized as\nN\u03c1(v) = {u : dist(v, u) < \u03c1},\n(13)\nwhere dist(u, v) is a metric function that measures the distance\nbetween nodes u and v in a specifically defined latent space and\n\u03c1 is a threshold to limit the number of neighbors. It is evident that\ndistance measurement plays a pivotal role in identifying suitable\npotential neighbors. Current methods can be categorized into three\nmain types based on the focal variables of distance measurement:\nstructure-based distance, feature-based distance, and hybrid dis-\ntance.\n4.2.1\nStructure-based Distance\nStructure-based distance methods typically search for potential\nneighbors that meet measurement criteria within the geometric\nrelationship latent space, which is defined by prior information\nabout the graph topology. Typically, Geom-GCN [34] maps the\ninput graph to a continuous latent space and defines the geomet-\nric relationships, i.e., split 2D Euclidean geometry locations, as\nthe criteria to discover potential neighbors. Apart from inherent\nneighbors in original input graphs, neighbors that conform to the\ndefined geometric relationships also participate in the message\naggregation of GCN. Specifically, based on the graph and latent\nspace, a structural neighborhood is built as ({N(v), N\u03c1(v)}, \u03c4)\nunderlying the relational operator \u03c4, where N(v) is the set of\nadjacent nodes of v in the graph, N\u03c1(v) is the set of potential\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nTABLE 2: Summary of non-local neighbor extension methods. \u2018Structure-based Distance\u2019, \u2018Feature-based Distance\u2019, and \u2018Hybrid Distanc-\netance\u2019 take the structure-based scheme, feature-based scheme, and hybrid scheme as the distance metric for potential neighbor discovery,\nrespectively.\nPublication Year and Venue\nMethod\nNeighbor Range\nDistance Measurement\nHigh-Order Neighbors Mixing\n[2019 ICML]\nMixHop [36]\n2-Hops\nShortest Distance\n[2021 CIKM]\nTDGNN [37]\nMulti-Hops\nShortest Distance\n[2022 NeurIPS]\nEvenNet [39]\nOdd-Hops\nShortest Distance\n[2023 ICLR]\nOrdered GNN [38]\nMulti-Hops\nShortest Distance\n[2024 ICLR]\nMPformer [43]\nMulti-Hops\nShortest Distance\nPotential Neighbors Discovery\n[2020 ICLR]\nGeom-GCN [34]\nMulti-Hops\nStructure-based Distance\n[2021 WSDM]\nSimP-GCN [41]\nMulti-Hops\nFeature-based Distance\n[2022 AAAI]\nHOG-GNN [44]\n2-Hops\nHybrid Distance\n[2022 ICML]\nGloGNN [45]\nMulti-Hops\nHybrid Distance\n[2022 NeurIPS]\nDiag-NSD [46]\nMulti-Hops\nFeature-based Distance\n[2023 TNNLS]\nHES-GSL [42]\nMulti-Hops\nFeature-based Distance\n[2023 WWW]\nSE-GSL [47]\nMulti-Hops\nHybrid Distance\n[2023 ICML]\nGOAL [21]\nMulti-Hops\nHybrid Distance\n[2023 ICASSP]\nGraphTU [48]\nMulti-Hops\nHybrid Distance\n[2023 CVPR]\nHopGNN [49]\nMulti-Hops\nFeature-based Distance\nHigh-Order Neighbors Mixing & Potential Neighbors Discovery\n[2021 NeurIPS]\nU-GCN [40]\n2-Hops\nFeature-based Distance\nneighbors from which the distance to v is less than a pre-given\nparameter \u03c1 in the latent space, described as\nN\u03c1(v) = {u : \u2225hu \u2212hv\u22252 < \u03c1},\n(14)\nwhere hu and hv are the representations for nodes u and v. \u2225\u00b7 \u22252\nis the Euclidean norm to measure relative positions between two\nnodes. \u03c1 is determined from zero until the average cardinality of\nNs(v) equals that of N(v), \u2200v \u2208V, that is, when the average\nneighborhood sizes in the graph and latent spaces are the same.\n4.2.2\nFeature-based Distance\nDifferent from the structure-based distance methods that use\nstructural information to determine potential neighbor nodes,\nfeature-based distance methods determine connection relation-\nships based on feature similarity.For instance, U-GCN [40] and\nSimP-GCN [41] choose top k similar node pairs in terms of\nfeature-level cosine similarity for each ego node to construct the\nneighbor set through the kNN algorithm. The potential neighbor\nset for this method can be built as\nNK(v) = {u : TopK(Cos(hu, hv), K)},\n(15)\nwhere Cos(\u00b7, \u00b7) is usually defined the cosine similarity function,\nK is the number of nearest neighbors set manually, TopK(\u00b7)\ndenotes a pooling operation for discovering potential neighbors\nwith K highest similarity. The recent work HES-GSL [42] still\ndetermines relationships of node pairs through cosine similarity,\nand further designs homophily-enhanced self-supervision to pro-\nvide more supervision for similarity learning.\n4.2.3\nHybrid Distance\nRecent works increasingly emphasize the comprehensive consid-\neration of structural and node feature information in potential\nneighbor discovery. These methods focus more on the potential\nmessage passing among global homophily neighbors in the graph.\nTypically, HOG-GCN [44] and GloGNN [45] design similarity\ncalculation modules to capture the correlations between global\nnodes through both topological and attribute information. Specif-\nically, HOG-GCN constructs a homophily degree matrix with\nthe label propagation technique to explore the extent to which\na pair of nodes belong to the same class in the entire heterophilic\ngraph from the perspective of topology space. By involving the\nclass-aware information during the propagation process, intra-\nclass nodes with higher heterophily (i.e., lower homophily degree)\nwould contribute more to the neighbor aggregation than underly-\ning inter-class nodes. More simplified than HOG-GCN, GloGNN\ndirectly measures potential correlations between global nodes in\nterms of both feature attribute similarity and topology similarity\nby further regularizing node representations with nodes\u2019 multi-\nhop reachabilities. In general, the potential neighbors set can be\ndefined as\nN(v) = NT (v) \u2229NS(v),\n(16)\nwhere NT (v) = {u : \u02dchu\u02dchT\nv\n\u0338= 0} and NS(v) = {u :\n\u02c6hu\u02c6hT\nv < \u03c1} are potential neighbor sets defined by measuring node\ncorrelations in terms of topology similarity and feature similarity,\n\u2229is the intersection operation, and \u02dchu and \u02c6hu are representations\nof node u through topology structure information (i.e., A) and\nnode attribute information (i.e., X), respectively.\nFurthermore, SE-GSL [47] offers an effective measure of\nthe information embedded in an arbitrary graph and structural\ndiversity (where NT (v) is characterized by both the kNN struc-\nture and the original topology structure) and presents a novel\nsampling-based mechanism for restoring the graph structure via\nnode structural entropy distribution. It increases the connectivity\namong nodes with larger uncertainty in lower-level communities.\nGraphTU [48] offers a probabilistic approach to exploring poten-\ntial neighbors. A key observation in this research is that the sta-\ntistical variances based features and topology information within\nlocal neighborhoods can be effectively harnessed to extend the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE 3: Summary of GNN architecture refinement methods. \u2018Feature-Related\u2019, \u2018Edge-Related\u2019 and \u2018Hybrid\u2019 respectively represent weight\nassignment schemes that work on node feature, edges, and both of them for adaptive message aggregation. \u2018\u2713\u2019 and \u2018\u2717\u2019 indicate whether to\ninclude the according schemes, respectively.\nPublication Year and Venue\nMethod\nWeight Assignment\nEgo-Neighbor Separation\nCombination Function\nIdentifiable Message Aggregation\n[2021 AAAI]\nFAGCN [25]\nEdge-Related\n\u2717\n\u2717\n[2021 NeurIPS]\nDMP [29]\nEdge-Related\n\u2717\n\u2717\n[2021 AAAI]\nCPGNN [23]\nFeature-Related\n\u2717\n\u2717\n[2021 ICDM]\nCGCN [28]\nFeature-Related\n\u2713\n\u2717\n[2022 ICLR]\nACM [50]\nEdge-Related\n\u2717\n\u2717\n[2022 WWW]\nGBK-GNN [51]\nFeature-Related\n\u2717\n\u2717\n[2022 WWW]\nF2GNN [52]\nEdge-Related\n\u2717\n\u2717\n[2022 WWW]\nMWGNN [53]\nHybrid\n\u2717\n\u2717\n[2022 ICASSP]\nMMP [54]\nFeature-Related\n\u2713\n\u2717\n[2022 ICML]\nGIND [55]\nFeature-Related\n\u2713\n\u2717\n[2022 NeurIPS]\nASGC [56]\nEdge-Related\n\u2717\n\u2717\n[2022 LoG]\nGESN [57]\n\u2717\n\u2713\n\u2717\n[2022 TNNLS]\nNCGNN [58]\nFeature-Related\n\u2717\n\u2717\n[2022 TNNLS]\nGDAMN [59]\nEdge-Related\n\u2717\n\u2717\n[2023 TNNLS]\nCGP [60]\nEdge-Related\n\u2717\n\u2717\n[2023 TNNLS]\nRFA-GNN [61]\nEdge-Related\n\u2717\n\u2717\n[2023 TKDE]\nAutoGCN [62]\nEdge-Related\n\u2717\n\u2717\n[2023 ICML]\nGOAT [63]\nEdge-Related\n\u2717\n\u2717\n[2023 WWW]\nMid-GCN [64]\nEdge-Related\n\u2717\n\u2717\n[2023 NeurIPS]\nTEDGCN [65]\nEdge-Related\n\u2717\n\u2717\n[2023 NeurIPS]\nLRGNN [66]\nEdge-Related\n\u2717\n\u2717\nInter-Layer Combination\n[2018 ICML]\nJK-Net [67]\n\u2717\n\u2717\nSkip Connections\n[2020 ICLR]\nGPR-GNN [30]\n\u2717\n\u2717\nAdaptive Connections\n[2022 NeurIPS]\nPowerEmbed [68]\n\u2717\n\u2717\nSkip Connections\nIdentifiable Message Aggregation & Inter-Layer Combination\n[2022 LoG]\nLW-GCN [69]\nHybrid\n\u2713\nAdaptive Connections\n[2023 TNNLS]\nCAGNNs [70]\n\u2717\n\u2713\nAdaptive Connections\ntraining distribution, creating novel potential neighbor sets through\na non-parametric method. This approach is particularly valuable\nfor addressing heterophily within the entire graph, especially in\nthe case of minor class nodes. To reveal attribute relationships\namong nodes in the entire graph, GOAL [21] augments the\nexisting graph by constructing a fully connected graph through\na graph completion process. This augmentation considers two\nmodes: homophilic connections and heterophilic connections. An\nimportant aspect involves developing a method to distinguish\nbetween connections that reflect a tendency for homophily and\nthose that indicate heterophily. This distinction is made based\non the Connected Structure Difference (CSD) computed between\nconnected node pairs and randomly selected node pairs, ultimately\nfacilitating the optimization of the complemented graph.\nIn summary, potential neighbor discovery methods focus on\nidentifying homophilic neighbors among high-order neighbors and\nincorporating them into local neighbor sets to enhance homophilic\nmessage-passing. By measuring the similarity of node features\nwith different distance calculation schemes, potential neighbor\ndiscovery methods are able to effectively identify potential ho-\nmophilic neighbors that share the same class labels.\n5\nHETEROPHILIC GNN ARCHITECTURE REFINE-\nMENT\nGeneral GNN architectures in Eq. (1) contain two essential com-\nponents: the aggregation function AGGREGATE(\u00b7) to integrate\ninformation from the discovered neighbors, and the update func-\ntion UPDATE(\u00b7) to combine the learned neighbor messages with\nthe initial ego representation. Given the original local neighbors\nand the extended non-local neighbors on heterophilic graphs,\nexisting GNN architecture refinement methods contribute to fully\nexploiting the neighbor information from the following aspects by\naccordingly revising AGGREGATE(\u00b7) and UPDATE(\u00b7): (1)\nIdentifiable message aggregation discriminates and enhances the\nmessages of similar neighbors from dissimilar ones; (2) Inter-layer\ncombination emphasizes the effect of different propagation ranges\n(i.e., the number of GNN layers) on node representation learning.\nAll these two aspects come to the same destination: boosting the\nexpressive ability of GNNs for heterophilic graphs by encouraging\ndistinguishable and discriminative node representations.\n5.1\nIdentifiable Message Aggregation\nGiven the neighbors to be aggregated, the key of integrating\nbeneficial messages on heterophilic graphs is distinguishing the\ninformation of similar neighbors (likely in the same class) from\nthat of dissimilar neighbors (likely in different classes). To make\nnode representations on heterophilic graphs more discriminative,\nidentifiable message aggregation methods alter the aggregation\noperation AGGREGATE(\u00b7) by imposing adaptive edge-aware\nweights a(l)\nuv for node pair (u, v) at the l-th layer as:\nm(l)\nv\n= AGGREGATE(l)({a(l)\nuv h(l\u22121)\nu\n: u \u2208N(v)}).\n(17)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nHeterophily Graph\nClass1\nClass2\nClass3\n\ud835\udc21\ud835\udfcf\n\ud835\udc21\ud835\udfd0\n\ud835\udc21\ud835\udfd1\n\ud835\udc21\ud835\udfd2\n\ud835\udc21\ud835\udfd3\n\ud835\udefc&'\n\ud835\udefc&(\n\ud835\udefc&)\n\ud835\udefc&*\n\ud835\udefc)*\nlow\nhigh\n\ud835\udefc&'\n+\n\ud835\udefc&'\n,\nEdge-Related Weight\n\ud835\udc21\ud835\udfcf\n-\n\ud835\udc21\ud835\udfcf\n\ud835\udc21\ud835\udfd0\n\ud835\udc21\ud835\udfd1\n\ud835\udc21\ud835\udfd2\n\ud835\udc21\ud835\udfd3\nFeat-Related Weight\n\ud835\udc21\ud835\udfcf\n-\n\ud835\udefd&\n\ud835\udefd'\n\u2026\n\ud835\udefd.\nFig. 4: Illustration of identifiable message passing with edge-related weight and feature-related weight assignment schemes in GNN architecture\nrefinement methods.\nIn this way, different methods develop various weight assignment\nschemes for a(l)\nuv to model the importance of similar and dissimilar\nneighbors during aggregation. In the following, we provide the de-\ntails of weight assignment schemes adopted by existing methods,\nwhich respectively work on node feature and edge related weights.\nFig. 4 provides the pipelines of two weight assignment schemes.\n5.1.1\nEdge-Related Weight\nIn general, edge-related methods simultaneously focus on spectral\ndomain and the spatial domain. To be concrete, spectral GNNs\nleverage the theory of graph signal processing to design graph\nfilters, and spatial GNNs focus on the graph structural topology to\ndevelop aggregation strategies.\nSpectral Perspective. In contrast to Laplacian smoothing [71] and\nlow-pass filtering [72] to approximate graph Fourier transforma-\ntion on homophilic graphs, spectral GNNs on heterophilic graphs\ninvolve both low-pass and high-pass filters to adaptively extract\nlow-frequency and high-frequency graph signals. The essential\nintuition behind this lies in that low-pass filters mainly retain the\ncommonality of node features, while high-pass filters capture the\ndifference between nodes.\nTypically, FAGCN [25] adopts a self-gating attention mecha-\nnism to learn the proportion of low-frequency and high-frequency\nsignals by splitting the a(l)\nuv into two components, i.e., a(l,LP )\nuv\nand a(l,HP )\nuv\n, corresponding to low-pass and high-pass filters,\nrespectively. Through adaptive frequency signal learning, FAGCN\ncould achieve expressive performance on different types of graphs\nwith homophily and heterophily. Formally, adaptive edge-aware\nweights a(l)\nuv are defined in FAGCN as\na(l)\nuv = a(l,LP )\nuv\n\u2212a(l,HP )\nuv\n\u221adudv\n,\n(18)\nwhere du and dv respectively denote the degree of node u and v,\na(l,LP )\nuv\n\u2212a(l,HP )\nuv\nis the coefficient learned through a shared self-\ngating mechanism tanh (qT[hu; hv]), from which [; ] denotes the\nconcatenation operation, g can be seen as a shared convolutional\nkernel, tanh (\u00b7) is the hyperbolic tangent function, which can\nnaturally limits the value of a(l,LP )\nuv\n\u2212a(l,HP )\nuv\nin [\u22121, 1]. In a sim-\nilar fashion, RFA-GNN [61] captures edge-aware weights using\na relation-based frequency adaptive mechanism. This mechanism\ntakes into account higher-order contextual information compared\nto FAGCN. It precisely defines edge-aware weights for a specific\nrelation. Specifically, the edge-aware weight of (l + 1)-order\ninformation between nodes u and v for relation k is described\nas a(l)\nuvk = tanh (qT\nlk[h(l)\nuk; h(l)\nvk]), where qlk is the attention\ncoefficient for relation k in the l-th iteration.\nApart from low-pass and high-pass filters, ACM [50] further\ninvolve the identity filter, which is the linear combination of low-\npass and high-pass filters, i.e.,\na(l)\nuv = Softmax\n\u0010\n([a(l,LP )\nuv\n, a(l,IP )\nuv\n, a(l,HP )\nuv\n]/\u03c4)W(l)\nuv\n\u0011\n, (19)\nwhere \u03c4 indicates a temperature parameter and W(l)\nuv \u2208R1\u00d73\nis used to learn which filters is important or not for each\nnode. a(l,LP )\nuv\n, a(l,IP )\nuv\nand a(l,HP )\nuv\nrespectively denotes low-pass,\nidentity and high-pass edge-aware weights, which learn from 3\nchannels features. AutoGCN [62] captures the full spectrum of\ngraph signals and automatically update the bandwidth of graph\nconvolutional filters. In addition, Mid-GCN [64] contains a mid-\npass filter determined by both low-pass and high-pass filters.\nThe robustness of signals passing through this mid-pass filter is\ntheoretically guaranteed by their analyses.\nIn this way, ACM, AutoGCN and Mid-GCN could adaptively\nexploit beneficial neighbor information from different filter chan-\nnels for each node; Meanwhile, their identity or mid-pass filters\ncould guarantee less information loss of the input signal.\nSpatial Perspective. Heterophilic GNNs in the spatial domain\nrequire the diverse topology-based aggregation of neighbors from\nthe same or different classes guided by the heterophily. There-\nfore, the edge-aware weights of neighbors should be assigned\naccording to the spatial graph topology and node labels. Taking\nnode attributes as weak labels, DMP method [29] considers node\nattribute heterophily for diverse message passing and specifies\nevery attribute propagation weight on each edge. Furthermore,\ninstead of the scalar weight a(l)\nuv that aggregates all the node\nattributes with the same weight at the node level, DMP extends\nthe weight to a vector a(l)\nuv through operating in the attribute\ndimension, and this vector can be calculated by either relaxing\nGAT [9] weights to real values or allowing an element-wise\naverage of neighbor weights.\nMoreover, certain methods learn edge-aware weights based on\nnode labels. For instance, Chen et al. [59] introduce the concept\nof graph decoupling attention Markov networks (GDAMNs).\nThis approach incorporates variational inference to model edge\nuncertainty and employs both hard and soft attention mechanisms\non node labels to improve the learning of edge-aware weights,\nrepresented as\na(l)\nuv =\nahard\nuv exp \u03b4(l\u22121)\nuv\nPN\nv=1 ahard\nuv exp \u03b4(l\u22121)\nuv\n,\n(20)\nwhere ahard\nuv\n= Auv \u00b7 \u02dcyTwuv\u02dcy is the hard attention with label\nsimilarity and \u03b4(l\u22121)\nuv\n= \u2212Cos(h(l\u22121)\nu\n, h(l\u22121)\nv\n) denotes the soft\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nattention with feature dissimilarity. In general, the hard attention\nis learned on labels for a refined graph structure with fewer inter-\nclass edges so that the aggregation\u2019s negative disturbance can be\nreduced. The soft attention aims to learn the aggregation weights\nbased on features over the refined graph structure to enhance\ninformation gains during message passing.\nRecent research has shown a growing interest in the efficiency\nof computing edge-aware weights for large-scale graphs. CGP [60]\nintroduces a graph pruning paradigm during training, enabling\nthe discovery of high-performing sparse GNNs within a single\ntraining process. Additionally, a global graph transformer model\nhas been proposed for large-scale heterophilic node classification\ntasks, known as GOAT [63]. GOAT samples potential neighbors\nfor each node from its k-hop neighbors. To distinguish between\nneighbors based on topology information during the propagation\nprocess, GOAT further incorporates a global attention module\ndesigned to learn attention scores between potential neighbors and\nsource nodes.\n5.1.2\nFeature-Related Weight\nApart from the adaptive edge-aware weight learning, there is\nanother solution working on the neighbor representation learning\nby revising h(l\u22121)\nu\nin AGGREGATE(l)({a(l)\nuv h(l\u22121)\nu\n: u \u2208\nN(v)}). Conventionally, the above-mentioned methods mainly\nutilize the contextual node representations of neighbors; In con-\ntrast, this solution transforms contextual node embeddings h(l\u22121)\nu\nto other node-level properties that reflect the heterophily. In this\nway, heterophilic GNNs learn node-level attention \u03b2(l)\nv , which\nis shared with each u \u2208N(v), to capture the beneficial in-\nformation of heterophily for distinguishable node representation\nlearning. As a result, the aggregation progress can be redefined\nas AGGREGATE(l)({\u03b2(l)\nv h(l\u22121)\nu\n: u \u2208N(v)}). Existing\nmethods mainly learn \u03b2(l)\nv\nthrough two schemes based on ho-\nmophily/heterophily prior knowledge and homophily attributes\nattention.\nPrior-Based. The prior-based method considers the incorporation\nof homophily/heterophily prior knowledge in the process of fea-\nture transformation, primarily estimating the assignment weight\n\u03b2(l)\nv\nthrough class information. Typically, instead of propagating\noriginal node feature representations, CPGNN [23] propagates a\nprior belief estimation based on a compatibility matrix, so that\nit can capture both heterophily and homophily by modeling the\nlikelihood of connections between nodes in different classes. The\nfeature-aware weight of embeddings h(l\u22121)\nv\nwith prior belief is\ndefined as\n\u03b2(l)\nv\n= Sinkhorn-Knopp(yv, Av, Bv),\n(21)\nwhere Sinkhorn-Knopp denotes the Sinkhorn-Knopp algorithm\nproposed in [73], which is to ensure that \u03b2(l)\nv\nis doubly stochastic.\nBv denotes an enhanced belief matrix for node v, depending on a\ntraining mask matrix and a prior belief. Going beyond the uniform\nGCN aggregation, NLGNN [74] and GPNN [75] consider the\nsequential aggregation where the neighbors are ranked based on\nthe class similarity (i.e., a homophily/heterophily prior belief) in\norder. A regular 1D-convolution layer is applied to extract the\naffinities between the sequential nodes whether the nodes are\nclose or distant in heterophilic graphs. GIND [55] extends the\nlinear isotropic diffusion to a more expressive nonlinear diffusion\nmechanism, which learns nonlinear flux features between node\npairs before aggregation. This design of the nonlinear diffusion\nensures that more information can be aggregated from similar\nneighbors and less from dissimilar neighbors by learning \u03b2(l)\nv ,\nmaking node features less likely to over-smooth.\nAttention-Based. The majority of research focuses on developing\ndiverse attention mechanisms for learning \u03b2(l)\nv . A straightforward\napproach involves learning attribute similarity and utilizing this\nsimilarity to determine attention on node features. CGCN [28] uses\nthe cosine similarity to send signed neighbor features under certain\nconstraints of relative node degrees. In this way, the messages are\nallowed to be optionally multiplied by a negative sign or a positive\nsign, i.e., for node v, its message is described as\nh(l+1)\nv\n= \u03c3\n\u0010\n\u03b2(l)\n0 \u02dch(l)\nv + \u03b2(l)\n1 (s(l)\nv,pos \u2299A(l)\nv )\u02dch(l)\nv\n+\u03b2(l)\n2 (s(l)\nv,neg \u2299A(l)\nv )\u02dch(l)\nv\n\u0011\n,\n(22)\nwhere \u03b2(l)\n0 , \u03b2(l)\n1 , and \u03b2(l)\n2\nare the l-th layer learned scalars.\ns(l)\nu,pos and s(l)\nu,neg respectively indicate positive matrix and neg-\native matrix, which are split from the matrix with sign in-\nformation. Intuitively, signed messages consist of the negated\nmessages sent by neighbors of the opposing classes, and the\npositive messages sent by neighbors of the same class. Sim-\nilarly, Du et al. [51] propose a method for defining positive\nand negative correlation weights when modeling the similar-\nity and dissimilarity between node features. They introduce a\nnovel GNN model called GBK-GNN, which is based on a bi-\nkernel feature transformation and a selection gate. The two\nkernels capture homophily and heterophily information, and the\ngate is used to determine which kernel should be applied to\na specific pair of nodes. Formally, the gate signal is described\nas \u03b2(l)\nv\n=\nP\nu\u2208N (v)\nSigmoid\n\u0010\nMLP(h(l\u22121)\nv\n, h(l\u22121)\nu\n; W(l\u22121)\nv\n)\n\u0011\n,\nwhere MLP(\u00b7) is a multilayer perceptron. MMP [54] decouples\nthe messages into two parts, i.e., memory for propagation and self-\nembedding for discrimination. The memory for propagation aims\nto endow each node with a memory cell and sends messages from\nthe memory cell instead of hidden self-embedding. After propa-\ngation, each node can leverage a learnable control mechanism to\nadaptively update its self-embedding and memory cell according\nto their recent states. CAGNNs [70] investigates the feature aggre-\ngation of inter-class edges from an entire neighbor identifiable\nperspective by a new metric based on von Neumann entropy.\nUsing an importance score \u03b2(l)\nv\nobtained by a mixer combines dis-\ncriminant feature and neighbor information, which are decoupled\nfrom embedding h(l\u22121)\nv\n. To overcome the limitations of traditional\nmessaging methods, NCGNN [58] represents nodes as collections\nof node-level capsules. Each capsule is responsible for extracting\ndistinct features from its associated node. For each node-level\ncapsule, a novel dynamic routing procedure is implemented to\nintelligently select the most suitable capsules for aggregation\nfrom a subgraph identified by the designed graph filter. NCGNN\nexclusively aggregates advantageous capsules while constraining\nirrelevant messages to prevent excessive feature mixing among\ninteracting nodes. As a result, this approach mitigates the problem\nof oversmoothing and enables the learning of effective node\nrepresentations in graphs characterized by either homophily or\nheterophily.\n5.1.3\nHybrid Weight\nHybrid methods aim to calculate edge weights guided by node\nfeatures and edge information simultaneously. A typical method\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nHeterophily Graph\nClass1\nClass2\nClass3\n\ud835\udc07(\")\n\ud835\udc17\n\ud835\udc68\nnode feature\ngraph structure\n\ud835\udc07($) \u2026\n\ud835\udc07(%)\n[ \ud835\udc07(\"), \ud835\udc07($),\u2026, \ud835\udc07(%)]\n(2) JK-Net\n\ud835\udf36\n\ud835\udc07(%&\") = \ud835\udefc\ud835\udc17+ (\ud835\udfcf\u2212\ud835\udefc)\ud835\udc07(%)\n(3) GCNII\n\ud835\udf38(\ud835\udfcf)\n\ud835\udf38(\ud835\udfd0)\n\ud835\udf38(\ud835\udc8e)\n\ud835\udf38(\ud835\udfcf)\ud835\udc07(\") + \ud835\udf38(\ud835\udfd0)\ud835\udc07($) +\u2026 + \ud835\udf38(\ud835\udc8e) \ud835\udc07(%)\n(1) GPR-GNN\nFig. 5: Illustration of typical inter-layer combination methods: (1) GPR-GNN [30]; (2) JK-Net [67]; and (3) GCNII [76].\nis MWGNN [53], which models the node local distribution from\nnode feature, topological structure, and positional identity aspects\nwith the meta-weight. Then, based on the meta-weight, an adaptive\ngraph convolution is derived to conduct node-specific weighted\naggregation for boosted node representation learning.\nRemark. On heterophilic graphs, an ego node is likely to be\ndissimilar with its neighbors in terms of class labels. Hence,\nencoding ego-node representations separately from the aggre-\ngated representations of neighbor nodes would benefit distin-\nguishable node embedding learning. In detail, ego-neighbor sep-\naration methods detach self-loop connections of the ego nodes\nin AGGREGATE(\u00b7). Meanwhile, they alter the UPDATE(\u00b7)\nas the non-mixing operations, e.g., concatenation, instead of the\nmixing operation, e.g., \u201caverage\u201d in vanilla GCN. H2GCN [24]\nfirst proposes to exclude the self-loop connection and points out\nthat the non-mixing operations in the update function ensure\nthat expressive node representations would survive over multiple\nrounds of propagation without becoming prohibitively similar.\nBesides, WRGNN [77] imposes different mapping functions on\nthe ego-node embedding and its neighbor aggregated messages,\nwhile GGCN [28] simplifies the mapping functions to learnable\nscalar parameters to separately learn ego-neighbor representations.\nMoreover, ACM [50] adopts the identity filter to separate the ego\nembedding and then conducts the channel-level combination with\nits neighbor information in the update function.\n5.2\nInter-Layer Combination\nDifferent from identifiable message aggregation methods focus-\ning on the fine-grained intra-layer design of GNN architectures\nwith heterophily, inter-layer combination methods consider layer-\nwise operations to boost the representation power of heterophilic\nGNNs. The intuition behind this strategy is that in the shallow\nlayers of GNNs, they collect local information, e.g., one-hop\nneighbor locality in two-layer vanilla GCN, when the layers go\ndeeper, GNNs gradually capture global information implicitly via\nmultiple rounds of neighbor propagation. Due to the heterophily\ncharacteristic, neighbors with similar information, i.e., class labels,\nmight locate in both local geometry and long-term global topology.\nHence, combining intermediate representations from each layer\ncontributes to leveraging different neighbor ranges with the con-\nsideration of both local and global structural properties, resulting\nin powerful heterophilic GNNs. Fig. 5 shows the formulation and\nillustrations of the closely related methods.\nThe prior idea first comes from JK-Net [67] which flexibly\ncaptures better structure-aware representation with different neigh-\nborhood ranges. The combination of features among different\nlayers is described as\n\u02c6hv = LA(h(1), h(2), . . . , h(L)),\n(23)\nwhere LA(\u00b7) denotes the layer aggregation, such as column-wise\ncombination, max pooling, attention with LSTM, etc.\nCompared with the methods using all previous intermediate\nrepresentations, GCNII [76] only integrates the first layer\u2019s node\nembedding at each layer with the initial residual connection,\ndefined as\nh(l)\nv =\u03b1h(0)\nv\n+AGGREGATE(l)({(1\u2212\u03b1)h(l\u22121)\nv\n:u\u2208N(v)}),\n(24)\nwhere \u03b1 is a hyperparameter that maintains a balance between the\nnode embedding of the first layer and the representation of the\ncurrent layer.\nInstead of using the simple concatenation operation, GPR-\nGNN [30] further assigns learnable weights to combine the rep-\nresentations of each layer adaptively via the Generalized PageR-\nank (GPR) technique. PowerEmbed [68] employs an inception\nnetwork to learn the rich representations that interpolate from\nlocal message-passing features to global spectral information.\nHence, inter-layer combination methods are able to conduct topo-\nlogical feature exploration and benefit from informative multi-\nround propagation, making node features of heterophilic graphs\ndistinguishable.\n6\nHYBRID METHODS\nRecently, researchers have recognized that simultaneously expand-\ning the non-local neighbor set and refining heterophily-guided\nGNN architectures, can significantly facilitate heterophilic graph\nrepresentation learning. Generally, existing hybrid methods can\nbe categorized into two groups: (1) local-level incorporation,\nwhich designs intra-layer heterophilic GNN message passing with\nthe neighbor extension; and (2) global-level incorporation, which\nenhances effective inter-layer information transfer of heterophilic\nGNNs with the neighbor extension. The diagram of two types of\nhybrid methods is presented in Fig. 6.\nThe majority of methods fall into the category of local-\nlevel incorporation, typically following a two-step process. First,\nthey establish a graph topology that emphasizes local homophily,\nand then they design a message aggregation method based on\nthis homophilic topology. For instance, WRGNN [77] employs\nthe degree sequence of neighbor nodes as a metric for mea-\nsuring structural similarity between ego nodes. This is used to\nreconstruct a multi-relational graph that captures homophily in\nrelational edges, followed by relational aggregation with explicit\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\nHeterophily Graph\nClass1\nClass2\nClass3\nLocal-level \nGlobal-level \nstepI: Local homophily \nenhancement \nHN or PN \nstepII:  Identifiable message \naggregation \n\u00d7\n\u00d7\n\u00d7\n\u00d7\nHN or PN \n\u06f6(\u0b35)\n\u2026\n\u06f6(\u0be0)\n\u0706\n\u086d\nGraph topology with \nlocal homophily\nInter-layer combination\n\u06f6(\u0be0\u0b3e\u0b35)\nFig. 6: A diagram of two types of hybrid methods: Local-level incorporation and global-level incorporation.\nTABLE 4: Summary of hybrid extension methods. \u2018HN\u2019 and \u2018PN\u2019 mean \u2018High-Order Neighbors Mixing\u2019 and \u2018Potential Neighbors Discovery\u2019,\nrespectively. \u2018IA\u2019 and \u2018IC\u2019 mean \u2018Identifiable Message Aggregation\u2019 and \u2018Inter-Layer Combination\u2019, respectively. \u2018Local-Level\u2019 and \u2018Global-\nLevel\u2019 indicate whether the GNN architecture refinement method, incorporating neighbor extension technology, primarily focuses on\nheterophilic design within the message-passing layer or enhances effective inter-layer information transfer throughout the entire network\narchitecture, respectively.\nPublication Year and Venue\nMethod\nCategory\nHybrid Mode\n[2020 NeurIPS]\nH2GCN [24]\nHN & IC & IA\nGlobal-Level\n[2021 KDD]\nWRGNN [77]\nPN & IA\nLocal-Level\n[2022 IJCAI]\nRAW-GNN [78]\nHN & IA\nLocal-Level\n[2022 TPAMI]\nNLGNN [74]\nPN & IA\nGlobal-Level\n[2022 AAAI]\nBM-GCN [79]\nPN & IA\nLocal-Level\n[2022 AAAI]\nDeformable GCN [80]\nPN & IA\nLocal-Level\n[2022 AAAI]\nGPNN [75]\nPN & IC\nLocal-Level\nlink weights. Additionally, BM-GCN [79] constructs a novel\nnetwork topology using a block similarity matrix. This approach\nallows it to explore block-guided neighbors and perform classified\naggregation with distinct aggregation rules for homophilic and\nheterophilic nodes. RAW-GNN [78] employs breadth-first random\nwalk searches to capture homophily information and depth-first\nsearches to gather heterophily information. Instead of traditional\nneighborhoods, it utilizes path-based neighborhoods and intro-\nduces a new path-based aggregator based on Recurrent Neural\nNetworks. GPNN [75] leverages a pointer network to rank the\npotential neighbor nodes according to the attention scores or\nthe relevant relationships to the ego node. In this way, potential\nneighbors that are most similar to the ego node in heterophilic\ngraphs can be discovered and selected. Deformable GCN, as\npresented in the recent work by Park et al. [80], dynamically\nconducts convolution in multiple latent spaces, enabling it to\ncapture both short and long-range dependencies between nodes.\nIn this approach, the model also learns the positional embeddings\n(coordinates) of nodes to infer the relationships between nodes in\nan end-to-end manner. Depending on the position of a node, the\nconvolution kernels are deformed using deformation vectors and\nmake distinct transformations being applied to neighbors.\nThe global-level incorporation method, which is intended to\nestablish effective homophilic guidance within the graph network\nframework. A typical example of such a method is H2GNN\n[24], which incorporates a set of crucial design elements. These\ninclude the separation of ego- and neighbor-embedding separation,\nhigher-order neighborhoods, and incorporation of intermediate\nrepresentations into a graph neural network. These design choices\ncollectively enhance the model\u2019s capacity to learn from the graph\nstructure, especially when dealing with heterophilic relationships.\nFurthermore, NLGNN [74] employs the attention mechanism to\nguide the sorting of non-local neighbors based on their impor-\ntance. The entire architecture achieves efficient node classification\non a heterogeneous graph using only two steps: straightforward\nattention-guided sorting and non-local aggregation.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\n7\nDISCUSSION\n7.1\nHeterophily GNNs on Diverse Graph Types\nWhile most current research primarily addresses the challenge of\nheterophilic problems in single relational static graphs, numerous\nreal-world applications are related to diverse and complex graph\ntypes. For instance, in a traffic controlling system, the traffic flows\nare typically presented with dynamic graphs that change along\nwith temporal information; Besides, graph data involving user-\nitem co-purchasing relationships [81], paper co-citation relation-\nships [82], film co-occurrence relationships [83], etc., is usually\ndetermined by hypergraphs.\nRecent studies [84], [85] have started to acknowledge this gap\nand introduced concepts like the heterophilic dynamic graph and\nthe heterophilic hypergraph. These advancements prompt a closer\nexamination of the definition and design of heterophily solutions\ntailored to specific graph types. However, this expansion also\nbrings forth a set of unique challenges for heterophilic GNNs to\naddress.\n7.1.1\nHeterophily GNNs on Dynamic Graphs\nAs a typical dynamic graph instantiation, spatial-temporal graph\ndata is often inherently heterophilic. Zhou et al. [84] recently\naimed to validate this assertion by exploring spatial-temporal\ngraphs in various real-world scenarios, such as traffic con-\ntrol, climate early-warning, and social networks. As depicted in\nFig. 7(a), observations in these scenarios tend to change over\ntime, leading to dynamic and time-varying characteristics in the\ncorrelation between nodes. The challenge posed by these dynamic\nand time-varying characteristics is termed \u201ctopology-task discor-\ndance\u201d in [84]. This challenge arises when employing homophilic\ngraph neural networks for node-level regression tasks on spatial-\ntemporal graphs. It refers to the utilization of a pre-defined\nfixed topology for message passing on spatial-temporal graphs\nwith diverse node-wise relations, resulting in the aggregation of\nneighbors that deviate from the intended target.\nTo formally measure spatial-temporal heterophily, two types\nof homophily measurements are introduced: \u201cintra-graph spa-\ntial homophily\u201d, capturing node correlations within the same\ngraph frames, and \u201cinter-graph transition homophily\u201d which\nextracts temporal evolution between adjacent temporal frames.\nThe study further investigates the average homophily ratios for\nfour real-world dynamic graphs: Metr-LA [86], PeMS-Bay [86],\nKnowAir [87], and Temperature [87]. The homophily ratios within\nintra-graph frames and across temporally adjacent frames are\nobserved to be low. This suggests that physically connected nodes\nmay not necessarily share similar observations or exhibit the same\ndirectional variations.\nFurthermore, adapting existing homophily theories to address\nthe topology-task discordance based on the spatial-temporal char-\nacteristics of node-wise relationships remains a challenging en-\ndeavor. The primary hurdles can be summarized as follows:\n\u2022\nDetermining which pairs of nodes belong to homophily\ncomponents in the absence of categorical labels.\n\u2022\nIncorporating target information to reconstruct node-\nwise correlations, thereby enabling improved and target-\noriented aggregations.\n\u2022\nLeveraging dynamic local neighborhood environments for\npersonalized high-order propagation.\nThese three aspects are expected to be key challenges in the\nstudy of heterophilic GNNs on dynamic graphs. This research\n!!\n!\"\n!#\n!$\n(a) Spatial-temporal graph\n!!\n!\"\n!#\n!$\n(b) Hypergraph\nFig. 7: Examples of (a) heterophilic dynamic graph and (b) het-\nerophilic hypergraph.\ndirection will continue to demand the dedicated efforts of a\nsignificant number of researchers.\n7.1.2\nHeterophily GNNs on Hypergraphs\nFig. 7(b) illustrates a heterophilic hypergraph using a social\nnetwork as an example. Each node represents a social user,\nand the hyperedge connects all users in the same community.\nIt is common for users within the same community to have\ndifferent identity backgrounds but be connected by attributes such\nas interests and hobbies, resembling the concept of heterophily\nin graphs. Recently, heterophily has been demonstrated in [88]\nto be a more prevalent phenomenon in hypergraphs compared\nto graphs. This is due to the challenge of expecting all nodes\nwithin a large hyperedge to share a common label. Consequently,\nrecent research on hypergraph neural network methods has shifted\nits focus towards highlighting their exceptional performance on\nheterophilic hypergraphs.\nInspired by hypergraph diffusion algorithms, ED-HNN [85]\nhas been developed as a novel hypergraph neural network (HNN)\narchitecture that can effectively approximate any continuous\nequivariant hypergraph diffusion operators. These operators have\nthe capability to model a wide array of higher-order relations.\nIn this research, it is asserted that predicting node labels in\nheterophilic hypergraphs is more intricate than in graphs since\na hyperedge may consist of nodes from multiple categories.\nTherefore, the research further analyzes its learnable equivariant\ndiffusion operator and demonstrates its superiority in predicting\nheterophilic node labels on four hypergraphs: Congress [89],\nSenate [89], Walmart [90], and House [91].\nHowever, based on our review of related works, there is\ncurrently no established characterization or algorithmic design for\nspecific heterophilic hypergraphs. The primary challenges revolve\naround accurately describing the heterophily of hypergraphs and\ndeveloping architecture refinement methods that capture the local\nhomophily of hypergraphs. This research direction is still promis-\ning but in its early stages of development.\n7.2\nUnveiling the Correlation Between Graph Het-\nerophily and Other Research Problems\n7.2.1\nRelation between Graph Heterophily and Adversarial\nRobustness\nAdversarial robustness is a critical aspect of GNN models, con-\ncerning the safety and trustworthiness of modern systems. It\npertains to the vulnerability or extreme non-robustness of a GNN\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\nClass1\nClass2\nClassifier with no defense\nEdge\nPerturbed edge\n\u089b\n\u089b\nPredict\nGNN\nPoisoned node is classified incorrectly\nPoisoned\nnode\nnorma\nFig. 8: Small, adversarial perturbations of the graph structure and node features lead GNN to misclassify target u.\nHeterophily Graph\nClass1\nClass2\nClass3\nnode of interest\n1-hop\n1-hop aggregation\nAfter\nSimilar to\nneighbors \nA Graph\n\u2026\n\u2026\n\u2026\nnode of interest\nshared neighbors\nothers\n\u2026\n\u2026\n\u2026\n# Number of Layers Increasing\n\u2026\n\u2026\n\u2026\nMore and More\nSimilar\u2026\n\u2026\nFig. 9: Illustration of the connection between the heterophily property (Left) and the over-smoothing issue (Right) on GNNs.\nmodel when confronted with an input that has been adversar-\nially perturbed. These perturbations are often imperceptible or\nindiscernible to humans, posing a significant challenge to the\nrobustness and reliability of the model. Early studies [92]\u2013[95]\nprimarily focused on robustness in the context of homophilic\nGNNs. Only recently has there been a growing recognition of the\nnecessity to analyze heterophilic problems to design robust GNNs.\nThe intuition behind this recognition is that structural attacks are\npredominantly heterophilic in nature, as shown in Fig. 8. The\ncurrent researches [96]\u2013[99] are centered around the design of\ndefense GNNs, which refer to a class of methods or techniques\nused to protect or defend GNN from different types of adversarial\nattacks. Focusing on heterophilic connections as structural attacks\non graphs, GNNGUARD [96] stands out as the first technique that\nshows a successful defense on heterophilic graphs. Specifically,\nGNNGUARD detects and quantifies the relationship between\ngraph structure and node features, if such a relationship exists,\nthen leverages this relationship to mitigate the adverse effects of\nthese attacks. Consequently, GNNGUARD allows for robust prop-\nagation of neural messages within the underlying GNN. Similarly,\nGARNET [98] and ATDGIA [97] also treat heterophilic edges as\nadversarial edges and concentrate on designing defensive GNNs\nwith anti-interference mechanisms for these adversarial edges. In\nrecent research, Zhu et al. [99] take an additional statement by\nproviding theoretical insights into the relation between adversarial\nstructural attacks and changes in the homophily level of the\nunderlying graphs. As an implication of this relation, it is further\ndiscussed that incorporating graph heterophily considerations into\nGNN designs can also contribute to enhancing model robustness.\nThe key findings from this study are as follows:\n\u2022\nThrough empirical validation on several strongly ho-\nmophilic graphs, including Cora, Pubmed, and Citeseer,\nit is observed that all changes introduced by effective\nattacks in the graph structure align with the conclusion that\nstructural attacks are primarily heterophilic in nature.\n\u2022\nA comprehensive benchmark study of GNN models is con-\nducted to validate that the incorporation of heterophilic\ndesign leads to improved empirical robustness.\n\u2022\nThrough empirical validation, models that incorporate\nidentified heterophilic design demonstrate significantly\nenhanced certifiable robustness compared to models\nlacking this design.\nRobustness and the interplay between homophily and het-\nerophily in structural attacks on GNNs are key areas of inves-\ntigation in these studies, with implications for improving robust\nGNN model designs.\n7.2.2\nRelation between Graph Heterophily and Over-\nSmoothing\nHeterophily and over-smoothing are two critical aspects that limit\nthe performance of modern GNNs, where the former violates the\ngeneral homophilic topological assumption, and the latter prevents\nmost GNNs from going deeper when the performance drops with\nthe increasing number of layers. Surprisingly, some studies show\nthat GNNs for heterophilic graphs could be empirically utilized\nto address the over-smoothing problem [25], [29], [74], and vice\nversa [76]. We illustrate the connection between the heterophily\nproperty of graphs and the over-smoothing issue in GNN learning\nin Fig. 9.\nWhile previous work has proposed empirical designs that\nsimultaneously address both of these issues, Yan et al. [28] took\nthe first step in explaining the connections between these two\naspects. They suggested that these issues could be attributed to a\ncommon underlying cause from a unified theoretical perspective:\nDiscriminative node representations are challenging to learn under\nover-smoothing/heterophily problems. More recently, Bodnar et\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\n\u08bb\n(\n)\ncorrectly\nnormals\nanomalies\nanomalies\n0.88\n0.12\nnomalies\n0.44\n0.56\naverage homophily\naverage heterophily\nFig. 10: A toy graph for anomaly detection. The average homophily\nand heterophily for anomalies and normals are presented.\nal. [46] utilized cellular sheaf theory to provide a novel topological\nperspective on heterophily and oversmoothing in GNNs. Their\nanalysis demonstrated that the underlying sheaf structure of the\ngraph is intimately linked with both of these critical factors that\ninfluence the performance of GNNs. Interestingly, both of these\nstudies found evidence supporting the benefits of negatively signed\nedges in GNNs, albeit with different mathematical justifications.\nThese theoretical findings also provide support for the effective-\nness of technologies represented by FAGCN [25].\nIn summary, these observations and analyses have yielded\npromising results in quest to comprehensively explore and un-\nderstand the relationship between heterophily and oversmoothing.\nThis opens up significant opportunities for researchers to delve\ndeeper into the solid foundations and evidence supporting these\nconcepts and collectively work towards overcoming them.\n7.2.3\nRelation between Graph Heterophily and Graph\nAnomaly Detection\nFraud detection is a well-studied area within anomaly detection,\nwhich is primarily focused on identifying anomalies or abnormal\nbehaviors within datasets. Recent studies [100]\u2013[103], referring to\nthese anomalies as abnormal nodes within a graph, have empha-\nsized that in fraud networks, these abnormal nodes often display\nsignificant heterophily. As shown in Fig. 10, this heterophily is\na consequence of the fact that these abnormal nodes tend to be\nclustered in minority subgroups while being surrounded by normal\nnodes, resulting in higher levels of heterophily compared to their\nnormal counterparts.\nMost graph-based anomaly detection (GAD) methods tend to\nemphasize how to obtain homophilic representations and often\nignore the impact of heterophilic attributes of abnormal nodes\nduring message aggregation [104]\u2013[106]. This poses challenges\nin the context of fraud detection. Gao et al. [100] acknowledge\nthese challenges and investigate structural distribution shift (SDS)\nacross anomaly and normal data. They observed discrepancies in\nSDS between anomaly and normal nodes and argued that this\ndiscrepancy is exacerbated by the class imbalance nature of GAD.\nAdditionally, Gong et al. [102] hold a similar viewpoint by calcu-\nlating the average homophily ratios of normal nodes and abnormal\nnodes in various fraud graphs and discovering that abnormal nodes\nconsistently demonstrate high heterophily. Therefore, they adopted\na strategy to sparsify the structures of target graphs to effec-\ntively reduce heterophilic connections and collaboratively learn\nnode representations. This approach robustly detects anomalies\nby uncovering the underlying dependency among node pairs in\n1\n2\n4\n6\n7\n8\n9\n10\n11\n12\n\u210b\u0b35= 2\n3\n\u210b\u0b36= 1\n2\n\u210b\u0b37= 0\n\u210b\u0b38= 1\n2\n\u210b\u0b39= 2\n5\n\u210b\u0b3a= 0\n\u0912\u0ae0= ?\n\u0912\u0ae1= ?\n\u0912\u0ae2= ? \u210b\u0b35\u0b34= 1\n2\n\u210b\u0b35\u0b35= 1\n2\n\u210b\u0b35\u0b36= 0\n\u210b\u0b35\u0b37= 0\nClass1\nClass2\nClass3\nUncertain Class\nHeterophilic Node\nHomophilic Node\n5\n3\n13\nFig. 11: Illustration of the uncertainty of connections in a graph.\nterms of homophily and heterophily. Similarly, researchers like\nShi et al. [101], Gao et al. [107], and Chai et al. [108] also\nrecognize the significance of addressing heterophily problems in\nfraud detection tasks. They argued that relying solely on low-pass\nproperties during GNN aggregation introduces noise containing\nheterophilic attributes into normal node representation learning,\nconsequently impairing fraud detection performance. As a solu-\ntion, various strategies have been devised in these studies to enable\nmodels to dynamically select low-frequency, high-frequency, or a\ncombination of both components to differentiate abnormal nodes.\nThis adaptive approach effectively captures both low-frequency\nand high-frequency graph signals to enhance fraud detection\nperformance.\n7.2.4\nRelation between Graph Heterophily and Uncertainty\nModeling\nThe modeling of uncertainty involves capturing and comprehend-\ning inherent uncertainties in data, a subject that has garnered sig-\nnificant attention in the development of reliable decision-making\nsystems [48], [109]\u2013[111]. Within the context of graph het-\nerophily, where nodes can exhibit diverse connections, uncertainty\nemerges as a pivotal factor. The varied degrees of heterophilic\nrelationships between nodes contribute to the complex information\nflow, introducing uncertainties regarding the nature and impact of\nthese connections. As depicted in Fig. 11, a toy graph illustrates\nthe uncertainty of connections.\nAn examination of graph structures from a node-level per-\nspective reveals a mixture of homophily and heterophily in un-\ncertain real-world scenarios, signifying the coexistence of both\nhomophilic and heterophilic nodes. In the presence of such a\nmixture, GNNs exhibit a notable bias towards homophilic nodes,\nresulting in a substantial performance decline on heterophilic\nnodes. Liu et al. [110] underscore this bias issue of GNNs and\nadvocate for model predictions to maintain high accuracy while\nminimizing model bias. Model bias refers to the phenomenon\nwhere GNNs trained on semi-homophilic graphs perform worse\non a heterophilic set than the same model exclusively trained\non the heterophilic segment of the graph. This leads to a more\npronounced performance bias, emphasizing the need for GNNs\nto effectively address heterophily to ensure robust and unbiased\npredictions across diverse nodes.\nTo mitigate the bias issue, Liu et al. [110] explore an\nUncertainty-aware Debiasing (UD) framework, which retains the\nknowledge of the biased model on certain nodes and compensates\nfor the nodes with high uncertainty. In particular, UD estimates\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n16\nthe uncertainty of the GNN output to recognize heterophilic\nnodes. Additionally, a probabilistic solution named Graph Topol-\nogy Uncertainty (GraphTU) is innovatively proposed in [48],\nwhich enables the synthetic nodes to better catch the informative\ncharacteristics from the topology space. For each source minor\nnode, Gao et al. [48] not only consider the feature but also the\nvaluable information from their crucial neighbors. Interestingly,\nthey observe that statistical variances in local neighborhoods can\nbe fully exploited to extend the training distribution for the minor\nclass in a non-parametric manner.\nIn conclusion, the relation between graph heterophily and\nuncertainty modeling provides a rich landscape for exploration.\nBy delving into this connection, researchers can enhance their\nunderstanding of how network structures influence uncertainty,\npaving the way for improved models and strategies in various\napplications, from social networks to biological systems.\nOther Promising Heterophily Explorations. Recently, re-\nsearchers have sought to extend the design of heterophilic GNNs\nbeyond the semi-supervised paradigm, focusing on self-supervised\nheterophilic GNNs [31], [112], [113]. It is noteworthy that He et\nal. [112] identified commonalities between the establishment of\nthe Graph Contrastive Learning (GCL) framework and the design\nof heterophilic GNNs. In the GCL framework, the task involves\nidentifying additional positive samples belonging to the same\nclass, while in the design of heterophilic GNNs, the objective is\nto identify other intra-class neighbors for each node. Recognizing\nthis similarity, they integrated the GCL sampling strategy with\nhomophily discrimination, creating a novel contrastive learning\nframework to address these crucial challenges. This framework\ndraws inspiration from homophily, breaking traditional data en-\nhancement strategies and providing a fresh perspective on estab-\nlishing a GCL framework. Furthermore, it executes contrastive\nlearning by sampling homophily neighbors as positive examples,\noffering innovative insights for the design of heterophilic GNNs.\nTo address the limitations imposed by the strong homophily\nassumption and label dependency, Li et al. [113] introduced a\nnovel multi-task model named PairE with a contrastive learning\nmethod. This model is designed to preserve information pertaining\nto both homophily and heterophily by transcending the localized\nnode view. It achieves this by harnessing higher-level entities with\nmore expressive power, enabling the representation of feature\nsignals beyond homophily. In a similar vein and motivated by\ncomparable considerations, Liu et al. [31] propose an innovative\nself-supervised Graph Representation Learning method called\nEdge Heterophily Discriminating (GREET). By discriminating\nedges, GREET obtains contrastive views with both homophilic\nand heterophilic edges, leveraging them to learn expressive repre-\nsentations.\n8\nFUTURE DIRECTIONS\nGNNs for heterophilic graphs are fast developing in the past\nfew years. Beyond current research, there still remain several\nopen challenges and opportunities worthy of further attention and\nexploration. In this section, we discuss the following directions to\nstimulate future research.\nInterpretable Heterophilic GNNs. Interpretability is a crucial\naspect of GNN models in risk-sensitive or privacy-related applica-\ntion fields, e.g., healthcare and cybersecurity. Although there are\nseveral studies on the interpretability [114]\u2013[116] for homophilic\nGNNs, how to explain the predictions of heterophilic GNNs\nis still under-explored. Heterophily makes interpretability more\nchallenging than homophily: since most local neighbor nodes are\nnot in the same class as the ego nodes, extracting explainable sub-\ngraphs from highly heterophilic graph data is much harder, where\nboth proximal and distant topological structures are required to\nbe discovered and exploited. For instance, ShapeGGen [116]\ntypically proposed a dataset generator that can automatically\ngenerate a variety of benchmark datasets (e.g., varying graph\nsizes, degree distributions, homophilic and heterophilic graphs),\naccompanied by ground-truth explanations From the data-centric\nview of heterophilic graphs, how to extract explainable subgraphs\nunder complex similarity relationships between ego nodes and\ntheir potential neighbors on heterophilic graphs for interpretability\nis an open question. Moreover, from the model-centric view of\nheterophilic GNNs, how to explain the model\u2019s predictions on\ngraphs with different degrees of heterophily also deserves further\nexploration.\nScalable Heterophilic GNNs. Current heterophilic GNNs are\ngenerally trained on relatively small graphs, which significantly\nlimits their ability to model large-scale data and explore more\ncomplicated heterophilic patterns. Although possible solutions to\ntackle scalability can be borrowed from the mainstream graph\nsampling strategies\n[117], [118] for homophilic GNNs, the\nconnections and relationships of heterophilic nodes would be\nundermined by sampling only mini-batches, especially when sim-\nilar and dissimilar neighbors contribute differently to learning\nthe ego-node representations. LINKX [27] recently verifies that\neven a simple MLP-based model could outperform GNNs for\nmini-batch training on large-scale heterophilic graphs. Moreover,\nHopGNN [49] introduces a hop interaction paradigm to address\nthe scalability and over-smoothing problem of GNNs simultane-\nously, where its core idea is to convert the interaction target among\nnodes to pre-processed multi-hop features inside each node.\nHence, addressing the scalability problem of heterophilic graphs\nrequires exploring more relationships between ego-neighbor node\nfeatures and multi-hop information. How to keep the inherent\nheterophily unchanged when conducting sampling on heterophilic\ngraphs is still an open question.\nTheoretical\nHeterophily\nExploration. Despite the notable\nachievements of heterophilic GNNs across diverse tasks and\ndatasets, their efficacy lacks a solid theoretical foundation to sub-\nstantiate their impact on enhancing graph representation learning.\nPresently, many methods are predominantly designed based on\nintuition and evaluated through empirical experiments. Notably,\na recent breakthrough by Ma et al. [119] has unveiled that,\nunder specific conditions, GCN can exhibit remarkable perfor-\nmance on heterophilic graphs. This discovery has been supported\nby comprehensive theoretical validation, outlining the distinctive\ncharacteristics and conditions under which heterophilic graphs can\nexcel with GCN. The elucidation of these theoretical findings\nposes a significant challenge to the current rationale underlying\nthe design of heterophilic GNNs. Consequently, there arises a\ncritical need for intensified theoretical exploration in the future.\nThis exploration should delve into aspects such as the impact\nof heterophily on altering the trends of generalization bounds.\nBridging this explanatory gap is crucial for comprehending the\nextent to which heterophily influences the performance constraints\nof homophilic GNNs.\nDiverse Heterophily Learning Tasks. The research on het-\nerophilic GNNs has primarily demonstrated success in node-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n17\nlevel tasks, covering both semi-supervised [36], [51], [63] and\nunsupervised scenarios [31], [112], [120]. A recent study by Zhou\net al. [121] introduces a novel heterophilic framework named\nDisenLink, specifically designed for addressing link-level tasks on\nheterophilic graphs. In heterophilic graphs, DisenLink reveals that\nlink formation is influenced by numerous latent factors, causing\nlinked nodes to share similarity in certain factors while being\ndissimilar in others. This results in an overall low similarity\nbetween linked nodes. Many existing link prediction approaches\noperate under the homophily assumption, using similarity-based\nheuristics or representation learning methods to predict links.\nTherefore, accurately predicting links with heterophilic attributes,\ndetermined by specific potential similar factors, poses a significant\nchallenge. DisenLink identifies the challenges of link-level tasks\non heterophilic graphs, but there are still significant open questions\nin this domain. These include the development of reasonable\nheterophilic benchmark datasets and the refinement of GNN archi-\ntectures for link-level tasks. Additionally, similar open questions\nexist for graph-level tasks, indicating that further exploration is\nneeded in these areas.\nBroader Scope of Practical Applications. In the real world,\nheterophilic relationships are pervasive and exist widely in var-\nious graph-structured data [122]\u2013[126]. However, current research\non heterophilic GNNs predominantly focuses on specific strong\nheterophilic graphs, such as social networks [122] and web\npage networks [123]. Other fields, such as biomedicine [124]\nand chemistry [125], [126], which also naturally exhibit the\nheterophilic property, deserve more explorations. For instance,\nin protein-protein interaction networks, proteins with interactions\noften belong to different gene ontologies. Moreover, in the field\nof drug discovery, DCMGCN [127] verifies the heterophily prop-\nerty of the drug-drug networks and proposed a combination of\nintermediate representations, and high-similarity neighborhoods,\nto boost GCN learning on the heterophily and sparse drug-drug\nnetworks. Besides, in the field of fraud detection, fraudsters tend to\nbe more connected to regular users represented by fraud networks.\nAnd some existing research, e.g., DRAG [128] and GAGA [103],\nsolved the problem by conducting the binary heterophilic graph\nclassification task. This highlights the untapped potential of het-\nerophilic GNNs in a broader range of application areas. There is\nan expectation to extend the use of heterophilic GNNs to various\nfields, including financial networks, network security, community\ndetection, biomedicine, and chemistry.\n9\nCONCLUSION\nIn this paper, we conducted a comprehensive overview of graph\nneural networks for heterophilic graphs. To achieve this, we\nproposed a novel and systematic taxonomy of existing heterophilic\nGNNs, categorizing them into non-local neighbor extension meth-\nods, GNN architecture refinement methods, and hybrid methods.\nOur contribution includes a thorough introduction and analysis\nof the current research progress and challenges in the field of\nheterophilic graph learning. To highlight the significance of het-\nerophilic GNNs, we delved into a thorough discussion of their\napplications on diverse graphs, revealing the correlation between\ngraph heterophily and various problem domains, such as model\nrobustness, over-smoothing, and graph anomaly detection. In the\nend, we shared our insights into future research opportunities and\ndirections that can contribute to the advancement of heterophilic\nGNNs.\nACKNOWLEDGMENTS\nThis research was supported by the Australian Research Council\n(ARC) under grants FT210100097 and DP240101547.\nREFERENCES\n[1]\nJ. Tang, J. Sun, C. Wang, and Z. Yang, \u201cSocial influence analysis\nin large-scale networks,\u201d in Proceedings of the 15th ACM SIGKDD\ninternational conference on Knowledge discovery & data mining, 2009,\npp. 807\u2013816.\n[2]\nH. Peng, R. Zhang, S. Li, Y. Cao, S. Pan, and P. Yu, \u201cReinforced,\nincremental and cross-lingual event detection from social messages,\u201d\nIEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\n[3]\nS. Vashishth, S. Sanyal, V. Nitin, and P. Talukdar, \u201cComposition-\nbased multi-relational graph convolutional networks,\u201d in International\nConference on Learning Representations, 2019.\n[4]\nZ. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, \u201cCon-\nnecting the dots: Multivariate time series forecasting with graph neural\nnetworks,\u201d in Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2020, pp. 753\u2013\n763.\n[5]\nR. Ying, R. He, K. Chen, P. Eksombatchai, W. L. Hamilton, and\nJ. Leskovec, \u201cGraph convolutional neural networks for web-scale\nrecommender systems,\u201d in Proceedings of the 24th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, 2018,\npp. 974\u2013983.\n[6]\nJ. Ma, C. Zhou, P. Cui, H. Yang, and W. Zhu, \u201cLearning disentangled\nrepresentations for recommendation,\u201d in Advances in Neural Informa-\ntion Processing Systems, 2019, pp. 5711\u20135722.\n[7]\nT. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph\nconvolutional networks,\u201d in International Conference on Learning Rep-\nresentations, 2017.\n[8]\nW. L. Hamilton, R. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d in Advances in Neural Information Process-\ning Systems, 2017, pp. 1025\u20131035.\n[9]\nP. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Li`o, and\nY. Bengio, \u201cGraph attention networks,\u201d in International Conference on\nLearning Representations, 2018.\n[10]\nK. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph\nneural networks?\u201d in International Conference on Learning Represen-\ntations, 2019.\n[11]\nZ. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA\ncomprehensive survey on graph neural networks,\u201d IEEE Transactions\non Neural Networks and Learning Systems, vol. 32, no. 1, pp. 4\u201324,\n2020.\n[12]\nS. Zhu, S. Pan, C. Zhou, J. Wu, Y. Cao, and B. Wang, \u201cGraph geometry\ninteraction learning,\u201d in Advances in Neural Information Processing\nSystems, vol. 33, 2020, pp. 7548\u20137558.\n[13]\nY. Liu, K. Ding, Q. Lu, F. Li, L. Y. Zhang, and S. Pan, \u201cTowards self-\ninterpretable graph-level anomaly detection,\u201d in Advances in Neural\nInformation Processing Systems, 2023.\n[14]\nX. Zheng, M. Zhang, C. Chen, Q. V. H. Nguyen, X. Zhu, and S. Pan,\n\u201cStructure-free graph condensation: From large-scale graphs to con-\ndensed graph-free data,\u201d in Advances in Neural Information Processing\nSystems, 2023.\n[15]\nX. Zheng, M. Zhang, C. Chen, S. Molaei, C. Zhou, and S. Pan,\n\u201cGnnevaluator: Evaluating gnn performance on unseen graphs without\nlabels,\u201d in Advances in Neural Information Processing Systems, 2023.\n[16]\nV. Ciotti, M. Bonaventura, V. Nicosia, P. Panzarasa, and V. Latora,\n\u201cHomophily and missing links in citation networks,\u201d EPJ Data Science,\nvol. 5, pp. 1\u201314, 2016.\n[17]\nX. Zheng, M. Zhang, C. Chen, Q. Zhang, C. Zhou, and S. Pan,\n\u201cAuto-heg: Automated graph neural network on heterophilic graphs,\u201d\nin Proceedings of the ACM Web Conference 2023, 2023, pp. 611\u2013620.\n[18]\nJ. Xu, E. Dai, X. Zhang, and S. Wang, \u201cHp-gmn: Graph memory\nnetworks for heterophilous graphs,\u201d in 2022 IEEE International Con-\nference on Data Mining (ICDM).\nIEEE, 2022, pp. 1263\u20131268.\n[19]\nY. Liu, Y. Zheng, D. Zhang, H. Chen, H. Peng, and S. Pan, \u201cTowards\nunsupervised deep graph structure learning,\u201d in Proceedings of the 31st\ninternational conference on World Wide Web, 2022.\n[20]\nX. Zheng, M. Zhang, C. Chen, C. Li, C. Zhou, and S. Pan, \u201cMulti-\nrelational graph neural architecture search with fine-grained message\npassing,\u201d in 2022 IEEE International Conference on Data Mining\n(ICDM).\nIEEE, 2022, pp. 783\u2013792.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n18\n[21]\nY. ZHENG, H. Zhang, V. Lee, Y. Zheng, X. Wang, and S. Pan, \u201cFinding\nthe missing-half: Graph complementary learning for homophily-prone\nand heterophily-prone graphs,\u201d in International Conference on Machine\nLearning, 2023.\n[22]\nS. Pandit, D. H. Chau, S. Wang, and C. Faloutsos, \u201cNetprobe: a fast\nand scalable system for fraud detection in online auction networks,\u201d in\nProceedings of the 16th international conference on World Wide Web,\n2007, pp. 201\u2013210.\n[23]\nJ. Zhu, R. A. Rossi, A. B. Rao, T. Mai, N. Lipka, N. K. Ahmed, and\nD. Koutra, \u201cGraph neural networks with heterophily,\u201d in Proceedings of\nthe AAAI Conference on Artificial Intelligence, 2021.\n[24]\nJ. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra,\n\u201cBeyond homophily in graph neural networks: Current limitations and\neffective designs,\u201d Advances in Neural Information Processing Systems,\nvol. 33, 2020.\n[25]\nD. Bo, X. Wang, C. Shi, and H. Shen, \u201cBeyond low-frequency infor-\nmation in graph convolutional networks,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, 2021, pp. 3950\u20133957.\n[26]\nD. Lim, X. Li, F. Hohne, and S.-N. Lim, \u201cNew benchmarks for\nlearning on non-homophilous graphs,\u201d in Workshop on Graph Learning\nBenchmarks, WebConf, 2021.\n[27]\nD. Lim, F. Hohne, X. Li, S. L. Huang, V. Gupta, O. Bhalerao, and S. N.\nLim, \u201cLarge scale learning on non-homophilous graphs: New bench-\nmarks and strong simple methods,\u201d in Advances in Neural Information\nProcessing Systems, vol. 34, 2021.\n[28]\nY. Yan, M. Hashemi, K. Swersky, Y. Yang, and D. Koutra, \u201cTwo sides of\nthe same coin: Heterophily and oversmoothing in graph convolutional\nneural networks,\u201d arXiv preprint arXiv:2102.06462, 2021.\n[29]\nL. Yang, M. Li, L. Liu, C. Wang, X. Cao, Y. Guo et al., \u201cDiverse\nmessage passing for attribute with heterophily,\u201d in Advances in Neural\nInformation Processing Systems, vol. 34, 2021.\n[30]\nE. Chien, J. Peng, P. Li, and O. Milenkovic, \u201cAdaptive universal gener-\nalized pagerank graph neural network,\u201d in International Conference on\nLearning Representations, 2020.\n[31]\nY. Liu, Y. Zheng, D. Zhang, V. C. Lee, and S. Pan, \u201cBeyond smooth-\ning: Unsupervised graph representation learning with edge heterophily\ndiscriminating,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, 2023, pp. 4516\u20134524.\n[32]\nX. Zheng, Y. Liu, S. Pan, M. Zhang, D. Jin, and P. S. Yu, \u201cGraph\nneural networks for graphs with heterophily: A survey,\u201d arXiv preprint\narXiv:2202.07082, 2022.\n[33]\nB. Bollob\u00b4as, Extremal graph theory.\nCourier Corporation, 2004.\n[34]\nH. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang, \u201cGeom-gcn:\nGeometric graph convolutional networks,\u201d in International Conference\non Learning Representations, 2020.\n[35]\nO. Platonov, D. Kuznedelev, A. Babenko, and L. Prokhorenkova, \u201cChar-\nacterizing graph datasets for node classification: Beyond homophily-\nheterophily dichotomy,\u201d in Advances in Neural Information Processing\nSystems, 2023.\n[36]\nS. Abu-El-Haija, B. Perozzi, A. Kapoor, N. Alipourfard, K. Lerman,\nH. Harutyunyan, G. Ver Steeg, and A. Galstyan, \u201cMixhop: Higher-order\ngraph convolutional architectures via sparsified neighborhood mixing,\u201d\nin International Conference on Machine Learning.\nPMLR, 2019, pp.\n21\u201329.\n[37]\nY. Wang and T. Derr, \u201cTree decomposed graph neural network,\u201d in\nProceedings of the 30th ACM International Conference on Information\n& Knowledge Management, 2021, pp. 2040\u20132049.\n[38]\nY. Song, C. Zhou, X. Wang, and Z. Lin, \u201cOrdered GNN: Ordering\nmessage passing to deal with heterophily and over-smoothing,\u201d in\nProceedings of International Conference on Learning Representations,\n2022.\n[39]\nR. Lei, Z. Wang, Y. Li, B. Ding, and Z. Wei, \u201cEvenNet: Ignoring\nodd-hop neighbors improves robustness of graph neural networks,\u201d in\nAdvances in Neural Information Processing Systems, 2022, pp. 4694\u2013\n4706.\n[40]\nD. Jin, Z. Yu, C. Huo, R. Wang, X. Wang, D. He, and J. Han, \u201cUni-\nversal graph convolutional networks,\u201d Advances in Neural Information\nProcessing Systems, vol. 34, 2021.\n[41]\nW. Jin, T. Derr, Y. Wang, Y. Ma, Z. Liu, and J. Tang, \u201cNode similarity\npreserving graph convolutional networks,\u201d in Proceedings of the 14th\nACM International Conference on Web Search and Data Mining, 2021,\npp. 148\u2013156.\n[42]\nL. Wu, H. Lin, Z. Liu, Z. Liu, Y. Huang, and S. Z. Li, \u201cHomophily-\nenhanced self-supervision for graph structure learning: Insights and\ndirections,\u201d IEEE Transactions on Neural Networks and Learning\nSystems, 2023, doi: 10.1109/TNNLS.2023.3257325.\n[43]\nAnonymous, \u201cMPformer: Advancing graph modeling through het-\nerophily relationship-based position encoding,\u201d in Proceedings of In-\nternational Conference on Learning Representations, 2023.\n[44]\nT. Wang, R. Wang, D. Jin, D. He, and Y. Huang, \u201cPowerful graph con-\nvolutioal networks with adaptive propagation mechanism for homophily\nand heterophily,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, 2022.\n[45]\nX. Li, R. Zhu, Y. Cheng, C. Shan, S. Luo, D. Li, and W. Qian, \u201cFinding\nglobal homophily in graph neural networks when meeting heterophily,\u201d\nin International Conference on Machine Learning.\nPMLR, 2022, pp.\n13 242\u201313 256.\n[46]\nC. Bodnar, F. Di Giovanni, B. Chamberlain, P. Li`o, and M. Bronstein,\n\u201cNeural sheaf diffusion: A topological perspective on heterophily and\noversmoothing in gnns,\u201d in Advances in Neural Information Processing\nSystems, vol. 35, 2022, pp. 18 527\u201318 541.\n[47]\nD. Zou, H. Peng, X. Huang, R. Yang, J. Li, J. Wu, C. Liu, and P. S. Yu,\n\u201cSE-GSL: A general and effective graph structure learning framework\nthrough structural entropy optimization,\u201d in Proceedings of the ACM\nWeb Conference, 2023, pp. 499\u2013510.\n[48]\nJ. Gao, J. Li, K. Zhang, and Y. Kong, \u201cTopology uncertainty modeling\nfor imbalanced node classification on graphs,\u201d in Proceedings of IEEE\nInternational Conference on Acoustics, Speech and Signal Processing.\nIEEE, 2023, pp. 1\u20135.\n[49]\nJ. Chen, Z. Li, Y. Zhu, J. Zhang, and J. Pu, \u201cFrom node interaction to\nhop interaction: New effective and scalable graph learning paradigm,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 7876\u20137885.\n[50]\nS. Luan, C. Hua, Q. Lu, J. Zhu, M. Zhao, S. Zhang, X.-W. Chang, and\nD. Precup, \u201cIs heterophily a real nightmare for graph neural networks\nto do node classification?\u201d arXiv preprint arXiv:2109.05641, 2021.\n[51]\nL. Du, X. Shi, Q. Fu, X. Ma, H. Liu, S. Han, and D. Zhang, \u201cGBK-\nGNN: Gated bi-kernel graph neural networks for modeling both ho-\nmophily and heterophily,\u201d in Proceedings of the ACM Web Conference,\n2022, pp. 1550\u20131558.\n[52]\nL. Wei, H. Zhao, and Z. He, \u201cDesigning the topology of graph neural\nnetworks: A novel feature fusion perspective,\u201d in Proceedings of the\nACM Web Conference, 2022, pp. 1381\u20131391.\n[53]\nX. Ma, Q. Chen, Y. Ren, G. Song, and L. Wang, \u201cMeta-weight\ngraph neural network: Push the limits beyond global homophily,\u201d in\nProceedings of the ACM Web Conference, 2022, pp. 1270\u20131280.\n[54]\nJ. Chen, W. Liu, and J. Pu, \u201cMemory-based message passing: Decou-\npling the message for propagation from discrimination,\u201d in Proceedings\nof IEEE International Conference on Acoustics, Speech and Signal\nProcessing.\nIEEE, 2022, pp. 4033\u20134037.\n[55]\nQ. Chen, Y. Wang, Y. Wang, J. Yang, and Z. Lin, \u201cOptimization-\ninduced graph implicit nonlinear diffusion,\u201d in International Conference\non Machine Learning.\nPMLR, 2022, pp. 3648\u20133661.\n[56]\nS. Chanpuriya and C. Musco, \u201cSimplified graph convolution with het-\nerophily,\u201d Advances in Neural Information Processing Systems, vol. 35,\npp. 27 184\u201327 197, 2022.\n[57]\nD. Tortorella and A. Micheli, \u201cLeave graphs alone: Addressing over-\nsquashing without rewiring,\u201d in Proceedings of Learning on Graphs\nConference, 2022.\n[58]\nR. Yang, W. Dai, C. Li, J. Zou, and H. Xiong, \u201cNCGNN: Node-level\ncapsule graph neural network for semisupervised classification,\u201d IEEE\nTransactions on Neural Networks and Learning Systems, 2022, doi:\n10.1109/TNNLS.2022.3179306.\n[59]\nJ. Chen, S. Chen, M. Bai, J. Pu, J. Zhang, and J. Gao, \u201cGraph\ndecoupling attention markov networks for semisupervised graph node\nclassification,\u201d IEEE Transactions on Neural Networks and Learning\nSystems, 2022, doi: 10.1109/TNNLS.2022.3161453.\n[60]\nC. Liu, X. Ma, Y. Zhan, L. Ding, D. Tao, B. Du, W. Hu, and D. P.\nMandic, \u201cComprehensive graph gradual pruning for sparse training in\ngraph neural networks,\u201d IEEE Transactions on Neural Networks and\nLearning Systems, 2023, doi: 10.1109/TNNLS.2023.3282049.\n[61]\nL. Wu, H. Lin, B. Hu, C. Tan, Z. Gao, Z. Liu, and S. Z. Li, \u201cBeyond ho-\nmophily and homogeneity assumption: Relation-based frequency adap-\ntive graph neural networks,\u201d IEEE Transactions on Neural Networks\nand Learning Systems, 2023, doi: 10.1109/TNNLS.2022.3230417.\n[62]\nZ. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, \u201cBeyond low-pass\nfiltering: Graph convolutional networks with automatic filtering,\u201d IEEE\nTransactions on Knowledge & Data Engineering, vol. 35, no. 07, pp.\n6687\u20136697, 2023.\n[63]\nK. Kong, J. Chen, J. Kirchenbauer, R. Ni, C. B. Bruss, and T. Goldstein,\n\u201cGOAT: A global transformer on large-scale graphs,\u201d in International\nConference on Machine Learning, 2023.\n[64]\nJ. Huang, L. Du, X. Chen, Q. Fu, S. Han, and D. Zhang, \u201cRobust mid-\npass filtering graph convolutional networks,\u201d in Proceedings of the ACM\nWeb Conference, 2023, pp. 328\u2013338.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n19\n[65]\nY. Yan, Y. Chen, H. Chen, M. Xu, M. Das, H. Yang, and H. Tong, \u201cFrom\ntrainable negative depth to edge heterophily in graphs,\u201d in Advances in\nNeural Information Processing Systems, 2023.\n[66]\nL. Liang, X. Hu, Z. Xu, Z. Song, and I. King, \u201cPredicting global label\nrelationship matrix for graph neural networks under heterophily,\u201d in\nAdvances in Neural Information Processing Systems, 2023.\n[67]\nK. Xu, C. Li, Y. Tian, T. Sonobe, K.-i. Kawarabayashi, and S. Jegelka,\n\u201cRepresentation learning on graphs with jumping knowledge networks,\u201d\nin International Conference on Machine Learning.\nPMLR, 2018, pp.\n5453\u20135462.\n[68]\nN. T. Huang, S. Villar, C. Priebe, D. Zheng, C. Huang, L. Yang, and\nV. Braverman, \u201cFrom local to global: Spectral-inspired graph neural\nnetworks,\u201d in Advances in Neural Information Processing Systems,\n2022.\n[69]\nE. Dai, S. Zhou, Z. Guo, and S. Wang, \u201cLabel-wise graph convolutional\nnetwork for heterophilic graphs,\u201d in Proceedings of Learning on Graphs\nConference, 2022.\n[70]\nJ. Chen, S. Chen, J. Gao, Z. Huang, J. Zhang, and J. Pu, \u201cExploit-\ning neighbor effect: Conv-agnostic GNN framework for graphs with\nheterophily,\u201d IEEE Transactions on Neural Networks and Learning\nSystems, 2023, doi: 10.1109/TNNLS.2023.3267902.\n[71]\nQ. Li, Z. Han, and X.-M. Wu, \u201cDeeper insights into graph convolutional\nnetworks for semi-supervised learning,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, 2018.\n[72]\nF. Wu, A. Souza, T. Zhang, C. Fifty, T. Yu, and K. Weinberger, \u201cSim-\nplifying graph convolutional networks,\u201d in International Conference on\nMachine Learning.\nPMLR, 2019, pp. 6861\u20136871.\n[73]\nR. Sinkhorn and P. Knopp, \u201cConcerning nonnegative matrices and\ndoubly stochastic matrices,\u201d Pacific Journal of Mathematics, vol. 21,\nno. 2, pp. 343\u2013348, 1967.\n[74]\nM. Liu, Z. Wang, and S. Ji, \u201cNon-local graph neural networks,\u201d IEEE\nTransactions on Pattern Analysis & Machine Intelligence, 2021.\n[75]\nT. Yang, Y. Wang, Z. Yue, Y. Yang, Y. Tong, and J. Bai, \u201cGraph pointer\nneural networks,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, 2022.\n[76]\nM. Chen, Z. Wei, Z. Huang, B. Ding, and Y. Li, \u201cSimple and deep\ngraph convolutional networks,\u201d in International Conference on Machine\nLearning.\nPMLR, 2020, pp. 1725\u20131735.\n[77]\nS. Suresh, V. Budde, J. Neville, P. Li, and J. Ma, \u201cBreaking the limit\nof graph neural networks by improving the assortativity of graphs with\nlocal mixing patterns,\u201d in Proceedings of the ACM SIGKDD Conference\non Knowledge Discovery & Data Mining, 2021, pp. 1541\u20131551.\n[78]\nD. Jin, R. Wang, M. Ge, D. He, X. Li, W. Lin, and W. Zhang, \u201cRAW-\nGNN: Random walk aggregation based graph neural network,\u201d in\nProceedings of International Joint Conference on Artificial Intelligence.\nIJCAI, 2022, pp. 2108\u20132114.\n[79]\nD. He, C. Liang, H. Liu, M. Wen, P. Jiao, and Z. Feng, \u201cBlock modeling-\nguided graph convolutional neural networks,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence, 2022.\n[80]\nJ. Park, S. Yoo, J. Park, and H. J. Kim, \u201cDeformable graph convolu-\ntional networks,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 36, no. 7, 2022, pp. 7949\u20137956.\n[81]\nF. Luo, J. Z. Wang, and E. Promislow, \u201cExploring local community\nstructures in large networks,\u201d in Proceedings of the IEEE/WIC/ACM\nInternational Conference on Web Intelligence, 2006, pp. 233\u2013239.\n[82]\nY. Ding, E. Yan, A. Frazho, and J. Caverlee, \u201cPagerank for ranking\nauthors in co-citation networks,\u201d Journal of the American Society for\nInformation Science and Technology, vol. 60, no. 11, pp. 2229\u20132243,\n2009.\n[83]\nG. Xiao, L. Chen, X. Chen, C. Jiang, A. Ni, C. Zhang, and F. Zong,\n\u201cA hybrid visualization model for knowledge mapping: Scientometrics,\nSAOM, and SAO,\u201d IEEE Transactions on Intelligent Transportation\nSystems, 2023, doi: 10.1109/TITS.2023.3327266.\n[84]\nZ. Zhou, G. Lin, K. Yang, L. BAI, Y. Wang et al., \u201cGReTo: Remedying\ndynamic graph topology-task discordance via target homophily,\u201d in\nProceedings of International Conference on Learning Representations,\n2023.\n[85]\nP. Wang, S. Yang, Y. Liu, Z. Wang, and P. Li, \u201cEquivariant hypergraph\ndiffusion neural operators,\u201d in Proceedings of International Conference\non Learning Representations, 2023.\n[86]\nY. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional recurrent\nneural network: Data-driven traffic forecasting,\u201d in Proceedings of\nInternational Conference on Learning Representations, 2018.\n[87]\nS. Wang, Y. Li, J. Zhang, Q. Meng, L. Meng, and F. Gao, \u201cPM2.5-\nGNN: A domain knowledge enhanced graph neural network for pm2.5\nforecasting,\u201d in Proceedings of International Conference on Advances\nin Geographic Information Systems, 2020, pp. 163\u2013166.\n[88]\nN. Veldt, A. R. Benson, and J. Kleinberg, \u201cHigher-order homophily is\ncombinatorially impossible,\u201d SIAM Review, 2022.\n[89]\nJ. H. Fowler, \u201cLegislative cosponsorship networks in the US House and\nSenate,\u201d Social networks, vol. 28, no. 4, pp. 454\u2013465, 2006.\n[90]\nI. Amburg, N. Veldt, and A. Benson, \u201cClustering in graphs and hy-\npergraphs with categorical edge labels,\u201d in Proceedings of The Web\nConference, 2020, pp. 706\u2013717.\n[91]\nP. S. Chodrow, N. Veldt, and A. R. Benson, \u201cHypergraph clustering:\nFrom blockmodels to modularity,\u201d Science Advances, 2021.\n[92]\nL. Sun, Y. Dou, C. Yang, J. Wang, P. S. Yu, L. He, and B. Li,\n\u201cAdversarial attack and defense on graph data: A survey,\u201d arXiv preprint\narXiv:1812.10528, 2018.\n[93]\nH. Wu, C. Wang, Y. Tyshetskiy, A. Docherty, K. Lu, and L. Zhu,\n\u201cAdversarial examples for graph data: Deep insights into attack and\ndefense,\u201d in Proceedings of International Joint Conference on Artificial\nIntelligence, 2019, pp. 4816\u20134823.\n[94]\nN. Entezari, S. A. Al-Sayouri, A. Darvishzadeh, and E. E. Papalexakis,\n\u201cAll you need is low (rank) defending against adversarial attacks on\ngraphs,\u201d in Proceedings of International Conference on Web Search\nand Data Mining, 2020, pp. 169\u2013177.\n[95]\nD. Zhu, Z. Zhang, P. Cui, and W. Zhu, \u201cRobust graph convolutional\nnetworks against adversarial attacks,\u201d in Proceedings of ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining,\n2019, pp. 1399\u20131407.\n[96]\nX. Zhang and M. Zitnik, \u201cGNNGUARD: Defending graph neural net-\nworks against adversarial attacks,\u201d in Advances in Neural Information\nProcessing Systems, vol. 33, 2020, pp. 9263\u20139275.\n[97]\nY. Chen, H. Yang, Y. Zhang, M. KAILI, T. Liu, B. Han, and J. Cheng,\n\u201cUnderstanding and improving graph injection attack by promoting un-\nnoticeability,\u201d in Proceedings of International Conference on Learning\nRepresentations, 2022.\n[98]\nC. Deng, X. Li, Z. Feng, and Z. Zhang, \u201cGARNET: Reduced-rank\ntopology learning for robust and scalable graph neural networks,\u201d in\nProceedings of Learning on Graphs Conference.\nPMLR, 2022, pp.\n3\u20131.\n[99]\nJ. Zhu, J. Jin, D. Loveland, M. T. Schaub, and D. Koutra, \u201cHow\ndoes heterophily impact the robustness of graph neural networks?\nTheoretical connections and practical implications,\u201d in Proceedings of\nACM SIGKDD Conference on Knowledge Discovery and Data Mining,\n2022, pp. 2637\u20132647.\n[100] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAlleviating\nstructural distribution shift in graph anomaly detection,\u201d in Proceedings\nof ACM International Conference on Web Search and Data Mining,\n2023, pp. 357\u2013365.\n[101] F. Shi, Y. Cao, Y. Shang, Y. Zhou, C. Zhou, and J. Wu, \u201cH2-fdetector:\nA gnn-based fraud detector with homophilic and heterophilic connec-\ntions,\u201d in Proceedings of the ACM Web Conference 2022, 2022, pp.\n1486\u20131494.\n[102] Z. Gong, G. Wang, Y. Sun, Q. Liu, Y. Ning, H. Xiong, and J. Peng,\n\u201cBeyond homophily: Robust graph anomaly detection via neural sparsi-\nfication,\u201d in Proceedings of International Joint Conference on Artificial\nIntelligence, 2023, pp. 2104\u20132113.\n[103] Y. Wang, J. Zhang, Z. Huang, W. Li, S. Feng, Z. Ma, Y. Sun, D. Yu,\nF. Dong, J. Jin et al., \u201cLabel information enhanced fraud detection\nagainst low homophily in graphs,\u201d in Proceedings of the ACM Web\nConference, 2023, pp. 406\u2013416.\n[104] X. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and\nL. Akoglu, \u201cA comprehensive survey on graph anomaly detection with\ndeep learning,\u201d IEEE Transactions on Knowledge and Data Engineer-\ning, 2021.\n[105] K. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detection\non attributed networks,\u201d in Proceedings of the 2019 SIAM International\nConference on Data Mining.\nSIAM, 2019, pp. 594\u2013602.\n[106] Y. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, \u201cAnomaly de-\ntection on attributed networks via contrastive self-supervised learning,\u201d\nIEEE transactions on neural networks and learning systems, vol. 33,\nno. 6, pp. 2378\u20132392, 2021.\n[107] Y. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAddressing\nheterophily in graph anomaly detection: A perspective of graph spec-\ntrum,\u201d in Proceedings of the ACM Web Conference, 2023, pp. 1528\u2013\n1538.\n[108] Z. Chai, S. You, Y. Yang, S. Pu, J. Xu, H. Cai, and W. Jiang, \u201cCan\nabnormality be detected by graph neural networks,\u201d in Proceedings of\nInternational Joint Conference on Artificial Intelligence, 2022, pp. 23\u2013\n29.\n[109] X. Tang, K. Yang, H. Wang, J. Wu, Y. Qin, W. Yu, and D. Cao,\n\u201cPrediction-uncertainty-aware decision-making for autonomous vehi-\ncles,\u201d IEEE Transactions on Intelligent Vehicles, vol. 7, no. 4, pp. 849\u2013\n862, 2022.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n20\n[110] Y. Liu, X. Ao, F. Feng, and Q. He, \u201cUD-GNN: Uncertainty-aware\ndebiased training on semi-homophilous graphs,\u201d in Proceedings of ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, 2022,\npp. 1131\u20131140.\n[111] D. Chen, Z. Xie, R. Liu, W. Yu, Q. Hu, X. Li, and S. X.\nDing, \u201cBayesian hierarchical graph neural networks with uncertainty\nfeedback for trustworthy fault diagnosis of industrial processes,\u201d\nIEEE Transactions on Neural Networks and Learning Systems, 2023,\ndoi:10.1109/TNNLS.2023.3319468.\n[112] D. He, J. Zhao, R. Guo, Z. Feng, D. Jin, Y. Huang, Z. Wang, and\nW. Zhang, \u201cContrastive learning meets homophily: Two birds with\none stone,\u201d in Proceedings of International Conference on Machine\nLearning, 2023.\n[113] Y. Li, B. Lin, B. Luo, and N. Gui, \u201cGraph representation learning\nbeyond node and homophily,\u201d IEEE Transactions on Knowledge and\nData Engineering, vol. 35, no. 5, pp. 4880\u20134893, 2023.\n[114] R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, \u201cGnnex-\nplainer: Generating explanations for graph neural networks,\u201d Advances\nin Neural Information Processing Systems, vol. 32, p. 9240, 2019.\n[115] D. Luo, W. Cheng, D. Xu, W. Yu, B. Zong, H. Chen, and X. Zhang,\n\u201cParameterized explainer for graph neural network,\u201d Advances in Neu-\nral Information Processing Systems, vol. 33, 2020.\n[116] C. Agarwal, O. Queen, H. Lakkaraju, and M. Zitnik, \u201cEvaluating\nexplainability for graph neural networks,\u201d Scientific Data, vol. 10, no. 1,\np. 144, 2023.\n[117] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh,\n\u201cCluster-gcn: An efficient algorithm for training deep and large graph\nconvolutional networks,\u201d in Proceedings of the 25th ACM SIGKDD\nInternational Conference on Knowledge Discovery & Data Mining,\n2019, pp. 257\u2013266.\n[118] H. Zeng, H. Zhou, A. Srivastava, R. Kannan, and V. Prasanna, \u201cGraph-\nsaint: Graph sampling based inductive learning method,\u201d in Interna-\ntional Conference on Learning Representations, 2019.\n[119] Y. Ma, X. Liu, N. Shah, and J. Tang, \u201cIs homophily a necessity for\ngraph neural networks?\u201d in Proceedings of International Conference on\nLearning Representations, 2021.\n[120] T. Xiao, Z. Chen, Z. Guo, Z. Zhuang, and S. Wang, \u201cDecoupled\nself-supervised learning for graphs,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 620\u2013634, 2022.\n[121] S. Zhou, Z. Guo, C. Aggarwal, X. Zhang, and S. Wang, \u201cLink prediction\non heterophilic graphs via disentangled representation learning,\u201d arXiv\npreprint arXiv:2208.01820, 2022.\n[122] A. Mele, \u201cA structural model of homophily and clustering in social\nnetworks,\u201d Journal of Business & Economic Statistics, vol. 40, no. 3,\npp. 1377\u20131389, 2022.\n[123] Y. Choi, J. Choi, T. Ko, H. Byun, and C.-K. Kim, \u201cFinding heterophilic\nneighbors via confidence-based subgraph matching for semi-supervised\nnode classification,\u201d in Proceedings of ACM International Conference\non Information & Knowledge Management, 2022, pp. 283\u2013292.\n[124] B.-M. Liu, Y.-L. Gao, F. Li, C.-H. Zheng, and J.-X. Liu, \u201cSLGCN:\nStructure-enhanced line graph convolutional network for predicting\ndrug-disease associations,\u201d Knowledge-Based Systems, vol. 283, p.\n111187, 2024.\n[125] S. Maekawa, K. Noda, Y. Sasaki et al., \u201cBeyond real-world benchmark\ndatasets: An empirical study of node classification with GNNs,\u201d in\nAdvances in Neural Information Processing Systems, vol. 35, 2022, pp.\n5562\u20135574.\n[126] H. Chen, Y. Lu, Y. Yang, and Y. Rao, \u201cA drug combination prediction\nframework based on graph convolutional network and heterogeneous\ninformation,\u201d IEEE/ACM Transactions on Computational Biology and\nBioinformatics, vol. 20, pp. 1917\u20131925, 2023.\n[127] \u2014\u2014, \u201cA drug combination prediction framework based on graph\nconvolutional network and heterogeneous information,\u201d IEEE/ACM\nTransactions on Computational Biology and Bioinformatics, 2022.\n[128] H. Kim, J. Choi, and J. J. Whang, \u201cDynamic relation-attentive graph\nneural networks for fraud detection,\u201d arXiv preprint arXiv:2310.04171,\n2023.\n[129] A. P. Garc\u00b4\u0131a-Plaza, V. Fresno, R. M. Unanue, and A. Zubiaga, \u201cUsing\nfuzzy logic to leverage html markup for web page representation,\u201d IEEE\nTransactions on Fuzzy Systems, vol. 25, no. 4, pp. 919\u2013933, 2016.\n[130] B. Rozemberczki, C. Allen, and R. Sarkar, \u201cMulti-scale attributed node\nembedding,\u201d Journal of Complex Networks, vol. 9, no. 2, p. cnab014,\n2021.\n[131] Y. Chen, H. Yang, Y. Zhang, K. Ma, T. Liu, B. Han, and J. Cheng,\n\u201cUnderstanding and improving graph injection attack by promoting un-\nnoticeability,\u201d in Proceedings of International Conference on Learning\nRepresentations, 2022.\n[132] M. Zou, Z. Gan, R. Cao, C. Guan, and S. Leng, \u201cSimilarity-navigated\ngraph neural networks for node classification,\u201d Information Sciences,\nvol. 633, pp. 41\u201369, 2023.\n[133] L. Wu, C. Tan, Z. Liu, Z. Gao, H. Lin, and S. Z. Li, \u201cLearning to\naugment graph structure for both homophily and heterophily graphs,\u201d\nin Proceedings of Joint European Conference on Machine Learning and\nKnowledge Discovery in Databases.\nSpringer, 2023, pp. 3\u201318.\n[134] X. Wu, H. Wu, R. Wang, D. Li, X. Zhou, and K. Lu, \u201cLeveraging free\nlabels to power up heterophilic graph learning in weakly-supervised\nsettings: An empirical study,\u201d in Proceedings of Joint European Con-\nference on Machine Learning and Knowledge Discovery in Databases.\nSpringer, 2023, pp. 140\u2013156.\n[135] O.\nPlatonov,\nD.\nKuznedelev,\nM.\nDiskin,\nA.\nBabenko,\nand\nL. Prokhorenkova, \u201cA critical look at the evaluation of GNNs under\nheterophily: Are we really making progress?\u201d in Proceedings of Inter-\nnational Conference on Learning Representations, 2022.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n21\nAPPENDIX A\nREAL-WORLD BENCHMARKS\nTo provide the data support for the development of heterophilic\ngraph learning, we collect and list current real-world benchmarks\nof heterophilic graph datasets with detailed statistics, as shown\nin Tables 5 - 7. All datasets can be divided in three categories:\nstandard datasets, large-scale datasets and high-quality datasets.\nA.1\nStandard Datasets\nMost current works conduct experiments for empirical evaluations\non the first 6 datasets in Table 5, which are originally presented\nby the work [34]. Specifically, Cornell, Texas, and Wisconsin are\nthree subdatasets from WebKB web-page dataset [129], where\nnodes are the web pages from computer science departments of\ndifferent universities, and edges are mutual links between pages.\nChameleon and Squirrel are two web page datasets collected from\nWikipedia [130], where nodes are web pages on specific topics\nand edges are hyperlinks between them. Actor is an actor co-\noccurrence network [1], where nodes are actors and edges mean\ntwo actors are co-occurred on the same Wikipedia page.\nA.2\nLarge-Scale Datasets\nSubsequently, a series of new benchmark datasets with larger\nscales from diverse areas are collected and released by [27]\nand [26], including a Wikipedia web page dataset (Wiki), two\ncitation network datasets (ArXiv-Year and Snap-Patents), five\nonline social network datasets (Penn94, Pokec, Genius, Deezer-\nEurope, and Twitch-Gamers), as well as a web-page hotel and\nrestaurant review dataset (YelpChi).\nA.3\nHigh-Quality Datasets\nRecent research [135] has highlighted significant issues with\nstandard datasets used to evaluate heterophily-specific models.\nThese issues make results obtained using such datasets unreliable.\nOne of the most notable drawbacks is the presence of a large\nnumber of duplicate nodes in datasets like squirrel and chameleon,\nwhich results in train-test data leakage. Removing these duplicate\nnodes has a strong impact on GNN performance in these datasets.\nTo address these challenges, a set of reliable heterophilic graphs\nwith diverse properties has been proposed. These graphs include\nRoman-Empire, Amazon-Ratings, Minesweeper, Tolokers, and\nQuestions. This effort aims to create a collection of trustworthy\nheterophilic benchmarks that can support and advance research\nand applications of GNNs on heterophilic graphs. It is expected\nthat more publicly available benchmarks for heterophily will be\ndeveloped to further facilitate research in this area.\nB\nOPEN-SOURCE IMPLEMENTATIONS\nWe have gathered the implementations of heterophilic GNN ap-\nproaches discussed in this survey if open-source code for these\nmethods is available. You can find the hyperlinks to the source\ncodes in Table 8.\nTABLE 5: Statistics of standard heterophilic graph benchmarks.\nTypes\nDatasets\n# Nodes\n# Edges\n# Features\n# Classes\nHnode\nHedge\nHclass\nWebKB Webpage\nCornell\n183\n295\n1,703\n5\n0.11\n0.30\n0.05\nTexas\n183\n309\n1,703\n5\n0.06\n0.11\n0.00\nWisconsin\n251\n499\n1,703\n5\n0.16\n0.21\n0.09\nAuthor Co-occurrence\nActor\n7,600\n33,544\n931\n5\n0.24\n0.22\n0.01\nWikipedia Webpage\nChameleon\n2,277\n36,101\n2,325\n5\n0.25\n0.23\n0.06\nSquirrel\n5,201\n217,073\n2,089\n5\n0.22\n0.22\n0.03\nTABLE 6: Statistics of large-scale heterophilic graph benchmarks.\nTypes\nDatasets\n# Nodes\n# Edges\n# Features\n# Classes\nHnode\nHedge\nHclass\nHadj\nLI\nWikipedia Webpage\nWiki\n1,925,342\n303,434,860\n600\n5\n-\n0.39\n-\n-\n-\nCitation\nArXiv-Year\n169,343\n1,166,243\n128\n5\n0.29\n0.22\n0.07\n0.01\n0.04\nSnap-Patents\n2,923,922\n13,975,788\n269\n5\n-\n0.07\n-\n-\n-\nSocial Networks\nDeezer-Europe\n28,281\n92,752\n31,241\n2\n0.53\n0.53\n0.03\n0.03\n0.00\nPenn94\n41,554\n1,362,229\n5\n2\n-\n0.47\n-\n-\n-\nTwitch-Gamers\n168,114\n6,797,557\n7\n2\n0.56\n0.55\n0.09\n0.09\n0.01\nGenius\n421,961\n984,979\n12\n2\n-\n0.62\n-\n-\n-\nPokec\n1,632,803\n30,622,564\n65\n2\n-\n0.45\n-\n-\n-\nWebpage Review\nYelpChi\n45,954\n3,846,979\n32\n2\n0.77\n0.77\n-\n-\n-\nTABLE 7: Statistics of high-quality heterophilic graph benchmarks.\nTypes\nDatasets\n# Nodes\n# Edges\n# Features\n# Classes\nHnode\nHedge\nHclass\nHadj\nLI\nWikipedia Webpage\nRoman-Empire\n22,662\n32,927\n300\n18\n0.05\n0.05\n0.02\n-0.05\n0.11\nProduct Co-Purchasing\nAmazon-Ratings\n24,492\n93,050\n300\n5\n0.38\n0.38\n0.13\n0.14\n0.04\nGame\nMinesweeper\n10,000\n39,402\n7\n2\n0.68\n0.68\n0.01\n0.01\n0.00\nSocial Networks\nTolokers\n11,758\n519,000\n10\n2\n0.63\n0.59\n0.18\n0.09\n0.01\nQuestions\n48,921\n153,540\n301\n2\n0.90\n0.84\n0.08\n0.02\n0.00\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n22\nTABLE 8: A summary of open-source implementations.\nMethod\nFramework\nGithub Link\nJK-Net (2018) [67]\nTensorFlow\nhttps://github.com/ShinKyuY/Representation Learning on Graphs with Jumping Knowledge Networks\nPyTorch\nhttps://github.com/xnuohz/JKNet-dgl\nMixHop (2019) [36]\nPyTorch\nhttps://github.com/samihaija/mixhop\nGCNII (2020) [76]\nPyTorch\nhttps://github.com/chennnM/GCNII\nGeom-GCN (2020) [34]\nPyTorch\nhttps://github.com/alexfanjn/GeomGCN PyG\nGPR-GNN (2020) [30]\nPyTorch\nhttps://github.com/jianhao2016/GPRGNN\nH2GCN (2020) [24]\nPyTorch\nhttps://github.com/GemsLab/H2GCN\nTDGNN (2021) [37]\nPyTorch\nhttps://github.com/Leo-Q-316/TDGNN\nSimP-GCN (2021) [41]\nPyTorch\nhttps://github.com/ChandlerBang/SimP-GCN\nU-GCN (2021) [40]\nPyTorch\nhttps://github.com/jindi-tju/U-GCN\nFAGCN (2021) [25]\nPyTorch\nhttps://github.com/bdy9527/FAGCN\nCPGNN (2021) [23]\nTensorFlow\nhttps://github.com/GemsLab/CPGNN\nCGCN (2021) [28]\nPyTorch\nhttps://github.com/Yujun-Yan/Heterophily and oversmoothing\nEvenNet (2022) [39]\nPyTorch\nhttps://github.com/Leirunlin/EvenNet\nDiag-NSD (2022) [46]\nPyTorch\nhttps://github.com/twitter-research/neural-sheaf-diffusion\nMPNN (2022) [68]\nPyTorch\nhttps://github.com/nhuang37/spectral-inspired-gnn\nGARNET (2022) [98]\nPyTorch\nhttps://github.com/cornell-zhang/GARNET\nLW-GNN (2022) [69]\nPyTorch\nhttps://github.com/EnyanDai/LWGCN\nGBK-GNN (2022) [51]\nPyTorch\nhttps://github.com/Xzh0u/GBK-GNN\nF2GNN (2022) [52]\nPyTorch\nhttps://github.com/LARS-research/F2GNN\nGloGNN (2022) [45]\nPyTorch\nhttps://github.com/RecklessRonan/GloGNN\nGIND (2022) [55]\nPyTorch\nhttps://github.com/7qchen/GIND\nBM-GCN (2022) [79]\nPyTorch\nhttps://github.com/hedongxiao-tju/BM-GCN\nDeformable GCN (2022) [80]\nPyTorch\nhttps://github.com/mlvlab/DeformableGCN\nGIA-HAO (2022) [131]\nPyTorch\nhttps://github.com/LFhase/GIA-HAO\nHeteRobust (2022) [23]\nPyTorch\nhttps://github.com/GemsLab/HeteRobust\nGREET (2023) [31]\nPyTorch\nhttps://github.com/yixinliu233/GREET\nHopGNN (2023) [49]\nPyTorch\nhttps://github.com/JC-202/HopGNN\nED-HNN (2023) [85]\nPyTorch\nhttps://github.com/Graph-COM/ED-HNN\nOrdered GNN (2023) [38]\nPyTorch\nhttps://github.com/LUMIA-Group/OrderedGNN\nGReTo (2023) [84]\nPyTorch\nhttps://github.com/zzyy0929/ICLR23-GReTo\nSNGNN (2023) [132]\nPyTorch\nhttps://github.com/MinhZou/SNGNN\nCAGNNs (2023) [70]\nPyTorch\nhttps://github.com/JC-202/CAGNN\nCGP (2023) [60]\nPyTorch\nhttps://github.com/LiuChuang0059/CGP\nRFA-GNN (2023) [61]\nPyTorch\nhttps://github.com/LirongWu/RFA-GNN\nHES-GSL (2023) [42]\nPyTorch\nhttps://github.com/LirongWu/Homophily-Enhanced-Self-supervision\nGDN (2023) [100]\nPyTorch\nhttps://github.com/blacksingular/wsdm GDN\nSE-GSL (2023) [47]\nPyTorch\nhttps://github.com/RingBDStack/SE-GSL\nGHRN (2023) [107]\nPyTorch\nhttps://github.com/blacksingular/GHRN\nAutoGCN (2023) [62]\nPyTorch\nhttps://github.com/nnzhan/AutoGCN\nGOAL (2023) [21]\nPyTorch\nhttps://github.com/zyzisastudyreallyhardguy/GOAL-Graph-Complementary-Learning\nL2A (2023) [133]\nPyTorch\nhttps://github.com/LirongWu/L2A\nGPRGNN-L (2023) [134]\nPyTorch\nhttps://github.com/lucio-win/PKDD2023\n2018\n2019\n JK-Net\nMixHop\n GPR-GNN\nTDGNN\nHOG-GNN\nOrdered \nGNN\n2020\n2021\n2022\n2023\n GCNII\nGeom-GCN\nH2GCN\nMMP\nSimP-GCN\nFAGCN \nCPGNN\nWRGNN\nNCGNN\nGBK-GNN\nNLGNN\nGOAL\nGOAT\nRFA-GNN\nGREET\nHigh-Order Neighbor Mixing\nHybrid Method\n \nPotential Neighbors Discovery\n \nIdentifiable Message Aggregation\n \nInter-Layer Combination\n \n \nFig. 12: A timeline of heterophilic GNNs development.\nC\nTIMELINE OF HETEROPHILY GNNS\nFig. 12 illustrates a timeline featuring some of the most notable\nheterophilic GNNs developed since 2018. Early works in the field\ninclude straightforward layer-inter combination methods like JK-\nNet [67], GPR-GNN [30], and GCNII [76]. Another category of\nearly works explores non-local neighbor mixing and discovery\nintuitively, exemplified by MixHop [36] and Geom-GCN [34].\nH2GCN [24] represents an early exploration into hybrid methods.\nIn 2021, the introduction of FAGCN [25] marked a signifi-\ncant development by incorporating adaptive edge-aware weighted\ntechniques, which arises the flourishing exploration of adaptive\nmessage aggregation. Subsequent in 2022 and 2023, witnessed\nthe integration of more advanced techniques with heterophilic\nGNNs, including dynamic routing (NCGNN [58]), self-supervised\nlearning (GREET [31]), and attention mechanisms with memory\n(MMP [54]).\n"
}