{
    "2310.06261": {
        "title": "Self-Discriminative Modeling for Anomalous Graph Detection",
        "abstract": "This paper studies the problem of detecting anomalous graphs using a machine\nlearning model trained on only normal graphs, which has many applications in\nmolecule, biology, and social network data analysis. We present a\nself-discriminative modeling framework for anomalous graph detection. The key\nidea, mathematically and numerically illustrated, is to learn a discriminator\n(classifier) from the given normal graphs together with pseudo-anomalous graphs\ngenerated by a model jointly trained, where we never use any true anomalous\ngraphs and we hope that the generated pseudo-anomalous graphs interpolate\nbetween normal ones and (real) anomalous ones. Under the framework, we provide\nthree algorithms with different computational efficiencies and stabilities for\nanomalous graph detection. The three algorithms are compared with several\nstate-of-the-art graph-level anomaly detection baselines on nine popular graph\ndatasets (four with small size and five with moderate size) and show\nsignificant improvement in terms of AUC. The success of our algorithms stems\nfrom the integration of the discriminative classifier and the well-posed\npseudo-anomalous graphs, which provide new insights for anomaly detection.\nMoreover, we investigate our algorithms for large-scale imbalanced graph\ndatasets. Surprisingly, our algorithms, though fully unsupervised, are able to\nsignificantly outperform supervised learning algorithms of anomalous graph\ndetection. The corresponding reason is also analyzed.",
        "date": "2023-10-10T02:08:09+00:00",
        "label": 1
    },
    "2402.12761": {
        "title": "FGAD: Self-boosted Knowledge Distillation for An Effective Federated Graph Anomaly Detection Framework",
        "abstract": "Graph anomaly detection (GAD) aims to identify anomalous graphs that\nsignificantly deviate from other ones, which has raised growing attention due\nto the broad existence and complexity of graph-structured data in many\nreal-world scenarios. However, existing GAD methods usually execute with\ncentralized training, which may lead to privacy leakage risk in some sensitive\ncases, thereby impeding collaboration among organizations seeking to\ncollectively develop robust GAD models. Although federated learning offers a\npromising solution, the prevalent non-IID problems and high communication costs\npresent significant challenges, particularly pronounced in collaborations with\ngraph data distributed among different participants. To tackle these\nchallenges, we propose an effective federated graph anomaly detection framework\n(FGAD). We first introduce an anomaly generator to perturb the normal graphs to\nbe anomalous, and train a powerful anomaly detector by distinguishing generated\nanomalous graphs from normal ones. Then, we leverage a student model to distill\nknowledge from the trained anomaly detector (teacher model), which aims to\nmaintain the personality of local models and alleviate the adverse impact of\nnon-IID problems. Moreover, we design an effective collaborative learning\nmechanism that facilitates the personalization preservation of local models and\nsignificantly reduces communication costs among clients. Empirical results of\nthe GAD tasks on non-IID graphs compared with state-of-the-art baselines\ndemonstrate the superiority and efficiency of the proposed FGAD method.",
        "date": "2024-02-20T07:03:59+00:00",
        "label": 1
    },
    "2401.13210": {
        "title": "Multitask Active Learning for Graph Anomaly Detection",
        "abstract": "In the web era, graph machine learning has been widely used on ubiquitous\ngraph-structured data. As a pivotal component for bolstering web security and\nenhancing the robustness of graph-based applications, the significance of graph\nanomaly detection is continually increasing. While Graph Neural Networks (GNNs)\nhave demonstrated efficacy in supervised and semi-supervised graph anomaly\ndetection, their performance is contingent upon the availability of sufficient\nground truth labels. The labor-intensive nature of identifying anomalies from\ncomplex graph structures poses a significant challenge in real-world\napplications. Despite that, the indirect supervision signals from other tasks\n(e.g., node classification) are relatively abundant. In this paper, we propose\na novel MultItask acTIve Graph Anomaly deTEction framework, namely MITIGATE.\nFirstly, by coupling node classification tasks, MITIGATE obtains the capability\nto detect out-of-distribution nodes without known anomalies. Secondly, MITIGATE\nquantifies the informativeness of nodes by the confidence difference across\ntasks, allowing samples with conflicting predictions to provide informative yet\nnot excessively challenging information for subsequent training. Finally, to\nenhance the likelihood of selecting representative nodes that are distant from\nknown patterns, MITIGATE adopts a masked aggregation mechanism for distance\nmeasurement, considering both inherent features of nodes and current labeled\nstatus. Empirical studies on four datasets demonstrate that MITIGATE\nsignificantly outperforms the state-of-the-art methods for anomaly detection.\nOur code is publicly available at: https://github.com/AhaChang/MITIGATE.",
        "date": "2024-01-24T03:43:45+00:00",
        "label": 1
    },
    "2405.17525": {
        "title": "SmoothGNN: Smoothing-based GNN for Unsupervised Node Anomaly Detection",
        "abstract": "The smoothing issue leads to indistinguishable node representations, which\nposes a significant challenge in the field of graph learning. However, this\nissue also presents an opportunity to reveal underlying properties behind\ndifferent types of nodes, which have been overlooked in previous studies.\nThrough empirical and theoretical analysis of real-world node anomaly detection\n(NAD) datasets, we observe that anomalous and normal nodes show different\npatterns in the smoothing process, which can be leveraged to enhance NAD tasks.\nMotivated by these findings, in this paper, we propose a novel unsupervised NAD\nframework. Specifically, according to our theoretical analysis, we design a\nSmoothing Learning Component. Subsequently, we introduce a Smoothing-aware\nSpectral Graph Neural Network, which establishes the connection between the\nspectral space of graphs and the smoothing process. Additionally, we\ndemonstrate that the Dirichlet Energy, which reflects the smoothness of a\ngraph, can serve as coefficients for node representations across different\ndimensions of the spectral space. Building upon these observations and\nanalyses, we devise a novel anomaly measure for the NAD task. Extensive\nexperiments on 9 real-world datasets show that SmoothGNN outperforms the best\nrival by an average of 14.66% in AUC and 7.28% in Precision, with 75x running\ntime speed-up, which validates the effectiveness and efficiency of our\nframework.",
        "date": "2024-05-27T14:23:30+00:00",
        "label": 1
    },
    "2310.02861": {
        "title": "Rayleigh Quotient Graph Neural Networks for Graph-level Anomaly Detection",
        "abstract": "Graph-level anomaly detection has gained significant attention as it finds\napplications in various domains, such as cancer diagnosis and enzyme\nprediction. However, existing methods fail to capture the spectral properties\nof graph anomalies, resulting in unexplainable framework design and\nunsatisfying performance. In this paper, we re-investigate the spectral\ndifferences between anomalous and normal graphs. Our main observation shows a\nsignificant disparity in the accumulated spectral energy between these two\nclasses. Moreover, we prove that the accumulated spectral energy of the graph\nsignal can be represented by its Rayleigh Quotient, indicating that the\nRayleigh Quotient is a driving factor behind the anomalous properties of\ngraphs. Motivated by this, we propose Rayleigh Quotient Graph Neural Network\n(RQGNN), the first spectral GNN that explores the inherent spectral features of\nanomalous graphs for graph-level anomaly detection. Specifically, we introduce\na novel framework with two components: the Rayleigh Quotient learning component\n(RQL) and Chebyshev Wavelet GNN with RQ-pooling (CWGNN-RQ). RQL explicitly\ncaptures the Rayleigh Quotient of graphs and CWGNN-RQ implicitly explores the\nspectral space of graphs. Extensive experiments on 10 real-world datasets show\nthat RQGNN outperforms the best rival by 6.74% in Macro-F1 score and 1.44% in\nAUC, demonstrating the effectiveness of our framework. Our code is available at\nhttps://github.com/xydong127/RQGNN.",
        "date": "2023-10-04T14:47:27+00:00",
        "label": 1
    },
    "2103.09430": {
        "title": "OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs",
        "abstract": "Enabling effective and efficient machine learning (ML) over large-scale graph\ndata (e.g., graphs with billions of edges) can have a great impact on both\nindustrial and scientific applications. However, existing efforts to advance\nlarge-scale graph ML have been largely limited by the lack of a suitable public\nbenchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of\nthree real-world datasets for facilitating the advancements in large-scale\ngraph ML. The OGB-LSC datasets are orders of magnitude larger than existing\nones, covering three core graph learning tasks -- link prediction, graph\nregression, and node classification. Furthermore, we provide dedicated baseline\nexperiments, scaling up expressive graph ML models to the massive datasets. We\nshow that expressive models significantly outperform simple scalable baselines,\nindicating an opportunity for dedicated efforts to further improve graph ML at\nscale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and\nattracted more than 500 team registrations globally, during which significant\nperformance improvements were made by a variety of innovative techniques. We\nsummarize the common techniques used by the winning solutions and highlight the\ncurrent best practices in large-scale graph ML. Finally, we describe how we\nhave updated the datasets after the KDD Cup to further facilitate research\nadvances. The OGB-LSC datasets, baseline code, and all the information about\nthe KDD Cup are available at https://ogb.stanford.edu/docs/lsc/ .",
        "date": "2021-03-17T04:08:03+00:00",
        "label": 1
    },
    "2210.12941": {
        "title": "Unsupervised Graph Outlier Detection: Problem Revisit, New Insight, and Superior Method",
        "abstract": "A large number of studies on Graph Outlier Detection (GOD) have emerged in\nrecent years due to its wide applications, in which Unsupervised Node Outlier\nDetection (UNOD) on attributed networks is an important area. UNOD focuses on\ndetecting two kinds of typical outliers in graphs: the structural outlier and\nthe contextual outlier. Most existing works conduct experiments based on\ndatasets with injected outliers. However, we find that the most widely-used\noutlier injection approach has a serious data leakage issue. By only utilizing\nsuch data leakage, a simple approach can achieve state-of-the-art performance\nin detecting outliers. In addition, we observe that existing algorithms have a\nperformance drop with the mitigated data leakage issue. The other major issue\nis on balanced detection performance between the two types of outliers, which\nhas not been considered by existing studies. In this paper, we analyze the\ncause of the data leakage issue in depth since the injection approach is a\nbuilding block to advance UNOD. Moreover, we devise a novel variance-based\nmodel to detect structural outliers, which outperforms existing algorithms\nsignificantly and is more robust at kinds of injection settings. On top of\nthis, we propose a new framework, Variance based Graph Outlier Detection\n(VGOD), which combines our variance-based model and attribute reconstruction\nmodel to detect outliers in a balanced way. Finally, we conduct extensive\nexperiments to demonstrate the effectiveness and efficiency of VGOD. The\nresults on 5 real-world datasets validate that VGOD achieves not only the best\nperformance in detecting outliers but also a balanced detection performance\nbetween structural and contextual outliers.",
        "date": "2022-10-24T04:09:35+00:00",
        "label": 1
    },
    "2308.10918": {
        "title": "Label-based Graph Augmentation with Metapath for Graph Anomaly Detection",
        "abstract": "Graph anomaly detection has attracted considerable attention from various\ndomain ranging from network security to finance in recent years. Due to the\nfact that labeling is very costly, existing methods are predominately developed\nin an unsupervised manner. However, the detected anomalies may be found out\nuninteresting instances due to the absence of prior knowledge regarding the\nanomalies looking for. This issue may be solved by using few labeled anomalies\nas prior knowledge. In real-world scenarios, we can easily obtain few labeled\nanomalies. Efficiently leveraging labelled anomalies as prior knowledge is\ncrucial for graph anomaly detection; however, this process remains challenging\ndue to the inherently limited number of anomalies available. To address the\nproblem, we propose a novel approach that leverages metapath to embed actual\nconnectivity patterns between anomalous and normal nodes. To further\nefficiently exploit context information from metapath-based anomaly subgraph,\nwe present a new framework, Metapath-based Graph Anomaly Detection (MGAD),\nincorporating GCN layers in both the dual-encoders and decoders to efficiently\npropagate context information between abnormal and normal nodes. Specifically,\nMGAD employs GNN-based graph autoencoder as its backbone network. Moreover,\ndual encoders capture the complex interactions and metapath-based context\ninformation between labeled and unlabeled nodes both globally and locally.\nThrough a comprehensive set of experiments conducted on seven real-world\nnetworks, this paper demonstrates the superiority of the MGAD method compared\nto state-of-the-art techniques. The code is available at\nhttps://github.com/missinghwan/MGAD.",
        "date": "2023-08-21T05:41:05+00:00",
        "label": 1
    },
    "1609.02907": {
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "abstract": "We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin.",
        "date": "2016-09-09T19:48:41+00:00",
        "label": 1
    },
    "2310.11829": {
        "title": "Towards Graph Foundation Models: A Survey and Beyond",
        "abstract": "Foundation models have emerged as critical components in a variety of\nartificial intelligence applications, and showcase significant success in\nnatural language processing and several other domains. Meanwhile, the field of\ngraph machine learning is witnessing a paradigm transition from shallow methods\nto more sophisticated deep learning approaches. The capabilities of foundation\nmodels to generalize and adapt motivate graph machine learning researchers to\ndiscuss the potential of developing a new graph learning paradigm. This\nparadigm envisions models that are pre-trained on extensive graph data and can\nbe adapted for various graph tasks. Despite this burgeoning interest, there is\na noticeable lack of clear definitions and systematic analyses pertaining to\nthis new domain. To this end, this article introduces the concept of Graph\nFoundation Models (GFMs), and offers an exhaustive explanation of their key\ncharacteristics and underlying technologies. We proceed to classify the\nexisting work related to GFMs into three distinct categories, based on their\ndependence on graph neural networks and large language models. In addition to\nproviding a thorough review of the current state of GFMs, this article also\noutlooks potential avenues for future research in this rapidly evolving domain.",
        "date": "2023-10-18T09:31:21+00:00",
        "label": 1
    },
    "2403.09039": {
        "title": "Detecting Anomalies in Dynamic Graphs via Memory enhanced Normality",
        "abstract": "Anomaly detection in dynamic graphs presents a significant challenge due to\nthe temporal evolution of graph structures and attributes. The conventional\napproaches that tackle this problem typically employ an unsupervised learning\nframework, capturing normality patterns with exclusive normal data during\ntraining and identifying deviations as anomalies during testing. However, these\nmethods face critical drawbacks: they either only depend on proxy tasks for\nrepresentation without directly pinpointing normal patterns, or they neglect to\ndifferentiate between spatial and temporal normality patterns. More recent\nmethods that use contrastive learning with negative sampling also face high\ncomputational costs, limiting their scalability to large graphs. To address\nthese challenges, we introduce a novel Spatial-Temporal memories-enhanced graph\nautoencoder (STRIPE). Initially, STRIPE employs Graph Neural Networks (GNNs)\nand gated temporal convolution layers to extract spatial and temporal features.\nThen STRIPE incorporates separate spatial and temporal memory networks to\ncapture and store prototypes of normal patterns, respectively. These stored\npatterns are retrieved and integrated with encoded graph embeddings through a\nmutual attention mechanism. Finally, the integrated features are fed into the\ndecoder to reconstruct the graph streams which serve as the proxy task for\nanomaly detection. This comprehensive approach not only minimizes\nreconstruction errors but also emphasizes the compactness and distinctiveness\nof the embeddings w.r.t. the nearest memory prototypes. Extensive experiments\non six benchmark datasets demonstrate the effectiveness and efficiency of\nSTRIPE, where STRIPE significantly outperforms existing methods with 5.8%\nimprovement in AUC scores and 4.62X faster in training time.",
        "date": "2024-03-14T02:26:10+00:00",
        "label": 1
    },
    "2312.17679": {
        "title": "Data Augmentation for Supervised Graph Outlier Detection with Latent Diffusion Models",
        "abstract": "Graph outlier detection is a prominent task of research and application in\nthe realm of graph neural networks. It identifies the outlier nodes that\nexhibit deviation from the majority in the graph. One of the fundamental\nchallenges confronting supervised graph outlier detection algorithms is the\nprevalent issue of class imbalance, where the scarcity of outlier instances\ncompared to normal instances often results in suboptimal performance.\nConventional methods mitigate the imbalance by reweighting instances in the\nestimation of the loss function, assigning higher weights to outliers and lower\nweights to inliers. Nonetheless, these strategies are prone to overfitting and\nunderfitting, respectively. Recently, generative models, especially diffusion\nmodels, have demonstrated their efficacy in synthesizing high-fidelity images.\nDespite their extraordinary generation quality, their potential in data\naugmentation for supervised graph outlier detection remains largely\nunderexplored.\n  To bridge this gap, we introduce GODM, a novel data augmentation for\nmitigating class imbalance in supervised Graph Outlier detection with latent\nDiffusion Models. Specifically, our proposed method consists of three key\ncomponents: (1) Variantioanl Encoder maps the heterogeneous information\ninherent within the graph data into a unified latent space. (2) Graph Generator\nsynthesizes graph data that are statistically similar to real outliers from\nlatent space, and (3) Latent Diffusion Model learns the latent space\ndistribution of real organic data by iterative denoising. Extensive experiments\nconducted on multiple datasets substantiate the effectiveness and efficiency of\nGODM. The case study further demonstrated the generation quality of our\nsynthetic data. To foster accessibility and reproducibility, we encapsulate\nGODM into a plug-and-play package and release it at the Python Package Index\n(PyPI).",
        "date": "2023-12-29T16:50:40+00:00",
        "label": 1
    },
    "2405.16771": {
        "title": "ARC: A Generalist Graph Anomaly Detector with In-Context Learning",
        "abstract": "Graph anomaly detection (GAD), which aims to identify abnormal nodes that\ndiffer from the majority within a graph, has garnered significant attention.\nHowever, current GAD methods necessitate training specific to each dataset,\nresulting in high training costs, substantial data requirements, and limited\ngeneralizability when being applied to new datasets and domains. To address\nthese limitations, this paper proposes ARC, a generalist GAD approach that\nenables a ``one-for-all'' GAD model to detect anomalies across various graph\ndatasets on-the-fly. Equipped with in-context learning, ARC can directly\nextract dataset-specific patterns from the target dataset using few-shot normal\nsamples at the inference stage, without the need for retraining or fine-tuning\non the target dataset. ARC comprises three components that are well-crafted for\ncapturing universal graph anomaly patterns: 1) smoothness-based feature\nAlignment module that unifies the features of different datasets into a common\nand anomaly-sensitive space; 2) ego-neighbor Residual graph encoder that learns\nabnormality-related node embeddings; and 3) cross-attentive in-Context anomaly\nscoring module that predicts node abnormality by leveraging few-shot normal\nsamples. Extensive experiments on multiple benchmark datasets from various\ndomains demonstrate the superior anomaly detection performance, efficiency, and\ngeneralizability of ARC.",
        "date": "2024-05-27T02:42:33+00:00",
        "label": 1
    },
    "2308.13821": {
        "title": "A Survey of Imbalanced Learning on Graphs: Problems, Techniques, and Future Directions",
        "abstract": "Graphs represent interconnected structures prevalent in a myriad of\nreal-world scenarios. Effective graph analytics, such as graph learning\nmethods, enables users to gain profound insights from graph data, underpinning\nvarious tasks including node classification and link prediction. However, these\nmethods often suffer from data imbalance, a common issue in graph data where\ncertain segments possess abundant data while others are scarce, thereby leading\nto biased learning outcomes. This necessitates the emerging field of imbalanced\nlearning on graphs, which aims to correct these data distribution skews for\nmore accurate and representative learning outcomes. In this survey, we embark\non a comprehensive review of the literature on imbalanced learning on graphs.\nWe begin by providing a definitive understanding of the concept and related\nterminologies, establishing a strong foundational understanding for readers.\nFollowing this, we propose two comprehensive taxonomies: (1) the problem\ntaxonomy, which describes the forms of imbalance we consider, the associated\ntasks, and potential solutions; (2) the technique taxonomy, which details key\nstrategies for addressing these imbalances, and aids readers in their method\nselection process. Finally, we suggest prospective future directions for both\nproblems and techniques within the sphere of imbalanced learning on graphs,\nfostering further innovation in this critical area.",
        "date": "2023-08-26T09:11:44+00:00",
        "label": 1
    },
    "2308.14181": {
        "title": "Class-Imbalanced Graph Learning without Class Rebalancing",
        "abstract": "Class imbalance is prevalent in real-world node classification tasks and\nposes great challenges for graph learning models. Most existing studies are\nrooted in a class-rebalancing (CR) perspective and address class imbalance with\nclass-wise reweighting or resampling. In this work, we approach the root cause\nof class-imbalance bias from an topological paradigm. Specifically, we\ntheoretically reveal two fundamental phenomena in the graph topology that\ngreatly exacerbate the predictive bias stemming from class imbalance. On this\nbasis, we devise a lightweight topological augmentation framework BAT to\nmitigate the class-imbalance bias without class rebalancing. Being orthogonal\nto CR, BAT can function as an efficient plug-and-play module that can be\nseamlessly combined with and significantly boost existing CR techniques.\nSystematic experiments on real-world imbalanced graph learning tasks show that\nBAT can deliver up to 46.27% performance gain and up to 72.74% bias reduction\nover existing techniques. Code, examples, and documentations are available at\nhttps://github.com/ZhiningLiu1998/BAT.",
        "date": "2023-08-27T19:01:29+00:00",
        "label": 1
    },
    "2212.05478": {
        "title": "Mul-GAD: a semi-supervised graph anomaly detection framework via aggregating multi-view information",
        "abstract": "Anomaly detection is defined as discovering patterns that do not conform to\nthe expected behavior. Previously, anomaly detection was mostly conducted using\ntraditional shallow learning techniques, but with little improvement. As the\nemergence of graph neural networks (GNN), graph anomaly detection has been\ngreatly developed. However, recent studies have shown that GNN-based methods\nencounter challenge, in that no graph anomaly detection algorithm can perform\ngeneralization on most datasets. To bridge the tap, we propose a multi-view\nfusion approach for graph anomaly detection (Mul-GAD). The view-level fusion\ncaptures the extent of significance between different views, while the\nfeature-level fusion makes full use of complementary information. We\ntheoretically and experimentally elaborate the effectiveness of the fusion\nstrategies. For a more comprehensive conclusion, we further investigate the\neffect of the objective function and the number of fused views on detection\nperformance. Exploiting these findings, our Mul-GAD is proposed equipped with\nfusion strategies and the well-performed objective function. Compared with\nother state-of-the-art detection methods, we achieve a better detection\nperformance and generalization in most scenarios via a series of experiments\nconducted on Pubmed, Amazon Computer, Amazon Photo, Weibo and Books. Our code\nis available at https://github.com/liuyishoua/Mul-Graph-Fusion.",
        "date": "2022-12-11T11:34:34+00:00",
        "label": 1
    },
    "2305.02496": {
        "title": "Revisiting Graph Contrastive Learning for Anomaly Detection",
        "abstract": "Combining Graph neural networks (GNNs) with contrastive learning for anomaly\ndetection has drawn rising attention recently. Existing graph contrastive\nanomaly detection (GCAD) methods have primarily focused on improving detection\ncapability through graph augmentation and multi-scale contrast modules.\nHowever, the underlying mechanisms of how these modules work have not been\nfully explored. We dive into the multi-scale and graph augmentation mechanism\nand observed that multi-scale contrast modules do not enhance the expression,\nwhile the multi-GNN modules are the hidden contributors. Previous studies have\ntended to attribute the benefits brought by multi-GNN to the multi-scale\nmodules. In the paper, we delve into the misconception and propose Multi-GNN\nand Augmented Graph contrastive framework MAG, which unified the existing GCAD\nmethods in the contrastive self-supervised perspective. We extracted two\nvariants from the MAG framework, L-MAG and M-MAG. The L-MAG is the lightweight\ninstance of the MAG, which outperform the state-of-the-art on Cora and Pubmed\nwith the low computational cost. The variant M-MAG equipped with multi-GNN\nmodules further improve the detection performance. Our study sheds light on the\ndrawback of the existing GCAD methods and demonstrates the potential of\nmulti-GNN and graph augmentation modules. Our code is available at\nhttps://github.com/liuyishoua/MAG-Framework.",
        "date": "2023-05-04T01:57:07+00:00",
        "label": 1
    },
    "2310.16376": {
        "title": "GADY: Unsupervised Anomaly Detection on Dynamic Graphs",
        "abstract": "Anomaly detection on dynamic graphs refers to detecting entities whose\nbehaviors obviously deviate from the norms observed within graphs and their\ntemporal information. This field has drawn increasing attention due to its\napplication in finance, network security, social networks, and more. However,\nexisting methods face two challenges: dynamic structure constructing challenge\n- difficulties in capturing graph structure with complex time information and\nnegative sampling challenge - unable to construct excellent negative samples\nfor unsupervised learning. To address these challenges, we propose Unsupervised\nGenerative Anomaly Detection on Dynamic Graphs (GADY). To tackle the first\nchallenge, we propose a continuous dynamic graph model to capture the\nfine-grained information, which breaks the limit of existing discrete methods.\nSpecifically, we employ a message-passing framework combined with positional\nfeatures to get edge embeddings, which are decoded to identify anomalies. For\nthe second challenge, we pioneer the use of Generative Adversarial Networks to\ngenerate negative interactions. Moreover, we design a loss function to alter\nthe training goal of the generator while ensuring the diversity and quality of\ngenerated samples. Extensive experiments demonstrate that our proposed GADY\nsignificantly outperforms the previous state-of-the-art method on three\nreal-world datasets. Supplementary experiments further validate the\neffectiveness of our model design and the necessity of each module.",
        "date": "2023-10-25T05:27:45+00:00",
        "label": 1
    },
    "2310.11676": {
        "title": "PREM: A Simple Yet Effective Approach for Node-Level Graph Anomaly Detection",
        "abstract": "Node-level graph anomaly detection (GAD) plays a critical role in identifying\nanomalous nodes from graph-structured data in various domains such as medicine,\nsocial networks, and e-commerce. However, challenges have arisen due to the\ndiversity of anomalies and the dearth of labeled data. Existing methodologies -\nreconstruction-based and contrastive learning - while effective, often suffer\nfrom efficiency issues, stemming from their complex objectives and elaborate\nmodules. To improve the efficiency of GAD, we introduce a simple method termed\nPREprocessing and Matching (PREM for short). Our approach streamlines GAD,\nreducing time and memory consumption while maintaining powerful anomaly\ndetection capabilities. Comprising two modules - a pre-processing module and an\nego-neighbor matching module - PREM eliminates the necessity for\nmessage-passing propagation during training, and employs a simple contrastive\nloss, leading to considerable reductions in training time and memory usage.\nMoreover, through rigorous evaluations of five real-world datasets, our method\ndemonstrated robustness and effectiveness. Notably, when validated on the ACM\ndataset, PREM achieved a 5% improvement in AUC, a 9-fold increase in training\nspeed, and sharply reduce memory usage compared to the most efficient baseline.",
        "date": "2023-10-18T02:59:57+00:00",
        "label": 1
    },
    "2402.11887": {
        "title": "Generative Semi-supervised Graph Anomaly Detection",
        "abstract": "This work considers a practical semi-supervised graph anomaly detection (GAD)\nscenario, where part of the nodes in a graph are known to be normal,\ncontrasting to the extensively explored unsupervised setting with a fully\nunlabeled graph. We reveal that having access to the normal nodes, even just a\nsmall percentage of normal nodes, helps enhance the detection performance of\nexisting unsupervised GAD methods when they are adapted to the semi-supervised\nsetting. However, their utilization of these normal nodes is limited. In this\npaper, we propose a novel Generative GAD approach (namely GGAD) for the\nsemi-supervised scenario to better exploit the normal nodes. The key idea is to\ngenerate pseudo anomaly nodes, referred to as 'outlier nodes', for providing\neffective negative node samples in training a discriminative one-class\nclassifier. The main challenge here lies in the lack of ground truth\ninformation about real anomaly nodes. To address this challenge, GGAD is\ndesigned to leverage two important priors about the anomaly nodes -- asymmetric\nlocal affinity and egocentric closeness -- to generate reliable outlier nodes\nthat assimilate anomaly nodes in both graph structure and feature\nrepresentations. Comprehensive experiments on six real-world GAD datasets are\nperformed to establish a benchmark for semi-supervised GAD and show that GGAD\nsubstantially outperforms state-of-the-art unsupervised and semi-supervised GAD\nmethods with varying numbers of training normal nodes. Code will be made\navailable at https://github.com/mala-lab/GGAD.",
        "date": "2024-02-19T06:55:50+00:00",
        "label": 1
    },
    "2205.13845": {
        "title": "Raising the Bar in Graph-level Anomaly Detection",
        "abstract": "Graph-level anomaly detection has become a critical topic in diverse areas,\nsuch as financial fraud detection and detecting anomalous activities in social\nnetworks. While most research has focused on anomaly detection for visual data\nsuch as images, where high detection accuracies have been obtained, existing\ndeep learning approaches for graphs currently show considerably worse\nperformance. This paper raises the bar on graph-level anomaly detection, i.e.,\nthe task of detecting abnormal graphs in a set of graphs. By drawing on ideas\nfrom self-supervised learning and transformation learning, we present a new\ndeep learning approach that significantly improves existing deep one-class\napproaches by fixing some of their known problems, including hypersphere\ncollapse and performance flip. Experiments on nine real-world data sets\ninvolving nine techniques reveal that our method achieves an average\nperformance improvement of 11.8% AUC compared to the best existing approach.",
        "date": "2022-05-27T09:17:57+00:00",
        "label": 1
    },
    "2402.16024": {
        "title": "HiGPT: Heterogeneous Graph Language Model",
        "abstract": "Heterogeneous graph learning aims to capture complex relationships and\ndiverse relational semantics among entities in a heterogeneous graph to obtain\nmeaningful representations for nodes and edges. Recent advancements in\nheterogeneous graph neural networks (HGNNs) have achieved state-of-the-art\nperformance by considering relation heterogeneity and using specialized message\nfunctions and aggregation rules. However, existing frameworks for heterogeneous\ngraph learning have limitations in generalizing across diverse heterogeneous\ngraph datasets. Most of these frameworks follow the \"pre-train\" and \"fine-tune\"\nparadigm on the same dataset, which restricts their capacity to adapt to new\nand unseen data. This raises the question: \"Can we generalize heterogeneous\ngraph models to be well-adapted to diverse downstream learning tasks with\ndistribution shifts in both node token sets and relation type heterogeneity?''\nTo tackle those challenges, we propose HiGPT, a general large graph model with\nHeterogeneous graph instruction-tuning paradigm. Our framework enables learning\nfrom arbitrary heterogeneous graphs without the need for any fine-tuning\nprocess from downstream datasets. To handle distribution shifts in\nheterogeneity, we introduce an in-context heterogeneous graph tokenizer that\ncaptures semantic relationships in different heterogeneous graphs, facilitating\nmodel adaptation. We incorporate a large corpus of heterogeneity-aware graph\ninstructions into our HiGPT, enabling the model to effectively comprehend\ncomplex relation heterogeneity and distinguish between various types of graph\ntokens. Furthermore, we introduce the Mixture-of-Thought (MoT) instruction\naugmentation paradigm to mitigate data scarcity by generating diverse and\ninformative instructions. Through comprehensive evaluations, our proposed\nframework demonstrates exceptional performance in terms of generalization\nperformance.",
        "date": "2024-02-25T08:07:22+00:00",
        "label": 1
    },
    "2306.12251": {
        "title": "GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection",
        "abstract": "With a long history of traditional Graph Anomaly Detection (GAD) algorithms\nand recently popular Graph Neural Networks (GNNs), it is still not clear (1)\nhow they perform under a standard comprehensive setting, (2) whether GNNs can\noutperform traditional algorithms such as tree ensembles, and (3) how about\ntheir efficiency on large-scale graphs. In response, we introduce GADBench -- a\nbenchmark tool dedicated to supervised anomalous node detection in static\ngraphs. GADBench facilitates a detailed comparison across 29 distinct models on\nten real-world GAD datasets, encompassing thousands to millions ($\\sim$6M)\nnodes. Our main finding is that tree ensembles with simple neighborhood\naggregation can outperform the latest GNNs tailored for the GAD task. We shed\nlight on the current progress of GAD, setting a robust groundwork for\nsubsequent investigations in this domain. GADBench is open-sourced at\nhttps://github.com/squareRoot3/GADBench.",
        "date": "2023-06-21T13:16:10+00:00",
        "label": 1
    },
    "2305.13573": {
        "title": "SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs",
        "abstract": "Anomaly detection aims to distinguish abnormal instances that deviate\nsignificantly from the majority of benign ones. As instances that appear in the\nreal world are naturally connected and can be represented with graphs, graph\nneural networks become increasingly popular in tackling the anomaly detection\nproblem. Despite the promising results, research on anomaly detection has\nalmost exclusively focused on static graphs while the mining of anomalous\npatterns from dynamic graphs is rarely studied but has significant application\nvalue. In addition, anomaly detection is typically tackled from semi-supervised\nperspectives due to the lack of sufficient labeled data. However, most proposed\nmethods are limited to merely exploiting labeled data, leaving a large number\nof unlabeled samples unexplored. In this work, we present semi-supervised\nanomaly detection (SAD), an end-to-end framework for anomaly detection on\ndynamic graphs. By a combination of a time-equipped memory bank and a\npseudo-label contrastive learning module, SAD is able to fully exploit the\npotential of large unlabeled samples and uncover underlying anomalies on\nevolving graph streams. Extensive experiments on four real-world datasets\ndemonstrate that SAD efficiently discovers anomalies from dynamic graphs and\noutperforms existing advanced methods even when provided with only little\nlabeled data.",
        "date": "2023-05-23T01:05:34+00:00",
        "label": 1
    },
    "1710.10903": {
        "title": "Graph Attention Networks",
        "abstract": "We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training).",
        "date": "2017-10-30T12:41:12+00:00",
        "label": 1
    },
    "1809.10341": {
        "title": "Deep Graph Infomax",
        "abstract": "We present Deep Graph Infomax (DGI), a general approach for learning node\nrepresentations within graph-structured data in an unsupervised manner. DGI\nrelies on maximizing mutual information between patch representations and\ncorresponding high-level summaries of graphs---both derived using established\ngraph convolutional network architectures. The learnt patch representations\nsummarize subgraphs centered around nodes of interest, and can thus be reused\nfor downstream node-wise learning tasks. In contrast to most prior approaches\nto unsupervised learning with GCNs, DGI does not rely on random walk\nobjectives, and is readily applicable to both transductive and inductive\nlearning setups. We demonstrate competitive performance on a variety of node\nclassification benchmarks, which at times even exceeds the performance of\nsupervised learning.",
        "date": "2018-09-27T04:53:24+00:00",
        "label": 1
    },
    "2311.06835": {
        "title": "Open-Set Graph Anomaly Detection via Normal Structure Regularisation",
        "abstract": "This paper considers an important Graph Anomaly Detection (GAD) task, namely\nopen-set GAD, which aims to train a detection model using a small number of\nnormal and anomaly nodes (referred to as seen anomalies) to detect both seen\nanomalies and unseen anomalies (i.e., anomalies that cannot be illustrated the\ntraining anomalies). The availability of those labelled training data provides\ncrucial prior knowledge about abnormalities for GAD models, enabling\nsubstantially reduced detection errors. However, current methods tend to\nover-emphasise fitting the seen anomalies, leading to a weak generalisation\nability to detect the unseen anomalies. Further, they were introduced to handle\nEuclidean data, failing to effectively capture important information on graph\nstructure and node attributes for GAD. In this work, we propose a novel\nopen-set GAD approach, namely Normal Structure Regularisation (NSReg) to\nachieve generalised detection ability to unseen anomalies, while maintaining\nits effectiveness on detecting seen anomalies. The key idea in NSReg is to\nintroduce a regularisation term that enforces the learning of compact,\nsemantically-rich representations of normal nodes based on their structural\nrelations to other nodes. When being optimised with supervised anomaly\ndetection losses, the regularisation term helps incorporate strong normality\ninto the modelling, and thus, it effectively avoids the overfitting the seen\nanomalies solely. In doing so, it helps learn better normality decision\nboundary, reducing the errors of detecting unseen anomalies as normal.\nExtensive empirical results on seven real-world datasets show the superiority\nof NSReg for open-set GAD.",
        "date": "2023-11-12T13:25:28+00:00",
        "label": 1
    },
    "2403.01121": {
        "title": "OpenGraph: Towards Open Graph Foundation Models",
        "abstract": "Graph learning has become indispensable for interpreting and harnessing\nrelational data in diverse fields, ranging from recommendation systems to\nsocial network analysis. In this context, a variety of GNNs have emerged as\npromising methodologies for encoding the structural information of graphs. By\neffectively capturing the graph's underlying structure, these GNNs have shown\ngreat potential in enhancing performance in graph learning tasks, such as link\nprediction and node classification. However, despite their successes, a\nsignificant challenge persists: these advanced methods often face difficulties\nin generalizing to unseen graph data that significantly differs from the\ntraining instances. In this work, our aim is to advance the graph learning\nparadigm by developing a general graph foundation model. This model is designed\nto understand the complex topological patterns present in diverse graph data,\nenabling it to excel in zero-shot graph learning tasks across different\ndownstream datasets. To achieve this goal, we address several key technical\nchallenges in our OpenGraph model. Firstly, we propose a unified graph\ntokenizer to adapt our graph model to generalize well on unseen graph data,\neven when the underlying graph properties differ significantly from those\nencountered during training. Secondly, we develop a scalable graph transformer\nas the foundational encoder, which effectively captures node-wise dependencies\nwithin the global topological context. Thirdly, we introduce a data\naugmentation mechanism enhanced by a LLM to alleviate the limitations of data\nscarcity in real-world scenarios. Extensive experiments validate the\neffectiveness of our framework. By adapting our OpenGraph to new graph\ncharacteristics and comprehending the nuances of diverse graphs, our approach\nachieves remarkable zero-shot graph learning performance across various\nsettings and domains.",
        "date": "2024-03-02T08:05:03+00:00",
        "label": 1
    },
    "2311.10370": {
        "title": "Few-shot Message-Enhanced Contrastive Learning for Graph Anomaly Detection",
        "abstract": "Graph anomaly detection plays a crucial role in identifying exceptional\ninstances in graph data that deviate significantly from the majority. It has\ngained substantial attention in various domains of information security,\nincluding network intrusion, financial fraud, and malicious comments, et al.\nExisting methods are primarily developed in an unsupervised manner due to the\nchallenge in obtaining labeled data. For lack of guidance from prior knowledge\nin unsupervised manner, the identified anomalies may prove to be data noise or\nindividual data instances. In real-world scenarios, a limited batch of labeled\nanomalies can be captured, making it crucial to investigate the few-shot\nproblem in graph anomaly detection. Taking advantage of this potential, we\npropose a novel few-shot Graph Anomaly Detection model called FMGAD (Few-shot\nMessage-Enhanced Contrastive-based Graph Anomaly Detector). FMGAD leverages a\nself-supervised contrastive learning strategy within and across views to\ncapture intrinsic and transferable structural representations. Furthermore, we\npropose the Deep-GNN message-enhanced reconstruction module, which extensively\nexploits the few-shot label information and enables long-range propagation to\ndisseminate supervision signals to deeper unlabeled nodes. This module in turn\nassists in the training of self-supervised contrastive learning. Comprehensive\nexperimental results on six real-world datasets demonstrate that FMGAD can\nachieve better performance than other state-of-the-art methods, regardless of\nartificially injected anomalies or domain-organic anomalies.",
        "date": "2023-11-17T07:49:20+00:00",
        "label": 1
    },
    "2312.06441": {
        "title": "Revisiting Graph-Based Fraud Detection in Sight of Heterophily and Spectrum",
        "abstract": "Graph-based fraud detection (GFD) can be regarded as a challenging\nsemi-supervised node binary classification task. In recent years, Graph Neural\nNetworks (GNN) have been widely applied to GFD, characterizing the anomalous\npossibility of a node by aggregating neighbor information. However, fraud\ngraphs are inherently heterophilic, thus most of GNNs perform poorly due to\ntheir assumption of homophily. In addition, due to the existence of heterophily\nand class imbalance problem, the existing models do not fully utilize the\nprecious node label information. To address the above issues, this paper\nproposes a semi-supervised GNN-based fraud detector SEC-GFD. This detector\nincludes a hybrid filtering module and a local environmental constraint module,\nthe two modules are utilized to solve heterophily and label utilization problem\nrespectively. The first module starts from the perspective of the spectral\ndomain, and solves the heterophily problem to a certain extent. Specifically,\nit divides the spectrum into various mixed-frequency bands based on the\ncorrelation between spectrum energy distribution and heterophily. Then in order\nto make full use of the node label information, a local environmental\nconstraint module is adaptively designed. The comprehensive experimental\nresults on four real-world fraud detection datasets denote that SEC-GFD\noutperforms other competitive graph-based fraud detectors. We release our code\nat https://github.com/Sunxkissed/SEC-GFD.",
        "date": "2023-12-11T15:18:51+00:00",
        "label": 1
    },
    "1710.09412": {
        "title": "mixup: Beyond Empirical Risk Minimization",
        "abstract": "Large deep neural networks are powerful, but exhibit undesirable behaviors\nsuch as memorization and sensitivity to adversarial examples. In this work, we\npropose mixup, a simple learning principle to alleviate these issues. In\nessence, mixup trains a neural network on convex combinations of pairs of\nexamples and their labels. By doing so, mixup regularizes the neural network to\nfavor simple linear behavior in-between training examples. Our experiments on\nthe ImageNet-2012, CIFAR-10, CIFAR-100, Google commands and UCI datasets show\nthat mixup improves the generalization of state-of-the-art neural network\narchitectures. We also find that mixup reduces the memorization of corrupt\nlabels, increases the robustness to adversarial examples, and stabilizes the\ntraining of generative adversarial networks.",
        "date": "2017-10-25T18:30:49+00:00",
        "label": 1
    },
    "2205.04816": {
        "title": "Reconstruction Enhanced Multi-View Contrastive Learning for Anomaly Detection on Attributed Networks",
        "abstract": "Detecting abnormal nodes from attributed networks is of great importance in\nmany real applications, such as financial fraud detection and cyber security.\nThis task is challenging due to both the complex interactions between the\nanomalous nodes with other counterparts and their inconsistency in terms of\nattributes. This paper proposes a self-supervised learning framework that\njointly optimizes a multi-view contrastive learning-based module and an\nattribute reconstruction-based module to more accurately detect anomalies on\nattributed networks. Specifically, two contrastive learning views are firstly\nestablished, which allow the model to better encode rich local and global\ninformation related to the abnormality. Motivated by the attribute consistency\nprinciple between neighboring nodes, a masked autoencoder-based reconstruction\nmodule is also introduced to identify the nodes which have large reconstruction\nerrors, then are regarded as anomalies. Finally, the two complementary modules\nare integrated for more accurately detecting the anomalous nodes. Extensive\nexperiments conducted on five benchmark datasets show our model outperforms\ncurrent state-of-the-art models.",
        "date": "2022-05-10T11:35:32+00:00",
        "label": 1
    },
    "2403.10339": {
        "title": "Generation is better than Modification: Combating High Class Homophily Variance in Graph Anomaly Detection",
        "abstract": "Graph-based anomaly detection is currently an important research topic in the\nfield of graph neural networks (GNNs). We find that in graph anomaly detection,\nthe homophily distribution differences between different classes are\nsignificantly greater than those in homophilic and heterophilic graphs. For the\nfirst time, we introduce a new metric called Class Homophily Variance, which\nquantitatively describes this phenomenon. To mitigate its impact, we propose a\nnovel GNN model named Homophily Edge Generation Graph Neural Network (HedGe).\nPrevious works typically focused on pruning, selecting or connecting on\noriginal relationships, and we refer to these methods as modifications.\nDifferent from these works, our method emphasizes generating new relationships\nwith low class homophily variance, using the original relationships as an\nauxiliary. HedGe samples homophily adjacency matrices from scratch using a\nself-attention mechanism, and leverages nodes that are relevant in the feature\nspace but not directly connected in the original graph. Additionally, we modify\nthe loss function to punish the generation of unnecessary heterophilic edges by\nthe model. Extensive comparison experiments demonstrate that HedGe achieved the\nbest performance across multiple benchmark datasets, including anomaly\ndetection and edgeless node classification. The proposed model also improves\nthe robustness under the novel Heterophily Attack with increased class\nhomophily variance on other graph classification tasks.",
        "date": "2024-03-15T14:26:53+00:00",
        "label": 1
    },
    "2302.06430": {
        "title": "Deep Orthogonal Hypersphere Compression for Anomaly Detection",
        "abstract": "Many well-known and effective anomaly detection methods assume that a\nreasonable decision boundary has a hypersphere shape, which however is\ndifficult to obtain in practice and is not sufficiently compact, especially\nwhen the data are in high-dimensional spaces. In this paper, we first propose a\nnovel deep anomaly detection model that improves the original hypersphere\nlearning through an orthogonal projection layer, which ensures that the\ntraining data distribution is consistent with the hypersphere hypothesis,\nthereby increasing the true positive rate and decreasing the false negative\nrate. Moreover, we propose a bi-hypersphere compression method to obtain a\nhyperspherical shell that yields a more compact decision region than a\nhyperball, which is demonstrated theoretically and numerically. The proposed\nmethods are not confined to common datasets such as image and tabular data, but\nare also extended to a more challenging but promising scenario, graph-level\nanomaly detection, which learns graph representation with maximum mutual\ninformation between the substructure and global structure features while\nexploring orthogonal single- or bi-hypersphere anomaly decision boundaries. The\nnumerical and visualization results on benchmark datasets demonstrate the\nsuperiority of our methods in comparison to many baselines and state-of-the-art\nmethods.",
        "date": "2023-02-13T15:10:18+00:00",
        "label": 1
    },
    "2202.07082": {
        "title": "Graph Neural Networks for Graphs with Heterophily: A Survey",
        "abstract": "Recent years have witnessed fast developments of graph neural networks (GNNs)\nthat have benefited myriads of graph analytic tasks and applications. In\ngeneral, most GNNs depend on the homophily assumption that nodes belonging to\nthe same class are more likely to be connected. However, as a ubiquitous graph\nproperty in numerous real-world scenarios, heterophily, i.e., nodes with\ndifferent labels tend to be linked, significantly limits the performance of\ntailor-made homophilic GNNs. Hence, GNNs for heterophilic graphs are gaining\nincreasing research attention to enhance graph learning with heterophily. In\nthis paper, we provide a comprehensive review of GNNs for heterophilic graphs.\nSpecifically, we propose a systematic taxonomy that essentially governs\nexisting heterophilic GNN models, along with a general summary and detailed\nanalysis. Furthermore, we discuss the correlation between graph heterophily and\nvarious graph research domains, aiming to facilitate the development of more\neffective GNNs across a spectrum of practical applications and learning tasks\nin the graph research community. In the end, we point out the potential\ndirections to advance and stimulate more future research and applications on\nheterophilic graph learning with GNNs.",
        "date": "2022-02-14T23:07:47+00:00",
        "label": 1
    }
}