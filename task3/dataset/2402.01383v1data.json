{
    "id": "2402.01383v1",
    "title": "LLM-based NLG Evaluation: Current Status and Challenges",
    "abstract": "Evaluating natural language generation (NLG) is a vital but challenging\nproblem in artificial intelligence. Traditional evaluation metrics mainly\ncapturing content (e.g. n-gram) overlap between system outputs and references\nare far from satisfactory, and large language models (LLMs) such as ChatGPT\nhave demonstrated great potential in NLG evaluation in recent years. Various\nautomatic evaluation methods based on LLMs have been proposed, including\nmetrics derived from LLMs, prompting LLMs, and fine-tuning LLMs with labeled\nevaluation data. In this survey, we first give a taxonomy of LLM-based NLG\nevaluation methods, and discuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evaluation. Lastly, we discuss several\nopen problems in this area and point out future research directions.",
    "date": "2024-02-02T13:06:35+00:00",
    "fulltext": "LLM-based NLG Evaluation: Current Status and Challenges\nMingqi Gao , Xinyu Hu , Jie Ruan , Xiao Pu ,\nXiaojun Wan\nPeking University\n{gaomingqi, huxinyu, wanxiaojun}@pku.edu.cn, {ruanjie, puxiao}@stu.pku.edu.cn\nAbstract\nEvaluating natural language generation (NLG) is\na vital but challenging problem in artificial intel-\nligence. Traditional evaluation metrics mainly cap-\nturing content (e.g. n-gram) overlap between sys-\ntem outputs and references are far from satisfactory,\nand large language models (LLMs) such as Chat-\nGPT have demonstrated great potential in NLG\nevaluation in recent years. Various automatic evalu-\nation methods based on LLMs have been proposed,\nincluding metrics derived from LLMs, prompting\nLLMs, and fine-tuning LLMs with labeled evalu-\nation data. In this survey, we first give a taxon-\nomy of LLM-based NLG evaluation methods, and\ndiscuss their pros and cons, respectively. We also\ndiscuss human-LLM collaboration for NLG evalu-\nation. Lastly, we discuss several open problems in\nthis area and point out future research directions.\n1\nIntroduction\nEvaluating natural language generation (NLG) is a key but\nchallenging issue. Traditional evaluation metrics like BLEU\nand ROUGE rely on the n-gram overlap between model out-\nputs and references to measure its quality. They have been\ncriticized for low correlation with human judgments [Sulem\net al., 2018], as surface-level matching cannot reliably evalu-\nate text. After the rise of deep learning, model-based eval-\nuation metrics like BERTScore [Zhang et al., 2020] and\nBARTScore [Yuan et al., 2021] have been continuously pro-\nposed and gradually adopted to evaluate the overall quality\nor various specific aspects of generated outputs (e.g., fluency,\ncoherence, coverage, faithfulness, etc.). Although better than\ntraditional metrics, their performance is still not satisfactory,\nand their application scope is very limited.\nFor example,\nBERTScore is reference-based and cannot be used without\na reference. With the emergence of large language models\n(LLMs) like ChatGPT, they have achieved unprecedented ef-\nfectiveness in following instructions, understanding content,\nand generating text. This inspired researchers to use LLMs\nfor NLG evaluation. Although this is a research direction that\nonly emerged in 2023, the past year has seen an enormous\namount of research work. It is no exaggeration to say that\nLLM-derived Metrics\nEmbedding-based\nProbability-based\nFine-tuning\nSpecialized\nEvaluation LLM\nPrompt\nHuman\nFine-tuning LLMs\nHuman-LLM \nCollaborative\nEvaluation\nFine-tuning LLMs\nPrompting\nLLMs\nLLM\nFigure 1: Schematic representation of the four categories of LLM-\nbased NLG evaluation.\nNLG evaluation has been revolutionized by LLMs. This arti-\ncle will review the existing literature and provide suggestions\nfor future research in this field.\nThis article mainly focuses on research that uses language\nmodels with over one billion parameters for NLG evaluation,\nwith necessary references to earlier model-based evaluation\nmetrics like BERTScore. To maintain focus, other types of\ngeneration like code generation and tasks involving images\nare not included in the scope of this article. As shown in Fig-\nure 1, according to how we utilize LLMs for NLG evaluation,\nwe categorize the research work into four types:\n\u2022 LLM-derived metrics (\u00a72): developing/deriving evalu-\nation metrics from embeddings or generation probabili-\nties of LLMs.\n\u2022 Prompting LLMs (\u00a73):\ndirectly inquiring existing\nLLMs via designed prompts.\n\u2022 Fine-tuning LLMs (\u00a74): using labeled evaluation data\nto fine-tune existing LLMs and improving their NLG\nevaluation capabilities.\n\u2022 Human-LLM Collaborative Evaluation (\u00a75): leverag-\ning the distinctive strengths of both human evaluators\nand LLMs to achieve robust and nuanced evaluations\nin challenging domains through human-LLM collabora-\ntion.\narXiv:2402.01383v1  [cs.CL]  2 Feb 2024\nWe will review each type of evaluation methods and discuss\nthe pros and cons respectively. Lastly, we will discuss future\ndirections in this area (\u00a76).\n2\nLLM-derived Metrics\n2.1\nOverview\nEarly model-based NLG evaluation methods,\nsuch as\nBERTScore and BARTScore, were primarily motivated by\nthe capability of traditional pre-trained language models to\ngenerate high-quality texts. Recently, the advent of LLMs\nhas prompted some research to adapt these ideas to stronger\nLLMs. Their inherent, powerful linguistic abilities are de-\nrived for direct NLG evaluation, which is expected to result\nin better performance. Such works can be categorized into\ntwo types: embedding-based and probability-based methods.\n2.2\nEmbedding-based Metrics\nThe embedding-based methods, like BERTScore, generally\nutilize representations of language models and thus compute\nthe semantic similarity between the reference and the target\ntext to evaluate, with different possible ways of implemen-\ntation. Recent work [ES et al., 2023] similarly obtains em-\nbeddings for the target text using the text-embedding-ada-002\nmodel through the OpenAI API. The higher the correspond-\ning similarity, the closer the target text aligns with the relevant\nrequirements, indicating a higher quality.\n2.3\nProbability-based Metrics\nGPTScore [Fu et al., 2023a] establishes tailored evaluation\ntemplates for each aspect to effectively guide multiple LLMs\nfor NLG evaluation. The generation probability is calculated\nunder the condition of source input designed with customized\nprompts and evaluation aspects, which endows the GPTScore\nwith better flexibility in evaluation.\nAnd similar methods\nhave also been applied to the hallucination detection of the\nLLM-generated text in recent reseach with three different at-\ntempts for calculating the probability score.\nOn the other hand, some works leverage the variation in\nprobabilities under changed conditions as the evaluation met-\nric. FFLM [Jia et al., 2023] proposes to evaluate the faith-\nfulness of the target text by calculating a combination of\nprobability changes based on the intuition that the genera-\ntion probability of a given text segment increases when more\nconsistent information is provided, and vice versa. Similarly,\nDELTASCORE [Xie et al., 2023] measures the quality of dif-\nferent story aspects according to the likelihood difference be-\ntween pre- and post-perturbation states with LLMs including\nGPT-3.5 (text-davinci-003) that provide logits. They believe\nthat the sensitivity to specific perturbations indicates the qual-\nity of related aspects, and their experiments demonstrate the\neffectiveness of their approach.\n2.4\nPros and Cons\nTraditional NLG evaluation approaches always fall short due\nto their surface-form similarity when the target text and ref-\nerence convey the same meaning but use different expres-\nsions. In contrast, LLM-derived metrics offer a remedy for\nthe limitation and demonstrate stronger correlations with hu-\nman judgments benefiting from the evolving modeling tech-\nniques. However, the flaws within LLMs can lead to some\nissues, as introduced in the following:\nRobustness. Some research has investigated the robust-\nness of LLM-derived metrics and found that they lack ro-\nbustness in different attack scenarios. Specifically, [He et al.,\n2023b] develops a set of stress tests to assess the robustness\nof various model-based metrics on some common NLG tasks.\nThey show a catalogue of the blind spots and potential errors\nidentified that are not detected by each metric.\nEfficiency. Compared to traditional metrics, LLM-derived\nevaluation methods are more time-consuming and require\nmore computational resources, especially when adopting\nLLMs with quite large parameter scales.\nTo address this,\nsome lightweight learning approaches and fast LLM infer-\nence and serving tools like vLLM 1 have been proposed.\nHowever, closed-source LLMs often do not provide param-\neters, representations, or logits, thus making it impossible to\napply LLM-derived methods to them.\nFairness. [Sun et al., 2022] assesses the social bias across\nvarious metrics for NLG evaluation on six sensitive attributes:\nrace, gender, religion, physical appearance, age, and socioe-\nconomic status. Their findings reveal that model-based met-\nrics carry noticeably more social bias than traditional metrics.\nRelevant biases can be categorized into two types: intrinsic\nbias encoded within pre-trained language models and extrin-\nsic bias injected during the computation of similarity. There-\nfore, current LLM-derived methods may have similar issues.\n3\nPrompting LLMs\n3.1\nOverview\nLLMs have demonstrated unprecedented instruction under-\nstanding and text generation abilities, which broadens re-\nsearchers\u2019 imagination of automatic evaluation for NLG. For\na long time, human evaluation has been viewed as the gold\nstandard for NLG evaluation. Recently, some studies claim\nthat LLMs are on par with crowdsourcing annotators in sev-\neral tasks. So, can LLMs simulate or even be an alternative to\nhumans in the human evaluation of NLG? Or, do the practices\nin human evaluation or other evaluative tasks (e.g. competi-\ntion race, paper review, etc.) inspire us to better NLG evalua-\ntion with LLMs? A large body of research and attempts based\non prompting LLMs are guided by these ideas. In these stud-\nies, instructions and the text to be evaluated are completely\nexpressed in the prompt given to LLMs, and the evaluation\nresults are generated by it.\nHuman evaluation typically includes the following ele-\nments:\n\u2022 Evaluation Methods: The way the preferences of an-\nnotators are captured and recorded, such as scoring, and\ncomparison.\n\u2022 Task Instructions: How the annotators should read or\nmanipulate different parts to complete the annotation.\n1https://github.com/vllm-project/vllm\nYou will be given a news article. You will then be given one \nsummary written for this article. Your task is to rate the summary \non one metric. Please make sure you read and understand these \ninstructions carefully. \nEvaluation Criteria:\nConsistency (1-5) - the factual alignment between the summary and \nthe summarized source. A factually consistent summary contains only \nstatements that are entailed by the source document. Annotators \nwere also asked to penalize summaries that contained hallucinated \nfacts. \nEvaluation Steps:\n1. Read the news article carefully and identify the main facts and \ndetails it presents.\n2. Read the summary and compare it to the article. Check if the \nsummary contains any factual errors that are not supported by the \narticle.\n3. Assign a score for consistency based on the Evaluation Criteria.\nSource Text: \nPaul Merson has restarted his row with Andros Townsend after the \nTottenham midfielder was brought on with only seven minutes \nremaining in his team's 0-0 draw with Burnley on Sunday. \u2026 \nSummary: \nPaul Merson was brought on with only seven minutes remaining \u2026.\nEvaluation Form: Answer by starting with \"Rating:\" and then give \nthe explanation of the rating on the next line by \"Rationale:\"\n- Consistency: \nRating: 2\nRationale:  The summary incorrectly states that Andros Townsend \u2026.\nPrompt\nResponse\nFigure 2: An example of prompting LLMs to score summary faith-\nfulness. It can be seen that there are task instructions, evaluation\ncriteria, and input content in the prompt. The rating and explanation\nare generated by LLMs.\n\u2022 Input Content: The target text to be evaluated and\nother required content. Other required content including\nsource documents, references, and external knowledge\nis provided as needed.\n\u2022 Evaluation Criteria: Also known as aspect, the general\ndefinition of how good or bad the text to be evaluated\nis in a particular aspect of quality, e.g. fluency, faithful-\nness.\n\u2022 Role and interaction: The roles annotators play in the\nevaluation and the interactions between them.\nThe focus of the existing research can always be mapped\nanalogously to one or more of these elements, and we orga-\nnize them according to the elements they address. An exam-\nple of prompting LLMs is shown in Figure 2.\n3.2\nEvaluation Methods\n[Kocmi and Federmann, 2023b] discover GPT-3.5 and GPT-\n4 achieve the state-of-the-art accuracy of evaluating trans-\nlation quality compared to human labels, outperforming all\nthe results from the metric shard task of WMT22 [Freitag et\nal., 2022]. [Wang et al., 2023a] experiment on five datasets\nacross summarization, story generation, and data-to-text, and\nChatGPT evaluators with a rating scale from 1 to 5 or 1 to\n100 have the state-of-the-art or comparative correlations with\nhuman judgments in most settings, compared with prior met-\nrics. Similar conclusions are also observed in open-domain\ndialogue response generation [Lin and Chen, 2023]. Besides\nEnglish, [Mendonc\n\u00b8a et al., 2023] show that ChatGPT with\nsimple rating prompts is a strong evaluator for multilingual\ndialogue evaluation, surpassing prior metrics based on en-\ncoders.\nComparison. Different from absolute scoring, compari-\nson refers to choosing the better of the two. [Luo et al., 2023]\nuse ChatGPT to compare the factual consistency of two sum-\nmaries. AuPEL [Wang et al., 2023d] evaluate personalized\ntext generation from three aspects in the form of compari-\nson with the PaLM 2 family [Anil et al., 2023]. According\nto [Liusie et al., 2023], pairwise comparison is better than\nscoring when medium-sized LLMs (e.g. FlanT5 [Chung et\nal., 2022] and Llama2 [Touvron et al., 2023]) are adopted as\nevaluators.\nRanking. Ranking can be viewed as an extended form of\ncomparison. In comparison, only two examples are involved\nat a time, whereas in ranking, the order of more than two\nexamples needs to be decided at once. [Ji et al., 2023] use\nChatGPT to rank five model-generated responses across sev-\neral use cases at once, indicating the ranking preferences of\nChatGPT align with those of humans to some degree. Simi-\nlarly, GPTRank is a method to rank summaries in a list-wise\nmanner [Liu et al., 2023c]. Moreover, [Liu et al., 2023b]\ncompare different evaluation methods in LLM-based summa-\nrization including scoring, comparison, and ranking, showing\nthat the optimal evaluation method for each backbone LLM\nmay vary.\nBoolean QA. Boolean QA requires LLMs to answer \u201dYes\u201d\nor \u201dNo\u201d to a question. It is adopted more in scenarios where\nhuman annotations are binary, such as grammaticality [Hu et\nal., 2023], faithfulness of summaries and statements [Luo et\nal., 2023; ES et al., 2023; Hu et al., 2023], factuality of gen-\nerated text [Fu et al., 2023b], and answerability of generated\nquestions [Wang et al., 2023f].\nError Analysis. Error Analysis refers to the evaluation of\na text by looking for errors that occur in the text according to a\nset of predefined error categories. Multidimensional Quality\nMetrics (MQM) [Jain et al., 2023] is an error analysis frame-\nwork prevalent in machine translation evaluation. According\nto MQM, [Kocmi and Federmann, 2023a] use ChatGPT or\nGPT-4 to automatically detect translation quality error spans.\nBOOOOKSCORE [Chang et al., 2023], an LLM-based eval-\nuation metric, assesses the coherence of book summaries by\nidentifying eight types of errors.\n3.3\nTask Instructions\nIn the human evaluation of NLG, task instructions typically\nconsist of a general task description and more detailed eval-\nuation steps, akin to Chain-of-Thought. The demonstrations\nused in few-shot prompting are also included in this part.\nForm and requirements. Studies from Eval4NLP 2023\nhave tested the effects of styles and lengths of task instruction,\nand some have used LLMs to either generate or refine task\ninstructions [Leiter et al., 2023]. [He et al., 2023a] evaluate\ngenerative reasoning by asking LLMs to generate their own\nanswers first, and then conduct a quantitative analysis of the\ntext to be evaluated.\nAnalysis and explanations. LLMs can include analysis or\nexplanation in their evaluations, which is a key point that dis-\ntinguishes them from previous automatic evaluation metrics.\nEarly explorations into prompting LLMs for NLG evaluation\nmostly do not examine the impact of whether LLMs are re-\nquired to analyze and explain on evaluation result. Findings\nsuggest that explicit analysis or explanation instructions can\nbetter align LLM evaluations with human judgments [Chiang\nand Lee, 2023], though the quality of LLM-generated expla-\nnations still needs manual review.\nDemonstrations.\nSometimes in-context examples are\nneeded for prompting LLMs. Some studies use only these\nexamples even without additional instructions for prompting\n[Jain et al., 2023], while others observe no significant ben-\nefit from one-shot settings, compared to zero-shot settings\n[Kotonya et al., 2023]. Iteratively updating in-context ex-\namples has been shown to enhance LLM evaluators\u2019 perfor-\nmance [Hasanbeig et al., 2023].\n3.4\nInput Content\nThe types of input content mainly depend on the evalua-\ntion criteria and are relatively fixed. For most task-specific\ncriteria, like summary faithfulness [Luo et al., 2023], both\nthe source document and target text are needed. For task-\nindependent criteria, such as fluency [Hu et al., 2023; Chi-\nang and Lee, 2023], only the target text is required, though\nthe source document is often included [Wang et al., 2023a;\nLiusie et al., 2023]. Different tasks may require additional\ninput types, such as providing or omitting references in ma-\nchine translation evaluations [Kocmi and Federmann, 2023b].\nUniquely, some studies incorporate the output of other auto-\nmatic evaluation metrics into the input for LLMs [Shu et al.,\n2023].\n3.5\nEvaluation Criteria\nThe evaluation targeting specific aspects is used in numerous\nstudies of human evaluation for NLG, such as text summa-\nrization, story generation, dialogue, and text simplification.\nEvaluation criteria, i.e., the definitions of aspects are key in\nthis context. Most evaluation criteria in LLM-based evalua-\ntion are directly derived from human evaluation. However, a\nfew studies have attempted to let LLMs generate or improve\nevaluation criteria. [Liu et al., 2023e] use a few human-rated\nexamples as seeds to let LLMs draft some candidate evalu-\nation criteria, and then further filter them based on the per-\nformance of LLMs using these criteria on a validation set, to\nobtain the final evaluation criteria. [Kim et al., 2023b] de-\nsigned an LLM-based interactive evaluation system, which\ninvolves using LLMs to review the evaluation criteria pro-\nvided by users, including eliminating ambiguities in crite-\nria, merging criteria with overlapping meanings, and decom-\nposing overly broad criteria. Additionally, [Ye et al., 2023]\npropose a hierarchical aspect classification system with 12\nsubcategories, demonstrating that under the proposed fine-\ngrained aspect definitions, human evaluation and LLM-based\nevaluation are highly correlated. Additionally, the chain-of-\naspects approach improves LLMs\u2019 ability to evaluate on a\nspecific aspect by having LLMs score on some related aspects\nbefore generating the final score [Gong and Mao, 2023].\n3.6\nRole and Interaction\nWe include in this section the evaluation strategies that ei-\nther use the same LLMs in different ways or involve different\nLLMs. The former can be further divided into chain-style and\nnetwork-style interactions.\nChain-style interaction. Inspired by human evaluators,\n[Yuan et al., 2024] have LLMs score a batch of examples\nto be evaluated each time. Specifically, the evaluation pro-\ncess is divided into three stages: analysis, ranking, and scor-\ning. Similar to QA-based evaluation metrics [Durmus et al.,\n2020], [Fu et al., 2023b] assess the faithfulness of summaries\nin two stages: treating LLMs as question generators to gen-\nerate a question from the summary; then having LLMs an-\nswer the question using the source document. Differently,\nwhen [Hu et al., 2023] use GPT-4 to evaluate the faithfulness\nof summaries, it first asks GPT-4 to extract event units from\nthe summary, then verifies whether these event units meet the\nrequirements, and finally judges whether the event units are\nfaithful to the source document.\nNetwork-style interaction.\nUnlike chain-style interac-\ntions, network-style interactions involve the dispersion and\naggregation of information.\nIn network-style interactions,\nLLMs on the same layer play similar roles. ChatEval [Chan\net al., 2023] is a framework for evaluating content through\ndebates among multiple LLMs, with three communication\nstrategies designed among the three types of LLMs: One-\nBy-One, Simultaneous-Talk, and Simultaneous-Talk-with-\nSummarizer. [Zhang et al., 2023b] find that under certain\nconditions, widening and deepening the network of LLMs\ncan better align its evaluation with human judgments. [Saha\net al., 2023] propose a branch-solve-merge strategy, assign-\ning LLMs the roles of decomposing problems, solving them,\nand aggregating answers, thereby improving the accuracy and\nreliability of evaluations. [Wu et al., 2023] assume that dif-\nferent people such as politicians and the general public have\ndifferent concerns about the quality of news summaries, use\nLLMs to play different roles in evaluation accordingly, and\naggregate the results finally.\nDifferent LLMs. Different from having the same LLM\nplay different roles, some research has used different LLMs\n(such as GPT-4 and Claude) in their studies. In pairwise com-\nparisons, previous work mostly used a single LLM as the\nevaluator, which may not be fair. In light of this, [Bai et\nal., 2023] design a decentralized Peer-examination method,\nusing different LLMs as evaluators and then aggregating the\nresults. Further, [Li et al., 2023c] let different LLMs serve\nas evaluators in pairwise comparisons and then have them go\nthrough a round of discussion to reach the final result. Addi-\ntionally, [Cohen et al., 2023] evaluate the factuality of texts\nthrough the interaction of two LLMs, where the LLM that\ngenerated the text acts as the examinee and the other LLM as\nthe examiner.\n3.7\nPros and Cons\nThe benefits of prompting LLMs for NLG evaluation are ex-\nciting. First, for the first time, people can express evaluation\ncriteria and evaluation methods in natural language within the\nprompts given to LLMs, providing great flexibility. Where\npreviously people needed to design specific evaluation met-\nrics for different NLG tasks or even different aspects of a sin-\ngle task, now they only need to modify the prompts for LLMs.\nSecondly, surprisingly, LLMs have the ability to generate ex-\nplanations while assessing texts, making this approach some-\nwhat interpretable. Furthermore, in many NLG task, prompt-\ning LLMs for evaluation has achieved state-of-the-art corre-\nlations with human judgments.\nHowever, as many studies have pointed out, this type of\napproach still has many limitations. [Wang et al., 2023b]\nnote that when using ChatGPT and GPT-4 for pairwise com-\nparisons, the order of the two texts can affect the evaluation\nresults, which is known as position bias. To alleviate this\nissue, [Li et al., 2023d] propose a strategy of splitting, align-\ning, and then merging the two texts to be evaluated into the\nprompt. Also, LLM evaluators tend to favor longer, more ver-\nbose responses [Zheng et al., 2023] and responses generated\nby themselves [Liu et al., 2023a]. [Wu and Aji, 2023] show\nthat compared to answers that are too short or grammatically\nincorrect, answers with factual errors are considered better by\nLLMs. [Liu et al., 2023d] demonstrate through adversarial\nmeta-evaluation that LLMs without references are not suit-\nable for evaluating dialogue responses in closed-ended sce-\nnarios: they tend to score highly on responses that conflict\nwith the facts in the dialogue history. [Zhang et al., 2023a]\nalso present the robustness issues of LLMs in dialogue eval-\nuation through adversarial perturbations. [Shen et al., 2023]\nindicate that LLM evaluators have a lower correlation with\nhuman assessments when scoring high-quality summaries. In\naddition, [Zhang et al., 2023a] state that evaluators based\non large models have a bias towards high scores, especially\nin non-Latin languages like Chinese and Japanese. Beyond\nthese shortcomings of performance, both ChatGPT and GPT-\n4 are proprietary models, and their opacity could lead to irre-\nproducible evaluation results.\n4\nFine-tuning LLMs\n4.1\nOverview\nAs mentioned above, despite the exciting performance of\nprompting LLMs like ChatGPT and GPT-4 for NLG evalu-\nation, several shortcomings in practice are inevitable, such\nas high costs, possibly irreproducible results, and potential\nbiases in LLMs. In response, recent research has shifted to-\nwards fine-tuning smaller, open-source LLMs specifically for\nevaluation purposes, aiming to achieve performance close to\nGPT-4 in NLG evaluation. Representative works of this type\ninclude PandaLM [Wang et al., 2023e], Prometheus [Kim\net al., 2023a], Shepherd [Wang et al., 2023c], TIGERScore\n[Jiang et al., 2023], INSTRUCTSCORE [Xu et al., 2023],\nAuto-J [Li et al., 2023a], CritiqueLLM [Ke et al., 2023] and\nJudgeLM [Zhu et al., 2023]. Their main ideas are similar, in-\nvolving the elaborate construction of high-quality evaluation\ndata, followed by accordingly fine-tuning open-source base\nLLMs. Nevertheless, there are certain discrepancies in the de-\nsigns across different works, such as the usage of references\nand evaluation criteria. We have summarized the key differ-\nent components of these methods in Table 1 for comparison,\nwhich will be elaborated on next.\n4.2\nData Construction\nDiverse data with high-quality annotations is crucial for the\nfine-tuning of evaluation models, which mainly involves task\nscenarios, inputs, target texts to evaluate, and evaluation re-\nsults. Early NLG evaluation research primarily focused on\nconventional NLG tasks, such as summarization and dialogue\ngeneration. Thus, the task scenarios, inputs, and target texts\nrefer to the corresponding NLP task, source inputs of the\ntask, and outputs generated by specialized systems based on\ntask requirements, respectively. And mainstream datasets for\nthese tasks predominantly employ human annotators to pro-\nvide evaluation results, which are often considered reliable.\nWith the recent rise of LLMs, the spectrum of NLG tasks\nhas been broadened to scenarios of instruction and response\nthat are more aligned with human needs. Traditional tasks\nlike summarization with corresponding source inputs can be\nviewed as kinds of instructions and requirements.\nMean-\nwhile, responses generated by various general LLMs gen-\nerally serve as the target texts now and require more flex-\nible evaluation so that the performance of different LLMs\ncan be compared, promoting further developments. There-\nfore, to keep pace with the current advancement of modeling\ntechniques, most evaluation methods have adopted the similar\ninstruction-response scenario.\nThe primary differences in these works actually lie in the\nconstruction of instructions, with the purpose of improving\neither diversity or reliability for the better generalization abil-\nity of the fine-tuned model. PandaLM and JudgeLM entirely\nsample from common instruction datasets, such as Alpaca\n52K, while CritiqueLLM adopts small-scale sampling fol-\nlowed by ChatGPT augmentation. In contrast, Prometheus\nand INSTRUCTSCORE rely on GPT-4 to generate all the in-\nstructions based on seed data, whereas Auto-J and Shepherd\nuse real-world data. Moreover, since large-scale human anno-\ntation is impractical, most works utilize GPT-4 as the power-\nful annotator, except for PandaLM and Shepherd, which use\nGPT-3.5 and human annotation on small-scale community\ndata, respectively.\nDuring the construction, they basically\nall design detailed prompts or guidance and apply heuristic\nfiltering strategies and post-processing methods to mitigate\nnoise. Overall, despite the possible higher quality of human\nannotation, the corresponding drawback is the difficulty in\nconstructing large-scale datasets, which in turn may hinder\nadequate model training, while using LLMs for construction\nis the opposite situation.\n4.3\nEvaluation Method\nAs with prompting LLMs, the evaluation methods adopted in\nthese works are highly diversified, involving different evalua-\ntion criteria, result modes, and usages of the reference. Given\nthat current instruction-response scenarios encompass differ-\nent types of tasks, it is unsuitable to specify unified evalu-\nation criteria as in traditional NLG tasks. However, some\nworks still do it this way, while some other methods let LLM\nannotators adaptively and implicitly reflect the required cri-\nteria in their evaluations, like PandaLM, TIGERScore, and\nMethod\nData Construction\nEvaluation Method\nBase LLM\nReference\nRequired\nInstruction Source\nAnnotator Scale\nResult Mode\nDetails\nSpecific Criteria\nPandaLM\nAlpaca 52K\nGPT-3.5\n300K\nComparison\nReason &\nReference\nUnified\nLLaMA\n7B\nNo\nPrometheus\nGPT-4 Construction\nGPT-4\n100K\nScoring\nReason\nExplicit\nLLaMA-2-Chat\n7B & 13B\nYes\nShepherd\nCommunity Critique Data\n& 9 NLP Tasks Data\nHuman\n1317\nOverall\nJudgement\nError Identifying\n& Refinement\nUnified\nLLaMA\n7B\nNo\nTIGERScore\n23 Distinctive Text\nGeneration Datasets\nGPT-4\n48K\nMQM\nError Analysis\nImplicit\nLLaMA-2\n7B & 13B\nNo\nINSTRUCTSCORE\nGPT-4 Construction\nGPT-4\n40K\nMQM\nError Analysis\nImplicit\nLLaMA\n7B\nYes\nAUTO-J\nReal-world User Queries\nfrom Preference Datasets\nGPT-4\n4396\nScoring &\nComparison\nReason\nImplicit\nLLaMA-2-Chat\n13B\nNo\nCritiqueLLM\nAlignBench &\nChatGPT Augmentation\nGPT-4\n9332\nScoring\nReason\nUnified\nChatGLM-2\n6B, 12B & 66B\nFlexible\nJudgeLM\nGPT4All-LAION, ShareGPT\nAlpaca-GPT4 & Dolly-15K\nGPT-4\n100K\nScoring &\nComparison\nReason\nUnified\nVicuna\n7B, 13B & 33B\nFlexible\nTable 1: Comparison of the different key components among the representative methods of fine-tuning LLMs.\nAUTO-J. In particular, AUTO-J has meticulously crafted 332\nevaluation criteria, matched to different tasks. Furthermore,\nPrometheus explicitly incorporates evaluation criteria into the\ninputs of the model, expecting flexible evaluation based on\nvarious customized criteria.\nMore details about the evaluation methods are shown in\nTable 1. All the works require models to provide detailed\ninformation, such as reasons for their evaluation results. And\nthe MQM mode can achieve more informative error analysis,\noffering stronger interpretability. Moreover, some works do\nnot necessarily require references and then have greater value\nin practice. And a more optimal method is to concurrently\nsupport both reference-based and reference-free evaluations\nas JudgeLM and CritiqueLLM.\n4.4\nFine-tuning Implementation\nThe fine-tuning process is uniformly implemented by differ-\nent works on their selected open-source LLMs, like LLaMA,\nand respective constructed data, with some targeted set-\ntings. Specifically, Prometheus maintains balanced data dis-\ntributions during fine-tuning, including the length and label.\nJudgeLM eliminates potential biases by randomly swapping\nsample pairs to be compared and randomly removing ref-\nerences. INSTRUCTSCORE utilizes GPT-4 to provide er-\nror annotations for the intermediate outputs of the fine-tuned\nmodel for further supervised reinforcement. Moreover, Cri-\ntiqueLLM implements separately, with and without refer-\nences, and explores the effects of data and model scale. Com-\npared to the vanilla fine-tuning setting, these methods have\nimproved the efficiency of model training and the robustness\nof evaluations.\n4.5\nPros and Cons\nThe shortcomings of prompting LLM methods can be signif-\nicantly alleviated due to the customized implementation of\ndata construction and fine-tuning. For instance, most fine-\ntuned models range between 7B and 13B in the scale of pa-\nrameters, facilitating low-cost inference use and good repro-\nducibility, with performance close to GPT4 in NLG evalua-\ntion. And specific measures can be adopted to prevent related\nbiases found in GPT4 during different stages. Furthermore,\nthis type of approach allows for continuous iteration and im-\nprovement of the model to address potential deficiencies or\nemerging issues discovered in future applications.\nHowever, some biases associated with GPT4 may still per-\nsist, as the data construction of most methods employs GPT4\nfor critical evaluation annotation. On the other hand, the base\nopen-source LLMs selected by existing works are primarily\nthe series of LLaMA. With the rapid updates and improve-\nments of open-source large models recently, it adheres to the\nintuition that employing a more powerful base LLM should\nlead to better evaluation performance. However, this means\nrepetitive fine-tuning processes and computational expenses\nsince directly migrating existing finetuned models to the new\nbase LLM is difficult. Additionally, although many existing\nmethods aspire for more flexible and comprehensive evalua-\ntion through fine-tuning, demanding many evaluation settings\nin model training may ultimately lead to poor performance,\nwhich has been observed in our practice and also mentioned\nrelationally in [Li et al., 2023a]. And considering the differ-\nent evaluation settings in existing works, it is challenging to\nconduct a horizontal comparison among them. These issues\nrequire further exploration in future research.\n5\nHuman-LLM Collaborative Evaluation\n5.1\nOverview\nWhile LLMs demonstrate robust evaluation capabilities, there\nexists a need for further enhancement in terms of their relia-\nbility, particularly in establishing a stronger correlation with\nhuman evaluation outcomes. Although human evaluation is\nthe gold-standard evaluation approach in NLG, it is recog-\nnized for its associated high costs and susceptibility to sub-\njective biases [Li et al., 2023b].\nThe robust and compre-\nhensive capabilities exhibited by LLMs underscore consider-\nable potential for the development of collaborative evaluation\nmethodologies that integrate both human and LLMs. In re-\ncent investigations, researchers have initiated the exploration\nof collaborative evaluation paradigms, which include tradi-\ntional NLG evaluation methods such as scoring and explain-\ning [Zhang et al., 2021; Li et al., 2023b], broader evaluation\nmethods such as testing and debugging [Ribeiro and Lund-\nberg, 2022], and auditing NLG models to ensure fairness\n[Rastogi et al., 2023]. Furthermore, scholars [Saunders et\nal., 2022] are actively engaging in efforts to address the intri-\ncate challenge of scalable oversight through the collaboration\nof humans and LLMs. The objective is to devise strategies\nfor effectively evaluating models in tasks that pose inherent\ndifficulties for human assessors. This collaborative approach\nseeks to leverage the distinctive strengths of both human eval-\nuators and sophisticated language models to achieve robust\nand nuanced evaluations in challenging domains.\n5.2\nScoring and Explaining\nAutomated evaluation frequently exhibits a limited correla-\ntion with human judgments, while human evaluation, though\nreliable, is labor-intensive. [Zhang et al., 2021] present a\nhuman-machine collaborative framework (HMCEval) which\nconceptualizes dialogue evaluation as a sample assignment\nproblem to ensure the reliability of evaluation outcomes while\nminimizing human effort and achieves 99% accuracy with\nhalf human effort. Recently, LLMs have emerged as a cost-\neffective alternative to human evaluations.\nHowever, both\nhumans and LLMs have limitations, including inherent sub-\njectivity and unreliable judgments, especially in open-ended\ntasks with diverse requirements.\nTo address challenges associated with inconsistent evalua-\ntion criteria in open-ended tasks and explore synergy between\nhumans and LLM-based evaluators, [Li et al., 2023b] pro-\nposes a Collaborative Evaluation pipeline (COEVAL), which\ninvolves designing a checklist of task-specific criteria and\nconducting detailed evaluations where LLMs generate initial\nideation and humans engage in scrutiny. Depending solely on\nscore predictions is insufficient for ensuring reliable evalua-\ntion and error detection, particularly when specific criteria de-\nmand nuanced analysis beyond straightforward scoring. CO-\nEVAL is assigned the additional task of generating explana-\ntions to elucidate evaluation outcomes to facilitate a trustwor-\nthy collaborative evaluation process. Results indicate CO-\nEVAL effectively evaluates lengthy texts by utilizing LLMs,\nsaving significant time and reducing human evaluation out-\nliers. Despite the involvement of LLMs, human scrutiny re-\nmains essential, contributing to the revision of around 20% of\nLLM evaluation scores for enhanced reliability.\n5.3\nBroader Evaluation Tasks\nThe broader evaluation of NLG models involves testing and\ndebugging the models. Current methods often rely on highly\nvariable human creativity and extensive manual effort or are\nlimited to addressing a very specific class of bugs. [Ribeiro\nand Lundberg, 2022] introduce AdaTest, a process that uses\nLLMs in collaboration with human feedback to automatically\ngenerate unit tests that highlight bugs in a target model, which\nproves to make users 5-10 times more effective at identifying\nbugs and assists users in effectively fixing bugs without intro-\nducing new ones. Moreover, LLMs have shown biases and ir-\nresponsible behavior, necessitating thorough auditing before\ndeployment. AdaTest++ [Rastogi et al., 2023] draw on in-\nsights from literature on human-AI collaboration and sense-\nmaking, and engage with research experts in safe and fair\nAI, which emphasizes the importance of sensemaking and ef-\nfective communication between humans and AI to capitalize\non their complementary strengths in collaborative auditing.\nAdaTest++ successfully leverages human strengths, such as\nschematization and hypothesis testing. Moreover, users iden-\ntified a range of failure modes across 26 different topics in\nissues that were revealed in formal audits and those that were\npreviously under-reported. Additionally, ensuring trustwor-\nthiness in LLMs for challenging tasks poses a crucial chal-\nlenge. Scalable oversight [Amodei et al., 2016] aims to effec-\ntively evaluate models on tasks challenging for humans and\nsuggests the use of AI for assistance. [Saunders et al., 2022]\nexplored providing critiques of model outputs as a form of as-\nsistance, demonstrating that model-generated critiques assist\nhumans in identifying overlooked flaws.\n5.4\nPros and Cons\nThe advantages of human-AI collaborative evaluation lie in\nachieving a balance between efficiency and cost, as demon-\nstrated by COEVAL [Li et al., 2023b] achieving this equi-\nlibrium.\nAdditionally, there are complementary strengths\nbetween humans and AI. For instance, AdaTest++ [Rastogi\net al., 2023] empowers users to consistently utilize their\nstrengths throughout the auditing process, benefiting signif-\nicantly from LLM. Users who generate the most topics heav-\nily rely on LLM suggestions while employing their contextual\nreasoning and semantic understanding to update their mental\nmodels vigilantly and identify model failures.\nHowever, there are drawbacks. The evaluation results of\nLLMs may be sensitive to the formats used to query the model\nand might require additional support for prompt writing [Li\net al., 2023b; Rastogi et al., 2023]. Furthermore, the current\ncapability to assess confidence levels is not strong enough,\nmaking it challenging to determine when to trust the LLM.\nFurthermore, certain level of human supervision is still nec-\nessary, making it less convenient and cost-effective compared\nto fully automated evaluation.\n6\nConclusions and Future Trends\nThrough the above review of studies on NLG evaluation\nbased on LLMs, we find that these four categories of ap-\nproaches have their respective strengths and weaknesses,\nand most of the existing work is concentrated on prompting\nLLMs. In view of this, we offer some suggestions for future\ndirections in this field.\nUnified benchmarks for LLM-based NLG evaluation\napproaches. As mentioned above, each of the studies that\nfine-tuned LLMs to construct specialized evaluation models\nuses different settings and data during testing, making them\nincomparable. In the research on prompting LLMs for NLG\nevaluation, there are some publicly available human judg-\nments on the same NLG task, such as SummEval for summa-\nrization. However, the existing human judgments have many\nproblems. Firstly, most of the existing data only involve one\ntype of NLG task and a single human evaluation method (e.g.,\nscoring), making it difficult to evaluate LLMs\u2019 performance\non different tasks, as well as using different evaluation meth-\nods on the same task. Secondly, many of the texts in these\nhuman judgments are generated by outdated models (such as\nPointer Network) and do not include texts generated by more\nadvanced LLMs. Lastly, many human evaluation datasets are\ntoo small in scale. There is an urgent need for large-scale,\nhigh-quality human evaluation data covering various NLG\ntasks and evaluation methods as a benchmark.\nNLG evaluation for low-resource languages and new\ntask scenarios. Almost all existing research focuses on En-\nglish data. However, it is doubtful whether LLMs have simi-\nlar levels of NLG evaluation capability for texts in other lan-\nguages, especially low-resource languages. As [Zhang et al.,\n2023a] points out, we should be more cautious about using\nLLMs to evaluate texts in non-Latin languages. Additionally,\nexisting research mainly focuses on more traditional NLG\ntasks such as translation, summarization, and dialogue. How-\never, there are many new scenarios in reality with different re-\nquirements and evaluation criteria. Research on low-resource\nlanguages and new task scenarios will provide a more com-\nprehensive understanding of LLMs\u2019 evaluation capabilities.\nDiverse forms of human-LLM collaborative NLG eval-\nuation. According to the literature reviewed above, there is\nlittle research on collaborative evaluation between humans\nand LLMs. Neither humans nor LLMs are perfect, and each\nhas its strengths. Since the ultimate goal of NLG research is\nto evaluate text quality more accurately and efficiently, we\nbelieve that collaboration between humans and LLMs can\nachieve better results than pure human evaluation or auto-\nmatic evaluation. In the collaboration between humans and\nLLMs, technologies in the field of human-computer interac-\ntion may bring new implementation methods to the collabo-\nration. In addition, what roles humans and LLMs should play\nin the evaluation and how they can better complement each\nother are still worth researching.\nReferences\n[Amodei et al., 2016] Dario Amodei, Chris Olah, Jacob\nSteinhardt, Paul F. Christiano, John Schulman, and\nDan Man\u00b4\ne.\nConcrete problems in AI safety.\nCoRR,\nabs/1606.06565, 2016.\n[Anil et al., 2023] Rohan Anil, Andrew M. Dai, Orhan Fi-\nrat, Melvin Johnson, Dmitry Lepikhin, Alexandre Passos,\nSiamak Shakeri, Emanuel Taropa, Paige Bailey, Zhifeng\nChen, Eric Chu, Jonathan H. Clark, Laurent El Shafey,\nYanping Huang, Kathy Meier-Hellstern, Gaurav Mishra,\nErica Moreira, Mark Omernick, Kevin Robinson, Sebas-\ntian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing\nZhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob\nAustin, Paul Barham, Jan Botha, James Bradbury, Sid-\ndhartha Brahma, Kevin Brooks, Michele Catasta, Yong\nCheng, Colin Cherry, Christopher A. Choquette-Choo,\nAakanksha Chowdhery, Cl\u00b4\nement Crepy, Shachi Dave,\nMostafa Dehghani, Sunipa Dev, Jacob Devlin, Mark D\u00b4\n\u0131az,\nNan Du, Ethan Dyer, Vlad Feinberg, Fangxiaoyu Feng,\nVlad Fienber, Markus Freitag, Xavier Garcia, Sebastian\nGehrmann, Lucas Gonzalez, Guy Gur-Ari, Steven Hand,\nHadi Hashemi, Le Hou, Joshua Howland, Andrea Hu,\nJeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Itty-\ncheriah, Matthew Jagielski, Wenhao Jia, Kathleen Ke-\nnealy, Maxim Krikun, Sneha Kudugunta, Chang Lan,\nKatherine Lee, Benjamin Lee, Eric Li, Music Li, Wei\nLi, YaGuang Li, Jian Li, Hyeontaek Lim, Hanzhao Lin,\nZhongtao Liu, Frederick Liu, Marcello Maggioni, Aroma\nMahendru, Joshua Maynez, Vedant Misra, Maysam Mous-\nsalem, Zachary Nado, John Nham, Eric Ni, Andrew Nys-\ntrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex\nPolozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan\nRichter, Parker Riley, Alex Castro Ros, Aurko Roy, Bren-\nnan Saeta, Rajkumar Samuel, Renee Shelby, Ambrose\nSlone, Daniel Smilkov, David R. So, Daniel Sohn, Si-\nmon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vo-\ndrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao\nWang, John Wieting, Yuhuai Wu, Kelvin Xu, Yunhan\nXu, Linting Xue, Pengcheng Yin, Jiahui Yu, Qiao Zhang,\nSteven Zheng, Ce Zheng, Weikang Zhou, Denny Zhou,\nSlav Petrov, and Yonghui Wu. Palm 2 technical report.\nComputing Research Repository, arxiv:2305.10403, 2023.\n[Bai et al., 2023] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv,\nYuze He, et al.\nBenchmarking foundation models with\nlanguage-model-as-an-examiner. CoRR, abs/2306.04181,\n2023.\n[Chan et al., 2023] Chi-Min Chan, Weize Chen, Yusheng\nSu, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and\nZhiyuan Liu. Chateval: Towards better llm-based evalua-\ntors through multi-agent debate. CoRR, abs/2308.07201,\n2023.\n[Chang et al., 2023] Yapei Chang, Kyle Lo, Tanya Goyal,\nand Mohit Iyyer. Booookscore: A systematic exploration\nof book-length summarization in the era of llms. CoRR,\nabs/2310.00785, 2023.\n[Chiang and Lee, 2023] David\nCheng-Han\nChiang\nand\nHung-yi Lee.\nA closer look into using large language\nmodels for automatic evaluation. In EMNLP (Findings),\n2023.\n[Chung et al., 2022] Hyung Won Chung, Le Hou, Shayne\nLongpre, Barret Zoph, Yi Tay, William Fedus, Eric Li,\nXuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Al-\nbert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac\nSuzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y. Zhao, Yan-\nping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov,\nEd H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny\nZhou, Quoc V. Le, and Jason Wei. Scaling instruction-\nfinetuned language models. CoRR, abs/2210.11416, 2022.\n[Cohen et al., 2023] Roi Cohen, May Hamri, Mor Geva, and\nAmir Globerson. LM vs LM: detecting factual errors via\ncross examination. In EMNLP, 2023.\n[Durmus et al., 2020] Esin Durmus, He He, and Mona T.\nDiab. FEQA: A question answering evaluation framework\nfor faithfulness assessment in abstractive summarization.\nIn ACL, 2020.\n[ES et al., 2023] Shahul ES, Jithin James, Luis Espinosa\nAnke, and Steven Schockaert.\nRAGAS: automated\nevaluation of retrieval augmented generation.\nCoRR,\nabs/2309.15217, 2023.\n[Freitag et al., 2022] Markus Freitag, Ricardo Rei, Nitika\nMathur, Chi-kiu Lo, Craig Stewart, Eleftherios Avramidis,\nTom Kocmi, George F. Foster, Alon Lavie, and Andr\u00b4\ne F. T.\nMartins. Results of WMT22 metrics shared task: Stop us-\ning BLEU - neural metrics are better and more robust. In\nWMT, 2022.\n[Fu et al., 2023a] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang,\nand Pengfei Liu. Gptscore: Evaluate as you desire. CoRR,\nabs/2302.04166, 2023.\n[Fu et al., 2023b] Xue-Yong\nFu,\nMd.\nTahmid\nRahman\nLaskar, Cheng Chen, and Shashi Bhushan TN. Are large\nlanguage models reliable judges? A study on the factual-\nity evaluation capabilities of llms. CoRR, abs/2311.00681,\n2023.\n[Gong and Mao, 2023] Peiyuan Gong and Jiaxin Mao. Coas-\ncore: Chain-of-aspects prompting for NLG evaluation.\nCoRR, abs/2312.10355, 2023.\n[Hasanbeig et al., 2023] Hosein Hasanbeig, Hiteshi Sharma,\nLeo Betthauser, Felipe Vieira Frujeri, and Ida Momenne-\njad.\nALLURE: auditing and improving llm-based eval-\nuation of text using iterative in-context-learning. CoRR,\nabs/2309.13701, 2023.\n[He et al., 2023a] Hangfeng He, Hongming Zhang, and Dan\nRoth. Socreval: Large language models with the socratic\nmethod for reference-free reasoning evaluation.\nCoRR,\nabs/2310.00074, 2023.\n[He et al., 2023b] Tianxing He, Jingyu Zhang, Tianle Wang,\nSachin Kumar, Kyunghyun Cho, James R. Glass, and Yu-\nlia Tsvetkov. On the blind spots of model-based evaluation\nmetrics for text generation. In ACL (1), 2023.\n[Hu et al., 2023] Yebowen Hu, Kaiqiang Song, Sangwoo\nCho, Xiaoyang Wang, Hassan Foroosh, and Fei Liu. De-\ncipherpref: Analyzing influential factors in human prefer-\nence judgments via GPT-4. In EMNLP, 2023.\n[Jain et al., 2023] Sameer Jain, Vaishakh Keshava, Swar-\nnashree Mysore Sathyendra, Patrick Fernandes, Pengfei\nLiu, Graham Neubig, and Chunting Zhou.\nMulti-\ndimensional evaluation of text summarization with in-\ncontext learning. In ACL (Findings), 2023.\n[Ji et al., 2023] Yunjie Ji, Yan Gong, Yiping Peng, Chao Ni,\nPeiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang\nLi. Exploring chatgpt\u2019s ability to rank content: A prelimi-\nnary study on consistency with human preferences. CoRR,\nabs/2303.07610, 2023.\n[Jia et al., 2023] Qi Jia, Siyu Ren, Yizhu Liu, and Kenny Q.\nZhu. Zero-shot faithfulness evaluation for text summariza-\ntion with foundation language model. In EMNLP, 2023.\n[Jiang et al., 2023] Dongfu Jiang, Yishan Li, Ge Zhang,\nWenhao Huang, Bill Yuchen Lin, and Wenhu Chen. Tiger-\nscore: Towards building explainable metric for all text\ngeneration tasks. CoRR, abs/2310.00752, 2023.\n[Ke et al., 2023] Pei Ke, Bosi Wen, Zhuoer Feng, Xiao Liu,\nXuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng,\nYuxiao Dong, Hongning Wang, Jie Tang, and Minlie\nHuang.\nCritiquellm: Scaling llm-as-critic for effective\nand explainable evaluation of large language model gen-\neration. CoRR, abs/2311.18702, 2023.\n[Kim et al., 2023a] Seungone Kim, Jamin Shin, Yejin Cho,\nJoel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun,\nSeongjin Shin, Sungdong Kim, James Thorne, and Min-\njoon Seo. Prometheus: Inducing fine-grained evaluation\ncapability in language models.\nCoRR, abs/2310.08491,\n2023.\n[Kim et al., 2023b] Tae Soo Kim, Yoonjoo Lee, Jamin Shin,\nYoung-Ho Kim, and Juho Kim. Evallm: Interactive eval-\nuation of large language model prompts on user-defined\ncriteria. CoRR, abs/2309.13633, 2023.\n[Kocmi and Federmann, 2023a] Tom Kocmi and Christian\nFedermann. GEMBA-MQM: detecting translation quality\nerror spans with GPT-4. In WMT, 2023.\n[Kocmi and Federmann, 2023b] Tom Kocmi and Christian\nFedermann.\nLarge language models are state-of-the-art\nevaluators of translation quality. In EAMT, 2023.\n[Kotonya et al., 2023] Neema Kotonya, Saran Krishnasamy,\nJoel R. Tetreault, and Alejandro Jaimes. Little giants: Ex-\nploring the potential of small llms as evaluation metrics in\nsummarization in the eval4nlp 2023 shared task. CoRR,\nabs/2311.00686, 2023.\n[Leiter et al., 2023] Christoph Leiter, Juri Opitz, Daniel\nDeutsch, Yang Gao, Rotem Dror, and Steffen Eger. The\neval4nlp 2023 shared task on prompting large language\nmodels as explainable metrics.\nCoRR, abs/2310.19792,\n2023.\n[Li et al., 2023a] Junlong Li, Shichao Sun, Weizhe Yuan,\nRun-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge\nfor evaluating alignment. CoRR, abs/2310.05470, 2023.\n[Li et al., 2023b] Qintong Li, Leyang Cui, Lingpeng Kong,\nand Wei Bi. Collaborative evaluation: Exploring the syn-\nergy of large language models and humans for open-ended\ngeneration evaluation. CoRR, abs/2310.19740, 2023.\n[Li et al., 2023c] Ruosen Li, Teerth Patel, and Xinya Du.\nPRD: peer rank and discussion improve large language\nmodel based evaluations. CoRR, abs/2307.02762, 2023.\n[Li et al., 2023d] Zongjie Li, Chaozheng Wang, Pingchuan\nMa, Daoyuan Wu, Shuai Wang, Cuiyun Gao, and Yang\nLiu. Split and merge: Aligning position biases in large\nlanguage model based evaluators. CoRR, abs/2310.01432,\n2023.\n[Lin and Chen, 2023] Yen-Ting Lin and Yun-Nung Chen.\nLlm-eval: Unified multi-dimensional automatic evaluation\nfor open-domain conversations with large language mod-\nels. In NLP4ConvAI 2023, 2023.\n[Liu et al., 2023a] Yang Liu, Dan Iter, Yichong Xu, Shuo-\nhang Wang, Ruochen Xu, and Chenguang Zhu. G-eval:\nNLG evaluation using gpt-4 with better human alignment.\nIn EMNLP, 2023.\n[Liu et al., 2023b] Yixin Liu, Alexander R. Fabbri, Jiawen\nChen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu,\nDragomir Radev, Chien-Sheng Wu, and Arman Cohan.\nBenchmarking generation and evaluation capabilities of\nlarge language models for instruction controllable summa-\nrization. CoRR, abs/2311.09184, 2023.\n[Liu et al., 2023c] Yixin Liu, Alexander R. Fabbri, Pengfei\nLiu, Dragomir Radev, and Arman Cohan.\nOn learning\nto summarize with large language models as references.\nCoRR, abs/2305.14239, 2023.\n[Liu et al., 2023d] Yongkang Liu, Shi Feng, Daling Wang,\nYifei Zhang, and Hinrich Sch\u00a8\nutze.\nEvaluate what you\ncan\u2019t evaluate: Unassessable generated responses quality.\nCoRR, abs/2305.14658, 2023.\n[Liu et al., 2023e] Yuxuan Liu,\nTianchi Yang,\nShaohan\nHuang, Zihan Zhang, Haizhen Huang, Furu Wei, Weiwei\nDeng, Feng Sun, and Qi Zhang.\nCalibrating llm-based\nevaluator. CoRR, abs/2309.13308, 2023.\n[Liusie et al., 2023] Adian Liusie, Potsawee Manakul, and\nMark J. F. Gales.\nLlm comparative assessment: Zero-\nshot nlg evaluation through pairwise comparisons using\nlarge language models. Computing Research Repository,\narxiv:2307.07889, 2023.\n[Luo et al., 2023] Zheheng Luo, Qianqian Xie, and Sophia\nAnaniadou.\nChatgpt as a factual inconsistency eval-\nuator\nfor\nabstractive\ntext\nsummarization.\nCoRR,\nabs/2303.15621, 2023.\n[Mendonc\n\u00b8a et al., 2023] John Mendonc\n\u00b8a, Patr\u00b4\n\u0131cia Pereira,\nJo\u02dc\nao Paulo Carvalho, Alon Lavie, and Isabel Trancoso.\nSimple LLM prompting is state-of-the-art for robust and\nmultilingual dialogue evaluation. In Proceedings of The\nEleventh Dialog System Technology Challenge, 2023.\n[Rastogi et al., 2023] Charvi Rastogi, Marco T\u00b4\nulio Ribeiro,\nNicholas King, Harsha Nori, and Saleema Amershi. Sup-\nporting human-ai collaboration in auditing llms with llms.\nIn AIES, 2023.\n[Ribeiro and Lundberg, 2022] Marco\nT\u00b4\nulio\nRibeiro\nand\nScott M. Lundberg.\nAdaptive testing and debugging of\nNLP models. In ACL (1), 2022.\n[Saha et al., 2023] Swarnadeep Saha, Omer Levy, Asli Ce-\nlikyilmaz, Mohit Bansal, Jason Weston, and Xian Li.\nBranch-solve-merge improves large language model eval-\nuation and generation. CoRR, abs/2310.15123, 2023.\n[Saunders et al., 2022] William Saunders, Catherine Yeh,\nJeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and\nJan Leike. Self-critiquing models for assisting human eval-\nuators. CoRR, abs/2206.05802, 2022.\n[Shen et al., 2023] Chenhui Shen, Liying Cheng, Xuan-Phi\nNguyen, Yang You, and Lidong Bing.\nLarge language\nmodels are not yet human-level evaluators for abstractive\nsummarization. In EMNLP (Findings), 2023.\n[Shu et al., 2023] Lei Shu, Nevan Wichers, Liangchen Luo,\nYun Zhu, Yinxiao Liu, Jindong Chen, and Lei Meng.\nFusion-eval:\nIntegrating evaluators with llms.\nCoRR,\nabs/2311.09204, 2023.\n[Sulem et al., 2018] Elior Sulem, Omri Abend, and Ari Rap-\npoport. BLEU is not suitable for the evaluation of text\nsimplification. In EMNLP, 2018.\n[Sun et al., 2022] Tianxiang Sun, Junliang He, Xipeng Qiu,\nand Xuanjing Huang. Bertscore is unfair: On social bias\nin language model-based metrics for text generation. In\nEMNLP, 2022.\n[Touvron et al., 2023] Hugo Touvron, Louis Martin, Kevin\nStone, Peter Albert, Amjad Almahairi, Yasmine Babaei,\nNikolay Bashlykov, Soumya Batra, Prajjwal Bhargava,\nShruti Bhosale, Dan Bikel, Lukas Blecher, Cristian\nCanton-Ferrer, Moya Chen, Guillem Cucurull, David Es-\niobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian\nFuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal,\nAnthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan\nInan, Marcin Kardas, Viktor Kerkez, Madian Khabsa,\nIsabel Kloumann, Artem Korenev, Punit Singh Koura,\nMarie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana\nLiskovich, Yinghai Lu, Yuning Mao, Xavier Martinet,\nTodor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin\nNie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta,\nKalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael\nSmith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh\nTang, Ross Taylor, Adina Williams, Jian Xiang Kuan,\nPuxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, An-\ngela Fan, Melanie Kambadur, Sharan Narang, Aur\u00b4\nelien\nRodriguez, Robert Stojnic, Sergey Edunov, and Thomas\nScialom. Llama 2: Open foundation and fine-tuned chat\nmodels. CoRR, abs/2307.09288, 2023.\n[Wang et al., 2023a] Jiaan Wang, Yunlong Liang, Fandong\nMeng, Haoxiang Shi, Zhixu Li, Jinan Xu, Jianfeng Qu,\nand Jie Zhou. Is ChatGPT a good NLG evaluator? a pre-\nliminary study. In Proceedings of the 4th New Frontiers in\nSummarization Workshop, 2023.\n[Wang et al., 2023b] Peiyi Wang, Lei Li, Liang Chen, Dawei\nZhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and\nZhifang Sui. Large language models are not fair evalua-\ntors. CoRR, abs/2305.17926, 2023.\n[Wang et al., 2023c] Tianlu Wang, Ping Yu, Xiaoqing Ellen\nTan, Sean O\u2019Brien, Ramakanth Pasunuru, Jane Dwivedi-\nYu, Olga Golovneva, Luke Zettlemoyer, Maryam Fazel-\nZarandi, and Asli Celikyilmaz. Shepherd: A critic for lan-\nguage model generation. CoRR, abs/2308.04592, 2023.\n[Wang et al., 2023d] Yaqing Wang, Jiepu Jiang, Mingyang\nZhang, Cheng Li, Yi Liang, Qiaozhu Mei, and Michael\nBendersky.\nAutomated evaluation of personalized\ntext generation using large language models.\nCoRR,\nabs/2310.11593, 2023.\n[Wang et al., 2023e] Yidong Wang, Zhuohao Yu, Zhengran\nZeng, Linyi Yang, Cunxiang Wang, et al. Pandalm: An\nautomatic evaluation benchmark for LLM instruction tun-\ning optimization. CoRR, abs/2306.05087, 2023.\n[Wang et al., 2023f] Zifan Wang, Kotaro Funakoshi, and\nManabu Okumura. Automatic answerability evaluation for\nquestion generation. CoRR, abs/2309.12546, 2023.\n[Wu and Aji, 2023] Minghao Wu and Alham Fikri Aji. Style\nover substance: Evaluation biases for large language mod-\nels. CoRR, abs/2307.03025, 2023.\n[Wu et al., 2023] Ning Wu, Ming Gong, Linjun Shou, Shin-\ning Liang, and Daxin Jiang.\nLarge language models\nare diverse role-players for summarization evaluation. In\nNLPCC (1), 2023.\n[Xie et al., 2023] Zhuohan Xie, Miao Li, Trevor Cohn, and\nJey Han Lau. Deltascore: Fine-grained story evaluation\nwith perturbations. In EMNLP (Findings), 2023.\n[Xu et al., 2023] Wenda Xu, Danqing Wang, Liangming\nPan, Zhenqiao Song, Markus Freitag, William Wang, and\nLei Li. INSTRUCTSCORE: towards explainable text gen-\neration evaluation with automatic feedback. In EMNLP,\n2023.\n[Ye et al., 2023] Seonghyeon Ye, Doyoung Kim, Sungdong\nKim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo,\nJames Thorne, Juho Kim, and Minjoon Seo.\nFLASK:\nfine-grained language model evaluation based on align-\nment skill sets. CoRR, abs/2307.10928, 2023.\n[Yuan et al., 2021] Weizhe Yuan,\nGraham Neubig,\nand\nPengfei Liu. Bartscore: Evaluating generated text as text\ngeneration. In NeurIPS, 2021.\n[Yuan et al., 2024] Peiwen Yuan, Shaoxiong Feng, Yiwei Li,\nXinglin Wang, Boyuan Pan, Heda Wang, and Kan Li.\nBatcheval: Towards human-like text evaluation.\nCoRR,\nabs/2401.00437, 2024.\n[Zhang et al., 2020] Tianyi Zhang, Varsha Kishore, Felix\nWu, Kilian Q. Weinberger, and Yoav Artzi.\nBertscore:\nEvaluating text generation with BERT. In ICLR, 2020.\n[Zhang et al., 2021] Yangjun Zhang,\nPengjie Ren,\nand\nMaarten de Rijke.\nA human-machine collaborative\nframework for evaluating malevolence in dialogues.\nIn\nACL/IJCNLP (1), 2021.\n[Zhang et al., 2023a] Chen Zhang, Luis Fernando D\u2019Haro,\nYiming Chen, Malu Zhang, and Haizhou Li.\nA com-\nprehensive analysis of the effectiveness of large lan-\nguage models as automatic dialogue evaluators.\nCoRR,\nabs/2312.15407, 2023.\n[Zhang et al., 2023b] Xinghua Zhang, Bowen Yu, Haiyang\nYu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and\nYongbin Li. Wider and deeper LLM networks are fairer\nLLM evaluators. CoRR, abs/2308.01862, 2023.\n[Zheng et al., 2023] Lianmin Zheng, Wei-Lin Chiang, Ying\nSheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang,\net al. Judging llm-as-a-judge with mt-bench and chatbot\narena. CoRR, abs/2306.05685, 2023.\n[Zhu et al., 2023] Lianghui Zhu, Xinggang Wang, and Xin-\nlong Wang. Judgelm: Fine-tuned large language models\nare scalable judges. CoRR, abs/2310.17631, 2023.\n",
    "ref": [
        "1606.06565",
        "2305.10403",
        "2306.04181",
        "2308.07201",
        "2310.00785",
        "2210.11416",
        "2309.15217",
        "2302.04166",
        "2311.00681",
        "2312.10355",
        "2309.13701",
        "2310.00074",
        "2303.07610",
        "2310.00752",
        "2311.18702",
        "2310.08491",
        "2309.13633",
        "2311.00686",
        "2310.19792",
        "2310.05470",
        "2310.19740",
        "2307.02762",
        "2310.01432",
        "2311.09184",
        "2305.14239",
        "2305.14658",
        "2309.13308",
        "2307.07889",
        "2303.15621",
        "2310.15123",
        "2206.05802",
        "2311.09204",
        "2307.09288",
        "2305.17926",
        "2308.04592",
        "2310.11593",
        "2306.05087",
        "2309.12546",
        "2307.03025",
        "2307.10928",
        "2401.00437",
        "2312.15407",
        "2308.01862",
        "2306.05685",
        "2310.17631"
    ]
}