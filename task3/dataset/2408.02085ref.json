{
    "2303.09540": {
        "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
        "abstract": "Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.",
        "date": "2023-03-16T17:53:24+00:00",
        "label": 1
    },
    "2303.08774": {
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "date": "2023-03-15T17:15:04+00:00",
        "label": 1
    },
    "2402.16827": {
        "title": "A Survey on Data Selection for Language Models",
        "abstract": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.",
        "date": "2024-02-26T18:54:35+00:00",
        "label": 1
    },
    "2401.13229": {
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
        "date": "2024-01-24T04:57:32+00:00",
        "label": 1
    },
    "2405.20541": {
        "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
        "abstract": "In this work, we investigate whether small language models can determine\nhigh-quality subsets of large-scale text datasets that improve the performance\nof larger language models. While existing work has shown that pruning based on\nthe perplexity of a larger model can yield high-quality data, we investigate\nwhether smaller models can be used for perplexity-based pruning and how pruning\nis affected by the domain composition of the data being pruned. We demonstrate\nthat for multiple dataset compositions, perplexity-based pruning of pretraining\ndata can \\emph{significantly} improve downstream task performance: pruning\nbased on perplexities computed with a 125 million parameter model improves the\naverage performance on downstream tasks of a 3 billion parameter model by up to\n2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach\ncommensurate baseline performance. Furthermore, we demonstrate that such\nperplexity-based data pruning also yields downstream performance gains in the\nover-trained and data-constrained regimes.",
        "date": "2024-05-30T23:50:20+00:00",
        "label": 1
    },
    "2407.14985": {
        "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
        "abstract": "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
        "date": "2024-07-20T21:24:40+00:00",
        "label": 1
    },
    "2406.09334": {
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models",
        "abstract": "Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper introduces ProxyLM, a scalable framework for predicting LM\nperformance using proxy models in multilingual tasks. These proxy models act as\nsurrogates, approximating the performance of the LM of interest. By leveraging\nproxy models, ProxyLM significantly reduces computational overhead on task\nevaluations, achieving up to a 37.08x speedup compared to traditional methods,\neven with our smallest proxy models. Additionally, our methodology showcases\nadaptability to previously unseen languages in pre-trained LMs, outperforming\nthe state-of-the-art performance by 1.89x as measured by root-mean-square error\n(RMSE). This framework streamlines model selection, enabling efficient\ndeployment and iterative LM enhancements without extensive computational\nresources.",
        "date": "2024-06-13T17:15:33+00:00",
        "label": 1
    },
    "2402.17327": {
        "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
        "abstract": "We study the data selection problem, whose aim is to select a small\nrepresentative subset of data that can be used to efficiently train a machine\nlearning model. We present a new data selection approach based on $k$-means\nclustering and sensitivity sampling. Assuming access to an embedding\nrepresentation of the data with respect to which the model loss is H\\\"older\ncontinuous, our approach provably allows selecting a set of ``typical'' $k +\n1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of\nthe whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an\nadditive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means\ncost for the input embeddings and $\\lambda$ is the H\\\"older constant.\n  We furthermore demonstrate the performance and scalability of our approach on\nfine-tuning foundation models and show that it outperforms state-of-the-art\nmethods. We also show how it can be applied on linear regression, leading to a\nnew sampling strategy that surprisingly matches the performances of leverage\nscore sampling, while being conceptually simpler and more scalable.",
        "date": "2024-02-27T09:03:43+00:00",
        "label": 1
    },
    "2405.12186": {
        "title": "Training Data Attribution via Approximate Unrolled Differentiation",
        "abstract": "Many training data attribution (TDA) methods aim to estimate how a model's\nbehavior would change if one or more data points were removed from the training\nset. Methods based on implicit differentiation, such as influence functions,\ncan be made computationally efficient, but fail to account for\nunderspecification, the implicit bias of the optimization algorithm, or\nmulti-stage training pipelines. By contrast, methods based on unrolling address\nthese issues but face scalability challenges. In this work, we connect the\nimplicit-differentiation-based and unrolling-based approaches and combine their\nbenefits by introducing Source, an approximate unrolling-based TDA method that\nis computed using an influence-function-like formula. While being\ncomputationally efficient compared to unrolling-based approaches, Source is\nsuitable in cases where implicit-differentiation-based approaches struggle,\nsuch as in non-converged models and multi-stage training pipelines.\nEmpirically, Source outperforms existing TDA techniques in counterfactual\nprediction, especially in settings where implicit-differentiation-based\napproaches fall short.",
        "date": "2024-05-20T17:17:44+00:00",
        "label": 1
    },
    "2309.16609": {
        "title": "Qwen Technical Report",
        "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, enabling natural language processing tasks that were previously\nthought to be exclusive to humans. In this work, we introduce Qwen, the first\ninstallment of our large language model series. Qwen is a comprehensive\nlanguage model series that encompasses distinct models with varying parameter\ncounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,\nthe chat models finetuned with human alignment techniques. The base language\nmodels consistently demonstrate superior performance across a multitude of\ndownstream tasks, and the chat models, particularly those trained using\nReinforcement Learning from Human Feedback (RLHF), are highly competitive. The\nchat models possess advanced tool-use and planning capabilities for creating\nagent applications, showcasing impressive performance even when compared to\nbigger models on complex tasks like utilizing a code interpreter. Furthermore,\nwe have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as\nwell as mathematics-focused models, Math-Qwen-Chat, which are built upon base\nlanguage models. These models demonstrate significantly improved performance in\ncomparison with open-source models, and slightly fall behind the proprietary\nmodels.",
        "date": "2023-09-28T17:07:49+00:00",
        "label": 1
    },
    "2203.14544": {
        "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning",
        "abstract": "The goal of continual learning (CL) is to efficiently update a machine\nlearning model with new data without forgetting previously-learned knowledge.\nMost widely-used CL methods rely on a rehearsal memory of data points to be\nreused while training on new data. Curating such a rehearsal memory to maintain\na small, informative subset of all the data seen so far is crucial to the\nsuccess of these methods. We devise a coreset selection method for\nrehearsal-based continual learning. Our method is based on the idea of gradient\nmatching: The gradients induced by the coreset should match, as closely as\npossible, those induced by the original training dataset. Inspired by the\nneural tangent kernel theory, we perform this gradient matching across the\nmodel's initialization distribution, allowing us to extract a coreset without\nhaving to train the model first. We evaluate the method on a wide range of\ncontinual learning scenarios and demonstrate that it improves the performance\nof rehearsal-based CL methods compared to competing memory management\nstrategies such as reservoir sampling.",
        "date": "2022-03-28T07:37:17+00:00",
        "label": 1
    },
    "1512.02985": {
        "title": "On Variants of k-means Clustering",
        "abstract": "\\textit{Clustering problems} often arise in the fields like data mining,\nmachine learning etc. to group a collection of objects into similar groups with\nrespect to a similarity (or dissimilarity) measure. Among the clustering\nproblems, specifically \\textit{$k$-means} clustering has got much attention\nfrom the researchers. Despite the fact that $k$-means is a very well studied\nproblem its status in the plane is still an open problem. In particular, it is\nunknown whether it admits a PTAS in the plane. The best known approximation\nbound in polynomial time is $9+\\eps$.\n  In this paper, we consider the following variant of $k$-means. Given a set\n$C$ of points in $\\mathcal{R}^d$ and a real $f > 0$, find a finite set $F$ of\npoints in $\\mathcal{R}^d$ that minimizes the quantity $f*|F|+\\sum_{p\\in C}\n\\min_{q \\in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local\nsearch PTAS for this problem. We also give a \"bi-criterion\" local search\nalgorithm for $k$-means which uses $(1+\\eps)k$ centers and yields a solution\nwhose cost is at most $(1+\\eps)$ times the cost of an optimal $k$-means\nsolution. The algorithm runs in polynomial time for any fixed dimension.\n  The contribution of this paper is two fold. On the one hand, we are being\nable to handle the square of distances in an elegant manner, which yields near\noptimal approximation bound. This leads us towards a better understanding of\nthe $k$-means problem. On the other hand, our analysis of local search might\nalso be useful for other geometric problems. This is important considering that\nvery little is known about the local search method for geometric approximation.",
        "date": "2015-12-09T18:37:49+00:00",
        "label": 1
    },
    "2006.14651": {
        "title": "Influence Functions in Deep Learning Are Fragile",
        "abstract": "Influence functions approximate the effect of training samples in test-time\npredictions and have a wide variety of applications in machine learning\ninterpretability and uncertainty estimation. A commonly-used (first-order)\ninfluence function can be implemented efficiently as a post-hoc method\nrequiring access only to the gradients and Hessian of the model. For linear\nmodels, influence functions are well-defined due to the convexity of the\nunderlying loss function and are generally accurate even across difficult\nsettings where model changes are fairly large such as estimating group\ninfluences. Influence functions, however, are not well-understood in the\ncontext of deep learning with non-convex loss functions. In this paper, we\nprovide a comprehensive and large-scale empirical study of successes and\nfailures of influence functions in neural network models trained on datasets\nsuch as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments,\nwe show that the network architecture, its depth and width, as well as the\nextent of model parameterization and regularization techniques have strong\neffects in the accuracy of influence functions. In particular, we find that (i)\ninfluence estimates are fairly accurate for shallow networks, while for deeper\nnetworks the estimates are often erroneous; (ii) for certain network\narchitectures and datasets, training with weight-decay regularization is\nimportant to get high-quality influence estimates; and (iii) the accuracy of\ninfluence estimates can vary significantly depending on the examined test\npoints. These results suggest that in general influence functions in deep\nlearning are fragile and call for developing improved influence estimation\nmethods to mitigate these issues in non-convex setups.",
        "date": "2020-06-25T18:25:59+00:00",
        "label": 1
    },
    "2401.06692": {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "date": "2024-01-12T16:56:54+00:00",
        "label": 1
    },
    "2312.06254": {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "label": 1
    },
    "2310.13032": {
        "title": "Quality-Diversity through AI Feedback",
        "abstract": "In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.",
        "date": "2023-10-19T12:13:58+00:00",
        "label": 1
    },
    "2311.14736": {
        "title": "Data Diversity Matters for Robust Instruction Tuning",
        "abstract": "Recent works have shown that by curating high quality and diverse instruction\ntuning datasets, we can significantly improve instruction-following\ncapabilities. However, creating such datasets is difficult and most works rely\non manual curation or proprietary language models. Automatic data curation is\ndifficult as it is still not clear how we can define diversity for instruction\ntuning, how diversity and quality depend on one other, and how we can optimize\ndataset quality and diversity. To resolve these issue, we propose a new\nalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple\nmethod to simultaneously control dataset diversity and quality, allowing us to\nconduct an in-depth study on the effect of diversity and quality on instruction\ntuning performance. From this study we draw two key insights (1) there is a\nnatural tradeoff between data diversity and quality and (2) increasing data\ndiversity significantly improves the worst case instruction following\nperformance, therefore improving robustness. We validate the performance of\nQDIT on several large scale instruction tuning datasets, where we find it can\nsubstantially improve worst and average case performance compared to\nquality-driven data selection.",
        "date": "2023-11-21T19:12:18+00:00",
        "label": 1
    },
    "2403.16898": {
        "title": "Concerned with Data Contamination? Assessing Countermeasures in Code Language Model",
        "abstract": "Various techniques have been proposed to leverage the capabilities of code\nlanguage models (CLMs) for SE tasks. While these techniques typically evaluate\ntheir effectiveness using publicly available datasets, the evaluation can be\nsubject to data contamination threats where the evaluation datasets have\nalready been used to train the concerned CLMs. This can significantly affect\nthe reliability of the evaluation. Different countermeasures have been\nsuggested to mitigate the data contamination threat. Countermeasures include\nusing more recent data, curating new data, and refactoring existing data are\nintroduced, yet it is unclear whether these countermeasures could really\nmitigate data contamination threats to model evaluation. To fill the gap, we\nsystematically study to quantify the impacts of these countermeasures on CLMs'\nperformance. To facilitate the study, we collected over 2 million Python\nfunctions with timestamps ranging from January 1st, 2018, to December 31st,\n2023. The data created before the models' cut-off date are considered\n\"contaminated data\", while the data where the countermeasures are taken are\nregarded as \"cleansed data\". We study the impact of these countermeasures by\ninvestigating the difference in CLMs' performance on contaminated and cleansed\ndata derived from different countermeasures. Our experiments yield several\ninteresting observations. For instance, CLMs do not necessarily perform worse\non data after the models' cut-off date; on the contrary, they sometimes perform\nbetter. In addition, refactoring did not always result in decreased\nperformance; it could lead to improvements instead. Furthermore, existing\nmetrics such as perplexity cannot distinguish contaminated/cleansed data. We\nhope that the results and observations could help deepen the understanding of\nCLMs' capabilities and inform the community about data contamination.",
        "date": "2024-03-25T16:10:25+00:00",
        "label": 1
    },
    "1702.05962": {
        "title": "Latent Variable Dialogue Models and their Diversity",
        "abstract": "We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.",
        "date": "2017-02-20T13:36:23+00:00",
        "label": 1
    },
    "2307.06290": {
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
        "abstract": "Large language models (LLMs) are initially pretrained for broad capabilities\nand then finetuned with instruction-following datasets to improve their\nperformance in interacting with humans. Despite advances in finetuning, a\nstandardized guideline for selecting high-quality datasets to optimize this\nprocess remains elusive. In this paper, we first propose InstructMining, an\ninnovative method designed for automatically selecting premium\ninstruction-following data for finetuning LLMs. Specifically, InstructMining\nutilizes natural language indicators as a measure of data quality, applying\nthem to evaluate unseen datasets. During experimentation, we discover that\ndouble descent phenomenon exists in large language model finetuning. Based on\nthis observation, we further leverage BlendSearch to help find the best subset\namong the entire dataset (i.e., 2,532 out of 100,000). Experiment results show\nthat InstructMining-7B achieves state-of-the-art performance on two of the most\npopular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",
        "date": "2023-07-12T16:37:31+00:00",
        "label": 1
    },
    "2308.07201": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "date": "2023-08-14T15:13:04+00:00",
        "label": 1
    },
    "2105.10446": {
        "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction",
        "abstract": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained \"white-box\"\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at\n\\url{https://github.com/Ma-Lab-Berkeley}.",
        "date": "2021-05-21T16:29:57+00:00",
        "label": 1
    },
    "2305.09246": {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
        "abstract": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "date": "2023-05-16T07:52:57+00:00",
        "label": 1
    },
    "2403.12776": {
        "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
        "abstract": "Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).",
        "date": "2024-03-19T14:44:45+00:00",
        "label": 1
    },
    "2307.08701": {
        "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
        "abstract": "Large language models (LLMs) strengthen instruction-following capability\nthrough instruction-finetuning (IFT) on supervised instruction/response data.\nHowever, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly\ncontain many low-quality instances with incorrect or irrelevant responses,\nwhich are misleading and detrimental to IFT. In this paper, we propose a simple\nand effective data selection strategy that automatically identifies and filters\nout low-quality data using a strong LLM (e.g., ChatGPT). To this end, we\nintroduce AlpaGasus, which is finetuned on only 9k high-quality data filtered\nfrom the 52k Alpaca data. AlpaGasus significantly outperforms the original\nAlpaca as evaluated by GPT-4 on multiple test sets and the controlled human\nevaluation. Its 13B variant matches $>90\\%$ performance of its teacher LLM\n(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the\nefficacy of our method across diverse datasets, base models, and LLM filters.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\nhttps://lichang-chen.github.io/AlpaGasus/",
        "date": "2023-07-17T17:59:40+00:00",
        "label": 1
    },
    "2406.14491": {
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
        "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
        "date": "2024-06-20T16:55:33+00:00",
        "label": 1
    },
    "2405.20456": {
        "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
        "abstract": "Recent works have shown that machine learning models improve at a predictable\nrate with the total amount of training data, leading to scaling laws that\ndescribe the relationship between error and dataset size. These scaling laws\ncan help design a model's training dataset, but they typically take an\naggregate view of the data by only considering the dataset's size. We introduce\na new perspective by investigating scaling behavior for the value of individual\ndata points: we find that a data point's contribution to model's performance\nshrinks predictably with the size of the dataset in a log-linear manner.\nInterestingly, there is significant variability in the scaling exponent among\ndifferent data points, indicating that certain points are more valuable in\nsmall datasets while others are relatively more useful as a part of large\ndatasets. We provide learning theory to support our scaling law, and we observe\nempirically that it holds across diverse model classes. We further propose a\nmaximum likelihood estimator and an amortized estimator to efficiently learn\nthe individualized scaling behaviors from a small number of noisy observations\nper data point. Using our estimators, we provide insights into factors that\ninfluence the scaling behavior of different data points. Finally, we\ndemonstrate applications of the individualized scaling laws to data valuation\nand data subset selection. Overall, our work represents a first step towards\nunderstanding and utilizing scaling properties for the value of individual data\npoints.",
        "date": "2024-05-30T20:10:24+00:00",
        "label": 1
    },
    "2311.09783": {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "abstract": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "date": "2023-11-16T11:03:04+00:00",
        "label": 1
    },
    "2109.06379": {
        "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",
        "abstract": "Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective that facilitates the design of metrics for a\nwide range of language generation tasks and quality aspects. Based on the\nnature of information change from input to output, we classify NLG tasks into\ncompression (e.g., summarization), transduction (e.g., text rewriting), and\ncreation (e.g., dialog). The information alignment, or overlap, between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. Using the uniform concept of information alignment, we develop a\nfamily of interpretable metrics for various NLG tasks and aspects, often\nwithout need of gold reference data. To operationalize the metrics, we train\nself-supervised models to approximate information alignment as a prediction\ntask. Experiments show the uniformly designed metrics achieve stronger or\ncomparable correlations with human judgement compared to state-of-the-art\nmetrics in each of diverse tasks, including text summarization, style transfer,\nand knowledge-grounded dialog. With information alignment as the intermediate\nrepresentation, we deliver a composable library for easy NLG evaluation and\nfuture metric design.",
        "date": "2021-09-14T01:00:42+00:00",
        "label": 1
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "date": "2018-10-11T00:50:01+00:00",
        "label": 1
    },
    "2305.14233": {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "abstract": "Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
        "date": "2023-05-23T16:49:14+00:00",
        "label": 1
    },
    "2002.06305": {
        "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
        "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.",
        "date": "2020-02-15T02:40:10+00:00",
        "label": 1
    },
    "2406.13542": {
        "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
        "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
        "date": "2024-06-19T13:29:53+00:00",
        "label": 1
    },
    "2304.06767": {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.",
        "date": "2023-04-13T18:22:40+00:00",
        "label": 1
    },
    "2311.15653": {
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning has become the de facto method to equip large language\nmodels (LLMs) with the ability of following user instructions. Usually,\nhundreds of thousands or millions of instruction-following pairs are employed\nto fine-tune the foundation LLMs. Recently, some studies show that a small\nnumber of high-quality instruction data is enough. However, how to select\nappropriate instruction data for a given LLM is still an open problem. To\naddress this problem, in this paper we present a model-oriented data selection\n(MoDS) approach, which selects instruction data based on a new criteria\nconsidering three aspects: quality, coverage and necessity. First, our approach\nutilizes a quality evaluation model to filter out the high-quality subset from\nthe original instruction dataset, and then designs an algorithm to further\nselect from the high-quality subset a seed instruction dataset with good\ncoverage. The seed dataset is applied to fine-tune the foundation LLM to obtain\nan initial instruction-following LLM. Finally, we develop a necessity\nevaluation model to find out the instruction data which are performed badly in\nthe initial instruction-following LLM and consider them necessary instructions\nto further improve the LLMs. In this way, we can get a small high-quality,\nbroad-coverage and high-necessity subset from the original instruction\ndatasets. Experimental results show that, the model fine-tuned with 4,000\ninstruction pairs selected by our approach could perform better than the model\nfine-tuned with the full original dataset which includes 214k instruction data.",
        "date": "2023-11-27T09:33:13+00:00",
        "label": 1
    },
    "1903.09722": {
        "title": "Pre-trained Language Model Representations for Language Generation",
        "abstract": "Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.",
        "date": "2019-03-22T22:14:51+00:00",
        "label": 1
    },
    "2401.12926": {
        "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
        "abstract": "When selecting data for training large-scale models, standard practice is to\nfilter for examples that match human notions of data quality. Such filtering\nyields qualitatively clean datapoints that intuitively should improve model\nbehavior. However, in practice the opposite can often happen: we find that\nselecting according to similarity with \"high quality\" data sources may not\nincrease (and can even hurt) performance compared to randomly selecting data.\n  To develop better methods for selecting data, we start by framing dataset\nselection as an optimization problem that we can directly solve for: given\ntarget tasks, a learning algorithm, and candidate data, select the subset that\nmaximizes model performance. This framework thus avoids handpicked notions of\ndata quality, and instead models explicitly how the learning process uses train\ndatapoints to predict on the target tasks. Our resulting method greatly\nimproves language model (LM) performance on both pre-specified tasks and\npreviously unseen tasks. Specifically, choosing target tasks representative of\nstandard LM problems and evaluating on diverse held-out benchmarks, our\nselected datasets provide a 2x compute multiplier over baseline methods.",
        "date": "2024-01-23T17:22:00+00:00",
        "label": 1
    },
    "2306.11670": {
        "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
        "abstract": "It is often advantageous to train models on a subset of the available train\nexamples, because the examples are of variable quality or because one would\nlike to train with fewer examples, without sacrificing performance. We present\nGradient Information Optimization (GIO), a scalable, task-agnostic approach to\nthis data selection problem that requires only a small set of (unlabeled)\nexamples representing a target distribution. GIO begins from a natural,\ninformation-theoretic objective that is intractable in practice. Our\ncontribution is in showing that it can be made highly scalable through a simple\nrelaxation of the objective and a highly efficient implementation. In\nexperiments with machine translation, spelling correction, and image\nrecognition, we show that GIO delivers outstanding results with very small\ntrain sets. These findings are robust to different representation models and\nhyperparameters for GIO itself. GIO is task- and domain-agnostic and can be\napplied out-of-the-box to new datasets and domains. We open source a\npip-installable implementation of the algorithm as \"pip install grad-info-opt\".",
        "date": "2023-06-20T16:43:38+00:00",
        "label": 1
    },
    "2007.01852": {
        "title": "Language-agnostic BERT Sentence Embedding",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "date": "2020-07-03T17:58:42+00:00",
        "label": 1
    },
    "2105.03075": {
        "title": "A Survey of Data Augmentation Approaches for NLP",
        "abstract": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
        "date": "2021-05-07T06:03:45+00:00",
        "label": 1
    },
    "1806.03884": {
        "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis",
        "abstract": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
        "date": "2018-06-11T09:44:23+00:00",
        "label": 1
    },
    "2402.05119": {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.",
        "date": "2024-02-03T04:45:25+00:00",
        "label": 1
    },
    "2308.03296": {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "abstract": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.",
        "date": "2023-08-07T04:47:42+00:00",
        "label": 1
    },
    "2306.11644": {
        "title": "Textbooks Are All You Need",
        "abstract": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.",
        "date": "2023-06-20T16:14:25+00:00",
        "label": 1
    },
    "2303.08114": {
        "title": "Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs",
        "abstract": "Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.",
        "date": "2023-03-14T17:47:25+00:00",
        "label": 1
    },
    "2203.15556": {
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "date": "2022-03-29T13:38:03+00:00",
        "label": 1
    },
    "2403.02839": {
        "title": "On the Limitations of Fine-tuned Judge Models for LLM Evaluation",
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this study, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\npropose an effective indicator to measure the reliability of fine-tuned judges,\nwith the aim of maximizing their utility in LLM evaluation.",
        "date": "2024-03-05T10:20:52+00:00",
        "label": 1
    },
    "2403.08763": {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "date": "2024-03-13T17:58:57+00:00",
        "label": 1
    },
    "2202.00622": {
        "title": "Datamodels: Predicting Predictions from Training Data",
        "abstract": "We present a conceptual framework, datamodeling, for analyzing the behavior\nof a model class in terms of the training data. For any fixed \"target\" example\n$x$, training set $S$, and learning algorithm, a datamodel is a parameterized\nfunction $2^S \\to \\mathbb{R}$ that for any subset of $S' \\subset S$ -- using\nonly information about which examples of $S$ are contained in $S'$ -- predicts\nthe outcome of training a model on $S'$ and evaluating on $x$. Despite the\npotential complexity of the underlying process being approximated (e.g.,\nend-to-end training and evaluation of deep neural networks), we show that even\nsimple linear datamodels can successfully predict model outputs. We then\ndemonstrate that datamodels give rise to a variety of applications, such as:\naccurately predicting the effect of dataset counterfactuals; identifying\nbrittle predictions; finding semantically similar examples; quantifying\ntrain-test leakage; and embedding data into a well-behaved and feature-rich\nrepresentation space. Data for this paper (including pre-computed datamodels as\nwell as raw predictions from four million trained deep neural networks) is\navailable at https://github.com/MadryLab/datamodels-data .",
        "date": "2022-02-01T18:15:24+00:00",
        "label": 1
    },
    "2210.14177": {
        "title": "Influence Functions for Sequence Tagging Models",
        "abstract": "Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.",
        "date": "2022-10-25T17:13:11+00:00",
        "label": 1
    },
    "2310.06825": {
        "title": "Mistral 7B",
        "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.",
        "date": "2023-10-10T17:54:58+00:00",
        "label": 1
    },
    "2401.04088": {
        "title": "Mixtral of Experts",
        "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
        "date": "2024-01-08T18:47:34+00:00",
        "label": 1
    },
    "2401.06059": {
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.",
        "date": "2024-01-11T17:24:49+00:00",
        "label": 1
    },
    "2306.02031": {
        "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
        "abstract": "Modern neural networks are known to give overconfident prediction for\nout-of-distribution inputs when deployed in the open world. It is common\npractice to leverage a surrogate outlier dataset to regularize the model during\ntraining, and recent studies emphasize the role of uncertainty in designing the\nsampling strategy for outlier dataset. However, the OOD samples selected solely\nbased on predictive uncertainty can be biased towards certain types, which may\nfail to capture the full outlier distribution. In this work, we empirically\nshow that diversity is critical in sampling outliers for OOD detection\nperformance. Motivated by the observation, we propose a straightforward and\nnovel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse\nand informative outliers. Specifically, we cluster the normalized features at\neach iteration, and the most informative outlier from each cluster is selected\nfor model training with absent category loss. With DOS, the sampled outliers\nefficiently shape a globally compact decision boundary between ID and OOD data.\nExtensive experiments demonstrate the superiority of DOS, reducing the average\nFPR95 by up to 25.79% on CIFAR-100 with TI-300K.",
        "date": "2023-06-03T07:17:48+00:00",
        "label": 1
    },
    "2402.05356": {
        "title": "Exploring Learning Complexity for Downstream Data Pruning",
        "abstract": "The over-parameterized pre-trained models pose a great challenge to\nfine-tuning with limited computation resources. An intuitive solution is to\nprune the less informative samples from the fine-tuning dataset. A series of\ntraining-based scoring functions are proposed to quantify the informativeness\nof the data subset but the pruning cost becomes non-negligible due to the heavy\nparameter updating. For efficient pruning, it is viable to adapt the similarity\nscoring function of geometric-based methods from training-based to\ntraining-free. However, we empirically show that such adaption distorts the\noriginal pruning and results in inferior performance on the downstream tasks.\nIn this paper, we propose to treat the learning complexity (LC) as the scoring\nfunction for classification and regression tasks. Specifically, the learning\ncomplexity is defined as the average predicted confidence of subnets with\ndifferent capacities, which encapsulates data processing within a converged\nmodel. Then we preserve the diverse and easy samples for fine-tuning. Extensive\nexperiments with vision datasets demonstrate the effectiveness and efficiency\nof the proposed scoring function for classification tasks. For the instruction\nfine-tuning of large language models, our method achieves state-of-the-art\nperformance with stable convergence, outperforming the full training with only\n10\\% of the instruction dataset.",
        "date": "2024-02-08T02:29:33+00:00",
        "label": 1
    },
    "2406.14026": {
        "title": "Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations",
        "abstract": "Language models (LMs) are known to suffer from forgetting of previously\nlearned examples when fine-tuned, breaking stability of deployed LM systems.\nDespite efforts on mitigating forgetting, few have investigated whether, and\nhow forgotten upstream examples are associated with newly learned tasks.\nInsights on such associations enable efficient and targeted mitigation of\nforgetting. In this paper, we empirically analyze forgetting that occurs in $N$\nupstream examples while the model learns $M$ new tasks and visualize their\nassociations with a $M \\times N$ matrix. We empirically demonstrate that the\ndegree of forgetting can often be approximated by simple multiplicative\ncontributions of the upstream examples and newly learned tasks. We also reveal\nmore complicated patterns where specific subsets of examples are forgotten with\nstatistics and visualization. Following our analysis, we predict forgetting\nthat happens on upstream examples when learning a new task with matrix\ncompletion over the empirical associations, outperforming prior approaches that\nrely on trainable LMs. Project website:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
        "date": "2024-06-20T06:46:23+00:00",
        "label": 1
    },
    "2402.01865": {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "abstract": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "date": "2024-02-02T19:43:15+00:00",
        "label": 1
    },
    "2110.08534": {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.",
        "date": "2021-10-16T09:59:33+00:00",
        "label": 1
    },
    "2207.05221": {
        "title": "Language Models (Mostly) Know What They Know",
        "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
        "date": "2022-07-11T22:59:39+00:00",
        "label": 1
    },
    "2001.08361": {
        "title": "Scaling Laws for Neural Language Models",
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "date": "2020-01-23T03:59:20+00:00",
        "label": 1
    },
    "2104.14337": {
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.",
        "date": "2021-04-07T17:49:17+00:00",
        "label": 1
    },
    "2405.06331": {
        "title": "LMD3: Language Model Data Density Dependence",
        "abstract": "We develop a methodology for analyzing language model task performance at the\nindividual example level based on training data density estimation. Experiments\nwith paraphrasing as a controlled intervention on finetuning data demonstrate\nthat increasing the support in the training distribution for specific test\nqueries results in a measurable increase in density, which is also a\nsignificant predictor of the performance increase caused by the intervention.\nExperiments with pretraining data demonstrate that we can explain a significant\nfraction of the variance in model perplexity via density measurements. We\nconclude that our framework can provide statistical evidence of the dependence\nof a target model's predictions on subsets of its training data, and can more\ngenerally be used to characterize the support (or lack thereof) in the training\ndata for a given test task.",
        "date": "2024-05-10T09:03:27+00:00",
        "label": 1
    },
    "2303.14753": {
        "title": "Does \"Deep Learning on a Data Diet\" reproduce? Overall yes, but GraNd at Initialization does not",
        "abstract": "The paper 'Deep Learning on a Data Diet' by Paul et al. (2021) introduces two\ninnovative metrics for pruning datasets during the training of neural networks.\nWhile we are able to replicate the results for the EL2N score at epoch 20, the\nsame cannot be said for the GraNd score at initialization. The GraNd scores\nlater in training provide useful pruning signals, however. The GraNd score at\ninitialization calculates the average gradient norm of an input sample across\nmultiple randomly initialized models before any training has taken place. Our\nanalysis reveals a strong correlation between the GraNd score at initialization\nand the input norm of a sample, suggesting that the latter could have been a\ncheap new baseline for data pruning. Unfortunately, neither the GraNd score at\ninitialization nor the input norm surpasses random pruning in performance. This\ncontradicts one of the findings in Paul et al. (2021). We were unable to\nreproduce their CIFAR-10 results using both an updated version of the original\nJAX repository and in a newly implemented PyTorch codebase. An investigation of\nthe underlying JAX/FLAX code from 2021 surfaced a bug in the checkpoint\nrestoring code that was fixed in April 2021\n(https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5).",
        "date": "2023-03-26T15:13:19+00:00",
        "label": 1
    },
    "2305.11383": {
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
        "abstract": "Recent works on instruction tuning (IT) have achieved great performance with\nzero-shot generalizability to unseen tasks. With additional context (e.g., task\ndefinition, examples) provided to models for fine-tuning, they achieved much\nhigher performance than untuned models. Despite impressive performance gains,\nwhat models learn from IT remains understudied. In this work, we analyze how\nmodels utilize instructions during IT by comparing model training with altered\nvs. original instructions. Specifically, we create simplified task definitions\nby removing all semantic components and only leaving the output space\ninformation, and delusive examples that contain incorrect input-output mapping.\nOur experiments show that models trained on simplified task definition or\ndelusive examples can achieve comparable performance to the ones trained on the\noriginal instructions and examples. Furthermore, we introduce a random baseline\nto perform zeroshot classification tasks, and find it achieves similar\nperformance (42.6% exact-match) as IT does (43% exact-match) in low resource\nsetting, while both methods outperform naive T5 significantly (30% per\nexact-match). Our analysis provides evidence that the impressive performance\ngain of current IT models can come from picking up superficial patterns, such\nas learning the output format and guessing. Our study highlights the urgent\nneed for more reliable IT methods and evaluation.",
        "date": "2023-05-19T02:00:47+00:00",
        "label": 1
    },
    "2311.00288": {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results\nby training large language models (LLMs) on a massive amount of diverse tasks\nwith instructions. However, how to select new tasks to improve the performance\nand generalizability of IT models remains an open question. Training on all\nexisting tasks is impractical due to prohibiting computation requirements, and\nrandomly selecting tasks can lead to suboptimal performance. In this work, we\npropose active instruction tuning based on prompt uncertainty, a novel\nframework to identify informative tasks, and then actively tune the models on\nthe selected tasks. We represent the informativeness of new tasks with the\ndisagreement of the current model outputs over perturbed prompts. Our\nexperiments on NIV2 and Self-Instruct datasets demonstrate that our method\nconsistently outperforms other baseline strategies for task selection,\nachieving better out-of-distribution generalization with fewer training tasks.\nAdditionally, we introduce a task map that categorizes and diagnoses tasks\nbased on prompt uncertainty and prediction probability. We discover that\ntraining on ambiguous (prompt-uncertain) tasks improves generalization while\ntraining on difficult (prompt-certain and low-probability) tasks offers no\nbenefit, underscoring the importance of task selection for instruction tuning.",
        "date": "2023-11-01T04:40:05+00:00",
        "label": 1
    },
    "2110.14049": {
        "title": "Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning",
        "abstract": "Data Shapley has recently been proposed as a principled framework to quantify\nthe contribution of individual datum in machine learning. It can effectively\nidentify helpful or harmful data points for a learning algorithm. In this\npaper, we propose Beta Shapley, which is a substantial generalization of Data\nShapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the\nShapley value, which is not critical for machine learning settings. Beta\nShapley unifies several popular data valuation methods and includes data\nShapley as a special case. Moreover, we prove that Beta Shapley has several\ndesirable statistical properties and propose efficient algorithms to estimate\nit. We demonstrate that Beta Shapley outperforms state-of-the-art data\nvaluation methods on several downstream ML tasks such as: 1) detecting\nmislabeled training data; 2) learning with subsamples; and 3) identifying\npoints whose addition or removal have the largest positive or negative impact\non the model.",
        "date": "2021-10-26T22:03:55+00:00",
        "label": 1
    },
    "2003.08529": {
        "title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections",
        "abstract": "Summarizing data samples by quantitative measures has a long history, with\ndescriptive statistics being a case in point. However, as natural language\nprocessing methods flourish, there are still insufficient characteristic\nmetrics to describe a collection of texts in terms of the words, sentences, or\nparagraphs they comprise. In this work, we propose metrics of diversity,\ndensity, and homogeneity that quantitatively measure the dispersion, sparsity,\nand uniformity of a text collection. We conduct a series of simulations to\nverify that each metric holds desired properties and resonates with human\nintuitions. Experiments on real-world datasets demonstrate that the proposed\ncharacteristic metrics are highly correlated with text classification\nperformance of a renowned model, BERT, which could inspire future applications.",
        "date": "2020-03-19T00:48:32+00:00",
        "label": 1
    },
    "1904.03122": {
        "title": "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        "abstract": "In a corpus of data, outliers are either errors: mistakes in the data that\nare counterproductive, or are unique: informative samples that improve model\nrobustness. Identifying outliers can lead to better datasets by (1) removing\nnoise in datasets and (2) guiding collection of additional data to fill gaps.\nHowever, the problem of detecting both outlier types has received relatively\nlittle attention in NLP, particularly for dialog systems. We introduce a simple\nand effective technique for detecting both erroneous and unique samples in a\ncorpus of short texts using neural sentence embeddings combined with\ndistance-based outlier detection. We also present a novel data collection\npipeline built atop our detection technique to automatically and iteratively\nmine unique data samples while discarding erroneous samples. Experiments show\nthat our outlier detection technique is effective at finding errors while our\ndata collection pipeline yields highly diverse corpora that in turn produce\nmore robust intent classification and slot-filling models.",
        "date": "2019-04-05T15:31:28+00:00",
        "label": 1
    },
    "2306.13840": {
        "title": "Beyond Scale: the Diversity Coefficient as a Data Quality Metric Demonstrates LLMs are Pre-trained on Formally Diverse Data",
        "abstract": "Current trends to pre-train capable Large Language Models (LLMs) mostly focus\non scaling of model and dataset size. However, the quality of pre-training data\nis an important factor for training powerful LLMs, yet it is a nebulous concept\nthat has not been fully characterized. Therefore, we use the recently proposed\nTask2Vec diversity coefficient to ground and understand formal aspects of data\nquality, to go beyond scale alone. Specifically, we measure the diversity\ncoefficient of publicly available pre-training datasets to demonstrate that\ntheir formal diversity is high when compared to theoretical lower and upper\nbounds. In addition, to build confidence in the diversity coefficient, we\nconduct interpretability experiments and find that the coefficient aligns with\nintuitive properties of diversity, e.g., it increases as the number of latent\nconcepts increases. We conclude the diversity coefficient is reliable, show\nit's high for publicly available LLM datasets, and conjecture it can be used to\nbuild useful diverse datasets for LLMs.",
        "date": "2023-06-24T02:25:56+00:00",
        "label": 1
    },
    "2107.06499": {
        "title": "Deduplicating Training Data Makes Language Models Better",
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "date": "2021-07-14T06:06:52+00:00",
        "label": 1
    },
    "2402.13064": {
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "date": "2024-02-20T15:00:35+00:00",
        "label": 1
    },
    "1510.03055": {
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.",
        "date": "2015-10-11T14:04:57+00:00",
        "label": 1
    },
    "2402.00530": {
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
        "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from\nlow-quality and redundant data. Data filtering for instruction tuning has\nproved important in improving both the efficiency and performance of the tuning\nprocess. But it also leads to extra cost and computation due to the involvement\nof LLMs in this process. To reduce the filtering cost, we study Superfiltering:\nCan we use a smaller and weaker model to select data for finetuning a larger\nand stronger model? Despite the performance gap between weak and strong\nlanguage models, we find their highly consistent capability to perceive\ninstruction difficulty and data selection results. This enables us to use a\nmuch smaller and more efficient model to filter the instruction data used to\ntrain a larger language model. Not only does it largely speed up the data\nfiltering, but the filtered-data-finetuned LLM achieves even better performance\non standard benchmarks. Extensive experiments validate the efficacy and\nefficiency of our approach.",
        "date": "2024-02-01T11:57:53+00:00",
        "label": 1
    },
    "2308.12032": {
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction\ndata quality and quantity is a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from open-source datasets, effectively minimizing manual curation and\npotential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to\nidentify discrepancies between a model's expected responses and its intrinsic\ngeneration capability. Through the application of IFD, cherry samples can be\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on datasets like Alpaca and WizardLM underpin our findings; with a\nmere $10\\%$ of original data input, our strategy showcases improved results.\nThis synthesis of self-guided cherry-picking and the IFD metric signifies a\ntransformative leap in the instruction tuning of LLMs, promising both\nefficiency and resource-conscious advancements. Codes, data, and models are\navailable: https://github.com/tianyi-lab/Cherry_LLM",
        "date": "2023-08-23T09:45:29+00:00",
        "label": 1
    },
    "2305.06161": {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.",
        "date": "2023-05-09T08:16:42+00:00",
        "label": 1
    },
    "2308.06259": {
        "title": "Self-Alignment with Instruction Backtranslation",
        "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.",
        "date": "2023-08-11T17:47:54+00:00",
        "label": 1
    },
    "2302.12366": {
        "title": "Less is More: Data Pruning for Faster Adversarial Training",
        "abstract": "Deep neural networks (DNNs) are sensitive to adversarial examples, resulting\nin fragile and unreliable performance in the real world. Although adversarial\ntraining (AT) is currently one of the most effective methodologies to robustify\nDNNs, it is computationally very expensive (e.g., 5-10X costlier than standard\ntraining). To address this challenge, existing approaches focus on single-step\nAT, referred to as Fast AT, reducing the overhead of adversarial example\ngeneration. Unfortunately, these approaches are known to fail against stronger\nadversaries. To make AT computationally efficient without compromising\nrobustness, this paper takes a different view of the efficient AT problem.\nSpecifically, we propose to minimize redundancies at the data level by\nleveraging data pruning. Extensive experiments demonstrate that the data\npruning based AT can achieve similar or superior robust (and clean) accuracy as\nits unpruned counterparts while being significantly faster. For instance,\nproposed strategies accelerate CIFAR-10 training up to 3.44X and CIFAR-100\ntraining to 2.02X. Additionally, the data pruning methods can readily be\nreconciled with existing adversarial acceleration tricks to obtain the striking\nspeed-ups of 5.66X and 5.12X on CIFAR-10, 3.67X and 3.07X on CIFAR-100 with\nTRADES and MART, respectively.",
        "date": "2023-02-23T23:48:20+00:00",
        "label": 1
    },
    "2310.07849": {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "abstract": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "date": "2023-10-11T19:51:13+00:00",
        "label": 1
    },
    "2211.09110": {
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.",
        "date": "2022-11-16T18:51:34+00:00",
        "label": 1
    },
    "2109.07958": {
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "abstract": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.",
        "date": "2021-09-08T17:15:27+00:00",
        "label": 1
    },
    "2401.17197": {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently\ngarnered considerable attention, where fine-tuning plays a key role in LLMs'\nadaptation. However, the cost of fine-tuning LLMs on rapidly expanding\nrecommendation data limits their practical application. To address this\nchallenge, few-shot fine-tuning offers a promising approach to quickly adapt\nLLMs to new recommendation data. We propose the task of data pruning for\nefficient LLM-based recommendation, aimed at identifying representative samples\ntailored for LLMs' few-shot fine-tuning. While coreset selection is closely\nrelated to the proposed task, existing coreset selection methods often rely on\nsuboptimal heuristic metrics or entail costly optimization on large-scale\nrecommendation data.\n  To tackle these issues, we introduce two objectives for the data pruning task\nin the context of LLM-based recommendation: 1) high accuracy aims to identify\nthe influential samples that can lead to high overall performance; and 2) high\nefficiency underlines the low costs of the data pruning process. To pursue the\ntwo objectives, we propose a novel data pruning method based on two scores,\ni.e., influence score and effort score, to efficiently identify the influential\nsamples. Particularly, the influence score is introduced to accurately estimate\nthe influence of sample removal on the overall performance. To achieve low\ncosts of the data pruning process, we use a small-sized surrogate model to\nreplace LLMs to obtain the influence score. Considering the potential gap\nbetween the surrogate model and LLMs, we further propose an effort score to\nprioritize some hard samples specifically for LLMs. Empirical results on three\nreal-world datasets validate the effectiveness of our proposed method. In\nparticular, the proposed method uses only 2% samples to surpass the full data\nfine-tuning, reducing time costs by 97%.",
        "date": "2024-01-30T17:31:19+00:00",
        "label": 1
    },
    "2401.08565": {
        "title": "Tuning Language Models by Proxy",
        "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. Interestingly, on TruthfulQA, proxy-tuned\nmodels are actually more truthful than directly tuned models, possibly because\ndecoding-time guidance better retains the model's factual knowledge. We then\ndemonstrate the generality of proxy-tuning by applying it to domain adaptation\non code, and task-specific finetuning on question-answering and math problems.\nFinally, we show how to proxy-tune a truly black-box LM, GPT-3.5, for temporal\nadaptation, increasing its knowledge about recent events. Our work demonstrates\nthe promise of using small tuned LMs to efficiently customize large,\npotentially proprietary LMs through decoding-time guidance.",
        "date": "2024-01-16T18:49:55+00:00",
        "label": 1
    },
    "2311.10774": {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "abstract": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.",
        "date": "2023-11-15T23:36:42+00:00",
        "label": 1
    },
    "2404.07840": {
        "title": "On Training Data Influence of GPT Models",
        "abstract": "Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.",
        "date": "2024-04-11T15:27:56+00:00",
        "label": 1
    },
    "2312.15685": {
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
        "abstract": "Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.",
        "date": "2023-12-25T10:29:28+00:00",
        "label": 1
    },
    "2403.09606": {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "date": "2024-03-14T17:47:20+00:00",
        "label": 1
    },
    "2402.18041": {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "date": "2024-02-28T04:35:51+00:00",
        "label": 1
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
        "date": "2019-07-26T17:48:29+00:00",
        "label": 1
    },
    "2406.14115": {
        "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
        "abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a\nhigh-quality subset from a given candidate dataset to train a Pending Fine-tune\nModel (PFM) into a Selective-Enhanced Model (SEM). It can improve the model\nperformance and accelerate the training process. Although a few surveys have\ninvestigated related works of data selection, there is a lack of comprehensive\ncomparison between existing methods due to their various experimental settings.\nTo address this issue, we first propose a three-stage scheme for data selection\nand comprehensively review existing works according to this scheme. Then, we\ndesign a unified comparing method with ratio-based efficiency indicators and\nranking-based feasibility indicators to overcome the difficulty of comparing\nvarious models with diverse experimental settings. After an in-depth\ncomparative analysis, we find that the more targeted method with data-specific\nand model-specific quality labels has higher efficiency, but the introduction\nof additional noise information should be avoided when designing selection\nalgorithms. Finally, we summarize the trends in data selection and highlight\nthe short-term and long-term challenges to guide future research.",
        "date": "2024-06-20T08:58:58+00:00",
        "label": 1
    },
    "2302.04062": {
        "title": "Machine Learning for Synthetic Data Generation: A Review",
        "abstract": "Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.",
        "date": "2023-02-08T13:59:31+00:00",
        "label": 1
    },
    "2203.08242": {
        "title": "Data Contamination: From Memorization to Exploitation",
        "abstract": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "date": "2022-03-15T20:37:16+00:00",
        "label": 1
    },
    "2309.04564": {
        "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale",
        "abstract": "Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.",
        "date": "2023-09-08T19:34:05+00:00",
        "label": 1
    },
    "1707.05589": {
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.",
        "date": "2017-07-18T12:35:53+00:00",
        "label": 1
    },
    "2208.01545": {
        "title": "The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence",
        "abstract": "Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by 1. proposing a novel metric\n-- the diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and\ntransfer learning under fair conditions (same architecture, same optimizer, and\nall models trained to convergence). Using the diversity coefficient, we show\nthat the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have\nlow diversity. This novel insight contextualizes claims that transfer learning\nsolutions are better than meta-learned solutions in the regime of low diversity\nunder a fair comparison. Specifically, we empirically find that a low diversity\ncoefficient correlates with a high similarity between transfer learning and\nMAML learned solutions in terms of accuracy at meta-test time and\nclassification layer similarity (using feature based distance metrics like\nSVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this\nmeta-test accuracy holds even as the model size changes. Therefore, we conclude\nthat in the low diversity regime, MAML and transfer learning have equivalent\nmeta-test performance when both are compared fairly. We also hope our work\ninspires more thoughtful constructions and quantitative evaluations of\nmeta-learning benchmarks in the future.",
        "date": "2022-08-02T15:49:11+00:00",
        "label": 1
    },
    "2008.03964": {
        "title": "DQI: A Guide to Benchmark Evaluation",
        "abstract": "A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.",
        "date": "2020-08-10T08:38:55+00:00",
        "label": 1
    },
    "2005.00816": {
        "title": "DQI: Measuring Data Quality in NLP",
        "abstract": "Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.",
        "date": "2020-05-02T12:34:17+00:00",
        "label": 1
    },
    "2403.00526": {
        "title": "Data Quality Assessment: Challenges and Opportunities",
        "abstract": "Data-oriented applications, their users, and even the law require data of\nhigh quality. Research has broken down the rather vague notion of data quality\ninto various dimensions, such as accuracy, consistency, and reputation, to name\nbut a few. To achieve the goal of high data quality, many tools and techniques\nexist to clean and otherwise improve data. Yet, systematic research on actually\nassessing data quality in all of its dimensions is largely absent, and with it\nthe ability to gauge the success of any data cleaning effort. It is our vision\nto establish a systematic and comprehensive framework for the (numeric)\nassessment of data quality for a given dataset and its intended use. Such a\nframework must cover the various facets that influence data quality, as well as\nthe many types of data quality dimensions. In particular, we identify five\nfacets that serve as a foundation of data quality assessment. For each facet,\nwe outline the challenges and opportunities that arise when trying to actually\nassign quality scores to data and create a data quality profile for it, along\nwith a wide range of technologies needed for this purpose.",
        "date": "2024-03-01T13:35:15+00:00",
        "label": 1
    },
    "2406.07545": {
        "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
        "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.",
        "date": "2024-06-11T17:59:47+00:00",
        "label": 1
    },
    "1908.01091": {
        "title": "Toward Understanding Catastrophic Forgetting in Continual Learning",
        "abstract": "We study the relationship between catastrophic forgetting and properties of\ntask sequences. In particular, given a sequence of tasks, we would like to\nunderstand which properties of this sequence influence the error rates of\ncontinual learning algorithms trained on the sequence. To this end, we propose\na new procedure that makes use of recent developments in task space modeling as\nwell as correlation analysis to specify and analyze the properties we are\ninterested in. As an application, we apply our procedure to study two\nproperties of a task sequence: (1) total complexity and (2) sequential\nheterogeneity. We show that error rates are strongly and positively correlated\nto a task sequence's total complexity for some state-of-the-art algorithms. We\nalso show that, surprisingly, the error rates have no or even negative\ncorrelations in some cases to sequential heterogeneity. Our findings suggest\ndirections for improving continual learning benchmarks and methods.",
        "date": "2019-08-02T23:30:35+00:00",
        "label": 1
    },
    "2405.02449": {
        "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
        "abstract": "Experimental design techniques such as active search and Bayesian\noptimization are widely used in the natural sciences for data collection and\ndiscovery. However, existing techniques tend to favor exploitation over\nexploration of the search space, which causes them to get stuck in local\noptima. This ``collapse\" problem prevents experimental design algorithms from\nyielding diverse high-quality data. In this paper, we extend the Vendi scores\n-- a family of interpretable similarity-based diversity metrics -- to account\nfor quality. We then leverage these quality-weighted Vendi scores to tackle\nexperimental design problems across various applications, including drug\ndiscovery, materials discovery, and reinforcement learning. We found that\nquality-weighted Vendi scores allow us to construct policies for experimental\ndesign that flexibly balance quality and diversity, and ultimately assemble\nrich and diverse sets of high-performing data points. Our algorithms led to a\n70%-170% increase in the number of effective discoveries compared to baselines.",
        "date": "2024-05-03T19:33:44+00:00",
        "label": 1
    },
    "1910.14599": {
        "title": "Adversarial NLI: A New Benchmark for Natural Language Understanding",
        "abstract": "We introduce a new large-scale NLI benchmark dataset, collected via an\niterative, adversarial human-and-model-in-the-loop procedure. We show that\ntraining models on this new dataset leads to state-of-the-art performance on a\nvariety of popular NLI benchmarks, while posing a more difficult challenge with\nits new test set. Our analysis sheds light on the shortcomings of current\nstate-of-the-art models, and shows that non-expert annotators are successful at\nfinding their weaknesses. The data collection method can be applied in a\nnever-ending learning scenario, becoming a moving target for NLU, rather than a\nstatic benchmark that will quickly saturate.",
        "date": "2019-10-31T16:50:43+00:00",
        "label": 1
    },
    "2406.13283": {
        "title": "Large-Scale Dataset Pruning in Adversarial Training through Data Importance Extrapolation",
        "abstract": "Their vulnerability to small, imperceptible attacks limits the adoption of\ndeep learning models to real-world systems. Adversarial training has proven to\nbe one of the most promising strategies against these attacks, at the expense\nof a substantial increase in training time. With the ongoing trend of\nintegrating large-scale synthetic data this is only expected to increase even\nfurther. Thus, the need for data-centric approaches that reduce the number of\ntraining samples while maintaining accuracy and robustness arises. While data\npruning and active learning are prominent research topics in deep learning,\nthey are as of now largely unexplored in the adversarial training literature.\nWe address this gap and propose a new data pruning strategy based on\nextrapolating data importance scores from a small set of data to a larger set.\nIn an empirical evaluation, we demonstrate that extrapolation-based pruning can\nefficiently reduce dataset size while maintaining robustness.",
        "date": "2024-06-19T07:23:51+00:00",
        "label": 1
    },
    "1905.05559": {
        "title": "Efficient Computation of Hessian Matrices in TensorFlow",
        "abstract": "The Hessian matrix has a number of important applications in a variety of\ndifferent fields, such as optimzation, image processing and statistics. In this\npaper we focus on the practical aspects of efficiently computing Hessian\nmatrices in the context of deep learning using the Python scripting language\nand the TensorFlow library. We define a general feed-forward neural network\nmodel and show how to efficiently compute two quantities: the cost function's\nexact Hessian matrix, and the cost function's approximate Hessian matrix, known\nas the Outer Product of Gradients (OPG) matrix. Furthermore, as the number of\nparameters (weights and biases) in deep learning usually is very large, we show\nhow to reduce the quadratic space complexity by an efficient implementation\nbased on approximate eigendecompositions.",
        "date": "2019-05-14T12:43:44+00:00",
        "label": 1
    },
    "2406.19976": {
        "title": "ScaleBiO: Scalable Bilevel Optimization for LLM Data Reweighting",
        "abstract": "Bilevel optimization has shown its utility across various machine learning\nsettings, yet most algorithms in practice require second-order information,\nmaking it challenging to scale them up. Only recently, a paradigm of\nfirst-order algorithms emerged, capable of effectively addressing bilevel\noptimization problems. Nevertheless, the practical efficiency of this paradigm\nremains unverified, particularly in the context of large language models\n(LLMs). This paper introduces the first scalable instantiation of this paradigm\ncalled ScaleBiO, focusing on bilevel optimization for large-scale LLM data\nreweighting. By combining with a recently proposed memory-efficient training\ntechnique called LISA, our novel algorithm allows the paradigm to scale to\n34-billion-parameter LLMs on eight A40 GPUs, marking the first successful\napplication of bilevel optimization under practical scenarios for large-sized\nLLMs. Empirically, extensive experiments on data reweighting verify the\neffectiveness of ScaleBiO for different-scaled models, including GPT-2,\nLLaMA-3-8B, GPT-NeoX-20B, and Yi-34B, where bilevel optimization succeeds in\nfiltering irrelevant data samples and selecting informative samples.\nTheoretically, ScaleBiO ensures the optimality of the learned data weights,\nalong with a convergence guarantee matching the conventional first-order\nbilevel optimization paradigm on smooth and strongly convex objectives.",
        "date": "2024-06-28T15:03:08+00:00",
        "label": 1
    },
    "2303.14186": {
        "title": "TRAK: Attributing Model Behavior at Scale",
        "abstract": "The goal of data attribution is to trace model predictions back to training\ndata. Despite a long line of work towards this goal, existing approaches to\ndata attribution tend to force users to choose between computational\ntractability and efficacy. That is, computationally tractable methods can\nstruggle with accurately attributing model predictions in non-convex settings\n(e.g., in the context of deep neural networks), while methods that are\neffective in such regimes require training thousands of models, which makes\nthem impractical for large models or datasets.\n  In this work, we introduce TRAK (Tracing with the Randomly-projected After\nKernel), a data attribution method that is both effective and computationally\ntractable for large-scale, differentiable models. In particular, by leveraging\nonly a handful of trained models, TRAK can match the performance of attribution\nmethods that require training thousands of models. We demonstrate the utility\nof TRAK across various modalities and scales: image classifiers trained on\nImageNet, vision-language models (CLIP), and language models (BERT and mT5). We\nprovide code for using TRAK (and reproducing our work) at\nhttps://github.com/MadryLab/trak .",
        "date": "2023-03-24T17:56:22+00:00",
        "label": 1
    },
    "2407.07263": {
        "title": "Reuse, Don't Retrain: A Recipe for Continued Pretraining of Language Models",
        "abstract": "As language models have scaled both their number of parameters and\npretraining dataset sizes, the computational cost for pretraining has become\nintractable except for the most well-resourced teams. This increasing cost\nmakes it ever more important to be able to reuse a model after it has completed\npretraining; allowing for a model's abilities to further improve without\nneeding to train from scratch. In this work, we detail a set of guidelines that\ncover how to design efficacious data distributions and learning rate schedules\nfor continued pretraining of language models. When applying these findings\nwithin a continued pretraining run on top of a well-trained 15B parameter\nmodel, we show an improvement of 9\\% in average model accuracy compared to the\nbaseline of continued training on the pretraining set. The resulting recipe\nprovides a practical starting point with which to begin developing language\nmodels through reuse rather than retraining.",
        "date": "2024-07-09T22:37:59+00:00",
        "label": 1
    },
    "2310.12952": {
        "title": "Cousins Of The Vendi Score: A Family Of Similarity-Based Diversity Metrics For Science And Machine Learning",
        "abstract": "Measuring diversity accurately is important for many scientific fields,\nincluding machine learning (ML), ecology, and chemistry. The Vendi Score was\nintroduced as a generic similarity-based diversity metric that extends the Hill\nnumber of order q=1 by leveraging ideas from quantum statistical mechanics.\nContrary to many diversity metrics in ecology, the Vendi Score accounts for\nsimilarity and does not require knowledge of the prevalence of the categories\nin the collection to be evaluated for diversity. However, the Vendi Score\ntreats each item in a given collection with a level of sensitivity proportional\nto the item's prevalence. This is undesirable in settings where there is a\nsignificant imbalance in item prevalence. In this paper, we extend the other\nHill numbers using similarity to provide flexibility in allocating sensitivity\nto rare or common items. This leads to a family of diversity metrics -- Vendi\nscores with different levels of sensitivity -- that can be used in a variety of\napplications. We study the properties of the scores in a synthetic controlled\nsetting where the ground truth diversity is known. We then test their utility\nin improving molecular simulations via Vendi Sampling. Finally, we use the\nVendi scores to better understand the behavior of image generative models in\nterms of memorization, duplication, diversity, and sample quality.",
        "date": "2023-10-19T17:52:04+00:00",
        "label": 1
    },
    "2310.06786": {
        "title": "OpenWebMath: An Open Dataset of High-Quality Mathematical Web Text",
        "abstract": "There is growing evidence that pretraining on high quality, carefully\nthought-out tokens such as code or mathematics plays an important role in\nimproving the reasoning abilities of large language models. For example,\nMinerva, a PaLM model finetuned on billions of tokens of mathematical documents\nfrom arXiv and the web, reported dramatically improved performance on problems\nthat require quantitative reasoning. However, because all known open source web\ndatasets employ preprocessing that does not faithfully preserve mathematical\nnotation, the benefits of large scale training on quantitive web documents are\nunavailable to the research community. We introduce OpenWebMath, an open\ndataset inspired by these works containing 14.7B tokens of mathematical\nwebpages from Common Crawl. We describe in detail our method for extracting\ntext and LaTeX content and removing boilerplate from HTML documents, as well as\nour methods for quality filtering and deduplication. Additionally, we run\nsmall-scale experiments by training 1.4B parameter language models on\nOpenWebMath, showing that models trained on 14.7B tokens of our dataset surpass\nthe performance of models trained on over 20x the amount of general language\ndata. We hope that our dataset, openly released on the Hugging Face Hub, will\nhelp spur advances in the reasoning abilities of large language models.",
        "date": "2023-10-10T16:57:28+00:00",
        "label": 1
    },
    "2103.07191": {
        "title": "Are NLP Models really able to Solve Simple Math Word Problems?",
        "abstract": "The problem of designing NLP solvers for math word problems (MWP) has seen\nsustained research activity and steady gains in the test accuracy. Since\nexisting solvers achieve high performance on the benchmark datasets for\nelementary level MWPs containing one-unknown arithmetic word problems, such\nproblems are often considered \"solved\" with the bulk of research attention\nmoving to more complex MWPs. In this paper, we restrict our attention to\nEnglish MWPs taught in grades four and lower. We provide strong evidence that\nthe existing MWP solvers rely on shallow heuristics to achieve high performance\non the benchmark datasets. To this end, we show that MWP solvers that do not\nhave access to the question asked in the MWP can still solve a large fraction\nof MWPs. Similarly, models that treat MWPs as bag-of-words can also achieve\nsurprisingly high accuracy. Further, we introduce a challenge dataset, SVAMP,\ncreated by applying carefully chosen variations over examples sampled from\nexisting datasets. The best accuracy achieved by state-of-the-art models is\nsubstantially lower on SVAMP, thus showing that much remains to be done even\nfor the simplest of the MWPs.",
        "date": "2021-03-12T10:23:47+00:00",
        "label": 1
    },
    "2306.01116": {
        "title": "The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only",
        "abstract": "Large language models are commonly trained on a mixture of filtered web data\nand curated high-quality corpora, such as social media conversations, books, or\ntechnical papers. This curation process is believed to be necessary to produce\nperformant models with broad zero-shot generalization abilities. However, as\nlarger models requiring pretraining on trillions of tokens are considered, it\nis unclear how scalable is curation and whether we will run out of unique\nhigh-quality data soon. At variance with previous beliefs, we show that\nproperly filtered and deduplicated web data alone can lead to powerful models;\neven significantly outperforming models from the state-of-the-art trained on\nThe Pile. Despite extensive filtering, the high-quality data we extract from\nthe web is still plentiful, and we are able to obtain five trillion tokens from\nCommonCrawl. We publicly release an extract of 600 billion tokens from our\nRefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",
        "date": "2023-06-01T20:03:56+00:00",
        "label": 1
    },
    "2304.03277": {
        "title": "Instruction Tuning with GPT-4",
        "abstract": "Prior work has shown that finetuning large language models (LLMs) using\nmachine-generated instruction-following data enables such models to achieve\nremarkable zero-shot capabilities on new tasks, and no human-written\ninstructions are needed. In this paper, we present the first attempt to use\nGPT-4 to generate instruction-following data for LLM finetuning. Our early\nexperiments on instruction-tuned LLaMA models show that the 52K English and\nChinese instruction-following data generated by GPT-4 leads to superior\nzero-shot performance on new tasks to the instruction-following data generated\nby previous state-of-the-art models. We also collect feedback and comparison\ndata from GPT-4 to enable a comprehensive evaluation and reward model training.\nWe make our data generated using GPT-4 as well as our codebase publicly\navailable.",
        "date": "2023-04-06T17:58:09+00:00",
        "label": 1
    },
    "2112.11446": {
        "title": "Scaling Language Models: Methods, Analysis & Insights from Training Gopher",
        "abstract": "Language modelling provides a step towards intelligent communication systems\nby harnessing large repositories of written human knowledge to better predict\nand understand the world. In this paper, we present an analysis of\nTransformer-based language model performance across a wide range of model\nscales -- from models with tens of millions of parameters up to a 280 billion\nparameter model called Gopher. These models are evaluated on 152 diverse tasks,\nachieving state-of-the-art performance across the majority. Gains from scale\nare largest in areas such as reading comprehension, fact-checking, and the\nidentification of toxic language, but logical and mathematical reasoning see\nless benefit. We provide a holistic analysis of the training dataset and\nmodel's behaviour, covering the intersection of model scale with bias and\ntoxicity. Finally we discuss the application of language models to AI safety\nand the mitigation of downstream harms.",
        "date": "2021-12-08T19:41:47+00:00",
        "label": 1
    },
    "1908.10084": {
        "title": "Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks",
        "abstract": "BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new\nstate-of-the-art performance on sentence-pair regression tasks like semantic\ntextual similarity (STS). However, it requires that both sentences are fed into\nthe network, which causes a massive computational overhead: Finding the most\nsimilar pair in a collection of 10,000 sentences requires about 50 million\ninference computations (~65 hours) with BERT. The construction of BERT makes it\nunsuitable for semantic similarity search as well as for unsupervised tasks\nlike clustering.\n  In this publication, we present Sentence-BERT (SBERT), a modification of the\npretrained BERT network that use siamese and triplet network structures to\nderive semantically meaningful sentence embeddings that can be compared using\ncosine-similarity. This reduces the effort for finding the most similar pair\nfrom 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while\nmaintaining the accuracy from BERT.\n  We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning\ntasks, where it outperforms other state-of-the-art sentence embeddings methods.",
        "date": "2019-08-27T08:50:17+00:00",
        "label": 1
    },
    "2005.04118": {
        "title": "Beyond Accuracy: Behavioral Testing of NLP models with CheckList",
        "abstract": "Although measuring held-out accuracy has been the primary approach to\nevaluate generalization, it often overestimates the performance of NLP models,\nwhile alternative approaches for evaluating models either focus on individual\ntasks or on specific behaviors. Inspired by principles of behavioral testing in\nsoftware engineering, we introduce CheckList, a task-agnostic methodology for\ntesting NLP models. CheckList includes a matrix of general linguistic\ncapabilities and test types that facilitate comprehensive test ideation, as\nwell as a software tool to generate a large and diverse number of test cases\nquickly. We illustrate the utility of CheckList with tests for three tasks,\nidentifying critical failures in both commercial and state-of-art models. In a\nuser study, a team responsible for a commercial sentiment analysis model found\nnew and actionable bugs in an extensively tested model. In another user study,\nNLP practitioners with CheckList created twice as many tests, and found almost\nthree times as many bugs as users without it.",
        "date": "2020-05-08T15:48:31+00:00",
        "label": 1
    },
    "2110.08207": {
        "title": "Multitask Prompted Training Enables Zero-Shot Task Generalization",
        "abstract": "Large language models have recently been shown to attain reasonable zero-shot\ngeneralization on a diverse set of tasks (Brown et al., 2020). It has been\nhypothesized that this is a consequence of implicit multitask learning in\nlanguage models' pretraining (Radford et al., 2019). Can zero-shot\ngeneralization instead be directly induced by explicit multitask learning? To\ntest this question at scale, we develop a system for easily mapping any natural\nlanguage tasks into a human-readable prompted form. We convert a large set of\nsupervised datasets, each with multiple prompts with diverse wording. These\nprompted datasets allow for benchmarking the ability of a model to perform\ncompletely held-out tasks. We fine-tune a pretrained encoder-decoder model\n(Raffel et al., 2020; Lester et al., 2021) on this multitask mixture covering a\nwide variety of tasks. The model attains strong zero-shot performance on\nseveral standard datasets, often outperforming models up to 16x its size.\nFurther, our approach attains strong performance on a subset of tasks from the\nBIG-bench benchmark, outperforming models up to 6x its size. All trained models\nare available at https://github.com/bigscience-workshop/t-zero and all prompts\nare available at https://github.com/bigscience-workshop/promptsource.",
        "date": "2021-10-15T17:08:57+00:00",
        "label": 1
    },
    "2009.07118": {
        "title": "It's Not Just Size That Matters: Small Language Models Are Also Few-Shot Learners",
        "abstract": "When scaled to hundreds of billions of parameters, pretrained language models\nsuch as GPT-3 (Brown et al., 2020) achieve remarkable few-shot performance.\nHowever, enormous amounts of compute are required for training and applying\nsuch big models, resulting in a large carbon footprint and making it difficult\nfor researchers and practitioners to use them. We show that performance similar\nto GPT-3 can be obtained with language models that are much \"greener\" in that\ntheir parameter count is several orders of magnitude smaller. This is achieved\nby converting textual inputs into cloze questions that contain a task\ndescription, combined with gradient-based optimization; exploiting unlabeled\ndata gives further improvements. We identify key factors required for\nsuccessful natural language understanding with small language models.",
        "date": "2020-09-15T14:18:53+00:00",
        "label": 1
    },
    "2112.03052": {
        "title": "Scaling Up Influence Functions",
        "abstract": "We address efficient calculation of influence functions for tracking\npredictions back to the training data. We propose and analyze a new approach to\nspeeding up the inverse Hessian calculation based on Arnoldi iteration. With\nthis improvement, we achieve, to the best of our knowledge, the first\nsuccessful implementation of influence functions that scales to full-size\n(language and vision) Transformer models with several hundreds of millions of\nparameters. We evaluate our approach on image classification and\nsequence-to-sequence tasks with tens to a hundred of millions of training\nexamples. Our code will be available at\nhttps://github.com/google-research/jax-influence.",
        "date": "2021-12-06T13:54:08+00:00",
        "label": 1
    },
    "2306.10165": {
        "title": "Data Selection for Fine-tuning Large Language Models Using Transferred Shapley Values",
        "abstract": "Although Shapley values have been shown to be highly effective for\nidentifying harmful training instances, dataset size and model complexity\nconstraints limit the ability to apply Shapley-based data valuation to\nfine-tuning large pre-trained language models. To address this, we propose\nTS-DShapley, an algorithm that reduces computational cost of Shapley-based data\nvaluation through: 1) an efficient sampling-based method that aggregates\nShapley values computed from subsets for valuation of the entire training set,\nand 2) a value transfer method that leverages value information extracted from\na simple classifier trained using representations from the target language\nmodel. Our experiments applying TS-DShapley to select data for fine-tuning\nBERT-based language models on benchmark natural language understanding (NLU)\ndatasets show that TS-DShapley outperforms existing data selection methods.\nFurther, TS-DShapley can filter fine-tuning data to increase language model\nperformance compared to training with the full fine-tuning dataset.",
        "date": "2023-06-16T20:07:38+00:00",
        "label": 1
    },
    "1708.00489": {
        "title": "Active Learning for Convolutional Neural Networks: A Core-Set Approach",
        "abstract": "Convolutional neural networks (CNNs) have been successfully applied to many\nrecognition and learning tasks using a universal recipe; training a deep model\non a very large dataset of supervised examples. However, this approach is\nrather restrictive in practice since collecting a large set of labeled images\nis very expensive. One way to ease this problem is coming up with smart ways\nfor choosing images to be labelled from a very large collection (ie. active\nlearning).\n  Our empirical study suggests that many of the active learning heuristics in\nthe literature are not effective when applied to CNNs in batch setting.\nInspired by these limitations, we define the problem of active learning as\ncore-set selection, ie. choosing set of points such that a model learned over\nthe selected subset is competitive for the remaining data points. We further\npresent a theoretical result characterizing the performance of any selected\nsubset using the geometry of the datapoints. As an active learning algorithm,\nwe choose the subset which is expected to yield best result according to our\ncharacterization. Our experiments show that the proposed method significantly\noutperforms existing approaches in image classification experiments by a large\nmargin.",
        "date": "2017-08-01T19:50:53+00:00",
        "label": 1
    },
    "2402.14526": {
        "title": "Balanced Data Sampling for Language Model Training with Clustering",
        "abstract": "Data plays a fundamental role in the training of Large Language Models\n(LLMs). While attention has been paid to the collection and composition of\ndatasets, determining the data sampling strategy in training remains an open\nquestion. Most LLMs are trained with a simple strategy, random sampling.\nHowever, this sampling strategy ignores the unbalanced nature of training data\ndistribution, which can be sub-optimal. In this paper, we propose ClusterClip\nSampling to balance the text distribution of training data for better model\ntraining. Specifically, ClusterClip Sampling utilizes data clustering to\nreflect the data distribution of the training set and balances the common\nsamples and rare samples during training based on the cluster results. A\nrepetition clip operation is introduced to mitigate the overfitting issue led\nby samples from certain clusters. Extensive experiments validate the\neffectiveness of ClusterClip Sampling, which outperforms random sampling and\nother cluster-based sampling variants under various training datasets and large\nlanguage models.",
        "date": "2024-02-22T13:20:53+00:00",
        "label": 1
    },
    "2405.14394": {
        "title": "Instruction Tuning With Loss Over Instructions",
        "abstract": "Instruction tuning plays a crucial role in shaping the outputs of language\nmodels (LMs) to desired styles. In this work, we propose a simple yet effective\nmethod, Instruction Modelling (IM), which trains LMs by applying a loss\nfunction to the instruction and prompt part rather than solely to the output\npart. Through experiments across 21 diverse benchmarks, we show that, in many\nscenarios, IM can effectively improve the LM performance on both NLP tasks\n(e.g., MMLU, TruthfulQA, and HumanEval) and open-ended generation benchmarks\n(e.g., MT-Bench and AlpacaEval). Remarkably, in the most advantageous case, IM\nboosts model performance on AlpacaEval 1.0 by over 100%. We identify two key\nfactors influencing the effectiveness of IM: (1) The ratio between instruction\nlength and output length in the training data; and (2) The number of training\nexamples. We observe that IM is especially beneficial when trained on datasets\nwith lengthy instructions paired with brief outputs, or under the Superficial\nAlignment Hypothesis (SAH) where a small amount of training examples are used\nfor instruction tuning. Further analysis substantiates our hypothesis that the\nimprovement can be attributed to reduced overfitting to instruction tuning\ndatasets. Our work provides practical guidance for instruction tuning LMs,\nespecially in low-resource scenarios.",
        "date": "2024-05-23T10:12:03+00:00",
        "label": 1
    },
    "1808.05697": {
        "title": "Deep Bayesian Active Learning for Natural Language Processing: Results of a Large-Scale Empirical Study",
        "abstract": "Several recent papers investigate Active Learning (AL) for mitigating the\ndata dependence of deep learning for natural language processing. However, the\napplicability of AL to real-world problems remains an open question. While in\nsupervised learning, practitioners can try many different methods, evaluating\neach against a validation set before selecting a model, AL affords no such\nluxury. Over the course of one AL run, an agent annotates its dataset\nexhausting its labeling budget. Thus, given a new task, an active learner has\nno opportunity to compare models and acquisition functions. This paper provides\na large scale empirical study of deep active learning, addressing multiple\ntasks and, for each, multiple datasets, multiple models, and a full suite of\nacquisition functions. We find that across all settings, Bayesian active\nlearning by disagreement, using uncertainty estimates provided either by\nDropout or Bayes-by Backprop significantly improves over i.i.d. baselines and\nusually outperforms classic uncertainty sampling.",
        "date": "2018-08-16T22:46:40+00:00",
        "label": 1
    },
    "2205.01497": {
        "title": "Semantic Diversity in Dialogue with Natural Language Inference",
        "abstract": "Generating diverse, interesting responses to chitchat conversations is a\nproblem for neural conversational agents. This paper makes two substantial\ncontributions to improving diversity in dialogue generation. First, we propose\na novel metric which uses Natural Language Inference (NLI) to measure the\nsemantic diversity of a set of model responses for a conversation. We evaluate\nthis metric using an established framework (Tevet and Berant, 2021) and find\nstrong evidence indicating NLI Diversity is correlated with semantic diversity.\nSpecifically, we show that the contradiction relation is more useful than the\nneutral relation for measuring this diversity and that incorporating the NLI\nmodel's confidence achieves state-of-the-art results. Second, we demonstrate\nhow to iteratively improve the semantic diversity of a sampled set of responses\nvia a new generation procedure called Diversity Threshold Generation, which\nresults in an average 137% increase in NLI Diversity compared to standard\ngeneration procedures.",
        "date": "2022-05-03T13:56:32+00:00",
        "label": 1
    },
    "2307.16382": {
        "title": "Does fine-tuning GPT-3 with the OpenAI API leak personally-identifiable information?",
        "abstract": "Machine learning practitioners often fine-tune generative pre-trained models\nlike GPT-3 to improve model performance at specific tasks. Previous works,\nhowever, suggest that fine-tuned machine learning models memorize and emit\nsensitive information from the original fine-tuning dataset. Companies such as\nOpenAI offer fine-tuning services for their models, but no prior work has\nconducted a memorization attack on any closed-source models. In this work, we\nsimulate a privacy attack on GPT-3 using OpenAI's fine-tuning API. Our\nobjective is to determine if personally identifiable information (PII) can be\nextracted from this model. We (1) explore the use of naive prompting methods on\na GPT-3 fine-tuned classification model, and (2) we design a practical word\ngeneration task called Autocomplete to investigate the extent of PII\nmemorization in fine-tuned GPT-3 within a real-world context. Our findings\nreveal that fine-tuning GPT3 for both tasks led to the model memorizing and\ndisclosing critical personally identifiable information (PII) obtained from the\nunderlying fine-tuning dataset. To encourage further research, we have made our\ncodes and datasets publicly available on GitHub at:\nhttps://github.com/albertsun1/gpt3-pii-attacks",
        "date": "2023-07-31T03:17:51+00:00",
        "label": 1
    },
    "2403.05789": {
        "title": "ItD: Large Language Models Can Teach Themselves Induction through Deduction",
        "abstract": "Although Large Language Models (LLMs) are showing impressive performance on a\nwide range of Natural Language Processing tasks, researchers have found that\nthey still have limited ability to conduct induction. Recent works mainly adopt\n``post processes'' paradigms to improve the performance of LLMs on induction\n(e.g., the hypothesis search & refinement methods), but their performance is\nstill constrained by the inherent inductive capability of the LLMs. In this\npaper, we propose a novel framework, Induction through Deduction (ItD), to\nenable the LLMs to teach themselves induction through deduction. The ItD\nframework is composed of two main components: a Deductive Data Generation\nmodule to generate induction data and a Naive Bayesian Induction module to\noptimize the fine-tuning and decoding of LLMs. Our empirical results showcase\nthe effectiveness of ItD on two induction benchmarks, achieving relative\nperformance improvement of 36% and 10% compared with previous state-of-the-art,\nrespectively. Our ablation study verifies the effectiveness of two key modules\nof ItD. We also verify the effectiveness of ItD across different LLMs and\ndeductors. The data and code of this paper can be found at\nhttps://anonymous.4open.science/r/ItD-E844.",
        "date": "2024-03-09T04:20:46+00:00",
        "label": 1
    },
    "2402.12052": {
        "title": "Small Models, Big Insights: Leveraging Slim Proxy Models To Decide When and What to Retrieve for LLMs",
        "abstract": "The integration of large language models (LLMs) and search engines represents\na significant evolution in knowledge acquisition methodologies. However,\ndetermining the knowledge that an LLM already possesses and the knowledge that\nrequires the help of a search engine remains an unresolved issue. Most existing\nmethods solve this problem through the results of preliminary answers or\nreasoning done by the LLM itself, but this incurs excessively high\ncomputational costs. This paper introduces a novel collaborative approach,\nnamely SlimPLM, that detects missing knowledge in LLMs with a slim proxy model,\nto enhance the LLM's knowledge acquisition process. We employ a proxy model\nwhich has far fewer parameters, and take its answers as heuristic answers.\nHeuristic answers are then utilized to predict the knowledge required to answer\nthe user question, as well as the known and unknown knowledge within the LLM.\nWe only conduct retrieval for the missing knowledge in questions that the LLM\ndoes not know. Extensive experimental results on five datasets with two LLMs\ndemonstrate a notable improvement in the end-to-end performance of LLMs in\nquestion-answering tasks, achieving or surpassing current state-of-the-art\nmodels with lower LLM inference costs.",
        "date": "2024-02-19T11:11:08+00:00",
        "label": 1
    },
    "2004.02990": {
        "title": "Evaluating the Evaluation of Diversity in Natural Language Generation",
        "abstract": "Despite growing interest in natural language generation (NLG) models that\nproduce diverse outputs, there is currently no principled method for evaluating\nthe diversity of an NLG system. In this work, we propose a framework for\nevaluating diversity metrics. The framework measures the correlation between a\nproposed diversity metric and a diversity parameter, a single parameter that\ncontrols some aspect of diversity in generated text. For example, a diversity\nparameter might be a binary variable used to instruct crowdsourcing workers to\ngenerate text with either low or high content diversity. We demonstrate the\nutility of our framework by: (a) establishing best practices for eliciting\ndiversity judgments from humans, (b) showing that humans substantially\noutperform automatic metrics in estimating content diversity, and (c)\ndemonstrating that existing methods for controlling diversity by tuning a\n\"decoding parameter\" mostly affect form but not meaning. Our framework can\nadvance the understanding of different diversity metrics, an essential step on\nthe road towards better NLG systems.",
        "date": "2020-04-06T20:44:10+00:00",
        "label": 1
    },
    "1812.05159": {
        "title": "An Empirical Study of Example Forgetting during Deep Neural Network Learning",
        "abstract": "Inspired by the phenomenon of catastrophic forgetting, we investigate the\nlearning dynamics of neural networks as they train on single classification\ntasks. Our goal is to understand whether a related phenomenon occurs when data\ndoes not undergo a clear distributional shift. We define a `forgetting event'\nto have occurred when an individual training example transitions from being\nclassified correctly to incorrectly over the course of learning. Across several\nbenchmark data sets, we find that: (i) certain examples are forgotten with high\nfrequency, and some not at all; (ii) a data set's (un)forgettable examples\ngeneralize across neural architectures; and (iii) based on forgetting dynamics,\na significant fraction of examples can be omitted from the training data set\nwhile still maintaining state-of-the-art generalization performance.",
        "date": "2018-12-12T21:24:15+00:00",
        "label": 1
    },
    "2302.13971": {
        "title": "LLaMA: Open and Efficient Foundation Language Models",
        "abstract": "We introduce LLaMA, a collection of foundation language models ranging from\n7B to 65B parameters. We train our models on trillions of tokens, and show that\nit is possible to train state-of-the-art models using publicly available\ndatasets exclusively, without resorting to proprietary and inaccessible\ndatasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,\nand LLaMA-65B is competitive with the best models, Chinchilla-70B and\nPaLM-540B. We release all our models to the research community.",
        "date": "2023-02-27T17:11:15+00:00",
        "label": 1
    },
    "2307.09288": {
        "title": "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        "abstract": "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.",
        "date": "2023-07-18T14:31:57+00:00",
        "label": 1
    },
    "2402.05123": {
        "title": "A Survey on Data Selection for LLM Instruction Tuning",
        "abstract": "Instruction tuning is a vital step of training large language models (LLM),\nso how to enhance the effect of instruction tuning has received increased\nattention. Existing works indicate that the quality of the dataset is more\ncrucial than the quantity during instruction tuning of LLM. Therefore, recently\na lot of studies focus on exploring the methods of selecting high-quality\nsubset from instruction datasets, aiming to reduce training costs and enhance\nthe instruction-following capabilities of LLMs. This paper presents a\ncomprehensive survey on data selection for LLM instruction tuning. Firstly, we\nintroduce the wildly used instruction datasets. Then, we propose a new taxonomy\nof the data selection methods and provide a detailed introduction of recent\nadvances,and the evaluation strategies and results of data selection methods\nare also elaborated in detail. Finally, we emphasize the open challenges and\npresent new frontiers of this task.",
        "date": "2024-02-04T13:32:01+00:00",
        "label": 1
    },
    "2402.05813": {
        "title": "Selective Forgetting: Advancing Machine Unlearning Techniques and Evaluation in Language Models",
        "abstract": "The aim of this study is to investigate Machine Unlearning (MU), a burgeoning\nfield focused on addressing concerns related to neural models inadvertently\nretaining personal or sensitive data. Here, a novel approach is introduced to\nachieve precise and selective forgetting within language models. Unlike\nprevious methodologies that adopt completely opposing training objectives, this\napproach aims to mitigate adverse effects on language model performance,\nparticularly in generation tasks. Furthermore, two innovative evaluation\nmetrics are proposed: Sensitive Information Extraction Likelihood (S-EL) and\nSensitive Information Memory Accuracy (S-MA), designed to gauge the\neffectiveness of sensitive information elimination. To reinforce the forgetting\nframework, an effective method for annotating sensitive scopes is presented,\ninvolving both online and offline strategies. The online selection mechanism\nleverages language probability scores to ensure computational efficiency, while\nthe offline annotation entails a robust two-stage process based on Large\nLanguage Models (LLMs).",
        "date": "2024-02-08T16:50:01+00:00",
        "label": 1
    },
    "2306.05087": {
        "title": "PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning Optimization",
        "abstract": "Instruction tuning large language models (LLMs) remains a challenging task,\nowing to the complexity of hyperparameter selection and the difficulty involved\nin evaluating the tuned models. To determine the optimal hyperparameters, an\nautomatic, robust, and reliable evaluation benchmark is essential. However,\nestablishing such a benchmark is not a trivial task due to the challenges\nassociated with evaluation accuracy and privacy protection. In response to\nthese challenges, we introduce a judge large language model, named PandaLM,\nwhich is trained to distinguish the superior model given several LLMs.\nPandaLM's focus extends beyond just the objective correctness of responses,\nwhich is the main focus of traditional evaluation datasets. It addresses vital\nsubjective factors such as relative conciseness, clarity, adherence to\ninstructions, comprehensiveness, and formality. To ensure the reliability of\nPandaLM, we collect a diverse human-annotated test dataset, where all contexts\nare generated by humans and labels are aligned with human preferences. Our\nresults indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation\nability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM\nenables the evaluation of LLM to be fairer but with less cost, evidenced by\nsignificant improvements achieved by models tuned through PandaLM compared to\ntheir counterparts trained with default Alpaca's hyperparameters. In addition,\nPandaLM does not depend on API-based evaluations, thus avoiding potential data\nleakage. All resources of PandaLM are released at\nhttps://github.com/WeOpenML/PandaLM.",
        "date": "2023-06-08T10:41:56+00:00",
        "label": 1
    },
    "2212.10560": {
        "title": "Self-Instruct: Aligning Language Models with Self-Generated Instructions",
        "abstract": "Large \"instruction-tuned\" language models (i.e., finetuned to respond to\ninstructions) have demonstrated a remarkable ability to generalize zero-shot to\nnew tasks. Nevertheless, they depend heavily on human-written instruction data\nthat is often limited in quantity, diversity, and creativity, therefore\nhindering the generality of the tuned model. We introduce Self-Instruct, a\nframework for improving the instruction-following capabilities of pretrained\nlanguage models by bootstrapping off their own generations. Our pipeline\ngenerates instructions, input, and output samples from a language model, then\nfilters invalid or similar ones before using them to finetune the original\nmodel. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute\nimprovement over the original model on Super-NaturalInstructions, on par with\nthe performance of InstructGPT-001, which was trained with private user data\nand human annotations. For further evaluation, we curate a set of\nexpert-written instructions for novel tasks, and show through human evaluation\nthat tuning GPT3 with Self-Instruct outperforms using existing public\ninstruction datasets by a large margin, leaving only a 5% absolute gap behind\nInstructGPT-001. Self-Instruct provides an almost annotation-free method for\naligning pre-trained language models with instructions, and we release our\nlarge synthetic dataset to facilitate future studies on instruction tuning. Our\ncode and data are available at https://github.com/yizhongw/self-instruct.",
        "date": "2022-12-20T18:59:19+00:00",
        "label": 1
    },
    "2307.12966": {
        "title": "Aligning Large Language Models with Human: A Survey",
        "abstract": "Large Language Models (LLMs) trained on extensive textual corpora have\nemerged as leading solutions for a broad array of Natural Language Processing\n(NLP) tasks. Despite their notable performance, these models are prone to\ncertain limitations such as misunderstanding human instructions, generating\npotentially biased content, or factually incorrect (hallucinated) information.\nHence, aligning LLMs with human expectations has become an active area of\ninterest within the research community. This survey presents a comprehensive\noverview of these alignment technologies, including the following aspects. (1)\nData collection: the methods for effectively collecting high-quality\ninstructions for LLM alignment, including the use of NLP benchmarks, human\nannotations, and leveraging strong LLMs. (2) Training methodologies: a detailed\nreview of the prevailing training methods employed for LLM alignment. Our\nexploration encompasses Supervised Fine-tuning, both Online and Offline human\npreference training, along with parameter-efficient training mechanisms. (3)\nModel Evaluation: the methods for evaluating the effectiveness of these\nhuman-aligned LLMs, presenting a multifaceted approach towards their\nassessment. In conclusion, we collate and distill our findings, shedding light\non several promising future research avenues in the field. This survey,\ntherefore, serves as a valuable resource for anyone invested in understanding\nand advancing the alignment of LLMs to better suit human-oriented tasks and\nexpectations. An associated GitHub link collecting the latest papers is\navailable at https://github.com/GaryYufei/AlignLLMHumanSurvey.",
        "date": "2023-07-24T17:44:58+00:00",
        "label": 1
    },
    "2109.01652": {
        "title": "Finetuned Language Models Are Zero-Shot Learners",
        "abstract": "This paper explores a simple method for improving the zero-shot learning\nabilities of language models. We show that instruction tuning -- finetuning\nlanguage models on a collection of tasks described via instructions --\nsubstantially improves zero-shot performance on unseen tasks.\n  We take a 137B parameter pretrained language model and instruction-tune it on\nover 60 NLP tasks verbalized via natural language instruction templates. We\nevaluate this instruction-tuned model, which we call FLAN, on unseen task\ntypes. FLAN substantially improves the performance of its unmodified\ncounterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we\nevaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,\nBoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number\nof finetuning datasets, model scale, and natural language instructions are key\nto the success of instruction tuning.",
        "date": "2021-09-03T17:55:52+00:00",
        "label": 1
    },
    "2402.09739": {
        "title": "QuRating: Selecting High-Quality Data for Training Language Models",
        "abstract": "Selecting high-quality pre-training data is important for creating capable\nlanguage models, but existing methods rely on simple heuristics. We introduce\nQuRating, a method for selecting pre-training data that can capture human\nintuitions about data quality. In this paper, we investigate four qualities -\nwriting style, required expertise, facts & trivia, and educational value - and\nfind that LLMs are able to discern these qualities, especially when making\npairwise judgments of texts. We train a QuRater model to learn scalar ratings\nfrom pairwise judgments, and use it to annotate a 260B training corpus with\nquality ratings for each of the four criteria. In our experiments, we select\n30B tokens according to the different quality ratings and train 1.3B-parameter\nlanguage models on the selected data. We find that it is important to balance\nquality and diversity. When we sample using quality ratings as logits over\ndocuments, our models obtain lower perplexity and stronger in-context learning\nperformance than baselines. Our best model is based on educational value and\nperforms similarly to a model trained with uniform sampling for 50% more steps.\nBeyond data selection, we use the quality ratings to construct a training\ncurriculum which improves performance without changing the training dataset. We\nextensively analyze the quality ratings and discuss their characteristics,\nbiases, and wider implications.",
        "date": "2024-02-15T06:36:07+00:00",
        "label": 1
    },
    "1910.03771": {
        "title": "HuggingFace's Transformers: State-of-the-art Natural Language Processing",
        "abstract": "Recent progress in natural language processing has been driven by advances in\nboth model architecture and model pretraining. Transformer architectures have\nfacilitated building higher-capacity models and pretraining has made it\npossible to effectively utilize this capacity for a wide variety of tasks.\n\\textit{Transformers} is an open-source library with the goal of opening up\nthese advances to the wider machine learning community. The library consists of\ncarefully engineered state-of-the art Transformer architectures under a unified\nAPI. Backing this library is a curated collection of pretrained models made by\nand available for the community. \\textit{Transformers} is designed to be\nextensible by researchers, simple for practitioners, and fast and robust in\nindustrial deployments. The library is available at\n\\url{https://github.com/huggingface/transformers}.",
        "date": "2019-10-09T03:23:22+00:00",
        "label": 1
    },
    "2311.08182": {
        "title": "Self-Evolved Diverse Data Sampling for Efficient Instruction Tuning",
        "abstract": "Enhancing the instruction-following ability of Large Language Models (LLMs)\nprimarily demands substantial instruction-tuning datasets. However, the sheer\nvolume of these imposes a considerable computational burden and annotation\ncost. To investigate a label-efficient instruction tuning method that allows\nthe model itself to actively sample subsets that are equally or even more\neffective, we introduce a self-evolving mechanism DiverseEvol. In this process,\na model iteratively augments its training subset to refine its own performance,\nwithout requiring any intervention from humans or more advanced LLMs. The key\nto our data sampling technique lies in the enhancement of diversity in the\nchosen subsets, as the model selects new data points most distinct from any\nexisting ones according to its current embedding space. Extensive experiments\nacross three datasets and benchmarks demonstrate the effectiveness of\nDiverseEvol. Our models, trained on less than 8% of the original dataset,\nmaintain or improve performance compared with finetuning on full data. We also\nprovide empirical evidence to analyze the importance of diversity in\ninstruction data and the iterative scheme as opposed to one-time sampling. Our\ncode is publicly available at https://github.com/OFA-Sys/DiverseEvol.git.",
        "date": "2023-11-14T14:10:40+00:00",
        "label": 1
    },
    "2310.06694": {
        "title": "Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning",
        "abstract": "The popularity of LLaMA (Touvron et al., 2023a;b) and other recently emerged\nmoderate-sized large language models (LLMs) highlights the potential of\nbuilding smaller yet powerful LLMs. Regardless, the cost of training such\nmodels from scratch on trillions of tokens remains high. In this work, we study\nstructured pruning as an effective means to develop smaller LLMs from\npre-trained, larger models. Our approach employs two key techniques: (1)\ntargeted structured pruning, which prunes a larger model to a specified target\nshape by removing layers, heads, and intermediate and hidden dimensions in an\nend-to-end manner, and (2) dynamic batch loading, which dynamically updates the\ncomposition of sampled data in each training batch based on varying losses\nacross different domains. We demonstrate the efficacy of our approach by\npresenting the Sheared-LLaMA series, pruning the LLaMA2-7B model down to 1.3B\nand 2.7B parameters. Sheared-LLaMA models outperform state-of-the-art\nopen-source models of equivalent sizes, such as Pythia, INCITE, OpenLLaMA and\nthe concurrent TinyLlama models, on a wide range of downstream and instruction\ntuning evaluations, while requiring only 3% of compute compared to training\nsuch models from scratch. This work provides compelling evidence that\nleveraging existing LLMs with structured pruning is a far more cost-effective\napproach for building competitive small-scale LLMs",
        "date": "2023-10-10T15:13:30+00:00",
        "label": 1
    },
    "2402.04333": {
        "title": "LESS: Selecting Influential Data for Targeted Instruction Tuning",
        "abstract": "Instruction tuning has unlocked powerful capabilities in large language\nmodels (LLMs), effectively using combined datasets to develop generalpurpose\nchatbots. However, real-world applications often require a specialized suite of\nskills (e.g., reasoning). The challenge lies in identifying the most relevant\ndata from these extensive datasets to effectively develop specific\ncapabilities, a setting we frame as targeted instruction tuning. We propose\nLESS, an optimizer-aware and practically efficient algorithm to effectively\nestimate data influences and perform Low-rank gradiEnt Similarity Search for\ninstruction data selection. Crucially, LESS adapts existing influence\nformulations to work with the Adam optimizer and variable-length instruction\ndata. LESS first constructs a highly reusable and transferable gradient\ndatastore with low-dimensional gradient features and then selects examples\nbased on their similarity to few-shot examples embodying a specific capability.\nExperiments show that training on a LESS-selected 5% of the data can often\noutperform training on the full dataset across diverse downstream tasks.\nFurthermore, the selected data is highly transferable: smaller models can be\nleveraged to select useful data for larger models and models from different\nfamilies. Our qualitative analysis shows that our method goes beyond surface\nform cues to identify data that exemplifies the necessary reasoning skills for\nthe intended downstream application.",
        "date": "2024-02-06T19:18:04+00:00",
        "label": 1
    },
    "2111.02080": {
        "title": "An Explanation of In-context Learning as Implicit Bayesian Inference",
        "abstract": "Large language models (LMs) such as GPT-3 have the surprising ability to do\nin-context learning, where the model learns to do a downstream task simply by\nconditioning on a prompt consisting of input-output examples. The LM learns\nfrom these examples without being explicitly pretrained to learn. Thus, it is\nunclear what enables in-context learning. In this paper, we study how\nin-context learning can emerge when pretraining documents have long-range\ncoherence. Here, the LM must infer a latent document-level concept to generate\ncoherent next tokens during pretraining. At test time, in-context learning\noccurs when the LM also infers a shared latent concept between examples in a\nprompt. We prove when this occurs despite a distribution mismatch between\nprompts and pretraining data in a setting where the pretraining distribution is\na mixture of HMMs. In contrast to messy large-scale datasets used to train LMs\ncapable of in-context learning, we generate a small-scale synthetic dataset\n(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond\nthe theory, experiments on GINC exhibit large-scale real-world phenomena\nincluding improved in-context performance with model scaling (despite the same\npretraining loss), sensitivity to example order, and instances where zero-shot\nis better than few-shot in-context learning.",
        "date": "2021-11-03T09:12:33+00:00",
        "label": 1
    },
    "2304.12244": {
        "title": "WizardLM: Empowering Large Language Models to Follow Complex Instructions",
        "abstract": "Training large language models (LLMs) with open-domain instruction following\ndata brings colossal success. However, manually creating such instruction data\nis very time-consuming and labor-intensive. Moreover, humans may struggle to\nproduce high-complexity instructions. In this paper, we show an avenue for\ncreating large amounts of instruction data with varying levels of complexity\nusing LLM instead of humans. Starting with an initial set of instructions, we\nuse our proposed Evol-Instruct to rewrite them step by step into more complex\ninstructions. Then, we mix all generated instruction data to fine-tune LLaMA.\nWe call the resulting model WizardLM. Human evaluations on a\ncomplexity-balanced test bed and Vicuna's testset show that instructions from\nEvol-Instruct are superior to human-created ones. By analyzing the human\nevaluation results of the high complexity part, we demonstrate that outputs\nfrom our WizardLM are preferred to outputs from OpenAI ChatGPT. In GPT-4\nautomatic evaluation, WizardLM achieves more than 90\\% capacity of ChatGPT on\n17 out of 29 skills. Even though WizardLM still lags behind ChatGPT in some\naspects, our findings suggest that fine-tuning with AI-evolved instructions is\na promising direction for enhancing LLMs. Our code and data are public at\nhttps://github.com/nlpxucan/WizardLM",
        "date": "2023-04-24T16:31:06+00:00",
        "label": 1
    },
    "2312.11508": {
        "title": "Rethinking the Instruction Quality: LIFT is What You Need",
        "abstract": "Instruction tuning, a specialized technique to enhance large language model\n(LLM) performance via instruction datasets, relies heavily on the quality of\nemployed data. Existing quality improvement methods alter instruction data\nthrough dataset expansion or curation. However, the expansion method risks data\nredundancy, potentially compromising LLM performance, while the curation\napproach confines the LLM's potential to the original dataset. Our aim is to\nsurpass the original data quality without encountering these shortcomings. To\nachieve this, we propose LIFT (LLM Instruction Fusion Transfer), a novel and\nversatile paradigm designed to elevate the instruction quality to new heights.\nLIFT strategically broadens data distribution to encompass more high-quality\nsubspaces and eliminates redundancy, concentrating on high-quality segments\nacross overall data subspaces. Experimental results demonstrate that, even with\na limited quantity of high-quality instruction data selected by our paradigm,\nLLMs not only consistently uphold robust performance across various tasks but\nalso surpass some state-of-the-art results, highlighting the significant\nimprovement in instruction quality achieved by our paradigm.",
        "date": "2023-12-12T03:30:21+00:00",
        "label": 1
    },
    "2406.08464": {
        "title": "Magpie: Alignment Data Synthesis from Scratch by Prompting Aligned LLMs with Nothing",
        "abstract": "High-quality instruction data is critical for aligning large language models\n(LLMs). Although some models, such as Llama-3-Instruct, have open weights,\ntheir alignment data remain private, which hinders the democratization of AI.\nHigh human labor costs and a limited, predefined scope for prompting prevent\nexisting open-source data creation methods from scaling effectively,\npotentially limiting the diversity and quality of public alignment datasets. Is\nit possible to synthesize high-quality instruction data at scale by extracting\nit directly from an aligned LLM? We present a self-synthesis method for\ngenerating large-scale alignment data named Magpie. Our key observation is that\naligned LLMs like Llama-3-Instruct can generate a user query when we input only\nthe left-side templates up to the position reserved for user messages, thanks\nto their auto-regressive nature. We use this method to prompt Llama-3-Instruct\nand generate 4 million instructions along with their corresponding responses.\nWe perform a comprehensive analysis of the extracted data and select 300K\nhigh-quality instances. To compare Magpie data with other public instruction\ndatasets, we fine-tune Llama-3-8B-Base with each dataset and evaluate the\nperformance of the fine-tuned models. Our results indicate that in some tasks,\nmodels fine-tuned with Magpie perform comparably to the official\nLlama-3-8B-Instruct, despite the latter being enhanced with 10 million data\npoints through supervised fine-tuning (SFT) and subsequent feedback learning.\nWe also show that using Magpie solely for SFT can surpass the performance of\nprevious public datasets utilized for both SFT and preference optimization,\nsuch as direct preference optimization with UltraFeedback. This advantage is\nevident on alignment benchmarks such as AlpacaEval, ArenaHard, and WildBench.",
        "date": "2024-06-12T17:52:30+00:00",
        "label": 1
    },
    "2407.10671": {
        "title": "Qwen2 Technical Report",
        "abstract": "This report introduces the Qwen2 series, the latest addition to our large\nlanguage models and large multimodal models. We release a comprehensive suite\nof foundational and instruction-tuned language models, encompassing a parameter\nrange from 0.5 to 72 billion, featuring dense models and a Mixture-of-Experts\nmodel. Qwen2 surpasses most prior open-weight models, including its predecessor\nQwen1.5, and exhibits competitive performance relative to proprietary models\nacross diverse benchmarks on language understanding, generation, multilingual\nproficiency, coding, mathematics, and reasoning.\n  The flagship model, Qwen2-72B, showcases remarkable performance: 84.2 on\nMMLU, 37.9 on GPQA, 64.6 on HumanEval, 89.5 on GSM8K, and 82.4 on BBH as a base\nlanguage model. The instruction-tuned variant, Qwen2-72B-Instruct, attains 9.1\non MT-Bench, 48.1 on Arena-Hard, and 35.7 on LiveCodeBench. Moreover, Qwen2\ndemonstrates robust multilingual capabilities, proficient in approximately 30\nlanguages, spanning English, Chinese, Spanish, French, German, Arabic, Russian,\nKorean, Japanese, Thai, Vietnamese, and more, underscoring its versatility and\nglobal reach.\n  To foster community innovation and accessibility, we have made the Qwen2\nmodel weights openly available on Hugging Face and ModelScope, and the\nsupplementary materials including example code on GitHub. These platforms also\ninclude resources for quantization, fine-tuning, and deployment, facilitating a\nwide range of applications and research endeavors.",
        "date": "2024-07-15T12:35:42+00:00",
        "label": 1
    },
    "2202.02842": {
        "title": "Evaluating natural language processing models with generalization metrics that do not need access to any training or testing data",
        "abstract": "Selecting suitable architecture parameters and training hyperparameters is\nessential for enhancing machine learning (ML) model performance. Several recent\nempirical studies conduct large-scale correlational analysis on neural networks\n(NNs) to search for effective \\emph{generalization metrics} that can guide this\ntype of model selection. Effective metrics are typically expected to correlate\nstrongly with test performance. In this paper, we expand on prior analyses by\nexamining generalization-metric-based model selection with the following\nobjectives: (i) focusing on natural language processing (NLP) tasks, as prior\nwork primarily concentrates on computer vision (CV) tasks; (ii) considering\nmetrics that directly predict \\emph{test error} instead of the\n\\emph{generalization gap}; (iii) exploring metrics that do not need access to\ndata to compute. From these objectives, we are able to provide the first model\nselection results on large pretrained Transformers from Huggingface using\ngeneralization metrics. Our analyses consider (I) hundreds of Transformers\ntrained in different settings, in which we systematically vary the amount of\ndata, the model size and the optimization hyperparameters, (II) a total of 51\npretrained Transformers from eight families of Huggingface NLP models,\nincluding GPT2, BERT, etc., and (III) a total of 28 existing and novel\ngeneralization metrics. Despite their niche status, we find that metrics\nderived from the heavy-tail (HT) perspective are particularly useful in NLP\ntasks, exhibiting stronger correlations than other, more popular metrics. To\nfurther examine these metrics, we extend prior formulations relying on power\nlaw (PL) spectral distributions to exponential (EXP) and\nexponentially-truncated power law (E-TPL) families.",
        "date": "2022-02-06T20:07:35+00:00",
        "label": 1
    },
    "2311.09006": {
        "title": "Data Similarity is Not Enough to Explain Language Model Performance",
        "abstract": "Large language models achieve high performance on many but not all downstream\ntasks. The interaction between pretraining data and task data is commonly\nassumed to determine this variance: a task with data that is more similar to a\nmodel's pretraining data is assumed to be easier for that model. We test\nwhether distributional and example-specific similarity measures (embedding-,\ntoken- and model-based) correlate with language model performance through a\nlarge-scale comparison of the Pile and C4 pretraining datasets with downstream\nbenchmarks. Similarity correlates with performance for multilingual datasets,\nbut in other benchmarks, we surprisingly find that similarity metrics are not\ncorrelated with accuracy or even each other. This suggests that the\nrelationship between pretraining data and downstream tasks is more complex than\noften assumed.",
        "date": "2023-11-15T14:48:08+00:00",
        "label": 1
    },
    "2402.17400": {
        "title": "Investigating Continual Pretraining in Large Language Models: Insights and Implications",
        "abstract": "This paper studies the evolving domain of Continual Learning (CL) in large\nlanguage models (LLMs), with a focus on developing strategies for efficient and\nsustainable training. Our primary emphasis is on continual domain-adaptive\npretraining, a process designed to equip LLMs with the ability to integrate new\ninformation from various domains while retaining previously learned knowledge\nand enhancing cross-domain knowledge transfer without relying on\ndomain-specific identification. Unlike previous studies, which mostly\nconcentrate on a limited selection of tasks or domains and primarily aim to\naddress the issue of forgetting, our research evaluates the adaptability and\ncapabilities of LLMs to changing data landscapes in practical scenarios. To\nthis end, we introduce a new benchmark designed to measure the adaptability of\nLLMs to these evolving data environments, offering a comprehensive framework\nfor evaluation. We examine the impact of model size on learning efficacy and\nforgetting, as well as how the progression and similarity of emerging domains\naffect the knowledge transfer within these models. Our findings uncover several\nkey insights: (i) when the sequence of domains shows semantic similarity,\ncontinual pretraining enables LLMs to better specialize in the current domain\ncompared to stand-alone fine-tuning, (ii) training across a diverse range of\ndomains enhances both backward and forward knowledge transfer, and (iii)\nsmaller models are particularly sensitive to continual pretraining, showing the\nmost significant rates of both forgetting and learning. We posit that our\nresearch marks a shift towards establishing a more realistic benchmark for\ninvestigating CL in LLMs, and has the potential to play a key role in guiding\nthe direction of future research in the field.",
        "date": "2024-02-27T10:47:24+00:00",
        "label": 1
    },
    "2406.06046": {
        "title": "MATES: Model-Aware Data Selection for Efficient Pretraining with Data Influence Models",
        "abstract": "Pretraining data selection has the potential to improve language model\npretraining efficiency by utilizing higher-quality data from massive web data\ncorpora. Current data selection methods, which rely on either hand-crafted\nrules or larger reference models, are conducted statically and do not capture\nthe evolving data preferences during pretraining. In this paper, we introduce\nmodel-aware data selection with data influence models (MATES), where a data\ninfluence model continuously adapts to the evolving data preferences of the\npretraining model and then selects the data most effective for the current\npretraining progress. Specifically, we fine-tune a small data influence model\nto approximate oracle data preference signals collected by locally probing the\npretraining model and to select data accordingly for the next pretraining\nstage. Experiments on Pythia and the C4 dataset demonstrate that MATES\nsignificantly outperforms random data selection on extensive downstream tasks\nin both zero- and few-shot settings. It doubles the gains achieved by recent\ndata selection approaches that leverage larger reference models and reduces the\ntotal FLOPs required to reach certain performances by half. Further analysis\nvalidates the ever-changing data preferences of pretraining models and the\neffectiveness of our data influence models to capture them. Our code is\nopen-sourced at https://github.com/cxcscmu/MATES.",
        "date": "2024-06-10T06:27:42+00:00",
        "label": 1
    },
    "2310.07641": {
        "title": "Evaluating Large Language Models at Evaluating Instruction Following",
        "abstract": "As research in large language models (LLMs) continues to accelerate,\nLLM-based evaluation has emerged as a scalable and cost-effective alternative\nto human evaluations for comparing the ever increasing list of models. This\npaper investigates the efficacy of these ``LLM evaluators'', particularly in\nusing them to assess instruction following, a metric that gauges how closely\ngenerated text adheres to the given instruction. We introduce a challenging\nmeta-evaluation benchmark, LLMBar, designed to test the ability of an LLM\nevaluator in discerning instruction-following outputs. The authors manually\ncurated 419 pairs of outputs, one adhering to instructions while the other\ndiverging, yet may possess deceptive qualities that mislead an LLM evaluator,\ne.g., a more engaging tone. Contrary to existing meta-evaluation, we discover\nthat different evaluators (i.e., combinations of LLMs and prompts) exhibit\ndistinct performance on LLMBar and even the highest-scoring ones have\nsubstantial room for improvement. We also present a novel suite of prompting\nstrategies that further close the gap between LLM and human evaluators. With\nLLMBar, we hope to offer more insight into LLM evaluators and foster future\nresearch in developing better instruction-following models.",
        "date": "2023-10-11T16:38:11+00:00",
        "label": 1
    },
    "2303.10158": {
        "title": "Data-centric Artificial Intelligence: A Survey",
        "abstract": "Artificial Intelligence (AI) is making a profound impact in almost every\ndomain. A vital enabler of its great success is the availability of abundant\nand high-quality data for building machine learning models. Recently, the role\nof data in AI has been significantly magnified, giving rise to the emerging\nconcept of data-centric AI. The attention of researchers and practitioners has\ngradually shifted from advancing model design to enhancing the quality and\nquantity of the data. In this survey, we discuss the necessity of data-centric\nAI, followed by a holistic view of three general data-centric goals (training\ndata development, inference data development, and data maintenance) and the\nrepresentative methods. We also organize the existing literature from\nautomation and collaboration perspectives, discuss the challenges, and tabulate\nthe benchmarks for various tasks. We believe this is the first comprehensive\nsurvey that provides a global view of a spectrum of tasks across various stages\nof the data lifecycle. We hope it can help the readers efficiently grasp a\nbroad picture of this field, and equip them with the techniques and further\nresearch ideas to systematically engineer data for building AI systems. A\ncompanion list of data-centric AI resources will be regularly updated on\nhttps://github.com/daochenzha/data-centric-AI",
        "date": "2023-03-17T17:44:56+00:00",
        "label": 1
    },
    "2407.15235": {
        "title": "TAGCOS: Task-agnostic Gradient Clustered Coreset Selection for Instruction Tuning Data",
        "abstract": "Instruction tuning has achieved unprecedented success in NLP, turning large\nlanguage models into versatile chatbots. However, the increasing variety and\nvolume of instruction datasets demand significant computational resources. To\naddress this, it is essential to extract a small and highly informative subset\n(i.e., Coreset) that achieves comparable performance to the full dataset.\nAchieving this goal poses non-trivial challenges: 1) data selection requires\naccurate data representations that reflect the training samples' quality, 2)\nconsidering the diverse nature of instruction datasets, and 3) ensuring the\nefficiency of the coreset selection algorithm for large models. To address\nthese challenges, we propose Task-Agnostic Gradient Clustered COreset Selection\n(TAGCOS). Specifically, we leverage sample gradients as the data\nrepresentations, perform clustering to group similar data, and apply an\nefficient greedy algorithm for coreset selection. Experimental results show\nthat our algorithm, selecting only 5% of the data, surpasses other unsupervised\nmethods and achieves performance close to that of the full dataset.",
        "date": "2024-07-21T17:59:20+00:00",
        "label": 1
    },
    "2310.10873": {
        "title": "IDEAL: Influence-Driven Selective Annotations Empower In-Context Learners in Large Language Models",
        "abstract": "In-context learning is a promising paradigm that utilizes in-context examples\nas prompts for the predictions of large language models. These prompts are\ncrucial for achieving strong performance. However, since the prompts need to be\nsampled from a large volume of annotated examples, finding the right prompt may\nresult in high annotation costs. To address this challenge, this paper\nintroduces an influence-driven selective annotation method that aims to\nminimize annotation costs while improving the quality of in-context examples.\nThe essence of our method is to select a pivotal subset from a large-scale\nunlabeled data pool to annotate for the subsequent sampling of prompts.\nSpecifically, a directed graph is first constructed to represent unlabeled\ndata. Afterward, the influence of candidate unlabeled subsets is quantified\nwith a diffusion process. A simple yet effective greedy algorithm for unlabeled\ndata selection is lastly introduced. It iteratively selects the data if it\nprovides a maximum marginal gain with respect to quantified influence. Compared\nwith previous efforts on selective annotations, our influence-driven method\nworks in an end-to-end manner, avoids an intractable explicit balance between\ndata diversity and representativeness, and enjoys theoretical support.\nExperiments confirm the superiority of the proposed method on various\nbenchmarks, achieving better performance under lower time consumption during\nsubset selection. The project page is available at\nhttps://skzhang1.github.io/IDEAL/.",
        "date": "2023-10-16T22:53:54+00:00",
        "label": 1
    },
    "2308.10792": {
        "title": "Instruction Tuning for Large Language Models: A Survey",
        "abstract": "This paper surveys research works in the quickly advancing field of\ninstruction tuning (IT), a crucial technique to enhance the capabilities and\ncontrollability of large language models (LLMs). Instruction tuning refers to\nthe process of further training LLMs on a dataset consisting of\n\\textsc{(instruction, output)} pairs in a supervised fashion, which bridges the\ngap between the next-word prediction objective of LLMs and the users' objective\nof having LLMs adhere to human instructions. In this work, we make a systematic\nreview of the literature, including the general methodology of IT, the\nconstruction of IT datasets, the training of IT models, and applications to\ndifferent modalities, domains and applications, along with an analysis on\naspects that influence the outcome of IT (e.g., generation of instruction\noutputs, size of the instruction dataset, etc). We also review the potential\npitfalls of IT along with criticism against it, along with efforts pointing out\ncurrent deficiencies of existing strategies and suggest some avenues for\nfruitful research. Project page: github.com/xiaoya-li/Instruction-Tuning-Survey",
        "date": "2023-08-21T15:35:16+00:00",
        "label": 1
    },
    "1904.09675": {
        "title": "BERTScore: Evaluating Text Generation with BERT",
        "abstract": "We propose BERTScore, an automatic evaluation metric for text generation.\nAnalogously to common metrics, BERTScore computes a similarity score for each\ntoken in the candidate sentence with each token in the reference sentence.\nHowever, instead of exact matches, we compute token similarity using contextual\nembeddings. We evaluate using the outputs of 363 machine translation and image\ncaptioning systems. BERTScore correlates better with human judgments and\nprovides stronger model selection performance than existing metrics. Finally,\nwe use an adversarial paraphrase detection task to show that BERTScore is more\nrobust to challenging examples when compared to existing metrics.",
        "date": "2019-04-21T23:08:53+00:00",
        "label": 1
    },
    "2406.06326": {
        "title": "Self-Tuning: Instructing LLMs to Effectively Acquire New Knowledge through Self-Teaching",
        "abstract": "Large language models (LLMs) often struggle to provide up-to-date information\ndue to their one-time training and the constantly evolving nature of the world.\nTo keep LLMs current, existing approaches typically involve continued\npre-training on new documents. However, they frequently face difficulties in\nextracting stored knowledge. Motivated by the remarkable success of the Feynman\nTechnique in efficient human learning, we introduce Self-Tuning, a learning\nframework aimed at improving an LLM's ability to effectively acquire new\nknowledge from raw documents through self-teaching. Specifically, we develop a\nSelf-Teaching strategy that augments the documents with a set of\nknowledge-intensive tasks created in a self-supervised manner, focusing on\nthree crucial aspects: memorization, comprehension, and self-reflection. In\naddition, we introduce three Wiki-Newpages-2023-QA datasets to facilitate an\nin-depth analysis of an LLM's knowledge acquisition ability concerning\nmemorization, extraction, and reasoning. Extensive experimental results on\nLlama2 family models reveal that Self-Tuning consistently exhibits superior\nperformance across all knowledge acquisition tasks and excels in preserving\nprevious knowledge.",
        "date": "2024-06-10T14:42:20+00:00",
        "label": 1
    },
    "2006.05929": {
        "title": "Dataset Condensation with Gradient Matching",
        "abstract": "As the state-of-the-art machine learning methods in many fields rely on\nlarger datasets, storing datasets and training models on them become\nsignificantly more expensive. This paper proposes a training set synthesis\ntechnique for data-efficient learning, called Dataset Condensation, that learns\nto condense large dataset into a small set of informative synthetic samples for\ntraining deep neural networks from scratch. We formulate this goal as a\ngradient matching problem between the gradients of deep neural network weights\nthat are trained on the original and our synthetic data. We rigorously evaluate\nits performance in several computer vision benchmarks and demonstrate that it\nsignificantly outperforms the state-of-the-art methods. Finally we explore the\nuse of our method in continual learning and neural architecture search and\nreport promising gains when limited memory and computations are available.",
        "date": "2020-06-10T16:30:52+00:00",
        "label": 1
    },
    "2407.12874": {
        "title": "SELF-GUIDE: Better Task-Specific Instruction Following via Self-Synthetic Finetuning",
        "abstract": "Large language models (LLMs) hold the promise of solving diverse tasks when\nprovided with appropriate natural language prompts. However, prompting often\nleads models to make predictions with lower accuracy compared to finetuning a\nmodel with ample training data. On the other hand, while finetuning LLMs on\ntask-specific data generally improves their performance, abundant annotated\ndatasets are not available for all tasks. Previous work has explored generating\ntask-specific data from state-of-the-art LLMs and using this data to finetune\nsmaller models, but this approach requires access to a language model other\nthan the one being trained, which introduces cost, scalability challenges, and\nlegal hurdles associated with continuously relying on more powerful LLMs. In\nresponse to these, we propose SELF-GUIDE, a multi-stage mechanism in which we\nsynthesize task-specific input-output pairs from the student LLM, then use\nthese input-output pairs to finetune the student LLM itself. In our empirical\nevaluation of the Natural Instructions V2 benchmark, we find that SELF-GUIDE\nimproves the performance of LLM by a substantial margin. Specifically, we\nreport an absolute improvement of approximately 15% for classification tasks\nand 18% for generation tasks in the benchmark's metrics. This sheds light on\nthe promise of self-synthesized data guiding LLMs towards becoming\ntask-specific experts without any external learning signals.",
        "date": "2024-07-16T04:41:58+00:00",
        "label": 1
    },
    "2407.08188": {
        "title": "Position: Measure Dataset Diversity, Don't Just Claim It",
        "abstract": "Machine learning (ML) datasets, often perceived as neutral, inherently\nencapsulate abstract and disputed social constructs. Dataset curators\nfrequently employ value-laden terms such as diversity, bias, and quality to\ncharacterize datasets. Despite their prevalence, these terms lack clear\ndefinitions and validation. Our research explores the implications of this\nissue by analyzing \"diversity\" across 135 image and text datasets. Drawing from\nsocial sciences, we apply principles from measurement theory to identify\nconsiderations and offer recommendations for conceptualizing, operationalizing,\nand evaluating diversity in datasets. Our findings have broader implications\nfor ML research, advocating for a more nuanced and precise approach to handling\nvalue-laden properties in dataset construction.",
        "date": "2024-07-11T05:13:27+00:00",
        "label": 1
    },
    "2403.13233": {
        "title": "Technical Report: Competition Solution For BetterMixture",
        "abstract": "In the era of flourishing large-scale models, the challenge of selecting and\noptimizing datasets from the vast and complex sea of data, to enhance the\nperformance of large language models within the constraints of limited\ncomputational resources, has become paramount. This paper details our solution\nfor the BetterMixture challenge, which focuses on the fine-tuning data mixing\nfor large language models. Our approach, which secured third place,\nincorporates data deduplication, low-level and high-level quality filtering,\nand diversity selection. The foundation of our solution is Ke-Data-Juicer, an\nextension of Data-Juicer, demonstrating its robust capabilities in handling and\noptimizing data for large language models.",
        "date": "2024-03-20T01:46:06+00:00",
        "label": 1
    },
    "2210.07197": {
        "title": "Towards a Unified Multi-Dimensional Evaluator for Text Generation",
        "abstract": "Multi-dimensional evaluation is the dominant paradigm for human evaluation in\nNatural Language Generation (NLG), i.e., evaluating the generated text from\nmultiple explainable dimensions, such as coherence and fluency. However,\nautomatic evaluation in NLG is still dominated by similarity-based metrics, and\nwe lack a reliable framework for a more comprehensive evaluation of advanced\nmodels. In this paper, we propose a unified multi-dimensional evaluator UniEval\nfor NLG. We re-frame NLG evaluation as a Boolean Question Answering (QA) task,\nand by guiding the model with different questions, we can use one evaluator to\nevaluate from multiple dimensions. Furthermore, thanks to the unified Boolean\nQA format, we are able to introduce an intermediate learning phase that enables\nUniEval to incorporate external knowledge from multiple related tasks and gain\nfurther improvement. Experiments on three typical NLG tasks show that UniEval\ncorrelates substantially better with human judgments than existing metrics.\nSpecifically, compared to the top-performing unified evaluators, UniEval\nachieves a 23% higher correlation on text summarization, and over 43% on\ndialogue response generation. Also, UniEval demonstrates a strong zero-shot\nlearning ability for unseen evaluation dimensions and tasks. Source code, data\nand all pre-trained evaluators are available on our GitHub repository\n(https://github.com/maszhongming/UniEval).",
        "date": "2022-10-13T17:17:03+00:00",
        "label": 1
    },
    "2406.19614": {
        "title": "A Survey on Data Quality Dimensions and Tools for Machine Learning",
        "abstract": "Machine learning (ML) technologies have become substantial in practically all\naspects of our society, and data quality (DQ) is critical for the performance,\nfairness, robustness, safety, and scalability of ML models. With the large and\ncomplex data in data-centric AI, traditional methods like exploratory data\nanalysis (EDA) and cross-validation (CV) face challenges, highlighting the\nimportance of mastering DQ tools. In this survey, we review 17 DQ evaluation\nand improvement tools in the last 5 years. By introducing the DQ dimensions,\nmetrics, and main functions embedded in these tools, we compare their strengths\nand limitations and propose a roadmap for developing open-source DQ tools for\nML. Based on the discussions on the challenges and emerging trends, we further\nhighlight the potential applications of large language models (LLMs) and\ngenerative AI in DQ evaluation and improvement for ML. We believe this\ncomprehensive survey can enhance understanding of DQ in ML and could drive\nprogress in data-centric AI. A complete list of the literature investigated in\nthis survey is available on GitHub at:\nhttps://github.com/haihua0913/awesome-dq4ml.",
        "date": "2024-06-28T02:41:33+00:00",
        "label": 1
    },
    "2310.17631": {
        "title": "JudgeLM: Fine-tuned Large Language Models are Scalable Judges",
        "abstract": "Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, and multi-turn chat.",
        "date": "2023-10-26T17:48:58+00:00",
        "label": 1
    }
}