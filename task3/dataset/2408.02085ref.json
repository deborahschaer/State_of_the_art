{
    "2303.09540": {
        "title": "SemDeDup: Data-efficient learning at web-scale through semantic deduplication",
        "abstract": "Progress in machine learning has been driven in large part by massive\nincreases in data. However, large web-scale datasets such as LAION are largely\nuncurated beyond searches for exact duplicates, potentially leaving much\nredundancy. Here, we introduce SemDeDup, a method which leverages embeddings\nfrom pre-trained models to identify and remove semantic duplicates: data pairs\nwhich are semantically similar, but not exactly identical. Removing semantic\nduplicates preserves performance and speeds up learning. Analyzing a subset of\nLAION, we show that SemDeDup can remove 50% of the data with minimal\nperformance loss, effectively halving training time. Moreover, performance\nincreases out of distribution. Also, analyzing language models trained on C4, a\npartially curated dataset, we show that SemDeDup improves over prior approaches\nwhile providing efficiency gains. SemDeDup provides an example of how simple\nways of leveraging quality embeddings can be used to make models learn faster\nwith less data.",
        "date": "2023-03-16T17:53:24+00:00",
        "label": 1
    },
    "2303.08774": {
        "title": "GPT-4 Technical Report",
        "abstract": "We report the development of GPT-4, a large-scale, multimodal model which can\naccept image and text inputs and produce text outputs. While less capable than\nhumans in many real-world scenarios, GPT-4 exhibits human-level performance on\nvarious professional and academic benchmarks, including passing a simulated bar\nexam with a score around the top 10% of test takers. GPT-4 is a\nTransformer-based model pre-trained to predict the next token in a document.\nThe post-training alignment process results in improved performance on measures\nof factuality and adherence to desired behavior. A core component of this\nproject was developing infrastructure and optimization methods that behave\npredictably across a wide range of scales. This allowed us to accurately\npredict some aspects of GPT-4's performance based on models trained with no\nmore than 1/1,000th the compute of GPT-4.",
        "date": "2023-03-15T17:15:04+00:00",
        "label": 1
    },
    "2402.16827": {
        "title": "A Survey on Data Selection for Language Models",
        "abstract": "A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.",
        "date": "2024-02-26T18:54:35+00:00",
        "label": 1
    },
    "2401.13229": {
        "title": "From Random to Informed Data Selection: A Diversity-Based Approach to Optimize Human Annotation and Few-Shot Learning",
        "abstract": "A major challenge in Natural Language Processing is obtaining annotated data\nfor supervised learning. An option is the use of crowdsourcing platforms for\ndata annotation. However, crowdsourcing introduces issues related to the\nannotator's experience, consistency, and biases. An alternative is to use\nzero-shot methods, which in turn have limitations compared to their few-shot or\nfully supervised counterparts. Recent advancements driven by large language\nmodels show potential, but struggle to adapt to specialized domains with\nseverely limited data. The most common approaches therefore involve the human\nitself randomly annotating a set of datapoints to build initial datasets. But\nrandomly sampling data to be annotated is often inefficient as it ignores the\ncharacteristics of the data and the specific needs of the model. The situation\nworsens when working with imbalanced datasets, as random sampling tends to\nheavily bias towards the majority classes, leading to excessive annotated data.\nTo address these issues, this paper contributes an automatic and informed data\nselection architecture to build a small dataset for few-shot learning. Our\nproposal minimizes the quantity and maximizes diversity of data selected for\nhuman annotation, while improving model performance.",
        "date": "2024-01-24T04:57:32+00:00",
        "label": 1
    },
    "2405.20541": {
        "title": "Perplexed by Perplexity: Perplexity-Based Data Pruning With Small Reference Models",
        "abstract": "In this work, we investigate whether small language models can determine\nhigh-quality subsets of large-scale text datasets that improve the performance\nof larger language models. While existing work has shown that pruning based on\nthe perplexity of a larger model can yield high-quality data, we investigate\nwhether smaller models can be used for perplexity-based pruning and how pruning\nis affected by the domain composition of the data being pruned. We demonstrate\nthat for multiple dataset compositions, perplexity-based pruning of pretraining\ndata can \\emph{significantly} improve downstream task performance: pruning\nbased on perplexities computed with a 125 million parameter model improves the\naverage performance on downstream tasks of a 3 billion parameter model by up to\n2.04 and achieves up to a $1.45\\times$ reduction in pretraining steps to reach\ncommensurate baseline performance. Furthermore, we demonstrate that such\nperplexity-based data pruning also yields downstream performance gains in the\nover-trained and data-constrained regimes.",
        "date": "2024-05-30T23:50:20+00:00",
        "label": 1
    },
    "2407.14985": {
        "title": "Generalization v.s. Memorization: Tracing Language Models' Capabilities Back to Pretraining Data",
        "abstract": "Despite the proven utility of large language models (LLMs) in real-world\napplications, there remains a lack of understanding regarding how they leverage\ntheir large-scale pretraining text corpora to achieve such capabilities. In\nthis work, we investigate the interplay between generalization and memorization\nin pretrained LLMs at scale, through a comprehensive $n$-gram analysis of their\ntraining data. Our experiments focus on three general task types: translation,\nquestion-answering, and multiple-choice reasoning. With various sizes of\nopen-source LLMs and their pretraining corpora, we observe that as the model\nsize increases, the task-relevant $n$-gram pair data becomes increasingly\nimportant, leading to improved task performance, decreased memorization,\nstronger generalization, and emergent abilities. Our results support the\nhypothesis that LLMs' capabilities emerge from a delicate balance of\nmemorization and generalization with sufficient task-related pretraining data,\nand point the way to larger-scale analyses that could further improve our\nunderstanding of these models.",
        "date": "2024-07-20T21:24:40+00:00",
        "label": 1
    },
    "2406.09334": {
        "title": "ProxyLM: Predicting Language Model Performance on Multilingual Tasks via Proxy Models",
        "abstract": "Performance prediction is a method to estimate the performance of Language\nModels (LMs) on various Natural Language Processing (NLP) tasks, mitigating\ncomputational costs associated with model capacity and data for fine-tuning.\nOur paper introduces ProxyLM, a scalable framework for predicting LM\nperformance using proxy models in multilingual tasks. These proxy models act as\nsurrogates, approximating the performance of the LM of interest. By leveraging\nproxy models, ProxyLM significantly reduces computational overhead on task\nevaluations, achieving up to a 37.08x speedup compared to traditional methods,\neven with our smallest proxy models. Additionally, our methodology showcases\nadaptability to previously unseen languages in pre-trained LMs, outperforming\nthe state-of-the-art performance by 1.89x as measured by root-mean-square error\n(RMSE). This framework streamlines model selection, enabling efficient\ndeployment and iterative LM enhancements without extensive computational\nresources.",
        "date": "2024-06-13T17:15:33+00:00",
        "label": 1
    },
    "2402.17327": {
        "title": "Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond",
        "abstract": "We study the data selection problem, whose aim is to select a small\nrepresentative subset of data that can be used to efficiently train a machine\nlearning model. We present a new data selection approach based on $k$-means\nclustering and sensitivity sampling. Assuming access to an embedding\nrepresentation of the data with respect to which the model loss is H\\\"older\ncontinuous, our approach provably allows selecting a set of ``typical'' $k +\n1/\\varepsilon^2$ elements whose average loss corresponds to the average loss of\nthe whole dataset, up to a multiplicative $(1\\pm\\varepsilon)$ factor and an\nadditive $\\varepsilon \\lambda \\Phi_k$, where $\\Phi_k$ represents the $k$-means\ncost for the input embeddings and $\\lambda$ is the H\\\"older constant.\n  We furthermore demonstrate the performance and scalability of our approach on\nfine-tuning foundation models and show that it outperforms state-of-the-art\nmethods. We also show how it can be applied on linear regression, leading to a\nnew sampling strategy that surprisingly matches the performances of leverage\nscore sampling, while being conceptually simpler and more scalable.",
        "date": "2024-02-27T09:03:43+00:00",
        "label": 1
    },
    "2405.12186": {
        "title": "Training Data Attribution via Approximate Unrolled Differentiation",
        "abstract": "Many training data attribution (TDA) methods aim to estimate how a model's\nbehavior would change if one or more data points were removed from the training\nset. Methods based on implicit differentiation, such as influence functions,\ncan be made computationally efficient, but fail to account for\nunderspecification, the implicit bias of the optimization algorithm, or\nmulti-stage training pipelines. By contrast, methods based on unrolling address\nthese issues but face scalability challenges. In this work, we connect the\nimplicit-differentiation-based and unrolling-based approaches and combine their\nbenefits by introducing Source, an approximate unrolling-based TDA method that\nis computed using an influence-function-like formula. While being\ncomputationally efficient compared to unrolling-based approaches, Source is\nsuitable in cases where implicit-differentiation-based approaches struggle,\nsuch as in non-converged models and multi-stage training pipelines.\nEmpirically, Source outperforms existing TDA techniques in counterfactual\nprediction, especially in settings where implicit-differentiation-based\napproaches fall short.",
        "date": "2024-05-20T17:17:44+00:00",
        "label": 1
    },
    "2309.16609": {
        "title": "Qwen Technical Report",
        "abstract": "Large language models (LLMs) have revolutionized the field of artificial\nintelligence, enabling natural language processing tasks that were previously\nthought to be exclusive to humans. In this work, we introduce Qwen, the first\ninstallment of our large language model series. Qwen is a comprehensive\nlanguage model series that encompasses distinct models with varying parameter\ncounts. It includes Qwen, the base pretrained language models, and Qwen-Chat,\nthe chat models finetuned with human alignment techniques. The base language\nmodels consistently demonstrate superior performance across a multitude of\ndownstream tasks, and the chat models, particularly those trained using\nReinforcement Learning from Human Feedback (RLHF), are highly competitive. The\nchat models possess advanced tool-use and planning capabilities for creating\nagent applications, showcasing impressive performance even when compared to\nbigger models on complex tasks like utilizing a code interpreter. Furthermore,\nwe have developed coding-specialized models, Code-Qwen and Code-Qwen-Chat, as\nwell as mathematics-focused models, Math-Qwen-Chat, which are built upon base\nlanguage models. These models demonstrate significantly improved performance in\ncomparison with open-source models, and slightly fall behind the proprietary\nmodels.",
        "date": "2023-09-28T17:07:49+00:00",
        "label": 1
    },
    "2203.14544": {
        "title": "Gradient-Matching Coresets for Rehearsal-Based Continual Learning",
        "abstract": "The goal of continual learning (CL) is to efficiently update a machine\nlearning model with new data without forgetting previously-learned knowledge.\nMost widely-used CL methods rely on a rehearsal memory of data points to be\nreused while training on new data. Curating such a rehearsal memory to maintain\na small, informative subset of all the data seen so far is crucial to the\nsuccess of these methods. We devise a coreset selection method for\nrehearsal-based continual learning. Our method is based on the idea of gradient\nmatching: The gradients induced by the coreset should match, as closely as\npossible, those induced by the original training dataset. Inspired by the\nneural tangent kernel theory, we perform this gradient matching across the\nmodel's initialization distribution, allowing us to extract a coreset without\nhaving to train the model first. We evaluate the method on a wide range of\ncontinual learning scenarios and demonstrate that it improves the performance\nof rehearsal-based CL methods compared to competing memory management\nstrategies such as reservoir sampling.",
        "date": "2022-03-28T07:37:17+00:00",
        "label": 1
    },
    "1512.02985": {
        "title": "On Variants of k-means Clustering",
        "abstract": "\\textit{Clustering problems} often arise in the fields like data mining,\nmachine learning etc. to group a collection of objects into similar groups with\nrespect to a similarity (or dissimilarity) measure. Among the clustering\nproblems, specifically \\textit{$k$-means} clustering has got much attention\nfrom the researchers. Despite the fact that $k$-means is a very well studied\nproblem its status in the plane is still an open problem. In particular, it is\nunknown whether it admits a PTAS in the plane. The best known approximation\nbound in polynomial time is $9+\\eps$.\n  In this paper, we consider the following variant of $k$-means. Given a set\n$C$ of points in $\\mathcal{R}^d$ and a real $f > 0$, find a finite set $F$ of\npoints in $\\mathcal{R}^d$ that minimizes the quantity $f*|F|+\\sum_{p\\in C}\n\\min_{q \\in F} {||p-q||}^2$. For any fixed dimension $d$, we design a local\nsearch PTAS for this problem. We also give a \"bi-criterion\" local search\nalgorithm for $k$-means which uses $(1+\\eps)k$ centers and yields a solution\nwhose cost is at most $(1+\\eps)$ times the cost of an optimal $k$-means\nsolution. The algorithm runs in polynomial time for any fixed dimension.\n  The contribution of this paper is two fold. On the one hand, we are being\nable to handle the square of distances in an elegant manner, which yields near\noptimal approximation bound. This leads us towards a better understanding of\nthe $k$-means problem. On the other hand, our analysis of local search might\nalso be useful for other geometric problems. This is important considering that\nvery little is known about the local search method for geometric approximation.",
        "date": "2015-12-09T18:37:49+00:00",
        "label": 1
    },
    "2006.14651": {
        "title": "Influence Functions in Deep Learning Are Fragile",
        "abstract": "Influence functions approximate the effect of training samples in test-time\npredictions and have a wide variety of applications in machine learning\ninterpretability and uncertainty estimation. A commonly-used (first-order)\ninfluence function can be implemented efficiently as a post-hoc method\nrequiring access only to the gradients and Hessian of the model. For linear\nmodels, influence functions are well-defined due to the convexity of the\nunderlying loss function and are generally accurate even across difficult\nsettings where model changes are fairly large such as estimating group\ninfluences. Influence functions, however, are not well-understood in the\ncontext of deep learning with non-convex loss functions. In this paper, we\nprovide a comprehensive and large-scale empirical study of successes and\nfailures of influence functions in neural network models trained on datasets\nsuch as Iris, MNIST, CIFAR-10 and ImageNet. Through our extensive experiments,\nwe show that the network architecture, its depth and width, as well as the\nextent of model parameterization and regularization techniques have strong\neffects in the accuracy of influence functions. In particular, we find that (i)\ninfluence estimates are fairly accurate for shallow networks, while for deeper\nnetworks the estimates are often erroneous; (ii) for certain network\narchitectures and datasets, training with weight-decay regularization is\nimportant to get high-quality influence estimates; and (iii) the accuracy of\ninfluence estimates can vary significantly depending on the examined test\npoints. These results suggest that in general influence functions in deep\nlearning are fragile and call for developing improved influence estimation\nmethods to mitigate these issues in non-convex setups.",
        "date": "2020-06-25T18:25:59+00:00",
        "label": 1
    },
    "2401.06692": {
        "title": "An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models",
        "abstract": "Supervised finetuning (SFT) on instruction datasets has played a crucial role\nin achieving the remarkable zero-shot generalization capabilities observed in\nmodern large language models (LLMs). However, the annotation efforts required\nto produce high quality responses for instructions are becoming prohibitively\nexpensive, especially as the number of tasks spanned by instruction datasets\ncontinues to increase. Active learning is effective in identifying useful\nsubsets of samples to annotate from an unlabeled pool, but its high\ncomputational cost remains a barrier to its widespread applicability in the\ncontext of LLMs. To mitigate the annotation cost of SFT and circumvent the\ncomputational bottlenecks of active learning, we propose using experimental\ndesign. Experimental design techniques select the most informative samples to\nlabel, and typically maximize some notion of uncertainty and/or diversity. In\nour work, we implement a framework that evaluates several existing and novel\nexperimental design techniques and find that these methods consistently yield\nsignificant gains in label efficiency with little computational overhead. On\ngenerative tasks, our methods achieve the same generalization performance with\nonly $50\\%$ of annotation cost required by random sampling.",
        "date": "2024-01-12T16:56:54+00:00",
        "label": 1
    },
    "2312.06254": {
        "title": "Modyn: A Platform for Model Training on Dynamic Datasets With Sample-Level Data Selection",
        "abstract": "Machine learning training data is often dynamic in real-world use cases,\ni.e., data is added or removed and may experience distribution shifts over\ntime. Models must incorporate this evolving training data to improve\ngeneralization, adapt to potential distribution shifts, and adhere to privacy\nregulations. However, the cost of model (re)training is proportional to how\noften the model trains and on how much data it trains on. While ML research\nexplores these topics in isolation, there is no end-to-end open-source platform\nto facilitate the exploration of model retraining and data selection policies\nand the deployment these algorithms efficiently at scale.\n  We present Modyn, a platform for model training on dynamic datasets that\nenables sample-level data selection and triggering policies. Modyn orchestrates\ncontinuous training pipelines while optimizing the underlying system\ninfrastructure to support fast access to arbitrary data samples for efficient\ndata selection. Modyn's extensible architecture allows users to run training\npipelines without modifying the platform code, and enables researchers to\neffortlessly extend the system. We evaluate Modyn's training throughput,\nshowing that even in memory-bound recommendation systems workloads, Modyn is\nable to reach 80 to 100 % of the throughput compared to loading big chunks of\ndata locally without sample-level data selection. Additionally, we showcase\nModyn's functionality with three different data selection policies.",
        "date": "2023-12-11T09:50:52+00:00",
        "label": 1
    },
    "2310.13032": {
        "title": "Quality-Diversity through AI Feedback",
        "abstract": "In many text-generation problems, users may prefer not only a single\nresponse, but a diverse range of high-quality outputs from which to choose.\nQuality-diversity (QD) search algorithms aim at such outcomes, by continually\nimproving and diversifying a population of candidates. However, the\napplicability of QD to qualitative domains, like creative writing, has been\nlimited by the difficulty of algorithmically specifying measures of quality and\ndiversity. Interestingly, recent developments in language models (LMs) have\nenabled guiding search through AI feedback, wherein LMs are prompted in natural\nlanguage to evaluate qualitative aspects of text. Leveraging this development,\nwe introduce Quality-Diversity through AI Feedback (QDAIF), wherein an\nevolutionary algorithm applies LMs to both generate variation and evaluate the\nquality and diversity of candidate text. When assessed on creative writing\ndomains, QDAIF covers more of a specified search space with high-quality\nsamples than do non-QD controls. Further, human evaluation of QDAIF-generated\ncreative texts validates reasonable agreement between AI and human evaluation.\nOur results thus highlight the potential of AI feedback to guide open-ended\nsearch for creative and original solutions, providing a recipe that seemingly\ngeneralizes to many domains and modalities. In this way, QDAIF is a step\ntowards AI systems that can independently search, diversify, evaluate, and\nimprove, which are among the core skills underlying human society's capacity\nfor innovation.",
        "date": "2023-10-19T12:13:58+00:00",
        "label": 1
    },
    "2311.14736": {
        "title": "Data Diversity Matters for Robust Instruction Tuning",
        "abstract": "Recent works have shown that by curating high quality and diverse instruction\ntuning datasets, we can significantly improve instruction-following\ncapabilities. However, creating such datasets is difficult and most works rely\non manual curation or proprietary language models. Automatic data curation is\ndifficult as it is still not clear how we can define diversity for instruction\ntuning, how diversity and quality depend on one other, and how we can optimize\ndataset quality and diversity. To resolve these issue, we propose a new\nalgorithm, Quality-Diversity Instruction Tuning (QDIT). QDIT provides a simple\nmethod to simultaneously control dataset diversity and quality, allowing us to\nconduct an in-depth study on the effect of diversity and quality on instruction\ntuning performance. From this study we draw two key insights (1) there is a\nnatural tradeoff between data diversity and quality and (2) increasing data\ndiversity significantly improves the worst case instruction following\nperformance, therefore improving robustness. We validate the performance of\nQDIT on several large scale instruction tuning datasets, where we find it can\nsubstantially improve worst and average case performance compared to\nquality-driven data selection.",
        "date": "2023-11-21T19:12:18+00:00",
        "label": 1
    },
    "2403.16898": {
        "title": "Concerned with Data Contamination? Assessing Countermeasures in Code Language Model",
        "abstract": "Various techniques have been proposed to leverage the capabilities of code\nlanguage models (CLMs) for SE tasks. While these techniques typically evaluate\ntheir effectiveness using publicly available datasets, the evaluation can be\nsubject to data contamination threats where the evaluation datasets have\nalready been used to train the concerned CLMs. This can significantly affect\nthe reliability of the evaluation. Different countermeasures have been\nsuggested to mitigate the data contamination threat. Countermeasures include\nusing more recent data, curating new data, and refactoring existing data are\nintroduced, yet it is unclear whether these countermeasures could really\nmitigate data contamination threats to model evaluation. To fill the gap, we\nsystematically study to quantify the impacts of these countermeasures on CLMs'\nperformance. To facilitate the study, we collected over 2 million Python\nfunctions with timestamps ranging from January 1st, 2018, to December 31st,\n2023. The data created before the models' cut-off date are considered\n\"contaminated data\", while the data where the countermeasures are taken are\nregarded as \"cleansed data\". We study the impact of these countermeasures by\ninvestigating the difference in CLMs' performance on contaminated and cleansed\ndata derived from different countermeasures. Our experiments yield several\ninteresting observations. For instance, CLMs do not necessarily perform worse\non data after the models' cut-off date; on the contrary, they sometimes perform\nbetter. In addition, refactoring did not always result in decreased\nperformance; it could lead to improvements instead. Furthermore, existing\nmetrics such as perplexity cannot distinguish contaminated/cleansed data. We\nhope that the results and observations could help deepen the understanding of\nCLMs' capabilities and inform the community about data contamination.",
        "date": "2024-03-25T16:10:25+00:00",
        "label": 1
    },
    "1702.05962": {
        "title": "Latent Variable Dialogue Models and their Diversity",
        "abstract": "We present a dialogue generation model that directly captures the variability\nin possible responses to a given input, which reduces the `boring output' issue\nof deterministic dialogue models. Experiments show that our model generates\nmore diverse outputs than baseline models, and also generates more consistently\nacceptable output than sampling from a deterministic encoder-decoder model.",
        "date": "2017-02-20T13:36:23+00:00",
        "label": 1
    },
    "2307.06290": {
        "title": "Instruction Mining: Instruction Data Selection for Tuning Large Language Models",
        "abstract": "Large language models (LLMs) are initially pretrained for broad capabilities\nand then finetuned with instruction-following datasets to improve their\nperformance in interacting with humans. Despite advances in finetuning, a\nstandardized guideline for selecting high-quality datasets to optimize this\nprocess remains elusive. In this paper, we first propose InstructMining, an\ninnovative method designed for automatically selecting premium\ninstruction-following data for finetuning LLMs. Specifically, InstructMining\nutilizes natural language indicators as a measure of data quality, applying\nthem to evaluate unseen datasets. During experimentation, we discover that\ndouble descent phenomenon exists in large language model finetuning. Based on\nthis observation, we further leverage BlendSearch to help find the best subset\namong the entire dataset (i.e., 2,532 out of 100,000). Experiment results show\nthat InstructMining-7B achieves state-of-the-art performance on two of the most\npopular benchmarks: LLM-as-a-judge and Huggingface OpenLLM leaderboard.",
        "date": "2023-07-12T16:37:31+00:00",
        "label": 1
    },
    "2308.07201": {
        "title": "ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate",
        "abstract": "Text evaluation has historically posed significant challenges, often\ndemanding substantial labor and time cost. With the emergence of large language\nmodels (LLMs), researchers have explored LLMs' potential as alternatives for\nhuman evaluation. While these single-agent-based approaches show promise,\nexperimental results suggest that further advancements are needed to bridge the\ngap between their current effectiveness and human-level evaluation quality.\nRecognizing that best practices of human evaluation processes often involve\nmultiple human annotators collaborating in the evaluation, we resort to a\nmulti-agent debate framework, moving beyond single-agent prompting strategies.\nThe multi-agent-based approach enables a group of LLMs to synergize with an\narray of intelligent counterparts, harnessing their distinct capabilities and\nexpertise to enhance efficiency and effectiveness in handling intricate tasks.\nIn this paper, we construct a multi-agent referee team called ChatEval to\nautonomously discuss and evaluate the quality of generated responses from\ndifferent models on open-ended questions and traditional natural language\ngeneration (NLG) tasks. Our analysis shows that ChatEval transcends mere\ntextual scoring, offering a human-mimicking evaluation process for reliable\nassessments. Our code is available at https://github.com/chanchimin/ChatEval.",
        "date": "2023-08-14T15:13:04+00:00",
        "label": 1
    },
    "2105.10446": {
        "title": "ReduNet: A White-box Deep Network from the Principle of Maximizing Rate Reduction",
        "abstract": "This work attempts to provide a plausible theoretical framework that aims to\ninterpret modern deep (convolutional) networks from the principles of data\ncompression and discriminative representation. We argue that for\nhigh-dimensional multi-class data, the optimal linear discriminative\nrepresentation maximizes the coding rate difference between the whole dataset\nand the average of all the subsets. We show that the basic iterative gradient\nascent scheme for optimizing the rate reduction objective naturally leads to a\nmulti-layer deep network, named ReduNet, which shares common characteristics of\nmodern deep networks. The deep layered architectures, linear and nonlinear\noperators, and even parameters of the network are all explicitly constructed\nlayer-by-layer via forward propagation, although they are amenable to\nfine-tuning via back propagation. All components of so-obtained \"white-box\"\nnetwork have precise optimization, statistical, and geometric interpretation.\nMoreover, all linear operators of the so-derived network naturally become\nmulti-channel convolutions when we enforce classification to be rigorously\nshift-invariant. The derivation in the invariant setting suggests a trade-off\nbetween sparsity and invariance, and also indicates that such a deep\nconvolution network is significantly more efficient to construct and learn in\nthe spectral domain. Our preliminary simulations and experiments clearly verify\nthe effectiveness of both the rate reduction objective and the associated\nReduNet. All code and data are available at\n\\url{https://github.com/Ma-Lab-Berkeley}.",
        "date": "2021-05-21T16:29:57+00:00",
        "label": 1
    },
    "2305.09246": {
        "title": "Maybe Only 0.5% Data is Needed: A Preliminary Exploration of Low Training Data Instruction Tuning",
        "abstract": "Instruction tuning for large language models (LLMs) has gained attention from\nresearchers due to its ability to unlock the potential of LLMs in following\ninstructions. While instruction tuning offers advantages for facilitating the\nadaptation of large language models (LLMs) to downstream tasks as a fine-tuning\napproach, training models with tens of millions or even billions of parameters\non large amounts of data results in unaffordable computational costs. To\naddress this, we focus on reducing the data used in LLM instruction tuning to\ndecrease training costs and improve data efficiency, dubbed as Low Training\nData Instruction Tuning (LTD Instruction Tuning). Specifically, this paper\nconducts a preliminary exploration into reducing the data used in LLM training\nand identifies several observations regarding task specialization for LLM\ntraining, such as the optimization of performance for a specific task, the\nnumber of instruction types required for instruction tuning, and the amount of\ndata required for task-specific models. The results suggest that task-specific\nmodels can be trained using less than 0.5% of the original dataset, with a 2%\nimprovement in performance over those trained on full task-related data.",
        "date": "2023-05-16T07:52:57+00:00",
        "label": 1
    },
    "2403.12776": {
        "title": "Automated Data Curation for Robust Language Model Fine-Tuning",
        "abstract": "Large Language Models have become the de facto approach to\nsequence-to-sequence text generation tasks, but for specialized tasks/domains,\na pretrained LLM lacks specific capabilities to produce accurate or\nwell-formatted responses. Supervised fine-tuning specializes a LLM by training\nit on dataset of example prompts with target responses, but real-world data\ntends to be noisy. While many fine-tuning algorithms exist, here we consider a\n\\emph{data-centric AI} perspective on LLM fine-tuning, studying how to\n\\emph{systematically} curate the training dataset to improve the LLM produced\nvia \\emph{any} fine-tuning algorithm.\n  We introduce an automated data curation pipeline CLEAR (Confidence-based LLM\nEvaluation And Rectification) for instruction tuning datasets, that can be used\nwith any LLM and fine-tuning procedure. CLEAR estimates which training data is\nlow-quality and either filters or corrects it. Automatically identifying which\ndata to filter or correct is done via LLM-derived confidence estimates, to\nensure only confident modifications to the dataset. Unlike existing data\ncuration techniques, CLEAR is a comprehensive framework that can improve a\ndataset (and trained model outputs) without additional fine-tuning\ncomputations. We don't assume access to a stronger LLM than the model being\nfine-tuned (e.g.\\ relying on GPT-4 when fine-tuning GPT-3.5), to see whether\nCLEAR can meaningfully improve the capabilities of any LLM. Experiments reveal\nthat CLEAR consistently improves the performance of fine-tuned models across\nmany datasets and models (like GPT-3.5 and Llama2).",
        "date": "2024-03-19T14:44:45+00:00",
        "label": 1
    },
    "2307.08701": {
        "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
        "abstract": "Large language models (LLMs) strengthen instruction-following capability\nthrough instruction-finetuning (IFT) on supervised instruction/response data.\nHowever, widely used IFT datasets (e.g., Alpaca's 52k data) surprisingly\ncontain many low-quality instances with incorrect or irrelevant responses,\nwhich are misleading and detrimental to IFT. In this paper, we propose a simple\nand effective data selection strategy that automatically identifies and filters\nout low-quality data using a strong LLM (e.g., ChatGPT). To this end, we\nintroduce AlpaGasus, which is finetuned on only 9k high-quality data filtered\nfrom the 52k Alpaca data. AlpaGasus significantly outperforms the original\nAlpaca as evaluated by GPT-4 on multiple test sets and the controlled human\nevaluation. Its 13B variant matches $>90\\%$ performance of its teacher LLM\n(i.e., Text-Davinci-003 generating the 52k data) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes. Moreover, the experiments prove the\nefficacy of our method across diverse datasets, base models, and LLM filters.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\nhttps://lichang-chen.github.io/AlpaGasus/",
        "date": "2023-07-17T17:59:40+00:00",
        "label": 1
    },
    "2406.14491": {
        "title": "Instruction Pre-Training: Language Models are Supervised Multitask Learners",
        "abstract": "Unsupervised multitask pre-training has been the critical method behind the\nrecent success of language models (LMs). However, supervised multitask learning\nstill holds significant promise, as scaling it in the post-training stage\ntrends towards better generalization. In this paper, we explore supervised\nmultitask pre-training by proposing Instruction Pre-Training, a framework that\nscalably augments massive raw corpora with instruction-response pairs to\npre-train LMs. The instruction-response pairs are generated by an efficient\ninstruction synthesizer built on open-source models. In our experiments, we\nsynthesize 200M instruction-response pairs covering 40+ task categories to\nverify the effectiveness of Instruction Pre-Training. In pre-training from\nscratch, Instruction Pre-Training not only consistently enhances pre-trained\nbase models but also benefits more from further instruction tuning. In\ncontinual pre-training, Instruction Pre-Training enables Llama3-8B to be\ncomparable to or even outperform Llama3-70B. Our model, code, and data are\navailable at https://github.com/microsoft/LMOps.",
        "date": "2024-06-20T16:55:33+00:00",
        "label": 1
    },
    "2405.20456": {
        "title": "Scaling Laws for the Value of Individual Data Points in Machine Learning",
        "abstract": "Recent works have shown that machine learning models improve at a predictable\nrate with the total amount of training data, leading to scaling laws that\ndescribe the relationship between error and dataset size. These scaling laws\ncan help design a model's training dataset, but they typically take an\naggregate view of the data by only considering the dataset's size. We introduce\na new perspective by investigating scaling behavior for the value of individual\ndata points: we find that a data point's contribution to model's performance\nshrinks predictably with the size of the dataset in a log-linear manner.\nInterestingly, there is significant variability in the scaling exponent among\ndifferent data points, indicating that certain points are more valuable in\nsmall datasets while others are relatively more useful as a part of large\ndatasets. We provide learning theory to support our scaling law, and we observe\nempirically that it holds across diverse model classes. We further propose a\nmaximum likelihood estimator and an amortized estimator to efficiently learn\nthe individualized scaling behaviors from a small number of noisy observations\nper data point. Using our estimators, we provide insights into factors that\ninfluence the scaling behavior of different data points. Finally, we\ndemonstrate applications of the individualized scaling laws to data valuation\nand data subset selection. Overall, our work represents a first step towards\nunderstanding and utilizing scaling properties for the value of individual data\npoints.",
        "date": "2024-05-30T20:10:24+00:00",
        "label": 1
    },
    "2311.09783": {
        "title": "Investigating Data Contamination in Modern Benchmarks for Large Language Models",
        "abstract": "Recent observations have underscored a disparity between the inflated\nbenchmark scores and the actual performance of LLMs, raising concerns about\npotential contamination of evaluation benchmarks. This issue is especially\ncritical for closed-source models and certain open-source models where training\ndata transparency is lacking. In this paper we study data contamination by\nproposing two methods tailored for both open-source and proprietary LLMs. We\nfirst introduce a retrieval-based system to explore potential overlaps between\nevaluation benchmarks and pretraining corpora. We further present a novel\ninvestigation protocol named \\textbf{T}estset \\textbf{S}lot Guessing\n(\\textit{TS-Guessing}), applicable to both open and proprietary models. This\napproach entails masking a wrong answer in a multiple-choice question and\nprompting the model to fill in the gap. Additionally, it involves obscuring an\nunlikely word in an evaluation example and asking the model to produce it. We\nfind that certain commercial LLMs could surprisingly guess the missing option\nin various test sets. Specifically, in the TruthfulQA benchmark, we find that\nLLMs exhibit notable performance improvement when provided with additional\nmetadata in the benchmark. Further, in the MMLU benchmark, ChatGPT and GPT-4\ndemonstrated an exact match rate of 52\\% and 57\\%, respectively, in guessing\nthe missing options in benchmark test data. We hope these results underscore\nthe need for more robust evaluation methodologies and benchmarks in the field.",
        "date": "2023-11-16T11:03:04+00:00",
        "label": 1
    },
    "2109.06379": {
        "title": "Compression, Transduction, and Creation: A Unified Framework for Evaluating Natural Language Generation",
        "abstract": "Natural language generation (NLG) spans a broad range of tasks, each of which\nserves for specific objectives and desires different properties of generated\ntext. The complexity makes automatic evaluation of NLG particularly\nchallenging. Previous work has typically focused on a single task and developed\nindividual evaluation metrics based on specific intuitions. In this paper, we\npropose a unifying perspective that facilitates the design of metrics for a\nwide range of language generation tasks and quality aspects. Based on the\nnature of information change from input to output, we classify NLG tasks into\ncompression (e.g., summarization), transduction (e.g., text rewriting), and\ncreation (e.g., dialog). The information alignment, or overlap, between input,\ncontext, and output text plays a common central role in characterizing the\ngeneration. Using the uniform concept of information alignment, we develop a\nfamily of interpretable metrics for various NLG tasks and aspects, often\nwithout need of gold reference data. To operationalize the metrics, we train\nself-supervised models to approximate information alignment as a prediction\ntask. Experiments show the uniformly designed metrics achieve stronger or\ncomparable correlations with human judgement compared to state-of-the-art\nmetrics in each of diverse tasks, including text summarization, style transfer,\nand knowledge-grounded dialog. With information alignment as the intermediate\nrepresentation, we deliver a composable library for easy NLG evaluation and\nfuture metric design.",
        "date": "2021-09-14T01:00:42+00:00",
        "label": 1
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "date": "2018-10-11T00:50:01+00:00",
        "label": 1
    },
    "2305.14233": {
        "title": "Enhancing Chat Language Models by Scaling High-quality Instructional Conversations",
        "abstract": "Fine-tuning on instruction data has been widely validated as an effective\npractice for implementing chat language models like ChatGPT. Scaling the\ndiversity and quality of such data, although straightforward, stands a great\nchance of leading to improved performance. This paper aims to improve the upper\nbound of open-source models further. We first provide a systematically\ndesigned, diverse, informative, large-scale dataset of instructional\nconversations, UltraChat, which does not involve human queries. Our objective\nis to capture the breadth of interactions that a human might have with an AI\nassistant and employs a comprehensive framework to generate multi-turn\nconversation iteratively. UltraChat contains 1.5 million high-quality\nmulti-turn dialogues and covers a wide range of topics and instructions. Our\nstatistical analysis of UltraChat reveals its superiority in various key\nmetrics, including scale, average length, diversity, coherence, etc.,\nsolidifying its position as a leading open-source dataset. Building upon\nUltraChat, we fine-tune a LLaMA model to create a powerful conversational\nmodel, UltraLLaMA. Our evaluations indicate that UltraLLaMA consistently\noutperforms other open-source models, including Vicuna, the previously\nrecognized state-of-the-art open-source model. The dataset and the model will\nbe publicly released\\footnote{\\url{https://github.com/thunlp/UltraChat}}.",
        "date": "2023-05-23T16:49:14+00:00",
        "label": 1
    },
    "2002.06305": {
        "title": "Fine-Tuning Pretrained Language Models: Weight Initializations, Data Orders, and Early Stopping",
        "abstract": "Fine-tuning pretrained contextual word embedding models to supervised\ndownstream tasks has become commonplace in natural language processing. This\nprocess, however, is often brittle: even with the same hyperparameter values,\ndistinct random seeds can lead to substantially different results. To better\nunderstand this phenomenon, we experiment with four datasets from the GLUE\nbenchmark, fine-tuning BERT hundreds of times on each while varying only the\nrandom seeds. We find substantial performance increases compared to previously\nreported results, and we quantify how the performance of the best-found model\nvaries as a function of the number of fine-tuning trials. Further, we examine\ntwo factors influenced by the choice of random seed: weight initialization and\ntraining data order. We find that both contribute comparably to the variance of\nout-of-sample performance, and that some weight initializations perform well\nacross all tasks explored. On small datasets, we observe that many fine-tuning\ntrials diverge part of the way through training, and we offer best practices\nfor practitioners to stop training less promising runs early. We publicly\nrelease all of our experimental data, including training and validation scores\nfor 2,100 trials, to encourage further analysis of training dynamics during\nfine-tuning.",
        "date": "2020-02-15T02:40:10+00:00",
        "label": 1
    },
    "2406.13542": {
        "title": "Self-play with Execution Feedback: Improving Instruction-following Capabilities of Large Language Models",
        "abstract": "One core capability of large language models (LLMs) is to follow natural\nlanguage instructions. However, the issue of automatically constructing\nhigh-quality training data to enhance the complex instruction-following\nabilities of LLMs without manual annotation remains unresolved. In this paper,\nwe introduce AutoIF, the first scalable and reliable method for automatically\ngenerating instruction-following training data. AutoIF transforms the\nvalidation of instruction-following data quality into code verification,\nrequiring LLMs to generate instructions, the corresponding code to check the\ncorrectness of the instruction responses, and unit test samples to verify the\ncode's correctness. Then, execution feedback-based rejection sampling can\ngenerate data for Supervised Fine-Tuning (SFT) and Reinforcement Learning from\nHuman Feedback (RLHF) training. AutoIF achieves significant improvements across\nthree training algorithms, SFT, Offline DPO, and Online DPO, when applied to\nthe top open-source LLMs, Qwen2 and LLaMA3, in self-alignment and\nstrong-to-weak distillation settings. Our code is publicly available at\nhttps://github.com/QwenLM/AutoIF.",
        "date": "2024-06-19T13:29:53+00:00",
        "label": 1
    },
    "2304.06767": {
        "title": "RAFT: Reward rAnked FineTuning for Generative Foundation Model Alignment",
        "abstract": "Generative foundation models are susceptible to implicit biases that can\narise from extensive unsupervised training data. Such biases can produce\nsuboptimal samples, skewed outcomes, and unfairness, with potentially serious\nconsequences. Consequently, aligning these models with human ethics and\npreferences is an essential step toward ensuring their responsible and\neffective deployment in real-world applications. Prior research has primarily\nemployed Reinforcement Learning from Human Feedback (RLHF) to address this\nproblem, where generative models are fine-tuned with RL algorithms guided by a\nhuman-feedback-informed reward model. However, the inefficiencies and\ninstabilities associated with RL algorithms frequently present substantial\nobstacles to the successful alignment, necessitating the development of a more\nrobust and streamlined approach. To this end, we introduce a new framework,\nReward rAnked FineTuning (RAFT), designed to align generative models\neffectively. Utilizing a reward model and a sufficient number of samples, our\napproach selects the high-quality samples, discarding those that exhibit\nundesired behavior, and subsequently enhancing the model by fine-tuning on\nthese filtered samples. Our studies show that RAFT can effectively improve the\nmodel performance in both reward learning and other automated metrics in both\nlarge language models and diffusion models.",
        "date": "2023-04-13T18:22:40+00:00",
        "label": 1
    },
    "2311.15653": {
        "title": "MoDS: Model-oriented Data Selection for Instruction Tuning",
        "abstract": "Instruction tuning has become the de facto method to equip large language\nmodels (LLMs) with the ability of following user instructions. Usually,\nhundreds of thousands or millions of instruction-following pairs are employed\nto fine-tune the foundation LLMs. Recently, some studies show that a small\nnumber of high-quality instruction data is enough. However, how to select\nappropriate instruction data for a given LLM is still an open problem. To\naddress this problem, in this paper we present a model-oriented data selection\n(MoDS) approach, which selects instruction data based on a new criteria\nconsidering three aspects: quality, coverage and necessity. First, our approach\nutilizes a quality evaluation model to filter out the high-quality subset from\nthe original instruction dataset, and then designs an algorithm to further\nselect from the high-quality subset a seed instruction dataset with good\ncoverage. The seed dataset is applied to fine-tune the foundation LLM to obtain\nan initial instruction-following LLM. Finally, we develop a necessity\nevaluation model to find out the instruction data which are performed badly in\nthe initial instruction-following LLM and consider them necessary instructions\nto further improve the LLMs. In this way, we can get a small high-quality,\nbroad-coverage and high-necessity subset from the original instruction\ndatasets. Experimental results show that, the model fine-tuned with 4,000\ninstruction pairs selected by our approach could perform better than the model\nfine-tuned with the full original dataset which includes 214k instruction data.",
        "date": "2023-11-27T09:33:13+00:00",
        "label": 1
    },
    "1903.09722": {
        "title": "Pre-trained Language Model Representations for Language Generation",
        "abstract": "Pre-trained language model representations have been successful in a wide\nrange of language understanding tasks. In this paper, we examine different\nstrategies to integrate pre-trained representations into sequence to sequence\nmodels and apply it to neural machine translation and abstractive\nsummarization. We find that pre-trained representations are most effective when\nadded to the encoder network which slows inference by only 14%. Our experiments\nin machine translation show gains of up to 5.3 BLEU in a simulated\nresource-poor setup. While returns diminish with more labeled data, we still\nobserve improvements when millions of sentence-pairs are available. Finally, on\nabstractive summarization we achieve a new state of the art on the full text\nversion of CNN/DailyMail.",
        "date": "2019-03-22T22:14:51+00:00",
        "label": 1
    },
    "2401.12926": {
        "title": "DsDm: Model-Aware Dataset Selection with Datamodels",
        "abstract": "When selecting data for training large-scale models, standard practice is to\nfilter for examples that match human notions of data quality. Such filtering\nyields qualitatively clean datapoints that intuitively should improve model\nbehavior. However, in practice the opposite can often happen: we find that\nselecting according to similarity with \"high quality\" data sources may not\nincrease (and can even hurt) performance compared to randomly selecting data.\n  To develop better methods for selecting data, we start by framing dataset\nselection as an optimization problem that we can directly solve for: given\ntarget tasks, a learning algorithm, and candidate data, select the subset that\nmaximizes model performance. This framework thus avoids handpicked notions of\ndata quality, and instead models explicitly how the learning process uses train\ndatapoints to predict on the target tasks. Our resulting method greatly\nimproves language model (LM) performance on both pre-specified tasks and\npreviously unseen tasks. Specifically, choosing target tasks representative of\nstandard LM problems and evaluating on diverse held-out benchmarks, our\nselected datasets provide a 2x compute multiplier over baseline methods.",
        "date": "2024-01-23T17:22:00+00:00",
        "label": 1
    },
    "2306.11670": {
        "title": "GIO: Gradient Information Optimization for Training Dataset Selection",
        "abstract": "It is often advantageous to train models on a subset of the available train\nexamples, because the examples are of variable quality or because one would\nlike to train with fewer examples, without sacrificing performance. We present\nGradient Information Optimization (GIO), a scalable, task-agnostic approach to\nthis data selection problem that requires only a small set of (unlabeled)\nexamples representing a target distribution. GIO begins from a natural,\ninformation-theoretic objective that is intractable in practice. Our\ncontribution is in showing that it can be made highly scalable through a simple\nrelaxation of the objective and a highly efficient implementation. In\nexperiments with machine translation, spelling correction, and image\nrecognition, we show that GIO delivers outstanding results with very small\ntrain sets. These findings are robust to different representation models and\nhyperparameters for GIO itself. GIO is task- and domain-agnostic and can be\napplied out-of-the-box to new datasets and domains. We open source a\npip-installable implementation of the algorithm as \"pip install grad-info-opt\".",
        "date": "2023-06-20T16:43:38+00:00",
        "label": 1
    },
    "2007.01852": {
        "title": "Language-agnostic BERT Sentence Embedding",
        "abstract": "While BERT is an effective method for learning monolingual sentence\nembeddings for semantic similarity and embedding based transfer learning\n(Reimers and Gurevych, 2019), BERT based cross-lingual sentence embeddings have\nyet to be explored. We systematically investigate methods for learning\nmultilingual sentence embeddings by combining the best methods for learning\nmonolingual and cross-lingual representations including: masked language\nmodeling (MLM), translation language modeling (TLM) (Conneau and Lample, 2019),\ndual encoder translation ranking (Guo et al., 2018), and additive margin\nsoftmax (Yang et al., 2019a). We show that introducing a pre-trained\nmultilingual language model dramatically reduces the amount of parallel\ntraining data required to achieve good performance by 80%. Composing the best\nof these methods produces a model that achieves 83.7% bi-text retrieval\naccuracy over 112 languages on Tatoeba, well above the 65.5% achieved by\nArtetxe and Schwenk (2019b), while still performing competitively on\nmonolingual transfer learning benchmarks (Conneau and Kiela, 2018). Parallel\ndata mined from CommonCrawl using our best model is shown to train competitive\nNMT models for en-zh and en-de. We publicly release our best multilingual\nsentence embedding model for 109+ languages at https://tfhub.dev/google/LaBSE.",
        "date": "2020-07-03T17:58:42+00:00",
        "label": 1
    },
    "2105.03075": {
        "title": "A Survey of Data Augmentation Approaches for NLP",
        "abstract": "Data augmentation has recently seen increased interest in NLP due to more\nwork in low-resource domains, new tasks, and the popularity of large-scale\nneural networks that require large amounts of training data. Despite this\nrecent upsurge, this area is still relatively underexplored, perhaps due to the\nchallenges posed by the discrete nature of language data. In this paper, we\npresent a comprehensive and unifying survey of data augmentation for NLP by\nsummarizing the literature in a structured manner. We first introduce and\nmotivate data augmentation for NLP, and then discuss major methodologically\nrepresentative approaches. Next, we highlight techniques that are used for\npopular NLP applications and tasks. We conclude by outlining current challenges\nand directions for future research. Overall, our paper aims to clarify the\nlandscape of existing literature in data augmentation for NLP and motivate\nadditional work in this area. We also present a GitHub repository with a paper\nlist that will be continuously updated at\nhttps://github.com/styfeng/DataAug4NLP",
        "date": "2021-05-07T06:03:45+00:00",
        "label": 1
    },
    "1806.03884": {
        "title": "Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis",
        "abstract": "Optimization algorithms that leverage gradient covariance information, such\nas variants of natural gradient descent (Amari, 1998), offer the prospect of\nyielding more effective descent directions. For models with many parameters,\nthe covariance matrix they are based on becomes gigantic, making them\ninapplicable in their original form. This has motivated research into both\nsimple diagonal approximations and more sophisticated factored approximations\nsuch as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In\nthe present work we draw inspiration from both to propose a novel approximation\nthat is provably better than KFAC and amendable to cheap partial updates. It\nconsists in tracking a diagonal variance, not in parameter coordinates, but in\na Kronecker-factored eigenbasis, in which the diagonal approximation is likely\nto be more effective. Experiments show improvements over KFAC in optimization\nspeed for several deep network architectures.",
        "date": "2018-06-11T09:44:23+00:00",
        "label": 1
    },
    "2402.05119": {
        "title": "A Closer Look at the Limitations of Instruction Tuning",
        "abstract": "Instruction Tuning (IT), the process of training large language models (LLMs)\nusing instruction-response pairs, has emerged as the predominant method for\ntransforming base pre-trained LLMs into open-domain conversational agents.\nWhile IT has achieved notable success and widespread adoption, its limitations\nand shortcomings remain underexplored. In this paper, through rigorous\nexperiments and an in-depth analysis of the changes LLMs undergo through IT, we\nreveal various limitations of IT. In particular, we show that (1) IT fails to\nenhance knowledge or skills in LLMs. LoRA fine-tuning is limited to learning\nresponse initiation and style tokens, and full-parameter fine-tuning leads to\nknowledge degradation. (2) Copying response patterns from IT datasets derived\nfrom knowledgeable sources leads to a decline in response quality. (3)\nFull-parameter fine-tuning increases hallucination by inaccurately borrowing\ntokens from conceptually similar instances in the IT dataset for generating\nresponses. (4) Popular methods to improve IT do not lead to performance\nimprovements over a simple LoRA fine-tuned model. Our findings reveal that\nresponses generated solely from pre-trained knowledge consistently outperform\nresponses by models that learn any form of new knowledge from IT on open-source\ndatasets. We hope the insights and challenges revealed in this paper inspire\nfuture work in related directions.",
        "date": "2024-02-03T04:45:25+00:00",
        "label": 1
    },
    "2308.03296": {
        "title": "Studying Large Language Model Generalization with Influence Functions",
        "abstract": "When trying to gain better visibility into a machine learning model in order\nto understand and mitigate the associated risks, a potentially valuable source\nof evidence is: which training examples most contribute to a given behavior?\nInfluence functions aim to answer a counterfactual: how would the model's\nparameters (and hence its outputs) change if a given sequence were added to the\ntraining set? While influence functions have produced insights for small\nmodels, they are difficult to scale to large language models (LLMs) due to the\ndifficulty of computing an inverse-Hessian-vector product (IHVP). We use the\nEigenvalue-corrected Kronecker-Factored Approximate Curvature (EK-FAC)\napproximation to scale influence functions up to LLMs with up to 52 billion\nparameters. In our experiments, EK-FAC achieves similar accuracy to traditional\ninfluence function estimators despite the IHVP computation being orders of\nmagnitude faster. We investigate two algorithmic techniques to reduce the cost\nof computing gradients of candidate training sequences: TF-IDF filtering and\nquery batching. We use influence functions to investigate the generalization\npatterns of LLMs, including the sparsity of the influence patterns, increasing\nabstraction with scale, math and programming abilities, cross-lingual\ngeneralization, and role-playing behavior. Despite many apparently\nsophisticated forms of generalization, we identify a surprising limitation:\ninfluences decay to near-zero when the order of key phrases is flipped.\nOverall, influence functions give us a powerful new tool for studying the\ngeneralization properties of LLMs.",
        "date": "2023-08-07T04:47:42+00:00",
        "label": 1
    },
    "2306.11644": {
        "title": "Textbooks Are All You Need",
        "abstract": "We introduce phi-1, a new large language model for code, with significantly\nsmaller size than competing models: phi-1 is a Transformer-based model with\n1.3B parameters, trained for 4 days on 8 A100s, using a selection of ``textbook\nquality\" data from the web (6B tokens) and synthetically generated textbooks\nand exercises with GPT-3.5 (1B tokens). Despite this small scale, phi-1 attains\npass@1 accuracy 50.6% on HumanEval and 55.5% on MBPP. It also displays\nsurprising emergent properties compared to phi-1-base, our model before our\nfinetuning stage on a dataset of coding exercises, and phi-1-small, a smaller\nmodel with 350M parameters trained with the same pipeline as phi-1 that still\nachieves 45% on HumanEval.",
        "date": "2023-06-20T16:14:25+00:00",
        "label": 1
    },
    "2303.08114": {
        "title": "Simfluence: Modeling the Influence of Individual Training Examples by Simulating Training Runs",
        "abstract": "Training data attribution (TDA) methods offer to trace a model's prediction\non any given example back to specific influential training examples. Existing\napproaches do so by assigning a scalar influence score to each training\nexample, under a simplifying assumption that influence is additive. But in\nreality, we observe that training examples interact in highly non-additive ways\ndue to factors such as inter-example redundancy, training order, and curriculum\nlearning effects.\n  To study such interactions, we propose Simfluence, a new paradigm for TDA\nwhere the goal is not to produce a single influence score per example, but\ninstead a training run simulator: the user asks, ``If my model had trained on\nexample $z_1$, then $z_2$, ..., then $z_n$, how would it behave on\n$z_{test}$?''; the simulator should then output a simulated training run, which\nis a time series predicting the loss on $z_{test}$ at every step of the\nsimulated run. This enables users to answer counterfactual questions about what\ntheir model would have learned under different training curricula, and to\ndirectly see where in training that learning would occur.\n  We present a simulator, Simfluence-Linear, that captures non-additive\ninteractions and is often able to predict the spiky trajectory of individual\nexample losses with surprising fidelity. Furthermore, we show that existing TDA\nmethods such as TracIn and influence functions can be viewed as special cases\nof Simfluence-Linear. This enables us to directly compare methods in terms of\ntheir simulation accuracy, subsuming several prior TDA approaches to\nevaluation. In experiments on large language model (LLM) fine-tuning, we show\nthat our method predicts loss trajectories with much higher accuracy than\nexisting TDA methods (doubling Spearman's correlation and reducing mean-squared\nerror by 75%) across several tasks, models, and training methods.",
        "date": "2023-03-14T17:47:25+00:00",
        "label": 1
    },
    "2203.15556": {
        "title": "Training Compute-Optimal Large Language Models",
        "abstract": "We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.",
        "date": "2022-03-29T13:38:03+00:00",
        "label": 1
    },
    "2403.02839": {
        "title": "On the Limitations of Fine-tuned Judge Models for LLM Evaluation",
        "abstract": "Recently, there has been a growing trend of utilizing Large Language Model\n(LLM) to evaluate the quality of other LLMs. Many studies have employed\nproprietary close-source models, especially GPT-4, as the evaluator.\nAlternatively, other works have fine-tuned judge models based on open-source\nLLMs as the evaluator. While the fine-tuned judge models are claimed to achieve\ncomparable evaluation capability with GPT-4, in this study, we conduct an\nempirical study of judge models. Our findings indicate that although the\nfine-tuned judge models achieve high performance on in-domain test sets, even\nsurpassing GPT-4, they underperform GPT-4 across several dimensions, including\ngeneralizability, fairness, aspect-specific evaluation, and scalability. We\nalso reveal that the fine-tuned judge model inherently operates as a\ntask-specific classifier, consequently imposing the limitations. Finally, we\npropose an effective indicator to measure the reliability of fine-tuned judges,\nwith the aim of maximizing their utility in LLM evaluation.",
        "date": "2024-03-05T10:20:52+00:00",
        "label": 1
    },
    "2403.08763": {
        "title": "Simple and Scalable Strategies to Continually Pre-train Large Language Models",
        "abstract": "Large language models (LLMs) are routinely pre-trained on billions of tokens,\nonly to start the process over again once new data becomes available. A much\nmore efficient solution is to continually pre-train these models, saving\nsignificant compute compared to re-training. However, the distribution shift\ninduced by new data typically results in degraded performance on previous data\nor poor adaptation to the new data. In this work, we show that a simple and\nscalable combination of learning rate (LR) re-warming, LR re-decaying, and\nreplay of previous data is sufficient to match the performance of fully\nre-training from scratch on all available data, as measured by the final loss\nand the average score on several language model (LM) evaluation benchmarks.\nSpecifically, we show this for a weak but realistic distribution shift between\ntwo commonly used LLM pre-training datasets (English$\\rightarrow$English) and a\nstronger distribution shift (English$\\rightarrow$German) at the $405$M\nparameter model scale with large dataset sizes (hundreds of billions of\ntokens). Selecting the weak but realistic shift for larger-scale experiments,\nwe also find that our continual learning strategies match the re-training\nbaseline for a 10B parameter LLM. Our results demonstrate that LLMs can be\nsuccessfully updated via simple and scalable continual learning strategies,\nmatching the re-training baseline using only a fraction of the compute.\nFinally, inspired by previous work, we propose alternatives to the cosine\nlearning rate schedule that help circumvent forgetting induced by LR re-warming\nand that are not bound to a fixed token budget.",
        "date": "2024-03-13T17:58:57+00:00",
        "label": 1
    },
    "2202.00622": {
        "title": "Datamodels: Predicting Predictions from Training Data",
        "abstract": "We present a conceptual framework, datamodeling, for analyzing the behavior\nof a model class in terms of the training data. For any fixed \"target\" example\n$x$, training set $S$, and learning algorithm, a datamodel is a parameterized\nfunction $2^S \\to \\mathbb{R}$ that for any subset of $S' \\subset S$ -- using\nonly information about which examples of $S$ are contained in $S'$ -- predicts\nthe outcome of training a model on $S'$ and evaluating on $x$. Despite the\npotential complexity of the underlying process being approximated (e.g.,\nend-to-end training and evaluation of deep neural networks), we show that even\nsimple linear datamodels can successfully predict model outputs. We then\ndemonstrate that datamodels give rise to a variety of applications, such as:\naccurately predicting the effect of dataset counterfactuals; identifying\nbrittle predictions; finding semantically similar examples; quantifying\ntrain-test leakage; and embedding data into a well-behaved and feature-rich\nrepresentation space. Data for this paper (including pre-computed datamodels as\nwell as raw predictions from four million trained deep neural networks) is\navailable at https://github.com/MadryLab/datamodels-data .",
        "date": "2022-02-01T18:15:24+00:00",
        "label": 1
    },
    "2210.14177": {
        "title": "Influence Functions for Sequence Tagging Models",
        "abstract": "Many language tasks (e.g., Named Entity Recognition, Part-of-Speech tagging,\nand Semantic Role Labeling) are naturally framed as sequence tagging problems.\nHowever, there has been comparatively little work on interpretability methods\nfor sequence tagging models. In this paper, we extend influence functions -\nwhich aim to trace predictions back to the training points that informed them -\nto sequence tagging tasks. We define the influence of a training instance\nsegment as the effect that perturbing the labels within this segment has on a\ntest segment level prediction. We provide an efficient approximation to compute\nthis, and show that it tracks with the true segment influence, measured\nempirically. We show the practical utility of segment influence by using the\nmethod to identify systematic annotation errors in two named entity recognition\ncorpora. Code to reproduce our results is available at\nhttps://github.com/successar/Segment_Influence_Functions.",
        "date": "2022-10-25T17:13:11+00:00",
        "label": 1
    },
    "2310.06825": {
        "title": "Mistral 7B",
        "abstract": "We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered\nfor superior performance and efficiency. Mistral 7B outperforms Llama 2 13B\nacross all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and\ncode generation. Our model leverages grouped-query attention (GQA) for faster\ninference, coupled with sliding window attention (SWA) to effectively handle\nsequences of arbitrary length with a reduced inference cost. We also provide a\nmodel fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses\nthe Llama 2 13B -- Chat model both on human and automated benchmarks. Our\nmodels are released under the Apache 2.0 license.",
        "date": "2023-10-10T17:54:58+00:00",
        "label": 1
    },
    "2401.04088": {
        "title": "Mixtral of Experts",
        "abstract": "We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model.\nMixtral has the same architecture as Mistral 7B, with the difference that each\nlayer is composed of 8 feedforward blocks (i.e. experts). For every token, at\neach layer, a router network selects two experts to process the current state\nand combine their outputs. Even though each token only sees two experts, the\nselected experts can be different at each timestep. As a result, each token has\naccess to 47B parameters, but only uses 13B active parameters during inference.\nMixtral was trained with a context size of 32k tokens and it outperforms or\nmatches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular,\nMixtral vastly outperforms Llama 2 70B on mathematics, code generation, and\nmultilingual benchmarks. We also provide a model fine-tuned to follow\ninstructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo,\nClaude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both\nthe base and instruct models are released under the Apache 2.0 license.",
        "date": "2024-01-08T18:47:34+00:00",
        "label": 1
    },
    "2401.06059": {
        "title": "Investigating Data Contamination for Pre-training Language Models",
        "abstract": "Language models pre-trained on web-scale corpora demonstrate impressive\ncapabilities on diverse downstream tasks. However, there is increasing concern\nwhether such capabilities might arise from evaluation datasets being included\nin the pre-training corpus -- a phenomenon known as \\textit{data contamination}\n-- in a manner that artificially increases performance. There has been little\nunderstanding of how this potential contamination might influence LMs'\nperformance on downstream tasks. In this paper, we explore the impact of data\ncontamination at the pre-training stage by pre-training a series of GPT-2\nmodels \\textit{from scratch}. We highlight the effect of both text\ncontamination (\\textit{i.e.}\\ input text of the evaluation samples) and\nground-truth contamination (\\textit{i.e.}\\ the prompts asked on the input and\nthe desired outputs) from evaluation data. We also investigate the effects of\nrepeating contamination for various downstream tasks. Additionally, we examine\nthe prevailing n-gram-based definitions of contamination within current LLM\nreports, pinpointing their limitations and inadequacy. Our findings offer new\ninsights into data contamination's effects on language model capabilities and\nunderscore the need for independent, comprehensive contamination assessments in\nLLM studies.",
        "date": "2024-01-11T17:24:49+00:00",
        "label": 1
    },
    "2306.02031": {
        "title": "DOS: Diverse Outlier Sampling for Out-of-Distribution Detection",
        "abstract": "Modern neural networks are known to give overconfident prediction for\nout-of-distribution inputs when deployed in the open world. It is common\npractice to leverage a surrogate outlier dataset to regularize the model during\ntraining, and recent studies emphasize the role of uncertainty in designing the\nsampling strategy for outlier dataset. However, the OOD samples selected solely\nbased on predictive uncertainty can be biased towards certain types, which may\nfail to capture the full outlier distribution. In this work, we empirically\nshow that diversity is critical in sampling outliers for OOD detection\nperformance. Motivated by the observation, we propose a straightforward and\nnovel sampling strategy named DOS (Diverse Outlier Sampling) to select diverse\nand informative outliers. Specifically, we cluster the normalized features at\neach iteration, and the most informative outlier from each cluster is selected\nfor model training with absent category loss. With DOS, the sampled outliers\nefficiently shape a globally compact decision boundary between ID and OOD data.\nExtensive experiments demonstrate the superiority of DOS, reducing the average\nFPR95 by up to 25.79% on CIFAR-100 with TI-300K.",
        "date": "2023-06-03T07:17:48+00:00",
        "label": 1
    },
    "2402.05356": {
        "title": "Exploring Learning Complexity for Downstream Data Pruning",
        "abstract": "The over-parameterized pre-trained models pose a great challenge to\nfine-tuning with limited computation resources. An intuitive solution is to\nprune the less informative samples from the fine-tuning dataset. A series of\ntraining-based scoring functions are proposed to quantify the informativeness\nof the data subset but the pruning cost becomes non-negligible due to the heavy\nparameter updating. For efficient pruning, it is viable to adapt the similarity\nscoring function of geometric-based methods from training-based to\ntraining-free. However, we empirically show that such adaption distorts the\noriginal pruning and results in inferior performance on the downstream tasks.\nIn this paper, we propose to treat the learning complexity (LC) as the scoring\nfunction for classification and regression tasks. Specifically, the learning\ncomplexity is defined as the average predicted confidence of subnets with\ndifferent capacities, which encapsulates data processing within a converged\nmodel. Then we preserve the diverse and easy samples for fine-tuning. Extensive\nexperiments with vision datasets demonstrate the effectiveness and efficiency\nof the proposed scoring function for classification tasks. For the instruction\nfine-tuning of large language models, our method achieves state-of-the-art\nperformance with stable convergence, outperforming the full training with only\n10\\% of the instruction dataset.",
        "date": "2024-02-08T02:29:33+00:00",
        "label": 1
    },
    "2406.14026": {
        "title": "Demystifying Forgetting in Language Model Fine-Tuning with Statistical Analysis of Example Associations",
        "abstract": "Language models (LMs) are known to suffer from forgetting of previously\nlearned examples when fine-tuned, breaking stability of deployed LM systems.\nDespite efforts on mitigating forgetting, few have investigated whether, and\nhow forgotten upstream examples are associated with newly learned tasks.\nInsights on such associations enable efficient and targeted mitigation of\nforgetting. In this paper, we empirically analyze forgetting that occurs in $N$\nupstream examples while the model learns $M$ new tasks and visualize their\nassociations with a $M \\times N$ matrix. We empirically demonstrate that the\ndegree of forgetting can often be approximated by simple multiplicative\ncontributions of the upstream examples and newly learned tasks. We also reveal\nmore complicated patterns where specific subsets of examples are forgotten with\nstatistics and visualization. Following our analysis, we predict forgetting\nthat happens on upstream examples when learning a new task with matrix\ncompletion over the empirical associations, outperforming prior approaches that\nrely on trainable LMs. Project website:\nhttps://inklab.usc.edu/lm-forgetting-prediction/",
        "date": "2024-06-20T06:46:23+00:00",
        "label": 1
    },
    "2402.01865": {
        "title": "What Will My Model Forget? Forecasting Forgotten Examples in Language Model Refinement",
        "abstract": "Language models deployed in the wild make errors. However, simply updating\nthe model with the corrected error instances causes catastrophic forgetting --\nthe updated model makes errors on instances learned during the instruction\ntuning or upstream training phase. Randomly replaying upstream data yields\nunsatisfactory performance and often comes with high variance and poor\ncontrollability. To this end, we try to forecast upstream examples that will be\nforgotten due to a model update for improved controllability of the replay\nprocess and interpretability. We train forecasting models given a collection of\nonline learned examples and corresponding forgotten upstream pre-training\nexamples. We propose a partially interpretable forecasting model based on the\nobservation that changes in pre-softmax logit scores of pretraining examples\nresemble that of online learned examples, which performs decently on BART but\nfails on T5 models. We further show a black-box classifier based on inner\nproducts of example representations achieves better forecasting performance\nover a series of setups. Finally, we show that we reduce forgetting of upstream\npretraining examples by replaying examples that are forecasted to be forgotten,\ndemonstrating the practical utility of forecasting example forgetting.",
        "date": "2024-02-02T19:43:15+00:00",
        "label": 1
    },
    "2110.08534": {
        "title": "Lifelong Pretraining: Continually Adapting Language Models to Emerging Corpora",
        "abstract": "Pretrained language models (PTLMs) are typically learned over a large, static\ncorpus and further fine-tuned for various downstream tasks. However, when\ndeployed in the real world, a PTLM-based model must deal with data\ndistributions that deviate from what the PTLM was initially trained on. In this\npaper, we study a lifelong language model pretraining challenge where a PTLM is\ncontinually updated so as to adapt to emerging data. Over a domain-incremental\nresearch paper stream and a chronologically-ordered tweet stream, we\nincrementally pretrain a PTLM with different continual learning algorithms, and\nkeep track of the downstream task performance (after fine-tuning). We evaluate\nPTLM's ability to adapt to new corpora while retaining learned knowledge in\nearlier corpora. Our experiments show distillation-based approaches to be most\neffective in retaining downstream performance in earlier domains. The\nalgorithms also improve knowledge transfer, allowing models to achieve better\ndownstream performance over the latest data, and improve temporal\ngeneralization when distribution gaps exist between training and evaluation\nbecause of time. We believe our problem formulation, methods, and analysis will\ninspire future studies towards continual pretraining of language models.",
        "date": "2021-10-16T09:59:33+00:00",
        "label": 1
    },
    "2207.05221": {
        "title": "Language Models (Mostly) Know What They Know",
        "abstract": "We study whether language models can evaluate the validity of their own\nclaims and predict which questions they will be able to answer correctly. We\nfirst show that larger models are well-calibrated on diverse multiple choice\nand true/false questions when they are provided in the right format. Thus we\ncan approach self-evaluation on open-ended sampling tasks by asking models to\nfirst propose answers, and then to evaluate the probability \"P(True)\" that\ntheir answers are correct. We find encouraging performance, calibration, and\nscaling for P(True) on a diverse array of tasks. Performance at self-evaluation\nfurther improves when we allow models to consider many of their own samples\nbefore predicting the validity of one specific possibility. Next, we\ninvestigate whether models can be trained to predict \"P(IK)\", the probability\nthat \"I know\" the answer to a question, without reference to any particular\nproposed answer. Models perform well at predicting P(IK) and partially\ngeneralize across tasks, though they struggle with calibration of P(IK) on new\ntasks. The predicted P(IK) probabilities also increase appropriately in the\npresence of relevant source materials in the context, and in the presence of\nhints towards the solution of mathematical word problems. We hope these\nobservations lay the groundwork for training more honest models, and for\ninvestigating how honesty generalizes to cases where models are trained on\nobjectives other than the imitation of human writing.",
        "date": "2022-07-11T22:59:39+00:00",
        "label": 1
    },
    "2001.08361": {
        "title": "Scaling Laws for Neural Language Models",
        "abstract": "We study empirical scaling laws for language model performance on the\ncross-entropy loss. The loss scales as a power-law with model size, dataset\nsize, and the amount of compute used for training, with some trends spanning\nmore than seven orders of magnitude. Other architectural details such as\nnetwork width or depth have minimal effects within a wide range. Simple\nequations govern the dependence of overfitting on model/dataset size and the\ndependence of training speed on model size. These relationships allow us to\ndetermine the optimal allocation of a fixed compute budget. Larger models are\nsignificantly more sample-efficient, such that optimally compute-efficient\ntraining involves training very large models on a relatively modest amount of\ndata and stopping significantly before convergence.",
        "date": "2020-01-23T03:59:20+00:00",
        "label": 1
    },
    "2104.14337": {
        "title": "Dynabench: Rethinking Benchmarking in NLP",
        "abstract": "We introduce Dynabench, an open-source platform for dynamic dataset creation\nand model benchmarking. Dynabench runs in a web browser and supports\nhuman-and-model-in-the-loop dataset creation: annotators seek to create\nexamples that a target model will misclassify, but that another person will\nnot. In this paper, we argue that Dynabench addresses a critical need in our\ncommunity: contemporary models quickly achieve outstanding performance on\nbenchmark tasks but nonetheless fail on simple challenge examples and falter in\nreal-world scenarios. With Dynabench, dataset creation, model development, and\nmodel assessment can directly inform each other, leading to more robust and\ninformative benchmarks. We report on four initial NLP tasks, illustrating these\nconcepts and highlighting the promise of the platform, and address potential\nobjections to dynamic benchmarking as a new standard for the field.",
        "date": "2021-04-07T17:49:17+00:00",
        "label": 1
    },
    "2405.06331": {
        "title": "LMD3: Language Model Data Density Dependence",
        "abstract": "We develop a methodology for analyzing language model task performance at the\nindividual example level based on training data density estimation. Experiments\nwith paraphrasing as a controlled intervention on finetuning data demonstrate\nthat increasing the support in the training distribution for specific test\nqueries results in a measurable increase in density, which is also a\nsignificant predictor of the performance increase caused by the intervention.\nExperiments with pretraining data demonstrate that we can explain a significant\nfraction of the variance in model perplexity via density measurements. We\nconclude that our framework can provide statistical evidence of the dependence\nof a target model's predictions on subsets of its training data, and can more\ngenerally be used to characterize the support (or lack thereof) in the training\ndata for a given test task.",
        "date": "2024-05-10T09:03:27+00:00",
        "label": 1
    },
    "2303.14753": {
        "title": "Does \"Deep Learning on a Data Diet\" reproduce? Overall yes, but GraNd at Initialization does not",
        "abstract": "The paper 'Deep Learning on a Data Diet' by Paul et al. (2021) introduces two\ninnovative metrics for pruning datasets during the training of neural networks.\nWhile we are able to replicate the results for the EL2N score at epoch 20, the\nsame cannot be said for the GraNd score at initialization. The GraNd scores\nlater in training provide useful pruning signals, however. The GraNd score at\ninitialization calculates the average gradient norm of an input sample across\nmultiple randomly initialized models before any training has taken place. Our\nanalysis reveals a strong correlation between the GraNd score at initialization\nand the input norm of a sample, suggesting that the latter could have been a\ncheap new baseline for data pruning. Unfortunately, neither the GraNd score at\ninitialization nor the input norm surpasses random pruning in performance. This\ncontradicts one of the findings in Paul et al. (2021). We were unable to\nreproduce their CIFAR-10 results using both an updated version of the original\nJAX repository and in a newly implemented PyTorch codebase. An investigation of\nthe underlying JAX/FLAX code from 2021 surfaced a bug in the checkpoint\nrestoring code that was fixed in April 2021\n(https://github.com/google/flax/commit/28fbd95500f4bf2f9924d2560062fa50e919b1a5).",
        "date": "2023-03-26T15:13:19+00:00",
        "label": 1
    },
    "2305.11383": {
        "title": "Do Models Really Learn to Follow Instructions? An Empirical Study of Instruction Tuning",
        "abstract": "Recent works on instruction tuning (IT) have achieved great performance with\nzero-shot generalizability to unseen tasks. With additional context (e.g., task\ndefinition, examples) provided to models for fine-tuning, they achieved much\nhigher performance than untuned models. Despite impressive performance gains,\nwhat models learn from IT remains understudied. In this work, we analyze how\nmodels utilize instructions during IT by comparing model training with altered\nvs. original instructions. Specifically, we create simplified task definitions\nby removing all semantic components and only leaving the output space\ninformation, and delusive examples that contain incorrect input-output mapping.\nOur experiments show that models trained on simplified task definition or\ndelusive examples can achieve comparable performance to the ones trained on the\noriginal instructions and examples. Furthermore, we introduce a random baseline\nto perform zeroshot classification tasks, and find it achieves similar\nperformance (42.6% exact-match) as IT does (43% exact-match) in low resource\nsetting, while both methods outperform naive T5 significantly (30% per\nexact-match). Our analysis provides evidence that the impressive performance\ngain of current IT models can come from picking up superficial patterns, such\nas learning the output format and guessing. Our study highlights the urgent\nneed for more reliable IT methods and evaluation.",
        "date": "2023-05-19T02:00:47+00:00",
        "label": 1
    },
    "2311.00288": {
        "title": "Active Instruction Tuning: Improving Cross-Task Generalization by Training on Prompt Sensitive Tasks",
        "abstract": "Instruction tuning (IT) achieves impressive zero-shot generalization results\nby training large language models (LLMs) on a massive amount of diverse tasks\nwith instructions. However, how to select new tasks to improve the performance\nand generalizability of IT models remains an open question. Training on all\nexisting tasks is impractical due to prohibiting computation requirements, and\nrandomly selecting tasks can lead to suboptimal performance. In this work, we\npropose active instruction tuning based on prompt uncertainty, a novel\nframework to identify informative tasks, and then actively tune the models on\nthe selected tasks. We represent the informativeness of new tasks with the\ndisagreement of the current model outputs over perturbed prompts. Our\nexperiments on NIV2 and Self-Instruct datasets demonstrate that our method\nconsistently outperforms other baseline strategies for task selection,\nachieving better out-of-distribution generalization with fewer training tasks.\nAdditionally, we introduce a task map that categorizes and diagnoses tasks\nbased on prompt uncertainty and prediction probability. We discover that\ntraining on ambiguous (prompt-uncertain) tasks improves generalization while\ntraining on difficult (prompt-certain and low-probability) tasks offers no\nbenefit, underscoring the importance of task selection for instruction tuning.",
        "date": "2023-11-01T04:40:05+00:00",
        "label": 1
    },
    "2110.14049": {
        "title": "Beta Shapley: a Unified and Noise-reduced Data Valuation Framework for Machine Learning",
        "abstract": "Data Shapley has recently been proposed as a principled framework to quantify\nthe contribution of individual datum in machine learning. It can effectively\nidentify helpful or harmful data points for a learning algorithm. In this\npaper, we propose Beta Shapley, which is a substantial generalization of Data\nShapley. Beta Shapley arises naturally by relaxing the efficiency axiom of the\nShapley value, which is not critical for machine learning settings. Beta\nShapley unifies several popular data valuation methods and includes data\nShapley as a special case. Moreover, we prove that Beta Shapley has several\ndesirable statistical properties and propose efficient algorithms to estimate\nit. We demonstrate that Beta Shapley outperforms state-of-the-art data\nvaluation methods on several downstream ML tasks such as: 1) detecting\nmislabeled training data; 2) learning with subsamples; and 3) identifying\npoints whose addition or removal have the largest positive or negative impact\non the model.",
        "date": "2021-10-26T22:03:55+00:00",
        "label": 1
    },
    "2003.08529": {
        "title": "Diversity, Density, and Homogeneity: Quantitative Characteristic Metrics for Text Collections",
        "abstract": "Summarizing data samples by quantitative measures has a long history, with\ndescriptive statistics being a case in point. However, as natural language\nprocessing methods flourish, there are still insufficient characteristic\nmetrics to describe a collection of texts in terms of the words, sentences, or\nparagraphs they comprise. In this work, we propose metrics of diversity,\ndensity, and homogeneity that quantitatively measure the dispersion, sparsity,\nand uniformity of a text collection. We conduct a series of simulations to\nverify that each metric holds desired properties and resonates with human\nintuitions. Experiments on real-world datasets demonstrate that the proposed\ncharacteristic metrics are highly correlated with text classification\nperformance of a renowned model, BERT, which could inspire future applications.",
        "date": "2020-03-19T00:48:32+00:00",
        "label": 1
    },
    "1904.03122": {
        "title": "Outlier Detection for Improved Data Quality and Diversity in Dialog Systems",
        "abstract": "In a corpus of data, outliers are either errors: mistakes in the data that\nare counterproductive, or are unique: informative samples that improve model\nrobustness. Identifying outliers can lead to better datasets by (1) removing\nnoise in datasets and (2) guiding collection of additional data to fill gaps.\nHowever, the problem of detecting both outlier types has received relatively\nlittle attention in NLP, particularly for dialog systems. We introduce a simple\nand effective technique for detecting both erroneous and unique samples in a\ncorpus of short texts using neural sentence embeddings combined with\ndistance-based outlier detection. We also present a novel data collection\npipeline built atop our detection technique to automatically and iteratively\nmine unique data samples while discarding erroneous samples. Experiments show\nthat our outlier detection technique is effective at finding errors while our\ndata collection pipeline yields highly diverse corpora that in turn produce\nmore robust intent classification and slot-filling models.",
        "date": "2019-04-05T15:31:28+00:00",
        "label": 1
    },
    "2306.13840": {
        "title": "Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data",
        "abstract": "Current trends in pre-training Large Language Models (LLMs) primarily focus\non the scaling of model and dataset size. While the quality of pre-training\ndata is considered an important factor for training powerful LLMs, it remains a\nnebulous concept that has not been rigorously characterized. To this end, we\npropose a formalization of one key aspect of data quality -- measuring the\nvariability of natural language data -- specifically via a measure we call the\ndiversity coefficient. Our empirical analysis shows that the proposed diversity\ncoefficient aligns with the intuitive properties of diversity and variability,\ne.g., it increases as the number of latent concepts increases. Then, we measure\nthe diversity coefficient of publicly available pre-training datasets and\ndemonstrate that their formal diversity is high compared to theoretical lower\nand upper bounds. Finally, we conduct a comprehensive set of controlled\ninterventional experiments with GPT-2 and LLaMAv2 that demonstrate the\ndiversity coefficient of pre-training data characterizes useful aspects of\ndownstream model evaluation performance -- totaling 44 models of various sizes\n(51M to 7B parameters). We conclude that our formal notion of diversity is an\nimportant aspect of data quality that captures variability and causally leads\nto improved evaluation performance.",
        "date": "2023-06-24T02:25:56+00:00",
        "label": 1
    },
    "2107.06499": {
        "title": "Deduplicating Training Data Makes Language Models Better",
        "abstract": "We find that existing language modeling datasets contain many near-duplicate\nexamples and long repetitive substrings. As a result, over 1% of the unprompted\noutput of language models trained on these datasets is copied verbatim from the\ntraining data. We develop two tools that allow us to deduplicate training\ndatasets -- for example removing from C4 a single 61 word English sentence that\nis repeated over 60,000 times. Deduplication allows us to train models that\nemit memorized text ten times less frequently and require fewer train steps to\nachieve the same or better accuracy. We can also reduce train-test overlap,\nwhich affects over 4% of the validation set of standard datasets, thus allowing\nfor more accurate evaluation. We release code for reproducing our work and\nperforming dataset deduplication at\nhttps://github.com/google-research/deduplicate-text-datasets.",
        "date": "2021-07-14T06:06:52+00:00",
        "label": 1
    },
    "2402.13064": {
        "title": "Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models",
        "abstract": "We introduce Generalized Instruction Tuning (called GLAN), a general and\nscalable method for instruction tuning of Large Language Models (LLMs). Unlike\nprior work that relies on seed examples or existing datasets to construct\ninstruction tuning data, GLAN exclusively utilizes a pre-curated taxonomy of\nhuman knowledge and capabilities as input and generates large-scale synthetic\ninstruction data across all disciplines. Specifically, inspired by the\nsystematic structure in human education system, we build the taxonomy by\ndecomposing human knowledge and capabilities to various fields, sub-fields and\nultimately, distinct disciplines semi-automatically, facilitated by LLMs.\nSubsequently, we generate a comprehensive list of subjects for every discipline\nand proceed to design a syllabus tailored to each subject, again utilizing\nLLMs. With the fine-grained key concepts detailed in every class session of the\nsyllabus, we are able to generate diverse instructions with a broad coverage\nacross the entire spectrum of human knowledge and skills. Extensive experiments\non large language models (e.g., Mistral) demonstrate that GLAN excels in\nmultiple dimensions from mathematical reasoning, coding, academic exams,\nlogical reasoning to general instruction following without using task-specific\ntraining data of these tasks. In addition, GLAN allows for easy customization\nand new fields or skills can be added by simply incorporating a new node into\nour taxonomy.",
        "date": "2024-02-20T15:00:35+00:00",
        "label": 1
    },
    "1510.03055": {
        "title": "A Diversity-Promoting Objective Function for Neural Conversation Models",
        "abstract": "Sequence-to-sequence neural network models for generation of conversational\nresponses tend to generate safe, commonplace responses (e.g., \"I don't know\")\nregardless of the input. We suggest that the traditional objective function,\ni.e., the likelihood of output (response) given input (message) is unsuited to\nresponse generation tasks. Instead we propose using Maximum Mutual Information\n(MMI) as the objective function in neural models. Experimental results\ndemonstrate that the proposed MMI models produce more diverse, interesting, and\nappropriate responses, yielding substantive gains in BLEU scores on two\nconversational datasets and in human evaluations.",
        "date": "2015-10-11T14:04:57+00:00",
        "label": 1
    },
    "2402.00530": {
        "title": "Superfiltering: Weak-to-Strong Data Filtering for Fast Instruction-Tuning",
        "abstract": "Instruction tuning is critical to improve LLMs but usually suffers from\nlow-quality and redundant data. Data filtering for instruction tuning has\nproved important in improving both the efficiency and performance of the tuning\nprocess. But it also leads to extra cost and computation due to the involvement\nof LLMs in this process. To reduce the filtering cost, we study Superfiltering:\nCan we use a smaller and weaker model to select data for finetuning a larger\nand stronger model? Despite the performance gap between weak and strong\nlanguage models, we find their highly consistent capability to perceive\ninstruction difficulty and data selection results. This enables us to use a\nmuch smaller and more efficient model to filter the instruction data used to\ntrain a larger language model. Not only does it largely speed up the data\nfiltering, but the filtered-data-finetuned LLM achieves even better performance\non standard benchmarks. Extensive experiments validate the efficacy and\nefficiency of our approach.",
        "date": "2024-02-01T11:57:53+00:00",
        "label": 1
    },
    "2308.12032": {
        "title": "From Quantity to Quality: Boosting LLM Performance with Self-Guided Data Selection for Instruction Tuning",
        "abstract": "In the realm of Large Language Models (LLMs), the balance between instruction\ndata quality and quantity is a focal point. Recognizing this, we introduce a\nself-guided methodology for LLMs to autonomously discern and select cherry\nsamples from open-source datasets, effectively minimizing manual curation and\npotential cost for instruction tuning an LLM. Our key innovation, the\nInstruction-Following Difficulty (IFD) metric, emerges as a pivotal metric to\nidentify discrepancies between a model's expected responses and its intrinsic\ngeneration capability. Through the application of IFD, cherry samples can be\npinpointed, leading to a marked uptick in model training efficiency. Empirical\nvalidations on datasets like Alpaca and WizardLM underpin our findings; with a\nmere $10\\%$ of original data input, our strategy showcases improved results.\nThis synthesis of self-guided cherry-picking and the IFD metric signifies a\ntransformative leap in the instruction tuning of LLMs, promising both\nefficiency and resource-conscious advancements. Codes, data, and models are\navailable: https://github.com/tianyi-lab/Cherry_LLM",
        "date": "2023-08-23T09:45:29+00:00",
        "label": 1
    },
    "2305.06161": {
        "title": "StarCoder: may the source be with you!",
        "abstract": "The BigCode community, an open-scientific collaboration working on the\nresponsible development of Large Language Models for Code (Code LLMs),\nintroduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context\nlength, infilling capabilities and fast large-batch inference enabled by\nmulti-query attention. StarCoderBase is trained on 1 trillion tokens sourced\nfrom The Stack, a large collection of permissively licensed GitHub repositories\nwith inspection tools and an opt-out process. We fine-tuned StarCoderBase on\n35B Python tokens, resulting in the creation of StarCoder. We perform the most\ncomprehensive evaluation of Code LLMs to date and show that StarCoderBase\noutperforms every open Code LLM that supports multiple programming languages\nand matches or outperforms the OpenAI code-cushman-001 model. Furthermore,\nStarCoder outperforms every model that is fine-tuned on Python, can be prompted\nto achieve 40\\% pass@1 on HumanEval, and still retains its performance on other\nprogramming languages. We take several important steps towards a safe\nopen-access model release, including an improved PII redaction pipeline and a\nnovel attribution tracing tool, and make the StarCoder models publicly\navailable under a more commercially viable version of the Open Responsible AI\nModel license.",
        "date": "2023-05-09T08:16:42+00:00",
        "label": 1
    },
    "2308.06259": {
        "title": "Self-Alignment with Instruction Backtranslation",
        "abstract": "We present a scalable method to build a high quality instruction following\nlanguage model by automatically labelling human-written text with corresponding\ninstructions. Our approach, named instruction backtranslation, starts with a\nlanguage model finetuned on a small amount of seed data, and a given web\ncorpus. The seed model is used to construct training examples by generating\ninstruction prompts for web documents (self-augmentation), and then selecting\nhigh quality examples from among these candidates (self-curation). This data is\nthen used to finetune a stronger model. Finetuning LLaMa on two iterations of\nour approach yields a model that outperforms all other LLaMa-based models on\nthe Alpaca leaderboard not relying on distillation data, demonstrating highly\neffective self-alignment.",
        "date": "2023-08-11T17:47:54+00:00",
        "label": 1
    },
    "2302.12366": {
        "title": "Less is More: Data Pruning for Faster Adversarial Training",
        "abstract": "Deep neural networks (DNNs) are sensitive to adversarial examples, resulting\nin fragile and unreliable performance in the real world. Although adversarial\ntraining (AT) is currently one of the most effective methodologies to robustify\nDNNs, it is computationally very expensive (e.g., 5-10X costlier than standard\ntraining). To address this challenge, existing approaches focus on single-step\nAT, referred to as Fast AT, reducing the overhead of adversarial example\ngeneration. Unfortunately, these approaches are known to fail against stronger\nadversaries. To make AT computationally efficient without compromising\nrobustness, this paper takes a different view of the efficient AT problem.\nSpecifically, we propose to minimize redundancies at the data level by\nleveraging data pruning. Extensive experiments demonstrate that the data\npruning based AT can achieve similar or superior robust (and clean) accuracy as\nits unpruned counterparts while being significantly faster. For instance,\nproposed strategies accelerate CIFAR-10 training up to 3.44X and CIFAR-100\ntraining to 2.02X. Additionally, the data pruning methods can readily be\nreconciled with existing adversarial acceleration tricks to obtain the striking\nspeed-ups of 5.66X and 5.12X on CIFAR-10, 3.67X and 3.07X on CIFAR-100 with\nTRADES and MART, respectively.",
        "date": "2023-02-23T23:48:20+00:00",
        "label": 1
    },
    "2310.07849": {
        "title": "Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations",
        "abstract": "The collection and curation of high-quality training data is crucial for\ndeveloping text classification models with superior performance, but it is\noften associated with significant costs and time investment. Researchers have\nrecently explored using large language models (LLMs) to generate synthetic\ndatasets as an alternative approach. However, the effectiveness of the\nLLM-generated synthetic data in supporting model training is inconsistent\nacross different classification tasks. To better understand factors that\nmoderate the effectiveness of the LLM-generated synthetic data, in this study,\nwe look into how the performance of models trained on these synthetic data may\nvary with the subjectivity of classification. Our results indicate that\nsubjectivity, at both the task level and instance level, is negatively\nassociated with the performance of the model trained on synthetic data. We\nconclude by discussing the implications of our work on the potential and\nlimitations of leveraging LLM for synthetic data generation.",
        "date": "2023-10-11T19:51:13+00:00",
        "label": 1
    },
    "2211.09110": {
        "title": "Holistic Evaluation of Language Models",
        "abstract": "Language models (LMs) are becoming the foundation for almost all major\nlanguage technologies, but their capabilities, limitations, and risks are not\nwell understood. We present Holistic Evaluation of Language Models (HELM) to\nimprove the transparency of language models. First, we taxonomize the vast\nspace of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)\nthat are of interest for LMs. Then we select a broad subset based on coverage\nand feasibility, noting what's missing or underrepresented (e.g. question\nanswering for neglected English dialects, metrics for trustworthiness). Second,\nwe adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,\nrobustness, fairness, bias, toxicity, and efficiency) for each of 16 core\nscenarios when possible (87.5% of the time). This ensures metrics beyond\naccuracy don't fall to the wayside, and that trade-offs are clearly exposed. We\nalso perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze\nspecific aspects (e.g. reasoning, disinformation). Third, we conduct a\nlarge-scale evaluation of 30 prominent language models (spanning open,\nlimited-access, and closed models) on all 42 scenarios, 21 of which were not\npreviously used in mainstream LM evaluation. Prior to HELM, models on average\nwere evaluated on just 17.9% of the core HELM scenarios, with some prominent\nmodels not sharing a single scenario in common. We improve this to 96.0%: now\nall 30 models have been densely benchmarked on the same core scenarios and\nmetrics under standardized conditions. Our evaluation surfaces 25 top-level\nfindings. For full transparency, we release all raw model prompts and\ncompletions publicly for further analysis, as well as a general modular\ntoolkit. We intend for HELM to be a living benchmark for the community,\ncontinuously updated with new scenarios, metrics, and models.",
        "date": "2022-11-16T18:51:34+00:00",
        "label": 1
    },
    "2109.07958": {
        "title": "TruthfulQA: Measuring How Models Mimic Human Falsehoods",
        "abstract": "We propose a benchmark to measure whether a language model is truthful in\ngenerating answers to questions. The benchmark comprises 817 questions that\nspan 38 categories, including health, law, finance and politics. We crafted\nquestions that some humans would answer falsely due to a false belief or\nmisconception. To perform well, models must avoid generating false answers\nlearned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a\nT5-based model. The best model was truthful on 58% of questions, while human\nperformance was 94%. Models generated many false answers that mimic popular\nmisconceptions and have the potential to deceive humans. The largest models\nwere generally the least truthful. This contrasts with other NLP tasks, where\nperformance improves with model size. However, this result is expected if false\nanswers are learned from the training distribution. We suggest that scaling up\nmodels alone is less promising for improving truthfulness than fine-tuning\nusing training objectives other than imitation of text from the web.",
        "date": "2021-09-08T17:15:27+00:00",
        "label": 1
    },
    "2401.17197": {
        "title": "Data-efficient Fine-tuning for LLM-based Recommendation",
        "abstract": "Leveraging Large Language Models (LLMs) for recommendation has recently\ngarnered considerable attention, where fine-tuning plays a key role in LLMs'\nadaptation. However, the cost of fine-tuning LLMs on rapidly expanding\nrecommendation data limits their practical application. To address this\nchallenge, few-shot fine-tuning offers a promising approach to quickly adapt\nLLMs to new recommendation data. We propose the task of data pruning for\nefficient LLM-based recommendation, aimed at identifying representative samples\ntailored for LLMs' few-shot fine-tuning. While coreset selection is closely\nrelated to the proposed task, existing coreset selection methods often rely on\nsuboptimal heuristic metrics or entail costly optimization on large-scale\nrecommendation data.\n  To tackle these issues, we introduce two objectives for the data pruning task\nin the context of LLM-based recommendation: 1) high accuracy aims to identify\nthe influential samples that can lead to high overall performance; and 2) high\nefficiency underlines the low costs of the data pruning process. To pursue the\ntwo objectives, we propose a novel data pruning method based on two scores,\ni.e., influence score and effort score, to efficiently identify the influential\nsamples. Particularly, the influence score is introduced to accurately estimate\nthe influence of sample removal on the overall performance. To achieve low\ncosts of the data pruning process, we use a small-sized surrogate model to\nreplace LLMs to obtain the influence score. Considering the potential gap\nbetween the surrogate model and LLMs, we further propose an effort score to\nprioritize some hard samples specifically for LLMs. Empirical results on three\nreal-world datasets validate the effectiveness of our proposed method. In\nparticular, the proposed method uses only 2% samples to surpass the full data\nfine-tuning, reducing time costs by 97%.",
        "date": "2024-01-30T17:31:19+00:00",
        "label": 1
    },
    "2401.08565": {
        "title": "Tuning Language Models by Proxy",
        "abstract": "Despite the general capabilities of large pretrained language models, they\nconsistently benefit from further adaptation to better achieve desired\nbehaviors. However, tuning these models has become increasingly\nresource-intensive, or impossible when model weights are private. We introduce\nproxy-tuning, a lightweight decoding-time algorithm that operates on top of\nblack-box LMs to achieve the same end as direct tuning, but by accessing only\nits predictions over the output vocabulary, not its parameters. Our method\ntunes a smaller LM, then applies the difference between the predictions of the\nsmall tuned and untuned LMs to shift the original predictions of the larger\nuntuned model in the direction of tuning, while retaining the benefits of\nlarger-scale pretraining. In experiments, when we apply proxy-tuning to\nLlama2-70B using proxies of only 7B size, we can close 88% of the gap between\nLlama2-70B and its truly-tuned chat version, when evaluated across knowledge,\nreasoning, and safety benchmarks. We then demonstrate the generality of\nproxy-tuning by applying it to domain adaptation on code, and task-specific\nfinetuning on question-answering and math problems. Finally, we show how to\nproxy-tune a truly black-box LM, GPT-3.5, for temporal adaptation, increasing\nits knowledge about recent events. Our work demonstrates the promise of using\nsmall tuned LMs to efficiently customize large, potentially proprietary LMs\nthrough decoding-time guidance.",
        "date": "2024-01-16T18:49:55+00:00",
        "label": 1
    },
    "2311.10774": {
        "title": "MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning",
        "abstract": "With the rapid development of large language models (LLMs) and their\nintegration into large multimodal models (LMMs), there has been impressive\nprogress in zero-shot completion of user-oriented vision-language tasks.\nHowever, a gap remains in the domain of chart image understanding due to the\ndistinct abstract components in charts. To address this, we introduce a\nlarge-scale MultiModal Chart Instruction (\\textbf{MMC-Instruction}) dataset\ncomprising 600k instances supporting diverse tasks and chart types. Leveraging\nthis data, we develop MultiModal Chart Assistant (\\textbf{MMCA}), an LMM that\nachieves state-of-the-art performance on existing chart QA benchmarks.\nRecognizing the need for a comprehensive evaluation of LMM chart understanding,\nwe also propose a MultiModal Chart Benchmark (\\textbf{MMC-Benchmark}), a\ncomprehensive human-annotated benchmark with nine distinct tasks evaluating\nreasoning capabilities over charts. Extensive experiments on MMC-Benchmark\nreveal the limitations of existing LMMs on correctly interpreting charts, even\nfor the most recent GPT-4V model. Our work provides an instruction-tuning\nmethodology and benchmark to advance multimodal understanding of charts. Code\nand data are available at https://github.com/FuxiaoLiu/MMC.",
        "date": "2023-11-15T23:36:42+00:00",
        "label": 1
    },
    "2404.07840": {
        "title": "On Training Data Influence of GPT Models",
        "abstract": "Amidst the rapid advancements in generative language models, the\ninvestigation of how training data shapes the performance of GPT models is\nstill emerging. This paper presents GPTfluence, a novel approach that leverages\na featurized simulation to assess the impact of training examples on the\ntraining dynamics of GPT models. Our approach not only traces the influence of\nindividual training instances on performance trajectories, such as loss and\nother key metrics, on targeted test points but also enables a comprehensive\ncomparison with existing methods across various training scenarios in GPT\nmodels, ranging from 14 million to 2.8 billion parameters, across a range of\ndownstream tasks. Contrary to earlier methods that struggle with generalization\nto new data, GPTfluence introduces a parameterized simulation of training\ndynamics, demonstrating robust generalization capabilities to unseen training\ndata. This adaptability is evident across both fine-tuning and\ninstruction-tuning scenarios, spanning tasks in natural language understanding\nand generation. We will make our code and data publicly available.",
        "date": "2024-04-11T15:27:56+00:00",
        "label": 1
    },
    "2312.15685": {
        "title": "What Makes Good Data for Alignment? A Comprehensive Study of Automatic Data Selection in Instruction Tuning",
        "abstract": "Instruction tuning is a standard technique employed to align large language\nmodels to end tasks and user preferences after the initial pretraining phase.\nRecent research indicates the critical role of data engineering in instruction\ntuning -- when appropriately selected, only limited data is necessary to\nachieve superior performance. However, we still lack a principled understanding\nof what makes good instruction tuning data for alignment, and how we should\nselect data automatically and effectively. In this work, we delve deeply into\nautomatic data selection strategies for alignment. We start with controlled\nstudies to measure data across three dimensions: complexity, quality, and\ndiversity, along which we examine existing methods and introduce novel\ntechniques for enhanced data measurement. Subsequently, we propose a simple\nstrategy to select data samples based on the measurement. We present deita\n(short for Data-Efficient Instruction Tuning for Alignment), a series of models\nfine-tuned from LLaMA and Mistral models using data samples automatically\nselected with our proposed approach. Empirically, deita performs better or on\npar with the state-of-the-art open-source alignment models with only 6K SFT\ntraining data samples -- over 10x less than the data used in the baselines.\nWhen further trained with direct preference optimization (DPO),\ndeita-Mistral-7B + DPO trained with 6K SFT and 10K DPO samples achieve 7.55\nMT-Bench and 90.06% AlpacaEval scores. We anticipate this work to provide tools\non automatic data selection, facilitating data-efficient alignment. We release\nour models as well as the selected datasets for future researches to\neffectively align models more efficiently.",
        "date": "2023-12-25T10:29:28+00:00",
        "label": 1
    },
    "2403.09606": {
        "title": "Large Language Models and Causal Inference in Collaboration: A Comprehensive Survey",
        "abstract": "Causal inference has shown potential in enhancing the predictive accuracy,\nfairness, robustness, and explainability of Natural Language Processing (NLP)\nmodels by capturing causal relationships among variables. The emergence of\ngenerative Large Language Models (LLMs) has significantly impacted various NLP\ndomains, particularly through their advanced reasoning capabilities. This\nsurvey focuses on evaluating and improving LLMs from a causal view in the\nfollowing areas: understanding and improving the LLMs' reasoning capacity,\naddressing fairness and safety issues in LLMs, complementing LLMs with\nexplanations, and handling multimodality. Meanwhile, LLMs' strong reasoning\ncapacities can in turn contribute to the field of causal inference by aiding\ncausal relationship discovery and causal effect estimations. This review\nexplores the interplay between causal inference frameworks and LLMs from both\nperspectives, emphasizing their collective potential to further the development\nof more advanced and equitable artificial intelligence systems.",
        "date": "2024-03-14T17:47:20+00:00",
        "label": 1
    },
    "2402.18041": {
        "title": "Datasets for Large Language Models: A Comprehensive Survey",
        "abstract": "This paper embarks on an exploration into the Large Language Model (LLM)\ndatasets, which play a crucial role in the remarkable advancements of LLMs. The\ndatasets serve as the foundational infrastructure analogous to a root system\nthat sustains and nurtures the development of LLMs. Consequently, examination\nof these datasets emerges as a critical topic in research. In order to address\nthe current lack of a comprehensive overview and thorough analysis of LLM\ndatasets, and to gain insights into their current status and future trends,\nthis survey consolidates and categorizes the fundamental aspects of LLM\ndatasets from five perspectives: (1) Pre-training Corpora; (2) Instruction\nFine-tuning Datasets; (3) Preference Datasets; (4) Evaluation Datasets; (5)\nTraditional Natural Language Processing (NLP) Datasets. The survey sheds light\non the prevailing challenges and points out potential avenues for future\ninvestigation. Additionally, a comprehensive review of the existing available\ndataset resources is also provided, including statistics from 444 datasets,\ncovering 8 language categories and spanning 32 domains. Information from 20\ndimensions is incorporated into the dataset statistics. The total data size\nsurveyed surpasses 774.5 TB for pre-training corpora and 700M instances for\nother datasets. We aim to present the entire landscape of LLM text datasets,\nserving as a comprehensive reference for researchers in this field and\ncontributing to future studies. Related resources are available at:\nhttps://github.com/lmmlzn/Awesome-LLMs-Datasets.",
        "date": "2024-02-28T04:35:51+00:00",
        "label": 1
    },
    "1907.11692": {
        "title": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
        "abstract": "Language model pretraining has led to significant performance gains but\ncareful comparison between different approaches is challenging. Training is\ncomputationally expensive, often done on private datasets of different sizes,\nand, as we will show, hyperparameter choices have significant impact on the\nfinal results. We present a replication study of BERT pretraining (Devlin et\nal., 2019) that carefully measures the impact of many key hyperparameters and\ntraining data size. We find that BERT was significantly undertrained, and can\nmatch or exceed the performance of every model published after it. Our best\nmodel achieves state-of-the-art results on GLUE, RACE and SQuAD. These results\nhighlight the importance of previously overlooked design choices, and raise\nquestions about the source of recently reported improvements. We release our\nmodels and code.",
        "date": "2019-07-26T17:48:29+00:00",
        "label": 1
    },
    "2406.14115": {
        "title": "Take the essence and discard the dross: A Rethinking on Data Selection for Fine-Tuning Large Language Models",
        "abstract": "Data selection for fine-tuning Large Language Models (LLMs) aims to select a\nhigh-quality subset from a given candidate dataset to train a Pending Fine-tune\nModel (PFM) into a Selective-Enhanced Model (SEM). It can improve the model\nperformance and accelerate the training process. Although a few surveys have\ninvestigated related works of data selection, there is a lack of comprehensive\ncomparison between existing methods due to their various experimental settings.\nTo address this issue, we first propose a three-stage scheme for data selection\nand comprehensively review existing works according to this scheme. Then, we\ndesign a unified comparing method with ratio-based efficiency indicators and\nranking-based feasibility indicators to overcome the difficulty of comparing\nvarious models with diverse experimental settings. After an in-depth\ncomparative analysis, we find that the more targeted method with data-specific\nand model-specific quality labels has higher efficiency, but the introduction\nof additional noise information should be avoided when designing selection\nalgorithms. Finally, we summarize the trends in data selection and highlight\nthe short-term and long-term challenges to guide future research.",
        "date": "2024-06-20T08:58:58+00:00",
        "label": 1
    },
    "2302.04062": {
        "title": "Machine Learning for Synthetic Data Generation: A Review",
        "abstract": "Machine learning heavily relies on data, but real-world applications often\nencounter various data-related issues. These include data of poor quality,\ninsufficient data points leading to under-fitting of machine learning models,\nand difficulties in data access due to concerns surrounding privacy, safety,\nand regulations. In light of these challenges, the concept of synthetic data\ngeneration emerges as a promising alternative that allows for data sharing and\nutilization in ways that real-world data cannot facilitate. This paper presents\na comprehensive systematic review of existing studies that employ machine\nlearning models for the purpose of generating synthetic data. The review\nencompasses various perspectives, starting with the applications of synthetic\ndata generation, spanning computer vision, speech, natural language processing,\nhealthcare, and business domains. Additionally, it explores different machine\nlearning methods, with particular emphasis on neural network architectures and\ndeep generative models. The paper also addresses the crucial aspects of privacy\nand fairness concerns related to synthetic data generation. Furthermore, this\nstudy identifies the challenges and opportunities prevalent in this emerging\nfield, shedding light on the potential avenues for future research. By delving\ninto the intricacies of synthetic data generation, this paper aims to\ncontribute to the advancement of knowledge and inspire further exploration in\nsynthetic data generation.",
        "date": "2023-02-08T13:59:31+00:00",
        "label": 1
    },
    "2203.08242": {
        "title": "Data Contamination: From Memorization to Exploitation",
        "abstract": "Pretrained language models are typically trained on massive web-based\ndatasets, which are often \"contaminated\" with downstream test sets. It is not\nclear to what extent models exploit the contaminated data for downstream tasks.\nWe present a principled method to study this question. We pretrain BERT models\non joint corpora of Wikipedia and labeled downstream datasets, and fine-tune\nthem on the relevant task. Comparing performance between samples seen and\nunseen during pretraining enables us to define and quantify levels of\nmemorization and exploitation. Experiments with two models and three downstream\ntasks show that exploitation exists in some cases, but in others the models\nmemorize the contaminated data, but do not exploit it. We show that these two\nmeasures are affected by different factors such as the number of duplications\nof the contaminated data and the model size. Our results highlight the\nimportance of analyzing massive web-scale datasets to verify that progress in\nNLP is obtained by better language understanding and not better data\nexploitation.",
        "date": "2022-03-15T20:37:16+00:00",
        "label": 1
    },
    "2309.04564": {
        "title": "When Less is More: Investigating Data Pruning for Pretraining LLMs at Scale",
        "abstract": "Large volumes of text data have contributed significantly to the development\nof large language models (LLMs) in recent years. This data is typically\nacquired by scraping the internet, leading to pretraining datasets comprised of\nnoisy web text. To date, efforts to prune these datasets down to a higher\nquality subset have relied on hand-crafted heuristics encoded as rule-based\nfilters. In this work, we take a wider view and explore scalable estimates of\ndata quality that can be used to systematically measure the quality of\npretraining data. We perform a rigorous comparison at scale of the simple data\nquality estimator of perplexity, as well as more sophisticated and\ncomputationally intensive estimates of the Error L2-Norm and memorization.\nThese metrics are used to rank and prune pretraining corpora, and we\nsubsequently compare LLMs trained on these pruned datasets. Surprisingly, we\nfind that the simple technique of perplexity outperforms our more\ncomputationally expensive scoring methods. We improve over our no-pruning\nbaseline while training on as little as 30% of the original training dataset.\nOur work sets the foundation for unexplored strategies in automatically\ncurating high quality corpora and suggests the majority of pretraining data can\nbe removed while retaining performance.",
        "date": "2023-09-08T19:34:05+00:00",
        "label": 1
    },
    "1707.05589": {
        "title": "On the State of the Art of Evaluation in Neural Language Models",
        "abstract": "Ongoing innovations in recurrent neural network architectures have provided a\nsteady influx of apparently state-of-the-art results on language modelling\nbenchmarks. However, these have been evaluated using differing code bases and\nlimited computational resources, which represent uncontrolled sources of\nexperimental variation. We reevaluate several popular architectures and\nregularisation methods with large-scale automatic black-box hyperparameter\ntuning and arrive at the somewhat surprising conclusion that standard LSTM\narchitectures, when properly regularised, outperform more recent models. We\nestablish a new state of the art on the Penn Treebank and Wikitext-2 corpora,\nas well as strong baselines on the Hutter Prize dataset.",
        "date": "2017-07-18T12:35:53+00:00",
        "label": 1
    },
    "2208.01545": {
        "title": "The Curse of Low Task Diversity: On the Failure of Transfer Learning to Outperform MAML and Their Empirical Equivalence",
        "abstract": "Recently, it has been observed that a transfer learning solution might be all\nwe need to solve many few-shot learning benchmarks -- thus raising important\nquestions about when and how meta-learning algorithms should be deployed. In\nthis paper, we seek to clarify these questions by 1. proposing a novel metric\n-- the diversity coefficient -- to measure the diversity of tasks in a few-shot\nlearning benchmark and 2. by comparing Model-Agnostic Meta-Learning (MAML) and\ntransfer learning under fair conditions (same architecture, same optimizer, and\nall models trained to convergence). Using the diversity coefficient, we show\nthat the popular MiniImageNet and CIFAR-FS few-shot learning benchmarks have\nlow diversity. This novel insight contextualizes claims that transfer learning\nsolutions are better than meta-learned solutions in the regime of low diversity\nunder a fair comparison. Specifically, we empirically find that a low diversity\ncoefficient correlates with a high similarity between transfer learning and\nMAML learned solutions in terms of accuracy at meta-test time and\nclassification layer similarity (using feature based distance metrics like\nSVCCA, PWCCA, CKA, and OPD). To further support our claim, we find this\nmeta-test accuracy holds even as the model size changes. Therefore, we conclude\nthat in the low diversity regime, MAML and transfer learning have equivalent\nmeta-test performance when both are compared fairly. We also hope our work\ninspires more thoughtful constructions and quantitative evaluations of\nmeta-learning benchmarks in the future.",
        "date": "2022-08-02T15:49:11+00:00",
        "label": 1
    },
    "2008.03964": {
        "title": "DQI: A Guide to Benchmark Evaluation",
        "abstract": "A `state of the art' model A surpasses humans in a benchmark B, but fails on\nsimilar benchmarks C, D, and E. What does B have that the other benchmarks do\nnot? Recent research provides the answer: spurious bias. However, developing A\nto solve benchmarks B through E does not guarantee that it will solve future\nbenchmarks. To progress towards a model that `truly learns' an underlying task,\nwe need to quantify the differences between successive benchmarks, as opposed\nto existing binary and black-box approaches. We propose a novel approach to\nsolve this underexplored task of quantifying benchmark quality by debuting a\ndata quality metric: DQI.",
        "date": "2020-08-10T08:38:55+00:00",
        "label": 1
    },
    "2005.00816": {
        "title": "DQI: Measuring Data Quality in NLP",
        "abstract": "Neural language models have achieved human level performance across several\nNLP datasets. However, recent studies have shown that these models are not\ntruly learning the desired task; rather, their high performance is attributed\nto overfitting using spurious biases, which suggests that the capabilities of\nAI systems have been over-estimated. We introduce a generic formula for Data\nQuality Index (DQI) to help dataset creators create datasets free of such\nunwanted biases. We evaluate this formula using a recently proposed approach\nfor adversarial filtering, AFLite. We propose a new data creation paradigm\nusing DQI to create higher quality data. The data creation paradigm consists of\nseveral data visualizations to help data creators (i) understand the quality of\ndata and (ii) visualize the impact of the created data instance on the overall\nquality. It also has a couple of automation methods to (i) assist data creators\nand (ii) make the model more robust to adversarial attacks. We use DQI along\nwith these automation methods to renovate biased examples in SNLI. We show that\nmodels trained on the renovated SNLI dataset generalize better to out of\ndistribution tasks. Renovation results in reduced model performance, exposing a\nlarge gap with respect to human performance. DQI systematically helps in\ncreating harder benchmarks using active learning. Our work takes the process of\ndynamic dataset creation forward, wherein datasets evolve together with the\nevolving state of the art, therefore serving as a means of benchmarking the\ntrue progress of AI.",
        "date": "2020-05-02T12:34:17+00:00",
        "label": 1
    },
    "2403.00526": {
        "title": "Data Quality Assessment: Challenges and Opportunities",
        "abstract": "Data-oriented applications, their users, and even the law require data of\nhigh quality. Research has broken down the rather vague notion of data quality\ninto various dimensions, such as accuracy, consistency, and reputation, to name\nbut a few. To achieve the goal of high data quality, many tools and techniques\nexist to clean and otherwise improve data. Yet, systematic research on actually\nassessing data quality in all of its dimensions is largely absent, and with it\nthe ability to gauge the success of any data cleaning effort. It is our vision\nto establish a systematic and comprehensive framework for the (numeric)\nassessment of data quality for a given dataset and its intended use. Such a\nframework must cover the various facets that influence data quality, as well as\nthe many types of data quality dimensions. In particular, we identify five\nfacets that serve as a foundation of data quality assessment. For each facet,\nwe outline the challenges and opportunities that arise when trying to actually\nassign quality scores to data and create a data quality profile for it, along\nwith a wide range of technologies needed for this purpose.",
        "date": "2024-03-01T13:35:15+00:00",
        "label": 1
    },
    "2406.07545": {
        "title": "Open-LLM-Leaderboard: From Multi-choice to Open-style Questions for LLMs Evaluation, Benchmark, and Arena",
        "abstract": "Multiple-choice questions (MCQ) are frequently used to assess large language\nmodels (LLMs). Typically, an LLM is given a question and selects the answer\ndeemed most probable after adjustments for factors like length. Unfortunately,\nLLMs may inherently favor certain answer choice IDs, such as A/B/C/D, due to\ninherent biases of priori unbalanced probabilities, influencing the prediction\nof answers based on these IDs. Previous research has introduced methods to\nreduce this ''selection bias'' by simply permutating options on a few test\nsamples and applying to new ones. Another problem of MCQ is the lottery ticket\nchoice by ''random guessing''. The LLM does not learn particular knowledge, but\nthe option is guessed correctly. This situation is especially serious for those\nsmall-scale LLMs. To address them, a more thorough approach involves shifting\nfrom MCQ to open-style questions, which can fundamentally eliminate selection\nbias and random guessing issues. However, transitioning causes its own set of\nchallenges in (1) identifying suitable open-style questions and (2) validating\nthe correctness of LLM open-style responses against human-annotated\nground-truths. This work aims to tackle these significant difficulties, and\nestablish a new LLM evaluation benchmark through entirely open-style questions.\nConsequently, we introduce the Open-LLM-Leaderboard to track various LLMs'\nperformance and reflect true capability of them, such as GPT-4o/4/3.5, Claude\n3, Gemini, etc. Our code and dataset are available at\nhttps://github.com/VILA-Lab/Open-LLM-Leaderboard.",
        "date": "2024-06-11T17:59:47+00:00",
        "label": 1
    },
    "1908.01091": {
        "title": "Toward Understanding Catastrophic Forgetting in Continual Learning",
        "abstract": "We study the relationship between catastrophic forgetting and properties of\ntask sequences. In particular, given a sequence of tasks, we would like to\nunderstand which properties of this sequence influence the error rates of\ncontinual learning algorithms trained on the sequence. To this end, we propose\na new procedure that makes use of recent developments in task space modeling as\nwell as correlation analysis to specify and analyze the properties we are\ninterested in. As an application, we apply our procedure to study two\nproperties of a task sequence: (1) total complexity and (2) sequential\nheterogeneity. We show that error rates are strongly and positively correlated\nto a task sequence's total complexity for some state-of-the-art algorithms. We\nalso show that, surprisingly, the error rates have no or even negative\ncorrelations in some cases to sequential heterogeneity. Our findings suggest\ndirections for improving continual learning benchmarks and methods.",
        "date": "2019-08-02T23:30:35+00:00",
        "label": 1
    },
    "2405.02449": {
        "title": "Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design",
        "abstract": "Experimental design techniques such as active search and Bayesian\noptimization are widely used in the natural sciences for data collection and\ndiscovery. However, existing techniques tend to favor exploitation over\nexploration of the search space, which causes them to get stuck in local\noptima. This ``collapse\" problem prevents experimental design algorithms from\nyielding diverse high-quality data. In this paper, we extend the Vendi scores\n-- a family of interpretable similarity-based diversity metrics -- to account\nfor quality. We then leverage these quality-weighted Vendi scores to tackle\nexperimental design problems across various applications, including drug\ndiscovery, materials discovery, and reinforcement learning. We found that\nquality-weighted Vendi scores allow us to construct policies for experimental\ndesign that flexibly balance quality and diversity, and ultimately assemble\nrich and diverse sets of high-performing data points. Our algorithms led to a\n70%-170% increase in the number of effective discoveries compared to baselines.",
        "date": "2024-05-03T19:33:44+00:00",
        "label": 1
    }
}