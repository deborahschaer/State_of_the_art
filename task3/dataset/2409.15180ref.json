{
    "2111.14203": {
        "title": "How Deep Are the Fakes? Focusing on Audio Deepfake: A Survey",
        "abstract": "Deepfake is content or material that is synthetically generated or\nmanipulated using artificial intelligence (AI) methods, to be passed off as\nreal and can include audio, video, image, and text synthesis. This survey has\nbeen conducted with a different perspective compared to existing survey papers,\nthat mostly focus on just video and image deepfakes. This survey not only\nevaluates generation and detection methods in the different deepfake\ncategories, but mainly focuses on audio deepfakes that are overlooked in most\nof the existing surveys. This paper critically analyzes and provides a unique\nsource of audio deepfake research, mostly ranging from 2016 to 2020. To the\nbest of our knowledge, this is the first survey focusing on audio deepfakes in\nEnglish. This survey provides readers with a summary of 1) different deepfake\ncategories 2) how they could be created and detected 3) the most recent trends\nin this domain and shortcomings in detection methods 4) audio deepfakes, how\nthey are created and detected in more detail which is the main focus of this\npaper. We found that Generative Adversarial Networks(GAN), Convolutional Neural\nNetworks (CNN), and Deep Neural Networks (DNN) are common ways of creating and\ndetecting deepfakes. In our evaluation of over 140 methods we found that the\nmajority of the focus is on video deepfakes and in particular in the generation\nof video deepfakes. We found that for text deepfakes there are more generation\nmethods but very few robust methods for detection, including fake news\ndetection, which has become a controversial area of research because of the\npotential of heavy overlaps with human generation of fake content. This paper\nis an abbreviated version of the full survey and reveals a clear need to\nresearch audio deepfakes and particularly detection of audio deepfakes.",
        "date": "2021-11-28T18:28:30+00:00",
        "label": 1
    },
    "2308.14970": {
        "title": "Audio Deepfake Detection: A Survey",
        "abstract": "Audio deepfake detection is an emerging active topic. A growing number of\nliteratures have aimed to study deepfake detection algorithms and achieved\neffective performance, the problem of which is far from being solved. Although\nthere are some review literatures, there has been no comprehensive survey that\nprovides researchers with a systematic overview of these developments with a\nunified evaluation. Accordingly, in this survey paper, we first highlight the\nkey differences across various types of deepfake audio, then outline and\nanalyse competitions, datasets, features, classifications, and evaluation of\nstate-of-the-art approaches. For each aspect, the basic techniques, advanced\ndevelopments and major challenges are discussed. In addition, we perform a\nunified comparison of representative features and classifiers on ASVspoof 2021,\nADD 2023 and In-the-Wild datasets for audio deepfake detection, respectively.\nThe survey shows that future research should address the lack of large scale\ndatasets in the wild, poor generalization of existing detection methods to\nunknown fake attacks, as well as interpretability of detection results.",
        "date": "2023-08-29T01:50:01+00:00",
        "label": 1
    },
    "2404.13914": {
        "title": "Audio Anti-Spoofing Detection: A Survey",
        "abstract": "The availability of smart devices leads to an exponential increase in\nmultimedia content. However, the rapid advancements in deep learning have given\nrise to sophisticated algorithms capable of manipulating or creating multimedia\nfake content, known as Deepfake. Audio Deepfakes pose a significant threat by\nproducing highly realistic voices, thus facilitating the spread of\nmisinformation. To address this issue, numerous audio anti-spoofing detection\nchallenges have been organized to foster the development of anti-spoofing\ncountermeasures. This survey paper presents a comprehensive review of every\ncomponent within the detection pipeline, including algorithm architectures,\noptimization techniques, application generalizability, evaluation metrics,\nperformance comparisons, available datasets, and open-source availability. For\neach aspect, we conduct a systematic evaluation of the recent advancements,\nalong with discussions on existing challenges. Additionally, we also explore\nemerging research topics on audio anti-spoofing, including partial spoofing\ndetection, cross-dataset evaluation, and adversarial attack defence, while\nproposing some promising research directions for future work. This survey paper\nnot only identifies the current state-of-the-art to establish strong baselines\nfor future experiments but also guides future researchers on a clear path for\nunderstanding and enhancing the audio anti-spoofing detection mechanisms.",
        "date": "2024-04-22T06:52:12+00:00",
        "label": 1
    },
    "2304.06632": {
        "title": "AI-Generated Content (AIGC): A Survey",
        "abstract": "To address the challenges of digital intelligence in the digital economy,\nartificial intelligence-generated content (AIGC) has emerged. AIGC uses\nartificial intelligence to assist or replace manual content generation by\ngenerating content based on user-inputted keywords or requirements. The\ndevelopment of large model algorithms has significantly strengthened the\ncapabilities of AIGC, which makes AIGC products a promising generative tool and\nadds convenience to our lives. As an upstream technology, AIGC has unlimited\npotential to support different downstream applications. It is important to\nanalyze AIGC's current capabilities and shortcomings to understand how it can\nbe best utilized in future applications. Therefore, this paper provides an\nextensive overview of AIGC, covering its definition, essential conditions,\ncutting-edge capabilities, and advanced features. Moreover, it discusses the\nbenefits of large-scale pre-trained models and the industrial chain of AIGC.\nFurthermore, the article explores the distinctions between auxiliary generation\nand automatic generation within AIGC, providing examples of text generation.\nThe paper also examines the potential integration of AIGC with the Metaverse.\nLastly, the article highlights existing issues and suggests some future\ndirections for application.",
        "date": "2023-03-26T02:22:12+00:00",
        "label": 1
    },
    "2304.10778": {
        "title": "Evaluating the Code Quality of AI-Assisted Code Generation Tools: An Empirical Study on GitHub Copilot, Amazon CodeWhisperer, and ChatGPT",
        "abstract": "Context: AI-assisted code generation tools have become increasingly prevalent\nin software engineering, offering the ability to generate code from natural\nlanguage prompts or partial code inputs. Notable examples of these tools\ninclude GitHub Copilot, Amazon CodeWhisperer, and OpenAI's ChatGPT.\n  Objective: This study aims to compare the performance of these prominent code\ngeneration tools in terms of code quality metrics, such as Code Validity, Code\nCorrectness, Code Security, Code Reliability, and Code Maintainability, to\nidentify their strengths and shortcomings.\n  Method: We assess the code generation capabilities of GitHub Copilot, Amazon\nCodeWhisperer, and ChatGPT using the benchmark HumanEval Dataset. The generated\ncode is then evaluated based on the proposed code quality metrics.\n  Results: Our analysis reveals that the latest versions of ChatGPT, GitHub\nCopilot, and Amazon CodeWhisperer generate correct code 65.2%, 46.3%, and 31.1%\nof the time, respectively. In comparison, the newer versions of GitHub CoPilot\nand Amazon CodeWhisperer showed improvement rates of 18% for GitHub Copilot and\n7% for Amazon CodeWhisperer. The average technical debt, considering code\nsmells, was found to be 8.9 minutes for ChatGPT, 9.1 minutes for GitHub\nCopilot, and 5.6 minutes for Amazon CodeWhisperer.\n  Conclusions: This study highlights the strengths and weaknesses of some of\nthe most popular code generation tools, providing valuable insights for\npractitioners. By comparing these generators, our results may assist\npractitioners in selecting the optimal tool for specific tasks, enhancing their\ndecision-making process.",
        "date": "2023-04-21T07:08:26+00:00",
        "label": 1
    },
    "2106.15561": {
        "title": "A Survey on Neural Speech Synthesis",
        "abstract": "Text to speech (TTS), or speech synthesis, which aims to synthesize\nintelligible and natural speech given text, is a hot research topic in speech,\nlanguage, and machine learning communities and has broad applications in the\nindustry. As the development of deep learning and artificial intelligence,\nneural network-based TTS has significantly improved the quality of synthesized\nspeech in recent years. In this paper, we conduct a comprehensive survey on\nneural TTS, aiming to provide a good understanding of current research and\nfuture trends. We focus on the key components in neural TTS, including text\nanalysis, acoustic models and vocoders, and several advanced topics, including\nfast TTS, low-resource TTS, robust TTS, expressive TTS, and adaptive TTS, etc.\nWe further summarize resources related to TTS (e.g., datasets, opensource\nimplementations) and discuss future research directions. This survey can serve\nboth academic researchers and industry practitioners working on TTS.",
        "date": "2021-06-29T16:50:51+00:00",
        "label": 1
    },
    "2006.07397": {
        "title": "The DeepFake Detection Challenge (DFDC) Dataset",
        "abstract": "Deepfakes are a recent off-the-shelf manipulation technique that allows\nanyone to swap two identities in a single video. In addition to Deepfakes, a\nvariety of GAN-based face swapping methods have also been published with\naccompanying code. To counter this emerging threat, we have constructed an\nextremely large face swap video dataset to enable the training of detection\nmodels, and organized the accompanying DeepFake Detection Challenge (DFDC)\nKaggle competition. Importantly, all recorded subjects agreed to participate in\nand have their likenesses modified during the construction of the face-swapped\ndataset. The DFDC dataset is by far the largest currently and publicly\navailable face swap video dataset, with over 100,000 total clips sourced from\n3,426 paid actors, produced with several Deepfake, GAN-based, and non-learned\nmethods. In addition to describing the methods used to construct the dataset,\nwe provide a detailed analysis of the top submissions from the Kaggle contest.\nWe show although Deepfake detection is extremely difficult and still an\nunsolved problem, a Deepfake detection model trained only on the DFDC can\ngeneralize to real \"in-the-wild\" Deepfake videos, and such a model can be a\nvaluable analysis tool when analyzing potentially Deepfaked videos. Training,\nvalidation and testing corpuses can be downloaded from\nhttps://ai.facebook.com/datasets/dfdc.",
        "date": "2020-06-12T18:15:55+00:00",
        "label": 1
    },
    "2408.16132": {
        "title": "SVDD 2024: The Inaugural Singing Voice Deepfake Detection Challenge",
        "abstract": "With the advancements in singing voice generation and the growing presence of\nAI singers on media platforms, the inaugural Singing Voice Deepfake Detection\n(SVDD) Challenge aims to advance research in identifying AI-generated singing\nvoices from authentic singers. This challenge features two tracks: a controlled\nsetting track (CtrSVDD) and an in-the-wild scenario track (WildSVDD). The\nCtrSVDD track utilizes publicly available singing vocal data to generate\ndeepfakes using state-of-the-art singing voice synthesis and conversion\nsystems. Meanwhile, the WildSVDD track expands upon the existing SingFake\ndataset, which includes data sourced from popular user-generated content\nwebsites. For the CtrSVDD track, we received submissions from 47 teams, with 37\nsurpassing our baselines and the top team achieving a 1.65% equal error rate.\nFor the WildSVDD track, we benchmarked the baselines. This paper reviews these\nresults, discusses key findings, and outlines future directions for SVDD\nresearch.",
        "date": "2024-08-28T20:48:04+00:00",
        "label": 1
    },
    "2311.15308": {
        "title": "AV-Deepfake1M: A Large-Scale LLM-Driven Audio-Visual Deepfake Dataset",
        "abstract": "The detection and localization of highly realistic deepfake audio-visual\ncontent are challenging even for the most advanced state-of-the-art methods.\nWhile most of the research efforts in this domain are focused on detecting\nhigh-quality deepfake images and videos, only a few works address the problem\nof the localization of small segments of audio-visual manipulations embedded in\nreal videos. In this research, we emulate the process of such content\ngeneration and propose the AV-Deepfake1M dataset. The dataset contains\ncontent-driven (i) video manipulations, (ii) audio manipulations, and (iii)\naudio-visual manipulations for more than 2K subjects resulting in a total of\nmore than 1M videos. The paper provides a thorough description of the proposed\ndata generation pipeline accompanied by a rigorous analysis of the quality of\nthe generated data. The comprehensive benchmark of the proposed dataset\nutilizing state-of-the-art deepfake detection and localization methods\nindicates a significant drop in performance compared to previous datasets. The\nproposed dataset will play a vital role in building the next-generation\ndeepfake localization methods. The dataset and associated code are available at\nhttps://github.com/ControlNet/AV-Deepfake1M .",
        "date": "2023-11-26T14:17:51+00:00",
        "label": 1
    },
    "1711.00354": {
        "title": "JSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis",
        "abstract": "Thanks to improvements in machine learning techniques including deep\nlearning, a free large-scale speech corpus that can be shared between academic\ninstitutions and commercial companies has an important role. However, such a\ncorpus for Japanese speech synthesis does not exist. In this paper, we designed\na novel Japanese speech corpus, named the \"JSUT corpus,\" that is aimed at\nachieving end-to-end speech synthesis. The corpus consists of 10 hours of\nreading-style speech data and its transcription and covers all of the main\npronunciations of daily-use Japanese characters. In this paper, we describe how\nwe designed and analyzed the corpus. The corpus is freely available online.",
        "date": "2017-10-28T05:28:01+00:00",
        "label": 1
    },
    "2002.10137": {
        "title": "Audio-driven Talking Face Video Generation with Learning-based Personalized Head Pose",
        "abstract": "Real-world talking faces often accompany with natural head movement. However,\nmost existing talking face video generation methods only consider facial\nanimation with fixed head pose. In this paper, we address this problem by\nproposing a deep neural network model that takes an audio signal A of a source\nperson and a very short video V of a target person as input, and outputs a\nsynthesized high-quality talking face video with personalized head pose (making\nuse of the visual information in V), expression and lip synchronization (by\nconsidering both A and V). The most challenging issue in our work is that\nnatural poses often cause in-plane and out-of-plane head rotations, which makes\nsynthesized talking face video far from realistic. To address this challenge,\nwe reconstruct 3D face animation and re-render it into synthesized frames. To\nfine tune these frames into realistic ones with smooth background transition,\nwe propose a novel memory-augmented GAN module. By first training a general\nmapping based on a publicly available dataset and fine-tuning the mapping using\nthe input short video of target person, we develop an effective strategy that\nonly requires a small number of frames (about 300 frames) to learn personalized\ntalking behavior including head pose. Extensive experiments and two user\nstudies show that our method can generate high-quality (i.e., personalized head\nmovements, expressions and good lip synchronization) talking face videos, which\nare naturally looking with more distinguishing head movement effects than the\nstate-of-the-art methods.",
        "date": "2020-02-24T10:02:10+00:00",
        "label": 1
    },
    "1904.02892": {
        "title": "WaveCycleGAN2: Time-domain Neural Post-filter for Speech Waveform Generation",
        "abstract": "WaveCycleGAN has recently been proposed to bridge the gap between natural and\nsynthesized speech waveforms in statistical parametric speech synthesis and\nprovides fast inference with a moving average model rather than an\nautoregressive model and high-quality speech synthesis with the adversarial\ntraining. However, the human ear can still distinguish the processed speech\nwaveforms from natural ones. One possible cause of this distinguishability is\nthe aliasing observed in the processed speech waveform via down/up-sampling\nmodules. To solve the aliasing and provide higher quality speech synthesis, we\npropose WaveCycleGAN2, which 1) uses generators without down/up-sampling\nmodules and 2) combines discriminators of the waveform domain and acoustic\nparameter domain. The results show that the proposed method 1) alleviates the\naliasing well, 2) is useful for both speech waveforms generated by\nanalysis-and-synthesis and statistical parametric speech synthesis, and 3)\nachieves a mean opinion score comparable to those of natural speech and speech\nsynthesized by WaveNet (open WaveNet) and WaveGlow while processing speech\nsamples at a rate of more than 150 kHz on an NVIDIA Tesla P100.",
        "date": "2019-04-05T06:53:37+00:00",
        "label": 1
    },
    "2006.04558": {
        "title": "FastSpeech 2: Fast and High-Quality End-to-End Text to Speech",
        "abstract": "Non-autoregressive text to speech (TTS) models such as FastSpeech can\nsynthesize speech significantly faster than previous autoregressive models with\ncomparable quality. The training of FastSpeech model relies on an\nautoregressive teacher model for duration prediction (to provide more\ninformation as input) and knowledge distillation (to simplify the data\ndistribution in output), which can ease the one-to-many mapping problem (i.e.,\nmultiple speech variations correspond to the same text) in TTS. However,\nFastSpeech has several disadvantages: 1) the teacher-student distillation\npipeline is complicated and time-consuming, 2) the duration extracted from the\nteacher model is not accurate enough, and the target mel-spectrograms distilled\nfrom teacher model suffer from information loss due to data simplification,\nboth of which limit the voice quality. In this paper, we propose FastSpeech 2,\nwhich addresses the issues in FastSpeech and better solves the one-to-many\nmapping problem in TTS by 1) directly training the model with ground-truth\ntarget instead of the simplified output from teacher, and 2) introducing more\nvariation information of speech (e.g., pitch, energy and more accurate\nduration) as conditional inputs. Specifically, we extract duration, pitch and\nenergy from speech waveform and directly take them as conditional inputs in\ntraining and use predicted values in inference. We further design FastSpeech\n2s, which is the first attempt to directly generate speech waveform from text\nin parallel, enjoying the benefit of fully end-to-end inference. Experimental\nresults show that 1) FastSpeech 2 achieves a 3x training speed-up over\nFastSpeech, and FastSpeech 2s enjoys even faster inference speed; 2) FastSpeech\n2 and 2s outperform FastSpeech in voice quality, and FastSpeech 2 can even\nsurpass autoregressive models. Audio samples are available at\nhttps://speechresearch.github.io/fastspeech2/.",
        "date": "2020-06-08T13:05:40+00:00",
        "label": 1
    },
    "2408.09300": {
        "title": "Malacopula: adversarial automatic speaker verification attacks using a neural-based generalised Hammerstein model",
        "abstract": "We present Malacopula, a neural-based generalised Hammerstein model designed\nto introduce adversarial perturbations to spoofed speech utterances so that\nthey better deceive automatic speaker verification (ASV) systems. Using\nnon-linear processes to modify speech utterances, Malacopula enhances the\neffectiveness of spoofing attacks. The model comprises parallel branches of\npolynomial functions followed by linear time-invariant filters. The adversarial\noptimisation procedure acts to minimise the cosine distance between speaker\nembeddings extracted from spoofed and bona fide utterances. Experiments,\nperformed using three recent ASV systems and the ASVspoof 2019 dataset, show\nthat Malacopula increases vulnerabilities by a substantial margin. However,\nspeech quality is reduced and attacks can be detected effectively under\ncontrolled conditions. The findings emphasise the need to identify new\nvulnerabilities and design defences to protect ASV systems from adversarial\nattacks in the wild.",
        "date": "2024-08-17T21:58:11+00:00",
        "label": 1
    },
    "2407.18517": {
        "title": "SLIM: Style-Linguistics Mismatch Model for Generalized Audio Deepfake Detection",
        "abstract": "Audio deepfake detection (ADD) is crucial to combat the misuse of speech\nsynthesized from generative AI models. Existing ADD models suffer from\ngeneralization issues, with a large performance discrepancy between in-domain\nand out-of-domain data. Moreover, the black-box nature of existing models\nlimits their use in real-world scenarios, where explanations are required for\nmodel decisions. To alleviate these issues, we introduce a new ADD model that\nexplicitly uses the StyleLInguistics Mismatch (SLIM) in fake speech to separate\nthem from real speech. SLIM first employs self-supervised pretraining on only\nreal samples to learn the style-linguistics dependency in the real class. The\nlearned features are then used in complement with standard pretrained acoustic\nfeatures (e.g., Wav2vec) to learn a classifier on the real and fake classes.\nWhen the feature encoders are frozen, SLIM outperforms benchmark methods on\nout-of-domain datasets while achieving competitive results on in-domain data.\nThe features learned by SLIM allow us to quantify the (mis)match between style\nand linguistic content in a sample, hence facilitating an explanation of the\nmodel decision.",
        "date": "2024-07-26T05:23:41+00:00",
        "label": 1
    },
    "2312.05187": {
        "title": "Seamless: Multilingual Expressive and Streaming Speech Translation",
        "abstract": "Large-scale automatic speech translation systems today lack key features that\nhelp machine-mediated communication feel seamless when compared to\nhuman-to-human dialogue. In this work, we introduce a family of models that\nenable end-to-end expressive and multilingual translations in a streaming\nfashion. First, we contribute an improved version of the massively multilingual\nand multimodal SeamlessM4T model-SeamlessM4T v2. This newer model,\nincorporating an updated UnitY2 framework, was trained on more low-resource\nlanguage data. SeamlessM4T v2 provides the foundation on which our next two\nmodels are initiated. SeamlessExpressive enables translation that preserves\nvocal styles and prosody. Compared to previous efforts in expressive speech\nresearch, our work addresses certain underexplored aspects of prosody, such as\nspeech rate and pauses, while also preserving the style of one's voice. As for\nSeamlessStreaming, our model leverages the Efficient Monotonic Multihead\nAttention mechanism to generate low-latency target translations without waiting\nfor complete source utterances. As the first of its kind, SeamlessStreaming\nenables simultaneous speech-to-speech/text translation for multiple source and\ntarget languages. To ensure that our models can be used safely and responsibly,\nwe implemented the first known red-teaming effort for multimodal machine\ntranslation, a system for the detection and mitigation of added toxicity, a\nsystematic evaluation of gender bias, and an inaudible localized watermarking\nmechanism designed to dampen the impact of deepfakes. Consequently, we bring\nmajor components from SeamlessExpressive and SeamlessStreaming together to form\nSeamless, the first publicly available system that unlocks expressive\ncross-lingual communication in real-time. The contributions to this work are\npublicly released and accessible at\nhttps://github.com/facebookresearch/seamless_communication",
        "date": "2023-12-08T17:18:42+00:00",
        "label": 1
    },
    "2106.04624": {
        "title": "SpeechBrain: A General-Purpose Speech Toolkit",
        "abstract": "SpeechBrain is an open-source and all-in-one speech toolkit. It is designed\nto facilitate the research and development of neural speech processing\ntechnologies by being simple, flexible, user-friendly, and well-documented.\nThis paper describes the core architecture designed to support several tasks of\ncommon interest, allowing users to naturally conceive, compare and share novel\nspeech processing pipelines. SpeechBrain achieves competitive or\nstate-of-the-art performance in a wide range of speech benchmarks. It also\nprovides training recipes, pretrained models, and inference scripts for popular\nspeech datasets, as well as tutorials which allow anyone with basic Python\nproficiency to familiarize themselves with speech technologies.",
        "date": "2021-06-08T18:22:56+00:00",
        "label": 1
    },
    "1706.03825": {
        "title": "SmoothGrad: removing noise by adding noise",
        "abstract": "Explaining the output of a deep network remains a challenge. In the case of\nan image classifier, one type of explanation is to identify pixels that\nstrongly influence the final decision. A starting point for this strategy is\nthe gradient of the class score function with respect to the input image. This\ngradient can be interpreted as a sensitivity map, and there are several\ntechniques that elaborate on this basic idea. This paper makes two\ncontributions: it introduces SmoothGrad, a simple method that can help visually\nsharpen gradient-based sensitivity maps, and it discusses lessons in the\nvisualization of these maps. We publish the code for our experiments and a\nwebsite with our results.",
        "date": "2017-06-12T19:53:30+00:00",
        "label": 1
    },
    "2308.12734": {
        "title": "Real-time Detection of AI-Generated Speech for DeepFake Voice Conversion",
        "abstract": "There are growing implications surrounding generative AI in the speech domain\nthat enable voice cloning and real-time voice conversion from one individual to\nanother. This technology poses a significant ethical threat and could lead to\nbreaches of privacy and misrepresentation, thus there is an urgent need for\nreal-time detection of AI-generated speech for DeepFake Voice Conversion. To\naddress the above emerging issues, the DEEP-VOICE dataset is generated in this\nstudy, comprised of real human speech from eight well-known figures and their\nspeech converted to one another using Retrieval-based Voice Conversion.\nPresenting as a binary classification problem of whether the speech is real or\nAI-generated, statistical analysis of temporal audio features through t-testing\nreveals that there are significantly different distributions. Hyperparameter\noptimisation is implemented for machine learning models to identify the source\nof speech. Following the training of 208 individual machine learning models\nover 10-fold cross validation, it is found that the Extreme Gradient Boosting\nmodel can achieve an average classification accuracy of 99.3% and can classify\nspeech in real-time, at around 0.004 milliseconds given one second of speech.\nAll data generated for this study is released publicly for future research on\nAI speech detection.",
        "date": "2023-08-24T12:26:15+00:00",
        "label": 1
    },
    "2403.11778": {
        "title": "Towards the Development of a Real-Time Deepfake Audio Detection System in Communication Platforms",
        "abstract": "Deepfake audio poses a rising threat in communication platforms,\nnecessitating real-time detection for audio stream integrity. Unlike\ntraditional non-real-time approaches, this study assesses the viability of\nemploying static deepfake audio detection models in real-time communication\nplatforms. An executable software is developed for cross-platform\ncompatibility, enabling real-time execution. Two deepfake audio detection\nmodels based on Resnet and LCNN architectures are implemented using the\nASVspoof 2019 dataset, achieving benchmark performances compared to ASVspoof\n2019 challenge baselines. The study proposes strategies and frameworks for\nenhancing these models, paving the way for real-time deepfake audio detection\nin communication platforms. This work contributes to the advancement of audio\nstream security, ensuring robust detection capabilities in dynamic, real-time\ncommunication scenarios.",
        "date": "2024-03-18T13:35:10+00:00",
        "label": 1
    },
    "2404.13146": {
        "title": "DeepFake-O-Meter v2.0: An Open Platform for DeepFake Detection",
        "abstract": "Deepfakes, as AI-generated media, have increasingly threatened media\nintegrity and personal privacy with realistic yet fake digital content. In this\nwork, we introduce an open-source and user-friendly online platform,\nDeepFake-O-Meter v2.0, that integrates state-of-the-art methods for detecting\nDeepfake images, videos, and audio. Built upon DeepFake-O-Meter v1.0, we have\nmade significant upgrades and improvements in platform architecture design,\nincluding user interaction, detector integration, job balancing, and security\nmanagement. The platform aims to offer everyday users a convenient service for\nanalyzing DeepFake media using multiple state-of-the-art detection algorithms.\nIt ensures secure and private delivery of the analysis results. Furthermore, it\nserves as an evaluation and benchmarking platform for researchers in digital\nmedia forensics to compare the performance of multiple algorithms on the same\ninput. We have also conducted detailed usage analysis based on the collected\ndata to gain deeper insights into our platform's statistics. This involves\nanalyzing two-month trends in user activity and evaluating the processing\nefficiency of each detector.",
        "date": "2024-04-19T19:24:20+00:00",
        "label": 1
    }
}