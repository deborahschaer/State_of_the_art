{
    "id": "2408.02085",
    "title": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data Assessment and Selection for Instruction Tuning of Language Models",
    "abstract": "Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.",
    "date": "2024-08-04T16:50:07+00:00",
    "fulltext": "Unleashing the Power of Data Tsunami: A Comprehensive Survey on\nData Assessment and Selection for Instruction Tuning of Language Models\nYulei Qin1, Yuncheng Yang1,2, Pengcheng Guo1, Gang Li1, Hang Shao1,\nYuchen Shi1, Zihan Xu1, Yun Gu2, Ke Li1, Xing Sun1,\n1Tencent YouTu Lab, 2Shanghai Jiao Tong University\nCorrespondence: yuleiqin@tencent.com\nAbstract\nInstruction tuning plays a critical role in align-\ning large language models (LLMs) with human\npreference. Despite the vast amount of open\ninstruction datasets, naively training a LLM\non all existing instructions may not be opti-\nmal and practical. To pinpoint the most benefi-\ncial datapoints, data assessment and selection\nmethods have been proposed in the fields of\nnatural language processing (NLP) and deep\nlearning. However, under the context of instruc-\ntion tuning, there still exists a gap in knowl-\nedge on what kind of data evaluation metrics\ncan be employed and how they can be inte-\ngrated into the selection mechanism. To bridge\nthis gap, we present a comprehensive review\non existing literature of data assessment and\nselection especially for instruction tuning of\nLLMs. We systematically categorize all ap-\nplicable methods into quality-based, diversity-\nbased, and importance-based ones where a uni-\nfied, fine-grained taxonomy is structured. For\neach category, representative methods are elab-\norated to describe the landscape of relevant re-\nsearch. In addition, comparison between lat-\nest methods is conducted on their officially re-\nported results to provide in-depth discussions\non their limitations. Finally, we summarize the\nopen challenges and propose the promosing av-\nenues for future studies. All related contents are\navailable at https://github.com/yuleiqin/\nfantastic-data-engineering.\n1\nIntroduction\nOne of the ultimate goal of developing large\nlnguage models (LLMs) is to unlock their poten-\ntials of generalization to unseen natural language\nprocessing (NLP) tasks. Towards this goal, a se-\nries of LLMs such as GPTs (Brown et al., 2020;\nAchiam et al., 2023), LLaMAs (Touvron et al.,\n2023a,b; AI@Meta, 2024), and Mistrals (Jiang\net al., 2023a, 2024a) have delivered high-level text\nunderstanding and generation capabilities via uti-\nlizing vast amount of high-quality web and human-\nannotated datasets for pre-training and preference\nalignment (Liu et al., 2023a, 2024c; Sun et al.,\n2024b; Edunov et al., 2019; Dong et al., 2019). Dur-\ning preference alignment, instruction tuning plays\nan important role in refining pre-trained LLMs to\nprovide accurate, pertinent, and harmless responses\non a collection of downstream tasks (Wei et al.,\n2021; Sanh et al., 2021; Zhang et al., 2023c; Peng\net al., 2023; Longpre et al., 2023; Shu et al., 2023;\nJang et al., 2023; Ghosh et al., 2024; Kung and\nPeng, 2023). For efficient and effective instruction\ntuning, existing studies (Ouyang et al., 2022; Taori\net al., 2023; Zhou et al., 2024a; Xia et al., 2024a)\nhave noticed that improving quality of instruction\ntuning data (e.g., formulation of well-defined and\ncomplete contexts), rather than simply piling up\ninstructions without analysis (e.g., exhaustive col-\nlection of open datasets), is of prioritized concerns.\nIn this work, we aim to unify a wide array of\ndata assessment and selection methods under the\ncontext of instruction tuning of LLMs. As revealed\nfrom the probabilistic view (John and Draper, 1975;\nMurphy, 2012; Albalak et al., 2024), the statistical\npatterns inherent in datasets determines the model-\ning performance. The overall evaluation of instruc-\ntion datapoints not only deciphers the distribution\nin various aspects (e.g., composition, task, and do-\nmain) and also help cherry-pick the most beneficial\nsubsets for higher performance with less training\ncost. Through this survey, we demonstrate that:\n1) existing resourceful data assessment methods\ncan be categorized into three main perspectives:\nquality, diversity, and importance (see Fig. 1). 2) a\nsystematic view of selection methods can be uni-\nfied even they more or less exhibit coupling with\nthe assessment techniques (see Fig. 2). It is noted\nthat quality, diversity, and importance might be\nused interchangeably without strict discrimination\nin previous studies. But here we provide a ratio-\nnalized organization taxonomy for structured elab-\noration. Despite the goal of being comprehensive,\narXiv:2408.02085v3  [cs.CV]  7 Aug 2024\nFigure 1: Categorization of data assessment and selection methods for efficient LLM instruction tuning.\nthe present survey only provides details of certain\ntypical, representative methods to avoid being te-\ndiously long. We hope the in-depth explanations\nand discussions on the selected methods provide\ninsights into developing robust data assessment and\nselection pipelines for further studies.\n1.1\nRelated Surveys\n(Liu et al., 2024d) studies the mainstream datasets\nfor building LLMs, including the pre-training\ncorpora, instruction tuning datasets, preference\ndatasets, evaluation benchmarks, and traditional\nNLP datasets. (Albalak et al., 2024) presents a sys-\ntematic overview of constructing the data pipeline\nfor language models. Any selection method, ei-\nther via distribution matching or diversification,\ncan be composed of: 1) utility function; 2) selec-\ntion mechanism. During different stages of the\npipeline (e.g., language filtering, data quality, do-\nmain knowledge, deduplication, toxic and explicit\ncontent removal, and data mixing), the selection\nmethod should be adjusted according to different\nselection objectives. (Wang et al., 2024a) focuses\non the data preparation for instruction tuning. Exist-\ning methods on building instruction tuning datasets\ninclude: 1) reformulating the discriminative NLP\ndatasets into generative ones; 2) self-instruct with\nseed prompts; 3) prompt mapping and evol-instruct.\nPopular methods on dataset selection can be simply\nclassified as: 1) system of indicators; 2) trainable\nLLMs; 3) powerful LLMs; and 4) small models.\n(Guo et al., 2022) starts from the general core-\nset selection method in the field of deep learn-\ning and categorize all selection manners into: 1)\ngeometry-based methods (e.g., herding, kcenter-\ngreedy); 2) uncertainty-based methods (e.g., least\nconfidence/entropy/margin); 3) error/loss-based\nmethods (forgetting; GraND/EL2N; importance re-\nsampling); 4) decision boundary-based (adversarial\ndeepfool; contrastive active learning); 5) gradient\nmatching-based (gradient approximation towards\nfull set); 6) bi-level optimization-based (inner loop\nof model optimization and outer loop of datapoint\nselection); 7) sub-modularity-based (e.g., graph\ncut; facility location); 8) proxy-based (preference\nof a small model on data selection). (Zhou et al.,\n2024b) investigates the potential metrics and as-\npects for data quality measurement and provides\na list of available tools for data evaluation. Apart\nfrom data assessment and selection methods that\nspecifically designed for NLP or LLM applica-\ntions (Moore and Lewis, 2010; Chen et al., 2024a;\nDodge et al., 2020; Kandpal et al., 2022; Li et al.,\n2022; Feng et al., 2021; Lee et al., 2021; Malho-\ntra and Bakal, 2015; Liu et al., 2024e), there ex-\nist many survey studies that tackle general quality\nmeasurement in machine learning (Gupta et al.,\n2021; Zha et al., 2023; Ehrlinger and W\u00f6\u00df, 2022;\nMohammed et al., 2024; Li et al., 2024c; Lu et al.,\n2023b; Dix et al., 2023; Priestley et al., 2023;\nByabazaire et al., 2020; Roh et al., 2019; Sidi et al.,\n2012; Batini et al., 2009) for constructing safe, un-\nbiased, and accurate datasets.\n1.2\nSurvey Scope\nAlthough \u201cdata evaluation\" has been so frequently\nmentioned that it appears as a clich\u00e9 problem in\ndeveloping machine learning algorithms, the opti-\nmal solution to establishing an overall data assess-\nment and selection pipeline still remains an open\nquestion. Especially under the context of instruc-\ntion tuning of LLMs, existing studies propose vari-\nous measurement and cleaning strategies to select\nthe \u201chigh-quality\" instructions from all datapoints.\nFigure 2: A high-level overview of comprehensive data assessment and selection. Most analysis aspects that evaluate\neach datapoint and the overall dataset are categorized into three groups marked in blue italic.\nHowever, very few studies notice that there ex-\nists no unified dimensions or aspects in measuring\ndata \u201cquality\" where previous works tend to put em-\nphasis on the domain-specific and task-dependent\ncharacteristics. In addition, the inherent, system-\natic coupling between data assessment and subset\nselection methods is not well demonstrated.\nUnder such circumstance, the present study\nstrives to provide a comprehensive review on evalu-\nating and decomposing massive instruction tuning\ndatasets. We categorize the main aspects of data\nassessment in terms of quality, diversity, and im-\nportance. In each aspect, we provide a detailed\nsurvey on both traditional (e.g., hand-crafted indi-\ncators) and machine learning (e.g., model-based\nindicators) methods. Besides, the coreset sampling\nmethods that fuses evaluation and selection are in-\ntroduced separately in diversity and importance\noriented subset construction. In consideration of\nthe properties of instruction tuning, we focus on the\ntext modality and start from classical text analysis\nmetrics. Metrics that are either specific to instruc-\ntion tuning or compatible with pre-training and\npreference alignment datasets are included since\nthey all share general rules in data assessment.\nThe survey is organized as follows. First, we\npresent the preliminaries for assessment and se-\nlection of instruction tuning datasets (Sec. \u00a72).\nNext, we present the surveying methods of data\nassessment and selection methods in terms of qual-\nity (Sec. \u00a73), diversity (Sec. \u00a74), and importance\n(Sec. \u00a75). Then, discussions on the existing meth-\nods are provided in (Sec. \u00a76), followed by the\npromising directions for future research (Sec. \u00a77).\nThe final conclusion is given in (Sec. \u00a78).\n2\nPreliminaries\nIn this section, we briefly introduce the instruction\ntuning of LLMs and the problem statement for\ndataset assessment and selection.\nInstruction Dataset Preparation\nIn instruction\ntuning, each text sample Ii is usually composed of\nthree parts: 1) instruction (either with or without\nsystem prompt), 2) input, and 3) response. For\nan off-the-shelf pre-trained LLM parameterized as\n\u03b8, a pre-determined instruction template is used\nto wrap Ii into the prompt pi with special tokens\nlike \u201c<|im_start|>\" and \u201c<|im_end|>\" for sep-\naration of roles (e.g., system, user, assistant,\nfunction, and observation) and their contents.\nThen, a LLM-associated tokenizer performs to-\nkenization on the instruction prompt pi for a se-\nquence of xi = {xi(1), xi(2), ..., xi(n)}, where xi(j)\ndenotes the j-th token of xi and n is the total num-\nber of tokens. Out of simplicity, the token sequence\nxi can be simply split into two parts by the index t\nwhere the content from the role assistant starts:\n1) the instruction (input) part (xi(<t)), and 2) the\nground-truth response part (xi(\u2265t)).\nInstruction Supervision\nGiven the tokenized in-\nstruction tuning dataset S = {xi}N\ni=1, the super-\nvised tuning is performed via cross-entropy loss:\nL =\nX\nxi\u2208S\nLi,\nLi = \u2212\n|xi|\nX\nj=t\nlogP(xi(j)|xi(<j); \u03b8).\n(1)\nFor each xi, the model iteratively predicts the next\ntoken given xi(j) all previous tokens including the\ninstruction context and the response completions\nup to the current token xi(< j).\nData Assessment and Selection\nWe aim at find-\ning the most informative subset Sb \u2282S from the\nentire set S under the given budget |Sb| \u2264b. Math-\nematically, the selection of Sb requires the quanti-\ntative evaluation q(\u00b7) on each datapoint xi and an\nelaborated sampling mechanism \u03c0:\nS\u2217\nb = \u03c0(arg max\nxi\u2208S q(xi), b),\n(2)\nwhere \u03c0(\u00b7, b) denotes the sampling process with a\nmaximum b datapoints. With respect to the detailed\nimplementation of \u03c0, either an iterative, greedy\nalgorithm or a batch-wise heuristic rule can be\nadopted for compatibility with q(\u00b7). The expected\nbenefits of such selection include: 1) the reduction\nof noise by ignoring those mislabeled, mismatched\ninstruction-response pairs, 2) the re-balance of data\ndistributions by down-sampling those easy, com-\nmon, and similar examples while up-sampling hard,\nrare ones, and 3) the expedition of training in return\nfor efficient iterations of LLMs.\n3\nQuality-based Selection\nIn this section, we present methods on quality as-\nsessment and selection. Without lose of general-\nity, the term \u201cquality\" here primarily refers to the\nintegrity, accuracy, and rationality of instruction-\nresponse datapoints.\nFor integrity, it measures\nwhether the instruction and response are under-\nstandable and complete in both format and content.\nFor accuracy, it estimates whether the \u201cground-\ntruth\" response truly corresponds to the instruction.\nFor rationality, we focus on the consistency and co-\nherency of the instruction context. Although these\nthree dimensions all contribute to the overall qual-\nity, in general, existing methods often formulate a\nunified scoring mechanism to implicitly consider\nthem partially or comprehensively.\n3.1\nHand-crafted Indicators\nOverview\nTraditional methods develop hand-\ncrafted indicators to evaluate the data quality in\nterms of linguistic analysis such as vocabulary, syn-\ntax, and inter-sample semantic similarity. Each\nindicator is manually, empirically designed with\nprior knowledge on the language, domain, and task\nof the corpus under investigation. The calculation\nof each indicator is explicitly defined and does not\nrequire training and inference of proxy models or\nlanguage models. Although the indicators are hand-\ncrafted, deep learning models such as sentence en-\ncoders might be leveraged to extract embedding\nrepresentations for each instruction text. For the\ndatapoint xi, its indicator INDi can be typically\ndefined as:\nINDi = f(IND1(xi), IND2(xi),\nIND3(xi), ...INDM(xi)),\n(3)\nwhere M denotes the total number of indicators\nand f is the aggregation function which depends\non both the instruction task and dataset. One can\nsimply use linear combination with pre-defined\nor dynamically adjusted weights while meticulous\ntuning might be needed for the ultimate f. Given\nthe indicators INDi for each xi, two intuitive se-\nlection methods can be adopted: 1) to filter out\ndatapoints whose indicator scores are below a pre-\ndefined threshold; 2) to keep only the samples\nwhose indicator scores rank within a certain range\nof percentiles. Mathematically, these two selection\nmechanism can be respectively represented as:\nS\u03c0 = {xi|\u03c4min < f(xi) < \u03c4max, 1 \u2264i \u2264N},\n(4)\nS\u03c0 = {xi|Pmin \u2264\u02c6\nFf(f(xi)) \u2264Pmax, 1 \u2264i \u2264N},\n(5)\nwhere \u03c4min and \u03c4max respectively denote the left\nand right threshold boundaries. The estimated \u02c6\nFf\nis the empirical cumulative distribution function of\nall indicators f. Pmin and Pmax respectively refer\nto the minimum and maximum percentile for en-\nclosing the selection range. In practice, both the\nthreshold and percentiles are hyper-parameters that\nrequire task-specific fine-tuning.\nTechnical Details\n(Mishra et al., 2020b) and\n(Mishra et al., 2020a) introduce a data quality met-\nric, namely the DQI, to quantify the differences\nbetween successive benchmarks by giving high\nscores to generalizable samples and low scores to\nbiased samples. Such a metric implies whether\na well-trained model truly learns the underlying\ntask rather than overfitting the spurious bias of\nspecific benchmarks. Specifically, DQI has seven\ncomponents including vocabulary, inter-sample N-\ngram frequency and relation, inter-sample semantic\ntextual similarities (STS), intra-sample word sim-\nilarity, intra-sample STS, N-Gram frequency per\nlabel, and inter-split STS. Based on the proposed\nDQI, (Mishra and Sachdeva, 2020) proposes to\nprune existing huge NLP datasets and demonstrates\nthat the model trained on only 2% of the SNLI\ndataset achieves near-equal performance with that\non the entire set. It first performs AFLite (Le Bras\net al., 2020), which is detailed in , to keep sam-\nples with predictability scores over a pre-defined\nthreshold and then delete bottom k samples with\nlowest DQI scores. (Dang and Verma, 2024) fur-\nther split DQI components into linguistic indicators\nand semantic indicators, and validate their respec-\ntive roles in detecting outliers, noises, and duplica-\ntions. Apart from training-oriented data selection,\nquality indicators can also be employed to identify\nthe most discriminative samples in the evaluation\nset to expedite evaluation of LLMs. (Saranathan\net al.) investigates key indicators such as spelling\nerrors (Yannakoudakis and Fawthrop, 1983), av-\nerage word length, excessive word repetition, and\nthe compound probability distribution. These in-\ndicators stem from the traditional studies on text\nreadability (i.e., readability formulas and sophis-\nticated features) (Klare et al., 1963, 1984; Dubay,\n2004; Kintsch and Vipond, 2014; Kemper, 1983).\nRecent studies on readability leverage NLP sys-\ntems to extract more advanced and informative fea-\ntures for readability measures (Si and Callan, 2001;\nCollins-Thompson and Callan, 2005; Schwarm and\nOstendorf, 2005; Feng et al., 2010). (Fran\u00e7ois,\n2010, 2011; Fran\u00e7ois and Fairon, 2012) systemati-\ncally analyze the lexical features, syntactic features,\nsemantic features, and language-specific features\nwith up to 46 indicators. (Fran\u00e7ois and Miltsakaki,\n2012) validates these manually-designed (classical)\nand NLP-enabled (non-classical) readability formu-\nlas, implying that high-quality text corpus can be\npinpointed by such carefully designed metrics. (Fe-\nlice and Specia, 2012) finds that the hand-crafted\nlinguistic features should be combined with other\nshallow features for better quality estimation.\nRemark\nThe hand-crafted indicators often stem\nfrom studies on linguistic analysis and readabil-\nity measurement. Although these indicators help\nfilter out instruction samples that are unreadable,\nnonsensical, and incoherent, they cannot detect mis-\nmatched instruction-response pairs and therefore\nfail to guarantee the instruction-following capabil-\nity of LLMs trained on highly-scored datasets.\n3.2\nModel-based Indicators\nOverview\nThe model-based indicators, on the\nother hand, leverage trainable models to predict\nthe indicators for each instruction datapoint. The\ntrainable models used for data quality measure-\nment can either share the same or similar architec-\nture with the language model under development,\nor possesses completely different implementation\nchoices. Accordingly, these indicators can be sim-\nply defined as:\nINDi = f(IND1\n\u03b8(xi), IND2\n\u03b8(xi),\nIND3\n\u03b8(xi), ...INDM\n\u03b8 (xi)),\n(6)\nwhere the learnable parameters \u03b8 highlight the dif-\nference between model-based and hand-crafted in-\ndicators. Based on the computed indicators, similar\nselection mechanisms (Eqs. 45) can be adopted to\nselect favorable datapoints.\nTechnical Details\nOne of the most intuitive\nmodel-based indicators is perplexity (Shannon,\n2001; Jelinek et al., 1977; Jelinek, 1980). It is\nfrequently mentioned as the evaluation metric for\npre-trained language models (Penedo et al., 2023;\nRadford et al., 2018, 2019; Brown et al., 2020;\nAchiam et al., 2023) but can also be employed as\na data quality indicator. (Ankner et al., 2024) pro-\nposes to use a small GPT-style reference model\nsuch as MPT 125M (Team, 2023) to prune dataset\nvia perplexity-based sampling for training a 3B\nmodel. Specifically, for any datapoint xi, the per-\nplexity is defined as the exponential of negative\nlikelihood with base of 2:\nNLLi =\n1\n|xi|\n|xi|\nX\nj=1\n\u2212logP(xi(j)|xi(<j); \u03b8)\nPPLXi = 2NLLi\n(7)\nBased on the perplexity inferred from a small\nmodel, samples at the high and medium per-\ncentiles are chosen by Eq. 5 for downstream fine-\ntuning.\n(Deng et al., 2021) develops a unified\nevaluator framework to score the generated out-\nputs for natural language generation tasks.\nA\nRoBERTa-based (Liu et al., 2019) discriminator\nlearns to score responses in terms of consistency,\nrelevance, preservation, engagingness, and ground-\nedness. One could simply adopt such a discrim-\ninator for evaluation of the instruction-response\npairs. (Zhong et al., 2022) further proposes a multi-\ndimensional scoring evaluator. For each evaluation\ndimension, the original ground-truth instruction-\nresponse pairs are converted into positive samples\nin the form of boolean question-answer problems.\nThe negative samples are respectively constructed\nvia rule-based transformation. The evaluator itself\nis implemented as T5 model (Raffel et al., 2020)\nand trained on these positive and negative samples\nfor scoring in the range from 0 to 1. (Jiang et al.,\n2024c) prunes the UltraChat (Ding et al., 2023)\ndataset by scoring each datapoint by learning com-\nplexity of a pre-trained Qwen-1.8B model (Bai\net al., 2023). Specifically, the learning complexity\nis calculated as the averaged prediction confidence\nof different subnets:\n\u02dc\nS(xi) = 1\nI\nI\nX\nj=1\nPPLX\u22121\ni;\u0398j,\n(8)\nwhere I is the number of subnets. Each subnet\n\u0398j is obtained by adjusting the dropout rate from\n10% to 90% on the original \u0398 of any pre-trained\nlanguage model. Instruction datapoints with small\n\u02dc\nS(xi) are easy ones and should be kept first during\npruning. Both (Bukharin and Zhao, 2023) and (Du\net al., 2023) employ reward models to assess the\nquality of each instruction pairs. They respectively\nutilize the raft model (Dong et al., 2023) and the\ndeberta-v3-large-v2 1 for reward scoring:\nRi = r\u03b8(xi(<t), xi(\u2265t)),\n(9)\nwhere r\u03b8 denotes the reward model. t is the in-\ndex where xi(<t) and xi(\u2265t) respectively denote\nthe instruction Q and response A. (Marion et al.,\n2023) investigates three classic metrics in clean\nset selection (Guo et al., 2022; Song et al., 2022;\nNatarajan et al., 2013; Qin et al., 2024): perplexity\n(Eq. 7), error l2-Norm (EL2N) (Paul et al., 2021),\nand memorization ranking (Biderman et al., 2024).\nSpecifically, EL2N is defined as:\nEL2Ni =\n1\n|xi|\n|xi|\nX\nj=1\n\u2225P(xi(<j); \u03b8)\u2212yi(j)\u22252, (10)\n1https://huggingface.co/OpenAssistant/reward-model-\ndeberta-v3-large-v2\nwhere yi(j) \u2208RNvocab denotes the one-hot vector\nof the vocabulary size Nvocab, where its element\nindexed at xi(j) equals one. The memorization\nranking is represented as:\nMEMi =\n1\nNwin\nNwin\nX\nj=1\n1(\u02c6\nxi(Moffset+j) = xi(Moffset+j)),\n(11)\nwhere Nwin denotes the length of a consecutive\nsequence and Moffset is an offset of the starting\nindex. The \u02c6\nxi(Moffset+j) refers to the generated to-\nken given input xi(<Moffset+j), and xi(Moffset+j) is its\nground-truth. (Cao et al., 2023) combines both\nhand-crafted indicators (e.g., input length, output\nlength, MTLD (McCarthy and Jarvis, 2010), and\nkNN-i (Dong et al., 2011)) and model-based in-\ndicators (e.g., reward score, perplexity, and Uni-\nEval metrics (Zhong et al., 2022)) for fitting the\nloss of a LLM on the evaluation set. The linear\nregression model is optimized via least squares\nmethod (Bjork, 1988) and the optimal selection of\ninstruction data is achieved via BlendSearch (Wang\net al., 2021a,b) for minimizing the estimated eval-\nuation loss. (Li et al., 2023a) is one of the most\npioneering works that leverages the target language\nmodel itself to perform self-guided data selection.\nThe language model is first \u201cwarmed-up\" with very\nfew samples randomly chosen from the pool to\nlearn from brief experience. Then, such an expe-\nrienced model evaluates each instruction-response\npair via the instruction-following difficulty (IFD)\nscore. The IFD score measures how much guid-\nance or assistance the instruction provides to the\ngeneration of ground-truth response, by compar-\ning the loss of causal language modeling on the\nresponse with and without instruction:\nIFDi = NLLA|Q\ni\nNLLA\ni\n,\nNLLA|Q\ni\n=\n1\n|xi(\u2265t)|\n|xi|\nX\nj=t\n\u2212logP(xi(j)|xi(<j); \u03b8),\nNLLA\ni =\n1\n|xi(\u2265t)|\n|xi|\nX\nj=t\n\u2212logP(xi(j)|xi(t\u2264,<j); \u03b8),\n(12)\nwhere the index t splits apart the instruction Q and\nthe response A. Samples whose IFD scores over\n\u03c4max = 1 are invalid datapoints with misaligned,\nmismatched instruction-response pairs. The empir-\nical setting of \u03c4min affects the trade-off between\nquality and diversity of the selected datapoints.\n(Zhao and Fang, 2024) comprehensively employs\nhand-crafted indicators for low-level quality filter-\ning, and uses perplexity and IFD score for high-\nlevel filtering. A voting mechanism is addition-\nally introduced with IFD scores from one pre-\ntrained base model and one fine-tuned experience\nmodel. (Li et al., 2024b) corroborates that both the\nperplexity and IFD scores inferred from a rather\nsmall GPT2-125M (Radford et al., 2019) are in-\ndicative in selecting high-quality datapoints for\ntraining LLaMA2-7B and LLaMA2-13B (Touvron\net al., 2023b), which greatly improves selection\nefficiency.\nAnother popular model-based quality filtering\nmethod is AF-Lite (Le Bras et al., 2020), which\nhas been applied and validated in recent NLP stud-\nies (Mishra and Sachdeva, 2020; Sakaguchi et al.,\n2021). It randomly partition all available datapoints\ninto training set and validation set. Then, a model\n(e.g., linear classifier or language model) is trained\non the training set and inferred on the validation\nset. Such process iterates m times for calculation\nof the predictability score, which is defined as the\nratio of the number of correctly predicted response\nover the number of total predictions:\nPREDi = |{\u02c6\nxi \u2208Ei, s.t. \u02c6\nxi = xi}|\nEi\n,\nEi = {\u02c6\nx\u03b81\ni , \u02c6\nx\u03b81\ni , ..., \u02c6\nx\u03b8j\ni , ..., \u02c6\nx\u03b8m\ni },\n(13)\nwhere \u02c6\nx\u03b8j\ni denotes the generated response from the\nmodel parameterized as \u03b8j. It is noted that xi is not\ninvolved for optimizing \u03b8j, and therefore a higher\nPREDi suggests better quality.\n(Bhatt et al., 2024) presents uncertainty-based\nquality indicators such as mean entropy (Settles,\n2011; Kremer et al., 2014), least confidence (Set-\ntles, 1995, 2011), mean margin (Tong and Koller,\n2001; Balcan et al., 2006; Settles, 2011), and min\nmargin (Nguyen et al., 2022). Mathematically, such\nuncertainty indicators are defined as:\nU entropy\ni\n=\n1\n|xi|\n|xi|\nX\nj=1\nP(xi(j)|xi(<j); \u03b8)\u00b7\nlogP(xi(j)|xi(<j); \u03b8).\n(14)\nU confidence\ni\n= \u2212\n|xi|\nY\nj=1\nP(xi(j)|xi(<j); \u03b8).\n(15)\nU margin\ni\n= \u22121\n|xi|\n|xi|\nX\nj=1\n(\u03b21(P(xi(<j); \u03b8))\u2212\n\u03b22(P(xi(<j); \u03b8))),\n(16)\nU min-margin\ni\n= \u2212\nmin\nj\u2208{1,2,...,|xi|}(\u03b21(P(xi(<j); \u03b8))\u2212\n\u03b22(P(xi(<j); \u03b8))),\n(17)\nwhere \u03b21 and \u03b22 denote the largest and second\nlargest element of the probability P(xi(<j); \u03b8) \u2208\nRNvocab for the newly generated j-th token. How-\never, (Wu et al., 2023) finds that such uncertainty-\nbased data sampling methods perform worse than\nrandom sampling on Databricks-Dolly (Conover\net al., 2023), SelfInstruct-Davinci (Taori et al.,\n2023), and SelfInstruct-GPT4 (Peng et al., 2023).\nRemark\nHybrid techniques that simultaneously\ncombines perplexity, uncertainty, reward scores,\nand other training-aware metrics are promising in\nselecting unbiased high quality samples. In con-\nsideration of the training and inference cost, it is\nfeasible to employ small proxy models as alterna-\ntives for computing model-based indicators.\n3.3\nGPT Score\nOverview\nThe invoking of OpenAI APIs (Tin-\ngiris\nand\nKinsella,\n2021;\nLappalainen\nand\nNarayanan, 2023; Sun et al., 2023; Kublik and Sa-\nboo, 2023) for ChatGPT services (e.g., GPT3.5,\nGPT4) allows automatic scoring of instruction tun-\ning datasets. Recent studies on bringing LLMs as\njudges (Zheng et al., 2024; Wang et al., 2023a; Zhu\net al., 2023; Huang et al., 2024; Zeng et al., 2023;\nChan et al., 2023) reveal that powerful language\nmodels like ChatGPT highly align with human pref-\nerence on judging the quality of instructions and re-\nsponses. Given a well-designed prompt with clear\ndefinition on grading criteria, ChatGPT produces\njustified quality scorings with explanations:\nGPTScorei = G(xi, pG),\n(18)\nwhere pG denotes the prompt template that de-\nfines the task and grading scheme with format con-\nstraints on outputs. G represents the quality score\nparsed from the GPT response. Samples with high\nGPTScorei can be selected using Eqs. 4 and 5.\nPrompt pG for scoring xi with instruction\n(input) and response in the <dimension>\nWe would like to request your feedback on the\nperformance of AI assistant in response to the\ninstruction and the given input displayed following.\nInstruction: <instruction>\nInput: <input>\nResponse: <response>\nPlease rate according to the <dimension> of\nthe response to the instruction and the input. Each\nassistant receives a score on a scale of 0 to 5,\nwhere a higher score indicates higher level of the\n<dimension>.\nPlease first output a single line containing the\nvalue indicating the scores. In the subsequent line,\nplease provide a comprehensive explanation of your\nevaluation, avoiding any potential bias.\nTechnical Details\n(Chen et al., 2023b) proposes\na surprisingly easy-yet-effective method that di-\nrectly uses GPT3.5 to score datapoints in terms\nof helpfulness and accuracy (see the detailed\nprompt 3.3). Both instructions and responses are\nscored on a scale from 0 to 5 and experimental\nresults show that general instruction datasets, ex-\ncept coding-related samples, can be distilled into\nsmaller subsets for better downstream performance.\n(Bukharin and Zhao, 2023) follows (Chen et al.,\n2023b) for filtering Alpaca (Taori et al., 2023).\n(Chen and Mueller, 2024) employs the BSDetec-\ntor (Chen and Mueller, 2023) to estimate the con-\nfidence of GPT3.5/GPT4 on the give instruction-\nresponse pair. It takes both the self-consistency\nand direct scoring into consideration. Only highly\nconfident samples are kept for fine-tuning domain-\nspecific LLMs and those less confident ones are\ncorrected automatically by these LLMs. (Xu et al.,\n2023b) directly evaluates instruction datasets in\nterms of accuracy, explanation, clarity, and diffi-\nculty for weighted scorings from GPT4. Then, both\nhand-crafted indicators (i.e., lengthwise semantic\nevaluation) and GPT4 scorings are employed for\nfinal ranking. (Liu et al., 2023b) argues that the\ndirect scoring of GPT4 on one single instruction\nsample is not well-calibrated and instead gives rela-\ntive ranking of multiple instruction variants at once.\nThe complexity of instructions (Xu et al., 2023a)\nand the quality of instruction-response pairs are\nsequentially obtained from GPT3.5. (Zhang et al.,\n2024c) uses GPT scorings to judge: 1) whether\nthe given text contains mathematical contents; 2)\nand if yes, whether these maths contents are of\nhigh quality for education purpose. Such scores\nare proved more effective than traditional \u201cmathe-\nmatical\" classifiers (Paster et al., 2023). (Lu et al.,\n2023a) proposes to use ChatGPT for annotating\nopen-ended, fine-grained intention tags on open\ndatasets. Then, the quality of the tag dataset is\nevaluated by humans and GPT4 in terms of tagging\nprecision and consistency. Instead of fully relying\non the GPT4, (Li et al., 2023c) exploits the model\nunder investigation itself (e.g., LLaMA 65B) to\niteratively derive quality scores on each augmented\nexample on a 5-point scale. Then a curated clean\nset is chosen via Eq. 4.\nQuRator (Wettig et al., 2024) manually defines\nquality criterion such as writing style, facts and\ntrivia, educational value, and required expertise.\nThen, quality comparison is conducted on two\ninstruction-response samples via GPT3.5 scoring.\nSuch pairwise scorings are used to fine-tune a\nsheared-LLaMA 1.3B model (Xia et al., 2023) in\na manner similar to DPO (Ouyang et al., 2022;\nRafailov et al., 2024). It is noted that pairwise scor-\ning (Ouyang et al., 2022; Dubois et al., 2024; Zeng\net al., 2023; Liu et al., 2023b) have been found\nmore reliable, consistent, and unbiased than indi-\nvidual scoring (Gunasekar et al., 2023; Chen et al.,\n2023b) during GPT-based quality analysis.\nRemark\nClosed-source LLMs such as ChatGPT\nenjoy a high level of alignment with human prefer-\nence and therefore can be utilized to score data\nquality. It would be more cost-efficient to col-\nlect few (e.g., <100K) GPT-scored samples first\nand then fine-tune an open-source LLM for quality\nmeasurement on massive instruction corpus.\n3.4\nHuman Evaluation\nOverview\nHuman annotation and evaluation is\nindispensable in constructing preference alignment\ndatasets (Wang et al., 2023b; Ouyang et al., 2022)\nfor helpfulness, honesty, and harmlessness. Specif-\nically, human annotators deliver grading results\nfollowing specific criteria in multiple dimensions:\nLabelScorei = f(LabelScore1(xi),\nLabelScore2(xi), ..., LabelScoreM(xi)),\n(19)\nwhere LabelScorem(xi) can be both bool or in-\nteger (e.g., range from 0 to 5) for the m-th fine-\ngrained aspect. The aggregation function f is com-\nmonly chosen as summation or averaging.\nGuidelines (excerpts) for human annota-\ntions\n# Guidelines\nBelow is a list of guidelines that should be adhered\nto for each possible task available when building the\ndataset. To see some examples of how the guidelines\ncan be applied, visit the examples document.\n## 1. General rules\n- Always make sure to read and understand the\nguidelines to each task before fulfilling it. - Try to\nfollow the guidelines as closely as possible. - If you\nare unsure whether a message violates a guidelines,\ncontact us at our Discord.\n- Use the thumbs-up/thumbs-down system to further\nmark messages that are of high or low quality.\n## 2.\nProviding an assistant reply #assistant-\nreply\n### Do:\n- Remain polite and treat the user with respect, even\nwhen not given the same courtesy.\n...\nTechnical Details\nThe OpenAssistant (K\u00f6pf\net al., 2024) dataset is featured by its high-quality\nhuman-generated, human-annotated multi-lingual\nconversations for both instruction tuning and re-\ninforcement learning from human feedback (see\nthe guidelines excerpts 3.4). For each instruction-\nresponse pair along the conversation tree, the hu-\nman annotators are asked to categorize them ac-\ncording to three dimensions: spam detection, guide-\nline adherence, and quality. The quality score is\nrated on a five-point Likert scale across aspects\nincluding quality, creativity, humorousness, polite-\nness, and harmlessness. These scores are used to\nsort instructions for analysis and preference op-\ntimization of LLMs.\n(Lu et al., 2023a) enrolls\nhuman annotators to provide judgements on the\ntagging of each instruction. To verify the quality\nscores provided by humans, counterfactual cases\nare prepared respectively for precision and consis-\ntency tasks. Results show that human annotators\nhave low false positive rates at tagging precision,\nbut lack proof of confidence on their original qual-\nity judgements. (Zhou et al., 2024a) proposes to\nuse human annotators for creation of small-yet-\neffective instruction datasets. To collect questions\nand answers from various sources, simple hand-\ncrafted indicators such as text length are used to\nfilter low-quality datapoints. Then, high quality\ninstruction-response pairs are manually selected\n(750) and written (250) via subjective quality con-\ntrol. The databricks-dolly dataset (Conover et al.,\n2023) contains 15K human-generated instruction-\nresponse pairs. Although quality is emphasized dur-\ning large-scale annotation, imperfect samples still\nexist where low-quality and inaccurate responses,\nincomplete and vague instructions, problematic\ntexts with toxic language and grammar errors are\nfound (He et al., 2024).\nRemark\nHuman evaluation play a irreplaceable\nrole in quality control of preference alignment. To\nreduce the inter-annotator inconsistency, detailed\nguidelines should be prepared for quality measure-\nment. In addition, supplementary quality measures\nsuch as GPT-Scores can be provided for manually\nevaluating and selecting high-quality datasets.\n4\nDiversity-based Selection\nIn this section, we introduce methods that empha-\nsize the diversity of instruction datasets. When it\ncomes to diversity, existing researches either mea-\nsure the individual diversity of each sample (e.g.,\nlexical and semantic richness) or the overall diver-\nsity of the entire dataset (e.g., the volume of the\nenclosed embedding space). Instruction datapoints\nwhose tasks and domains are of minority classes in\na long-tailed distribution are preferred during sub-\nset selection. Such sampling philosophy strikes to\nmaintain or approximate the spread of the original\nembedding clusters but with much less sparsity.\n4.1\nHand-crafted Indicators\nOverview\nThe diversity of datasets is the key to\ndevelop less biased, more generalizable machine\nlearning models. However, recent studies (Zhao\net al., 2024c,b) show that existing vision and lan-\nguage datasets do not share a unified and concrete\ndefinition of diversity in terms of dataset composi-\ntion, source, domain, subject, annotator, and pro-\nmote (fairness). With respect to the diversity mea-\nsures specific in instruction tuning datasets, hand-\ncrafted indicators, similar to Eq. 3 in traditional\nNLP studies, can be used as a good starting point.\nTechnical Details\nOne of the most popular diver-\nsity measure is lexical diversity, which refers to the\nrange of different words occurring in one text. The\ngreater range implies greater diversity and quality.\nType-token ratio (TTR) (Templin, 1957; Richards,\n1987) is originally proposed as:\nTTRi = |Unique(xi)|\n|xi|\n,\n(20)\nwhere Unique(xi) denotes the set of unique tokens\npresent in xi. To reduce the sensitivity of TTR to\nthe variation of text length, several studies (Cov-\nington and McFall, 2008, 2010; Kettunen, 2014;\nMatlach et al., 2021) standardized the length by in-\ntroducing logarithms or n-grams into the formula.\nLater, computational approaches to measure lex-\nical diversity have been developed such as vocab-\nulary diversity (vocd-D) (Malvern and Richards,\n1997; Malvern et al., 2004; Silverman and Ratner,\n2002; deBoer, 2014), the measure of textual lexi-\ncal diversity (MTLD) (McCarthy and Jarvis, 2010;\nJarvis and Daller, 2013), and hypergeometric distri-\nbution diversity (HD-D) (Jarvis, 2013; McCarthy,\n2005). All these metrics require multi-step compu-\ntation for approximation. Specifically for vocd-D,\nrandom sampling is first performed on xi for a se-\nries of sub-sequences with varying lengths k (e.g.,\n10, 20, 30 tokens). Then, TTRk is:\nTTRk\ni = |Unique(xi(j\u2264,<j+k))|\n|xi(j\u2264,<j+k)|\n, 1 \u2264j \u2264|xi|\u2212k,\n(21)\nwhere xi(j\u2264,<j+k)) denotes the sub-sequence of\nxi starting from the randomly chosen index j and\nending at the index j+k. Then, the curve of TTRk\ni\nversus the lengths k is plotted and a mathematical\nmodel is built for fitting the curve:\n\u02c6\nTTR\nk\ni = D\nk [(1 + 2 k\nD)\n1\n2 \u22121],\n(22)\nwhere D is the only parameter required to be esti-\nmated. By approximating\n\u02c6\nTTR\nk\ni towards TTRk\ni\nwith the least squares, we have Dbest fit = D:\nvocd-Di = D.\n(23)\nA larger D reflects the higher diversity of xi. The\ncomputation of MTLD, on the other hand, first\ndetermines the TTRi as a pre-defined threshold,\nand then partitions xi into M different contiguous\nsubsequences {x1\ni , x2\ni , ..., xm\ni , ..., xM\ni }. Each sub-\nsequence xm\ni\n= xi(j\u2264,<j+k), \u2200k > 0, \u22001 \u2264j \u2264\n|xi| \u2212k maintains a TTRk\ni above the threshold\nTTRi. The MTLD is defined as:\nMTLDi = 1\nM\nM\nX\ni=1\n|xm\ni |.\n(24)\nThe HD-D shares the same idea behind vocd-D but\nstems from the hypergeometric distribution (Mc-\nCarthy and Jarvis, 2010). With M-times sampling,\nthe HD-D represents the probability of drawing a\ncertain number of tokens of the given type from the\nsubsequence of xi with a particular size k:\nHD-Di =\n|Unique(xi)|\nX\nt=1\n1\nM\nM\nX\nm=1\n1(xm\ni(n) = ut,\n\u22031 \u2264n \u2264|xm\ni |), ut \u2208Unique(xi),\nxm\ni =xi(j\u2264,<j+k), \u2200k > 0, \u22001 \u2264j \u2264|xi| \u2212k.\n(25)\nOther variants of TTR indicators such as MT-\nTRSS (Malvern et al., 2004), MSTTR (Malvern\net al., 2004), MATTR (Covington and McFall,\n2010), and MTLD-W (Vidal and Jarvis, 2020; Kyle\net al., 2021) all target at improving the solutions\nto two fundamental problems (Bestgen, 2023): 1)\nthe sensitivity of indicators to text length, and 2)\nthe impact of the indicator parameters. (Li et al.,\n2015) proposes two rather simplified TTR scores\nas distinct-1 and distinct-2, where the number of\ndistinct unigrams and bigrams of xi are respectively\ndevided by the total number of words. Many other\nstudies (Cao and Clark, 2017; Zhu et al., 2018; Shu\net al., 2019; Tevet and Berant, 2020) extend the\napplication of n-gram-based diversity measures for\nmodel-generated responses.\nApart from lexical diversity, there exists many\nefficient diversity indicators that are built upon the\nsemantics of each example. (Dong et al., 2011)\nproposes to approximate k-nearest neighbor (k-NN)\ngraph (Peterson, 2009) with arbitrary similarity\nmeasures on semantic embeddings of large-scale\ndatasets. Such efficient construction of a k-NN\ngraph allows the distance of xi to its j-th nearest\nneighbors to be a feasible diversity measure:\nkNNj\ni = d(g(xi), g(Nj(xi))),\n(26)\nwhere Nj(xi) denotes the j-th closest neighbor of\nxi in the embedding space projected by g(\u00b7). The\ncommon choices of the distance function d(\u00b7, \u00b7)\ninclude the Euclidean distance, cosine distance,\nand Jaccard coefficient distance (Huang et al.,\n2008). The projection from text (e.g., instruction-\nresponse pairs) into the embedding space can be\nachieved with pre-trained sentence BERT (Reimers\nand Gurevych, 2019; Feng et al., 2020), where\nan additional pooling operation is performed on\nthe final output of BERT (Devlin et al., 2018) for\nsentence embeddings. Note that a higher kNNi im-\nplies that the sample xi is more unique and should\nbe kept in subset selection for higher diversity.\nDue to the fine-grained representation capability of\nBERT, existing hand-crafted indicators often rely\non BERT embeddings for similarity or diversity\nmeasurement (Tevet and Berant, 2020; Zhang et al.,\n2019; Larson et al., 2019; Yauney et al., 2023).\nTo improve the generalization of diversity mea-\nsure, (Xu et al., 2023b) argues that the statistics\nof feature embedding of each sample itself should\nbe considered. It does not require additional prior\nknowledge on the structure of embeddings. Given\nall datapoints xi \u2208S, their semantic embeddings\nfrom any sentence encoder can be represented as\nX = [g(x1), g(x2), ..., g(xN)] \u2208R|S|\u00d7H. The\nrow variance V ari of each embedding g(xi) in\nthe reduced dimensional space R|S|\u00d7k by principal\ncomponents analysis (PCA) (Wold et al., 1987) is\nused as the diversity indicator:\nV ari =\n1\nk \u22121\nX\n(j = 1)k(Yij \u2212\u00b5i)2,\n\u00b5i = 1\nk\nk\nX\nj=1\nYij\n(27)\nwhere the PCA chooses the top-k eigenvectors\n(V\n= [v1, v2, ..., vk] with \u03bb1 \u2265\u03bb2 \u2265... \u2265\n\u03bbk) of the covariance matrix Cov = Q\u039bQT =\n1\n|S|\u22121(X \u2212\u00b5X)T (X \u2212\u00b5X), \u00b5X =\n1\n|S|\nP|S|\ni=1 Xi to\nproject the original embeddings into more compact\nand reduced ones via Y = (X \u2212\u00b5X)V . Samples\nwith the highest 20% V ari (via Eq. 5) are selected\nas the variety-curated dataset.\nWhen it comes to the overall diversity of a\ndataset S, the average distance of any sample xi to\nits closest neighbor in the dataset, namely kNNi,\ncan be leveraged intuitively:\nDkNN(S) = 1\n|S|\n|S|\nX\ni=1\nkNN1\ni , xi \u2208S.\n(28)\nSuch a diversity measure has been widely used in\ndataset construction and content retrieval (Stasaski\net al., 2020; Stasaski and Hearst, 2022; Mithun\net al., 2019; Spyromitros-Xioufis et al., 2015; Sun\net al., 2024a; Ionescu et al., 2018). (Du and Black,\n2019) simply performs clustering on all samples\nwith k-means (Ikotun et al., 2023) into K clusters\n(C1,C2,...,CK) in the embedding space, and then\nuses the cluster inertia as diversity indicators:\nDinertia(S) =\nK\nX\nj=1\nX\nxi\u2208Cj\n\u2225g(xi) \u2212\u00b5j\u22252,\n\u00b5j =\n1\n|Cj|\nX\nxi\u2208Cj\ng(xi).\n(29)\n(Lai et al., 2020) develops a diversity metric on the\ndispersion of a cluster induced by embeddings of\nall samples, where the cluster is approximated by a\nmulti-variate Gaussian distribution:\nDradius(S) =\nH\nv\nu\nu\nt\nH\nY\nj=1\n\u03c3j,\n(30)\nwhere H is the dimension of the projected embed-\ndings g(xi) \u2208RH and \u03c3j denotes the radius of the\nellipsoid along the j-th axis of the dataset S. The\ninter-cluster (class) distance can also be used for\ndiversity measure (Dang and Verma, 2024):\nDICD(S) = 1\nK\nK\nX\nj=1\ndivJS(Pj||P\u0338=j),\n(31)\nwhere Pj denotes the inverse-document frequency\n(IDF) distribution (Sparck Jones, 1972) of the clus-\nter Cj and divJS is the Jensen-Shannon divergence.\nRemark\nBoth lexical and semantic diversity\nshould be considered with hand-crafted indicators.\nThe optimization of individual diversity would con-\ntribute to the overall diversity of the entire dataset.\n4.2\nModel-based Indicators\nOverview\nSimilar to Eq. 6, model-based indica-\ntors on diversity also rely on the target or proxy\nlanguage model for computing the indices.\nTechnical Details\nThe diversity of a dataset S\ncan be intuitively defined as the sum of rarity mea-\nsures of each constituting element xi. Accord-\ningly, entropy-related methods are proposed to es-\ntimate such rarity. The more uncommon, various\nsamples exist, the higher diversity the dataset be-\ncomes. Mathematically, the vanilla entropy (Shan-\nnon, 1948) is proposed for diversity measures:\nDentropy(S) = \u2212\nX\nxi\u2208S\nP(xi|\u03b8) \u00b7 log2(P(xi|\u03b8)),\n(32)\nwhere P(xi) denotes the probability of xi occur-\nring in the dataset. Later, R\u00e9nyi entropy (R\u00e9nyi,\n1961) introduces an additional parameter \u03b1 >\n0, \u03b1 \u0338= 1 for a generalized entropy definition:\nDRE\n\u03b1 (S) =\n1\n1 \u2212\u03b1 log2(\nX\nxi\u2208S\nP(xi|\u03b8)\u03b1).\n(33)\nThe parameter \u03b1 adjusts the element-wise emphasis\non rare or frequent events.\nStudies on biology and ecology (Mouillot and\nLepretre, 1999; Peet, 1974; He and Hu, 2005; Gre-\ngorius and Gillet, 2008) investigate Simpson\u2019s In-\ndex (SI) (Simpson, 1949; Wu et al., 2024, 2022)\nfor measuring the biodiversity of species and ge-\nnetics. (Zhou et al., 2020) proposes a variant of the\noriginal SI with a more flexible statistic metric:\nDSI(S) = 2\nP\nxi,xj\u2208S,i\u2264j 1(xi = xj|\u03b8)\n|S|(|S| + 1)\n,\n(34)\nwhere the equivalence of xi and xj is judged by an\nindicator function parameterized as \u03b8.\nVendi Score (VS) (Dan Friedman and Dieng,\n2023; Pasarkar and Dieng, 2023; Nguyen and\nDieng, 2024) is rencently proposed for diversity\nmeasurement in machine learning researches. In-\nspired by the R\u00e9nyi entropy, a generalized VS met-\nric (Pasarkar and Dieng, 2023) is defined as below:\nDV S\n\u03b1 (S) = exp(\n1\n1 \u2212\u03b1 log2(\n|S|\nX\ni=1,i\u2208supp(\u00af\n\u03bb)\n\u00af\n\u03bb\u03b1\ni|\u03b8)),\n(35)\nwhere \u00af\n\u03bbi|\u03b8 denotes the normalized eigenvalues of\nthe similarity kernel matrix KS|\u03b8, and supp(\u00af\n\u03bb) is\nthe set of indices of all non-zero eigenvalues. The\nsmaller \u03b1 < 1 makes the scoring more sensitive to\nrare classes and therefore allows accurate diversity\nmeasurement even under severe class imbalance.\nOne simple implementation of the similarity kernel\nKS|\u03b8 is to use the Gaussian Radial Basis function k\nwith feature embeddings as k(g(xi|\u03b8), g(xj|\u03b8)) =\nexp(\u22121\n2\u2225g(xi|\u03b8) \u2212g(xj|\u03b8)\u22252). (Nguyen and Di-\neng, 2024) further introduces quality scoring into\nEq. 35 where for each subset Sb \u2282S, its average\nquality score Q(Sb) =\n1\n|Sb|\nP\nxi\u2208Sb INDi is multi-\nplied with DV S\n\u03b1 (Sb) for comprehensive evaluation\nin terms of quality and diversity.\n(Miranda et al., 2022) proposes an intrinsic diver-\nsity coefficient to measure the diversity of a dataset\nwith Task2Vec embeddings (Achille et al., 2019;\nNguyen et al., 2019) for distance computation be-\ntween different tasks. The Task2Vec encodes data\nfrom different tasks by the diagonal entries of the\nFisher Information Matrix (FIM). The FIM results\nfrom fine-tuning only the final (e.g., token classi-\nfication) layer of a pre-trained model, namely a\nprobe model (e.g., GPT2 (Radford et al., 2019)),\nto solve the task. Given a batch of samples B, the\nmathematical representation of FIM is defined as:\n\u02c6\nFB = Exi,j,\u02c6\nxi(j)\u2207\u03b8 log P(\u02c6\nxi(j)|xi(<j); \u03b8)\u00b7\n\u2207\u03b8 log P(\u02c6\nxi(j)|xi(<j); \u03b8)T ,\n(36)\nwhere \u02c6\nxi(j) denotes the j-th token predicted from\nthe model parameterized as \u03b8 given the real se-\nquence input xi(<j). The expectation Exi,j,\u02c6\nxi(j)\ntakes an average over the sequence length |xi| for\neach xi sampled randomly from the batch xi \u2208B.\nThe Task2Vec embedding\n\u2192\nfB = diag(FB), where\ndiag(\u00b7) denotes the diagonal entries of FB. Based\non the Task2Vec embeddings, (Lee et al., 2023)\nproposes to compute the diversity coefficients \u02c6\ndiv\nspecifically for NLP datasets:\nD\n\u02c6\ndiv(S) = EB1,B2\u223cSd(\n\u2192\nfB1,\n\u2192\nfB2),\nD\n\u02c6\ndiv(S1, S2) = EB1\u223cS1,B2\u223cS2d(\n\u2192\nfB1,\n\u2192\nfB2),\n(37)\nwhere d denotes distance measurement (e.g., cosine\ndistance). Both B1 and B2 are two batches sampled\nrespectively from the same or different datasets\nfor diversity measures within or across datasets.\nExperiments confirm that hand-crafted indicators\nsuch as the number of latent concepts (Xie et al.,\n2021) and the richness of vocabulary are positively\nassociated with the proposed \u02c6\ndiv coefficients.\n(Lu et al., 2023a) develops a diversity measure\nby open-ended tagging. Specifically, a tagging\nmodel parameterized by \u03b8 is trained with GPT4-\nlabeled tagging pairs to describe each instruction\ntuning datapoint xi by its fine-grained, atomic in-\ntentions and semantics (e.g., tasks and domains).\nCorrespondingly, the number of tags can be viewed\nas a diversity indicator for sampling a instruction\nsubset Sb from the whole set S (see Alg. 1).\nRemark\nThe model-based indicators are high-\nlighted by their flexibility in handling various as-\npects of diversity either implicitly or explicitly.\n4.3\nGeometry-based Coreset Sampling\nOverview\nInstead of explicitly calculating the\ndiversity-aware indicators, recent studies on se-\nlecting instruction datasets tend to introduce core-\nset sampling methods for a systematic considera-\ntion (Guo et al., 2022). Specifically, coreset sam-\npling aims to find the most informative-and-diverse\nsubset that represents the entire dataset the most,\nso that close or even surpassing performance can\nbe achieved on the language model trained on the\nsubset with respect to that on the entire set.\nTechnical Details\nAmong different categories\nof coreset sampling methods, geometry-based\nmethods are the most intuitive and widely-used\nones (Chen et al., 2012; Agarwal et al., 2020; Sener\nAlgorithm 1 TagLM-based Diverse Sampling (Lu\net al., 2023a)\nRequire: data xi \u2208S, a tagging LLM T\u03b8, a vis-\nited tag set DB\nb , and a budget b\n1: Initialize Sb = \u2205\n2: for each xi \u2208S do\n3:\nObtain tags Dxi = T\u03b8(xi)\n4: end for\n5: repeat\n6:\nInitialize DB\nb = \u2205\n7:\nfor each xi = arg maxxi\u2208S Dxi do\n8:\nif |DB\nb \u222aDxi| > |DB\nb | then\n9:\nSb = Sb \u222a{xi}\n10:\nDB\nb = DB\nb \u222aDxi\n11:\nS = S\\{xi}\n12:\nend if\n13:\nend for\n14: until |Sb| = b\n15: return Sb\nand Savarese, 2017; Sinha et al., 2020; Kamalov,\n2020; Rezazadegan Tavakoli et al., 2011; Kirchen-\nbauer et al., 2024; Zhou et al., 2023). The intu-\nition behind is that close samples in the embedding\nspace often share similar properties with low diver-\nsity. Therefore, redundant information can be effec-\ntively suppressed by controlling the minimum dis-\ntance between any two samples for subset selection.\nSpecifically, k-center greedy is a typical diversity-\noriented sampling method for massive pretraining\nand instruction-tuning corpus (Chen et al., 2023a;\nBhatt et al., 2024; Wu et al., 2023; Zhao and Fang,\n2024; Du et al., 2023). It solves the minimax facil-\nity location (FL) problem (Cornu\u00e9jols et al., 1983;\nFarahani and Hekmatfar, 2009), i.e., selecting the\nsubset Sb under the given size budget b from the\nfull set S so that the largest distance between an\nexample in S\\Sb and its closest example in Sb is\nminimized:\nmin\nSb\u2282S, |Sb|=b max\nxi\u2208S\\Sb\nmin\nxj\u2208Sb d(g(xi), g(xj)).\n(38)\nThe direct solution to Eq. 38 is NP-hard (Cook\net al., 1994) and a greedy approximation is pro-\nposed (Sener and Savarese, 2017) (see Alg. 2). For\ninitialization of S0\nb , one can either choose randomly\nsampled datapoints from S, or use the cluster cen-\nter points from K clusters (C1, C2, ..., CK) of S\nvia k-means clustering. Similarly, the farthest point\nsampling method (Eldar et al., 1997) shares the\nsame principle that each iteration time only the\nAlgorithm\n2\nK-Center\nGreedy\n(Sener\nand\nSavarese, 2017)\nRequire: data xi \u2208S, existing pool S0\nb and a\nbudget b\n1: Initialize Sb = S0\nb\n2: repeat\n3:\nu = arg maxxi\u2208S\\Sb\nminxj\u2208Sb d(g(xi), g(xj))\n4:\nSb = Sb \u222a{u}\n5: until |Sb| = b + |S0\nb |\n6: return Sb\\S0\nb\nfarthest datapoint relative to the already selected\ncoreset is chosen from the candidates.\nIn addition to the k-center greedy, the herding\nmethod (Chen et al., 2012; Welling, 2009; Husz\u00e1r\nand Duvenaud, 2012; Adhikary and Boots, 2022)\nselects datapoints xi so that the distance between\nthe coreset center and the full set center is min-\nimized in the embedding space. For efficiency,\nit is also approximated via greedy implementa-\ntion (Chen et al., 2016; Harvey and Samadi, 2014)\nby adding one sample each time into the Sb to mini-\nmize the distance between two centers (see Alg. 3).\nAlgorithm 3 Herding Greedy (Harvey and Samadi,\n2014)\nRequire: data xi \u2208S, a budget b\n1: Initialize \u00b5 = 1\nn\nPn\ni=1 g(xi)\n2: Initialize Sb = \u2205\n3: for t = 1 to b do\n4:\nu = arg minxi\u2208S\\Sb \u2225\u00b5\u2212\n1\n|Sb|+1\nP\nxj\u2208Sb\u222a{xi} g(xj)\u22252\n5:\nSb = Sb \u222a{u}\n6: end for\n7: return Sb\nFurthermore, recent studies tend to develop\ncomplex heuristic sampling methods that takes\ngeometry-based diversity into consideration (Jiang\net al., 2023c; Chan et al., 2021; Xia et al., 2022).\nSpecifically, the inter-sample similarity of the se-\nlected coreset is minimized in return for an overall\nhigh diversity. (Jiang et al., 2024c) proposes to\npreserve informative subset with the learning com-\nplexity (see Eq. 8) and implicitly puts constraints\non its diversity via sampling on the k-means clus-\nters:\nDdist(S) = 1\n|S|\nX\nxi\u2208S\nmin\nj\u0338=i d(xi, xj) \u2265C,\n(39)\nwhere C denotes the constant that controls the de-\ngree of diversity. A larger C represents the larger\ndiversity of the dataset. The detailed procedure can\nbe found in Alg. 4.\nAlgorithm 4 Easy and Diverse First Sam-\npling (Jiang et al., 2024c)\nRequire: data xi \u2208S, existing pool S0\nb , a budget\nb, and the number of clusters K\n1: Initialize Sb = S0\nb\n2: arg minC\nPK\nj=1\nP\nxi\u2208Cj\u2282S \u2225g(xi)\n\u2225g(xi)\u2225\u2212\u00b5j\u22252,\n\u00b5j =\n1\n|Cj|\nP\nxi\u2208Cj\ng(xi)\n\u2225g(xi)\u2225.\n3: for j = 1 to K do\n4:\nSj\nb = {xi| \u02c6\nF \u02dc\nS( \u02dc\nS(xi)) \u2264b\nK , xi \u2208Cj}\n5:\nSb = Sb \u222aSj\nb\n6: end for\n7: return Sb\n(Bukharin and Zhao, 2023) proposes the quality-\ndiversity instruction tuning (QDIT). It also uses FL\nfunctions for diversity measure of the subset Sb:\nDFL(Sb) =\nX\nxj\u2208S\nmax\nxi\u2208Sb sim(g(xi), g(xj)), (40)\nwhere sim(\u00b7, \u00b7) denotes the similarity function\n(e.g., cosine similarity). If the selected Sb can be\nwell-representative of the entire set S, then Sb is\nassumed of high diversity. Given quality scores de-\nfined by Eq. 18, the detailed mechanism of QDIT\nis described in Alg. 5 with greedy approximation.\nAlgorithm 5 QDIT sampling (Bukharin and Zhao,\n2023)\nRequire: data xi \u2208S, a budget b, and the trade-\noff hyper-parameter \u03b1\n1: Initialize Sb = \u2205\n2: for t = 1 to b do\n3:\nu = arg maxxi\u2208S\\Sb(1 \u2212\u03b1) \u00b7 DFL(Sb \u222a\n{xi}) + \u03b1 \u00b7 GPTScorei\n4:\nSb = Sb \u222a{u}\n5: end for\n6: return Sb\n(Liu et al., 2023b) adopts the quality score-\nfirst and diversity-aware data selection method\n(DEITA), where all datapoints are first scored and\nsorted by quality measurement, and then selected\nby a geometry-based heuristic criterion (i.e., Repr\nFilter). Specifically, it considers that for each cho-\nsen datapoint in Sb, its kNN1\ni (Eq. 26) should be\nabove a certain threshold \u03c4 so that the overall di-\nversity DkNN(Sb) (Eq. 28) can be improved. As\nshown in Alg. 6, the quality and complexity of each\nsample xi is respectively measured by the trained\ncomplexity scoring model \u03b8C and the quality scor-\ning model \u03b8Q with prompts pC and pQ. Then, sam-\nples with high GCQ are prioritized but only those\ndissimilar ones can be kept for the diversity of Sb.\nAlgorithm 6 DEITA Sampling (Liu et al., 2023b)\nRequire: data xi \u2208S and a budget b\n1: Compute the combined complexity and qual-\nity score\nGCQ(xi)\n=\nG(xi, pC|\u03b8C) \u00b7\nG(xi, pQ|\u03b8Q)\n2: u = arg maxxi\u2208S GCQ(xi|\u03b8)\n3: Initialize Sb = {u}\n4: S = S\\{u}\n5: while |Sb| < b do\n6:\nu = arg maxxi\u2208S GCQ(xi|\u03b8)\n7:\nif d(g(u), g(N0(u)) > \u03c4, N0(u) \u2208Sb\nthen\n8:\nSb = Sb \u222a{u}\n9:\nend if\n10:\nS = S\\{u}\n11: end while\n12: return Sb\nAnother series of geometry-based methods\nfocus on the organization of data structures\nvia developing clustering-based sampling tech-\nniques (Citovsky et al., 2021; Tirumala et al., 2024;\nAxiotis et al., 2024; Shao et al., 2024; Alcoforado\net al., 2024; Saranathan et al.). With respect to\nthe clustering criterion, traditional methods em-\nploy topic modeling with LDA (Blei et al., 2003;\nRaghuveer et al., 2012; Bui et al., 2017), NMF (Lee\nand Seung, 2000; Wang and Zhang, 2012; Shen\nand Si, 2010; Lazar and Doncescu, 2009), TF-\nIDF (Sparck Jones, 1972; Bafna et al., 2016; Patil\nand Atique, 2013; Roul et al., 2014), and latent con-\ncepts (Xie et al., 2021) to assign text corpus into\nthematic clusters. Most recent studies exploit sen-\ntence encoding methods (Reimers and Gurevych,\n2019; Feng et al., 2020) to perform clustering in the\nembedding space, where the vanilla k-means clus-\ntering and its variants (Sinaga and Yang, 2020; Ka-\nnungo et al., 2000; Bandyapadhyay and Varadara-\njan, 2015), DBSCAN (Deng, 2020; Khan et al.,\n2014; Cre\u00b8\ntulescu et al., 2019), and spectral cluster-\ning (Bach and Jordan, 2003; Von Luxburg, 2007;\nJia et al., 2014) are widely used. Specifically, (Tiru-\nmala et al., 2024) proposes to use SemDeDup (Ab-\nbas et al., 2023) to remove semantically similar\nexamples for deduplication, which provides a basis\nof diversity sampling. Then, k-means clustering is\nperformed in the embedding space and prototype-\nbased sampling technique (Sorscher et al., 2022) is\nused. The \u201cprototypical\" samples, whose distance\nto their assigned cluster centers are small, should\nbe discarded first to allow more \u201coutliers\" to be\nkept in Sb during iterative sampling (see Alg. 7).\nAlgorithm 7 D4 Sampling (Liu et al., 2023b)\nRequire: data xi \u2208S, a budget b, the number of\nclusters for SemDeDup K1 and the number of\nclusters for prototypicality K2\n1: Initialize Sd = \u2205, Sb = \u2205\n2: arg minC\nPK1\nj=1\nP\nxi\u2208Cj\u2282S \u2225g(xi)\n\u2225g(xi)\u2225\u2212\u00b5j\u22252,\n\u00b5j =\n1\n|Cj|\nP\nxi\u2208Cj\ng(xi)\n\u2225g(xi)\u2225.\n3: for j = 1 to K1 do\n4:\nCv\nj = \u2205\n5:\nwhile |Cv\nj | < |Cj| do\n6:\nu = arg minxi\u2208Cj\\Cv\nj sim(g(xi), \u00b5j)\n7:\nif maxxi\u2208Cj sim(g(u), g(xi))\n<\n\u03c4\nthen\n8:\nSd = Sd \u222a{u}\n9:\nend if\n10:\nCv\nj = Cv\nj \u222a{u}\n11:\nend while\n12: end for\n13: arg minC\nPK2\nj=1\nP\nxi\u2208Cj\u2282Sd \u2225g(xi)\n\u2225g(xi)\u2225\u2212\u00b5j\u22252,\n\u00b5j =\n1\n|Cj|\nP\nxi\u2208Cj\ng(xi)\n\u2225g(xi)\u2225.\n14: for j = 1 to K2 do\n15:\nSj\nb = {xi| \u02c6\nFd(d(xi, \u00b5j) >\nb\nK2 , xi \u2208Cj}\n16:\nSb = Sb \u222aSj\nb\n17: end for\n18: return Sb\n(Axiotis et al., 2024) proposed a k-means cluster-\nbased sensitivity sampling technique. For each\nsample in one cluster, its distance to the cluster\ncenter and the proxy evaluation loss (Feldman and\nLangberg, 2011) of the center datapoint are both\nproportional to the probability of being chosed.\n(Shao et al., 2024) proposes the balanced Cluster-\nClip sampling. It first performs k-means clustering\nand then sample datapoints uniformly from each\ncluster. Different from the uniform sampling, the\nproposed ClusterClip puts constraints on the max-\nimum number of each cluster being sampled, and\ntherefore avoids overfitting of small clusters.\n(Alcoforado et al., 2024) comprehensively com-\npare different geometry-based diversity sampling\ntechniques such as similarity or distance-based\ngreedy sampling and clustering-based sampling.\nIt proposes three approaches to select subsets Sb\nfor human annotation: 1) reverse semantic search,\n2) ordered clustering, and 3) limited lexical similar-\nity. For the reverse semantic search, two datapoints\n(xi, xj) that share the least semantic similarity are\nfirst sampled as S0\nb and then iterative selection of\nthe next most dissimilar element from S is added\ninto S0\nb . Its implementation is quite similar to the\nk-center greedy algorithm (see Alg. 2) except for\nthe initialization of S0\nb . For the limited lexical\nsimilarity approach, the first sample x0 is chosen\nrandomly for initialization of S0\nb . For the remain-\ning b \u22121 quota, each sample xi is also randomly\nchosen from S\\Sb as long as sim(xi, xi\u22121) \u2264\u03c4,\nwhere sim(\u00b7, \u00b7) here denotes the lexical similarity\nsuch as BLEU (Papineni et al., 2002) and ROUGE\nscores (Lin, 2004). The ordered clustering applies\na hierarchical and density-based clustering algo-\nrithm like HDBSCAN (Campello et al., 2013) on\nall samples and sequentially (i.e., from large to\nsmall clusters) choose the samples of the lowest\nmembership in each cluster into the subset Sb. Ex-\nperimental results show that the reverse semantic\nsearch performs most consistently and competi-\ntively, while the limited lexical similarity is sen-\nsitive to the hyper-parameter threshold \u03c4. The or-\ndered clustering is not robust across datasets and\nfails to select high-quality samples.\nRemark\nGeometry-based sampling is intuitive\nand effective in diversity control. Most solutions\nto optimizing the overall diversity can be refor-\nmulated as variants of an iterative similarity or\ndistance-based greedy sampling technique. Cluster-\ning does play an explanatory role in deciphering the\nembedding structures, making it easier and preciser\nto control the proportion of selection.\n4.4\nBilevel Optimization-based Coreset\nSampling\nOverview\nThe selection of coreset can also be\nviewede as a bilevel optimization problem (Colson\net al., 2007; Zhang, 2024; Sinha et al., 2017; Bor-\nsos et al., 2020; Killamsetty et al., 2021b,c; Zhang\net al., 2022; Borsos et al., 2024; Pan et al., 2024)\nthat consists of two loops: 1) the outer loop of opti-\nmizing the hard masks or soft weights for selecting\nthe subset Sb from S; 2) the inner loop of optimiz-\ning the model parameters \u03b8 on Sb. Without lose\nof generalizability, the bilevel optimization with\nthe self-supervised language modeling loss can be\nwritten as follows:\nS\u2217\nb = arg min\nSb\u2282S\nX\nxi\u2208Sb,\u03b8=\u03b8\u2217\nNLLA|Q\ni\n,\ns.t. \u03b8\u2217= arg min\n\u03b8\nX\nxi\u2208Sb\nNLLA|Q\ni\n.\n(41)\nTechnical Details\nThe retrieve method proposed\nby (Killamsetty et al., 2021c) takes both labeled\nand unlabeled datasets into consideration, where\nthe self-supervised loss from the unlabeled set (e.g.,\nconsistency regularization (Xie et al., 2020; Wang\net al., 2021c) and entropy regularization (Zhao\net al., 2020b; Grandvalet and Bengio, 2004; Erkan\nand Altun, 2010)) contributes to the inter-level and\nouter-level optimization as well. To improve the\nrobustness, Glister (Killamsetty et al., 2021b) opti-\nmizes the outer-level coreset selection on the addi-\ntionally prepared validation set for the minimized\nvalidation loss. (Li et al., 2023d) further empha-\nsizes the role of the validation set in bilevel op-\ntimization. It not only computes the loss on the\nvalidation set for adversarial training, but also intro-\nduces gradient matching (Killamsetty et al., 2021a)\nwhere the gradient of the model on the selected\nsubset Sb should be close to that on the entire S.\n(Borsos et al., 2024) reformulates the coreset\nsampling as a cardinality-constrained bilevel op-\ntimization problem. It proposes greedy forward\nselection and first-order methods that apply to any\ntwice differentiable models. Variants of the so-\nlution for acceleration are extended: 1) binary\nweights, inverse-hessian-vector product approxi-\nmations, and batch-wise selection; 2) small proxy\nmodels for fast estimation; 3) enforced sparsity-\ninducing penalty in the outer loop.\nThe ScaleBiO (Pan et al., 2024) specifically ad-\ndresses the data reweighting problem for large-\nscale LLM instruction tuning. It also prepares an\nextra validation set Sval for the minimization of\nthe outer loop. ScaleBio transforms the bilevel op-\ntimization into the single loop framework with an\nouter-level problem plus a constraint of the inner-\nlevel problem. A multiplier \u03b1 > 0 and a proxy u\nfor optimizing the original inner loop (i.e., model\nweights \u03b8) are introduced into the minimax formu-\nlation (Kwon et al., 2023; Lu and Mei, 2024).\nIn contrast to a fixed budget b, (Xia et al., 2024b)\nproposes a lexicographic bilevel-optimization\nmethod (Borsos et al., 2020; Killamsetty et al.,\n2021b,c) where the inner loop optimizes model\nparameters and the outer loop optimizes data se-\nlection. When optimizing the selection mask, the\nminimization of loss terms is relaxed to allow the\nsize of the final coreset smaller than b.\nRemark\nThe bilevel optimization methods of-\nten involve optimization regularization tricks as a\nrelaxation to the original problem with nested outer-\ninner loops. Compared with the hard masks, the\nsoft weights-based objective guarantees a higher\nlevel of diversity as each sample contributes more\nor less to the overall optimization.\n5\nImportance-based Selection\nThis section provides the review of methods on\nimportance measurement and selection. By im-\nportance we mean the necessity of adding one\ninstruction-response sample into the training set.\nDue to the pre-training nature of LLMs, a wide\nrange of materials have been \u201cparameterized\" as\ninternal knowledge and therefore several common\ntasks can be correctly solved without additional\nfine-tuning. In this case, alignment is not required\nfor easy samples but becomes indispensable for\ndifficult ones. The selected datapoints provide sup-\nplementary knowledge to activate the pre-trained\nLLMs on following complex instructions.\n5.1\nHand-crafted Indicators\nOverview\nExisting researches on importance\nmeasurement of datapoints often stem from two as-\npects: 1) from the perspective of a datapoint itself,\ni.e., the difficulty or complexity of each datapoint\nand the amount of information it provides; 2) from\nthe perspective of the model under development,\ni.e., the necessity of learning from such a datapoint\nbased on the current performance and confidence\n(uncertainty), Most hand-crafted indicators are pro-\nposed to analyze the text difficulty.\nTechnical\nDetails\nThe\nreadability\nin-\ndices (Young\nand Shishido, 2023) can\nbe\nused to assess both quality (see Sec. \u00a73.1) and\ndifficulty of text samples. Specifically, samples\nwith intricate grammar, advanced vocabulary, and\ninference dependency are deemed as difficult\nones and can be used to evaluate robustness of\nmodels across benchmarks of various difficulty\nlevels (Smith and Johnson, 2020; Kiela et al.,\n2021; Ethayarajh et al., 2022; Belinkov and Glass,\n2019; Nie et al., 2019; Ribeiro et al., 2020). For\nspecialized domains such as solving maths prob-\nlems, the education level (e.g., elementary-level,\nhigh school-level, and university-level) determines\nthe difficulty of samples (Patel et al., 2021; Huang\net al., 2016; Koncel-Kedziorski et al., 2016).\nOne of the pioneering studies on readability\nscores for difficulty assessment is to compute the\npercentage of difficult or easy words in one sen-\ntence (Klare, 1974; Begeny and Greene, 2014).\nThe words on a pre-defined list are counted as\nfamiliar words, and those not listed are unfamil-\niar, advanced words. Besides, the average num-\nber of syllables per word, the number of single-\nsyllable words, and the number of multi-syllable\nwords are also indicative in assessing the text mate-\nrials (Connatser, 1999; Carrell, 1987; Zakaluk and\nSamuels, 1988; Dale and Chall, 1949). Notably,\nthere exist three representative readability metrics:\n1) the Dale Chall formula (Chall and Dale, 1995),\n2) the flesch reading ease (Flesch, 1948), and 3) the\ngunning fog index (Gunning, 1952). Given these\nmetrics, (Saranathan et al.) conducts a thorough\nanalysis on existing NLP datasets S to select the\nmost challenging subsets for efficient evaluation of\nLLMs. The easiest and hardest samples from the\nTruthfulQA (Lin et al., 2021) via these indicators\nare confirmed positively correlated with the actual\ncomplexity. The selection of difficult instruction-\nresponse pairs via Eq. 5 allows the wider perfor-\nmance distribution of models under investigation,\nmaking it accurate to keep the relative rank of dif-\nferent models unchanged on subsets Sb.\nRemark\nThe computing of difficulty indices\nhelps comprehensively analyze the robustness of\nmodels across samples and datasets. In addition, it\nalso presents guidelines in curating and construct-\ning discriminating NLP benchmarks.\n5.2\nModel-based Indicators\nOverview\nTo avoid potential confusion, the\nmodel-based importance indicators discussed in\nthis section are mainly categorized as three kinds:\n1) uncertainty-based; 2) reward score-based; and\n3) data model-based. Methods that employ train-\ning/inference losses, errors (metrics), and gradients,\ndespite their involvement of the language model\nfor importance sampling, are not included.\nTechnical Details\nInspired from uncertainty in-\ndicators (Siddhant and Lipton, 2018; Kung et al.,\n2023; Nieth et al., 2024) proposes the prompt uncer-\ntainty, which measures the disagreement of model\nresponses on different perturbed versions of the\nsame instruction:\nU prompt\ni\n= \u22121\nK\nK\nX\nk=1\n|xi|\nX\nj=t\n|P(xi(j)|xi(<j); \u03b8)\u2212\nP(xi(j)|\u02dc\nxk\ni(<j); \u03b8)|,\n(42)\nwhere K denotes the number of perturbations and\n\u02dc\nxk\ni is the k-th perturbed prompt. Note that only the\ninstruction part xi(<t) is perturbed and sent to the\nmodel for the following likelihood measurement\non the original response xi(j), j = t, t + 1, ..., |xi|.\nSamples with high prompt uncertainty should be\nchosen for fine-tuning since the model does not\nperform consistently on such instructions.\n(Jiang et al., 2023b) targets at the over-\nconfidence problem of LLMs after instruction tun-\ning (Kadavath et al., 2022), and proposes to cali-\nbrate the uncertainty with augmented prompt en-\nsembles.\nIt casts the uncertainty estimation of\neither discriminative or generative tasks into a\nmultiple-choice selection problem. Specifically\nfor open-generation tasks, different candidate re-\nsponses are designed to be as diverse as possible\nby: 1) prompting explicitly to encourage seman-\ntically distinct answers, or 2) clustering sampled\nresponses (with a high temperature) into groups\nand choosing the prototype response from each\ngroup. Such calibrated uncertainty can be used to\nprecisely choose important samples.\nApart from the uncertainty, the reward model\ncan also be used beyond quality scorer. Since most\nof the knowledge and capabilities are acquired dur-\ning pre-training (Zhou et al., 2024a), the instruction\ntuning datasets are aimed at aligning the behavior\nof models with human preference and expectations.\nTherefore, for any given instruction xi, if the gener-\nated response is of high quality, then the necessity\nof fine-tuning on this instruction is low. Accord-\ningly, xi is deemed as \u201cunimportant\" and will not\nbe chosen into the subset. In that case, the lan-\nguage model parameterized as \u03b8 is first prompted\nwith xi(<t) to generate the response \u02c6\nx\u03b8\ni(\u2265t). Then, a\nreward model parameterized as \u03d5 acts as a necessity\nevaluation model:\n\u02c6\nRi = r\u03d5(xi(<t), \u02c6\nx\u03b8\ni(\u2265t)),\n(43)\nSamples whose necessity score \u02c6\nRi below a pre-\ndetermined threshold are selected via Eq. 4, imply-\ning that the model \u03b8 does not own the capabilities\nto handle xi and requires fine-tuning.\nAnother series of model-based importance es-\ntimation methods are based on datamodels (Ilyas\net al., 2022; Park et al., 2023; Jain et al., 2023;\nKang et al., 2024; Chhabra et al., 2024; Saunshi\net al., 2022; Ye et al., 2024), where the contribution\nof each datapoint to the model\u2019s behavior is esti-\nmated. The datamodels can be implemented in any\nmachine learning model which targets at predicting\nthe influence of each datapoint on the performance\nof the trained model (Koh and Liang, 2017; Jain\net al., 2022; Liu et al., 2024b; Picard et al., 2024;\nBae et al., 2024; Covert et al., 2024).\n(Engstrom et al., 2024) proposes to use data-\nmodels to select subsets that maximize the overall\nperformance. Specifically, it chooses the subset\nSb \u2282S, S = {x1, x2, ..., x|S|} by estimating the\nloss of the model trained on it. Out of simplicity,\nthe datamodel \u03c4\u03b8x can be implemented as a linear\nmodel and it learns to approximate the actual loss\nvia the TARK estimator (Park et al., 2023):\n\u03b8xj = arg min\n\u03b8\n\u02c6\nE(m)\nSi\u223cSb\u2282S[Lreg(\u03c4\u03b8(1Si)), Lxj(Si)],\n1Sb \u2208{0, 1}|S|, (1Sb)i =\n(\n1,\nif xi \u2208Sb,\n0,\notherwise. ,\n\u03c4\u03b8x(1Sb) = \u03b8T\nx 1Sb,\n(44)\nwhere Lxj(Si) denotes the loss of the model\n(trained on Si) on the sample xj. The \u02c6\nE(m) is\na m-sample empirical expectation and Lreg(\u00b7, \u00b7) is\na regression loss function (e.g., mean squared er-\nror). Intuitively, what the datamodel \u03c4\u03b8x does is\nto approximate the real loss Lxj(Si) under vari-\nous compositions of subset Si \u223cSb. Given any\nsubset Sb, the averaged loss approximated by the\ndatamodel on all xj \u2208Seval is calculated on the\nevaluation set Seval and minimized to find the opti-\nmal Sb, |Sb| = b:\nS\u2217\nb = arg min\nSb\u2282S\n\u02c6\nLSeval(Sb),\n\u02c6\nLSeval(Sb) = \u02c6\nE(n)\nxj\u223cSeval[\u03c4\u03b8xj (1Sb)]\n=\n1\n|Seval|\nX\nxj\u2208Seval\n\u03b8T\nxj1Sb\n= 1T\nSb(\n1\n|Seval|\nX\nxj\u2208Seval\n\u03b8xj).\n(45)\nThe importance of xi \u2208S is therefore measured\nby\n1\n|Seval|\nP\nxj\u2208Seval \u03b8xj and its smallest b elements\nare chosen for the minimum loss \u02c6\nLSeval.\n(Liu et al., 2024b) also proposes a simulence-\nbased (Guu et al., 2023) linear datamodel that cor-\nrelates the training samples with the validation or\ntest set loss. A featurized simulator, namely GPT-\nfluence, models the training dynamics (e.g., loss,\nBLEU and ROUGE scores) across time via an n-th\norder Markov process. It extracts representations\ng(xi), xi \u2208S from BERT or GPT, and generates\nboth multiplicative and additive factors to reflect\nthe influence of any training example on the test-\ning set. The testing performance \u03d5t at any time t\nis affected by: 1) its performance at preceding n\ntimes and 2) the current training batch ct:\n\u03d5t(xk) =\nn\nX\nj=1\n\u03b1j(ct)\u03d5t\u2212j(xk) + \u03b2(ct), \u2200xk \u2208Seval,\n\u03b1j(ct) =\n|ct|\nX\ni=1\nAi,j, \u03b2(ct) =\n|ct|\nX\ni=1\nBi, \u2200xi \u2208ct \u2282S,\nAij = \u27e8WT\n(j)g(xi)j, UT\n(j)g(xk)\u27e9F ,\nBi = \u27e8W\u2032T g(xi)j, U\u2032g(xk)\u27e9F ,\n(46)\nwhere WT\n(j), UT\n(j), W\u2032, U\u2032 are learnable weights\nwhich are optimized by minimizing PT\nt=1(yt \u2212\n\u03d5t(xk))2 with yt being the ground-truth metric\nscore monitored during training at step t. The\n\u27e8\u00b7, \u00b7\u27e9F denotes the Frobenius inner product. Based\non the datamodel, samples that reduce evaluation\nloss the most are selected as influential data.\nInstead of performing off-line data selection, (Yu\net al., 2024) proposes MATES where a small data-\nmodel continuously selects the most effective sub-\nset for the current training of the LLM. The data-\nmodel is updated alternatively, like a partner, to\nadapt to the constantly changing data preferences\nof the model under development.\nUnlike previous datamodels that predict the in-\nfluence of datapoints on the testing performance\nof the model, (Xie et al., 2023) proposes the DSIR\nwith importance scores estimated by the distribu-\ntional resemblance. It simply assumes that training\nsamples that resemble the evaluation set are impor-\ntant, and these datapoints should be selected with\nhigher probability. Given the hashed n-grams fea-\ntures h(xi) \u2208Nm of xi, its importance score wi is\ncalculated as:\nwi =\n\u02c6\nwi\nP|S|\ni=1 \u02c6\nwi\n,\n\u02c6\nwi = \u02c6\npfeat(h(xi))\n\u02c6\nqfeat(h(xi)),\n\u02c6\npfeat(h(xi)) =\nm\nY\nj=1\n\u03b3h(xi)j\nj\n,\n\u02c6\nqfeat(h(xi)) =\nm\nY\nj=1\n\u03b2h(xi)j\nj\n,\n\u02c6\n\u03b3 =\n1\nP\nxi\u2208Seval 1T h(xi)\nX\nxj\u2208Seval\nh(xj),\n\u02c6\n\u03b2 =\n1\nP\nxi\u2208S 1T h(xi)\nX\nxj\u2208S\nh(xj),\n(47)\nwhere S and Seval respectively denote the training\nset and the evaluation set. Given the budget b, the\nsubset Sb is obtained by importance-weighted sam-\npling without replacement b times. (Zhang et al.,\n2023b) also proposes to use a independent-cascade\ndiffusion model (Li et al., 2018; Du et al., 2014)\nto mimic the information diffusion process upon a\ndirected graph on embeddings of datapoints. The\nmost influential datapoint are selected for annota-\ntion and serve as in-context learning examples for\nLLMs.\nRemark\nCompared with uncertainty and reward\nscore, datamodel-based importance indicators are\nmore correlated with the downstream performance\nsince the task-specific evaluation set is introduced\nto provide feedback for the selection scheme.\n5.3\nLoss and Error-based Coreset Sampling\nOverview\nDuring training, samples that con-\ntribute more to the loss or cause worse performance\nare considered more important. Compared with\nthe datamodels, the influence of each datapoint is\nalso measured in the loss and error-based coreset\nsampling but differs in that such measurement is\nperformed with the same LLM under development\nrather than a specifically designed datamodel.\nTechnical Details\nOne kind of methods that\nrecord the errors of each sample during training\nto estimate importance is forgetting score or for-\ngetting event (Toneva et al., 2018). It counts how\nmany times the forgetting happens with the itera-\ntion of training step t. For any given sample xi\nin a batch B (xi \u2208B \u2282S), if the previous ac-\ncuracy acct\u22121\ni\nsurpasses the current accuracy acct\ni\n(acct\ni > acct+1\ni\n), then the example xi undergoes\na forgetting event. Conversely, a learning event\noccurs if acct\ni < acct+1\ni\n. The number of forget-\nting events implies whether the sample is difficult\nand indispensable for training. An example xi is\ndefined as unforgettable if it satisfies:\nUnforgeti =\n\uf8f1\n\uf8f4\n\uf8f2\n\uf8f4\n\uf8f3\n1,\n\u2203t\u2217< \u221e, s.t. acct\ni < acct+1\ni\nand \u2200k \u2265t\u2217, acck\ni > acck\u22121\ni\n,\n0,\notherwise.\n(48)\nThe easy samples with Unforgeti = 1 can be\nsimply discarded and the important subset Sb =\n{xi|Unforgeti = 0, xi \u2208S} is selected for train-\ning. Recent studies on both pre-training and in-\nstruction tuning have investigated the effectiveness\nof using the forgetting score for efficient data prun-\ning (Sorscher et al., 2022; Paul et al., 2021; Zhang\net al., 2023a; Jin and Ren, 2024a; Maini et al.,\n2022).\nIn contrast to the term \u201cforgetting\", researchers\nintroduce the concept \u201cmemorization\" (Feldman,\n2020; Tirumala et al., 2022; Antoniades et al.,\n2024) for analysis on the generalization of deep\nmodels (Zhang et al., 2021). The memorization of\ntraining samples is necessary for reducing close-\nto-optimal generalization error especially when a\nlong-tailed disttribution is observed for the training\nset (Feldman, 2020). Specifically, the amount of la-\nbel memorization on the instruction-response pair\n(xi(<t), xi(\u2265t)) is defined as follows:\nMemoi =\n1\n|xi| \u2212t\n|xi|\nX\nj=t\n(P(xi(j)|xi(<j); \u03b8S)\u2212\nP(xi(j)|xi(<j); \u03b8S\\xi)),\n(49)\nwhere \u03b8S and \u03b8S\\xi respectively refer to the lan-\nguage model parameters optimized with the entire\nset with and without xi. Accordingly, the influ-\nence (Feldman and Zhang, 2020) of the sample xi\non other samples xk, xk \u0338= xi can be defined as:\nInflik =\n1\n|xk| \u2212t\n|xk|\nX\nj=t\n(P(xk(j)|xk(<j); \u03b8S)\u2212\nP(xk(j)|xk(<j); \u03b8S\\xi)),\n(50)\nwhere xk(<t) and xk(\u2265t)) respectively denote the\ninstruction and response part of xk. In practice,\nthe memorization and influence scores are approxi-\nmated via batch-wise sampling where N batches\nB1, B2, ..., BN are sampled from S with |Bi| = n.\nFor each batch Bi, a language model parameter-\nized as \u03b8Bi is trained to compute the memoriza-\ntion and influence scores of each sample xi. It is\nnoted that some batches contain xi and the others\ndo not. Therefore, the two probability terms in\nEqs. 49 and 50 are respectively averaged over mul-\ntiple probability outputs of the models trained on\nbatches with and without xi. (Sorscher et al., 2022)\nconfirms that memorization scores (Eq. 49) demon-\nstrate stronger performance on pruning the dataset\ninto a significantly smaller subset Sb than random\nsampling, EL2N (Eq. 10), and influence scores\n(Eq. 50). (Suzuki et al., 2023) and (Schoch et al.,\n2023) also follow (Feldman and Zhang, 2020) to\nselect the high-quality influential subset for LLM\ntraining.\nFurthermore, (Chen et al., 2024b) uses the evalu-\nation loss to check whether the current task requires\ncertain skills or capabilities that can be obtained by\nlearning from the prerequisite tasks. For each task,\nit selects the skill-dependent datapoints that reduce\nevaluation loss. (Mishra and Sachdeva, 2020) pro-\nposes a rather simple method that adopts a proxy\nmodel (e.g., logistic regression and SVM) to train\non the randomly selected subset Sb and evaluate\non the remaining set S\\Sb. Such process iterates\nover multiple times to ensure that each sample is\nat least validated once. The probability of each\nsample being correctly predicted is used as impor-\ntance measurement. Likewise, (Lin et al., 2022)\nalso quantifies the average marginal effect (AME)\nas influence of xi. It can be viewed as a variant of\nshapley value (Jia et al., 2019; Ghorbani and Zou,\n2019; Schoch et al., 2023; Kwon and Zou, 2021).\nDifferent subsets are randomly sampled to train\nmultiple submodels and each submodel is evalu-\nated for jointly estimating the AME via LASSO\nregression (Lecu\u00e9 and Mendelson, 2018).\nRemark\nThe loss and error-based selection meth-\nods are intuitive and effective to select the data-\npoints with high difficulty and influence. To ac-\ncelerate the computation of marginal effect (gain)\nof each datapoint, iterative approximations can be\nadopted with small proxy models.\n5.4\nGradient-based Coreset Sampling\nOverview\nSince gradients directly affect the opti-\nmization of language models, two kinds of intuitive\nmethods for data selection are presented: 1) gradi-\nent matching (Zhao et al., 2020a; Killamsetty et al.,\n2021a; Jiang et al., 2023d; Zhao and Bilen, 2023;\nDu et al., 2024; Balles et al., 2022; Zhang et al.,\n2024a), i.e., the gradients of the entire set S being\napproximated by the weighted gradients of the sub-\nset Sb, and 2) gradient-based influence (Pruthi et al.,\n2020; Brophy et al., 2023; Koh and Liang, 2017;\nBasu et al., 2020; Picard et al., 2024; Alaa and Van\nDer Schaar, 2020), i.e., the influence of each sam-\nple xi on a testing datapoint xt being measured by\nupweighted gradient multiplication. Specifically,\nthe gradient matching aims to minimize the differ-\nence below:\n\u03b8\u2217, S\u2217\nb = arg min\n\u03b8,S d( 1\n|S|\nX\nxi\u2208S\n\u2207\u03b8NLLA|Q\ni\n,\n1\nP\ni wi\nX\nxi\u2208Sb\nwi\u2207\u03b8NLLA|Q\ni\n), Sb \u2208S, wi > 0,\n(51)\nwhere d(\u00b7, \u00b7) denotes the distance measurement and\nwi is the weight for the gradient of xi.\nThe gradient-based influence methods, on the\nother hand, aim at selecting the most influential\ndatapoints in terms of the variation of model pa-\nrameters \u03b8. Given the optimal parameters \u03b8\u2217, the\nupdated parameters \u03b8\u03f5\n{xi} by up-weighting the loss\nof xi with \u03f5 can be derived as the first-order Taylor\nseries expansion as follows:\n\u03b8\u03f5\nxi = arg min\n\u03b8\n1\n|S|\nX\nxj\u2208S\nNLLA|Q\nj\n+ \u03f5NLLA|Q\ni\n,\n\u03b8\u03f5\nxi \u2248\u03b8\u2217\u2212\u03f5H\u22121\n\u03b8\u2217\u2207\u03b8NLLA|Q\ni\n,\n(52)\nwhere H\u03b8\u2217represents the Hessian with respect to\nthe model parameters \u03b8\u2217. Accordingly, the influ-\nence function of a sample xi on the model parame-\nters and its effect on the performance of a particular\nsample xj can be respectively denoted as:\nInflFi = d\u03b8\u03f5\nxi\nd\u03f5 |\u03f5=0 = \u2212H\u22121\n\u03b8\u2217\u2207\u03b8NLLA|Q\ni\n,\nInflFij = \u2212\u2207\u03b8NLLA|Q\nj\nT H\u22121\n\u03b8\u2217\u2207\u03b8NLLA|Q\ni\n.\n(53)\nThe importance indicator InflFij approximately\nmeasures the change of the loss on xj when xi\nis removed from the training set. To expedite the\ncomputation of Hessian matrix for large models,\na combination of Hessian-vector product and opti-\nmization techniques are developed (Pearlmutter,\n1994; Nilsen et al., 2019; Mathieu and LeCun,\n2014; Agarwal et al., 2016; Shewchuk et al., 1994).\nAnother kind of influence score is defined as the\nexpected gradient norm (GraNd score) (Paul et al.,\n2021; Kirsch, 2023; B\u00f6ther et al., 2023), where the\nGraNd score controls the contribution of a training\nsample to the change of the loss.\nGraNdi = E\u03b8\u2225\u2207\u03b8NLLA|Q\ni\n\u22252\n(54)\nExperiments (Paul et al., 2021) suggest that the\nGraNd score (Eq. 54) can be well approximated by\nEL2N score (Eq. 10) for efficient data pruning.\nTechnical Details\n(Xia et al., 2024a) proposes to\nfind the most influential training data that resem-\nble the testing set the most via low-rank gradient\nsimilarity search. (Tan et al., 2024a) introduces the\nmoving-one-sample-out (MoSo) by pinpointing the\nleast informative samples via gradient-based influ-\nence assessment. To avoid the costly retraining\nprocedure by iteratively moving one sample out, a\ngradient-based approximator is proposed to select\nsamples whose gradients are consistently aligned\nwith the average gradients of the entire training set\nFor the detailed definition of distance measure\nof Eq. 51, (Everaert and Potts, 2023) exploits the\nKL-divergence to measure the difference between\nthe selected subset and the testing set. Note that\nhere the objective is to approach the distribution\nof the testing set rather than the entire training\nset.\n(Killamsetty et al., 2021a) speeds up the\ngradient matching between the selected dataset\nand the validation set via an orthogonal match-\ning pursuit algorithm. (Lin et al., 2024) applies\ngradient-based influence scores on recommenda-\ntion datasets for effective LLM instruction tun-\ning. (Schioppa et al., 2021) chooses a different\nway (Arnoldi, 1951) to accelerate the computation\nof the inverse Hessian matrix in Eq. 52 and suc-\ncessfully scales up the influence scoring for LLMs\nwith several hundreds of millions of parameters.\n(Grosse et al., 2023) uses the influence functions\nto study the generalization properties of LLMs To\nscale up influence functions for LLMs up to 52 bil-\nlions, an approximation technique via Eigenvalue-\ncorrected Kronecker-Factored Approximate Curva-\nture (EK-FAC) (George et al., 2021) to efficiently\nfind the most influential samples to the pre-trained\nLLMs over maths and programming abilities, cross-\nlingual generalization, and role-playing behavior.\n(Zhao et al., 2021) condenses the datasets into small\ninformative synthetic samples where the gradients\nof the model on the synthetic data are matching\nthose on the real data of the entire training set.\nRemark\nThe gradient-based coreset sampling\ntechniques are highly dependent on the LLMs un-\nder development, where the gradients describe the\nmodel\u2019s inherent knowledge and uncertainty about\neach training sample. Despite the precision of\ngradient-based selection methods, it is noted that\napproximation is unavoidable for application on\nLLMs. The efficiency and accuracy of various ap-\nproximation techniques should be considered.\n6\nResults and Discussions\nIn this chapter, we classify different methods ac-\ncording to their different emphases, and then sum-\nmarize and present the experimental results. First,\nwe classify the methods according to their different\nemphases (quality, diversity, importance) and sum-\nmarize them in Tab. 1 as well as the datasets/data\nvolume used.\nQuality\nThe quality of data directly impacts the\neffectiveness of model training. Quality control\nmeasures include data scoring, quality assessment,\nand more. In Tab. 2, we have summarized the re-\nsults of different methods focusing on data quality.\nIn the table, we list the data used by different meth-\nods and the proportion/size of the data selected. It\ncan be seen that the method of selecting data based\non quality can match the results of training with\nfull data even when using less data, and is also su-\nperior to the results of randomly selecting part of\nthe data. In the table, WK stands for World Knowl-\nedge, CR stands for Commonsense Reasoning, LU\nstands for Language Understanding, SPS stands\nfor Symbolic Problem Solving, and RC stands for\nReading Comprehension.\nDiversity\nData engineers enhance the general-\nization ability of models by introducing diverse\ndatasets. This diversity may encompass data from\ndifferent sources, with varying features, and dis-\ntinct distributions. Research indicates that merely\nselecting datasets that are similar to downstream\ntasks is insufficient. Diversity is important for data\nselection, and this is reflected in the experiments.\nTab. 3 demonstrates the importance of diversity in\ndata selection. Compared to random selection and\nuniform selection of data, the scheme of selecting\ndata with diversity criteria is superior. In addi-\ntion, compared to only selecting high-quality data,\nthe criteria that combine quality and diversity can\nachieve better performance than simply selecting\nhigh-quality data.\nImportance\nIdentifying and utilizing key data\nthat significantly impacts model performance is\nMethods\nQuality\nDiversity\nImportance\nTraining Set\nTraining Set Size\nIFD (Li et al., 2023a)\n!\n%\n%\nAlpaca\n52K\nWizardLM\n70K\nLIFT (Xu et al., 2023b)\n!\n!\n%\nOpen-Platypus\n25K\nCodeAlpaca\n20K\nDQ (Zhou et al., 2023)\n%\n!\n%\nAlpaca\n52K\nPPL (Ankner et al., 2024)\n!\n%\n%\nThe Pile\nNA\nDolma\nNA\nInstructionMining (Cao et al., 2023)\n!\n%\n%\nOpenOrca\n50K\nDolly\n15K\nFL (Bhatt et al., 2024)\n!\n!\n%\nFLAN v2\n99K\nAlpagasus (Chen et al., 2023b)\n!\n%\n%\nAlpaca\n52K\nBSDetector (Chen and Mueller, 2024)\n!\n%\n%\nSQuAD-N\nNA\nEmails-N\nNA\nDROP-N\nNA\nDEITA (Liu et al., 2023b)\n!\n!\n%\nMixed(ShareGPT+UltraChat+WizardLM)\n206K\nAutoDS (Zhang et al., 2024c)\n!\n%\n%\nOpenWebMath\nNA\nQurator (Wettig et al., 2024)\n!\n%\n%\nQuRatedPajama\n260B tokens\nClusterClip (Shao et al., 2024)\n%\n!\n%\nOpenOrca\n4.2M\nProof-Pile-2\n2.7M\nQDIT (Bukharin and Zhao, 2023)\n!\n!\n%\nUltraChat\n1.3M\nLMSYS\n1M\nAlpaca\n52K\nMixed (Alpaca+OIG+Dolly)\n270K\nDolly\n15K\nDsDm (Engstrom et al., 2024)\n%\n%\n!\nC4\nNA\nMATES (Yu et al., 2024)\n%\n%\n!\nC4\nNA\nDSIR (Xie et al., 2023)\n%\n%\n!\nThe Pile\n1.6B\nSkill-it (Chen et al., 2024b)\n%\n%\n!\nRedPajama\n1.2T tokens\nLESS (Xia et al., 2024a)\n%\n%\n!\nMixed(FLAN v2+Dolly+OpenAssistant+COT)\n270K\nTable 1: Statistics of datasets in existing representative data assessment and selection methods.\ncrucial. As shown in Tab. 4, the importance-based\ndata selection approach combines data selection\nand model training, aiming to maximize the final\nperformance. It addresses the challenges in the im-\nplementation framework by using point-wise data\nimpact and data impact parameterization. More-\nover, by performing importance resampling in the\nfeature space that provides structure, it selects ex-\namples similar to the target distribution, thereby\nenhancing the performance of the target task. Ex-\nisting work has found that the importance sampling-\nbased approach can effectively improve the perfor-\nmance of the target task and enhance the model\u2019s\ncapabilities.\n7\nFuture Directions: Challenges and\nOpportunities\nIn this section, we present the existing challenges\nand potential solutions to developing advanced data\nassessment and selection methods.\n7.1\nBenchmarking Instruction-Tuned LLMs\nThere exists a gap between the effectiveness of\ndata selection and the reported performance on\nbenchmarks.\nIn existing researches, the ablation\nstudies on the effectiveness of assessment and selec-\ntion methods are often carried out by comparing the\nperformance of LLMs fine-tuned with the selected\nand the full dataset. However, for coreset sampling\nmethods that use losses and gradients as proxies\nfor data quality, the downstream performance may\nnot be positively correlated with the selection effec-\ntiveness. The reason behind is that the evaluation\nloss itself (Yang et al., 2022; Hoffmann et al., 2022;\nKaplan et al., 2020) is not informative enough for\nuniversal estimation of benchmark performance.\n(AI@Meta, 2024) demonstrates that the correlation\nbetween the negative log-likelihood loss on down-\nstream tasks and the accuracy metrics should be\nmodeled task-by-task and model-by-model. In the\nlight of this statement, it is impractical to simply\ncount on losses or gradients to pinpoint the most\nbeneficial data for improving the downstream per-\nformance, let alone methods that try to predict the\nloss based on various indicators (Cao et al., 2023).\nFurthermore, even if the metrics are exhaustively\ncomputed for the selection of each sample, the\ngains brought by one sample might be limited in\nMethods\nTraining Set\nModel\nSelection Ratio/Size\nReported Results on Testing Sets\nIFD\n(Li et al., 2023a)\nARC\nHellaSwag\nMMLU\nTruthfulQA\nAlpacaEval\nAlpaca\nLLaMA-7B\nFull\n0.427\n0.769\n0.417\n0.396\n0.265\n5%\n0.539\n0.795\n0.365\n0.383\n0.347\nWizardLM\nFull\n0.531\n0.774\n0.378\n0.429\n0.620\n10%\n0.529\n0.790\n0.331\n0.414\n0.614\nAlpaca\nLLaMA2-7B\nFull\n0.544\n0.787\n0.470\n0.410\n0.278\n5%\n0.558\n0.579\n0.804\n0.442\n0.368\n10%\n0.580\n0.804\n0.466\n0.402\nNA\n15%\n0.564\n0.574\n0.807\n0.464\nNA\nWizardLM\nFull\n0.576\n0.820\n0.541\n0.415\n0.350\n5%\n0.624\n0.840\n0.557\n0.428\n0.468\n10%\n0.630\n0.839\n0.553\n0.419\nNA\n15%\n0.624\n0.835\n0.556\n0.434\nNA\nARC\nHellaSwag\nMMLU\nTruthfulQA\nLIFT\n(Xu et al., 2023b)\nOpen-\nMistral-7B\nRandom 15K\n0.607\n0.820\n0.625\n0.438\nPlatypus\nLIFT 15K\n0.643\n0.844\n0.645\n0.490\nHumanEval\nMBPP\nCode-\nStarCoder-15B\nRandom 10K\n0.381\n0.431\nAlpaca\nLIFT 10K\n0.550\n0.495\nWK\nCR\nLU\nSPS\nRC\nPPL\n(Ankner et al., 2024)\nThe Pile\nMPT-1B\nFull\n0.155\n0.103\n0.281\n0.035\n0.112\nLow 50%\n0.111\n0.058\n0.187\n0.035\n0.087\nMid 50%\n0.161\n0.090\n0.281\n0.034\n0.109\nHigh 50%\n0.182\n0.128\n0.332\n0.034\n0.106\nDolma\nFull\n0.165\n0.123\n0.289\n0.036\n0.080\nLow 50%\n0.161\n0.101\n0.273\n0.345\n0.079\nMid 50%\n0.180\n0.130\n0.319\n0.034\n0.104\nHigh 50%\n0.167\n0.131\n0.311\n0.032\n0.086\nARC\nHellaSwag\nMMLU\nTruthfulQA\nInstructionMining\n(Cao et al., 2023)\nOpenOrca &\nDolly\nLLaMA2-7B\nSelseted 10K\n0.567\n0.798\n0.499\n0.483\nSelected 40K\n0.544\n0.801\n0.526\n0.498\nRandom 10K\n0.548\n0.796\n0.490\n0.516\nRandom 40K\n0.548\n0.799\n0.512\n0.500\nMMLU\nBBH\nFL\n(Bhatt et al., 2024)\nFLAN v2\nLLaMA2-7B\nRandom 20K\n0.443\n0.390\nFL 20K\n0.451\n0.383\nRandom 30K\n0.449\n0.394\nFL 30K\n0.471\n0.411\nRandom 45K\n0.460\n0.394\nFL 45K\n0.476\n0.413\nBBH\nDROP\nHumanEval\nMMLU\nAlpagasus\n(Chen et al., 2023b)\nAlpaca\nLLaMA2-7B\nRandom 9K\n0.319\n0.259\n0.116\n0.369\nFull 52K\n0.330\n0.259\n0.117\n0.409\nAlpagasus 9K\n0.338\n0.260\n0.122\n0.388\nLLaMA2-13B\nRandom 9K\n0.386\n0.334\n0.152\n0.450\nFull 52k\n0.387\n0.338\n0.157\n0.479\nAlpagasus 9K\n0.389\n0.344\n0.159\n0.461\nSQuQA-N\nEmails-N\nDROP-N\nBSDetector\n(Chen and Mueller, 2024)\nSQuAD-N\nLLaMA2-7B-\nChat\nFull\n0.499\nNA\nNA\nAuto-filter\n0.599\nNA\nNA\nAuto-correct\n0.714\nNA\nNA\nEmails-N\nFull\nNA\n0.507\nNA\nAuto-filter\nNA\n0.497\nNA\nAuto-correct\nNA\n0.523\nNA\nDROP-N\nFull\nNA\nNA\n0.447\nAuto-filter\nNA\nNA\n0.474\nAuto-correct\nNA\nNA\n0.505\nMATH\nGSM8K\nBBH\nARC-E\nARC-C\nAutoDS\n(Zhang et al., 2024c)\nOpenWebMath\nMistral-7B\nRandom 2.5B tokens\n0.143\n0.441\n0.565\n0.842\n0.567\nAutoDS 2.5B tokens\n0.161\n0.454\n0.586\n0.842\n0.552\nLogiQ\nBoolQ\nNQ\nMMLU\nHellaSwag\nRandom 2.5B tokens\n0.310\n0.838\n0.292\n0.522\n0.622\nAutoDS 2.5B tokens\n0.310\n0.831\n0.291\n0.523\n0.627\nPIQA\nWinogrande\nSciQ\nRandom 2.5B tokens\n0.822\n0.802\n0.972\nAutoDS 2.5B tokens\n0.822\n0.800\n0.968\nRC\nCR\nWK\nQuRator\nQuRated-\nSheared-\nRandom 30B tokens\n0.509\n0.55\n0.149\n(Wettig et al., 2024)\nPajama\nLLaMA-1.3B\nQurator 30B tokens\n0.521\n0.555\n0.152\nTable 2: Experimental results of quality-based selection methods. Results are directly cited from their papers. WK,\nCR, LU, SPS, and RC respectively stand for compound datasets of World Knowledge, Commonsense Reasoning,\nLanguage Understanding, Symbolic Problem Solving, and Reading Comprehension.\nMethods\nTraining Set\nModel\nSelection Ratio/Size\nReported Results on Testing Sets\nARC\nHellaSwag\nMMLU\nTruthfulQA\nDEITA\n(Liu et al., 2023b)\nMixed\nLLaMA-13B\nRandom 10K\n0.558\n0.800\n0.474\n0.574\nDEITA 10K\n0.595\n0.820\n0.606\n0.550\nLLaMA2-13B\nRandom 10K\n0.615\n0.837\n0.552\n0.448\nDEITA 10K\n0.589\n0.821\n0.553\n0.546\nMistral-7B\nRandom 10K\n0.554\n0.792\n0.587\n0.536\nDEITA 6K\n0.578\n0.803\n0.619\n0.598\nSuperGLUE\nGSM8k\nOBQA\nMT-Bench\nClusterClip\n(Shao et al., 2024)\nOpenOrca\nMistral-7B\nRandom 5B tokens\n0.621\n0.615\n0.798\n6.600\nUniform 5B tokens\n0.630\n0.588\n0.782\n6.750\nClusterClip\n0.643\n0.587\n0.814\n6.900\nMATH\nGSM8K\nMMLU\nBBH\nProof-Pile-2\nLLaMA2-7B\nRandom 20B tokens\n0.065\n0.256\n0.488\n0.418\nUniform 20B tokens\n0.076\n0.260\n0.500\n0.429\nClusterClip\n0.079\n0.248\n0.511\n0.428\nMMLU\nBBH\nARC\nQDIT\n(Bukharin and Zhao, 2023)\nUltraChat\nLLaMA-7B\nRandom 10K\n0.321\n0.332\n0.583\nQDIT 10K\n0.361\n0.321\n0.607\nLMSYS\nRandom 10K\n0.331\n0.326\n0.602\nQDIT 10K\n0.373\n0.325\n0.614\nAlpaca\nRandom 3K\n0.362\n0.303\n0.617\nQDIT 3K\n0.355\n0.304\n0.620\nMixed\nRandom 10K\n0.329\n0.309\n0.583\nQDIT 10K\n0.343\n0.312\n0.607\nDolly\nRandom 1K\n0.281\n0.273\n0.594\nQDIT 1K\n0.338\n0.303\n0.598\nDROP\nLAMBADA\nSciQ\nUltraChat\nRandom 10K\n0.262\n0.698\n0.854\nQDIT 10K\n0.267\n0.698\n0.868\nLMSYS\nRandom 10K\n0.251\n0.685\n0.867\nQDIT 10K\n0.264\n0.693\n0.850\nAlpaca\nRandom 3K\n0.263\n0.716\n0.870\nQDIT 3K\n0.270\n0.697\n0.841\nMixed\nRandom 10K\n0.203\n0.681\n0.841\nQDIT 10K\n0.260\n0.697\n0.898\nDolly\nRandom 1K\n0.173\n0.717\n0.807\nQDIT 1K\n0.226\n0.723\n0.806\nBBH\nDROP\nMMLU\nHumanEval\nDQ\n(Zhou et al., 2023)\nAlpaca\nLLaMA-7B\nFull\n0.329\n0.263\n0.416\n0.100\n20%\n0.327\n0.267\n0.398\n0.092\n2%\n0.329\n0.276\n0.366\n0.085\nTable 3: Experimental results of diversity-based selection methods. Results are directly cited from their papers.\nfew tasks. Therefore, to comprehensively reflect\nthe effectiveness of sample selection, the evalua-\ntion of instruction-tuned models should be accom-\npanied by the specialised evaluation of the selected\ndatapoints. For the former, all sorts of evaluation\nstrategies have been proposed to precisely evaluate\nthe LLMs (Melis et al., 2017; Chang et al., 2024;\nXu et al., 2022; Liang et al., 2022). The multiple-\nchoice QA tasks are not enlightening in judging if\nthe instruction-tuned model truly understands the\nproblem rather than simply memorizing the answer\nchoices given the instruction context. For the later,\na benchmark for documenting and comparing the\nstatistics of the selected instruction-response pairs\nin terms of quality, diversity, and importance needs\nto be constructed in the future. It would benefit the\ntask-wise customized data selection according to\nthe statistical indicators on such a benchmark.\nTest set contamination should be considered dur-\ning instruction data selection.\nFor instruction\ntuning on publicly released pre-trained LLMs, it\ncannot be too careful to check the potential data\nleakage where the testing instructions are already\nmodeled during pre-training (Rae et al., 2021; Li\net al., 2023b; Magar and Schwartz, 2022; Carlini\net al., 2019; Marone and Van Durme, 2024; Deng\net al., 2023; Cao et al., 2024; Jiang et al., 2024b;\nMagar and Schwartz, 2022). To improve the perfor-\nmance of pre-trained models on downstream tasks,\ninstruction datapoints (i.e., instruction-like conver-\nsations) are already added into the annealing phase\nof pre-training (AI@Meta, 2024; Bilibili, 2024;\nYang et al., 2024; Bai et al., 2023). Therefore, po-\ntential risks of data contamination are raised for\nbenchmarking the fine-tuned LLMs. To avoid the\nnegative effect of data leakage on evaluation of the\ndata assessment and selection, it is encouraged to\nfollow (Li et al., 2023a) to adopt the pre-trained\nmodel for experiencing the instruction datapoints\nbefore fine-tuning. If the model exhibits overfit-\nting behaviors (i.e., accurately generating the in-\nstruction part or producing the same answer choice\neven with permutation on the choice letters), data\ncontamination is likely to exist and thereafter the\ntesting set should be replaced. For future studies,\nMethods\nTraining Set\nModel\nSelection Ratio/Size\nReported Results on Testing Set\nCOPA\nOBQA\nPIQA\nCBT\nHellaswag\nDsDm\n(Engstrom et al., 2024)\nC4\nChinchilla-\noptimal-1.3B\nRandom\n0.620\n0.334\n0.689\n0.864\n0.449\nDsDm\n0.630\n0.312\n0.690\n0.882\n0.423\nWinogrande\nBoolQ\nCOQA\nARC-E\nTriviaQA\nRandom\n0.522\n0.549\n0.188\n0.448\n0.037\nDsDm\n0.511\n0.580\n0.255\n0.476\n0.071\nSciQ\nARC-E\nARC-C\nLogiQA\nMATES\n(Yu et al., 2024)\nC4\nPythia-410M\nRandom 20%\n0.641\n0.402\n0.256\n0.247\nMATES 20%\n0.660\n0.418\n0.250\n0.257\nPythia-1B\nRandom 20%\n0.658\n0.437\n0.256\n0.275\nMATES 20%\n0.673\n0.449\n0.259\n0.287\nOBQA\nBoolQ\nHellaSwag\nPIQA\nWinogrande\nPythia-410M\nRandom 20%\n0.294\n0.589\n0.397\n0.671\n0.506\nMATES 20%\n0.308\n0.606\n0.410\n0.687\n0.527\nPythia-1B\nRandom 20%\n0.318\n0.602\n0.438\n0.689\n0.507\nMATES 20%\n0.322\n0.609\n0.453\n0.695\n0.524\nMNLI\nQNLI\nQQP\nRTE\nDSIR\n(Xie et al., 2023)\nThe Pile\nRoBERTa-\nBase (125M)\nRandom 51.2M\n0.826\n0.869\n0.896\n0.674\nDSIR 51.2M\n0.831\n0.891\n0.898\n0.751\nSST-2\nMRPC\nCoLA\nSTS-B\nRandom 51.2M\n0.901\n0.874\n0.494\n0.886\nDSIR 51.2M\n0.905\n0.877\n0.540\n0.892\nARC-C\nARC-E\nBoolQ\nCOPA\nSkill-it\n(Chen et al., 2024b)\nRedPajama\nGPT-Neo-3B\nSkill-it 1B\n0.346\n0.612\n0.682\n0.820\nUniform 1B\n0.354\n0.652\n0.689\n0.810\nSkill-it 1B\n0.349\n0.617\n0.686\n0.810\nUniform 1B\n0.353\n0.624\n0.677\n0.800\nSkill-it 1B\n0.348\n0.620\n0.687\n0.810\nUniform 1B\n0.346\n0.625\n0.672\n0.810\nHellaSwag\nLAMBADA\nPIQA\nWinogrande\nSkill-it 1B\n0.637\n0.670\n0.750\n0.639\nUniform 1B\n0.639\n0.644\n0.748\n0.628\nSkill-it 1B\n0.639\n0.667\n0.752\n0.632\nUniform 1B\n0.638\n0.659\n0.755\n0.639\nSkill-it 1B\n0.639\n0.660\n0.757\n0.631\nUniform 1B\n0.640\n0.668\n0.750\n0.634\nMMLU\nTYDIQA\nBBH\nLESS\n(Xia et al., 2024a)\nMixed\nLLaMA2-7B\nFull\n0.516\n0.540\n0.432\nRandom 5%\n0.465\n0.527\n0.389\nLESS 5%\n0.502\n0.562\n0.415\nLLaMA2-13B\nFull\n0.545\n0.543\n0.508\nRandom 5%\n0.534\n0.530\n0.470\nLESS 5%\n0.540\n0.546\n0.506\nMistral-7B\nFull\n0.604\n0.577\n0.530\nRandom 5%\n0.600\n0.569\n0.545\nLESS 5%\n0.618\n0.603\n0.560\nTable 4: Experimental results of importance-based selection methods. Results are directly cited from their papers.\nit would be more reliable to decouple the evalua-\ntion of data selection and that of fine-tuned LLMs,\nwhere the performance consistency between these\ntwo evaluation results can be analyzed to rule out\nthe possibility of contamination.\n7.2\nUnveiling the Definitions of Good Data\nWhat signifies the most a good instruction data-\npoint remains an open question.\nUnfortunately,\nthere exists no unified criteria on discriminating\n\u201cgood\" instructions from \u201cbad\" ones. Essentially,\nthe definitions on the general data \u201cquality\" differ\nfrom task to task and domain to domain (Evans\nand Murshudov, 2013; Flach, 2012; Albalak et al.,\n2024). Although existing quality measurement\nmethods can be categorized in terms of quality,\ndiversity, and importance under the present study,\nthey all exhibit more or less ad-hoc properties in\nmethodology. First, studies on instruction tuning\nare often targeted at improving the performance\nof LLMs on downstream task. No matter whether\nthese tasks are of general-purpose (e.g., common\nNLP tasks on leaderboard (Myrzakhan et al., 2024;\nWolf et al., 2019)) or domain-specific applications,\nsuch task-orientated data selection itself is only a\n\u201cproxy\" for exploring the underlying \u201cquality\" mea-\nsurement. Especially for coreset sampling methods\nthat directly employ the evaluation set or testing set\nfor distribution matching or importance estimation,\ninstructions that resemble the most to the testing\nset or bring about performance gains are judged as\n\u201cgood\" data. However, such \u201cgood\" data cannot be\neasily transferred to another LLM of completely\ndifferent architecture and parameters. Each time\nthe entire pipeline has to be enforced for a novel\ntask, making it difficult to accumulate universally-\nacknowledged high-quality data for archiving. Sec-\nond, each method has an individual quality eval-\nuation system and very few of them ever tried to\njustify their design and interpret the philosophy be-\nhind. It is difficult to validate whether certain com-\nponent of the selection pipeline can be replaced or\nremoved for better serving a new task-of-interest.\nAccordingly, further academic explorations in-\nclude: 1) to present a more unified, generally appli-\ncable definitions on \u201cgood\" instruction datapoints\nin terms of fine-grained aspects, and 2) to improve\ninterpretability and explanability of the selection\npipeline beyond empirical design.\nThe expected model behavior retrospectively de-\ntermines the trade-off between quality, diversity,\nand importance for data selection.\nThe three\naspects we used to categorize data assessment meth-\nods are actually overlapping with each other, where\nthe \u201cboundary\" between two measuring dimension\nis often hard to explicitly defined. Under such\ncircumstance, the definition of good data can be\nperceived as the weighted, biased mixture of qual-\nity, diversity, and importance. Existing methods\nare not flexible in dynamically adjusting the mix-\ning weights to adapt to different downstream tasks.\nInstead, their priority order of the three dimensions\nis implicitly encoded into the selection of instruc-\ntions. For instance, (Liu et al., 2023b) emphasizes\nquality and importance equally by first establishing\nthe relative ranking of all samples in both quality\nand complexity. The subset is formed by consecu-\ntively selecting the top-ranked samples in sequence,\nwith diversity intervened via ruling out heavily ho-\nmogenized examples. Such hard-coded, greedy\ntreatment to quality, diversity, and importance is\nnot applicable to scenarios where the behavior of\nLLMs is expected to cater to varied preference.\nIn general, the data assessment and selection\nmethods that can adapt to the model requirement\nunder different application scenarios are yet to be\nsystematically developed. For generation tasks like\nrole playing and creative writing, the preferred in-\nstruction tuning datapoints should be distinct from\nthose for discriminative tasks like named entity\nrecognition and sentiment analysis.\n7.3\nScaling Up Datasets\nThe optimal scale of the selected subset becomes\nless explicit with the expansion of datasets.\nIn\nthe analysis of the disadvantages in exploiting the\nentire instruction dataset for alignment, putting\naside the issue of long training time, we notice that\nthe performance of fine-tuning the entire dataset\nmight not be the optimal. There often exists a criti-\ncal point of the best selection proportion, and such\nproportion varies from dataset to dataset. When\nmore instruction datasets from diverse domains\nand tasks are incorporated, it becomes more dif-\nficult to nail down the best selection proportion\nfor three main reasons. First, a large proportion\nof noise exists in the open datasets and few noisy\nsamples can already cause tremendous negative im-\npact on performance (Song et al., 2022). During\nthe pre-processing of instruction dataset, noise can\nbe unintentionally introduced in instruction prepa-\nration (e.g., missing context or system prompt), re-\nsponse generation (e.g., unverified or mismatched\nanswers), format wrapping (e.g., invalid JSON and\nunresolved code), and text augmentation (e.g., syn-\nonym replacement and reorder of words). Second,\nfor specialized tasks in \u201cvertical\" domains, the over-\nfitting of specific prompts occurs (Ma et al., 2023)\nwhen the diversity of input instructions is rather\nlimited. Despite the accuracy and rationality of the\ninstruction-response pairs, LLMs tend to overfit\ncertain patterns of the input instruction rather than\ntruly comprehend the task. Therefore, the increase\nof dataset size instead reduces the generalization\nof trained LLMs with lower instruction following\ncapabilities. Third, the forgetting (Zhang and Wu,\n2024; Jin and Ren, 2024b; Wang et al., 2024b)\nbecomes a severe problem when more instruction\ndatasets are introduced without setting a proper\nre-playing schedule of pre-training or previously\nvisited instruction tuning datapoints (Parmar et al.,\n2024; Jin et al., 2021; Ibrahim et al., 2024). The\nskill cultivation of a LLM on any new instruction\ntask heavily relies on its preceding skills acquired\nduring pre-training or previous fine-tuning. Con-\nsequently, the expansion of samples for high-level\nskills would \u201cdilute\" those for low-level skills and\ndegrade performance.\nTo sum up, with the dataset scaling up, fine-\ntuning with the selected subset instead of the entire\nset becomes a must-have strategy. To help deter-\nmine the optimal selection ratio, we suggest the\nfollowing three guidelines: 1) One may first de-\nvelop a complex quality measure scheme that uses\nboth indicators and human verification to estimate\nthe noise percentage of each constituting dataset.\nWithout lose of generality, random sampling can\nbe performed to accelerate quality measurement.\nTo combat noise, a lower ratio (i.e., smaller Sb)\nshould be considered for data selection from the\nnoisy S. 2) To combat overfitting, both the di-\nversity of instruction datapoints within and across\ndatasets should be emphasized. A higher keep-\ning proportion should be established for datasets\nwith diverse instruction styles, prompt formations,\nand response patterns, which helps improve the\nmodel\u2019s instruction following capabilities. 3) For\ncontinual fine-tuning, datasets that share similar\ndistributions with pre-training and previous fine-\ntuning datapoints should be kept to fight against\nforgetting. The optimal selection ratio and propor-\ntion for each dataset is built upon the meticulous\nand thorough analysis on each instruction dataset,\nand therefore case-by-case adjustment is needed.\nFor future studies, one may investigate the automa-\ntion of assessment and selection recipe to minimize\nthe human intervention.\nThe optimization of a scalable pipeline for data\nassessment and selection is of urgent need.\nIn\nconsideration of the cost of building human-labeled\nand human-verified instruction tuning datasets,\nmethods that employ powerful LLMs like GPT4\nfor instruction synthesis (Bradley et al., 2023; Li\net al., 2023e; Xu et al., 2023a; Li et al., 2024a;\nZhao et al., 2024a; Dong et al., 2024) have gained\nincreasing attention. The synthetic instructions pro-\nliferate cost-effectively with fine-grained control of\ncharacteristics such as difficulty and style. There-\nfore, it is expected to witness a surge of instruction\ndatapoints (e.g., tens or even hundreds millions)\nin the short future. In that case, datasets of such\nquantity pose a significant challenge to the robust-\nness, efficiency, and precision of the selection meth-\nods. Previous studies like DSIM (Xie et al., 2023)\ndemonstrated that cheap approximation of features\nby bag-of-n-grams achieves similar performance\nbut requires much less computing resources. For\nfuture research, one may draw inspiration from the\ndata deduplication and filtering techniques in han-\ndling billions of pre-training tokens. Especially\nfor the measurement of diversity, the computing\nof embedding-based pairwise similarity and clus-\ntering can be greatly reduced with simplified rep-\nresentations. In addition, the hierarchical philoso-\nphy (Hmida et al., 2016; Talavera, 1999; Cabezas\net al., 2023; Ran et al., 2023) might be a promis-\ning approach to select data from coarse-grained to\nfine-grained structures. One may apply the devide-\nand-conquer strategy to recursively handle each\nsubset of the instruction dataset, limiting the peak\nresource consumption under budget.\n7.4\nScaling Up LLMs\nThe cost-efficiency of data assessment and se-\nlection diminishes with larger LLMs involved\nin the pipeline.\nThe model-based indicators and\ncoreset sampling methods often require the lan-\nguage model itself to be involved for computation\nof metrics (Li et al., 2023a), losses (Chen et al.,\n2024b), and gradients (Xia et al., 2024a). With\nthe increase of model sizes, it becomes more and\nmore cumbersome to implement the entire pipeline\nfor quality measurement and selection. To expe-\ndite the process, one important direction for future\nstudy is to develop proper efficient proxy mod-\nels. Small proxy models have been successfully\napplied in accelerated fine-tuning of language mod-\nels (Hoffmann et al., 2022; Liu et al., 2024a), filter-\ning datasets by perplexity (Ankner et al., 2024), in-\ntervention on retrieval-augmented generation (Tan\net al., 2024b), and performance prediction (Anu-\ngraha et al., 2024; Ngu et al., 2024). Such proxy\nmodels often share the same architecture design\nwith the LLMs under development but own much\nless parameters. The scaling law (Kaplan et al.,\n2020) confirms the expected consistent behavior\nbetween data quantity and model scale, providing\npractical guidelines on the development of such\nproxy LLMs.\nOn the other hand, under the context of data\nevaluation, it calls upon on rethinking of tradi-\ntional machine learning techniques such as effi-\ncient optimization tricks and dimensionality re-\nduction approaches. For example, in the assess-\nment of loss-based datapoint influence (Feldman\nand Zhang, 2020), the exhaustive measurement on\nthe marginal performance by moving-each-sample-\nout and model re-training can be simply approxi-\nmated by iterative batch-wise sampling tricks with\na greedy principle behind. For efficient assessment,\nPCA (Xu et al., 2023b) and random projection (Xia\net al., 2024a; Park et al., 2023) are popular choices\nfor obtaining low-rank representations of embed-\ndings and gradients, which facilitates not only met-\nric computation but also storing of datapoints.\nThe marginal benefits of instruction tuning\ndiminishes with increasing size of LLMs for\nknowledge supplement.\nRecent studies on the\neffectiveness of instruction tuning in injecting\ntask-specific or domain-specific knowledge into\nLLMs (Shi and Lipani, 2023; Goyal et al., 2023;\nZhang et al., 2024b; Y\u0131ld\u0131z et al., 2024) show that\nthe stand-alone instruction tuning might not be the\nmost appropriate method. Compared with strate-\ngies like continual pre-training (Cossu et al., 2024;\nKe, 2024; Cossu et al., 2024) and instruction mod-\neling (Lou et al., 2024; Cheng et al., 2024; Wang\net al., 2022; Xu et al., 2024; Shi et al., 2024),\ninstruction tuning counts the response sequences\nfor loss computation without sufficient perception\nof instruction context. For specialized domains\nlike medicine, finance, and laws, if the pre-trained\nLLMs are in lack of the prerequisite knowledge,\nthe instruction tuning cannot properly activates the\nparameterized \u201cmemory\" for alignment but only\ncauses overfitting of the given prompt. In that case,\nthe benefits of data selection are limited with poor\ngeneralizability.\nAnother noteworthy phenomenon in data assess-\nment and selection studies is that due to limited\nbudgets of computing resources, most of the exper-\niments are performed on LLMs of small and moder-\nate size (e.g., less than 7B) to validate the effective-\nness of the quality measurement and the selection\nstrategy. Small pre-trained LLMs, by their nature\nof small parameter size, are more sensitive to the\ninstruction datasets during fine-tuning or continual\nlearning (Schick and Sch\u00fctze, 2020; Y\u0131ld\u0131z et al.,\n2024). They exhibit the most significant rates of\nboth forggeting (old knowledge) and learning (new\nknowledge). In the light of such statement, small\nLLMs tend to sacrifice the task-irrelevant knowl-\nedge in return for rapid adaptation towards novel\ndomains and tasks. The selected datasets by vari-\nous quality measures can impose immediate effect\non the parameters of small LLMs, but may weaken\non those of huge ones. It remains unknown whether\nthe same quality measurement and data selection\npipeline can achieve similar performance gains on\nboth small and large LLMs. For future research\nof data assessment and selection, extensive exper-\niments are required to validate their efficiency on\nhuge LLMs (e.g., 70B and 405B) (AI@Meta, 2024)\nand LLMs of mixture-of-experts (MoE) architec-\ntures (e.g., Mixtral 8x22B) (Jiang et al., 2024a).\nIn consideration of the pre-training corpus, ex-\ntremely large LLMs already experienced a vast\namount of multi-lingual, multi-domain web texts\nduring pre-training, and therefore the priority of\nthe dimensions in data assessment (i.e., quality, di-\nversity, and importance) differs from small LLMs.\nThe association between the model scale and the\ndata selection criteria is yet to be studied.\n8\nConclusion\nIn this study, we have thoroughly examined the\nstate-of-the-art data assessment and selection meth-\nods for instruction tuning of LLMs. The present\nreview presents a unified organization and catego-\nrizes these methods in terms of measuring dimen-\nsionality: quality, diversity, and importance. In\neach dimensionality, we outline the representative\nstrategies in details and describe the factors to con-\nsider when selecting data for instruction tuning.\nFurthermore, we report the performance of typical\ndata selection methods and provide discussions on\nthe comparison between these methods. Last but\nnot least, the existing challenges and potential so-\nlutions for future studies are summarized in hope\nfor benefitting the research community.\nReferences\nAmro Abbas, Kushal Tirumala, D\u00e1niel Simig, Surya\nGanguli, and Ari S Morcos. 2023. Semdedup: Data-\nefficient learning at web-scale through semantic dedu-\nplication. arXiv preprint arXiv:2303.09540.\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama\nAhmad, Ilge Akkaya, Florencia Leoni Aleman,\nDiogo Almeida, Janko Altenschmidt, Sam Altman,\nShyamal Anadkat, et al. 2023. Gpt-4 technical report.\narXiv preprint arXiv:2303.08774.\nAlessandro Achille, Michael Lam, Rahul Tewari,\nAvinash Ravichandran, Subhransu Maji, Charless C\nFowlkes, Stefano Soatto, and Pietro Perona. 2019.\nTask2vec: Task embedding for meta-learning. In\nProceedings of the IEEE/CVF international confer-\nence on computer vision, pages 6430\u20136439.\nSandesh Adhikary and Byron Boots. 2022. Sampling\nover riemannian manifolds using kernel herding. In\n2022 International Conference on Robotics and Au-\ntomation (ICRA), pages 3646\u20133653. IEEE.\nNaman Agarwal, Brian Bullins, and Elad Hazan. 2016.\nSecond-order stochastic optimization in linear time.\nstat, 1050:15.\nSharat Agarwal, Himanshu Arora, Saket Anand, and\nChetan Arora. 2020. Contextual diversity for active\nlearning. In Computer Vision\u2013ECCV 2020: 16th\nEuropean Conference, Glasgow, UK, August 23\u201328,\n2020, Proceedings, Part XVI 16, pages 137\u2013153.\nSpringer.\nAI@Meta. 2024. Llama 3 model card.\nAhmed Alaa and Mihaela Van Der Schaar. 2020. Dis-\ncriminative jackknife: Quantifying uncertainty in\ndeep learning via higher-order influence functions.\nIn International Conference on Machine Learning,\npages 165\u2013174. PMLR.\nAlon Albalak, Yanai Elazar, Sang Michael Xie, Shayne\nLongpre, Nathan Lambert, Xinyi Wang, Niklas\nMuennighoff, Bairu Hou, Liangming Pan, Haewon\nJeong, et al. 2024. A survey on data selection for\nlanguage models. arXiv preprint arXiv:2402.16827.\nAlexandre Alcoforado,\nThomas Palmeira Ferraz,\nLucas Hideki Okamura, Israel Campos Fama,\nArnold Moya Lavado, B\u00e1rbara Dias Bueno, Bruno\nVeloso, and Anna Helena Reali Costa. 2024. From\nrandom to informed data selection: A diversity-based\napproach to optimize human annotation and few-shot\nlearning. arXiv preprint arXiv:2401.13229.\nZachary Ankner, Cody Blakeney, Kartik Sreenivasan,\nMax Marion, Matthew L Leavitt, and Mansheej Paul.\n2024. Perplexed by perplexity: Perplexity-based data\npruning with small reference models. arXiv preprint\narXiv:2405.20541.\nAntonis Antoniades, Xinyi Wang, Yanai Elazar, Al-\nfonso Amayuelas, Alon Albalak, Kexun Zhang,\nand William Yang Wang. 2024.\nGeneralization\nvs memorization: Tracing language models\u2019 capa-\nbilities back to pretraining data.\narXiv preprint\narXiv:2407.14985.\nDavid Anugraha, Genta Indra Winata, Chenyue Li,\nPatrick Amadeus Irawan, and En-Shiun Annie Lee.\n2024. Proxylm: Predicting language model perfor-\nmance on multilingual tasks via proxy models. arXiv\npreprint arXiv:2406.09334.\nWalter E. Arnoldi. 1951. The principle of minimized\niterations in the solution of the matrix eigenvalue\nproblem. Quarterly of Applied Mathematics, 9:17\u2013\n29.\nKyriakos Axiotis, Vincent Cohen-Addad, Monika Hen-\nzinger, Sammy Jerome, Vahab Mirrokni, David\nSaulpic, David Woodruff, and Michael Wunder. 2024.\nData-efficient learning via clustering-based sensitiv-\nity sampling: Foundation models and beyond. arXiv\npreprint arXiv:2402.17327.\nFrancis Bach and Michael Jordan. 2003. Learning spec-\ntral clustering. Advances in neural information pro-\ncessing systems, 16.\nJuhan Bae, Wu Lin, Jonathan Lorraine, and Roger\nGrosse. 2024.\nTraining data attribution via ap-\nproximate unrolled differentation. arXiv preprint\narXiv:2405.12186.\nPrafulla Bafna, Dhanya Pramod, and Anagha Vaidya.\n2016. Document clustering: Tf-idf approach. In\n2016 International Conference on Electrical, Elec-\ntronics, and Optimization Techniques (ICEEOT),\npages 61\u201366. IEEE.\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang,\nXiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei\nHuang, et al. 2023. Qwen technical report. arXiv\npreprint arXiv:2309.16609.\nMaria-Florina Balcan, Alina Beygelzimer, and John\nLangford. 2006. Agnostic active learning. In Pro-\nceedings of the 23rd international conference on Ma-\nchine learning, pages 65\u201372.\nLukas Balles, Giovanni Zappella, and C\u00e9dric Ar-\nchambeau. 2022. Gradient-matching coresets for\nrehearsal-based continual learning. arXiv preprint\narXiv:2203.14544.\nSayan Bandyapadhyay and Kasturi Varadarajan. 2015.\nOn variants of k-means clustering. arXiv preprint\narXiv:1512.02985.\nSamyadeep Basu, Philip Pope, and Soheil Feizi. 2020.\nInfluence functions in deep learning are fragile. arXiv\npreprint arXiv:2006.14651.\nCarlo Batini, Cinzia Cappiello, Chiara Francalanci, and\nAndrea Maurino. 2009. Methodologies for data qual-\nity assessment and improvement. ACM computing\nsurveys (CSUR), 41(3):1\u201352.\nJohn C Begeny and Diana J Greene. 2014. Can readabil-\nity formulas be used to successfully gauge difficulty\nof reading materials?\nPsychology in the Schools,\n51(2):198\u2013215.\nYonatan Belinkov and James Glass. 2019. Analysis\nmethods in neural language processing: A survey.\nTransactions of the Association for Computational\nLinguistics, 7:49\u201372.\nYves Bestgen. 2023. Measuring lexical diversity in\ntexts: The twofold length problem. Language Learn-\ning.\nGantavya Bhatt, Yifang Chen, Arnav M Das, Jifan\nZhang, Sang T Truong, Stephen Mussmann, Yinglun\nZhu, Jeffrey Bilmes, Simon S Du, Kevin Jamieson,\net al. 2024.\nAn experimental design framework\nfor label-efficient supervised finetuning of large lan-\nguage models. arXiv preprint arXiv:2401.06692.\nStella Biderman, Usvsn Prashanth, Lintang Sutawika,\nHailey Schoelkopf, Quentin Anthony, Shivanshu\nPurohit, and Edward Raff. 2024. Emergent and pre-\ndictable memorization in large language models. Ad-\nvances in Neural Information Processing Systems,\n36.\nBilibili. 2024. Index1.9b technical report.\nA Bjork. 1988. Least squares methods: Handbook of\nnumerical analysis. Finite Difference Methods Solu-\ntions of Equations in Rn, 1.\nDavid M Blei, Andrew Y Ng, and Michael I Jordan.\n2003. Latent dirichlet allocation. Journal of machine\nLearning research, 3(Jan):993\u20131022.\nZal\u00e1n Borsos, Mojmir Mutny, and Andreas Krause.\n2020. Coresets via bilevel optimization for continual\nlearning and streaming. Advances in neural informa-\ntion processing systems, 33:14879\u201314890.\nZal\u00e1n Borsos, Mojm\u00edr Mutn`\ny, Marco Tagliasacchi, and\nAndreas Krause. 2024.\nData summarization via\nbilevel optimization. Journal of Machine Learning\nResearch, 25(73):1\u201353.\nMaximilian B\u00f6ther, Viktor Gsteiger, Ties Robroek, and\nAna Klimovic. 2023. Modyn: A platform for model\ntraining on dynamic datasets with sample-level data\nselection. arXiv preprint arXiv:2312.06254.\nHerbie Bradley, Andrew Dai, Hannah Teufel, Jenny\nZhang, Koen Oostermeijer, Marco Bellagente, Jeff\nClune, Kenneth Stanley, Gr\u00e9gory Schott, and Joel\nLehman. 2023. Quality-diversity through ai feedback.\narXiv preprint arXiv:2310.13032.\nJonathan Brophy, Zayd Hammoudeh, and Daniel Lowd.\n2023. Adapting and evaluating influence-estimation\nmethods for gradient-boosted decision trees. Journal\nof Machine Learning Research, 24(154):1\u201348.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nQuang Vu Bui, Karim Sayadi, Soufian Ben Amor, and\nMarc Bui. 2017. Combining latent dirichlet alloca-\ntion and k-means for documents clustering: effect\nof probabilistic based distance measures. In Intelli-\ngent Information and Database Systems: 9th Asian\nConference, ACIIDS 2017, Kanazawa, Japan, April\n3-5, 2017, Proceedings, Part I 9, pages 248\u2013257.\nSpringer.\nAlexander Bukharin and Tuo Zhao. 2023. Data diversity\nmatters for robust instruction tuning. arXiv preprint\narXiv:2311.14736.\nJohn Byabazaire, Gregory O\u2019Hare, and Declan Delaney.\n2020. Data quality and trust: A perception from\nshared data in iot. In 2020 IEEE International Con-\nference on Communications Workshops (ICC Work-\nshops), pages 1\u20136. IEEE.\nLuben MC Cabezas, Rafael Izbicki, and Rafael B Stern.\n2023. Hierarchical clustering: Visualization, feature\nimportance and model selection. Applied Soft Com-\nputing, 141:110303.\nRicardo JGB Campello, Davoud Moulavi, and J\u00f6rg\nSander. 2013. Density-based clustering based on\nhierarchical density estimates. In Pacific-Asia confer-\nence on knowledge discovery and data mining, pages\n160\u2013172. Springer.\nJialun Cao, Wuqi Zhang, and Shing-Chi Cheung. 2024.\nConcerned with data contamination? assessing coun-\ntermeasures in code language model. arXiv preprint\narXiv:2403.16898.\nKris Cao and Stephen Clark. 2017. Latent variable\ndialogue models and their diversity. arXiv preprint\narXiv:1702.05962.\nYihan Cao, Yanbin Kang, and Lichao Sun. 2023. In-\nstruction mining: High-quality instruction data se-\nlection for large language models. arXiv preprint\narXiv:2307.06290.\nNicholas Carlini, Chang Liu, \u00dalfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX security symposium\n(USENIX security 19), pages 267\u2013284.\nPatricia L Carrell. 1987. Readability in esl.\nJeanne Sternlicht Chall and Edgar Dale. 1995. Readabil-\nity revisited: The new dale-chall readability formula.\nEdgar Brookline Books.\nChi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu,\nWei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan\nLiu. 2023. Chateval: Towards better llm-based eval-\nuators through multi-agent debate. arXiv preprint\narXiv:2308.07201.\nKHR Chan, Y Yu, C You, H Qi, J Wright, and YR Ma.\n2021. a white-box deep network from the principle\nof maximizing rate reduction. arxiv. 2021. arXiv\npreprint arXiv:2105.10446.\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu,\nLinyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi,\nCunxiang Wang, Yidong Wang, et al. 2024. A sur-\nvey on evaluation of large language models. ACM\nTransactions on Intelligent Systems and Technology,\n15(3):1\u201345.\nDaoyuan Chen, Yilun Huang, Zhijian Ma, Hesen Chen,\nXuchen Pan, Ce Ge, Dawei Gao, Yuexiang Xie,\nZhaoyang Liu, Jinyang Gao, et al. 2024a. Data-juicer:\nA one-stop data processing system for large language\nmodels. In Companion of the 2024 International\nConference on Management of Data, pages 120\u2013134.\nHao Chen, Yiming Zhang, Qi Zhang, Hantao Yang, Xi-\naomeng Hu, Xuetao Ma, Yifan Yanggong, and Junbo\nZhao. 2023a. Maybe only 0.5% data is needed: A\npreliminary exploration of low training data instruc-\ntion tuning. arXiv preprint arXiv:2305.09246.\nJiuhai Chen and Jonas Mueller. 2023. Quantifying un-\ncertainty in answers from any language model and\nenhancing their trustworthiness.\nJiuhai Chen and Jonas Mueller. 2024.\nAutomated\ndata curation for robust language model fine-tuning.\narXiv preprint arXiv:2403.12776.\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa\nGunaratna, Vikas Yadav, Zheng Tang, Vijay Srini-\nvasan, Tianyi Zhou, Heng Huang, et al. 2023b. Al-\npagasus: Training a better alpaca with fewer data.\narXiv preprint arXiv:2307.08701.\nMayee Chen, Nicholas Roberts, Kush Bhatia, Jue Wang,\nCe Zhang, Frederic Sala, and Christopher R\u00e9. 2024b.\nSkill-it! a data-driven skills framework for under-\nstanding and training language models. Advances in\nNeural Information Processing Systems, 36.\nYutian Chen, Luke Bornn, Nando De Freitas, Mareija\nEskelin, Jing Fang, and Max Welling. 2016. Herded\ngibbs sampling. The Journal of Machine Learning\nResearch, 17(1):263\u2013291.\nYutian Chen, Max Welling, and Alex Smola. 2012.\nSuper-samples from kernel herding. arXiv preprint\narXiv:1203.3472.\nDaixuan Cheng, Yuxian Gu, Shaohan Huang, Junyu Bi,\nMinlie Huang, and Furu Wei. 2024. Instruction pre-\ntraining: Language models are supervised multitask\nlearners. arXiv preprint arXiv:2406.14491.\nAnshuman Chhabra, Peizhao Li, Prasant Mohapatra,\nand Hongfu Liu. 2024.\n\" what data benefits my\nclassifier?\" enhancing model performance and inter-\npretability through influence-based data selection. In\nThe Twelfth International Conference on Learning\nRepresentations.\nGui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros\nKarydas, Anand Rajagopalan, Afshin Rostamizadeh,\nand Sanjiv Kumar. 2021. Batch active learning at\nscale. Advances in Neural Information Processing\nSystems, 34:11933\u201311944.\nKevyn Collins-Thompson and Jamie Callan. 2005. Pre-\ndicting reading difficulty with statistical language\nmodels. Journal of the american society for informa-\ntion science and technology, 56(13):1448\u20131462.\nBeno\u00eet Colson, Patrice Marcotte, and Gilles Savard.\n2007. An overview of bilevel optimization. Annals\nof operations research, 153:235\u2013256.\nBradford R Connatser. 1999. Last rites for readabil-\nity formulas in technical communication. Journal\nof technical writing and communication, 29(3):271\u2013\n287.\nMike Conover, Matt Hayes, Ankit Mathur, Jianwei Xie,\nJun Wan, Sam Shah, Ali Ghodsi, Patrick Wendell,\nMatei Zaharia, and Reynold Xin. 2023. Free dolly:\nIntroducing the world\u2019s first truly open instruction-\ntuned llm. Company Blog of Databricks.\nWilliam J Cook, William H Cunningham, William R\nPulleyblank, and Alexander Schrijver. 1994. Com-\nbinatorial optimization. Unpublished manuscript,\n10:75\u201393.\nG\u00e9rard Cornu\u00e9jols, George Nemhauser, and Laurence\nWolsey. 1983. The uncapicitated facility location\nproblem. Technical report, Cornell University Opera-\ntions Research and Industrial Engineering.\nAndrea Cossu, Antonio Carta, Lucia Passaro, Vincenzo\nLomonaco, Tinne Tuytelaars, and Davide Bacciu.\n2024. Continual pre-training mitigates forgetting in\nlanguage and vision. Neural Networks, page 106492.\nIan Covert, Wenlong Ji, Tatsunori Hashimoto, and\nJames Zou. 2024.\nScaling laws for the value of\nindividual data points in machine learning. arXiv\npreprint arXiv:2405.20456.\nM Covington and Joe D McFall. 2008. The moving-\naverage type-token ratio. Linguistics Society of Amer-\nica, Chicago, IL.\nMichael A Covington and Joe D McFall. 2010. Cutting\nthe gordian knot: The moving-average type\u2013token\nratio (mattr).\nJournal of quantitative linguistics,\n17(2):94\u2013100.\nRadu G Cre\u00b8\ntulescu, Daniel I Morariu, Macarie Breazu,\nand Daniel Volovici. 2019. Dbscan algorithm for\ndocument clustering. International Journal of Ad-\nvanced Statistics and IT&C for Economics and Life\nSciences, 9(1):58\u201366.\nEdgar Dale and Jeanne S Chall. 1949. The concept of\nreadability. Elementary English, 26(1):19\u201326.\nDan Dan Friedman and Adji Bousso Dieng. 2023. The\nvendi score: A diversity evaluation metric for ma-\nchine learning. Transactions on machine learning\nresearch.\nVu Minh Hoang Dang and Rakesh M Verma. 2024.\nData quality in nlp: Metrics and a comprehensive\ntaxonomy. In International Symposium on Intelligent\nData Analysis, pages 217\u2013229. Springer.\nFredrik deBoer. 2014. Evaluating the comparability of\ntwo measures of lexical diversity. System, 47:139\u2013\n145.\nChunyuan Deng, Yilun Zhao, Xiangru Tang, Mark Ger-\nstein, and Arman Cohan. 2023. Investigating data\ncontamination in modern benchmarks for large lan-\nguage models. arXiv preprint arXiv:2311.09783.\nDingsheng Deng. 2020. Dbscan clustering algorithm\nbased on density. In 2020 7th international forum\non electrical engineering and automation (IFEEA),\npages 949\u2013953. IEEE.\nMingkai Deng, Bowen Tan, Zhengzhong Liu, Eric P\nXing, and Zhiting Hu. 2021. Compression, trans-\nduction, and creation: A unified framework for eval-\nuating natural language generation. arXiv preprint\narXiv:2109.06379.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nNing Ding, Yulin Chen, Bokai Xu, Yujia Qin, Zhi\nZheng, Shengding Hu, Zhiyuan Liu, Maosong Sun,\nand Bowen Zhou. 2023. Enhancing chat language\nmodels by scaling high-quality instructional conver-\nsations. arXiv preprint arXiv:2305.14233.\nMarcel Dix, Gianluca Manca, Kenneth Chigozie Okafor,\nReuben Borrison, Konstantin Kirchheim, Divyasheel\nSharma, Kr Chandrika, Deepti Maduskar, and Frank\nOrtmeier. 2023. Measuring the robustness of ml mod-\nels against data quality issues in industrial time series\ndata. In 2023 IEEE 21st International Conference on\nIndustrial Informatics (INDIN), pages 1\u20138. IEEE.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali\nFarhadi, Hannaneh Hajishirzi, and Noah Smith. 2020.\nFine-tuning pretrained language models: Weight ini-\ntializations, data orders, and early stopping. arXiv\npreprint arXiv:2002.06305.\nGuanting Dong, Keming Lu, Chengpeng Li, Tingyu\nXia, Bowen Yu, Chang Zhou, and Jingren Zhou.\n2024. Self-play with execution feedback: Improving\ninstruction-following capabilities of large language\nmodels. arXiv preprint arXiv:2406.13542.\nHanze Dong, Wei Xiong, Deepanshu Goyal, Yihan\nZhang, Winnie Chow, Rui Pan, Shizhe Diao, Jipeng\nZhang, Kashun Shum, and Tong Zhang. 2023. Raft:\nReward ranked finetuning for generative foundation\nmodel alignment. arXiv preprint arXiv:2304.06767.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. Advances in neural information process-\ning systems, 32.\nWei Dong, Charikar Moses, and Kai Li. 2011. Efficient\nk-nearest neighbor graph construction for generic\nsimilarity measures. In Proceedings of the 20th in-\nternational conference on World wide web, pages\n577\u2013586.\nJiawei Du, Qin Shi, and Joey Tianyi Zhou. 2024. Se-\nquential subset matching for dataset distillation. Ad-\nvances in Neural Information Processing Systems,\n36.\nNan Du, Yingyu Liang, Maria Balcan, and Le Song.\n2014.\nInfluence function learning in information\ndiffusion networks. In International Conference on\nMachine Learning, pages 2016\u20132024. PMLR.\nQianlong Du, Chengqing Zong, and Jiajun Zhang. 2023.\nMods: Model-oriented data selection for instruction\ntuning. arXiv preprint arXiv:2311.15653.\nWenchao Du and Alan W Black. 2019. Boosting dialog\nresponse generation. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics.\nWilliam H Dubay. 2004. The principles of readability.\nimpact information. Costa Mesa, CA.\nYann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi\nZhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin,\nPercy S Liang, and Tatsunori B Hashimoto. 2024.\nAlpacafarm: A simulation framework for methods\nthat learn from human feedback. Advances in Neural\nInformation Processing Systems, 36.\nSergey Edunov, Alexei Baevski, and Michael Auli. 2019.\nPre-trained language model representations for lan-\nguage generation. arXiv preprint arXiv:1903.09722.\nLisa Ehrlinger and Wolfram W\u00f6\u00df. 2022.\nA survey\nof data quality measurement and monitoring tools.\nFrontiers in big data, 5:850611.\nYuval Eldar, Michael Lindenbaum, Moshe Porat, and\nYehoshua Y Zeevi. 1997. The farthest point strategy\nfor progressive image sampling. IEEE transactions\non image processing, 6(9):1305\u20131315.\nLogan Engstrom, Axel Feldmann, and Aleksander\nMadry. 2024. Dsdm: Model-aware dataset selection\nwith datamodels. arXiv preprint arXiv:2401.12926.\nAyse Erkan and Yasemin Altun. 2010. Semi-supervised\nlearning via generalized maximum entropy. In Pro-\nceedings of the Thirteenth International Conference\non Artificial Intelligence and Statistics, pages 209\u2013\n216. JMLR Workshop and Conference Proceedings.\nKawin\nEthayarajh,\nYejin\nChoi,\nand\nSwabha\nSwayamdipta.\n2022.\nUnderstanding\ndataset\ndifficulty with V-usable information.\nIn Interna-\ntional Conference on Machine Learning, pages\n5988\u20136008. PMLR.\nPhilip R Evans and Garib N Murshudov. 2013. How\ngood are my data and what is the resolution? Acta\nCrystallographica Section D: Biological Crystallog-\nraphy, 69(7):1204\u20131214.\nDante Everaert and Christopher Potts. 2023. Gio: Gra-\ndient information optimization for training dataset\nselection. arXiv preprint arXiv:2306.11670.\nReza Zanjirani Farahani and Masoud Hekmatfar. 2009.\nFacility location: concepts, models, algorithms and\ncase studies. Springer Science & Business Media.\nDan Feldman and Michael Langberg. 2011. A unified\nframework for approximating and clustering data. In\nProceedings of the forty-third annual ACM sympo-\nsium on Theory of computing, pages 569\u2013578.\nVitaly Feldman. 2020. Does learning require memoriza-\ntion? a short tale about a long tail. In Proceedings\nof the 52nd Annual ACM SIGACT Symposium on\nTheory of Computing, pages 954\u2013959.\nVitaly Feldman and Chiyuan Zhang. 2020. What neural\nnetworks memorize and why: Discovering the long\ntail via influence estimation. Advances in Neural\nInformation Processing Systems, 33:2881\u20132891.\nMariano Felice and Lucia Specia. 2012. Linguistic\nfeatures for quality estimation. In Proceedings of the\nSeventh Workshop on Statistical Machine Translation,\npages 96\u2013103.\nFangxiaoyu Feng, Yinfei Yang, Daniel Cer, Naveen\nArivazhagan, and Wei Wang. 2020.\nLanguage-\nagnostic bert sentence embedding. arXiv preprint\narXiv:2007.01852.\nLijun Feng, Martin Jansche, Matt Huenerfauth, and\nNo\u00e9mie Elhadad. 2010. A comparison of features for\nautomatic readability assessment. In Coling 2010:\nPosters, pages 276\u2013284.\nSteven Y Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush Vosoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation ap-\nproaches for nlp. arXiv preprint arXiv:2105.03075.\nPeter Flach. 2012. Machine learning: the art and sci-\nence of algorithms that make sense of data. Cam-\nbridge university press.\nRudolph Flesch. 1948. A new readability yardstick.\nJournal of applied psychology, 32(3):221.\nThomas Fran\u00e7ois. 2010. La lisibilit\u00e9 computationnelle:\nun renouveau pour la lisibilit\u00e9 du fran\u00e7ais langue\npremi\u00e8re et seconde? ITL-International Journal of\nApplied Linguistics, 160(1):75\u201399.\nThomas Fran\u00e7ois. 2011.\nLes apports du traitement\nautomatique du langage \u00e0 la lisibilit\u00e9 du fran\u00e7ais\nlangue \u00e9trang\u00e8re. Ph.D. thesis, Ph. D. thesis, Uni-\nversit\u00e9 Catholique de Louvain. Thesis Supervisors:\nC\u00e9drick . . . .\nThomas Fran\u00e7ois and C\u00e9drick Fairon. 2012. An \u201cai\nreadability\u201d formula for french as a foreign language.\nIn Proceedings of the 2012 joint conference on em-\npirical methods in Natural Language Processing and\ncomputational natural language learning, pages 466\u2013\n477.\nThomas Fran\u00e7ois and Eleni Miltsakaki. 2012. Do nlp\nand machine learning improve traditional readability\nformulas? In Proceedings of the First Workshop on\nPredicting and Improving Text Readability for target\nreader populations, pages 49\u201357.\nThomas George, C\u00e9sar Laurent, Xavier Bouthillier,\nNicolas Ballas, and Pascal Vincent. 2021. Fast ap-\nproximate natural gradient descent in a kronecker-\nfactored eigenbasis. Preprint, arXiv:1806.03884.\nAmirata Ghorbani and James Zou. 2019. Data shapley:\nEquitable valuation of data for machine learning. In\nInternational conference on machine learning, pages\n2242\u20132251. PMLR.\nSreyan Ghosh, Chandra Kiran Reddy Evuru, Sonal Ku-\nmar, Deepali Aneja, Zeyu Jin, Ramani Duraiswami,\nDinesh Manocha, et al. 2024.\nA closer look at\nthe limitations of instruction tuning. arXiv preprint\narXiv:2402.05119.\nSachin Goyal, Ananya Kumar, Sankalp Garg, Zico\nKolter, and Aditi Raghunathan. 2023. Finetune like\nyou pretrain: Improved finetuning of zero-shot vision\nmodels. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 19338\u201319347.\nYves Grandvalet and Yoshua Bengio. 2004.\nSemi-\nsupervised learning by entropy minimization. Ad-\nvances in neural information processing systems, 17.\nHans-Rolf Gregorius and Elizabeth M Gillet. 2008.\nGeneralized simpson-diversity.\nEcological Mod-\nelling, 211(1-2):90\u201396.\nRoger Grosse, Juhan Bae, Cem Anil, Nelson Elhage,\nAlex Tamkin, Amirhossein Tajdini, Benoit Steiner,\nDustin Li, Esin Durmus, Ethan Perez, Evan Hubinger,\nKamil\u02d9\ne Luko\u0161i\u00af\nut\u02d9\ne, Karina Nguyen, Nicholas Joseph,\nSam McCandlish, Jared Kaplan, and Samuel R.\nBowman. 2023.\nStudying large language model\ngeneralization with influence functions. Preprint,\narXiv:2308.03296.\nSuriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio\nC\u00e9sar Teodoro Mendes, Allie Del Giorno, Sivakanth\nGopi, Mojan Javaheripi, Piero Kauffmann, Gustavo\nde Rosa, Olli Saarikivi, et al. 2023. Textbooks are all\nyou need. arXiv preprint arXiv:2306.11644.\nRobert Gunning. 1952. The technique of clear writing.\n(No Title).\nChengcheng Guo, Bo Zhao, and Yanbing Bai. 2022.\nDeepcore: A comprehensive library for coreset selec-\ntion in deep learning. In International Conference\non Database and Expert Systems Applications, pages\n181\u2013195. Springer.\nNitin Gupta, Shashank Mujumdar, Hima Patel, Satoshi\nMasuda, Naveen Panwar, Sambaran Bandyopadhyay,\nSameep Mehta, Shanmukha Guttula, Shazia Afzal,\nRuhi Sharma Mittal, et al. 2021. Data quality for\nmachine learning tasks. In Proceedings of the 27th\nACM SIGKDD conference on knowledge discovery\n& data mining, pages 4040\u20134041.\nKelvin Guu, Albert Webson, Ellie Pavlick, Lucas Dixon,\nIan Tenney, and Tolga Bolukbasi. 2023.\nSimflu-\nence: Modeling the influence of individual training\nexamples by simulating training runs. arXiv preprint\narXiv:2303.08114.\nNick Harvey and Samira Samadi. 2014. Near-optimal\nherding. In Conference on Learning Theory, pages\n1165\u20131182. PMLR.\nFangliang He and Xin-Sheng Hu. 2005. Hubbell\u2019s fun-\ndamental biodiversity parameter and the simpson di-\nversity index. Ecology Letters, 8(4):386\u2013390.\nJimming He, Sanjana Garg, and Jonas Mueller. 2024.\nHow to detect bad data in your instruction tuning\ndataset (for better llm fine-tuning).\nHmida Hmida, Sana Ben Hamida, Amel Borgi, and\nMarta Rukoz. 2016.\nHierarchical data topol-\nogy based selection for large scale learning.\nIn\n2016 Intl IEEE Conferences on Ubiquitous In-\ntelligence & Computing, Advanced and Trusted\nComputing,\nScalable Computing and Commu-\nnications,\nCloud\nand\nBig\nData\nComputing,\nInternet of People, and Smart World Congress\n(UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld),\npages 1221\u20131226. IEEE.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nAnna Huang et al. 2008. Similarity measures for text\ndocument clustering. In Proceedings of the sixth new\nzealand computer science research student confer-\nence (NZCSRSC2008), Christchurch, New Zealand,\nvolume 4, pages 9\u201356.\nDanqing Huang, Shuming Shi, Chin-Yew Lin, Jian Yin,\nand Wei-Ying Ma. 2016. How well do computers\nsolve math word problems? large-scale dataset con-\nstruction and evaluation. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 887\u2013896.\nHui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and\nTiejun Zhao. 2024.\nAn empirical study of llm-\nas-a-judge for llm evaluation:\nFine-tuned judge\nmodels are task-specific classifiers. arXiv preprint\narXiv:2403.02839.\nFerenc Husz\u00e1r and David Duvenaud. 2012. Optimally-\nweighted herding is bayesian quadrature.\narXiv\npreprint arXiv:1204.1664.\nAdam Ibrahim, Benjamin Th\u00e9rien, Kshitij Gupta,\nMats L Richter, Quentin Anthony, Timoth\u00e9e Lesort,\nEugene Belilovsky, and Irina Rish. 2024. Simple\nand scalable strategies to continually pre-train large\nlanguage models. arXiv preprint arXiv:2403.08763.\nAbiodun M Ikotun, Absalom E Ezugwu, Laith Abuali-\ngah, Belal Abuhaija, and Jia Heming. 2023. K-means\nclustering algorithms: A comprehensive review, vari-\nants analysis, and advances in the era of big data.\nInformation Sciences, 622:178\u2013210.\nAndrew Ilyas, Sung Min Park, Logan Engstrom, Guil-\nlaume Leclerc, and Aleksander Madry. 2022. Data-\nmodels: Predicting predictions from training data.\narXiv preprint arXiv:2202.00622.\nBogdan Ionescu, Mihai Lupu, Maia Rohm, Alexan-\ndru Lucian G\u00eensca, and Henning M\u00fcller. 2018.\nDatasets column: diversity and credibility for social\nimages and image retrieval. ACM SIGMultimedia\nRecords, 9(3):7\u20137.\nSaachi Jain, Hadi Salman, Alaa Khaddaj, Eric Wong,\nSung Min Park, and Aleksander M \u02db\nadry. 2023. A\ndata-based perspective on transfer learning. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 3613\u20133622.\nSarthak Jain, Varun Manjunatha, Byron C Wallace, and\nAni Nenkova. 2022. Influence functions for sequence\ntagging models. arXiv preprint arXiv:2210.14177.\nJoel Jang, Seungone Kim, Seonghyeon Ye, Doyoung\nKim, Lajanugen Logeswaran, Moontae Lee, Kyung-\njae Lee, and Minjoon Seo. 2023. Exploring the bene-\nfits of training expert language models over instruc-\ntion tuning. In International Conference on Machine\nLearning, pages 14702\u201314729. PMLR.\nScott Jarvis. 2013. Capturing the diversity in lexical\ndiversity. Language learning, 63:87\u2013106.\nScott Jarvis and M Daller. 2013. Defining and measur-\ning lexical diversity. Vocabulary knowledge: Human\nratings and automated measures. Amsterdam, The\nNetherlands.\nFred Jelinek, Robert L Mercer, Lalit R Bahl, and\nJames K Baker. 1977. Perplexity\u2014a measure of the\ndifficulty of speech recognition tasks. The Journal of\nthe Acoustical Society of America, 62(S1):S63\u2013S63.\nFrederick Jelinek. 1980.\nInterpolated estimation of\nmarkov source parameters from sparse data. In Proc.\nWorkshop on Pattern Recognition in Practice, 1980.\nHongjie Jia, Shifei Ding, Xinzheng Xu, and Ru Nie.\n2014. The latest research progress on spectral clus-\ntering. Neural Computing and Applications, 24:1477\u2013\n1486.\nRuoxi Jia, David Dao, Boxin Wang, Frances Ann Hubis,\nNick Hynes, Nezihe Merve G\u00fcrel, Bo Li, Ce Zhang,\nDawn Song, and Costas J Spanos. 2019. Towards\nefficient data valuation based on the shapley value.\nIn The 22nd International Conference on Artificial\nIntelligence and Statistics, pages 1167\u20131176. PMLR.\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Men-\nsch, Chris Bamford, Devendra Singh Chaplot, Diego\nde las Casas, Florian Bressand, Gianna Lengyel, Guil-\nlaume Lample, Lucile Saulnier, et al. 2023a. Mistral\n7b. arXiv preprint arXiv:2310.06825.\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine\nRoux, Arthur Mensch, Blanche Savary, Chris Bam-\nford, Devendra Singh Chaplot, Diego de las Casas,\nEmma Bou Hanna, Florian Bressand, et al. 2024a.\nMixtral of experts. arXiv preprint arXiv:2401.04088.\nMingjian Jiang, Yangjun Ruan, Sicong Huang, Saifei\nLiao, Silviu Pitis, Roger Baker Grosse, and Jimmy\nBa. 2023b. Calibrating language models via aug-\nmented prompt ensembles. Workshop on Challenges\nin Deployable Generative AI at International Confer-\nence on Machine Learning.\nMinhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan\nSchaeffer, Siru Ouyang, Jiawei Han, and Sanmi\nKoyejo. 2024b.\nInvestigating data contamination\nfor pre-training language models. arXiv preprint\narXiv:2401.06059.\nWenyu Jiang, Hao Cheng, Mingcai Chen, Chongjun\nWang, and Hongxin Wei. 2023c. Dos: Diverse out-\nlier sampling for out-of-distribution detection. arXiv\npreprint arXiv:2306.02031.\nWenyu Jiang, Zhenlong Liu, Zejian Xie, Songxin Zhang,\nBingyi Jing, and Hongxin Wei. 2024c. Exploring\nlearning complexity for downstream data pruning.\narXiv preprint arXiv:2402.05356.\nZixuan Jiang, Jiaqi Gu, Mingjie Liu, and David Z Pan.\n2023d.\nDelving into effective gradient matching\nfor dataset condensation.\nIn 2023 IEEE Interna-\ntional Conference on Omni-layer Intelligent Systems\n(COINS), pages 1\u20136. IEEE.\nXisen Jin and Xiang Ren. 2024a. Demystifying for-\ngetting in language model fine-tuning with statisti-\ncal analysis of example associations. arXiv preprint\narXiv:2406.14026.\nXisen Jin and Xiang Ren. 2024b. What will my model\nforget? forecasting forgotten examples in language\nmodel refinement. arXiv preprint arXiv:2402.01865.\nXisen Jin, Dejiao Zhang, Henghui Zhu, Wei Xiao,\nShang-Wen Li, Xiaokai Wei, Andrew Arnold, and\nXiang Ren. 2021. Lifelong pretraining: Continu-\nally adapting language models to emerging corpora.\narXiv preprint arXiv:2110.08534.\nRC St John and Norman R Draper. 1975. D-optimality\nfor regression designs: a review.\nTechnometrics,\n17(1):15\u201323.\nSaurav Kadavath, Tom Conerly, Amanda Askell, Tom\nHenighan, Dawn Drain, Ethan Perez, Nicholas\nSchiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli\nTran-Johnson, et al. 2022.\nLanguage models\n(mostly) know what they know.\narXiv preprint\narXiv:2207.05221.\nFiruz Kamalov. 2020. Kernel density estimation based\nsampling for imbalanced class distribution. Informa-\ntion Sciences, 512:1192\u20131201.\nNikhil Kandpal, Eric Wallace, and Colin Raffel. 2022.\nDeduplicating training data mitigates privacy risks\nin language models. In International Conference on\nMachine Learning, pages 10697\u201310707. PMLR.\nFeiyang Kang, Hoang Anh Just, Anit Kumar Sahu, and\nRuoxi Jia. 2024. Performance scaling via optimal\ntransport: Enabling data selection from partially re-\nvealed sources. Advances in Neural Information Pro-\ncessing Systems, 36.\nTapas Kanungo, David M Mount, Nathan S Netanyahu,\nChristine Piatko, Ruth Silverman, and Angela Y Wu.\n2000. The analysis of a simple k-means clustering\nalgorithm. In Proceedings of the sixteenth annual\nsymposium on Computational geometry, pages 100\u2013\n109.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models.\narXiv\npreprint arXiv:2001.08361.\nZixuan Ke. 2024. Continual Learning with Language\nModels.\nPh.D. thesis, University of Illinois at\nChicago.\nSusan Kemper. 1983. Measuring the inference load of a\ntext. Journal of educational psychology, 75(3):391.\nKimmo Kettunen. 2014. Can type-token ratio be used to\nshow morphological complexity of languages? Jour-\nnal of Quantitative Linguistics, 21(3):223\u2013245.\nKamran Khan, Saif Ur Rehman, Kamran Aziz, Simon\nFong, and Sababady Sarasvady. 2014. Dbscan: Past,\npresent and future. In The fifth international confer-\nence on the applications of digital information and\nweb technologies (ICADIWT 2014), pages 232\u2013238.\nIEEE.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, et al. 2021. Dynabench: Rethinking benchmark-\ning in nlp. arXiv preprint arXiv:2104.14337.\nKrishnateja\nKillamsetty,\nSivasubramanian\nDurga,\nGanesh Ramakrishnan, Abir De, and Rishabh Iyer.\n2021a. Grad-match: Gradient matching based data\nsubset selection for efficient deep model training.\nIn International Conference on Machine Learning,\npages 5464\u20135474. PMLR.\nKrishnateja\nKillamsetty,\nDurga\nSivasubramanian,\nGanesh Ramakrishnan, and Rishabh Iyer. 2021b.\nGlister: Generalization based data subset selection\nfor efficient and robust learning. In Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol-\nume 35, pages 8110\u20138118.\nKrishnateja Killamsetty, Xujiang Zhao, Feng Chen, and\nRishabh Iyer. 2021c. Retrieve: Coreset selection for\nefficient and robust semi-supervised learning. Ad-\nvances in neural information processing systems,\n34:14488\u201314501.\nWalter Kintsch and Douglas Vipond. 2014. Reading\ncomprehension and readability in educational prac-\ntice and psychological theory. In Perspectives on\nmemory research, pages 329\u2013365. Psychology Press.\nJohn\nKirchenbauer,\nGarrett\nHonke,\nGowthami\nSomepalli, Jonas Geiping, Daphne Ippolito, Kather-\nine Lee, Tom Goldstein, and David Andre. 2024.\nLmd3: Language model data density dependence.\narXiv preprint arXiv:2405.06331.\nAndreas Kirsch. 2023. Does\" deep learning on a data\ndiet\" reproduce? overall yes, but grand at initializa-\ntion does not. arXiv preprint arXiv:2303.14753.\nGeorge R Klare. 1974. Assessing readability. Reading\nresearch quarterly, pages 62\u2013102.\nGeorge R Klare et al. 1963. The measurement of read-\nability. Iowa State University Press Ames.\nGeorge R Klare et al. 1984. Readability. Handbook of\nreading research, 1:681\u2013744.\nPang Wei Koh and Percy Liang. 2017. Understanding\nblack-box predictions via influence functions. In\nInternational conference on machine learning, pages\n1885\u20131894. PMLR.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini, Nate\nKushman, and Hannaneh Hajishirzi. 2016. Mawps:\nA math word problem repository. In Proceedings of\nthe 2016 conference of the north american chapter of\nthe association for computational linguistics: human\nlanguage technologies, pages 1152\u20131157.\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte,\nSotiris Anagnostidis, Zhi Rui Tam, Keith Stevens,\nAbdullah Barhoum, Duc Nguyen, Oliver Stan-\nley, Rich\u00e1rd Nagyfi, et al. 2024.\nOpenassistant\nconversations-democratizing large language model\nalignment. Advances in Neural Information Process-\ning Systems, 36.\nJan Kremer, Kim Steenstrup Pedersen, and Christian\nIgel. 2014. Active learning with support vector ma-\nchines. Wiley Interdisciplinary Reviews: Data Min-\ning and Knowledge Discovery, 4(4):313\u2013326.\nSandra Kublik and Shubham Saboo. 2023. GPT-3: The\nUltimate Guide to Building NLP Products with Ope-\nnAI API. Packt Publishing Ltd.\nPo-Nien Kung and Nanyun Peng. 2023.\nDo mod-\nels really learn to follow instructions? an empir-\nical study of instruction tuning.\narXiv preprint\narXiv:2305.11383.\nPo-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang,\nand Nanyun Peng. 2023.\nActive instruction tun-\ning: Improving cross-task generalization by train-\ning on prompt sensitive tasks.\narXiv preprint\narXiv:2311.00288.\nJeongyeol Kwon, Dohyun Kwon, Stephen Wright, and\nRobert D Nowak. 2023. A fully first-order method\nfor stochastic bilevel optimization. In International\nConference on Machine Learning, pages 18083\u2013\n18113. PMLR.\nYongchan Kwon and James Zou. 2021.\nBeta shap-\nley:\na unified and noise-reduced data valuation\nframework for machine learning.\narXiv preprint\narXiv:2110.14049.\nKristopher Kyle, Scott A Crossley, and Scott Jarvis.\n2021. Assessing the validity of lexical diversity in-\ndices using direct judgements. Language Assessment\nQuarterly, 18(2):154\u2013170.\nYi-An Lai, Xuan Zhu, Yi Zhang, and Mona Diab. 2020.\nDiversity, density, and homogeneity: Quantitative\ncharacteristic metrics for text collections.\narXiv\npreprint arXiv:2003.08529.\nYrjo Lappalainen and Nikesh Narayanan. 2023. Aisha:\nA custom ai library chatbot using the chatgpt api.\nJournal of Web Librarianship, 17(3):37\u201358.\nStefan Larson,\nAnish Mahendran,\nAndrew Lee,\nJonathan K Kummerfeld, Parker Hill, Michael A\nLaurenzano, Johann Hauswald, Lingjia Tang, and\nJason Mars. 2019. Outlier detection for improved\ndata quality and diversity in dialog systems. arXiv\npreprint arXiv:1904.03122.\nCosmin Lazar and Andrei Doncescu. 2009. Non nega-\ntive matrix factorization clustering capabilities; appli-\ncation on multivariate image segmentation. In 2009\nInternational Conference on Complex, Intelligent and\nSoftware Intensive Systems, pages 924\u2013929. IEEE.\nRonan Le Bras, Swabha Swayamdipta, Chandra Bha-\ngavatula, Rowan Zellers, Matthew Peters, Ashish\nSabharwal, and Yejin Choi. 2020. Adversarial fil-\nters of dataset biases. In International conference on\nmachine learning, pages 1078\u20131088. Pmlr.\nGuillaume Lecu\u00e9 and Shahar Mendelson. 2018. Regu-\nlarization and the small-ball method i: sparse recov-\nery.\nAlycia Lee, Brando Miranda, Sudharsan Sundar, and\nSanmi Koyejo. 2023. Beyond scale: the diversity\ncoefficient as a data quality metric demonstrates\nllms are pre-trained on formally diverse data. arXiv\npreprint arXiv:2306.13840.\nDaniel Lee and H Sebastian Seung. 2000. Algorithms\nfor non-negative matrix factorization. Advances in\nneural information processing systems, 13.\nKatherine Lee, Daphne Ippolito, Andrew Nystrom,\nChiyuan Zhang, Douglas Eck, Chris Callison-Burch,\nand Nicholas Carlini. 2021. Deduplicating training\ndata makes language models better. arXiv preprint\narXiv:2107.06499.\nBohan Li, Yutai Hou, and Wanxiang Che. 2022. Data\naugmentation approaches in natural language pro-\ncessing: A survey. Ai Open, 3:71\u201390.\nHaoran Li, Qingxiu Dong, Zhengyang Tang, Chaojun\nWang, Xingxing Zhang, Haoyang Huang, Shaohan\nHuang, Xiaolong Huang, Zeqiang Huang, Dongdong\nZhang, et al. 2024a. Synthetic data (almost) from\nscratch: Generalized instruction tuning for language\nmodels. arXiv preprint arXiv:2402.13064.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2015. A diversity-promoting objec-\ntive function for neural conversation models. arXiv\npreprint arXiv:1510.03055.\nMing Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu\nZhao, Jianzong Wang, Ning Cheng, and Tianyi\nZhou. 2024b. Superfiltering: Weak-to-strong data\nfiltering for fast instruction-tuning. arXiv preprint\narXiv:2402.00530.\nMing Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang\nChen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and\nJing Xiao. 2023a. From quantity to quality: Boosting\nllm performance with self-guided data selection for\ninstruction tuning. arXiv preprint arXiv:2308.12032.\nNa Li, Yiyang Qi, Chaoran Li, and Zhiming Zhao.\n2024c. Active learning for data quality control: A\nsurvey. ACM Journal of Data and Information Qual-\nity.\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas\nMuennighoff, Denis Kocetkov, Chenghao Mou, Marc\nMarone, Christopher Akiki, Jia Li, Jenny Chim, et al.\n2023b.\nStarcoder: may the source be with you!\narXiv preprint arXiv:2305.06161.\nXian Li, Ping Yu, Chunting Zhou, Timo Schick, Luke\nZettlemoyer, Omer Levy, Jason Weston, and Mike\nLewis. 2023c. Self-alignment with instruction back-\ntranslation. arXiv preprint arXiv:2308.06259.\nYize Li, Pu Zhao, Xue Lin, Bhavya Kailkhura, and\nRyan Goldhahn. 2023d. Less is more: Data prun-\ning for faster adversarial training. arXiv preprint\narXiv:2302.12366.\nYuchen Li, Ju Fan, Yanhao Wang, and Kian-Lee Tan.\n2018. Influence maximization on social graphs: A\nsurvey. IEEE Transactions on Knowledge and Data\nEngineering, 30(10):1852\u20131872.\nZhuoyan Li, Hangxiao Zhu, Zhuoran Lu, and Ming\nYin. 2023e. Synthetic data generation with large\nlanguage models for text classification: Potential and\nlimitations. arXiv preprint arXiv:2310.07849.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, et al. 2022. Holistic evaluation of language\nmodels. arXiv preprint arXiv:2211.09110.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries.\nIn Text summarization\nbranches out, pages 74\u201381.\nJinkun Lin, Anqi Zhang, Mathias L\u00e9cuyer, Jinyang\nLi, Aurojit Panda, and Siddhartha Sen. 2022. Mea-\nsuring the effect of training data on deep learning\npredictions via randomized experiments. In Inter-\nnational Conference on Machine Learning, pages\n13468\u201313504. PMLR.\nStephanie Lin, Jacob Hilton, and Owain Evans. 2021.\nTruthfulqa: Measuring how models mimic human\nfalsehoods. arXiv preprint arXiv:2109.07958.\nXinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli\nFeng, Yinwei Wei, and Tat-Seng Chua. 2024. Data-\nefficient fine-tuning for llm-based recommendation.\nPreprint, arXiv:2401.17197.\nAlisa Liu, Xiaochuang Han, Yizhong Wang, Yulia\nTsvetkov, Yejin Choi, and Noah A Smith. 2024a.\nTuning language models by proxy. arXiv preprint\narXiv:2401.08565.\nFuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen,\nKaiqiang Song, Sangwoo Cho, Yaser Yacoob, and\nDong Yu. 2023a.\nMmc: Advancing multimodal\nchart understanding with large-scale instruction tun-\ning. arXiv preprint arXiv:2311.10774.\nQingyi Liu, Yekun Chai, Shuohuan Wang, Yu Sun,\nKeze Wang, and Hua Wu. 2024b.\nOn training\ndata influence of gpt models.\narXiv preprint\narXiv:2404.07840.\nWei Liu, Weihao Zeng, Keqing He, Yong Jiang, and\nJunxian He. 2023b.\nWhat makes good data for\nalignment?\na comprehensive study of automatic\ndata selection in instruction tuning. arXiv preprint\narXiv:2312.15685.\nXiaoyu Liu, Paiheng Xu, Junda Wu, Jiaxin Yuan, Yifan\nYang, Yuhang Zhou, Fuxiao Liu, Tianrui Guan, Hao-\nliang Wang, Tong Yu, et al. 2024c. Large language\nmodels and causal inference in collaboration: A com-\nprehensive survey. arXiv preprint arXiv:2403.09606.\nYang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and\nLianwen Jin. 2024d. Datasets for large language\nmodels: A comprehensive survey. arXiv preprint\narXiv:2402.18041.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nZiche Liu, Rui Ke, Feng Jiang, and Haizhou Li. 2024e.\nTake the essence and discard the dross: A rethink-\ning on data selection for fine-tuning large language\nmodels. arXiv preprint arXiv:2406.14115.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. In International Conference on\nMachine Learning, pages 22631\u201322648. PMLR.\nRenze Lou, Kai Zhang, and Wenpeng Yin. 2024. Large\nlanguage model instruction following: A survey of\nprogresses and challenges. Computational Linguis-\ntics, pages 1\u201310.\nKeming Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Jun-\nyang Lin, Chuanqi Tan, Chang Zhou, and Jingren\nZhou. 2023a. # instag: Instruction tagging for analyz-\ning supervised fine-tuning of large language models.\nIn The Twelfth International Conference on Learning\nRepresentations.\nYingzhou Lu, Minjie Shen, Huazheng Wang, Xiao\nWang, Capucine van Rechem, and Wenqi Wei. 2023b.\nMachine learning for synthetic data generation: a re-\nview. arXiv preprint arXiv:2302.04062.\nZhaosong Lu and Sanyou Mei. 2024. First-order penalty\nmethods for bilevel optimization. SIAM Journal on\nOptimization, 34(2):1937\u20131969.\nChengcheng Ma, Yang Liu, Jiankang Deng, Lingxi\nXie, Weiming Dong, and Changsheng Xu. 2023.\nUnderstanding and mitigating overfitting in prompt\ntuning for vision-language models. IEEE Transac-\ntions on Circuits and Systems for Video Technology,\n33(9):4616\u20134629.\nInbal Magar and Roy Schwartz. 2022. Data contami-\nnation: From memorization to exploitation. arXiv\npreprint arXiv:2203.08242.\nPratyush Maini, Saurabh Garg, Zachary Lipton, and\nJ Zico Kolter. 2022. Characterizing datapoints via\nsecond-split forgetting. Advances in Neural Informa-\ntion Processing Systems, 35:30044\u201330057.\nJyoti Malhotra and Jagdish Bakal. 2015. A survey and\ncomparative study of data deduplication techniques.\nIn 2015 International Conference on Pervasive Com-\nputing (ICPC), pages 1\u20135. IEEE.\nDavid Malvern, Brian Richards, Ngoni Chipere, and\nPilar Dur\u00e1n. 2004. Lexical diversity and language\ndevelopment. Springer.\nDavid D Malvern and Brian J Richards. 1997. A new\nmeasure of lexical diversity. British Studies in Ap-\nplied Linguistics, 12:58\u201371.\nMax Marion, Ahmet \u00dcst\u00fcn, Luiza Pozzobon, Alex\nWang, Marzieh Fadaee, and Sara Hooker. 2023.\nWhen less is more:\nInvestigating data pruning\nfor pretraining llms at scale.\narXiv preprint\narXiv:2309.04564.\nMarc Marone and Benjamin Van Durme. 2024. Data\nportraits: Recording foundation model training data.\nAdvances in Neural Information Processing Systems,\n36.\nMichael Mathieu and Yann LeCun. 2014. Fast approx-\nimation of rotations and hessians matrices. arXiv\npreprint arXiv:1404.7195.\nVladimir Matlach, Diego Krivochen, and Jiri Mili\u02c7\ncka.\n2021. A method for comparison of general sequences\nvia type-token ratio. Language and Text: Data, mod-\nels, information and applications. Amsterdam: John\nBenjamins, pages 37\u201354.\nPhilip M McCarthy. 2005. An assessment of the range\nand usefulness of lexical diversity measures and the\npotential of the measure of textual, lexical diversity\n(MTLD). Ph.D. thesis, The University of Memphis.\nPhilip M McCarthy and Scott Jarvis. 2010. Mtld, vocd-\nd, and hd-d: A validation study of sophisticated ap-\nproaches to lexical diversity assessment. Behavior\nresearch methods, 42(2):381\u2013392.\nG\u00e1bor Melis, Chris Dyer, and Phil Blunsom. 2017. On\nthe state of the art of evaluation in neural language\nmodels. arXiv preprint arXiv:1707.05589.\nBrando Miranda, Patrick Yu, Yu-Xiong Wang, and\nSanmi Koyejo. 2022. The curse of low task diver-\nsity: On the failure of transfer learning to outperform\nmaml and their empirical equivalence. arXiv preprint\narXiv:2208.01545.\nSwaroop Mishra,\nAnjana Arunkumar,\nBhavdeep\nSachdeva, Chris Bryan, and Chitta Baral. 2020a. Dqi:\nA guide to benchmark evaluation. arXiv preprint\narXiv:2008.03964.\nSwaroop Mishra,\nAnjana Arunkumar,\nBhavdeep\nSachdeva, Chris Bryan, and Chitta Baral. 2020b.\nDqi: Measuring data quality in nlp. arXiv preprint\narXiv:2005.00816.\nSwaroop Mishra and Bhavdeep Singh Sachdeva. 2020.\nDo we need to create big datasets to learn a task?\nIn Proceedings of SustaiNLP: Workshop on Simple\nand Efficient Natural Language Processing, pages\n169\u2013173.\nNiluthpol Chowdhury Mithun, Rameswar Panda, and\nAmit K Roy-Chowdhury. 2019. Construction of di-\nverse image datasets from web collections with lim-\nited labeling. IEEE Transactions on Circuits and\nSystems for Video Technology, 30(4):1147\u20131161.\nSedir Mohammed, Hazar Harmouch, Felix Naumann,\nand Divesh Srivastava. 2024. Data quality assess-\nment: Challenges and opportunities. arXiv preprint\narXiv:2403.00526.\nRobert C Moore and William Lewis. 2010. Intelligent\nselection of language model training data. In Pro-\nceedings of the ACL 2010 conference short papers,\npages 220\u2013224.\nDavid Mouillot and Alain Lepretre. 1999. A compar-\nison of species diversity estimators. Researches on\nPopulation Ecology, 41:203\u2013215.\nKevin P Murphy. 2012. Machine learning: a probabilis-\ntic perspective. MIT press.\nAidar Myrzakhan, Sondos Mahmoud Bsharat, and\nZhiqiang Shen. 2024. Open-llm-leaderboard: From\nmulti-choice to open-style questions for llms eval-\nuation, benchmark, and arena.\narXiv preprint\narXiv:2406.07545.\nNagarajan Natarajan, Inderjit S Dhillon, Pradeep K\nRavikumar, and Ambuj Tewari. 2013. Learning with\nnoisy labels. Advances in neural information pro-\ncessing systems, 26.\nNoel Ngu, Nathaniel Lee, and Paulo Shakarian. 2024.\nDiversity measures: Domain-independent proxies for\nfailure in language model queries. In 2024 IEEE 18th\nInternational Conference on Semantic Computing\n(ICSC), pages 176\u2013182. IEEE.\nCuong V Nguyen, Alessandro Achille, Michael Lam,\nTal Hassner, Vijay Mahadevan, and Stefano Soatto.\n2019.\nToward understanding catastrophic for-\ngetting in continual learning.\narXiv preprint\narXiv:1908.01091.\nQuan Nguyen and Adji Bousso Dieng. 2024. Quality-\nweighted vendi scores and their application to\ndiverse experimental design.\narXiv preprint\narXiv:2405.02449.\nVu-Linh Nguyen, Mohammad Hossein Shaker, and\nEyke H\u00fcllermeier. 2022. How to measure uncertainty\nin uncertainty sampling for active learning. Machine\nLearning, 111(1):89\u2013122.\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal,\nJason Weston, and Douwe Kiela. 2019. Adversarial\nnli: A new benchmark for natural language under-\nstanding. arXiv preprint arXiv:1910.14599.\nBj\u00f6rn Nieth, Thomas Altstidl, Leo Schwinn, and Bj\u00f6rn\nEskofier. 2024. Large-scale dataset pruning in adver-\nsarial training through data importance extrapolation.\narXiv preprint arXiv:2406.13283.\nGeir K Nilsen, Antonella Z Munthe-Kaas, Hans J\nSkaug, and Morten Brun. 2019. Efficient computa-\ntion of hessian matrices in tensorflow. arXiv preprint\narXiv:1905.05559.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in neural in-\nformation processing systems, 35:27730\u201327744.\nRui Pan, Jipeng Zhang, Xingyuan Pan, Renjie Pi, Xi-\naoyu Wang, and Tong Zhang. 2024. Scalebio: Scal-\nable bilevel optimization for llm data reweighting.\narXiv preprint arXiv:2406.19976.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311\u2013318.\nSung Min Park, Kristian Georgiev, Andrew Ilyas, Guil-\nlaume Leclerc, and Aleksander Madry. 2023. Trak:\nAttributing model behavior at scale. arXiv preprint\narXiv:2303.14186.\nJupinder Parmar, Sanjev Satheesh, Mostofa Patwary,\nMohammad Shoeybi, and Bryan Catanzaro. 2024.\nReuse, don\u2019t retrain: A recipe for continued pre-\ntraining of language models.\narXiv preprint\narXiv:2407.07263.\nAmey Pasarkar and Adji Bousso Dieng. 2023. Cousins\nof the vendi score: A family of similarity-based diver-\nsity metrics for science and machine learning. arXiv\npreprint arXiv:2310.12952.\nKeiran Paster, Marco Dos Santos, Zhangir Azerbayev,\nand Jimmy Ba. 2023.\nOpenwebmath: An open\ndataset of high-quality mathematical web text. arXiv\npreprint arXiv:2310.06786.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021.\nAre nlp models really able to solve\nsimple math word problems?\narXiv preprint\narXiv:2103.07191.\nLeena H Patil and Mohammed Atique. 2013. A novel\napproach for feature selection method tf-idf in doc-\nument clustering. In 2013 3rd IEEE international\nadvance computing conference (IACC), pages 858\u2013\n862. IEEE.\nMansheej Paul, Surya Ganguli, and Gintare Karolina\nDziugaite. 2021. Deep learning on a data diet: Find-\ning important examples early in training. Advances\nin neural information processing systems, 34:20596\u2013\n20607.\nBarak A. Pearlmutter. 1994. Fast exact multiplication\nby the hessian. Neural Computation, 6(1):147\u2013160.\nRobert K Peet. 1974. The measurement of species di-\nversity. Annual review of ecology and systematics,\npages 285\u2013307.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The refinedweb dataset\nfor falcon llm: outperforming curated corpora with\nweb data, and web data only.\narXiv preprint\narXiv:2306.01116.\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Gal-\nley, and Jianfeng Gao. 2023. Instruction tuning with\ngpt-4. arXiv preprint arXiv:2304.03277.\nLeif E Peterson. 2009. K-nearest neighbor. Scholarpe-\ndia, 4(2):1883.\nAgustin Picard, Lucas Hervier, Thomas Fel, and David\nVigouroux. 2024. Influenci\u00e6: A library for tracing\nthe influence back to the data-points. In World Con-\nference on Explainable Artificial Intelligence, pages\n193\u2013204. Springer.\nMaria Priestley, Fionnt\u00e1n O\u2019donnell, and Elena Simperl.\n2023. A survey of data quality requirements that\nmatter in ml development pipelines. ACM Journal of\nData and Information Quality, 15(2):1\u201339.\nGarima Pruthi, Frederick Liu, Satyen Kale, and Mukund\nSundararajan. 2020. Estimating training data influ-\nence by tracing gradient descent. Advances in Neural\nInformation Processing Systems, 33:19920\u201319930.\nYulei Qin, Xingyu Chen, Yunhang Shen, Chaoyou\nFu, Yun Gu, Ke Li, Xing Sun, and Rongrong Ji.\n2024. Capro: webly supervised learning with cross-\nmodality aligned prototypes. Advances in Neural\nInformation Processing Systems, 36.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\narXiv preprint arXiv:2112.11446.\nRafael Rafailov, Archit Sharma, Eric Mitchell, Christo-\npher D Manning, Stefano Ermon, and Chelsea Finn.\n2024. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in Neu-\nral Information Processing Systems, 36.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a unified text-to-text\ntransformer. Journal of machine learning research,\n21(140):1\u201367.\nK Raghuveer et al. 2012. Legal documents clustering\nusing latent dirichlet allocation. IAES Int. J. Artif.\nIntell, 2(1):34\u201337.\nXingcheng Ran, Yue Xi, Yonggang Lu, Xiangwen\nWang, and Zhenyu Lu. 2023. Comprehensive sur-\nvey on hierarchical clustering algorithms and the re-\ncent developments. Artificial Intelligence Review,\n56(8):8219\u20138264.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\narXiv preprint arXiv:1908.10084.\nAlfr\u00e9d R\u00e9nyi. 1961. On measures of entropy and in-\nformation. In Proceedings of the fourth Berkeley\nsymposium on mathematical statistics and probabil-\nity, volume 1: contributions to the theory of statistics,\nvolume 4, pages 547\u2013562. University of California\nPress.\nHamed Rezazadegan Tavakoli, Esa Rahtu, and Janne\nHeikkil\u00e4. 2011. Fast and efficient saliency detection\nusing sparse sampling and kernel density estimation.\nIn Image Analysis: 17th Scandinavian Conference,\nSCIA 2011, Ystad, Sweden, May 2011. Proceedings\n17, pages 666\u2013675. Springer.\nMarco Tulio Ribeiro, Tongshuang Wu, Carlos Guestrin,\nand Sameer Singh. 2020. Beyond accuracy: Behav-\nioral testing of nlp models with checklist.\narXiv\npreprint arXiv:2005.04118.\nBrian Richards. 1987. Type/token ratios: What do they\nreally tell us? Journal of child language, 14(2):201\u2013\n209.\nYuji Roh, Geon Heo, and Steven Euijong Whang.\n2019. A survey on data collection for machine learn-\ning: a big data-ai integration perspective.\nIEEE\nTransactions on Knowledge and Data Engineering,\n33(4):1328\u20131347.\nRajendra Kumar Roul, Omanwar Rohit Devanand, and\nSanjay Kumar Sahay. 2014. Web document cluster-\ning and ranking using tf-idf based apriori approach.\narXiv preprint arXiv:1406.5617.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99\u2013106.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nGayathri Saranathan, Mahammad Parwez Alam, James\nLim, Suparna Bhattacharya, Soon Yee Wong, Martin\nFoltin, and Cong Xu. Dele: Data efficient llm eval-\nuation. In ICLR 2024 Workshop on Navigating and\nAddressing Data Problems for Foundation Models.\nNikunj Saunshi, Arushi Gupta, Mark Braverman, and\nSanjeev Arora. 2022. Understanding influence func-\ntions and datamodels via harmonic analysis. In The\nEleventh International Conference on Learning Rep-\nresentations.\nTimo Schick and Hinrich Sch\u00fctze. 2020. It\u2019s not just\nsize that matters: Small language models are also\nfew-shot learners. arXiv preprint arXiv:2009.07118.\nAndrea Schioppa, Polina Zablotskaia, David Vilar, and\nArtem Sokolov. 2021. Scaling up influence functions.\nPreprint, arXiv:2112.03052.\nStephanie Schoch, Ritwick Mishra, and Yangfeng Ji.\n2023. Data selection for fine-tuning large language\nmodels using transferred shapley values.\narXiv\npreprint arXiv:2306.10165.\nSarah E Schwarm and Mari Ostendorf. 2005. Reading\nlevel assessment using support vector machines and\nstatistical language models. In Proceedings of the\n43rd annual meeting of the Association for Computa-\ntional Linguistics (ACL\u201905), pages 523\u2013530.\nOzan Sener and Silvio Savarese. 2017. Active learn-\ning for convolutional neural networks: A core-set\napproach. arXiv preprint arXiv:1708.00489.\nBurr Settles. 1995. Active learning literature survey.\nScience, 10(3):237\u2013304.\nBurr Settles. 2011. From theories to queries: Active\nlearning in practice. In Active learning and experi-\nmental design workshop in conjunction with AISTATS\n2010, pages 1\u201318. JMLR Workshop and Conference\nProceedings.\nClaude Elwood Shannon. 1948. A mathematical theory\nof communication. The Bell system technical journal,\n27(3):379\u2013423.\nClaude Elwood Shannon. 2001. A mathematical the-\nory of communication. ACM SIGMOBILE mobile\ncomputing and communications review, 5(1):3\u201355.\nYunfan Shao, Linyang Li, Zhaoye Fei, Hang Yan, Dahua\nLin, and Xipeng Qiu. 2024. Balanced data sampling\nfor language model training with clustering. arXiv\npreprint arXiv:2402.14526.\nBin Shen and Luo Si. 2010. Non-negative matrix factor-\nization clustering on multiple manifolds. In Proceed-\nings of the AAAI Conference on Artificial Intelligence,\nvolume 24, pages 575\u2013580.\nJonathan Richard Shewchuk et al. 1994. An introduc-\ntion to the conjugate gradient method without the\nagonizing pain.\nZhengxiang Shi and Aldo Lipani. 2023. Don\u2019t stop pre-\ntraining? make prompt-based fine-tuning powerful\nlearner. Advances in Neural Information Processing\nSystems, 36:5827\u20135849.\nZhengyan Shi, Adam X Yang, Bin Wu, Laurence Aitchi-\nson, Emine Yilmaz, and Aldo Lipani. 2024. Instruc-\ntion tuning with loss over instructions. arXiv preprint\narXiv:2405.14394.\nManli Shu, Jiongxiao Wang, Chen Zhu, Jonas Geiping,\nChaowei Xiao, and Tom Goldstein. 2023. On the ex-\nploitability of instruction tuning. Advances in Neural\nInformation Processing Systems, 36:61836\u201361856.\nRaphael Shu, Hideki Nakayama, and Kyunghyun Cho.\n2019. Generating diverse translations with sentence\ncodes. In Proceedings of the 57th annual meeting of\nthe association for computational linguistics, pages\n1823\u20131827.\nLuo Si and Jamie Callan. 2001. A statistical model\nfor scientific readability. In Proceedings of the tenth\ninternational conference on Information and knowl-\nedge management, pages 574\u2013576.\nAditya Siddhant and Zachary C Lipton. 2018. Deep\nbayesian active learning for natural language process-\ning: Results of a large-scale empirical study. arXiv\npreprint arXiv:1808.05697.\nFatimah Sidi, Payam Hassany Shariat Panahy, Lilly Suri-\nani Affendey, Marzanah A Jabar, Hamidah Ibrahim,\nand Aida Mustapha. 2012. Data quality: A survey\nof data quality dimensions. In 2012 International\nConference on Information Retrieval & Knowledge\nManagement, pages 300\u2013304. IEEE.\nStacy Silverman and Nan Bernstein Ratner. 2002. Mea-\nsuring lexical diversity in children who stutter: Ap-\nplication of vocd.\nJournal of fluency disorders,\n27(4):289\u2013304.\nEdward H Simpson. 1949. Measurement of diversity.\nnature, 163(4148):688\u2013688.\nKristina P Sinaga and Miin-Shen Yang. 2020. Unsu-\npervised k-means clustering algorithm. IEEE access,\n8:80716\u201380727.\nAnkur Sinha, Pekka Malo, and Kalyanmoy Deb. 2017.\nA review on bilevel optimization: From classical\nto evolutionary approaches and applications. IEEE\ntransactions on evolutionary computation, 22(2):276\u2013\n295.\nSamarth Sinha, Han Zhang, Anirudh Goyal, Yoshua\nBengio, Hugo Larochelle, and Augustus Odena. 2020.\nSmall-gan: Speeding up gan training using core-sets.\nIn International Conference on Machine Learning,\npages 9005\u20139015. PMLR.\nJohn Smith and Lisa Johnson. 2020.\nStrategies for\ndifficulty sampling providing diversity in datasets.\nJournal of Machine Learning Research, 10:100\u2013120.\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju\nShin, and Jae-Gil Lee. 2022. Learning from noisy\nlabels with deep neural networks: A survey. IEEE\ntransactions on neural networks and learning sys-\ntems, 34(11):8135\u20138153.\nBen Sorscher, Robert Geirhos, Shashank Shekhar, Surya\nGanguli, and Ari Morcos. 2022. Beyond neural scal-\ning laws: beating power law scaling via data pruning.\nAdvances in Neural Information Processing Systems,\n35:19523\u201319536.\nKaren Sparck Jones. 1972. A statistical interpretation\nof term specificity and its application in retrieval.\nJournal of documentation, 28(1):11\u201321.\nEleftherios Spyromitros-Xioufis, Symeon Papadopou-\nlos, Alexandru Lucian Ginsca, Adrian Popescu, Yian-\nnis Kompatsiaris, and Ioannis Vlahavas. 2015. Im-\nproving diversity in image search via supervised rel-\nevance scoring. In Proceedings of the 5th ACM on\nInternational Conference on Multimedia Retrieval,\npages 323\u2013330.\nKatherine Stasaski and Marti A Hearst. 2022. Semantic\ndiversity in dialogue with natural language inference.\narXiv preprint arXiv:2205.01497.\nKatherine Stasaski, Grace Hui Yang, and Marti A Hearst.\n2020. More diverse dialogue datasets via diversity-\ninformed data collection. In Proceedings of the 58th\nannual meeting of the association for computational\nlinguistics, pages 4958\u20134968.\nAlbert Yu Sun, Eliott Zemour, Arushi Saxena, Udith\nVaidyanathan, Eric Lin, Christian Lau, and Vaikkunth\nMugunthan. 2023. Does fine-tuning gpt-3 with the\nopenai api leak personally-identifiable information?\narXiv preprint arXiv:2307.16382.\nPeng Sun, Bei Shi, Daiwei Yu, and Tao Lin. 2024a. On\nthe diversity and realism of distilled dataset: An effi-\ncient dataset distillation paradigm. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 9390\u20139399.\nWangtao Sun, Haotian Xu, Xuanqing Yu, Pei Chen,\nShizhu He, Jun Zhao, and Kang Liu. 2024b.\nItd:\nLarge language models can teach them-\nselves induction through deduction. arXiv preprint\narXiv:2403.05789.\nJun Suzuki, Heiga Zen, and Hideto Kazawa. 2023. Ex-\ntracting representative subset from extensive text data\nfor training pre-trained language models. Informa-\ntion Processing & Management, 60(3):103249.\nLuis Talavera. 1999. Feature selection as a preprocess-\ning step for hierarchical clustering. In ICML, vol-\nume 99, pages 389\u2013397.\nHaoru Tan, Sitong Wu, Fei Du, Yukang Chen, Zhibin\nWang, Fan Wang, and Xiaojuan Qi. 2024a. Data\npruning via moving-one-sample-out. Advances in\nNeural Information Processing Systems, 36.\nJiejun Tan, Zhicheng Dou, Yutao Zhu, Peidong Guo,\nKun Fang, and Ji-Rong Wen. 2024b. Small models,\nbig insights: Leveraging slim proxy models to decide\nwhen and what to retrieve for llms. arXiv preprint\narXiv:2402.12052.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B Hashimoto. 2023.\nAlpaca: A\nstrong, replicable instruction-following model. Stan-\nford Center for Research on Foundation Models.\nhttps://crfm. stanford. edu/2023/03/13/alpaca. html,\n3(6):7.\nMosaicML NLP Team. 2023. Introducing mpt-7b: A\nnew standard for open-source, commercially usable\nllms. Accessed: 2023-05-05.\nMildred C Templin. 1957. Certain language skills in\nchildren; their development and interrelationships.\nUniversity of Minnesota Press.\nGuy Tevet and Jonathan Berant. 2020. Evaluating the\nevaluation of diversity in natural language generation.\narXiv preprint arXiv:2004.02990.\nSteve Tingiris and Bret Kinsella. 2021. Exploring GPT-\n3: An unofficial first look at the general-purpose\nlanguage processing API from OpenAI. Packt Pub-\nlishing Ltd.\nKushal Tirumala, Aram Markosyan, Luke Zettlemoyer,\nand Armen Aghajanyan. 2022. Memorization with-\nout overfitting: Analyzing the training dynamics of\nlarge language models. Advances in Neural Informa-\ntion Processing Systems, 35:38274\u201338290.\nKushal Tirumala, Daniel Simig, Armen Aghajanyan,\nand Ari Morcos. 2024. D4: Improving llm pretrain-\ning via document de-duplication and diversification.\nAdvances in Neural Information Processing Systems,\n36.\nMariya Toneva, Alessandro Sordoni, Remi Tachet des\nCombes, Adam Trischler, Yoshua Bengio, and Geof-\nfrey J Gordon. 2018. An empirical study of exam-\nple forgetting during deep neural network learning.\narXiv preprint arXiv:1812.05159.\nSimon Tong and Daphne Koller. 2001. Support vec-\ntor machine active learning with applications to text\nclassification. Journal of machine learning research,\n2(Nov):45\u201366.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal\nAzhar, et al. 2023a.\nLlama:\nOpen and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023b.\nLlama 2: Open founda-\ntion and fine-tuned chat models.\narXiv preprint\narXiv:2307.09288.\nKarina Vidal and Scott Jarvis. 2020. Effects of english-\nmedium instruction on spanish students\u2019 proficiency\nand lexical diversity in english. Language Teaching\nResearch, 24(5):568\u2013587.\nUlrike Von Luxburg. 2007. A tutorial on spectral clus-\ntering. Statistics and computing, 17:395\u2013416.\nChi Wang, Qingyun Wu, Silu Huang, and Amin Saied.\n2021a. Economic hyperparameter optimization with\nblended search strategy. In International Conference\non Learning Representations.\nChi Wang, Qingyun Wu, Markus Weimer, and Erkang\nZhu. 2021b. Flaml: A fast and lightweight automl\nlibrary. Proceedings of Machine Learning and Sys-\ntems, 3:434\u2013447.\nJiahao Wang, Bolin Zhang, Qianlong Du, Jiajun Zhang,\nand Dianhui Chu. 2024a.\nA survey on data se-\nlection for llm instruction tuning. arXiv preprint\narXiv:2402.05123.\nLingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai\nWong, and Georg Gottlob. 2024b.\nSelective for-\ngetting: Advancing machine unlearning techniques\nand evaluation in language models. arXiv preprint\narXiv:2402.05813.\nYidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi\nYang, Cunxiang Wang, Hao Chen, Chaoya Jiang,\nRui Xie, Jindong Wang, Xing Xie, et al. 2023a.\nPandalm: An automatic evaluation benchmark for\nllm instruction tuning optimization. arXiv preprint\narXiv:2306.05087.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage models with self-generated instructions. arXiv\npreprint arXiv:2212.10560.\nYu-Xiong Wang and Yu-Jin Zhang. 2012. Nonnegative\nmatrix factorization: A comprehensive review. IEEE\nTransactions on knowledge and data engineering,\n25(6):1336\u20131353.\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xing-\nshan Zeng, Wenyong Huang, Lifeng Shang, Xin\nJiang, and Qun Liu. 2023b.\nAligning large lan-\nguage models with human: A survey. arXiv preprint\narXiv:2307.12966.\nYulin Wang, Gao Huang, Shiji Song, Xuran Pan, Yi-\ntong Xia, and Cheng Wu. 2021c. Regularizing deep\nnetworks with semantic data augmentation. IEEE\nTransactions on Pattern Analysis and Machine Intel-\nligence, 44(7):3733\u20133748.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nMax Welling. 2009. Herding dynamical weights to\nlearn. In Proceedings of the 26th annual interna-\ntional conference on machine learning, pages 1121\u2013\n1128.\nAlexander Wettig, Aatmik Gupta, Saumya Malik, and\nDanqi Chen. 2024. Qurating: Selecting high-quality\ndata for training language models. arXiv preprint\narXiv:2402.09739.\nSvante Wold, Kim Esbensen, and Paul Geladi. 1987.\nPrincipal component analysis. Chemometrics and\nintelligent laboratory systems, 2(1-3):37\u201352.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz,\net al. 2019. Huggingface\u2019s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\nHaolun Wu, Yansen Zhang, Chen Ma, Fuyuan Lyu, Fer-\nnando Diaz, and Xue Liu. 2022. A survey of diver-\nsification techniques in search and recommendation.\nCoRR arXiv, 2212.\nHaolun Wu, Yansen Zhang, Chen Ma, Fuyuan Lyu,\nBowei He, Bhaskar Mitra, and Xue Liu. 2024. Re-\nsult diversification in search and recommendation: A\nsurvey. IEEE Transactions on Knowledge and Data\nEngineering.\nShengguang Wu, Keming Lu, Benfeng Xu, Junyang Lin,\nQi Su, and Chang Zhou. 2023. Self-evolved diverse\ndata sampling for efficient instruction tuning. arXiv\npreprint arXiv:2311.08182.\nMengzhou Xia, Tianyu Gao, Zhiyuan Zeng, and Danqi\nChen. 2023. Sheared llama: Accelerating language\nmodel pre-training via structured pruning.\narXiv\npreprint arXiv:2310.06694.\nMengzhou Xia, Sadhika Malladi, Suchin Gururangan,\nSanjeev Arora, and Danqi Chen. 2024a. Less: Se-\nlecting influential data for targeted instruction tuning.\narXiv preprint arXiv:2402.04333.\nXiaobo Xia, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and\nTongliang Liu. 2022. Moderate coreset: A universal\nmethod of data selection for real-world data-efficient\ndeep learning. In The Eleventh International Confer-\nence on Learning Representations.\nXiaobo Xia, Jiale Liu, Shaokun Zhang, Qingyun Wu,\nHongxin Wei, and Tongliang Liu. 2024b. Refined\ncoreset selection: Towards minimal coreset size un-\nder model performance constraints. In Forty-first\nInternational Conference on Machine Learning.\nQizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and\nQuoc Le. 2020. Unsupervised data augmentation for\nconsistency training. Advances in neural information\nprocessing systems, 33:6256\u20136268.\nSang Michael Xie, Aditi Raghunathan, Percy Liang, and\nTengyu Ma. 2021. An explanation of in-context learn-\ning as implicit bayesian inference. arXiv preprint\narXiv:2111.02080.\nSang Michael Xie, Shibani Santurkar, Tengyu Ma, and\nPercy S Liang. 2023. Data selection for language\nmodels via importance resampling.\nAdvances in\nNeural Information Processing Systems, 36:34201\u2013\n34227.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023a.\nWizardlm: Empowering large lan-\nguage models to follow complex instructions. arXiv\npreprint arXiv:2304.12244.\nFrank F Xu, Uri Alon, Graham Neubig, and Vincent Jo-\nsua Hellendoorn. 2022. A systematic evaluation of\nlarge language models of code. In Proceedings of\nthe 6th ACM SIGPLAN International Symposium on\nMachine Programming, pages 1\u201310.\nYang Xu, Yongqiang Yao, Yufan Huang, Mengnan\nQi, Maoquan Wang, Bin Gu, and Neel Sundaresan.\n2023b. Rethinking the instruction quality: Lift is\nwhat you need. arXiv preprint arXiv:2312.11508.\nZhangchen Xu, Fengqing Jiang, Luyao Niu, Yun-\ntian Deng, Radha Poovendran, Yejin Choi, and\nBill Yuchen Lin. 2024. Magpie: Alignment data\nsynthesis from scratch by prompting aligned llms\nwith nothing. arXiv preprint arXiv:2406.08464.\nAn Yang, Baosong Yang, Binyuan Hui, Bo Zheng,\nBowen Yu, Chang Zhou, Chengpeng Li, Chengyuan\nLi, Dayiheng Liu, Fei Huang, et al. 2024. Qwen2\ntechnical report. arXiv preprint arXiv:2407.10671.\nYaoqing Yang, Ryan Theisen, Liam Hodgkinson,\nJoseph E Gonzalez, Kannan Ramchandran, Charles H\nMartin, and Michael W Mahoney. 2022. Evaluating\nnatural language processing models with generaliza-\ntion metrics that do not need access to any training\nor testing data. arXiv preprint arXiv:2202.02842.\nEmmanuel J Yannakoudakis and David Fawthrop. 1983.\nThe rules of spelling errors. Information Processing\n& Management, 19(2):87\u201399.\nGregory Yauney, Emily Reif, and David Mimno.\n2023.\nData similarity is not enough to explain\nlanguage model performance.\narXiv preprint\narXiv:2311.09006.\nJingwen Ye, Ruonan Yu, Songhua Liu, and Xinchao\nWang. 2024. Distilled datamodel with reverse gra-\ndient matching. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 11954\u201311963.\n\u00c7a\u02d8\ngatay Y\u0131ld\u0131z, Nishaanth Kanna Ravichandran, Pr-\nishruit Punia, Matthias Bethge, and Beyza Ermis.\n2024. Investigating continual pretraining in large\nlanguage models: Insights and implications. arXiv\npreprint arXiv:2402.17400.\nJulio Christian Young and Makoto Shishido. 2023. In-\nvestigating openai\u2019s chatgpt potentials in generating\nchatbot\u2019s dialogue for english as a foreign language\nlearning. International journal of advanced com-\nputer science and applications, 14(6).\nZichun Yu, Spandan Das, and Chenyan Xiong. 2024.\nMates: Model-aware data selection for efficient pre-\ntraining with data influence models. arXiv preprint\narXiv:2406.06046.\nBeverly L Zakaluk and S Jay Samuels. 1988. Readabil-\nity: Its Past, Present, and Future. ERIC.\nZhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya\nGoyal, and Danqi Chen. 2023.\nEvaluating large\nlanguage models at evaluating instruction following.\narXiv preprint arXiv:2310.07641.\nDaochen Zha, Zaid Pervaiz Bhat, Kwei-Herng Lai, Fan\nYang, Zhimeng Jiang, Shaochen Zhong, and Xia Hu.\n2023. Data-centric artificial intelligence: A survey.\narXiv preprint arXiv:2303.10158.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin\nRecht, and Oriol Vinyals. 2021.\nUnderstanding\ndeep learning (still) requires rethinking generaliza-\ntion. Communications of the ACM, 64(3):107\u2013115.\nChiyuan Zhang, Daphne Ippolito, Katherine Lee,\nMatthew Jagielski, Florian Tram\u00e8r, and Nicholas Car-\nlini. 2023a. Counterfactual memorization in neural\nlanguage models. Advances in Neural Information\nProcessing Systems, 36:39321\u201339362.\nJipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong\nZhang, Rui Pan, and Tong Zhang. 2024a.\nTag-\ncos: Task-agnostic gradient clustered coreset se-\nlection for instruction tuning data. arXiv preprint\narXiv:2407.15235.\nLei Zhang. 2024. Bilevel optimization in the deep learn-\ning era: Methods and applications.\nShaokun Zhang, Xiaobo Xia, Zhaoqing Wang, Ling-\nHao Chen, Jiale Liu, Qingyun Wu, and Tongliang\nLiu. 2023b. Ideal: Influence-driven selective annota-\ntions empower in-context learners in large language\nmodels. arXiv preprint arXiv:2310.10873.\nShengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang,\nXiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tian-\nwei Zhang, Fei Wu, et al. 2023c. Instruction tuning\nfor large language models: A survey. arXiv preprint\narXiv:2308.10792.\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q\nWeinberger, and Yoav Artzi. 2019. Bertscore: Eval-\nuating text generation with bert.\narXiv preprint\narXiv:1904.09675.\nXiao Zhang and Ji Wu. 2024. Dissecting learning and\nforgetting in language model finetuning.\nIn The\nTwelfth International Conference on Learning Repre-\nsentations.\nXiaoying Zhang, Baolin Peng, Ye Tian, Jingyan Zhou,\nYipeng Zhang, Haitao Mi, and Helen Meng. 2024b.\nSelf-tuning: Instructing llms to effectively acquire\nnew knowledge through self-teaching. arXiv preprint\narXiv:2406.06326.\nYifan Zhang, Yifan Luo, Yang Yuan, and Andrew C\nYao. 2024c. Autonomous data selection with lan-\nguage models for mathematical texts. In ICLR 2024\nWorkshop on Navigating and Addressing Data Prob-\nlems for Foundation Models.\nYihua Zhang, Yuguang Yao, Parikshit Ram, Pu Zhao,\nTianlong Chen, Mingyi Hong, Yanzhi Wang, and\nSijia Liu. 2022. Advancing model pruning via bi-\nlevel optimization. Advances in Neural Information\nProcessing Systems, 35:18309\u201318326.\nBo Zhao and Hakan Bilen. 2023. Dataset condensa-\ntion with distribution matching. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of\nComputer Vision, pages 6514\u20136523.\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen.\n2020a. Dataset condensation with gradient matching.\narXiv preprint arXiv:2006.05929.\nBo Zhao, Konda Reddy Mopuri, and Hakan Bilen.\n2021. Dataset condensation with gradient matching.\nPreprint, arXiv:2006.05929.\nChenyang Zhao, Xueying Jia, Vijay Viswanathan,\nTongshuang Wu,\nand Graham Neubig. 2024a.\nSelf-guide: Better task-specific instruction follow-\ning via self-synthetic finetuning.\narXiv preprint\narXiv:2407.12874.\nDora Zhao, Jerone TA Andrews, Orestis Papakyri-\nakopoulos, and Alice Xiang. 2024b. Position: Mea-\nsure dataset diversity, don\u2019t just claim it.\narXiv\npreprint arXiv:2407.08188.\nDorothy Zhao,\nJerone TA Andrews,\nAI Sony,\nTokyo Orestis Papakyriakopoulos, and Alice Xiang.\n2024c. Measuring diversity in datasets. Interna-\ntional Conference on Learning Representations.\nShanshan Zhao, Mingming Gong, Tongliang Liu, Huan\nFu, and Dacheng Tao. 2020b. Domain generaliza-\ntion via entropy regularization. Advances in neural\ninformation processing systems, 33:16096\u201316107.\nShuaijiang Zhao and Xiaoquan Fang. 2024. Technical\nreport: Competition solution for bettermixture. arXiv\npreprint arXiv:2403.13233.\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan\nZhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin,\nZhuohan Li, Dacheng Li, Eric Xing, et al. 2024.\nJudging llm-as-a-judge with mt-bench and chatbot\narena. Advances in Neural Information Processing\nSystems, 36.\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu\nJiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and\nJiawei Han. 2022.\nTowards a unified multi-\ndimensional evaluator for text generation.\narXiv\npreprint arXiv:2210.07197.\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer,\nJiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping\nYu, Lili Yu, et al. 2024a. Lima: Less is more for\nalignment. Advances in Neural Information Process-\ning Systems, 36.\nDaquan Zhou, Kai Wang, Jianyang Gu, Xiangyu Peng,\nDongze Lian, Yifan Zhang, Yang You, and Jiashi\nFeng. 2023. Dataset quantization. In Proceedings\nof the IEEE/CVF International Conference on Com-\nputer Vision, pages 17205\u201317216.\nJianghong Zhou,\nEugene Agichtein,\nand Surya\nKallumadi. 2020. Diversifying multi-aspect search\nresults using simpson\u2019s diversity index. In Proceed-\nings of the 29th ACM International conference on\ninformation & knowledge management, pages 2345\u2013\n2348.\nYuhan Zhou, Fengjiao Tu, Kewei Sha, Junhua Ding,\nand Haihua Chen. 2024b. A survey on data quality\ndimensions and tools for machine learning. arXiv\npreprint arXiv:2406.19614.\nLianghui Zhu, Xinggang Wang, and Xinlong Wang.\n2023.\nJudgelm:\nFine-tuned large language\nmodels are scalable judges.\narXiv preprint\narXiv:2310.17631.\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan\nZhang, Jun Wang, and Yong Yu. 2018. Texygen: A\nbenchmarking platform for text generation models.\nIn The 41st international ACM SIGIR conference\non research & development in information retrieval,\npages 1097\u20131100.\n",
    "ref": [
        "2303.09540",
        "2303.08774",
        "2402.16827",
        "2401.13229",
        "2405.20541",
        "2407.14985",
        "2406.09334",
        "2402.17327",
        "2405.12186",
        "2309.16609",
        "2203.14544",
        "1512.02985",
        "2006.14651",
        "2401.06692",
        "2312.06254",
        "2310.13032",
        "2311.14736",
        "2403.16898",
        "1702.05962",
        "2307.06290",
        "2308.07201",
        "2105.10446",
        "2305.09246",
        "2403.12776",
        "2307.08701",
        "2406.14491",
        "2405.20456",
        "2311.09783",
        "2109.06379",
        "1810.04805",
        "2305.14233",
        "2002.06305",
        "2406.13542",
        "2304.06767",
        "2311.15653",
        "1903.09722",
        "2401.12926",
        "2306.11670",
        "2007.01852",
        "2105.03075",
        "1806.03884",
        "2402.05119",
        "2308.03296",
        "2306.11644",
        "2303.08114",
        "2203.15556",
        "2403.02839",
        "2403.08763",
        "2202.00622",
        "2210.14177",
        "2310.06825",
        "2401.04088",
        "2401.06059",
        "2306.02031",
        "2402.05356",
        "2406.14026",
        "2402.01865",
        "2110.08534",
        "2207.05221",
        "2001.08361",
        "2104.14337",
        "2405.06331",
        "2303.14753",
        "2305.11383",
        "2311.00288",
        "2110.14049",
        "2003.08529",
        "1904.03122",
        "2306.13840",
        "2107.06499",
        "2402.13064",
        "1510.03055",
        "2402.00530",
        "2308.12032",
        "2305.06161",
        "2308.06259",
        "2302.12366",
        "2310.07849",
        "2211.09110",
        "2109.07958",
        "2401.17197",
        "2401.08565",
        "2311.10774",
        "2404.07840",
        "2312.15685",
        "2403.09606",
        "2402.18041",
        "1907.11692",
        "2406.14115",
        "2302.04062",
        "2203.08242",
        "2309.04564",
        "1707.05589",
        "2208.01545",
        "2008.03964",
        "2005.00816",
        "2403.00526",
        "2406.07545",
        "1908.01091",
        "2405.02449",
        "1910.14599",
        "2406.13283",
        "1905.05559",
        "2406.19976",
        "2303.14186",
        "2407.07263",
        "2310.12952",
        "2310.06786",
        "2103.07191",
        "2306.01116",
        "2304.03277",
        "2112.11446",
        "1908.10084",
        "2005.04118",
        "2110.08207",
        "2009.07118",
        "2112.03052",
        "2306.10165",
        "1708.00489",
        "2402.14526",
        "2405.14394",
        "1808.05697",
        "2205.01497",
        "2307.16382",
        "2403.05789",
        "2402.12052",
        "2004.02990",
        "1812.05159",
        "2302.13971",
        "2307.09288",
        "2402.05123",
        "2402.05813",
        "2306.05087",
        "2212.10560",
        "2307.12966",
        "2109.01652",
        "2402.09739",
        "1910.03771",
        "2311.08182",
        "2310.06694",
        "2402.04333",
        "2111.02080",
        "2304.12244",
        "2312.11508",
        "2406.08464",
        "2407.10671",
        "2202.02842",
        "2311.09006",
        "2402.17400",
        "2406.06046",
        "2310.07641",
        "2303.10158",
        "2407.15235",
        "2310.10873",
        "2308.10792",
        "1904.09675",
        "2406.06326",
        "2006.05929",
        "2006.05929",
        "2407.12874",
        "2407.08188",
        "2403.13233",
        "2210.07197",
        "2406.19614",
        "2310.17631"
    ]
}