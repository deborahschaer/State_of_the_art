{
    "id": "2409.09957",
    "title": "Deep Graph Anomaly Detection: A Survey and New Perspectives",
    "abstract": "Graph anomaly detection (GAD), which aims to identify unusual graph instances\n(nodes, edges, subgraphs, or graphs), has attracted increasing attention in\nrecent years due to its significance in a wide range of applications. Deep\nlearning approaches, graph neural networks (GNNs) in particular, have been\nemerging as a promising paradigm for GAD, owing to its strong capability in\ncapturing complex structure and/or node attributes in graph data. Considering\nthe large number of methods proposed for GNN-based GAD, it is of paramount\nimportance to summarize the methodologies and findings in the existing GAD\nstudies, so that we can pinpoint effective model designs for tackling open GAD\nproblems. To this end, in this work we aim to present a comprehensive review of\ndeep learning approaches for GAD. Existing GAD surveys are focused on\ntask-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique\nchallenges in GAD. To fill this gap, we first discuss the problem complexities\nand their resulting challenges in GAD, and then provide a systematic review of\ncurrent deep GAD methods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD, and graph anomaly\nmeasures. To deepen the discussions, we further propose a taxonomy of 13\nfine-grained method categories under these three perspectives to provide more\nin-depth insights into the model designs and their capabilities. To facilitate\nthe experiments and validation, we also summarize a collection of widely-used\nGAD datasets and empirical comparison. We further discuss multiple open\nproblems to inspire more future high-quality research. A continuously updated\nrepository for datasets, links to the codes of algorithms, and empirical\ncomparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.",
    "date": "2024-09-16T03:05:11+00:00",
    "fulltext": "AUGUST 2024\n1\nDeep Graph Anomaly Detection: A Survey and\nNew Perspectives\nHezhe Qiao, Hanghang Tong Fellow, IEEE, Bo An Senior Member, IEEE, Irwin King Fellow, IEEE, Charu\nAggarwal Fellow, IEEE, Guansong Pang Member, IEEE\nAbstract\u2014Graph anomaly detection (GAD), which aims to identify unusual graph instances (e.g., nodes, edges, subgraphs, or graphs),\nhas attracted increasing attention in recent years due to its significance in a wide range of applications. Deep learning approaches, graph\nneural networks (GNNs) in particular, have been emerging as a promising paradigm for GAD, owing to its strong capability in capturing\ncomplex structure and/or node attributes in graph data. Considering the large number of methods proposed for GNN-based GAD, it is of\nparamount importance to summarize the methodologies and findings in the existing GAD studies, so that we can pinpoint effective model\ndesigns for tackling open GAD problems. To this end, in this work we aim to present a comprehensive review of deep learning\napproaches for GAD. Existing GAD surveys are focused on task-specific discussions, making it difficult to understand the technical\ninsights of existing methods and their limitations in addressing some unique challenges in GAD. To fill this gap, we first discuss the\nproblem complexities and their resulting challenges in GAD, and then provide a systematic review of current deep GAD methods from\nthree novel perspectives of methodology, including GNN backbone design, proxy task design for GAD, and graph anomaly measures. To\ndeepen the discussions, we further propose a taxonomy of 13 fine-grained method categories under these three perspectives to provide\nmore in-depth insights into the model designs and their capabilities. To facilitate the experiments and validation of the GAD methods, we\nalso summarize a collection of widely-used datasets for GAD and empirical performance comparison on these datasets. We further\ndiscuss multiple important open research problems in GAD to inspire more future high-quality research in this area. A continuously\nupdated repository for GAD datasets, links to the codes of GAD algorithms, and empirical comparison is available at\nhttps://github.com/mala-lab/Awesome-Deep-Graph-Anomaly-Detection.\nIndex Terms\u2014Graph Anomaly Detection, Graph Neural Networks, Deep Learning, Anomaly Detection, Graph Representation Learning\n\u2726\n1\nINTRODUCTION\nGraph anomaly detection (GAD) aims to identify graph\ninstances (e.g., node, edge, sub-graph, and graph) that\ndo not conform with the normal regime. It has been an\nactive research area with wide application in detecting\nabnormal instances in a variety of graph/network data, e.g.,\nabusive user behaviors in online user networks, fraudulent\nactivities in financial networks, and spams in social networks.\nFurthermore, since the relations between data samples can be\nmodeled as similarity graphs, one can also use GAD methods\nto discover anomalies in any set of data objects (as long as\nan appropriate pairwise similarity function is available).\nDue to the complex structure of graphs, traditional\nanomaly detection methods cannot be directly applied to\ngraph data. In recent years, graph neural networks (GNNs)\nhave shown promising capabilities in modeling and learning\nthe representation of graphs by capturing structural patterns,\ninspiring a large number of GNN-based approaches for GAD.\nHowever, the popular GNN designs, such as aggregation\nof node representations and optimization objectives, may\nlead to over-smoothing, indistinguishable representations of\n\u2022\nHezhe Qiao and Guansong Pang are with School of Computing and\nInformation Systems, Singapore Management University. Hanghang\nTong is with Department of Computer Science, University of Illinois\nat Urbana-Champaign. Bo An is with College of Computing and\nData Science, Nanyang Technological University. Irwin King is with\nDepartment of Computer Science & Engineering, Chinese University of\nHong Kong. Charu Aggarwal is with IBM T. J. Watson Research Center.\nCorresponding author: G. Pang (gspang@smu.edu.sg)\nnormal and abnormal graph instances, which significantly\nlimits their applications in real-world use cases. Many novel\nGNN-based approaches specifically designed for GAD have\nbeen proposed to tackle the these challenges. In this work,\nto summarize the current methodologies and findings, we\nprovide a systematic and comprehensive review of current\ndeep GAD techniques and how they may tackle various types\nof challenges in GAD. We also propose several important\nopen research problems in GAD to inspire more future\nresearch in this area.\nRelated surveys. There have been several reviews on\nanomaly detection in recent years, e.g., [2], [4], [81], [89], [103],\n[113], but most of them are focused on non-deep-learning-\nbased methods for GAD [2], [4], [103], or on general data\nrather than graph data [7], [89]. The studies [81], [113] are on\ndeep GAD, but the reviews are limited to a relatively narrow\npoint of view. For example, Ma et al. [81] focus on task-\nspecific discussions, with limited reviews on the technical\ndevelopment, while Liu et al. [64] and Tang et al. [113] focus\non establishing a performance benchmark for unsupervised\nand supervised GAD methods respectively. Another related\nwork is Liu et al. [71], but it is restricted to supervised\nimbalanced graph learning. Although these surveys provide\nuseful guidelines for the development of methods for GAD,\nit is difficult to understand the technical insights of existing\nmethods and their limitations in addressing some unique\nchallenges in GAD.\nOur work. To fill this gap, we aim to offer a distinctive\nreview on GAD to discuss these insights, the limitations,\nand the future research opportunities in this crucial topic.\narXiv:2409.09957v1  [cs.LG]  16 Sep 2024\nAUGUST 2024\n2\nSpecifically, we start with the discussion on the problem\ncomplexities and their resulting unique challenges in GAD.\nWe then provide a systematic review of current deep GAD\nmethods from three novel perspectives of methodology,\nincluding GNN backbone design, proxy task design for GAD,\nand graph anomaly measures. To deepen the discussions,\nwe further propose a taxonomy of 13 fine-grained method\ncategories under these three perspectives to provide more in-\ndepth insights into the model designs and their capabilities.\nTo facilitate the experiments and validation of the GAD\nmethods, we also summarize a collection of widely-used\ndatasets for GAD and empirical performance comparison on\nthese datasets. A comparison of our work to these related\nsurveys is summarized in Table 1.\nIn summary, our major contributions are as follows:\n\u2022\nThe survey provides important insights into the\nproblem complexities and the resulting challenges\nthat are unique for the task of GAD (Sec. 2).\n\u2022\nWe introduce a novel taxonomy of current deep GAD\nmethods, which offers in-depth understanding of the\nmethods from three technical design perspectives,\nincluding GNN backbone design, proxy task design,\nand graph anomaly measures (Sec. 3).\n\u2022\nWe then introduce 13 fine-grained method categories\nunder these three perspectives to provide more in-\ndepth insights into the model designs (i.e., key intu-\nition, assumption, learning objectives, advantages and\ndisadvantages) and their capabilities in addressing\nthe unique challenges in GAD (Secs. 4, 5, and 6).\n\u2022\nWe further discuss multiple important future research\ndirections that involve largely unsolved open prob-\nlems in GAD. Solutions in these directions would\nopen up new opportunities for addressing the unique\nchallenges in GAD (Sec. 7).\n\u2022\nWe also summarize a large number of representative\ndeep GAD methods from the 13 categories and a\nlarge collection of GAD benchmark datasets, and\nfurther provide quantitative comparison results on\nthese datasets (Appendices A, B, and C).\n2\nPROBLEMS AND CHALLENGES IN GAD\nThis section discusses some unique complexities and chal-\nlenges in GAD.\n2.1\nMajor Problem Complexities\nThe complexities in GAD can be summarized in two ways.\nOne source of the complexities lies in some inherent charac-\nteristics of graph data.\n\u2022\nP1. Structural dependency. The samples are typically\ncorrelated/connected with each other instead of\nbeing independent. The connections are of different\nsemantics, e.g., it could be a purchase relationship\nin a social network, or a citation relationship in a\ncitation network. The complexity of graph structure\nis reflected in connectivity patterns, dependency, or\ninfluence at different levels of graph data, which play\na significant role in defining what is abnormal on\ngraphs [4]. For example, different from i.i.d. data,\nwhere the anomalies are independent of the context,\nthe anomalies in graphs often depend on the context\nof a graph data instance, e.g., the neighboring nodes\nof a node. Anomalies may be considered as normal\nin one context but abnormal in another.\n\u2022\nP2. Diverse types of graph. There are many types\nof graphs in the real world, each serving different\npurposes and applications. Graphs can be categorized\ninto static and dynamic types, depending on whether\nthey change over time. It can also be divided into\nheterophilic and homophilic graphs according to the\ntype of connection [154], [163]. The definition of\nanomaly in one type of graph can differ significantly\nfrom that in other types of graph. In particular, a\ngraph instance (e.g., node/edge/graph) that is clearly\nabnormal in a dynamic graph at a specific time step\n(i.e., a static graph) can demonstrate strong normality\nwhen looking at the evolution of the graph; similarly,\nwe can have opposite abnormality of a graph instance\nin a homophilic graph vs. in a heterophilic graph.\nDealing with diverse types of graphs requires the\nGAD methods to adapt its learning strategy based on\nits unique properties of graphs.\n\u2022\nP3. Computational complexity in handling large-\nscale graphs. With the increasing amount of online\ndata, modern applications can include very large-scale\ngraph data with millions/billions of nodes and/or\nedges [43], [48], [98], [113], such as those in web-scale\nsocial networks, financial transaction networks, cyber\nnetworks, user-product e-commerce networks, and\ncitation networks. To identify anomalies using global\nstructural contexts, it is essential to consider the full\ngraph structural information, or a large proportion\nof the structural relations. The key complexity here\nis to deal with the time and space complexities when\nloading such large-scale structural relation data.\nAnother source is from the variety of graph abnormalities.\n\u2022\nP4. Diverse graph anomaly instances. In contrast to\nanomaly detection in other forms of data, anomalies\nwithin graph data can arise from different compo-\nnents, such as nodes, edges, sub-graphs, or the entire\ngraph [81]. Moreover, graph anomalies can manifest\nthemselves in diverse ways, depending on the struc-\nture and attribute information of graph data. This\nhighlights the need for GAD methods to incorporate\na range of techniques focused on identifying irregular\npatterns across nodes, edges, subgraphs, and the\nentirety of the graph.\n\u2022\nP5. Large variation in graph abnormality. Anomalies\nin graphs can manifest in different forms, including\nabnormality in graph attributes, graph structure, or\nthe composition of graph attributes and structure\n[4]. Some exemplars include attribute anomalies\n(i.e., graph instances that are exceptional in a graph\nattribute set) [3], [89], structural anomalies (i.e., graph\ninstances that connect different communities, forming\ndense connections with others) [18], [81], contextual\nanomalies (i.e., graph instances that have different\nattribute values compared to other nodes in the same\ncommunity) [18], [81], and local affinity anomalies\n(i.e., graph instances that demonstrate significantly\nAUGUST 2024\n3\nTABLE 1: A comparison of our work to existing surveys on anomaly detection.\nGeneric Data\nGraph Data\nGAD Perspectives\nEmpirical Evaluation\nSurvey\nYear\n-\nNode\nEdge\nGraph\nGNN Backbone\nProxy Task\nAnomaly Measures\nDataset\nCode\nComparison\nAggarwal et al. [2]\n2014\n\u2022\n\u2022\nAkoglu et al. [4]\n2015\n\u2022\n\u2022\n\u2022\n\u2022\nRanshous et al. [103]\n2015\n\u2022\n\u2022\n\u2022\n\u2022\nYu et al. [139]\n2016\n\u2022\nPourhabibi et al. [97]\n2020\n\u2022\n\u2022\nBoukerche et al. [7]\n2020\n\u2022\nMa et al. [81]\n2021\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nPang et al. [89]\n2021\n\u2022\n\u2022\n\u2022\n\u2022\nLiu et al. [64]\n2022\n\u2022\n\u2022\n\u2022\n\u2022\nTang et al. [113]\n2023\n\u2022\n\u2022\n\u2022\n\u2022\nLiu et al. [71]\n2023\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nOurs\n2024\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\n\u2022\nweak affinity to their connected instances compared\nto other instances [69], [98]). Also, graph abnormality\nmay vary from a local context to a global context,\ne.g., the node attributes in a 1-hop neighborhood\nvs. that in the full graph. This can lead to a highly\ncomplex composition set of graph abnormalities, i.e.,\nabnormality in attributes, structure, or both attributes\nand structure conditioned on a certain context, having\nvery different definitions to the conventional anomaly\ntypes \u2013 point, conditional, and group anomalies [3],\n[11], [89] \u2013 in general AD.\nIn addition, there are some complexities inherited from\ngeneral AD but amplified in GAD:\n\u2022\nP6. Unknowingness of abnormality. Many abnormal-\nities remain unknown until they actually occur [89],\nin which no prior knowledge about the abnormality\nis available for the modeling. Further, one type of\nabnormality can show very different behaviors to\nthe other types of abnormality in a single graph, e.g.,\nthe heterogeneous abnormality in graph attributes vs.\nthat in graph structure, in a node set vs. in an edge\nor subgraph set, in a 1-hop local structure context vs.\nin a higher-order structural context. Thus, knowing\none or more types of graph anomalies may not be\ngeneralize to the other types of anomalies.\n\u2022\nP7. Data imbalance. Due to the rare occurrence of\nanomalies, there is typically a large sample imbalance\nbetween normal and anomaly classes [3], [11], [89].\nThis also applies to GAD, but this complexity is\nlargely amplified in GAD due to potential long-tailed\ndistributions in graph structure/attributes [2], [71], in\naddition to the imbalance in the class sample size.\n\u2022\nP8. Abnormality camouflage. Bad actors may adjust\ntheir behaviors to camouflage the abnormality of the\nanomalous instances, making them difficult to detect\nusing popular AD methods. The anomaly camouflage\nin GAD refers to the phenomenon where anomalous\ngraph instances disguise themselves as normal within\na local neighborhood or the global graph. This may\nbe done through various mechanisms, e.g., attribute\nmanipulation and structural manipulation in a graph\n[25], [73], [99], [138].\n2.2\nMajor Challenges\nThe aforementioned problem complexities lead to the follow-\ning largely unsolved challenges in GAD, which deep GAD\napproaches can tackle to various extent:\n\u2022\nC1. Graph structure-aware GAD. As discuss in P1,\ngraph anomalies are not solely determined by their\nown attributes but also by their structural context.\nThus, the GAD methods are required to effectively\ncapture those structural dependency in their anomaly\nscoring functions. The effect of this dependency in\nanomaly scoring may vary significantly from ho-\nmophilic graphs to heterophilic graphs, and from\nstatic graphs to dynamic graphs (P2). On the other\nhand, oversmoothing is a common issue when model-\ning the graph structure information, which is referred\nto as a phenomenon in graph representation learning\nwhere the learned representations of different nodes\nbecome overly similar due to the iterative aggregation\nof representations of neighboring nodes to obtain\nthe representations of the target nodes. In GAD,\nthis can lead to node/subgraph representations that\nsmooth out anomalies as well, making them indis-\ntinguishable between normal and abnormal graph\ninstances [25], [98]. Therefore, it is challenging to\nmodel diverse structural influences in the anomaly\nscoring on graphs, while avoiding adverse effects like\nrepresentation oversmoothing.\n\u2022\nC2. GAD at scale. As discussed in P3, large-scale\ngraphs with millions or even billions of nodes/edges\npresents a significant computational challenge to\nGAD methods that aim to model global or higher-\norder structure information [113]. Existing large-\ngraph modeling methods are challenging to apply\ndirectly to GAD due to the extreme imbalance in\nthe data (P7). While some subgraph and sampling\ntechniques have been proposed to address this issue,\nthey often fail to capture the full structural informa-\ntion, resulting in sub-optimal performance [25], [69],\nparticularly for unsupervised GAD. Consequently,\nperforming anomaly detection on large-scale graphs\nremains a long-standing challenge in the area.\n\u2022\nC3. Generalization to different graph anomalies.\nAs discussed in P4, there are various types of graph\nanomaly instances, making it hard to apply a one-for-\nall approach. Achieving this requires a combination\nof robust feature extraction and versatile detection\nmodels. Further, the anomalies can manifest in vari-\nous forms, ranging from attributes, structure and their\ncomposition (P5). However, most existing methods\nare designed for a specific type of anomaly in an un-\nsupervised manner, which typically have a low recall\nrate [18], [69], [98]. The challenge is amplified when\nthe training data does not illustrate every possible\nAUGUST 2024\n4\nclass of anomaly (P6), regardless of unsupervised or\nsupervised methods [1], [16], [90]\u2013[92], [121], [161].\n\u2022\nC4. Balanced GAD. As discussed in P7, since the\nnumber of normal instances is significantly larger\nthan that of abnormal instances, the models tend to\nbias towards the majority class during the training,\ni.e., they perceive the normal patterns more frequently.\nConsequently, the models might be overly specialized\nin recognizing normal instances while generalizing\npoorly on the anomalies. Often the detection decision\nthresholds are crucial for making predictions for some\nmethods [25], [113], and poorly chosen thresholds\ncan worsen the effects of data imbalance. Thus, the\nchallenge is to avoid biased GAD.\n\u2022\nC5. Robust and interpretable GAD. GAD in real\napplications needs to be robust against various ad-\nverse conditions, such as abnormality camouflage (P8)\n[25], [37], [113] and unknown anomaly contamination\n[98], [99]. Addressing the abnormality camouflage\nor anomaly contamination may require models that\ncan capture subtle differences between normal graph\ninstances and camouflaged instances, and complex\nrelationships within the graph as well. Besides, an\nexplanation of why a graph instances is detected as\nanomaly can be crucial for the utility of the predic-\ntions in real applications [62], [106], [106], but it is a\nlargely unexplored area. For example, in bank fraud\ndetection, it is essential to provide a comprehensive\nexplanation of the detected fraudulent activity for\nfacilitating the subsequent investigation, but it is\nchallenging to link the fraud to specific attributes\nof particular transactions (nodes) and their relations\n(edges) at a specific time period.\n3\nCATEGORIZATION OF DEEP GAD\n3.1\nPreliminaries\nGAD aims to recognize the anomaly instances in graph data\nthat may vary from nodes, edges to subgraphs by learning\nan anomaly scoring function. Traditional GAD methods\nachieve anomaly detection using matrix decomposition and\nresidual analysis [4]. However, their performance is often bot-\ntlenecked due to the lack of representation power to capture\nthe rich structure-attribute semantics of the graph data and\nto handle high-dimensional node attributes. In recent years,\nGNNs have been widely used in GAD due to their powerful\nrepresentation learning ability. Some representative GNNs\nlike GCN [54], GraphSage [39], and GCL [137] attract much\nattention in node representation learning in graphs. These\nGNNs can be leveraged to learn the expressive representation\nof different graph instances for GAD.\nDefinition and Notation. In this section, we introduce\nthe definitions and notations used throughout the paper. We\ndenote a graph by G = (V, E) where V and E denote the\nnode set and edge set respectively. For the graph G, we use\nX \u2208RN\u00d7M to denote the matrix of node attributes and xi \u2208\nRM is the attribute vector of vi \u2208V, and A \u2208{0, 1}N\u00d7N\nis the adjacency matrix of G with Aij = 1 iff (vi, vj) \u2208E,\nwhere N is the number of node.\nProblem Statement. GAD can be divided into anomaly\ndetection at the node-level, edge-level, sub-graph level and\ngraph-level settings. The node-, edge- and subgraph-level\nAD tasks are typically performed within a single large graph\nG, where the input samples are nodes v \u2208G, edges e \u2208G,\nand subgraphs s \u2282G, respectively. For the graph-level AD\ntask, the input samples are a set of graphs G = {G1, G2, \u00b7 \u00b7 \u00b7 }.\nFor the sake of simplicity and generality across different\nlevels of GAD, we uniformly denote the input samples as\no, i.e., o can denote a node v, an edge e, a subgraph s, or a\nfull graph G, depending on their use in specific algorithms\nor models. Then GAD aims to learn an anomaly scoring\nfunction f : {o1, o2, \u00b7 \u00b7 \u00b7 } \u2192R, such that f(o) < f(o\u2032), \u2200o \u2208\nOn, o\u2032 \u2208Oa, where On and Oa denote the set of normal and\nabnormal graph instances, respectively. Since anomalies are\nrare samples, it is typically assumed that |On| \u226b|Oa|.\n3.2\nCategorization of Deep GAD Methods\nIn order to facilitate a comprehensive understanding of the\nresearch progress in GAD, we introduce a new taxonomy\nthat categorizes current GAD methods into three main\ngroups,including GNN backbone design, GAD proxy task\ndesign, and graph anomaly measures, depending on the\ninsights offered by each method. This enables us to review\nthe GAD methods from three different technical perspectives.\nTo elaborate the insights in each perspective, we further\ncategorize the methods into fine-grained 13 groups. An\noverview of the taxonomy is shown in Figure 1.\nMore specifically, general GNNs can not be directly\napplied to GAD due to the aforementioned problem com-\nplexities, and thus, there is a group of studies that focus on\ndesigning suitable GNN backbones for GAD. The design\nof the GNN backbones can be divided into discriminative\nGNNs and generative GNNs according to the improvement\nof different modules in GNNs. The second main category of\nmethods is on the GAD models constructed by optimizing\na diverse set of well-crafted learning objective functions to\nform a proxy task that can guide the GAD models to capture\ndiverse graph anomaly/normal patterns without the need for\nground-truth labels. This category of methods can be further\ndivided into five subcategories based on the modeling in\nthe proxy tasks. Lastly, there is a group of methods that\nbuild GAD models based on anomaly measures that are\ndesigned specifically for graph data. These methods can be\nfurther grouped into four subcategories depending on the\ntype of graph anomaly measures used. A summarization of\nrepresentative algorithms for each type of GAD approaches\nis presented in Table 2 in Appendix A.\n4\nGNN BACKBONE DESIGN\nThis category of methods aims at leveraging GNNs to learn\neffective representations of graph instances for downstream\nanomaly detection tasks. Due to its strong capability to\nrepresent graph-structured data, GNNs can effectively obtain\nexpressive node representations through aggregation among\nthe connected nodes. However, unlike general node/graph\nclassification datasets, GAD datasets are often extremely\nclass-imbalanced, which prevents GNNs from being directly\napplied to GAD datasets. Therefore, several GNNs have\nbeen proposed to handle the imbalance problem for more\neffective GAD. Concretely, this type of methods can be\nAUGUST 2024\n5\nDeep GAD\nGNN Backbone Design (\u00a74)\nDiscriminative GNNs\nAggregation\nMechanism\nCARE-GNN [25]; GraphConsis [73]; PCGNN\n[66]; NGS [100]; GHRN [36]; H2-FDetector\n[108]; BLS [22]; FRAUDRE [142]; GAGA\n[124]; GDN [35]; GmapAD [82]; MTIGATE\n[12]; HedGe [147]; PMP [165]; RAND [6];\niGAD [143]\nFeature\nTransformation\nGDN [35]; AMNet [10]; BWGNN [114] SEC-\nGFD [133]; RQGNN [24]; SplitGNN [125];\nSmoothGNN [23]\nGenerative GNNs\nFeature\nInterpolation\nGraphSMOTE [150]; GraphENS [94]; DA-\nGAD [60]; AuGAN [159]\nNoise\nPerturbation\nGGAD [99]; SDGG [8]; GODM [65]; DIFFAD\n[80]; ConsisGAD [14]\nProxy Task Design (\u00a75)\nGraph Reconstruction\nDOMINANT [18]; AnomalyDAE [31]; GUIDE [141]; HO-GAT\n[45]; SpecAE [58]; ComGA [77]; ALARM [96]; REMAD [146];\nMSAD [52]; GAD-NR [104]; Netwalk [140]; MUL-GAD [74];\nAdoNE [5]; ResGCN [95]; Sub-CR [145]; VGOD [49]; AANE [26];\nSTRIPE [63]; HimNet [84]\nGraph Contrastive\nLearning\nCoLA [69] ; SL-GAD [155]; GCCAD [13]; ANEMONE [51];\nGADMSL [27] PREM [85]; GRADATE [27]; CONAD [135] ;\nFMGAD [132]; SIGNET [67]; MAG [75]; Sub-CR [145]; ARISE\n[28]; HCM-A [47]; OCGTL [101]; NLGAD [29]; ACT [120];\nFedCAD [55]\nGraph Representation\nDistillation\nGlocalKD [79]; GLADST [59]; FGAD [9]\nAdversarial\nGraph Learning\nAEGIS [17]; GAAN [15]; CFAD [131]; GADY [76]; GGA [83]\nScore Prediction\nMeta-GAD [21]; SAD [116]; WEDGE [157]\nGraph Anomaly\nMeasures (\u00a76)\nOne-class Distance\nOCGNN [122]; AAGNN [160]; DOHSC [148]; OCGTL [101];\nDeepSphere [115]; Netwalk [140]; HRGCN [56]\nCommunity Adherence\nMHGL [158]; Netwalk [140]\nLocal Affinity\nTAM [98]; CLAD [53]; PREM [85]; ARC [68]\nGraph Isolation\nDIF [134]; GCAD [164]\nFig. 1: Overview of the proposed taxonomy of deep GAD from three high-level and 13 fine-grained technical perspectives.\n(a) Hard Edge Selection\n(b) Soft Edge Selection\nPruned Edge \nConstructed Edge\n1iw\ni\n1\n2\n3\n4\n2\niw\n3\niw\n4\niw\ni\n1\n2\n3\n4\n(c) Edge Synthesis\ni\n1\n2\n4\n5\n6\nFig. 2: Three categories of aggregation mechanism.\n(a)  Gradient-based Feature Scoring\n(b) Spectral Graph Filter\nFeature Separability\nHigh-frequency\nLow-frequency\nNormal\nAbnormal\n\uf06c\n( )\ng \uf06c\n...\n...\n...\n...\n...\n...\n...\nImportance Score\nFig. 3: Two categories of feature transformation.\nroughly divided into discriminative- and generative-based\nGNNs for GAD.\n4.1\nDiscriminative GNNs\nDiscriminative GNN-based GAD methods refer to a GNN\narchitecture specifically designed for discriminating nor-\nmal graph instances from the abnormal ones, where the\ndiscrimination is typically achieved through a supervised\nlearning manner. Thus, the discriminative GNNs are typically\ntrained on the labeled graph dataset containing examples of\nboth normal and abnormal instances. The core idea in these\nmethods is to adapt the conventional GNN backbones in a\nway so that the message passing in GNNs can capture the\nmajority patterns or the deviation patterns better.\nLet hi be the feature representation of a graph instance oi\nthat is obtained through l layers of feature aggregation (FAG)\nin a GNN, i.e., hi = FAG1:l(oi, Ni) where Ni represents\nthe neighbor set of a graph instance oi, these methods are\ntypically optimized via a general cross-entropy loss to train\nthe discriminative GAD model.\nLcls = \u2212\nX\noi\u2208G\n[yi log pi + (1 \u2212yi) log(1 \u2212pi)],\n(1)\nwhere yi denotes the class label of the instance oi and\npi = MLP1:m(hi) is the output of a mapping function that\ngoes through m layers of multiple perceptrons (MLP) to\nproject the feature hi to a probability of the sample being\nabnormal/normal. During inference, given a graph instance,\noj, pj or its inverse 1 \u2212pj can be used as its anomaly score.\nDifferent GNN-based encoders can be used to obtain\nthe feature representation h, such as graph convolutional\nnetworks (GCNs) [54], graph attention networks (GAT) [117],\nor GraphSage [39]. Depending on which part of the learning\nAUGUST 2024\n6\npipeline is focused, we group the existing methods in this\ncategory into two sub-categories, including the methods\nthat focus on the GNN neighborhood aggregation design,\ni.e., FAG1:l(oi, Ni), and that focus on feature transformation\nwith the raw attributes or node embeddings as input, i.e.,\nTransformation(hi) or Transformation(xi). Below we introduce\neach of them in detail.\n4.1.1\nAggregation Mechanism\nAs a simple and effective way to obtain the representation\nof nodes in GNNs, feature aggregation plays a crucial role\nin learning node representations by aggregating information\nfrom neighboring nodes in a graph. Thus, to create GNN-\nbased methods for GAD, one principled approach is to craft\nsuitable feature aggregation FAG designs that are sensible\nfor graph anomaly instances.\nAssumption. The GAD methods in this line assume that\nthe connected instances from the same class in graphs have\nsimilar characteristics, from which we can perform feature\naggregation to obtain discriminative normality/abnormality\npatterns.\nA widely-used FAG mechanism is as follows [54]:\nh(l)\ni\n= \u03c3\n\u0010\nW(l) \u0010\nh(l\u22121)\ni\n+ AGG\n\u0010n\nh(l\u22121)\nj\n| oj \u2208Ni\no\u0011\u0011\u0011\n,\n(2)\nwhere h(l)\ni\nis the feature representation of instance i in\nthe l-th layer, \u03c3 is an activation function, and W(l) is the\ntraining parameters in the l-th layer. The graph instance\noi is often set as a node vi, and Ni is typically the 1-\nhop neighborhood of the node vi. However, due to the\noversmoothing representation issue (C1 in Sec. 2.2) , directly\napplying such neighborhood aggregation mechanism can\nlargely reduce the discriminability of of graph anomalies,\nespecially for those whose abnormal behaviors are subtle (C3\nin Sec. 2.2). Therefore, a variety of methods were proposed\nto enforce distinguishable representations for normal and\nabnormal graph instances throughout a number of feature\naggregation iterations. These methods can be summarized\nvia the following principled framework:\n\u02c6h(l)\ni\n= AGG\n\u0010n\nh(l\u22121)\nj\n| oj \u2208\u03a6 (Ni) \u222a\u03a8(V)\no\u0011\n,\nh(l)\ni\n= \u03c3\n\u0010\nW(l) \u0010\nh(l\u22121)\ni\n+ \u02c6h(l)\ni\n\u0011\u0011\n,\n(3)\nwhere \u03a6(\u00b7) and \u03a8(\u00b7) represent a filtering function on the\nneighborhood set N and an edge synthesizer on the full node\nset V, respectively. Depending on how the methods specify\nthe \u03a6 or \u03a8 function, we further categorize them into two fine-\ngrained groups \u2013 hard/soft edge selection and edge synthesis\n\u2013 to gain better insights into these existing methods.\n\u2022 Hard Edge Selection. Popular aggregation mecha-\nnisms in GNN methods are built upon a homophily assump-\ntion that connected nodes come from the same class. Thus,\nthe existence of non-homophily edges (i.e., edges that connect\nnodes of different classes, also referred to as heterophily\nedges below) in a GAD dataset can greatly hinders the\ndiscriminability of the learned feature representations. As\nshown in Fig. 2(a), one popular strategy is to instantiate\n\u03a6 (Ni) that prune the heterophily edges w.r.t. the normal\nclass, referred to as hard edge selection. Below we review\nthe methods in this line.\nIn order to enhance the homophily relations, CARE-GNN\n[25] devises a label-aware similarity measure to find informa-\ntive neighboring nodes during the aggregation where \u03a6 (Ni)\nis instantiated by a node selector that chooses the neighbors\nwith high similarity. Moreover, a reinforcement learning\nmodule is also used in [25] to find the optimal amounts\nof neighbors to be selected. MITIGATE [12] implements\n\u03a6 (Ni) via a masked aggregation mechanism that utilizes\nthe distance-based clustering algorithm to choose a subset\nof high-representative nodes, in which the nodes that are\nclosest to the cluster centers are chosen. GmpaAD [82] takes\na similar clustering-based approach as MITIGATE, but it\nuses a differential evolutionary algorithm to find the optimal\nmapping strategy and generate the representative nodes\ngiven the selected candidates from a clustering method. On\nthe other hand, H2-FDetector [108] categorizes the edges\ninto homophily and heterophily connections in the graph,\nand further designs a new information aggregation strategy\nensure that the homophily connections propagate similar\ninformation while the heterophily connections propagate\ndifferent information.\nIn addition to using distance, \u03a6 can also be specified via\nmeta learning or reinforcement learning. BLS [22] is the rep-\nresentative method that enhances the FAG mechanism under\nimbalanced and noisy scenarios by selecting important nodes\nvia a meta-learning gradient of the learning loss. AO-GNN\n[46] employs a reinforcement learning method supervised by\na surrogate reward based on AUC performance to prune the\nheterophily edges. NGS [100] takes a meta-graph learning\napproach that devises a differentiable neural architecture to\ndetermine a set of optimized message passing structures and\nthen combines multiple searched meta-graphs in FAG.\n\u2022 Soft Edge Selection. Another research line is adopt-\ning an attention mechanism in GNNs by assigning the\nweights for each edge for soft edge selection for GAD,\nrather than hard edge selection, as demonstrated in Fig. 2(c).\nThis weight is generally obtained through the relationship\nbetween node embeddings, which serves as an effective way\nto enforce the importance of some specific edge relations in\nthe feature aggregation. GAT [117] is widely used as the basic\nbackbone, on top of which a variety of designs is introduced\nin the methods of this category for GAD. Specifically, the\ngeneral attention mechanism in GAT can be formulated as:\nh(l)\ni\n= \u03c3\n\uf8eb\n\uf8edX\noj\u2208N(i)\n\u03a6 (hi, hj; \u03b8) Wh(l)\nj\n\uf8f6\n\uf8f8,\n(4)\nwhere \u03a6 indicates a weight learning function with param-\neters \u03b8 applied on the embedding of a graph instance\noi and its neighbors oj. It represents the contribution of\nrelations/neighbors to the target instance oi, where the\ninstance oi is often specified as a node vi. For instantiating\n\u03a6, GraphConsis [73] reveals an inconsistency phenomenon\nin node connections that abnormal nodes can have a high\nlikelihood of being connected to normal nodes to camouflage\ntheir abnormality. It then introduces a consistency scoring-\nbased method based on node embedding similarities and\na self-attention mechanism to assign weights for different\nconnections in the aggregation in \u03a6. FRAUDRE [142] extends\nthe inconsistency-based scoring method to include three\ntypes of graph inconsistencies in features, topology, and\nAUGUST 2024\n7\nstructural relations to consider the importance of different\nconnections. On the other hand, PMP [165] introduces\na partitioning message passing to independently handle\nthe heterophily and homophily neighbors preventing the\ngradients in the optimization from being dominated by\nnormal nodes. To achieve this, \u03a6 is implemented by a weight\ngenerator function to adaptively adjust the influence of\nneighbors from different classes to the target node.\n\u2022 Edge Synthesis. The edge selection function \u03a6 focuses\non local structure only. Edge synthesis function \u03a8 can often\nbe used to complement \u03a6 in capturing more global patterns\nfor GAD, as demonstrated in Fig. 2(b). For example, GHRN\n[36] prunes the inter-class edges by emphasizing and delin-\neating the high-frequency components of the graph. Apart\nfrom edge reduction, the indirect link between nodes can\nalso be beneficial for the normality representation learning\n[36]. To this end, GHRN introduces a global node selector\n\u03a8(V) that chooses nodes beyond the neighboring nodes of\nthe target node to introduce this information into the feature\naggregation, which can be seen as an edge synthesizer that\nconnects distant nodes to a target node. PCGNN [66] specifies\n\u03a8(V) using a subgraph construction method consisting\nof class-label-balanced node and edge samplers to tackle\npotential issues arising from the skewed class distribution.\nTo this end, it incorporates the label information into the\nsampling process to choose nodes and edges for its sub-\ngraph construction. A distance function is also used to\nsimplify the neighborhood in the sub-graphs. NSReg [121]\nleverages a novel normal structure regularization method\nwhere the normal-node-oriented relation is used to enforce\nstrong normality into the representation learning to avoid\noverfitting to the labeled abnormal nodes. The relation repre-\nsentations are generated through a learnable transformation\nthat fuses the representations of relevant nodes, which are\nsubsequently used to optimize the normal-node-oriented\nrelation prediction and the representation learner.\nAdvantages. The key advantages of discriminative GNN-\nbased GAD methods are as follows. (i) Treating anomaly\ndetection as an end-to-end imbalanced classification task\nsimplifies the GAD problem, allowing the use of available\nabnormal samples to detect known graph anomaly instances.\n(ii) This approach does not rely on the reconstruction of the\ngraph structure, which significantly reduces memory usage\nand enhances its scalability for large-scale graphs.\nDisadvantages. They also have some major disadvan-\ntages. (i) Since these methods require some labeled graph\ndata as supervision information, they become inapplicable\nor less effective in practical applications where such data is\ndifficult to obtain. (ii) Edge selection may lead to the loss of\nimportant structure information while edge synthesis may\nintroduce noise or some irrelevant structure information into\nthe message passing in GNNs.\nChallenges Addressed. The tailored GNNs for GAD en-\nable effective modeling of graph structure and its interaction\nwith graph attribute information (C1). They may also be able\nto learn more discriminative features for detecting subtle\nanomalies that are similar to labeled abnormal samples (C3).\nSince these methods require local feature aggregation, they\ncan often scale up to large graphs (C2).\n4.1.2\nFeature Transformation\nIn addition to the efforts on the aggregation mecha-\nnism, another popular approach to obtain discrimina-\ntive features for GAD is to perform feature transforma-\ntion on either the graph instance representations from\nGNNs, i.e., Transformation(hi), or raw attributes, i.e.,\nTransformation(xi). This is crucial since datasets used in\nGAD can often contain a substantial proportion of feature in-\nformation that is irrelevant, or even noisy, to GAD. There are\ntwo popular approaches to instantiate this Transformation(\u00b7)\nfunction: one is to use gradient information and another is\nto use spectral graph filters.\nAssumption. It is assumed that there is irrelevant or noisy\ninformation in the raw attributes or graph structure w.r.t.\nGAD, which should be discarded during feature aggregation.\n\u2022 Gradient-based Feature Scoring.\nExtracting\nclass-related features specific to the characteristics of\na particular class is one straightforward way to obtain\ndiscriminative features. Inspired by variable decomposition\n[32],\ngradient\ninformation-based\nmethods\nhave\nbeen\nemerging as one main approach to obtain discriminative\nrepresentations from GNNs [78]. The key idea here is to\nselect features from the representation hi based on the\ngradient backpropagated from the softmax probability of\nbeing a specific class, as illustrated in Fig. 3(a). Let \u03b1c\nk be\nthe gradient score of an anomaly class c w.r.t. a feature k,\nthen it represents the contribution of this feature to anomaly\ndetection, which can be formulated as follows:\n\u03b1c\nk = 1\nN\n\f\f\f\f\f\nN\nX\ni=1\n\u2202yc\n\u2202hk,i\n\f\f\f\f\f ,\n(5)\nwhere yc is the predicted probability of being the anomaly\nclass c and hk,i is the k-th feature of the representation of\nnode i from a hidden layer. After obtaining a gradient score\nfor each feature dimension, the top K features with the\nlargest gradient scores are selected to represent the nodes in\na reduced feature space. This gradient score-based approach\nis used in GDN [35] to select abnormal and normal features\nin a supervised manner, and these features are found often to\nbe invariant to structure distribution shift. It helps reduce the\nnegative influence of irrelevant features while preserving the\nextracted abnormal/normal graph patterns, thus enhancing\nthe overall performance of GAD. Similarly, GraphENS [94]\ndetermines the importance of each node feature via a\ngradient score-based method. Apart from gradient score,\nexisting feature selection methods for anomaly detection or\nimbalanced classification [57], [86]\u2013[88], [93] may be adapted\nfor GAD.\n\u2022 Spectral Graph Filter. Spectral graph filter, which\ncombines the strengths of spectral graph theory and GNNs,\nis widely applied to capture and analyze the structural prop-\nerties of graphs for GAD tasks. This approach utilizes a set of\ngraph filters to transform the raw attributes to latent space to\nextract discriminative graph representations, as illustrated in\nFig. 3(b). Each graph filter assumes that normal nodes tend\nto have similar features with their neighbors, which can be\nregarded as low-frequency information, whereas abnormal\nnodes in the graph are characterized by deviations from the\nnorm, which are often accompanied by high-frequency infor-\nmation since abnormal nodes often have different character-\nAUGUST 2024\n8\nistics from surrounding nodes. The distinction between low-\nfrequency and high-frequency information is closely related\nto the spectral properties of the graph. In particular, the low-\nfrequency and high-frequency variations on the graph can\nbe effectively captured by the lower and higher eigenvalues\nof the graph Laplacian matrix, respectively. To leverage this\ninformation for GAD, graph Fourier transformation [110]\nbased graph filtering operation is often used. Formally, let\nL be the symmetrically normalized Laplacian, with eigen\ndecomposition L = U\u039bUT , where \u039b = diag [\u03bb1, \u00b7 \u00b7 \u00b7 , \u03bbn],\nthen a signal x \u2208Rn is transferred by using a graph filter\ng, Transformation(x) = g \u22c6x = Ug(\u039b)UTx, and thus, the\ngraph signal filtered by the k-th filter can be defined as\nhi,k = Ugk(\u039b)UT xi = Udiag [gk (\u03bb1) , . . . , gk (\u03bbn)] UT xi.\n(6)\nTo better capture the feature separability, a set of multi-\nfrequency filters is often employed to learn the node repre-\nsentations [10]. Accordingly, the final representation hi of\nnode vi can be obtained by\nhi =\nX\nk\n\u03b1i,khi,k = U\nX\nk\n\u03b1i,kgk(\u039b)UT xi,\n(7)\nwhere \u03b1i,k is an important score for the graph filter gk(\u039b).\nVarious spectral graph filters utilizing the frequency at\ndifferent level have been proposed for GAD. Specifically,\nAMNet [10] is an early work that adaptively integrates\ndifferent graph signals with mixed frequency patterns via a\nmulti-frequency graph filter group. It uses a restricted Bern-\nstein polynomial parameterization method to approximate\nfilters in multi-frequency groups. BWGNN [114] reveals a\n\u2018right-shift\u2019 phenomenon in GAD datasets with synthetic or\nreal-world anomalies, i.e., low-frequency energy is gradually\ntransferred to the high-frequency part when the degree of\nanomaly becomes larger. According to this phenomenon,\nthey justify the necessity of spectral localized band-pass\nfilters in GAD. Current GNNs with adaptive filters cannot\nguarantee to be band-pass or spectral localized. Therefore,\nthey build on top of Hammond\u2019s graph wavelet theory [40]\nto develop a new GNN architecture with a Beta kernel to\nbetter detect higher-frequency anomalies.\nThe aforementioned filters can be challenged by het-\nerophily graphs since homophily graphs are assumed in\nthese filters. To tackle this challenge, SEC-GFD [133] employs\na hybrid band-pass filter to partition the graph spectrum into\nhybrid frequency bands, while SplitGNN [125] combines a\nband-pass graph filter with a tunable Beta Wavelet GNN to\naddress the heterophily issue in node representation learning\nfor GAD (C1 in Sec. 2.2).\nAdvantages. (i) The feature transformation approach\nsupports the extraction of discriminative features from data\nwith various amount of irrelevant/noisy information. (ii) It\ncan also provide rich and informative graph representations\ncapturing beyond local graph properties, such as connectivity,\ncentrality, and community structure, for more effective GAD.\nDisadvantages. (i) Depending on what criterion or\nspectral filter is used, this approach might overlook some\nimportant information in feature transformation that could\nbe crucial for GAD, since one feature scoring criterion or\nfilter often fails to capture all possible discriminative features.\n(a) Feature Interpolation\n(b) Noise Perturbation   \n1\uf06c\n1\n1 \uf06c\n\uf02d\n2\n\uf06c\n2\n1 \uf06c\n\uf02d\n3\n\uf06c\n3\n1 \uf06c\n\uf02d\nPerturbation\nGenerated Node\n...\n...\n...\n...\n...\n...\n...\n...\nGenerated Node\nFig. 4: Two categories of generative GNNs.\n(ii) The approach may confined to a specific type of graph.\nFor example, spectral GNNs are primarily designed for\nhomogeneous graphs, and thus, it may not be suitable for a\nheterogeneous graph with different types of nodes.\nChallenges Addressed. The feature transformation ap-\nproach enhances GNNs by focusing them on specific types of\ndiscriminative features, thereby providing effective solutions\nto the graph structure-aware anomaly detection problem\n(C1). Furthermore, this approach focuses on discriminative\nfeatures and does not require costly graph structure recon-\nstruction, and thus, it is often good for GAD on large-scale\ngraph data (C2).\n4.2\nGenerative GNNs\nGenerative GNN-based methods focus on synthesizing new\ngraph instances to augment the existing graph data for\nenhancing model training on the new graph for GAD. This\napproach is motivated by the problem complexities like\nthe scarcity of graph anomaly instances and their large\nvariations (P5 and P7 in Sec. 2.2). The underlying key idea\nin these GAD methods is to synthesize outliers that can\nsimulate graph anomalies in some specific properties to\nprovide pseudo anomaly information for training the GAD\nmodels. In general, these methods can be summarized by the\nfollowing formulation:\nonew\ni\n= g\u03d5(oi, X, A; \u03f5),\n(8)\nwhere oi is an existing graph instance, g\u03d5 is a graph instance\ngenerator with parameters \u03d5 that uses oi, existing attribute\ninformation X, graph structure information A, and some\nauxiliary information \u03f5 to generate the augmented graph\ninstance, onew\ni\n. It is then followed by a one-class or binary\nclassification loss Lcls defined as\nLgen =\nX\n\u2113(Y, f\u03b8 (Xnew\ni\n, Anew\ni\n)),\n(9)\nwhere Xnew and Anew are the augmented version of X and\nA, Y represents the label set of the graph instances consisting\nof pseudo labels of the generated anomaly instances (and\nthe labels in the original graph if any), and f is a GAD\nmodel. Depending on the source of \u03f5 in g\u03d5 in Eq. 8, these\nmethods can be categorized into feature interpolation and\nnoise perturbation generation methods. The former focuses\non specifying \u03f5 using data from existing graphs, while the\nlatter focuses on utilizing prior distribution to specify \u03f5.\n4.2.1\nFeature Interpolation\nFeature interpolation is a commonly employed technique\nin imbalance learning for augmenting data, where the\nAUGUST 2024\n9\nrepresentations of synthesized graph instances are created by\ninterpolating the representations of existing graph instances.\nIt has been explored in popular algorithms like SMOTE [33]\nand Mixup [144] to oversample the minority classes or gen-\nerate diverse, large-scale samples for training deep models.\nThe approach can be generally formulated as follows:\nhnew = (1 \u2212\u03bb) \u00b7 h(i)\na + \u03bb \u00b7 h(j)\nb ,\n(10)\nwhere hnew is the representation of the generated graph\ninstance using a convex combination of the representations\nof two existing graph instances from the same class, ha and\nhb, as illustrated in Fig. 4(a).\nAssumption. The interpolated feature representations\nbetween graph instances can well align with those of the\ninstances from the anomaly class.\nGraphSMOTE [150], GraphMixup [126] and GraphENS\n[94] are representative methods that utilize feature inter-\npolation to address the challenge of biased GAD (C4 in\nSec. 2.2). These methods are designed for imbalanced node\nclassification, which can be considered as supervised GAD in\na closed-set setting since they do not consider the detection of\nnovel/unknown anomaly types that are not illustrated by the\nlabeled training anomaly examples [1], [16], [90], [92], [121],\n[161]. In particular, GraphSMOTE [150] extends SMOTE to\nsynthesize new nodes and edges in graph data. Different\nfrom generic data, generating new nodes in a graph requires\nthe connections of these nodes to existing nodes. Thus, an\nedge generator is trained simultaneously in GraphSMOTE\nto model the relations between existing and new nodes\nwhen using the SMOTE-style approach to generate the new\nnodes. A similar work is done in AugAN [159] that performs\ninterpolation on the feature representations of the nodes that\nare similar to labeled anomalies to increase the number of\ntraining anomaly examples. However, directly performing\nthe interpolation may produce out-of-domain samples due to\nthe extreme sparsity of the minority classes. To alleviate this\nissue, GraphMixup [94] is introduced to construct semantic\nrelation spaces that allow the interpolation to be performed\nat the semantic level. In GraphENS [94], the synthesized\nnodes are created using an adaptive interpolation rate that\nis determined by the distance between the minority and\nmajority class nodes, and its neighborhood is built in a\nstochastic manner based on the distance between the ego-\nnetwork nodes of a minority node and a target node. BAT\n[72] generates virtual nodes for each class as \u201cshortcuts\u201d\nconnecting to the other nodes based on posterior likelihoods,\nwhere the feature representation of the generated nodes is\ngenerated based on a feature interpolation operation.\nAdvantages. (i) Feature interpolation is a simple yet effec-\ntive way to create more samples for the under-represented\nanomaly classes. (ii) Mixing up feature representations from\ndifferent classes can diversify the training data, enhancing\nthe training of a more generalized GAD model.\nDisadvantages. (i) Feature interpolation focuses on gen-\nerating node representations but lacks the ability to generate\ngraph structural information. Typically, the methods in this\ncategory require the incorporation of an additional local\nstructure generator to address this limitation. (ii) Feature\ninterpolation also has the risk of producing some ambiguous\ngraph samples which may lead to harder separation between\nnormal and anomaly classes.\nChallenges Addressed. Feature interpolation provides a\nsimple but effective way to augment the anomaly data, which\nhelps mitigate the bias due to data imbalance (C4). Further,\nit may generate abnormal graph instances that are dissimilar\nto the training anomaly instances, thereby improving the\ngeneralization ability of GAD models to some unknown\nanomalies (C3).\n4.2.2\nNoise Perturbation\nUnlike the feature interpolation methods that generate\nnew graph instances based on interpolation between rep-\nresentations of existing instances, the noise perturbation\ngeneration methods aim to generate graph instances using\nprior-driven noise perturbation, as shown in Fig. 4(b). This\napproach can incorporate prior knowledge of the graph\nnormality/abnormality into the generation process for more\neffective GAD. The generated graph samples as abnormal\ngraph instances, combined with the given labels for existing\nnodes, can then be leveraged to guide the training of a\ndiscriminator for GAD.\nThe approach can be generally formulated as follows\nXnew, Anew = g\u03d5(X, A; \u03f5),\nLcls =\nN\nP\ni=1\n\u2113(yi, f\u03b8 (Xi, Ai)) +\nM\nP\ni=1\n\u2113(1, f\u03b8 (Xnew\ni\n, Anew\ni\n)) ,\n(11)\nwhere \u03f5 is noise perturbation typically generated from a\nprobability distribution, such as a Gaussian distribution,\ng\u03d5(\u00b7) is a generation function parameterized by \u03d5, and M is\nthe number of generated graph instances.\nAssumption. Certain prior distributions can be used as a\nsource of noise perturbation to generate pseudo-abnormal\ngraph instances and/or diversify normal graph instances.\nOne group of methods in this category [8], [14], [60],\n[99] takes a representation permutation approach, which\nfocuses on applying permutation to graph representations\nto instantiate g\u03d5(\u00b7), i.e., Permutation(Z). It first utilizes a\nGNN to obtain the representations from the original graph\ndata X and A. Then it applies the permutation to the\nrepresentations to generate the representation of anomalous\nsamples, denoted as \u02dcZ.\n\u02dcZ = Permutation(Z; \u03f5), Z = GNN (X, A)\n(12)\nwhere \u03f5 are the hyperparameters of permutation. For ex-\nample, DAGAD [60] employs the permutation and concate-\nnates on the representation learned from a limited number\nof labeled instances to generate the anomalous sample,\nthereby enriching the knowledge of anomalies captured in\nthe training set. On the other hand, GGAD [99] aims to\ngenerate outlier nodes that assimilate anomaly nodes in both\nlocal structure and node representations by leveraging the\ntwo priors of anomaly nodes, including asymmetric local\naffinity and egocentric closeness, to impose constraints the\nrepresentations. SDGG [8] takes a similar approach as GGAD,\nbut it is focused on generating abnormal graphs that closely\nresemble fringe normal graphs, which are then used to train\ngraph-level anomaly detectors. Unlike GGAD and SDGG that\nwork on exclusively normal training data, ConsisGAD [14]\nfocuses on unlabeled nodes. It creates a noise version of the\nunlabeled nodes by injecting noise into their representations\nto synthesize instances that maintain high consistency with\nAUGUST 2024\n10\nthe original instances while involving as much diversity\nas possible. This operation helps mitigate the scarcity of\nabnormal node instances while also enriching the diversity\nof normal nodes.\nApart from the representation permutation-based genera-\ntion, denoising diffusion probabilistic models (DDPMs) [42]\nhave recently been emerging as another major approach to\ngenerate anomalous graph instances [65], [80]. The diffusion\nprocess is defined as a Markov chain that progressively\nadds a sequence of scheduled Gaussian noise to corrupt the\noriginal data x:\nZs = Z0 + \u03c3(s)\u03b5, \u03b5 \u223cN(0, I),\n(13)\nwhere Z0 is the feature representations of the graph instances\nand the variance of the noise (the noise level) is exclusively\ndetermined by \u03c3(s). The full denoising process is equivalent\nto a reverse Markov chain that attempts to recover the origi-\nnal data from noise. It iteratively denoises \u02c6Zs to obtain \u02c6Zs\ni\u22121\nvia denoising function. The estimated \u02c6Z0 is then fed into a\ngraph instance generator to generate anomalous graphs for\nGAD. In particular, GODM [65] employs iterative denoising\nto synthesize pseudo anomalous graph instances that have\nclose distribution with the real anomalies in a latent space.\nDIFFAD [80] leverages the generative power of denoising\ndiffusion models to synthesize training samples that align\nwith the original graph instance in egonet similarity. The\ngeneration of supplementary and effective training samples\nis utilized to mitigate the shortage of labeled anomalies.\nAdvantages. (i) The graph instance generation may\nallow the synthesis of novel anomalies that facilitate the\nidentification of unseen anomalies. (ii) Noise perturbation\ngeneration can create more diversified training samples,\nmaking the trained models less sensitive to small changes in\nthe input data.\nDisadvantages. (i) The generated instances can inevitably\ninclude some out-of-domain examples, which may deviate\nfrom the optimization objective, rendering the detection\nmodels less effective. (ii) Determining the optimal level of\nnoise in the noise perturbation can be challenging and may\nrequire extensive experimentation.\nChallenge Addressed. The generation of instances can\nenhance the generalization of detecting different graph\nanomalies (C3). It also helps achieve a balanced GAD by\nenhancing the training samples through the GAD-oriented\ngeneration (C4).\n5\nPROXY TASK DESIGN\nThe proxy task design-based approaches aim to capture di-\nverse normal/abnormal graph patterns by optimizing a well-\ncrafted learning objective function that aids the detection\nof anomalies in the graph data without the use of human-\nannotated labels. One of the crucial challenges in proxy task\ndesign is to guarantee that i) the proxy task is associated\nwith GAD, and ii) it can deal with rich structure information\nand complex relationships in graphs. We roughly divide the\nmethods in this group into five categories according to the\nemployed proxy tasks, including reconstruction, contrastive\nlearning, knowledge distillation, adversarial learning, and\nscore prediction. Their respective framework is shown in Fig.\n5. Below we introduce each of them in detail.\nGNN\nEncoder\nAttribute Reconstruction\nStructure Reconstruction\nGNN\nDecoder\n\uf028\n\uf029\n*\n\uf073\nT\nZ Z\n(b) Graph Contrastive Learning\nGNN\nEncoder\n(c) Graph Representation Distillation\n(d) Adversarial Graph Learning\nGNN\nEncoder\nNoise Data\nD\nG\nTeacher\nModel\nStudent \nModel\nRandom Network\nPrediction Network\n(a)  Graph Reconstruction\nDiscriminator\nReadout\n...\n(e) Score Prediction\nRepresentation\nRepresentation\nRepresentation\nTarget Node\nSubgraph\nGNN\nEncoder\nAnomaly Score\nPredictor\nPredicted Score\n...\nZ\n2\nF\n\uf02d\nA\nA\n\u2016\n\u2016\n2\nF\n\uf02d\nX\nX\n\u2016\n\u2016\n...\n\uf028\n\uf029\n Readout \ni\ni\n\uf03d\nz\nE\nPositive Pair\nNegative Pair\n\u02c6\nih\nih\n\uf028\n\uf029\n,\ni\ni\nKD h h\n~ ( )\np\nz\nz\nz\nz\nFig. 5: Five categories of proxy task design.\n5.1\nGraph Reconstruction\nData reconstruction aims to learn low-dimensional feature\nrepresentations of data for reconstructing given data in-\nstances, which is widely used in tabular data, and im-\nage/video data to detect anomalies [91]. Given that the\nnormal samples often occupy most of the dataset, it helps\nguarantee that the representations can retain the normal\ninformation, and thus, normal samples will be reconstructed\nwith a smaller reconstruction error than anomaly samples.\nAs a commonly used technique in this category, Graph\nautoencoder (GAE) [41] consists of a graph encoder and\na graph decoder and it is easy to implement. Formally, given\na graph with X and A, the graph reconstruction can be\nformulated as\nZ = GNNenc (X, A; \u0398enc) ,\nbX = GNNdec (Z, A; \u0398dec) ,\nbAij = p\n\u0010\nbAij = 1 | zi, zj\n\u0011\n= sigmoid\n\u0010\nzizT\nj\n\u0011\n,\n(14)\nwhere the GNN encoder and decoder are parameterized by\n\u0398Enc and \u0398Dec respectively, Z are the representations of the\nnodes in the latent space. \u02c6X and bA are the reconstructed\nattributes and graph structure respectively. The optimization\nobjective of the methods in this group can be unified as\nfollows\nLreconstruction = R(A, bA) + S(X, bX)) + \u03a6(A, X; \u0398), (15)\nwhere R(\u00b7) and S(\u00b7) are two reconstruction functions on\ngraph attributes and structure respectively, and \u03a6(A, X; \u0398)\nis an auxiliary optimization task to enhance the reconstruc-\ntion for better GAD. A general assumption underlying the\nmethods of this approach is as follows.\nAssumption. Normal graph instances can be well recon-\nstructed while reconstructing the anomalies in terms of graph\nattributes and/or structure will lead to a large error.\nThere have been many GAE-based GAD methods, among\nwhich early methods typically instantiate the encoder param-\neter \u0398enc and decoder parameter \u0398dec using different GNN\nmodels [5], [18], [31], [45], [58], [77], [96], [140]. DOMINANT\n[18] is among the seminal studies applying GAE to detect\nanomalies, in which both \u0398enc and \u0398dec are specified by\na three-layer graph convolution. The optimization terms R\nand S are specified using the pointwise difference between\nmatrices:\nLreconstruction = (1 \u2212\u03b1)||A \u2212\u02c6A||2\nF + \u03b1||X \u2212\u02c6X||2\nF ,\n(16)\nAUGUST 2024\n11\nwhere \u03b1 represents the adjustment ratio between attribute\nand structure reconstruction. The anomaly score is often\ndefined by the mean squared errors (MSE):\nScore(vi) = (1 \u2212\u03b1)||ai \u2212\u02c6ai||2 + \u03b1||xi \u2212\u02c6xi||2,\n(17)\nwhere ai and \u02c6ai are the original and reconstructed structure\nassociated with node vi, xi and \u02c6xi are the original and\nreconstructed attributes of node vi. Later studies explore the\nuse of more advance GAE by incorporating attention, spectral\ninformation, etc [31], [77], [96]. For example, AnomalyDAE\n[31] and GUIDE [141] employ a graph attention network\n(GAT) or an attention module to evaluate the significance\nof neighbors to nodes to enhance the reconstruction of the\nspecific nodes during the training. HO-GAT [45] develops\na hybrid order GAT network to specify the \u0398enc that can\ndetect abnormal nodes and motif instances simultaneously.\nDifferent from the focus on the importance of neighboring\nnodes, ComGA [77] is focused on enhancing node repre-\nsentations for more effective reconstruction. It achieves this\nby utilizing a community-aware GNN that propagates a\ncommunity-specific representation of each node into the\nfeature representations to encode and decode the modularity\nmatrix of the graph. Besides node reconstruction, some\nmethods explore node relation construction, e.g., GAD-NR\n[104] that aims to reconstruct the neighborhoods\u2019 structure\nand attributes. GAE can also be extended to perform multi-\nview graph reconstruction for learning more expressive\nrepresentations from heterogeneous attributes [96]. Despite\ntheir simplicity, such GAE-based methods have shown\neffective performance in not only anomalous node detection\nbut also anomalous edge detection [26], [140].\nAnother line of research is to complement the recon-\nstruction loss with some auxiliary tasks, i.e., \u03a6(A, X; \u0398) in\nEq. 15, to capture additional information for GAD, since\nthe reconstruction task is often too simple and vulnerable\n[52], [58], [84], [146]. In SpecAE [58], it integrates a density\nestimation into the reconstruction by leveraging Laplacian\nsharpening to amplify the distance between representations\nof anomalies and the majority of nodes. MSAD [52] employs\na number of weighted meta-paths, e.g., unknown-anomaly-\nunknown and anomaly-unknown-unknown node paths, to\nextract context-aware information of nodes as an auxiliary\ntask. In HimNet [84], hierarchical memory learning is incor-\nporated via \u03a6(A, X; \u0398), where the node-level and graph-\nlevel memory modules are jointly optimized to detect both\nlocally and globally anomalous graphs. Netwalk [140] incor-\nporates clustering as an auxiliary task in the reconstruction\nto learn the representation of nodes for streaming graph data.\nOn the other hand, \u03a6(A, X; \u0398) is specified using adversarial\nlearning in DONE [5] to generate node embeddings with an\nobjective to minimize the adverse effects from outlier nodes.\nAdvantages. (i) Data reconstruction is a simple but\neffective way to detect anomalies in graph data. Meanwhile,\nit does not require labeled data for training. (ii) The data\nreconstruction models can be generalized to detect anomalies\nat different levels of graph instances, e.g., anomalous nodes,\nedges, and graphs.\nDisadvantages. (i) To perform the reconstruction of graph\nstructure and attributes, it is necessary to load the entire\ngraph, which may require significant computing resources.\n(ii) The reconstruction is vulnerable to noisy or irrelevant\nattributes. The presence of noisy or erroneous graph instances\ncan disrupt the reconstruction process, resulting in false\npositives or diminished detection accuracy.\nChallenge Addressed. Reconstruction methods model\nthe entire graph structure, capturing complex structural\npatterns for GAD (C1). Due to simplicity and straight\nintuition, they are also applicable to the detection of different\nlevels of graph anomalies (C3).\n5.2\nGraph Contrastive Learning\nGraph contrastive learning (GCL) aims to learn effective\nrepresentations without human annotated labels so that\nsimilar graph instances are pulled together in the repre-\nsentation space, while dissimilar instances are far apart.\nA principled GCL framework for GAD is to devise a self-\nsupervised contrastive learning task with suitable positive\nand negative pairs to learn the underlying dominant (normal)\npatterns in graph-structured data. The framework generally\nfirst utilizes a GNN network to learn the representations\nof graph instances, and then it employs a contrastive loss\nfunction Lc to maximize the similarity of the positive views\nwhile minimizing the similarity of the negative pairs [118]:\nhi = GNN (xi, X, A; \u0398) ,\nsi = Positive(vi, A, X; \u2126p),\u02dcsi = Negative(vi, A, X; \u2126n),\nLc = E(X,A) [log D (hi, si) + log (1 \u2212D (hi,\u02dcsi))] ,\n(18)\nwhere hi is the representation of target node vi, si and \u02dcsi are\nthe graph instances generated from two sampling functions\nPositive(\u00b7) and Negative(\u00b7) with parameters \u2126p and \u2126n\nthat are used to sample the positive and negative sample\npairs for the target node vi. One key assumption made in\nthe GCL-based GAD methods is that non-neighboring nodes\ncan be treated as \u2018anomalous\u2019 graph instances relative to a\ntarget node:\nAssumption. Non-neighboring nodes to a target node\nare dissimilar, and thus, they can serve as effective negative\n(\u2019anomalous\u2019) samples to the target node.\nThis assumption works in that the datasets typically\ncontain a substantial number of normal graph instances.\nEffective GCL training would enable the learning of the\nmajority patterns on the deviated non-neighboring nodes\nw.r.t. the target nodes. Built upon this assumption, GCL-\nbased methods are focused on how the positive and negative\ngraph instances s and \u02dcs can be generated to be more aligned\nwith the GAD task.\nOne group of methods in this line aims to generate\nnode-level contrastive sample pairs [13], [123]. In particular,\nmotivated by the success of the popular GCL method\nDGI [118], DCI [123] formulates the GCL objective as the\nclassification of the pairs of target nodes and the nodes\nunder perturbation against the pairs of target nodes and\ncluster-based global representations. Unlike the cluster-based\nrepresentation, GCCAD [13] specifies s using the neighbors\nof target normal nodes and \u02dcs using the representations of\nabnormal nodes. During inference, the anomaly score may be\ndefined in various ways, e.g., via the classification probability\n[123] or similarity between the target node\u2019s representation\nand the graph representation [13].\nIn addition to the node-level contrasts, other methods use\nsubgraphs as the target of contrastive learning to help the\nAUGUST 2024\n12\nmodel learn better representations for GAD. CoLA [69] is\nan early GCL framework for GAD that learns the relations\nbetween each node and its neighboring substructures. Given\na target node, s is a subgraph generated by a random walk\naround the target node, while the negative subgraph is\ngenerated using the random walk around the other nodes.\nLet hi and Ei be the representations of a target node and\na subgraph, in which Ei is often obtained by a readout\nfunction as follows\nsi = Readout (Ei) =\nni\nX\nk=1\nvk\nni\n,\n(19)\nwhere ni is the number of nodes in Ei, vk is the embedding\nof node k in the subgraph, then a Bilinear(\u00b7) function is\noften used to combine the representations of the node and\nthe subgraphs [69], [155]:\nyi = Bilinear (hi, si) = \u03c3\n\u0000hiWsi\u22a4\u0001 ,\n\u02dcyi = Bilinear (hi, sj) = \u03c3\n\u0000hiWsj\u22a4\u0001 ,\n(20)\nparameterized by W, in which yi and \u02dcyi are the predicted\nresults for positive pairs (hi, si) and negative pairs (hi, sj)\n(i.e., sj acts as \u02dcsi here), respectively. The anomaly score in\nCoLA is defined as the difference between the positive and\nnegative pairs:\nScore (vi) = 1\nR\nR\nX\nr=1\n(\u02dcyi,r \u2212yi,r),\n(21)\nwhere R is the number of node-subgraph pairs sampled\nduring inference to obtain a stable anomaly score. This frame-\nwork inspires a number of follow-up methods, including the\nuse of prior knowledge of different anomaly types to generate\nthe negative samples [135], supervised positive/negative sub-\ngraph pairs [157], and multi-view/scale subgraph generation\n[27], [51], [132], [155].\nThe above methods are focused on node-level anomaly\ndetection in static graph data. Contrastive learning is also\nused in dynamic GAD or graph-level anomaly detection. For\nexample, TADDY [70] applies a dynamic graph transformer\nthat aggregates spatial and temporal knowledge simultane-\nously to learn the representations of edges. It specifies si and\n\u02dcsi by constructing the positive edge using the existing edges\nin the training set and generates the anomalous edges via\nnegative sampling. SIGNET [67] is designed for graph-level\nanomaly detection, which first constructs two different views\nusing dual hypergraph transformation and then maximizes\nthe mutual information between the bottleneck subgraph\nfrom two views. The estimated mutual information can be\nused to evaluate the graph-level abnormality.\nAdvantages. (i) Many existing GCL approaches and\ntheories may be adapted to enable GAD. (ii) The rich graph\nstructure information provides flexible options to generate\ndiverse positive/negative views for effective GAD. (iii) Since\nmany methods rely on only local graph information in their\ntraining, they can handle very large graph data.\nDisadvantages. (i) Since GCL is focused on representation\nlearning, it is crucial to develop an effective anomaly scoring\nmethod based on the learned representations. (ii) As GCL\nmethods rely on GNNs without class information, the\nproblem of over-smoothing between normal and abnormal\ninstances remains prevalent. (iii) The subgraph generation\nin some methods may incur significant computational costs\ndue to the need to traverse numerous nodes and edges.\nChallenge Addressed. Contrastive learning models can\nbe designed to capture different levels of graph structure\nand graph anomalies (C1, C3). Without the need to load the\nfull graph structure information, they can often scale up to\nlarge-scale graph data (C2).\n5.3\nGraph Representation Distillation\nKnowledge Distillation (KD) [38] aims to train a simple\nmodel (student) that distills feature representations from a\nlarge (teacher) model while maintaining similar accuracy as\nthe large model. The key intuition of KD-based GAD is that\nthe representation distillation can capture the majority pat-\nterns of graph instances and the difference in the distillation\ncan be used to measure the abnormality of samples.\nAssumption. The graph representation distillation can be\nseen as a process of extracting the prevalent patterns of the\ngraph instances, representing the normal patterns for GAD.\nThis category of methods learns the representation from\nthe teacher model initialized by a GNN. Subsequently, a\nGNN-based student network is trained to replicate the\nrepresentation outputs of the teacher model. The GNN-based\nteacher and student networks are formulated as follows\nhi = GNNteacher (xi, X, A; \u0398) ,\n\u02c6hi = GNNstudent\n\u0010\nxi, X, A; \u02c6\u0398\n\u0011\n,\n(22)\nwhere \u0398 and \u02c6\u0398 are respectively the training parameters\nof student and teacher networks, hi and \u02c6hi are the repre-\nsentations learned by the two networks respectively. Both\nrepresentations are then integrated into the loss function\nwhich can be formulated as the following:\nLKD = 1\nN\nN\nX\ni=1\nKD\n\u0010\nhi, bhi; \u02c6\u0398, \u0398\n\u0011\n,\n(23)\nwhere KD(\u00b7) is a distillation function that measures the\ndifference between the two feature representations. Overall,\nthe goal of the distillation-based GAD is to make the student\nmodel as close as possible in predicting the corresponding\noutputs of the teacher model that is built upon normal graph\ndata. Therefore, the anomaly score can be defined as the\ndifference in the representations between the teacher and\nstudent models.\nScore\n\u0010\nvi; \u02c6\u0398, \u0398\n\u0011\n=\n\r\r\rhi \u2212c\nhi\n\r\r\r\n2 .\n(24)\nGlocaKD [79] is an early framework, in which the teacher\nmodel GNNteacher is implemented using a random GCN.\nThen the distillation function KD(\u00b7) is instantiated using KL\ndivergence to minimize the graph- and node-level prediction\nerrors of the representations yielded by the random GCN.\nThe anomaly score in GlocaKD is defined as the prediction\nerror at the graph and node levels. To support better\ndistillation of the normal graph representations, several\ndistillation models have been proposed for GAD with new\narchitectures. For example, the dual-student-teacher model,\ncalled GLADST, consists of one teacher model and two\nstudent models [59], in which the teacher model, trained\nwith a heuristic loss, is designed to make the representations\nAUGUST 2024\n13\nmore divergent. Unlike the traditional teacher-student model,\nGLADST trains the two student models on normal and\nabnormal graphs separately to capture the normality and\nabnormality better. Unlike GlocalKD that uses a random\nGNN to be the teach network, the approach FGAD uses a\npre-trained anomaly detector as the teacher model [9]. Then\nthe student model is designed with a graph isomorphism\nnetwork (GIN) backbone and a projection head to improve\nthe robustness of GAD.\nAdvantages. (i) The distillation from the teacher model\ninto a simpler student model provides a new way to extract\nthe normality of graph instances. (ii) Distillation enables the\ndevelopment of smaller, more efficient models by transferring\nknowledge from a larger model. This compression speeds up\ninference and reduces computational resource requirements\nfor normality extraction. (iii) Distilling knowledge from a\nwell-trained teacher model enables the student model to\nbetter generalize across various types of graphs and quickly\nadapt to new data or target domains.\nDisadvantages. (i) Choosing an appropriate teacher\nmodel can be challenging. If the teacher model is too complex\nor not well-suited to GAD, the knowledge distilled to the\nstudent model may not be optimal. (ii) The student model\nmay fail to capture all the nuances and intricacies in the\nteacher model, which can impact the extraction of normality.\nChallenge Addressed. GNN-enabled knowledge distil-\nlation enhances the ability of addressing graph structure-\naware GAD. (C1). By distilling knowledge from a well-\ntrained teacher model, the student model gains improved\ngeneralization across different GAD scenarios, showcasing\nenhanced robustness in GAD (C5).\n5.4\nAdversarial Graph Learning\nGenerative adversarial learning (GAN) provides an effective\nsolution to generate realistic synthetic samples, which can be\nused for normal pattern learning for GAD. The key intuition\nof this group of methods is to learn latent features that\ncan capture the normality perceived in a generative GNN\nnetwork. Specifically, the GANs employ a generator network\naiming to generate samples that are statistically similar to\nthe real data while the discriminator network learns to\ndistinguish between real and generated graph instances\n[15], [17]. The normal data with a prior distribution can be\neasily captured by the generator network while the anomalies\nstruggle to be simulated by the generator due to the nature\nof the distribution.\nIt is worth mentioning that the purpose of generation\nis different from the noise perturbation generation in the\ngenerative GNNs in Sec. 4.2. The former mainly uses the\ngraph structure information and interpolation operations\nin the latent space to generate new node representations\nwithout using adversarial learning process, while the latter\nis focused on generating node representations from a prior\ndistribution through the adversarial learning to learn the\nlatent normality.\nAssumption. A generator GNN can capture the majority\nof patterns in the graph data if it can generate instances that\nclosely resemble the distribution of real graph instances.\nThis group of methods typically employs GNNs to learn\nthe representations of graph instances and a generator\nnetwork for generating graph instances based on a prior.\nFormally, these methods follow the following framework:\nhi = GNN (xi, A, X; \u0398) ,\n\u02dchi = G (\u02dczi; \u03f5) , \u02dczi \u223cp(\u02dcz),\n(25)\nwhere h is the feature representation of a node learned by\nGNN with parameter \u0398, \u02dch is the representation of a gener-\nated node, and p(ez) is the prior distribution. The generator\nG(\u00b7) takes noises sampled from the prior distribution p(\u02dcz) as\nthe input and generates synthetic pseudo abnormal graph\ninstances via:\nmin\nG max\nD\nEh\u223cp(h)[log D(h)] + E\u02dcz\u223cp(\u02dcz)[log(1 \u2212D(G(\u02dcz; \u03f5)))],\n(26)\nwhere the discriminator D(\u00b7) is often specified by a classifier\nthat tries to distinguish whether an input is the representation\nof a normal node or a generated anomaly. The anomaly score\nis typically defined based on the output of the discriminator:\nScore(si) = 1 \u2212D(hi).\n(27)\nAEGIS [17] and GAAN [15] are two representative works\nthat apply GANs to GAD by generating node representations\nfrom Gaussian noise. The discriminator is trained to deter-\nmine whether nodes are real or generated pseudo abnormal\ninstances. Some graph adversarial learning methods are also\nproposed to address the multi-class imbalance problem at the\nnode level [102], [109]. They incorporate adversarial training\nto make the model learn robust representations for both ma-\njority and minority classes, thereby benefiting the separation\nof the nodes from different classes. This graph adversarial\nlearning is also applied to anomalous edge detection. GADY\n[76] employs an anomaly generator to generate abnormal\ninteractions through input noise. The generated interactions\nare then combined with normal interactions as the input of\na discriminator network trained to determine whether the\ninteraction is normal or abnormal.\nAdvantages. (i) GANs provide a distinctive method\nfor learning structural normality by utilizing their graph\nstructure-aware generation capability. (ii) Its adversarial\ntraining can generate realistic samples from noise, enabling\nthe detection models to learn beyond the abnormal samples\nin the graph.\nDisadvantages. (i) It is difficult to generate samples that\naccurately simulate real graph instances in terms of both\ngraph structure and attributes, and thus, the generated graph\ninstances may impair the detection performance. (ii) The\ntraining of GANs is relatively less stable compared to the\nGNN training in other groups of methods.\nChallenge Addressed. GANs can model and generate\ndifferent types of abnormal graph instances, facilitating the\ndetection of different graph anomalies, e.g., anomalies that\nare unseen during training (C3). Also, GANs can generate\nextensive abnormal samples for more balanced GAD (C4).\n5.5\nScore Prediction\nThis group of methods focuses on how to make full use of\nlabeled data to build an end-to-end anomaly score prediction\nmodel. Unlike approaches that directly apply GNNs for\nclassification with labeled abnormal and normal nodes, this\nmethod is designed for the scenarios where only some graph\ninstances are known to be normal and abnormal instances.\nAUGUST 2024\n14\nAssumption. The anomaly scores of normal graph in-\nstances follow a prior distribution while that for abnormal\ninstances significantly deviate from the distribution.\nThe score prediction-based methods refer to training a\npredictor fpred : G \u2192R which is instantiated with GNNs to\ndirectly predict the anomaly score\nScore(si) = fpred (xi, A, X; \u0398p),\n(28)\nwhere \u0398p is the training parameters of the score prediction\nnetwork. DevNet [91] is a seminal work for score prediction\nnetwork, which was originally proposed to identify anoma-\nlies in tabular data. It employs a Z-score-based deviation loss\nto learn the anomaly scores in an end-to-end manner:\nLdeviation = (1 \u2212yi) \u00b7 |Dev(si)| + yi \u00b7 max (0, m \u2212Dev(si)) ,\n(29)\nwhere yi is the class label, m is a pre-defined margin based\non the prior distribution, and Dev(si) is defined as follows\nDev (si) = Score(si) \u2212\u00b5r\n\u03c3r\n,\n(30)\nwhere \u00b5r and \u03c3r are the estimated mean and standard\ndeviation of the anomaly scores based on the prior N (\u00b5, \u03c3).\nMeta-GDN [21] applies the deviation loss to the graph\ndata that leverages a small number of labeled anomalies to\nenforce significant deviation of the anomaly scores of the\nnormal nodes from the abnormal nodes. SAD [116] adapts\nDevNet to dynamic graph data, in which contrastive learning\nis also used to fully exploit the potential of labeled graph\ninstances on evolving graph streams. In WEDGE [157], the\ndeviation loss function is defined for at the subgraph level.\nBy minimizing the deviation loss, the score network predictor\nwill enforce a large positive deviation of the anomaly score\nof an anomalous subgraph from that of the prior-based\nreference scores.\nAdvantages. (i) By integrating the prior distribution\ninto the model\u2019s learning process, it can produce more\ninterpretable anomaly scores compared to other detection\nmethods. (ii) The studied scenarios where some labeled\nnormal and anomalous graph instances are available are\noften common in real-world applications.\nDisadvantages. (i) The performance of the score predic-\ntion model is dependent on the prior and the predefined\nmargin used during training. (ii) The score prediction net-\nwork is better suited for tabular data because the samples\nare independent, but a single prior distribution may not be\nable to effectively capture the dependent scores across the\ngraph instances.\nChallenge Addressed. The score prediction provides a\nGNN-based end-to-end anomaly score learning framework\nfor GAD, having good scalability to large-scale graph data\n(C1, C2). It also provides an effective way to achieve\ngeneralized GAD in the application scenarios where part\nof the graph instances are labeled (C3).\n6\nGRAPH ANOMALY MEASURES\nThis category of methods aims to discuss GAD methods\nthat focus on designing anomaly measures for evaluating\nthe abnormality of instances in the graph. These methods\nGNN\nEncoder\n(a) One-class Distance\n...\nHypersphere\nGNN\nEncoder\n(b) Local Affinity\nLocal Node Affinity\nGNN\nEncoder\n(c) Community Adherence\n(d) Graph Isolation\nGNN\nEncoder\n1\n2\n3\n4\nc\nNormal\nAbnormal\n1c\n2c\n3c\nNormal \nAbnormal \nFig. 6: Four categories of graph anomaly measure.\ngenerally perform anomaly scoring by incorporating some\nkey abnormal graph characteristics into deep graph learning\nmethods. As shown in Fig. 6, they can be generally divided\ninto four categories, including one-class distance measure,\nlocal affinity measure, community adherence measure, and\ngraph isolation-based approaches.\n6.1\nOne-class Classification Measure\nThe one-class classification measure refers to evaluating the\ndistance between each instance and a one-class center of the\ninstances for anomaly scoring [105]. This method can also be\napplied to graph data, where the GNN is typically trained\nto minimize the volume of a hypersphere that encloses the\nrepresentations of the graph instances. The key intuition is\nthat anomalous graph instances differ significantly from the\nnormal ones, causing them to fall outside the hypersphere\nthat encompasses most of the normal graph instances.\nAssumption. Normal graph instances exhibit similar\npatterns that can be encapsulated via a one-class hypersphere,\nfrom which anomalies show largely deviated patterns.\nThe one-class classification on the graph can be generally\nformulated as the following\nLone\u2212class = 1\nN\nN\nX\ni=1\n||\u03d5 (Xi, Ai; W) \u2212c||2 + \u03a6(\u0398),\n(31)\nwhere c is the central representation of the one-class hy-\npersphere, \u03d5 (Xi, Ai; W\u2217) is the representation of graph\ninstance si learned by GNN, and \u03a6(\u0398) is a regularization\nterm or auxiliary task which can benefit the one-class distance\nmeasure. Anomalies are expected to samples that have a\nlarge distance to the center. Thus, the anomaly score can be\ndetermined by the distance of a graph instance to the center\nof the hypersphere:\nScore(si) = \u2225\u03d5 (Xi, Ai; W\u2217) \u2212c\u22252 ,\n(32)\nwhere W\u2217are the parameters of the trained one-class model\nand c is the representation of the one-class center.\nThere have been some methods that adopt this one-\nclass classification approach for GAD [122], [160]. OCGNN\n[122] applies a one-class SVM to graphs, leveraging the\npowerful representation capabilities of GNNs. The objective\nof OCGNN is to generate node embeddings that are close\nto the center. Since the feature representations are crucial\nin one-class learning, various methods have explored the\nuse of GNNs from different perspectives to enhance the\nrepresentation learning for one-class GAD. For example,\nAUGUST 2024\n15\nAAGNN [160] designs a subtractive aggregation [160] rather\nthan the commonly used summation-based aggregation for\none-class GNN learning. DOHSC [148] adds an orthogonal\nprojection layer [148] to ensure the training data distribution\nis consistent with the decision hypersphere. Other methods\noptimize the one-class learning with some auxiliary tasks,\nsuch as node feature reconstruction [115], relation prediction\n[56], and self-supervision [101], to avoid notorious issues in\nthis approach like model collapse.\nAdvantages. (i) One-class classification does not require\nlabeled anomaly data for training, making it suitable for the\nscenario where such data is scarce or unavailable. (ii) The\none-class measure can handle isolated nodes well.\nDisadvantages. (i) Normal graph patterns can manifest\nin various ways, making it challenging for a one-class hyper-\nsphere to capture the full spectrum of normality. (ii) Learning\nthe one-class hypersphere is prone to model collapse.\nChallenge Addressed. One-class classification with ap-\npropriate GNNs enables the learning of the majority struc-\ntural pattern in the graph data (normal graph instances) (C1).\nThis approach also does not require the full graph structure\ninformation during training, resulting in good scalability to\nlarge-scale graphs (C2).\n6.2\nCommunity Adherence\nCommunity adherence-based GAD [140], [158] aims to\nidentify the anomalies based on the adherence of instances\nto graph communities. The key intuition is that anomalies\nare not well-distributed and exhibit weak adherence to the\ncommunities, whereas normal graph instances typically have\nstrong adherence to at least one community.\nAssumption. Normal instances adhere to at least one\ncommunity, whereas anomalies are unfit to any community.\nThis community adherence-based method first leverages\na mapping function to learn the representation of graph\ninstances. A clustering or community discovery method is\nthen applied to group the graph instances. Since anomalies\ngenerally exhibit significantly weaker community adherence,\nthe anomaly score can be defined by the minimum distance\nto the centers of the communities.\nScore (si) = min \u2225\u03d5 (xi, X, A; W\u2217) \u2212cj\u22252\n2 , \u2200j \u2208{1, . . . , p},\n(33)\nwhere p is the number of communities, cj is the center of\ncommunity Cj, and W\u2217is the optimal parameters of the\nmapping function.\nThis approach is analogous to clustering-based anomaly\ndetection in non-graph data [89], but here it needs to capture\nthe graph characteristics for GAD. To this end, MHGL [158]\nutilizes GNNs and a multi-hypersphere learning objective\nto learn multiple groups of fine-grained normal patterns,\nenclosing each group using a corresponding hypersphere\nin the latent space while simultaneously pushing labeled\nanomalies far away from these hyperspheres. The anomaly\nscore is defined as the Euclidean distance between a test\ninstance and the nearest hypersphere center. Netwalk [140]\nis a method for both anomalous node and edge detection\nwhere k-means clustering was applied to group the existing\nnode/edge into different groups in a feature representation\nspace. The anomaly score of node/edge is measured as its\ndistance to the center of its closest cluster.\nAdvantages. (i) By utilizing graph communities, the\nnormality of data beyond one-hop graph structures can be\nmore effectively captured. (ii) The community adherence\nenables a fine-grained modeling of normal patterns, which\ncould be important for identifying some types of graph\nanomalies that depend on the context of graph communities.\nDisadvantages. (i) Community adherence-based methods\nare sensitive to hyperparameters like the number of clusters.\n(ii) Community adherence measures rely heavily on the\neffectiveness of the community detection methods.\nChallenge Addressed. Graph communities can be useful\nfor discovering important structural contexts (e.g., those\nbeyond a fixed-hop neighborhood) to detect anomalies that\ndeviate from the communities (C1). Community adherence\ncan provide one way for interpreting anomalies based on\ndeviations from expected community-based behaviors (C5).\n6.3\nLocal Affinity\nThere are many graph properties that can be important\nfor GAD, such as connectivity, degree distribution, and\nclustering coefficient. Local affinity is a graph property that\nintegrates multiple properties for evaluating the normality\nand abnormality of graph instances. The affinity may be\ndefined in various ways, such as the number of connections\nof a graph instances to neighboring instances and clustering\ncoefficient of the connected subgraphs. The key intuition\nis that the normal graph instances typically have a strong\naffinity with its neighbors, whereas an anomalous instance\nhas a significantly weaker affinity with its neighbors. Thus,\nthe local affinity can serve as the inverse of anomaly score.\nAssumption. Normal instances are connected with other\nnormal instances with similar attributes while anomalies are\noften graph instances that are less similar to their neighbors.\nFormally, the local affinity \u03c4(si) of an instance si can be\ndefined based on its average similarity to its neighbors:\n\u03c4 (si) =\n1\n|N (si)|\nX\nsj\u2208N(si)\nsim (hi, hj) ,\n(34)\nwhere hi and hj are the representation of instance si and sj,\nN (si) represents the neighboring instance si.\nTAM [98] is a seminal work that introduces local affinity\nas an anomaly measure. It aims to learn tailored node\nrepresentations for GAD by maximizing the local affinity of\nnodes to their neighbors. It is optimized on truncated graphs\nwhere non-homophily edges are removed iteratively to\nmitigate its adverse effects on the local affinity measure. The\nlearned representations result in a significantly stronger local\naffinity for the normal nodes than the abnormal nodes. CLAD\n[53] instead measures the affinity based on the discrepancy\nbetween a node and its neighbors using Jenson-Shannon\nDivergence. The anomaly score is obtained from the affinity\nfor each node in terms of both graph structure and attributes.\nPREM [85] eliminates the message-passing propagation in\nthe regular GNNs by using an ego neighbor matching-based\ncontrastive learning module. It aims to learn discriminative\npatterns between the local ego network and the neighboring\ninstances. These GAD methods are focused on the anomaly\nscore of a node based on its affinity to its neighboring nodes.\nAUGUST 2024\n16\nExploring beyond the node-level affinity can be one potential\napproach for subgraph- or graph-level anomaly detection.\nAdvantages. (i) The local affinity measure offers a novel\nway to quantify the abnormality of graph instances at a local\nscope. (ii) The measure provides a principled framework\nfor evaluating the normality from both the graph structure\nand attributes. (iii) It can be more interpretable than those\nidentified through task proxy-based GAD methods.\nDisadvantages. (i) Its effectiveness may vary if the affinity\nis specified differently. (ii) It is often designed based on\npredefined graph properties, making it difficult to generalize\nto the anomalies that do not conform to the properties.\nChallenge Addressed. Local affinity-based GAD methods\ncan adapt to and leverage various structural properties\nfor the detection of various types of anomalies (C1, C3),\nthough the prior knowledge about the properties is required.\nLocal affinity also provides a way to explain why a node is\nconsidered anomalous based on the local context, enhancing\nthe interpretability of GAD (C5).\n6.4\nGraph Isolation\nIsolation-based methods [61] is among the most popular\nmethods for anomaly detection. Due to its general effective-\nness across different datasets, it is also applied to identify\nanomalous graph instances [134], [164]. Its use for GAD\nis based on the isolation of graph instances in a feature\nrepresentation space.\nAssumption. Anomalous graph instances can be isolated\nmore easily than normal instances in the representation\nspace.\nThe methods in this group need to first learn the repre-\nsentations of the instances using a graph encoder GNNenc:\nhi = GNNenc(si, X, A; \u0398enc),\n(35)\nwhere hi is the representation of the graph instance si. An\nisolation mechanism is then applied on the representations,\ni.e., Isolation(hi), where the split process is formulated as\nthe following\nP2k \u2190\nn\nhi | h(jk)\ni\n\u2264\u03b7k, hi \u2208Pk\no\n,\nP2k+1 \u2190\nn\nhi | h(jk)\ni\n> \u03b7k, hi \u2208Pk\no\n,\n(36)\nwhere Pk is the node set of in the k-th binary partition tree\nand j is the dimension in h used to partition the feature\nspace. The abnormality of a graph instance s is evaluated by\nthe isolation difficulty in each tree of the tree set T:\nF(s | T) = \u2126\u03c4i\u223cTI (s | \u03c4i) ,\n(37)\nwhere I (s | \u03c4i) denotes a function to measure the isolation\ndifficulty in tree \u03c4i and \u2126denotes an integration function.\nDIF [134] presents a new representation scheme that\ncombines data partition and deep representation learning\nto perform isolation in randomly projected deep representa-\ntions for anomaly detection, showing good effectiveness in\nanomaly detection in various data types, including graph-\nlevel anomaly detection. GCAD [164] uses isolation forest for\nanomalous node detection. It uses the node representations\nafter subgraph normalization as the input to graph isolation.\nThe anomaly score is defined using a depth-based weighted\nscore that aggregates scores from various associated sub-\ngraphs. If we treat isolation-based measures as simpler\nalternatives to density estimation, there have been multiple\nother extensions [30], [30], [58], [151] that leverage the learned\ngraph representations to estimate a density-based anomaly\nscore for GAD.\nAdvantages. (i) Graph isolation measures are built on\nwell established isolation-based methodology for anomaly\ndetection. (ii) Many existing isolation-based methods may be\nadapted to anomaly detection on graph data.\nDisadvantages. (i) The isolation measure operates on\na continuous feature space, so its effectiveness relies on\nthe learning of an expressive representation space. (ii) The\nheuristic of isolation is difficult to be incorporated into GNN-\nbased representation learning, leading to less effective feature\nrepresentations for the subsequent isolation mechanism.\nChallenge Addressed. The graph isolation measure can\nadapt traditional anomaly measures to GAD with the power\nof GNN-based representation learning (C1). The isolation\nmechanism is highly efficient, allowing good scalability to\nGAD on large graphs (C2).\n7\nRESEARCH OPPORTUNITIES\nDespite the remarkable success of numerous existing GAD\nmethods, there are a range of research opportunities that\ncould be explored to tackle some largely unsolved GAD\nproblems.\nAdvanced Graph Anomaly Measures. Most current\nGAD methods are built upon proxy tasks or focused on\nthe GNN backbones using traditional non-graph anomaly\nmeasures, as summarized in Table 2. Consequently, they may\nfail to learn feature representations that encapsulate holistic\nconsideration of graph structure and attributes specifically\nfor GAD. Therefore, it is crucial to devise anomaly measures\nthat go beyond traditional anomaly measures and proxy\ntasks, such as local node affinity [98], for developing more\ndedicated methods for GAD.\nGAD on Complex Graphs. Most existing GAD meth-\nods are focused on small-scale (e.g., less than millions of\nnodes/edges), static, or homogeneous graph data, as shown\nin Tables 3 and 4 in Appendix B. However, many real-\nworld graphs can involve millions/billions of heterogeneous\nnodes/edges [56], such as real-life citation networks, social\nnetworks, and financial networks. The nodes/edges may\nappear in a streaming fashion, where the models can access\nto only limited graph data at one time step and may need to\nadapt to new normal/abnormal patterns as the graph evolves\n[103]. Current methods may be adapted to handle these\ngraphs, but their performance would become less effective\nsince their primary design do not consider those complexity.\nGAD methods designed for graphs with two or more of these\ncomplexities are required.\nHandling Anomaly Camouflage and Contamination. In\nGAD, anomalies might easily hide their abnormal charac-\nteristics by mimicking the structure and attributes of their\nneighboring instances. There have been some approaches\nfor addressing this problem, e.g., via selecting relevant fea-\ntures, incorporating domain knowledge, or using adversarial\ntraining [25], [99], but they often rely on the prior knowledge\nabout what specific features the attackers may use in the\nAUGUST 2024\n17\ncamouflage. A related important problem is the issue of\nanomaly contamination in the training data. Current methods\nare mostly unsupervised, working on anomaly-contaminated\ntraining data, but the anomalous instances in the graph\ncan largely bias the message-passing in the GNNs [98],\nleading to less expressive feature representations. Recent\napproaches, such as semi-supervised GAD on a small set of\nlabeled normal graph instances [99] or training GNNs using\ntruncated graph data [98], may offer effective methodologies\nfor handling these issues.\nInterpretable GAD. As shown by the summarized\ndetection performance results in Tables 5, 6 and 7 in Ap-\npendix C, current GAD methods have shown impressive\nsuccess in detecting anomalous graph instances, but they\ngenerally ignore the interpretability of their detection results.\nIn addition to accurate detection, interpretable GAD also\nrequires an explanation about why a graph instance is\nidentified as anomalous within a given graph structure,\nmaking it different from explaining anomalies in non-graph\ndata. Exploring information such as local graph structure,\nhuman feedback, and/or domain knowledge [67] would\nbe some interesting directions for providing the structure-\naware anomaly explanation. Also, the obtained anomaly\nexplanation may in turn be further leveraged to improve the\ndetection performance.\nOpen-set Supervised GAD. As shown in Table 2, there\nhave been many supervised GAD methods, most of which es-\nsentially tackle an imbalanced binary classification problem.\nSuch formulation is often questionable since anomalies per\nse can draw from very different distributions and cannot be\ntreated as from one concrete class. Open-set supervised GAD\nalso trains the detectors with labeled normal and anomalous\nexamples (i.e., seen anomalies), but it assumes an open set of\nanomaly classes (i.e., there are anomaly classes that are not\nillustrated by the training anomaly samples) rather than the\nclosed-set assumption in most existing studies [121]. Thus,\nit is a more realistic supervised GAD setting. Methods for\nthis setting have shown significantly better performance than\nunsupervised and fully supervised methods on visual data\n[1], [16], [136], [161] and tabular data [90]\u2013[92]. Recent studies\nin this line also show similar advantages on graph data [121],\n[158]. Exploring better modeling of the normal patterns while\nfitting the seen anomalies could be an effective approach\nto avoid overfitting of the seen anomalies (i.e., reducing\nmisclassification of the unseen anomalies as normal).\nFoundation Models for GAD. Leveraging foundation\nmodels for downstream tasks has been emerging as one effec-\ntive direction to empower the sample-efficient performance\nin the downstream tasks, including graph-related tasks [62],\n[111], [112], [130], owing to their superior generalization\nability. Two main directions for the GAD task involve the\ntraining of graph foundation models (GFMs) for GAD and\nthe exploitation of large language models (LLMs) for GAD.\nThere have been a number of successful tuning of foundation\nmodels for anomaly detection on image data [50], [156], [162]\nand video data [107], [127]\u2013[129] via proper prompt crafting,\nprompt learning, or in-context learning. Similar approaches\nmay be explored for GAD, such as the in-context learning-\nbased GAD method in [68] inspiring from [162]. This new\nparadigm is plausible for GAD in several aspects, such as\nzero/few-shot detection on target graph data, inductive GAD\n(most existing GAD methods take the transductive approach),\nand interpretable GAD with text description.\n8\nCONCLUSION\nIn this survey, we first discuss the complexities and existing\nchallenges in GAD. Then we present a novel taxonomy for\ndeep GAD methods from three new perspectives, including\nGNN backbone design, proxy task, and graph anomaly\nmeasures. We further deepen the discussions in each perspec-\ntive by discussing more fine-grained categories of methods\nthere. Along with each fine-grained methodology category,\nwe not only review the associated GAD methods, but also\nanalyze their general assumption, pros and cons, and their\ncapabilities in addressing the unique challenges in GAD. We\nlastly discuss six important directions for future research on\nGAD. By tackling the problems in these directions, we expect\nmuch more advanced generation of methods for solving\nreal-life GAD problems.\nREFERENCES\n[1]\nA. Acsintoae, A. Florescu, M.-I. Georgescu, T. Mare, P. Sumedrea,\nR. T. Ionescu, F. S. Khan, and M. Shah, \u201cUbnormal: New bench-\nmark for supervised open-set video anomaly detection,\u201d in CVPR,\n2022, pp. 20 143\u201320 153.\n[2]\nC. Aggarwal and K. Subbian, \u201cEvolutionary network analysis: A\nsurvey,\u201d ACM Computing Surveys (CSUR), vol. 47, no. 1, pp. 1\u201336,\n2014.\n[3]\nC. C. Aggarwal, Outlier analysis.\nSpringer, 2017.\n[4]\nL. Akoglu, H. Tong, and D. Koutra, \u201cGraph based anomaly\ndetection and description: a survey,\u201d Data mining and knowledge\ndiscovery, vol. 29, pp. 626\u2013688, 2015.\n[5]\nS. Bandyopadhyay, L. N, S. V. Vivek, and M. N. Murty, \u201cOutlier\nresistant unsupervised deep architectures for attributed network\nembedding,\u201d in WSDM, 2020, pp. 25\u201333.\n[6]\nY. Bei, S. Zhou, Q. Tan, H. Xu, H. Chen, Z. Li, and J. Bu,\n\u201cReinforcement neighborhood selection for unsupervised graph\nanomaly detection,\u201d in ICDM.\nIEEE, 2023, pp. 11\u201320.\n[7]\nA. Boukerche, L. Zheng, and O. Alfandi, \u201cOutlier detection:\nMethods, models, and classification,\u201d ACM Computing Surveys\n(CSUR), vol. 53, no. 3, pp. 1\u201337, 2020.\n[8]\nJ. Cai, Y. Zhang, and J. Fan, \u201cSelf-discriminative modeling for\nanomalous graph detection,\u201d arXiv:2310.06261, 2023.\n[9]\nJ. Cai, Y. Zhang, Z. Lu, W. Guo, and S.-k. Ng, \u201cFgad: Self-boosted\nknowledge distillation for an effective federated graph anomaly\ndetection framework,\u201d arXiv:2402.12761, 2024.\n[10]\nZ. Chai, S. You, Y. Yang, S. Pu, J. Xu, H. Cai, and W. Jiang, \u201cCan\nabnormality be detected by graph neural networks,\u201d in IJCAI,\n2022, pp. 23\u201329.\n[11]\nV. Chandola, A. Banerjee, and V. Kumar, \u201cAnomaly detection: A\nsurvey,\u201d ACM Computing Surveys, vol. 41, no. 3, p. 15, 2009.\n[12]\nW. Chang, K. Liu, K. Ding, P. S. Yu, and J. Yu, \u201cMultitask active\nlearning for graph anomaly detection,\u201d arXiv:2401.13210, 2024.\n[13]\nB. Chen, J. Zhang, X. Zhang, Y. Dong, J. Song, P. Zhang, K. Xu,\nE. Kharlamov, and J. Tang, \u201cGccad: Graph contrastive learning\nfor anomaly detection,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2022.\n[14]\nN. Chen, Z. Liu, B. Hooi, B. He, R. Fathony, J. Hu, and J. Chen,\n\u201cConsistency training with learnable data augmentation for graph\nanomaly detection with limited supervision,\u201d in ICLR, 2023.\n[15]\nZ. Chen, B. Liu, M. Wang, P. Dai, J. Lv, and L. Bo, \u201cGenerative\nadversarial attributed network anomaly detection,\u201d in CIKM, 2020,\npp. 1989\u20131992.\n[16]\nC. Ding, G. Pang, and C. Shen, \u201cCatching both gray and black\nswans: Open-set supervised anomaly detection,\u201d in CVPR, 2022,\npp. 7388\u20137398.\n[17]\nK. Ding, J. Li, N. Agarwal, and H. Liu, \u201cInductive anomaly\ndetection on attributed networks,\u201d in IJCAI, 2021, pp. 1288\u20131294.\n[18]\nK. Ding, J. Li, R. Bhanushali, and H. Liu, \u201cDeep anomaly detection\non attributed networks,\u201d in SDM.\nSIAM, 2019, pp. 594\u2013602.\nAUGUST 2024\n18\n[19]\nK. Ding, J. Li, and H. Liu, \u201cInteractive anomaly detection on\nattributed networks,\u201d in WSDM, 2019, pp. 357\u2013365.\n[20]\nK. Ding, X. Shan, and H. Liu, \u201cTowards anomaly-resistant graph\nneural networks via reinforcement learning,\u201d in CIKM, 2021, pp.\n2979\u20132983.\n[21]\nK. Ding, Q. Zhou, H. Tong, and H. Liu, \u201cFew-shot network\nanomaly detection via cross-network meta-learning,\u201d in WebConf,\n2021, pp. 2448\u20132456.\n[22]\nL. Dong, Y. Liu, X. Ao, J. Chi, J. Feng, H. Yang, and Q. He, \u201cBi-level\nselection via meta gradient for graph-based fraud detection,\u201d in\nDASFAA.\nSpringer, 2022, pp. 387\u2013394.\n[23]\nX. Dong, X. Zhang, Y. Sun, L. Chen, M. Yuan, and S. Wang,\n\u201cSmoothgnn: Smoothing-based gnn for unsupervised node\nanomaly detection,\u201d arXiv:2405.17525, 2024.\n[24]\nX. Dong, X. Zhang, and S. Wang, \u201cRayleigh quotient graph neural\nnetworks for graph-level anomaly detection,\u201d arXiv:2310.02861,\n2023.\n[25]\nY. Dou, Z. Liu, L. Sun, Y. Deng, H. Peng, and P. S. Yu, \u201cEnhancing\ngraph neural network-based fraud detectors against camouflaged\nfraudsters,\u201d in CIKM, 2020, pp. 315\u2013324.\n[26]\nD. Duan, L. Tong, Y. Li, J. Lu, L. Shi, and C. Zhang, \u201cAane:\nAnomaly aware network embedding for anomalous link detec-\ntion,\u201d in ICDM.\nIEEE, 2020, pp. 1002\u20131007.\n[27]\nJ. Duan, S. Wang, P. Zhang, E. Zhu, J. Hu, H. Jin, Y. Liu, and\nZ. Dong, \u201cGraph anomaly detection via multi-scale contrastive\nlearning networks with augmented view,\u201d in AAAI, vol. 37, no. 6,\n2023, pp. 7459\u20137467.\n[28]\nJ. Duan, B. Xiao, S. Wang, H. Zhou, and X. Liu, \u201cArise: Graph\nanomaly detection on attributed networks via substructure aware-\nness,\u201d IEEE transactions on neural networks and learning systems,\n2023.\n[29]\nJ. Duan, P. Zhang, S. Wang, J. Hu, H. Jin, J. Zhang, H. Zhou, and\nX. Liu, \u201cNormality learning-based graph anomaly detection via\nmulti-scale contrastive learning,\u201d in ACM MM, 2023, pp. 7502\u2013\n7511.\n[30]\nM. Ester, H.-P. Kriegel, J. Sander, X. Xu et al., \u201cA density-based\nalgorithm for discovering clusters in large spatial databases with\nnoise,\u201d in kdd, vol. 96, no. 34, 1996, pp. 226\u2013231.\n[31]\nH. Fan, F. Zhang, and Z. Li, \u201cAnomalydae: Dual autoencoder for\nanomaly detection on attributed networks,\u201d in ICASSP.\nIEEE,\n2020, pp. 5685\u20135689.\n[32]\nS. Fan, X. Wang, C. Shi, K. Kuang, N. Liu, and B. Wang, \u201cDebiased\ngraph neural networks with agnostic label selection bias,\u201d IEEE\ntransactions on neural networks and learning systems, 2022.\n[33]\nA. Fern\u00b4andez, S. Garcia, F. Herrera, and N. V. Chawla, \u201cSmote for\nlearning from imbalanced data: progress and challenges, marking\nthe 15-year anniversary,\u201d Journal of artificial intelligence research,\nvol. 61, pp. 863\u2013905, 2018.\n[34]\nY. Gao, J. Fang, Y. Sui, Y. Li, X. Wang, H. Feng, and Y. Zhang,\n\u201cGraph anomaly detection with bi-level optimization,\u201d in Proceed-\nings of the ACM on Web Conference 2024, 2024, pp. 4383\u20134394.\n[35]\nY. Gao, X. Wang, X. He et al., \u201cAlleviating structural distribution\nshift in graph anomaly detection,\u201d in WSDM, 2023, pp. 357\u2013365.\n[36]\nY. Gao, X. Wang, X. He, Z. Liu, H. Feng, and Y. Zhang, \u201cAddressing\nheterophily in graph anomaly detection: A perspective of graph\nspectrum,\u201d in WebConf, 2023, pp. 1528\u20131538.\n[37]\nZ. Gong, G. Wang, Y. Sun, Q. Liu, Y. Ning, H. Xiong, and J. Peng,\n\u201cBeyond homophily: Robust graph anomaly detection via neural\nsparsification,\u201d in IJCAI, 2023, pp. 2104\u20132113.\n[38]\nJ. Gou, B. Yu, S. J. Maybank, and D. Tao, \u201cKnowledge distillation:\nA survey,\u201d International Journal of Computer Vision, vol. 129, no. 6,\npp. 1789\u20131819, 2021.\n[39]\nW. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d NeurIPS, vol. 30, 2017.\n[40]\nD. K. Hammond, P. Vandergheynst, and R. Gribonval, \u201cWavelets\non graphs via spectral graph theory,\u201d Applied and Computational\nHarmonic Analysis, vol. 30, no. 2, pp. 129\u2013150, 2011.\n[41]\nG. E. Hinton and R. R. Salakhutdinov, \u201cReducing the dimension-\nality of data with neural networks,\u201d science, vol. 313, no. 5786, pp.\n504\u2013507, 2006.\n[42]\nJ. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d NeurIPS, vol. 33, pp. 6840\u20136851, 2020.\n[43]\nW. Hu, M. Fey, H. Ren, M. Nakata, Y. Dong, and J. Leskovec,\n\u201cOgb-lsc: A large-scale challenge for machine learning on graphs,\u201d\narXiv:2103.09430, 2021.\n[44]\nW. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta,\nand J. Leskovec, \u201cOpen graph benchmark: Datasets for machine\nlearning on graphs,\u201d in NeurIPS, vol. 33, 2020, pp. 22 118\u201322 133.\n[45]\nL. Huang, Y. Zhu, Y. Gao, T. Liu, C. Chang, C. Liu, Y. Tang,\nand C.-D. Wang, \u201cHybrid-order anomaly detection on attributed\nnetworks,\u201d IEEE Transactions on Knowledge and Data Engineering,\n2021.\n[46]\nM. Huang, Y. Liu, X. Ao, K. Li, J. Chi, J. Feng, H. Yang, and\nQ. He, \u201cAuc-oriented graph neural network for fraud detection,\u201d\nin WebConf, 2022, pp. 1311\u20131321.\n[47]\nT. Huang, Y. Pei, V. Menkovski, and M. Pechenizkiy, \u201cHop-count\nbased self-supervised anomaly detection on attributed networks,\u201d\nin ECMLPKDD.\nSpringer, 2022, pp. 225\u2013241.\n[48]\nX. Huang, Y. Yang, Y. Wang, C. Wang, Z. Zhang, J. Xu, L. Chen,\nand M. Vazirgiannis, \u201cDgraph: A large-scale financial dataset for\ngraph anomaly detection,\u201d NeurIPS, vol. 35, pp. 22 765\u201322 777,\n2022.\n[49]\nY. Huang, L. Wang, F. Zhang, and X. Lin, \u201cAre we really\nmaking much progress in unsupervised graph outlier detection?\nrevisiting the problem with new insight and superior method,\u201d\narXiv:2210.12941, 2022.\n[50]\nJ. Jeong, Y. Zou, T. Kim, D. Zhang, A. Ravichandran, and\nO. Dabeer, \u201cWinclip: Zero-/few-shot anomaly classification and\nsegmentation,\u201d in CVPR, 2023, pp. 19 606\u201319 616.\n[51]\nM. Jin, Y. Liu, Y. Zheng, L. Chi, Y.-F. Li, and S. Pan, \u201cAnemone:\nGraph anomaly detection with multi-scale contrastive learning,\u201d\nin CIKM, 2021, pp. 3122\u20133126.\n[52]\nH. Kim, J. Kim, B. S. Lee, and S. Lim, \u201cDeep semi-supervised\nanomaly detection with metapath-based context knowledge,\u201d\narXiv:2308.10918, 2023.\n[53]\nJ. Kim, Y. In, K. Yoon, J. Lee, and C. Park, \u201cClass label-aware graph\nanomaly detection,\u201d in CIKM, 2023, pp. 4008\u20134012.\n[54]\nT. N. Kipf and M. Welling, \u201cSemi-supervised classification with\ngraph convolutional networks,\u201d arXiv:1609.02907, 2016.\n[55]\nX. Kong, W. Zhang, H. Wang, M. Hou, X. Chen, X. Yan, and\nS. K. Das, \u201cFederated graph anomaly detection via contrastive\nself-supervised learning,\u201d IEEE Transactions on Neural Networks\nand Learning Systems, 2024.\n[56]\nJ. Li, G. Pang, L. Chen, and M.-R. Namazi-Rad, \u201cHrgcn: Heteroge-\nneous graph-level anomaly detection with hierarchical relation-\naugmented graph neural networks,\u201d in DSAA.\nIEEE, 2023, pp.\n1\u201310.\n[57]\nJ. Li, K. Cheng, S. Wang, F. Morstatter, R. P. Trevino, J. Tang, and\nH. Liu, \u201cFeature selection: A data perspective,\u201d ACM computing\nsurveys (CSUR), vol. 50, no. 6, pp. 1\u201345, 2017.\n[58]\nY. Li, X. Huang, J. Li, M. Du, and N. Zou, \u201cSpecae: Spectral\nautoencoder for anomaly detection in attributed networks,\u201d in\nCIKM, 2019, pp. 2233\u20132236.\n[59]\nF. Lin, X. Luo, J. Wu, J. Yang, S. Xue, Z. Wang, and H. Gong,\n\u201cDiscriminative graph-level anomaly detection via dual-students-\nteacher model,\u201d in ADMA.\nSpringer, 2023, pp. 261\u2013276.\n[60]\nF. Liu, X. Ma, J. Wu, J. Yang, S. Xue, A. Beheshti, C. Zhou, H. Peng,\nQ. Z. Sheng, and C. C. Aggarwal, \u201cDagad: Data augmentation for\ngraph anomaly detection,\u201d in ICDM.\nIEEE, 2022, pp. 259\u2013268.\n[61]\nF. T. Liu, K. M. Ting, and Z.-H. Zhou, \u201cIsolation forest,\u201d in ICDM.\nIEEE, 2008, pp. 413\u2013422.\n[62]\nJ. Liu, C. Yang, Z. Lu, J. Chen, Y. Li, M. Zhang, T. Bai, Y. Fang,\nL. Sun, P. S. Yu et al., \u201cTowards graph foundation models: A survey\nand beyond,\u201d arXiv:2310.11829, 2023.\n[63]\nJ. Liu, X. Shang, X. Han, W. Zhang, and H. Yin, \u201cSpatial-temporal\nmemories enhanced graph autoencoder for anomaly detection in\ndynamic graphs,\u201d arXiv:2403.09039, 2024.\n[64]\nK. Liu, Y. Dou, Y. Zhao, X. Ding, X. Hu, R. Zhang, K. Ding, C. Chen,\nH. Peng, K. Shu et al., \u201cBond: Benchmarking unsupervised outlier\nnode detection on static attributed graphs,\u201d NeurIPS, vol. 35, pp.\n27 021\u201327 035, 2022.\n[65]\nK. Liu, H. Zhang, Z. Hu, F. Wang, and P. S. Yu, \u201cData augmentation\nfor supervised graph outlier detection with latent diffusion\nmodels,\u201d arXiv:2312.17679, 2023.\n[66]\nY. Liu, X. Ao, Z. Qin, J. Chi, J. Feng, H. Yang, and Q. He, \u201cPick\nand choose: a gnn-based imbalanced learning approach for fraud\ndetection,\u201d in WebConf, 2021, pp. 3168\u20133177.\n[67]\nY. Liu, K. Ding, Q. Lu, F. Li, L. Y. Zhang, and S. Pan, \u201cTowards\nself-interpretable graph-level anomaly detection,\u201d NeurIPS, vol. 36,\n2024.\nAUGUST 2024\n19\n[68]\nY. Liu, S. Li, Y. Zheng, Q. Chen, C. Zhang, and S. Pan, \u201cArc:\nA generalist graph anomaly detector with in-context learning,\u201d\narXiv:2405.16771, 2024.\n[69]\nY. Liu, Z. Li, S. Pan, C. Gong, C. Zhou, and G. Karypis, \u201cAnomaly\ndetection on attributed networks via contrastive self-supervised\nlearning,\u201d IEEE transactions on neural networks and learning systems,\nvol. 33, no. 6, pp. 2378\u20132392, 2021.\n[70]\nY. Liu, S. Pan, Y. G. Wang, F. Xiong, L. Wang, Q. Chen, and\nV. C. Lee, \u201cAnomaly detection in dynamic graphs via transformer,\u201d\nIEEE Transactions on Knowledge and Data Engineering, vol. 35, no. 12,\npp. 12 081\u201312 094, 2021.\n[71]\nZ. Liu, Y. Li, N. Chen, Q. Wang, B. Hooi, and B. He, \u201cA survey of\nimbalanced learning on graphs: Problems, techniques, and future\ndirections,\u201d arXiv:2308.13821, 2023.\n[72]\nZ. Liu, Z. Zeng, R. Qiu, H. Yoo, D. Zhou, Z. Xu, Y. Zhu,\nK. Weldemariam, J. He, and H. Tong, \u201cTopological augmentation\nfor class-imbalanced node classification,\u201d arXiv:2308.14181, 2023.\n[73]\nZ. Liu, Y. Dou, P. S. Yu, Y. Deng, and H. Peng, \u201cAlleviating the\ninconsistency problem of applying graph neural network to fraud\ndetection,\u201d in SIGIR, 2020, pp. 1569\u20131572.\n[74]\nZ. Liu, C. Cao, and J. Sun, \u201cMul-gad: a semi-supervised graph\nanomaly detection framework via aggregating multi-view infor-\nmation,\u201d arXiv:2212.05478, 2022.\n[75]\nZ. Liu, C. Cao, F. Tao, and J. Sun, \u201cRevisiting graph contrastive\nlearning for anomaly detection,\u201d arXiv:2305.02496, 2023.\n[76]\nS. Lou, Q. Zhang, S. Yang, Y. Tian, Z. Tan, and M. Luo,\n\u201cGady: Unsupervised anomaly detection on dynamic graphs,\u201d\narXiv:2310.16376, 2023.\n[77]\nX. Luo, J. Wu, A. Beheshti, J. Yang, X. Zhang, Y. Wang, and S. Xue,\n\u201cComga: Community-aware attributed graph anomaly detection,\u201d\nin WSDM, 2022, pp. 657\u2013665.\n[78]\nJ. Ma, P. Cui, K. Kuang, X. Wang, and W. Zhu, \u201cDisentangled\ngraph convolutional networks,\u201d in ICML.\nPMLR, 2019, pp. 4212\u2013\n4221.\n[79]\nR. Ma, G. Pang, L. Chen, and A. van den Hengel, \u201cDeep graph-\nlevel anomaly detection by glocal knowledge distillation,\u201d in\nWSDM, 2022, pp. 704\u2013714.\n[80]\nX. Ma, R. Li, F. Liu, K. Ding, J. Yang, and J. Wu, \u201cNew recipes for\ngraph anomaly detection: Forward diffusion dynamics and graph\ngeneration,\u201d 2023.\n[81]\nX. Ma, J. Wu, S. Xue, J. Yang, C. Zhou, Q. Z. Sheng, H. Xiong, and\nL. Akoglu, \u201cA comprehensive survey on graph anomaly detection\nwith deep learning,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2021.\n[82]\nX. Ma, J. Wu, J. Yang, and Q. Z. Sheng, \u201cTowards graph-level\nanomaly detection via deep evolutionary mapping,\u201d in KDD,\n2023, pp. 1631\u20131642.\n[83]\nL. Meng, H. Mostafa, M. Nassar, X. Zhang, and J. Zhang, \u201cGener-\native graph augmentation for minority class in fraud detection,\u201d\nin CIKM, 2023, pp. 4200\u20134204.\n[84]\nC. Niu, G. Pang, and L. Chen, \u201cGraph-level anomaly detection\nvia hierarchical memory networks,\u201d in ECMLPKDD.\nSpringer,\n2023, pp. 201\u2013218.\n[85]\nJ. Pan, Y. Liu, Y. Zheng, and S. Pan, \u201cPrem: A simple yet\neffective approach for node-level graph anomaly detection,\u201d\narXiv:2310.11676, 2023.\n[86]\nG. Pang, L. Cao, L. Chen, D. Lian, and H. Liu, \u201cSparse modeling-\nbased sequential ensemble learning for effective outlier detection\nin high-dimensional numeric data,\u201d in AAAI, vol. 32, no. 1, 2018.\n[87]\nG. Pang, L. Cao, L. Chen, and H. Liu, \u201cUnsupervised feature\nselection for outlier detection by modelling hierarchical value-\nfeature couplings,\u201d in ICDM.\nIEEE, 2016, pp. 410\u2013419.\n[88]\nG. Pang, L. Cao Longbing, Chen, and H. Liu, \u201cLearning homophily\ncouplings from non-iid data for joint feature selection and noise-\nresilient outlier detection,\u201d in IJCAI, 2017, pp. 2585\u20132591.\n[89]\nG. Pang, C. Shen, L. Cao, and A. V. D. Hengel, \u201cDeep learning for\nanomaly detection: A review,\u201d ACM computing surveys (CSUR),\nvol. 54, no. 2, pp. 1\u201338, 2021.\n[90]\nG. Pang, C. Shen, H. Jin, and A. van den Hengel, \u201cDeep weakly-\nsupervised anomaly detection,\u201d in KDD, 2023, pp. 1795\u20131807.\n[91]\nG. Pang, C. Shen, and A. van den Hengel, \u201cDeep anomaly\ndetection with deviation networks,\u201d in KDD, 2019, pp. 353\u2013362.\n[92]\nG. Pang, A. van den Hengel, C. Shen, and L. Cao, \u201cToward\ndeep supervised anomaly detection: Reinforcement learning from\npartially labeled anomaly data,\u201d in KDD, 2021, pp. 1298\u20131308.\n[93]\nG. Pang, H. Xu, L. Cao, and W. Zhao, \u201cSelective value coupling\nlearning for detecting outliers in high-dimensional categorical\ndata,\u201d in CIKM, 2017, pp. 807\u2013816.\n[94]\nJ. Park, J. Song, and E. Yang, \u201cGraphens: Neighbor-aware ego\nnetwork synthesis for class-imbalanced node classification,\u201d in\nICLR, 2021.\n[95]\nY. Pei, T. Huang, W. van Ipenburg, and M. Pechenizkiy, \u201cResgcn:\nattention-based deep residual modeling for anomaly detection\non attributed networks,\u201d Machine Learning, vol. 111, no. 2, pp.\n519\u2013541, 2022.\n[96]\nZ. Peng, M. Luo, J. Li, L. Xue, and Q. Zheng, \u201cA deep multi-view\nframework for anomaly detection on attributed networks,\u201d IEEE\nTransactions on Knowledge and Data Engineering, vol. 34, no. 6, pp.\n2539\u20132552, 2020.\n[97]\nT. Pourhabibi, K.-L. Ong, B. H. Kam, and Y. L. Boo, \u201cFraud\ndetection: A systematic literature review of graph-based anomaly\ndetection approaches,\u201d Decision Support Systems, vol. 133, p.\n113303, 2020.\n[98]\nH. Qiao and G. Pang, \u201cTruncated affinity maximization: One-\nclass homophily modeling for graph anomaly detection,\u201d NeurIPS,\nvol. 36, 2024.\n[99]\nH. Qiao, Q. Wen, X. Li, E.-P. Lim, and G. Pang, \u201cGenerative semi-\nsupervised graph anomaly detection,\u201d arXiv:2402.11887, 2024.\n[100] Z. Qin, Y. Liu, Q. He, and X. Ao, \u201cExplainable graph-based fraud\ndetection via neural meta-graph search,\u201d in CIKM, 2022, pp. 4414\u2013\n4418.\n[101] C. Qiu, M. Kloft, S. Mandt, and M. Rudolph, \u201cRaising the bar in\ngraph-level anomaly detection,\u201d arXiv:2205.13845, 2022.\n[102] L. Qu, H. Zhu, R. Zheng, Y. Shi, and H. Yin, \u201cImgagn: Imbalanced\nnetwork embedding via generative adversarial graph networks,\u201d\nin KDD, 2021, pp. 1390\u20131398.\n[103] S. Ranshous, S. Shen, D. Koutra, S. Harenberg, C. Faloutsos, and\nN. F. Samatova, \u201cAnomaly detection in dynamic networks: a\nsurvey,\u201d Wiley Interdisciplinary Reviews: Computational Statistics,\nvol. 7, no. 3, pp. 223\u2013247, 2015.\n[104] A. Roy, J. Shu, O. Elshocht, J. Smeets, R. Zhang, and P. Li, \u201cGad-\nebm: Graph anomaly detection using energy-based models,\u201d in\nNeurIPS 2023 Workshop: New Frontiers in Graph Learning, 2023.\n[105] L. Ruff, R. Vandermeulen, N. Goernitz, L. Deecke, S. A. Siddiqui,\nA. Binder, E. M\u00a8uller, and M. Kloft, \u201cDeep one-class classification,\u201d\nin ICML.\nPMLR, 2018, pp. 4393\u20134402.\n[106] B. Sanchez-Lengeling, J. Wei, B. Lee, E. Reif, P. Wang, W. Qian,\nK. McCloskey, L. Colwell, and A. Wiltschko, \u201cEvaluating attri-\nbution for graph neural networks,\u201d in NeurIPS, vol. 33, 2020, pp.\n5898\u20135910.\n[107] F. Sato, R. Hachiuma, and T. Sekii, \u201cPrompt-guided zero-shot\nanomaly action recognition using pretrained deep skeleton fea-\ntures,\u201d in CVPR, 2023, pp. 6471\u20136480.\n[108] F. Shi, Y. Cao, Y. Shang, Y. Zhou, C. Zhou, and J. Wu, \u201cH2-fdetector:\nA gnn-based fraud detector with homophilic and heterophilic\nconnections,\u201d in WebConf, 2022, pp. 1486\u20131494.\n[109] M. Shi, Y. Tang, X. Zhu, D. Wilson, and J. Liu, \u201cMulti-class\nimbalanced graph convolutional network learning,\u201d in IJCAI,\n2020.\n[110] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Van-\ndergheynst, \u201cThe emerging field of signal processing on graphs:\nExtending high-dimensional data analysis to networks and other\nirregular domains,\u201d IEEE signal processing magazine, vol. 30, no. 3,\npp. 83\u201398, 2013.\n[111] X. Sun, H. Cheng, J. Li, B. Liu, and J. Guan, \u201cAll in one: Multi-\ntask prompting for graph neural networks,\u201d in KDD, 2023, pp.\n2120\u20132131.\n[112] J. Tang, Y. Yang, W. Wei, L. Shi, L. Xia, D. Yin, and C. Huang,\n\u201cHigpt: Heterogeneous graph language model,\u201d arXiv:2402.16024,\n2024.\n[113] J. Tang, F. Hua, Z. Gao, P. Zhao, and J. Li, \u201cGadbench: Revis-\niting and benchmarking supervised graph anomaly detection,\u201d\narXiv:2306.12251, 2023.\n[114] J. Tang, J. Li, Z. Gao, and J. Li, \u201cRethinking graph neural networks\nfor anomaly detection,\u201d in ICML.\nPMLR, 2022, pp. 21 076\u201321 089.\n[115] X. Teng, M. Yan, A. M. Ertugrul, and Y.-R. Lin, \u201cDeep into\nhypersphere: Robust and unsupervised anomaly discovery in\ndynamic networks,\u201d in IJCAI, 2018.\n[116] S. Tian, J. Dong, J. Li, W. Zhao, X. Xu, B. Song, C. Meng, T. Zhang,\nL. Chen et al., \u201cSad: Semi-supervised anomaly detection on\ndynamic graphs,\u201d arXiv:2305.13573, 2023.\nAUGUST 2024\n20\n[117] P. Veli\u02c7ckovi\u00b4c, G. Cucurull, A. Casanova, A. Romero, P. Lio, and\nY. Bengio, \u201cGraph attention networks,\u201d arXiv:1710.10903, 2017.\n[118] P. Veli\u02c7ckovi\u00b4c, W. Fedus, W. L. Hamilton, P. Li`o, Y. Bengio, and\nR. D. Hjelm, \u201cDeep graph infomax,\u201d arXiv:1809.10341, 2018.\n[119] D. Wang, J. Lin, P. Cui, Q. Jia, Z. Wang, Y. Fang, Q. Yu, J. Zhou,\nS. Yang, and Y. Qi, \u201cA semi-supervised graph attentive network\nfor financial fraud detection,\u201d in ICDM.\nIEEE, 2019, pp. 598\u2013607.\n[120] Q. Wang, G. Pang, M. Salehi, W. Buntine, and C. Leckie, \u201cCross-\ndomain graph anomaly detection via anomaly-aware contrastive\nalignment,\u201d in AAAI, vol. 37, no. 4, 2023, pp. 4676\u20134684.\n[121] Q. Wang, G. Pang, M. Salehi et al., \u201cOpen-set graph anomaly\ndetection via normal structure regularisation,\u201d arXiv:2311.06835,\n2023.\n[122] X. Wang, B. Jin, Y. Du, P. Cui, Y. Tan, and Y. Yang, \u201cOne-class graph\nneural networks for anomaly detection in attributed networks,\u201d\nNeural computing and applications, vol. 33, pp. 12 073\u201312 085, 2021.\n[123] Y. Wang, J. Zhang, S. Guo, H. Yin, C. Li, and H. Chen, \u201cDecoupling\nrepresentation learning and classification for gnn-based anomaly\ndetection,\u201d in SIGIR, 2021, pp. 1239\u20131248.\n[124] Y. Wang, J. Zhang, Z. Huang, W. Li, S. Feng, Z. Ma, Y. Sun, D. Yu,\nF. Dong, J. Jin et al., \u201cLabel information enhanced fraud detection\nagainst low homophily in graphs,\u201d in WebConf, 2023, pp. 406\u2013416.\n[125] B. Wu, X. Yao, B. Zhang, K.-M. Chao, and Y. Li, \u201cSplitgnn: Spectral\ngraph neural network for fraud detection against heterophily,\u201d in\nCIKM, 2023, pp. 2737\u20132746.\n[126] L. Wu, J. Xia, Z. Gao, H. Lin, C. Tan, and S. Z. Li, \u201cGraphmixup:\nImproving class-imbalanced node classification by reinforcement\nmixup and self-supervised context prediction,\u201d in ECMLPKDD.\nSpringer, 2022, pp. 519\u2013535.\n[127] P. Wu, X. Zhou, G. Pang, Y. Sun, J. Liu, P. Wang, and Y. Zhang,\n\u201cOpen-vocabulary video anomaly detection,\u201d in CVPR, 2024, pp.\n18 297\u201318 307.\n[128] P. Wu, X. Zhou, G. Pang, Z. Yang, Q. Yan, P. WANG, and Y. Zhang,\n\u201cWeakly supervised video anomaly detection and localization with\nspatio-temporal prompts,\u201d in ACM MM, 2024.\n[129] P. Wu, X. Zhou, G. Pang, L. Zhou, Q. Yan, P. Wang, and Y. Zhang,\n\u201cVadclip: Adapting vision-language models for weakly supervised\nvideo anomaly detection,\u201d in AAAI, vol. 38, no. 6, 2024, pp. 6074\u2013\n6082.\n[130] L. Xia, B. Kao, and C. Huang, \u201cOpengraph: Towards open graph\nfoundation models,\u201d arXiv:2403.01121, 2024.\n[131] C. Xiao, X. Xu, Y. Lei, K. Zhang, S. Liu, and F. Zhou, \u201cCounterfac-\ntual graph learning for anomaly detection on attributed networks,\u201d\nIEEE Transactions on Knowledge and Data Engineering, 2023.\n[132] F. Xu, N. Wang, X. Wen, M. Gao, C. Guo, and X. Zhao, \u201cFew-\nshot message-enhanced contrastive learning for graph anomaly\ndetection,\u201d arXiv:2311.10370, 2023.\n[133] F. Xu, N. Wang, H. Wu, X. Wen, and X. Zhao, \u201cRevisiting graph-\nbased fraud detection in sight of heterophily and spectrum,\u201d\narXiv:2312.06441, 2023.\n[134] H. Xu, G. Pang, Y. Wang, and Y. Wang, \u201cDeep isolation forest\nfor anomaly detection,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2023.\n[135] Z. Xu, X. Huang, Y. Zhao, Y. Dong, and J. Li, \u201cContrastive\nattributed network anomaly detection with data augmentation,\u201d\nin PAKDD.\nSpringer, 2022, pp. 444\u2013457.\n[136] X. Yao, R. Li, J. Zhang, J. Sun, and C. Zhang, \u201cExplicit bound-\nary guided semi-push-pull contrastive learning for supervised\nanomaly detection,\u201d in CVPR, 2023, pp. 24 490\u201324 499.\n[137] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, \u201cGraph\ncontrastive learning with augmentations,\u201d NeurIPS, vol. 33, pp.\n5812\u20135823, 2020.\n[138] H. Yu, Z. Liu, and X. Luo, \u201cBarely supervised learning for graph-\nbased fraud detection,\u201d in AAAI, vol. 38, no. 15, 2024, pp. 16 548\u2013\n16 557.\n[139] R. Yu, H. Qiu, Z. Wen, C. Lin, and Y. Liu, \u201cA survey on social\nmedia anomaly detection,\u201d ACM SIGKDD Explorations Newsletter,\nvol. 18, no. 1, pp. 1\u201314, 2016.\n[140] W. Yu, W. Cheng, C. C. Aggarwal, K. Zhang, H. Chen, and\nW. Wang, \u201cNetwalk: A flexible deep embedding approach for\nanomaly detection in dynamic networks,\u201d in KDD, 2018, pp. 2672\u2013\n2681.\n[141] X. Yuan, N. Zhou, S. Yu, H. Huang, Z. Chen, and F. Xia, \u201cHigher-\norder structure based anomaly detection on attributed networks,\u201d\nin BigData.\nIEEE, 2021, pp. 2691\u20132700.\n[142] G. Zhang, J. Wu, J. Yang, A. Beheshti, S. Xue, C. Zhou, and\nQ. Z. Sheng, \u201cFraudre: Fraud detection dual-resistant to graph\ninconsistency and imbalance,\u201d in ICDM.\nIEEE, 2021, pp. 867\u2013876.\n[143] G. Zhang, Z. Yang, J. Wu, J. Yang, S. Xue, H. Peng, J. Su, C. Zhou,\nQ. Z. Sheng, L. Akoglu et al., \u201cDual-discriminative graph neural\nnetwork for imbalanced graph-level anomaly detection,\u201d Advances\nin Neural Information Processing Systems, vol. 35, pp. 24 144\u201324 157,\n2022.\n[144] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, \u201cmixup:\nBeyond empirical risk minimization,\u201d arXiv:1710.09412, 2017.\n[145] J. Zhang, S. Wang, and S. Chen, \u201cReconstruction enhanced multi-\nview contrastive learning for anomaly detection on attributed\nnetworks,\u201d arXiv:2205.04816, 2022.\n[146] L. Zhang, J. Yuan, Z. Liu, Y. Pei, and L. Wang, \u201cA robust\nembedding method for anomaly detection on attributed networks,\u201d\nin IJCNN.\nIEEE, 2019, pp. 1\u20138.\n[147] R. Zhang, D. Cheng, X. Liu, J. Yang, Y. Ouyang, X. Wu, and\nY. Zheng, \u201cGeneration is better than modification: Combating\nhigh class homophily variance in graph anomaly detection,\u201d\narXiv:2403.10339, 2024.\n[148] Y. Zhang, Y. Sun, J. Cai, and J. Fan, \u201cDeep graph-level\northogonal hypersphere compression for anomaly detection,\u201d\narXiv:2302.06430, 2023.\n[149] L. Zhao and L. Akoglu, \u201cOn using classification datasets to\nevaluate graph outlier detection: Peculiar observations and new\ninsights,\u201d Big Data, vol. 11, no. 3, pp. 151\u2013180, 2023.\n[150] T. Zhao, X. Zhang, and S. Wang, \u201cGraphsmote: Imbalanced node\nclassification on graphs with graph neural networks,\u201d in WSDM,\n2021, pp. 833\u2013841.\n[151] T. Zhao, B. Ni, W. Yu, Z. Guo, N. Shah, and M. Jiang, \u201cAction\nsequence augmentation for early graph-based anomaly detection,\u201d\nin CIKM, 2021, pp. 2668\u20132678.\n[152] L. Zheng, Z. Li, J. Li, Z. Li, and J. Gao, \u201cAddgraph: Anomaly\ndetection in dynamic graph using attention-based temporal gcn.\u201d\nin IJCAI, vol. 3, 2019, p. 7.\n[153] M. Zheng, C. Zhou, J. Wu, S. Pan, J. Shi, and L. Guo, \u201cFraudne: a\njoint embedding approach for fraud detection,\u201d in IJCNN.\nIEEE,\n2018, pp. 1\u20138.\n[154] X. Zheng, Y. Wang, Y. Liu, M. Li, M. Zhang, D. Jin, P. S. Yu, and\nS. Pan, \u201cGraph neural networks for graphs with heterophily: A\nsurvey,\u201d arXiv:2202.07082, 2022.\n[155] Y. Zheng, M. Jin, Y. Liu, L. Chi, K. T. Phan, and Y.-P. P. Chen,\n\u201cGenerative and contrastive self-supervised learning for graph\nanomaly detection,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2021.\n[156] Q. Zhou, G. Pang, Y. Tian, S. He, and J. Chen, \u201cAnomalyclip:\nObject-agnostic prompt learning for zero-shot anomaly detection,\u201d\nin ICLR, 2024.\n[157] Q. Zhou, K. Ding, H. Liu, and H. Tong, \u201cLearning node abnormal-\nity with weak supervision,\u201d in CIKM, 2023, pp. 3584\u20133594.\n[158] S. Zhou, X. Huang, N. Liu, Q. Tan, and F.-L. Chung, \u201cUnseen\nanomaly detection on networks via multi-hypersphere learning,\u201d\nin SDM.\nSIAM, 2022, pp. 262\u2013270.\n[159] S. Zhou, X. Huang, N. Liu, H. Zhou, F.-L. Chung, and L.-K. Huang,\n\u201cImproving generalizability of graph anomaly detection models\nvia data augmentation,\u201d IEEE Transactions on Knowledge and Data\nEngineering, 2023.\n[160] S. Zhou, Q. Tan, Z. Xu, X. Huang, and F.-l. Chung, \u201cSubtractive\naggregation for attributed network anomaly detection,\u201d in CIKM,\n2021, pp. 3672\u20133676.\n[161] J. Zhu, C. Ding, Y. Tian, and G. Pang, \u201cAnomaly heterogeneity\nlearning for open-set supervised anomaly detection,\u201d in CVPR,\n2024, pp. 17 616\u201317 626.\n[162] J. Zhu and G. Pang, \u201cToward generalist anomaly detection via\nin-context residual learning with few-shot sample prompts,\u201d in\nCVPR, 2024, pp. 17 826\u201317 836.\n[163] J. Zhu, Y. Yan, L. Zhao, M. Heimann, L. Akoglu, and D. Koutra,\n\u201cBeyond homophily in graph neural networks: Current limitations\nand effective designs,\u201d NeurIPS, vol. 33, pp. 7793\u20137804, 2020.\n[164] Z. Zhuang, K. M. Ting, G. Pang, and S. Song, \u201cSubgraph\ncentralization: a necessary step for graph anomaly detection,\u201d\nin SDM.\nSIAM, 2023, pp. 703\u2013711.\n[165] W. Zhuo, Z. Liu, B. Hooi, B. He, G. Tan, R. Fathony, and J. Chen,\n\u201cPartitioning message passing for graph fraud detection,\u201d in ICLR,\n2023.\nAUGUST 2024\n21\nAPPENDIX A\nALGORITHMS\nTo gain a more in-depth understanding of Deep GAD\nmethods, in Table 2, we review and summarize the key char-\nacteristics of representative algorithms from each category.\nSome key observations are as follows. (i) Most current meth-\nods focus on supervised and unsupervised methods. GNN\nbackbone-based methods are mostly supervised methods,\nwhile Proxy task design-based and anomaly measure meth-\nods are mostly unsupervised. (ii) The type of datasets used\nto evaluate each method is different. Supervised methods\nusually use data sets with real anomalies, while unsuper-\nvised methods usually use data sets with synthetic/injected\nanomalies. There are a small number of methods that use\nboth types of data sets. (iii) There have been new types of\nmethods for GAD, such as semi-supervised settings with\nsome labeled normal nodes, or open-set supervised GAD.\nAPPENDIX B\nDATASETS\nWe also collected and summarized publicly available GAD\ndata sets, including node-level, graph-level, and dynamic\ngraph datasets. Typically, these datasets can be categorized\ninto synthetic datasets with injected anomalies and real-\nworld datasets with genuine anomalies. Some studies inject\nspecific types of exceptional samples into existing graph\ndatasets, such as contextual and structure-based anomalies\n[18], [69]. In Table 3 and Table 4, we provide some statistical\ninformation about the dataset, including the number of\nnodes, the data volume of edges, the ratio of anomalies,\nand whether the anomalies are real or injected.\nAPPENDIX C\nQUANTITATIVE COMPARISON\nThe way we compare the performance of different methods\nis to collect experimental results using the same dataset\nfrom their original papers. In this subsection, we do such\nan empirical comparison. Tables 5 and 6 highlight the\nperformance of several methods on both real and synthetic\nGAD datasets.\nAUGUST 2024\n22\nTABLE 2: Key characteristics of representative deep GAD methods ordered first by publication time and then the methodology.\nMethod (Ref.)\nSupervision\nGraph Type\nGraph Instance\nAnomaly Type\nMethodology\nYear\nCode\nNetwalk [140]\nUnsupervised\nDynamic\nEdge\nGenuine\nProxy Task Design\n2018\nLink\nFraduNE [153]\nSupervised\nStatic\nSubGraph\nGenuine\nAnomaly Measures\n2018\nLink\nSemiGNN [119]\nSupervised\nStatic\nNode\nInjected\nGNN Backbone\n2019\nN/A\nAddGraph [152]\nUnsupervised\nDynamic\nEdge\nGenuine\nProxy Task Design\n2019\nLink\nDOMINANT [18]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2019\nLink\nGraphUCB [19]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2019\nLink\nGraphConsis [73]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2020\nLink\nCARE-GNN [25]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2020\nLink\nAnomalyDAE [31]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2020\nLink\nGAAN [15]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2020\nLink\nALARM [96]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2020\nLink\nAdoNE [5]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2020\nLink\nAANE [26]\nUnsupervised\nStatic\nEdge\nGenuine\nAnomaly Measures\n2020\nN/A\nPC-GNN [66]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2021\nLink\nFRAUDRE [142]\nSupervised\nStatic\nNode\nInjected\nGNN Backbone\n2021\nLink\nDCI [123]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2021\nLink\nRARE-GNN [20]\nSupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nN/A\nCoLA [69]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nLink\nANEMONE [51]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nLink\nSL-GAD [155]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nLink\nAEGIS [17]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nLink\nMeta-GDN [21]\nSupervised\nStatic\nNode\nInjected\nProxy Task Design\n2021\nLink\nOCGNN [122]\nUnsupervised\nStatic\nNode\nInjected\nAnomaly Measures\n2021\nLink\nAAGNN [160]\nUnsupervised\nStatic\nNode\nInjected\nAnomaly Measures\n2021\nLink\nNGS [100]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nLink\nH2-FDetector [108]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nLink\niGAD [143]\nSupervised\nStatic\nGraph\nGenuine\nGNN Backbone\n2022\nN/A\nBLS [22]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nN/A\nAO-GNN [46]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nN/A\nDAGAD [60]\nSupervised\nStatic\nNod\nInjected\nGNN Backbone\n2022\nLink\nBWGNN [114]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nLink\nAMNet [10]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2022\nLink\nCONAD [135]\nUnsupervised\nStatic\nNode\nBoth\nProxy Task Design\n2022\nLink\nHCM-A [47]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2022\nLink\nComGA [77]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2022\nLink\nResGCN [95]\nUnsupervised\nStatic\nNode\nBoth\nProxy Task Design\n2022\nLink\nGCCAD [13]\nSupervised\nStatic\nGraph\nGenuine\nProxy Task Design\n2022\nLink\nGlocaKD [79]\nUnsupervised\nStatic\nGraph\nGenuine\nProxy Task Design\n2022\nLink\nSub-CR [145]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2022\nLink\nOCGTL [101]\nUnsupervised\nStatic\nGraph\nGenuine\nAnomaly Measures\n2022\nLink\nMHGL [158]\nUnsupervised\nStatic\nNode\nGenuine\nAnomaly Measures\n2022\nLink\nSDGG [8]\nSupervised\nStatic\nGraph\nGenuine\nGNN Backbone\n2023\nN/A\nGDN [35]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nLink\nGHRN [36]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nLink\nGODM [65]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nLink\nDIFFAD [80]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nN/A\nGADY [76]\nUnsupervised\nDynamic\nNode\nInjected\nGNN Backone\n2023\nLink\nRAND [6]\nUnsupervised\nStatic\nNode\nBoth\nGNN Backbone\n2023\nLink\nSplitGNN [125]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nLink\nSEC-GFD [133]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nN/A\nNSReg [121]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2023\nN/A\nGmapAD [82]\nUnsupervised\nStatic\nGraph\nGenuine\nGNN Backbone\n2023\nLink\nRQGNN [24]\nUnsupervised\nStatic\nGraph\nGenunie\nGNN Backbone\n2023\nLink\nAuGAN [159]\nSupervised\nStatic\nNode\nBoth\nGNN Backbone\n2023\nLink\nHimNet [84]\nUnsupervised\nStatic\nGraph\nGenuine\nProxy Task Design\n2023\nLink\nGGA [83]\nSupervised\nStatic\nNode\nGenuine\nProxy Task Design\n2023\nN/A\nGRADATE [27]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2023\nLink\nGAD-NR [104]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2023\nLink\nCFAD [131]\nSupervised\nStatic\nNode\nGenuine\nProxy Task Design\n2023\nLink\nACT [120]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2023\nLink\nWEDGE [157]\nUnsupervised\nStatic\nNode\nInjected\nProxy Task Design\n2023\nN/A\nDIF [134]\nUnsupervised\nStatic\nNode\nGenuine\nAnomaly Measures\n2023\nLink\nCLAD [53]\nUnsupervised\nStatic\nNode\nInjected\nAnomaly Measures\n2023\nLink\nPREM [85]\nUnsupervised\nStatic\nNode\nInjected\nAnomaly Measures\n2023\nLink\nHRGCN [56]\nUnsupervised\nStatic\nGraph\nGenuine\nAnomaly Measures\n2023\nLink\nTAM [98]\nUnsupervised\nStatic\nNode\nBoth\nAnomaly Measures\n2023\nLink\nGCAD [164]\nUnsupervised\nStatic\nNode\nInjected\nAnomaly Measures\n2023\nLink\nConsisGAD [14]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nLink\nPMP [165]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nLink\nHedGe [147]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nLink\nBioGNN [34]\nSupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nLink\nMITIGATE [12]\nSupervised\nStatic\nNode\nInjected\nGNN Backbone\n2024\nLink\nGGAD [99]\nSemi-Supervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nLink\nSmoothGNN [23]\nUnsupervised\nStatic\nNode\nGenuine\nGNN Backbone\n2024\nN/A\nFGAD [9]\nSupervised\nStatic\nGraph\nGenuine\nProxy Task Design\n2024\nN/A\nSTRIPE [63]\nUnsupervised\nDynamic\nNode\nInjected\nProxy Task design\n2024\nN/A\nDOHSC [148]\nUnsupervised\nStatic\nGraph\nGenuine\nAnomaly Measures\n2024\nLink\nARC [68]\nSupervised\nStatic\nNode\nBoth\nAnomaly Measures\n2024\nN/A\nAUGUST 2024\n23\nTABLE 3: Publicly accessible node-level GAD datasets.\nDataset\n# Nodes\n# Edges\n# Attributes\nSize\nAnomaly\nAnomaly Type\nDomain\nReferences\nCora\n2,708\n5,429\n1,433\nSmall\n5.5%\nInjected\nCitation Networks\n[18], [31], [69], [77]\nCitersee\n3,327\n4,732\n3,703\nSmall\n4.5%\nInjected\nCitation Networks\n[18], [31], [69], [77]\nACM\n16,484\n71,980\n8,337\nMedium\n3.6%\nInjected\nCitation Networks\n[18], [31], [69], [77]\nBlogCatalog\n5,196\n171,743\n8,189\nSmall\n5.8%\nInjected\nSocial Networks\n[18], [31], [69], [77]\nFlickr\n7,575\n239,738\n12,407\nMedium\n5.2%\nInjected\nSocial Networks\n[18], [31], [69], [77]\nOGB-arXiv\n169,343\n1,166,243\n128\nLarge\n3.5%\nInjected\nCitation Networks\n[44], [69]\nAmazon\n11,944\n4,398,392\n25\nLarge\n9.5%\nGenuine\nTransaction Record\n[25], [98], [113], [114]\nYelpChi\n45,954\n3,846,979\n32\nLarge\n14.5%\nGenuine\nReviewer Interaction\n[25], [98], [113], [114]\nT-Finance\n39,357\n21,222,543\n10\nLarge\n4.6%\nGenuine\nTransaction Record\n[36], [113], [114]\nT-Social\n5,781,065\n73,105,508\n10\nLarge\n3.0%\nGenuine\nSocial Network\n[36], [113], [114]\nWeibo\n8,405\n407,963\n400\nSmall\n10.3%\nGenuine\nUnder Same Hashtag\n[113], [114]\nDGraph\n3,700,550\n4,300,999\n17\nLarge\n1.3%\nGenuine\nLoan Guarantor\n[48], [113]\nElliptic\n203,769\n234,355\n166\nLarge\n9.8%\nGenuine\nPayment Flow\n[10], [23], [113], [121]\nTolokers\n11,758\n519,000\n10\nMedium\n21.8%\nGenuine\nWork Collaboration\n[23], [113]\nQuestions\n48,921\n153,540\n301\nMedium\n3.0%\nGenuine\nQuestion Answering\n[23], [113]\nDisney\n124\n335\n28\nSmall\n4.8%\nGenuine\nCo-purchase\n[64], [104]\nBooks\n1,418\n3,695\n21\nSmall\n2.0%\nGenuine\nCo-purchase\n[64], [104]\nEnron\n13,533\n176,987\n18\nMedium\n0.4%\nGenuine\nEmail network\n[64], [104]\nReddit\n10,984\n168,016\n64\nMedium\n3.3%\nGenuine\nUser-subreddit\n[64], [98], [99], [113]\nTABLE 4: Publicly accessible graph-level GAD datasets. Homo. and Heter. indicate the graph is homogeneous and heterogeneous,\nrespectively. Graph-level GAD methods are typically trained using anomaly-free data, so the anomaly rate is applied to the test\ndata only.\nDataset\n# Graphs\n# Avg. Nodes\n# Edges\nAnomaly\nDomain\nHomo./Heter.\nReferences\nKKI\n83\n190\n237.4\n44.6%\nBioinformatics\nHomo.\n[79], [82], [122]\nOHSU\n79\n82.01\n199.66\n44.3%\nBioinformatics\nHomo.\n[79], [82]\nMUTAG\n188\n17.93\n19.79\n33.5%\nMolecules\nHomo.\n[67], [79], [82]\nPROTEINSfull\n1,113\n39.06\n72.82\n40.4%\nBioinformatics\nHomo.\n[67], [79], [82], [122]\nENZYMES\n600\n32.63\n62.14\n16.7%\nBioinformatics\nHomo.\n[79]\nAIDS\n2,000\n15.69\n16.2\n20.0%\nChemical Structure\nHomo.\n[67], [79], [82], [122]\nBZR\n405\n35.75\n38.36\n21.0%\nMolecules\nHomo.\n[67], [79], [122]\nCOX2\n467\n41.22\n43.45\n21.8%\nMolecules\nHomo.\n[67], [79], [122]\nDD\n1,178\n284.32\n715.66\n41.3%\nBioinformatics\nHomo.\n[67], [79], [122]\nNCI1\n4,110\n29.87\n32.3\n49.9%\nMolecules\nHomo.\n[67], [82], [122]\nIMDB\n1,000\n19.77\n96.53\n50.0%\nSocial Networks\nHomo.\n[67], [82]\nREDDIT\n2,000\n429.63\n497.75\n50.0%\nSocial Networks\nHomo.\n[67], [79], [82], [122]\nHSE\n8,417\n16.89\n17.23\n5.2%\nMolecules\nHomo.\n[79], [122]\nMMP\n7,558\n17.62\n17.98\n15.6%\nMolecules\nHomo.\n[79], [122]\np53\n8,903\n17.92\n18.34\n6.3%\nMolecules\nHomo.\n[79], [122]\nPPAR-gamma\n8,451\n17.38\n17.72\n2.8%\nMolecules\nHomo.\n[79], [122]\nCOLLAB\n5,000\n74.49\n2,457.78\n15.5%\nSocial Networks\nHomo.\n[79], [122]\nMutagenicit\n4,337\n30.32\n30.77\n44.6%\nMolecules\nHomo.\n[82]\nDHFR\n756\n42.43\n44.54\n39.0%\nMolecules\nHomo.\n[67], [79], [122]\nTraceLog\n132,485\n205\n224\n17.6%\nLog Sequences\nHeter.\n[56]\nFlowGraph\n600\n8,411\n12,730\n16.7%\nSystem Flow\nHeter.\n[56]\nAUGUST 2024\n24\nTABLE 5: Quantitative comparison of node-level anomaly detection on datasets with manually injected (synthetic) anomalies.\nMetric\nMethod\nDataset\nAUROC\nCora\nCiteseer\nACM\nBlogCatalog\nFlicker\nPubmed\nFacebook\nReddit\nWeibo\nDOMINANT [18]\n0.815\n0.825\n0.760\n0.746\n0.744\n0.808\n0.554\n0.560\n0.850\nCoLA [69]\n0.878\n0.896\n0.823\n0.785\n0.751\n0.951\n/\n0.603\n/\nSL-GAD [155]\n0.913\n0.913\n0.853\n0.818\n0.796\n0.967\n/\n0.567\n/\nCONAD [135]\n0.788\n/\n/\n/\n/\n/\n0.863\n0.561\n0.854\nAEGIS [17]\n/\n/\n/\n0.743\n0.738\n0.773\n/\n/\n/\nOCGNN [122]\n0.881\n0.856\n/\n/\n/\n0.747\n0.793\n/\n/\nComGA [77]\n0.884\n0.916\n0.849\n0.814\n0.799\n0.922\n0.659\n/\n/\nAAGNN [160]\n/\n/\n/\n0.818\n0.829\n0.856\n/\n/\n0.925\nHCM-A [47]\n/\n/\n0.761\n0.798\n0.792\n/\n/\n/\n/\nGAAN [15]\n0.742\n/\n0.877\n0.765\n0.753\n/\n/\n0.554\n0.925\nAnomalyDAE [31]\n0.762\n0.727\n0.778\n0.783\n0.751\n0.810\n/\n0.557\n0.915\nGAD-NR [104]\n0.835\n/\n/\n/\n/\n/\n/\n/\n0.623\nTAM [98]\n/\n/\n0.887\n0.824\n/\n/\n0.914\n0.602\n/\nAURPC\nDOMINANT [18]\n0.200\n/\n/\n0.338\n0.324\n0.299\n/\n0.037\n/\nCoLA [69]\n/\n/\n0.323\n0.327\n/\n/\n0.211\n0.044\n/\nSL-GAD [155]\n/\n/\n/\n0.388\n0.378\n/\n0.131\n0.041\n/\nCONAD [135]\n/\n/\n/\n/\n/\n/\n/\n0.037\n/\nAEGIS [17]\n/\n/\n/\n0.339\n0.324\n0.373\n/\n/\n/\nOCGNN [122]\n/\n/\n/\n/\n/\n/\n/\n/\n/\nComGA [77]\n/\n/\n/\n/\n/\n/\n/\n/\n/\nAAGNN [160]\n/\n/\n/\n0.435\n0.421\n0.428\n/\n/\n/\nHCM-A [47]\n/\n/\n/\n/\n/\n/\n/\n/\n/\nGAAN [15]\n/\n/\n/\n0.338\n0.324\n0.337\n/\n0.037\n/\nAnomalyDAE [31]\n0.183\n/\n/\n/\n/\n/\n/\n/\n/\nGAD-NR [104]\n/\n/\n/\n/\n/\n/\n/\n/\n/\nTAM [98]\n/\n/\n0.512\n0.418\n/\n/\n0.223\n0.044\n/\nTABLE 6: Quantitative comparison of node-level anomaly detection on datasets with genuine anomalies. Results of DevNet and\nPReNet are taken from [121].\nMetric\nSetting\nMethod\nDataset\nAmazon YelpChi T-Finance Question Elliptic Reddit Tolokers Weibo DGraph T-Social Photo\nCS\nAUROC\nUnsupervised\nDOMINANT [18]\n0.694\n0.539\n0.538\n/\n0.296\n0.556\n/\n/\n0.574\n/\n0.514 0.402\nCoLA [69]\n0.261\n0.480\n0.483\n/\n/\n0.603\n/\n/\n/\n/\n/\n0.481\nCLAD [53]\n0.203\n0.476\n0.139\n0.621\n0.419\n0.578\n0.406\n/\n/\n/\n/\n/\nGRADATE [27]\n0.478\n0.492\n0.406\n0.554\n/\n0.526\n0.537\n/\n/\n/\n/\n/\nGAD-NR [104]\n0.260\n0.470\n0.579\n0.587\n0.400\n0.553\n0.576\n/\n/\n/\n/\n/\nPrem [85]\n0.278\n0.490\n0.448\n0.603\n0.497\n0.551\n0.565\n/\n/\n/\n/\n/\nTAM [98]\n0.802\n0.548\n0.690\n0.504\n/\n0.572\n0.469\n/\n/\n/\n/\n/\nSmoothGNN [23]\n0.840\n0.575\n0.755\n0.644\n0.572\n0.594\n0.687\n/\n0.649\n0.703\n/\n/\nSemi-supervised\nGGAD [99]\n0.944\n/\n0.823\n/\n0.729\n/\n/\n/\n0.594\n/\n0.648\n/\nSupervised\nBWGNN [114]\n0.980\n0.849\n0.961\n0.718\n0.852\n0.654\n0.804\n0.973\n0.763\n0.920\n/\n/\nDCI [123]\n0.946\n0.778\n0.868\n0.692\n0.828\n0.665\n0.755\n0.942\n0.747\n0.808\n/\n/\nAMNet [10]\n0.970\n0.826\n0.937\n0.681\n0.773\n0.684\n0.768\n0.953\n0.731\n0.536\n/\n/\nGHRN [35]\n0.981\n0.853\n0.96\n0.718\n0.854\n0.660\n0.804\n0.967\n0.761\n0.790\n/\n/\nNGS [100]\n0.973\n0.921\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\nPCGNN [66]\n0.973\n0.797\n0.933\n0.699\n0.858\n0.532\n0.728\n0.902\n0.720\n0.692\n/\n/\nGDN [36]\n0.971\n0.903\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\nDevNet [91]\n/\n/\n0.654\n/\n/\n/\n/\n/\n/\n/\n0.599 0.606\nPReNet [90]\n/\n/\n0.892\n/\n/\n/\n/\n/\n/\n/\n0.698 0.632\nNSReg [121]\n/\n/\n0.929\n/\n/\n/\n/\n/\n/\n/\n0.908 0.797\nAUPRC\nUnsupervised\nDOMINANT [18]\n0.102\n0.165\n0.047\n/\n/\n0.036\n/\n0.008\n/\n0.104 0.187\nCoLA [69]\n0.052\n0.136\n0.041\n/\n/\n0.045\n/\n/\n/\n/\n0.246 0.253\nCLAD [53]\n0.040\n0.128\n0.025\n0.051\n0.081\n0.050\n0.192\n/\n/\n/\n/\n/\nGRADATE [27]\n0.063\n0.145\n0.038\n0.035\n/\n0.039\n0.236\n/\n/\n/\n/\n/\nGADNR [104]\n0.042\n0.139\n0.054\n0.057\n0.077\n0.037\n0.299\n/\n/\n/\n/\n/\nPrem [85]\n0.074\n0.137\n0.039\n0.043\n0.090\n0.041\n0.259\n/\n/\n/\n/\n/\nTAM [98]\n0.332\n0.173\n0.128\n0.039\n/\n0.042\n0.196\n/\n/\n/\n/\n/\nSmoothGNN [23]\n0.395\n0.182\n0.140\n0.059\n0.116\n0.043\n0.351\n/\n0.019\n0.063\n/\n/\nSemi-supervised\nGGAD [99]\n0.792\n/\n0.183\n/\n0.243\n0.061\n/\n/\n0.008\n/\n0.144\n/\nSupervised\nBWGNN [114]\n0.891\n0.551\n0.866\n0.167\n0.260\n0.069\n0.497\n0.930\n0.040\n0.549\n/\n/\nDCI [123]\n0.815\n0.395\n0.626\n0.141\n0.254\n0.061\n0.399\n0.896\n0.036\n0.138\n/\n/\nAMNet [10]\n0.873\n0.488\n0.743\n0.146\n0.147\n0.073\n0.432\n0.897\n0.028\n0.031\n/\n/\nGHRN [36]\n0.895\n0.566\n0.866\n0.167\n0.277\n0.072\n0.499\n0.918\n0.04\n0.163\n/\n/\nNGS [100]\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\nPCGNN [66]\n0.878\n0.437\n0.698\n0.144\n0.356\n0.042\n0.381\n0.819\n0.028\n0.087\n/\n/\nDevNet [91]\n/\n/\n0.323\n/\n/\n/\n/\n/\n/\n/\n0.468 0.537\nPReNet [90]\n/\n/\n0.571\n/\n/\n/\n/\n/\n/\n/\n0.460 0.557\nNSReg [121]\n/\n/\n0.757\n/\n/\n/\n/\n/\n/\n/\n0.836 0.752\nAUGUST 2024\n25\nTABLE 7: Quantitative comparison of graph-level anomaly detection.\nMetric\nMethod\nDataset\nPROTEINS-F ENZYMES AIDS DHFR BZR COX2\nDD\nNCI1 IMDB COLLAB HSE MMP\nP53\nTraceLog FlowGraph\nAUROC\nGlocalKD [79]\n0.773\n0.613\n0.932\n0.567 0.694 0.593 0.801 0.684 0.521\n0.674\n0.593 0.675 0.640\n/\n/\nOCGIN [149]\n0.708\n0.587\n0.781\n0.492 0.659 0.535 0.722 0.719 0.601\n/\n/\n/\n/\n/\n/\nSIGNET [67]\n0.752\n0.629\n0.972\n0.740 0.814 0.714 0.727 0.748 0.664\n/\n/\n/\n/\n/\n/\nOCGTL [101]\n0.765\n0.620\n0.994\n0.599 0.639 0.552 0.794 0.734 0.640\n/\n/\n/\n/\n/\n/\nOCGCN [122]\n0.718\n0.613\n0.664\n0.495 0.658 0.628 0.605 0.627 0.536\n/\n0.388 0.457 0.483\n/\n/\nHimNet [84]\n0.772\n0.589\n0.997\n0.701 0.703 0.637 0.806 0.686 0.553\n0.683\n0.613 0.703 0.646\n/\n/\nGLADST [59]\n/\n0.694\n0.976\n0.773 0.810 0.630\n/\n0.681\n/\n0.776\n0.547 0.685 0.688\n/\n/\nDIF [134]\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n0.737 0.715 0.680\n/\n/\nHRGCN [56]\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n/\n0.864\n1.000\n",
    "ref": [
        "2310.06261",
        "2402.12761",
        "2401.13210",
        "2405.17525",
        "2310.02861",
        "2103.09430",
        "2210.12941",
        "2308.10918",
        "1609.02907",
        "2310.11829",
        "2403.09039",
        "2312.17679",
        "2405.16771",
        "2308.13821",
        "2308.14181",
        "2212.05478",
        "2305.02496",
        "2310.16376",
        "2310.11676",
        "2402.11887",
        "2205.13845",
        "2402.16024",
        "2306.12251",
        "2305.13573",
        "1710.10903",
        "1809.10341",
        "2311.06835",
        "2403.01121",
        "2311.10370",
        "2312.06441",
        "1710.09412",
        "2205.04816",
        "2403.10339",
        "2302.06430",
        "2202.07082"
    ]
}