{
    "id": "2408.02464",
    "title": "Fairness and Bias Mitigation in Computer Vision: A Survey",
    "abstract": "Computer vision systems have witnessed rapid progress over the past two\ndecades due to multiple advances in the field. As these systems are\nincreasingly being deployed in high-stakes real-world applications, there is a\ndire need to ensure that they do not propagate or amplify any discriminatory\ntendencies in historical or human-curated data or inadvertently learn biases\nfrom spurious correlations. This paper presents a comprehensive survey on\nfairness that summarizes and sheds light on ongoing trends and successes in the\ncontext of computer vision. The topics we discuss include 1) The origin and\ntechnical definitions of fairness drawn from the wider fair machine learning\nliterature and adjacent disciplines. 2) Work that sought to discover and\nanalyze biases in computer vision systems. 3) A summary of methods proposed to\nmitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze,\nand mitigate bias and enhance fairness. 5) Discussion of the field's success,\ncontinuing trends in the context of multimodal foundation and generative\nmodels, and gaps that still need to be addressed. The presented\ncharacterization should help researchers understand the importance of\nidentifying and mitigating bias in computer vision and the state of the field\nand identify potential directions for future research.",
    "date": "2024-08-05T13:44:22+00:00",
    "fulltext": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n1\nFairness and Bias Mitigation in Computer Vision:\nA Survey\nSepehr Dehdashtian\u2217, Ruozhen He\u2217, Yi Li, Guha Balakrishnan, Nuno Vasconcelos, Fellow, IEEE,\nVicente Ordonez, Member, IEEE, and Vishnu Naresh Boddeti Member, IEEE\nAbstract\u2014Computer vision systems have witnessed rapid progress over the past two decades due to multiple advances in the field.\nAs these systems are increasingly being deployed in high-stakes real-world applications, there is a dire need to ensure that they do not\npropagate or amplify any discriminatory tendencies in historical or human-curated data or inadvertently learn biases from spurious\ncorrelations. This paper presents a comprehensive survey on fairness that summarizes and sheds light on ongoing trends and\nsuccesses in the context of computer vision. The topics we discuss include 1) The origin and technical definitions of fairness drawn\nfrom the wider fair machine learning literature and adjacent disciplines. 2) Work that sought to discover and analyze biases in computer\nvision systems. 3) A summary of methods proposed to mitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze, and mitigate bias and enhance fairness. 5)\nDiscussion of the field\u2019s success, continuing trends in the context of multimodal foundation and generative models, and gaps that still\nneed to be addressed. The presented characterization should help researchers understand the importance of identifying and mitigating\nbias in computer vision and the state of the field and identify potential directions for future research.\nIndex Terms\u2014Computer Vision, Fairness, Bias Mitigation, Visual Recognition, Visual Representation Learning, Survey.\n\u2726\n1\nINTRODUCTION\nT\nHE field of computer vision has gone through several\nmajor advances throughout the years. The introduction\nof machine learning and statistical methods created a wave\nof interest and progress in visual recognition, e.g. [1, 2, 3],\nwhich eventually motivated much of the recent advances in\ndeep learning methods using neural networks [4, 5, 6] and\nlarge-scale datasets [7, 8]. The rapid progress in the recogni-\ntion problem also inspired a search for the right methods\nand models for a diverse array of other problems, such\nas U-Nets [9] for image segmentation or Latent Diffusion\nModels [10] for image synthesis.\nMachine learning and statistical methods, however, rely\non training datasets and loss functions that can induce,\npropagate, or magnify statistical biases. Such biases are\nundesirable when correlated to sensitive protected attributes\nsuch as demographic variables related to people, e.g. race,\ngender, age, or ethnicity. Models that learn the inherent\ncorrelations or rely on spurious correlations with these\nattributes can produce disparate outcomes, thereby leading\nto ethical or legal concerns [11, 12]. The goal of fairness and\nbias mitigation [13, 14] is to prevent or minimize the impact\nof such biases on model decisions.\n\u2022\nS. Dehdashtian and V. N. Boddeti are with the Department of Computer\nScience and Engineering at Michigan State University, East Lansing,\nMichigan. R. He and V. Ordonez are with the Department of Computer\nScience at Rice University, Houston, Texas. G. Balakrishnan is with the\nDepartment of Electrical and Computer Engineering at Rice University,\nHouston, Texas. Y. Li and N. Vasconcelos are with the Department of\nElectrical and Computer Engineering at the University of California, San\nDiego, California.\nE-mail: sepehr, visnhu@msu.edu, vicenteor, catherine.he, guha@rice.edu,\nyil898, nuno@ucsd.edu\n\u2217. Equal contribution\nTo make computer vision systems widely adopted, ac-\ncepted, and trusted, it is necessary to avoid societal in-\nequalities and enhance their reliability. This has motivated\ninterest in issues of fairness and biases, intending to de-\nvelop responsible visual recognition and related systems\ncapable of serving society equitably. From early studies\nrevealing biases in image captioning [15] or face recogni-\ntion [16] to recent efforts in mitigating biases in various\ntasks [14, 17, 18, 19], there has been a significant body of\nwork in studying fairness and proposing bias mitigation\nmethods for computer vision. In this paper, we survey this\nliterature and related problems solved by machine learning\nsystems trained with large-scale datasets for applications\nwhere societal biases are relevant.\nThe survey first introduces the notation, origins, and def-\ninitions of fairness while summarizing the commonalities\nwith fairness studies in the broader machine-learning litera-\nture. Then, we briefly discuss prior work on discovering and\nanalyzing bias in computer vision datasets and models. We\nthen present a synthesis of the proposed methodologies and\ndatasets used to study bias and its mitigation. Finally, we\ndiscuss current trends in discovering and mitigating bias in\nmultimodal foundation models and open problems in this\nfield. The survey aims to serve as a quick reference and start-\ning point for new research on adapting or designing novel\nmethods to maximize the fairness of emerging computer\nvision models in a rapidly evolving space.\nWhat makes the study of fairness in computer vision\nmodels distinct from those in other domains, such as tabular\ndata and graphs? The general framework of fairness con-\nsists of quantifying the disparate outcomes from a model\nfor groups belonging to different categories of a sensitive\nprotected attribute and proposing methods to alleviate or\nmitigate these disparities. For instance, COMPAS [20], a\narXiv:2408.02464v1  [cs.CV]  5 Aug 2024\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n2\nDiscriminative Modeling\nlighter faces\n0.7% error\ndarker faces\n12.9% error\nVQA\nQ: What is the girl\ndoing? A: Cleaning\nQ: What is the boy\ndoing? A: Rinsing\nIranian\nMexican\nIndian\nGenerative Modeling\nDoctor\nCashier\nProfessor\nFig. 1: Examples of Bias and Unfairness in discriminative and generative computer vision systems. Left: Bias in discrimina-\ntive modeling shown through face recognition [16] and Situation Recognition [21, 27] examples. Right: Stereotypical bias\nin generative modeling with examples from three cultures and three professions [28].\ncommonly used tabular dataset to analyze fairness in ma-\nchine learning, uses race as a sensitive protected attribute,\nwhich is included as a categorical variable. In contrast,\ncomputer vision datasets usually lack explicit categorical\nlabels for sensitive attributes. Instead, these attributes are\nimplicitly encoded in the combination of input image pixels\nand task-specific target attributes that are to be inferred by\na model. For instance, in the absence of bias mitigation, a\ncomputer vision model trained to predict human activities\nfrom images, e.g. cooking vs. not-cooking, will likely predict\nthe activities at disparate rates for images depicting people\nof different gender [21]. The challenge is to disentangle the\neffect of people\u2019s appearance, which is typically correlated\nwith gender, and the activities being performed. Since this is\na difficult goal, bias mitigation in computer vision presents\nunique challenges that are not present in tabular datasets.\nThis justifies a comprehensive survey of computer vision\nmethods, with a brief review of the more general literature\non fairness. For a comprehensive survey on fairness in\nmachine learning, we refer the reader to Mehrabi et al.\n[22], Pessach and Shmueli [23], Le Quy et al. [24], Caton\nand Haas [25]. Perhaps more related and complementary\nto ours is the recent survey by Parraga et al. [26], which\nfocuses on vision-and-language models. In contrast, our\nsurvey provides a more comprehensive summary of the fair-\nness literature related to more traditional computer vision\ntasks such as image classification, object detection, activity\nrecognition, and face recognition and analysis.\nAnother challenge in computer vision is the lack of\naccess to explicit labels for sensitive protected attributes. Com-\nmonly, information for demographic variables such as gen-\nder, race, or ethnicity is not annotated or provided explic-\nitly by the same individuals depicted in computer vision\ndatasets. Therefore, most annotations on these datasets can\nonly be considered as proxies for the real values based\non the perceived judgments of data annotators. Moreover,\nScheuerman and Brubaker [29] argue that tech workers\nand scientists have also had a significant role in defining\ncategories related to identity for people in computer vision\ndatasets. As a result, demographic markers such as gender\nhave only been studied as binary variables for previous\nworks, and race is often studied as a set of discrete cate-\ngories. Several works summarized in this survey acknowl-\nedge some of these issues, but the overall field should be\njudged in this context.\nBeyond these issues, navigating the challenges of fair-\nness and bias mitigation in computer vision is still a com-\nplex endeavor due to the nature of biases, the diversity of\ndatasets and tasks, and trade-offs between model perfor-\nmance and fairness. This survey explores core computer\nvision tasks and identifies primary challenges associated\nwith achieving fairness and mitigating biases for each task.\nFigure 1 illustrates the type of demographic bias and unfair-\nness prevalent in computer vision systems. Tables 1 and 2\nextensively summarize task-specific debiasing methods de-\nveloped in the computer vision literature and the associated\ndatasets employed for studying bias and fairness, respec-\ntively. A detailed overview of common methods for bias\nmitigation and a comprehensive discussion of the datasets\ncategorized by bias attributes and tasks can be found in\nSection 4 and Section 5, respectively.\n2\nORIGINS AND DEFINITIONS OF UNFAIRNESS\nWe start with a note on terminology. The term bias has\nbeen overloaded in the context of the study of fairness.\nA statistical bias simply refers to the degree to which a\ncertain methodology provides a skewed representation of a\ntrue phenomenon. For instance, opinion surveys conducted\nonly through the workplace overlook unemployed people\nand are thus not representative of the sentiment of the\ngeneral population. In computer vision, biases can manifest\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n3\nTABLE 1: Methods: Bias analysis and mitigation techniques by task and protected attributes. While task-specific bias\nmitigation methods have been proposed bias mitigation for generic visual representation learning received a lot of attention.\nTask\nAttribute\nReferences\nRepresentation Learning\nGender\nPark et al. [30], Li et al. [31], Dehdashtian et al. [19], Dehdashtian et al. [14], Sadeghi\net al. [32], Qraitem et al. [33], Jang and Wang [34], Wang and Russakovsky [35],\nZhang et al. [36], Tang et al. [37], Meister et al. [38], Ranjit et al. [39], Jeon et al.\n[40], Park et al. [41], Wang et al. [42], Seo et al. [43], Chai and Wang [44], Zhu\net al. [45], Tartaglione et al. [46], Ramaswamy et al. [47], Kim et al. [48], Wang and\nRussakovsky [49], Wang et al. [50], Wang et al. [51], Seth et al. [52], Hall et al. [53],\nChuang et al. [54], Van Miltenburg [55]\nColor\nPark et al. [30], Jang and Wang [34], Zhang et al. [36], Park et al. [41], Seo et al. [43],\nZhu et al. [45], Wang et al. [56], Jung et al. [57], Tartaglione et al. [46], Kim et al.\n[48], Wang et al. [50], Li and Vasconcelos [58]\nCorruption\nPark et al. [30], Zhang et al. [36]\nAge\nLi et al. [31], Dehdashtian et al. [14], Sadeghi et al. [32], Qraitem et al. [33], Park\net al. [41], Zhu et al. [45], Tartaglione et al. [46], Seth et al. [52], Chuang et al. [54]\nRace\nQraitem et al. [33], Dehdashtian et al. [19], Dehdashtian et al. [14], Sadeghi et al.\n[32], Wang and Russakovsky [35], Park et al. [41], Chai and Wang [44], Zhu et al.\n[45], Seth et al. [52], Chuang et al. [54], Van Miltenburg [55]\nGeography\nWang and Russakovsky [35], Shankar et al. [59], Wang et al. [60]\nContext\nZhang et al. [36], Wang et al. [56], Chuang et al. [54], Wang et al. [60]\nScene\nMo et al. [61]\nSkin Tone\nSchumann et al. [62]\nTexture\nWang et al. [56], Kim et al. [48]\nAction\nLi and Vasconcelos [58]\nAnalysis\nSocial\nSirotkin et al. [63], Birhane et al. [64], Brinkmann et al. [65]\nGender\nMeister et al. [38], Birhane et al. [64], Iofinova et al. [66], Guilbeault et al. [67]\nClassification\nGender\nKim et al. [68], Zietlow et al. [69], Bendekgey and Sudderth [70], Lee et al. [71], Jung\net al. [72], Zhang et al. [73], Roy and Boddeti [74], Sadeghi et al. [75], Dehdashtian\net al. [19], Sadeghi et al. [32], Dehdashtian et al. [14], Gustafson et al. [76]\nAge\nKim et al. [68], Sadeghi et al. [75], Dehdashtian et al. [19], Sadeghi et al. [32],\nDehdashtian et al. [14], Gustafson et al. [76]\nRace\nLee et al. [71], Jung et al. [72], Dehdashtian et al. [19]\nIllumination\nRoy and Boddeti [74], Sadeghi et al. [75]\nHair Color\nDehdashtian et al. [14]\nSkin Tone\nGustafson et al. [76]\nOther\nSingh et al. [77], Kim et al. [68], Chiu et al. [78], Jia et al. [79], Li and Xu [80]\nAction Recognition\nScene\nChoi et al. [81], Zhai et al. [82], Li et al. [17]\nContextual\nChoi et al. [81]\nFace Recognition\nGender\nBuolamwini and Gebru [16], Vera-Rodriguez et al. [83] Quadrianto et al. [84],\nDomnich and Anbarjafari [85], Dhar et al. [86], Gong et al. [87], Ma et al. [88],\nLiang et al. [89], Dooley et al. [90], Chen and Joo [91], Chouldechova et al. [92],\nTerh\u00a8orst et al. [93], Shankar et al. [59], Zietlow et al. [69], Georgopoulos et al. [94],\nLi and Abd-Almageed [95], Gong et al. [96]\nRace\nBuolamwini and Gebru [16], Wang and Deng [97], Gong et al. [87], Ma et al. [88],\nLiang et al. [89], Dooley et al. [90], Chouldechova et al. [92], Terh\u00a8orst et al. [93],\nShankar et al. [59], Georgopoulos et al. [94], Gong et al. [96]\nData Imbalance\nLiu et al. [98], [93]\nSkin Tone\nBalakrishnan et al. [99], Dhar et al. [86], Terh\u00a8orst et al. [93], Georgopoulos et al. [94]\nAge, Hair & Facial Hair\nBalakrishnan et al. [99], Terh\u00a8orst et al. [93], Shankar et al. [59], Georgopoulos et al.\n[94], Gong et al. [96]\nOther\nTerh\u00a8orst et al. [93]\nGenerative Models\nRace\nMaluleke et al. [100], Tan et al. [101], Wu et al. [102]\nData Imbalance\nYu et al. [103], Zhao et al. [104]\nGender\nXu et al. [105], Tan et al. [101], Karakas et al. [106], Choi et al. [107], Wu et al. [102]\nAge\nTan et al. [101], Karakas et al. [106]\nOther\nJalal et al. [108], Wu et al. [102], Choi et al. [107], Kenfack et al. [109], Karakas et al.\n[106], Tan et al. [101]\nObject Detection\nIncome\nSudhakar et al. [110]\nSkin Tone\nWilson et al. [111]\nOther\n-\nKong et al. [112], Yenamandra et al. [113], Qiu et al. [114], Shankar et al. [59], Chu\net al. [115], Garcia et al. [116], Biswas and Ji [117], Tang et al. [118]\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n4\nTABLE 2: Datasets: Tasks, datasets, and sensitive protected attributes studied for bias quantification and mitigation. Next\nto each dataset, we reference either the original paper or the paper that adapted it specifically for bias analysis.\nTask\nProtected Attribute\nDatasets\nBasic Image Bias\nAnalysis\nSocial ContextS, GenderG,\nAgeA, BackgroundB\nSocial Context [119][63]S, MSCOCO [8][21][38]G, OpenImages [120] [38]GA,\nCelebA [121][66]G, IAT [122][67]G, Waterbird [123][113]B, NICO++ [124][113]B\nRepresentation\nLearning\nGeographyP ,\nGenderG,\nColorL,\nBackgroundB,\nAgeA,\nEthnicityE,\nContextC,\nTextureX,\nOtherO\nImageNet [7] [59]P , Open Images [120] [59]P , MSCOCO [8] [21] [51]G, CIFAR-\n10S [125] [50]L, Corrupted CIFAR10 [125] [126] [48]O, BAR [127] [48]B,\nbFFHQ [128] [48]G, IMDB Face [129] [68] [46]GA, CelebA [121] [127] [46]G,\nUTKFace [130] [84]E, NICO [131] [56]C, ImageNet-A [56, 132]LX, mPower [68]\n[45]A, Adult [133]\n[45] GRO, LFW [134]\n[42]G, DollarStreet [135]\n[35]P ,\nGeoDE [136] [35]P , MST [62]T , PATA [52]C, VisoGender [53] G\nImage\nClassification\nColorL, AgeA, GenderG,\nContextC,\nEthnicityE,\nBackgroundB,\nSocial\nContextS, OtherO\nColored MNIST [137] [68]L, Dogs and Cats [138] [68]L, IMDB Face [139] [68]AG,\nMSCOCO-Stuff [140] [77]C, UnRel [141] [77]C, Deep Fashion [142] [77]C,\nAwA [143] [77]C, CelebA [121] [80] GEAO, Faces of the World [144] [70]G, UTK-\nFace [130][72]EG, COMPAS [145] [72]EG, CIFAR-10-B [125][78][78]B, CIFAR-\n100-B [125][78][78]B, Extended Yale B [146] [74]GL, Waterbird [123] [19]B,\nCFD [147] [19]G, FairFace [148] [19]S\nAction Recognition\nBackgroundB\nUCF-101 [149] [81]B, HMDB-51 [150] [81]B, Diving48 [151] [81]B, THUMOS-\n14 [152] [81]B, JHMDB [153] [81]B, MiTv2 [154] [82]B, SCUBA [17]B,\nSCUFO [17]B\nFace Recognition\nand Analysis\nGenderG,\nRaceR,\nAgeA,\nSkin ToneT , OtherO\nPPB [16][99]GR, IJB-A [155][16]GR, Adience [156][16]GR, DiF [157][84]G,\nAdult\n[133][45]G,\nLFW[134][98]O,\nYTF[158][98]O,\nMegaFace[159][98]O,\nTransect[99]GT AO,\nIJB-C[160]GT ,\nRFW\n[161]GR,\nMFR\n[162][88]R,\nIJB-B\n[163][88]G,\nDigiFace-1M\n[164][88]O,\nVGGFace2\n[165][90]GR,\nKDEF [166][91]G, CFD [147][19]GR, ExpW [167][91]G, RAF-DB [168][91]G,\nAffectNet [169][91]G, CausalFaces [89]GR, MORPH [170][94]GRA, MAAD-\nFace (47 attributes) [171][93], FairFace [148]GRA, CelebA [69, 121][84]G,\nCACD\n[172][94]GA,\nKANFace\n[173][94]GA,\nUTKFace\n[130][72]GAT ,\nMS1MV2 [174, 175][176]O, MS-Celeb-1M [96, 174]GAR, CFP-FP[177][176]\nImage Retrieval\nGenderG\nOccupation 1 [178][112]G, Occupation 2 [179][112]G, MSCOCO [8][112]G,\nFlickr30k [180][112]G\nObject Detection\nSkin ToneT , IncomeI\nBDD100K [181][111]T I\nPerson\nReidentification\nClothingH\nPRCC-ReID [182][18]H, LTCC-ReID [183][18]H\nImage Captioning\nGenderG, RaceR\nMSCOCO-Bias [15]G, MSCOCO-Balanced [15]G, MSCOCO [8][184]GR\nImage Question\nAnswering\nLanguageN,\nContextC,\nGenderG, RaceR\nVQA\n[185][186]N,\nMSCOCO\n[8][186]N,\nVQA-CP-v2\n[187][188]N,\nVQA-\nv2\n[189][188]N C,\nVQA-Gender\n[190][190]G,\nVQA-introspect\n[191]C,\nIV-\nVQA [192]C, CV-VQA [192]C, VQA-CP [187]L, GQA-OOD [193]L, VQA-\nCE [194]L, Visual7W [195]RG, OK-VQA [196]G\nScene Graph\nGeneration\nCompositionM\nVG [197][198]M, MSCOCO [8][118]M\nText-to-Image\nSynthesis\nGenderG, Skin ToneT\nCelebA [121][199]G, FAIR [200][199]T , FairFace [148][100]GR\nin multiple ways. For example, Torralba and Efros [201]\nstudied bias in early visual recognition models, showing\nthat training on a particular dataset did not generalize well\nto others. Classifiers trained on one dataset were skewed\nto produce satisfactory results only for images resembling\nthose in that dataset. In this case, the bias is w.r.t. the dataset\nvariable. Ideally, a classifier trained on a combination of\nseveral datasets should perform well across test splits for\nall datasets. In the context of action recognition, Li et al.\n[151] showed that datasets frequently have clues, such as\nobjects, backgrounds, etc., that enable good recognition per-\nformance by video representations that only account for a\nsingle or a few video frames. In this case, the bias is w.r.t. the\nrepresentation variable. Various datasets [151, 202] have since\nbeen introduced to combat this problem by requiring the\nclassification of fine-grained actions, distinguishable only\nby long-term motion patterns. We will refer to methods\nproposed to correct biases as mitigation techniques, which\nare also often referred to as debiasing techniques.\nReferences to fairness and mitigating biases in machine\nlearning models are often used interchangeably when bias\nmitigation targets a sensitive protected attribute. Typical ex-\namples of this type of attribute in computer vision in-\nclude sensitive demographic variables such as the gender,\nrace/ethnicity, age, and skin tone of people depicted in\nimages. For instance, the work of Buolamwini and Gebru\n[16] showed disparities in the success rate of a gender\nclassifier depending on the skin tone of the depicted individ-\nuals. However, depending on the context, other variables,\nsuch as geographical location, could be considered sensitive\nprotected attributes. For instance, Shankar et al. [59] uses\ngeo-location as a protected attribute to study disparities in\nthe performance of visual recognition models for images\nobtained from different parts of the world. Our survey\naims to cover bias analysis, and mitigation works that deal\nwith sensitive protected attributes to improve the fairness\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n5\nX\nf(X)\nS\nY\n\u02c6Y\nX\nf(X)\nS\nY\n\u02c6Y\n(a)\n(b)\nFig. 2: Dependence graphs [19] illustrating how biases from\n(a) inherent relations and (b) spurious correlations arise.\nof computer vision model predictions. However, we also\nconsider work that uses synthetic or simulated protected\nattributes introduced to study fairness. For instance, Wang\net al. [50] proposes a variant of the CIFAR-10 dataset where\na percentage of images were converted to grayscale and uses\ncoloring as a protected attribute. Similarly, we also consider\nwork that proposes bias mitigation techniques where the\nprotected attributes are contextual cues from language or\nimage backgrounds that should be irrelevant to the intended\ntask. For instance, Choi et al. [81] uses the background\nscene of a video as a protected attribute for the action\nrecognition task. A well-behaved action recognition model\nshould detect an action regardless of the background scene\nin which it takes place. We also acknowledge that there is\nsignificant work outside the scope of this survey that might\nshare a similar methodology but whose main goal is to\nimprove privacy, transparency, or accountability.\nThe rest of this section discusses two important aspects\nof fairness. In Section 2.1, we discuss factors that contribute\nto biases in current computer vision models. In Section 2.2,\nwe discuss criteria used in the literature to define fairness\nacross sensitive protected attributes.\n2.1\nBias Origins\nIt is well documented that multiple machine learning and\ncomputer vision models have exhibited biases w.r.t. sensitive\nprotected attributes in various contexts and applications e.g.,\ngender or skin tone [14, 64, 203, 204, 205] and even non-\ndemographic attributes e.g., image background or illumina-\ntion [206, 207]. These biases are a manifestation of both social\nand machine learning biases, with the former largely arising\nfrom the training data on which the models are trained.\nFrom a social perspective, the world is frequently biased;\nfor example, expensive cars are more common in affluent\nthan poor neighborhoods. These biases can be amplified\nby the publication of data on the internet, where most\nof the large public datasets are collected, e.g., expensive\ncar manufacturers or owners tend to post images of the\ncars against landmarks or beautiful scenery. As a result,\nbiases found in trained models are largely inherited from\nthe data used to train them, which studies have shown\nto exhibit similar biases [21, 51, 208, 209]. Datasets can\nalso amplify biases due to data collection practices, e.g.,\ndata collection predominantly in some countries or con-\ntinents [210, 211, 212]. Ultimately, the biases in the data\nare either inherited from human biases as reflected on the\nInternet or the methodology used to collect and annotate it.\nFrom a machine learning perspective, biases can be under-\nstood based on dependencies between data attributes, as\nillustrated in Fig. 2. The data X (e.g., face images) depends\nboth on the target attribute Y (e.g., identity) and a sensitive\nattribute S (e.g., skin tone) that induces bias. The goal of bias\nmitigation is to ensure that the prediction \u02c6Y is statistically\nindependent of S. The biases can be grouped into those\narising from two scenarios: (1) Y and S are inherently\ndependent (fig. 2a). We refer to this type of correlation as\nintrinsic dependence. (2) Y and S are independent (fig. 2 b). In\nthis case, we refer to any observed correlation as a spurious\ncorrelation. In the latter case, we expect a bias-free model\u2019s\nperformance w.r.t. Y to be independent of S. However,\nthe same is not so in the former case, where there will\nnecessarily be a trade-off between performance and fairness.\nBeyond the biases in the dataset, model design choices\nsuch as the objective function optimized during training,\nthe sampling process used during training [58], neural net-\nwork architecture, etc., also have an influence on biases in\nthe model predictions. These choices can either amplify or\nmitigate the biases in the training data. This is evidenced in\nthe fact that models trained from the same biased training\ndata can be made more or less biased by bias mitigation\nstrategies [15, 21, 50, 51].\nTo understand the impacts of biases, it helps to separate\ndemographic biases from non-demographic biases. Demographic\nbiases occur when models behave differently for different\ndemographic groups. These groups can be defined in many\nways and are usually specified by a protected attribute,\nsuch as gender, race, or age, among several others. Ideally,\nwe expect the task performance of a bias-free model to\nbe independent of such attributes. This reflects the goal of\nproducing computer vision systems that are fair, inclusive,\nand equitable across segments of the world population.\nFor example, the face recognition system of Fig. 1 should\nnot be more accurate for lighter than darker faces. Non-\ndemographic biases are not related to such demographic\nissues. For example, a person re-identification system can\nperform very effectively on certain datasets by simply\nmatching clothing. However, this is only an illusion of good\nperformance, as such systems will not be able to match\npeople across images collected on different days. In this\ncase, biases are spurious correlations that the computer\nvision systems learn to solve the task. These biases are\nnot necessarily w.r.t. a known attribute, even though such\nattributes can be identified for many tasks, e.g. the clothing\nattribute for re-identification, or the scene and context at-\ntributes for all recognition problems. Demographic and non-\ndemographic biases are quite similar in the sense that they\ntend to originate from dataset or model biases and can be\nmitigated by similar algorithms. Hence, in what follows, we\ncover the two types of biases without much differentiation.\n2.2\nFairness Definitions\nMultiple definitions of fairness [213] have originated from\nsocial studies. Next, we describe the primary definitions of\nfairness in the computer vision literature.\n2.2.1\nIndividual Fairness\nIndividual fairness seeks to \u201ctreat similar individuals sim-\nilarly\u201d [214]. One of the first attempts to formulate this\nobjective was made by [215], where Lipschitz conditions\nwere employed. According to this condition, a small distance\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n6\nin feature space must translate to a small change in the model\u2019s\ndecision. The objective is defined as\ndist(\u02c6yi, \u02c6yj) < L \u00b7 dist(zi, zj)\n(1)\nwhere \u02c6yi and \u02c6yj are decisions of the model, zi = f(xi) and\nzj = f(zj) are features of sample i and j, respectively, and\nL is the Lipschitz constant.\n2.2.2\nGroup Fairness\nGroup fairness requires the model\u2019s decisions to be inde-\npendent or conditionally independent of a sensitive (group)\nattribute. For example, university admissions\u2019 approval or\nrejection decisions must be independent of the applicant\u2019s\ngender. In this example, the sensitive (group) attribute is\ngender. There are three main definitions of group fairness:\nDemographic Parity (DP), Equal Opportunity (EO), and\nEquality of Odds (EOO). DP is defined as\nP( \u02c6Y = y|S = s) = P( \u02c6Y = y|S = s\u2032)\n\u2200s, s\u2032 \u2208S, \u2200y \u2208Y\n(2)\nThe university admissions example requires that the accep-\ntance probability be equal for all genders. Although DP\nis a popular fairness definition, some studies\n[14, 216]\nhave argued that it is not practically relevant since it does\nnot consider the true target label Y for the decision. This\nproblem is addressed by the other two definitions.\nEqual opportunity (EO) is defined as\nP( \u02c6Y = y|Y = y, S = s) = P( \u02c6Y = y|Y = y, S = s\u2032)\n\u2200s, s\u2032 \u2208S, \u2200y \u2208Y\n(3)\nIn the university example, EO requires that the acceptance\nprobability must be equal for all eligible applicants from dif-\nferent sensitive groups. Finally, equality of odds (EOO) re-\nquires equal probability for mistakenly classifying accepted\napplicants from different sensitive groups as accepted appli-\ncants. It is formally defined as,\nP( \u02c6Y = y1|Y = y2, S = s) = P( \u02c6Y = y1|Y = y2, S = s\u2032)\n\u2200s, s\u2032 \u2208S, \u2200y1, y2 \u2208Y\n(4)\n2.2.3\nCounterfactual Fairness\nCounterfactual fairness, defined by Kusner et al. [217],\nrequires identical decision probabilities for a sample and\nits counterfactual counterpart. It requires intervention on\nsensitive attributes to not change the distribution of the\nmodel\u2019s decision [218]. It is formally defined as,\nP\n\u0010\n\u02c6YS\u2190s(U) = y|X = x, S = s\n\u0011\n= P\n\u0010\n\u02c6YS\u2190s\u2032(U) = y|X = x, S = s\n\u0011\n\u2200s, s\u2032 \u2208S\n(5)\nwhere U is an unobserved variable in the causal graph\n(fig. 2). In the university\u2019s admission example, if the model\naccepts a male applicant, it should make the same decision\nif the applicant were female, assuming all other attributes\nare adjusted accordingly. Note that counterfactual samples\nare not created merely by changing the sensitive attribute;\ninstead, they are generated by considering the changes in\nother attributes that result from the alteration of the sensi-\ntive attribute due to causal relationships between them.\n2.2.4\nBias Amplification\nAnother phenomenon studied in bias quantification is the\nexacerbation of biases beyond those present in the dataset\nduring model training. This is usually called bias amplifica-\ntion [21, 49]. It is understood as the difference in the biases\nexhibited by a trained machine learning model relative to\nthe biases present in the data used to train such a model.\nThis term, first used in Zhao et al. [21] for the task of\nsituation recognition, has been used to provide a notion\nof bias that does not depend on any pre-existing notion\nof fairness w.r.t. parity. Reducing bias amplification in a\nmodel is equivalent to reducing the biases only to the extent\nto which they are already present in the training set. For\ninstance, a model that predicts a label at disparate rates\nfor people of different genders will only be considered to\nsuffer from bias amplification if the rates are different from\nthose present in the training data. Wang and Russakovsky\n[49] define the notion of directional bias amplification, which\nfurther refines the bias amplification measure by accounting\nfor varying base rates of the protected attributes.\n3\nBIAS DISCOVERY AND ANALYSIS\nThis section discusses a series of works that discover or\nanalyze biases in computer vision datasets and models. The\ngoal is to identify inherent biases that threaten fairness and\ngeneralization. Uncovering such biases raises awareness of\npotential limitations and biases in computer vision systems\nand helps guide future work to develop more equitable and\nrobust computer vision systems.\n3.1\nBiases in Datasets\nDataset biases can propagate to computer vision models\nor get amplified by the models, thereby influencing their\nfairness and performance. We review studies that analyze\nbiases in commonly used datasets.\nMeister et al. [38] delve into gender biases in large-\nscale visual datasets, exploring how gender information\ncan be removed from datasets and how visual cues, or\n\u201cgender artifacts\u201d, influence model predictions. Guilbeault\net al. [67] compare gender biases in images and text across\nmassive online corpora, revealing how visual content may\namplify gender biases more than textual content. Shankar\net al. [59] investigate the geographical diversity of large\ndatasets such as ImageNet [219] and Open Images [120],\nrevealing noticeable Amerocentric and Eurocentric biases\nthat affect model performance across different global re-\ngions. In the context of facial image datasets Chen and Joo\n[91] found that significant gender biases were introduced\nin the annotations, especially related to facial expressions.\nMore recently, in the context of foundation models, Birhane\net al. [204, 205] examined the presence of hate content in\nthe text annotations of LAION [220], a large-scale dataset\ncommonly used for training vision-language models. They\nfound significant levels of hate content which increased by\n12.26% when scaling from LAION-400M to LAION-2B.\n3.2\nBiases in Models\nBiases in computer vision models can impact their per-\nformance and fairness, especially when these models are\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n7\ndeployed in the real world. Numerous studies sought to\nidentify the presence and impact of biases in different types\nof learning methods and pretrained models across a diverse\nset of visual recognition tasks.\nIn their pioneering work, Buolamwini and Gebru [16]\nevaluated and uncovered gender and skin-tone biases in\nmany commercial face recognition systems and highlighted\nserious implications for high-stakes contexts such as health-\ncare and law enforcement. Domnich and Anbarjafari [85]\ninvestigated bias w.r.t. gender in various facial expression\nrecognition models, assessing and identifying which archi-\ntectures and emotions are more influenced by gender.\nSirotkin et al. [63] investigated the origins and impact\nof social biases in self-supervised learning (SSL) methods,\nrevealing the correlations between the different SSL algo-\nrithms and the number of inherent biases. Iofinova et al. [66]\nexamine how model compression algorithms like pruning\ninduce or exacerbate biases, particularly affecting marginal-\nized groups by increasing systematic and category biases\nunder high sparsity levels. Analysis on vision transform-\ners [65] measured factors contributing to social biases by\ninvestigating training data, objectives, and architectures.\nWilson et al. [111] explore the performance disparities of\nobject detection models in autonomous driving, explicitly\nrevealing poorer detection rates for pedestrians with Fitz-\npatrick skin types 4 to 6, and investigate contributing factors\nsuch as training set composition, measurement issues, and\nthe impact of loss function prioritization.\nA few studies proposed approaches to audit computer\nvision models for potential biases. Ranjit et al. [39] propose\na framework to audit and analyze pretrained visual recogni-\ntion models for biases w.r.t. sensitive visual attributes, eval-\nuating how these biases change after fine-tuning. Studies\non biases of pretrained models on downstream tasks show\nthat such models can inherit biases related to spurious cor-\nrelations and underrepresentation, but these biases can be\nmitigated by carefully curating the finetuning dataset [35].\nSimilarly, Birhane et al. [64] found that transformer-based\nCLIP models inherited racial biases prevalent in the LAION\ndataset on which they were trained.\nConcurrently, Sadeghi et al. [32] and Dehdashtian et al.\n[14] defined and estimated the near-optimal trade-offs be-\ntween model performance (accuracy of predicting target\nattributes) and different group fairness definitions. These\ntrade-offs were utilized to evaluate (a) more than 100 pre-\ntrained CLIP models from OpenCLIP [221], (b) more than\n900 pre-trained image models from Pytorch Image Mod-\nels [222], and (c) existing fair representation methods on\nCelebA and FairFace datasets. The results (shown in Fig.4)\nrevealed that, out of the box, pre-trained models were far\nfrom the best achievable limits of performance and fairness,\nthus identifying the significant limitations of existing com-\nputer vision models and the dire need for further research\nto make computer vision systems more socially responsible\nand equitable. Furthermore, such an evaluation can also\nhelp the community identify trends and pre-trained models\nthat best suit their specific task and dataset.\n3.3\nBiases Beyond Demographic Attributes\nVarious forms of biases beyond demographic attributes\nhave also been studied in computer vision datasets and\nmodels. Torralba and Efros [201] first introduce the notion\nof dataset bias, and identify the distribution gaps between\ndifferent vision datasets w.r.t. viewpoints, styles, and scenes.\nGeirhos et al. [223] discover and analyze the texture bias in\nCNN object classifiers, finding the models more sensitive to\nlocal textures while overlooking object shapes. Li et al. [151]\nstudy the representation bias in action recognition datasets,\nin which action labels are implied through scenes and\nbackground objects. Zhang et al. [224] investigate unimodal\nbias in VQA datasets and show that many visual questions\ncan be answered correctly by using language prior alone.\nThese studies pave the way for the collection of new,\nbias-controlled datasets [151, 187, 223], either to evaluate\nthe models under an unbiased setting or as training data to\nremedy model bias. They also offer insights into how vision\nmodels inherit biases from the data, and as discussed next\nleading to various mitigation methods for training models\nless susceptible to biases [14, 19, 32, 75, 81, 225, 226, 227].\n4\nBIAS MITIGATION METHODS\nThis section summarizes common approaches to mitigate\nbias across various tasks. Each subsection is dedicated to a\nspecific category of algorithms, detailing their applications.\nBy organizing the algorithms this way, we aim to provide a\nclear understanding of bias mitigation in computer vision.\n4.1\nFairness through Unawareness\nA naive approach to fairness is to withdraw sensitive\nprotected attributes from data or to avoid those protected\nattributes as input to the machine learning model. This is\noften referred to as fairness through blindness or fairness\nthrough unawareness. It has been well documented in the\nmachine learning literature that this approach is frequently\nineffective. As discussed above, correlations between sensi-\ntive protected attributes and other attributes can still lead to\nbiases in model predictions, e.g., zip codes being informa-\ntive of race in the case of credit scoring systems.\nIn computer vision, fairness through unawareness could\nbe attempted by blurring people\u2019s faces or removing peo-\nple entirely from images. Such an approach parallels the\ndrawbacks observed in the machine learning literature. For\ninstance, background pixels that are not blurred or obscured\nmay correlate highly with sensitive attributes, e.g. men\nmore commonly wear ties and women more commonly\nwear dresses, so clothing will correlate highly with gender.\nAnother drawback is that for many vision tasks, such as\nhuman activity recognition, the visual features of people are\nessential to accomplish the task accurately. Hence, blurring\nor obscuring people interferes with this goal. Wang et al.\n[51] used fairness through unawareness as a baseline and\ndemonstrated that adversarial bias mitigation by explicitly\nmodeling the sensitive attribute leads to better outcomes.\n4.2\nFair Representation Learning\nOver the last decade, several approaches have been devel-\noped for learning fair image representations (see Figure 3\nfor an illustration). These approaches follow the template of\nadopting a fairness constraint (e.g., Z \u22a5\u22a5S for demographic\nparity or Z \u22a5\u22a5S|Y\n= y for Equality of Odds) as a\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n8\nTABLE 3: Bias analysis and mitigation for vision and language models\nTask\nAttribute\nReferences\nImage Captioning\nGender\nBurns et al. [228], Bhargava and Forsyth [184], Hirota et al. [229]\nRace\nZhao et al. [230]\nSocial\nHirota et al. [231]\nText-to-Image Synthesis\nGender\nEsposito et al. [232], Friedrich et al. [233], He et al. [234], Luccioni et al. [235], Cho\net al. [236]\nRace\nEsposito et al. [232], Friedrich et al. [233], Bansal et al. [237], He et al. [234], Luccioni\net al. [235]\nAdjective\nLuccioni et al. [235]\nProfession\nWang et al. [238], Luccioni et al. [235], Cho et al. [236]\nGeneral\nChinchure et al. [239], Zhang et al. [199]\nStereotype\nBianchi et al. [28]\nPose\nRuiz et al. [240]\nCulture\nLiu et al. [241]\nGeography\nBasu et al. [242]\nSkin Tone\nCho et al. [236]\nQuestion Answering\nLanguage\nManjunatha et al. [186], Kv and Mittal [188], Niu et al. [243], Kervadec et al. [193],\nDancette et al. [194], Wen et al. [244], Cho et al. [245], Basu et al. [246]\nGender\nPark et al. [190], Hirota et al. [198]\nVisual Context\nSelvaraju et al. [191]\nCorrelations\nAgarwal et al. [192], Gupta et al. [247]\nRace\nHirota et al. [198]\nCLIP De-biasing\nGender\nDehdashtian et al. [19], Seth et al. [52], Chuang et al. [54], Berg et al. [248],\nAlabdulmohsin et al. [249]\nRace\nDehdashtian et al. [19], Berg et al. [248], Chuang et al. [54], Seth et al. [52]\nBackground\nDehdashtian et al. [19], Chuang et al. [54], Phan et al. [250]\nOther\n-\nKong et al. [112], Yenamandra et al. [113], Qiu et al. [114], Shankar et al. [59], Chu\net al. [115], Garcia et al. [116], Biswas and Ji [117], Tang et al. [118], Cui et al. [251]\nf(\u00b7)\nZ\nfY\nLY\nfS\nLS\nFig. 3: Fair Representation Learning. An encoder f maps\nimages to a representation Z. A target branch maximizes\nthe statistical dependence between Z and Y , while a fairness\nbranch minimizes the statistical dependence between Z and\nthe protected attribute S. Methods in this class differ in the\nchoice of loss functions LY , LS, models for fY and fS, and\nlearning (iterative vs closed-form, local vs global optima).\nregularizer in addition to the objective for the target task.\nThe approaches differ in two respects: (a) the choice of mea-\nsure as a proxy for quantifying the statistical dependence\ncorresponding to Z \u22a5\u22a5S and Z \u22a5\u22a5S|Y = y, and (b) the\nassociated optimization technique.\nFrom a proxy dependence measure perspective, existing ap-\nproaches either measure (i) the degree of linear dependence\nbetween Y and Z, (ii) the degree of mean dependence,\ni.e., E(Z) \u22a5\u22a5S or matching only the first moment of the\ndistribution, or (iii) the degree of full statistical dependence,\ni.e., matching all moments of the distribution. Adversar-\nial representation learning (ARL) [51, 74, 252, 253, 254]\nadopts neural network-based classifiers or regressors as a\nproxy measure of statistical dependence between Z and S,\nwhich is equivalent to mean dependence [255] only. State-\nof-the-art approaches [14, 19, 32, 84], however, adopt non-\nparametric independence measures like the Hilbert-Schmidt\nIndependence Criterion (HSIC) [256] and its variants [32],\nwhich measures full statistical dependence and can enforce\nindependence-based fairness constraints more effectively.\nFrom an optimization perspective, solutions have either\nadopted iterative approaches like two-player zero-sum min-\nmax optimization for ARL [253] that converge to local\noptima or closed-form solvers [32, 75] that lead to global op-\ntima of the underlying optimization. Due to the inherent in-\nstability of zero-sum min-max optimization, several variants\nof ARL have been proposed. Roy and Boddeti [74] proposed\na non-zero-sum variant of ARL to improve the convergence\nproperties of the ARL optimization. Sadeghi et al. [75]\nstudied ARL from an optimization perspective and obtained\na closed-form solution that affords global optima of the\nARL optimization through spectral learning and provided\ntheoretical guarantees for achieving utility and fairness.\nSadeghi et al. [257] used a kernel ridge regressor to model\nthe adversary and backpropagated through its closed-form\nsolution, resulting in stable optimization and improved\nperformance utility-fairness trade-off. Sadeghi et al. [32]\nproposed a non-parametric dependence measure to capture\nall non-linear statistical dependencies and obtained a global\noptimum of the underlying optimization problem through a\nclosed-form solution, thus obtaining provably near-optimal\nutility-fairness trade-offs.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n9\nA majority of the debiasing methods in computer vision\nare based on ARL. For instance, cross-sample adversarial\ndebiasing (CSAD) disentangles target and bias features to\nprevent biased decision-making [45]. The Causal Attention\nModule (CaaM) employs an adversarial minimax fashion to\ndisentangle and optimize complementary attention mech-\nanisms [56]. The Lottery Ticket Hypothesis is adopted to\nfind fair and accurate subnetworks without weight training,\nleveraging fairness regularization and adversarial training\nfor bias mitigation [37]. Furthermore, fairness-aware adver-\nsarial perturbation (FAAP) modifies input data to conceal\nfairness-related features from deployed models without ad-\njusting the model parameters or structures [42]. FAIRRE-\nPROGRAM introduces fairness triggers appended to inputs,\noptimizing them under a min-max formulation with an\nadversarial loss to obscure demographic biases [73].\nIn face recognition, adversarial learning reduces the en-\ncoding of sensitive attributes in face representations. For\ninstance, adversarial learning frameworks can disentangle\ndemographic information from identity representations, re-\nducing bias in face recognition and demographic attribute\nestimation [96]. The Protected Attribute Suppression Sys-\ntem (PASS) employs a discriminator to prevent networks\nfrom embedding protected attribute information, thereby\nmitigating gender and skin tone biases without end-to-\nend training [86]. Techniques using the Hilbert-Schmidt\nindependence criterion transform input data into fair repre-\nsentations that maintain semantic meaning while ensuring\nstatistical independence from protected characteristics [98].\nFor generative models, adversarial methods harmonize ad-\nversarial training with reconstructive generation to improve\ndata coverage and include minority groups more effec-\ntively [103]. Lastly, unknown biased attributes in classifiers\ncan be identified by optimizing a hyperplane in a generative\nmodel\u2019s latent space using total variation loss and orthogo-\nnalization penalty [80].\n4.3\nAccuracy-Unfairness Trade-Offs\nIn scenarios where the target attribute Y and the sensitive\nattribute S exhibit considerable statistical dependency, the\nobjectives of learning a fair representation\u2014specifically, re-\nmoving information related to S while retaining informa-\ntion pertinent to Y \u2014are in conflict. This conflict impacts the\nperformance of these objectives. Consequently, a trade-off\nexists between the retention of Y -related information and\nthe removal of S-related information. This trade-off can be\nobserved through the accuracy, MSE loss, F1 score, etc., of\npredicting Y and the fairness of the decisions made by the\nmodel. We generally use the word utility to refer to the\nmodel\u2019s performance in retaining Y -related information.\nThe existence of a utility-fairness trade-off has been\nwell established both theoretically [32, 258, 259, 260] and\nempirically [14, 32]. Sadeghi et al. [32] characterize the near-\noptimal trade-off for multidimensional continuous/discrete\nattributes using a closed-form solution on the extracted fea-\ntures from a frozen feature extractor. Additionally, Dehdash-\ntian et al. [14] define two trade-offs: the Data Space Trade-\nOff (DST) and the Label Space Trade-Off (LST). These trade-\noffs capture the intrinsic relationship between the Y and S\nlabels independently of the samples. They achieve this by\nPossible\nImpossible\nPossible with Extra Data\nDST\nLST\nUtility (Y)\nUnfairness (S)\n0\n10\n20\n30\nEOD (%)\n30\n40\n50\n60\n70\n80\n90\n100\nAccuracy (%)\nDST\nLST\nZero-Shot\nSupervised\nFig. 4: The utility-fairness trade-offs.[14] (Left) Models can\nbe evaluated by their utility (e.g., accuracy, MSE loss, F1\nscore, etc.) w.r.t. a target label Y and their unfairness w.r.t.\na sensitive attribute S. Dehdashtian et al. [14] introduce\ntwo trade-offs, Data Space Trade-Off (DST) and Label Space\nTrade-Off (LST). (Right) Dehdashtian et al. [14] empirically\nestimate DST and LST on CelebA and evaluate the utility\n(high cheekbones) and fairness (gender & age) of over 100\nzero-shot and 900 supervised image models.\nemploying a trainable feature extractor alongside a closed-\nform solution for the fair encoder. Although these studies\nfocus on estimating trade-offs, the methods proposed can\nalso serve as state-of-the-art bias mitigation techniques, as\nthey identify and operate within the best achievable regions.\nEstimating these trade-offs can be beneficial in several\nways [14]: (1) they provide users with more information\nto make informed decisions when choosing a pre-trained\nmodel, and (2) they illustrate the extent to which fine-tuning\ncan improve the utility or fairness of the model. Figure 4\nillustrates the plausible trade-offs, their empirical estimation\non CelebA [121], and their utility in empirically evaluating\nrepresentations from pre-trained models.\n4.4\nCounterfactual Data Rebalancing\nCounterfactual data rebalancing addresses bias by generat-\ning or reweighting data to create balanced representations\nof different groups. Several approaches have been proposed\nto operationalize this conceptual idea. In image classifica-\ntion, GAN-based data augmentation has been combined\nwith adaptive sampling for disadvantaged group accuracy\nenhancement [69]. FlowAug employs flow-based generative\nmodels to create semantically augmented images, reducing\nsubgroup performance discrepancies by addressing spuri-\nous correlations [78]. The Confidence-based Group Label\nassignment (CGL) method assigns pseudo group labels to\nunlabeled samples based on prediction confidence [72].\nIn face recognition, methods like INV-REG self-annotate\ndemographic attributes and impose invariant regularization\nduring training to learn causal features robust across diverse\ndemographic groups [88]. StyleGANs transfer multiple de-\nmographic attributes simultaneously, enhancing dataset di-\nversity and mitigating bias in face recognition systems [94].\nIt is also possible to generate synthetic images to supple-\nment imbalanced datasets, creating a semi-synthetic bal-\nanced dataset to improve fairness in facial attribute and\ngender classification tasks [95].\nIn semantic segmentation, randomly dropping class-\nspecific feature maps disentangles class representations and\nmitigates dataset biases [115]. In image captioning, a De-\nbiasing Caption Generator (DCG) has been proposed to\ncorrect gender-biased captions, forming a model-agnostic\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n10\ndebiasing framework [229]. In action recognition, StillMix\nmitigates background and foreground static bias by mixing\nbias-inducing frames with training videos [17]. Person Re-\nidentification methods introduce causal inference to elimi-\nnate clothing bias from identity representation learning [18].\nIn scene graph generation, counterfactual causality iso-\nlates and removes the effects of context bias [118]. In feder-\nated learning, Bias-Eliminating Augmenters (BEA) at each\nclient generate bias-conflicting samples, thereby mitigating\nlocal data biases during distributed training [261]. For bias\ndiscovery, model reliance on spurious correlations is ampli-\nfied to segregate bias-conflicting samples, which are then\nidentified and mitigated through a slicing strategy [113].\nIn representation learning, biases can be mitigated\nthrough resampling, penalizing examples that are easily\nclassified by a specific feature representation, and reweight-\ning the dataset through a minimax optimization prob-\nlem [58]. Methods like BiaSwap create balanced datasets\nthrough an unsupervised image translation-based augmen-\ntation framework that identifies and swaps bias-relevant\nregions in images [48]. Some methods use GANs to generate\nimages with altered combinations of target and protected\nattributes to decorrelate them [47]. Creating bias-reducing\npositive and negative sample pairs from a self-supervised\nobject localization method can also mitigate scene bias [61].\nA fair contrastive learning method uses gradient-based\nreweighting to learn fair representations without demo-\ngraphic information by incorporating a small labeled set\ninto the self-supervised training process [44]. Other methods\nidentify bias pseudo-attributes via clustering and reweight\nthese clusters based on their size and task-specific accu-\nracy to improve worst-group generalization [43]. Some ap-\nproaches identify intermediate attribute samples near de-\ncision boundaries and use them for conditional attribute\ninterpolation to learn debiased representations [36]. Class-\nconditioned sampling mitigates bias by creating multiple\nbalanced dataset variants, each with a subsampled distribu-\ntion that mimics the bias distribution of the target class [33].\nThe Debiased Contrastive Weight Pruning (DCWP) method\nidentifies bias-conflicting samples and uses them to train a\npruned neural network [30].\n4.5\nScore Calibration and Loss Regularization\nScore calibration adjusts the decision thresholds or predic-\ntion scores of models to ensure fair outcomes across differ-\nent demographic groups. In image classification, score cali-\nbration can mitigate contextual bias by minimizing overlap\nin class activation maps and learning uncorrelated feature\nrepresentations to ensure accurate recognition of both in and\nout of typical category contexts [77]. Some methods pro-\npose fairness surrogates to optimize constraints in network\ntraining [70], while others address spurious correlations\nand intrinsic dependencies with non-parametric measures\nof statistical dependence [19]. U-FaTE [14] quantifies utility-\nfairness trade-offs by optimizing a weighted combination\nof statistical dependence measures to evaluate and improve\nthe fairness of pre-trained models.\nIn face recognition, a fair loss with an adaptive margin\nstrategy optimized via reinforcement learning has been\nproposed to address class imbalance [98]. The Group\nAdaptive Classifier (GAC) uses adaptive convolution ker-\nnels and channel-wise attention maps to learn general\nand demographic-specific patterns and reduce demographic\nbias in face recognition [87].\nAn Equalizer Model with two complementary losses has\nbeen proposed for image captioning to leverage gender-\nspecific visual evidence and generate gender-neutral words\nwhen uncertain [15]. To achieve equal representation in the\nimage retrieval task, a test-time post-processing algorithm\ncreates fair retrieval subsets by using predicted gender\nor race attributes from the classifier or zero-shot infer-\nence [112]. For Bayesian networks, posterior inference can\ncombine within-triplet priors with uncertain evidence to\nmitigate long-tailed bias [117]. In continual learning, task-\ninduced bias can be mitigated using causal interventions\nwith attention mechanisms that transform biased features\ninto unbiased features [114].\nIn representation learning, a regularization strategy en-\ntangles features from the same target class and disentan-\ngles biased features [46]. Fairness-aware feature distillation\nimproves fairness using maximum mean discrepancy to\nalign the distributions of group-conditioned features from\na student model with the group-averaged features of a\nteacher model [57]. A fair contrastive loss and a group-wise\nnormalization are proposed in [41] to prevent the inclusion\nof sensitive attribute information and balance loss based\non group cardinality, respectively. Leveraging hierarchical\nfeatures and orthogonal regularization has also been shown\nto mitigate unknown biases [40].\n5\nDATASETS\nIn this section, we summarize various datasets used in\nfairness-related tasks in computer vision along with their\ncorresponding sensitive attributes. The tasks and their\ndatasets are listed in Table 1. The tasks range from ac-\ntion recognition and text-to-image to face recognition and\nclassification. We also discuss datasets used in multiple\ntasks, those specialized for a single task, and the attributes\ninvestigated most and least.\n5.1\nDiversity of Datasets and Attributes\nSome datasets in the table are used across multiple fairness-\nrelated computer vision tasks. For instance, MSCOCO [8]\nand its variants [15, 140] are used in bias analysis and eval-\nuation of fairness in the context of classification, image cap-\ntioning, image retrieval, scene graph generation, and VQA.\nSimilarly, CelebA [121] is extensively used in evaluating and\nmitigating bias for classification, face recognition, and text-\nto-image tasks, emphasizing the need to address biases in\ngender, ethnicity, and age. UTKFace [130] is employed in\nfairness in classification, face recognition, and representa-\ntion learning for investigating and mitigating biases related\nto age, gender, ethnicity, and skin tone. OpenImages [120] is\nalso used in both bias analysis and representation learning\nwith a focus on gender and geography biases. The repeated\nuse of these datasets underscores their importance and\nhighlights their value in providing diverse annotations for\nevaluating and mitigating bias across computer vision tasks.\nIt is evident from analyzing Table 2 that certain sensitive\nattributes are more frequently investigated across various\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n11\ncomputer vision tasks, while others receive less attention.\nGender stands out as the most frequently studied attribute\nacross tasks such as bias analysis, classification, face recog-\nnition and analysis, image captioning, image retrieval, rep-\nresentation learning, text-to-image, and VQA. Datasets like\nMSCOCO [8, 15, 140], CelebA [121], OpenImages [120],\nIMDB Face [139], FairFace [148], and UTKFace [130] are\noften used to explore and mitigate gender biases. This re-\nflects the bias of the computer vision community towards\nevaluating and mitigating gender biases.\nRace and ethnicity are also critical attributes, especially in\nface recognition and representation learning tasks. Datasets\nsuch as PPB [16], IJB-A [155], Fairface [148], UTKFace [130],\nMSCOCO [8], and Adience [156] are frequently used to\ninvestigate these biases. Similarly, age is a commonly investi-\ngated attribute in classification, face recognition, and repre-\nsentation learning tasks with datasets like IMDB Face [129],\nUTKFace [130], MORPH [170], CACD [172], and MS-Celeb-\n1M [96, 174] being used to examine age-related biases.\nContext is another frequently considered attribute, es-\npecially in classification, image captioning, representation\nlearning, and VQA. This attribute is investigated using\ndatasets like MSCOCO [8], UnRel [141], Deep Fashion [142],\nNICO++ [124], and PATA [52].\nIn contrast to the above-mentioned attributes, certain\nattributes have received less attention. For example, the\nillumination attribute is only addressed by the Extended\nYale B [146] dataset within the classification task. Similarly,\nHair Color is primarily referenced in the CelebA dataset.\nWhile Skin Tone is addressed in some tasks, it appears less\nfrequently compared to attributes like gender or race. Addi-\ntionally, Texture is an attribute less commonly investigated,\nappearing mainly in the ImageNet-A [132] dataset for repre-\nsentation learning. Lastly, corruption is mainly mentioned in\nthe context of federated learning and representation learn-\ning in datasets like Corrupted CIFAR-10 [126].\n5.2\nTask Specific Diversity of Datasets and Attributes\nUnderscoring the need for a more detailed examination of\nbiases in specific domains, a diverse array of datasets and\nattributes have been specialized for each domain.\nThe action recognition task exhibits a moderate diver-\nsity in its datasets, incorporating a range of scene contexts\nfrom UCF-101 [149] and HMDB-51 [150] to specialized\ndatasets like Diving48 [151] and THUMOS-14 [152]. These\ndatasets provide various environments and activities, en-\nsuring varied training and evaluation conditions. However,\nthe sensitive attribute used in this task is scene, as biases\ncan arise from the background, objects, or context in which\naction occurs. This emphasis on scene-based attributes high-\nlights the need to mitigate biases that stem from environ-\nmental contexts affecting action recognition performance.\nWithout such mitigation, computer vision models can lever-\nage context as a shortcut to solve the action recognition\nproblem without understanding any action [151].\nPapers in the bias analysis task leverage a diverse array\nof datasets, including MSCOCO [8], OpenImages [120], and\nCelebA [121], which cover multiple demographic and con-\ntextual attributes. The most commonly studied attributes in\nthis task are gender and age, reflecting a significant concern\nwithin the computer vision community regarding the im-\npact of these biases on model decisions.\nFrequently studied attributes in face recognition and\nanalysis studies include gender, ethnicity/race, skin tone, and\nage. These studies have been performed on a diverse set of\ndatasets like PPB [16], IJB-A [155], and Adience [156].\nFederated learning tasks use specialized datasets, such\nas Colored MNIST [137] and Corrupted CIFAR-10 [126],\nfocusing on attributes like color and corruption. While the\ndiversity of the attributes is limited compared to other tasks,\nthey are tailored to study specific biases and robustness\nissues prevalent in federated learning environments.\nImage captioning tasks utilize datasets like MSCOCO-\nBias [15] and MSCOCO-Balanced [15], which are designed\nto highlight and mitigate gender and racial biases in image\ndescriptions, reflecting a desire for fair and representative\ncaptions. The diversity of these datasets lies in their annota-\ntions and the variety of contexts they provide.\nThe image retrieval task makes use of datasets such as\nOccupation 1 [178], Occupation 2 [179], MSCOCO [8], and\nFlickr30k [180], with a primary focus on gender.\nFairness\nstudies\nin\nobject\ndetection\nutilize\nthe\nBDD100K [181] dataset, focusing on attributes like skin tone\nand income. The diversity in this task is centered around ad-\ndressing biases that are particularly relevant in autonomous\ndriving and other detection-based applications.\nPerson re-identification tasks use datasets such as\nPRCC-ReID [182] and LTCC-ReID [183], primarily focusing\non the attribute of clothing, highlighting the importance of\nmitigating biases w.r.t. clothing for this task.\nRepresentation learning tasks demonstrate high diver-\nsity with datasets like ImageNet [7, 59], Open Images [59,\n120], MSCOCO [8], and CIFAR-10S [262], addressing a broad\nrange of attributes including geography, gender, color, cor-\nruption, scene, and context. The focus on multiple attributes\nindicates an effort to create robust models that generalize\nwell across various demographic and environmental factors.\nScene graph generation employs datasets such as\nVG [197] and MSCOCO [8], with an emphasis on the\nattribute of composition. This task\u2019s diversity in datasets\nis geared towards understanding and mitigating biases in\nscene understanding and object relationships.\nFor\ndebiasing\ntext-to-image\nmodels\ndatasets\nlike\nCelebA [121], FAIR [200], and FairFace [148] are used. The\nprimary focus in this task has been debiasing with respect to\nattributes such as gender and skin tone. By leveraging these\ndiverse datasets, researchers aim to address biases that can\narise from textual prompts influencing image generation.\nIn visual question answering (VQA), a variety of\ndatasets are utilized, including VQA [185], MSCOCO [8],\nVQA-CP v2 [187], and Visual7W [195]. These datasets ad-\ndress attributes like language, context, gender, and race, en-\nsuring comprehensive evaluation and mitigation of biases\nin multimodal understanding.\n6\nCURRENT TRENDS AND FUTURE WORK\nFairness in Generative Models: The availability of large\nmultimodal datasets [220], coupled with significant ad-\nvancements in generative modeling [263, 264, 265, 266, 267,\n268, 269, 270], has substantially increased the capabilities\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n12\nand potential applications of generative models [199]. Con-\ncurrently, some studies [28, 266, 268] began demonstrating\nthat the content generated by these models exhibits biases.\nHowever, these have predominantly focused on the diver-\nsity of the generated images across different demographic\ngroups. As such, a formal and precise mathematical defini-\ntion of fairness still does not exist for generative models.\nIn\nthe\ncontext\nof\nText-to-Image\n(TTI)\ngeneration,\napproaches\nfor\nincreasing\nthe\ndiversity\nof\ngenerated\ncontent\n[108,\n233,\n235]\ncan\nbe\ncategorized\ninto\ntwo\nmain groups, namely prompt engineering and guidance.\nPrompt Engineering: Methods [271] in this category focus on\ndesigning better text prompts to force the model to generate\nmore diverse images. Guidance: These methods [272, 273,\n274] seek to improve the diversity of generated images by\nmodifying the distribution of generated images. However,\nall of these methods still struggle to achieve fine-grained\ncontrol over the generation process, often resulting in unin-\ntended changes to other attributes when attempting to mod-\nify the protected ones. This challenge is primarily because\ndifferent attributes are entangled with each other [271].\nFairness in Foundation Models: Recent advances in trans-\nformer architectures and large-scale pretraining have led\nto the development of families of multimodal foundation\nmodels [275, 276, 277] that demonstrate remarkable gen-\neralization capacity to novel tasks and domains. Just like\napplication-specific models, foundation models also exhibit\ndemographic and other biases across different downstream\ntasks. This has been observed for tasks including classifica-\ntion [278], retrieval [248], captioning [229] and visual ques-\ntion answering [279]. Bias analysis and mitigation methods\nin this area mainly concern two types of foundation models.\nImage-Text Models: Some efforts have been made to ad-\ndress bias in the image and text representations learned by\ncontrastive models such as CLIP [275] and SigLIP [280] in\nthe context of zero-shot image classification and image-text\nretrieval tasks. These approaches typically employ a set of\nsensitive text or image queries to debias CLIP embeddings\nthrough prompt tuning [248], auxiliary modules [52, 207],\nand linear [54] or nonlinear [19] feature mappings. A dis-\ntinguishing feature of FairerCLIP [19] is its ability to debias\nthe representations without needing ground-truth labels Y\nand S. Data rebalancing [249] has also been explored as an\nalternative to model debiasing. Although these debiasing\napproaches have focused on zero-shot image classification\nand image-text retrieval tasks, understanding and mitigat-\ning biases in aligned image-text representations, have far\ngreater implications, as CLIP-style models are commonly\nused as feature extractors in large multimodal models and\ntext-to-image generation.\nLarge Multimodal Models: A few recent efforts [251, 281]\nhave also been made to uncover biases in large multimodal\nmodels (LMMs), such as GPT-4V [282] and LLaVA [283],\ncapable of more versatile tasks including captioning and\nVQA. Owing to the variety of tasks and the rapid evolution\nof multimodal architectures, work in this area is still scarce\nand does not fully capture the complexities of LMM fairness.\nApart from the initial forays discussed above, under-\nstanding and mitigating biases in foundation models largely\nremain an open problem. This state of affairs offers both new\nopportunities and new challenges to the computer vision\ncommunity. First, foundation models are often applied to\nsolve downstream tasks in zero-shot or few-shot settings,\nwhich have not been the subject of much study in the\ndebiasing literature. Second, while this literature typically\naddresses the fairness of specific tasks, there are no unified\nmeasures of fairness for the diverse tasks and contexts on\nwhich foundation models are evaluated. Finally, the most\nwidely adopted notions of fairness are defined with respect\nto closed vocabularies of target labels and sensitive attributes.\nThis is insufficient to quantify the fairness of foundation\nmodels, which target open set applications using vocabular-\nies based on natural language.\n7\nCONCLUDING REMARKS\nIn this paper, we reviewed the advancements made by the\nresearch community in measuring and mitigating bias w.r.t.\nsensitive protected groups in various computer vision tasks.\nSpecifically, we discussed the social and technical origins of\nbias in computer vision systems, the different definitions of\nfairness considered by the research community, and differ-\nent bias mitigation techniques and benchmark datasets to\nevaluate and compare them. Finally, we discussed fairness\nand bias in the context of modern multimodal foundation\nand generative models. We hope this survey provides a\nhelpful and detailed overview for new researchers and\npractitioners, provides a convenient reference for relevant\nexperts, and encourages future progress in the informed\ndesign of fair and equitable computer vision systems.\nAcknowledgments. This work was funded by NSF Awards\nNo. 2221943, 2041009, and 2147116 and gift funding from\nAmazon through the NSF Program on Fairness in Artificial\nIntelligence in Collaboration with Amazon. We also thank\nthe organizers of the NSF-Amazon PI Meeting in Arlington,\nVA, on January 2024.\nREFERENCES\n[1]\nH. A. Rowley, S. Baluja, and T. Kanade, \u201cNeural\nnetwork-based face detection,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 20, no. 1,\npp. 23\u201338, 1998.\n[2]\nP. Viola and M. Jones, \u201cRapid object detection using a\nboosted cascade of simple features,\u201d in CVPR, 2001.\n[3]\nN. Dalal and B. Triggs, \u201cHistograms of oriented gra-\ndients for human detection,\u201d in CVPR, 2005.\n[4]\nA. Krizhevsky, I. Sutskever, and G. E. Hinton, \u201cIm-\nagenet classification with deep convolutional neural\nnetworks,\u201d NeurIPS, 2012.\n[5]\nK. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual\nlearning for image recognition,\u201d in CVPR, 2016.\n[6]\nA. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weis-\nsenborn,\nX.\nZhai,\nT.\nUnterthiner,\nM.\nDehghani,\nM. Minderer, G. Heigold, S. Gelly et al., \u201cAn image\nis worth 16x16 words: Transformers for image recog-\nnition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[7]\nO. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh,\nS. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bern-\nstein et al., \u201cImagenet large scale visual recognition\nchallenge,\u201d International Journal of Computer Vision, vol.\n115, pp. 211\u2013252, 2015.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n13\n[8]\nT.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona,\nD. Ramanan, P. Doll\u00b4ar, and C. L. Zitnick, \u201cMicrosoft\nCOCO: common objects in context,\u201d in ECCV, 2014.\n[9]\nO. Ronneberger, P. Fischer, and T. Brox, \u201cU-Net: con-\nvolutional networks for biomedical image segmenta-\ntion,\u201d in MICCAI, 2015.\n[10]\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, \u201cHigh-resolution image synthesis with\nlatent diffusion models,\u201d in CVPR, 2022.\n[11]\nA. Howard, J. Borenstein, and K. Gosha, \u201cNSF-funded\nFairness, Ethics, Accountability, and Transparency\n(FEAT) Workshop Report,\u201d in NSF Workshop Reports,\n2019.\n[12]\nJ. Angwin, J. Larson, S. Mattu, and L. Kirchner, \u201cMa-\nchine bias,\u201d in Ethics of data and analytics.\nAuerbach\nPublications, 2022, pp. 254\u2013264.\n[13]\nV. Mhasawade, Y. Zhao, and R. Chunara, \u201cMachine\nlearning and algorithmic fairness in public and popu-\nlation health,\u201d Nature Machine Intelligence, vol. 3, no. 8,\npp. 659\u2013666, 2021.\n[14]\nS. Dehdashtian, B. Sadeghi, and V. N. Boddeti,\n\u201cUtility-Fairness trade-offs and how to find them,\u201d in\nCVPR, 2024.\n[15]\nL. A. Hendricks, K. Burns, K. Saenko, T. Darrell, and\nA. Rohrbach, \u201cWomen also snowboard: Overcoming\nbias in captioning models,\u201d in ECCV, 2018.\n[16]\nJ. Buolamwini and T. Gebru, \u201cGender Shades: inter-\nsectional accuracy disparities in commercial gender\nclassification,\u201d in FAccT, 2018.\n[17]\nH. Li, Y. Liu, H. Zhang, and B. Li, \u201cMitigating and\nevaluating static bias of action representations in the\nbackground and the foreground,\u201d in ICCV, 2023.\n[18]\nZ. Yang, M. Lin, X. Zhong, Y. Wu, and Z. Wang, \u201cGood\nis Bad: causality inspired cloth-debiasing for cloth-\nchanging person re-identification,\u201d in CVPR, 2023.\n[19]\nS. Dehdashtian, L. Wang, and V. N. Boddeti, \u201cFair-\nerCLIP: Debiasing clip\u2019s zero-shot predictions using\nfunctions in rkhss,\u201d ICLR, 2024.\n[20]\nJ. Angwin, J. Larson, S. Mattu, and L. Kirchner, \u201cMa-\nchine bias,\u201d ProPublica, vol. 23, pp. 139\u2013159, 2016.\n[21]\nJ. Zhao, T. Wang, M. Yatskar, V. Ordonez, and K.-W.\nChang, \u201cMen also like shopping: Reducing gender\nbias amplification using corpus-level constraints,\u201d in\nEMNLP, 2017.\n[22]\nN. Mehrabi, F. Morstatter, N. Saxena, K. Lerman,\nand A. Galstyan, \u201cA survey on bias and fairness in\nmachine learning,\u201d ACM Computing Surveys, vol. 54,\nno. 6, pp. 1\u201335, 2021.\n[23]\nD. Pessach and E. Shmueli, \u201cA review on fairness in\nmachine learning,\u201d ACM Computing Surveys, vol. 55,\nno. 3, pp. 1\u201344, 2022.\n[24]\nT. Le Quy, A. Roy, V. Iosifidis, W. Zhang, and\nE. Ntoutsi, \u201cA survey on datasets for fairness-aware\nmachine learning,\u201d Wiley Interdisciplinary Reviews:\nData Mining and Knowledge Discovery, vol. 12, no. 3,\np. e1452, 2022.\n[25]\nS. Caton and C. Haas, \u201cFairness in machine learning:\nA survey,\u201d ACM Computing Surveys, vol. 56, no. 7, pp.\n1\u201338, 2024.\n[26]\nO. Parraga, M. D. More, C. M. Oliveira, N. S. Gaven-\nski, L. S. Kupssinsk\u00a8u, A. Medronha, L. V. Moura, G. S.\nSim\u02dcoes, and R. C. Barros, \u201cFairness in deep learning:\nA survey on vision and language research,\u201d ACM\nComputing Surveys, 2023.\n[27]\nM.\nYatskar,\nV.\nOrdonez,\nL.\nZettlemoyer,\nand\nA. Farhadi, \u201cCommonly uncommon: Semantic spar-\nsity in situation recognition,\u201d in CVPR, 2017.\n[28]\nF. Bianchi, P. Kalluri, E. Durmus, F. Ladhak, M. Cheng,\nD. Nozza, T. Hashimoto, D. Jurafsky, J. Zou, and\nA. Caliskan, \u201cEasily accessible text-to-image gen-\neration amplifies demographic stereotypes at large\nscale,\u201d in FAccT, 2023.\n[29]\nM. K. Scheuerman and J. R. Brubaker, \u201cProducts of po-\nsitionality: How tech workers shape identity concepts\nin computer vision,\u201d in CHI, 2024.\n[30]\nG. Y. Park, S. Lee, S. W. Lee, and J. C. Ye, \u201cTraining\ndebiased subnetworks with contrastive weight prun-\ning,\u201d in CVPR, 2023.\n[31]\nJ. Li, D. M. Vo, and H. Nakayama, \u201cPartition-and-\ndebias: Agnostic biases mitigation via a mixture of\nbiases-specific experts,\u201d in ICCV, 2023.\n[32]\nB. Sadeghi, S. Dehdashtian, and V. Boddeti, \u201cOn\ncharacterizing the trade-off in invariant representation\nlearning,\u201d Transactions on Machine Learning Research,\n2022, Featured Certification.\n[33]\nM. Qraitem, K. Saenko, and B. A. Plummer, \u201cBias\nMimicking: a simple sampling approach for bias mit-\nigation,\u201d in CVPR, 2023.\n[34]\nT. Jang and X. Wang, \u201cDifficulty-based sampling\nfor debiased contrastive representation learning,\u201d in\nCVPR, 2023.\n[35]\nA. Wang and O. Russakovsky, \u201cOverwriting pre-\ntrained bias with finetuning data,\u201d in ICCV, 2023.\n[36]\nY.-K. Zhang, Q.-W. Wang, D.-C. Zhan, and H.-J. Ye,\n\u201cLearning debiased representations via conditional\nattribute interpolation,\u201d in CVPR, 2023.\n[37]\nP. Tang, W. Yao, Z. Li, and Y. Liu, \u201cFair Scratch Tickets:\nfinding fair sparse networks without weight training,\u201d\nin CVPR, 2023.\n[38]\nN. Meister, D. Zhao, A. Wang, V. V. Ramaswamy,\nR. Fong, and O. Russakovsky, \u201cGender artifacts in\nvisual datasets,\u201d in ICCV, 2023.\n[39]\nJ. Ranjit, T. Wang, B. Ray, and V. Ordonez, \u201cVariation\nof gender biases in visual recognition models before\nand after finetuning,\u201d arXiv preprint arXiv:2303.07615,\n2023.\n[40]\nM. Jeon, D. Kim, W. Lee, M. Kang, and J. Lee, \u201cA con-\nservative approach for unbiased learning on unknown\nbiases,\u201d in CVPR, 2022.\n[41]\nS. Park, J. Lee, P. Lee, S. Hwang, D. Kim, and H. Byun,\n\u201cFair contrastive learning for facial attribute classifi-\ncation,\u201d in CVPR, 2022.\n[42]\nZ. Wang, X. Dong, H. Xue, Z. Zhang, W. Chiu, T. Wei,\nand K. Ren, \u201cFairness-aware adversarial perturbation\ntowards bias mitigation for deployed deep models,\u201d\nin CVPR, 2022.\n[43]\nS. Seo, J.-Y. Lee, and B. Han, \u201cUnsupervised learning\nof debiased representations with pseudo-attributes,\u201d\nin CVPR, 2022.\n[44]\nJ. Chai and X. Wang, \u201cSelf-supervised fair representa-\ntion learning without demographics,\u201d NeurIPS, 2022.\n[45]\nW. Zhu, H. Zheng, H. Liao, W. Li, and J. Luo, \u201cLearn-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n14\ning bias-invariant representation by cross-sample mu-\ntual information minimization,\u201d in ICCV, 2021.\n[46]\nE. Tartaglione, C. A. Barbano, and M. Grangetto,\n\u201cEnD: entangling and disentangling deep representa-\ntions for bias correction,\u201d in CVPR, 2021.\n[47]\nV. V. Ramaswamy, S. S. Kim, and O. Russakovsky,\n\u201cFair attribute classification through latent space de-\nbiasing,\u201d in CVPR, 2021.\n[48]\nE. Kim, J. Lee, and J. Choo, \u201cBiaSwap: removing\ndataset bias with bias-tailored swapping augmenta-\ntion,\u201d in ICCV, 2021.\n[49]\nA. Wang and O. Russakovsky, \u201cDirectional bias am-\nplification,\u201d in ICML, 2021.\n[50]\nZ. Wang, K. Qinami, I. C. Karakozis, K. Genova,\nP. Nair, K. Hata, and O. Russakovsky, \u201cTowards fair-\nness in visual recognition: Effective strategies for bias\nmitigation,\u201d in CVPR, 2020.\n[51]\nT. Wang, J. Zhao, M. Yatskar, K.-W. Chang, and V. Or-\ndonez, \u201cBalanced datasets are not enough: Estimating\nand mitigating gender bias in deep image representa-\ntions,\u201d in ICCV, 2019.\n[52]\nA. Seth, M. Hemani, and C. Agarwal, \u201cDear: Debias-\ning vision-language models with additive residuals,\u201d\nin CVPR, 2023.\n[53]\nS. M. Hall, F. Gonc\u00b8alves Abrantes, H. Zhu, G. So-\ndunke, A. Shtedritski, and H. R. Kirk, \u201cVisogender:\nA dataset for benchmarking gender bias in image-text\npronoun resolution,\u201d NeurIPS, 2024.\n[54]\nC.-Y. Chuang, V. Jampani, Y. Li, A. Torralba, and\nS. Jegelka, \u201cDebiasing vision-language models via bi-\nased prompts,\u201d arXiv preprint arXiv:2302.00070, 2023.\n[55]\nE. Van Miltenburg, \u201cStereotyping and bias in the\nflickr30k dataset,\u201d arXiv preprint arXiv:1605.06083,\n2016.\n[56]\nT. Wang, C. Zhou, Q. Sun, and H. Zhang, \u201cCausal\nattention for unbiased visual recognition,\u201d in ICCV,\n2021.\n[57]\nS. Jung, D. Lee, T. Park, and T. Moon, \u201cFair feature\ndistillation for visual recognition,\u201d in CVPR, 2021.\n[58]\nY. Li and N. Vasconcelos, \u201cREPAIR: removing repre-\nsentation bias by dataset resampling,\u201d in CVPR, 2019.\n[59]\nS. Shankar, Y. Halpern, E. Breck, J. Atwood, J. Wil-\nson, and D. Sculley, \u201cNo classification without rep-\nresentation: Assessing geodiversity issues in open\ndata sets for the developing world,\u201d arXiv preprint\narXiv:1711.08536, 2017.\n[60]\nA. Wang, A. Liu, R. Zhang, A. Kleiman, L. Kim,\nD. Zhao, I. Shirai, A. Narayanan, and O. Russakovsky,\n\u201cREVISE: a tool for measuring and mitigating bias\nin visual datasets,\u201d International Journal of Computer\nVision, vol. 130, no. 7, pp. 1790\u20131810, 2022.\n[61]\nS. Mo, H. Kang, K. Sohn, C.-L. Li, and J. Shin, \u201cObject-\naware contrastive learning for debiased scene repre-\nsentation,\u201d NeurIPS, 2021.\n[62]\nC. Schumann, F. Olanubi, A. Wright, E. Monk, C. Hel-\ndreth, and S. Ricco, \u201cConsensus and subjectivity of\nskin tone annotation for ml fairness,\u201d NeurIPS, 2024.\n[63]\nK. Sirotkin, P. Carballeira, and M. Escudero-Vi\u02dcnolo,\n\u201cA study on the distribution of social biases in self-\nsupervised learning visual models,\u201d in CVPR, 2022.\n[64]\nA. Birhane, S. Dehdashtian, V. Prabhu, and V. Boddeti,\n\u201cThe Dark Side of Dataset Scaling: evaluating racial\nclassification in multimodal models,\u201d in FAccT, 2024.\n[65]\nJ. Brinkmann, P. Swoboda, and C. Bartelt, \u201cA multi-\ndimensional analysis of social biases in vision trans-\nformers,\u201d in ICCV, 2023.\n[66]\nE. Iofinova, A. Peste, and D. Alistarh, \u201cBias in pruned\nvision models: In-depth analysis and countermea-\nsures,\u201d in CVPR, 2023.\n[67]\nD. Guilbeault, S. Delecourt, T. Hull, B. S. Desikan,\nM. Chu, and E. Nadler, \u201cOnline images amplify gen-\nder bias,\u201d Nature, vol. 626, no. 8001, pp. 1049\u20131055,\n2024.\n[68]\nB. Kim, H. Kim, K. Kim, S. Kim, and J. Kim, \u201cLearning\nnot to learn: Training deep neural networks with\nbiased data,\u201d in CVPR, 2019.\n[69]\nD. Zietlow, M. Lohaus, G. Balakrishnan, M. Kleindess-\nner, F. Locatello, B. Sch\u00a8olkopf, and C. Russell, \u201cLevel-\ning down in computer vision: Pareto inefficiencies in\nfair deep classifiers,\u201d in CVPR, 2022.\n[70]\nH. C. Bendekgey and E. Sudderth, \u201cScalable and\nstable surrogates for flexible classifiers with fairness\nconstraints,\u201d NeurIPS, 2021.\n[71]\nS. Lee, Z. J. Wang, J. Hoffman, and D. H. P. Chau, \u201cVis-\nCUIT: visual auditor for bias in cnn image classifier,\u201d\nin CVPR, 2022.\n[72]\nS. Jung, S. Chun, and T. Moon, \u201cLearning fair classi-\nfiers with partially annotated group labels,\u201d in CVPR,\n2022.\n[73]\nG. Zhang, Y. Zhang, Y. Zhang, W. Fan, Q. Li, S. Liu,\nand S. Chang, \u201cFairness reprogramming,\u201d NeurIPS,\n2022.\n[74]\nP. C. Roy and V. N. Boddeti, \u201cMitigating informa-\ntion leakage in image representations: A maximum\nentropy approach,\u201d in CVPR, 2019.\n[75]\nB. Sadeghi, R. Yu, and V. Boddeti, \u201cOn the global\noptima of kernelized adversarial representation learn-\ning,\u201d in ICCV, 2019.\n[76]\nL. Gustafson, C. Rolland, N. Ravi, Q. Duval, A. Ad-\ncock, C.-Y. Fu, M. Hall, and C. Ross, \u201cFACET: fairness\nin computer vision evaluation benchmark,\u201d in ICCV,\n2023.\n[77]\nK. K. Singh, D. Mahajan, K. Grauman, Y. J. Lee,\nM. Feiszli, and D. Ghadiyaram, \u201cDon\u2019t judge an object\nby its context: Learning to overcome contextual bias,\u201d\nin CVPR, 2020.\n[78]\nM.-C. Chiu, P.-Y. Chen, and X. Ma, \u201cBetter may not\nbe fairer: A study on subgroup discrepancy in image\nclassification,\u201d in ICCV, 2023.\n[79]\nM. Jia, L. Tang, B.-C. Chen, C. Cardie, S. Belongie,\nB. Hariharan, and S.-N. Lim, \u201cVisual prompt tuning,\u201d\nin ECCV, 2022.\n[80]\nZ. Li and C. Xu, \u201cDiscover the unknown biased at-\ntribute of an image classifier,\u201d in ICCV, 2021.\n[81]\nJ. Choi, C. Gao, J. C. Messou, and J.-B. Huang, \u201cWhy\ncan\u2019t I dance in the mall? Learning to mitigate scene\nbias in action recognition,\u201d NeurIPS, 2019.\n[82]\nY. Zhai, Z. Liu, Z. Wu, Y. Wu, C. Zhou, D. Doermann,\nJ. Yuan, and G. Hua, \u201cSOAR: scene-debiasing open-set\naction recognition,\u201d in ICCV, 2023.\n[83]\nR.\nVera-Rodriguez,\nM.\nBlazquez,\nA.\nMorales,\nE. Gonzalez-Sosa, J. C. Neves, and H. Proenc\u00b8a, \u201cFace-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n15\nGenderID: exploiting gender information in dcnns\nface recognition systems,\u201d in CVPRW, 2019.\n[84]\nN. Quadrianto, V. Sharmanska, and O. Thomas, \u201cDis-\ncovering fair representations in the data domain,\u201d in\nCVPR, 2019.\n[85]\nA. Domnich and G. Anbarjafari, \u201cResponsible AI:\ngender bias assessment in emotion recognition,\u201d arXiv\npreprint arXiv:2103.11436, 2021.\n[86]\nP. Dhar, J. Gleason, A. Roy, C. D. Castillo, and R. Chel-\nlappa, \u201cPass: protected attribute suppression system\nfor mitigating bias in face recognition,\u201d in ICCV, 2021.\n[87]\nS. Gong, X. Liu, and A. K. Jain, \u201cMitigating face recog-\nnition bias via group adaptive classifier,\u201d in CVPR,\n2021.\n[88]\nJ. Ma, Z. Yue, K. Tomoyuki, S. Tomoki, K. Jayashree,\nS. Pranata, and H. Zhang, \u201cInvariant feature regular-\nization for fair face recognition,\u201d in ICCV, 2023.\n[89]\nH. Liang, P. Perona, and G. Balakrishnan, \u201cBench-\nmarking algorithmic bias in face recognition: An ex-\nperimental approach using synthetic faces and human\nevaluation,\u201d in ICCV, 2023.\n[90]\nS. Dooley, R. Sukthanker, J. Dickerson, C. White,\nF. Hutter, and M. Goldblum, \u201cRethinking Bias Miti-\ngation: fairer architectures make for fairer face recog-\nnition,\u201d NeurIPS, 2024.\n[91]\nY. Chen and J. Joo, \u201cUnderstanding and mitigating\nannotation bias in facial expression recognition,\u201d in\nICCV, 2021.\n[92]\nA. Chouldechova, S. Deng, Y. Wang, W. Xia, and\nP. Perona, \u201cUnsupervised and semi-supervised bias\nbenchmarking in face recognition,\u201d in ECCV, 2022.\n[93]\nP. Terh\u00a8orst, J. N. Kolf, M. Huber, F. Kirchbuchner,\nN. Damer, A. M. Moreno, J. Fierrez, and A. Kuijper,\n\u201cA comprehensive study on face recognition biases\nbeyond demographics,\u201d IEEE Transactions on Technol-\nogy and Society, vol. 3, no. 1, pp. 16\u201330, 2021.\n[94]\nM. Georgopoulos, J. Oldfield, M. A. Nicolaou, Y. Pana-\ngakis, and M. Pantic, \u201cMitigating demographic bias in\nfacial datasets with style-based multi-attribute trans-\nfer,\u201d International Journal of Computer Vision, vol. 129,\nno. 7, pp. 2288\u20132307, 2021.\n[95]\nJ. Li and W. Abd-Almageed, \u201cCAT: controllable at-\ntribute translation for fair facial attribute classifica-\ntion,\u201d in ECCV, 2022.\n[96]\nS. Gong, X. Liu, and A. K. Jain, \u201cJointly de-biasing face\nrecognition and demographic attribute estimation,\u201d in\nECCV, 2020.\n[97]\nM. Wang and W. Deng, \u201cMitigating bias in face recog-\nnition using skewness-aware reinforcement learning,\u201d\nin CVPR, 2020.\n[98]\nB. Liu, W. Deng, Y. Zhong, M. Wang, J. Hu, X. Tao, and\nY. Huang, \u201cFair Loss: margin-aware reinforcement\nlearning for deep face recognition,\u201d in ICCV, 2019.\n[99]\nG. Balakrishnan, Y. Xiong, W. Xia, and P. Perona,\n\u201cTowards causal benchmarking of biasin face analysis\nalgorithms,\u201d Deep Learning-Based Face Analytics, pp.\n327\u2013359, 2021.\n[100] V. H. Maluleke, N. Thakkar, T. Brooks, E. Weber,\nT. Darrell, A. A. Efros, A. Kanazawa, and D. Guillory,\n\u201cStudying bias in gans through the lens of race,\u201d in\nECCV, 2022.\n[101] S. Tan, Y. Shen, and B. Zhou, \u201cImproving the fairness\nof deep generative models without retraining,\u201d arXiv\npreprint arXiv:2012.04842, 2020.\n[102] C. H. Wu, S. Motamed, S. Srivastava, and F. D. De la\nTorre, \u201cGenerative visual prompt: Unifying distri-\nbutional control of pre-trained generative models,\u201d\nNeurIPS, 2022.\n[103] N. Yu, K. Li, P. Zhou, J. Malik, L. Davis, and M. Fritz,\n\u201cInclusive GAN: improving data and minority cover-\nage in generative models,\u201d in ECCV, 2020.\n[104] S. Zhao, H. Ren, A. Yuan, J. Song, N. Goodman, and\nS. Ermon, \u201cBias and generalization in deep generative\nmodels: An empirical study,\u201d NeurIPS, 2018.\n[105] D. Xu, S. Yuan, L. Zhang, and X. Wu, \u201cFairGAN:\nFairness-aware generative adversarial networks,\u201d in\nIEEE BigData, 2018.\n[106] C. E. Karakas, A. Dirik, E. Yalc\u00b8\u0131nkaya, and P. Ya-\nnardag, \u201cFairStyle: debiasing stylegan2 with style\nchannel manipulations,\u201d in ECCV.\nSpringer, 2022.\n[107] K. Choi, A. Grover, T. Singh, R. Shu, and S. Ermon,\n\u201cFair generative modeling via weak supervision,\u201d in\nICML, 2020.\n[108] A. Jalal, S. Karmalkar, J. Hoffmann, A. Dimakis, and\nE. Price, \u201cFairness for image generation with uncer-\ntain sensitive attributes,\u201d in ICML, 2021.\n[109] P.\nJ.\nKenfack,\nK.\nSabbagh,\nA.\nR.\nRivera,\nand\nA. Khan, \u201cRepFair-GAN: mitigating representation\nbias in gans using gradient clipping,\u201d arXiv preprint\narXiv:2207.10653, 2022.\n[110] S.\nSudhakar,\nV.\nPrabhu,\nO.\nRussakovsky,\nand\nJ. Hoffman, \u201cICON2: reliably benchmarking pre-\ndictive inequity in object detection,\u201d arXiv preprint\narXiv:2306.04482, 2023.\n[111] B. Wilson, J. Hoffman, and J. Morgenstern, \u201cPre-\ndictive inequity in object detection,\u201d arXiv preprint\narXiv:1902.11097, 2019.\n[112] F. Kong, S. Yuan, W. Hao, and R. Henao, \u201cMitigating\ntest-time bias for fair image retrieval,\u201d NeurIPS, 2024.\n[113] S. Yenamandra, P. Ramesh, V. Prabhu, and J. Hoffman,\n\u201cFACTS: first amplify correlations and then slice to\ndiscover bias,\u201d in ICCV, 2023.\n[114] B. Qiu, H. Li, H. Wen, H. Qiu, L. Wang, F. Meng,\nQ. Wu, and L. Pan, \u201cCafeBoost: causal feature boost\nto eliminate task-induced bias for class incremental\nlearning,\u201d in CVPR, 2023.\n[115] S. Chu, D. Kim, and B. Han, \u201cLearning debiased and\ndisentangled representations for semantic segmenta-\ntion,\u201d NeurIPS, 2021.\n[116] N. Garcia, Y. Hirota, Y. Wu, and Y. Nakashima, \u201cUn-\ncurated image-text datasets: Shedding light on demo-\ngraphic bias,\u201d in CVPR, 2023.\n[117] B. A. Biswas and Q. Ji, \u201cProbabilistic debiasing of\nscene graphs,\u201d in CVPR, 2023.\n[118] K. Tang, Y. Niu, J. Huang, J. Shi, and H. Zhang, \u201cUn-\nbiased scene graph generation from biased training,\u201d\nin CVPR, 2020.\n[119] J. Li, Y. Wong, Q. Zhao, and M. S. Kankanhalli, \u201cDual-\nglance model for deciphering social relationships,\u201d in\nICCV, 2017.\n[120] I. Krasin, T. Duerig, N. Alldrin, V. Ferrari, S. Abu-El-\nHaija, A. Kuznetsova, H. Rom, J. Uijlings, S. Popov,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n16\nA. Veit et al., \u201cOpenImages: a public dataset for large-\nscale multi-label and multi-class image classification,\u201d\nDataset\navailable\nfrom\nhttps://github.com/openimages,\nvol. 2, no. 3, p. 18, 2017.\n[121] Z. Liu, P. Luo, X. Wang, and X. Tang, \u201cDeep learning\nface attributes in the wild,\u201d in ICCV, 2015.\n[122] U. Schimmack, \u201cThe implicit association test: A\nmethod in search of a construct,\u201d Perspectives on Psy-\nchological Science, vol. 16, no. 2, pp. 396\u2013414, 2021.\n[123] S. Sagawa, P. W. Koh, T. B. Hashimoto, and P. Liang,\n\u201cDistributionally robust neural networks for group\nshifts: On the importance of regularization for worst-\ncase generalization,\u201d arXiv preprint arXiv:1911.08731,\n2019.\n[124] X. Zhang, Y. He, R. Xu, H. Yu, Z. Shen, and P. Cui,\n\u201cNICO++: towards better benchmarking for domain\ngeneralization,\u201d in CVPR, 2023.\n[125] A. Krizhevsky, G. Hinton et al., \u201cLearning multiple\nlayers of features from tiny images,\u201d Toronto, ON,\nCanada, 2009.\n[126] D. Hendrycks and T. Dietterich, \u201cBenchmarking neu-\nral network robustness to common corruptions and\nperturbations,\u201d in ICLR, 2019.\n[127] J. Nam, H. Cha, S. Ahn, J. Lee, and J. Shin, \u201cLearning\nfrom failure: De-biasing classifier from biased classi-\nfier,\u201d NeurIPS, 2020.\n[128] T. Karras, S. Laine, and T. Aila, \u201cA style-based genera-\ntor architecture for generative adversarial networks,\u201d\nin CVPR, 2019.\n[129] R. Rothe, R. Timofte, and L. Van Gool, \u201cDeep ex-\npectation of real and apparent age from a single\nimage without facial landmarks,\u201d International Journal\nof Computer Vision, vol. 126, no. 2, pp. 144\u2013157, 2018.\n[130] Z.\nZhang,\nY.\nSong,\nand\nH.\nQi,\n\u201cAge\nprogres-\nsion/regression by conditional adversarial autoen-\ncoder,\u201d in CVPR, 2017.\n[131] Y. He, Z. Shen, and P. Cui, \u201cTowards non-iid image\nclassification: A dataset and baselines,\u201d Pattern Recog-\nnition, vol. 110, p. 107383, 2021.\n[132] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and\nD. Song, \u201cNatural adversarial examples,\u201d in CVPR,\n2021.\n[133] M. Yurochkin, A. Bower, and Y. Sun, \u201cTraining in-\ndividually fair ml models with sensitive subspace\nrobustness,\u201d ICLR, 2020.\n[134] G. B. Huang, M. Ramesh, T. Berg, and E. Learned-\nMiller, \u201cLabeled faces in the wild: A database for\nstudying face recognition in unconstrained environ-\nments,\u201d University of Massachusetts, Amherst, Tech.\nRep. 07-49, October 2007.\n[135] W. A. G. Rojas, S. Diamos, K. R. Kini, D. Kanter, V. J.\nReddi, and C. Coleman, \u201cThe dollar street dataset: Im-\nages representing the geographic and socioeconomic\ndiversity of the world,\u201d in NeurIPS, 2022.\n[136] V. V. Ramaswamy, S. Y. Lin, D. Zhao, A. Adcock,\nL. van der Maaten, D. Ghadiyaram, and O. Rus-\nsakovsky, \u201cGeoDE: a geographically diverse evalua-\ntion dataset for object recognition,\u201d NeurIPS, 2024.\n[137] M. Arjovsky, L. Bottou, I. Gulrajani, and D. Lopez-\nPaz, \u201cInvariant risk minimization,\u201d arXiv preprint\narXiv:1907.02893, 2019.\n[138] W. Cukierski, \u201cDogs vs. cats,\u201d 2013. [Online]. Avail-\nable: https://kaggle.com/competitions/dogs-vs-cats\n[139] F. Wang, L. Chen, C. Li, S. Huang, Y. Chen, C. Qian,\nand C. C. Loy, \u201cThe devil of face recognition is in the\nnoise,\u201d in ECCV, 2018.\n[140] H. Caesar, J. Uijlings, and V. Ferrari, \u201cCOCO-Stuff:\nthing and stuff classes in context,\u201d in CVPR, 2018.\n[141] J. Peyre, I. Laptev, C. Schmid, and J. Sivic, \u201cWeakly-\nsupervised learning of visual relations,\u201d in ICCV,\n2017.\n[142] Z. Liu, P. Luo, S. Qiu, X. Wang, and X. Tang, \u201cDeep-\nFashion: powering robust clothes recognition and re-\ntrieval with rich annotations,\u201d in CVPR, 2016.\n[143] Y. Xian, C. H. Lampert, B. Schiele, and Z. Akata,\n\u201cZero-shot learning\u2014a comprehensive evaluation of\nthe good, the bad and the ugly,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 41, no. 9,\npp. 2251\u20132265, 2018.\n[144] S. Escalera, X. Bar\u00b4o, H. J. Escalante, and I. Guyon,\n\u201cChaLearn looking at people: A review of events and\nresources,\u201d in IJCNN, 2017.\n[145] J. Angwin, J. Larson, S. Mattu, and L. Kirchner,\n\u201cThere\u2019s software used across the country to predict\nfuture criminals and it\u2019s biased against blacks.\u201d 2020.\n[146] A. Georghiades, P. Belhumeur, and D. Kriegman,\n\u201cFrom few to many: illumination cone models for\nface recognition under variable lighting and pose,\u201d\nIEEE Transactions on Pattern Analysis and Machine In-\ntelligence, vol. 23, no. 6, pp. 643\u2013660, 2001.\n[147] D. S. Ma, J. Correll, and B. Wittenbrink, \u201cThe chicago\nface database: A free stimulus set of faces and norm-\ning data,\u201d Behavior Research Methods, vol. 47, pp. 1122\u2013\n1135, 2015.\n[148] K. K\u00a8arkk\u00a8ainen and J. Joo, \u201cFairFace: face attribute\ndataset for balanced race, gender, and age,\u201d arXiv\npreprint arXiv:1908.04913, 2019.\n[149] K. Soomro, A. R. Zamir, and M. Shah, \u201cUCF101: a\ndataset of 101 human actions classes from videos in\nthe wild,\u201d arXiv preprint arXiv:1212.0402, 2012.\n[150] H. Kuehne, H. Jhuang, E. Garrote, T. Poggio, and\nT. Serre, \u201cHmdb: A large video database for human\nmotion recognition,\u201d in ICCV, 2011.\n[151] Y. Li, Y. Li, and N. Vasconcelos, \u201cRESOUND: towards\naction recognition without representation bias,\u201d in\nECCV, 2018.\n[152] Y.-G. Jiang, J. Liu, A. Roshan Zamir, G. Toderici,\nI. Laptev, M. Shah, and R. Sukthankar, \u201cTHUMOS\nchallenge: Action recognition with a large number of\nclasses,\u201d http://crcv.ucf.edu/THUMOS14/, 2014.\n[153] H. Jhuang, J. Gall, S. Zuffi, C. Schmid, and M. J.\nBlack, \u201cTowards understanding action recognition,\u201d\nin ICCV, 2013.\n[154] M. Monfort, A. Andonian, B. Zhou, K. Ramakrishnan,\nS. A. Bargal, T. Yan, L. Brown, Q. Fan, D. Gutfreund,\nC. Vondrick et al., \u201cMoments in time dataset: one\nmillion videos for event understanding,\u201d IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence,\nvol. 42, no. 2, pp. 502\u2013508, 2019.\n[155] B. F. Klare, B. Klein, E. Taborsky, A. Blanton, J. Ch-\neney, K. Allen, P. Grother, A. Mah, and A. K. Jain,\n\u201cPushing the frontiers of unconstrained face detection\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n17\nand recognition: Iarpa janus benchmark a,\u201d in CVPR,\n2015.\n[156] E. Eidinger, R. Enbar, and T. Hassner, \u201cAge and gen-\nder estimation of unfiltered faces,\u201d IEEE Transactions\non Information Forensics and Security, vol. 9, no. 12, pp.\n2170\u20132179, 2014.\n[157] M. Merler, N. Ratha, R. S. Feris, and J. R. Smith,\n\u201cDiversity in faces,\u201d arXiv preprint arXiv:1901.10436,\n2019.\n[158] L. Wolf, T. Hassner, and I. Maoz, \u201cFace recognition\nin unconstrained videos with matched background\nsimilarity,\u201d in CVPR, 2011.\n[159] A. Nech and I. Kemelmacher-Shlizerman, \u201cLevel play-\ning field for million scale face recognition,\u201d in CVPR,\n2017.\n[160] B. Maze, J. Adams, J. A. Duncan, N. Kalka, T. Miller,\nC. Otto, A. K. Jain, W. T. Niggel, J. Anderson, J. Ch-\neney, and P. Grother, \u201cIARPA Janus Benchmark-C:\nface dataset and protocol,\u201d in ICB, 2018.\n[161] M. Wang, W. Deng, J. Hu, X. Tao, and Y. Huang,\n\u201cRacial faces in the wild: Reducing racial bias by infor-\nmation maximization adaptation network,\u201d in ICCV,\n2019.\n[162] J. Deng, J. Guo, X. An, Z. Zhu, and S. Zafeiriou,\n\u201cMasked face recognition challenge: The insightface\ntrack report,\u201d in ICCV, 2021.\n[163] C. Whitelam, E. Taborsky, A. Blanton, B. Maze,\nJ. Adams, T. Miller, N. Kalka, A. K. Jain, J. A. Dun-\ncan, K. Allen et al., \u201cIARPA Janus Benchmark-B Face\nDataset,\u201d in CVPRW, 2017.\n[164] G. Bae, M. de La Gorce, T. Baltru\u02c7saitis, C. Hewitt,\nD. Chen, J. Valentin, R. Cipolla, and J. Shen, \u201cDigiFace-\n1M: 1 million digital face images for face recognition,\u201d\nin WACV, 2023.\n[165] Q. Cao, L. Shen, W. Xie, O. M. Parkhi, and A. Zis-\nserman, \u201cVGGFace2: a dataset for recognising faces\nacross pose and age,\u201d in FG, 2018.\n[166] D. Lundqvist, A. Flykt, and A. \u00a8Ohman, \u201cKarolinska\ndirected emotional faces,\u201d PsycTESTS Dataset, vol. 91,\np. 630, 1998.\n[167] Z. Zhang, P. Luo, C.-C. Loy, and X. Tang, \u201cLearning\nsocial relation traits from face images,\u201d in ICCV, 2015.\n[168] S. Li, W. Deng, and J. Du, \u201cReliable crowdsourcing\nand deep locality-preserving learning for expression\nrecognition in the wild,\u201d in CVPR, 2017.\n[169] A. Mollahosseini, B. Hasani, and M. H. Mahoor, \u201cAf-\nfectNet: a database for facial expression, valence, and\narousal computing in the wild,\u201d IEEE Transactions on\nAffective Computing, vol. 10, no. 1, pp. 18\u201331, 2017.\n[170] K. Ricanek and T. Tesafaye, \u201cMORPH: a longitudinal\nimage database of normal adult age-progression,\u201d in\nFG, 2006.\n[171] P. Terh\u00a8orst, D. F\u00a8ahrmann, J. N. Kolf, N. Damer,\nF. Kirchbuchner, and A. Kuijper, \u201cMAAD-Face: a mas-\nsively annotated attribute dataset for face images,\u201d\nIEEE Transactions on Information Forensics and Security,\nvol. 16, pp. 3942\u20133957, 2021.\n[172] B.-C. Chen, C.-S. Chen, and W. H. Hsu, \u201cCross-age\nreference coding for age-invariant face recognition\nand retrieval,\u201d in ECCV, 2014.\n[173] M. Georgopoulos, Y. Panagakis, and M. Pantic, \u201cIn-\nvestigating bias in deep face analysis: The kanface\ndataset and empirical study,\u201d Image and Vision Com-\nputing, vol. 102, p. 103954, 2020.\n[174] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcFace: ad-\nditive angular margin loss for deep face recognition,\u201d\nin CVPR, 2019.\n[175] Y. Guo, L. Zhang, Y. Hu, X. He, and J. Gao, \u201cMs-\nceleb-1m: A dataset and benchmark for large-scale\nface recognition,\u201d in ECCV.\nSpringer, 2016, pp. 87\u2013\n102.\n[176] Q. Meng, S. Zhao, Z. Huang, and F. Zhou, \u201cMagface:\nA universal representation for face recognition and\nquality assessment,\u201d in CVPR, 2021, pp. 14 225\u201314 234.\n[177] S. Sengupta, J.-C. Chen, C. Castillo, V. M. Patel,\nR. Chellappa, and D. W. Jacobs, \u201cFrontal to profile face\nverification in the wild,\u201d in WACV, 2016.\n[178] M. Kay, C. Matuszek, and S. A. Munson, \u201cUnequal\nrepresentation and gender stereotypes in image search\nresults for occupations,\u201d in ACM CHI, 2015.\n[179] L. E. Celis and V. Keswani, \u201cImplicit diversity in\nimage summarization,\u201d Proceedings of the ACM on\nHuman-Computer Interaction, vol. 4, no. CSCW2, pp.\n1\u201328, 2020.\n[180] B. A. Plummer, L. Wang, C. M. Cervantes, J. C.\nCaicedo, J. Hockenmaier, and S. Lazebnik, \u201cFlickr30k\nEntities: Collecting region-to-phrase correspondences\nfor richer image-to-sentence models,\u201d in ICCV, 2015.\n[181] F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu,\nV. Madhavan, and T. Darrell, \u201cBDD100K: a diverse\ndriving dataset for heterogeneous multitask learn-\ning,\u201d in CVPR, 2020.\n[182] Q. Yang, A. Wu, and W.-S. Zheng, \u201cPerson re-\nidentification by contour sketch under moderate\nclothing change,\u201d IEEE Transactions on Pattern Analysis\nand Machine Intelligence, vol. 43, no. 6, pp. 2029\u20132046,\n2019.\n[183] X. Qian, W. Wang, L. Zhang, F. Zhu, Y. Fu, T. Xiang,\nY.-G. Jiang, and X. Xue, \u201cLong-term cloth-changing\nperson re-identification,\u201d in ACCV, 2020.\n[184] S. Bhargava and D. Forsyth, \u201cExposing and correcting\nthe gender bias in image captioning datasets and\nmodels,\u201d arXiv preprint arXiv:1912.00578, 2019.\n[185] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra,\nC. L. Zitnick, and D. Parikh, \u201cVQA: Visual Question\nAnswering,\u201d in ICCV, 2015.\n[186] V. Manjunatha, N. Saini, and L. S. Davis, \u201cExplicit bias\ndiscovery in visual question answering models,\u201d in\nCVPR, 2019.\n[187] A. Agrawal, D. Batra, D. Parikh, and A. Kembhavi,\n\u201cDon\u2019t just assume; look and answer: Overcoming\npriors for visual question answering,\u201d in CVPR, 2018.\n[188] G. Kv and A. Mittal, \u201cReducing language biases in\nvisual question answering with visually-grounded\nquestion encoder,\u201d in ECCV, 2020.\n[189] Y. Goyal, T. Khot, D. Summers-Stay, D. Batra, and\nD. Parikh, \u201cMaking the V in VQA matter: Elevating\nthe role of image understanding in visual question\nanswering,\u201d in CVPR, 2017.\n[190] S. Park, S. Hwang, J. Hong, and H. Byun, \u201cFair-VQA:\nfairness-aware visual question answering through\nsensitive attribute prediction,\u201d IEEE Access, vol. 8, pp.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n18\n215 091\u2013215 099, 2020.\n[191] R. R. Selvaraju, P. Tendulkar, D. Parikh, E. Horvitz,\nM. T. Ribeiro, B. Nushi, and E. Kamar, \u201cSquinting at\nVQA models: Introspecting VQA models with sub-\nquestions,\u201d in CVPR, 2020.\n[192] V. Agarwal, R. Shetty, and M. Fritz, \u201cTowards causal\nVQA: Revealing and reducing spurious correlations\nby invariant and covariant semantic editing,\u201d in\nCVPR, 2020.\n[193] C. Kervadec, G. Antipov, M. Baccouche, and C. Wolf,\n\u201cRoses are red, violets are blue... but should vqa\nexpect them to?\u201d in CVPR, 2021.\n[194] C. Dancette, R. Cadene, D. Teney, and M. Cord, \u201cBe-\nyond question-based biases: Assessing multimodal\nshortcut learning in visual question answering,\u201d in\nICCV, 2021.\n[195] Y. Zhu, O. Groth, M. Bernstein, and L. Fei-Fei, \u201cVi-\nsual7W: grounded question answering in images,\u201d in\nCVPR, 2016.\n[196] K. Marino, M. Rastegari, A. Farhadi, and R. Mottaghi,\n\u201cOK-VQA: a visual question answering benchmark\nrequiring external knowledge,\u201d in CVPR, 2019.\n[197] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata,\nJ. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A.\nShamma et al., \u201cVisual Genome: connecting language\nand vision using crowdsourced dense image annota-\ntions,\u201d International Journal of Computer Vision, vol. 123,\npp. 32\u201373, 2017.\n[198] Y. Hirota, Y. Nakashima, and N. Garcia, \u201cGender and\nracial bias in visual question answering datasets,\u201d in\nFAccT, 2022.\n[199] C. Zhang, X. Chen, S. Chai, C. H. Wu, D. Lagun,\nT. Beeler, and F. De la Torre, \u201cITI-GEN: inclusive text-\nto-image generation,\u201d in ICCV, 2023.\n[200] H. Feng, T. Bolkart, J. Tesch, M. J. Black, and V. Abre-\nvaya, \u201cTowards racially unbiased skin tone estimation\nvia scene disambiguation,\u201d in ECCV, 2022.\n[201] A. Torralba and A. A. Efros, \u201cUnbiased look at dataset\nbias,\u201d in CVPR, 2011.\n[202] R.\nGoyal,\nS.\nEbrahimi\nKahou,\nV.\nMichalski,\nJ. Materzynska, S. Westphal, H. Kim, V. Haenel, I. Fru-\nend, P. Yianilos, M. Mueller-Freitag et al., \u201cThe \u201dsome-\nthing something\u201d video database for learning and\nevaluating visual common sense,\u201d in ICCV, 2017.\n[203] J. Wang, Y. Liu, and X. E. Wang, \u201cAre gender-neutral\nqueries really gender-neutral? mitigating gender bias\nin image search,\u201d arXiv preprint arXiv:2109.05433,\n2021.\n[204] A. Birhane, V. Prabhu, S. Han, and V. N. Boddeti, \u201cOn\nhate scaling laws for data-swamps,\u201d arXiv preprint\narXiv:2306.13141, 2023.\n[205] A. Birhane, V. U. Prabhu, S. Han, V. N. Boddeti, and\nS. Luccioni, \u201cInto the LAION\u2019s Den: investigating hate\nin multimodal datasets,\u201d NeurIPS, 2023.\n[206] Y. Du, F. Wei, Z. Zhang, M. Shi, Y. Gao, and G. Li,\n\u201cLearning to prompt for open-vocabulary object de-\ntection with vision-language model,\u201d in CVPR, 2022.\n[207] M. Zhang and C. R\u00b4e, \u201cContrastive adapters for\nfoundation model group robustness,\u201d arXiv preprint\narXiv:2207.07180, 2022.\n[208] I. Misra, C. Lawrence Zitnick, M. Mitchell, and R. Gir-\nshick, \u201cSeeing through the human reporting bias: Vi-\nsual classifiers from noisy human-centric labels,\u201d in\nCVPR, 2016.\n[209] T. De Vries, I. Misra, C. Wang, and L. Van der\nMaaten, \u201cDoes object recognition work for everyone?\u201d\nin CVPRW, 2019.\n[210] M. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba,\n\u201cCoding facial expressions with gabor wavelets,\u201d in\nFG, 1998.\n[211] M. J. Lyons, \u201c\u201dexcavating ai\u201d Re-excavated: Debunk-\ning a fallacious account of the JAFFE dataset,\u201d arXiv\npreprint arXiv:2107.13998, 2021.\n[212] S. Singh and S. Benedict, \u201cIndian semi-acted facial\nexpression (iSAFE) dataset for human emotions recog-\nnition,\u201d in SIRS, 2020.\n[213] A. Castelnovo, R. Crupi, G. Greco, D. Regoli, I. G.\nPenco, and A. C. Cosentini, \u201cA clarification of the\nnuances in the fairness metrics landscape,\u201d Scientific\nReports, vol. 12, no. 1, p. 4209, 2022.\n[214] W. Fleisher, \u201cWhat\u2019s fair about individual fairness?\u201d\nin AIES, 2021.\n[215] C. Dwork, M. Hardt, T. Pitassi, O. Reingold, and\nR. Zemel, \u201cFairness through awareness,\u201d in ITCS,\n2012.\n[216] M. Hardt, E. Price, and N. Srebro, \u201cEquality of oppor-\ntunity in supervised learning,\u201d NeurIPS, 2016.\n[217] M. J. Kusner, J. Loftus, C. Russell, and R. Silva, \u201cCoun-\nterfactual fairness,\u201d NeurIPS, 2017.\n[218] Z. Zuo, M. Khalili, and X. Zhang, \u201cCounterfactually\nfair representation,\u201d NeurIPS, 2023.\n[219] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and\nL. Fei-Fei, \u201cImageNet: a large-scale hierarchical image\ndatabase,\u201d in CVPR, 2009.\n[220] C. Schuhmann, R. Beaumont, R. Vencu, C. Gor-\ndon, R. Wightman, M. Cherti, T. Coombes, A. Katta,\nC. Mullis, M. Wortsman et al., \u201cLAION-5B: An open\nlarge-scale dataset for training next generation image-\ntext models,\u201d NeurIPS, 2022.\n[221] G. Ilharco, M. Wortsman, R. Wightman, C. Gor-\ndon, N. Carlini, R. Taori, A. Dave, V. Shankar,\nH. Namkoong, J. Miller, H. Hajishirzi, A. Farhadi, and\nL. Schmidt, \u201cOpenCLIP,\u201d 2021.\n[222] R.\nWightman,\n\u201cPytorch\nimage\nmodels,\u201d\nhttps:\n//github.com/rwightman/pytorch-image-models,\n2019.\n[223] R. Geirhos, P. Rubisch, C. Michaelis, M. Bethge, F. A.\nWichmann, and W. Brendel, \u201cImagenet-trained cnns\nare biased towards texture; increasing shape bias\nimproves accuracy and robustness,\u201d arXiv preprint\narXiv:1811.12231, 2018.\n[224] P. Zhang, Y. Goyal, D. Summers-Stay, D. Batra, and\nD. Parikh, \u201cYin and Yang: balancing and answering\nbinary visual questions,\u201d in CVPR, 2016.\n[225] A. Khosla, T. Zhou, T. Malisiewicz, A. A. Efros, and\nA. Torralba, \u201cUndoing the damage of dataset bias,\u201d in\nECCV, 2012.\n[226] R. Cadene, C. Dancette, M. Cord, D. Parikh et al.,\n\u201cRUBi: reducing unimodal biases for visual question\nanswering,\u201d NeurIPS, 2019.\n[227] H. Bahng, S. Chun, S. Yun, J. Choo, and S. J. Oh,\n\u201cLearning de-biased representations with biased rep-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n19\nresentations,\u201d in ICML, 2020.\n[228] K. Burns, L. A. Hendricks, K. Saenko, T. Darrell,\nand A. Rohrbach, \u201cWomen also snowboard: Over-\ncoming bias in captioning models,\u201d arXiv preprint\narXiv:1803.09797, 2018.\n[229] Y. Hirota, Y. Nakashima, and N. Garcia, \u201cModel-\nagnostic\ngender\ndebiased\nimage\ncaptioning,\u201d\nin\nCVPR, 2023.\n[230] D. Zhao, A. Wang, and O. Russakovsky, \u201cUnderstand-\ning and evaluating racial biases in image captioning,\u201d\nin ICCV, 2021.\n[231] Y. Hirota, Y. Nakashima, and N. Garcia, \u201cQuantifying\nsocietal bias amplification in image captioning,\u201d in\nCVPR, 2022.\n[232] P. Esposito, P. Atighehchian, A. Germanidis, and\nD. Ghadiyaram, \u201cMitigating stereotypical biases in\ntext to image generative systems,\u201d arXiv preprint\narXiv:2310.06904, 2023.\n[233] F. Friedrich, M. Brack, L. Struppek, D. Hintersdorf,\nP. Schramowski, S. Luccioni, and K. Kersting, \u201cFair\nDiffusion: instructing text-to-image generation mod-\nels on fairness,\u201d arXiv preprint arXiv:2302.10893, 2023.\n[234] R. He, C. Xue, H. Tan, W. Zhang, Y. Yu, S. Bai, and\nX. Qi, \u201cDebiasing text-to-image diffusion models,\u201d\narXiv preprint arXiv:2402.14577, 2024.\n[235] S. Luccioni, C. Akiki, M. Mitchell, and Y. Jernite,\n\u201cStable Bias: evaluating societal representations in\ndiffusion models,\u201d NeurIPS, 2024.\n[236] J. Cho, A. Zala, and M. Bansal, \u201cDALL-Eval: probing\nthe reasoning skills and social biases of text-to-image\ngeneration models,\u201d in ICCV, 2023.\n[237] H. Bansal, D. Yin, M. Monajatipoor, and K.-W. Chang,\n\u201cHow well can text-to-image generative models un-\nderstand ethical natural language interventions?\u201d\narXiv preprint arXiv:2210.15230, 2022.\n[238] J. Wang, X. G. Liu, Z. Di, Y. Liu, and X. E.\nWang, \u201cT2IAT: measuring valence and stereotypi-\ncal biases in text-to-image generation,\u201d arXiv preprint\narXiv:2306.00905, 2023.\n[239] A. Chinchure, P. Shukla, G. Bhatt, K. Salij, K. Hosana-\ngar, L. Sigal, and M. Turk, \u201cTIBET: identifying and\nevaluating biases in text-to-image generative models,\u201d\narXiv preprint arXiv:2312.01261, 2023.\n[240] N. Ruiz, Y. Li, V. Jampani, Y. Pritch, M. Rubinstein,\nand K. Aberman, \u201cDreamBooth: fine tuning text-to-\nimage diffusion models for subject-driven genera-\ntion,\u201d in CVPR, 2023.\n[241] Z. Liu, P. Schaldenbrand, B.-C. Okogwu, W. Peng,\nY. Yun, A. Hundt, J. Kim, and J. Oh, \u201cSCoFT: self-\ncontrastive fine-tuning for equitable image genera-\ntion,\u201d arXiv preprint arXiv:2401.08053, 2024.\n[242] A. Basu, R. V. Babu, and D. Pruthi, \u201cInspecting the\ngeographical representativeness of images from text-\nto-image models,\u201d in ICCV, 2023.\n[243] Y. Niu, K. Tang, H. Zhang, Z. Lu, X.-S. Hua, and J.-\nR. Wen, \u201cCounterfactual VQA: A cause-effect look at\nlanguage bias,\u201d in CVPR, 2021.\n[244] Z. Wen, G. Xu, M. Tan, Q. Wu, and Q. Wu, \u201cDebiased\nvisual question answering from feature and sample\nperspectives,\u201d NeurIPS, 2021.\n[245] J. W. Cho, D.-J. Kim, H. Ryu, and I. S. Kweon, \u201cGen-\nerative bias for robust visual question answering,\u201d in\nCVPR, 2023.\n[246] A. Basu, S. Addepalli, and R. V. Babu, \u201cRMLVQA: a\nmargin loss approach for visual question answering\nwith language biases,\u201d in CVPR, 2023.\n[247] V. Gupta, Z. Li, A. Kortylewski, C. Zhang, Y. Li,\nand A. Yuille, \u201cSwapMix: diagnosing and regularizing\nthe over-reliance on visual context in visual question\nanswering,\u201d in CVPR, 2022.\n[248] H. Berg, S. M. Hall, Y. Bhalgat, W. Yang, H. R.\nKirk, A. Shtedritski, and M. Bain, \u201cA prompt ar-\nray keeps the bias away: Debiasing vision-language\nmodels with adversarial learning,\u201d arXiv preprint\narXiv:2203.11933, 2022.\n[249] I. Alabdulmohsin, X. Wang, A. Steiner, P. Goyal,\nA. D\u2019Amour, and X. Zhai, \u201cCLIP the Bias: how use-\nful is balancing data in multimodal learning?\u201d arXiv\npreprint arXiv:2403.04547, 2024.\n[250] H. Phan, A. G. Wilson, and Q. Lei, \u201cControllable\nprompt tuning for balancing group distributional ro-\nbustness,\u201d arXiv preprint arXiv:2403.02695, 2024.\n[251] C. Cui, Y. Zhou, X. Yang, S. Wu, L. Zhang, J. Zou,\nand H. Yao, \u201cHolistic analysis of hallucination in GPT-\n4V(ision): bias and interference challenges,\u201d arXiv\npreprint arXiv:2311.03287, 2023.\n[252] H.\nEdwards\nand\nA.\nStorkey,\n\u201cCensoring\nrep-\nresentations\nwith\nan\nadversary,\u201d\narXiv\npreprint\narXiv:1511.05897, 2015.\n[253] Q. Xie, Z. Dai, Y. Du, E. Hovy, and G. Neubig,\n\u201cControllable invariance through adversarial feature\nlearning,\u201d NeurIPS, 2017.\n[254] D. Madras, E. Creager, T. Pitassi, and R. Zemel,\n\u201cLearning adversarially fair and transferable repre-\nsentations,\u201d in ICML, 2018.\n[255] E. Adeli, Q. Zhao, A. Pfefferbaum, E. V. Sullivan,\nL. Fei-Fei, J. C. Niebles, and K. M. Pohl, \u201cRepresenta-\ntion learning with statistical independence to mitigate\nbias,\u201d in WACV, 2021.\n[256] A. Gretton, O. Bousquet, A. Smola, and B. Sch\u00a8olkopf,\n\u201cMeasuring\nstatistical\ndependence\nwith\nhilbert-\nschmidt norms,\u201d in ALT, 2005.\n[257] B. Sadeghi, L. Wang, and V. N. Boddeti, \u201cAdversarial\nrepresentation learning with closed-form solvers,\u201d in\nECML PKDD, 2021.\n[258] A. K. Menon and R. C. Williamson, \u201cThe cost of\nfairness in binary classification,\u201d in FAccT, 2018.\n[259] H. Zhao and G. J. Gordon, \u201cInherent tradeoffs in learn-\ning fair representations,\u201d Journal of Machine Learning\nResearch, vol. 23, no. 57, pp. 1\u201326, 2022.\n[260] H. Wang, L. He, R. Gao, and F. Calmon, \u201cAleatoric\nand epistemic discrimination: Fundamental limits of\nfairness interventions,\u201d NeurIPS, 2024.\n[261] Y.-Y.\nXu,\nC.-S.\nLin,\nand\nY.-C.\nF.\nWang,\n\u201cBias-\neliminating augmentation learning for debiased fed-\nerated learning,\u201d in CVPR, 2023.\n[262] K. M. Collins, U. Bhatt, and A. Weller, \u201cEliciting and\nlearning with soft labels from every annotator,\u201d in\nHCOMP, 2022.\n[263] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M\u00a8uller,\nH. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel\net al., \u201cScaling rectified flow transformers for high-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE (UNDER REVIEW)\n20\nresolution image synthesis,\u201d in ICML, 2024.\n[264] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan,\nand S. Ganguli, \u201cDeep unsupervised learning using\nnonequilibrium thermodynamics,\u201d in ICML, 2015.\n[265] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion\nprobabilistic models,\u201d NeurIPS, 2020.\n[266] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and\nM. Chen, \u201cHierarchical text-conditional image genera-\ntion with clip latents,\u201d arXiv preprint arXiv:2204.06125,\n2022.\n[267] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss,\nA. Radford, M. Chen, and I. Sutskever, \u201cZero-shot\ntext-to-image generation,\u201d in ICML, 2021.\n[268] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, \u201cHigh-resolution image synthesis with\nlatent diffusion models,\u201d in CVPR, 2022.\n[269] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding condi-\ntional control to text-to-image diffusion models,\u201d in\nICCV, 2023.\n[270] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dock-\nhorn, J. M\u00a8uller, J. Penna, and R. Rombach, \u201cSDXL:\nimproving latent diffusion models for high-resolution\nimage synthesis,\u201d arXiv preprint arXiv:2307.01952,\n2023.\n[271] C. Wu and F. De la Torre, \u201cContrastive prompts\nimprove disentanglement in text-to-image diffusion\nmodels,\u201d arXiv preprint arXiv:2402.13490, 2024.\n[272] P. Dhariwal and A. Nichol, \u201cDiffusion models beat\ngans on image synthesis,\u201d NeurIPS, 2021.\n[273] X. Liu, D. H. Park, S. Azadi, G. Zhang, A. Chopikyan,\nY. Hu, H. Shi, A. Rohrbach, and T. Darrell, \u201cMore con-\ntrol for free! image synthesis with semantic diffusion\nguidance,\u201d in WACV, 2023.\n[274] A.\nNichol,\nP.\nDhariwal,\nA.\nRamesh,\nP.\nShyam,\nP. Mishkin, B. McGrew, I. Sutskever, and M. Chen,\n\u201cGLIDE: towards photorealistic image generation and\nediting with text-guided diffusion models,\u201d arXiv\npreprint arXiv:2112.10741, 2021.\n[275] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark\net al., \u201cLearning transferable visual models from nat-\nural language supervision,\u201d in ICML, 2021.\n[276] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,\nY.\nHasson,\nK.\nLenc,\nA.\nMensch,\nK.\nMillican,\nM. Reynolds et al., \u201cFlamingo: a visual language\nmodel for few-shot learning,\u201d NeurIPS, 2022.\n[277] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,\nL. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y.\nLo et al., \u201cSegment anything,\u201d in ICCV, 2023.\n[278] S. Agarwal, G. Krueger, J. Clark, A. Radford, J. W.\nKim, and M. Brundage, \u201cEvaluating CLIP: towards\ncharacterization of broader capabilities and down-\nstream implications,\u201d arXiv preprint arXiv:2108.02818,\n2021.\n[279] G. Ruggeri, D. Nozza et al., \u201cA multi-dimensional\nstudy on bias in vision-language models,\u201d in ACL,\n2023.\n[280] X. Zhai, B. Mustafa, A. Kolesnikov, and L. Beyer,\n\u201cSigmoid loss for language image pre-training,\u201d in\nICCV, 2023.\n[281] Y.-F. Zhang, W. Yu, Q. Wen, X. Wang, Z. Zhang,\nL. Wang, R. Jin, and T. Tan, \u201cDebiasing large visual\nlanguage models,\u201d arXiv preprint arXiv:2403.05262,\n2024.\n[282] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya,\nF. L. Aleman, D. Almeida, J. Altenschmidt, S. Alt-\nman, S. Anadkat et al., \u201cGPT-4 technical report,\u201d arXiv\npreprint arXiv:2303.08774, 2023.\n[283] H. Liu, C. Li, Q. Wu, and Y. J. Lee, \u201cVisual instruction\ntuning,\u201d NeurIPS, 2023.\nSepehr Dehdashtian is a Ph.D. student in the\nDepartment of Computer Science and Engineer-\ning at Michigan State University. He received a\nMSc degree in Electrical Engineering from Sharif\nUniversity of Technology, Iran. His research in-\nterests are in Responsible AI and Fairness in\nMultimodal and Generative models.\nRuozhen (Catherine) He is a Ph.D. Student in\nthe Department of Computer Science at Rice\nUniversity. She received a BSc degree (First\nClass Honors) from the City University of Hong\nKong. Her primary research interests lie in com-\nputer vision, focusing on efficient algorithms for\nmultimodal models under limited supervision.\nYi Li is a Ph.D. candidate in the Department of\nElectrical and Computer Engineering at the Uni-\nversity of California San Diego. He received his\nB.Eng. degree (First Class Honors) from the Chi-\nnese University of Hong Kong. His research in-\nterests include representation learning and bias\nmitigation for computer vision and multimodal\nmachine learning.\nGuha Balakrishnan is an Assistant Professor of\nElectrical and Computer Engineering working in\nthe fields of computer vision and graphics. He\nis interested in the theory, practical design, and\ndownstream applications of generative models\nfor complex visual data. He is particularly ex-\ncited by their application to promote fairness and\naccountability in vision systems. He received a\nPh.D. in EECS from MIT in 2018.\nNuno Vasconcelos (Fellow, IEEE) is a Profes-\nsor of Electrical and Computer Engineering at\nthe University of California, San Diego, where he\nheads the Statistical Visual Computing Labora-\ntory. He has received an NSF CAREER award, a\nHellman Fellowship, several best paper awards,\nand authored more than 200 peer-reviewed pub-\nlications. He has been the Area Chair of multiple\ncomputer vision conferences and the Associate\nEditor of the IEEE Transactions on PAMI.\nVicente Ord\u00b4o\u02dcnez (Member, IEEE) is an Asso-\nciate Professor in the Department of Computer\nScience at Rice University. His research inter-\nests are at the intersection of Computer Vision\nand Natural Language Processing. He received\na Ph.D. in Computer Science from the University\nof North Carolina at Chapel Hill in 2015. He\nreceived the Marr Prize at ICCV 2013 and a Best\nPaper award at EMNLP 2017.\nVishnu Naresh Boddeti (Member, IEEE) is\nan Associate Professor in the Department of\nComputer Science and Engineering at Michigan\nState University. He received a Ph.D. in Electrical\nand Computer Engineering from Carnegie Mel-\nlon University. His research interests are Com-\nputer Vision, Pattern Recognition, and Machine\nLearning. Papers co-authored by him have re-\nceived Best Paper Awards at BTAS 2013 and\nGECCO 2019 and Best Student Paper Awards\nat ACCV 2018, SMAIS 2022, IJCB 2022, and\nTBIOM 2023.\n",
    "ref": [
        "2010.11929",
        "2303.07615",
        "2302.00070",
        "1605.06083",
        "1711.08536",
        "2103.11436",
        "2012.04842",
        "2207.10653",
        "2306.04482",
        "1902.11097",
        "1911.08731",
        "1907.02893",
        "1908.04913",
        "1901.10436",
        "1912.00578",
        "2109.05433",
        "2306.13141",
        "2207.07180",
        "2107.13998",
        "1811.12231",
        "1803.09797",
        "2310.06904",
        "2302.10893",
        "2402.14577",
        "2210.15230",
        "2306.00905",
        "2312.01261",
        "2401.08053",
        "2203.11933",
        "2403.04547",
        "2403.02695",
        "2311.03287",
        "1511.05897",
        "2204.06125",
        "2307.01952",
        "2402.13490",
        "2112.10741",
        "2108.02818",
        "2403.05262",
        "2303.08774"
    ]
}