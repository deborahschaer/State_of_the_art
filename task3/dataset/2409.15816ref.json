{
    "2401.00713": {
        "title": "Graph Neural Networks in Intelligent Transportation Systems: Advances, Applications and Trends",
        "abstract": "Intelligent Transportation System (ITS) is crucial for improving traffic\ncongestion, reducing accidents, optimizing urban planning, and more. However,\nthe complexity of traffic networks has rendered traditional machine learning\nand statistical methods less effective. With the advent of artificial\nintelligence, deep learning frameworks have achieved remarkable progress across\nvarious fields and are now considered highly effective in many areas. Since\n2019, Graph Neural Networks (GNNs) have emerged as a particularly promising\ndeep learning approach within the ITS domain, owing to their robust ability to\nmodel graph-structured data and address complex problems. Consequently, there\nhas been increasing scholarly attention to the applications of GNNs in\ntransportation, which have demonstrated excellent performance. Nevertheless,\ncurrent research predominantly focuses on traffic forecasting, with other ITS\ndomains, such as autonomous vehicles and demand prediction, receiving less\nattention. This paper aims to review the applications of GNNs across six\nrepresentative and emerging ITS research areas: traffic forecasting, vehicle\ncontrol system, traffic signal control, transportation safety, demand\nprediction, and parking management. We have examined a wide range of\ngraph-related studies from 2018 to 2023, summarizing their methodologies,\nfeatures, and contributions in detailed tables and lists. Additionally, we\nidentify the challenges of applying GNNs in ITS and propose potential future\nresearch directions.",
        "date": "2024-01-01T09:53:24+00:00",
        "label": 1
    },
    "2312.08248": {
        "title": "A Survey of Generative AI for Intelligent Transportation Systems",
        "abstract": "Intelligent transportation systems play a crucial role in modern traffic\nmanagement and optimization, greatly improving traffic efficiency and safety.\nWith the rapid development of generative artificial intelligence (Generative\nAI) technologies in the fields of image generation and natural language\nprocessing, generative AI has also played a crucial role in addressing key\nissues in intelligent transportation systems, such as data sparsity, difficulty\nin observing abnormal scenarios, and in modeling data uncertainty. In this\nreview, we systematically investigate the relevant literature on generative AI\ntechniques in addressing key issues in different types of tasks in intelligent\ntransportation systems. First, we introduce the principles of different\ngenerative AI techniques, and their potential applications. Then, we classify\ntasks in intelligent transportation systems into four types: traffic\nperception, traffic prediction, traffic simulation, and traffic\ndecision-making. We systematically illustrate how generative AI techniques\naddresses key issues in these four different types of tasks. Finally, we\nsummarize the challenges faced in applying generative AI to intelligent\ntransportation systems, and discuss future research directions based on\ndifferent application scenarios.",
        "date": "2023-12-13T16:13:23+00:00",
        "label": 1
    },
    "2404.18886": {
        "title": "A Survey on Diffusion Models for Time Series and Spatio-Temporal Data",
        "abstract": "The study of time series is crucial for understanding trends and anomalies\nover time, enabling predictive insights across various sectors. Spatio-temporal\ndata, on the other hand, is vital for analyzing phenomena in both space and\ntime, providing a dynamic perspective on complex system interactions. Recently,\ndiffusion models have seen widespread application in time series and\nspatio-temporal data mining. Not only do they enhance the generative and\ninferential capabilities for sequential and temporal data, but they also extend\nto other downstream tasks. In this survey, we comprehensively and thoroughly\nreview the use of diffusion models in time series and spatio-temporal data,\ncategorizing them by model category, task type, data modality, and practical\napplication domain. In detail, we categorize diffusion models into\nunconditioned and conditioned types and discuss time series and spatio-temporal\ndata separately. Unconditioned models, which operate unsupervised, are\nsubdivided into probability-based and score-based models, serving predictive\nand generative tasks such as forecasting, anomaly detection, classification,\nand imputation. Conditioned models, on the other hand, utilize extra\ninformation to enhance performance and are similarly divided for both\npredictive and generative tasks. Our survey extensively covers their\napplication in various fields, including healthcare, recommendation, climate,\nenergy, audio, and transportation, providing a foundational understanding of\nhow these models analyze and generate data. Through this structured overview,\nwe aim to provide researchers and practitioners with a comprehensive\nunderstanding of diffusion models for time series and spatio-temporal data\nanalysis, aiming to direct future innovations and applications by addressing\ntraditional challenges and exploring innovative solutions within the diffusion\nmodel framework.",
        "date": "2024-04-29T17:19:40+00:00",
        "label": 1
    },
    "2011.13456": {
        "title": "Score-Based Generative Modeling through Stochastic Differential Equations",
        "abstract": "Creating noise from data is easy; creating data from noise is generative\nmodeling. We present a stochastic differential equation (SDE) that smoothly\ntransforms a complex data distribution to a known prior distribution by slowly\ninjecting noise, and a corresponding reverse-time SDE that transforms the prior\ndistribution back into the data distribution by slowly removing the noise.\nCrucially, the reverse-time SDE depends only on the time-dependent gradient\nfield (\\aka, score) of the perturbed data distribution. By leveraging advances\nin score-based generative modeling, we can accurately estimate these scores\nwith neural networks, and use numerical SDE solvers to generate samples. We\nshow that this framework encapsulates previous approaches in score-based\ngenerative modeling and diffusion probabilistic modeling, allowing for new\nsampling procedures and new modeling capabilities. In particular, we introduce\na predictor-corrector framework to correct errors in the evolution of the\ndiscretized reverse-time SDE. We also derive an equivalent neural ODE that\nsamples from the same distribution as the SDE, but additionally enables exact\nlikelihood computation, and improved sampling efficiency. In addition, we\nprovide a new way to solve inverse problems with score-based models, as\ndemonstrated with experiments on class-conditional generation, image\ninpainting, and colorization. Combined with multiple architectural\nimprovements, we achieve record-breaking performance for unconditional image\ngeneration on CIFAR-10 with an Inception score of 9.89 and FID of 2.20, a\ncompetitive likelihood of 2.99 bits/dim, and demonstrate high fidelity\ngeneration of 1024 x 1024 images for the first time from a score-based\ngenerative model.",
        "date": "2020-11-26T19:39:10+00:00",
        "label": 1
    },
    "2310.07771": {
        "title": "DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model",
        "abstract": "With the increasing popularity of autonomous driving based on the powerful\nand unified bird's-eye-view (BEV) representation, a demand for high-quality and\nlarge-scale multi-view video data with accurate annotation is urgently\nrequired. However, such large-scale multi-view data is hard to obtain due to\nexpensive collection and annotation costs. To alleviate the problem, we propose\na spatial-temporal consistent diffusion framework DrivingDiffusion, to generate\nrealistic multi-view videos controlled by 3D layout. There are three challenges\nwhen synthesizing multi-view videos given a 3D layout: How to keep 1)\ncross-view consistency and 2) cross-frame consistency? 3) How to guarantee the\nquality of the generated instances? Our DrivingDiffusion solves the problem by\ncascading the multi-view single-frame image generation step, the single-view\nvideo generation step shared by multiple cameras, and post-processing that can\nhandle long video generation. In the multi-view model, the consistency of\nmulti-view images is ensured by information exchange between adjacent cameras.\nIn the temporal model, we mainly query the information that needs attention in\nsubsequent frame generation from the multi-view images of the first frame. We\nalso introduce the local prompt to effectively improve the quality of generated\ninstances. In post-processing, we further enhance the cross-view consistency of\nsubsequent frames and extend the video length by employing temporal sliding\nwindow algorithm. Without any extra cost, our model can generate large-scale\nrealistic multi-camera driving videos in complex urban scenes, fueling the\ndownstream driving tasks. The code will be made publicly available.",
        "date": "2023-10-11T18:00:08+00:00",
        "label": 1
    },
    "2402.07369": {
        "title": "Diff-RNTraj: A Structure-aware Diffusion Model for Road Network-constrained Trajectory Generation",
        "abstract": "Trajectory data is essential for various applications as it records the\nmovement of vehicles. However, publicly available trajectory datasets remain\nlimited in scale due to privacy concerns, which hinders the development of\ntrajectory data mining and trajectory-based applications. To address this\nissue, some methods for generating synthetic trajectories have been proposed to\nexpand the scale of the dataset. However, all existing methods generate\ntrajectories in the geographical coordinate system, which poses two limitations\nfor their utilization in practical applications: 1) the inability to ensure\nthat the generated trajectories are constrained on the road. 2) the lack of\nroad-related information. In this paper, we propose a new problem to meet the\npractical application need, \\emph{i.e.}, road network-constrained trajectory\n(RNTraj) generation, which can directly generate trajectories on the road\nnetwork with road-related information. RNTraj is a hybrid type of data, in\nwhich each point is represented by a discrete road segment and a continuous\nmoving rate. To generate RNTraj, we design a diffusion model called\nDiff-RNTraj. This model can effectively handle the hybrid RNTraj using a\ncontinuous diffusion framework by incorporating a pre-training strategy to\nembed hybrid RNTraj into continuous representations. During the sampling stage,\na RNTraj decoder is designed to map the continuous representation generated by\nthe diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj\nintroduces a novel loss function to enhance the spatial validity of the\ngenerated trajectories. Extensive experiments conducted on two real-world\ntrajectory datasets demonstrate the effectiveness of the proposed model.",
        "date": "2024-02-12T01:59:51+00:00",
        "label": 1
    },
    "2311.16203": {
        "title": "ChatTraffic: Text-to-Traffic Generation via Diffusion Model",
        "abstract": "Traffic prediction is one of the most significant foundations in Intelligent\nTransportation Systems (ITS). Traditional traffic prediction methods rely only\non historical traffic data to predict traffic trends and face two main\nchallenges. 1) insensitivity to unusual events. 2) limited performance in\nlong-term prediction. In this work, we explore how generative models combined\nwith text describing the traffic system can be applied for traffic generation,\nand name the task Text-to-Traffic Generation (TTG). The key challenge of the\nTTG task is how to associate text with the spatial structure of the road\nnetwork and traffic data for generating traffic situations. To this end, we\npropose ChatTraffic, the first diffusion model for text-to-traffic generation.\nTo guarantee the consistency between synthetic and real data, we augment a\ndiffusion model with the Graph Convolutional Network (GCN) to extract spatial\ncorrelations of traffic data. In addition, we construct a large dataset\ncontaining text-traffic pairs for the TTG task. We benchmarked our model\nqualitatively and quantitatively on the released dataset. The experimental\nresults indicate that ChatTraffic can generate realistic traffic situations\nfrom the text. Our code and dataset are available at\nhttps://github.com/ChyaZhang/ChatTraffic.",
        "date": "2023-11-27T08:52:10+00:00",
        "label": 1
    },
    "2205.09991": {
        "title": "Planning with Diffusion for Flexible Behavior Synthesis",
        "abstract": "Model-based reinforcement learning methods often use learning only for the\npurpose of estimating an approximate dynamics model, offloading the rest of the\ndecision-making work to classical trajectory optimizers. While conceptually\nsimple, this combination has a number of empirical shortcomings, suggesting\nthat learned models may not be well-suited to standard trajectory optimization.\nIn this paper, we consider what it would look like to fold as much of the\ntrajectory optimization pipeline as possible into the modeling problem, such\nthat sampling from the model and planning with it become nearly identical. The\ncore of our technical approach lies in a diffusion probabilistic model that\nplans by iteratively denoising trajectories. We show how classifier-guided\nsampling and image inpainting can be reinterpreted as coherent planning\nstrategies, explore the unusual and useful properties of diffusion-based\nplanning methods, and demonstrate the effectiveness of our framework in control\nsettings that emphasize long-horizon decision-making and test-time flexibility.",
        "date": "2022-05-20T07:02:03+00:00",
        "label": 1
    },
    "2207.12598": {
        "title": "Classifier-Free Diffusion Guidance",
        "abstract": "Classifier guidance is a recently introduced method to trade off mode\ncoverage and sample fidelity in conditional diffusion models post training, in\nthe same spirit as low temperature sampling or truncation in other types of\ngenerative models. Classifier guidance combines the score estimate of a\ndiffusion model with the gradient of an image classifier and thereby requires\ntraining an image classifier separate from the diffusion model. It also raises\nthe question of whether guidance can be performed without a classifier. We show\nthat guidance can be indeed performed by a pure generative model without such a\nclassifier: in what we call classifier-free guidance, we jointly train a\nconditional and an unconditional diffusion model, and we combine the resulting\nconditional and unconditional score estimates to attain a trade-off between\nsample quality and diversity similar to that obtained using classifier\nguidance.",
        "date": "2022-07-26T01:42:07+00:00",
        "label": 1
    },
    "2211.15657": {
        "title": "Is Conditional Generative Modeling all you need for Decision-Making?",
        "abstract": "Recent improvements in conditional generative modeling have made it possible\nto generate high-quality images from language descriptions alone. We\ninvestigate whether these methods can directly address the problem of\nsequential decision-making. We view decision-making not through the lens of\nreinforcement learning (RL), but rather through conditional generative\nmodeling. To our surprise, we find that our formulation leads to policies that\ncan outperform existing offline RL approaches across standard benchmarks. By\nmodeling a policy as a return-conditional diffusion model, we illustrate how we\nmay circumvent the need for dynamic programming and subsequently eliminate many\nof the complexities that come with traditional offline RL. We further\ndemonstrate the advantages of modeling policies as conditional diffusion models\nby considering two other conditioning variables: constraints and skills.\nConditioning on a single constraint or skill during training leads to behaviors\nat test-time that can satisfy several constraints together or demonstrate a\ncomposition of skills. Our results illustrate that conditional generative\nmodeling is a powerful tool for decision-making.",
        "date": "2022-11-28T18:59:02+00:00",
        "label": 1
    },
    "2309.17080": {
        "title": "GAIA-1: A Generative World Model for Autonomous Driving",
        "abstract": "Autonomous driving promises transformative improvements to transportation,\nbut building systems capable of safely navigating the unstructured complexity\nof real-world scenarios remains challenging. A critical problem lies in\neffectively predicting the various potential outcomes that may emerge in\nresponse to the vehicle's actions as the world evolves.\n  To address this challenge, we introduce GAIA-1 ('Generative AI for\nAutonomy'), a generative world model that leverages video, text, and action\ninputs to generate realistic driving scenarios while offering fine-grained\ncontrol over ego-vehicle behavior and scene features. Our approach casts world\nmodeling as an unsupervised sequence modeling problem by mapping the inputs to\ndiscrete tokens, and predicting the next token in the sequence. Emerging\nproperties from our model include learning high-level structures and scene\ndynamics, contextual awareness, generalization, and understanding of geometry.\nThe power of GAIA-1's learned representation that captures expectations of\nfuture events, combined with its ability to generate realistic samples,\nprovides new possibilities for innovation in the field of autonomy, enabling\nenhanced and accelerated training of autonomous driving technology.",
        "date": "2023-09-29T09:20:37+00:00",
        "label": 1
    },
    "2311.01017": {
        "title": "Copilot4D: Learning Unsupervised World Models for Autonomous Driving via Discrete Diffusion",
        "abstract": "Learning world models can teach an agent how the world works in an\nunsupervised manner. Even though it can be viewed as a special case of sequence\nmodeling, progress for scaling world models on robotic applications such as\nautonomous driving has been somewhat less rapid than scaling language models\nwith Generative Pre-trained Transformers (GPT). We identify two reasons as\nmajor bottlenecks: dealing with complex and unstructured observation space, and\nhaving a scalable generative model. Consequently, we propose Copilot4D, a novel\nworld modeling approach that first tokenizes sensor observations with VQVAE,\nthen predicts the future via discrete diffusion. To efficiently decode and\ndenoise tokens in parallel, we recast Masked Generative Image Transformer as\ndiscrete diffusion and enhance it with a few simple changes, resulting in\nnotable improvement. When applied to learning world models on point cloud\nobservations, Copilot4D reduces prior SOTA Chamfer distance by more than 65%\nfor 1s prediction, and more than 50% for 3s prediction, across NuScenes, KITTI\nOdometry, and Argoverse2 datasets. Our results demonstrate that discrete\ndiffusion on tokenized agent experience can unlock the power of GPT-like\nunsupervised learning for robotics.",
        "date": "2023-11-02T06:21:56+00:00",
        "label": 1
    },
    "2404.12624": {
        "title": "Dragtraffic: A Non-Expert Interactive and Point-Based Controllable Traffic Scene Generation Framework",
        "abstract": "The evaluation and training of autonomous driving systems require diverse and\nscalable corner cases. However, most existing scene generation methods lack\ncontrollability, accuracy, and versatility, resulting in unsatisfactory\ngeneration results. To address this problem, we propose Dragtraffic, a\ngeneralized, point-based, and controllable traffic scene generation framework\nbased on conditional diffusion. Dragtraffic enables non-experts to generate a\nvariety of realistic driving scenarios for different types of traffic agents\nthrough an adaptive mixture expert architecture. We use a regression model to\nprovide a general initial solution and a refinement process based on the\nconditional diffusion model to ensure diversity. User-customized context is\nintroduced through cross-attention to ensure high controllability. Experiments\non a real-world driving dataset show that Dragtraffic outperforms existing\nmethods in terms of authenticity, diversity, and freedom.",
        "date": "2024-04-19T04:49:28+00:00",
        "label": 1
    },
    "2309.09777": {
        "title": "DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving",
        "abstract": "World models, especially in autonomous driving, are trending and drawing\nextensive attention due to their capacity for comprehending driving\nenvironments. The established world model holds immense potential for the\ngeneration of high-quality driving videos, and driving policies for safe\nmaneuvering. However, a critical limitation in relevant research lies in its\npredominant focus on gaming environments or simulated settings, thereby lacking\nthe representation of real-world driving scenarios. Therefore, we introduce\nDriveDreamer, a pioneering world model entirely derived from real-world driving\nscenarios. Regarding that modeling the world in intricate driving scenes\nentails an overwhelming search space, we propose harnessing the powerful\ndiffusion model to construct a comprehensive representation of the complex\nenvironment. Furthermore, we introduce a two-stage training pipeline. In the\ninitial phase, DriveDreamer acquires a deep understanding of structured traffic\nconstraints, while the subsequent stage equips it with the ability to\nanticipate future states. The proposed DriveDreamer is the first world model\nestablished from real-world driving scenarios. We instantiate DriveDreamer on\nthe challenging nuScenes benchmark, and extensive experiments verify that\nDriveDreamer empowers precise, controllable video generation that faithfully\ncaptures the structural constraints of real-world traffic scenarios.\nAdditionally, DriveDreamer enables the generation of realistic and reasonable\ndriving policies, opening avenues for interaction and practical applications.",
        "date": "2023-09-18T13:58:42+00:00",
        "label": 1
    },
    "2312.02934": {
        "title": "WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera Driving Scene Generation",
        "abstract": "Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.",
        "date": "2023-12-05T18:05:14+00:00",
        "label": 1
    },
    "1609.02907": {
        "title": "Semi-Supervised Classification with Graph Convolutional Networks",
        "abstract": "We present a scalable approach for semi-supervised learning on\ngraph-structured data that is based on an efficient variant of convolutional\nneural networks which operate directly on graphs. We motivate the choice of our\nconvolutional architecture via a localized first-order approximation of\nspectral graph convolutions. Our model scales linearly in the number of graph\nedges and learns hidden layer representations that encode both local graph\nstructure and features of nodes. In a number of experiments on citation\nnetworks and on a knowledge graph dataset we demonstrate that our approach\noutperforms related methods by a significant margin.",
        "date": "2016-09-09T19:48:41+00:00",
        "label": 1
    },
    "2010.02502": {
        "title": "Denoising Diffusion Implicit Models",
        "abstract": "Denoising diffusion probabilistic models (DDPMs) have achieved high quality\nimage generation without adversarial training, yet they require simulating a\nMarkov chain for many steps to produce a sample. To accelerate sampling, we\npresent denoising diffusion implicit models (DDIMs), a more efficient class of\niterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is defined as the reverse of a\nMarkovian diffusion process. We construct a class of non-Markovian diffusion\nprocesses that lead to the same training objective, but whose reverse process\ncan be much faster to sample from. We empirically demonstrate that DDIMs can\nproduce high quality samples $10 \\times$ to $50 \\times$ faster in terms of\nwall-clock time compared to DDPMs, allow us to trade off computation for sample\nquality, and can perform semantically meaningful image interpolation directly\nin the latent space.",
        "date": "2020-10-06T06:15:51+00:00",
        "label": 1
    },
    "1805.00123": {
        "title": "CrowdHuman: A Benchmark for Detecting Human in a Crowd",
        "abstract": "Human detection has witnessed impressive progress in recent years. However,\nthe occlusion issue of detecting human in highly crowded environments is far\nfrom solved. To make matters worse, crowd scenarios are still under-represented\nin current human detection benchmarks. In this paper, we introduce a new\ndataset, called CrowdHuman, to better evaluate detectors in crowd scenarios.\nThe CrowdHuman dataset is large, rich-annotated and contains high diversity.\nThere are a total of $470K$ human instances from the train and validation\nsubsets, and $~22.6$ persons per image, with various kinds of occlusions in the\ndataset. Each human instance is annotated with a head bounding-box, human\nvisible-region bounding-box and human full-body bounding-box. Baseline\nperformance of state-of-the-art detection frameworks on CrowdHuman is\npresented. The cross-dataset generalization results of CrowdHuman dataset\ndemonstrate state-of-the-art performance on previous dataset including\nCaltech-USA, CityPersons, and Brainwash without bells and whistles. We hope our\ndataset will serve as a solid baseline and help promote future research in\nhuman detection tasks.",
        "date": "2018-04-30T22:49:54+00:00",
        "label": 1
    },
    "2003.09003": {
        "title": "MOT20: A benchmark for multi object tracking in crowded scenes",
        "abstract": "Standardized benchmarks are crucial for the majority of computer vision\napplications. Although leaderboards and ranking tables should not be\nover-claimed, benchmarks often provide the most objective measure of\nperformance and are therefore important guides for research. The benchmark for\nMultiple Object Tracking, MOTChallenge, was launched with the goal to establish\na standardized evaluation of multiple object tracking methods. The challenge\nfocuses on multiple people tracking, since pedestrians are well studied in the\ntracking community, and precise tracking and detection has high practical\nrelevance. Since the first release, MOT15, MOT16, and MOT17 have tremendously\ncontributed to the community by introducing a clean dataset and precise\nframework to benchmark multi-object trackers. In this paper, we present our\nMOT20benchmark, consisting of 8 new sequences depicting very crowded\nchallenging scenes. The benchmark was presented first at the 4thBMTT MOT\nChallenge Workshop at the Computer Vision and Pattern Recognition Conference\n(CVPR) 2019, and gives to chance to evaluate state-of-the-art methods for\nmultiple object tracking when handling extremely crowded scenarios.",
        "date": "2020-03-19T20:08:24+00:00",
        "label": 1
    },
    "1512.03012": {
        "title": "ShapeNet: An Information-Rich 3D Model Repository",
        "abstract": "We present ShapeNet: a richly-annotated, large-scale repository of shapes\nrepresented by 3D CAD models of objects. ShapeNet contains 3D models from a\nmultitude of semantic categories and organizes them under the WordNet taxonomy.\nIt is a collection of datasets providing many semantic annotations for each 3D\nmodel such as consistent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned annotations. Annotations are\nmade available through a public web-based interface to enable data\nvisualization of object attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research in computer graphics\nand vision. At the time of this technical report, ShapeNet has indexed more\nthan 3,000,000 models, 220,000 models out of which are classified into 3,135\ncategories (WordNet synsets). In this report we describe the ShapeNet effort as\na whole, provide details for all currently available datasets, and summarize\nfuture plans.",
        "date": "2015-12-09T19:42:48+00:00",
        "label": 1
    },
    "2403.09190": {
        "title": "Intention-aware Denoising Diffusion Model for Trajectory Prediction",
        "abstract": "Trajectory prediction is an essential component in autonomous driving,\nparticularly for collision avoidance systems. Considering the inherent\nuncertainty of the task, numerous studies have utilized generative models to\nproduce multiple plausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable training issues.\nTo overcome these limitations, we propose utilizing the diffusion model to\ngenerate the distribution of future trajectories. Two cruxes are to be settled\nto realize such an idea. First, the diversity of intention is intertwined with\nthe uncertain surroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the inference phase,\nrendering it unrealistic to implement in a real-time driving system. We propose\nan Intention-aware denoising Diffusion Model (IDM), which tackles the above two\nproblems. We decouple the original uncertainty into intention uncertainty and\naction uncertainty and model them with two dependent diffusion processes. To\ndecrease the inference time, we reduce the variable dimensions in the\nintention-aware diffusion process and restrict the initial distribution of the\naction-aware diffusion process, which leads to fewer diffusion steps. To\nvalidate our approach, we conduct experiments on the Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-art results, with\nan FDE of 13.83 pixels on the SDD dataset and 0.36 meters on the ETH/UCY\ndataset. Compared with the original diffusion model, IDM reduces inference time\nby two-thirds. Interestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion process of fewer\nsteps.",
        "date": "2024-03-14T09:05:25+00:00",
        "label": 1
    },
    "2301.00493": {
        "title": "Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting",
        "abstract": "We introduce Argoverse 2 (AV2) - a collection of three datasets for\nperception and forecasting research in the self-driving domain. The annotated\nSensor Dataset contains 1,000 sequences of multimodal data, encompassing\nhigh-resolution imagery from seven ring cameras, and two stereo cameras in\naddition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain\n3D cuboid annotations for 26 object categories, all of which are\nsufficiently-sampled to support training and evaluation of 3D perception\nmodels. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point\nclouds and map-aligned pose. This dataset is the largest ever collection of\nlidar sensor data and supports self-supervised learning and the emerging task\nof point cloud forecasting. Finally, the Motion Forecasting Dataset contains\n250,000 scenarios mined for interesting and challenging interactions between\nthe autonomous vehicle and other actors in each local scene. Models are tasked\nwith the prediction of future motion for \"scored actors\" in each scenario and\nare provided with track histories that capture object location, heading,\nvelocity, and category. In all three datasets, each scenario contains its own\nHD Map with 3D lane and crosswalk geometry - sourced from data captured in six\ndistinct cities. We believe these datasets will support new and existing\nmachine learning research problems in ways that existing datasets do not. All\ndatasets are released under the CC BY-NC-SA 4.0 license.",
        "date": "2023-01-02T00:36:22+00:00",
        "label": 1
    },
    "2004.07219": {
        "title": "D4RL: Datasets for Deep Data-Driven Reinforcement Learning",
        "abstract": "The offline reinforcement learning (RL) setting (also known as full batch\nRL), where a policy is learned from a static dataset, is compelling as progress\nenables RL methods to take advantage of large, previously-collected datasets,\nmuch like how the rise of large datasets has fueled results in supervised\nlearning. However, existing online RL benchmarks are not tailored towards the\noffline setting and existing offline RL benchmarks are restricted to data\ngenerated by partially-trained agents, making progress in offline RL difficult\nto measure. In this work, we introduce benchmarks specifically designed for the\noffline setting, guided by key properties of datasets relevant to real-world\napplications of offline RL. With a focus on dataset collection, examples of\nsuch properties include: datasets generated via hand-designed controllers and\nhuman demonstrators, multitask datasets where an agent performs different tasks\nin the same environment, and datasets collected with mixtures of policies. By\nmoving beyond simple benchmark tasks and data collected by partially-trained RL\nagents, we reveal important and unappreciated deficiencies of existing\nalgorithms. To facilitate research, we have released our benchmark tasks and\ndatasets with a comprehensive evaluation of existing algorithms, an evaluation\nprotocol, and open-source examples. This serves as a common starting point for\nthe community to identify shortcomings in existing offline RL methods and a\ncollaborative route for progress in this emerging area.",
        "date": "2020-04-15T17:18:19+00:00",
        "label": 1
    },
    "2106.11810": {
        "title": "NuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles",
        "abstract": "In this work, we propose the world's first closed-loop ML-based planning\nbenchmark for autonomous driving. While there is a growing body of ML-based\nmotion planners, the lack of established datasets and metrics has limited the\nprogress in this area. Existing benchmarks for autonomous vehicle motion\nprediction have focused on short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop evaluation with L2-based\nmetrics, which are not suitable for fairly evaluating long-term planning. Our\nbenchmark overcomes these limitations by introducing a large-scale driving\ndataset, lightweight closed-loop simulator, and motion-planning-specific\nmetrics. We provide a high-quality dataset with 1500h of human driving data\nfrom 4 cities across the US and Asia with widely varying traffic patterns\n(Boston, Pittsburgh, Las Vegas and Singapore). We will provide a closed-loop\nsimulation framework with reactive agents and provide a large set of both\ngeneral and scenario-specific planning metrics. We plan to release the dataset\nat NeurIPS 2021 and organize benchmark challenges starting in early 2022.",
        "date": "2021-06-22T14:24:55+00:00",
        "label": 1
    },
    "1910.03088": {
        "title": "INTERACTION Dataset: An INTERnational, Adversarial and Cooperative moTION Dataset in Interactive Driving Scenarios with Semantic Maps",
        "abstract": "Behavior-related research areas such as motion prediction/planning,\nrepresentation/imitation learning, behavior modeling/generation, and algorithm\ntesting, require support from high-quality motion datasets containing\ninteractive driving scenarios with different driving cultures. In this paper,\nwe present an INTERnational, Adversarial and Cooperative moTION dataset\n(INTERACTION dataset) in interactive driving scenarios with semantic maps. Five\nfeatures of the dataset are highlighted. 1) The interactive driving scenarios\nare diverse, including urban/highway/ramp merging and lane changes, roundabouts\nwith yield/stop signs, signalized intersections, intersections with\none/two/all-way stops, etc. 2) Motion data from different countries and\ndifferent continents are collected so that driving preferences and styles in\ndifferent cultures are naturally included. 3) The driving behavior is highly\ninteractive and complex with adversarial and cooperative motions of various\ntraffic participants. Highly complex behavior such as negotiations,\naggressive/irrational decisions and traffic rule violations are densely\ncontained in the dataset, while regular behavior can also be found from\ncautious car-following, stop, left/right/U-turn to rational lane-change and\ncycling and pedestrian crossing, etc. 4) The levels of criticality span wide,\nfrom regular safe operations to dangerous, near-collision maneuvers. Real\ncollision, although relatively slight, is also included. 5) Maps with complete\nsemantic information are provided with physical layers, reference lines,\nlanelet connections and traffic rules. The data is recorded from drones and\ntraffic cameras. Statistics of the dataset in terms of number of entities and\ninteraction density are also provided, along with some utilization examples in\na variety of behavior-related research areas. The dataset can be downloaded via\nhttps://interaction-dataset.com.",
        "date": "2019-09-30T17:26:51+00:00",
        "label": 1
    },
    "2403.06845": {
        "title": "DriveDreamer-2: LLM-Enhanced World Models for Diverse Driving Video Generation",
        "abstract": "World models have demonstrated superiority in autonomous driving,\nparticularly in the generation of multi-view driving videos. However,\nsignificant challenges still exist in generating customized driving videos. In\nthis paper, we propose DriveDreamer-2, which builds upon the framework of\nDriveDreamer and incorporates a Large Language Model (LLM) to generate\nuser-defined driving videos. Specifically, an LLM interface is initially\nincorporated to convert a user's query into agent trajectories. Subsequently, a\nHDMap, adhering to traffic regulations, is generated based on the trajectories.\nUltimately, we propose the Unified Multi-View Model to enhance temporal and\nspatial coherence in the generated driving videos. DriveDreamer-2 is the first\nworld model to generate customized driving videos, it can generate uncommon\ndriving videos (e.g., vehicles abruptly cut in) in a user-friendly manner.\nBesides, experimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and tracking).\nFurthermore, video generation quality of DriveDreamer-2 surpasses other\nstate-of-the-art methods, showcasing FID and FVD scores of 11.2 and 55.7,\nrepresenting relative improvements of 30% and 50%.",
        "date": "2024-03-11T16:03:35+00:00",
        "label": 1
    },
    "2306.04873": {
        "title": "Complexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model",
        "abstract": "The Origin-Destination~(OD) networks provide an estimation of the flow of\npeople from every region to others in the city, which is an important research\ntopic in transportation, urban simulation, etc. Given structural regional urban\nfeatures, generating the OD network has become increasingly appealing to many\nresearchers from diverse domains. However, existing works are limited in\nindependent generation of each OD pair, i.e., flow of people from one region to\nanother, overlooking the relations within the overall network. In this paper,\nwe instead propose to generate the OD network, and design a graph denoising\ndiffusion method to learn the conditional joint probability distribution of the\nnodes and edges within the OD network given city characteristics at region\nlevel. To overcome the learning difficulty of the OD networks covering over\nthousands of regions, we decompose the original one-shot generative modeling of\nthe diffusion model into two cascaded stages, corresponding to the generation\nof network topology and the weights of edges, respectively. To further\nreproduce important network properties contained in the city-wide OD network,\nwe design an elaborated graph denoising network structure including a node\nproperty augmentation module and a graph transformer backbone. Empirical\nexperiments on data collected in three large US cities have verified that our\nmethod can generate OD matrices for new cities with network statistics\nremarkably similar with the ground truth, further achieving superior\noutperformance over competitive baselines in terms of the generation realism.",
        "date": "2023-06-08T02:02:55+00:00",
        "label": 1
    },
    "2401.08119": {
        "title": "SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting",
        "abstract": "Traffic forecasting, a crucial application of spatio-temporal graph (STG)\nlearning, has traditionally relied on deterministic models for accurate point\nestimations. Yet, these models fall short of quantifying future uncertainties.\nRecently, many probabilistic methods, especially variants of diffusion models,\nhave been proposed to fill this gap. However, existing diffusion methods\ntypically deal with individual sensors separately when generating future time\nseries, resulting in limited usage of spatial information in the probabilistic\nlearning process. In this work, we propose SpecSTG, a novel spectral diffusion\nframework, to better leverage spatial dependencies and systematic patterns\ninherent in traffic data. More specifically, our method generates the Fourier\nrepresentation of future time series, transforming the learning process into\nthe spectral domain enriched with spatial information. Additionally, our\napproach incorporates a fast spectral graph convolution designed for Fourier\ninput, alleviating the computational burden associated with existing models.\nCompared with state-of-the-arts, SpecSTG achieves up to 8% improvements on\npoint estimations and up to 0.78% improvements on quantifying future\nuncertainties. Furthermore, SpecSTG's training and validation speed is 3.33X of\nthe most efficient existing diffusion method for STG forecasting. The source\ncode for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG.",
        "date": "2024-01-16T05:23:34+00:00",
        "label": 1
    },
    "2306.16927": {
        "title": "End-to-end Autonomous Driving: Challenges and Frontiers",
        "abstract": "The autonomous driving community has witnessed a rapid growth in approaches\nthat embrace an end-to-end algorithm framework, utilizing raw sensor input to\ngenerate vehicle motion plans, instead of concentrating on individual tasks\nsuch as detection and motion prediction. End-to-end systems, in comparison to\nmodular pipelines, benefit from joint feature optimization for perception and\nplanning. This field has flourished due to the availability of large-scale\ndatasets, closed-loop evaluation, and the increasing need for autonomous\ndriving algorithms to perform effectively in challenging scenarios. In this\nsurvey, we provide a comprehensive analysis of more than 270 papers, covering\nthe motivation, roadmap, methodology, challenges, and future trends in\nend-to-end autonomous driving. We delve into several critical challenges,\nincluding multi-modality, interpretability, causal confusion, robustness, and\nworld models, amongst others. Additionally, we discuss current advancements in\nfoundation models and visual pre-training, as well as how to incorporate these\ntechniques within the end-to-end driving framework. we maintain an active\nrepository that contains up-to-date literature and open-source projects at\nhttps://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.",
        "date": "2023-06-29T14:17:24+00:00",
        "label": 1
    },
    "2112.03126": {
        "title": "Label-Efficient Semantic Segmentation with Diffusion Models",
        "abstract": "Denoising diffusion probabilistic models have recently received much research\nattention since they outperform alternative approaches, such as GANs, and\ncurrently provide state-of-the-art generative performance. The superior\nperformance of diffusion models has made them an appealing tool in several\napplications, including inpainting, super-resolution, and semantic editing. In\nthis paper, we demonstrate that diffusion models can also serve as an\ninstrument for semantic segmentation, especially in the setup when labeled data\nis scarce. In particular, for several pretrained diffusion models, we\ninvestigate the intermediate activations from the networks that perform the\nMarkov step of the reverse diffusion process. We show that these activations\neffectively capture the semantic information from an input image and appear to\nbe excellent pixel-level representations for the segmentation problem. Based on\nthese observations, we describe a simple segmentation method, which can work\neven if only a few training images are provided. Our approach significantly\noutperforms the existing alternatives on several datasets for the same amount\nof human supervision.",
        "date": "2021-12-06T15:55:30+00:00",
        "label": 1
    },
    "2404.04629": {
        "title": "DifFUSER: Diffusion Model for Robust Multi-Sensor Fusion in 3D Object Detection and BEV Segmentation",
        "abstract": "Diffusion models have recently gained prominence as powerful deep generative\nmodels, demonstrating unmatched performance across various domains. However,\ntheir potential in multi-sensor fusion remains largely unexplored. In this\nwork, we introduce DifFUSER, a novel approach that leverages diffusion models\nfor multi-modal fusion in 3D object detection and BEV map segmentation.\nBenefiting from the inherent denoising property of diffusion, DifFUSER is able\nto refine or even synthesize sensor features in case of sensor malfunction,\nthereby improving the quality of the fused output. In terms of architecture,\nour DifFUSER blocks are chained together in a hierarchical BiFPN fashion,\ntermed cMini-BiFPN, offering an alternative architecture for latent diffusion.\nWe further introduce a Gated Self-conditioned Modulated (GSM) latent diffusion\nmodule together with a Progressive Sensor Dropout Training (PSDT) paradigm,\ndesigned to add stronger conditioning to the diffusion process and robustness\nto sensor failures. Our extensive evaluations on the Nuscenes dataset reveal\nthat DifFUSER not only achieves state-of-the-art performance with a 70.04% mIOU\nin BEV map segmentation tasks but also competes effectively with leading\ntransformer-based fusion techniques in 3D object detection.",
        "date": "2024-04-06T13:25:29+00:00",
        "label": 1
    },
    "2312.11578": {
        "title": "Diffusion-Based Particle-DETR for BEV Perception",
        "abstract": "The Bird-Eye-View (BEV) is one of the most widely-used scene representations\nfor visual perception in Autonomous Vehicles (AVs) due to its well suited\ncompatibility to downstream tasks. For the enhanced safety of AVs, modeling\nperception uncertainty in BEV is crucial. Recent diffusion-based methods offer\na promising approach to uncertainty modeling for visual perception but fail to\neffectively detect small objects in the large coverage of the BEV. Such\ndegradation of performance can be attributed primarily to the specific network\narchitectures and the matching strategy used when training. Here, we address\nthis problem by combining the diffusion paradigm with current state-of-the-art\n3D object detectors in BEV. We analyze the unique challenges of this approach,\nwhich do not exist with deterministic detectors, and present a simple technique\nbased on object query interpolation that allows the model to learn positional\ndependencies even in the presence of the diffusion noise. Based on this, we\npresent a diffusion-based DETR model for object detection that bears\nsimilarities to particle methods. Abundant experimentation on the NuScenes\ndataset shows equal or better performance for our generative approach, compared\nto deterministic state-of-the-art methods. Our source code will be made\npublicly available.",
        "date": "2023-12-18T09:52:14+00:00",
        "label": 1
    },
    "2208.09801": {
        "title": "PointDP: Diffusion-driven Purification against Adversarial Attacks on 3D Point Cloud Recognition",
        "abstract": "3D Point cloud is becoming a critical data representation in many real-world\napplications like autonomous driving, robotics, and medical imaging. Although\nthe success of deep learning further accelerates the adoption of 3D point\nclouds in the physical world, deep learning is notorious for its vulnerability\nto adversarial attacks. In this work, we first identify that the\nstate-of-the-art empirical defense, adversarial training, has a major\nlimitation in applying to 3D point cloud models due to gradient obfuscation. We\nfurther propose PointDP, a purification strategy that leverages diffusion\nmodels to defend against 3D adversarial attacks. We extensively evaluate\nPointDP on six representative 3D point cloud architectures, and leverage 10+\nstrong and adaptive attacks to demonstrate its lower-bound robustness. Our\nevaluation shows that PointDP achieves significantly better robustness than\nstate-of-the-art purification methods under strong attacks. Results of\ncertified defenses on randomized smoothing combined with PointDP will be\nincluded in the near future.",
        "date": "2022-08-21T04:49:17+00:00",
        "label": 1
    },
    "2302.10463": {
        "title": "Multimodal Trajectory Prediction: A Survey",
        "abstract": "Trajectory prediction is an important task to support safe and intelligent\nbehaviours in autonomous systems. Many advanced approaches have been proposed\nover the years with improved spatial and temporal feature extraction. However,\nhuman behaviour is naturally multimodal and uncertain: given the past\ntrajectory and surrounding environment information, an agent can have multiple\nplausible trajectories in the future. To tackle this problem, an essential task\nnamed multimodal trajectory prediction (MTP) has recently been studied, which\naims to generate a diverse, acceptable and explainable distribution of future\npredictions for each agent. In this paper, we present the first survey for MTP\nwith our unique taxonomies and comprehensive analysis of frameworks, datasets\nand evaluation metrics. In addition, we discuss multiple future directions that\ncan help researchers develop novel multimodal trajectory prediction systems.",
        "date": "2023-02-21T06:11:08+00:00",
        "label": 1
    },
    "2401.02916": {
        "title": "Uncovering the human motion pattern: Pattern Memory-based Diffusion Model for Trajectory Prediction",
        "abstract": "Human trajectory forecasting is a critical challenge in fields such as\nrobotics and autonomous driving. Due to the inherent uncertainty of human\nactions and intentions in real-world scenarios, various unexpected occurrences\nmay arise. To uncover latent motion patterns in human behavior, we introduce a\nnovel memory-based method, named Motion Pattern Priors Memory Network. Our\nmethod involves constructing a memory bank derived from clustered prior\nknowledge of motion patterns observed in the training set trajectories. We\nintroduce an addressing mechanism to retrieve the matched pattern and the\npotential target distributions for each prediction from the memory bank, which\nenables the identification and retrieval of natural motion patterns exhibited\nby agents, subsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experiments validate the\neffectiveness of our approach, achieving state-of-the-art trajectory prediction\naccuracy. The code will be made publicly available.",
        "date": "2024-01-05T17:39:52+00:00",
        "label": 1
    },
    "2403.11643": {
        "title": "Diffusion-Based Environment-Aware Trajectory Prediction",
        "abstract": "The ability to predict the future trajectories of traffic participants is\ncrucial for the safe and efficient operation of autonomous vehicles. In this\npaper, a diffusion-based generative model for multi-agent trajectory prediction\nis proposed. The model is capable of capturing the complex interactions between\ntraffic participants and the environment, accurately learning the multimodal\nnature of the data. The effectiveness of the approach is assessed on\nlarge-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential motion constraints on the model output, we\nillustrate that our model is capable of generating a diverse set of realistic\nfuture trajectories. Through the use of an interaction-aware guidance signal,\nwe further demonstrate that the model can be adapted to predict the behavior of\nless cooperative agents, emphasizing its practical applicability under\nuncertain traffic conditions.",
        "date": "2024-03-18T10:35:15+00:00",
        "label": 1
    },
    "2311.01223": {
        "title": "Diffusion Models for Reinforcement Learning: A Survey",
        "abstract": "Diffusion models surpass previous generative models in sample quality and\ntraining stability. Recent works have shown the advantages of diffusion models\nin improving reinforcement learning (RL) solutions. This survey aims to provide\nan overview of this emerging field and hopes to inspire new avenues of\nresearch. First, we examine several challenges encountered by RL algorithms.\nThen, we present a taxonomy of existing methods based on the roles of diffusion\nmodels in RL and explore how the preceding challenges are addressed. We further\noutline successful applications of diffusion models in various RL-related\ntasks. Finally, we conclude the survey and offer insights into future research\ndirections. We are actively maintaining a GitHub repository for papers and\nother related resources in utilizing diffusion models in RL:\nhttps://github.com/apexrl/Diff4RLSurvey.",
        "date": "2023-11-02T13:23:39+00:00",
        "label": 1
    },
    "2208.06193": {
        "title": "Diffusion Policies as an Expressive Policy Class for Offline Reinforcement Learning",
        "abstract": "Offline reinforcement learning (RL), which aims to learn an optimal policy\nusing a previously collected static dataset, is an important paradigm of RL.\nStandard RL methods often perform poorly in this regime due to the function\napproximation errors on out-of-distribution actions. While a variety of\nregularization methods have been proposed to mitigate this issue, they are\noften constrained by policy classes with limited expressiveness that can lead\nto highly suboptimal solutions. In this paper, we propose representing the\npolicy as a diffusion model, a recent class of highly-expressive deep\ngenerative models. We introduce Diffusion Q-learning (Diffusion-QL) that\nutilizes a conditional diffusion model to represent the policy. In our\napproach, we learn an action-value function and we add a term maximizing\naction-values into the training loss of the conditional diffusion model, which\nresults in a loss that seeks optimal actions that are near the behavior policy.\nWe show the expressiveness of the diffusion model-based policy, and the\ncoupling of the behavior cloning and policy improvement under the diffusion\nmodel both contribute to the outstanding performance of Diffusion-QL. We\nillustrate the superiority of our method compared to prior works in a simple 2D\nbandit example with a multimodal behavior policy. We then show that our method\ncan achieve state-of-the-art performance on the majority of the D4RL benchmark\ntasks.",
        "date": "2022-08-12T09:54:11+00:00",
        "label": 1
    },
    "2401.03629": {
        "title": "DDM-Lag : A Diffusion-based Decision-making Model for Autonomous Vehicles with Lagrangian Safety Enhancement",
        "abstract": "Decision-making stands as a pivotal component in the realm of autonomous\nvehicles (AVs), playing a crucial role in navigating the intricacies of\nautonomous driving. Amidst the evolving landscape of data-driven methodologies,\nenhancing decision-making performance in complex scenarios has emerged as a\nprominent research focus. Despite considerable advancements, current\nlearning-based decision-making approaches exhibit potential for refinement,\nparticularly in aspects of policy articulation and safety assurance. To address\nthese challenges, we introduce DDM-Lag, a Diffusion Decision Model, augmented\nwith Lagrangian-based safety enhancements. This work conceptualizes the\nsequential decision-making challenge inherent in autonomous driving as a\nproblem of generative modeling, adopting diffusion models as the medium for\nassimilating patterns of decision-making. We introduce a hybrid policy update\nstrategy for diffusion models, amalgamating the principles of behavior cloning\nand Q-learning, alongside the formulation of an Actor-Critic architecture for\nthe facilitation of updates. To augment the model's exploration process with a\nlayer of safety, we incorporate additional safety constraints, employing a\nsophisticated policy optimization technique predicated on Lagrangian relaxation\nto refine the policy learning endeavor comprehensively. Empirical evaluation of\nour proposed decision-making methodology was conducted across a spectrum of\ndriving tasks, distinguished by their varying degrees of complexity and\nenvironmental contexts. The comparative analysis with established baseline\nmethodologies elucidates our model's superior performance, particularly in\ndimensions of safety and holistic efficacy.",
        "date": "2024-01-08T02:17:09+00:00",
        "label": 1
    },
    "2404.02524": {
        "title": "Versatile Scene-Consistent Traffic Scenario Generation as Optimization with Diffusion",
        "abstract": "Generating realistic and controllable agent behaviors in traffic simulation\nis crucial for the development of autonomous vehicles. This problem is often\nformulated as imitation learning (IL) from real-world driving data by either\ndirectly predicting future trajectories or inferring cost functions with\ninverse optimal control. In this paper, we draw a conceptual connection between\nIL and diffusion-based generative modeling and introduce a novel framework\nVersatile Behavior Diffusion (VBD) to simulate interactive scenarios with\nmultiple traffic participants. Our model not only generates scene-consistent\nmulti-agent interactions but also enables scenario editing through multi-step\nguidance and refinement. Experimental evaluations show that VBD achieves\nstate-of-the-art performance on the Waymo Sim Agents benchmark. In addition, we\nillustrate the versatility of our model by adapting it to various applications.\nVBD is capable of producing scenarios conditioning on priors, integrating with\nmodel-based optimization, sampling multi-modal scene-consistent scenarios by\nfusing marginal predictions, and generating safety-critical scenarios when\ncombined with a game-theoretic solver.",
        "date": "2024-04-03T07:26:15+00:00",
        "label": 1
    },
    "2404.02082": {
        "title": "WcDT: World-centric Diffusion Transformer for Traffic Scene Generation",
        "abstract": "In this paper, we introduce a novel approach for autonomous driving\ntrajectory generation by harnessing the complementary strengths of diffusion\nprobabilistic models (a.k.a., diffusion models) and transformers. Our proposed\nframework, termed the \"World-Centric Diffusion Transformer\" (WcDT), optimizes\nthe entire trajectory generation process, from feature extraction to model\ninference. To enhance the scene diversity and stochasticity, the historical\ntrajectory data is first preprocessed and encoded into latent space using\nDenoising Diffusion Probabilistic Models (DDPM) enhanced with Diffusion with\nTransformer (DiT) blocks. Then, the latent features, historical trajectories,\nHD map features, and historical traffic signal information are fused with\nvarious transformer-based encoders. The encoded traffic scenes are then decoded\nby a trajectory decoder to generate multimodal future trajectories.\nComprehensive experimental results show that the proposed approach exhibits\nsuperior performance in generating both realistic and diverse trajectories,\nshowing its potential for integration into automatic driving simulation\nsystems.",
        "date": "2024-04-02T16:28:41+00:00",
        "label": 1
    },
    "2405.03520": {
        "title": "Is Sora a World Simulator? A Comprehensive Survey on General World Models and Beyond",
        "abstract": "General world models represent a crucial pathway toward achieving Artificial\nGeneral Intelligence (AGI), serving as the cornerstone for various applications\nranging from virtual environments to decision-making systems. Recently, the\nemergence of the Sora model has attained significant attention due to its\nremarkable simulation capabilities, which exhibits an incipient comprehension\nof physical laws. In this survey, we embark on a comprehensive exploration of\nthe latest advancements in world models. Our analysis navigates through the\nforefront of generative methodologies in video generation, where world models\nstand as pivotal constructs facilitating the synthesis of highly realistic\nvisual content. Additionally, we scrutinize the burgeoning field of\nautonomous-driving world models, meticulously delineating their indispensable\nrole in reshaping transportation and urban mobility. Furthermore, we delve into\nthe intricacies inherent in world models deployed within autonomous agents,\nshedding light on their profound significance in enabling intelligent\ninteractions within dynamic environmental contexts. At last, we examine\nchallenges and limitations of world models, and discuss their potential future\ndirections. We hope this survey can serve as a foundational reference for the\nresearch community and inspire continued innovation. This survey will be\nregularly updated at:\nhttps://github.com/GigaAI-research/General-World-Models-Survey.",
        "date": "2024-05-06T14:37:07+00:00",
        "label": 1
    },
    "2210.02303": {
        "title": "Imagen Video: High Definition Video Generation with Diffusion Models",
        "abstract": "We present Imagen Video, a text-conditional video generation system based on\na cascade of video diffusion models. Given a text prompt, Imagen Video\ngenerates high definition videos using a base video generation model and a\nsequence of interleaved spatial and temporal video super-resolution models. We\ndescribe how we scale up the system as a high definition text-to-video model\nincluding design decisions such as the choice of fully-convolutional temporal\nand spatial super-resolution models at certain resolutions, and the choice of\nthe v-parameterization of diffusion models. In addition, we confirm and\ntransfer findings from previous work on diffusion-based image generation to the\nvideo generation setting. Finally, we apply progressive distillation to our\nvideo models with classifier-free guidance for fast, high quality sampling. We\nfind Imagen Video not only capable of generating videos of high fidelity, but\nalso having a high degree of controllability and world knowledge, including the\nability to generate diverse videos and text animations in various artistic\nstyles and with 3D object understanding. See\nhttps://imagen.research.google/video/ for samples.",
        "date": "2022-10-05T14:41:38+00:00",
        "label": 1
    },
    "2310.01415": {
        "title": "GPT-Driver: Learning to Drive with GPT",
        "abstract": "We present a simple yet effective approach that can transform the OpenAI\nGPT-3.5 model into a reliable motion planner for autonomous vehicles. Motion\nplanning is a core challenge in autonomous driving, aiming to plan a driving\ntrajectory that is safe and comfortable. Existing motion planners predominantly\nleverage heuristic methods to forecast driving trajectories, yet these\napproaches demonstrate insufficient generalization capabilities in the face of\nnovel and unseen driving scenarios. In this paper, we propose a novel approach\nto motion planning that capitalizes on the strong reasoning capabilities and\ngeneralization potential inherent to Large Language Models (LLMs). The\nfundamental insight of our approach is the reformulation of motion planning as\na language modeling problem, a perspective not previously explored.\nSpecifically, we represent the planner inputs and outputs as language tokens,\nand leverage the LLM to generate driving trajectories through a language\ndescription of coordinate positions. Furthermore, we propose a novel\nprompting-reasoning-finetuning strategy to stimulate the numerical reasoning\npotential of the LLM. With this strategy, the LLM can describe highly precise\ntrajectory coordinates and also its internal decision-making process in natural\nlanguage. We evaluate our approach on the large-scale nuScenes dataset, and\nextensive experiments substantiate the effectiveness, generalization ability,\nand interpretability of our GPT-based motion planner. Code is now available at\nhttps://github.com/PointsCoder/GPT-Driver.",
        "date": "2023-10-02T17:59:57+00:00",
        "label": 1
    },
    "2403.18344": {
        "title": "LC-LLM: Explainable Lane-Change Intention and Trajectory Predictions with Large Language Models",
        "abstract": "To ensure safe driving in dynamic environments, autonomous vehicles should\npossess the capability to accurately predict lane change intentions of\nsurrounding vehicles in advance and forecast their future trajectories.\nExisting motion prediction approaches have ample room for improvement,\nparticularly in terms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM, an explainable\nlane change prediction model that leverages the strong reasoning capabilities\nand self-explanation abilities of Large Language Models (LLMs). Essentially, we\nreformulate the lane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information as natural language\nprompts for LLMs and employing supervised fine-tuning to tailor LLMs\nspecifically for lane change prediction task. Additionally, we finetune the\nChain-of-Thought (CoT) reasoning to improve prediction transparency and\nreliability, and include explanatory requirements in the prompts during\ninference stage. Therefore, our LC-LLM model not only predicts lane change\nintentions and trajectories but also provides CoT reasoning and explanations\nfor its predictions, enhancing its interpretability. Extensive experiments\nbased on the large-scale highD dataset demonstrate the superior performance and\ninterpretability of our LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for predicting lane\nchange behavior. Our study shows that LLMs can effectively encode comprehensive\ninteraction information for driving behavior understanding.",
        "date": "2024-03-27T08:34:55+00:00",
        "label": 1
    },
    "2404.02903": {
        "title": "LidarDM: Generative LiDAR Simulation in a Generated World",
        "abstract": "We present LidarDM, a novel LiDAR generative model capable of producing\nrealistic, layout-aware, physically plausible, and temporally coherent LiDAR\nvideos. LidarDM stands out with two unprecedented capabilities in LiDAR\ngenerative modeling: (i) LiDAR generation guided by driving scenarios, offering\nsignificant potential for autonomous driving simulations, and (ii) 4D LiDAR\npoint cloud generation, enabling the creation of realistic and temporally\ncoherent sequences. At the heart of our model is a novel integrated 4D world\ngeneration framework. Specifically, we employ latent diffusion models to\ngenerate the 3D scene, combine it with dynamic actors to form the underlying 4D\nworld, and subsequently produce realistic sensory observations within this\nvirtual environment. Our experiments indicate that our approach outperforms\ncompeting algorithms in realism, temporal coherency, and layout consistency. We\nadditionally show that LidarDM can be used as a generative world model\nsimulator for training and testing perception models.",
        "date": "2024-04-03T17:59:28+00:00",
        "label": 1
    },
    "1810.04805": {
        "title": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
        "abstract": "We introduce a new language representation model called BERT, which stands\nfor Bidirectional Encoder Representations from Transformers. Unlike recent\nlanguage representation models, BERT is designed to pre-train deep\nbidirectional representations from unlabeled text by jointly conditioning on\nboth left and right context in all layers. As a result, the pre-trained BERT\nmodel can be fine-tuned with just one additional output layer to create\nstate-of-the-art models for a wide range of tasks, such as question answering\nand language inference, without substantial task-specific architecture\nmodifications.\n  BERT is conceptually simple and empirically powerful. It obtains new\nstate-of-the-art results on eleven natural language processing tasks, including\npushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI\naccuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering\nTest F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).",
        "date": "2018-10-11T00:50:01+00:00",
        "label": 1
    },
    "2310.02239": {
        "title": "MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens",
        "abstract": "The effectiveness of Multimodal Large Language Models (MLLMs) demonstrates a\nprofound capability in multimodal understanding. However, the simultaneous\ngeneration of images with coherent texts is still underdeveloped. Addressing\nthis, we introduce a novel interleaved vision-and-language generation method,\ncentered around the concept of ``generative vokens\". These vokens serve as\npivotal elements contributing to coherent image-text outputs. Our method is\nmarked by a unique two-stage training strategy for description-free multimodal\ngeneration, which does not necessitate extensive descriptions of images. We\nintegrate classifier-free guidance to enhance the alignment of generated images\nand texts, ensuring more seamless and contextually relevant multimodal\ninteractions. Our model, MiniGPT-5, exhibits substantial improvement over the\nbaseline models on multimodal generation datasets, including MMDialog and VIST.\nThe human evaluation shows MiniGPT-5 is better than the baseline model on more\nthan 56\\% cases for multimodal generation, highlighting its efficacy across\ndiverse benchmarks.",
        "date": "2023-10-03T17:49:04+00:00",
        "label": 1
    },
    "2310.08949": {
        "title": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs",
        "abstract": "We present EasyGen, an efficient model designed to enhance multimodal\nunderstanding and generation by harnessing the capabilities of diffusion models\nand large language models (LLMs), Unlike existing multimodal models that\npredominately depend on encoders like CLIP or ImageBind and need ample amounts\nof training data to bridge modalities,EasyGen leverages BiDiffuser,a\nbidirectional conditional diffusion model, to foster more efficient modality\ninteractions. Easygen achieves text generation by training a projection layer\nlinking BiDiffuser and an LLM, and facilities image generation by training an\nadapter to align the LLM's text space with the BiDiffuser's image space,\nComprehensive quantitative and qualitative experiments show that EasyGen excels\nin data-efficient training, high-quality image generation, and extendibility,\neffectively addressing the challenges in multimodal generation. The source code\nis available at https://github.com/zxy556677/EasyGen.",
        "date": "2023-10-13T08:38:56+00:00",
        "label": 1
    },
    "2202.00512": {
        "title": "Progressive Distillation for Fast Sampling of Diffusion Models",
        "abstract": "Diffusion models have recently shown great promise for generative modeling,\noutperforming GANs on perceptual quality and autoregressive models at density\nestimation. A remaining downside is their slow sampling time: generating high\nquality samples takes many hundreds or thousands of model evaluations. Here we\nmake two contributions to help eliminate this downside: First, we present new\nparameterizations of diffusion models that provide increased stability when\nusing few sampling steps. Second, we present a method to distill a trained\ndeterministic diffusion sampler, using many steps, into a new diffusion model\nthat takes half as many sampling steps. We then keep progressively applying\nthis distillation procedure to our model, halving the number of required\nsampling steps each time. On standard image generation benchmarks like\nCIFAR-10, ImageNet, and LSUN, we start out with state-of-the-art samplers\ntaking as many as 8192 steps, and are able to distill down to models taking as\nfew as 4 steps without losing much perceptual quality; achieving, for example,\na FID of 3.0 on CIFAR-10 in 4 steps. Finally, we show that the full progressive\ndistillation procedure does not take more time than it takes to train the\noriginal model, thus representing an efficient solution for generative modeling\nusing diffusion at both train and test time.",
        "date": "2022-02-01T16:07:25+00:00",
        "label": 1
    },
    "2303.01469": {
        "title": "Consistency Models",
        "abstract": "Diffusion models have significantly advanced the fields of image, audio, and\nvideo generation, but they depend on an iterative sampling process that causes\nslow generation. To overcome this limitation, we propose consistency models, a\nnew family of models that generate high quality samples by directly mapping\nnoise to data. They support fast one-step generation by design, while still\nallowing multistep sampling to trade compute for sample quality. They also\nsupport zero-shot data editing, such as image inpainting, colorization, and\nsuper-resolution, without requiring explicit training on these tasks.\nConsistency models can be trained either by distilling pre-trained diffusion\nmodels, or as standalone generative models altogether. Through extensive\nexperiments, we demonstrate that they outperform existing distillation\ntechniques for diffusion models in one- and few-step sampling, achieving the\nnew state-of-the-art FID of 3.55 on CIFAR-10 and 6.20 on ImageNet 64x64 for\none-step generation. When trained in isolation, consistency models become a new\nfamily of generative models that can outperform existing one-step,\nnon-adversarial generative models on standard benchmarks such as CIFAR-10,\nImageNet 64x64 and LSUN 256x256.",
        "date": "2023-03-02T18:30:16+00:00",
        "label": 1
    }
}