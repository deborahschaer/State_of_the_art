{
    "2401.00713": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\ni\nGraph Neural Networks in Intelligent Transportation\nSystems: Advances, Applications and Trends\nHourun Li, Yusheng Zhao, Zhengyang Mao, Yifang Qin, Zhiping Xiao, Jiaqi Feng, Yiyang Gu,\nWei Ju, Member, IEEE, Xiao Luo, and Ming Zhang\nAbstract\u2014Intelligent Transportation System (ITS) is crucial for\nimproving traffic congestion, reducing accidents, optimizing urban\nplanning, and more. However, the complexity of traffic networks\nhas rendered traditional machine learning and statistical methods\nless effective. With the advent of artificial intelligence, deep\nlearning frameworks have achieved remarkable progress across\nvarious fields and are now considered highly effective in many\nareas. Since 2019, Graph Neural Networks (GNNs) have emerged\nas a particularly promising deep learning approach within the ITS\ndomain, owing to their robust ability to model graph-structured\ndata and address complex problems. Consequently, there has\nbeen increasing scholarly attention to the applications of GNNs in\ntransportation, which have demonstrated excellent performance.\nNevertheless, current research predominantly focuses on traffic\nforecasting, with other ITS domains, such as autonomous vehicles\nand demand prediction, receiving less attention. This paper aims\nto review the applications of GNNs across six representative and\nemerging ITS research areas: traffic forecasting, vehicle control\nsystem, traffic signal control, transportation safety, demand\nprediction, and parking management. We have examined a wide\nrange of graph-related studies from 2018 to 2023, summarizing\ntheir methodologies, features, and contributions in detailed tables\nand lists. Additionally, we identify the challenges of applying\nGNNs in ITS and propose potential future research directions.\nIndex Terms\u2014Intelligent Transportation System, Graph Neural\nNetwork, Spatio-temporal Analysis\nI. INTRODUCTION\nA\nS cities grow and transportation systems evolve, several\nissues have become increasingly apparent, such as traffic\ncongestion, environmental pollution, and a rising number of\ntraffic accidents. To address these challenges and improve\ntraffic flow, route planning, and transportation safety, the\nIntelligent Transportation System (ITS) was introduced over\nfive decades ago in the U.S. [1]. Today, ITS applications are\nintegral to everyday life, including Electronic Toll Collection\n(ETC), Traffic Management Systems (TMS), Global Positioning\nSystems (GPS), and Commercial Vehicle Operations (CVO).\nITS encompasses a broad range of areas, including traffic\nforecasting, autonomous vehicles, traffic signal control, and\nmore. Notably, traffic forecasting has emerged as a prominent\nHourun Li, Yusheng Zhao, Zhengyang Mao, Yifang Qin, Jiaqi Feng,\nYiyang Gu, Wei Ju, and Ming Zhang are with School of Computer\nScience, National Key Laboratory for Multimedia Information Process-\ning, Peking University-Anker Embodied AI Lab, Peking University, Bei-\njing, China. (e-mail: lihourun@stu.pku.edu.cn, yusheng.zhao@stu.pku.edu.cn,\nzhengyang.mao@stu.pku.edu.cn, qinyifang@pku.edu.cn, sf77@stu.pku.edu.cn,\nyiyanggu@pku.edu.cn, juwei@pku.edu.cn, mzhang cs@pku.edu.cn)\nZhiping Xiao and Xiao Luo are with Department of Computer Science,\nUniversity of California, Los Angeles, USA. (e-mail: patricia.xiao@cs.ucla.edu,\nxiaoluo@cs.ucla.edu)\nresearch area, garnering significant attention due to its critical\napplications in optimizing route planning, facilitating road\ntraffic, and reducing traffic accidents.\nAccording to Verses et al. [2], addressing practical challenges\nsuch as managing massive and noisy data, scalability, and\ngeneralization remains difficult. Over the past three decades,\nstatistical methods, including simple linear time series models\nlike Autoregressive Integrated Moving Average (ARIMA) [3],\n[4], and traditional machine learning methods such as Logistic\nRegression (LR), Support Vector Regression (SVR), and k-\nNearest Neighbors (KNN) [5]\u2013[7] have been proposed to tackle\nthese issues. However, the increasing volume of data and the\ncomplexity of road conditions have made traditional methods\nless effective. Consequently, there is a need for more efficient\nalgorithms and scalable models to fully leverage massive data\nand develop accurate and efficient ITS solutions. Additionally,\nadvancements in computational techniques, such as graphical\nprocessing units, have propelled the effectiveness of deep\nlearning models. Since 2015, deep-learning models for traffic\nforecasting have seen significant progress, with GNNs emerging\nas the most popular models after 2019 [8]. GNNs excel not only\nat modeling graph-structured problems but also at capturing\ntemporal-spatial dependencies and representing relationships\nin non-Euclidean spaces [8]\u2013[10].\nAfter conducting a comprehensive survey of research in the\nfield of ITS, we found that a significant portion of studies pri-\nmarily focuses on traffic forecasting. However, we believe that\nother critical domains within ITS also warrant greater attention.\nDespite the recent shift toward promising techniques such as\ndeep learning and reinforcement learning, GNNs still require\nmore exploration and application. Considering the graph-based\nnature of traffic networks and the inherent advantages offered\nby GNNs, we argue that GNNs represent a highly promising\nand competitive solution for ITS. Our investigation centers\non papers related to GNNs in ITS published between 2018\nand 2023, providing a detailed summary of their contributions.\nWe also identified key research challenges within ITS and\nproposed potential future directions for leveraging GNNs. Our\nmain contribution can be summarized as follows:\n\u2022 Comprehensive Review. Extensive research work and\nsurveys from 2018 to 2023 for ITS are reviewed in detail,\ncovering six distinct research domains rather than focusing\nsolely on traffic forecasting. Moreover, we offer an in-\ndepth analysis of the papers reviewed, summarize methods\nand challenges, and present informative tables and lists.\n\u2022 A Comprehensive Taxonomy. We categorized the reviewed\nstudies based on various criteria, including research\narXiv:2401.00713v3  [cs.LG]  19 Sep 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nii\ndomain, graph methods, and domain-specific challenges.\nThis approach helps readers comprehensively understand\neach research domain in ITS.\n\u2022 Challenges and Future Directions. After a comprehensive\nreview, we summarize the key challenges when applying\nGNNs in ITS and propose potential future directions,\nwhich provide valuable insights for researchers looking\nto explore and advance this field.\nWe organize the rest of the survey as follows: In section\nII, we quickly review the related surveys in transportation\ndomains and briefly introduce them. In Section III, we present\nthe foundational knowledge on ITS and GNNs, along with\na discussion on problem formulation. In Section IV, we\ninvestigate and review extensive graph-based studies in six\ndomains, including traffic forecasting, vehicle control system,\ntraffic signal control, transportation safety, demand prediction,\nand parking management. In Section V, we summarize the\nchallenges and potential future directions in GNNs for ITS\nbased on the previous review results. Finally, we present the\nconclusions in Section VI.\nII. RELATED SURVEYS\nIn this section, we review the most relevant and representative\nsurveys in Intelligent Transportation Systems (ITS), primarily\nfocusing on those published in the past five years. Special em-\nphasis is given to approaches based on Graph Neural Networks\n(GNNs), for which we provide a thorough introduction.\nAs previously mentioned, ITS encompass a variety of\nresearch fields. However, recent surveys have predominantly\nconcentrated on traffic forecasting [8], [9], [11]\u2013[16], with only\na few exploring other ITS domains [2], [10], [17], [18]. Traffic\nflow prediction has been the most prominent research topic\nin traffic forecasting since 2015, as highlighted by Liu et al.\n[19]. Surveys prior to 2015 primarily focused on statistical\nmethods [3], [4], [20]\u2013[23] and traditional machine learning\nmodels [5]\u2013[7], [24]. Notable early surveys by Vlahogianni\net al. [15], [25] in 2004 and 2014 concentrated on short-term\ntraffic forecasting. However, these traditional methods have\nlimitations in addressing complex transportation problems due\nto their shallow architectures. With advancements in theory and\nhardware, deep learning models have gained popularity since\nthe mid-2010s, significantly advancing traffic forecasting [8],\n[12]. Since 2019, GNNs have become particularly prominent,\nunderscoring their growing importance in ITS [9]\u2013[11], [14].\nIn recent years, researchers have increasingly focused on the\ntemporal and spatial dependencies of traffic data [26], [27],\nleading to the exploration of new research trends and directions.\nThe detailed development of models and GNN applications in\nITS is illustrated in Fig.1.\nIt is worth noting that Jiang et al. published a comprehensive\nreview of GNNs for traffic forecasting [9], which summarizes\nthe research on this topic. They reviewed 212 articles published\nbetween 2018 and 2020, provided a detailed taxonomy of\nproblems and methods, and compiled information on open-\nsource data and code resources. In the following year, they\nextended their work with another survey [14], which updates the\nprevious review by describing the latest research developments\nand trends up to 2022. This follow-up also highlighted specific\nchallenges and proposed informative future directions. While\ntheir surveys offer an exhaustive overview of traffic forecasting,\nthey do not cover other research areas within ITS.\nThe most relevant survey is the work by Rahmani et al.\n[10], which stands out as the most recent and comprehensive\nreview of GNNs in general ITS research. This review covers\nfundamental concepts such as graphs and GNNs and spans\nseven ITS research domains: traffic forecasting, demand predic-\ntion, autonomous vehicles, intersection management, parking\nmanagement, urban planning, and transportation safety. For\neach domain, it provides a brief introduction followed by an\nindependent review of the relevant literature. In contrast, our\nsurvey offers a deeper examination of the application of graphs\nand GNNs in ITS. We provide an extensive exploration of graph\nconstruction, GNN customization for specific challenges, and\nperformance assessment across various ITS domains, offering\ninsights that are not covered in [10]. Furthermore, instead of\nintroducing individual papers independently, our systematic\napproach to summarizing related literature offers readers a more\nreliable and comprehensive analysis. Our review encompasses\nsix domains: traffic forecasting, autonomous vehicles, traffic\nsignal control, transportation safety, demand prediction, and\nparking management. It offers a more thorough and detailed\nexploration of ITS compared to existing surveys.\nIII. BACKGROUND\nIn this section, we will provide background information on\nITS, graphs, and GNNs. We will begin by introducing the key\nconcepts of ITS and the associated research domains. Next,\nwe will explain fundamental graph concepts, covering different\ntypes of graph data and their characteristics. Finally, we will\nprovide an overview of GNN variants and basic GNN models,\nlaying the foundation for the more detailed discussions in the\nsubsequent sections.\nA. Intelligent Transportation System (ITS)\nBefore 1980, ITS was primarily a forward-looking concept\naimed at overcoming the limitations of surface transportation\ncapacity [1], [28]. During this period, the focus was on\nimproving road network efficiency through optimized traffic\nsignals and in-vehicle navigation systems [28]\u2013[30]. In the\n21st century, technological advancements have made traffic\ndata more accessible, leading to a surge in data-driven ap-\nproaches [31]. Real-time traffic data is now used extensively\nfor traffic management, including road condition prediction,\ncongestion identification, and navigation [31]\u2013[36]. Nowadays,\nITS is a rapidly growing interdisciplinary field [37], offering\ninnovative services to enhance performance, improve travel\nsafety, and provide useful information to users. Zhang et\nal. [31] identify six primary subsystems in ITS. For our\nfocus on applying GNNs to address specific issues, we have\nfurther categorized these subsystems into specific research\nfields: traffic forecasting, vehicle control system, traffic signal\ncontrol, transportation safety, demand prediction, and parking\nmanagement, as illustrated in Fig.2.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\niii\nDevelopment of \nModels in ITS\nStatistic-\nbased\nMethods\n1979\nMultivariative\nMethod\nARIMA\n2001\n2003\nReinforcement \nLearning Method\n2019\n2006\nNeural \nNetworks\nCNN, RNN\nArchitectures\n2015\nSpatio-temporal\nModels\n2017\nGNN-based\nModels\n2019\nSTGCN\nDynamic\nGNN\nGAT\n\u2026\nDevelopment of \nGNN applications \nin ITS\nTra\ufb03c\nPrediction\nVolume\nPrediction\nFlow\nPrediction\n1979\n2004\n2006\nTransportation\nManagement\nTra\ufb03c Signal\nControl\n2013\nParking\nManagement\nDemand\nPrediction\nShort-term\nForecasting\n2005\nZone-based\nOrient-\nDestination\n2017\n2019\n2019\nTransportation\nSafety\n2017\nVehicle\nControl\nUrban\nPlaning\n2022\nPerception\nTrajectory \nPrediction\nFig. 1: Development of Models and Application in Intelligent Transportation System\nGNNs meet ITS\nTraffic Management \nSystem\nUrban Transportation \nSystem\nVehicle Control \nSystem\nTraveler Information \nSystem\nPublic Transportation \nSystem\nTraffic Signal Control\nTraffic Forecasting\nTransportation Safety\nDemand Prediction\nParking Management\nAutonomous Vehicle\nUrban Planning\nTrajectory Prediction\nRisk Analysis\nFlow Forecasting\nSpeed Forecasting\nTime Forecasting\nPerception\nAccidents Prediction\nLand Use Planning\nRoad Planning\n\u2026\u2026\nMore Research Fields\u2026\u2026\nFig. 2: Research Fields in Intelligent Transportation System\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\niv\nB. Graphs and Graph Neural Networks in ITS\nIn this section, we will briefly introduce graphs and GNNs,\nwhich are necessary for the following discussion.\n1) Graphs and Graph Types:\nIn general, a graph consists\nof a set of nodes V = {v1, v2, . . . , v|V|} and a corresponding\nset of edges E, denoted as G = (V, E). The graph G can\nbe represented by an adjacency matrix A \u2208R|V|\u00d7|V|, where\nAij = 1 if eij = (vi, vj) \u2208E, and Aij = 0 otherwise. In\naddition, each node vi can be associated with node features\nxi \u2208Rd, and the feature matrix of the graph is denoted as X \u2208\nR|V|\u00d7d, where d is the dimension of the features. Therefore, a\ngraph can also be represented as G = {X, A} and the graph\ncan be further categorized into several types:\n\u2022 Directed/Undirected Graphs. In an undirected graph,\nif there is an edge (vi, vj) \u2208E, then (vj, vi) \u2208E must\nalso hold, and vice versa. While in a directed graph, this\nconstraint does not apply.\n\u2022 Weighted/Unweighted Graphs. In a weighted graph,\nedges are assigned weight values to indicate their vary-\ning importance or other necessary information. In an\nunweighted graph, all edges are considered equal.\n\u2022 Signed/Unsighed Graphs.Signed graphs consist of edges\nwith positive or negative signs, representing different types\nof relationships. In contrast, unsigned graphs have edges\nwithout sign distinctions, indicating neutral relationships.\n\u2022 Static/Dynamic Graphs. Static graphs have fixed node\nand edge features that remain constant throughout. In\ncontrast, dynamic graphs evolve, where new nodes or\nedges can emerge or disappear at any time step.Therefore,\nit is essential to model temporal information accurately to\ncapture the changes over time.Normally, we represent a\ndynamic graph as a sequence of static graph screen-shots:\nG = {G1, G2, . . . GT } ,\nwhere Gt = (Vt, Et), t \u2208{1, 2, . . . , T} and T is the total\nnumber of time steps.\n\u2022 Homogeneous/Heterogeneous Graphs. Homogeneous\ngraphs consist of nodes and edges of a single type,\nrepresenting uniform entities and relationships. In contrast,\nheterogeneous graphs involve multiple types of nodes and\nedges, capturing diverse entities and complex interactions.\n\u2022 Nested Graphs. Nested graphs are hierarchical, with\nnodes that can be expanded into subgraphs containing\nadditional nodes and links. Nested graphs are valuable\nin ITS for managing multi-level information flows and\nconducting multi-level prediction tasks.\n\u2022 Hypergraphs. Hypergraphs enhance traditional graphs\nby allowing edges to connect any number of nodes,\nwhich enables the modeling of more complex relationships\namong groups of entities.\n2) Graph Construction in ITS: Graph data is highly effective\nfor representing traffic flow data. For instance, in a system\nwith N road sections, we can model this as a graph G =\n(V, E), where V = {v1, v2, . . . , vN} denotes the set of nodes,\neach corresponding to a road section. The edges, represented\nby E, capture the relationships between these nodes, such as\nconnectivity. If two road sections are directly connected, an\nedge is included in the edge set, denoted as (vi, vj) \u2208E, where\nvi and vj are the connected road sections, and E is the set\nof all such edges. This graph construction method provides a\nclear way to model and analyze traffic flow and connectivity.\nHowever, graph construction varies across different ITS\nresearch domains and is highly problem-specific. For example,\nin urban traffic management, a dynamic graph is often used\nwhere road intersections are represented as nodes and roads\nas edges, capturing traffic connectivity and flow. Edges in\nthis graph might be weighted to reflect parameters such as\ntravel time, congestion levels, or distance. While in demand\nprediction, nodes might represent bus stops or train stations,\nwhile edges could indicate routes with attributes such as\nfrequency, capacity, and schedule adherence. The diverse\nmethods of graph construction highlight the need for tailored\napproaches that address the unique requirements and challenges\nof each application. Therefore, in the following sections, we\nwill explore the typical graph construction methods used in\neach research domain in detail.\nC. Graph Neural Networks (GNNs)\n1) Definition of GNNs: Graph Neural Networks (GNNs) are\ndesigned specifically for graph data. A central mechanism of\nGNNs is the message-passing paradigm [38], which iteratively\naggregates information from neighboring nodes to capture the\nstructural characteristics of the graph. Let h(l)\nv\nindicates the\nembedding of v at layer l \u2208{1, . . . , L}, the message-passing\nmechanism can be denoted as:\nh(l)\nv\n= U(l)\u0010\nh(l\u22121)\nv\n, A(l)\u0010\b\nh(l\u22121)\nu\n\t\nu\u2208N (v)\n\u0011\u0011\n,\n(1)\nwhere A(l) and U(l) represent the message aggregating and\nembedding updating function at layer l respectively, and N(v)\ndenotes the neighbors of v. After L iterations, we can obtain the\nnode-level representation h(l)\nv . The graph-level representation\nhG can be obtained by aggregating all node representations at\nlayerL with a readout function. Formally, it can be defined as:\nhG = READOUT({h(L)\nv\n}v\u2208V),\n(2)\nwhere READOUT could be averaging or other graph-level\npooling functions depending on the model [39]\u2013[41].\n2) Variants of GNNs: In the rapidly evolving field of GNNs,\na variety of variants have emerged as crucial tools for effec-\ntively utilizing graph-structured data and tackling the unique\nchallenges across different domains. The existing literature\noffers numerous taxonomies to classify these variants based on\nstructural, operational, and application-oriented characteristics.\nFor the purpose of our survey, which specifically targets ITS,\nwe will not present an exhaustive taxonomy of GNNs. Instead,\nwe will introduce a few preliminary GNN variants that are\nhighly relevant to our discussions on ITS applications. For those\ninterested in a more thorough exploration of GNN taxonomies\nand a broader classification of these networks, we recommend\nreferring to the comprehensive works by Wu et al. [42], Zhou\net al. [43], and Zhang et al. [44].\n\u2022 Convolutional GNNs. The underpinning of Convolutional\nGNNs is rooted in the concept of expanding traditional\nconvolution operations from Euclidean to non-Euclidean\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nv\ndomains. Convolutional GNNs can be categorized into\nspectral and spatial methods. Spectral methods are based\non spectral graph theory but suffer from computational\ninefficiencies and a lack of localization in the spatial\ndomain. Spatial methods directly compute convolutions\nby aggregating and transforming features from a node\u2019s\nneighborhood, avoiding costly spectral transformations.\nIn conclusion, convolutional GNNs adapt convolutional\noperations to graph-structured data, offering a robust\nframework for feature learning on graphs.\n\u2022 Recurrent-Based GNNs. Recurrent-based GNNs combine\nthe characteristics of GNNs and recurrent neural networks\n(RNNs) to effectively capture dependencies in sequential\nand graph-based data. By employing a message-passing\nmechanism and recurrent updates, recurrent-based GNNs\ncan dynamically adapt to changes in the graph structure.\nThis approach provides a flexible and adaptable framework\nthat can effectively handle spatial and temporal complexi-\nties in real-world datasets, particularly in dynamic social\nand traffic networks.\n\u2022 Spatial-Temporal GNNs. Spatial-temporal GNNs inte-\ngrate the spatial structure of a graph using a spatial graph\nneural network and a time-series processing model such\nas RNN, LSTM, or Transformer. This integration allows\nfor simultaneous data processing in both the temporal and\nspatial dimensions. Therefore, spatial-temporal GNNs are\ncommonly employed to analyze graph data with temporal\ndynamics, particularly for traffic forecasting [9].\n\u2022 Graph Autoencoders. Graph autoencoders (GAEs) are\nunsupervised generative models designed to encode graph-\nstructured data into a low-dimensional representation\nusing encoders such as GCN [45]. Subsequently, GAEs\nreconstruct the original graph from this representation.\nGAEs are commonly utilized for tasks such as node\nembedding, link prediction, and graph generation. An\nexample of a well-known GAE is the Variational Graph\nAutoencoder (VGAE) [46].\n\u2022 Graph Reinforcement Learning. Graph reinforcement\nlearning (GRL) is an innovative approach that inte-\ngrates reinforcement learning (RL) with graph-based\nrepresentations to address challenges associated with\ngraph-structured data where strategic decision-making is\ndesirable. In GRL, agents develop optimal policies by\ninteracting with graph-structured environments, leveraging\nthe connections and relationships between nodes to inform\ntheir decisions. GRL is applied to optimize network\nrouting and conduct molecular design on graphs.\nIV. THE APPLICATIONS OF GNNS IN ITS\nThis section will review research across six critical domains\nwithin ITS: traffic forecasting, vehicle control system, traffic\nsignal control, transportation safety, demand prediction, and\nparking management. For each domain, we define the core\nproblems, evaluate their potential impact, and identify the asso-\nciated challenges or requirements. Next, we present a logically\norganized review of the research, emphasizing key findings and\ntrends rather than discussing each paper independently. Finally,\nwe discuss how graphs are constructed within each specific\ndomain and examine the modifications made to standard GNN\nto address the unique needs and challenges of each area.\nA. Traffic Forecasting\nTraffic forecasting, also referred to as traffic prediction, has\nbeen a prominent research topic in ITS over the last few decades\n[47]. Its objective is to predict the future traffic state on a road\nnetwork. Based on the observed metrics, traffic forecasting can\nbe roughly categorized into two sub-types: flow forecasting\nand speed/time forecasting.\n1) Traffic Flow Forecasting: Traffic flow forecasting, which\nuses flow as a metric, involves estimating the volume of vehicles\nduring specific periods and across different segments of the\ntransportation network. Accurate traffic flow forecasting is\nessential for managing congestion, planning routes, handling\nincidents, and improving overall transportation infrastructure\nefficiency. The challenges arise from the complex and con-\nstantly changing factors influencing traffic flow, such as daily\ncommuting patterns, road conditions, weather, special events,\naccidents, and construction work [49].\nRecent research has introduced various models that can\neffectively capture the complex interdependencies in spatial\nand temporal data. These models, such as STSGCN by Song\net al. [55], DyHSL by Zhao et al. [49], DCRNN by Li et al.\n[58], ASTGCN by Guo et al. [56], utilize advanced neural\nnetwork architectures to handle dynamic relationships within\ntraffic systems. They provide a comprehensive understanding\nof traffic behavior by addressing spatial aspects (e.g. road\nconnectivity, road intersections) and temporal factors (e.g.\ntraffic flow variations) over time.\nOne line of research involves utilizing GNNs in combination\nwith RNNs [70], [71] to recursively capture both spatial\nand temporal information [53], [54], [58]. For instance, the\nDiffusion Convolutional Recurrent Neural Network (DCRNN)\nby Li et al. [58] replaces fully connected layers in the GRU\n[71] with diffusion convolution. Similarly, the Adaptive Graph\nConvolutional Recurrent Network (AGCRN) by Bai et al. [53]\nfocuses on learning node-specific features and uncovering hid-\nden inter-dependencies through an adaptive graph convolutional\nrecurrent methodology. AGCRN reflects a trend toward tailoring\nmodels to understand complex network dynamics. Furthermore,\nthe Hypergraph Convolutional Recurrent Neural Network\n(HGC-RNN) by Yi et al. [54] combines hypergraph convolution\nwith RNNs, specifically targeting traffic flow forecasting. This\ncombination highlights the potential of integrating different\nneural network architectures to improve prediction accuracy.\nAnother line of research [49], [55]\u2013[57], [72] involves\ndeveloping a large spatio-temporal graph and utilizing GNNs\nto capture spatio-temporal correlations. For example, the\nSpatial-temporal Synchronous Graph Convolutional Network\n(STSGCN) by Song et al. [55] creates a spatio-temporal graph\nstructure to perform localized graph convolution operations,\nthereby improving data processing capabilities. Attention-based\nSpatial-temporal Graph Convolutional Network (ASTGCN) by\nGuo et al. [56] incorporates an attention mechanism within\nthe spatio-temporal graph context [55] to enhance performance\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nvi\nTABLE I: A Comprehensive Overview of Most Related Studies for Traffic Forecasting\nModel\nArticle\nYear\nTask\nGraph Construction\nSpatial Module\nTemporal Module\nSummary\nFPTN\n[48]\n2023\nFlow\nroad network\ntransformer\ntransformer\nFPTN improves traffic forecasting with sensor-based data division, triple types of\nembeddings, and an efficient Transformer encoder, reducing computational demands.\nDyHSL\n[49]\n2023\nFlow\nlearned hypergraph\nHGNN\nHGNN\nDyHSL improves traffic forecasting with hypergraphs for dynamics and interactive\nconvolutions for spatio-temporal relations, effective across multiple datasets.\nDSTAGNN\n[50]\n2022\nFlow\ndynamic\nGNN\nGNN\nDSTAGNN dynamically models spatial-temporal road network interactions by\nutilizing enhanced multi-head attention and multi-scale gated convolution.\nBi-STAT\n[51]\n2022\nFlow\nroad network\ntransformer\ntransformer\nBi-STAT enhances traffic forecasting with adaptive spatial-temporal transformers,\nhandling diverse task complexities and leveraging past data for improved prediction.\nSTFGNN\n[52]\n2021\nFlow\nroad network\nGNN\nGNN\nSTFGNN enhances traffic forecasting by fusing data-driven temporal and spatial\ngraphs and employing gated convolutions, effectively handling long sequences.\nAGCRN\n[53]\n2020\nFlow\ngenerated\nGCN\nRNN\nAGCRN enhances prediction by two adaptive modules, focusing on node-specific\npatterns and automatic inter-dependency learning without pre-defined graphs.\nHGC-RNN\n[54]\n2020\nFlow\nroad network\nHGNN\nRNN\nHGC-RNN leverages hypergraph convolution and RNNs for structured time-series\nsensor network data, capturing complex structural and temporal dependencies.\nSTSGCN\n[55]\n2020\nFlow\nroad network\nGCN\nGCN\nSTSGCN models localized spatial-temporal correlations and accounts for heterogen-\neities across different periods, simplifying spatial-temporal network data forecasting.\nASTGCN\n[56]\n2019\nFlow\nroad network\nGCN\nattention\nASTGCN improve forecasting with a spatial-temporal attention mechanism and\nconvolutions, focusing on dynamic correlations to make more accurate predictions.\nLRGCN\n[57]\n2019\nFlow\nroad network\nRGCN\nRGCN\nLRGCN, designed for time-evolving graph path classification, integrating temporal\ndependencies and graph dynamics by relational GCN to process time-based relations.\nDCRNN\n[58]\n2017\nFlow\nroad network\nGCN\nRNN\nDCRNN models forecasting as a diffusion process on directed graphs, using bidirec-\ntional random walks and an encoder-decoder architecture with scheduled sampling.\nCAGRU\n[59]\n2021\nSpeed\nroad network\nGAT\nGRU\nCAGRU predicts traffic speed and identifies patterns using a convolutional attention-\nbased neural network based on traffic flow data without relying on historical speed data.\nDMSTGCN\n[60]\n2021\nSpeed\nlearned\nDGNN\nDGNN\nDMSTGCN learns dynamic spatial dependencies between road segments and incorpo-\nrates multi-varied traffic data, capturing multifaceted spatio-temporal traffic features.\nFASTGNN\n[61]\n2021\nSpeed\nroad network\nASTGCN\nASTGCN\nFASTGNN, a federated learning framework, features a differential privacy-based me-\nthod to protect topological information and an innovative aggregation approach.\n-\n[62]\n2020\nSpeed\nroad network\nGraphSAGE\n-\nThis paper uses GraphSAGE to forecast spatially heterogeneous traffic speed and impu-\ntes missing data for segment networks with nonlinear spatial-temporal correlations.\nATT-LSTM\n[63]\n2020\nSpeed\nroad network\nGAT\nLSTM\nAttention-based LSTM (ATT-LSTM), a short-term level prediction model, predicts\ntraffic speed and imputes missing traffic data with a data preprocessing module.\nGATCN\n[64]\n2020\nSpeed\nroad network\nGAT\nTCN\nGATCN, a deep learning framework combining GAT and TCN, effectively learns spatio-\ntemporal traffic flow characteristics and neighborhood information with multiple layers.\nMTL-GRU\n[65]\n2020\nSpeed\nroad network\nGNN\nGRU\nMTL-GRU, a multitask learning GRU model with residual mappings, selects\nthe most informative features to enhance traffic flow and speed forecasting.\nDSTL-GR\n[66]\n2023\nTime\nroad network\nGraphSAGE\nLSTM\nDLSF-GR enhances travel time prediction by considering spatial and temporal dep-\nendence, as well as exogenous variables, through a combination of GNNs and RNNs.\nDeepTrans\n[67]\n2020\nTime\nroad network\nDCRNN\nDCRNN\nDeepTRANS enhances travel time estimation by incorporating traffic forecasting into\nan existing deep learning-based bus ETA model, improving congestion prediction.\nSST-GNN\n[68]\n2020\nTime\nroad network\nSGNN\nSGNN\nSST-GNN predicts by encoding spatial correlations, using neighborhood aggregati-\non and a spatio-temporal mechanism with position encoding for periodic patterns.\n-\n[69]\n2019\nTime\nroad segment\nclustering\n-\nThe model predicts bus travel times using real-time taxi and bus data, dividing\nroutes into dwelling and transit segments with two tailored models for each.\nby focusing on salient features. Additionally, the Long Short-\nTerm Memory R-GCN (LRGCN) approach by Li et al. [57]\nis designed to encode spatio-temporal graphs more efficiently,\naddressing the inherent complexities of such data structures.\nThe third line of research involves utilizing GNNs in\nconjunction with transformers [48], [51], [73], [74], leveraging\nthe success of transformers across various domains [75]\u2013[77].\nThis approach effectively merges the transformers\u2019 ability to\nhandle long-range dependencies with the spatial processing\nstrengths of GNNs, resulting in two distinct types of method-\nologies for traffic flow forecasting. The first type features\nmodular integration where spatial and temporal components\nare distinctly processed and then combined. For example, the\nSpatial-Temporal Transformer Networks (STTNs) introduced\nby Xu et al. [73] employ a spatial transformer to dynamically\nmodel directed spatial dependencies via self-attention, capturing\nreal-time traffic conditions and directional flow. This is com-\nplemented by a temporal transformer that handles long-range\nbidirectional temporal dependencies, optimizing long-term\nforecasting accuracy. Similarly, the Multi-Spatial-Temporal\nEncoder-Decoder Model (MST-EDM) [78] processes different\ntemporal scales with distinct encoders that are later fused,\noffering a granular analysis of temporal correlations.\nThe second type adopts a more unified or pure transformer\napproach, leveraging a streamlined architecture to enhance\nprocessing efficiency and scalability. The Fast Pure Transformer\nNetwork (FPTN) [48], for instance, restructures traffic flow data\nanalysis by aligning it along the sensor dimension, applying\nmultiple layers of Transformer encoders to simultaneously\ncapture complex spatio-temporal correlations. This method\nsignificantly reduces computational time and resource usage.\nAdditionally, the Bidirectional Spatial-Temporal Adaptive\nTransformer (Bi-STAT) [51] uses an adaptive encoder-decoder\narchitecture that dynamically adjusts to the complexity of\ntraffic forecasting tasks. It integrates innovative features like\na dynamic halting module (DHM) for iterative computation,\nenhancing the adaptability and accuracy of the forecasts.\nThe diversity of these models demonstrates the breadth of\ninnovation in this area. Each approach provides unique insights\nand methodologies, contributing to an extensive and more\ndiverse toolkit for traffic analysts and urban planners.\n2) Traffic Speed/Time Forecasting: Traffic speed forecasting\nand time forecasting are closely intertwined within ITS,\nplaying vital roles in improving navigational routing and\nestimating arrival times in various applications. Traffic speed\nforecasting involves estimating the average velocity of vehicles\nover a specific segment during a defined time interval. In\ncontrast, traffic time forecasting predicts the duration required\nto travel through a particular route or segment. Due to the\nintrinsic relationship between travel speed and travel time, both\nforecasting types face similar challenges, such as information\nscarcity and heterogeneous traffic conditions. Recent research\nin this field [59]\u2013[68], [79]\u2013[81], has effectively utilized both\nspatial and temporal dimensions in traffic speed data, similar\nin traffic flow forecasting.\nInformation scarcity is a biggest problem in these fields,\nwhich highlights the difficulty in generating accurate predictions\nwhen faced with limited, incomplete, or sparse traffic data [63],\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nvii\n[82]. Such scarcity can stem from various reasons, including the\nlack of coverage by sensor networks, the high costs associated\nwith the deployment and maintenance of extensive traffic\nmonitoring systems, and the challenges in collecting data on\nroads with low traffic volumes or in remote areas. In response\nto these challenges, Liu et al. [62] have developed a technique\nthat applies a data recovery algorithm based on identifying\nnonlinear spatial and temporal correlations within the road\nnetwork. This algorithm helps impute missing speed data for\ndifferent segments and enables traffic speed forecasting across\na diverse and heterogeneous road network. Moreover, Huang\net al. [82] have utilized Probabilistic Principal Component\nAnalysis (PPCA) to model travel speeds reliably, even when\ndata is missing from specific road segments. They have also\nemployed spectral clustering to categorize roads with similar\ntraffic conditions into clusters, which reduces the variability of\ntraffic conditions within each group. This enhances predictive\nconsistency and facilitates parallel computing to improve\noverall prediction performance.\nIn traffic speed forecasting, Liu et al. [62] employs the\nGraphSAGE model [83], a novel approach tailored for sparse\nnetwork conditions, to enhance the accuracy of traffic speed\npredictions. This approach emphasizes the importance of\nspatial information in the context of sparse connectivity.\nKhodabandelou et al. [59] proposed CAGRU, combining graph\nconvolution techniques with attention-based gated recurrent\nunits [71] to capture both spatial and temporal relationships\nwithin traffic speed data. This fusion approach enriches the\nmodel\u2019s understanding of complex traffic dynamics. Zhang\net al. [65] proposed a multi-task learning framework (MTL-\nGRU) that simultaneously processes traffic flow and speed\nforecasting. This approach enables the model to learn from the\nintertwined nature of traffic speed and flow, leading to a more\nnuanced representation of spatio-temporal data and enhancing\nthe predictive accuracy for both metrics.\nIn traffic time forecasting, Tran et al. [67] have taken the lead\nin this field by incorporating traffic flow forecasting models\ninto their travel time prediction system, called DeepTrans. Their\nmethodology uses machine learning to examine vast datasets of\nhistorical traffic patterns, allowing for more precise travel time\nestimations. Diving deeper into the interplay between spatial\nand temporal factors, Kang et al. [80] introduced a novel\nspatio-temporal forecasting framework focused on the urban\ncontext. This approach can process and integrate multifaceted\ndata streams, capturing the intricate dynamics of urban traffic.\nThe model considers not only the physical layout of the\ntransportation network but also the fluctuating congestion levels\nover time. By assimilating this spatio-temporal information,\ntheir model extracts essential representations that significantly\nimprove the reliability of travel time forecasts.\nAlthough traffic speed/time forecasting and traffic flow fore-\ncasting are related, they are still distinct areas in transportation\ndomains. Traffic flow forecasting offers a macroscopic view,\nfocusing on overall traffic conditions and trends across a broader\narea or network [49], [58], [84], which involves understanding\ntraffic patterns, volume, and congestion across a network. On\nthe other hand, traffic speed/time forecasting delves into the\nmicroscopic details, emphasizing the temporal elements of\ntravel. It provides detailed insights into the travel duration\nbetween specific locations, making it valuable for journey\nplanning and management [85]\u2013[87]. More details about the\nrelated work for traffic forecasting can be found in Table.I.\n3) Discussion: How to Construct a Graph in Traffic\nForecasting? Contemporary methods primarily align with\ntwo categories: empirical graph construction and learning-\nbased graph construction. Empirically constructed graphs, such\nas road networks, are commonly used [48], [53], [56], [58].\nTechniques like dynamic time wrapping also play a role in\ncapturing dependencies among time series [52], [88]. Recent\nadvancements have introduced methods to learn underlying\ngraph structures from spatio-temporal data [49], [50], [52],\n[89], [90]. Some approaches [52], [89] focus on dissecting\nand understanding spatial and temporal structures individually,\nthen integrating them for advanced spatio-temporal forecasting.\nFor instance, Li et al. [52] proposed Spatial-Temporal Fusion\nGraph Neural Networks (STFGNN), which employ a spatial\nfusion graph along with a temporal graph, showcasing the\nefficacy of multi-dimensional graph structures in analysis. In\ncontrast, other studies [49], [50], [90] aim to simultaneously\nmodel spatio-temporal structures. Zhao et al. [49] introduced\na dynamic hypergraph over the spatio-temporal graph to\nenhance forecasting accuracy. Meanwhile, the Dynamic Spatial-\nTemporal Aware Graph Neural Network (DSTAGNN) [50]\nfocuses on learning integrated spatio-temporal graphs, utilizing\nmulti-head attention [75] to capture dynamic spatial relevancies.\nHow standard GNNs are modified for traffic forecasting?\nTraffic forecasting involves predicting future traffic patterns by\nleveraging temporal information for accuracy. This process is\ndivided into two sub-types, both of which utilize similar method-\nologies and share the fundamental principle that integrating\nspatio-temporal information is essential for enhancing model\nperformance. To achieve this, a combination of GNNs [83], [91],\n[92] and sequential models such as RNNs, Long Short-Term\nMemory networks (LSTMs) [70], [93], and Gated Recurrent\nUnits (GRUs) [71] is commonly employed. Sequential models\nexcel at processing time-series data, making them particularly\neffective for capturing and forecasting time-dependent traffic\npatterns. When combined with GNNs, which model spatial\nrelationships, this approach adeptly captures both temporal\nsequences and spatial dependencies within traffic data, resulting\nin a more accurate and comprehensive traffic forecasting model.\nB. Vehicle Control System\nVehicle Control System (VCS) is an essential component of\nITS, which is responsible for optimizing the operation, safety,\nand efficiency of autonomous vehicles in traffic networks.\nRecently, learning-based methods, especially GNNs, have\ndemonstrated significant potential in addressing the complexi-\nties of urban environments. This section will delve into two key\nsubsystems within VCS: perception, which involves interpreting\nsensory data, and trajectory prediction, which focuses on\nforecasting vehicle movement. Finally, we present an overview\nof the most related studies for VCS in Table.II.\n1) Perception: Perception plays a vital role in vehicle control\nsystem in identifying and categorizing objects around a vehicle.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nviii\nTABLE II: A Comprehensive Overview of Most Related Studies for Vehicle Control System\nModel\nArticle\nYear\nDatasets\nGNN Module\nSummary\nGTNet\n[94]\n2023\nModelNet40, ShapeNet part\nGraph Transformer\nGTNet uses a Local Transformer to calculate neighboring point weights through dynamic graph-based\ncross-attention within domains, and a Global Transformer to expand its range using global self-attention.\nMHNet\n[95]\n2023\nModelNet40, NTU\nSpectral GNN\nMHNet introduces a polynomial hypergraph filter, which dynamically extracts multi-scale node features.\nDiffConv\n[96]\n2022\nModelNet40, Toronto3D,\nShepeNet part\nSpectral GNN\nDiffConv uses density-dilated neighborhoods where each point\u2019s radius depends on its kernel density.\nIt also uses masked attention to introduce task-specific learned variations to the neighborhood.\nDeltaConv\n[97]\n2022\nModelNet40, SHREC11,\nScanObjectNN, ShapeNet\nSpectral GNN\nDeltaConv uses a graph-based anisotropic convolutional operator by combining a set of geometric\noperators defined on scalar and vector fields to encode the directional information of each surface point.\n3DCTN\n[98]\n2022\nModelNet40, ScanObjectNN\nGraph Transformer\n3DCTN combines convolutions and transformers to learn local and global features. It uses a multi-scale\nlocal feature aggregation block and a global feature learning block to process downsampled point sets.\nPoint Transformer\n[99]\n2021\nS3DIS, ModelNet40,\nShapeNet part\nGraph Transformer\nPoint Transformer introduces an expressive transformer layer tailored for point cloud processing.\nIt employs local self-attention and integrates vector attention to achieve elevated accuracy levels.\nPCT\n[100]\n2021\nModelNet40, ShapeNet\nGraph Transformer\nPoint Cloud Transformer (PCT) improves capturing local context capture within the point cloud by using\ncoordinate-based input embedding with the help of farthest point sampling and nearest neighbor search.\nCurveNet\n[101]\n2021\nModelNet40, ModelNet10,\nShapeNet part\nSpatial GNN\nCurveNet enhances point cloud shape descriptors by organizing connected points through guided\nwalks within point clouds and aggregating them to enhance their individual point-wise features.\nLDGCNN\n[102]\n2021\nModelNet40, ShapeNet\nSpatial GNN\nLDGCNN is a linked dynamic graph CNN created for direct classification and segmentation of point clouds,\naddressing sparsity and unstructured nature. It also includes theoretical analysis and model visualization.\n3D-GCN\n[103]\n2020\nModelNet40, ModelNet10,\nShapeNet part\nSpatial GNN\n3D-GCN is a novel approach for processing 3D point clouds in computer vision that offers scale and shift\ninvariance by utilizing learnable kernels and a graph max-pooling mechanism to extract robust features.\nDHGNN\n[104]\n2019\nModelNet40, NTU\nSpectral GNN\nDHGNN addresses limitations in graph/hypergraph-based deep learning by dynamically updating hyper-\ngraph structures and encoding high-order data relations through vertex and hyperedge convolutions.\nDGCNN\n[105]\n2019\nModelNet40\nSpatial GNN\nDGCNN, a novel neural network module dubbed EdgeConv suitable for point clouds, enhances CNN-\nbased high-level tasks by incorporating local neighborhood information and adapting to topology.\nRGCNN\n[106]\n2018\nShapeNet part\nSpectral GNN\nRGCNN directly processes point clouds, utilizing spectral graph theory and Chebyshev polynomial\napproximation to capture dynamic graph structures adaptively, enhancing point cloud understanding.\nAGCN\n[107]\n2018\nSydney urban\nSpectral GNN\nAGCN, a flexible Graph CNN that takes data of arbitrary graph structure as input, enables task-driven\nadaptive graph and distance metric learning for diverse data such as molecular and social networks.\nKCNet\n[108]\n2018\nModelNet40, ShapeNet\nSpatial GNN\nKCNet improves semantic learning efficiency for 3D point clouds by introducing a point-set kernel for 3D\ngeometry and recursive feature aggregation on a nearest-neighbor graph that focuses on local structures.\nLocal-SpecGCN\n[109]\n2018\nModelNet40, McGill Shape,\nShapeNet part,\nScanNet Indoor Scene\nSpectral GNN\nLocal-SpecGCN uses spectral graph convolution on local graphs and a graph pooling strategy for point\ncloud feature learning, enhancing feature descriptors by aggregating information from clustered nodes.\nECC\n[110]\n2017\nSydney Urban Objects,\nModelNet10, ModelNet40\nSpatial GNN\nECC adapts convolution operators for arbitrary graphs, avoiding the spectral domain, and uses specific edge\nlabels in a vertex\u2019s neighborhood to condition filter weights, enabling diverse graph classification tasks.\nPerception involves two critical tasks: semantic segmentation\nwith classification and object detection with tracking [111],\nwhich includes clustering and assigning specific classes to\npixels in an image. In perception, point cloud is a common\nmethod for representing 3D data. It can capture complex\n3D shapes and their unique irregular structures. Traditional\ndeep learning methods usually convert point clouds into 3D\nvoxel grids or collections of images before feeding them into\ndeep neural networks, potentially resulting in information loss\nand computational overhead [112]. An alternative approach\nleverages the graph-like nature of point clouds, leading to a\ngrowth in research efforts employing GNNs to enhance the\nefficiency and accuracy of 3D data analysis. In the following\nsections, we will discuss GNN-based methods for learning\nrepresentations from point cloud data.\nGraph-Based Methods in Spatial Domain. Spatial Convo-\nlutional Graph Neural Networks can be described as the process\nof spreading node features to neighboring nodes by utilizing\na convolutional kernel. This is then followed by applying an\nactivation function using a trainable weight matrix to transform\nthese features into the subsequent hidden layer [113].\nAs a pioneering approach, Simonovsky et al. [110] intro-\nduced Edge-Conditioned Convolution (ECC) as the first graph-\nbased method in a spatial domain. This method uses edge\nlabels in vertex neighborhoods to calculate adaptive convolution\nkernel weights, thus enabling more effective utilization of edge\ninformation compared to traditional point-based convolutions.\nHowever, ECC [110] has a limitation in that it primarily\ndepends on the inherent graph structure of the input point\ncloud, which inherently restricts its flexibility to model non-\nlocal relations. To address the limitation, several methods [102],\n[105], [114] have been proposed.\nDynamic Graph Convolutional Neural Network (DGCNN)\n[105] introduces an EdgeConv neural network architecture\nto segment point clouds and capture semantically related\nstructures. This approach learns a dynamic graph representation\nof the point cloud, which evolves across layers and during the\ntraining phase as learnable parameters are updated. Building\nupon earlier developments like ECC and DGCNN, the Linked\nDynamic Graph Convolutional Neural Network (LDGCNN)\n[102] enhances the capabilities of DGCNN by establishing links\nbetween hierarchical features derived from various dynamic\ngraphs. This linkage enables the computation of informative\nedge vectors while simultaneously reducing the model\u2019s size.\nTo capture the local neighborhood structural information of a\npoint, kernel-based approaches have been extensively explored\n[101], [103], [108]. For instance, KCNet [108] introduced\na point-set kernel of learnable 3D points. They utilized a\nkernel correlation layer to compute affinities between each\ndata point\u2019s nearest neighbors and these point-set kernels. They\nalso implemented recursive feature propagation and aggregation\nalong the edges, effectively utilizing local high-dimensional fea-\nture structures. Similarly, 3D-GCN [103] proposed deformable\nkernels that were designed to extract shift and scale-invariant\nlocal 3D features from point clouds. Furthermore, Xiang et\nal. [101] introduced a method to arrange connected points\nthrough guided walks within the point clouds and subsequently\naggregated them to enhance their point-wise features, effectively\nimproving the representation of point cloud geometry.\nGraph-Based Methods in Spectral Domain. Spectral\nConvolutional Graph Neural Networks are based on spectral\ngraph theory [115]. In this framework, graph signals are\nfiltered through the eigendecomposition of the graph Laplacian.\nRegularized Graph CNN (RGCNN) [106] perform graph\nconvolution and feature learning based on spectral graph\ntheory, which treats point cloud features as signals on a graph\nand employs Chebyshev polynomial approximation for graph\nconvolution. RGCNN adapts to the corresponding learned\nfeatures by updating the graph Laplacian matrix in each\nlayer, effectively capturing evolving graph structures during\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nix\nthe learning process. Traditional spectral GCNs require the\nprior computation of graph Laplacians and pooling hierarchies\nfor the entire graph, which can be computationally intensive.\nIn dealing with the challenges of diverse graph topology in\ndata, two promising approaches have been proposed. One\napproach is the Adaptive Graph Convolutional Neural Network\n(AGCN) [107], which enhances the generalization capacity\nof GCNs by incorporating a learnable distance metric to\nparameterize the similarity between two vertices within a graph,\nallowing for the dynamic construction of graphs. The other\napproach is Local-SpecGCN [109], which conducts spectral\nfiltering on dynamically generated local graphs. It uses recursive\nclustering based on spectral coordinates to facilitate graph\npooling, which enhances the learning process by mitigating\npoint isolation. Instead of using conventional max pooling,\nWang et al. introduced a recursive clustering and pooling\nstrategy that enables the amalgamation of information from\nnodes within clusters defined by their spectral coordinates.\nHypergraphs attract the attention of researchers as a tool for\ncapturing high-order data correlations. One notable example\nis Hypergraph Neural Networks (HGNN) [104], which uses\na hyperedge convolution operation to capture high-order data\ncorrelations and represent complex structures within point\nclouds. This operation aggregates node features into hyperedge\nfeatures and then updates node features through hyperedge\nfeature aggregation. Hypergraph Gragh Convolutional Network\n(HyperGCN) [116] uses non-linear Laplacian operators [117]\nto convert hypergraphs into more straightforward graphs by\nbreaking hyperedges down into subgraphs with edge weights\nthat depend solely on their degrees. Hypergraph convolution\nrelies on a predefined structure for propagation. To address this\nlimitation, Bai et al. [118] introduced an attention mechanism\nfor dynamic connection learning among hyperedges. This\nmechanism ensures that information propagates and gathers\nin graph regions relevant to specific tasks, leading to the\nlearning of more discriminative node embeddings. Multi-modal\nHypergraph Neural Network (MHNet) [95] uses hypergraph\nstructures to model high-order and multi-modal data correla-\ntions effectively. It achieves this by employing a polynomial\nhypergraph filter that dynamically extracts multi-scale node\nfeatures through parametric polynomial fitting.\nRecent advancements have been made in convolution oper-\nations for point clouds. Conventional approaches commonly\nperform convolution operations to the irregular point clouds by\nimposing a fixed view, such as using fixed neighborhood sizes.\nTo address this issue, DiffConv [96] introduced density-dilated\nneighborhoods, where the radius for each point depends on its\nkernel density. DiffConv uses masked attention, which intro-\nduces task-specific irregularity to the neighborhood, making\nthe convolution more flexible and effective. Another approach,\nDeltaConv. [97] proposed a new method to construct anisotropic\nconvolution layers for geometric CNNs. It designed a graph-\nbased anisotropic convolutional operator by combining a set\nof geometric operators defined on scalar and vector fields to\nencode directional information for each surface point.\nGraph Transformer-based Methods. While transformers\nhave been extensively used in computer vision, graph-based\ntransformers are explicitly tailored for learning 3D point cloud\nrepresentation. The transformer architecture is well-suited for\npoint cloud analysis due to its self-attention operator, which\nfunctions as a set operator by preserving permutation and\ncardinality invariance of input elements [99]. As an example\nwithin this category, Point Transformer (PT) [99] introduces\na transformer layer that is highly expressive and specifically\ndesigned for point cloud processing. The Point Transformer\nemploys local self-attention that ensures scalability even in large\nscenes. Additionally, integrating vector attention is pivotal in\nachieving elevated accuracy levels. Another tailored transformer\nfor point clouds is Point cloud transformer (PCT) [100]. PCT\nemploys a coordinate-based input embedding module to learn\ndistinctive features by combining raw positional encoding and\ninput embedding, harnessing the individual spatial coordinates\nof each point. Furthermore, it enhances performance by\nsubstituting the original self-attention module with an offset-\nattention module. Unlike PT, PCT excels in capturing global\ninteraction and local neighborhood information.\nTo improve efficiency in point cloud classification, 3D\nConvolution-Transformer Network (3DCTN) [98] by Lu et al.\ncombines GNN convolutions with transformers, which helps\neffectively learn local and global features. To be more specific,\n3DCTN processes downsampled point sets using a combination\nof a multi-scale local feature aggregating block and a global\nfeature learning block, which are implemented by GNNs\nand Transformers, respectively. While most Transformer-based\napproaches primarily rely on global attention mechanisms to\nextract point cloud features, they often fail to adequately capture\nfeature learning from local neighbors. To address this challenge,\nGraph Transformer Network (GTNet) [94] employs local and\nglobal Transformer modules. The local Transformer module\ncalculates neighboring point weights through dynamic graph-\nbased intra-domain cross-attention, which assigns different\nweights to each neighboring point\u2019s influence on the centroid\u2019s\nfeatures. In contrast, the global Transformer module expands\nthe local Transformer\u2019s reach by utilizing global self-attention,\nwhich enables a broader feature extraction.\n2) Trajectory Prediction: Trajectory prediction involves\nanticipating the future paths of road users based on their\npast trajectories and the surrounding environment. Road users\ninclude vehicles, cyclists, and pedestrians. The environment\nincludes static factors such as terrain and obstacles and dynamic\nfactors such as the movements of nearby agents [119].\nSeveral models have been developed to improve trajectory\nprediction by adopting the paradigm of spatial and temporal\nconvolution through GNNs. For instance, GRIP [120] enhanced\ntrajectory prediction by incorporating interactions among\nadjacent objects, represented as an undirected graph. It employs\na GCN module to model the graph network, and the output\nof GCN is then input into an LSTM encoder-decoder for\npredicting the trajectories of surrounding vehicles. SCALE-Net\n[121] creates an efficient and scalable framework to maintain\nhigh prediction performance for numerous vehicles. It employs\nan Edge-Enhanced Graph Convolutional Network (EGCN)\nto update node features based on an attention mechanism\ninfluenced by edge features from neighboring nodes. Social-\nSTGCNN [122] represents pedestrian trajectories as spatio-\ntemporal graphs and employs GCN and TCN to operate on\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nx\nthese graphs, enabling the model to predict the entire sequence\nsimultaneously. Chandra et al. [123] uses a two-layer Graph-\nLSTM architecture for trajectory prediction. The initial layer is\napplied to forecast future trajectories of traffic participants,\nwhile the second layer captures interaction-related factors\namong participants using a weighted dynamic geometric graph\nnetwork (DGG). Additionally, they introduced a regularization\nalgorithm based on spectral clustering to minimize errors in\nlong-term predictions. GSTCN [124] uses a GCN to capture\nspatial interactions and a CNN to handle temporal correlations\namong neighboring vehicles. The spatial-temporal features are\nencoded and decoded using a GRU network in their framework.\nRecently, the attention mechanism has gained popularity for\nvarious sequence-based tasks, including predicting the trajectory\nof autonomous vehicle systems. The Spatial-Temporal Graph\nAttention network (STGAT) [125] utilizes an LSTM encoder\nto encode trajectories, followed by GAT for attention-weighted\ninteraction information, and an LSTM decoder for trajectory\nprediction. SCOUT [126] utilizes GAT to incorporate dynamic\nagent interactions, aiming to improve socially aware and\nconsistent trajectory predictions. The Attention-based Spatio-\nTemporal Graph Neural Network (AST-GNN) [127] utilizes\na dual-attention mechanism: the first for capturing spatial\ninteractions among all agents and the second for considering\nthe temporal movement patterns of each agent in the past.\nThe Spatio-Temporal Graph Dual-Attention Network (STG-\nDAT) [128] employs a dual-attention mechanism to learn\nrepresentations on spatio-temporal dynamic graphs, considering\nhistorical and future features from state, relation, and scene\ncontext information. The Triple Policies Fused Hierarchical\nGraph Networks (Tri-HGNN) [129] proposed triple policies\nfused hierarchical GNN for pedestrian trajectory prediction.\nMore specifically, the extrinsic-level policy uses GAT for spatial\nand temporal embeddings, the intrinsic-level policy captures\nhuman intention with GCN, and the basic-level policy combines\ninformation for predictions through TCN. The Heterogeneous\nDriving Graph Transformer (HDGT) [130] models the driving\nscene as a heterogeneous graph, where agents, lanes, and traffic\nsigns are considered as different types of nodes and edges.\nBesides, the transformer structure is applied hierarchically to\naccommodate the heterogeneous inputs.\n3) Discussion: How to Construct a graph in Vehicle\nControl System? In vehicle perception, 3D representations of\nreal-world objects can be obtained using 3D sensors or Light\nDetection and Ranging (LiDAR) technology. The individual\npoints in a point cloud can be seen as nodes in a graph,\nwhile determining the graph structure is a complex process.\nConnecting all edges for the entire point cloud can require a\nlot of memory. Therefore, a simplified method involves using\nthe K-Nearest Neighbors (KNN) approach to create a locally\ndirected graph [102]. Moreover, there have been proposals for\nusing kernel-based methods to calculate the affinities between\neach data point [101], [103], [108]. In trajectory prediction,\nboth the agent (such as a vehicle or pedestrian) and static\nfeatures in the environment (such as traffic signals and road\nsigns) can be viewed as nodes in a graph. The connections\nbetween these nodes can represent their physical proximity\nor potential interaction. Node attributes may encompass the\nentity\u2019s speed, acceleration, heading, and previous path, while\nedge properties may include relative speed, relative heading,\nand distance between the two entities. To account for temporal\naspects, temporal encoding methods can be utilized, such as\nusing time steps as node or edge attributes or employing\nrecursive GNN variations for processing time series data.\nHow standard GNNs are modified for Vehicle Control\nSystem? When GNNs are deployed in real-time and high-\nstakes environments such as VCS, adapting standard GNN\narchitectures is essential to meet specific operational demands.\nOne key challenge is the dynamic nature of the traffic environ-\nment, where the graph structure frequently changes. To address\nthis, methods like edge weighting are crucial for dynamically\nupdating the graph structure without retraining the model from\nscratch. Another important consideration is incorporating both\nspatial and temporal awareness. Spatial-temporal GNNs are\noften employed to integrate spatial relationships and temporal\ndynamics into the model, which enhances its ability to under-\nstand and predict traffic patterns. Additionally, autonomous\nvehicles require real-time or near-real-time responses, making\nthe computational efficiency of GNNs critical. Standard GNNs\ncan be computationally intensive, especially with large graphs,\nso optimizing the network for low latency is essential. This\ncan be achieved through model simplification, pruning, and\nreducing the number of layers to accelerate computation.\nFinally, in high-stakes scenarios, GNNs must account for\nuncertainty in their predictions. Techniques such as Bayesian\nGNNs [131] can provide a measure of confidence, which is\nvital for safe and reliable decision-making.\nC. Traffic Signal Control\nTraffic signal control (TSC) involves managing and coordinat-\ning traffic lights at intersections and road junctions to regulate\ntraffic flow, improve road safety, and minimize delays. Effective\nTSC helps reduce the likelihood of accidents by managing the\nmovement of vehicles and pedestrians. It also has the potential\nto reduce travel time and fuel consumption, leading to lower\nvehicle emissions and supporting environmental sustainability.\nHowever, the intricate interactions between intersections and the\ncontinuously changing traffic conditions make the real-world\nnetwork of intersections highly complex, posing a significant\nchallenge for adaptive traffic signal control.\n1) Multi-agent Graph Reinforcement Learning: Single-agent\nreinforcement learning methods are constrained to managing\ntraffic signals in a single intersection due to the curse of\ndimensionality when using a global single model for all\nintersections. As a result, single-agent RL is typically restricted\nto an isolated intersection without coordination with neighbor-\ning intersections [143]. In managing multiple intersections,\na practical approach is to obtain neighborhood intersection\ninformation by combining the states of intersections and\ntheir adjacent areas [144]. However, this method encounters\ndifficulties when dealing with increasing input dimensionality,\nleading to challenges in model convergence. Thankfully, multi-\nagent reinforcement learning allows for the individual control\nof each signal using a reinforcement learning (RL) agent\nand developing policies for each intersection, resulting in\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxi\nTABLE III: A Comprehensive Overview of Most Related Studies for Traffic Signal Control\nModel\nArticle\nYear\nDatasets\nSimulator\nTemporal Module\nSpatial Module\nAttention Based\nSummary\nAFMRL\n[132]\n2023\nSimulated and real-world data\n(Jinan, Hangzhou, Manhattan)\nCityFlow\n-\nGNN\n$\nA multi-agent reinforcement learning approach for multi-intersection TSC.\nAdaptive partitioning is emphasized and feudal hierarchy is explored.\nKeyLight\n[133]\n2023\nSimulated and real-world data\n(Jinan, Hangzhou, New York)\nCityFlow\n-\nGAT\n\"\nKeyLight integrates reinforcement learning and GNNs. NOV-LADLE\nstate representation and residual connections are used in the model.\nHG-M2I\n[134]\n2023\nreal world data (Chengdu)\nSUMO\nGRU\nBi-GRU\n\"\nThe HG-M2I algorithm, spatial-temporal analysis and multi-agent RL based,\noptimizes TSC by hierarchical graph structures and input-output correlation.\nMetaSTGAT\n[135]\n2022\nSimulated and real-world data\n(Jinan, Hangzhou)\nCityFlow\nLSTM\nGAT\n\"\nMetaSTGAT, meta-learning based, merges GAT and LSTM to address\nspatial-temporal correlations and dynamic interaction of intersections.\nPRGLight\n[136]\n2022\nSimulated and real-world data\n(Jinan, Hangzhou, New York)\nCityFlow\n-\nGNN\n$\nPRCOL uses lane capacity for the RL reward function and GNN modules\nto help RL decide the light phase and duration by predicting traffic flow.\nDynSTGAT\n[137]\n2021\nSimulated and real world data\n(Jinan, Hangzhou, New York)\nCityFlow\nTCN, LSTM,\nSTGAT\nSTGAT\n\"\nDynSTGAT combines spatial-temporal graph attention networks\nand temporal convolutional network to enhance adaptive TSC.\nIHG-MA\n[138]\n2021\nSimulated and real-world data\n(Chengdu)\nSUMO\nBi-GRU\nBi-GRU\n\"\nIHG-MA uses inductive heterogeneous GNNs to capture traffic features\nand a decentralized multi-agent actor-critic framework to optimize TSC.\nGraphLight\n[139]\n2021\nSimulated data\nSUMO\n-\nGCNN\n$\nGraphLight is a decentralized, graph-based, multi-agent system using actor-\ncritic methods for TSC, distinguishing neighboring intersection impacts.\nTSC-GNN\n[140]\n2021\nreal world data\n(Jinan, Hangzhou)\n-\n-\nGAT\n\"\nTSC-GNN is a graph-based model for TSC utilizing probabilistic\nneural networks, to manage uncertainties and calculate Q-values.\nSTMARL\n[141]\n2020\nSimulated and real-world data\n(Hefei, Hangzhou)\nCityFlow\nRNN\nGNN\n\"\nSTMARL applies spatial-temporal RL for TSC, using graphs, RNNs,\nGNNs, and deep Q-learning for distributed decision-making.\nCoLight\n[142]\n2019\nSimulated and real-world data\n(Jinan, Hangzhou, New York)\nCityFlow\n-\nGAT\n\"\nCoLight uses graph attention networks for TSC, and captures spatial-\ntemporal impacts from nearby intersections without indexing\nsignificant advancements. Additionally, the application of multi-\nagent graph reinforcement learning has demonstrated promising\nprogress [136], [137], [139], [145], [146].\nNishi et al. [147] are among the ones who first combine\nmulti-agent reinforcement learning and graph neural networks\nto address the multi-intersection interaction problem and the\nspatial dependency. Their work employs GCNs to extract\nthe geometric features. Zhong et al. [140] proposed a model\nnamed TSC-GNN to handle a problem that most studies model\ntraffic state deterministically and to exploit the uncertainties\nof traffic conditions. Yoon et al. [148] claimed that the RL\nmethod encountered a restricted exploration problem, which\nmeans it cannot handle unseen conditions. They proposed a\nnovel approach to obtain a transferable policy by using graph\nrepresentation for the state and training it by GNNs. Based\non Multi-Agent Reinforcement Learning (MARL), Saki et al.\n[149] used multi-objective reinforcement learning (MORL) to\nfurther improve the performance by determining the policy\ncorresponding to each traffic flow ratio, which achieved the\nshorted average travel times in all environments compared with\nruled based and single objective reinforcement learning. Some\nmore similar literature [133], [134], [136]\u2013[139], [150] based\non graph reinforcement learning is listed in Table.III .\n2) Attention Mechanism: Another issue is distinguishing the\nimpact of neighboring traffic signals on the target intersection.\nFor example, intersections on the main road may significantly\naffect the target intersection more than those on the side road.\nMost existing research does not differentiate the influence of\nsurrounding intersections on the target intersection [147], [151].\nThe attention mechanism has proven valuable in adaptive signal\nlight control. CoLight [142] advanced this by using GATs to\nidentify the impact of neighboring intersections and effectively\nleverage joint intersections. This was achieved by developing\nan index-free model of neighboring intersections and averaging\ntheir influences using learned attention parameters. Sun et\nal.\n[133] introduced NOV-LADLE to address potential\nconvergence failures in the attention mechanism, maintaining\na concise state and emphasizing essential intersections. They\nalso added a residual connection structure to GAT to accelerate\nconvergence and enhance performance based on CoLight.\nFurther studies, including DynSTGAT and TSC-GNN [137],\n[140], have also explored using the graph attention mechanism\nto tackle this issue. Table III summarizes these works.\n3) Spatial and Temporal Dependency: Similar to the previ-\nous discussion, considering spatial and temporal relations is\ncrucial. The historical states of surrounding intersections affect\nthe prediction of future signals at a target intersection.\nWang et al. [141] pioneered the study of spatio-temporal\ndependencies among multiple traffic signals. They used graph\nstructures to capture spatial features and recurrent neural\nnetworks to integrate historical traffic data, with decisions for\neach signal based on deep Q-learning. Li et al. [152] proposed a\nmodel using LSTM and GCN to extract spatial-temporal traffic\nfeatures of intersection networks. LSTM processed variable-\nlength inputs and extracted valid features from historical data,\nwhile GCN handled the LSTM output, linking interactions of\nintersections. They incorporated imitation learning instead of\nreinforcement learning. Yang [134] introduced the Hierarchical\nGraph Multi-agent Mutual Information (HG-M2I) algorithm to\ngenerate optimal embeddings of traffic networks. This algorithm\nfused multi-granularity information from each agent\u2019s current\nand historical states to develop optimal TSC policies, measuring\nthe correlation between input states and output embeddings by\nmaximizing mutual information.\nDespite numerous studies attempting to incorporate temporal\nand spatial influences of surrounding intersections into the\ntarget intersection, they typically consider the spatial-temporal\ninformation separately. Wu et al. [137] proposed DynSTGAT,\nwhich uses TCN to simultaneously capture historical and\ncurrent spatial-temporal information. To address dynamically\nchanging traffic, Wang et al. [135] proposed MetaSTGAT, a\nmeta-learning model based on GATs that adapts to dynamic\ntraffic flow and fully utilizes the spatial-temporal characteris-\ntics of multi-intersections. Other literature also explores the\nexploitation of spatial and temporal information [135], [138].\n4) Discussion: How to construct a graph in traffic signal\ncontrol? In real-world scenarios, intersections often exhibit\ncomplicated structures. For instance, different roads can be\nconnected to the same intersections, each with a varying number\nof lanes. To address this complexity, we can create a directed\ntraffic light adjacency graph that reflects the spatial relationships\namong traffic lights. In this graph, nodes represent intersections,\nwith additional states indicating the presence of traffic lights.\nThe edges represent the roads connecting these intersections,\nwith directional attributes specifying whether roads are one-way\nor two-way. Moreover, additional features, such as the number\nof lanes on each road, the queue length and the number of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxii\nvehicles in the lane, and the average speed of vehicles, should\nbe considered. Models\u2019 performance can be assessed based on\nthe average travel time of vehicles [141].\nHow standard GNNs are modified for traffic signal\ncontrol? In the context of traffic signal control, the adaptation\nof standard GNNs involves addressing the unique challenges\nof dynamic traffic environments and the interdependencies\nbetween intersections. The first adaptation involves integrat-\ning GNNs with multi-agent reinforcement learning (RL) to\neffectively manage multiple intersections. Each intersection\nis controlled by an RL agent operating collaboratively within\na Markov Decision Process. By employing multi-agent RL,\nGNNs enable these agents to model the spatial relationships and\ngeographical structures of intersections, facilitating seamless\ninformation sharing and integration. The second adaptation\ninvolves combining GNNs with components that process\nsequential data, such as RNNs or LSTM networks. This\napproach captures temporal patterns by using GCNs or GATs\nto understand spatial relationships, while LSTMs handle the\ntemporal sequencing of traffic data, which is crucial for\npredicting future states. Furthermore, the incorporation of\nattention mechanisms allows for differential weighting of the\ninfluence of various intersections and temporal phases. These\nadaptations render GNNs as a dynamic solution capable of\nhandling the temporal and spatial complexities inherent in\ntraffic signal control systems.\nD. Transportation Safety\nTransportation safety aims to identify high-risk areas for\naccidents and incidents, crucial for pinpointing hotspots for\nfuture improvements. By understanding the factors contributing\nto these risks, researchers can develop targeted interventions\nand policies to mitigate them, thereby enhancing overall\ntransportation system safety.\n1) Spatial and Temporal Granularity: In transportation\nsafety, prediction models can be classified into four categories\nbased on temporal and spatial granularity. Prediction periods\ndivide models into long-term (day-level) [158], [162], [163]\nand mid-term (hour-level) [155], [160], [164]\u2013[166]. Prediction\nregions distinguish models into link-level [156], [161] and\nregion-level [153], [157], [167].\nZhou et al.\n[160] introduced a three-stage RiskOracle\nframework for minute-level citywide traffic accident prediction\nusing a Multi-task Differential Time-varying GCN (Multi-task\nDTGN) to model dynamic subregion correlations. Zhang et\nal.\n[158] proposed GraphCast, a multi-modal sensing and\nGNN-based approach for regional accident prediction, utilizing\nsocial media and remote sensing data to handle noisy and\nheterogeneous multi-modal data. Tran et al. [153] developed a\nMulti-structured GNN (MSGNN) for predicting traffic incidents\nacross entire networks, extracting area-wide features from\nvarious data sources for faster and more efficient incident\nprediction rather than link-level synchronization and map-\nmatching. Huang et al. [168] noted that many machine learning\ntechniques predict the number of traffic accidents in each\ncell of a discretized grid without considering the underlying\ngraph structure of road networks. Accurate link-level prediction\nrequires a complex fusion of heterogeneous data resources and\n\u201dmap-matching\u201d to represent all data with different granularity\nin the same map system.\n2) Spatial-temporal Correlation: The work by Yu et al.\n[156] and Yuan et al. [162] both highlight that existing methods\neither ignore spatial-temporal correlations or make predictions\nat a coarse-grained level without considering the underlying\ngraph structure of road networks.\nZhou et al. [159] proposed RiskSeq that addresses sporadic\nevents with a self-adaptive ranking method. It employs a\nDifferential Time-varying GCN (DT-GCN) enhanced with\nnode-wise proximity and signal-wise differential operations\nto capture dynamic traffic and accident correlations. The\nframework also includes a Context-Guided LSTM for decoding\nrisks across multiple spatial scales, focusing on spatiotemporal\nmulti-granularity urban traffic risk prediction. Yu et al. [156]\ntackled link-level accident prediction with a spatio-temporal\nconvolutional network framework. Their model predicts link-\nlevel incident risk by learning spatial-temporal features from\na road network graph, using graph convolutional operations\nto capture dynamic spatial and temporal variations. Liu et al.\n[154] proposed a multi-task learning framework (TAP) based on\nedge computing, using spatio-temporal variational graph auto-\nencoders to enhance prediction accuracy by analyzing dynamic\nspatial-temporal traffic data correlations and integrating external\nfactors. Wang et al. [157] introduced GSNet, a region-wide\naccident prediction model capturing geographical and semantic\nspatial-temporal correlations. The model features a weighted\nloss function to address the zero-inflation issue. Table.IV lists\nother spatial-temporal models [161], [169].\n3) Discussion: How to construct a graph in transporta-\ntion safety? A straightforward way to construct a graph\nin transportation safety is using zone-based methods, which\nmodels target area as several medium-sized regions or grids,\nrepresented as a vertex in the graph. Edges between vertices\nindicate the connectedness between subregions, existing when\ntraffic elements within those subregions have strong correlations.\nMore specifically, the traffic elements contributing to the vertex\nfeatures include static road network features (e.g., road types\nand number of lanes) and dynamic traffic features (e.g., traffic\nvolume and average speed). These elements help define the\nrelationships or connectedness within the urban graph.\nHow standard GNNs are modified for transportation\nsafety? When adapting GNN to transportation safety, a major\nchallenge is dealing with the zero-inflated problem due to the\nrarity of accidents. As we create zone-based graphs for GNN,\nthe frequency of zero values in the training data increases\nwith higher spatio-temporal resolution, making it harder to\nmake accurate predictions. Existing work addresses imbalanced\nanomaly data through various methods, including loss function\nhandling [155], [157] and data preprocessing techniques\nlike priori knowledge-based data enhancement [159], [160],\nnegative sample undersampling [156], and graph augmentation\n[170]. Wang et al. [170] employed graph augmentation and\ncontrastive loss to enhance latent representations in training,\nproposing an improved contrastive GNN-based framework\nfor traffic anomaly analysis in ITS. Yu et al. [156] used\nnegative sample undersampling to balance data by matching\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxiii\nTABLE IV: A Comprehensive Overview of Most Related Studies for Transportation Safety\nModel\nArticle\nYear\nDatasets\nSpatial Granularity\nTemporal Granularity\nSolution of zero-inflated\nSummary\nMSGNN\n[153]\n2023\nloop detectors,\nGPS probe data\n(Brisbane, Gold Coast)\nRegion Level\nShort-Term (1h)\nclustering-based data imputation\nMSGNN, as a sub-area level accident prediction model, captures spatio and\ntemporal relations and uses a data imputation approach for sparse datasets.\nTAP\n[154]\n2023\nReal world dataset\nRegion Level\nShort-Term (30min)\n-\nMulti-task learning framework (TAP) predicts traffic accidents using Spatio-\ntemporal Variational Graph Auto-Encoders, accelerated by edge computing\nGGCMT\n[155]\n2022\nReal world dataset\n(NYC)\nRegion Level\nMid-Term (3h)\nprior risk data enhancement method.\nComputer-vision-based GGCMT predicts accidents using a gated graph convol-\nutional multi-task model, improved with prior risk data enhancement methods.\nDSTGCN\n[156]\n2021\nReal world dataset\nLink Level\nShort-Term\nNegative sample undersampling method\nDSTGCN predicts traffic accidents from heterogeneous data, capturing spatio-\ntemporal correlations by Deep Spatio-Temporal Graph Convolutional Network.\nGSNet\n[157]\n2021\nReal world dataset\n(NYC, Chicago)\nRegion Level\nShort-term(1h)\nWeight loss function\nGSNet predicts traffic accidents by analyzing spatio-temporal and\nheterogeneous data, using a specialized loss function for rare events.\nGraphCast\n[158]\n2020\nReal world dataset\n(NYC)\nRegion Level\nLong-term (7day)\n-\nGraphCast, a multi-modal graph neural network, forecasts fine-grained\ntraffic risks in cities by combining social media and remote sensing data.\nRiskSeq\n[159]\n2020\nReal world dataset\n(NYC, Suzhou)\nRegion Level\nShort-term (10min)\npriori knowledge-based data enhancement\nRiskSeq, a fine-grained and multi-step accident prediction model, combi-\nnes Differential Time-varying GCN and hierarchical sequence learning.\nRiskOracle\n[160]\n2020\nReal world dataset\n(NYC, Suzhou)\nRegion Level\nShort-term (30min)\npriori knowledge-based data enhancement\nRiskOracle, a minute-level accident prediction model, combines Differential\nTime-varying Graph network, multi-task and region selection strategies.\nTA-STAN\n[161]\n2019\nReal world dataset\n(NYC)\nRegion Level\nMid-Term (12h)\n-\nTA-STAN predicts accidents by analyzing real-world traffic data, vehicle\ntypes, and external factors with a Spatial-Temporal Attention Network.\nnon-accident samples with accident samples, leveraging spatial-\ntemporal GNNs for more accurate predictions. Wang et al.\n[167] explored a chain-like triggering mechanism connecting\naccident occurrences, utilizing Spatial-Temporal Categorical\nGNNs (STC-GNN) to manage multi-dimensional and chain\neffects for fine-grained temporal accident prediction.\nE. Traffic Demand Forecasting\nTraffic demand prediction involves predicting the volume of\nusers needing transport to or from specific areas or locations,\nwhich is crucial for scheduling services efficiently and en-\nhancing overall transport system responsiveness. However, the\ngrowth of modern cities has caused an increase in traffic-related\nissues, which puts much pressure on public transportation\nsystems. To tackle this problem, ride-hailing services such as\nUber, Lyft, and DiDi, as well as bike-sharing services like\nMoBike have emerged as potential solutions [171], [172]. As\na result, there is now a pressing need for accurate traffic\ndemand prediction that can precisely forecast future crowd\ndemands, thus scheduling future transportation services and\nother downstream tasks. [173]. However, predicting traffic\ndemand poses challenges. On the one hand, human behavior,\ninfluenced by weather and special events, introduces variability\nthat makes accurate forecasting even more complicated. On\nthe other hand, integrating diverse data sources for compre-\nhensive analysis requires advanced models that are capable of\nprocessing complex, large-scale datasets in real-time.\n1) Traffic Zone-based Graph Methods: One of the pioneer-\ning works in utilizing graph learning methods for demand\nprediction tasks is Spatio-Temporal Multi-Graph Convolution\nNetwork (ST-MGCN) [174]. It proposes to exploit graph\nstructures from multiple perspectives to capture comprehen-\nsive information on the spatio-temporal characteristics of\ntraffic systems. Specifically, ST-MGCN builds the graphs of\nzones from three angles: a neighborhood graph based on the\nspatial proximity, a functionality graph defined by the POI\nsimilarity, and a transportation connectivity graph induced\nby road networks such as motorways, highways, or public\ntransportation systems like subways. A multi-graph convolution\nis then introduced to model the spatial dependencies between\nregions and provide informative representations for downstream\ndemand prediction tasks. Similarly, PGDRT [175] builds the\nzone-wise relational graph using three types of temporal\ncharacteristics: adjacent visual characteristics, periodic charac-\nteristics, and representative characteristics, to provide a more\ncomprehensive view of temporal features in traffic systems.\nTo fully exploit the rich information from multiple traffic\nsystems, Multiview Spatio-Temporal Graph Neural Networks\n(MSTGNN) [176] proposes a multiview graph that jointly\ndepicts the demand relationship between bus, metro, and taxi\ndemands. The multiview graph enables MSTGNN to capture the\ninteraction dependencies among the travel demands of different\ntransportation systems. An auxiliary loss is used to encourage\nthe consistency between graph features from multiple views\nand enhance the performance of TGCN modules.\n2) Spatio-temporal Graph-based Methods: Classical GNN\nmodels for traffic demand prediction treat the spatial depen-\ndency as a static graph and cannot depict dynamic features.\nHowever, in reality, the spatial dependencies between most\nnodes change over time, while others remain relatively constant.\nTo address this limitation, the Dynamical Spatio-Temporal\nGraph Neural Network (DSTGNN) [177] evaluates the stability\nof a node\u2019s spatial dependence based on the number of\ndissimilar neighbors and constructs a spatio-temporal graph that\nevolves over time. To encode the spatio-temporal information,\nthe model uses a spatio-temporal embedding network that\ncombines a Diffusion Convolution Neural Network (DCNN)\nwith a modified transformer.\n3) Dynamic Graph-based Methods: The traditional approach\nto modeling cities is to divide them into grid-like zones and con-\nstruct graphs based on these divisions. However, this approach\ncan lead to suboptimal solutions, and adapting to dynamic graph\nstructures remains challenging. A new solution called Deep\nMulti-View Spatio-temporal Virtual Graph Neural Network\n(DMVST-VGNN) [183] improves learning capabilities related\nto spatial dynamics and long-term temporal dependencies. The\nDMVST-VGNN method proposes a graph generation process\nthat provides a more flexible and fine-grained perspective on the\nspatio-temporal relationships between regions, as opposed to\nthe simplistic grid-based division of the map. Another proposal\nby Nazzal et al. [180] extends the idea of dynamic and flexible\ngraph structures to decentralized edge-computing scenarios\nand introduces a heterogeneous GNN-LSTM algorithm. This\nalgorithm is designed to handle dynamic taxi graphs where\ntaxis serve as nodes. The proposed heterogeneous GNN-LSTM\nstructure has demonstrated the ability to capture dynamic\ndecentralized graph structures and has shown promising results\nin taxi-level demand and supply forecasting.\n4) Improvements on Graph Encoders: The traditional graph\nconvolution network has limited capability to represent the\ncomplex information in traffic zone graphs. However, some\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxiv\nTABLE V: A Comprehensive Overview of Most Related Studies for Demand Prediction\nModel\nArticle\nYear\nPrediction Task\nGraph Views\nGNN Module\nTemporal Module\nSummary\nPGDRT\n[175]\n2023\nTaxi Passenger\nNeighborhood, Function, Connectivity\nGCN\nConvLSTM\nPGDRT considers a region\u2019s unique characteristics and the influence of\nregions on the model of the dependent relationship between regions.\nMSTGNN\n[176]\n2023\nBus, Metro, Taxi\nNeighborhood, Connectivity\nGCN\nTemporal GCN\nMSTGNN uses a multiview graph consisting of bus, metro, and taxi\nviews, with each view containing both local and global graphs.\nSTGMT\n[178]\n2023\nTaxi & Highway\nTraffic Network\nNode2Vec\nMulti-head Attention\nSTGMT combines Multi-head Temporal Attention (MTA) and Multi-\nhead Temporal Interactive Attention (MTIA) for temporal features.\nPAG-TSN\n[179]\n2023\nRide-hailing\nDistance, POI relation\nBAT-GCN\nPA-GRU\nPAG-TSN uses a bicomponent attention GCN and a periodic attent-\nional GRU to integrate the extracted spatio-temporal information.\nHetGNN-LSTM\n[180]\n2023\nTaxi\nDecentralized taxi graph\nHetGNN\nLSTM\nHetGNN-LSTM proposes a semi-decentralized approach utilizing\nmultiple cloudlets, moderately sized storage, and computation devices.\nMFGCN\n[181]\n2023\nRide-hailing\nOD network\nMODGCN\nTAS-LSTM\nMFGCN is a multimodal fusion GCN that consists of a multimodal\nmodule to incorporate weather and temporal activity patterns.\nSGCNPM\n[182]\n2023\nDockless Bike-Sharing\nDistance, Function, Interconnectio\nMGCN\nLSTM\nSGCNPM considers time, built environment, and weather to create\na prediction method considering the influence of multiple factors.\nDSTGNN\n[177]\n2022\nTaxi & Bike\nSpatial dependency\nDCNN\nMulti-head Attention\nDSTGNN builds spatial graphs based on the stability of the node\u2019s\nspatial dependence to capture the dynamical relationship.\nDMVST-VGNN\n[183]\n2022\nRide-hailing\nMulti-view Graph Generation\nGAT\nMulti-head Attention\nThe Model integrates 1D CNN, Multi-Graph Attention Neural Networks,\nand Transformer to construct multiview spatio-temporal information.\nST-MGCN\n[174]\n2019\nRide-hailing\nNeighborhood, Function, Connectivity\nChebNet\nRNN\nST-MGCN uses GNNs to model non-Euclidean pair-wise correlations\nbetween different regions by designing a spatio-temporal multi-graph.\nworks aim to enhance the expressiveness of graph encoders.\nSTGMT [178] proposes the Sandwich-Transformer for pro-\ncessing spatio-temporal traffic graphs, which is composed\nof a Multi-head Temporal Attention (MTA) and a Multi-\nhead Temporal Interactive Attention (MTIA). PAG-TSN [179]\nconstructs a Bicomponent Attention Graph Convolution model\n(BAT-GCN) and a periodic attentional gated recurrent unit\nmodel to capture geographical relationships and temporal\nfeatures of different periods, respectively. While previous\nresearch primarily concentrates on processing plain time-\nseries traffic demand data for predictions, it is essential to\nrecognize that contextual information and multimodal attributes,\nsuch as weather conditions, significantly impact ride-hailing\nand other public traffic systems. To tackle these challenges,\nMultimodal Fusion Graph Convolutional Network (MFGCN)\n[181] introduces an innovative Multimodal Fusion Graph\nConvolutional Network for traffic demand prediction. MFGCN\nincorporates a Multimodal Origin-Destination GCN (MOD-\nGCN) that comprises three GCNs to capture spatial patterns\nand a Multimodal Attribute Enhancement (MAE) module for\nintegrating dynamic weather and metadata. SGCNPM [182]\nutilizes multiple modules that consist of GCN and LSTM\noperators to model the multiple factors in a dynamic traffic\nsystem, including time periods, built environment, and weather,\nto predict the short-term demand of a dockless bike-sharing\nsystem. A comprehensive overview of most related studies for\ndemand prediction can be found in Table.V.\n5) Discussion: How to construct a graph in demand\nprediction? In demand prediction, in addition to the zone-\nbased construction approach discussed in the transportation\nsafety section, we can also create a traffic graph based on\nnetwork topology, where nodes represent stations, bus stops,\netc., and edges can be constructed based on the correlation\namong the features of the nodes. Research has shown the\neffectiveness of location-based graphs, but constructing graphs\nfrom multiple perspectives has proven to introduce more infor-\nmative relationships between geographical zones. For instance,\nthe Spatio-Temporal Multi-Graph Convolution Network (ST-\nMGCN) [174] builds zone graphs from three perspectives: a\nneighborhood graph based on geographical distances between\nneighborhoods, a functionality graph derived from the similarity\nbetween POI vectors, and a transportation connectivity graph\ninduced by road networks such as motorways, highways, or\npublic transportation systems like subways.\nHow standard GNNs are modified for demand prediction?\nIn the field of demand prediction, it is essential to adapt\nGNNs to accommodate the spatio-temporal characteristics of\nthe input flow graph. This involves incorporating temporal and\nspatial modules and other necessary modifications. Additionally,\nbased on the enhancement of the multi-view approach we have\nexplored [175], [176], [183], a multi-view fusion mechanism\nis required to aggregate from different representations. Further-\nmore, to capture the dynamic nature of demands, GNNs can\nbe combined with other sequence models such as RNNs [175],\n[179], [180] and self-attention layers [177], [178].\nF. Parking Management\nParking management has become a significant concern due\nto the limited resources of parking spaces and the increasing\ntraffic pressure. One crucial research area of intelligent parking\nmanagement is predicting parking availability, which involves\nforecasting future parking occupancy. Being able to predict\nparking availability on a city-wide scale is crucial for develop-\ning effective Parking Guidance and Information (PGI) systems,\nsuch as Baidu Map [184] and Google Map [185].\nPredicting the availability of parking spaces faces several\nchallenges, such as the non-Euclidean spatial autocorrelation\nbetween parking lots, the dynamic temporal autocorrelation\nwithin and between parking lots, and the information scarcity\ndue to the lack of real-time data obtained from sensors to\ndetermine parking availability. GNNs have been recognized\nas an effective approach for processing spatial-temporal and\ngraph-based structures, addressing the mentioned challenges,\nand providing more accurate prediction accuracy.\nAs one of the pioneering works on modeling the parking\navailability prediction with graph-based models, SHARE [186]\nand its variant SHARE-X [187] proposes a Semi-supervised\nHierarchical Recurrent Graph Neural Network to analyze\nspatio-temporal parking data. Specifically, SHARE proposes\na hierarchical graph convolution module that captures non-\nEuclidean spatial correlations between parking lots. It consists\nof two blocks: a contextual graph convolution block for local\nspatial dependencies and a soft clustering graph convolution\nblock for global spatial dependencies. SHARE-X extends the\nidea of SHARE to address the lack of real-time sensors in real-\nworld scenarios. Particularly, It leverages a parking availability\napproximation module to estimate parking availability for\nparking lots without sensor monitoring.\nTo better depict the strong spatiotemporal contextual auto-\ncorrelation between vacant parking spaces, dConvLSTM-DCN\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxv\n[188] analyzed the historical zone-wise parking space data and\nfound that there is both a temporal correlation within each\nparking lot and a spatial correlation among different parking\nspaces. Based on this observation, Feng et al. proposed a deep\nlearning framework called dual Convolutional Long Short-Term\nMemory with Dense Convolutional Network (dConvLSTM-\nDCN) to predict the availability of vacant parking places in\nthe short-term (within 30 minutes) and long-term (over 30\nminutes) zone-wisely. The framework consists of two parallel\nConvLSTM components that capture the spatial correlations\namong parking lots and provide an informative representation\nof the prediction process.\nThe traditional methods of obtaining real-time on-street\nparking information rely heavily on densely deployed sensors.\nTherefore, the high costs of current parking availability predic-\ntion models have made it difficult to use them in many cities and\nareas. To tackle this issue and avoid the high costs of deploying\nnew sensors, MePark [189] aims to predict real-time on-street\nparking availability across a city by using existing infrastructure\nand easily accessible data without relying solely on specially\ndeployed sensors. More specifically, MePark uses an iterative\nmechanism to combine the aggregated inflow and individual\nparking duration predictions to exploit the transaction data\nadequately. Additionally, it extracts distinctive features from\nmultiple data sources, combining the MGCN and the LSTM\nnetwork to capture complex spatio-temporal correlations.\n1) Discussion: How to construct a graph in parking man-\nagement? Constructing the graph in the parking management\nsystem follows a similar approach to previous research domains,\nsuch as transportation safety and demand prediction. The zone-\nbased construction method can predict parking availability in a\nmedium-sized grid or area, while the network topology-based\nmethod can predict availability in specific parking lots based\non their geometric information.\nHow standard GNNs are modified for parking man-\nagement? Firstly, the dynamic temporal autocorrelation and\nnon-Euclidean relationship between parking lots is crucial for\naccurate short-term parking availability prediction. Many exist-\ning efforts suggest integrating recurrent network structures, such\nas LSTMs, into GNN architectures [186], [188], [189], while\nsome methods directly reform GNNs with recurrent structures,\nsuch as SHARE and SHARE-X [187]. Secondly, information\nscarcity poses another challenge, prompting researchers to\nexplore GNNs to exploit non-sensor data, like transaction data,\nto harness the prediction potential [189].\nV. CHALLENGES AND FUTURE DIRECTIONS\nAfter analyzing the current studies on GNNs in ITS, we\ndiscuss the challenges and future directions for applying GNNs\nin ITS. This is important to identify any gaps that need to be\naddressed and to provide insights for further research.\nA. Research Challenges\n1) Data: Constructing datasets is one of the primary\nchallenges when applying GNNs in ITS. Data privacy is a\nmajor concern when collecting information from traffic sensors\nor GPS data. Currently, there are limited publicly available\ndata sources, such as Data.gov, The University of Sydney\u2019s\nIntelligent Vehicles and Safety Systems, and the Connected\nVehicle DataSets from the Safety Pilot Model Deployment\n[18]. Some researchers have explored multi-modal models to\naccess data from richer sources like social media [95], [181],\n[190], [191]. However, these sources often face issues related\nto credibility and a lack of valuable information. As a result,\ngenerating a large, high-quality, and comprehensive dataset for\nITS remains a formidable task, requiring continued effort to\naddress data privacy concerns and improve data sources.\n2) Model: Domain-specific Model Design. The transporta-\ntion network is a complex system that includes various nodes\nand edges, such as roads, intersections, and vehicles. Designing\nGNN models in ITS that can efficiently learn from such a\nheterogeneous and complex structure requires significant effort.\nThe design of GNN applications in ITS heavily depends on the\nspecific goals of the corresponding applications, as different\ngoals require using different graph models and construction\ntechniques. For instance, GNNs are commonly used in traffic\nforecasting and travel demand modeling to predict features or\nvariables over graph nodes. While in areas such as traffic signal\ncontrol, GNNs focus on learning control policies or unraveling\nagent interactions that involve learning or predicting over edges\nor the entire graph. Besides, GNNs face different challenges in\nvarious transportation domains. The pure GNN models can not\neffectively solve the problem, so some scholars have explored\nthe potential of combining GNNs with other approaches.\nFor instance, in decision-making problems, such as traffic\nsignal control, reinforcement learning is an effective technique.\nWhen multiple intersections interact, multi-agent reinforcement\nlearning methods combined with GNNs have been proposed\n[139], [139], [147]. In some particular scenarios, such as traffic\naccident prediction, positive samples like accidents can be rare\nwhen predicting within a fine-grained granularity. To improve\naccuracy, we can use data augmentation techniques like a\npriori knowledge-based data enhancement [159], [160] and\nnegative sample undersampling methods [156]. Nearly every\ntransportation domain has its own domain-specific problems\nand unique characteristics. Therefore, combining GNNs and\nother techniques requires nuanced graph construction, tailored\nproblem analysis, and painstaking design.\nDynamic Spatio-temporal Dependency. Utilizing GNNs\nto model spatio-temporal dependencies in ITS presents a\nsignificant challenge [26]. This difficulty arises from the need\nto precisely capture both the complex and dynamic spatial\ninteractions within the transportation network and the temporal\ndynamics that reflect the constantly changing traffic patterns.\nTransportation networks frequently exhibit dynamic spatial\ndependencies, as the graph structure can evolve over time due\nto the constantly changing urban environment. For example,\nin trajectory prediction, it is crucial to identify key agents\nand objects, such as vehicles, cyclists, and pedestrians, that\ncan influence the predicted trajectory. Consequently, a graph\nframework capable of adapting to these real-time changes\nis necessary to maintain accurate predictions. Meanwhile, in\nterms of temporal dependencies, traffic conditions at any given\nmoment are shaped by a multitude of past events. Accurately\ncapturing these long-term dependencies is crucial for precise\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxvi\nforecasting, but it poses significant computational challenges\nand demands sophisticated memory mechanisms within the\nmodel [68], [192]. Furthermore, real-time data processing is\nvital for practical applications in ITS, adding another layer of\ncomplexity [193], [194]. The model must integrate and process\nthis multifaceted data while adapting in an environment marked\nby continuous change and uncertainty. Therefore, effectively\nmodeling spatio-temporal dependencies remains a pivotal yet\nchallenging aspect of utilizing GNNs in ITS.\nRobustness, Reliability, Interpretability. Deep learning\nhas faced criticism for its lack of interpretability and black-\nbox nature, which makes it difficult to assess the rationale\nbehind recommendations made by graph-based deep learning\napproaches in transportation safety and other high-stakes fields.\nFurthermore, it is essential to ensure that GNN models remain\nreliable in large-scale real-world applications, such as during\nrush hours, sensor failures, or cyber-attacks [195]. As we\nstrive to enhance model performance, we must also be vigilant\nabout potential failures and undetected anomalies. Additionally,\nscalability remains a critical concern. Current GNN frameworks,\nincluding those based on TensorFlow, PyTorch, DGL, and PyG,\nface limitations in scalability, which restrict their application\nto large-scale graphs due to inadequate system support [27].\n3) Computation: Processing, storing, and transmitting vast\namounts of data has become increasingly critical in today\u2019s\nextensive data landscape, particularly within ITS. While GNNs\nand deep learning techniques are extensively utilized in ITS,\nthey face substantial challenges due to their high computational\ndemands. These challenges are amplified when performing real-\ntime or near-real-time inference and processing large volumes\nof data from extensive camera networks. Additionally, the\nlimited resources of IoT devices\u2014such as constrained memory\nand computing power\u2014complicate these issues further. To\naddress these challenges, researchers have proposed various\nsolutions, including edge computing, graph sampling, hardware\nacceleration, and optimized algorithms.\nB. Future Directions\n1) More Integration of Advanced Techniques: As noted,\nGNNs are highly effective for capturing spatial-temporal rela-\ntionships and making inferences on graph-based data structures.\nHowever, since different problems have unique characteristics\nand challenges, it is crucial to design tailored models for each\nspecific issue. Additionally, integrating other techniques into\nGNN frameworks can significantly enhance model performance\nand facilitate practical applications. For example, incorporating\nthe edge learning paradigm [196] into GNN frameworks can\naddress the storage, memory, and computational limitations of\ndata-producing devices. This approach allows distributed edge\ndevices to collaboratively train models and perform inferences,\nthus ensuring privacy and security [196]. Similarly, leveraging\ntransfer learning [197] and meta-learning [135] can significantly\nimprove a model\u2019s adaptability to different cities with varying\ntraffic patterns. In conclusion, combining GNNs with advanced\ntechniques such as reinforcement learning, transfer learning,\nmeta-learning, generative adversarial networks (GANs), semi-\nsupervised learning, and Bayesian networks opens new avenues\nfor addressing domain-specific problems and challenges. This\nsynergistic approach not only leads to more robust and versatile\nsolutions but also unveils exciting possibilities for solving\ncomplex, real-world problems across various domains. As\nresearch continues, it is crucial to explore these integrations\nfurther, pushing the boundaries of what can be achieved with\nGNNs and their allied technologies.\n2) More Expanding Applications of GNNs: Currently, most\nresearch on GNNs in ITS focuses on traffic prediction. However,\nsignificant untapped potential remains for further developing\nGNNs and exploring their broader applications within this\nfield. To enhance GNNs, efforts should be directed towards\nimproving their efficiency, robustness, and generality. One\napproach is to implement multi-modal learning [95], [198],\nwhich allows models to integrate a richer set of contextual\ninformation. Additionally, incorporating more complex graph\nstructures, such as heterogeneous graphs [180] and hypergraphs\n[116], can facilitate handling larger and more intricate graph\nstructures. Furthermore, exploring GNN applications in other\nITS research domains also holds promise. For example, the\n3D structural understanding required for autonomous vehicles\npresents a unique opportunity for GNNs, especially compared\nto traditional transformer architectures [99], [100] used in point\ncloud processing, which often lack efficiency. By combining\ngraph convolution with self-attention mechanisms, we can\nsignificantly enhance feature extraction and capture both local\nand global contexts effectively. This integration has the potential\nto greatly improve GNN performance in ITS applications [98].\nWhile we have covered various domains, from traffic prediction\nto traffic safety, additional areas remain to explore, such as\nroute planning, urban land-use planning, and traffic pattern\nrecognition. Further investigation into these applications of\nGNNs could yield new insights and enhance their performance\nacross a broader range of ITS scenarios.\n3) More Comprehensive Experiments: Currently, some\nresearch experiments in the field of ITS rely on simulators.\nHowever, data generated by traffic simulation software may not\nalways accurately reflect real-world conditions due to factors\nsuch as variations in driver behavior and alternative route\nplanning [199]. Furthermore, even when models are evaluated\nusing real-world data, such testing is often limited in scale,\nor the model\u2019s running time may not be fully reported. These\nlimitations do not necessarily ensure the model\u2019s reliability,\nrobustness, or generalization ability to real-world scenarios.\nAccording to work by Shi et al. [200], some models, such as\nthose based on Deep Q-Networks (DQNs) for reinforcement\nlearning, experience performance degradation when dealing\nwith large-scale road networks or incomplete data, complicating\ntheir generalization. Therefore, it is essential to conduct more\ncomprehensive experiments using large-scale real-world data\nto fully assess and validate the effectiveness of these models.\nVI. CONCLUSION\nWith the rapid advancement of deep learning, GNNs have\nemerged as a promising tool in ITS. However, most existing\nresearch on GNNs in ITS has concentrated on traffic forecasting\nwhile overlooking other critical areas, such as autonomous\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxvii\nvehicles and transportation safety. In this work, we have\nreviewed and analyzed representative papers from 2018 to\n2023 that explore various applications of GNNs across six\ndomains within ITS. We have summarized and categorized\nthese studies based on their research focus, the graph methods\nemployed, and the domain-specific challenges encountered,\nand presented the findings in detailed tables and lists. Our\nanalysis reveals that while many studies are limited to specific\nfunctionalities of GNNs, such as modeling graph-structured\ndata and capturing spatio-temporal relationships, a vast potential\nis waiting to be fully realized. GNNs can be expanded to other\nareas of ITS, opening up new possibilities and avenues for\nresearch and development. Additionally, we have identified\ncommon challenges that need to be addressed when applying\nGNNs in ITS, including issues related to data, model, and\ncomputation. We also highlight future directions for GNN\nresearch in ITS, emphasizing the crucial role of integrating\nGNNs with other techniques. This approach can lead to more\ncomprehensive solutions, broadening the application scope of\nGNNs and fostering more extensive experiments.\nACKNOWLEDGMENT\nThe authors are grateful to the anonymous reviewers for crit-\nically reading this article and for giving important suggestions\nto improve this article.\nThis paper is partially supported by the National Key\nResearch and Development Program of China with Grant\nNo. 2023YFC3341203, and the National Natural Science\nFoundation of China (NSFC Grant Numbers 62306014 and\n62276002).\nREFERENCES\n[1] J. Wootton, A. Garcia-Ortiz, and S. Amin, \u201cIntelligent transportation\nsystems: a global perspective,\u201d Mathematical and computer modelling,\nvol. 22, no. 4-7, 1995.\n[2] M. Veres and M. Moussa, \u201cDeep learning for intelligent transportation\nsystems: A survey of emerging trends,\u201d TITS, vol. 21, no. 8, 2019.\n[3] S. Lee and D. B. Fambro, \u201cApplication of subset autoregressive\nintegrated moving average model for short-term freeway traffic volume\nforecasting,\u201d TRR, vol. 1678, no. 1, 1999.\n[4] B. M. Williams, \u201cMultivariate vehicular traffic flow prediction: evalua-\ntion of arimax modeling,\u201d TRR, vol. 1776, no. 1, 2001.\n[5] P. Jian John Lu PHD and L. Rajaram, \u201cEvaluation of intelligent\ntransportation system operations using logistic regression models,\u201d\nInstitute of Transportation Engineers. ITE Journal, vol. 83, no. 3,\np. 40, 2013.\n[6] C.-H. Wu, J.-M. Ho, and D.-T. Lee, \u201cTravel-time prediction with support\nvector regression,\u201d T-ITS, vol. 5, no. 4, pp. 276\u2013281, 2004.\n[7] H. Chang, Y. Lee, B. Yoon, and S. Baek, \u201cDynamic near-term traffic\nflow prediction: system-oriented approach based on past experiences,\u201d\nIET intelligent transport systems, vol. 6, no. 3, pp. 292\u2013305, 2012.\n[8] X. Fan, C. Xiang, L. Gong, X. He, Y. Qu, S. Amirgholipour, Y. Xi,\nP. Nanda, and X. He, \u201cDeep learning for intelligent traffic sensing and\nprediction: recent advances and future challenges,\u201d CCF Transactions\non Pervasive Computing and Interaction, vol. 2, 2020.\n[9] W. Jiang and J. Luo, \u201cGraph neural network for traffic forecasting: A\nsurvey,\u201d ESWA, vol. 207, 2022.\n[10] S. Rahmani, A. Baghbani, N. Bouguila, and Z. Patterson, \u201cGraph neural\nnetworks for intelligent transportation systems: A survey,\u201d TITS, 2023.\n[11] Z. Cui, K. Henrickson, R. Ke, and Y. Wang, \u201cTraffic graph convolutional\nrecurrent neural network: A deep learning framework for network-scale\ntraffic learning and forecasting,\u201d TITS, vol. 21, no. 11, 2019.\n[12] D. A. Tedjopurnomo, Z. Bao, B. Zheng, F. M. Choudhury, and A. K.\nQin, \u201cA survey on modern deep neural network for traffic prediction:\nTrends, methods and challenges,\u201d TKDE, vol. 34, no. 4, 2020.\n[13] X. Yin, G. Wu, J. Wei, Y. Shen, H. Qi, and B. Yin, \u201cDeep learning\non traffic prediction: Methods, analysis, and future directions,\u201d TITS,\nvol. 23, no. 6, 2021.\n[14] W. Jiang, J. Luo, M. He, and W. Gu, \u201cGraph neural network for traffic\nforecasting: The research progress,\u201d ISPRS International Journal of\nGeo-Information, vol. 12, no. 3, 2023.\n[15] E. I. Vlahogianni, M. G. Karlaftis, and J. C. Golias, \u201cShort-term traffic\nforecasting: Where we are and where we\u2019re going,\u201d Transportation\nResearch Part C: Emerging Technologies, vol. 43, 2014.\n[16] K. Lee, M. Eo, E. Jung, Y. Yoon, and W. Rhee, \u201cShort-term traffic\nprediction with deep neural networks: A survey,\u201d IEEE Access, vol. 9,\n2021.\n[17] A. K. Haghighat, V. Ravichandra-Mouli, P. Chakraborty, Y. Esfandiari,\nS. Arabi, and A. Sharma, \u201cApplications of deep learning in intelligent\ntransportation systems,\u201d Journal of Big Data Analytics in Transportation,\nvol. 2, 2020.\n[18] J. Guerrero-Iba\u02dcnez, J. Contreras-Castillo, and S. Zeadally, \u201cDeep\nlearning support for intelligent transportation systems,\u201d Transactions\non Emerging Telecommunications Technologies, vol. 32, no. 3, 2021.\n[19] J. Liu, N. Wu, Y. Qiao, and Z. Li, \u201cA scientometric review of research on\ntraffic forecasting in transportation,\u201d IET Intelligent Transport Systems,\nvol. 15, no. 1, 2021.\n[20] M. S. Ahmed and A. R. Cook, Analysis of freeway traffic time-series\ndata by using Box-Jenkins techniques, 1979, no. 722.\n[21] M. Levin and Y.-D. Tsao, \u201cOn forecasting freeway occupancies and\nvolumes (abridgment),\u201d TRR, no. 773, 1980.\n[22] B. M. Williams and L. A. Hoel, \u201cModeling and forecasting vehicular\ntraffic flow as a seasonal arima process: Theoretical basis and empirical\nresults,\u201d Journal of Transportation Engineering, vol. 129, no. 6, 2003.\n[23] Y. Kamarianakis and P. Prastacos, \u201cForecasting traffic flow conditions\nin an urban network: Comparison of multivariate and univariate\napproaches,\u201d TRR, vol. 1857, no. 1, 2003.\n[24] L. Vanajakshi and L. R. Rilett, \u201cA comparison of the performance of\nartificial neural networks and support vector machines for the prediction\nof traffic speed,\u201d in IV, 2004, pp. 194\u2013199.\n[25] E. I. Vlahogianni, J. C. Golias, and M. G. Karlaftis, \u201cShort-term traffic\nforecasting: Overview of objectives and methods,\u201d Transport reviews,\nvol. 24, no. 5, 2004.\n[26] K.-H. N. Bui, J. Cho, and H. Yi, \u201cSpatial-temporal graph neural network\nfor traffic forecasting: An overview and open research issues,\u201d Applied\nIntelligence, vol. 52, no. 3, 2022.\n[27] Y. Li, D. Yu, Z. Liu, M. Zhang, X. Gong, and L. Zhao, \u201cGraph neural\nnetwork for spatiotemporal data: methods and applications,\u201d arXiv\npreprint arXiv:2306.00012, 2023.\n[28] A. Auer, S. Feese, S. Lockwood, and B. A. Hamilton, \u201cHistory of\nintelligent transportation systems.\u201d Tech. Rep., 2016.\n[29] R. Mundy, Management of Public Transportation Systems in the\n1980s:(the Emergence of Paraprivate Transportation).\nDepartment of\nMarketing and Transportation, College of Business, 1981.\n[30] R. C. Bushnell, J. T. Low, and J. B. Wiley, \u201cTransportation network\nmodels: Past problems and prospects for the 1980s,\u201d International\nJournal of Physical Distribution & Materials Management, vol. 11,\nno. 8, 1981.\n[31] J. Zhang, F.-Y. Wang, K. Wang, W.-H. Lin, X. Xu, and C. Chen, \u201cData-\ndriven intelligent transportation systems: A survey,\u201d TITS, vol. 12, no. 4,\n2011.\n[32] R. Jia, P. Jiang, L. Liu, L. Cui, and Y. Shi, \u201cData driven congestion\ntrends prediction of urban transportation,\u201d IEEE Internet of Things\nJournal, vol. 5, no. 2, 2017.\n[33] R. G\u00a8unther, T. Wenzel, M. Wegner, and R. Rettig, \u201cBig data driven\ndynamic driving cycle development for busses in urban public trans-\nportation,\u201d Transportation Research Part D: Transport and Environment,\nvol. 51, 2017.\n[34] A. Saroj, S. Roy, A. Guin, M. Hunter, and R. Fujimoto, \u201cSmart city\nreal-time data-driven transportation simulation,\u201d in Winter Simulation\nConference, 2018.\n[35] Y. Wang and Z. Zeng, Data-driven solutions to transportation problems,\n2018.\n[36] J. N. Njoku, C. I. Nwakanma, G. C. Amaizu, and D.-S. Kim, \u201cProspects\nand challenges of metaverse application in data-driven intelligent\ntransportation systems,\u201d IET Intelligent Transport Systems, vol. 17,\nno. 1, 2023.\n[37] Y. Lin, P. Wang, and M. Ma, \u201cIntelligent transportation system (its):\nConcept, challenge and opportunity,\u201d in BigDataSecurity, 2017.\n[38] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl,\n\u201cNeural message passing for quantum chemistry,\u201d in International\nconference on machine learning.\nPMLR, 2017, pp. 1263\u20131272.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxviii\n[39] J. Lee, I. Lee, and J. Kang, \u201cSelf-attention graph pooling,\u201d in Interna-\ntional conference on machine learning, 2019, pp. 3734\u20133743.\n[40] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec,\n\u201cHierarchical graph representation learning with differentiable pooling,\u201d\nAdvances in neural information processing systems, vol. 31, 2018.\n[41] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, \u201cAn end-to-end deep\nlearning architecture for graph classification,\u201d in Proceedings of the\nAAAI conference on artificial intelligence, vol. 32, no. 1, 2018.\n[42] Z. Wu, S. Pan, F. Chen, G. Long, C. Zhang, and S. Y. Philip, \u201cA\ncomprehensive survey on graph neural networks,\u201d TNNLS, vol. 32,\nno. 1, 2020.\n[43] J. Zhou, G. Cui, S. Hu, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, and\nM. Sun, \u201cGraph neural networks: A review of methods and applications,\u201d\nAI Open, vol. 1, 2020.\n[44] Z. Zhang, P. Cui, and W. Zhu, \u201cDeep learning on graphs: A survey,\u201d\nTKDE, vol. 34, no. 1, 2020.\n[45] S. Abadal, A. Jain, R. Guirado, J. L\u00b4opez-Alonso, and E. Alarc\u00b4on,\n\u201cComputing graph neural networks: A survey from algorithms to\naccelerators,\u201d CSUR, vol. 54, no. 9, 2021.\n[46] T. N. Kipf and M. Welling, \u201cVariational graph auto-encoders,\u201d arXiv\npreprint arXiv:1611.07308, 2016.\n[47] E. L. Manibardo, I. La\u02dcna, and J. Del Ser, \u201cDeep learning for road\ntraffic forecasting: Does it make a difference?\u201d IEEE Transactions on\nIntelligent Transportation Systems, vol. 23, no. 7, pp. 6164\u20136188, 2021.\n[48] J. Zhang, J. Jin, J. Tang, and Z. Qu, \u201cFptn: Fast pure transformer\nnetwork for traffic flow forecasting,\u201d in ICANN, 2023.\n[49] Y. Zhao, X. Luo, W. Ju, C. Chen, X.-S. Hua, and M. Zhang, \u201cDynamic\nhypergraph structure learning for traffic flow forecasting,\u201d in ICDE,\n2023.\n[50] S. Lan, Y. Ma, W. Huang, W. Wang, H. Yang, and P. Li, \u201cDstagnn:\nDynamic spatial-temporal aware graph neural network for traffic flow\nforecasting,\u201d in ICML, 2022.\n[51] C. Chen, Y. Liu, L. Chen, and C. Zhang, \u201cBidirectional spatial-temporal\nadaptive transformer for urban traffic flow forecasting,\u201d TNNLS, 2022.\n[52] M. Li and Z. Zhu, \u201cSpatial-temporal fusion graph neural networks for\ntraffic flow forecasting,\u201d in AAAI, 2021.\n[53] L. Bai, L. Yao, C. Li, X. Wang, and C. Wang, \u201cAdaptive graph\nconvolutional recurrent network for traffic forecasting,\u201d in NeurIPS,\n2020.\n[54] J. Yi and J. Park, \u201cHypergraph convolutional recurrent neural network,\u201d\nin KDD, 2020.\n[55] C. Song, Y. Lin, S. Guo, and H. Wan, \u201cSpatial-temporal synchronous\ngraph convolutional networks: A new framework for spatial-temporal\nnetwork data forecasting,\u201d in AAAI, 2020.\n[56] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, \u201cAttention based spatial-\ntemporal graph convolutional networks for traffic flow forecasting,\u201d in\nAAAI, 2019.\n[57] J. Li, Z. Han, H. Cheng, J. Su, P. Wang, J. Zhang, and L. Pan, \u201cPredicting\npath failure in time-evolving graphs,\u201d in KDD, 2019.\n[58] Y. Li, R. Yu, C. Shahabi, and Y. Liu, \u201cDiffusion convolutional recurrent\nneural network: Data-driven traffic forecasting,\u201d in ICLR, 2018.\n[59] G. Khodabandelou, W. Kheriji, and F. H. Selem, \u201cLink traffic speed\nforecasting using convolutional attention-based gated recurrent unit,\u201d\nApplied Intelligence, vol. 51, 2021.\n[60] L. Han, B. Du, L. Sun, Y. Fu, Y. Lv, and H. Xiong, \u201cDynamic and\nmulti-faceted spatio-temporal deep learning for traffic speed forecasting,\u201d\nin KDD, 2021.\n[61] C. Zhang, S. Zhang, J. James, and S. Yu, \u201cFastgnn: A topological\ninformation protected federated learning approach for traffic speed\nforecasting,\u201d IEEE Transactions on Industrial Informatics, vol. 17,\nno. 12, 2021.\n[62] J. Liu, G. P. Ong, and X. Chen, \u201cGraphsage-based traffic speed\nforecasting for segment network with sparse data,\u201d TITS, vol. 23, no. 3,\n2020.\n[63] P. Wu, Z. Huang, Y. Pian, L. Xu, J. Li, and K. Chen, \u201cA combined\ndeep learning method with attention-based lstm model for short-term\ntraffic speed forecasting,\u201d JAT, vol. 2020, 2020.\n[64] G. Guo and W. Yuan, \u201cShort-term traffic speed forecasting based on\ngraph attention temporal convolutional networks,\u201d Neurocomputing, vol.\n410, 2020.\n[65] K. Zhang, L. Wu, Z. Zhu, and J. Deng, \u201cA multitask learning model\nfor traffic flow and speed forecasting,\u201d IEEE Access, vol. 8, 2020.\n[66] D. Wang, J. Zhu, Y. Yin, J. Ignatius, X. Wei, and A. Kumar, \u201cDynamic\ntravel time prediction with spatiotemporal features: using a gnn-based\ndeep learning method,\u201d Annals of Operations Research, 2023.\n[67] L. Tran, M. Y. Mun, M. Lim, J. Yamato, N. Huh, and C. Shahabi,\n\u201cDeeptrans: a deep learning system for public bus travel time estimation\nusing traffic forecasting,\u201d VLDB, vol. 13, no. 12, 2020.\n[68] A. Roy, K. K. Roy, A. Ahsan Ali, M. A. Amin, and A. M. Rahman, \u201cSst-\ngnn: simplified spatio-temporal traffic forecasting model using graph\nneural network,\u201d in PAKDD, 2021.\n[69] J. Ma, J. Chan, G. Ristanoski, S. Rajasegarar, and C. Leckie, \u201cBus\ntravel time prediction with real-time traffic information,\u201d Transportation\nResearch Part C: Emerging Technologies, vol. 105, 2019.\n[70] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural\nComputation, vol. 9, no. 8, 1997.\n[71] K. Cho, B. van Merrienboer, D. Bahdanau, and Y. Bengio, \u201cOn the\nproperties of neural machine translation: Encoder-decoder approaches,\u201d\nin SSST@EMNLP, 2014.\n[72] W. Ju, Y. Zhao, Y. Qin, S. Yi, J. Yuan, Z. Xiao, X. Luo, X. Yan, and\nM. Zhang, \u201cCool: A conjoint perspective on spatio-temporal graph\nneural network for traffic forecasting,\u201d Information Fusion, p. 102341,\n2024.\n[73] M. Xu, W. Dai, C. Liu, X. Gao, W. Lin, G.-J. Qi, and H. Xiong,\n\u201cSpatial-temporal transformer networks for traffic flow forecasting,\u201d\narXiv preprint arXiv:2001.02908, 2020.\n[74] G. Huo, Y. Zhang, B. Wang, J. Gao, Y. Hu, and B. Yin, \u201cHierarchical\nspatio\u2013temporal graph convolutional networks and transformer network\nfor traffic flow forecasting,\u201d TITS, vol. 24, no. 4, 2023.\n[75] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\n\u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NeurIPS,\n2017.\n[76] D. He, Y. Zhao, J. Luo, T. Hui, S. Huang, A. Zhang, and S. Liu,\n\u201cTransrefer3d: Entity-and-relation aware transformer for fine-grained\n3d visual grounding,\u201d in Proceedings of the 29th ACM International\nConference on Multimedia, 2021.\n[77] Y. Zhao, J. Chen, C. Gao, W. Wang, L. Yang, H. Ren, H. Xia, and\nS. Liu, \u201cTarget-driven structured transformer planner for vision-language\nnavigation,\u201d in Proceedings of the 30th ACM International Conference\non Multimedia, 2022.\n[78] Y. Zhang and X. Gu, \u201cTraffic flow prediction based on transformer\nand multi-spatial-temporal encoder-decoder,\u201d in 2023 4th International\nConference on Computer Engineering and Application (ICCEA), 2023.\n[79] X. Mi, C. Yu, X. Liu, G. Yan, F. Yu, and P. Shang, \u201cA dynamic ensemble\ndeep deterministic policy gradient recursive network for spatiotemporal\ntraffic speed forecasting in an urban road network,\u201d Digital Signal\nProcessing, vol. 129, 2022.\n[80] L. Kang, G. Hu, H. Huang, W. Lu, and L. Liu, \u201cUrban traffic travel time\nshort-term prediction model based on spatio-temporal feature extraction,\u201d\nJAT, vol. 2020, 2020.\n[81] D. P. M. Abellana, \u201cMultivariate travel time forecasting in a traffic\nnetwork using fuzzy cognitive mapping,\u201d in AIC, 2023.\n[82] L. Huang, Y. Yang, X. Zhao, C. Ma, and H. Gao, \u201cSparse data-\nbased urban road travel speed prediction using probabilistic principal\ncomponent analysis,\u201d IEEE Access, vol. 6, 2018.\n[83] W. Hamilton, Z. Ying, and J. Leskovec, \u201cInductive representation\nlearning on large graphs,\u201d in NeurIPS, 2017.\n[84] C. Zheng, X. Fan, C. Wang, and J. Qi, \u201cGman: A graph multi-attention\nnetwork for traffic prediction,\u201d in AAAI, 2020.\n[85] C.-H. Wei and Y. Lee, \u201cDevelopment of freeway travel time forecasting\nmodels by integrating different sources of traffic data,\u201d IEEE Transac-\ntions on Vehicular Technology, vol. 56, no. 6, 2007.\n[86] J. Barcel\u00a8o, L. Montero, L. Marqu\u00b4es, and C. Carmona, \u201cTravel time\nforecasting and dynamic origin-destination estimation for freeways\nbased on bluetooth traffic monitoring,\u201d TRR, vol. 2175, no. 1, 2010.\n[87] A. Comi and A. Polimeni, \u201cBus travel time: Experimental evidence\nand forecasting,\u201d Forecasting, vol. 2, no. 3, 2020.\n[88] M. Li, Y. Zhu, T. Zhao, and M. Angelova, \u201cWeighted dynamic time\nwarping for traffic flow clustering,\u201d Neurocomputing, vol. 472, pp.\n266\u2013279, 2022.\n[89] Y. Jin, K. Chen, and Q. Yang, \u201cTransferable graph structure learning\nfor graph-based traffic forecasting across cities,\u201d in Proceedings of the\n29th ACM SIGKDD Conference on Knowledge Discovery and Data\nMining, 2023, pp. 1032\u20131043.\n[90] Q. Zhang, J. Chang, G. Meng, S. Xiang, and C. Pan, \u201cSpatio-temporal\ngraph structure learning for traffic forecasting,\u201d in Proceedings of the\nAAAI conference on artificial intelligence, vol. 34, no. 01, 2020, pp.\n1177\u20131185.\n[91] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph\nconvolutional networks,\u201d in ICLR, 2017.\n[92] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, \u201cHow powerful are graph\nneural networks?\u201d in ICLR, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxix\n[93] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning\nwith neural networks,\u201d in NeurIPS, 2014.\n[94] W. Zhou, Q. Wang, W. Jin, X. Shi, D. Wang, X. Hao, and Y. Yu,\n\u201cGtnet: Graph transformer network for 3d point cloud classification and\nsemantic segmentation,\u201d arXiv preprint arXiv:2305.15213, 2023.\n[95] Z. Liu, Y. Luo, X. Pu, G. Min, and C. Luo, \u201cA multi-modal hypergraph\nneural network via parametric filtering and feature sampling,\u201d TBD,\n2023.\n[96] M. Lin and A. Feragen, \u201cdiffconv: Analyzing irregular point clouds\nwith an irregular view,\u201d in European Conference on Computer Vision.\nSpringer, 2022.\n[97] R. Wiersma, A. Nasikun, E. Eisemann, and K. Hildebrandt, \u201cDeltaconv:\nanisotropic operators for geometric deep learning on point clouds,\u201d TOG,\nvol. 41, no. 4, 2022.\n[98] D. Lu, Q. Xie, K. Gao, L. Xu, and J. Li, \u201c3dctn: 3d convolution-\ntransformer network for point cloud classification,\u201d TITS, vol. 23, no. 12,\n2022.\n[99] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun, \u201cPoint transformer,\u201d\nin ICCV, 2021.\n[100] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M.\nHu, \u201cPct: Point cloud transformer,\u201d Computational Visual Media, vol. 7,\n2021.\n[101] T. Xiang, C. Zhang, Y. Song, J. Yu, and W. Cai, \u201cWalk in the cloud:\nLearning curves for point clouds shape analysis,\u201d in ICCV, 2021.\n[102] K. Zhang, M. Hao, J. Wang, X. Chen, Y. Leng, C. W. de Silva, and\nC. Fu, \u201cLinked dynamic graph cnn: Learning through point cloud by\nlinking hierarchical features,\u201d in M2VIP, 2021.\n[103] Z.-H. Lin, S.-Y. Huang, and Y.-C. F. Wang, \u201cConvolution in the cloud:\nLearning deformable kernels in 3d graph convolution networks for point\ncloud analysis,\u201d in CVPR, 2020.\n[104] Y. Feng, H. You, Z. Zhang, R. Ji, and Y. Gao, \u201cHypergraph neural\nnetworks,\u201d in AAAI, 2019.\n[105] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M.\nSolomon, \u201cDynamic graph cnn for learning on point clouds,\u201d TOG,\nvol. 38, no. 5, 2019.\n[106] G. Te, W. Hu, A. Zheng, and Z. Guo, \u201cRgcnn: Regularized graph cnn\nfor point cloud segmentation,\u201d in ACMMM, 2018.\n[107] R. Li, S. Wang, F. Zhu, and J. Huang, \u201cAdaptive graph convolutional\nneural networks,\u201d in AAAI, 2018.\n[108] Y. Shen, C. Feng, Y. Yang, and D. Tian, \u201cMining point cloud local\nstructures by kernel correlation and graph pooling,\u201d in CVPR, 2018.\n[109] C. Wang, B. Samari, and K. Siddiqi, \u201cLocal spectral graph convolution\nfor point set feature learning,\u201d in ECCV, 2018.\n[110] M. Simonovsky and N. Komodakis, \u201cDynamic edge-conditioned filters\nin convolutional neural networks on graphs,\u201d in CVPR, 2017.\n[111] H.-H. Jebamikyous and R. Kashef, \u201cAutonomous vehicles perception\n(avp) using deep learning: Modeling, assessment, and challenges,\u201d IEEE\nAccess, vol. 10, 2022.\n[112] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, and M. Bennamoun, \u201cDeep\nlearning for 3d point clouds: A survey,\u201d TPAMI, vol. 43, no. 12, 2020.\n[113] M. Balcilar, G. Renton, P. H\u00b4eroux, B. Gauzere, S. Adam, and P. Honeine,\n\u201cBridging the gap between spectral and spatial domains in graph neural\nnetworks,\u201d arXiv preprint arXiv:2003.11702, 2020.\n[114] Y. Wang and J. M. Solomon, \u201cObject dgcnn: 3d object detection using\ndynamic graphs,\u201d in NeurIPS, 2021.\n[115] F. R. Chung, Spectral graph theory.\nAmerican Mathematical Soc.,\n1997, vol. 92.\n[116] N. Yadati, M. Nimishakavi, P. Yadav, V. Nitin, A. Louis, and P. Talukdar,\n\u201cHypergcn: A new method for training graph convolutional networks\non hypergraphs,\u201d in NeurIPS, 2019.\n[117] T.-H. H. Chan and Z. Liang, \u201cGeneralizing the hypergraph laplacian\nvia a diffusion process with mediators,\u201d TCS, vol. 806, 2020.\n[118] S. Bai, F. Zhang, and P. H. Torr, \u201cHypergraph convolution and\nhypergraph attention,\u201d Pattern Recognition, vol. 110, 2021.\n[119] R. Huang, H. Xue, M. Pagnucco, F. Salim, and Y. Song, \u201cMultimodal\ntrajectory prediction: A survey,\u201d arXiv preprint arXiv:2302.10463, 2023.\n[120] X. Li, X. Ying, and M. C. Chuah, \u201cGrip: Graph-based interaction-aware\ntrajectory prediction,\u201d in ITSC, 2019.\n[121] H. Jeon, J. Choi, and D. Kum, \u201cScale-net: Scalable vehicle trajectory\nprediction network under random number of interacting vehicles via\nedge-enhanced graph convolutional neural network,\u201d in IROS, 2020.\n[122] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, \u201cSocial-stgcnn:\nA social spatio-temporal graph convolutional neural network for human\ntrajectory prediction,\u201d in CVPR, 2020.\n[123] R. Chandra, T. Guan, S. Panuganti, T. Mittal, U. Bhattacharya, A. Bera,\nand D. Manocha, \u201cForecasting trajectory and behavior of road-agents\nusing spectral clustering in graph-lstms,\u201d IEEE Robotics and Automation\nLetters, vol. 5, no. 3, 2020.\n[124] Z. Sheng, Y. Xu, S. Xue, and D. Li, \u201cGraph-based spatial-temporal\nconvolutional network for vehicle trajectory prediction in autonomous\ndriving,\u201d TITS, vol. 23, no. 10, 2022.\n[125] Y. Huang, H. Bi, Z. Li, T. Mao, and Z. Wang, \u201cStgat: Modeling spatial-\ntemporal interactions for human trajectory prediction,\u201d in ICCV, 2019.\n[126] S. Carrasco, D. F. Llorca, and M. Sotelo, \u201cScout: Socially-consistent\nand understandable graph attention network for trajectory prediction of\nvehicles and vrus,\u201d in IEEE Intelligent Vehicles Symposium, 2021.\n[127] H. Zhou, D. Ren, H. Xia, M. Fan, X. Yang, and H. Huang, \u201cAst-gnn:\nAn attention-based spatio-temporal graph neural network for interaction-\naware pedestrian trajectory prediction,\u201d Neurocomputing, vol. 445, 2021.\n[128] J. Li, H. Ma, Z. Zhang, J. Li, and M. Tomizuka, \u201cSpatio-temporal graph\ndual-attention network for multi-agent prediction and tracking,\u201d TITS,\nvol. 23, no. 8, 2022.\n[129] W. Zhu, Y. Liu, P. Wang, M. Zhang, T. Wang, and Y. Yi, \u201cTri-hgnn:\nLearning triple policies fused hierarchical graph neural networks for\npedestrian trajectory prediction,\u201d PR, 2023.\n[130] X. Jia, P. Wu, L. Chen, Y. Liu, H. Li, and J. Yan, \u201cHdgt: Heterogeneous\ndriving graph transformer for multi-agent trajectory prediction via scene\nencoding,\u201d TPAMI, 2023.\n[131] F. Zhou, Q. Yang, T. Zhong, D. Chen, and N. Zhang, \u201cVariational graph\nneural networks for road traffic prediction in intelligent transportation\nsystems,\u201d IEEE Transactions on Industrial Informatics, vol. 17, no. 4,\npp. 2802\u20132812, 2020.\n[132] J. Ma and F. Wu, \u201cFeudal multi-agent reinforcement learning with\nadaptive network partition for traffic signal control,\u201d arXiv preprint\narXiv:2205.13836, 2022.\n[133] Y. S. K. LIN and A. K. Bashir, \u201cKeylight: Intelligent traffic signal control\nmethod based on improved graph neural network,\u201d IEEE Transactions\non Consumer Electronics, 2023.\n[134] S. Yang, \u201cHierarchical graph multi-agent reinforcement learning for\ntraffic signal control,\u201d Information Sciences, vol. 634, 2023.\n[135] M. Wang, L. Wu, M. Li, D. Wu, X. Shi, and C. Ma, \u201cMeta-learning\nbased spatial-temporal graph attention network for traffic signal control,\u201d\nKnowledge-based Systems, vol. 250, 2022.\n[136] C. Zhao and G. Wang, \u201cDynamic traffic light control with reinforcement\nlearning based on gnn prediction,\u201d Available at SSRN 4040526, 2022.\n[137] L. Wu, M. Wang, D. Wu, and J. Wu, \u201cDynstgat: Dynamic spatial-\ntemporal graph attention network for traffic signal control,\u201d in CIKM,\n2021.\n[138] S. Yang, B. Yang, Z. Kang, and L. Deng, \u201cIhg-ma: Inductive heteroge-\nneous graph multi-agent reinforcement learning for multi-intersection\ntraffic signal control,\u201d Neural Networks, vol. 139, 2021.\n[139] Z. Zeng, \u201cGraphlight: graph-based reinforcement learning for traffic\nsignal control,\u201d in ICCCS, 2021.\n[140] T. Zhong, Z. Xu, and F. Zhou, \u201cProbabilistic graph neural networks for\ntraffic signal control,\u201d in ICASSP, 2021.\n[141] Y. Wang, T. Xu, X. Niu, C. Tan, E. Chen, and H. Xiong, \u201cStmarl:\nA spatio-temporal multi-agent reinforcement learning approach for\ncooperative traffic light control,\u201d TMC, vol. 21, no. 6, 2020.\n[142] H. Wei, N. Xu, H. Zhang, G. Zheng, X. Zang, C. Chen, W. Zhang,\nY. Zhu, K. Xu, and Z. Li, \u201cColight: Learning network-level cooperation\nfor traffic signal control,\u201d in CIKM, 2019.\n[143] F. I. Shashi, S. M. Sultan, A. Khatun, T. Sultana, and T. Alam, \u201cA\nstudy on deep reinforcement learning based traffic signal control for\nmitigating traffic congestion,\u201d in ECBIOS, 2021.\n[144] Y. Huo, Q. Tao, and J. Hu, \u201cCooperative control for multi-intersection\ntraffic signal based on deep reinforcement learning and imitation\nlearning,\u201d IEEE Access, vol. 8, 2020.\n[145] T. Chu, J. Wang, L. Codec`a, and Z. Li, \u201cMulti-agent deep reinforcement\nlearning for large-scale traffic signal control,\u201d TITS, vol. 21, no. 3, 2019.\n[146] J. K. Gupta, M. Egorov, and M. Kochenderfer, \u201cCooperative multi-agent\ncontrol using deep reinforcement learning,\u201d in Autonomous Agents and\nMultiagent Systems: AAMAS Workshops, 2017.\n[147] T. Nishi, K. Otaki, K. Hayakawa, and T. Yoshimura, \u201cTraffic signal\ncontrol based on reinforcement learning with graph convolutional neural\nnets,\u201d in ITSC, 2018.\n[148] J. Yoon, K. Ahn, J. Park, and H. Yeo, \u201cTransferable traffic signal\ncontrol: Reinforcement learning with graph centric state representation,\u201d\nTransportation Research Part C: Emerging Technologies, vol. 130, 2021.\n[149] T. Saiki and S. Arai, \u201cFlexible traffic signal control via multi-objective\nreinforcement learning,\u201d IEEE Access, 2023.\n[150] J. Ma and F. Wu, \u201cLearning to coordinate traffic signals with adaptive\nnetwork partition,\u201d TITS, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\nxx\n[151] Z. Yu, S. Liang, L. Wei, Z. Jin, J. Huang, D. Cai, X. He, and X.-S. Hua,\n\u201cMacar: Urban traffic light control via active multi-agent communication\nand action rectification,\u201d in IJCAI, 2021.\n[152] X. Li, Z. Guo, X. Dai, Y. Lin, J. Jin, F. Zhu, and F.-Y. Wang, \u201cDeep\nimitation learning for traffic signal control and operations based on\ngraph convolutional neural networks,\u201d in ITSC, 2020.\n[153] T. Tran, D. He, J. Kim, and M. Hickman, \u201cMsgnn: A multi-structured\ngraph neural network model for real-time incident prediction in\nlarge traffic networks,\u201d Transportation Research Part C: Emerging\nTechnologies, vol. 156, 2023.\n[154] Z. Liu, Y. Chen, F. Xia, J. Bian, B. Zhu, G. Shen, and X. Kong,\n\u201cTap: Traffic accident profiling via multi-task spatio-temporal graph\nrepresentation learning,\u201d TKDD, vol. 17, no. 4, 2023.\n[155] Y. Huang, F. Zhang, and J. Hu, \u201cDeep spatial\u2013temporal graph modeling\nof urban traffic accident prediction,\u201d in The International Conference\non Image, Vision and Intelligent Systems, 2022.\n[156] L. Yu, B. Du, X. Hu, L. Sun, L. Han, and W. Lv, \u201cDeep spatio-\ntemporal graph convolutional network for traffic accident prediction,\u201d\nNeurocomputing, vol. 423, 2021.\n[157] B. Wang, Y. Lin, S. Guo, and H. Wan, \u201cGsnet: learning spatial-temporal\ncorrelations from geographical and semantic aspects for traffic accident\nrisk forecasting,\u201d in AAAI, 2021.\n[158] Y. Zhang, X. Dong, L. Shang, D. Zhang, and D. Wang, \u201cA multi-modal\ngraph neural network approach to traffic risk forecasting in smart urban\nsensing,\u201d in SECON, 2020.\n[159] Z. Zhou, Y. Wang, X. Xie, L. Chen, and C. Zhu, \u201cForesee urban sparse\ntraffic accidents: A spatiotemporal multi-granularity perspective,\u201d TKDE,\nvol. 34, no. 8, 2020.\n[160] Z. Zhou, Y. Wang, X. Xie, L. Chen, and H. Liu, \u201cRiskoracle: A minute-\nlevel citywide traffic accident forecasting framework,\u201d in AAAI, 2020.\n[161] L. Zhu, T. Li, and S. Du, \u201cTa-stan: A deep spatial-temporal attention\nlearning framework for regional traffic accident risk prediction,\u201d in\nIJCNN, 2019.\n[162] Z. Yuan, X. Zhou, and T. Yang, \u201cHetero-convlstm: A deep learning\napproach to traffic accident prediction on heterogeneous spatio-temporal\ndata,\u201d in KDD, 2018.\n[163] C. Huang, C. Zhang, P. Dai, and L. Bo, \u201cDeep dynamic fusion network\nfor traffic accident forecasting,\u201d in CIKM, 2019.\n[164] J. Bao, P. Liu, and S. V. Ukkusuri, \u201cA spatiotemporal deep learning\napproach for citywide short-term crash risk prediction with multi-source\ndata,\u201d Accident Analysis & Prevention, vol. 122, 2019.\n[165] C. Chen, X. Fan, C. Zheng, L. Xiao, M. Cheng, and C. Wang, \u201cSdcae:\nStack denoising convolutional autoencoder model for accident risk\nprediction via traffic big data,\u201d in CBD, 2018.\n[166] H. Ren, Y. Song, J. Wang, Y. Hu, and J. Lei, \u201cA deep learning approach\nto the citywide traffic accident risk prediction,\u201d in ITSC, 2018.\n[167] Z. Wang, R. Jiang, Z. Cai, Z. Fan, X. Liu, K.-S. Kim, X. Song, and\nR. Shibasaki, \u201cSpatio-temporal-categorical graph neural networks for\nfine-grained multi-incident co-prediction,\u201d in CIKM, 2021.\n[168] B. Huang and B. Hooi, \u201cTraffic accident prediction using graph neural\nnetworks: New datasets and the travel model,\u201d Traffic, vol. 27, no. 29,\n2022.\n[169] Y. Zhang and T. Cheng, \u201cGraph deep learning model for network-\nbased predictive hotspot mapping of sparse spatio-temporal events,\u201d\nComputers, Environment and Urban Systems, vol. 79, 2020.\n[170] Y. Wang, X. Lin, J. Wu, A. K. Bashir, W. Yang, J. Li, and M. Imran,\n\u201cContrastive gnn-based traffic anomaly analysis against imbalanced\ndataset in iot-based its,\u201d in GLOBECOM, 2022.\n[171] Y. Tong, Y. Chen, Z. Zhou, L. Chen, J. Wang, Q. Yang, J. Ye, and\nW. Lv, \u201cThe simpler the better: a unified approach to predicting original\ntaxi demands based on large-scale online platforms,\u201d in KDD, 2017.\n[172] W. Zi, W. Xiong, H. Chen, and L. Chen, \u201cTagcn: Station-level demand\nprediction for bike-sharing system via a temporal attention graph\nconvolution network,\u201d Information Sciences, vol. 561, 2021.\n[173] J. L. Toole, S. Colak, B. Sturt, L. P. Alexander, A. Evsukoff, and\nM. C. Gonz\u00b4alez, \u201cThe path most traveled: Travel demand estimation\nusing big data resources,\u201d Transportation Research Part C: Emerging\nTechnologies, vol. 58, 2015.\n[174] X. Geng, Y. Li, L. Wang, L. Zhang, Q. Yang, J. Ye, and Y. Liu, \u201cSpa-\ntiotemporal multi-graph convolution network for ride-hailing demand\nforecasting,\u201d in AAAI, 2019.\n[175] E. Lee, H. Choi, D.-G. Kim et al., \u201cPgdrt: Prediction demand based on\ngraph convolutional network for regional demand-responsive transport,\u201d\nJAT, 2023.\n[176] T. Zhao, Z. Huang, W. Tu, F. Biljecki, and L. Chen, \u201cDeveloping a\nmultiview spatiotemporal model based on deep graph neural networks\nto predict the travel demand by bus,\u201d IJGIS, 2023.\n[177] F. Huang, P. Yi, J. Wang, M. Li, J. Peng, and X. Xiong, \u201cA dynamical\nspatial-temporal graph neural network for traffic demand prediction,\u201d\nInformation Sciences, vol. 594, 2022.\n[178] Y. Wen, Z. Li, X. Wang, and W. Xu, \u201cTraffic demand prediction based on\nspatial-temporal guided multi graph sandwich-transformer,\u201d Information\nSciences, vol. 643, 2023.\n[179] J. Li, F. Lin, G. Han, Y. Wang, R. Yu, A. M. Oguti, and Z. Li, \u201cPag-tsn:\nRidership demand forecasting model for shared travel services of smart\ntransportation,\u201d TITS, 2023.\n[180] M. Nazzal, A. Khreishah, J. Lee, and S. Angizi, \u201cSemi-decentralized\ninference in heterogeneous graph neural networks for traffic de-\nmand forecasting: An edge-computing approach,\u201d arXiv preprint\narXiv:2303.00524, 2023.\n[181] L. Liao, B. Li, F. Zou, and D. Huang, \u201cMfgcn: A multimodal fusion\ngraph convolutional network for online car-hailing demand prediction,\u201d\nIEEE Intelligent Systems, 2023.\n[182] Y. Yang, X. Shao, Y. Zhu, E. Yao, D. Liu, F. Zhao et al., \u201cShort-term\nforecasting of dockless bike-sharing demand with the built environment\nand weather,\u201d JAT, 2023.\n[183] G. Jin, Z. Xi, H. Sha, Y. Feng, and J. Huang, \u201cDeep multi-view\ngraph-based network for citywide ride-hailing demand prediction,\u201d\nNeurocomputing, vol. 510, 2022.\n[184] Y. Rong, Z. Xu, R. Yan, and X. Ma, \u201cDu-parking: Spatio-temporal big\ndata tells you realtime parking availability,\u201d in KDD, 2018.\n[185] N. Arora, J. Cook, R. Kumar, I. Kuznetsov, Y. Li, H.-J. Liang, A. Miller,\nA. Tomkins, I. Tsogsuren, and Y. Wang, \u201cHard to park? estimating\nparking difficulty at scale,\u201d in KDD, 2019.\n[186] W. Zhang, H. Liu, Y. Liu, J. Zhou, and H. Xiong, \u201cSemi-supervised\nhierarchical recurrent graph neural network for city-wide parking\navailability prediction,\u201d in AAAI, 2020.\n[187] W. Zhang, H. Liu, Y. Liu, J. Zhou, T. Xu, and H. Xiong, \u201cSemi-\nsupervised city-wide parking availability prediction via hierarchical\nrecurrent graph neural network,\u201d TKDE, vol. 34, no. 8, pp. 3984\u20133996,\n2020.\n[188] Y. Feng, Y. Xu, Q. Hu, S. Krishnamoorthy, and Z. Tang, \u201cPredicting\nvacant parking space availability zone-wisely: A hybrid deep learning\napproach,\u201d Complex & Intelligent Systems, vol. 8, no. 5, 2022.\n[189] D. Zhao, C. Ju, G. Zhu, J. Ning, D. Luo, D. Zhang, and H. Ma, \u201cMepark:\nUsing meters as sensors for citywide on-street parking availability\nprediction,\u201d TITS, vol. 23, no. 7, 2021.\n[190] P. Fafoutellis and E. I. Vlahogianni, \u201cTraffic demand prediction using\na social multiplex networks representation on a multimodal and\nmultisource dataset,\u201d IJTST, 2023.\n[191] C. Wang, Y. Xia, and H.-L. Shen, \u201cRouting and congestion in multi-\nmodal transportation networks,\u201d International Journal of Modern\nPhysics C, vol. 34, no. 03, 2023.\n[192] W. Shao, Z. Jin, S. Wang, Y. Kang, X. Xiao, H. Menouar, Z. Zhang,\nJ. Zhang, and F. Salim, \u201cLong-term spatio-temporal forecasting via\ndynamic multiple-graph attention,\u201d arXiv preprint arXiv:2204.11008,\n2022.\n[193] A. Sharma, A. Sharma, P. Nikashina, V. Gavrilenko, A. Tselykh,\nA. Bozhenyuk, M. Masud, and H. Meshref, \u201cA graph neural network\n(gnn)-based approach for real-time estimation of traffic speed in\nsustainable smart cities,\u201d Sustainability, vol. 15, no. 15, 2023.\n[194] C. Conlan, J. Oakley, G. V. Demirci, A. Sfyridis, and H. Ferhatosman-\noglu, \u201cReal-time spatio-temporal forecasting with dynamic urban event\nand vehicle-level flow information,\u201d in CEUR Workshop Proceedings,\nvol. 3379.\nRWTH Aachen University, 2023.\n[195] J. Jiang, B. Wu, L. Chen, K. Zhang, and S. Kim, \u201cEnhancing\nthe robustness via adversarial learning and joint spatial-temporal\nembeddings in traffic forecasting,\u201d in Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management,\n2023, pp. 987\u2013996.\n[196] J. Zhang, Z. Qu, C. Chen, H. Wang, Y. Zhan, B. Ye, and S. Guo, \u201cEdge\nlearning: The enabling technology for distributed big data analytics in\nthe edge,\u201d CSUR, vol. 54, no. 7, 2021.\n[197] Z. Mao, J. Li, N. Zheng, K. Tei, and S. Honiden, \u201cTransfer learning\nmethod in reinforcement learning-based traffic signal control,\u201d in GCCE,\n2021.\n[198] T. Yin, X. Zhou, and P. Kr\u00a8ahenb\u00a8uhl, \u201cMultimodal virtual point 3d\ndetection,\u201d in NeurIPS, 2021.\n[199] S. Zhancheng, \u201cResearch on application of deep reinforcement learning\nin traffic signal control,\u201d in ICFSP, 2021.\n[200] T. Shi, F.-X. Devailly, D. Larocque, and L. Charlin, \u201cImproving the\ngeneralizability and robustness of large-scale traffic signal control,\u201d\narXiv preprint arXiv:2306.01925, 2023.\n",
    "2312.08248": "A Survey of Generative AI for Intelligent Transportation\nSystems\nHUAN YAN and YONG LI, Beijing National Research Center for Information Science and Technology\n(BNRist), Department of Electronic Engineering, Tsinghua University, China\nIntelligent transportation systems play a crucial role in modern traffic management and optimization, greatly\nimproving traffic efficiency and safety. With the rapid development of generative artificial intelligence (Gen-\nerative AI) technologies in the fields of image generation and natural language processing, generative AI\nhas also played a crucial role in addressing key issues in intelligent transportation systems, such as data\nsparsity, difficulty in observing abnormal scenarios, and in modeling data uncertainty. In this review, we\nsystematically investigate the relevant literature on generative AI techniques in addressing key issues in\ndifferent types of tasks in intelligent transportation systems. First, we introduce the principles of different\ngenerative AI techniques, and their potential applications. Then, we classify tasks in intelligent transportation\nsystems into four types: traffic perception, traffic prediction, traffic simulation, and traffic decision-making.\nWe systematically illustrate how generative AI techniques addresses key issues in these four different types\nof tasks. Finally, we summarize the challenges faced in applying generative AI to intelligent transportation\nsystems, and discuss future research directions based on different application scenarios.\nAdditional Key Words and Phrases: Intelligent transportation system, generative AI, autonomous driving,\ntraffic flow\n1\nINTRODUCTION\nThe transportation system contains a range of traffic infrastructure, equipment, and management\nstrategies, including roads, traffic control, and traffic planning, designed to manage traffic flow\nand meet people\u2019s travel needs. Its efficiency significantly impacts both societal and the economic\nactivities. The rapid urbanization has led to a steep increase in the number of vehicles, resulting in\nprevalent issues like congestion and accidents. These challenges inconvenience travelers and disrupt\nurban operations, emphasizing the urgency of traffic management and optimization. In recent\nyears, the rapid advancement of computer technology has given rise to Intelligent Transportation\nSystems (ITS). ITS harnesses cutting-edge technologies, such as artificial intelligence, big data\nanalysis, and the Internet of Things, to create a comprehensive system integrating people, roads,\nand vehicles. Its goals include reducing congestion, enhancing safety, conserving energy, lowering\ncarbon emissions, and supporting autonomous driving technology.\nA fundamental component for realizing ITS is the effective utilization of traffic data. For traffic\nauthorities, they benefit from real-time data collected through sensors, cameras, and various devices,\nenabling them to monitor and manage road conditions. Furthermore, analyzing historical traffic\ndata can be helpful for predictions regarding future traffic flow and congestion. For travelers,\nthey benefit from real-time traffic data through navigation apps and traffic management systems,\nwhich provide up-to-date traffic information to assist in route selection and congestion avoidance.\nIn addition, autonomous vehicles heavily rely on numerous sensors and data to perceive their\nsurroundings, make effective decisions for safe and efficient driving. These aspects indicate the\nimportant of traffic data within intelligent transportation systems.\nIn this context, traditional deep learning methods have been widely adopted due to their efficient\napplication in traffic data analysis and modeling. These algorithms can process massive amounts\nof traffic data, automatically recognize patterns, predict trends, and optimize decisions, thereby\nimproving traffic efficiency and safety. For example, traditional deep learning techniques such as\nAuthors\u2019 address: Huan Yan, yanhuanthu@gmail.com; Yong Li, liyong07@tsinghua.edu.cn, Beijing National Research Center\nfor Information Science and Technology (BNRist), Department of Electronic Engineering, Tsinghua University, China.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\narXiv:2312.08248v1  [cs.AI]  13 Dec 2023\nHuan Yan and Yong Li\nconvolutional neural networks (CNNs) are used to analyze traffic camera images, further identifying\nvehicles, pedestrians, traffic signs, and road conditions to monitor traffic flow and situations.\nRecurrent neural networks (RNNs) are suitable for processing sequence data, such as time series\ndata generated by traffic sensors, and can be used to predict traffic flow, congestion, and the\nlikelihood of accidents. Deep reinforcement learning (DRL) methods are applied to the development\nof autonomous driving technology, training models to make driving decisions by simulating\nthe behavior of intelligent agents (such as autonomous vehicles) in traffic environments. These\nmodels can enable safe autonomous driving. By utilizing these deep learning models, intelligent\ntransportation systems can analyze road condition patterns more effectively, predict potential\ncongestion points, and provide adaptive decision support for optimizing traffic efficiency.\nHowever, with the increasing complexity of transportation networks, the interweaving of various\nmodes of transportation, and the growing demand for travel, the traffic environment has become\nincreasingly intricate. This complexity poses numerous challenges for traditional deep learning\nmethods in addressing traffic-related issues.\n\u2022 Large amounts of sparse or low-quality data. Traffic data is often sparse and frequently suffers\nfrom missing values or noise contamination. Traditional deep learning methods impose high\nrequirements on data quality, especially supervised learning approaches that typically require\na substantial amount of labeled input and output data for supervised training. The scarcity\nor absence of traffic data can significantly impact model performance.\n\u2022 Rare abnormal scenarios. Traffic environments exhibit high levels of dynamism, with occa-\nsional occurrences of anomalies such as traffic accidents that pose significant threats to traffic\nsafety. These anomalies are challenging to capture as they are often rare events, resulting\nin a lack of relevant samples in training data. Traditional deep learning methods are more\nsuitable for processing tasks under normal circumstances, and struggle to effectively address\nrare or previously unseen abnormal situations.\n\u2022 Unexplored modeling of uncertainty. In the transportation field, there are many factors that\nmay cause uncertainty. For example, adverse weather conditions such as heavy rain, snow, or\nsmog, as well as the uncertainty of various traffic participants\u2019 behaviors such as changing\nlanes and emergency braking, make traffic flow prediction more challenging. Traditional\ndeep learning methods usually adopt deterministic approaches, relying on pre-processed\nfeatures, thus making it difficult to effectively capture such uncertainty.\nGenerative Artificial Intelligence (Generative AI) technology has recently developed rapidly. It\ncan automatically generate various forms of content based on user input, demands, or instructions,\nincluding but not limited to text, images, videos, audio, etc. The emergence of generative AI\ntechnology makes content production more efficient and convenient, and has broad application\nprospects. In the field of generative AI, many mature technologies have been widely applied. For\nexample, Variational Auto-Encoder (VAE) [112] is a generative adversarial network that can model\nand generate complex data. Generative Adversarial Network (GAN) [77] improves the quality of\ngenerated content through the adversarial process of two neural networks (i.e., generators and\ndiscriminators). Normalizing Flow [195] is a normalization technique used for generating complex\ndata distributions, while Energy-Based Model [120] is a generative model based on energy that\ncan effectively improve the quality of generation. Additionally, Generative Model from Physical\nProcess [153] is a generative model based on physical processes that can be used to generate content\nwith physical constraints. Diffusion Model [212] is a probabilistic method that generates data by\nadding noise to data gradually to reduce its quality. Generative Pre-trained Transformer (GPT) [71]\nis based on the Transformer architecture [223], which pre-trains a large amount of text data and\nthen uses it to generate new text content.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nCategory\nReference\nDescription\nITS\nComprehensive\n[187], [7], [211], [74], [65], [61], [268], [69], [245], [160], [220], [196],[206], [87], [193]\nThese works give a comprehensive review of ITS, covering\nkey technologies, infrastructure, and applications. However,\nfew generative AI techniques are systematically introduced.\nTechnologies\nDRL [89], GNN [190], NLP [184], SVM [85], DL [80, 224], Positioning [58, 95], GAN [143],\nBlockchain [48, 218], Edge computing [75, 303], Dynamic pricing [201], Fuzzy logic [218],\nSwarm intelligence [163], V2X downloading [227], Data mining [8, 59, 305],\nCommunication [26, 50, 110, 158, 162, 168, 205]\nThese works conduct surveys on specific techniques within\nthe field of in ITS.\nInfrastructure\nIoT [18, 28, 181], VANET [16, 221], Bluetooth [68], Radar-on-chip [203], Hardware devices [47],\nDistributed architecture [171], External infrastructure [45]\nThese works conduct surveys on specific components in ITS.\nApplications\nSecurity management [3, 4, 6, 17, 86, 119], Traffic scheduling [170], Traffic detection [147],\nDriver behavior detection [42], Vehicle classification [72, 256], Urban mobility [161],\nUrban monitoring [142], Traffic sign detection [60, 154], Traffic prediction [215, 260],\nAutonomous driving [5], Metaverse [175], Trust management [157], Spectrum regulation [43]\nThese surveys focus on specified applications in ITS.\nGenerative AI\nComprehensive\n[27], [238], [270], [67]\nThese works conduct a comprehensive review of generative\nAI techniques.\nApplications\n3D [124], Biology [277], ChatGPT [233, 269], Metaverse [185], Edge cloud computing [234],\nMobile network [56, 250], Security & Privacy [29, 228], Vehicle network [280]\nThese surveys concentrate on the specific applications of\ngenerative AI techniques in a particular field.\nTable 1. Related reviews of intelligent transportation system (ITS) and generative AI. DRL: Deep reinforcement\nlearning, GNN: Graph neural network, NLP: Natural language processing, V2X: Vehicle to everything. DL:\nDeep learning. IoT: Internet of things. VANET: Vehicle ad-hoc network.\nThese generative AI techniques offer significant advantages in data generation, transformation,\nand repair, and they can effectively model the uncertainty of data. As a result, these technologies\nhave garnered significant attention in different fields. Especially in the field of intelligent trans-\nportation, generative AI technology has played a crucial role and introduced innovative solutions\nto address traffic-related challenges [155]. Numerous researchers are actively engaged in the ap-\nplication of diverse generative AI techniques to traffic-related tasks. For example, in autonomous\ndriving scenarios, generative AI technology proves invaluable in generating high-fidelity driving\nscene images and videos, which are essential for the training and testing of autonomous driving\nsystems [115, 118]. Utilizing generative AI-generated simulation scenarios, autonomous vehicles\ncan conduct extensive driving simulations within virtual environments, thereby enhancing their\nability to make precise real-world decisions. In traffic flow prediction tasks, generative AI tech-\nnology can learn and model the distribution of traffic data, thus generating future traffic flow\ndata [236, 265, 272]. This is very helpful for traffic authorities to carry out effective traffic planning\nand optimization.\nAs shown in Table 1, there are numerous surveys on intelligent transportation systems or\ngenerative AI. However, most of them rarely explore the practical applications of generative AI\ntechnology in intelligent transportation systems. Although the authors in [143] have explored\nthe role of GANs in intelligent transportation systems, they do not comprehensively cover other\ngenerative AI techniques. Importantly, they do not systematically examine the advantages of\ngenerative AI techniques over traditional deep learning methods in intelligent transportation\nsystems. Therefore, in this paper, we will explore the applications of generative AI techniques in\ntraffic perception, prediction, simulation, and decision-making within intelligent transportation\nsystems, as shown in Figure 1. Our goal is to provide a comprehensive analysis of their role in\npromoting intelligent transportation, and offer a constructive insight to help readers gain a deeper\nunderstanding of the challenges and opportunities of generative AI in intelligent transportation\nsystems. We expect this survey to provide useful insights for further research and advancement of\nintelligent transportation systems.\nIn conclusion, the primary contributions of this paper are illustrated as follows:\n\u2022 To the best of our knowledge, we are the first to offer an in-depth literature review of\ngenerative AI for intelligent transportation systems.\n\u2022 We provide a systematic introduction to mainstream generative AI techniques, conduct\nin-depth method comparisons from both horizontal and vertical perspectives, and provide a\nsystematical analysis of how generative AI technology can effectively address key issues in\nintelligent transportation systems from the aspects of traffic perception, traffic prediction,\ntraffic simulation, and traffic decision-making.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nGenerative AI\nVAE, GAN, \nNormalizing \nFlow, EBM, \nDPM, GenPhys, \nGPT\nTraffic Perception\np Traffic Data Imputation\n\u2022 Low-dimension traffic data\n\u2022 High-dimension traffic data\np Traffic Estimation\np Traffic Data Mining\n\u2022 Trajectories\n\u2022 Traffic images\n\u2022 Traffic videos\np Traffic Anomaly Detection\nTraffic Prediction\np Human-related Prediction\n\u2022 Individual behavior\n\u2022 Human mobility\np Vehicle-related Prediction\n\u2022 Vehicle trajectory\n\u2022 Crash risk\np Road Segment-related Prediction\np Region-specific Prediction\nTraffic Simulation\np Driver Behavior Simulation\np Traffic Scenario Generation\n\u2022 Vehicle trajectory generation\n\u2022 Human trajectory generation\n\u2022 Anomaly data generation\np Traffic Flow Generation\nTraffic Decision-making\np Autonomous Driving\n\u2022 Train movement\n\u2022 Vehicle multi-task driving\nFig. 1. An overview of generative AI in intelligent transportation systems.\n\u2022 We discuss the open challenges encountered in applying generative AI technology in intelli-\ngent transportation systems, and explore potential directions for future research.\nThe structure of this paper is as follows. Section 2 introduces mainstream generative AI techniques\nand their application achievements in different fields, followed by a detailed comparative analysis.\nThe following four sections respectively discuss the applications of generative AI technology\nin traffic perception, traffic prediction, traffic simulation, and traffic decision-making. Finally, in\nSection 7, we introduce the challenges faced by generative AI in the application of intelligent\ntransportation systems, and look forward to future research directions.\n2\nTECHNOLOGIES, APPLICATIONS AND ADVANTAGES OF GENERATIVE AI\nAs described in previous works [27, 67, 233, 238], generative AI refers to a subset of AI algorithms\nthat focus on creating or producing personalized, and high-quality content, including text, images,\nvideos, and 3D assets, in response to user input or specific requirements. This section provides a\ncomprehensive overview of the underlying technologies, highlights their significant applications,\nand summarizes the advantages of generative AI techniques in comparison to traditional deep\nlearning methods.\n2.1\nKey Technologies\nWith the rapid growth of computer technology and computing power, generative AI technology has\nwitnessed significant progress over the past decade. Variational Autoencoders (VAE), a probabilistic\ngraphical model, was pioneered by Kingma and Welling in 2013 for data generation [112]. In\n2014, Ian Goodfellow et al. designed generative adversarial networks (GANs) [76], which led to\na major transformation in the field of generative AI. In 2018, OpenAI proposed Generative Pre-\ntrained Transformer (GPT), which also brought important breakthroughs to the field. Beyond\nthese approaches, technologies like Normalizing Flow, Energy-Based Models (EBMs), Diffusion\nProbabilistic Models (DPMs) and Generative Models from Physical Process (GenPhys) have also\ndemonstrated substantial potential in the domains of text and image generation. Figure 2 depicts\nthe evolution timeline of generative AI techniques, including VAE, GANs, Normalizing Flow, EBMs,\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n2013\n2015\n2017\n2019\nVAE\nNormalizing Flow\nGAN\nEBM\nDPM\nGPT\n2023\nGenPhys\nFig. 2. The history of generative AI techniques.\nDPMs, GenPhys, and GPT. In this subsection, we will provide an initial introduction to these\ntechniques, followed by a comparative analysis.\n2.1.1\nVAE. VAE [112] aims to learn the latent representations of data and generate new data\nsamples. It achieves this by mapping data to a probability distribution in a latent space and then\nre-mapping the latent variables back to the data space through a decoder.\nFormally, given observed data x sampled from some unknown distribution \ud835\udc5d\ud835\udf03(x), the goal is to\nlearn this distribution. However, it is challenging to learn it directly. Hence, latent variables z are\nintroduced, along with a generative model \ud835\udc5d\ud835\udf03(x|z). Then, the data distribution can be expressed\nas \ud835\udc5d\ud835\udf03(x) =\n\u222b\n\ud835\udc67\ud835\udc5d(\ud835\udc67)\ud835\udc5d\ud835\udf03(x|z)\ud835\udc51\ud835\udc67. However, since the potentially infinite scope of the integral makes\ndirect computation infeasible, it is intractable to optimize it via maximum likelihood. To solve this\nproblem, VAE starts from the posterior estimation, using an encoder \ud835\udc5e\ud835\udf19(z|x) to fit its posterior\nprobability \ud835\udc5d\ud835\udf03(z|x), and the goal is transformed as minimizing the Kullback\u2013Leibler (KL) divergence\nbetween the two distributions.\n2.1.2\nGAN. GAN [76] consists of a generator and a discriminator. They are trained together\nthrough an adversarial process to generate realistic data samples. The generator \ud835\udc3atakes random\nnoise as input, drawn from a prior distribution \ud835\udc5d\ud835\udc67(\ud835\udc67), and generates fake data samples \ud835\udc3a(\ud835\udc67) that\nresemble real data samples. The discriminator \ud835\udc37receives both real data samples \ud835\udc65from the dataset\nfollowing \ud835\udc5ddata (\ud835\udc99) and fake data samples \ud835\udc3a(\ud835\udc67) generated by the generator. Its objective is to\ndifferentiate between real and fake samples. The objective of GAN is to find a Nash equilibrium\nwhere the generator generates data that is indistinguishable from real data. This equilibrium is\nachieved when the discriminator cannot reliably distinguish between real and fake samples.\n2.1.3\nNormalizing Flows. The core idea of normalizing flows [195] is to map a simple probability\ndistribution to a complex one by applying a sequence of reversible transformations, so as to\ngenerate samples that match the complex data distribution. These reversible transformations can\nbe combined using the chain rule to efficiently compute the logarithmic probability density of the\ngenerated samples. The key feature is that these transformations are all reversible, allowing for\neasy mapping from the data space to the latent space and vice versa.\nLet \ud835\udc53be an invertible and smooth mapping function, and its inverse function \ud835\udc53\u22121 = \ud835\udc54. By\ntransforming a random variable z \u223c\ud835\udc5d(z) using \ud835\udc53, the distribution of the resulting random variable\ny = \ud835\udc53(z) is:\n\ud835\udc5d(y) = \ud835\udc5d(z)\n\f\f\f\fdet \ud835\udf15\ud835\udc53\u22121\n\ud835\udf15y\n\f\f\f\f = \ud835\udc5d(z)\n\f\f\f\fdet \ud835\udf15\ud835\udc53\n\ud835\udf15z\n\f\f\f\f\n\u22121\n,\n(1)\nwhere the chain rule is applied. By combining multiple simple mappings and successively using\nEquation 1, arbitrarily complex densities can be derived. The density \ud835\udc5d\ud835\udc3e(z) calculated by the\nsuccessive transformations of a random variable z0 following the distribution \ud835\udc5d0 through a chain of\n\ud835\udc3etransformations is written as:\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nz\ud835\udc3e= \ud835\udc53\ud835\udc3e\u25e6. . . \u25e6\ud835\udc532 \u25e6\ud835\udc531 (z0) ,\n(2)\nln\ud835\udc5d\ud835\udc3e(z\ud835\udc3e) = ln\ud835\udc5d0 (z0) \u2212\n\ud835\udc3e\n\u2211\ufe01\n\ud835\udc58=1\nln\n\f\f\f\fdet \ud835\udf15\ud835\udc53\ud835\udc58\n\ud835\udf15z\ud835\udc58\u22121\n\f\f\f\f .\n(3)\nHere, the path that is traversed by the random variables z\ud835\udc58= \ud835\udc53\ud835\udc58(z\ud835\udc58\u22121), which are generated from\nthe initial distribution \ud835\udc5d0(z0), is known as the flow. The path created by the consecutive distributions\n\ud835\udc5d\ud835\udc58is referred to as a normalizing flow. By selecting suitable transformations \ud835\udc53\ud835\udc58, we can begin with\nsimple, factored distributions like an independent Gaussian. Then, by applying normalizing flows\nof varying lengths, we can obtain increasingly complex and multi-modal distributions.\n2.1.4\nEnergy-Based Models. EBMs [57, 120] focus on defining an energy function that assigns a\nscalar value to each data point. The energy function measures how well a data point fits the model,\nwith lower energy values indicating a better fit. EBMs aim to assign lower energy values to realistic\ndata points and higher energy values to data points that are not part of the true distribution. In\nother words, they can approximate the likelihood of the observed data and offer a quantitative\nassessment of how well a given data point aligns with the learned distribution.\nFor the observed data x, its probability density function \ud835\udc5d\ud835\udf03(x) is defined via the Boltzmann\ndistribution:\n\ud835\udc5d\ud835\udf03(x) =\n\ud835\udc52\u2212\ud835\udc38\ud835\udf03(x)\n\u222b\n\u02dcx\u2208X \ud835\udc52\u2212\ud835\udc38\ud835\udf03( \u02dcx) ,\n(4)\nwhere \ud835\udc38\ud835\udf03(x) is an energy function with parameters \ud835\udf03.\nA common approach to optimize EBMs is contrastive divergence. In contrastive divergence,\nthe model first samples a sample from the positive data distribution and another sample from the\nnegative data distribution. Subsequently, the model updates its parameters by comparing the energy\nvalues of these two samples. The objective is to decrease the energy value of positive samples while\nincreasing the energy value of negative samples. This process is typically repeated iteratively to\ngradually optimize the model.\n2.1.5\nDiffusion Probabilistic Model. The essence of DPMs [212] lies in its step-by-step denoising\nprocess that gradually recovers the underlying original data samples.\nFormally, DPMs can be represented as the following form: \ud835\udc5d\ud835\udf03(x) =\n\u222b\n\ud835\udc5d\ud835\udf03(x0:\ud835\udc47)\ud835\udc51x0:\ud835\udc47, where x\ud835\udc61for\n\ud835\udc61= 1, ...,\ud835\udc47is a sequence of latent variables with the same dimension as x0 \u223c\ud835\udc5e(x0). They consist of\nforward process and reverse process.\nIn the forward process, a Markov chain successively adds Gaussian noise to the data x0. Hence,\nwe can obtain the approximate posterior \ud835\udc5e(\ud835\udc991:\ud835\udc47| \ud835\udc990):\n\ud835\udc5e(\ud835\udc991:\ud835\udc47| \ud835\udc990) =\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc99\ud835\udc61| \ud835\udc99\ud835\udc61\u22121) ,\n(5)\nwhere \ud835\udc5e(\ud835\udc99\ud835\udc61| \ud835\udc99\ud835\udc61\u22121) is a Gaussian distribution.\nIn the reverse process, the original data point is recovered by iteratively removing noise from the\nnoisy data. Specifically, this process is structured as a Markov chain, leveraging learnable Gaussian\ntransitions that initiate from the distribution \ud835\udc5d(x\ud835\udc47) = N (x\ud835\udc47; 0, I):\n\ud835\udc5d\ud835\udf03(\ud835\udc990:\ud835\udc47) = \ud835\udc5d(\ud835\udc99\ud835\udc47)\n1\n\u00d6\n\ud835\udc61=\ud835\udc47\n\ud835\udc5d\ud835\udf03(\ud835\udc99\ud835\udc61\u22121 | \ud835\udc99\ud835\udc61) ,\n(6)\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nwhere \ud835\udc5d\ud835\udf03(\ud835\udc99\ud835\udc61\u22121 | \ud835\udc99\ud835\udc61) represents a reverse process.\nDuring each training iteration, the network attempts to predict the exact noise added to the\ndata at a specific diffusion step. The training objective is to minimize the difference between the\nnetwork\u2019s predicted noise and the actual noise.\n2.1.6\nGenerative Models from Physical Process. GenPhys [153] transform physical partial differen-\ntial equations (PDEs) into generative models. They provide a more general generative framework,\nwith diffusion probabilistic model being a special case within the GenPhys framework.\nGiven observed data sampled from the probability distribution \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(x), the task aims to produce\nnew sample from \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(x). A continuous physical process can be described by\n\ud835\udc51x\n\ud835\udc51\ud835\udc61= v(x,\ud835\udc61),\n(7)\nwhere v represents a velocity field.\nThis process dynamically evolves the probability distribution \ud835\udc5d(x,\ud835\udc61) according to the following\nequation:\n\ud835\udf15\ud835\udc5d(x,\ud835\udc61)\n\ud835\udf15\ud835\udc61\n+ \u2207\u00b7 [\ud835\udc5d(x,\ud835\udc61)v(x,\ud835\udc61)] \u2212\ud835\udc45(x,\ud835\udc61) = 0,\n(8)\nwhere \ud835\udc45(x,\ud835\udc61) can be interpreted as birth or death. When \ud835\udc45> 0, it means indicates the birth of\nparticles in the forward process and their subsequent death in the backward process, and vice versa.\nThis equation is known as density flow equation. The goal is to design \ud835\udc5d(x,\ud835\udc61), v(x,\ud835\udc61) and \ud835\udc45(x,\ud835\udc61) in\na manner that fulfills two conditions. First, it should ensure that \ud835\udc5d(x, 0) = \ud835\udc5d\ud835\udc51\ud835\udc4e\ud835\udc61\ud835\udc4e(x). The second\ncondition is that \ud835\udc5dprior(x) \u2261\ud835\udc5d(x,\ud835\udc47) becomes asymptotically independent of \ud835\udc5ddata(x) as \ud835\udc47\u2192\u221e.\nContinuous physical processes are governed by PDEs, which is written as:\n\u02c6\ud835\udc3f\ud835\udf19\u2261\ud835\udc39\u0000\ud835\udf19,\ud835\udf19\ud835\udc61,\ud835\udf19\ud835\udc61\ud835\udc61, \u2207\ud835\udf19, \u22072\ud835\udf19, . . .\u0001 = \ud835\udc53(x,\ud835\udc61),\n(9)\nwhere \u02c6\ud835\udc3frepresents a differential operator on the scalar function \ud835\udf19(x,\ud835\udc61). \ud835\udc53(x,\ud835\udc61) is the source term.\nNext, the crucial step is to map the solutions of physical PDEs to those of density flows. This\nrequires density flows and physical PDEs are equivalent equations. The final step is to produce x(0)\nby the reverse physical process. To begin, x(\ud835\udc47) is drawn from the distribution \ud835\udc5d(x,\ud835\udc47). Then, we\nsimulate the backward evolution of \ud835\udc51x\n\ud835\udc51\ud835\udc61= s\ud835\udf03(x,\ud835\udc61) from \ud835\udc61= \ud835\udc47to \ud835\udc61= 0 using the branching process\n\ud835\udc4a\ud835\udefc(x,\ud835\udc61). This simulation ultimately produces x(0).\n2.1.7\nGenerative Pre-trained Transformer. Generative Pre-trained Transformer (GPT) is a neural\nnetwork framework based on Transformer, specifically developed for tasks involving natural\nlanguage processing (NLP) and generation. GPT models are pre-trained on a large number of textual\ndatasets and are capable of generating text that is both coherent and contextually meaningful.\nGPT utilizes a transformer architecture [223] with decoder layers. Each decoder layer consists of\nself-attention mechanisms and feedforward neural networks. These mechanisms allow the model to\ncapture long-range dependencies in sequential data like text. To handle sequential input data, GPT\nincludes positional encodings and learned word embeddings. Layer normalization is applied after\neach sub-layer in the transformer decoder to stabilize training. GPT employs an autoregressive\ngeneration approach, producing output tokens sequentially, building upon previously produced\ntokens. An important feature of GPT is parameter sharing across its decoder layers, ensuring\nconsistency in data representation. GPT models undergo unsupervised pre-training on extensive\ntext corpora and can be fine-tuned for specific tasks with labeled data. Their scalability has led to\nincreasingly large models, such as GPT-2 [173], GPT-3 [23] and GPT-4 [177], achieving superior\nperformance across various NLP tasks.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nTechnologies\nAdvantages\nDisadvantages\nVariational Autoencoder [112]\n- Explicit encoder and decoder\n- Capable of probabilistic generation, suitable for data with uncertainty\n- Can learn latent representations of data\n- Training is relatively stable\n- May produce blurry images\n- Challenges in selecting likelihood functions\n- Limited expressiveness of the prior distribution\nGenerative Adversarial Network [77]\n- Capable of generating high-quality images\n- Provides diversity in the generation process\n- Training process can be unstable\n- Mode collapse issue is common\nNormalizing Flow [195]\n- Can accurately model probability distributions\n- Supports both generation and inference\n- Applicable to various data types\n- Training and inference complexities are high\n- Requires more parameters for high-dimensional data\nEnergy-Based Model [120]\n- Can model complex probability distributions\n- Supports both generation and inference\n- Robust to outliers\n- Requires careful tuning of hyperparameters\n- Training process may be slow\nDiffusion Probabilistic Model [212]\n- Robust to noise and uncertainty\n- Flexible noise schedule choice\n- Allows for controlled generation\n- Longer generation time\n- Training complex diffusion models can be slow\nGenerative Models from Physical Process [153]\n- Generates data based on physical processes\n- Offers interpretability\n- Requires prior knowledge of physical parameters\n- Typically limited to specific application domains\nGenerative Pre-trained Transformer [71]\n- Performs exceptionally well in natural language processing tasks\n- Capable of generating continuous text\n- Effective modeling of long-term dependencies\n- Requires substantial computing resources for training\n- Dependence on large training datasets\nTable 2. The advantages and disadvantages of Generative AI techniques, partly referenced by the work [280].\n2.1.8\nComparative Analysis of Generative AI Techniques. In previous subsections, we have in-\ntroduced several major generative AI techniques. Each of them has its distinct advantages and\nlimitations. For example, VAE offer a probabilistic framework for encoding data into a lower-\ndimensional space but often face challenges in selecting the appropriate likelihood functions and\ngenerating high-quality images. GANs aim to generate highly realistic data samples by pitting a\ngenerator against a discriminator in a adversarial training scheme. They produce impressive results\nin image generation but can be challenging to train and prone to mode collapse. We summarize the\nadvantages and disadvantages of these generative AI techniques, as shown in Table 2.\n2.2\nImportant Generation Tasks\nThe advancement of generative technologies has provided impressive solutions for various gen-\nerative tasks. In this section, we will explore the applications of generative technologies in these\ngeneration tasks from the perspectives of text, images, videos, and cross-modal data.\n2.2.1\nText Generation. Text generation focuses on using AI techniques to autonomously generate\nreasonable and meaningful text. This technology is often used in tasks such as machine translation,\nchatbots, and article summarization. In recent years, with the advancement of deep learning\ntechnologies, especially the emergence of the Transformer architecture and the GPT series of\nmodels, both the quality and diversity of text generation have significantly improved. ChatGPT [178],\ndeveloped by OpenAI, is a specialized version of the GPT model fine-tuned for conversational\ninteractions. Microsoft\u2019s Bing [2] has integrated ChatGPT technology, enhancing the conversational\nsearch experience for users. This integration empowers users to comprehend intricate queries,\nreceive detailed responses, and benefits from continuous learning and optimization capabilities.\n2.2.2\nImage Generation. Image generation focuses on using AI algorithms to create new images or\nvisual content using given information such as images and text. For example, generative methods\nleverage the power of neural networks to learn complex data representations, making them a\nprominent approach for restoring images to their original high-quality state [25, 121, 197, 212].\nSeveral works focus on conversion between two images, encompassing tasks such as style transfer\nand image morphing [107, 294]. Recently, diffusion models are utilized to address the tasks of image\nediting, achieving superior image generation quality [1, 11, 125].\n2.2.3\nVideo Generation. Video generation involves creating a series of frames that are semantically\ncoherent and appear in a sequence to form a video. This task is challenging due to its requirement\nto deal with high-dimensional video data and simultaneously model spatial and temporal depene-\ndencies. The related works are usually classified into two types. Unconditional video generation\naims to generate new videos without requiring additional information. It achieves this by extracting\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nvaluable information from the training data. Numerous works that tackle the task of unconditional\nvideo generation utilize generative AI technology [79, 231, 232]. Unlike unconditional video gen-\neration, conditional video generation incorporates additional condition to guide the process of\nvideo generation. VAE and GAN are are two popular methods used to generate videos with specific\nsettings such as text [137], sentences [152] and captions [180].\n2.2.4\nCross-modality Data Generation. Cross-modality data generation refers to generate data in\none modality such as image, text and video based on the features from a different modality. The\nprimary challenge in implementing cross-modality data generation lies in effectively modeling the\nrelationships between different modalities, enabling control over the generation process through\ninputs from another modality [67]. Text-to-image generation aims to generate images based on\na provided textual description. Current approaches for text-to-image generation are mainly built\nupon GANs [123, 252], Transformer [52, 53, 191] and diffusion models [174, 188]. Image-to-text\ngeneration involves generating textual description or captions that describe the content of an\nimage. Most works adopt the encoder-decoder architectures to deal with this task. The language\ndecoder plays a crucial role in model design. Advancements in the NLP field have led to a transition\nin language decoder architecture from RNNs to Transformer-based generative models [136, 279].\nText-to-video generation refers to creating videos from the textual prompts. Three commonly\nused approaches to address the challenge of text-to-video generation include GAN-based [12] and\nVAE-based [165], and diffusion-based [189, 209] methods.\n2.3\nAdvantages of generative AI technology\nGenerative AI technology has significant advantages over traditional deep learning methods such\nas MLP, CNN, RNN in many aspects. In this subsection, we will discuss them in terms of data\ngeneration, creativity, data utilization, model generalization, and cross-domain applications.\nData generation. Generative AI not only has the capability to process and analyze existing data\nbut also to generate new data samples based on the distribution of the available data. For instance,\nin the field of computer vision, GANs can produce highly realistic images. In natural language\nprocessing, generative pre-trained models like the GPT series are proficient at generating fluent\ntext, which can be used for tasks such as automated content generation, chatbot systems, and\ntranslation services. Furthermore, generative AI finds applications in various domains, including\naudio synthesis, video generation, medical image synthesis, and simulating experiments in scientific\nresearch. While traditional deep learning methods excel in data analysis and classification tasks,\nthey typically lack the direct capability to generate entirely new data samples. This makes generative\nAI particularly advantageous in data augmentation and simulation among other areas.\nCreativity. Generative AI has the ability to generate novel, interesting, and diverse data, which\nenables it to have extensive application potential in the creative field. For example, in the field\nof literature, generative pre-trained models such as GPT-3 [66] have been able to automatically\ngenerate articles, essays, poems, and stories. Generative AI is also used in music composition [167,\n208, 278]. Music generation models can generate various styles and types of music, from classical to\npopular. In the field of scientific research, generative AI is also used for new drug discovery [19, 41].\nBy simulating molecular structures and chemical reactions, generative AI can generate potential\nnew drug candidates, accelerating the process of drug development. Traditional deep learning\nmethods rely more on existing data for extraction and classification, and are relatively weak in\nterms of creativity.\nReasoning. Generative AI methods have more advantages than traditional deep learning meth-\nods in terms of reasoning. Firstly, they often employ probabilistic modeling, which can effectively\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nmodel uncertainties. Secondly, they have the ability to capture long-term dependencies. For exam-\nple, the self-attention mechanism in the GPT architecture allows the model to capture information\nfrom any position in the sequence during the processing of sequence data, rather than just local\ndependencies. Traditional deep learning methods typically focus on end-to-end learning, with\nthe main goal of mapping input data to output data rather than specific reasoning. Therefore, the\nprobabilistic modeling and long-term dependency extraction of the generative AI methods make it\nmore outstanding in reasoning.\nModel generalization. Generative AI not only learns broader knowledge through large-scale\ndata training, but also benefits from its architecture with numerous shared parameters, allowing it\nto transfer learned knowledge across different tasks. For example, GPT can be applied to a variety\nof NLP tasks including text generation, language translation, and dialogue generation, without the\nneed to design new models for each task separately. Similarly, in computer vision, generative AI-\nbased models can initially be trained for image classification, and then are employed for other tasks\nsuch as object detection without the requirement for redesigning network structures. Achieving\nsuch adaptability and versatility proves challenging for traditional deep learning methods, which\ntypically require task-specific network architectures.\nCross-domain applications. Generative AI methods can enable cross-domain data generation,\nallowing knowledge acquired in one domain to seamlessly transfer to another. For example, Google\u2019s\nImagen uses a diffusion model to convert text descriptions into images [90]. This cross-domain\ntransfer learning provides new opportunity for knowledge sharing between different domains. In\ncontrast, traditional deep learning methods are relatively weak in cross-domain applications. They\nusually require redesigning and training models specific to the new domain to adapt to the data\nand tasks of different domains. This leads to increased complexity and resource requirements for\ncross-domain knowledge transfer, limiting their extensive applicability across different domains.\n3\nGENERATIVE AI FOR TRAFFIC PERCEPTION\nTraffic perception refers to the ability of intelligent transportation systems to collect and understand\nvarious sensory information in the traffic environment. This information includes visual data from\ncameras, trajectory data from GPS-based devices, motion data from accelerometers, environmental\ndata like weather and road conditions, and other traffic-related information such as the presence of\nother vehicles, pedestrians, and road signs. It plays a crucial role in making effective decisions and\nensuring safe driving. In the context of autonomous vehicles, traffic perception typically involves\nthe use of cameras, LiDAR, millimeter-wave radar, and other technologies to collect and process\ndata from the vehicle\u2019s surroundings, enabling autonomous driving and navigation.\nHowever, the accurate perception and comprehension of the intricate dynamics within traffic\nenvironments pose several significant challenges: First, sensors may fail to provide data in certain\nsituations, such as when cameras are obstructed. These data gaps can disrupt the continuous\nmonitoring of traffic conditions, potentially leading to incomplete information. Second, sensor data\ncollected in the traffic environment is subject to various sources of noise, including sensor inaccu-\nracies, weather conditions, and lighting variations. This noise introduces uncertainty into the data,\nmaking it harder to derive precise information. Third, traffic environments are inherently complex\nand dynamic, involving numerous interacting factors such as vehicles, pedestrians, traffic signals,\nand road conditions. This complexity makes it challenging to accurately model and understand\ntraffic behavior.\nThe advance of generative AI technology holds the potential to offer solutions to these chal-\nlenges. We carried an in-depth literature review focusing on the application of generative AI to\naddress challenges in traffic perception. The reviewed literature covers various aspects of traffic\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nGenerative AI for \ntraffic perception\nData imputation\nTraffic \nestimation\nTraffic data \nmining\nTraffic anomaly \ndetection\nCorrupted or \nmissing data\nData collection \ncompleteness\nNormal data \nanalysis\nAnomaly data \nanalysis\nFig. 3. The relation between different topics in generative AI for traffic perception research.\nperception, including traffic data imputation, traffic estimation, traffic data analysis and traffic\nanomaly detection. Their relation is illustrated in Figure 3.\n3.1\nTraffic Data Imputation\nThe task of traffic data imputation involves learning a function, denoted as \ud835\udc53(\u00b7), which takes as\ninput the corrupted data or missing data \ud835\udc4b, in order to reconstruct the complete data \u02c6\ud835\udc4b. Formally,\nthe reconstructed data \u02c6\ud835\udc4bcan be expressed as \u02c6\ud835\udc4b= \ud835\udc53(\ud835\udc4b, \u03a8), where \u03a8 denotes other important factors\nlike road network graph.\nLow-dimensional traffic data. The low-dimensional traffic data like speed, flow and travel time\nis crucial for various traffic applications such as route planning and traffic prediction. Missing it is a\ncommon issue due to sensor device damage or transmission disruptions. This has led researchers to\nemploy advanced generative AI techniques for imputing missing data using available measurements.\nVAE. In previous research [22], a VAE was employed to facilitate the imputation of missing traffic\ndata in an online unsupervised manner by drawing from the learned data distribution. Chen et\nal. [31] proposed a spatio-temporal VAE method for imputing missing data in traffic raster datasets.\nThis method leverages 3D gated convolution and a multi-attention network to effectively capture\nspatio-temporal dependencies under the VAE architecture.\nGAN. Recently, more researchers have adopted GAN-based methods to address this task [36,\n83, 108]. For example, Chen et al. [36] proposed using real or corrupted data as latent codes and\nintroducing representation loss within a GAN model, significantly improving the performance of\ntraffic data imputation. IGANI [108] introduced a novel iterative GAN imputation architecture,\nwhich iterated over imputed data while preserving the reversibility of the generative imputer.\nAn important characteristic of traffic data lies in its spatiotemporal correlation. Thus, numerous\nworks proposed to incorporate the spatio-temporal modeling in GAN-based architectures [91,\n92, 126, 128, 183, 207, 254, 255, 282, 282, 283]. For example, The authors in [139] introduced a 3D\nconvolutional GAN for imputing missing traffic data, utilizing a fractional-strided 3D convolutional\nneural network (CNN) for the generator and a 3D CNN for the discriminator to capture spatio-\ntemporal features efficiently. STGAN [262] incorporated generation and center losses to minimize\nreconstruction errors and maintain local spatio-temporal distributions, while the discriminator\nused a CNN classifier to assess global spatio-temporal distribution compliance, and the generator\nemployed skip connections and dilated convolutions to capture spatio-temporal correlations in\ntraffic data. GAE-GAN-LSTM [249] leveraged an enhanced graph autoencoder to extract spatio-\ntemporal features, and employed a GAN for generating complete spatio-temporal features from the\nmissing data, with a generator built on long short-term memory and a discriminator using a fully\nconnected neural network.\nDiffusion model. Recently, diffusion models have attracted more researchers\u2019 attentions in traffic\ndata imputation [151, 263, 281]. For instance, Liu et al. [151] introduced an enhanced prior modeling\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nMethod\nSpatio-Temporal Modeling\nPapers\nVAE\nMLP\n[22]\n3D gated CNN, multi-attention mechanism\n[31]\nGAN\n3D CNN\n[139]\nDilated CNN\n[262]\nCNN\n[255, 274]\nCNN, self-attention mechanism\n[283]\nTemporal: Gramian Angular Summation Field\n[91]\nTemporal: discrete Wavelet transform\n[92]\nTemporal: multi-view temporal factorizations\n[128]\nSpatial: learnable bidirectional attention map; Temporal: multi-channel matrix\n[254]\nSpatial: GCN; Temporal: LSTM\n[207, 249]\nSpatial: multi-layer FCN; Temporal: LSTM\n[126]\nSpatial: dynamic GCN; Temporal: multi-head self-attention network\n[183]\nSpatial: spatial attention GCN; Temporal: self-attention GRU\n[282]\nDPM\nSpatial: GNN\n[263]\nSpatial: GCN; Temporal: Transformer\n[151]\nSpatial: dynamic GCN; Temporal: global temporal convolution\n[281]\nTable 3. Spatio-temporal modeling in generative AI methods for low-dimensional traffic data imputation.\nconditional diffusion framework for spatio-temporal imputation, which effectively handles various\nmissing patterns and scenarios, including high missing rates and sensor failures, by leveraging\nconditional feature extraction and noise estimation modules. SaSDim [281] introduced a self-\nadaptive noise scaling mechanism based on diffusion model. This model incorporated a new loss\nfunction to scale the noise to a consistent intensity level, and a spatial-temporal global convolution\nmodule to capture complex spatial-temporal patterns.\nFinally, to provide a clear illustration of the spatio-temporal modeling within the generative AI\nframework for this task, we summarize the related works in Table 3.\nHigh-dimensional traffic data. High-dimensional traffic data, such as traffic videos and 3D\nLiDAR vision, plays a critical role, particularly in the context of autonomous driving, where\ncomplete dataset would greatly enhance situational awareness and decision-making capabilities.\nHowever, the loss of high-dimensional traffic data remains a prevalent issue in real-world scenarios.\nTo combat this challenge, Wu et al. [240] introduced a novel complementary model based on GANs\nfor addressing missing frames in traffic videos. The model utilized a feature pyramid network to\nextract multi-scale feature maps from input video frames, enhancing the integration of semantic\ninformation across frames by fusing features from different scales. The inclusion of local block\ndiscriminators within the discriminator model effectively ensured the accuracy and continuity of\nthe generated frames. TDC-GAN [239] utilized a multi-scale semantic information extraction model\nbased on the feature pyramid network and a discriminator model with global and local branches to\nensure spatio-temporal consistency in consecutive frames. For incomplete point clouds in LiDAR\napplications, Tu et al. [222] introduced a GAN-based point cloud repair network specifically applied\nwithin the autonomous driving context, enhancing overall safety in autonomous driving systems.\n3.2\nTraffic Estimation\nTraffic estimation aims to learn a estimator to estimate the current traffic condition of all roads in\nthe transportation network. This task differs from traffic data imputation in that it addresses the\nchallenge of incomplete data collection due to constraints such as cost and time limitations.\nSeveral works designed GAN-based approaches to estimate traffic condition [140, 166, 219,\n253, 259, 290, 291]. GAA [140] incorporated traffic-flow theory into a deep GAN model, while\nGCGA [259] combined the graph modeling capabilities of GCN and the generative capabilities\nof GAN . Curb-GAN [288] enhanced the conditional GAN structure by modeling various travel\ndemands. GE-GAN [248] learns road network features through graph embedding and applies them\nto a GAN network to estimate traffic conditions. PA-GAN [219] incorporated Bayesian inference to\nadaptively adjust parameters for various traffic patterns based on contextual features, while also\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nutilizing GAN to learn these traffic patterns from sparsely sampled data. Zhang et al. [291] models\nthe intricate correlations between road traffic and urban conditions in their proposed conditional\nGAN-based traffic estimation model. Unlike them, the authors in [261] introduced parameterization\nof noise factors in a diffusion-based model, transforming the traffic matrix estimation problem into\na gradient-descent optimization task.\n3.3\nTraffic Data Mining\nIn this section, we introduce the related works that utilize generative AI techniques to analyze traffic\ndata from various sources, including trajectories, traffic images, and traffic videos, as summarized\nin Table 4.\nTrajectories. Trajectories refer to a sequence of timestamped positions that describe the move-\nment or path of an object over time, which provides valuable insights for analyzing various aspects\nsuch as urban transportation modes and human decision-making preferences. To identify the trans-\nportation modes, the authors in [284] introduced a semi-supervised method based on the Dirichlet\nVAE, which fuses geographic information with motion features from GPS trajectories. Zhang et\nal. [286] proposed a trajectory GAIL framework to learn human decision-making behaviors. This\nframework modeled the human decision processes using Markov decision processes with vary-\ning lengths to capture long-term decision dependencies. Further, it inferred the decision-making\nstrategy from the historical dataset of the human agent in an inverse manner.\nTraffic images. In autonomous driving scenarios, traffic images captured by cameras offer\nvital information about the surrounding driving environment, enabling autonomous vehicles to\nmake accurate decisions. To overcome the challenge of limited traffic image diversity in real urban\nenvironments, Li et al. [138] introduced a GAN-based framework with structural information.\nThis approach enables various urban image transformations while maintaining the integrity of\nforeground objects and image structural details. Zhao et al. [297] focused on the quality of an in-\nvehicle image. They proposed a GAN-based single-image raindrop removal network that combines\ntask-specific visual attention and content perception mechanisms, allowing the network to focus\non raindrop regions and their surroundings for effective raindrop removal.\nUnlike them, some researchers focus on extracting valuable information from traffic images for\ntasks such as vehicle detection [229], license plate identification [20, 82, 94, 122], traffic density\nrecognition [70], driving behaviors [237] and map enrichment [299]. These tasks often face chal-\nlenges related to low resolution and inadequate lighting conditions. For example, the authors in [88]\npresented a novel vehicle detection approach that utilizes two different GANs to address low-light\ntraffic scenarios. Boby et al. [20] conducted a comparative analysis between a GAN-based super-\nresolution model and an iterative refinement approach utilizing diffusion-based super-resolution\nmodels for license plate detection. In addition, the authors in [70] presented a dual-discriminator\nconditional GAN for recognizing traffic density in both homogeneous and heterogeneous traffic\nscenarios.\nTraffic videos. Compared to traffic images, traffic videos provide a wealth of redundant traffic\ninformation for autonomous vehicles, guaranteeing both safety and efficiency in navigation. A\npivotal stage in numerous intelligent video processing applications, such as vehicle behavior\nanalysis, is the segmentation of moving objects. RMS-GAN [182] is an end-to-end GAN using a\nrecurrent technique, which integrates foreground probability information using residual and weight-\nsharing techniques to achieve precise segmentation. Krishna et al. [116] progressively combined\ngenerative and discriminative models to efficiently classify activities in traffic videos. Cheng et\nal. [39] proposed a deep conditional generative model to detect interactions between vehicles and\nvulnerable road users. This model utilized a conditional VAE-based framework with Gaussian latent\nvariables to capture the behavior of road users and predict interactions in a probabilistic manner.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nCategory\nTask\nMethod\nPapers\nTrajectory\nTransportation mode identification\nVAE\n[284]\nDecision-making behavior analysis\nGAN\n[286]\nTraffic image\nImage transformation\nGAN\n[138]\nImage denoise\nGAN\n[297]\nImage resolution enhancement\nGAN\n[20, 94]\nVehicle classification\nGAN\n[229]\nVehicle detection\nGAN\n[88]\nLicense plate recognition\nGAN\n[82, 122]\nTraffic density identification\nGAN\n[70]\nBehavior detection\nGAN\n[237]\nRoad annotation\nGPT\n[103]\nFlow inference\nNormalizing Flow\n[299]\nTraffic video\nVehicle-road user interaction detection\nVAE\n[39]\nObject segmentation\nGAN\n[182]\nActivity classification\nGAN\n[116]\nTable 4. Related works about traffic data mining.\n3.4\nTraffic Anomaly Detection\nTraffic anomalies refer to unusual events, patterns, or behaviors within a transportation system.\nThese anomalies can disrupt the regular traffic flow or pose potential safety risks. Numerous studies\nhave explored the use of generative AI techniques for traffic anomaly detection [117, 146, 186, 242].\nChen et al. [34] presented a multi-modal GAN model to detect and classify the traffic events,\nleveraging data from multiple modalities to enhance detection accuracy. The authors in [202]\nproposed a hybrid architecture that combines CNN and VAE to detect and classify anomalies.\nDifferent from the above GAN-based methods, the work by Kang et al. [106] employed conditional\nnormalizing flow to detect abnormal temporal snapshots, followed by road segment-level anomaly\ndetection using a kernel density estimator. A-VAE [10] was designed as an attention-based VAE\narchitecture for anomaly detection in traffic videos. It utilized a combination of 2D CNN and\nBiLSTM layers integrated with an attention network for effective representation learning.\nCyberattacks would pose significant safety risks in the context of automated driving. To address\nit, Zhao et al. [296] presented a GAN-based intrusion system for connected vehicles to evaluate the\nlegitimacy of incoming messages. Ding et al. [54] investigated the susceptibility of deep generative\nmodels in autonomous driving to Trojan attacks, revealing that manipulating training data and\ninjecting carefully crafted data can instill Trojan behaviors in a deep generative model without\naltering its original training objective. Li et al. [134] identified two potential cyberattack scenarios:\nindividual vehicle-targeted attacks and sensor measurement data injection attacks. To address these\nthreats, they introduced a real-time anomaly detection model using GANs for detection purposes.\n4\nGENERATIVE AI FOR TRAFFIC PREDICTION\nTraffic prediction refers to estimate or forecast future traffic conditions, encompassing various\naspects such as travel demand, travel time, traffic flow, and the movements and behaviors of both\nvehicles and individuals within the transportation system. Traffic prediction is vital for urban\ntraffic planning and management. However, achieving accurate predictions presents significant\nchallenges. First, urban traffic systems exhibit complex spatial and temporal dynamics. Second,\ntraffic conditions are highly dynamic, susceptible to rapid changes due to factors like real-time\nincidents, ongoing construction activities, and even human behavior. Third, obtaining high-quality\ndata from various sensors, cameras, and GPS devices is important for traffic prediction, but such data\nis often limited and incomplete. Generative AI technology offers powerful solutions to overcome\nthese challenges. In our comprehensive literature review, we concentrate on the utilization of\ngenerative AI in addressing traffic prediction challenges. We categorize these works into four main\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nGenerative AI for \ntraffic prediction\nHuman-related \nprediction\nVehicle-related \nprediction\nRoad segment-\nrelated prediction\nRegion-related \nprediction\nMicro \nscale\nMacro \nscale\nIndividual behavior\nHuman mobility\nVehicle trajectory\nCrash risk\nFig. 4. The relation between different topics in generative AI for traffic prediction research.\naspects: human-related, vehicle-related, road segment-related and region-related traffic prediction,\nencompassing the key components of transportation systems. Their relation is given in Figure 4.\n4.1\nHuman-related Traffic Prediction\nHuman-related traffic prediction involves forecasting individual behavior and human mobility.\nIndividual behavior. We discuss individual behavior prediction based on their roles within\ntransportation systems. For drivers, modeling and predicting driver behavior plays a crucial role in\nenhancing safety and preventing risky actions [15, 97]. For example, Bao et al. [15] introduced a\nprobabilistic sequence-to-sequence approach utilizing a conditional VAE to predict various driving\nbehaviors. For pedestrians, accurately forecasting human motion behavior is vital for autonomous\nvehicles to plan preemptive actions and avoid collisions, ensuring pedestrian safety [46, 217]. For\ninstance, in [46], the authors proposed a temporal convolutional GAN for high-fidelity future\npose prediction, incorporating two discriminators: one for fidelity assessment and another for\nconsistency evaluation in long sequences.\nHuman mobility. Human mobility refers to an individual\u2019s movement from one location to\nanother. Feng et al. [64] employed GAIL to model courier decision-making in route choice, enabling\nthe prediction of their future routes. The COVID-19 pandemic presented a great challenge in\npredicting human mobility. Bao et al. [14] introduced a spatio-temporal conditional GAN Covid-\nGAN to estimate mobility during the pandemic. Further, Covid-GAN+ [13] addressed spatial\nheterogeneity by using the local Moran statistic and outliers by redesigning the training objective\nto learn the estimated mobility changes.\n4.2\nVehicle-related Traffic Prediction\nIn the reviewed literature, vehicle-related traffic prediction tasks encompass vehicle trajectory\nprediction and crash risk prediction.\nVehicle trajectory. A vehicle trajectory refers to the path a vehicle takes from one location to\nanother over a specific time period. Precise vehicle trajectory prediction is vital for a variety of\ntraffic-related applications, including traffic management, navigation, and route planning. However,\nachieving precise trajectory predictions is a challenging task for several reasons. First, vehicle\nmovements are influenced by various factors, including driver behavior, traffic conditions, road\ninfrastructure, and unexpected events. Second, traffic conditions can change rapidly due to incidents,\ncongestion, or accidents. Third, the accuracy of trajectory prediction models heavily relies on the\nquality and quantity of input data, such as GPS data. Noisy or incomplete data can lead to inaccurate\npredictions. To address these challenges, more researchers proposed different methods based on\ngenerative AI technology to predict vehicle trajectory.\nMost problems of vehicle trajectory prediction are defined as using historical trajectories to infer\nfuture trajectories. Formally, given a vehicle \ud835\udc56with a historical trajectory x\ud835\udc56= {\ud835\udc5f\ud835\udc61\n\ud835\udc56},\ud835\udc61= 1, 2, ...,\ud835\udc47,\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nwhere \ud835\udc5f\ud835\udc61\n\ud835\udc56is location information (e.g., longitude and latitude) at time \ud835\udc61. The task is to learn a\nmapping function \ud835\udc54(\u00b7) to predict the future trajectory y\ud835\udc56= {\ud835\udc5f\ud835\udc61\n\ud835\udc56},\ud835\udc61= \ud835\udc47+ 1, ...,\ud835\udc47+ \ud835\udc5b. Many scholars\nadopt different generative AI techniques to address above challenges.\nVAE. Several researchers have designed prediction models based on VAE [40, 111, 176, 243].\nDVAE [172] incorporated expert knowledge into the decoder of a VAE, which can provide the\nsimilar prediction accuracy and enhance interpretability. The authors in [98, 99] utilized conditional\nVAEs to encode past trajectories of vehicles and traffic scenes for vehicle path prediction. Gui et\nal. [78] incorporated more information like yaw angle and velocity in their proposed conditional\nVAE-based trajectory prediction model.\nGAN. GAN is also an alternative technique for trajectory prediction [135, 200, 302]. For example,\nKang et al. [104] applied social GAN, originally designed for pedestrian trajectory prediction, for\nthe prediction of vehicle trajectories. Differently, Zhou et al. [302] focused on predicting trajectories\nof surrounding vehicles. They introduced a self-attention social GAN, which not only addressed\nthe challenge of missing information from long trajectory sequences but also better modeled the\ninteractions between vehicles.\nGPT. Recently, GPT models has great potential in vehicle trajectory prediction [63, 214, 225].\nSu et al. [214] proposed a crossmodal transformer based generative framework, which leveraged\nsequences of cues from various modalities and pedestrian attributes for more accurate predictions.\nIn this model, transformer is adopted to capture the cross-relation patterns of different modality\npairs. Feng et al. [63] developed a pre-trained large traffic model using the Transformer architecture.\nThis model has ability to capture the diverse trajectories within a population of vehicles, leading to\nhigher accuracy in vehicle trajectory prediction.\nDPM. Diffusion models also attract more researchers\u2019 attentions. For instance, EquiDiff [32]\ncombined the conditional diffusion model with an equivariant transformer, leveraging the geometric\nproperties of position coordinates and social interactions among vehicles for accurate prediction.\nCrash risk. Crash risk prediction is important in traffic safety management and precaution.\nDue to imbalanced data between crash and non-crash cases, accurately predicting crash risk is\nchallenging. To deal with this, Cai et al. [24] proposed a deep convolutional GAN model to generate\nmore synthetic data about crashes. Similarly, in [159], Wasserstein GAN was developed to address\nthe imbalanced data. Different from approaches using GAN, Zhang et al. [271] proposed an enhanced\naccelerated testing method based on Normalizing Flows. This method leverages Normalizing Flows\nto learn the distribution of rare events and preserve the correlated relationships between scenario\nvariables.\n4.3\nRoad Segment-related Traffic Prediction\nRoad segment-related traffic prediction focuses on forecasting traffic conditions (e.g., traffic flow,\nspeed, travel time) for specific segments or sections of roads within a transportation network.\nAccurate prediction faces three main challenges. First, traffic conditions within the same road\nsegment can vary over time, but there is often similarity between adjacent time periods. Additionally,\nthe traffic state of one road segment can affect neighboring segments, resulting in complex spatial\nand temporal patterns in traffic conditions. Second, the limited number of road sensors and the\npossibility of sensor malfunctions can result in missing or inaccurate data, posing challenges for\nensuring data quality and completeness. Third, external factors like accidents and weather further\nenhance the uncertainty of prediction. Generative AI technology has gained significant attention\nfrom scholars due to its ability to learn data distributions and model uncertainties. Numerous\nmodels based on generative AI technology are proposed for predicting traffic flow and travel time.\nTraffic flow prediction. The task of traffic flow prediction is to learn a non-linear function \u210e(\u00b7)\nfrom previous \ud835\udc47time steps of traffic conditions to forecast traffic conditions for the next \ud835\udc47\n\u2032 steps\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nTask\nMethod\nSpatio-temporal modeling\nPapers\nTraffic flow prediction\nVAE\nTemporal: LSTM\n[101]\nGAN\nTemporal: LSTM\n[247]\nSpatial: GCN\n[258, 298]\nSpatial: GCN, Temporal: RNN\n[100]\nSpatial: GCN, Temporal: GRU and self-attention\n[109]\nSpatial: GCN, Temporal: TCN\n[132]\nSaptial: GraphSAGE; Temporal: LSTM\n[293]\nSpatial: GCN; Temporal: attention network\n[51]\nNormalizing flows\nAutoregressive network with convolution operations\n[265]\nDPM\nSpatial: GCN, Temporal: TCN\n[236]\nTemporal: RNN\n[192]\nTravel time estimation\nGAN\nSpatial: GCN\n[213]\nDPM\nMasked Vision Transformer\n[148]\nTable 5. Related works about road segment-related traffic prediction.\nbased on road network graph \ud835\udc3a. Many scholars employ different generative AI techniques to tackle\nthis task. We summarize the related works in Table 5.\nVAE. Some researches propose VAE-based models. The authors in [21] designed a VAE-based\nprediction model with the ability to learn the data distribution. PFVAE [101] employed LSTM as the\nautoencoder and designed the VAE as a predictor for time-series data to deal with the noise issues.\nGAN. GAN-based models are commonly adopted in this task [51, 145, 204, 275, 292]. For example,\nForGAN [113] employed GAN to learn the data generating distribution and generate probabilistic\ntraffic forecasts. Furthermore, many researchers designed several methods to model spatial and\ntemporal correlations in their proposed GAN-based prediction model [100, 109, 132, 247, 258, 266,\n293, 298]. For example, PL-WGAN [100] utilized GCN, RNN and attention mechanism to model\nspatiotemporal correlations in a Wasserstein GAN-based model. ASTGAN [150] used the attention\nmechanism and an mask graph convolutional recurrent network to characterize the temporal and\nspatial dependencies.\nNormalizing flows. Consider the capability of conditional normalizing flows in representing com-\nplex distributions characterized by high dimensionality and strong interdimensional relationships.\nMotionFlow [265] utilized conditional normalizing flows for traffic prediction, which conditionally\nmodels the output distributions on spatio-temporal input features in an autoregressive manner.\nDPM. Because DPM can effectively handle uncertainties within spatio-temporal graph neural\nnetwork (STGNN), DiffSTG [236] combined the spatiotemporal learning capability of STGNN with\nthe uncertainty measurement of DPM, resulting in higher prediction accuracy. The authors in [192]\ndeveloped an autoregressive denoising diffusion model for multivariate probabilistic time series\nforecasting, utilizing gradient estimation to draw samples from the data distribution at every time\nstep.\nTravel time estimation. Travel time estimation task aims to estimate the duration it takes to\ntravel between two locations or along a specific route given the departure time. Several scholars\nemploy generative AI techniques to address this task [148, 213]. To be specific, GCGTTE [213]\nintegrated GCN and GAN to estimate the travel time in a probabilistic distribution form. Lin et\nal. [148] proposed a two-stage diffusion-based origin-destination (OD) travel time estimation model.\nThey incorporated a conditioned pixelated trajectory denoiser into the model to capture correlations\nbetween OD pairs and historical trajectories.\n4.4\nRegion-specific Traffic Prediction\nRegion-specific traffic prediction focuses on forecasting traffic conditions such as traffic flow\nwithin specific geographic regions in a transportation network. The main challenges in achieving\naccurate predictions include complex spatio-temporal dependencies, limited data, and noise. Recent\nworks [62, 127, 169, 273, 287, 289] have aimed to employ generative AI techniques to tackle these\nchallenges. To be specific, Zhang et al. [272, 273] proposed a spatiotemporal GAN model for\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nGenerative AI for \ntraffic simulation\nDriver behavior \nsimulation\nTraffic scenario \ngeneration\nTraffic flow \ngeneration\nMicro \nscale\nMacro \nscale\nVehicle trajectory\nHuman trajectory\nAnomaly data\nNormal  \nscenario\nAnomaly  \nscenario\nFig. 5. The relation between different topics in generative AI for traffic simulation research.\npredicting traffic flow in subway stations, which used gated TCN and weight sharing GCN to\ncapture spatiotemporal correlations. Kang et al. [105] incorporated gated convolution and dilated\nconvolution to capture the traffic patterns in urban regions for region-level traffic prediction. Li\net al. [131] proposed a seq2seq spatial-temporal semantic GAN model, which takes into account\nboth semantic factors and spatio-temporal features. The authors in [169] incorporated recurrent\nnetwork mode and conventional network model into GAN to predict the taxi demand in specific\nareas.\n5\nGENERATIVE AI FOR TRAFFIC SIMULATION\nTraffic simulation involves the mathematical modeling of transportation systems, which can be\nused to generate the movement and behavior of vehicles and pedestrians. This technology can be\napplied in various fields, including urban planning, evaluation of policy changes, and testing and\nvalidating autonomous vehicle algorithms. However, achieving a more realistic traffic simulation\nposes several challenges. First, real-world traffic scene data is typically collected through road\ncameras or sensors. However, obtaining such data is not only costly but also challenging to\ncapture in rare or hazardous situations. Second, traffic dynamics are inherently complex due to the\nnumerous variables involved, ranging from individual driver behaviors, decisions, and reactions\nto the unpredictable nature of external factors such as weather or sudden events. Capturing\nthe intricacies of these dynamics is a considerable challenge. Additionally, the interconnected\nrelationships between vehicles, infrastructure, and pedestrians further complicate the situation.\nExisting works using traditional deep learning methods [81, 241] often require extensive labeled\ndata, and may not generalize well to unseen or rare situations. In addition, these methods may\nstruggle to capture the vast array of dynamic interactions in traffic scenarios [129, 235].\nGenerative AI techniques offer potential solutions for traffic simulation challenges. They can\ncreate realistic traffic scenarios, including rare or extreme situations that might be missing from\nreal-world data. Their adaptability allows them to adjust to ever-changing traffic dynamics, thereby\ncapturing intricate interactions more holistically. Furthermore, by learning from a small number of\nreal traffic data, they can refine their simulations continuously, ensuring they mirror the complexities\nof real-world traffic more closely. Next, we will introduce the related works using generative AI\ntechniques from three aspects: driver behavior simulation, traffic scenario generation and traffic\nflow generation. We draw their relation in Figure 5.\n5.1\nDriver Behavior Simulation\nIn autonomous driving scenarios, the focus is on how drivers make decisions in response to traffic\nconditions [30, 73, 210, 257, 264]. Yun et al. [264] utilized GAN to learn the human-driving abilities\nin complex traffic scenarios. Chen et al. [30] proposed a novel two-stage driver model that integrates\nmodel-based and data-driven techniques, leveraging reward-augmented GAIL to produce reliable\nhuman driving behaviors. Jin et al. [102] proposed a generative driver agent simulation framework\nbuilt upon LLM to simulate human driving behaviors based on human driver experience. To alleviate\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nTask\nMethod\nControllability\nPapers\nVehicle trajectory generation\nVAE\nUnconditional\n[35, 276]\nGAN\nUnconditional\n[44, 118, 141, 141, 179]\nDPM\nUnconditional\n[216, 246, 251, 306]\nVAE, GAN\nUnconditional\n[114, 115]\nGAN\nConditional\n[9]\nDPM\nConditional\n[300, 301]\nHuman trajectory generation\nGAN\nUnconditional\n[244]\nDPM\nConditional\n[194]\nAnomaly data generation\nVAE\nUnconditional\n[24, 96]\nGAN\nUnconditional\n[33, 38, 93]\nGAN\nConditional\n[267]\nTable 6. Related works about traffic scenario generation.\nthe computational load of large-scale simulations, Ye et al. [257] proposed an efficient method for\ndynamically calculating agent decision parameters in ABM calibration by using VAEs to accelerate\nstate transfer probability computation.\nLane changing and car following are two common driving behaviors, and numerous studies have\nexplored methods for simulating these behaviors [55, 133, 144, 156]. Ma et al. [156] introduced a\nphysics-informed conditional GAN that integrates the advantages of both physics-based and deep\nlearning models to to improve the modeling of car-following behavior over multiple time steps in\ncomplex traffic flow environments. Lin et al [144] introduced a constrained-GAIL framework that\nemploys reward augmentation to guide driving agents in avoiding undesirable states by imposing\nadditional constraints through a manually designed reward function. In [55], the authors leveraged\nthe transformer and diffusion models to generate lane-changing trajectories that closely mimic\nhuman behavior.\nUnlike autonomous driving scenarios, in transportation services, a variety of factors such as\nestimated travel demand tend to dominate drivers\u2019 behaviors. For example, Zhang et al. [285]\ndeveloped conditional GAIL model capable of learning the driver\u2019s decision-making tendencies\nand strategies through knowledge transfer among taxi driver agents and across various locations.\n5.2\nTraffic Scenario Generation\nTraffic scenario generation refers to produce specific traffic situations or anomalous events for the\npurpose of testing and validating autonomous driving systems. As autonomous vehicles need to\noperate safely in a vast array of driving conditions, it is crucial for developers to test these systems\nagainst a wide range of potential real-world scenarios, especially those that are rare or potentially\nhazardous. In this literature review, we will focus on three key areas: generating vehicle trajectories,\ngenerating human trajectories, and generating anomaly data, as summarized in Table 6.\nVehicle trajectory generation. Many researchers use generative AI technology to generate\nmore vehicle trajectory data [35, 44, 114, 115, 118, 141, 179, 216, 246, 276]. In [115], the authors\nintroduced trajectory GAN and trajectory VAE to generate realistic synthetic lane change maneuver\ntrajectories without expert knowledge. Diff-Traj [306] was proposed for generating GPS trajec-\ntories by effectively learning spatio-temporal features from historical trajectories and leveraging\nthe generation capability of the diffusion model. Xu et al. [251] designed a generative AI-based\nautonomous driving architecture that utilizes large text-to-image models based on DPM to generate\nconditioned traffic and driving data in simulations. Differently, some searchers focused on the\ncontrolled vehicle trajectory generation [9, 300, 301]. For example, the authors in [9] introduced a\nrecurrent conditional GAN model that generates sensor errors with long-term temporal correlation\nin both the generator and the discriminator networks. Zhong et al. [300] introduced a scene-level\nconditional diffusion model that incorporates a spatio-temporal transformer architecture to produce\nboth realistic and controllable traffic data.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nHuman trajectory generation. Regarding human trajectory generation, Xiong et al. [244]\nproposed a semantic-guiding GAN model equipped with semantic knowledge to generate human\ntrajectories. Rempe et al. [194] studied controlled pedestrian trajectory generation. They utilized\nguided diffusion modeling for test-time controllability, and integrating it with a physics-based\nhumanoid controller for a closed-loop system that places large crowds in various terrains while\nmeeting user-defined goals.\nAnomaly data generation. Anomaly data such as traffic accident and crash data is essential\nin autonomous driving for safety assessment and scenario testing. However, this type of traffic\ndata is hard to collect. For example, as illustrated in [24], the ratio of crash data to non-crash data\nis 1 to 11,000, indicating a significant imbalance. Several works use generative AI techniques to\naddress this problem [33, 38, 93, 96, 267]. For instance, Islam et al. [96], employed VAE to encode\nevents into a latent space, from which they subsequently sampled to generate crash data. In [93], a\ntext-to-traffic GAN model was proposed, which integrates traffic data with semantic information\ncollected from social media to produce traffic situations. Zarei et al. [267] developed a conditional\nGAN-based model that utilizes crash count as a condition to generate crash data.\n5.3\nTraffic Flow Generation\nTraffic flow generation refers to generate vehicular traffic volumes or crowd flow within a specific\narea or between regions in a transportation network. Numerous works adopt generative AI tech-\nniques to address this task [37, 130, 198, 199, 304]. Specifically, Li et al. [130] presented an attentive\ndual-head spatial-temporal GAN model for generating crowd flow. This model incorporated an\nattentive mechanism for tracking temporal changes and a self-attention network to capture intricate\nspatiotemporal crowd flow patterns, and it adopted a dual-head discriminator that implements a\ntraining strategy with two distinct objectives to mitigate the adverse effects of rapid overfitting.\nZhou et al. [304] constructed an urban knowledge graph to extract region features and model the\nurban environment, and they subsequently designed a knowledge-aware spatio-temporal diffusion\nmodel for generating urban flow. In [198], the authors employed a graph denoising diffusion tech-\nnique to generate urban OD flow, enabling the learning of how the likelihood of nodes and edges\nare interconnected in the OD network.\n6\nGENERATIVE AI FOR TRAFFIC DECISION-MAKING\nIn autonomous driving scenarios, traffic decision-making is a series of decision-making processes\nmade by vehicles based on current dynamic traffic environment and task requirements when\ndriving on the road, such as driving routes, speeds, driving modes and so on. It is a challenging task\nfor ensuring that vehicles make safe and accurate decisions in complex environments. The first\nchallenge we face is dealing with the dynamic and unpredictable nature of traffic environments.\nThese environments are constantly changing, with various factors like road conditions, unexpected\nmoves by human drivers, and sudden decisions by pedestrians and cyclists. For example, a driver\nmight suddenly brake or change lanes without warning, or pedestrians might cross the road\nunexpectedly. These uncertainties compel autonomous systems to be perpetually alert and adaptive,\nwhich becomes computationally intensive and requires robust prediction algorithms. The second\nchallenge involves rare but critical events that can test the safety of autonomous vehicles, such as a\nchild running onto the road or a mechanical failure causing loss of control. Gathering real-world\ndata on these rare events is difficult but crucial for creating a comprehensive training dataset.\nWithout exposure to these scenarios, autonomous systems may not know how to handle them\nproperly, potentially leading to safety risks. The third challenge relates to the limitations of sensors\nlike LiDAR, RADAR, and cameras. These sensors have restrictions in terms of their range, resolution,\nand susceptibility to environmental factors like fog, rain, and snow. These limitations can sometimes\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nlead to uncertainty or incorrect decision-making by the autonomous vehicle, which can compromise\nboth safety and efficiency.\nTraditional decision-making methods, such as rule-based methods [49, 164] and game theory-\nbased methods [84, 226, 295], often have limited effectiveness in dealing with complex traffic\nsituations, while reinforcement learning (RL), as a flexible, efficient, and powerful method, has\ngained ground in autonomous driving decision-making. However, its general application is hindered\nby training complexity, especially in handling multiple tasks. Generative AI technology holds\npromise in tackling these challenges. Leveraging their robust generative capabilities, they can\ngenerate realistic training scenarios mimicking the unpredictability of real-world driving, reducing\nthe need for extensive road testing. Moreover, they can produce rare traffic events to ensure\nautonomous systems are robust against critical scenarios. Additionally, generative AI can enhance\nsensor data, helping vehicles adapt to challenging environmental conditions.\nRecently, researchers have shifted their focus to using generative AI techniques in traffic decison-\nmaking. Wang et al. [230] proposed an autonomous decision-making approach for high-speed\ntrains, integrating GAN-based learning approach and a distributed tracking controller. Addressing\ndata insufficiency, they introduced a GAN-based data augmentation method to generate time\nseries samples for training purposes. They then developed a trajectory prediction network that\ncompute the reference speed trajectory in real-time conditions. Further, they designed a distributed\ntracking control model, utilizing model predictive control and dual decomposition, to optimize\ntrain movement.\nLiu et al. [149] proposed a multi-task decision-making GPT model for autonomous driving at\nunsignalized intersections. By leveraging the exceptional learning capabilities and computational\nefficiency of RL, this model had ability to simultaneously manages multiple driving tasks. The\nauthors first trained RL expert models for single decision-making tasks, and then used the sampled\nexpert data to guide the offline GPT training. As a result, the model outperforms conventional\nsingle-task RL decision-making models.\n7\nOPEN CHALLENGES AND FUTURE DIRECTIONS\nWhile generative AI methods have demonstrated much success in the realm of intelligent trans-\nportation systems, several challenges must be addressed. In this section, we will discuss the primary\nopen challenges, followed by an introduction to some potential research directions.\n7.1\nOpen Challenges\nIn this subsection, we will discuss several open challenges for generative AI methods in intelligent\ntransportation systems.\nMulti-modal traffic data. Multi-modal traffic data integrates various data sources and types,\nincluding roadside camera videos, traffic signal sensor data, weather conditions, traffic incident\nreports, and vehicle GPS trajectories. These data provides a comprehensive view of dynamic traffic\nconditions in transportation systems. Leveraging generative AI technology to address traffic-related\nproblems with multi-modal data present several challenges. Firstly, the correlations of multi-\nmodal data are more complex than those of single-modal data. For example, the characteristics\nof images and texts, as well as the differences in data distributions and representation methods,\nare distinct. Secondly, accurate alignment between different modalities is required, e.g., matching\ntraffic camera images with traffic sensor data or weather information at the same time. Additionally,\ntraining models on multi-modal data can be more unstable, especially when maximizing mutual\ninformation across modalities. Therefore, how to effectively use generative AI technology to tackle\ntransportation challenges using multi-modal data remains challenging in the field of transportation.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nComplex spatio-temporal correlations. Traffic involves complex relationships in time, space,\nand spatiotemporal interactions. In the temporal dimension, traffic data can be influenced by\nseasonal variations such as weekdays, weekends, and holidays. Simultaneously, the traffic flow at a\nspecific moment can be impacted by the traffic patterns at that location over the preceding few\nhours. In the spatial dimension, traffic data involves the traffic conditions at different locations,\nand it is necessary to consider the traffic flow and congestion between different locations, and\nanalyze their correlation. Moreover, there is a complex interaction between time and space in traffic\nconditions, and the traffic flow at a location may be affected by the traffic flow at adjacent locations\nin the past few hours. While prior research has explored the use of RNNs, CNNs, and GNNs in\ngenerative AI-related solutions to model spatiotemporal relationships, yielding some promising\nresults, most of these approaches focus on local or micro-scale spatiotemporal modeling and fail\nto capture global or macro-scale spatiotemporal patterns. Furthermore, spatiotemporal modeling\nremains somewhat disconnected from the core principles of generative AI. Thus, the challenge in\nleveraging generative AI technology to address traffic-related issues lies in effectively modeling the\nintricate spatiotemporal correlations within traffic data.\nSparse or missing traffic data. The sparsity or missing of traffic data can be attributed to\nvarious factors, including equipment failures or routine maintenance, network communication\nissues, and concerns related to the costs and privacy of data collection. The sparse or missing nature\nof traffic data can have significant implications for traffic management and planning, including\ninaccurate traffic predictions and an incomplete understanding of traffic conditions. Generative AI\nhas powerful data generation capabilities and can generate relatively realistic data by leveraging the\ncharacteristics of existing data. To address the problem of data sparsity or absence, many researchers\nuse generative AI technology to perform tasks such as data imputation, traffic data estimation,\nand traffic scene generation, but there are still shortcomings. First, due to the spatial-temporal\ncomplexity of traffic data, generative AI models may generate some seemingly realistic data that is\ninconsistent in space and time. Second, when generating data, the generative model may tend to\ngenerate common patterns in training data while neglecting less common cases. This may lead\nto a lack of diversity in the generated data and inability to cover all the changes in traffic data.\nThird, different degrees of sparsity also have a significant impact on the generation quality, and\nhow to flexibly cope with this is an important issue. Therefore, overcoming the challenges of\ndata sparsity and absence in addressing traffic issues with generative AI technology remains an\nimportant problem that needs further research.\nAdversarial attacks. In the realm of intelligent transportation, generative AI technology offers\ncutting-edge solutions that drive continuous advancements in traffic management and optimization.\nHowever, the widespread adoption of this technology also introduces new security risks, particularly\nthose associated with adversarial attacks [67]. One of the most notable concerns is the backdoor\nattack, where malicious actors may inject harmful data during the model training phase, causing\nthe model to generate inappropriate outputs under specific conditions. In complex intelligent\ntransportation networks, a single misjudgment or erroneous decision can trigger a chain reaction of\ntraffic accidents or congestion, resulting in substantial economic and societal losses. For instance, an\nintelligent transportation monitoring system compromised by a backdoor attack may erroneously\nclassify a safe vehicle as a potential threat, or an autonomous vehicle may make incorrect driving\ndecisions as a result. Therefore, the critical challenge lies in accurately identifying potential risk\npoints and devising effective methods to ensure the efficient and secure operation of transportation\nsystems.\nModel interpretability. Model interpretability is crucial in intelligent transportation systems.\nFor example, in autonomous driving scenarios, it is necessary to predict the behavior of other road\nusers (pedestrians, cyclists, other vehicles) to ensure safe driving. This requires the model to explain\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nwhy a particular behavior is considered the most likely, such as why the model believes a pedestrian\nin front may cross the road. Although generative AI models perform well in processing traffic data\nand optimizing transportation systems, these models are often considered black boxes, making\nit difficult to understand their decision-making processes and reasoning foundations. However,\ndesigning interpretable generative AI models is challenging. This is because traffic systems are\nvery complex, involving many variables and uncertain factors such as road conditions, traffic flow,\npedestrian behavior, and other vehicle behavior. These complexities and uncertainties make it\ndifficult to design a simple and accurate model to explain traffic data and autonomous driving\ndecisions. In autonomous driving, autonomous driving decisions involve multiple levels, including\nperception, planning, and control, and designing an interpretable model requires being able to\nexplain the decision-making processes at these different levels, which are often very complex.\nReal-time requirements. In intelligent transportation systems, taking timely and effective\nresponse measures is of paramount importance. This becomes particularly critical when dealing with\nrapidly changing situations and unforeseen events. In such scenarios, traffic-related models must\npossess real-time perception capabilities, including the detection of other vehicles, pedestrians, and\nobstacles, while simultaneously making split-second decisions to ensure safe driving. In addition,\ntraffic congestion can lead to rapid changes in traffic flow, so these models need to have the ability\nto predict real-time changes in traffic flow to take measures in advance and avoid traffic accidents\nand congestion. However, achieving this high level of responsiveness poses some challenges in\ngenerative AI-based solutions. First, real-time data may be affected by noise and incompleteness, so\nmodels must have the ability to handle imperfect data to accurately perceive the traffic environment.\nSecond, certain generative AI methods may be very complex and require more time for reasoning.\nIn real-time applications, it is necessary to balance the complexity and real-time performance of\nthe model. Third, real-time applications should be equipped to handle exceptional circumstances\nlike sensor failures or sudden events. Generative AI methods need to be capable of identifying such\nanomalies and executing appropriate actions to ensure both safety and reliability.\n7.2\nPotential Research Directions\nLarge-scale language models designed using generative AI technology have achieved significant\nsuccess in fields such as NLP and image processing, which also provide crucial opportunities and\nresearch directions for addressing traffic-related challenges. In this subsection, we will introduce\nseveral important research directions from three distinct perspectives concerning large-scale traffic\nmodels.\nIntegrating multi-modal data for large-scale traffic models. Traffic data usually come from multiple\ndifferent sources, including images and video data from traffic cameras, as well as GPS trajectory\ndata from vehicles, covering a variety of different data types. These data offer multiple perspectives\non the spatiotemporal dynamics of traffic conditions, facilitating a deeper comprehension of traffic\noperation\u2019s underlying mechanisms and patterns. Therefore, these data serve as an important\nfoundation for training large-scale models in the field of transportation. Efficiently using these\ndata for model training is crucial and also an important research direction. In addition to aligning\nthe data in the spatiotemporal dimension, it is necessary to unify their representation, mapping\ndifferent data modalities to the same data space. This requires designing appropriate mapping\nmethods to ensure that the mapped data can effectively capture spatial and temporal dependencies\nacross different data modalities.\nDesigning and training well-performed large-scale traffic models. Traditional transportation models\nand algorithms are typically developed separately for specific tasks, necessitating significant time\nand resource investments for model adjustments when encountering new tasks or changing prob-\nlem requirements. Furthermore, these conventional models often exhibit excessive specialization,\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nrendering cross-task transfer learning challenging and thereby limiting their overall generalizability\nand scalability. In contrast, large-scale language models based on generative AI technology have\nhigher generalizability and scalability, and possess the potential for transfer learning in a variety of\ntraffic tasks. For instance, in autonomous driving, which encompasses continuous tasks such as\nperception, prediction, and decision-making, we can design a large-scale transportation model to\nhandle all three tasks simultaneously. In model design, special attention needs to be paid to shared\nfeatures, such as spatiotemporal characteristics, and these features should be modeled effectively\nwhile combining the uniqueness of each task to achieve consistent excellent performance across\nall tasks. Additionally, another potential research direction is to develop large-scale traffic models\ndriven by both data and knowledge, leveraging the diversity of traffic data and the reliability of\ntraffic physical knowledge to effectively model traffic patterns. Finally, it is crucial to consider how\nto ensure that the designed large-scale model can achieve the desired performance and effectiveness.\nThis requires the development of reasonable dataset annotation strategies and training strategies.\nTraffic planning and decision-making based on large-scale language models. Traffic planning and\ndecision-making are important research problems in the transportation domain. In real-world\nscenarios, traffic conditions are constantly changing, with the possibility of incidents such as traffic\ncongestion, accidents, and adverse weather conditions, all of which may impact the effectiveness of\ntraffic planning and decision-making. Therefore, it is essential to develop real-time traffic planning\nand decision-making algorithms based on large-scale language models. This entails exploring\nhow to combine the real-time traffic perception capabilities of generative AI technology with the\ndecision-making capabilities of RL. Such an approach holds the potential to yield more efficient\nand adaptable solutions to tackle the intricacies and uncertainties inherent in traffic management.\nAs an illustrative example, consider driving decision-making in autonomous vehicles. Vehicles\nneed to make decisions such as overtaking, decelerating, and stopping based on perceived data\nand environmental conditions. Large-scale models, equipped with powerful knowledge reasoning\nand generation capabilities, are expected to achieve precise traffic decisions in complex traffic\nenvironments. Therefore, future research can focus on how to design algorithms based on large-\nscale language models to make accurate decisions while ensuring a balance in real-time performance\nand safety.\n8\nCONCLUSIONS\nGenerative AI technology is playing an increasingly important role in perception, prediction,\nsimulation, and control of transportation systems, providing powerful tools and methods to enhance\ntraffic efficiency, safety, and sustainability. In this survey, we systematically investigate the critical\nsolutions of generative AI techniques within intelligent transportation systems, and explore the\nchallenging issues that generative AI has not yet fully addressed in this field. We hope that this\nsurvey will promote further innovation and development of generative AI technology in the\nfield of intelligent transportation systems, assist researchers in effectively utilizing generative\nAI technology to tackle more practical traffic problems, and encourage the development of more\nadvanced generative AI methods targeting the transportation domain.\nREFERENCES\n[1] Johannes Ackermann and Minjun Li. 2022. High-resolution image editing via multi-stage blended diffusion. arXiv preprint\narXiv:2210.12965 (2022).\n[2] Daniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha,\nGaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977 (2020).\n[3] Abasi-Amefon O Affia, Raimundas Matulevi\u010dius, and Alexander Nolte. 2019. Security risk management in cooperative\nintelligent transportation systems: A systematic literature review. In On the Move to Meaningful Internet Systems: OTM\n2019 Conferences: Confederated International Conferences: CoopIS, ODBASE, C&TC 2019, Rhodes, Greece, October 21\u201325, 2019,\nProceedings. Springer, 282\u2013300.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n[4] Qazi Ejaz Ali, Naveed Ahmad, Abdul Haseeb Malik, Gauhar Ali, and Waheed Ur Rehman. 2018. Issues, challenges, and\nresearch opportunities in intelligent transport system for security and privacy. Applied Sciences 8, 10 (2018), 1964.\n[5] Insha Altaf and Ajay Kaul. 2021. A survey on autonomous vehicles in the field of intelligent transport system. In Applications\nof Networks, Sensors and Autonomous Systems Analytics: Proceedings of ICANSAA 2020. Springer, 11\u201331.\n[6] Mohammed Arif Amin, Samah Hadouej, and Tasneem SJ Darwish. 2019. Big data role in improving intelligent transportation\nsystems safety: A survey. In Advances in Internet, Data and Web Technologies: The 7th International Conference on Emerging\nInternet, Data and Web Technologies (EIDWT-2019). Springer, 187\u2013199.\n[7] Sheng-hai An, Byung-Hyug Lee, and Dong-Ryeol Shin. 2011. A survey of intelligent transportation systems. In 2011 third\ninternational conference on computational intelligence, communication systems and networks. IEEE, 332\u2013337.\n[8] Sesham Anand, P Padmanabham, A Govardhan, and Rajesh H Kulkarni. 2018. An extensive review on data mining methods\nand clustering models for intelligent transportation system. Journal of Intelligent Systems 27, 2 (2018), 263\u2013273.\n[9] Henrik Arnelid, Edvin Listo Zec, and Nasser Mohammadiha. 2019. Recurrent conditional generative adversarial networks for\nautonomous driving sensor modelling. In 2019 IEEE Intelligent transportation systems conference (ITSC). IEEE, 1613\u20131618.\n[10] Nazia Aslam and Maheshkumar H Kolekar. 2023. A-VAE: Attention based Variational Autoencoder for Traffic Video Anomaly\nDetection. In 2023 IEEE 8th International Conference for Convergence in Technology (I2CT). IEEE, 1\u20137.\n[11] Omri Avrahami, Ohad Fried, and Dani Lischinski. 2023. Blended latent diffusion. ACM Transactions on Graphics (TOG) 42, 4\n(2023), 1\u201311.\n[12] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. 2019. Conditional GAN with Discrimi-\nnative Filter Generation for Text-to-Video Synthesis.. In IJCAI, Vol. 1. 2.\n[13] Han Bao, Xun Zhou, Yiqun Xie, Yingxue Zhang, and Yanhua Li. 2022. COVID-GAN+: Estimating human mobility responses to\nCOVID-19 through spatio-temporal generative adversarial networks with enhanced features. ACM Transactions on Intelligent\nSystems and Technology (TIST) 13, 2 (2022), 1\u201323.\n[14] Han Bao, Xun Zhou, Yingxue Zhang, Yanhua Li, and Yiqun Xie. 2020. Covid-gan: Estimating human mobility responses\nto covid-19 pandemic through spatio-temporal conditional generative adversarial networks. In Proceedings of the 28th\ninternational conference on advances in geographic information systems. 273\u2013282.\n[15] Naren Bao, Alexander Carballo, and Takeda Kazuya. 2021. Prediction of personalized driving behaviors via driver-adaptive\ndeep generative models. In 2021 IEEE Intelligent Vehicles Symposium (IV). IEEE, 616\u2013621.\n[16] Sarah Baras, Iman Saeed, Hadeel A Tabaza, and Mourad Elhadef. 2018. VANETs-based intelligent transportation systems: An\noverview. Advances in Computer Science and Ubiquitous Computing: CSA-CUTE 17 (2018), 265\u2013273.\n[17] Elyes Ben Hamida, Hassan Noura, and Wassim Znaidi. 2015. Security of cooperative intelligent transport systems: Standards,\nthreats analysis and cryptographic countermeasures. Electronics 4, 3 (2015), 380\u2013423.\n[18] Kartik Krishna Bhardwaj, Anirudh Khanna, Deepak Kumar Sharma, and Anshuman Chhabra. 2019. Designing energy-efficient\nIoT-based intelligent transport system: need, architecture, characteristics, challenges, and applications. Energy Conservation\nfor IoT Devices: Concepts, Paradigms and Solutions (2019), 209\u2013233.\n[19] Camille Bilodeau, Wengong Jin, Tommi Jaakkola, Regina Barzilay, and Klavs F Jensen. 2022. Generative models for molecular\ndiscovery: Recent advances and challenges. Wiley Interdisciplinary Reviews: Computational Molecular Science 12, 5 (2022),\ne1608.\n[20] Alden Boby, Dane Brown, and James Connan. 2023. Iterative Refinement Versus Generative Adversarial Networks for\nSuper-Resolution Towards Licence Plate Detection. In Inventive Systems and Control: Proceedings of ICISC 2023. Springer,\n349\u2013362.\n[21] Guillem Boquet, Antoni Morell, Javier Serrano, and Jose Lopez Vicario. 2020. A variational autoencoder solution for road traffic\nforecasting systems: Missing data imputation, dimension reduction, model selection and anomaly detection. Transportation\nResearch Part C: Emerging Technologies 115 (2020), 102622.\n[22] Guillem Boquet, Jose Lopez Vicario, Antoni Morell, and Javier Serrano. 2019. Missing data in traffic estimation: A variational\nautoencoder imputation method. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2882\u20132886.\n[23] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information\nprocessing systems 33 (2020), 1877\u20131901.\n[24] Qing Cai, Mohamed Abdel-Aty, Jinghui Yuan, Jaeyoung Lee, and Yina Wu. 2020. Real-time crash prediction on expressways\nusing deep generative models. Transportation research part C: emerging technologies 117 (2020), 102697.\n[25] Weiwei Cai and Zhanguo Wei. 2020. PiiGAN: generative adversarial networks for pluralistic image inpainting. IEEE Access 8\n(2020), 48451\u201348463.\n[26] Fernando Camacho, C\u00e9sar C\u00e1rdenas, and David Mu\u00f1oz. 2018. Emerging technologies and research challenges for intelligent\ntransportation systems: 5G, HetNets, and SDN. International Journal on Interactive Design and Manufacturing (IJIDeM) 12\n(2018), 327\u2013335.\n[27] Yihan Cao, Siyu Li, Yixin Liu, Zhiling Yan, Yutong Dai, Philip S Yu, and Lichao Sun. 2023. A comprehensive survey of\nai-generated content (aigc): A history of generative ai from gan to chatgpt. arXiv preprint arXiv:2303.04226 (2023).\n[28] H Varun Chand and J Karthikeyan. 2018. Survey on the role of IoT in intelligent transportation system. Indonesian Journal of\nElectrical Engineering and Computer Science 11, 3 (2018), 936\u2013941.\n[29] Chuan Chen, Zhenpeng Wu, Yanyi Lai, Wenlin Ou, Tianchi Liao, and Zibin Zheng. 2023. Challenges and Remedies to Privacy\nand Security in AIGC: Exploring the Potential of Privacy Computing, Blockchain, and Beyond. arXiv preprint arXiv:2306.00419\n(2023).\n[30] Haonan Chen, Tianchen Ji, Shuijing Liu, and Katherine Driggs-Campbell. 2022. Combining Model-Based Controllers and\nGenerative Adversarial Imitation Learning for Traffic Simulation. In 2022 IEEE 25th International Conference on Intelligent\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nTransportation Systems (ITSC). IEEE, 1698\u20131704.\n[31] Jiayuan Chen, Shuo Zhang, Xiaofei Chen, Qiao Jiang, Hejiao Huang, and Chonglin Gu. 2021. Learning Traffic as Videos:\nA Spatio-Temporal VAE Approach for Traffic Data Imputation. In International Conference on Artificial Neural Networks.\nSpringer, 615\u2013627.\n[32] Kehua Chen, Xianda Chen, Zihan Yu, Meixin Zhu, and Hai Yang. 2023. EquiDiff: A Conditional Equivariant Diffusion Model\nFor Trajectory Prediction. arXiv preprint arXiv:2308.06564 (2023).\n[33] Mu-Yen Chen, Hsiu-Sen Chiang, and Wei-Kai Huang. 2022. Efficient generative adversarial networks for imbalanced traffic\ncollision datasets. IEEE Transactions on Intelligent Transportation Systems 23, 10 (2022), 19864\u201319873.\n[34] Qi Chen, Wei Wang, Kaizhu Huang, Suparna De, and Frans Coenen. 2021. Multi-modal generative adversarial networks for\ntraffic event detection in smart cities. Expert Systems with Applications 177 (2021), 114939.\n[35] Xinyu Chen, Jiajie Xu, Rui Zhou, Wei Chen, Junhua Fang, and Chengfei Liu. 2021. Trajvae: A variational autoencoder model\nfor trajectory generation. Neurocomputing 428 (2021), 332\u2013339.\n[36] Yuanyuan Chen, Yisheng Lv, and Fei-Yue Wang. 2019. Traffic flow imputation using parallel data and generative adversarial\nnetworks. IEEE Transactions on Intelligent Transportation Systems 21, 4 (2019), 1624\u20131630.\n[37] Yuanyuan Chen, Yisheng Lv, and Fenghua Zhu. 2021. Traffic Flow Synthesis Using Generative Adversarial Networks via\nSemantic Latent Codes Manipulation. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE,\n1451\u20131456.\n[38] Zhijun Chen, Jingming Zhang, Yishi Zhang, and Zihao Huang. 2021. Traffic accident data generation based on improved\ngenerative adversarial networks. Sensors 21, 17 (2021), 5767.\n[39] Hao Cheng, Li Feng, Hailong Liu, Takatsugu Hirayama, Hiroshi Murase, and Monika Sester. 2021. Interaction Detection\nBetween Vehicles and Vulnerable Road Users: A Deep Generative Approach with Attention. arXiv preprint arXiv:2105.03891\n(2021).\n[40] Hao Cheng, WLMY Yang, M Sester, and B Rosenhahn. 2020. Context conditional variational autoencoder for predicting\nmulti-path trajectories in mixed traffic. arXiv preprint arXiv:2002.05966 (2020).\n[41] Yu Cheng, Yongshun Gong, Yuansheng Liu, Bosheng Song, and Quan Zou. 2021. Molecular design in drug discovery: a\ncomprehensive review of deep generative models. Briefings in bioinformatics 22, 6 (2021), bbab344.\n[42] Rishu Chhabra, Seema Verma, and C Rama Krishna. 2017. A survey on driver behavior detection techniques for intelligent\ntransportation systems. In 2017 7th International Conference on Cloud Computing, Data Science & Engineering-Confluence.\nIEEE, 36\u201341.\n[43] Junsung Choi, Vuk Marojevic, Carl B Dietrich, Jeffrey H Reed, and Seungyoung Ahn. 2020. Survey of spectrum regulation for\nintelligent transportation systems. IEEE Access 8 (2020), 140145\u2013140160.\n[44] Seongjin Choi, Jiwon Kim, and Hwasoo Yeo. 2021. TrajGAIL: Generating urban vehicle trajectories using generative adversarial\nimitation learning. Transportation Research Part C: Emerging Technologies 128 (2021), 103091.\n[45] Christian Cre\u00df, Zhenshan Bing, and Alois C Knoll. 2021. Intelligent transportation systems using external infrastructure: A\nliterature survey. arXiv preprint arXiv:2112.05615 (2021).\n[46] Qiongjie Cui, Huaijiang Sun, Yue Kong, Xiaoqian Zhang, and Yanmeng Li. 2021. Efficient human motion prediction using\ntemporal convolutional generative adversarial network. Information Sciences 545 (2021), 427\u2013447.\n[47] Issam Damaj, Salwa K Al Khatib, Tarek Naous, Wafic Lawand, Zainab Z Abdelrazzak, and Hussein T Mouftah. 2022. Intelligent\ntransportation systems: A survey on modern hardware devices for the era of machine learning. Journal of King Saud\nUniversity-Computer and Information Sciences 34, 8 (2022), 5921\u20135942.\n[48] Debashis Das, Sourav Banerjee, Pushpita Chatterjee, Uttam Ghosh, and Utpal Biswas. 2023. Blockchain for Intelligent\nTransportation Systems: Applications, Challenges, and Opportunities. IEEE Internet of Things Journal (2023).\n[49] Pierre De Beaucorps, Thomas Streubel, Anne Verroust-Blondet, Fawzi Nashashibi, Benazouz Bradai, and Paulo Resende.\n2017. Decision-making for automated vehicles at intersections adapting human-like behavior. In 2017 IEEE Intelligent Vehicles\nSymposium (IV). IEEE, 212\u2013217.\n[50] Xiaoheng Deng, Leilei Wang, Jinsong Gui, Ping Jiang, Xuechen Chen, Feng Zeng, and Shaohua Wan. 2023. A review of 6G\nautonomous intelligent transportation systems: Mechanisms, applications and challenges. Journal of Systems Architecture\n(2023), 102929.\n[51] Praveen Devadhas Sujakumari and Paulraj Dassan. 2023. Generative Adversarial Networks (GAN) and HDFS-Based Realtime\nTraffic Forecasting System Using CCTV Surveillance. Symmetry 15, 4 (2023), 779.\n[52] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang,\net al. 2021. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems\n34 (2021), 19822\u201319835.\n[53] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. 2022. Cogview2: Faster and better text-to-image generation via\nhierarchical transformers. Advances in Neural Information Processing Systems 35 (2022), 16890\u201316902.\n[54] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. 2019. Trojan attack on deep generative models in\nautonomous driving. In Security and Privacy in Communication Networks: 15th EAI International Conference, SecureComm\n2019, Orlando, FL, USA, October 23-25, 2019, Proceedings, Part I 15. Springer, 299\u2013318.\n[55] Jiqian Dong, Sikai Chen, and Samuel Labi. 2023. Transfusor: Transformer Diffusor for Controllable Human-like Generation of\nVehicle Lane Changing Trajectories. arXiv preprint arXiv:2308.14943 (2023).\n[56] Hongyang Du, Zonghang Li, Dusit Niyato, Jiawen Kang, Zehui Xiong, Dong In Kim, et al. 2023. Enabling AI-generated\ncontent (AIGC) services in wireless edge networks. arXiv preprint arXiv:2301.03220 (2023).\n[57] Yilun Du and Igor Mordatch. 2019. Implicit generation and modeling with energy based models. Advances in Neural\nInformation Processing Systems 32 (2019).\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n[58] Yujun Du, Jinling Wang, Chris Rizos, and Ahmed El-Mowafy. 2021. Vulnerabilities and integrity of precise point positioning\nfor intelligent transport systems: Overview and analysis. Satellite Navigation 2, 1 (2021), 1\u201322.\n[59] Nour-Eddin El Faouzi, Henry Leung, and Ajeesh Kurian. 2011. Data fusion in intelligent transportation systems: Progress and\nchallenges\u2013A survey. Information Fusion 12, 1 (2011), 4\u201310.\n[60] Ayoub Ellahyani, Ilyas El Jaafari, and Said Charfi. 2021. Traffic sign detection for intelligent transportation systems: a survey.\nIn E3S web of conferences, Vol. 229. EDP Sciences, 01006.\n[61] Danish Fayaz. 2018. Intelligent Transport System-A Review. Electronic resource (2018).\n[62] Fenling Feng, Jiaqi Zhang, Chengguang Liu, Wan Li, and Qiwei Jiang. 2021. Short-term railway passenger demand forecast\nusing improved Wasserstein generative adversarial nets and web search terms. IET Intelligent Transport Systems 15, 3 (2021),\n432\u2013445.\n[63] Ruyi Feng, Zhibin Li, Bowen Liu, Yan Ding, and Ou Zheng. 2023. TrTr: A Versatile Pre-Trained Large Traffic Model based on\nTransformer for Capturing Trajectory Diversity in Vehicle Population. arXiv preprint arXiv:2309.12677 (2023).\n[64] Tao Feng, Huan Yan, Huandong Wang, Wenzhen Huang, Yuyang Han, Hongsen Liao, Jinghua Hao, and Yong Li. 2023. ILRoute:\nA Graph-based Imitation Learning Method to Unveil Riders\u2019 Routing Strategies in Food Delivery Service. In Proceedings of the\n29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 4024\u20134034.\n[65] Lino Figueiredo, Isabel Jesus, JA Tenreiro Machado, Jose Rui Ferreira, and JL Martins De Carvalho. 2001. Towards the\ndevelopment of intelligent transportation systems. ITSC 2001. 2001 IEEE intelligent transportation systems. Proceedings (Cat.\nNo. 01TH8585) (2001), 1206\u20131211.\n[66] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30\n(2020), 681\u2013694.\n[67] Lin Geng Foo, Hossein Rahmani, and Jun Liu. 2023. AIGC for Various Data Modalities: A Survey. arXiv preprint arXiv:2308.14177\n(2023).\n[68] Marcia R Friesen and Robert D McLeod. 2015. Bluetooth in intelligent transportation systems: a survey. International Journal\nof Intelligent Transportation Systems Research 13 (2015), 143\u2013153.\n[69] Tanya Garg and Gurjinder Kaur. 2023. A systematic review on intelligent transport systems. Journal of Computational and\nCognitive Engineering 2, 3 (2023), 175\u2013188.\n[70] Tukaram K Gawali and Shailesh S Deore. 2023. Dual-discriminator conditional Giza pyramids construction generative\nadversarial network based traffic density recognition using road vehicle images. International Journal of Machine Learning\nand Cybernetics (2023), 1\u201318.\n[71] Benyamin Ghojogh and Ali Ghodsi. 2020. Attention mechanism, transformers, BERT, and GPT: tutorial and survey. (2020).\n[72] Ashkan Gholamhosseinian and Jochen Seitz. 2021. Vehicle classification in intelligent transport systems: An overview,\nmethods and software perspective. IEEE Open Journal of Intelligent Transportation Systems 2 (2021), 173\u2013194.\n[73] Arna Ghosh, Biswarup Bhattacharya, and Somnath Basu Roy Chowdhury. 2016. Sad-gan: Synthetic autonomous driving\nusing generative adversarial networks. arXiv preprint arXiv:1611.08788 (2016).\n[74] Raksha Ghosh, R Pragathi, S Ullas, and Surekha Borra. 2017. Intelligent transportation systems: A survey. In 2017 International\nConference on Circuits, Controls, and Communications (CCUBE). IEEE, 160\u2013165.\n[75] Taiyuan Gong, Li Zhu, F Richard Yu, and Tao Tang. 2023. Edge Intelligence in Intelligent Transportation Systems: A Survey.\nIEEE Transactions on Intelligent Transportation Systems (2023).\n[76] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\nBengio. 2014. Generative adversarial nets. Advances in neural information processing systems 27 (2014).\n[77] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\nBengio. 2020. Generative adversarial networks. Commun. ACM 63, 11 (2020), 139\u2013144.\n[78] Ning Gui, Tianchu Zeng, Jianming Hu, and Yi Zhang. 2022. Visual-Angle Attention Predictor: A Multi-Agent Trajectory\nPredictor Based on Variational Auto-Encoder. In CICTP 2022. 866\u2013877.\n[79] Sonam Gupta, Arti Keshari, and Sukhendu Das. 2021. G3an++ exploring wide gans with complementary feature learning for\nvideo generation. In Proceedings of the Twelfth Indian Conference on Computer Vision, Graphics and Image Processing. 1\u20139.\n[80] Arya Ketabchi Haghighat, Varsha Ravichandra-Mouli, Pranamesh Chakraborty, Yasaman Esfandiari, Saeed Arabi, and\nAnuj Sharma. 2020. Applications of deep learning in intelligent transportation systems. Journal of Big Data Analytics in\nTransportation 2 (2020), 115\u2013145.\n[81] Zahid Halim, Rizwana Kalsoom, and Abdul Rauf Baig. 2016. Profiling drivers based on driver dependent vehicle driving\nfeatures. Applied Intelligence 44 (2016), 645\u2013664.\n[82] Byung-Gil Han, Jong Taek Lee, Kil-Taek Lim, and Doo-Hyun Choi. 2020. License plate image generation using generative\nadversarial networks for end-to-end license plate character recognition from a small set of real images. Applied Sciences 10, 8\n(2020), 2780.\n[83] Lingyi Han, Kan Zheng, Long Zhao, Xianbin Wang, and Huimin Wen. 2020. Content-aware traffic data completion in ITS\nbased on generative adversarial nets. IEEE Transactions on Vehicular Technology 69, 10 (2020), 11950\u201311962.\n[84] Peng Hang, Chen Lv, Yang Xing, Chao Huang, and Zhongxu Hu. 2020. Human-like decision making for autonomous driving:\nA noncooperative game theoretic approach. IEEE Transactions on Intelligent Transportation Systems 22, 4 (2020), 2076\u20132087.\n[85] LIN Hao, LI Leixiao, and WANG Hui. 2020. Survey on research and application of support vector machines in intelligent\ntransportation system. Journal of Frontiers of Computer Science & Technology 14, 6 (2020), 901.\n[86] Julie Harvey and Sathish Kumar. 2020. A survey of intelligent transportation systems security: challenges and solutions. In\n2020 IEEE 6th Intl Conference on Big Data Security on Cloud (BigDataSecurity), IEEE Intl Conference on High Performance and\nSmart Computing,(HPSC) and IEEE Intl Conference on Intelligent Data and Security (IDS). IEEE, 263\u2013268.\n[87] Muhammad Abul Hassan, Roofia Javed, Fabrizio Granelli, Xin Gen, Muhammad Rizwan, Syed Haider Ali, Hazrat Junaid, Sana\nUllah, et al. 2023. Intelligent transportation systems in smart city: a systematic survey. In 2023 International Conference on\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nRobotics and Automation in Industry (ICRAI). IEEE, 1\u20139.\n[88] Md Saif Hassan Onim, Hussain Nyeem, Md Wahiduzzaman Khan Arnob, and Arunima Dey Pooja. 2023. Unleashing the\npower of generative adversarial networks: A novel machine learning approach for vehicle detection and localisation in the\ndark. Cognitive Computation and Systems (2023).\n[89] Ammar Haydari and Yasin Y\u0131lmaz. 2020. Deep reinforcement learning for intelligent transportation systems: A survey. IEEE\nTransactions on Intelligent Transportation Systems 23, 1 (2020), 11\u201332.\n[90] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. 2022. Imagen video: High definition video generation with diffusion models. arXiv\npreprint arXiv:2210.02303 (2022).\n[91] Tongge Huang, Pranamesh Chakraborty, and Anuj Sharma. 2021. Deep convolutional generative adversarial networks for\ntraffic data imputation encoding time series as images. International Journal of Transportation Science and Technology (2021).\n[92] Yifei Huang and Feng Chen. 2022. Data Interpolation of Traffic Flow Algorithm Using Wavelet Transform for Traffic\nGenerative Modeling. IEEE Journal of Radio Frequency Identification 6 (2022), 739\u2013742.\n[93] Guangyu Huo, Yong Zhang, Boyue Wang, Yongli Hu, and Baocai Yin. 2021. Text-to-traffic generative adversarial network for\ntraffic situation generation. IEEE Transactions on Intelligent Transportation Systems 23, 3 (2021), 2623\u20132636.\n[94] H Ibrahim, Omar M Fahmy, and Mustafa A Elattar. 2022. License plate image analysis empowered by generative adversarial\nneural networks (GANs). IEEE Access 10 (2022), 30846\u201330857.\n[95] Davide Imparato, Ahmed El-Mowafy, Chris Rizos, and Jinling Wang. 2018. Vulnerabilities in SBAS and RTK positioning in\nintelligent transport systems: An overview. In Proceedings of the International Global Navigation Satellite System Association\nIGNSS Symposium.\n[96] Zubayer Islam, Mohamed Abdel-Aty, Qing Cai, and Jinghui Yuan. 2021. Crash data augmentation using variational autoencoder.\nAccident Analysis & Prevention 151 (2021), 105950.\n[97] Boris Ivanovic, Karen Leung, Edward Schmerling, and Marco Pavone. 2020. Multimodal deep generative models for trajectory\nprediction: A conditional variational autoencoder approach. IEEE Robotics and Automation Letters 6, 2 (2020), 295\u2013302.\n[98] DN Jagadish, Arun Chauhan, and Lakshman Mahto. 2021. Autonomous vehicle path prediction using conditional variational\nautoencoder networks. In Pacific-Asia Conference on Knowledge Discovery and Data Mining. Springer, 129\u2013139.\n[99] DN Jagadish, Arun Chauhan, and Lakshman Mahto. 2022. Conditional Variational Autoencoder Networks for Autonomous\nVehicle Path Prediction. Neural Processing Letters 54, 5 (2022), 3965\u20133978.\n[100] Junchen Jin, Dingding Rong, Tong Zhang, Qingyuan Ji, Haifeng Guo, Yisheng Lv, Xiaoliang Ma, and Fei-Yue Wang. 2022. A\nGAN-based short-term link traffic prediction approach for urban road networks under a parallel learning framework. IEEE\nTransactions on Intelligent Transportation Systems 23, 9 (2022), 16185\u201316196.\n[101] Xue-Bo Jin, Wen-Tao Gong, Jian-Lei Kong, Yu-Ting Bai, and Ting-Li Su. 2022. PFVAE: a planar flow-based variational\nauto-encoder prediction model for time series data. Mathematics 10, 4 (2022), 610.\n[102] Ye Jin, Xiaoxi Shen, Huiling Peng, Xiaoan Liu, Jingli Qin, Jiayang Li, Jintao Xie, Peizhong Gao, Guyue Zhou, and Jiangtao\nGong. 2023. SurrealDriver: Designing Generative Driver Agent Simulation Framework in Urban Contexts based on Large\nLanguage Model. arXiv preprint arXiv:2309.13193 (2023).\n[103] Levente Juh\u00e1sz, Peter Mooney, Hartwig H Hochmair, and Boyuan Guan. 2023. ChatGPT as a mapping assistant: A novel\nmethod to enrich maps with generative AI and content derived from street-level photographs. arXiv preprint arXiv:2306.03204\n(2023).\n[104] Li-Wei Kang, Chih-Chung Hsu, I-Shan Wang, Ting-Lei Liu, Shih-Yu Chen, and Chuan-Yu Chang. 2020. Vehicle trajectory\nprediction based on social generative adversarial network for self-driving car applications. In 2020 international symposium\non computer, consumer and control (IS3C). IEEE, 489\u2013492.\n[105] Yan Kang, Jinyuan Li, Shin-Jye Lee, and Hao Li. 2020. Generative adversarial network-based regional epitaxial traffic flow\nprediction. In Advances in Natural Computation, Fuzzy Systems and Knowledge Discovery: Volume 2. Springer, 804\u2013814.\n[106] Zhuangwei Kang, Ayan Mukhopadhyay, Aniruddha Gokhale, Shijie Wen, and Abhishek Dubey. 2022. Traffic Anomaly\nDetection Via Conditional Normalizing Flow. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems\n(ITSC). IEEE, 2563\u20132570.\n[107] Tero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In\nProceedings of the IEEE/CVF conference on computer vision and pattern recognition. 4401\u20134410.\n[108] Amir Kazemi and Hadi Meidani. 2021. IGANI: Iterative generative adversarial networks for imputation with application to\ntraffic data. IEEE Access 9 (2021), 112966\u2013112977.\n[109] Alkilane Khaled, Alfateh M Tag Elsir, and Yanming Shen. 2022. TFGAN: Traffic forecasting using generative adversarial\nnetwork with multi-graph convolutional network. Knowledge-Based Systems 249 (2022), 108990.\n[110] Aidil Redza Khan, Mohd Faizal Jamlos, Nurmadiha Osman, Muhammad Izhar Ishak, Fatimah Dzaharudin, You Kok Yeow, and\nKhairil Anuar Khairi. 2022. DSRC technology in Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) IoT system for\nIntelligent Transportation System (ITS): A review. Recent Trends in Mechatronics Towards Industry 4.0: Selected Articles from\niM3F 2020, Malaysia (2022), 97\u2013106.\n[111] Dongchan Kim, Hyukju Shon, Nahyun Kweon, Seungwon Choi, Chanuk Yang, and Kunsoo Huh. 2021. Driving Style-Based\nConditional Variational Autoencoder for Prediction of Ego Vehicle Trajectory. IEEE Access 9 (2021), 169348\u2013169356.\n[112] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).\n[113] Alireza Koochali, Peter Schichtel, Andreas Dengel, and Sheraz Ahmed. 2019. Probabilistic forecasting of sensory data with\ngenerative adversarial networks\u2013forgan. IEEE Access 7 (2019), 63868\u201363880.\n[114] Robert Krajewski, Tobias Moers, Adrian Meister, and Lutz Eckstein. 2019. B\u00e9zierVAE: Improved trajectory modeling using\nvariational autoencoders for the safety validation of highly automated vehicles. In 2019 IEEE Intelligent Transportation Systems\nConference (ITSC). IEEE, 3788\u20133795.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n[115] Robert Krajewski, Tobias Moers, Dominik Nerger, and Lutz Eckstein. 2018. Data-driven maneuver modeling using generative\nadversarial networks and variational autoencoders for safety validation of highly automated vehicles. In 2018 21st International\nConference on Intelligent Transportation Systems (ITSC). IEEE, 2383\u20132390.\n[116] Mahesh Venkata Krishna and Joachim Denzler. 2014. A combination of generative and discriminative models for fast\nunsupervised activity recognition from traffic scene videos. In IEEE Winter Conference on Applications of Computer Vision.\nIEEE, 640\u2013645.\n[117] PM Ashok Kumar, D Kavitha, and S Arun Kumar. 2020. A hybrid generative-discriminative model for abnormal event\ndetection in surveillance video scenes. International Journal of Information and Computer Security 12, 2-3 (2020), 253\u2013268.\n[118] R Sampath Kumar, VP Krishnamurthy, Venkateswararao Podile, G Yamini Priyanka, and Vemulapalli Neha. 2023. Generative\nAdversarial Networks to Improve the Nature of Training in Autonomous Vehicles. In 2023 International Conference on\nDisruptive Technologies (ICDT). IEEE, 161\u2013164.\n[119] Ayyoub Lamssaggad, Nabil Benamar, Abdelhakim Senhaji Hafid, and Mounira Msahli. 2021. A survey on the current security\nlandscape of intelligent transportation systems. IEEE Access 9 (2021), 9180\u20139208.\n[120] Yann LeCun, Sumit Chopra, Raia Hadsell, M Ranzato, and Fujie Huang. 2006. A tutorial on energy-based learning. Predicting\nstructured data 1, 0 (2006).\n[121] Christian Ledig, Lucas Theis, Ferenc Husz\u00e1r, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan\nTejani, Johannes Totz, Zehan Wang, et al. 2017. Photo-realistic single image super-resolution using a generative adversarial\nnetwork. In Proceedings of the IEEE conference on computer vision and pattern recognition. 4681\u20134690.\n[122] Younkwan Lee, JaeWoong Yun, Yoojin Hong, Juhyun Lee, and Moongu Jeon. 2018. Accurate license plate recognition and\nsuper-resolution using a generative adversarial networks on traffic surveillance video. In 2018 IEEE International Conference\non Consumer Electronics-Asia (ICCE-Asia). IEEE, 1\u20134.\n[123] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. 2019. Controllable text-to-image generation. Advances in\nNeural Information Processing Systems 32 (2019).\n[124] Chenghao Li, Chaoning Zhang, Atish Waghwase, Lik-Hang Lee, Francois Rameau, Yang Yang, Sung-Ho Bae, and Choong Seon\nHong. 2023. Generative AI meets 3D: A Survey on Text-to-3D in AIGC Era. arXiv preprint arXiv:2305.06131 (2023).\n[125] Gang Li, Heliang Zheng, Chaoyue Wang, Chang Li, Changwen Zheng, and Dacheng Tao. 2022. 3ddesigner: Towards\nphotorealistic 3d object generation and editing with text-guided diffusion models. arXiv preprint arXiv:2211.14108 (2022).\n[126] Haitao Li, Qian Cao, Qiaowen Bai, Zhihui Li, and Hongyu Hu. 2023. Multistate time series imputation using generative\nadversarial network with applications to traffic data. Neural Computing and Applications 35, 9 (2023), 6545\u20136567.\n[127] Jinyuan Li, Hao Li, Guorong Cui, Yan Kang, Yang Hu, and Yingnan Zhou. 2020. GACNet: A Generative Adversarial Capsule\nNetwork for Regional Epitaxial Traffic Flow Prediction. Computers, Materials & Continua 64, 2 (2020).\n[128] Jinlong Li, Ruonan Li, Zilin Huang, Pan Wu, and Lunhui Xu. 2023. Dynamic adaptive generative adversarial networks with\nmulti-view temporal factorizations for hybrid recovery of missing traffic data. Neural Computing and Applications 35, 10\n(2023), 7677\u20137696.\n[129] Jinning Li, Liting Sun, Jianyu Chen, Masayoshi Tomizuka, and Wei Zhan. 2021. A safe hierarchical planning framework for\ncomplex driving scenarios based on reinforcement learning. In 2021 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 2660\u20132666.\n[130] Jianxue Li, Yang Xiao, Jiawei Wu, Yaozhi Chen, and Jun Liu. 2022. Attentive Dual-Head Spatial-Temporal Generative\nAdversarial Networks for Crowd Flow Generation. In 2022 IEEE 33rd Annual International Symposium on Personal, Indoor and\nMobile Radio Communications (PIMRC). IEEE, 800\u2013806.\n[131] Lincan Li, Jichao Bi, Kaixiang Yang, and Fengji Luo. 2022. Spatial-Temporal Semantic Generative Adversarial Networks for\nFlexible Multi-step Urban Flow Prediction. In International Conference on Artificial Neural Networks. Springer, 763\u2013775.\n[132] Lincan Li, Jichao Bi, Kaixiang Yang, Fengji Luo, and Luxing Yang. 2022. MGC-GAN: Multi-Graph Convolutional Generative\nAdversarial Networks for Accurate Citywide Traffic Flow Prediction. In 2022 IEEE International Conference on Systems, Man,\nand Cybernetics (SMC). IEEE, 2557\u20132562.\n[133] Tao Li, Xu Han, Jiaqi Ma, Marilia Ramos, and Changju Lee. 2023. Operational safety of automated and human driving in\nmixed traffic environments: A perspective of car-following behavior. Proceedings of the Institution of Mechanical Engineers,\nPart O: Journal of Risk and Reliability 237, 2 (2023), 355\u2013366.\n[134] Tianyi Li, Mingfeng Shang, Shian Wang, Matthew Filippelli, and Raphael Stern. 2022. Detecting stealthy cyberattacks on\nautomated vehicles via generative adversarial networks. In 2022 IEEE 25th International Conference on Intelligent Transportation\nSystems (ITSC). IEEE, 3632\u20133637.\n[135] Xiao Li, Guy Rosman, Igor Gilitschenski, Cristian-Ioan Vasile, Jonathan A DeCastro, Sertac Karaman, and Daniela Rus. 2021.\nVehicle trajectory prediction using generative adversarial network with temporal logic syntax tree features. IEEE Robotics\nand Automation Letters 6, 2 (2021), 3459\u20133466.\n[136] X Li, X Yin, and Oscar LI C. 2020. Object-Semantics Aligned Pre-training for Vision-Language Tasks [C]. In European\nConference on Computer Vision. Springer, Cham. 121\u2013137.\n[137] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. 2018. Video generation from text. In Proceedings\nof the AAAI conference on artificial intelligence, Vol. 32.\n[138] Yaochen Li, Xiao Wu, Danhui Lu, Ling Li, Yuehu Liu, and Li Zhu. 2020. Style transfer of urban road images using generative\nadversarial networks with structural details. IEEE MultiMedia 27, 3 (2020), 54\u201365.\n[139] Zhimin Li, Haifeng Zheng, and Xinxin Feng. 2018. 3D convolutional generative adversarial networks for missing traffic data\ncompletion. In 2018 10th International Conference on Wireless Communications and Signal Processing (WCSP). IEEE, 1\u20136.\n[140] Yunyi Liang, Zhiyong Cui, Yu Tian, Huimiao Chen, and Yinhai Wang. 2018. A deep generative adversarial architecture for\nnetwork-wide spatial-temporal traffic-state estimation. Transportation Research Record 2672, 45 (2018), 87\u2013105.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\n[141] Zeguang Liao, Han Cheng, Xuan Wang, Xin Tao, Yihuan Zhang, Yifan Dai, and Keqiang Li. 2022. ITGAN: An Interactive\nTrajectories Generative Adversarial Network Model for Automated Driving Scenario Generation. In Society of Automotive\nEngineers (SAE)-China Congress. Springer, 554\u2013566.\n[142] S Lima, S Barbosa, P Palmeira, L Matos, I Secundo, and R Nascimento. 2017. Systematic review: Techniques and methods of\nurban monitoring in intelligent transport systems. ICWMC 2017 17 (2017), 9.\n[143] Hongyi Lin, Yang Liu, Shen Li, and Xiaobo Qu. 2023. How generative adversarial networks promote the development of\nintelligent transportation systems: A survey. IEEE/CAA Journal of Automatica Sinica (2023).\n[144] Lin Lin, Jiwon Kim, and Sanghyung Ahn. 2021. Car Following Modelling with Constrained Generative Adversarial Imitation\nLearning. (2021).\n[145] Yilun Lin, Xingyuan Dai, Li Li, and Fei-Yue Wang. 2018. Pattern sensitive prediction of traffic flow based on generative\nadversarial framework. IEEE Transactions on Intelligent Transportation Systems 20, 6 (2018), 2395\u20132400.\n[146] Yi Lin, Linchao Li, Hailong Jing, Bin Ran, and Dongye Sun. 2020. Automated traffic incident detection with a smaller dataset\nbased on generative adversarial networks. Accident Analysis & Prevention 144 (2020), 105628.\n[147] Yongjie Lin, Qihang Li, Duanya Lyu, and Xiaofei Wang. 2022. A Review of Wi-Fi-Based Traffic Detection Technology in the\nField of Intelligent Transportation Systems. Buildings 12, 4 (2022), 428.\n[148] Yan Lin, Huaiyu Wan, Jilin Hu, Shengnan Guo, Bin Yang, Youfang Lin, and Christian S Jensen. 2023. Origin-Destination\nTravel Time Oracle for Map-based Services. arXiv preprint arXiv:2307.03048 (2023).\n[149] Jiaqi Liu, Peng Hang, Jianqiang Wang, Jian Sun, et al. 2023. MTD-GPT: A Multi-Task Decision-Making GPT Model for\nAutonomous Driving at Unsignalized Intersections. arXiv preprint arXiv:2307.16118 (2023).\n[150] Kai Liu and Hongbo Zhang. 2022. Attention based spatio-temporal generative adversarial network for sparse traffic forecasting.\nIn International Conference on Cloud Computing, Performance Computing, and Deep Learning (CCPCDL 2022), Vol. 12287. SPIE,\n492\u2013497.\n[151] Mingzhe Liu, Han Huang, Hao Feng, Leilei Sun, Bowen Du, and Yanjie Fu. 2023. PriSTI: A Conditional Diffusion Framework\nfor Spatiotemporal Imputation. arXiv preprint arXiv:2302.09746 (2023).\n[152] Yue Liu, Xin Wang, Yitian Yuan, and Wenwu Zhu. 2019. Cross-modal dual learning for sentence-to-video generation. In\nProceedings of the 27th ACM international conference on multimedia. 1239\u20131247.\n[153] Ziming Liu, Di Luo, Yilun Xu, Tommi Jaakkola, and Max Tegmark. 2023. GenPhys: From Physical Processes to Generative\nModels. arXiv preprint arXiv:2304.02637 (2023).\n[154] Shuanghu Luo, Ling Yu, Zhongqin Bi, and Yongbin Li. 2020. Traffic sign detection and recognition for intelligent transportation\nsystems: A survey. Journal of Internet Technology 21, 6 (2020), 1773\u20131784.\n[155] Yisheng Lv. 2023. Artificial Intelligence-Generated Content in Intelligent Transportation Systems: Learning to Copy, Change,\nand Create![Editor\u2019s Column]. IEEE Intelligent Transportation Systems Magazine 15, 5 (2023), 2\u20133.\n[156] Lijing Ma, Shiru Qu, Lijun Song, Zhiteng Zhang, and Jie Ren. 2023. A Physics-Informed Generative Car-Following Model for\nConnected Autonomous Vehicles. Entropy 25, 7 (2023), 1050.\n[157] Shuo Ma, Ouri Wolfson, and Jie Lin. 2011. A survey on trust management for intelligent transportation system. In Proceedings\nof the 4th ACM SIGSPATIAL International Workshop on Computational Transportation Science. 18\u201323.\n[158] Athanasios Maimaris and George Papageorgiou. 2016. A review of intelligent transportation systems from a communications\ntechnology perspective. In 2016 IEEE 19th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 54\u201359.\n[159] Cheuk Ki Man, Mohammed Quddus, Athanasios Theofilatos, Rongjie Yu, and Marianna Imprialou. 2022. Wasserstein\ngenerative adversarial network to address the imbalanced data problem in real-time crash risk prediction. IEEE Transactions\non Intelligent Transportation Systems 23, 12 (2022), 23002\u201323013.\n[160] Ratna Mandal, Ankita Mandal, Soumi Dutta, Munshi Yusuf Alam, Sujoy Saha, and Subrata Nandi. 2022. Framework of\nintelligent transportation system: A survey. In Proceedings of International Conference on Frontiers in Computing and Systems:\nCOMSYS 2021. Springer, 93\u2013108.\n[161] Riccardo Mangiaracina, Alessandro Perego, Giulio Salvadori, and Angela Tumino. 2017. A comprehensive view of intelligent\ntransport systems for urban smart mobility. International Journal of Logistics Research and Applications 20, 1 (2017), 39\u201352.\n[162] Renata Maria Mar\u00e8, Claudio Luiz Marte, and Carlos Eduardo Cugnasca. 2016. Visible light communication applied to intelligent\ntransport systems: an overview. IEEE Latin America Transactions 14, 7 (2016), 3199\u20133207.\n[163] Elezabeth Mathew. 2020. Swarm intelligence for intelligent transport systems: opportunities and challenges. Swarm Intelligence\nfor Resource Management in Internet of Things (2020), 131\u2013145.\n[164] Matthew McNaughton, Chris Urmson, John M Dolan, and Jin-Woo Lee. 2011. Motion planning for autonomous driving with\na conformal spatiotemporal lattice. In 2011 IEEE International Conference on Robotics and Automation. IEEE, 4889\u20134895.\n[165] Gaurav Mittal, Tanya Marwah, and Vineeth N Balasubramanian. 2017. Sync-draw: Automatic video generation using deep\nrecurrent attentive architectures. In Proceedings of the 25th ACM international conference on Multimedia. 1096\u20131104.\n[166] Zhaobin Mo, Yongjie Fu, and Xuan Di. 2022. Quantifying uncertainty in traffic state estimation using generative adversarial\nnetworks. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2769\u20132774.\n[167] Aashiq Muhamed, Liang Li, Xingjian Shi, Suri Yaddanapudi, Wayne Chi, Dylan Jackson, Rahul Suresh, Zachary C Lipton, and\nAlex J Smola. 2021. Symbolic music generation with transformer-gans. In Proceedings of the AAAI conference on artificial\nintelligence, Vol. 35. 408\u2013417.\n[168] Rola Naja. 2013. A survey of communications for intelligent transportation systems. In Wireless vehicular networks for car\ncollision avoidance. Springer, 3\u201335.\n[169] Hasan AH Naji, Qingji Xue, Huijun Zhu, and Tianfeng Li. 2021. Forecasting Taxi Demands Using Generative Adversarial\nNetworks with Multi-Source Data. Applied Sciences 11, 20 (2021), 9675.\n[170] Mahima Nama, Ankita Nath, Nancy Bechra, Jitendra Bhatia, Sudeep Tanwar, Manish Chaturvedi, and Balqies Sadoun. 2021.\nMachine learning-based traffic scheduling techniques for intelligent transportation system: Opportunities and challenges.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\nInternational Journal of Communication Systems 34, 9 (2021), e4814.\n[171] Robayet Nasim and Andreas Kassler. 2012. Distributed architectures for intelligent transport systems: A survey. In 2012\nSecond Symposium on Network Cloud Computing and Applications. IEEE, 130\u2013136.\n[172] Marion Neumeier, Michael Botsch, Andreas Tollk\u00fchn, and Thomas Berberich. 2021. Variational autoencoder-based vehicle\ntrajectory prediction with an interpretable latent space. In 2021 IEEE International Intelligent Transportation Systems Conference\n(ITSC). IEEE, 820\u2013827.\n[173] Minh-Thuan Nguyen, Phuong-Thai Nguyen, Van-Vinh Nguyen, and Quang-Minh Nguyen. 2021. Generating Product\nDescription with Generative Pre-trained Transformer 2. In 2021 6th International Conference on Innovative Technology in\nIntelligent System and Industrial Applications (CITISIA). IEEE, 1\u20137.\n[174] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark\nChen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741 (2021).\n[175] Judith Nkechinyere Njoku, Cosmas Ifeanyi Nwakanma, Gabriel Chukwunonso Amaizu, and Dong-Seong Kim. 2023. Prospects\nand challenges of Metaverse application in data-driven intelligent transportation systems. IET Intelligent Transport Systems\n17, 1 (2023), 1\u201321.\n[176] Geunseob Oh and Huei Peng. 2022. Cvae-h: Conditionalizing variational autoencoders via hypernetworks and trajectory\nforecasting for autonomous driving. arXiv preprint arXiv:2201.09874 (2022).\n[177] OpenAI. 2023. Gpt-4 technical report. https://cdn.openai.com/papers/gpt-4.pdf (2023).\n[178] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal,\nKatarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in\nNeural Information Processing Systems 35 (2022), 27730\u201327744.\n[179] Anil Ozturk, Mustafa Burak Gunel, Melih Dal, Ugur Yavas, and Nazim Kemal Ure. 2020. Development of a stochastic traffic\nenvironment with generative time-series models for improving generalization capabilities of autonomous driving agents. In\n2020 IEEE Intelligent Vehicles Symposium (IV). IEEE, 1343\u20131348.\n[180] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei. 2017. To create what you tell: Generating videos from\ncaptions. In Proceedings of the 25th ACM international conference on Multimedia. 1789\u20131798.\n[181] Palak Patel, Zunnun Narmawala, and Ankit Thakkar. 2019. A survey on intelligent transportation system using internet\nof things. Emerging Research in Computing, Information, Communication and Applications: ERCICA 2018, Volume 1 (2019),\n231\u2013240.\n[182] Prashant W Patil, Akshay Dudhane, and Subrahmanyam Murala. 2020. End-to-end recurrent generative adversarial network\nfor traffic and surveillance applications. IEEE Transactions on Vehicular Technology 69, 12 (2020), 14550\u201314562.\n[183] Wenchuang Peng, Youfang Lin, Shengnan Guo, Weiwen Tang, Le Liu, and Huaiyu Wan. 2023. Generative-Contrastive-\nAttentive Spatial-Temporal Network for Traffic Data Imputation. In Pacific-Asia Conference on Knowledge Discovery and Data\nMining. Springer, 45\u201356.\n[184] Tsarina Dwi Putri et al. 2021. Intelligent transportation systems (ITS): A systematic review using a Natural Language\nProcessing (NLP) approach. Heliyon 7, 12 (2021).\n[185] Hua Xuan Qin and Pan Hui. 2023. Empowering the Metaverse with Generative AI: Survey and Future Directions. (2023).\n[186] Yuning Qiu, Teruhisa Misu, and Carlos Busso. 2019. Driving anomaly detection with conditional generative adversarial\nnetwork using physiological and can-bus data. In 2019 International Conference on Multimodal Interaction. 164\u2013173.\n[187] Kashif Naseer Qureshi and Abdul Hanan Abdullah. 2013. A survey on intelligent transportation systems. Middle-East Journal\nof Scientific Research 15, 5 (2013), 629\u2013642.\n[188] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International\nconference on machine learning. PMLR, 8748\u20138763.\n[189] Tanzila Rahman, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, Shweta Mahajan, and Leonid Sigal. 2023. Make-a-story: Visual\nmemory conditioned consistent story generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. 2493\u20132502.\n[190] Saeed Rahmani, Asiye Baghbani, Nizar Bouguila, and Zachary Patterson. 2023. Graph Neural Networks for Intelligent\nTransportation Systems: A Survey. IEEE Transactions on Intelligent Transportation Systems (2023).\n[191] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021.\nZero-shot text-to-image generation. In International Conference on Machine Learning. PMLR, 8821\u20138831.\n[192] Kashif Rasul, Calvin Seward, Ingmar Schuster, and Roland Vollgraf. 2021. Autoregressive denoising diffusion models for\nmultivariate probabilistic time series forecasting. In International Conference on Machine Learning. PMLR, 8857\u20138868.\n[193] Shashi Ravi and Mohan Rao Mamdikar. 2022. A Review on ITS (Intelligent Transportation Systems) Technology. In 2022\nInternational Conference on Applied Artificial Intelligence and Computing (ICAAIC). IEEE, 155\u2013159.\n[194] Davis Rempe, Zhengyi Luo, Xue Bin Peng, Ye Yuan, Kris Kitani, Karsten Kreis, Sanja Fidler, and Or Litany. 2023. Trace\nand Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. 13756\u201313766.\n[195] Danilo Rezende and Shakir Mohamed. 2015. Variational inference with normalizing flows. In International conference on\nmachine learning. PMLR, 1530\u20131538.\n[196] Benjamin B Rhoades and James M Conrad. 2017. A survey of alternate methods and implementations of an intelligent\ntransportation system. In SoutheastCon 2017. IEEE, 1\u20138.\n[197] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 10684\u201310695.\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\n[198] Can Rong, Jingtao Ding, Zhicheng Liu, and Yong Li. 2023. Complexity-aware Large Scale Origin-Destination Network\nGeneration via Diffusion Model. arXiv preprint arXiv:2306.04873 (2023).\n[199] Can Rong, Huandong Wang, and Yong Li. 2023. Origin-Destination Network Generation via Gravity-Guided GAN. arXiv\npreprint arXiv:2306.03390 (2023).\n[200] Debaditya Roy, Tetsuhiro Ishizaka, C Krishna Mohan, and Atsushi Fukuda. 2019. Vehicle trajectory prediction at intersections\nusing interaction based generative adversarial networks. In 2019 IEEE Intelligent transportation systems conference (ITSC).\nIEEE, 2318\u20132323.\n[201] Sandeep Saharan, Seema Bawa, and Neeraj Kumar. 2020. Dynamic pricing techniques for Intelligent Transportation System\nin smart cities: A systematic review. Computer Communications 150 (2020), 603\u2013625.\n[202] Kelathodi Kumaran Santhosh, Debi Prosad Dogra, Partha Pratim Roy, and Adway Mitra. 2021. Vehicular trajectory classification\nand traffic anomaly detection in videos using a hybrid CNN-VAE Architecture. IEEE Transactions on Intelligent Transportation\nSystems 23, 8 (2021), 11891\u201311902.\n[203] Sergio Saponara, Maria Sabrina Greco, and Fulvio Gini. 2019. Radar-on-chip/in-package in autonomous driving vehicles and\nintelligent transport systems: Opportunities and challenges. IEEE Signal Processing Magazine 36, 5 (2019), 71\u201384.\n[204] Divya Saxena and Jiannong Cao. 2019. D-GAN: Deep generative adversarial nets for spatio-temporal prediction. arXiv\npreprint arXiv:1907.08556 (2019).\n[205] Khaled Shaaban, Md Hosne Mobarok Shamim, and Khadija Abdur-Rouf. 2021. Visible light communication for intelligent\ntransportation systems: A review of the latest technologies. Journal of traffic and transportation engineering (English edition)\n8, 4 (2021), 483\u2013492.\n[206] Shivani Sharma and Sateesh Kumar Awasthi. 2022. Introduction to intelligent transportation system: overview, classification\nbased on physical architecture, and challenges. International Journal of Sensor Networks 38, 4 (2022), 215\u2013240.\n[207] Guojiang Shen, Nali Liu, Yinghui Liu, Wenfeng Zhou, and Xiangjie Kong. 2022. Traffic Flow Imputation Based on Multi-\nPerspective Spatiotemporal Generative Adversarial Networks. In Proceedings of CECNet 2022. IOS Press, 62\u201373.\n[208] Yi-Jen Shih, Shih-Lun Wu, Frank Zalkow, Meinard Muller, and Yi-Hsuan Yang. 2022. Theme transformer: Symbolic music\ngeneration with theme-conditioned transformer. IEEE Transactions on Multimedia (2022).\n[209] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran\nGafni, et al. 2022. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792 (2022).\n[210] Arun Kumar Singh, Jatan Shrestha, and Nicola Albarella. 2023. Bi-level optimization augmented with conditional variational\nautoencoder for autonomous driving in dense traffic. In 2023 IEEE 19th International Conference on Automation Science and\nEngineering (CASE). IEEE, 1\u20138.\n[211] Bhupendra Singh and Ankit Gupta. 2015. Recent trends in intelligent transportation systems: a review. Journal of transport\nliterature 9 (2015), 30\u201334.\n[212] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using\nnonequilibrium thermodynamics. In International conference on machine learning. PMLR, 2256\u20132265.\n[213] Xiaozhuang Song, Chenhan Zhang, and JQ James. 2021. Learn travel time distribution with graph deep learning and generative\nadversarial network. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE, 1385\u20131390.\n[214] Zhaoxin Su, Gang Huang, Sanyuan Zhang, and Wei Hua. 2022. Crossmodal transformer based generative framework for\npedestrian trajectory prediction. In 2022 International Conference on Robotics and Automation (ICRA). IEEE, 2337\u20132343.\n[215] S Suhas, V Vismaya Kalyan, Manoj Katti, BV Ajay Prakash, and C Naveena. 2017. A comprehensive review on traffic prediction\nfor intelligent transport system. In 2017 International Conference on Recent Advances in Electronics and Communication\nTechnology (ICRAECT). IEEE, 138\u2013143.\n[216] Shuo Sun, Zekai Gu, Tianchen Sun, Jiawei Sun, Chengran Yuan, Yuhang Han, Dongen Li, and Marcelo H Ang Jr. 2023.\nDriveSceneGen: Generating Diverse and Realistic Driving Scenarios from Scratch. arXiv preprint arXiv:2309.14685 (2023).\n[217] Yasheng Sun, Tao He, Jie Hu, Haiqing Huang, and Biao Chen. 2019. Intent-Aware Conditional Generative Adversarial Network\nfor Pedestrian Path Prediction. In 2019 IEEE International Conference on Artificial Intelligence and Computer Applications\n(ICAICA). IEEE, 155\u2013160.\n[218] Nikunja K Swain. 2006. A survey of application of fuzzy logic in intelligent transportation systems (ITS) and rural ITS. In\nProceedings of the IEEE SoutheastCon 2006. IEEE, 85\u201390.\n[219] Jing Tian, Xianmin Song, Pengfei Tao, and Jiahui Liang. 2022. Pattern-adaptive generative adversarial network with sparse\ndata for traffic state estimation. Physica A: Statistical Mechanics and its Applications 608 (2022), 128254.\n[220] Cuong NN Tran, Thang Tran Huynh Tat, Vivian WY Tam, and Duc Hoc Tran. 2023. Factors affecting intelligent transport\nsystems towards a smart city: A critical review. International Journal of Construction Management 23, 12 (2023), 1982\u20131998.\n[221] Hardik Trivedi, Sudeep Tanwar, and Priyank Thakkar. 2019. Software defined network-based vehicular adhoc networks for\nintelligent transportation system: recent advances and future challenges. In Futuristic Trends in Network and Communication\nTechnologies: First International Conference, FTNCT 2018, Solan, India, February 9\u201310, 2018, Revised Selected Papers 1. Springer,\n325\u2013337.\n[222] Jingzhi Tu, Gang Mei, and Francesco Piccialli. 2021. Incomplete vehicle information completion using generative adversarial\nnetwork to enhance the safety of autonomous driving. In Second International Conference on Industrial IoT, Big Data, and\nSupply Chain, Vol. 12128. SPIE, 61\u201366.\n[223] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\n2017. Attention is all you need. Advances in neural information processing systems 30 (2017).\n[224] Matthew Veres and Medhat Moussa. 2019. Deep learning for intelligent transportation systems: A survey of emerging trends.\nIEEE Transactions on Intelligent transportation systems 21, 8 (2019), 3152\u20133168.\n[225] Chalavadi Vishnu, Vineel Abhinav, Debaditya Roy, C Krishna Mohan, and Ch Sobhan Babu. 2023. Improving Multi-Agent\nTrajectory Prediction Using Traffic States on Interactive Driving Scenarios. IEEE Robotics and Automation Letters 8, 5 (2023),\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n2708\u20132715.\n[226] Meng Wang, Serge P Hoogendoorn, Winnie Daamen, Bart van Arem, and Riender Happee. 2015. Game theoretic approach for\npredictive lane-changing and car-following control. Transportation Research Part C: Emerging Technologies 58 (2015), 73\u201392.\n[227] Tong Wang, Xiaodan Wang, Ziping Cui, Yue Cao, and Chakkaphong Suthaputchakun. 2019. Survey on cooperatively V2X\ndownloading for intelligent transport systems. IET Intelligent Transport Systems 13, 1 (2019), 13\u201321.\n[228] Tao Wang, Yushu Zhang, Shuren Qi, Ruoyu Zhao, Zhihua Xia, and Jian Weng. 2023. Security and privacy on generative data\nin aigc: A survey. arXiv preprint arXiv:2309.09435 (2023).\n[229] Xu Wang, Xiaoming Chen, and Yanping Wang. 2021. Small vehicle classification in the wild using generative adversarial\nnetwork. Neural Computing and Applications 33 (2021), 5369\u20135379.\n[230] Xi Wang, Tianpeng Xin, Hongwei Wang, Li Zhu, and Dongliang Cui. 2022. A generative adversarial network based learning\napproach to the autonomous decision making of high-speed trains. IEEE Transactions on Vehicular Technology 71, 3 (2022),\n2399\u20132412.\n[231] Yaohui Wang, Piotr Bilinski, Francois Bremond, and Antitza Dantcheva. 2020. G3AN: Disentangling appearance and motion\nfor video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5264\u20135273.\n[232] Yaohui Wang, Francois Bremond, and Antitza Dantcheva. 2021. Inmodegan: Interpretable motion decomposition generative\nadversarial network for video generation. arXiv preprint arXiv:2101.03049 (2021).\n[233] Yuntao Wang, Yanghe Pan, Miao Yan, Zhou Su, and Tom H Luan. 2023. A Survey on ChatGPT: AI-Generated Contents,\nChallenges, and Solutions. arXiv preprint arXiv:2305.18339 (2023).\n[234] Yun-Cheng Wang, Jintang Xue, Chengwei Wei, and C-C Jay Kuo. 2023. An Overview on Generative AI at Scale with\nEdge-Cloud Computing. (2023).\n[235] Junqing Wei, Jarrod M Snider, Tianyu Gu, John M Dolan, and Bakhtiar Litkouhi. 2014. A behavioral planning framework for\nautonomous driving. In 2014 IEEE Intelligent Vehicles Symposium Proceedings. IEEE, 458\u2013464.\n[236] Haomin Wen, Youfang Lin, Yutong Xia, Huaiyu Wan, Roger Zimmermann, and Yuxuan Liang. 2023. Diffstg: Probabilistic\nspatio-temporal graph forecasting with denoising diffusion models. arXiv preprint arXiv:2301.13629 (2023).\n[237] Nurhadi Wijaya, Sri Hasta Mulyani, and Albertus Christian Noviadi Prabowo. 2022. DeepDrive: effective distracted driver\ndetection using generative adversarial networks (GAN) algorithm. Iran Journal of Computer Science 5, 3 (2022), 221\u2013227.\n[238] Jiayang Wu, Wensheng Gan, Zefeng Chen, Shicheng Wan, and Hong Lin. 2023. Ai-generated content (aigc): A survey. arXiv\npreprint arXiv:2304.06632 (2023).\n[239] Lan Wu, Tian Gao, Chenglin Wen, Kunpeng Zhang, and Fanshi Kong. 2021. A High-Dimensional Video Sequence Completion\nMethod with Traffic Data Completion Generative Adversarial Networks. Wireless Communications and Mobile Computing\n2021 (2021), 1\u20139.\n[240] Lan Wu, Han Wang, Tian Gao, Binquan Li, and Fanshi Kong. 2022. A Traffic Video Completion Model Based on Generative\nAdversarial Networks. In Proceedings of 2021 Chinese Intelligent Automation Conference. Springer, 658\u2013669.\n[241] Zhu Xiao, Jinmei Shu, Hongbo Jiang, Geyong Min, Hongyang Chen, and Zhu Han. 2022. Perception task offloading with\ncollaborative computation for autonomous driving. IEEE Journal on Selected Areas in Communications 41, 2 (2022), 457\u2013473.\n[242] Lei Xie, Tao Guo, Jiliang Chang, Chengpeng Wan, Xinyuan Hu, Yang Yang, and Changkui Ou. 2023. A Novel Model for\nShip Trajectory Anomaly Detection Based on Gaussian Mixture Variational Autoencoder. IEEE Transactions on Vehicular\nTechnology (2023).\n[243] Hao Xing, Jianming Hu, and Zuo Zhang. 2022. Multi-modal Vehicle Trajectory Prediction via Attention-based Conditional\nVariational Autoencoder. In 2022 37th Youth Academic Annual Conference of Chinese Association of Automation (YAC). IEEE,\n1367\u20131373.\n[244] Gang Xiong, Zhishuai Li, Meihua Zhao, Yu Zhang, Qinghai Miao, Yisheng Lv, and Fei-Yue Wang. 2023. TrajSGAN: A\nSemantic-Guiding Adversarial Network for Urban Trajectory Generation. IEEE Transactions on Computational Social Systems\n(2023).\n[245] Zhang Xiong, Hao Sheng, WenGe Rong, and Dave E Cooper. 2012. Intelligent transportation systems for smart cities: a\nprogress review. Science China Information Sciences 55 (2012), 2908\u20132914.\n[246] Chejian Xu, Ding Zhao, Alberto Sangiovanni-Vincentelli, and Bo Li. 2023. DiffScene: Diffusion-Based Safety-Critical Scenario\nGeneration for Autonomous Vehicles. In The Second Workshop on New Frontiers in Adversarial Machine Learning.\n[247] Dongwei Xu, Peng Peng, Chenchen Wei, Defeng He, and Qi Xuan. 2020. Road traffic network state prediction based on a\ngenerative adversarial network. IET Intelligent Transport Systems 14, 10 (2020), 1286\u20131294.\n[248] Dongwei Xu, Chenchen Wei, Peng Peng, Qi Xuan, and Haifeng Guo. 2020. GE-GAN: A novel deep learning framework for\nroad traffic state estimation. Transportation Research Part C: Emerging Technologies 117 (2020), 102635.\n[249] Dongwei Xu, Zefeng Yu, Tian Tian, and Yanfang Yang. 2022. Generative Adversarial Network for Imputation of Road Network\nTraffic State Data. In China National Conference on Big Data and Social Computing. Springer, 80\u201396.\n[250] Minrui Xu, Hongyang Du, Dusit Niyato, Jiawen Kang, Zehui Xiong, Shiwen Mao, Zhu Han, Abbas Jamalipour, Dong In Kim,\nVictor Leung, et al. 2023. Unleashing the power of edge-cloud generative ai in mobile networks: A survey of aigc services.\narXiv preprint arXiv:2303.16129 (2023).\n[251] Minrui Xu, Dusit Niyato, Junlong Chen, Hongliang Zhang, Jiawen Kang, Zehui Xiong, Shiwen Mao, and Zhu Han. 2023. Gener-\native AI-empowered simulation for autonomous driving in vehicular mixed reality metaverses. arXiv preprint arXiv:2302.08418\n(2023).\n[252] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. 2018. Attngan: Fine-\ngrained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition. 1316\u20131324.\n[253] Xovee Xu, Yutao Wei, Pengyu Wang, Xucheng Luo, Fan Zhou, and Goce Trajcevski. 2023. Diffusion Probabilistic Modeling\nfor Fine-Grained Urban Traffic Flow Inference with Relaxed Structural Constraint. In ICASSP 2023-2023 IEEE International\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nHuan Yan and Yong Li\nConference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 1\u20135.\n[254] Bing Yang, Yan Kang, YaoYao Yuan, Xin Huang, and Hao Li. 2021. ST-LBAGAN: Spatio-temporal learnable bidirectional\nattention generative adversarial networks for missing traffic data imputation. Knowledge-Based Systems 215 (2021), 106705.\n[255] Bing Yang, Yan Kang, Yaoyao Yuan, Hao Li, and Fei Wang. 2022. ST-FVGAN: filling series traffic missing values with generative\nadversarial network. Transportation Letters 14, 4 (2022), 407\u2013415.\n[256] Zi Yang and Lilian SC Pun-Cheng. 2018. Vehicle detection in intelligent transportation systems and its applications under\nvarying environments: A review. Image and Vision Computing 69 (2018), 143\u2013154.\n[257] Peijun Ye, Fenghua Zhu, Yisheng Lv, Xiao Wang, and Yuanyuan Chen. 2022. Efficient Calibration of Agent-Based Traffic\nSimulation Using Variational Auto-Encoder. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems\n(ITSC). IEEE, 3077\u20133082.\n[258] Byeonghyeop Yu, Yongjin Lee, and Keemin Sohn. 2020. Forecasting road traffic speeds by considering area-wide spatio-\ntemporal dependencies based on a graph convolutional neural network (GCN). Transportation research part C: emerging\ntechnologies 114 (2020), 189\u2013204.\n[259] James Jian Qiao Yu and Jiatao Gu. 2019. Real-time traffic speed estimation with graph convolutional generative autoencoder.\nIEEE Transactions on Intelligent Transportation Systems 20, 10 (2019), 3940\u20133951.\n[260] Haitao Yuan and Guoliang Li. 2021. A survey of traffic prediction: from spatio-temporal data to intelligent transportation.\nData Science and Engineering 6 (2021), 63\u201385.\n[261] Xinyu Yuan, Yan Qiao, Pei Zhao, Rongyao Hu, and Benchu Zhang. 2023. Traffic Matrix Estimation based on Denoising\nDiffusion Probabilistic Model. In 2023 IEEE Symposium on Computers and Communications (ISCC). IEEE, 316\u2013322.\n[262] Ye Yuan, Yong Zhang, Boyue Wang, Yuan Peng, Yongli Hu, and Baocai Yin. 2022. STGAN: Spatio-temporal generative\nadversarial network for traffic data imputation. IEEE Transactions on Big Data 9, 1 (2022), 200\u2013211.\n[263] Taeyoung Yun, Haewon Jung, and Jiwoo Son. 2023. Imputation as Inpainting: Diffusion models for SpatioTemporal Data\nImputation. (2023).\n[264] Y Yun, D Jeong, and S Lim. 2019. Data-driven human-like cut-in driving model using generative adversarial network.\nElectronics Letters 55, 24 (2019), 1288\u20131290.\n[265] Mohsen Zand, Ali Etemad, and Michael Greenspan. 2023. Flow-based Spatio-Temporal Structured Prediction of Dynamics.\nIEEE Transactions on Pattern Analysis and Machine Intelligence (2023).\n[266] Di Zang, Yang Fang, Zhihua Wei, Keshuang Tang, and Jiujun Cheng. 2019. Traffic flow data prediction using residual\ndeconvolution based deep generative network. IEEE Access 7 (2019), 71311\u201371322.\n[267] Mohammad Zarei and Bruce Hellinga. 2021. Crash Data Augmentation Using Conditional Generative Adversarial Networks\n(CGAN) for Improving Safety Performance Functions. arXiv preprint arXiv:2112.12263 (2021).\n[268] Aditi Zear, Pradeep Kumar Singh, and Yashwant Singh. 2016. Intelligent transport system: A progressive review. (2016).\n[269] Chaoning Zhang, Chenshuang Zhang, Chenghao Li, Yu Qiao, Sheng Zheng, Sumit Kumar Dam, Mengchun Zhang, Jung Uk\nKim, Seong Tae Kim, Jinwoo Choi, et al. 2023. One small step for generative ai, one giant leap for agi: A complete survey on\nchatgpt in aigc era. arXiv preprint arXiv:2304.06488 (2023).\n[270] Chaoning Zhang, Chenshuang Zhang, Sheng Zheng, Yu Qiao, Chenghao Li, Mengchun Zhang, Sumit Kumar Dam, Chu Myaet\nThwal, Ye Lin Tun, Le Luang Huy, et al. 2023. A complete survey on generative ai (aigc): Is chatgpt from gpt-4 to gpt-5 all you\nneed? arXiv preprint arXiv:2303.11717 (2023).\n[271] He Zhang, Jian Sun, and Ye Tian. 2022. Accelerated Testing for Highly Automated Vehicles: A Combined Method Based on\nImportance Sampling and Normalizing Flows. In 2022 IEEE 25th International Conference on Intelligent Transportation Systems\n(ITSC). IEEE, 574\u2013579.\n[272] Jinlei Zhang, Hua Li, Lixing Yang, Guangyin Jin, Jianguo Qi, and Ziyou Gao. 2022. STG-GAN: A spatiotemporal graph\ngenerative adversarial networks for short-term passenger flow prediction in urban rail transit systems. arXiv preprint\narXiv:2202.06727 (2022).\n[273] Jinlei Zhang, Hua Li, Shuxin Zhang, Lixing Yang, Guangyin Jin, and Jianguo Qi. 2023. A spatiotemporal graph generative\nadversarial networks for short-term passenger flow prediction in urban rail transit systems. International Journal of General\nSystems (2023), 1\u201328.\n[274] Kunpeng Zhang, Zhengbing He, Liang Zheng, Liang Zhao, and Lan Wu. 2021. A generative adversarial network for travel\ntimes imputation using trajectory data. Computer-Aided Civil and Infrastructure Engineering 36, 2 (2021), 197\u2013212.\n[275] Liang Zhang, Jianqing Wu, Jun Shen, Ming Chen, Rui Wang, Xinliang Zhou, Cankun Xu, Quankai Yao, and Qiang Wu. 2021.\nSATP-GAN: Self-attention based generative adversarial network for traffic flow prediction. Transportmetrica B: Transport\nDynamics 9, 1 (2021), 552\u2013568.\n[276] Liming Zhang, Liang Zhao, and Dieter Pfoser. 2022. Factorized deep generative models for end-to-end trajectory generation\nwith spatiotemporal validity constraints. In Proceedings of the 30th International Conference on Advances in Geographic\nInformation Systems. 1\u201312.\n[277] Mengchun Zhang, Maryam Qamar, Taegoo Kang, Yuna Jung, Chenshuang Zhang, Sung-Ho Bae, and Chaoning Zhang. 2023. A\nsurvey on graph diffusion models: Generative ai in science for molecule, protein and material. arXiv preprint arXiv:2304.01565\n(2023).\n[278] Ning Zhang. 2020. Learning adversarial transformer for symbolic music generation. IEEE transactions on neural networks and\nlearning systems (2020).\n[279] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. 2021. Vinvl:\nRevisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. 5579\u20135588.\n[280] Ruichen Zhang, Ke Xiong, Hongyang Du, Dusit Niyato, Jiawen Kang, Xuemin Shen, and H Vincent Poor. 2023. Generative\nAI-enabled Vehicular Networks: Fundamentals, Framework, and Case Study. arXiv preprint arXiv:2304.11098 (2023).\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\nA Survey of Generative AI for Intelligent Transportation Systems\n[281] Shunyang Zhang, Senzhang Wang, Xianzhen Tan, Ruochen Liu, Jian Zhang, and Jianxin Wang. 2023. sasdim: self-adaptive\nnoise scaling diffusion model for spatial time series imputation. arXiv preprint arXiv:2309.01988 (2023).\n[282] Tong Zhang, Jianlong Wang, and Jie Liu. 2021. A gated generative adversarial imputation approach for signalized road\nnetworks. IEEE Transactions on Intelligent Transportation Systems 23, 8 (2021), 12144\u201312160.\n[283] Weibin Zhang, Pulin Zhang, Yinghao Yu, Xiying Li, Salvatore Antonio Biancardo, and Junyi Zhang. 2021. Missing data repairs\nfor traffic flow with self-attention generative adversarial imputation net. IEEE Transactions on Intelligent Transportation\nSystems 23, 7 (2021), 7919\u20137930.\n[284] Xiaoxi Zhang, Yuan Gao, Xin Wang, Jun Feng, and Yan Shi. 2022. GeoSDVA: A Semi-Supervised Dirichlet Variational\nAutoencoder Model for Transportation Mode Identification. ISPRS International Journal of Geo-Information 11, 5 (2022), 290.\n[285] Xin Zhang, Yanhua Li, Xun Zhou, and Jun Luo. 2020. cgail: Conditional generative adversarial imitation learning\u2014an\napplication in taxi drivers\u2019 strategy learning. IEEE transactions on big data 8, 5 (2020), 1288\u20131300.\n[286] Xin Zhang, Yanhua Li, Xun Zhou, Ziming Zhang, and Jun Luo. 2020. Trajgail: Trajectory generative adversarial imitation\nlearning for long-term decision analysis. In 2020 IEEE International Conference on Data Mining (ICDM). IEEE, 801\u2013810.\n[287] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2019. TrafficGAN: Off-deployment traffic estimation\nwith traffic generative adversarial networks. In 2019 IEEE International Conference on Data Mining (ICDM). IEEE, 1474\u20131479.\n[288] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2020. Curb-gan: Conditional urban traffic estimation\nthrough spatio-temporal generative adversarial networks. In Proceedings of the 26th ACM SIGKDD International Conference on\nKnowledge Discovery & Data Mining. 842\u2013852.\n[289] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2020. Off-deployment traffic estimation\u2014A traffic\ngenerative adversarial networks approach. IEEE transactions on big data 8, 4 (2020), 1084\u20131095.\n[290] Yingxue Zhang, Yanhua Li, Xun Zhou, Xiangnan Kong, and Jun Luo. 2022. STrans-GAN: Spatially-Transferable Generative\nAdversarial Networks for Urban Traffic Estimation. In 2022 IEEE International Conference on Data Mining (ICDM). IEEE,\n743\u2013752.\n[291] Yingxue Zhang, Yanhua Li, Xun Zhou, Zhenming Liu, and Jun Luo. 2021. C 3-GAN: Complex-Condition-Controlled Urban\nTraffic Estimation through Generative Adversarial Networks. In 2021 IEEE International Conference on Data Mining (ICDM).\nIEEE, 1505\u20131510.\n[292] Yuxuan Zhang, Senzhang Wang, Bing Chen, and Jiannong Cao. 2019. GCGAN: Generative adversarial nets with graph CNN\nfor network-scale traffic prediction. In 2019 International Joint Conference on Neural Networks (IJCNN). IEEE, 1\u20138.\n[293] Han Zhao, Ruikang Luo, Bowen Yao, Yiyi Wang, Shaoqing Hu, and Rong Su. 2022. GraphSAGE-Based Generative Adversarial\nNetwork for Short-Term Traffic Speed Prediction Problem. In 2022 17th International Conference on Control, Automation,\nRobotics and Vision (ICARCV). IEEE, 837\u2013842.\n[294] Jiaxin Zhao, Yang Zhang, Xiaohu Ma, Dongdong Yang, Yao Shen, and Hualiang Jiang. 2021. MA-GAN: A Method Based on\nGenerative Adversarial Network for Calligraphy Morphing. In Neural Information Processing: 28th International Conference,\nICONIP 2021, Sanur, Bali, Indonesia, December 8\u201312, 2021, Proceedings, Part I 28. Springer, 266\u2013278.\n[295] Xiaocong Zhao, Ye Tian, and Jian Sun. 2021. Yield or rush? Social-preference-aware driving interaction modeling using\ngame-theoretic framework. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC). IEEE, 453\u2013459.\n[296] Yilin Zhao, Yijie Xun, Jiajia Liu, and Siyu Ma. 2022. GVIDS: A Reliable Vehicle Intrusion Detection System Based on Generative\nAdversarial Network. In GLOBECOM 2022-2022 IEEE Global Communications Conference. IEEE, 4310\u20134315.\n[297] Zihao Zhao, Min Jiang, Jia Guo, Xiaoyu Yang, Yudie Hu, and Xianlong Zhou. 2022. Raindrop Removal for In-Vehicle Camera\nImages with Generative Adversarial Network. In 2022 IEEE International Conference on Systems, Man, and Cybernetics (SMC).\nIEEE, 3131\u20133136.\n[298] Hongling Zheng, Xiang Li, Yongfeng Li, Ziqin Yan, and Tinghong Li. 2022. GCN-GAN: integrating graph convolutional\nnetwork and generative adversarial network for traffic flow prediction. IEEE Access 10 (2022), 94051\u201394062.\n[299] Ting Zhong, Haoyang Yu, Rongfan Li, Xovee Xu, Xucheng Luo, and Fan Zhou. 2022. Probabilistic Fine-Grained Urban\nFlow Inference with Normalizing Flows. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal\nProcessing (ICASSP). IEEE, 3663\u20133667.\n[300] Ziyuan Zhong, Davis Rempe, Yuxiao Chen, Boris Ivanovic, Yulong Cao, Danfei Xu, Marco Pavone, and Baishakhi Ray. 2023.\nLanguage-Guided Traffic Simulation via Scene-Level Diffusion. arXiv preprint arXiv:2306.06344 (2023).\n[301] Ziyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. 2023. Guided\nconditional diffusion for controllable traffic simulation. In 2023 IEEE International Conference on Robotics and Automation\n(ICRA). IEEE, 3560\u20133566.\n[302] Danyang Zhou, Huxiao Wang, Wei Li, Yi Zhou, Nan Cheng, and Ning Lu. 2021. SA-SGAN: A Vehicle Trajectory Prediction\nModel Based on Generative Adversarial Networks. In 2021 IEEE 94th Vehicular Technology Conference (VTC2021-Fall). IEEE,\n1\u20135.\n[303] Xuan Zhou, Ruimin Ke, Hao Yang, and Chenxi Liu. 2021. When intelligent transportation systems sensing meets edge\ncomputing: Vision and challenges. Applied Sciences 11, 20 (2021), 9680.\n[304] Zhilun Zhou, Jingtao Ding, Yu Liu, Depeng Jin, and Yong Li. 2023. Towards Generative Modeling of Urban Flow through\nKnowledge-enhanced Denoising Diffusion. arXiv preprint arXiv:2309.10547 (2023).\n[305] Li Zhu, Fei Richard Yu, Yige Wang, Bin Ning, and Tao Tang. 2018. Big data analytics in intelligent transportation systems: A\nsurvey. IEEE Transactions on Intelligent Transportation Systems 20, 1 (2018), 383\u2013398.\n[306] Yuanshao Zhu, Yongchao Ye, Xiangyu Zhao, and James JQ Yu. 2023. Diffusion Model for GPS Trajectory Generation. arXiv\npreprint arXiv:2304.11582 (2023).\nACM Comput. Surv., Vol. 1, No. 1, Article . Publication date: December 2022.\n",
    "2404.18886": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nA Survey on Diffusion Models for Time Series\nand Spatio-Temporal Data\nYiyuan Yang, Ming Jin, Haomin Wen, Chaoli Zhang, Yuxuan Liang\u2020, Lintao Ma, Yi Wang, Chenghao Liu,\nBin Yang, Zenglin Xu, Jiang Bian, Shirui Pan, Qingsong Wen\u2020\nAbstract\u2014The study of time series is crucial for understanding trends and anomalies over time, enabling predictive insights across\nvarious sectors. Spatio-temporal data, on the other hand, is vital for analyzing phenomena in both space and time, providing a dynamic\nperspective on complex system interactions. Recently, diffusion models have seen widespread application in time series and\nspatio-temporal data mining. Not only do they enhance the generative and inferential capabilities for sequential and temporal data, but\nthey also extend to other downstream tasks. In this survey, we comprehensively and thoroughly review the use of diffusion models in\ntime series and spatio-temporal data, categorizing them by model category, task type, data modality, and practical application domain.\nIn detail, we categorize diffusion models into unconditioned and conditioned types and discuss time series and spatio-temporal data\nseparately. Unconditioned models, which operate unsupervised, are subdivided into probability-based and score-based models,\nserving predictive and generative tasks such as forecasting, anomaly detection, classification, and imputation. Conditioned models, on\nthe other hand, utilize extra information to enhance performance and are similarly divided for both predictive and generative tasks. Our\nsurvey extensively covers their application in various fields, including healthcare, recommendation, climate, energy, audio, and\ntransportation, providing a foundational understanding of how these models analyze and generate data. Through this structured\noverview, we aim to provide researchers and practitioners with a comprehensive understanding of diffusion models for time series and\nspatio-temporal data analysis, aiming to direct future innovations and applications by addressing traditional challenges and exploring\ninnovative solutions within the diffusion model framework.\nIndex Terms\u2014survey, diffusion model, time series, spatio-temporal data, generative model, temporal data, DDPM, Score SDE.\n\u2726\n1\nINTRODUCTION\nD\nIffusion models represent a family of probabilistic\ngenerative models that undergo optimization through\na two-step process involving the injection and subsequent\nremoval of noise across a set of training samples. This\nprocess comprises a forward phase, referred to as diffusion,\nand a reverse phase, known as denoising. By training the\nmodel to remove the noise added during the diffusion\nprocess, the model learns to generate effective data samples\n\u2022\nYiyuan Yang is with the Department of Computer Science, University of\nOxford (yiyuan.yang@cs.ox.ac.uk).\n\u2022\nMing Jin and Shirui Pan are with the School of Information and\nCommunication Technology, Griffith University (mingjinedu@gmail.com;\ns.pan@griffith.edu.au).\n\u2022\nHaomin Wen is with Beijing Jiaotong University and the Hong\nKong\nUniversity\nof\nScience\nand\nTechnology\n(Guangzhou)\n(wen-\nhaomin@bjtu.edu.cn).\n\u2022\nChaoli Zhang is with the School of Computer Science and Technology,\nZhejiang Normal University (chaolizcl@zjnu.edu.cn).\n\u2022\nYuxuan Liang is with INTR & DSA Thrust, Hong Kong University of\nScience and Technology (Guangzhou) (yuxliang@outlook.com).\n\u2022\nLintao Ma is with Ant Group (lintao.mlt@antgroup.com).\n\u2022\nYi Wang is with The University of Hong Kong (yiwang@eee.hku.hk).\n\u2022\nChenghao Liu is with Salesforce Research (chenghao.liu@salesforce.com).\n\u2022\nBin\nYang\nis\nwith\nEast\nChina\nNormal\nUniversity\n(byang@dase.ecnu.edu.cn).\n\u2022\nZenglin Xu is with Fudan University (zenglin@gmail.com).\n\u2022\nJiang Bian is with Microsoft Research Asia (jiang.bian@microsoft.com).\n\u2022\nQingsong Wen is with Squirrel Ai Learning (qingsongedu@gmail.com).\nRepo: https://github.com/yyysjz1997/Awesome-TimeSeries-SpatioTemporal-Diffusion-Model\n\u2020Corresponding authors: Yuxuan Liang and Qingsong Wen.\nVersion date: June 12, 2024.\nWaveGrad\nTimeGrad\nCSDI\nCDiffuSE\nCARD\nUNIVERSE\nD3VAE\nCDDRec\nTimeDiff\nDiffTime\nTS-Diffusion\nTime Weaver\nQuery =\u201cdiffusion model\u201dAND\u201ctime series\u201d\nVideo-Diffusion\nDiffSTG\nPriSTI\nLDM\nSTPP\nDyffusion\nUSTD\nSpecSTG\nIDM\nDST-DDPM\nQuery =\u201cdiffusion model\u201dAND\u201cspatial temporal\u201d\nFig. 1: Trends in the cumulative number of papers related to\ndiffusion models for time series and spatio-temporal data.\nduring inference that align closely with the distribution of\nthe training data [2], [189].\nIn recent years, diffusion models have risen to promi-\nnence and significantly influenced various domains, includ-\ning computer vision (CV) [2], [8], [190], [191], [192], natural\nlanguage processing (NLP) [172], [193], [194], [195], and gen-\neral multimodal learning [112], [196], [197], [198]. This chal-\nlenges the long-time supremacy of generative adversarial\nnetworks (GANs) [115], [187]. Within these areas, diffusion\nmodels have demonstrated remarkable capabilities in appli-\ncations such as text-to-image [112], [199], instance segmenta-\ntion [200], [201], 3D shape generation [202], [203], molecule\ndesign [204], [205], [292], and audio generation [91], [206].\nRemarkably, diffusion models have also gained popularity\nas a non-autoregressive alternative for tasks conventionally\ndominated by autoregressive methods [189]. Recently, the\narXiv:2404.18886v3  [cs.LG]  11 Jun 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nMeta Data\nDiffusion process\nDenoising process\nData\nNoise\n\ud835\udc5e\ud835\udc65! \ud835\udc65!\"# \u2254\ud835\udca9(\ud835\udc65!;\n1 \u2212\ud835\udefd!\ud835\udc65!\"#, \ud835\udefd!\u0399)\n\ud835\udc5d$ \ud835\udc65!\"# \ud835\udc65! \u2254\ud835\udca9(\ud835\udc65!\"#; \ud835\udf07$ \ud835\udc65!, \ud835\udc58, \ud835\udf0e$ \ud835\udc65!, \ud835\udc58\u0399)\nMeta Data\nDomain\nknowledge\nConstrains\nMeta Data\nTime Series\nSpatio-temporal\nData\nMeta Data\nUnconditional\nMeta Data\nConditional\n\u2026\nForecasting\nImputation\nGeneration\nForecasting\nImputation\n\u2026\nFig. 2: An overview of diffusion models for time series and spatio-temporal data analysis. In diffusion process, xk and\nxk\u22121 denote the results after adding noise at step k and k \u22121, respectively. This process can be represented by the size of\nthe controlling steps \u03b2k \u2208(0, 1), the identity matrix I, and a Gaussian distribution N(x; \u00b5, \u03c3) of x with the mean \u00b5 and the\ncovariance \u03c3. During the denoising process, the model attempts to iteratively learn the data distribution by modelling the\ndistribution p\u03b8(xk\u22121|xk). The functions \u00b5\u03b8(\u00b7) and variance \u03c3\u03b8(\u00b7) are the model learnable parameters.\nintroduction of OpenAI Sora [87] marks the advent of dif-\nfusion models in modeling the physical world embedded\nwithin the spacetime continuum, highlighting their critical\nimportance. In addition, AlphaFold 3 [292] proposed by\nGoogle DeepMind uses diffusion models to generate 3D\natomic coordinates and predict biomolecular structures like\nproteins, DNA, and RNA.\nTemporal data, which primarily includes time series and\nspatio-temporal data, encapsulates the dynamics of the vast\nmajority of real-world systems [76]. These forms of temporal\ndata have been extensively studied and are recognized as\ncrucial for numerous applications [77], [207], [208]. How-\never, deriving universal dynamic laws in the physical world\nfrom various data modalities remains a significant challenge\nwithin the field. Recently, the area of time series and spatio-\ntemporal modeling has experienced a substantial shift from\nsensory intelligence towards general intelligence [209]. This\nshift is characterized by the emergence of unified foun-\ndation models (FMs) that possess versatile temporal data\nanalytical capabilities [76], [209], challenging the supremacy\nof domain-specific models. Diffusion models have achieved\nstate-of-the-art results on many modalities, including im-\nages, speech, and video [210]. Benefiting from the vast and\ndiverse available data in these fields, diffusion models often\nserve as generative FMs alongside large language models\n(LLMs) or other foundation models, facilitating rapid de-\nvelopment in these areas [7], [190]. In recent years, there has\nalso been an increasing number of diffusion models crafted\nfor modeling time series and spatio-temporal data (Fig. 1\n). Also, we have become aware of an increasing number\nof attempts to use diffusion models for temporal modeling\n(see Tab. 1 ). Observing the success of diffusion models, an\nintriguing question arises: what kind of sparks will emerge\nfrom the intersection of time series/spatio-temporal data\nanalysis and diffusion models?\nTime series and spatio-temporal data analysis funda-\nmentally rely on a profound understanding of their inherent\ntemporal dynamics, wherein primary tasks predominantly\nfocus on the generative capabilities of backbone models,\nsuch as forecasting [13], [86], [253], imputation [56], [69],\n[262] and generation [182], [198]. These analyses center on\ngenerating temporal data samples for specific purposes in\nconditional or unconditional manners. Having witnessed\nthe recent development of time series and spatio-temporal\nfoundation models [76], [254], whether built upon LLMs\nor trained from scratch, their success can be attributed to\nthe ability to estimate the distribution of training samples\nwhere effective data representations can be drawn. In this\nregard, diffusion models emerge as a powerful generative\nframework that enables (1) the modeling of complex pat-\nterns within temporal data and (2) the support of a wide\nrange of downstream tasks, as depicted in Fig. 2 .\nTo generate valid data samples for specific tasks, time\nseries and spatio-temporal diffusion models usually op-\nerate in an unconditional manner without the need for\nsupervision signals. Given the partially-observed nature of\nreal-world applications [255], conditional diffusion models\nhave emerged. They leverage data labels (e.g., instructions,\nmetadata, or exogenous variables) to regulate the generation\nprocess, thereby enabling effective cross-modal prompting\nthat leads to more tailored and improved outcomes [182].\nWe present a roadmap in Fig. 3 . By training on large-scale\ntemporal data, diffusion models effectively fill the gap of\ntime series/spatio-temporal data generation and exhibit sig-\nnificant potential in solving the puzzle of next-generation,\nLLM-empowered temporal data-centric agents [209], [256].\nDespite the promising prospects and rapid advancement\nof diffusion models in handling time series and spatio-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nConditional\n\u00a0Time Series\nSpatio-Temporal Data\nD3R\nDiffAD\nCSDI\nDiffSTG\nCDiffuSE\nUNIVERSE\nUnconditional\u00a0\nTSDiff\nLDT\nTimeGrad\nCARD\nWaveGrad\nScoreGrad\nDYffusion\n2021\n2024\n2022\n2023\nDiffWave\nD3VAE\nDiffASR\nTS-Diffusion\nDSTPP\nEHRDiff\nDiffRec\nDS-DDPM\nFreeInit\nDiffASR\nLADM\nDST-DDPM\nCDDRec\nGMCD\nTSGM\nDiffTraj\nDiffESM\nSF-DM\nGuidedDiffTime\nUSTD\nDiffDA\nSRNDiff\nTime Weaver\nFig. 3: Representative diffusion models for time series and spatio-temporal data in recent years.\ntemporal data, there has been a conspicuous lack of system-\natic analysis of this model family in the existing literature.\nThis article aims to bridge this gap by providing a forward-\nlooking review that elucidates both the \u2019why\u2019 and the\n\u2019how\u2019 \u2014- detailing the reasons diffusion models are suited\nfor these data modalities and unveiling the mechanisms\nthrough which they confer advantages. In this survey, we\noffer a detailed categorization, engage in thorough reviews,\nand identify burgeoning trends within this rapidly evolv-\ning landscape. Our main contributions are summarized as\nfollows:\n\u2022 Comprehensive and up-to-date review. We present a\ncomprehensive, up-to-date, and forward-looking review\nof diffusion models for time series and spatio-temporal\ndata. Our survey highlights the suitability of diffusion\nmodels for these data modalities and discusses the bene-\nfits they confer. By covering both a broad spectrum of the\nfield and the specifics of individual methods, we furnish\nreaders with a deep insight into this subject area.\n\u2022 Unified and structured categorization. We introduce a\nclear and organized framework for categorizing the ex-\nisting literature into two main types: unconditional and\nconditional diffusion models, focusing on time series and\nspatio-temporal data that span both predictive and gener-\native tasks. This categorization offers the reader a coherent\nroadmap of the topic from multiple perspectives.\n\u2022 Insights into emerging advances. We discuss cutting-\nedge techniques in both unconditional and conditional\ndiffusion models, focusing on time series and spatio-\ntemporal data. Our coverage includes the latest tech-\nniques and emerging trends such as multimodal condi-\ntional generation.\n\u2022 Summary of challenges and future directions. We iden-\ntify key challenges faced in the current research landscape\nand highlight several promising directions for future ex-\nploration.\nThe remainder of this paper is structured as follows:\nSec. 2\nprovides a comprehensive background on diffu-\nsion models, detailing their development, theoretical foun-\ndations, and various implementations. Sec. 3\npresents a\nstructured overview and categorization of diffusion models\napplied to time series and spatio-temporal data, setting the\nstage for a deeper exploration of model perspectives in\nSec. 4 , which discusses both standard and advanced diffu-\nsion models. Sec. 5 focuses on task perspectives, examining\nhow diffusion models tackle forecasting, generation, impu-\ntation, anomaly detection, and more. Sec. 6 discusses data\nperspectives, highlighting challenges and solutions specific\nto time series and spatio-temporal data. Sec. 7\nexplores\nthe application of diffusion models across various domains,\nsuch as healthcare, traffic, and energy, demonstrating their\nbroad utility. Finally, Sec. 8\nconcludes the paper with an\noutlook on future opportunities and summarizing remarks.\n2\nBACKGROUND\nThis paper primarily reviews recent improvements in using\ndiffusion models to solve time series and spatio-temporal\ndata challenges. In this section, we will first define time\nseries and spatio-temporal data, as well as their correspond-\ning tasks in various fields. Then, we will introduce the\nhistory of diffusion model and its advantages. Finally, some\ndifferent kinds of diffusion models and their variants based\non theoretical formula derivations and comparisons with\nother generative models will be presented.\n2.1\nOverview of Time Series and Spatio-Temporal Data\nTemporal data, particularly time series and spatio-temporal\ndata, are important data structures for a wide range of\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\n(c) Spatio-Temporal Graph\nTime\n1t\n2t\n3t\n\u2026\n\u2026\nSpace\n(d) Spatio-Temporal Trajectory\nTime\n1t\n2t\n3t\n\u2026\n\u2026\nSpace\nTrajectory 1\nTrajectory 2\n(a) Univariate Time Series\n(b) Multivariate Time Series\nTime\nTime\nValue\nValue\nFig. 4: Illustrations of time series and spatio-temporal data.\nreal-world applications [76]. A time series is defined as\na sequential arrangement of data points, categorized by\ntheir temporal order. These sequences may be univariate,\ninvolving a single variable over time, or multivariate, incor-\nporating multiple variables. For instance, daily air quality\nmeasurements in a city constitute a univariate time series,\nwhile combining daily temperature and humidity readings\ngenerates a multivariate series. In our discussions, we em-\nploy bold uppercase letters (e.g., X) to represent matrices,\nbold lowercase (e.g., x) for vectors, calligraphic uppercase\n(e.g., X ) for sets, and standard lowercase (e.g., x) for scalars.\nWe base our formal definitions of time series on those\nprovided in [76], [77]. Specifically, a univariate time series\n(Fig. 4 (a)), denoted as x = (x1, x2, . . . , xT ) \u2208RT , consists\nof a sequence of T data points arranged chronologically,\nwhere each xt \u2208R represents the series\u2019 value at time t.\nConversely, a multivariate time series (Fig. 4 (b)), repre-\nsented by X = (x1, x2, . . . , xT ) \u2208RT \u00d7D, encompasses a\nsequence of T data points also in temporal sequence but\nacross D feature channels, with xt \u2208RD(1 \u2264t \u2264T)\nindicating the series\u2019 values at time t across D different\nchannels. For a comprehensive exploration of time series,\nwe direct the reader to [77].\nSpatio-temporal\ndata,\nin\ncontrast,\nencompasses\nse-\nquences of data points characterized by both their temporal\nand spatial dimensions. This type of data integrates the\naspect of time, as seen in time series, with the additional\ndimension of space, capturing the complex dynamics of\nphenomena as they unfold over time and across different\nlocations. From this perspective, multivariate time series\nmay also be considered as a form of spatio-temporal data.\nSuch data is instrumental in fields ranging from geography\nand meteorology to urban planning and environmental\nmonitoring, where understanding the interplay between\nspatial patterns and temporal evolution is crucial.\nIn practical applications, spatio-temporal data refers to a\ncollection of observations where each data point is defined\nby its position in space and time, encapsulating a diverse\nrange of data structures such as graphs, trajectories, and\neven videos, as noted in [76].\nFor instance, a spatio-temporal graph (Fig. 4 (c)) rep-\nresenting urban traffic flow [286], [287] over time can be\nunderstood as spatio-temporal data where each node rep-\nresents a specific location with specific attributes and edges\nare weighted by traffic volume, which changes over time.\nLikewise, trajectory data [281], [288] (Fig. 4 (d)) captures\nthe movement of objects through space over time, including\ntheir paths, speeds, and changes in direction. Such data\nis vital for applications in transportation studies, wildlife\ntracking, and mobile network optimization, where analyz-\ning the patterns of movement and predicting future loca-\ntions based on historical data are of paramount importance.\nOn this basis, these and other spatio-temporal constructs can\nbe systematically characterized and analyzed.\nBuilding on the definitions provided, we now proceed to\nsuccinctly introduce the representative tasks associated with\neach data category [76].\n\u2022 Time Series Analysis. The analysis of time series with\ndiffusion models encompasses four primary tasks: fore-\ncasting, generation, anomaly detection, and imputation. Fore-\ncasting focuses on predicting future values within a time\nseries, which can be subdivided into short-term and long-\nterm forecasts based on the temporal scope of the predic-\ntions. Generation involves creating new time series based\non the statistical properties of a given dataset, serving as a\nway to simulate possible scenarios or enhance data diver-\nsity for training models. Anomaly detection, a specialized\nform of classification, aims to distinguish atypical series\nfrom normal ones. Imputation addresses the challenge of\nfilling in missing values within a series, which is crucial\nfor maintaining the integrity and utility of time series.\n\u2022 Spatio-Temporal Data Analysis. Spatio-temporal data\nanalysis, while encompassing tasks similar to those in\ntime series analysis, often applies these methodologies\nwithin specific application scenarios. For instance, fore-\ncasting may focus on traffic flow [43] or air quality [78],\nutilizing historical data patterns to predict future con-\nditions. Generation tasks might involve creating syn-\nthetic trajectories, offering privacy-compliant alternatives\nto original datasets by replacing sensitive information\nwith generated, anonymized data [81], [290]. Anomaly\ndetection becomes particularly crucial in scenarios such\nas vehicle trajectory analysis, where deviations from gen-\nerated normative patterns may indicate unusual or sus-\npicious behaviors [83]. Additionally, spatio-temporal im-\nputation plays a vital role in addressing missing values in\nmultivariate time series [69], ensuring comprehensive and\naccurate datasets for further analysis. More discussion is\nin Sec. 5 .\n2.2\nWhy Diffusion Model and Its History\nDiffusion models are a class of probability-based generative\nmodels. They are called after the mathematical process of\ndiffusion, which is commonly used to describe phenomena\nsuch as particle movement in a gas or liquid [1]. In detail,\nthe concept of diffusion models first appeared in statistical\nphysics, used to describe the process of particles moving\nfrom areas of high concentration to areas of low concentra-\ntion [10]. Early diffusion models were primarily concerned\nwith accurately simulating the random diffusion behavior\nin the generation process.\nOne of the key breakthroughs in diffusion models oc-\ncurred in 2015, when researchers proposed a method that\ncombines variational inference to effectively train these\nmodels [1]. Since then, the field has experienced rapid\ndevelopment, especially in the area of high-resolution image\ngeneration [7]. Since 2020, diffusion models have begun to\nshow their potential in more fields, such as text2image, mu-\nsic generation, and speech synthesis [32], [90], [112]. These\nadvances are due to the optimization of model structures,\nimprovements in training methods, and increased computa-\ntional resources.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nIn addition to the field of application, in terms of theory,\nresearchers began to explore how to generate data with\nspecific features by controlling the reverse process. This\nstudy direction finally resulted in the development of dif-\nfusion models capable of producing high-quality, multidi-\nmensional data. In updated computer algorithms, it can be\nrepresented as the process of gradually modifying the data\ndistribution, progressively injecting noise until it matches\nthe target distribution, in order to obtain high-quality and\nrealistic synthetic data samples [2].\nRecently, more and more researchers and engineers are\nnow focusing their perspectives on the diffusion model,\nand it has become one of the first choices for generating\nmodels [115]. Diffusion models excel at generating high-\nquality, complex sequences, including time series and spatial-\ntemporal data, with detailed coherence by gradually re-\nmoving noise. They offer strong control over generations,\nallowing fine-tuning based on the conditions [114]. These\nmodels are flexible and adaptable across various data types\nand modalities, robust against errors with a gradual noise\nreduction mechanism, and capable of exploring data diver-\nsity for creative outputs. Moreover, they can be integrated with\nother model types, like autoencoders, to enhance generation\nquality and control [115].\n2.3\nTypical Diffusion Models\nTypically, the training process includes two steps: the for-\nward process (diffusion) and the reverse process (denoising).\nDiffusion models start with a noise distribution, which is\ngradually altered through a series of steps. In the forward\nprocess, the model incrementally adds noise to the original\ndata over multiple steps until the data turns into pure\nrandom noise. This process is usually Markovian, meaning\nthat each step depends only on the preceding one. Then the\nreverse process takes place, involving learning to remove\nthe noise from the data, essentially reversing the forward\nprocess. By training the model to remove the noise added\nduring the diffusion process, the model learns to generate\nsamples from the same distribution as the training data.\nThe entire training process involves optimizing the model\nto denoise effectively. This is typically done using a loss\nfunction that encourages the model to produce samples that\nare close to the true data distribution [3].\nThe\ncurrently\ncommon\nframeworks\nfor\ndiffusion\nmodels include denoised diffusion probabilistic models\n(DDPMs) [1], [2], score-based stochastic differential equa-\ntions (Score SDEs) [4], [6], conditional diffusion models [7],\n[8], [9], etc. Following, we will introduce the subclasses of\nthe diffusion model through theoretical formula derivation.\n2.3.1\nDenoised Diffusion Probabilistic Models (DDPMs)\nDenoised diffusion probabilistic models are built around a\nwell-defined probabilistic process via dual Markov chains\nthat consist of two parts: a diffusion (or forward) process\nthat gradually transforms the data into noise with pre-\ndetermined noise, such as Gaussian noise, and a denoising\n(or reverse) process that attempts to recover the original data\nby deep neural networks.\nForward (Diffusion) Process. Given a data distribution\nq(x) and sample an initial clean data x0 \u223cq(x0) from\nit. The subsequent forward diffusion process incrementally\nadulterates the initial data distribution by superimposing\nGaussian noise, and finally progresses towards convergence\nwith the standard Gaussian distribution. In the diffusion\nprocess up to step K, a sequence of distributed latent data\nx1, x2, \u00b7 \u00b7 \u00b7 , xK materializes. The diffusion process can be\ndefined as a Markov chain transforms xk\u22121 to xk with a\ndiffusion transition kernel:\nq(xk|xk\u22121) := N(xk;\np\n1 \u2212\u03b2kxk\u22121, \u03b2kI),\n(1)\nfor \u2200k \u2208{1, \u00b7 \u00b7 \u00b7 , K} with the size of the controlling steps\n\u03b2k \u2208(0, 1), the identity matrix I, and a Gaussian distribu-\ntion N(x; \u00b5, \u03c3) of x with the mean \u00b5 and the covariance\n\u03c3. According to the properties of the Gaussian kernel, it is\nfeasible to get xk directly from x0 by equation 1, and collect\nnoise samples straight from the original input x0 for any\nstep, that is,\nq(xk|x0) :=\nK\nY\nk=1\nq(xk|xk\u22121) := N(xk; \u221a\u00af\n\u03b1kx0,\n\u221a\n1 \u2212\u00af\n\u03b1kI),\n(2)\nwhere \u03b1k := 1 \u2212\u03b2k, and \u00af\n\u03b1k :=\nK\nY\ni=1\n\u03b1i.\n(3)\nTherefore, xk = \u221a\u00af\n\u03b1kx0 + \u221a1 \u2212\u00af\n\u03b1k\u03f5 with Gaussian noise\n\u03f5 \u223cN(0, I). Typically, it is designed \u00af\n\u03b1k \u22480, s.t., q(xk) :=\nR q(xK|x0)q(x0)dx0 \u2248N(xk; 0, I), i.e., the backward chain\ncan begin with any Gaussian noise. Overall, the forward\nprocess gradually injects noise into the data until all struc-\ntures have disappeared.\nReverse (Denoising) Process. The reverse process per-\nforms the denoising task at each step with a series of Markov\nchains until the damaged original data is reconstructed.\nSpecifically, the series of reverse Markov chains start with\na distribution p(xK) = N(xK; 0, I) and a learnable kernel\np\u03b8(xk\u22121|xk) to generate p\u03b8(x0). The learnable Gaussian\ntransition kernels p\u03b8 can be represented as:\np\u03b8(xk\u22121|xk) := N(xk\u22121; \u00b5\u03b8(xk, k), \u03c3\u03b8(xk, k)I),\n(4)\nwhere the mean \u00b5\u03b8(\u00b7) and variance \u03c3\u03b8(\u00b7) are the model\nlearnable parameters. The model tries to learn the data\ndistribution by the model distribution p\u03b8(x0) during the\nreverse denoising process.\nTraining. In order to approximate the real data distribu-\ntion, the diffusion model is trained to minimize variational\nconstraints on the negative log-likelihood (NLL):\nE [\u2212log p\u03b8 (x0)] \u2264Eq\n\u0014\n\u2212log\np\u03b8 (x0:K)\nq (x1:K | x0)\n\u0015\n= Eq\n\uf8ee\n\uf8f0\u2212log p (xK) \u2212\nX\nk\u22651\nlog p\u03b8 (xk\u22121 | xk)\nq (xk | xk\u22121)\n\uf8f9\n\uf8fb\n=: L.\n(5)\nIt is equivalent to Kullback\u2013Leibler divergence (KL diver-\ngence) format as mentioned in the DDPM paper [2] with\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nthree parts: the prior loss LK, the divergence of the for-\nwarding step and the corresponding reversing step Lk\u22121,\nand the reconstruction loss L0:\nL :=Eq[DKL (q (xK | x0) \u2225p (xK))\n|\n{z\n}\nLK\n+\nX\nk>1\nDKL (q (xk\u22121 | xk, x0) \u2225p\u03b8 (xk\u22121 | xk))\n|\n{z\n}\nLk\u22121\n\u2212log p\u03b8 (x0 | x1)\n|\n{z\n}\nL0\n].\n(6)\nEspecially, to minimize the NLL, we can only train the\ndivergence loss between two steps Lk\u22121, and using Baye\u2019s\nrule to parameterize the posterior q (xk\u22121 | xk, x0), that is:\nq (xk\u22121 | xk, x0) = N\n\u0010\nxk\u22121; \u02dc\u00b5k (xk, x0) , \u02dc\u03b2kI\n\u0011\n,\n(7)\n\u02dc\u00b5t(xk, x0) :=\n\u221a\u00af\u03b1k\u22121\u03b2k\n1 \u2212\u00af\u03b1k\nx0 +\n\u221a\u03b1k(1 \u2212\u00af\u03b1k\u22121)\n1 \u2212\u00af\u03b1k\nxk ,\n(8)\n\u02dc\u03b2k := 1 \u2212\u00af\u03b1k\u22121\n1 \u2212\u00af\u03b1k\n\u03b2k.\n(9)\nwhere \u03b1k is 1 \u2212\u03b2k and \u00af\u03b1k indicates QK\nk=1 \u03b1k. Lk\u22121 can be\nequated to the expected value of the \u21132-loss between the two\nmean coefficients:\nLk\u22121 = Eq\n\u0014 1\n2\u03c32\nk\n\u2225\u02dc\u00b5k (xk, x0) \u2212\u00b5\u03b8 (xk, k)\u22252\n\u0015\n+ C.\n(10)\nHo et al. [2] emphasize that, rather than parameterizing the\nmean \u00b5\u03b8(xk, k), predicting the noise vector at each time\nstep in the forward process by parameterizing \u03f5\u03b8(xk, k) for\nsimplification:\nEk\u223cU(1,K),x0\u223cq(x0),\u03f5\u223cN (0,I)\n\u0014\n\u03bb(k) \u2225\u03f5 \u2212\u03f5\u03b8(xk, k)\u22252\n\u0015\n,\n(11)\nwhere \u03bb(k) =\n\u03b2k\n2\n2\u03c3K 2\u03b1k(1\u2212\u00af\u03b1k) is a weight that changes noise\nscale, and \u03f5\u03b8 is a model for Gaussian noise prediction. After\ntraining using the above loss function, \u03f5\u03b8 will be used in the\nreverse process of ancestor sampling.\nInference (Sampling). Given the noisy data xK and\nstarting with step K denoising, the final time series is\ngenerated through the equation:\np\u03b8(xk\u22121|xk) = N(xk\u22121; \u00b5\u03b8(xk, k), \u03c32\n\u03b8(xk, k)I)\n\u223c\n1\n\u221a\u03b1k\n(xk \u2212\n\u03b2k\n\u221a1 \u2212\u03b1k\n\u03f5\u03b8(xk, k)) + \u03c3\u03b8(xk, k)z,\n(12)\nwhere z \u223cN(0, I), also \u03b2k \u2248\u03c32\n\u03b8(xk, k) in practice.\n2.3.2\nScore SDE Formulation\nDDPM achieved a set of discrete steps in the forward\nprocessing, so it has some limitations about training designs.\nScore SDE further generalizes DDPM\u2019s discrete system to a\ncontinuous framework based on the stochastical differential\nequation [6]. Here we use T instead of the step size k in\nDDPM.\nForward Process. The corresponding continuous diffu-\nsion process can be represented using It\u02c6o SDE [291], includ-\ning a mean shift and a Brownian motion (standard Wiener\nprocess) as follows:\ndx = f(x, t)dt + g(t)dw, t \u2208[0, T],\n(13)\nwhere f(\u00b7, t) represents the drift coefficient for the stochastic\nprocess x(t), and g(\u00b7) is the diffusion coefficient linked with\nthe Brownian motion w.\nSimilar with DDPM, x0 and xT represent sequence from\nthe clean distribution p0 = N(x0; 0, I) and the standard\nGaussian distribution pT = N(xT ; 0, I), respectively. And\nthe corresponding SDE is:\ndx = \u22121\n2\u03b2(t)x dt +\nq\n\u03b2(t)dw.\n(14)\nReverse Process. The new samples can be synthesized\nfrom the known prior distribution pT by solving the reverse-\ntime SDE [174]:\ndx =\n\u0002f(x, t) \u2212g2(t)\u2207x log pt(x)\n\u0003 dt + g(t)d \u00afw,\n(15)\nWhere\n\u00afw is a Brownian motion with reversed time\nflows [148]. The solution to the reverse-time SDE is approx-\nimated by a time-dependent neural network s\u03b8(x, t) to a\nscore function \u2207x log pt(x). The solution of the forward SDE\nequation is that:\nL := Et{\u03bb(t)Ex0Eq(xt|x0)[\u2225s\u03b8(xtt) \u2212\u2207xt log p(xt|x0)\u22252\n2]},\n(16)\nwhere x0 is sampled from distribution p0 and \u03bb(t) is the\npositive weighting function. This method circumvents the\ndirect approximation of the computationally infeasible score\nfunction by estimating the transition probability that fol-\nlows a Gaussian distribution during the forward diffusion\nprocess [6].\nBesides, there are some simplified explanations of\nreverse-time SDE solvers. Using these techniques, samples\ncan be generated after training using various methods.\n\u2022 Euler-Maruyama\n(EM)\nMethod\n[147]:\nSolves\nthe\nreverse-time SDE through a simple discretization tech-\nnique, replacing dx with \u25b3t and d \u00afw with Gaussian\nnoise z.\n\u2022 Prediction-Correction (PC) Method: Operates in a se-\nquential manner, alternating between predictor and cor-\nrector steps. The predictor can use any numerical solver,\nsuch as the EM method, for the reverse-time SDE, while\nthe corrector can be any score-based Markov Chain\nMonte Carlo (MCMC) method.\n\u2022 Probability Flow ODE Method [6]: Reformulates the\nforward SDE into an ODE that maintains the same\nmarginal probability density pt as the SDE. Sampling by\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nsolving this reverse-time ODE is equivalent to solving\nthe time-reversed SDE. There are also some advanced\nODE solvers to speed up the process [145], [146].\n2.3.3\nConditional Diffusion Models\nIn the previous sections, we discuss the DDPM and Score\nSDE from an unconditional view. They generate data with-\nout any explicit conditions or guidance. The model learns to\nproduce outputs from the learned distribution of the input\ndata. The general diffusion models are capable of generating\ndata samples not just from an unconditional distribution\np0, but also from a conditional distribution p0(x|c) when\ngiven a condition c. This condition could be class labels\nor features related to the input data x [7]. During training,\nthe score network s\u03b8(x, t, c) takes condition c as an input.\nAdditionally, there are specific sampling algorithms de-\nsigned for conditional generation, such as label-based con-\nditions [8], label-free conditions [9], and further distillation-\nbased guidance [8], self guidance [150], [171], textual based\nguidance [149], [172], graph-based guidance [173], physical\nbased guidance [163], task-based guidance [165], etc. These\nconditional mechanisms are more conducive to the gener-\nation of application-specific fields by using the control of\nother information to generate results [115].\nIn detail, sampling under labels and classifiers\u2019 condi-\ntions involves using gradient guidance at each step, which\ntypically requires an additional classifier with encoder archi-\ntecture (e.g., U-Net [151] and Transformer [152]) to generate\ncondition gradients for specific labels [8]. These labels are\nflexible, and can be textual or categorical, binary, or based\non extracted features [8], [146], [153], [154], [155], [157],\n[158], [167], [169], [201]. Correspondingly, sampling under\nunlabeled conditions relies solely on self-information for\nguidance [150], [159], [164]. Compared to the high accuracy\nof the labeled conditional diffusion model, the unlabeled\none has advantages in generating innovative and diverse\ndata. Therefore, unlabelled models are more suitable for\nexploratory and creative application scenarios [160], [161].\nFurthermore, there are also more conditional methods being\nproposed, which use information about the data itself [164],\nother modalities [153], [263], other representations [163],\nand other knowledge [168] as conditions to guide the\ndiffusion model for generation. Currently, the condition-\nbased diffusion model is also the most common method in\nvarious application scenarios due to its highly specific and\ncontrolled outputs [162], [199].\n2.3.4\nImprovements with Diffusion Model and Its Variants\nWhile diffusion models have generally yielded satisfactory\nresults in generation and various tasks, practical appli-\ncations reveal certain limitations. These include the slow\niterative sampling process and the computational complex-\nity arising from high-dimensional input, which affects effi-\nciency. Furthermore, there are concerns regarding their gen-\neralization performance across different distributions and\nthe challenges in integrating them with other generative\nmodels. Next, we will explore the variants of diffusion mod-\nels from efficiency and performance improvement, as shown\nin Fig. 5, highlighting the modifications and optimizations\nthey bring in comparison to original diffusion models.\nImproved Diffusion Model\nEfficiency \nEnhancement\nPerformance \nEnhancement\nForward Processes Improvement\nReverse Processes Improvement\nIntegration with Other Models\nScheduler Functions\nModel Architecture\nTraining Schedules\nData Bridging\nConditional and Guidance Strategies\nOpenAI: DALL-E series, GLIDE, SORA\nStability AI: Stable Diffusion, Stable series\nAcademia: ControlNet, DiT, SiT\nFig. 5: Categorization of improvements with diffusion\nmodel and its variants.\nEfficiency Enhancement. (1) Forward Processes Im-\nprovement shifted towards leveraging other physical phe-\nnomena to enhance model efficiency and robustness. For ex-\nample, inspired by electric field dynamics, PFGM [211] and\nits extension PFGM++ [212] [213] were proposed. They are\nguided distributions along electric field lines and employing\naugmented dimensions for improved performance. More-\nover, innovations like Cold Diffusion explore the use of im-\nage transformations as a forward process [214]. Also, some\nimproved Gaussian perturbation kernel methods were pre-\nsented [215]. (2) Reverse Processes Improvement usually\nreduces the number of generation steps or uses lightweight\nmodels to enhance efficiency. Training-free sampling meth-\nods offer a paradigm where the acceleration of the sampling\nprocess does not necessitate retraining and optimize the\ntrajectory from noise to data distributions, such as ODE-\nbased methods DDIM [12], its extensions gDDIM [216],\nPNDM [145], EDM [215], DEIS [217], DPM-Solver [146], etc,\nand SDE-based methods through advanced techniques like\nrestart sampling [218], [219]. On the other hand, some meth-\nods apply knowledge distillation to create an efficient and\nsmaller network by transferring insights from large, intricate\nteacher models to simpler student ones [220], [221]. (3) In-\ntegration with Other Models is a common trick to enhance\nefficiency, such as combining with VAE [224], [225] and\nGAN [222], [223]. Additionally, there are methods that use\nthe latent space as an input for the diffusion model, which\ncan improve efficiency by reducing the input dimensional-\nity [7], [225], [227], [228]. (4) Scheduler Functions improve\nefficiency by optimizing the reverse process, enabling faster\nconvergence and reducing the number of required itera-\ntions. Common improved schedulers and their algorithms\ninclude CMS [221], DDIM [12], IDDPM [3], DEIS [217],\nDPM-Solvers [146], [229], Euler and Heun scheduler [215],\nLCM [230], RePaintScheduler [231], TCD [232], UniPC [233],\nand VQD [234].\nPerformance Enhancement. (1) Model Architecture.\nTraditional DDPM uses the UNet architecture for its ef-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nficiency in reducing computational costs by leveraging a\ndetailed feature space. UNet-based Innovations included\nnormalization [220], [236], and different attentions with\nposition encoding\n[217], [235], [237]. Recently, to im-\nprove performance, transformer-based architecture is being\nused more often [152]. Reverse noise prediction incorpo-\nrates adaptations to accommodate temporal and conditional\ninputs. Common methods include ViT [239], SwinTrans-\nformer [240], [241], DiT [242], and SiT [243]. (2) Training\nSchedules involve advanced decoding strategies, mainly\nclassified into optimizing the diffusion stages and inno-\nvative projection methods. Optimization strategies focus\non adjusting the diffusion steps to fine-tune the model\nperformance [222], [224]. Meanwhile, projection techniques\ninvestigate various diffusion processes, like using linear\ndistortions and different kernels, to increase model versa-\ntility [244], [245]. Moreover, innovations in diffusion mod-\nels tweak noise adjustment for better output and quicker\nalignment [3], [235], [246]. Also, there are some optimization\nmethods focusing on loss and matching [247], [248]. (3) Data\nBridging is designed to address the limitations of gener-\nating arbitrary Gaussian distributions with complex distri-\nbutions. Research has innovated with SDE/ODE principles\nto bridge gaps between distributions. Techniques like \u03b1-\nblending create deterministic paths, using diffusion models\nfor Gaussian-related cases [249]. Rectified Flow and other\nmethods introduce optimizations and explore ODE creation\nbetween distributions [226], [250]. Additionally, leveraging\nthe Schr\u00a8odinger Bridge concept or Gaussian distributions\nas intermediaries offers new avenues in distribution trans-\nportation [251], [252]. (4) Conditional and Guidance Strate-\ngies are crucial for diffusion models, directing generation\nand enhancing output relevance and quality in response to\nspecific conditions. Related works have been discussed in\nSec. 2.3.3.\nCurrently, research institutions, such as OpenAI (DALL-\nE series, GLIDE, SORA) and Stability AI (Stable Diffusion),\nhave launched numerous outstanding diffusion models,\nwhich have exploded in popularity in both academic re-\nsearch and applications. For instance, Stable Diffusion is\na state-of-the-art diffusion model known for its ability to\ngenerate high-quality, detailed images from textual descrip-\ntions, offering a powerful tool for creative and generative\ntasks [7]. ControlNet is a framework designed to enhance\ncontrol over the attributes and structure of generated im-\nages, enabling precise manipulation of visual elements in\ngenerative models [199]. The consistency model achieves\nstate-of-the-art performance without the slow iterative pro-\ncess of traditional diffusion models and offers capabilities\nlike zero-shot editing and efficient training options [221].\n2.4\nDiffusion Model vs. Other Generative Models\nIn addition to the diffusion model, there are many clas-\nsical generative models, we take the most widely used\nvariational autoencoders (VAEs), generative adversarial net-\nworks (GANs), and flow-based generative models as ex-\namples to introduce them and analyse their advantages,\ndisadvantages and differences with the diffusion model. The\nworkflow of these models is shown in Fig. 6.\nVAE is a probabilistic generative model that encodes\ninput data into a latent space z and decodes from that latent\nDiscriminator\n\ud835\udc37(\ud835\udc65, \ud835\udc65\u2032)\n0/1\n\ud835\udc65\n\ud835\udc65\u2032\nGenerator\n\ud835\udc3a(\ud835\udc67)\n\ud835\udc67\n\ud835\udc65\u2032\nEncoder\n\ud835\udc5e!(\ud835\udc67|\ud835\udc65)\n\ud835\udc65\nDecoder\n\ud835\udc5d\"(\ud835\udc65|\ud835\udc67)\n\ud835\udc67\n\ud835\udc65\u2032\n(a) VAE\nForward flow\n\ud835\udc53(\ud835\udc65)\n\ud835\udc65\nInverse flow\n\ud835\udc53#$(\ud835\udc67)\n\ud835\udc67\n\ud835\udc65\u2032\n(b) GAN\n(c) Flow\nForward\n\ud835\udc5e(\ud835\udc65%|\ud835\udc65%#$)\n\ud835\udc65\nReverse\n\ud835\udc5d\"(\ud835\udc65%#$|\ud835\udc65%)\n\ud835\udc65!\n\ud835\udc65\u2032\n(d) Diffusion\n\ud835\udc3e\u22121 steps\n\ud835\udc3e\u22121 steps\nFig. 6: Different types of generative models. (a) VAE is a\nprobabilistic generative model based on encoding data into\na latent space and decoding from this space for genera-\ntion. (b) GAN generates new data similar to the training\ndata by pitting two neural networks, a generator, and a\ndiscriminator, against each other in a game-like scenario.\n(c) Flow-based model generates data using invertible trans-\nformations, enabling precise computation of the probability\ndensity function of the data. (d) The diffusion model learns\nthe underlying distribution of data by gradually introducing\nand then removing noise, enabling the generation of high-\nquality and diverse data samples.\nspace to generate data [186], as shown in Fig. 6(a). VAEs\naim to maximize the lower bound of the log-likelihood of\nthe data, known as the Evidence Lower Bound (ELBO).\nELBO = Eq\u03d5(z|x)[logp\u03b8(x|z)] + KL[q\u03d5(z|x)||p\u03b8(z)],\n(17)\nwhere q\u03d5(z|x) is the posterior distribution of the latent\nspace output by the encoder and p\u03b8(x|z) is the conditional\ndistribution of the data given the latent variable z.\nGAN consists of a generator G(z) and a discriminator\nD(x, x\u2032), as shown in Fig. 6(b). The generator tries to pro-\nduce samples as close to the real data x as possible, while\nthe discriminator tries to distinguish between real data x\nand generated data x\u2032. The training of GANs involves a\nzero-sum game, continually optimizing both the generator\nand the discriminator [187]. The objective function can be\nsummarised as follows:\nminGmaxDV (D, G) = Ex\u223cpdata(x)[logD(x, x\u2032)]\n+Ez\u223cpz(z)[log(1 \u2212D((G(z)))].\n(18)\nFlow-based generative model (normalizing flows) trans-\nform data into a simpler distribution (e.g., Gaussian)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nthrough a sequence of reversible transformations, as shown\nin Fig. 6(c), ensuring precise likelihood evaluation due to\nthe transformations being invertible [188]. The flow-based\ntransformation formula is:\nz = f(x);\nx\u2032 = f \u22121(z),\n(19)\nwhere x and z are the points in the data space and latent\nspace, respectively. f is an invertible function. The log-\nlikelihood of the transformation is given by:\nlogp(x) = logpz(f(x)) + log\n\f\f\f\fdet( df\ndx)\n\f\f\f\f .\n(20)\nActually, different generative models each have their\nown unique strengths and limitations, making them suitable\nfor specific applications. VAE is appreciated for its simplic-\nity, stability, and clear theoretical underpinnings, yet it tends\nto produce lower-quality data and has limited expressive-\nness in its latent spaces. GAN is renowned for having pow-\nerful generative capabilities but is notoriously difficult to\ntrain and prone to instability and mode collapse. The flow-\nbased model provides precise likelihood estimation and fa-\ncilitates high-quality generation, but it demands significant\ncomputational resources and involves complex model de-\nsigns. Diffusion models excel in generating high-quality and\ndetailed output, offering flexibility and solid probabilistic\nfoundations, but suffer from long training times and high\ncomputational costs. However, many variants of diffusion\nmodels and improvements have mitigated these issues, as\ndescribed in Sec. 2.3.4. Currently, the diffusion model is the\nmost popular generative model.\n3\nOVERVIEW AND CATEGORIZATION\nThis section presents an overview and classification of\ndiffusion models for addressing challenges in time series\nand spatio-temporal data analysis. Our survey organizes\nthe discussion along four primary dimensions: categories\nof diffusion models, types of tasks, data modalities, and\npractical applications. A comprehensive summary of no-\ntable related works is depicted in Fig. 7 . We categorize the\nexisting literature into two primary groups: unconditioned\nand conditioned diffusion models, focusing on time series\nand spatio-temporal data.\nIn the unconditioned category, diffusion models operate\nin an unsupervised manner to generate data samples with-\nout the need for supervision signals. This setting represents\nthe foundational approach for analyzing time series and\nspatio-temporal data. Within this category, literature can be\nfurther divided into probability-based and score-based dif-\nfusion models. Examples include denoised diffusion prob-\nabilistic models (DDPMs) [2] and score-based stochastic\ndifferential equations (Score SDEs) [4], [6], as introduced in\nSec. 2 . Research in this category is broadly organized into\ntwo task groups: predictive and generative tasks. Predictive\ntasks typically involve forecasting and anomaly detection,\nleveraging historical data and patterns to anticipate current\nand/or future events. Generative tasks, conversely, focus on\nidentifying patterns within extensive datasets to generate\nnew content, such as time series imputation and augmenta-\ntion. Methods are developed for both primary data modal-\nities: time series and spatio-temporal data, catering to a\nwide range of applications across various sectors, including\nhealthcare, energy, climate, traffic, and more.\nIn the conditioned category, diffusion models are tai-\nlored for conditioned analysis of time series and spatio-\ntemporal data. Empirical studies have shown that con-\nditional generative models, which utilize data labels, are\neasier to train and yield superior performance compared to\ntheir unconditional counterparts [75]. In this context, labels\n(a.k.a. conditions) often derive from various sources, such as\nextracted short-term trends [34] and urban flow maps [35],\nto enhance model inferences. This category embraces both\nprobability-based and score-based diffusion models for pre-\ndictive and generative tasks, offering a fresher perspective\non leveraging diffusion models to tackle practical challenges\nin time series and spatio-temporal data analysis under spe-\ncific constraints.\nBuilding on the foundational understanding of model\ncategories, task types, data modalities, and application do-\nmains, we delve deeper into the exploration of diffusion\nmodels for time series and spatio-temporal data analysis\nacross the following sections. Each section is designed to\nunpack the complexities and nuances inherent in the ap-\nplication of diffusion models, providing a comprehensive\noverview from multiple perspectives. In Sec. 4 , we ex-\nplore diffusion model landscapes, highlighting distinctions\nbetween unconditioned and conditioned approaches and\ntheir implications. Sec. 5\nanalyzes tasks from predictive\nand generative viewpoints, detailing specific functions such\nas forecasting, generation, anomaly detection, and data im-\nputation. Sec. 6\nexamines data modalities, differentiating\nbetween time series and spatio-temporal data to outline\nmodel challenges and applicability. Lastly, Sec. 7\nextends\nthe discussion to application fields, demonstrating diffusion\nmodels\u2019 utility across sectors like healthcare, traffic, se-\nquential recommendation, climate, energy, and audio. This\nstructured exploration aims to equip readers with an in-\ndepth understanding of the potential and current state of\ndiffusion models for addressing complex time series and\nspatio-temporal data challenges.\n4\nMODEL PERSPECTIVE\nIn this section, we will analyze how to use diffusion models\nfor time series and spatio-temporal data from a model\nperspective. Specifically, we will focus on standard diffusion\nmodels (DDPM and score SDE) and improved diffusion\nmodels (conditional diffusion model, LDM, DDIM, and\nothers).\n4.1\nStandard Diffusion Model\nStandard diffusion models include probability-based model,\ni.e., DDPM and score-based model, score SDE, which have\nbeen described in detail in Sec. 2.3.1 and Sec. 2.3.2, respec-\ntively. Currently, diffusion models based on these two are\nalso the most common methods for time series and spatio-\ntemporal data analysis.\n4.1.1\nProbability-Based Model\nBased on the probabilistic-based standard model, we mainly\nintroduce DDPM there, which uses a discrete framework,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nTABLE 1: Summary and main papers of the diffusion models for time series and spatio-temporal data modeling. Red\nindicates univariate time series , blue-violet is multivariate time series , and yellow shows spatio-temporal data .\nMethod\nData\nModel\nTask\nApplication\nInstitute\nVenue\nYear\nWaveGrad [20]\nUnivariate\nDDPM\nGeneration\nAudio\nGoogle\nICLR\n2021\nDiffWave [91]\nUnivariate\nDDPM\nGeneration\nAudio\nUCSD\nICLR\n2021\nD-Va [47]\nUnivariate\nDDPM\nForecasting\nFinance\nNUS\nCIKM\n2023\nDiffLoad [28]\nUnivariate\nDDPM\nForecasting\nElectricity\nHKU\nArXiv\n2023\nTDSTF [289]\nUnivariate\nDDPM\nForecasting\nHealthcare\nU of A\nArXiv\n2023\nDiffuASR [23]\nUnivariate\nDDPM\nGeneration\nRecommendation\nXJTU\nCIKM\n2023\nDiffRec [132]\nUnivariate\nDDPM\nGeneration\nRecommendation\nNUS\nSIGIR\n2023\nRecFusion [130]\nUnivariate\nDDPM\nGeneration\nRecommendation\nUvA\nArXiv\n2023\nDiffRec [134]\nUnivariate\nDDPM\nGeneration\nRecommendation\nSCU\nArXiv\n2023\nDiffuRec [95]\nUnivariate\nDDPM\nGeneration\nRecommendation\nWHU\nArXiv\n2023\nPDRec [135]\nUnivariate\nDDPM\nGeneration\nRecommendation\nSDU\nArXiv\n2024\nTimeGrad [13]\nMultivariate\nDDPM\nForecasting\nGeneral\nZalando\nICML\n2021\nCARD [98]\nMultivariate\nDDPM\nClassification\nGeneral\nUT-Austin\nNeurIPS\n2022\nBVAE [86]\nMultivariate\nDDPM\nForecasting\nGeneral\nBaidu\nNeurIPS\n2023\nTSDiff [164]\nMultivariate\nDDPM\nForecasting\nGeneral\nAmazon\nNeurIPS\n2023\nKuo et al. [19]\nMultivariate\nDDPM\nGeneration\nHealthcare\nUNSW\nArXiv\n2023\nTS-Diffusion [278]\nMultivariate\nDDPM\nGeneration\nGeneral\nCambridge\nArXiv\n2023\nSSSD [275]\nMultivariate\nDDPM\nImputation\nGeneral\nOldenburg\nTMLR\n2023\nDA-TASWDM [276]\nMultivariate\nDDPM\nImputation\nHealthcare\nHKBU\nCIKM\n2023\nPintilie et al. [259]\nMultivariate\nDDPM\nAnomaly Detection\nGeneral\nBucharest\nICDM\n2023\nDDMT [99]\nMultivariate\nDDPM\nAnomaly Detection\nGeneral\nFNU\nArXiv\n2023\nD3A-TS [101]\nMultivariate\nDDPM\nImputation&Generation\nGeneral\nSeville\nArXiv\n2023\nTSDM [103]\nMultivariate\nDDPM\nGeneration\nEnvironment\nHIT\nArXiv\n2023\nTimeDDPM [102]\nMultivariate\nDDPM\nGeneration\nEnvironment\nZUST\nIEEE Sens. J.\n2023\nDiffEEG [18]\nMultivariate\nDDPM\nForecasting\nHealthcare\nUSTC\nArXiv\n2023\nDiff-E [89]\nMultivariate\nDDPM\nClassification\nHealthcare\nKorea University\nInterspeech\n2023\nTosato et al. [119]\nMultivariate\nDDPM\nClassification\nHealthcare\nTilburg\nSynapsium\n2023\nDiffCharge [96]\nMultivariate\nDDPM\nGeneration\nElectricity\nHKUST\nArXiv\n2023\nYang et al. [238]\nMultivariate\nDDPM\nImputation&Anomaly Detection\nAIOps\nMicrosoft\nESEC/FSE\n2023\nAnoDDPM [46]\nMultivariate\nDDPM\nAnomaly Detection\nGeneral\nBeihang\nIEEE Sens. J.\n2024\nDiffShape [277]\nMultivariate\nDDPM\nClassification\nGeneral\nSCUT\nAAAI\n2024\nSTPP [70]\nSpatio-temporal\nDDPM\nForecasting\nGeneral Event\nTHU\nKDD\n2023\nERDiff [25]\nSpatio-temporal\nDDPM\nAlignment\nGeneral\nGIT\nNeurIPS\n2023\nDVGNN [59]\nSpatio-temporal\nDDPM\nForecasting\nTransportation\nHalmstad\nArXiv\n2023\nYun et al. [69]\nSpatio-temporal\nDDPM\nImputation\nGeneral\nKAIST\nOpenReview\n2023\nDiffTAD [83]\nSpatio-temporal\nDDPM\nAnomaly Detection\nTransportation\nXidian\nKNOWL-BASED SYST\n2024\nSpecSTG [88]\nSpatio-temporal\nDDPM\nForecasting\nTransportation\nUSYD\nArXiv\n2024\nDST-DDPM [78]\nSpatio-temporal\nDDPM\nForecasting\nEnvironment\nHKUST\nENVIRON RES\n2024\nWestny et al. [80]\nSpatio-temporal\nDDPM\nForecasting\nTransportation\nLiU\nArXiv\n2024\nUTD-PTP [79]\nSpatio-temporal\nDDPM\nForecasting\nTransportation\nBIT\nIEEE Sens. J.\n2024\nSGMSE [284]\nUnivariate\nScore-based\nGeneration\nAudio\nUHH\nInterspeech\n2022\nDeScoD-ECG [57]\nUnivariate\nScore-based\nDenoising\nHealthcare\nU of A\nIEEE J BIOMED HEALTH\n2023\nScoreGrad [71]\nMultivariate\nScore-based\nForecasting\nGeneral\nBIT\nArXiv\n2021\nBilo et al. [104]\nMultivariate\nScore-based\nForecasting\nGeneral\nTUM\nICML\n2023\nStoRM [105]\nMultivariate\nScore-based\nDenoising&Generation\nAudio\nUHH\nIEEE-ACM T AUDIO SPE\n2023\nLay et al. [178]\nMultivariate\nScore-based\nDenoising&Generation\nAudio\nUHH\nInterspeech\n2023\nRisk-sensitive SDE [280]\nMultivariate\nScore-based\nGeneration\nGeneral\nCambridge\nArXiv\n2024\nTimeADDM [282]\nMultivariate\nScore-based\nAnomaly Detection\nGeneral\nHFUT\nICASSP\n2024\nSasdim [72]\nSpatio-temporal\nScore-based\nImputation\nGeneral\nCSU\nArXiv\n2023\nDyffusion [285]\nSpatio-temporal\nScore-based\nForecasting\nGeneral\nUCSD\nNeurIPS\n2023\nLu et al. [175]\nUnivariate\nConditional\nGeneration\nAudio\nCMU\nICASSP\n2022\nWang et al. [142]\nUnivariate\nConditional\nGeneration\nElectricity\nU of Macau\nArXiv\n2023\nPulseDiff [144]\nUnivariate\nConditional\nImputation\nHealthcare\nIC\nArXiv\n2023\nDCDR [131]\nUnivariate\nConditional\nGeneration\nRecommendation\nKuaishou\nArXiv\n2023\nWang et al. [36]\nUnivariate\nConditional\nGeneration\nRecommendation\nUIC\nArXiv\n2023\nGiffCF [38]\nUnivariate\nConditional\nGeneration\nRecommendation\nUSTC\nArXiv\n2023\nDR-DiffuSE [176]\nUnivariate\nConditional\nGeneration\nAudio\nUESTC\nAAAI\n2023\nDose [177]\nUnivariate\nConditional\nGeneration\nAudio\nUESTC\nNeurIPS\n2023\nCRA-DIFFUSE [179]\nUnivariate\nConditional\nGeneration\nAudio\nXJU\nICME\n2023\nDiffsFormer [52]\nUnivariate\nConditional\nImputation&Generation\nFinance\nUSTC\nArXiv\n2024\nDreamRec [167]\nUnivariate\nConditional\nGeneration\nRecommendation\nUSTC\nNeurIPS\n2024\nCSDI [56]\nMultivariate\nConditional\nImputation\nGeneral\nStanford\nNeurIPS\n2021\nSF-DM [169]\nMultivariate\nConditional\nClassification\nManufacture\nAalto\nUBICOMP\n2023\nTimeDiff [34]\nMultivariate\nConditional\nForecasting\nGeneral\nHKUST\nICML\n2023\nDiffAD [260]\nMultivariate\nConditional\nAnomaly Detection\nGeneral\nHENU\nKDD\n2023\nD3R [261]\nMultivariate\nConditional\nAnomaly Detection\nGeneral\nBUPT\nNeurIPS\n2023\nCLDM [107]\nMultivariate\nConditional\nGeneration\nEnergy\nNCEPU\nIEEE T SUSTAIN ENERG\n2023\nMIDM [262]\nMultivariate\nConditional\nImputation\nGeneral\nUSTC\nKDD\n2023\nMEDiC [110]\nMultivariate\nConditional\nImputation\nHealthcare\nIIT\nNeurIPS\n2023\nVGCDM [263]\nMultivariate\nConditional\nAnomaly Detection\nElectricity\nXJTU\nArXiv\n2023\nDiffPLF [143]\nMultivariate\nConditional\nForecasting\nElectricity\nHKUST(GZ)\nArXiv\n2024\nTime Weaver [182]\nMultivariate\nConditional\nGeneration\nGeneral\nUT-Austin\nArXiv\n2024\nFu et al. [266]\nMultivariate\nConditional\nGeneration\nElectricity\nNUS\nArXiv\n2024\nImDiffusion [257]\nMultivariate\nConditional\nAnomaly Detection\nGeneral\nPKU\nVLDB\n2024\nWang [264]\nMultivariate\nConditional\nForecasting\nBusiness\nUSTC\nICASSP\n2024\nDiffDA [26]\nMultivariate\nConditional\nForecasting\nClimate\nETH Zurich\nArXiv\n2024\nBioDiffusion [51]\nMultivariate\nConditional\nGeneration\nHealthcare\nTSU\nArXiv\n2024\nDiffSTOCK [41]\nMultivariate\nConditional\nForecasting\nFinance\nPurdue\nICASSP\n2024\nRF-Diffusion [44]\nMultivariate\nConditional\nGeneration\nNetwork\nTHU\nMobiCom\n2024\nKlein et al. [42]\nMultivariate\nConditional\nGeneration\nHealthcare\nRadboud\nArXiv\n2024\nDiffSTG [43]\nSpatio-temporal\nConditional\nForecasting\nGeneral\nBJTU\nSIGSPATIAL\n2023\nPriSTI [111]\nSpatio-temporal\nConditional\nImputation\nGeneral\nBeihang\nICDE\n2023\nDiffUFlow [35]\nSpatio-temporal\nConditional\nForecasting\nTransportation\nCSU\nCIKM\n2023\nDiffTraj [81]\nSpatio-temporal\nConditional\nGeneration\nTransportation\nSUSTech\nNeurIPS\n2023\nControlTraj [290]\nSpatio-temporal\nConditional\nGeneration\nTransportation\nHKUST(GZ)\narXiv\n2024\nUSTD [170]\nSpatio-temporal\nConditional\nForecasting\nGeneral\nNUS\nArXiv\n2023\nIDM [184]\nSpatio-temporal\nConditional\nForecasting\nTransportation\nZJU\nArXiv\n2024\nDiff-RNTraj [17]\nSpatio-temporal\nConditional\nGeneration\nTransportation\nBJTU\nArXiv\n2024\nEvans et al. [185]\nUnivariate\nLDM\nGeneration\nAudio\nStability AI\nArXiv\n2024\nLDT [253]\nMultivariate\nLDM\nForecasting\nGeneral\nNTU\nAAAI\n2024\nAristimunha et al. [128]\nMultivariate\nLDM\nGeneration\nHealthcare\nUPSaclay\nNeurIPS\n2023\nTSDM [269]\nMultivariate\nDDIM\nImputation&Anomaly Detection\nElectricity\nHUST\nArXiv\n2023\nLADM [265]\nSpatio-temporal\nLDM\nForecasting\nTransportation\nXJU\nIEEE T INSTRUM MEAS\n2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nDiffusion Models for\nTime Series and\nSpatio-Temporal Data\nUnconditional\nDiffusion Models\n(Sec. 4.1 )\nProbability-based\nDiffusion Models\n(Sec. 4.1.1)\nPredictive Tasks\n(Sec. 5.1 Sec. 5.4 )\nTime Series\n(Sec. 6.1 )\nHealthcare: DiffEEG [18] DiffECG [74]\nEnergy: Capel et al. [14]\nGeneral: TimeGrad [13] D3VAE [86]\nSpatio-Temporal Data\n(Sec. 6.2 )\nClimate: SwinRDM [16] SEEDS [84]\nTraffic: DVGNN [59] SpecSTG [88]\nOthers: DSTPP [70] DYffusion [29]\nGenerative Tasks\n(Sec. 5.2 Sec. 5.3 )\nTime Series\n(Sec. 6.1 )\nHealthcare: Kuo et al. [19] Diff-E [89]\nAudio: WaveGrad [20] DiffWave [91]\nRecommendation: DiffuASR [23] DiffuRec [95]\nOthers: TransFusion [24] DiffCharge [96]\nSpatio-Temporal Data\n(Sec. 6.2 )\nTraffic: TrajGDM [60]\nGeneral: Yun et al. [69] Wang et al. [25]\nOther Tasks\nTime Series\n(Sec. 6.1 )\nClassification: CARD [98]\nAnomaly Detection: DDMT [99]\nDenoising: Duan et al. [100]\nSpatio-Temporal Data\n(Sec. 6.2 )\nAlignment: ERDiff [25]\nAnomaly Detection: DiffTAD [97]\nImputation: Imputation as Inpainting [69]\nScore-based\nDiffusion Models\n(Sec. 4.1.2)\nPredictive Tasks\n(Sec. 5.1 Sec. 5.4 )\nTime Series\n(Sec. 6.1 )\nGeneral: ScoreGrad [71] Bilo\u02c7s et al. [104]\nSpatio-Temporal Data\n(Sec. 6.2 )\nClimate: Hatanaka et al. [27]\nGeneral: DYffusion [29]\nGenerative Tasks\n(Sec. 5.2 Sec. 5.3 )\nTime Series\n(Sec. 6.1 )\nAudio: SGMSE [32] StoRM [105]\nHealthcare: DeScoD-ECG [57]\nOthers: SaSDim [72] Crabb\u00b4e et al. [106]\nSpatio-Temporal Data\n(Sec. 6.2 )\nHealthcare: EHRDiff [30]\nConditional\nDiffusion Models\n(Sec. 4.2.1)\nProbability-based\nDiffusion Models\n(Sec. 4.2.1)\nPredictive Tasks\n(Sec. 5.1 Sec. 5.4 )\nTime Series\n(Sec. 6.1 )\nAIOps: Maat [37]\nManufacture: Zhang et al. [108] SCDDPM [181]\nEnergy: VGCDM [263] Dong at al. [107]\nRecommendation: Yang et al. [167]\nOthers: TimeDiff [34] Wang et al. [262]\nSpatio-Temporal Data\n(Sec. 6.2 )\nTraffic: DiffUFlow [35] DiffSTG [43]\nClimate: DiffESM [15] DiffSTG [43]\nGeneral: Hu et al. [170]\nGenerative Tasks\n(Sec. 5.2 Sec. 5.3 )\nTime Series\n(Sec. 6.1 )\nHealthcare: BioDiffusion [51] MEDiC [110]\nFinance: DiffsFormer [52]\nAudio: CDiffuSE [175] DOSE [177]\nOthers: Yan et al. [53] DiffTime [117]\nGeneral: Time Weaver [182] Shirzad at al. [183]\nSpatio-Temporal Data\n(Sec. 6.2 )\nTraffic: DiffTraj [81] ControlTraj [290]\nOthers: PriSTI [111]\nScore-based\nDiffusion Models\n(Sec. 4.2.1)\nGenerative Tasks\n(Sec. 5.2 Sec. 5.3 )\nTime Series\n(Sec. 6.1 )\nHealthcare: Seki et al. [54]\nAudio: UNIVERSE [180]\nGeneral: CSDI [56] Lim et al. [116]\nFig. 7: A comprehensive taxonomy of diffusion models for time series and spatio-temporal data, categorized according to\nmethodologies (i.e., unconditional vs. conditional), tasks (e.g., predictive versus generative), data types, and applications.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\ndecomposing the diffusion process into a fixed number\nof steps. This method adds noise to the data at discrete\nintervals and then learns to reverse this process, generating\ndata from noise. Both the training and inference process of\nthe model are conducted on these discrete steps, making the\nprocess essentially stepwise and iterative.\nTimeGrad is among the most classic DDPM-based time\nseries forecasting methods, identified as a score-matching\nmodel that, through rigorous validation, has been proven ef-\nfective in real-world datasets comprising thousands of inter-\nconnected dimensions [13]. D3VAE proposes a bidirectional\nvariational auto-encoder that integrates diffusion, denois-\ning, and disentanglement to enhance time series data with-\nout introducing extraneous uncertainties. This model mar-\nries multi-scale denoising score matching with bi-directional\nvariational autoencoding for forecasting tasks [86]. In a\nsimilar vein, TSDiff leverages implicit probability densities\nto iteratively refine the base forecasts, thereby addressing\nthree distinct tasks: forecasting, refining, and generating\nsynthetic data [164]. For the universal challenge of anomaly\ndetection, Pintilie et al. have proposed two multivariate\ntime series anomaly detection algorithms based on diffu-\nsion models, securing leading outcomes [259]. Concurrently,\nD3R tackles temporal anomaly detection drifts through de-\ncomposition and reconstruction, employing data-time mix-\nattention for dynamic decomposition alongside diffusion\nmodels for end-to-end training, thus overcoming the lim-\nitations imposed by local sliding windows and unstable\ndata scenarios [261]. CARD presents a denoising diffusion-\nbased generative model alongside a pretrained conditional\nmean estimator to serve both classification and regression\ntasks [98]. TS-Diffusion is specifically designed for com-\nplex sequences marked by irregular sampling, missingness,\nand extensive temporal feature dimensions, introducing a\ncomprehensive model consisting of an ODE encoder, a\nrepresentational learning module, and an ODE decoder to\nhandle such intricate time series [278]. Furthermore, Wave-\nGrad [20], DiffWave [91], and DiffuASR [23] have each suc-\ncessfully applied DDPM to waveforms, audio generation,\nand sequence recommendation, respectively.\nOn the other hand, in the field of spatio-temporal data,\nsignificant advances have been made with DDPM predomi-\nnantly starting from 2023, indicating an exciting opportunity\nfor more applications and theoretical breakthroughs. Yun\net al. have put forward \u201dImputation as Inpainting,\u201d which\nintegrates an unconditional diffusion model based on graph\nneural networks to first forecast complete spatio-temporal\ndata. This approach adjusts the generation process by\nsampling in unobserved areas using the information from\nobserved data, thus estimating missing values in spatio-\ntemporal data [69]. For traffic and specifically trajectory\nforecasting, the adoption of graph-based DDPM methods is\nbecoming more common. For example, MID++ leverages a\nGNN-based ALEncoder to extract spatio-temporal features\nand improves the training process with an important sam-\npling strategy, leading to better trajectory forecasting [279].\nIn a similar vein, DiffTAD incorporates a Transformer-based\ndecoupled time and space encoder to model spatial interac-\ntions among vehicles and conducts anomaly detection by\nevaluating the differences between query trajectories and\ntheir reconstructions [83]. Moreover, SpecSTG tackles the\nchallenges of insufficient representation of spatial network\nfeatures and the inability to detect unexpected fluctuations\nin future observations in traffic flow forecasting. It achieves\nthis by converting the learning process to the spectral\ndomain, generating Fourier representations of future time\nseries that carry spatial information [88].\n4.1.2\nScore-Based Model\nUnlike the discrete DDPM, score-based SDE models rep-\nresent diffusion and the reverse process in a continuous\nform using stochastic differential equations, thereby cover-\ning continuous time. This approach allows for a theoreti-\ncally more flexible and in-depth treatment of the diffusion\nprocess, capable of generating samples at any point in time,\nrather than being confined to fixed steps.\nScoreGrad and TimeGrad share similar goals and can\nbe described as SDE-based TimeGrad models. They expand\nthe diffusion process into a continuous spectrum and em-\nploy time series feature extraction modules along with a\ncondition-based stochastic differential equation for score\nmatching to facilitate forecasting [71]. This structure finds\nparallels in the work of Bilo\u02c7s et al. [104]. Additionally, recent\nstudies by Crabb\u00b4e et al. delve into representing time series\ndata in the frequency domain using SDEs, showcasing how\ndenoising score matching methods can facilitate diffusion\nmodeling within the frequency domain based on differ-\ning time-frequency spaces [106]. TimeADDM introduces a\nscore-based diffusion model for unsupervised anomaly de-\ntection in multivariate time series, applying diffusion steps\nto representations that encapsulate global time correlations\nthrough recurrent embeddings and designing a suite of\nreconstruction strategies to compute anomaly scores at vari-\nous diffusion intensities [282]. Moreover, Li et al. extensively\ndiscuss the impact of noise samples in SDEs on data quality\nand the corresponding robustness of models [280].\nFrom an application standpoint, score-based diffusion\nmodels have shown great promise in continuous data fields\nsuch as healthcare and acoustic data. EHRDiff [30] and\nDeScoD-ECG [57] leverage score-based diffusion models for\nrealistic EHR synthesis and ECG data generation, respec-\ntively, incorporating algorithms or structures pertinent to\nthe features of medical data, thus improving applicability\nin real-world settings. In audio-related applications, the\nfocus has been on speech enhancement and dereverberation,\nwith initial proposals by Welker et al. [284] and Richter\net al. [32] for employing score-based diffusion models for\nspeech enhancement, transforming time domain speech\ndata into time-frequency information, and treating them\nas image input for models. StoRM introduces a stochastic\nresampling method, utilizing forecasting from a predictive\nmodel as guidance for subsequent diffusion steps, achieving\nhigher-quality samples under complex conditions with high\nsignal-to-noise ratios using fewer diffusion steps and more\nstreamlined models [105]. Similarly, Lay et al. emphasize\nimproving model efficiency and optimizing model parame-\nters, proposing a Brownian bridge-based forward process\nto reconcile the gap between the forward process\u2019s end\ndistribution and the prior distribution used in inference for\nthe reverse process [178].\nIn the realm of spatio-temporal data, methods remain\nscarce, with Sasdim and DYffusion standing out. Sasdim, an\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\nadaptive noise scaling diffusion model, effectively performs\nspatial-temporal data imputation by capturing dynamic\nspatio-temporal dependencies through novel loss functions\nand global spatio-temporal convolution modules [72]. DYf-\nfusion, a dynamics-informed diffusion model for spatio-\ntemporal data forecasting, distinguishes itself from proba-\nbilistic models by integrating temporal dynamics directly\ninto the diffusion steps and training a stochastic, time-\nconditioned interpolator alongside a predictor network to\nsimulate the forward and reverse processes of the standard\ndiffusion model, thus striking a balance between perfor-\nmance and efficiency [285].\n4.2\nImproved/Advanced Diffusion Model\nWe have already discussed the improvement methods of\ndiffusion model in Sec. 2.3.3 and Sec. 2.3.4. In the following,\nwe will analyze the methods from the more frequently used\nones.\n4.2.1\nConditional Diffusion Model\nCompared to standard approaches, conditional diffusion\nmodels are able to use given conditional information\n(e.g., different representations, different modalities, etc.) to\nsteer the generative process and produce high-quality out-\nputs that are tightly correlated with conditions [34]. More\ncondition-based models are used for high-quality genera-\ntion tasks.\nCSDI and MIDM use conditional diffusion models for\ntime series imputation [56], [262]. They utilize score-based\nand probability-based diffusion models conditioned on ob-\nserved values and through explicit imputation training.\nThey can leverage the correlations among observed values\nto further enhance performance. Similar to CSDI, DiffAD\nand ImDiffusion employ a similar method based on condi-\ntional weight-incremental diffusion to enhance the imputa-\ntion performance of missing values and are applied for time\nseries anomaly detection [257], [260]. These approaches pre-\nserve the information of observed values and significantly\nimprove the generation quality for stable anomaly detection.\nOn the other hand, there are models that utilize different\nguiding information [266]. For example, [169] adjusts the\ndiffusion model based on statistical information such as\nmean, standard deviation, Z-scores, and skewness, thereby\nsynthesizing sensor data. [177] proposes two different con-\nditional enhancement techniques to enable the model to\nadaptively consider conditional information a priori, thus\nperforming the speech enhancement task. MEDiC [110]\nintroduces a class-conditional DDPM approach to generate\nsynthetic EEG embeddings. VGCDM [263] employs a pulse\nvoltage-guided diffusion model along with a cross-attention\nmechanism for more efficient generation of electrical signals.\nMeanwhile, Wang et al. leverage multimodal information\n(images and text) as generative conditions to enhance the\nquality of real-time sales forecasts [264]. DiffShape [277]\nintroduces a self-supervised diffusion learning mechanism\nthat uses real sub-sequences as conditions. By leveraging a\nlarge amount of unlabeled data, it enhances the similarity\nbetween learned shapelets and actual sub-sequences for\nclassification.\nFor spatio-temporal data, most models adopt graph\nstructures for representation, but the method of using con-\nditions is similar to that of time series data [85], [184].\nPriSTI draws inspiration from CSDI of conditioning on the\nobserved value, employing a conditional feature extraction\nmodule based on linear interpolation for generating miss-\ning data [111]. DiffTraj estimates noise levels accurately\nusing various external factors, such as the region of the\ntrip and departure time [81]. ControlTraj further extends\nDiffTraj with the constraint of road network structures [290].\nAdditionally, many works guide their models using graph\nstructures or feature graphs as conditions. For instance,\nDiffSTG [43] and USTD [170] use historical graph signals\nand graph structures as conditions, combining the spatio-\ntemporal learning capabilities of GNNs and the uncertainty\nmeasurement of diffusion models to enhance the perfor-\nmance of forecasting tasks. DiffUFlow [35] utilizes extracted\nspatio-temporal feature graphs overlaid on coarse-grained\nflow graphs as a condition to guide the reverse process.\n4.2.2\nLatent Diffusion Model\nThe latent diffusion model (LDM) performs the diffusion\nprocess in a lower-dimensional latent space, which allows\nmore efficient generation or other tasks. It allows the model\nto handle more complex data distributions, while reducing\ncomputational resource consumption and maintaining out-\nput quality.\nLDCast utilizes LDM for near-term precipitation fore-\ncasting, enhancing training stability, optimizing computa-\ntional demands, and accurately representing the uncertainty\nof forecasting compared to GAN [93]. CLDM presents a\nconditional latent diffusion model based on latent space\nmapping, which breaks down the generative task into\ndeterministic forecasting and the generation of predictive\nerror scenarios, thus increasing efficiency within the latent\nspace [107]. Similarly, Aristimunha et al. suggest the use\nof LDM for generating sleep EEG signals, where the la-\ntent feature maps outputted by encoders serve as input to\nthe diffusion model, with the generated results obtained\nthrough the corresponding decoder [128]. In a similar man-\nner, LADM proposes a method for spatio-temporal tra-\njectory forecasting, where VAE acts as a generator and\nDDPM as a refiner [265]. Furthermore, Feng et al. develop\na latent diffusion transformer for time series forecasting,\naimed at compressing multivariate time stamp patterns into\nsuccinct latent representations and efficiently generating\nauthentic multivariate time stamp values in a continuous\nlatent space [253].\n4.2.3\nOther Variants of Diffusion Models\nThere are also other methods that incorporate the differ-\nent models above or use other more advanced diffusion\nmodels. For example, to accelerate generation, MedDiff for\nthe first time utilizes DDIM with a new sampling strategy\nto generate high-dimensional large-scale electronic medi-\ncal records [48]. Specifically, it incorporates a classifier to\nguide the sampling process, assigning high probabilities\nto the data with the correct labels. DCM is a diffusion-\nbased causal inference model accelerated by DDIM, that\nmore accurately captures counterfactual distributions under\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\nunmeasured confounding factors [267]. Asperti et al. pro-\npose GED, which uses conditional DDIM integrated with\nadditional information and a post-processing network for\nhigh-performance weather forecasting [268].\nBesides, some algorithms have also been proposed that\nuse a two-stage or a diffusion model to fuse with other mod-\nels. TSDM adopts an improved two-stage diffusion model\nfor identifying and reconstructing measurements with vari-\nous uncertainties for power system measurement recovery,\nwhere the first stage includes a classifier-guided condi-\ntional anomaly detection component, and the second stage\ninvolves a diffusion-based measurement imputation com-\nponent [269]. Mueller proposes using attention-enhanced\ncondition-guided DDIM to tackle sample imbalance and\ndata scarcity issues, along with simple DDPM for signal\ndenoising, thereby synthesizing effective machine fault data\nfor efficient anomaly detection [270]. Similarly, Wong et al.\nutilize DDIM for load domain adaptation, further enhanced\nwith CNN for bearing fault anomaly detection [272].\n5\nTASK PERSPECTIVE\nIn this section, we will explore the use of diffusion models in\ndifferent tasks, such as forecasting, generation, imputation,\nand anomaly detection, also highlighting their effectiveness\nin complex time series and spatio-temporal data analysis\nacross various domains.\n5.1\nForecasting\nThe field of time series forecasting has seen significant\nadvancements with the incorporation of diffusion models.\nTimeGrad [13] and D3VAE [86] both employ diffusion\nprobabilistic models to enhance forecasting, with TimeGrad\nfocusing on autoregressive techniques (i.e., RNN) for prob-\nabilistic forecasting and D3VAE introducing a bidirectional\nvariational auto-encoder that includes diffusion and denois-\ning processes. This generative approach is further extended\nin the work by D-Va [47], which addresses the stochas-\ntic nature of stock price data through a deep hierarchi-\ncal VAE combined with diffusion probabilistic techniques.\nMeanwhile, the study presented in [104] takes a different\napproach by modeling temporal data as continuous func-\ntions, allowing for the handling of irregularly sampled data.\nBuilding upon the diffusion model, TimeDiff [34] intro-\nduces novel conditioning mechanisms, future mixup, and\nautoregressive initialization, to improve time series fore-\ncasting. Lastly, the research in [164] explores task-agnostic\nunconditional diffusion models, proposing TSDiff, which\nemploys a self-guidance mechanism for versatile time series\napplications.\nFor spatio-temporal data, various diffusion model-based\napproaches have been proposed to tackle complex forecast-\ning problems. DiffSTG [43] is the first work that generalizes\ndenoising diffusion probabilistic models to spatio-temporal\ngraphs, aiming to model complex spatio-temporal depen-\ndencies and intrinsic uncertainties within spatio-temporal\ngraph data for better forecasting. DYffusion [285] intro-\nduces a framework that trains a stochastic, time-conditioned\ninterpolator and a forecaster network to perform multi-\nstep and long-range probabilistic forecasting for spatio-\ntemporal data. On the other hand, DiffUFlow [35] focuses\non urban data, which aims to address the challenge of fine-\ngrained flow inference. DSTPP [70] provides a novel param-\neterization for spatio-temporal point processes. Meanwhile,\nSwinRDM [16] and DOT [65] demonstrate the adaptability\nof diffusion models in improving the quality of weather\nforecasts and travel time estimations, showcasing the wide\napplicability of these models across different domains and\ntasks. These works highlight the growing impact and poten-\ntial of diffusion models in advancing spatio-temporal data\nanalysis.\n5.2\nGeneration\nInspired by the powerful ability of the diffusion model in\nhigh-dimensional distribution learning, an intuitive usage\nis applying the learned diffusion model for data generation.\nCurrently, a variety of diffusion models have been proposed\nfor generating audio data. WaveGrad [20] is a pioneering\nwork that utilizes gradient-based sampling to generate high-\nfidelity audio waveforms, marking a significant advance-\nment in audio synthesis. Similarly, DiffWave [91] employs a\nnon-autoregressive approach to achieve efficient and high-\nquality raw audio synthesis through a Markov chain pro-\ncess. Both WaveGrad and DiffWave are part of the same\nresearch lineage, leveraging the power of diffusion models\nto create complex waveforms from simple noise distribu-\ntions. DOSE [177] takes a different approach by focusing on\nthe speech enhancement task, which introduces a model-\nagnostic method that integrates condition information into\nthe diffusion process.\nBesides, diffusion model is also used in sequential rec-\nommendation. DiffuASR [23] proposes a diffusion-based\nsequence generation framework to address the data sparsity\nand long-tail user problems in sequential recommendation\nsystems. It introduces a sequential U-Net designed for\ndiscrete sequence generation tasks and utilizes two guide\nstrategies to assimilate preferences between generated and\noriginal sequences. DreamRec [167] employs a Transformer\nencoder to create guidance representations as the condition\nin the diffusion process.\nIn the realm of spatial-temporal data, diffusion model is\nadopted to generate trajectory data. For instance, DiffTraj\n[81], [290] proposes the first effort that generates high-\nquality human trajectory with an unconditioned model,\nwith the motivation to protect privacy. Meanwhile, [65]\ngenerates trajectories in the format of grids, by a condition\nmodel guided by the origin-destination information.\n5.3\nImputation\nIn the domain of time series and spatial-temporal data anal-\nysis, imputation refers to generating the unobserved data\nconditioned on the given observed data. CSDI [56] proposes\na score-based diffusion model that probabilistically imputes\nmissing time series and spatio-temporal data. MIDM [262]\nredefines the evidence lower bound (ELBO) for conditional\ndiffusion models tailored for multivariate time series im-\nputation, which ensures the consistency of observed and\nmissing values. PriSTI [111] introduces a conditional dif-\nfusion framework specifically designed for spatio-temporal\ndata imputation, using global context priors and geographic\nrelationships to tackle scenarios with high missing data\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n15\nrates due to sensor failures. More recently, TabCSDI [33]\nand MissDiff [113] leverage diffusion models for imputing\nmissing values in tabular data, each addressing distinct\naspects of this complex challenge. TabCSDI manages cate-\ngorical and numerical data effectively, providing a tailored\nsolution to diverse data types common in tabular datasets.\nOn the other hand, MissDiff enhances the diffusion model\u2019s\ntraining process by introducing a novel masking technique\nfor regression loss, which ensures consistent learning of data\ndistributions and robustness against different missing data\nscenarios.\n5.4\nAnomaly Detection\nAnomaly detection aims to identify anomalies in the given\ntime series or spatial-temporal data, which is quite practical\nand crucial for many real-world applications. A majority of\nworks have been proposed for general time series anomaly\ndetection. Initially, DiffAD [260] and ImDiffusion [257] both\nexplored the synergy of imputation techniques with diffu-\nsion models for time series anomaly detection, enhancing\nthe robustness of anomaly detection processes by accurately\nmodeling complex dependencies. Concurrently, [259] and\n[258] use similar diffusion-based methodologies to address\nanomaly detection but apply different enhancements to\nimprove performance and computational efficiency. Mean-\nwhile, [261] and DDMT [99] tackled the specific challenges\nof instability and noise, implementing advanced diffusion\nreconstruction techniques to maintain accuracy in dynamic\nenvironments.\nBesides the above methods designed for general time\nseries anomaly detection. There are other works that apply\nadvanced diffusion techniques across various domains. For\ninstance, Maat [37] anticipates performance metric anoma-\nlies in cloud services using a conditional denoising dif-\nfusion model to forecast metrics and detect anomalies.\n[270] enhances data synthesis for machine fault diagnosis\nusing an attention mechanism within a conditional diffu-\nsion model framework. Diffusion-UDA [271] proposes a\ndiffusion-based method for unsupervised domain adapta-\ntion in submersible fault diagnosis, leveraging diffusion\nprocesses to adapt domains for effective fault recognition.\n6\nDATA PERSPECTIVE\nIn this section, we analyze current work from a data per-\nspective, including time series and spatio-temporal data.\nWe focus on introducing how diffusion models capture the\nunique properties of different data modalities.\n6.1\nTime Series Data\nTime series analysis is a critical issue explored across vari-\nous real-world scenarios, including retail sales forecasting,\nfilling in missing data in economic time series, identifying\nanomalies in industrial maintenance, and categorizing time\nseries from different domains.\n6.1.1\nUnivariate Time Series\nUnivariate time series is characterized by having only one\nvariable of interest observed over a period of time. Data\nin this category include ECG signal, audio, electricity load,\netc. The complex sequential patterns (e.g., the trend and the\nperiodicity) are the core data property in univariate time\nseries, which pose great challenges in developing relevant\ndiffusion models.\nDiffusion Model for Univariate Time Series. Diffusion\nmodels for univariate time series are primarily developed\nto model the uncertainty present in the data, facilitating\ntasks such as probabilistic forecasting or data generation.\nAn essential element within the diffusion model is the\ndenoising network, which determines the extent of noise\nremoval at each stage. In the context of univariate time\nseries, the denoising network commonly employs the 1-D\nCNN to identify sequential patterns in the input data. For\ninstance, both WaveGrad [20] and DiffWave [91] incorporate\na 1-D CNN in the denoising network to extract local se-\nquential features from audio sequences. Furthermore, some\nstudies opt to transform the univariate time series into the\nfrequency domain initially to enhance the capture of long-\nterm sequential correlations from a global standpoint. This\ntransformation converts the time series from 1-D to 2-D\ndata, enabling the use of a 2-D CNN in the denoising net-\nwork to capture correlations within the frequency domain,\nsuch as [47] for the stock price forecasting and DiffLoad [28]\nfor electricity load forecasting.\n6.1.2\nMultivariate Time Series\nMultivariate time series is characterized by having mul-\ntiple variables of interest observed over the same period\nof time. Instead of the sequential patterns in each variate,\nmultivariate time series also leverage the interdependencies\namong different variables to capture more comprehensive\ninformation downstream tasks.\nDiffusion Models for Multivaritate Time Series. Beyond\napproaches that focus on univariate time series, there have\nbeen efforts towards multivariate time series. Multivariate\ntime series is naturally a 2-D data similar to the image data,\nwhere the signal of each variate can be considered as one\nchannel in the image. To this end, diffusion models for\nmultivariate time series usually adopt the vanilla Unet as\nthe denoising net, which is a common practice in diffusion\nmodels for images.\nFor example, [5] first transforms demultiplex, denoising,\nand interpolation into image-to-image transformation tasks,\nthen leverages the diffusion model for different tasks. In\nthe demultiplex task, the diffusion model learns to remove\nthe multiples without removing primary energy. In the\ndenoising and interpolation task, the diffusion model learns\nto eliminate undesired uncorrelated noise and recover the\nmissing values, while preserving the inherent characteristics\nof the data. MIDM [262] proposes a novel multivariate\nimputation diffusion model that incorporates correlations\nbetween observed and missing values. By re-deriving the\nELBO of the conditional diffusion model, MIDM ensures\nconsistency between observed and missing data, thus lead-\ning to improved imputation accuracy. Diff-E [109] further\nutilizes the diffusion model for representation learning,\nwhich combines DDPMs with a conditional autoencoder to\nenhance the decoding performance of speech-related EEG\nsignals.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n16\n6.2\nSpatio-Temporal Data\nIn real-world systems, a myriad of elements interact with\neach other both spatially and temporally, resulting in a\nspatio-temporal composition. Spatio-temporal data (STD)\nis the de facto most popular data structure for injecting\nsuch structural information into the formulation of practical\nproblems. In this section, we introduce developments of\ndiffusion models for spatial-temporal data, mainly with two\ndata modalities: (1) the Spatio-Temporal Graph (STG) and\n(2) the Spatio-Temporal Trajectory (STT). Unlike diffusion\nmodels for time series data, existing works in STD need\nto model the temporal dependencies as well as the spatial\ndependencies, making the task even more challenging.\n6.2.1\nSpatio-Temporal Graph\nThe spatio-temporal graph is generated by sensors at differ-\nent places in a period of time, where the correlation of those\nsensors is typically described as a graph. We categorize\ndiffusion models for spatial-temporal graph into domain-\noriented and domain-agnostic works.\nMost of the existing works fall into the category of\ndomain-oriented spatial-temporal graph diffusion models.\nThey typically leverage the powerful distribution learning\nabilities of diffusion models for spatial-temporal data min-\ning tasks in specific domains, such as traffic and climate.\nIn this research trend, the majority of works come from\nthe traffic domain. DiffUFlow [35] represents pioneering\nefforts as traffic diffusion models, which convert the fine-\ngrained urban flow inference as a denoising diffusion pro-\ncess. SpecSTG [88] advances the field by conducting the\ndiffusion process in the spectral space projected by the STG,\nresulting in faster inference speeds. In a similar vein, [59]\nuses a diffusion model to infer the link probability and\nreconstruct causal graphs in the decoder stage adaptively\nfor the STG forecasting task. Furthermore, [65] solves the\norigin-destination travel time estimation problem with a\ntwo-step process, where the first step predicts the possible\ntravel route with a diffusion model conditioned on the\ngiven origin-destination pair. Beyond the traffic domain,\nthere have been initiatives towards the climate. A notable\nexample is [78], which quantifies the uncertainty of air\nquality forecasting based on the spatial-temporal diffusion\nmodel. Similarly, SRNDiff [85] conducts the short-term rain-\nfall nowcasting through the conditional diffusion model.\nIn the second group, researchers focus on developing\ndomain-agnostic models, which can achieve promising per-\nformance across a variety of domains. For instance, DiffSTG\nrepresents a pioneering effort by introducing a unified diffu-\nsion framework for multiple STG tasks such as forecasting\nand imputation. Concurrently, PriSTI [111] and [69] study\nthe spatial-temporal imputation task while treating the task\nof recovering unobserved values as a denoising process\nconditioned on the observed values. Similarly, DYffusion\n[29] models the temporal dynamics directly within diffusion\nsteps, leading to a stochastic, time conditioned forecasting\nnetwork. Another noteworthy contribution is [170] under\nthe domain-agnostic and task-agnostic setting, which aims\nto harness the power of diffusion models, and unify diffu-\nsion models for probabilistic spatio-temproal graph learn-\ning.\nEEG (Healthcare)\nEEG\nAudio\nElectricity Load\nEnergy and Electricity\nEEG\nAudio\nElectricity Load\nEEG\nAudio\nElectricity L\nTraffic\nRecommendation\nClimate and Weather\nAIOps\nFinance\n\u2026\n\u2026\nAudio\nFig. 8: Examples of time series and spatio-temporal data for\ndifferent application scenarios.\n6.2.2\nSpatio-Temporal Trajectory\nSpatial-temporal trajectory is a sequence of locations or-\ndered by time that describe the movements of an object in\na geographical space. Analysis of spatial-temporal modality\nis particularly crucial to discovering the mobility patterns\nof moving objects, which serves as the foundation for many\ndownstream tasks, such as POI recommendation and next-\nlocation forecasting. In terms of diffusion models for trajec-\ntory data, most works are developed to solve the trajectory\ngeneration task. They leverage the ability of the diffusion\nmodel to learn high-dimensional data distribution while\ninjecting spatial-temporal correlation into the diffusion pro-\ncess. As exemplified by DiffTraj [81], which generates the\nGPS trajectory with the diffusion probabilistic model in an\nunconditioned manner. Similarly, [60] utilizes the diffusion\nmodel for generating high-quality human mobility data.\nMore recently, Diff-RNTraj [17] further generates the trajec-\ntory constrained by the road-network. Beyond generation,\nthere is a growing interest in applying diffusion models for\ntrajectory forecasting tasks, and [66] and [184] exemplify this\ntrend.\n7\nAPPLICATION PERSPECTIVE\nIn this section, we summarize and discuss the most im-\nportant applications of diffusion models in time series and\nspatio-temporal data, including healthcare, smart city, rec-\nommendation, climate and weather, energy and electricity,\naudio, video and so on. The toy signal examples of each\napplication are shown in Fig. 8 .\nIn detail, we will discuss the models in different applica-\ntions, aiming to show light on the model design of practical\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n17\nTABLE 2: Summary of dataset resources in different application domains.\nApplications\nDataset\nDescription\nTimeframe\nSource\nCitations\nHealthcare\nSeed V dataset\nFrom 16 participants in three emotions test sessions\nthrough video presentation.\nEach test is around 50 min-\nutes.\n[118]\n[119]\nQT Database\n105 fifteen-minute excerpts of two-channel ECG Holter\nrecordings. Examples of each morphology were included\nin this subset of annotated beats; at least 30 beats in each\nrecord, 3622 beats in all, were manually annotated in the\ndatabase.\n105*15 minutes\n[120]\n[57]\nMIT-BIH NST dataset\n12 half-hour ECG recordings and 3 half-hour recordings of\nnoise typical in ambulatory ECG recordings.\n15 half hour\n[121]\n[57]\nPTB-XL dataset\nCollection of clinical 12-lead ECG data comprising 21,837\nrecords from 18,885 patients.\nEach 10 seconds length.\n[122]\n[123], [124]\nMIT-BIH dataset\nHealthy with 90,589 ob- servations, atrial premature with\n2,779 observations, ventricular malfunction with 7,236\nobservations, fusion of ventricular and normal with 803\nobservations and a last class of 8,039 unclassified observa-\ntions.\n48 half-hour excerpts of two-\nchannel\nambulatory\nECG\nrecordings.\n[125]\n[124]\nAlzheimer\u2019s\nDisease\nEEG dataset\n88 participants, categorized into three groups: 36 individ-\nuals diagnosed with Alzheimer\u2019s disease(AD group), 23\ndiagnosed with Frontotemporal Dementia (FTD group),\nand 29 healthy subjects (CN group).\nThe duration of the disease\nwas measured in months and\nthe median value was 25\nwith IQR range (Q1-Q3) be-\ning 24 - 28.5 months.\n[126]\n[110]\nTraffic\nCab\nTrajectory\ndatasets\nReal World Trajectory data of Chengdu, Harbin and Xi\u2019an\nCity from Didi Chuxing, around 5.6 million trajectory\nnumber.\nStarting from November 1,\n2016, to November 30, 2016.\n[81]\n[17], [65],\n[81]\nPeMS08\nThe traffic data are aggregated into every 5-minute inter-\nval from the raw data collected by the Caltrans Perfor-\nmance Measurement System.\nFrom July to August in 2016,\ncontaining 1979 detectors on\n8 roads.\n[82]\n[59], [62],\n[63]\nMAAD-Highway\ndataset\nA dataset for multi-agent anomaly detection based on the\nOpenAI Gym MultiCarRacing-v0 environment.\nThe\nsequences\nare\nsub-\nsampled to 10 Hz with a\nsegment length of T = 1.5\nseconds.\n[64]\n[66], [67]\nLargeST\nTraffic flow data of California, including a total number of\n8,600 sensors over road networks.\nStarting\nfrom\nJanuary\n1,\n2017, to December 31, 2021.\n[283]\n[283]\nRecommendation\nAmazon-book\nThis dataset contains product reviews and metadata from\nAmazon, including 142.8 million reviews.\nMay 1996 - July 2014.\n\\\n[132], [135]\nYelp\n908,915 tips by 1,987,897 users Over 1.2 million business\nattributes like hours, parking, availability, and ambience\nAggregated check-ins over time for each of the 131,930\nbusinesses\n\\\n\\\n[132]\nML-IM\n1 million ratings from 6000 users on 4000 movies.\n\\\n[133]\n[95], [132],\n[134]\nAmazon-beauty\nOver 2 Million+ customer reviews and ratings of Beauty\nrelated products sold on their website.\n\\\n\\\n[36], [95],\n[134]\nAmazon-toys\n10,000 toy products on Amazon.com\n\\\n\\\n[36], [95],\n[134], [135]\nClimate and Weather\nERA5\nThe fifth generation ECMWF atmospheric reanalysis of the\nglobal climate covering the period from January 1940 to\npresent. ERA5 provides hourly estimates of a large num-\nber of atmospheric, land and oceanic climate variables.\nThe data cover the Earth on a 31km grid and resolve\nthe atmosphere using 137 levels from the surface up to\na height of 80km. ERA5 includes information about uncer-\ntainties for all variables at reduced spatial and temporal\nresolutions.\n1940 to present.\n[136]\n[16], [27],\n[84], [137]\nData\nfrom\nIPSL-\nCM5A ESM\nA full earth system climate.\n\\\n[138]\n[15]\nGOES West data\nHighresolution atmospheric measurements over the Pa-\ncific Ocean in near real-time with the explicit goal of\nimproving weather forecasting capabilities.\n\\\n\\\n[27]\nEnergy and Electricity\nGEFCom2014\nLoad\nForecasting Data\n\\\n7 years of matching load and\ntemperature data\n[139]\n[28], [107]\nACN-Data\nCollect detailed data about each charging session in the\nsystem.\n\\\n[140]\n[96]\nLow Carbon London\nProject Data\nContains electricity consumption data of 5198 customers\nfrom November 2011 to February 2014 with a time granu-\nlarity of 30 minutes.\nNovember 2011 to February\n2014\n[141]\n[142]\nLow Carbon London\nProject Data\nContains electricity consumption data of 5198 customers\nfrom November 2011 to February 2014 with a time granu-\nlarity of 30 minutes.\nNovember 2011 to February\n2014\n[141]\n[142]\nAudio\nLJ Speech dataset\n13,100 short audio clips of a single speaker reading pas-\nsages from 7 non-fiction books.\nThe texts were published be-\ntween 1884 and 1964, and are\nin the public domain. The au-\ndio was recorded in 2016-17\nby the LibriVox project and is\nalso in the public domain.\n[68]\n[20], [21],\n[22], [91]\nSC09 dataset\n31,158 training utterances (8.7 hours in total) by 2,032\nspeakers.\n\\\n[273]\n[91], [274]\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n18\napplications. Besides, the main datasets in each application\ndomains are shown in Tab. 2 , offering convenience for\nfuture research.\n7.1\nHealthcare\nIn recent years, diffusion models have emerged as a power-\nful class of generative models with significant implications\nfor time series in healthcare. These models, known for their\nability to generate high-fidelity samples through a process of\ngradual refinement, have found diverse applications rang-\ning from the synthesis of electronic health records (EHRs) to\nthe enhancement of biomedical signal analysis. The follow-\ning are some main topics in this area:.\n\u2022 EEG Signal Synthesis and Enhancement: [74] introduces\na generalized probabilistic diffusion model tailored for the\nsynthesis of electrocardiogram (ECG) signals, aiming to\nsupport cardiac health research and diagnostic training\nwith realistic data generation. [128] demonstrate the po-\ntential of diffusion models to create synthetic EEG sig-\nnals, facilitating sleep disorder studies and neurological\nresearch. [57], [123] focus on enhancing the quality of ECG\nsignals, addressing issues like baseline wander and noise,\nand generating conditional ECG signals based on specific\npatient states.\n\u2022 Healthcare Data Augmentation and Synthesis: [18] high-\nlights the use of diffusion models to augment datasets\nfor seizure forecasting, thereby improving the robustness\nand accuracy of predictive models. [30], [31] explore the\nsynthesis of realistic EHRs, tackling the challenges of data\nprivacy and scarcity by providing synthetic datasets for\nresearch and model training.\n\u2022 Diagnostic and Predictive Analytics: [50] leverages gen-\nerative models to enhance the detection of autism, show-\ncasing the broad applicability of these models in diagnos-\ning and understanding complex conditions. In [49], diffu-\nsion models are applied to forecast critical physiological\nparameters in ICUs, demonstrating their potential to save\nlives by predicting adverse events before they occur.\n\u2022 Novel Methodologies and Techniques: [58] exemplifies\nthe versatility of diffusion models in handling multiple\ntasks simultaneously, such as predicting mortality in criti-\ncally ill patients, showcasing the potential for comprehen-\nsive care management systems.\n7.2\nTraffic\nThere are also many works for traffic applications using\ndiffusion models. The following are some of them. Dif-\nfUFlow [35] employs a denoising diffusion model to en-\nable the model to capture the stochastic nature of urban\nflows and generate accurate and fine-grained forecasting.\nDiffTraj [81] presents a groundbreaking approach to gen-\nerating GPS trajectories using a spatial-temporal diffusion\nprobabilistic model. The model works by reconstructing\ngeographic trajectories from white noise through a reverse\ntrajectory denoising process, effectively turning random\nnoise into meaningful trajectory data that reflects real-world\nmovement patterns. DVGNN architecture [59] comprises\nan encoder component that learns latent node embeddings\nthrough GCN layers and a decoder component that treats\nthe relationships of latent states as a diffusion process sub-\nject to stochastic differential equations. This allows for the\ninference of internal causal relationships among neighbor\nnodes and the formation of dynamic causal graphs. [61]\naddresses the growing risk of collisions between space\nobjects by developing a diffusion model that predicts the\npositional uncertainty of objects during close encounters.\n[83] introduces a model that utilizes denoising diffusion\nprobabilistic models for identifying anomalies in vehicle\ntrajectories, aimed at improving the detection of unusual\npatterns in vehicle movements. SpecSTG [88] aims to ef-\nficiently handle the complexities of spatio-temporal data,\noffering a probabilistic approach to forecasting traffic flows.\nThis method is distinguished by its speed and accuracy in\ngenerating forecasts and is a promising solution for real-\ntime traffic management and planning applications.\n7.3\nSequential Recommendation\nIn the scenario of sequential recommendation, diffusion\nmodels are also widely applied. [23] and [132] both explore\nthe application of diffusion models in enhancing sequen-\ntial recommendation systems, yet they focus on different\naspects and methodologies within this domain. That is, [23]\naims to bridge the gap between discrete item identities and\nthe continuous nature of the data generated by diffusion\nmodels, while [132] focuses on predicting users\u2019 future\ninteraction probabilities by corrupting and then denoising\ntheir interaction histories and address challenges specific,\nsuch as high resource costs and temporal shifts in user\npreferences. DCDR [131] is a discrete forward process with\ntractable posteriors and a conditional reverse process tai-\nlored for sequence generation. RecFusion [130] introduces\na binomial diffusion process tailored for one-dimensional\ndata, such as time series of user interactions, showcasing\nits effectiveness in sequential recommendation scenarios.\nDiffuRec [95] proposes a specific diffusion model frame-\nwork designed for sequential recommendation, focusing on\nefficiently capturing and predicting evolving user interests.\nDiffusion models have shown significant promise in\nimproving sequential recommendation systems. Their abil-\nity to simulate the spread of information and preferences\noffers a nuanced method for predicting user behavior. Fu-\nture research could explore hybrid models, scalability, and\nreal-time adaptation, further enhancing the relevance and\npersonalization of recommendations.\n7.4\nClimate and Weather\nDiffusion models, initially developed for image generation,\nhave found novel applications in weather forecasting due\nto their ability to generate high-resolution, realistic outputs.\nThese models work by gradually refining a signal from a\nrandom noise distribution towards a desired output, mak-\ning them well-suited for predicting complex atmospheric\nphenomena. [93] focuses on precipitation nowcasting with\nan emphasis on accurate uncertainty quantification, lever-\naging latent space representations to efficiently model and\npredict rainfall intensity and distribution. [94] presents a\nwind resolution-enhancing model that improves the detail\nand accuracy of wind speed forecasts by refining coarse\nforecasting to higher resolutions. SwinRDM [16] integrates\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n19\nSwin Recurrent Neural Networks (SwinRNN) with diffu-\nsion models for state-of-the-art weather forecasting, achiev-\ning high resolution and quality by capturing both spatial\nand temporal dynamics. DiffESM [15] utilizes diffusion\nmodels for the conditional emulation of Earth System Mod-\nels (ESMs), offering a computationally efficient alternative\nto traditional simulation techniques with improved accu-\nracy. [27] addresses the challenge of high-resolution solar\nforecasting, showcasing the model\u2019s ability to accurately\npredict solar irradiance variations. DiffAD [26] introduces a\ndiffusion model-based approach for weather-scale data as-\nsimilation, enhancing the integration of observational data\ninto forecast models for improved accuracy.\n7.5\nEnergy and Electricity\nDiffusion models are now being adapted to tackle chal-\nlenges in energy systems, offering novel solutions for prob-\nabilistic forecasting, signal synthesis, and system security.\nDiffLoad [28] is a novel approach to load forecasting that\nleverages diffusion models to quantify uncertainties effec-\ntively. By incorporating uncertainty quantification, DiffLoad\noffers a more reliable forecast, which is crucial for grid\nstability and operational planning. [142] explores how con-\nditional diffusion models can synthesize load profiles for\nindividual electricity customers, enhancing the personaliza-\ntion of demand-side management strategies. DiffPLF [143]\naddresses the challenges for load forecasting by provid-\ning probabilistic forecasts of EV charging loads, facilitating\nbetter grid management and infrastructure planning. [14]\ndiscusses the application of denoising diffusion probabilis-\ntic models in energy forecasting, emphasizing the model\u2019s\nability to handle uncertainty and provide probabilistic fore-\ncasts, which are essential for integrating renewable energy\nsources. [107] demonstrates how conditional latent diffusion\nmodels can be used to generate realistic short-term wind\npower scenarios, aiding in system operation and planning.\n7.6\nAudio\nDiffusion models have also found promising applications\nin generating waveforms, synthesizing audio, music gener-\nation and enhancing speech, offering significant improve-\nments in quality, realism, and control over the generated or\nenhanced audio.\n\u2022 Waveform Generation and Audio Synthesis WaveG-\nrad [20] introduces a gradient-based approach to generate\nhigh-fidelity waveforms, demonstrating the potential of\ndiffusion models in audio synthesis without the need for\nautoregressive models. DiffWave [91] extends the capabil-\nities of diffusion models in audio synthesis, showcasing\ntheir versatility across different audio synthesis tasks,\nincluding voice and music.\n\u2022 Music Generation [92] explores the fusion of diffusion\nmodels and GANs for generating symbolic music, al-\nlowing for emotion-driven control over the generation\nprocess. DiffuseRoll [90] highlights the application of dif-\nfusion models in multi-track music generation, providing\nnuanced control over various musical attributes.\n\u2022 Speech Enhancement [32] introduces a diffusion-based\ngenerative approach to simultaneously enhance speech\nand reduce reverberation, showcasing significant im-\nprovements over traditional methods. [180] focuses on the\nuniversal applicability of diffusion models and demon-\nstrates effectiveness in enhancing speech across a wide\nrange of conditions. [178] proposes a method to reduce\nthe prior mismatch in stochastic differential equations,\nenhancing model performance in speech tasks. CRA-\nDIFFUSE [179] introduces a pre-denoising step in the\ntime-frequency domain to improve cross-domain speech\nenhancement, illustrating the potential for methodological\ninnovations within diffusion model applications.\nThe application of diffusion models in audio processing\nhas opened new avenues for research and development,\noffering novel solutions to longstanding challenges. As the\nfield progresses, future research may focus on improving\nmodel efficiency, reducing computational demands, and\nexploring untapped applications within audio processing.\n7.7\nOthers\nBesides the applications discussed above, there are also\nsome other applications of diffusion models. The following\nare some of them:\n\u2022 AIOps: Cloud computing\u2019s reliability is paramount, yet\nit faces challenges like performance anomalies, unpre-\ndictable network traffic, and incomplete data. Diffusion\nmodels have emerged as a powerful tool to tackle these\nissues, offering new approaches for predictive mainte-\nnance, realistic traffic simulation, and enhanced failure\nforecasting. Maat [37] applies conditional diffusion mod-\nels to anticipate performance metric anomalies in cloud\nservices, demonstrating the potential for early anomaly\ndetection, potentially reducing downtime, and improving\nservice reliability. NetDiffus [39] applies diffusion models\nto time-series imaging for generating realistic network\ntraffic patterns, showing how simulated traffic can sup-\nport network planning, testing, and anomaly detection,\nenhancing network management and security. [45] inves-\ntigates diffusion models for imputing missing data in time\nseries, aiming to improve cloud failure forecasting accu-\nracy, highlighting the effectiveness of diffusion models in\nhandling data gaps, leading to better predictive outcomes\nfor cloud service failures.\n\u2022 Finance: The complexity and stochastic nature of financial\nmarkets make them an ideal candidate for the application\nof diffusion models, which can capture non-linearities\nand intricate patterns in data. Recent advancements have\nhighlighted the potential of diffusion models in enhancing\nstock price forecasting, generating realistic financial tabu-\nlar data, and augmenting stock factor data for improved\ninvestment strategies. [47] presents a novel approach to\npredict stock prices using a Diffusion Variational Autoen-\ncoder, aiming to better capture the stochastic nature of the\nmarket, highlighting the model\u2019s ability to handle market\nvolatility. FinDiff [40] focuses on generating synthetic\nfinancial tabular data to address the scarcity of publicly\navailable financial datasets. Synthetic data generated by\nFinDiff closely mimics real financial datasets, potentially\naiding in model training and regulatory compliance with-\nout compromising sensitive information. DiffSTOCK [41]\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n20\nemploys diffusion models for probabilistic relational rea-\nsoning in stock market forecasting, aiming to capture\nthe complex interdependencies between different market\nfactors.\n8\nOUTLOOK AND FUTURE OPPORTUNITIES\nIn this section, we point out some future research directions\nof diffusion models for time series and spatio-temporal data\nthat are worthy of further investigation.\n8.1\nScalability and Efficiency\nThe computational complexity of diffusion models presents\nchallenges for their application in resource-constrained or\nreal-time environments. Therefore, enhancing their scalabil-\nity and efficiency is pivotal for their deployment in real-\nworld scenarios. Future directions can explore lighter and\nfaster versions of diffusion models that significantly reduce\ncomputational demands while maintaining performance.\nBesides, further efforts include model compression, parallel\ncomputing, and efficient sampling strategies optimized for\ndiffusion models adopted in time series and spatio-temporal\ndata.\n8.2\nRobustness and Generalization\nData challenges like noise, missing data, anomalies, and\ndistribution shifts often exist in real-world time series and\nspatio-temporal data. It is crucial to investigate and en-\nhance the robustness of diffusion models against these data\nchallenges. Therefore, enhancing the model\u2019s generalization\ncapabilities across different datasets and scenarios is also an\ninteresting direction to expand applicability and reliability\nin various domains. Furthermore, research could also focus\non developing frameworks that adapt to new data char-\nacteristics or changing environments dynamically without\nhuman intervention.\n8.3\nPrior Knowledge Guided Generation\nThe generation process of time series and spatio-temporal\ndata should adhere to unique constraints; for instance, gen-\nerated trajectories should propagate over road networks,\npopulation migration data should conform to societal evo-\nlutionary patterns, and the spread of fires should adhere\nto thermodynamic principles. Most of the existing diffu-\nsion models, while capable of generating corresponding\ntime series or spatio-temporal data based on some useful\nconditions, still lack adequate consideration for such prior\nknowledge in practice.\n8.4\nMultimodal Data Fusion\nIn complex real-world scenarios, time series and spatio-\ntemporal data are often accompanied by other data types,\nsuch as textual and visual information. Exploring the fu-\nsion of multimodal data sources within diffusion models\ncould significantly boost performance. This is particularly\nuseful in domains like finance and healthcare, where inte-\ngrating diverse data sources can lead to more comprehen-\nsive and accurate analyses. Future research could develop\nnovel architectures that more effectively merge these diverse\ndata streams, enhancing predictive performance and contex-\ntual understanding for multimodal time series and spatio-\ntemporal data.\n8.5\nIntegration of LLMs and Diffusion Models\nThe integration of LLMs and diffusion models for time\nseries and spatio-temporal data analysis offers promising\npotential to advance the understanding of complex sys-\ntems and improve decision-making. Specifically, leveraging\nthe natural language understanding capabilities of LLMs\ncan enhance temporal reasoning and offer a more holistic\nview of complex systems. Future research could include\ndeveloping combined models that utilize the generative\ncapabilities of diffusion models along with the rich semantic\nand syntactic processing of LLMs, potentially opening new\navenues for automated reasoning and decision systems.\n9\nCONCLUSION\nIn this survey, we presented a comprehensive overview of\nthe advancements and applications of diffusion models in\nthe context of time series and spatio-temporal data analysis.\nWe categorized diffusion models into unconditioned and\nconditioned types, each offering distinct advantages and\nchallenges. Furthermore, we examined the various tasks\nassociated with these models, including forecasting, gen-\neration, imputation, and anomaly detection. Additionally,\nwe explored different application scenarios and provided\ninsights into future opportunities and directions in this\nresearch field. It is our hope that this survey will contribute\nto the advancement of research in the area of diffusion\nmodels for time series and spatio-temporal data analysis.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n21\nREFERENCES\n[1]\nJ. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermody-\nnamics,\u201d in International conference on machine learning.\nPMLR,\n2015, pp. 2256\u20132265.\n[2]\nJ. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d Advances in neural information processing systems, vol. 33,\npp. 6840\u20136851, 2020.\n[3]\nA. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffu-\nsion probabilistic models,\u201d in International Conference on Machine\nLearning.\nPMLR, 2021, pp. 8162\u20138171.\n[4]\nY. Song and S. Ermon, \u201cGenerative modeling by estimating\ngradients of the data distribution,\u201d Advances in neural information\nprocessing systems, vol. 32, 2019.\n[5]\nR. Durall, A. Ghanim, M. R. Fernandez, N. Ettrich, and J. Keuper,\n\u201cDeep diffusion models for seismic processing,\u201d Computers &\nGeosciences, vol. 177, p. 105377, 2023.\n[6]\nY. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\nB. Poole, \u201cScore-based generative modeling through stochastic\ndifferential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.\n[7]\nR. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d\nin Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 10 684\u201310 695.\n[8]\nP. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on im-\nage synthesis,\u201d Advances in neural information processing systems,\nvol. 34, pp. 8780\u20138794, 2021.\n[9]\nJ. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv\npreprint arXiv:2207.12598, 2022.\n[10]\nC. Jarzynski, \u201cEquilibrium free-energy differences from nonequi-\nlibrium measurements: A master-equation approach,\u201d Physical\nReview E, vol. 56, no. 5, p. 5018, 1997.\n[11]\nE. Hoogeboom, D. Nielsen, P. Jaini, P. Forr\u00b4e, and M. Welling,\n\u201cArgmax flows and multinomial diffusion: Learning categorical\ndistributions,\u201d Advances in Neural Information Processing Systems,\nvol. 34, pp. 12 454\u201312 465, 2021.\n[12]\nJ. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit\nmodels,\u201d arXiv preprint arXiv:2010.02502, 2020.\n[13]\nK. Rasul, C. Seward, I. Schuster, and R. Vollgraf, \u201cAutoregressive\ndenoising diffusion models for multivariate probabilistic time\nseries forecasting,\u201d in International Conference on Machine Learning.\nPMLR, 2021, pp. 8857\u20138868.\n[14]\nE. H. Capel and J. Dumas, \u201cDenoising diffusion probabilistic\nmodels for probabilistic energy forecasting,\u201d in 2023 IEEE Bel-\ngrade PowerTech.\nIEEE, 2023, pp. 1\u20136.\n[15]\nS. Bassetti, B. Hutchinson, C. Tebaldi, and B. Kravitz, \u201cDiffesm:\nConditional emulation of earth system models with diffusion\nmodels,\u201d arXiv preprint arXiv:2304.11699, 2023.\n[16]\nL. Chen, F. Du, Y. Hu, Z. Wang, and F. Wang, \u201cSwinrdm: integrate\nswinrnn with diffusion model towards high-resolution and high-\nquality weather forecasting,\u201d in Proceedings of the AAAI Conference\non Artificial Intelligence, vol. 37, no. 1, 2023, pp. 322\u2013330.\n[17]\nT. Wei, Y. Lin, S. Guo, Y. Lin, Y. Huang, C. Xiang, Y. Bai, M. Ya,\nand H. Wan, \u201cDiff-rntraj: A structure-aware diffusion model for\nroad network-constrained trajectory generation,\u201d arXiv preprint\narXiv:2402.07369, 2024.\n[18]\nK. Shu, Y. Zhao, L. Wu, A. Liu, R. Qian, and X. Chen, \u201cData\naugmentation for seizure prediction with generative diffusion\nmodel,\u201d arXiv preprint arXiv:2306.08256, 2023.\n[19]\nI. Nicholas, H. Kuo, F. Garcia, A. Sonnerborg, M. Bohm, R. Kaiser,\nM. Zazzi, L. Jorm, and S. Barbieri, \u201cSynthetic health-related\nlongitudinal data with mixed-type variables generated using\ndiffusion models,\u201d in NeurIPS 2023 Workshop on Synthetic Data\nGeneration with Generative AI, 2023.\n[20]\nN. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan,\n\u201cWavegrad: Estimating gradients for waveform generation,\u201d\narXiv preprint arXiv:2009.00713, 2020.\n[21]\nK. Lakhotia, E. Kharitonov, W.-N. Hsu, Y. Adi, A. Polyak, B. Bolte,\nT.-A. Nguyen, J. Copet, A. Baevski, A. Mohamed et al., \u201cOn gener-\native spoken language modeling from raw audio,\u201d Transactions of\nthe Association for Computational Linguistics, vol. 9, pp. 1336\u20131354,\n2021.\n[22]\nA. H. Liu, W.-N. Hsu, M. Auli, and A. Baevski, \u201cTowards end-\nto-end unsupervised speech recognition,\u201d in 2022 IEEE Spoken\nLanguage Technology Workshop (SLT).\nIEEE, 2023, pp. 221\u2013228.\n[23]\nQ. Liu, F. Yan, X. Zhao, Z. Du, H. Guo, R. Tang, and F. Tian,\n\u201cDiffusion augmentation for sequential recommendation,\u201d in\nProceedings of the 32nd ACM International Conference on Information\nand Knowledge Management, 2023, pp. 1576\u20131586.\n[24]\nM. F. Sikder, R. Ramachandranpillai, and F. Heintz, \u201cTransfusion:\ngenerating long, high fidelity time series using diffusion models\nwith transformers,\u201d arXiv preprint arXiv:2307.12667, 2023.\n[25]\nY. Wang, Z. Wu, C. Li, and A. Wu, \u201cExtraction and recovery of\nspatio-temporal structure in latent dynamics alignment with dif-\nfusion model,\u201d Advances in Neural Information Processing Systems,\nvol. 36, 2024.\n[26]\nL. Huang, L. Gianinazzi, Y. Yu, P. D. Dueben, and T. Hoefler,\n\u201cDiffda: a diffusion model for weather-scale data assimilation,\u201d\narXiv preprint arXiv:2401.05932, 2024.\n[27]\nY. Hatanaka, Y. Glaser, G. Galgon, G. Torri, and P. Sadowski, \u201cDif-\nfusion models for high-resolution solar forecasts,\u201d arXiv preprint\narXiv:2302.00170, 2023.\n[28]\nZ. Wang, Q. Wen, C. Zhang, L. Sun, and Y. Wang, \u201cDiffload:\nuncertainty quantification in load forecasting with diffusion\nmodel,\u201d arXiv preprint arXiv:2306.01001, 2023.\n[29]\nS. R. Cachay, B. Zhao, H. James, and R. Yu, \u201cDyffusion: A\ndynamics-informed diffusion model for spatiotemporal forecast-\ning,\u201d arXiv preprint arXiv:2306.01984, 2023.\n[30]\nH. Yuan, S. Zhou, and S. Yu, \u201cEhrdiff: Exploring realistic ehr\nsynthesis with diffusion models,\u201d arXiv preprint arXiv:2303.05656,\n2023.\n[31]\nT. Ceritli, G. O. Ghosheh, V. K. Chauhan, T. Zhu, A. P. Creagh, and\nD. A. Clifton, \u201cSynthesizing mixed-type electronic health records\nusing diffusion models,\u201d arXiv preprint arXiv:2302.14679, 2023.\n[32]\nJ. Richter, S. Welker, J.-M. Lemercier, B. Lay, and T. Gerkmann,\n\u201cSpeech enhancement and dereverberation with diffusion-based\ngenerative models,\u201d IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 2023.\n[33]\nS. Zheng and N. Charoenphakdee, \u201cDiffusion models for\nmissing value imputation in tabular data,\u201d arXiv preprint\narXiv:2210.17128, 2022.\n[34]\nL. Shen and J. Kwok, \u201cNon-autoregressive conditional dif-\nfusion\nmodels\nfor\ntime\nseries\nprediction,\u201d\narXiv\npreprint\narXiv:2306.05043, 2023.\n[35]\nY. Zheng, L. Zhong, S. Wang, Y. Yang, W. Gu, J. Zhang, and\nJ. Wang, \u201cDiffuflow: Robust fine-grained urban flow inference\nwith denoising diffusion model,\u201d in Proceedings of the 32nd ACM\nInternational Conference on Information and Knowledge Management,\n2023, pp. 3505\u20133513.\n[36]\nY. Wang, Z. Liu, L. Yang, and P. S. Yu, \u201cConditional denois-\ning diffusion for sequential recommendation,\u201d arXiv preprint\narXiv:2304.11433, 2023.\n[37]\nC. Lee, T. Yang, Z. Chen, Y. Su, and M. R. Lyu, \u201cMaat: Perfor-\nmance metric anomaly anticipation for cloud services with condi-\ntional diffusion,\u201d in 2023 38th IEEE/ACM International Conference\non Automated Software Engineering (ASE).\nIEEE, 2023, pp. 116\u2013\n128.\n[38]\nY. Zhu, C. Wang, and H. Xiong, \u201cTowards graph-aware dif-\nfusion modeling for collaborative filtering,\u201d arXiv\npreprint\narXiv:2311.08744, 2023.\n[39]\nN. Sivaroopan, D. Bandara, C. Madarasingha, G. Jourjon, A. Jaya-\nsumana, and K. Thilakarathna, \u201cNetdiffus: Network traffic gen-\neration by diffusion models through time-series imaging,\u201d arXiv\npreprint arXiv:2310.04429, 2023.\n[40]\nT. Sattarov, M. Schreyer, and D. Borth, \u201cFindiff: Diffusion models\nfor financial tabular data generation,\u201d in Proceedings of the Fourth\nACM International Conference on AI in Finance, 2023, pp. 64\u201372.\n[41]\nD. Daiya, M. Yadav, and H. S. Rao, \u201cDiffstock: Probabilistic\nrelational stock market predictions using diffusion models,\u201d\nin ICASSP 2024-2024 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP). IEEE, 2024, pp. 7335\u20137339.\n[42]\nG. Klein, P. Guetschel, G. Silvestri, and M. Tangermann, \u201cSynthe-\nsizing eeg signals from event-related potential paradigms with\nconditional diffusion models,\u201d arXiv preprint arXiv:2403.18486,\n2024.\n[43]\nH. Wen, Y. Lin, Y. Xia, H. Wan, Q. Wen, R. Zimmermann, and\nY. Liang, \u201cDiffstg: Probabilistic spatio-temporal graph forecasting\nwith denoising diffusion models,\u201d in Proceedings of the 31st ACM\nInternational Conference on Advances in Geographic Information Sys-\ntems, 2023, pp. 1\u201312.\n[44]\nG. Chi, Z. Yang, C. Wu, J. Xu, Y. Gao, Y. Liu, and T. X. Han, \u201cRf-\ndiffusion: Radio signal generation via time-frequency diffusion,\u201d\narXiv preprint arXiv:2404.09140, 2024.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n22\n[45]\nF. Yang, W. Yin, L. Wang, T. Li, P. Zhao, B. Liu, P. Wang, B. Qiao,\nY. Liu, M. Bj\u00a8orkman et al., \u201cDiffusion-based time series data\nimputation for cloud failure prediction at microsoft 365,\u201d in\nProceedings of the 31st ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineer-\ning, 2023, pp. 2050\u20132055.\n[46]\nJ. Sui, J. Yu, Y. Song, and J. Zhang, \u201cAnomaly detection for\ntelemetry time series using a denoising diffusion probabilistic\nmodel,\u201d IEEE Sensors Journal, 2024.\n[47]\nK. J. Koa, Y. Ma, R. Ng, and T.-S. Chua, \u201cDiffusion variational\nautoencoder for tackling stochasticity in multi-step regression\nstock price prediction,\u201d in Proceedings of the 32nd ACM Interna-\ntional Conference on Information and Knowledge Management, 2023,\npp. 1087\u20131096.\n[48]\nH. He, S. Zhao, Y. Xi, and J. C. Ho, \u201cMeddiff: Generating\nelectronic health records using accelerated denoising diffusion\nmodel,\u201d arXiv preprint arXiv:2302.04355, 2023.\n[49]\nP. Chang, H. Li, S. F. Quan, S. Lu, S.-F. Wung, J. Roveda, and A. Li,\n\u201cA transformer-based diffusion probabilistic model for heart rate\nand blood pressure forecasting in intensive care unit,\u201d Computer\nMethods and Programs in Biomedicine, p. 108060, 2024.\n[50]\nY. Li, I. Y. Liao, N. Zhong, F. Toshihiro, Y. Wang, and S. Wang,\n\u201cGenerative ai enables the detection of autism using eeg signals,\u201d\nin Chinese Conference on Biometric Recognition.\nSpringer, 2023, pp.\n375\u2013384.\n[51]\nX. Li, M. Sakevych, G. Atkinson, and V. Metsis, \u201cBiodiffusion: A\nversatile diffusion model for biomedical signal synthesis,\u201d arXiv\npreprint arXiv:2401.10282, 2024.\n[52]\nY. Gao, H. Chen, X. Wang, Z. Wang, X. Wang, J. Gao, and\nB. Ding, \u201cDiffsformer: A diffusion transformer on stock factor\naugmentation,\u201d arXiv preprint arXiv:2402.06656, 2024.\n[53]\nJ. Yan, P. Li, and Y. Huang, \u201cA short-term wind power scenario\ngeneration method based on conditional diffusion model,\u201d in\n2023 IEEE Sustainable Power and Energy Conference (iSPEC). IEEE,\n2023, pp. 1\u20136.\n[54]\nM. Seki, Y.-Z. Zhang, and S. Imoto, \u201cImputing time-series micro-\nbiome abundance profiles with diffusion model,\u201d in 2023 IEEE\nInternational Conference on Bioinformatics and Biomedicine (BIBM).\nIEEE, 2023, pp. 914\u2013919.\n[55]\nD. Scassola, S. Saccani, G. Carbone, and L. Bortolussi, \u201cCondi-\ntioning score-based generative models by neuro-symbolic con-\nstraints,\u201d arXiv preprint arXiv:2308.16534, 2023.\n[56]\nY. Tashiro, J. Song, Y. Song, and S. Ermon, \u201cCsdi: Conditional\nscore-based diffusion models for probabilistic time series impu-\ntation,\u201d Advances in Neural Information Processing Systems, vol. 34,\npp. 24 804\u201324 816, 2021.\n[57]\nH. Li, G. Ditzler, J. Roveda, and A. Li, \u201cDescod-ecg: Deep\nscore-based diffusion model for ecg baseline wander and noise\nremoval,\u201d IEEE Journal of Biomedical and Health Informatics, 2023.\n[58]\nW. Zhao, Z. Chen, P. Xie, J. Liu, S. Hou, L. Xu, Y. Qiu, D. Wu,\nJ. Xiao, and K. He, \u201cMulti-task oriented diffusion model for\nmortality prediction in shock patients with incomplete data,\u201d\nInformation Fusion, vol. 105, p. 102207, 2024.\n[59]\nG. Liang, P. Tiwari, S. Nowaczyk, S. Byttner, and F. Alonso-\nFernandez,\n\u201cDynamic\ncausal\nexplanation\nbased\ndiffusion-\nvariational graph neural network for spatio-temporal forecast-\ning,\u201d arXiv preprint arXiv:2305.09703, 2023.\n[60]\nC. Chu, H. Zhang, P. Wang, and F. Lu, \u201cSimulating human mo-\nbility with a trajectory generation framework based on diffusion\nmodel,\u201d International Journal of Geographical Information Science,\npp. 1\u201332, 2024.\n[61]\nM. Guimar\u02dcaes, C. Soares, and C. Manfletti, \u201cPredicting the po-\nsition uncertainty at the time of closest approach with diffusion\nmodels,\u201d arXiv preprint arXiv:2311.05417, 2023.\n[62]\nJ. Jiang, C. Han, W. X. Zhao, and J. Wang, \u201cPdformer: Propa-\ngation delay-aware dynamic long-range transformer for traffic\nflow prediction,\u201d in Proceedings of the AAAI conference on artificial\nintelligence, vol. 37, no. 4, 2023, pp. 4365\u20134373.\n[63]\nY. Y. Choi, M. Lee, S. W. Park, S. Lee, and J. Ko, \u201cA gated\nmlp architecture for learning topological dependencies in spatio-\ntemporal graphs,\u201d arXiv preprint arXiv:2401.15894, 2024.\n[64]\nJ. Wiederer, A. Bouazizi, M. Troina, U. Kressel, and V. Belagiannis,\n\u201cAnomaly detection in multi-agent trajectories for automated\ndriving,\u201d in Conference on Robot Learning.\nPMLR, 2022, pp. 1223\u2013\n1233.\n[65]\nY. Lin, H. Wan, J. Hu, S. Guo, B. Yang, Y. Lin, and C. S. Jensen,\n\u201cOrigin-destination travel time oracle for map-based services,\u201d\nProceedings of the ACM on Management of Data, vol. 1, no. 3, pp.\n1\u201327, 2023.\n[66]\nY. Yao, Y. Liu, X. Dai, S. Chen, and Y. Lv, \u201cA graph-based scene en-\ncoder for vehicle trajectory prediction using the diffusion model,\u201d\nin 2023 International Annual Conference on Complex Systems and\nIntelligent Science (CSIS-IAC), 2023, pp. 981\u2013986.\n[67]\nI. Kotseruba and J. K. Tsotsos, \u201cData limitations for mod-\neling top-down effects on drivers\u2019 attention,\u201d arXiv preprint\narXiv:2404.08749, 2024.\n[68]\nK. Ito and L. Johnson, \u201cThe lj speech dataset,\u201d https://keithito.\ncom/LJ-Speech-Dataset/, 2017.\n[69]\nT. Yun, H. Jung, and J. Son, \u201cImputation as inpainting: Diffusion\nmodels for spatiotemporal data imputation,\u201d 2023.\n[70]\nY. Yuan, J. Ding, C. Shao, D. Jin, and Y. Li, \u201cSpatio-temporal\ndiffusion point processes,\u201d arXiv preprint arXiv:2305.12403, 2023.\n[71]\nT. Yan, H. Zhang, T. Zhou, Y. Zhan, and Y. Xia, \u201cScore-\ngrad: Multivariate probabilistic time series forecasting with\ncontinuous energy-based generative models,\u201d arXiv preprint\narXiv:2106.10121, 2021.\n[72]\nS. Zhang, S. Wang, X. Tan, R. Liu, J. Zhang, and J. Wang, \u201csasdim:\nself-adaptive noise scaling diffusion model for spatial time series\nimputation,\u201d arXiv preprint arXiv:2309.01988, 2023.\n[73]\nY. Cheng, K. Yamashita, J. Follum, and N. Yu, \u201cAdversarial\npurification for data-driven power system event classifiers with\ndiffusion models,\u201d arXiv preprint arXiv:2311.07110, 2023.\n[74]\nN. Neifar, A. Ben-Hamadou, A. Mdhaffar, and M. Jmaiel, \u201cDif-\nfecg: A generalized probabilistic diffusion model for ecg signals\nsynthesis,\u201d arXiv preprint arXiv:2306.01875, 2023.\n[75]\nF. Bao, C. Li, J. Sun, and J. Zhu, \u201cWhy are conditional gen-\nerative models better than unconditional ones?\u201d arXiv preprint\narXiv:2212.00362, 2022.\n[76]\nM. Jin, Q. Wen, Y. Liang, C. Zhang, S. Xue, X. Wang, J. Zhang,\nY. Wang, H. Chen, X. Li et al., \u201cLarge models for time series\nand spatio-temporal data: A survey and outlook,\u201d arXiv preprint\narXiv:2310.10196, 2023.\n[77]\nM. Jin, H. Y. Koh, Q. Wen, D. Zambon, C. Alippi, G. I. Webb,\nI. King, and S. Pan, \u201cA survey on graph neural networks for\ntime series: Forecasting, classification, imputation, and anomaly\ndetection,\u201d arXiv preprint arXiv:2307.03759, 2023.\n[78]\nK. Chen, G. Li, H. Li, Y. Wang, W. Wang, Q. Liu, and H. Wang,\n\u201cQuantifying uncertainty: Air quality forecasting based on dy-\nnamic spatial-temporal denoising diffusion probabilistic model,\u201d\nEnvironmental Research, p. 118438, 2024.\n[79]\nY. Tang, H. He, Y. Wang, and Y. Wu, \u201cUtilizing a diffusion model\nfor pedestrian trajectory prediction in semi-open autonomous\ndriving environments,\u201d IEEE Sensors Journal, pp. 1\u20131, 2024.\n[80]\nT.\nWestny,\nB.\nOlofsson,\nand\nE.\nFrisk,\n\u201cDiffusion-based\nenvironment-aware\ntrajectory\nprediction,\u201d\narXiv\npreprint\narXiv:2403.11643, 2024.\n[81]\nY. Zhu, Y. Ye, S. Zhang, X. Zhao, and J. Yu, \u201cDifftraj: Generating\ngps trajectory with diffusion probabilistic model,\u201d Advances in\nNeural Information Processing Systems, vol. 36, 2024.\n[82]\nS. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, \u201cAttention based\nspatial-temporal graph convolutional networks for traffic flow\nforecasting,\u201d in Proceedings of the AAAI Conference on Artificial\nIntelligence, vol. 33, 2019, pp. 922\u2013929.\n[83]\nC. Li, G. Feng, Y. Li, R. Liu, Q. Miao, and L. Chang, \u201cDifftad:\nDenoising diffusion probabilistic models for vehicle trajectory\nanomaly detection,\u201d Knowledge-Based Systems, vol. 286, p. 111387,\n2024.\n[84]\nL. Li, R. Carver, I. Lopez-Gomez, F. Sha, and J. Anderson, \u201cSeeds:\nEmulation of weather forecast ensembles with diffusion models,\u201d\narXiv preprint arXiv:2306.14066, 2023.\n[85]\nX. Ling, C. Li, F. Qin, P. Yang, and Y. Huang, \u201cSrndiff: Short-\nterm rainfall nowcasting with condition diffusion model,\u201d arXiv\npreprint arXiv:2402.13737, 2024.\n[86]\nY. Li, X. Lu, Y. Wang, and D. Dou, \u201cGenerative time series fore-\ncasting with diffusion, denoise, and disentanglement,\u201d Advances\nin Neural Information Processing Systems, vol. 35, pp. 23 009\u201323 022,\n2022.\n[87]\nOpenAI\nSora,\n\u201cVideo\ngeneration\nmodels\nas\nworld\nsimulators,\u201d\nhttps://openai.com/research/\nvideo-generation-models-as-world-simulators, accessed: 2024-\n04-29.\n[88]\nL. Lin, D. Shi, A. Han, and J. Gao, \u201cSpecstg: A fast spectral\ndiffusion framework for probabilistic spatio-temporal traffic fore-\ncasting,\u201d arXiv preprint arXiv:2401.08119, 2024.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n23\n[89]\nS. Kim, Y.-E. Lee, S.-H. Lee, and S.-W. Lee, \u201cDiff-e: Diffusion-\nbased learning for decoding imagined speech eeg,\u201d arXiv preprint\narXiv:2307.14389, 2023.\n[90]\nH. Wang, \u201cDiffuseroll: Multi-track multi-category music genera-\ntion based on diffusion model,\u201d arXiv preprint arXiv:2303.07794,\n2023.\n[91]\nZ. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDif-\nfwave: A versatile diffusion model for audio synthesis,\u201d arXiv\npreprint arXiv:2009.09761, 2020.\n[92]\nJ. Zhang, G. Fazekas, and C. Saitis, \u201cFast diffusion gan model\nfor symbolic music generation controlled by emotions,\u201d arXiv\npreprint arXiv:2310.14040, 2023.\n[93]\nJ. Leinonen, U. Hamann, D. Nerini, U. Germann, and G. Franch,\n\u201cLatent diffusion models for generative precipitation nowcast-\ning with accurate uncertainty quantification,\u201d arXiv preprint\narXiv:2304.12891, 2023.\n[94]\nR. Kurinchi-Vendhan, \u201cWirediff: a wind resolution-enhancing\ndiffusion model.\u201d\n[95]\nZ. Li, A. Sun, and C. Li, \u201cDiffurec: A diffusion model for sequen-\ntial recommendation,\u201d arXiv preprint arXiv:2304.00686, 2023.\n[96]\nS. Li, H. Xiong, and Y. Chen, \u201cDiffcharge: Generating ev charging\nscenarios via a denoising diffusion model,\u201d IEEE Transactions on\nSmart Grid, 2024.\n[97]\nC. Li, G. Feng, Y. Li, R. Liu, Q. Miao, and L. Chang, \u201cDifftad:\nDenoising diffusion probabilistic models for vehicle trajectory\nanomaly detection,\u201d Knowledge-Based Systems, vol. 286, p. 111387,\n2024.\n[98]\nX. Han, H. Zheng, and M. Zhou, \u201cCard: Classification and regres-\nsion diffusion models,\u201d Advances in Neural Information Processing\nSystems, vol. 35, pp. 18 100\u201318 115, 2022.\n[99]\nC. Yang, T. Wang, and X. Yan, \u201cDdmt: Denoising diffusion\nmask transformer models for multivariate time series anomaly\ndetection,\u201d arXiv preprint arXiv:2310.08800, 2023.\n[100] Y. Duan, J. Zhou, Z. Wang, Y.-C. Chang, Y.-K. Wang, and C.-T.\nLin, \u201cDomain-specific denoising diffusion probabilistic models\nfor brain dynamics,\u201d arXiv preprint arXiv:2305.04200, 2023.\n[101] D. Solis-Martin, J. Galan-Paez, and J. Borrego-Diaz, \u201cD3a-\nts: Denoising-driven data augmentation in time series,\u201d arXiv\npreprint arXiv:2312.05550, 2023.\n[102] Y. Dai, C. Yang, K. Liu, A. Liu, and Y. Liu, \u201cTimeddpm: Time\nseries augmentation strategy for industrial soft sensing,\u201d IEEE\nSensors Journal, 2023.\n[103] H. Yi, L. Hou, Y. Jin, and N. A. Saeed, \u201cTime series diffusion\nmethod: A denoising diffusion probabilistic model for vibration\nsignal generation,\u201d arXiv preprint arXiv:2312.07981, 2023.\n[104] M.\nBilo\u02c7s,\nK.\nRasul,\nA.\nSchneider,\nY.\nNevmyvaka,\nand\nS. G\u00a8unnemann, \u201cModeling temporal data as continuous func-\ntions with process diffusion,\u201d arXiv preprint arXiv:2211.02590,\n2022.\n[105] J.-M. Lemercier, J. Richter, S. Welker, and T. Gerkmann, \u201cStorm:\nA diffusion-based stochastic regeneration model for speech en-\nhancement and dereverberation,\u201d IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, 2023.\n[106] J. Crabb\u00b4e, N. Huynh, J. Stanczuk, and M. van der Schaar,\n\u201cTime series diffusion in the frequency domain,\u201d arXiv preprint\narXiv:2402.05933, 2024.\n[107] X. Dong, Z. Mao, Y. Sun, and X. Xu, \u201cShort-term wind power sce-\nnario generation based on conditional latent diffusion models,\u201d\nIEEE Transactions on Sustainable Energy, 2023.\n[108] Z. Zhang, H. Yang, J. Chen, and Z. Yin, \u201cMulti-scale conditional\ndiffusion model for deposited droplet volume measurement in\ninkjet printing manufacturing,\u201d Journal of Manufacturing Systems,\nvol. 71, pp. 595\u2013608, 2023.\n[109] S. Kim, S.-H. Lee, Y.-E. Lee, J.-W. Lee, J.-H. Park, and S.-W. Lee,\n\u201cBrain-driven representation learning based on diffusion model,\u201d\narXiv preprint arXiv:2311.07925, 2023.\n[110] G. Sharma, A. Dhall, and R. Subramanian, \u201cMedic: Mitigating\neeg data scarcity via class-conditioned diffusion model,\u201d in Deep\nGenerative Models for Health Workshop NeurIPS 2023, 2023.\n[111] M. Liu, H. Huang, H. Feng, L. Sun, B. Du, and Y. Fu, \u201cPristi: A\nconditional diffusion framework for spatiotemporal imputation,\u201d\narXiv preprint arXiv:2302.09746, 2023.\n[112] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\nK. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans\net al., \u201cPhotorealistic text-to-image diffusion models with deep\nlanguage understanding,\u201d Advances in Neural Information Process-\ning Systems, vol. 35, pp. 36 479\u201336 494, 2022.\n[113] Y. Ouyang, L. Xie, C. Li, and G. Cheng, \u201cMissdiff: Training\ndiffusion models on tabular data with missing values,\u201d in ICML\n2023 Workshop on Structured Probabilistic Inference {\\&} Generative\nModeling, 2023.\n[114] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, \u201cDiffusion\nmodels in vision: A survey,\u201d IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023.\n[115] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive\nsurvey of methods and applications,\u201d ACM Computing Surveys,\nvol. 56, no. 4, pp. 1\u201339, 2023.\n[116] H. Lim, M. Kim, S. Park, and N. Park, \u201cRegular time-series\ngeneration using sgm,\u201d arXiv preprint arXiv:2301.08518, 2023.\n[117] A. Coletta, S. Gopalakrishnan, D. Borrajo, and S. Vyetrenko,\n\u201cOn the constrained time-series generation problem,\u201d Advances\nin Neural Information Processing Systems, vol. 36, 2024.\n[118] W. Liu, J.-L. Qiu, W.-L. Zheng, and B.-L. Lu, \u201cComparing recog-\nnition performance and robustness of multimodal deep learning\nmodels for multimodal emotion recognition,\u201d IEEE Transactions\non Cognitive and Developmental Systems, vol. 14, no. 2, pp. 715\u2013\n729, 2021.\n[119] G. Tosato, C. M. Dalbagno, and F. Fumagalli, \u201cEeg synthetic data\ngeneration using probabilistic diffusion models,\u201d arXiv preprint\narXiv:2303.06068, 2023.\n[120] P. Laguna, R. G. Mark, A. Goldberg, and G. B. Moody, \u201cA\ndatabase for evaluation of algorithms for measurement of qt and\nother waveform intervals in the ecg,\u201d in Computers in cardiology\n1997.\nIEEE, 1997, pp. 673\u2013676.\n[121] G. B. Moody and R. G. Mark, \u201cThe mit-bih arrhythmia database\non cd-rom and software for use with it,\u201d in [1990] Proceedings\nComputers in Cardiology.\nIEEE, 1990, pp. 185\u2013188.\n[122] R. Bousseljot and D. Kreiseler, \u201cWaveform recognition with\n10,000 ecgs,\u201d in Computers in Cardiology 2000. Vol. 27 (Cat.\n00CH37163).\nIEEE, 2000, pp. 331\u2013334.\n[123] J. M. L. Alcaraz and N. Strodthoff, \u201cDiffusion-based conditional\necg generation with structured state space models,\u201d Computers in\nBiology and Medicine, p. 107115, 2023.\n[124] J. F. N\u00b4u\u02dcnez, J. Arjona, A. Tormos, D. Garc\u00b4\u0131a, and J. B\u00b4ejar, \u201cApply-\ning generative models and transfer learning to physiological data\nclassification,\u201d in Artificial Intelligence Research and Development.\nIOS Press, 2023, pp. 28\u201337.\n[125] G. B. Moody and R. G. Mark, \u201cThe impact of the mit-bih arrhyth-\nmia database,\u201d IEEE engineering in medicine and biology magazine,\nvol. 20, no. 3, pp. 45\u201350, 2001.\n[126] A. Miltiadous, K. D. Tzimourta, T. Afrantou, P. Ioannidis,\nN. Grigoriadis, D. G. Tsalikakis, P. Angelidis, M. G. Tsipouras,\nE. Glavas, N. Giannakeas et al., \u201cA dataset of scalp eeg recordings\nof alzheimer\u2019s disease, frontotemporal dementia and healthy\nsubjects from routine eeg,\u201d Data, vol. 8, no. 6, p. 95, 2023.\n[127] B. Kemp, A. H. Zwinderman, B. Tuk, H. A. Kamphuisen, and J. J.\nOberye, \u201cAnalysis of a sleep-dependent neuronal feedback loop:\nthe slow-wave microcontinuity of the eeg,\u201d IEEE Transactions on\nBiomedical Engineering, vol. 47, no. 9, pp. 1185\u20131194, 2000.\n[128] B. Aristimunha, R. Y. de Camargo, S. Chevallier, O. Lucena, A. G.\nThomas, M. J. Cardoso, W. H. L. Pinaya, and J. Dafflon, \u201cSynthetic\nsleep eeg signal generation using latent diffusion models,\u201d in\nDeep Generative Models for Health Workshop NeurIPS 2023, 2023.\n[129] G.-Q. Zhang, L. Cui, R. Mueller, S. Tao, M. Kim, M. Rueschman,\nS. Mariani, D. Mobley, and S. Redline, \u201cThe national sleep re-\nsearch resource: towards a sleep data commons,\u201d Journal of the\nAmerican Medical Informatics Association, vol. 25, no. 10, pp. 1351\u2013\n1358, 2018.\n[130] G. B\u00b4en\u00b4edict, O. Jeunen, S. Papa, S. Bhargav, D. Odijk, and\nM. de Rijke, \u201cRecfusion: A binomial diffusion process for 1d data\nfor recommendation,\u201d arXiv preprint arXiv:2306.08947, 2023.\n[131] X. Lin, X. Chen, C. Wang, H. Shu, L. Song, B. Li et al., \u201cDiscrete\nconditional diffusion for reranking in recommendation,\u201d arXiv\npreprint arXiv:2308.06982, 2023.\n[132] W. Wang, Y. Xu, F. Feng, X. Lin, X. He, and T.-S. Chua, \u201cDiffusion\nrecommender model,\u201d arXiv preprint arXiv:2304.04971, 2023.\n[133] F. M. Harper and J. A. Konstan, \u201cThe movielens datasets: History\nand context,\u201d Acm transactions on interactive intelligent systems\n(tiis), vol. 5, no. 4, pp. 1\u201319, 2015.\n[134] H. Du, H. Yuan, Z. Huang, P. Zhao, and X. Zhou, \u201cSequen-\ntial recommendation with diffusion models,\u201d arXiv preprint\narXiv:2304.04541, 2023.\n[135] H. Ma, R. Xie, L. Meng, X. Chen, X. Zhang, L. Lin, and Z. Kang,\n\u201cPlug-in diffusion model for sequential recommendation,\u201d arXiv\npreprint arXiv:2401.02913, 2024.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n24\n[136] H. Hersbach, B. Bell, P. Berrisford, S. Hirahara, A. Hor\u00b4anyi,\nJ. Mu\u02dcnoz-Sabater, J. Nicolas, C. Peubey, R. Radu, D. Schepers\net al., \u201cThe era5 global reanalysis,\u201d Quarterly Journal of the Royal\nMeteorological Society, vol. 146, no. 730, pp. 1999\u20132049, 2020.\n[137] L. Chuyao, X. Li, and Y. Ye, \u201cPfst-lstm: A spatiotemporal lstm\nmodel with pseudoflow prediction for precipitation nowcasting,\u201d\nIEEE Journal of Selected Topics in Applied Earth Observations and\nRemote Sensing, vol. PP, no. 99, pp. 1\u20131, 2020.\n[138] P. Sepulchre, A. Caubel, J.-B. Ladant, L. Bopp, O. Boucher, P. Bra-\nconnot, P. Brockmann, A. Cozic, Y. Donnadieu, J.-L. Dufresne\net al., \u201cIpsl-cm5a2\u2013an earth system model designed for multi-\nmillennial climate simulations,\u201d Geoscientific Model Development,\nvol. 13, no. 7, pp. 3011\u20133053, 2020.\n[139] T. Hong, P. Pinson, S. Fan, H. Zareipour, A. Troccoli, and R. J.\nHyndman, \u201cProbabilistic energy forecasting: Global energy fore-\ncasting competition 2014 and beyond,\u201d pp. 896\u2013913, 2016.\n[140] Z. J. Lee, T. Li, and S. H. Low, \u201cAcn-data: Analysis and applica-\ntions of an open ev charging dataset,\u201d in Proceedings of the tenth\nACM international conference on future energy systems, 2019, pp.\n139\u2013149.\n[141] J. R. Schofield, R. Carmichael, S. Tindemans, M. Bilton, M. Woolf,\nG. Strbac et al., \u201cLow carbon london project: Data from the dy-\nnamic time-of-use electricity pricing trial, 2013,\u201d uK Data Service,\nSN, vol. 7857, no. 2015, pp. 7857\u20137851, 2015.\n[142] Z. Wang and H. Zhang, \u201cCustomized load profiles synthesis\nfor electricity customers based on conditional diffusion models,\u201d\narXiv preprint arXiv:2304.12076, 2023.\n[143] S. Li, H. Xiong, and Y. Chen, \u201cDiffplf: A conditional diffusion\nmodel for probabilistic forecasting of ev charging load,\u201d arXiv\npreprint arXiv:2402.13548, 2024.\n[144] A. Jenkins, Z. Chen, F. S. Ng, and D. Mandic, \u201cImproving dif-\nfusion models for ecg imputation with an augmented template\nprior,\u201d arXiv preprint arXiv:2310.15742, 2023.\n[145] L. Liu, Y. Ren, Z. Lin, and Z. Zhao, \u201cPseudo numerical\nmethods for diffusion models on manifolds,\u201d arXiv preprint\narXiv:2202.09778, 2022.\n[146] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, \u201cDpm-solver:\nA fast ode solver for diffusion probabilistic model sampling\nin around 10 steps,\u201d Advances in Neural Information Processing\nSystems, vol. 35, pp. 5775\u20135787, 2022.\n[147] X. Mao, \u201cThe truncated euler\u2013maruyama method for stochastic\ndifferential equations,\u201d Journal of Computational and Applied Math-\nematics, vol. 290, pp. 370\u2013384, 2015.\n[148] P. Vincent, \u201cA connection between score matching and denoising\nautoencoders,\u201d Neural computation, vol. 23, no. 7, pp. 1661\u20131674,\n2011.\n[149] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson,\nV. Manohar, Y. Adi, J. Mahadeokar et al., \u201cVoicebox: Text-guided\nmultilingual universal speech generation at scale,\u201d Advances in\nneural information processing systems, vol. 36, 2024.\n[150] D. Epstein, A. Jabri, B. Poole, A. Efros, and A. Holynski, \u201cDiffu-\nsion self-guidance for controllable image generation,\u201d Advances\nin Neural Information Processing Systems, vol. 36, 2024.\n[151] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional\nnetworks for biomedical image segmentation,\u201d in Medical Image\nComputing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th\nInternational Conference, Munich, Germany, October 5-9, 2015, Pro-\nceedings, Part III 18.\nSpringer, 2015, pp. 234\u2013241.\n[152] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d\nAdvances in neural information processing systems, vol. 30, 2017.\n[153] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. Mc-\nGrew, I. Sutskever, and M. Chen, \u201cGlide: Towards photorealistic\nimage generation and editing with text-guided diffusion mod-\nels,\u201d arXiv preprint arXiv:2112.10741, 2021.\n[154] C. Meng, R. Rombach, R. Gao, D. Kingma, S. Ermon, J. Ho,\nand T. Salimans, \u201cOn distillation of guided diffusion models,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 14 297\u201314 306.\n[155] M. Hu, Y. Wang, T.-J. Cham, J. Yang, and P. N. Suganthan, \u201cGlobal\ncontext with discrete diffusion in vector quantised modelling for\nimage generation,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 11 502\u201311 511.\n[156] J. Wolleb, F. Bieder, R. Sandk\u00a8uhler, and P. C. Cattin, \u201cDiffusion\nmodels for medical anomaly detection,\u201d in International Confer-\nence on Medical image computing and computer-assisted intervention.\nSpringer, 2022, pp. 35\u201345.\n[157] S. Chen, P. Sun, Y. Song, and P. Luo, \u201cDiffusiondet: Diffusion\nmodel for object detection,\u201d in Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, 2023, pp. 19 830\u201319 843.\n[158] D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and\nA. Babenko, \u201cLabel-efficient semantic segmentation with diffu-\nsion models,\u201d arXiv preprint arXiv:2112.03126, 2021.\n[159] C.-H. Chao, W.-F. Sun, B.-W. Cheng, and C.-Y. Lee, \u201cQuasi-\nconservative score-based generative models,\u201d arXiv preprint\narXiv:2209.12753, 2022.\n[160] H. Chung, B. Sim, and J. C. Ye, \u201cCome-closer-diffuse-faster:\nAccelerating conditional diffusion models for inverse problems\nthrough stochastic contraction,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n12 413\u201312 422.\n[161] J. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon, \u201cIlvr: Condition-\ning method for denoising diffusion probabilistic models,\u201d arXiv\npreprint arXiv:2108.02938, 2021.\n[162] A. Bansal, H.-M. Chu, A. Schwarzschild, S. Sengupta, M. Gold-\nblum, J. Geiping, and T. Goldstein, \u201cUniversal guidance for\ndiffusion models,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 843\u2013852.\n[163] Y. Yuan, J. Song, U. Iqbal, A. Vahdat, and J. Kautz, \u201cPhysdiff:\nPhysics-guided human motion diffusion model,\u201d in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2023,\npp. 16 010\u201316 021.\n[164] M. Kollovieh, A. F. Ansari, M. Bohlke-Schneider, J. Zschiegner,\nH. Wang, and Y. B. Wang, \u201cPredict, refine, synthesize: Self-\nguiding diffusion models for probabilistic time series forecast-\ning,\u201d Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[165] S. An, H. Lee, J. Jo, S. Lee, and S. J. Hwang, \u201cDiffusionnag: Task-\nguided neural architecture generation with diffusion models,\u201d\narXiv preprint arXiv:2305.16943, 2023.\n[166] N. Torenvliet and J. Zelek, \u201cEvaluating diffusion models for the\nautomation of ultrasonic nondestructive evaluation data analy-\nsis,\u201d Algorithms, vol. 17, no. 4, p. 167, 2024.\n[167] Z. Yang, J. Wu, Z. Wang, X. Wang, Y. Yuan, and X. He, \u201cGen-\nerate what you prefer: Reshaping sequential recommendation\nvia guided diffusion,\u201d Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[168] P. Shao, J. Feng, J. Lu, P. Zhang, and C. Zou, \u201cData-driven and\nknowledge-guided denoising diffusion model for flood forecast-\ning,\u201d Expert Systems with Applications, vol. 244, p. 122908, 2024.\n[169] S. Zuo, V. F. Rey, S. Suh, S. Sigg, and P. Lukowicz, \u201cUnsupervised\nstatistical feature-guided diffusion model for sensor-based hu-\nman activity recognition,\u201d arXiv preprint arXiv:2306.05285, 2023.\n[170] J. Hu, X. Liu, Z. Fan, Y. Liang, and R. Zimmermann, \u201cTowards\nunifying diffusion models for probabilistic spatio-temporal graph\nlearning,\u201d arXiv preprint arXiv:2310.17360, 2023.\n[171] L. Yang, Z. Zhang, W. Zhang, and S. Hong, \u201cScore-based graph\ngenerative modeling with self-guided latent diffusion,\u201d 2022.\n[172] S. Gong, M. Li, J. Feng, Z. Wu, and L. Kong, \u201cDiffuseq: Sequence\nto sequence text generation with diffusion models,\u201d arXiv preprint\narXiv:2210.08933, 2022.\n[173] A. Schneuing, Y. Du, C. Harris, A. Jamasb, I. Igashov, W. Du,\nT. Blundell, P. Li\u00b4o, C. Gomes, M. Welling et al., \u201cStructure-based\ndrug design with equivariant diffusion models,\u201d arXiv preprint\narXiv:2210.13695, 2022.\n[174] B. D. Anderson, \u201cReverse-time diffusion equation models,\u201d\nStochastic Processes and their Applications, vol. 12, no. 3, pp. 313\u2013\n326, 1982.\n[175] Y.-J. Lu, Z.-Q. Wang, S. Watanabe, A. Richard, C. Yu, and\nY. Tsao, \u201cConditional diffusion probabilistic model for speech\nenhancement,\u201d in ICASSP 2022-2022 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2022,\npp. 7402\u20137406.\n[176] W. Tai, F. Zhou, G. Trajcevski, and T. Zhong, \u201cRevisiting de-\nnoising diffusion probabilistic models for speech enhancement:\nCondition collapse, efficiency and refinement,\u201d in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 37, no. 11, 2023,\npp. 13 627\u201313 635.\n[177] W. Tai, Y. Lei, F. Zhou, G. Trajcevski, and T. Zhong, \u201cDose:\nDiffusion dropout with adaptive prior for speech enhancement,\u201d\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[178] B. Lay, S. Welker, J. Richter, and T. Gerkmann, \u201cReducing the\nprior mismatch of stochastic differential equations for diffusion-\nbased speech enhancement,\u201d arXiv preprint arXiv:2302.14748,\n2023.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n25\n[179] Z. Qiu, Y. Guo, M. Fu, H. Huang, Y. Hu, L. He, and F. Sun, \u201cCra-\ndiffuse: Improved cross-domain speech enhancement based on\ndiffusion model with tf domain pre-denoising,\u201d in 2023 IEEE\nInternational Conference on Multimedia and Expo (ICME).\nIEEE,\n2023, pp. 1709\u20131714.\n[180] J. Serr`a, S. Pascual, J. Pons, R. O. Araz, and D. Scaini, \u201cUniversal\nspeech enhancement with score-based diffusion,\u201d arXiv preprint\narXiv:2206.03065, 2022.\n[181] H. Chen, X. Dong, Y. Kong, Z. Chen, S. Zheng, X. Hu, and\nX. Zhao, \u201cOnline prediction of mechanical and electrical quality\nin ultrasonic metal welding using time series generation and\ndeep learning,\u201d Engineering Failure Analysis, p. 108162, 2024.\n[182] S. S. Narasimhan, S. Agarwal, O. Akcin, S. Sanghavi, and\nS. Chinchali, \u201cTime weaver: A conditional time series generation\nmodel,\u201d arXiv preprint arXiv:2403.02682, 2024.\n[183] H. Shirzad, R. Deng, H. Zhao, and F. Tung, \u201cConditional diffusion\nmodels as self-supervised learning backbone for irregular time\nseries,\u201d in ICLR 2024 Workshop on Learning from Time Series For\nHealth, 2024.\n[184] C. Liu, S. He, H. Liu, and J. Chen, \u201cIntention-aware denois-\ning diffusion model for trajectory prediction,\u201d arXiv preprint\narXiv:2403.09190, 2024.\n[185] Z. Evans, J. D. Parker, C. Carr, Z. Zukowski, J. Taylor, and\nJ. Pons, \u201cLong-form music generation with latent diffusion,\u201d\narXiv preprint arXiv:2404.10301, 2024.\n[186] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d\narXiv preprint arXiv:1312.6114, 2013.\n[187] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-\nFarley, S. Ozair, A. Courville, and Y. Bengio, \u201cGenerative ad-\nversarial nets,\u201d Advances in neural information processing systems,\nvol. 27, 2014.\n[188] D. Rezende and S. Mohamed, \u201cVariational inference with nor-\nmalizing flows,\u201d in International conference on machine learning.\nPMLR, 2015, pp. 1530\u20131538.\n[189] Z. Chang, G. A. Koulieris, and H. P. Shum, \u201cOn the design\nfundamentals of diffusion models: A survey,\u201d arXiv preprint\narXiv:2306.04542, 2023.\n[190] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J.\nFleet, \u201cVideo diffusion models,\u201d Advances in Neural Information\nProcessing Systems, vol. 35, pp. 8633\u20138646, 2022.\n[191] C. Saharia, W. Chan, H. Chang, C. Lee, J. Ho, T. Salimans, D. Fleet,\nand M. Norouzi, \u201cPalette: Image-to-image diffusion models,\u201d in\nACM SIGGRAPH 2022 conference proceedings, 2022, pp. 1\u201310.\n[192] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and\nM. Norouzi, \u201cImage super-resolution via iterative refinement,\u201d\nIEEE transactions on pattern analysis and machine intelligence,\nvol. 45, no. 4, pp. 4713\u20134726, 2022.\n[193] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg,\n\u201cStructured denoising diffusion models in discrete state-spaces,\u201d\nAdvances in Neural Information Processing Systems, vol. 34, pp.\n17 981\u201317 993, 2021.\n[194] J. Lovelace, V. Kishore, C. Wan, E. Shekhtman, and K. Q. Wein-\nberger, \u201cLatent diffusion for language generation,\u201d Advances in\nNeural Information Processing Systems, vol. 36, 2024.\n[195] X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto,\n\u201cDiffusion-lm improves controllable text generation,\u201d Advances\nin Neural Information Processing Systems, vol. 35, pp. 4328\u20134343,\n2022.\n[196] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHi-\nerarchical text-conditional image generation with clip latents,\u201d\narXiv preprint arXiv:2204.06125, vol. 1, no. 2, p. 3, 2022.\n[197] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu,\n\u201cDiffsound: Discrete diffusion model for text-to-sound gener-\nation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, 2023.\n[198] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu,\n\u201cMotiondiffuse: Text-driven human motion generation with dif-\nfusion model,\u201d IEEE Transactions on Pattern Analysis and Machine\nIntelligence, 2024.\n[199] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding conditional control\nto text-to-image diffusion models,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023, pp. 3836\u20133847.\n[200] T. Amit, T. Shaharbany, E. Nachmani, and L. Wolf, \u201cSegdiff:\nImage segmentation with diffusion probabilistic models,\u201d arXiv\npreprint arXiv:2112.00390, 2021.\n[201] J. Wolleb, R. Sandk\u00a8uhler, F. Bieder, P. Valmaggia, and P. C. Cattin,\n\u201cDiffusion models for implicit image segmentation ensembles,\u201d\nin International Conference on Medical Imaging with Deep Learning.\nPMLR, 2022, pp. 1336\u20131348.\n[202] G. Nam, M. Khlifi, A. Rodriguez, A. Tono, L. Zhou, and P. Guer-\nrero, \u201c3d-ldm: Neural implicit 3d shape generation with latent\ndiffusion models,\u201d arXiv preprint arXiv:2212.00842, 2022.\n[203] A. Vahdat, F. Williams, Z. Gojcic, O. Litany, S. Fidler, K. Kreis\net al., \u201cLion: Latent point diffusion models for 3d shape genera-\ntion,\u201d Advances in Neural Information Processing Systems, vol. 35,\npp. 10 021\u201310 039, 2022.\n[204] M. Xu, L. Yu, Y. Song, C. Shi, S. Ermon, and J. Tang, \u201cGeodiff:\nA geometric diffusion model for molecular conformation gener-\nation,\u201d arXiv preprint arXiv:2203.02923, 2022.\n[205] Z. Guo, J. Liu, Y. Wang, M. Chen, D. Wang, D. Xu, and J. Cheng,\n\u201cDiffusion models in bioinformatics and computational biology,\u201d\nNature Reviews Bioengineering, pp. 1\u201319, 2023.\n[206] R. Huang, M. W. Lam, J. Wang, D. Su, D. Yu, Y. Ren, and Z. Zhao,\n\u201cFastdiff: A fast conditional diffusion model for high-quality\nspeech synthesis,\u201d arXiv preprint arXiv:2204.09934, 2022.\n[207] H. Harutyunyan, H. Khachatrian, D. C. Kale, G. Ver Steeg, and\nA. Galstyan, \u201cMultitask learning and benchmarking with clinical\ntime series data,\u201d Scientific data, vol. 6, no. 1, p. 96, 2019.\n[208] G. Jin, Y. Liang, Y. Fang, Z. Shao, J. Huang, J. Zhang, and\nY. Zheng, \u201cSpatio-temporal graph neural networks for predictive\nlearning in urban computing: A survey,\u201d IEEE Transactions on\nKnowledge and Data Engineering, 2023.\n[209] M. Jin, Y. Zhang, W. Chen, K. Zhang, Y. Liang, B. Yang, J. Wang,\nS. Pan, and Q. Wen, \u201cPosition paper: What can large lan-\nguage models tell us about time series analysis,\u201d arXiv preprint\narXiv:2402.02713, 2024.\n[210] S. Khanna, P. Liu, L. Zhou, C. Meng, R. Rombach, M. Burke,\nD. Lobell, and S. Ermon, \u201cDiffusionsat: A generative founda-\ntion model for satellite imagery,\u201d arXiv preprint arXiv:2312.03606,\n2023.\n[211] Y. Xu, Z. Liu, M. Tegmark, and T. Jaakkola, \u201cPoisson flow\ngenerative models,\u201d ArXiv, vol. abs/2209.11178, 2022.\n[212] Y. Xu, Z. Liu, Y. Tian, S. Tong, M. Tegmark, and T. Jaakkola,\n\u201cPfgm++: Unlocking the potential of physics-inspired generative\nmodels,\u201d ArXiv, vol. abs/2302.04265, 2023.\n[213] Z. Liu, D. Luo, Y. Xu, T. Jaakkola, and M. Tegmark, \u201cGen-\nphys: From physical processes to generative models,\u201d ArXiv, vol.\nabs/2304.02637, 2023.\n[214] A. Bansal,\nE. Borgnia, H.-M.\nChu, J.\nS. Li,\nH.\nKazemi,\nF. Huang, M. Goldblum, J. Geiping, and T. Goldstein, \u201cCold\ndiffusion: Inverting arbitrary image transforms without noise,\u201d\narXiv:2208.09392, 2022.\n[215] T.\nKarras,\nM.\nAittala,\nT.\nAila,\nand\nS.\nLaine,\n\u201cElucidat-\ning the design space of diffusion-based generative models,\u201d\narXiv:2206.00364, 2022.\n[216] Q. Zhang, M. Tao, and Y. Chen, \u201cgddim: Generalized denoising\ndiffusion implicit models,\u201d arXiv preprint arXiv:2206.05564, 2022.\n[217] Q. Zhang and Y. Chen, \u201cFast sampling of diffusion models with\nexponential integrator,\u201d arXiv preprint arXiv:2204.13902, 2022.\n[218] A. Jolicoeur-Martineau, K. Li, R. Pich\u00b4e-Taillefer, T. Kachman, and\nI. Mitliagkas, \u201cGotta go fast when generating data with score-\nbased models,\u201d arXiv preprint arXiv:2105.14080, 2021.\n[219] Y. Xu, M. Deng, X. Cheng, Y. Tian, Z. Liu, and T. Jaakkola,\n\u201cRestart sampling for improving generative processes,\u201d Advances\nin Neural Information Processing Systems, vol. 36, pp. 76 806\u201376 838,\n2023.\n[220] D. Berthelot, A. Autef, J. Lin, D. A. Yap, S. Zhai, S. Hu,\nD. Zheng, W. Talbott, and E. Gu, \u201cTract: Denoising diffusion\nmodels with transitive closure time-distillation,\u201d arXiv preprint\narXiv:2303.04248, 2023.\n[221] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, \u201cConsistency\nmodels,\u201d arXiv preprint arXiv:2303.01469, 2023.\n[222] H. Zheng, P. He, W. Chen, and M. Zhou, \u201cTruncated diffusion\nprobabilistic models,\u201d arXiv preprint arXiv:2202.09671, vol. 1, no.\n3.1, p. 2, 2022.\n[223] D. Watson, J. Ho, M. Norouzi, and W. Chan, \u201cLearning to\nefficiently sample from diffusion probabilistic models,\u201d arXiv\npreprint arXiv:2106.03802, 2021.\n[224] Z. Lyu, X. Xu, C. Yang, D. Lin, and B. Dai, \u201cAccelerating diffusion\nmodels via early stop of the diffusion process,\u201d arXiv preprint\narXiv:2205.12524, 2022.\n[225] K. Pandey, A. Mukherjee, P. Rai, and A. Kumar, \u201cDiffusevae:\nEfficient, controllable and high-fidelity generation from low-\ndimensional latents,\u201d arXiv preprint arXiv:2201.00308, 2022.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n26\n[226] X. Liu, C. Gong, and Q. Liu, \u201cFlow straight and fast: Learning\nto generate and transfer data with rectified flow,\u201d arXiv preprint\narXiv:2209.03003, 2022.\n[227] A. Vahdat, K. Kreis, and J. Kautz, \u201cScore-based generative mod-\neling in latent space,\u201d Advances in neural information processing\nsystems, vol. 34, pp. 11 287\u201311 302, 2021.\n[228] H. Zhang, R. Feng, Z. Yang, L. Huang, Y. Liu, Y. Zhang, Y. Shen,\nD. Zhao, J. Zhou, and F. Cheng, \u201cDimensionality-varying dif-\nfusion process,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 14 307\u201314 316.\n[229] C. Lu, Y. Zhou, F. Bao, J. Chen, C. Li, and J. Zhu, \u201cDpm-solver++:\nFast solver for guided sampling of diffusion probabilistic mod-\nels,\u201d arXiv preprint arXiv:2211.01095, 2022.\n[230] S. Luo, Y. Tan, L. Huang, J. Li, and H. Zhao, \u201cLatent consistency\nmodels: Synthesizing high-resolution images with few-step infer-\nence,\u201d arXiv preprint arXiv:2310.04378, 2023.\n[231] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and\nL. Van Gool, \u201cRepaint: Inpainting using denoising diffusion\nprobabilistic models,\u201d in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2022, pp. 11 461\u201311 471.\n[232] J. Zheng, M. Hu, Z. Fan, C. Wang, C. Ding, D. Tao, and\nT.-J. Cham, \u201cTrajectory consistency distillation,\u201d arXiv preprint\narXiv:2402.19159, 2024.\n[233] W. Zhao, L. Bai, Y. Rao, J. Zhou, and J. Lu, \u201cUnipc: A uni-\nfied predictor-corrector framework for fast sampling of diffu-\nsion models,\u201d Advances in Neural Information Processing Systems,\nvol. 36, 2024.\n[234] S. Gu, D. Chen, J. Bao, F. Wen, B. Zhang, D. Chen, L. Yuan,\nand B. Guo, \u201cVector quantized diffusion model for text-to-image\nsynthesis,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 10 696\u201310 706.\n[235] D. Kingma, T. Salimans, B. Poole, and J. Ho, \u201cVariational dif-\nfusion models,\u201d Advances in neural information processing systems,\nvol. 34, pp. 21 696\u201321 707, 2021.\n[236] B. Kawar, M. Elad, S. Ermon, and J. Song, \u201cDenoising diffusion\nrestoration models,\u201d Advances in Neural Information Processing\nSystems, vol. 35, pp. 23 593\u201323 606, 2022.\n[237] S. Rissanen, M. Heinonen, and A. Solin, \u201cGenerative modelling\nwith inverse heat dissipation,\u201d arXiv preprint arXiv:2206.13397,\n2022.\n[238] R. Yang, P. Srivastava, and S. Mandt, \u201cDiffusion probabilistic\nmodeling for video generation,\u201d Entropy, vol. 25, no. 10, p. 1469,\n2023.\n[239] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly\net al., \u201cAn image is worth 16x16 words: Transformers for image\nrecognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\n[240] Z. Liu, H. Hu, Y. Lin, Z. Yao, Z. Xie, Y. Wei, J. Ning, Y. Cao,\nZ. Zhang, L. Dong et al., \u201cSwin transformer v2: Scaling up\ncapacity and resolution,\u201d in Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, 2022, pp. 12 009\u201312 019.\n[241] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,\n\u201cSwin transformer: Hierarchical vision transformer using shifted\nwindows,\u201d in Proceedings of the IEEE/CVF international conference\non computer vision, 2021, pp. 10 012\u201310 022.\n[242] W. Peebles and S. Xie, \u201cScalable diffusion models with trans-\nformers,\u201d in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2023, pp. 4195\u20134205.\n[243] N. Ma, M. Goldstein, M. S. Albergo, N. M. Boffi, E. Vanden-\nEijnden, and S. Xie, \u201cSit: Exploring flow and diffusion-based\ngenerative models with scalable interpolant transformers,\u201d arXiv\npreprint arXiv:2401.08740, 2024.\n[244] E. Hoogeboom and T. Salimans, \u201cBlurring diffusion models,\u201d\narXiv preprint arXiv:2209.05557, 2022.\n[245] G. Daras, M. Delbracio, H. Talebi, A. G. Dimakis, and P. Milanfar,\n\u201cSoft diffusion: Score matching for general corruptions,\u201d arXiv\npreprint arXiv:2209.05442, 2022.\n[246] Z. Kong and W. Ping, \u201cOn fast sampling of diffusion probabilistic\nmodels,\u201d arXiv preprint arXiv:2106.00132, 2021.\n[247] Y. Song, C. Durkan, I. Murray, and S. Ermon, \u201cMaximum like-\nlihood training of score-based diffusion models,\u201d Advances in\nneural information processing systems, vol. 34, pp. 1415\u20131428, 2021.\n[248] C.-W. Huang, J. H. Lim, and A. C. Courville, \u201cA variational per-\nspective on diffusion-based generative models and score match-\ning,\u201d Advances in Neural Information Processing Systems, vol. 34,\npp. 22 863\u201322 876, 2021.\n[249] E. Heitz, L. Belcour, and T. Chambon, \u201cIterative \u03b1-(de) blending:\nA minimalist deterministic diffusion model,\u201d in ACM SIGGRAPH\n2023 Conference Proceedings, 2023, pp. 1\u20138.\n[250] M. S. Albergo and E. Vanden-Eijnden, \u201cBuilding normal-\nizing\nflows\nwith\nstochastic\ninterpolants,\u201d\narXiv\npreprint\narXiv:2209.15571, 2022.\n[251] X. Su, J. Song, C. Meng, and S. Ermon, \u201cDual diffusion im-\nplicit bridges for image-to-image translation,\u201d arXiv preprint\narXiv:2203.08382, 2022.\n[252] G.-H. Liu, A. Vahdat, D.-A. Huang, E. A. Theodorou, W. Nie, and\nA. Anandkumar, \u201cI2sb: Image-to-image schr\\\u201dodinger bridge,\u201d\narXiv preprint arXiv:2302.05872, 2023.\n[253] S. Feng, C. Miao, Z. Zhang, and P. Zhao, \u201cLatent diffusion trans-\nformer for probabilistic time series forecasting,\u201d in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 38, no. 11, 2024,\npp. 11 979\u201311 987.\n[254] Y. Liang, H. Wen, Y. Nie, Y. Jiang, M. Jin, D. Song, S. Pan, and\nQ. Wen, \u201cFoundation models for time series analysis: A tutorial\nand survey,\u201d arXiv preprint arXiv:2403.14735, 2024.\n[255] Y. Wang, H. Wu, J. Dong, Y. Liu, Y. Qiu, H. Zhang, J. Wang,\nand M. Long, \u201cTimexer: Empowering transformers for time\nseries forecasting with exogenous variables,\u201d arXiv preprint\narXiv:2402.19072, 2024.\n[256] S. Wu, H. Fei, L. Qu, W. Ji, and T.-S. Chua, \u201cNext-gpt: Any-to-any\nmultimodal llm,\u201d arXiv preprint arXiv:2309.05519, 2023.\n[257] Y. Chen, C. Zhang, M. Ma, Y. Liu, R. Ding, B. Li, S. He, S. Raj-\nmohan, Q. Lin, and D. Zhang, \u201cImdiffusion: Imputed diffusion\nmodels for multivariate time series anomaly detection,\u201d arXiv\npreprint arXiv:2307.00754, 2023.\n[258] V. Livernoche, V. Jain, Y. Hezaveh, and S. Ravanbakhsh,\n\u201cOn diffusion modeling for anomaly detection,\u201d arXiv preprint\narXiv:2305.18593, 2023.\n[259] I. Pintilie, A. Manolache, and F. Brad, \u201cTime series anomaly de-\ntection using diffusion-based models,\u201d in 2023 IEEE International\nConference on Data Mining Workshops (ICDMW).\nIEEE, 2023, pp.\n570\u2013578.\n[260] C. Xiao, Z. Gou, W. Tai, K. Zhang, and F. Zhou, \u201cImputation-\nbased time-series anomaly detection with conditional weight-\nincremental diffusion models,\u201d in Proceedings of the 29th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining, 2023,\npp. 2742\u20132751.\n[261] C. Wang, Z. Zhuang, Q. Qi, J. Wang, X. Wang, H. Sun, and J. Liao,\n\u201cDrift doesn\u2019t matter: Dynamic decomposition with diffusion\nreconstruction for unstable multivariate time series anomaly de-\ntection,\u201d Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[262] X. Wang, H. Zhang, P. Wang, Y. Zhang, B. Wang, Z. Zhou, and\nY. Wang, \u201cAn observed value consistent diffusion model for im-\nputing missing values in multivariate time series,\u201d in Proceedings\nof the 29th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining, 2023, pp. 2409\u20132418.\n[263] X. Liu, J. Chen, J. Xie, and Y. Chang, \u201cGenerating hsr bogie\nvibration signals via pulse voltage-guided conditional diffusion\nmodel,\u201d arXiv preprint arXiv:2311.00496, 2023.\n[264] L. Wang, \u201cMulti-modality conditional diffusion model for time\nseries forecasting of live sales volume,\u201d in ICASSP 2024-2024\nIEEE International Conference on Acoustics, Speech and Signal Pro-\ncessing (ICASSP).\nIEEE, 2024, pp. 2675\u20132679.\n[265] K. Lv, L. Yuan, and X. Ni, \u201cLearning autoencoder diffusion mod-\nels of pedestrian group relationships for multimodal trajectory\nprediction,\u201d IEEE Transactions on Instrumentation and Measure-\nment, 2024.\n[266] C. Fu, H. Kazmi, M. Quintana, and C. Miller, \u201cCreating synthetic\nenergy meter data using conditional diffusion and building meta-\ndata,\u201d arXiv preprint arXiv:2404.00525, 2024.\n[267] T. Shimizu, \u201cDiffusion model in causal inference with unmea-\nsured confounders,\u201d in 2023 IEEE Symposium Series on Computa-\ntional Intelligence (SSCI).\nIEEE, 2023, pp. 683\u2013688.\n[268] A. Asperti, F. Merizzi, A. Paparella, G. Pedrazzi, M. Angelinelli,\nand S. Colamonaco, \u201cPrecipitation nowcasting with generative\ndiffusion models,\u201d arXiv preprint arXiv:2308.06733, 2023.\n[269] J. Pei, J. Wang, D. Shi, and P. Wang, \u201cImproved efficient two-\nstage denoising diffusion power system measurement recovery\nagainst false data injection attacks and data losses,\u201d arXiv preprint\narXiv:2312.04346, 2023.\n[270] P. N. Mueller, \u201cAttention-enhanced conditional-diffusion-based\ndata synthesis for data augmentation in machine fault diagno-\nsis,\u201d Engineering Applications of Artificial Intelligence, vol. 131, p.\n107696, 2024.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n27\n[271] P. Zhao, X. Wang, Y. Zhang, Y. Li, H. Wang, and Y. Yang,\n\u201cDiffusion-uda: Diffusion-based unsupervised domain adapta-\ntion for submersible fault diagnosis,\u201d Electronics Letters, vol. 60,\nno. 3, p. e13122, 2024.\n[272] T. Y. Wong, M. H. Lim, W. K. Ngui, and M. S. Leong, \u201cDenoising\ndiffusion implicit model for bearing fault diagnosis under dif-\nferent working loads,\u201d in ITM Web of Conferences, vol. 63.\nEDP\nSciences, 2024, p. 01025.\n[273] K. Goel, A. Gu, C. Donahue, and C. R\u00b4e, \u201cIt\u2019s raw! audio gener-\nation with state-space models,\u201d arXiv preprint arXiv:2202.09729,\n2022.\n[274] C. Donahue, J. McAuley, and M. Puckette, \u201cAdversarial audio\nsynthesis,\u201d in International Conference on Learning Representations,\n2019.\n[275] J. M. L. Alcaraz and N. Strodthoff, \u201cDiffusion-based time series\nimputation and forecasting with structured state space models,\u201d\narXiv preprint arXiv:2208.09399, 2022.\n[276] J. Xu, F. Lyu, and P. C. Yuen, \u201cDensity-aware temporal attentive\nstep-wise diffusion model for medical time series imputation,\u201d in\nProceedings of the 32nd ACM International Conference on Information\nand Knowledge Management, 2023, pp. 2836\u20132845.\n[277] Z. Liu, W. Pei, D. Lan, and Q. Ma, \u201cDiffusion language-shapelets\nfor semi-supervised time-series classification,\u201d in Proceedings of\nthe AAAI Conference on Artificial Intelligence, vol. 38, no. 13, 2024,\npp. 14 079\u201314 087.\n[278] Y. Li, \u201cTs-diffusion: Generating highly complex time series with\ndiffusion models,\u201d arXiv preprint arXiv:2311.03303, 2023.\n[279] Y. Yao, Y. Liu, X. Dai, S. Chen, and Y. Lv, \u201cA graph-based scene en-\ncoder for vehicle trajectory prediction using the diffusion model,\u201d\nin 2023 International Annual Conference on Complex Systems and\nIntelligent Science (CSIS-IAC).\nIEEE, 2023, pp. 981\u2013986.\n[280] Y. Li, M. R. Luyten, and M. van der Schaar, \u201cRisk-sensitive diffu-\nsion: Learning the underlying distribution from noisy samples,\u201d\narXiv preprint arXiv:2402.02081, 2024.\n[281] Y. Liang, K. Ouyang, H. Yan, Y. Wang, Z. Tong, and R. Zimmer-\nmann, \u201cModeling trajectories with neural ordinary differential\nequations.\u201d in IJCAI, 2021, pp. 1498\u20131504.\n[282] R. Hu, X. Yuan, Y. Qiao, B. Zhang, and P. Zhao, \u201cUnsupervised\nanomaly detection for multivariate time series using diffusion\nmodel,\u201d in ICASSP 2024-2024 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2024, pp.\n9606\u20139610.\n[283] X. Liu, Y. Xia, Y. Liang, J. Hu, Y. Wang, L. Bai, C. Huang, Z. Liu,\nB. Hooi, and R. Zimmermann, \u201cLargest: A benchmark dataset for\nlarge-scale traffic forecasting,\u201d in Advances in Neural Information\nProcessing Systems, 2023.\n[284] S. Welker, J. Richter, and T. Gerkmann, \u201cSpeech enhancement\nwith score-based generative models in the complex stft domain,\u201d\narXiv preprint arXiv:2203.17004, 2022.\n[285] S. R\u00a8uhling Cachay, B. Zhao, H. Joren, and R. Yu, \u201cDyffusion: A\ndynamics-informed diffusion model for spatiotemporal forecast-\ning,\u201d Advances in Neural Information Processing Systems, vol. 36,\n2024.\n[286] J. Sun, J. Zhang, Q. Li, X. Yi, Y. Liang, and Y. Zheng, \u201cPredicting\ncitywide crowd flows in irregular regions using multi-view graph\nconvolutional networks,\u201d IEEE Transactions on Knowledge and Data\nEngineering, vol. 34, no. 5, pp. 2348\u20132359, 2020.\n[287] Y. Liang, K. Ouyang, J. Sun, Y. Wang, J. Zhang, Y. Zheng,\nD. Rosenblum, and R. Zimmermann, \u201cFine-grained urban flow\nprediction,\u201d in Proceedings of the Web Conference 2021, 2021, pp.\n1833\u20131845.\n[288] W. Chen, Y. Liang, Y. Zhu, Y. Chang, K. Luo, H. Wen, L. Li,\nY. Yu, Q. Wen, C. Chen et al., \u201cDeep learning for trajectory data\nmanagement and mining: A survey and beyond,\u201d arXiv preprint\narXiv:2403.14151, 2024.\n[289] P. Chang, H. Li, S. F. Quan, S. Lu, S.-F. Wung, J. Roveda, and\nA. Li, \u201cTdstf: Transformer-based diffusion probabilistic model for\nsparse time series forecasting,\u201d arXiv preprint arXiv:2301.06625,\n2023.\n[290] Y. Zhu, J. J. Yu, X. Zhao, Q. Liu, Y. Ye, W. Chen, Z. Zhang,\nX. Wei, and Y. Liang, \u201cControltraj: Controllable trajectory gener-\nation with topology-constrained diffusion model,\u201d arXiv preprint\narXiv:2404.15380, 2024.\n[291] K. Ito, K. It\u02c6o, K. It\u02c6o, J. Math\u00b4ematicien, K. It\u02c6o, and J. Mathemati-\ncian, On stochastic differential equations.\nAmerican Mathematical\nSociety New York, 1951, vol. 4.\n[292] J. Abramson, J. Adler, J. Dunger, R. Evans, T. Green, A. Pritzel,\nO. Ronneberger, L. Willmore, A. J. Ballard, J. Bambrick et al.,\n\u201cAccurate structure prediction of biomolecular interactions with\nalphafold 3,\u201d Nature, pp. 1\u20133, 2024.\n",
    "2011.13456": "Published as a conference paper at ICLR 2021\nSCORE-BASED GENERATIVE MODELING\nTHROUGH\nSTOCHASTIC DIFFERENTIAL EQUATIONS\nYang Song\u02da\nStanford University\nyangsong@cs.stanford.edu\nJascha Sohl-Dickstein\nGoogle Brain\njaschasd@google.com\nDiederik P. Kingma\nGoogle Brain\ndurk@google.com\nAbhishek Kumar\nGoogle Brain\nabhishk@google.com\nStefano Ermon\nStanford University\nermon@cs.stanford.edu\nBen Poole\nGoogle Brain\npooleb@google.com\nABSTRACT\nCreating noise from data is easy; creating data from noise is generative modeling.\nWe present a stochastic differential equation (SDE) that smoothly transforms a com-\nplex data distribution to a known prior distribution by slowly injecting noise, and a\ncorresponding reverse-time SDE that transforms the prior distribution back into the\ndata distribution by slowly removing the noise. Crucially, the reverse-time SDE\ndepends only on the time-dependent gradient \ufb01eld (a.k.a., score) of the perturbed\ndata distribution. By leveraging advances in score-based generative modeling, we\ncan accurately estimate these scores with neural networks, and use numerical SDE\nsolvers to generate samples. We show that this framework encapsulates previous\napproaches in score-based generative modeling and diffusion probabilistic mod-\neling, allowing for new sampling procedures and new modeling capabilities. In\nparticular, we introduce a predictor-corrector framework to correct errors in the\nevolution of the discretized reverse-time SDE. We also derive an equivalent neural\nODE that samples from the same distribution as the SDE, but additionally enables\nexact likelihood computation, and improved sampling ef\ufb01ciency. In addition, we\nprovide a new way to solve inverse problems with score-based models, as demon-\nstrated with experiments on class-conditional generation, image inpainting, and\ncolorization. Combined with multiple architectural improvements, we achieve\nrecord-breaking performance for unconditional image generation on CIFAR-10\nwith an Inception score of 9.89 and FID of 2.20, a competitive likelihood of 2.99\nbits/dim, and demonstrate high \ufb01delity generation of 1024 \u02c6 1024 images for the\n\ufb01rst time from a score-based generative model.\n1\nINTRODUCTION\nTwo successful classes of probabilistic generative models involve sequentially corrupting training\ndata with slowly increasing noise, and then learning to reverse this corruption in order to form a\ngenerative model of the data. Score matching with Langevin dynamics (SMLD) (Song & Ermon,\n2019) estimates the score (i.e., the gradient of the log probability density with respect to data) at each\nnoise scale, and then uses Langevin dynamics to sample from a sequence of decreasing noise scales\nduring generation. Denoising diffusion probabilistic modeling (DDPM) (Sohl-Dickstein et al., 2015;\nHo et al., 2020) trains a sequence of probabilistic models to reverse each step of the noise corruption,\nusing knowledge of the functional form of the reverse distributions to make training tractable. For\ncontinuous state spaces, the DDPM training objective implicitly computes scores at each noise scale.\nWe therefore refer to these two model classes together as score-based generative models.\nScore-based generative models, and related techniques (Bordes et al., 2017; Goyal et al., 2017; Du &\nMordatch, 2019), have proven effective at generation of images (Song & Ermon, 2019; 2020; Ho\net al., 2020), audio (Chen et al., 2020; Kong et al., 2020), graphs (Niu et al., 2020), and shapes (Cai\n\u02daWork partially done during an internship at Google Brain.\n1\narXiv:2011.13456v2  [cs.LG]  10 Feb 2021\nPublished as a conference paper at ICLR 2021\n\u00a0\n\u00a0\u00a0\nForward SDE (data\u00a0\u2192 noise) \nReverse SDE (noise \u2192 data) \nscore function\nFigure 1:\nSolving a reverse-\ntime SDE yields a score-based\ngenerative model. Transform-\ning data to a simple noise dis-\ntribution can be accomplished\nwith a continuous-time SDE.\nThis SDE can be reversed if we\nknow the score of the distribu-\ntion at each intermediate time\nstep, \u2207x log ptpxq.\net al., 2020). To enable new sampling methods and further extend the capabilities of score-based\ngenerative models, we propose a uni\ufb01ed framework that generalizes previous approaches through the\nlens of stochastic differential equations (SDEs).\nSpeci\ufb01cally, instead of perturbing data with a \ufb01nite number of noise distributions, we consider a\ncontinuum of distributions that evolve over time according to a diffusion process. This process\nprogressively diffuses a data point into random noise, and is given by a prescribed SDE that does not\ndepend on the data and has no trainable parameters. By reversing this process, we can smoothly mold\nrandom noise into data for sample generation. Crucially, this reverse process satis\ufb01es a reverse-time\nSDE (Anderson, 1982), which can be derived from the forward SDE given the score of the marginal\nprobability densities as a function of time. We can therefore approximate the reverse-time SDE by\ntraining a time-dependent neural network to estimate the scores, and then produce samples using\nnumerical SDE solvers. Our key idea is summarized in Fig. 1.\nOur proposed framework has several theoretical and practical contributions:\nFlexible sampling and likelihood computation: We can employ any general-purpose SDE solver\nto integrate the reverse-time SDE for sampling. In addition, we propose two special methods not\nviable for general SDEs: (i) Predictor-Corrector (PC) samplers that combine numerical SDE solvers\nwith score-based MCMC approaches, such as Langevin MCMC (Parisi, 1981) and HMC (Neal et al.,\n2011); and (ii) deterministic samplers based on the probability \ufb02ow ordinary differential equation\n(ODE). The former uni\ufb01es and improves over existing sampling methods for score-based models.\nThe latter allows for fast adaptive sampling via black-box ODE solvers, \ufb02exible data manipulation\nvia latent codes, a uniquely identi\ufb01able encoding, and notably, exact likelihood computation.\nControllable generation: We can modulate the generation process by conditioning on information\nnot available during training, because the conditional reverse-time SDE can be ef\ufb01ciently estimated\nfrom unconditional scores. This enables applications such as class-conditional generation, image\ninpainting, colorization and other inverse problems, all achievable using a single unconditional\nscore-based model without re-training.\nUni\ufb01ed framework: Our framework provides a uni\ufb01ed way to explore and tune various SDEs for\nimproving score-based generative models. The methods of SMLD and DDPM can be amalgamated\ninto our framework as discretizations of two separate SDEs. Although DDPM (Ho et al., 2020) was\nrecently reported to achieve higher sample quality than SMLD (Song & Ermon, 2019; 2020), we show\nthat with better architectures and new sampling algorithms allowed by our framework, the latter can\ncatch up\u2014it achieves new state-of-the-art Inception score (9.89) and FID score (2.20) on CIFAR-10,\nas well as high-\ufb01delity generation of 1024 \u02c6 1024 images for the \ufb01rst time from a score-based model.\nIn addition, we propose a new SDE under our framework that achieves a likelihood value of 2.99\nbits/dim on uniformly dequantized CIFAR-10 images, setting a new record on this task.\n2\nBACKGROUND\n2.1\nDENOISING SCORE MATCHING WITH LANGEVIN DYNAMICS (SMLD)\nLet p\u03c3p\u02dcx | xq :\u201c Np\u02dcx; x, \u03c32Iq be a perturbation kernel, and p\u03c3p\u02dcxq :\u201c\n\u015f\npdatapxqp\u03c3p\u02dcx | xqdx, where\npdatapxq denotes the data distribution. Consider a sequence of positive noise scales \u03c3min \u201c \u03c31 \u0103\n\u03c32 \u0103 \u00a8 \u00a8 \u00a8 \u0103 \u03c3N \u201c \u03c3max. Typically, \u03c3min is small enough such that p\u03c3minpxq \u00ab pdatapxq, and \u03c3max is\n2\nPublished as a conference paper at ICLR 2021\nlarge enough such that p\u03c3maxpxq \u00ab Npx; 0, \u03c32\nmaxIq. Song & Ermon (2019) propose to train a Noise\nConditional Score Network (NCSN), denoted by s\u03b8px, \u03c3q, with a weighted sum of denoising score\nmatching (Vincent, 2011) objectives:\n\u03b8\u02da \u201c arg min\n\u03b8\nN\n\u00ff\ni\u201c1\n\u03c32\ni EpdatapxqEp\u03c3ip\u02dcx|xq\n\u201c\n\u2225s\u03b8p\u02dcx, \u03c3iq \u00b4 \u2207\u02dcx log p\u03c3ip\u02dcx | xq\u22252\n2\n\u2030\n.\n(1)\nGiven suf\ufb01cient data and model capacity, the optimal score-based model s\u03b8\u02dapx, \u03c3q matches\n\u2207x log p\u03c3pxq almost everywhere for \u03c3 P t\u03c3iuN\ni\u201c1. For sampling, Song & Ermon (2019) run M steps\nof Langevin MCMC to get a sample for each p\u03c3ipxq sequentially:\nxm\ni \u201c xm\u00b41\ni\n` \u03f5is\u03b8\u02dapxm\u00b41\ni\n, \u03c3iq `\n?\n2\u03f5izm\ni ,\nm \u201c 1, 2, \u00a8 \u00a8 \u00a8 , M,\n(2)\nwhere \u03f5i \u0105 0 is the step size, and zm\ni is standard normal. The above is repeated for i \u201c N, N \u00b4\n1, \u00a8 \u00a8 \u00a8 , 1 in turn with x0\nN \u201e Npx | 0, \u03c32\nmaxIq and x0\ni \u201c xM\ni`1 when i \u0103 N. As M \u00d1 8 and \u03f5i \u00d1 0\nfor all i, xM\n1 becomes an exact sample from p\u03c3minpxq \u00ab pdatapxq under some regularity conditions.\n2.2\nDENOISING DIFFUSION PROBABILISTIC MODELS (DDPM)\nSohl-Dickstein et al. (2015); Ho et al. (2020) consider a sequence of positive noise scales\n0 \u0103 \u03b21, \u03b22, \u00a8 \u00a8 \u00a8 , \u03b2N \u0103 1. For each training data point x0 \u201e pdatapxq, a discrete Markov chain\ntx0, x1, \u00a8 \u00a8 \u00a8 , xNu is constructed such that ppxi | xi\u00b41q \u201c Npxi; ?1 \u00b4 \u03b2ixi\u00b41, \u03b2iIq, and therefore\np\u03b1ipxi | x0q \u201c Npxi; ?\u03b1ix0, p1 \u00b4 \u03b1iqIq, where \u03b1i :\u201c \u015bi\nj\u201c1p1 \u00b4 \u03b2jq. Similar to SMLD, we can\ndenote the perturbed data distribution as p\u03b1ip\u02dcxq :\u201c\n\u015f\npdatapxqp\u03b1ip\u02dcx | xqdx. The noise scales are pre-\nscribed such that xN is approximately distributed according to Np0, Iq. A variational Markov chain\nin the reverse direction is parameterized with p\u03b8pxi\u00b41|xiq \u201c Npxi\u00b41;\n1\n?1\u00b4\u03b2i pxi`\u03b2is\u03b8pxi, iqq, \u03b2iIq,\nand trained with a re-weighted variant of the evidence lower bound (ELBO):\n\u03b8\u02da \u201c arg min\n\u03b8\nN\n\u00ff\ni\u201c1\np1 \u00b4 \u03b1iqEpdatapxqEp\u03b1ip\u02dcx|xqr\u2225s\u03b8p\u02dcx, iq \u00b4 \u2207\u02dcx log p\u03b1ip\u02dcx | xq\u22252\n2s.\n(3)\nAfter solving Eq. (3) to get the optimal model s\u03b8\u02dapx, iq, samples can be generated by starting from\nxN \u201e Np0, Iq and following the estimated reverse Markov chain as below\nxi\u00b41 \u201c\n1\n?1 \u00b4 \u03b2i\npxi ` \u03b2is\u03b8\u02dapxi, iqq `\na\n\u03b2izi,\ni \u201c N, N \u00b4 1, \u00a8 \u00a8 \u00a8 , 1.\n(4)\nWe call this method ancestral sampling, since it amounts to performing ancestral sampling from\nthe graphical model \u015bN\ni\u201c1 p\u03b8pxi\u00b41 | xiq. The objective Eq. (3) described here is Lsimple in Ho et al.\n(2020), written in a form to expose more similarity to Eq. (1). Like Eq. (1), Eq. (3) is also a weighted\nsum of denoising score matching objectives, which implies that the optimal model, s\u03b8\u02dap\u02dcx, iq, matches\nthe score of the perturbed data distribution, \u2207x log p\u03b1ipxq. Notably, the weights of the i-th summand\nin Eq. (1) and Eq. (3), namely \u03c32\ni and p1\u00b4\u03b1iq, are related to corresponding perturbation kernels in the\nsame functional form: \u03c32\ni 91{Er\u2225\u2207x log p\u03c3ip\u02dcx | xq\u22252\n2s and p1 \u00b4 \u03b1iq91{Er\u2225\u2207x log p\u03b1ip\u02dcx | xq\u22252\n2s.\n3\nSCORE-BASED GENERATIVE MODELING WITH SDES\nPerturbing data with multiple noise scales is key to the success of previous methods. We propose to\ngeneralize this idea further to an in\ufb01nite number of noise scales, such that perturbed data distributions\nevolve according to an SDE as the noise intensi\ufb01es. An overview of our framework is given in Fig. 2.\n3.1\nPERTURBING DATA WITH SDES\nOur goal is to construct a diffusion process txptquT\nt\u201c0 indexed by a continuous time variable t P r0, Ts,\nsuch that xp0q \u201e p0, for which we have a dataset of i.i.d. samples, and xpTq \u201e pT , for which we\nhave a tractable form to generate samples ef\ufb01ciently. In other words, p0 is the data distribution and\npT is the prior distribution. This diffusion process can be modeled as the solution to an It\u02c6o SDE:\ndx \u201c fpx, tqdt ` gptqdw,\n(5)\n3\nPublished as a conference paper at ICLR 2021\n\u00a0\n\u00a0\u00a0\nForward SDE\nData\nPrior\nData\nReverse SDE\n\u00a0\n\u00a0\nFigure 2: Overview of score-based generative modeling through SDEs. We can map data to a\nnoise distribution (the prior) with an SDE (Section 3.1), and reverse this SDE for generative modeling\n(Section 3.2). We can also reverse the associated probability \ufb02ow ODE (Section 4.3), which yields a\ndeterministic process that samples from the same distribution as the SDE. Both the reverse-time SDE\nand probability \ufb02ow ODE can be obtained by estimating the score \u2207x log ptpxq (Section 3.3).\nwhere w is the standard Wiener process (a.k.a., Brownian motion), fp\u00a8, tq : Rd \u00d1 Rd is a vector-\nvalued function called the drift coef\ufb01cient of xptq, and gp\u00a8q : R \u00d1 R is a scalar function known as\nthe diffusion coef\ufb01cient of xptq. For ease of presentation we assume the diffusion coef\ufb01cient is a\nscalar (instead of a d \u02c6 d matrix) and does not depend on x, but our theory can be generalized to hold\nin those cases (see Appendix A). The SDE has a unique strong solution as long as the coef\ufb01cients\nare globally Lipschitz in both state and time (\u00d8ksendal, 2003). We hereafter denote by ptpxq the\nprobability density of xptq, and use pstpxptq | xpsqq to denote the transition kernel from xpsq to xptq,\nwhere 0 \u010f s \u0103 t \u010f T.\nTypically, pT is an unstructured prior distribution that contains no information of p0, such as a\nGaussian distribution with \ufb01xed mean and variance. There are various ways of designing the SDE in\nEq. (5) such that it diffuses the data distribution into a \ufb01xed prior distribution. We provide several\nexamples later in Section 3.4 that are derived from continuous generalizations of SMLD and DDPM.\n3.2\nGENERATING SAMPLES BY REVERSING THE SDE\nBy starting from samples of xpTq \u201e pT and reversing the process, we can obtain samples xp0q \u201e p0.\nA remarkable result from Anderson (1982) states that the reverse of a diffusion process is also a\ndiffusion process, running backwards in time and given by the reverse-time SDE:\ndx \u201c rfpx, tq \u00b4 gptq2\u2207x log ptpxqsdt ` gptqd \u00afw,\n(6)\nwhere \u00afw is a standard Wiener process when time \ufb02ows backwards from T to 0, and dt is an\nin\ufb01nitesimal negative timestep. Once the score of each marginal distribution, \u2207x log ptpxq, is known\nfor all t, we can derive the reverse diffusion process from Eq. (6) and simulate it to sample from p0.\n3.3\nESTIMATING SCORES FOR THE SDE\nThe score of a distribution can be estimated by training a score-based model on samples with\nscore matching (Hyv\u00a8arinen, 2005; Song et al., 2019a). To estimate \u2207x log ptpxq, we can train a\ntime-dependent score-based model s\u03b8px, tq via a continuous generalization to Eqs. (1) and (3):\n\u03b8\u02da \u201c arg min\n\u03b8\nEt\n!\n\u03bbptqExp0qExptq|xp0q\n\u201c \r\rs\u03b8pxptq, tq \u00b4 \u2207xptq log p0tpxptq | xp0qq\n\r\r2\n2\n\u2030)\n.\n(7)\nHere \u03bb : r0, Ts \u00d1 R\u01050 is a positive weighting function, t is uniformly sampled over r0, Ts,\nxp0q \u201e p0pxq and xptq \u201e p0tpxptq | xp0qq. With suf\ufb01cient data and model capacity, score matching\nensures that the optimal solution to Eq. (7), denoted by s\u03b8\u02dapx, tq, equals \u2207x log ptpxq for almost all\nx and t. As in SMLD and DDPM, we can typically choose \u03bb91{E\n\u201c \r\r\u2207xptq log p0tpxptq | xp0qq\n\r\r2\n2\n\u2030\n.\nNote that Eq. (7) uses denoising score matching, but other score matching objectives, such as sliced\n4\nPublished as a conference paper at ICLR 2021\nscore matching (Song et al., 2019a) and \ufb01nite-difference score matching (Pang et al., 2020) are also\napplicable here.\nWe typically need to know the transition kernel p0tpxptq | xp0qq to ef\ufb01ciently solve Eq. (7). When\nfp\u00a8, tq is af\ufb01ne, the transition kernel is always a Gaussian distribution, where the mean and variance are\noften known in closed-forms and can be obtained with standard techniques (see Section 5.5 in S\u00a8arkk\u00a8a\n& Solin (2019)). For more general SDEs, we may solve Kolmogorov\u2019s forward equation (\u00d8ksendal,\n2003) to obtain p0tpxptq | xp0qq. Alternatively, we can simulate the SDE to sample from p0tpxptq |\nxp0qq and replace denoising score matching in Eq. (7) with sliced score matching for model training,\nwhich bypasses the computation of \u2207xptq log p0tpxptq | xp0qq (see Appendix A).\n3.4\nEXAMPLES: VE, VP SDES AND BEYOND\nThe noise perturbations used in SMLD and DDPM can be regarded as discretizations of two different\nSDEs. Below we provide a brief discussion and relegate more details to Appendix B.\nWhen using a total of N noise scales, each perturbation kernel p\u03c3ipx | x0q of SMLD corresponds to\nthe distribution of xi in the following Markov chain:\nxi \u201c xi\u00b41 `\nb\n\u03c32\ni \u00b4 \u03c32\ni\u00b41zi\u00b41,\ni \u201c 1, \u00a8 \u00a8 \u00a8 , N,\n(8)\nwhere zi\u00b41 \u201e Np0, Iq, and we have introduced \u03c30 \u201c 0 to simplify the notation. In the limit of\nN \u00d1 8, t\u03c3iuN\ni\u201c1 becomes a function \u03c3ptq, zi becomes zptq, and the Markov chain txiuN\ni\u201c1 becomes\na continuous stochastic process txptqu1\nt\u201c0, where we have used a continuous time variable t P r0, 1s\nfor indexing, rather than an integer i. The process txptqu1\nt\u201c0 is given by the following SDE\ndx \u201c\nc\nd r\u03c32ptqs\ndt\ndw.\n(9)\nLikewise for the perturbation kernels tp\u03b1ipx | x0quN\ni\u201c1 of DDPM, the discrete Markov chain is\nxi \u201c\na\n1 \u00b4 \u03b2ixi\u00b41 `\na\n\u03b2izi\u00b41,\ni \u201c 1, \u00a8 \u00a8 \u00a8 , N.\n(10)\nAs N \u00d1 8, Eq. (10) converges to the following SDE,\ndx \u201c \u00b41\n2\u03b2ptqx dt `\na\n\u03b2ptq dw.\n(11)\nTherefore, the noise perturbations used in SMLD and DDPM correspond to discretizations of SDEs\nEqs. (9) and (11). Interestingly, the SDE of Eq. (9) always gives a process with exploding variance\nwhen t \u00d1 8, whilst the SDE of Eq. (11) yields a process with a \ufb01xed variance of one when the initial\ndistribution has unit variance (proof in Appendix B). Due to this difference, we hereafter refer to\nEq. (9) as the Variance Exploding (VE) SDE, and Eq. (11) the Variance Preserving (VP) SDE.\nInspired by the VP SDE, we propose a new type of SDEs which perform particularly well on\nlikelihoods (see Section 4.3), given by\ndx \u201c \u00b41\n2\u03b2ptqx dt `\nb\n\u03b2ptqp1 \u00b4 e\u00b42\n\u015ft\n0 \u03b2psqdsqdw.\n(12)\nWhen using the same \u03b2ptq and starting from the same initial distribution, the variance of the stochastic\nprocess induced by Eq. (12) is always bounded by the VP SDE at every intermediate time step (proof\nin Appendix B). For this reason, we name Eq. (12) the sub-VP SDE.\nSince VE, VP and sub-VP SDEs all have af\ufb01ne drift coef\ufb01cients, their perturbation kernels p0tpxptq |\nxp0qq are all Gaussian and can be computed in closed-forms, as discussed in Section 3.3. This makes\ntraining with Eq. (7) particularly ef\ufb01cient.\n4\nSOLVING THE REVERSE SDE\nAfter training a time-dependent score-based model s\u03b8, we can use it to construct the reverse-time\nSDE and then simulate it with numerical approaches to generate samples from p0.\n5\nPublished as a conference paper at ICLR 2021\nTable 1: Comparing different reverse-time SDE solvers on CIFAR-10. Shaded regions are obtained\nwith the same computation (number of score function evaluations). Mean and standard deviation\nare reported over \ufb01ve sampling runs. \u201cP1000\u201d or \u201cP2000\u201d: predictor-only samplers using 1000 or\n2000 steps. \u201cC2000\u201d: corrector-only samplers using 2000 steps. \u201cPC1000\u201d: Predictor-Corrector (PC)\nsamplers using 1000 predictor and 1000 corrector steps.\nVariance Exploding SDE (SMLD)\nVariance Preserving SDE (DDPM)\nPredictor\nFID\u00d3\nSampler\nP1000\nP2000\nC2000\nPC1000\nP1000\nP2000\nC2000\nPC1000\nancestral sampling\n4.98 \u02d8 .06\n4.88 \u02d8 .06\n3.62 \u02d8 .03\n3.24 \u02d8 .02\n3.24 \u02d8 .02\n3.21 \u02d8 .02\nreverse diffusion\n4.79 \u02d8 .07\n4.74 \u02d8 .08\n3.60 \u02d8 .02\n3.21 \u02d8 .02\n3.19 \u02d8 .02\n3.18 \u02d8 .01\nprobability \ufb02ow\n15.41 \u02d8 .15\n10.54 \u02d8 .08\n20.43 \u02d8 .07\n3.51 \u02d8 .04\n3.59 \u02d8 .04\n3.23 \u02d8 .03\n19.06 \u02d8 .06\n3.06 \u02d8 .03\n4.1\nGENERAL-PURPOSE NUMERICAL SDE SOLVERS\nNumerical solvers provide approximate trajectories from SDEs. Many general-purpose numerical\nmethods exist for solving SDEs, such as Euler-Maruyama and stochastic Runge-Kutta methods (Kloe-\nden & Platen, 2013), which correspond to different discretizations of the stochastic dynamics. We\ncan apply any of them to the reverse-time SDE for sample generation.\nAncestral sampling, the sampling method of DDPM (Eq. (4)), actually corresponds to one special\ndiscretization of the reverse-time VP SDE (Eq. (11)) (see Appendix E). Deriving the ancestral\nsampling rules for new SDEs, however, can be non-trivial. To remedy this, we propose reverse\ndiffusion samplers (details in Appendix E), which discretize the reverse-time SDE in the same way\nas the forward one, and thus can be readily derived given the forward discretization. As shown in\nTable 1, reverse diffusion samplers perform slightly better than ancestral sampling for both SMLD and\nDDPM models on CIFAR-10 (DDPM-type ancestral sampling is also applicable to SMLD models,\nsee Appendix F.)\n4.2\nPREDICTOR-CORRECTOR SAMPLERS\nUnlike generic SDEs, we have additional information that can be used to improve solutions. Since we\nhave a score-based model s\u03b8\u02dapx, tq \u00ab \u2207x log ptpxq, we can employ score-based MCMC approaches,\nsuch as Langevin MCMC (Parisi, 1981; Grenander & Miller, 1994) or HMC (Neal et al., 2011) to\nsample from pt directly, and correct the solution of a numerical SDE solver.\nSpeci\ufb01cally, at each time step, the numerical SDE solver \ufb01rst gives an estimate of the sample\nat the next time step, playing the role of a \u201cpredictor\u201d. Then, the score-based MCMC approach\ncorrects the marginal distribution of the estimated sample, playing the role of a \u201ccorrector\u201d. The\nidea is analogous to Predictor-Corrector methods, a family of numerical continuation techniques for\nsolving systems of equations (Allgower & Georg, 2012), and we similarly name our hybrid sampling\nalgorithms Predictor-Corrector (PC) samplers. Please \ufb01nd pseudo-code and a complete description\nin Appendix G. PC samplers generalize the original sampling methods of SMLD and DDPM: the\nformer uses an identity function as the predictor and annealed Langevin dynamics as the corrector,\nwhile the latter uses ancestral sampling as the predictor and identity as the corrector.\nWe test PC samplers on SMLD and DDPM models (see Algorithms 2 and 3 in Appendix G) trained\nwith original discrete objectives given by Eqs. (1) and (3). This exhibits the compatibility of PC\nsamplers to score-based models trained with a \ufb01xed number of noise scales. We summarize the\nperformance of different samplers in Table 1, where probability \ufb02ow is a predictor to be discussed\nin Section 4.3. Detailed experimental settings and additional results are given in Appendix G. We\nobserve that our reverse diffusion sampler always outperform ancestral sampling, and corrector-only\nmethods (C2000) perform worse than other competitors (P2000, PC1000) with the same computation\n(In fact, we need way more corrector steps per noise scale, and thus more computation, to match the\nperformance of other samplers.) For all predictors, adding one corrector step for each predictor step\n(PC1000) doubles computation but always improves sample quality (against P1000). Moreover, it\nis typically better than doubling the number of predictor steps without adding a corrector (P2000),\nwhere we have to interpolate between noise scales in an ad hoc manner (detailed in Appendix G) for\nSMLD/DDPM models. In Fig. 9 (Appendix G), we additionally provide qualitative comparison for\n6\nPublished as a conference paper at ICLR 2021\nTable 2: NLLs and FIDs (ODE) on CIFAR-10.\nModel\nNLL Test \u00d3 FID \u00d3\nRealNVP (Dinh et al., 2016)\n3.49\n-\niResNet (Behrmann et al., 2019)\n3.45\n-\nGlow (Kingma & Dhariwal, 2018)\n3.35\n-\nMintNet (Song et al., 2019b)\n3.32\n-\nResidual Flow (Chen et al., 2019)\n3.28\n46.37\nFFJORD (Grathwohl et al., 2018)\n3.40\n-\nFlow++ (Ho et al., 2019)\n3.29\n-\nDDPM (L) (Ho et al., 2020)\n\u010f 3.70*\n13.51\nDDPM (Lsimple) (Ho et al., 2020)\n\u010f 3.75*\n3.17\nDDPM\n3.28\n3.37\nDDPM cont. (VP)\n3.21\n3.69\nDDPM cont. (sub-VP)\n3.05\n3.56\nDDPM++ cont. (VP)\n3.16\n3.93\nDDPM++ cont. (sub-VP)\n3.02\n3.16\nDDPM++ cont. (deep, VP)\n3.13\n3.08\nDDPM++ cont. (deep, sub-VP)\n2.99\n2.92\nTable 3: CIFAR-10 sample quality.\nModel\nFID\u00d3\nIS\u00d2\nConditional\nBigGAN (Brock et al., 2018)\n14.73\n9.22\nStyleGAN2-ADA (Karras et al., 2020a)\n2.42\n10.14\nUnconditional\nStyleGAN2-ADA (Karras et al., 2020a)\n2.92\n9.83\nNCSN (Song & Ermon, 2019)\n25.32 8.87 \u02d8 .12\nNCSNv2 (Song & Ermon, 2020)\n10.87 8.40 \u02d8 .07\nDDPM (Ho et al., 2020)\n3.17\n9.46 \u02d8 .11\nDDPM++\n2.78\n9.64\nDDPM++ cont. (VP)\n2.55\n9.58\nDDPM++ cont. (sub-VP)\n2.61\n9.56\nDDPM++ cont. (deep, VP)\n2.41\n9.68\nDDPM++ cont. (deep, sub-VP)\n2.41\n9.57\nNCSN++\n2.45\n9.73\nNCSN++ cont. (VE)\n2.38\n9.83\nNCSN++ cont. (deep, VE)\n2.20\n9.89\nmodels trained with the continuous objective Eq. (7) on 256 \u02c6 256 LSUN images and the VE SDE,\nwhere PC samplers clearly surpass predictor-only samplers under comparable computation, when\nusing a proper number of corrector steps.\n4.3\nPROBABILITY FLOW AND CONNECTION TO NEURAL ODES\nScore-based models enable another numerical method for solving the reverse-time SDE. For all\ndiffusion processes, there exists a corresponding deterministic process whose trajectories share the\nsame marginal probability densities tptpxquT\nt\u201c0 as the SDE. This deterministic process satis\ufb01es an\nODE (more details in Appendix D.1):\ndx \u201c\n\u201d\nfpx, tq \u00b4 1\n2gptq2\u2207x log ptpxq\n\u0131\ndt,\n(13)\nwhich can be determined from the SDE once scores are known. We name the ODE in Eq. (13) the\nprobability \ufb02ow ODE. When the score function is approximated by the time-dependent score-based\nmodel, which is typically a neural network, this is an example of a neural ODE (Chen et al., 2018).\nExact likelihood computation Leveraging the connection to neural ODEs, we can compute the\ndensity de\ufb01ned by Eq. (13) via the instantaneous change of variables formula (Chen et al., 2018).\nThis allows us to compute the exact likelihood on any input data (details in Appendix D.2). As an\nexample, we report negative log-likelihoods (NLLs) measured in bits/dim on the CIFAR-10 dataset\nin Table 2. We compute log-likelihoods on uniformly dequantized data, and only compare to models\nevaluated in the same way (omitting models evaluated with variational dequantization (Ho et al.,\n2019) or discrete data), except for DDPM (L/Lsimple) whose ELBO values (annotated with *) are\nreported on discrete data. Main results: (i) For the same DDPM model in Ho et al. (2020), we obtain\nbetter bits/dim than ELBO, since our likelihoods are exact; (ii) Using the same architecture, we\ntrained another DDPM model with the continuous objective in Eq. (7) (i.e., DDPM cont.), which\nfurther improves the likelihood; (iii) With sub-VP SDEs, we always get higher likelihoods compared\nto VP SDEs; (iv) With improved architecture (i.e., DDPM++ cont., details in Section 4.4) and the\nsub-VP SDE, we can set a new record bits/dim of 2.99 on uniformly dequantized CIFAR-10 even\nwithout maximum likelihood training.\nManipulating latent representations By integrating Eq. (13), we can encode any datapoint xp0q\ninto a latent space xpTq. Decoding can be achieved by integrating a corresponding ODE for the\nreverse-time SDE. As is done with other invertible models such as neural ODEs and normalizing\n\ufb02ows (Dinh et al., 2016; Kingma & Dhariwal, 2018), we can manipulate this latent representation for\nimage editing, such as interpolation, and temperature scaling (see Fig. 3 and Appendix D.4).\nUniquely identi\ufb01able encoding Unlike most current invertible models, our encoding is uniquely\nidenti\ufb01able, meaning that with suf\ufb01cient training data, model capacity, and optimization accuracy,\nthe encoding for an input is uniquely determined by the data distribution (Roeder et al., 2020). This\nis because our forward SDE, Eq. (5), has no trainable parameters, and its associated probability \ufb02ow\n7\nPublished as a conference paper at ICLR 2021\n100\n101\n102\n103\nEvaluation number\n0.0\n0.5\n1.0\nEvaluation timepoint\nODE Evaluation Points\nPrecision\n1e-1\n1e-3\n1e-5\nNFE=14    \nNFE=86    \nNFE=548   \nInterpolation\nFigure 3: Probability \ufb02ow ODE enables fast sampling with adaptive stepsizes as the numerical\nprecision is varied (left), and reduces the number of score function evaluations (NFE) without harming\nquality (middle). The invertible mapping from latents to images allows for interpolations (right).\nODE, Eq. (13), provides the same trajectories given perfectly estimated scores. We provide additional\nempirical veri\ufb01cation on this property in Appendix D.5.\nEf\ufb01cient sampling As with neural ODEs, we can sample xp0q \u201e p0 by solving Eq. (13) from\ndifferent \ufb01nal conditions xpTq \u201e pT . Using a \ufb01xed discretization strategy we can generate com-\npetitive samples, especially when used in conjuction with correctors (Table 1, \u201cprobability \ufb02ow\nsampler\u201d, details in Appendix D.3). Using a black-box ODE solver (Dormand & Prince, 1980) not\nonly produces high quality samples (Table 2, details in Appendix D.4), but also allows us to explicitly\ntrade-off accuracy for ef\ufb01ciency. With a larger error tolerance, the number of function evaluations\ncan be reduced by over 90% without affecting the visual quality of samples (Fig. 3).\n4.4\nARCHITECTURE IMPROVEMENTS\nWe explore several new architecture designs for score-based models using both VE and VP SDEs\n(details in Appendix H), where we train models with the same discrete objectives as in SMLD/DDPM.\nWe directly transfer the architectures for VP SDEs to sub-VP SDEs due to their similarity. Our\noptimal architecture for the VE SDE, named NCSN++, achieves an FID of 2.45 on CIFAR-10 with\nPC samplers, while our optimal architecture for the VP SDE, called DDPM++, achieves 2.78.\nBy switching to the continuous training objective in Eq. (7), and increasing the network depth, we can\nfurther improve sample quality for all models. The resulting architectures are denoted as NCSN++\ncont. and DDPM++ cont. in Table 3 for VE and VP/sub-VP SDEs respectively. Results reported in\nTable 3 are for the checkpoint with the smallest FID over the course of training, where samples are\ngenerated with PC samplers. In contrast, FID scores and NLL values in Table 2 are reported for the\nlast training checkpoint, and samples are obtained with black-box ODE solvers. As shown in Table 3,\nVE SDEs typically provide better sample quality than VP/sub-VP SDEs, but we also empirically\nobserve that their likelihoods are worse than VP/sub-VP SDE counterparts. This indicates that\npractitioners likely need to experiment with different SDEs for varying domains and architectures.\nOur best model for sample quality, NCSN++ cont. (deep, VE), doubles the network depth and sets\nnew records for both inception score and FID on unconditional generation for CIFAR-10. Surprisingly,\nwe can achieve better FID than the previous best conditional generative model without requiring\nlabeled data. With all improvements together, we also obtain the \ufb01rst set of high-\ufb01delity samples\non CelebA-HQ 1024 \u02c6 1024 from score-based models (see Appendix H.3). Our best model for\nlikelihoods, DDPM++ cont. (deep, sub-VP), similarly doubles the network depth and achieves a\nlog-likelihood of 2.99 bits/dim with the continuous objective in Eq. (7). To our best knowledge, this\nis the highest likelihood on uniformly dequantized CIFAR-10.\n5\nCONTROLLABLE GENERATION\nThe continuous structure of our framework allows us to not only produce data samples from p0, but\nalso from p0pxp0q | yq if ptpy | xptqq is known. Given a forward SDE as in Eq. (5), we can sample\n8\nPublished as a conference paper at ICLR 2021\nFigure 4: Left: Class-conditional samples on 32 \u02c6 32 CIFAR-10. Top four rows are automobiles and\nbottom four rows are horses. Right: Inpainting (top two rows) and colorization (bottom two rows)\nresults on 256 \u02c6 256 LSUN. First column is the original image, second column is the masked/gray-\nscale image, remaining columns are sampled image completions or colorizations.\nfrom ptpxptq | yq by starting from pT pxpTq | yq and solving a conditional reverse-time SDE:\ndx \u201c tfpx, tq \u00b4 gptq2r\u2207x log ptpxq ` \u2207x log ptpy | xqsudt ` gptqd \u00afw.\n(14)\nIn general, we can use Eq. (14) to solve a large family of inverse problems with score-based generative\nmodels, once given an estimate of the gradient of the forward process, \u2207x log ptpy | xptqq. In some\ncases, it is possible to train a separate model to learn the forward process log ptpy | xptqq and\ncompute its gradient. Otherwise, we may estimate the gradient with heuristics and domain knowledge.\nIn Appendix I.4, we provide a broadly applicable method for obtaining such an estimate without the\nneed of training auxiliary models.\nWe consider three applications of controllable generation with this approach: class-conditional\ngeneration, image imputation and colorization. When y represents class labels, we can train a\ntime-dependent classi\ufb01er ptpy | xptqq for class-conditional sampling. Since the forward SDE\nis tractable, we can easily create training data pxptq, yq for the time-dependent classi\ufb01er by \ufb01rst\nsampling pxp0q, yq from a dataset, and then sampling xptq \u201e p0tpxptq | xp0qq. Afterwards, we\nmay employ a mixture of cross-entropy losses over different time steps, like Eq. (7), to train the\ntime-dependent classi\ufb01er ptpy | xptqq. We provide class-conditional CIFAR-10 samples in Fig. 4\n(left), and relegate more details and results to Appendix I.\nImputation is a special case of conditional sampling. Suppose we have an incomplete data point\ny where only some subset, \u2126pyq is known. Imputation amounts to sampling from ppxp0q | \u2126pyqq,\nwhich we can accomplish using an unconditional model (see Appendix I.2). Colorization is a special\ncase of imputation, except that the known data dimensions are coupled. We can decouple these data\ndimensions with an orthogonal linear transformation, and perform imputation in the transformed\nspace (details in Appendix I.3). Fig. 4 (right) shows results for inpainting and colorization achieved\nwith unconditional time-dependent score-based models.\n6\nCONCLUSION\nWe presented a framework for score-based generative modeling based on SDEs. Our work enables a\nbetter understanding of existing approaches, new sampling algorithms, exact likelihood computation,\nuniquely identi\ufb01able encoding, latent code manipulation, and brings new conditional generation\nabilities to the family of score-based generative models.\nWhile our proposed sampling approaches improve results and enable more ef\ufb01cient sampling, they\nremain slower at sampling than GANs (Goodfellow et al., 2014) on the same datasets. Identifying\nways of combining the stable learning of score-based generative models with the fast sampling of\nimplicit models like GANs remains an important research direction. Additionally, the breadth of\nsamplers one can use when given access to score functions introduces a number of hyper-parameters.\nFuture work would bene\ufb01t from improved methods to automatically select and tune these hyper-\nparameters, as well as more extensive investigation on the merits and limitations of various samplers.\n9\nPublished as a conference paper at ICLR 2021\nACKNOWLEDGEMENTS\nWe would like to thank Nanxin Chen, Ruiqi Gao, Jonathan Ho, Kevin Murphy, Tim Salimans and\nHan Zhang for their insightful discussions during the course of this project. This research was\npartially supported by NSF (#1651565, #1522054, #1733686), ONR (N000141912145), AFOSR\n(FA95501910024), and TensorFlow Research Cloud. Yang Song was partially supported by the Apple\nPhD Fellowship in AI/ML.\nREFERENCES\nEugene L Allgower and Kurt Georg. Numerical continuation methods: an introduction, volume 13.\nSpringer Science & Business Media, 2012.\nBrian D O Anderson. Reverse-time diffusion equation models. Stochastic Process. Appl., 12(3):\n313\u2013326, May 1982.\nJens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and J\u00a8orn-Henrik Jacobsen.\nInvertible residual networks. In International Conference on Machine Learning, pp. 573\u2013582,\n2019.\nFlorian Bordes, Sina Honari, and Pascal Vincent. Learning to generate samples from noise through\ninfusion training. arXiv preprint arXiv:1703.06975, 2017.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high \ufb01delity natural\nimage synthesis. In International Conference on Learning Representations, 2018.\nRuojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and\nBharath Hariharan. Learning gradient \ufb01elds for shape generation. In Proceedings of the European\nConference on Computer Vision (ECCV), 2020.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wavegrad:\nEstimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, 2020.\nRicky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary\ndifferential equations. In Advances in neural information processing systems, pp. 6571\u20136583,\n2018.\nRicky TQ Chen, Jens Behrmann, David K Duvenaud, and J\u00a8orn-Henrik Jacobsen. Residual \ufb02ows\nfor invertible generative modeling. In Advances in Neural Information Processing Systems, pp.\n9916\u20139926, 2019.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. arXiv\npreprint arXiv:1605.08803, 2016.\nJohn R Dormand and Peter J Prince. A family of embedded runge-kutta formulae. Journal of\ncomputational and applied mathematics, 6(1):19\u201326, 1980.\nYilun Du and Igor Mordatch. Implicit generation and modeling with energy based models. In\nH. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alch\u00b4e-Buc, E. Fox, and R. Garnett (eds.), Advances\nin Neural Information Processing Systems, volume 32, pp. 3608\u20133618. Curran Associates, Inc.,\n2019.\nBradley Efron. Tweedie\u2019s formula and selection bias. Journal of the American Statistical Association,\n106(496):1602\u20131614, 2011.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural informa-\ntion processing systems, pp. 2672\u20132680, 2014.\nAnirudh Goyal Alias Parth Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio. Variational\nwalkback: Learning a transition operator as a stochastic recurrent net. In Advances in Neural\nInformation Processing Systems, pp. 4392\u20134402, 2017.\n10\nPublished as a conference paper at ICLR 2021\nWill Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models. In International Confer-\nence on Learning Representations, 2018.\nUlf Grenander and Michael I Miller. Representations of knowledge in complex systems. Journal of\nthe Royal Statistical Society: Series B (Methodological), 56(4):549\u2013581, 1994.\nJonathan Ho, Xi Chen, Aravind Srinivas, Yan Duan, and Pieter Abbeel. Flow++: Improving \ufb02ow-\nbased generative models with variational dequantization and architecture design. In International\nConference on Machine Learning, pp. 2722\u20132730, 2019.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33, 2020.\nMichael F Hutchinson. A stochastic estimator of the trace of the in\ufb02uence matrix for Laplacian\nsmoothing splines. Communications in Statistics-Simulation and Computation, 19(2):433\u2013450,\n1990.\nAapo Hyv\u00a8arinen. Estimation of non-normalized statistical models by score matching. Journal of\nMachine Learning Research, 6(Apr):695\u2013709, 2005.\nAlexia Jolicoeur-Martineau, R\u00b4emi Pich\u00b4e-Taillefer, R\u00b4emi Tachet des Combes, and Ioannis Mitliagkas.\nAdversarial score matching and improved sampling for image generation.\narXiv preprint\narXiv:2009.05475, 2020.\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for\nimproved quality, stability, and variation. In International Conference on Learning Representations,\n2018.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pp. 4401\u20134410, 2019.\nTero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training\ngenerative adversarial networks with limited data. Advances in Neural Information Processing\nSystems, 33, 2020a.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing\nand improving the image quality of StyleGAN. In Proc. CVPR, 2020b.\nDurk P Kingma and Prafulla Dhariwal. Glow: Generative \ufb02ow with invertible 1x1 convolutions. In\nAdvances in Neural Information Processing Systems, pp. 10215\u201310224, 2018.\nPeter E Kloeden and Eckhard Platen. Numerical solution of stochastic differential equations, vol-\nume 23. Springer Science & Business Media, 2013.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. Diffwave: A versatile\ndiffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In\nProceedings of International Conference on Computer Vision (ICCV), December 2015.\nDimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker-planck\nequations through gradient-log-density estimation. arXiv preprint arXiv:2006.00702, 2020.\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,\n2(11):2, 2011.\nChenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Permu-\ntation invariant graph generation via score-based generative modeling. volume 108 of Proceedings\nof Machine Learning Research, pp. 4474\u20134484, Online, 26\u201328 Aug 2020. PMLR.\n11\nPublished as a conference paper at ICLR 2021\nBernt \u00d8ksendal. Stochastic differential equations. In Stochastic differential equations, pp. 65\u201384.\nSpringer, 2003.\nTianyu Pang, Kun Xu, Chongxuan Li, Yang Song, Stefano Ermon, and Jun Zhu. Ef\ufb01cient learning of\ngenerative models via \ufb01nite-difference score matching. arXiv preprint arXiv:2007.03317, 2020.\nGiorgio Parisi. Correlation functions and computer simulations. Nuclear Physics B, 180(3):378\u2013384,\n1981.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-\ufb01delity images with\nvq-vae-2. In Advances in Neural Information Processing Systems, pp. 14837\u201314847, 2019.\nGeoffrey Roeder, Luke Metz, and Diederik P Kingma. On linear identi\ufb01ability of learned representa-\ntions. arXiv preprint arXiv:2007.00810, 2020.\nSimo S\u00a8arkk\u00a8a and Arno Solin. Applied stochastic differential equations, volume 10. Cambridge\nUniversity Press, 2019.\nJohn Skilling. The eigenvalues of mega-dimensional matrices. In Maximum Entropy and Bayesian\nMethods, pp. 455\u2013466. Springer, 1989.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\npp. 2256\u20132265, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn Advances in Neural Information Processing Systems, pp. 11895\u201311907, 2019.\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models.\nAdvances in Neural Information Processing Systems, 33, 2020.\nYang Song, Sahaj Garg, Jiaxin Shi, and Stefano Ermon. Sliced score matching: A scalable approach\nto density and score estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in\nArti\ufb01cial Intelligence, UAI 2019, Tel Aviv, Israel, July 22-25, 2019, pp. 204, 2019a.\nYang Song, Chenlin Meng, and Stefano Ermon. Mintnet: Building invertible neural networks with\nmasked convolutions. In Advances in Neural Information Processing Systems, pp. 11002\u201311012,\n2019b.\nMatthew Tancik, Pratul P. Srinivasan, Ben Mildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh\nSinghal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier features let networks learn\nhigh frequency functions in low dimensional domains. NeurIPS, 2020.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-\ntion, 23(7):1661\u20131674, 2011.\nFisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun:\nConstruction of a large-scale image dataset using deep learning with humans in the loop. arXiv\npreprint arXiv:1506.03365, 2015.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146,\n2016.\nRichard Zhang. Making convolutional networks shift-invariant again. In ICML, 2019.\n12\nPublished as a conference paper at ICLR 2021\nAPPENDIX\nWe include several appendices with additional details, derivations, and results. Our framework\nallows general SDEs with matrix-valued diffusion coef\ufb01cients that depend on the state, for which\nwe provide a detailed discussion in Appendix A. We give a full derivation of VE, VP and sub-VP\nSDEs in Appendix B, and discuss how to use them from a practitioner\u2019s perspective in Appendix C.\nWe elaborate on the probability \ufb02ow formulation of our framework in Appendix D, including a\nderivation of the probability \ufb02ow ODE (Appendix D.1), exact likelihood computation (Appendix D.2),\nprobability \ufb02ow sampling with a \ufb01xed discretization strategy (Appendix D.3), sampling with black-\nbox ODE solvers (Appendix D.4), and experimental veri\ufb01cation on uniquely identi\ufb01able encoding\n(Appendix D.5). We give a full description of the reverse diffusion sampler in Appendix E, the\nDDPM-type ancestral sampler for SMLD models in Appendix F, and Predictor-Corrector samplers in\nAppendix G. We explain our model architectures and detailed experimental settings in Appendix H,\nwith 1024 \u02c6 1024 CelebA-HQ samples therein. Finally, we detail on the algorithms for controllable\ngeneration in Appendix I, and include extended results for class-conditional generation (Appendix I.1),\nimage inpainting (Appendix I.2), colorization (Appendix I.3), and a strategy for solving general\ninverse problems (Appendix I.4).\nA\nTHE FRAMEWORK FOR MORE GENERAL SDES\nIn the main text, we introduced our framework based on a simpli\ufb01ed SDE Eq. (5) where the diffusion\ncoef\ufb01cient is independent of xptq. It turns out that our framework can be extended to hold for more\ngeneral diffusion coef\ufb01cients. We can consider SDEs in the following form:\ndx \u201c fpx, tqdt ` Gpx, tqdw,\n(15)\nwhere fp\u00a8, tq : Rd \u00d1 Rd and Gp\u00a8, tq : Rd \u00d1 Rd\u02c6d. We follow the It\u02c6o interpretation of SDEs\nthroughout this paper.\nAccording to (Anderson, 1982), the reverse-time SDE is given by (cf., Eq. (6))\ndx \u201c tfpx, tq \u00b4 \u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 Gpx, tqGpx, tqT\u2207x log ptpxqudt ` Gpx, tqd \u00afw,\n(16)\nwhere we de\ufb01ne \u2207\u00a8 Fpxq :\u201c p\u2207\u00a8 f 1pxq, \u2207\u00a8 f 2pxq, \u00a8 \u00a8 \u00a8 , \u2207\u00a8 f dpxqqT for a matrix-valued function\nFpxq :\u201c pf 1pxq, f 2pxq, \u00a8 \u00a8 \u00a8 , f dpxqqT throughout the paper.\nThe probability \ufb02ow ODE corresponding to Eq. (15) has the following form (cf., Eq. (13), see a\ndetailed derivation in Appendix D.1):\ndx \u201c\n\"\nfpx, tq \u00b4 1\n2\u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 1\n2Gpx, tqGpx, tqT\u2207x log ptpxq\n*\ndt.\n(17)\nFinally for conditional generation with the general SDE Eq. (15), we can solve the conditional\nreverse-time SDE below (cf., Eq. (14), details in Appendix I):\ndx \u201c tfpx, tq \u00b4 \u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 Gpx, tqGpx, tqT\u2207x log ptpxq\n\u00b4 Gpx, tqGpx, tqT\u2207x log ptpy | xqudt ` Gpx, tqd \u00afw.\n(18)\nWhen the drift and diffusion coef\ufb01cient of an SDE are not af\ufb01ne, it can be dif\ufb01cult to compute the\ntransition kernel p0tpxptq | xp0qq in closed form. This hinders the training of score-based models,\nbecause Eq. (7) requires knowing \u2207xptq log p0tpxptq | xp0qq. To overcome this dif\ufb01culty, we can\nreplace denoising score matching in Eq. (7) with other ef\ufb01cient variants of score matching that do not\nrequire computing \u2207xptq log p0tpxptq | xp0qq. For example, when using sliced score matching (Song\net al., 2019a), our training objective Eq. (7) becomes\n\u03b8\u02da \u201c arg min\n\u03b8\nEt\n\"\n\u03bbptqExp0qExptqEv\u201epv\n\u201e1\n2 \u2225s\u03b8pxptq, tq\u22252\n2 ` vTs\u03b8pxptq, tqv\n\uf6be*\n,\n(19)\nwhere \u03bb : r0, Ts \u00d1 R` is a positive weighting function, t \u201e Up0, Tq, Ervs \u201c 0, and Covrvs \u201c I.\nWe can always simulate the SDE to sample from p0tpxptq | xp0qq, and solve Eq. (19) to train the\ntime-dependent score-based model s\u03b8px, tq.\n13\nPublished as a conference paper at ICLR 2021\nB\nVE, VP AND SUB-VP SDES\nBelow we provide detailed derivations to show that the noise perturbations of SMLD and DDPM\nare discretizations of the Variance Exploding (VE) and Variance Preserving (VP) SDEs respectively.\nWe additionally introduce sub-VP SDEs, a modi\ufb01cation to VP SDEs that often achieves better\nperformance in both sample quality and likelihoods.\nFirst, when using a total of N noise scales, each perturbation kernel p\u03c3ipx | x0q of SMLD can be\nderived from the following Markov chain:\nxi \u201c xi\u00b41 `\nb\n\u03c32\ni \u00b4 \u03c32\ni\u00b41zi\u00b41,\ni \u201c 1, \u00a8 \u00a8 \u00a8 , N,\n(20)\nwhere zi\u00b41 \u201e Np0, Iq, x0 \u201e pdata, and we have introduced \u03c30 \u201c 0 to simplify the notation. In the\nlimit of N \u00d1 8, the Markov chain txiuN\ni\u201c1 becomes a continuous stochastic process txptqu1\nt\u201c0,\nt\u03c3iuN\ni\u201c1 becomes a function \u03c3ptq, and zi becomes zptq, where we have used a continuous time variable\nt P r0, 1s for indexing, rather than an integer i P t1, 2, \u00a8 \u00a8 \u00a8 , Nu. Let x\n` i\nN\n\u02d8\n\u201c xi, \u03c3\n` i\nN\n\u02d8\n\u201c \u03c3i,\nand z\n` i\nN\n\u02d8\n\u201c zi for i \u201c 1, 2, \u00a8 \u00a8 \u00a8 , N. We can rewrite Eq. (20) as follows with \u2206t \u201c\n1\nN and\nt P\n\u2423\n0, 1\nN , \u00a8 \u00a8 \u00a8 , N\u00b41\nN\n(\n:\nxpt ` \u2206tq \u201c xptq `\na\n\u03c32pt ` \u2206tq \u00b4 \u03c32ptq zptq \u00ab xptq `\nc\nd r\u03c32ptqs\ndt\n\u2206t zptq,\nwhere the approximate equality holds when \u2206t ! 1. In the limit of \u2206t \u00d1 0, this converges to\ndx \u201c\nc\nd r\u03c32ptqs\ndt\ndw,\n(21)\nwhich is the VE SDE.\nFor the perturbation kernels tp\u03b1ipx | x0quN\ni\u201c1 used in DDPM, the discrete Markov chain is\nxi \u201c\na\n1 \u00b4 \u03b2ixi\u00b41 `\na\n\u03b2izi\u00b41,\ni \u201c 1, \u00a8 \u00a8 \u00a8 , N,\n(22)\nwhere zi\u00b41 \u201e Np0, Iq. To obtain the limit of this Markov chain when N \u00d1 8, we de\ufb01ne an\nauxiliary set of noise scales t\u00af\u03b2i \u201c N\u03b2iuN\ni\u201c1, and re-write Eq. (22) as below\nxi \u201c\nc\n1 \u00b4\n\u00af\u03b2i\nN xi\u00b41 `\nc \u00af\u03b2i\nN zi\u00b41,\ni \u201c 1, \u00a8 \u00a8 \u00a8 , N.\n(23)\nIn the limit of N \u00d1 8, t\u00af\u03b2iuN\ni\u201c1 becomes a function \u03b2ptq indexed by t P r0, 1s. Let \u03b2\n` i\nN\n\u02d8\n\u201c \u00af\u03b2i,\nxp i\nN q \u201c xi, zp i\nN q \u201c zi. We can rewrite the Markov chain Eq. (23) as the following with \u2206t \u201c 1\nN\nand t P t0, 1, \u00a8 \u00a8 \u00a8 , N\u00b41\nN u:\nxpt ` \u2206tq \u201c\na\n1 \u00b4 \u03b2pt ` \u2206tq\u2206t xptq `\na\n\u03b2pt ` \u2206tq\u2206t zptq\n\u00ab xptq \u00b4 1\n2\u03b2pt ` \u2206tq\u2206t xptq `\na\n\u03b2pt ` \u2206tq\u2206t zptq\n\u00ab xptq \u00b4 1\n2\u03b2ptq\u2206t xptq `\na\n\u03b2ptq\u2206t zptq,\n(24)\nwhere the approximate equality holds when \u2206t ! 1. Therefore, in the limit of \u2206t \u00d1 0, Eq. (24)\nconverges to the following VP SDE:\ndx \u201c \u00b41\n2\u03b2ptqx dt `\na\n\u03b2ptq dw.\n(25)\nSo far, we have demonstrated that the noise perturbations used in SMLD and DDPM correspond to\ndiscretizations of VE and VP SDEs respectively. The VE SDE always yields a process with exploding\nvariance when t \u00d1 8. In contrast, the VP SDE yields a process with bounded variance. In addition,\nthe process has a constant unit variance for all t P r0, 8q when ppxp0qq has a unit variance. Since the\nVP SDE has af\ufb01ne drift and diffusion coef\ufb01cients, we can use Eq. (5.51) in S\u00a8arkk\u00a8a & Solin (2019) to\nobtain an ODE that governs the evolution of variance\nd\u03a3VPptq\ndt\n\u201c \u03b2ptqpI \u00b4 \u03a3VPptqq,\n14\nPublished as a conference paper at ICLR 2021\nwhere \u03a3VPptq :\u201c Covrxptqs for txptqu1\nt\u201c0 obeying a VP SDE. Solving this ODE, we obtain\n\u03a3VPptq \u201c I ` e\n\u015ft\n0 \u00b4\u03b2psqdsp\u03a3VPp0q \u00b4 Iq,\n(26)\nfrom which it is clear that the variance \u03a3VPptq is always bounded given \u03a3VPp0q. Moreover, \u03a3VPptq \u201d\nI if \u03a3VPp0q \u201c I. Due to this difference, we name Eq. (9) as the Variance Exploding (VE) SDE, and\nEq. (11) the Variance Preserving (VP) SDE.\nInspired by the VP SDE, we propose a new SDE called the sub-VP SDE, namely\ndx \u201c \u00b41\n2\u03b2ptqx dt `\nb\n\u03b2ptqp1 \u00b4 e\u00b42\n\u015ft\n0 \u03b2psqdsqdw.\n(27)\nFollowing standard derivations, it is straightforward to show that Erxptqs is the same for both VP and\nsub-VP SDEs; the variance function of sub-VP SDEs is different, given by\n\u03a3sub-VPptq \u201c I ` e\u00b42\n\u015ft\n0 \u03b2psqdsI ` e\u00b4\n\u015ft\n0 \u03b2psqdsp\u03a3sub-VPp0q \u00b4 2Iq,\n(28)\nwhere \u03a3sub-VPptq :\u201c Covrxptqs for a process txptqu1\nt\u201c0 obtained by solving Eq. (27). In addition,\nwe observe that (i) \u03a3sub-VPptq \u010f \u03a3VPptq for all t \u011b 0 with \u03a3sub-VPp0q \u201c \u03a3VPp0q and shared \u03b2psq;\nand (ii) limt\u00d18 \u03a3sub-VPptq \u201c limt\u00d18 \u03a3VPptq \u201c I if limt\u00d18\n\u015ft\n0 \u03b2psqds \u201c 8. The former is why we\nname Eq. (27) the sub-VP SDE\u2014its variance is always upper bounded by the corresponding VP\nSDE. The latter justi\ufb01es the use of sub-VP SDEs for score-based generative modeling, since they can\nperturb any data distribution to standard Gaussian under suitable conditions, just like VP SDEs.\nVE, VP and sub-VP SDEs all have af\ufb01ne drift coef\ufb01cients. Therefore, their perturbation kernels\np0tpxptq | xp0qq are all Gaussian and can be computed with Eqs. (5.50) and (5.51) in S\u00a8arkk\u00a8a & Solin\n(2019):\np0tpxptq | xp0qq \u201c\n$\n\u2019\n&\n\u2019\n%\nN\n`\nxptq; xp0q, r\u03c32ptq \u00b4 \u03c32p0qsI\n\u02d8\n,\n(VE SDE)\nN\n`\nxptq; xp0qe\u00b4 1\n2\n\u015ft\n0 \u03b2psqds, I \u00b4 Ie\u00b4\n\u015ft\n0 \u03b2psqds\u02d8\n(VP SDE)\nN\n`\nxptq; xp0qe\u00b4 1\n2\n\u015ft\n0 \u03b2psqds, r1 \u00b4 e\u00b4\n\u015ft\n0 \u03b2psqdss2I\n\u02d8\n(sub-VP SDE)\n.\n(29)\nAs a result, all SDEs introduced here can be ef\ufb01ciently trained with the objective in Eq. (7).\nC\nSDES IN THE WILD\nBelow we discuss concrete instantiations of VE and VP SDEs whose discretizations yield SMLD\nand DDPM models, and the speci\ufb01c sub-VP SDE used in our experiments. In SMLD, the noise\nscales t\u03c3iuN\ni\u201c1 is typically a geometric sequence where \u03c3min is \ufb01xed to 0.01 and \u03c3max is chosen\naccording to Technique 1 in Song & Ermon (2020). Usually, SMLD models normalize image inputs\nto the range r0, 1s. Since t\u03c3iuN\ni\u201c1 is a geometric sequence, we have \u03c3p i\nN q \u201c \u03c3i \u201c \u03c3min\n\u00b4\n\u03c3max\n\u03c3min\n\u00af i\u00b41\nN\u00b41\nfor i \u201c 1, 2, \u00a8 \u00a8 \u00a8 , N. In the limit of N \u00d1 8, we have \u03c3ptq \u201c \u03c3min\n\u00b4\n\u03c3max\n\u03c3min\n\u00aft\nfor t P p0, 1s. The\ncorresponding VE SDE is\ndx \u201c \u03c3min\n\u02c6\u03c3max\n\u03c3min\n\u02d9tc\n2 log \u03c3max\n\u03c3min\ndw,\nt P p0, 1s,\n(30)\nand the perturbation kernel can be derived via Eq. (29):\np0tpxptq | xp0qq \u201c N\n\u02c6\nxptq; xp0q, \u03c32\nmin\n\u00b4\u03c3max\n\u03c3min\n\u00af2t\nI\n\u02d9\n,\nt P p0, 1s.\n(31)\nThere is one subtlety when t \u201c 0: by de\ufb01nition, \u03c3p0q \u201c \u03c30 \u201c 0 (following the convention in Eq. (20)),\nbut \u03c3p0`q :\u201c limt\u00d10` \u03c3ptq \u201c \u03c3min \u2030 0. In other words, \u03c3ptq for SMLD is not differentiable since\n\u03c3p0q \u2030 \u03c3p0`q, causing the VE SDE in Eq. (21) unde\ufb01ned for t \u201c 0. In practice, we bypass this issue\nby always solving the SDE and its associated probability \ufb02ow ODE in the range t P r\u03f5, 1s for some\nsmall constant \u03f5 \u0105 0, and we use \u03f5 \u201c 10\u00b45 in our VE SDE experiments.\n15\nPublished as a conference paper at ICLR 2021\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0\n500\n1000\n1500\n2000\n2500\nVariance\nVariance of Perturbation Kernels\nSMLD original\nVE SDE\n(a) SMLD\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nScaling Factor of Means\nMean of Perturbation Kernels\nDDPM original\nVP SDE\n(b) DDPM (mean)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nt\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nVariance\nVariance of Perturbation Kernels\nDDPM original\nVP SDE\n(c) DDPM (variance)\nFigure 5: Discrete-time perturbation kernels and our continuous generalizations match each other\nalmost exactly. (a) compares the variance of perturbation kernels for SMLD and VE SDE; (b)\ncompares the scaling factors of means of perturbation kernels for DDPM and VP SDE; and (c)\ncompares the variance of perturbation kernels for DDPM and VP SDE.\nFor DDPM models, t\u03b2iuN\ni\u201c1 is typically an arithmetic sequence where \u03b2i \u201c\n\u00af\u03b2min\nN `\ni\u00b41\nNpN\u00b41qp\u00af\u03b2max \u00b4\n\u00af\u03b2minq for i \u201c 1, 2, \u00a8 \u00a8 \u00a8 , N. Therefore, \u03b2ptq \u201c \u00af\u03b2min ` tp\u00af\u03b2max \u00b4 \u00af\u03b2minq for t P r0, 1s in the limit of\nN \u00d1 8. This corresponds to the following instantiation of the VP SDE:\ndx \u201c \u00b41\n2p\u00af\u03b2min ` tp\u00af\u03b2max \u00b4 \u00af\u03b2minqqxdt `\nb\n\u00af\u03b2min ` tp\u00af\u03b2max \u00b4 \u00af\u03b2minqdw,\nt P r0, 1s,\n(32)\nwhere xp0q \u201e pdatapxq. In our experiments, we let \u00af\u03b2min \u201c 0.1 and \u00af\u03b2max \u201c 20 to match the settings in\nHo et al. (2020). The perturbation kernel is given by\np0tpxptq | xp0qq\n\u201c N\n\u00b4\nxptq; e\u00b4 1\n4 t2p \u00af\u03b2max\u00b4 \u00af\u03b2minq\u00b4 1\n2 t \u00af\u03b2minxp0q, I \u00b4 Ie\u00b4 1\n2 t2p \u00af\u03b2max\u00b4 \u00af\u03b2minq\u00b4t \u00af\u03b2min\n\u00af\n,\nt P r0, 1s.\n(33)\nFor DDPM, there is no discontinuity issue with the corresponding VP SDE; yet, there are numerical\ninstability issues for training and sampling at t \u201c 0, due to the vanishing variance of xptq as t \u00d1 0.\nTherefore, same as the VE SDE, we restrict computation to t P r\u03f5, 1s for a small \u03f5 \u0105 0. For sampling,\nwe choose \u03f5 \u201c 10\u00b43 so that the variance of xp\u03f5q in VP SDE matches the variance of x1 in DDPM;\nfor training and likelihood computation, we adopt \u03f5 \u201c 10\u00b45 which empirically gives better results.\nAs a sanity check for our SDE generalizations to SMLD and DDPM, we compare the perturbation\nkernels of SDEs and original discrete Markov chains in Fig. 5. The SMLD and DDPM models both\nuse N \u201c 1000 noise scales. For SMLD, we only need to compare the variances of perturbation\nkernels since means are the same by de\ufb01nition. For DDPM, we compare the scaling factors of means\nand the variances. As demonstrated in Fig. 5, the discrete perturbation kernels of original SMLD and\nDDPM models align well with perturbation kernels derived from VE and VP SDEs.\nFor sub-VP SDEs, we use exactly the same \u03b2ptq as VP SDEs. This leads to the following perturbation\nkernel\np0tpxptq | xp0qq\n\u201c N\n\u00b4\nxptq; e\u00b4 1\n4 t2p \u00af\u03b2max\u00b4 \u00af\u03b2minq\u00b4 1\n2 t \u00af\u03b2minxp0q, r1 \u00b4 e\u00b4 1\n2 t2p \u00af\u03b2max\u00b4 \u00af\u03b2minq\u00b4t \u00af\u03b2mins2I\n\u00af\n,\nt P r0, 1s.\n(34)\nWe also restrict numerical computation to the same interval of r\u03f5, 1s as VP SDEs.\nEmpirically, we observe that smaller \u03f5 generally yields better likelihood values for all SDEs. For\nsampling, it is important to use an appropriate \u03f5 for better Inception scores and FIDs, although\nsamples across different \u03f5 look visually the same to human eyes.\nD\nPROBABILITY FLOW ODE\nD.1\nDERIVATION\nThe idea of probability \ufb02ow ODE is inspired by Maoutsa et al. (2020), and one can \ufb01nd the derivation\nof a simpli\ufb01ed case therein. Below we provide a derivation for the fully general ODE in Eq. (17). We\n16\nPublished as a conference paper at ICLR 2021\nconsider the SDE in Eq. (15), which possesses the following form:\ndx \u201c fpx, tqdt ` Gpx, tqdw,\nwhere fp\u00a8, tq : Rd \u00d1 Rd and Gp\u00a8, tq : Rd \u00d1 Rd\u02c6d. The marginal probability density ptpxptqq\nevolves according to Kolmogorov\u2019s forward equation (Fokker-Planck equation) (\u00d8ksendal, 2003)\nBptpxq\nBt\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nrfipx, tqptpxqs ` 1\n2\nd\u00ff\ni\u201c1\nd\u00ff\nj\u201c1\nB2\nBxiBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq\n\u0131\n.\n(35)\nWe can easily rewrite Eq. (35) to obtain\nBptpxq\nBt\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nrfipx, tqptpxqs ` 1\n2\nd\u00ff\ni\u201c1\nd\u00ff\nj\u201c1\nB2\nBxiBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq\n\u0131\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nrfipx, tqptpxqs ` 1\n2\nd\u00ff\ni\u201c1\nB\nBxi\n\u201d\nd\u00ff\nj\u201c1\nB\nBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq\n\u0131\u0131\n. (36)\nNote that\nd\u00ff\nj\u201c1\nB\nBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq\n\u0131\n\u201c\nd\u00ff\nj\u201c1\nB\nBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tq\n\u0131\nptpxq `\nd\u00ff\nj\u201c1\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq B\nBxj\nlog ptpxq\n\u201cptpxq\u2207\u00a8 rGpx, tqGpx, tqTs ` ptpxqGpx, tqGpx, tqT\u2207x log ptpxq,\nbased on which we can continue the rewriting of Eq. (36) to obtain\nBptpxq\nBt\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nrfipx, tqptpxqs ` 1\n2\nd\u00ff\ni\u201c1\nB\nBxi\n\u201d\nd\u00ff\nj\u201c1\nB\nBxj\n\u201d\nd\u00ff\nk\u201c1\nGikpx, tqGjkpx, tqptpxq\n\u0131\u0131\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nrfipx, tqptpxqs\n` 1\n2\nd\u00ff\ni\u201c1\nB\nBxi\n\u201d\nptpxq\u2207\u00a8 rGpx, tqGpx, tqTs ` ptpxqGpx, tqGpx, tqT\u2207x log ptpxq\n\u0131\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\n!\nfipx, tqptpxq\n\u00b4 1\n2\n\u201d\n\u2207\u00a8 rGpx, tqGpx, tqTs ` Gpx, tqGpx, tqT\u2207x log ptpxq\n\u0131\nptpxq\n)\n\u201c \u00b4\nd\u00ff\ni\u201c1\nB\nBxi\nr \u02dcfipx, tqptpxqs,\n(37)\nwhere we de\ufb01ne\n\u02dcfpx, tq :\u201c fpx, tq \u00b4 1\n2\u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 1\n2Gpx, tqGpx, tqT\u2207x log ptpxq.\nInspecting Eq. (37), we observe that it equals Kolmogorov\u2019s forward equation of the following\nSDE with \u02dcGpx, tq :\u201c 0 (Kolmogorov\u2019s forward equation in this case is also known as the Liouville\nequation.)\ndx \u201c \u02dcfpx, tqdt ` \u02dcGpx, tqdw,\nwhich is essentially an ODE:\ndx \u201c \u02dcfpx, tqdt,\nsame as the probability \ufb02ow ODE given by Eq. (17). Therefore, we have shown that the probability\n\ufb02ow ODE Eq. (17) induces the same marginal probability density ptpxq as the SDE in Eq. (15).\n17\nPublished as a conference paper at ICLR 2021\nD.2\nLIKELIHOOD COMPUTATION\nThe probability \ufb02ow ODE in Eq. (17) has the following form when we replace the score \u2207x log ptpxq\nwith the time-dependent score-based model s\u03b8px, tq:\ndx \u201c\n\"\nfpx, tq \u00b4 1\n2\u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 1\n2Gpx, tqGpx, tqTs\u03b8px, tq\n*\nloooooooooooooooooooooooooooooooooooooooooomoooooooooooooooooooooooooooooooooooooooooon\n\u201c:\u02dcf\u03b8px,tq\ndt.\n(38)\nWith the instantaneous change of variables formula (Chen et al., 2018), we can compute the log-\nlikelihood of p0pxq using\nlog p0pxp0qq \u201c log pT pxpTqq `\n\u017c T\n0\n\u2207\u00a8 \u02dcf\u03b8pxptq, tqdt,\n(39)\nwhere the random variable xptq as a function of t can be obtained by solving the probability \ufb02ow\nODE in Eq. (38). In many cases computing \u2207\u00a8 \u02dcf\u03b8px, tq is expensive, so we follow Grathwohl et al.\n(2018) to estimate it with the Skilling-Hutchinson trace estimator (Skilling, 1989; Hutchinson, 1990).\nIn particular, we have\n\u2207\u00a8 \u02dcf\u03b8px, tq \u201c Epp\u03f5qr\u03f5T\u2207\u02dcf\u03b8px, tq\u03f5s,\n(40)\nwhere \u2207\u02dcf\u03b8 denotes the Jacobian of \u02dcf\u03b8p\u00a8, tq, and the random variable \u03f5 satis\ufb01es Epp\u03f5qr\u03f5s \u201c 0 and\nCovpp\u03f5qr\u03f5s \u201c I. The vector-Jacobian product \u03f5T\u2207\u02dcf\u03b8px, tq can be ef\ufb01ciently computed using reverse-\nmode automatic differentiation, at approximately the same cost as evaluating \u02dcf\u03b8px, tq. As a result,\nwe can sample \u03f5 \u201e pp\u03f5q and then compute an ef\ufb01cient unbiased estimate to \u2207\u00a8 \u02dcf\u03b8px, tq using\n\u03f5T\u2207\u02dcf\u03b8px, tq\u03f5. Since this estimator is unbiased, we can attain an arbitrarily small error by averaging\nover a suf\ufb01cient number of runs. Therefore, by applying the Skilling-Hutchinson estimator Eq. (40)\nto Eq. (39), we can compute the log-likelihood to any accuracy.\nIn our experiments, we use the RK45 ODE solver (Dormand & Prince, 1980) provided by\nscipy.integrate.solve_ivp in all cases. The bits/dim values in Table 2 are computed\nwith atol=1e-5 and rtol=1e-5, same as Grathwohl et al. (2018). To give the likelihood results\nof our models in Table 2, we average the bits/dim obtained on the test dataset over \ufb01ve different runs\nwith \u03f5 \u201c 10\u00b45 (see de\ufb01nition of \u03f5 in Appendix C).\nD.3\nPROBABILITY FLOW SAMPLING\nSuppose we have a forward SDE\ndx \u201c fpx, tqdt ` Gptqdw,\nand one of its discretization\nxi`1 \u201c xi ` fipxiq ` Gizi,\ni \u201c 0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1,\n(41)\nwhere zi \u201e Np0, Iq. We assume the discretization schedule of time is \ufb01xed beforehand, and thus\nwe absorb the dependency on \u2206t into the notations of fi and Gi. Using Eq. (17), we can obtain the\nfollowing probability \ufb02ow ODE:\ndx \u201c\n\"\nfpx, tq \u00b4 1\n2GptqGptqT\u2207x log ptpxq\n*\ndt.\n(42)\nWe may employ any numerical method to integrate the probability \ufb02ow ODE backwards in time for\nsample generation. In particular, we propose a discretization in a similar functional form to Eq. (41):\nxi \u201c xi`1 \u00b4 fi`1pxi`1q ` 1\n2Gi`1GT\ni`1s\u03b8\u02dapxi`1, i ` 1q,\ni \u201c 0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1,\nwhere the score-based model s\u03b8\u02dapxi, iq is conditioned on the iteration number i. This is a determin-\nistic iteration rule. Unlike reverse diffusion samplers or ancestral sampling, there is no additional\nrandomness once the initial sample xN is obtained from the prior distribution. When applied to\nSMLD models, we can get the following iteration rule for probability \ufb02ow sampling:\nxi \u201c xi`1 ` 1\n2p\u03c32\ni`1 \u00b4 \u03c32\ni qs\u03b8\u02dapxi`1, \u03c3i`1q,\ni \u201c 0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1.\n(43)\nSimilarly, for DDPM models, we have\nxi \u201c p2 \u00b4\na\n1 \u00b4 \u03b2i`1qxi`1 ` 1\n2\u03b2i`1s\u03b8\u02dapxi`1, i ` 1q,\ni \u201c 0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1.\n(44)\n18\nPublished as a conference paper at ICLR 2021\nD.4\nSAMPLING WITH BLACK-BOX ODE SOLVERS\nFor producing \ufb01gures in Fig. 3, we use a DDPM model trained on 256 \u02c6 256 CelebA-HQ with the\nsame settings in Ho et al. (2020). All FID scores of our models in Table 2 are computed on samples\nfrom the RK45 ODE solver implemented in scipy.integrate.solve_ivp with atol=1e-5\nand rtol=1e-5. We use \u03f5 \u201c 10\u00b45 for VE SDEs and \u03f5 \u201c 10\u00b43 for VP SDEs (see also Appendix C).\nAside from the interpolation results in Fig. 3, we demonstrate more examples of latent space\nmanipulation in Fig. 6, including interpolation and temperature scaling. The model tested here is a\nDDPM model trained with the same settings in Ho et al. (2020).\nAlthough solvers for the probability \ufb02ow ODE allow fast sampling, their samples typically have\nhigher (worse) FID scores than those from SDE solvers if no corrector is used. We have this\nempirical observation for both the discretization strategy in Appendix D.3, and black-box ODE\nsolvers introduced above. Moreover, the performance of probability \ufb02ow ODE samplers depends on\nthe choice of the SDE\u2014their sample quality for VE SDEs is much worse than VP SDEs especially\nfor high-dimensional data.\nD.5\nUNIQUELY IDENTIFIABLE ENCODING\nAs a sanity check, we train two models (denoted as \u201cModel A\u201d and \u201cModel B\u201d) with different\narchitectures using the VE SDE on CIFAR-10. Here Model A is an NCSN++ model with 4 layers per\nresolution trained using the continuous objective in Eq. (7), and Model B is all the same except that it\nuses 8 layers per resolution. Model de\ufb01nitions are in Appendix H.\nWe report the latent codes obtained by Model A and Model B for a random CIFAR-10 image in\nFig. 7. In Fig. 8, we show the dimension-wise differences and correlation coef\ufb01cients between latent\nencodings on a total of 16 CIFAR-10 images. Our results demonstrate that for the same inputs, Model\nA and Model B provide encodings that are close in every dimension, despite having different model\narchitectures and training runs.\nE\nREVERSE DIFFUSION SAMPLING\nGiven a forward SDE\ndx \u201c fpx, tqdt ` Gptqdw,\nand suppose the following iteration rule is a discretization of it:\nxi`1 \u201c xi ` fipxiq ` Gizi,\ni \u201c 0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1\n(45)\nwhere zi \u201e Np0, Iq. Here we assume the discretization schedule of time is \ufb01xed beforehand, and\nthus we can absorb it into the notations of fi and Gi.\nBased on Eq. (45), we propose to discretize the reverse-time SDE\ndx \u201c rfpx, tq \u00b4 GptqGptqT\u2207x log ptpxqsdt ` Gptqd \u00afw,\nwith a similar functional form, which gives the following iteration rule for i P t0, 1, \u00a8 \u00a8 \u00a8 , N \u00b4 1u:\nxi \u201c xi`1 \u00b4 fi`1pxi`1q ` Gi`1GT\ni`1s\u03b8\u02dapxi`1, i ` 1q ` Gi`1zi`1,\n(46)\nwhere our trained score-based model s\u03b8\u02dapxi, iq is conditioned on iteration number i.\nWhen applying Eq. (46) to Eqs. (10) and (20), we obtain a new set of numerical solvers for the\nreverse-time VE and VP SDEs, resulting in sampling algorithms as shown in the \u201cpredictor\u201d part of\nAlgorithms 2 and 3. We name these sampling methods (that are based on the discretization strategy\nin Eq. (46)) reverse diffusion samplers.\nAs expected, the ancestral sampling of DDPM (Ho et al., 2020) (Eq. (4)) matches its reverse diffusion\ncounterpart when \u03b2i \u00d1 0 for all i (which happens when \u2206t \u00d1 0 since \u03b2i \u201c \u00af\u03b2i\u2206t, see Appendix B),\n19\nPublished as a conference paper at ICLR 2021\nFigure 6: Samples from the probability \ufb02ow ODE for VP SDE on 256 \u02c6 256 CelebA-HQ. Top:\nspherical interpolations between random samples. Bottom: temperature rescaling (reducing norm of\nembedding).\n20\nPublished as a conference paper at ICLR 2021\n0\n20\n40\n60\n80\n100\nDimension\n100\n0\n100\nLatent value\nModel A\nModel B\nFigure 7: Comparing the \ufb01rst 100 dimensions of the latent code obtained for a random CIFAR-10\nimage. \u201cModel A\u201d and \u201cModel B\u201d are separately trained with different architectures.\n0\n100\n200\n300\nDifference in encodings\nModel A \n vs.\n Model B\nModel A (shuffled) \n vs. \n Model B (shuffled)\n0.00\n0.25\n0.50\n0.75\n1.00\nCorrelation Coefficient\n0\n200\n400\n600\nCount\nModel A\nModel B\nr=0.96\nx1(T)\nFigure 8: Left: The dimension-wise difference between encodings obtained by Model A and B. As a\nbaseline, we also report the difference between shuf\ufb02ed representations of these two models. Right:\nThe dimension-wise correlation coef\ufb01cients of encodings obtained by Model A and Model B.\nbecause\nxi \u201c\n1\na\n1 \u00b4 \u03b2i`1\npxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1qq `\na\n\u03b2i`1zi`1\n\u201c\n\u02c6\n1 ` 1\n2\u03b2i`1 ` op\u03b2i`1q\n\u02d9\npxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1qq `\na\n\u03b2i`1zi`1\n\u00ab\n\u02c6\n1 ` 1\n2\u03b2i`1\n\u02d9\npxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1qq `\na\n\u03b2i`1zi`1\n\u201c\n\u02c6\n1 ` 1\n2\u03b2i`1\n\u02d9\nxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q ` 1\n2\u03b22\ni`1s\u03b8\u02dapxi`1, i ` 1q `\na\n\u03b2i`1zi`1\n\u00ab\n\u02c6\n1 ` 1\n2\u03b2i`1\n\u02d9\nxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q `\na\n\u03b2i`1zi`1\n\u201c\n\u201e\n2 \u00b4\n\u02c6\n1 \u00b4 1\n2\u03b2i`1\n\u02d9\uf6be\nxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q `\na\n\u03b2i`1zi`1\n\u00ab\n\u201e\n2 \u00b4\n\u02c6\n1 \u00b4 1\n2\u03b2i`1\n\u02d9\n` op\u03b2i`1q\n\uf6be\nxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q `\na\n\u03b2i`1zi`1\n\u201cp2 \u00b4\na\n1 \u00b4 \u03b2i`1qxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q `\na\n\u03b2i`1zi`1.\nTherefore, the original ancestral sampler of Eq. (4) is essentially a different discretization to the same\nreverse-time SDE. This uni\ufb01es the sampling method in Ho et al. (2020) as a numerical solver to the\nreverse-time VP SDE in our continuous framework.\nF\nANCESTRAL SAMPLING FOR SMLD MODELS\nThe ancestral sampling method for DDPM models can also be adapted to SMLD models. Consider a\nsequence of noise scales \u03c31 \u0103 \u03c32 \u0103 \u00a8 \u00a8 \u00a8 \u0103 \u03c3N as in SMLD. By perturbing a data point x0 with these\nnoise scales sequentially, we obtain a Markov chain x0 \u00d1 x1 \u00d1 \u00a8 \u00a8 \u00a8 \u00d1 xN, where\nppxi | xi\u00b41q \u201c Npxi; xi\u00b41, p\u03c32\ni \u00b4 \u03c32\ni\u00b41qIq,\ni \u201c 1, 2, \u00a8 \u00a8 \u00a8 , N.\n21\nPublished as a conference paper at ICLR 2021\nAlgorithm 1 Predictor-Corrector (PC) sampling\nRequire:\nN: Number of discretization steps for the reverse-time SDE\nM: Number of corrector steps\n1: Initialize xN \u201e pT pxq\n2: for i \u201c N \u00b4 1 to 0 do\n3:\nxi \u00d0 Predictorpxi`1q\n4:\nfor j \u201c 1 to M do\n5:\nxi \u00d0 Correctorpxiq\n6: return x0\nHere we assume \u03c30 \u201c 0 to simplify notations. Following Ho et al. (2020), we can compute\nqpxi\u00b41 | xi, x0q \u201c N\n\u02c6\nxi\u00b41; \u03c32\ni\u00b41\n\u03c32\ni\nxi `\n\u00b4\n1 \u00b4 \u03c32\ni\u00b41\n\u03c32\ni\n\u00af\nx0, \u03c32\ni\u00b41p\u03c32\ni \u00b4 \u03c32\ni\u00b41q\n\u03c32\ni\nI\n\u02d9\n.\nIf we parameterize the reverse transition kernel as p\u03b8pxi\u00b41 | xiq \u201c Npxi\u00b41; \u00b5\u03b8pxi, iq, \u03c4 2\ni Iq, then\nLt\u00b41 \u201c EqrDKLpqpxi\u00b41 | xi, x0qq } p\u03b8pxi\u00b41 | xiqs\n\u201c Eq\n\u00ab\n1\n2\u03c4 2\ni\n\r\r\r\r\n\u03c32\ni\u00b41\n\u03c32\ni\nxi `\n\u00b4\n1 \u00b4 \u03c32\ni\u00b41\n\u03c32\ni\n\u00af\nx0 \u00b4 \u00b5\u03b8pxi, iq\n\r\r\r\r\n2\n2\n\ufb00\n` C\n\u201c Ex0,z\n\u00ab\n1\n2\u03c4 2\ni\n\r\r\r\rxipx0, zq \u00b4 \u03c32\ni \u00b4 \u03c32\ni\u00b41\n\u03c3i\nz \u00b4 \u00b5\u03b8pxipx0, zq, iq\n\r\r\r\r\n2\n2\n\ufb00\n` C,\nwhere Lt\u00b41 is one representative term in the ELBO objective (see Eq. (8) in Ho et al. (2020)), C is\na constant that does not depend on \u03b8, z \u201e Np0, Iq, and xipx0, zq \u201c x0 ` \u03c3iz. We can therefore\nparameterize \u00b5\u03b8pxi, iq via\n\u00b5\u03b8pxi, iq \u201c xi ` p\u03c32\ni \u00b4 \u03c32\ni\u00b41qs\u03b8pxi, iq,\nwhere s\u03b8pxi, iq is to estimate z{\u03c3i. As in Ho et al. (2020), we let \u03c4i \u201c\nc\n\u03c32\ni\u00b41p\u03c32\ni \u00b4\u03c32\ni\u00b41q\n\u03c32\ni\n. Through\nancestral sampling on \u015bN\ni\u201c1 p\u03b8pxi\u00b41 | xiq, we obtain the following iteration rule\nxi\u00b41 \u201c xi ` p\u03c32\ni \u00b4 \u03c32\ni\u00b41qs\u03b8\u02dapxi, iq `\nd\n\u03c32\ni\u00b41p\u03c32\ni \u00b4 \u03c32\ni\u00b41q\n\u03c32\ni\nzi, i \u201c 1, 2, \u00a8 \u00a8 \u00a8 , N,\n(47)\nwhere xN \u201e Np0, \u03c32\nNIq, \u03b8\u02da denotes the optimal parameter of s\u03b8, and zi \u201e Np0, Iq. We call\nEq. (47) the ancestral sampling method for SMLD models.\nG\nPREDICTOR-CORRECTOR SAMPLERS\nPredictor-Corrector (PC) sampling\nThe predictor can be any numerical solver for the reverse-\ntime SDE with a \ufb01xed discretization strategy. The corrector can be any score-based MCMC approach.\nIn PC sampling, we alternate between the predictor and corrector, as described in Algorithm 1. For\nexample, when using the reverse diffusion SDE solver (Appendix E) as the predictor, and annealed\nLangevin dynamics (Song & Ermon, 2019) as the corrector, we have Algorithms 2 and 3 for VE and\nVP SDEs respectively, where t\u03f5iuN\u00b41\ni\u201c0 are step sizes for Langevin dynamics as speci\ufb01ed below.\nThe corrector algorithms\nWe take the schedule of annealed Langevin dynamics in Song & Ermon\n(2019), but re-frame it with slight modi\ufb01cations in order to get better interpretability and empirical\nperformance. We provide the corrector algorithms in Algorithms 4 and 5 respectively, where we call\nr the \u201csignal-to-noise\u201d ratio. We determine the step size \u03f5 using the norm of the Gaussian noise \u2225z\u22252,\nnorm of the score-based model \u2225s\u03b8\u02da\u22252 and the signal-to-noise ratio r. When sampling a large batch\nof samples together, we replace the norm \u2225\u00a8\u22252 with the average norm across the mini-batch. When\nthe batch size is small, we suggest replacing \u2225z\u22252 with\n?\nd, where d is the dimensionality of z.\n22\nPublished as a conference paper at ICLR 2021\nAlgorithm 2 PC sampling (VE SDE)\n1: xN \u201e Np0, \u03c32\nmaxIq\n2: for i \u201c N \u00b4 1 to 0 do\n3:\nx1\ni \u00d0 xi`1 ` p\u03c32\ni`1 \u00b4 \u03c32\ni qs\u03b8\u02dapxi`1, \u03c3i`1q\n4:\nz \u201e Np0, Iq\n5:\nxi \u00d0 x1\ni `\nb\n\u03c32\ni`1 \u00b4 \u03c32\ni z\n6:\nfor j \u201c 1 to M do\n7:\nz \u201e Np0, Iq\n8:\nxi \u00d0 xi ` \u03f5is\u03b8\u02dapxi, \u03c3iq ` ?2\u03f5iz\n9: return x0\nAlgorithm 3 PC sampling (VP SDE)\n1: xN \u201e Np0, Iq\n2: for i \u201c N \u00b4 1 to 0 do\n3:\nx1\ni \u00d0 p2 \u00b4 ?1 \u00b4 \u03b2i`1qxi`1 ` \u03b2i`1s\u03b8\u02dapxi`1, i ` 1q\n4:\nz \u201e Np0, Iq\n5:\nxi \u00d0 x1\ni ` ?\u03b2i`1z\n6:\nfor j \u201c 1 to M do\n7:\nz \u201e Np0, Iq\n8:\nxi \u00d0 xi ` \u03f5is\u03b8\u02dapxi, iq ` ?2\u03f5iz\n9: return x0\nAlgorithm 4 Corrector algorithm (VE SDE).\nRequire: t\u03c3iuN\ni\u201c1, r, N, M.\n1: x0\nN \u201e Np0, \u03c32\nmaxIq\n2: for i \u00d0 N to 1 do\n3:\nfor j \u00d0 1 to M do\n4:\nz \u201e Np0, Iq\n5:\ng \u00d0 s\u03b8\u02dapxj\u00b41\ni\n, \u03c3iq\n6:\n\u03f5 \u00d0 2pr \u2225z\u22252 { \u2225g\u22252q2\n7:\nxj\ni \u00d0 xj\u00b41\ni\n` \u03f5 g `\n?\n2\u03f5 z\n8:\nx0\ni\u00b41 \u00d0 xM\ni\nreturn x0\n0\nAlgorithm 5 Corrector algorithm (VP SDE).\nRequire: t\u03b2iuN\ni\u201c1, t\u03b1iuN\ni\u201c1, r, N, M.\n1: x0\nN \u201e Np0, Iq\n2: for i \u00d0 N to 1 do\n3:\nfor j \u00d0 1 to M do\n4:\nz \u201e Np0, Iq\n5:\ng \u00d0 s\u03b8\u02dapxj\u00b41\ni\n, iq\n6:\n\u03f5 \u00d0 2\u03b1ipr \u2225z\u22252 { \u2225g\u22252q2\n7:\nxj\ni \u00d0 xj\u00b41\ni\n` \u03f5 g `\n?\n2\u03f5 z\n8:\nx0\ni\u00b41 \u00d0 xM\ni\nreturn x0\n0\nDenoising\nFor both SMLD and DDPM models, the generated samples typically contain small\nnoise that is hard to detect by humans. As noted by Jolicoeur-Martineau et al. (2020), FIDs can be\nsigni\ufb01cantly worse without removing this noise. This unfortunate sensitivity to noise is also part of\nthe reason why NCSN models trained with SMLD has been performing worse than DDPM models\nin terms of FID, because the former does not use a denoising step at the end of sampling, while the\nlatter does. In all experiments of this paper we ensure there is a single denoising step at the end of\nsampling, using Tweedie\u2019s formula (Efron, 2011).\nFigure 9: PC sampling for LSUN bedroom and church. The vertical axis corresponds to the total\ncomputation, and the horizontal axis represents the amount of computation allocated to the corrector.\nSamples are the best when computation is split between the predictor and corrector.\nTraining\nWe use the same architecture in Ho et al. (2020) for our score-based models. For the\nVE SDE, we train a model with the original SMLD objective in Eq. (1); similarly for the VP SDE,\nwe use the original DDPM objective in Eq. (3). We apply a total number of 1000 noise scales for\ntraining both models. For results in Fig. 9, we train an NCSN++ model (de\ufb01nition in Appendix H) on\n23\nPublished as a conference paper at ICLR 2021\nTable 4: Comparing different samplers on CIFAR-10, where \u201cP2000\u201d uses the rounding interpolation\nbetween noise scales. Shaded regions are obtained with the same computation (number of score\nfunction evaluations). Mean and standard deviation are reported over \ufb01ve sampling runs.\nVariance Exploding SDE (SMLD)\nVariance Preserving SDE (DDPM)\nPredictor\nFID\u00d3\nSampler\nP1000\nP2000\nC2000\nPC1000\nP1000\nP2000\nC2000\nPC1000\nancestral sampling\n4.98 \u02d8 .06\n4.92 \u02d8 .02\n3.62 \u02d8 .03\n3.24 \u02d8 .02\n3.11 \u02d8 .03\n3.21 \u02d8 .02\nreverse diffusion\n4.79 \u02d8 .07\n4.72 \u02d8 .07\n3.60 \u02d8 .02\n3.21 \u02d8 .02\n3.10 \u02d8 .03\n3.18 \u02d8 .01\nprobability \ufb02ow\n15.41 \u02d8 .15\n12.87 \u02d8 .09\n20.43 \u02d8 .07\n3.51 \u02d8 .04\n3.59 \u02d8 .04\n3.25 \u02d8 .04\n19.06 \u02d8 .06\n3.06 \u02d8 .03\nTable 5: Optimal signal-to-noise ratios of different samplers. \u201cP1000\u201d or \u201cP2000\u201d: predictor-only\nsamplers using 1000 or 2000 steps. \u201cC2000\u201d: corrector-only samplers using 2000 steps. \u201cPC1000\u201d:\nPC samplers using 1000 predictor and 1000 corrector steps.\nVE SDE (SMLD)\nVP SDE (DDPM)\nPredictor\nr\nSampler\nP1000\nP2000\nC2000\nPC1000\nP1000\nP2000\nC2000\nPC1000\nancestral sampling\n-\n-\n0.17\n-\n-\n0.01\nreverse diffusion\n-\n-\n0.16\n-\n-\n0.01\nprobability \ufb02ow\n-\n-\n0.22\n0.17\n-\n-\n0.27\n0.04\n256 \u02c6 256 LSUN bedroom and church outdoor (Yu et al., 2015) datasets with the VE SDE and our\ncontinuous objective Eq. (7). The batch size is \ufb01xed to 128 on CIFAR-10 and 64 on LSUN.\nAd-hoc interpolation methods for noise scales\nModels in this experiment are all trained with\n1000 noise scales. To get results for P2000 (predictor-only sampler using 2000 steps) which requires\n2000 noise scales, we need to interpolate between 1000 noise scales at test time. The speci\ufb01c\narchitecture of the noise-conditional score-based model in Ho et al. (2020) uses sinusoidal positional\nembeddings for conditioning on integer time steps. This allows us to interpolate between noise scales\nat test time in an ad-hoc way (while it is hard to do so for other architectures like the one in Song &\nErmon (2019)). Speci\ufb01cally, for SMLD models, we keep \u03c3min and \u03c3max \ufb01xed and double the number\nof time steps. For DDPM models, we halve \u03b2min and \u03b2max before doubling the number of time steps.\nSuppose ts\u03b8px, iquN\u00b41\ni\u201c0\nis a score-based model trained on N time steps, and let ts1\n\u03b8px, iqu2N\u00b41\ni\u201c0\ndenote the corresponding interpolated score-based model at 2N time steps. We test two different\ninterpolation strategies for time steps: linear interpolation where s1\n\u03b8px, iq \u201c s\u03b8px, i{2q and rounding\ninterpolation where s1\n\u03b8px, iq \u201c s\u03b8px, ti{2uq. We provide results with linear interpolation in Table 1,\nand give results of rounding interpolation in Table 4. We observe that different interpolation methods\nresult in performance differences but maintain the general trend of predictor-corrector methods\nperforming on par or better than predictor-only or corrector-only samplers.\nHyper-parameters of the samplers\nFor Predictor-Corrector and corrector-only samplers on\nCIFAR-10, we search for the best signal-to-noise ratio (r) over a grid that increments at 0.01.\nWe report the best r in Table 5. For LSUN bedroom/church outdoor, we \ufb01x r to 0.075. Unless\notherwise noted, we use one corrector step per noise scale for all PC samplers. We use two corrector\nsteps per noise scale for corrector-only samplers on CIFAR-10. For sample generation, the batch size\nis 1024 on CIFAR-10 and 8 on LSUN bedroom/church outdoor.\nH\nARCHITECTURE IMPROVEMENTS\nWe explored several architecture designs to improve score-based models for both VE and VP SDEs.\nOur endeavor gives rise to new state-of-the-art sample quality on CIFAR-10, new state-of-the-art\nlikelihood on uniformly dequantized CIFAR-10, and enables the \ufb01rst high-\ufb01delity image samples of\nresolution 1024 \u02c6 1024 from score-based generative models. Code and checkpoints are open-sourced\nat https://github.com/yang-song/score sde.\n24\nPublished as a conference paper at ICLR 2021\nH.1\nSETTINGS FOR ARCHITECTURE EXPLORATION\nUnless otherwise noted, all models are trained for 1.3M iterations, and we save one checkpoint per\n50k iterations. For VE SDEs, we consider two datasets: 32 \u02c6 32 CIFAR-10 (Krizhevsky et al., 2009)\nand 64 \u02c6 64 CelebA (Liu et al., 2015), pre-processed following Song & Ermon (2020). We compare\ndifferent con\ufb01gurations based on their FID scores averaged over checkpoints after 0.5M iterations.\nFor VP SDEs, we only consider the CIFAR-10 dataset to save computation, and compare models\nbased on the average FID scores over checkpoints obtained between 0.25M and 0.5M iterations,\nbecause FIDs turn to increase after 0.5M iterations for VP SDEs.\nAll FIDs are computed on 50k samples with tensorflow gan. For sampling, we use the PC\nsampler discretized at 1000 time steps. We choose reverse diffusion (see Appendix E) as the predictor.\nWe use one corrector step per update of the predictor for VE SDEs with a signal-to-noise ratio of\n0.16, but save the corrector step for VP SDEs since correctors there only give slightly better results\nbut require double computation. We follow Ho et al. (2020) for optimization, including the learning\nrate, gradient clipping, and learning rate warm-up schedules. Unless otherwise noted, models are\ntrained with the original discrete SMLD and DDPM objectives in Eqs. (1) and (3) and use a batch\nsize of 128. The optimal architectures found under these settings are subsequently transferred to\ncontinuous objectives and deeper models. We also directly transfer the best architecture for VP SDEs\nto sub-VP SDEs, given the similarity of these two SDEs.\nCIFAR-10\nCelebA\ndataset\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\nFIR\nFalse\nTrue\nCIFAR-10\nCelebA\ndataset\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\nskip_rescale\nFalse\nTrue\nCIFAR-10\nCelebA\ndataset\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\nresblock_type\nddpm\nbiggan\nCIFAR-10\nCelebA\ndataset\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\nnum_res_blocks\n2\n4\nCIFAR-10\nCelebA\ndataset\n2.5\n3.0\n3.5\n4.0\n4.5\nFID\nProgressive Arch. (input, output)\nnone, none\ninput_skip, none\nresidual, none\nnone, output_skip\ninput_skip, output_skip\nresidual, output_skip\nnone, residual\ninput_skip, residual\nresidual, residual\nFigure 10: The effects of different architecture components for score-based models trained with VE\nperturbations.\nOur architecture is mostly based on Ho et al. (2020). We additionally introduce the following\ncomponents to maximize the potential improvement of score-based models.\n1. Upsampling and downsampling images with anti-aliasing based on Finite Impulse Re-\nsponse (FIR) (Zhang, 2019). We follow the same implementation and hyper-parameters in\nStyleGAN-2 (Karras et al., 2020b).\n2. Rescaling all skip connections by 1{\n?\n2. This has been demonstrated effective in several best-\nin-class GAN models, including ProgressiveGAN (Karras et al., 2018), StyleGAN (Karras\net al., 2019) and StyleGAN-2 (Karras et al., 2020b).\n3. Replacing the original residual blocks in DDPM with residual blocks from BigGAN (Brock\net al., 2018).\n4. Increasing the number of residual blocks per resolution from 2 to 4.\n25\nPublished as a conference paper at ICLR 2021\n5. Incorporating progressive growing architectures. We consider two progressive architectures\nfor input: \u201cinput skip\u201d and \u201cresidual\u201d, and two progressive architectures for output: \u201coutput\nskip\u201d and \u201cresidual\u201d. These progressive architectures are de\ufb01ned and implemented according\nto StyleGAN-2.\nWe also tested equalized learning rates, a trick used in very successful models like Progressive-\nGAN (Karras et al., 2018) and StyleGAN (Karras et al., 2019). However, we found it harmful at an\nearly stage of our experiments, and therefore decided not to explore more on it.\nThe exponential moving average (EMA) rate has a signi\ufb01cant impact on performance. For models\ntrained with VE perturbations, we notice that 0.999 works better than 0.9999, whereas for models\ntrained with VP perturbations it is the opposite. We therefore use an EMA rate of 0.999 and 0.9999\nfor VE and VP models respectively.\nH.2\nRESULTS ON CIFAR-10\nAll architecture components introduced above can improve the performance of score-based models\ntrained with VE SDEs, as shown in Fig. 10. The box plots demonstrate the importance of each\ncomponent when other components can vary freely. On both CIFAR-10 and CelebA, the additional\ncomponents that we explored always improve the performance on average for VE SDEs. For\nprogressive growing, it is not clear which combination of con\ufb01gurations consistently performs the\nbest, but the results are typically better than when no progressive growing architecture is used.\nOur best score-based model for VE SDEs 1) uses FIR upsampling/downsampling, 2) rescales skip\nconnections, 3) employs BigGAN-type residual blocks, 4) uses 4 residual blocks per resolution\ninstead of 2, and 5) uses \u201cresidual\u201d for input and no progressive growing architecture for output. We\nname this model \u201cNCSN++\u201d, following the naming convention of previous SMLD models (Song &\nErmon, 2019; 2020).\nWe followed a similar procedure to examine these architecture components for VP SDEs, except that\nwe skipped experiments on CelebA due to limited computing resources. The NCSN++ architecture\nworked decently well for VP SDEs, ranked 4th place over all 144 possible con\ufb01gurations. The top con-\n\ufb01guration, however, has a slightly different structure, which uses no FIR upsampling/downsampling\nand no progressive growing architecture compared to NCSN++. We name this model \u201cDDPM++\u201d,\nfollowing the naming convention of Ho et al. (2020).\nThe basic NCSN++ model with 4 residual blocks per resolution achieves an FID of 2.45 on CIFAR-10,\nwhereas the basic DDPM++ model achieves an FID of 2.78. Here in order to match the convention\nused in Karras et al. (2018); Song & Ermon (2019) and Ho et al. (2020), we report the lowest FID\nvalue over the course of training, rather than the average FID value over checkpoints after 0.5M\niterations (used for comparing different models of VE SDEs) or between 0.25M and 0.5M iterations\n(used for comparing VP SDE models) in our architecture exploration.\nSwitching from discrete training objectives to continuous ones in Eq. (7) further improves the FID\nvalues for all SDEs. To condition the NCSN++ model on continuous time variables, we change\npositional embeddings, the layers in Ho et al. (2020) for conditioning on discrete time steps, to\nrandom Fourier feature embeddings (Tancik et al., 2020). The scale parameter of these random\nFourier feature embeddings is \ufb01xed to 16. We also reduce the number of training iterations to 0.95M\nto suppress over\ufb01tting. These changes improve the FID on CIFAR-10 from 2.45 to 2.38 for NCSN++\ntrained with the VE SDE, resulting in a model called \u201cNCSN++ cont.\u201d. In addition, we can further\nimprove the FID from 2.38 to 2.20 by doubling the number of residual blocks per resolution for\nNCSN++ cont., resulting in the model denoted as \u201cNCSN++ cont. (deep)\u201d. All quantitative results\nare summarized in Table 3, and we provide random samples from our best model in Fig. 11.\nSimilarly, we can also condition the DDPM++ model on continuous time steps, resulting in a model\n\u201cDDPM++ cont.\u201d. When trained with the VP SDE, it improves the FID of 2.78 from DDPM++ to\n2.55. When trained with the sub-VP SDE, it achieves an FID of 2.61. To get better performance,\nwe used the Euler-Maruyama solver as the predictor for continuously-trained models, instead of the\nancestral sampling predictor or the reverse diffusion predictor. This is because the discretization\nstrategy of the original DDPM method does not match the variance of the continuous process well\nwhen t \u00d1 0, which signi\ufb01cantly hurts FID scores. As shown in Table 2, the likelihood values are\n3.21 and 3.05 bits/dim for VP and sub-VP SDEs respectively. Doubling the depth, and trainin with\n26\nPublished as a conference paper at ICLR 2021\nFigure 11: Unconditional CIFAR-10 samples from NCSN++ cont. (deep, VE).\n27\nPublished as a conference paper at ICLR 2021\nFigure 12: Samples on 1024 \u02c6 1024 CelebA-HQ from a modi\ufb01ed NCSN++ model trained with the\nVE SDE.\n28\nPublished as a conference paper at ICLR 2021\n0.95M iterations, we can improve both FID and bits/dim for both VP and sub-VP SDEs, leading to a\nmodel \u201cDDPM++ cont. (deep)\u201d. Its FID score is 2.41, same for both VP and sub-VP SDEs. When\ntrained with the sub-VP SDE, it can achieve a likelihood of 2.99 bits/dim. Here all likelihood values\nare reported for the last checkpoint during training.\nH.3\nHIGH RESOLUTION IMAGES\nEncouraged by the success of NCSN++ on CIFAR-10, we proceed to test it on 1024 \u02c6 1024 CelebA-\nHQ (Karras et al., 2018), a task that was previously only achievable by some GAN models and\nVQ-VAE-2 (Razavi et al., 2019). We used a batch size of 8, increased the EMA rate to 0.9999, and\ntrained a model similar to NCSN++ with the continuous objective (Eq. (7)) for around 2.4M iterations\n(please \ufb01nd the detailed architecture in our code release.) We use the PC sampler discretized at 2000\nsteps with the reverse diffusion predictor, one Langevin step per predictor update and a signal-to-noise\nratio of 0.15. The scale parameter for the random Fourier feature embeddings is \ufb01xed to 16. We use\nthe \u201cinput skip\u201d progressive architecture for the input, and \u201coutput skip\u201d progressive architecture for\nthe output. We provide samples in Fig. 12. Although these samples are not perfect (e.g., there are\nvisible \ufb02aws on facial symmetry), we believe these results are encouraging and can demonstrate the\nscalability of our approach. Future work on more effective architectures are likely to signi\ufb01cantly\nadvance the performance of score-based generative models on this task.\nI\nCONTROLLABLE GENERATION\nConsider a forward SDE with the following general form\ndx \u201c fpx, tqdt ` Gpx, tqdw,\nand suppose the initial state distribution is p0pxp0q | yq. The density at time t is ptpxptq | yq when\nconditioned on y. Therefore, using Anderson (1982), the reverse-time SDE is given by\ndx \u201c tfpx, tq \u00b4 \u2207\u00a8 rGpx, tqGpx, tqTs \u00b4 Gpx, tqGpx, tqT\u2207x log ptpx | yqudt ` Gpx, tqd \u00afw. (48)\nSince ptpxptq | yq9ptpxptqqppy | xptqq, the score \u2207x log ptpxptq | yq can be computed easily by\n\u2207x log ptpxptq | yq \u201c \u2207x log ptpxptqq ` \u2207x log ppy | xptqq.\n(49)\nThis subsumes the conditional reverse-time SDE in Eq. (14) as a special case. All sampling methods\nwe have discussed so far can be applied to the conditional reverse-time SDE for sample generation.\nI.1\nCLASS-CONDITIONAL SAMPLING\nWhen y represents class labels, we can train a time-dependent classi\ufb01er ptpy | xptqq for class-\nconditional sampling. Since the forward SDE is tractable, we can easily create a pair of training\ndata pxptq, yq by \ufb01rst sampling pxp0q, yq from a dataset and then obtaining xptq \u201e p0tpxptq | xp0qq.\nAfterwards, we may employ a mixture of cross-entropy losses over different time steps, like Eq. (7),\nto train the time-dependent classi\ufb01er ptpy | xptqq.\nTo\ntest\nthis\nidea,\nwe\ntrained\na\nWide\nResNet\n(Zagoruyko\n&\nKomodakis,\n2016)\n(Wide-ResNet-28-10) on CIFAR-10 with VE perturbations.\nThe classi\ufb01er is condi-\ntioned on log \u03c3i using random Fourier features (Tancik et al., 2020), and the training objective is\na simple sum of cross-entropy losses sampled at different scales. We provide a plot to show the\naccuracy of this classi\ufb01er over noise scales in Fig. 13. The score-based model is an unconditional\nNCSN++ (4 blocks/resolution) in Table 3, and we generate samples using the PC algorithm with\n2000 discretization steps. The class-conditional samples are provided in Fig. 4, and an extended set\nof conditional samples is given in Fig. 13.\nI.2\nIMPUTATION\nImputation is a special case of conditional sampling. Denote by \u2126pxq and \u00af\u2126pxq the known and un-\nknown dimensions of x respectively, and let f\u00af\u2126p\u00a8, tq and G\u00af\u2126p\u00a8, tq denote fp\u00a8, tq and Gp\u00a8, tq restricted\nto the unknown dimensions. For VE/VP SDEs, the drift coef\ufb01cient fp\u00a8, tq is element-wise, and the\ndiffusion coef\ufb01cient Gp\u00a8, tq is diagonal. When fp\u00a8, tq is element-wise, f\u00af\u2126p\u00a8, tq denotes the same\n29\nPublished as a conference paper at ICLR 2021\n10\n2\n10\n1\n100\n101\n0.2\n0.4\n0.6\n0.8\nAccuracy\nAccuracy vs. noise scale\nFigure 13: Class-conditional image generation by solving the conditional reverse-time SDE with PC.\nThe curve shows the accuracy of our noise-conditional classi\ufb01er over different noise scales.\n30\nPublished as a conference paper at ICLR 2021\nelement-wise function applied only to the unknown dimensions. When Gp\u00a8, tq is diagonal, G\u00af\u2126p\u00a8, tq\ndenotes the sub-matrix restricted to unknown dimensions.\nFor imputation, our goal is to sample from pp\u00af\u2126pxp0qq | \u2126pxp0qq \u201c yq. De\ufb01ne a new diffusion\nprocess zptq \u201c \u00af\u2126pxptqq, and note that the SDE for zptq can be written as\ndz \u201c f\u00af\u2126pz, tqdt ` G\u00af\u2126pz, tqdw.\nThe reverse-time SDE, conditioned on \u2126pxp0qq \u201c y, is given by\ndz \u201c\n\u2423\nf\u00af\u2126pz, tq \u00b4 \u2207\u00a8 rG\u00af\u2126pz, tqG\u00af\u2126pz, tqTs\n\u00b4 G\u00af\u2126pz, tqG\u00af\u2126pz, tqT\u2207z log ptpz | \u2126pzp0qq \u201c yq\n(\ndt ` G\u00af\u2126pz, tqd \u00afw.\nAlthough ptpzptq | \u2126pxp0qq \u201c yq is in general intractable, it can be approximated. Let A denote the\nevent \u2126pxp0qq \u201c y. We have\nptpzptq | \u2126pxp0qq \u201c yq \u201c ptpzptq | Aq \u201c\n\u017c\nptpzptq | \u2126pxptqq, Aqptp\u2126pxptqq | Aqd\u2126pxptqq\n\u201c Eptp\u2126pxptqq|Aqrptpzptq | \u2126pxptqq, Aqs\n\u00ab Eptp\u2126pxptqq|Aqrptpzptq | \u2126pxptqqqs\n\u00ab ptpzptq | \u02c6\u2126pxptqqq,\nwhere \u02c6\u2126pxptqq is a random sample from ptp\u2126pxptqq | Aq, which is typically a tractable distribution.\nTherefore,\n\u2207z log ptpzptq | \u2126pxp0qq \u201c yq \u00ab \u2207z log ptpzptq | \u02c6\u2126pxptqqq\n\u201c \u2207z log ptprzptq; \u02c6\u2126pxptqqsq,\nwhere rzptq; \u02c6\u2126pxptqqs denotes a vector uptq such that \u2126puptqq \u201c\n\u02c6\u2126pxptqq and \u00af\u2126puptqq \u201c\nzptq, and the identity holds because \u2207z log ptprzptq; \u02c6\u2126pxptqqsq \u201c \u2207z log ptpzptq | \u02c6\u2126pxptqqq `\n\u2207z log pp\u02c6\u2126pxptqqq \u201c \u2207z log ptpzptq | \u02c6\u2126pxptqqq.\nWe provided an extended set of inpainting results in Figs. 14 and 15.\nI.3\nCOLORIZATION\nColorization is a special case of imputation, except that the known data dimensions are coupled.\nWe can decouple these data dimensions by using an orthogonal linear transformation to map the\ngray-scale image to a separate channel in a different space, and then perform imputation to complete\nthe other channels before transforming everything back to the original image space. The orthogonal\nmatrix we used to decouple color channels is\n\u02dc0.577\n\u00b40.816\n0\n0.577\n0.408\n0.707\n0.577\n0.408\n\u00b40.707\n\u00b8\n.\nBecause the transformations are all orthogonal matrices, the standard Wiener process wptq will still\nbe a standard Wiener process in the transformed space, allowing us to build an SDE and use the same\nimputation method in Appendix I.2. We provide an extended set of colorization results in Figs. 16\nand 17.\nI.4\nSOLVING GENERAL INVERSE PROBLEMS\nSuppose we have two random variables x and y, and we know the forward process of generating y\nfrom x, given by ppy | xq. The inverse problem is to obtain x from y, that is, generating samples\nfrom ppx | yq. In principle, we can estimate the prior distribution ppxq and obtain ppx | yq using\nBayes\u2019 rule: ppx | yq \u201c ppxqppy | xq{ppyq. In practice, however, both estimating the prior and\nperforming Bayesian inference are non-trivial.\nLeveraging Eq. (48), score-based generative models provide one way to solve the inverse problem.\nSuppose we have a diffusion process txptquT\nt\u201c0 generated by perturbing x with an SDE, and a\n31\nPublished as a conference paper at ICLR 2021\ntime-dependent score-based model s\u03b8\u02dapxptq, tq trained to approximate \u2207x log ptpxptqq. Once we\nhave an estimate of \u2207x log ptpxptq | yq, we can simulate the reverse-time SDE in Eq. (48) to sample\nfrom p0pxp0q | yq \u201c ppx | yq. To obtain this estimate, we \ufb01rst observe that\n\u2207x log ptpxptq | yq \u201c \u2207x log\n\u017c\nptpxptq | yptq, yqppyptq | yqdyptq,\nwhere yptq is de\ufb01ned via xptq and the forward process ppyptq | xptqq. Now assume two conditions:\n\u2022 ppyptq | yq is tractable. We can often derive this distribution from the interaction between\nthe forward process and the SDE, like in the case of image imputation and colorization.\n\u2022 ptpxptq | yptq, yq \u00ab ptpxptq | yptqq. For small t, yptq is almost the same as y so the\napproximation holds. For large t, y becomes further away from xptq in the Markov chain,\nand thus have smaller impact on xptq. Moreover, the approximation error for large t matter\nless for the \ufb01nal sample, since it is used early in the sampling process.\nGiven these two assumptions, we have\n\u2207x log ptpxptq | yq \u00ab \u2207x log\n\u017c\nptpxptq | yptqqppyptq | yqdy\n\u00ab \u2207x log ptpxptq | \u02c6yptqq\n\u201c \u2207x log ptpxptqq ` \u2207x log ptp\u02c6yptq | xptqq\n\u00ab s\u03b8\u02dapxptq, tq ` \u2207x log ptp\u02c6yptq | xptqq,\n(50)\nwhere \u02c6yptq is a sample from ppyptq | yq. Now we can plug Eq. (50) into Eq. (48) and solve the\nresulting reverse-time SDE to generate samples from ppx | yq.\n32\nPublished as a conference paper at ICLR 2021\nFigure 14: Extended inpainting results for 256 \u02c6 256 bedroom images.\n33\nPublished as a conference paper at ICLR 2021\nFigure 15: Extended inpainting results for 256 \u02c6 256 church images.\n34\nPublished as a conference paper at ICLR 2021\nFigure 16: Extended colorization results for 256 \u02c6 256 bedroom images.\n35\nPublished as a conference paper at ICLR 2021\nFigure 17: Extended colorization results for 256 \u02c6 256 church images.\n36\n",
    "2310.07771": "DrivingDiffusion: Layout-Guided multi-view driving scene video generation with\nlatent diffusion model\nXiaofan Li, Yifu Zhang, Xiaoqing Ye\nBaidu Inc.\nProject page: https://drivingdiffusion.github.io\n!\nManmade Scenario\n\u00b7 Multi-Agent Trajectories \n\u00b7 Road Structure\nTemporal Multi-View Layouts \nFRONT_LEFT             FRONT            FRONT_RIGHT     BACK_RIGHT           BACK      \nBACK_LEFT \nFRONT_LEFT                      FRONT                      FRONT_RIGHT              BACK_RIGHT                     BACK                        BACK_LEFT \nFrame 1\nFrame 2\nFrame 3\nFrame 4\nFrame 5\nFrame 6\nFrame 1\nFrame 6\n!\nDrivingDiffusion \nMulti-View Video\nRender\nFigure 1. Examples of the generated multi-view video frames from a man-made 3D layout (including the 3D bounding boxes of obstacles\nand the road structure) by DrivingDiffusion. The 3D layout is projected to the six camera views using camera parameters. We present the\ngenerated results of six camera views and six consecutive frames.\n1\narXiv:2310.07771v1  [cs.CV]  11 Oct 2023\nAbstract\nWith the increasing popularity of autonomous driving\nbased on the powerful and unified bird\u2019s-eye-view (BEV)\nrepresentation, a demand for high-quality and large-scale\nmulti-view video data with accurate annotation is urgently\nrequired. However, such large-scale multi-view data is hard\nto obtain due to expensive collection and annotation costs.\nTo alleviate the problem, we propose a spatial-temporal\nconsistent diffusion framework DrivingDiffusion, to gen-\nerate realistic multi-view videos controlled by 3D layout.\nThere are three challenges when synthesizing multi-view\nvideos given a 3D layout: How to keep 1) cross-view con-\nsistency and 2) cross-frame consistency? 3) How to guar-\nantee the quality of the generated instances? Our Driv-\ningDiffusion solves the problem by cascading the multi-\nview single-frame image generation step, the single-view\nvideo generation step shared by multiple cameras, and post-\nprocessing that can handle long video generation. In the\nmulti-view model, the consistency of multi-view images is\nensured by information exchange between adjacent cam-\neras. In the temporal model, we mainly query the infor-\nmation that needs attention in subsequent frame generation\nfrom the multi-view images of the first frame. We also in-\ntroduce the local prompt to effectively improve the qual-\nity of generated instances. In post-processing, we further\nenhance the cross-view consistency of subsequent frames\nand extend the video length by employing temporal sliding\nwindow algorithm. Without any extra cost, our model can\ngenerate large-scale realistic multi-camera driving videos\nin complex urban scenes, fueling the downstream driving\ntasks. The code will be made publicly available.\n1. Introduction\nAutonomous driving has drawn extensive attention both\nfrom industry and academia for decades. The well-known\nbird\u2019s-eye-view (BEV) is a natural and straightforward can-\ndidate view to serve as a unified representation. BEV per-\nception takes multi-view images as input and learns a uni-\nfied BEV representation for downstream tasks such as 3D\nobject detection [14,17,38], segmentation [4,10,22], track-\ning [11, 42, 43] or trajectory prediction [5, 12, 16]. High-\nquality and large-scale multi-view video data is critical for\nlearning a powerful BEV representation for multiple per-\nception tasks. However, high-quality data is often difficult\nto obtain due to the unaffordable labeling costs. As a result,\nthere is a large demand for generating multi-view temporal-\nconsistent video data that conforms to the real distribution\nand guarantees high consistency with 3D labeling and im-\nages.\nWe tackle the new task of multi-view video data gener-\nation in complex urban scenes. The generated data is con-\ntrolled by a 3D layout around the ego-car. Figure 1 shows\nour generated results from 3D layout to multi-view con-\nsecutive frames. Previous or concurrent works tackle simi-\nlar tasks such as layout-to-image generation [2,44], single-\nview driving video generation [1]. The most relevant task\nto ours is generating multi-view urban scene images from\na BEV layout, i.e. BEV segmentation mask, proposed in\nBEVGen [31]. Compared with these data generation tasks,\nour task is more complex (i.e. multi-view vs. single-view,\nmulti-frame vs.\nsingle-frame, complex urban scenes vs.\nsimple road scenes), more controllable (i.e. with 3D lay-\nout), and has wider applications. For example, it can serve\nas a kind of data augmentation for BEV detection models.\nBy controlling the 3D layout, we can obtain a large amount\nmulti-view videos for model training. Another application\nis editing specific objects in the video to simulate some rare\nscenarios such as collision. This helps the simulator to im-\nprove the safety of the driving system.\nThere are three main challenges for the task of multi-\nview video data generation. First, we should maintain the\ncross-view consistency of the generated multi-view images\nof different cameras. For instance, the multi-view images\nin the same frame should appear like they were taken in the\nsame physical location. Second, the cross-frame (or tem-\nporal) consistency should be maintained to produce a visu-\nally coherent video. The video frames should be temporally\naligned. Third, it is significant to the guarantee quality of\nthe generated instances (i.e. vehicles and pedestrians) in the\nvideo. The instances are direct subjects of the perception\ntasks.\nWe propose a multi-stage scheme DrivingDiffusion to\ngenerate multi-view videos controlled by artificial 3D lay-\nout in complex urban scenes. It is based on the widely used\nimage synthesis diffusion model. The 3D layout is utilized\nas additional control information for the diffusion model.\nDrivingDiffusion contains a multi-view single-frame image\ngeneration model, a single-view temporal model shared by\nmultiple cameras, and post-processing that includes a multi-\nview model specifically designed to enhance the consis-\ntency of subsequent generated frames and a sliding win-\ndow processing that extends the video. In the multi-view\nmodel, we propose a cross-view attention module to ensure\nthe consistency of multi-view images. An additional con-\nsistency loss is achieved by adding geometric constraints\nbetween different camera views. We also introduce local\nprompt to guide the relationship between the whole im-\nage and local instances, therefore effectively improving the\nquality of instance generation. In the single-view tempo-\nral model, we propose a Key-Frame controller that utilizes\nthe first frame of the multi-view video sequence as a con-\ntrol condition to ensure cross-frame consistency. To avoid\nthe need to train a separate video generation model for each\ncamera, we embed statistics-based optical flow prior to each\n2\n\ud835\udf00\n\ud835\udc65\n\ud835\udc67!\n\ud835\udc67\"\n\u00d7T \ud835\udc47\ud835\udc56\ud835\udc5a\ud835\udc52\ud835\udc60\nReconstruction Loss\nMulti-View Single Frame\nSingle-View Multi Frames\n\"A pedestrian is crossing the road. \"\n3D Layout\nController\nKey-Frame\nController\n\"car\"\nCLIP\nMasked Cross\nAttn\nPseudo\n3D Conv\nView\nTime\nCross Attn\nConsistency Loss\nMulti-View Keyframes\n\u2026\n\ud835\udc5e\ud835\udc9b!:# \ud835\udc9b$)\nLayouts\nClass Mask\nTraining pipeline\na)\nb)\nc)\nd)\nCLIP\nController\nOptical Flow Prior\nEnc\nLocal Prompt\nGlobal Prompt\nCross-View\nAttn\nTemporal Attn\nOnly Temporal Model\nOnly Multi-View Model\nFuse\n\u2026\ncopy\nFigure 2. Diagram of the multi-view video generation framework DrivingDiffusion . For training, we separately train the multi-view model\nand the temporal model. These two models share similar structures, with the exception of the orange and purple components. During the\ninference stage, the two models are concatenated in a cascaded manner. First, the multi-view model generates the initial multi-view frame\nof the video. This frame is then set as the keyframe for the temporal model. Lastly, the temporal model generates video frames for each\nview, forming the final multi-view video.\ncamera in the network. In post-processing, the \u201dfine-tune\u201d\nmulti-view model we trained takes a subsequent generated\nframe and the first frame as conditions to further enhance\nthe subsequent multi-view consistency. Finally, we use new\nkeyframes in the sliding window to extend the video length.\nIn summary, our key contributions are as follows:\n(1) We address the new problem of multi-view video data\ngeneration from 3D layout in complex urban scenes.\n(2) We propose a generative model DrivingDiffusion to\nensure the cross-view, cross-frame consistency and the in-\nstance quality of the generated videos.\n(3) We achieve state-of-the-art video synthesis perfor-\nmance on nuScenes dataset.\n2. Related Work\n2.1. Text-to-Image Generation\nThe text-to-image (T2I) generation task aims at generat-\ning realistic images based on text inputs. Early works solve\nthe problem by turning it into a sequence-to-sequence prob-\nlem. For example, DALL-E [25] translates text tokens to\ndiscrete image embeddings obtained by VQ-VAE [34]. The\nfollowing works improve the quality of the generated im-\nages by more advanced architectures such as image tokeniz-\ners [40], encoder-decoder architectures [39] or hierarchical\ntransformers [3]. Recently, Denoising Diffusion Probabilis-\ntic Models (DDPM) [7] begin to conquer the T2I task. The\nquality of the generated image is further increased by uti-\nlizing the excellent ability to generate realistic images of\ndiffusion models [19,27] or improving the text-image align-\nments [24] using powerful text encoders [23].\n2.2. Text-to-Video Generation\nThe text-to-video (T2V) generation task generates video\nfrom the text descriptions Compared with the T2I task, the\ntext-to-video (T2V) generation task is more challenging due\nto the temporally consistent characteristics involved. Some\nmethods extend T2I methods to perform T2V by changing\nimage tokens to video tokens [36] or equipping the tem-\nporal attention modules to T2I models [9]. The following\nwork [37] proposes an auto-regressive framework to solve\nboth T2I and T2V tasks. More recently, Video Diffusion\nModels [8] utilizes a space-time factorized U-Net with joint\nimage and video data training. Make-A-Video [29] directly\ntranslates the tremendous recent progress in T2I to T2V by\nintroducing an effective spatial-temporal module on top of\nT2I models. Similar to Make-A-Video, we also adopt pre-\ntrained T2I models to generate multi-view videos.\n2.3. Layout-to-Image generation\nThis task can be seen as the reverse process of object\ndetection or segmentation, which generates images with\nthe input of bounding boxes or segmentation maps. Pre-\nvious works utilize GANs [6, 15, 20, 32] or diffusion mod-\n3\nels [2,44] to generate synthetic images based on 2D layout.\nThese methods encode the layout as a condition image that\nis downsampled and upsampled jointly with the data. Re-\ncently, BEVGen [31] generates multi-view urban scene im-\nages from BEV layout based on VQ-VAE [34]. Compared\nwith BEVGen, we tackle a more complex task that gener-\nates temporally consistent multi-view videos. We also adopt\ndiffusion models [8] to generate more realistic videos.\n3. Method\nIn this section, we begin with a brief description of\nthe denoising diffusion probabilistic model (DDPM) and\nthe latent diffusion model (LDM) on which our method\nis based, and this can be found in Section 3.1.\nSubse-\nquently, we introduced DrivingDiffusion \u2019s pipeline: the\ncross-view model, temporal model and post-processing, in-\ncluding training and inference strategy, which is discussed\nin Section 3.2. We also present our methods for enhancing\ncross-view and cross-frame consistency in the video as well\nas our local prompt design for improving instance genera-\ntion quality in Section 3.3 and Section 3.4, respectively. An\noverview of our approach is shown in Figure 2.\n3.1. Preliminary: Latent Diffusion Models\nLatent diffusion models (LDM) [26] are a type of diffu-\nsion model that models the distribution of the latent space\nof images and have recently shown remarkable performance\nin image synthesis. The LDM consists of two models: an\nautoencoder and a diffusion model.\nThe autoencoder learns to compress and reconstruct im-\nages using an encoder E and a decoder D.\nThe encoder first projects the image x to a lower-\ndimensional latent space z, and the decoder then recon-\nstructs the original image from the latent space, resulting\nin \u02dcx = D(z).\nThen the latent generative model is trained to recreate a\nfixed forward Markov chain x1, . . . , xT via DDPMs [7].\nGiven the data distribution z0 \u223cq(z0), the Markov tran-\nsition q(zt|zt\u22121) is defined as a Gaussian distribution with\na variance schedule \u03b2t \u2208(0, 1), which can be formulaic as,\n  q(z_t|z_ { t-1})\n \n=  \\mathca l {N}\n( z _t ;  \\ s qrt {1-\\beta _t} z_{t-1}, \\beta _t \\mathbb {I}), \\quad t = 1, \\ldots , T. \n(1)\nAccording to the Bayes\u2019 rule and the Markov property,\nwe can derive explicit expressions for the conditional prob-\nabilities q(zt|z0) and q(zt\u22121|zt, z0) as follows.\n  q(z_t| z _0) & = \\math ca l  {N}(z_\nt ;  \\ s q r t  {\n\\Ba\nr {\\alpha }_t }  z_0, ( 1 - \\Ba r {\\ alpha \n} _ t)  \\ m a th\nbb \n{I}), \n\\q ua d  t = 1 ,\n \\\nldo\nts , T ,  \\ \\ q(z\n_ { t-1\n}|z\n_t,\n z_0) & = \\ m\nathcal\n { N}(\nz_ {\nt-1};  \\Tilde\n { \\mu\n }_t(z_t, z_0), \\Tilde {\\beta }_t \\mathbb {I}), \\quad t = 1, \\ldots , T, \\\\ w.r.t.~~~~ \\alpha _t = & 1 - \\beta _t, ~ \\Bar {\\alpha }_t = \\prod _{s=1}^t \\alpha _s, ~ \\Tilde {\\beta }_t = \\frac {1 - \\Bar {\\alpha }_{t-1}}{1 - \\Bar {\\alpha }_t} \\beta _t, \\\\ \\Tilde {\\mu }_t(z_t, z_0) & = \\frac {\\sqrt {\\Bar {\\alpha }_t}\\beta _t}{1 - \\Bar {\\alpha }_t} z_0 + \\frac {\\sqrt {{\\alpha }_t} (1 - \\Bar {\\alpha }_{t-1})}{1 - \\Bar {\\alpha }_t} z_t.\n(5)\nDDPMs leverage the reverse process with a prior dis-\ntribution p(zT ) = N(zT ; 0, I) and Gaussian transitions to\ngenerate the Markov chain z1, . . . , zT ,\n  p_\\theta ( z_{t-1} |z_t) = \\ mathca l {N\n} ( z_ { t - 1 }; \\mu _\\theta (z_t, t), \\Sigma _\\theta (z_t, t)), \\quad t = T, \\ldots , 1. \n(6)\nHere \u03b8 is the learnable parameter we use to ensure that\nthe generated reverse process is close to the forward pro-\ncess.\nDDPMs follow the variational inference principle by\nmaximizing the variational lower bound of the negative\nlog-likelihood, which has a closed form given the KL di-\nvergence among Gaussian distributions. Empirically, these\nmodels can be interpreted as a sequence of weight-sharing\ndenoising autoencoders \u03f5\u03b8(zt, t), which are trained to pre-\ndict a denoised variant of their input zt. The objective can\nbe simplified as follows.\n  \\mat hbb {E}\n_\n{z ,  \\epsi lon \n\\\ns\nim \\mathcal {N}(0, 1), t} \\left [ \\| \\epsilon - \\epsilon _\\theta (z_t, t) \\|^2_2 \\right ]. \n(7)\n3.2. DrivingDiffusion\nOverview.\nThe prevalent diffusion model for image syn-\nthesis typically utilizes a U-Net architecture for denois-\ning, which entails two rounds of spatial downsampling fol-\nlowed by upsampling. It comprises numerous layers of 2D\nconvolution residuals and attention blocks, containing self-\nattention, cross-attention, and a feedforward network. Spa-\ntial self-attention evaluates the interdependence between\npixels within the feature map, whereas cross-attention con-\nsiders the alignment between pixels and conditional inputs,\nsuch as text. When generating videos, it is customary to\nutilize an elongated 1 \u00d7 3 \u00d7 3 convolution kernel for the\nvideo input to retain the parameters of the single-frame pre-\ntrained diffusion model and expand them to novel dimen-\nsions. In our multi-view and temporal model, we introduce\nadditional dimensions for view and time separately. Similar\nto VDM [8], we expand the 2D convolutional layer into a\npseudo-3D convolutional layer with a 1\u00d73\u00d73 kernel and in-\ntegrate a time self-focusing layer in each transformer block\n4\nto facilitate inter-dimensional information exchange. Fur-\nthermore, we have incorporated several modules, such as\nthe 3D layout controller, key-frame controller, cross-view /\ncross-frame consistency module, and a local prompt guid-\nance module, to enhance instance generation quality. Figure\n2 presents an overview of our proposed DrivingDiffusion.\nOur experimental findings indicate that this technique can\ngenerate multi-view videos that are accurately aligned with\n3D layout while exhibiting data quality comparable to real-\nworld data.\n3D Layout Controller.\nIn order to ensure an accurate\nmapping between the target instance position in the phys-\nical space around the ego-car and the generated image, we\nproject the road structure and each instance onto the 3D lay-\nout image using the parameters of the camera. It is impor-\ntant to mention that we incorporate the road-structure in-\nformation, the target category, and the target instance ID\ncorresponding to each pixel, and encode this information\ninto RGB values. Next, the aforementioned input is fed\ninto a ResNet-like 3D layout controller to encode the U-Net\nmodel at different resolutions (i.e. 64 \u00d7 64, 32 \u00d7 32, 16 \u00d7\n16, 8 \u00d7 8) corresponding to different levels of features. We\ninject this additional control information at different reso-\nlutions into each layer of the U-Net model through residual\nconnections, as is shown in a) Figure 2. The advantage of\nthis type of control mechanism is that it is able to influence\nthe diffusion model from various receptive fields without\nintroducing too many parameters (i.e., a trainable copy of\nControlNet [41]).\nMulti-View Model.\nFor the multi-view single-frame\nmodel, we employed the 3D layout of all views as input,\nalong with textual descriptions of the scene, to generate\nhighly aligned, multi-view images. At this stage, we only\nutilized one 3D layout controller.\nTemporal Model.\nAs for the temporal single-perspective\nmodel, we input the 3D layout of all video frames from a\nsingle perspective, the first frame image of the video se-\nquence, and the optical flow prior corresponding to the cam-\nera. The output is a temporally consistent video from a\nsingle viewpoint. Specifically, the added dimension in our\nmodel changes from multiple views to multiple frames. Ad-\nditionally, we include a Key-Frame controller that utilizes\nthe first frame of the multi-view video sequence as a con-\ntrol condition for generating consistent videos. To avoid\nthe need to train a separate video generation model for each\ncamera, statistical optical flow trend at each camera view,\nwith a fixed value as a prior and encoded as a token, is input\ninto the model in the same way as time embedding.\n!\n1) Multi-View KeyFrame Inference\n!!\n2) View-Parallel Temporal Inference\n4) Sliding Windows\n\ud835\udc47!\n\ud835\udc47\"\n\ud835\udc47#\n!\nOther Cameras\n!!\nNew Key Frames\n3) Temporal-Parallel Multi-ViewInference\n!\n\ud835\udc37\ud835\udc37\ud835\udc3c\ud835\udc40\n\ud835\udc67!\n\ud835\udca9~\n\ud835\udc9f\n\ud835\udc67\"\nLayouts\nInference pipeline & Postprocess\nKey Frames\nMulti-View Images\ngenerated by \nTemporal model\nGlobal\nPrompt\nLocal\nPrompt\nCamera Emb\nFigure 3. Multi-view Long video generation pipeline.\nPost-Processing.\nIn our temporal model, we take the\nmulti-view images at the initial moment as the keyframe.\nThis is because, in a short period of time, we assume that\nmost of the information needed for subsequent frames of\neach view can be found in the multi-view images at the ini-\ntial moment. However, when the time interval is too long,\nthere is still a probability that multiple views of the subse-\nquent frames of the video may be inconsistent due to fac-\ntors such as occlusion. To handle long sequence videos,\nwe train a finetune model after the cross-view and temporal\nmodels to enhance the consistency of the following frames.\nConcretely, the finetune model utilizes the existing structure\nand parameters of the cross-view model, with the exception\nof an extra controller for inputting frames generated by the\ntemporal model. In the training, the finetune model uses a\nlarge number of temporal subsequent frames generated by\nthe temporal model through the original multi-view images\nin the dataset as inputs, and the corresponding original im-\nages in the dataset as truth values. During inference, the\nmodel inputs the multi-view images of subsequent frames\nof the generated video and outputs the images after fine-\ntuning. Since it is only engineering work to improve the\ngeneration quality, we see it as a part of post-processing for\nlong video generation tasks.\nFinally, we take a certain moment of subsequent multi-\nview images as new keyframes and conduct temporal infer-\nence again. The video length is extended with the idea of\nsliding window.\nMulti-stage pipeline for long video.\nWe introduce a\nmulti-stage inference strategy to generate multi-view long\nvideos, as shown in Figure 3: 1) We first adopt the multi-\n5\nConsistency Module\nLocal Prompt\n\"car\"\nCLIP\n\ud835\udc11\ud835\udc1e\ud835\udc20\ud835\udc28,\ud835\udc13\ud835\udc1e\ud835\udc20\ud835\udc28\n\ud835\udc11\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30, \ud835\udc13\ud835\udc2f\ud835\udc22\ud835\udc1e\ud835\udc30\nSoftmax & MatMul\nMatMul\nV\nK\nQ\n\" A sunny day, a car \non the street\"\nCLIP\nMatMul\nV\nK\nQ\nSoftmax & MatMul\nConsistency\nAttention\nshare\nshare\nGlobal Prompt\nt = 0\nt = n\nFRONT\nFRONT RIGHT\nFRONTRIGHT\nFigure 4. A schematic of the Consistency module(the right part)\nand local prompt(the left part) methods.\nview model to generate the first frame panoramic image\nof the video sequence. 2) Then we use the generated im-\nage from each perspective as input for the temporal model,\nallowing for parallel sequence generation for each corre-\nsponding viewpoint. 3) For subsequent frames, we employ\nthe finetune model for parallel inference as well. 4) Extend\nthe video after identifying new keyframes, just like the slid-\ning window algorithm does. Finally we obtain the entire\nsynthetic multi-view video.\n3.3. Cross-View and Cross-Frame Consistency\nAs described in section 3.2, the input of 3D layout pro-\nvides the model with the location, category, and instance in-\nformation of each target, to some extent, aiding in enhanc-\ning local multi-view and temporal consistency. However,\nthis alone is insufficient to ensure global consistency, par-\nticularly in static scenes. To address this, we propose an\nefficient attention mechanism that emphasizes cross-view\nand cross-frame interactions and introduces geometric con-\nstraints to supervise generated results.\nThe right part of Figure 4 shows the range in which\nthe Consistency Module is in effect: (1) adjacent views\nin multi-view, (2) the current frame and the first/previous\nframes in multi-frame.\nConsistency Attention.\nFor the cross-view model, we\nposit that each perspective should primarily focus on its ad-\njacent left and right perspectives. The latent representation\nof a view and its left and right neighbors in the multi-view\nmodel v are denoted as zi\nv, zi\u22121\nv\n, and zi+1\nv\n.\nWe attach a consistency attention layer to each atten-\ntion block to model the new dimensions. For the multi-\nview model, in order to enhance the cross-view consistency,\ncross-view attention was calculated using the latent expres-\nsion of the i th view zi\nv and the sum of the latent expression\nof the two adjacent views zi\u22121\nv\nand zi+1\nv\n. Our cross-view\nconsistency attention is formulated as:\n  \\begin {aligned} & At ten tio n _{cross-\nv\ni ew}(\nQ\n_ v\n, K_ v\n,V_v\n)\n = So\nf\ntma\nx\n(\n \\\n\\\n &\\ f\nrac \n{\n\\ left\n \n(W^{Q_v} z_v^i \\right ) \\left ( W^{K_v} \\left [ z_{v}^{i-1}, z_{v}^{i+1} \\right ] \\right )^T} {\\sqrt {d}}) \\cdot \\left (W^{V_v} \\left [ z_{v}^{i-1}, z_{v}^{i+1} \\right ]\\right ) \\end {aligned} (8)\nwhere [, ] denotes concatenation operation. W Qv, W Kv,\nand W Vv are learnable matrices in the cross-view model\nthat project the inputs to query, key and value, respectively,\nand d is the output dimension of key and query features.\nFor the temporal model, the jth frame in the temporal\nmodel t such that j > 1 is denoted as zj\nt . Cross-frame at-\ntention was calculated using the sum of the latent represen-\ntation of the jth frame and the sum of the first frame with the\n(j-1)th frame. Similarly, we implement cross-frame consis-\ntency attention as:\n  \\begin {aligned} & Att ent ion _ {cross-f\nr\na me}(\nQ\n_ t\n, K_ t\n,V\n_ t ) = \nS\noft\nm\na\nx (\n \n\\ \\ &\n\\f\nr a c {\\\nl\neft (W^{Q_t} z_t^j \\right ) \\left (W^{K_t} \\left [ z_{t}^{1}, z_{t}^{j-1} \\right ] \\right )^T} {\\sqrt {d}}) \\cdot \\left (W^{V_t} \\left [ z_{t}^{1}, z_{t}^{j-1} \\right ]\\right ) \\end {aligned} \n(9)\nwhere W Qt, W Kt, and W Vt are learnable matrices in\nthe temporal model.\nConsistency Loss.\nWe supervise the consistency of the\ngenerated results using a pre-trained network F that can\nperform image matching and regression relative poses. Dur-\ning training, we fix the parameter of F and use the actual\nrelative pose as the ground truth. For the cross-view model,\nwe supervise the pose between adjacent frames, and for the\ntemporal model, we supervise the adjacent frames in a time\nsequence.\n3.4. Local Prompt\nTo enhance the generation quality of instances, we pro-\npose the local prompt module. We can refer to d) of Figure\n2. First, we store the category k and the text Tk correspond-\ning to the category name in advance, and encode the cate-\ngory embedding Fk = \u03d5(Tk), where \u03d5 is the CLIP [23] en-\ncoder. We utilize the smallest surrounding rectangular area\nof the projected 3D layouts as the mask Mk for each cate-\ngory k. Cross attention is then computed using latent repre-\nsentation computation z and category text encoding Ek in\nthe same way as the global prompt, using Mk as the mask\nfor attention. The left part of Figure 4 illustrates how global\nprompt and local prompts cooperate with each other.\nWe did not employ fixed digit encoding during training,\nas we hypothesized that the concepts of local and global\nprompts were well-aligned, with the only difference be-\ning their respective scopes. Therefore, our local prompt\nreplicates the same structure and parameters as the global\n6\n\u201cDrive at night.\u201d\n\u201cWet streets on rainy days.\u201d\nLayout\nControl\nOnly\nLayout\nControl\n&\nText\nControl\nFigure 5. Street view generation results of the multi-view model. We adopt the 3D layout to generate the images of 6 surrounding cameras\nin 4 different scenarios. Results with and without text control are displayed separately.\nprompt, enabling it to fully leverage the parameters of the\npre-trained diffusion model, which already encompasses an\nunderstanding of our target class.\n4. Experiment\nIn this section, we will evaluate DrivingDiffusion based\non several metrics. Firstly, we will provide a detailed de-\nscription of the experimental setup, dataset, and evaluation\ncriteria. Secondly, we will analyze our method qualitatively\nand compare it quantitatively with other methods . Lastly,\nwe will conduct an ablative analysis, which will be thor-\noughly discussed and analyzed.\n4.1. Experiment Details\nDataset\nWe present the results of generated images from\nall six camera views trained on the nuScenes dataset. We\ncompared the image quality of our method with other multi-\nview single-frame approaches, and the video quality with\nother single-view video methods. To evaluate our work, we\nutilized metrics such as FID and FVD, along with quantita-\ntive demonstrations highlighting the proposed method\u2019s ad-\nvantages for autonomous driving. Furthermore, we qualita-\ntively showcased our work through visualization. In multi-\nview visualizations, we horizontally flipped the back left\nand right cameras to emphasize our model\u2019s consistency\nacross images. Consequently, the side, front, and back cam-\neras align at their outer edges in all figures.\nDetails\nWe implement our approach based on the official\ncode base for stable diffusion [26] and the publicly avail-\nable 1.4 billion parameter T2I model. We freeze the image\nautoencoder and encode each view or video frame individu-\nally as a potential representation. We trained the cross-view\n7\nTIME\nFRONT\nRIGHT\nFRONT\nFRONT\nRIGHT\nFRONT\nFrame 1\nFrame 2\nFrame 3\nVIEW\nFrame 4\nFrame 5\nFrame 6\nFigure 6. Street view generation results of the temporal model. We show the visualization results of 6 consecutive frames of 2 adjacent\nviews.\nmodel on nuScenes for 50,000 steps, the temporal model\nfor 40,000 steps, and the fine-tune model for 10,000 steps.\nSix groups of six cameras with a resolution of 512 \u00d7 512\nwere selected for training. The training process was car-\nried out on eight NVIDIA Tesla 40G-A100 GPUs, and both\nthe cross-view model and cross-frame model could be com-\npleted within 36 hours. In terms of reasoning, we used a\nDDIM sampler [30] to generate a multi-view video guided\nby 3D layouts in the experiment.\n4.2. Experimental Results\nQualitative Results\nWe demonstrate the quality and the\nconsistency of single-frame multi-view and single-view\nvideo generation in Figure 5 and Figure 6, respectively. We\ncan see that our model obtains both cross-view and cross-\nframe consistency. The generated instances are accurately\ncontrolled by the input 3D layout. When the 3D layout of\nthe vehicle category is very close to the main vehicle, the\nlayout tends to be projected into the images of two per-\nspectives, and even some images are out of the scope. In\nthis case, our model can still generate cross-view consis-\ntent adjacent vehicles. For the quality of instance genera-\ntion, whether it is the non-rigid pedestrian class, highly oc-\ncluded/truncated vehicles, or the cone bucket class with less\nsample size in the dataset, our model can give high-quality\ninstance generation results and maintain self-consistency\nwith the surrounding environment.\nQuantitative Results\nTo assess the performance of our\nmodels, we employ the frame-wise Fr\u00b4echet Inception Dis-\ntance (FID) [21] to evaluate the quality of generated images.\nFor video quality evaluation, we utilize the Fr\u00b4echet Video\nDistance (FVD) [33]. Unless otherwise noted, all metrics\nare calculated on the nuScenes validation set.\nDue to the lack of related work, we have sorted out the\nrecent work and compared the quality of image/video gen-\neration under various tasks. For example, tasks of image\n8\nMethod\nMulti-View\nMulti-Frame\nFID\u2193\nFVD\u2193\nBEVGen [13]\n!\n25.54\n-\nDriveDreamer [35]\n!\n52.6\n452\nDrivingDiffusion\n!\n15.89\n-\nDrivingDiffusion\n!\n15.85\n335\nDrivingDiffusion\n!\n!\n15.83\n332\nTable 1. Comparison with other methods on nuScenes validation.\nThe FID indicator and FVD indicator feed back the image and\nvideo quality, respectively.\nMethod\nRoad mIoU\u2191\nVehicle mIoU\u2191\nDrivable mIoU\u2191\nObject NDS\u2191\nBaseline\n71.3\n36.0\n81.7\n41.2\nBEVGen\n50.2 (-21.1%)\n5.9 (-39.1%)\n-\n-\nSparse BEVGen\n50.9 (-20.4%)\n6.7 (-29.4%)\n-\n-\nDrivingDiffusion\n63.2(-8.1%)\n31.6(-4.4%)\n67.8 (-13.9%)\n33.1 (-8.1%)\nTable 2. Detection and segmentation results of different validation\ndata over all 6 views on the validation set of nuScenes. In green is\nthe relative drop compared with the standard nuScenes validation\ndata. Road mIoU and Vehicle mIoU are obtained by the CVT [45]\nsegmentation model while Drivable mIoU and Vehicle NDS are\nobtained by the BEVFusion [18] multi-task model.\ngeneration in autonomous driving scenes like BEVGen [31]\nand tasks of video generation like DriveDreamer [35].\nAs presented in Table 1, we made every effort to use\nthe most closely related metrics and settings for compari-\nson. Despite generating both multi-view and multi-frame\noutputs without performing frame interpolation or super-\nresolution, our method achieved FID and FVD scores of\n15.83 and 332, respectively. These results demonstrate con-\nsiderable advantages over other approaches. The last three\nrows in Table 1 show that the indexes of the complete Driv-\ningDiffusion and multi-view/temporal model are similar,\nproving our method\u2019s stability.\nAlthough methods such as FID are commonly used to\nmeasure the quality of image synthesis, they do not fully\ncapture the design objectives of our task, nor do they re-\nflect the quality of synthesis across different semantic cate-\ngories. Since we are trying to generate multi-view images\nconsistent with the 3D layout, we want to measure our per-\nformance on this consistency.\nWe take advantage of the official model provided by\nCVT [45] and BEVFusion [18] as evaluators. We adopted\nthe same set of generated images conditioned on a ground-\ntruth 3D layout as the nuScenes validation set, applied CVT\nand BevFusion to each set of generated images, and then\ncompared the predicted layout to the ground-truth BEV lay-\nout. We report average intersection crossing (mIoU) scores\nfor drivable areas and NDS for all the object classes in Ta-\nble 2.\nTo ensure a fair comparison, we focus on the drop com-\npared to the standard nuScenes validation data. The green\nnumbers in Table 2 represent this drop. Observing the road\ngeneration quality, we can see that DrivingDiffusion signif-\nicantly outperforms BEVGen, with a difference of -8.1%\nSetting\nExtra-Data Amount\nObject NDS\u2191\nmAOE\u2193\n(a)\n0\n0.412\n0.5613\n(b)\n2,000\n0.418\n0.5295 (-3.18%)\n(c)\n6,000\n0.434 (+2.2%)\n0.5130 (-4.83%)\nTable 3. The impact of data augmentation using synthetic data on\nperceptual performance. We use the BEVFusion visual model as\nthe baseline.\nSetting\nConsistency Module\nLocal Prompt\nFID\u2193\nFVD\u2193\nObject NDS\u2191\n(a)\n17.30\n420\n28.1\n(b)\n!\n16.67\n365\n28.9\n(c)\n!\n17.62\n409\n32.6\n(d)\n!\n!\n15.83\n332\n33.1\nTable 4. Ablation of the consistency module and the local prompt.\ncompared to BEVGen\u2019s -21.1%. Similarly, for the instance\ngeneration quality, DrivingDiffusion surpasses BEVGen by\na substantial margin, with a difference of -4.4% compared\nto BEVGen\u2019s -39.1%.\nThese remarkable improvements\ndemonstrate the effectiveness of our proposed approach.\nTable 3 presents the performance improvement achieved\nin the BEV perception task through synthesized data-\naugmentation. In the original training data, there was an\nissue with long-tail distribution, specifically with small tar-\ngets, close-range vehicles, and vehicle orientation angles.\nWe focused on generating additional data for these cate-\ngories with limited samples to address this. In Setting (b),\nwe only added 2,000 frames of synthetic data, focusing\non improving the distribution of obstacle orientation an-\ngles. Comparing Setting (a) and (b), and using BEVFu-\nsion as the baseline, although NDS only showed a slight\nimprovement, mAOE significantly decreased from 0.5613\nto 0.5295. Comparing Setting (a) and (c), by augmenting\nthe data with 6,000 frames of synthetic data, which is more\nfocused on rare scenarios, we observed notable enhance-\nments on the nuScenes validation set. The NDS results in-\ncreased from 0.412 to 0.434, while the mAOE decreased\nfrom 0.5613 to 0.5130. This proves that the data augmenta-\ntion of synthetic data brings significant benefits to percep-\ntual tasks.\n4.3. Ablation Study\nTo verify the validity of our design choices, an ablation\nstudy was performed on the key features of the model, in-\ncluding the consistency module and the local prompt. We\nrun these experiments on the same nuScenes validation set\nin Table 4.\nEffectiveness of Consistency Module and Local Prompt.\nTo verify the necessity of the consistency design and the lo-\ncal prompt, we conducted ablation experiments on the con-\nsistency module and the local prompt. We evaluated both\n9\nLet\u2019s Control \uff01\nRemove\nMove back\nMove along\nMove Left\nMove Closer\nTurn Right\nTurn Right More\nCopy\nCloser\nMore Closer\nCollision !\nFly !\nOriginal layouts of the nuScenes dataset.\nOriginal images of the nuScenes dataset.\nImages generated by DrivingDiffusion when seed=22\nImages generated by DrivingDiffusion when seed=33\nWithout Consistency Module\nWithout Local Prompt\nFigure 7. The generation effectiveness and the ability of the layout control model under different conditions.\nthe global quality by FID and FVD under different settings\nand the instance quality by Object NDS.\nCompared to Setting (a) and (b), we observed a decrease\nin FID and FVD by 0.63 and 55, respectively. This indicates\nthat the consistency module significantly improves the over-\nall image and video quality, with a higher impact on FVD.\nThis may be due to the feedback of cross-frame consistency\non FVD, as it directly affects the temporal coherence of the\nvideo. On the other hand, the impact on FID is more indi-\nrect, as it mainly measures the similarity between generated\nand real images.\nComparing Setting (a) and (c), we found that the local\nprompt significantly improves the instance quality, with Ob-\nject NDS increasing from 28.1 to 32.6. This demonstrates\nthat the local prompt effectively guides the generation pro-\ncess to focus on specific objects, leading to better instance-\n10\nlevel performance.\nFinally, Setting (d) shows that the combination of the\nconsistency module and the local prompt achieves the opti-\nmal generation quality: FID, FVD, and NDS reached 15.83,\n332, and 33.1, respectively. By jointly considering global\nconsistency and local guidance, our approach effectively\nbalances the overall image/video quality and instance-level\nperformance. This further validates the necessity of incor-\nporating both consistency design and local prompt in our\nframework.\nVisualization of Difference Settings.\nIn the upper por-\ntion of Figure 7, we compare the original nuScenes image\nwith multi-view images generated using various seeds to\ndemonstrate the diversity of our results. Next, we highlight\nthe significance of our core designs. As evident from the\nfigure, the removal of the Local Prompt leads to a decrease\nin the quality of instances, while the elimination of the Con-\nsistency Module results in view-inconsistent perspectives.\nIn the lower part of the figure, we present the outcome of\ndisturbing the original layout. Our method even is capable\nof simulating collision scenes and cars suspended in the air.\n5. Conclusion\nOur work is the first to realize multi-view video gen-\neration of autonomous driving scenes and implement pre-\ncise layout control, which has great significance for au-\ntonomous driving data generation and simulation.\nOur\nparadigm can be easily extended to the depth map, point\ncloud, occupancy, rendered assets, etc.\nfor control of\nvideo generation. Furthermore, our approach can be trained\nusing autoregressive techniques on a substantial number\nof videos(preferably those obtained through data mining).\nThis not only allows us to get a model that can predict fu-\nture videos but also makes it an excellent pre-trained model\nfor subsequent downstream tasks. This may be due to the\nmodel\u2019s enhanced understanding of driving scenarios by\npredicting future events.\nIn future work, we will explore the memory-friendly\nend-to-end generation of video across views, which is a\nquestion of how to compress the extra dimensions. In ad-\ndition, we believe that NerF-based approaches can provide\nbetter spatial and temporal consistency, and we will explore\nintroducing Nerf to help improve the quality of our multi-\nview video generation.\nReferences\n[1] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models.\narXiv preprint arXiv:2304.08818,\n2023. 2\n[2] Jiaxin Cheng, Xiao Liang, Xingjian Shi, Tong He, Tianjun\nXiao, and Mu Li. Layoutdiffuse: Adapting foundational dif-\nfusion models for layout-to-image generation. arXiv preprint\narXiv:2302.08908, 2023. 2, 4\n[3] Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang.\nCogview2: Faster and better text-to-image generation via hi-\nerarchical transformers. arXiv preprint arXiv:2204.14217,\n2022. 3\n[4] Shaoheng Fang, Zi Wang, Yiqi Zhong, Junhao Ge, Siheng\nChen, and Yanfeng Wang.\nTbp-former: Learning tempo-\nral bird\u2019s-eye-view pyramid for joint perception and predic-\ntion in vision-centric autonomous driving.\narXiv preprint\narXiv:2303.09998, 2023. 2\n[5] Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen,\nYilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-\nend visual trajectory prediction via 3d agent queries. arXiv\npreprint arXiv:2208.01582, 2022. 2\n[6] Sen He, Wentong Liao, Michael Ying Yang, Yongxin Yang,\nYi-Zhe Song, Bodo Rosenhahn, and Tao Xiang. Context-\naware layout to image generation with enhanced object ap-\npearance.\nIn Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 15049\u2013\n15058, 2021. 3\n[7] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. Advances in Neural Information\nProcessing Systems, 33:6840\u20136851, 2020. 3, 4\n[8] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William\nChan, Mohammad Norouzi, and David J Fleet. Video dif-\nfusion models. arXiv:2204.03458, 2022. 3, 4\n[9] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu,\nand Jie Tang.\nCogvideo:\nLarge-scale pretraining for\ntext-to-video generation via transformers.\narXiv preprint\narXiv:2205.15868, 2022. 3\n[10] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00b4\u0131a Dudas, Jef-\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\nAlex Kendall. Fiery: future instance prediction in bird\u2019s-\neye view from surround monocular cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15273\u201315282, 2021. 2\n[11] Hou-Ning Hu, Yung-Hsu Yang, Tobias Fischer, Trevor Dar-\nrell, Fisher Yu, and Min Sun.\nMonocular quasi-dense 3d\nobject tracking. IEEE Transactions on Pattern Analysis and\nMachine Intelligence, 45(2):1992\u20132008, 2022. 2\n[12] Bo Jiang, Shaoyu Chen, Xinggang Wang, Bencheng Liao,\nTianheng Cheng, Jiajie Chen, Helong Zhou, Qian Zhang,\nWenyu Liu, and Chang Huang. Perceive, interact, predict:\nLearning dynamic and static clues for end-to-end motion pre-\ndiction. arXiv preprint arXiv:2212.02181, 2022. 2\n[13] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds.\nIn CVPR, pages\n12697\u201312705, 2019. 9\n[14] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera\nimages via spatiotemporal transformers.\narXiv preprint\narXiv:2203.17270, 2022. 2\n11\n[15] Zejian Li, Jingyu Wu, Immanuel Koh, Yongchuan Tang, and\nLingyun Sun.\nImage synthesis from layout with locality-\naware mask adaption. In Proceedings of the IEEE/CVF In-\nternational Conference on Computer Vision, pages 13819\u2013\n13828, 2021. 3\n[16] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu,\nSergio Casas, and Raquel Urtasun. Pnpnet: End-to-end per-\nception and prediction with tracking in the loop. In Proceed-\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 11553\u201311562, 2020. 2\n[17] Yingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun.\nPetr: Position embedding transformation for multi-view 3d\nobject detection. arXiv preprint arXiv:2203.05625, 2022. 2\n[18] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with unified bird\u2019s-eye view repre-\nsentation. arXiv preprint arXiv:2205.13542, 2022. 9\n[19] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 3\n[20] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan\nZhu. Semantic image synthesis with spatially-adaptive nor-\nmalization. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pages 2337\u20132346,\n2019. 3\n[21] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu.\nOn\naliased resizing and surprising subtleties in gan evaluation.\nIn CVPR, 2022. 8\n[22] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding\nimages from arbitrary camera rigs by implicitly unprojecting\nto 3d. In ECCV, 2020. 2\n[23] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. In International conference on machine learning, pages\n8748\u20138763. PMLR, 2021. 3, 6\n[24] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 3\n[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,\nChelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever.\nZero-shot text-to-image generation. In International Confer-\nence on Machine Learning, pages 8821\u20138831. PMLR, 2021.\n3\n[26] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, pages 10684\u2013\n10695, 2022. 4, 7\n[27] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily Denton, Seyed Kamyar Seyed\nGhasemipour,\nBurcu Karagol Ayan,\nS Sara Mahdavi,\nRapha Gontijo Lopes, et al.\nPhotorealistic text-to-image\ndiffusion models with deep language understanding. arXiv\npreprint arXiv:2205.11487, 2022. 3\n[28] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki\nCheung, Alec Radford, and Xi Chen. Improved techniques\nfor training gans. Advances in neural information processing\nsystems, 29, 2016.\n[29] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 3\n[30] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 8\n[31] Alexander Swerdlow, Runsheng Xu, and Bolei Zhou. Street-\nview image generation from a bird\u2019s-eye view layout. arXiv\npreprint arXiv:2301.04634, 2023. 2, 4, 9\n[32] Tristan Sylvain, Pengchuan Zhang, Yoshua Bengio, R Devon\nHjelm, and Shikhar Sharma. Object-centric image genera-\ntion from layouts. In Proceedings of the AAAI Conference on\nArtificial Intelligence, volume 35, pages 2647\u20132655, 2021. 3\n[33] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 8\n[34] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete\nrepresentation learning. Advances in neural information pro-\ncessing systems, 30, 2017. 3, 4\n[35] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and\nJiwen Lu. Drivedreamer: Towards real-world-driven world\nmodels for autonomous driving, 2023. 9\n[36] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji,\nFan Yang, Guillermo Sapiro, and Nan Duan. Godiva: Gen-\nerating open-domain videos from natural descriptions. arXiv\npreprint arXiv:2104.14806, 2021. 3\n[37] Chenfei Wu, Jian Liang, Lei Ji, Fan Yang, Yuejian Fang,\nDaxin Jiang, and Nan Duan. N\u00a8uwa: Visual synthesis pre-\ntraining for neural visual world creation.\nIn Computer\nVision\u2013ECCV 2022: 17th European Conference, Tel Aviv,\nIsrael, October 23\u201327, 2022, Proceedings, Part XVI, pages\n720\u2013736. Springer, 2022. 3\n[38] Kaixin Xiong, Shi Gong, Xiaoqing Ye, Xiao Tan, Ji Wan,\nErrui Ding, Jingdong Wang, and Xiang Bai. Cape: Camera\nview position embedding for multi-view 3d object detection.\narXiv preprint arXiv:2303.10209, 2023. 2\n[39] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang,\nJames Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge,\nand Yonghui Wu.\nVector-quantized image modeling with\nimproved vqgan. arXiv preprint arXiv:2110.04627, 2021. 3\n[40] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gun-\njan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yin-\nfei Yang, Burcu Karagol Ayan, et al.\nScaling autoregres-\nsive models for content-rich text-to-image generation. arXiv\npreprint arXiv:2206.10789, 2022. 3\n[41] Lvmin Zhang and Maneesh Agrawala. Adding conditional\ncontrol to text-to-image diffusion models.\narXiv preprint\narXiv:2302.05543, 2023. 5\n[42] Tianyuan Zhang, Xuanyao Chen, Yue Wang, Yilun Wang,\nand Hang Zhao. Mutr3d: A multi-camera tracking frame-\nwork via 3d-to-2d queries. In Proceedings of the IEEE/CVF\n12\nConference on Computer Vision and Pattern Recognition,\npages 4537\u20134546, 2022. 2\n[43] Yifu Zhang, Xinggang Wang, Xiaoqing Ye, Wei Zhang,\nJincheng Lu, Xiao Tan, Errui Ding, Peize Sun, and Jing-\ndong Wang.\nBytetrackv2: 2d and 3d multi-object track-\ning by associating every detection box.\narXiv preprint\narXiv:2303.15334, 2023. 2\n[44] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi,\nYing Shan, and Xi Li. Layoutdiffusion: Controllable diffu-\nsion model for layout-to-image generation. arXiv preprint\narXiv:2303.17189, 2023. 2, 4\n[45] Brady Zhou and Philipp Kr\u00a8ahenb\u00a8uhl. Cross-view transform-\ners for real-time map-view semantic segmentation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 13760\u201313769, 2022. 9\n13\n",
    "2402.07369": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n1\nDiff-RNTraj: A Structure-aware Diffusion Model for\nRoad Network-constrained Trajectory Generation\nTonglong Wei, Youfang Lin, Shengnan Guo, Yan Lin, Yiheng Huang,\nChenyang Xiang, Yuqing Bai, Huaiyu Wan\nAbstract\u2014Trajectory data is essential for various applications. However, publicly available trajectory datasets remain limited in scale due\nto privacy concerns, which hinders the development of trajectory mining and applications. Although some trajectory generation methods\nhave been proposed to expand dataset scale, they generate trajectories in the geographical coordinate system, posing two limitations\nfor practical applications: 1) failing to ensure that the generated trajectories are road-constrained. 2) lacking road-related information. In\nthis paper, we propose a new problem, road network-constrained trajectory (RNTraj) generation, which can directly generate trajectories\non the road network with road-related information. Specifically, RNTraj is a hybrid type of data, in which each point is represented by a\ndiscrete road segment and a continuous moving rate. To generate RNTraj, we design a diffusion model called Diff-RNTraj, which can\neffectively handle the hybrid RNTraj using a continuous diffusion framework by incorporating a pre-training strategy to embed hybrid\nRNTraj into continuous representations. During the sampling stage, a RNTraj decoder is designed to map the continuous representation\ngenerated by the diffusion model back to the hybrid RNTraj format. Furthermore, Diff-RNTraj introduces a novel loss function to enhance\ntrajectory\u2019s spatial validity. Extensive experiments conducted on two datasets demonstrate the effectiveness of Diff-RNTraj.\nIndex Terms\u2014Spatial-temporal data mining, diffusion model, trajectory generation, road network.\n\u2726\n1\nINTRODUCTION\nT\nHE widespread deployment of GPS devices in intelli-\ngent transportation systems (ITS) makes it convenient\nto record where a vehicle is. In this case, a sequence of\nordered geographical coordinates of a vehicle is termed a\ntrajectory. Mining trajectory data can support the devel-\nopment of various downstream applications in ITS, such\nas user demand prediction [1]\u2013[3], vehicle navigation [4],\nand personalized route recommendation [5]\u2013[7]. Despite the\npivotal role of trajectory data, publicly available trajectory\ndatasets are usually limited in scale. This is mainly due to\nprivacy policies and public concern, including regulatory\nrestrictions on trajectory data storage and sharing in many\ncountries and areas, and the concern of users on personal\nlocation embedded in trajectories being exposed. The avail-\nable small-scale trajectory datasets hinder the study of tra-\njectory data mining, since the state-of-the-art trajectory data\nmining solutions, especially deep-learning based solutions,\nusually require large-scale datasets to achieve optimal re-\nsults.\nTo obtain large-scale trajectory data, the study on trajec-\ntory generation has gained much attention [8]\u2013[11]. It aims\nto analyze human potential spatial transfer patterns in cities\nand generate simulated GPS location sequences, which is\nvital for many trajectory-based applications. For example,\ntraffic simulation systems utilize the generated trajectory to\nsimulate vehicle movement across road networks, thereby\nCorresponding author: Shengnan Guo\n\u2022\nT. Wei, Y. Lin, S. Guo, Y. Lin, Y. Huang, C. Xiang, Y. Bai and H. Wan\nare with School of Computer and Information Technology, Beijing Jiaotong\nUniversity, Beijing 100044, China; And the Beijing Key Laboratory of\nTraffic Data Analysis and Mining, Beijing 100044, China.\nE-mail: {weitonglong, yflin, guoshn, ylincs, huangyiheng, cyxiang,\nbaiyuqing, hywan} @bjtu.edu.cn\nManuscript received xx xx, xxxx; revised xx xx, xxxx.\ncalculating high-fidelity traffic conditions that prompt the\nadvancement of navigation systems [8], [12]. Companies\nsuch as Uber leverage synthetic trajectories to expand the\nscale of trajectory datasets and develop larger models to link\nusers and trajectories [13]. Employing generated trajectory\ndata for research also addresses privacy concerns, as it is\ndevoid of personal information [14].\nExisting efforts for trajectory generation can be catego-\nrized into rule-based methods [15], [16] and deep learning\nmethods [8], [17], [18]. Rule-based methods simulate hu-\nman movement under the assumption that moving behav-\nior adheres to predefined rules. However, they struggle to\ncapture the complexity and uncertainty inherent in human\ntravel patterns, limiting their ability to generate realistic\ntrajectories. By comparison, deep learning methods such\nas variational auto-encoder (VAE)-based methods [19], [20]\nand generative adversarial network (GAN)-based methods\n[21], [22] are more flexible, since they are able to repre-\nsent the complicated distribution of trajectories. However,\nall these methods represent trajectories by generating se-\nquences of GPS points (i.e., latitude and longitude) in the\ngeographical coordinate system, causing two limitations\nwhen the generated trajectories are utilized by downstream\napplications. Firstly, directly generating GPS points fails to\nensure that the trajectories are on the road. As illustrated\nin Figure 1(a), the green GPS points p1 \u223cp6 produced\nby current trajectory generation methods cannot accurately\nreplicate the real moving patterns of vehicles on the road,\nas a valid and logical GPS point in the trajectory ought to\nstay on the road consistently. Secondly, most downstream\ntrajectory mining tasks, especially road-related applications,\nrequire not only precise GPS information (e.g., latitude and\nlongitude) but also road-related information (e.g., the road\nsegments on which the vehicle), yet such road-related infor-\narXiv:2402.07369v2  [cs.LG]  11 Sep 2024\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n2\n\ud835\udc91\ud835\udfcf\n\ud835\udc91\ud835\udfd0\n\ud835\udc91\ud835\udfd1\n\ud835\udc91\ud835\udfd2\n\ud835\udc91\ud835\udfd3\n\ud835\udc91\ud835\udfd4\nGPS point\nMap-matched point\nRoute prediction\nVehicle navigation\nDrive behavior analysis\nTrajectory-based Applications \n(a) Two-stage solution\n(b) Our solution\n\ud835\udc11\ud835\udc1a\ud835\udc27\ud835\udc1d\ud835\udc28\ud835\udc26\ud835\udc25\ud835\udc32\ud835\udc12\ud835\udc1a\ud835\udc26\ud835\udc29\ud835\udc25\ud835\udc1e\n\ud835\udc1a\ud835\udc06\ud835\udc1a\ud835\udc2e\ud835\udc2c\ud835\udc2c\ud835\udc22\ud835\udc1a\ud835\udc27\ud835\udc27\ud835\udc28\ud835\udc22\ud835\udc2c\ud835\udc1e\n(1) GPS point Generation\nEnd-to-end generates \ntrajectory on the road\nMap-matching is unreliable \n\ud835\udc86\ud835\udfcf\n\ud835\udc86\ud835\udfd0\n\ud835\udc86\ud835\udfd1\n\ud835\udc86\ud835\udfd2\n\ud835\udc86\ud835\udfd3\n\ud835\udc86\ud835\udfd4\n\ud835\udc86\ud835\udfd5\n(2) Map Matching\n\ud835\udc91\ud835\udfcf\n\ud835\udc92\ud835\udfcf\n\ud835\udc91\ud835\udfd0\n\ud835\udc91\ud835\udfd1\n\ud835\udc91\ud835\udfd2\n\ud835\udc91\ud835\udfd3\n\ud835\udc91\ud835\udfd4\n\ud835\udc92\ud835\udfd1\n\ud835\udc92\ud835\udfd3\n\ud835\udc92\ud835\udfd4\n\uff1f\n\uff1f\n\uff1f\n\ud835\udc92\ud835\udfd2\n\ud835\udc92\ud835\udfd0\nFig. 1. Comparison between the two-stage solution and our end-to-end\nsolution. Two-stage solution: first generate GPS points, then perform\nmap matching process. Our solution: directly generate trajectories on\nthe road end-to-end.\n\ud835\udc52!\n\ud835\udc52\"\n\ud835\udc52#\n\ud835\udc52$\n\ud835\udc52%\n\ud835\udc52&\n\ud835\udc52'\n\ud835\udc5f\" =\n\ud835\udc51!\n\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e\n\ud835\udc5f# =\n\ud835\udc51\"\n\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e\u2212\ud835\udc51!\n\ud835\udc5e!\n\ud835\udc5e$\nRNTraj point\n\u03c4 =< \ud835\udc5d!, \ud835\udc5d\", \ud835\udc5d#, \ud835\udc5d$>\nRocT \u03c4()*+ =< \ud835\udc5e!, \ud835\udc5e\", \ud835\udc5e#, \ud835\udc5e$>\nRoad Segment\n\ud835\udc51!\n\ud835\udc51\"\n\ud835\udc59\ud835\udc52\ud835\udc5b\ud835\udc54\ud835\udc61\u210e\n\ud835\udc5e#\n\ud835\udc5e\"\nFig. 2. A RNTraj is a point sequence formed by \u27e8q1, q2, q3, q4\u27e9. Each\nRNTraj point q can be determined by road segment e and moving ratio\nr, e.g., the road segment of RNTraj point q2 is e4, and the moving ratio\nis r2. For q3, its road segment is e4, and the moving ratio is r3.\nmation can not be retrieved directly from the generated GPS\npoints.\nTherefore, to utilize the generated trajectory in down-\nstream tasks, existing trajectory generation methods require\nan additional process, i.e. applying map-matching [23] al-\ngorithm to the generated GPS sequences to ensure the final\ntrajectories are valid and contain road-related information.\nHowever, such a two-stage strategy that first generates\nGPS points and then performs map matching is error-\naccumulating since the map-matching algorithm may return\nan unreliable result, especially when dealing with generated\nGPS points that significantly deviate from the road. For\nexample, in Figure 1(a), suppose that a vehicle\u2019s real travel\npath is Path1 : e1 \u2192e2 \u2192e4 \u2192e5 \u2192e7. To reconstruct\nthis trajectory, a two-stage strategy first generates six GPS\npoints \u27e8p1, p2, \u00b7 \u00b7 \u00b7 , p6\u27e9. Then, a map-matching algorithm is\napplied, which may map GPS points p2, p4 to road seg-\nments e3 and e6, respectively. Finally, an undesired path\nPath2 : e1 \u2192e3 \u2192e4 \u2192e6 \u2192e7 is generated, deviating\nfrom the real one.\nDrawing on these observations, we propose a novel task\ncalled road network-constrained trajectory generation, i.e.,\ndirectly generating GPS trajectories on the road network\nin an end-to-end manner, thereby yielding trajectories di-\nrectly applicable to subsequent applications. As shown in\nFigure 1(b), our newly proposed trajectory generation task\nnot only ensures the validity of trajectory but also main-\ntains detailed road-related information on vehicles\u2019 moving\nbehavior. Our proposed generation task calls for defining a\nnew term Road Network-constrained Trajectory (RNTraj).\nA RNTraj \u03c4RN = \u27e8q1, q2, \u00b7 \u00b7 \u00b7 , qT \u27e9is a sequence of T\npoints in the road network coordinate system. Specifically,\neach RNTraj point q = (e, r) is determined by a road\nsegment e on which the vehicle is traveling, and a moving\nratio r which indicates the proportion of a vehicle\u2019s newly\ntraveled distance along the road segment to the initial\nremaining untraveled length on that segment. Figure 2\nshows an example of RNTraj. The road network-constrained\ntrajectory generation task aims to generate trajectories in the\nform of RNTraj, ensuring that the generated trajectories are\nalways on the roads and contain road-related information.\nIn this study, we propose to generate RNTraj based on\ndiffusion model [24], [25], since diffusion model\u2019s capability\nof generating high-quality, realistic data have been proven in\nmultiple domains, including image generation [26]\u2013[28] and\naudio generation [29], [30]. It demonstrates higher reliability\nand robustness compared to previous generative methods\nbased on VAE and GAN [31]. Furthermore, the diffusion\nmodel generates data from random Gaussian noise, which\neliminates the risk of privacy leakage. Despite the promising\nperformance of diffusion model, generating RNTraj with\na diffusion model still faces the following unsolved chal-\nlenges.\n(1) Hybrid data generation. The state-of-the-art dif-\nfusion models can achieve superior performance in data\ngeneration, particularly for continuous data type such as\nimage [32], [33] and audio [34], [35]. However, in our case,\nRNTraj is \u201chybrid\u201d, as it consists of a discrete road segment\ne and a continuous moving ratio r. So existing diffusion\nmodels tailored toward the generation of a single type of\ndata cannot be directly implemented for the generation\nof RNTraj. Although employing two diffusion models to\ngenerate road segments and moving ratios separately is\nan option, it fails to consider the correlation between road\nsegments and moving ratios in RNTraj. Furthermore, some\nstudies [36]\u2013[38] indicate that the diffusion model for dis-\ncrete data generation is still in the initial stage, and it faces\nchallenges of producing high-quality results.\n(2) Spatial validity. A generated trajectory is considered\nreasonable only when it adheres to fundamental physical\nprinciples and exhibits conventional driving behaviors. For\nexample, in Figure 2, a reasonable trajectory can follow the\nroute e1 \u2192e4 \u2192e6 rather than e1 \u2192e6 \u2192e3, as road\nsegment e6 is disconnected to its predecessor e1 and suc-\ncessor e3. However, diffusion models generate data through\na multi-step denoising process, where each denoising step\ndirectly predicts the added noise, without taking into ac-\ncount the spatial validity of the final generated trajectories.\nThus, how to incorporate spatial validity to guide trajectory\ngeneration in diffusion models requires consideration.\nTo tackle the aforementioned challenges and achieve\nend-to-end RNTraj generation effectively, we propose a\nstructure-aware diffusion model for RNTraj generation (Diff-\nRNTraj). Specifically, Diff-RNTraj can handle hybrid data\nusing continuous diffusion models by introducing a novel\nidea to convert a hybrid RNTraj into a continuous represen-\ntation. This involves utilizing a pre-trained strategy to first\nlearn the representation of discrete road segments, followed\nby concatenating the representation with the moving ratio\ntogether to obtain a vectorized RNTraj in the continuous\nspace. Then, during the sampling stage, we propose a hy-\nbrid RNTraj decoder to map the generated continuous rep-\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n3\nresentations back to RNTraj format. Additionally, to ensure\nthe spatial validity of the generated RNTraj, we incorporate\na novel spatial validity loss at each step of the diffusion\nmodel\u2019s denoising process to guide model training.\nIn summary, the main contributions of this work are\nsummarized as follows:\n\u2022\nWe propose a new problem, called road network-\nconstrained trajectory generation, which directly\ngenerates trajectory on the road network, provid-\ning improved support for downstream trajectory-\nbased applications compared to conventional trajec-\ntory generation.\n\u2022\nWe design Diff-RNTraj to generate hybrid RNTraj by\nutilizing a continuous diffusion model. Diff-RNTraj\nintegrates a pre-trained strategy to embed hybrid\nRNTraj into continuous representations, along with\na decoder to convert the generated continuous rep-\nresentation back to hybrid RNTraj. Additionally, we\nintroduce a novel spatial validity loss function to\nguarantee the spatial validity of the generated RN-\nTraj.\n\u2022\nExtensive experiments on two real-world trajectory\ndatasets show that compared with other baselines,\nthe trajectories produced by our Diff-RNTraj exhibit\nthe highest level of rationality across a range of\nstatistical metrics.\n2\nRELATED WORK\n2.1\nTrajectory Generation\nGenerating large-scale, realistic trajectory data has gar-\nnered much attention in the research community. Early\napproaches [39]\u2013[41] rely on predefined rules to simulate\nuser behavior, disregarding the inherent randomness and\ncomplexity of human movement. Consequently, the gener-\nated trajectories fail to accurately capture real-world human\nmovement patterns.\nWith the advance of generative models such as Varia-\ntional Autoencoders (VAE) [8], [10], [17], [20] and Genera-\ntive Adversarial Networks (GAN) [18], [22], [42], researchers\nhave proposed a range of trajectory generation models that\nhave shown promising results. TrajVAE [10] follows the\nVAE framework, and utilizes Long Short-Term Memory\nnetworks (LSTM) to capture the temporal dependence and\nmodel trajectory data as a Gaussian distribution. It gen-\nerates trajectory points step-by-step by sampling Gaussian\nnoise and employing LSTM. EETG [8] is a VAE-like model\nthat effectively integrates both global and local trajectory\nsemantic information to generate. SeqGAN [42] employs\nthe policy gradient algorithm for training. Its generator is\nimplemented as an LSTM network and generates trajecto-\nries step by step. Furthermore, some works [43]\u2013[45] utilize\ngenerative adversarial imitation learning [46] to generate\ntrajectory. In addition to directly generating GPS points,\nsome researchers explore gridding trajectories [9], [18]. They\nfirst employ a generative model to produce grid data, then\nconvert it into trajectory data. DiffTraj [47] proposes a dif-\nfusion model method for trajectory generation, which first\nadds noise to the trajectory data and then removes the noise\nstep-by-step to generate trajectory. However, these methods\nfocus on generating GPS points in the geographic coordinate\nsystem that require the map-matching process to project the\ngenerated GPS points onto the road before the generated\ntrajectories are used. This two-stage solution will decrease\nthe quality of generated trajectories since map-matching on\nthe GPS points with errors is unreliable.\nThere are some works that focus on generating the road-\nlevel trajectory [14], [48], [49], which only generates the\nroad segment sequences. Compared to the RNTraj gener-\nation task studied in this paper, road-level trajectory gen-\neration is coarse-grained. It cannot obtain precise latitude\nand longitude information for a vehicle, which will bring\nshortcomings when analyzing the trajectory behavior, e.g.,\nthe travel speed and acceleration patterns.\n2.2\nDiffusion Model\nThe diffusion model is an iterative generative model that\nis proposed by [24] and further improved by [25], [50],\n[51], which consists of two Markov processes: a forward\ndiffusion process and a reverse denoising process. In the\nforward diffusion process, a clean sample is perturbed by\ngradually adding noise until it becomes a Gaussian noise. In\nthe reverse denoising process, the added noise is removed\nstep by step until a clean sample is obtained. Due to its\nhigh-quality generation capabilities, the diffusion model\nhas gained widespread attention in the field of computer\nvision [26], [52] and natural language processing [53], [54].\nAccording to the types of generated samples, diffusion\nmodels are divided into two types: continuous diffusion\nmodels, which are used for generating continuous data such\nas images [52] and speech data [35], and discrete diffusion\nmodels, which are used for generating discrete data like\ntext [55]. However, there exists a gap in utilizing diffusion\nmodels to directly generate hybrid data that combines both\ncontinuous and discrete data. For example, our RNTraj\nconsists of discrete road segments and continuous moving\nrates, which is what our work focuses on.\n3\nPRELIMINARIES\nIn this section, we first define the basic conceptions and\nthen introduce the problem definition of road network-\nconstrained trajectory generation.\nDefinition 1 (Trajectory). A trajectory \u03c4 is a sequence of\nGPS points, \u03c4 = \u27e8p1, p2, \u00b7 \u00b7 \u00b7 , pT \u27e9, where pi = (loni, lati)\nrepresent the longtitude and latitude of i-th trajectory\npoint, and T = |\u03c4| is the length of trajectory.\nDefinition 2 (Road Network). The road network is defined\nas a directed graph G = (V, E), where V represents the\nset of the intersections between road segments, with each\nintersection having lon and lat attributes. E represents\nthe set of road segments that link two intersections. Each\nroad segment, denoted as e \u2208E, has three attributes:\n\u2022\nstart: The start intersection of a road segment.\n\u2022\nend: The terminate intersection of a road segment.\n\u2022\nlength: The length of a road segment in meters.\nDefinition 3 (Road Network-constrained Trajectory). We\ndefine a road network-constrained trajectory, i.e., RNTraj,\nof length T as \u03c4RN = \u27e8q1, q2, \u00b7 \u00b7 \u00b7 , qT \u27e9, where each RNTraj\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n4\npoint q = (e, r) is located in the road coordinate system.\nHere, e denotes the road segment where q is located,\nand r \u2208[0, 1] is the moving ratio, representing the\nproportion of newly traveled distance covered along the\nroad segment to the initial remaining untraveled length\non that segment.\nTaking Figure 2 as an example, we illustrate the meaning\nof the moving ratio r. Here, we have a RNTraj form by\n\u27e8q1, \u00b7 \u00b7 \u00b7 , q4\u27e9and points q2 and q3 both pass through road\ne4, so their road segment are both e4. For q2, it is the first\npoint to through road segment e4, so its initial remaining\nuntraveled length is equal to the total distance of road\nsegment e4. The distance it traveled on this road is from\nthe start point of e4 to the location of q2, i.e., d1. Therefore,\nthe moving ratio of q2 is calculated as r2 =\nd1\ne4.length.\nFor q3, it is not the first point to pass through this road.\nConsequently, its initial remaining untraveled length is the\ntotal length of the road segment minus the distance from\nthe beginning of the road segment to its predecessor RNTraj\npoint, i.e., e4.length \u2212d1. The newly traveled distance on\nthis road segment is d2, and its moving rate is computed\nas\nd2\ne4.length\u2212d1 . Therefore, we define the moving ratio rt for\nthe t-th point qt in RNTraj as:\nrt =\n( dis(qt.e.start, qt)\nqt.e.length\nif qt.e \u0338= qt\u22121.e or t = 1\ndis(qt\u22121, qt)\nqt.e.length\u2212dis(qt.e.start, qt\u22121)\notherwise,\n(1)\nwhere dis(a, b) is the function to calculate the distance along\nthe road network from a to b.\nGiven a RNTraj \u03c4RN\n= \u27e8q1, \u00b7 \u00b7 \u00b7 , qT \u27e9, we can easily\ncalculate the precise GPS coordinates of each RNTraj point\naccording to Algorithm 1.\nAlgorithm 1 Calculate GPS Coordinate of Each RNTraj Point\nInput: A RNTraj \u03c4RN = \u27e8q1, \u00b7 \u00b7 \u00b7 , qT \u27e9;\nOutput: The longitude and latitude of each RNTraj point;\nTemporary variable: temp r, used to calculate the final\ncoordinates;\n1: for t = 1 to T do\n2:\nif t = 1 or et \u0338= et\u22121 then\n3:\ntemp r \u2190rt;\n4:\nelse\n5:\ntemp r \u2190temp r + (1 \u2212temp r) \u2217rt;\n6:\nend if\n7:\nqt.lon \u2190et.start.lon\n8:\n+(et.end.lon \u2212et.start.lon) \u2217temp r;\n9:\nqt.lat \u2190et.start.lat\n10:\n+(et.end.lat \u2212et.start.lat) \u2217temp r;\n11: end for\nDefinition 4 (Road Network Constrained Trajectory Gener-\nation). Given a real-world trajectory dataset, the goal of\nthis problem is to train a deep learning model, which can\ngenerate a RNTraj dataset that follows the distribution of\noriginal trajectory data.\n4\nMETHODOLOGY\nIn this section, we introduce the Diff-RNTraj framework,\nwhich enables end-to-end generation of RNTraj using the\ndiffusion model. The framework, depicted in Figure 3, is\ncomposed of three main modules: the RNTraj vectorization\nmodule, the diffusion module, and the RNTraj decoder\nmodule. The RNTraj vectorization module is responsible for\nconverting a RNTraj from the hybrid format into a contin-\nuous representation space. This continuous representation\nserves as the input to the subsequent continuous diffusion\nmodel. The diffusion module learns the data distribution of\nRNTraj by applying a multi-step process of adding noise and\ndenoising. Once the training process is completed, it allows\nsampling a vectorized RNTraj in continuous space from\nGaussian noise. Finally, the generated continuous vectorized\nRNTraj is then mapped back to the hybrid format by the\nRNTraj decoder. In addition, to ensure that the generated\ntrajectory is spatially valid, we incorporate a novel spatial\nvalidity loss function to train the Diff-RNTraj framework.\n4.1\nRNTraj Vectorization\nIn order to leverage the prominent capabilities of the contin-\nuous diffusion model, it is essential to transform the hybrid\nRNTraj into a continuous representation. The core of this\ntransformation is to convert the road segment from the\ndiscrete type into a continuous representation, meanwhile\nthe continuous representations of the road segments should\nreflect the travel patterns of users on the road network.\nTo achieve this, we propose a pre-training strategy for\nperforming this conversion process.\nSpecifically, to effectively learn road segment represen-\ntations, it is crucial to model the relationship between road\nsegments. An intuitive approach to model these relation-\nships is by directly utilizing the topological structure of\nthe road network. However, it is important to note that\nthese topological relationships do not always align with the\npractical relationships observed in users\u2019 common travel\npatterns. For example, some physically connected roads\nare practically impassable due to traffic control measures.\nBesides, some road segments are frequently traveled suc-\ncessively by users (e.g., major roads), while others are\nnot. Therefore, the weights between the road segments are\ndifferent and cannot be accurately represented by the road\nnetwork topology.\nTo this end, we introduce UTGraph, a road segment con-\nnectivity graph constructed from user trajectories. UTGraph\ndirectly establishes connections between road segments\nbased on real user trajectories. It is defined as G\u03c4 = (R, E\u03c4),\nwhere R is the set of nodes representing road segments,\nand E\u03c4 is the set of edges representing transition frequencies\nbetween road segments, which can be extracted from road\nnetwork constrained trajectory as follows.\n\u2022\nFor nodes ri, rj \u2208R, there exists an edge eij \u2208E\u03c4 if\nthere is a RNTraj consecutively passing road segment\ni and road segment j.\n\u2022\nThe weight of edge ei,j is the total number of times\nthat road segments i and j are consecutively tra-\nversed in all trajectory data.\nAfter constructing UTGraph, we employ the Node2vec\nalgorithm [56] to calculate the road segment representation\nE \u2208R|R|\u00d7D, where D is the dimension of the road segment\nrepresentation, |R| is the number of road segments.\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n5\n\ud835\udc86\ud835\udfcf\nRNTraj \ud835\udf49\ud835\udc79\ud835\udc75\n\u2026\n\ud835\udc86\ud835\udfd0\n\ud835\udc86\ud835\udc7b\n\ud835\udc86\ud835\udfd1\n\ud835\udc93\ud835\udfcf\n\ud835\udc93\ud835\udfd0\n\ud835\udc93\ud835\udc7b\n\ud835\udc93\ud835\udfd1\u2026\n||\nScale: [0,1]\u2192[-1,1]\n(\ud835\udc86\ud835\udfcf\n\u2026\n(\ud835\udc86\ud835\udfd0\n(\ud835\udc86\ud835\udc7b\n(\ud835\udc86\ud835\udfd1\n(\ud835\udc93\ud835\udfcf\n(\ud835\udc93\ud835\udfd0\n(\ud835\udc93\ud835\udc7b\n(\ud835\udc93\ud835\udfd1\u2026\nPre-trained Road\nSegment Representation\nRescale: [-1,1]\u2192[0,1]\n\u2026\n\u2026\n\u2026\n\u2026\nb) Reverse Denoising Process\na) Forward Diffusion Process\n\ud835\udc7f\ud835\udfce\u2208\u211d\ud835\udc7b\u2217(\ud835\udc6b1\ud835\udfcf)\n\ud835\udc7f\ud835\udc8f'\ud835\udfcf\u2208\u211d\ud835\udc7b\u2217(\ud835\udc6b1\ud835\udfcf)\n\ud835\udc7f\ud835\udc8f\u2208\u211d\ud835\udc7b\u2217(\ud835\udc6b1\ud835\udfcf)\n\ud835\udc7f\ud835\udc75\u2208\u211d\ud835\udc7b\u2217(\ud835\udc6b1\ud835\udfcf)\n1\ud835\udc7f\ud835\udc86\u2208\u211d\ud835\udc7b\u2217\ud835\udc6b\nGenerated RNTraj (\ud835\udf49\ud835\udc79\ud835\udc75\nAdd noise\n\ud835\udc92(\ud835\udc7f\ud835\udc8f|\ud835\udc7f\ud835\udc8f-\ud835\udfcf)\n\ud835\udc91\ud835\udf3d(\ud835\udc7f\ud835\udc8f-\ud835\udfcf|\ud835\udc7f\ud835\udc8f, \ud835\udc8f)\nRNTraj Vectorization\nRNTraj Decoder\n\ud835\udc7f\ud835\udc86\u2208\u211d\ud835\udc7b\u2217\ud835\udc6b\nDiffusion Module\nNoise Schedule\nDenoising Network\nDenoising\nSpatial Validity Loss\nUTGraph \ud835\udc6e\ud835\udf49\nPre-trained Road Segment Representation\n\u2026\n\u2026\nRoad Representation\nRNTraj dataset\nAggregation\nNode2Vec\nChannel Split\nSimilarity\nCalculation \n& Argmax\nTraining Process\nSampling Process\n||\nConcatenate Operation\n\ud835\udc7f\ud835\udc75~\ud835\udcdd(\ud835\udfce, \ud835\udc08)\nGaussian Noise\n\u2026 \u2026\nFig. 3. The architecture of Diff-RNTraj, which consists of a RNTraj vectorization module, a diffusion module, and a RNTraj decoder module.\nBy combining the road segment representation and the\nmoving ratio along the channel dimension, we can represent\na RNTraj \u03c4RN with length T as a tensor X \u2208RT \u00d7(D+1),\nwhich is referred to as vectorized RNTraj. The first D\ndimensions of the vectorized RNTraj correspond to road\nsegment representations, denoted as Xe \u2208RT \u00d7D, and the\nlast dimension represents the moving rate r. To scale the\nmoving rate within the range [\u22121, 1], we use the transfor-\nmation X[:, D + 1] = 2 \u2217r \u22121.\nWith this approach, we can represent a RNTraj in con-\ntinuous space. Next, we employ the continuous diffusion\nmodel to generate the vectorized RNTraj.\n4.2\nDiffusion Model for Vectorized RNTraj Generation\nTo learn a distribution p\u03b8(X) that approximates the contin-\nuous distribution of vectorized RNTraj q(X) and to allow\neasy sampling, we utilize the diffusion model framework.\nThis framework encompasses two key steps: the forward\ndiffusion process and the reverse denoising process.\n4.2.1\nThe Forward Diffusion Process\nThe forward diffusion process gradually introduces noise to\nthe sample until the sample follows the standard Gaussian\ndistribution. Suppose that we have an original vectorized\nRNTraj, X0, the forward diffusion model aims to obtain\na noisy vectorized RNTraj, denoted as XN, through N\nsteps forward process. This process can be defined as the\nfollowing Markov chains:\nq(XN | X0) =\nN\nY\nn=1\nq(Xn | Xn\u22121),\n(2)\nwhere the forward diffusion process q(Xn | Xn\u22121) at each\nstep follows a Gaussian distribution:\nq(Xn | Xn\u22121) = N(Xn;\nq\n(1 \u2212\u03b2n)Xn\u22121, \u03b2nI),\n(3)\nwhere I represents the identity matrix. \u03b2n \u2208(0, 1) is fixed\nwhile \u03b21, \u03b22, \u00b7 \u00b7 \u00b7 , \u03b2N gradually increase to control the level\nof noise added in each forward step.\nSince \u03b2n is fixed at each step, we can simplify Equation 3\nby utilizing the properties of the Gaussian process to derive\nthe distribution of Xn given X0 for each step as:\nq(Xn | X0) = N(Xn; \u221a\u00af\u03b1nX0, (1 \u2212\u00af\u03b1n)I),\n(4)\nwhere \u00af\u03b1n = Qn\ni=1 \u03b1i, and \u03b1i = 1 \u2212\u03b2i. By utilizing the\nreparameterization technique [57], we can directly compute\nXn by:\nXn = \u221a\u00af\u03b1nX0 +\nq\n(1 \u2212\u00af\u03b1n)\u03f5,\n(5)\nwhere \u03f5 \u223cN(0, I).\nFinally, the noisy vectorized RNTraj in the last step of\nthe forward diffusion process follows the standard Gaussian\ndistribution.\nXN \u223cN(XN; 0, I)\n(6)\n4.2.2\nThe Reverse Denoising Process\nThe objective of the reverse denoising process is to trans-\nform XN into a noise-free vectorized RNTraj by gradually\nremoving noise step-by-step. Each step of the reverse de-\nnoising process is defined as:\np(Xn\u22121 | Xn) = N(Xn\u22121; \u00b5n\u22121, P\nn\u22121),\n(7)\nwhere \u00b5n\u22121 and P\nn\u22121 denote the mean and variance at\nthe (n \u22121)-th step, respectively. If the values of mean and\nvariance at each step are known, Xn\u22121 can be sampled\ndirectly. However, these values are unknown in practice.\nTherefore, we employ a neural network with parameters \u03b8\nfor estimating the mean and variance. The Equation 7 can\nbe reformulated as follows.\np\u03b8(Xn\u22121 | Xn) = N(Xn\u22121; \u00b5\u03b8(Xn, n), P\n\u03b8(Xn, n)),\n(8)\nwhere \u00b5\u03b8(Xn, n) and P\n\u03b8(Xn, n) are the learned mean and\nvariance. Following DDPM [25], we predict the added noise\nat each step of the forward diffusion process rather than\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n6\n\ud835\udc8f\nConv\nFC\n+\nDilation Conv (\ud835\udc85= \ud835\udfd0\ud835\udc8d%\ud835\udc8e)\ntanh\n\ud835\udf0e\n5\nConv\nConv\n+\nRDCL\n+\nConv\nReparameterization\n\ud835\udc7f\ud835\udc8f\n\ud835\udc7f\ud835\udc8f#\ud835\udfcf\n\ud835\udf50\ud835\udf3d(\ud835\udc7f\ud835\udc8f, \ud835\udc8f)\n\ud835\udc17\")#$\n\ud835\udc77\ud835\udc6c[\ud835\udc8f]\n\ud835\udc07)\n\ud835\udc0f$\n)\n\ud835\udc0f*\n)\n\ud835\udc17\")\n\ud835\udc0f)\n\ud835\udc19)\nFig. 4. The architecture of the denoising network, which is stacked by L\nresidual dilation convolution layers (RDCLs).\ndirectly calculating the mean. Therefore, we re-parameterize\nthe mean \u00b5\u03b8(\u00b7) and P\n\u03b8(\u00b7) as follows.\n\u00b5\u03b8(Xn, n) =\n1\n\u221a\u03b1n\n(Xn \u2212\n\u03b2n\n\u221a1 \u2212\u00af\u03b1n\n\u03f5\u03b8(Xn, n))\nP\n\u03b8(Xn, n) = 1 \u2212\u00af\u03b1n\u22121\n1 \u2212\u00af\u03b1n\n\u03b2n,\n(9)\nwhere \u03f5\u03b8(\u00b7) is the denoising network that is used to estimate\nthe amount of noise to be removed at the current step. And\n\u03f5\u03b8(Xn, n) is the estimated noise.\nThe complete reverse denoising process to infer a vector-\nized RNTraj can be formulated as follows:\np(X0|XN) = p(XN)\n1\nY\nn=N\np(Xn\u22121|Xn),\n(10)\nSince XN follows a standard Gaussian distribution, as\ndefined in Equation 6, we can directly get a clean vectorized\nRNTraj \u02c6X0 in the continuous space from a random Gaussian\nnoise by the reverse denoising process of N steps.\n4.2.3\nDenoising Network\nThe denoising network \u03f5\u03b8(\u00b7) in Equation 9 takes the noisy\nvectorized RNTraj Xn and the step n as input to predict\nthe added noise, which can be used to calculate Xn\u22121.\nGenerally, the denoising network is designed as a Wavenet-\nbased architecture for sequence-related tasks [35], [54], [58],\nusing convolution operation to capture the temporal depen-\ndence of sequences. Following DiffWave [35], we utilize the\nresidual dilation convolutional layer as the basic component\nof the denoising network to capture temporal correlations of\ntrajectory data and expand the receptive field.\nSpecifically, the framework of the denoising network is\nshown in Figure 4. At the n-th step of the denoising process,\nwe first implement the positional encoding proposed in\nDiffWare [35] to project the step n into a F-demensional\nvector PE[n] \u2208RF :\nPE[n] = [sin(10\n0\u22174\nF/2 n), sin(10\n1\u22174\nF/2 n), \u00b7 \u00b7 \u00b7 , sin(10\n(F/2\u22121)\u22174\nF/2\nn),\ncos(10\n0\u22174\nF/2 n), cos(10\n1\u22174\nF/2 n), \u00b7 \u00b7 \u00b7 , cos(10\n(F/2\u22121)\u22174\nF/2\nn)],\n(11)\nand we implement two fully connected layers on the vector.\nTo capture the temporal correlation within trajectory data,\nwe stack L Residual Dilation Convolution Layers (RDCL).\nThese layers are grouped into L\nm blocks, each consisting of\nm layers.\nThe input to the l-th RDCL consists of two parts, in-\ncluding the output Xl\u22121\nn\n\u2208RT \u00d7C of the (l \u22121)-th layer,\nand the positional encoding of n. For the first RDCL, X0\nn is\ntransformed from sample Xn. Formally,\nZl = Xl\u22121\nn\n+ FC(PE[n]) \u2208RT \u00d7C\nFC(\u00b7) : RF \u2192RC,\n(12)\nwhere C represents the number of channels in RDCL. Then,\nwe input it into a dilation convolution to capture the tem-\nporal dependencies at various scales by setting the dilation\nrate d = 2l%m. The dilation rate doubles at each layer within\na block, i.e., [1, 2, . . . , 2m\u22121]. Subsequently, the output of the\ndilation convolution Hl \u2208RT \u00d72C is fed into a gate unit to\nfilter out useful information:\nPl = \u03c3(Hl) \u2299tanh(Hl) \u2208RT \u00d72C,\n(13)\nwhere \u03c3(\u00b7) denotes the sigmoid function and \u2299is element-\nwise multiplication. Pl is further split into Pl\n1 and Pl\n2 along\nthe channel dimension:\nPl\n1 = Pl[:, 0 : C] \u2208RT \u00d7C\nPl\n2 = Pl[:, C : 2C] \u2208RT \u00d7C\n(14)\nWe take Pl\n1 and the input of current layer Xl\u22121\nn\nthrough\nresidual connections as the output of l-th layer, i.e., Xl\nn =\nXl\u22121\nn\n+Conv(Pl\n1). Finally, we sum Pl\n2 of each layer to output\nthe estimated noise, i.e.,\n\u03f5\u03b8(Xn, n) = Conv(PL\ni=1 Conv(Pl\n2)) \u2208RT \u00d7(D+1)\n(15)\nOnce we get the estimated noise, we can re-parameterize\nXn\u22121 by:\nXn\u22121 =\n1\n\u221a\u03b1n\n(Xn \u2212\n\u03b2n\n\u221a1 \u2212\u00af\u03b1n\n\u03f5\u03b8(Xn, n))\n+\ns\n1 \u2212\u00af\u03b1n\u22121\n1 \u2212\u00af\u03b1n\n\u03b2n\n(16)\n4.3\nRNTraj Decoder\nIn order to convert the sampled vectorized RNTraj \u02c6X0 \u2208\nRT \u00d7(D+1) into discrete road segments \u02c6e and continuous\nmoving ratio \u02c6r, we split \u02c6X0 along the channel dimensions to\nobtain the road segment representation \u02c6Xe and the moving\nratio \u02c6r:\n\u02c6Xe = \u02c6X0[:, 0 : D] \u2208RT \u00d7D\n\u02c6r =\n\u02c6X0[:, D + 1] + 1\n2\n\u2208RT \u00d71\n(17)\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n7\nAlgorithm 2 Sampling of Diff-RNTraj\nInput: Pre-trained road segment representations E, trained\ndenoising network \u03f5\u03b8(\u00b7), the length of RNTraj T, the total of\ndiffusion steps N;\nOutput: A generated RNTraj;\n1: Sample XN \u223cN(0, I) where XN \u2208RT \u00d7(D+1);\n2: for n = (N \u22121) to 0 do\n3:\nCalculate Xn \u2208RT \u00d7(D+1) using Equation 16;\n4: end for\n5: Split generated \u02c6X0 along the channel dimension, obtain\nRNTraj road segment representation \u02c6Xe \u2208RT \u00d7D and\nmoving ratio \u02c6r according to Equation 17;\n6: Calculate cosine similarity between E and \u02c6Xe use Equa-\ntion 18, and use argmax function to obtain generate road\nsegment sequence \u02c6e1, \u02c6e2, \u00b7 \u00b7 \u00b7 , \u02c6eT ;\n7: Obtain the GPS coordinate of RNTraj according to Algo-\nthrim 1.\nTo determine the road segment \u02c6e, we first calculate\nthe cosine similarity between the pre-trained road segment\nrepresentations E and \u02c6Xe,\nS =\n\u02c6Xe \u00b7 E\u22a4\n\r\r\r \u02c6Xe\n\r\r\r\n2 \u00b7 \u2225E\u22252\n\u2208RT \u00d7|R|,\n(18)\nThen, we use argmax function to obtain road segment \u02c6et of\nt-th RNTraj point, i.e., \u02c6et = argmax\ni\nS[t, i].\nThe generation process of RNTraj can be found in Algo-\nrithm 2. Next, we will detail how to train the parameters \u03b8\nof \u03f5\u03b8(\u00b7) in the next section.\n4.4\nModel Training\nTo train the parameters \u03b8, DDPM employs the following\nloss function to minimize the error between the added noise\n\u03f5 and the estimated noise.\nL1(\u03b8) = En,X0,\u03f5||\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1nX0 +\n\u221a\n1 \u2212\u00af\u03b1n\u03f5, n)||2\n(19)\nHowever, this loss function only considers the added\nnoise for guiding model training, and fails to take the\nstructure information of RNTraj into consideration, which\nled to the generated RNTraj being invalid. Inspire by [53],\nat each step of the reverse denoising process, we further use\nthe estimated vectorized RNTraj \u02c6X0 based on the estimated\nnoise to fit the real vectorized RNTraj X0, since X0 contains\nthe structure of the sample. Formally,\nL2 = ||X0 \u2212\u02c6X0||2,\n(20)\nwhere \u02c6X0 = (Xn \u2212\u221a1 \u2212\u00af\u03b1n\u03f5\u03b8(Xn, n))/\u221a\u00af\u03b1n, as described\nin Equation 5.\nFurthermore, in order to consider sequence dependency\nand enhance the spatial validity of the generated trajecto-\nries, we propose a novel spatial validity loss function that\nexplicitly models the connectivity of generated road seg-\nments. Based on the estimated \u02c6X0, we reconstruct RNTraj\nfollowing Section 4.3 and obtain a generated road segment\nsequence \u27e8\u02c6e1, \u02c6e2, \u00b7 \u00b7 \u00b7 , \u02c6eT \u27e9. To ensure that the adjacent road\nsegments are connected in the UTGraph G\u03c4, we penalize\ncases where they are not connected, as these two road\nAlgorithm 3 Training of Diff-RNTraj\nInput: Pre-trained road segment representation E, the distri-\nbution of train dataset, the total forward diffusion step N,\nthe variable schedule {\u03b21, \u00b7 \u00b7 \u00b7 , \u03b2N}, UTGraph G\u03c4 ;\nOutput: Trained denoising network \u03f5\u03b8(\u00b7);\n1: while model not converged do\n2:\nSample a RNTraj, calculate the corresponding vector-\nized RNTraj X0 in continuous space;\n3:\nSample n \u223cUniform({1, \u00b7 \u00b7 \u00b7 , N}), \u03f5 \u223cN(0, I);\n4:\nCalculate Xn according to Equation 5;\n5:\nCalculate the estimated \u03f5\u03b8(Xn, n),\n\u02c6X0, and recon-\nstruct RNTraj;\n6:\nCalculate the loss L = L1 + L2 + L3;\n7:\nTake the gradient step;\n8: end while\nsegments would be unreachable. Hence, our loss function\nis defined as:\nL3 =\nT \u22121\nX\ni=1\n(1 \u22121{G\u03c4(\u02c6ei, \u02c6ei+1)}),\n(21)\nwhere 1{\u00b7} is an indicator function that 1{G\u03c4(\u02c6ei, \u02c6ei+1)} = 1\nif the weight of \u02c6ei and \u02c6ei+1 in G\u03c4 not equals 0; Otherwise,\n1{G\u03c4(\u02c6ei, \u02c6ei+1)} = 0.\nOverall, our total loss function is defined as:\nL = L1 + L2 + L3\n(22)\nThe detailed training algorithm is presented in Algorithm 3.\n5\nEXPERIMENTS\nIn this section, we conduct extensive experiments to evalu-\nate the quality of the generated trajectory by the proposed\nmodel Diff-RNTraj on two real-world trajectory datasets and\ncompare it with other baselines.\nTABLE 1\nDataset description.\nTypes\nPorto\nChengdu\nLatitude\n(41.1405, 41.1865)\n(30.6550, 30.7291)\nLongtitude\n(-8.6887, -8.5557)\n(104.0393, 104.1271)\n# Trajectory\n537,681\n527,353\n# Road segment\n13695\n6256\n# Time interval\n15s\n30s\nMean trajectory length\n41.50\n29.81\n5.1\nDataset\nIn experiments, we use two publicly available trajectory\ndatasets from Porto, Portugal, and Chengdu, China. The\nPorto dataset1 contains trajectory data of 442 taxis from\nJanuary 2013 to June 2014. The Chengdu dataset2 records\ntaxi trajectory data during October 2016. We download the\nroad network from OpenStreetMap3 using osmnx tools.\n1. https://www.kaggle.com/competitions/\npkdd-15-predict-taxi-service-trajectory-i/data\n2. https://outreach.didichuxing.com/\n3. http://www.openstreetmap.org/\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n8\nWe remove the stationary points and filter out trajectories\nthat traveled less than 5 minutes or more than 30 minutes.\nThen, we apply the map matching algorithm [23] to project\nGPS points onto the road network and obtain the ground\ntruth of RNTraj. Table 1 provides statistical information\nof datasets after pre-processing. Since we used datasets\nwith fixed time intervals between adjacent GPS points, and\nour generated trajectories also obey the corresponding time\npatterns, which is helpful for further analysis of trajectory\nbehavior, e.g., varying speeds and acceleration patterns.\n5.2\nEvaluated Metrics\nAs our goal is to generate a realistic synthetic trajectory\ndataset that can benefit various trajectory mining tasks.\nHence, it is essential to evaluate the \u201csimilarity\u201d between\nthe generated trajectory dataset and the real dataset from a\nmacro perspective. Following [17], [45], we use the Jensen-\nShannon Divergence (JSD) to quantify the disparity between\nthem. For two distributions p and q, the JSD between them\ncan be calculated by:\nJSD(p, q) = 1\n2KL(p||p + q\n2\n) + 1\n2KL(q||p + q\n2\n),\n(23)\nwhere KL(\u00b7||\u00b7) is the Kullback-Leibler divergence [59]. A\nsmaller JSD value indicates a higher similarity between\ndistributions p and q. We assess the quality of the gener-\nated trajectories by calculating the JSD with the following\nmetrics:\n\u2022\nTrajectory Total Distance (JSD-TD) is used to calcu-\nlate the distribution of the total distance of a trajec-\ntory.\n\u2022\nTrajectory Segment Distance (JSD-SD) is used\nto evaluate the travel distance in trajectory seg-\nments, i.e., the length between two adjacent trajectory\npoints.\n\u2022\nGrid Point Density (JSD-GPD) measures the distri-\nbution of all trajectory points. Specifically, we divide\nthe map into multiple grids with grid size 50m \u00d7\n50m, and we count the probability that the trajectory\npoint is located in each grid.\n\u2022\nRoad Segment (JSD-RS) quantifies the road segment\ndistribution of our generated trajectory.\nBesides evaluating the overall trajectory distribution, we\ndesign a novel metric, called Road Segment Connectiv-\nity (RSC), to assess the spatial validity of the generated\ntrajectories. RSC measures the reachable proportion of ad-\njacent points within generated trajectories. We determine\nthe road segment connectivity by UTGraph G\u03c4 since it\nis reflected by the real vehicle trajectory. For a generated\nRNTraj \u02c6\u03c4RN = \u27e8\u02c6q1, \u00b7 \u00b7 \u00b7 , \u02c6qT \u27e9, its RSC is calculated as follows:\nRSC =\nPT \u22121\nt=1 1{G\u03c4(\u02c6qt.e, \u02c6qt+1.e)}\nT \u22121\n\u00d7 100%\n(24)\nA higher RSC value signifies a higher reasonableness in\ngenerated trajectories.\n5.3\nBaselines\nWe compare our proposed model with the following base-\nlines:\n\u2022\nRandom Walk on the Road Network (RWRN).\nRWRN randomly selects a point as the origin, then\nperforms the random walk on the road network\naccording to a random speed and direction.\n\u2022\nMarkov [60]. Markov treats the road segments as\nstates and constructs a transition matrix to capture\nthe first-order transition probabilities between these\nroad segments.\n\u2022\nTrajVAE [10] + Hidden Markov Model (HMM) [23].\nTrajVAE learns the trajectory representation using\nLSTM and VAE, and reconstructs the GPS points\nthrough a decoder. Then, a map-matching algothrim\nbased on the HMM is implemented to project GPS\npoints into the road network.\n\u2022\nTrajGAN [42] + HMM. TrajGAN employs an LSTM\nnetwork in its generator to generate GPS points step\nby step, which is trained using the policy gradient\nalgorithm. Subsequently, these GPS points are pro-\njected onto the road network based on HMM.\n\u2022\nEETG [8] + HMM [23]. EETG is a VAE-like tra-\njectory generate method, which combines VAE and\nVRNN [61] to encode trajectory, and generate trajec-\ntory based on LSTM. Then, HMM is introduced to\nobtain the road network-constrained trajectory.\nIn addition, we design the following variants to demonstrate\nthe effectiveness of the components in the proposed model.\n\u2022\nDiff-Bit. We utilize the binary encoding to represent\nthe discrete road segment [55] and concatenate the\nmoving ratio to vectorize hybrid RNTraj. During\nthe sampling stages, we decode the discrete road\nsegment by threshold operations.\n\u2022\nDiff-RNTraj / Pretrain. Instead of using pre-trained\nmethods, we randomly initialize the representation\nfor each road segment and optimize it together with\nDiff-RNTraj.\n\u2022\nDiff-Transformer. We use the Transformer in the\ndenoising network to capture the temporal corre-\nlation of the noisy vectorized RNTraj. Compared\nwith our residual dilation convolution layers, Diff-\nTransformer considers the global correlation between\nall trajectory points.\n\u2022\nDiff-LSTM. We replace the residual dilation con-\nvolution layers in the denoising network with two\nLSTM layers, which only capture the temporal de-\npendence among previous trajectory points.\n\u2022\nDiff-RNTraj / SL. We remove the spatial vality loss\nfunction, and only use L1 loss to train Diff-RNTraj.\n5.4\nSetting\nWe implement Diff-RNTraj4 by PyTorch framework with the\nAdam optimizer for 30 epochs and a batch size of 256. The\nlearning rate is set to 1e-3 and is halved every 3 epochs.\nFor the denoising network, we set the channel dimensional\nC = 512, the positional encoding dimensional F = 512,\nthe layer of each block m = 4 and the kernel size to\n3 in dilation convolution with padding followed by [35].\nWe adopt the quadratic schedule for the variance schedule:\n4. https://github.com/wtl52656/Diff-RNTraj\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n9\nTABLE 2\nPerformance comparison of our model and baselines on two real-world trajectory datasets.\nThe best results are highlighted in bold, while the underline indicates the second-best results. \u2020 represents our model.\nMethods\nPorto\nChengdu\nJSD-TD\nJSD-SD\nJSD-GPD\nJSD-RS\nRSC\nJSD-TD\nJSD-SD\nJSD-GPD\nJSD-RS\nRSC\nRWRN\n0.6058\n0.0149\n0.1724\n0.9206\n100%\n0.4267\n0.0023\n0.5819\n0.9108\n100%\nMarkov\n0.5650\n0.0122\n0.1066\n0.6143\n100%\n0.3269\n0.0019\n0.1539\n0.4433\n100%\nTrajVAE + HMM\n0.3399\n0.0093\n0.0791\n0.7251\n13.30%\n0.3925\n0.0026\n0.1666\n0.5662\n6.95%\nTrajGAN + HMM\n0.3876\n0.0123\n0.0885\n0.7520\n30.07%\n0.4602\n0.0029\n0.1902\n0.8868\n6.43%\nEETG + HMM\n0.1956\n0.0014\n0.0963\n0.8652\n23.83%\n0.1773\n0.0015\n0.1489\n0.5170\n18.09%\nDiff-Bit\n0.1277\n0.0025\n0.0377\n0.0868\n27.63%\n0.1397\n0.0021\n0.1028\n0.0591\n37.34%\nDiff-RNTraj / PreTrain\n0.0975\n0.0015\n0.0449\n0.2083\n25.40%\n0.1279\n0.0018\n0.0891\n0.2544\n48.87%\nDiff-LSTM\n0.0512\n0.0013\n0.0372\n0.1396\n56.68%\n0.1381\n0.0018\n0.0934\n0.0819\n71.41%\nDiff-Transformer\n0.0579\n0.0016\n0.0356\n0.0804\n85.86%\n0.1247\n0.0012\n0.0946\n0.0544\n86.75%\nDiff-RNTraj / SL\n0.0483\n0.0008\n0.0300\n0.0614\n79.68%\n0.1658\n0.0015\n0.0952\n0.0467\n87.79%\nDiff-RNTraj\u2020\n0.0221\n0.0007\n0.0281\n0.0456\n91.01%\n0.1165\n0.0009\n0.0856\n0.0399\n92.80%\n\u03b2n = ( N\u2212n\nN\u22121\n\u221a\u03b21+ n\u22121\nN\u22121\n\u221a\u03b2N)2 with the minimum noise level\n\u03b21 = 0.0001 and the maximum noise level \u03b2N = 0.02. We\nsearch the number of the diffusion steps N and residual\ndilation convolution layers L within the parameter space\n(N \u2208{50, 100, 200, 500, 1000}, L \u2208{4, 8, 10, 15, 20}). To\nobtain the pre-trained road segment representation E using\nthe Node2vec algorithm, we generate 100 random paths for\neach node, each with a length of 80. We set the contextual\nwindow size of 10, perform a total of 1000 iterations, and\nsearch the number of embedding dimensions D across the\nparameter space {16, 32, 64, 128}.\nConsidering that our denoising network can only handle\nfixed-length shapes in a batch, we initially group trajectories\naccording to their length, and select trajectories of the same\nlength within each batch to train the model. During the\nsampling stage, we generate an equal number of trajectories\nat each length as the original dataset.\n5.5\nExperiments Results\nWe use Diff-RNTraj and several baseline models to generate\nan equal amount of trajectory data as the original dataset,\nand we compare the distributions of the generated and\noriginal trajectory data across JSD-TD, JSD-SD, JSD-GPD,\nJSD-RS, and RSC metrics. As depicted in Table 2, we have\nthe following observations:\nFirstly, rule-based methods RWRN and Markov can ef-\nfectively maintain the spatial validity of trajectories. How-\never, the generated trajectory data struggle to capture real\nmovement patterns, resulting in significant disparities com-\npared to the characteristics of the original trajectories. This\nindicates that describing complex human travel patterns\nusing simple rules is challenging.\nSecondly, the two-stage approaches that first generate\nGPS points and then perform map matching, show perfor-\nmance improvement compared with rule-based methods.\nNonetheless, they still display dissimilarities in character-\nistics compared to real trajectory data. Furthermore, these\nmethods face difficulties in reliably mapping all GPS points\nto the road network, achieving only around 82.13% and\n60.85% in the Porto and Chengdu datasets, making it chal-\nlenging for further utilization.\nOur Diff-RNTraj achieves the best results across all met-\nrics, with an average improvement of 55.05% on the Porto\ndataset and 33.12% on the Chengdu dataset. Diff-RNTraj\nnotably improves the overall distribution of trajectory data,\nindicating the effectiveness of the diffusion model in gen-\nerating trajectories. Additionally, Diff-RNTraj achieves RSC\nscores surpassing 91%, demonstrating that our generated\ntrajectory is valid.\n5.6\nAblation Study\nTo explore the effectiveness of different components in our\nDiff-RNTraj, we conduct five ablation experiments on the\nPorto and Chengdu datasets. These experiments involve:\nutilizing binary encoding for discrete roads (Diff-Bit), re-\nmoving the pre-trained strategy for road segment repre-\nsentation learning (Diff-RNTraj / Pretrain), replacing resid-\nual dilation convolution layers with a transformer (Diff-\nTransformer) and LSTM (Diff-LSTM), and omitting spatial\nvalidity loss (Diff-RNTraj / SL).\nThe experimental results are shown in Table 2. We\nobserve that when removing the pre-trained strategy, the\nquality of the generated trajectory decreases, indicating that\nit is helpful to capture the correlation between road seg-\nments. Diff-Bit has a low RSC, indicating that using binary-\ncoded road segments is inefficient. It easily compromises\nthe spatial validity of trajectories since a single bit error can\ndisrupt the entire trajectory. The removal of spatial validity\nloss led to a decrease in RSC metrics on both the Porto\ndataset (from 91.01% to 79.68%) and the Chengdu dataset\n(from 92.80% to 87.79%). This indicates that our spatial\nvalidity loss effectively improves the quality of generated\ntrajectories, making them more similar to real trajectories.\nThe performance is decreased when replacing the residual\ndilation convolution layer in the denoising network with the\nTransformer or LSTM. This is because the Diff-Transformer\nconsiders the correlation of all trajectory points simultane-\nously, which introduces additional noise from potentially\nunrelated distant GPS points, yet Diff-LSTM fails to cap-\nture the temporal correlation of future trajectory points. In\ncontrast, the residual dilation convolution layer enables a\nfocus on temporal relationships among nearby trajectory\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n10\n16\n32\n64\n128\nD\n0.00\n0.05\n0.10\n0.15\nJSD-TD\nJSD-TD\nJSD-GPD\n0.00\n0.05\n0.10\n0.15\nJSD-GPD\nPorto\n16\n32\n64\n128\nD\n0.10\n0.15\n0.20\n0.25\nJSD-TD\nJSD-TD\nJSD-GPD\n0.05\n0.10\n0.15\nJSD-GPD\nChengdu\nFig. 5. Hyperparameter analysis of embedding dimensional D of the\npre-trained road segment representation.\npoints, which facilitates improved learning of trajectory\ncorrelations.\n5.7\nHyperparameter Analysis\nWe further explore the impact of various hyperparameters\nin Diff-RNTraj on the Porto and Chengdu datasets. These\nhyperparameters include the number of hidden dimensions\nD in the pre-trained road segment representation, the total\ndiffusion steps N, and the number of residual dilation\nconvolution layers L.\n\u2022\nPre-trained road segment representation dimension\nD. We vary D within {16, 32, 64, 128} to analyze\nits impact on trajectory generation. As illustrated\nin Figure 5, we observe that as D increases, the\nperformance of Diff-RNTraj improves. This suggests\nthat a higher-dimensional space is more effective in\ncapturing road segment information. When D sur-\npasses 64, the performance gains become marginal.\nTherefore, we choose D = 64 for the Porto and\nChengdu datasets.\n\u2022\nDiffusion steps N. As shown in Figure 6, we set N to\n50, 100, 200, 500, and 1000. We observe that the qual-\nity of trajectory generation improves as N increases.\nWhen N is greater than 500, the performance gains\nbecome less. Therefore, we set N = 500 to balance\neffectiveness and efficiency.\n\u2022\nThe number of residual dilation convolution layers\nL in the denoising network. We conduct experiments\nusing various values of L, including 4, 8, 10, 15, and\n20, to examine how the number of residual dilation\nconvolution layers affects trajectory generation. As\ndepicted in Figure 7, an increase in L enables trajec-\ntory points to better capture correlations with neigh-\nboring points, progressively aligning the generated\ndistribution with the actual distribution. However,\nwhen L exceeds 10, the generated trajectories deviate\nfrom the real distribution. This divergence is caused\nby the excessive layer stacking in Diff-RNTraj, which\nleads each trajectory point to incorporate information\nfrom distant points, resulting in noise and reduced\nperformance.\n5.8\nEffectiveness Analysis of the Generated Road Seg-\nment Representation\nTo assess the effectiveness of our model in generating road\nsegments, we conduct an experiment to check whether our\ngenerated road segment representations can be aligned with\npre-trained representations. Specifically, we generate 2,000\n50\n100\n200\n500\n1000\nN\n0.00\n0.05\n0.10\n0.15\nJSD-TD\nJSD-TD\nJSD-GPD\n0.00\n0.05\n0.10\n0.15\nJSD-GPD\nPorto\n50\n100\n200\n500\n1000\nN\n0.10\n0.15\n0.20\n0.25\nJSD-TD\nJSD-TD\nJSD-GPD\n0.05\n0.10\n0.15\nJSD-GPD\nChengdu\nFig. 6. Hyperparameter analysis of number of diffusion steps N.\n4\n8\n10\n15\n20\nL\n0.00\n0.05\n0.10\n0.15\nJSD-TD\nJSD-TD\nJSD-GPD\n0.00\n0.05\n0.10\n0.15\nJSD-GPD\nPorto\n4\n8\n10\n15\n20\nL\n0.10\n0.15\n0.20\n0.25\nJSD-TD\nJSD-TD\nJSD-GPD\n0.05\n0.10\n0.15\nJSD-GPD\nChengdu\nFig. 7. Hyperparameter analysis of layer of residual dilation convolution\nlayers L in the denoising network.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTop-K\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nCosine Similarity\n0.9880\n0.9489\n0.9302\n0.9139\n0.8998\n0.8869\n0.8763\n0.8669\n0.8591\n0.8519\nPorto\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nTop-K\n0.84\n0.86\n0.88\n0.90\n0.92\n0.94\n0.96\n0.98\n1.00\nCosine Similarity\n0.9967\n0.9502\n0.9289\n0.9114\n0.8954\n0.8808\n0.8693\n0.8595\n0.8506\n0.8419\nChengdu\nFig. 8. Average cosine similarity of the generated and pre-trained road\nsegment representation at different top-k.\ntrajectories with a length of 20 by N steps reverse denoising\nprocess, i.e., we obtain the road segment representations \u02c6Xe\nfor the 40,000 trajectory points. For each generated road seg-\nment representation, we calculate its cosine similarity with\nthe pre-trained road segment representations and record the\ntop 10 most similar road segments. Then, we calculate the\naverage similarity across all trajectory points for different\ntop-k similarity values (k = 1, \u00b7 \u00b7 \u00b7 , 10).\nAs shown in Figure 8, we observe that it achieves 0.9967\nsimilarity score in the top-1 road segment in the Chengdu\ndataset. Even in the Porto dataset, which contains 13,696\nroad segments, the similarity score remains notably high at\n0.9880. In contrast, top-2 \u223ctop-10 road segments exhibit\nlower similarity, which makes the similar calculation and\nargmax easy and robust.\n5.9\nVisualization Analysis\nTo intuitively observe the quality of the generated trajec-\ntories, we conduct two visualization experiments, including\nthe trajectory point distributions visualization and the single\ntrajectory visualization.\n5.9.1\nTrajectory Point Distribution Visualization\nIn Figures 9 and 10, we generate an equal number of\ntrajectory points as the original dataset and visualize their\navailability using the probability density function (PDF). By\ncomparing the heatmaps generated by our Diff-RNTraj, Diff-\nBit, EETG+HMM, and real trajectory points, we observe\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n11\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) The real trajectories.\nJSD-GPD = 0.0281\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) The trajectories generated by\nour model.\nJSD-GPD = 0.0377\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) The trajectories generated by\nDiff-Bit.\nJSD-GPD = 0.0963\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) The trajectories generated by\nEETG+HMM.\nFig. 9. Heatmaps of trajectories in Porto.\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) The real trajectories.\nJSD-GPD = 0.0856\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) The trajectories generated by\nour model.\nJSD-GPD = 0.1028\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(c) The trajectories generated by\nDiff-Bit.\nJSD-GPD = 0.1489\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(d) The trajectories generated by\nEETG+HMM.\nFig. 10. Heatmaps of trajectories in Chengdu.\nthat our model accurately captures the distribution of the\ntrajectory points and reflects the popularity of different\nroads.\n5.9.2\nTrajectory Visualization\nWe randomly generate trajectories of different lengths (T\n= 20, 30, 40, and 50) and plot the GPS points on the road\nnetwork in Figure 11. We observe that these trajectories\nexhibit patterns consistent with common driving behaviors\nand show a preference for major roads. Additionally, the\nvehicles consistently move forward along the roads without\nany instances of reversing, thus validating that using RNTraj\nto represent trajectory is suitable.\n5.9.3\nTrajectory Diversity Visualization\nTo explore the diversity of the generated trajectories, we\nrandomly select three origin-destination pairs from the\nChengdu dataset and plot the generated. As shown in Fig-\nure 12, our model generated multiple paths for each origin-\ndestination pair, ensuring a level of diversity and avoiding\noverly deterministic movement patterns. The majority of\nthese trajectories followed the major roads, with transitions\nFig. 11. Visualizations of generated trajectories with lengths 20, 30, 40,\nand 50 in Chengdu.\nFig. 12. Visualization of trajectory diversity between the same origin and\ndestination regions in Chengdu.\nto alternative routes as they approached the destination,\nreflecting common driving behavior.\nHowever, it is challenging to determine whether certain\ngenerated trajectories are abnormal, such as whether the\nbrown trajectory in the middle of Figure 12 represents\na detour. This limitation is primarily due to the model\u2019s\nemphasis on modeling the overall distribution and its lack\nof consideration for conditional information guidance.\n5.10\nScability Analysis\nIn practical situations, collected trajectory data is usually\nsmall-scale. To evaluate the model\u2019s ability to generate\ntrajectories with limited data, we conduct scalability ex-\nperiments on two datasets. Diff-RNTraj is trained using\n25%, 50%, and 75% of the trajectory data, generating an\nequivalent amount of trajectory to the original dataset. The\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n12\nTABLE 3\nScalability analysis. The similarity between generated and real trajectory data when using different ratio data to train Diff-RNTraj.\nDataset ratio\nPorto\nChengdu\nJSD-TD\nJSD-SD\nJSD-GPD\nJSD-RS\nRSC\nJSD-TD\nJSD-SD\nJSD-GPD\nJSD-RS\nRSC\n25%\n0.0313\n0.0009\n0.0420\n0.0773\n89.87%\n0.1317\n0.0012\n0.0930\n0.0714\n90.05%\n50%\n0.0225\n0.0007\n0.0407\n0.0536\n90.26%\n0.1292\n0.0011\n0.0925\n0.0549\n91.86%\n75%\n0.0222\n0.0007\n0.0366\n0.0479\n90.82%\n0.1174\n0.0010\n0.0890\n0.0456\n92.20%\n100%\n0.0221\n0.0007\n0.0281\n0.0456\n91.01%\n0.1165\n0.0009\n0.0856\n0.0399\n92.80%\n0%\n1%\n2%\n5%\n10%\n20%\n50%\n100%\nThe ratio of real trajectory\n0\n10\n20\n30\n40\n50\n60\n70\nAcc (%)\n 0.38\n 33.14 39.72\n 25.90  36.21 40.15\n 32.31 38.59\n 41.43\n 40.93\n 41.39\n 43.21\n 46.08\n 46.41\n 47.11\n 48.78\n 49.94\n 50.05\n 51.60\n 53.01\n 53.86\n 55.26\n 55.26\n 55.26\nPorto\nEETG\nDiff-Bit\nDiff-RNTraj\nFig. 13. The performance of trajectory prediction on the Porto dataset\nby mixing real and generated trajectories.\n0%\n1%\n2%\n5%\n10%\n20%\n50%\n100%\nThe ratio of real trajectory\n0\n10\n20\n30\n40\n50\n60\n70\nAcc (%)\n 1.03\n 37.40 41.04\n 19.66\n 40.73 49.08\n 34.25  43.50 49.51\n 44.41 49.95\n 50.41\n 49.03\n 51.03\n 51.60\n 52.88\n 53.08\n 53.38\n 54.29\n 56.20\n 56.77\n 57.83\n 57.83\n 57.83\nChengdu\nEETG\nDiff-Bit\nDiff-RNTraj\nFig. 14. The performance of trajectory prediction on the Chengdu\ndataset by mixing real and generated trajectories.\ngenerated results are reported in the Table 3. We observe\nthat even when trained on just 25% of the data, Diff-RNTraj\ngenerates trajectories that closely resemble real data. This\ndemonstrates the effectiveness of our model in scenarios\ninvolving small-scale trajectory data, which is more useful\nin practice.\n5.11\nDownstream Task\nIn this experiment, we take trajectory prediction as the\ndownstream task to evaluate the quality of the generated\ntrajectory. Specifically, we aim to predict the next road\nsegment, given the historical road segment sequence. We\ndivide the real trajectory dataset into training, validation,\nand testing sets with a ratio of 6:2:2. In the training set, we\nkeep the overall number of trajectories constant, gradually\ndecrease the ratio of the real trajectories, increase the ratio of\nthe generated trajectories, and train the trajectory prediction\nmodel. As the ratio of real trajectories decreases, if the\nprediction performance change is smaller at the test set, it\nproves that the spatial transfer patterns of the generated tra-\njectories are closer to the real trajectories, and the generated\ntrajectories are more realistic.\nSpecifically, we choose LSTM as the trajectory prediction\nmodel and vary the proportion of real data in the training set\nTABLE 4\nComputation Cost. NA means the model does not require\nmap-matching.\nDataset\nChengdu / Porto\nMethods\nParameters\nTraining Time\nInference Time (seconds / batch)\n(million)\n(seconds / epoch)\nGeneration\nHMM\nTotal\nTrajVAE + HMM\n12.03\n217.57 / 269.56\n2.79 / 2.92\n128.41 / 182.58\n131.20 / 185.50\nTrajGAN + HMM\n0.36\n231.35 / 240.97\n2.46 / 3.14\n128.41 / 182.58\n130.87 / 185.72\nEETG + HMM\n8.29\n327.94 / 404.21\n6.04 / 6.37\n128.41 / 182.58\n134.45 / 188.95\nDiff-RNTraj\n27.18\n194.53 / 226.97\n13.07 / 13.19\nNA\n13.07 / 13.19\nto 0%, 1%, 2%, 5%, 10%, 20%, 50%, and 100%. The trajectory\nprediction results of different generated models are shown\nin Figure 13 and 14. We observe that Diff-RNTraj demon-\nstrates superior trajectory prediction performance compared\nto EETG and Diff-Bit, proving that the real trajectories can\nbe replaced by our generated trajectories. It is worth noting\nthat when the proportion of real data is less than 10%, or\neven completely relying on generated results (i.e., with 0%\nreal data), our model still achieves satisfactory performance,\nwith an average improvement of 3.18% and 3.81% on the\nPorto and Chengdu datasets compared to Diff-Bit. This\nis because Diff-RNTraj incorporates the pre-trained road\nsegment representation module to model the user\u2019s road\ntransfer pattern, enabling the generation of realistic trajec-\ntories. This demonstrates the effectiveness of our model for\ntrajectory-based applications.\n5.12\nComputation Cost\nIn this section, we conduct computational cost analysis on\nthe Chengdu and Porto datasets using a batch size of 256\nwith the NVIDIA A40 card. As shown in Table 4, Diff-\nRNTraj has a larger number of parameters compared to the\nbaselines, as it stacks multiple RDCLs. Despite this, our\nmodel\u2019s training time is shorter than the baselines. This\nefficiency is due to RDCL\u2019s convolution-based structure,\nwhich allows for parallel processing. In contrast, TrajVAE,\nTrajGAN, and EETG, which are based on the LSTM archi-\ntecture, process trajectories sequentially.\nFor the inference phase, we calculate the time cost\nof each model to generate a batch of trajectories. When\ngenerating trajectories, our model\u2019s speed is slower than\nbaselines due to the requirement of 500 iterative denois-\ning steps. However, the baselines necessitate an additional\nHMM-based map-matching process after trajectory genera-\ntion, which adds considerable computational expense. This\nprocess has a computational complexity of O(TR2), where\nT represents the trajectory length and R is the number\nof road segments. Our model, on the other hand, does\nnot require the map-matching step. Consequently, when\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n13\ngenerating road network-constrained trajectories, the total\ntime of Diff-RNTraj still is the fastest.\n6\nCONCLUSION\nIn this paper, we propose a new problem that directly\ngenerates trajectories on the road network. To accomplish\nthis, we utilize a RNTraj of the hybrid format to represent\nthe trajectory and propose a new model called Diff-RNTraj\nfor generating it. Our approach involves first vectorizing\nthe RNTraj in the continuous space, which serves as the\ninput for the diffusion model. We then develop a RN-\nTraj decoder to map the generated representation from the\ndiffusion model back into RNTraj data. Furthermore, we\nenhance the validity of the trajectories by introducing a new\nspatial validity loss at each step of the reverse denoising\nprocess. Extensive experiments conducted on two datasets\ndemonstrate the superiority of our proposed model.\nIn real scenarios, the drive path of a vehicle usually\nshows different characteristics under changes in different\nobjective factors, such as traffic conditions and departure\ntime. In the future, we plan to study more specific trajectory\ngeneration tasks, e.g., using the different objective factors as\nconditions to generate trajectories under different patterns.\nREFERENCES\n[1]\nK. Zhao, D. Khryashchev, and H. Vo, \u201cPredicting taxi and uber\ndemand in cities: Approaching the limit of predictability,\u201d IEEE\nTransactions on Knowledge and Data Engineering, vol. 33, no. 6, pp.\n2723\u20132736, 2019.\n[2]\nY. Wang, H. Yin, T. Chen, C. Liu, B. Wang, T. Wo, and J. Xu,\n\u201cGallat: A spatiotemporal graph attention network for passenger\ndemand prediction,\u201d in 2021 IEEE 37th International Conference on\nData Engineering (ICDE).\nIEEE, 2021, pp. 2129\u20132134.\n[3]\nN. Zhang, L. Qin, P. Yu, W. Gao, and Y. Li, \u201cGrey-markov model\nof user demands prediction based on online reviews,\u201d Journal of\nEngineering Design, vol. 34, no. 7, pp. 487\u2013521, 2023.\n[4]\nR. R. Joshi, \u201cA new approach to map matching for in-vehicle\nnavigation systems: the rotational variation metric,\u201d in ITSC 2001.\n2001 IEEE Intelligent Transportation Systems. Proceedings (Cat. No.\n01TH8585).\nIEEE, 2001, pp. 33\u201338.\n[5]\nJ. Dai, B. Yang, C. Guo, and Z. Ding, \u201cPersonalized route recom-\nmendation using big trajectory data,\u201d in 2015 IEEE 31st interna-\ntional conference on data engineering.\nIEEE, 2015, pp. 543\u2013554.\n[6]\nY. Ge, H. Li, and A. Tuzhilin, \u201cRoute recommendations for intel-\nligent transportation services,\u201d IEEE Transactions on Knowledge and\nData Engineering, vol. 33, no. 3, pp. 1169\u20131182, 2019.\n[7]\nJ. Wang, N. Wu, and W. X. Zhao, \u201cPersonalized route recommen-\ndation with neural network enhanced search algorithm,\u201d IEEE\nTransactions on Knowledge and Data Engineering, vol. 34, no. 12, pp.\n5910\u20135924, 2021.\n[8]\nL. Zhang, L. Zhao, and D. Pfoser, \u201cFactorized deep generative\nmodels for end-to-end trajectory generation with spatiotemporal\nvalidity constraints,\u201d in Proceedings of the 30th International Confer-\nence on Advances in Geographic Information Systems, 2022, pp. 1\u201312.\n[9]\nX. Wang, X. Liu, Z. Lu, and H. Yang, \u201cLarge scale gps trajectory\ngeneration using map based on two stage gan,\u201d Journal of Data\nScience, vol. 19, no. 1, pp. 126\u2013141, 2021.\n[10] X. Chen, J. Xu, R. Zhou, W. Chen, J. Fang, and C. Liu, \u201cTrajvae:\nA variational autoencoder model for trajectory generation,\u201d Neu-\nrocomputing, vol. 428, pp. 332\u2013339, 2021.\n[11] S. Choi, J. Kim, and H. Yeo, \u201cTrajgail: Generating urban vehicle\ntrajectories using generative adversarial imitation learning,\u201d Trans-\nportation Research Part C: Emerging Technologies, vol. 128, p. 103091,\n2021.\n[12] R. Choe, J. Puig, V. Cichella, E. Xargay, and N. Hovakimyan, \u201cTra-\njectory generation using spatial pythagorean hodograph b\u00b4ezier\ncurves,\u201d in AIAA Guidance, Navigation, and Control Conference, 2015,\np. 0597.\n[13] Q. Gao, G. Trajcevski, F. Zhou, K. Zhang, T. Zhong, and F. Zhang,\n\u201cDeeptrip: Adversarially understanding human mobility for trip\nrecommendation,\u201d in Proceedings of the 27th ACM SIGSPATIAL\ninternational conference on advances in geographic information systems,\n2019, pp. 444\u2013447.\n[14] W. Jiang, W. X. Zhao, J. Wang, and J. Jiang, \u201cContinuous\ntrajectory generation based on two-stage gan,\u201d arXiv preprint\narXiv:2301.07103, 2023.\n[15] S. Isaacman, R. Becker, R. C\u00b4aceres, M. Martonosi, J. Rowland,\nA. Varshavsky, and W. Willinger, \u201cHuman mobility modeling at\nmetropolitan scales,\u201d in Proceedings of the 10th international confer-\nence on Mobile systems, applications, and services, 2012, pp. 239\u2013252.\n[16] P. Zhao, H. Jiang, J. Li, F. Zeng, X. Zhu, K. Xie, and G. Zhang, \u201cSyn-\nthesizing privacy preserving traces: Enhancing plausibility with\nsocial networks,\u201d IEEE/ACM Transactions on Networking, vol. 27,\nno. 6, pp. 2391\u20132404, 2019.\n[17] Q. Long, H. Wang, T. Li, L. Huang, K. Wang, Q. Wu, G. Li, Y. Liang,\nL. Yu, and Y. Li, \u201cPractical synthetic human trajectories generation\nbased on variational point processes,\u201d in Proceedings of the 29th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining,\n2023, pp. 4561\u20134571.\n[18] K. Ouyang, R. Shokri, D. S. Rosenblum, and W. Yang, \u201cA non-\nparametric generative model for human trajectories.\u201d in IJCAI,\nvol. 18, 2018, pp. 3812\u20133817.\n[19] D. Huang, X. Song, Z. Fan, R. Jiang, R. Shibasaki, Y. Zhang,\nH. Wang, and Y. Kato, \u201cA variational autoencoder based gener-\native model of urban human mobility,\u201d in 2019 IEEE conference on\nmultimedia information processing and retrieval (MIPR).\nIEEE, 2019,\npp. 425\u2013430.\n[20] D. P. Kingma and M. Welling, \u201cAn introduction to variational\nautoencoders,\u201d arXiv preprint arXiv:1906.02691, 2019.\n[21] D. Smolyak, K. Gray, S. Badirli, and G. Mohler, \u201cCoupled igmm-\ngans with applications to anomaly detection in human mobility\ndata,\u201d ACM Transactions on Spatial Algorithms and Systems (TSAS),\nvol. 6, no. 4, pp. 1\u201314, 2020.\n[22] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial\nnets,\u201d Advances in neural information processing systems, vol. 27,\n2014.\n[23] P. Newson and J. Krumm, \u201cHidden markov map matching\nthrough noise and sparseness,\u201d in Proceedings of the 17th ACM\nSIGSPATIAL international conference on advances in geographic infor-\nmation systems, 2009, pp. 336\u2013343.\n[24] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermody-\nnamics,\u201d in International conference on machine learning.\nPMLR,\n2015, pp. 2256\u20132265.\n[25] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d Advances in neural information processing systems, vol. 33,\npp. 6840\u20136851, 2020.\n[26] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d in\nProceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, 2022, pp. 10 684\u201310 695.\n[27] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P.\nKingma, B. Poole, M. Norouzi, D. J. Fleet et al., \u201cImagen video:\nHigh definition video generation with diffusion models,\u201d arXiv\npreprint arXiv:2210.02303, 2022.\n[28] H. Cao, C. Tan, Z. Gao, Y. Xu, G. Chen, P.-A. Heng, and S. Z. Li,\n\u201cA survey on generative diffusion models,\u201d IEEE Transactions on\nKnowledge and Data Engineering, 2024.\n[29] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and\nM. D. Plumbley, \u201cAudioldm: Text-to-audio generation with latent\ndiffusion models,\u201d arXiv preprint arXiv:2301.12503, 2023.\n[30] M. Jeong, H. Kim, S. J. Cheon, B. J. Choi, and N. S. Kim, \u201cDiff-\ntts: A denoising diffusion model for text-to-speech,\u201d arXiv preprint\narXiv:2104.01409, 2021.\n[31] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive\nsurvey of methods and applications,\u201d ACM Computing Surveys,\n2022.\n[32] M. Hu, Y. Wang, T.-J. Cham, J. Yang, and P. N. Suganthan, \u201cGlobal\ncontext with discrete diffusion in vector quantised modelling for\nimage generation,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 11 502\u201311 511.\n[33] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d in\nIEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. XX, NO. XX, XX XX\n14\nNeurIPS 2021 Workshop on Deep Generative Models and Downstream\nApplications, 2021.\n[34] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan,\n\u201cWavegrad: Estimating gradients for waveform generation,\u201d arXiv\npreprint arXiv:2009.00713, 2020.\n[35] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDiffwave:\nA versatile diffusion model for audio synthesis,\u201d in International\nConference on Learning Representations, 2020.\n[36] Z. Gao, J. Guo, X. Tan, Y. Zhu, F. Zhang, J. Bian, and L. Xu,\n\u201cDifformer: Empowering diffusion model on embedding space for\ntext generation,\u201d arXiv preprint arXiv:2212.09412, 2022.\n[37] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg,\n\u201cStructured denoising diffusion models in discrete state-spaces,\u201d\nAdvances in Neural Information Processing Systems, vol. 34, pp.\n17 981\u201317 993, 2021.\n[38] Z. He, T. Sun, K. Wang, X. Huang, and X. Qiu, \u201cDiffusionbert:\nImproving generative masked language models with diffusion\nmodels,\u201d arXiv preprint arXiv:2211.15029, 2022.\n[39] D. Pfoser and Y. Theodoridis, \u201cGenerating semantics-based tra-\njectories of moving objects,\u201d Computers, Environment and Urban\nSystems, vol. 27, no. 3, pp. 243\u2013263, 2003.\n[40] Y. Theodoridis and M. A. Nascimento, \u201cGenerating spatiotempo-\nral datasets on the www,\u201d ACM SIGMOD Record, vol. 29, no. 3,\npp. 39\u201343, 2000.\n[41] N.\nPelekis,\nC.\nNtrigkogias,\nP.\nTampakis,\nS.\nSideridis,\nand\nY. Theodoridis, \u201cHermoupolis: a trajectory generator for simulat-\ning generalized mobility patterns,\u201d in Machine Learning and Knowl-\nedge Discovery in Databases: European Conference, ECML PKDD 2013,\nPrague, Czech Republic, September 23-27, 2013, Proceedings, Part III\n13.\nSpringer, 2013, pp. 659\u2013662.\n[42] L. Yu, W. Zhang, J. Wang, and Y. Yu, \u201cSeqgan: Sequence generative\nadversarial nets with policy gradient,\u201d in Proceedings of the AAAI\nconference on artificial intelligence, vol. 31, no. 1, 2017.\n[43] J. Feng, Z. Yang, F. Xu, H. Yu, M. Wang, and Y. Li, \u201cLearning to\nsimulate human mobility,\u201d in Proceedings of the 26th ACM SIGKDD\ninternational conference on knowledge discovery & data mining, 2020,\npp. 3426\u20133433.\n[44] Y. Yuan, J. Ding, H. Wang, D. Jin, and Y. Li, \u201cActivity trajectory\ngeneration via modeling spatiotemporal dynamics,\u201d in Proceedings\nof the 28th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining, 2022, pp. 4752\u20134762.\n[45] H. Wang, C. Gao, Y. Wu, D. Jin, L. Yao, and Y. Li, \u201cPategail: a\nprivacy-preserving mobility trajectory generator with imitation\nlearning,\u201d in Proceedings of the AAAI Conference on Artificial Intelli-\ngence, vol. 37, no. 12, 2023, pp. 14 539\u201314 547.\n[46] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d\nAdvances in neural information processing systems, vol. 29, 2016.\n[47] Y. Zhu, Y. Ye, S. Zhang, X. Zhao, and J. Yu, \u201cDifftraj: Generating\ngps trajectory with diffusion probabilistic model,\u201d in Thirty-seventh\nConference on Neural Information Processing Systems, 2023.\n[48] Y. Wang, G. Li, K. Li, and H. Yuan, \u201cA deep generative model\nfor trajectory modeling and utilization,\u201d Proceedings of the VLDB\nEndowment, vol. 16, no. 4, pp. 973\u2013985, 2022.\n[49] J. Li and W. Zhao, \u201cTrajectory generation of ultra-low-frequency\ntravel routes in large-scale complex road networks,\u201d Systems,\nvol. 11, no. 2, p. 61, 2023.\n[50] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\nB. Poole, \u201cScore-based generative modeling through stochastic\ndifferential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.\n[51] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit\nmodels,\u201d in International Conference on Learning Representations,\n2020.\n[52] V. Voleti, A. Jolicoeur-Martineau, and C. Pal, \u201cMcvd-masked con-\nditional video diffusion for prediction, generation, and interpola-\ntion,\u201d Advances in Neural Information Processing Systems, vol. 35, pp.\n23 371\u201323 385, 2022.\n[53] X. Li, J. Thickstun, I. Gulrajani, P. S. Liang, and T. B. Hashimoto,\n\u201cDiffusion-lm improves controllable text generation,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 4328\u20134343, 2022.\n[54] J. Liu, C. Li, Y. Ren, F. Chen, and Z. Zhao, \u201cDiffsinger: Singing\nvoice synthesis via shallow diffusion mechanism,\u201d in Proceedings\nof the AAAI conference on artificial intelligence, vol. 36, no. 10, 2022,\npp. 11 020\u201311 028.\n[55] T. Chen, R. ZHANG, and G. Hinton, \u201cAnalog bits: Generating\ndiscrete data using diffusion models with self-conditioning,\u201d in\nThe Eleventh International Conference on Learning Representations,\n2022.\n[56] A. Grover and J. Leskovec, \u201cnode2vec: Scalable feature learning\nfor networks,\u201d in Proceedings of the 22nd ACM SIGKDD international\nconference on Knowledge discovery and data mining, 2016, pp. 855\u2013864.\n[57] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d\narXiv preprint arXiv:1312.6114, 2013.\n[58] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-tts: A generative flow\nfor text-to-speech via monotonic alignment search,\u201d Advances in\nNeural Information Processing Systems, vol. 33, pp. 8067\u20138077, 2020.\n[59] T. M. Cover, Elements of information theory.\nJohn Wiley & Sons,\n1999.\n[60] S. Gambs, M.-O. Killijian, and M. N. del Prado Cortez, \u201cNext place\nprediction using mobility markov chains,\u201d in Proceedings of the first\nworkshop on measurement, privacy, and mobility, 2012, pp. 1\u20136.\n[61] J. Chung, K. Kastner, L. Dinh, K. Goel, A. C. Courville, and\nY. Bengio, \u201cA recurrent latent variable model for sequential data,\u201d\nAdvances in neural information processing systems, vol. 28, 2015.\n",
    "2311.16203": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nChatTraffic: Text-to-Traffic Generation via Diffusion\nModel\nChengyang Zhang, Yong Zhang, Qitan Shao, Bo Li, Yisheng Lv, Xinglin Piao, and Baocai Yin\nAbstract\u2014Traffic prediction is one of the most significant\nfoundations in Intelligent Transportation Systems (ITS). Tradi-\ntional traffic prediction methods rely only on historical traffic\ndata to predict traffic trends and face two main challenges.\n1) insensitivity to unusual events. 2) limited performance in\nlong-term prediction. In this work, we explore how generative\nmodels combined with text describing the traffic system can be\napplied for traffic generation, and name the task Text-to-Traffic\nGeneration (TTG). The key challenge of the TTG task is how\nto associate text with the spatial structure of the road network\nand traffic data for generating traffic situations. To this end,\nwe propose ChatTraffic, the first diffusion model for text-to-\ntraffic generation. To guarantee the consistency between synthetic\nand real data, we augment a diffusion model with the Graph\nConvolutional Network (GCN) to extract spatial correlations of\ntraffic data. In addition, we construct a large dataset contain-\ning text-traffic pairs for the TTG task. We benchmarked our\nmodel qualitatively and quantitatively on the released dataset.\nThe experimental results indicate that ChatTraffic can generate\nrealistic traffic situations from the text. Our code and dataset\nare available at https://github.com/ChyaZhang/ChatTraffic.\nIndex Terms\u2014Intelligent transportation systems, traffic gener-\nation, diffusion models.\nI. INTRODUCTION\nT\nRAFFIC prediction is a fundamental and pivotal task\nwithin the realm of Intelligent Transportation Systems\n(ITS). Its primary objective is to predict future traffic situations\nbased on historical data [1]\u2013[3]. This task plays a significant\nrole in facilitating peak flow warnings, alleviating congestion,\nand optimizing travel routes. Urban transportation systems,\ndue to their complexity, are susceptible to various influencing\nfactors, including weather, traffic accidents, road construction,\netc. Consequently, the consideration of these diverse factors is\nof paramount importance in enhancing the accuracy of traffic\nprediction.\nMost traffic prediction works use historical data to predict\nfuture data [1]\u2013[7]. Although these methods have demon-\nstrated advanced short-term prediction capabilities on specific\nYong Zhang is the corresponding author.\nChengyang\nZhang,\nYong\nZhang,\nBo\nLi;\nXinglin\nPiao;\nBaocai\nYin\nare\nwith\nBeijing\nKey\nLaboratory\nof\nMultimedia\nand\nIntel-\nligent\nSoftware\nTechnology,\nBeijing\nArtificial\nIntelligence\nInstitute,\nFaculty\nof\nInformation\nTechnology,\nBeijing\nUniversity\nof\nTechnol-\nogy,\nBeijing,\nChina,\n100124.\n(Cy Zhang@emails.bjut.edu.cn;\nzhangy-\nong2010@bjut.edu.cn; shaoqt@emails.bjut.edu.cn; bo li@emails.bjut.edu.cn;\npiaoxl@bjut.edu.cn; ybc@bjut.edu.cn)\nYisheng Lv is with the Institute of Automation, Chinese Academy of\nSciences. (yisheng.lv@ia.ac.cn)\nThe research project is partially supported by the National Key R&D Pro-\ngram of China (No. 2021ZD0111902), National Natural Science Foundation of\nChina (No.62072015, U21B2038, U19B2039, 61902053) and Beijing Natural\nScience Foundation (4222021).\nChallenge I: Insensitive to abnormal events\nChallenge II: Limited performance in long-term prediction\nWhen applying 6-hour traffic \nspeed prediction, the prediction \nerror is massive.\nThe prediction of morning and\nevening traffic peak regularity\nis relatively accurate. When \nabnormal event occurs, large \nprediction error occurs as well.\nTrain\nPredict\nTruth\nPrediction\nFig. 1. Two main challenges confronted by existing traffic prediction methods.\n1) Insensitive to abnormal events. 2) Limited performance in long-term\nprediction.\ndatasets, they still confront two main challenges, as depicted in\nFigure 1. 1) Insensitive to abnormal events. Real-world urban\ntransportation systems frequently experience abnormal events,\nsuch as car accidents, road construction, and extreme weather,\nwhich can significantly disrupt traffic situations. Consequently,\nthese events lead to deviations from the typical traffic patterns.\nMethods trained solely on historical data often struggle to\nprovide accurate predictions when such abnormal events occur.\n2) Limited performance in long-term prediction. Long-term\nprediction plays a significant role in traffic management. While\nexisting traffic prediction methods excel in short-term predic-\ntions, typically within 30 minutes, there remains considerable\nroom for enhancement in long-term prediction. Therefore,\nexisting traffic prediction methods are often unable to address\nmany practical scenarios. For instance, a concert will be\nheld at the Beijing Workers\u2019 Stadium next Saturday night.\nThe traffic situation near the stadium may be much different\nfrom the normal patterns. Traditional prediction methods are\nnot suitable for anticipating traffic situations for this specific\nSaturday evening scenario.\nTo address the limitations of traditional traffic prediction\nmethods, some researchers have already integrated traffic data\nwith text for traffic generation in previous studies. For in-\nstance, Huo et al. [8] construct a dataset containing the traffic-\narXiv:2311.16203v3  [cs.LG]  5 Feb 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\nMarch\n21,\n2022,\n18:00.\nRoad closure on south second\nring road. A general traffic\naccident on s50 east fifth ring\nroad. Road closure on tuanhe\nroad. Construction and road\nclosure on jingliang road. A\ngeneral traffic accident on west\nfourth ring middle road. Road\ntraffic control and a general\ntraffic accident on s12 airport\nexpressway. \u2026\u2026\nInput: Traffic-related Text\nOutput: Traffic situation\nSpeed (km/h)\nCongestion level\nPassing time (h)\nFig. 2.\nText-to-traffic generation via diffusion model. Given a piece of text describing the transportation system (including time and events), we present\nthe diffusion-based ChatTraffic to generate the traffic situation. For the first time, our proposed ChatTraffic is capable of generating traffic situations (speed,\ncongestion level, and passing time) according to the text. This enables ChatTraffic to provide predictions of how future events (road construction, unexpected\naccidents, unusual weather) will affect the urban transportation system, pushing this domain a considerable step forward.\nrelated text data collected from social media and the corre-\nsponding passenger flow data, and propose T2GAN generating\ntraffic situations from the text. However, T2GAN necessitates\nthe division of the city into regular grids for traffic generation,\nwhich significantly constrains its applicability. Additionally,\nthese texts are not specific enough to describe the traffic\nsituations, such as \u201cThe overall traffic situation in Beijing\nis good. The passenger flow is small.\u201d. Moreover, T2GAN\nrequires both text and traffic data as inputs in the inference\nphase. To this end, we explore a novel multimodal traffic\nprediction task called Text-to-Traffic Generation (TTG), as\nshown in Figure 2. In TTG, we not only leverage traffic data\nas input for training but also incorporate textual descriptions\ndepicting the concurrent traffic situations, while only text is\nneeded for inferencing. To better promote multimodal learning\nin the field of traffic prediction, we also construct a substantial\ntraffic dataset containing detailed text descriptions for the TTG\ntask.\nTo associate traffic-related text with traffic data, generative\nmodels provide a feasible way. Among them, Generative Ad-\nversarial Networks (GAN) based methods [9]\u2013[11] implicitly\nfit the data distribution via adversarial training to achieve high-\nquality generation. However, GAN-based methods are always\nrestricted by mode collapse and unstable training. On the other\nhand, Variational Autoencoder (VAE) based methods [12], [13]\nare trained relatively stable but suffer from blurred details\nand offer low-quality outputs compared to GAN. Compared\nto GAN and VAE, diffusion models exhibit greater ease of\ntraining and superior generative capabilities, firmly establish-\ning themselves as one of the most robust generative models\navailable today. Diffusion-based methods [14]\u2013[18] have been\nproven successful in numerous generative tasks, especially\nin generating images from text. Inspired by the success of\ndiffusion models, we believe that diffusion models also have\nthe potential to address the TTG task well.\nIn this paper, we frame the traffic generation as a series\nof diffusion steps and introduce ChatTraffic, a simple yet\neffective framework based on Latent Diffusion Model (LDM)\n[17], for the TTG task. To overcome the challenges confronted\nby traditional traffic prediction methods, texts containing time\nand events are applied to guide the denoising process to\nachieve traffic generation. Furthermore, we augment a dif-\nfusion model with the Graph Convolutional Network (GCN)\n[19], a commonly employed network in traffic prediction\nmethods. Except for time and events, traffic situations are also\naffected by the structure of the road network. In view of this,\nthe primary idea of introducing GCN is to utilize the spatial\ninformation of the road network as a constraint to adjust the\ntraffic features for more accurate conditional traffic generation.\nAs shown in Figure 2, the proposed ChatTraffic is capable\nof generating traffic situations (speed, congestion level, and\npassing time) from the text.\n\u2022 We explore a novel traffic prediction task called Text-to-\nTraffic Generation (TTG). In this task, the first diffusion-\nbased text-to-traffic generation model is proposed, named\nChatTraffic.\n\u2022 We augment the diffusion model with the Graph Con-\nvolutional Networks (GCN), which leverage the spatial\ninformation inherent in the road network, leading to more\nrealistic and accurate traffic generation.\n\u2022 We construct the first substantial text-traffic dataset for\nthe TTG task. The dataset covers 1,260 roads within\nthe fifth ring road area of Beijing, providing more than\n20,000 text-traffic pairs.\nThe rest of the paper is structured as follows. Section II\ndelves into the discussion of related works, while section\nIII provides a comprehensive overview of both the diffusion\nmodel and the proposed ChatTraffic. Section IV presents the\nexperimental results along with a detailed analysis. Lastly,\nSection V encompasses the summary and outlook.\nII. RELATED WORKS\nIn this section, we offer a brief overview of the current\nresearch status in traffic prediction and relevant studies on\nsocial media data-related traffic applications. Additionally, the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nrecent researches on text-based generative models are also\nreviewed.\nA. Traffic Prediction\nTraffic prediction can assist governments in better managing\nthe urban transportation system and holds substantial impor-\ntance within contemporary intelligent transportation systems.\nTraffic prediction methods are broadly classified into two cate-\ngories: machine learning methods and deep learning methods.\nIn the early stages of research, machine learning theories are\npredominantly employed to formulate traffic prediction models\n[4], [5], [20]\u2013[22]. An early attempt [4] proposes the sea-\nsonal Auto-Regressive Integrated Moving Average (ARIMA)\nto analyze the properties of highway traffic flow. Jeong et\nal. [21] introduce the Online Learning Weighted Support-\nVector Regression (OLWSVR) to consider the time difference\nbetween traffic flow data.\nWith the continuous development of urban transportation\nsystems, these methods can no longer cope with the massive\nand complex traffic data. To this end, deep learning meth-\nods [1]\u2013[3], [6], [7], [23]\u2013[26] have gradually become the\nmainstream of research. Typically, Long Short-Term Memory\nNetworks (LSTM) [6], [7] are applied to extract temporal\ncorrelations in traffic prediction. Shi et al. [25] propose a\nconvolutional LSTM to effectively capture spatio-temporal\ninformation in traffic data. Fu et al. [24] associate the LSTM\nand Gated Recurrent Unit (GRU) to predict the short-term\ntraffic flow. To better represent the irregular traffic network\nstructure, the researchers integrate GCN into the traffic pre-\ndiction task to extract spatial correlations. Specifically, the\nSpatio-Temporal Graph Convolutional Networks (STGCN) [1]\npioneers the combination of graph convolution and gated\ncausal convolution to tackle the time series prediction problem\nin the traffic domain. Wu et al. [2] introduce Graph WaveNet\n(GWN), an innovative graph neural network designed for\nmodeling spatial-temporal graphs. This model innovatively\nemploys an adaptive dependency matrix, learned via node\nembedding, enabling precise identification of hidden spatial\ndependencies within the data.\nCurrent traffic prediction methods have achieved good short-\nterm prediction performance, but long-term prediction remains\nan unsolved challenge. Despite the intricate nature of trans-\nportation systems, most traffic prediction methods rely solely\non a single data type, neglecting the potential benefits of\nheterogeneous data. In this paper, we aim to empower the\nmodel with better long-term prediction performance and the\nability to perceive abnormal events by leveraging multimodal\ndata.\nB. Social Media Data-related Traffic Applications\nIn recent years, some researchers have expanded the scope\nof traffic prediction beyond just historical traffic data, incor-\nporating additional sources such as social media data. These\napproaches have proven to be beneficial in enhancing the\nstudy and application of intelligent transportation systems.\nTypically, Ni et al. [27] develop a systematic methodology\nfor scrutinizing social media activities and detecting event\nincidents. Chen et al. [28] utilize the continuous bag-of-words\nmodel to acquire word embedding representations from a vast\ndataset consisting of three billion microblogs, and propose\nLSTM-CNN to extract microblogs relevant to traffic, using\nthe acquired word embeddings as inputs. Moreover, Yao et al.\n[29] suggest the utilization of Twitter messages as a probing\ntechnique for comprehending the influence of individuals\u2019\nwork and rest patterns during the evening and midnight from\nthe previous day to the following morning on traffic conditions.\nWang et al. [30] propose a method to alleviate the issue of\ndata sparsity by integrating traffic event signals extracted from\nsocial media with GPS probe data.\nThe above studies demonstrate the effectiveness of social\nmedia data in assisting traffic prediction and identifying ab-\nnormal traffic events. However, most of these studies con-\ncentrate on the statistical analysis of social media content,\nlargely overlooking the rich semantic information presented\non social media platforms. Therefore, the key to enhancing\ntraffic situation generation lies in effectively correlating text\ndata related to traffic situations with actual traffic data.\nC. Text-based Generative Models\nIn the realm of computer vision, the task of generating\nimages from text has been extensively explored. In the earlier\nstages, methods based on GAN [9]\u2013[11], [31]\u2013[34] demon-\nstrate strong capabilities in associating text descriptions with\nimages. Reed et al. [31] develop a novel architecture and GAN\nformulation for visual concept translation from characters to\npixels. Stack-GAN [32] generates realistic images based on\ntext descriptions. Xue et al. [33] propose the Attention Gen-\nerative Adversarial Network (AttnGAN), which incorporates\nan attention mechanism into GAN to generate fine-grained\nimage details. Zhu et al. [34] design a memory write gate to\nselectively filter crucial segments of text and introduce the Dy-\nnamic Memory Generative Adversarial Network (DM-GAN)\nfor generating high-quality images using textual descriptions.\nDiffusion models [14], [15] are emerging generative models\ncapable of synthesizing high-quality images. Following the\npioneering work of leveraging diffusion steps for data dis-\ntribution learning [35], diffusion models have demonstrated\nremarkable capabilities in various synthesis tasks, such as\nimage editing [36]\u2013[38], image in-painting [18], [39], [40],\nimage super-resolution [41], [42], text-to-image generation\n[17], [43]\u2013[45], and other applications [46]\u2013[49]. Our work\nis mainly related to text-guided synthesis tasks. In the text-to-\nimage generation task, the diffusion-based methods demon-\nstrate performance beyond that of GAN. Typically, Saharia et\nal. [44] propose Imagen, which achieves profound language\nunderstanding with photo-realistic image generation. Nichol\net al. [18] present Guided Language to Image Diffusion\nfor Generation and Editing (GLIDE), a diffusion model that\nleverages textual guidance for both realistic image generation\nand image editing.\nThese text-to-image generation models provide a potential\nidea for generating traffic situations. Inspired by the text-to-\nimage diffusion model, we propose the first diffusion-based\nmethod for text-to-traffic generation by associating traffic data\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\n\ud835\udc99\ud835\udfce\nForward Diffusion Process\n\ud835\udc99\ud835\udc7b\n\ud835\udc99\ud835\udfce\n\ud835\udc99\ud835\udc7b#\ud835\udfcf\nDenoising U-Net\n\u00d7( T\u2013 1 )\nTextual\nEmbeddings \nData\nProcessing\nJanuary 19, 2022, 11:28. Road closure on wufang \nbridge. Construction and road closure on jingliang \nroad. A general traffic accident on lianhuachi east \nroad. Road closure on chaoyangmen north road. \u2026\nData\nVisualization\nGT Data\nGenerated Data\n\u2130\ud835\udcaf\nEvents Description\nAttention\nResNet\nAttention\nResNet\nAttention\nResNet\nAttention\nResNet\n\u2026\n\u2026\n\u2026\nRoad Network\nTopology\nAdjacency Matrix\nGCN\nFig. 3.\nMethod overview. The core components of our proposed ChatTraffic are a UNet consisting of ResNet and cross-attention, and a GCN. We first\npopulate and reshape the data to make it more suitable for use as input to a diffusion model. We use a text encoder to extract feature embeddings from text\ndescribing the traffic system. Furthermore, we introduce a GCN to achieve stronger generative consistency. The GCN takes the noisy traffic data xt and the\nadjacency matrix A describing the spatial correlations of the road network as inputs to associate structural and state features of the road network. The UNet\ntakes the textual feature embeddings and the outputs of the GCN as inputs to predict the denoised traffic data.\nwith text. Text-to-traffic generation can improve the robustness\nof prediction results to abnormal events, making it more\nsuitable for real-world applications.\nIII. METHODOLOGY\nIn this section, we first briefly summarize how the TTG\ntask differs from the traditional traffic prediction task in\nsection III-A. We then introduce the diffusion model for traffic\ngeneration in section III-B. Finally, we illustrate the proposed\nChatTraffic in section III-C. Given a piece of text describing\nthe traffic system (including time, location, and events), the\ngoal of ChatTraffic is to generate traffic situations (speed,\ncongestion level, and passing time) that align well with the\ntext.\nA. Motivation and Task Definition\nTraditional traffic prediction methods rely on historical data\nand lack the integration of multimodal features. Consequently,\nthese methods are limited to forecasting regular traffic patterns,\nsuch as morning and evening peaks. To enhance the robustness\nof the prediction model to abnormal events, we explore a mul-\ntimodal traffic prediction task called Text-to-Traffic Generation\n(TTG) and introduce a feasible solution for it.\nThe key challenge of the TTG task is how to associate\ntext with the spatial structure of the road network and traffic\ndata for generating traffic situations. For example, given text\ndescribing \u201cA general traffic accident on east third ring middle\nroad.\u201d, the generated traffic situation is supposed to reflect\nslower speeds on the east third ring middle road. In view of\nthis, users, especially traffic managers, who are concerned with\nthe traffic situations of a certain scenario or event, can simply\nprovide a text description without any other specialized and\ncomplex operations. In this work, we construct a substantial\ntext-traffic dataset and propose a diffusion-based framework\ncoupled with GCN to address this challenge.\nB. Traffic Generation Diffusion\nWe present an overview of ChatTraffic in Figure 3, which\nis built upon the diffusion model. Diffusion models [14], [15],\n[35] are generative models capable of generating high-quality\ncontent. Its basic form consists of two Markov chains in\nopposite directions while two processes are transiting through\nthe chain. The forward diffusion process gradually adds noise\nto the traffic data until it is completely corrupted by Gaussian\nnoise and becomes indistinguishable. The inverse process fo-\ncuses on learning how to recover the original data distribution\nfrom this noise.\n1) Forward diffusion process: Given the clean traffic data\nx0 sampled from a real data distribution q(x), we gradually\nadd a total of T steps of Gaussian noise to x0, obtaining a\nseries of variables x1, x2, ..., xT . In the forward process, the\ndata xt is only related to the previous moment data xt\u22121 and\ncan be expressed as\nq (xt | xt\u22121) = N\n\u0010\nxt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI\n\u0011\n,\n(1)\nwhere \u03b2t determines the mean and variance of the added noise\nand satisfies \u03b21<\u03b22<...<\u03b2T . This indicates that as t increases,\nthe added noise progressively grows larger. Since it is defined\nas a Markov chain, the joint distribution of x1:T for a given\nx0 is\nq (x1:T | x0) =\nT\nY\nt=1\nq (xt | xt\u22121) .\n(2)\nTo avoid iterative computation, we can directly compute xt\nfrom x0 by utilizing the reparameterization technique. Given\n\u03b1t = 1 \u2212\u03b2t and \u00af\u03b1t = Qt\ni=1 \u03b1i , xt can be represented as\nxt =\n\u221a\n\u00af\u03b1x0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5t,\n(3)\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\nwhere \u03f5t is the noise used to destroy xt and \u03f5t \u223cN(0, I).\nThe relationship between xt and x0 is denoted as\nq (xt | x0) = N\n\u0010\nxt;\n\u221a\n\u00af\u03b1x0, (1 \u2212\u00af\u03b1t) I\n\u0011\n.\n(4)\n2) Inverse generative process: The inverse process recovers\nthe original traffic data from the Gaussian noise and is also\na Markov chain process. Since the noise we add each time\nin the forward process is small, we can use a Gaussian\ntransform p\u03b8 (xt\u22121 | xt) parameterized by the neural network\n\u03b8 to recover xt\u22121 from xt, which is represented as\np\u03b8 (xt\u22121 | xt) = N\n \nxt\u22121; \u00b5\u03b8 (xt, t) ,\nX\n\u03b8\n(xt, t)\n!\n,\n(5)\nwhere \u00b5\u03b8 (xt, t) and P\n\u03b8 (xt, t) are the predicted mean and\ncovariance at time step t. To achieve higher synthesis quality,\nHo et al. [50] estimate \u03f5\u03b8 (xt, t) rather than directly predict\n\u00b5\u03b8 (xt, t). Based on Bayesian theory, \u00b5\u03b8 (xt, t) can be ex-\npressed as\n\u00b5\u03b8 (xt, t) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212\u00af\u03b1t\n\u03f5\u03b8 (xt, t)\n\u0013\n.\n(6)\nC. ChatTraffic\nGiven a text y describing the traffic system, ChatTraffic\ndenoise the data xt to the final noise-free x0. We first process\nthe data to make it more suitable as the input to the diffusion\nmodel. Concurrently, the provided text is encoded into feature\nembeddings using a text encoder. Combined with the cross-\nattention mechanism, GCN and Unet, ChatTraffic predicts\n\u03f5\u03b8 (xt, t) to get the cleaner xt\u22121. After T time steps, the\nnoise-free x0 is obtained. With data visualization, we present\nthe traffic situations on the map to provide a more intuitive\nrepresentation.\n1) Data processing: The released dataset comprises over\n20,000 text-traffic pairs within the fifth ring road in Beijing.\nEach data x contains three types of features (speed, con-\ngestion level, passing time) derived from 1260 roads, where\nx \u2208RN\u00d7d, N = 1260, and d = 3. However, there exists\na considerable disparity in the dimensions of x, rendering it\nunsuitable as input for the diffusion model. To address this\nissue, we rearrange the traffic data x in the form of images.\nSpecifically, we define 36 additional \u201cempty roads\u201d and pad\nthem into x to make x \u2208RN \u2032\u00d7d, N \u2032 = 1296. x is then\nreshaped to x \u2208RH\u00d7W \u00d7d, where H and W equal to 36. If\nwe view x as an image, the three channels represent speed,\ncongestion level, and passing time, and each pixel represents\na road. In Figure 4, we illustrate how the traffic data x evolves\nthroughout the forward process of ChatTraffic.\n2) U-Net & textual encoder: To associate text with the\ntraffic data, we formulate the TTG task as a conditional\ngeneration problem and implement it using a modified LDM.\nLDM transforms the data x into the latent space to obtain z,\nachieved through an encoder E(\u00b7). Then, a diffusion process\nis applied to z, and the reconstructed x is obtained by using\na decoder D(\u00b7). In our setup, due to the small dimensions (H\nand W) of the traffic \u201cimage\u201d, we ignore E(\u00b7) and D(\u00b7), and\ntake x as input directly. We retained the denoising network of\nInput\nAdd noise\nFig. 4. Illustration of adding noise to traffic data. From left to right represents\nthe gradual addition of noise to the traffic data. From top to bottom are four\ndifferent junctures of traffic data presented in the form of \u2018images\u2019.\nthe LDM considering the effectiveness of the combination of\nResNet and cross-attention employed by U-Net in the LDM.\nIn LDM, the features from the conditional encoder are applied\nthrough the cross-attention mechanism, which is formulated as\nA = Softmax\n\u0012QKT\n\u221a\nd\n\u0013\n,\n(7)\nwhere Q = W (i)\nQ \u03c6i (xt), K = W (i)\nK ET (y). \u03c6i (xt) is an\nintermediate representation of U-Net. W (i)\nQ\nand W (i)\nK\nare\nlearnable parameters. ET (\u00b7) is the conditional encoder, where\nwe use BERT [51] to extract the text embedding.\n3) Graph convolutional network: Diversity and consistency\nare two opposing goals when sampling conditional generative\nmodels. The image generation tasks, such as text-to-image\ngeneration, often prioritize diverse results. However, the TTG\ntask places a higher emphasis on generative consistency.\nSpecifically, the TTG task aims to produce consistent traffic\nsituations when generating from the same textual input. To\nachieve this consistency, we integrate GCN into the diffusion\nmodel to provide stronger guidance by introducing spatial\ninformation of the road network. In the constructed dataset,\nwe provide a neighborhood matrix A to represent the spatial\nassociation of all roads. Given the adjacency matrix A and\ndata xt, a two-layer GCN can be represented as\nf (xt, A) = \u03c3\n\u0010\n\u02c6A ReLU\n\u0010\n\u02c6AxtW0\n\u0011\nW1\n\u0011\n,\n(8)\nwhere \u02c6A = eD\u22121\n2 eA eD\u22121\n2 , eA = A+IN. eD is the degree matrix\nof eA, and IN is the unit matrix. W0 and W1 are the weight\nmatrices. \u03c3 (\u00b7) is the activation function, and the sigmoid is\nused here. We further reformulate the optimization objective\nof ChatTraffic to\nLCT = Ex,y,\u03f5,t\nh\n\u2225\u03f5 \u2212\u03f5\u03b8 (f (xt, A) , t, ET (y))\u22252\n2\ni\n,\n(9)\nwhere y denotes text, and \u03f5\u03b8 (f (xt, A) , t, ET (y)) is a series\nof denoising functions implemented via U-Net.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\nTABLE I\nQUANTITATIVE COMPARISON OF CHATTRAFFIC WITH TRADITIONAL TRAFFIC PREDICTION METHODS ON THREE SPECIFIC JUNCTURES. TRADITIONAL\nMETHODS PREDICT DATA FOR THE 20TH, 40TH, AND 60TH MINUTES IN THE FUTURE BY INPUTTING ONE HOUR OF HISTORICAL DATA. CHATTRAFFIC\nDIRECTLY INPUTS THE TEXT CORRESPONDING TO THE THREE JUNCTURES. THE BEST PERFORMANCE IS IN BOLD.\nMethod\nInput\nJanuary 21, 2022, 18:20\nJanuary 21, 2022, 18:40\nJanuary 21, 2022, 19:00\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nSTGCN [1]\nHistorical data during\nJanuary 21, 2022, 17:00 -\nJanuary 21, 2022, 18:00\n3.94\n6.04\n4.37\n6.51\n4.69\n7.12\nGWN [2]\n3.71\n5.68\n3.96\n6.09\n4.11\n6.32\nSTSGCN [52]\n3.66\n5.55\n4.09\n6.08\n4.22\n6.41\nASTGCN [53]\n3.68\n5.61\n4.06\n6.17\n4.18\n6.39\nMTGNN [3]\n3.63\n5.57\n3.89\n5.95\n4.02\n6.13\nSTG-NCDE [54]\n3.65\n5.60\n3.84\n5.89\n4.09\n6.21\nLDM [17]\nText\n3.81\n5.84\n3.56\n5.61\n3.60\n5.56\nChatTraffic\n3.73\n5.76\n3.38\n5.14\n3.44\n5.25\n4) Data visualization: After completing the training of\nChatTraffic, given a piece of text describing the traffic sit-\nuation, we get the predicted traffic data \u02c6x \u2208RH\u00d7W \u00d7d. Each\npixel point in \u02c6x corresponds to a specific road, and we can\nvisualize the traffic situation of these roads on a map, as shown\nin Figure 2. We use three different color bars to represent\nthree different features, where speed and passing time are\ncontinuous data and congestion level is discrete data.\nIV. EXPERIMENTS AND ANALYSIS\nA. Experimental Setup\n1) Dataset: We construct a substantial dataset and carry out\nexperiments on it since there is no publicly available dataset\nsuitable for the TTG task. The dataset covers 1,260 roads\nwithin the fifth ring road area of Beijing, providing 22,320\ntext-traffic pairs. Each pair contains the traffic data x, where\nx \u2208R1260\u00d73, and a corresponding piece of text describing the\nstate of the traffic system. We set 80% of the entire dataset as\nthe training set and the remaining 20% as the testing set. The\ndata collection interval is 4 minutes. The three dimensions of\ntraffic data are the speed, the passing time of vehicles on each\nroad, and the congestion level of each road. The text includes\nthe time as well as the type and location of occurring abnormal\nevents, which is structured as follows:\n\u2022 March 21, 2022, 18:00. Road closure on south second\nring road. A general traffic accident on s50 east fifth ring\nroad. Road closure on tuanhe road. Construction and road\nclosure on jingliang road. A general traffic accident on\nwest fourth ring middle road. Road traffic control and a\ngeneral traffic accident on s12 airport expressway. ......\n2) Compared methods: To demonstrate the viability and\neffectiveness of our ChatTraffic, we choose five deep-learning\nmethods for traditional traffic prediction to compare with\nour method. The five methods are STGCN [1], GWN [2],\nSTSGCN [52], ASTGCN [53], MTGNN [3] and STG-NCDE\n[54]. It is worth noting that in the testing phase, the traditional\ntraffic prediction methods rely on historical traffic data as\ninput, whereas ChatTraffic utilizes text as input.\n3) Evaluation metrics: To evaluate the performance of all\nmethods, we employ two standard metrics. Mean Absolute\nError (MAE) and Root Mean Squared Error (RMSE) are used\nto quantify the prediction accuracy:\nMAE = 1\nN\nN\nX\ni=1\n|xi \u2212\u02c6xi| ,\n(10)\nRMSE =\nv\nu\nu\nt 1\nN\nN\nX\ni=1\n|xi \u2212\u02c6xi|2,\n(11)\nwhere MAE calculates the average distance between the gen-\nerated value \u02c6xi and the real value xi, providing an assessment\nof the global quality of the generated traffic situation. RMSE\nevaluates the local quality of the generated traffic situation\nsince it is more sensitive to outliers and noise. N represents\nthe number of predicted moments.\n4) Implementation details: For traditional traffic prediction\nmethods, we use the default parameters for training. For\nChatTraffic, we train it with T = 1000 noising steps and\na linear noise schedule. The linear noise schedule starts at\n0.00085 and ends at 0.012. The base learning rate is set to\n10\u22125 while the batch size is set to 4. All the experiments are\nconducted on a single Nvidia GeForce 4090 (\u223c24GB).\nB. Quantitative and Qualitative Comparisons\nWe quantitatively compare our proposed ChatTraffic with\nseveral state-of-the-art traffic prediction methods. The ex-\nperimental results are listed in Table I. For the traditional\ntraffic prediction method, we choose to input 15 consecutive\ndata from the test set, covering a 1-hour duration, and make\npredictions for the subsequent 5, 10, and 15 data.\nWith ChatTraffic, we directly input the text corresponding\nto the three specific junctures that require predictions. Table\nI illustrates that the performance of traffic prediction methods\ndeteriorates as time increases. In contrast, ChatTraffic consis-\ntently maintains a low and stable prediction error across all\nthree junctures. We then quantitatively compare ChatTraffic\nwith traffic prediction methods on the entire test set, as\nillustrated in Table III. Similarly, we employ 15 consecutive\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nSTGCN\nGWN\nChatTraffic\nGround Truth\n(1)\n(2)\n(3)\n(4)\n(5)\nFig. 5. Qualitative comparison of ChatTraffic with two traditional traffic prediction methods on five specific junctures. The first to fifth rows represent five\nspecific junctures. Best viewed in red boxes.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\nTABLE II\nABLATION RESULTS ON GCN. CHATTRAFFIC GENERATES THE MOST ACCURATE PREDICTIONS WHEN THE NUMBER OF LAYERS OF THE GCN IS SET TO\n2. NUMBER OF SAMPLES INDICATES HOW MANY SAMPLES TO PRODUCE FOR THE GIVEN PROMPT. AS THE NUMBER OF SAMPLES GENERATED\nINCREASES, THE TIME TAKEN TO GENERATE THE SAMPLES ALSO INCREASES. THE BEST PERFORMANCE IS IN BOLD.\nLayers of GCN\nNumbers of samples\nTime consumption (sec)\nCongestion level\nSpeed (km/h)\nPassing time (sec)\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\n0\n1\n9.76\n0.07\n0.26\n4.42\n6.47\n105.73\n216.21\n5\n45.14\n0.06\n0.21\n3.46\n5.08\n83.29\n169.93\n10\n92.93\n0.05\n0.20\n3.31\n4.92\n79.83\n164.12\n1\n1\n9.81\n0.06\n0.24\n4.31\n6.35\n100.21\n209.52\n5\n44.97\n0.05\n0.20\n3.35\n5.01\n80.74\n166.97\n10\n93.14\n0.05\n0.19\n3.26\n4.86\n77.19\n161.26\n2\n1\n9.79\n0.06\n0.24\n4.23\n6.29\n98.98\n204.54\n5\n46.62\n0.05\n0.19\n3.35\n4.98\n79.01\n164.59\n10\n93.68\n0.05\n0.18\n3.21\n4.78\n75.89\n158.92\n3\n1\n10.03\n0.06\n0.26\n4.35\n6.28\n102.75\n213.78\n5\n47.31\n0.05\n0.19\n3.38\n4.98\n82.31\n168.32\n10\n94.25\n0.05\n0.19\n3.29\n4.86\n78.23\n163.51\nTABLE III\nQUANTITATIVE COMPARISON OF CHATTRAFFIC WITH TRADITIONAL\nTRAFFIC PREDICTION METHODS ON THE TEST SET.\nMethod\nH=51\nH=102\nH=153\nMAE\nRMSE\nMAE\nRMSE\nMAE\nRMSE\nSTGCN [1]\n3.86\n5.85\n4.40\n6.55\n4.45\n6.83\nGWN [2]\n3.56\n5.51\n3.78\n5.98\n3.95\n6.23\nSTSGCN [52]\n3.73\n5.70\n4.07\n6.21\n4.25\n6.65\nASTGCN [53]\n3.80\n5.84\n4.06\n6.34\n4.10\n6.43\nMTGNN [3]\n3.49\n5.39\n3.85\n5.94\n4.28\n6.14\nSTG-NCDE [54]\n3.40\n5.17\n3.88\n6.23\n4.03\n6.51\nMethod\nTest set4\nMAE\nRMSE\nLDM [17]\n3.31\n4.92\nChatTraffic\n3.21\n4.78\n1,2,3 For traditional prediction methods, we take 15 data as input, for a total\ntime span of 1 hour. We then make predictions for the 5th data (20 minutes),\nthe 10th data (40 minutes), and the 15th data (60 minutes).\n4 ChatTraffic accepts the text corresponding to each data in the test set as\ninput and directly generates predictions for each specific moment.\ndata as input for the traditional methods and move through\nthe entire test set using the sliding window approach. Table I\nand Table III demonstrate that ChatTraffic not only matches\nthe performance of state-of-the-art traffic prediction methods\nin short-term predictions but also maintains such performance\nin long-term predictions. This evidence confirms that the\npoint-to-point generation strategy employed by ChatTraffic\nremains unaffected by the duration of the prediction period.\nIn other words, ChatTraffic effectively alleviates the challenge\nof limited performance in long-time prediction of current\ntraffic prediction methods. Consequently, ChatTraffic holds the\ncapability to generate future traffic situations, especially in\nscenarios influenced by abnormal events.\nTo intuitively observe the ability of ChatTraffic to perceive\nabnormal road events, we also conduct a qualitative analysis.\nIn Figure 5, we present a comprehensive visual comparison of\nChatTraffic with two representative methods. The first to fifth\nrows represent five specific junctures. From the red boxes,\nit can be observed that outputs of ChatTraffic closely align\nwith the ground truth, demonstrating its capacity to reflect\nthe influence of abnormal events on roads. On the contrary,\nthe two traditional methods seldom succeed in predicting\nanomalies, such as individual road congestion. These visu-\nalizations demonstrate that ChatTraffic is more sensitive to\ntraffic anomalies compared to traditional traffic prediction\nmethods, resulting in more accurate predictions of traffic\nsituations.\nC. Ablation Study\n1) Effectiveness of GCN: In order to understand the ef-\nfectiveness of GCN, we conduct an ablation experiment on\nthe number of layers of GCN in ChatTraffic. Results are\npresented in Table II. Incorporating Graph Convolutional Net-\nworks (GCN) into ChatTraffic has been proven to enhance\nits generation accuracy, with the minimum generation error\nobserved when using two layers of GCN. It\u2019s essential to\nhighlight that as the number of GCN layers continues to\nincrease, the error begins to rise instead. This phenomenon can\nbe attributed to the road network\u2019s adjacency matrix having a\nnotably sparse structure. When the sparsity of this adjacency\nmatrix is low, there is a risk of over-smoothing, rendering the\nutilization of multi-layer GCN less effective. Besides, the time\nconsumption of ChatTraffic to generate a single prediction\nsample is approximately ten seconds. When five samples are\ngenerated and averaged based on a test prompt, the MAE\nand RMSE decrease substantially compared to when only one\nsample is generated. However, while generating ten samples\nand averaging them further reduces the MAE and RMSE,\nthe improvement is not markedly significant. Therefore, gen-\nerating five samples is the preferred approach when using\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nMarch 07, 2022, 1:00. \nMarch 07, 2022, 9:00. \nMarch 07, 2022, 18:00. \nInput text: only time\nOutput: speed(km/h)\nOutput: congestion level\nFig. 6. Output visualization. ChatTraffic can predict routine traffic patterns, i.e., the morning and evening peaks, when the input text contains only time.\n3.21\n3.38\n3.63\n4.04\n4.59\n5.31\n4.78\n4.95\n5.42\n6.29\n6.97\n9.17\n2\n4\n6\n8\n10\n0%\n10%\n30%\n50%\n70%\n90%\nSpeed (km/h)\nMAE\nRMSE\n0.05\n0.09\n0.14\n0.16\n0.23\n0.35\n0.18\n0.28\n0.41\n0.46\n0.57\n0.81\n0\n0.2\n0.4\n0.6\n0.8\n1\n0%\n10%\n30%\n50%\n70%\n90%\nCongestion level\nMAE\nRMSE\n75.89\n92.31\n104.76\n122.93\n141.28\n177.47\n158.92\n191.15\n218.66\n252.27\n293.54\n385.83\n50\n140\n230\n320\n410\n0%\n10%\n30%\n50%\n70%\n90%\nPassing time (sec)\nMAE\nRMSE\nFig. 7. Ablation results on the missing rate of input events prompt. The missing rate is set to 10%, 30%, 50%, 70%, and 90%. As the missing rate of input\nevent increases, there is a corresponding rise in the MAE and RMSE of the three generated traffic data.\nChatTraffic for an optimal balance between performance and\ntime efficiency.\n2) Input partial events: To verify how the quantity of input\nevents impacts the traffic situations generated by ChatTraffic,\nwe use incomplete event prompts from the test set as inputs\nin this ablation study. We randomly erase events from each\ncomplete event prompt in the test set with the missing rates\nof 10%, 30%, 50%, 70%, and 90%. The results are illustrated\nin Figure 7, showing that as the missing rate of input event\nincreases, there is a corresponding rise in the MAE and RMSE\nof the three generated traffic data. This phenomenon proves\nthat the traffic situations generated by ChatTraffic have a\nstrong correlation with the input event data. It is worth noting\nthat the increases in MAE and RMSE are relatively stable\nand remain within a low range, without any drastic spikes.\nThis observation suggests that ChatTraffic might have the\nability to generate traffic situations that align with the cyclic\npatterns of the traffic system based solely on temporal factors,\neven without any specific event input. To further explore the\nhypothesis, we conduct experiments in the next ablation study.\n3) Input time only: In this study, we aim to demonstrate\nthat ChatTraffic can predict routine traffic patterns, such as\nthe morning and evening peaks, similar to traditional traffic\nprediction methods. Therefore, we input text containing only\ntime information to ChatTraffic. Figure 6 illustrates the speed\nand road congestion level generated by ChatTraffic at three dif-\nferent junctures: 1:00, 9:00, and 18:00. Based on conventional\nknowledge, traffic congestion should be minimal at 1:00, while\n9:00 and 18:00 correspond to the morning and evening rush\nhours, during which certain roads are expected to experience\ncongestion. From the data represented in Table 6, it is evident\nthat ChatTraffic generates results that align closely with this\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nanticipated pattern. It\u2019s noteworthy that the speeds of most\nroads at these three junctures are relatively modest. This is\nprimarily because of the presence of speed limits on urban\nroads. Therefore, it can be reasonably inferred that ChatTraffic\ncan deduce the influence of time variations on the traffic\nsystem from text containing only time, even in the absence\nof historical data.\nV. CONCLUSION\nIn this paper, we explore a novel multimodal traffic pre-\ndiction task called Text-to-Traffic Generation, which aims\nto generate traffic situations described by text. To that end,\nwe propose ChatTraffic, the first diffusion-based text-to-\ntraffic generative model, and construct a substantial text-traffic\ndataset. Different from traditional traffic prediction methods\nthat only use historical data, combining the advancement of\nthe diffusion model and the spatial perception offered by\nthe GCN, ChatTraffic is armed with realistic and accurate\ntraffic generation capability. We demonstrate the superiority\nof ChatTraffic over traditional prediction methods by conduct-\ning comparison experiments on the constructed dataset and\nshowing the visualizations. We anticipate a growing interest\nfrom researchers in the TTG domain due to its significant\npractical applications, notably in enhancing the efficiency of\nurban transportation system management. In the future, we\nwill further explore improving the accuracy of traffic situation\ngeneration through multimodal learning.\nREFERENCES\n[1] B. Yu, H. Yin, and Z. Zhu, \u201cSpatio-temporal graph convolutional net-\nworks: a deep learning framework for traffic forecasting,\u201d in Proceedings\nof the International Joint Conference on Artificial Intelligence, 2018, pp.\n3634\u20133640.\n[2] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, \u201cGraph wavenet for deep\nspatial-temporal graph modeling,\u201d arXiv preprint arXiv:1906.00121,\n2019.\n[3] Z. Wu, S. Pan, G. Long, J. Jiang, X. Chang, and C. Zhang, \u201cConnecting\nthe dots: Multivariate time series forecasting with graph neural net-\nworks,\u201d in Proceedings of the ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining, 2020.\n[4] B. M. Williams and L. A. Hoel, \u201cModeling and forecasting vehicular\ntraffic flow as a seasonal arima process: Theoretical basis and empirical\nresults,\u201d Journal of Transportation Engineering, vol. 129, no. 6, pp.\n664\u2013672, 2003.\n[5] N. Caceres, L. M. Romero, F. G. Benitez, and J. M. del Castillo, \u201cTraffic\nflow estimation models using cellular phone data,\u201d IEEE Transactions\non Intelligent Transportation Systems, vol. 13, no. 3, pp. 1430\u20131441,\n2012.\n[6] Y. Tian and L. Pan, \u201cPredicting short-term traffic flow by long short-\nterm memory recurrent neural network,\u201d in Proceedings of the IEEE\nInternational Conference on Smart City/SocialCom/SustainCom, 2015,\npp. 153\u2013158.\n[7] Z. Zhao, W. Chen, X. Wu, P. C. Chen, and J. Liu, \u201cLstm network: a\ndeep learning approach for short-term traffic forecast,\u201d IET Intelligent\nTransport Systems, vol. 11, no. 2, pp. 68\u201375, 2017.\n[8] G. Huo, Y. Zhang, B. Wang, Y. Hu, and B. Yin, \u201cText-to-traffic\ngenerative adversarial network for traffic situation generation,\u201d IEEE\nTransactions on Intelligent Transportation Systems, vol. 23, no. 3, pp.\n2623\u20132636, 2021.\n[9] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial networks,\u201d\nCommunications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[10] R. Abdal, Y. Qin, and P. Wonka, \u201cImage2stylegan++: How to edit the\nembedded images?\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2020, pp. 8296\u20138305.\n[11] O. Patashnik, Z. Wu, E. Shechtman, D. Cohen-Or, and D. Lischinski,\n\u201cStyleclip: Text-driven manipulation of stylegan imagery,\u201d in Proceed-\nings of the IEEE/CVF International Conference on Computer Vision,\n2021, pp. 2085\u20132094.\n[12] D. P. Kingma and M. Welling, \u201cAuto-encoding variational bayes,\u201d arXiv\npreprint arXiv:1312.6114, 2013.\n[13] P. Esser, E. Sutter, and B. Ommer, \u201cA variational u-net for conditional\nappearance and shape generation,\u201d in Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition, 2018, pp. 8857\u20138866.\n[14] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d\nin Proceedings of the International Conference on Machine Learning,\n2015, pp. 2256\u20132265.\n[15] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d\nAdvances in Neural Information Processing Systems, vol. 33, pp. 6840\u2013\n6851, 2020.\n[16] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d Advances in Neural Information Processing Systems, vol. 34,\npp. 8780\u20138794, 2021.\n[17] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-\nresolution image synthesis with latent diffusion models,\u201d in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, 2022, pp. 10 684\u201310 695.\n[18] A. Nichol, P. Dhariwal, A. Ramesh, P. Shyam, P. Mishkin, B. McGrew,\nI. Sutskever, and M. Chen, \u201cGlide: Towards photorealistic image gen-\neration and editing with text-guided diffusion models,\u201d arXiv preprint\narXiv:2112.10741, 2021.\n[19] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph\nconvolutional networks,\u201d arXiv preprint arXiv:1609.02907, 2016.\n[20] I. Lana, J. Del Ser, M. Velez, and E. I. Vlahogianni, \u201cRoad traffic\nforecasting: Recent advances and new challenges,\u201d IEEE Intelligent\nTransportation Systems Magazine, vol. 10, no. 2, pp. 93\u2013109, 2018.\n[21] Y.-S. Jeong, Y.-J. Byon, M. M. Castro-Neto, and S. M. Easa, \u201cSupervised\nweighting-online learning algorithm for short-term traffic flow predic-\ntion,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 14,\nno. 4, pp. 1700\u20131707, 2013.\n[22] J. Van Lint and C. Van Hinsbergen, \u201cShort-term traffic and travel\ntime prediction models,\u201d Artificial Intelligence Applications to Critical\nTransportation Issues, vol. 22, no. 1, pp. 22\u201341, 2012.\n[23] Y. Lv, Y. Duan, W. Kang, Z. Li, and F.-Y. Wang, \u201cTraffic flow\nprediction with big data: A deep learning approach,\u201d IEEE Transactions\non Intelligent Transportation Systems, vol. 16, no. 2, pp. 865\u2013873, 2014.\n[24] R. Fu, Z. Zhang, and L. Li, \u201cUsing lstm and gru neural network methods\nfor traffic flow prediction,\u201d in Proceedings of the Youth Academic Annual\nConference of Chinese Association of Automation, 2016, pp. 324\u2013328.\n[25] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c.\nWoo, \u201cConvolutional lstm network: A machine learning approach for\nprecipitation nowcasting,\u201d Advances in Neural Information Processing\nSystems, vol. 28, 2015.\n[26] J. Zhang, Y. Zheng, and D. Qi, \u201cDeep spatio-temporal residual networks\nfor citywide crowd flows prediction,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 31, no. 1, 2017.\n[27] M. Ni, Q. He, and J. Gao, \u201cForecasting the subway passenger flow under\nevent occurrences with social media,\u201d IEEE Transactions on Intelligent\nTransportation Systems, vol. 18, no. 6, pp. 1623\u20131632, 2016.\n[28] Y. Chen, Y. Lv, X. Wang, L. Li, and F.-Y. Wang, \u201cDetecting traffic\ninformation from social media texts with deep learning approaches,\u201d\nIEEE Transactions on Intelligent Transportation Systems, vol. 20, no. 8,\npp. 3049\u20133058, 2018.\n[29] W. Yao and S. Qian, \u201cFrom twitter to traffic predictor: Next-day morning\ntraffic prediction using social media data,\u201d Transportation research part\nC: emerging technologies, vol. 124, p. 102938, 2021.\n[30] S. Wang, X. Zhang, F. Li, S. Y. Philip, and Z. Huang, \u201cEfficient\ntraffic estimation with multi-sourced data by parallel coupled hidden\nmarkov model,\u201d IEEE Transactions on Intelligent Transportation Sys-\ntems, vol. 20, no. 8, pp. 3010\u20133023, 2018.\n[31] S. Reed, Z. Akata, X. Yan, L. Logeswaran, B. Schiele, and H. Lee,\n\u201cGenerative adversarial text to image synthesis,\u201d in Proceedings of the\nInternational Conference on Machine Learning, 2016, pp. 1060\u20131069.\n[32] H. Zhang, T. Xu, H. Li, S. Zhang, X. Wang, X. Huang, and D. N.\nMetaxas, \u201cStackgan: Text to photo-realistic image synthesis with stacked\ngenerative adversarial networks,\u201d in Proceedings of the IEEE Interna-\ntional Conference on Computer Vision, 2017, pp. 5907\u20135915.\n[33] T. Xu, P. Zhang, Q. Huang, H. Zhang, Z. Gan, X. Huang, and\nX. He, \u201cAttngan: Fine-grained text to image generation with attentional\ngenerative adversarial networks,\u201d in Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition, 2018, pp. 1316\u20131324.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\n[34] M. Zhu, P. Pan, W. Chen, and Y. Yang, \u201cDm-gan: Dynamic memory\ngenerative adversarial networks for text-to-image synthesis,\u201d in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2019, pp. 5802\u20135810.\n[35] Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d Advances in Neural Information Processing\nSystems, vol. 32, 2019.\n[36] G. Couairon, J. Verbeek, H. Schwenk, and M. Cord, \u201cDiffedit: Diffusion-\nbased semantic image editing with mask guidance,\u201d arXiv preprint\narXiv:2210.11427, 2022.\n[37] A. Hertz, R. Mokady, J. Tenenbaum, K. Aberman, Y. Pritch, and\nD. Cohen-Or, \u201cPrompt-to-prompt image editing with cross attention\ncontrol,\u201d arXiv preprint arXiv:2208.01626, 2022.\n[38] B. Kawar, S. Zada, O. Lang, O. Tov, H. Chang, T. Dekel, I. Mosseri, and\nM. Irani, \u201cImagic: Text-based real image editing with diffusion models,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 6007\u20136017.\n[39] O. Avrahami, D. Lischinski, and O. Fried, \u201cBlended diffusion for text-\ndriven editing of natural images,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2022, pp.\n18 208\u201318 218.\n[40] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and\nL. Van Gool, \u201cRepaint: Inpainting using denoising diffusion probabilistic\nmodels,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2022, pp. 11 461\u201311 471.\n[41] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi,\n\u201cImage super-resolution via iterative refinement,\u201d IEEE Transactions on\nPattern Analysis and Machine Intelligence, vol. 45, no. 4, pp. 4713\u2013\n4726, 2022.\n[42] C. Wang, K. Yeo, X. Jin, A. Codas, L. J. Klein, and B. Elmegreen, \u201cS3rp:\nSelf-supervised super-resolution and prediction for advection-diffusion\nprocess,\u201d arXiv preprint arXiv:2111.04639, 2021.\n[43] A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, \u201cHierarchical\ntext-conditional image generation with clip latents,\u201d arXiv preprint\narXiv:2204.06125, vol. 1, no. 2, p. 3, 2022.\n[44] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton,\nK. Ghasemipour, R. Gontijo Lopes, B. Karagol Ayan, T. Salimans\net al., \u201cPhotorealistic text-to-image diffusion models with deep language\nunderstanding,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 36 479\u201336 494, 2022.\n[45] Y. Balaji, S. Nah, X. Huang, A. Vahdat, J. Song, K. Kreis, M. Aittala,\nT. Aila, S. Laine, B. Catanzaro et al., \u201cediffi: Text-to-image diffu-\nsion models with an ensemble of expert denoisers,\u201d arXiv preprint\narXiv:2211.01324, 2022.\n[46] A. K. Bhunia, S. Khan, H. Cholakkal, R. M. Anwer, J. Laaksonen,\nM. Shah, and F. S. Khan, \u201cPerson image synthesis via denoising\ndiffusion model,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 5968\u20135976.\n[47] H.-Y. Tseng, Q. Li, C. Kim, S. Alsisan, J.-B. Huang, and J. Kopf,\n\u201cConsistent view synthesis with pose-guided diffusion models,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2023, pp. 16 773\u201316 783.\n[48] J. Tang, Y. Nie, L. Markhasin, A. Dai, J. Thies, and M. Nie\u00dfner,\n\u201cDiffuscene: Scene graph denoising diffusion probabilistic model for\ngenerative indoor scene synthesis,\u201d arXiv preprint arXiv:2303.14207,\n2023.\n[49] Z. Chen, J. Qing, T. Xiang, W. L. Yue, and J. H. Zhou, \u201cSeeing beyond\nthe brain: Conditional diffusion model with sparse masked modeling\nfor vision decoding,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 22 710\u201322 720.\n[50] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv\npreprint arXiv:2207.12598, 2022.\n[51] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.\n[52] C. Song, Y. Lin, S. Guo, and H. Wan, \u201cSpatial-temporal synchronous\ngraph convolutional networks: A new framework for spatial-temporal\nnetwork data forecasting,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 34, no. 01, 2020, pp. 914\u2013921.\n[53] J. Zhu, Q. Wang, C. Tao, H. Deng, L. Zhao, and H. Li, \u201cAst-gcn:\nAttribute-augmented spatiotemporal graph convolutional network for\ntraffic forecasting,\u201d IEEE Access, vol. 9, pp. 35 973\u201335 983, 2021.\n[54] J. Choi, H. Choi, J. Hwang, and N. Park, \u201cGraph neural controlled\ndifferential equations for traffic forecasting,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 36, no. 6, 2022, pp. 6367\u2013\n6374.\nChengyang Zhang received his bachelor degrees\nfrom Beijing Information Science and Technology\nUniversity. He is currently a graduate student at\nthe Department of Informatics, Beijing University\nof Technology. His research interests include com-\nputer vision, traffic prediction and biomedical image\nanalysis.\nYong\nZhang\n(Member,\nIEEE)\nreceived\nthe\nPh.D.degree in computer science from the Beijing\nUniversity of Technology in 2010. He is currently\nan Associate Professor in computer science with the\nBeijing University of Technology. His research in-\nterests include intelligent transportation systems, big\ndata analysis, visualization, and computer graphics.\nQitan Shao received the bachelor\u2019s degree in com-\nputer science and technology from the Beijing Uni-\nversity of Technology. He is currently a graduate\nstudent at the Department of Informatics, Beijing\nUniversity of Technology. His research interest is\nintelligent transportation systems.\nBo Li received his bachelor degrees from Beijing\nInformation Science and Technology University. He\nis currently a graduate student at the Department\nof Informatics, Beijing University of Technology.\nHis research interests include computer vision and\nbiomedical image analysis.\nYisheng Lv received the B.E. and M.E. degrees\nin transportation engineering from Harbin Institute\nof Technology, Harbin, China, in 2005 and 2007,\nrespectively, and the Ph.D. degree in control theory\nand control engineering from Chinese Academy of\nSciences, Beijing, China, in 2010. He is an Assistant\nProfessor with State Key Laboratory of Manage-\nment and Control for Complex Systems, Institute of\nAutomation, Chinese Academy of Sciences. His re-\nsearch interests include traffic data analysis, dynamic\ntraffic modeling, and parallel traffic management and\ncontrol systems.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\nXinglin Piao received the Ph.D. degree from the\nBeijing University of Technology, Beijing, China,\nin 2017. He is currently a lecturer in the Faculty\nof Information Technology at Beijing University of\nTechnology. His research interests include intelligent\ntraffic, pattern recognition, and multimedia technol-\nogy.\nBaocai Yin (Member, IEEE) received the B.S., M.S.,\nand Ph.D. degrees in computational mathematics\nfrom the Dalian University of Technology, Dalian,\nChina, in 1985, 1988, and 1993, respectively. He is\ncurrently a Professor with the Beijing Key Labora-\ntory of Multimedia and Intelligent Software Tech-\nnology, Faculty of Information Technology, Beijing\nUniversity of Technology. His research interests in-\nclude multimedia, image processing, computer vi-\nsion, and pattern recognition.\n",
    "2205.09991": "Planning with Diffusion for Flexible Behavior Synthesis\nMichael Janner * 1 Yilun Du * 2 Joshua B. Tenenbaum 2 Sergey Levine 1\nAbstract\nModel-based reinforcement learning methods\noften use learning only for the purpose of\nestimating an approximate dynamics model,\nof\ufb02oading the rest of the decision-making\nwork to classical trajectory optimizers. While\nconceptually simple, this combination has a\nnumber of empirical shortcomings, suggesting\nthat learned models may not be well-suited to\nstandard trajectory optimization. In this paper,\nwe consider what it would look like to fold as\nmuch of the trajectory optimization pipeline as\npossible into the modeling problem, such that\nsampling from the model and planning with it\nbecome nearly identical. The core of our technical\napproach lies in a diffusion probabilistic model\nthat plans by iteratively denoising trajectories.\nWe show how classi\ufb01er-guided sampling and\nimage inpainting can be reinterpreted as coherent\nplanning strategies, explore the unusual and\nuseful properties of diffusion-based planning\nmethods, and demonstrate the effectiveness of our\nframework in control settings that emphasize long-\nhorizon decision-making and test-time \ufb02exibility.\n1\nIntroduction\nPlanning with a learned model is a conceptually simple\nframework for reinforcement learning and data-driven\ndecision-making. Its appeal comes from employing learning\ntechniques only where they are the most mature and\neffective: for the approximation of unknown environment\ndynamics in what amounts to a supervised learning problem.\nAfterwards, the learned model may be plugged into classical\ntrajectory optimization routines (Tassa et al., 2012; Posa\net al., 2014; Kelly, 2017), which are similarly well-\nunderstood in their original context.\n*Equal contribution\n1University of California, Berkeley 2MIT.\nCorrespondence to: janner@berkeley.edu, yilundu@mit.edu.\nProceedings of the 39 th International Conference on Machine\nLearning, Baltimore, Maryland, USA, PMLR 162, 2022.\nCopyright 2022 by the author(s).\np\u03b8(\u03c4 i\u22121 |\u03c4 i)\nq(\u03c4 i |\u03c4 i\u22121)\ndenoising\ndiffusion\nFigure 1. Diffuser is a diffusion probabilistic model that plans by\niteratively re\ufb01ning trajectories.\nHowever, this combination rarely works as described.\nBecause powerful trajectory optimizers exploit learned\nmodels, plans generated by this procedure often look more\nlike adversarial examples than optimal trajectories (Talvitie,\n2014; Ke et al., 2018). As a result, contemporary model-\nbased reinforcement learning algorithms often inherit more\nfrom model-free methods, such as value functions and policy\ngradients (Wang et al., 2019), than from the trajectory\noptimization toolbox.\nThose methods that do rely on\nonline planning tend to use simple gradient-free trajectory\noptimization routines like random shooting (Nagabandi\net al., 2018) or the cross-entropy method (Botev et al., 2013;\nChua et al., 2018) to avoid the aforementioned issues.\nIn this work, we propose an alternative approach to data-\ndriven trajectory optimization. The core idea is to train a\nmodel that is directly amenable to trajectory optimization,\nin the sense that sampling from the model and planning\nwith it become nearly identical. This goal requires a shift\nin how the model is designed. Because learned dynamics\nmodels are normally meant to be proxies for environment\ndynamics, improvements are often achieved by structuring\nthe model according to the underlying causal process (Bapst\net al., 2019). Instead, we consider how to design a model\nin line with the planning problem in which it will be used.\nFor example, because the model will ultimately be used\nfor planning, action distributions are just as important as\nstate dynamics and long-horizon accuracy is more important\nthan single-step error. On the other hand, the model should\nremain agnostic to reward function so that it may be used\nCode and visualizations of the learned denoising process are\navailable at diffusion-planning.github.io.\narXiv:2205.09991v2  [cs.LG]  21 Dec 2022\nPlanning with Diffusion for Flexible Behavior Synthesis\nplanning horizon\ndenoising\nDiffuser\nlocal receptive field\nFigure 2. Diffuser samples plans by iteratively denoising two-\ndimensional arrays consisting of a variable number of state-action\npairs. A small receptive \ufb01eld constrains the model to only enforce\nlocal consistency during a single denoising step. By composing\nmany denoising steps together, local consistency can drive global\ncoherence of a sampled plan. An optional guide function J can\nbe used to bias plans toward those optimizing a test-time objective\nor satisfying a set of constraints.\nin multiple tasks, including those unseen during training.\nFinally, the model should be designed so that its plans, and\nnot just its predictions, improve with experience and are\nresistant to the myopic failure modes of standard shooting-\nbased planning algorithms.\nWe instantiate this idea as a trajectory-level diffusion\nprobabilistic model (Sohl-Dickstein et al., 2015; Ho et al.,\n2020) called Diffuser, visualized in Figure 2. Whereas\nstandard model-based planning techniques predict forward\nin time autoregressively, Diffuser predicts all timesteps of\na plan simultaneously. The iterative sampling process of\ndiffusion models leads to \ufb02exible conditioning, allowing\nfor auxiliary guides to modify the sampling procedure to\nrecover trajectories with high return or satisfying a set\nof constraints. This formulation of data-driven trajectory\noptimization has several appealing properties:\nLong-horizon scalability\nDiffuser is trained for the\naccuracy of its generated trajectories rather than its single-\nstep error, so it does not suffer from the compounding\nrollout errors of single-step dynamics models and scales\nmore gracefully with respect to long planning horizon.\nTask compositionality Reward functions provide auxiliary\ngradients to be used while sampling a plan, allowing for\na straightforward way of planning by composing multiple\nrewards simultaneously by adding together their gradients.\nTemporal compositionality\nDiffuser generates globally\ncoherent\ntrajectories\nby\niteratively\nimproving\nlocal\nconsistency, allowing it to generalize to novel trajectories\nby stitching together in-distribution subsequences.\nEffective non-greedy planning\nBy blurring the line\nbetween model and planner, the training procedure that\nimproves the model\u2019s predictions also has the effect of\nimproving its planning capabilities. This design yields a\nlearned planner that can solve the types of long-horizon,\nsparse-reward problems that prove dif\ufb01cult for many\nconventional planning methods.\nThe core contribution of this work is a denoising diffusion\nmodel designed for trajectory data and an associated\nprobabilistic framework for behavior synthesis.\nWhile\nunconventional compared to the types of models routinely\nused in deep model-based reinforcement learning, we\ndemonstrate that Diffuser has a number of useful properties\nand is particularly effective in of\ufb02ine control settings that\nrequire long-horizon reasoning and test-time \ufb02exibility.\n2\nBackground\nOur approach to planning is a learning-based analogue\nof past work in behavioral synthesis using trajectory\noptimization (Witkin & Kass, 1988; Tassa et al., 2012). In\nthis section, we provide a brief background on the problem\nsetting considered by trajectory optimization and the class\nof generative models we employ for that problem.\n2.1\nProblem Setting\nConsider a system governed by the discrete-time dynamics\nst+1 = f(st, at) at state st given an action at. Trajectory\noptimization refers to \ufb01nding a sequence of actions a\u2217\n0:T\nthat maximizes (or minimizes) an objective J factorized\nover per-timestep rewards (or costs) r(st, at):\na\u2217\n0:T = arg max\na0:T\nJ (s0, a0:T ) = arg max\na0:T\nT\nX\nt=0\nr(st, at)\nwhere T is the planning horizon. We use the abbreviation\n\u03c4 = (s0, a0, s1, a1, . . . , sT , aT ) to refer to a trajectory\nof interleaved states and actions and J (\u03c4 ) to denote the\nobjective value of that trajectory.\n2.2\nDiffusion Probabilistic Models\nDiffusion probabilistic models (Sohl-Dickstein et al., 2015;\nHo et al., 2020) pose the data-generating process as an\niterative denoising procedure p\u03b8(\u03c4 i\u22121 | \u03c4 i). This denoising\nis the reverse of a forward diffusion process q(\u03c4 i | \u03c4 i\u22121)\nthat slowly corrupts the structure in data by adding noise.\nThe data distribution induced by the model is given by:\np\u03b8(\u03c4 0) =\nZ\np(\u03c4 N)\nN\nY\ni=1\np\u03b8(\u03c4 i\u22121 | \u03c4 i)d\u03c4 1:N\nwhere p(\u03c4 N) is a standard Gaussian prior and \u03c4 0 denotes\n(noiseless) data. Parameters \u03b8 are optimized by minimizing\nPlanning with Diffusion for Flexible Behavior Synthesis\na variational bound on the negative log likelihood of the\nreverse process: \u03b8\u2217= arg min\u03b8 \u2212E\u03c4 0 \u0002\nlog p\u03b8(\u03c4 0)\n\u0003\n. The\nreverse process is often parameterized as Gaussian with\n\ufb01xed timestep-dependent covariances:\np\u03b8(\u03c4 i\u22121 | \u03c4 i) = N(\u03c4 i\u22121 | \u00b5\u03b8(\u03c4 i, i), \u03a3i).\nThe forward process q(\u03c4 i | \u03c4 i\u22121) is typically prespeci\ufb01ed.\nNotation.\nThere are two \u201ctimes\u201d at play in this work: that\nof the diffusion process and that of the planning problem.\nWe use superscripts (i when unspeci\ufb01ed) to denote diffusion\ntimestep and subscripts (t when unspeci\ufb01ed) to denote\nplanning timestep. For example, s0\nt refers to the tth state in a\nnoiseless trajectory. When it is unambiguous from context,\nsuperscripts of noiseless quantities are omitted: \u03c4 = \u03c4 0.\nWe overload notation slightly by referring to the tth state (or\naction) in a trajectory \u03c4 as \u03c4st (or \u03c4at).\n3\nPlanning with Diffusion\nA major obstacle to using trajectory optimization techniques\nis that they require knowledge of the environment dynamics\nf. Most learning-based methods attempt to overcome this\nobstacle by training an approximate dynamics model and\nplugging it in to a conventional planning routine. However,\nlearned models are often poorly suited to the types of\nplanning algorithms designed with ground-truth models in\nmind, leading to planners that exploit learned models by\n\ufb01nding adversarial examples.\nWe propose a tighter coupling between modeling and\nplanning. Instead of using a learned model in the context\nof a classical planner, we subsume as much of the planning\nprocess as possible into the generative modeling framework,\nsuch that planning becomes nearly identical to sampling. We\ndo this using a diffusion model of trajectories, p\u03b8(\u03c4 ). The\niterative denoising process of a diffusion model lends itself\nto \ufb02exible conditioning by way of sampling from perturbed\ndistributions of the form:\n\u02dcp\u03b8(\u03c4 ) \u221dp\u03b8(\u03c4 )h(\u03c4 ).\n(1)\nThe function h(\u03c4 ) can contain information about prior\nevidence (such as an observation history), desired outcomes\n(such as a goal to reach), or general functions to optimize\n(such as rewards or costs).\nPerforming inference in\nthis perturbed distribution can be seen as a probabilistic\nanalogue to the trajectory optimization problem posed\nin Section 2.1, as it requires \ufb01nding trajectories that are\nboth physically realistic under p\u03b8(\u03c4 ) and high-reward (or\nconstraint-satisfying) under h(\u03c4 ). Because the dynamics\ninformation is separated from the perturbation distribution\nh(\u03c4 ), a single diffusion model p\u03b8(\u03c4 ) may be reused for\nmultiple tasks in the same environment.\nIn this section, we describe Diffuser, a diffusion model\ndesigned for learned trajectory optimization.\nWe then\ndiscuss two speci\ufb01c instantiations of planning with Diffuser,\nrealized as reinforcement learning counterparts to classi\ufb01er-\nguided sampling and image inpainting.\n3.1\nA Generative Model for Trajectory Planning\nTemporal ordering.\nBlurring the line between sampling\nfrom a trajectory model and planning with it yields\nan unusual constraint: we can no longer predict states\nautoregressively in temporal order.\nConsider the goal-\nconditioned inference p(s1 | s0, sT ); the next state s1\ndepends on a future state as well as a prior one.\nThis\nexample is an instance of a more general principle: while\ndynamics prediction is causal, in the sense that the present\nis determined by the past, decision-making and control can\nbe anti-causal, in the sense that decisions in the present\nare conditional on the future.1 Because we cannot use a\ntemporal autoregressive ordering, we design Diffuser to\npredict all timesteps of a plan concurrently.\nTemporal locality.\nDespite not being autoregressive or\nMarkovian, Diffuser features a relaxed form of temporal\nlocality. In Figure 2, we depict a dependency graph for a\ndiffusion model consisting of a single temporal convolution.\nThe receptive \ufb01eld of a given prediction only consists of\nnearby timesteps, both in the past and the future. As a\nresult, each step of the denoising process can only make\npredictions based on local consistency of the trajectory. By\ncomposing many of these denoising steps together, however,\nlocal consistency can drive global coherence.\nTrajectory representation.\nDiffuser is a model of\ntrajectories designed for planning, meaning that the\neffectiveness of the controller derived from the model is\njust as important as the quality of the state predictions.\nAs a result, states and actions in a trajectory are predicted\njointly; for the purposes of prediction the actions are simply\nadditional dimensions of the state. Speci\ufb01cally, we represent\ninputs (and outputs) of Diffuser as a two-dimensional array:\n\u03c4 =\n\u0014s0\ns1\n...\nsT\na0\na1\naT\n\u0015\n.\n(2)\nwith one column per timestep of the planning horizon.\nArchitecture.\nWe now have the ingredients needed to\nspecify a Diffuser architecture: (1) an entire trajectory\nshould be predicted non-autoregressively, (2) each step\nof the denoising process should be temporally local,\n1In general reinforcement learning contexts, conditioning on the\nfuture emerges from the assumption of future optimality for the\npurpose of writing a dynamic programming recursion. Concretely,\nthis appears as the future optimality variables Ot:T in the action\ndistribution log p(at | st, Ot:T ) (Levine, 2018).\nPlanning with Diffusion for Flexible Behavior Synthesis\nAlgorithm 1 Guided Diffusion Planning\n1: Require Diffuser \u00b5\u03b8, guide J , scale \u03b1, covariances \u03a3i\n2: while not done do\n3:\nObserve state s; initialize plan \u03c4 N \u223cN(0, I)\n4:\nfor i = N, . . . , 1 do\n5:\n// parameters of reverse transition\n6:\n\u00b5 \u2190\u00b5\u03b8(\u03c4 i)\n7:\n// guide using gradients of return\n8:\n\u03c4 i\u22121 \u223cN(\u00b5 + \u03b1\u03a3\u2207J (\u00b5), \u03a3i)\n9:\n// constrain first state of plan\n10:\n\u03c4 i\u22121\ns0\n\u2190s\n11:\nExecute \ufb01rst action of plan \u03c4 0\na0\nand (3) the trajectory representation should allow for\nequivariance along one dimension (the planning horizon)\nbut not the other (the state and action features).\nWe\nsatisfy these criteria with a model consisting of repeated\n(temporal) convolutional residual blocks.\nThe overall\narchitecture resembles the types of U-Nets that have\nfound success in image-based diffusion models, but with\ntwo-dimensional spatial convolutions replaced by one-\ndimensional temporal convolutions (Figure A1). Because\nthe model is fully convolutional, the horizon of the\npredictions is determined not by the model architecture,\nbut by the input dimensionality; it can change dynamically\nduring planning if desired.\nTraining.\nWe use Diffuser to parameterize a learned\ngradient \u03f5\u03b8(\u03c4 i, i) of the trajectory denoising process, from\nwhich the mean \u00b5\u03b8 can be solved in closed form (Ho et al.,\n2020). We use the simpli\ufb01ed objective for training the \u03f5-\nmodel, given by:\nL(\u03b8) = Ei,\u03f5,\u03c4 0 \u0002\n\u2225\u03f5 \u2212\u03f5\u03b8(\u03c4 i, i)\u22252\u0003\n,\nin which i \u223cU{1, 2, . . . , N} is the diffusion timestep,\n\u03f5 \u223cN(0, I) is the noise target, and \u03c4 i is the trajectory\n\u03c4 0 corrupted with noise \u03f5. Reverse process covariances \u03a3i\nfollow the cosine schedule of Nichol & Dhariwal (2021).\n3.2\nReinforcement Learning as Guided Sampling\nIn order to solve reinforcement learning problems with\nDiffuser, we must introduce a notion of reward.\nWe\nappeal to the control-as-inference graphical model (Levine,\n2018) to do so.\nLet Ot be a binary random variable\ndenoting the optimality of timestep t of a trajectory, with\np(Ot = 1) = exp(r(st, at)). We can sample from the set\nof optimal trajectories by setting h(\u03c4 ) = p(O1:T | \u03c4 ) in\nEquation 1:\n\u02dcp\u03b8(\u03c4 ) = p(\u03c4 | O1:T = 1) \u221dp(\u03c4 )p(O1:T = 1 | \u03c4 ).\nWe have exchanged the reinforcement learning problem for\none of conditional sampling. Thankfully, there has been\nmuch prior work on conditional sampling with diffusion\nmodels.\nWhile it is intractable to sample from this\ndistribution exactly, when p(O1:T\n| \u03c4 i) is suf\ufb01ciently\nsmooth, the reverse diffusion process transitions can be\napproximated as Gaussian (Sohl-Dickstein et al., 2015):\np\u03b8(\u03c4 i\u22121 | \u03c4 i, O1:T ) \u2248N(\u03c4 i\u22121; \u00b5 + \u03a3g, \u03a3)\n(3)\nwhere \u00b5, \u03a3 are the parameters of the original reverse process\ntransition p\u03b8(\u03c4 i\u22121 | \u03c4 i) and\ng = \u2207\u03c4 log p(O1:T | \u03c4 )|\u03c4 =\u00b5\n=\nT\nX\nt=0\n\u2207st,atr(st, at)|(st,at)=\u00b5t = \u2207J (\u00b5).\nThis\nrelation\nprovides\na\nstraightforward\ntranslation\nbetween classi\ufb01er-guided sampling, used to generate class-\nconditional images (Dhariwal & Nichol, 2021), and the\nreinforcement learning problem setting. We \ufb01rst train a\ndiffusion model p\u03b8(\u03c4 ) on the states and actions of all\navailable trajectory data. We then train a separate model\nJ\u03c6 to predict the cumulative rewards of trajectory samples\n\u03c4 i. The gradients of J\u03c6 are used to guide the trajectory\nsampling procedure by modifying the means \u00b5 of the reverse\nprocess according to Equation 3.\nThe \ufb01rst action of a\nsampled trajectory \u03c4 \u223cp(\u03c4 | O1:T = 1) may be executed\nin the environment, after which the planning procedure\nbegins again in a standard receding-horizon control loop.\nPseudocode for the guided planning method is given in\nAlgorithm 1.\n3.3\nGoal-Conditioned RL as Inpainting\nSome planning problems are more naturally posed as\nconstraint satisfaction than reward maximization. In these\nsettings, the objective is to produce any feasible trajectory\nthat satis\ufb01es a set of constraints, such as terminating at\na goal location. Appealing to the two-dimensional array\nrepresentation of trajectories described by Equation 2,\nthis setting can be translated into an inpainting problem,\nin which state and action constraints act analogously to\nobserved pixels in an image (Sohl-Dickstein et al., 2015).\nAll unobserved locations in the array must be \ufb01lled in by the\ndiffusion model in a manner consistent with the observed\nconstraints.\nThe perturbation function required for this task is a\nDirac delta for observed values and constant elsewhere.\nConcretely, if ct is state constraint at timestep t, then\nh(\u03c4 ) = \u03b4ct(s0, a0, . . . , sT , aT ) =\n(\n+\u221e\nif ct = st\n0\notherwise\nThe de\ufb01nition for action constraints is identical.\nIn\npractice, this may be implemented by sampling from\nPlanning with Diffusion for Flexible Behavior Synthesis\na\ndenoising\nb\ndata\nplan\nc\n\u03c4 N\n\u2192\n\u03c4 N\n\u2192\nd\nreward \u2212\n+\nplan\nFigure 3. (Properties of diffusion planners) (a) Learned long-horizon planning: Diffuser\u2019s learned planning procedure does not suffer\nfrom the myopic failure modes common to shooting algorithms and is able to plan over long horizons with sparse reward. (b) Temporal\ncompositionality: Even though the model is not Markovian, it generates trajectories via iterated re\ufb01nements to local consistency. As a\nresult, it exhibits the types of generalization usually associated with Markovian models, with the ability to stitch together snippets of\ntrajectories from the training data to generate novel plan. (c) Variable-length plans: Despite being a trajectory-level model, Diffuser\u2019s\nplanning horizon is not determined by its architecture. The horizon can be updated after training by changing the dimensionality of the\ninput noise. (d) Task compositionality: Diffuser can be composed with new reward functions to plan for tasks unseen during training. In\nall sub\ufb01gures,\ndenotes a starting state and\ndenotes a goal state.\nthe unperturbed reverse process \u03c4 i\u22121 \u223cp\u03b8(\u03c4 i\u22121 | \u03c4 i) and\nreplacing the sampled values with conditioning values ct\nafter all diffusion timesteps i \u2208{0, 1, . . . , N}.\nEven reward maximization problems require conditioning-\nby-inpainting because all sampled trajectories should begin\nat the current state. This conditioning is described by line\n10 in Algorithm 1.\n4\nProperties of Diffusion Planners\nWe discuss a number of Diffuser\u2019s important properties,\nfocusing on those that are are either distinct from\nstandard dynamics models or unusual for non-autoregressive\ntrajectory prediction.\nLearned long-horizon planning.\nSingle-step models\nare typically used as proxies for ground-truth environment\ndynamics f, and as such are not tied to any planning\nalgorithm in particular. In contrast, the planning routine\nin Algorithm 1 is closely tied to the speci\ufb01c affordances\nof diffusion models.\nBecause our planning method is\nnearly identical to sampling (with the only difference\nbeing guidance by a perturbation function h(\u03c4 )), Diffuser\u2019s\neffectiveness as a long-horizon predictor directly translates\nto effective long-horizon planning. We demonstrate the\nbene\ufb01ts of learned planning in a goal-reaching setting in\nFigure 3a, showing that Diffuser is able to generate feasible\ntrajectories in the types of sparse reward settings where\nshooting-based approaches are known to struggle.\nWe\nexplore a more quantitative version of this problem setting\nin Section 5.1.\nTemporal compositionality. Single-step models are often\nmotivated using the Markov property, allowing them to\ncompose in-distribution transitions to generalize to out-\nof-distribution trajectories.\nBecause Diffuser generates\nglobally coherent trajectories by iteratively improving local\nconsistency (Section 3.1), it can also stitch together familiar\nsubsequences in novel ways. In Figure 3b, we train Diffuser\non trajectories that only travel in a straight line, and show\nthat it can generalize to v-shaped trajectories by composing\ntrajectories at their point of intersection.\nVariable-length plans.\nBecause our model is fully\nconvolutional in the horizon dimension of its prediction,\nits planning horizon is not speci\ufb01ed by architectural choices.\nInstead, it is determined by the size of the input noise \u03c4 N \u223c\nN(0, I) that initializes the denoising process, allowing for\nvariable-length plans (Figure 3c).\nTask compositionality.\nWhile Diffuser contains\ninformation\nabout\nboth\nenvironment\ndynamics\nand\nbehaviors, it is independent of reward function. Because the\nmodel acts as a prior over possible futures, planning can be\nguided by comparatively lightweight perturbation functions\nh(\u03c4 ) (or even combinations of multiple perturbations)\ncorresponding to different rewards. We demonstrate this by\nplanning for a new reward function unseen during training\nof the diffusion model (Figure 3d).\nPlanning with Diffusion for Flexible Behavior Synthesis\nEnvironment\nMPPI\nCQL\nIQL\nDiffuser\nMaze2D\nU-Maze\n33.2\n5.7\n47.4\n113.9 \u00b13.1\nMaze2D\nMedium\n10.2\n5.0\n34.9\n121.5 \u00b12.7\nMaze2D\nLarge\n5.1\n12.5\n58.6\n123.0 \u00b16.4\nSingle-task Average\n16.2\n7.7\n47.0\n119.5\nMulti2D\nU-Maze\n41.2\n-\n24.8\n128.9 \u00b11.8\nMulti2D\nMedium\n15.4\n-\n12.1\n127.2 \u00b13.4\nMulti2D\nLarge\n8.0\n-\n13.9\n132.1 \u00b15.8\nMulti-task Average\n21.5\n-\n16.9\n129.4\nTable 1. (Long-horizon planning) The performance of Diffuser\nand prior model-free algorithms in the Maze2D environment,\nwhich tests long-horizon planning due to its sparse reward\nstructure.\nThe Multi2D setting refers to a multi-task variant\nwith goal locations resampled at the beginning of every episode.\nDiffuser substantially outperforms prior approaches in both\nsettings. Appendix A details the sources for the scores of the\nbaseline algorithms.\n5\nExperimental Evaluation\nThe focus of our experiments is to evaluate Diffuser on\nthe capabilities we would like from a data-driven planner.\nIn particular, we evaluate (1) the ability to plan over long\nhorizons without manual reward shaping, (2) the ability to\ngeneralize to new con\ufb01gurations of goals unseen during\ntraining, and (3) the ability to recover an effective controller\nfrom heterogeneous data of varying quality. We conclude by\nstudying practical runtime considerations of diffusion-based\nplanning, including the most effective ways of speeding up\nthe planning procedure while suffering minimally in terms\nof performance.\n5.1\nLong Horizon Multi-Task Planning\nWe evaluate long-horizon planning in the Maze2D\nenvironments (Fu et al., 2020), which require traversing\nto a goal location where a reward of 1 is given. No reward\nshaping is provided at any other location. Because it can\ntake hundreds of steps to reach the goal location, even the\nbest model-free algorithms struggle to adequately perform\ncredit assignment and reliably reach the goal (Table 1).\nWe plan with Diffuser using the inpainting strategy to\ncondition on a start and goal location. (The goal location is\nalso available to the model-free methods; it is identi\ufb01able\nby being the only state in the dataset with non-zero\nreward.) We then use the sampled trajectory as an open-loop\nplan. Diffuser achieves scores over 100 in all maze sizes,\nindicating that it outperforms a reference expert policy. We\nvisualize the reverse diffusion process generating Diffuser\u2019s\nplans in Figure 4.\nWhile the training data in Maze2D is undirected \u2013 consisting\nof a controller navigating to and from randomly selected\nU-Maze\nMedium\nLarge\ndenoising\nFigure 4. (Planning as inpainting) Plans are generated in the\nMaze2D environment by sampling trajectories consistent with\na speci\ufb01ed start\nand goal\ncondition. The remaining states\nare \u201cinpainted\u201d by the denoising process.\nlocations \u2013 the evaluation is single-task in that the goal\nis always the same. In order to test multi-task \ufb02exibility,\nwe modify the environment to randomize the goal location\nat the beginning of each episode. This setting is denoted\nas Multi2D in Table 1. Diffuser is naturally a multi-task\nplanner; we do not need to retrain the model from the\nsingle-task experiments and simply change the conditioning\ngoal. As a result, Diffuser performs as well in the multi-\ntask setting as in the single-task setting. In contrast, there\nis a substantial performance drop of the best model-free\nalgorithm in the single-task setting (IQL; Kostrikov et al.\n2022) when adapted to the multi-task setting. Details of\nour multi-task IQL with hindsight experience relabeling\n(Andrychowicz et al., 2017) are provided in Appendix A.\nMPPI uses the ground-truth dynamics; its poor performance\ncompared to the learned planning algorithm of Diffuser\nhighlights the dif\ufb01culty posed by long-horizon planning\neven when there are no prediction inaccuracies.\n5.2\nTest-time Flexibility\nIn order to evaluate the ability to generalize to new test-time\ngoals, we construct a suite of block stacking tasks with three\nsettings: (1) Unconditional Stacking, for which the task is\nto build a block tower as tall as possible; (2) Conditional\nStacking, for which the task is to construct a block tower\nwith a speci\ufb01ed order of blocks, and (3) Rearrangement,\nfor which the task is to match a set of reference blocks\u2019\nlocations in a novel arrangement. We train all methods\non 10000 trajectories from demonstrations generated by\nPDDLStream (Garrett et al., 2020); rewards are equal to\none upon successful stack placements and zero otherwise.\nThese block stacking are challenging diagnostics of test-\nPlanning with Diffusion for Flexible Behavior Synthesis\nEnvironment\nBCQ\nCQL\nDiffuser\nUnconditional Stacking\n0.0\n24.4\n58.7 \u00b12.5\nConditional Stacking\n0.0\n0.0\n45.6 \u00b13.1\nRearrangement\n0.0\n0.0\n58.9 \u00b13.4\nAverage\n0.0\n8.1\n54.4\nTable 3. (Test-time \ufb02exibility) Performance of BCQ, CQL, and\nDiffuser on block stacking tasks. A score of 100 corresponds to a\nperfectly executed stack; 0 is that of a random policy.\ntime \ufb02exibility; in the course of executing a partial stack\nfor a randomized goal, a controller will venture into novel\nstates not included in the training con\ufb01guration.\nWe use one trained Diffuser for all block-stacking tasks, only\nmodifying the perturbation function h(\u03c4 ) between settings.\nIn the Unconditional Stacking task, we directly sample\nfrom the unperturbed denoising process p\u03b8(\u03c4 ) to emulate\nthe PDDLStream controller. In the Conditional Stacking\nand Rearrangement tasks, we compose two perturbation\nfunctions h(\u03c4 ) to bias the sampled trajectories: the \ufb01rst\nmaximizes the likelihood of the trajectory\u2019s \ufb01nal state\nmatching the goal con\ufb01guration, and the second enforces\na contact constraint between the end effector and a cube\nduring stacking motions. (See Appendix B for details.)\nWe compare with two prior model-free of\ufb02ine reinforcement\nlearning algorithms: BCQ (Fujimoto et al., 2019) and\nCQL (Kumar et al., 2020), training standard variants for\nUnconditional Stacking and goal-conditioned variants for\nConditional Stacking and Rearrangement. (Baseline details\nare provided in Appendix A.) Quantitative results are given\nin Table 3, in which a score of 100 corresponds to a perfect\nexecution of the task. Diffuser substantially outperforms\nboth prior methods, with the conditional settings requiring\n\ufb02exible behavior generation proving especially dif\ufb01cult\nfor the model-free algorithms. A visual depiction of an\nexecution by Diffuser is provided in Figure 5.\n\u2192\nFigure 5. (Block stacking) A block stacking sequence executed\nby Diffuser. This task is best illustrated by videos viewable at\ndiffusion-planning.github.io.\n5.3\nOf\ufb02ine Reinforcement Learning\nFinally, we evaluate the capacity to recover an effective\nsingle-task controller from heterogeneous data of varying\nquality using the D4RL of\ufb02ine locomotion suite (Fu et al.,\n2020). We guide the trajectories generated by Diffuser\ntoward high-reward regions using the sampling procedure\ndescribed in Section 3.2 and condition the trajectories on\nthe current state using the inpainting procedure described in\nSection 3.3. The reward predictor J\u03c6 is trained on the same\ntrajectories as the diffusion model.\nWe compare to a variety of prior algorithms spanning\nother approaches to data-driven control, including the\nmodel-free reinforcement learning algorithms CQL (Kumar\net al., 2020) and IQL (Kostrikov et al., 2022); return-\nconditioning approaches like Decision Transformer (DT;\nChen et al. 2021b); and model-based reinforcement learning\napproaches including Trajectory Transformer (TT; Janner\net al. 2021), MOPO (Yu et al., 2020), MOReL (Kidambi\net al., 2020), and MBOP (Argenson & Dulac-Arnold, 2021).\nIn the single-task setting, Diffuser performance comparably\nto prior algorithms: better than the model-based MOReL\nand MBOP and return-conditioning DT, but worse than\nDataset\nEnvironment\nBC\nCQL\nIQL\nDT\nTT\nMOPO\nMOReL\nMBOP\nDiffuser\nMedium-Expert\nHalfCheetah\n55.2\n91.6\n86.7\n86.8\n95.0\n63.3\n53.3\n105.9\n88.9 \u00b10.3\nMedium-Expert\nHopper\n52.5\n105.4\n91.5\n107.6\n110.0\n23.7\n108.7\n55.1\n103.3 \u00b11.3\nMedium-Expert\nWalker2d\n107.5\n108.8\n109.6\n108.1\n101.9\n44.6\n95.6\n70.2\n106.9 \u00b10.2\nMedium\nHalfCheetah\n42.6\n44.0\n47.4\n42.6\n46.9\n42.3\n42.1\n44.6\n42.8 \u00b10.3\nMedium\nHopper\n52.9\n58.5\n66.3\n67.6\n61.1\n28.0\n95.4\n48.8\n74.3 \u00b11.4\nMedium\nWalker2d\n75.3\n72.5\n78.3\n74.0\n79.0\n17.8\n77.8\n41.0\n79.6 \u00b10.55\nMedium-Replay\nHalfCheetah\n36.6\n45.5\n44.2\n36.6\n41.9\n53.1\n40.2\n42.3\n37.7 \u00b10.5\nMedium-Replay\nHopper\n18.1\n95.0\n94.7\n82.7\n91.5\n67.5\n93.6\n12.4\n93.6 \u00b10.4\nMedium-Replay\nWalker2d\n26.0\n77.2\n73.9\n66.6\n82.6\n39.0\n49.8\n9.7\n70.6 \u00b11.6\nAverage\n51.9\n77.6\n77.0\n74.7\n78.9\n42.1\n72.9\n47.8\n77.5\nTable 2. (Of\ufb02ine reinforcement learning) The performance of Diffuser and a variety of prior algorithms on the D4RL locomotion\nbenchmark (Fu et al., 2020). Results for Diffuser correspond to the mean and standard error over 150 planning seeds. We detail the\nsources for the performance of prior methods in Appendix A.3. Following Kostrikov et al. (2022), we emphasize in bold scores within 5\npercent of the maximum per task (\u22650.95 \u00b7 max).\nPlanning with Diffusion for Flexible Behavior Synthesis\ndenoising\nplanning horizon\nFigure 6. (Guided sampling) Diffuser generates all timesteps of\na plan concurrently, instead of autoregressively, through the\ndenoising process.\nthe best of\ufb02ine techniques designed speci\ufb01cally for single-\ntask performance. We also investigated a variant using\nDiffuser as a dynamics model in conventional trajectory\noptimizers such as MPPI (Williams et al., 2015), but found\nthat this combination performed no better than random,\nsuggesting that the effectiveness of Diffuser stems from\ncoupled modeling and planning, and not from improved\nopen-loop predictive accuracy.\n5.4\nWarm-Starting Diffusion for Faster Planning\nA limitation of Diffuser is that individual plans are slow\nto generate (due to iterative generation). Na\u00a8\u0131vely, as we\nexecute plans open loop, a new plan must be regenerated\nat each step of execution. To improve execution speed of\nDiffuser, we may further reuse previously generated plans\nto warm-start generations of subsequent plans.\nTo warm-start planning, we may run a limited number of\nforward diffusion steps from a previously generated plan and\nthen run a corresponding number of denoising steps from\nthis partially noised trajectory to regenerate an updated plan.\nIn Figure 7, we illustrate the trade-off between performance\nand runtime budget as we vary the underlying number of\ndenoising steps used to regenerate each a new plan from 2 to\n100. We \ufb01nd that we may reduce the planning budget of our\napproach markedly with only modest drop in performance.\n6\nRelated Work\nAdvances in deep generative modeling have recently\nmade inroads into model-based reinforcement learning,\nwith multiple lines of work exploring dynamics models\nparameterized as convolutional U-networks (Kaiser et al.,\n2020), stochastic recurrent networks (Ke et al., 2018;\n10\u22121\n100\nplanning budget (seconds)\n75\n85\n95\n105\nnormalized score (a.u.)\nPerformance versus runtime\nFigure 7. (Fast Planning) Performance of Diffuser on Walker2d\nMedium-Expert when varying the number of diffusion steps to\nwarm-start planning. Performance suffers only minimally even\nwhen using one-tenth the number of diffusion steps, as long as\nplans are initialized from the previous timestep\u2019s plan.\nHafner et al., 2021a; Ha & Schmidhuber, 2018), vector-\nquantized autoencoders (Hafner et al., 2021b; Ozair et al.,\n2021), neural ODEs (Du et al., 2020a), normalizing \ufb02ows\n(Rhinehart et al., 2020; Janner et al., 2020), generative\nadversarial networks (Eysenbach et al., 2021), energy-based\nmodels (EBMs; Du et al. 2019), graph neural networks\n(Sanchez-Gonzalez et al., 2018), neural radiance \ufb01elds (Li\net al., 2021), and Transformers (Janner et al., 2021; Chen\net al., 2021a). Further, Lambert et al. 2020 have studied\nnon-autoregressive trajectory-level dynamics models for\nlong-horizon prediction. These investigations generally\nassume an abstraction barrier between the model and\nplanner. Speci\ufb01cally, the role of learning is relegated to\napproximating environment dynamics; once learning is\ncomplete the model may be inserted into any of a variety\nof planning (Botev et al., 2013; Williams et al., 2015)\nor policy optimization (Sutton, 1990; Wang et al., 2019)\nalgorithms because the form of the planner does not depend\nstrongly on the form of the model. Our goal is to break\nthis abstraction barrier by designing a model and planning\nalgorithm that are trained alongside one another, resulting\nin a non-autoregressive trajectory-level model for which\nsampling and planning are nearly identical.\nA number of parallel lines of work have studied how\nto break the abstraction barrier between model learning\nand planning in different ways.\nApproaches include\ntraining an autoregressive latent-space model for reward\nprediction (Tamar et al., 2016; Oh et al., 2017; Schrittwieser\net al., 2019); weighing model training objectives by state\nvalues (Farahmand et al., 2017); and applying collocation\ntechniques to learned single-step energies (Du et al., 2019;\nRybkin et al., 2021).\nIn contrast, our method plans\nby modeling and generating all timesteps of a trajectory\nPlanning with Diffusion for Flexible Behavior Synthesis\nconcurrently, instead of autoregressively, and conditioning\nthe sampled trajectories with auxiliary guidance functions.\nDiffusion models have emerged as a promising class\nof generative model that formulates the data-generating\nprocess as an iterative denoising procedure (Sohl-Dickstein\net al., 2015; Ho et al., 2020). The denoising procedure\ncan be seen as parameterizing the gradients of the data\ndistribution (Song & Ermon, 2019), connecting diffusion\nmodels to score matching (Hyv\u00a8arinen, 2005) and EBMs\n(LeCun et al., 2006; Du & Mordatch, 2019; Nijkamp\net al., 2019; Grathwohl et al., 2020). Iterative, gradient-\nbased sampling lends itself towards \ufb02exible conditioning\n(Dhariwal & Nichol, 2021) and compositionality (Du\net al., 2020b), which we use to recover effective behaviors\nfrom heterogeneous datasets and plan for reward functions\nunseen during training. While diffusion models have been\ndeveloped for the generation of images (Song et al., 2021),\nwaveforms (Chen et al., 2021c), 3D shapes (Zhou et al.,\n2021), and text (Austin et al., 2021), to the best of our\nknowledge they have not previously been used in the context\nof reinforcement learning or decision-making.\n7\nConclusion\nWe have presented Diffuser, a denoising diffusion model for\ntrajectory data. Planning with Diffuser is almost identical\nto sampling from it, differing only in the addition of\nauxiliary perturbation functions that serve to guide samples.\nThe learned diffusion-based planning procedure has a\nnumber of useful properties, including graceful handling of\nsparse rewards, the ability to plan for new rewards without\nretraining, and a temporal compositionality that allows it to\nproduce out-of-distribution trajectories by stitching together\nin-distribution subsequences. Our results point to a new\nclass of diffusion-based planning procedures for deep model-\nbased reinforcement learning.\nCode References\nWe used the following open-source libraries for this work:\nNumPy (Harris et al., 2020), PyTorch (Paszke et al., 2019),\nand Diffusion Models in PyTorch (Wang, 2020).\nAcknowledgements\nWe thank Ajay Jain for feedback on an early draft and Leslie\nKaelbling, Tom\u00b4as Lozano-P\u00b4erez, Jascha Sohl-Dickstein,\nBen Eysenbach, Amy Zhang, Colin Li, and Toru Lin for\nhelpful discussions. This work was partially supported by\ncomputational resource donations from Microsoft. M.J.\nis supported by fellowships from the National Science\nFoundation and the Open Philanthropy Project.\nY.D.\nis supported by a fellowship from the National Science\nFoundation.\nReferences\nAndrychowicz, M., Wolski, F., Ray, A., Schneider, J., Fong,\nR., Welinder, P., McGrew, B., Tobin, J., Abbeel, P., and\nZaremba, W. Hindsight experience replay. In Advances\nin Neural Information Processing Systems. 2017.\nArgenson, A. and Dulac-Arnold, G. Model-based of\ufb02ine\nplanning.\nIn International Conference on Learning\nRepresentations, 2021.\nAustin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den\nBerg, R.\nStructured denoising diffusion models in\ndiscrete state-spaces. In Advances in Neural Information\nProcessing Systems, 2021.\nBapst, V., Sanchez-Gonzalez, A., Doersch, C., Stachenfeld,\nK. L., Kohli, P., Battaglia, P. W., and Hamrick,\nJ. B. Structured agents for physical construction. In\nInternational Conference on Machine Learning, 2019.\nBotev, Z. I., Kroese, D. P., Rubinstein, R. Y., and L\u2019Ecuyer, P.\nThe cross-entropy method for optimization. In Handbook\nof Statistics, volume 31, chapter 3. 2013.\nChen, C., Yoon, J., Wu, Y.-F., and Ahn, S. TransDreamer:\nReinforcement learning with transformer world models,\n2021a.\nChen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,\nLaskin, M., Abbeel, P., Srinivas, A., and Mordatch,\nI.\nDecision transformer: Reinforcement learning via\nsequence modeling. In Advances in Neural Information\nProcessing Systems, 2021b.\nChen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and\nChan, W. Wavegrad: Estimating gradients for waveform\ngeneration. In International Conference on Learning\nRepresentations, 2021c.\nChua, K., Calandra, R., McAllister, R., and Levine, S.\nDeep reinforcement learning in a handful of trials using\nprobabilistic dynamics models. In Advances in Neural\nInformation Processing Systems. 2018.\nDhariwal, P. and Nichol, A. Q. Diffusion models beat GANs\non image synthesis. In Advances in Neural Information\nProcessing Systems, 2021.\nDu, J., Futoma, J., and Doshi-Velez, F.\nModel-\nbased reinforcement learning for semi-markov decision\nprocesses with neural odes.\nIn Advances in Neural\nInformation Processing Systems, 2020a.\nDu, Y. and Mordatch, I.\nImplicit generation and\ngeneralization in energy-based models. In Advances in\nNeural Information Processing Systems, 2019.\nPlanning with Diffusion for Flexible Behavior Synthesis\nDu, Y., Lin, T., and Mordatch, I. Model based planning with\nenergy based models. In Conference on Robot Learning,\n2019.\nDu, Y., Li, S., and Mordatch, I.\nCompositional visual\ngeneration with energy based models. In Advances in\nNeural Information Processing Systems, 2020b.\nEysenbach,\nB.,\nKhazatsky,\nA.,\nLevine,\nS.,\nand\nSalakhutdinov, R. Mismatched no more: Joint model-\npolicy optimization for model-based rl. arXiv preprint\narXiv:2110.02758, 2021.\nFarahmand, A.-M., Barreto, A., and Nikovski, D. Value-\naware loss function for model-based reinforcement\nlearning.\nIn International Conference on Arti\ufb01cial\nIntelligence and Statistics, 2017.\nFu, J., Kumar, A., Nachum, O., Tucker, G., and Levine,\nS. D4RL: Datasets for deep data-driven reinforcement\nlearning. arXiv preprint arXiv:2004.07219, 2020.\nFujimoto, S., Meger, D., and Precup, D.\nOff-policy\ndeep reinforcement learning without exploration.\nIn\nInternational Conference on Machine Learning, 2019.\nGarrett, C. R., Lozano-P\u00b4erez, T., and Kaelbling, L. P.\nPddlstream: Integrating symbolic planners and blackbox\nsamplers\nvia\noptimistic\nadaptive\nplanning.\nIn\nInternational Conference on Automated Planning and\nScheduling, 2020.\nGrathwohl, W., Wang, K.-C., Jacobsen, J.-H., Duvenaud, D.,\nand Zemel, R. Learning the stein discrepancy for training\nand evaluating energy-based models without sampling.\nIn International Conference on Machine Learning, 2020.\nHa, D. and Schmidhuber, J.\nRecurrent world models\nfacilitate policy evolution.\nIn Advances in Neural\nInformation Processing Systems, 2018.\nHafner, D., Lillicrap, T., Fischer, I., Villegas, R., Ha, D.,\nLee, H., and Davidson, J. Learning latent dynamics for\nplanning from pixels. In International Conference on\nMachine Learning, 2021a.\nHafner, D., Lillicrap, T. P., Norouzi, M., and Ba,\nJ.\nMastering atari with discrete world models.\nIn\nInternational Conference on Learning Representations,\n2021b.\nHarris, C. R., Millman, K. J., van der Walt, S. J., Gommers,\nR., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J.,\nBerg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van\nKerkwijk, M. H., Brett, M., Haldane, A., del R\u00b4\u0131o, J. F.,\nWiebe, M., Peterson, P., G\u00b4erard-Marchant, P., Sheppard,\nK., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C.,\nand Oliphant, T. E. Array programming with NumPy.\nNature, 585(7825):357\u2013362, 2020.\nHo, J., Jain, A., and Abbeel, P.\nDenoising diffusion\nprobabilistic models. In Advances in Neural Information\nProcessing Systems, 2020.\nHyv\u00a8arinen, A. Estimation of non-normalized statistical\nmodels by score matching. Journal of Machine Learning\nResearch, 2005.\nJanner, M., Mordatch, I., and Levine, S.\n\u03b3-models:\nGenerative temporal difference learning for in\ufb01nite-\nhorizon prediction. In Advances in Neural Information\nProcessing Systems, 2020.\nJanner, M., Li, Q., and Levine, S. Of\ufb02ine reinforcement\nlearning as one big sequence modeling problem.\nIn\nAdvances in Neural Information Processing Systems,\n2021.\nKaiser, L., Babaeizadeh, M., Mi\u0142os, P., Osi\u00b4nski, B.,\nCampbell, R. H., Czechowski, K., Erhan, D., Finn, C.,\nKozakowski, P., Levine, S., Mohiuddin, A., Sepassi,\nR., Tucker, G., and Michalewski, H.\nModel based\nreinforcement learning for atari.\nIn International\nConference on Learning Representations, 2020.\nKe, N. R., Singh, A., Touati, A., Goyal, A., Bengio, Y.,\nParikh, D., and Batra, D. Modeling the long term future\nin model-based reinforcement learning. In International\nConference on Learning Representations, 2018.\nKelly, M. An introduction to trajectory optimization: How\nto do your own direct collocation. SIAM Review, 59(4):\n849\u2013904, 2017.\nKidambi, R., Rajeswaran, A., Netrapalli, P., and Joachims,\nT. MOReL: Model-based of\ufb02ine reinforcement learning.\nIn Advances in Neural Information Processing Systems,\n2020.\nKingma, D. P. and Ba, J. Adam: A method for stochastic\noptimization. In International Conference on Learning\nRepresentations, 2015.\nKostrikov, I., Nair, A., and Levine, S. Of\ufb02ine reinforcement\nlearning with implicit Q-learning.\nIn International\nConference on Learning Representations, 2022.\nKumar, A., Zhou, A., Tucker, G., and Levine, S.\nConservative Q-learning for of\ufb02ine reinforcement\nlearning. In Advances in Neural Information Processing\nSystems, 2020.\nLambert, N. O., Wilcox, A., Zhang, H., Pister, K. S., and\nCalandra, R.\nLearning accurate long-term dynamics\nfor model-based reinforcement learning. arXiv preprint\narXiv:2012.09156, 2020.\nPlanning with Diffusion for Flexible Behavior Synthesis\nLeCun, Y., Chopra, S., Hadsell, R., Huang, F. J., and\net al. A tutorial on energy-based learning. In Predicting\nStructured Data. MIT Press, 2006.\nLevine, S.\nReinforcement learning and control as\nprobabilistic inference: Tutorial and review.\narXiv\npreprint arXiv:1805.00909, 2018.\nLi, Y., Li, S., Sitzmann, V., Agrawal, P., and Torralba, A. 3d\nneural scene representations for visuomotor control. In\nConference on Robot Learning, 2021.\nMisra, D. Mish: A self regularized non-monotonic neural\nactivation function. In British Machine Vision Conference,\n2019.\nNagabandi, A., Kahn, G., S. Fearing, R., and Levine,\nS.\nNeural network dynamics for model-based deep\nreinforcement learning with model-free \ufb01ne-tuning. In\nInternational Conference on Robotics and Automation,\n2018.\nNichol, A. Q. and Dhariwal, P. Improved denoising diffusion\nprobabilistic models. In International Conference on\nMachine Learning, 2021.\nNijkamp, E., Hill, M., Zhu, S.-C., and Wu, Y. N. Learning\nnon-convergent non-persistent short-run MCMC toward\nenergy-based model. In Advances in Neural Information\nProcessing Systems, 2019.\nOh, J., Singh, S., and Lee, H. Value prediction network.\nIn Advances in Neural Information Processing Systems,\n2017.\nOzair, S., Li, Y., Razavi, A., Antonoglou, I., Van Den Oord,\nA., and Vinyals, O. Vector quantized models for planning.\nIn International Conference on Machine Learning, 2021.\nPaszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J.,\nChanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga,\nL., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison,\nM., Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L.,\nBai, J., and Chintala, S. Pytorch: An imperative style,\nhigh-performance deep learning library. In Advances in\nNeural Information Processing Systems. 2019.\nPosa, M., Cantu, C., and Tedrake, R. A direct method for\ntrajectory optimization of rigid bodies through contact.\nThe International Journal of Robotics Research, 2014.\nRhinehart, N., McAllister, R., and Levine, S. Deep imitative\nmodels for \ufb02exible inference, planning, and control. In\nInternational Conference on Learning Representations,\n2020.\nRybkin, O., Zhu, C., Nagabandi, A., Daniilidis, K.,\nMordatch, I., and Levine, S. Model-based reinforcement\nlearning via latent-space collocation. In International\nConference on Machine Learning, pp. 9190\u20139201.\nPMLR, 2021.\nSanchez-Gonzalez, A., Heess, N., Springenberg, J. T.,\nMerel, J., Riedmiller, M., Hadsell, R., and Battaglia, P.\nGraph networks as learnable physics engines for inference\nand control. In International Conference on Machine\nLearning, 2018.\nSchrittwieser, J., Antonoglou, I., Hubert, T., Simonyan, K.,\nSifre, L., Schmitt, S., Guez, A., Lockhart, E., Hassabis,\nD., Graepel, T., Lillicrap, T., and Silver, D. Mastering\natari, go, chess and shogi by planning with a learned\nmodel. arXiv preprint arXiv:1911.08265, 2019.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N.,\nand Ganguli, S.\nDeep unsupervised learning using\nnonequilibrium thermodynamics.\nIn International\nConference on Machine Learning, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. In International Conference on Learning\nRepresentations, 2021.\nSong, Y. and Ermon, S. Generative modeling by estimating\ngradients of the data distribution. In Advances in Neural\nInformation Processing Systems, 2019.\nSutton, R. S.\nIntegrated architectures for learning,\nplanning, and reacting based on approximating dynamic\nprogramming. In International Conference on Machine\nLearning, 1990.\nTalvitie, E. Model regularization for stable sample rollouts.\nIn Conference on Uncertainty in Arti\ufb01cial Intelligence,\n2014.\nTamar, A., Wu, Y., Thomas, G., Levine, S., and Abbeel,\nP.\nValue iteration networks.\nIn Advances in Neural\nInformation Processing Systems. 2016.\nTassa, Y., Erez, T., and Todorov, E.\nSynthesis and\nstabilization of complex behaviors through online\ntrajectory optimization. In International Conference on\nIntelligent Robots and Systems, 2012.\nWang,\nP.\nImplementation of denoising diffusion\nprobabilistic\nmodels\nin\npytorch,\n2020.\nURL\nhttps://github.com/lucidrains/\ndenoising-diffusion-pytorch.\nWang, T., Bao, X., Clavera, I., Hoang, J., Wen, Y.,\nLanglois, E., Zhang, S., Zhang, G., Abbeel, P., and Ba,\nJ. Benchmarking model-based reinforcement learning.\narXiv preprint arXiv:1907.02057, 2019.\nPlanning with Diffusion for Flexible Behavior Synthesis\nWilliams, G., Aldrich, A., and Theodorou, E.\nModel\npredictive path integral control using covariance variable\nimportance sampling. arXiv preprint arXiv:1509.01149,\n2015.\nWitkin, A. and Kass, M. Spacetime constraints. ACM\nSiggraph Computer Graphics, 1988.\nWu, Y. and He, K. Group normalization. In European\nConference on Computer Vision, 2018.\nYu, T., Thomas, G., Yu, L., Ermon, S., Zou, J., Levine,\nS., Finn, C., and Ma, T. MOPO: Model-based of\ufb02ine\npolicy optimization. In Advances in Neural Information\nProcessing Systems, 2020.\nZhou, L., Du, Y., and Wu, J.\n3D shape generation\nand completion through point-voxel diffusion.\nIn\nInternational Conference on Computer Vision, 2021.\nPlanning with Diffusion for Flexible Behavior Synthesis\nAppendix A\nBaseline details and sources\nIn this section, we provide details about baselines we ran\nourselves. For scores of baselines previously evaluated on\nstandardized tasks, we provide the source of the listed score.\nA.1\nMaze2D experiments\nSingle-task.\nThe performance of CQL and IQL on the\nstandard Maze2D environments is reported in the D4RL\nwhitepaper (Fu et al., 2020) in Table 2.\nWe ran IQL using the of\ufb01cal implementation from the\nauthors:\ngithub.com/ikostrikov/implicit q learning.\nWe tuned over two hyperparameters:\n1. temperature \u2208[3, 10]\n2. expectile \u2208[0.65, 0.95]\nMulti-task.\nWe only evaluated IQL on the Multi2D\nenvironments because it is the strongest baseline in the\nsingle-task Maze2D environments by a sizeable margin. To\nadapt IQL to the multi-task setting, we modi\ufb01ed the Q-\nfunctions, value function, and policy to be goal-conditioned.\nTo select goals during training, we employed a strategy\nbased on hindsight experience replay, in which we sampled\na goal from among those states encountered in the future of a\ntrajectory. For a training backup (st, at, st+1), we sampled\ngoals according to a geometric distribution over the future\n\u2206\u223cGeom(1 \u2212\u03b3)\ng = st+\u2206,\nrecalculated rewards based on the sampled goal, and\nconditioned all relevant models on the goal during updating.\nDuring testing, we conditioned the policy on the ground-\ntruth goal.\nWe tuned over the same IQL parameters as in the single-task\nsetting.\nA.2\nBlock stacking experiments\nSingle-task.\nWe\nran\nCQL\nusing\nthe\nfollowing\nimplementation\nhttps://github.com/young-geng/cql.\nand used default hyperparameters in the code. We ran BCQ\nusing the author\u2019s original implementation\nhttps://github.com/sfujim/BCQ.\nFor BCQ, we tuned over two hyperparameters:\n1. discount factor \u2208[0.9, 0.999]\n2. tau \u2208[0.001, 0.01]\nt\nx \nConv1D \nFC Layer\nConv1D \nGN Mish\nGN, Mish\nFigure A1. Diffuser has a U-Net architecture with residual blocks\nconsisting of temporal convolutions, group normalization, and\nMish nonlinearities.\nMulti-task.\nTo evaluate BCQ and CQL in the multi-\ntask setting, we modi\ufb01ed the Q-functions, value function\nand policy to be goal-conditioned. We trained using goal\nrelabeling as in the Multi2D environments. We tuned over\nthe same hyperparameters described in the single-task block\nstacking experiments.\nA.3\nOf\ufb02ine Locomotion\nThe scores for BC, CQL, IQL, and AWAC are from Table 1\nin Kostrikov et al. (2022). The scores for DT are from\nTable 2 in Chen et al. (2021b). The scores for TT are from\nTable 1 in Janner et al. (2021). The scores for MOReL are\nfrom Table 2 in Kidambi et al. (2020). The scores for MBOP\nare from Table 1 in Argenson & Dulac-Arnold (2021).\nAppendix B\nTest-time Flexibility\nTo guide Diffuser to stack blocks in speci\ufb01ed con\ufb01gurations,\nwe used two separate perturbation functions h(\u03c4 ) to specify\na given stack of block A on top of block B, which we detail\nbelow.\nFinal State Matching\nTo enforce a \ufb01nal state consisting\nof block A on top of block B, we trained a perturbation\nfunction hmatch(\u03c4 ) as a per-timestep classi\ufb01er determining\nwhether a a state s exhibits a stack of block A on top of\nblock B. We train the classi\ufb01er on the demonstration data as\nthe diffusion model.\nContact Constraint\nTo guide the Kuka arm to stack\nblock A on top of block B, we construct a perturbation\nfunction hcontact(\u03c4 ) = P64\ni=0 \u22121 \u2217\u2225\u03c4ci \u22121\u22252, where \u03c4ci\ncorresponds to the underlying dimension in state \u03c4si that\nspeci\ufb01es the presence or absence of contact between the\nKuka arm and block A. We apply the contact constraint\nbetween the Kuka arm and block A for the \ufb01rst 64 timesteps\nin a trajectory, corresponding to initial contact with block A\nin a plan.\nAppendix C\nImplementation Details\nIn this section we describe the architecture and record\nhyperparameters.\nPlanning with Diffusion for Flexible Behavior Synthesis\n1. The architecture of Diffuser (Figure A1) consists of\na U-Net structure with 6 repeated residual blocks.\nEach block consisted of two temporal convolutions,\neach followed by group norm (Wu & He, 2018), and\na \ufb01nal Mish nonlinearity (Misra, 2019). Timestep\nembeddings are produced by a single fully-connected\nlayer and added to the activations of the \ufb01rst temporal\nconvolution within each block.\n2. We train the model using the Adam optimizer (Kingma\n& Ba, 2015) with a learning rate of 4e\u221205 and batch\nsize of 32. We train the models for 500k steps.\n3. The return predictor J has the structure of the \ufb01rst half\nof the U-Net used for the diffusion model, with a \ufb01nal\nlinear layer to produce a scalar output.\n4. We use a planning horizon\nT\nof 32 in all\nlocomotion tasks, 128 for block-stacking, 128 in\nMaze2D / Multi2D U-Maze, 265 in Maze2D\n/ Multi2D Medium,\nand 384 in Maze2D /\nMulti2D Large.\n5. We found that we could reduce the planning horizon\nfor many tasks, but that the guide scale would need\nto be lowered (e.g., to 0.001 for a horizon of 4 in\nthe halfcheetah tasks) to accommodate.\nThe\ncon\ufb01guration \ufb01le in the open-source code demonstrates\nhow to run with a modi\ufb01ed scale and horizon.\n6. We use N = 20 diffusion steps for locomotion tasks\nand N = 100 for block-stacking.\n7. We use a guide scale of \u03b1 = 0.1 for all tasks\nexcept hopper-medium-expert, in which we use\na smaller scale of 0.0001.\n8. We used a discount factor of 0.997 for the return\nprediction J\u03c6, though found that above \u03b3 = 0.99\nplanning was fairly insensitive to changes in discount\nfactor.\n9. We\nfound\nthat\ncontrol\nperformance\nwas\nnot\nsubstantially affected by the choice of predicting noise\n\u03f5 versus uncorrupted data \u03c4 0 with the diffusion model.\n",
    "2207.12598": "CLASSIFIER-FREE DIFFUSION GUIDANCE\nJonathan Ho & Tim Salimans\nGoogle Research, Brain team\n{jonathanho,salimans}@google.com\nABSTRACT\nClassi\ufb01er guidance is a recently introduced method to trade off mode coverage\nand sample \ufb01delity in conditional diffusion models post training, in the same spirit\nas low temperature sampling or truncation in other types of generative models.\nClassi\ufb01er guidance combines the score estimate of a diffusion model with the\ngradient of an image classi\ufb01er and thereby requires training an image classi\ufb01er\nseparate from the diffusion model. It also raises the question of whether guidance\ncan be performed without a classi\ufb01er. We show that guidance can be indeed\nperformed by a pure generative model without such a classi\ufb01er: in what we\ncall classi\ufb01er-free guidance, we jointly train a conditional and an unconditional\ndiffusion model, and we combine the resulting conditional and unconditional score\nestimates to attain a trade-off between sample quality and diversity similar to that\nobtained using classi\ufb01er guidance.\n1\nINTRODUCTION\nDiffusion models have recently emerged as an expressive and \ufb02exible family of generative models,\ndelivering competitive sample quality and likelihood scores on image and audio synthesis tasks (Sohl-\nDickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021b; Kingma et al., 2021;\nSong et al., 2021a). These models have delivered audio synthesis performance rivaling the quality\nof autoregressive models with substantially fewer inference steps (Chen et al., 2021; Kong et al.,\n2021), and they have delivered ImageNet generation results outperforming BigGAN-deep (Brock\net al., 2019) and VQ-VAE-2 (Razavi et al., 2019) in terms of FID score and classi\ufb01cation accuracy\nscore (Ho et al., 2021; Dhariwal & Nichol, 2021).\nDhariwal & Nichol (2021) proposed classi\ufb01er guidance, a technique to boost the sample quality of\na diffusion model using an extra trained classi\ufb01er. Prior to classi\ufb01er guidance, it was not known\nhow to generate \u201clow temperature\u201d samples from a diffusion model similar to those produced by\ntruncated BigGAN (Brock et al., 2019) or low temperature Glow (Kingma & Dhariwal, 2018):\nnaive attempts, such as scaling the model score vectors or decreasing the amount of Gaussian noise\nadded during diffusion sampling, are ineffective (Dhariwal & Nichol, 2021). Classi\ufb01er guidance\ninstead mixes a diffusion model\u2019s score estimate with the input gradient of the log probability of a\nFigure 1: Classi\ufb01er-free guidance on the malamute class for a 64x64 ImageNet diffusion model. Left\nto right: increasing amounts of classi\ufb01er-free guidance, starting from non-guided samples on the left.\nA short version of this paper appeared in the NeurIPS 2021 Workshop on Deep Generative Models and\nDownstream Applications: https://openreview.net/pdf?id=qw8AKxfYbI\n1\narXiv:2207.12598v1  [cs.LG]  26 Jul 2022\nFigure 2: The effect of guidance on a mixture of three Gaussians, each mixture component represent-\ning data conditioned on a class. The leftmost plot is the non-guided marginal density. Left to right\nare densities of mixtures of normalized guided conditionals with increasing guidance strength.\nclassi\ufb01er. By varying the strength of the classi\ufb01er gradient, Dhariwal & Nichol can trade off Inception\nscore (Salimans et al., 2016) and FID score (Heusel et al., 2017) (or precision and recall) in a manner\nsimilar to varying the truncation parameter of BigGAN.\nWe are interested in whether classi\ufb01er guidance can be performed without a classi\ufb01er. Classi\ufb01er\nguidance complicates the diffusion model training pipeline because it requires training an extra\nclassi\ufb01er, and this classi\ufb01er must be trained on noisy data so it is generally not possible to plug\nin a pre-trained classi\ufb01er. Furthermore, because classi\ufb01er guidance mixes a score estimate with\na classi\ufb01er gradient during sampling, classi\ufb01er-guided diffusion sampling can be interpreted as\nattempting to confuse an image classi\ufb01er with a gradient-based adversarial attack. This raises the\nquestion of whether classi\ufb01er guidance is successful at boosting classi\ufb01er-based metrics such as FID\nand Inception score (IS) simply because it is adversarial against such classi\ufb01ers. Stepping in direction\nof classi\ufb01er gradients also bears some resemblance to GAN training, particularly with nonparameteric\ngenerators; this also raises the question of whether classi\ufb01er-guided diffusion models perform well\non classi\ufb01er-based metrics because they are beginning to resemble GANs, which are already known\nto perform well on such metrics.\nTo resolve these questions, we present classi\ufb01er-free guidance, our guidance method which avoids\nany classi\ufb01er entirely. Rather than sampling in the direction of the gradient of an image classi\ufb01er,\nclassi\ufb01er-free guidance instead mixes the score estimates of a conditional diffusion model and a\njointly trained unconditional diffusion model. By sweeping over the mixing weight, we attain a\nFID/IS tradeoff similar to that attained by classi\ufb01er guidance. Our classi\ufb01er-free guidance results\ndemonstrate that pure generative diffusion models are capable of synthesizing extremely high \ufb01delity\nsamples possible with other types of generative models.\n2\nBACKGROUND\nWe train diffusion models in continuous time (Song et al., 2021b; Chen et al., 2021; Kingma et al.,\n2021): letting x \u223cp(x) and z = {z\u03bb | \u03bb \u2208[\u03bbmin, \u03bbmax]} for hyperparameters \u03bbmin < \u03bbmax \u2208R,\nthe forward process q(z|x) is the variance-preserving Markov process (Sohl-Dickstein et al., 2015):\nq(z\u03bb|x) = N(\u03b1\u03bbx, \u03c32\n\u03bbI), where \u03b12\n\u03bb = 1/(1 + e\u2212\u03bb), \u03c32\n\u03bb = 1 \u2212\u03b12\n\u03bb\n(1)\nq(z\u03bb|z\u03bb\u2032) = N((\u03b1\u03bb/\u03b1\u03bb\u2032)z\u03bb\u2032, \u03c32\n\u03bb|\u03bb\u2032I), where \u03bb < \u03bb\u2032, \u03c32\n\u03bb|\u03bb\u2032 = (1 \u2212e\u03bb\u2212\u03bb\u2032)\u03c32\n\u03bb\n(2)\nWe will use the notation p(z) (or p(z\u03bb)) to denote the marginal of z (or z\u03bb) when x \u223cp(x) and\nz \u223cq(z|x). Note that \u03bb = log \u03b12\n\u03bb/\u03c32\n\u03bb, so \u03bb can be interpreted as the log signal-to-noise ratio of z\u03bb,\nand the forward process runs in the direction of decreasing \u03bb.\nConditioned on x, the forward process can be described in reverse by the transitions q(z\u03bb\u2032|z\u03bb, x) =\nN(\u02dc\u00b5\u03bb\u2032|\u03bb(z\u03bb, x), \u02dc\u03c32\n\u03bb\u2032|\u03bbI), where\n\u02dc\u00b5\u03bb\u2032|\u03bb(z\u03bb, x) = e\u03bb\u2212\u03bb\u2032(\u03b1\u03bb\u2032/\u03b1\u03bb)z\u03bb + (1 \u2212e\u03bb\u2212\u03bb\u2032)\u03b1\u03bb\u2032x,\n\u02dc\u03c32\n\u03bb\u2032|\u03bb = (1 \u2212e\u03bb\u2212\u03bb\u2032)\u03c32\n\u03bb\u2032\n(3)\nThe reverse process generative model starts from p\u03b8(z\u03bbmin) = N(0, I). We specify the transitions:\np\u03b8(z\u03bb\u2032|z\u03bb) = N(\u02dc\u00b5\u03bb\u2032|\u03bb(z\u03bb, x\u03b8(z\u03bb)), (\u02dc\u03c32\n\u03bb\u2032|\u03bb)1\u2212v(\u03c32\n\u03bb|\u03bb\u2032)v)\n(4)\n2\nDuring sampling, we apply this transition along an increasing sequence \u03bbmin = \u03bb1 < \u00b7 \u00b7 \u00b7 < \u03bbT =\n\u03bbmax for T timesteps; in other words, we follow the discrete time ancestral sampler of Sohl-Dickstein\net al. (2015); Ho et al. (2020). If the model x\u03b8 is correct, then as T \u2192\u221e, we obtain samples from an\nSDE whose sample paths are distributed as p(z) (Song et al., 2021b), and we use p\u03b8(z) to denote the\ncontinuous time model distribution. The variance is a log-space interpolation of \u02dc\u03c32\n\u03bb\u2032|\u03bb and \u03c32\n\u03bb|\u03bb\u2032 as\nsuggested by Nichol & Dhariwal (2021); we found it effective to use a constant hyperparameter v\nrather than learned z\u03bb-dependent v. Note that the variances simplify to \u02dc\u03c32\n\u03bb\u2032|\u03bb as \u03bb\u2032 \u2192\u03bb, so v has an\neffect only when sampling with non-in\ufb01nitesimal timesteps as done in practice.\nThe reverse process mean comes from an estimate x\u03b8(z\u03bb) \u2248x plugged into q(z\u03bb\u2032|z\u03bb, x) (Ho et al.,\n2020; Kingma et al., 2021) (x\u03b8 also receives \u03bb as input, but we suppress this to keep our notation\nclean). We parameterize x\u03b8 in terms of \u03f5-prediction (Ho et al., 2020): x\u03b8(z\u03bb) = (z\u03bb\u2212\u03c3\u03bb\u03f5\u03b8(z\u03bb))/\u03b1\u03bb,\nand we train on the objective\nE\u03f5,\u03bb\n\u0002\n\u2225\u03f5\u03b8(z\u03bb) \u2212\u03f5\u22252\n2\n\u0003\n(5)\nwhere \u03f5 \u223cN(0, I), z\u03bb = \u03b1\u03bbx + \u03c3\u03bb\u03f5, and \u03bb is drawn from a distribution p(\u03bb) over [\u03bbmin, \u03bbmax].\nThis objective is denoising score matching (Vincent, 2011; Hyv\u00a8arinen & Dayan, 2005) over multiple\nnoise scales (Song & Ermon, 2019), and when p(\u03bb) is uniform, the objective is proportional to the\nvariational lower bound on the marginal log likelihood of the latent variable model\nR\np\u03b8(x|z)p\u03b8(z)dz,\nignoring the term for the unspeci\ufb01ed decoder p\u03b8(x|z) and for the prior at z\u03bbmin (Kingma et al., 2021).\nIf p(\u03bb) is not uniform, the objective can be interpreted as weighted variational lower bound whose\nweighting can be tuned for sample quality (Ho et al., 2020; Kingma et al., 2021). We use a p(\u03bb)\ninspired by the discrete time cosine noise schedule of Nichol & Dhariwal (2021): we sample \u03bb\nvia \u03bb = \u22122 log tan(au + b) for uniformly distributed u \u2208[0, 1], where b = arctan(e\u2212\u03bbmax/2) and\na = arctan(e\u2212\u03bbmin/2) \u2212b. This represents a hyperbolic secant distribution modi\ufb01ed to be supported\non a bounded interval. For \ufb01nite timestep generation, we use \u03bb values corresponding to uniformly\nspaced u \u2208[0, 1], and the \ufb01nal generated sample is x\u03b8(z\u03bbmax).\nBecause the loss for \u03f5\u03b8(z\u03bb) is denoising score matching for all \u03bb, the score \u03f5\u03b8(z\u03bb) learned by our\nmodel estimates the gradient of the log-density of the distribution of our noisy data z\u03bb, that is\n\u03f5\u03b8(z\u03bb) \u2248\u2212\u03c3\u03bb\u2207z\u03bb log p(z\u03bb); note, however, that because we use unconstrained neural networks to\nde\ufb01ne \u03f5\u03b8, there need not exist any scalar potential whose gradient is \u03f5\u03b8. Sampling from the learned\ndiffusion model resembles using Langevin diffusion to sample from a sequence of distributions p(z\u03bb)\nthat converges to the conditional distribution p(x) of the original data x.\nIn the case of conditional generative modeling, the data x is drawn jointly with conditioning informa-\ntion c, i.e. a class label for class-conditional image generation. The only modi\ufb01cation to the model is\nthat the reverse process function approximator receives c as input, as in \u03f5\u03b8(z\u03bb, c).\n3\nGUIDANCE\nAn interesting property of certain generative models, such as GANs and \ufb02ow-based models, is the\nability to perform truncated or low temperature sampling by decreasing the variance or range of noise\ninputs to the generative model at sampling time. The intended effect is to decrease the diversity of\nthe samples while increasing the quality of each individual sample. Truncation in BigGAN (Brock\net al., 2019), for example, yields a tradeoff curve between FID score and Inception score for low and\nhigh amounts of truncation, respectively. Low temperature sampling in Glow (Kingma & Dhariwal,\n2018) has a similar effect.\nUnfortunately, straightforward attempts of implementing truncation or low temperature sampling\nin diffusion models are ineffective. For example, scaling model scores or decreasing the variance\nof Gaussian noise in the reverse process cause the diffusion model to generate blurry, low quality\nsamples (Dhariwal & Nichol, 2021).\n3.1\nCLASSIFIER GUIDANCE\nTo obtain a truncation-like effect in diffusion models, Dhariwal & Nichol (2021) introduce classi\ufb01er\nguidance, where the diffusion score \u03f5\u03b8(z\u03bb, c) \u2248\u2212\u03c3\u03bb\u2207z\u03bb log p(z\u03bb|c) is modi\ufb01ed to include the\n3\nAlgorithm 1 Joint training a diffusion model with classi\ufb01er-free guidance\nRequire: puncond: probability of unconditional training\n1: repeat\n2:\n(x, c) \u223cp(x, c)\n\u25b7Sample data with conditioning from the dataset\n3:\nc \u2190\u2205with probability puncond \u25b7Randomly discard conditioning to train unconditionally\n4:\n\u03bb \u223cp(\u03bb)\n\u25b7Sample log SNR value\n5:\n\u03f5 \u223cN(0, I)\n6:\nz\u03bb = \u03b1\u03bbx + \u03c3\u03bb\u03f5\n\u25b7Corrupt data to the sampled log SNR value\n7:\nTake gradient step on \u2207\u03b8 \u2225\u03f5\u03b8(z\u03bb, c) \u2212\u03f5\u22252\n\u25b7Optimization of denoising model\n8: until converged\ngradient of the log likelihood of an auxiliary classi\ufb01er model p\u03b8(c|z\u03bb) as follows:\n\u02dc\u03f5\u03b8(z\u03bb, c) = \u03f5\u03b8(z\u03bb, c) \u2212w\u03c3\u03bb\u2207z\u03bb log p\u03b8(c|z\u03bb) \u2248\u2212\u03c3\u03bb\u2207z\u03bb[log p(z\u03bb|c) + w log p\u03b8(c|z\u03bb)],\nwhere w is a parameter that controls the strength of the classi\ufb01er guidance. This modi\ufb01ed score\n\u02dc\u03f5\u03b8(z\u03bb, c) is then used in place of \u03f5\u03b8(z\u03bb, c) when sampling from the diffusion model, resulting in\napproximate samples from the distribution\n\u02dcp\u03b8(z\u03bb|c) \u221dp\u03b8(z\u03bb|c)p\u03b8(c|z\u03bb)w.\nThe effect is that of up-weighting the probability of data for which the classi\ufb01er p\u03b8(c|z\u03bb) assigns\nhigh likelihood to the correct label: data that can be classi\ufb01ed well scores high on the Inception score\nof perceptual quality (Salimans et al., 2016), which rewards generative models for this by design.\nDhariwal & Nichol therefore \ufb01nd that by setting w > 0 they can improve the Inception score of their\ndiffusion model, at the expense of decreased diversity in their samples.\nFigure 2 illustrates the effect of numerically solved guidance \u02dcp\u03b8(z\u03bb|c) \u221dp\u03b8(z\u03bb|c)p\u03b8(c|z\u03bb)w on a\ntoy 2D example of three classes, in which the conditional distribution for each class is an isotropic\nGaussian. The form of each conditional upon applying guidance is markedly non-Gaussian. As\nguidance strength is increased, each conditional places probability mass farther away from other\nclasses and towards directions of high con\ufb01dence given by logistic regression, and most of the mass\nbecomes concentrated in smaller regions. This behavior can be seen as a simplistic manifestation of\nthe Inception score boost and sample diversity decrease that occur when classi\ufb01er guidance strength\nis increased in an ImageNet model.\nApplying classi\ufb01er guidance with weight w + 1 to an unconditional model would theoretically lead\nto the same result as applying classi\ufb01er guidance with weight w to a conditional model, because\np\u03b8(z\u03bb|c)p\u03b8(c|z\u03bb)w \u221dp\u03b8(z\u03bb)p\u03b8(c|z\u03bb)w+1; or in terms of scores,\n\u03f5\u03b8(z\u03bb) \u2212(w + 1)\u03c3\u03bb\u2207z\u03bb log p\u03b8(c|z\u03bb) \u2248\u2212\u03c3\u03bb\u2207z\u03bb[log p(z\u03bb) + (w + 1) log p\u03b8(c|z\u03bb)]\n= \u2212\u03c3\u03bb\u2207z\u03bb[log p(z\u03bb|c) + w log p\u03b8(c|z\u03bb)],\nbut interestingly, Dhariwal & Nichol obtain their best results when applying classi\ufb01er guidance to an\nalready class-conditional model, as opposed to applying guidance to an unconditional model. For\nthis reason, we will stay in the setup of guiding an already conditional model.\n3.2\nCLASSIFIER-FREE GUIDANCE\nWhile classi\ufb01er guidance successfully trades off IS and FID as expected from truncation or low\ntemperature sampling, it is nonetheless reliant on gradients from an image classi\ufb01er and we seek to\neliminate the classi\ufb01er for the reasons stated in Section 1. Here, we describe classi\ufb01er-free guidance,\nwhich achieves the same effect without such gradients. Classi\ufb01er-free guidance is an alternative\nmethod of modifying \u03f5\u03b8(z\u03bb, c) to have the same effect as classi\ufb01er guidance, but without a classi\ufb01er.\nAlgorithms 1 and 2 describe training and sampling with classi\ufb01er-free guidance in detail.\nInstead of training a separate classi\ufb01er model, we choose to train an unconditional denoising diffusion\nmodel p\u03b8(z) parameterized through a score estimator \u03f5\u03b8(z\u03bb) together with the conditional model\np\u03b8(z|c) parameterized through \u03f5\u03b8(z\u03bb, c). We use a single neural network to parameterize both\nmodels, where for the unconditional model we can simply input a null token \u2205for the class identi\ufb01er\nc when predicting the score, i.e. \u03f5\u03b8(z\u03bb) = \u03f5\u03b8(z\u03bb, c = \u2205). We jointly train the unconditional and\n4\nAlgorithm 2 Conditional sampling with classi\ufb01er-free guidance\nRequire: w: guidance strength\nRequire: c: conditioning information for conditional sampling\nRequire: \u03bb1, . . . , \u03bbT : increasing log SNR sequence with \u03bb1 = \u03bbmin, \u03bbT = \u03bbmax\n1: z1 \u223cN(0, I)\n2: for t = 1, . . . , T do\n\u25b7Form the classi\ufb01er-free guided score at log SNR \u03bbt\n3:\n\u02dc\u03f5t = (1 + w)\u03f5\u03b8(zt, c) \u2212w\u03f5\u03b8(zt)\n\u25b7Sampling step (could be replaced by another sampler, e.g. DDIM)\n4:\n\u02dcxt = (zt \u2212\u03c3\u03bbt\u02dc\u03f5t)/\u03b1\u03bbt\n5:\nzt+1 \u223cN(\u02dc\u00b5\u03bbt+1|\u03bbt(zt, \u02dcxt), (\u02dc\u03c32\n\u03bbt+1|\u03bbt)1\u2212v(\u03c32\n\u03bbt|\u03bbt+1)v) if t < T else zt+1 = \u02dcxt\n6: end for\n7: return zT +1\nconditional models simply by randomly setting c to the unconditional class identi\ufb01er \u2205with some\nprobability puncond, set as a hyperparameter. (It would certainly be possible to train separate models\ninstead of jointly training them together, but we choose joint training because it is extremely simple\nto implement, does not complicate the training pipeline, and does not increase the total number of\nparameters.) We then perform sampling using the following linear combination of the conditional\nand unconditional score estimates:\n\u02dc\u03f5\u03b8(z\u03bb, c) = (1 + w)\u03f5\u03b8(z\u03bb, c) \u2212w\u03f5\u03b8(z\u03bb)\n(6)\nEq. (6) has no classi\ufb01er gradient present, so taking a step in the \u02dc\u03f5\u03b8 direction cannot be interpreted as\na gradient-based adversarial attack on an image classi\ufb01er. Furthermore, \u02dc\u03f5\u03b8 is constructed from score\nestimates that are non-conservative vector \ufb01elds due to the use of unconstrained neural networks, so\nthere in general cannot exist a scalar potential such as a classi\ufb01er log likelihood for which \u02dc\u03f5\u03b8 is the\nclassi\ufb01er-guided score.\nDespite the fact that there in general may not exist a classi\ufb01er for which Eq. (6) is the classi\ufb01er-\nguided score, it is in fact inspired by the gradient of an implicit classi\ufb01er pi(c|z\u03bb) \u221dp(z\u03bb|c)/p(z\u03bb).\nIf we had access to exact scores \u03f5\u2217(z\u03bb, c) and \u03f5\u2217(z\u03bb) (of p(z\u03bb|c) and p(z\u03bb), respectively), then\nthe gradient of this implicit classi\ufb01er would be \u2207z\u03bb log pi(c|z\u03bb) = \u22121\n\u03c3\u03bb [\u03f5\u2217(z\u03bb, c) \u2212\u03f5\u2217(z\u03bb)], and\nclassi\ufb01er guidance with this implicit classi\ufb01er would modify the score estimate into \u02dc\u03f5\u2217(z\u03bb, c) =\n(1 + w)\u03f5\u2217(z\u03bb, c) \u2212w\u03f5\u2217(z\u03bb). Note the resemblance to Eq. (6), but also note that \u02dc\u03f5\u2217(z\u03bb, c) dif-\nfers fundamentally from \u02dc\u03f5\u03b8(z\u03bb, c). The former is constructed from the scaled classi\ufb01er gradient\n\u03f5\u2217(z\u03bb, c)\u2212\u03f5\u2217(z\u03bb); the latter is constructed from the estimate \u03f5\u03b8(z\u03bb, c)\u2212\u03f5\u03b8(z\u03bb), and this expression\nis not in general the (scaled) gradient of any classi\ufb01er, again because the score estimates are the\noutputs of unconstrained neural networks.\nIt is not obvious a priori that inverting a generative model using Bayes\u2019 rule yields a good classi\ufb01er that\nprovides a useful guidance signal. For example, Grandvalet & Bengio (2004) \ufb01nd that discriminative\nmodels generally outperform implicit classi\ufb01ers derived from generative models, even in arti\ufb01cial\ncases where the speci\ufb01cation of those generative models exactly matches the data distribution. In\ncases such as ours, where we expect the model to be misspeci\ufb01ed, classi\ufb01ers derived by Bayes\u2019 rule\ncan be inconsistent (Gr\u00a8unwald & Langford, 2007) and we lose all guarantees on their performance.\nNevertheless, in Section 4, we show empirically that classi\ufb01er-free guidance is able to trade off\nFID and IS in the same way as classi\ufb01er guidance. In Section 5 we discuss the implications of\nclassi\ufb01er-free guidance in relation to classi\ufb01er guidance.\n4\nEXPERIMENTS\nWe train diffusion models with classi\ufb01er-free guidance on area-downsampled class-conditional\nImageNet (Russakovsky et al., 2015), the standard setting for studying tradeoffs between FID and\nInception scores starting from the BigGAN paper (Brock et al., 2019).\nThe purpose of our experiments is to serve as a proof of concept to demonstrate that classi\ufb01er-free\nguidance is able to attain a FID/IS tradeoff similar to classi\ufb01er guidance and to understand the\nbehavior of classi\ufb01er-free guidance, not necessarily to push sample quality metrics to state of the art\n5\nFigure 3: Classi\ufb01er-free guidance on 128x128 ImageNet. Left: non-guided samples, right: classi\ufb01er-\nfree guided samples with w = 3.0. Interestingly, strongly guided samples such as these display\nsaturated colors. See Fig. 8 for more.\non these benchmarks. For this purpose, we use the same model architectures and hyperparameters as\nthe guided diffusion models of Dhariwal & Nichol (2021) (apart from continuous time training as\nspeci\ufb01ed in Section 2); those hyperparameter settings were tuned for classi\ufb01er guidance and hence\nmay be suboptimal for classi\ufb01er-free guidance. Furthermore, since we amortize the conditional and\nunconditional models into the same architecture without an extra classi\ufb01er, we in fact are using less\nmodel capacity than previous work. Nevertheless, our classi\ufb01er-free guided models still produce\ncompetitive sample quality metrics and sometimes outperform prior work, as can be seen in the\nfollowing sections.\n4.1\nVARYING THE CLASSIFIER-FREE GUIDANCE STRENGTH\nHere we experimentally verify the main claim of this paper: that classi\ufb01er-free guidance is able\nto trade off IS and FID in a manner like classi\ufb01er guidance or GAN truncation. We apply our\nproposed classi\ufb01er-free guidance to 64\u00d764 and 128\u00d7128 class-conditional ImageNet generation. In\nTable 1 and Fig. 4, we show sample quality effects of sweeping over the guidance strength w on our\n6\n64 \u00d7 64 ImageNet models; Table 2 and Fig. 5 show the same for our 128 \u00d7 128 models. We consider\nw \u2208{0, 0.1, 0.2, . . . , 4} and calculate FID and Inception Scores with 50000 samples for each value\nfollowing the procedures of Heusel et al. (2017) and Salimans et al. (2016). All models used log\nSNR endpoints \u03bbmin = \u221220 and \u03bbmax = 20. The 64 \u00d7 64 models used sampler noise interpolation\ncoef\ufb01cient v = 0.3 and were trained for 400 thousand steps; the 128 \u00d7 128 models used v = 0.2 and\nwere trained for 2.7 million steps.\nWe obtain the best FID results with a small amount of guidance (w = 0.1 or w = 0.3, depending on\nthe dataset) and the best IS result with strong guidance (w \u22654). Between these two extremes we see\na clear trade-off between these two metrics of perceptual quality, with FID monotonically decreasing\nand IS monotonically increasing with w. Our results compare favorably to Dhariwal & Nichol (2021)\nand Ho et al. (2021), and in fact our 128 \u00d7 128 results are the state of the art in the literature. At\nw = 0.3, our model\u2019s FID score on 128 \u00d7 128 ImageNet outperforms the classi\ufb01er-guided ADM-G,\nand at w = 4.0, our model outperforms BigGAN-deep at both FID and IS when BigGAN-deep is\nevaluated its best-IS truncation level.\nFigures 1, 3 and 6 to 8 show randomly generated samples from our model for different levels of\nguidance: here we clearly see that increasing classi\ufb01er-free guidance strength has the expected effect\nof decreasing sample variety and increasing individual sample \ufb01delity.\nModel\nFID (\u2193)\nIS (\u2191)\nADM (Dhariwal & Nichol, 2021)\n2.07\n-\nCDM (Ho et al., 2021)\n1.48\n67.95\nOurs\npuncond = 0.1/0.2/0.5\nw = 0.0\n1.8 / 1.8 / 2.21\n53.71 / 52.9 / 47.61\nw = 0.1\n1.55 / 1.62 / 1.91\n66.11 / 64.58 / 56.1\nw = 0.2\n2.04 / 2.1 / 2.08\n78.91 / 76.99 / 65.6\nw = 0.3\n3.03 / 2.93 / 2.65\n92.8 / 88.64 / 74.92\nw = 0.4\n4.3 / 4 / 3.44\n106.2 / 101.11 / 84.27\nw = 0.5\n5.74 / 5.19 / 4.34\n119.3 / 112.15 / 92.95\nw = 0.6\n7.19 / 6.48 / 5.27\n131.1 / 122.13 / 102\nw = 0.7\n8.62 / 7.73 / 6.23\n141.8 / 131.6 / 109.8\nw = 0.8\n10.08 / 8.9 / 7.25\n151.6 / 140.82 / 116.9\nw = 0.9\n11.41 / 10.09 / 8.21\n161 / 150.26 / 124.6\nw = 1.0\n12.6 / 11.21 / 9.13\n170.1 / 158.29 / 131.1\nw = 2.0\n21.03 / 18.79 / 16.16\n225.5 / 212.98 / 183\nw = 3.0\n24.83 / 22.36 / 19.75\n250.4 / 237.65 / 208.9\nw = 4.0\n26.22 / 23.84 / 21.48\n260.2 / 248.97 / 225.1\nTable 1: ImageNet 64x64 results (w = 0.0 refers to non-guided models).\n50\n100\n150\n200\n250\n0\n10\n20\nIS\nFID\npuncond = 0.1\npuncond = 0.2\npuncond = 0.5\nFigure 4: IS/FID curves over guidance strengths for ImageNet 64x64 models. Each curve represents\na model with unconditional training probability puncond. Accompanies Table 1.\n7\n4.2\nVARYING THE UNCONDITIONAL TRAINING PROBABILITY\nThe main hyperparameter of classi\ufb01er-free guidance at training time is puncond, the probability\nof training on unconditional generation during joint training of the conditional and unconditional\ndiffusion models. Here, we study the effect of training models on varying puncond on 64 \u00d7 64\nImageNet.\nTable 1 and Fig. 4 show the effects of puncond on sample quality. We trained models with puncond \u2208\n{0.1, 0.2, 0.5}, all for 400 thousand training steps, and evaluated sample quality across various\nguidance strengths. We \ufb01nd puncond = 0.5 consistently performs worse than puncond \u2208{0.1, 0.2}\nacross the entire IS/FID frontier; puncond \u2208{0.1, 0.2} perform about equally as well as each other.\nBased on these \ufb01ndings, we conclude that only a relatively small portion of the model capacity of\nthe diffusion model needs to be dedicated to the unconditional generation task in order to produce\nclassi\ufb01er-free guided scores effective for sample quality. Interestingly, for classi\ufb01er guidance,\nDhariwal & Nichol report that relatively small classi\ufb01ers with little capacity are suf\ufb01cient for effective\nclassi\ufb01er guided sampling, mirroring this phenomenon that we found with classi\ufb01er-free guided\nmodels.\n4.3\nVARYING THE NUMBER OF SAMPLING STEPS\nSince the number of sampling steps T is known to have a major impact on the sample quality of a\ndiffusion model, here we study the effect of varying T on our 128 \u00d7 128 ImageNet model. Table 2\nand Fig. 5 show the effect of varying T \u2208{128, 256, 1024} over a range of guidance strengths. As\nexpected, sample quality improves when T is increased, and for this model T = 256 attains a good\nbalance between sample quality and sampling speed.\nNote that T = 256 is approximately the same number of sampling steps used by ADM-G (Dhariwal\n& Nichol, 2021), which is outperformed by our model. However, it is important to note that each\nsampling step for our method requires evaluating the denoising model twice, once for the conditional\n\u03f5\u03b8(z\u03bb, c) and once for the unconditional \u03f5\u03b8(z\u03bb). Because we used the same model architecture as\nADM-G, the fair comparison in terms of sampling speed would be our T = 128 setting, which\nunderperforms compared to ADM-G in terms of FID score.\nModel\nFID (\u2193)\nIS (\u2191)\nBigGAN-deep, max IS (Brock et al., 2019)\n25\n253\nBigGAN-deep (Brock et al., 2019)\n5.7\n124.5\nCDM (Ho et al., 2021)\n3.52\n128.8\nLOGAN (Wu et al., 2019)\n3.36\n148.2\nADM-G (Dhariwal & Nichol, 2021)\n2.97\n-\nOurs\nT = 128/256/1024\nw = 0.0\n8.11 / 7.27 / 7.22\n81.46 / 82.45 / 81.54\nw = 0.1\n5.31 / 4.53 / 4.5\n105.01 / 106.12 / 104.67\nw = 0.2\n3.7 / 3.03 / 3\n130.79 / 132.54 / 130.09\nw = 0.3\n3.04 / 2.43 / 2.43\n156.09 / 158.47 / 156\nw = 0.4\n3.02 / 2.49 / 2.48\n183.01 / 183.41 / 180.88\nw = 0.5\n3.43 / 2.98 / 2.96\n206.94 / 207.98 / 204.31\nw = 0.6\n4.09 / 3.76 / 3.73\n227.72 / 228.83 / 226.76\nw = 0.7\n4.96 / 4.67 / 4.69\n247.92 / 249.25 / 247.89\nw = 0.8\n5.93 / 5.74 / 5.71\n265.54 / 267.99 / 265.52\nw = 0.9\n6.89 / 6.8 / 6.81\n280.19 / 283.41 / 281.14\nw = 1.0\n7.88 / 7.86 / 7.8\n295.29 / 297.98 / 294.56\nw = 2.0\n15.9 / 15.93 / 15.75\n378.56 / 377.37 / 373.18\nw = 3.0\n19.77 / 19.77 / 19.56\n409.16 / 407.44 / 405.68\nw = 4.0\n21.55 / 21.53 / 21.45\n422.29 / 421.03 / 419.06\nTable 2: ImageNet 128x128 results (w = 0.0 refers to non-guided models).\n8\n50\n100\n150\n200\n250\n300\n350\n400\n450\n10\n20\nIS\nFID\nT = 128\nT = 256\nT = 512\nFigure 5: IS/FID curves over guidance strengths for ImageNet 128x128 models. Each curve represents\nsampling with a different number of timesteps T. Accompanies Table 2.\n5\nDISCUSSION\nThe most practical advantage of our classi\ufb01er-free guidance method is its extreme simplicity: it is\nonly a one-line change of code during training\u2014to randomly drop out the conditioning\u2014and during\nsampling\u2014to mix the conditional and unconditional score estimates. Classi\ufb01er guidance, by contrast,\ncomplicates the training pipeline since it requires training an extra classi\ufb01er. This classi\ufb01er must be\ntrained on noisy z\u03bb, so it is not possible to plug in a standard pre-trained classi\ufb01er.\nSince classi\ufb01er-free guidance is able to trade off IS and FID like classi\ufb01er guidance without needing an\nextra trained classi\ufb01er, we have demonstrated that guidance can be performed with a pure generative\nmodel. Furthermore, our diffusion models are parameterized by unconstrained neural networks and\ntherefore their score estimates do not necessarily form conservative vector \ufb01elds, unlike classi\ufb01er\ngradients (Salimans & Ho, 2021). Therefore, our classi\ufb01er-free guided sampler follows step directions\nthat do not resemble classi\ufb01er gradients at all and thus cannot be interpreted as a gradient-based\nadversarial attack on a classi\ufb01er, and hence our results show that boosting the classi\ufb01er-based IS and\nFID metrics can be accomplished with pure generative models with a sampling procedure that is not\nadversarial against image classi\ufb01ers using classi\ufb01er gradients.\nWe also have arrived at an intuitive explanation for how guidance works: it decreases the uncondi-\ntional likelihood of the sample while increasing the conditional likelihood. Classi\ufb01er-free guidance\naccomplishes this by decreasing the unconditional likelihood with a negative score term, which to\nour knowledge has not yet been explored and may \ufb01nd uses in other applications.\nClassi\ufb01er-free guidance as presented here relies on training an unconditional model, but in some\ncases this can be avoided. If the class distribution is known and there are only a few classes, we can\nuse the fact that P\nc p(x|c)p(c) = p(x) to obtain an unconditional score from conditional scores\nwithout explicitly training for the unconditional score. Of course, this would require as many forward\npasses as there are possible values of c and would be inef\ufb01cient for high dimensional conditioning.\nA potential disadvantage of classi\ufb01er-free guidance is sampling speed. Generally, classi\ufb01ers can be\nsmaller and faster than generative models, so classi\ufb01er guided sampling may be faster than classi\ufb01er-\nfree guidance because the latter needs to run two forward passes of the diffusion model, one for\nconditional score and another for the unconditional score. The necessity to run multiple passes of the\ndiffusion model might be mitigated by changing the architecture to inject conditioning late in the\nnetwork, but we leave this exploration for future work.\nFinally, any guidance method that increases sample \ufb01delity at the expense of diversity must face the\nquestion of whether decreased diversity is acceptable. There may be negative impacts in deployed\nmodels, since sample diversity is important to maintain in applications where certain parts of the data\nare underrepresented in the context of the rest of the data. It would be an interesting avenue of future\nwork to try to boost sample quality while maintaining sample diversity.\n9\n6\nCONCLUSION\nWe have presented classi\ufb01er-free guidance, a method to increase sample quality while decreasing\nsample diversity in diffusion models. Classi\ufb01er-free guidance can be thought of as classi\ufb01er guidance\nwithout a classi\ufb01er, and our results showing the effectiveness of classi\ufb01er-free guidance con\ufb01rm that\npure generative diffusion models are capable of maximizing classi\ufb01er-based sample quality metrics\nwhile entirely avoiding classi\ufb01er gradients. We look forward to further explorations of classi\ufb01er-free\nguidance in a wider variety of settings and data modalities.\n7\nACKNOWLEDGEMENTS\nWe thank Ben Poole and Mohammad Norouzi for discussions.\nREFERENCES\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high \ufb01delity\nnatural image synthesis. In International Conference on Learning Representations, 2019.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-\nGrad: Estimating gradients for waveform generation. International Conference on Learning\nRepresentations, 2021.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. arXiv preprint\narXiv:2105.05233, 2021.\nYves Grandvalet and Yoshua Bengio. Semi-supervised learning by entropy minimization. In\nProceedings of the 17th International Conference on Neural Information Processing Systems, pp.\n529\u2013536, 2004.\nPeter Gr\u00a8unwald and John Langford. Suboptimal behavior of bayes and mdl in classi\ufb01cation under\nmisspeci\ufb01cation. Machine Learning, 66(2-3):119\u2013149, 2007.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances\nin Neural Information Processing Systems, pp. 6626\u20136637, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems, pp. 6840\u20136851, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans.\nCascaded diffusion models for high \ufb01delity image generation. arXiv preprint arXiv:2106.15282,\n2021.\nAapo Hyv\u00a8arinen and Peter Dayan. Estimation of non-normalized statistical models by score matching.\nJournal of Machine Learning Research, 6(4), 2005.\nDiederik P Kingma and Prafulla Dhariwal. Glow: Generative \ufb02ow with invertible 1x1 convolutions.\nIn Advances in Neural Information Processing Systems, pp. 10215\u201310224, 2018.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. arXiv\npreprint arXiv:2107.00630, 2021.\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile\nDiffusion Model for Audio Synthesis. International Conference on Learning Representations,\n2021.\nAlex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. International\nConference on Machine Learning, 2021.\nAli Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-\ufb01delity images with\nVQ-VAE-2. In Advances in Neural Information Processing Systems, pp. 14837\u201314847, 2019.\n10\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,\nAndrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition\nchallenge. International Journal of Computer Vision, 115(3):211\u2013252, 2015.\nTim Salimans and Jonathan Ho. Should EBMs model the energy or the score? In Energy Based\nModels Workshop-ICLR 2021, 2021.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training GANs. In Advances in Neural Information Processing Systems,\npp. 2234\u20132242, 2016.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\npp. 2256\u20132265, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn Advances in Neural Information Processing Systems, pp. 11895\u201311907, 2019.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of\nscore-based diffusion models. arXiv e-prints, pp. arXiv\u20132101, 2021a.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. International\nConference on Learning Representations, 2021b.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Computa-\ntion, 23(7):1661\u20131674, 2011.\nYan Wu, Jeff Donahue, David Balduzzi, Karen Simonyan, and Timothy Lillicrap. LOGAN: Latent\noptimisation for generative adversarial networks. arXiv preprint arXiv:1912.00953, 2019.\n11\nA\nSAMPLES\n(a) Non-guided conditional sampling: FID=1.80, IS=53.71\n(b) Classi\ufb01er-free guidance with w = 1.0: FID=12.6, IS=170.1\n(c) Classi\ufb01er-free guidance with w = 3.0: FID=24.83, IS=250.4\nFigure 6: Classi\ufb01er-free guidance on ImageNet 64x64. Left: random classes. Right: single class\n(malamute). The same random seed was used for sampling in each sub\ufb01gure.\n12\n(a) Non-guided conditional sampling: FID=7.27, IS=82.45\n(b) Classi\ufb01er-free guidance with w = 1.0: FID=7.86, IS=297.98\n(c) Classi\ufb01er-free guidance with w = 4.0: FID=21.53, IS=421.03\nFigure 7: Classi\ufb01er-free guidance on ImageNet 128x128. Left: random classes. Right: single class\n(malamute). The same random seed was used for sampling in each sub\ufb01gure.\n13\nFigure 8: More examples of classi\ufb01er-free guidance on 128x128 ImageNet. Left: non-guided samples,\nright: classi\ufb01er-free guided samples with w = 3.0.\n14\n",
    "2211.15657": "Published as a conference paper at ICLR 2023\nIS CONDITIONAL GENERATIVE MODELING ALL YOU\nNEED FOR DECISION-MAKING?\nAnurag Ajay\u2217\u2020\u00a7\u00b6, Yilun Du *\u00a7\u00b6, Abhi Gupta*\u2021\u00a7\u00b6, Joshua Tenenbaum\u00b6, Tommi Jaakkola\u2021\u00a7\u00b6,\nPulkit Agrawal\u2020\u00a7\u00b6\nImprobable AI Lab\u2020\nOperations Research Center\u2021\nComputer Science and Artificial Intelligence Lab\u00a7\nMassachusetts Institute of Technology\u00b6\nABSTRACT\nRecent improvements in conditional generative modeling have made it possible\nto generate high-quality images from language descriptions alone. We investigate\nwhether these methods can directly address the problem of sequential decision-\nmaking. We view decision-making not through the lens of reinforcement learning\n(RL), but rather through conditional generative modeling. To our surprise, we\nfind that our formulation leads to policies that can outperform existing offline\nRL approaches across standard benchmarks. By modeling a policy as a return-\nconditional diffusion model, we illustrate how we may circumvent the need for\ndynamic programming and subsequently eliminate many of the complexities\nthat come with traditional offline RL. We further demonstrate the advantages\nof modeling policies as conditional diffusion models by considering two other\nconditioning variables: constraints and skills. Conditioning on a single constraint\nor skill during training leads to behaviors at test-time that can satisfy several\nconstraints together or demonstrate a composition of skills. Our results illustrate\nthat conditional generative modeling is a powerful tool for decision-making.\n1\nINTRODUCTION\nOver the last few years, conditional generative modeling has yielded impressive results in a\nrange of domains, including high-resolution image generation from text descriptions (DALL-E,\nImageGen) (Ramesh et al., 2022; Saharia et al., 2022), language generation (GPT) (Brown et al.,\n2020), and step-by-step solutions to math problems (Minerva) (Lewkowycz et al., 2022). The success\nof generative models in countless domains motivates us to apply them to decision-making.\nConveniently, there exists a wide body of research on recovering high-performing policies from data\nlogged by already operational systems (Kostrikov et al., 2022; Kumar et al., 2020; Walke et al., 2022).\nThis is particularly useful in real-world settings where interacting with the environment is not always\npossible, and exploratory decisions can have fatal consequences (Dulac-Arnold et al., 2021). With\naccess to such offline datasets, the problem of decision-making reduces to learning a probabilistic\nmodel of trajectories, a setting where generative models have already found success.\nIn offline decision-making, we aim to recover optimal reward-maximizing trajectories by stitching\ntogether sub-optimal reward-labeled trajectories in the training dataset. Prior works (Kumar et al.,\n2020; Kostrikov et al., 2022; Wu et al., 2019; Kostrikov et al., 2021; Dadashi et al., 2021; Ajay\net al., 2020; Ghosh et al., 2022) have tackled this problem with reinforcement learning (RL) that uses\ndynamic programming for trajectory stitching. To enable dynamic programming, these works learn\na value function that estimates the discounted sum of rewards from a given state. However, value\nfunction estimation is prone to instabilities due to function approximation, off-policy learning, and\nbootstrapping together, together known as the deadly triad (Sutton & Barto, 2018). Furthermore, to\nstabilize value estimation in offline regime, these works rely on heuristics to keep the policy within\nthe dataset distribution. These challenges make it difficult to scale existing offline RL algorithms.\n\u2217denotes equal contribution. Correspondence to aajay@mit.edu, yilundu@mit.edu, abhig@mit.edu\n1\narXiv:2211.15657v4  [cs.LG]  10 Jul 2023\nPublished as a conference paper at ICLR 2023\n\u03c4 \u03c4\n\u03c4 \u03c4\n\u03c4\n\u03c4\n\u03c4 \u03c4 \u03c4 \u03c4\n\u03c4 \u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4 \u03c4\n\u03c4\nskills\nconstraints\nreward\nlow\nx2 + y2 \u2265 r2\nx2 + y2 \u2264 R2\nrun\njump\nhigh\ncompose\nskills\nsatisfy\nconstraints\nmaximize\nreward\nr2 \u2264 x2 + y2 \u2264 R2\nrun and jump\noptimal\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4 \u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\n\u03c4\nDecision Diffuser\ngenerated \ntrajectories\nlabelled trajec\ntories\nFigure 1: Decision Making using Conditional Generative Modeling. Framing decision making as a\nconditional generative modeling problem allows us to maximize rewards, satisfy constraints and compose skills.\nIn this paper, we ask if we can perform dynamic programming to stitch together sub-optimal\ntrajectories to obtain an optimal trajectory without relying on value estimation. Since conditional\ndiffusion generative models can generate novel data points by composing training data (Saharia et al.,\n2022; Ramesh et al., 2022), we leverage it for trajectory stitching in offline decision-making. Given a\nfixed dataset of reward-labeled trajectories, we adapt diffusion models (Sohl-Dickstein et al., 2015) to\nlearn a return-conditional model of the trajectory. During inference, we use classifier-free guidance\nwith low-temperature sampling, which we hypothesize to implicitly perform dynamics programming\nto capture the best behaviors in the dataset and glean return maximizing trajectories (detailed in\nAppendix A). Our straightforward conditional generative modeling formulation outperforms existing\napproaches on standard D4RL tasks (Fu et al., 2020).\nViewing offline decision-making through the lens of conditional generative modeling allows going\nbeyond conditioning on returns (Figure 1). Consider an example (detailed in Appendix A) where a\nrobot with linear dynamics navigates an environment containing two concentric circles (Figure 2). We\nare given a dataset of state-action trajectories of the robot, each satisfying one of two constraints: (i)\nthe final position of the robot is within the larger circle, and (ii) the final position of the robot is outside\nthe smaller circle. With conditional diffusion modeling, we can use the datasets to learn a constraint-\nconditioned model that can generate trajectories satisfying any set of constraints. During inference,\nthe learned trajectory model can merge constraints from the dataset and generate trajectories that\nsatisfy the combined constraint. Figure 2 shows that the constraint-conditioned model can generate\ntrajectories such that the final position of the robot lies between the concentric circles.\nEnvironment\nTraining Dataset\nGeneration\n(x,y)\nx2 + y2 \u2264 R2\nr2 \u2264 x2 + y2 \u2264 R2\nx2 + y2 \u2265 r2\nFigure 2: Illustrative example. We visualize the 2d robot navigation environment and the constraints satisfied\nby the trajectories in the dataset derived from the environment. We show the ability of the conditional diffusion\nmodel to generate trajectories that satisfy the combined constraints.\nHere, we demonstrate the benefits of modeling policies as conditional generative models. First,\nconditioning on constraints allows policies to not only generate behaviors satisfying individual\nconstraints but also generate novel behaviors by flexibly combining constraints at test time. Further,\nconditioning on skills allows policies to not only imitate individual skills but also generate novel\nbehaviors by composing those skills. We instantiate this idea with a state-sequence based diffusion\nprobabilistic model (Ho et al., 2020) called Decision Diffuser, visualized in Figure 1. In summary,\nour contributions include (i) illustrating conditional generative modeling as an effective tool in\noffline decision making, (ii) using classifier-free guidance with low-temperature sampling, instead of\n2\nPublished as a conference paper at ICLR 2023\ndynamic programming, to get return-maximizing trajectories and, (iii) leveraging the framework of\nconditional generative modeling to combine constraints and compose skills during inference flexibly.\n2\nBACKGROUND\n2.1\nREINFORCEMENT LEARNING\nWe formulate the sequential decision-making problem as a discounted Markov Decision Process\n(MDP) defined by the tuple \u27e8\u03c10, S, A, T , R, \u03b3\u27e9, where \u03c10 is the initial state distribution, S and A are\nstate and action spaces, T : S \u00d7 A \u2192S is the transition function, R : S \u00d7 A \u00d7 S \u2192R gives the\nreward at any transition and \u03b3 \u2208[0, 1) is a discount factor (Puterman, 2014). The agent acts with a\nstochastic policy \u03c0 : S \u2192\u2206A, generating a sequence of state-action-reward transitions or trajectory\n\u03c4 := (sk, ak, rk)k\u22650 with probability p\u03c0(\u03c4) and return R(\u03c4) := P\nk\u22650 \u03b3krk. The standard objective\nin RL is to find a return-maximizing policy \u03c0\u2217= arg max\u03c0 E\u03c4\u223cp\u03c0[R(\u03c4)].\nTemporal Difference Learning\nTD methods (Fujimoto et al., 2018; Lillicrap et al., 2015) estimate\nQ\u2217(s, a) := E\u03c4\u223cp\u03c0\u2217[R(\u03c4)|s0 = s, a0 = a], the return achieved under the optimal policy \u03c0\u2217when\nstarting in state s and taking action a, with a parameterized Q-function. This requires minimizing the\nfollowing TD loss:\nLTD(\u03b8) := E(s,a,r,s\u2032)\u2208D[(r + \u03b3 max\na\u2032\u2208A Q\u03b8(s\u2032, a\u2032) \u2212Q\u03b8(s, a))2]\n(1)\nContinuous action spaces further require learning a parametric policy \u03c0\u03d5(a|s) that plays the role of\nthe maximizing action in equation 1. This results in a policy objective that must be maximized:\nJ (\u03d5) := Es\u2208D,a\u223c\u03c0\u03d5(\u00b7|s)[Q(s, a)]\n(2)\nHere, the dataset of transitions D evolves as the agent interacts with the environment and both Q\u03b8\nand \u03c0\u03d5 are trained together. These methods make use of function approximation, off-policy learning,\nand bootstrapping, leading to several instabilities in practice (Sutton, 1988; Van Hasselt et al., 2018).\nOffline RL\nIn this setting, we must find a return-maximizing policy from a fixed dataset of\ntransitions collected by an unknown behavior policy \u00b5 (Levine et al., 2020). Using TD-learning\nnaively causes the state visitation distribution d\u03c0\u03d5(s) to move away from the distribution of the\ndataset d\u00b5(s). In turn, the policy \u03c0\u03d5 begins to take actions that are substantially different from\nthose already seen in the data. Offline RL algorithms resolve this distribution-shift by imposing a\nconstraint of the form D(d\u03c0\u03d5||d\u00b5), where D is some divergence metric, directly in the TD-learning\nprocedure. The constrained optimization problem now demands additional hyper-parameter tuning\nand implementation heuristics to achieve any reasonable performance (Kumar et al., 2021). The\nDecision Diffuser, in comparison, doesn\u2019t have any of these disadvantages. It does not require\nestimating any kind of Q-function, thereby sidestepping TD methods altogether. It also does not face\nthe risk of distribution-shift as generative models are trained with maximum-likelihood estimation.\n2.2\nDIFFUSION PROBABILISTIC MODELS\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a specific type of generative model\nthat learn the data distribution q(x) from a dataset D := {xi}0\u2264i<M. They have been used most\nnotably for synthesizing high-quality images from text descriptions (Saharia et al., 2022; Nichol et al.,\n2021). Here, the data-generating procedure is modelled with a predefined forward noising process\nq(xk+1|xk) := N(xk+1; \u221a\u03b1kxk, (1 \u2212\u03b1k)I) and a trainable reverse process p\u03b8(xk\u22121|xk) :=\nN(xk\u22121|\u00b5\u03b8(xk, k), \u03a3k), where N(\u00b5, \u03a3) denotes a Gaussian distribution with mean \u00b5 and variance\n\u03a3, \u03b1k \u2208R determines the variance schedule, x0 := x is a sample, x1, x2, ..., xK\u22121 are the latents,\nand xK \u223cN(0, I) for carefully chosen \u03b1k and long enough K. Starting with Gaussian noise,\nsamples are then iteratively generated through a series of \u201ddenoising\u201d steps.\nAlthough a tractable variational lower-bound on log p\u03b8 can be optimized to train diffusion models,\nHo et al. (2020) propose a simplified surrogate loss:\nLdenoise(\u03b8) := Ek\u223c[1,K],x0\u223cq,\u03f5\u223cN (0,I)[||\u03f5 \u2212\u03f5\u03b8(xk, k)||2]\n(3)\nThe predicted noise \u03f5\u03b8(xk, k), parameterized with a deep neural network, estimates the noise \u03f5 \u223c\nN(0, I) added to the dataset sample x0 to produce noisy xk. This is equivalent to predicting the\nmean of p\u03b8(xk\u22121|xk) since \u00b5\u03b8(xk, k) can be calculated as a function of \u03f5\u03b8(xk, k) (Ho et al., 2020).\n3\nPublished as a conference paper at ICLR 2023\nst+h\nat+h\nreturns\nconstraints\nskills\nst+2\nat+1\nst+1\nat\nst+h\nat+h\nst+2\nat+1\nst+1\nat\nst+h\nat+h\nst+2\nat+1\nst+1\nst\nFigure 3: Planning with Decision Diffuser. Given the current state st and conditioning, Decision Diffuser\nuses classifier-free guidance with low-temperature sampling to generate a sequence of future states. It then uses\ninverse dynamics to extract and execute the action at that leads to the immediate future state st+1.\nGuided Diffusion\nModelling the conditional data distribution q(x|y) makes it possible to generate\nsamples with attributes of the label y. The equivalence between diffusion models and score-\nmatching (Song et al., 2021), which shows \u03f5\u03b8(xk, k) \u221d\u2207xk log p(xk), leads to two kinds of methods\nfor conditioning: classifier-guided (Nichol & Dhariwal, 2021) and classifier-free (Ho & Salimans,\n2022). The former requires training an additional classifier p\u03d5(y|xk) on noisy data so that samples\nmay be generated at test-time with the perturbed noise \u03f5\u03b8(xk, k) \u2212\u03c9\u221a1 \u2212\u00af\u03b1k\u2207xk log p(y|xk),\nwhere \u03c9 is referred to as the guidance scale. The latter does not separately train a classifier but\nmodifies the original training setup to learn both a conditional \u03f5\u03b8(xk, y, k) and an unconditional\n\u03f5\u03b8(xk, k) model for the noise.\nThe unconditional noise is represented, in practice, as the\nconditional noise \u03f5\u03b8(xk, \u00d8, k) where a dummy value \u00d8 takes the place of y. The perturbed noise\n\u03f5\u03b8(xk, k) + \u03c9(\u03f5\u03b8(xk, y, k) \u2212\u03f5\u03b8(xk, k)) is used to later generate samples.\n3\nGENERATIVE MODELING WITH THE DECISION DIFFUSER\nIt is useful to solve RL from offline data, both without relying on TD-learning and without risking\ndistribution-shift. To this end, we formulate sequential decision-making as the standard problem of\nconditional generative modeling:\nmax\n\u03b8\nE\u03c4\u223cD[log p\u03b8(x0(\u03c4)|y(\u03c4))]\n(4)\nOur goal is to estimate the conditional data distribution with p\u03b8 so we can later generate portions of a\ntrajectory x0(\u03c4) from information y(\u03c4) about it. Examples of y could include the return under the\ntrajectory, the constraints satisfied by the trajectory, or the skill demonstrated in the trajectory. We\nconstruct our generative model according to the conditional diffusion process:\nq(xk+1(\u03c4)|xk(\u03c4)),\np\u03b8(xk\u22121(\u03c4)|xk(\u03c4), y(\u03c4))\n(5)\nAs usual, q represents the forward noising process while p\u03b8 the reverse denoising process. In the\nfollowing, we discuss how we may use diffusion for decision making. First, we discuss the modeling\nchoices for diffusion in Section 3.1. Next, we discuss how we may utilize classifier-free guidance to\ncapture the best aspects of trajectories in Section 3.2. We then discuss the different behaviors that\nmay be implemented with conditional diffusion models in Section 3.3. Finally, we discuss practical\ntraining details of our approach in Section 3.4.\n3.1\nDIFFUSING OVER STATES\nIn images, the diffusion process is applied across all pixel values in an image. Na\u00a8\u0131vely, it would\ntherefore be natural to apply a similar process to model the state and actions of a trajectory. However,\nin the reinforcement learning setting, directly modeling actions using a diffusion process has several\n4\nPublished as a conference paper at ICLR 2023\npractical issues. First, while states are typically continuous in nature in RL, actions are more varied,\nand are often discrete in nature. Furthermore, sequences over actions, which are often represented as\njoint torques, tend to be more high-frequency and less smooth, making them much harder to predict\nand model (Tedrake, 2022). Due to these practical issues, we choose to diffuse only over states, as\ndefined below:\nxk(\u03c4) := (st, st+1, ..., st+H\u22121)k\n(6)\nHere, k denotes the timestep in the forward process and t denotes the time at which a state was visited\nin trajectory \u03c4. Moving forward, we will view xk(\u03c4) as a noisy sequence of states from a trajectory\nof length H. We represent xk(\u03c4) as a two-dimensional array with one column for each timestep of\nthe sequence.\nActing with Inverse-Dynamics.\nSampling states from a diffusion model is not enough for defining\na controller. A policy can, however, be inferred from estimating the action at that led the state st to\nst+1 for any timestep t in x0(\u03c4). Given two consecutive states, we generate an action according to\nthe inverse dynamics model (Agrawal et al., 2016; Pathak et al., 2018):\nat := f\u03d5(st, st+1)\n(7)\nNote that the same offline data used to train the reverse process p\u03b8 can also be used to learn f\u03d5. We\nillustrate in Table 2 how the design choice of directly diffusing state distributions, with an inverse\ndynamics model to predict action, significantly improves performance over diffusing across both\nstates and actions jointly. Furthermore, we empirically compare and analyze when to use inverse\ndynamics and when to diffuse over actions in Appendix F.\n3.2\nPLANNING WITH CLASSIFIER-FREE GUIDANCE\nGiven a diffusion model representing the different trajectories in a dataset, we next discuss how\nwe may utilize the diffusion model for planning. To use the model for planning, it is necessary\nto additionally condition the diffusion process on characteristics y(\u03c4). One approach could be to\ntrain a classifier p\u03d5(y(\u03c4)|xk(\u03c4)) to predict y(\u03c4) from noisy trajectories xk(\u03c4). In the case that y(\u03c4)\nrepresents the return under a trajectory, this would require estimating a Q-function, which requires a\nseparate, complex dynamic programming procedure.\nOne approach to avoid dynamic programming is to directly train a conditional diffusion model\nconditioned on the returns y(\u03c4) in the offline dataset. However, as our dataset consists of a set\nof sub-optimal trajectories, the conditional diffusion model will be polluted by such sub-optimal\nbehaviors. To circumvent this issue, we utilize classifier-free guidance (Ho & Salimans, 2022) with\nlow-temperature sampling, to extract high-likelihood trajectories in the dataset. We find that such\ntrajectories correspond to the best set of behaviors in the dataset. For a detailed discussion comparing\nQ-function guidance and classifier-free guidance, please refer to Appendix K. Formally, to implement\nclassifier free guidance, a x0(\u03c4) is sampled by starting with Gaussian noise xK(\u03c4) and refining\nxk(\u03c4) into xk\u22121(\u03c4) at each intermediate timestep with the perturbed noise:\n\u02c6\u03f5 := \u03f5\u03b8(xk(\u03c4), \u00d8, k) + \u03c9(\u03f5\u03b8(xk(\u03c4), y(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k)),\n(8)\nwhere the scalar \u03c9 applied to (\u03f5\u03b8(xk(\u03c4), y(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k)) seeks to augment and extract\nthe best portions of trajectories in the dataset that exhibit y(\u03c4). With these ingredients, sampling from\nthe Decision Diffuser becomes similar to planning in RL. First, we observe a state in the environment.\nNext, we sample states later into the horizon with our diffusion process conditioned on y and history\nof last C states observed. Finally, we identify the action that should be taken to reach the most\nimmediate predicted state with our inverse dynamics model. This procedure repeats in a standard\nreceding-horizon control loop described in Algorithm 1 and visualized in Figure 3.\n3.3\nCONDITIONING BEYOND RETURNS\nSo far we have not explicitly defined the conditioning variable y(\u03c4). Though we have mentioned that\nit can be the return under a trajectory, we may also consider guiding our diffusion process towards\nsequences of states that satisfy relevant constraints or demonstrate specific behavior.\nMaximizing Returns\nTo generate trajectories that maximize return, we condition the noise\nmodel on the return of a trajectory so \u03f5\u03b8(xk(\u03c4), y(\u03c4), k) := \u03f5\u03b8(xk(\u03c4), R(\u03c4), k). These returns are\nnormalized to keep R(\u03c4) \u2208[0, 1]. Sampling a high return trajectory amounts to conditioning on\n5\nPublished as a conference paper at ICLR 2023\nAlgorithm 1 Conditional Planning with the Decision Diffuser\n1: Input: Noise model \u03f5\u03b8, inverse dynamics f\u03d5, guidance scale \u03c9, history length C, condition y\n2: Initialize h \u2190Queue(length = C), t \u21900\n// Maintain a history of length C\n3: while not done do\n4:\nObserve state s; h.insert(s); Initialize xK(\u03c4) \u223cN(0, \u03b1I)\n5:\nfor k = K . . . 1 do\n6:\nxk(\u03c4)[: length(h)] \u2190h\n// Constrain plan to be consistent with history\n7:\n\u02c6\u03f5 \u2190\u03f5\u03b8(xk(\u03c4), k) + \u03c9(\u03f5\u03b8(xk(\u03c4), y, k) \u2212\u03f5\u03b8(xk(\u03c4), k))\n// Classifier-free guidance\n8:\n(\u00b5k\u22121, \u03a3k\u22121) \u2190Denoise(xk(\u03c4), \u02c6\u03f5)\n9:\nxk\u22121 \u223cN(\u00b5k\u22121, \u03b1\u03a3k\u22121)\n10:\nend for\n11:\nExtract (st, st+1) from x0(\u03c4)\n12:\nExecute at = f\u03d5(st, st+1); t \u2190t + 1\n13: end while\nR(\u03c4) = 1. Note that we do not make use of any Q-values, which would then require dynamic\nprogramming.\nSatisfying Constraints\nTrajectories may satisfy a variety of constraints, each represented by the\nset Ci, such as reaching a specific goal, visiting states in a particular order, or avoiding parts of the\nstate space. To generate trajectories satisfying a given constraint Ci, we condition the noise model on\na one-hot encoding so that \u03f5\u03b8(xk(\u03c4), y(\u03c4), k) := \u03f5\u03b8(xk(\u03c4), 1(\u03c4 \u2208Ci), k). Although we train with\nan offline dataset in which trajectories satisfy only one of the available constraints, at inference we\ncan satisfy several constraints together.\nComposing Skills\nA skill i can be specified from a set of demonstrations Bi. To generate\ntrajectories that demonstrate a given skill, we condition the noise model on a one-hot encoding so\nthat \u03f5\u03b8(xk(\u03c4), y(\u03c4), k) := \u03f5\u03b8(xk(\u03c4), 1(\u03c4 \u2208Bi), k). Although we train with individual skills, we\nmay further compose these skills together during inference.\nAssuming we have learned the data distributions q(x0(\u03c4)|y1(\u03c4)), . . . , q(x0(\u03c4)|yn(\u03c4)) for\nn different conditioning variables, we can sample from the composed data distribution\nq(x0(\u03c4)|y1(\u03c4), . . . , yn(\u03c4)) using the perturbed noise (Liu et al., 2022):\n\u02c6\u03f5 := \u03f5\u03b8(xk(\u03c4), \u00d8, k) + \u03c9\nn\nX\ni=1\n(\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k))\n(9)\nThis property assumes that {yi(\u03c4)}n\ni=1 are conditionally independent given the state trajectory x0(\u03c4).\nHowever, we empirically observe that this assumption doesn\u2019t have to be strictly satisfied as long as\nthe composition of conditioning variables is feasible. For more detailed discussion, please refer to\nAppendix D. We use this property to compose more than one constraint or skill together at test-time.\nWe also show how Decision Diffuser can avoid particular constraint or skill (NOT) in Appendix J.\n3.4\nTRAINING THE DECISION DIFFUSER\nThe Decision Diffuser, our conditional generative model for decision-making, is trained in a\nsupervised manner. Given a dataset D of trajectories, each labeled with the return it achieves,\nthe constraint that it satisfies, or the skill that it demonstrates, we simultaneously train the reverse\ndiffusion process p\u03b8, parameterized through the noise model \u03f5\u03b8, and the inverse dynamics model f\u03d5\nwith the following loss:\nL(\u03b8, \u03d5) := Ek,\u03c4\u2208D,\u03b2\u223cBern(p)[||\u03f5\u2212\u03f5\u03b8(xk(\u03c4), (1\u2212\u03b2)y(\u03c4)+\u03b2\u00d8, k)||2]+E(s,a,s\u2032)\u2208D[||a\u2212f\u03d5(s, s\u2032)||2]\nFor each trajectory \u03c4, we first sample noise \u03f5 \u223cN(0, I) and a timestep k \u223cU{1, . . . , K}. Then, we\nconstruct a noisy array of states xk(\u03c4) and finally predict the noise as \u02c6\u03f5\u03b8 := \u03f5\u03b8(xk(\u03c4), y(\u03c4), k). Note\nthat with probability p we ignore the conditioning information and the inverse dynamics is trained\nwith individual transitions rather than trajectories.\nArchitecture\nWe parameterize \u03f5\u03b8 with a temporal U-Net architecture, a neural network consisting\nof repeated convolutional residual blocks (Janner et al., 2022). This effectively treats a sequence of\nstates xk(\u03c4) as an image where the height represents the dimension of a single state and the width\n6\nPublished as a conference paper at ICLR 2023\nD4RL Locomotion\n25\n50\n75\n100\nDecision Diffuser\nPerformance\nD4RL Kitchen\nKuka Block Stacking\nTD-learning\nBehavior Cloning\nFigure 4: Results Overview. Decision Diffuser performs better than both TD learning (CQL) and Behavorial\nCloning (BC) across D4RL locomotion tasks, D4RL Kitchen tasks and Kuka Block Stacking tasks (single\nconstraint) using only a conditional generative modeling objective. For performance metric, we use normalized\naverage returns (Fu et al., 2020) for D4RL tasks (Locomotion and Kitchen) and success rate for Block Stacking.\ndenotes the length of the trajectory. We encode the conditioning information y(\u03c4) as either a scalar\nor a one-hot vector and project it into a latent variable z \u2208Rh with a multi-layer perceptron (MLP).\nWhen y(\u03c4) = \u00d8, we zero out the entries of z. We also parameterize the inverse dynamics f\u03d5 with an\nMLP. For implementation details, please refer to the Appendix B.\nLow-temperature Sampling\nIn the denoising step of Algorithm 1, we compute \u00b5k\u22121 and\n\u03a3k\u22121 from a noisy sequence of states and a predicted noise. We find that sampling xk\u22121 \u223c\nN(\u00b5k\u22121, \u03b1\u03a3k\u22121) where the variance is scaled by \u03b1 \u2208[0, 1) leads to better quality sequences\n(corresponding to sampling lower temperature samples). For a proper ablation study, please refer to\nAppendix C.\n4\nEXPERIMENTS\nIn our experiments section, we explore the efficacy of the Decision Diffuser on a variety of different\ndecision making tasks (performance illustrated in Figure 4). In particular, we evaluate (1) the ability\nto recover effective RL policies from offline data, (2) the ability to generate behavior that satisfies\nmultiple sets of constraints, (3) the ability compose multiple different skills together. In addition,\nwe empirically justify use of classifier-free guidance, low-temperature sampling (Appendix C), and\ninverse dynamics (Appendix F) and test the robustness of Decision Diffuser to stochastic dynamics\n(Appendix G).\n4.1\nOFFLINE REINFORCEMENT LEARNING\nSetup\nWe first test whether the Decision Diffuser can generate return-maximizing trajectories.\nTo test this, we train a state diffusion process and inverse dynamics model on publicly available\nD4RL datasets (Fu et al., 2020). We compare with existing offline RL methods, including model-\nfree algorithms like CQL (Kumar et al., 2020) and IQL (Kostrikov et al., 2022), and model-based\nalgorithms such as trajectory transformer (TT, Janner et al. (2021)) and MoReL (Kidambi et al., 2020).\nWe also compare with sequence-models like the Decision Transformer (DT) (Chen et al. (2021) and\ndiffusion models like Diffuser (Janner et al., 2022).\nResults\nAcross a broad suite of different offline reinforcement learning tasks, we find that the\nDecision Diffuser is either competitive or outperforms many of our offline RL baselines (Table 1). It\nalso outperforms Diffuser and sequence modeling approaches, such as Decision Transformer and\nTrajectory Transformer. The difference between Decision Diffuser and other methods becomes even\nmore significant on harder D4RL Kitchen tasks which require long-term credit assignment.\nTo convey the importance of classifier-free guidance, we also compare with the baseline\nCondDiffuser, which diffuses over both state and action sequences as in Diffuser without classifier-\nguidance.\nIn Table 2, we observe that CondDiffuser improves over Diffuser in 2 out of 3\nenvironments. Decision Diffuser further improves over CondDiffuser, performing better across all\n3 environments. We conclude that learning the inverse dynamics is a good alternative to diffusing\nover actions. We further empirically analyze when to use inverse dynamics and when to diffuse\nover actions in Appendix F. We also compare against CondMLPDiffuser, a policy where the current\n7\nPublished as a conference paper at ICLR 2023\naction is denoised according to a diffusion process conditioned on both the state and return. We see\nthat CondMLPDiffuser performs the worst amongst diffusion models. Till now, we mainly tested on\noffline RL tasks that have deterministic (or near deterministic) environment dynamics. Hence, we\ntest the robustness of Decision Diffuser to stochastic dynamics and compare it to Diffuser and CQL as\nwe vary the stochasticity in environment dynamics, in Appendix G. Finally, we analyze the runtime\ncharacteristics of Decision Diffuser in Appendix E.\n4.2\nCONSTRAINT SATISFACTION\nSetup We next evaluate how well we can generate trajectories that satisfy a set of constraints using\nthe Kuka Block Stacking environment (Janner et al., 2022) visualized in Figure 5. In this domain,\nthere are four blocks which can be stacked as a single tower or rearranged into several towers. A\nconstraint like BlockHeight(i) > BlockHeight(j) requires that block i be placed above block\nj. We train the Decision Diffuser from 10, 000 expert demonstrations each satisfying one of these\nconstraints. We randomize the positions of these blocks and consider two tasks at inference: sampling\ntrajectories that satisfy a single constraint seen before in the dataset or satisfy a group of constraints\nfor which demonstrations were never provided. In the latter, we ask the Decision Diffuser to generate\ntrajectories so BlockHeight(i) > BlockHeight(j) > BlockHeight(k) for three of the four\nblocks i, j, k. For more details, please refer to Appendix H.\nResults\nIn both the stacking and rearrangement settings, Decision Diffuser satisfies single\nconstraints with greater success rate than Diffuser (Table 3). We also compare with BCQ (Fujimoto\net al., 2019) and CQL (Kumar et al., 2020), but they consistently fail to stack or rearrange the blocks\nleading to a 0.0 success rate. Unlike these baselines, our method can just as effectively satisfy several\nconstraints together according to Equation 9. For a visualization of these generated trajectories,\nplease see the website https://anuragajay.github.io/decision-diffuser/.\n4.3\nSKILL COMPOSITION\nSetup\nFinally, we look at how to compose different skills together. We consider the Unitree-go-\nrunning environment (Margolis & Agrawal, 2022), where a quadruped robot can be found running\nwith various gaits, like bounding, pacing, and trotting. We explore if it is possible to generate\ntrajectories that transition between these gaits after only training on individual gaits. For each gait,\nwe collect a dataset of 2500 demonstrations on which we train Decision Diffuser.\nResults\nDuring testing, we use the noise model of our reverse diffusion process according to\nequation 9 to sample trajectories of the quadruped robot with entirely new running behavior. Figure 6\nshows a trajectory that begins with bounding but ends with pacing. Appendix I provides additional\nvisualizations of running gaits being composed together. Although it visually appears that trajectories\nDataset\nEnvironment\nBC\nCQL\nIQL\nDT\nTT\nMOReL\nDiffuser\nDD\nMed-Expert\nHalfCheetah\n55.2\n91.6\n86.7\n86.8\n95\n53.3\n79.8\n90.6 \u00b11.3\nMed-Expert\nHopper\n52.5\n105.4\n91.5\n107.6 110.0 108.7\n107.2\n111.8 \u00b11.8\nMed-Expert\nWalker2d\n107.5 108.8\n109.6\n108.1 101.9 95.6\n108.4\n108.8 \u00b11.7\nMedium\nHalfCheetah\n42.6\n44.0\n47.4\n42.6\n46.9\n42.1\n44.2\n49.1 \u00b11.0\nMedium\nHopper\n52.9\n58.5\n66.3\n67.6\n61.1\n95.4\n58.5\n79.3 \u00b13.6\nMedium\nWalker2d\n75.3\n72.5\n78.3\n74.0\n79\n77.8\n79.7\n82.5 \u00b11.4\nMed-Replay\nHalfCheetah\n36.6\n45.5\n44.2\n36.6\n41.9\n40.2\n42.2\n39.3 \u00b14.1\nMed-Replay\nHopper\n18.1\n95\n94.7\n82.7\n91.5\n93.6\n96.8\n100 \u00b10.7\nMed-Replay\nWalker2d\n26.0\n77.2\n73.9\n66.6\n82.6\n49.8\n61.2\n75 \u00b14.3\nAverage\n51.9\n77.6\n77\n74.7\n78.9\n72.9\n75.3\n81.8\nMixed\nKitchen\n51.5\n52.4\n51\n-\n-\n-\n-\n65 \u00b12.8\nPartial\nKitchen\n38\n50.1\n46.3\n-\n-\n-\n-\n57 \u00b12.5\nAverage\n44.8\n51.2\n48.7\n-\n-\n-\n-\n61\nTable 1: Offline Reinforcement Learning Performance. We show that Decision Diffuser (DD) either matches\nor outperforms current offline RL approaches on D4RL tasks in terms of normalized average returns (Fu et al.,\n2020). We report the mean and the standard error over 5 random seeds.\n8\nPublished as a conference paper at ICLR 2023\nHopper-*\nDiffuser CondDiffuser CondMLPDiffuser Decision Diffuser\nMed-Expert\n107.6\n111.3\n105.6\n111.8 \u00b11.6\nMedium\n58.5\n66.3\n54.1\n79.3 \u00b13.6\nMed-Replay\n96.8\n76.5\n66.5\n100 \u00b10.7\nTable 2: Ablations. Using classifier-free guidance with Diffuser, resulting in CondDiffuser, improves\nperformance in 2 (out of 3) environments. Additionally, using inverse dynamics for action prediction in Decision\nDiffuser improves performance in all 3 environments. CondMLPDiffuser, that diffuses over current action\ngiven the current state and the target return, doesn\u2019t perform as well.\nFigure 5:\nKuka Block\nStacking task.\nEnvironment\nDiffuser\nDD\nSingle Constraint - Stacking\n45.6 \u00b13.1\n58.0 \u00b13.1\nSingle Constraint - Rearrangement\n58.9 \u00b13.4\n62.7 \u00b13.1\nSingle Constraint Average\n52.3\n60.4\nMultiple Constraints - Stacking\n-\n60.3 \u00b13.1\nMultiple Constraints - Rearrangement\n-\n67.2 \u00b13.1\nMultiple Constraints Average\n-\n63.8\nTable 3: Block Stacking through Constraint Minimization. Decision\nDiffuser (DD) improves over Diffuser in terms of the success rate of\ngenerating trajectories satisfying a set of block-stacking constraints. It\ncan also flexibly combine multiple constraints during test time. We report\nthe mean success rate and the standard error over 5 random seeds.\ngenerated with the Decision Diffuser contain more than one gait, we would like to quantify exactly\nhow well different gaits can be composed. To this end, we train a classifier to predict at every\ntime-step or frame in a trajectory the running gait of the quadruped (i.e. bound, pace, or trott). We\nreuse the demonstrations collected for training the Decision Diffuser to also train this classifier, where\nour inputs are defined as robot joint states over a fixed period of time (i.e. state sub-sequences of\nlength 10) and the label is the gait demonstrated in this sequence. The complete details of our gait\nclassification procedure can be found in Appendix I.\nTrott\nGait Probability\nOnly Bound\nOnly Pace\nBound + Pace\n50\n100\n150\n200\n50\n100\n150\n200\n50\n100\n150\n200\n.20\n.40\n.60\n.80\nPace\nBound\nCondition\nTrott\nPace\nBound\nOnly Bound\n0.8\n1.0\n98.2\nOnly Pace\n1.4\n97.7\n0.9\nBound + Pace\n1.4\n38.5\n60.1\nFigure 7: Classifying Running Gaits. A classifier predicts the running gait of the quadruped at every timestep.\nOn trajectories generated by conditioning on a single skill, like only bounding or pacing, the classifier predicts\nthe respective gait with largest probability. When conditioned on both skills, some timesteps are classified as\nbounding while others as pacing.\nWe use our running gait classifier in two ways: to evaluate how the behavior of the quadruped changes\nover the course of a single, generated trajectory and to measure how often each gait emerges over\nseveral generated trajectories. In the former, we first sample three trajectories from the Decision\nDiffuser conditioned either on the bounding gait, the pacing gait, or both. For every trajectory, we\nseparately plot the classification probability of each gait over the length of the sequence. As shown in\nthe plots of Figure 7, the classifier predicts bound and pace respectively to be the most likely running\ngait in trajectories sampled with this condition. When the trajectory is generated by conditioning\non both gaits, the classifier transitions between predicting one gait with largest probability to the\nother. In fact, there are several instances where the behavior of the quadruped switches between\nbounding and pacing according to the classifier. This is consistent with the visualizations reported in\nFigure 6. In the table depicted in Figure 7, we consider 1000 trajectories generated with the Decision\nDiffuser when conditioned on one or both of the gaits as listed. We record the fraction of time that the\nquadruped\u2019s running gait was classified as either trott, pace, or bound. It turns out that the classifier\n9\nPublished as a conference paper at ICLR 2023\nBound\nPace\nBound + Pace\n \nFigure 6: Composing Movement Skills. Decision Diffuser can imitate individual running gaits using expert\ndemonstrations and compose multiple different skills together during test time. The results are best illustrated by\nvideos viewable at https://anuragajay.github.io/decision-diffuser/.\nidentifies the behavior as bounding for 38.5% of the time and as pacing for the other 60.1% when\ntrajectories are sampled by composing both gaits. This corroborates the fact that the Decision Diffuser\ncan indeed compose running behaviors despite only being trained on individual gaits.\n5\nRELATED WORK\nDiffusion Models\nDiffusion Models have shown great promise in learning generative models of\nimage and text data (Saharia et al., 2022; Nichol et al., 2021; Nichol & Dhariwal, 2021). It formulates\nthe data sampling process as an iterative denoising procedure (Sohl-Dickstein et al., 2015; Ho et al.,\n2020). The denoising procedure can be alternatively interpreted as parameterizing the gradients\nof the data distribution (Song et al., 2021) optimizing the score matching objective (Hyv\u00a8arinen,\n2005) and thus as a Energy-Based Model (Du & Mordatch, 2019; Nijkamp et al., 2019; Grathwohl\net al., 2020). To generate data samples (eg: images) conditioned on some additional information\n(eg:text), prior works (Nichol & Dhariwal, 2021) have learned a classifier to facilitate the conditional\nsampling. More recent works (Ho & Salimans, 2022) have argued to leverage gradients of an implicit\nclassifier, formed by the difference in score functions of a conditional and an unconditional model, to\nfacilitate conditional sampling. The resulting classifier-free guidance has been shown to generate\nbetter conditional samples than classifier-based guidance. All these above mentioned works have\nmostly focused on generation of text or images. Recent works have also used diffusion models to\nimitate human behavior (Pearce et al., 2023) and to parameterize policy in offline RL (Wang et al.,\n2022). Janner et al. (2022) generate trajectories consisting of states and actions with an unconditional\ndiffusion model, therefore requiring a trained reward function on noisy state-action pairs. At inference,\nthe estimated reward function guides the reverse diffusion process towards samples of high-return\ntrajectories. In contrast, we do not train reward functions or diffusion processes separately, but rather\nmodel the trajectories in our dataset with a single, conditional generative model instead. This ensures\nthat the sampling procedure of the learned diffusion process is the same at inference as it is during\ntraining.\nReward Conditioned Policies\nPrior works (Kumar et al., 2019; Schmidhuber, 2019; Srivastava\net al., 2019; Emmons et al., 2021; Chen et al., 2021) have studied learning of reward conditioned\npolicies via reward conditioned behavioral cloning. Chen et al. (2021) used a transformer (Vaswani\net al., 2017) to model the reward conditioned policies and obtained a performance competitive with\noffline RL approaches. Emmons et al. (2021) obtained similar performance as Chen et al. (2021)\nwithout using a transformer policy but relied on careful capacity tuning of MLP policy. In contrast to\nthese works, in addition to modeling returns, Decision Diffuser can also model constraints or skills\nand generate novel behaviors by flexibly combining multiple constraints or skills during test time.\n6\nDISCUSSION\nWe propose Decision Diffuser, a conditional generative model for sequential decision making. It\nframes offline sequential decision making as conditional generative modeling and sidesteps the\nneed of reinforcement learning, thereby making the decision making pipeline simpler. By sampling\nfor high returns, it is able to capture the best behaviors in the dataset and outperforms existing\noffline RL approaches on standard benchmarks (such as D4RL). In addition to returns, it can also\n10\nPublished as a conference paper at ICLR 2023\nbe conditioned on constraints or skills and can generate novel behaviors by flexibly combining\nconstraints or composing skills during test time.\nIn this work, we focused on offline sequential decision making, thus circumventing the need for\nexploration. Using ideas from Zheng et al. (2022), future works could look into online fine-tuning of\nDecision Diffuser by leveraging entropy of the state-sequence model for exploration. While our work\nfocused on state based environments, it can be extended to image based environments by performing\nthe diffusion in latent space, rather than observation space, as done in Rombach et al. (2022). For a\ndetailed discussion on limitations of Decision Diffuser, please refer to Appendix M.\nACKNOWLEDGEMENTS\nThe authors would like to thank Ofir Nachum, Anthony Simeonov and Richard Li for their helpful\nfeedback on an earlier draft of the work; Jay Whang and Ge Yang for discussions on classifier-\nfree guidance; Gabe Margolis for helping with unitree experiments; Micheal Janner for providing\nvisualization code for Kuka block stacking; and the members of Improbable AI Lab for discussions\nand helpful feedback. We thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center\nfor providing compute resources. This research was supported by an NSF graduate fellowship, a\nDARPA Machine Common Sense grant, ARO MURI Grant Number W911NF-21-1-0328 and an\nMIT-IBM grant.\nThis research was also partly sponsored by the United States Air Force Research Laboratory and the\nUnited States Air Force Artificial Intelligence Accelerator and was accomplished under Cooperative\nAgreement Number FA8750-19- 2-1000. The views and conclusions contained in this document\nare those of the authors and should not be interpreted as representing the official policies, either\nexpressed or implied, of the United States Air Force or the U.S. Government. The U.S. Government\nis authorized to reproduce and distribute reprints for Government purposes, notwithstanding any\ncopyright notation herein.\nAUTHOR CONTRIBUTIONS\nAnurag Ajay conceived the framework of viewing decision-making as conditional diffusion\ngenerative modeling, implemented the Decision Diffuser algorithm, ran experiments on Offline\nRL and Skill Composition, and helped in paper writing.\nYilun Du helped in conceiving the framework of viewing decision-making as conditional diffusion\ngenerative modeling, ran experiments on Constraint Satisfaction, helped in paper writing and advised\nAnurag.\nAbhi Gupta helped in running experiments on Offline RL and Skill Composition, participated in\nresearch discussions, and played the leading role in paper writing and making figures.\nJoshua Tenenbaum participated in research discussions.\nTommi Jaakkola participated in research discussions and suggested the experiment of classifying\nrunning gaits.\nPulkit Agrawal was involved in research discussions, suggested experiments related to dynamic\nprogramming, provided feedback on writing, positioning of the work and overall advising.\nREFERENCES\nPulkit Agrawal, Ashvin V Nair, Pieter Abbeel, Jitendra Malik, and Sergey Levine. Learning to poke\nby poking: Experiential learning of intuitive physics. Advances in neural information processing\nsystems, 29, 2016.\nAnurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Ofir Nachum. Opal: Offline primitive\ndiscovery for accelerating offline reinforcement learning. arXiv preprint arXiv:2010.13611, 2020.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\n11\nPublished as a conference paper at ICLR 2023\nHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,\nJeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,\nand Dario Amodei. Language models are few-shot learners. In Advances in Neural Information\nProcessing Systems, 2020.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. In Advances in Neural Information Processing Systems, 2021.\nRobert Dadashi, Shideh Rezaeifar, Nino Vieillard, L\u00b4eonard Hussenot, Olivier Pietquin, and Matthieu\nGeist. Offline reinforcement learning with pseudometric learning. arXiv preprint arXiv:2103.01948,\n2021.\nSander Dieleman. Guidance: a cheat code for diffusion models, 2022. URL https://benanne.\ngithub.io/2022/05/26/guidance.html.\nYilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models. In\nAdvances in Neural Information Processing Systems, 2019.\nGabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,\nand Todd Hester. Challenges of real-world reinforcement learning: definitions, benchmarks and\nanalysis. Machine Learning, 110(9):2419\u20132468, 2021.\nScott Emmons, Benjamin Eysenbach, Ilya Kostrikov, and Sergey Levine. Rvs: What is essential for\noffline rl via supervised learning? arXiv preprint arXiv:2112.10751, 2021.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nScott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-\ncritic methods. In International conference on machine learning, pp. 1587\u20131596. PMLR, 2018.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. In International Conference on Machine Learning, 2019.\nDibya Ghosh, Anurag Ajay, Pulkit Agrawal, and Sergey Levine. Offline rl policies should be trained\nto be adaptive. In International Conference on Machine Learning, pp. 7513\u20137530. PMLR, 2022.\nWill Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, and Richard Zemel.\nLearning the stein discrepancy for training and evaluating energy-based models without sampling.\nIn International Conference on Machine Learning, 2020.\nAbhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, and Sergey Levine.\nMeta-\nreinforcement learning of structured exploration strategies. Advances in neural information\nprocessing systems, 31, 2018.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International Conference\non Machine Learning, 2018.\nJonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598,\n2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in\nNeural Information Processing Systems, 2020.\nAapo Hyv\u00a8arinen. Estimation of non-normalized statistical models by score matching. Journal of\nMachine Learning Research, 2005.\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. In Advances in Neural Information Processing Systems, 2021.\nMichael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. In International Conference on Machine Learning, 2022.\n12\nPublished as a conference paper at ICLR 2023\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. MOReL: Model-\nbased offline reinforcement learning. In Advances in Neural Information Processing Systems,\n2020.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Advances\nin neural information processing systems, 34:21696\u201321707, 2021.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International\nConference on Learning Representations, 2015.\nIlya Kostrikov, Jonathan Tompson, Rob Fergus, and Ofir Nachum. Offline reinforcement learning\nwith fisher divergence critic regularization. arXiv preprint arXiv:2103.08050, 2021.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit\nQ-learning. In International Conference on Learning Representations, 2022.\nAviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint\narXiv:1912.13465, 2019.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline\nreinforcement learning. In Advances in Neural Information Processing Systems, 2020.\nAviral Kumar, Anikait Singh, Stephen Tian, Chelsea Finn, and Sergey Levine. A workflow for offline\nmodel-free robotic reinforcement learning. arXiv preprint arXiv:2109.10813, 2021.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Offline reinforcement learning: Tutorial,\nreview, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay\nRamasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, et al.\nSolving\nquantitative reasoning problems with language models. arXiv preprint arXiv:2206.14858, 2022.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual\ngeneration with composable diffusion models. arXiv preprint arXiv:2206.01714, 2022.\nCalvin Luo. Understanding diffusion models: A unified perspective. arXiv preprint arXiv:2208.11970,\n2022.\nGabriel Margolis and Pulkit Agrawal. Walk these ways: Gait-conditioned policies yield diversified\nquadrupedal agility. In Conference on Robot Learning, 2022.\nDiganta Misra. Mish: A self regularized non-monotonic neural activation function. In British\nMachine Vision Conference, 2019.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nIn International Conference on Machine Learning, 2021.\nErik Nijkamp, Mitch Hill, Song-Chun Zhu, and Ying Nian Wu. Learning non-convergent non-\npersistent short-run MCMC toward energy-based model. In Advances in Neural Information\nProcessing Systems, 2019.\nPedro A Ortega, Markus Kunesch, Gr\u00b4egoire Del\u00b4etang, Tim Genewein, Jordi Grau-Moya, Joel Veness,\nJonas Buchli, Jonas Degrave, Bilal Piot, Julien Perolat, et al. Shaking the foundations: delusions\nin sequence models for interaction and control. arXiv preprint arXiv:2110.10819, 2021.\nKeiran Paster, Sheila McIlraith, and Jimmy Ba. You can\u2019t count on luck: Why decision transformers\nfail in stochastic environments. arXiv preprint arXiv:2205.15967, 2022.\n13\nPublished as a conference paper at ICLR 2023\nDeepak Pathak, Parsa Mahmoudieh, Guanghao Luo, Pulkit Agrawal, Dian Chen, Yide Shentu, Evan\nShelhamer, Jitendra Malik, Alexei A Efros, and Trevor Darrell. Zero-shot visual imitation. In\nProceedings of the IEEE conference on computer vision and pattern recognition workshops, pp.\n2050\u20132053, 2018.\nTim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,\nSergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating\nhuman behaviour with diffusion models. arXiv preprint arXiv:2301.10677, 2023.\nMartin L Puterman. Markov decision processes: discrete stochastic dynamic programming. John\nWiley & Sons, 2014.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 10684\u201310695, 2022.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed\nKamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al.\nPhotorealistic text-to-image diffusion models with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\nJuergen Schmidhuber. Reinforcement learning upside down: Don\u2019t predict rewards\u2013just map them to\nactions. arXiv preprint arXiv:1912.02875, 2019.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learning,\n2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon.\nDenoising diffusion implicit models.\nIn\nInternational Conference on Learning Representations, 2021.\nRupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja\u00b4skowski, and J\u00a8urgen Schmidhuber.\nTraining agents using upside-down reinforcement learning. arXiv preprint arXiv:1912.02877,\n2019.\nRichard S. Sutton. Learning to predict by the methods of temporal differences. Machine Learning,\n1988.\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nRuss Tedrake. Underactuated Robotics. 2022. URL http://underactuated.mit.edu.\nHado Van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, and Joseph\nModayil. Deep reinforcement learning and the deadly triad. arXiv preprint arXiv:1812.02648,\n2018.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information\nProcessing Systems, 2017.\nAdam R Villaflor, Zhe Huang, Swapnil Pande, John M Dolan, and Jeff Schneider. Addressing\noptimism bias in sequence modeling for reinforcement learning. In International Conference on\nMachine Learning, pp. 22270\u201322283. PMLR, 2022.\nHomer Walke, Jonathan Yang, Albert Yu, Aviral Kumar, Jedrzej Orbik, Avi Singh, and Sergey Levine.\nDon\u2019t start from scratch: Leveraging prior data to automate robotic reinforcement learning. arXiv\npreprint arXiv:2207.04703, 2022.\nZhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Diffusion policies as an expressive policy\nclass for offline reinforcement learning. arXiv preprint arXiv:2208.06193, 2022.\n14\nPublished as a conference paper at ICLR 2023\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\narXiv preprint arXiv:1911.11361, 2019.\nYuxin Wu and Kaiming He. Group normalization. In European Conference on Computer Vision,\n2018.\nMengjiao Yang, Dale Schuurmans, Pieter Abbeel, and Ofir Nachum.\nDichotomy of control:\nSeparating what you can control from what you cannot. arXiv preprint arXiv:2210.13435, 2022.\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical\nrisk minimization. arXiv preprint arXiv:1710.09412, 2017.\nQinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. arXiv preprint\narXiv:2202.05607, 2022.\n15\nPublished as a conference paper at ICLR 2023\nAppendix\nIn this appendix, we discuss details of the illustrative examples in Section A. Next, we discuss\nhyperparameters and architectural details in Section B. We analyze the importance of low temperature\nsampling in Section C, further explain composition of conditioning variable in Section D, discuss the\nrun-time characteristics of decision diffuser in Section E, discuss when to use inverse dynamics in\nSection F and analyze robustness of Decision Diffuser to stochastic dynamics in Section G. Finally,\nwe provide details of the Kuka Block Stacking environment in Section H and the Unitree environment\nin Section I.\nA\nILLUSTRATIVE EXAMPLES\nA.1\nIMPLICIT DYNAMIC PROGRAMMING\nTraining dataset\nGeneration\nFigure A1: Illustrative example. We demonstrate the ability of Decision Diffuser to stitch together sub-\noptimal trajectories in training dataset to obtain (near) optimal trajectories, thereby implicitly performing\ndynamic programming in Maze2D-open environment from Fu et al. (2020).\nWe empirically demonstrate the ability of Decision Diffuser to perform implicit dynamic programming\nin Maze2D-open environment from Fu et al. (2020). The task in Maze2D-open environment is to\nreach point C and the reward is negative distance from point C. The training dataset consists of 500\ntrajectories from point A to point B and 500 trajectories from point B to point C. The maximum\ntrajectory length is 50. During test time, the agent starts from point A and needs to reach point C\nas quickly as possible. As shown in Figure A1, Decision Diffuser can stitch trajectories in training\ndataset to form trajectories that goes from point A to point B in (near) straight lines.\nA.2\nCONSTRAINT COMBINATION\nSetup\nIn linear system robot navigation, Decision Diffuser is trained on 1000 expert trajectories\neither satisfying the constraint \u2225sT \u2225\u2264R (R = 1) or the constraint \u2225sT \u2225\u2265r (r = 0.7). Here,\nsT = [xT , yT ] represents the final robot state in a trajectory, specifying its final 2d position. The\nmaximum trajectory length is 50. During test time, Decision Diffuser is asked to generate trajectories\nsatisfying \u2225sT \u2225\u2264R and \u2225sT \u2225\u2265r to test its ability to satisfy single constraints. Furthermore,\nDecision Diffuser is also asked to generate trajectories satisfying r \u2264\u2225sT \u2225\u2264R to test its ability to\nsatisfy combined constraints.\nResults\nFigure 2 shows that Decision Diffuser learns to generate trajectories perfectly (i.e. with\n100% success rate) satisfying single constraints in linear system robot navigation. Furthermore, it\nlearns to generate trajectories satisfying the composed constraint in linear system robot navigation\nwith 91.3%(\u00b12.6%) accuracy where the standard error is calculated over 5 random seeds.\nB\nHYPERPARAMETER AND ARCHITECTURAL DETAILS\nIn this section, we describe various architectural and hyperparameter details:\n\u2022 We represent the noise model \u03f5\u03b8 with a temporal U-Net (Janner et al., 2022), consisting of a U-Net\nstructure with 6 repeated residual blocks. Each block consisted of two temporal convolutions, each\nfollowed by group norm (Wu & He, 2018), and a final Mish nonlinearity (Misra, 2019). Timestep\nand condition embeddings, both 128-dimensional vectors, are produced by separate 2-layered MLP\n(with 256 hidden units and Mish nonlinearity) and are concatenated together before getting added\nto the activations of the first temporal convolution within each block. We borrow the code for\ntemporal U-Net from https://github.com/jannerm/diffuser.\n16\nPublished as a conference paper at ICLR 2023\n\u2022 We represent the inverse dynamics f\u03d5 with a 2-layered MLP with 512 hidden units and ReLU\nactivations.\n\u2022 We represent the gait classifier with a 3-layered MLP with 1024 hidden units and ReLU activations.\n\u2022 We train \u03f5\u03b8 and f\u03d5 using the Adam optimizer (Kingma & Ba, 2015) with a learning rate of 2e \u22124\nand batch size of 32 for 2e6 train steps.\n\u2022 We train the gait classifier using the Adam optimizer with a learning rate of 2e \u22124 and batch size\nof 64 for 1e6 train steps.\n\u2022 We choose the probability p of removing the conditioning information to be 0.25.\n\u2022 We use K = 100 diffusion steps.\n\u2022 We use a planning horizon H of 100 in all the D4RL locomotion tasks, 56 in D4RL kitchen tasks,\n128 in Kuka block stacking, 56 in unitree-go-running tasks, 50 in the illustrative example and 60 in\nBlock push tasks.\n\u2022 We use a guidance scale s \u2208{1.2, 1.4, 1.6, 1.8} but the exact choice varies by task.\n\u2022 We choose \u03b1 = 0.5 for low temperature sampling.\n\u2022 We choose context length C = 20.\nC\nIMPORTANCE OF LOW TEMPERATURE SAMPLING\nIn Algorithm 1, we compute \u00b5k\u22121 and \u03a3k\u22121 from a noisy sequence of states and predicted noise. We\nfind that sampling xk\u22121 \u223cN(\u00b5k\u22121, \u03b1\u03a3k\u22121) (where \u03b1 \u2208[0, 1)) with a reduced variance produces\nhigh-likelihood state sequences. We refer to this as low-temperature sampling. To empirically show\nits importance, we compare performances of Decision Diffuser with different values of \u03b1 (Table A1).\nWe show that low temperature sampling (\u03b1 = 0.5) gives the best average returns. However, reducing\nthe \u03b1 to 0 eliminates the entropy in sampling and leads to lower returns. On the other hand, \u03b1 = 1.0\nleads to a higher variance in terms of returns of the trajectories.\nDecision Diffuser\nHopper-Medium-Expert\n\u03b1 = 0\n104.3 \u00b1 0.7\n\u03b1 = 0.5\n111.8 \u00b11.6\n\u03b1 = 1.0\n107.1 \u00b1 3.5\nTable A1: Low-temperature sampling (\u03b1 = 0.5) allows us to get high return trajectories consistently.\nWhile \u03b1 = 1.0 leads to a higher variance in returns of the trajectories, \u03b1 = 0.0 eliminates entropy in\nthe sampling and leads to lower returns.\nD\nCOMPOSING CONDITIONING VARIABLES\nIn this section, we detail how Decision Diffuser trained with different conditioning variables\n{yi(\u03c4)}n\ni=1 composes these conditioning variables together.\nIt learns the denoising model\n\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k) for a given conditioning variable yi(\u03c4).\nFrom the derivations outlined\nin prior works (Luo, 2022; Song et al., 2021), we know that \u2207xk(\u03c4) log q(xk(\u03c4)|yi(\u03c4)) \u221d\n\u2212\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k). Therefore, each conditional trajectory distribution {q(xk(\u03c4)|yi(\u03c4))}n\ni=1 can\nbe modelled with a single denoising model \u03f5\u03b8 that conditions on the respective variable yi(\u03c4).\nIn order to compose n different conditioning variables (i.e. skills or constraints), we would like\nto model q(xk(\u03c4)|{yi(\u03c4)}n\ni=1). We assume that {yi(\u03c4)}n\ni=1 are conditionally independent given\n17\nPublished as a conference paper at ICLR 2023\nxk(\u03c4). Thus, we can factorize as follows:\nq(xk(\u03c4)|{yi(\u03c4)}n\ni=1) \u221dq(xk(\u03c4))\nn\nY\ni=1\nq(xk(\u03c4)|yi(\u03c4))\nq(xk(\u03c4))\n(Bayes Rule)\n\u21d2log q(xk(\u03c4)|{yi(\u03c4)}n\ni=1) \u221dlog q(xk(\u03c4)) +\nn\nX\ni=1\n(log q(xk(\u03c4)|yi(\u03c4)) \u2212log q(xk(\u03c4)))\n\u21d2\u2207xk(\u03c4) log q(xk(\u03c4)|{yi(\u03c4)}n\ni=1) = \u2207xk(\u03c4) log q(xk(\u03c4))\n+\nn\nX\ni=1\n(\u2207xk(\u03c4) log q(xk(\u03c4)|yi(\u03c4)) \u2212\u2207xk(\u03c4) log q(xk(\u03c4)))\n\u21d2\u03f5\u03b8(xk(\u03c4), {yi(\u03c4)}n\ni=1, k) = \u03f5\u03b8(xk(\u03c4), \u00d8, k) +\nn\nX\ni=1\n(\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k))\nUsing the above equations, we can sample from q(x0(\u03c4)|{yi(\u03c4)}n\ni=1) with classifier free guidance\nusing the perturbed noise:\n\u02c6\u03f5 := \u03f5\u03b8(xk(\u03c4), \u00d8, k) + \u03c9(\u03f5\u03b8(xk(\u03c4), {yi(\u03c4)}n\ni=1, k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k))\n= \u03f5\u03b8(xk(\u03c4), \u00d8, k) + \u03c9\nn\nX\ni=1\n(\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k))\nWe use the perturbed noise to compose skills or combine constraints at test time. This derivation was\nborrowed from Liu et al. (2022) and is presented here for completeness.\nWhile the composition of conditioning variables {yi(\u03c4)}n\ni=1 requires them to be conditionally\nindependent given the state trajectory x0(\u03c4), we empirically observe that this condition doesn\u2019t have\nto be strictly satisfied. However, we require composition of conditioning variables to be feasible (i.e.\n\u2203x0(\u03c4) that satisfies all the conditioning variables). When the composition is infeasible, Decision\nDiffuser produces trajectories with incoherent behavior, as expected. This is best illustrated by videos\nviewable at https://anuragajay.github.io/decision-diffuser/.\nRequirements on the dataset\nFirst, the dataset should have a diverse set of demonstrations\nthat shows different ways of satisfying each conditioning variable yi(\u03c4).\nThis would allow\nDecision Diffuser to learn diverse ways of satisfying each conditioning variable yi(\u03c4). Since\nwe use inverse dynamics to extract actions from the predicted state trajectory x0(\u03c4), we assume\nthat the state trajectory x0(\u03c4) resulting from the composition of different conditioning variables\ncontains consecutive state pairs (st, st+1) that come from the same distribution that generated the\ndemonstration dataset. Otherwise, inverse dynamics can give erroneous predictions.\nE\nRUNTIME CHARACTERISTIC OF DECISION DIFFUSER\nWe analyze the runtime characteristics of Decision Diffuser in this section. After training the Decision\nDiffuser on trajectories from the D4RL Hopper-Medium-Expert dataset, we plan in the corresponding\nenvironment according to Algorithm 1. Every action taken in the environment requires running 100\nreverse diffusion steps to generate a state sequence taking on average 1.26s in wall-clock time. We\ncan improve the run-time of planning by warm-starting the state diffusion as suggested in Janner et al.\n(2022). Here, we start with a generated state sequence (from the previous environment step), run\nforward diffusion for a fixed number of steps, and finally run the same number of reverse diffusion\nsteps from the partially noised state sequence to generate another state sequence. Warm-starting in\nthis way allows us to decrease the number of denoising steps to 40 (0.48s on average) without any\nloss in performance, to 20 (0.21s on average) with minimal loss in performance, and to 5 with less\nthan 20% loss in performance (0.06s on average). We demonstrate the trade-off between performance,\nmeasured by normalized average return achieved in the environment, and planning time, measured in\nwall-clock time after warm-starting the reverse diffusion process, in Figure A2.\n18\nPublished as a conference paper at ICLR 2023\nFigure A2: Performance vs planning time. We visualize the trade-off between performance, measured by\nnormalized average return achieved in the environment, and planning time, measured in wall-clock time after\nwarm-starting the reverse diffusion process.\nFigure A3:\nBlock push\nenvironment.\nEnvironment\nBC\nCondDiffuser\nDecision Diffuser\nPosition Control\n57.3 \u00b11.2\n87.3 \u00b13.1\n87.8 \u00b12.8\nTorque Control\n55.2 \u00b11.5\n71.8 \u00b13.4\n84.7 \u00b12.2\nTable A2: Block pushing with different controls. Decision Diffuser\nand CondDiffuser perform similarly when the agent uses position control.\nHowever, when the agent uses torque control, CondDiffuser performs\nworse than Decision Diffuser given it\u2019s harder to diffuse over non-smooth\naction trajectories. We use the success rate of the red cube reaching the\ngreen circle as the performance metric. We report the mean success rate\nand the standard error over 5 random seeds.\nF\nWHEN TO USE INVERSE DYNAMICS?\nIn this section, we try to analyze further when using inverse dynamics is better than diffusing over\nactions. Table 2 showed that Decision Diffuser outperformed CondDiffuser on 3 hopper environment,\nthereby suggesting that inverse dynamics is a better alternative to diffusing over actions. Our intuition\nwas that sequences over actions, represented as joint torques in our environments, tend to be more\nhigh-frequency and less smooth, thus making it harder for the diffusion model to predict (Kingma\net al., 2021). We now try to verify this intuition empirically.\nSetup We choose Block Push environment adapted from Gupta et al. (2018) where the goal is to\npush the red cube to the green circle. When the red cube reaches the green circle, the agent gets a\nreward of +1. The state space is 10-dimensional consisting of joint angles (3) and velocities (3) of\nthe gripper, COM of the gripper (2) and position of the red cube (2). The green circle\u2019s position is\nfixed and at an initial distance of 0.5 from COM of the gripper. The red cube (of size 0.03) is initially\nat a distance of 0.1 from COM of the gripper and at an angle \u03b8 sampled from U(\u2212\u03c0/4, \u03c0/4) at the\nstart of every episode. The task horizon is 60 timesteps.\nThere are 2 control types: (i) torque control, where the agent needs to specify joint torques (3\ndimensional) and (ii) position control where the agent needs to specify the position change of COM\nof the gripper and the angular change in gripper\u2019s orientation (\u2206x, \u2206y, \u2206\u03d5) (3 dimensional). While\naction trajectories from position control are smooth, the action trajectories from torque control have\nhigher frequency components.\nOffline dataset collection To collect the offline data, we use Soft Actor-Critic (SAC) (Haarnoja\net al., 2018) first to train an expert policy for 1 million environment steps. We then use 1 million\nenvironment transitions as our offline dataset, which contains expert trajectories collected towards\nthe end of the training and random action trajectories collected at the beginning of the training. We\ncollect 2 datasets, one for each control type.\n19\nPublished as a conference paper at ICLR 2023\nResults Table A2 shows that Decision Diffuser and CondDiffuser perform similarly when the\nagent uses position control. This is because action trajectories resulting from position control are\nsmoother and hence easier to model with diffusion. However, when the agent uses torque control,\nCondDiffuser performs worse than Decision Diffuser, given the action trajectories have higher\nfrequency components and hence are harder to model with diffusion.\nG\nROBUSTNESS TO STOCHASTIC DYNAMICS\np\nBC\nDecision Diffuser\nDiffuser\nCQL\n0.00\n55.2\u00b11.5\n84.7\u00b12.2\n72.4\u00b11.4\n73.2\u00b12.3\n0.05\n49.3\u00b13.6\n77.3\u00b13.1\n63.2\u00b12.9\n61.8\u00b13.7\n0.10\n25.8\u00b13.8\n53.2\u00b14.1\n52.3\u00b14.6\n51.2\u00b14.3\n0.15\n15.1\u00b14.3\n41.3\u00b14.9\n41.6\u00b15.1\n42.2\u00b15.5\nTable A3: Robustness to stochastic dynamics. Decision Diffuser\u2019s performance suffers when stochasticity is\nintroduced in dynamics function. While it still outperforms Diffuser and CQL when p = 0.05, its performance\nbecomes similar to that of Diffuser and CQL for higher p values. We use the success rate of the red cube reaching\nthe green circle as the performance metric. We report the mean success rate and the standard error over 5 random\nseeds.\nWe empirically analyze robustness of Decision Diffuser to stochasticity in dynamics function.\nSetup\nWe use Block Push environment, described in Appendix F, with torque control. However,\nwe inject stochasticity into the environment dynamics. For every environment step, we either sample\na random action from U([\u22121, \u22121, \u22121], [1, 1, 1]) with probability p or execute the action given by the\npolicy with probability (1 \u2212p). We use p \u2208{0, 0.05, 0.1, 0.15} in our experiments.\nOffline dataset collection\nWe collect separate offline datasets for different block push\nenvironments, each characterized by a different value of p. Each offline dataset consists of 1\nmillion environment transitions collected using the method described in Appendix F.\nResults\nTable A3 characterizes how the performance of BC, Decision Diffuser, Diffuser, and CQL\nchanges with increasing stochasticity in the environment dynamics. We observe that the Decision\nDiffuser outperforms Diffuser and CQL for p = 0.05, however all methods including the Decision\nDiffuser settle to a similar performance for larger values of p.\nSeveral works (Paster et al., 2022; Yang et al., 2022) have shown that the performance of return-\nconditioned policies suffers as the stochasticity in environment dynamics increases. This is because\nthe return-conditioned policies aren\u2019t able to distinguish between high returns from good actions\nand high returns from environment stochasticity. Hence, these return-conditioned policies can learn\nsub-optimal actions that got associated with high-return trajectories in the dataset due to environment\nstochasticity. Given Decision diffuser uses return conditioning to generate actions in offline RL, its\nperformance also suffers when stochasticity in environment dynamics increases.\nSome recent works (Yang et al., 2022; Villaflor et al., 2022) address the above issue by learning a\nlatent model for future states and then conditioning the policy on predicted latent future states rather\nthan returns. Conditioning Decision Diffuser on future state information, rather than returns, would\nmake it more robust to stochastic dynamics and could be an interesting avenue for future works.\nH\nKUKA BLOCK STACKING\nIn the Kuka blocking stacking environment, the underlying goal is to stack a set of blocks on top\nof each other. Models have trained on a set of demonstration data, where a set of 4 blocks are\nsequentially stacked on top of each other to form a block tower.\nWe construct state-space plans of length 128. Following (Janner et al., 2022), we utilize a close-loop\ncontroller to generate actions for each state in our state-space plan (controlling the 7 degrees of\nfreedom in joints). The total maximum trajectory length plan in Kuka block stacking is 384. We\ndetail differences between the two consider conditional stacking environments below:\n20\nPublished as a conference paper at ICLR 2023\n\u2022 Stacking In the stacking environment, at test time we wish to again construct a tower of four\nblocks.\n\u2022 Rearrangement In the rearrangement environment, at test time wish to stack blocks in a\nconfiguration where a set of blocks are above a second set. This set of stack-place relations\nmay not precisely correspond to a single block tower (can instead construct two block towers),\nmaking this environment an out-of-distribution challenge.\nIn addition to Diffuser (Janner et al., 2022), we used goal-conditioned variants of CQL (Kumar et al.,\n2020) and BCQ (Fujimoto et al., 2019) as baselines for the block stacking and rearrangement with\nsingle constraint. However, they get a success rate of 0.0.\nI\nUNITREE GO RUNNING\nWe consider Unitree-go-running environment (Margolis & Agrawal, 2022) where a quadruped robot\nruns in 3 different gaits: bounding, pacing, and trotting. The state space is 56 dimensional, the action\nspace is 12 dimensional, and the maximum trajectory length is 250.\nAs described in Section 4.3, we train Decision Diffuser on expert trajectories demonstrating individual\ngaits. During testing, we compose the noise model of our reverse diffusion process according to\nequation 9. This allows us to sample trajectories of the quadruped robot with entirely new running\nbehavior. Figures A4,A5,A6 shows the ability of Decision Diffuser to imitate bounding, trotting and\npacing and their combinations.\nI.1\nQUANTITATIVE VERIFICATION OF COMPOSITION\nWe now try to quantitatively verify whether the trajectories resulting from composition of 2 gaits\ndoes indeed contain only those 2 gaits.\nSetup\nWe learn a gait classifier that takes in a sub-sequence of states (of length 10) and predicts\nthe gait-ID. It is represented by a 3-layered MLP with 1024 hidden units and ReLU activations that\nconcatenates the sub-sequence of states (of length 10) into a single vector of dimension 560 before\ntaking it in as an input. We train the gait classifier on the demonstration dataset. To ensure that\nthe learned classifier can predict gait-ID on trajectories generated by the composition of skills, we\nuse MixUp-style (Zhang et al., 2017) data augmentation during training. We create a synthetic sub-\nsequence of length 10 by concatenating two sampled sub-sequence (from the demonstration dataset)\nof length li and lj (where li +lj = 10) from gaits with ID i and j and give it a label\nli\nli+lj one-hot(i)+\nlj\nli+lj one-hot(j). During training, we sample a sub-sequence from the demonstration dataset with\n70% probability and a sythenthic sub-sequence with 30% probability. We train the classifier for 2e6\ntrain steps with a learning rate of 2e \u22124 and a batch size of 64.\nResults\nFigures A4,A5,A6 show that the classifier\u2019s prediction is consistent with the visualized\ncomposed trajectories. Furthermore, we use Decision diffuser to act in the environment and generate\n1000 trott trajectories, 1000 pace trajectories, 1000 bound trajectories, and 1000 composed trajectories\nfor each possible pair of individual gaits. We then evaluate the learned gait classifier on these\ntrajectories and compute the percentage of timesteps a particular gait has the highest probability.\nFrom Figures A4,A5,A6, we can see that if trajectories are generated by the composition of two\ngaits, then those two gaits will have the two highest probabilities across different timesteps in those\ntrajectories.\nI.2\nA SIMPLE BASELINE FOR COMPOSITION\nLet one-hot(i) and one-hot(j) represent two different gaits that can be generated using noise models\n\u03f5\u03b8(xk(\u03c4), one-hot(i), k) and \u03f5\u03b8(xk(\u03c4), one-hot(j), k) respectively. To compose these gaits, we\ncompose the above-mentioned noise models using equation 9. As an alternative, we see if the noise\nmodel \u03f5\u03b8(xk(\u03c4), one-hot(i) + one-hot(j), k) can lead to composed gaits. However, we observe\nthat \u03f5\u03b8(xk(\u03c4), one-hot(i) + one-hot(j), k) catastrophically fail to generate any gait (see videos\nat https://anuragajay.github.io/decision-diffuser/). This happens because the condition variable\none-hot(i) + one-hot(j) was never seen by the noise model \u03f5\u03b8 during training.\n21\nPublished as a conference paper at ICLR 2023\nTrott\nPace\nTrott + Pace\nTrott\nGait Probability\nOnly Trott\nOnly Pace\nTrott + Pace\n50\n100\n150\n200\n50\n100\n150\n200\n50\n100\n150\n200\n.20\n.40\n.60\n.80\nPace\nBound\nCondition\nTrott\nPace\nBound\nOnly Trott\n97.2\n1.7\n1.1\nOnly Pace\n0.8\n98.1\n1.1\nTrott + Pace\n55.6\n43.6\n0.8\nFigure A4: Composing Trott and Pace. Decision Diffuser can imitate individual running gaits using expert\ndemonstrations and compose multiple different skills together during test time. The results are best illustrated by\nvideos viewable at https://anuragajay.github.io/decision-diffuser/.\nTrott\nBound\nTrott + Bound\nTrott\nGait Probability\nOnly Trott\nOnly Bound\nTrott + Bound\n50\n100\n150\n200\n50\n100\n150\n200\n50\n100\n150\n200\n.20\n.40\n.60\n.80\nPace\nBound\nCondition\nTrott\nPace\nBound\nOnly Trott\n96.4\n2.2\n1.4\nOnly Bound\n1.6\n0.6\n97.8\nTrott + Bound\n51.3\n0.9\n47.8\nFigure A5: Composing Trott and Bound. Decision Diffuser can imitate individual running gaits using expert\ndemonstrations and compose multiple different skills together during test time. The results are best illustrated by\nvideos viewable at https://anuragajay.github.io/decision-diffuser/.\nJ\nNOT COMPOSITIONS WITH DECISION DIFFUSER\nDecision diffuser can also support \u201dNOT\u201d composition. Suppose we wanted to sample from\nq(x0(\u03c4)|NOT yj(\u03c4)). Let {yi(\u03c4)}n\ni=1 be the set of all conditioning variables. Then, following\nderivations from Liu et al. (2022) and using \u03b2 = 1, we can sample from q(x0(\u03c4)|NOT yj(\u03c4)) using\nthe perturbed noise:\n\u02c6\u03f5 := \u03f5\u03b8(xk(\u03c4), \u00d8, k) + \u03c9(\nX\ni\u0338=j\n(\u03f5\u03b8(xk(\u03c4), yi(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k))\n\u2212(\u03f5\u03b8(xk(\u03c4), yj(\u03c4), k) \u2212\u03f5\u03b8(xk(\u03c4), \u00d8, k)))\nWe demonstrate the ability of Decision Diffuser to support \u201dNOT\u201d composition by using it to satisfy\nconstraint of type BlockHeight(i) > BlockHeight(j) AND (NOT BlockHeight(j) >\nBlockHeight(i))\nin\nKuka\nblock\nstacking\ntask,\nas\nvisualized\nin\nvideos\nat\nhttps://anuragajay.github.io/decision-diffuser/.\nAs the Decision Diffuser does not provide\nan explicit density estimate for each skill, it can\u2019t natively support OR composition.\n22\nPublished as a conference paper at ICLR 2023\nBound\nPace\nBound + Pace\n \nTrott\nGait Probability\nOnly Bound\nOnly Pace\nBound + Pace\n50\n100\n150\n200\n50\n100\n150\n200\n50\n100\n150\n200\n.20\n.40\n.60\n.80\nPace\nBound\nCondition\nTrott\nPace\nBound\nOnly Bound\n0.8\n1.0\n98.2\nOnly Pace\n1.4\n97.7\n0.9\nBound + Pace\n1.4\n38.5\n60.1\nFigure A6: Composing Bound and Pace. Decision Diffuser can imitate individual running gaits using expert\ndemonstrations and compose multiple different skills together during test time. The results are best illustrated by\nvideos viewable at https://anuragajay.github.io/decision-diffuser/.\nK\nCOMPARING Q-FUNCTION GUIDED DIFFUSION AND CLASSIFIER-FREE\nGUIDED DIFFUSION\nClassifier-free guided diffusion and Q-value guided diffusion are theoretically equivalent. However, as\nnoted in several works (Nichol et al., 2021; Ho & Salimans, 2022; Saharia et al., 2022), classifier-free\nguidance performs better than classifier guidance (i.e. Q function guidance in our case) in practice.\nThis is due to following reasons:\n\u2022 Classifier-guided diffusion models learns an unconditional diffusion model along with\na classifier (Q-function in our case) and uses gradients from the classifier to perform\nconditional sampling. However, the unconditional diffusion model doesn\u2019t need to focus on\nconditional modeling during training and only cares about conditional generation during\ntesting after it has been trained. In contrast, classifier-free guidance relies on conditional\ndiffusion model to estimate gradients of the implicit classifier. Since the conditional diffusion\nmodel, learned when using classifier-free guidance, focuses on conditional modeling during\ntrain time, it performs better in conditional generation during test time.\n\u2022 Q function trained on an offline dataset can erroneously predict high Q values for out-of-\ndistribution actions given any state. This problem has been extensively studied in offline\nRL literature (Kumar et al., 2020; Fujimoto et al., 2019; Levine et al., 2020). In online RL,\nthis issue is automatically corrected when the policy acts in the environment, thinking an\naction to be good but then receives a low reward for it. In offline RL, this issue can\u2019t be\ncorrected easily; hence, the learned Q-function can often guide the diffusion model towards\nout-of-distribution actions that might be sub-optimal. In contrast, classifier-free guidance\ncircumvents the issue of learning a Q-function and directly conditions the diffusion model on\nreturns. Hence, classifier-free guidance doesn\u2019t suffer due to errors in learned Q-functions\nand hence performs better than Q-function guided diffusion.\nL\nCOMPARING DECISION TRANSFORMER AND DECISION DIFFUSER\nDecision Transformer models the likelihood of the next action given target return and sequence of past\nstates, actions, and rewards. In contrast, Decision Diffuser models the score-function of future state\ntrajectory given target return and past state trajectory. While both these methods learn a conditional\nprobabilistic model of trajectories, Decision Diffuser allows for composition of conditioning variables\nby composing their respective score functions (see Appendix D for details). In theory, being a\nlikelihood model, Decision Transformer can also model score function of trajectories. However, this\nmight be computationally inefficient (Dieleman, 2022) and thus impractical.\n23\nPublished as a conference paper at ICLR 2023\nM\nLIMITATIONS OF DECISION DIFFUSER\nWe summarize the limitations of Decision Diffuser:\n\u2022 No partial observability Decision Diffuser works with fully observable MDPs. Naive\nextensions to partially observed MDPs (POMDPs) may cause self-delusions (Ortega et al.,\n2021) in Decision Diffuser. Hence, extending Decision Diffuser to POMDPs could be an\nexciting avenue for future work.\n\u2022 Inability to explore the environment and update itself in online setting In this work, we\nfocused on offline sequential decision making, thus circumventing the need for exploration.\nUsing ideas from Zheng et al. (2022), future works could look into online fine-tuning of\nDecision Diffuser by leveraging entropy of the state-sequence model for exploration.\n\u2022 Experiments on only state-based environments While our work focused on state based\nenvironments, it can be extended to image based environments by performing the diffusion\nin latent space, rather than observation space, as done in Rombach et al. (2022).\n\u2022 Only AND and NOT compositions are supported Since Decision Diffuser does not\nprovide an explicit density estimate for each condition variable, it can\u2019t natively support OR\ncomposition.\n\u2022 Performance degradation in environments with stochastic dynamics In environments\nwith highly stochastic dynamics, Decision Diffuser loses its advantage and performs similarly\nto Diffuser and CQL. To tackle environments with stochastic dynamics, recent works (Yang\net al., 2022; Villaflor et al., 2022) propose learning a latent model for future states and then\nconditioning the policy on predicted latent future states rather than returns. Conditioning\nDecision Diffuser on future state information, rather than returns, would make it more robust\nto stochastic dynamics and could be an interesting avenue for future works.\n\u2022 Performance in limited data regime Since diffusion models are prone to overfitting in case\nof limited data, Decision Diffuser is also prone to overfitting in limited data regime.\n24\n",
    "2309.17080": "GAIA-1:\nA Generative World Model for Autonomous Driving\nAnthony Hu*\nLloyd Russell*\nHudson Yeo*\nZak Murez\nGeorge Fedoseev\nAlex Kendall\nJamie Shotton\nGianluca Corrado\nWayve\nresearch@wayve.ai\n* equal contributions\nAbstract\nAutonomous driving promises transformative improvements to transportation, but\nbuilding systems capable of safely navigating the unstructured complexity of\nreal-world scenarios remains challenging. A critical problem lies in effectively\npredicting the various potential outcomes that may emerge in response to the\nvehicle\u2019s actions as the world evolves.\nTo address this challenge, we introduce GAIA-1 (\u2018Generative AI for Autonomy\u2019),\na generative world model that leverages video, text, and action inputs to generate\nrealistic driving scenarios while offering fine-grained control over ego-vehicle\nbehavior and scene features. Our approach casts world modeling as an unsuper-\nvised sequence modeling problem by mapping the inputs to discrete tokens, and\npredicting the next token in the sequence. Emerging properties from our model\ninclude learning high-level structures and scene dynamics, contextual awareness,\ngeneralization, and understanding of geometry. The power of GAIA-1\u2019s learned\nrepresentation that captures expectations of future events, combined with its ability\nto generate realistic samples, provides new possibilities for innovation in the field\nof autonomy, enabling enhanced and accelerated training of autonomous driving\ntechnology.\n1\nIntroduction\nPredicting future events is a fundamental and critical aspect of autonomous systems. Accurate future\nprediction enables autonomous vehicles to anticipate and plan their actions, enhancing safety and\nefficiency on the road. To achieve this, the development of a robust model of the world is imperative\n[1] and huge efforts have been made in the past to build such predictive world models for autonomous\ndriving [2, 3, 4, 5, 6]. A world model [7, 8] learns a structured representation and understanding of\nthe environment that can be leveraged for making informed decisions when driving.\nHowever, current approaches have had significant limitations. World models have been successfully\napplied to control tasks in both simulation [9, 10, 11, 12, 13] and to real-world robotics tasks [14, 15].\nThese methods often rely on labeled data, which is challenging to obtain at scale, and models that\nwork on simulated data may not fully capture the complexities of real-world scenarios. Furthermore,\ndue to their low-dimensional representations, these models may struggle to generate highly realistic\nsamples of future events, posing challenges in achieving a high level of fidelity in predictions for\ncomplex real-world applications such as autonomous driving.\narXiv:2309.17080v1  [cs.CV]  29 Sep 2023\nVideo rollout\nText-conditioned\nrollout\nAction-conditioned\nrollout\nText-conditioned\ngeneration\nText and action\nconditioned\ngeneration\nCONTEXT\nCONDITIONING\nGENERATED FRAMES\nText:\n\u201cThe traffic light \nis green\u201d\nAction:\nSpeed: --\nCurvature: LEFT\nText:\n\u201cIt is night\u201d\nMODE\nText:\n\u201cIt is snowing\u201d\nText:\n\u201cWe are 15 meters\nbehind a bus\u201d\nAction:\nSpeed: ACCELERATE\nCurvature: RIGHT\nUnconditional\ngeneration\nFigure 1: GAIA-1 multimodal video generation. GAIA-1 can generate videos by performing future\nrollouts starting from a video prompt. These future rollouts can be further conditioned on actions\nto influence particular behaviors of the ego-vehicle (e.g. steer left), or on text to drive a change in\nsome aspects of the scene (change the color of the traffic light). For speed and curvature we condition\nthe model by passing the sequence of future speed and / or curvature values. Our model can also\ngenerate realistic videos from text prompts, or by simply drawing samples from its prior distribution\n(fully unconditional generation).\nMeanwhile, progress in generative image and video generation has harnessed the power of self-\nsupervised learning to learn from large quantities of real-world data to generate remarkably realistic\nvideo samples [16, 17, 18]. Yet, a significant challenge persists in this domain: the difficulty of\nlearning a representation that captures the expected future events. While such generative models\nexcel at generating visually convincing content, they may fall short in learning representations of the\nevolving world dynamics that are crucial for precise future predictions and robust decision-making in\ncomplex scenarios.\nIn this work we introduce GAIA-1, a method designed with the goal of maintaining the benefits\nof both world models and generative video generation. It combines the scalability and realism of\ngenerative video models with the ability of world models to learn meaningful representations of\nthe evolution into the future. GAIA-1 works as follows. First, we partition the model into two\ncomponents: the world model and the video diffusion decoder. The world model reasons about the\nscene\u2019s high-level components and dynamics, while the diffusion model takes on the responsibility of\ntranslating latent representations back into high-quality videos with realistic detail.\n2\nworld model\nautoregressive\nprediction\nvideo\ndecoder\noutput\ntokens\n...\n...\ninput\ntokens\noutput\nvideo\n1\n1\n2\nn\n1\n2\nn\n2\n2\n1\n1\n2\nn\ntext\nencoder\naction\nencoder\nimage\nencoder\n\u201cI am approaching a\ncrossing yielding\nto pedestrians\u201d\n...\n\u201cIt is safe to move\nso I am now\naccelerating\u201d\n1\ninput\nvideo\n2\nFigure 2: Architecture of GAIA-1. First, we encode information from all input modalities (video, text,\naction) into a common representation: images, text and actions are encoded as a sequence of tokens.\nThe world model is an autoregressive transformer that predicts the next image token conditioned on\npast image, text, and action tokens. Finally, the video decoder maps the predicted image tokens back\nto the pixel space, at a higher temporal resolution.\nFor the world model, we use vector-quantized representations of video frames to discretize each\nframe, transforming them into a sequence of tokens. Subsequently, we reframe the challenge of\npredicting the future into predicting the next token in the sequence [10, 19]. This approach has been\nwidely employed in recent years to train large language models [20, 21, 22, 23], and it is recognized\nfor its effectiveness in enhancing model performance through the scaling of model size and data. It\nis possible to generate samples within the latent space of the world model through autoregressive\ngeneration.\nThe second component is a multi-task video diffusion decoder that is able to perform high-resolution\nvideo rendering as well as temporal upsampling to generate smooth videos from the information\nautoregressively generated by the world model. Similarly to large language models, video diffusion\nmodels have demonstrated a clear correlation between scale of training and overall performance,\nmaking both components of GAIA-1 suitable for effective compound scaling.\nGAIA-1 is designed to be multimodal, allowing video, text and action to be used as prompts to\ngenerate diverse and realistic driving scenarios, as demonstrated in Figure 1. By training it on a\nlarge corpus of real-world UK urban driving data, GAIA-1 learns to understand and disentangle\nimportant concepts such as static and dynamic elements, including cars, buses, pedestrians, cyclists,\nroad layouts, buildings, and even traffic lights. Further, it provides fine-grained control over both\nego-vehicle behavior and other scene features through action and language conditioning.\nGAIA-1 demonstrates the ability to manifest the generative rules of the real world. Emerging\nproperties such as learning high-level structures, generalization, creativity, and contextual awareness\nindicate that the model can comprehend and reproduce the rules and behaviors of the world. Moreover,\nGAIA-1 exhibits understanding of 3D geometry, for example, by effectively capturing the intricate\ninterplay of pitch and roll induced by road irregularities such as speed bumps. It showcases reactive\nbehaviors of other agents demonstrating the ability to understand causality in decision making of\nroad users. Surprisingly, it shows the capability to successfully extrapolate beyond the training data,\nfor example to driving outside of the boundaries of the road. See Section 7 for a comprehensive list\nof examples.\nThe power of GAIA-1\u2019s learned representations to predict future events, paired with control over both\nego-vehicle dynamics and scene elements, is an exciting advance that paves the way for improving\nembodied intelligence and providing synthetic data to accelerate training and validation. World\nmodels, such as GAIA-1, are the basis for the ability to predict what might happen next, which is\nfundamentally important for decision-making in autonomous driving.\n3\n2\nModel\nIn this section we describe the model architecture of the trainable components of GAIA-1. The\ngeneral architecture is presented in Figure 2.\n2.1\nEncoding Video, Text and Action\nGAIA-1 can leverage three different input modalities (video, text, action), which are encoded into a\nshared d-dimensional space.\nImage tokens.\nEach image frame of a video is represented as discrete tokens. To achieve this, we\nuse a pre-trained image tokenizer for discretization (for details about the pre-training see Section 2.2).\nFormally, let us consider a sequence of T images (x1, . . . , xT ), where each image xt in this sequence\nis discretized into n = 576 discrete tokens using the pre-trained image tokenizer. We obtain a\nsequence denoted as (z1, . . . , zT ), where each zt = (zt,1, . . . , zt,n) \u2208Rn corresponds to n = H\nD \u00d7 W\nD\ndiscrete tokens. Here, H and W represent the height and width of the input image, while D denotes\nthe downsampling factor of the image tokenizer. These discrete tokens are then mapped to a d-\ndimensional space via an embedding layer that is trained alongside the world model.\nText tokens.\nAt each time step t, we incorporate information from both text and action. Textual\ninput is encoded using the pre-trained T5-large model [24], resulting in m = 32 text tokens per\ntime step. These tokens are mapped to a d-dimensional space through a linear layer that is trained\nin conjunction with the world model. This process yields a text representation denoted as ct =\n(ct,1, . . . , ct,m) \u2208Rm\u00d7d.\nAction tokens.\nFor actions, we consider l = 2 scalar values (representing speed and curvature).\nEach scalar is independently mapped to the d-dimensional space via a linear layer that is trained with\nthe world model. Consequently, the action at time step t is represented as at = (at,1, . . . , at,l) \u2208\nRl\u00d7d.\nFor each time step, the input tokens are interleaved in the following order: text - image - action. The\nfinal input of the world model is therefore (c1, z1, a1, . . . , cT , zT , aT ). To encode the position of the\ninput tokens, we use a factorized spatio-temporal positional embedding. 1) A learnable temporal\nembedding is shared across all the tokens of a given time step, i.e. there are T temporal embeddings.\n2) A learnable spatial embedding indicates the position of a token within a time step, i.e. there\nare m + n + l = 610 spatial embeddings (m text tokens, n image tokens, and l action tokens) of\ndimension d = 4096.\n2.2\nImage Tokenizer\nWhen modeling discrete input data with a sequence model, there is a trade-off between the sequence\nlength and the vocabulary size. The sequence length refers to the number of discrete tokens that\nare needed to describe the data. The vocabulary size corresponds to the number of possible values\na single token can take. For language, there are two obvious choices for tokens: characters and\nwords. When using character-level tokens, the input data has a longer sequence length, and each\nindividual token belongs to a smaller vocabulary, but conveys little meaning. When using word-level\ntokens, the input data has a shorter sequence length, and each token contains a lot of semantics but the\nvocabulary is extremely large. Most language models [25, 26, 24, 21, 27, 22] use byte-pair encoding\n(or equivalent) as a trade-off between character-level and word-level tokenization.\nLikewise for video, we would like to reduce the sequence length of the input, while possibly making\nthe vocabulary larger, but with tokens that are more semantically meaningful than raw pixels. We do\nthis with a discrete image autoencoder [28]. There are two objectives we would like to achieve in this\nfirst stage:\n1. Compress the information from raw pixels to make the sequence modeling problem tractable.\nImages contain a lot of redundant and noisy information. We would like to reduce the\nsequence length needed to describe the input data.\n2. Guide the compression towards meaningful representations, such as semantics, instead of\nhigh-frequency signals. The resulting input space for the world model will be simpler to\n4\n(a) Input image\n(b) Base VQ-GAN tokens\n(c) DINO-distilled tokens\nFigure 3: Increasing semantic content of image tokens through DINO distillation. Visualization\nshows the top 3 PCA components of token embeddings mapped to RGB values. DINO-distilled\ntokens corresponding to a semantic class (e.g. vehicle, road, or sky) have similar embeddings.\ncompose with, and less dominated by high-frequency signals that can considerably slow\ndown the learning process.\nWe reduce the sequence length of the input data by downsampling each input image by a factor\nD = 16 in both height and width. Each image xt of size H \u00d7 W is described by n = H\nD \u00d7 W\nD\ntokens with a vocabulary size K. Inspired by [29], we guide the compression towards meaningful\nrepresentations by regressing to the latent features of a pre-trained DINO model [30], a self-supervised\nimage model that is known to contain semantic information. See Figure 3 for a qualitative example.\nThe discrete autoencoder is a fully convolutional 2D U-Net [31]. The encoder E\u03b8 quantizes the image\nfeatures using nearest neighbor look-up from a learnable embedding table [28], resulting in image\ntokens zt = E\u03b8(xt). Note that the decoder is only used to train the image autoencoder, solely the\ndiscrete encoder E\u03b8 is part of the final GAIA-1 model. Due to the decoder being trained on single\nimages it lacks temporal consistency when decoding to a video. For this reason we also train a video\ndecoder that is described in Section 2.4.\nThe training losses for the image autoencoder are the following:\n\u2022 Image reconstruction loss. The image reconstruction loss is a weighted sum of L1, L2,\nperceptual loss Lperceptual [32], and GAN loss LGAN [33].\n\u2022 Quantization loss. To update the embedding vectors, we use the embedding loss and the\ncommitment loss from [28]. We adopted the linear projection of the embedding and L2\nnormalization from [34] as we found this helped increase vocabulary usage.\n\u2022 Inductive bias loss. The quantized image features are encouraged to match the image\nfeatures of a pre-trained DINO [30] model with a cosine similarity loss. Distilling the\ninformation from DINO into the learned tokens is important as it allows them to benefit\nfrom the inductive biases of this model.\n2.3\nWorld Model\nAs described in Section 2.1 the input of the world model is (c1, z1, a1, ..., cT , zT , aT ). The world\nmodel is an autoregressive transformer network that models the sequence input. Its training objective\nis to predict the next image token in the sequence conditioned on all past tokens, using causal masking\nin the attention matrix of the transformer blocks [35].\nLworld model = \u2212\nT\nX\nt=1\nn\nX\ni=1\nlog p(zt,i|z<t, zt,j<i, c\u2264t, a<t)\n(1)\nWe randomly dropout conditioning tokens during training so that the world model can do (i) uncondi-\ntional generation, (ii) action-conditioned generation, and (iii) text-conditioned generation.\nTo further reduce the sequence length of our world model we temporally subsample videos from\n25Hz to 6.25Hz. This allows the world model to reason over longer periods without leading to\nintractable sequence lengths. To recover video predictions at full frame rate we perform temporal\nsuper-resolution using the video decoder described in Section 2.4.\n5\n2.4\nVideo Decoder\nFollowing the recent advances in image [36, 37] and video generation [16, 18] we use denoising\nvideo diffusion models for the GAIA-1 decoder. A naive approach of independently decoding each\nframe-tokens to pixel space results in a temporally inconsistent video output. Modeling the problem as\ndenoising a sequence of frames during the diffusion process, where the model can access information\nacross time, greatly improves temporal consistency of the output video.\nWe follow [38] and use a 3D U-Net with factorized spatial and temporal attention layers. During\ntraining, our video diffusion model is conditioned on the image tokens obtained by discretizing input\nimages with the pre-trained image tokenizer E\u03b8. During inference, the diffusion model is conditioned\non the predicted image tokens from the world model.\nWe train a single model jointly on both image and video generation tasks. Training on videos\nteaches the decoder to be temporally consistent, while training on images is crucial for the quality of\nindividual frames [16] as it teaches the model to extract information from conditioning image tokens.\nWe disable temporal layers when training on images.\nTo train our video diffusion decoder for multiple inference tasks we take inspiration from [17] where\nwe can perform multiple tasks by masking certain frames or the conditioning image tokens. We\nchoose to train a single video diffusion model for all tasks as it has been shown that multi-task training\nimproves performance on individual tasks [17]. The tasks include image generation, video generation,\nautoregressive decoding, and video interpolation. Each task is sampled equally. For example, for\nthe autoregressive generation task, we provide previously generated past frames as context and\nconditioning image tokens for frames we want to predict. We include both forward and backward\nautoregressive tasks. See Figure 4 for examples of each task. We also apply a conditioning dropout\nby randomly masking out each conditioning image token with probability p = 0.15 as it helps the\nmodel generalize beyond relying on tokens for information and improves temporal consistency.\nThe video decoder is trained on the noise prediction objective. More specifically, we use the\nv-parameterization as proposed in [39] because it avoided unnatural color shifts and maintained\nlong-term consistency as similarly found in [16]. In practice, we use a weighted average of L1 and\nL2 losses. The video decoder loss Lvideo is:\nLvideo = E\u03f5,t\u2032\nh\n\u2225\u03f5\u03b8(xt\u2032, t\u2032, z, m) \u2212\u03f5\u22252\n2\ni\n(2)\nwhere:\n\u2022 \u03f5\u03b8 is the denoising video model.\n\u2022 \u03f5 is the denoising target, which uses the v-parameterization.\n\u2022 t\u2032 \u223cU(0, 1) is the sampled discrete diffusion time.\n\u2022 x = (x1, ..., xT \u2032) is a video sequence of length T \u2032.\n\u2022 xt\u2032 = \u03b1t\u2032x + \u03c3t\u2032\u03f5 represents the noised video, with \u03b1t\u2032 and \u03c3t\u2032 functions of t\u2032 that define\nthe noise schedule.\n\u2022 z = (z1, ..., zT \u2032) = E\u03b8(x) is the sequence of conditioning image tokens.\n\u2022 m = (m1, ..., mT \u2032) is a sequence of image masks as specified by the training task (see\nFigure 4).\n3\nData\nOur training dataset consists of 4,700 hours at 25Hz of proprietary driving data collected in London,\nUK between 2019 and 2023. This corresponds to approximately 420M unique images. During\ntraining we balance over a customizable set of features to control the distribution of data (Figure 5).\nWe achieve this by sampling individual data points with weighting inversely proportional to the\n(binned and precomputed) empirical distribution of a given feature. For a given example we take the\njoint probability across all features to balance and stochastically decide whether to include or discard\nthat example. We can control the strength of balancing by raising the sampling weight to an exponent,\nwhere an exponent of 0 would result in the empirical distribution (no balancing) and an exponent of 1\n6\ntokens\ntokens\ntokens\n+\n+\n+\nnoise\nnoise\nnoise\ndecoded\nframe\ndecoded\nframe\ndecoded\nframe\n(a) Image generation\n+\n+\n+\ntokens\ntokens\ntokens\nnoise\nnoise\nnoise\ndecoded\nframes\n(b) Video generation\ndecoded\nframes\nprevious\nframe\n+\n+\ntokens\ntokens\nnoise\nnoise\n(c) Autoregressive video generation\ninterpolated\nframe\nframe 1\nframe 2\nnoise\n(d) Video interpolation\nFigure 4: Video decoder training tasks. Each task is defined by masking ground truth images and\ncontext tokens. We pass noise as input for frames we want to predict. Tokens are provided for\npredicted frames except for video interpolation task where the diffusion process is guided solely by\nimage context.\nwould result in a uniformly balanced distribution. We used an exponent of 0.5 for all features as a\ncompromise between final balancing achieved and the severity of discarding samples for training\nefficiency.\nFor the tokenizer we balanced over (latitude, longitude, weather category) to account for geography\nand visually distinct weather conditions ensuring our tokenizer can adequately represent a diverse\nrange of scenes.\nFor the world model and the video diffusion model we balanced over (latitude, longitude, weather\ncategory, steering behavior category, speed behavior category), additionally considering speed and\nsteering behaviors to ensure the dynamics of different behaviors are captured and sufficiently modeled\nby the world model and the temporal decoder.\nOur validation dataset contains 400 hours of driving data from runs not included in the training set.\nThe runs selected for validation are those that pass through predetermined geofences as well as a\nselection of randomly selected runs. We further split our validation set into strict geofences in order\nto analyze only those samples strictly within the validation geofence (i.e., roads never seen during\ntraining) and another geofence around our main data collection routes (i.e., roads seen during training)\nas a way to monitor overfitting and generalization.\n7\n51.50\n51.52\n51.54\n51.56\n51.58\n51.60\nLatitude (deg)\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125\nProportion\nEmpirical\nSampled\n\u22120.25\n\u22120.20\n\u22120.15\n\u22120.10\nLongitude (deg)\n0.00\n0.05\n0.10\n0.15\nProportion\nEmpirical\nSampled\nClear Cloudy Rain\nSnow\nFog\nNight Indoor\nWeather category\n0.0\n0.1\n0.2\n0.3\n0.4\nProportion\nEmpirical\nSampled\nEmpirical\n0.0005\n0.0010\n0.0015\n0.0020\nProportion\nof samples\n0.0025\nSampled\n0.0005\n0.0010\n0.0015\n0.0020\nProportion\nof samples\n0.0025\nLorem ipsum\nValidation\n0.0005\n0.0010\n0.0015\n0.0020\nProportion\nof samples\n0.0025\nFigure 5: Data sampling. The top row shows the empirical distribution and the sampled distribution\nfor three features we selected to balance over during training: latitude, longitude and weather\ncondition. Dashed lines indicate data outside of the range we balanced over. The bottom row shows\nthe geographical heatmap of sampled latitude and longitude coordinates for the whole training set,\nthe sampled training set and the geofenced validation set.\n4\nTraining Procedure\nIn this section, we describe how the three trainable components of GAIA-1 were optimized. We\nprovide details of hyperparameter configurations, hardware used and training times.\n4.1\nImage Tokenizer\nThe image tokenizer (0.3B parameters) was trained on images of resolution H \u00d7 W = 288 \u00d7 512\n(9/16 ratio). The spatial downsampling of the encoder is D = 16, therefore each image is encoded\nas n = 18 \u00d7 32 = 576 discrete tokens with a vocabulary size K = 8192. The bit compression is\n288\u00d7512\u00d73\u00d78\n18\u00d732\u00d713\n\u2248470.\nThe discrete autoencoder was optimised with AdamW [40] and a learning rate of 1 \u00d7 10\u22124, weight\ndecay 0.01, beta coefficients (0.5, 0.9). The loss weights are \u03bbL1 = 0.2, \u03bbL2 = 2.0, \u03bbLperceptual = 0.1,\n\u03bbLGAN = 1.0, \u03bbLcodebook = 1.0, \u03bbLDINO = 0.1.\nThe model was trained for 200k steps in 4 days with a batch size equal to 160, split across 32 A100\n80GB GPUs. We used 5k of linear warm-up and 10k of cosine decay to a final learning rate of\n1 \u00d7 10\u22125.\n4.2\nWorld Model\nThe world model (6.5B parameters) was trained on video sequences of size T = 26 at 6.25 Hz, which\ncorrespond to 4s-long videos. The text was encoded as m = 32 text tokens per time step, and the\naction as l = 2 tokens. The total sequence length of the world model is therefore T \u00d7 (m + n + l) =\n15860.\nThe world model was optimized with AdamW and a learning rate of 1\u00d710\u22124, weight decay 0.1, beta\ncoefficients (0.9, 0.95), norm gradient clipping 1.0. Training examples were either unconditioned,\n8\n0\n128\n256\n384\n512\nToken position\n0\n25\n50\n75\n100\nPerplexity\nReal\nArgmax\n(a) Argmax.\n0\n128\n256\n384\n512\nToken position\n0\n25\n50\n75\n100\nPerplexity\nReal\nSampling\n(b) Sampling.\n0\n128\n256\n384\n512\nToken position\n0\n25\n50\n75\n100\nPerplexity\nReal\nTop-k=50\n(c) Top-k sampling.\nFigure 6: Perplexity of the world model as a function of the position of the generated token. We\nconsider the n = 576 tokens of a single image frame. We compare the perplexity of the tokens from\na real image, to those generated with the following strategies: argmax, sampling, or top-k. In (a),\nby inspecting the perplexity of a real image we notice it oscillates between low and high values,\nmeaning there is a good range of diversity in those tokens. In contrast, if we look at the argmax\nstrategy, we notice the perplexity only takes extremely low values (no diversity, manifesting in the\npredicted frames to repeat themselves). Conversely in (b), if we sample from the entire distribution,\nthe perplexity of some tokens can take extremely high values, due to sampling from the unreliable tail.\nIn (c), we observe that top-k=50 sampling produces tokens that have a similar perplexity distribution\nto real tokens.\naction-conditioned, or text conditioned. The ratios of these respective conditioning modes were\n20%/40%/40%.\nThe model was trained for 100k steps in 15 days, with 2.5k of linear warm-up and 97.5k of cosine\ndecay reducing the learning rate by a factor of 10 over the course of training. The batch size was\n128 split across 64 A100 80GB GPUs. We used the FlashAttention v2 implementation [41] in the\ntransformer module, as it offered significant advantages in terms of both memory utilization and\ninference speed. To optimize distributed training, we used the Deepspeed ZeRO-2 training strategy\n[42] with activation checkpointing.\n4.3\nVideo Decoder\nThe video decoder (2.6B) was trained on sequences of T \u2032 = 7 images of resolution H \u00d7 W =\n288 \u00d7 512 sampled from the dataset at either 6.25 Hz, 12.5 Hz or 25 Hz. The training tasks (Figure 4)\nwere sampled with equal probability. We used a cosine \u03b2-noise schedule [43].\nThe video decoder was optimized with AdamW and a learning rate of 5 \u00d7 10\u22125, weight decay 0.01,\nbeta coefficients (0.9, 0.99), norm gradient clipping 1.0. The model was trained for 300k steps in 15\ndays, with 2.5k of linear warm-up and 5k of cosine decay to a final learning rate of 1 \u00d7 10\u22126. We\nused a weighted average of L1 and L2 losses with weights \u03bbL1 = 0.1 and \u03bbL2 = 1.0. The batch\nsize was 64 split across 32 A100 80GB GPUs. We used an exponential moving average for the\nparameters with a decay of 0.999. The training strategy was also Deepspeed ZeRO-2 with activation\ncheckpointing.\n5\nInference\nIn this section, we describe in more detail the inference procedure of the world model and the video\ndecoder.\n5.1\nWorld Model\nSampling.\nThe world model autoregressively predicts the next image token, conditioned on previous\ntext, image and action tokens. Given the past tokens we perform n forward steps to generate one\nnew image frame. At each step we must sample a token from the predicted logits to select the next\ntoken in our sequence. Empirically we observed that maximization-based sampling (i.e. argmax)\ngenerates futures that get stuck in a repetitive loop, similarly to language models [44]. Conversely,\nif we simply sample from the logits, the selected token can come from the unreliable tail of the\nprobability distribution, which throws the model out-of-distribution, see Figure 6.\n9\n\u201cThe scene contains a red bus\u201d\nSCALE=1\nSCALE=20\nSCALE=100\n(a) Guidance scale factor\n0\n2\n4\n6\n8\n10\n12\nPredicted frame (since prompt injected)\nmax\nmin\nmax\nmin\nmax\nmin\nToken, Frame, Combined schedule\n(b) Guidance schedule\nFigure 7: Classifier-free guidance.\nTo encourage diversity as well as realism we employ top-k sampling to sample the next image token\nfrom the top-k most likely choices. The chosen value of k is a function of the number of tokens that\nconstitute an image frame as well as the pre-learnt codebook (vocabulary) size.\nOur world model can be used to roll out possible futures given starting context as well as generating\nfutures from scratch without any starting context. For long video generation, if the length of the video\nexceeds the context length of the world model, we employ a sliding window.\nText-conditioning.\nThe video prediction can be prompted, and thus directed, with text. At training\ntime, we condition our video sequences with text coming from either online narration or offline\nmetadata sources. Because these text sources are imperfect, to improve the alignment between\ngenerated futures and the text prompt, we employ classifier-free guidance [45, 46] at inference time.\nThe effect of guidance is to increase text-image alignment by decreasing the diversity of possible\nsamples. More precisely, for each next token to predict, we compute logits conditioned on text as\nwell as logits with no conditioning (unconditioned). At inference, we can then amplify the differences\nbetween the unconditioned and the text-conditioned logits with a scale factor to give the final logits\nused for sampling.\nlfinal = (1 + t)lconditioned \u2212tlunconditioned\n(3)\nBy substituting the unconditioned logits with those conditioned on another text prompt, we can\nperform \u201cnegative\u201d prompting [47]. Pushing the logits away from the negative prompt and towards\nthe positive one encourages the future tokens to include the \u201cpositive\u201d prompt features while removing\nthe \u201cnegative\u201d ones.\nWe found it was important to schedule the scale factor used for guidance over tokens as well as\nframes. Scheduling over tokens allows some to be sampled with high guidance (hence adhering\nstrongly to the prompt) and others to be sampled with low guidance (hence increasing sample\ndiversity). Scheduling over frames allows for controlling the transition from earlier frames as well\nas mitigating compounding guidance over subsequent consecutive frames. In Figure 7 we show an\nexample guidance schedule over twelve frames. Typically we used a schedule that sampled tokens\nwith linearly decreasing guidance over tokens and we lowered the guidance over future frames with\na cosine decay, with or without an initial plateau. We note that guidance scale and schedule are\nhyperparameters to be tuned to particular use cases.\n5.2\nVideo Decoder\nTo decode a sequence of generated tokens from the world model, we use the following video decoding\nmethod:\n1. Decode the first T \u2032 = 7 frames, conditioned on the corresponding T \u2032 image tokens.\n2. Autoregressively decode the next T \u2032 \u22122 frames, using 2 past overlapping frames as image\ncontext, and the following T \u2032 \u22122 image tokens.\n3. Repeat the autoregressive process until the N frames have been generated at 6.25 Hz.\n4. Temporally upsample the N frames from 6.25 Hz to 12.5 Hz\n5. Temporally upsample the 2N \u22121 frames from 12.5 Hz to 25.0 Hz\nWe use the DDIM sampler [48] with 50 diffusion steps. During autoregressive decoding, we\nsee a trade-off between reflecting token information content in the generated video and temporal\n10\n1016\n1018\n1020\n1022\nCompute (FLOPs)\n2.0\n3.0\n4.0\n5.0\nValidation cross-entropy\nObservations\nPrediction\nGAIA-1\n(a) The final performance of the GAIA-1 world model\ncould be predicted with smaller models trained with\nless than 20\u00d7 the compute.\n1015\n1017\n1019\n1021\n1023\nCompute (FLOPs)\n2.0\n4.0\n6.0\n8.0\nTraining cross-entropy\n0.0001x (0.65M)\n0.001x (6.6M)\n0.01x (66M)\n0.1x (650M)\nGAIA-1 (6.5B)\n(b) Training loss curves for world models up to 10,000x\nsmaller. We used an exponential moving average to\nsmooth the training loss curves.\nFigure 8: Validation and training cross-entropy of the world model.\nconsistency. To balance between these two objectives, we calculate a weighted average of the two\ntasks [18].\n\u02dc\u03f5\u03b8(xt\u2032, t\u2032, z, m) = w \u00b7 \u03f5\u03c0\n\u03b8 (xt\u2032, t\u2032, z, m) + (1 \u2212w) \u00b7 \u03f5\u03b8(xt\u2032, t\u2032, z, m)\n(4)\nwhere function \u03f5\u03c0\n\u03b8 (xt\u2032, t\u2032, z, m) denoises each frame individually as images and function\n\u03f5\u03b8(xt\u2032, t\u2032, z, m) denoises the sequence of frames jointly as a video. In practice, we simply switch on\nand off the temporal layers. We apply this weighted average randomly for each diffusion step with\nprobability p = 0.25 and weight w = 0.5.\nWhile exploring different inference approaches for video decoding we found that decoding video\nframes autoregressively backwards starting from the end of the sequence led to more stable objects\nand less flickering on the horizon. In our overall video decoding method, we thus decode the last T \u2032\nframes and autoregressively decodes the remaining frames backward from there.\n6\nScaling\nThe formulation of the world modeling task in GAIA-1 shares a commonality with the approach\nfrequently used in large language models (LLMs). In both instances, the task is streamlined to focus\non predicting the next token. Although this approach is adapted for world modeling in GAIA-1\nrather than the traditional language tasks seen in LLMs, it is intriguing to observe that scaling laws\n[49, 21, 27], analogous to those observed in LLMs, are also applicable to GAIA-1. This suggests the\nbroader applicability of scaling principles in modern AI models across diverse domains, including\nautonomous driving.\nTo explore scaling laws with GAIA-1, we predicted the final performance of the world model using\nmodels trained with less than 20\u00d7 the compute. We evaluated those models on a held-out geofenced\nvalidation set by measuring cross-entropy. A power-law of the form f(x) = c + (x/a)b was then\nfitted to the data points. In Figure 8a we can see that the final cross-entropy of GAIA-1 could be\npredicted with high accuracy.\nThe models used to fit the power-law ranged from 10,000x to 10x smaller models in terms of\nparameters (0.65M to 650M), as visualized in Figure 8b. Similarly to [49], the compute was estimated\nas a function of the parameter count. If we denote by C the compute and by N the parameter count\n(excluding embedding layers), the number of floating point operations for a forward-backward pass\nof a single token is given by C = 6N. To obtain the total amount of compute, this value is multiplied\nby the number of training tokens.\nIt is worth noting that our extrapolation leads us to the conclusion that there is substantial potential\nfor further improvement through the expansion of both data and computational resources.\n11\nFigure 9: Images generated by GAIA-1, highlighting the diversity of the generated driving scenes.\n12\n7\nCapabilities and Emerging Properties\nIn this section we showcase the capabilities and emerging properties of GAIA-1 through a series\nof qualitative examples. The comprehensive list of video examples can be found here. Figure 9\nshows the variety of scenarios that can be generated by our model. As evidenced by the examples\npresented in the rest of this section, GAIA-1 exhibits a level of understanding and summarization of\nthe generative rules of the world through the following emergent properties:\n1. Learning high-level structures and scene dynamics: it generates coherent scenes with objects\npositioned in plausible locations and exhibiting realistic object interactions, such as traffic\nlights, rules of the road, give ways, etc. This suggests that the model is not just memorizing\nstatistical patterns but is understanding the underlying rules that govern the arrangement and\nbehavior of objects in the world (see Section 7.1).\n2. Generalization and creativity: it can generate novel and diverse videos that go beyond specific\ninstances in the training set. It can produce unique combinations of objects, movements,\nand scenes that were not explicitly present in the training data, demonstrating remarkable\nextrapolation capabilities. This demonstrates a certain level of generalization and creativity,\nwhich suggests an understanding of the underlying generative rules that govern video\nsequences (see Section 7.2).\n3. Contextual awareness: GAIA-1 can capture contextual information and generate videos that\nreflect this understanding. For example, it can generate coherent actions and responses in\nvideos based on the initial conditions or the context provided. Moreover, GAIA-1 exhibits\nthe understanding of 3D geometry, effectively capturing the intricate interplay of pitch\nand roll induced by road irregularities (e.g. speed bumps). This contextual awareness\nsuggests that the models are not merely reproducing statistical patterns but are actively\nprocessing and summarizing the given information to generate appropriate video sequences\n(see Section 7.3).\n7.1\nGeneration of Long Driving Scenarios\nGAIA-1 can generate stable long videos (minutes) entirely from imagination (Figure 10). In order\nto do this, the model leverages its learned implicit prior distribution of the world to generate fully-\nimagined realistic driving scenarios, with complex road layouts, buildings, cars, pedestrians, and\nmore. This is a demonstration that GAIA-1 understands the rules that underpin the world we inhabit\nand its structures and dynamics.\n7.2\nGeneration of Multiple Plausible Futures\nGAIA-1 has the ability to generate a variety of distinct future scenarios based on a single initial\nprompt. When presented with a brief video as context, it can generate numerous plausible and diverse\noutcomes by repeatedly sampling. GAIA-1 accurately models multiple potential future scenarios\nin response to the video prompt while maintaining consistency with the initial conditions observed\nin the video. As seen in Figure 11, the world model can reason about (i) dynamic interactions with\nroad users (e.g. giving way or not giving way), (ii) multimodal ego-behaviors (e.g. going straight or\nturning at a roundabout), and (iii) multimodal dynamic scene (e.g. variable traffic density and types\nof road users such as pedestrians, cyclists, motorcyclists, vehicles) and static scene (e.g. road layout,\nbuildings, vegetation).\n7.3\nFine-Grained Control of the Ego-Vehicle Behavior and Driving Scenes\nGAIA-1 can generate videos from text prompts only, completely imagining the scene. To demonstrate\nthis we showcase how we can generate driving scenarios from text prompts that guide the model\ntowards specific weather or lighting conditions in Figure 12.\nNext, we present compelling examples where the model exhibits fine-grained control over the vehicle\ndynamics in the video. By leveraging this control, we can prompt the model to generate videos\ndepicting scenarios that lie outside the bounds of the training data. This shows that GAIA-1 is able to\ndisentangle the ego-vehicle dynamics from the surrounding environment and effectively generalize\nto unfamiliar scenarios. It provides explicit ability to reason about the impact of our actions on the\n13\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 40 s\n+ 40 s\n+ 40 s\n+ 40 s\n+ 40 s\n+ 40 s\n+ 40 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\n+ 0 s\n+ 10 s\n+ 20 s\n+ 30 s\nFigure 10: Long, diverse driving scenarios generated entirely from imagination by the world model.\nenvironment (safety), it allows richer understanding of dynamic scenes (intelligence), it unlocks\nmodel-based policy learning (planning in the world model), and it enables exploration in closed-loop\n(by considering the world model as a neural simulator). To showcase this, we make GAIA-1 generate\nfutures where the ego-vehicle steers left or right, deviating from its lane (Figure 13). GAIA-1 would\nnever have seen these incorrect behaviors in the expert driving dataset used to train it, indicating\nthat it can extrapolate driving concepts previously unseen in the training data. We also see realistic\nreactions of other agents to the ego-vehicle\u2019s controlled behavior.\nFinally we demonstrate the ability of GAIA-1 to leverage both text and action to fully imagine\na driving scenario. In this particular case we prompt the model to generate a bus in front of the\nego-vehicle and then we force its actions to overtake the bus (see Figure 1).\n14\nCONTEXT\nGENERATED FRAMES\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 2 s\n+ 4 s\n+ 6 s\n+ 8 s\n+ 2 s\n+ 4 s\n+ 6 s\n+ 8 s\nFigure 11: Examples of multiple plausible futures predicted by the world model from a given video\ncontext. 1) We observe a complex giving way interaction between the white vehicle and the ego-\nvehicle. In the first future, the white vehicle reverses to give way to the ego-vehicle. In the second\nfuture, the opposite occurs and the ego-vehicle slows down to give way to the white vehicle. 2) We\nsee two plausible ego-behaviors: going straight or turning right at the roundabout. 3) The model\npredicts two futures with varying traffic levels.\n15\nCONDITIONING\nGENERATED FRAMES\nText:\n\u201cIt is sunny\u201d\nText:\n\u201cIt\u2019s raining\u201d\nText:\n\u201cIt is foggy\u201d\nText:\n\u201cIt is snowing\u201d\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n(a) Weather.\nCONDITIONING\nGENERATED FRAMES\nText:\n\u201cIt is daytime. We\n are in direct\n sunlight\u201d\nText:\n\u201cThe sky is grey\u201d\nText:\n\u201cIt is twilight\u201d\nText:\n\u201cIt is night\u201d\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 0 s\n+ 1 s\n+ 2 s\n+ 3 s\n(b) Illumination.\nFigure 12: Generation from a text prompt, showing that the world model has learned different\nconcepts such as weather or illumination.\n16\nCONTEXT + CONDITIONING\nGENERATED FRAMES\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\n+ 1 s\n+ 2 s\n+ 3 s\n+ 4 s\nFigure 13: The world model can predict different outcomes conditioned on its actions. In the first four\nrows, we execute different out-of-distribution actions (left, strong left, right, strong right \u2014 while\nmaintaining speed) from a given context video. The world model can predict the corresponding states\nwith accurate geometry. In the last row, we force the ego-vehicle to steer right while maintaining\nspeed, and as a consequence, we observe the oncoming vehicle reacting and making a maneuver to\navoid a collision.\n17\n8\nRelated Work\nVideo generative models.\nVideo generative models are neural networks that can generate realistic\nvideo samples. They can be grouped in four categories: VAE-based (variational autoencoder [50],\nGAN-based (generative adversarial network [51]), diffusion-based [52], and autoregressive-based\n[53].\nLatent-variable video models (VAE-based) try to infer the underlying latent process that generated\nthe videos [54, 55, 56, 57, 58]. One known limitation of those models is that they tend to generate\nblurry outputs due to limited representational power, inadequate choice of prior distribution, and\nthe optimization of a lower-bound instead of the true likelihood. GAN-based methods produce\nmore realistic videos [59, 60, 61, 62, 63, 64] but are known to suffer from training instability and a\nlack of generation diversity [65]. Diffusion-based methods have yielded significant enhancements\nin realism, controllability, and temporal consistency. They can operate either at the pixel level\n[38, 17, 66, 67, 68, 69, 16] or in the latent space of a pre-trained image tokenizer [70, 18, 71].\nDiffusion models are expressive neural networks that can fit complex data distributions, but rely on\na long Markov chain of diffusion steps to generate samples. Lastly, autoregressive-based methods\nare conceptually simple and rely on tractable exact likelihood optimization (fits the entire data\ndistribution). Likewise, they can operate at the pixel level [72, 73], or in a discrete learned token\nspace [74, 75, 76, 77]. A known limitation is the slow generation speed, but this issue could be\nalleviated by future research on parallel sampling [78, 79, 80], reducing the number of latent variables\n[81], and improvements in hardware accelerators.\nWorld models.\nA world model is a predictive model of the future that learns a general representation\nof the world in order to understand the consequences of its actions [7, 8]. The main use cases are:\npure representation learning, planning (look-ahead search), or learning a policy in the world model\n(neural simulator).\nWorld modeling has been used as a pre-training task to learn a compact and general representation\nin a self-supervised way [82, 83]. Subsequently, using this representation as a state for traditional\nreinforcement learning (RL) algorithms significantly accelerated convergence speed. World models\ncan also be utilized for look-ahead search, in order to plan by imagining the outcomes of future\nactions. They have proven to be highly effective in game environments or board games [9, 84].\nAdditionally, world models can be a solution to the sample efficiency issues of RL algorithms by\nacting as a simulator of the environment [7, 85, 86, 62, 13, 15, 87], although this assumes the world\nmodel is an accurate model of the environment.\nA recent line of work suggests casting world modeling as a single sequence model, treating states,\nactions and rewards as simply a stream of data [10, 19, 14, 88, 12, 89]. The advantage of such a\nperspective is that world models can benefit from scaling properties of high-capacity sequence model\narchitectures applied to large-scale unsupervised training [26]. This is the approach that GAIA-1\ntakes, leveraging those scaling properties to model complex environments such as real-world driving\nscenes.\nScaling.\nLarge language models have shown clear benefits in scaling model size and data [90, 24,\n26, 20, 21, 22, 23]. In particular, [49] showed predictable relationships between model/data size and\nloss over multiple orders of magnitude. [49] derived power laws for transformer based language\nmodels in order to optimally allocate the compute budget between the model and data size. Those\nlaws were then refined by [27] by adapting the learning rate schedule when changing the dataset size.\nAnother direction of research to improve the training efficiency of language models is data quality.\n[91] showed that the quality of the training data plays a critical role in the performance of language\nmodels in downstream tasks.\nTransferring the scaling principles from large language models to the visual domain holds the potential\nfor delivering consistent and expected performance improvements [92, 93, 43, 16, 94]. In this work,\nby casting the problem of world modeling as unsupervised sequence modeling, we have shown that\nsimilar scaling trends from language models also applied to world models.\n18\n9\nConclusions and Future Work\nGAIA-1 is a generative world model for autonomous driving. The world model uses vector-quantized\nrepresentations to turn the task of future prediction into a next token prediction task, a technique that\nhas been successfully employed in large language models. GAIA-1 has demonstrated its capability to\nacquire a comprehensive understanding of the environment, distinguishing between various concepts\nsuch as cars, trucks, buses, pedestrians, cyclists, road layouts, buildings, and traffic lights \u2014 all\nthrough self-supervision. Further, GAIA-1 harnesses the capabilities of video diffusion models to\ngenerate realistic driving scenarios, thereby functioning as an advanced neural simulator. GAIA-1 is\na multimodal approach that enables the control of the ego-vehicle\u2019s actions and other scene attributes\nthrough a combination of textual and action-based instructions.\nWhile our method demonstrated promising results that have the potential to push the boundaries of\nautonomous driving, it is important to acknowledge current limitations. For instance, the autore-\ngressive generation process, while highly effective, does not yet run at real-time. Nevertheless, it is\nnoteworthy that this process lends itself well to parallelization, allowing for the concurrent generation\nof multiple samples.\nThe significance of GAIA-1 extends beyond its generative capabilities. World models represent a\ncrucial step towards achieving autonomous systems that can understand, predict, and adapt to the\ncomplexities of the real world. Furthermore, by incorporating world models into driving models,\nwe can enable them to better understand their own decisions and ultimately generalize to more\nreal-world situations. Lastly, GAIA-1 can also serve as a valuable neural simulator, allowing the\ngeneration of unlimited data, including adversarial examples, for training and validating autonomous\ndriving systems.\nAcknowledgments\nThis work was made possible through the expertise and generous help of many teams and people across Wayve.\nIn particular we would like to thank: Giulio D\u2019Ippolito, Dan Reisman, Alex Persin, Przemyslaw Mazur, Oleg\nSinavski, Long Chen, Fergal Cotter, Corina Gurau, Shu Ishida, Remi Tachet, Rudi Rankin, Tilly Pielichaty, Rod\nBauer, Charlie Lyons-Rothbart, Harriett-Rose Follas, Robert Weston, Becky Goldman, Sasha Harrison, Saurabh\nNair, Prajwal Chidananda, Tom Newton, Benoit Hanotte, Ana-Maria Marcu, Thomas Sajot, Giacomo Gallino,\nAlex Garcia Mayans, Tim Geypens, Robin Tweedie, Rebecca Hills, Tim Williams-Silvera, Darren Jenner, Matt\nWood, Dave Chilvers, Danny Ly, Joseph Rodrigo, Will Dias, Naomi Standard, and Theepa Balasubramaniam.\n19\nReferences\n[1] A. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M. Allen, V.-D. Lam, A. Bewley, and\nA. Shah. Learning to drive in a day. In Proceedings of the International Conference on Robotics\nand Automation (ICRA), 2019.\n[2] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan,\nand O. Beijbom. nuScenes: A multimodal dataset for autonomous driving. Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\n[3] A. Hu, F. Cotter, N. Mohan, C. Gurau, and A. Kendall. Probabilistic Future Prediction for Video\nScene Understanding. In Proceedings of the European Conference on Computer Vision (ECCV),\n2020.\n[4] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai, B. Sapp, C. Qi, Y. Zhou,\nZ. Yang, A. Chouard, P. Sun, J. Ngiam, V. Vasudevan, A. McCauley, J. Shlens, and D. Anguelov.\nLarge Scale Interactive Motion Forecasting for Autonomous Driving : The Waymo Open\nMotion Dataset. In Proceedings of the IEEE International Conference on Computer Vision\n(ICCV), 2021.\n[5] A. Hu, Z. Murez, N. Mohan, S. Dudas, J. Hawke, V. Badrinarayanan, R. Cipolla, and A. Kendall.\nFIERY: Future Instance Prediction in Bird\u2019s-Eye View From Surround Monocular Cameras.\nIn Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages\n15273\u201315282, 2021.\n[6] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang, L. Lu, X. Jia,\nQ. Liu, J. Dai, Y. Qiao, and H. Li. Planning-oriented autonomous driving. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[7] D. Ha and J. Schmidhuber. Recurrent world models facilitate policy evolution. In Advances in\nNeural Information Processing Systems (NeurIPS), 2018.\n[8] Y. LeCun. A Path Towards Autonomous Machine Intelligence. In arXiv preprint, 2022.\n[9] J. Schrittwieser, I. Antonoglou, T. Hubert, K. Simonyan, L. Sifre, S. Schmitt, A. Guez, E. Lock-\nhart, D. Hassabis, T. Graepel, T. Lillicrap, and D. Silver. Mastering Atari, Go, Chess and Shogi\nby Planning with a Learned Model. In Nature, 2020.\n[10] M. Janner, Q. Li, and S. Levine. Offline reinforcement learning as one big sequence modeling\nproblem. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\n[11] A. Hu, G. Corrado, N. Griffiths, Z. Murez, C. Gurau, H. Yeo, A. Kendall, R. Cipolla, and\nJ. Shotton. Model-Based Imitation Learning for Urban Driving. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[12] V. Micheli, E. Alonso, and F. Fleuret. Transformers are sample-efficient world models. In\nProceedings of the International Conference on Learning Representations (ICLR), 2023.\n[13] D. Hafner, J. Pasukonis, J. Ba, and T. Lillicrap. Mastering diverse domains through world\nmodels. In arXiv preprint, 2023.\n[14] S. Reed, K. Zolna, E. Parisotto, S. G\u00f3mez, A. Novikov, G. Barth-Maron, M. Gimenez, Y. Sulsky,\nJ. Kay, J. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen,\nR. Hadsell, O. Vinyals, M. Bordbar, and N. Freitas. A generalist agent. In Transactions on\nMachine Learning Research (TMLR), 2022.\n[15] P. Wu, A. Escontrela, D. Hafner, P. Abbeel, and K. Goldberg. Daydreamer: World models for\nphysical robot learning. In Proceedings of the Conference on Robot Learning (CoRL), 2023.\n[16] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P. Kingma, B. Poole, M. Norouzi,\nD. J. Fleet, and T. Salimans. Imagen video: High definition video generation with diffusion\nmodels. In arXiv preprint, 2022.\n[17] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood. Flexible diffusion modeling\nof long videos. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\n20\n[18] P. Esser, J. Chiu, P. Atighehchian, J. Granskog, and A. Germanidis. Structure and content-guided\nvideo synthesis with diffusion models. In arXiv preprint, 2023.\n[19] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel, A. Srinivas, and\nI. Mordatch. Decision transformer: Reinforcement learning via sequence modeling. In Advances\nin Neural Information Processing Systems (NeurIPS), 2021.\n[20] S. Smith, M. M. A. Patwary, B. Norick, P. Legresley, S. Rajbhandari, J. Casper, Z. Liu,\nS. Prabhumoye, G. Zerveas, V. Korthikanti, E. Zhang, R. Child, R. Aminabadi, J. Bernauer,\nX. Song, M. Shoeybi, Y. He, M. Houston, S. Tiwary, and B. Catanzaro. Using DeepSpeed and\nMegatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model.\nIn arXiv preprint, 2022.\n[21] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H. W.\nChung, C. Sutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao,\nP. Barnes, Y. Tay, N. M. Shazeer, V. Prabhakaran, E. Reif, N. Du, B. C. Hutchinson, R. Pope,\nJ. Bradbury, J. Austin, M. Isard, G. Gur-Ari, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev,\nH. Michalewski, X. Garc\u00eda, V. Misra, K. Robinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan,\nH. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan, S. Agrawal, M. Omernick, A. M. Dai,\nT. S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O. Polozov, K. Lee, Z. Zhou,\nX. Wang, B. Saeta, M. D\u00edaz, O. Firat, M. Catasta, J. Wei, K. S. Meier-Hellstern, D. Eck, J. Dean,\nS. Petrov, and N. Fiedel. PaLM: Scaling language modeling with pathways. arXiv preprint,\n2022.\n[22] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal,\nE. Hambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample. Llama: Open and\nefficient foundation language models. In arXiv preprint, 2023.\n[23] OpenAI. GPT-4 Technical Report. In arXiv preprint, 2023.\n[24] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J.\nLiu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of\nMachine Learning Research, 2020.\n[25] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and\nV. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint, 2019.\n[26] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan,\nP. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child,\nA. Ramesh, D. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and D. Amodei. Lan-\nguage Models are Few-Shot Learners. In Advances in Neural Information Processing Systems\n(NeurIPS), 2020.\n[27] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de las Casas,\nL. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,\nB. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, O. Vinyals, J. W. Rae, and L. Sifre. An\nempirical analysis of compute-optimal large language model training. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[28] A. van den Oord, O. Vinyals, and K. Kavukcuoglu. Neural discrete representation learning. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2017.\n[29] Z. Peng, L. Dong, H. Bao, Q. Ye, and F. Wei. BEiT v2: Masked Image Modeling with\nVector-Quantized Visual Tokenizers. arXiv preprint, 2022.\n[30] M. Caron, H. Touvron, I. Misra, H. J\u00e9gou, J. Mairal, P. Bojanowski, and A. Joulin. Emerging\nproperties in self-supervised vision transformers. In Proceedings of the IEEE International\nConference on Computer Vision (ICCV), 2021.\n[31] O. Ronneberger, P. Fischer, and T. Brox. U-Net: Convolutional Networks for Biomedical Image\nSegmentation. In Medical Image Computing and Computer-Assisted Intervention (MICCAI),\n2015.\n21\n[32] J. Johnson, A. Alahi, and L. Fei-Fei. Perceptual losses for real-time style transfer and super-\nresolution. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.\n[33] P. Esser, R. Rombach, and B. Ommer. Taming transformers for high-resolution image synthesis.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),\n2021.\n[34] J. Yu, X. Li, J. Y. Koh, H. Zhang, R. Pang, J. Qin, A. Ku, Y. Xu, J. Baldridge, and Y. Wu.\nVector-quantized image modeling with improved VQGAN. In Proceedings of the International\nConference on Learning Representations (ICLR), 2022.\n[35] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, and I. Polo-\nsukhin. Attention is all you need. In Advances in Neural Information Processing Systems\n(NeurIPS), 2017.\n[36] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis\nwith latent diffusion models. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), 2022.\n[37] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. Denton, S. K. S. Ghasemipour, B. K. Ayan,\nS. S. Mahdavi, R. G. Lopes, T. Salimans, J. Ho, D. J. Fleet, and M. Norouzi. Photorealistic\ntext-to-image diffusion models with deep language understanding. In Advances in Neural\nInformation Processing Systems (NeurIPS), 2022.\n[38] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models.\nIn arXiv preprint, 2022.\n[39] J. H. Tim Salimans. Progressive distillation for fast sampling of diffusion models. In Proceedings\nof the International Conference on Learning Representations (ICLR), 2022.\n[40] I. Loshchilov and F. Hutter. Decoupled Weight Decay Regularization. In Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2019.\n[41] T. Dao, D. Y. Fu, S. Ermon, A. Rudra, and C. R\u00e9. FlashAttention: Fast and memory-efficient\nexact attention with IO-awareness. In Advances in Neural Information Processing Systems\n(NeurIPS), 2022.\n[42] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He. DeepSpeed: System Optimizations Enable\nTraining Deep Learning Models with Over 100 Billion Parameters. In Proceedings of the 26th\nACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020.\n[43] E. Hoogeboom, J. Heek, and T. Salimans. simple diffusion: End-to-end diffusion for high\nresolution images. In Proceedings of the International Conference on Machine Learning\n(ICML), 2023.\n[44] A. Holtzman, J. Buys, L. Du, M. Forbes, and Y. Choi.\nThe curious case of neural text\ndegeneration. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2020.\n[45] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint, 2022.\n[46] H. Chang, H. Zhang, J. Barber, A. Maschinot, J. Lezama, L. Jiang, M.-H. Yang, K. Murphy,\nW. T. Freeman, M. Rubinstein, et al. Muse: Text-to-image generation via masked generative\ntransformers. arXiv preprint, 2023.\n[47] Negative prompt.\nhttps://github.com/AUTOMATIC1111/stable-diffusion-webui/\nwiki/Negative-prompt, 2022.\n[48] J. Song, C. Meng, and S. Ermon. Denoising diffusion implicit models. Proceedings of the\nInternational Conference on Learning Representations (ICLR), 2021.\n[49] J. Kaplan, S. McCandlish, T. Henighan, T. B. Brown, B. Chess, R. Child, S. Gray, A. Radford,\nJ. Wu, and D. Amodei. Scaling laws for neural language models. In arXiv preprint, 2020.\n22\n[50] D. P. Kingma and M. Welling. Auto-encoding variational bayes. Proceedings of the International\nConference on Learning Representations (ICLR), 2014.\n[51] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and\nY. Bengio. Generative adversarial nets. In Advances in Neural Information Processing Systems\n(NeurIPS), 2014.\n[52] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli. Deep unsupervised learning\nusing nonequilibrium thermodynamics. In Proceedings of the International Conference on\nMachine Learning (ICML), 2015.\n[53] A. van den Oord, N. Kalchbrenner, L. Espeholt, k. kavukcuoglu, O. Vinyals, and A. Graves.\nConditional image generation with pixelcnn decoders. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2016.\n[54] M. Babaeizadeh, C. Finn, D. Erhan, R. H. Campbell, and S. Levine. Stochastic variational\nvideo prediction. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2018.\n[55] E. Denton and R. Fergus. Stochastic Video Generation with a Learned Prior. In Proceedings of\nthe International Conference on Machine Learning (ICML), 2018.\n[56] R. Villegas, A. Pathak, H. Kannan, D. Erhan, Q. Le, and H. Lee. High fidelity video prediction\nwith large stochastic recurrent neural networks. In Advances in Neural Information Processing\nSystems (NeurIPS), 2019.\n[57] J.-Y. Franceschi, E. Delasalles, M. Chen, S. Lamprier, and P. Gallinari. Stochastic latent residual\nvideo prediction. In Proceedings of the International Conference on Machine Learning (ICML),\n2020.\n[58] M. Babaeizadeh, M. Saffar, S. Nair, S. Levine, C. Finn, and D. Erhan. Fitvid: Overfitting in\npixel-level video prediction. In arXiv preprint, 2021.\n[59] C. Vondrick, H. Pirsiavash, and A. Torralba. Generating videos with scene dynamics. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2016.\n[60] S. Tulyakov, M.-Y. Liu, X. Yang, and J. Kautz. MoCoGAN: Decomposing motion and content\nfor video generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018.\n[61] A. Clark, J. Donahue, and K. Simonyan. Adversarial Video Generation on Complex Datasets.\nIn arXiv preprint, 2019.\n[62] S. W. Kim, J. Philion, A. Torralba, and S. Fidler. DriveGAN: Towards a controllable high-quality\nneural simulation. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021.\n[63] I. Skorokhodov, S. Tulyakov, and M. Elhoseiny. StyleGAN-V: A continuous video generator\nwith the price, image quality and perks of StyleGAN2. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2022.\n[64] T. Brooks, J. Hellsten, M. Aittala, T.-C. Wang, T. Aila, J. Lehtinen, M.-Y. Liu, A. Efros, and\nT. Karras. Generating long videos of dynamic scenes. In Advances in Neural Information\nProcessing Systems (NeurIPS), 2022.\n[65] I. Goodfellow. NIPS 2016 Tutorial: Generative Adversarial Networks. In arXiv preprint, 2016.\n[66] V. Voleti, A. Jolicoeur-Martineau, and C. Pal. MCVD: Masked conditional video diffusion\nfor prediction, generation, and interpolation. In Advances in Neural Information Processing\nSystems (NeurIPS), 2022.\n[67] T. H\u00f6ppe, A. Mehrjou, S. Bauer, D. Nielsen, and A. Dittadi. Diffusion models for video\nprediction and infilling. In Transactions on Machine Learning Research (TMLR), 2022.\n23\n[68] U. Singer, A. Polyak, T. Hayes, X. Yin, J. An, S. Zhang, Q. Hu, H. Yang, O. Ashual, O. Gafni,\nD. Parikh, S. Gupta, and Y. Taigman. Make-A-Video: Text-to-video generation without text-\nvideo data. In arXiv preprint, 2022.\n[69] E. Molad, E. Horwitz, D. Valevski, A. R. Acha, Y. Matias, Y. Pritch, Y. Leviathan, and Y. Hoshen.\nDreamix: Video diffusion models are general video editors. arXiv preprint, 2023.\n[70] D. Zhou, W. Wang, H. Yan, W. Lv, Y. Zhu, and J. Feng. Magicvideo: Efficient video generation\nwith latent diffusion models. arXiv preprint, 2022.\n[71] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align\nyour latents: High-resolution video synthesis with latent diffusion models. In Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[72] N. Kalchbrenner, A. van den Oord, K. Simonyan, I. Danihelka, O. Vinyals, A. Graves, and\nK. Kavukcuoglu. Video Pixel Networks. In Proceedings of the International Conference on\nMachine Learning (ICML), 2017.\n[73] D. Weissenborn, O. T\u00e4ckstr\u00f6m, and J. Uszkoreit. Scaling autoregressive video models. Pro-\nceedings of the International Conference on Learning Representations (ICLR), 2020.\n[74] W. Yan, Y. Zhang, P. Abbeel, and A. Srinivas. VideoGPT: Video generation using vq-vae and\ntransformers. In arXiv preprint, 2021.\n[75] G. L. Moing, J. Ponce, and C. Schmid. CCVS: Context-aware controllable video synthesis. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2021.\n[76] S. Ge, T. Hayes, H. Yang, X. Yin, G. Pang, D. Jacobs, J.-B. Huang, and D. Parikh. Long\nvideo generation with time-agnostic vqgan and time-sensitive transformer. Proceedings of the\nEuropean Conference on Computer Vision (ECCV), 2022.\n[77] Y. Seo, K. Lee, F. Liu, S. James, and P. Abbeel. HARP: Autoregressive latent video prediction\nwith high-fidelity image generator. In Proceedings of the IEEE International Conference on\nImage Processing (ICIP), 2022.\n[78] W. Yan, D. Hafner, S. James, and P. Abbeel. Temporally consistent transformers for video\ngeneration. In Proceedings of the International Conference on Machine Learning (ICML),\n2023.\n[79] R. Villegas, M. Babaeizadeh, P.-J. Kindermans, H. Moraldo, H. Zhang, M. T. Saffar, S. Castro,\nJ. Kunze, and D. Erhan. Phenaki: Variable length video generation from open domain textual\ndescription. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2023.\n[80] L. Yu, Y. Cheng, K. Sohn, J. Lezama, H. Zhang, H. Chang, A. G. Hauptmann, M.-H. Yang,\nY. Hao, I. Essa, and L. Jiang. MAGVIT: Masked Generative Video Transformer. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\n[81] C. Hawthorne, A. Jaegle, C. Cangea, S. Borgeaud, C. Nash, M. Malinowski, S. Dieleman,\nO. Vinyals, M. Botvinick, I. Simon, H. Sheahan, N. Zeghidour, J.-B. Alayrac, J. Carreira,\nand J. Engel. General-purpose, long-context autoregressive modeling with Perceiver AR. In\nProceedings of the International Conference on Machine Learning (ICML), 2022.\n[82] M. Schwarzer, A. Anand, R. Goel, R. D. Hjelm, A. C. Courville, and P. Bachman. Data-efficient\nreinforcement learning with self-predictive representations. In Proceedings of the International\nConference on Learning Representations (ICLR), 2020.\n[83] P. Wu, A. Majumdar, K. Stone, Y. Lin, I. Mordatch, P. Abbeel, and A. Rajeswaran. Masked\ntrajectory models for prediction, representation, and control. In Proceedings of the International\nConference on Machine Learning (ICML), 2023.\n[84] W. Ye, S. Liu, T. Kurutach, P. Abbeel, and Y. Gao. Mastering atari games with limited data. In\nAdvances in Neural Information Processing Systems (NeurIPS), 2021.\n24\n[85] L. Kaiser, M. Babaeizadeh, P. Milos, B. Osinski, R. Campbell, K. Czechowski, D. Erhan,\nC. Finn, P. Kozakowski, S. Levine, A. Mohiuddin, R. Sepassi, G. Tucker, and H. Michalewski.\nModel-based reinforcement learning for atari. In Proceedings of the International Conference\non Learning Representations (ICLR), 2020.\n[86] D. Hafner, T. Lillicrap, M. Norouzi, and J. Ba. Mastering atari with discrete world models.\nProceedings of the International Conference on Learning Representations (ICLR), 2021.\n[87] X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu. DriveDreamer: Towards Real-world-driven\nWorld Models for Autonomous Driving. arXiv preprint, 2023.\n[88] F. Liu, H. Liu, A. Grover, and P. Abbeel. Masked autoencoding for scalable and generalizable\ndecision making. Advances in Neural Information Processing Systems (NeurIPS), 2022.\n[89] commaVQ. https://github.com/commaai/commavq, 2023.\n[90] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of Deep Bidirectional\nTransformers for Language Understanding. North American Chapter of the Association for\nComputational Linguistics (NAACL), 2019.\n[91] N. Du, Y. Huang, A. M. Dai, S. Tong, D. Lepikhin, Y. Xu, M. Krikun, Y. Zhou, A. W. Yu,\nO. Firat, B. Zoph, L. Fedus, M. Bosma, Z. Zhou, T. Wang, Y. E. Wang, K. Webster, M. Pellat,\nK. Robinson, K. S. Meier-Hellstern, T. Duke, L. Dixon, K. Zhang, Q. V. Le, Y. Wu, Z. Chen, and\nC. Cui. GLaM: Efficient scaling of language models with mixture-of-experts. In Proceedings\nof the International Conference on Machine Learning (ICML), 2022.\n[92] Y. Tay, M. Dehghani, J. Rao, W. Fedus, S. Abnar, H. W. Chung, S. Narang, D. Yogatama,\nA. Vaswani, and D. Metzler. Scale efficiently: Insights from pre-training and fine-tuning\ntransformers. In Proceedings of the International Conference on Learning Representations\n(ICLR), 2022.\n[93] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski, J. Heek, J. Gilmer, A. Steiner, M. Caron,\nR. Geirhos, I. Alabdulmohsin, R. Jenatton, L. Beyer, M. Tschannen, A. Arnab, X. Wang,\nC. Riquelme, M. Minderer, J. Puigcerver, U. Evci, M. Kumar, S. van Steenkiste, G. F. Elsayed,\nA. Mahendran, F. Yu, A. Oliver, F. Huot, J. Bastings, M. P. Collier, A. Gritsenko, V. Birodkar,\nC. Vasconcelos, Y. Tay, T. Mensink, A. Kolesnikov, F. Paveti\u00b4c, D. Tran, T. Kipf, M. Lu\u02c7ci\u00b4c,\nX. Zhai, D. Keysers, J. Harmsen, and N. Houlsby. Scaling vision transformers to 22 billion\nparameters. In Proceedings of the International Conference on Machine Learning (ICML),\n2023.\n[94] W. Peebles and S. Xie. Scalable diffusion models with transformers. In arXiv preprint, 2023.\n25\n",
    "2311.01017": "Published as a conference paper at ICLR 2024\nCOPILOT4D: LEARNING UNSUPERVISED\nWORLD MODELS FOR AUTONOMOUS DRIVING VIA\nDISCRETE DIFFUSION\nLunjun Zhang\nYuwen Xiong\nZe Yang\nSergio Casas\nRui Hu\nRaquel Urtasun\nWaabi\nUniversity of Toronto\n{lzhang, yxiong, zyang, sergio, rhu, urtasun}@waabi.ai\nABSTRACT\nLearning world models can teach an agent how the world works in an unsuper-\nvised manner. Even though it can be viewed as a special case of sequence model-\ning, progress for scaling world models on robotic applications such as autonomous\ndriving has been somewhat less rapid than scaling language models with Gener-\native Pre-trained Transformers (GPT). We identify two reasons as major bottle-\nnecks: dealing with complex and unstructured observation space, and having a\nscalable generative model. Consequently, we propose Copilot4D, a novel world\nmodeling approach that first tokenizes sensor observations with VQVAE, then\npredicts the future via discrete diffusion. To efficiently decode and denoise tokens\nin parallel, we recast Masked Generative Image Transformer as discrete diffu-\nsion and enhance it with a few simple changes, resulting in notable improvement.\nWhen applied to learning world models on point cloud observations, Copilot4D\nreduces prior SOTA Chamfer distance by more than 65% for 1s prediction, and\nmore than 50% for 3s prediction, across NuScenes, KITTI Odometry, and Argov-\nerse2 datasets. Our results demonstrate that discrete diffusion on tokenized agent\nexperience can unlock the power of GPT-like unsupervised learning for robotics.\nPedestrians\nVehicles\nCurrent Observation\nFuture GT\nWorld Model Prediction\nAccurate \nNear-Term \n1s Prediction\nDiverse \nMulti-Future \n3s Prediction\nPrediction vs GT\nCurrent Observation\nWorld Model Future 1\nWorld Model Future 2\nWorld Model Future 3\nPrediction\nGround Truth\nTra\ufb03c on the other side of road\nFigure 1: Our unsupervised world model Copilot4D can produce accurate near-term 1s predic-\ntions and diverse multi-future 3s predictions directly on the level of point cloud observations.\n1\nINTRODUCTION\nWorld models explicitly represent the knowledge of an autonomous agent about its environment.\nThey are defined as a generative model that predicts the next observation in an environment given\npast observations and the current action. Such a generative model can learn from any unlabeled\nagent experience, and can be used for both learning and planning in the model-based reinforcement\nlearning framework (Sutton, 1991). This approach has excelled in domains such as Atari (Kaiser\net al., 2019), robotic manipulation (Nagabandi et al., 2020), and Minecraft (Hafner et al., 2023).\nLearning world models can be viewed as a special case of sequence modeling on agent experience.\nWhile Generative Pre-trained Transformers (GPT) (Brown et al., 2020) have enabled rapid progress\n1\narXiv:2311.01017v4  [cs.CV]  1 Apr 2024\nPublished as a conference paper at ICLR 2024\nin natural language processing (NLP) via sequence modeling of unlabeled text corpus, progress for\nscaling world models has been less rapid in robotic applications such as autonomous driving. Predic-\ntion systems in autonomous driving still require supervised learning, either on the level of bounding\nboxes (Luo et al., 2018), semantic segmentation (Sadat et al., 2020), or instance segmentation (Hu\net al., 2021). However, just as GPT learns to understand language via next token prediction, if a\nworld model can predict unlabeled future observations really well, it must have developed a general\nunderstanding of the scene including geometry and dynamics. We ask: what makes it difficult to\nlearn an unsupervised world model that directly predicts future observations?\n(i) The observation space can be complex and unstructured. Whether it is autonomous driving or\nrobotic hands solving Rubik\u2019s cubes (Akkaya et al., 2019), selecting a loss on the observation space\nand building a generative model that captures meaningful likelihoods can be highly non-trivial.\nBy contrast, in natural language processing, language models like GPT (Brown et al., 2020) first\ntokenize a text corpus, then predict discrete indices like a classifier, leading to impressive success.\nFortunately, this gap can addressed by training a VQVAE-like (Van Den Oord et al., 2017) model to\ntokenize any inputs, from images (Ramesh et al., 2021) to point clouds (Xiong et al., 2023).\n(ii) The generative model needs to be scalable. In particular, language models are known to scale\nwell (Kaplan et al., 2020), but they only decode one token at a time. In domains such as autonomous\ndriving, a single observation has tens of thousands of tokens, so parallel decoding of tokens becomes\na must. On the other hand, decoding all the tokens of an observation in parallel, which is sufficient\nfor achieving success in Minecraft (Hafner et al., 2023), would incorrectly assume that all those\ntokens are conditionally independent given past observations. Thanks to Masked Generative Image\nTransformer (MaskGIT) (Chang et al., 2022), we can train a model to iteratively decode an arbitrary\nnumber of tokens in parallel. In this work, we recast MaskGIT into the discrete diffusion framework\n(Austin et al., 2021), resulting in a few simple changes that notably improve upon MaskGIT.\nThe analysis above sheds light on a scalable approach to building world models: tokenize each\nobservation frame with VQVAE, apply discrete diffusion on each frame, and autoregressively predict\nthe future. We apply this approach to the task of point cloud forecasting in autonomous driving\n(Weng et al., 2021; Mersch et al., 2022; Khurana et al., 2023), which aims to predict future point\ncloud observations given past observations and future ego vehicle poses. This task is essentially\nabout building an unsupervised world model on Lidar sensor observations. We design our neural\narchitectures to leverage prior knowledge for this task: our tokenizer uses an implicit representation\nfor volume rendering (Mildenhall et al., 2021) of ray depth in the VQVAE decoder; the world model\nuses a Transformer that interleaves spatial (Liu et al., 2021) and temporal blocks in Bird-Eye View\n(BEV). After tokenization, our world model operates entirely on discrete token indices.\nOur approach, named Copilot4D, significantly outperforms prior state-of-the-art for point cloud\nforecasting in autonomous driving. On NuScenes (Caesar et al., 2020), KITTI Odometry (Geiger\net al., 2012), and Argoverse2 (Wilson et al., 2023) datasets, Copilot4D reduces prior SOTA Cham-\nfer distance by 65%\u221275% for 1s prediction, and more than 50% for 3s prediction. In Figure 1,\nwe showcase that not only is our world model able to make accurate predictions on a 1s time hori-\nzon, it has also managed to learn the multi-modality of future observations on a 3s time horizon. The\nresults validate our analysis that the combination of tokenization and discrete diffusion can unlock\nthe possibility of learning world models at scale on real-world data.\n2\nRELATED WORK\nWorld Models predict the next observation in an environment given the current action and the past\nobservations. The idea of learning a world model from data dates back to adaptive control (Slotine\net al., 1991), which applies parameter estimation to a fixed structure of the dynamics. Under model-\nbased reinforcement learning frameworks such as Dyna (Sutton, 1991), many attempts have been\nmade to use deep generative models as world models. Ha & Schmidhuber (2018) trained a VAE\n(Kingma & Welling, 2013) to encode observations, and a recurrent neural net (RNN) on the latent\ncodes to model the dynamics. Dreamer-v2 (Hafner et al., 2020) finds that replacing Gaussian latents\nwith discrete latents significantly improves world modeling for Atari. IRIS (Micheli et al., 2022)\nshows that using a Transformer (Vaswani et al., 2017) rather than an RNN for dynamics modeling\nfurther improves Atari results. Those prior works provide a valuable guide for building world models\nfor autonomous driving; we tackle the point-cloud forecasting task (Weng et al., 2021; Mersch et al.,\n2022; Weng et al., 2022; Khurana et al., 2023) using lessons learned from those other domains.\n2\nPublished as a conference paper at ICLR 2024\nDiffusion Models are a class of generative models that define a forward process from data distribu-\ntion to noise distribution in closed-form, and then learn the reverse process from noise distribution\nto data distribution (Sohl-Dickstein et al., 2015). Diffusion for continuous random variables typi-\ncally uses Gaussian noise, and utilizes properties of Gaussian distributions to simplify the training\nobjectives (Ho et al., 2020; Kingma et al., 2021) and speed up inference (Song et al., 2020). In com-\nparison, diffusion for discrete data (Austin et al., 2021) has received less attention from the commu-\nnity. Recently, Masked Generative Image Transformer (MaskGIT) (Chang et al., 2022) shows that\ntraining a BERT (Devlin et al., 2018) on image tokens with an aggressive masking schedule can out-\nperform Gaussian diffusion. MaskGIT has been successfully applied to many applications such as\ntext-to-image generation (Chang et al., 2023), video prediction (Gupta et al., 2022; Yu et al., 2023),\nand point cloud generation (Xiong et al., 2023). Our work sheds light on the connection between\nMaskGIT and discrete diffusion, and how MaskGIT can be further improved based on diffusion.\n3D Representation for Point Clouds has long been studied in both robot perception and 3D scene\ngeneration. For self-driving perception, point cloud data from Lidar sensor typically plays an im-\nportant role. Modern approaches such as VoxelNet (Zhou & Tuzel, 2018) and PointPillars (Lang\net al., 2019) first apply a PointNet (Qi et al., 2017) on each voxel or pillar, typically followed by 2D\nconvolution in Bird-Eye View (BEV) (Yang et al., 2018) for tasks such as 3D object detection. For\n3D generation, implicit neural scene representation has been gaining popularity since Neural Radi-\nance Fields (NeRF) (Mildenhall et al., 2021); with an implicit function for occupancy, point clouds\ncan be obtained through differentaible depth rendering (Rematas et al., 2022; Yang et al., 2023).\nThis representation is also used in prior work on point cloud forecasting (Khurana et al., 2023). Our\ntokenizer draws ideas from 3D detection to design the encoder, and from implicit neural scene rep-\nresentation to design the decoder. After tokenization, however, the specific 3D representations are\nabstracted away from the world model, which operates on discrete tokens.\n3\nBACKGROUND: DIFFUSION MODELS\nWe review diffusion for a single random variable x0 (which can be trivially extended to multi-variate\nx0). Given x0 and the forward process q(x1:K|x0), diffusion typically learns a reverse process\np\u03b8(xk\u22121|xk) by maximizing an evidence-lower bound (ELBO) log p\u03b8(x0) \u2265\u2212Lelbo(x0, \u03b8) =\nEq(x1:K|x0)\nh\n\u2212\nX\nk>1\nDKL(q(xk\u22121|xk, x0) \u2225p\u03b8(xk\u22121|xk)) + log p\u03b8(x0|x1) \u2212DKL(q(xK|x0) \u2225p(xK))\ni\nFor discrete random variables, since we can easily sum over its probabilities, p\u03b8(xk\u22121|xk) is often\nparameterized to directly infer x0, as done in D3PM (Austin et al., 2021):\np\u03b8(xk\u22121 | xk) =\nX\nx0\nq(xk\u22121 | xk, x0)p\u03b8(x0 | xk)\n(1)\nCalculating the posterior q(xk\u22121|xk, x0) = q(xk\u22121|x0)q(xk|xk\u22121)/q(xk|x0) is necessary in the\noriginal diffusion loss, which means that the cumulative forward transition matrix defined in\nq(xk|x0) often requires a closed-form solution. In D3PM, absorbing diffusion recasts BERT (Devlin\net al., 2018) as a one-step diffusion model, where the forward diffusion process gradually masks out\nground-truth tokens. VQ-Diffusion (Gu et al., 2022) points out that, when xk \u0338= x0, the posterior in\nabsorbing diffusion is not well-defined since in that case q(xk|x0) = 0, which motivated adding uni-\nform diffusion in their model. By contrast, MaskGIT (Chang et al., 2022) has significantly simpler\ntraining and sampling procedures based on BERT alone: for training, it masks a part of the input\ntokens (with an aggressive masking schedule) and then predicts the masked tokens from the rest; for\nsampling, it iteratively decodes tokens in parallel based on predicted confidence.\nClassifier-free diffusion guidance (Ho & Salimans, 2022) has become a standard tool for diffusion-\nbased conditional generation. Given context c, it has been shown that sampling from \u02dcp\u03b8(x0|xk, c) \u221d\np\u03b8(x0|xk, c)(p\u03b8(x0|xk, c)/p\u03b8(x0|xk))w rather than directly from p\u03b8(x0|xk, c) performs signifi-\ncantly better. Chang et al. (2023) has proposed directly modifying the logits of MaskGIT:\nlogitscfg(\u02dcx0|xk+1, c) = logits(\u02dcx0|xk+1, c) + w \u00b7 (logits(\u02dcx0|xk+1, c) \u2212logits(\u02dcx0|xk+1))\n(2)\nin order to perform classifier-free guidance analogous to its counterpart in diffusion.\n4\nMETHOD: COPILOT4D\nGiven a sequence of agent experience (o(1), a(1), \u00b7 \u00b7 \u00b7 , o(T \u22121), a(T \u22121), o(T )) where o is an observa-\ntion and a is an action, we aim to learn a world model p\u03b8 that predicts the next observation given\n3\nPublished as a conference paper at ICLR 2024\nVQ\nCodebook Lookup\nBEV Tokens\nRay Distance\nRender\nTime\nt-2\nt-1\nt\n\u2026 \u2026\nSpatio-Temporal \nTransformer\nDi\ufb00usion steps\n\u2026 \u2026\nt+1\nObservation\nReconstruction\nDecoder\nEncoder\nPast observation tokens and actions\nMask Token\nAutoregressively \npredict future frames\nt+2\n\u2026 \u2026\nPredict \nframe t+1\nPredict \nframe t+2\nAction at t+1\nFigure 2: An overview of our method for Copilot4D, which first tokenizes sensor observations\nwith a VQVAE-like tokenizer, then predicts the future via discrete diffusion. The tokenizer encodes\npoint clouds into discrete latents in Bird-Eye View (BEV), and does reconstruction via differentiable\ndepth rendering. The world model is a discrete diffusion model that operates on BEV tokens.\nthe past observations and actions. In the autonomous driving setting that we tackle, the observations\n{o(t)}t are point clouds from the Lidar sensor, and the actions {a(t)}t are SE(3) poses of the ego\nvehicle. We first tokenize each observation o(t) into x(t) \u2208{0, \u00b7 \u00b7 \u00b7 , |V | \u22121}N, where N is the\nnumber of tokens in each observation, and V is the vocabulary defined by the learned codebook in\nVQVAE. We denote x(t) as the tokenized observation t. The learning objective is:\narg max\n\u03b8\nX\nt\nlog p\u03b8(x(t) | x(1), a(1), \u00b7 \u00b7 \u00b7 , x(t\u22121), a(t\u22121))\n(3)\nOur world model is a discrete diffusion model (Austin et al., 2021; Lezama et al., 2023) that is able to\nperform conditional generation given past observations and actions. We denote x(t)\nk as the tokenized\nobservation t under forward diffusion step k. k = 0 is the original data distribution, and the total\nnumber of steps K can be arbitrary at inference. We outline the inference process as follows: to\npredict an observation at timestep t+1, the world model first tokenizes past observations o(1) \u00b7 \u00b7 \u00b7 o(t)\ninto x(1) \u00b7 \u00b7 \u00b7 x(t), applies discrete diffusion for next frame prediction to decode the initially fully\nmasked x(t+1)\nK\ninto fully decoded x(t+1)\n0\n, and then passes x(t+1)\n0\ninto the decoder of the tokenizer\nto render the next observation o(t+1). For a visual overview of our method, see Figure 2.\n4.1\nTOKENIZE THE 3D WORLD\nWe propose a novel VQVAE-like (Van Den Oord et al., 2017) model to tokenize the 3D world\nrepresented by point clouds (Xiong et al., 2023). The model learns latent codes in Bird-Eye View\n(BEV) and is trained to reconstruct point clouds via differentiable depth rendering.\nThe encoder uses standard components from point-cloud based object detection literature: first,\naggregate point-wise features of each voxel with a PointNet (Qi et al., 2017); second, aggregate\nvoxel-wise features into BEV pillars (Lang et al., 2019); finally, apply a Swin Transformer backbone\n(Liu et al., 2021) to obtain a feature map that is 8x downsampled from the initial voxel size in BEV.\nThe output of the encoder, z = E(o), goes through a vector quantization layer to produce \u02c6z.\nThe novelty of our tokenizer lies in the decoder, which produce two branches of outputs after a few\nSwin Transformer blocks. The first branch uses an implicit representation (Mildenhall et al., 2021)\nso that we can query occupancy values at continuous coordinates. To query (x, y, z), we apply\nbilinear interpolation on a 3D neural feature grid (NFG) (Yang et al., 2023) outputted by the decoder\nto obtain a feature descriptor, which then goes through a multi-layer perceptron (MLP) and sigmoid\nto arrive at an occupancy value \u03b1 in [0, 1]. Given a ray r(h) = p + hd starting at point p and\ntraveling in direction d, the expected depth D can be calculated via differentiable depth rendering\non Nr sampled points {(xi, yi, zi)}Nr\ni=1 along the ray:\n\u03b1i = \u03c3(MLP(interp(NFG(\u02c6z), (xi, yi, zi))))\nwi = \u03b1i\ni\u22121\nY\nj=1\n(1 \u2212\u03b1j)\nD(r, \u02c6z) =\nNr\nX\ni=1\nwihi (4)\n4\nPublished as a conference paper at ICLR 2024\nAlgorithm 1 Training\n1: repeat\n2:\nx0 : {1, \u00b7 \u00b7 \u00b7 , |V |}N \u223cq(x0)\n3:\nu0 \u223cUniform(0, 1)\n4:\nRandomly mask \u2308\u03b3(u0)N\u2309tokens in x0\n5:\nu1 \u223cUniform(0, 1)\n6:\nRandomly noise (u1 \u00b7 \u03b7)% of remaining tokens\n7:\nxk \u2190masked-and-noised x0\n8:\narg max\u03b8 log p\u03b8(x0 | xk) with cross entropy\n9: until converged\nAlgorithm 2 Sampling\n1: xK = all mask tokens\n2: for k = K \u22121, . . . , 0 do\n3:\n\u02dcx0 \u223cp\u03b8(\u00b7 | xk+1)\n4:\nlk = log p\u03b8(\u02dcx0 | xk+1)+Gumbel(0, 1)\u00b7k/K\n5:\nOn non-mask indices of xk+1: lk \u2190+\u221e\n6:\nM = \u2308\u03b3(k/K)N\u2309\n7:\nxk \u2190\u02dcx0 on top-M indices of lk\n8: end for\n9: return x0\nFigure 3:\nOur improved discrete diffusion algorithm. Differences with MaskGIT (Chang et al.,\n2022) are highlighted in blue . \u03b3(u) = cos(u\u03c0/2) is the mask schedule. We set \u03b7 = 20 by default.\nThe second branch learns a coarse reconstruction of the point clouds by predicting whether a voxel\nhas points in its inputs. We denote this binary probability as v. During inference, this branch is used\nfor spatial skipping (Li et al., 2023) to speed up point sampling in rendering.\nThe loss function for the tokenizer is a combination of the vector quantization loss Lvq and the\nrendering loss Lrender. The vector quantization loss learns the codebook and regularizes the latents:\nLvq = \u03bb1\u2225sg[E(o)] \u2212\u02c6z\u22252\n2 + \u03bb2\u2225sg[\u02c6z] \u2212E(o)\u22252\n2. In the rendering loss, supervision is applied on\nboth branches: the depth rendering branch has an L1 loss on depth with an additional term that\nencourages wi to concentrate within \u03f5 of the surface (Yang et al., 2023); the spatial skipping branch\noptimizes binary cross entropy. The tokenizer is trained end-to-end to reconstruct the observation:\nLrender = Er\nh\n\u2225D(r, \u02c6z) \u2212Dgt\u22251 +\nX\ni\n1(|hi \u2212Dgt| > \u03f5)\u2225wi\u22252\ni\n+ BCE(v, vgt)\n(5)\nWith a pretrained tokenizer, both the inputs and the outputs of the world model are discrete tokens.\n4.2\nMASKGIT AS A DISCRETE DIFFUSION MODEL\nMasked Generative Image Transformer (MaskGIT) (Chang et al., 2022) has been shown to scale\nfor a variety of applications. Interestingly, discrete diffusion models such as D3PM (Austin et al.,\n2021) have not yet seen similar success, despite having a much more first-principled framework\nand toolbox. We observe that the key to recasting MaskGIT as a discrete diffusion model is the\nfollowing proposition in Campbell et al. (2022). It turns out that the parameterization introduced in\nEquation (1) allows a further lower bound on ELBO under data distribution q(x0) (also see A.4),\nEq(x0)[log p\u03b8(x0)] \u2265Eq(x0)[\u2212Lelbo(x0, \u03b8)] \u2265\nK\nX\nk=1\nEq(x0)q(xk|x0)[log p\u03b8(x0 | xk)] + C\n(6)\nWhich is almost the same loss as MaskGIT loss, except that: for the diffusion posterior q(xk|x0) to\nbe well-defined when xk \u0338= x0, uniform diffusion in non-masked locations is needed; and the loss is\napplied not just to masked locations. This implies that a few simple changes can turn MaskGIT into\nan absorbing-uniform discrete diffusion model. During training, after masking a random proportion\nof tokens in x0, we inject up to \u03b7% of uniform noise into the remaining tokens, and apply a cross\nentropy loss to reconstruct x0. \u03b7 is a fixed hyper-parameter. During sampling, besides parallel\ndecoding, we allow the model to iteratively denoise earlier sampled tokens. The differences with\nMaskGIT are highlighted in Algorithms 1 and 2. For conditional generation, classifier-free diffusion\nguidance can be applied by modifying the logits of p\u03b8 for sampling \u02dcx0 and calculating lk according\nto Equation (2). While resampling tokens has been known to help MaskGIT (Lezama et al., 2022;\n2023), our method only requires training a single model rather than two separate ones.\nWe now use our discrete diffusion algorithm to build a world model on top of observation tokens.\n4.3\nLEARNING A WORLD MODEL\nFor an autonomous agent, the environment can be viewed as a black box that receives an ac-\ntion and outputs the next observation. A world model is a learned generative model that can be\n5\nPublished as a conference paper at ICLR 2024\nAttention mask \nbetween frames\nQuery\nKey\nWorld Model \ninputs\nTraining targets\nCondition on the past, \npredict the future\nJoint modeling of \nthe past and the future\nModel each frame \nindividually\nTime\nTime\nTime\nTraining objectives\nQuery\nKey\nQuery\nKey\nGround Truth Token\nMask Token\nNoised Token (randomly sampled from codebook)\nFigure 4: The training objectives of our world model in Copilot4D. Other than future prediction,\nwe also train the model to jointly model the past and the future, and individually model each frame.\nJoint modeling ensures that the model can accurately predict the future even with imperfect past\nconditioning. Individual frame modeling is necessary for classifier-free diffusion guidance. During\ntraining, which objective to optimize is randomly sampled at each iteration.\nused in place of the environment. Let \u03c4 = (x(1), a(1), \u00b7 \u00b7 \u00b7 , x(T )). Given the context c(t\u22121) =\n(x(1)\n0 , a(1), \u00b7 \u00b7 \u00b7 , x(t\u22121)\n0\n, a(t\u22121)) as past agent history, a discrete diffusion world model p\u03b8 learns to\npredict the next observation x(t)\n0 , starting from fully masked x(t)\nK , going through K intermediate\nsteps x(t)\nK \u2192x(t)\nK\u22121 \u00b7 \u00b7 \u00b7 \u2192x(t)\n0\nduring the reverse process of diffusion. Using Equation (6),\nEq(\u03c4)\nh X\nt=1\nlog p\u03b8(x(t)\n0\n| c(t\u22121))\n|\n{z\n}\nAutoregressive future prediction\ni\n\u2265Eq(\u03c4)\nh X\nt=1\nX\nk=1\nEq(x(t)\nk |x(t)\n0 )[log p\u03b8(x(t)\n0\n| x(t)\nk , c(t\u22121))]\n|\n{z\n}\nDiscrete diffusion on each observation\n+C\ni\n(7)\nHowever, in the GPT-like formulation of autoregressive modeling, the model is always able to see\nall past ground-truth tokens for next frame prediction during training. In robotics, depending on the\ndiscretization of time, the world might only change incrementally within the immediate next frame;\nlearning to predict only the immediate next observation will not necessarily lead to long-horizon\nreasoning abilities even if the loss is optimized well. Therefore, training the world model should\ngo beyond next observation prediction and instead predict an entire segment of future observations\ngiven the past. Accordingly, we design the world model to be similar to a spatio-temporal version\nof BERT, with causal masking in the temporal dimension. Future prediction is done via masking,\ninfilling, and further denoising. The model is trained with a mixture of objectives (see Figure 4):\n1. 50% of the time, condition on the past, denoise the future.\n2. 40% of the time, denoise the past and the future jointly.\n3. 10% of the time, denoise each frame individually, regardless of past or future.\nThe first objective is about future prediction. The second objective also has a future prediction\ncomponent, but jointly models the future and the past, resulting in a harder pretraining task. The third\nobjective aims to learn an unconditional generative model, which is necessary for applying classifier-\nfree diffusion guidance during inference. By the word denoise, we are referring to Algorithm 1,\nwhere parts of the inptus are first masked and noised, and the model learns to reconstruct the original\ninputs with a cross-entropy loss. All three objectives can be viewed as maximizing the following:\nEq(\u03c4),k1,\u00b7\u00b7\u00b7 ,kT \u223cSampleObj(\u00b7)\nq(x(1)\nk1 |x(1)\n0\n),\u00b7\u00b7\u00b7q(x(T )\nkT |x(T )\n0\n)\n[log p\u03b8(\nx(1)\n0 , \u00b7 \u00b7 \u00b7 x(t\u22121)\n0\n,\n|\n{z\n}\nIgnored for Objective type 1\nx(t)\n0 , \u00b7 \u00b7 \u00b7 x(T )\n0\n| x(1)\nk1 , \u00b7 \u00b7 \u00b7 x(T )\nkT , a(1), \u00b7 \u00b7 \u00b7 a(T \u22121))]\nDuring inference, we still autoregressively predict one frame at a time. Each frame is sampled using\nAlgorithm 2 with classifier-free diffusion guidance (CFG) in Equation (2). At each timestep t, the\ncontext in diffusion guidance is c(t\u22121), the past observation and action history of the agent. See\nFigure 10 in the Appendix for an illustration of how CFG is used in our world model.\nNext, we outline how both training (with our mixture of objectives) and inference (with classifier-\nfree diffusion guidance) can be implemented with a spatio-temporal Transformer.\n6\nPublished as a conference paper at ICLR 2024\nFigure 5: Qualitative comparison against prior state-of-the-art method 4D Occupancy (4D-Occ)\non Argoverse2 Lidar dataset. Copilot4D achieves significantly better results, demonstrating greater\ncapabilities on novel-view synthesis of an environment as the ego vehicle moves, understanding the\nmotion of other vehicles in the scene, and modeling the Lidar pattern of ground points.\n4.4\nA SPATIO-TEMPORAL TRANSFORMER FOR WORLD MODELING\nThe architecture of our world model is a spatio-temporal Transformer that simply interleaves spatial\nattention and temporal attention. For spatial attention, we use Swin Transformer (Liu et al., 2021) on\neach individual frame. For temporal attention, we use GPT2 blocks (Radford et al., 2019) to attend\nover the same feature location across time. We use a U-Net (Ronneberger et al., 2015) structure\nthat combine three levels of feature with residual connections, and make predictions at the same\nresolution as the initial inputs. Actions, which in our case are the poses of the ego vehicle, are added\nto the beginning of each feature level corresponding to their observations, after being flattened and\ngoing through two linear layers with LayerNorm (Ba et al., 2016) in between.\nTemporal attention mask plays a crucial role in both training and inference. During training, when\noptimizing the first two types of objective, causal masking is applied to all temporal Transformer\nblocks; when optimizing the third type of objective to learn an unconditional generative model, the\ntemporal attention mask becomes an identity matrix such that each frame can only attend to itself.\nDuring inference, the model decodes and denoises one frame at a time; classifier-free diffusion\nguidance can be efficiently implemented by increasing temporal sequence length by 1, and setting\nthe attention mask to be a causal mask within the previous sequence length, and an identity mask\nfor the last frame, so that this added frame becomes unconditional generation (see Figure 10).\n5\nEXPERIMENTS\nIn this section, we aim to answer the following questions: (1) Can our proposed world modeling\napproach outperform previous state-of-the-art point cloud forecasting methods on large-scale self-\n7\nPublished as a conference paper at ICLR 2024\nTable 1: Results on NuScenes and KITTI Odometry datasets. Comparison against previous state-\nof-the-art methods: SPFNet (Weng et al., 2021), S2Net (Weng et al., 2022), ST3DCNN (Mersch\net al., 2022), and 4D Occupancy (4D-Occ) (Khurana et al., 2023). The color magenta means that a\nmetric is computed within the Region of Interest defined in Khurana et al. (2023): \u221270m to +70m\nin both x-axis and y-axis, \u22124.5m to +4.5m in z-axis. L1 Med means median L1 error within ROI;\nAbsRel Med means the median absolute relative L1 error percentage within ROI.\nNuScenes 1s\nChamfer\u2193\nL1 Med\u2193\nAbsRel Med\u2193\nL1 Mean\u2193\nAbsRel\u2193\nChamfer\u2193\nSPFNet\n2.24\n-\n-\n4.58\n34.87\n4.17\nS2Net\n1.70\n-\n-\n3.49\n28.38\n2.75\n4D-Occ\n1.41\n0.26\n4.02\n1.40\n10.37\n2.81\nCopilot4D\n0.36\n0.10\n1.30\n1.30\n8.58\n2.01\nNuScenes 3s\nSPFNet\n2.50\n-\n-\n5.11\n32.74\n4.14\nS2Net\n2.06\n-\n-\n4.78\n30.15\n3.47\n4D-Occ\n1.40\n0.43\n6.88\n1.71\n13.48\n4.31\nCopilot4D\n0.58\n0.14\n1.86\n1.51\n10.38\n2.47\nKITTI 1s\nST3DCNN\n4.11\n-\n-\n3.13\n26.94\n4.51\n4D-Occ\n0.51\n0.20\n2.52\n1.12\n9.09\n0.61\nCopilot4D\n0.18\n0.11\n1.32\n0.95\n8.59\n0.21\nKITTI 3s\nST3DCNN\n4.19\n-\n-\n3.25\n28.58\n4.83\n4D-Occ\n0.96\n0.32\n3.99\n1.45\n12.23\n1.50\nCopilot4D\n0.45\n0.17\n2.18\n1.27\n11.50\n0.67\nTable 2: Results on Argoverse 2 Lidar Dataset. We evaluate on evenly subsampled 4000 frames\non the Argoverse 2 Lidar validation set. All metrics are computed within the ROI.\n1s Prediction\nChamfer\u2193\nL1 Med\u2193\nAbsRel Med\u2193\nL1 Mean\u2193\nAbsRel Mean\u2193\n4D-Occ\n1.42\n0.24\n1.67\n2.04\n11.02\nCopilot4D\n0.26\n0.15\n0.94\n1.61\n8.75\n3s Prediction\n4D-Occ\n1.99\n0.42\n2.88\n2.62\n15.66\nCopilot4D\n0.55\n0.19\n1.26\n1.99\n11.86\ndriving datasets? (2) How important is classifier-free diffusion guidance in a discrete diffusion world\nmodel? (3) Does our improved discrete diffusion algorithm achieve better performance compared to\nMaskGIT, in terms of learning a world model?\nDatasets and Experiment Setting: We use NuScenes (Caesar et al., 2020), KITTI Odometry\n(Geiger et al., 2012), and Argoverse2 Lidar (Wilson et al., 2023), three commonly used large-scale\ndatasets for autonomous driving. Our evaluation protocol follows Khurana et al. (2023): on each\ndataset, we evaluate 1s prediction and 3s prediction by training two models. Each model is given\npast point cloud observations and future poses (which are the actions) of the ego vehicle. For KITTI\nand Argoverse2, each model receives 5 past frames and outputs 5 future frames (spanning either 1s\nor 3s). For NuScenes, the 2Hz dataset is used; as a result, 1s prediction takes in two past frames and\noutputs two future frames; 3s prediction takes in 6 past frames and outputs 6 future frames. While\nour world modeling approach is not limited to this experimental setting, we follow the same protocol\nto be able to directly compare against prior methods.\nMetrics: we follow the common metrics for point cloud forecasting (Khurana et al., 2023), which\ninclude Chamfer distance, L1 depth for raycasting (L1 Mean), and relative L1 error ratio (AbsRel).\nHowever, we notice an issue with the previously proposed metrics: while model predictions are\nmade only within the region of interest (ROI), the ground-truth point clouds are not cropped accord-\n8\nPublished as a conference paper at ICLR 2024\nTable 3: Our method for classifier-free diffusion guidance (CFG) significantly improves prediction\nresults and especially the Chamfer Distance metric.\nNuScenes 3s\nChamfer\u2193\nL1 Med\u2193\nAbsRel Med\u2193\nL1 Mean\u2193\nAbsRel Mean\u2193\nw = 0.0 (no CFG)\n1.40\n0.13\n1.81\n1.23\n8.34\nw = 1.0\n0.56 (60% \u2193)\n0.13\n1.78\n1.22\n9.32\nw = 2.0\n0.58\n0.14\n1.86\n1.27\n9.90\nTable 4: Our proposed discrete diffusion algorithm significantly improves upon previous masked\nmodeling method MaskGIT (Chang et al., 2022). Both models use only 10 sampling steps per frame\nof prediction (128 \u00d7 128 tokens), applied with classifier-free diffusion guidance w = 2.0.\nNuScenes 3s\nChamfer\u2193\nL1 Med\u2193\nAbsRel Med\u2193\nL1 Mean\u2193\nAbsRel Mean\u2193\nMaskGIT\n0.82\n0.16\n2.09\n1.41\n11.66\nOurs\n0.58 (29% \u2193)\n0.14\n1.86\n1.27\n9.90\ning to the ROI, resulting in artifically high error metrics simply because the ROI might not cover\nthe full point cloud range. Consequently, we report metrics computed within the ROI in magenta,\nwhich better reflects the performance of a model. We also report the median of L1 depth error (L1\nMed), since the median is more robust to outliers than the mean. Following previous evaluation\nprotocols, the ROI is defined to be \u221270m to +70m in both x-axis and y-axis, \u22124.5m to +4.5m in\nz-axis around the ego vehicle.\nBenchmark against state-of-the-art: Table 1 and 2 show quantitative comparisons with state-\nof-the-art unsupervised point cloud forecasting methods on three datasets. Our baselines include\nSPFNet (Weng et al., 2021), S2Net (Weng et al., 2022), ST3DCNN (Mersch et al., 2022), and 4D\nOccupancy (Khurana et al., 2023). Our method Copilot4D is able to outperform prior methods by\na significant margin across all three datasets. In particular, for 1s prediction, we are able to see a\n65% \u221275% reduction in Chamfer Distance compared to prior SOTA across all three datasets; for\n3s prediction, we are able to see more than 50% reduction in Chamfer. We also present a qualitative\ncomparison with previous SOTA (4D Occupancy) in Figure 5. Copilot4D learns qualitatively better\nfuture predictions, demonstrating an impressive ability to synthesize novel views on the background\nas the ego vehicle moves and forecast the motion of other vehicles.\nClassifier-free diffusion guidance (CFG) is important: in Table 3, we show that CFG reduces\nChamfer by as much as 60% in an ablation on NuScenes 3s prediction. The result is not surprising\nconsidering that CFG has become a standard tool in text-to-image generation; here we show that\nusing past agent history (c(t\u22121) in Section 4.3) as CFG conditioning improves world modeling.\nIntuitively, CFG amplifies the contribution of the conditioned information in making predictions.\nImprovement upon MaskGIT: Our proposed simple changes to MaskGIT train the model to do\na harder denoising task in Algorithm 1 and thus allow the inference procedure in Algorithm 2 to\niteratively revise chosen tokens. Those changes notably improve MaskGIT on an ablation run of\nNuScenes 3s prediction task, shown in Table 4. In our case, each frame has 128 \u00d7 128 tokens with\n10 diffusion steps; on average each diffusion step decodes 1600 new tokens in parallel. Being able\nto denoise and resample already decoded tokens reduces Chamfer distance of 3s prediction by 29%.\n6\nCONCLUSION\nLearning unsupervised world models is a promising paradigm where GPT-like pretraining for\nrobotics can potentially scale.\nIn this work, we first identify two practical bottlenecks of this\nparadigm: simplifying the complex observation space, and building a scalable generative model\nfor spatio-temporal data. We then propose Copilot4D, which combines observation tokenization,\ndiscrete diffusion, and the Transformer architecture as a new approach for building unsupervised\nworld models, achieving state-of-the-art results for the point cloud forecasting task in autonomous\ndriving. One particularly exciting aspect of Copilot4D is that it is broadly applicable to many do-\nmains. We hope that future work will combine our world modeling approach with model-based\nreinforcement learning to improve the decision making capabilities of autonomous agents.\n9\nPublished as a conference paper at ICLR 2024\n7\nACKNOWLEDGEMENT\nWe thank Chris Zhang, Anqi Joyce Yang, Thomas Gilles, and many others on the Waabi team for\nhelpful discussions and valuable support throughout the project.\nREFERENCES\nIlge Akkaya, Marcin Andrychowicz, Maciek Chociej, Mateusz Litwin, Bob McGrew, Arthur Petron,\nAlex Paino, Matthias Plappert, Glenn Powell, Raphael Ribas, et al. Solving rubik\u2019s cube with a\nrobot hand. arXiv preprint arXiv:1910.07113, 2019.\nJacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured\ndenoising diffusion models in discrete state-spaces. Advances in Neural Information Processing\nSystems, 34:17981\u201317993, 2021.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\nYoshua Bengio, Nicholas L\u00b4eonard, and Aaron Courville.\nEstimating or propagating gradients\nthrough stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush\nKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for\nautonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11621\u201311631, 2020.\nAndrew Campbell, Joe Benton, Valentin De Bortoli, Thomas Rainforth, George Deligiannidis, and\nArnaud Doucet. A continuous time framework for discrete denoising models. Advances in Neural\nInformation Processing Systems, 35:28266\u201328279, 2022.\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T Freeman. Maskgit: Masked generative\nimage transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 11315\u201311325, 2022.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nPrafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever.\nJukebox: A generative model for music. arXiv preprint arXiv:2005.00341, 2020.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An\nimage is worth 16x16 words: Transformers for image recognition at scale.\narXiv preprint\narXiv:2010.11929, 2020.\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti\nvision benchmark suite. In 2012 IEEE conference on computer vision and pattern recognition,\npp. 3354\u20133361. IEEE, 2012.\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and\nBaining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10696\u201310706, 2022.\n10\nPublished as a conference paper at ICLR 2024\nAgrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n, and Li Fei-Fei.\nMaskvit: Masked visual pre-training for video prediction.\narXiv preprint arXiv:2206.11894,\n2022.\nDavid Ha and J\u00a8urgen Schmidhuber. Recurrent world models facilitate policy evolution. Advances\nin neural information processing systems, 31, 2018.\nDanijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with dis-\ncrete world models. arXiv preprint arXiv:2010.02193, 2020.\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. Mastering diverse domains\nthrough world models. arXiv preprint arXiv:2301.04104, 2023.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp.\n770\u2013778, 2016.\nDan Hendrycks and Kevin Gimpel.\nGaussian error linear units (gelus).\narXiv preprint\narXiv:1606.08415, 2016.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nneural information processing systems, 33:6840\u20136851, 2020.\nAnthony Hu, Zak Murez, Nikhil Mohan, Sof\u00b4\u0131a Dudas, Jeffrey Hawke, Vijay Badrinarayanan,\nRoberto Cipolla, and Alex Kendall. Fiery: Future instance prediction in bird\u2019s-eye view from\nsurround monocular cameras.\nIn Proceedings of the IEEE/CVF International Conference on\nComputer Vision, pp. 15273\u201315282, 2021.\nMinyoung Huh, Brian Cheung, Pulkit Agrawal, and Phillip Isola. Straightening out the straight-\nthrough estimator: Overcoming optimization challenges in vector quantized networks.\narXiv\npreprint arXiv:2305.08842, 2023.\nLukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej Osinski, Roy H Campbell, Konrad\nCzechowski, Dumitru Erhan, Chelsea Finn, Piotr Kozakowski, Sergey Levine, et al. Model-based\nreinforcement learning for atari. arXiv preprint arXiv:1903.00374, 2019.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\nTarasha Khurana, Peiyun Hu, David Held, and Deva Ramanan. Point cloud forecasting as a proxy\nfor 4d occupancy forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 1116\u20131124, 2023.\nDiederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. Ad-\nvances in neural information processing systems, 34:21696\u201321707, 2021.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nAlex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Point-\npillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, pp. 12697\u201312705, 2019.\nJos\u00b4e Lezama, Huiwen Chang, Lu Jiang, and Irfan Essa. Improved masked image generation with\ntoken-critic. In European Conference on Computer Vision, pp. 70\u201386. Springer, 2022.\nJose Lezama, Tim Salimans, Lu Jiang, Huiwen Chang, Jonathan Ho, and Irfan Essa.\nDiscrete\npredictor-corrector diffusion models for image synthesis. In International Conference on Learn-\ning Representations (ICLR), 2023.\n11\nPublished as a conference paper at ICLR 2024\nRuilong Li, Hang Gao, Matthew Tancik, and Angjoo Kanazawa. Nerfacc: Efficient sampling accel-\nerates nerfs. arXiv preprint arXiv:2305.04966, 2023.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo.\nSwin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the\nIEEE/CVF international conference on computer vision, pp. 10012\u201310022, 2021.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nWenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection,\ntracking and motion forecasting with a single convolutional net. In Proceedings of the IEEE\nconference on Computer Vision and Pattern Recognition, pp. 3569\u20133577, 2018.\nChris J Maddison, Andriy Mnih, and Yee Whye Teh. The concrete distribution: A continuous\nrelaxation of discrete random variables. arXiv preprint arXiv:1611.00712, 2016.\nBenedikt Mersch, Xieyuanli Chen, Jens Behley, and Cyrill Stachniss. Self-supervised point cloud\nprediction using 3d spatio-temporal convolutional networks. In Conference on Robot Learning,\npp. 1444\u20131454. PMLR, 2022.\nVincent Micheli, Eloi Alonso, and Franc\u00b8ois Fleuret. Transformers are sample efficient world models.\narXiv preprint arXiv:2209.00588, 2022.\nBen Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and\nRen Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications\nof the ACM, 65(1):99\u2013106, 2021.\nAnusha Nagabandi, Kurt Konolige, Sergey Levine, and Vikash Kumar. Deep dynamics models\nfor learning dexterous manipulation. In Conference on Robot Learning, pp. 1101\u20131112. PMLR,\n2020.\nOfir Press and Lior Wolf. Using the output embedding to improve language models. arXiv preprint\narXiv:1608.05859, 2016.\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets\nfor 3d classification and segmentation. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pp. 652\u2013660, 2017.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language under-\nstanding by generative pre-training. OpenAI Technical Report, 2018.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nKonstantinos Rematas, Andrew Liu, Pratul P Srinivasan, Jonathan T Barron, Andrea Tagliasacchi,\nThomas Funkhouser, and Vittorio Ferrari. Urban radiance fields. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 12932\u201312942, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomed-\nical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013\nMICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceed-\nings, Part III 18, pp. 234\u2013241. Springer, 2015.\nAbbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Per-\nceive, predict, and plan: Safe motion planning through interpretable semantic representations. In\nComputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XXIII 16, pp. 414\u2013430. Springer, 2020.\n12\nPublished as a conference paper at ICLR 2024\nJean-Jacques E Slotine, Weiping Li, et al. Applied nonlinear control, volume 199. Prentice hall\nEnglewood Cliffs, NJ, 1991.\nShaden Smith, Mostofa Patwary, Brandon Norick, Patrick LeGresley, Samyam Rajbhandari, Jared\nCasper, Zhun Liu, Shrimai Prabhumoye, George Zerveas, Vijay Korthikanti, et al. Using deep-\nspeed and megatron to train megatron-turing nlg 530b, a large-scale generative language model.\narXiv preprint arXiv:2201.11990, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International conference on machine learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nRichard S Sutton. Dyna, an integrated architecture for learning, planning, and reacting. ACM Sigart\nBulletin, 2(4):160\u2013163, 1991.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nAaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in\nneural information processing systems, 30, 2017.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017.\nXinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani, and Nicholas Rhinehart. Inverting the\npose forecasting pipeline with spf2: Sequential pointcloud forecasting for sequential pose fore-\ncasting. In Conference on robot learning, pp. 11\u201320. PMLR, 2021.\nXinshuo Weng, Junyu Nan, Kuan-Hui Lee, Rowan McAllister, Adrien Gaidon, Nicholas Rhinehart,\nand Kris M Kitani. S2net: Stochastic sequential pointcloud forecasting. In European Conference\non Computer Vision, pp. 549\u2013564. Springer, 2022.\nBenjamin Wilson, William Qi, Tanmay Agarwal, John Lambert, Jagjeet Singh, Siddhesh Khan-\ndelwal, Bowen Pan, Ratnesh Kumar, Andrew Hartnett, Jhony Kaesemodel Pontes, et al. Ar-\ngoverse 2: Next generation datasets for self-driving perception and forecasting. arXiv preprint\narXiv:2301.00493, 2023.\nYuwen Xiong, Wei-Chiu Ma, Jingkang Wang, and Raquel Urtasun. Learning compact represen-\ntations for lidar completion and generation. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 1074\u20131083, 2023.\nBin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-time 3d object detection from point clouds.\nIn Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pp. 7652\u2013\n7660, 2018.\nZe Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam, Wei-Chiu Ma, Anqi Joyce Yang, and\nRaquel Urtasun. Unisim: A neural closed-loop sensor simulator. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 1389\u20131399, 2023.\nLijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han Zhang, Huiwen Chang, Alexander G\nHauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video\ntransformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-\nnition, pp. 10459\u201310469, 2023.\nYin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3d object detection.\nIn Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4490\u2013\n4499, 2018.\n13\nPublished as a conference paper at ICLR 2024\nA\nAPPENDIX\nA.1\nADDITIONAL QUANTITATIVE RESULTS\nTable 5: Zero-shot transfer performance across datasets: training on Argoverse 2 (AV2) Lidar and\nevaluating on KITTI Odometry. Compared to 4d Occupancy (4d-Occ) (Khurana et al., 2023), our\nmethod Copilot4D achieves significantly better dataset transfer results.\n[AV2 \u2192KITTI] 1s Prediction\nChamfer\u2193\nL1 Mean\u2193\nAbsRel\u2193\nChamfer\u2193\n4D-Occ\n2.52\n1.71\n14.85\n3.18\nCopilot4D\n0.36\n1.16\n11.1\n0.44\n[AV2 \u2192KITTI] 3s Prediction\n4D-Occ\n4.83\n2.52\n23.87\n5.79\nCopilot4D\n1.12\n1.91\n19.0\n1.65\nZero-shot transfer performance across datasets: Table 5 follows prior protocols of training on\nArgoverse2 and testing on KITTI Odometry, and shows that: Copilot4D achieves more than 4\u00d7\nsmaller Chamfer distance compared to 4D Occupancy, on both 1s and 3s prediction. Our results\nindicate that tokenization, discrete diffusion, and a spatio-temporal Transformer form a powerful\ncombination that can achieve a greater degree of cross-dataset transfer.\nTable 6: Results on the test set of Argoverse 2 Lidar dataset. Besides the results on validation set\npresented in the main paper, we also evaluate on evenly subsampled 4000 frames on the Argoverse\n2 Lidar test set. The results are very similar; we present the test-set results here for completeness.\nAll metrics are computed within the ROI.\n1s Prediction\nChamfer\u2193\nL1 Med\u2193\nAbsRel Med\u2193\nL1 Mean\u2193\nAbsRel Mean\u2193\n4D-Occ\n1.51\n0.25\n1.66\n2.07\n11.21\nCopilot4D\n0.25\n0.15\n0.96\n1.64\n9.02\n3s Prediction\n4D-Occ\n2.12\n0.45\n3.05\n2.69\n16.48\nCopilot4D\n0.61\n0.20\n1.29\n2.08\n12.49\nAblation studies on the tokenizer:\n\u2022 Table 7 provides an ablation on the effect of spatial skipping (Li et al., 2023), where we\ntrain another tokenizer from scratch without the coarse reconstruction branch or the spatial\nskipping process; this tokenizer is only supervised with the depth rendering loss. The table\nshows that spatial skipping improves point cloud reconstructions. We illustrate the details\nof the spatial skipping process in Figure 8.\n\u2022 We also show that differentiable depth rendering using implicit representation is crucial. In\nFigure 9, we provide a qualitative comparison between the VQVAE in UltraLiDAR (Xiong\net al., 2023) and our proposed tokenizer. The UltraLiDAR model only predicts whether a\nvoxel has points present, and is similar to our coarse reconstruction branch. The results\nshow that UltraLiDAR is unable to reconstruct fine-grained geometry due to the limited\nresolution of voxel predictions, whereas our tokenizer is able to produce high-fidelity point\nclouds that recover the details in the input point clouds.\nA.2\nMODEL DETAILS\nBoth the tokenizer and the world model are quite lightweight. The tokenizer is a 13-Million parame-\nter model; the world model is a 39-Million parameter model. To put the parameter counts in context,\nthe tokenizer has fewer parameters than a ResNet-34 (He et al., 2016) (21.8 M parameters), and the\ncurrent world model has fewer parameter count than a ResNet-101 (44.5M parameters). Achieving\nour results on such a small model scale is another highlight of our algorithm.\n14\nPublished as a conference paper at ICLR 2024\nObservation\n1024\n1024\n64\n3D Voxel Feature Volume\nPointNet-like Voxel Encoder\n3D Points in a voxel\nVoxel center\n\u2026\nLinear\nLN\nReLU\nLinear\ndim=16\ndim=16\nMLP\nMLP\nMLP\n\u2026\nSum\nDi\ufb00erences\nLN\nShape:\n(1024, 1024, 64, 16)\nBEV Pillar Pooling\n\u2026\nz-axis embed\nconcat\nMLP (dim=64)\nMLP (dim=64)\n\u2026\nSum\nLN\n2D BEV Features\nVector Quantization\n4x downsampled\nViT \npositional \nembed\n(1024, 1024, 64)\nWindow size=8\nNum of heads=8\n(256, 256, 128)\n(128, 128, 256)\nWindow size=8\nNum of heads=8\nLN\nGELU+ \nLinear\nVQ\nCodebook Lookup\n(1024, 1024)\n(128, 128, 1024)\n2 Swin Blocks\n6 Swin Blocks\nPatch Embed\nPatchify\nLinear\nLN\n8x downsampled\nPatch Merging\nPatchify\nLN\nLinear\nShapes\n8x downsampled\n4x downsampled\nLinear\n6 Swin Blocks\nPatch Upsample\nDeConv\nLN\nLinear\n2 Swin Blocks\n512\n512\n64\n 3D Neural Feature Grid\nLN\nLinear\nBilinear \ninterpolation\nLinear\nReLU\nLinear\ndim=16\nMLP\nSigmoid\nQuery continuous \ncoordinates along the ray\n(256, 256, 128)\n(128, 128, 256)\nShapes\n(512, 512, 64, 16)\nLN\nLinear\n1024\n1024\n64\n(1024, 1024, 64)\n3D Binary \nVoxel Logits\nRender\nSpatial \nSkipping\nReconstruction\nRay\nMax \nPooling\nSigmoid\n(128, 128, 64)\nThreshold\nRay\nSensor origin\nRendering\nSpatial \nSkipping\nFigure 6:\nDetailed architecture of our point cloud tokenizer, which combines PointNet, Bird-\nEye View (BEV) representation, Neural Feature Grid (NFG), and differentiable depth rendering.\n2 Swin\nBlocks\nSpatio-Temporal Block\n2 Spatio-Temporal Blocks\nspatial \nembed\ntemporal \nembed\n1 Temporal \nBlock\nDim=256\nPatch Merging\n2 Spatio-Temporal Blocks\nDim=384\nPatch Merging\n1 Spatio-Temporal Block\nDim=512\nLevel Merging\n1 Spatio-Temporal Block\nLevel Merging\n2 Spatio-Temporal Blocks\nActions\nLogits\nLogits\nLogits\nSpatial Resolutions\n(32, 32)\n(64, 64)\n(128, 128)\nBEV Tokens\nLogit Shapes: (128, 128, 1024)\n\u2026\n\u2026\n\u2026\nFigure 7: Detailed architecture of our U-Net based Transformer world model, which interleaves\nspatial and temporal Transformer blocks and applies multiple levels of spatial feature resolutions.\n15\nPublished as a conference paper at ICLR 2024\nVoxels that intersect \nwith query ray\nQuery Ray\nGround-truth point \non the query ray\nOthers points  present\nNon-empty voxels\nSampled points along \nthe ray to query\nMax-pooled non-empty voxels\nMax-pool the binary mask of \nwhether voxels are non-empty\nFind all non-empty \nvoxels\nSpatial Skipping in \nPoint Cloud Tokenizer\nSample points inside max-pooled \nnon-empty voxels along the ray\nGround-truth depth\nTraining:\nSampled points along \nthe ray to query\nMax-pool the binary mask\n(to increase recall)\nPredict non-emptiness \nof voxels\nSample points inside max-pooled \nnon-empty voxels along the ray\nInference:\nDepth \nrendering\nDepth \nrendering\nFigure 8:\nIllustration of the spatial skipping process. We find that spatial skipping (Li et al.,\n2023) not only speeds up tokenizer inference but also results in slightly better results. Rather than\nuniformly sampling points along the ray for depth rendering, we sample points only within the\nestimated non-empty regions along the ray. Specifically, we predict a binary mask indicating which\nvoxels are non-empty, apply max pooling on this binary mask to increase recall of non-empty voxels,\nand sample (ray) points only within the max-pooled non-empty voxels. During training, the ground-\ntruth binary mask of non-emptiness can be computed from original point clouds; this mask is also\nused to supervise the coarse reconstruction branch. During inference, we threshold the prediction\nfrom the coarse reconstruction branch to produce the mask.\nNuScenes [Reconstruction]\nChamfer (within ROI)\u2193\nL1 Median\u2193\nL1 Mean\u2193\nChamfer\u2193\nNo spatial skipping\n0.148\n0.044\n1.42\n1.66\nWith spatial skipping\n0.082\n0.044\n0.82\n1.64\nTable 7: Ablation study on spatial skipping in the tokenizer. Here we train another tokenizer\nfrom scratch without the coarse reconstruction branch or the spatial skipping process. Our results\nshow that spatial skipping leads to improved reconstructions, which is likely due to the fact that it\nallows the sampling of points along the rays to focus on the non-empty regions.\nA.2.1\nTOKENIZER\nThe tokenizer follows a VQVAE (Van Den Oord et al., 2017) structure to encode point clouds into\nBird-Eye View (BEV) tokens and reconstruct input point clouds via differentiable depth rendering.\nTokenizer encoder\nThe initial layer in the encoder is a voxel-wise PointNet (Qi et al., 2017)\nsimilar to VoxelNet (Zhou & Tuzel, 2018) that encodes the distance of each point to its corre-\nsponding voxel center, with one modification to PointNet: while the initial PointNet uses max\npooling as the permutation-invariant aggregation function, we use a sum operation + Layer-\nNorm (Ba et al., 2016), which is also a permutation-invariant function. We use a voxel size of\n15.625cm \u00d7 15.625cm \u00d7 14.0625cm in the x, y, z dimensions, following an input voxel size similar\nto the one used in Xiong et al. (2023), since it was noted that voxel sizes could matter a lot. We\nmodel the 3D world in the [\u221280m, 80m] \u00d7 [\u221280m, 80m] \u00d7 [\u22124.5m, 4.5m] region around the ego\nvehicle; after initial PointNet (with feature dimension 64), we obtain a 3D feature volume of tensor\nshape 1024 \u00d7 1024 \u00d7 64 \u00d7 64. Following the 3D object detection literature, we pool the 3D feature\nvolume into a 2D Bird-Eye View (BEV) representation, using our aggregation function (sum oper-\nation + LayerNorm) on the z-axis, after going through another Linear layer and adding a learnable\nembedding based on the z-axis of a voxel.\nThe encoder backbone is a Swin Transformer (Liu et al., 2021). We add ViT-style (Dosovitskiy\net al., 2020) absolute positional encodings of spatial coordinates to the beginning of the backbone.\n16\nPublished as a conference paper at ICLR 2024\nUltraLiDAR \nReconstruction\nOur Tokenizer \nReconstruction\nInput \nPoint Clouds\nNuScenes\nKITTI\nArgoverse\nFigure 9: Qualitative comparison between Copilot4D tokenizer and UltraLiDAR (Xiong et al.,\n2023). The UltraLiDAR model also uses a VQ-VAE for point clouds, but it only predicts whether\na voxel has points present or not, and is unable to reconstruct more fine-grained geometry. Our\ntokenizer overcomes this challenge via a combination of implicit representation and differentiable\ndepth rendering, leading to significantly improved reconstructions and qualitatively different results.\nThe initial patch size is 4 for the first two Swin layers (with the feature dimension being 128, and\nthe number of attention heads being 8), leading to a feature map shape of 256 \u00d7 256 \u00d7 128. We\nthen downsample the feature map and increase the patch size from 4 to 8 via a Patch Merging layer,\nand apply 6 Swin Transformer layers (with the feature dimension being 256, and the number of\nattention heads being 16), leading to a feature map shape of 128 \u00d7 128 \u00d7 256. The resolution of the\nencoder output is 128 \u00d7 128, which is 8\u00d7 downsampled from the initial voxel size, meaning that\neach feature map grid is in charge of a 1.25m \u00d7 1.25m region in BEV. We find it important to place\na LayerNorm (followed by GELU activation (Hendrycks & Gimpel, 2016) and a Linear layer) on\nthe encoder output before vector quantization, which has also been observed in Huh et al. (2023).\nFollowing Xiong et al. (2023), we increase the feature dimension to 1024 with a linear layer before\nvector quantization; we also find this to be important for good reconstruction.\nVector quantization layer\nCodebook collapse, meaning that only a few codes are used, is known\nto be a common issue in VQVAE training. We empirically find that the random restart strategy\nproposed in Dhariwal et al. (2020) is insufficient for avoiding codebook collapse in our case. We\ninstead resort to the K-Means clustering strategy in Xiong et al. (2023). More specifically, we set up\na memory bank to store the most recent encoder outputs before vector quantization; the size of the\nmemory bank is 10 times the size of the codebook. We define a code to be a dead code if it has not\nbeen used for 256 iterations. If more than 3% of the entire codebook have become dead codes, then\nwe run K-Means clustering on the memory bank to re-intialize the entire codebook. However, each\ncodebook must go through at least 200 iterations before it can be re-initialized.\nWe use the straight-through gradient estimator (Bengio et al., 2013), as done in the original VQVAE.\nIn the vector quantization loss Lvq = \u03bb1\u2225sg[E(o)] \u2212\u02c6z\u22252\n2 + \u03bb2\u2225sg[\u02c6z] \u2212E(o)\u22252\n2, we set \u03bb1 = 0.25\nand \u03bb2 = 1.0; the intuition is that the codebook should change more slowly than the features.\nTokenizer decoder\nThe decoder backbone is also a Swin Transformer, symmetrical to the en-\ncoder backbone. The Patch Merging layer for downsampling in the encoder corresponds to a Patch\nUpsample layer for upsampling in the decoder: first, we use a linear layer to upsample the feature\nmap (similar to a deconvolution layer); then, apply LayerNorm on each upsampled feature; finally,\nuse another Linear layer to reduce the feature dimension accordingly. The output of the decoder\nbackbone is a 256 \u00d7 256 \u00d7 128 tensor in 2D BEV, which is then fed into two separate branches.\n17\nPublished as a conference paper at ICLR 2024\nAs mentioned in the main paper, the first branch is a 3D neural feature grid (NFG) that supports\nbilinear interpolation for querying continuous coordinates. The NFG branch uses a LayerNorm and\na Linear(128, 22\u00d764\u00d716) layer to get a 3D feature volume of tensor shape 512\u00d7512\u00d764\u00d716, with\neach voxel represented by a 16-dimensional vector. Once this 3D feature volume has been computed,\neach query (xi, yi, zi) along different rays only requires bilinear interpolation on this feature volume\nto obtain an initial 16-dimensional vector, which can then be passed into a lightweight two-layer\nMLP (we set its hidden dimension to be 32, with ReLU activation) to produce an occupancy value\nwith a sigmoid gate at the end. This occupancy value can then be used for differentiable depth\nrendering in Equation (4). An L1 loss is applied to supervise the rendered depth, with an additional\nterm that encourages the learned sample weight distribution wi to concentrate within \u03f5 meters of the\nsurface, as outlined in Equation (5). We set the margin \u03f5 = 0.4.\nThe second branch is a coarse reconstruction branch that is used for spatial skipping during infer-\nence. Given the 2D BEV feature map of tensor shape 256 \u00d7 256 \u00d7 128 from the decoder backbone,\nwe apply a LayerNorm on each feature, followed by a Linear(128, 42 \u00d7 64) layer, to produce a 3D\nvolume of binary classification logits 1024 \u00d7 1024 \u00d7 64, which is trained to estimate whether each\nvoxel has points present in the input observation. The bias of the final logit is initialized to be \u22125.0,\nsince most voxels are empty in point cloud observations.\nAt inference, given discrete BEV tokens and query rays, the tokenizer decoder takes in the BEV\ntokens to produce the 3D NFG and the coarse binary voxel predictions from the two branches. At\nfirst, the spatial skipping branch can provide a coarse estimate of where to sample the points along\nthe rays. This is achieved by adding Logistic noise to the binary logits (Maddison et al., 2016) and\nthen thresholding the binary probabilities. We then apply max pooling on this binary 3D volume\nto produce a much coarser estimate of binary voxel predictions and consequently increase recall\n(in our case, we chose to use a max pooling factor of 8 in Bird-Eye View). For a query ray, we\nsearch among all intersecting coarse voxels and find all of the coarse voxels estimated to have points\npresent. Subsequently, following standard spatial skipping (Li et al., 2023), for each ray, we only\nsample points within those coarse voxel grids to query the 3D NFG, leading to much more efficient\nsampling. The final point clouds are obtained via depth rendering on the NFG branch.\nA.2.2\nWORLD MODEL\nThe world model follows a U-Net (Ronneberger et al., 2015) structure with three feature levels\ncorresponding to 1\u00d7, 2\u00d7, and 4\u00d7 downsampled resolutions. The inputs are 128 \u00d7 128 tokens; the\noutputs are 128 \u00d7 128 \u00d7 1024-dimensional logits where 1024 is the vocabulary size of our VQVAE\ncodebook. Adopting the common practice of language models, we use weight tying (Press & Wolf,\n2016) between the embedding layer and the final softmax layer. We use Swin Transformer (Liu\net al., 2021) for all spatial attention layers, and GPT-2 blocks (Radford et al., 2019) for all temporal\nattention layers. Spatial attention and temporal attention are interleaved in the following manner:\nevery 2 spatial blocks are followed by 1 temporal block. The temporal blocks apply attention on\nsame feature map location across frames. The feature dimensions for the three feature levels are\n(256, 384, 512), with the number of attention heads being (8, 12, 16), meaning that we use a fixed\n32-dimension per head across all levels.\nWe design our network to be similar to the GPT-2 Transformer, in the sense that the overall\nstructure of every feature level should be: sum of all residuals in Transformer blocks \u2192one final\nLayerNorm \u2192one final Linear layer. The benefit of such a design is that, since the structure of\nour neural net becomes similar to GPT-2, we can directly apply the initialization and optimization\nstrategies found to work well for language models. To make this design compatible with the U-\nNet structure (which first transitions down and then transition up to make dense predictions), how\nto merge information when transitioning up across feature levels becomes important. We use the\nfollowing architecture:\n\u2022 The first feature level transitioning down has (2 spatial \u21921 temporal \u21922 spatial \u21921\ntemporal) blocks, with a window size 8 in spatial blocks. Following Swin Transformer, the\nPatch Merging layer is used for downsampling.\n\u2022 The second feature level transitioning down also has (2 spatial \u21921 temporal \u21922 spatial\n\u21921 temporal) blocks, with a window size 8 in spatial blocks.\n18\nPublished as a conference paper at ICLR 2024\n\u2022 The third feature level transitioning down has (2 spatial \u21921 temporal) blocks, with a\nwindow size 16 in spatial blocks.\n\u2022 The network then transitions up to the second feature level using an upsampling layer\nbefore adding a residual connection. The goal is to upsample the higher-level feature map\nand merge it with the lower-level feature map. We name such a layer Level Merging, which\nborrows designs from the Patch Merging layer: first we use a linear layer to output the 2\u00d7\nupsampled feature map (similar to a deconvolution layer), concatenate with the lower-level\nfeature map, applies LayerNorm on every feature, and uses a linear projection to reduce the\nfeature dimension. A residual connection is then applied.\n\u2022 Back to the second feature level transitioning up, the feature map goes through (2 spatial \u2192\n1 temporal) blocks, followed by another Level Merging layer to return to the first feature\nlevel. Predictions are made at the first feature level.\n\u2022 Back to the first feature level, the feature map goes through additional (2 spatial \u21921\ntemporal \u21922 spatial \u21921 temporal) blocks.\n\u2022 We use a final LayerNorm followed by a weight matrix (under weight tying with the initial\nembedding layer) to output the logits.\nAll our Transformer layers are pre-norm layers, meaning that LayerNorm (Ba et al., 2016) is moved\nto the input of each sub-block, as done in both GPT-2 and Swin Transformer. In a pre-norm Trans-\nformer, the main branch of the network becomes just a sum of the residuals since LayerNorm is only\nplaced inside each residual. Therefore, any information we want to condition the network on can be\nsimply added at the very beginning of the network, and will be processed by all following residual\nmodules. The following inputs are added to the beginning of the Transformer world model:\n\u2022 The embeddings of given observation tokens (which are discrete indices). The weight of\nthis embedding layer is also used for the final softmax layer. On the input side, masking\nin done via an additional learnable token, as done in BERT. After the embedding layer, we\nadditonally apply Linear \u2192LayerNorm \u2192Linear.\n\u2022 The ego vehicle poses, which are the actions of the ego vehicle. Those 4 \u00d7 4 matrices are\nflattened into a 16-dimensional vector, which then goes through Linear \u2192LayerNorm \u2192\nLinear, and added to all feature map locations of corresponding temporal frames;\n\u2022 ViT-style (Dosovitskiy et al., 2020) absolute positional encodings of spatial coordinates,\nwhich are the same across all temporal frames;\n\u2022 Learnable temporal positional encodings, broadcast to all feature map locations of corre-\nsponding temporal frames;\nOther than the spatio-temporal positional encodings, any information we want to condition the neu-\nral net on should first go through Linear \u2192LayerNorm \u2192Linear; we have found that applying\nLayerNorm on the inputs before any Transformer layers is important for learning stability.\nWe remove the bias parameter in all Linear layers (Touvron et al., 2023), except for the Linear layers\noutputting query, key, and value during attention in Swin Transformer blocks.\nA.3\nTRAINING TRANSFORMERS\nInitialization\nTransformer initialization has long been known to be important for successful op-\ntimization. We adopt the initialization and optimization strategies found to work well for language\nmodels. Following MT-NLG (Smith et al., 2022), all weights are initialized using fan-in initializa-\ntion with a normal distribution of 0 mean and a standard deviation of\np\n1/(3H), where H is the\ninput feature dimension of each layer. We note that this initialization strategy is the closest to the\nGPT-1 (Radford et al., 2018) initialization scheme. In addition, we use residual scaling at initializa-\ntion as done in GPT-2: on each feature level, we count the number of residual connections L (which\nis the number of Transformer blocks times 2), and rescale the weight matrix of each linear layer\nbefore the residual connection by\np\n1/L.\nOptimization\nThe learning rate schedule has a linear warmup followed by cosine decay (with\nthe minimum of the cosine decay set to be 10% of the peak learning rate), and the \u03b22 of AdamW\n19\nPublished as a conference paper at ICLR 2024\nBEV \nTokens\nt-3\nt-2\nt-1\nt\nt+1\nConditional \nLogits\nUnconditional \nLogits\nClassi\ufb01er-Free Guidance (CFG) logits\nTimesteps\nActions\n(ego poses)\nt+1\nWorld Model\nWorld Model\nMask Token\n+ w\n(\n.\n)\n=\nShared\nFigure 10: Illustration of classifier-free diffusion guidance (CFG) during inference for our world\nmodel. The conditional logits are conditioned upon the past agent history (past observations and\nactions). The unconditional logits are not conditioned on the past. Classifier-free diffusion guidance\namplifies the difference between the two to produce the CFG logits used for sampling. Note that\nCFG can be efficiently implemented with a single forward pass at each diffusion step by increasing\ntemporal sequence length by 1, and setting the attention mask to be a causal mask within the previous\nsequence length and an identity mask for the last frame. In addition, thanks to causal masking and\nour factorized spatio-temporal Transformer, for all past timesteps, we only need cached keys and\nvalues from the temporal blocks.\nCFG Logits\nWorld \nModel\nMask Token\nSampling per \nlocation\nSampled Tokens\nLog likelihoods of \nsampled tokens\nBinary mask\nSet the log likelihoods at \nnon-mask locations to inf\nAdd Gumbel noise\nDi\ufb00usion step k\nSort\nSelect the\ntop M locations\nOutput tokens\nAdd mask \ntokens\nFigure 11:\nIllutration of discrete diffusion sampling at each diffusion step. This procedure\ncorresponds to Algorithm 2. k is the current diffusion step, K is the total number of diffusion steps,\nN is the number of tokens in each observation, \u03b3(u) = cos(u\u03c0/2) is the mask schedule.\n(Loshchilov & Hutter, 2017) is lowered to be 0.95. A weight decay of 0.0001 is applied, but any\nbias parameters, embedding or un-embedding layers, and LayerNorm parameters are excluded from\nweight decay. For training tokenizers, we use a learning rate of 0.001, a linear warmup length of\n4000 iterations, a max gradient norm clipping of 0.1, a batch size of 16, and a cosine decay length of\n0.4 million iterations. For training the world model, we use a learning rate of 0.001, a linear warmup\nlength of 2000 iterations, a max gradient norm clipping of 5.0, a batch size of 8, and a cosine decay\nlength of 0.75 million iterations. The cross entropy loss uses 0.1 label smoothing. The same set of\nhyperparameters are used across all datasets.\n20\nPublished as a conference paper at ICLR 2024\nA.4\nALTERNATIVE DERIVATION FOR DISCRETE DIFFUSION TRAINING OBJECTIVE\nIn discrete diffusion, the forward diffusion process is (Austin et al., 2021)\nq(xk | xk\u22121) = Cat(xk; p = xk\u22121Qk)\nwhere Cat refers to a categorical distribution, and Qk is the forward transition matrix, [Qk]ij =\nq(xk = j|xk\u22121 = i) (meaning that each row of Q sums to 1, but not necessarily the columns).\nThe cumulative transition matrix is\nQk = Q1Q2 \u00b7 \u00b7 \u00b7 Qk\nAnd as a result, if Qk can be written in closed-form, then q(xk | x0) can be written in terms of Qk.\nq(xk | x0) = Cat(xk; p = x0Qk)\nWe aim to learn a neural net parameterized by \u03b8 to reverse the forward diffusion process:\np\u03b8(xk\u22121 | xk) =\nX\n\u02dcx0\nq(xk\u22121 | xk, \u02dcx0)\u02dcp\u03b8(\u02dcx0 | xk)\nWe present an alternative derivation for the lower bound that we optimize in this paper:\nEq(x0)[log p\u03b8(x0)]\n= Eq(x0)[log\nZ\np\u03b8(x0, x1 \u00b7 \u00b7 \u00b7 xK)dx1 \u00b7 \u00b7 \u00b7 xK]\n= Eq(x0)\n\u001a\nlog Eq(x1:K|x0)\n\u0014p\u03b8(x0:K\u22121 | xK)\nq(x1:K | x0)\np(xK)\n\u0015\u001b\n\u2265Eq(x0)q(x1:K|x0)\n\u0014\nlog p\u03b8(x0:K\u22121 | xK)\nq(x1:K | x0)\n+ log p(xK)\n\u0015\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nlog p\u03b8(xk\u22121 | xk)\nq(xk | xk\u22121) + log p(xK)\n\u0015\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nlog p\u03b8(xk\u22121 | xk) + log p(xK) \u2212\nK\nX\nk\u22651\nlog q(xk | xk\u22121)\n\u0015\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nlog\nX\n\u02dcx0\nq(xk\u22121 | xk, \u02dcx0)\u02dcp\u03b8(\u02dcx0 | xk)\n\u0015\n+ Eq(x0:K)\n\u0014\nlog p(xK) \u2212\nK\nX\nk\u22651\nlog q(xk | xk\u22121)\n\u0015\n|\n{z\n}\nC1\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nlog\nX\n\u02dcx0\nq(xk\u22121, \u02dcx0 | xk)\nq(\u02dcx0 | xk)\n\u02dcp\u03b8(\u02dcx0 | xk)\n\u0015\n+ C1\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nlog\nX\n\u02dcx0\nq(\u02dcx0 | xk\u22121)\nq(\u02dcx0 | xk)\nq(xk|xk\u22121)q(xk\u22121)/q(xk)\nz\n}|\n{\nq(xk\u22121 | xk)\n\u02dcp\u03b8(\u02dcx0 | xk)\n\u0015\n+ C1\n\u2265Eq(x0:K)\n\u0014 K\nX\nk\u22651\nX\n\u02dcx0\nq(\u02dcx0 | xk\u22121) log\n\u0012q(xk\u22121 | xk)\nq(\u02dcx0 | xk) \u02dcp\u03b8(\u02dcx0 | xk)\n\u0013\u0015\n+ C1\n= Eq(x0:K)\n\u0014 K\nX\nk\u22651\nX\n\u02dcx0\nq(\u02dcx0 | xk\u22121) log \u02dcp\u03b8(\u02dcx0 | xk)\n\u0015\n+ C1 + Eq(x0:K)\n\u0014 K\nX\nk\u22651\nX\n\u02dcx0\nq(\u02dcx0 | xk\u22121) log q(xk\u22121 | xk)\nq(\u02dcx0 | xk)\n\u0015\n|\n{z\n}\nC2\n21\nPublished as a conference paper at ICLR 2024\n=\nK\nX\nk\u22651\nEq(x0,xk\u22121,xk)\n\u0014 X\n\u02dcx0\nq(\u02dcx0 | xk\u22121) log \u02dcp\u03b8(\u02dcx0 | xk)\n\u0015\n+ C1 + C2\n=\nK\nX\nk\u22651\nEq(x0,xk\u22121,xk)q(\u02dcx0|xk\u22121)[log \u02dcp\u03b8(\u02dcx0 | xk)] + C1 + C2\n=\nK\nX\nk\u22651\nEq(x0|xk\u22121)q(xk|xk\u22121)q(xk\u22121)q(\u02dcx0|xk\u22121)[log \u02dcp\u03b8(\u02dcx0 | xk)] + C1 + C2\n=\nK\nX\nk\u22651\nEq(xk|xk\u22121)q(xk\u22121,\u02dcx0)[log \u02dcp\u03b8(\u02dcx0 | xk)] + C1 + C2\n=\nK\nX\nk\u22651\nEq(xk,x0)[log \u02dcp\u03b8(x0 | xk)] + C1 + C2\nAnalyzing the constants C1 and C2:\nC1 = Eq(x0:K)\n\u0014\n\u2212\nK\nX\nk=1\nlog q(xk | xk\u22121) +\nlog p(xK)\n|\n{z\n}\nNote that p(xK)=q(xK)\n\u0015\n= Eq(x0:K)\n\u0014\n\u2212\nK\nX\nk=1\nlog q(xk, xk\u22121) +\nK\nX\nk=0\nlog q(xk)\n\u0015\nC2 = Eq(x0:K)\n\u0014 K\nX\nk=1\nlog q(xk\u22121|xk)\n\u0015\n\u2212Eq(x0:K)\n\u0014 K\nX\nk=1\nX\n\u02dcx0\nq(\u02dcx0 | xk\u22121) log q(\u02dcx0 | xk)\n\u0015\n= Eq(x0:K)\n\u0014 K\nX\nk=1\nlog q(xk, xk\u22121) \u2212\nK\nX\nk=1\nlog q(xk)\n\u0015\n\u2212\nK\nX\nk=1\nEq(x0:K)q(\u02dcx0|xk\u22121)[log q(\u02dcx0 | xk)]\nC1 + C2 = Eq(x0:K)[log q(x0) \u2212\nK\nX\nk=1\nlog q(x0 | xk)]\nTherefore, this inequality eventually becomes:\nEq(x0)[log p\u03b8(x0)] \u2265\nK\nX\nk=1\nEq(xk,x0)[log p\u03b8(x0|xk)] + Eq(x0:K)[log q(x0) \u2212\nK\nX\nk=1\nlog q(x0|xk)]\n=\nK\nX\nk=1\nEq(x0)q(xk|x0)[log p\u03b8(x0 | xk)] + C\nWhich arrives at Equation (6). This loss function avoids the computation of q(xk\u22121 | xk, x0) as\ndefined in Equation (8) and p\u03b8(xk\u22121 | xk) as defined in Equation (1).\n22\nPublished as a conference paper at ICLR 2024\nA.5\nABSORBING-UNIFORM DISCRETE DIFFUSION\nIn this section, we review absorbing-uniform discrete diffusion, and illustrate its connection to our\nimproved version of MaskGIT in detail. First, we aim to express all transition matrices in terms of\nmatrix exponential, which provides a convenient expression for the cumulative transtion matrix:\nQk = exp(\u03bbkR) =\n\u221e\nX\nn=0\n\u03bbn\nk\nn! Rn\nQk = exp\n\u0010\u0010 X\ns\u2264k\n\u03bbs\n\u0011\nR\n\u0011\nIn absorbing diffusion, each token has a \u03b1k probability of turning into a mask token at step k (using\na slightly overloaded notation, not to be confused with the occupancy values in the depth rendering\nequation). Let em be a one-hot vector where the index of the mask token is 1.\nQa\nk = exp(\u03b3kRa) = exp(\u03b3k(1e\u22a4\nm \u2212I))\n= exp(\u2212\u03b3k)I + (1 \u2212exp(\u2212\u03b3k))1e\u22a4\nm\n= (1 \u2212\u03b1k)I + \u03b1k1e\u22a4\nm\nWhich utilizes the fact that (1e\u22a4\nm \u2212I)(1e\u22a4\nm \u2212I) = (\u22121)(1e\u22a4\nm \u2212I) and therefore (1e\u22a4\nm \u2212I)n =\n(\u22121)n+1(1e\u22a4\nm \u2212I). Breaking down cumulative absorbing transition matrix Q\na\nk:\nQ\na\nk = exp\n\u0010\u0010 X\ns\u2264k\n\u03b3s\n\u0011\n(1e\u22a4\nm \u2212I)\n\u0011\n= exp\n\u0010\n\u2212\nX\ns\u2264k\n\u03b3s\n\u0011\nI +\n\u0010\n1 \u2212exp\n\u0010\n\u2212\nX\ns\u2264k\n\u03b3s\n\u0011\u0011\n1e\u22a4\nm\n=\nY\ns\u2264k\n(1 \u2212\u03b1s)I +\n\u0010\n1 \u2212\nY\ns\u2264k\n(1 \u2212\u03b1s)\n\u0011\n1e\u22a4\nm\nIn uniform diffusion, each token has a \u03b2k probability of turning into a random non-mask token at\nstep k. With M non-mask categories (M = |V |), the transition matrix can be expressed as:\nQu\nk = I \u2212\u03b2k(I \u2212eme\u22a4\nm) + \u03b2k\nM (1 \u2212em)(1 \u2212em)\u22a4\n= (1 \u2212\u03b2k)I + \u03b2k\n\u0010\neme\u22a4\nm + 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u0011\n= exp\n\u0010\n\u03b7kRu\n\u0011\n= exp\n\u0010\n\u03b7k\nh 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u2212(I \u2212eme\u22a4\nm)\ni\u0011\n= exp(\u2212\u03b7k)I + (1 \u2212exp(\u2212\u03b7k))\n\u0010\neme\u22a4\nm + 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u0011\nBreaking down cumulative uniform transition matrix Q\nu\nk:\nQ\nu\nk = exp\n\u0010\u0010 X\ns\u2264k\n\u03b7s\n\u0011h 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u2212(I \u2212eme\u22a4\nm)\ni\u0011\n= exp\n\u0010\n\u2212\nX\ns\u2264k\n\u03b7s\n\u0011\nI +\n\u0010\n1 \u2212exp\n\u0010\n\u2212\nX\ns\u2264k\n\u03b7s\n\u0011\u0011\u0010\neme\u22a4\nm + 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u0011\n=\nY\ns\u2264k\n(1 \u2212\u03b2s)I +\n\u0010\n1 \u2212\nY\ns\u2264k\n(1 \u2212\u03b2s)\n\u0011\u0010\neme\u22a4\nm + 1\nM (1 \u2212em)(1 \u2212em)\u22a4\u0011\n23\nPublished as a conference paper at ICLR 2024\nIn absorbing-uniform diffusion, with e\u22a4\nm being the (M + 1)-th row, the transition matrix is:\nQk = Qa\nkQu\nk\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03c9k + \u03bdk\n\u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03b1k\n\u03bdk\n\u03c9k + \u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03b1k\n\u03bdk\n\u03bdk\n\u03c9k + \u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03b1k\n...\n...\n...\n...\n...\n...\n\u03bdk\n\u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03c9k + \u03bdk\n\u03b1k\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u03bdk = \u03b2k\nM\n\u03c9k = 1 \u2212\u03b2k \u2212\u03b1k\nNote that Qa\nk and Qu\nk are commuting matrices; Ra and Ru inside matrix exponentials also commute\n(so that exp(\u03b7kRu) exp(\u03b3kRa) = exp(\u03b7kRu + \u03b3kRa)). This property allows us to write the\ncumulative transition matrix as:\nQk = Q\na\nkQ\nu\nk\n=\n\uf8ee\n\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0\n\u03c9k + \u03bdk\n\u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03c7k\n\u03bdk\n\u03c9k + \u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03c7k\n\u03bdk\n\u03bdk\n\u03c9k + \u03bdk\n\u00b7 \u00b7 \u00b7\n\u03bdk\n\u03c7k\n...\n...\n...\n...\n...\n...\n\u03bdk\n\u03bdk\n\u03bdk\n\u00b7 \u00b7 \u00b7\n\u03c9k + \u03bdk\n\u03c7k\n0\n0\n0\n\u00b7 \u00b7 \u00b7\n0\n1\n\uf8f9\n\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\n\u03c9k =\nY\ns\u2264k\n(1 \u2212\u03b2s \u2212\u03b1s)\n\u03c7k = 1 \u2212\nY\ns\u2264k\n(1 \u2212\u03b1s)\n\u03bdk = 1\nM (1 \u2212\u03c9k \u2212\u03c7k)\nIntuitively, \u03c9k is the probability that a ground-truth token neither flips into a random token nor gets\nabsorbed into the mask token after k forward diffusion steps. \u03c7k is the probability of having a mask\ntoken after k forward diffusion steps.\nGiven that the posterior can be expressed as\nq(xk\u22121 | xk, x0) = q(xk | xk\u22121, x0)q(xk\u22121 | x0)\nq(xk | x0)\n= q(xk\u22121 | x0)\nq(xk | x0) q(xk | xk\u22121)\n(8)\nWe can express it in matrix form for discrete diffusion:\nq(xk\u22121 = i | xk = j, x0 = l) = [Qk\u22121]li \u00b7 [Qk]ij\n[Qk]lj\nq(xk\u22121 | xk, x0) = x0Qk\u22121 \u2299xkQ\u22a4\nk\nx0Qkx\u22a4\nk\nAllowing us to efficiently compute the closed-form posterior from Qk\u22121, Qk, and Qk defined earlier.\nA.6\nTRAINING AND INFERENCE OF ABSORBING-UNIFORM DISCRETE DIFFUSION\nDuring training, we first apply masking, which corresponds to absorbing diffusion with Q\na\nk, and\nthen add uniform noise, which corresponds to uniform diffusion with Q\nu\nk. We add at a maximum \u03b7%\nof uniform noise under a linear schedule, where we set \u03b7 = 20 by default. \u03b7 controls the minimum\nsignal-to-noise ratio among the non-mask tokens.\n\u2022 Despite its simplicity, Algorithm 1 is training an absorbing-uniform discrete diffusion\nmodel. To see this, we simply need to define the corresponding noise schedules for uni-\nform diffusion and absorbing diffusion. The uniform discrete diffusion noise schedule is\nset to be \u03b2k = 1/(K/\u03b7 \u2212k + 1) and consequently (1 \u2212Q\ns\u2264k(1 \u2212\u03b2s)) = \u03b7 \u00b7 k/K. Since\n(1 \u2212Q\ns\u2264k(1 \u2212\u03b2s)) has now become a linear function with respect to k, we can simply\nuniformly sample a noise ratio without computing any transition matrix, as done in Algo-\nrithm 1. As for the absorbing diffusion noise schedule, we define \u03b1k = Q\ns\u2264k(1 \u2212\u03b1s) as\na cosine schedule, and \u03b1k can be solved accordingly with \u03b1k = 1 \u2212\u03b1k/\u03b1k\u22121.\n24\nPublished as a conference paper at ICLR 2024\nDuring inference, we find that when decoding xk, marginalizing over x0 as done in p\u03b8(xk|xk+1) =\nP\nx0 q(xk | xk+1, x0)p\u03b8(x0 | xk+1) is not necessary; instead, we directly decode the predicted x0\nfrom p\u03b8(x0|xk+1), as done in MaskGIT. During sampling, once a token has transitioned from a\nmask token to a non-mask token, even though it can be further revised and resampled, it cannot go\nback to become masked again, as one would expect from the reverse process of absorbing-uniform\ndiffusion. This is achieved by setting lk \u2190+\u221eon non-mask indices of xk+1 in Algorithm 2.\nWe find that, during the sampling process \u02dcx0 \u223cp\u03b8(\u00b7 | xk+1), the sampling quality can be improved\nby using top-K sampling rather than vanilla categorical sampling (similar to what we would expect\nfrom a language model). In our experiments, we sample from the top 3 logits per feature location.\nA.7\nADDITIONAL QUALITATIVE RESULTS\nWe present additional qualitative results: Figure 13 and Figure 14 outline our results on NuScenes;\nFigure 15 shows our results on KITTI, and Figure 16 and Figure 17 adds to the results on Argoverse\n2 Lidar. Overall, our models achieve considerably better qualitative results on all three datasets.\nPredictions under counterfactual actions\nWe visualize how well the world model can predict\nthe future under counterfactual actions in Figure 12. Here we modify the future trajectories of the\nego vehicle (which are action inputs to the world model), and we demonstrate that the world model\nis able to imagine alternative futures given different actions. The imagined futures are consistent\nwith the counterfactual action inputs. Moreover, the world model has learned that other vehicles in\nthe scene are reactive agents.\nFailure cases of our current models\nWe present some failure modes of our current models us-\ning red arrows in Figure 14, Figure 16, and Figure 17. Modeling vehicle behaviours on a 3s time\nhorizon needs further improvement, as the accuracy for 3s prediction seems lower than 1s predic-\ntion; however, we note that this inaccuracy could be partly due to the multi-modality of 3s future\nprediction (multiple futures could be equally valid). Additionally, on a 3s time horizon, sometimes\nnew vehicles (not present in the past or present frames) enter the scene. Currently, the world model\ndoes not seem to have learned how to hallucinate new vehicles coming into the region of interest.\nNevertheless, those failure cases do not necessarily reflect fundamental limitations of our proposed\nalgorithm; we believe that the issues listed above can be addressed to a large extent by increasing\ndata, compute, and model size under our framework.\n25\nPublished as a conference paper at ICLR 2024\nScene 1\nGT\nOriginal \nActions\nCounter\nfactual \nActions\nScene 2\nGT\nEgo Vehicle Action\nEgo Vehicle Action\nOriginal \nActions\nCounter\nfactual \nActions\nFigure 12: Visualization of predicted future under counterfactual actions. Here we modify the\nfuture trajectories of the ego vehicle (which are action inputs to the world model), and demonstrate\nthat the world model is able to imagine alternative futures given different actions. The orange\ntrajectory represents the actions taken by the ego vehicle regarding where to drive. The imagined\nfutures are consistent with the counterfactual action inputs. In addition, our world model is able to\nlearn that other vehicles in the scene are reactive agents. Notably, in Scene 1, the counterfactual\naction is for the ego vehicle to brake, and our world model has learned that the vehicle behind will\nalso react by braking to avoid a collision with the ego vehicle.\n26\nPublished as a conference paper at ICLR 2024\nFigure 13:\nQualitative comparison on NuScenes 1s prediction. The last column overlaps the\npoint clouds from 0.5s prediction and 1s prediction to make clear how vehicles are moving.\n27\nPublished as a conference paper at ICLR 2024\nCurrent Observation\n1s Future GT\n2s Future GT\n3s Future GT\nTokenizer Reconstruction\n1s Prediction\n2s Prediction\n3s Prediction\nTokenizer Reconstruction\n0.6s Prediction\n2s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n2s Future GT\n3s Future GT\n1s Prediction\n1s Future GT\n0.6s Prediction\n2s Prediction\n3s Prediction\n1s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\n0.6s Prediction\n2s Prediction\n3s Prediction\n1s Prediction\nCurrent Observation\nPrevious SOTA\n(4D-Occ)\nOurs\nGround Truth\nTokenizer Reconstruction\n0.6s Prediction\n2s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n2s Future GT\n3s Future GT\n1s Prediction\n1s Future GT\n0.6s Prediction\n2s Prediction\n3s Prediction\n1s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\nFigure 14:\nQualitative comparison on NuScenes 3s prediction. The orange circles highlight\nwhere our model does well; the red arrows point out some failure cases of our model.\n28\nPublished as a conference paper at ICLR 2024\nCurrent Observation\n1.8s Future GT\n2.4s Future GT\n3s Future GT\nTokenizer Reconstruction\n1.8s Future GT\n2.4s Future GT\n3s Prediction\nTokenizer Reconstruction\n0.6s Prediction\n2.4s Future GT\n3s Prediction\nCurrent Observation\n0.6s Future GT\n2.4s Future GT\n3s Future GT\n1.8s Future GT\n1.8s Future GT\n0.6s Prediction\n2.4s Future GT\n3s Prediction\n1.8s Future GT\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\n0.6s Prediction\n2.4s Future GT\n3s Prediction\n1.8s Future GT\nCurrent Observation\nPrevious SOTA\n(4D-Occ)\nOurs\nGround Truth\nTokenizer Reconstruction\n0.6s Prediction\n2.4s Future GT\n3s Prediction\nCurrent Observation\n0.6s Future GT\n2.4s Future GT\n3s Future GT\n1.8s Future GT\n1.8s Future GT\n0.6s Prediction\n2.4s Future GT\n3s Prediction\n1.8s Future GT\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\nFigure 15: Qualitative comparison on KITTI Odometry. Note that the color of a point is merely\ndependent on its height (z-axis value); therefore, if the colors of the predicted points are different\nfrom the colors of the ground-truth points, it means that the predicted point heights are off. Our\nmethod clearly achieves significantly better qualitative results compared to prior SOTA.\n29\nPublished as a conference paper at ICLR 2024\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\n0.6s Prediction\n0.6s Future GT\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nCurrent Observation\nPrevious SOTA\n(4D-Occ)\nOurs\nGround Truth\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\n0.6s Prediction\n0.6s Future GT\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\nFigure 16: Additional qualitative comparison on Argoverse2 Lidar dataset. The orange circles\nhighlight where our model does well; the red arrows point out some failure cases of our model.\n30\nPublished as a conference paper at ICLR 2024\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\n0.6s Prediction\n0.6s Future GT\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nCurrent Observation\nPrevious SOTA\n(4D-Occ)\nOurs\nGround Truth\nTokenizer Reconstruction\n0.6s Prediction\n1s Prediction\n3s Prediction\nCurrent Observation\n0.6s Future GT\n1s Future GT\n3s Future GT\n0.6s Prediction\n0.6s Future GT\n0.6s Prediction\n1s Prediction\n3s Prediction\n0.6s Prediction\nPrevious SOTA\n(4D-Occ)\nCurrent Observation\nOurs\nGround Truth\nFigure 17: Additional qualitative comparison on Argoverse2 Lidar dataset. The orange circles\nhighlight where our model does well; the red arrows point out some failure cases of our model.\n31\n",
    "2404.12624": "Dragtraffic: A Non-Expert Interactive and Point-Based Controllable\nTraffic Scene Generation Framework\nSheng WANG1, Ge SUN1, Fulong MA2, Tianshuai Hu1, Yongkang Song3, Lei Zhu2, Ming Liu2\nAbstract\u2014 The evaluation and training of autonomous driv-\ning systems require diverse and scalable corner cases. However,\nmost existing scene generation methods lack controllability,\naccuracy, and versatility, resulting in unsatisfactory generation\nresults. To address this problem, we propose Dragtraffic, a gen-\neralized, point-based, and controllable traffic scene generation\nframework based on conditional diffusion. Dragtraffic enables\nnon-experts to generate a variety of realistic driving scenarios\nfor different types of traffic agents through an adaptive mixture\nexpert architecture. We use a regression model to provide a\ngeneral initial solution and a refinement process based on the\nconditional diffusion model to ensure diversity. User-customized\ncontext is introduced through cross-attention to ensure high\ncontrollability. Experiments on a real-world driving dataset\nshow that Dragtraffic outperforms existing methods in terms\nof authenticity, diversity, and freedom. Demo videos and code\nare available at https://chantsss.github.io/Dragtraffic/.\nI. INTRODUCTION\nThe safety of autonomous driving systems relies heavily\non the richness of the dataset scenarios. However, due\nto various constraints such as safety issues, geographical\nenvironment, and weather changes, it is difficult for collected\ndata to cover all situations. This poses challenges for training\nand evaluating planning and prediction modules, especially\nfor extreme scenarios. To address this, simulators such as\nSUMO [1] and CARLA [2] have been used to manually set\nscenarios. While rule-based simulations offer interpretability\nand feasible trajectories without requiring extensive data,\nthey have limitations in accuracy, generalization capability,\nand adaptive updating. They also require significant expert\nknowledge to establish the rules [3].\nTo address issues with simulated environments, data-\ndriven methods have been proposed to make agents in these\nenvironments imitate the behaviors of real traffic participants.\nThe recent Sim Agents Challenge [4] based on the Waymo\ndataset provides a standard benchmark and specifies input\nand output forms for scene generation tasks. Several works\nhave used learning-based methods to achieve good results,\nparticularly in terms of accuracy [5] [6] [7]. However, most\nof these works formulate the scene generation task as a\nmotion prediction task, requiring 10 frames of historical\ninformation as input and limiting the freedom of scene\nconstruction. As a result, they can only reason about future\nscenarios based on existing historical trajectories, while\nignoring requirements such as scene editing and agent inser-\ntion. Other researchers have looked at generating challenging\n1The Hong Kong University of Science and Technology, Hong Kong\nSAR, China. 2The Hong Kong University of Science and Technology\n(Guangzhou), Nansha, Guangzhou, 511400, Guangdong, China. 3Lotus\nTechnology Ltd, China.(Email: swangei@connect.ust.hk).\nvi\nvj\nvk\nx4\nvi\nx1\nx5\nxn\nx2\nx1\nx5\nxn\nx2\nCollection Space\nAugmented Space\n\u2026\nx3\nx4\n\u2026\nx3\nvl\nFig. 1: The dataset sample space The left picture represents\nthe distribution of collected data, while the second picture\nrepresents the sample space expanded through data augmen-\ntation. An ideal sample data distribution should be evenly\npresented in each dimension. However, due to factors such\nas cost and security, high-value data is often scarce.\nscenes in a more flexible way, such as SceneGen [8] and\nTrafficgen [9], which propose building scenes in two stages:\nvehicle placement and trajectory generation. However, a\nmain shortcoming of these methods is the lack of control-\nlability, which means they cannot ensure expected behavior.\nThis serious problem leads to the generation process being\ndirectionless and extremely inefficient when the sample space\nis large, as shown in the figure 1. Another problem is\nthat they only focus on vehicles and ignore other types\nof traffic participants, even though these participants have\nmany interactions. On the other hand, generative models\nhave been well developed in contentmade great progress in\nsequence generation tasks such as text, pictures, and videos\n[10] [11] [12] [13] [14]. Some prior works are inspired\nof using generative models to create traffic scenes such as\n[15], [16] utilizing GAN to generate multiple trajectories\nfor traffic agents. In order to obtain a more operational\ntrajectory generation method, some researchers try to use\nconditional diffusion models such as motiondiffuser [17], and\nSceneDM [18] etc. These studies provide a good foundation\nfor using generative models to create scenes, but there is a\ngeneral problem. They require significant expert knowledge\nto establish the complex definition of loss function or post-\nprocessing. CTG++ [19] provides a idea to introduce the\nlarge language model in loss function design in a user\narXiv:2404.12624v1  [cs.RO]  19 Apr 2024\nfriendly way. However, this method requires repeated and\ncomplex training processes for different tasks.\nIn this paper, we propose Dragtraffic, an instance behavior\nlevel traffic scene generator that is capable of generating\nrealistic and diverse scenes while maintaining a high degree\nof freedom on controllability. To achieve realism, we employ\na regress model to provide initial guesses. To account for the\nbehavioral differences among various traffic agents, we adopt\na symmetric hybrid expert architecture that imitates the real\nbehavior of traffic participants on the road from an agent-\ncentric perspective by using a separate model dedicated to the\ncorresponding agent. Inspired by Draggan [20], we adopt the\nconditional diffusion model to achieve diversity and specific\ndefined context, including position, velocity, heading, length,\nand width. All these controls can be done through dragging\ntyping context in an interactive and user-friendly way. Our\ncontributions include:\n\u2022 First, we propose an interactive traffic generation frame-\nwork, which, to the best of our knowledge, is the first\nframework that provides the highest degree of freedom\nfor generating traffic scenes.\n\u2022 Second, we propose a solution that uses a regression\nmodel to provide initial solutions, a conditional dif-\nfusion model to provide diversity, and a Mixture of\nExperts to adapt to multiple agent types. This solution\nallows us to generate realistic and diverse traffic scenes\nwith a high degree of controllability.\n\u2022 Third, we conduct experiments on a real-world driving\ndataset to evaluate the performance of Dragtraffic. The\nresults demonstrate that Dragtraffic outperforms other\nexisting methods in terms of authenticity, diversity, and\nfreedom, among other metrics.\nII. RELATED WORK\nA. Trajectory Prediction\nCurrent methods for testing and developing automated\ndriving systems, such as scenario replay and rule-based\napproaches, have limitations in accuracy, generalization abil-\nity, and adaptive updating. These methods also require a\nlarge amount of expert knowledge to build the rules. To\naddress these shortcomings, recent studies have explored\ndeep learning-based motion prediction methods that can\nmodel multi-modal traffic scenes. These methods can be\nbroadly categorized into supervised learning and generative\nlearning approaches. Supervised learning trains a model with\nlogged trajectories with supervised losses such as L2 loss.\nOne of the challenges is to model inherent multi-modal\nbehavior of the agents. For instance, Trajeglish [5] employs\nthe template sets to help the model generate realistic multi-\nmodal trajectories by providing a structured framework for\ninteractions. a series works, MultiPath++ [21], DenseTNT\n[22] and Wayformer [23] use static anchors or learned goals\nto represent the multiple hypothises. GoHome [24] and Ynet\n[25] predict future occupancy heatmaps, and then decode\ntrajectories from the samples. Many of these approaches\nuse ensembles for further diversified predictions. The next\nsection covers generative approaches.\nB. Generative models for Scene Generation\nGenerative models have made significant progress in se-\nquence generation tasks such as text, pictures, and videos.\nSome prior works have explored the use of generative models\nto create traffic scenes. For example, GANs have been used\nto generate multiple trajectories for traffic agents in [15] and\n[16]. VAEs have been employed in MTG [26] and CVAE-\nH [27] to extract representations of historical trajectories\nof agents and generate future trajectories. CTG [19] uses\na conditional diffusion model for generating controllable\ntraffic simulations, while CTG++ [19] introduces a scene-\nlevel conditional diffusion model guided by language in-\nstructions. MotionDiffuser [17] and SceneDM [18] both use\ndiffusion-based models for predicting multi-agent motion\nand achieving state-of-the-art results on the Waymo Open\nMotion Dataset and Waymo Sim Agents Benchmark, respec-\ntively. While the above methods have achieved good perfor-\nmance, they often require professionals to design complex\noptimization constraints and loss functions. In contrast, our\nproposed framework simplifies the task of scene generation\nby allowing users to control the generation process through\nsimple drag and click, while ensuring the quality of the\ngenerated scenes.\nIII. PROBLEM FORMULATION\nOur aim is to generate the expected future motions for\nagents in a scenario. We adopt a structured vectorized rep-\nresentation to depict the map and agents. The trajectory of a\nspecific agent is denoted as \u03c40:t = {s0, s1, ..., st}, where st \u2208\nRD indicating the states including the type, location, heading\nangle, velocity at time step t. The road map is denoted as\nL = {li}, where li \u2208RN\u00d7F representing ith lane has N\nsegments and each segment has F lane semantic attributes\n(e.g., intersections and crosswalks). Specifically, and for the\ntask of existing scenario augmentation or inpainting, we aim\nto generate tF steps future trajectories:\n\u03c41:tF = f(s0\n0...sM\n0 , s0\ntF , L),\nwhere s01...s0M indicates the initial states of M agents.\ns0\ntF represents the condition information. For the task of\nnew scenario creation, we aim to generate tF steps future\ntrajectories following:\n\u03c41:tF = f(G(L), s0\ntF , L),\nwhere G(\u00b7) indicates the initial states generation, which can\nbe simply realized through dragging, typing or an agent\nplacement module.\nIV. METHODOLOGY\nA. Conditional Diffusion Model Preliminaries\nDiffusion models consist of a diffusion process that grad-\nually transforms a data distribution into unstructured noise\nand a reverse process to recover the data distribution [28].\nThe forward diffusion process acting on \u03c41:F is defined as:\nQKV\nQKV\nQKV\nQKV\nDenoising Step\u00a0\nCross\nAttention\nSkip Connection\nGenerated Scene\nDenoise at \n\u00a0\nInitial Guess at \nDataset\nHuman Interaction\nInitial Regression\u00a0Model\nCondition Conetext\u00a0Query\nDrag agent0 here\nCondition Diffusion Model\u00a0\nMixture of Experts Gate\nContext Description\nType: vehicle\nL. & Width: 5.2m, 2.7m\nStart\nVelocity 5m/s\nHeading 0\nGoal\nVelocity 15m/s\nHeading\u00a0\n......\nFig. 2: The generation pipeline. The Condition Context Query acquires personalized information from the user, which\ncan be queried through an interactive UI or retrieved from the dataset. The Mixture of Experts Gate selects the appropriate\nmodel for inference based on the agent type. The input data is in the form of agent-centric vector. After obtaining the initial\nsolution, it will be further refined through diffusion to ultimately generate the scene.\nq(\u03c4 1:k\n1:F |\u03c4 0\n1:F ) :=\nk\nY\nk=1\nq(\u03c4 k\n1:F |\u03c4 k\u22121\n1:F )\nq(\u03c4 k\n1:F |\u03c4 k\u22121\n1:F ) := N(\u03c4 k\n1:F ;\np\n1 \u2212\u03b2kak\u22121, \u03b2kI),\nwhere the variance schedule \u03b21, \u03b22, \u00b7 \u00b7 \u00b7 \u03b2k is fixed and de-\ntermines the amount of noise injected at each diffusion\nstep, leading to a gradual corruption of the signal into an\nisotropic Gaussian distribution. To generate trajectories, we\naim to reverse this diffusion process by utilizing a learned\nconditional denoising model, which is iteratively applied\nstarting from sampled noise. Given the context information\nc(\u03c40, s0\ntF , L), the reverse diffusion process is:\np\u03b8(\u03c4 0:k\n1:F |c) := p(\u03c4 k\n1:F )\nk\nY\nk=1\np\u03b8(\u03c4 k\u22121\n1:F |\u03c4 k\n1:F , c)\np\u03b8(\u03c4 k\u22121\n1:F |\u03c4 k\n1:F , c) := N(\u03c4 k\u22121\n1:F ; \u00b5\u03b8(\u03c4 k, k, c), \u03a3\u03b8(\u03c4 k\n1:F , k, c)).\nThe distribution p(\u03c4 k\n1:F ) is a normal distribution and \u03b8\ndenotes the parameters of the diffusion model. Note that at\neach step, the model takes both \u03c4 k and the resulting states\n\u03c4 k\n1:F = f(\u03c4 0\n1:F , \u03c4 k\n1:F ) as input. In this work, we adopt the idea\nproposed in [29] and use the conditional diffusion model as\na refinement module as shown in Figure 2. This is to say, a\nskip connection is used to generate \u03c4 0\n1:F as follow:\n\u03c4 k\n1:F \u223c\u03c4 \u22c6\n1:F = finit(c)\n\u03c4 \u03b3\n1:F = fdenoise(\u03c4 \u03b3+1\n1:F , c), \u03b3 = k \u22121, \u00b7 \u00b7 \u00b7 , 0,\nwhere finit(\u00b7) is a standard motion forecasting model pro-\nviding the initial gueses for better regression performance\npurpose, we will elaborate its effectiveness in following\nexperiments section.\nB. Context Description\nDue to the high degree of freedom of our generation\nscheme, the context description can consist of some or all\nof the following information: vehicle type, length and width,\nstarting position, starting speed, starting orientation, target\nposition, target speed, target orientation.\nC. Initial Trajectory Generation\nWe utilize Multipath++ [21] as the initial backbone for\ntrajectory generation, which employs multi-context gating\n(MCG) blocks. MCG can be seen as an approximation of\ncross-attention as an approximation to crossattention. Instead\nof having each of the n elements attend to all m elements\nof the other set, MCG condenses the other set into a single\ncontext vector. The prediction heads take c then output the\nfuture \u03c4 and K probabilities. This initial regression module\nis trained by minimizing the MSE loss of the predicted\ntrajectory which is the closest to the ground truth trajectory.\nD. Diffusion Refinement\nHere we elaborate the design of a denoising module\nfdenoise (\u00b7) , which denoises the trajectory \u03c4 \u03b3+1\n1:F\nconditioned\non current c(\u03c40, s0\ntF , L). In a denoising module, two parts are\ntrainable: a MCG-based context encoder fcontext(\u00b7) to learn a\nsocial-temporal embedding for current states and condition\nstates and a noise estimation module f\u03f5(\u00b7) to estimate the\nnoise to reduce. Mathematically, the \u03b3 th denoising step\nworks as follows:\n\u03f5\u03b3\n\u03b8 = f\u03f5 (\u03c4 \u03b3\n1:F , fcontext, \u03b3 + 1)\n(1)\n\u03c4 \u03b3\n1:F =\n1\n\u221a\u03b1\u03b3\n \n\u03c4 \u03b3+1\n1:F \u2212\n1 \u2212\u03b1\u03b3\np1 \u2212\u00af\u03b1\u03b3\n\u03f5\u03b3\n\u03b8\n!\n+\np\n1 \u2212\u03b1\u03b3z\n(2)\nwhere \u03b1\u03b3 and \u00af\u03b1\u03b3 = Q\u03b3\ni=1 \u03b1i are parameters in the diffusion\nprocess and z \u223cN(z; 0, I) is a noise.\nE. Training Objective\nTo achieve the agent type sensive performance, we use\na mixture of experts strucure, which includes a gate on the\ntop to switch the suitable model according to the certain\nagent type. Then the scene generation framework served by\nsymmetric models. Here for simplication purpose, we only\nillustrate one of them in details. To train a the dragtraffic\nmodel, we consider a two stage training strategy, where the\nfirst stage trains a denoising module and the second stage\nfocuses on a leapfrog initializer. The reason to use two stages\nis because the training of leapfrog initializer is more stable\ngiven fixed distribution P(\u03c41:F ), avoiding non-convergent\ntraining. The noise estimation loss is\nLNE = \u2225\u03f5 \u2212f\u03f5(\u03c41:F , fcontext(c), \u03b3 + 1)\u22252,\nwhere \u03b3 \u223cU{1, 2, \u00b7 \u00b7 \u00b7 , \u0393}, \u03f5 \u223cN(\u03f5; 0, I) and the diffused\ntrajectory \u03c4 \u03b3+1\n1:F = \u221a\u00af\u03b1\u03b3 \u2212\u03c4 0\n1:F + p1 \u2212\u00af\u03b1\u03b3\u03f5. We then back-\npropagate this loss and train the parameters in the context\nencoder fcontext(\u00b7) and the noise estimation module f\u03f5(\u00b7).\nIn the second stage, we optimize a the pretrained diffusion\nmodel with a trainable initializer model and frozen denoising\nmodules. For each sample, the loss function is\nL =\nLMSE + LNLL.\n(3)\nWe use the distance based loss to minimize the displacement\nerror and adopts the Negative Log-Likelihood Loss function\nto optimize scores, which are common used in motion\nforecasting task.\nV. EXPERIMENTAL RESULTS\nA. Dataset\nWe utilize the Waymo Open Dataset [30] to train Drag-\nTraffic, a traffic simulation framework that aims to provide\na more realistic and adaptable simulation experience. The\ndataset consists of around 70,000 scenarios, each with 20-\nsecond trajectories. To optimize the dataset for our purposes,\nwe split each 20-second scenario into 6-second intervals and\nremoved scenarios with less than 32 agents. We then cropped\na rectangular area with a 120-meter side length centered on\nthe ego agent and classified scenarios into three datasets:\nego-centered, cyclist-centered, and pedestrian-centered. We\nfurther filtered out scenarios with less than 30 frames and\ninvalid end points, resulting in 49,884, 29,046, and 9,344\ncases, respectively. We then split the remaining cases into\ntraining, non-overlapping validation, and test datasets in\nan 80%, 10%, 10% ratio. To ensure fair comparison with\nother methods, we benchmarked the trained models on the\ntest set and followed the placement and generation pipeline\nof trafficGen, which we considered a robust baseline. Our\nevaluation produced both quantitative and qualitative results.\nB. Metrics\nTo evaluate the performance of our framework, we em-\nployed two metrics: scenario collision rate (SCR) and motion\nforecasting related metrics. The SCR measures the consis-\ntency of the generated vehicle\u2019s behaviors by calculating the\naverage percentage of vehicles that collide with others in\neach scenario. We consider two vehicles as colliding if their\nbounding boxes overlap above a predefined IOU threshold.\nFor the open-loop evaluation, we used the common metrics\nMinADEk, MinFDEk, Heading error and Speed error, which\nare calculated based on the trajectory with the cloest endpoint\nto the ground truth over k predictions. Since our condition\ncontext includes endpoint information, we treated the point\nbefore the last one as the endpoint when calculating the\nMinFDE.\nC. Implementation details\nDuring the model training phase, we pre-train the condi-\ntional diffusion model for 100 epochs and then freeze its\nparameters. Similar to [29], we set \u03c4 = 5 and use the stan-\ndard U-net diffusion model. The total number of denoising\nsteps is 100 for both training and testing. For the Initializer\nmodel, we add an MLP layer on top of the backbone to\nencode the condition information of dimension 8 and obtain\nhidden features of length 1024, which are then concatenated\nwith the outputs of the state encoder and the lane encoder.\nWe fine-tune the diffusion model and the Initializer model\ntogether for 40 epochs. All other baselines that do not use\nthe diffusion model are trained for 150 epochs, and we select\nthe one with the lowest loss for experimental evaluation. All\nmodels are trained using learning rate decay of 0.0004 on 4\nRTX3090 and set the initial learning rate to 0.0003.\nD. Results and discussion\nWe hold the belief that an outstanding scene generator\nshould possess two essential characteristics. Firstly, it should\nhave the ability to generate realistic and reasonable tra-\njectories by providing the ground truth context. Similar to\nthe trajectory prediction task, we adopt displacement error\nmetrics to evaluate its performance. Secondly, a remarkable\nscene generator should offer a high degree of freedom and\ncontrollability, allowing users to generate realistic scenes\nwhile exploring rare and high-value corner cases. This is\nwhere DragTraffic excels in comparison to other existing\nframeworks. In the following sections, we will elaborate on\nthe experimental results, with a particular focus on these two\ncharacteristics.\nFig. 3: The demonstration of creating, editing and correction. We use colored boxes to represent Agents, different agents\nhave different sizes. A motorcycle will be a long bar, a pedestrian will be a square. A series of colored dots in front of the\nagent is the generated trajectory of the corresponding agent. The shadow represents the past actions.\n1) Scene Generation: To assess the diversity of scenario\ngeneration, we conducted three subsets of tests on MinADE\nand MinFDE for different agent types. The results, presented\nin the Table I, demonstrate that our approach achieves\ngood predictive performance for various agents and can\napproximate the ground truth. In contrast, plain TrafficGen\nperforms well for vehicle agents but exhibits significant\nperformance degradation for pedestrians and bicycles due\nto its reliance on vehicle data, which oversimplifies the\nproblem and reduces model generalizability. To ensure fair-\nness, we trained trafficgen on a mixed dataset, which also\nyielded promising results on some metrics. This highlights\nthe importance of establishing a framework for different\nagent types. DragTraffic outperforms other models on both\nvehicle and pedestrian datasets, thanks to its MOE structure,\nwhich enhances its generalizability. However, we observe\nTABLE I: Scene Generation Quality Evaluation\nDataset\nModel\nMin ADE6\nMean FDE6\nHeading Error\nSpeed Error\nTrafficGen\n3.32\n5.41\n0.05\n0.05\nTrafficGen Mixture Training with Condition\n3.09\n4.40\n0.04\n0.05\nVehicle\nDragTraffic with Condition\n2.53\n3.33\n0.05\n0.03\nTrafficGen\n6.50\n9.04\n0.10\n0.40\nTrafficGen Mixture Training with Condition\n2.05\n3.53\n0.02\n0.36\nPedestrian\nDragTraffic with Condition\n1.59\n2.74\n0.02\n0.40\nTrafficGen\n17.05\n23.62\n0.41\n0.31\nTrafficGen Mixture Training with Condition\n3.34\n4.31\n0.06\n0.21\nCyclist\nDragTraffic with Condition\n3.93\n6.21\n0.07\n0.37\nthat DragTraffic does not perform as well on the Cyclist\ndataset, possibly because the proportion of cyclists in the\nscene data is too small. In Table II, we present the results of\nthe Interaction Reasoning Evaluation with sampling intervals\nof 3s, 6s, and 9s. We collected 300 scenes with high inter-\nactivity between different agents for evaluation, including\n200 scenes related to pedestrians and 100 scenes related\nto cyclists. The performance of Dragtraffic under the three\nsampling conditions is superior to the baseline. Interestingly,\nwe observe that the SCR varies with the sampling interval,\nwhich is contrary to the results obtained in Trafficgen.\nHowever, we believe this is reasonable because multiple\nrollouts can introduce more cumulative errors.\n2) Scene Editing & Inpaiting: Figure 3 demonstrates the\nquality of scenes generated by DragTraffic based on existing\ndata. We use the current frame information in the dataset and\nthe information of the next nine seconds as conditions. The\nleft-turn and right-turn scenes at the intersection where the\nmost interactions occur are respectively shown in columns\n(a) and (b) of figure 3. These scenes reflect courtesy and\ncompetition for right of way among different agents. For\nexample, in (a), Agent No. 9 and Agent No. 1 engage in a\nfierce competition for the right of way, while Agent No. 0\ngives way. This shows that DragTraffic can simulate traffic\nparticipants in different situations under highly dynamic and\ncomplex traffic intersections to make reasonable, smooth, and\nrealistic future actions, reflecting real-world characteristics.\nNext, we verify that this capability is not limited to simple\nlog replay in more complex scenarios (c) and (d). We focus\non agent No. 1, which is waiting at an intersection for\nthe motorcycle in front to start. However, the movement\nof a large number of pedestrians around interferes with the\ndecision-making of agent No. 1, which has a conservative\ndriving style. As a result, it continues to watch the move-\nments of pedestrians even when it is already driving to the\nintersection, leading to the phenomenon of robot freezing,\nwhich is common in the fields of autonomous driving and\nTABLE II: Interaction Reasoning Evaluation\nRollout Interval\nModel\nSCR(%)\n3\nT.G. Mixture Training with Condition\n15.00\nD.T. with Condition\n9.20\n6\nT.G. Mixture Training with Condition\n14.43\nT.G. with Condition\n8.97\n9\nT.G. Mixture Training with Condition\n13.59\nT.G. with Condition\n3.33\nrobotics. To address this issue, we design similar scenarios by\nsetting the condition information for agent No. 1 (100 meters\nin front of it, longitudinal speed of 20m/s, and horizontal\nspeed of -2m/s) to inspire more proactive behavior. As shown\nin (d), the results generated by DragTraffic are subject to\nour good scene editing control. Note that we only show the\ncontrol of agent No. 1 for the convenience of explanation,\nbut in fact, DragTraffic allows us to control the generation\nof multiple agents simultaneously. Unlike other methods of\ncontrolling generation (such as complex optimization con-\nstraints and loss design), these manipulation processes only\nrequire simple dragging or typing interactions, demonstrating\nthe superiority of this framework in controlling degrees of\nfreedom.\nVI. FUTURE WORK\nAlthough our proposed framework has the potential to\ngenerate realistic and diverse traffic scenarios, there are\nstill areas that can be improved. For instance, we can use\npost-processing or sampling techniques to ensure that the\ngenerated driving actions meet dynamic constraints. Another\npromising direction for future work is to develop a more user-\nfriendly interaction approach, such as incorporating large lan-\nguage models for text to motion generation. Additionally, due\nto space limitations, we were only able to present a portion of\nthe editing potential. We encourage readers to explore more\nscene generation possibilities using Dragtraffic.\nVII. CONCLUSIONS\nIn this paper, we presented DragTraffic, a generalized,\npoint-based, and controllable traffic scene generation frame-\nwork based on conditional diffusion. Our framework ad-\ndresses the lack of controllability, accuracy, and versatility\nin existing scene generation methods, enabling non-experts\nto generate a variety of realistic driving scenarios for dif-\nferent types of traffic agents. We achieve this through an\nadaptive mixture expert architecture that uses a regression\nmodel to provide a general initial solution and a refine-\nment process based on the conditional diffusion model to\nensure diversity. The denoising process also introduces user-\ncustomized specific context through cross-attention, ensuring\nhigh controllability. Our experiments on a real-world driving\ndataset demonstrate that DragTraffic outperforms existing\nmethods in terms of authenticity, diversity, and freedom. We\nbelieve that our framework can significantly contribute to\nthe evaluation and training of autonomous driving systems\nby providing diverse and scalable corner cases.\nREFERENCES\n[1] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Fl\u00a8otter\u00a8od,\nR. Hilbrich, L. L\u00a8ucken, J. Rummel, P. Wagner, and E. Wiessner, \u201cMi-\ncroscopic traffic simulation using sumo,\u201d in 2018 21st International\nConference on Intelligent Transportation Systems (ITSC), 2018, pp.\n2575\u20132582.\n[2] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun,\n\u201cCARLA: An open urban driving simulator,\u201d in Proceedings of the\n1st Annual Conference on Robot Learning, 2017, pp. 1\u201316.\n[3] D. Chen, M. Zhu, H. Yang, X. Wang, and Y. Wang, \u201cData-driven\ntraffic simulation: A comprehensive review,\u201d 2023.\n[4] N. Montali, J. Lambert, P. Mougin, A. Kuefler, N. Rhinehart, M. Li,\nC. Gulino, T. Emrich, Z. Yang, S. Whiteson, et al., \u201cThe waymo open\nsim agents challenge,\u201d Advances in Neural Information Processing\nSystems, vol. 36, 2024.\n[5] J. Philion, X. B. Peng, and S. Fidler, \u201cTrajeglish: Learning the lan-\nguage of driving scenarios,\u201d arXiv preprint arXiv:2312.04535, 2023.\n[6] Z. Zhou, Z. Wen, J. Wang, Y.-H. Li, and Y.-K. Huang, \u201cQcnext: A\nnext-generation framework for joint multi-agent trajectory prediction,\u201d\narXiv preprint arXiv:2306.10508, 2023.\n[7] S. Shi, L. Jiang, D. Dai, and B. Schiele, \u201cMtr++: Multi-agent mo-\ntion prediction with symmetric scene modeling and guided intention\nquerying,\u201d 2023.\n[8] S. Tan, K. Wong, S. Wang, S. Manivasagam, M. Ren, and R. Urtasun,\n\u201cScenegen: Learning to generate realistic traffic scenes,\u201d 2021.\n[9] L. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou, \u201cTrafficgen: Learning\nto generate diverse and realistic traffic scenarios,\u201d in 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA). IEEE,\n2023, pp. 3567\u20133575.\n[10] H. Zhang, H. Song, S. Li, M. Zhou, and D. Song, \u201cA survey\nof controllable text generation using transformer-based pre-trained\nlanguage models,\u201d ACM Computing Surveys, vol. 56, no. 3, pp. 1\u2013\n37, 2023.\n[11] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d 2022.\n[12] A. R. et al, \u201cHierarchical text-conditional image generation with clip\nlatents,\u201d 2022.\n[13] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon,\n\u201cSdedit: Guided image synthesis and editing with stochastic differen-\ntial equations,\u201d 2022.\n[14] W. Peebles and S. Xie, \u201cScalable diffusion models with transformers,\u201d\n2023.\n[15] W. Ding, B. Chen, B. Li, K. J. Eun, and D. Zhao, \u201cMultimodal\nsafety-critical scenarios generation for decision-making algorithms\nevaluation,\u201d IEEE Robotics and Automation Letters, vol. 6, no. 2, pp.\n1551\u20131558, 2021.\n[16] Z.-H. Yin, L. Sun, L. Sun, M. Tomizuka, and W. Zhan, \u201cDiverse\ncritical interaction generation for planning and planner evaluation,\u201d in\n2021 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2021, pp. 7036\u20137043.\n[17] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov,\net al., \u201cMotiondiffuser: Controllable multi-agent motion prediction\nusing diffusion,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 9644\u20139653.\n[18] Z. Guo, X. Gao, J. Zhou, X. Cai, and B. Shi, \u201cScenedm: Scene-level\nmulti-agent trajectory generation with consistent diffusion models,\u201d\narXiv preprint arXiv:2311.15736, 2023.\n[19] Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone,\nand B. Ray, \u201cLanguage-guided traffic simulation via scene-level diffu-\nsion,\u201d in Conference on Robot Learning.\nPMLR, 2023, pp. 144\u2013177.\n[20] X. Pan, A. Tewari, T. Leimk\u00a8uhler, L. Liu, A. Meka, and C. Theobalt,\n\u201cDrag your gan: Interactive point-based manipulation on the generative\nimage manifold,\u201d 2023.\n[21] B. Varadarajan, A. Hefny, A. Srivastava, K. S. Refaat, N. Nayakanti,\nA. Cornman, K. Chen, B. Douillard, C. Lam, D. Anguelov, and\nB. Sapp, \u201cMultipath++: Efficient information fusion and trajectory\naggregation for behavior prediction,\u201d CoRR, vol. abs/2111.14973,\n2021. [Online]. Available: https://arxiv.org/abs/2111.14973\n[22] J. Gu, C. Sun, and H. Zhao, \u201cDensetnt: End-to-end trajectory\nprediction from dense goal sets,\u201d CoRR, vol. abs/2108.09640, 2021.\n[Online]. Available: https://arxiv.org/abs/2108.09640\n[23] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and\nB. Sapp, \u201cWayformer: Motion forecasting via simple efficient attention\nnetworks,\u201d in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA), 2023, pp. 2980\u20132987.\n[24] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,\n\u201cGOHOME:\ngraph-oriented\nheatmap\noutput\nfor\nfuture\nmotion\nestimation,\u201d CoRR, vol. abs/2109.01827, 2021. [Online]. Available:\nhttps://arxiv.org/abs/2109.01827\n[25] K. Mangalam, Y. An, H. Girase, and J. Malik, \u201cFrom goals,\nwaypoints & paths to long term human trajectory forecasting,\u201d\nCoRR,\nvol.\nabs/2012.01526,\n2020.\n[Online].\nAvailable:\nhttps:\n//arxiv.org/abs/2012.01526\n[26] W. Ding, W. Wang, and D. Zhao, \u201cMulti-vehicle trajectories gener-\nation for vehicle-to-vehicle encounters,\u201d in 2019 IEEE International\nConference on Robotics and Automation (ICRA), 2019.\n[27] G. Oh and H. Peng, \u201cCvae-h: Conditionalizing variational autoen-\ncoders via hypernetworks and trajectory forecasting for autonomous\ndriving,\u201d arXiv preprint arXiv:2201.09874, 2022.\n[28] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d 2020.\n[29] W. Mao, C. Xu, Q. Zhu, S. Chen, and Y. Wang, \u201cLeapfrog diffusion\nmodel for stochastic trajectory prediction,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2023, pp. 5517\u20135526.\n[30] W. LLC, \u201cWaymo open dataset: An autonomous driving dataset,\u201d\n2019.\n",
    "2309.09777": "DriveDreamer: Towards Real-world-driven World Models\nfor Autonomous Driving\nXiaofeng Wang*1 Zheng Zhu*1\f Guan Huang1,2 Xinze Chen1 Jiagang Zhu1 Jiwen Lu2\n1GigaAI\n2Tsinghua University\nProject Page: https://drivedreamer.github.io\nxd\nxd\nDriveDreamer\nxd\nDriveDreamer\nDifferent Driving Actions\nDifferent Text Prompts\n\u201cSunny\u201d\u201cRainy\u201d\u201cNight\u201d\nInitial Condition\nDriveDreamer\nDriving Video Generation with Traffic Condition and Different Text Prompts\nFuture Driving Video Generation with Action Interaction\nFuture Driving Action Generation\nInitial Driving Actions\nInitial Conditions\nInitial Condition\nFigure 1. DriveDreamer demonstrates a comprehensive understanding of driving scenarios. It excels in controllable driving video gener-\nation, aligning seamlessly with text prompts and structured traffic constraints. DriveDreamer can also interact with the driving scene and\npredict different future driving videos, based on input actions. Furthermore, DriveDreamer extends its utility to anticipate future actions.\nAbstract\nWorld models, especially in autonomous driving, are\ntrending and drawing extensive attention due to their ca-\npacity for comprehending driving environments. The estab-\nlished world model holds immense potential for the gen-\neration of high-quality driving videos, and driving poli-\ncies for safe maneuvering. However, a critical limitation\nin relevant research lies in its predominant focus on gam-\ning environments or simulated settings, thereby lacking the\nrepresentation of real-world driving scenarios. Therefore,\nwe introduce DriveDreamer, a pioneering world model en-\ntirely derived from real-world driving scenarios. Regarding\nthat modeling the world in intricate driving scenes entails\nan overwhelming search space, we propose harnessing the\n1\narXiv:2309.09777v2  [cs.CV]  27 Nov 2023\npowerful diffusion model to construct a comprehensive rep-\nresentation of the complex environment. Furthermore, we\nintroduce a two-stage training pipeline. In the initial phase,\nDriveDreamer acquires a deep understanding of structured\ntraffic constraints, while the subsequent stage equips it with\nthe ability to anticipate future states. The proposed Drive-\nDreamer is the first world model established from real-\nworld driving scenarios. We instantiate DriveDreamer on\nthe challenging nuScenes benchmark, and extensive exper-\niments verify that DriveDreamer empowers precise, con-\ntrollable video generation that faithfully captures the struc-\ntural constraints of real-world traffic scenarios. Addition-\nally, DriveDreamer enables the generation of realistic and\nreasonable driving policies, opening avenues for interac-\ntion and practical applications.\n1. Introduction\nSpurred by insights from AGI (Artificial General Intelli-\ngence) and the principles of embodied AI, a profound trans-\nformation in autonomous driving is underway. Autonomous\nvehicles rely on sophisticated systems that engage with and\ncomprehend the real driving world. At the heart of this evo-\nlution is the integration of world models [15,17\u201319]. World\nmodels hold great promise for generating diverse and real-\nistic driving videos, encompassing even long-tail scenarios,\nwhich can be utilized to train various driving perception ap-\nproaches. Furthermore, the predictive capabilities in world\nmodels facilitate end-to-end driving, ushering in a new era\nof autonomous driving experiences.\nDeriving\nlatent\ndynamics\nof\nworld\nmodels\nfrom\nvisual signals was initially introduced in video predic-\ntion [8, 11, 19].\nBy extrapolating from observed visual\nsequences, video prediction methods can infer future states\nof the environment, effectively modeling how objects and\nentities within a scene will evolve over time.\nHowever,\nmodeling the intricate driving scenarios in pixel space is\nchallenging due to the large sampling space [5, 7].\nTo\nalleviate this problem, recent research endeavors have\nsought innovative strategies to enhance sampling efficiency.\nISO-Dream [52] explicitly disentangles visual dynamics\ninto controllable and uncontrollable states.\nMILE [29]\nstrategically\nincorporates\nworld\nmodeling\nwithin\nthe\nBird\u2019s Eye View (BEV) semantic segmentation space,\ncomplementing world modeling with imitation learning.\nSEM2 [13] further extends the Dreamer framework into\nBEV segmentation maps, utilizing Reinforce Learning\n(RL) for training. Despite the progress witnessed in world\nmodels, a critical limitation in relevant research lies in its\npredominant focus on simulation environments.\nIn this paper, we propose DriveDreamer, which pioneers\nthe construction of comprehensive world models from real\ndriving videos and human driver behaviors. Considering the\nintricate nature of modeling real-world driving scenes, we\nintroduce the Autonomous-driving Diffusion Model (Auto-\nDM), which empowers the ability to create a comprehen-\nsive representation of the complex driving environment. We\npropose a two-stage training pipeline.\nIn the first stage,\nwe train Auto-DM by incorporating traffic structural infor-\nmation as intermediate conditions, which significantly en-\nhances sampling efficiency. Consequently, Auto-DM ex-\nhibits remarkable capabilities in comprehending real-world\ndriving scenes, particularly concerning the dynamic fore-\nground objects and the static background. In the second-\nstage training, we establish the world model through video\nprediction. Specifically, driving actions are employed to it-\neratively update future traffic structural conditions, which\nenables DriveDreamer to anticipate variations in the driving\nenvironment based on different driving strategies. More-\nover, DriveDreamer extends its predictive prowess to fore-\nsee forthcoming driving policies, drawing from historical\nobservations and Auto-DM features. Thus creating a exe-\ncutable, and predictable driving world model.\nThe main contributions of this paper can be summa-\nrized as follows: (1) We introduce DriveDreamer, which\nis the first world model derived from real-world driving\nscenarios. DriveDreamer can jointly enable the generation\nof high-quality driving videos and reasonable driving poli-\ncies. (2) To enhance the comprehension of real-world driv-\ning scenes and expedite the world model convergence, we\nintroduce the Autonomous-driving Diffusion Model and a\ntwo-stage training pipeline. The first-stage training enables\nthe comprehension of traffic structural information, and the\nsecond-stage video prediction training empowers the pre-\ndictive capacity. (3) DriveDreamer can controllably gener-\nate driving scene videos that are highly aligned with traffic\nconstraints (see Fig. 1), enhancing the training of driving\nperception methods (e.g., 3D detection). Besides, Drive-\nDreamer can generate future driving policies based on his-\ntorical observations and Auto-DM features. Notably, Drive-\nDreamer achieves promising planning results in open-loop\nassessments on the nuScenes dataset.\n2. Related Work\n2.1. Diffusion Model\nDiffusion models represent a family of probabilistic gen-\nerative models that progressively introduce noise to data\nand subsequently learn to reverse this process for the pur-\npose of generating samples [73]. These models have re-\ncently garnered significant attention due to their exceptional\nperformance in various applications, setting new bench-\nmarks in image synthesis [1, 14, 49, 55, 57], video gener-\nation [21, 23, 35, 60, 67, 74], and 3D content generation\n[6, 43, 53, 69]. To enhance the controllable generation ca-\npability, ControlNet [76], GLIGEN [42], T2I-Adapter [48]\n2\nand Composer [32] have been introduced to utilize various\ncontrol inputs, including depth maps, segmentation maps,\ncanny edges, and sketches. Concurrently, BEVControl [72],\nMagicDrive [12] and DrivingDiffuson [41] incorporate lay-\nout conditions to enhance image generation. The funda-\nmental essence of diffusion-based generative models lies in\ntheir capacity to comprehend and understand the intricacies\nof the world. Harnessing the power of these diffusion mod-\nels, DriveDreamer seeks to comprehend the complex realm\nof autonomous-driving scenarios.\n2.2. Video Generation\nVideo generation and video prediction are effective\napproaches to understanding the visual world.\nIn the\nrealm of video generation, several standard architectures\nhave been employed, including Variational Autoencoders\n(VAEs) [8,28], auto-regressive models [34,56,61,70], flow-\nbased models [40], and Generative Adversarial Networks\n(GANs) [46, 58, 62, 65]. Recently, the burgeoning diffu-\nsion models [9, 24, 25, 49, 50, 57] have also been extended\nto the domain of video generation. Video diffusion mod-\nels [21, 23, 35, 60, 67, 74] exhibit higher-quality video gen-\neration capabilities, producing realistic frames and transi-\ntions between frames while offering enhanced controllabil-\nity.\nThey accommodate various input control conditions\nsuch as text, canny, sketch, semantic maps, and depth maps.\nVideo prediction models represent a specialized form\nof video generation models, sharing numerous similari-\nties. In particular, video prediction involves anticipating\nfuture video changes based on historical video observa-\ntions [8, 11, 19, 27, 64]. DriveGAN [36] establishes asso-\nciations between driving actions and pixels, predicting fu-\nture driving videos by specifying future driving policies. In\ncontrast, DriveDreamer incorporates structured traffic con-\nditions, text prompts, and driving actions as inputs, empow-\nering precise, realistic video and action generation that are\nfaithfully aligned with real-world driving scenarios.\n2.3. World Models\nWorld models have been extensively explored in model-\nbased imitation learning, demonstrating remarkable suc-\ncess in various applications [15\u201320, 37, 44, 59, 71]. These\napproaches typically leverage Variational Autoencoders\n(VAE) [39] and Long Short-Term Memory (LSTM) [26]\nto model transition dynamics and rendering functionality.\nWorld methods target at establishing dynamic models of en-\nvironments, enabling agents to be predictive of the future.\nThis aspect is of paramount importance in autonomous driv-\ning, where precise predictions about the future are essential\nfor safe maneuvering. However, constructing world mod-\nels in autonomous driving presents unique challenges, pri-\nmarily due to the high sample complexity inherent in real-\nworld driving tasks [5]. To address these problems, ISO-\nDriveDreamer\nFirst-stage Training\nSecond-stage Training\nSingle Condition\nSequential Conditions\nDriveDreamer\nImage Generation\nVideo Generation\nStep 1\nStep 2\nImage Supervision\nVideo Supervision\nSingle Condition\nSequential Driving \nActions\nDriveDreamer\nFuture Driving \nActions\nFuture Video \nGeneration\nVideo Supervision\nAction Supervision\n( ActionFormer )\n( Auto-DM )\nFigure 2. Two-stage training pipeline of DriveDreamer.\nDream [52] introduces an explicit disentanglement of visual\ndynamics into controllable and uncontrollable states. MILE\n[29] strategically incorporates world modeling within the\nBEV semantic segmentation space, enhancing world mod-\neling through imitation learning. SEM2 [13] extends the\nDreamer framework into BEV segmentation maps, em-\nploying reinforcement learning for training.\nDespite the\nprogress witnessed in world models, a critical limitation in\nrelevant research lies in its predominant focus on simulation\nenvironments. The transition to real-world driving scenar-\nios remains an under-explored frontier.\n3. DriveDreamer\nThe overall framework of DriveDreamer is depicted in\nFig 3.\nThe framework begins with an initial reference\nframe I0 and its corresponding road structural informa-\ntion (i.e., HDMap H0 and 3D box B0). Within this con-\ntext, DriveDreamer leverages the proposed ActionFormer\nto predict forthcoming road structural features in the latent\nspace.\nThese predicted features serve as conditions and\nare provided to Auto-DM, which generates future driving\nvideos. Simultaneously, the utilization of text prompts al-\nlows for dynamic adjustments to the driving scenario style\n(e.g., weather and time of the day).\nMoreover, Drive-\nDreamer incorporates historical action information and the\nmulti-scale latent features extracted from Auto-DM, which\nare combined to generate reasonable future driving actions.\nIn essence, DriveDreamer offers a comprehensive frame-\nwork that seamlessly integrates multi-modal inputs to gen-\nerate future driving videos and driving policies, thereby ad-\nvancing the capabilities of autonomous-driving systems.\nRegarding the extensive search space of establishing\nworld models in real-world driving scenarios, we introduce\na two-stage training strategy for DriveDreamer. This strat-\negy is designed to significantly enhance sampling efficiency\nand expedite model convergence. The two-stage training is\n3\nText\n\"A realistic\ndriving scene\u00a0\"\nHDMap\n3D Box\nReference Image\nActionFormer\nActions\nVideo\nDecoder\nAction\nDecoder\nEnc\nEnc\nEnc\nCLIP\nCLIP Embedding\nEnc\nReference Style Embedding\nAutonomous-driving\nDiffusion Model\nDenoising\nCondition Input\nFuture Driving Actions\nOutput\nTemporal-attention\nCross-attention\nGated Self-attention\nCross-Attention\nSelf-Attention\nCross-Attention\nSelf-Attention\nCross-Attention\nSelf-Attention\nFuture Driving Videos\nFigure 3. Overall framework of DriveDreamer. The framework initiates with reference frame I0 and road structural information (i.e.,\nHDMap H0 and 3D box B0). DriveDreamer employs the ActionFormer to predict future road structural features, which serve as conditions\nprovided to Auto-DM, generating future driving videos \u02c6IN\u22121\ni=0 . Additionally, text prompts enable dynamic scenario style adjustments. The\nmodel integrates past driving actions and multi-scale features from Auto-DM to generate plausible future driving actions \u02c6AN+M\ni=N .\nillustrated in Fig. 2. There are two steps in the first-stage\ntraining. Step 1 involves utilizing the single-frame struc-\ntured condition, which guides DriveDreamer to generate\ndriving scene image, facilitating its comprehension of struc-\ntural traffic constraints. Step 2 extends its understanding\ninto video generation. The second-stage training enables\nDriveDreamer to interact with the environment and predict\nfuture states effectively. This phase takes an initial frame\nimage along with its corresponding structured information\nas input. Simultaneously, sequential driving actions are pro-\nvided, with the model expected to generate future driving\nvideos and future driving actions. In the following sections,\nwe delve into the specifics of the model architecture and\ntraining pipelines.\n3.1. First-stage Training\nAuto-DM. In DriveDreamer, we introduce Auto-DM, to\nmodel and comprehend driving scenarios from real-world\ndriving videos.\nIt is noted that comprehending driving\nscenes solely from pixel space presents challenges due to\nextensive search space in real-world driving scenarios. To\nmitigate this, we explicitly incorporate structured traffic in-\nformation as conditional inputs.\nThe overall structure of Auto-DM is illustrated in Fig. 4,\nwhere traffic conditions are projected onto the image plane,\ngenerating HDMap conditions {Hi}N\u22121\ni=0\n\u2208RN\u00d7H\u00d7W \u00d73,\nand 3D boxes conditions {Bi}N\u22121\ni=0\n\u2208RN\u00d7NB\u00d716, along\nwith the box categories {Ci}N\u22121\ni=0\n\u2208RN\u00d7NB (N is the\nnumber of video frames, and NB is the predefined maxi-\nmum box numbers with zero padded). In this following,\nunless specified, the subscript i is omitted for readability.\nTo enable controllability, the spatially aligned conditions H\nare encoded by convolution layers and then concatenated\nwith Zt, where {Zt}N\u22121\ni=0 are noisy latent features generated\nby the forward diffusion process [57]. For position condi-\ntions (i.e., 3D boxes) that are not spatially aligned with Zt,\nwe first aggregate position embeddings Hp:\n  H ^p = \\m athcal {F}_\\alpha ( [ C_e,\\text {Fourier}(B) ] ), \n(1)\nwhere Fa is MLP layers, Ce is CLIP [54] embed box cat-\negories features, Fourier(\u00b7) is Fourier embedding [47], and\n[\u00b7] is the concatenation operation. Then gated self-attention\n[42] is leveraged to integrate position embeddings Hp with\nvisual signals v from the original UNet features [57]:\n  v  = v + \\ t ext {tanh }(\\eta )\\cdot \\text {TS}(\\mathcal {F}_s([v, H^p])), \n(2)\nwhere \u03b7 is a learnable parameter, Fs is self-attention, and\nTS(\u00b7) is the token selection operation that considers visual\ntokens only [42].\nTo further empower Auto-DM with comprehension of\ndriving dynamics, we introduce temporal attention layers\nFt to enhance frame coherence in the generated videos:\n  \\ma t hcal {F}_t(v) = \\tex t  {Reshape}(\\mathcal {F}_s(\\text {Reshape}(v+\\mathcal {T}_{\\text {pos}}))), \n(3)\nwhere\nwe\nfirst\nreshape\nthe\nvisual\nsignal\nv\nfrom\nRN\u00d7C\u00d7H\u00d7W to RC\u00d7NHW .\nThe shape transformation\n4\nText\n\"A realiVWic driYing\nVcene, da\\Wime\"\nHDMaps\n3D Boxes\nGT Video\nEnc\nEnc\nCLIP\nSpatiall\\ Aligned\nConditions\nVWepV\nTemSRUal-aWWenWiRn\nCURVV-aWWenWiRn\nGaWed Self-aWWenWiRn\nEnc\nDec\nGenerated Video\nCRQdiWiRQV\nDiffXViRQ SWeSV\nLoVV\nDenoiVing\nPosition Embedding\nCLIP Embedding\nPUed\nNRiVe\nGaXVVian\nNRiVe\n\"Car /\u00acPedeVWUian/...\"\nCLIP\nFigure 4. Overall structure of the Auto-DM. Auto-DM takes three types of control conditions as inputs. Spatially aligned conditions\n(i.e., HDMap HN\u22121\ni=0 ), are concatenated with noise images and fed into the diffusion steps. Position conditions, represented by 3D boxes\nBN\u22121\ni=0 and their labels, are flattened and utilized in the gated self-attention. Text prompts are incorporated into diffusion steps using cross-\nattention, influencing the style of the generated driving video. Temporal attention layers are employed to ensure the consistency of the\ngenerated video frames. The diffusion steps estimate noise and generate loss with the input noise to optimize Auto-DM.\nfacilitates the frame-wise self-attention layers Fs to learn\ninter-frame dynamics. Tpose denotes temporal position em-\nbeddings that are encoded by sinusoidal function [2]. Fi-\nnally, we restore the visual signal to its original dimensions,\nthus ensuring the feature integrity. Notably, the same archi-\ntecture can be extended to generate multi-view images (see\nFig. 6), where the Fs solely attends to neighbor views. Ad-\nditionally, a stack of frame-wise attention and view-wise at-\ntention contributes to multi-view video generation (see sup-\nplement for more details).\nFurthermore, cross-attention layers [57] are utilized to\nfacilitate feature interactions between text inputs and visual\nsignals, empowering text descriptions to influence driving\nscene attributes such as weather and time of day. In the\nnext, we will elaborate on the first-stage training pipeline,\nwhich involves two steps.\nStep 1 training. The Auto-DM incorporates input solely\nfrom a single frame of structured traffic conditions, coupled\nwith supervision from a single-frame image. For structured\ntraffic conditions, HDMaps and 3D boxes are obtained ei-\nther from human annotations or pertained perception meth-\nods (e.g., LAV [4], BEVerse [77], UniAD [31]). Then three-\nchannel HDMaps (lane boundary, lane divider, and pedes-\ntrian crossing) and eight-corner 3D boxes are projected onto\nthe image plane to generate corresponding conditions. No-\ntably, during step 1 training, temporal attention layers are\nomitted, which enables the network to focus exclusively\non learning the traffic structural constraints, expediting the\nconvergence of the training process.\nStep 2 training.\nThe Auto-DM incorporates input from\nmultiple frames of structured traffic conditions and is su-\npervised using driving videos. In contrast to step 1, learning\nfrom videos allows Auto-DM to gain a deeper understand-\ning of the intricate motion transitions in driving scenarios.\nBuilding upon the pretrained models established in step 1,\nstep 2 incorporates temporal attention layers into the model\narchitecture. These additional parameters enable the Auto-\nDM to focus on the temporal dynamics present in the input\ndata, further enhancing its ability to capture and interpret\nthe nuanced temporal aspects of driving scenes.\nIn step 1 and step 2 training, the proposed Auto-DM is\ntrained using the same noise schedule as the underlying im-\nage model [57]. Specifically, the forward process gradually\nadds noise \u03f5 to the latent feature Z0, resulting in the noisy\nlatent feature ZT . Then we train \u03f5\u03d5 to predict the noise we\nadded, and the trainable parameters \u03d5 are optimized via:\n  \\\nm i n  _{\\phi  } \\mathc\na\nl { L} =\\ma th bb {\nE\n}\n_{\\mathcal {Z}_{0}, \\epsilon \\sim \\mathcal {N}(\\mathbf {0}, \\mathbf {I}), t, c}\\left [\\left \\|\\epsilon -\\epsilon _{\\phi }\\left (\\mathcal {Z}_{t}, t, c\\right )\\right \\|_{2}^{2}\\right ], \n(4)\nwhere \u03d5 denotes the trainable parameters involved in the\ngated self-attention, temporal attention, and cross-attention\nlayers, and time step t is uniformly sampled from [1, T].\n3.2. Second-stage Training\nBased on the first-stage training, DriveDreamer has ob-\ntained comprehension of the structured traffic information.\nHowever, the desired world model should also be predictive\nof the future and can interact with the environment. There-\nfore, we embark on the second phase of our approach. In\nthis phase, we leverage the video prediction task to establish\n5\nSelf-Attention\nMLP\nEnc\nEnc\nSelf-Attention\nCross-Attention\nGRU Blocks\nEnc\nConcat\nDec\nDec\nDec\n\"A realistic driving\nscene, daytime\"\nAutonomous-driving Diffusion Model\nCLIP\nEnc\nObserving\nPredicting\nSelf-Attention\nCross-Attention\nGRU Blocks\nEnc\nConcat\nFigure 5. Overall structure of ActionFormer. The initial structural conditions H0 and B0 are first encoded and flattened into a 1D latent\nspace. These latent features are then concatenated and processed through self-attention and MLP layers, generating the hidden state.\nCross-attention layers establish associations between hidden states and driving actions. Gated Recurrent Units (GRUs) are employed to\niteratively predict future hidden states. These predicted hidden states are further concatenated with action features and decoded into future\ntraffic structural conditions to be fed into Auto-DM.\nthe driving world model. Specifically, the video prediction\ntask entails providing an initial observation I0, H0, B0, as\nwell as driving actions {Ai}T \u22121\ni=0 , with the desired outcome\nbeing the future driving videos {Ii}T\ni=1, and future driving\nactions {Ai}T +N\ni=T .\nActionFormer. Recall that the trained Auto-DM can gen-\nerate driving videos {Ii}T\ni=0 based on sequential structured\ninformation {Hi}T\ni=0, {Bi}T\ni=0. However, in the video pre-\ndiction task, future traffic structural conditions beyond the\npresent timestamp is unavailable. To address this challenge,\nwe introduce the ActionFormer, which leverages driving ac-\ntions {Ai}T \u22121\ni=0 to iteratively predict future structural condi-\ntions. The overall architecture of ActionFormer is in Fig. 5.\nFirstly the initial structural conditions H0, B0 are encoded\nand flattened into 1D latent space. The latent features are\nconcatenated and aggregated by self-attention and MLP lay-\ners to generate the hidden state h0. Subsequently, cross-\nattention layers Fca are utilized to construct associations\nbetween hidden states and driving actions. Then latent vari-\nable st is parameterized as:\n  \\ m ath bf {s}_{ t} \\ s im  \\mathca l {N }\\ left (\\mu _{\\theta }\\left (\\mathcal {F}_{ca}(\\mathbf {h}_t, A_t)\\right ), \\sigma _{\\theta }\\left (\\mathcal {F}_{ca}(\\mathbf {h}_t, A_t)\\right ) \\boldsymbol {I}\\right ), \n(5)\nwhere \u00b5\u03b8, \u03c3\u03b8 are layers to learn Gaussian parameters. To\npredict future hidden states, we employ Gated Recurrent\nUnits (GRUs) to iteratively make updates:\n  \\m a thbf {h} _{t+1} = \\mathcal {F}_\\text {GRU}(\\mathbf {h}_t, \\mathbf {s}_t). \\label {eq:hidden} \n(6)\nThese hidden states are concatenated with action features\nand are decoded into future traffic structural conditions. It\u2019s\nnoted that the Actionformer forecasts future traffic condi-\ntions at the feature level, which mitigates noise interference\nat the pixel level, resulting in more robust predictions. Be-\nsides the traffic structural conditions generated by Action-\nformer and the text prompt condition, we process the refer-\nence image condition I0 similar to [2]. Based on the above\nconditions, we extend Auto-DM to jointly generate future\ndriving videos {Ii}T\ni=1 and driving actions {Ai}T +N\ni=T . We\nformalize this process as a generative probabilistic model,\nwhere the joint probability can be factorized as:\n  \\beg i n {a lig ned}  &p\\l eft\n ( I_{0:  T} , A_ { 0: T + N}, \\ma thbf  {h }_{\n0\n:\n T}\n,  \\ma th b f {s} _{0:T -1}  \\right )\\\\ &\\quad =p\\left (I_{1:T}, A_{T:T+N} \\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right )\\\\ &\\quad \\quad \\prod _{t=0}^{T} p\\left (\\mathbf {h}_{t}, \\mathbf {s}_{t} \\mid \\mathbf {h}_{t-1}, \\mathbf {s}_{t-1}, A_{t}\\right ), \\end {aligned} \n(7)\nwhere\n  \\be gi n  {ali gned}  &p\n\\ l eft  (\\mat hbf { h }_{ t }, \\ma\nt hbf { s }_ {t } \\ mid \\ math bf {h}_ {t- 1},\n \\ mathb f  {s} _ {t-1 }, A_{t }\\r igh\nt  ) \\\\  & \\ quad  \\qua d \\ quad  =p \\le ft (\\mathbf {h}_{t} \\mid \\mathbf {h}_{t-1}, \\mathbf {s}_{t-1}\\right ) p\\left (\\mathbf {s}_{t} \\mid \\mathbf {h}_{t}, A_{t}\\right ) \\\\ &p\\left (I_{1:T}, A_{T:T+N} \\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right ) \\\\ &\\quad \\quad \\quad =p\\left (I_{1:T}\\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right ) \\\\ &\\quad \\quad \\quad \\quad \\quad p\\left (A_{T:T+N}\\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right ). \\end {aligned} \n(8)\n6\nConsidering updating hidden states p(ht | ht\u22121, st\u22121) is\na deterministic process (Eq. 6), only latent variables st are\nneeded to be inferred to maximize the marginal likelihood\nof observation p(I1:T , AT :T +N). Therefore, variational dis-\ntribution qvd is introduced to conduct variational inference:\n  \\ b e gin { a lign e d} q _ \\tex t {\nv\nd\n}\n &\\\nt ria n gleq q\\lef t  (\\ m athb f {h }_{1: T}, \\mathbf {s}_{1: T} \\mid I_{0: T}, A_{0: T+N}\\right )\\\\ &=\\prod _{t=1}^{T} q\\left (\\mathbf {h}_{t} \\mid \\mathbf {h}_{t-1}, \\mathbf {s}_{t-1}\\right ) q\\left (\\mathbf {s}_{t} \\mid I_{\\leq t}, A_{<t}\\right ), \\end {aligned} \n(9)\nwhere q (ht | ht\u22121, st\u22121) = p (ht | ht\u22121, st\u22121). Similar\nto [29], the variational lower bound can be derived as:\n  \\ b egin { al ig ned }\n &\\log p \\le ft (I _{1: T},\\rig h t . \\ l eft .  A_{ T: T+N} \\ri ght\n \n)\\\ng\ne \\\\ &\\mathbb {\nE }_{ q \\le ft  ( \\ math b f {h }_{ 1: T }, \\ma\nt\nhb\nf\n {s}_{ 1: T} \\mid\n I_{0:T}, A_{0:T+N}\\right )}[\\underbrace {\\log p\\left (I_{1:T} \\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right )}_{\\text {video prediction }}\\\\ &\\quad +\\underbrace {\\log p\\left (A_{T:T+N} \\mid \\mathbf {h}_{0:T}, \\mathbf {s}_{0:T-1}, A_{0:T-1}, I_0\\right )}_{\\text {action predcition}}]. \\label {eq:vi} \\end {aligned} \n(10)\nNote that the posterior and prior matching [29] is not in-\ncluded, as we empirically find the simplified variational\nlower bound produces similar plausible results. In Eq. 10,\nthe video prediction and action prediction parts can be mod-\neled by Gaussian distributions N(G(h0:T , A0:T \u22121, I0), I)\nand Laplace distribution Laplace(\u03c0(h0:T , A0:T \u22121, I0), 1).\nTherefore, we employ mean-squared error and L1 loss to\noptimize the video prediction training. G, \u03c0 are learnable\nlayers involved in ActionFormer, Auto-DM, video decoder\n(i.e., VAE decoder) and action decoder. For action predic-\ntion details, we first pool multi-scale UNet features from\nAuto-DM. The pooled features are concatenated with histor-\nical action features, which are then decoded by MLP layers\nto generate future driving actions.\nBased on the two-stage training, DriveDreamer has ac-\nquired a comprehensive understanding of the driving world,\nencompassing the structural constraints of traffic, predic-\ntions of future driving states, and interaction with the es-\ntablished world model.\n4. Experiment\n4.1. Experiment Details\nDataset. The training data is sourced from the real-world\ndriving dataset nuScenes [3], comprising a total of 700\ntraining videos and 150 validation videos. Each video in-\ncludes \u223c20 seconds of footage captured by six surround-\nview cameras. The videos have a frame rate of 12Hz, re-\nsulting in \u223c1M video frames available for training. During\nthe first-stage training, we utilize the nuScenes-devkit [51]\nto acquire HDMap annotations (lane boundary, lane divider,\nand pedestrian crossing) corresponding to 12Hz frames,\nwhich are then projected onto the image plane. Consider-\ning the nuScenes dataset only provides 2Hz 3D bounding\nbox annotations, we supplement this with 12Hz bounding\nbox annotations from [68]. In the second-stage training, we\nemploy the yaw angle and velocity of the ego-car as the\ndriving action inputs. Besides, we extract scene description\ninformation (e.g., weather and time) from the nuScenes an-\nnotation, which serves as text conditions.\nTraining. The proposed Auto-DM is built upon Stable Dif-\nfusion v1.4 [57], whose original parameters are frozen. In\nstep 1 of first-stage training, our model is trained for 40\nepochs with a batch size of 16. In step 2, Auto-DM is trained\nfor 10 epochs with a batch size of 1, with video frame length\nN = 32, and spatial size of 448\u00d7256. During second-stage\nvideo prediction training, our model predicts 16 frame driv-\ning videos I1:16 and 16 future driving actions I17:32, and the\nmodel is trained for 10 epochs on a batch size of 1. All the\nexperiments are conducted on A800 GPUs, and we use the\nAdamW optimizer [38] with a learning rate 5 \u00d7 10\u22125.\nEvaluation.\nWe conducted a comprehensive evaluation\nof the proposed DriveDreamer, employing both qualita-\ntive and quantitative assessments. We utilized frame-wise\nFr\u00b4echet Inception Distance (FID) [22] and Fr\u00b4echet Video\nDistance (FVD) [63] to evaluate the generation quality,\nwhere the evaluated image is resized to 448 \u00d7 256. Be-\nsides, to verify the generated images enhance the training\nof driving perception methods, DriveDreamer is evaluated\nthrough 3D object detection, with FCOS3D [66] and BEV-\nFusion [45] as baseline methods. Furthermore, we test the\nperformance of driving policy generation. Following the\nsettings in [30], we evaluate output driving trajectories for\nfuture 3 seconds.\n4.2. Controllable Driving Video Generation\nThe proposed DriveDreamer exhibits a profound com-\nprehension of driving scenarios, capable of controllably\ngenerating diverse driving videos. In this subsection, we\nfirst demonstrate that, based on first-stage training, Drive-\nDreamer can generate diverse driving videos under struc-\ntured traffic conditions. Besides, we verify that the gener-\nated images can enhance the training of driving perception\nmethods. Furthermore, DriveDreamer showcases its versa-\ntility by responding to different input actions, allowing for\nthe control of the vehicle\u2019s trajectory and consequently gen-\nerating diverse driving videos.\nAs shown in Fig. 1 and Fig 6, DriveDreamer exhibits\nproficiency in producing images and videos that adhere\nmeticulously to structured traffic conditions (more visual-\nizations are in supplement). Significantly, we can also ma-\nnipulate the text prompt to induce variations in the gener-\nated videos, encompassing changes in weather and time of\nday. To further validate the generation quality, we extract\n4K traffic conditions (from the nuScenes training set) to\ngenerate driving images. The generated images are com-\nbined with real images for training the 3D detection task.\nResults in Tab. 1 indicate that training with our synthetic\n7\nGround Truth images\nGenerated images\nGround Truth images\nGenerated images\nFigure 6. Visualizations of generated multi-view images, where the generation conditions (HDMaps, 3D boxes) are from nuScenes valida-\ntion set. Regions highlighted by red rectangles and yellow circles indicate that the generated images share multi-view consistency and are\naligned with ground truth conditions.\nMethods\nResolution\nData\nmAP (\u2191)\nNDS (\u2191)\nFCOS3D [66]\n1600 \u00d7 900\nw/o synthetic data\n30.2\n38.1\nFCOS3D [66]\n1600 \u00d7 900\nw 4K synthetic data\n30.9 (+0.7)\n38.3 (+0.2)\nBEVFusion [45]\n704 \u00d7 256\nw/o synthetic data\n32.8\n37.6\nBEVFusion [45]\n704 \u00d7 256\nw 4K synthetic data\n35.8 (+3.0)\n39.5 (+1.9)\nTable 1. Performance of synthetic data augmentation on training\n3D object detection.\ndata significantly enhances the performance of 3D detec-\ntion. Specifically, compared with training without synthetic\ndata, the mAP metrics of FCOS3D and BEVFusion are im-\nproved by 0.7 and 3.0.\nIn addition to the utilization of structured traffic condi-\ntions for generating driving videos, DriveDreamer exhibits\nthe capability to diversify the generated driving videos by\nadapting to different driving actions. As depicted in Fig. 1\n(more visualizations are in supplement), starting from an\ninitial frame paired with its corresponding structural infor-\nmation, DriveDreamer can generate distinct videos based\non various driving actions, such as videos depicting left and\nright turns. In summary, DriveDreamer excels in produc-\ning a wide spectrum of driving scene videos, characterized\nby both high controllability and diversity.\nThus, Drive-\nDreamer holds promise for training autonomous-driving\nsystems across a wide range of tasks, encompassing even\ncorner cases and long-tail scenarios.\nIn the quantitative experiment, we extract ego-car driv-\ning actions from the nuScenes validation set as conditions\nto generate driving videos. For comparison, we train Drive-\nGAN [36] on the nuScenes dataset, employing the same\ntraining settings as those used for Drivedreamer. Besides,\nwe train Drivedreamer without ActionFormer as a baseline\nMethods\n1st-stage train\n2nd-stage train\nFID (\u2193)\nFVD (\u2193)\n(Auto-DM)\n(ActionFormer)\nDriveGAN [36]\n-\n-\n27.8\n390.8\nDriveDreamer\n15.9\n363.3\nDriveDreamer\n\u2713\n15.3\n349.6\nDriveDreamer\n\u2713\n\u2713\n14.9\n340.8\nTable 2. Comparison of generation quality on nuScenes validation.\n(specifically, the action features are directly concatenated\nwith the zero-padded structured traffic conditions). The re-\nsults are presented in Tab. 2, where we evaluate the quality\nof generated videos. Notably, our approach without first-\nstage training achieves superior FID and FVD scores com-\npared to DriveGAN. This observation underscores the ef-\nfectiveness of leveraging a powerful diffusion model in vi-\nsually comprehending driving scenarios. Furthermore, our\nfindings reveal that Drivedreamer after first-stage training,\nexhibits an improved understanding of the structured in-\nformation within driving scenes, resulting in higher-quality\nvideo generation. Lastly, we observe that the proposed Ac-\ntionFormer effectively leverages the traffic structural infor-\nmation knowledge acquired during the first-stage training.\nCompared to the concatenation baseline approach, the Ac-\ntionFormer iteratively updates future structured information\nbased on input actions, which further enhances the quality\nof generated videos.\n4.3. Driving Action Generation\nIn addition to its capacity for generating highly control-\nlable driving videos, DriveDreamer demonstrates the abil-\nity to predict reasonable driving actions. As depicted in\n8\nMethod\nVisual Info.\nAction Info.\nL2 Avg. (m)\nCol. Avg. (%)\nST-P3 [30]\n\u2713\n2.11\n0.71\nUniAD [31]\n\u2713\n1.65\n0.31\nAD-MLP [75]\n\u2713\n0.29\n0.19\nVAD [33]\n\u2713\n\u2713\n0.37\n0.14\nDriveDreamer\n\u2713\n\u2713\n0.29\n0.15\nTable 3. Open-loop planning performance on nuScenes validation\nset. The evaluation settings are the same as ST-P3 [30].\nFig. 1, provided with an initial frame condition and past\ndriving actions, DriveDreamer can generate future driving\nactions that align with real-world scenarios . Furthermore,\nwe conduct a quantitative assessment of the prediction ac-\ncuracy. Specifically, MLP layers [75] are utilized to encode\npast driving action information. Additionally, multi-scale\nUNet features are pooled as visual cues. The two modality\nfeatures are then concatenated to learn future driving trajec-\ntories (more implementation details are in supplement). The\nresults of open-loop evaluation on the nuScenes dataset are\npresented in Tab. 3. Remarkably, the average L2 trajectory\nerror of DriveDreamer is merely 0.29m, surpassing the per-\nformance of the multi-modality method VAD [33]. In addi-\ntion, DriveDreamer relatively decreases the average colli-\nsion rate reported in [75] by 21%, confirming that the visual\nfeatures learned by DriveDreamer contribute to end-to-end\nautonomous driving, thereby enhancing driving safety.\n5. Discussion and Conclusion\nDriveDreamer represents a significant advancement in\nthe field of world modeling, particularly in the context\nof autonomous driving. By focusing on real-world driv-\ning scenarios and harnessing the power of the diffusion\nmodel, DriveDreamer has demonstrated its ability to com-\nprehend complex environments, generate high-quality driv-\ning videos, and formulate realistic driving policies. While\nprior research primarily concentrated on gaming or simu-\nlated environments, DriveDreamer extends the boundaries\nof world modeling to encompass the intricacies of actual\ndriving conditions. DriveDreamer paves the way for future\nresearch in autonomous driving, emphasizing the impor-\ntance of real-world representation for more accurate mod-\neling and decision-making in this critical domain.\nReferences\n[1] Fan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu,\nYaole Wang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.\nOne transformer fits all distributions in multi-modal diffu-\nsion at scale. arXiv preprint arXiv:2303.06555, 2023. 2\n[2] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dock-\nhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis.\nAlign your latents: High-resolution video synthesis with la-\ntent diffusion models. In CVPR, 2023. 5, 6\n[3] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. CVPR, 2019. 7, 12\n[4] Dian Chen and Philipp Kr\u00a8ahenb\u00a8uhl. Learning from all vehi-\ncles. In CVPR, 2022. 5\n[5] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger,\nAndreas Geiger,\nand Hongyang Li.\nEnd-to-end au-\ntonomous driving: Challenges and frontiers. arXiv preprint\narXiv:2306.16927, 2023. 2, 3\n[6] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia.\nFantasia3d:\nDisentangling geometry and appearance for\nhigh-quality text-to-3d content creation.\narXiv preprint\narXiv:2303.13873, 2023. 2\n[7] Felipe Codevilla, Eder Santana, Antonio M L\u00b4opez, and\nAdrien Gaidon.\nExploring the limitations of behavior\ncloning for autonomous driving. In CVPR, 2019. 2\n[8] Emily Denton and Rob Fergus. Stochastic video generation\nwith a learned prior. In ICML, 2018. 2, 3\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models\nbeat gans on image synthesis. NeurIPS, 2021. 3\n[10] Stefan Elfwing, Eiji Uchibe, and Kenji Doya.\nSigmoid-\nweighted linear units for neural network function approxi-\nmation in reinforcement learning. Neural networks, 2018.\n12\n[11] Jean-Yves Franceschi, Edouard Delasalles, Micka\u00a8el Chen,\nSylvain Lamprier, and Patrick Gallinari. Stochastic latent\nresidual video prediction. In ICML, 2020. 2, 3\n[12] Ruiyuan Gao, Kai Chen, Enze Xie, Lanqing Hong, Zhenguo\nLi, Dit-Yan Yeung, and Qiang Xu. Magicdrive: Street view\ngeneration with diverse 3d geometry control. arXiv preprint\narXiv:2310.02601, 2023. 3\n[13] Zeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren,\nJianyu Chen, Shengbo Eben Li, Ping Luo, and Yanfeng Lu.\nEnhance sample efficiency and robustness of end-to-end ur-\nban autonomous driving via semantic masked world model.\narXiv preprint arXiv:2210.04017, 2022. 2, 3\n[14] Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo\nZhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vec-\ntor quantized diffusion model for text-to-image synthesis. In\nCVPR, 2022. 2\n[15] David Ha and J\u00a8urgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. NeurIPS, 2018. 2, 3\n[16] Danijar Hafner, Kuang-Huei Lee, Ian Fischer, and Pieter\nAbbeel. Deep hierarchical planning from pixels. NeurIPS,\n2022. 3\n[17] Danijar Hafner, Timothy Lillicrap, Jimmy Ba, and Moham-\nmad Norouzi. Dream to control: Learning behaviors by la-\ntent imagination. arXiv preprint arXiv:1912.01603, 2019. 2,\n3\n[18] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and\nJimmy Ba. Mastering atari with discrete world models. arXiv\npreprint arXiv:2010.02193, 2020. 2, 3\n[19] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 2, 3\n9\n[20] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy\nLillicrap. Mastering diverse domains through world models.\narXiv preprint arXiv:2301.04104, 2023. 3\n[21] William Harvey, Saeid Naderiparizi, Vaden Masrani, Chris-\ntian Weilbach, and Frank Wood. Flexible diffusion modeling\nof long videos. NeurIPS, 2022. 2, 3\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. Gans trained by a\ntwo time-scale update rule converge to a local nash equilib-\nrium. NeurIPS, 2017. 7\n[23] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben\nPoole, Mohammad Norouzi, David J Fleet, et al. Imagen\nvideo: High definition video generation with diffusion mod-\nels. arXiv preprint arXiv:2210.02303, 2022. 2, 3\n[24] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 2020. 3\n[25] Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet,\nMohammad Norouzi, and Tim Salimans. Cascaded diffusion\nmodels for high fidelity image generation. JMLR, 2022. 3\n[26] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term\nmemory. Neural computation, 1997. 3\n[27] Tobias H\u00a8oppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen,\nand Andrea Dittadi. Diffusion models for video prediction\nand infilling. arXiv preprint arXiv:2206.07696, 2022. 3\n[28] Jun-Ting Hsieh, Bingbin Liu, De-An Huang, Li F Fei-Fei,\nand Juan Carlos Niebles. Learning to decompose and disen-\ntangle representations for video prediction. NeurIPS, 2018.\n3\n[29] Anthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary\nMurez, Corina Gurau, Hudson Yeo, Alex Kendall, Roberto\nCipolla, and Jamie Shotton. Model-based imitation learning\nfor urban driving. NeurIPS, 2022. 2, 3, 7\n[30] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi\nYan, and Dacheng Tao. St-p3: End-to-end vision-based au-\ntonomous driving via spatial-temporal feature learning. In\nECCV, 2022. 7, 9\n[31] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima,\nXizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai\nWang, Lewei Lu, Xiaosong Jia, Qiang Liu, Jifeng Dai, Yu\nQiao, and Hongyang Li. Planning-oriented autonomous driv-\ning. In CVPR, 2023. 5, 9\n[32] Lianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao,\nand Jingren Zhou. Composer: Creative and controllable im-\nage synthesis with composable conditions. arXiv preprint\narXiv:2302.09778, 2023. 3\n[33] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jia-\njie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang\nHuang, and Xinggang Wang. Vad: Vectorized scene rep-\nresentation for efficient autonomous driving. arXiv preprint\narXiv:2303.12077, 2023. 9\n[34] Nal Kalchbrenner, A\u00a8aron Oord, Karen Simonyan, Ivo Dani-\nhelka, Oriol Vinyals, Alex Graves, and Koray Kavukcuoglu.\nVideo pixel networks. In ICML, 2017. 3\n[35] Levon Khachatryan, Andranik Movsisyan, Vahram Tade-\nvosyan,\nRoberto\nHenschel,\nZhangyang\nWang,\nShant\nNavasardyan, and Humphrey Shi. Text2video-zero: Text-to-\nimage diffusion models are zero-shot video generators. arXiv\npreprint arXiv:2303.13439, 2023. 2, 3\n[36] Seung Wook Kim, Jonah Philion, Antonio Torralba, and\nSanja Fidler. Drivegan: Towards a controllable high-quality\nneural simulation. In CVPR, 2021. 3, 8\n[37] Seung Wook Kim, Yuhao Zhou, Jonah Philion, Antonio Tor-\nralba, and Sanja Fidler. Learning to simulate dynamic envi-\nronments with gamegan. In CVPR, 2020. 3\n[38] Diederik P Kingma and Jimmy Ba. Adam: A method for\nstochastic optimization.\narXiv preprint arXiv:1412.6980,\n2014. 7\n[39] Diederik P Kingma and Max Welling. Auto-encoding varia-\ntional bayes. arXiv preprint arXiv:1312.6114, 2013. 3\n[40] Manoj Kumar, Mohammad Babaeizadeh, Dumitru Erhan,\nChelsea Finn, Sergey Levine, Laurent Dinh, and Durk\nKingma.\nVideoflow: A flow-based generative model for\nvideo. arXiv preprint arXiv:1903.01434, 2019. 3\n[41] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye.\nDrivingdif-\nfusion:\nLayout-guided multi-view driving scene video\ngeneration with latent diffusion model.\narXiv preprint\narXiv:2310.07771, 2023. 3\n[42] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jian-\nwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee.\nGligen: Open-set grounded text-to-image generation.\nIn\nCVPR, 2023. 2, 4\n[43] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa,\nXiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler,\nMing-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution\ntext-to-3d content creation. In CVPR, 2023. 2\n[44] Jessy Lin, Yuqing Du, Olivia Watkins, Danijar Hafner, Pieter\nAbbeel, Dan Klein, and Anca Dragan. Learning to model\nthe world with language. arXiv preprint arXiv:2308.01399,\n2023. 3\n[45] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang,\nHuizi Mao, Daniela L Rus, and Song Han. Bevfusion: Multi-\ntask multi-sensor fusion with unified bird\u2019s-eye view repre-\nsentation. In ICRA, 2023. 7, 8, 13\n[46] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep\nmulti-scale video prediction beyond mean square error.\narXiv preprint arXiv:1511.05440, 2015. 3\n[47] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik,\nJonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:\nRepresenting scenes as neural radiance fields for view syn-\nthesis. Communications of the ACM, 2021. 4, 12\n[48] Chong Mou, Xintao Wang, Liangbin Xie, Jian Zhang, Zhon-\ngang Qi, Ying Shan, and Xiaohu Qie. T2i-adapter: Learning\nadapters to dig out more controllable ability for text-to-image\ndiffusion models. arXiv preprint arXiv:2302.08453, 2023. 2\n[49] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav\nShyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and\nMark Chen. Glide: Towards photorealistic image generation\nand editing with text-guided diffusion models. arXiv preprint\narXiv:2112.10741, 2021. 2, 3\n[50] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models. In ICML, 2021. 3\n10\n[51] nuScenes Contributors. The devkit of the nuscenes dataset.\nhttps : / / github . com / nutonomy / nuscenes -\ndevkit, 2019. 7\n[52] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang\nYang. Iso-dream: Isolating and leveraging noncontrollable\nvisual dynamics in world models. NeurIPS, 2022. 2, 3\n[53] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Milden-\nhall.\nDreamfusion: Text-to-3d using 2d diffusion.\narXiv\npreprint arXiv:2209.14988, 2022. 2\n[54] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-\ning transferable visual models from natural language super-\nvision. In ICML. 4\n[55] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,\nand Mark Chen. Hierarchical text-conditional image gen-\neration with clip latents. arXiv preprint arXiv:2204.06125,\n2022. 2\n[56] MarcAurelio Ranzato, Arthur Szlam, Joan Bruna, Michael\nMathieu, Ronan Collobert, and Sumit Chopra. Video (lan-\nguage) modeling: a baseline for generative models of natural\nvideos. arXiv preprint arXiv:1412.6604, 2014. 3\n[57] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer. High-resolution image syn-\nthesis with latent diffusion models. In CVPR, 2022. 2, 3, 4,\n5, 7\n[58] Masaki Saito, Eiichi Matsumoto, and Shunta Saito. Tempo-\nral generative adversarial nets with singular value clipping.\nIn ICCV, 2017. 3\n[59] Younggyo Seo, Danijar Hafner, Hao Liu, Fangchen Liu,\nStephen James, Kimin Lee, and Pieter Abbeel.\nMasked\nworld models for visual control. In CoRL, 2023. 3\n[60] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An,\nSongyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,\nOran Gafni, et al. Make-a-video: Text-to-video generation\nwithout text-video data. arXiv preprint arXiv:2209.14792,\n2022. 2, 3\n[61] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudi-\nnov. Unsupervised learning of video representations using\nlstms. In ICML, 2015. 3\n[62] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan\nKautz.\nMocogan: Decomposing motion and content for\nvideo generation. In CVPR, 2018. 3\n[63] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach,\nRaphael Marinier, Marcin Michalski, and Sylvain Gelly. To-\nwards accurate generative models of video: A new metric &\nchallenges. arXiv preprint arXiv:1812.01717, 2018. 7\n[64] Vikram Voleti, Alexia Jolicoeur-Martineau, and Chris Pal.\nMcvd-masked conditional video diffusion for prediction,\ngeneration, and interpolation. NeurIPS, 2022. 3\n[65] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba.\nGenerating videos with scene dynamics. NeurIPS, 29, 2016.\n3\n[66] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.\nFcos3d: Fully convolutional one-stage monocular 3d object\ndetection. In CVPR, 2021. 7, 8, 13\n[67] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen,\nJiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao,\nand Jingren Zhou.\nVideocomposer: Compositional video\nsynthesis with motion controllability.\narXiv preprint\narXiv:2306.02018, 2023. 2, 3\n[68] Xiaofeng Wang, Zheng Zhu, Yunpeng Zhang, Guan Huang,\nYun Ye, Wenbo Xu, Ziwei Chen, and Xingang Wang. Are we\nready for vision-centric driving streaming perception? the\nasap benchmark. In CVPR, 2023. 7\n[69] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan\nLi, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and\ndiverse text-to-3d generation with variational score distilla-\ntion. arXiv preprint arXiv:2305.16213, 2023. 2\n[70] Dirk Weissenborn, Oscar T\u00a8ackstr\u00a8om, and Jakob Uszkor-\neit.\nScaling autoregressive video models.\narXiv preprint\narXiv:1906.02634, 2019. 3\n[71] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter\nAbbeel, and Ken Goldberg. Daydreamer: World models for\nphysical robot learning. In CoRL, 2023. 3\n[72] Kairui Yang, Enhui Ma, Jibin Peng, Qing Guo, Di Lin, and\nKaicheng Yu.\nBevcontrol: Accurately controlling street-\nview elements with multi-perspective consistency via bev\nsketch layout. arXiv preprint arXiv:2308.01661, 2023. 3\n[73] Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Run-\nsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin\nCui, and Ming-Hsuan Yang. Diffusion models: A compre-\nhensive survey of methods and applications. arXiv preprint\narXiv:2209.00796, 2022. 2\n[74] Ruihan Yang, Prakhar Srivastava, and Stephan Mandt. Dif-\nfusion probabilistic modeling for video generation.\narXiv\npreprint arXiv:2203.09481, 2022. 2, 3\n[75] Jiang-Tian Zhai, Ze Feng, Jihao Du, Yongqiang Mao, Jiang-\nJiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jing-\ndong Wang. Rethinking the open-loop evaluation of end-\nto-end autonomous driving in nuscenes.\narXiv preprint\narXiv:2305.10430, 2023. 9, 12\n[76] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding\nconditional control to text-to-image diffusion models, 2023.\n2\n[77] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,\nGuan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-\nception and prediction in birds-eye-view for vision-centric\nautonomous driving.\narXiv preprint arXiv:2205.09743,\n2022. 5\n11\nIn the supplement materials, we first elaborate on the\nimplementation details of DriveDreamer, including model\narchitecture and synthetic data training details. Then, we\npresent additional visualization results.\n6. Implementation Details\nCondition encoders. In DriveDreamer, diverse encoders\nare employed to embed different condition inputs, includ-\ning the reference image, HDMap, 3D box, and action. The\ndetailed architectures of these encoders are listed in Table 4.\nFor spatially aligned conditions, such as the reference im-\nage I \u2208RH\u00d7W \u00d73 and HDMap H \u2208RH\u00d7W \u00d73, a stack\nof 2D convolution layers is utilized to perform downsam-\npling, ensuring the final output dimensions align with those\nof the diffusion noise. For unstructured conditions like the\n3D box B \u2208RN\u00d7NB\u00d716 and action A \u2208RN\u00d72, Multi-\nlayer Perceptron (MLP) layers are employed for encoding\nfeatures.\nConditions\nLayer Description\nOutput Size\nRef. Img. (Step A)\nConv2D, 4 \u00d7 4, S4\nH/4 \u00d7 W/4 \u00d7 4\nRef. Img. (Step B)\nConv2D, 4 \u00d7 4, S4\nH/16 \u00d7 W/16 \u00d7 4\nRef. Img. (Step C)\nConv2D, 4 \u00d7 4, S4\nH/64 \u00d7 W/64 \u00d7 8\nHDMap (Step A)\nConv2D, 4 \u00d7 4, S4\nH/4 \u00d7 W/4 \u00d7 4\nHDMap (Step B)\nConv2D, 4 \u00d7 4, S4\nH/16 \u00d7 W/16 \u00d7 4\nHDMap (Step C)\nConv2D, 4 \u00d7 4, S4\nH/64 \u00d7 W/64 \u00d7 8\n3D Box (Step A)\nFourierEmbedder [47]\nN \u00d7 NB \u00d7 256\n3D Box (Step B)\nMLP\nN \u00d7 NB \u00d7 512\n3D Box (Step C)\nMLP\nN \u00d7 NB \u00d7 768\nAction (Step A)\nMLP\nN \u00d7 32\nAction (Step B)\nMLP\nN \u00d7 128\nTable 4. Encoder architecture details, where S denotes stride, and\neach convolution layer and MLP layer are followed by Sigmoid\nLinear Units [10].\nMulti-view generation. The framework of DriveDreamer\ncan be easily extended to multi-view image/video genera-\ntion.\nThe model architecture comparison between video\ngeneration, multi-view image generation and multi-view\nvideo generation are shown in Fig. 7. For multi-view im-\nage generation, the model framework is the same as that of\nvideo generation, except that the frame-vise attention lay-\ners are replaced with view-wise attention layers. Besides,\nthe view-wise attention layers construct associations solely\nbetween adjacent views. For multi-view video generation,\nview-wise attention layers and frame-wise attention layers\nare stacked to process diffusion latent features, which re-\nsults in view-consistent and frame-consistent videos (see\nFig. 8).\nAction prediction architecture. For action prediction, the\nmulti-modal features are first concatenated:\n  \\text {CONCA T}(\\mat hcal {F }_{\\tex t {p}}(U_0), \\mathcal {F}_{\\text {p}}(U_1), \\mathcal {F}_{\\text {p}}(U_2), \\mathcal {F}_{\\text {p}}(U_3), A_f), \n(11)\nsteps\nDenoising\nView-wise-attention\nCross-attention\nGated Self-attention\nsteps\nDenoising\nFrame-wise-attention\nsteps\nDenoising\nVideo Generation\nMulti-view Image Generation\nMulti-view Video Generation\nFigure 7. Model architecture comparison between video genera-\ntion, multi-view image generation and multi-view video genera-\ntion.\nwhere Fp is the average pooling operation, Ui(i\n=\n0, 1, 2, 3) are multi-scale UNet features, and Af is the en-\ncoded driving action (i.e., velocity and yaw angle) features.\nThen we use MLP layers [75] to learn future driving ac-\ntions. For trajectory prediction evaluation, following [75],\nAf is additionally extracted from high-level command, ac-\ncelerate and past trajectories, and we use the same action\nfeature encoder of [75].\nSynthetic data training.\nWe leverage data generated\nby DriveDreamer to augment the training of 3D detec-\ntion tasks. Specifically, DriveDreamer is fine-tuned with\nhigher-resolution images, where the training data is from\nnuScenes [3]. Consequently, DriveDreamer can generate\nhigh-fidelity images with a resolution of 768 \u00d7 448. Then\nthe generated images are resized to the original resolution\nof 1600 \u00d7 900, which can be utilized to train various off-\nthe-shelf 3D detectors. During the training process, we ran-\ndomly select 4000 samples (3D boxes and HDMap) from\nthe nuScenes training set, which are employed to generate\nmulti-view images. These synthetic data are mixed with\nthe original training set to train 3D detectors. In the ex-\n12\nFigure 8. Visualizations of the generated multi-view video. Regions highlighted by red circles indicate that the generated videos are view-\nconsistent and frame-consistent.\nperiment, we train each baseline (i.e., FCOS3D [66] and\nBEVFusion [45]) for 12 epochs. The results presented in\nTab. 1 demonstrate that our approach significantly improves\nthe performance of downstream tasks.\n7. Visualizations\nAs shown in Fig. 9, DriveDreamer exhibits significant\nproficiency in producing a diverse range of driving scene\nvideos that adhere meticulously to structured traffic condi-\ntions, comprising elements such as HDMaps and 3D boxes.\nSignificantly, we can also manipulate the text prompt to\ninduce variations in the generated videos, encompassing\nchanges in weather and time of day. This heightened adapt-\nability contributes substantially to the multifaceted nature of\nthe generated video outputs. In addition to the utilization of\nstructured traffic conditions for generating driving videos,\nDriveDreamer exhibits the capability to diversify the gen-\nerated driving videos by adapting to different driving ac-\ntions. As depicted in Fig. 10, starting from an initial frame\npaired with its corresponding structural information, Drive-\nDreamer can generate distinct videos based on various driv-\ning actions, such as videos depicting left and right turns.\nApart from its capacity for generating highly controllable\ndriving videos, DriveDreamer demonstrates the ability to\npredict reasonable driving actions. As depicted in Fig. 11,\nprovided with an initial frame condition and past driving\nactions, DriveDreamer can generate future driving actions\nthat align with real-world scenarios. Comparative analysis\nof the generated actions against corresponding ground truth\nvideos reveals that DriveDreamer consistently predicts sen-\nsible driving actions, even in complex situations such as in-\ntersections, obeying traffic lights, and executing turns.\n13\n\u201cRealistic autonousmous\n     driving scen, sunny\u201d\n\u201cRealistic autonousmous\n     driving scene, night\u201d\n\u201cRealistic autonousmous\n     driving scene, rainy\u201d\nStructured traffic conditions\n    HDMaps, 3D Bboxes\n\u201cRealistic autonousmous\n     driving scene, sunny\u201d\n\u201cRealistic autonousmous\n     driving scene, night\u201d\n\u201cRealistic autonousmous\n     driving scene, rainy\u201d\nStructured traffic conditions\n    HDMaps, 3D Bboxes\n\u201cRealistic autonousmous\n     driving scene, sunny\u201d\n\u201cRealistic autonousmous\n     driving scene, night\u201d\n\u201cRealistic autonousmous\n     driving scene, rainy\u201d\nStructured traffic conditions\n    HDMaps, 3D Bboxes\nFigure 9. Driving video generation with structured traffic conditions (HDMaps and 3D boxes), where text prompts are utilized to adjust\ndriving scenario style (e.g., weather and time of the day).\n14\nInitial condition\nInitial Conditions\nInitial Conditions\nInitial Condition\nTurn Right\nTurn Left\n\u201cRealistic autonousmous\n     driving scene\u201d\n\u201cRealistic autonousmous\n     driving scene\u201d\n\u201cRealistic autonousmous\n     driving scene\u201d\nTurn Right\nTurn Left\nTurn Right\nTurn Left\nFigure 10. Future driving video generation with driving actions interaction, where different driving actions (e.g. turn left, turn right) can\nproduce corresponding driving videos.\nInitial Condition\nInitial Condition\nInitial Condition\nInitial Driving Actions\nInitial Driving Actions\nInitial Driving Actions\nFigure 11. Visualization of the predicted future driving actions, along with the corresponding ground truth driving video.\n15\n",
    "2312.02934": "WoVoGen: World Volume-aware Diffusion for\nControllable Multi-camera Driving Scene\nGeneration\nJiachen Lu1, Ze Huang1, Zeyu Yang1, Jiahui Zhang1, and Li Zhang1\u22c6\nSchool of Data Science, Fudan University\nhttps://github.com/fudan-zvg/WoVoGen\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nMulti-camera driving scene generation\nWorld volume generation\n+ Control\n\u201cDrive in sunny day\u201d\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nHD MAP\nOCCUPANCY\nFig. 1: Our WoVoGen is crafted to generate future world volumes (i.e., HD maps\nand occupancy) and high-quality multi-camera street-view images, with the input of\npast world-volumes. The bottom row shows the weather-based control of WoVoGen.\nSpecifically, with the predicted world volume at time t4 and a weather description,\nthe multi-camera images transit from rainy to sunny conditions, while maintaining the\nstreet layout.\nAbstract. Generating multi-camera street-view videos is critical for\naugmenting autonomous driving datasets, addressing the urgent demand\nfor extensive and varied data. Due to the limitations in diversity and\nchallenges in handling lighting conditions, traditional rendering-based\nmethods are increasingly being supplanted by diffusion-based methods.\nHowever, a significant challenge in diffusion-based methods is ensuring\nthat the generated sensor data preserve both intra-world consistency and\ninter-sensor coherence. To address these challenges, we combine an ad-\nditional explicit world volume and propose World Volume-aware Multi-\ncamera Driving Scene Generator (WoVoGen). This system is specifically\n\u22c6Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author.\narXiv:2312.02934v4  [cs.CV]  8 Aug 2024\n2\nJ. Lu, et al.\ndesigned to leverage 4D world volume as a foundational element for video\ngeneration. Our model operates in two distinct phases: (i) envisioning the\nfuture 4D temporal world volume based on vehicle control sequences,\nand (ii) generating multi-camera videos, informed by this envisioned 4D\ntemporal world volume and sensor interconnectivity. The incorporation\nof the 4D world volume empowers WoVoGen not only to generate high-\nquality street-view videos in response to vehicle control inputs but also\nto facilitate scene editing tasks.\nKeywords: Diffusion \u00b7 Autonomous driving \u00b7 World volume\n1\nIntroduction\nThe burgeoning field of vision-based autonomous driving perception [20,23,25]\nunderscores the need for high-quality multi-camera street-view datasets [2]. With\nthe notorious costs of labeling in autonomous driving datasets, there is a signif-\nicant demand for generating high-quality multi-camera street-view videos that\naccurately mirror real-world 3D data distributions and maintain consistency\nacross multiple sensors.\nRecent driving scene content synthesis techniques can be categorized into two\nmain groups: rendering-based [3,10,43,44,46] and diffusion-based [9,18,34,40,45]\nmethods. Rendering-based methods benefit from an explicit 3D or 4D world\nstructure, ensuring stringent 3D consistency. Yet, this approach tends to offer\nlimited diversity and requires considerable effort to produce multi-camera videos\nthat comply with real-world lighting, weather conditions, etc. On the other hand,\nmethods based on finetuned stable diffusion models boast high diversity and can\neasily generate images that follow real-world distributions. Despite the noted\nadvantages, diffusion-based approaches [9, 18, 34, 40, 45] that employ bounding\nboxes or HD maps as control signals offer a sparse representation of the driving\nscene. Consequently, the generated videos often lack critical information about\nthe scene, leading to an incomplete understanding. This limitation underscores\nthe necessity for a diffusion-based model that incorporates a dense representa-\ntion of the world as a condition, ensuring a more comprehensive and accurate\ndepiction of the driving environment.\nTo overcome the limitations of diffusion-based methods, we introduce World\nVolume-aware Multi-camera Driving Scene Generator (WoVoGen), a framework\ndesigned to endow diffusion-based generative models with an explicit 4D world\nvolume. Our approach operates in two stages: initially, we envision a 4D world\nvolume using a reference scene combined with a future vehicle control sequence.\nSubsequently, this volume guides the generation of multi-camera footage. Con-\ncretely, our 4D world volume manifests as a dense voxel volume spanning four\ndimensions: time, height, length, and width, corresponding to the scope of the\nbird\u2019s-eye view (BEV) domain. This representation encapsulates the scene\u2019s com-\nprehensive data, comprising object occupancy, high-definition maps, background\ndetails, and road attributes. Each voxel within this 4D construct is annotated\nWoVoGen\n3\nwith a class label, providing a rich, multi-faceted understanding of the environ-\nment.\nIn the preliminary stage, our method is to train an autoencoder model [38]\nthat encodes a single-frame 3D world volume into a 2D latent representation.\nSubsequently, we stack these 2D latents along the temporal axis to form a 2D\ntemporal latent series. We further refine the UNet by introducing temporal ver-\nsions of its residual and cross-attention blocks, which can effectively process the\ntime-varying information, guided by the vehicle control sequence as the condi-\ntional context. Upon generating the future 2D temporal latent, we proceed to\ndecode it back into the 4D world voxel volume using the autoencoder\u2019s decoder.\nHaving generated the future 4D world voxel volume, we employ a combination\nof CLIP and 3D sparse CNN to convert it into a 4D world feature. This feature\nis then transformed geometrically to sample 3D image volumes for each cam-\nera, corresponding to each time step. Subsequently, these 3D image volumes are\ncondensed into 2D image features, which serve as conditional inputs for Control-\nNet [47]. Alongside image features, we employ textual prompts as scene guidance\nakin to those used in Stable Diffusion [27] to govern the overall scene conditions\nsuch as weather, lighting, location, and the scene at large. For more precise ob-\nject location control within the scene, textual prompts are utilized with greater\nspecificity. We map the 4D world volume labels onto each 2D pixel, utilizing the\nlabel names as textual conditions to objectively guide over each pixel\u2019s charac-\nteristics. For achieving inter-sensor consistency, we concatenate surround-view\nimages into a meta-image and leverage the diffusion model to learn the distri-\nbution of real-world multi-view image sequences. To ensure temporal coherence,\nwe utilize the same temporal Transformer blocks previously described.\nIn summary, we make the following contributions: (i) We propose WoVo-\nGen, a framework that leverages an explicit world volume to guide the diffusion\nmodel generation, ensuring intra-world and inter-sensor consistency. (ii) We in-\ntroduce a two-phase strategy: initially envisioning the future 4D world volume,\nfollowed by generating multi-camera videos based on this envisioned world vol-\nume. (iii) Our system not only excels in producing street-view videos that ex-\nhibit consistency within the world and across various cameras, driven by vehicle\ncontrol inputs, but also facilitates scene editing tasks.\n2\nRelated work\nDiffusion model for visual content generation Diffusion models, as demon-\nstrated in [5, 12, 27, 32], exhibit the capability to generate a diverse array of\nimages through a learned denoising process. Building upon the foundation of a\nwell-trained image generation model, several works have emerged to augment its\ncontrollable generation capabilities, such as text-guided approaches [6,26,28,29],\nas well as layout-guided approaches [15,19,47,48], etc. Further advancements in\nimage generation extend to video generation, emphasizing temporal consistency\nin recent works [1,31,42,49]. Additionally, there has been a surge in interest in\nnovel view generation techniques [21, 22, 24, 30], focused on generating diverse\n4\nJ. Lu, et al.\n\ud835\udc6c\n\ud835\udc6c\n\ud835\udc68\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udfcf, \ud835\udc68\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udfd0, \u22ef, \ud835\udc68\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udc75\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc96\ud835\udc93\ud835\udc86\n\ud835\udc6b\n\ud835\udc6b\n(\ud835\udc9b\ud835\udc95\n\ud835\udfce, \ud835\udf50\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udfcf)\n\ud835\udc9b\ud835\udfce\n\ud835\udfce\nDiffusion\n(\ud835\udc9b\ud835\udc95\n\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95, \ud835\udf50\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udc75\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc96\ud835\udc93\ud835\udc86)\n\ud835\udc9b\ud835\udfce\n\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\nConcat\n\ud835\udc9b\ud835\udfce\n\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95#\ud835\udfcf\n\ud835\udc9b\ud835\udfce\n\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95#\ud835\udc75\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc96\ud835\udc93\ud835\udc86\n\ud835\udc98\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udfcf\n\ud835\udc98\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udc75\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc96\ud835\udc93\ud835\udc86\n\ud835\udc6b\n\ud835\udc6b\n\ud835\udc70\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udfcf\n\ud835\udc70\ud835\udc75\ud835\udc91\ud835\udc82\ud835\udc94\ud835\udc95\"\ud835\udc75\ud835\udc87\ud835\udc96\ud835\udc95\ud835\udc96\ud835\udc93\ud835\udc86\nWorld\nencoder\nWorld\nencoder\nConcat\nConcat\n\u201cDrive in Singapore\u201d\nScene guide\nObject guide\n\ud835\udf50\nFrustum feature\nWorld model\nWorld volume-aware diffusion\nFig. 2: Overall framework of WoVoGen. Top: world model branch. We finetune\nthe AutoencoderKL and train the 4D diffusion model from scratch to generate future\nworld volumes based on past world volumes and the actions of the ego car. Bottom:\nworld volume-aware synthesis branch. Leveraging the generated future volumes\nas input, Fw are derived through the world encoder. Subsequent sampling yields Fimg,\nwhich are then aggregated. The process is finalized by applying panoptic diffusion to\nproduce future videos.\nperspectives from existing images. These advancements necessitate models with\nenhanced capabilities to ensure spatial consistency in generated outputs.\nWe consolidate various functionalities from preceding diffusion models and\nintroduce WoVoGen, designed specifically for generating driving scene content.\nWoVoGen encompasses several innovative features: (i) Text-guided and world\nvolume-aware controllable image generation, (ii) Spatially consistent multi-view\nimage generation and novel view image generation, and (iii) Temporally consis-\ntent video generation.\nRendering-based driving scene synthesis The rendering-based approaches [10,\n43,44,46] initially reconstruct comprehensive world structures, followed by image\nor video rendering based on this reconstructed data. While these methods yield\nhigh-quality synthetic results, they often lack robust control capabilities. Efforts\nlike NeuralField-LDM [16] have attempted to introduce diffusion models into\nvolume rendering to address diversity issues and improve control in rendering-\nbased synthesis. However, they still rely heavily on numerous real-world samples\nfrom various perspectives or additional supervision during model training, rais-\ning concerns about their efficacy.\nDiffusion-based driving scene synthesis The diffusion-based approaches [9,\n18,34,40,45] leverage pre-trained diffusion models and incorporate sparse condi-\ntions, i.e., HD maps and bounding boxes, enabling precise control over driving\nscene content generation. Part of the works [9,34,45] primarily focus on single-\nframe image generation. As a unified pipeline, they encode ground truth condi-\ntion information and inject conditional features into the diffusion model to gen-\nWoVoGen\n5\nerate images corresponding to the specified condition. Other works [18,40] have\nventured into driving scene video generation. Building upon trained single-frame\ngeneration models, they finetune the temporal module to ensure the temporal\ncoherence of generated elements across consecutive frames in the video.\nTemporal self-attention / 3D convolution block\nSelf-attention / 2D convolution block\nCross-attention block\nSpatial conv block\nTemporal conv block\nSpatial attention block\nTemporal attention block\nAction attention block\nFeed-forward network\nb t c [h w]\nb c [t h w]\nb t [h w] c\nb h w [n] c\nb t [h w] c\nSpatial conv block\nSpatial attention block\nGuidance attention block\nGuidance attention block\nTemporal attention block\nFeed-forward network\nb t c [h w]\nb t [h w] c\nb t [h w] c\nb t [h w] c\nb h w [n] c\n\ud835\udc9b\ud835\udc64\n\ud835\udc9b\ud835\udc5d\ud835\udc4e\ud835\udc5b\ud835\udc5c\n\ud835\udc9c\nCLIP\nfeature\n(a)\n(b)\nCLIP\nfeature\nFig. 3: (a): an action attention block en-\nhances the model by incorporating ac-\ntion information. (b): a guidance attention\nblock integrates the CLIP feature of a spe-\ncific object into the latent representation.\nThe proposed WoVoGen excels in\ngenerating high-quality driving scene\nimages and videos. Distinguishing it-\nself from other driving scene video\ngeneration works [18, 40], WoVoGen\npossesses unique traits: (i) It em-\nploys a self-organized dense world vol-\nume as a condition, facilitating the\nnatural generation of geometrically\nconsistent images from multiple per-\nspectives. (ii) WoVoGen integrates a\nworld model capable of generating fu-\nture world volumes, eliminating the\nnecessity for ground truth conditions\nfor each video frame generation. (iii)\nWith the incorporation of explicit 4D\ninformation, i.e., temporal world vol-\numes, WoVoGen offers a comprehen-\nsive and accurate representation of\nthe driving environment, resulting in\na strong correlation with critical scene\ndetails in the generated content.\n3\nMethod\n3.1\nPreliminary\nThe latent diffusion model (LDM) [27] is a generative model adept at generating\nhigh-resolution images in two stages. Initially, an autoencoder compresses data\ninto a latent space (perceptual compression), where z = E(x) and \u02c6x = D(z) rep-\nresent encoding and decoding processes, respectively. Subsequently, a denoising\ndiffusion probabilistic model (DDPM) [12] models image distributions in this\nlatent space.\nThe generative process reverses the diffusion sequence z1, . . . , zT using a\nlearned Gaussian transition, formalized as:\n  q\\left (  \\\nm\nath\nb\nf  {z}_{\\t au \n}\n | \\mathbf {z}_{\\tau -1} \\right ) = \\mathcal {N} \\left ( \\mathbf {z}_{\\tau }; \\sqrt {1-\\beta _\\tau } \\mathbf {z}_{\\tau -1}, \\beta _\\tau \\mathbf {I} \\right ), \n(1)\n  p_{\\theta  }\n\\\nleft ( \\mat hbf  {z\n}\n_{\\tau -1} | \\mathbf {z}_{\\tau } \\right ) = \\mathcal {N} \\left ( \\mathbf {z}_{\\tau -1}; \\mu _{\\theta }(\\mathbf {z}_{\\tau }, \\tau ), \\tilde \\beta _\\tau \\right ). \n(2)\nHere, \u00b5\u03b8(z\u03c4, \u03c4) is given by a trainable noise predictor \u03f5\u03b8(z\u03c4, \u03c4).\n6\nJ. Lu, et al.\nDDPMs, viewed as a series of weight-sharing denoising autoencoders, train\nto predict the initial noise from a noisy input. The training goal is to maximize\nthe variational lower bound of the negative log-likelihood:\n  \\mat\nh\nbb  {E}_{z , \\e\np\ns\nilon , \\tau } \\left [ \\| \\epsilon - \\epsilon _\\theta (\\mathbf {z}_{\\tau }, \\tau ) \\|^2_2 \\right ]. \n(3)\nFinally, sampling from the latent distribution and then using the latent de-\ncoder generates novel images from Gaussian noise.\n3.2\nOverall architecture\nThe proposed architecture, referred to as WoVoGen, comprises two distinct op-\nerational branches: the world model branch and the world volume-aware\ngeneration branch. The world model branch is responsible for generating fu-\nture world volumes, incorporating action inputs and several initial frames of the\nworld volume to inform its predictions. Meanwhile, the world volume-aware gen-\neration branch focuses on the generation of multi-camera video outputs based on\ntemporal world volumes. An overview of our comprehensive pipeline is visually\nrepresented in Figure 2.xi\n3.3\nWorld volume\nTo effectively harness the potential of rapidly evolving generative architectures, it\nis imperative to transform the environmental context into a standardized format.\nOur primary focus herein lies on the extraction of high-level, abstract structural\ndata pertinent to autonomous driving scenarios, encompassing road layouts, se-\nmantic occupancies, and other related elements.\nRecognizing the critical role of three-dimensional data in rendering pro-\ncesses and the benefits derived from fine-grained constraints in view genera-\ntion, we propose the encoding of scene information within a three-dimensional\nworld volume, denoted as W \u2208RZ\u00d7H\u00d7W \u00d7C. Specifically, at any given time\ninstance t, we amalgamate various facets of driving-related information to en-\ncapsulate the environmental context around the ego vehicle within a prede-\nfined range. This integration is achieved through the concatenation process, i.e.,\nW = concat(O, M), where O \u2208RZ\u00d7H\u00d7W \u00d7Cocc represents the three-dimensional\nsemantic occupancy grid, with Cocc indicating the number of semantic classes.\nConcurrently, M \u2208R1\u00d7H\u00d7W \u00d7Cmap captures the road map information, con-\nstrained to the voxel plane at zero height.\nTo maintain uniformity across the height dimension, we apply zero-padding\nto M along the Z axis. For a more streamlined representation, both road el-\nements and semantic classes are encoded into the RGB spectrum, i.e., setting\nCmap = 3.\n3.4\nWorld model\nGiven a clip of multi-camera videos captured during driving, denoted as\nWoVoGen\n7\n{I0, \u00b7 \u00b7 \u00b7 , INpast}, we initially employ off-the-shelf scene understanding models\nBEVDet-occ [14] to infer the environment. This allows us to obtain the 3D\nworld volume at these moments, represented as {Wt}t=1,\u00b7\u00b7\u00b7 ,Npast. Subsequently,\nthe world volume for the time instance Npast + 1 is inferred, utilizing the past\nworld volumes and the corresponding driving actions.\nLatent world volume autoencoder Given the world volume W \u2208RZ\u00d7H\u00d7W \u00d7C,\nthe encoder is designed to compress W into its latent representation, zw =\nEW(W) \u2208R\nZ\ns \u00d7 H\ns \u00d7 W\ns \u00d7Cz. Here, s represents the downsampling factor, and is\nset equal to Z to achieve a 2D latent representation. The decoder, DW, is\nthen employed to reconstruct the original world volume from the latent rep-\nresentation zw. The autoencoder is trained using a reconstruction loss and vq-\nregularization [7], aimed at minimizing the variance within the latent space.\n4D Latent world diffusion After the training of the latent autoencoder, we\ncommence the training of the world model, with both the encoder and the de-\ncoder remaining frozen. As illustrated in Figure 2, our world model is a diffusion\nmodel that captures the conditional distribution of the latent world volume in\nthe future, conditioned on the past world volumes and the driving actions of\nthe ego vehicle. The noise predictor \u03f5\u03d5 in this stage is implemented as a time-\nconditioned UNet [12]. The world volumes of preceding frames are first encoded\nby the same latent encoder to dimensions commensurate with the noised latent\nzi, and then channel-wise concatenate with zi to serve as the input for the noise\npredictor.\nFor the driving actions, we initially tokenize the velocity vi and the steering\nangle ai of the ego vehicle through Fourier embedding [39], resulting in a sequence\nof action tokens A =\n\b\nv1, a1, \u00b7 \u00b7 \u00b7 , vNpast, aNpast\n\t\n. These action tokens are then\nrefined by a shallow Transformer. Finally, the updated action tokens are injected\ninto the noise predictor via cross-attention layers.\nJointly modeling of the consecutive frames To enhance the temporal con-\nsistency of predicted future world volumes and achieve more realistic scene dy-\nnamics, we transition to generating consecutive world volumes by considering a\njoint distribution of a world volume sequence.\nAs shown in Figure 3, we build upon the basic spatial Transformer block of\nStable Diffusion [27], which comprises spatial convolution, spatial self-attention,\nand spatial cross-attention. Our temporal variant augments this framework by\nintegrating temporal attention with residual connections into the spatial Trans-\nformers.\nSpecifically, let us denote zw \u2208RB\u00d7Nfuture\u00d7C\u00d7H\u00d7W as the sequence of latent\nworld volumes, adding the batch dimension for clarity. The computation within\n8\nJ. Lu, et al.\nthe modified spatial-temporal Transformer is formalized as follows:\n  \\ mathbf {z }_w &=  \\ t e x t  { re ar ra ng e\n} \\ left  (\\ma thbf { z}_\nw, (b\\ n)\n\\ h \\ w\\ c \\r ight ar ro w (b \\  n) \\  ( h \\ w\n)\\  c \\r ight ), \\\\  \\ma\nthbf {z}_w\n & =  \\text {M HSA}  \\ l ef t  (  \\ te xt  { No r\nm}  \\lef t ( \\ math bf { z}_\nw \\right\n )  \\ri ght )  + \\m a thbf {z}_w, \\quad \\text {(Spatial)}\\\\ \\mathbf {z}_w &= \\text {rearrange} \\left (\\mathbf {z}_w, (b\\ n)\\ (h\\ w)\\ c \\rightarrow (b\\ h\\ w)\\ n\\ c \\right ), \\\\ \\mathbf {z}_w &= \\text {MHSA} \\left ( \\text {Norm} \\left ( \\mathbf {z}_w \\right ) \\right ) + \\mathbf {z}_w, \\quad \\text {(Temporal)}\\\\ \\mathbf {z}_w &= \\text {rearrange} \\left (\\mathbf {z}_w, (b\\ h\\ w)\\ n\\ c \\rightarrow (b\\ n)\\ (h\\ w)\\ c \\right ), \\\\ \\mathbf {z}_w &= \\text {MHCA} \\left ( \\text {Norm} \\left ( \\mathbf {z}_w, \\mathcal {A} \\right ) \\right ) + \\mathbf {z}_w, \\quad \\text {(Action)}\\\\ \\mathbf {z}_w &= \\text {FFN} \\left ( \\text {Norm} \\left ( \\mathbf {z}_w \\right ) \\right ) + \\mathbf {z}_w,\nwhere Norm represents the group normalization, MHSA stands for multi-head\nself-attention, MHCA denotes multi-head cross-attention, and FFN refers to the\nfeed-forward network. Both MHSA and MHCA perform attention operations on the\nsecond dimension l of the (b l c) configuration.\n3.5\nWorld volume-aware 2D feature\nWith the temporally continuous world volumes W = {Wt}t=1,2,\u00b7\u00b7\u00b7 generated by\nour diffusion-based world model, we further decode them into relational camera\nimages for autonomous driving video generating.\nWorld volume encoding The world volumes inherently possess semantic in-\nformation, initially represented in the form of simplistic labels. To enhance their\ninformativeness, we employ a featurization process utilizing CLIP [25]:\n  \\ mathca l {F }_w =  \\tex t {SPConv}\\left (\\text {PCA}\\left (\\text {CLIP}\\left (\\mathcal {W}\\right )\\right )\\right ), \n(4)\nwhere CLIP encodes the class name into the class feature. PCA is used to decrease\nthe dimension of the CLIP feature, thereby reducing the computational cost.\nSPConv (Sparse Convolution) [4] then processes the reduced-dimensional world\nvolume feature.\nCamera volume sampling To incorporate the world volume into image gener-\nation, we sample from it using dense rays emitted from the camera. We construct\na 3D grid for each camera\u2019s frustum, denoted as pc, with dimensions Dc\u00d7Hc\u00d7Wc.\nThis grid is based on the camera\u2019s intrinsic and extrinsic properties. The process\nis formalized as follows:\n  \\m a thcal {F}_{ cam}  = \\text {interpolate}\\left (p_c, \\mathcal {F}_w\\right ), \n(5)\nwhere Fcam \u2208RB\u00d7Nfuture\u00d7C\u00d7Dc\u00d7Hc\u00d7Wc represents the camera frustum. We then\napply a squeeze-and-excitation operation [13] on the depth channel and sum\nalong the depth dimension to obtain the world volume-aware 2D image feature:\n  \\m a\nth\nc\nal \n{F}_{img} =  \\ su m _{ i=1}^{D_c} \\text {SE}(\\mathcal {F}_{cam})_{[:,\\ :,\\ :,\\ i,\\ :,\\ :]}. \n(6)\nWoVoGen\n9\nOccupancy generation\nHD map generation\nFRONT\nFRONT\nFRONT\nFRONT\nFRONT\nBACK\nBACK\nBACK\nBACK\nBACK\nVideo generation\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\n\ud835\udc95\ud835\udfd3\nFig. 4: WoVoGen excels in producing future world volumes (top two rows) with tem-\nporal consistency. Subsequently, it utilizes the world volume-aware 2D image features\nderived from the world model\u2019s outputs to synthesize a driving video (bottom two\nrows) with both multi-camera consistency and temporal consistency.\n3.6\nWorld volume-aware diffusion generation\nBased on the ControlNet framework [47], we implement a controller that in-\njects the aforementioned world volume-aware 2D image feature Fimg into the\npretrained latent diffusion model. The Transformer architecture of the UNet is\nillustrated in Figure 3. However, we initially omit the Temporal Attention Block\nwhen training the single-frame Generation model.\nPanoptic diffusion We aggregate the world volume-aware 2D image feature\nfrom different view into a single panoptic feature Fpano input to the diffusion\nmodel:\n  \\ma t\nh\ncal {F }_{pano} = \\begin {bmatrix} \\mathcal {F}_{img}^{\\text {front left}} & \\mathcal {F}_{img}^{\\text {front}} & \\mathcal {F}_{img}^{\\text {front right}}\\\\ \\mathcal {F}_{img}^{\\text {back right}} & \\mathcal {F}_{img}^{\\text {back}} & \\mathcal {F}_{img}^{\\text {back left}} \\end {bmatrix}. t\n(7)\nNaturally, the decoding target is transformed into the corresponding panoptic\nimage. This operation shifts the focus from ensuring consistency between latent\ncodes for different views to maintaining inherent consistency within a single\nlatent code zpano \u2208RC\u00d72Hc\u00d73Wc, resulting in a unified and coherent appearance\nin multi-camera image generation.\nScene guidance Except for the conditional feature, we also introduce text\nprompt-based scene guidance into the latent representations. This involves ex-\ntracting the CLIP feature from a text-based description of an image and mapping\nthis text feature to the intermediate layers of the latent diffusion model, following\na similar approach as LDM [27].\n10\nJ. Lu, et al.\nObject guidance When it comes to objects that require specific placement\nwithin the generated image, we emphasize their pixel-level locations in the latent\nspace using a cross-attention calculation.\nSpecifically, we employ the voxel-based projection of the initial world volume\nonto each camera plane, resulting in the creation of preliminary masks mclass\ndistinguished by the world volumes\u2019 semantic categories. Then, cross-attention\nis calculated by:\n  \\ma t hbf {z}_{pano} = \\ tex t {MHCA}(\\ma t hbf {z}_{pano}(m_{\\text {class}}=1), \\text {CLIP}(\\text {class})) + \\mathbf {z}_{pano}, \nwhere CLIP(class) represents the category-specific CLIP feature, with semantic\ncategories class being defined as bus, car, pedestrian, truck, construction, and\nvegetation.\n3.7\nVideo generation\nBefore we generate video, we first train the single-frame multi-camera generation\nmodel, and then we add temporal attention block and only train the temporal\nattention block as shown in the right side of Figure 3 and freeze the other blocks.\nConditional generation\nGenerate with random seed \nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nGenerate with weather conditioned text prompt\nGenerate with location conditioned text prompt\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nRandom seed: 887\n\u201cDrive in rain\u201d\n\u201cDrive in Singapore \nOnenorth\u201d\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nGround truth\nFig. 5: Examples of conditional generation on nuScenes [2] validation dataset. WoVo-\nGen empowers diverse and controllable scene generation. Altering the random seed\nallows for the generation of various scenarios. Additionally, adjustment to weather\n(such as sunny, rainy, night, etc.) and location (Singapore, Boston, etc.) within the\nprompt enables the modification of weather conditions and city styles within the gen-\nerated scene.\nTemporal consistency The temporal attention block is added to ensure the\ngeneration of multi-frame images consistent, which calculates the attention by:\nWoVoGen\n11\n  \\ma t hbf {z}_{ pano} & = \\t ex t { r ea r ra n ge }\n \\lef t  (\\m athbf  {z}_{pa n o}, (b\n\\ n)\\ (h\\ w)\\ c \\rightarrow (b\\ h\\ w)\\ n\\ c \\right ), \\\\ \\mathbf {z}_{pano} &= \\text {MHSA} \\left ( \\text {Norm} \\left ( \\mathbf {z}_{pano} \\right ) \\right ) + \\mathbf {z}_{pano}, \\quad \\text {(Temporal)}\nCamera editing\nAll camera rotate 45 \u00b0\nGround truth\nAll camera rotate 90 \u00b0\nAll camera rotate 180 \u00b0\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nWorld volume editing\nFRONT  LEFT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nAdd a car\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nGround Truth\nAdd trees\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nReset all vehicles \nFRONT\nGround truth\nFig. 6: Examples of controlled editing on nuScenes [2] validation dataset. Top: the\nability to selectively add or remove specific objects (such as trees, buildings, cars,\netc., highlighted by red circles in the figure) within the world volume empowers the\nprecise and coherent generation of diverse driving scenarios across multiple cameras.\nBottom: due to WoVoGen\u2019s advanced 3D understanding capabilities, the rotation\nof perspectives across multiple cameras can be achieved by modifying the camera\u2019s\nextrinsic parameters.\n12\nJ. Lu, et al.\n4\nExperiments\n4.1\nExperimental setup\nDataset We conducted experiments using the nuScenes [2] dataset which com-\nprises 700 training videos and 150 validation videos. Our study utilized images\nfrom the six camera views.\nFor ground truth occupancy, we use the data from CVPR 2023 3D Occu-\npancy Prediction Challenge [35,36], aligned with semantic labels consistent with\nthe nuScenes-lidarseg dataset [8]. The ego car-centric local map is derived from\nthe global HD map using the vehicle\u2019s ego pose and precisely aligned with the\noccupancy data.\nData preprocessing Images are resized to a resolution of 256 \u00d7 448, and six\nconsecutive frames are selected for video training. Text prompts are constructed\nusing a structured template: \u201cDrive in {weather description} in {location}. The\ndriving scene is in {environment description}, captured by multi-view camera.\u201d\nThe descriptive details enclosed in brackets are filled using scene descriptions\nfrom nuScenes.\nTraining The training process utilized eight NVIDIA A6000 GPUs, encom-\npassing 30,000 iterations for the world volume auto-encoder, 50,000 iterations\nfor volume diffusion, 50,000 iterations for single-frame generation training, and\n3,500 iterations for refining video generation.\nInferring We begin with three initial world volumes organized from ground-\ntruth data and proceed to generate the subsequent three world volumes by in-\ncorporating actions as input. This iterative process continues, producing a quasi-\nlong world volume video sequence. Ultimately, the world volume video sequence\nis decoded to form a multi-view camera video.\nEvaluation We conducted a comparative analysis of our method\u2019s image qual-\nity against other approximate works in the following text. To quantitatively\nassess our work, we employed established metrics such as Fr\u00e9chet Inception Dis-\ntance [11] (FID) and Fr\u00e9chet Video Distance [37] (FVD) for evaluating the qual-\nity of the generated video. Additionally, we provided qualitative demonstrations\nthat underscore the advantages of our proposed method.\n4.2\nResults\n4D World volume generation The distinctive feature of WoVoGen lies in its\nability to predict high-level driving environments in the first stage, including oc-\ncupancy and HD maps, as vividly illustrated in the left two columns of Figure 1\nand the top two rows of Figure 4. Given vehicle actions from the dataset, WoVo-\nGen successfully foresees high-quality future driving environments. Notably, our\nmodel retains its effectiveness in predicting single frames while ensuring a high\ndegree of consistency across the generated future frames. This dual capability\nof WoVoGen \u2014 excelling in both short-term prediction and long-term scene\nevolution \u2014 highlights its potential as a powerful tool for advanced driving en-\nvironment simulation.\nWoVoGen\n13\nMulti-camera single-frame image generation Figure 5 shows the quali-\ntative results of the multi-camera single-frame image generation under diverse\nconditions. We can observe that the generated images have highly consistency.\nFurthermore, from the first row of Figure 5, we can see that the generated im-\nages accurately correspond to the occupancy guide. Beyond that, we can also\ncontrol the style of the generated images through natural language. In the last\ntwo rows of the Figure 5, our WoVoGen demonstrates a profound comprehension\nof the specified textual conditions, skillfully creating highly realistic simulations\nof rainy day scenarios. When prompted with \u201cDrive in Singapore Onenorth,\u201d the\nmodel skillfully generates a scene embodying the garden city essence typical of\nthat region.\nThe quantitative results in Table 1 underscore that WoVoGen showcases\nmarkedly lower FID (27.6) compared to DriveGan [17] and DriveDreamer [40],\nwhile these methods are limited to single-view camera image generation. This re-\nsult solidifies the improved capacity of our approach in generating more realistic\nautonomous driving images.\nMulti-camera single-frame image editing Given the world volume-aware\ncharacter of WoVoGen, we can realize editing based on the edited world volume.\nRearranging objects: We can add, delete, or transform the objects within the\noccupancy. As shown in Figure 6, we can add trees, vehicles, or rearrange the\nlocation of objects.\nCamera extrinsic editing: More interestingly, we can rotate the camera stereo\nwhen projecting the world volume to the camera volume and generate the cor-\nresponding scene under a new camera setting.\nTable 1: Quantitative comparison of image/video generation quality on Nuscenes\nvalidation set. WoVoGen achieve both multi-view and multi-frame generation, demon-\nstrating the lowest FID and FVD scores among all methods.\nMethod Multi-view Multi-frame\nFID\u2193FVD\u2193\nDriveGan [17]\n\u221a\n73.4\n502.3\nDriveDreamer [40]\n\u221a\n52.6\n452.0\nOurs(single-frame)\n\u221a\n27.6\n-\nOurs(video)\n\u221a\n\u221a\n-\n417.7\nMulti-camera video generation Figure 4 showcases the qualitative results\nof the multi-camera video generation. Thanks to the high-quality world volume\ngeneration and finetuning for temporal consistency, the generated video demon-\nstrates a remarkable level of temporal coherence. The appearance of objects and\nbackground within the video maintains consistency throughout the sequence.\nFurthermore, the quantitative results of video quality, indicated by the FVD\n(417.7) in Table 1, underscores the superiority of our method compared to the\nother works.\n14\nJ. Lu, et al.\n4.3\nAblation studies\nEffectiveness of world volume and object guidance In this study, we\nexamine the significance of world volume and object guidance, as detailed in\nTable 2. We analyze three scenarios: object guidance alone, world volume alone,\nand the combination of world volume with object guidance. When solely relying\non object guidance, interpreted as object mask information, the generated results\nare notably inferior. In contrast, incorporating world volume, regardless of the\npresence of object guidance, significantly enhances the generation outcomes. This\nunderscores the critical importance of world volume in the generative process.\nAdditionally, the incorporation of object guidance leads to more precise object\nlocalization, especially when evaluating the fidelity of the generated images, as\nindicated by the highest FID scores.\nConsistency To assess the quantitative outcomes of multi-camera and temporal\nconsistency, we adopt a quantitative methodology utilizing Key Points Matching\n(KPM) [41]. This approach involves a pre-trained matching model [33] to cal-\nculate the average number of matching keypoints. Table 2 reveals that there is\nan impressive match rate of up to 80.2% for keypoints across successive frames,\nunderscoring a significant accomplishment in maintaining temporal consistency.\nRegarding multi-view consistency, we observe that 90.0% of keypoints match\nbetween adjacent cameras, further highlighting our methodology\u2019s efficacy in\nensuring robust multi-camera consistency.\nTable 2: Left: Quantitative comparison of image generation quality on the nuScenes\nvalidation set with and without object guidance. Right: Quantitative results of multi-\nview consistency and tmerpoal consistency. KPM is used for evaluation\nWorld Volume Object Guidance FID\u2193\nConsistency KPM(%)\u2191\n\u00d7\n\u2713\n28.6\nMulti-view\n90.0\n\u2713\n\u00d7\n27.8\nTemporal\n80.2\n\u2713\n\u2713\n27.6\n5\nConclusion\nIn this paper, we propose WoVoGen, which marks a significant advancement\nin generating multi-camera driving scene videos. Leveraging a novel 4D world\nvolume, it adeptly integrates time with spatial data, addressing the complexity\nof creating content from multi-sensor data while ensuring consistency. This two-\nphase system not only produces high-quality videos based on vehicle controls but\nalso enables complex scene editing, showcasing its potential as a comprehensive\ntool for advancing autonomous driving technologies.\nWoVoGen\n15\nAcknowledgments\nThis work was supported in part by National Natural Science Foundation of\nChina (Grant No. 62106050 and 62376060), Natural Science Foundation of Shang-\nhai (Grant No. 22ZR1407500), USyd-Fudan BISA Flagship Research Program\nand Lingang Laboratory (Grant No. LG-QS-202202-07).\nReferences\n1. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D.,\nLevi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling\nlatent video diffusion models to large datasets. arXiv preprint (2023) 3\n2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,\nPan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous\ndriving. In: CVPR (2020) 2, 10, 11, 12, 18, 19\n3. Chen, Y., Gu, C., Jiang, J., Zhu, X., Zhang, L.: Periodic vibration gaussian: Dy-\nnamic urban scene reconstruction and real-time rendering. arXiv preprint (2023)\n2\n4. Contributors, S.: Spconv: Spatially sparse convolution library. https://github.\ncom/traveller59/spconv (2022) 8\n5. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis (2021) 3\n6. Ding, M., Yang, Z., Hong, W., Zheng, W., Zhou, C., Yin, D., Lin, J., Zou, X., Shao,\nZ., Yang, H., et al.: Cogview: Mastering text-to-image generation via transformers\n(2021) 3\n7. Esser, P., Rombach, R., Ommer, B.: Taming transformers for high-resolution image\nsynthesis. In: CVPR (2021) 7\n8. Fong, W.K., Mohan, R., Hurtado, J.V., Zhou, L., Caesar, H., Beijbom, O., Valada,\nA.: Panoptic nuscenes: A large-scale benchmark for lidar panoptic segmentation\nand tracking (2022) 12\n9. Gao, R., Chen, K., Xie, E., Hong, L., Li, Z., Yeung, D.Y., Xu, Q.: Magicdrive:\nStreet view generation with diverse 3d geometry control. arXiv preprint (2023) 2,\n4\n10. Guo, J., Deng, N., Li, X., Bai, Y., Shi, B., Wang, C., Ding, C., Wang, D., Li, Y.:\nStreetsurf: Extending multi-view implicit surface reconstruction to street views.\narXiv preprint (2023) 2, 4\n11. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained\nby a two time-scale update rule converge to a local nash equilibrium (2017) 12\n12. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models (2020) 3, 5, 7\n13. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: CVPR (2018) 8\n14. Huang, J., Huang, G., Zhu, Z., Ye, Y., Du, D.: Bevdet: High-performance multi-\ncamera 3d object detection in bird-eye-view. arXiv preprint (2021) 7, 19\n15. Huang, L., Chen, D., Liu, Y., Shen, Y., Zhao, D., Zhou, J.: Composer: Creative and\ncontrollable image synthesis with composable conditions. arXiv preprint (2023) 3\n16. Kim, S.W., Brown, B., Yin, K., Kreis, K., Schwarz, K., Li, D., Rombach, R.,\nTorralba, A., Fidler, S.: Neuralfield-ldm: Scene generation with hierarchical latent\ndiffusion models. In: CVPR (2023) 4\n17. Kim, S.W., Philion, J., Torralba, A., Fidler, S.: Drivegan: Towards a controllable\nhigh-quality neural simulation. In: CVPR (2021) 13\n16\nJ. Lu, et al.\n18. Li, X., Zhang, Y., Ye, X.: Drivingdiffusion: Layout-guided multi-view driving scene\nvideo generation with latent diffusion model. arXiv preprint (2023) 2, 4, 5\n19. Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., Lee, Y.J.: Gligen: Open-set\ngrounded text-to-image generation. In: CVPR (2023) 3\n20. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., Dai, J.: Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera images via spatiotem-\nporal transformers. In: ECCV (2022) 2\n21. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-\n1-to-3: Zero-shot one image to 3d object. In: ICCV (2023) 3\n22. Liu, Y., Lin, C., Zeng, Z., Long, X., Liu, L., Komura, T., Wang, W.: Syncdreamer:\nGenerating multiview-consistent images from a single-view image. arXiv preprint\n(2023) 3\n23. Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera\nrigs by implicitly unprojecting to 3d. In: ECCV (2020) 2\n24. Qian, G., Mai, J., Hamdi, A., Ren, J., Siarohin, A., Li, B., Lee, H.Y., Skorokhodov,\nI., Wonka, P., Tulyakov, S., et al.: Magic123: One image to high-quality 3d object\ngeneration using both 2d and 3d diffusion priors. arXiv preprint (2023) 3\n25. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,\nG., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models\nfrom natural language supervision. In: ICML (2021) 2, 8\n26. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint (2022) 3\n27. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022) 3, 5, 7, 9\n28. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-\nbooth: Fine tuning text-to-image diffusion models for subject-driven generation.\nIn: CVPR (2023) 3\n29. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nK., Gontijo Lopes, R., Karagol Ayan, B., Salimans, T., et al.: Photorealistic text-\nto-image diffusion models with deep language understanding (2022) 3\n30. Shi, Y., Wang, P., Ye, J., Long, M., Li, K., Yang, X.: Mvdream: Multi-view diffusion\nfor 3d generation. arXiv preprint (2023) 3\n31. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H.,\nAshual, O., Gafni, O., et al.: Make-a-video: Text-to-video generation without text-\nvideo data. arXiv preprint (2022) 3\n32. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2023)\n3\n33. Sun, J., Shen, Z., Wang, Y., Bao, H., Zhou, X.: Loftr: Detector-free local fea-\nture matching with transformers. In: Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition. pp. 8922\u20138931 (2021) 14\n34. Swerdlow, A., Xu, R., Zhou, B.: Street-view image generation from a bird\u2019s-eye\nview layout. IEEE Robotics and Automation Letters (2024) 2, 4\n35. Tian, X., Jiang, T., Yun, L., Wang, Y., Wang, Y., Zhao, H.: Occ3d: A large-scale\n3d occupancy prediction benchmark for autonomous driving. arXiv preprint (2023)\n12\n36. Tong, W., Sima, C., Wang, T., Chen, L., Wu, S., Deng, H., Gu, Y., Lu, L., Luo,\nP., Lin, D., et al.: Scene as occupancy. In: ICCV (2023) 12\n37. Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly,\nS.: Towards accurate generative models of video: A new metric & challenges. arXiv\npreprint (2018) 12\nWoVoGen\n17\n38. Van Den Oord, A., Vinyals, O., et al.: Neural discrete representation learning\n(2017) 3\n39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\n\u0141., Polosukhin, I.: Attention is all you need (2017) 7\n40. Wang, X., Zhu, Z., Huang, G., Chen, X., Lu, J.: Drivedreamer: Towards real-world-\ndriven world models for autonomous driving. arXiv preprint (2023) 2, 4, 5, 13\n41. Wang, Y., He, J., Fan, L., Li, H., Chen, Y., Zhang, Z.: Driving into the future: Mul-\ntiview visual forecasting and planning with world model for autonomous driving.\narXiv preprint (2023) 14\n42. Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie,\nX., Shou, M.Z.: Tune-a-video: One-shot tuning of image diffusion models for text-\nto-video generation. In: ICCV (2023) 3\n43. Wu, Z., Liu, T., Luo, L., Zhong, Z., Chen, J., Xiao, H., Hou, C., Lou, H., Chen,\nY., Yang, R., et al.: Mars: An instance-aware, modular and realistic simulator for\nautonomous driving. arXiv preprint (2023) 2, 4\n44. Xie, Z., Zhang, J., Li, W., Zhang, F., Zhang, L.: S-nerf: Neural radiance fields for\nstreet views. In: ICLR (2023) 2, 4\n45. Yang, K., Ma, E., Peng, J., Guo, Q., Lin, D., Yu, K.: Bevcontrol: Accurately\ncontrolling street-view elements with multi-perspective consistency via bev sketch\nlayout. arXiv preprint (2023) 2, 4\n46. Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun,\nR.: Unisim: A neural closed-loop sensor simulator. In: CVPR (2023) 2, 4\n47. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023) 3, 9\n48. Zheng, G., Zhou, X., Li, X., Qi, Z., Shan, Y., Li, X.: Layoutdiffusion: Controllable\ndiffusion model for layout-to-image generation. In: CVPR (2023) 3\n49. Zhou, D., Wang, W., Yan, H., Lv, W., Zhu, Y., Feng, J.: Magicvideo: Efficient\nvideo generation with latent diffusion models. arXiv preprint (2022) 3\n18\nJ. Lu, et al.\nA\nAppendix\nA.1\nMasks for object guidance\nWe have devised an efficient approach to compute masks for object guidance.\nSpecifically, for the original occupancy voxel grids belongs to a certain class\nPclass \u2208RN\u00d73, our initial step involves projecting them onto the camera imag-\ning plane using the camera\u2019s intrinsic properties K and extrinsic properties T,\nperformed as follows:\n  P_{ p roj}=K(RP _ {class}+t), \n(8)\nwhere R = T \u22121[: 3, : 3] and t = T \u22121[: 3, 3] represent the rotation matrix and\ntranslation vector of the inverse of T, respectively. Following this, we initialize an\nall-zero image mclass, matching the dimensions of the camera image. For each\nnormalized projected point P i\nproj, we efficiently simulate voxel projections by\nmarking a square-shaped region in mclass as occupied:\n  m_{cla ss\n}[P_ { p r o j}\n^{i_ { x} } -\\\ndelt a  : P _{\nproj } ^{ i _{x}}+\\delta ,P_{proj}^{i_{y}}-\\delta :P_{proj}^{i_{y}}+\\delta ]=1, \n(9)\nwhere P ix\nproj and P iy\nproj represent the normalized x and y components of P i\nproj,\nwhile \u03b4 is inversely proportional to the depth of the projection points P iz\nproj:\n  \\de lt\na =d/P_{proj}^{i_{z}}, \n(10)\nwhere d is a hyperparameter set to 375. Several generated mask samples are\nillustrated in Figure 8.\nA.2\nZero-shot generation\nIn Figure 7, we present intriguing findings regarding zero-shot generation out-\ncomes.\nBeyond modifying the camera\u2019s extrinsic parameters, we also experimented\nwith changing the intrinsic settings of the camera, configurations not encountered\nin the dataset. The successful generations demonstrate our model\u2019s ability to\ngeneralize across different world volumes, indicating that it genuinely learns\nfrom world volume features instead of merely overfitting to the data.\nWe experimented with an unseen weather condition, \"heavy snow,\" by alter-\ning the weather condition component of the scene guidance. This snow weather\ncondition is entirely absent in the nuScenes dataset [2]. Nonetheless, our model\nincorporates the concept of snow from the Stable Diffusion pretrained model,\nintegrating it into the scene based on its understanding of snow weather. This\ndemonstrates the model\u2019s ability to amalgamate concepts from the pretrained\nmodel with the fine-tuned street view images, showcasing its versatility and\nadaptability in generating content under novel conditions.\nInterestingly, we altered the location word to the entirely different city of\nShanghai, which presents significant differences from Singapore and Boston. This\nmodification resulted in the generation of images that depict foggy-like weather\nWoVoGen\n19\nand the architectural style characteristic of Shanghai\u2019s highways. This ability to\nadapt the visual representation to a distinct geographical context further illus-\ntrates the model\u2019s capacity to inherit and apply concepts from the Stable Diffu-\nsion pretraining. It showcases the model\u2019s remarkable flexibility in synthesizing\nelements that reflect specific environmental and cultural attributes associated\nwith various locations.\nWe also explored out-of-domain modifications in object guidance by sub-\nstituting the word \"car\" with \"purple car.\" Purple cars are quite rare in the\ndataset. Nonetheless, our model effectively integrated the concept of purple,\nsuccessfully altering all cars in the scene to purple. This experiment underscores\nthe model\u2019s ability to comprehend and apply color attributes to specific objects\nwithin a scene, demonstrating its proficiency in adapting to and implementing\nnovel characteristics not commonly found in the training data.\nA.3\nDownstream tasks\nTo enhance performance through more comprehensive data training, we integrate\nour generation results with the nuScenes [2] training set as a supplementary\ndata source. We test our results on 3D object detection, with BEVDet [14] as\nbaseline. We evaluate our outcomes in 3D object detection, using BEVDet as our\nbaseline model. Our approach involves generating a complete training dataset\nbased on the ground truth. Subsequently, we train the 3D object detection model\nusing BEVDet on both the original nuScenes dataset and our newly generated\nnuScenes dataset. Since our world volume class encompasses only cars, trucks,\nand buses, our testing focuses exclusively on vehicles within these categories.\nMethod\nNDSv \u2191mAPv \u2191mATEv \u2193mASEv \u2193mAOEv \u2193mAVEv \u2193mAAEv \u2193\noriginal\n17.45\n34.9\n69.2\n20.4\n17.9\n124.6\n28.6\n+ generated\n18.10\n36.2\n68.6\n20.1\n15.7\n123.4\n28.1\nTable 3: The enhancement brought by our generated data for 3D object detection.\nThe evaluation is on the vehicle classes of cars, trucks, and buses.\nAs indicated in Table 3, we observed a significant improvement in the mean\nAverage Precision (mAP). Upon analyzing the error contributions, it\u2019s evident\nthat the most substantial improvement arises from a reduction in orientation\nerror, which decreased from 17.9 to 15.7. This demonstrates that our gener-\nated data indeed enhances the training of downstream tasks, such as 3D object\ndetection.\nA.4\nAblation studies\nEffectiveness of temporal finetuning To verify the effectiveness of the video con-\nsistency module, we illustrate the simulation outcomes of the front and back\ncameras in Figure 9 after removing the temporal finetuning.\n20\nJ. Lu, et al.\nUnseen camera \nintrinsic:\nchanging focal \nlength.\nUnseen weather \ncondition:\n\u201cheavy snow\u201d\nUnseen city \nlocation:\n\u201cShanghai\u201d\nUnseen color:\n\u201cpurple car\u201d\nFig. 7: Examples of out-of-domain generation\nObviously, the finetuned model notably maintains consistent object and back-\nground appearances across frames. In contrast, the single-frame model struggles\nto achieve this level of consistency.\nWoVoGen\n21\nA.5\nVideo results\nWe show the video results in file 10410_video.mp4.\nA.6\nAdditional single-frame image generation results\nWe provide additional single-frame image generation samples showcasing more\ndiverse editing and controlling capabilities in Figure 10.\nUtilizing the Lego-style edit-friendly features of the world volume offers\nboundless possibilities for editing and controlling, resulting in numerous po-\ntential generation outcomes, some of which may even be rare in the real world.\ntarget image\nbus \ncar \npedestrian \ntruck \nmanmade\nvegetation\nFig. 8: Object masks calculated by voxel projection. Origin images and per-class\nmasks are organized as: Top: front left, front, front right; Bottom: back right, back,\nback left.\nVideo generation with single-frame model\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\n\ud835\udc95\ud835\udfd3\n\ud835\udc95\ud835\udfd4\nFRONT\nFRONT\nFRONT\nFRONT\nFRONT\nFRONT\nBACK\nBACK\nBACK\nBACK\nBACK\nBACK\nVideo generation with temporal attention\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\n\ud835\udc95\ud835\udfd3\n\ud835\udc95\ud835\udfd4\nFRONT\nFRONT\nFRONT\nFRONT\nFRONT\nFRONT\nBACK\nBACK\nBACK\nBACK\nBACK\nBACK\nFig. 9: Qualitative ablation study on temporal finetuning. Consistency in ob-\nject and background preservation across frames is evident when conduct temporal\nfinetuning.\n22\nJ. Lu, et al.\nControlling and editing\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nGround truth\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nGenerated\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\n\u201cDrive at night\u201d\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\n\u201cDrive at night in rain\u201d\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\n\u201cDrive in singaporehollandvillage\u201d\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nRemove all vehicles \nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nCongestion \nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nBuses and cars block the road \nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nTaller buildings on the left side/ closer buildings on the right side\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nPedestrians crossing road\nFig. 10: Additional controlling and editing samples.\nWoVoGen\n23\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nMulti-camera driving scene generation\nWorld volume generation\nFig. 11: Additional videos generated from the generated world volumes\n24\nJ. Lu, et al.\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nMulti-camera driving scene generation\nWorld volume generation\nFig. 12: Additional videos generated from the generated world volumes\nWoVoGen\n25\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nMulti-camera driving scene generation\nWorld volume generation\nFig. 13: Additional videos generated from the generated world volumes\n26\nJ. Lu, et al.\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nFRONT  LEFT\nFRONT\nFRONT RIGHT\nBACK RIGHT\nBACK\nBACK LEFT\nHD MAP\nOCCUPANCY\n\ud835\udc95\ud835\udfcf\n\ud835\udc95\ud835\udfd0\n\ud835\udc95\ud835\udfd1\n\ud835\udc95\ud835\udfd2\nMulti-camera driving scene generation\nWorld volume generation\nFig. 14: Additional videos generated from the generated world volumes\n",
    "1609.02907": "Published as a conference paper at ICLR 2017\nSEMI-SUPERVISED CLASSIFICATION WITH\nGRAPH CONVOLUTIONAL NETWORKS\nThomas N. Kipf\nUniversity of Amsterdam\nT.N.Kipf@uva.nl\nMax Welling\nUniversity of Amsterdam\nCanadian Institute for Advanced Research (CIFAR)\nM.Welling@uva.nl\nABSTRACT\nWe present a scalable approach for semi-supervised learning on graph-structured\ndata that is based on an ef\ufb01cient variant of convolutional neural networks which\noperate directly on graphs. We motivate the choice of our convolutional archi-\ntecture via a localized \ufb01rst-order approximation of spectral graph convolutions.\nOur model scales linearly in the number of graph edges and learns hidden layer\nrepresentations that encode both local graph structure and features of nodes. In\na number of experiments on citation networks and on a knowledge graph dataset\nwe demonstrate that our approach outperforms related methods by a signi\ufb01cant\nmargin.\n1\nINTRODUCTION\nWe consider the problem of classifying nodes (such as documents) in a graph (such as a citation\nnetwork), where labels are only available for a small subset of nodes. This problem can be framed\nas graph-based semi-supervised learning, where label information is smoothed over the graph via\nsome form of explicit graph-based regularization (Zhu et al., 2003; Zhou et al., 2004; Belkin et al.,\n2006; Weston et al., 2012), e.g. by using a graph Laplacian regularization term in the loss function:\nL = L0 + \u03bbLreg ,\nwith\nLreg =\nX\ni,j\nAij\u2225f(Xi) \u2212f(Xj)\u22252 = f(X)\u22a4\u2206f(X) .\n(1)\nHere, L0 denotes the supervised loss w.r.t. the labeled part of the graph, f(\u00b7) can be a neural network-\nlike differentiable function, \u03bb is a weighing factor and X is a matrix of node feature vectors Xi.\n\u2206= D \u2212A denotes the unnormalized graph Laplacian of an undirected graph G = (V, E) with\nN nodes vi \u2208V, edges (vi, vj) \u2208E, an adjacency matrix A \u2208RN\u00d7N (binary or weighted) and\na degree matrix Dii = P\nj Aij. The formulation of Eq. 1 relies on the assumption that connected\nnodes in the graph are likely to share the same label. This assumption, however, might restrict\nmodeling capacity, as graph edges need not necessarily encode node similarity, but could contain\nadditional information.\nIn this work, we encode the graph structure directly using a neural network model f(X, A) and\ntrain on a supervised target L0 for all nodes with labels, thereby avoiding explicit graph-based\nregularization in the loss function. Conditioning f(\u00b7) on the adjacency matrix of the graph will\nallow the model to distribute gradient information from the supervised loss L0 and will enable it to\nlearn representations of nodes both with and without labels.\nOur contributions are two-fold. Firstly, we introduce a simple and well-behaved layer-wise prop-\nagation rule for neural network models which operate directly on graphs and show how it can be\nmotivated from a \ufb01rst-order approximation of spectral graph convolutions (Hammond et al., 2011).\nSecondly, we demonstrate how this form of a graph-based neural network model can be used for\nfast and scalable semi-supervised classi\ufb01cation of nodes in a graph. Experiments on a number of\ndatasets demonstrate that our model compares favorably both in classi\ufb01cation accuracy and ef\ufb01-\nciency (measured in wall-clock time) against state-of-the-art methods for semi-supervised learning.\n1\narXiv:1609.02907v4  [cs.LG]  22 Feb 2017\nPublished as a conference paper at ICLR 2017\n2\nFAST APPROXIMATE CONVOLUTIONS ON GRAPHS\nIn this section, we provide theoretical motivation for a speci\ufb01c graph-based neural network model\nf(X, A) that we will use in the rest of this paper. We consider a multi-layer Graph Convolutional\nNetwork (GCN) with the following layer-wise propagation rule:\nH(l+1) = \u03c3\n\u0010\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 H(l)W (l)\u0011\n.\n(2)\nHere, \u02dcA = A + IN is the adjacency matrix of the undirected graph G with added self-connections.\nIN is the identity matrix, \u02dcDii = P\nj \u02dcAij and W (l) is a layer-speci\ufb01c trainable weight matrix. \u03c3(\u00b7)\ndenotes an activation function, such as the ReLU(\u00b7) = max(0, \u00b7). H(l) \u2208RN\u00d7D is the matrix of ac-\ntivations in the lth layer; H(0) = X. In the following, we show that the form of this propagation rule\ncan be motivated1 via a \ufb01rst-order approximation of localized spectral \ufb01lters on graphs (Hammond\net al., 2011; Defferrard et al., 2016).\n2.1\nSPECTRAL GRAPH CONVOLUTIONS\nWe consider spectral convolutions on graphs de\ufb01ned as the multiplication of a signal x \u2208RN (a\nscalar for every node) with a \ufb01lter g\u03b8 = diag(\u03b8) parameterized by \u03b8 \u2208RN in the Fourier domain,\ni.e.:\ng\u03b8 \u22c6x = Ug\u03b8U \u22a4x ,\n(3)\nwhere U is the matrix of eigenvectors of the normalized graph Laplacian L = IN \u2212D\u22121\n2 AD\u22121\n2 =\nU\u039bU \u22a4, with a diagonal matrix of its eigenvalues \u039b and U \u22a4x being the graph Fourier transform\nof x. We can understand g\u03b8 as a function of the eigenvalues of L, i.e. g\u03b8(\u039b). Evaluating Eq. 3 is\ncomputationally expensive, as multiplication with the eigenvector matrix U is O(N 2). Furthermore,\ncomputing the eigendecomposition of L in the \ufb01rst place might be prohibitively expensive for large\ngraphs. To circumvent this problem, it was suggested in Hammond et al. (2011) that g\u03b8(\u039b) can be\nwell-approximated by a truncated expansion in terms of Chebyshev polynomials Tk(x) up to Kth\norder:\ng\u03b8\u2032(\u039b) \u2248\nK\nX\nk=0\n\u03b8\u2032\nkTk(\u02dc\u039b) ,\n(4)\nwith a rescaled \u02dc\u039b =\n2\n\u03bbmax \u039b \u2212IN. \u03bbmax denotes the largest eigenvalue of L. \u03b8\u2032 \u2208RK is now a\nvector of Chebyshev coef\ufb01cients. The Chebyshev polynomials are recursively de\ufb01ned as Tk(x) =\n2xTk\u22121(x) \u2212Tk\u22122(x), with T0(x) = 1 and T1(x) = x. The reader is referred to Hammond et al.\n(2011) for an in-depth discussion of this approximation.\nGoing back to our de\ufb01nition of a convolution of a signal x with a \ufb01lter g\u03b8\u2032, we now have:\ng\u03b8\u2032 \u22c6x \u2248\nK\nX\nk=0\n\u03b8\u2032\nkTk(\u02dcL)x ,\n(5)\nwith \u02dcL =\n2\n\u03bbmax L \u2212IN; as can easily be veri\ufb01ed by noticing that (U\u039bU \u22a4)k = U\u039bkU \u22a4. Note that\nthis expression is now K-localized since it is a Kth-order polynomial in the Laplacian, i.e. it depends\nonly on nodes that are at maximum K steps away from the central node (Kth-order neighborhood).\nThe complexity of evaluating Eq. 5 is O(|E|), i.e. linear in the number of edges. Defferrard et al.\n(2016) use this K-localized convolution to de\ufb01ne a convolutional neural network on graphs.\n2.2\nLAYER-WISE LINEAR MODEL\nA neural network model based on graph convolutions can therefore be built by stacking multiple\nconvolutional layers of the form of Eq. 5, each layer followed by a point-wise non-linearity. Now,\nimagine we limited the layer-wise convolution operation to K = 1 (see Eq. 5), i.e. a function that is\nlinear w.r.t. L and therefore a linear function on the graph Laplacian spectrum.\n1We provide an alternative interpretation of this propagation rule based on the Weisfeiler-Lehman algorithm\n(Weisfeiler & Lehmann, 1968) in Appendix A.\n2\nPublished as a conference paper at ICLR 2017\nIn this way, we can still recover a rich class of convolutional \ufb01lter functions by stacking multiple\nsuch layers, but we are not limited to the explicit parameterization given by, e.g., the Chebyshev\npolynomials. We intuitively expect that such a model can alleviate the problem of over\ufb01tting on\nlocal neighborhood structures for graphs with very wide node degree distributions, such as social\nnetworks, citation networks, knowledge graphs and many other real-world graph datasets. Addition-\nally, for a \ufb01xed computational budget, this layer-wise linear formulation allows us to build deeper\nmodels, a practice that is known to improve modeling capacity on a number of domains (He et al.,\n2016).\nIn this linear formulation of a GCN we further approximate \u03bbmax \u22482, as we can expect that neural\nnetwork parameters will adapt to this change in scale during training. Under these approximations\nEq. 5 simpli\ufb01es to:\ng\u03b8\u2032 \u22c6x \u2248\u03b8\u2032\n0x + \u03b8\u2032\n1 (L \u2212IN) x = \u03b8\u2032\n0x \u2212\u03b8\u2032\n1D\u22121\n2 AD\u22121\n2 x ,\n(6)\nwith two free parameters \u03b8\u2032\n0 and \u03b8\u2032\n1. The \ufb01lter parameters can be shared over the whole graph.\nSuccessive application of \ufb01lters of this form then effectively convolve the kth-order neighborhood of\na node, where k is the number of successive \ufb01ltering operations or convolutional layers in the neural\nnetwork model.\nIn practice, it can be bene\ufb01cial to constrain the number of parameters further to address over\ufb01tting\nand to minimize the number of operations (such as matrix multiplications) per layer. This leaves us\nwith the following expression:\ng\u03b8 \u22c6x \u2248\u03b8\n\u0010\nIN + D\u22121\n2 AD\u22121\n2\n\u0011\nx ,\n(7)\nwith a single parameter \u03b8 = \u03b8\u2032\n0 = \u2212\u03b8\u2032\n1. Note that IN + D\u22121\n2 AD\u22121\n2 now has eigenvalues in\nthe range [0, 2]. Repeated application of this operator can therefore lead to numerical instabilities\nand exploding/vanishing gradients when used in a deep neural network model. To alleviate this\nproblem, we introduce the following renormalization trick: IN +D\u22121\n2 AD\u22121\n2 \u2192\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 , with\n\u02dcA = A + IN and \u02dcDii = P\nj \u02dcAij.\nWe can generalize this de\ufb01nition to a signal X \u2208RN\u00d7C with C input channels (i.e. a C-dimensional\nfeature vector for every node) and F \ufb01lters or feature maps as follows:\nZ = \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 X\u0398 ,\n(8)\nwhere \u0398 \u2208RC\u00d7F is now a matrix of \ufb01lter parameters and Z \u2208RN\u00d7F is the convolved signal\nmatrix. This \ufb01ltering operation has complexity O(|E|FC), as \u02dcAX can be ef\ufb01ciently implemented\nas a product of a sparse matrix with a dense matrix.\n3\nSEMI-SUPERVISED NODE CLASSIFICATION\nHaving introduced a simple, yet \ufb02exible model f(X, A) for ef\ufb01cient information propagation on\ngraphs, we can return to the problem of semi-supervised node classi\ufb01cation. As outlined in the in-\ntroduction, we can relax certain assumptions typically made in graph-based semi-supervised learn-\ning by conditioning our model f(X, A) both on the data X and on the adjacency matrix A of the\nunderlying graph structure. We expect this setting to be especially powerful in scenarios where the\nadjacency matrix contains information not present in the data X, such as citation links between doc-\numents in a citation network or relations in a knowledge graph. The overall model, a multi-layer\nGCN for semi-supervised learning, is schematically depicted in Figure 1.\n3.1\nEXAMPLE\nIn the following, we consider a two-layer GCN for semi-supervised node classi\ufb01cation on a graph\nwith a symmetric adjacency matrix A (binary or weighted). We \ufb01rst calculate \u02c6A = \u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 in\na pre-processing step. Our forward model then takes the simple form:\nZ = f(X, A) = softmax\n\u0010\n\u02c6A ReLU\n\u0010\n\u02c6AXW (0)\u0011\nW (1)\u0011\n.\n(9)\n3\nPublished as a conference paper at ICLR 2017\nC\ninput layer\nX1\nX2\nX3\nX4\nF\noutput layer\nZ1\nZ2\nZ3\nZ4\nhidden\nlayers\nY1\nY4\n1\n(a) Graph Convolutional Network\n30\n20\n10\n0\n10\n20\n30\n30\n20\n10\n0\n10\n20\n30\n(b) Hidden layer activations\nFigure 1: Left: Schematic depiction of multi-layer Graph Convolutional Network (GCN) for semi-\nsupervised learning with C input channels and F feature maps in the output layer. The graph struc-\nture (edges shown as black lines) is shared over layers, labels are denoted by Yi. Right: t-SNE\n(Maaten & Hinton, 2008) visualization of hidden layer activations of a two-layer GCN trained on\nthe Cora dataset (Sen et al., 2008) using 5% of labels. Colors denote document class.\nHere, W (0) \u2208RC\u00d7H is an input-to-hidden weight matrix for a hidden layer with H feature maps.\nW (1) \u2208RH\u00d7F is a hidden-to-output weight matrix. The softmax activation function, de\ufb01ned as\nsoftmax(xi) = 1\nZ exp(xi) with Z = P\ni exp(xi), is applied row-wise. For semi-supervised multi-\nclass classi\ufb01cation, we then evaluate the cross-entropy error over all labeled examples:\nL = \u2212\nX\nl\u2208YL\nF\nX\nf=1\nYlf ln Zlf ,\n(10)\nwhere YL is the set of node indices that have labels.\nThe neural network weights W (0) and W (1) are trained using gradient descent. In this work, we\nperform batch gradient descent using the full dataset for every training iteration, which is a viable\noption as long as datasets \ufb01t in memory. Using a sparse representation for A, memory requirement\nis O(|E|), i.e. linear in the number of edges. Stochasticity in the training process is introduced via\ndropout (Srivastava et al., 2014). We leave memory-ef\ufb01cient extensions with mini-batch stochastic\ngradient descent for future work.\n3.2\nIMPLEMENTATION\nIn practice, we make use of TensorFlow (Abadi et al., 2015) for an ef\ufb01cient GPU-based imple-\nmentation2 of Eq. 9 using sparse-dense matrix multiplications. The computational complexity of\nevaluating Eq. 9 is then O(|E|CHF), i.e. linear in the number of graph edges.\n4\nRELATED WORK\nOur model draws inspiration both from the \ufb01eld of graph-based semi-supervised learning and from\nrecent work on neural networks that operate on graphs. In what follows, we provide a brief overview\non related work in both \ufb01elds.\n4.1\nGRAPH-BASED SEMI-SUPERVISED LEARNING\nA large number of approaches for semi-supervised learning using graph representations have been\nproposed in recent years, most of which fall into two broad categories: methods that use some\nform of explicit graph Laplacian regularization and graph embedding-based approaches. Prominent\nexamples for graph Laplacian regularization include label propagation (Zhu et al., 2003), manifold\nregularization (Belkin et al., 2006) and deep semi-supervised embedding (Weston et al., 2012).\n2Code to reproduce our experiments is available at https://github.com/tkipf/gcn.\n4\nPublished as a conference paper at ICLR 2017\nRecently, attention has shifted to models that learn graph embeddings with methods inspired by\nthe skip-gram model (Mikolov et al., 2013). DeepWalk (Perozzi et al., 2014) learns embeddings\nvia the prediction of the local neighborhood of nodes, sampled from random walks on the graph.\nLINE (Tang et al., 2015) and node2vec (Grover & Leskovec, 2016) extend DeepWalk with more\nsophisticated random walk or breadth-\ufb01rst search schemes. For all these methods, however, a multi-\nstep pipeline including random walk generation and semi-supervised training is required where each\nstep has to be optimized separately. Planetoid (Yang et al., 2016) alleviates this by injecting label\ninformation in the process of learning embeddings.\n4.2\nNEURAL NETWORKS ON GRAPHS\nNeural networks that operate on graphs have previously been introduced in Gori et al. (2005);\nScarselli et al. (2009) as a form of recurrent neural network. Their framework requires the repeated\napplication of contraction maps as propagation functions until node representations reach a stable\n\ufb01xed point. This restriction was later alleviated in Li et al. (2016) by introducing modern practices\nfor recurrent neural network training to the original graph neural network framework. Duvenaud\net al. (2015) introduced a convolution-like propagation rule on graphs and methods for graph-level\nclassi\ufb01cation. Their approach requires to learn node degree-speci\ufb01c weight matrices which does not\nscale to large graphs with wide node degree distributions. Our model instead uses a single weight\nmatrix per layer and deals with varying node degrees through an appropriate normalization of the\nadjacency matrix (see Section 3.1).\nA related approach to node classi\ufb01cation with a graph-based neural network was recently introduced\nin Atwood & Towsley (2016). They report O(N 2) complexity, limiting the range of possible appli-\ncations. In a different yet related model, Niepert et al. (2016) convert graphs locally into sequences\nthat are fed into a conventional 1D convolutional neural network, which requires the de\ufb01nition of a\nnode ordering in a pre-processing step.\nOur method is based on spectral graph convolutional neural networks, introduced in Bruna et al.\n(2014) and later extended by Defferrard et al. (2016) with fast localized convolutions. In contrast\nto these works, we consider here the task of transductive node classi\ufb01cation within networks of\nsigni\ufb01cantly larger scale. We show that in this setting, a number of simpli\ufb01cations (see Section 2.2)\ncan be introduced to the original frameworks of Bruna et al. (2014) and Defferrard et al. (2016) that\nimprove scalability and classi\ufb01cation performance in large-scale networks.\n5\nEXPERIMENTS\nWe test our model in a number of experiments: semi-supervised document classi\ufb01cation in cita-\ntion networks, semi-supervised entity classi\ufb01cation in a bipartite graph extracted from a knowledge\ngraph, an evaluation of various graph propagation models and a run-time analysis on random graphs.\n5.1\nDATASETS\nWe closely follow the experimental setup in Yang et al. (2016). Dataset statistics are summarized\nin Table 1. In the citation network datasets\u2014Citeseer, Cora and Pubmed (Sen et al., 2008)\u2014nodes\nare documents and edges are citation links. Label rate denotes the number of labeled nodes that are\nused for training divided by the total number of nodes in each dataset. NELL (Carlson et al., 2010;\nYang et al., 2016) is a bipartite graph dataset extracted from a knowledge graph with 55,864 relation\nnodes and 9,891 entity nodes.\nTable 1: Dataset statistics, as reported in Yang et al. (2016).\nDataset\nType\nNodes\nEdges\nClasses\nFeatures\nLabel rate\nCiteseer\nCitation network\n3,327\n4,732\n6\n3,703\n0.036\nCora\nCitation network\n2,708\n5,429\n7\n1,433\n0.052\nPubmed\nCitation network\n19,717\n44,338\n3\n500\n0.003\nNELL\nKnowledge graph\n65,755\n266,144\n210\n5,414\n0.001\n5\nPublished as a conference paper at ICLR 2017\nCitation networks\nWe consider three citation network datasets: Citeseer, Cora and Pubmed (Sen\net al., 2008). The datasets contain sparse bag-of-words feature vectors for each document and a list\nof citation links between documents. We treat the citation links as (undirected) edges and construct\na binary, symmetric adjacency matrix A. Each document has a class label. For training, we only use\n20 labels per class, but all feature vectors.\nNELL\nNELL is a dataset extracted from the knowledge graph introduced in (Carlson et al., 2010).\nA knowledge graph is a set of entities connected with directed, labeled edges (relations). We follow\nthe pre-processing scheme as described in Yang et al. (2016). We assign separate relation nodes\nr1 and r2 for each entity pair (e1, r, e2) as (e1, r1) and (e2, r2). Entity nodes are described by\nsparse feature vectors. We extend the number of features in NELL by assigning a unique one-hot\nrepresentation for every relation node, effectively resulting in a 61,278-dim sparse feature vector per\nnode. The semi-supervised task here considers the extreme case of only a single labeled example\nper class in the training set. We construct a binary, symmetric adjacency matrix from this graph by\nsetting entries Aij = 1, if one or more edges are present between nodes i and j.\nRandom graphs\nWe simulate random graph datasets of various sizes for experiments where we\nmeasure training time per epoch. For a dataset with N nodes we create a random graph assigning\n2N edges uniformly at random. We take the identity matrix IN as input feature matrix X, thereby\nimplicitly taking a featureless approach where the model is only informed about the identity of each\nnode, speci\ufb01ed by a unique one-hot vector. We add dummy labels Yi = 1 for every node.\n5.2\nEXPERIMENTAL SET-UP\nUnless otherwise noted, we train a two-layer GCN as described in Section 3.1 and evaluate pre-\ndiction accuracy on a test set of 1,000 labeled examples. We provide additional experiments using\ndeeper models with up to 10 layers in Appendix B. We choose the same dataset splits as in Yang et al.\n(2016) with an additional validation set of 500 labeled examples for hyperparameter optimization\n(dropout rate for all layers, L2 regularization factor for the \ufb01rst GCN layer and number of hidden\nunits). We do not use the validation set labels for training.\nFor the citation network datasets, we optimize hyperparameters on Cora only and use the same set\nof parameters for Citeseer and Pubmed. We train all models for a maximum of 200 epochs (training\niterations) using Adam (Kingma & Ba, 2015) with a learning rate of 0.01 and early stopping with a\nwindow size of 10, i.e. we stop training if the validation loss does not decrease for 10 consecutive\nepochs. We initialize weights using the initialization described in Glorot & Bengio (2010) and\naccordingly (row-)normalize input feature vectors. On the random graph datasets, we use a hidden\nlayer size of 32 units and omit regularization (i.e. neither dropout nor L2 regularization).\n5.3\nBASELINES\nWe compare against the same baseline methods as in Yang et al. (2016), i.e. label propagation\n(LP) (Zhu et al., 2003), semi-supervised embedding (SemiEmb) (Weston et al., 2012), manifold\nregularization (ManiReg) (Belkin et al., 2006) and skip-gram based graph embeddings (DeepWalk)\n(Perozzi et al., 2014). We omit TSVM (Joachims, 1999), as it does not scale to the large number of\nclasses in one of our datasets.\nWe further compare against the iterative classi\ufb01cation algorithm (ICA) proposed in Lu & Getoor\n(2003) in conjunction with two logistic regression classi\ufb01ers, one for local node features alone and\none for relational classi\ufb01cation using local features and an aggregation operator as described in\nSen et al. (2008). We \ufb01rst train the local classi\ufb01er using all labeled training set nodes and use\nit to bootstrap class labels of unlabeled nodes for relational classi\ufb01er training. We run iterative\nclassi\ufb01cation (relational classi\ufb01er) with a random node ordering for 10 iterations on all unlabeled\nnodes (bootstrapped using the local classi\ufb01er). L2 regularization parameter and aggregation operator\n(count vs. prop, see Sen et al. (2008)) are chosen based on validation set performance for each dataset\nseparately.\nLastly, we compare against Planetoid (Yang et al., 2016), where we always choose their best-\nperforming model variant (transductive vs. inductive) as a baseline.\n6\nPublished as a conference paper at ICLR 2017\n6\nRESULTS\n6.1\nSEMI-SUPERVISED NODE CLASSIFICATION\nResults are summarized in Table 2. Reported numbers denote classi\ufb01cation accuracy in percent. For\nICA, we report the mean accuracy of 100 runs with random node orderings. Results for all other\nbaseline methods are taken from the Planetoid paper (Yang et al., 2016). Planetoid* denotes the best\nmodel for the respective dataset out of the variants presented in their paper.\nTable 2: Summary of results in terms of classi\ufb01cation accuracy (in percent).\nMethod\nCiteseer\nCora\nPubmed\nNELL\nManiReg [3]\n60.1\n59.5\n70.7\n21.8\nSemiEmb [28]\n59.6\n59.0\n71.1\n26.7\nLP [32]\n45.3\n68.0\n63.0\n26.5\nDeepWalk [22]\n43.2\n67.2\n65.3\n58.1\nICA [18]\n69.1\n75.1\n73.9\n23.1\nPlanetoid* [29]\n64.7 (26s)\n75.7 (13s)\n77.2 (25s)\n61.9 (185s)\nGCN (this paper)\n70.3 (7s)\n81.5 (4s)\n79.0 (38s)\n66.0 (48s)\nGCN (rand. splits)\n67.9 \u00b1 0.5\n80.1 \u00b1 0.5\n78.9 \u00b1 0.7\n58.4 \u00b1 1.7\nWe further report wall-clock training time in seconds until convergence (in brackets) for our method\n(incl. evaluation of validation error) and for Planetoid. For the latter, we used an implementation pro-\nvided by the authors3 and trained on the same hardware (with GPU) as our GCN model. We trained\nand tested our model on the same dataset splits as in Yang et al. (2016) and report mean accuracy\nof 100 runs with random weight initializations. We used the following sets of hyperparameters for\nCiteseer, Cora and Pubmed: 0.5 (dropout rate), 5 \u00b7 10\u22124 (L2 regularization) and 16 (number of hid-\nden units); and for NELL: 0.1 (dropout rate), 1 \u00b7 10\u22125 (L2 regularization) and 64 (number of hidden\nunits).\nIn addition, we report performance of our model on 10 randomly drawn dataset splits of the same\nsize as in Yang et al. (2016), denoted by GCN (rand. splits). Here, we report mean and standard\nerror of prediction accuracy on the test set split in percent.\n6.2\nEVALUATION OF PROPAGATION MODEL\nWe compare different variants of our proposed per-layer propagation model on the citation network\ndatasets. We follow the experimental set-up described in the previous section. Results are summa-\nrized in Table 3. The propagation model of our original GCN model is denoted by renormalization\ntrick (in bold). In all other cases, the propagation model of both neural network layers is replaced\nwith the model speci\ufb01ed under propagation model. Reported numbers denote mean classi\ufb01cation\naccuracy for 100 repeated runs with random weight matrix initializations. In case of multiple vari-\nables \u0398i per layer, we impose L2 regularization on all weight matrices of the \ufb01rst layer.\nTable 3: Comparison of propagation models.\nDescription\nPropagation model\nCiteseer\nCora\nPubmed\nChebyshev \ufb01lter (Eq. 5)\nK = 3\nPK\nk=0 Tk(\u02dcL)X\u0398k\n69.8\n79.5\n74.4\nK = 2\n69.6\n81.2\n73.8\n1st-order model (Eq. 6)\nX\u03980 + D\u22121\n2 AD\u22121\n2 X\u03981\n68.3\n80.0\n77.5\nSingle parameter (Eq. 7)\n(IN + D\u22121\n2 AD\u22121\n2 )X\u0398\n69.3\n79.2\n77.4\nRenormalization trick (Eq. 8)\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 X\u0398\n70.3\n81.5\n79.0\n1st-order term only\nD\u22121\n2 AD\u22121\n2 X\u0398\n68.7\n80.5\n77.8\nMulti-layer perceptron\nX\u0398\n46.5\n55.1\n71.4\n3https://github.com/kimiyoung/planetoid\n7\nPublished as a conference paper at ICLR 2017\n6.3\nTRAINING TIME PER EPOCH\n1k\n10k\n100k\n1M\n10M\n# Edges\n10-3\n10-2\n10-1\n100\n101\nSec./epoch\n*\nGPU\nCPU\nFigure 2: Wall-clock time per epoch for random\ngraphs. (*) indicates out-of-memory error.\nHere, we report results for the mean training\ntime per epoch (forward pass, cross-entropy\ncalculation, backward pass) for 100 epochs on\nsimulated random graphs, measured in seconds\nwall-clock time. See Section 5.1 for a detailed\ndescription of the random graph dataset used\nin these experiments. We compare results on\na GPU and on a CPU-only implementation4 in\nTensorFlow (Abadi et al., 2015). Figure 2 sum-\nmarizes the results.\n7\nDISCUSSION\n7.1\nSEMI-SUPERVISED MODEL\nIn the experiments demonstrated here, our method for semi-supervised node classi\ufb01cation outper-\nforms recent related methods by a signi\ufb01cant margin. Methods based on graph-Laplacian regular-\nization (Zhu et al., 2003; Belkin et al., 2006; Weston et al., 2012) are most likely limited due to their\nassumption that edges encode mere similarity of nodes. Skip-gram based methods on the other hand\nare limited by the fact that they are based on a multi-step pipeline which is dif\ufb01cult to optimize.\nOur proposed model can overcome both limitations, while still comparing favorably in terms of ef-\n\ufb01ciency (measured in wall-clock time) to related methods. Propagation of feature information from\nneighboring nodes in every layer improves classi\ufb01cation performance in comparison to methods like\nICA (Lu & Getoor, 2003), where only label information is aggregated.\nWe have further demonstrated that the proposed renormalized propagation model (Eq. 8) offers both\nimproved ef\ufb01ciency (fewer parameters and operations, such as multiplication or addition) and better\npredictive performance on a number of datasets compared to a na\u00a8\u0131ve 1st-order model (Eq. 6) or\nhigher-order graph convolutional models using Chebyshev polynomials (Eq. 5).\n7.2\nLIMITATIONS AND FUTURE WORK\nHere, we describe several limitations of our current model and outline how these might be overcome\nin future work.\nMemory requirement\nIn the current setup with full-batch gradient descent, memory requirement\ngrows linearly in the size of the dataset. We have shown that for large graphs that do not \ufb01t in GPU\nmemory, training on CPU can still be a viable option. Mini-batch stochastic gradient descent can\nalleviate this issue. The procedure of generating mini-batches, however, should take into account the\nnumber of layers in the GCN model, as the Kth-order neighborhood for a GCN with K layers has to\nbe stored in memory for an exact procedure. For very large and densely connected graph datasets,\nfurther approximations might be necessary.\nDirected edges and edge features\nOur framework currently does not naturally support edge fea-\ntures and is limited to undirected graphs (weighted or unweighted). Results on NELL however\nshow that it is possible to handle both directed edges and edge features by representing the original\ndirected graph as an undirected bipartite graph with additional nodes that represent edges in the\noriginal graph (see Section 5.1 for details).\nLimiting assumptions\nThrough the approximations introduced in Section 2, we implicitly assume\nlocality (dependence on the Kth-order neighborhood for a GCN with K layers) and equal impor-\ntance of self-connections vs. edges to neighboring nodes. For some datasets, however, it might be\nbene\ufb01cial to introduce a trade-off parameter \u03bb in the de\ufb01nition of \u02dcA:\n\u02dcA = A + \u03bbIN .\n(11)\n4Hardware used: 16-core Intel R\u20ddXeon R\u20ddCPU E5-2640 v3 @ 2.60GHz, GeForce R\u20ddGTX TITAN X\n8\nPublished as a conference paper at ICLR 2017\nThis parameter now plays a similar role as the trade-off parameter between supervised and unsuper-\nvised loss in the typical semi-supervised setting (see Eq. 1). Here, however, it can be learned via\ngradient descent.\n8\nCONCLUSION\nWe have introduced a novel approach for semi-supervised classi\ufb01cation on graph-structured data.\nOur GCN model uses an ef\ufb01cient layer-wise propagation rule that is based on a \ufb01rst-order approx-\nimation of spectral convolutions on graphs. Experiments on a number of network datasets suggest\nthat the proposed GCN model is capable of encoding both graph structure and node features in a\nway useful for semi-supervised classi\ufb01cation. In this setting, our model outperforms several recently\nproposed methods by a signi\ufb01cant margin, while being computationally ef\ufb01cient.\nACKNOWLEDGMENTS\nWe would like to thank Christos Louizos, Taco Cohen, Joan Bruna, Zhilin Yang, Dave Herman,\nPramod Sinha and Abdul-Saboor Sheikh for helpful discussions. This research was funded by SAP.\nREFERENCES\nMart\u00b4\u0131n Abadi et al. TensorFlow: Large-scale machine learning on heterogeneous systems, 2015.\nJames Atwood and Don Towsley. Diffusion-convolutional neural networks. In Advances in neural\ninformation processing systems (NIPS), 2016.\nMikhail Belkin, Partha Niyogi, and Vikas Sindhwani. Manifold regularization: A geometric frame-\nwork for learning from labeled and unlabeled examples. Journal of machine learning research\n(JMLR), 7(Nov):2399\u20132434, 2006.\nUlrik Brandes, Daniel Delling, Marco Gaertler, Robert Gorke, Martin Hoefer, Zoran Nikoloski,\nand Dorothea Wagner. On modularity clustering. IEEE Transactions on Knowledge and Data\nEngineering, 20(2):172\u2013188, 2008.\nJoan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally\nconnected networks on graphs. In International Conference on Learning Representations (ICLR),\n2014.\nAndrew Carlson, Justin Betteridge, Bryan Kisiel, Burr Settles, Estevam R. Hruschka Jr, and Tom M.\nMitchell. Toward an architecture for never-ending language learning. In AAAI, volume 5, pp. 3,\n2010.\nMicha\u00a8el Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on\ngraphs with fast localized spectral \ufb01ltering. In Advances in neural information processing systems\n(NIPS), 2016.\nBrendan L. Douglas. The Weisfeiler-Lehman method and graph isomorphism testing. arXiv preprint\narXiv:1101.5211, 2011.\nDavid K. Duvenaud, Dougal Maclaurin, Jorge Iparraguirre, Rafael Bombarell, Timothy Hirzel, Al\u00b4an\nAspuru-Guzik, and Ryan P. Adams. Convolutional networks on graphs for learning molecular\n\ufb01ngerprints. In Advances in neural information processing systems (NIPS), pp. 2224\u20132232, 2015.\nXavier Glorot and Yoshua Bengio. Understanding the dif\ufb01culty of training deep feedforward neural\nnetworks. In AISTATS, volume 9, pp. 249\u2013256, 2010.\nMarco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains.\nIn Proceedings. 2005 IEEE International Joint Conference on Neural Networks., volume 2, pp.\n729\u2013734. IEEE, 2005.\nAditya Grover and Jure Leskovec. node2vec: Scalable feature learning for networks. In Proceedings\nof the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining.\nACM, 2016.\n9\nPublished as a conference paper at ICLR 2017\nDavid K. Hammond, Pierre Vandergheynst, and R\u00b4emi Gribonval. Wavelets on graphs via spectral\ngraph theory. Applied and Computational Harmonic Analysis, 30(2):129\u2013150, 2011.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recog-\nnition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\nThorsten Joachims. Transductive inference for text classi\ufb01cation using support vector machines. In\nInternational Conference on Machine Learning (ICML), volume 99, pp. 200\u2013209, 1999.\nDiederik P. Kingma and Jimmy Lei Ba. Adam: A method for stochastic optimization. In Interna-\ntional Conference on Learning Representations (ICLR), 2015.\nYujia Li, Daniel Tarlow, Marc Brockschmidt, and Richard Zemel. Gated graph sequence neural\nnetworks. In International Conference on Learning Representations (ICLR), 2016.\nQing Lu and Lise Getoor. Link-based classi\ufb01cation. In International Conference on Machine Learn-\ning (ICML), volume 3, pp. 496\u2013503, 2003.\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine\nLearning Research (JMLR), 9(Nov):2579\u20132605, 2008.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S. Corrado, and Jeff Dean. Distributed repre-\nsentations of words and phrases and their compositionality. In Advances in neural information\nprocessing systems (NIPS), pp. 3111\u20133119, 2013.\nMathias Niepert, Mohamed Ahmed, and Konstantin Kutzkov. Learning convolutional neural net-\nworks for graphs. In International Conference on Machine Learning (ICML), 2016.\nBryan Perozzi, Rami Al-Rfou, and Steven Skiena.\nDeepwalk: Online learning of social repre-\nsentations. In Proceedings of the 20th ACM SIGKDD international conference on Knowledge\ndiscovery and data mining, pp. 701\u2013710. ACM, 2014.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.\nThe graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2009.\nPrithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad.\nCollective classi\ufb01cation in network data. AI magazine, 29(3):93, 2008.\nNitish Srivastava, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov.\nDropout: a simple way to prevent neural networks from over\ufb01tting. Journal of Machine Learning\nResearch (JMLR), 15(1):1929\u20131958, 2014.\nJian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. Line: Large-scale\ninformation network embedding. In Proceedings of the 24th International Conference on World\nWide Web, pp. 1067\u20131077. ACM, 2015.\nBoris Weisfeiler and A. A. Lehmann. A reduction of a graph to a canonical form and an algebra\narising during this reduction. Nauchno-Technicheskaya Informatsia, 2(9):12\u201316, 1968.\nJason Weston, Fr\u00b4ed\u00b4eric Ratle, Hossein Mobahi, and Ronan Collobert. Deep learning via semi-\nsupervised embedding. In Neural Networks: Tricks of the Trade, pp. 639\u2013655. Springer, 2012.\nZhilin Yang, William Cohen, and Ruslan Salakhutdinov. Revisiting semi-supervised learning with\ngraph embeddings. In International Conference on Machine Learning (ICML), 2016.\nWayne W. Zachary. An information \ufb02ow model for con\ufb02ict and \ufb01ssion in small groups. Journal of\nanthropological research, pp. 452\u2013473, 1977.\nDengyong Zhou, Olivier Bousquet, Thomas Navin Lal, Jason Weston, and Bernhard Sch\u00a8olkopf.\nLearning with local and global consistency. In Advances in neural information processing systems\n(NIPS), volume 16, pp. 321\u2013328, 2004.\nXiaojin Zhu, Zoubin Ghahramani, and John Lafferty. Semi-supervised learning using gaussian \ufb01elds\nand harmonic functions. In International Conference on Machine Learning (ICML), volume 3,\npp. 912\u2013919, 2003.\n10\nPublished as a conference paper at ICLR 2017\nA\nRELATION TO WEISFEILER-LEHMAN ALGORITHM\nA neural network model for graph-structured data should ideally be able to learn representations of\nnodes in a graph, taking both the graph structure and feature description of nodes into account. A\nwell-studied framework for the unique assignment of node labels given a graph and (optionally) dis-\ncrete initial node labels is provided by the 1-dim Weisfeiler-Lehman (WL-1) algorithm (Weisfeiler\n& Lehmann, 1968):\nAlgorithm 1: WL-1 algorithm (Weisfeiler & Lehmann, 1968)\nInput: Initial node coloring (h(0)\n1 , h(0)\n2 , ..., h(0)\nN )\nOutput: Final node coloring (h(T )\n1\n, h(T )\n2\n, ..., h(T )\nN )\nt \u21900;\nrepeat\nfor vi \u2208V do\nh(t+1)\ni\n\u2190hash\n\u0010P\nj\u2208Ni h(t)\nj\n\u0011\n;\nt \u2190t + 1;\nuntil stable node coloring is reached;\nHere, h(t)\ni\ndenotes the coloring (label assignment) of node vi (at iteration t) and Ni is its set of\nneighboring node indices (irrespective of whether the graph includes self-connections for every node\nor not). hash(\u00b7) is a hash function. For an in-depth mathematical discussion of the WL-1 algorithm\nsee, e.g., Douglas (2011).\nWe can replace the hash function in Algorithm 1 with a neural network layer-like differentiable\nfunction with trainable parameters as follows:\nh(l+1)\ni\n= \u03c3\n\uf8eb\n\uf8edX\nj\u2208Ni\n1\ncij\nh(l)\nj W (l)\n\uf8f6\n\uf8f8,\n(12)\nwhere cij is an appropriately chosen normalization constant for the edge (vi, vj). Further, we can\ntake h(l)\ni\nnow to be a vector of activations of node i in the lth neural network layer. W (l) is a\nlayer-speci\ufb01c weight matrix and \u03c3(\u00b7) denotes a differentiable, non-linear activation function.\nBy choosing cij =\np\ndidj, where di = |Ni| denotes the degree of node vi, we recover the propaga-\ntion rule of our Graph Convolutional Network (GCN) model in vector form (see Eq. 2)5.\nThis\u2014loosely speaking\u2014allows us to interpret our GCN model as a differentiable and parameter-\nized generalization of the 1-dim Weisfeiler-Lehman algorithm on graphs.\nA.1\nNODE EMBEDDINGS WITH RANDOM WEIGHTS\nFrom the analogy with the Weisfeiler-Lehman algorithm, we can understand that even an untrained\nGCN model with random weights can serve as a powerful feature extractor for nodes in a graph. As\nan example, consider the following 3-layer GCN model:\nZ = tanh\n\u0010\n\u02c6A tanh\n\u0010\n\u02c6A tanh\n\u0010\n\u02c6AXW (0)\u0011\nW (1)\u0011\nW (2)\u0011\n,\n(13)\nwith weight matrices W (l) initialized at random using the initialization described in Glorot & Bengio\n(2010). \u02c6A, X and Z are de\ufb01ned as in Section 3.1.\nWe apply this model on Zachary\u2019s karate club network (Zachary, 1977). This graph contains 34\nnodes, connected by 154 (undirected and unweighted) edges. Every node is labeled by one of\nfour classes, obtained via modularity-based clustering (Brandes et al., 2008). See Figure 3a for an\nillustration.\n5Note that we here implicitly assume that self-connections have already been added to every node in the\ngraph (for a clutter-free notation).\n11\nPublished as a conference paper at ICLR 2017\n(a) Karate club network\n(b) Random weight embedding\nFigure 3: Left: Zachary\u2019s karate club network (Zachary, 1977), colors denote communities obtained\nvia modularity-based clustering (Brandes et al., 2008). Right: Embeddings obtained from an un-\ntrained 3-layer GCN model (Eq. 13) with random weights applied to the karate club network. Best\nviewed on a computer screen.\nWe take a featureless approach by setting X = IN, where IN is the N by N identity matrix. N is\nthe number of nodes in the graph. Note that nodes are randomly ordered (i.e. ordering contains no\ninformation). Furthermore, we choose a hidden layer dimensionality6 of 4 and a two-dimensional\noutput (so that the output can immediately be visualized in a 2-dim plot).\nFigure 3b shows a representative example of node embeddings (outputs Z) obtained from an un-\ntrained GCN model applied to the karate club network. These results are comparable to embeddings\nobtained from DeepWalk (Perozzi et al., 2014), which uses a more expensive unsupervised training\nprocedure.\nA.2\nSEMI-SUPERVISED NODE EMBEDDINGS\nOn this simple example of a GCN applied to the karate club network it is interesting to observe how\nembeddings react during training on a semi-supervised classi\ufb01cation task. Such a visualization (see\nFigure 4) provides insights into how the GCN model can make use of the graph structure (and of\nfeatures extracted from the graph structure at later layers) to learn embeddings that are useful for a\nclassi\ufb01cation task.\nWe consider the following semi-supervised learning setup: we add a softmax layer on top of our\nmodel (Eq. 13) and train using only a single labeled example per class (i.e. a total number of 4 labeled\nnodes). We train for 300 training iterations using Adam (Kingma & Ba, 2015) with a learning rate\nof 0.01 on a cross-entropy loss.\nFigure 4 shows the evolution of node embeddings over a number of training iterations. The model\nsucceeds in linearly separating the communities based on minimal supervision and the graph struc-\nture alone. A video of the full training process can be found on our website7.\n6We originally experimented with a hidden layer dimensionality of 2 (i.e. same as output layer), but observed\nthat a dimensionality of 4 resulted in less frequent saturation of tanh(\u00b7) units and therefore visually more\npleasing results.\n7http://tkipf.github.io/graph-convolutional-networks/\n12\nPublished as a conference paper at ICLR 2017\n(a) Iteration 25\n(b) Iteration 50\n(c) Iteration 75\n(d) Iteration 100\n(e) Iteration 200\n(f) Iteration 300\nFigure 4: Evolution of karate club network node embeddings obtained from a GCN model after a\nnumber of semi-supervised training iterations. Colors denote class. Nodes of which labels were\nprovided during training (one per class) are highlighted (grey outline). Grey links between nodes\ndenote graph edges. Best viewed on a computer screen.\n13\nPublished as a conference paper at ICLR 2017\nB\nEXPERIMENTS ON MODEL DEPTH\nIn these experiments, we investigate the in\ufb02uence of model depth (number of layers) on classi\ufb01cation\nperformance. We report results on a 5-fold cross-validation experiment on the Cora, Citeseer and\nPubmed datasets (Sen et al., 2008) using all labels. In addition to the standard GCN model (Eq. 2),\nwe report results on a model variant where we use residual connections (He et al., 2016) between\nhidden layers to facilitate training of deeper models by enabling the model to carry over information\nfrom the previous layer\u2019s input:\nH(l+1) = \u03c3\n\u0010\n\u02dcD\u22121\n2 \u02dcA \u02dcD\u22121\n2 H(l)W (l)\u0011\n+ H(l) .\n(14)\nOn each cross-validation split, we train for 400 epochs (without early stopping) using the Adam\noptimizer (Kingma & Ba, 2015) with a learning rate of 0.01. Other hyperparameters are chosen as\nfollows: 0.5 (dropout rate, \ufb01rst and last layer), 5 \u00b7 10\u22124 (L2 regularization, \ufb01rst layer), 16 (number\nof units for each hidden layer) and 0.01 (learning rate). Results are summarized in Figure 5.\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\nAccuracy\nCiteseer\nTrain\nTrain (Residual)\nTest\nTest (Residual)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\n0.85\n0.90\n0.95\nAccuracy\nCora\nTrain\nTrain (Residual)\nTest\nTest (Residual)\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nNumber of layers\n0.76\n0.78\n0.80\n0.82\n0.84\n0.86\n0.88\nAccuracy\nPubmed\nTrain\nTrain (Residual)\nTest\nTest (Residual)\nFigure 5: In\ufb02uence of model depth (number of layers) on classi\ufb01cation performance. Markers\ndenote mean classi\ufb01cation accuracy (training vs. testing) for 5-fold cross-validation. Shaded areas\ndenote standard error. We show results both for a standard GCN model (dashed lines) and a model\nwith added residual connections (He et al., 2016) between hidden layers (solid lines).\nFor the datasets considered here, best results are obtained with a 2- or 3-layer model. We observe\nthat for models deeper than 7 layers, training without the use of residual connections can become\ndif\ufb01cult, as the effective context size for each node increases by the size of its Kth-order neighbor-\nhood (for a model with K layers) with each additional layer. Furthermore, over\ufb01tting can become\nan issue as the number of parameters increases with model depth.\n14\n",
    "2010.02502": "Published as a conference paper at ICLR 2021\nDENOISING DIFFUSION IMPLICIT MODELS\nJiaming Song, Chenlin Meng & Stefano Ermon\nStanford University\n{tsong,chenlin,ermon}@cs.stanford.edu\nABSTRACT\nDenoising diffusion probabilistic models (DDPMs) have achieved high qual-\nity image generation without adversarial training, yet they require simulating a\nMarkov chain for many steps in order to produce a sample. To accelerate sam-\npling, we present denoising diffusion implicit models (DDIMs), a more ef\ufb01cient\nclass of iterative implicit probabilistic models with the same training procedure as\nDDPMs. In DDPMs, the generative process is de\ufb01ned as the reverse of a particular\nMarkovian diffusion process. We generalize DDPMs via a class of non-Markovian\ndiffusion processes that lead to the same training objective. These non-Markovian\nprocesses can correspond to generative processes that are deterministic, giving rise\nto implicit models that produce high quality samples much faster. We empirically\ndemonstrate that DDIMs can produce high quality samples 10\u00d7 to 50\u00d7 faster in\nterms of wall-clock time compared to DDPMs, allow us to trade off computation\nfor sample quality, perform semantically meaningful image interpolation directly\nin the latent space, and reconstruct observations with very low error.\n1\nINTRODUCTION\nDeep generative models have demonstrated the ability to produce high quality samples in many\ndomains (Karras et al., 2020; van den Oord et al., 2016a). In terms of image generation, genera-\ntive adversarial networks (GANs, Goodfellow et al. (2014)) currently exhibits higher sample quality\nthan likelihood-based methods such as variational autoencoders (Kingma & Welling, 2013), autore-\ngressive models (van den Oord et al., 2016b) and normalizing \ufb02ows (Rezende & Mohamed, 2015;\nDinh et al., 2016). However, GANs require very speci\ufb01c choices in optimization and architectures\nin order to stabilize training (Arjovsky et al., 2017; Gulrajani et al., 2017; Karras et al., 2018; Brock\net al., 2018), and could fail to cover modes of the data distribution (Zhao et al., 2018).\nRecent works on iterative generative models (Bengio et al., 2014), such as denoising diffusion prob-\nabilistic models (DDPM, Ho et al. (2020)) and noise conditional score networks (NCSN, Song &\nErmon (2019)) have demonstrated the ability to produce samples comparable to that of GANs, with-\nout having to perform adversarial training. To achieve this, many denoising autoencoding models\nare trained to denoise samples corrupted by various levels of Gaussian noise. Samples are then\nproduced by a Markov chain which, starting from white noise, progressively denoises it into an im-\nage. This generative Markov Chain process is either based on Langevin dynamics (Song & Ermon,\n2019) or obtained by reversing a forward diffusion process that progressively turns an image into\nnoise (Sohl-Dickstein et al., 2015).\nA critical drawback of these models is that they require many iterations to produce a high quality\nsample. For DDPMs, this is because that the generative process (from noise to data) approximates\nthe reverse of the forward diffusion process (from data to noise), which could have thousands of\nsteps; iterating over all the steps is required to produce a single sample, which is much slower\ncompared to GANs, which only needs one pass through a network. For example, it takes around 20\nhours to sample 50k images of size 32 \u00d7 32 from a DDPM, but less than a minute to do so from\na GAN on a Nvidia 2080 Ti GPU. This becomes more problematic for larger images as sampling\n50k images of size 256 \u00d7 256 could take nearly 1000 hours on the same GPU.\nTo close this ef\ufb01ciency gap between DDPMs and GANs, we present denoising diffusion implicit\nmodels (DDIMs). DDIMs are implicit probabilistic models (Mohamed & Lakshminarayanan, 2016)\nand are closely related to DDPMs, in the sense that they are trained with the same objective function.\n1\narXiv:2010.02502v4  [cs.LG]  5 Oct 2022\nPublished as a conference paper at ICLR 2021\nFigure 1: Graphical models for diffusion (left) and non-Markovian (right) inference models.\nIn Section 3, we generalize the forward diffusion process used by DDPMs, which is Markovian,\nto non-Markovian ones, for which we are still able to design suitable reverse generative Markov\nchains. We show that the resulting variational training objectives have a shared surrogate objective,\nwhich is exactly the objective used to train DDPM. Therefore, we can freely choose from a large\nfamily of generative models using the same neural network simply by choosing a different, non-\nMarkovian diffusion process (Section 4.1) and the corresponding reverse generative Markov Chain.\nIn particular, we are able to use non-Markovian diffusion processes which lead to \u201dshort\u201d generative\nMarkov chains (Section 4.2) that can be simulated in a small number of steps. This can massively\nincrease sample ef\ufb01ciency only at a minor cost in sample quality.\nIn Section 5, we demonstrate several empirical bene\ufb01ts of DDIMs over DDPMs. First, DDIMs have\nsuperior sample generation quality compared to DDPMs, when we accelerate sampling by 10\u00d7 to\n100\u00d7 using our proposed method. Second, DDIM samples have the following \u201cconsistency\u201d prop-\nerty, which does not hold for DDPMs: if we start with the same initial latent variable and generate\nseveral samples with Markov chains of various lengths, these samples would have similar high-level\nfeatures. Third, because of \u201cconsistency\u201d in DDIMs, we can perform semantically meaningful image\ninterpolation by manipulating the initial latent variable in DDIMs, unlike DDPMs which interpolates\nnear the image space due to the stochastic generative process.\n2\nBACKGROUND\nGiven samples from a data distribution q(x0), we are interested in learning a model distribution\np\u03b8(x0) that approximates q(x0) and is easy to sample from. Denoising diffusion probabilistic mod-\nels (DDPMs, Sohl-Dickstein et al. (2015); Ho et al. (2020)) are latent variable models of the form\np\u03b8(x0) =\nZ\np\u03b8(x0:T )dx1:T ,\nwhere\np\u03b8(x0:T ) := p\u03b8(xT )\nT\nY\nt=1\np(t)\n\u03b8 (xt\u22121|xt)\n(1)\nwhere x1, . . . , xT are latent variables in the same sample space as x0 (denoted as X). The parame-\nters \u03b8 are learned to \ufb01t the data distribution q(x0) by maximizing a variational lower bound:\nmax\n\u03b8\nEq(x0)[log p\u03b8(x0)] \u2264max\n\u03b8\nEq(x0,x1,...,xT ) [log p\u03b8(x0:T ) \u2212log q(x1:T |x0)]\n(2)\nwhere q(x1:T |x0) is some inference distribution over the latent variables. Unlike typical latent vari-\nable models (such as the variational autoencoder (Rezende et al., 2014)), DDPMs are learned with a\n\ufb01xed (rather than trainable) inference procedure q(x1:T |x0), and latent variables are relatively high\ndimensional. For example, Ho et al. (2020) considered the following Markov chain with Gaussian\ntransitions parameterized by a decreasing sequence \u03b11:T \u2208(0, 1]T :\nq(x1:T |x0) :=\nT\nY\nt=1\nq(xt|xt\u22121), where q(xt|xt\u22121) := N\n\u0012r \u03b1t\n\u03b1t\u22121\nxt\u22121,\n\u0012\n1 \u2212\n\u03b1t\n\u03b1t\u22121\n\u0013\nI\n\u0013\n(3)\nwhere the covariance matrix is ensured to have positive terms on its diagonal. This is called the\nforward process due to the autoregressive nature of the sampling procedure (from x0 to xT ). We\ncall the latent variable model p\u03b8(x0:T ), which is a Markov chain that samples from xT to x0, the\ngenerative process, since it approximates the intractable reverse process q(xt\u22121|xt). Intuitively,\nthe forward process progressively adds noise to the observation x0, whereas the generative process\nprogressively denoises a noisy observation (Figure 1, left).\nA special property of the forward process is that\nq(xt|x0) :=\nZ\nq(x1:t|x0)dx1:(t\u22121) = N(xt; \u221a\u03b1tx0, (1 \u2212\u03b1t)I);\n2\nPublished as a conference paper at ICLR 2021\nso we can express xt as a linear combination of x0 and a noise variable \u03f5:\nxt = \u221a\u03b1tx0 +\n\u221a\n1 \u2212\u03b1t\u03f5,\nwhere\n\u03f5 \u223cN(0, I).\n(4)\nWhen we set \u03b1T suf\ufb01ciently close to 0, q(xT |x0) converges to a standard Gaussian for all x0, so it\nis natural to set p\u03b8(xT ) := N(0, I). If all the conditionals are modeled as Gaussians with trainable\nmean functions and \ufb01xed variances, the objective in Eq. (2) can be simpli\ufb01ed to1:\nL\u03b3(\u03f5\u03b8) :=\nT\nX\nt=1\n\u03b3tEx0\u223cq(x0),\u03f5t\u223cN (0,I)\nh\n\u2225\u03f5(t)\n\u03b8 (\u221a\u03b1tx0 +\n\u221a\n1 \u2212\u03b1t\u03f5t) \u2212\u03f5t\u2225\n2\n2\ni\n(5)\nwhere \u03f5\u03b8 := {\u03f5(t)\n\u03b8 }T\nt=1 is a set of T functions, each \u03f5(t)\n\u03b8\n: X \u2192X (indexed by t) is a function with\ntrainable parameters \u03b8(t), and \u03b3 := [\u03b31, . . . , \u03b3T ] is a vector of positive coef\ufb01cients in the objective\nthat depends on \u03b11:T . In Ho et al. (2020), the objective with \u03b3 = 1 is optimized instead to maximize\ngeneration performance of the trained model; this is also the same objective used in noise conditional\nscore networks (Song & Ermon, 2019) based on score matching (Hyv\u00a8arinen, 2005; Vincent, 2011).\nFrom a trained model, x0 is sampled by \ufb01rst sampling xT from the prior p\u03b8(xT ), and then sampling\nxt\u22121 from the generative processes iteratively.\nThe length T of the forward process is an important hyperparameter in DDPMs. From a variational\nperspective, a large T allows the reverse process to be close to a Gaussian (Sohl-Dickstein et al.,\n2015), so that the generative process modeled with Gaussian conditional distributions becomes a\ngood approximation; this motivates the choice of large T values, such as T = 1000 in Ho et al.\n(2020). However, as all T iterations have to be performed sequentially, instead of in parallel, to ob-\ntain a sample x0, sampling from DDPMs is much slower than sampling from other deep generative\nmodels, which makes them impractical for tasks where compute is limited and latency is critical.\n3\nVARIATIONAL INFERENCE FOR NON-MARKOVIAN FORWARD PROCESSES\nBecause the generative model approximates the reverse of the inference process, we need to rethink\nthe inference process in order to reduce the number of iterations required by the generative model.\nOur key observation is that the DDPM objective in the form of L\u03b3 only depends on the marginals2\nq(xt|x0), but not directly on the joint q(x1:T |x0). Since there are many inference distributions\n(joints) with the same marginals, we explore alternative inference processes that are non-Markovian,\nwhich leads to new generative processes (Figure 1, right). These non-Markovian inference process\nlead to the same surrogate objective function as DDPM, as we will show below. In Appendix A, we\nshow that the non-Markovian perspective also applies beyond the Gaussian case.\n3.1\nNON-MARKOVIAN FORWARD PROCESSES\nLet us consider a family Q of inference distributions, indexed by a real vector \u03c3 \u2208RT\n\u22650:\nq\u03c3(x1:T |x0) := q\u03c3(xT |x0)\nT\nY\nt=2\nq\u03c3(xt\u22121|xt, x0)\n(6)\nwhere q\u03c3(xT |x0) = N(\u221a\u03b1T x0, (1 \u2212\u03b1T )I) and for all t > 1,\nq\u03c3(xt\u22121|xt, x0) = N\n\u0012\u221a\u03b1t\u22121x0 +\nq\n1 \u2212\u03b1t\u22121 \u2212\u03c32\nt \u00b7 xt \u2212\u221a\u03b1tx0\n\u221a1 \u2212\u03b1t\n, \u03c32\nt I\n\u0013\n.\n(7)\nThe mean function is chosen to order to ensure that q\u03c3(xt|x0) = N(\u221a\u03b1tx0, (1 \u2212\u03b1t)I) for all\nt (see Lemma 1 of Appendix B), so that it de\ufb01nes a joint inference distribution that matches the\n\u201cmarginals\u201d as desired. The forward process3 can be derived from Bayes\u2019 rule:\nq\u03c3(xt|xt\u22121, x0) = q\u03c3(xt\u22121|xt, x0)q\u03c3(xt|x0)\nq\u03c3(xt\u22121|x0)\n,\n(8)\n1Please refer to Appendix C.2 for details.\n2We slightly abuse this term (as well as joints) when only conditioned on x0.\n3We overload the term \u201cforward process\u201d for cases where the inference model is not a diffusion.\n3\nPublished as a conference paper at ICLR 2021\nwhich is also Gaussian (although we do not use this fact for the remainder of this paper). Unlike the\ndiffusion process in Eq. (3), the forward process here is no longer Markovian, since each xt could\ndepend on both xt\u22121 and x0. The magnitude of \u03c3 controls the how stochastic the forward process\nis; when \u03c3 \u21920, we reach an extreme case where as long as we observe x0 and xt for some t, then\nxt\u22121 become known and \ufb01xed.\n3.2\nGENERATIVE PROCESS AND UNIFIED VARIATIONAL INFERENCE OBJECTIVE\nNext, we de\ufb01ne a trainable generative process p\u03b8(x0:T ) where each p(t)\n\u03b8 (xt\u22121|xt) leverages knowl-\nedge of q\u03c3(xt\u22121|xt, x0). Intuitively, given a noisy observation xt, we \ufb01rst make a prediction4\nof the corresponding x0, and then use it to obtain a sample xt\u22121 through the reverse conditional\ndistribution q\u03c3(xt\u22121|xt, x0), which we have de\ufb01ned.\nFor some x0 \u223cq(x0) and \u03f5t \u223cN(0, I), xt can be obtained using Eq. (4). The model \u03f5(t)\n\u03b8 (xt) then\nattempts to predict \u03f5t from xt, without knowledge of x0. By rewriting Eq. (4), one can then predict\nthe denoised observation, which is a prediction of x0 given xt:\nf (t)\n\u03b8 (xt) := (xt \u2212\n\u221a\n1 \u2212\u03b1t \u00b7 \u03f5(t)\n\u03b8 (xt))/\u221a\u03b1t.\n(9)\nWe can then de\ufb01ne the generative process with a \ufb01xed prior p\u03b8(xT ) = N(0, I) and\np(t)\n\u03b8 (xt\u22121|xt) =\n(\nN(f (1)\n\u03b8\n(x1), \u03c32\n1I)\nif t = 1\nq\u03c3(xt\u22121|xt, f (t)\n\u03b8 (xt))\notherwise,\n(10)\nwhere q\u03c3(xt\u22121|xt, f (t)\n\u03b8 (xt)) is de\ufb01ned as in Eq. (7) with x0 replaced by f (t)\n\u03b8 (xt). We add some\nGaussian noise (with covariance \u03c32\n1I) for the case of t = 1 to ensure that the generative process is\nsupported everywhere.\nWe optimize \u03b8 via the following variational inference objective (which is a functional over \u03f5\u03b8):\nJ\u03c3(\u03f5\u03b8) := Ex0:T \u223cq\u03c3(x0:T )[log q\u03c3(x1:T |x0) \u2212log p\u03b8(x0:T )]\n(11)\n= Ex0:T \u223cq\u03c3(x0:T )\n\"\nlog q\u03c3(xT |x0) +\nT\nX\nt=2\nlog q\u03c3(xt\u22121|xt, x0) \u2212\nT\nX\nt=1\nlog p(t)\n\u03b8 (xt\u22121|xt) \u2212log p\u03b8(xT )\n#\nwhere we factorize q\u03c3(x1:T |x0) according to Eq. (6) and p\u03b8(x0:T ) according to Eq. (1).\nFrom the de\ufb01nition of J\u03c3, it would appear that a different model has to be trained for every choice\nof \u03c3, since it corresponds to a different variational objective (and a different generative process).\nHowever, J\u03c3 is equivalent to L\u03b3 for certain weights \u03b3, as we show below.\nTheorem 1. For all \u03c3 > 0, there exists \u03b3 \u2208RT\n>0 and C \u2208R, such that J\u03c3 = L\u03b3 + C.\nThe variational objective L\u03b3 is special in the sense that if parameters \u03b8 of the models \u03f5(t)\n\u03b8\nare not\nshared across different t, then the optimal solution for \u03f5\u03b8 will not depend on the weights \u03b3 (as global\noptimum is achieved by separately maximizing each term in the sum). This property of L\u03b3 has\ntwo implications. On the one hand, this justi\ufb01ed the use of L1 as a surrogate objective function for\nthe variational lower bound in DDPMs; on the other hand, since J\u03c3 is equivalent to some L\u03b3 from\nTheorem 1, the optimal solution of J\u03c3 is also the same as that of L1. Therefore, if parameters are\nnot shared across t in the model \u03f5\u03b8, then the L1 objective used by Ho et al. (2020) can be used as a\nsurrogate objective for the variational objective J\u03c3 as well.\n4\nSAMPLING FROM GENERALIZED GENERATIVE PROCESSES\nWith L1 as the objective, we are not only learning a generative process for the Markovian inference\nprocess considered in Sohl-Dickstein et al. (2015) and Ho et al. (2020), but also generative processes\nfor many non-Markovian forward processes parametrized by \u03c3 that we have described. Therefore,\nwe can essentially use pretrained DDPM models as the solutions to the new objectives, and focus on\n\ufb01nding a generative process that is better at producing samples subject to our needs by changing \u03c3.\n4Learning a distribution over the predictions is also possible, but empirically we found little bene\ufb01ts of it.\n4\nPublished as a conference paper at ICLR 2021\nFigure 2: Graphical model for accelerated generation, where \u03c4 = [1, 3].\n4.1\nDENOISING DIFFUSION IMPLICIT MODELS\nFrom p\u03b8(x1:T ) in Eq. (10), one can generate a sample xt\u22121 from a sample xt via:\nxt\u22121 = \u221a\u03b1t\u22121\n \nxt \u2212\u221a1 \u2212\u03b1t\u03f5(t)\n\u03b8 (xt)\n\u221a\u03b1t\n!\n|\n{z\n}\n\u201c predicted x0\u201d\n+\nq\n1 \u2212\u03b1t\u22121 \u2212\u03c32\nt \u00b7 \u03f5(t)\n\u03b8 (xt)\n|\n{z\n}\n\u201cdirection pointing to xt\u201d\n+\n\u03c3t\u03f5t\n|{z}\nrandom noise\n(12)\nwhere \u03f5t \u223cN(0, I) is standard Gaussian noise independent of xt, and we de\ufb01ne \u03b10 := 1. Different\nchoices of \u03c3 values results in different generative processes, all while using the same model \u03f5\u03b8, so\nre-training the model is unnecessary. When \u03c3t =\np\n(1 \u2212\u03b1t\u22121)/(1 \u2212\u03b1t)\np\n1 \u2212\u03b1t/\u03b1t\u22121 for all t,\nthe forward process becomes Markovian, and the generative process becomes a DDPM.\nWe note another special case when \u03c3t = 0 for all t5; the forward process becomes deterministic\ngiven xt\u22121 and x0, except for t = 1; in the generative process, the coef\ufb01cient before the random\nnoise \u03f5t becomes zero. The resulting model becomes an implicit probabilistic model (Mohamed &\nLakshminarayanan, 2016), where samples are generated from latent variables with a \ufb01xed procedure\n(from xT to x0). We name this the denoising diffusion implicit model (DDIM, pronounced /d:Im/),\nbecause it is an implicit probabilistic model trained with the DDPM objective (despite the forward\nprocess no longer being a diffusion).\n4.2\nACCELERATED GENERATION PROCESSES\nIn the previous sections, the generative process is considered as the approximation to the reverse\nprocess; since of the forward process has T steps, the generative process is also forced to sample T\nsteps. However, as the denoising objective L1 does not depend on the speci\ufb01c forward procedure\nas long as q\u03c3(xt|x0) is \ufb01xed, we may also consider forward processes with lengths smaller than T,\nwhich accelerates the corresponding generative processes without having to train a different model.\nLet us consider the forward process as de\ufb01ned not on all the latent variables x1:T , but on a\nsubset {x\u03c41, . . . , x\u03c4S}, where \u03c4 is an increasing sub-sequence of [1, . . . , T] of length S.\nIn\nparticular, we de\ufb01ne the sequential forward process over x\u03c41, . . . , x\u03c4S such that q(x\u03c4i|x0) =\nN(\u221a\u03b1\u03c4ix0, (1 \u2212\u03b1\u03c4i)I) matches the \u201cmarginals\u201d (see Figure 2 for an illustration). The generative\nprocess now samples latent variables according to reversed(\u03c4), which we term (sampling) trajec-\ntory. When the length of the sampling trajectory is much smaller than T, we may achieve signi\ufb01cant\nincreases in computational ef\ufb01ciency due to the iterative nature of the sampling process.\nUsing a similar argument as in Section 3, we can justify using the model trained with the L1 ob-\njective, so no changes are needed in training. We show that only slight changes to the updates in\nEq. (12) are needed to obtain the new, faster generative processes, which applies to DDPM, DDIM,\nas well as all generative processes considered in Eq. (10). We include these details in Appendix C.1.\nIn principle, this means that we can train a model with an arbitrary number of forward steps but only\nsample from some of them in the generative process. Therefore, the trained model could consider\nmany more steps than what is considered in (Ho et al., 2020) or even a continuous time variable t\n(Chen et al., 2020). We leave empirical investigations of this aspect as future work.\n5Although this case is not covered in Theorem 1, we can always approximate it by making \u03c3t very small.\n5\nPublished as a conference paper at ICLR 2021\n4.3\nRELEVANCE TO NEURAL ODES\nMoreover, we can rewrite the DDIM iterate according to Eq. (12), and its similarity to Euler inte-\ngration for solving ordinary differential equations (ODEs) becomes more apparent:\nxt\u2212\u2206t\n\u221a\u03b1t\u2212\u2206t\n=\nxt\n\u221a\u03b1t\n+\n s\n1 \u2212\u03b1t\u2212\u2206t\n\u03b1t\u2212\u2206t\n\u2212\nr\n1 \u2212\u03b1t\n\u03b1t\n!\n\u03f5(t)\n\u03b8 (xt)\n(13)\nTo derive the corresponding ODE, we can reparameterize (\u221a1 \u2212\u03b1/\u221a\u03b1) with \u03c3 and (x/\u221a\u03b1) with\n\u00afx. In the continuous case, \u03c3 and x are functions of t, where \u03c3 : R\u22650 \u2192R\u22650 is continous, increasing\nwith \u03c3(0) = 0. Equation (13) with can be treated as a Euler method over the following ODE:\nd\u00afx(t) = \u03f5(t)\n\u03b8\n\u0012\n\u00afx(t)\n\u221a\n\u03c32 + 1\n\u0013\nd\u03c3(t),\n(14)\nwhere the initial conditions is x(T) \u223cN(0, \u03c3(T)) for a very large \u03c3(T) (which corresponds to the\ncase of \u03b1 \u22480). This suggests that with enough discretization steps, the we can also reverse the\ngeneration process (going from t = 0 to T), which encodes x0 to xT and simulates the reverse of\nthe ODE in Eq. (14). This suggests that unlike DDPM, we can use DDIM to obtain encodings of\nthe observations (as the form of xT ), which might be useful for other downstream applications that\nrequires latent representations of a model.\nIn a concurrent work, (Song et al., 2020) proposed a \u201cprobability \ufb02ow ODE\u201d that aims to recover the\nmarginal densities of a stochastic differential equation (SDE) based on scores, from which a similar\nsampling schedule can be obtained. Here, we state that the our ODE is equivalent to a special case\nof theirs (which corresponds to a continuous-time analog of DDPM).\nProposition 1. The ODE in Eq. (14) with the optimal model \u03f5(t)\n\u03b8\nhas an equivalent probability \ufb02ow\nODE corresponding to the \u201cVariance-Exploding\u201d SDE in Song et al. (2020).\nWe include the proof in Appendix B. While the ODEs are equivalent, the sampling procedures are\nnot, since the Euler method for the probability \ufb02ow ODE will make the following update:\nxt\u2212\u2206t\n\u221a\u03b1t\u2212\u2206t\n=\nxt\n\u221a\u03b1t\n+ 1\n2\n\u00121 \u2212\u03b1t\u2212\u2206t\n\u03b1t\u2212\u2206t\n\u22121 \u2212\u03b1t\n\u03b1t\n\u0013\n\u00b7\nr\n\u03b1t\n1 \u2212\u03b1t\n\u00b7 \u03f5(t)\n\u03b8 (xt)\n(15)\nwhich is equivalent to ours if \u03b1t and \u03b1t\u2212\u2206t are close enough. In fewer sampling steps, however,\nthese choices will make a difference; we take Euler steps with respect to d\u03c3(t) (which depends less\ndirectly on the scaling of \u201ctime\u201d t) whereas Song et al. (2020) take Euler steps with respect to dt.\n5\nEXPERIMENTS\nIn this section, we show that DDIMs outperform DDPMs in terms of image generation when fewer\niterations are considered, giving speed ups of 10\u00d7 to 100\u00d7 over the original DDPM generation\nprocess. Moreover, unlike DDPMs, once the initial latent variables xT are \ufb01xed, DDIMs retain high-\nlevel image features regardless of the generation trajectory, so they are able to perform interpolation\ndirectly from the latent space. DDIMs can also be used to encode samples that reconstruct them\nfrom the latent code, which DDPMs cannot do due to the stochastic sampling process.\nFor each dataset, we use the same trained model with T = 1000 and the objective being L\u03b3\nfrom Eq. (5) with \u03b3 = 1; as we argued in Section 3, no changes are needed with regards to the\ntraining procedure. The only changes that we make is how we produce samples from the model;\nwe achieve this by controlling \u03c4 (which controls how fast the samples are obtained) and \u03c3 (which\ninterpolates between the deterministic DDIM and the stochastic DDPM).\nWe consider different sub-sequences \u03c4 of [1, . . . , T] and different variance hyperparameters \u03c3 in-\ndexed by elements of \u03c4. To simplify comparisons, we consider \u03c3 with the form:\n\u03c3\u03c4i(\u03b7) = \u03b7\nq\n(1 \u2212\u03b1\u03c4i\u22121)/(1 \u2212\u03b1\u03c4i)\nq\n1 \u2212\u03b1\u03c4i/\u03b1\u03c4i\u22121,\n(16)\nwhere \u03b7 \u2208R\u22650 is a hyperparameter that we can directly control. This includes an original DDPM\ngenerative process when \u03b7 = 1 and DDIM when \u03b7 = 0. We also consider DDPM where the random\nnoise has a larger standard deviation than \u03c3(1), which we denote as \u02c6\u03c3: \u02c6\u03c3\u03c4i =\np\n1 \u2212\u03b1\u03c4i/\u03b1\u03c4i\u22121 .\nThis is used by the implementation in Ho et al. (2020) only to obtain the CIFAR10 samples, but\nnot samples of the other datasets. We include more details in Appendix D.\n6\nPublished as a conference paper at ICLR 2021\nTable 1: CIFAR10 and CelebA image generation measured in FID. \u03b7 = 1.0 and \u02c6\u03c3 are cases of\nDDPM (although Ho et al. (2020) only considered T = 1000 steps, and S < T can be seen as\nsimulating DDPMs trained with S steps), and \u03b7 = 0.0 indicates DDIM.\nCIFAR10 (32 \u00d7 32)\nCelebA (64 \u00d7 64)\nS\n10\n20\n50\n100\n1000\n10\n20\n50\n100\n1000\n\u03b7\n0.0\n13.36\n6.84\n4.67\n4.16\n4.04\n17.33\n13.73\n9.17\n6.53\n3.51\n0.2\n14.04\n7.11\n4.77\n4.25\n4.09\n17.66\n14.11\n9.51\n6.79\n3.64\n0.5\n16.66\n8.35\n5.25\n4.46\n4.29\n19.86\n16.06\n11.01\n8.09\n4.28\n1.0\n41.07\n18.36\n8.01\n5.78\n4.73\n33.12\n26.03\n18.48\n13.93\n5.98\n\u02c6\u03c3\n367.43\n133.37\n32.72\n9.99\n3.17\n299.71\n183.83\n71.71\n45.20\n3.26\n0.0\n0.2\n0.5\n1.0\ndim( ) = 10\n0.0\n0.2\n0.5\n1.0\ndim( ) = 100\n0.0\n0.2\n0.5\n1.0\ndim( ) = 10\n0.0\n0.2\n0.5\n1.0\ndim( ) = 100\nFigure 3: CIFAR10 and CelebA samples with dim(\u03c4) = 10 and dim(\u03c4) = 100.\n5.1\nSAMPLE QUALITY AND EFFICIENCY\nIn Table 1, we report the quality of the generated samples with models trained on CIFAR10 and\nCelebA, as measured by Frechet Inception Distance (FID (Heusel et al., 2017)), where we vary\nthe number of timesteps used to generate a sample (dim(\u03c4)) and the stochasticity of the process\n(\u03b7). As expected, the sample quality becomes higher as we increase dim(\u03c4), presenting a trade-\noff between sample quality and computational costs. We observe that DDIM (\u03b7 = 0) achieves the\nbest sample quality when dim(\u03c4) is small, and DDPM (\u03b7 = 1 and \u02c6\u03c3) typically has worse sample\nquality compared to its less stochastic counterparts with the same dim(\u03c4), except for the case for\ndim(\u03c4) = 1000 and \u02c6\u03c3 reported by Ho et al. (2020) where DDIM is marginally worse. However, the\nsample quality of \u02c6\u03c3 becomes much worse for smaller dim(\u03c4), which suggests that it is ill-suited for\nshorter trajectories. DDIM, on the other hand, achieves high sample quality much more consistently.\nIn Figure 3, we show CIFAR10 and CelebA samples with the same number of sampling steps and\nvarying \u03c3. For the DDPM, the sample quality deteriorates rapidly when the sampling trajectory has\n10 steps. For the case of \u02c6\u03c3, the generated images seem to have more noisy perturbations under short\ntrajectories; this explains why the FID scores are much worse than other methods, as FID is very\nsensitive to such perturbations (as discussed in Jolicoeur-Martineau et al. (2020)).\nIn Figure 4, we show that the amount of time needed to produce a sample scales linearly with the\nlength of the sample trajectory. This suggests that DDIM is useful for producing samples more\nef\ufb01ciently, as samples can be generated in much fewer steps. Notably, DDIM is able to produce\nsamples with quality comparable to 1000 step models within 20 to 100 steps, which is a 10\u00d7 to\n50\u00d7 speed up compared to the original DDPM. Even though DDPM could also achieve reasonable\nsample quality with 100\u00d7 steps, DDIM requires much fewer steps to achieve this; on CelebA, the\nFID score of the 100 step DDPM is similar to that of the 20 step DDIM.\n5.2\nSAMPLE CONSISTENCY IN DDIMS\nFor DDIM, the generative process is deterministic, and x0 would depend only on the initial state xT .\nIn Figure 5, we observe the generated images under different generative trajectories (i.e. different \u03c4)\nwhile starting with the same initial xT . Interestingly, for the generated images with the same initial\nxT , most high-level features are similar, regardless of the generative trajectory. In many cases,\nsamples generated with only 20 steps are already very similar to ones generated with 1000 steps in\nterms of high-level features, with only minor differences in details. Therefore, it would appear that\nxT alone would be an informative latent encoding of the image; and minor details that affects sample\n7\nPublished as a conference paper at ICLR 2021\n10\n30\n100\n300\n1000\n# steps\n0.2\n0.5\n2\n5\n20\nHours\nCIFAR10\n10\n30\n100\n300\n1000\n# steps\n10\n30\n100\n300\n1000\nHours\nBedroom\nFigure 4: Hours to sample 50k images with one Nvidia 2080 Ti GPU and samples at different steps.\n10\n20\n50\n100\n1000\nsample timesteps\n10\n100\nsample timesteps\n10\n100\nsample timesteps\nFigure 5: Samples from DDIM with the same random xT and different number of steps.\nquality are encoded in the parameters, as longer sample trajectories gives better quality samples but\ndo not signi\ufb01cantly affect the high-level features. We show more samples in Appendix D.4.\n5.3\nINTERPOLATION IN DETERMINISTIC GENERATIVE PROCESSES\nFigure 6: Interpolation of samples from DDIM with dim(\u03c4) = 50.\nSince the high level features of the DDIM sample is encoded by xT , we are interested to see whether\nit would exhibit the semantic interpolation effect similar to that observed in other implicit proba-\n8\nPublished as a conference paper at ICLR 2021\nTable 2: Reconstruction error with DDIM on CIFAR-10 test set, rounded to 10\u22124.\nS\n10\n20\n50\n100\n200\n500\n1000\nError\n0.014\n0.0065\n0.0023\n0.0009\n0.0004\n0.0001\n0.0001\nbilistic models, such as GANs (Goodfellow et al., 2014). This is different from the interpolation\nprocedure in Ho et al. (2020), since in DDPM the same xT would lead to highly diverse x0 due to\nthe stochastic generative process6. In Figure 6, we show that simple interpolations in xT can lead to\nsemantically meaningful interpolations between two samples. We include more details and samples\nin Appendix D.5. This allows DDIM to control the generated images on a high level directly through\nthe latent variables, which DDPMs cannot.\n5.4\nRECONSTRUCTION FROM LATENT SPACE\nAs DDIM is the Euler integration for a particular ODE, it would be interesting to see whether it\ncan encode from x0 to xT (reverse of Eq. (14)) and reconstruct x0 from the resulting xT (forward\nof Eq. (14))7. We consider encoding and decoding on the CIFAR-10 test set with the CIFAR-10\nmodel with S steps for both encoding and decoding; we report the per-dimension mean squared\nerror (scaled to [0, 1]) in Table 2. Our results show that DDIMs have lower reconstruction error for\nlarger S values and have properties similar to Neural ODEs and normalizing \ufb02ows. The same cannot\nbe said for DDPMs due to their stochastic nature.\n6\nRELATED WORK\nOur work is based on a large family of existing methods on learning generative models as transi-\ntion operators of Markov chains (Sohl-Dickstein et al., 2015; Bengio et al., 2014; Salimans et al.,\n2014; Song et al., 2017; Goyal et al., 2017; Levy et al., 2017). Among them, denoising diffusion\nprobabilistic models (DDPMs, Ho et al. (2020)) and noise conditional score networks (NCSN, Song\n& Ermon (2019; 2020)) have recently achieved high sample quality comparable to GANs (Brock\net al., 2018; Karras et al., 2018). DDPMs optimize a variational lower bound to the log-likelihood,\nwhereas NCSNs optimize the score matching objective (Hyv\u00a8arinen, 2005) over a nonparametric\nParzen density estimator of the data (Vincent, 2011; Raphan & Simoncelli, 2011).\nDespite their different motivations, DDPMs and NCSNs are closely related. Both use a denoising\nautoencoder objective for many noise levels, and both use a procedure similar to Langevin dynamics\nto produce samples (Neal et al., 2011). Since Langevin dynamics is a discretization of a gradient\n\ufb02ow (Jordan et al., 1998), both DDPM and NCSN require many steps to achieve good sample quality.\nThis aligns with the observation that DDPM and existing NCSN methods have trouble generating\nhigh-quality samples in a few iterations.\nDDIM, on the other hand, is an implicit generative model (Mohamed & Lakshminarayanan, 2016)\nwhere samples are uniquely determined from the latent variables. Hence, DDIM has certain prop-\nerties that resemble GANs (Goodfellow et al., 2014) and invertible \ufb02ows (Dinh et al., 2016), such\nas the ability to produce semantically meaningful interpolations. We derive DDIM from a purely\nvariational perspective, where the restrictions of Langevin dynamics are not relevant; this could par-\ntially explain why we are able to observe superior sample quality compared to DDPM under fewer\niterations. The sampling procedure of DDIM is also reminiscent of neural networks with continuous\ndepth (Chen et al., 2018; Grathwohl et al., 2018), since the samples it produces from the same latent\nvariable have similar high-level visual features, regardless of the speci\ufb01c sample trajectory.\n7\nDISCUSSION\nWe have presented DDIMs \u2013 an implicit generative model trained with denoising auto-encoding /\nscore matching objectives \u2013 from a purely variational perspective. DDIM is able to generate high-\n6Although it might be possible if one interpolates all T noises, like what is done in Song & Ermon (2020).\n7Since xT and x0 have the same dimensions, their compression qualities are not our immediate concern.\n9\nPublished as a conference paper at ICLR 2021\nquality samples much more ef\ufb01ciently than existing DDPMs and NCSNs, with the ability to perform\nmeaningful interpolations from the latent space. The non-Markovian forward process presented here\nseems to suggest continuous forward processes other than Gaussian (which cannot be done in the\noriginal diffusion framework, since Gaussian is the only stable distribution with \ufb01nite variance). We\nalso demonstrated a discrete case with a multinomial forward process in Appendix A, and it would\nbe interesting to investigate similar alternatives for other combinatorial structures.\nMoreover, since the sampling procedure of DDIMs is similar to that of an neural ODE, it would\nbe interesting to see if methods that decrease the discretization error in ODEs, including multi-\nstep methods such as Adams-Bashforth (Butcher & Goodwin, 2008), could be helpful for further\nimproving sample quality in fewer steps (Queiruga et al., 2020). It is also relevant to investigate\nwhether DDIMs exhibit other properties of existing implicit models (Bau et al., 2019).\nACKNOWLEDGEMENTS\nThe authors would like to thank Yang Song and Shengjia Zhao for helpful discussions over the\nideas, Kuno Kim for reviewing an earlier draft of the paper, and Sharvil Nanavati and Sophie Liu\nfor identifying typos. This research was supported by NSF (#1651565, #1522054, #1733686), ONR\n(N00014-19-1-2145), AFOSR (FA9550-19-1-0024), and Amazon AWS.\nREFERENCES\nMartin Arjovsky, Soumith Chintala, and L\u00b4eon Bottou.\nWasserstein GAN.\narXiv preprint\narXiv:1701.07875, January 2017.\nDavid Bau, Jun-Yan Zhu, Jonas Wulff, William Peebles, Hendrik Strobelt, Bolei Zhou, and An-\ntonio Torralba. Seeing what a gan cannot generate. In Proceedings of the IEEE International\nConference on Computer Vision, pp. 4502\u20134511, 2019.\nYoshua Bengio, Eric Laufer, Guillaume Alain, and Jason Yosinski.\nDeep generative stochastic\nnetworks trainable by backprop. In International Conference on Machine Learning, pp. 226\u2013234,\nJanuary 2014.\nChristopher M Bishop. Pattern recognition and machine learning. springer, 2006.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high \ufb01delity\nnatural image synthesis. arXiv preprint arXiv:1809.11096, September 2018.\nJohn Charles Butcher and Nicolette Goodwin. Numerical methods for ordinary differential equa-\ntions, volume 2. Wiley Online Library, 2008.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. WaveG-\nrad: Estimating gradients for waveform generation. arXiv preprint arXiv:2009.00713, September\n2020.\nRicky T Q Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differ-\nential equations. arXiv preprint arXiv:1806.07366, June 2018.\nLaurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. arXiv\npreprint arXiv:1605.08803, May 2016.\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair,\nAaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in neural infor-\nmation processing systems, pp. 2672\u20132680, 2014.\nAnirudh Goyal, Nan Rosemary Ke, Surya Ganguli, and Yoshua Bengio.\nVariational walkback:\nLearning a transition operator as a stochastic recurrent net. In Advances in Neural Information\nProcessing Systems, pp. 4392\u20134402, 2017.\nWill Grathwohl, Ricky T Q Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. FFJORD:\nFree-form continuous dynamics for scalable reversible generative models.\narXiv preprint\narXiv:1810.01367, October 2018.\n10\nPublished as a conference paper at ICLR 2021\nIshaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Im-\nproved training of wasserstein gans. In Advances in Neural Information Processing Systems, pp.\n5769\u20135779, 2017.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two Time-Scale update rule converge to a local nash equilibrium.\narXiv\npreprint arXiv:1706.08500, June 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint\narXiv:2006.11239, June 2020.\nAapo Hyv\u00a8arinen. Estimation of Non-Normalized statistical models by score matching. Journal of\nMachine Learning Researc h, 6:695\u2013709, 2005.\nAlexia Jolicoeur-Martineau, R\u00b4emi Pich\u00b4e-Taillefer, R\u00b4emi Tachet des Combes, and Ioannis\nMitliagkas. Adversarial score matching and improved sampling for image generation. September\n2020.\nRichard Jordan, David Kinderlehrer, and Felix Otto. The variational formulation of the fokker\u2013\nplanck equation. SIAM journal on mathematical analysis, 29(1):1\u201317, 1998.\nTero Karras, Samuli Laine, and Timo Aila. A Style-Based generator architecture for generative\nadversarial networks. arXiv preprint arXiv:1812.04948, December 2018.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyz-\ning and improving the image quality of stylegan. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8110\u20138119, 2020.\nDiederik P Kingma and Max Welling.\nAuto-Encoding variational bayes.\narXiv preprint\narXiv:1312.6114v10, December 2013.\nDaniel Levy, Matthew D Hoffman, and Jascha Sohl-Dickstein. Generalizing hamiltonian monte\ncarlo with neural networks. arXiv preprint arXiv:1711.09268, 2017.\nShakir Mohamed and Balaji Lakshminarayanan. Learning in implicit generative models. arXiv\npreprint arXiv:1610.03483, October 2016.\nRadford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo,\n2(11):2, 2011.\nAlejandro F Queiruga, N Benjamin Erichson, Dane Taylor, and Michael W Mahoney. Continuous-\nin-depth neural networks. arXiv preprint arXiv:2008.02389, 2020.\nMartin Raphan and Eero P Simoncelli.\nLeast squares estimation without priors or supervision.\nNeural computation, 23(2):374\u2013420, February 2011. ISSN 0899-7667, 1530-888X.\nDanilo Jimenez Rezende and Shakir Mohamed. Variational inference with normalizing \ufb02ows. arXiv\npreprint arXiv:1505.05770, May 2015.\nDanilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation and\napproximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation. In International Conference on Medical image computing and computer-\nassisted intervention, pp. 234\u2013241. Springer, 2015.\nTim Salimans, Diederik P Kingma, and Max Welling. Markov chain monte carlo and variational\ninference: Bridging the gap. arXiv preprint arXiv:1410.6460, October 2014.\nKen Shoemake. Animating rotation with quaternion curves. In Proceedings of the 12th annual\nconference on Computer graphics and interactive techniques, pp. 245\u2013254, 1985.\nJascha Sohl-Dickstein, Eric A Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. arXiv preprint arXiv:1503.03585, March\n2015.\n11\nPublished as a conference paper at ICLR 2021\nJiaming Song, Shengjia Zhao, and Stefano Ermon. A-nice-mc: Adversarial training for mcmc. arXiv\npreprint arXiv:1706.07561, June 2017.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\narXiv preprint arXiv:1907.05600, July 2019.\nYang Song and Stefano Ermon. Improved techniques for training Score-Based generative models.\narXiv preprint arXiv:2006.09011, June 2020.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456, 2020.\nAaron van den Oord, Sander Dieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals, Alex Graves,\nNal Kalchbrenner, Andrew Senior, and Koray Kavukcuoglu. WaveNet: A generative model for\nraw audio. arXiv preprint arXiv:1609.03499, September 2016a.\nAaron van den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks.\narXiv preprint arXiv:1601.06759, January 2016b.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural compu-\ntation, 23(7):1661\u20131674, 2011.\nSergey Zagoruyko and Nikos Komodakis.\nWide residual networks.\narXiv preprint\narXiv:1605.07146, May 2016.\nShengjia Zhao, Hongyu Ren, Arianna Yuan, Jiaming Song, Noah Goodman, and Stefano Ermon.\nBias and generalization in deep generative models: An empirical study. In Advances in Neural\nInformation Processing Systems, pp. 10792\u201310801, 2018.\n12\nPublished as a conference paper at ICLR 2021\nA\nNON-MARKOVIAN FORWARD PROCESSES FOR A DISCRETE CASE\nIn this section, we describe a non-Markovian forward processes for discrete data and corresponding\nvariational objectives. Since the focus of this paper is to accelerate reverse models corresponding to\nthe Gaussian diffusion, we leave empirical evaluations as future work.\nFor a categorical observation x0 that is a one-hot vector with K possible values, we de\ufb01ne the\nforward process as follows. First, we have q(xt|x0) as the following categorical distribution:\nq(xt|x0) = Cat(\u03b1tx0 + (1 \u2212\u03b1t)1K)\n(17)\nwhere 1K \u2208RK is a vector with all entries being 1/K, and \u03b1t decreasing from \u03b10 = 1 for t = 0 to\n\u03b1T = 0 for t = T. Then we de\ufb01ne q(xt\u22121|xt, x0) as the following mixture distribution:\nq(xt\u22121|xt, x0) =\n\uf8f1\n\uf8f2\n\uf8f3\nCat(xt)\nwith probability \u03c3t\nCat(x0)\nwith probability (\u03b1t\u22121 \u2212\u03c3t\u03b1t)\nCat(1K)\nwith probability (1 \u2212\u03b1t\u22121) \u2212(1 \u2212\u03b1t)\u03c3t\n,\n(18)\nor equivalently:\nq(xt\u22121|xt, x0) = Cat (\u03c3txt + (\u03b1t\u22121 \u2212\u03c3t\u03b1t)x0 + ((1 \u2212\u03b1t\u22121) \u2212(1 \u2212\u03b1t)\u03c3t)1K) ,\n(19)\nwhich is consistent with how we have de\ufb01ned q(xt|x0).\nSimilarly, we can de\ufb01ne our reverse process p\u03b8(xt\u22121|xt) as:\np\u03b8(xt\u22121|xt) = Cat\n\u0010\n\u03c3txt + (\u03b1t\u22121 \u2212\u03c3t\u03b1t)f (t)\n\u03b8 (xt) + ((1 \u2212\u03b1t\u22121) \u2212(1 \u2212\u03b1t)\u03c3t)1K\n\u0011\n,\n(20)\nwhere f (t)\n\u03b8 (xt) maps xt to a K-dimensional vector. As (1 \u2212\u03b1t\u22121) \u2212(1 \u2212\u03b1t)\u03c3t \u21920, the sampling\nprocess will become less stochastic, in the sense that it will either choose xt or the predicted x0\nwith high probability. The KL divergence\nDKL(q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt))\n(21)\nis well-de\ufb01ned, and is simply the KL divergence between two categoricals. Therefore, the resulting\nvariational objective function should be easy to optimize as well. Moreover, as KL divergence is\nconvex, we have this upper bound (which is tight when the right hand side goes to zero):\nDKL(q(xt\u22121|xt, x0)\u2225p\u03b8(xt\u22121|xt)) \u2264(\u03b1t\u22121 \u2212\u03c3t\u03b1t)DKL(Cat(x0)\u2225Cat(f (t)\n\u03b8 (xt))).\nThe right hand side is simply a multi-class classi\ufb01cation loss (up to constants), so we can arrive at\nsimilar arguments regarding how changes in \u03c3t do not affect the objective (up to re-weighting).\nB\nPROOFS\nLemma 1. For q\u03c3(x1:T |x0) de\ufb01ned in Eq. (6) and q\u03c3(xt\u22121|xt, x0) de\ufb01ned in Eq. (7), we have:\nq\u03c3(xt|x0) = N(\u221a\u03b1tx0, (1 \u2212\u03b1t)I)\n(22)\nProof. Assume for any t \u2264T, q\u03c3(xt|x0) = N(\u221a\u03b1tx0, (1 \u2212\u03b1t)I) holds, if:\nq\u03c3(xt\u22121|x0) = N(\u221a\u03b1t\u22121x0, (1 \u2212\u03b1t\u22121)I)\n(23)\nthen we can prove the statement with an induction argument for t from T to 1, since the base case\n(t = T) already holds.\nFirst, we have that\nq\u03c3(xt\u22121|x0) :=\nZ\nxt\nq\u03c3(xt|x0)q\u03c3(xt\u22121|xt, x0)dxt\nand\nq\u03c3(xt|x0) = N(\u221a\u03b1tx0, (1 \u2212\u03b1t)I)\n(24)\nq\u03c3(xt\u22121|xt, x0) = N\n\u0012\u221a\u03b1t\u22121x0 +\nq\n1 \u2212\u03b1t\u22121 \u2212\u03c32\nt \u00b7 xt \u2212\u221a\u03b1tx0\n\u221a1 \u2212\u03b1t\n, \u03c32\nt I\n\u0013\n.\n(25)\n13\nPublished as a conference paper at ICLR 2021\nFrom Bishop (2006) (2.115), we have that q\u03c3(xt\u22121|x0) is Gaussian, denoted as N(\u00b5t\u22121, \u03a3t\u22121)\nwhere\n\u00b5t\u22121 = \u221a\u03b1t\u22121x0 +\nq\n1 \u2212\u03b1t\u22121 \u2212\u03c32\nt \u00b7\n\u221a\u03b1tx0 \u2212\u221a\u03b1tx0\n\u221a1 \u2212\u03b1t\n(26)\n= \u221a\u03b1t\u22121x0\n(27)\nand\n\u03a3t\u22121 = \u03c32\nt I + 1 \u2212\u03b1t\u22121 \u2212\u03c32\nt\n1 \u2212\u03b1t\n(1 \u2212\u03b1t)I = (1 \u2212\u03b1t\u22121)I\n(28)\nTherefore, q\u03c3(xt\u22121|x0) = N(\u221a\u03b1t\u22121x0, (1 \u2212\u03b1t\u22121)I), which allows us to apply the induction\nargument.\nTheorem 1. For all \u03c3 > 0, there exists \u03b3 \u2208RT\n>0 and C \u2208R, such that J\u03c3 = L\u03b3 + C.\nProof. From the de\ufb01nition of J\u03c3:\nJ\u03c3(\u03f5\u03b8) := Ex0:T \u223cq(x0:T )\n\"\nlog q\u03c3(xT |x0) +\nT\nX\nt=2\nlog q\u03c3(xt\u22121|xt, x0) \u2212\nT\nX\nt=1\nlog p(t)\n\u03b8 (xt\u22121|xt)\n#\n(29)\n\u2261Ex0:T \u223cq(x0:T )\n\" T\nX\nt=2\nDKL(q\u03c3(xt\u22121|xt, x0))\u2225p(t)\n\u03b8 (xt\u22121|xt)) \u2212log p(1)\n\u03b8 (x0|x1)\n#\nwhere we use \u2261to denote \u201cequal up to a value that does not depend on \u03f5\u03b8 (but may depend on q\u03c3)\u201d.\nFor t > 1:\nEx0,xt\u223cq(x0,xt)[DKL(q\u03c3(xt\u22121|xt, x0))\u2225p(t)\n\u03b8 (xt\u22121|xt))]\n= Ex0,xt\u223cq(x0,xt)[DKL(q\u03c3(xt\u22121|xt, x0))\u2225q\u03c3(xt\u22121|xt, f (t)\n\u03b8 (xt)))]\n\u2261Ex0,xt\u223cq(x0,xt)\n\uf8ee\n\uf8f0\u2225x0 \u2212f (t)\n\u03b8 (xt)\u2225\n2\n2\n2\u03c32\nt\n\uf8f9\n\uf8fb\n(30)\n= Ex0\u223cq(x0),\u03f5\u223cN (0,I),xt=\u221a\u03b1tx0+\u221a1\u2212\u03b1t\u03f5\n\uf8ee\n\uf8ef\uf8f0\n\u2225(xt\u2212\u221a1\u2212\u03b1t\u03f5)\n\u221a\u03b1t\n\u2212(xt\u2212\u221a1\u2212\u03b1t\u03f5(t)\n\u03b8 (xt))\n\u221a\u03b1t\n\u2225\n2\n2\n2\u03c32\nt\n\uf8f9\n\uf8fa\uf8fb\n(31)\n= Ex0\u223cq(x0),\u03f5\u223cN (0,I),xt=\u221a\u03b1tx0+\u221a1\u2212\u03b1t\u03f5\n\uf8ee\n\uf8f0\u2225\u03f5 \u2212\u03f5(t)\n\u03b8 (xt)\u2225\n2\n2\n2d\u03c32\nt \u03b1t\n\uf8f9\n\uf8fb\n(32)\nwhere d is the dimension of x0. For t = 1:\nEx0,x1\u223cq(x0,x1)\nh\n\u2212log p(1)\n\u03b8 (x0|x1)\ni\n\u2261Ex0,x1\u223cq(x0,x1)\n\uf8ee\n\uf8f0\u2225x0 \u2212f (t)\n\u03b8 (x1)\u2225\n2\n2\n2\u03c32\n1\n\uf8f9\n\uf8fb\n(33)\n= Ex0\u223cq(x0),\u03f5\u223cN (0,I),x1=\u221a\u03b11x0+\u221a1\u2212\u03b1t\u03f5\n\uf8ee\n\uf8f0\u2225\u03f5 \u2212\u03f5(1)\n\u03b8 (x1)\u2225\n2\n2\n2d\u03c32\n1\u03b11\n\uf8f9\n\uf8fb\n(34)\nTherefore, when \u03b3t = 1/(2d\u03c32\nt \u03b1t) for all t \u2208{1, . . . , T}, we have\nJ\u03c3(\u03f5\u03b8) \u2261\nT\nX\nt=1\n1\n2d\u03c32\nt \u03b1t\nE\nh\n\u2225\u03f5(t)\n\u03b8 (xt) \u2212\u03f5t\u2225\n2\n2\ni\n= L\u03b3(\u03f5\u03b8)\n(35)\nfor all \u03f5\u03b8. From the de\ufb01nition of \u201c\u2261\u201d, we have that J\u03c3 = L\u03b3 + C.\nProposition 1. The ODE in Eq. (14) with the optimal model \u03f5(t)\n\u03b8\nhas an equivalent probability \ufb02ow\nODE corresponding to the \u201cVariance-Exploding\u201d SDE in Song et al. (2020).\n14\nPublished as a conference paper at ICLR 2021\nProof. In the context of the proof, we consider t as a continous, independent \u201ctime\u201d variable and x\nand \u03b1 as functions of t. First, let us consider a reparametrization between DDIM and the VE-SDE8\nby introducing the variables \u00afx and \u03c3:\n\u00afx(t) = \u00afx(0) + \u03c3(t)\u03f5,\n\u03f5 \u223cN(0, I),\n(36)\nfor t \u2208[0, \u221e) and an increasing continuous function \u03c3 : R\u22650 \u2192R\u22650 where \u03c3(0) = 0.\nWe can then de\ufb01ne \u03b1(t) and x(t) corresponding to DDIM case as:\n\u00afx(t) =\nx(t)\np\n\u03b1(t)\n(37)\n\u03c3(t) =\ns\n1 \u2212\u03b1(t)\n\u03b1(t)\n.\n(38)\nThis also means that:\nx(t) =\n\u00afx(t)\np\n\u03c32(t) + 1\n(39)\n\u03b1(t) =\n1\n1 + \u03c32(t),\n(40)\nwhich establishes an bijection between (x, \u03b1) and (\u00afx, \u03c3). From Equation (4) we have (note that\n\u03b1(0) = 1):\nx(t)\np\n\u03b1(t)\n=\nx(0)\np\n\u03b1(0)\n+\ns\n1 \u2212\u03b1(t)\n\u03b1(t)\n\u03f5,\n\u03f5 \u223cN(0, I)\n(41)\nwhich can be reparametrized into a form that is consistent with VE-SDE:\n\u00afx(t) = \u00afx(0) + \u03c3(t)\u03f5.\n(42)\nNow, we derive the ODE forms for both DDIM and VE-SDE and show that they are equivalent.\nODE form for DDIM\nWe repeat Equation (13) here:\nxt\u2212\u2206t\n\u221a\u03b1t\u2212\u2206t\n=\nxt\n\u221a\u03b1t\n+\n s\n1 \u2212\u03b1t\u2212\u2206t\n\u03b1t\u2212\u2206t\n\u2212\nr\n1 \u2212\u03b1t\n\u03b1t\n!\n\u03f5(t)\n\u03b8 (xt),\n(43)\nwhich is equivalent to:\n\u00afx(t \u2212\u2206t) = \u00afx(t) + (\u03c3(t \u2212\u2206t) \u2212\u03c3(t)) \u00b7 \u03f5(t)\n\u03b8 (x(t))\n(44)\nDivide both sides by (\u2212\u2206t) and as \u2206t \u21920, we have:\nd\u00afx(t)\ndt\n= d\u03c3(t)\ndt\n\u03f5(t)\n\u03b8\n \n\u00afx(t)\np\n\u03c32(t) + 1\n!\n,\n(45)\nwhich is exactly what we have in Equation (14).\nWe note that for the optimal model, \u03f5(t)\n\u03b8\nis a minimizer:\n\u03f5(t)\n\u03b8\n= arg min\nft\nEx(0)\u223cq(x),\u03f5\u223cN (0,I)[\u2225ft(x(t)) \u2212\u03f5\u22252\n2]\n(46)\nwhere x(t) =\np\n\u03b1(t)x(t) +\np\n1 \u2212\u03b1(t)\u03f5.\n8Refer to (Song et al., 2020) for more details of VE-SDE.\n15\nPublished as a conference paper at ICLR 2021\nODE form for VE-SDE\nDe\ufb01ne pt(\u00afx) as the data distribution perturbed with \u03c32(t) variance Gaus-\nsian noise. The probability \ufb02ow for VE-SDE is de\ufb01ned as Song et al. (2020):\nd\u00afx = \u22121\n2g(t)2\u2207\u00afx log pt(\u00afx)dt\n(47)\nwhere g(t) =\nq\nd\u03c32(t)\ndt\nis the diffusion coef\ufb01cient, and \u2207\u00afx log pt(\u00afx) is the score of pt.\nThe \u03c3(t)-perturbed score function \u2207\u00afx log pt(\u00afx) is also a minimizer (from denoising score match-\ning (Vincent, 2011)):\n\u2207\u00afx log pt = arg min\ngt\nEx(0)\u223cq(x),\u03f5\u223cN (0,I)[\u2225gt(\u00afx) + \u03f5/\u03c3(t)\u22252\n2]\n(48)\nwhere \u00afx(t) = \u00afx(t) + \u03c3(t)\u03f5.\nSince there is an equivalence between x(t) and \u00afx(t), we have the following relationship:\n\u2207\u00afx log pt(\u00afx) = \u2212\n\u03f5(t)\n\u03b8\n\u0012\n\u00afx(t)\n\u221a\n\u03c32(t)+1\n\u0013\n\u03c3(t)\n(49)\nfrom Equation (46) and Equation (48). Plug Equation (49) and de\ufb01nition of g(t) in Equation (47),\nwe have:\nd\u00afx(t) = 1\n2\nd\u03c32(t)\ndt\n\u03f5(t)\n\u03b8\n\u0012\n\u00afx(t)\n\u221a\n\u03c32(t)+1\n\u0013\n\u03c3(t)\ndt,\n(50)\nand we have the following by rearranging terms:\nd\u00afx(t)\ndt\n= d\u03c3(t)\ndt\n\u03f5(t)\n\u03b8\n \n\u00afx(t)\np\n\u03c32(t) + 1\n!\n(51)\nwhich is equivalent to Equation (45). In both cases the initial conditions are \u00afx(T) \u223cN(0, \u03c32(T)I),\nso the resulting ODEs are identical.\nC\nADDITIONAL DERIVATIONS\nC.1\nACCELERATED SAMPLING PROCESSES\nIn the accelerated case, we can consider the inference process to be factored as:\nq\u03c3,\u03c4(x1:T |x0) = q\u03c3,\u03c4(x\u03c4S|x0)\nS\nY\ni=1\nq\u03c3,\u03c4(x\u03c4i\u22121|x\u03c4i, x0)\nY\nt\u2208\u00af\u03c4\nq\u03c3,\u03c4(xt|x0)\n(52)\nwhere \u03c4 is a sub-sequence of [1, . . . , T] of length S with \u03c4S = T, and let \u00af\u03c4 := {1, . . . , T} \\ \u03c4\nbe its complement. Intuitively, the graphical model of {x\u03c4i}S\ni=1 and x0 form a chain, whereas the\ngraphical model of {xt}t\u2208\u00af\u03c4 and x0 forms a star graph. We de\ufb01ne:\nq\u03c3,\u03c4(xt|x0) = N(\u221a\u03b1tx0, (1 \u2212\u03b1t)I)\n\u2200t \u2208\u00af\u03c4 \u222a{T}\n(53)\nq\u03c3,\u03c4(x\u03c4i\u22121|x\u03c4i, x0) = N\n\u0012\n\u221a\u03b1\u03c4i\u22121x0 +\nq\n1 \u2212\u03b1\u03c4i\u22121 \u2212\u03c32\u03c4i \u00b7 x\u03c4i \u2212\u221a\u03b1\u03c4ix0\n\u221a1 \u2212\u03b1\u03c4i\n, \u03c32\n\u03c4iI\n\u0013\n\u2200i \u2208[S]\nwhere the coef\ufb01cients are chosen such that:\nq\u03c3,\u03c4(x\u03c4i|x0) = N(\u221a\u03b1\u03c4ix0, (1 \u2212\u03b1\u03c4i)I)\n\u2200i \u2208[S]\n(54)\ni.e., the \u201cmarginals\u201d match.\nThe corresponding \u201cgenerative process\u201d is de\ufb01ned as:\np\u03b8(x0:T ) := p\u03b8(xT )\nS\nY\ni=1\np(\u03c4i)\n\u03b8\n(x\u03c4i\u22121|x\u03c4i)\n|\n{z\n}\nuse to produce samples\n\u00d7\nY\nt\u2208\u00af\u03c4\np(t)\n\u03b8 (x0|xt)\n|\n{z\n}\nin variational objective\n(55)\n16\nPublished as a conference paper at ICLR 2021\nwhere only part of the models are actually being used to produce samples. The conditionals are:\np(\u03c4i)\n\u03b8\n(x\u03c4i\u22121|x\u03c4i) = q\u03c3,\u03c4(x\u03c4i\u22121|x\u03c4i, f (\u03c4i)\n\u03b8\n(x\u03c4i\u22121))\nif i \u2208[S], i > 1\n(56)\np(t)\n\u03b8 (x0|xt) = N(f (t)\n\u03b8 (xt), \u03c32\nt I)\notherwise,\n(57)\nwhere we leverage q\u03c3,\u03c4(x\u03c4i\u22121|x\u03c4i, x0) as part of the inference process (similar to what we have done\nin Section 3). The resulting variational objective becomes (de\ufb01ne x\u03c4L+1 = \u2205for conciseness):\nJ(\u03f5\u03b8) = Ex0:T \u223cq\u03c3,\u03c4 (x0:T )[log q\u03c3,\u03c4(x1:T |x0) \u2212log p\u03b8(x0:T )]\n(58)\n= Ex0:T \u223cq\u03c3,\u03c4 (x0:T )\n\" X\nt\u2208\u00af\u03c4\nDKL(q\u03c3,\u03c4(xt|x0)\u2225p(t)\n\u03b8 (x0|xt)\n(59)\n+\nL\nX\ni=1\nDKL(q\u03c3,\u03c4(x\u03c4i\u22121|x\u03c4i, x0)\u2225p(\u03c4i)\n\u03b8\n(x\u03c4i\u22121|x\u03c4i)))\n#\nwhere each KL divergence is between two Gaussians with variance independent of \u03b8. A similar\nargument to the proof used in Theorem 1 can show that the variational objective J can also be\nconverted to an objective of the form L\u03b3.\nC.2\nDERIVATION OF DENOISING OBJECTIVES FOR DDPMS\nWe note that in Ho et al. (2020), a diffusion hyperparameter \u03b2t9 is \ufb01rst introduced, and then relevant\nvariables \u03b1t := 1 \u2212\u03b2t and \u00af\u03b1t = QT\nt=1 \u03b1t are de\ufb01ned. In this paper, we have used the notation\n\u03b1t to represent the variable \u00af\u03b1t in Ho et al. (2020) for three reasons. First, it makes it more clear\nthat we only need to choose one set of hyperparameters, reducing possible cross-references of the\nderived variables. Second, it allows us to introduce the generalization as well as the acceleration\ncase easier, because the inference process is no longer motivated by a diffusion. Third, there exists\nan isomorphism between \u03b11:T and 1, . . . , T, which is not the case for \u03b2t.\nIn this section, we use \u03b2t and \u03b1t to be more consistent with the derivation in Ho et al. (2020), where\n\u03b1t =\n\u03b1t\n\u03b1t\u22121\n(60)\n\u03b2t = 1 \u2212\n\u03b1t\n\u03b1t\u22121\n(61)\ncan be uniquely determined from \u03b1t (i.e. \u00af\u03b1t).\nFirst, from the diffusion forward process:\nq(xt\u22121|xt, x0) = N\n \u221a\u03b1t\u22121\u03b2t\n1 \u2212\u03b1t\nx0 +\n\u221a\u03b1t(1 \u2212\u03b1t\u22121)\n1 \u2212\u03b1t\nxt\n|\n{z\n}\n\u02dc\u00b5(xt,x0)\n, 1 \u2212\u03b1t\u22121\n1 \u2212\u03b1t\n\u03b2tI\n!\nHo et al. (2020) considered a speci\ufb01c type of p(t)\n\u03b8 (xt\u22121|xt):\np(t)\n\u03b8 (xt\u22121|xt) = N (\u00b5\u03b8(xt, t), \u03c3tI)\n(62)\nwhich leads to the following variational objective:\nL := Ex0:T \u223cq(x0:T )\n\"\nq(xT |x0) +\nT\nX\nt=2\nlog q(xt\u22121|xt, x0) \u2212\nT\nX\nt=1\nlog p(t)\n\u03b8 (xt\u22121|xt)\n#\n(63)\n\u2261Ex0:T \u223cq(x0:T )\n\uf8ee\n\uf8ef\uf8f0\nT\nX\nt=2\nDKL(q(xt\u22121|xt, x0))\u2225p(t)\n\u03b8 (xt\u22121|xt))\n|\n{z\n}\nLt\u22121\n\u2212log p(1)\n\u03b8 (x0|x1)\n\uf8f9\n\uf8fa\uf8fb\n9In this section we use teal to color notations used in Ho et al. (2020).\n17\nPublished as a conference paper at ICLR 2021\nOne can write:\nLt\u22121 = Eq\n\u0014 1\n2\u03c32\nt\n\u2225\u00b5\u03b8(xt, t) \u2212\u02dc\u00b5(xt, x0)\u22252\n2\n\u0015\n(64)\nHo et al. (2020) chose the parametrization\n\u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n(65)\nwhich can be simpli\ufb01ed to:\nLt\u22121 = Ex0,\u03f5\n\u0014\n\u03b2t2\n2\u03c32\nt (1 \u2212\u03b1t)\u03b1t\n\u2225\u03f5 \u2212\u03f5\u03b8(\u221a\u03b1tx0 +\n\u221a\n1 \u2212\u03b1t\u03f5, t)\u2225\n2\n2\n\u0015\n(66)\nD\nEXPERIMENTAL DETAILS\nD.1\nDATASETS AND ARCHITECTURES\nWe consider 4 image datasets with various resolutions: CIFAR10 (32 \u00d7 32, unconditional), CelebA\n(64 \u00d7 64), LSUN Bedroom (256 \u00d7 256) and LSUN Church (256 \u00d7 256). For all datasets, we\nset the hyperparameters \u03b1 according to the heuristic in (Ho et al., 2020) to make the results directly\ncomparable. We use the same model for each dataset, and only compare the performance of different\ngenerative processes. For CIFAR10, Bedroom and Church, we obtain the pretrained checkpoints\nfrom the original DDPM implementation; for CelebA, we trained our own model using the denoising\nobjective L1.\nOur architecture for \u03f5(t)\n\u03b8 (xt) follows that in Ho et al. (2020), which is a U-Net (Ronneberger et al.,\n2015) based on a Wide ResNet (Zagoruyko & Komodakis, 2016). We use the pretrained models\nfrom Ho et al. (2020) for CIFAR10, Bedroom and Church, and train our own model for the CelebA\n64 \u00d7 64 model (since a pretrained model is not provided). Our CelebA model has \ufb01ve feature map\nresolutions from 64 \u00d7 64 to 4 \u00d7 4, and we use the original CelebA dataset (not CelebA-HQ) using\nthe pre-processing technique from the StyleGAN (Karras et al., 2018) repository.\nTable 3: LSUN Bedroom and Church image generation results, measured in FID. For 1000 steps\nDDPM, the FIDs are 6.36 for Bedroom and 7.89 for Church.\nBedroom (256 \u00d7 256)\nChurch (256 \u00d7 256)\ndim(\u03c4)\n10\n20\n50\n100\n10\n20\n50\n100\nDDIM (\u03b7 = 0.0)\n16.95\n8.89\n6.75\n6.62\n19.45\n12.47\n10.84\n10.58\nDDPM (\u03b7 = 1.0)\n42.78\n22.77\n10.81\n6.81\n51.56\n23.37\n11.16\n8.27\nD.2\nREVERSE PROCESS SUB-SEQUENCE SELECTION\nWe consider two types of selection procedure for \u03c4 given the desired dim(\u03c4) < T:\n\u2022 Linear: we select the timesteps such that \u03c4i = \u230aci\u230bfor some c;\n\u2022 Quadratic: we select the timesteps such that \u03c4i = \u230aci2\u230bfor some c.\nThe constant value c is selected such that \u03c4\u22121 is close to T. We used quadratic for CIFAR10 and\nlinear for the remaining datasets. These choices achieve slightly better FID than their alternatives in\nthe respective datasets.\nD.3\nCLOSED FORM EQUATIONS FOR EACH SAMPLING STEP\nFrom the general sampling equation in Eq. (12), we have the following update equation:\nx\u03c4i\u22121(\u03b7) = \u221a\u03b1\u03c4i\u22121\n \nx\u03c4i \u2212\u221a1 \u2212\u03b1\u03c4i\u03f5(\u03c4i)\n\u03b8\n(x\u03c4i)\n\u221a\u03b1\u03c4i\n!\n+\nq\n1 \u2212\u03b1\u03c4i\u22121 \u2212\u03c3\u03c4i(\u03b7)2 \u00b7 \u03f5(\u03c4i)\n\u03b8\n(x\u03c4i) + \u03c3\u03c4i(\u03b7)\u03f5\n18\nPublished as a conference paper at ICLR 2021\nFigure 7: CIFAR10 samples from 1000 step DDPM, 1000 step DDIM and 100 step DDIM.\nwhere\n\u03c3\u03c4i(\u03b7) = \u03b7\ns\n1 \u2212\u03b1\u03c4i\u22121\n1 \u2212\u03b1\u03c4i\nr\n1 \u2212\u03b1\u03c4i\n\u03b1\u03c4i\u22121\nFor the case of \u02c6\u03c3 (DDPM with a larger variance), the update equation becomes:\nx\u03c4i\u22121 = \u221a\u03b1\u03c4i\u22121\n \nx\u03c4i \u2212\u221a1 \u2212\u03b1\u03c4i\u03f5(\u03c4i)\n\u03b8\n(x\u03c4i)\n\u221a\u03b1\u03c4i\n!\n+\nq\n1 \u2212\u03b1\u03c4i\u22121 \u2212\u03c3\u03c4i(1)2 \u00b7 \u03f5(\u03c4i)\n\u03b8\n(x\u03c4i) + \u02c6\u03c3\u03c4i\u03f5\nwhich uses a different coef\ufb01cient for \u03f5 compared with the update for \u03b7 = 1, but uses the same\ncoef\ufb01cient for the non-stochastic parts. This update is more stochastic than the update for \u03b7 = 1,\nwhich explains why it achieves worse performance when dim(\u03c4) is small.\nD.4\nSAMPLES AND CONSISTENCY\nWe show more samples in Figure 7 (CIFAR10), Figure 8 (CelebA), Figure 10 (Church) and consis-\ntency results of DDIM in Figure 9 (CelebA).\nD.5\nINTERPOLATION\nTo generate interpolations on a line, we randomly sample two initial xT values from the standard\nGaussian, interpolate them with spherical linear interpolation (Shoemake, 1985), and then use the\nDDIM to obtain x0 samples.\nx(\u03b1)\nT\n= sin((1 \u2212\u03b1)\u03b8)\nsin(\u03b8)\nx(0)\nT\n+ sin(\u03b1\u03b8)\nsin(\u03b8) x(1)\nT\n(67)\nwhere \u03b8 = arccos\n\u0012\n(x(0)\nT )\u22a4x(1)\nT\n\u2225x(0)\nT \u2225\u2225x(1)\nT \u2225\n\u0013\n. These values are used to produce DDIM samples.\nTo generate interpolations on a grid, we sample four latent variables and separate them in to two\npairs; then we use slerp with the pairs under the same \u03b1, and use slerp over the interpolated samples\nacross the pairs (under an independently chosen interpolation coef\ufb01cient). We show more grid\ninterpolation results in Figure 11 (CelebA), Figure 12 (Bedroom), and Figure 13 (Church).\n19\nPublished as a conference paper at ICLR 2021\nFigure 8: CelebA samples from 1000 step DDPM, 1000 step DDIM and 100 step DDIM.\n20\n50\n100\n1000\nsample timesteps\nFigure 9: CelebA samples from DDIM with the same random xT and different number of steps.\nFigure 10: Church samples from 100 step DDPM and 100 step DDIM.\n20\nPublished as a conference paper at ICLR 2021\nFigure 11: More interpolations from the CelebA DDIM with dim(\u03c4) = 50.\nFigure 12: More interpolations from the Bedroom DDIM with dim(\u03c4) = 50.\n21\nPublished as a conference paper at ICLR 2021\nFigure 13: More interpolations from the Church DDIM with dim(\u03c4) = 50.\n22\n",
    "1805.00123": "CrowdHuman: A Benchmark for Detecting Human in a Crowd\nShuai Shao\u2217\nZijian Zhao\u2217\nBoxun Li\nTete Xiao\nGang Yu\nXiangyu Zhang\nJian Sun\nMegvii Inc. (Face++)\n{shaoshuai, zhaozijian, liboxun, xtt, yugang, zhangxiangyu, sunjian}@megvii.com\nAbstract\nHuman detection has witnessed impressive progress in\nrecent years. However, the occlusion issue of detecting hu-\nman in highly crowded environments is far from solved.\nTo make matters worse, crowd scenarios are still under-\nrepresented in current human detection benchmarks.\nIn\nthis paper, we introduce a new dataset, called Crowd-\nHuman1, to better evaluate detectors in crowd scenar-\nios. The CrowdHuman dataset is large, rich-annotated and\ncontains high diversity.\nThere are a total of 470K hu-\nman instances from the train and validation subsets, and\n22.6 persons per image, with various kinds of occlusions\nin the dataset.\nEach human instance is annotated with\na head bounding-box, human visible-region bounding-box\nand human full-body bounding-box. Baseline performance\nof state-of-the-art detection frameworks on CrowdHuman\nis presented.\nThe cross-dataset generalization results of\nCrowdHuman dataset demonstrate state-of-the-art perfor-\nmance on previous dataset including Caltech-USA, CityPer-\nsons, and Brainwash without bells and whistles. We hope\nour dataset will serve as a solid baseline and help promote\nfuture research in human detection tasks.\n1. Introduction\nDetecting people in images is among the most important\ncomponents of computer vision and has attracted increasing\nattention in recent years [29, 14, 32, 30, 10, 5, 4, 6, 18]. A\nsystem that is able to detect human accurately plays an es-\nsential role in applications such as autonomous cars, smart\nsurveillance, robotics, and advanced human machine inter-\nactions. Besides, it is a fundamental component for research\ntopics like multiple-object tracking [13], human pose esti-\nmation [28], and person search [24]. Coupled with the de-\nvelopment and blooming of convolutional neural networks\n(CNNs) [12, 22, 8], modern human detectors [1, 29, 26]\nhave achieved remarkable performance on several major hu-\n\u2217Equal contribution.\n1Our CrowdHuman dataset can be downloaded from https://\nsshao0516.github.io/CrowdHuman/\nman detection benchmarks.\nHowever, as the algorithms improve, more challenging\ndatasets are necessary to evaluate human detection systems\nin more complicated real world scenarios, where crowd\nscenes are relatively common. In crowd scenarios, differ-\nent people occlude with each other with high overlaps and\ncause great dif\ufb01culty of crowd occlusion.\nFor example,\nwhen a target pedestrian T is largely overlapped with other\npedestrians, the detector may fail to identify the boundaries\nof each person as they have similar appearances. Therefore,\ndetector will treat the crowd as a whole, or shift the tar-\nget bounding box of T to other pedestrians mistakenly. To\nmake matters worse, even though the detectors are able to\ndiscriminate different pedestrians in the crowd, the highly\noverlapped bounding boxes will also be suppressed by the\npost process of non-maximum suppression (NMS). As a re-\nsult, crowd occlusion makes the detector sensitive to the\nthreshold of NMS. A lower threshold may lead to drasti-\ncally drop on recall, while a higher threshold brings more\nfalse positives.\nCurrent datasets and benchmarks for human detection,\nsuch as Caltech-USA [6], KITTI [25], CityPersons [31],\nand \u201cperson\u201d subset of MSCOCO [17], have contributed\nto a rapid progress in the human detection. Nevertheless,\ncrowd scenarios are still under-represented in these datasets.\nFor example, the statistical number of persons per image is\nonly 0.32 in Caltech-USA, 4.01 in COCOPersons, and 6.47\nin CityPersons. And the average of pairwise overlap be-\ntween two human instances (larger than 0.5 IoU) in these\ndatasets is only 0.02, 0.02, and 0.32, respectively. Further-\nmore, the annotators for these datasets are more likely to\nannotate crowd human as a whole ignored region, which\ncannot be counted as valid samples in training and evalua-\ntion.\nOur goal is to push the boundary of human detection by\nspeci\ufb01cally targeting the challenging crowd scenarios. We\ncollect and annotate a rich dataset, termed CrowdHuman,\nwith considerable amount of crowded pedestrians. Crowd-\nHuman contains 15, 000, 4, 370 and 5, 000 images for train-\ning, validation, and testing respectively. The dataset is ex-\nhaustively annotated and contains diverse scenes. There are\n1\narXiv:1805.00123v1  [cs.CV]  30 Apr 2018\nFigure 1. Illustrative examples from different human dataset benchmarks. The images inside the green, yellow, blue boxes are from the\nCOCO [17], Caltech [6], and CityPersons [31] datasets, respectively. The images from the second row inside the red box are from our\nCrowdHuman benchmark with full body, visible body, and head bounding box annotations for each person.\ntotally 470k individual persons in the train and validation\nsubsets, and the average number of pedestrians per image\nreaches 22.6. We also provide the visible region bounding-\nbox annotation, and head region bounding-box annotation\nalong with its full body annotation for each person. Fig. 1\nshows examples in our dataset compared with those in other\nhuman detection datasets.\nTo summarize, we propose a new dataset called Crowd-\nHuman with the following three contributions:\n\u2022 To the best of our knowledge, this is the \ufb01rst dataset\nwhich speci\ufb01cally targets to address the crowd issue in\nhuman detection task. More speci\ufb01cally, the average\nnumber of persons in an image is 22.6 and the aver-\nage of pairwise overlap between two human instances\n(larger than 0.5 IoU) is 2.4, both of which are much\nlarger than the existing benchmarks like CityPersons,\nKITTI and Caltech.\n\u2022 The proposed CrowdHuman dataset provides annota-\ntions with three categories of bounding boxes: head\nbounding-box, human visible-region bounding-box,\nand human full-body bounding-box.\nFurthermore,\nthese three categories of bounding-boxes are bound for\neach human instance.\n\u2022 Experiments of cross-dataset generalization ability\ndemonstrate our dataset can serve as a powerful pre-\ntraining dataset for many human detection tasks. A\nframework originally designed for general object de-\ntection without any speci\ufb01c modi\ufb01cation provides\nstate-of-the-art results on every previous benchmark\nincluding Caltech and CityPersons for pedestrian de-\ntection, COCOPerson for person detection, and Brain-\nwash for head detection.\n2. Related Work\n2.1. Human detection datasets.\nPioneer works of pedestrian detection datasets involve\nINRIA [3], TudBrussels [27], and Daimler [7].\nThese\ndatasets have contributed to spurring interest and progress\nof human detection, However, as algorithm performance\nimproves,\nthese datasets are replaced by larger-scale\ndatasets like Caltech-USA [6] and KITTI [25]. More re-\ncently, Zhang et al. build a rich and diverse pedestrian de-\ntection dataset CityPersons [31] on top of CityScapes [2]\ndataset. It is recorded by a car traversing various cities, con-\ntains dense pedestrians, and is annotated with high-quality\nbounding boxes.\nDespite the prevalence of these datasets, they all suffer\na problem of from low density. Statistically, the Caltech-\nUSA and KITTI datasets have less than one person per\n2\nimage, while the CityPersons has \u223c6 persons per image.\nIn these datasets, the crowd scenes are signi\ufb01cantly under-\nrepresented. Even worse, protocols of these datasets allow\nannotators to ignore and discard the regions with a large\nnumber of persons as exhaustively annotating crowd re-\ngions is incredibly dif\ufb01cult and time consuming.\nHuman detection frameworks. Traditional human detec-\ntors, such as ACF [4], LDCF [19], and Checkerboard [32],\nexploit various \ufb01lters based on Integral Channel Features\n(IDF) [5] with sliding window strategy.\nRecently, the CNN-based detectors have become a\npredominating trend in the \ufb01eld of pedestrian detection.\nIn [29], self-learned features are extracted from deep neu-\nral networks and a boosted decision forest is used to detect\npedestrians. Cai et al. [1] propose an architecture which\nuses different levels of features to detect persons at various\nscales. Mao et al. [18] propose a multi-task network to fur-\nther improve detection performance. Hosang et al. [9] pro-\npose a learning method to improve the robustness of NMS.\nPart-based models are utilized in [20, 33] to alleviate occlu-\nsion problem. Repulsion loss is proposed to detect persons\nin crowd scenes [26].\n3. CrowdHuman Dataset\nIn this section, we describe our CrowdHuman dataset in-\ncluding the collection process, annotation protocols, and in-\nformative statistics.\n3.1. Data Collection\nWe would like our dataset to be diverse for real world\nscenarios.\nThus, we crawl images from Google image\nsearch engine with \u223c150 keywords for query.\nExem-\nplary keywords include \u201cPedestrians on the Fifth Avenue\u201d,\n\u201cpeople crossing the roads\u201d, \u201cstudents playing basketball\u201d\nand \u201cfriends at a party\u201d.\nThese keywords cover more\nthan 40 different cities around the world, various activities\n(e.g., party, traveling, and sports), and numerous viewpoints\n(e.g., surveillance viewpoint and horizontal viewpoint). The\nnumber of images crawled from a keyword is limited to\n500 to make the distribution of images balanced. We crawl\n\u223c60, 000 candidate images in total. The images with only\na small number of persons, or with small overlaps between\npersons, are \ufb01ltered.\nFinally, \u223c25, 000 images are col-\nlected in the CrowdHuman dataset. We randomly select\n15, 000, 4, 370 and 5, 000 images for training, validation,\nand testing, respectively.\n3.2. Image Annotation\nWe annotate individual persons in the following steps.\n\u2022 We annotate a full bounding box of each individual ex-\nhaustively. If the individual is partly occluded, the an-\nnotator is required to complete the invisible part and\ndraw a full bounding box. Different from the existing\ndatasets like CityPersons, where the bounding boxes\nannotated are generated via drawing a line from top of\nthe head and the middle of feet with a \ufb01xed aspect ra-\ntio (0.41), our annotation protocol is more \ufb02exible in\nreal world scenarios which have various human poses.\nWe also provide bounding boxes for human-like ob-\njects, e.g., statue, with a speci\ufb01c label. Following the\nmetrics of [6], these bounding-boxes will be ignored\nduring evaluation.\n\u2022 We crop each annotated instance from the images, and\nsend these cropped regions for annotators to draw a\nvisible bounding box.\n\u2022 We further send the cropped regions to annotate a head\nbounding box. All the annotations are double-checked\nby at least one different annotator to ensure the anno-\ntation quality.\nFig. 2 shows the three kinds of bounding boxes associated\nwith an individual person as well as an example of anno-\ntated image.\nWe compare our CrowdHuman dataset with previous\ndatasets in terms of annotation types in Table 1.\nBe-\nsides from the popular pedestrian detection datasets, we\nalso include the COCO [17] dataset with only a \u201cperson\u201d\nclass. Compared with CrowdHuman, which provides vari-\nous types of annotations, Caltech and CityPersons have only\nnormalized full bounding boxes and visible boxes, KITTI\nhas only full bounding boxes, and COCOPersons has only\nvisible bounding boxes. More importantly, none of them\nhas head bounding boxes associated with each individual\nperson, which may serve as a possible means to address the\ncrowd occlusion problem.\n3.3. Dataset Statistics\nDataset Size.\nThe volume of the CrowdHuman training\nsubset is illustrated in the \ufb01rst three lines of Table 2. In\na total of 15, 000 images, there are \u223c340k person and \u223c\n99k ignore region annotations in the CrowdHuman training\nsubset. The number is more than 10x boosted compared\nwith previous challenging pedestrian detection dataset like\nCityPersons. The total number of persons is also noticeably\nlarger than the others.\nDensity.\nIn terms of density, on average there are \u223c22.6\npersons per image in CrowdHuman dataset, as shown in\nthe fourth line of Table 2. We also report the density from\nthe existing datasets in Table 3. Obviously, CrowdHuman\ndataset is of much higher crowdness compared with all pre-\nvious datasets. Caltech and KITTI suffer from extremely\nlow-density, for that on average there is only \u223c1 person per\n3\nHead BBox\nVisible BBox\nFull BBox\nHead BBox\nVisible BBox\nFull BBox\n(a)\n(b)\nFigure 2. (a) provides an illustrative example of our three kinds of annotations: Head Bounding-Box, Visible Bounding-Box, and Full\nBounding-Box. (b) is an example image with our human annotations where magenta mask illustrates the ignored region.\nCaltech\nKITTI\nCityPersons\nCOCOPersons\nCrowdHuman\nFull BBox\n\u2713\n\u2713\n\u2713\u2020\n\u00d7\n\u2713\nVisible BBox\n\u2713\n\u00d7\n\u2713\n\u2713\n\u2713\nHead BBox\n\u00d7\n\u00d7\n\u00d7\n\u00d7\n\u2713\nTable 1. Comparison of different annotation types for the popular human detection benchmarks.\u2020 : Aligned to a certain ratio.\nCaltech\nKITTI\nCityPersons\nCOCOPersons\nCrowdHuman\n# images\n42, 782\n3, 712\n2, 975\n64, 115\n15, 000\n# persons\n13, 674\n2, 322\n19, 238\n257, 252\n339, 565\n# ignore regions\n50, 363\n45\n6, 768\n5, 206\n99, 227\n# person/image\n0.32\n0.63\n6.47\n4.01\n22.64\n# unique persons\n1, 273\n< 2, 322\n19, 238\n257, 252\n339, 565\nTable 2. Volume, density and diversity of different human detection datasets. For fair comparison, we only show the statistics of training\nsubset.\nimage. The number in CityPersons reaches \u223c7, a signi\ufb01-\ncant boost while still not dense enough. As for COCOPer-\nsons, although its volume is relatively large, it is insuf\ufb01cient\nto serve as a ideal benchmark for the challenging crowd\nscenes. Thanks to the pre-\ufb01ltering and annotation proto-\ncol of our dataset, CrowdHuman can reach a much better\ndensity.\nDiversity.\nDiversity is an important factor of a dataset.\nCOCOPersons and CrowdHuman contain people in unlim-\nited poses in a wide range of domains, while Caltech, KITTI\nand CityPersons are all recorded by a car traversing on\nstreets. The number of identical persons is also critical. As\nreported in the \ufb01fth line in Table 2, this number amounts to\n\u223c33k in CrowdHuman while images in Caltech and KITTI\nare not sparsely sampled, resulting in less amount of identi-\ncal persons.\nOcclusion.\nTo better analyze the distribution of occlu-\nsion levels, we divide the dataset into the \u201cbare\u201d sub-\nset (occlusion \u226430%), the \u201cpartial\u201d subset (30% <\nocclusion \u226470%), and the \u201cheavy\u201d subset (occlusion >\n70%). In Fig. 3, we compare the distribution of persons at\ndifferent occlusion levels for CityPersons2. The bare sub-\nset and partial subset in CityPersons constitute 46.79% and\n24.19% of entire dataset respectively, while the ratios for\nCrowdHuman are 29.89% and 32.13%. The occlusion lev-\nels are more balanced in CrowdHuman, in contrary to those\nin CityPersons, which have more persons with low occlu-\nsion.\nWe also provide statistics on pair-wise occlusion. For\neach image, We count the number of person pairs with dif-\nferent intersection over union (IoU) threshold. The results\nare shown in Table 4. In average, few person pairs with\nan IoU threshold of 0.3 are included in Caltech, KITTI\nor COCOPersons. For CityPersons dataset, the number is\nless than one pair per image. However, the number is 9\nfor CrowdHuman. Moreover, There are averagely 2.4 pairs\nwhose IoU is greater than 0.5 in the CrowdHuman dataset.\n2The statistics is computed without group people\n4\nWe further count the occlusion levels for triples of persons.\nAs shown in Table 5, such cases can be hardly found in pre-\nvious datasets, while they are well-represented in Crowd-\nHuman.\n4. Experiments\nIn this section, we will \ufb01rst discuss the experiments on\nour CrowdHuman dataset, including full body detection,\nvisible body detection and head detection.\nMeanwhile,\nthe generalization ability of our CrowdHuman dataset will\nbe evaluated on standard pedestrian benchmarks like Cal-\ntech and CityPersons, person detection benchmark on CO-\nCOPersons, and head detection benchmark on Brainwash\ndataset. We use FPN [15] and RetinaNet [16] as two base-\nline detectors to represent the two-stage algorithms and one-\nstage algorithms, respectively.\n4.1. Baseline Detectors\nOur baseline detectors are Faster R-CNN [21] and Reti-\nnaNet [16], both based on the Feature Pyramid Network\n(FPN) [15] with a ResNet-50 [8] back-bone network. Faster\nR-CNN and RetinaNet are both proposed for general object\ndetection, and they have dominated the \ufb01eld of object de-\ntection in recent years.\n4.2. Evaluation Metric\nThe training and validation subsets of CrowdHuman can\nbe downloaded from our website. In the following exper-\niments, our algorithms are trained based on CrowdHuman\ntrain subset and the results are evaluated in the validation\nsubset. An online evaluation server will help to evaluate the\nperformance of the testing subset and a leaderboard will be\nmaintained. The annotations of testing subset will not be\nmade publicly available.\nWe follow the evaluation metric used for Caltech [6], de-\nnoted as mMR, which is the average log miss rate over false\npositives per-image ranging in\n\u0002\n10\u22122, 100\u0003\n. mMR is a good\nindicator for the algorithms applied in the real world appli-\ncations. Results on ignored regions will not considered in\nthe evaluation. Besides, Average Precision (AP) and recall\nof the algorithms are included for reference.\n4.3. Implementation Details\nWe use the same setting of anchor scales as [15]\nand [16]. For all the experiments related to full body de-\ntection, we modify the height v.s.\nwidth ratios of an-\nchors as {1 : 1, 1.5 : 1, 2 : 1, 2.5 : 1, 3 : 1} in consideration\nof the human body shape.\nWhile for visible body de-\ntection and human head detection, the ratios are set to\n{1 : 2, 1 : 1, 2 : 1}, in comparison with the original papers.\nThe input image sizes of Caltech and CityPersons are set to\n2\u00d7 and 1\u00d7 of the original images according to [31]. As the\nimages of CrowdHuman and MSCOCO are both collected\nfrom the Internet with various sizes, we resize the input so\nthat their short edge is at 800 pixels while the long edge\nshould be no more than 1400 pixels at the same time. The\ninput sizes of Brainwash is set as 640 \u00d7 480.\nWe train all datasets with 600k and 750k iterations for\nFPN and RetinaNet, respectively. The base learning rate\nis set to 0.02 and decreased by a factor of 10 after 150k\nand 450k for FPN, and 180k and 560k for RetinaNet. The\nStochastic Gradient Descent (SGD) solver is adopted to op-\ntimize the networks on 8 GPUs. A mini-batch involves 2\nimages per GPU, except for CityPersons where a mini-batch\ninvolves only 1 image due to the physical limitation of GPU\nmemory. Weight decay and momentum are set to 0.0001\nand 0.9. We do not \ufb01netune the batch normalization [11]\nlayers. Multi-scale training/testing are not applied to ensure\nfair comparisons.\n4.4. Detection results on CrowdHuman\nVisible Body Detection As the human have different poses\nand occlusion conditions, the visible regions may be quite\ndifferent for each individual person, which brings many dif-\n\ufb01culties to human detection. Table 6 illustrates the results\nfor the visible part detection based on FPN and RetinaNet.\nFPN outperforms RetinaNet in this case. According to Ta-\nble 6, the proposed CrowdHuman dataset is a challenging\nbenchmark, especially for the state-of-the-art human detec-\ntion algorithms. The illustrative examples of visible body\ndetection based on FPN are shown in Fig. 5.\nFull Body Detection Detecting full body regions is more\ndif\ufb01cult than detecting the visible part as the detectors\nshould predict the occluded boundaries of the full body. To\nmake matters worse, the ground-truth annotation might be\nsuffered from high variance caused by different decision-\nmakings by different annotators.\nDifferent from the visible part detection, the aspect ra-\ntios of the anchors for the full body detection are set as\n[1.0, 1.5, 2.0, 2.5, 3.0] to make the detector tend to predict\nthe slim and tall bounding boxes. Another important thing\nis that the RoIs are not clipped into the limitation of the im-\nage boundaries, as there are many full body bounding boxes\nextended out of images. The results are shown in Table 7\nand the illustrative examples of FPN are shown in Fig. 4.\nSimilar to the Visible body detection, FPN has a signi\ufb01cant\ngain over RetinaNet.\nIn Table 7, we also report the FPN pedestrian detec-\ntion results3 on Caltech, i.e., 10.08 mMR, and CityPersons,\ni.e., 14.81 mMR. It shows that our CrowdHuman dataset\nis much challenging than the standard pedestrian detection\nbenchmarks based on the detection performance.\n3The results are evaluated on the standard reasonable set\n5\nperson/img \u2265\nCaltech\nKITTI\nCityPersons\nCOCOPersons\nCrowdHuman\n1\n7839\n18.3%\n969\n26.1%\n2482\n83.4%\n64115\n100.0%\n15000\n100.0%\n2\n3257\n7.6%\n370\n10.0%\n2082\n70.0%\n39283\n61.3%\n15000\n100.0%\n3\n1265\n3.0%\n273\n7.4%\n1741\n58.5%\n28553\n44.5%\n14996\n100.0%\n5\n282\n0.7%\n164\n4.4%\n1225\n41.2%\n18775\n29.3%\n14220\n94.8%\n10\n36\n0.1%\n19\n0.5%\n610\n20.5%\n9604\n15.0%\n10844\n72.3%\n20\n0\n0.0%\n0\n0.0%\n227\n7.6%\n0\n0.0%\n5907\n39.4%\n30\n0\n0.0%\n0\n0.0%\n94\n3.2%\n0\n0.0%\n3294\n21.9%\nTable 3. Comparison of the human density against the widely used human detection dataset. The \ufb01rst column refers to the number of\nhuman instances in the image.\n0.0%\n2.5%\n5.0%\n7.5%\n10.0%\n12.5%\n15.0%\n17.5%\n20.0%\n22.5%\n(0,0.1]\n(0.1,0.2]\n(0.2,0.3]\n(0.3,0.4]\n(0.4,0.5]\n(0.5,0.6]\n(0.6,0.7]\n(0.7,0.8]\n(0.8,0.9]\n(0.9,1.0]\n% Proportion\nVisible Ratio\nCrowdHuman\nCityPersons\nFigure 3. Comparison of the visible ratio between our CrowdHuman and CityPersons dataset. Visible Ratio is de\ufb01ned as the ratio of visible\nbounding box to the full bounding box.\npair/img\nCal\nCity\nCOCO\nCrowdHuman\niou>0.3\n0.06\n0.96\n0.13\n9.02\niou>0.4\n0.03\n0.58\n0.05\n4.89\niou>0.5\n0.02\n0.32\n0.02\n2.40\niou>0.6\n0.01\n0.17\n0.01\n1.01\niou>0.7\n0.00\n0.08\n0.00\n0.33\niou>0.8\n0.00\n0.02\n0.00\n0.07\niou>0.9\n0.00\n0.00\n0.00\n0.01\nTable 4. Comparison of pair-wise overlap between two human in-\nstances.\nHead Detection Head is one of the most obvious parts of\na whole body. Head detection is widely used in the practi-\ncal applications such as people number counting, face de-\ntection and tracking. We compare the results of FPN and\nRetinaNet as shown in Table 8. The illustrative examples of\nhead detection on CrowdHuman by FPN detector are shown\nin Fig. 6.\npair/img\nCal\nCity\nCOCO\nCrowdHuman\niou>0.1\n0.02\n0.30\n0.02\n8.70\niou>0.2\n0.00\n0.11\n0.00\n2.09\niou>0.3\n0.00\n0.04\n0.00\n0.51\niou>0.4\n0.00\n0.01\n0.00\n0.12\niou>0.5\n0.00\n0.00\n0.00\n0.03\nTable 5. Comparison of high-order overlaps among three human\ninstances.\nTable 6. Evaluation of visible body detection on CrowdHuman\nbenchmark.\nRecall\nAP\nmMR\nFPN [15]\n91.51\n85.60\n55.94\nRetinaNet [16]\n90.96\n77.19\n65.47\n4.5. Cross-dataset Evaluation\nAs shown in Section 3, the size of CrowdHuman dataset\nis obviously larger than the existing benchmarks, like Cal-\ntech and CityPersons. In this section, we evaluate that the\n6\nFigure 4. Qualitative results for the full body detection of FPN based on CrowdHuman dataset.\nFigure 5. Qualitative results for the visible body detection of FPN based on CrowdHuman dataset.\nTable 7. Evaluation of full body detection on CrowdHuman bench-\nmark.\nRecall\nAP\nmMR\nFPN [15]\n90.24\n84.95\n50.42\nRetinaNet [16]\n93.80\n80.83\n63.33\nFPN on Caltech\n99.76\n89.95\n10.08\nFPN on CityPersons\n97.97\n94.35\n14.81\nTable 8. Evaluation of Head detection on CrowdHuman bench-\nmark.\nRecall\nAP\nmMR\nFPN [15]\n81.10\n77.95\n52.06\nRetinaNet [16]\n78.43\n71.36\n60.64\ngeneralization ability of our CrowdHuman dataset. More\nspeci\ufb01cally, we \ufb01rst train the model on our CrowdHuman\ndataset and then \ufb01netune it on the visible body detection\nbenchmarks like COCOPersons [17], full body detection\nbenchmarks like Caltech [6] and CityPersons [31], and head\ndetection benchmarks like Brainwash [23]. As reported in\nSection 4.4, FPN is superior to RetinaNet in all three cases.\nTherefore, in the following experiments, we adopt FPN as\nour baseline detector.\nCOCOPersons COCOPersons is a subset of MSCOCO\nfrom the images with groundtruth bounding box of \u201cper-\nson\u201d. The other 79 classes are ignored in our evaluation.\nAfter the \ufb01ltering process, there are 64115 images from the\ntrainval minus minival for training, and the other 2639 im-\nages from minival for validation. All the persons in CO-\nCOPersons are annotated as the visible body with different\ntype of human poses. The results are illustrated in Table 9.\nBased on the pretraining of our CrowdHuman dataset, our\nalgorithm has superior performance on the COCOPersons\nTable 9. Experimental results on COCOPersons.\nTrain-set\nRecall\nAP\nmMR\nCOCOPersons\n95.57\n83.83\n41.89\nCrowd\u21d2COCO\n95.87\n85.02 39.79\nbenchmark against the one without CrowdHuman pretrain-\ning.\nCaltech and CityPersons Caltech and CityPersons are\nwidely used benchmarks for pedestrian detection, both of\nthem are usually adopted to evaluate full body detection\nalgorithms. We use the reasonable set for Caltech dataset\nwhere the object size is larger than 50 pixels. Table 10 and\nTable 11 show the results on Caltech and CityPersons, re-\nspectively. We compare the algorithms in the \ufb01rst part of\nthe tables with:\n\u2022 FPN trained on the Caltech\n\u2022 FPN trained on CityPersons\n\u2022 FPN trained on CrowdHuman\n\u2022 FPN model pretrained on CrowdHuman and then \ufb01ne-\ntuned on the corresponding target training set\nAlso, state-of-art algorithms on Caltech and CityPersons are\nreported in the second part of tables as well. To summarize,\nthe results illustrated in Table 10 and Table 11 demonstrate\nthat our CrowdHuman dataset can serve as an effective pre-\ntraining dataset for pedestrian detection task on Caltech and\nCityPersons 4 for full body detection.\nBrainwash Brainwash [23] is a head detection dataset\nwhose images are extracted from the video footage at ev-\nery 100 seconds. Following the step of [23], the training\n4The evaluation is based on 1\u00d7 scale.\n7\nFigure 6. Qualitative results for the head detection of FPN based on CrowdHuman dataset.\nTable 10. Experimental results on Caltech dataset.\nTrain-set\nRecall AP mMR\nCaltech\n99.76 89.95 10.08\nCityPersons\n99.05 85.81 14.69\nCrowdHuman\n99.88 90.58 8.81\nCrowd\u21d2Calt\n99.88 95.69 3.46\nCityPersons\u21d2Calt [31]\n-\n-\n5.1\nRepulsion [26]\n-\n-\n4.0\n[18]\n-\n-\n5.5\nTable 11. Experimental reslts on CityPersons.\nTrain-set\nRecall AP\nmMR\nCaltech\n87.21 65.87 45.52\nCityPersons\n97.97 94.35 14.81\nCrowdHuman\n98.73 98.10 21.18\nCrowd\u21d2City\n97.78 95.58 10.67\nCityPersons [31]\n-\n-\n14.8\nRepulsion [26]\n-\n-\n13.2\nTable 12. Experimental results on Brainwash.\nTrain-set\nRecall\nAP\nmMR\nBrainwash\n98.52\n95.74\n19.77\nCrowd\u21d2Brain\n98.66\n96.15 17.24\n[23]\n-\n78.0\n-\nset has 10,917 images with 82,906 instances and the vali-\ndation set has 500 images with 3318 instances. Similar to\nvisible body detection and full body detection, Brainwash\ndataset is evaluated to validate the generalization ability of\nour CrowdHuman dataset for head detection.\nTable 12 shows the results of head detection task on\nBrainwash dataset. By using the FPN as the head detector,\nthe performance is already much better than the state-of-\nart in [23]. On top of that, pretraining on the CrowdHuman\ndataset further boost the result by 2.5% of mMR, which val-\nidates the generalization ability of our CrowdHuman dataset\nfor head detection.\n5. Conclusion\nIn this paper, we present a new human detection bench-\nmark designed to address the crowd problem. There are\nthree contributions of our proposed CrowdHuman dataset.\nFirstly, compared with the existing human detection bench-\nmark, the proposed dataset is larger-scale with much higher\ncrowdness. Secondly, the full body bounding box, the visi-\nble bounding box, and the head bounding box are annotated\nfor each human instance. The rich annotations enables a\nlot of potential visual algorithms and applications. Last but\nnot least, our CrowdHuman dataset can serve as a powerful\npretraining dataset. State-of-the-art results have been re-\nported on benchmarks of pedestrian detection benchmarks\nlike Caltech and CityPersons, and Head detection bench-\nmark like Brainwash. The dataset as well as the code and\nmodels discussed in the paper will be released 5.\nReferences\n[1] Zhaowei Cai, Quanfu Fan, Rogerio S Feris, and Nuno\nVasconcelos.\nA uni\ufb01ed multi-scale deep convolutional\nneural network for fast object detection.\narXiv preprint\narXiv:1607.07155, 2016. 1, 3\n[2] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld,\nMarkus Enzweiler,\nRodrigo Benenson,\nUwe\nFranke, Stefan Roth, and Bernt Schiele.\nThe cityscapes\ndataset for semantic urban scene understanding. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 3213\u20133223, 2016. 2\n[3] Navneet Dalal and Bill Triggs. Histograms of oriented gra-\ndients for human detection. In Computer Vision and Pat-\ntern Recognition, 2005. CVPR 2005. IEEE Computer Soci-\nety Conference on, volume 1, pages 886\u2013893. IEEE, 2005.\n2\n[4] Piotr Doll\u00b4ar, Ron Appel, Serge Belongie, and Pietro Per-\nona.\nFast feature pyramids for object detection.\nIEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n36(8):1532\u20131545, 2014. 1, 3\n[5] Piotr Doll\u00b4ar, Zhuowen Tu, Pietro Perona, and Serge Be-\nlongie. Integral channel features. 2009. 1, 3\n[6] Piotr Doll\u00b4ar, Christian Wojek, Bernt Schiele, and Pietro Per-\nona. Pedestrian detection: A benchmark. In Computer Vision\nand Pattern Recognition, 2009. CVPR 2009. IEEE Confer-\nence on, pages 304\u2013311. IEEE, 2009. 1, 2, 3, 5, 7\n[7] Markus Enzweiler and Dariu M Gavrila. Monocular pedes-\ntrian detection: Survey and experiments. IEEE transactions\non pattern analysis and machine intelligence, 31(12):2179\u2013\n2195, 2009. 2\n5https://sshao0516.github.io/CrowdHuman/\n8\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 1, 5\n[9] Jan Hosang, Rodrigo Benenson, and Bernt Schiele. Learn-\ning non-maximum suppression.\nIn The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), July\n2017. 3\n[10] Jan Hosang, Mohamed Omran, Rodrigo Benenson, and\nBernt Schiele. Taking a deeper look at pedestrians. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 4073\u20134082, 2015. 1\n[11] Sergey Ioffe and Christian Szegedy. Batch normalization:\nAccelerating deep network training by reducing internal co-\nvariate shift. arXiv preprint arXiv:1502.03167, 2015. 5\n[12] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImagenet classi\ufb01cation with deep convolutional neural net-\nworks. In Advances in neural information processing sys-\ntems, pages 1097\u20131105, 2012. 1\n[13] Ian Reid Stefan Roth Konrad Schindler Laura Leal-Taix, An-\nton Milan. Motchallenge 2015: Towards a benchmark for\nmulti-target tracking. 2015. 1\n[14] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, and\nShuicheng Yan. Scale-aware fast r-cnn for pedestrian detec-\ntion. arXiv preprint arXiv:1510.08160, 2015. 1\n[15] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyramid\nnetworks for object detection. In CVPR, volume 1, page 4,\n2017. 5, 6, 7\n[16] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. arXiv\npreprint arXiv:1708.02002, 2017. 5, 6, 7\n[17] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nEuropean Conference on Computer Vision, pages 740\u2013755.\nSpringer, 2014. 1, 2, 3, 6\n[18] Jiayuan Mao, Tete Xiao, Yuning Jiang, and Zhimin Cao.\nWhat can help pedestrian detection? In The IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR),\n2017. 1, 3, 8\n[19] Woonhyun Nam, Piotr Doll\u00b4ar, and Joon Hee Han.\nLo-\ncal decorrelation for improved detection.\narXiv preprint\narXiv:1406.1134, 2014. 3\n[20] Wanli Ouyang and Xiaogang Wang. A discriminative deep\nmodel for pedestrian detection with occlusion handling. In\nComputer Vision and Pattern Recognition (CVPR), 2012\nIEEE Conference on, pages 3258\u20133265. IEEE, 2012. 3\n[21] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster r-cnn: Towards real-time object detection with region\nproposal networks. In Advances in neural information pro-\ncessing systems, pages 91\u201399, 2015. 5\n[22] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014. 1\n[23] Russell Stewart, Mykhaylo Andriluka, and Andrew Y Ng.\nEnd-to-end people detection in crowded scenes. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 2325\u20132333, 2016. 7, 8\n[24] Bochao Wang Liang Lin Xiaogang Wang Tong Xiao,\nShuang Li. Joint detection and identi\ufb01cation feature learn-\ning for person search. In The IEEE Conference on Computer\nVision and Pattern Recognition (CVPR), 2017. 1\n[25] Andreas Geigerand Philip Lenzand Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2012. 1, 2\n[26] Xinlong Wang, Tete Xiao, Yuning Jiang, and Shen Chunhua\nSun, Jian. Repulsion loss: Detecting pedestrians in a crowd.\nIn The IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2018. 1, 3, 8\n[27] Christian Wojek, Stefan Walk, and Bernt Schiele. Multi-cue\nonboard pedestrian detection. In Computer Vision and Pat-\ntern Recognition, 2009. CVPR 2009. IEEE Conference on,\npages 794\u2013801. IEEE, 2009. 2\n[28] Yuxiang\nPeng\nZhiqiang\nZhang\nGang\nYu\nJian\nSun\nYilun Chen, Zhicheng Wang.\nCascaded pyramid network\nfor multi-person pose estimation. In The IEEE Conference\non Computer Vision and Pattern Recognition (CVPR), 2018.\n1\n[29] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He.\nIs faster r-cnn doing well for pedestrian detection?\narXiv\npreprint arXiv:1607.07032, 2016. 1, 3\n[30] Shanshan Zhang, Rodrigo Benenson, Mohamed Omran, Jan\nHosang, and Bernt Schiele. How far are we from solving\npedestrian detection? In IEEE Conference on Computer Vi-\nsion and Pattern Recognition. IEEE Computer Society, 2016.\n1\n[31] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele.\nCitypersons: A diverse dataset for pedestrian detection. 1,\n2, 5, 7, 8\n[32] Shanshan Zhang, Rodrigo Benenson, and Bernt Schiele. Fil-\ntered channel features for pedestrian detection.\nIn 2015\nIEEE Conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 1751\u20131760. IEEE, 2015. 1, 3\n[33] Chunluan Zhou and Junsong Yuan. Multi-label learning of\npart detectors for heavily occluded pedestrian detection. In\nProceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition, pages 3486\u20133495, 2017. 3\n9\n",
    "2003.09003": "1\nMOT20: A benchmark for multi object tracking in\ncrowded scenes\nPatrick Dendorfer, Hamid Rezato\ufb01ghi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid,\nStefan Roth, Konrad Schindler, and Laura Leal-Taix\u00b4e\nAbstract\u2014Standardized benchmarks are crucial for the majority of computer vision applications. Although leaderboards and ranking\ntables should not be over-claimed, benchmarks often provide the most objective measure of performance and are therefore important\nguides for research. The benchmark for Multiple Object Tracking, MOTChallenge, was launched with the goal to establish a standardized\nevaluation of multiple object tracking methods. The challenge focuses on multiple people tracking, since pedestrians are well studied in\nthe tracking community, and precise tracking and detection has high practical relevance. Since the \ufb01rst release, MOT15 [5], MOT16 [8],\nand MOT17 [8] have tremendously contributed to the community by introducing a clean dataset and precise framework to benchmark\nmulti-object trackers. In this paper, we present our MOT20 benchmark, consisting of 8 new sequences depicting very crowded\nchallenging scenes. The benchmark was presented \ufb01rst at the 4th BMTT MOT Challenge Workshop at the Computer Vision and\nPattern Recognition Conference (CVPR) 2019, and gives to chance to evaluate state-of-the-art methods for multiple object tracking\nwhen handling extremely crowded scenarios.\nIndex Terms\u2014multiple people tracking, benchmark, evaluation metrics, dataset\n!\n1\nINTRODUCTION\nSince its \ufb01rst release in 2014, MOTChallenge has attracted\nmore than 1, 000 active users who have successfully\nsubmitted their trackers and detectors to \ufb01ve different\nchallenges, spanning 44 sequences with 2.7M bounding\nboxes over a total length of 36k seconds. As evaluating\nand comparing multi-target tracking methods is not\ntrivial (cf. e.g. [10]), MOTChallenge provides carefully\nannotated datasets and clear metrics to evaluate the per-\nformance of tracking algorithms and pedestrian detec-\ntors. Parallel to the MOTChallenge all-year challenges, we\norganize workshop challenges on multi-object tracking\nfor which we often introduce new data.\nIn this paper, we introduce the MOT20 benchmark,\nconsisting of 8 novel sequences out of 3 very crowded\nscenes. All sequences have been carefully selected and\nannotated according to the evaluation protocol of pre-\nvious challenges [5], [8]. This benchmark addresses the\nchallenge of very crowded scenes in which the density\ncan reach values of 246 pedestrians per frame. The\nP. Dendorfer and L. Leal-Taix\u00b4e are with the Dynamic Vision and Learning\nGroup at TUM Munich, Germany.\nH. Rezato\ufb01ghi, J. Shi, and I. Reid are with the Australian Institute for\nMachine Learning and the School of Computer Science at University of\nAdelaide.\nA. Milan is with Amazon, Berlin, Germany. This work was done prior to\njoining Amazon.\nK. Schindler is with the Photogrammetry and Remote Sensing Group at ETH\nZurich, Switzerland.\nD. Cremers is with the Computer Vision Group at TUM Munich, Germany.\nS. Roth is with the Department of Computer Science, Technische Universit\u00a8at\nDarmstadt, Germany.\nPrimary contacts: patrick.dendorfer@tum.de, leal.taixe@tum.de\nsequences were \ufb01lmed in both indoor and outdoor lo-\ncations, and include day and night time shots. Figure 1\nshows the split of the sequences of the three scenes into\ntraining and testing sets. The testing data consists of\nsequences from known as well as from an unknown\nscenes in order to measure the genralization capabilities\nof both detectors and trackers. We make available the\nimages for all sequences, the ground truth annotations\nfor the training set as well as a set of public detections\n(obtained from a Faster R-CNN trained on the training\ndata) for the tracking challenge.\nThe MOT20 challenges and all data, current ranking\nand submission guidelines can be found at:\nhttp://www.motchallenge.net/\n2\nANNOTATION RULES\nFor the annotation of the dataset, we follow the protocol\nintroduced in MOT16, ensuring that every moving per-\nson or vehicle within each sequence is annotated with a\nbounding box as accurately as possible. In the following,\nwe de\ufb01ne a clear protocol that was obeyed throughout\nthe entire dataset to guarantee consistency.\n2.1\nTarget class\nIn this benchmark, we are interested in tracking moving\nobjects in videos. In particular, we are interested in\nevaluating multiple people tracking algorithms, hence,\npeople will be the center of attention of our annotations.\nWe divide the pertinent classes into three categories:\narXiv:2003.09003v1  [cs.CV]  19 Mar 2020\n2\nFig. 1: An overview of the MOT20 dataset. The dataset consists of 8 different sequences from 3 different scenes.\nThe test dataset has two known and one unknown scene. Top: training sequences; bottom: test sequences.\nFig. 2: we provide for the challenges. Left: Image of each frame of the sequences; middle: ground truth labels\nincluding all classes. Only provided for training set; right: public detections from trained Faster R-CNN.\n(i) moving pedestrians;\n(ii) people that are not in an upright position, not moving,\nor arti\ufb01cial representations of humans; and\n(iii) vehicles and occluders.\nIn the \ufb01rst group, we annotate all moving pedestrians\nthat appear in the \ufb01eld of view and can be determined\nas such by the viewer. Furthermore, if a person brie\ufb02y\nbends over or squats, e.g., to pick something up or to talk\nto a child, they shall remain in the standard pedestrian\nclass. The algorithms that submit to our benchmark are\nexpected to track these targets.\nIn the second group, we include all people-like objects\nwhose exact classi\ufb01cation is ambiguous and can vary de-\npending on the viewer, the application at hand, or other\nfactors. We annotate all static people, e.g., sitting, lying\ndown, or do stand still at the same place over the whole\nsequence. The idea is to use these annotations in the\nevaluation such that an algorithm is neither penalized\nnor rewarded for tracking, e.g., a sitting or not moving\nperson.\nIn the third group, we annotate all moving vehicles\nsuch as cars, bicycles, motorbikes and non-motorized\nvehicles (e.g., strollers), as well as other potential oc-\ncluders. These annotations will not play any role in\nthe evaluation, but are provided to the users both for\ntraining purposes and for computing the level of occlu-\nsion of pedestrians. Static vehicles (parked cars, bicycles)\nare not annotated as long as they do not occlude any\npedestrians.\n3\nDATASETS\nThe dataset for the new benchmark has been carefully\nselected to challenge trackers and detectors on extremely\ncrowded scenes. In contrast to previous challenges, some\nof the new sequences show a pedestrian density of 246\npedestrians per frame. In Fig. 1 and Tab. 1, we show an\noverview of the sequences included in the benchmark.\n3.1\nMOT 20 sequences\nWe have compiled a total of 8 sequences, of which we\nuse half for training and half for testing. The annotations\nof the testing sequences will not be released in order\nto avoid (over)\ufb01tting of the methods to the speci\ufb01c\nsequences. The sequences are \ufb01lmed in three different\nscenes. Several sequences are \ufb01lmed per scene and dis-\ntributed in the train and test sets. One of the scenes\nthough, is reserved for test time, in order to challenge\nthe generalization capabilities of the methods.\nThe new data contains circa 3 times more bounding\nboxes for training and testing compared to MOT17.\nAll sequences are \ufb01lmed in high resolution from an\nelevated viewpoint, and the mean crowd density reaches\n246 pedestrians per frame which 10 times denser when\ncompared to the \ufb01rst benchmark release. Hence, we\nexpect the new sequences to be more challenging for\nthe tracking community and to push the models to their\nlimits when it comes to handling extremely crowded\nscenes. In Tab. 1, we give an overview of the training\nand testing sequence characteristics for the challenge,\nincluding the number of bounding boxes annotated.\nAside from pedestrians, the annotations also include\nother classes like vehicles or bicycles, as detailed in\nSec. 2. In Tab. 2, we detail the types of annotations that\ncan be found in each sequence of MOT20.\n3\nTraining sequences\nName\nFPS\nResolution\nLength\nTracks\nBoxes\nDensity\nDescription\nSource\nMOT20-01\n25\n1920x1080\n429 (00:17)\n74\n19,870\n46.32\nindoor\nnew\nMOT20-02\n25\n1920x1080\n2,782 (01:51)\n270\n154,742\n55.62\nindoor\nnew\nMOT20-03\n25\n1173x880\n2,405 (01:36)\n702\n313,658\n130.42\noutdoor, night\nnew\nMOT20-05\n25\n1654x1080\n3,315 (02:13)\n1169\n646,344\n194.98\noutdoor, night\nnew\nTotal training\n8,931 (05:57)\n2,215\n1,134,614\n127.04\nTesting sequences\nName\nFPS\nResolution\nLength\nTracks\nBoxes\nDensity\nDescription\nSource\nMOT20-04\n25\n1545x1080\n2,080 (01:23)\n669\n274,084\n131.77\noutdoor, night\nnew\nMOT20-06\n25\n1920x734\n1,008 (00:40)\n271\n132,757\n131.70\noutdoor, day\nnew\nMOT20-07\n25\n1920x1080\n585 (00:23)\n111\n33,101\n56.58\nindoor\nnew\nMOT20-08\n25\n1920x734\n806 (00:32)\n191\n77,484\n96.13\noutdoor, day\nnew\nTotal training\n4,479 (02:58)\n1,242\n517,426\n115.52\nTABLE 1: Overview of the sequences currently included in the MOT20 benchmark considering pedestrians.\nAnnotation classes\nSequence\nPedestrian\nNon motorized vehicle\nStatic person\nOccluder on the ground\ncrowd\nTotal\nMOT20-01\n19,870\n0\n2,574\n4,203\n0\n26,647\nMOT20-02\n154,742\n4,021\n11,128\n32,324\n0\n202,215\nMOT20-03\n313,658\n1,436\n22,310\n16,835\n2,489\n356,728\nMOT20-04\n274,084\n3,110\n92,251\n2,080\n0\n371,525\nMOT20-05\n646,344\n7,513\n90,843\n6,630\n0\n751,330\nMOT20-06\n132,757\n1,248\n60,102\n12,428\n1,008\n207,543\nMOT20-07\n33,101\n800\n3,685\n3,510\n0\n41,096\nMOT20-08\n77,484\n4,237\n52,998\n9,776\n806\n145,301\nTotal\n1,652,040\n22,365\n335,891\n87,786\n4,303\n2,102,385\nTABLE 2: Overview of the types of annotations currently found in the MOT20 benchmark.\n3.2\nDetections\nWe trained a Faster R-CNN [11] with ResNet101 [4]\nbackbone on the MOT20 training sequences, obtaining\nthe detection results presented in Table 6. This evaluation\nfollows the standard protocol for the MOT20 challenge\nand only accounts for pedestrians. Static persons and\nother classes are not considered and \ufb01ltered out from\nboth, the detections, as well as the ground truth.\nA detailed breakdown of detection bounding boxes on\nindividual sequences is provided in Tab. 3.\nSeq\nnDet.\nnDet./fr.\nmin height\nmax height\nMOT20-01\n12610\n29.39\n30\n289\nMOT20-02\n89837\n32.29\n25\n463\nMOT20-03\n177347\n73.74\n16\n161\nMOT20-04\n228298\n109.76\n23\n241\nMOT20-05\n381349\n115.04\n26\n245\nMOT20-06\n69467\n68.92\n27\n304\nMOT20-07\n20330\n34.75\n21\n381\nMOT20-08\n43703\n54.22\n37\n302\nTotal\n1022941\n76.28\n16\n463\nTABLE 3: Detection bounding box statistics.\nFor the tracking challenge, we provide these public\ndetections as a baseline to be used for training and\ntesting of the trackers. For the MOT20 challenge, we will\nonly accept results on public detections. When later the\nbenchmark will be open for continuous submissions, we\nwill accept both public as well as private detections.\n3.3\nData format\nAll images were converted to JPEG and named sequen-\ntially to a 6-digit \ufb01le name (e.g. 000001.jpg). Detection\nand annotation \ufb01les are simple comma-separated value\n(CSV) \ufb01les. Each line represents one object instance and\ncontains 9 values as shown in Tab. 4.\nThe \ufb01rst number indicates in which frame the object\nappears, while the second number identi\ufb01es that object\nas belonging to a trajectory by assigning a unique ID (set\nto \u22121 in a detection \ufb01le, as no ID is assigned yet). Each\nobject can be assigned to only one trajectory. The next\nfour numbers indicate the position of the bounding box\nof the pedestrian in 2D image coordinates. The position\nis indicated by the top-left corner as well as width\nand height of the bounding box. This is followed by a\nsingle number, which in case of detections denotes their\ncon\ufb01dence score. The last two numbers for detection \ufb01les\nare ignored (set to -1).\nAn example of such a 2D detection \ufb01le is:\n1, -1, 794.2, 47.5, 71.2, 174.8, 67.5, -1, -1\n1, -1, 164.1, 19.6, 66.5, 163.2, 29.4, -1, -1\n1, -1, 875.4, 39.9, 25.3, 145.0, 19.6, -1, -1\n2, -1, 781.7, 25.1, 69.2, 170.2, 58.1, -1, -1\nFor the ground truth and results \ufb01les, the 7th value\n(con\ufb01dence score) acts as a \ufb02ag whether the entry is to\nbe considered. A value of 0 means that this particular\ninstance is ignored in the evaluation, while a value of\n1 is used to mark it as active. The 8th number indicates\nthe type of object annotated, following the convention of\nTab. 5. The last number shows the visibility ratio of each\n4\nPosition\nName\nDescription\n1\nFrame number\nIndicate at which frame the object is present\n2\nIdentity number\nEach pedestrian trajectory is identi\ufb01ed by a unique ID (\u22121 for detections)\n3\nBounding box left\nCoordinate of the top-left corner of the pedestrian bounding box\n4\nBounding box top\nCoordinate of the top-left corner of the pedestrian bounding box\n5\nBounding box width\nWidth in pixels of the pedestrian bounding box\n6\nBounding box height\nHeight in pixels of the pedestrian bounding box\n7\nCon\ufb01dence score\nDET:\nIndicates\nhow\ncon\ufb01dent\nthe\ndetector\nis\nthat\nthis\ninstance\nis\na\npedestrian.\nGT: It acts as a \ufb02ag whether the entry is to be considered (1) or ignored (0).\n8\nClass\nGT: Indicates the type of object annotated\n9\nVisibility\nGT: Visibility ratio, a number between 0 and 1 that says how much of that object is visible. Can be due\nto occlusion and due to image border cropping.\nTABLE 4: Data format for the input and output \ufb01les, both for detection (DET) and annotation/ground truth (GT)\n\ufb01les.\nLabel\nID\nPedestrian\n1\nPerson on vehicle\n2\nCar\n3\nBicycle\n4\nMotorbike\n5\nNon motorized vehicle\n6\nStatic person\n7\nDistractor\n8\nOccluder\n9\nOccluder on the ground\n10\nOccluder full\n11\nRe\ufb02ection\n12\nCrowd\n13\nTABLE 5: Label classes present in the annotation \ufb01les and\nID appearing in the 8th column of the \ufb01les as described\nin Tab. 4.\nbounding box. This can be due to occlusion by another\nstatic or moving object, or due to image border cropping.\nAn example of such an annotation 2D \ufb01le is:\n1, 1, 794.2, 47.5, 71.2, 174.8, 1, 1, 0.8\n1, 2, 164.1, 19.6, 66.5, 163.2, 1, 1, 0.5\n2, 4, 781.7, 25.1, 69.2, 170.2, 0, 12, 1.\nIn this case, there are 2 pedestrians in the \ufb01rst frame\nof the sequence, with identity tags 1, 2. In the second\nframe, we can see a static person (class 7), which is to\nbe considered by the evaluation script and will neither\ncount as a false negative, nor as a true positive, inde-\npendent of whether it is correctly recovered or not. Note\nthat all values including the bounding box are 1-based,\ni.e. the top left corner corresponds to (1, 1).\nTo obtain a valid result for the entire benchmark,\na separate CSV \ufb01le following the format described\nabove must be created for each sequence and called\n\u2018\u2018Sequence-Name.txt\u2019\u2019. All \ufb01les must be com-\npressed into a single ZIP \ufb01le that can then be uploaded\nto be evaluated.\n4\nEVALUATION\nOur framework is a platform for fair comparison of state-\nof-the-art tracking methods. By providing authors with\nstandardized ground truth data, evaluation metrics and\nscripts, as well as a set of precomputed detections, all\nmethods are compared under the exact same conditions,\nthereby isolating the performance of the tracker from\neverything else. In the following paragraphs, we detail\nthe set of evaluation metrics that we provide in our\nbenchmark.\n4.1\nEvaluation metrics\nIn the past, a large number of metrics for quantitative\nevaluation of multiple target tracking have been pro-\nposed [2], [6], [12]\u2013[14], [16]. Choosing \u201cthe right\u201d one\nis largely application dependent and the quest for a\nunique, general evaluation metric is still ongoing. On the\none hand, it is desirable to summarize the performance\ninto one single number to enable a direct comparison. On\nthe other hand, one might not want to lose information\nabout the individual errors made by the algorithms and\nprovide several performance estimates, which precludes\na clear ranking.\nFollowing a recent trend [1], [9], [15], we employ two\nsets of measures that have established themselves in the\nliterature: The CLEAR metrics proposed by Stiefelhagen\net al. [14], and a set of track quality measures introduced\nby Wu and Nevatia [16]. The evaluation scripts used in\nour benchmark are publicly available.1\n4.1.1\nTracker-to-target assignment\nThere are two common prerequisites for quantifying the\nperformance of a tracker. One is to determine for each\nhypothesized output, whether it is a true positive (TP)\nthat describes an actual (annotated) target, or whether\nthe output is a false alarm (or false positive, FP). This\ndecision is typically made by thresholding based on\na de\ufb01ned distance (or dissimilarity) measure d (see\nSec. 4.1.2). A target that is missed by any hypothesis\nis a false negative (FN). A good result is expected to\nhave as few FPs and FNs as possible. Next to the\nabsolute numbers, we also show the false positive ratio\nmeasured by the number of false alarms per frame (FAF),\nsometimes also referred to as false positives per image\n(FPPI) in the object detection literature.\nObviously, it may happen that the same target is\ncovered by multiple outputs. The second prerequisite\nbefore computing the numbers is then to establish the\n1. http://motchallenge.net/devkit\n5\nTraining sequences\nSequence\nAP\nRcll\nPrcn\nFAR\nGT\nTP\nFP\nFN\nMODA\nMODP\nMOT20-01\n0.82\n86.5\n99.5\n0.14\n12945\n11199\n58\n1746\n86.06\n91.61\nMOT20-02\n0.82\n85.9\n99.5\n0.15\n93107\n79971\n421\n13136\n85.44\n92.13\nMOT20-03\n0.54\n59.0\n98.4\n1.10\n278148\n163988\n2653\n114160\n58.00\n86.00\nMOT20-05\n0.63\n64.2\n99.4\n0.60\n528037\n338826\n1979\n189211\n63.79\n87.59\nTesting sequences\nSequence\nAP\nRcll\nPrcn\nFAR\nGT\nTP\nFP\nFN\nMODA\nMODP\nMOT20-04\n0.63\n69.7\n98.0\n1.55\n230729\n160783\n3230\n69946\n68.29\n81.41\nMOT20-06\n0.43\n57.9\n74.4\n12.64\n63889\n37002\n12745\n26887\n37.97\n73.67\nMOT20-07\n0.78\n83.6\n92.5\n1.89\n16298\n13627\n1106\n2671\n76.83\n79.11\nMOT20-08\n0.38\n55.2\n61.6\n13.93\n32608\n17998\n11230\n14610\n20.76\n71.55\nTABLE 6: Overview of performance of Faster R-CNN detector trained on the MOT20 training dataset.\nGT\nTraj.\nID sw.\nID sw.\nFP\nTP FN Tracked\nFrag.\nID sw.\n(a)\n(b)\n(c)\n(d)\nFrag.\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\n1\n2\n3\n4\n5\n6\nframe\nFig. 3: Four cases illustrating tracker-to-target assignments. (a) An ID switch occurs when the mapping switches\nfrom the previously assigned red track to the blue one. (b) A track fragmentation is counted in frame 3 because\nthe target is tracked in frames 1-2, then interrupts, and then reacquires its \u2018tracked\u2019 status at a later point. A new\n(blue) track hypothesis also causes an ID switch at this point. (c) Although the tracking results is reasonably good,\nan optimal single-frame assignment in frame 1 is propagated through the sequence, causing 5 missed targets (FN)\nand 4 false positives (FP). Note that no fragmentations are counted in frames 3 and 6 because tracking of those\ntargets is not resumed at a later point. (d) A degenerate case illustrating that target re-identi\ufb01cation is not handled\ncorrectly. An interrupted ground truth trajectory will cause a fragmentation. Note the less intuitive ID switch, which\nis counted because blue is the closest target in frame 5 that is not in con\ufb02ict with the mapping in frame 4.\ncorrespondence between all annotated and hypothesized\nobjects under the constraint that a true object should be\nrecovered at most once, and that one hypothesis cannot\naccount for more than one target.\nFor the following, we assume that each ground truth\ntrajectory has one unique start and one unique end\npoint, i.e. that it is not fragmented. Note that the current\nevaluation procedure does not explicitly handle target\nre-identi\ufb01cation. In other words, when a target leaves\nthe \ufb01eld-of-view and then reappears, it is treated as an\nunseen target with a new ID. As proposed in [14], the\noptimal matching is found using Munkre\u2019s (a.k.a. Hun-\ngarian) algorithm. However, dealing with video data,\nthis matching is not performed independently for each\nframe, but rather considering a temporal correspon-\ndence. More precisely, if a ground truth object i is\nmatched to hypothesis j at time t \u22121 and the distance\n(or dissimilarity) between i and j in frame t is below\ntd, then the correspondence between i and j is carried\nover to frame t even if there exists another hypothesis\nthat is closer to the actual target. A mismatch error (or\nequivalently an identity switch, IDSW) is counted if a\nground truth target i is matched to track j and the last\nknown assignment was k \u0338= j. Note that this de\ufb01nition\nof ID switches is more similar to [6] and stricter than\nthe original one [14]. Also note that, while it is certainly\ndesirable to keep the number of ID switches low, their\nabsolute number alone is not always expressive to assess\nthe overall performance, but should rather be considered\nin relation to the number of recovered targets. The\nintuition is that a method that \ufb01nds twice as many\ntrajectories will almost certainly produce more identity\nswitches. For that reason, we also state the relative\nnumber of ID switches, i.e., IDSW/Recall.\nThese relationships are illustrated in Fig. 3. For sim-\nplicity, we plot ground truth trajectories with dashed\ncurves, and the tracker output with solid ones, where\nthe color represents a unique target ID. The grey areas\nindicate the matching threshold (see next section). Each\ntrue target that has been successfully recovered in one\nparticular frame is represented with a \ufb01lled black dot\nwith a stroke color corresponding to its matched hypoth-\nesis. False positives and false negatives are plotted as\nempty circles. See \ufb01gure caption for more details.\nAfter determining true matches and establishing cor-\nrespondences it is now possible to compute the metrics.\nWe do so by concatenating all test sequences and eval-\nuating on the entire benchmark. This is in general more\nmeaningful instead of averaging per-sequences \ufb01gures\ndue to the large variation in the number of targets.\n6\n4.1.2\nDistance measure\nIn the most general case, the relationship between\nground truth objects and a tracker output is established\nusing bounding boxes on the image plane. Similar to\nobject detection [3], the intersection over union (a.k.a.\nthe Jaccard index) is usually employed as the similarity\ncriterion, while the threshold td is set to 0.5 or 50%.\n4.1.3\nTarget-like annotations\nPeople are a common object class present in many\nscenes, but should we track all people in our benchmark?\nFor example, should we track static people sitting on\na bench? Or people on bicycles? How about people\nbehind a glass? We de\ufb01ne the target class of CVPR19\nas all upright walking people that are reachable along\nthe viewing ray without a physical obstacle, i.e. re\ufb02ec-\ntions, people behind a transparent wall or window are\nexcluded. We also exclude from our target class people\non bicycles or other vehicles. For all these cases where\nthe class is very similar to our target class (see Figure 4),\nwe adopt a similar strategy as in [7]. That is, a method\nis neither penalized nor rewarded for tracking or not\ntracking those similar classes. Since a detector is likely to\n\ufb01re in those cases, we do not want to penalize a tracker\nwith a set of false positives for properly following that\nset of detections, i.e. of a person on a bicycle. Likewise,\nwe do not want to penalize with false negatives a tracker\nthat is based on motion cues and therefore does not track\na sitting person.\nIn order to handle these special cases, we adapt the\ntracker-to-target assignment algorithm to perform the\nfollowing steps:\n1) At each frame, all bounding boxes of the result \ufb01le\nare matched to the ground truth via the Hungarian\nalgorithm.\n2) In contrast to MOT17 we account for the very\ncrowded scenes and exclude result boxes that over-\nlap > 75% with one of these classes (distractor,\nstatic person, re\ufb02ection, person on vehicle) and\nremove them from the solution in the detection\nchallenge.\n3) During the \ufb01nal evaluation, only those boxes that\nare annotated as pedestrians are used.\n4.1.4\nMultiple Object Tracking Accuracy\nThe MOTA [14] is perhaps the most widely used metric\nto evaluate a tracker\u2019s performance. The main reason for\nthis is its expressiveness as it combines three sources of\nerrors de\ufb01ned above:\nMOTA = 1 \u2212\nP\nt (FNt + FPt + IDSWt)\nP\nt GTt\n,\n(1)\nwhere t is the frame index and GT is the number of\nground truth objects. We report the percentage MOTA\n(\u2212\u221e, 100] in our benchmark. Note that MOTA can also\nbe negative in cases where the number of errors made\nby the tracker exceeds the number of all objects in the\nscene.\nEven though the MOTA score gives a good indica-\ntion of the overall performance, it is highly debatable\nwhether this number alone can serve as a single perfor-\nmance measure.\nRobustness. One incentive behind compiling this\nbenchmark was to reduce dataset bias by keeping the\ndata as diverse as possible. The main motivation is to\nchallenge state-of-the-art approaches and analyze their\nperformance in unconstrained environments and on un-\nseen data. Our experience shows that most methods can\nbe heavily over\ufb01tted on one particular dataset, and may\nnot be general enough to handle an entirely different\nsetting without a major change in parameters or even in\nthe model.\nTo indicate the robustness of each tracker across all\nbenchmark sequences, we show the standard deviation\nof their MOTA score.\n4.1.5\nMultiple Object Tracking Precision\nThe Multiple Object Tracking Precision is the average\ndissimilarity between all true positives and their corre-\nsponding ground truth targets. For bounding box over-\nlap, this is computed as\nMOTP =\nP\nt,i dt,i\nP\nt ct\n,\n(2)\nwhere ct denotes the number of matches in frame t\nand dt,i is the bounding box overlap of target i with\nits assigned ground truth object. MOTP thereby gives\nthe average overlap between all correctly matched hy-\npotheses and their respective objects and ranges between\ntd := 50% and 100%.\nIt is important to point out that MOTP is a measure of\nlocalization precision, not to be confused with the positive\npredictive value or relevance in the context of precision /\nrecall curves used, e.g., in object detection.\nIn practice, it mostly quanti\ufb01es the localization ac-\ncuracy of the detector, and therefore, it provides little\ninformation about the actual performance of the tracker.\n4.1.6\nTrack quality measures\nEach ground truth trajectory can be classi\ufb01ed as mostly\ntracked (MT), partially tracked (PT), and mostly lost\n(ML). This is done based on how much of the trajectory is\nrecovered by the tracking algorithm. A target is mostly\ntracked if it is successfully tracked for at least 80% of\nits life span. Note that it is irrelevant for this measure\nwhether the ID remains the same throughout the track.\nIf a track is only recovered for less than 20% of its\ntotal length, it is said to be mostly lost (ML). All other\ntracks are partially tracked. A higher number of MT and\nfew ML is desirable. We report MT and ML as a ratio\nof mostly tracked and mostly lost targets to the total\nnumber of ground truth trajectories.\nIn certain situations one might be interested in ob-\ntaining long, persistent tracks without gaps of untracked\nperiods. To that end, the number of track fragmentations\n7\nFig. 4: The annotations include different classes. The target class are pedestrians (left). Besides pedestrians there\nexist special classes in the data such as static person and non-motorized vehicles (non mot vhcl). However, these\nclasses are \ufb01lter out during evaluation and do not effect the test score. Thirdly, we annotate occluders and crowds.\n(FM) counts how many times a ground truth trajectory is\ninterrupted (untracked). In other words, a fragmentation\nis counted each time a trajectory changes its status from\ntracked to untracked and tracking of that same trajectory\nis resumed at a later point. Similarly to the ID switch\nratio (cf. Sec. 4.1.1), we also provide the relative number\nof fragmentations as FM / Recall.\n4.1.7\nTracker ranking\nAs we have seen in this section, there are a number of\nreasonable performance measures to assess the quality\nof a tracking system, which makes it rather dif\ufb01cult to\nreduce the evaluation to one single number. To never-\ntheless give an intuition on how each tracker performs\ncompared to its competitors, we compute and show\nthe average rank for each one by ranking all trackers\naccording to each metric and then averaging across all\nperformance measures.\n5\nCONCLUSION AND FUTURE WORK\nWe have presented a new challenging set of sequences\nwithin the MOTChallenge benchmark. Theses sequences\ncontain a large number of targets to be tracked and the\nscenes are substantially more crowded when compared\nto previous MOTChallenge releases. The scenes are care-\nfully chosen and included indoor/ outdoor and day/\nnight scenarios.\nWe believe that the MOT20 release within the already\nestablished MOTChallenge benchmark provides a fairer\ncomparison of state-of-the-art tracking methods, and\nchallenges researchers to develop more generic methods\nthat perform well in unconstrained environments and on\nvery crowded scenes.\nREFERENCES\n[1]\nS.-H. Bae and K.-J. Yoon.\nRobust online multi-object tracking\nbased on tracklet con\ufb01dence and online discriminative appear-\nance learning. In CVPR 2014.\n[2]\nK. Bernardin and R. Stiefelhagen.\nEvaluating multiple object\ntracking performance: The CLEAR MOT metrics. Image and Video\nProcessing, 2008(1):1\u201310, May 2008.\n[3]\nM. Everingham, L. Van Gool, C. Williams, J. Winn, and A. Zisser-\nman. The PASCAL Visual Object Classes Challenge 2012 (VOC2012)\nResults. 2012.\n[4]\nK. He, X. Zhang, S. Ren, and J. Sun.\nDeep Residual Learning\nfor Image Recognition. arXiv e-prints, page arXiv:1512.03385, Dec\n2015.\n[5]\nL. Leal-Taix\u00b4e, A. Milan, I. Reid, S. Roth, and K. Schindler.\nMOTChallenge 2015: Towards a benchmark for multi-target track-\ning. arXiv:1504.01942, 2015.\n[6]\nY. Li, C. Huang, and R. Nevatia. Learning to associate: Hybrid-\nboosted multi-target tracker for crowded scene. In CVPR 2009.\n[7]\nM. Mathias, R. Benenson, M. Pedersoli, and L. V. Gool.\nFace\ndetection without bells and whistles. In ECCV 2014, 2014.\n[8]\nA. Milan, L. Leal-Taix\u00b4e, I. Reid, S. Roth, and K. Schindler. MOT16:\nA benchmark for multi-object tracking. arXiv:1603.00831 [cs], Mar.\n2016. arXiv: 1603.00831.\n[9]\nA. Milan, S. Roth, and K. Schindler.\nContinuous energy mini-\nmization for multitarget tracking. PAMI, 36(1):58\u201372, 2014.\n[10] A. Milan, K. Schindler, and S. Roth. Challenges of ground truth\nevaluation of multi-target tracking. In 2013 IEEE Conference on\nComputer Vision and Pattern Recognition Workshops (CVPRW), pages\n735\u2013742, June 2013.\n[11] S. Ren, K. He, R. Girshick, and J. Sun. Faster R-CNN: Towards\nReal-Time Object Detection with Region Proposal Networks.\narXiv e-prints, page arXiv:1506.01497, Jun 2015.\n[12] D. Schuhmacher, B.-T. Vo, and B.-N. Vo. A consistent metric for\nperformance evaluation of multi-object \ufb01lters. IEEE Transactions\non Signal Processing, 56(8):3447\u20133457, Aug. 2008.\n[13] K. Smith, D. Gatica-Perez, J.-M. Odobez, and S. Ba. Evaluating\nmulti-object tracking. In Workshop on Empirical Evaluation Methods\nin Computer Vision (EEMCV).\n[14] R.\nStiefelhagen,\nK.\nBernardin,\nR.\nBowers,\nJ.\nS.\nGarofolo,\nD. Mostefa, and P. Soundararajan. The clear 2006 evaluation. In\nCLEAR, 2006.\n[15] L. Wen, W. Li, J. Yan, Z. Lei, D. Yi, and S. Z. Li. Multiple target\ntracking based on undirected hierarchical relation hypergraph. In\nCVPR 2014.\n[16] B. Wu and R. Nevatia. Tracking of multiple, partially occluded\nhumans based on static body part detection. In CVPR 2006, pages\n951\u2013958, 2006.\n",
    "1512.03012": "ShapeNet: An Information-Rich 3D Model Repository\nhttp://www.shapenet.org\nAngel X. Chang1, Thomas Funkhouser2, Leonidas Guibas1, Pat Hanrahan1, Qixing Huang3, Zimo Li3,\nSilvio Savarese1, Manolis Savva\u22171, Shuran Song2, Hao Su\u22171, Jianxiong Xiao2, Li Yi1, and Fisher Yu2\n1Stanford University \u2014 2Princeton University \u2014 3Toyota Technological Institute at Chicago\nAuthors listed alphabetically\nAbstract\nWe present ShapeNet: a richly-annotated, large-scale\nrepository of shapes represented by 3D CAD models of ob-\njects. ShapeNet contains 3D models from a multitude of\nsemantic categories and organizes them under the Word-\nNet taxonomy. It is a collection of datasets providing many\nsemantic annotations for each 3D model such as consis-\ntent rigid alignments, parts and bilateral symmetry planes,\nphysical sizes, keywords, as well as other planned anno-\ntations. Annotations are made available through a pub-\nlic web-based interface to enable data visualization of ob-\nject attributes, promote data-driven geometric analysis, and\nprovide a large-scale quantitative benchmark for research\nin computer graphics and vision. At the time of this techni-\ncal report, ShapeNet has indexed more than 3,000,000 mod-\nels, 220,000 models out of which are classi\ufb01ed into 3,135\ncategories (WordNet synsets). In this report we describe the\nShapeNet effort as a whole, provide details for all currently\navailable datasets, and summarize future plans.\n1. Introduction\nRecent technological developments have led to an ex-\nplosion in the amount of 3D data that we can generate and\nstore. Repositories of 3D CAD models are expanding con-\ntinuously, predominantly through aggregation of 3D content\non the web. RGB-D sensors and other technology for scan-\nning and reconstruction are providing increasingly higher\n\ufb01delity geometric representations of objects and real envi-\nronments that can eventually become CAD-quality models.\nAt the same time, there are many open research prob-\nlems due to fundamental challenges in using 3D content.\nComputing segmentations of 3D shapes, and establishing\ncorrespondences between them are two basic problems in\ngeometric shape analysis. Recognition of shapes from par-\n\u2217Contact authors: {msavva,haosu}@cs.stanford.edu\ntial scans is a research goal shared by computer graphics\nand vision. Scene understanding from 2D images is a grand\nchallenge in vision that has recently bene\ufb01ted tremendously\nfrom 3D CAD models [28, 34]. Navigation of autonomous\nrobots and planning of grasping manipulations are two large\nareas in robotics that bene\ufb01t from an understanding of 3D\nshapes.\nAt the root of all these research problems lies\nthe need for attaching semantics to representations of 3D\nshapes, and doing so at large scale.\nRecently, data-driven methods from the machine learn-\ning community have been exploited by researchers in vision\nand NLP (natural language processing). \u201cBig data\u201d in the\nvisual and textual domains has led to tremendous progress\ntowards associating semantics with content in both \ufb01elds.\nMirroring this pattern, recent work in computer graphics\nhas also applied similar approaches to speci\ufb01c problems in\nthe synthesis of new shape variations [10] and new arrange-\nments of shapes [6]. However, a critical bottleneck facing\nthe adoption of data-driven methods for 3D content is the\nlack of large-scale, curated datasets of 3D models that are\navailable to the community.\nMotivated by the far-reaching impact of dataset efforts\nsuch as the Penn Treebank [20], WordNet [21] and Ima-\ngeNet [4], which collectively have tens of thousands of ci-\ntations, we propose establishing ShapeNet: a large-scale 3D\nmodel dataset. Making a comprehensive, semantically en-\nriched shape dataset available to the community can have\nimmense impact, enabling many avenues of future research.\nIn constructing ShapeNet we aim to ful\ufb01ll several goals:\n\u2022 Collect and centralize 3D model datasets, helping to\norganize effort in the research community.\n\u2022 Support data-driven methods requiring 3D model data.\n\u2022 Enable evaluation and comparison of algorithms for\nfundamental tasks involving geometry (e.g., segmen-\ntation, alignment, correspondence).\n\u2022 Serve as a knowledge base for representing real-world\nobjects and their semantics.\n1\narXiv:1512.03012v1  [cs.GR]  9 Dec 2015\nThese goals imply several desiderata for ShapeNet:\n\u2022 Broad and deep coverage of objects observed in the\nreal world, with thousands of object categories and\nmillions of total instances.\n\u2022 Categorization scheme connected to other modalities\nof knowledge such as 2D images and language.\n\u2022 Annotation of salient physical attributes on models,\nsuch as canonical orientations, planes of symmetry,\nand part decompositions.\n\u2022 Web-based interfaces for searching, viewing and re-\ntrieving models in the dataset through several modali-\nties: textual keywords, taxonomy traversal, image and\nshape similarity search.\nAchieving these goals and providing the resulting dataset\nto the community will enable many advances and applica-\ntions in computer graphics and vision.\nIn this report, we \ufb01rst situate ShapeNet, explaining the\noverall goals of the effort and the types of data it is in-\ntended to contain, as well as motivating the long-term vi-\nsion and infrastructural design decisions (Section 3). We\nthen describe the acquisition and validation of annotations\ncollected so far (Section 4), summarize the current state of\nall available ShapeNet datasets, and provide basic statistics\non the collected annotations (Section 5). We end with a dis-\ncussion of ShapeNet\u2019s future trajectory and connect it with\nseveral research directions (Section 7).\n2. Background and Related Work\nThere has been substantial growth in the number of of\n3D models available online over the last decade, with repos-\nitories like the Trimble 3D Warehouse providing millions\nof 3D polygonal models covering thousands of object and\nscene categories. Yet, there are few collections of 3D mod-\nels that provide useful organization and annotations. Mean-\ningful textual descriptions are rarely provided for individ-\nual models, and online repositories are usually either un-\norganized or grouped into gross categories (e.g., furniture,\narchitecture, etc. [7]). As a result, they have been poorly\nutilized in research and applications.\nThere have been previous efforts to build organized col-\nlections of 3D models (e.g., [5, 7]). However, they have\nprovided quite small datasets, covered only a small num-\nber of semantic categories, and included few structural and\nsemantic annotations. Most of these previous collections\nhave been developed for evaluating shape retrieval and clas-\nsi\ufb01cation algorithms. For example, datasets are created an-\nnually for the Shape Retrieval Contest (SHREC) that com-\nmonly contains sets of models organized in object cate-\ngories. However, those datasets are very small \u2014 the most\nrecent SHREC iteration in 2014 [17] contains a \u201clarge\u201d\ndataset with around 9,000 models consisting of models from\na variety of sources organized into 171 categories (Table 1).\nThe Princeton Shape Benchmark is probably the most\nwell-known and frequently used 3D shape collection to date\n(with over 1000 citations) [27]. It contains around 1,800 3D\nmodels grouped into 90 categories, but has no annotations\nbeyond category labels.\nOther commonly-used datasets\ncontain segmentations [2], correspondences [13, 12], hier-\narchies [19], symmetries [11], salient features [3], seman-\ntic segmentations and labels [36], alignments of 3D models\nwith images [35], semantic ontologies [5], and other func-\ntional annotations \u2014 but again only for small size datasets.\nFor example, the Benchmark for 3D Mesh Segmentation\ncontains just 380 models in 19 object classes [2].\nIn contrast, there has been a \ufb02urry of activity on collect-\ning, organizing, and labeling large datasets in computer vi-\nsion and related \ufb01elds. For example, ImageNet [4] provides\na set of 14M images organized into 20K categories asso-\nciated with \u201csynsets\u201d of WordNet [21]. LabelMe provides\nsegmentations and label annotations of hundreds of thou-\nsands of objects in tens of thousands of images [24]. The\nSUN dataset provides 3M annotations of objects in 4K cat-\negories appearing in 131K images of 900 types of scenes.\nRecent work demonstrated the bene\ufb01t of a large dataset\nof 120K 3D CAD models in training a convolutional neu-\nral network for object recognition and next-best view pre-\ndiction in RGB-D data [34]. Large datasets such as this\nand others (e.g., [14, 18]) have revitalized data-driven al-\ngorithms for recognition, detection, and editing of images,\nwhich have revolutionized computer vision.\nSimilarly, large collections of annotated 3D data have\nhad great in\ufb02uence on progress in other disciplines. For ex-\nample, the Protein Data Bank [1] provides a database with\n100K protein 3D structures, each labeled with its source\nand links to structural and functional annotations [15]. This\ndatabase is a common repository of all 3D protein structures\nsolved to date and provides a shared infrastructure for the\ncollection and transfer of knowledge about each entry. It has\naccelerated the development of data-driven algorithms, fa-\ncilitated the creation of benchmarks, and linked researchers\nand industry from around the world. We aim to provide a\nsimilar resource for 3D models of everyday objects.\n3. ShapeNet: An Information-Rich 3D Model\nRepository\nShapeNet is a large, information-rich repository of 3D\nmodels. It contains models spanning a multitude of seman-\ntic categories. Unlike previous 3D model repositories, it\nprovides extensive sets of annotations for every model and\nlinks between models in the repository and other multime-\ndia data outside the repository.\nLike ImageNet, ShapeNet provides a view of the con-\ntained data in a hierarchical categorization according to\nWordNet synsets (Figure 1). Unlike other model reposi-\ntories, ShapeNet also provides a rich set of annotations for\n2\nBenchmarks\nTypes\n# models\n# classes\nAvg # models per class\nSHREC14LSGTB\nGeneric\n8,987\n171\n53\nPSB\nGeneric\n907+907 (train+test)\n90+92 (train+test)\n10+10 (train+test)\nSHREC12GTB\nGeneric\n1200\n60\n20\nTSB\nGeneric\n10,000\n352\n28\nCCCC\nGeneric\n473\n55\n9\nWMB\nWatertight (articulated)\n400\n20\n20\nMSB\nArticulated\n457\n19\n24\nBAB\nArchitecture\n2257\n183+180 (function+form)\n12+13 (function+form)\nESB\nCAD\n867\n45\n19\nTable 1. Source datasets from SHREC 2014:\nPrinceton Shape Benchmark (PSB) [27], SHREC 2012 generic Shape Benchmark\n(SHREC12GTB) [16], Toyohashi Shape Benchmark (TSB) [29], Konstanz 3D Model Benchmark (CCCC) [32], Watertight Model Bench-\nmark (WMB) [31], McGill 3D Shape Benchmark (MSB) [37], Bonn Architecture Benchmark (BAB) [33], Purdue Engineering Shape\nBenchmark (ESB) [9].\neach shape and correspondences between shapes. The an-\nnotations include geometric attributes such as upright and\nfront orientation vectors, parts and keypoints, shape sym-\nmetries (re\ufb02ection plane, other rotational symmetries), and\nscale of object in real world units. These attributes provide\nvaluable resources for processing, understanding and visu-\nalizing 3D shapes in a way that is aware of the semantics of\nthe shape.\nWe have currently collected approximately 3 million\nshapes from online 3D model repositories, and categorized\n300 thousand of them against the WordNet taxonomy. We\nhave also annotated a subset of these models with shape\nproperties such as upright and front orientations, symme-\ntries, and hierarchical part decompositions. We are contin-\nuing the process of expanding the annotated set of models\nand also collecting new models from new data sources.\nIn the following sections, we discuss how 3D models\nare collected for ShapeNet, what annotations will be added,\nhow those annotations will be generated, how annotations\nwill be updated as the dataset evolves over time, and what\ntools will be provided for the community to search, browse,\nand utilize existing data, as well as contribute new data.\n3.1. Data Collection\nThe raw 3D model data for ShapeNet comes from public\nonline repositories or existing research datasets. ShapeNet\nis intended to be an evolving repository with regular updates\nas more and more 3D models become available, as more\npeople contribute annotations, and as the data captured with\nnew 3D sensors become prevalent.\nWe have collected 3D polygonal models from two\npopular public repositories: Trimble 3D Warehouse1 and\nYobi3D2. The Trimble 3D Warehouse contains 2.4M user-\ndesigned 3D models and scenes. Yobi3D contains 350K\nadditional models collected from a wide range of other on-\nline repositories. Together, they provide a diverse set of\n1https://3dwarehouse.sketchup.com/\n2https://yobi3d.com\nFigure 1. Screenshot of the online ShapeNet taxonomy view, or-\nganizing contained 3D models under WordNet synsets.\nshapes from a broad set of object and scene categories\n\u2014 e.g., many organic shape categories (e.g., humans and\nmammals), which are rare in Warehouse3D, are plentiful in\nYobi3D. For more detailed statistics on the currently avail-\nable ShapeNet models refer to Section 5.\nThough the tools developed for this project will be\ngeneral-purpose, we intend to include only 3D models of\nobjects encountered by people in the everyday world. That\nis, it will not include CAD mechanical parts, molecular\nstructures, or other domain-speci\ufb01c objects. However, we\nwill include scenes (e.g., of\ufb01ce), objects (e.g., laptop com-\nputer), and parts of objects (e.g., keyboard). Models are\norganized under WordNet [21] noun \u201csynsets\u201d (synonym\nsets). WordNet provides a broad and deep taxonomy with\nover 80K distinct synsets representing distinct noun con-\ncepts arranged as a DAG network of hyponym relationships\n(e.g., \u201ccanary\u201d is a hyponym of \u201cbird\u201d). This taxonomy has\nbeen used by ImageNet to describe categories of objects at\n3\nmultiple scales [4]. Though we \ufb01rst use WordNet due to its\npopularity, the ShapeNet UI is designed to allow multiple\nviews into the collection of shapes that it contains, includ-\ning different taxonomy views and faceted navigation.\n3.2. Annotation Types\nWe envision ShapeNet as far more than a collection of\n3D models. ShapeNet will include a rich set of annota-\ntions that provide semantic information about those mod-\nels, establish links between them, and links to other modal-\nities of data (e.g., images). These annotations are exactly\nwhat make ShapeNet uniquely valuable. Figure 2 illustrates\nthe value of this dense network of interlinked attributes on\nshapes, which we describe below.\nLanguage-related\nAnnotations:\nNaming\nobjects\nby\ntheir basic category is useful for indexing, grouping, and\nlinking to related sources of data. As described in the pre-\nvious section, we organize ShapeNet based on the Word-\nNet [21] taxonomy. Synsets are interlinked with various\nrelations, such as hyper and hyponym, and part-whole rela-\ntions. Due to the popularity of WordNet, we can leverage\nother resources linked to WordNet such as ImageNet, Con-\nceptNet, Freebase, and Wikipedia. In particular, linking to\nImageNet [4] will help transport information between im-\nages and shapes. We assign each 3D model in ShapeNet to\none or more synsets in the WordNet taxonomy (i.e., we pop-\nulate each synset with a collection of shapes). Please refer\nto Section 4.1 for details on the acquisition and validation\nof basic category annotations. Future planned annotations\ninclude natural language descriptions of objects and object\npart-part relation descriptions.\nGeometric Annotations:\nA critical property that distin-\nguishes ShapeNet from image and video datasets is the \ufb01-\ndelity with which 3D geometry represents real-world struc-\ntures.\nWe combine algorithmic predictions and manual\nannotations to organize shapes by category-level geomet-\nric properties and further derive rich geometric annotations\nfrom the raw 3D model geometry.\n\u2022 Rigid Alignments: Establishing a consistent canon-\nical orientation (e.g., upright and front) for every\nmodel is important for various tasks such as visual-\nizing shapes [13], shape classi\ufb01cation [8] and shape\nrecognition [34]. Fortunately, most raw 3D model data\nis by default placed in an upright orientation, and the\nfront orientations are typically aligned with an axis.\nThis allows us to use a hierarchical clustering and\nalignment approach to ensure consistent rigid align-\nments within each category (see Section 4.2).\n\u2022 Parts and Keypoints: Many shapes contain or have\nnatural decompositions into important parts, as well as\nsigni\ufb01cant keypoints related to both their geometry and\ntheir semantics. For example, often different materials\nare associated with different parts. We intend to cap-\nture as much of that as possible into ShapeNet.\n\u2022 Symmetry: Bilateral symmetry planes and rotational\nsymmetries are prevalent in arti\ufb01cial and natural ob-\njects, and deeply connected with the alignment and\nfunctionality of shapes. We refer to Section 4.4 for\nmore details on how we compute symmetries for the\nshapes in ShapeNet.\n\u2022 Object Size: Object size is useful for many applica-\ntions, such as reducing the hypothesis space in object\nrecognition.\nSize annotations are discussed in Sec-\ntion 5.2.\nFunctional Annotations:\nMany objects, especially man-\nmade artifacts such as furniture and appliances, can be used\nby humans. Functional annotations describe these usage\npatterns. Such annotations are often highly correlated with\nspeci\ufb01c regions of an object. In addition, it is often related\nwith the speci\ufb01c type of human action. ShapeNet aims to\nstore functional annotations at the global shape level and at\nthe object part level.\n\u2022 Functional Parts: Parts are critical for understand-\ning object structure, human activities involving a 3D\nshape, and ergonomic product design. We plan to an-\nnotate parts according to their function \u2014 in fact the\nvery de\ufb01nition of parts has to be based on both geo-\nmetric and functional criteria.\n\u2022 Affordances: We are interested in affordance annota-\ntions that are function and activity speci\ufb01c. Examples\nof such annotations include supporting plane annota-\ntions, and graspable region annotations for various ob-\nject manipulations.\nPhysical Annotations:\nReal objects exist in the physical\nworld and typically have \ufb01xed physical properties such as\ndimensions and densities.\nThus, it is important to store\nphysical attribute annotations for 3D shapes.\n\u2022 Surface Material: We are especially interested in the\noptical properties and semantic names of surface mate-\nrials. They are important for applications such as ren-\ndering and structural strength estimation.\n\u2022 Weight: A basic property of objects which is very use-\nful for physical simulations, and reasoning about sta-\nbility and static support.\nIn general, the issue of compact and informative rep-\nresentations for all the above attributes over shapes raises\nmany interesting questions that we will need to address\nas part of the ShapeNet effort. Many annotations are cur-\nrently ongoing projects and involve interesting open re-\nsearch problems.\n4\nSwivel chair\nBackrest\nSeat\nBase\nLeg\nWheel\nImageNet\nWordNet synset\nPart Hierarchy\nPart Correspondences\nLink to WordNet Taxonomy\nSwivel chair: a chair that swivels \non its base\n Hypernyms: chair > seat > furniture > ...\n Part meronyms: backrest, seat, base\n Sister terms: armchair, barber chair, ...\nAlignment+Symmetry\nDim: 50 x 45 x 5 cm\nMaterial: foam, fabric\nMass: 5 Kg\nFunction: support\nFigure 2. ShapeNet annotations illustrated for an example chair model. Left: links to the WordNet taxonomy provide de\ufb01nitions of objects,\nis-a and has-a relations, and a connection to images from ImageNet. Middle-left: shape is aligned to a consistent upright and front\norientation, and symmetries are computed Middle-right: hierarchical decomposition of shape into parts on which various attributes are\nde\ufb01ned: names, symmetries, dimensions, materials, and masses. Right: part-to-part and point-to-point connections are established at all\nlevels within ShapeNet producing a dense and semantically rich network of correspondences. The gray background indicates annotations\nthat are currently ongoing and not yet available for release.\n3.3. Annotation Methodology\nThough at \ufb01rst glance it might seem reasonable to collect\nthe annotations we describe purely through manual human\neffort, we will in general take a hybrid approach. For anno-\ntation types where it is possible, we will \ufb01rst algorithmically\npredict the annotation for each model instance (e.g., global\nsymmetry planes, consistent rigid alignments). We will then\nverify these predictions through crowd-sourcing pipelines\nand inspection by human experts. This hybrid strategy is\nsensible in the context of 3D shape data as there are already\nvarious algorithms we can leverage, and collecting corre-\nsponding annotations entirely through manual effort can be\nextremely labor intensive. In particular, since objects in a\n3D representation are both more pure and more complete\nthan objects in images, we can expect better and easier to\nestablish correspondences between 3D shapes, enabling al-\ngorithmic transport of semantic annotations. In many cases,\nthe design of the human annotation interfaces themselves is\nan open question \u2014 which stands in contrast to largely man-\nual image labeling efforts such as ImageNet. As a concrete\nexample, shape part annotation can be presented and per-\nformed in various ways with different trade-offs in the type\nof obtained part annotation, the accuracy and the ef\ufb01ciency\nof the annotation process.\nCoupled with this hybrid annotation strategy, we also\ntake particular care to preserve the provenance and con\ufb01-\ndence of each algorithmic and human annotation. The anno-\ntation source (whether an algorithm, or human effort), and\na measure of the trust we can place in each annotation are\ncritical pieces of information especially when we have to\ncombine, aggregate, and reconcile several annotations.\n3.4. Annotation Schema and Web API\nTo provide convenient access to all of the model and an-\nnotation data contained within ShapeNet, we construct an\nindex over all the 3D models and their associated annota-\ntions using the Apache Solr framework.3 Each stored an-\nnotation for a given 3D model is contained within the index\nas a separate attribute that can be easily queried and \ufb01ltered\nthrough a simple web-based UI. In addition, to make the\ndataset conveniently accessible to researchers, we provide a\nbatched download capability.\n4. Annotation Acquisition and Validation\nA key challenge in constructing ShapeNet is the method-\nology for acquiring and validating annotations. Our goal\nis to provide all annotations with high accuracy. In cases\nwhere full veri\ufb01cation is not yet available, we aim to es-\ntimate a con\ufb01dence metric for each annotation, as well as\nrecord its provenance. This will enable others to properly\nestimate the trustworthiness of the information we provide\nand use it for different applications.\n4.1. Category Annotation\nAs described in Section 3.2, we assign each 3D model to\none or more synsets in the WordNet taxonomy.\nAnnotation\nModels are retrieved by textual query into the\nonline repositories that we collected, and the initial category\nannotation is set to the used textual query for each retrieved\n3http://lucene.apache.org/solr/\n5\nmodel. After we retrieve these models we use the popular-\nity score of each model on the repository to sort models and\nask human workers to verify the assigned category annota-\ntion. This is sensible since the more popular models tend to\nbe high quality and correctly retrieved through the category\nkeyword textual query. We stop verifying category annota-\ntions with people once the positive ratio is lower than a 2%\nthreshold.\nClean-up\nIn order for the dataset to be easily usable by re-\nsearchers it should contain clean and high quality 3D mod-\nels. Through inspection, we identify and group 3D models\ninto the following categories: single 3D models, 3D scenes,\nbillboards, and big ground plane.\n\u2022 Single 3D models: semantically distinct objects; focus\nof our ShapeNetCore annotation effort.\n\u2022 3D scenes: detected by counting the number of con-\nnected components in a voxelized representation. We\nmanually verify these detections and mark scenes for\nfuture analysis.\n\u2022 Billboards: planes with a painted texture. Often used\nto represent people and trees. These models are gen-\nerally not useful for geometric analysis. They can be\ndetected by checking whether a single plane can \ufb01t all\nvertices.\n\u2022 Big ground plane: object of interest placed on a large\nhorizontal plane or in front of large vertical plane. Al-\nthough we do not currently use these models, the plane\ncan easily be identi\ufb01ed and removed through simple\ngeometric analysis.\nWe currently include the single 3D models in the\nShapeNetCore subset of ShapeNet.\n4.2. Hierarchical Rigid Alignment\nThe goal of this step is to establish a consistent canon-\nical orientation for models within each category.\nSuch\nalignment is important for various tasks such as visualizing\nshapes, shape classi\ufb01cation and shape recognition. Figure 3\nshows several categories in ShapeNet that have been con-\nsistently aligned.\nThough the concept of consistent orientation seems nat-\nural, one issue has to be addressed. We explain by an ex-\nample. \u201carmchair\u201d, \u201cchair\u201d and \u201cseat\u201d are three categories\nin our taxonomy, each being a subcategory of its succes-\nsor. Consistent orientation can be well de\ufb01ned for shapes in\nthe \u201carmchair\u201d category, by checking arms, legs and backs.\nYet, it becomes dif\ufb01cult to de\ufb01ne for the \u201cchair\u201d category.\nFor example, \u201cside chair\u201d and \u201cswivel chair\u201d are both sub-\ncategories of \u201cchair\u201d, however, swivel chairs have a very\ndifferent leg structure than most side chairs. It becomes\neven more ambiguous to de\ufb01ne for \u201cseat\u201d, which has sub-\ncategories such as \u201cstool\u201d, \u201ccouch\u201d, and \u201cchair\u201d. However,\nFigure 3. Examples of aligned models in the chair, laptop, bench,\nand airplane synsets.\nthe concept of an upright orientation still applies throughout\nmost levels of the taxonomy.\nFollowing the above discussion, it is natural for us to pro-\npose a hierarchical alignment method, with a small amount\nof human supervision. The basic idea is to hierarchically\nalign models following the taxonomy of ShapeNet in a\nbottom-up manner, i.e., we start from aligning shapes in\nlow-level categories and then gradually elevate to higher\nlevel categories. When we proceed to the higher level, the\nself-consistent orientation within a subcategory should be\nmaintained. For the alignment at each level, we \ufb01rst use\na geometric algorithm described in the Appendix A.1, and\nthen ask human experts to check and correct possible mis-\nalignments. With this strategy, we ef\ufb01ciently obtain consis-\ntent orientations. In practice, most shapes in the same low-\nlevel categories can be well aligned algorithmically, requir-\ning limited manual correction. Though the proportion of\nmanual corrections increases for aligning higher-level cate-\ngories, the number of categories at each level becomes log-\narithmically smaller.\n4.3. Parts and Keypoints\nTo obtain part and keypoint annotations we start from\nsome curated part annotations within each category. For\nparts, this acquisition can be speeded up by having algo-\nrithmically generated segmentations and then having users\naccept or modify parts from these. We intend to experiment\nwith both 2D and 3D interfaces for this task. We then ex-\nploit a number of different algorithmic techniques to propa-\ngate this information to other nearby shapes. Such methods\ncan rely on rigid alignments in 3D, feature descriptor align-\nments in an appropriately de\ufb01ned feature space, or general\nshape correspondences. We iterate this pipeline, using ac-\ntive learning to estimate the 3D models and regions of these\n6\nmodels where further human annotation would be most in-\nformative, generate a new set of crowd-sourced annotation\ntasks, algorithmically propagate their results, and so on. In\nthe end we have users verify all proposed parts and key-\npoints, as veri\ufb01cation is much faster than direct annotation.\n4.4. Symmetry Estimation\nWe provide bilateral symmetry plane detections for all\n3D models in ShapeNetCore. Our method is a modi\ufb01ed\nversion of [22]. The basic idea is to use hough transform\nto vote on the parameters of the symmetry plane. More\nspeci\ufb01cally, we generate all combinations of pairs of ver-\ntices from the mesh. Each pair casts a vote of a possible\nsymmetry plane in the discretized space of plane parame-\nters partitioned evenly. We then pick the parameter with\nthe most votes as the symmetry plane candidate. As a \ufb01nal\nstep, this candidate is veri\ufb01ed to ensure that every vertex\nhas a symmetric counterpart.\n4.5. Physical Property Estimation\nBefore computing physical attribute annotations, the di-\nmensions of the models need to be correspond to the real\nworld. We estimate the absolute dimensions of models us-\ning prior work in size estimation [25], followed by man-\nual veri\ufb01cation. With the given absolute dimensions, we\nnow compute the total solid volume of each model through\n\ufb01lled-in voxelization. We use the space carving approach\nimplemented by Binvox [23]. Categories of objects that are\nknown to be container-like (i.e., bottles, microwaves) are\nannotated as such and only the surface voxelization volume\nis used instead. We then estimate the proportional mate-\nrial composition of each object category and use a table of\nmaterial densities along with each model instance volume\nto compute a rough total weight estimate for that instance.\nMore details about the acquisition of these physical attribute\nannotations are available separately [26].\n5. Current Statistics\nAt the time of this technical report, ShapeNet has in-\ndexed roughly 3,000,000 models.\n220,000 models of\nthese models are classi\ufb01ed into 3,135 categories (Word-\nNet synsets). Below we provide detailed statistics for the\ncurrently annotated models in ShapeNet as a whole, as\nwell as details of the available publicly released subsets of\nShapeNet.\nCategory Distribution\nFigure 4 shows the distributions\nof the number of shapes per synset at various taxonomy\nlevels for the current ShapeNetCore corpus. To the best of\nour knowledge, ShapeNet is the largest clean shape dataset\navailable in terms of total number of shapes, average num-\nber of shapes per category, as well as the number of cate-\ngories.\nWe observe that ShapeNet as a whole is strongly biased\ntowards categories of rigid man-made artifacts, due to the\nbias of the source 3D model repositories. This is in con-\ntrast to common image database statistics that contain more\nnatural objects such as plants and animals [30]. This distri-\nbution bias is probably due to a combination of factors: 1)\nmeshes of natural objects are more dif\ufb01cult to design using\ncommon CAD software; 2) 3D model consumers are typi-\ncally more interested in arti\ufb01cial objects such as those ob-\nserved in modern urban lifestyles. The former factor can be\nmitigated in the near future by using the rapidly improving\ndepth sensing and 3D scanning technology.\n5.1. ShapeNetCore\nShapeNetCore is a subset of the full ShapeNet dataset\nwith single clean 3D models and manually veri\ufb01ed category\nand alignment annotations. It covers 55 common object cat-\negories with about 51,300 unique 3D models. The 12 object\ncategories of PASCAL 3D+[35], a popular computer vision\n3D benchmark dataset, are all covered by ShapeNetCore.\nThe category distribution of ShapeNetCore is shown in Ta-\nble 2.\n5.2. ShapeNetSem\nShapeNetSem is a smaller, more densely annotated sub-\nset consisting of 12,000 models spread over a broader set of\n270 categories. In addition to manually veri\ufb01ed category\nlabels and consistent alignments, these models are anno-\ntated with real-world dimensions, estimates of their mate-\nrial composition at the category level, and estimates of their\ntotal volume and weight. The total numbers of models for\nthe top 100 categories in this subset are given in Table 3.\n6. Discussion and Future Work\nThe construction of ShapeNet is a continuous, ongoing\neffort. Here we have just described the initial steps we have\ntaken in de\ufb01ning ShapeNet and populating a core subset\nof model annotations that we hope will prove useful to the\ncommunity. We plan to grow ShapeNet in four distinct di-\nrections:\nAdditional annotation types\nWe will introduce several\nadditional types of annotations that have strong connections\nto the semantics and functionality of objects. Firstly, hierar-\nchical part decompositions of objects will provide a useful\n\ufb01ner granularity description of object structure that can be\nleveraged for part segmentation and shape synthesis. Sec-\nondly, physical object property annotations such as materi-\nals and their attributes will allow higher \ufb01delity physics and\nappearance simulation, adding another layer of understand-\ning to methods in vision and graphics.\n7\n0K 5K\n10K 15K 20K 25K 30K 35K 40K 45K 50K 55K 60K 65K 70K 75K 80K 85K 90K 95K 100K105K110K115K 120K\nartifact\nplant\nperson\nanimal\nathletics\nnatural object\ngeological\nRoot\n0K\n2K\n4K\n6K\n8K\n10K\n12K\n14K\n16K\n18K\n20K\n22K\n24K\n26K\n28K\n30K\n32K\n34K\ndevice\nfurniture\ncontainer\ntransport\nequipment\nimplement\nweaponry\nArtifact\n0K\n1K\n2K\n3K\n4K\n5K\n6K\n7K\n8K\nseat\ntable\ncabinet\nlamp\nbedroom furniture\nbookcase\nwardrobe\noffice furniture\nFurniture\n0K\n1K\n2K\n3K\n4K\n5K\nmotor vehicle\narmored vehicle\nrecreational vehicle\ntracked vehicle\ntrailer\nlocomotive\nrailcar\ntractor\nskateboard\nWheeled Vehicle\n0K\n1K\n2K\n3K\n4K\n5K\n6K\nchair\nsofa\nstool\nbench\nottoman\ntoilet seat\nSeat\n0\n500\n1000 1500 2000 2500\nside chair\narmchair\nclub chair\nswivel chair\nfolding chair\ncantilever chair\nchaise\nrocking chair\nlawn chair\nrex chair\nzigzag chair\nwheelchair\nbarcelona chair\ntulip chair\nball chair\nChair\n0\n100\n200\n300\n400\nsedan\nsport utility\ncoupe\nracer\nsports car\nconvertible\nlimousine\nminivan\nambulance\nroadster\nstation wagon\ntouring car\ncruiser\njeep\nhot rod\nhatchback\ncab\nCar\n0\n500\n1000\n1500\n2000\n2500\ndesk\nrectangular table\nworkshop table\nside table\ncounter\nshort table\nworktable\nTable\nFigure 4. Plots of the distribution of ShapeNet models over WordNet synsets at multiple levels of the taxonomy (only the top few children\nsynsets are shown at each level). The highest level (root) is at the top and the taxonomy levels become lower downwards and to the right.\nNote the bias towards rigid man-made artifacts at the top and the broad coverage of many low level categories towards the bottom.\nID\nName\nNum\nID\nName\nNum\nID\nName\nNum\n04379243\ntable\n8443\n03593526\njar\n597\n04225987\nskateboard\n152\n02958343\ncar\n7497\n02876657\nbottle\n498\n04460130\ntower\n133\n03001627\nchair\n6778\n02871439\nbookshelf\n466\n02942699\ncamera\n113\n02691156\nairplane\n4045\n03642806\nlaptop\n460\n02801938\nbasket\n113\n04256520\nsofa\n3173\n03624134\nknife\n424\n02946921\ncan\n108\n04090263\nri\ufb02e\n2373\n04468005\ntrain\n389\n03938244\npillow\n96\n03636649\nlamp\n2318\n02747177\ntrash bin\n343\n03710193\nmailbox\n94\n04530566\nwatercraft\n1939\n03790512\nmotorbike\n337\n03207941\ndishwasher\n93\n02828884\nbench\n1816\n03948459\npistol\n307\n04099429\nrocket\n85\n03691459\nloudspeaker\n1618\n03337140\n\ufb01le cabinet\n298\n02773838\nbag\n83\n02933112\ncabinet\n1572\n02818832\nbed\n254\n02843684\nbirdhouse\n73\n03211117\ndisplay\n1095\n03928116\npiano\n239\n03261776\nearphone\n73\n04401088\ntelephone\n1052\n04330267\nstove\n218\n03759954\nmicrophone\n67\n02924116\nbus\n939\n03797390\nmug\n214\n04074963\nremote\n67\n02808440\nbathtub\n857\n02880940\nbowl\n186\n03085013\nkeyboard\n65\n03467517\nguitar\n797\n04554684\nwasher\n169\n02834778\nbicycle\n59\n03325088\nfaucet\n744\n04004475\nprinter\n166\n02954340\ncap\n56\n03046257\nclock\n655\n03513137\nhelmet\n162\n03991062\n\ufb02owerpot\n602\n03761084\nmicrowaves\n152\nTotal\n57386\nTable 2. Statistics of ShapeNetCore synsets. ID corresponds to WordNet synset offset, which is aligned with ImageNet.\n8\nCategory\nNum\nCategory\nNum\nCategory\nNum\nCategory\nNum\nCategory\nNum\nChair\n696\nMonitor\n127\nWallLamp\n78\nGun\n54\nFlagPole\n38\nLamp\n663\nRoundTable\n120\nSideChair\n77\nNightstand\n53\nTvStand\n38\nChestOfDrawers\n511\nTrashBin\n117\nVideoGameConsole\n75\nMug\n51\nFireplace\n37\nTable\n427\nDrinkingUtensil\n112\nMediaStorage\n73\nAccentChair\n50\nRack\n37\nCouch\n413\nDeskLamp\n110\nPainting\n73\nChessBoard\n49\nLightSwitch\n36\nComputer\n244\nClock\n101\nDesktop\n71\nRug\n49\nOven\n36\nDresser\n234\nToyFigure\n101\nAccentTable\n70\nWallUnit\n46\nAirplane\n35\nTV\n233\nPlant\n98\nCamera\n70\nMirror\n45\nDresserWithMirror\n35\nWallArt\n222\nArmoire\n95\nPicture\n69\nBowl\n44\nCalculator\n34\nBed\n221\nQueenBed\n94\nRefrigerator\n68\nSodaCan\n44\nTableClock\n34\nCabinet\n221\nStool\n92\nSpeaker\n68\nVideoGameController\n44\nToilet\n34\nFloorLamp\n201\nEndTable\n91\nSideboard\n67\nWallClock\n43\nCup\n33\nDesk\n189\nBottle\n88\nBarstool\n66\nPrinter\n42\nStapler\n33\nPottedPlant\n188\nDiningTable\n88\nGuitar\n65\nSword\n40\nPaperBox\n32\nFoodItem\n180\nBookcase\n87\nMediaPlayer\n62\nUSBStick\n40\nSpaceShip\n32\nLaptop\n173\nCeilingLamp\n86\nIpod\n59\nChaise\n39\nToy\n32\nVase\n163\nBench\n84\nPersonStanding\n57\nOf\ufb01ceSideChair\n39\nToiletPaper\n31\nTableLamp\n142\nBook\n84\nPiano\n56\nPoster\n39\nKnife\n30\nOf\ufb01ceChair\n137\nCoffeeTable\n81\nCurtain\n55\nSink\n39\nPictureFrame\n30\nCellPhone\n130\nPencil\n80\nCandle\n54\nTelephone\n39\nRecliner\n30\nTable 3. Total number of models for the top 100 ShapeNetSem categories (out of 270 categories). Each category is also linked to the\ncorresponding WordNet synset, establishing the same linkage to WordNet and ImageNet as with ShapeNetCore.\nCorrespondences\nOne of the most important goals of\nShapeNet is to provide a dense network of correspondences\nbetween 3D models and their parts. This will be invalu-\nable for enabling much shape analysis research and helping\nto improve and evaluate methods for many traditional tasks\nsuch as alignment and segmentation. Additionally, we plan\nto provide correspondences between 3D model parts and\nimage patches in ImageNet \u2014 a link that will be critical\nfor propagating information between image space and 3D\nmodels.\nRGB-D data\nThe rapid proliferation of commodity\nRGB-D sensors is already making the process of capturing\nreal-world environments better and more ef\ufb01cient. Expand-\ning ShapeNet to include shapes reconstructed from scanned\nRGB-D data is a critical goal. We foresee that over time,\nthe amount of available reconstructed shape data will over-\nshadow the existing designed 3D model data and as such\nthis is a natural growth direction for ShapeNet. A related ef-\nfort that we are currently undertaking is to align 3D models\nto objects observed in RGB-D frames. This will establish\na powerful connection between real world observations and\n3D models.\nAnnotation coverage\nWe will continue to expand the set\nof annotated models to cover a bigger subset of the entirety\nof ShapeNet. We will explore combinations of algorithmic\npropagation methods and crowd-sourcing for veri\ufb01cation of\nthe algorithmic results.\n7. Conclusion\nWe \ufb01rmly believe that ShapeNet will prove to be an im-\nmensely useful resource to several research communities in\nseveral ways:\nData-driven research\nBy establishing ShapeNet as the\n\ufb01rst large-scale 3D shape dataset of its kind we can help\nto move computer graphics research toward a data-driven\ndirection following recent developments in vision and NLP.\nAdditionally, we can help to enable larger-scale quantitative\nanalysis of proposed systems that can clarify the bene\ufb01ts of\nparticular methodologies against a broader and more repre-\nsentative variety of 3D model data.\nTraining resource\nBy providing a large-scale, richly an-\nnotated dataset we can also promote a broad class of re-\ncently resurgent machine learning and neural network meth-\nods for applications dealing with geometric data.\nMuch\nlike research in computer vision and natural language un-\nderstanding, computational geometry and graphics stand\nto bene\ufb01t immensely from these data-driven learning ap-\nproaches.\nBenchmark dataset\nWe hope that ShapeNet will grow to\nbecome a canonical benchmark dataset for several evalua-\ntion tasks and challenges. In this way, we would like to en-\ngage the broader research community in helping us de\ufb01ne\nand grow ShapeNet to be a pivotal dataset with long-lasting\nimpact.\nReferences\n[1] Helen M Berman, John Westbrook, Zukang Feng, Gary\nGilliland, TN Bhat, Helge Weissig, Ilya N Shindyalov, and\nPhilip E Bourne. The protein data bank. Nucleic Acids Res,\n28:235\u2013242, 2000. 2\n[2] Xiaobai\nChen,\nAleksey\nGolovinskiy,\nand\nThomas\nFunkhouser.\nA benchmark for 3D mesh segmentation.\nACM TOG, 28(3):73:1\u201373:12, July 2009. 2\n9\n[3] Xiaobai Chen, Abulhair Saparov, Bill Pang, and Thomas\nFunkhouser. Schelling points on 3D surface meshes. ACM\nTOG, August 2012. 2\n[4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 2009. 1, 2, 4\n[5] Bianca\nFalcidieno.\nAim@shape.\nhttp://www.\naimatshape.net/ontologies/shapes/, 2005. 2\n[6] Matthew Fisher, Daniel Ritchie, Manolis Savva, Thomas\nFunkhouser, and Pat Hanrahan. Example-based synthesis of\n3D object arrangements. ACM TOG, 31(6):135, 2012. 1\n[7] Paul-Louis George.\nGamma.\nhttp://www.rocq.\ninria.fr/gamma/download/download.php,\n2007. 2\n[8] Qixing Huang, Hao Su, and Leonidas Guibas. Fine-grained\nsemi-supervised labeling of large shape collections. ACM\nTOG, 32:190:1\u2013190:10, 2013. 4, 11\n[9] Subramaniam Jayanti, Yagnanarayanan Kalyanaraman, Na-\ntraj Iyer, and Karthik Ramani. Developing an engineering\nshape benchmark for CAD models. Computer-Aided Design,\n2006. 3\n[10] Evangelos Kalogerakis,\nSiddhartha Chaudhuri,\nDaphne\nKoller, and Vladlen Koltun.\nA probabilistic model for\ncomponent-based shape synthesis. ACM TOG, 31:55, 2012.\n1\n[11] Vladimir Kim, Yaron Lipman, Xiaobai Chen, and Thomas\nFunkhouser.\nMobius transformations for global intrinsic\nsymmetry analysis.\nSymposium on Geometry Processing,\nJuly 2010. 2\n[12] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Siddhartha\nChaudhuri, Stephen DiVerdi, and Thomas Funkhouser.\nLearning part-based templates from large collections of 3D\nshapes. ACM TOG, 32(4):70:1\u201370:12, July 2013. 2\n[13] Vladimir G. Kim, Wilmot Li, Niloy J. Mitra, Stephen\nDiVerdi, and Thomas Funkhouser.\nExploring collections\nof 3D models using fuzzy correspondences.\nACM TOG,\n31(4):54:1\u201354:11, July 2012. 2, 4\n[14] Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei.\n3D object representations for \ufb01ne-grained categorization. In\n4th International IEEE Workshop on 3D Representation and\nRecognition (3dRR-13), Sydney, Australia, 2013. 2\n[15] Roman A Laskowski, E Gail Hutchinson, Alex D Michie,\nAndrew C Wallace, Martin L Jones, and Janet M Thornton.\nPDBsum: A web-based database of summaries and analyses\nof all PDB structures. Trends Biochem. Sci., 22:488\u2013490,\n1997. 2\n[16] Bo Li, Afzal Godil, Masaki Aono, X Bai, Takahiko Furuya,\nL Li, R L\u00b4opez-Sastre, Henry Johan, Ryutarou Ohbuchi, Car-\nolina Redondo-Cabrera, et al. SHREC\u201912 track: generic 3D\nshape retrieval. In 5th Eurographics Conference on 3D Ob-\nject Retrieval, 2012. 3\n[17] Bo Li, Yijuan Lu, Chunyuan Li, Afzal Godil, Tobias\nSchreck, Masaki Aono, Qiang Chen, Nihad Karim Chowd-\nhury, Bin Fang, Takahiko Furuya, et al. SHREC\u201914 track:\nLarge scale comprehensive 3D shape retrieval.\nIn Euro-\ngraphics Workshop on 3D Object Retrieval, 2014. 2\n[18] Joerg Liebelt and Cordelia Schmid. Multi-view object class\ndetection with a 3D geometric model. In CVPR, pages 1688\u2013\n1695. IEEE, 2010. 2\n[19] Tianqiang Liu, Siddhartha Chaudhuri, Vladimir G. Kim, Qi-\nXing Huang, Niloy J. Mitra, and Thomas Funkhouser. Cre-\nating consistent scene graphs using a probabilistic grammar.\nACM TOG, December 2014. 2\n[20] Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice\nSantorini. Building a large annotated corpus of english: The\nPenn Treebank. Computational linguistics, 19(2):313\u2013330,\n1993. 1\n[21] George A. Miller. WordNet: a lexical database for English.\nCACM, 1995. 1, 2, 3, 4\n[22] Niloy J Mitra, Mark Pauly, Michael Wand, and Duygu Cey-\nlan. Symmetry in 3D geometry: Extraction and applications.\nIn Computer Graphics Forum, volume 32, pages 1\u201323, 2013.\n7\n[23] Fakir S. Nooruddin and Greg Turk. Simpli\ufb01cation and repair\nof polygonal models using volumetric techniques. Visualiza-\ntion and Computer Graphics, IEEE Transactions on, 2003.\n7\n[24] Bryan C Russell and Antonio Torralba. Building a database\nof 3D scenes from user annotations. In CVPR, 2009. 2\n[25] Manolis Savva, Angel X. Chang, Gilbert Bernstein, Christo-\npher D. Manning, and Pat Hanrahan.\nOn being the right\nscale: Sizing large collections of 3D models. In SIGGRAPH\nAsia 2014 Workshop on Indoor Scene Understanding: Where\nGraphics meets Vision, 2014. 7\n[26] Manolis Savva,\nAngel X. Chang,\nand Pat Hanrahan.\nSemantically-Enriched\n3D\nModels\nfor\nCommon-sense\nKnowledge.\nCVPR 2015 Workshop on Functionality,\nPhysics, Intentionality and Causality, 2015. 7\n[27] Philip Shilane, Patrick Min, Michael Kazhdan, and Thomas\nFunkhouser.\nThe Princeton shape benchmark.\nIn Shape\nModeling Applications. IEEE, 2004. 2, 3\n[28] Shuran Song and Jianxiong Xiao. Sliding shapes for 3D ob-\nject detection in depth images. In ECCV, 2014. 1\n[29] Atsushi Tatsuma, Hitoshi Koyanagi, and Masaki Aono. A\nlarge-scale shape benchmark for 3D object retrieval: Toy-\nohashi shape benchmark. In Asia Paci\ufb01c Signal and Infor-\nmation Processing Association, 2012. 3\n[30] Antonio Torralba, Bryan C Russell, and Jenny Yuen. La-\nbelMe: Online image annotation and applications. Proceed-\nings of the IEEE, 98(8):1467\u20131484, 2010. 7\n[31] Remco C. Veltkamp and FB ter Harr. SHREC 2007 3D shape\nretrieval contest. Technical report, Utrecht University Tech-\nnical Report UU-CS-2007-015, 2007. 3\n[32] Dejan V Vrani\u00b4c. 3D model retrieval. University of Leipzig,\nGermany, PhD thesis, 2004. 3\n10\n[33] Raoul Wessel, Ina Bl\u00a8umel, and Reinhard Klein. A 3D shape\nbenchmark for retrieval and automatic classi\ufb01cation of ar-\nchitectural data. In Eurographics 2009 Workshop on 3D Ob-\nject Retrieval, pages 53\u201356. The Eurographics Association,\n2009. 3\n[34] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin-\nguang Zhang, Xiaoou Tang, and Jianxiong Xiao.\n3D\nShapeNets: A Deep Representation for Volumetric Shapes.\nCVPR, 2015. 1, 2, 4\n[35] Yu Xiang, Roozbeh Mottaghi, and Silvio Savarese. Beyond\nPASCAL: A benchmark for 3D object detection in the wild.\nIn WACV, 2014. 2, 7\n[36] Jianxiong Xiao, Andrew Owens, and Antonio Torralba.\nSUN3D: A database of big spaces reconstructed using SfM\nand object labels. In ICCV, pages 1625\u20131632, 2013. 2\n[37] Juan Zhang, Kaleem Siddiqi, Diego Macrini, Ali Shokoufan-\ndeh, and Sven Dickinson. Retrieving articulated 3-D mod-\nels using medial surfaces and their graph spectra. In Energy\nminimization methods in computer vision and pattern recog-\nnition, 2005. 3\nA. Appendix\nA.1. Hierarchical Rigid Alignment\nIn the following, we describe our hierarchical rigid align-\nment algorithm in more detail.\nAs a pre-processing step, we \ufb01rst semi-automatically\nalign the upright orientation of each shape.\nFortunately,\nmost shapes downloaded from the web are by default placed\nin the upright orientations. For those that are not, we \ufb01lter\nthem out by manual inspection. We then convert models to\npoint clouds through furthest point sampling and perform\nPCA on the point sets. Finally, we ask a person to pick\nthe vector of correct upright orientation from six candidates\ncontaining the PCA axes and their reverse directions.\nStarting from a leaf category in ShapeNet, we jointly\nalign all shapes following prior work [8]. If a leaf cate-\ngory has more than 100 shapes, we further partition it into\nsmaller, more coherent clusters by k-means clustering us-\ning pose-invariant global features, such as phase-invariant\nHoG features [see appendix]. Here we brie\ufb02y review [8].\nEach shape is associated with a random variable, denot-\ning the transformation of the shape from its original pose\nto the consistent canonical pose. Over the set of shapes, a\nMarkov Random Field (MRF) is constructed, whose energy\nfunction measures the consistency of all pairs of shapes af-\nter applying their transformations. In practice, the space of\nrigid transformations is discretized into N bins. We perform\nMAP inference over the MRF to \ufb01nd the optimal transfor-\nmation for each shape. We then manual inspect the results\nand correct occasional errors.\nAfter this step, we represent each leaf node category by\nthe shape in the centroid of the feature space. Then, we\ngather the representative shapes for all leaf categories of an\nintermediate category and apply [8] again for joint align-\nment. This higher-level algorithmic alignment is veri\ufb01ed\nby a person again. The procedure is applied along the tax-\nonomy hierarchy until the root node is reached.\n11\n",
    "2403.09190": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n1\nIntention-aware Denoising Diffusion Model for\nTrajectory Prediction\nChen Liu, Shibo He, Haoyu Liu, Jiming Chen\nAbstract\u2014Trajectory prediction is an essential component in\nautonomous driving, particularly for collision avoidance systems.\nConsidering the inherent uncertainty of the task, numerous\nstudies have utilized generative models to produce multiple\nplausible future trajectories for each agent. However, most of\nthem suffer from restricted representation ability or unstable\ntraining issues. To overcome these limitations, we propose uti-\nlizing the diffusion model to generate the distribution of future\ntrajectories. Two cruxes are to be settled to realize such an idea.\nFirst, the diversity of intention is intertwined with the uncertain\nsurroundings, making the true distribution hard to parameterize.\nSecond, the diffusion process is time-consuming during the\ninference phase, rendering it unrealistic to implement in a real-\ntime driving system. We propose an Intention-aware denoising\nDiffusion Model (IDM), which tackles the above two problems.\nWe decouple the original uncertainty into intention uncertainty\nand action uncertainty, and model them with two dependent\ndiffusion processes. To decrease the inference time, we reduce\nthe variable dimensions in the intention-aware diffusion process\nand restrict the initial distribution of the action-aware diffusion\nprocess, which leads to fewer diffusion steps. To validate our\napproach, we conduct experiments on Stanford Drone Dataset\n(SDD) and ETH/UCY dataset. Our methods achieve state-of-the-\nart results, with an FDE of 13.83 pixels on SDD dataset and\n0.36 meters on ETH/UCY datasets. Compared with the original\ndiffusion model, IDM reduces inference time by two-thirds.\nInterestingly, our experiments further reveal that introducing\nintention information is beneficial in modeling the diffusion\nprocess of fewer steps.\nIndex Terms\u2014trajectory prediction, diffusion models\nI. INTRODUCTION\nA\nLTHOUGH high-level autonomous driving systems can\nsignificantly enhance our convenience, it remains a chal-\nlenging task to maintain its security [1]. As mentioned in [2]\u2013\n[4], self-driving vehicles must anticipate the future movements\nof surrounding agents (including vehicles and pedestrians)\nin order to plan proactive motions and avoid collision with\nthem. Therefore, trajectory prediction plays a crucial role in\nautonomous driving systems [5].\nTraditional deterministic trajectory prediction methods aim\nto provide a single future trajectory for each agent [6]\u2013\n[9]. However, it has been recognized that not all relevant\nclues, such as the intentions and habits of the agents, can\nbe fully acquired. Consequently, there may exist multiple\nplausible future trajectories for an agent [10], which can be\nChen Liu, Shibo He, Jiming Chen are with State Key Lab. of Industrial\nControl Technology, Zhejiang University, Hangzhou 310027, China. Email:\n{liu777ch, s18he, cjm}@zju.edu.cn. Haoyu Liu is with Fuxi AI Lab, NetEase\nGames, Hangzhou 310052, and also with State Key Laboratory of Industrial\nControl Technology, Zhejiang University, Hangzhou 310027, China. Email:\nliuhaoyu03@corp.netease.com.\nturn right\nturn left\ngo straight\n(a) Intention uncertainty\ngoal\n(b) Action uncertainty\nFig. 1. Two kinds of uncertainty: (a) A pedestrian may turn left, turn right, or\nproceed straight based on his own will; (b) A pedestrian with a deterministic\ngoal can also choose different paths in order to avoid collisions with their\nsurroundings.\nignored by these deterministic methods. To address this issue,\nprobabilistic trajectory prediction attempts to generate multiple\npredictions to encompass all possible outcomes, and it has\ngarnered growing interest in recent years [11].\nPrevious studies have investigated different generative mod-\nels for probabilistic trajectory prediction, including noise-\nbased models, bivariate-Gaussian (BG)-based models, con-\nditional variational autoencoder (CVAE)-based models, and\ngenerative adversarial network (GAN)-based models [12].\nNoise-based models, used by Gupta et al., Thiede et al.,\nand Deo et al. [13]\u2013[15] involve injecting random noise into\nneural networks to generate multiple future trajectories and\ntraining the networks using a variety loss. However, the variety\nloss fails to penalize unrealistic trajectories, resulting in the\ninability to accurately describe the true distribution of possible\ntrajectories [16]. BG-based models, used by Alal et al. [17]\nand Mohamd et al. [18] assume that future positions follow\na bivariate-Gaussian distribution and estimate the distribution\nparameters using maximum likelihood estimation. However,\nreal-world trajectories may follow more complex distributions,\nleading to the capped performance of their methods [12].\nCVAE-based models aim to incorporate uncertainty into the\npredictions by introducing a latent distribution [19]\u2013[22]. Nev-\nertheless, CVAE is observed to generate unnatural trajectories\ndue to its limited ability to model complex distributions [23].\nAdditionally, GANs are employed to fit parameterized prob-\nability distributions [24]\u2013[26]. However, the unstable training\nprocedure of GANs hinders their practical development [27].\nIn summary, all these generative models suffer from limited\nability to represent sophisticated trajectory distributions or\nunstable training processes.\nRecently, diffusion models have shown their potential in\novercoming two aforementioned limitations and have gained\nprominence among generative models [28]\u2013[30]. Compared\nwith CVAE and BG-based models, diffusion models offer\narXiv:2403.09190v1  [cs.CV]  14 Mar 2024\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n2\ngreater representation ability, as they can generate complex\ndistributions through a series of steps [31]. Additionally, the\ntraining process of diffusion models is more stable compared\nto GANs [32]. As a result, diffusion models have achieved\nstate-of-the-art performance in various domains, including\nimage synthesis [28]\u2013[30], video generation [33], [34] and\naudio processing [35], [36]. Considering the aforementioned\nadvantages of diffusion models, we propose adopting diffusion\nmodels for the trajectory prediction problem. However, two\nmain challenges need to be addressed to realize this idea.\nHow to clearly model the uncertainty of multimodal\nfuture? The uncertainty of agent movement is significantly in-\nfluenced by both internal stimulus and external environmental\nfactors. For example, at a crossroad, individuals may choose\nto turn left, turn right, or proceed straight based on their\nintentions, as depicted in Fig. 1(a). Furthermore, even when\nthe goal (endpoint) is deterministic, they may select different\npaths. Fig. 1(b) illustrates a scenario where an individual\nintends to go straight but adjusts their movement to the left\nor right to avoid a collision with another person. It has\nbeen recognized in [10] that the future trajectory distribution\nshould be multimodal, with each mode representing a different\nintention of the agent. Applying the original diffusion model\nto this task would neglect the multimodal property of the\ntrajectory [32], as it only models the uncertainty of paths\nwithout considering intention information. As a result, it may\nproduce stochastic and unnatural predictions.\nHow to enhance the efficiency of diffusion model? The\ndiffusion model typically performs poorly in terms of time\nefficiency, resulting in a significant drawback when applied\nto trajectory prediction for autonomous driving systems. The\nconsiderable time consumption can be attributed to two fac-\ntors. Firstly, the diffusion model follows a Markov chain,\nwhere the ground truth distribution is transformed into a prior\nGaussian noise distribution by injecting noises step by step.\nTo ensure that the prior noise distribution follows a Gaussian\ndistribution, the diffusion process entails a large number of\nsteps [37]. Consequently, during the reverse process, obtaining\nthe ground truth distribution from the Gaussian noise also\nnecessitates a large number of denoising steps, resulting in\nlow efficiency. Secondly, the inefficiency is further exacerbated\nby the high dimensions of data involved in the diffusion\nprocess [38]. Dealing with high-dimensional data requires\nmore computational resources and time, making the overall\nprocess slower.\nTo address the aforementioned challenges, we propose an\nintention-aware diffusion model called IDM for trajectory\nprediction. Our approach involves decoupling the uncertainty\nof trajectory prediction into intention uncertainty and action\nuncertainty. To model the intention uncertainty, we utilize a\ndiffusion process to estimate the distribution of the agent\u2019s\ngoal. For action uncertainty, we employ another diffusion\nprocess to estimate the distribution of paths conditioned on\na specific goal. During the prediction stage, we first infer\nmultiple possible goals of the agent and then generate trajecto-\nries conditioned on all of these inferred goals. This approach\nallows for capturing the multimodal nature of the trajectory\ndistribution, which can integrate both intention and action\nuncertainties. To enhance computational efficiency, in the goal\ndiffusion process, we consider each trajectory\u2019s endpoint as\nthe particle instead of the entire trajectory, thus reducing the\ndimension of the data being modeled. In the path diffusion\nprocess, rather than using prior Gaussian noise distribution,\nwe propose to estimate the prior noise distribution by a neural\nnetwork, thereby reducing the number of steps required in\nthe diffusion process. Additionally, we design a corresponding\nloss function to enable end-to-end training of the two diffusion\nprocesses. We evaluate our model on two real-world datasets,\nand the results demonstrate IDM\u2019s superior performance. It\nachieves state-of-the-art results in both Average Displacement\nError (ADE) and Final Displacement Error (FDE). Compared\nto traditional denoising diffusion models, our model reduces\ninference times by two-thirds. Interestingly, we also discover\nthe significance of intention in providing valuable clues for\nestimating the prior noise distribution accurately. The contri-\nbutions are summarized as follows:\n\u2022 We propose a two-stage diffusion model for trajectory\ngeneration called IDM, which decouples the uncertainty\nof trajectory prediction into goal uncertainty and action\nuncertainty, and model them by two dependent diffusion\nprocess.\n\u2022 To enhance computational efficiency, we propose a Pri-\norNet for estimating prior noise distribution and utilize a\ntailored loss function during training, which significantly\nreduces the required number of steps in the diffusion pro-\ncess. Interestingly, we also find that the goal can provide\nvaluable information for accurate prior distribution noise\nestimation.\n\u2022 We perform extensive experiments on two real-world\ndatasets. Our findings show that IDM achieves state-of-\nthe-art detection performance in terms of ADE and FDE.\nAdditionally, IDM significantly reduces inference time\nby about two-thirds compared to the original diffusion\nmodel.\nThe rest of this paper is organized as follows. In Section\nII, we provide a comprehensive review of related works\non trajectory prediction and diffusion models. Section III\nformulates the problem and provides preliminary knowledge.\nSection IV presents a detailed explanation of our proposed\nmethod. In Section V, we conduct experiments to assess the\nperformance of our method. Finally, we conclude our study in\nSection VI.\nII. RELATED WORK\nA. Trajectory Prediction\nTrajectory prediction plays a crucial role in the field of\nself-driving vehicles. Previous studies in this area can be\nbroadly categorized into three main approaches: physics-\nbased, planning-based, and pattern-based [4].\n1) Physics-based approaches: Previous works on trajectory\nprediction use hand-crafted dynamic models based on New-\nton\u2019s laws of motion. Early approaches often employ methods\nlike autoregressive models [39], Kalman filters [40], and\nparticle filters [41] to make one-step-ahead predictions. These\npredictions are based on classical kinematic models like the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n3\nconstant velocity model (CV), the constant acceleration model\n(CA), the bicycle model and so on [42]\u2013[44]. Other works\nincorporate map-related contextual clues [45]\u2013[47] and social\ninteraction information [48]\u2013[50] into physics-based models.\nWhile these methods perform well under mild conditions,\ntheir performance deteriorates over longer prediction horizons\ndue to the absence of future control signals, limiting their\napplicability to long-term prediction tasks of more than 1\nsecond [1].\n2) Planning-based approaches: Planning-based approaches\nassume that agents are rational decision-makers during action.\nTherefore, they turn the prediction problem into a sequential\ndecision-making problem aimed at finding an optimal motion\nsequence that minimizes a cost function [4]. The approaches\ncan be classified into forward-planning ones and inverse-\nplanning ones. The forward-planning methods generate plau-\nsible trajectories using path planning with hand-crafted cost-\nfunction [51]. The interactions among multi-agents are also\nconsidered through cooperative planning in joint state-space\n[52]\u2013[54]. However, these works all predefine an explicit cost\nor reward function, which sometimes obviates from reality.\nThe inverse planning methods estimate the cost function from\nall observations via Inverse Reinforcement Learning (IRL)\n[55]. Other works employ Generative Adversarial Imitation\nLearning (GAIL) [31] to model the future motion state dis-\ntribution directly without learning the reward function first\n[56], [57]. These methods are computationally intensive and\nconsume large training costs.\n3) Pattern-based approaches: With the development of\ndeep learning, researchers develop approaches that learn tra-\njectory patterns directly from data. Most of the existing deep\nlearning models for trajectory prediction adopt an encoder-\ndecoder architecture [58]. The encoder captures meaningful\ninformation for prediction, such as historical trajectories, map\ninformation, and states of neighbor agents. To model historical\ntrajectories, previous works utilize sequence modeling tech-\nniques like long short-term memory networks (LSTM) [17],\n[59], temporal convolutional network (TCN) [60], and Trans-\nformer [15]. To provide prior knowledge of road structure,\nthe high-definition map can be encoded by rasterization [61]\nor vectorization [9]. The interactions among multiple agents\nare often modeled by the social pooling mechanism [62],\nattention mechanisms [63]\u2013[65] or graph neural networks [18],\n[66], [67]. The decoder network aggregates all information\nand generates the eventual predictions. Song et al. focus on\nthe feasibility of predictions, by imposing dynamic and scene-\ncompliant restrictions on the output sequence [68]\u2013[70]. Other\nworks pay attention to stochastic predictions. A common\npractice is to incorporate a noise signal into the decoder and\nutilize the variety loss [13] to train the decoder. However, Guo\net al. point out that the variety loss can not penalize those\nunrealistic trajectories, and may generate unfeasible samples\nduring inference [16]. Therefore, the following works turn\nto adopt generative models. For example, Salzmann et al.\nfirstly propose a Conditional Variational Autoencoder (CVAE)\nbased method Trajectron++ [19], and several works follow\nthe architecture [20]\u2013[22]. Other works apply Generative Ad-\nversarial Network (GAN) to trajectory generation [24]\u2013[26].\nTo generate multimodal trajectories, the latent variable which\nimplies the intention of the agents such as goal endpoints\n[23], [71] and goal lanes [15], [72] are estimated to enhance\nthe training [10], [73]. However, the CVAE-based methods\nencounter the performance bottleneck when handling compli-\ncated distribution, while the GAN-based methods are trapped\nin unstable training. In this work, we aim to employ diffusion\nmodels to achieve multimodal and stochastic prediction. The\nmost related works to us are [32] and [74]. Our work differs\nfrom them in two aspects. Firstly, we consider the multimodal\nproperty and devise a goal-aware diffusion process. Secondly,\nwe significantly reduce the number of steps in the diffu-\nsion process by estimating the multi-modal prior distribution\ninstead of using the normal distribution assumption, which\nsignificantly shortens the inference time.\nB. Diffusion Model\nRecent years have witnessed the success of diffusion model\nin various domains such as image synthesis [28]\u2013[30], video\ngeneration [33], [34], sequence modeling [75], [76] and audio\nprocessing [35], [36]. As a powerful and novel deep generative\nmodel, the diffusion model originates from non-equilibrium\nthermodynamics and is first proposed in [77]. The key concept\nof the diffusion model is to turn original signals into a known\nnoise distribution by injecting noise gradually and then reverse\nthe process during generation [78]. The denoising process\nis modeled by a parameterized Markov chain, which can\nbe learned by a neural network. Compared with classical\ngenerative models like VAE [79] and GAN [80], the diffusion\nmodel shows great potential in representation learning and has\na stable training procedure with solid theoretical supports [81].\nHowever, the diffusion model has an inherent weakness of\nslow generation process [82], which limits its application in\nreal-time scenarios such as autonomous driving. To address\nthis issue, we propose an efficient diffusion model for trajec-\ntory prediction, taking inspiration from previous works. Robin\net al. propose a diffusion process targeting low-dimension\nlatent features to reduce the cost of training and inference\n[38]. Therefore, utilizing intention (goal) instead of the whole\ntrajectory to establish the diffusion process can result in\nsignificant cost savings. Furthermore, Meng et al. claim that\nthe diffusion process removes signals from high-frequency to\nlow-frequency [37]. In our proposed framework, we assume\nthat the agent\u2019s intention (goal) contains low-frequency infor-\nmation, while the path contains high-frequency information.\nTherefore, we utilize the predicted goal information to estimate\nthe intermediate distribution with low-frequency signals. This\nallows us to model the process from the original distribution to\nthis intermediate distribution, effectively reducing the number\nof steps required in the diffusion process.\nIII. SYSTEM MODEL\nA. Problem Statement\nThe goal of trajectory prediction is to predict the future\nlocations of agents on the road, such as pedestrians, riders,\nor vehicles, based on their historical tracepoints. Due to\nthe inherent uncertainty of moving objects, there are many\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n4\n\ud835\udc66\ud835\udc660\n\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61\n\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61+1\n\ud835\udc66\ud835\udc66\ud835\udc3e\ud835\udc3e\n\u2026\n\u2026\n\ud835\udc5d\ud835\udc5d(\ud835\udc66\ud835\udc660|\ud835\udc66\ud835\udc661)\n\ud835\udc5d\ud835\udc5d(\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61|\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61+1)\n\ud835\udc5d\ud835\udc5d(\ud835\udc66\ud835\udc66\ud835\udc3e\ud835\udc3e\u22121|\ud835\udc66\ud835\udc66\ud835\udc3e\ud835\udc3e)\n\ud835\udc5e\ud835\udc5e(\ud835\udc66\ud835\udc661|\ud835\udc66\ud835\udc660)\n\ud835\udc5e\ud835\udc5e(\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61+1|\ud835\udc66\ud835\udc66\ud835\udc61\ud835\udc61)\n\ud835\udc5e\ud835\udc5e(\ud835\udc66\ud835\udc66\ud835\udc3e\ud835\udc3e|\ud835\udc66\ud835\udc66\ud835\udc3e\ud835\udc3e\u22121)\nDiffusion Process\nDenoising Process\nFig. 2. Denoising Diffusion Probabilistic Model\nplausible routes that the agents could follow in the future,\nso our focus is on generating a trajectory distribution that is\nas realistic as possible, guided by all available observations.\nMathematically, given a target agent with trajectories in the\npast x =\n\b\nst \u2208R2 | t = \u2212TP + 1, \u2212TP + 2, ..., 0\n\t\n, where st\ndenotes its 2-D locations at time t, and TP is the time step of\ntrajectory, we aim to predict its future trajectory distribution\ny = {p(st) | t = 1, 2..., TQ}, where TQ is the length of\nprediction, and p(st) represents the distribution of all possible\nlocations at time t. In realistic scenario, we could predict\nK trajectories\n\b\nyi \u2208RTQ\u00d72 | i = 1, 2, ...K\n\t\nin the future. In\naddition, environmental factors such as historical locations\nof neighbor agents e = {sne\nt\n| t = \u2212TP + 1, \u2212TP + 2, ..., 0},\nand the map M can be vectorized [9] to assist the prediction.\nWe summarize all the notations used in our work in Table I.\nTABLE I\nSUMMARY OF NOTATIONS\nSymbol\nDescription\nx\nThe agent\u2019s historical trajectory\ny\nThe agent\u2019s future trajectory\ne\nThe neighbor\u2019s historical trajectory\nM\nThe map of the scene\nTP\nNumber of historical steps\nTQ\nNumber of future steps\nK\nNumber of future predictions\nK\nNumber of steps in the goal diffusion process\nS\nNumber of steps in the trajectory diffusion process\n\u03b1g\nk\nThe diffusion coefficient at k-th step for goal\n\u03b1t\ns\nThe diffusion coefficient at s-th step for trajectory\ng\u03c8\nEncoder with parameters \u03c8\n\u03f5\u03b8\nEndNet with parameters \u03b8\n\u00b5\u03d5\nPriorNet with parameters \u03d5\n\u03f5\u03c6\nPathNet with parameters \u03c6\n\u03bb1\nThe coefficient for trajectory diffusion loss\n\u03bb2\nThe coefficient for trajectory prior loss\nx\nThe context vectors generated by the encoder\nck\nThe goal at k-th step in diffusion process\nys\nThe trajectory at s-th step in diffusion process\nc0\nThe ground truth goal\ny0\nThe ground truth trajectory\nB. Denoising Diffusion Probabilistic Model\nGenerative models aim to discover the real distribution of\nobserved samples by representing uncertainty through latent\nvariables. Inspired by non-equilibrium thermodynamics, the\ndenoising diffusion probabilistic model (DDPM) describes the\ndistribution of samples as particles in motion, akin to the\nbehavior of particles in thermodynamics. As noise is gradually\nadded to the observed samples, the distribution of real data,\nwhich initially has low uncertainty, transforms into a random\ndistribution with high uncertainty. This transformation process\nis commonly referred to as the diffusion process or forward\nprocess. As shown in Fig 2, the diffusion process is modeled\nusing a parameterized Markov chain:\nq (y1:K | y0) :=\nK\nY\nk=1\nq (yk | yk\u22121)\nq (yk | yk\u22121) := N\n\u0010\nyk;\np\n1 \u2212\u03b2tyk\u22121, \u03b2kI\n\u0011\n(1)\nwhere \u03b21, \u03b22, ...\u03b2k denote the variance schedulers that control\nthe level of Gaussian noise that is injected into the original\nsignals. On the other hand, the reverse process, also known\nas the denoising process, involves transforming the noisy\ndistribution back into the sample distribution. This process\ncan also be represented as a Markov chain:\np\u03b8 (y0:K) := p (yK)\nK\nY\nk=1\np\u03b8 (yk\u22121 | yk)\np\u03b8 (yk\u22121 | yk) := N (yk\u22121; \u00b5\u03b8 (yk, k) , \u03a3\u03b8 (yk, k))\n(2)\nwhere the mean \u00b5\u03b8 (xk, k) and variance \u03a3\u03b8 (xk, k) of reverse\ntransition is learnt by a neural network.\nBased on the fixed forward process parameters, We can\nacquire yt at any time step as follows:\nq (yk | y0) = N\n\u0000yk; \u221a\u00af\u03b1ky0, (1 \u2212\u00af\u03b1k) I\n\u0001\n(3)\nusing the notations: \u03b1k := 1 \u2212\u03b2k and \u00af\u03b1k := Qk\ns=1 \u03b1s. It\ncan be seen that the final noised distribution yk approaches\nthe normal distribution as k increases. A common practice is\nto set a large step number K, which can result in yK being\nequivalent to a normal distribution. By using this setting, we\ncan obtain real samples from the normal distribution through\nthe reverse process.\nOur aim is to learn p\u03b8 (yk\u22121 | yk) that match the diffusion\nprocess. The training objective is to maximize the likelihood\nfunction E [log p\u03b8 (y0)], using variational inference:\nE [log p\u03b8 (y0)] = Eq(y1:K|y0)\n\u0014\nlog\np\u03b8 (y0:K)\np\u03b8 (y1:K | y0)\n\u0015\n= Eq(y1:K|y0)\n\u0014\nlog\np\u03b8 (y0:K)\nq (y1:K | y0)\n\u0015\n+ DKL (q (y1:K | y0, ) \u2225p\u03b8 (y1:K | y0))\n(4)\nthen we can maximize the first term called evidence lower\nbound:\nELBO = Eq\n\"\nlog p (yK) +\nK\nX\nk=1\nlog p\u03b8 (yk\u22121 | yk)\nq (yk | yk\u22121)\n#\n(5)\nsince yK is determined by the real data distribution y0 and\nthe predefined variance schedulers, it represents a constant\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n5\ndistribution and can be disregarded during training. Therefore,\nthe loss function is defined as follows:\nL(\u03b8) = Eq\n\" K\nX\nk=1\nlog p\u03b8 (yk\u22121 | yk, )\nq (yk | yk\u22121)\n#\n= Eq\n\" K\nX\nk=1\nDKL (q (yk\u22121 | yk, y0) \u2225p\u03b8 (yk\u22121 | yk))\n#\n(6)\nIndeed, posterior distribution q (yk\u22121 | yk, y0) is also a Gaus-\nsian distribution:\nq (yk\u22121 | yk, y0) = N\n\u0010\nyk\u22121; \u02dc\u00b5k (yk, y0) , \u02dc\u03b2kI\n\u0011\n(7)\nwhere the parameters are calculated according to:\n\u02dc\u00b5k (yk, y0) =\n\u221a\u00af\u03b1k\u22121\u03b2k\n1 \u2212\u03b1k\ny0 +\n\u221a\u03b1k (1 \u2212\u00af\u03b1k\u22121)\n1 \u2212\u00af\u03b1k\nyk\n=\n1\n\u221a\u03b1k\n\u0012\nyk \u2212\n\u03b2k\n\u221a1 \u2212\u00af\u03b1k\n\u03f5\n\u0013\n\u02dc\u03b2k = 1 \u2212\u00af\u03b1k\u22121\n1 \u2212\u00af\u03b1k\n\u03b2kI.\n(8)\nwhere \u03f5 \u223cN (0, I). Based on (2), the loss function can be\nregarded as the KL divergence of two Gaussian distributions.\nTo simplify the optimization, we set \u03a3\u03b8 (yk, k) = \u02dc\u03b2k, and\nreparameterize \u00b5\u03b8 (yk, k) as:\n\u00b5\u03b8 (yk, k) =\n1\n\u221a\u03b1k\n\u0012\nyk \u2212\n\u03b2k\n\u221a1 \u2212\u00af\u03b1k\n\u03f5\u03b8 (yk, k)\n\u0013\n(9)\nwhere yk = \u221a\u00af\u03b1ky0 + (1 \u2212\u00af\u03b1k) \u03f5, then the eventual loss is:\nL(\u03b8) = Eq\nh\n\u03bb \u2225\u02dc\u00b5k (yk, y0) \u2212\u00b5\u03b8 (yk, k)\u22252i\n\u221dEq \u2225\u03f5 \u2212\u03f5\u03b8 (yk, k)\u22252\n(10)\nDuring inference, we recover original sample y0 from yK \u223c\nN (0, I) step by step according to:\nyk\u22121 =\n1\n\u221a\u03b1k\n\u0012\nyk \u2212\n\u03b2k\n\u221a1 \u2212\u00af\u03b1k\n\u03f5\u03b8 (yk, k)\n\u0013\n(11)\nC. Conditional Denoising Diffusion Probabilistic Model\nWhen using the diffusion model for trajectory prediction, it\nis important to consider the impacts of historical trajectory and\nenvironmental information on the future trajectory distribution.\nMathematically, we represent all inputs by a context vector x\nand aim to analyze the conditional distribution p\u03b8 (y0 | x). The\nforwarding process is defined as follows:\nq (y1:K | y0, x) :=\nK\nY\nk=1\nq (yk | yk\u22121)\nq (yk | yk\u22121) := N\n\u0010\nyk;\np\n1 \u2212\u03b2tyk\u22121, \u03b2kI\n\u0011\n(12)\nwhile the reverse process is parameterized by:\np\u03b8 (y0:K | x) := p (yK | x)\nK\nY\nk=1\np\u03b8 (yk\u22121 | yk, x)\np\u03b8 (yk\u22121 | yk, x) := N (yk\u22121; \u00b5\u03b8 (yk, x, k) , \u03a3\u03b8 (yk, x, k))\n(13)\nSimilarly,\nvariational\ninference\nis\nused\nto\nmaximize\nlog p\u03b8 (y0 | x), and the evidence lower bound is computed\nas:\nELBO = Eq log p (yK | x) + Eq\nK\nX\nk=1\nlog p\u03b8 (yk\u22121 | yk, x)\nq (yk | yk\u22121)\n]\n(14)\nIf we choose a large step K, then the first term can also be\nignored like DDPM because:\nlog p (yK | x) = log Ep(y0|x)q (yK | y0)\n(15)\nwhere q (yK | y0) is always a normal distribution. This is what\n[32] does. The final loss function will be:\nL(\u03b8) = Eq \u2225\u03f5 \u2212\u03f5\u03b8 (yk, x, k)\u22252\n(16)\nwhere the noised added in the k-th step will be predicted under\nthe guidance of the context vector.\nTo implement the aforementioned process, it is crucial to\nensure that q (yK | y0) is always a normal distribution under\nany context vector. This requirement usually necessitates a\nlarge number of diffusion steps, which can be inefficient\nand time-consuming for inference. Additionally, when dealing\nwith complex original distributions, determining the appro-\npriate number of steps for completing the diffusion process\nis difficult. Therefore, in the following section, we present\nour solution which aims to generate multimodal trajectory\ndistributions more efficiently.\nIV. MODEL DESCRIPTION\nThis section starts with a concise overview of IDM\u2019s ar-\nchitecture. We subsequently delve into more comprehensive\nexplanations of the diffusion processes for goals and trajecto-\nries. Finally, we present implementation details.\nA. Architecture\nThe overall architecture of IDM is presented in Fig 3. We\nfollow an encoder-decoder architecture. The encoder takes\nall observations as inputs and produces a context vector,\nreferred to as the \u201ccontext\u201d for simplicity. The decoder then\ntransforms this context into multiple future trajectories. In this\npaper, we focus on designing the decoder, making the model\nindependent of the specific encoder used. The decoder consists\nof two diffusion processes: 1) Goal Diffusion Process: This\nprocess aims to model the uncertainty associated with the goal\n(endpoint). The denoising function in the reverse process is\nparameterized by an EndNet. 2) Trajectory Diffusion Process:\nThis process models the uncertainty of action given a specific\ngoal. To expedite the diffusion process, we develop a PriorNet\nthat estimates the prior noise distribution. Furthermore, we\ndiscover that the goal itself can provide clues for the estimation\nof the prior noise distribution. With the prior noise distribution\nestablished, we employ a PathNet to iteratively eliminate noise\nand generate the final trajectories.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n6\n\ud835\udf16~\ud835\udc41(0,1)\nEndNet\nPathNet\nEncoder\nContext\nIntention-aware \ndistribution\n\ud835\udc504\n\ud835\udc50#\n\ud835\udc50#+%\n\ud835\udc50\"\n\ud835\udc5d(\ud835\udc50#+%|\ud835\udc50#, \ud835\udc65)\n\ud835\udc5e(\ud835\udc50#|\ud835\udc50#+%)\nPriorNet\n\ud835\udc66\"\n\ud835\udc66#\n\ud835\udc66#+%\n\ud835\udc664\n\ud835\udc5d(\ud835\udc66#+%|\ud835\udc66#, \ud835\udc65)\n\ud835\udc5e(\ud835\udc66#|\ud835\udc66#+%)\n\u2026\n\u2026\n\u2026\n\u2026\nhistory trajectory\nfuture endpoint\nfuture trajectory\nFig. 3. Overall structure of intention-aware trajectory diffusion model: a) EndNet: It models the diffusion process of the agent\u2019s endpoints. b) PathNet: It\nmodels the diffusion process of the agent\u2019s trajectories conditioned on a specific endpoint. c) PriorNet: It estimates the initial noise distribution of the trajectory\ndiffusion process with fewer steps.\nB. Goal Diffusion Process\nThe goal-based diffusion process aims to learn the distri-\nbution of endpoints under a specific context. This process is\ndenoted as (c0, c1, ..., cK), where K represents the number of\ndiffusion steps, and c0 represents the ground truth distribution.\nAs the diffusion progresses, c0 gradually transitions towards a\nrandom noise distribution. The forward process is formulated\nas follows:\nq (c1:K | c0, x) :=\nK\nY\nk=1\nq (ck | ck\u22121)\nq (ck | ck\u22121) := N\n\u0010\nck;\np\n1 \u2212\u03b2tck\u22121, \u03b2kI\n\u0011\n(17)\nIn our approach, we adopt a large value of K, similar\nto conventional DDPM. As the diffusion process progresses,\ncK can be considered as following a normal distribution.\nAccording to Sec. III, the noise injected at each diffusion\nstep is parameterized by a neural network called EndNet,\nrepresented as\u03f5\u03b8 (ck, x, k). To generate the eventual endpoints,\nwe follow a step-by-step procedure, utilizing the EndNet-\ngenerated noise. More specifically, the endpoints are generated\niteratively as follows:\nck\u22121 =\n1\n\u221a\u03b1k\n\u0012\nck \u2212\n\u03b2k\n\u221a1 \u2212\u00af\u03b1k\n\u03f5\u03b8 (ck, x, k)\n\u0013\n(18)\nwhere \u03b8 represents the parameters of the EndNet. The loss\nfunction for the goal diffusion process is:\nLgoal = Eq \u2225\u03f5 \u2212\u03f5\u03b8 (ck, x, k)\u22252\n(19)\nIn this process, the sample being modeled is a 2-D location,\nwhich leads to a significant reduction in inference time.\nC. Trajectory Diffusion Process\nWhen the goal is determined, the trajectory of the agent\nmay include uncertainties. For example, a pedestrian may\nreact to the presence of surrounding objects by opting for a\nmoderate or aggressive action to avoid a collision, as illustrated\nin Fig. 3. To address these uncertainties, we introduce an\nadditional diffusion process that is specifically designed to\nincorporate the agent\u2019s intention. This integration of intention\ninto the diffusion process enables more precise predictions of\nthe agent\u2019s future movements.\nFirst, the forward process is defined as:\nq (y1:S | y0, x) :=\nK\nY\nk=1\nq (ys | ys\u22121)\nq (ys | ys\u22121) := N\n\u0010\nys;\np\n1 \u2212\u03b2tys\u22121, \u03b2sI\n\u0011\n(20)\nwhere S is the number of steps and differs from that of goal\ndiffusion K. If S is small, the first term of Eq 14 can not be\nignored, because yS is not a constant normal distribution. The\nnew evidence lower bound is computed as follows:\nELBO\n= Eq log p (yS | x) + Eq\nS\nX\ns=1\nlog p\u03b8 (ys\u22121 | ys, x)\nq (ys | ys\u22121)\n= Eq\n\"\nlog p (yS | x) p\u03b8 (y0 | y1, x)\nq (y1 | y0)\n+\nS\nX\ns=2\np\u03b8 (ys\u22121 | ys, x)\nq (ys | ys\u22121)\n#\n= Eq\n\"\nlog p (yS | x) p\u03b8 (y0 | y1, x)\nq (yS | y0)\n+\nS\nX\ns=2\np\u03b8 (ys\u22121 | ys, x)\nq (ys\u22121 | ys, y0)\n#\n= Eq log p\u03b8 (y0 | y1, x) \u2212DKL (q (yS | y0) \u2225p (yS | x))\n\u2212\nS\nX\ns=2\nEq [DKL (q (ys\u22121 | ys, y0) \u2225p\u03b8 (ys\u22121 | ys, x))]\n(21)\nwhere the sum of the first and third terms equals the EVLB\nof the original diffusion process. The second term varies only\nwith p (yS | x), and a large S leads to an extreme condition\nwhere the second term is a constant. Then the loss function\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n7\nof the trajectory diffusion process is defined according to:\nLtraj = Ldiff + Lprior\n(22)\nwhere Ldiff is the negative sum of the first and third term, and\nLprior is the negative of the second term. The computation of\nLdiff is the same as in the conventional diffusion process:\nLdiff = Eq \u2225\u03f5 \u2212\u03f5\u03c6 (ys, x, s)\u22252\n(23)\nwhere \u03c6 denotes the parameters of the network that estimates\nthe noise at each step.\nTo compute Lprior, an intuitive practice is to parameter-\nize the p (yS | x) with another neural network represented\nby \u03d5, like what the variational autoencoder does. Consid-\nering that q (yS | y0) is a g distribution q (yS | y0)\n=\nN (yS; \u221a\u00af\u03b1Sy0, (1 \u2212\u00af\u03b1S) I), according to Eq. 3, we also take\np (yS | x) as a G distribution. The neural network estimates\nthe mean \u00b5\u03d5 (x) and variance \u03c3\u03d5 (x) of p (yS | x). To simplify\nthe computation, we make \u03c3\u03d5 (x) = (1 \u2212\u00af\u03b1S) I, and compute\nLprior as follows:\nLprior =\n\r\r\u00b5\u03d5 (x) \u2212\u221a\u00af\u03b1Sy0\n\r\r2\n(24)\nIn this computation, there is a question about whether a\nGaussian distribution is sufficient to capture all the information\nin p (yS | x). Previous study [10] highlights that the trajectory\ndistribution can be decomposed into multiple independent\ndistributions conditioned on the goals of the agent.\np (y0 | x) =\nZ\np (c | x) p (y0 | x, c) dc\n(25)\nwhere c denotes the goal of the agent. Then p (yS | x) is\nformulated as:\np (yS | x) =\nZ\np (y0 | x) p (yS | x, y0) dy0\n=\nZZ\np (c | x) p (y0 | x, c) q (yS | y0) dcdy0\n=\nZ\np (c | x) p (yS | x, c) dc\n(26)\nIf we decompose the distribution p (yS | x) by conditioning\non targets, we can calculate Lprior as follows:\nLprior = DKL (q (yS | y0) \u2225p (yS | x))\n= DKL\n\u0012\nq (yS | y0) \u2225\nZ\np (c | x) p (yS | x, c) dc\n\u0013\n= \u2212Eq(yS|y0) log Ep(c|x)p (yS | x, c)\nq (yS | y0)\n\u2264\u2212Eq(yS|y0)Ep(c|x) log p (yS | x, c)\nq (yS | y0)\n= Ep(c|x)DKL (q (yS | y0) \u2225p (yS | x, c))\n(27)\nwhere the upper bound of Lprior is induced according to\nJensen\u2019s inequality. During training, we minimize the upper\nbound of Lprior, which we call Lprior latter. Instead of\nmodeling the whole trajectory distribution, we parameterize\nthe distribution of the trajectory conditioned on the goal with\na neural network \u03d5. The final Lprior is calculated by:\nLprior = Ep(c|x)\n\r\r\u00b5\u03d5 (x, c) \u2212\u221a\u00af\u03b1Sy0\n\r\r2\n(28)\nCompared with p (yS | x), p (yS | x, c) is more suitable\nto be modeled as a Gaussian distribution, and could lead to\nsmall prior loss. The insight is in accord with previous works\n[73], [83], which regard the future trajectory as a multimodal\ndistribution. Our experiments verify this point in Sec. V-D.\nD. Model details\nA model instantiated from our architecture consists of four\nblocks: Encoder, EndNet, PriorNet, and PathNet, which we\nwill elaborate literally as follows.\n1) Encoder: Our architecture focuses on the decoder based\non the diffusion process and therefore is encoder-agnostic. In\nthis work, we utilize the encoder from a classical model called\nTrajectron++ [19]. Trajectron++ follows a CVAE framework.\nThe encoder in Trajetron++ incorporates all observations into\na hidden embedding, while the decoder transforms the em-\nbedding into plenty of future trajectories, It consists of three\nmain blocks: 1) LSTM network modeling the agent history;\n2) Aggregation block that encodes agent interactions; 3) CNN\nbased map encoder which represents the semantic maps by a\nvector. The final output of the encoder is concatenated by the\nabove embeddings. We follow their encoder settings.\n2) EndNet:\nEndNet is responsible for parameterizing\n\u03f5\u03b8 (ck, x, k). It takes x, step k, and the goal at that step ck\nas inputs, and produces the estimated noise. These inputs are\nconcatenated and fed into a Multi-Layer Perceptron (MLP).\nWe evaluate the performance of EndNet with different num-\nbers of layers in Sec. V-D.\n3) PriorNet: PriorNet is used to model \u00b5\u03d5 (x, c), which\nrepresents the mean of the prior distribution p (yS | x). The\ninputs include the context vector x \u2208RD and the goal of\nthe agent c \u2208R2. The output of PriorNet is yS \u2208RTQ\u00d72,\nwhere TQ represents the number of future time steps. There\ncould exist temporal correlations among future trajectories.\nTherefore, we generate the prior distribution based on a neural\nnetwork that captures temporal correlation. MLP, RNN, TCN,\nand Transformer are all validated in our experiments.\n4) PathNet: PathNet tries to parameterize the \u03f5\u03c6 (ys, x, s).\nThe network takes ys \u2208RTQ\u00d72, x \u2208RD and s as inputs\nand produces the estimated noise \u03f5 \u2208RTQ\u00d72. To utilize\nthe temporal correlation, we also develop several versions of\nPathNet based on MLP, RNN, TCN, and Transformer.\nE. Training and Inference\n1) Training: During training, we know both the historical\nobservations and future trajectories. The training loss is de-\nfined by:\nLtotal = Lgoal + \u03bb1Ldiff + \u03bb2Lprior\n(29)\nwhere Lgoal and Ldiff are calculated according to Eq. 19 and\nEq. 23. \u03bb1 and \u03bb2 control the importance of the loss for the\ntrajectory diffusion process and loss for prior distribution esti-\nmation, respectively. Because ck is sampled from a Gaussian\ndistribution based on c0 according to Eq. 3, which is non-\ndifferentiable during training, we apply the reparameterization\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n8\ntrick. Specifically, we get a sample \u03f5 from normal distribution,\nand obtain ck by:\nck = \u221a\u00af\u03b1kc0 + (1 \u2212\u00af\u03b1k) \u03f5\n(30)\nys in Ldiff is acquired by the same way.\nSimilar to previous works [10], we acquire Lprior by a\nteacher-forcing manner, where the realistic goal c0 is used\nto guide the calculation:\nLprior =\n\r\r\u00b5\u03d5 (x, c0) \u2212\u221a\u00af\u03b1Sy0\n\r\r2\n(31)\nThe complete training procedure is shown in Algorithm 1.\nAlgorithm 1 Training\nInput: x, e, M, y; \u03b1g\n1:K, \u03b1t\n1:S; \u03bb1, \u03bb2\nOutput: Encoder g\u03c8, EndNet \u03f5\u03b8, PriorNet \u00b5\u03d5, PathNet \u03f5\u03c6\nrepeat\nCompute context vector x = g\u03c8(x, e, M)\ny0 = y, c0 = y[\u22121]\nk \u223cUniform({1, 2, ..., K}), s \u223cUniform({1, 2, ..., S})\n\u03f5 \u223cN(0, I)\nck =\np\n\u00af\u03b1g\nkc0 + (1 \u2212\u00af\u03b1g\nk) \u03f5\nys =\np\n\u00af\u03b1tsy0 + (1 \u2212\u00af\u03b1t\ns) \u03f5\nCompute Lgoal, Ldiff, Lprior according to Eq. 19,23,31\nTake gradient descent step on\n\u2207\u03c8,\u03b8,\u03c6,\u03d5(Lgoal + \u03bb1Ldiff + \u03bb2Lprior)\nuntil converged\n2) Inference: In the inference phase, we aim to generate\nK plausible predictions. The goal is predicted by the iterative\ndiffusion process first, and is then utilized to initialize yS.\nFinally, the trajectory conditioned on the goal is generated\nby another diffusion process with fewer steps. The details are\ndescribed in Algorithm 2.\nAlgorithm 2 Inference\nInput: x, e, M, y; \u03b1g\n1:K, \u03b1t\n1:S, g\u03c8, \u03f5\u03b8, \u00b5\u03d5, \u03f5\u03c6\nOutput:\n\b\nyi \u2208RTQ\u00d72 | i = 1, 2, ...K\n\t\nCompute context vector x = g\u03c8(x, e, M)\nfor i = 1, ..., K do\ncK \u223cN(0, I)\nfor k = K, ..., 1 do\nck\u22121 =\n1\n\u221a\n\u03b1g\nk\n\u0012\nck \u2212\n\u03b2g\nk\n\u221a\n1\u2212\u00af\u03b1g\nk \u03f5\u03b8 (ck, x, k)\n\u0013\nend for\n\u03f5 \u223cN(0, I)\nyS = \u00b5\u03d5(x, c0) + (1 \u2212\u00af\u03b1S) \u03f5\nfor s = S, ..., 1 do\nys\u22121 =\n1\n\u221a\n\u03b1ts\n\u0012\nys \u2212\n\u03b2t\ns\n\u221a\n1\u2212\u00af\u03b1ts \u03f5\u03c6 (ys, x, s)\n\u0013\nend for\nyi = y0\nend for\nreturn\n\b\nyi \u2208RTQ\u00d72 | i = 1, 2, ...K\n\t\nV. EXPERIMENTS\nA. Experiment Setup\n1) Dataset: We use Stanford Drone Datasets (SDD) and\nUCY/ETH to evaluate our methods.\nSDD: It is a large-scale dataset aimed at trajectory predic-\ntion. The data is collected by a drone from a bird\u2019s-eye view\nperspective. It contains 60 recordings referring to 20 scenes\naround a university campus and describes the movements of\nmultiple pedestrians and vehicles. The trajectory data is sam-\npled at 2.5 FPS. Following previous works [71], we conduct\nexperiments on 47 recordings about pedestrian trajectory, with\n30 of them used as training data, while the remaining 17\nrecordings are used to test the model\u2019s performance.\nETH/UCY: The dataset is a classical benchmark for pedes-\ntrian trajectory prediction. The data comes from the surveil-\nlance video on the street. The dataset comprises five scenes,\nnamely, ETH, Hotel, UNIV, ZARA1, and ZARA2. The sam-\npling frequency is 2.5 FPS. We adopt the leave-one-scene-out\nsetting like [13], where 4 scenes are used for training and the\nremaining scene is used for testing.\nTABLE II\nPERFORMANCE ON SDD DATASET\nMethods\nDeocder\nUgoal\nUtraj\nADE\nFDE\nSocial-LSTM\nBG\n-\n\u2713\n57.00\n31.20\nExpert+GMM\nBG\n-\n\u2713\n10.67\n14.38\nSimAug\nGrid\n\u2713\n-\n10.27\n19.71\nPCCSNET\nGrid\n\u2713\n-\n8.62\n16.16\nY-Net\nGrid\n\u2713\n\u2713\n8.97\n14.61\nSocial-GAN\nGAN\n-\n\u2713\n27.23\n41.44\nGoal-GAN\nGAN\n\u2713\n-\n12.20\n22.10\nMG-GAN\nGAN\n-\n\u2713\n13.60\n25.80\nCGNS\nCVAE\n-\n\u2713\n15.60\n28.20\nTrajectron++\nCVAE\n-\n\u2713\n8.98\n19.02\nPECNET\nCVAE\n\u2713\n-\n9.96\n15.88\nLB-EBM\nEnergy\n\u2713\n\u2713\n8.87\n15.61\nMID\nDiffusion\n-\n\u2713\n7.61\n14.30\nIDM\nDiffusion\n\u2713\n\u2713\n7.46\n13.83\n2) Metrics: To evaluate the effectiveness of our model,\nADE and FDE are used to measure the accuracy of predictions.\nAverage Displacement Error (ADE) represents the average\npoint-to-point Euclidean distance between the predicted tra-\njectory and the ground truth. Final Displacement Error (FDE)\ndescribes the error between the predicted endpoint and the\nrealistic endpoint. The error is measured by pixel in SDD and\nby meter in ETH/UCY. Given multiple predictions, the best-\nof-N strategy [13] is utilized to compute the final metric.\n3) Implementation Details: During the training stage, the\nbatch size is set to 256, the learning rate is set to 0.0001, \u03bb1 is\nset to 1, and \u03bb2 is set to 0.5. All experiments are conducted on\na single RTX 3090. During the prediction stage, we generate\n20 predictions for each agent and calculate the minimum\nADE/FDE based on these predictions. For each dataset, we\nrepeat the evaluation five times and report the average results.\nB. Comparison with SOTA methods\nThe results for the SDD dataset are presented in Table II.\nThe table includes all the methods along with their decoder\ntype and whether they model the uncertainty of the goal\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n9\nTABLE III\nPERFORMANCE ON ETH/UCY DATASETS\nMethods\nDeocder\nSample\nETH\nHOTEL\nUNIV\nZARA1\nZARA2\nAVG\nADE\nFDE\nADE\nFDE\nADE\nFDE\nADE\nFDE\nADE\nFDE\nADE\nFDE\nSocial-LSTM\nBG\n20\n1.09\n2.35\n0.79\n1.76\n0.67\n1.40\n0.47\n1.00\n0.56\n1.17\n0.72\n1.54\nSocial-STGCN\nBG\n20\n0.64\n1.11\n0.49\n0.85\n0.44\n0.79\n0.34\n0.53\n0.30\n0.48\n0.44\n0.75\nCausal-STGCN\nBG\n20\n0.64\n1.00\n0.38\n0.45\n0.49\n0.81\n0.34\n0.53\n0.32\n0.49\n0.43\n0.66\nSTAR\nBG\n20\n0.36\n0.65\n0.17\n0.36\n0.31\n0.62\n0.26\n0.55\n0.22\n0.46\n0.26\n0.53\nExpert-GMM\nBG\n20\n0.37\n0.65\n0.11\n0.15\n0.20\n0.44\n0.15\n0.31\n0.12\n0.26\n0.19\n0.36\nPCCSNET\nGrid\n20\n0.28\n0.54\n0.11\n0.19\n0.29\n0.60\n0.21\n0.44\n0.15\n0.34\n0.21\n0.42\nSocial-GAN\nGAN\n20\n0.81\n1.52\n0.72\n1.61\n0.60\n1.26\n0.34\n0.69\n0.42\n0.84\n0.58\n1.18\nGoal-GAN\nGAN\n20\n0.59\n1.18\n0.19\n0.35\n0.60\n1.19\n0.43\n0.87\n0.32\n0.65\n0.43\n0.85\nSocial-BiGAT\nGAN\n20\n0.69\n1.29\n0.49\n1.01\n0.55\n1.32\n0.30\n0.62\n0.36\n0.75\n0.48\n1.00\nMG-GAN\nGAN\n20\n0.47\n0.91\n0.14\n0.24\n0.54\n1.07\n0.36\n0.73\n0.29\n0.60\n0.36\n0.71\nCGNS\nCVAE\n20\n0.62\n1.40\n0.70\n0.93\n0.48\n1.22\n0.32\n0.59\n0.35\n0.71\n0.49\n0.97\nPECNET\nCVAE\n20\n0.54\n0.87\n0.18\n0.24\n0.35\n0.60\n0.22\n0.39\n0.17\n0.30\n0.29\n0.48\nTrajectron++\nCVAE\n20\n0.39\n0.83\n0.12\n0.21\n0.20\n0.44\n0.15\n0.33\n0.11\n0.25\n0.19\n0.41\nLB-EBM\nEnergy\n20\n0.30\n0.52\n0.13\n0.20\n0.27\n0.52\n0.20\n0.37\n0.15\n0.29\n0.21\n0.38\nMID\nDiffusion\n20\n0.39\n0.66\n0.13\n0.22\n0.22\n0.45\n0.17\n0.30\n0.13\n0.27\n0.21\n0.38\nIDM\nDiffusion\n20\n0.41\n0.62\n0.15\n0.25\n0.20\n0.42\n0.17\n0.28\n0.12\n0.25\n0.21\n0.36\nand trajectory. The Bivariate Gaussian (BG) models assume\nthat future positions follow a bivariate Gaussian distribution\nand utilize maximum likelihood estimation to compute the\nparameters of the distribution. The methods make strong as-\nsumptions about the trajectory distribution, which can result in\na poor fit to real data. The grid-based models divide the scene\ninto grid cells and predict the occupancy probability of each\ncell in the future. While these methods achieve outstanding\nperformance, they require significant effort to construct and\ntrain the trajectory map [23]. Generative models like GAN and\nCVAE are also used to generate future trajectories. However,\ntraining GAN-based models can be uncontrollable [16], while\nthe CVAE-based models tend to generate unnatural trajectories\n[32]. Overall, our method achieves the best performance\namong all existing methods in both metrics, with an ADE\nof 7.46 and an FDE of 13.83 in pixel coordinates. In addition,\ntwo findings can be acquired from the results. First, MID\nand IDM perform better than all other kinds of methods,\ndemonstrating that diffusion models have great potential in\ntrajectory prediction. Second, our model also outperforms\nMID. This is because MID does not consider the uncertainty\nof the goal, and can therefore lose accuracy, especially in long-\nterm prediction. Our model reduces the FDE by 0.47 compared\nto MID. Hence, it is essential to model the uncertainty of\nagents\u2019 intentions and trajectories.\nTable III presents the results on ETH/UCY dataset. The\nprediction results of IDM are comparable to those of ex-\nisting methods, with an average ADE of 0.21 and FDE of\n0.36. While IDM may not improve the MID in all datasets,\nit achieves similar performance with less time and mem-\nory requirement, as demonstrated in Sec. V-E. Furthermore,\ncompared to MID, our method can generate more natural\ntrajectories, as shown in Sec. V-C.\nC. Visualization\nIn order to investigate the diversity and accuracy of our\nmodel, we generate 20 predictions using both our model and\nMID on all datasets and visualize the predictions. Fig. 4 and\nFig. 5 display the results on SDD and ETH/UCY, respectively.\nThe black dashed line represents the historical trajectory, the\nwhite dashed line represents the ground truth future trajectory,\nand the blue solid line represents the predicted trajectory.\nBoth MID and IDM generate diverse predictions, including\nsome that closely approach the ground truth future trajectories.\nHowever, MID tends to produce some unnatural trajectories.\nFor example, on SDD datasets, a pedestrian is predicted to turn\naround in the first case and abruptly steer in the third case.\nAdditionally, the predictions generated by MID sometimes\nappear uneven as seen in the sixth case where a pedestrian\nis expected to follow a twisted trajectory. Similar results\nhave been observed in predictions on ETH/UCY datasets. In\ncontrast, IDM tends to generate smoother and more realistic\ntrajectories that resemble plausible future movements.\nTo quantify these differences, we reduce the number of\npredictions and observe the minimum ADE and FDE of\nthe two methods. The results are presented in Table IV.\nOur method consistently outperformed MID in both metrics,\nregardless of the number of predictions made. Furthermore, the\nsuperiority of our method becomes more distinct as the number\nof samples decreases. This is because MID is more prone to\npredicting unnatural trajectories that deviate significantly from\nthe ground truth. When making fewer predictions, there is a\nhigher likelihood those correct and natural predictions may be\nexcluded. Our model mitigates this problem by incorporating\nthe predicted goals into the generation of trajectories.\nWe also present the predicted trajectory distribution of MID\nand our method as contours in Fig. 6. It can be observed\nthat, compared with MID, our method predicts a smaller\nwalkable region, indicating that more implausible trajectories\ncan be excluded. Furthermore, the uncertainty of the trajectory\npredicted by our method increases with the prediction step,\nwhich aligns with our intuition. In contrast, the predictions of\nMID present high uncertainty even though in the early future.\nAdditionally, as shown in the fourth case, MID may generate\nunnatural distributions where pedestrians have a higher chance\nof turning around. Overall, by introducing intention informa-\ntion, our method enables predictions that better fit the real\ndistribution.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n10\nLittle2-27\nLittle0-27\nLittle1-106\nLittle3-27\nNexus7-7\nMID\nOurs\nquad1-27\nFig. 4. Prediction on SDD\nuniv2-8\nMID\nOurs\nzara1-8\nzara2-65\nhotel-57\neth-1118\nETH\nHotel\nUNIV\nZara1\nZara2\nFig. 5. Prediction on ETH/UCY\nTABLE IV\nADE/FDE UNDER DIFFERENT NUMBER OF PREDICTIONS\n4\n8\n12\n16\n20\nMID\n12.40/27.45\n10.11/21.30\n9.09/19.27\n8.48/16.42\n7.61/14.30\nOurs\n11.19/24.58\n9.44/19.74\n8.38/17.79\n7.85/15.80\n7.46/13.83\nD. Ablation Study\nIn this section, we begin by analyzing the role of each\ncomponent in our architecture. Next, we validate the selection\nof each component. Finally, we evaluate the impact of the step\nnumber on the prediction results.\nTable V presents the prediction results when each compo-\nnent is absent. First, the variants without the PriorNet apply\nfewer steps in the diffusion process and still consider initial\nsignals as Gaussian noise. During the training process, it does\nnot consider the prior loss in Eq. 28. This variant exhibits\nthe highest prediction error in both ADE and FDE. This\nhighlights the importance of estimating the initial distribution\nfor the diffusion process with fewer steps. The second variant\nonly models the distribution of the goal and uses MLP to\ngenerate trajectories conditioned on the given goal. The variant\nachieves a higher ADE and FDE because it ignores the\naction uncertainty, which can be modeled by another diffusion\nprocess. The third variant models the action uncertainty with\nthe trajectory diffusion process. However, the estimation of\nthe prior distribution solely relies on the context vector and\ndoes not take into account any information about the goal. Our\nmodel surpasses it by 6.8% in ADE and 10.3% in FDE. This\ndemonstrates that incorporating goal information can provide\ncrucial cues for accurately estimating the prior distribution of\nthe diffusion process.\nTo make the most of our model\u2019s efficacy, we explore\ndifferent selections for each component. Fig. 7(a) shows the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n11\nLittle2-27-0\nLittle0-27-3\nx: (-120, 140)\nY: (-160, 300)\nLittle1-106-0\nx: (-350, 200)\ny: (-100, 100)\nLittle3-27-0\nX: (-160, 50)\nY: (-150,260)\nNexus6-7-0\nX: (-60, 100)\nY: (-50, 50)\nquad1-27-0\nX: (-100, 190)\nY: (-50, 140)\nMID\nOurs\nGround Truth\nHistory\nFig. 6. Counter map of predictions\nADE\nFDE\n(a) PriorNet\nADE\nFDE\n(b) PathNet\nADE\nFDE\n(c) EndNet\nFig. 7. Selection of each component\nperformance of different PriorNet. Surprisingly, the MLP se-\nlection achieves the best performance. This suggests that, given\na fixed goal, the most probable prior trajectory distribution\nis simple, and complex networks are more susceptible to\noverfitting. For PathNet, we try similar networks, but find that\nthe complex Transformer performs better than other choices\n(Fig. 7(b)). It indicates that Transformer excels at modeling the\nuncertainty of trajectories that exhibit temporal correlation. We\nalso experiment with different layers in the MLP for EndNet\nand find that three is the most suitable number (Fig. 7(c)).\nIncreasing the number of layers does not result in improved\nperformance.\nFurthermore, we discuss the influence of step numbers on\nthe model. Fig. 9 shows the performance under different\nsteps of the goal and trajectory diffusion process. Firstly,\nincreasing the number of steps in the goal diffusion process\nresults in more precise predictions, because accurate goal\nprediction helps in estimating the prior distribution in the\ntrajectory diffusion process. Secondly, given the specific goal,\nthe prediction result is not susceptible to the number of steps in\nthe trajectory diffusion process. This is because our PriorNet\ncan predict the suitable prior distribution for different steps.\nThe best result, as reported above, is achieved when the goal\nand trajectory steps are set to 100 and 10, respectively.\nTABLE V\nPERFORMANCE INFLUENCED BY EACH COMPONENT\nEndNet\nPriorNet\nPathNet\nADE\nFDE\n\u2713\n\u2212\n\u2713\n9.40\n17.34\n\u2713\n\u2212\n\u2212\n7.85\n15.44\n\u2212\n\u2713\n\u2713\n7.98\n15.41\n\u2713\n\u2713\n\u2713\n7.46\n13.83\nE. Efficiency\nWe compare the inference speed of our model with that of\nthe conventional diffusion model. Fig. 8(a) demonstrates the\nexecution time per prediction of the two methods on different\ndatasets. Compared to MID, our method experiences nearly\na threefold decrease in inference time. This superiority stems\nfrom two reasons. Firstly, our goal diffusion process handles\n2-D locations, whereas MID handles sequences. Secondly, our\nmodel completes the trajectory diffusion process in just one-\ntenth of the steps. Additionally, in the scenes with high popu-\nlation density, the inference time further increases. Across all\ndatasets, our method achieves a prediction in less than 240ms.\nFig. 8(b) illustrates the inference time of our model with\ndifferent steps of the goal and trajectory diffusion process. As\nthe number of steps increases, the inference time demonstrates\nan approximately linear increase. Moreover, when comparing\nthe same number of steps, goal diffusion requires significantly\nless time than the trajectory diffusion process, indicating that\nreducing the target dimension is effective in shortening the\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n12\nInference time/sample(s)\n(a) Efficiency comparison for different datasets\nade/fde\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nMID\n0.42\n0.80\n1.18\n1.56\n1.91\n2.23\n2.61\n2.95\n3.34\n3.79\nOurs\n0.13\n0.24\n0.36\n0.47\n0.58\n0.70\n0.82\n0.93\n1.06\n1.15\nInference time/sample(s)\nstep in diffusion process\n(b) Efficiency under different steps \u03c4\nADE\nNumber of Model Parameters(\u00d7 106)\nMG-GAN\nOurs\nMID\nExpert\nPECNet\nTrajectron++\nY-net\n(c) Total performance under different \u03c4\nFig. 8. Inference time and model parameter\nFig. 9. Selection of diffusion steps for goal and trajectory\ntime.\nFurthermore, we also summarize the model parameters\nof our model. As shown in Fig. 8(c), our model reduces\nthe parameters by one-fifth compared to the original MID.\nHowever, the superiority of the diffusion model in trajectory\nprediction comes at the expense of more model parameters. In\nthe future, we plan to propose the development of a lightweight\ndiffusion model for trajectory prediction.\nVI. CONCLUSION\nIn this work, we design an intention-aware diffusion model\nfor multimodal trajectory prediction. The uncertainty of trajec-\ntories is decoupled into goal uncertainty and action uncertainty,\nand is modeled by two interconnected diffusion processes.\nTo improve the efficiency of the inference process, instead\nof assuming a normal prior noise distribution, we devise\na PriorNet to estimate the specific prior distribution of the\ndiffusion process, thus reducing the required number of steps\nfor a complete diffusion process. Additionally, we augment the\noriginal loss function by incorporating the estimation error of\nthe prior distribution. We conduct experiments on two real-\nworld datasets, SDD and ETH/UCY. Our method achieves\nstate-of-the-art results. Compared to the original diffusion\nmodel, our approach reduces the inference time by two-\nthirds. Furthermore, our experiments reveal the benefits of\nintroducing intention information in modeling the diffusion\nprocess with fewer steps.\nREFERENCES\n[1] Y. Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, \u201cA\nsurvey on trajectory-prediction methods for autonomous driving,\u201d IEEE\nTransactions on Intelligent Vehicles, vol. 7, no. 3, pp. 652\u2013674, 2022.\n[2] W. Zhu, Y. Liu, M. Zhang, and Y. Yi, \u201cReciprocal consistency prediction\nnetwork for multi-step human trajectory prediction,\u201d IEEE Transactions\non Intelligent Transportation Systems, 2023.\n[3] X. Wang, J. Alonso-Mora, and M. Wang, \u201cProbabilistic risk metric for\nhighway driving leveraging multi-modal trajectory predictions,\u201d IEEE\nTransactions on Intelligent Transportation Systems, vol. 23, no. 10,\npp. 19399\u201319412, 2022.\n[4] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila,\nand K. O. Arras, \u201cHuman motion trajectory prediction: A survey,\u201d The\nInternational Journal of Robotics Research, vol. 39, no. 8, pp. 895\u2013935,\n2020.\n[5] H. Hu, Q. Wang, M. Cheng, and Z. Gao, \u201cTrajectory prediction neural\nnetwork and model interpretation based on temporal pattern attention,\u201d\nIEEE Transactions on Intelligent Transportation Systems, vol. 24, no. 3,\npp. 2746\u20132759, 2022.\n[6] D. Helbing and P. Molnar, \u201cSocial force model for pedestrian dynamics,\u201d\nPhysical review E, vol. 51, no. 5, p. 4282, 1995.\n[7] H. Xue, D. Q. Huynh, and M. Reynolds, \u201cSs-lstm: A hierarchical\nlstm model for pedestrian trajectory prediction,\u201d in 2018 IEEE Winter\nConference on Applications of Computer Vision (WACV), pp. 1186\u2013\n1194, IEEE, 2018.\n[8] E. Wang, H. Cui, S. Yalamanchi, M. Moorthy, and N. Djuric, \u201cImproving\nmovement predictions of traffic actors in bird\u2019s-eye view models using\ngans and differentiable trajectory rasterization,\u201d in Proceedings of the\n26th ACM SIGKDD International Conference on Knowledge Discovery\n& Data Mining, pp. 2340\u20132348, 2020.\n[9] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid,\n\u201cVectornet: Encoding hd maps and agent dynamics from vectorized rep-\nresentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 11525\u201311533, 2020.\n[10] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y. Shen,\nY. Shen, Y. Chai, C. Schmid, et al., \u201cTnt: Target-driven trajectory\nprediction,\u201d in Conference on Robot Learning, pp. 895\u2013904, PMLR,\n2021.\n[11] V. Trentin, A. Artu\u02dcnedo, J. Godoy, and J. Villagra, \u201cMulti-modal\ninteraction-aware motion prediction at unsignalized intersections,\u201d IEEE\nTransactions on Intelligent Vehicles, 2023.\n[12] R. Huang, H. Xue, M. Pagnucco, F. Salim, and Y. Song, \u201cMultimodal\ntrajectory prediction: A survey,\u201d arXiv preprint arXiv:2302.10463, 2023.\n[13] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, \u201cSocial gan:\nSocially acceptable trajectories with generative adversarial networks,\u201d\nin Proceedings of the IEEE conference on computer vision and pattern\nrecognition, pp. 2255\u20132264, 2018.\n[14] L. A. Thiede and P. P. Brahma, \u201cAnalyzing the variety loss in the context\nof probabilistic trajectory prediction,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 9954\u20139963, 2019.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n13\n[15] N. Deo, E. Wolff, and O. Beijbom, \u201cMultimodal trajectory prediction\nconditioned on lane-graph traversals,\u201d in Conference on Robot Learning,\npp. 203\u2013212, PMLR, 2022.\n[16] K. Guo, W. Liu, and J. Pan, \u201cEnd-to-end trajectory distribution predic-\ntion based on occupancy grid maps,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 2242\u2013\n2251, 2022.\n[17] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and\nS. Savarese, \u201cSocial lstm: Human trajectory prediction in crowded\nspaces,\u201d in Proceedings of the IEEE conference on computer vision and\npattern recognition, pp. 961\u2013971, 2016.\n[18] A. Mohamed, K. Qian, M. Elhoseiny, and C. Claudel, \u201cSocial-stgcnn:\nA social spatio-temporal graph convolutional neural network for human\ntrajectory prediction,\u201d in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 14424\u201314432, 2020.\n[19] T. Salzmann, B. Ivanovic, P. Chakravarty, and M. Pavone, \u201cTrajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data,\u201d\nin Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow,\nUK, August 23\u201328, 2020, Proceedings, Part XVIII 16, pp. 683\u2013700,\nSpringer, 2020.\n[20] H. Ben-Younes, E. Zablocki, M. Chen, P. P\u00b4erez, and M. Cord, \u201cRais-\ning context awareness in motion forecasting,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 4409\u20134418, 2022.\n[21] Y. Liu, Q. Yan, and A. Alahi, \u201cSocial nce: Contrastive learning of\nsocially-aware motion representations,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 15118\u201315129, 2021.\n[22] G. Chen, J. Li, N. Zhou, L. Ren, and J. Lu, \u201cPersonalized trajectory pre-\ndiction via distribution discrimination,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 15580\u201315589, 2021.\n[23] K. Mangalam, Y. An, H. Girase, and J. Malik, \u201cFrom goals, waypoints &\npaths to long term human trajectory forecasting,\u201d in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pp. 15233\u2013\n15242, 2021.\n[24] P. Dendorfer, S. Elflein, and L. Leal-Taix\u00b4e, \u201cMg-gan: A multi-generator\nmodel preventing out-of-distribution samples in pedestrian trajectory\nprediction,\u201d in Proceedings of the IEEE/CVF International Conference\non Computer Vision, pp. 13158\u201313167, 2021.\n[25] L. Fang, Q. Jiang, J. Shi, and B. Zhou, \u201cTpnet: Trajectory proposal\nnetwork for motion prediction,\u201d in Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp. 6797\u20136806,\n2020.\n[26] V. Kosaraju, A. Sadeghian, R. Mart\u00b4\u0131n-Mart\u00b4\u0131n, I. Reid, H. Rezatofighi,\nand S. Savarese, \u201cSocial-bigat: Multimodal trajectory forecasting using\nbicycle-gan and graph attention networks,\u201d Advances in Neural Infor-\nmation Processing Systems, vol. 32, 2019.\n[27] R. Liang, Y. Li, J. Zhou, and X. Li, \u201cStglow: A flow-based generative\nframework with dual graphormer for pedestrian trajectory prediction,\u201d\narXiv preprint arXiv:2211.11220, 2022.\n[28] J. Choi, S. Kim, Y. Jeong, Y. Gwon, and S. Yoon, \u201cIlvr: Conditioning\nmethod for denoising diffusion probabilistic models,\u201d arXiv preprint\narXiv:2108.02938, 2021.\n[29] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d Advances in Neural Information Processing Systems, vol. 34,\npp. 8780\u20138794, 2021.\n[30] A. Q. Nichol and P. Dhariwal, \u201cImproved denoising diffusion prob-\nabilistic models,\u201d in International Conference on Machine Learning,\npp. 8162\u20138171, PMLR, 2021.\n[31] J. Ho and S. Ermon, \u201cGenerative adversarial imitation learning,\u201d Ad-\nvances in neural information processing systems, vol. 29, 2016.\n[32] T. Gu, G. Chen, J. Li, C. Lin, Y. Rao, J. Zhou, and J. Lu, \u201cStochastic tra-\njectory prediction via motion indeterminacy diffusion,\u201d in Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 17113\u201317122, 2022.\n[33] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet,\n\u201cVideo diffusion models,\u201d arXiv preprint arXiv:2204.03458, 2022.\n[34] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood,\n\u201cFlexible\ndiffusion\nmodeling\nof\nlong\nvideos,\u201d\narXiv\npreprint\narXiv:2205.11495, 2022.\n[35] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan,\n\u201cWavegrad: Estimating gradients for waveform generation,\u201d arXiv\npreprint arXiv:2009.00713, 2020.\n[36] Z. Kong, W. Ping, J. Huang, K. Zhao, and B. Catanzaro, \u201cDif-\nfwave: A versatile diffusion model for audio synthesis,\u201d arXiv preprint\narXiv:2009.09761, 2020.\n[37] C. Meng, Y. He, Y. Song, J. Song, J. Wu, J.-Y. Zhu, and S. Ermon,\n\u201cSdedit: Guided image synthesis and editing with stochastic differential\nequations,\u201d in International Conference on Learning Representations,\n2021.\n[38] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, \u201cHigh-\nresolution image synthesis with latent diffusion models,\u201d in Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\ntion, pp. 10684\u201310695, 2022.\n[39] A. Elnagar and K. Gupta, \u201cMotion prediction of moving objects based\non autoregressive model,\u201d IEEE Transactions on Systems, Man, and\nCybernetics-Part A: Systems and Humans, vol. 28, no. 6, pp. 803\u2013810,\n1998.\n[40] A. Barth and U. Franke, \u201cWhere will the oncoming vehicle be the next\nsecond?,\u201d in 2008 IEEE Intelligent Vehicles Symposium, pp. 1068\u20131073,\nIEEE, 2008.\n[41] Y. Cai, N. de Freitas, and J. J. Little, \u201cRobust visual tracking for multiple\ntargets,\u201d in Computer Vision\u2013ECCV 2006: 9th European Conference on\nComputer Vision, Graz, Austria, May 7-13, 2006, Proceedings, Part IV\n9, pp. 107\u2013118, Springer, 2006.\n[42] A. Elnagar, \u201cPrediction of moving objects in dynamic environments us-\ning kalman filters,\u201d in Proceedings 2001 IEEE International Symposium\non Computational Intelligence in Robotics and Automation (Cat. No.\n01EX515), pp. 414\u2013419, IEEE, 2001.\n[43] R. Schubert, E. Richter, and G. Wanielik, \u201cComparison and evaluation\nof advanced motion models for vehicle tracking,\u201d in 2008 11th interna-\ntional conference on information fusion, pp. 1\u20136, IEEE, 2008.\n[44] A. M\u00f8gelmose, M. M. Trivedi, and T. B. Moeslund, \u201cTrajectory analysis\nand prediction for improved pedestrian safety: Integrated framework and\nevaluations,\u201d in 2015 IEEE intelligent vehicles symposium (IV), pp. 330\u2013\n335, IEEE, 2015.\n[45] C. Yang, M. Bakich, and E. Blasch, \u201cNonlinear constrained tracking of\ntargets on roads,\u201d in 2005 7th International Conference on Information\nFusion, vol. 1, pp. 8\u2013pp, IEEE, 2005.\n[46] I. Batkovic, M. Zanon, N. Lubbe, and P. Falcone, \u201cA computationally\nefficient model for pedestrian motion prediction,\u201d in 2018 European\ncontrol conference (ECC), pp. 374\u2013379, IEEE, 2018.\n[47] D. Petrich, T. Dang, D. Kasper, G. Breuel, and C. Stiller, \u201cMap-based\nlong term motion prediction for vehicles in traffic environments,\u201d in 16th\nInternational IEEE Conference on Intelligent Transportation Systems\n(ITSC 2013), pp. 2166\u20132172, IEEE, 2013.\n[48] X. Yan, I. A. Kakadiaris, and S. K. Shah, \u201cModeling local behavior\nfor predicting social interactions towards human tracking,\u201d Pattern\nRecognition, vol. 47, no. 4, pp. 1626\u20131641, 2014.\n[49] I. Karamouzas, P. Heil, P. Van Beek, and M. H. Overmars, \u201cA predictive\ncollision avoidance model for pedestrian simulation,\u201d in Motion in\nGames: Second International Workshop, MIG 2009, Zeist, The Nether-\nlands, November 21-24, 2009. Proceedings 2, pp. 41\u201352, Springer, 2009.\n[50] F. Zanlungo, T. Ikeda, and T. Kanda, \u201cSocial force model with explicit\ncollision prediction,\u201d Europhysics Letters, vol. 93, no. 6, p. 68005, 2011.\n[51] H. Gong, J. Sim, M. Likhachev, and J. Shi, \u201cMulti-hypothesis motion\nplanning for visual object tracking,\u201d in 2011 International Conference\non Computer Vision, pp. 619\u2013626, IEEE, 2011.\n[52] M. Bahram, A. Lawitzky, J. Friedrichs, M. Aeberhard, and D. Woll-\nherr, \u201cA game-theoretic approach to replanning-aware interactive scene\nprediction and planning,\u201d IEEE Transactions on Vehicular Technology,\nvol. 65, no. 6, pp. 3981\u20133992, 2015.\n[53] Y. F. Chen, M. Liu, M. Everett, and J. P. How, \u201cDecentralized non-\ncommunicating multiagent collision avoidance with deep reinforcement\nlearning,\u201d in 2017 IEEE international conference on robotics and\nautomation (ICRA), pp. 285\u2013292, IEEE, 2017.\n[54] C. R\u00a8osmann, F. Hoffmann, and T. Bertram, \u201cTimed-elastic-bands for\ntime-optimal point-to-point nonlinear model predictive control,\u201d in 2015\neuropean control conference (ECC), pp. 3352\u20133357, IEEE, 2015.\n[55] F. Previtali, A. Bordallo, L. Iocchi, and S. Ramamoorthy, \u201cPredict-\ning future agent motions for dynamic environments,\u201d in 2016 15th\nIEEE International Conference on Machine Learning and Applications\n(ICMLA), pp. 94\u201399, IEEE, 2016.\n[56] A. Kuefler, J. Morton, T. Wheeler, and M. Kochenderfer, \u201cImitating\ndriver behavior with generative adversarial networks,\u201d in 2017 IEEE\nIntelligent Vehicles Symposium (IV), pp. 204\u2013211, IEEE, 2017.\n[57] Y. Li, J. Song, and S. Ermon, \u201cInfogail: Interpretable imitation learning\nfrom visual demonstrations,\u201d Advances in Neural Information Process-\ning Systems, vol. 30, 2017.\n[58] I. Sutskever, O. Vinyals, and Q. V. Le, \u201cSequence to sequence learning\nwith neural networks,\u201d Advances in neural information processing\nsystems, vol. 27, 2014.\n[59] C. Zhang, Z. Ni, and C. Berger, \u201cSpatial-temporal-spectral lstm: A trans-\nferable model for pedestrian trajectory prediction,\u201d IEEE Transactions\non Intelligent Vehicles, 2023.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021\n14\n[60] S. Li, Y. Zhou, J. Yi, and J. Gall, \u201cSpatial-temporal consistency network\nfor low-latency trajectory forecasting,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pp. 1940\u20131949, 2021.\n[61] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, \u201cMultipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction,\u201d arXiv\npreprint arXiv:1910.05449, 2019.\n[62] I. Bae, J.-H. Park, and H.-G. Jeon, \u201cLearning pedestrian group repre-\nsentations for multi-modal trajectory prediction,\u201d in Computer Vision\u2013\nECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327,\n2022, Proceedings, Part XXII, pp. 270\u2013289, Springer, 2022.\n[63] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n\u201cAttention based vehicle trajectory prediction,\u201d IEEE Transactions on\nIntelligent Vehicles, vol. 6, no. 1, pp. 175\u2013185, 2020.\n[64] K. Zhang, L. Zhao, C. Dong, L. Wu, and L. Zheng, \u201cAi-tp: Attention-\nbased interaction-aware trajectory prediction for autonomous driving,\u201d\nIEEE Transactions on Intelligent Vehicles, vol. 8, no. 1, pp. 73\u201383, 2022.\n[65] Z. Li, Y. Wang, and Z. Zuo, \u201cInteraction-aware prediction for cut-\nin trajectories with limited observable neighboring vehicles,\u201d IEEE\nTransactions on Intelligent Vehicles, 2023.\n[66] C. Xu, M. Li, Z. Ni, Y. Zhang, and S. Chen, \u201cGroupnet: Multiscale\nhypergraph neural networks for trajectory prediction with relational\nreasoning,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 6498\u20136507, 2022.\n[67] L. Shi, L. Wang, C. Long, S. Zhou, M. Zhou, Z. Niu, and G. Hua, \u201cSgcn:\nSparse graph convolution network for pedestrian trajectory prediction,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 8994\u20139003, 2021.\n[68] H. Song, D. Luan, W. Ding, M. Y. Wang, and Q. Chen, \u201cLearning to\npredict vehicle trajectories with model-based planning,\u201d in Conference\non Robot Learning, pp. 1035\u20131045, PMLR, 2022.\n[69] Y. Chen, B. Ivanovic, and M. Pavone, \u201cScept: Scene-consistent, policy-\nbased trajectory predictions for planning,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 17103\u201317112, 2022.\n[70] T. Phan-Minh, E. C. Grigore, F. A. Boulton, O. Beijbom, and E. M.\nWolff, \u201cCovernet: Multimodal behavior prediction using trajectory sets,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 14074\u201314083, 2020.\n[71] L. F. Chiara, P. Coscia, S. Das, S. Calderara, R. Cucchiara, and L. Ballan,\n\u201cGoal-driven self-attentive recurrent networks for trajectory prediction,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 2518\u20132527, 2022.\n[72] J. Wang, T. Ye, Z. Gu, and J. Chen, \u201cLtp: Lane-based trajectory\nprediction for autonomous driving,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 17134\u2013\n17142, 2022.\n[73] J. Gu, C. Sun, and H. Zhao, \u201cDensetnt: End-to-end trajectory prediction\nfrom dense goal sets,\u201d in Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 15303\u201315312, 2021.\n[74] W. Mao, C. Xu, Q. Zhu, S. Chen, and Y. Wang, \u201cLeapfrog diffu-\nsion model for stochastic trajectory prediction,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 5517\u20135526, 2023.\n[75] X. L. Li, J. Thickstun, I. Gulrajani, P. Liang, and T. B. Hashimoto,\n\u201cDiffusion-lm improves controllable text generation,\u201d arXiv preprint\narXiv:2205.14217, 2022.\n[76] Y. Tashiro, J. Song, Y. Song, and S. Ermon, \u201cCsdi: Conditional score-\nbased diffusion models for probabilistic time series imputation,\u201d Ad-\nvances in Neural Information Processing Systems, vol. 34, pp. 24804\u2013\n24816, 2021.\n[77] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d in\nInternational Conference on Machine Learning, pp. 2256\u20132265, PMLR,\n2015.\n[78] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, Y. Shao,\nW. Zhang, B. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive\nsurvey of methods and applications,\u201d arXiv preprint arXiv:2209.00796,\n2022.\n[79] D. J. Rezende, S. Mohamed, and D. Wierstra, \u201cStochastic backprop-\nagation and approximate inference in deep generative models,\u201d in\nInternational conference on machine learning, pp. 1278\u20131286, PMLR,\n2014.\n[80] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial networks,\u201d\nCommunications of the ACM, vol. 63, no. 11, pp. 139\u2013144, 2020.\n[81] H. Cao, C. Tan, Z. Gao, G. Chen, P.-A. Heng, and S. Z. Li, \u201cA survey\non generative diffusion model,\u201d arXiv preprint arXiv:2209.02646, 2022.\n[82] C. Luo, \u201cUnderstanding diffusion models: A unified perspective,\u201d arXiv\npreprint arXiv:2208.11970, 2022.\n[83] I. Bae, J.-H. Park, and H.-G. Jeon, \u201cNon-probability sampling network\nfor stochastic human trajectory prediction,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 6477\u20136487, 2022.\n",
    "2301.00493": "Argoverse 2: Next Generation Datasets for\nSelf-Driving Perception and Forecasting\nBenjamin Wilson\u2217\u2020,1, William Qi\u2217\u2020, Tanmay Agarwal\u2217\u2020, John Lambert\u2020, Jagjeet Singh\u2020,\nSiddhesh Khandelwal2, Bowen Pan\u2020,3, Ratnesh Kumar\u2020, Andrew Hartnett\u2020,\nJhony Kaesemodel Pontes\u2020, Deva Ramanan\u2020,4, Peter Carr\u2020, James Hays\u2020,1\n1Georgia Tech, 2UBC, 3MIT, 4CMU\nAbstract\nWe introduce Argoverse 2 (AV2) \u2014 a collection of three datasets for perception and\nforecasting research in the self-driving domain. The annotated Sensor Dataset con-\ntains 1,000 sequences of multimodal data, encompassing high-resolution imagery\nfrom seven ring cameras, and two stereo cameras in addition to lidar point clouds,\nand 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26\nobject categories, all of which are suf\ufb01ciently-sampled to support training and\nevaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences\nof unlabeled lidar point clouds and map-aligned pose. This dataset is the largest\never collection of lidar sensor data and supports self-supervised learning and the\nemerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset\ncontains 250,000 scenarios mined for interesting and challenging interactions be-\ntween the autonomous vehicle and other actors in each local scene. Models are\ntasked with the prediction of future motion for \u201cscored actors\" in each scenario\nand are provided with track histories that capture object location, heading, velocity,\nand category. In all three datasets, each scenario contains its own HD Map with 3D\nlane and crosswalk geometry \u2014 sourced from data captured in six distinct cities.\nWe believe these datasets will support new and existing machine learning research\nproblems in ways that existing datasets do not. All datasets are released under the\nCC BY-NC-SA 4.0 license.\n1\nIntroduction\nIn order to achieve the goal of safe, reliable autonomous driving, a litany of machine learning tasks\nmust be addressed, from stereo depth estimation to motion forecasting to 3D object detection. In recent\nyears, numerous high quality self-driving datasets have been released to support research into these and\nother important machine learning tasks. Many datasets are annotated \u201csensor\u201d datasets [4, 45, 39, 40,\n24, 33, 18, 14, 41, 36] in the spirit of the in\ufb02uential KITTI dataset [17]. The Argoverse 3D Tracking\ndataset [6] was the \ufb01rst such dataset with \u201cHD maps\u201d \u2014 maps containing lane-level geometry. Also\nin\ufb02uential are self-driving \u201cmotion prediction\u201d datasets [12, 22, 34, 4, 52] \u2014 containing abstracted\nobject tracks instead of raw sensor data \u2014 of which the Argoverse Motion Forecasting dataset [6]\nwas the \ufb01rst.\nIn the last two years, the Argoverse team has hosted six competitions on 3D tracking, stereo depth\nestimation, and motion forecasting. We maintain evaluation servers and leaderboards for these tasks,\n*Equal contribution.\n\u2020Work completed while at Argo AI.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks.\narXiv:2301.00493v1  [cs.CV]  2 Jan 2023\nas well as 3D detection. The leaderboards collectively contain thousands of submissions from four\nhundred teams1. We also maintain the Argoverse API and have addressed more than one hundred\nissues2. From these experiences we have formed the following guiding principles to guide the creation\nof the next iteration of Argoverse datasets.\n1. Bigger isn\u2019t always better. Self-driving vehicles capture a \ufb02ood of sensor data which is logisti-\ncally dif\ufb01cult to work with. Sensor datasets are several terabytes in size, even when compressed.\nIf standard benchmarks grow further, we risk alienating much of the academic community and\nleaving progress to well-resourced industry groups. For this reason, we match but do not exceed\nthe scale of sensor data in nuScenes [4] and Waymo Open [45].\n2. Make every instance count. Much of driving is boring. Datasets should focus on the dif\ufb01cult,\ninteresting scenarios where current forecasting and perception systems struggle. Therefore we\nmine for especially crowded, dynamic, and kinematically unusual scenarios.\n3. Diversity matters. Training on data from wintertime Detroit is not suf\ufb01cient for detecting objects\nin Miami \u2014 Miami has 15 times the frequency of motorcycles and mopeds. Behaviors differ\nas well, so learned pedestrian motion behavior might not generalize. Accordingly, each of our\ndatasets are drawn from six diverse cities \u2014 Austin, Detroit, Miami, Palo Alto, Pittsburgh, and\nWashington D.C. \u2014 and different seasons, as well, from snowy to sunny.\n4. Map the world. HD maps are powerful priors for perception and forecasting. Learning-based\nmethods that found clever ways to encode map information [31] performed well in Argoverse\ncompetitions. For this reason, we augment our HD map representation with 3D lane geometry,\npaint markings, crosswalks, higher resolution ground height, and more.\n5. Self-supervise. Other machine learning domains have seen enormous success from self-supervised\nlearning in recent years. Large-scale lidar data from dynamic scenes, paired with HD maps, could\nlead to better representations than current supervised approaches. For this reason, we build the\nlargest dataset of lidar sensor data.\n6. Fight the heavy tail. Passenger vehicles are common, and thus we can assess our forecasting\nand detection accuracy for cars. However, with existing datasets, we cannot assess forecasting\naccuracy for buses and motorcycles with their distinct behaviors, nor can we evaluate stroller and\nwheel chair detection. Thus we introduce the largest taxonomy to date for sensor and forecasting\ndatasets, and we ensure enough samples of rare objects to train and evaluate models.\nWith these guidelines in mind we built the three Argoverse 2 (AV2) datasets. Below, we highlight\nsome of their contributions.\n1. The 1,000 scenario Sensor dataset has the largest self-driving taxonomy to date \u2013 30 categories.\n26 categories contain at least 6,000 cuboids to enable diverse taxonomy training and testing. The\ndataset also has stereo imagery, unlike recent self-driving datasets.\n2. The 20,000 scenario Lidar dataset is the largest dataset for self-supervised learning on lidar. The\nonly similar dataset, concurrently developed ONCE [36], does not have HD maps.\n3. The 250,000 scenario Motion Forecasting Dataset has the largest taxonomy \u2013 5 types of dynamic\nactors and 5 types of static actors \u2013 and covers the largest mapped area of any such dataset.\nWe believe these datasets will support research into problems such as 3D detection, 3D tracking,\nmonocular and stereo depth estimation, motion forecasting, visual odometry, pose estimation, lane\ndetection, map automation, self-supervised learning, structure from motion, scene \ufb02ow, optical \ufb02ow,\ntime to contact estimation, and point cloud forecasting.\n2\nRelated Work\nThe last few years have seen rapid progress in self-driving perception and forecasting research,\ncatalyzed by many high quality datasets.\nSensor datasets and 3D Object Detection and Tracking. New sensor datasets for 3D object\ndetection [4, 45, 39, 40, 24, 33, 18, 14, 41, 36] have led to in\ufb02uential detection methods such as\n1This count includes private submissions not posted to the public leaderboards.\n2https://github.com/argoverse/argoverse-api\n2\nanchor-based approaches like PointPillars [27], and more recent anchor-free approaches such as\nAFDet [16] and CenterPoint [51]. These methods have led to dramatic accuracy improvements on all\ndatasets. In turn, these improvements have made isolation of object-speci\ufb01c point clouds possible,\nwhich has proven invaluable for offboard detection and tracking [42], and for simulation [8], which\npreviously required human-annotated 3D bounding boxes [35]. New approaches explore alternate\npoint cloud representations, such as range images [5, 2, 46]. Streaming perception [29, 21] introduces\na paradigm to explore the tradeoff between accuracy and latency. A detailed comparison between the\nAV2 Sensor Dataset and recent 3D object detection datasets is provided in Table 1.\nMotion Forecasting. For motion forecasting, the progress has been just as signi\ufb01cant. A transition\nto attention-based methods [28, 38, 37] has led to a variety of new vector-based representations for\nmap and trajectory data [15, 31]. New datasets have also paved the way for new algorithms, with\nnuScenes [4], Lyft L5 [22], and the Waymo Open Motion Dataset [12] all releasing lane graphs\nafter they proved to be essential in Argoverse 1 [6]. Lyft also introduced traf\ufb01c/speed control data,\nwhile Waymo added crosswalk polygons, lane boundaries (with marking type), speed limits, and stop\nsigns to the map. More recently, Yandex has released the Shifts [34] dataset, which is the largest (by\nscenario hours) collection of forecasting data available to date. Together, these datasets have enabled\nexploration of multi-actor, long-range motion forecasting leveraging both static and dynamic maps.\nFollowing upon the success of Argoverse 1.1, we position AV2 as a large-scale repository of high-\nquality motion forecasting scenarios - with guarantees on data frequency (exactly 10 Hz) and diversity\n(>2000 km of unique roadways covered across 6 cities). This is in contrast to nuScenes (reports data\nat just 2 Hz) and Lyft (collected on a single 10 km segment of road), but is complementary to Waymo\nOpen Motion Dataset (employs a similar approach for scenario mining and data con\ufb01guration).\nComplementary datasets are essential for these safety critical problems as they provide opportunities\nto evaluate generalization and explore transfer learning. To improve ease of use, we have also\ndesigned AV2 to be widely accessible both in terms of data size and format \u2014 a detailed comparison\nvs. other recent forecasting datasets is provided in Table 2.\nBroader Problems of Perception for Self-Driving. Aside from the tasks of object detection and\nmotion forecasting, new, large-scale sensor datasets for self-driving present opportunities to explore\ndozens of new problems for perception, especially those that can be potentially solved via self-\nsupervision. A number of new problems have been recently proposed; real-time 3D semantic\nsegmentation in video has received attention thanks to SemanticKITTI [1]. HD map automation\n[54, 30] and HD map change detection [26] have received additional attention, along with 3D\nscene \ufb02ow and pixel-level scene simulation [50, 8]. Datasets exist with unique modalities such as\nthermal imagery [10, 9]. Our new Lidar Dataset enables large-scale self-supervised training of new\napproaches for freespace forecasting [23] or point cloud forecasting [48, 49].\n3\nThe Argoverse 2 Datasets\n3.1\nSensor Dataset\nThe Argoverse 2 Sensor Dataset is the successor to the Argoverse 1 3D Tracking Dataset. AV2 is\nlarger, with 1,000 scenes, up from 113 in Argoverse 1, but each AV2 scene is also richer \u2013 there\nare 23x as many non-vehicle, non-pedestrian cuboids in AV2. The constituent 30 s scenarios in\nthe Argoverse 2 Sensor Dataset were manually selected by the authors to contain crowded scenes\nwith under-represented objects, noteworthy weather, and interesting behaviors, e.g., cut ins and\njaywalking. Each scenario is \ufb01fteen seconds in duration. Table 1 compares the AV2 Sensor Dataset\nwith a selection of self-driving datasets. Figures 1, 2, and 3 plot how the scenarios of AV2 compare\nfavorably to other datasets in terms of annotation range, object diversity, object density, and scene\ndynamism.\nThe most similar sensor dataset to ours is the highly in\ufb02uential nuScenes [4] \u2013 both datasets have\n1,000 scenarios and HD maps, although Argoverse is unique in having ground height maps. nuScenes\ncontains radar data while AV2 contains stereo imagery. nuScenes has a large taxonomy \u2013 twenty-three\nobject categories of which ten have suitable data for training and evaluation. Our dataset contains\nthirty object categories of which twenty-six are well sampled enough for training and evaluation.\nnuScenes spans two cities, while our proposed dataset spans six.\n3\nTable 1: Comparison of the Argoverse 2 Sensor and Lidar datasets with other sensor datasets.\nName\n# Scenes\nCities\nLidar?\n# Cameras\nStereo\nHD Maps?\n# Classes\n# Evaluated Classes\nArgoverse 1 [6]\n113\n2\n\u2713\n7\n\u2713\n\u2713\n15\n3\nKITTI [17]\n22\n1\n\u2713\n2\n\u2713\n3\n3\nnuScenes [4]\n1,000\n2\n\u2713\n6\n\u2713\n23\n10\nONCE [36]\n581\n\u2013\n\u2713\n7\n5\n3\nWaymo Open [45]\n1,150\n3\n\u2713\n5\n4\n4\nArgoverse 2 Sensor\n1,000\n6\n\u2713\n9\n\u2713\n\u2713\n30\n26\nArgoverse 2 Lidar\n20,000\n6\n\u2713\n-\n\u2713\n-\n-\nRegular vehicle\nPedestrian\nBollard\nConstruction cone\nConstruction barrel\nStop sign\nBicycle\nLarge vehicle\nWheeled device\nBus\nBox truck\nSign\nTruck\nMotorcycle\nBicyclist\nVehicular trailer\nTruck cab\nMotorcyclist\nDog\nSchool bus\nWheeled rider\nStroller\nArticulated bus\nMessage board trailer\nMobile pedestrian sign\nWheelchair\nRailed vehicle\nOfficial signaler\nTraffic light trailer\nAnimal\n100\n2\n5\n1000\n2\n5\n10k\n2\n5\n100k\n2\n5\n1M\n2\n5\n10M\nArgoverse 1\nArgoverse 2\nnuScenes\nONCE\nWaymo Open\nNumber of 3D cuboids\nFigure 1: Number of annotated 3D cuboids per category for Argoverse 1 3D Tracking, Argoverse\n2 Sensor Dataset, nuScenes, ONCE, and Waymo Open. The nuScenes annotation rate is 2 Hz,\ncompared to 10 Hz for Argoverse, but that does not account for the relative increase in object diversity\nin Argoverse 2.\nSensor Suite.\nLidar sweeps are collected at 10 Hz, along with 20 fps imagery from 7 cameras\npositioned to provide a fully panoramic \ufb01eld of view. In addition, camera intrinsics, extrinsics and\n6-DOF ego-vehicle pose in a global coordinate system are provided. Lidar returns are captured by\ntwo 32-beam lidars, spinning at 10 Hz in the same direction, but separated in orientation by 180\u00b0.\nThe cameras trigger in-sync with both lidars, leading to a 20 Hz frame-rate. The seven global shutter\ncameras are synchronized to the lidar to have their exposure centered on the lidar sweeping through\ntheir \ufb01elds of view. In the Appendix, we provide a a schematic \ufb01gure illustrating the car sensor suite\nand its coordinate frames.\nLidar synchronization accuracy. In AV2, we improve the synchronization of cameras and lidars\nsigni\ufb01cantly over Argoverse 1. Our synchronization accuracy is within [\u22121.39, 1.39] ms, which\ncompares favorably to the Waymo Open Dataset, which is reported as [\u22126, 7] ms [45].\nAnnotations. The AV2 Sensor Dataset contains 10 Hz 3D cuboid annotations for objects within our\n30 class taxonomy (Figure 1). Cuboids have track identi\ufb01ers that are consistent over time for the\n0\n50\n100\n150\n200\n250\n0\n50k\n100k\n150k\n200k\n250k\nWaymo Open\nArgoverse 2\nnuScenes\nArgoverse 1\nONCE\nRange (m)\nNumber of 3D cuboids\n0\n50\n100\n150\n200\n250\n300\n0\n500\n1000\n1500\n2000\n2500\nWaymo Open\nArgoverse 2\nnuScenes\nArgoverse 1\nONCE\nNumber of 3D cuboids\nNumber of lidar frames\nFigure 2: Left: Number of annotated 3D cuboids by range in the Argoverse 2 Sensor Dataset. About\n14% of the Argoverse 2 cuboids are beyond 75 m \u2013 Waymo Open, nuScenes, and ONCE have less\nthan 1%. Right: Number of 3D cuboids per lidar frame. Argoverse 2 has an average of 75 3D\ncuboids per lidar frame \u2013 Waymo Open has an average of 61, nuScenes 33, and ONCE 30.\n4\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9 10 11 12 13 14 15 16 17 18 19\n0\n20k\n40k\n60k\n80k\n100k\nWaymo Open\nArgoverse 2\nArgoverse 1\nnuScenes\nONCE\nNumber of different categories\nNumber of lidar frames\n5\n10\n15\n20\n25\n0\n20k\n40k\n60k\n80k\nArgoverse 2\nWaymo Open\nArgoverse 1\nnuScenes\nSpeed (m/s)\nNumber of 3D vehicle cuboids \n (speed > 0.5 m/s)\nFigure 3: Left: Number of annotated categories per lidar frame in the Argoverse 2 Sensor Dataset.\nPer scene, Argoverse 2 is about 2\u00d7 more diverse than Argoverse 1 and 2.3\u00d7 more diverse than\nWaymo Open. Right: Speed distribution for the vehicle category. We consider only moving vehicles\nwith speeds greater than 0.5 m/s. Argoverse 2 has about 1.3\u00d7 more moving vehicles than Waymo\nOpen. About 28% of the vehicles in Argoverse 2 are moving with an average speed of 7.27 m/s. We\ndid not compare against the ONCE dataset because it does not provide tracking information for the\n3D cuboids.\nsame object instance. Objects are annotated if they are within the \u201cregion of interest\u201d (ROI) \u2013 within\n\ufb01ve meters of the mapped \u201cdriveable\u201d area.\nPrivacy. All faces and license plates, whether inside vehicles or outside of the driveable area, are\nblurred extensively to preserve privacy.\nSensor Dataset splits. We randomly partition the dataset with train, validation, and test splits of 700,\n150, and 150 scenarios, respectively.\n3.2\nLidar Dataset\nThe Argoverse 2 Lidar Dataset is intended to support research into self-supervised learning in the\nlidar domain as well as point cloud forecasting [48, 49]. Because lidar data is more compact than the\nfull sensor suite, we can include double-length scenarios (30 s instead of 15 s), and far more \u2013 20,000\ninstead of 1,000 \u2013 equating to roughly 40x as many driving hours, for 5x the space budget. The AV2\nLidar Dataset is mined with the same criteria as the Forecasting Dataset (Section 3.3.2) to ensure that\neach scene is interesting. While the Lidar Dataset does not have 3D object annotations, each scenario\ncarries an HD map with rich, 3D information about the scene.\nOur dataset is the largest such collection to date with 20,000 thirty second sequences. The only\nsimilar dataset, concurrently released ONCE [36], contains 1 M lidar frames compared to 6 M lidar\nframes in ours. Our dataset is sampled at 10 Hz instead of 2 Hz, as in ONCE, making our dataset\nmore suitable for point cloud forecasting or self-supervision tasks where point cloud evolution over\ntime is important.\nLidar Dataset splits. We randomly partition the dataset with train, validation, and test splits of\n16,000, 2,000, and 2,000 scenarios, respectively.\n3.3\nMotion Forecasting Dataset\nMotion forecasting addresses the problem of predicting future states (or occupancy maps) for dynamic\nactors within a local environment. Some examples of relevant actors for autonomous driving include:\nvehicles (both parked and moving), pedestrians, cyclists, scooters, and pets. Predicted futures\ngenerated by a forecasting system are consumed as the primary inputs in motion planning, which\nconditions trajectory selection on such forecasts. Generating these forecasts presents a complex,\nmulti-modal problem involving many diverse, partially-observed, and socially interacting agents.\nHowever, by taking advantage of the ability to \u201cself-label\u201d data using observed ground truth futures,\nmotion forecasting becomes an ideal domain for application of machine learning.\nBuilding upon the success of Argoverse 1, the Argoverse 2 Motion Forecasting dataset provides\nan updated set of prediction scenarios collected from a self-driving \ufb02eet. The design decisions\nenumerated below capture the collective lessons learned from both our internal research/development,\n5\nTable 2: Comparison between the Argoverse 2 Motion Forecasting dataset and other recent motion\nforecasting datasets. Hyphens \"-\" indicate that attributes are either not applicable, or not available.\nWe de\ufb01ne \u201cmined for interestingness\u201d to be true if interesting scenarios/actors are mined after data\ncollection, instead of taking all/random samples. \u2020 Public leaderboard counts as retrieved on Aug. 27,\n2021.\nARGOVERSE [6]\nINTER [52]\nLYFT [22]\nWAYMO [12]\nNUSCENES [4]\nYANDEX [34]\nOURS\n# SCENARIOS\n324k\n-\n170k\n104k\n41k\n600k\n250k\n# UNIQUE TRACKS\n11.7M\n40k\n53.4M\n7.6M\n-\n17.4M\n13.9M\nAVERAGE TRACK LENGTH\n2.48 s\n19.8 s\n1.8 s\n7.04 s\n-\n-\n5.16 s\nTOTAL TIME\n320 h\n16.5 h\n1118 h\n574 h\n5.5 h\n1667 h\n763 h\nSCENARIO DURATION\n5 s\n-\n25 s\n9.1 s\n8 s\n10 s\n11 s\nTEST FORECAST HORIZON\n3 s\n3 s\n5 s\n8 s\n6 s\n5 s\n6 s\nSAMPLING RATE\n10 Hz\n10 Hz\n10 Hz\n10 Hz\n2 Hz\n5 Hz\n10 Hz\n# CITIES\n2\n6\n1\n6\n2\n6\n6\nUNIQUE ROADWAYS\n290 km\n2 km\n10 km\n1750 km\n-\n-\n2220 km\nAVG. # TRACKS PER SCENARIO\n50\n-\n79\n-\n75\n29\n73\n# EVALUATED OBJECT CATEGORIES\n1\n1\n3\n3\n1\n2\n5\nMULTI-AGENT EVALUATION\n\u00d7\n\u2713\n\u2713\n\u2713\n\u00d7\n\u2713\n\u2713\nMINED FOR INTERESTINGNESS\n\u2713\n\u00d7\n-\n\u2713\n\u00d7\n\u00d7\n\u2713\nVECTOR MAP\n\u2713\n\u00d7\n\u00d7\n\u2713\n\u2713\n\u00d7\n\u2713\nDOWNLOAD SIZE\n4.8 GB\n-\n22 GB\n1.4 TB\n48 GB\n120 GB\n58 GB\n# PUBLIC LEADERBOARD ENTRIES\u2020\n194\n-\n935\n23\n18\n3\n-\nas well as feedback from more than 2,700 submissions by nearly 260 unique teams3 across 3\ncompetitions [43]:\n1. Motion forecasting is a safety critical system in a long-tailed domain. Consequently, our\ndataset is biased towards diverse and interesting scenarios containing different types of focal\nagents (see section 3.3.2). Our goal is to encourage the development of methods that ensure safety\nduring tail events, rather than to optimize the expected performance on \u201ceasy miles\u201d.\n2. There is a \u201cGoldilocks zone\u201d of task dif\ufb01culty. Performance on the Argoverse 1 test set has\nbegun to plateau, as shown in Figure 10 of the appendix. Argoverse 2 is designed to increase\nprediction dif\ufb01culty incrementally, spurring productive focused research for the next few years.\nThese changes are intended to incentivize methods that perform well on extended forecast horizons\n(3 s \u21926 s), handle multiple types of dynamic objects (1 \u21925), and ensure safety in scenarios\nfrom the long tail. Future Argoverse releases could continue to increase the problem dif\ufb01culty by\nreducing observation windows and increasing forecasting horizons.\n3. Usability matters. Argoverse 1 bene\ufb01ted from a large and active research community\u2014in large\npart due to the simplicity of setup and usage. Consequently, we took care to ensure that existing\nArgoverse models can be easily ported to run on Argoverse 2. In particular, we have prioritized\nintuitive access to map elements, encouraging methods which use the lane graph as a strong prior.\nTo improve training and generalization, all poses have also been interpolated and resampled at\nexactly 10 Hz (Argoverse 1 was approximate). The new dataset includes fewer, but longer and\nmore complex scenarios; this ensures that total dataset size remains large enough to train complex\nmodels but small enough to be readily accessible.\n3.3.1\nData Representation\nThe dataset consists of 250,000 non-overlapping scenarios (80/10/10 train/val/test random splits)\nmined from six unique urban driving environments in the United States. It contains a total of 10\nobject types, with 5 from each of the dynamic and static categories (see Figure 4). Each scenario\nincludes a local vector map and 11 s (10 Hz) of trajectory data (2D position, velocity, and orientation)\nfor all tracks observed by the ego-vehicle in the local environment. The \ufb01rst 5 s of each scenario is\ndenoted as the observed window, while the subsequent 6 s is denoted as the forecasted horizon.\nWithin each scenario, we mark a single track as the \u201cfocal agent\u201d. Focal tracks are guaranteed to\nbe fully observed throughout the duration of the scenario and have been speci\ufb01cally selected to\nmaximize interesting interactions with map features and other nearby actors (see Section 3.3.2). To\nevaluate multi-agent forecasting, we also mark a subset of tracks as \u201cscored actors\u201d (as shown in\nFigure 5), with guarantees for scenario relevance and minimum data quality.\n3This count includes private submissions not posted to the public leaderboards.\n6\nFigure 4: Object type and geographic histograms for the Motion Forecasting Dataset. Left: Histogram\nof object types over the \u201cfocal\u201d and \u201cscored\u201d categories. Center: Histogram of object types over all\ntracks present in the dataset. The \ufb01ne grained distinctions between different static object types (e.g.\nConstruction Cone vs Riderless Bicycle) are unique among forecasting datasets. Right: Histogram of\nmetropolitan areas included in the dataset.\nFigure 5: Visualization of a few interesting scenarios from the Motion Forecasting Dataset. The\nscenarios demonstrate a mix of the various object types (Vehicle, Pedestrian, Bus, Cyclist, or Motor-\ncyclist). The ego-vehicle is indicated in green, the focal agent is purple, and scored actors are orange.\nOther un-scored tracks are shown in blue. Object positions are captured at the last timestep of the\nobserved history. For visualization purposes the full 5 s history and 6 s future are rendered for the\nfocal agent, while only 1.5 s of future are shown for the other scored actors. Left shows a pedestrian\ncrossing in front of the ego-vehicle, while center and right depict a motorcyclist weaving through\ntraf\ufb01c.\n3.3.2\nMining Interesting Scenarios\nThe source data for Argoverse 2 was drawn from \ufb02eet logs tagged with annotations consistent\nwith interesting or dif\ufb01cult-to-forecast events. Each log was trimmed to 30 s and run through an\ninterestingness scoring module in order to bias data selection towards examples from the long-tail of\nthe natural distribution. We employ heuristics to score each track in the scene across \ufb01ve dimensions:\nobject category, kinematics, map complexity, social context, and relation to the ego-vehicle (details\nin Appendix).\nThe \ufb01nal scenarios are generated by extracting non-overlapping 11 s windows where at least one\ncandidate track is fully observed for the entire duration. The highest scoring candidate track is\ndenoted as the \u201cfocal agent\u201d; all other fully observed tracks within 30 m of the ego-vehicle are\ndenoted as \u201cscored actors\u201d. The resulting dataset is diverse, challenging, and still right-sized for\nwidespread use (see the download size in Table 2). In Figure 6, we show that the resulting dataset is\nsigni\ufb01cantly more interesting than Argoverse 1.1 and validate our intuition that actors scoring highly\nin our heuristic module are more challenging to accurately forecast.\n3.4\nHD Maps\nEach scenario in the three datasets described above shares the same HD map representation. Each\nscenario carries its own local map region, similar to the Waymo Open Motion [12] dataset. This\nis a departure from the original Argoverse datasets in which all scenarios were localized onto two\ncity-scale maps\u2014one for Pittsburgh and one for Miami. In the Appendix, we provide examples.\n7\n0\n2\n4\n6\n8\n10\n12\n14\nTotal Interestingness Score\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nMiss Rate (K=6)\nFitted Regression Model\nBin Centers (1000 Scenarios Each)\nFigure 6: Left: Histogram comparing the distribution of interestingness scores assigned to focal\nagents in both Argoverse 1.1 and 2. Right: Plot showing the relationship between total interestingness\nscore and prediction dif\ufb01culty on the Argoverse 2 test split. We evaluate WIMP [25] over each\nscenario and \ufb01t a regression model to the computed miss rate (K=6, 2m threshold).\nAdvantages of per-scenario maps include more ef\ufb01cient queries and their ability to handle map\nchanges. A particular intersection might be observed multiple times in our datasets, and there could\nbe changes to the lanes, crosswalks, or even ground height in that time.\nLane graph. The core feature of the HD map is the lane graph, consisting of a graph G = (V, E),\nwhere V are individual lane segments. In the Appendix, we enumerate and de\ufb01ne the attributes we\nprovide for each lane segment. Unlike Argoverse 1, we provide the actual 3D lane boundaries, instead\nof only centerlines. However, our API provides code to quickly infer the centerlines at any desired\nsampling resolution. Polylines are quantized to 1 cm resolution. Our representation is richer than\nnuScenes, which provides lane geometry only in 2D, not 3D.\nDriveable area. Instead of providing driveable area segmentation in a rasterized format, as we did in\nArgoverse 1, we release it in a vector format, i.e. as 3D polygons. This offers multiple advantages,\nchie\ufb02y in compression, allowing us to store separate maps for tens of thousands of scenarios, yet the\nraster format is still easily derivable. The polygon vertices are quantized to 1 cm resolution.\nGround surface height. Only the sensor dataset includes a dense ground surface height map\n(although other datasets still have sparse 3D height information on polylines). Ground surface height\nis provided for areas within a 5 m isocontour of the driveable area boundary, which we de\ufb01ne as\nthe region of interest (ROI) [6]. We do so because the notion of ground surface height is ill-de\ufb01ned\nfor the interior of buildings and interior of densely constructed city blocks, areas where ground\nvehicles cannot observe due to occlusion. The raster grid is quantized to a 30 cm resolution, a higher\nresolution than the 1 m resolution in Argoverse 1.\nArea of Local Maps. Each scenario\u2019s local map includes all entities found within a 100 m dilation\nin l2-norm from the ego-vehicle trajectory.\n4\nExperiments\nArgoverse 2 supports a variety of downstream tasks. In this section we highlight three different\nlearning problems: 3D object detection, point cloud forecasting, and motion forecasting \u2014 each\nsupported by the sensor, lidar, and motion forecasting datasets, respectively. First, we illustrate the\nchallenging and diverse taxonomy within the Argoverse 2 sensor dataset by training a state-of-the-\nart 3D detection model on our twenty-six evaluation classes including \u201clong-tail\u201d classes such as\nstroller, wheel chairs, and dogs. Second, we showcase the utility of the Argoverse 2 lidar dataset\nthrough large-scale, self-supervised learning through the point cloud forecasting task. Lastly, we\ndemonstrate motion forecasting experiments which provide the \ufb01rst baseline for broad taxonomy\nmotion prediction.\n4.1\n3D Object Detection\n8\nTable 3: 3d object detection results on the Argoverse 2 Sensor Dataset, taken from the leaderboard\non Dec 21, 2022. Detectors is the winner of the CVPR 2022 Workshop on Autonomous Driving\nArgoverse 2 3D Object Detection challenge.\nMETHOD\nMCDS (\u2191)\nMAP (\u2191)\nMATE (\u2193)\nMASE (\u2193)\nMAOE (\u2193)\nCENTERPOINT (OURS)\n0.14\n0.18\n0.49\n0.34\n0.72\nDETECTORS [13]\n0.34\n0.41\n0.40\n0.30\n0.54\nBEVFUSION [32]\n0.37\n0.46\n0.40\n0.30\n0.50\nRegular Vehicle\nBus\nPedestrian\nStop Sign\nBox Truck\nBollard\nConstruction Barrel\nMotorcyclist\nTruck\nBicyclist\nMobile Crossing Sign\nAverage Metrics\nMotorcycle\nBicycle\nArticulated Bus\nSchool Bus\nTruck Cab\nConstruction Cone\nVehicular Trailer\nSign\nWheeled Device\nLarge Vehicle\nStroller\nMessage Board Trailer\nDog\nWheeled Rider\nWheelchair\nClass Names\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nAP\nFigure 7: Average precision of our 3D object de-\ntection baseline on the validation split of the Sen-\nsor Dataset (Beta). Our experiments showcase\nboth our diverse taxonomy and dif\ufb01cult \u201clong-tail\u201d\nclasses.\nWe provide baseline 3D detection results using\na state-of-the-art, anchorless 3D object detec-\ntion model \u2013 CenterPoint [51].\nOur Center-\nPoint implementation takes a point cloud as\ninput and crops it to a 200 m \u00d7 200 m grid\nwith a voxel resolution of [0.1 m, 0.1 m] in the\nxy (bird\u2019s-eye-view) plane and 0.2 m in the z-\naxis. To accommodate our larger taxonomy, we\ninclude six detection heads to encourage fea-\nture specialization. Figure 7 characterizes the\nperformance of our 3D detection baseline us-\ning the nuScenes [4] average precision met-\nric. Our large taxonomy allows us to evaluate\nclasses such as \u201cWheeled Device\u201d (e-Scooter),\n\u201cStroller\u201d, \u201cDog\u201d, and \u201cWheelchair\u201d and we \ufb01nd\nthat performance on these categories with strong\nbaselines is poor despite signi\ufb01cant amounts of\ntraining data.\nIn Table 3, we provide a snapshot of submissions\nto the Argoverse 2 3D Object Detection Leaderboard.\n4.2\nPoint Cloud Forecasting\nWe perform point cloud forecasting according to the experimental protocol of SPF2 [49] using the\nArgoverse 2 Lidar Dataset. Given a sequence of past scene point clouds, a model is required to predict\na sequence of future scene point clouds. We take the scene point clouds in the past 1 s (10 Hz) in\nthe range image format as input, and then predict the next 1 s of range images. SPFNet predicts two\noutput maps at each time step \u2013 the \ufb01rst output map is the predicted range values, while the second\noutput is a validity mask. Previous point cloud forecasting models were evaluated on smaller datasets\nsuch as KITTI or nuScenes. To explore how the amount of training data affects the performance, we\nuse increasing amounts of data for training the same model architecture, up to the full training set of\n16,000 sequences.\nEvaluation.\nWe use three metrics to evaluate the performance of our forecasting model: mean IoU,\nl1-norm, and Chamfer distance. The mean IoU evaluates the predicted range mask. The l1-norm\nmeasures the average l1 distance between the pixel sets of predicted range image and the ground-\ntruth image, which are both masked out by the ground-truth range mask. The Chamfer distance is\nobtained by adding up the Chamfer distances in both directions (forward and backward) between the\nground-truth point cloud and the predicted scene point cloud which is obtained by back-projecting\nthe predicted range image.\nTable 4: Results of point cloud forecasting on the test split of the Lidar Dataset.\n# TRAIN LOGS\n125\n250\n500\n1k\n2k\n4k\n16k\nMEAN IOU (%) (\u2191)\n55.5\n63.4\n61.7\n65.1\n68.0\n68.4\n70.9\nl1-NORM (\u2193)\n13.5\n12.5\n11.8\n9.9\n8.9\n8.7\n7.4\nCHAMFER DIST. (\u2193)\n31.1\n25.9\n22.4\n22.9\n20.5\n18.2\n14.0\n9\nResults of SPF2 and Discussion.\nTable 4 contains the results of our point cloud forecasting\nexperiments. With increasing training data, the performance of the model grows steadily in all three\nmetrics. These results and the works from the self-supervised learning literature [3, 7] indicate\nthat a large amount of training data can make a substantial difference. Another observation is that\nthe Chamfer distances for predictions on our dataset are signi\ufb01cantly higher than predictions on\nKITTI [49]. We conjecture that this could be due to two reasons: (1) the Argoverse 2 Lidar Dataset\nhas a much larger sensing range (above 200 m versus 120 m of the KITTI lidar sensor), which tends\nto signi\ufb01cantly increase the value of Chamfer distance. (2) the Argoverse 2 Lidar Dataset has a higher\nproportion of dynamic scenes compared with KITTI Dataset.\n4.3\nMotion Forecasting\nWe present several forecasting baselines [6] which try to make use of different aspects of the data.\nThose which are trained using the focal agent only and do not capture any social interaction include:\nconstant velocity, nearest neighbor, and LSTM encoder-decoder models (both with and without a\nmap-prior). We also evaluate WIMP [25] as an example of a graph-based attention method that\ncaptures social interaction. All hyper-parameters are obtained from the reference implementations.\nEvaluation.\nBaseline approaches are evaluated according to standard metrics. Following [6],\nwe use minADE and minFDE as the metrics; they evaluate the average and endpoint L2 distance\nrespectively, between the best forecasted trajectory and the ground truth. We also use Miss Rate\n(MR) which represents the proportion of test samples where none of the forecasted trajectories were\nwithin 2.0 meters of ground truth according to endpoint error. The resulting performance illustrates\nboth the community\u2019s progress on the problem and the signi\ufb01cant increase in dataset dif\ufb01culty when\ncompared with Argoverse 1.1.\nTable 5: Performance of motion forecasting baseline methods on vehicle-like (vehicle, bus, mo-\ntorcyclist) object types from the Argoverse 2 Motion Forecasting (Beta) Dataset. Usage of map\nprior indicates access to map information whereas usage of social context entails encoding other\nactors\u2019 states in the feature representation. Mining intersection (multimodal) scenarios leads to poor\nperformance at K=1 for all methods. Constant Velocity models have particularly poor performance\ndue to the dataset bias towards kinematically interesting trajectories. Note that modern deep methods\nsuch as WIMP still have a miss rate of 0.42 at K=6, indicating the increased dif\ufb01culty of the Argoverse\n2 dataset. Numbers within 1% of the best are in bold.\nK=1\nK=6\nMODEL\nMAP PRIOR\nSOCIAL CONTEXT\nMINADE \u2193\nMINFDE \u2193\nMR \u2193\nMINADE \u2193\nMINFDE \u2193\nMR \u2193\nCONST. VEL. [6]\n7.75\n17.44\n0.89\n-\n-\n-\nNN [6]\n4.46\n11.71\n0.81\n2.18\n4.94\n0.60\nNN [6]\n\u2713\n6.45\n15.51\n0.84\n4.30\n10.08\n0.78\nLSTM [6]\n3.05\n8.28\n0.85\n-\n-\n-\nLSTM [6]\n\u2713\n5.07\n12.71\n0.90\n3.73\n9.09\n0.85\nWIMP [25]\n\u2713\n\u2713\n3.09\n7.71\n0.84\n1.47\n2.90\n0.42\nTable 6: Motion forecasting results on the Argoverse 2 Motion Forecasting Dataset, taken from\nthe online leaderboard on Dec 21, 2022. BANet is the winner of the CVPR 2022 Workshop on\nAutonomous Driving Argoverse 2 Motion Forecasting challenge (#1), and QML and GANet received\nhonorable mention (HM) prizes. Entries are sorted below according to Brier-minFDE.\nK=1\nK=6\nMETHOD\nMINADE \u2193\nMINFDE \u2193\nMR \u2193\nMINADE \u2193\nMINFDE \u2193\nMR \u2193\nBRIER-MINFDE \u2193\nTHOMAS (GOHOME SCALAR) [20]\n1.95\n4.71\n0.64\n0.88\n1.51\n0.20\n2.16\nGORELA (W/O ENSEMBLE) [11]\n1.82\n4.62\n0.61\n0.76\n1.48\n0.22\n2.01\nGANET (ENSEMBLE) (HM) [47]\n1.81\n4.57\n0.61\n0.73\n1.36\n0.17\n1.98\nGANET (W/O ENSEMBLE) [47]\n1.77\n4.48\n0.59\n0.72\n1.34\n0.17\n1.96\nQML (HM) [44]\n1.84\n4.98\n0.62\n0.69\n1.39\n0.19\n1.95\nBANET (OPPRED) (#1) [53]\n1.79\n4.61\n0.60\n0.71\n1.36\n0.19\n1.92\nBaseline Results. Table 5 summarizes the results of baselines. For K=1, Argoverse 1 [6] showed that\na constant velocity model (minFDE=7.89) performed better than NN+map(prior) (minFDE=8.12),\n10\nwhich is not the case here. This further proves that Argoverse 2 is kinematically more diverse and\ncannot be solved by making constant velocity assumptions. Surprisingly, NN and LSTM variants that\nmake use of a map prior perform worse than those which do not, illustrating the scope of improvement\nin how these baselines leverage the map. For K=6, WIMP signi\ufb01cantly outperforms every other\nbaseline. This emphasizes that it is imperative to train expressive models that can leverage map\nprior and social context along with making diverse predictions. The trends are similar to our past 3\nArgoverse Motion Forecasting competitions [43]: Graph-based attention methods (e.g. [25, 31, 37])\ncontinued to dominate the competition, and were nearly twice as accurate as the next best baseline\n(Nearest Neighbor) at K=6. That said, some of the rasterization-based (e.g. [19]) methods also\nshowed promising results. Finally, we also evaluated baseline methods in the context of transfer\nlearning and varied object types, the results of which are summarized in the Appendix.\nIn Table 6, we provide a snapshot of submissions to the Argoverse 2 Motion Forecasting Leaderboard.\n5\nConclusion\nDiscussion. In this work, we have introduced three new datasets that constitute Argoverse 2. We\nprovide baseline explorations for three tasks \u2013 3d object detection, point cloud forecasting and motion\nforecasting. Our datasets provide new opportunities for many other tasks. We believe our datasets\ncompare favorably to existing datasets, with HD maps, rich taxonomies, geographic diversity, and\ninteresting scenes.\nLimitations. As in any human annotated dataset, there is label noise, although we seek to minimize\nit before release. 3D bounding boxes of objects are not included in the motion forecasting dataset,\nbut one can make reasonable assumptions about the object extent given the object type. The motion\nforecasting dataset also has imperfect tracking, consistent with state-of-the-art 3D trackers.\nReferences\n[1] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and\nJurgen Gall. SemanticKITTI: A dataset for semantic scene understanding of lidar sequences. In\nICCV, October 2019.\n[2] Alex Bewley, Pei Sun, Thomas Mensink, Drago Anguelov, and Cristian Sminchisescu. Range\nconditioned dilated convolutions for scale invariant 3d object detection. In Conference on Robot\nLearning, 2020.\n[3] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu,\nAnush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A Multimodal\nDataset for Autonomous Driving. In CVPR, 2020.\n[5] Yuning Chai, Pei Sun, Jiquan Ngiam, Weiyue Wang, Benjamin Caine, Vijay Vasudevan, Xiao\nZhang, and Dragomir Anguelov. To the point: Ef\ufb01cient 3d object detection in the range image\nwith graph convolution kernels. In CVPR, June 2021.\n[6] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew\nHartnett, De Wang, Peter Carr, Simon Lucey, Deva Ramanan, and James Hays. Argoverse: 3D\nTracking and Forecasting With Rich Maps. In CVPR, 2019.\n[7] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework\nfor contrastive learning of visual representations. In ICML, 2020.\n[8] Yun Chen, Frieda Rong, Shivam Duggal, Shenlong Wang, Xinchen Yan, Sivabalan Mani-\nvasagam, Shangjie Xue, Ersin Yumer, and Raquel Urtasun. GeoSim: Realistic video simulation\nvia geometry-aware composition for self-driving. In CVPR, June 2021.\n[9] Yukyung Choi, Namil Kim, Soonmin Hwang, Kibaek Park, Jae Shin Yoon, Kyounghwan An,\nand In So Kweon. Kaist multi-spectral day/night data set for autonomous and assisted driving.\nIEEE Transactions on Intelligent Transportation Systems, 19(3):934\u2013948, 2018.\n11\n[10] Yukyung Choi, Namil Kim, Kibaek Park, Soonmin Hwang, Jae Shin Yoon, Yoon In, and Inso\nKweon. All-day visual place recognition: Benchmark dataset and baseline. In IEEE Conference\non Computer Vision and Pattern Recognition Workshops. Workshop on Visual Place Recognition\nin Changing Environments, 2015.\n[11] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and Raquel Urtasun. Gorela: Go\nrelative for viewpoint-invariant motion forecasting. arXiv preprint arXiv:2211.02545, 2022.\n[12] Scott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu, Hang Zhao, Sabeek Pradhan,\nYuning Chai, Benjamin Sapp, Charles Qi, Yin Zhou, Zoey Yang, Aurelien Chouard, Pei\nSun, Jiquan Ngiam, Vijay Vasudevan, Alexander McCauley, Jonathon Shlens, and Dragomir\nAnguelov. Large scale interactive motion forecasting for autonomous driving : The waymo\nopen motion dataset. CoRR, abs/2104.10133, 2021.\n[13] Jin Fang, Qinghao Meng, Dingfu Zhou, Chulin Tang, Jianbing Shen, Cheng-Zhong Xu, and\nLiangjun Zhang. Technical report for cvpr 2022 workshop on autonomous driving argoverse 3d\nobject detection competition, 2022.\n[14] Nils G\u00e4hlert, Nicolas Jourdan, Marius Cordts, Uwe Franke, and Joachim Denzler. Cityscapes\n3d: Dataset and benchmark for 9 dof vehicle detection. CoRR, abs/2006.07864, 2020.\n[15] Jiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir Anguelov, Congcong Li, and Cordelia\nSchmid. VectorNet: Encoding hd maps and agent dynamics from vectorized representation. In\nCVPR, June 2020.\n[16] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li Huang, and Yuan Li.\nAfdet: Anchor free one stage 3d object detection. In CVPR Workshops, 2020.\n[17] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? The\nKITTI vision benchmark suite. In CVPR, June 2012.\n[18] Jakob Geyer, Yohannes Kassahun, Mentar Mahmudi, Xavier Ricou, Rupesh Durgesh, Andrew S.\nChung, Lorenz Hauswald, Viet Hoang Pham, Maximilian M\u00fchlegg, Sebastian Dorn, Tiffany\nFernandez, Martin J\u00e4nicke, Sudesh Mirashi, Chiragkumar Savani, Martin Sturm, Oleksandr\nVorobiov, Martin Oelker, Sebastian Garreis, and Peter Schuberth. A2D2: audi autonomous\ndriving dataset. CoRR, abs/2004.06320, 2020.\n[19] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde.\nHome: Heatmap output for future motion estimation. arXiv preprint arXiv:2105.10968, 2021.\n[20] Thomas Gilles, Stefano Sabatini, Dzmitry Tsishkou, Bogdan Stanciulescu, and Fabien Moutarde.\nThomas: Trajectory heatmap output with learned multi-agent sampling. In ICLR, 2022.\n[21] Wei Han, Zhengdong Zhang, Benjamin Caine, Brandon Yang, Christoph Sprunk, Ouais Alsharif,\nJiquan Ngiam, Vijay Vasudevan, Jonathon Shlens, and Zhifeng Chen. Streaming object detection\nfor 3-d point clouds. In ECCV, 2020.\n[22] John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy\nOmari, Vladimir Iglovikov, and Peter Ondruska. One Thousand and One Hours: Self-driving\nMotion Prediction Dataset. arXiv:2006.14480 [cs], November 2020. Comment: Presented at\nCoRL2020.\n[23] Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion\nplanning with self-supervised freespace forecasting. In CVPR, June 2021.\n[24] R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low,\nA. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky,\nW. Jiang, and V. Shet. Lyft level 5 av dataset. arXiv, 2019.\n[25] Siddhesh Khandelwal, William Qi, Jagjeet Singh, Andrew Hartnett, and Deva Ramanan. What-if\nmotion prediction for autonomous driving. arXiv preprint arXiv:2008.10587, 2020.\n[26] John W. Lambert and James Hays. Trust, but Verify: Cross-modality fusion for hd map change\ndetection. In Advances in Neural Information Processing Systems Track on Datasets and\nBenchmarks, 2021.\n[27] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom.\nPointPillars: Fast encoders for object detection from point clouds. In CVPR, June 2019.\n[28] Edouard Leurent and Jean Mercat. Social attention for autonomous decision-making in dense\ntraf\ufb01c. CoRR, abs/1911.12250, 2019.\n12\n[29] Mengtian Li, Yu-Xiong Wang, and Deva Ramanan. Towards streaming perception. In ECCV,\n2020.\n[30] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet: An online HD map construction\nand evaluation framework. CoRR, abs/2107.06307, 2021.\n[31] Ming Liang, Bin Yang, Rui Hu, Yun Chen, Renjie Liao, Song Feng, and Raquel Urtasun.\nLearning lane graph representations for motion forecasting. In ECCV, 2020.\n[32] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song\nHan. Bevfusion: Multi-task multi-sensor fusion with uni\ufb01ed bird\u2019s-eye view representation.\narXiv preprint arXiv:2205.13542, 2022.\n[33] Yuexin Ma, Xinge Zhu, Sibo Zhang, Ruigang Yang, Wenping Wang, and Dinesh Manocha.\nTraf\ufb01cpredict: Trajectory prediction for heterogeneous traf\ufb01c-agents. CoRR, abs/1811.02146,\n2018.\n[34] Andrey Malinin, Neil Band, Alexander Ganshin, German Chesnokov, Yarin Gal, Mark J. F.\nGales, Alexey Noskov, Andrey Ploskonosov, Liudmila Prokhorenkova, Ivan Provilkov, Vatsal\nRaina, Vyas Raina, Mariya Shmatova, Panos Tigas, and Boris Yangel. Shifts: A dataset of real\ndistributional shift across multiple large-scale tasks. CoRR, abs/2107.07455, 2021.\n[35] Sivabalan Manivasagam, Shenlong Wang, Kelvin Wong, Wenyuan Zeng, Mikita Sazanovich,\nShuhan Tan, Bin Yang, Wei-Chiu Ma, and Raquel Urtasun. LiDARsim: Realistic lidar simula-\ntion by leveraging the real world. In CVPR, June 2020.\n[36] Jiageng Mao, Minzhe Niu, Chenhan Jiang, Hanxue Liang, Jingheng Chen, Xiaodan Liang,\nYamin Li, Chaoqiang Ye, Wei Zhang, Zhenguo Li, Jie Yu, Hang Xu, and Chunjing Xu. One\nMillion Scenes for Autonomous Driving: ONCE Dataset. arXiv:2106.11037 [cs], August 2021.\nComment: Accepted to NeurIPS 2021 Datasets and Benchmarks Track.\n[37] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois, and\nGuillermo Pita Gil. Multi-head attention for multi-modal joint vehicle motion forecasting. In\nICRA. IEEE, 2020.\n[38] Jean Mercat, Thomas Gilles, Nicole El Zoghby, Guillaume Sandou, Dominique Beauvois, and\nGuillermo Pita Gil. Multi-head attention for multi-modal joint vehicle motion forecasting, 2019.\n[39] Abhishek Patil, Srikanth Malla, Haiming Gang, and Yi-Ting Chen.\nThe H3D dataset\nfor full-surround 3d multi-object detection and tracking in crowded urban scenes. CoRR,\nabs/1903.01568, 2019.\n[40] Quang-Hieu Pham, Pierre Sevestre, Ramanpreet Singh Pahwa, Huijing Zhan, Chun Ho Pang,\nYuda Chen, Armin Mustafa, Vijay Chandrasekhar, and Jie Lin. A*3d dataset: Towards au-\ntonomous driving in challenging environments. CoRR, abs/1909.07541, 2019.\n[41] Matthew Pitropov, Danson Evan Garcia, Jason Rebello, Michael Smart, Carlos Wang, Krzysztof\nCzarnecki, and Steven Waslander. Canadian adverse driving conditions dataset. The Interna-\ntional Journal of Robotics Research, 40(4-5):681\u2013690, Dec 2020.\n[42] Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo, Boyang Deng, and Dragomir\nAnguelov. Offboard 3d object detection from point cloud sequences. In CVPR, June 2021.\n[43] Jagjeet Singh, William Qi, Tanmay Agarwal, and Andrew Hartnett. Argoverse motion forecast-\ning competition. https://eval.ai/web/challenges/challenge-page/454/overview.\nAccessed: 08-27-2021.\n[44] Tong Su, Xishun Wang, and Xiaodong Yang. Qml for argoverse 2 motion forecasting challenge,\n2022.\n[45] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul\nTsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan\nNgiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya\nJoshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in\nPerception for Autonomous Driving: Waymo Open Dataset. In CVPR, 2020.\n[46] Pei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang, Cristian\nSminchisescu, and Dragomir Anguelov. Rsn: Range sparse net for ef\ufb01cient, accurate lidar 3d\nobject detection. In CVPR, June 2021.\n13\n[47] Mingkun Wang, Xinge Zhu, Changqian Yu, Wei Li, Yuexin Ma, Ruochun Jin, Xiaoguang Ren,\nDongchun Ren, Mingxu Wang, and Wenjing Yang. Ganet: Goal area network for motion\nforecasting, 2022.\n[48] Xinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani, and Nicholas Rhinehart. 4d forecast-\ning: Sequential forecasting of 100,000 points. In Proceedings of ECCV \u201920 Workshops, August\n2020.\n[49] Xinshuo Weng, Jianren Wang, Sergey Levine, Kris Kitani, and Nick Rhinehart. Inverting the\nforecasting pipeline with spf2: Sequential pointcloud forecasting for sequential pose forecasting.\nIn Proceedings of (CoRL) Conference on Robot Learning, November 2020.\n[50] Zhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Sean\nRafferty, and Henrik Kretzschmar. Surfelgan: Synthesizing realistic sensor data for autonomous\ndriving. In CVPR, June 2020.\n[51] Tianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3d object detection and\ntracking. In CVPR, June 2021.\n[52] Wei Zhan, Liting Sun, Di Wang, Haojie Shi, Aubrey Clausse, Maximilian Naumann, Julius\nKummerle, Hendrik Konigshof, Christoph Stiller, Arnaud de La Fortelle, et al. Interaction\ndataset: An international, adversarial and cooperative motion dataset in interactive driving\nscenarios with semantic maps. arXiv preprint arXiv:1910.03088, 2019.\n[53] Chen Zhang, Honglin Sun, Chen Chen, and Yandong Guo. Banet: Motion forecasting with\nboundary aware network, 2022.\n[54] Jannik Z\u00fcrn, Johan Vertens, and Wolfram Burgard. Lane graph estimation for scene understand-\ning in urban driving. CoRR, abs/2105.00195, 2021.\n6\nAppendix\n6.1\nAdditional Information About Sensor Suite\nIn Figure 8, we provide a diagram of the sensor suite used to capture the Argoverse 2 datasets.\nFigure 9 shows the speed distribution for annotated pedestrian 3D cuboids and the yaw distribution.\nFigure 8: Car sensor schematic showing the three coordinate systems: (1) the vehicle frame in the\nrear axle; (2) the camera frame; and the lidar frame.\n6.2\nAdditional Information About Motion Forecasting Dataset\n6.2.1\nInterestingness Scores\nKinematic scoring selects for trajectories performing sharp turns or signi\ufb01cant (de)accelerations. The\nmap complexity program biases the data set towards trajectories complex traversals of the underlying\nlane graph. In particular, complex map regions, paths through intersections, and lane-changes score\n14\n0\n0.5\n1\n1.5\n2\n2.5\n3\n3.5\n0\n50k\n100k\nArgoverse 2\nWaymo Open\nnuScenes\nArgoverse 1\nSpeed (m/s)\nNumber of 3D pedestrian cuboids \n (speed > 0.5 m/s)\n\u2212180 \u2212135\n\u221290\n\u221245\n0\n45\n90\n135\n2\n5\n1000\n2\n5\n10k\n2\n5\n100k\n2\n5\nArgoverse 1\nArgoverse 2\nYaw (degrees)\nNumber of 3D cuboids\nFigure 9: Left: Number of moving 3D cuboids for pedestrians by speed distribution. We de\ufb01ne\nmoving objects when the speed is greater than 0.5 m/s. Right: Number of annotated 3D cuboids by\nyaw distribution.\nhighly. Social scoring rewards tracks through dense regions of other actors. Social scoring also selects\nfor non-vehicle object classes to ensure adequate samples from rare classes, such as motorcycles, for\ntraining and evaluation. Finally, the autonomous vehicle scoring program encourages the selection of\ntracks that intersect the ego-vehicle\u2019s desired route.\n2019/10\n2019/11\n2019/12\n2020/02\n2020/03\n2020/05\n2020/06\n2020/07\n2020/08\n2020/09\n2020/10\n2020/11\n2020/12\n2021/01\n2021/02\n2021/03\n2021/04\nYear/Month of Submission\n0\n1\n2\n3\n4\n5\n6\n7\n8\nminFDE (K=6)\nState-of-art minFDE (K=6)\nCompetition Phase\nNeurips 2019\nCVPR 2020\nNone\nCVPR 2021\nFigure 10: MinFDE metric values for submissions on Argoverse 1.1 over time. Individual points\nindicate submissions to the public leader board. Colors indicate speci\ufb01c competition phases. The solid\nblack line indicates SOTA performance. The research community made massive gains which have\nplateaued since early 2020. However, we note that the number and diversity of methods performing\nat or near the SOTA continues to grow. Additionally, later competitions sorted the leaderboard by\n\u201cMiss Rate\u201d and probability weighted FDE, and those metrics showed progress. Still, minFDE did\nnot improve signi\ufb01cantly.\n6.3\nAdditional Information About HD Maps\nExamples of HD maps from the Sensor Dataset\nIn Figure 12, we display examples of local HD\nmaps associated with individual logs/scenarios.\n6.4\nAdditional 3D Detection Results\nIn Figure 13, we show additional evaluation metrics for our detection baseline.\n15\nFigure 11: Histogram of the number of actors (both scored and all types) present in the Motion\nForecasting Dataset scenarios. The Lidar Dataset is mined by the same criteria and thus follows the\nsame distribution.\nMAP ENTITY\nPROVIDED ATTRIBUTES\nTYPE\nDESCRIPTION\nLANE\nSEGMENTS\nIS_INTERSECTION\nBOOLEAN\nWHETHER OR NOT THIS LANE SEGMENT LIES WITHIN AN INTERSECTION.\nLANE TYPE\nENUMERATED TYPE\nDESIGNATION OF WHICH VEHICLE TYPES MAY LEGALLY UTILIZE THIS LANE FOR TRAVEL.\nLEFT LANE BOUNDARY\n3D POLYLINE\nTHE POLYLINE OF THE LEFT BOUNDARY IN THE CITY MAP COORDINATE SYSTEM\nRIGHT LANE BOUNDARY\n3D POLYLINE\nTHE POLYLINE OF THE RIGHT BOUNDARY IN THE CITY MAP COORDINATE SYSTEM.\nLEFT LANE MARK TYPE\nENUMERATED TYPE\nTYPE OF PAINTED LANE MARKING TO THE LEFT OF THE LANE SEGMENT ON THE ROAD.\nRIGHT LANE MARK TYPE\nENUMERATED TYPE\nTYPE OF PAINTED LANE MARKING TO THE RIGHT OF THE LANE SEGMENT ON THE ROAD.\nLEFT NEIGHBOR\nINTEGER\nTHE UNIQUE LANE SEGMENT IMMEDIATELY TO THE LEFT OF SEGMENT, OR NONE.\nRIGHT NEIGHBOR\nINTEGER\nTHE UNIQUE LANE SEGMENT IMMEDIATELY TO THE RIGHT OF SEGMENT, OR NONE.\nSUCCESSOR IDS\nINTEGER LIST\nLANE SEGMENTS THAT MAY BE ENTERED BY FOLLOWING FORWARD.\nID\nINTEGER\nUNIQUE IDENTIFIER\nDRIVABLE\nAREA\nAREA BOUNDARY\n3D POLYGONS\nAREA WHERE IT IS POSSIBLE FOR THE AV TO DRIVE WITHOUT DAMAGING ITSELF\nID\nINTEGER\nUNIQUE IDENTIFIER\nPEDESTRIAN\nCROSSINGS\nEDGE1, EDGE2\n3D POLYLINES\nENDPOINTS OF BOTH EDGE ALONG THE PRINCIPAL AXIS, THUS DEFINING A POLYGON.\nID\nINTEGER\nUNIQUE IDENTIFIER\nGROUND SURFACE HEIGHT\n-\n2D RASTER ARRAY\nRASTER GRID QUANTIZED TO A 30 cm RESOLUTION.\nTable 7: HD map attributes for each Argoverse 2 scenario.\nAverage Precision (AP)\nAP =\n1\n101\nX\nt\u2208T\nX\nr\u2208R\npinterp(r)\n(1)\nTrue Positive Metrics Average Translation Error (ATE)\nATE = \u2225tdet \u2212tgt\u22252\n(2)\nAverage Scaling Error (ASE)\nASE = 1 \u2212\nY\nd\u2208D\nmin(ddet, dgt)\nmax(ddet, dgt)\n(3)\nAverage Orientation Error (AOE)\nAOE = |\u03b8det \u2212\u03b8gt|\n(4)\nComposite Detection Score (CDS)\nCDS = mAP \u00b7\nX\nx\u2208X\n(1 \u2212x)\n(5)\nwhere X = {mATEunit, mASEunit, mAOEunit}\n16\n(a) Washington D.C.\n(b) Washington D.C.\n(c) Washington D.C.\n(d) Pittsburgh, PA\n(e) Pittsburgh, PA\n(f) Pittsburgh, PA\n(g) Miami, FL\n(h) Miami, FL\n(i) Miami, FL\n(j) Detroit, MI\n(k) Detroit, MI\n(l) Austin, TX\nFigure 12: Examples of egovehicle (AV) trajectories on local vector maps from the Sensor Dataset\nacross several different cities. A 100m \u00d7 100m local map region is shown. Crosswalks are indicated\nin purple. Red circles denote the AV pose discretely sampled at 1 Hz for the purposes of illustration.\nPose is provided at >20 Hz in the dataset, as indicated by the trajectory path indicated by a red line.\nCity layouts vary dramatically, e.g. roadways in Miami are usually aligned parallel to a north-south,\neast-west grid, while roadways in Pittsburgh are generally not.\n17\nRegular Vehicle\nBus\nPedestrian\nStop Sign\nBox Truck\nBollard\nConstruction Barrel\nMotorcyclist\nTruck\nBicyclist\nMobile Crossing Sign\nAverage Metrics\nMotorcycle\nBicycle\nArticulated Bus\nSchool Bus\nTruck Cab\nConstruction Cone\nVehicular Trailer\nSign\nWheeled Device\nLarge Vehicle\nStroller\nMessage Board Trailer\nDog\nWheeled Rider\nWheelchair\nClass Names\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nCDS\nRegular Vehicle\nBus\nPedestrian\nStop Sign\nBox Truck\nBollard\nConstruction Barrel\nMotorcyclist\nTruck\nBicyclist\nMobile Crossing Sign\nAverage Metrics\nMotorcycle\nBicycle\nArticulated Bus\nSchool Bus\nTruck Cab\nConstruction Cone\nVehicular Trailer\nSign\nWheeled Device\nLarge Vehicle\nStroller\nMessage Board Trailer\nDog\nWheeled Rider\nWheelchair\nClass Names\n0.0\n0.5\n1.0\n1.5\n2.0\nATE\nRegular Vehicle\nBus\nPedestrian\nStop Sign\nBox Truck\nBollard\nConstruction Barrel\nMotorcyclist\nTruck\nBicyclist\nMobile Crossing Sign\nAverage Metrics\nMotorcycle\nBicycle\nArticulated Bus\nSchool Bus\nTruck Cab\nConstruction Cone\nVehicular Trailer\nSign\nWheeled Device\nLarge Vehicle\nStroller\nMessage Board Trailer\nDog\nWheeled Rider\nWheelchair\nClass Names\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\nASE\nRegular Vehicle\nBus\nPedestrian\nStop Sign\nBox Truck\nBollard\nConstruction Barrel\nMotorcyclist\nTruck\nBicyclist\nMobile Crossing Sign\nAverage Metrics\nMotorcycle\nBicycle\nArticulated Bus\nSchool Bus\nTruck Cab\nConstruction Cone\nVehicular Trailer\nSign\nWheeled Device\nLarge Vehicle\nStroller\nMessage Board Trailer\nDog\nWheeled Rider\nWheelchair\nClass Names\n0.0\n0.5\n1.0\n1.5\n2.0\n2.5\n3.0\n3.5\nAOE\nFigure 13: 3D object detection performance on the validation split of the Sensor Dataset (Beta).\nTop Row: Composite detection score (left). Average translation error (right) Bottom Row: Average\nscaling error (left), and average orientation error (right). Results are shown on the validation set of\nthe Sensor Dataset.\n6.5\nTraining Details of SPF2 baseline\nWe sample 2-second training snippets (representing 1 second of past and 1 second of future data)\nevery 0.5 seconds. Thus, for a training log with 30 second duration, 59 training snippets would be\nsampled. We train the model for 16 epochs by using the Adam optimizer with the learning rate of\n4e \u22123, betas of 0.9 and 0.999, and batch size of 16 per GPU.\n6.6\nAdditional Motion Forecasting Experiments\n6.6.1\nTransfer Learning\nThe results of transfer learning experiments are summarized in Table 8. WIMP was trained and tested\nin different settings with Argoverse 1.1 and Argoverse 2. As expected, the model works best when\nit is trained and tested on the same distribution (i.e. both train and test data come from Argoverse\n1.1, or both from Argoverse 2). For example, when WIMP is tested on Argoverse 2 (6s), the model\ntrained on Argoverse 2 (6s) has a minFDE of 2.91, whereas the one trained on Argoverse 1.1 (3s) has\na minFDE of 6.82 (i.e. approximately 2.3x worse). Likewise, in the reverse setting, when WIMP is\ntested on Argoverse 1.1 (3s), the model trained on Argoverse 1.1 (3s) has a minFDE of 1.14 and the\none trained on Argoverse 2 (6s) has minFDE of 2.05 (i.e. approximately 1.8x worse). This indicates\nthat transfer learning from Argoverse 2 (Beta) to Argoverse 1.1 is more useful than the reverse setting,\ndespite being smaller in the number of scenarios. However, the publicly released version of Argoverse\n2 Motion Forecasting (the non-beta 2.0 version) has comparable size with Argoverse 1.1.\nWe note that it is a common practice to train and test sequential models on varied sequence length (e.g.\nmachine translation). As such, it is still reasonable to expect a model trained with 3s to do well on 6s\nhorizon. Several factors may contribute to distribution shift, including differing prediction horizon,\ncities, mining protocols, object types. Notably, however, these results indicate that Argoverse 2 is\nsigni\ufb01cantly more challenging and diverse than its predecessor.\n18\n6.6.2\nExperiment with different object types\nTable 9 shows the results on Nearest Neighbor baseline (without map prior) on different object types.\nAs one would expect, the displacement errors in pedestrians are signi\ufb01cantly lower than other object\ntypes. This occurs because they move at signi\ufb01cantly slower velocities. However, this does not imply\nthat pedestrian motion forecasting is a solved problem and one should rather focus on other object\ntypes. This instead means that we need to come up with better metrics that can capture that fact lower\ndisplacement errors in pedestrians can often be more critical than higher errors in vehicles. We leave\nthis line of work for future scope.\nTable 8: Performance of WIMP when trained and tested on different versions of Argoverse motion\nforecasting datasets. Training and evaluation is restricted to vehicle-like (vehicle, bus, motorcyclist)\nobject types as only vehicles were present in Argoverse 1.1. All the results are for K=6, and prediction\nhorizon is speci\ufb01ed in parentheses. Notably, the model trained on a 3s horizon performs poorly on the\nlonger 6s horizon. \u2018Argoverse 2\u2019 below denotes the Argoverse 2 (Beta) Motion Forecasting Dataset.\nTrain Split (pred. horizon)\nTest Split (pred. horizon)\nminADE \u2193\nminFDE \u2193\nMR \u2193\nArgoverse 1.1 (3s)\nArgoverse 1.1 (3s)\n0.75\n1.14\n0.12\nArgoverse 2 (6s)\nArgoverse 1.1 (3s)\n1.68\n2.05\n0.27\nArgoverse 1.1 (3s)\nArgoverse 2 (3s)\n0.94\n1.88\n0.26\nArgoverse 1.1 (3s)\nArgoverse 2 (6s)\n4.93\n6.82\n0.77\nArgoverse 2 (6s)\nArgoverse 2 (6s)\n1.48\n2.91\n0.43\nTable 9: Performance of Nearest Neighbor baseline on different object types for K=6. The most\naccurately predicted object type for each evaluation metric is highlighted in bold.\nObject Type\n#Samples\nminADE \u2193\nminFDE \u2193\nMR \u2193\nAll\n9955\n2.48\n5.52\n0.66\nVehicle\n8713\n2.62\n5.87\n0.70\nBus\n439\n2.69\n5.59\n0.73\nPedestrians\n677\n0.69\n1.31\n0.17\nMotorcyclist\n39\n2.33\n5.07\n0.61\nCyclist\n87\n1.48\n2.80\n0.42\n7\nDatasheet for Argoverse 2\nFor what purpose was the dataset created?\nWas there a speci\ufb01c task in mind? Was there a\nspeci\ufb01c gap that needed to be \ufb01lled? Please provide a description.\nArgoverse was created to support the global research community in improving the state of the art in\nmachine learning tasks vital for self driving. The Argoverse 2 datasets described in this manuscript\nimprove upon the initial Argoverse datasets. These datasets support many tasks, from 3D perception\nto motion forecasting to HD map automation.\nThe three datasets proposed in this manuscript address different gaps in this space. See the comparison\ncharts in the main manuscript for a more detailed breakdown.\nThe Argoverse 2 Sensor Dataset has a richer taxonomy than similar datasets. It is the only dataset of\nsimilar size to have stereo imagery. The 1,000 logs in the dataset were chosen to have a variety of\nobject types with diverse interactions.\nThe Argoverse 2 Motion Forecasting Dataset also has a richer taxonomy than existing datasets. The\nscenarios in the dataset were mined with an emphasis on unusual behaviors that are dif\ufb01cult to predict.\nThe Argoverse 2 Lidar Dataset is the largest Lidar Dataset. Only the concurrent ONCE dataset is\nsimilarly sized to enable self-supervised learning in lidar space. Unlike ONCE, our dataset contains\nHD maps and high frame rate lidar.\nWho created this dataset (e.g., which team, research group) and on behalf of which entity\n(e.g., company, institution, organization)?\n19\nThe Argoverse 2 datasets were created by researchers at Argo AI.\nWhat support was needed to make this dataset? (e.g.who funded the creation of the dataset? If\nthere is an associated grant, provide the name of the grantor and the grant name and number, or if it\nwas supported by a company or government agency, give those details.)\nThe creation of this dataset was funded by Argo AI.\nAny other comments?\nn/a\nCOMPOSITION\nWhat do the instances that comprise the dataset represent (e.g., documents, photos, people,\ncountries)?\nAre there multiple types of instances (e.g., movies, users, and ratings; people and\ninteractions between them; nodes and edges)? Please provide a description.\nThe three constituent datasets of Argoverse 2 have different attributes, but the core instances for each\nare brief \u201cscenarios\u201d or \u201clogs\u201d of 11, 15, or 30 seconds that represent a continuous observation of a\nscene around a self-driving vehicle.\nEach scenario in all three datasets has an HD map that includes lane boundaries, crosswalks, driveable\narea, etc. Scenarios for the Sensor Dataset additionally contain a raster map of ground height at .3\nmeter resolution.\nHow many instances are there in total (of each type, if appropriate)?\nThe Sensor Dataset has 1,000 15 second scenarios.\nThe Lidar Dataset has 20,000 30 second scenarios.\nThe Motion Forecasting Dataset has 250,000 11 second scenarios.\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of\ninstances from a larger set?\nIf the dataset is a sample, then what is the larger set? Is the\nsample representative of the larger set (e.g., geographic coverage)? If so, please describe how\nthis representativeness was validated/veri\ufb01ed. If it is not representative of the larger set, please\ndescribe why not (e.g., to cover a more diverse range of instances, because instances were withheld\nor unavailable).\nThe scenarios in the dataset are a sample of the set of observations made by a \ufb02eet of self-driving\nvehicles. The data is not uniformly sampled. The particular samples were chosen to be geographically\ndiverse (spanning 6 cities - Pittsburgh, Detroit, Austin, Palo Alto, Miami, and Washington D.C.), to\ninclude interesting behavior (e.g. cars making unexpected maneuvers), to contain interesting weather\n(e.g. rain and snow), and to contain scenes with many objects of diverse types in motion (e.g. a\ncrowd walking, riders on e-scooters splitting lanes between many vehicles, an excavator operating at\na construction site, etc.).\nWhat data does each instance consist of?\n\u201cRaw\u201d data (e.g., unprocessed text or images) or\nfeatures? In either case, please provide a description.\nEach Sensor Dataset scenario is 15 seconds in duration. Each scenario has 20 fps video from 7 ring\ncameras, 20 fps video from two forward facing stereo cameras, and 10 hz lidar returns from two\nout-of-phase 32 beam lidars. The ring cameras are synchronized to \ufb01re when either lidar sweeps\nthrough their \ufb01eld of view. Each scenario contains vehicle pose over time and calibration data to\nrelate the various sensors.\nEach Lidar Dataset scenario is 30 seconds in duration. These scenarios are similar to those of the\nSensor Dataset, except that there is no imagery.\nEach Motion Forecasting scenario is 11 seconds in duration. These scenarios contain no sensor data,\nbut instead contain tracks of objects such as vehicles, pedestrians, and bicycles. The tracks specify\nthe category of each object (e.g. bus or bicycle) as well as their location and heading at a 10 hz\nsampling interval.\n20\nThe HD map associated with all three types of scenarios contains polylines describing lanes, cross-\nwalks, and driveable area. Lanes form a graph with predecessors and successors, e.g. a lane that\nsplits can have two successors. Lanes have precisely localized lane boundaries that include paint\ntype (e.g. double solid yellow). Driveable area, also described by a polygon, is the area where it is\npossible but not necessarily legal to drive. It includes areas such as road shoulders.\nIs there a label or target associated with each instance? If so, please provide a description.\nEach Sensor Dataset scenario has 3D track annotations for dynamic objects such as vehicles, pedes-\ntrians, strollers, dogs, etc. The tracks are suitable as ground truth for tasks such as 3D object detection\nand 3D tracking. The 3D track labels are intentionally held out from the test set. The HD map\ncould also be thought of as labels for each instance, and would be suitable as ground truth for lane\ndetection or map automation. The vehicle pose data could be considered ground truth labels for visual\nodometry. The lidar depth estimates can act as ground truth for monocular or stereo depth estimation.\nThe Lidar Dataset does not have human annotations beyond the HD map. Still, the evolving point\ncloud itself can be considered ground truth for point cloud forecasting.\nEach Motion Forecasting Dataset scenario provides labels specifying which tracks are associated\nwith \u201cscored actors\u201d. These tracks exhibit interesting behavior and are guaranteed to be observed\nover the entire duration of each scenario; algorithms will be asked to forecast the future motion for\nthese tracks. The future motion of actors in each scenario is intentionally held out in the test set.\nIs any information missing from individual instances?\nIf so, please provide a description,\nexplaining why this information is missing (e.g., because it was unavailable). This does not include\nintentionally removed information, but might include, e.g., redacted text.\nIn the Sensor Dataset, objects are only labeled within 5 meters of the driveable area. For example, a\nperson sitting on their front porch will not be labeled.\nIn the Sensor Dataset and Motion Forecasting Dataset, instances are not necessarily labeled for the\nfull duration of each scenario if the objects move out of observation range or become occluded.\nZ Are relationships between individual instances made explicit (e.g., users\u2019 movie ratings,\nsocial network links)? If so, please describe how these relationships are made explicit.\nThe instances of the three datasets are disjoint. They each carry their own HD map for the region\naround the scenario. These HD maps may overlap spatially, though. For example, many forecasting\nscenarios may take place in the same intersection. If a user of the dataset wanted to recover the\nspatial relationship between scenarios, they could do so through the Argoverse API.\nAre there recommended data splits (e.g., training, development/validation, testing)?\nIf so,\nplease provide a description of these splits, explaining the rationale behind them.\nWe de\ufb01ne splits of each dataset. The Sensor Dataset is split 700 / 150 / 150 between train, validation,\nand test. The Lidar Dataset is split 16,000 / 2,000 / 2,000 and the Motion Forecasting Dataset is split\n200,000 / 25,000 / 25,000. In all cases, the splits are designed to make the training dataset as large\nas possible while keeping the validation and test datasets large and diverse enough to accurately\nbenchmark models learned on the training set.\nAre there any errors, sources of noise, or redundancies in the dataset? If so, please provide a\ndescription.\nEvery sensor used in the dataset \u2013 ring cameras, stereo cameras, and lidar \u2013 has noise associated with\nit. Pixel intensities, lidar intensities, and lidar point 3D locations all have noise. Lidar points are\nalso quantized to \ufb02oat16 which leads to roughly a centimeter of quantization error. Six degree of\nfreedom vehicle pose also has noise. The calibration specifying the relationship between sensors can\nbe imperfect.\nThe HD map for each scenario can contain noise, both in terms of lane boundary locations and precise\nground height.\nThe 3D object annotations in the Sensor Dataset do not always match the spatial extent and motion of\nan object in the real world. For example, we assume that objects do not change size during a scenario,\nbut this could be violated by a car opening its door. 3D annotations for distant objects with relatively\nfew pixels and lidar returns are less accurate.\n21\nThe object tracks in the Motion Forecasting dataset are imperfect and contain errors typical of a\nreal-time 3D tracking method. Our expectation is that a motion forecasting algorithm should operate\nwell despite this noise.\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g.,\nwebsites, tweets, other datasets)?\nIf it links to or relies on external resources, a) are there\nguarantees that they will exist, and remain constant, over time; b) are there of\ufb01cial archival versions\nof the complete dataset (i.e., including the external resources as they existed at the time the dataset\nwas created); c) are there any restrictions (e.g., licenses, fees) associated with any of the external\nresources that might apply to a future user? Please provide descriptions of all external resources and\nany restrictions associated with them, as well as links or other access points, as appropriate.\nThe data itself is self-hosted, like Argoverse 1 [see https://www.argoverse.org/], and we\nmaintain public links to all previous versions of the dataset in case of updates. The data is independent\nof any previous datasets, including Argoverse 1.\nDoes the dataset contain data that might be considered con\ufb01dential (e.g., data that is\nprotected by legal privilege or by doctor-patient con\ufb01dentiality, data that includes the content\nof individuals\u2019 non-public communications)? If so, please provide a description.\nNo.\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threaten-\ning, or might otherwise cause anxiety? If so, please describe why.\nNo.\nDoes the dataset relate to people? If not, you may skip the remaining questions in this section.\nYes, the dataset contains images and behaviors of thousands of people on public streets.\nDoes the dataset identify any subpopulations (e.g., by age, gender)? If so, please describe how\nthese subpopulations are identi\ufb01ed and provide a description of their respective distributions within\nthe dataset.\nNo.\nIs it possible to identify individuals (i.e., one or more natural persons), either directly or\nindirectly (i.e., in combination with other data) from the dataset? If so, please describe how.\nWe do not believe so. All image data has been anonymized. Faces and license plates are obfuscated\nby replacing them with a 5x5 grid, where each grid cell is the average color of the original pixels in\nthat grid cell. This anonymization is done manually and is not limited by our 3D annotation policy.\nFor example, a person sitting on their front porch 10 meters from the road would not be labeled with\na 3D cuboid, but their face would still be obscured.\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that\nreveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or\nunion memberships, or locations; \ufb01nancial or health data; biometric or genetic data; forms of\ngovernment identi\ufb01cation, such as social security numbers; criminal history)?\nIf so, please\nprovide a description.\nNo.\nAny other comments?\nn/a\nCOLLECTION\nHow was the data associated with each instance acquired?\nWas the data directly observable\n(e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly\ninferred/derived from other data (e.g., part-of-speech tags, model-based guesses for age or language)?\nIf data was reported by subjects or indirectly inferred/derived from other data, was the data\n22\nvalidated/veri\ufb01ed? If so, please describe how.\nThe sensor data was directly acquired by a \ufb02eet of autonomous vehicles.\nOver what timeframe was the data collected? Does this timeframe match the creation timeframe\nof the data associated with the instances (e.g., recent crawl of old news articles)? If not, please\ndescribe the timeframe in which the data associated with the instances was created. Finally, list when\nthe dataset was \ufb01rst published.\nThe data was collected in 2020 and 2021. The dataset was made public after NeurIPS 2021, in March\n2022.\nWhat mechanisms or procedures were used to collect the data (e.g., hardware apparatus\nor sensor, manual human curation, software program, software API)?\nHow were these\nmechanisms or procedures validated?\nThe Argoverse 2 data comes from Argo \u2018Z1\u2019 \ufb02eet vehicles. These vehicles use Velodyne lidars and\ntraditional RGB cameras. All sensors are calibrated by Argo. HD maps and 3D object annotations\nare created and validated through a combination of computational tools and human annotations.\nObject tracks in the Motion Forecasting Dataset are created by a 3D tracking algorithm.\nWhat was the resource cost of collecting the data? (e.g. what were the required computational\nresources, and the associated \ufb01nancial costs, and energy consumption - estimate the carbon footprint.\nSee Strubell et al. for approaches in this area.)\nThe data was captured during normal \ufb02eet operations, so there was minimal overhead for logging\nparticular events. The transformation and post-processing of several terabytes of data consumed an\nestimated 1,000 machine hours. We estimate a Carbon footprint of roughly 1,000 lbs based on the\nCPU-centric workload.\nIf the dataset is a sample from a larger set, what was the sampling strategy (e.g., deterministic,\nprobabilistic with speci\ufb01c sampling probabilities)?\nThe Sensor Dataset scenarios were chosen from a larger set through manual review. The Lidar\nDataset and Motion Forecasting Dataset scenarios were chosen by heuristics which looked for\ninteresting object behaviors during \ufb02eet operations.\nWho was involved in the data collection process (e.g., students, crowdworkers, contractors)\nand how were they compensated (e.g., how much were crowdworkers paid)?\nArgo employees and Argo interns curated the data. Data collection and data annotation was done by\nArgo employees. Crowdworkers were not used.\nWere any ethical review processes conducted (e.g., by an institutional review board)?\nIf so,\nplease provide a description of these review processes, including the outcomes, as well as a link or\nother access point to any supporting documentation.\nNo.\nDoes the dataset relate to people?\nIf not, you may skip the remainder of the questions in this\nsection.\nYes.\nDid you collect the data from the individuals in question directly, or obtain it via third parties\nor other sources (e.g., websites)?\nThe data is collected from vehicles on public roads, not from a third party.\nWere the individuals in question noti\ufb01ed about the data collection? If so, please describe (or\nshow with screenshots or other information) how notice was provided, and provide a link or other\naccess point to, or otherwise reproduce, the exact language of the noti\ufb01cation itself.\nNo, but the data collection was not hidden. The Argo \ufb02eet vehicles are well marked and have obvious\n23\ncameras and lidar sensors. The vehicles only capture data from public roads.\nDid the individuals in question consent to the collection and use of their data?\nIf so, please\ndescribe (or show with screenshots or other information) how consent was requested and provided,\nand provide a link or other access point to, or otherwise reproduce, the exact language to which the\nindividuals consented.\nNo. People in the dataset were in public settings and their appearance has been anonymized. Drivers,\npedestrians, and vulnerable road users are an intrinsic part of driving on public roads, so it is impor-\ntant that datasets contain people so that the community can develop more accurate perception systems.\nIf consent was obtained, were the consenting individuals provided with a mechanism to\nrevoke their consent in the future or for certain uses? If so, please provide a description, as well\nas a link or other access point to the mechanism (if appropriate)\nn/a\nHas an analysis of the potential impact of the dataset and its use on data subjects (e.g., a data\nprotection impact analysis) been conducted? If so, please provide a description of this analysis,\nincluding the outcomes, as well as a link or other access point to any supporting documentation.\nNo.\nAny other comments?\nn/a\nPREPROCESSING / CLEANING / LABELING\nWas any preprocessing/cleaning/labeling of the data done (e.g., discretization or bucketing,\ntokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing\nof missing values)? If so, please provide a description. If not, you may skip the remainder of the\nquestions in this section.\nYes. Images are reduced from their full resolution. 3D point locations are quantized to \ufb02oat16.\nGround height maps are quantized to .3 meter resolution from their full resolution. HD map polygon\nvertex locations are quantized to .01 meter resolution. 3D annotations are smoothed. For the Motion\nForecasting Dataset, transient 3D tracks are suppressed and object locations are smoothed over time.\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned/labeled data (e.g., to\nsupport unanticipated future uses)? If so, please provide a link or other access point to the \u201craw\u201d\ndata.\nYes, but such data is not public.\nIs the software used to preprocess/clean/label the instances available? If so, please provide a\nlink or other access point.\nNo.\nAny other comments?\nn/a\nUSES\nHas the dataset been used for any tasks already? If so, please provide a description.\nYes, this manuscript benchmarks a contemporary 3D object detection method on the Sensor Dataset\nand a contemporary motion forecasting method on the Motion Forecasting Dataset.\n24\nIs there a repository that links to any or all papers or systems that use the dataset?\nIf so,\nplease provide a link or other access point.\nYes, the Argoverse 2 API can be found at https://github.com/argoverse/av2-api.\nFor the Argoverse 2 datasets, we maintain two leaderboards for 3D Detection [https://eval.ai/\nweb/challenges/challenge-page/1710] and Motion Forecasting [https://eval.ai/web/\nchallenges/challenge-page/1719].\nFor\nthe\nArgoverse\n1\ndatasets,\nwe\nmaintain\nfour\nleaderboards\nfor\n3D\nTracking\n[https://eval.ai/web/challenges/challenge-page/453/overview],\n3D\nDetection\n[https://eval.ai/web/challenges/challenge-page/725/overview],\nMotion Forecast-\ning\n[https://eval.ai/web/challenges/challenge-page/454/overview],\nand\nStereo\nDepth Estimation [https://eval.ai/web/challenges/challenge-page/917/overview].\nArgoverse 1 was also used as the basis for a Streaming Perception challenge [https:\n//eval.ai/web/challenges/challenge-page/800/overview].\nWhat (other) tasks could the dataset be used for?\nThe datasets could be used for research on visual odometry, pose estimation, lane detection, map\nautomation, self-supervised learning, structure-from-motion, scene \ufb02ow, optical \ufb02ow, time to contact\nestimation, pseudo-lidar, and point cloud forecasting.\nIs there anything about the composition of the dataset or the way it was collected and\npreprocessed/cleaned/labeled that might impact future uses?\nFor example, is there anything\nthat a future user might need to know to avoid uses that could result in unfair treatment of individuals\nor groups (e.g., stereotyping, quality of service issues) or other undesirable harms (e.g., \ufb01nancial\nharms, legal risks) If so, please provide a description. Is there anything a future user could do to\nmitigate these undesirable harms?\nNo.\nAre there tasks for which the dataset should not be used? If so, please provide a description.\nThe dataset should not be used for tasks which depend on faithful appearance of faces or license\nplates since that data has been obfuscated. For example, running a face detector to try and estimate\nhow often pedestrians use crosswalks will not result in meaningful data.\nAny other comments?\nn/a\nDISTRIBUTION\nWill the dataset be distributed to third parties outside of the entity (e.g., company, institution,\norganization) on behalf of which the dataset was created? If so, please provide a description.\nYes, the dataset is hosted on https://www.argoverse.org/ like Argoverse 1 and 1.1.\nHow will the dataset will be distributed (e.g., tarball on website, API, GitHub)?\nDoes the\ndataset have a digital object identi\ufb01er (DOI)?\nWe provide both tar.gz archives and raw \ufb01les for two of the Argoverse 2 datasets (Motion Forecasting,\nSensor), but provide only raw \ufb01les for the Lidar datasets), available via AWS transfer. See https:\n//www.argoverse.org/av2.html#download-link.\nThe Argoverse 1 and Argoverse 1.1 were distributed as a series of tar.gz \ufb01les (See\nhttps://www.argoverse.org/av1.html#download-link.\nThe \ufb01les are broken up to\nmake the process more robust to interruption (e.g. a single 2 TB \ufb01le failing after 3 days would be\nfrustrating) and to allow easier \ufb01le manipulation (an end user might not have 2 TB free on a single\ndrive, and if they do they might not be able to decompress the entire \ufb01le at once).\n25\nWhen will the dataset be distributed?\nThe data was made available for download after NeurIPS 2021, in March 2022.\nWill the dataset be distributed under a copyright or other intellectual property (IP) license,\nand/or under applicable terms of use (ToU)? If so, please describe this license and/or ToU, and\nprovide a link or other access point to, or otherwise reproduce, any relevant licensing terms or ToU,\nas well as any fees associated with these restrictions.\nYes, the dataset was released under the same Creative Commons license as Argoverse 1 \u2013 CC BY-\nNC-SA 4.0. Details can be seen at https://www.argoverse.org/about.html#terms-of-use.\nHave any third parties imposed IP-based or other restrictions on the data associated with\nthe instances?\nIf so, please describe these restrictions, and provide a link or other access point\nto, or otherwise reproduce, any relevant licensing terms, as well as any fees associated with these\nrestrictions.\nNo.\nDo any export controls or other regulatory restrictions apply to the dataset or to individual\ninstances? If so, please describe these restrictions, and provide a link or other access point to, or\notherwise reproduce, any supporting documentation.\nNo.\nAny other comments?\nn/a\nMAINTENANCE\nWho is supporting/hosting/maintaining the dataset?\nArgo AI\nHow can the owner/curator/manager of the dataset be contacted (e.g., email address)?\nThe Argoverse team responds through the Github page for the Argoverse 2 API: https://github.\ncom/argoverse/av2-api/issues.\nThe Argoverse team responds through the Github page for the Argoverse 1 API: https://github.\ncom/argoverse/argoverse-api/issues. It currently contains 2 open issues and 126 closed\nissues.\nFor privacy concerns, contact information can be found here: https://www.argoverse.org/\nabout.html#privacy\nIs there an erratum? If so, please provide a link or other access point.\nNo.\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete\ninstances)? If so, please describe how often, by whom, and how updates will be communicated to\nusers (e.g., mailing list, GitHub)?\nIt is possible that the constituent Argoverse 2 datasets are updated to correct errors. This was the\ncase with Argoverse 1 which was incremented to Argoverse 1.1. Updates will be communicated on\nGithub and through our mailing list.\nIf the dataset relates to people, are there applicable limits on the retention of the data\nassociated with the instances (e.g., were individuals in question told that their data would be\nretained for a \ufb01xed period of time and then deleted)?\nIf so, please describe these limits and\nexplain how they will be enforced.\n26\nNo.\nWill older versions of the dataset continue to be supported/hosted/maintained? If so, please\ndescribe how. If not, please describe how its obsolescence will be communicated to users.\nYes.\nWe still host Argoverse 1 even though we have declared it \u201cdeprecated\u201d.\nSee\nhttps://www.argoverse.org/av1.html#download-link.\nWe will use the same warn-\ning if we ever deprecate Argoverse 2. Note: Argoverse 2 does not deprecate Argoverse 1. They are\nindependent collections of datasets.\nIf others want to extend/augment/build on/contribute to the dataset, is there a mechanism for\nthem to do so? If so, please provide a description. Will these contributions be validated/veri\ufb01ed? If\nso, please describe how. If not, why not? Is there a process for communicating/distributing these\ncontributions to other users? If so, please provide a description.\nYes. For example, the streaming perception challenge was built by CMU researchers who added new\n2D object annotations to Argoverse 1.1 data. The Creative Commons license we use for Argoverse 2\nensures that the community can do the same thing without needing Argo\u2019s permission.\nWe do not have a mechanism for these contributions/additions to be incorporated back into the \u2018base\u2019\nArgoverse 2. Our preference would generally be to keep the \u2018base\u2019 dataset as is, and to give credit to\nnoteworthy additions by linking to them as we have done in the case of the Streaming Perception\nChallenge (see link at the top of this Argoverse page https://www.argoverse.org/tasks.html).\nAny other comments?\nn/a\nEnvironmental Impact Statement. Amount of Compute Used: We estimate 2,000 CPU and 500\nGPU hours were used in the collection of the dataset and the performance of baseline experiments.\n27\n",
    "2004.07219": "D4RL: DATASETS FOR\nDEEP DATA-DRIVEN REINFORCEMENT LEARNING\nJustin Fu\nUC Berkeley\njustinjfu@eecs.berkeley.edu\nAviral Kumar\nUC Berkeley\naviralk@berkeley.edu\nO\ufb01r Nachum\nGoogle Brain\nofirnachum@google.com\nGeorge Tucker\nGoogle Brain\ngjt@google.com\nSergey Levine\nUC Berkeley, Google Brain\nsvlevine@eecs.berkeley.edu\nABSTRACT\nThe of\ufb02ine reinforcement learning (RL) setting (also known as full batch RL),\nwhere a policy is learned from a static dataset, is compelling as progress enables\nRL methods to take advantage of large, previously-collected datasets, much like\nhow the rise of large datasets has fueled results in supervised learning. However,\nexisting online RL benchmarks are not tailored towards the of\ufb02ine setting and ex-\nisting of\ufb02ine RL benchmarks are restricted to data generated by partially-trained\nagents, making progress in of\ufb02ine RL dif\ufb01cult to measure. In this work, we in-\ntroduce benchmarks speci\ufb01cally designed for the of\ufb02ine setting, guided by key\nproperties of datasets relevant to real-world applications of of\ufb02ine RL. With a fo-\ncus on dataset collection, examples of such properties include: datasets generated\nvia hand-designed controllers and human demonstrators, multitask datasets where\nan agent performs different tasks in the same environment, and datasets collected\nwith mixtures of policies. By moving beyond simple benchmark tasks and data\ncollected by partially-trained RL agents, we reveal important and unappreciated\nde\ufb01ciencies of existing algorithms. To facilitate research, we have released our\nbenchmark tasks and datasets with a comprehensive evaluation of existing algo-\nrithms, an evaluation protocol, and open-source examples. This serves as a com-\nmon starting point for the community to identify shortcomings in existing of\ufb02ine\nRL methods and a collaborative route for progress in this emerging area.\n1\nINTRODUCTION\nFigure 1: A selection of pro-\nposed benchmark tasks.\nImpressive progress across a range of machine learning applica-\ntions has been driven by high-capacity neural network models with\nlarge, diverse training datasets (Goodfellow et al., 2016). While\nreinforcement learning (RL) algorithms have also bene\ufb01ted from\ndeep learning (Mnih et al., 2015), active data collection is typi-\ncally required for these algorithms to succeed, limiting the extent\nto which large, previously-collected datasets can be leveraged. Of-\n\ufb02ine RL (Lange et al., 2012) (also known as full batch RL), where\nagents learn from previously-collected datasets, provides a bridge\nbetween RL and supervised learning. The promise of of\ufb02ine RL\nis leveraging large, previously-collected datasets in the context of\nWebsite with code, examples, tasks, and data is available at https://sites.google.com/view/\nd4rl/\n1\narXiv:2004.07219v4  [cs.LG]  6 Feb 2021\nsequential decision making, where reward-driven learning can produce policies that reason over\ntemporally extended horizons. This could have profound implications for a range of application\ndomains, such as robotics, autonomous driving, and healthcare.\nCurrent of\ufb02ine RL methods have not ful\ufb01lled this promise yet. While recent work has investigated\ntechnical reasons for this (Fujimoto et al., 2018a; Kumar et al., 2019; Wu et al., 2019), a major\nchallenge in addressing these issues has been the lack of standard evaluation benchmarks. Ideally,\nsuch a benchmark should: a) be composed of tasks that re\ufb02ect challenges in real-world applications\nof data-driven RL, b) be widely accessible for researchers and de\ufb01ne clear evaluation protocols for\nreproducibility, and c) contain a range of dif\ufb01culty to differentiate between algorithms, especially\nchallenges particular to the of\ufb02ine RL setting.\nMost recent works (Fujimoto et al., 2018b; Wu et al., 2019; Kumar et al., 2019; Peng et al., 2019;\nAgarwal et al., 2019b) use existing online RL benchmark domains and data collected from training\nruns of online RL methods. However, these benchmarks were not designed with of\ufb02ine RL in\nmind and such datasets do not re\ufb02ect the heterogenous nature of data collected in practice. Wu\net al. (2019) \ufb01nd that existing benchmark datasets are not suf\ufb01cient to differentiate between simple\nbaseline approaches and recently proposed algorithms. Furthermore, the aforementioned works do\nnot propose a standard evaluation protocol, which makes comparing methods challenging.\nWhy simulated environments? While relying on (existing) real-world datasets is appealing, evalu-\nating a candidate policy is challenging because it weights actions differently than the data collection\nand may take actions that were not collected. Thus, evaluating a candidate policy requires either col-\nlecting additional data from the real-world system, which is hard to standardize and make broadly\navailable, or employing off-policy evaluation, which is not yet reliable enough (e.g., the NeurIPS\n2017 Criteo Ad Placement Challenge used off-policy evaluation, however, in spite of an unprece-\ndentedly large dataset because of the variance in the estimator, top entries were not statistically\ndistinguishable from the baseline). Both options are at odds with a widely-accessible and repro-\nducible benchmark. As a compromise, we use high-quality simulations that have been battle-tested\nin prior domain-speci\ufb01c work, such as in robotics and autonomous driving. These simulators allow\nresearchers to evaluate candidate policies accurately.\nOur primary contribution is the introduction of Datasets for Deep Data-Driven Reinforcement\nLearning (D4RL): a suite of tasks and datasets for benchmarking progress in of\ufb02ine RL. We fo-\ncus our design around tasks and data collection strategies that exercise dimensions of the of\ufb02ine RL\nproblem likely to occur in practical applications, such as partial observability, passively logged data,\nor human demonstrations. To serve as a reference, we benchmark state-of-the-art of\ufb02ine RL algo-\nrithms (Haarnoja et al., 2018b; Kumar et al., 2019; Wu et al., 2019; Agarwal et al., 2019b; Fujimoto\net al., 2018a; Nachum et al., 2019; Peng et al., 2019; Kumar et al., 2020) and provide reference\nimplementations as a starting point for future work. While previous studies (e.g., (Wu et al., 2019))\nfound that all methods including simple baselines performed well on the limited set of tasks used in\nprior work, we \ufb01nd that most algorithms struggle to perform well on tasks with properties crucial to\nreal-world applications such as passively logged data, narrow data distributions, and limited human\ndemonstrations. By moving beyond simple benchmark tasks and data collected by partially-trained\nRL agents, we reveal important and unappreciated de\ufb01ciencies of existing algorithms. To facilitate\nadoption, we provide an easy-to-use API for tasks, datasets, and a collection of benchmark imple-\nmentations of existing algorithms (https://sites.google.com/view/d4rl/). This is a\ncommon starting point for the community to identify shortcomings in existing of\ufb02ine RL methods,\nand provides a meaningful metric for progress in this emerging area.\n2\nRELATED WORK\nRecent work in of\ufb02ine RL has primarily used datasets generated by a previously trained behavior\npolicy, ranging from a random initial policy to a near-expert online-trained policy. This approach\nhas been used for continuous control for robotics (Fujimoto et al., 2018a; Kumar et al., 2019; Wu\net al., 2019; Gulcehre et al., 2020), navigation (Laroche et al., 2019), industrial control (Hein et al.,\n2017), and Atari video games (Agarwal et al., 2019b). To standardize the community around com-\nmon datasets, several recent works have proposed benchmarks for of\ufb02ine RL algorithms. Agarwal\net al. (2019b); Fujimoto et al. (2019) propose benchmarking based on the discrete Atari domain.\n2\nConcurrently to our work, Gulcehre et al. (2020) proposed a benchmark1 based on locomotion and\nmanipulation tasks with perceptually challenging input and partial observability. While these are\nimportant contributions, both benchmarks suffer from the same shortcomings as prior evaluation\nprotocols: they rely on data collected from online RL training runs. In contrast, with D4RL, in\naddition to collecting data from online RL training runs, we focus on a range of dataset collection\nprocedures inspired by real-world applications, such as human demonstrations, exploratory agents,\nand hand-coded controllers. As alluded to by Wu et al. (2019) and as we show in our experiments,\nthe performance of current methods depends strongly on the data collection procedure, demonstrat-\ning the importance of modeling realistic data collection procedures in a benchmark.\nOf\ufb02ine RL using large, previously-collect datasets has been successfully applied to real-world sys-\ntems such as in robotics (Cabi et al., 2019), recommender systems (Li et al., 2010; Strehl et al., 2010;\nThomas et al., 2017), and dialogue systems (Henderson et al., 2008; Pietquin et al., 2011; Jaques\net al., 2019). These successes point to the promise of of\ufb02ine RL, however, they rely on private\nreal-world systems or expensive human labeling for evaluation which is not scalable or accessible\nfor a benchmark. Moreover, signi\ufb01cant efforts have been made to incorporate large-scale datasets\ninto off-policy RL (Kalashnikov et al., 2018; Mo et al., 2018; Gu et al., 2017), but these works use\nlarge numbers of robots to collect online interaction during training. Pure online systems for real-\nworld learning have included model-based approaches (Hester et al., 2011), or approaches which\ncollect large amounts of data in parallel (Gu et al., 2017). While we believe these are promising\ndirections for future research, the focus of this work is to provide a varied and accessible platform\nfor developing algorithms.\n3\nBACKGROUND\nThe of\ufb02ine reinforcement learning problem statement is formalized within a Markov decision pro-\ncess (MDP), de\ufb01ned by a tuple (S, A, P, R, \u03c10, \u03b3), where S denotes the state space, A denotes\nthe action space, P(s\u2032|s, a) denotes the transition distribution, \u03c10(s) denotes the initial state dis-\ntribution, R(s, a) denotes the reward function, and \u03b3 \u2208(0, 1) denotes the discount factor. The\ngoal in RL is to \ufb01nd a policy \u03c0(a|s) that maximizes the expected cumulative discounted rewards\nJ(\u03c0) = E\u03c0,P,\u03c10[P\u221e\nt=0 \u03b3tR(st, at)], also known as the discounted returns.\nIn episodic RL, the algorithm is given access to the MDP via trajectory samples for arbitrary \u03c0 of\nthe algorithm\u2019s choosing. Off-policy methods may use experience replay (Lin, 1992) to store these\ntrajectories in a replay buffer D of transitions (st, at, st+1, rt), and use an off-policy algorithm such\nas Q-learning (Watkins & Dayan, 1992) to optimize \u03c0. However, these methods still iteratively\ncollect additional data, and omitting this collection step can produce poor results. For example,\nrunning state-of-the-art off-policy RL algorithms on trajectories collected from an expert policy can\nresult in diverging Q-values (Kumar et al., 2019).\nIn of\ufb02ine RL, the algorithm no longer has access to the MDP, and is instead presented with a \ufb01xed\ndataset of transitions D. The (unknown) policy that generated this data is referred to as a behavior\npolicy \u03c0B. Effective of\ufb02ine RL algorithms must handle distribution shift, as well as data collected\nvia processes that may not be representable by the chosen policy class. Levine et al. (2020) provide\na comprehensive discussion of the problems affecting of\ufb02ine RL.\n4\nTASK DESIGN FACTORS\nIn order to design a benchmark that provides a meaningful measure of progress towards realistic\napplications of of\ufb02ine RL, we choose datasets and tasks to cover a range of properties designed to\nchallenge existing RL algorithms. We discuss these properties as follows:\nNarrow and biased data distributions, such as those from deterministic policies, are problematic\nfor of\ufb02ine RL algorithms and may cause divergence both empirically (Fujimoto et al., 2018a; Kumar\net al., 2019) and theoretically (Munos, 2003; Farahmand et al., 2010; Kumar et al., 2019; Agarwal\net al., 2019a; Du et al., 2020). Narrow datasets may arise in human demonstrations, or when using\n1Note that the benchmark proposed by Gulcehre et al. (2020) contains the Real-World RL Challenges\nbenchmark (Dulac-Arnold et al., 2020) based on (Dulac-Arnold et al., 2019) and also uses data collected from\npartially-trained RL agents.\n3\nhand-crafted policies. An important challenge in of\ufb02ine RL is to be able to gracefully handle diverse\ndata distributions without algorithms diverging or producing performance worse than the provided\nbehavior. A common approach for dealing with such data distributions is to adopt a conservative\napproach which tries to keep the behavior close to the data distribution (Fujimoto et al., 2018a;\nKumar et al., 2019; Wu et al., 2019).\nUndirected and multitask data naturally arises when data is passively logged, such as recording\nuser interactions on the internet or recording videos of a car for autonomous driving. This data\nmay not necessarily be directed towards the speci\ufb01c task one is trying to accomplish. However,\npieces of trajectories can still provide useful information to learn from. For example, one may be\nable to combine sub-trajectories to accomplish a task. In the \ufb01gure to the upper-right, if an agent\nis given trajectories from A-B and B-C in a dataset (left image), it can form a trajectory from A-\nC by combining the corresponding halves of the original trajectories. We refer to this property as\nstitching, since the agent can use portions of existing trajectories in order to solve a task, rather than\nrelying on generalization outside of the dataset.\nFigure 2: An example of stitching to-\ngether subtrajectories to solve a task.\nSparse rewards.\nSparse reward problems pose chal-\nlenges to traditional RL methods due to the dif\ufb01culty of\ncredit assignment and exploration. Because of\ufb02ine RL\nconsiders \ufb01xed datasets without exploration, sparse re-\nward problems provide an unique opportunity to isolate\nthe ability of algorithms to perform credit assignment de-\ncoupled from exploration.\nSuboptimal data. For tasks with a clear objective, the\ndatasets may not contain behaviors from optimal agents. This represents a challenge for approaches\nsuch as imitation learning, which generally require expert demonstrations. We note prior work\n(e.g., (Fujimoto et al., 2018a; Kumar et al., 2019; Wu et al., 2019)) predominantly uses data with\nthis property.\nNon-representable behavior policies, non-Markovian behavior policies, and partial observ-\nability. When the dataset is generated from a partially-trained agent, we ensure that the behavior\npolicy can be realized within our model class. However, real-life behavior may not originate from a\npolicy within our model class, which can introduce additional representational errors. For example,\ndata generated from human demonstrations or hand-crafted controllers may fall outside of the model\nclass. More generally, non-Markovian policies and tasks with partial observability can introduce\nadditional modeling errors when we estimate action probabilities under the assumption that the data\nwas generated from a Markovian policy. These errors can cause additional bias for of\ufb02ine RL al-\ngorithms, especially in methods that assume access to action probabilities from a Markovian policy\nsuch as importance weighting (Precup et al., 2000).\nRealistic domains. As we discussed previously, real-world evaluation is the ideal setting for bench-\nmarking of\ufb02ine RL, however, it is at odds with a widely-accessible and reproducible benchmark. To\nstrike a balance, we opted for simulated environments which have been previously studied and are\nbroadly accepted by the research community. These simulation packages (such as MuJoCo, Flow,\nand CARLA) have been widely used to benchmark online RL methods and are known to \ufb01t well into\nthat role. Moreover, on several domains we utilize human demonstrations or mathematical models\nof human behavior in order to provide datasets generated from realistic processes. However, this did\nhave the effect of restricting our choice of tasks. While recent progress has been made on simulators\nfor recommender systems (e.g., (Ie et al., 2019)), they use \u201cstylized\u201d user models and they have not\nbeen thoroughly evaluated by the community yet. In the future as simulators mature, we hope to\ninclude additional tasks.\nIn addition, we include a variety of qualitatively different tasks to provide broad coverage of the\ntypes of domains where of\ufb02ine RL could be used. We include locomotion, traf\ufb01c management,\nautonomous driving, and robotics tasks. We also provide tasks with a wide range in dif\ufb01culty, from\ntasks current algorithms can already solve to harder problems that are currently out of reach. Finally,\nfor consistency with prior works, we also include the OpenAI Gym robotic locomotion tasks and\nsimilar datasets used by Fujimoto et al. (2018a); Kumar et al. (2019); Wu et al. (2019).\n4\n5\nTASKS AND DATASETS\nGiven the properties outlined in Section 4, we assembled the following tasks and datasets. All tasks\nconsist of an of\ufb02ine dataset (typically 106 steps) of trajectory samples for training, and a simulator\nfor evaluation. The mapping is not one-to-one \u2013 several tasks use the same simulator with different\ndatasets. Appendix C lists domains and dataset types along with their sources and Appendix A\ncontains a more comprehensive table of statistics such as size. Our code and datasets have been\nreleased open-source and are on our website at https://sites.google.com/view/d4rl/.\nMaze2D.\n(Non-markovian policies, undirected and multitask data)\nThe Maze2D domain is a navigation task requiring a 2D agent to reach\na \ufb01xed goal location. The tasks are designed to provide a simple test\nof the ability of of\ufb02ine RL algorithms to stitch together previously col-\nlected subtrajectories to \ufb01nd the shortest path to the evaluation goal.\nThree maze layouts are provided. The \u201cumaze\u201d and \u201cmedium\u201d mazes\nare shown to the right, and the \u201clarge\u201d maze is shown below.\nThe data is generated by selecting goal locations at random and then using a\nplanner that generates sequences of waypoints that are followed using a PD\ncontroller. In the \ufb01gure on the left, the waypoints, represented by circles, are\nplanned from the starting location (1) along the path to a goal (2). Upon reach-\ning a threshold distance to a waypoint, the controller updates its internal state\nto track the next waypoint along the path to the goal. Once a goal is reached,\na new goal is selected (3) and the process continues. The trajectories in the\ndataset are visualized in Appendix G. Because the controllers memorize the reached waypoints, the\ndata collection policy is non-Markovian.\nAntMaze.\n(Non-markovian policies, sparse rewards, undirected and multitask data) The AntMaze\ndomain is a navigation domain that replaces the 2D ball from Maze2D with the more complex\n8-DoF \u201cAnt\u201d quadraped robot. We introduce this domain to test the stitching challenge using a\nmorphologically complex robot that could mimic real-world robotic navigation tasks. Additionally,\nfor this task we use a sparse 0-1 reward which is activated upon reaching the goal.\nThe data is generated by training a goal reaching policy and us-\ning it in conjunction with the same high-level waypoint genera-\ntor from Maze2D to provide subgoals that guide the agent to the\ngoal. The same 3 maze layouts are used: \u201cumaze\u201d, \u201cmedium\u201d,\nand \u201clarge\u201d.\nWe introduce three \ufb02avors of datasets: 1) the ant\nis commanded to reach a speci\ufb01c goal from a \ufb01xed start location\n(antmaze-umaze-v0), 2) in the \u201cdiverse\u201d datasets, the ant is\ncommanded to a random goal from a random start location, 3) in\nthe \u201cplay\u201d datasets, the ant is commanded to speci\ufb01c hand-picked locations in the maze (which\nare not necessarily the goal at evaluation), starting from a different set of hand-picked start loca-\ntions. As in Maze2D, the controllers for this task are non-Markovian as they rely on tracking visited\nwaypoints. Trajectories in the dataset are visualized in Appendix G.\nGym-MuJoCo.\n(Suboptimal agents, narrow data distributions) The Gym-MuJoCo tasks (Hopper,\nHalfCheetah, Walker2d) are popular benchmarks used in prior work in of\ufb02ine deep RL (Fujimoto\net al., 2018a; Kumar et al., 2019; Wu et al., 2019). For consistency, we provide standardized datasets\nsimilar to previous work, and additionally propose mixing datasets to test the impact of heterogenous\npolicy mixtures. We expect that methods that rely on regularizing to the behavior policy may fail\nwhen the data contains poorly performing trajectories.\nThe \u201cmedium\u201d dataset is generated by \ufb01rst train-\ning a policy online using Soft Actor-Critic (Haarnoja\net al., 2018a), early-stopping the training, and collect-\ning 1M samples from this partially-trained policy. The\n\u201crandom\u201d datasets are generated by unrolling a ran-\ndomly initialized policy on these three domains. The\n\u201cmedium-replay\u201d dataset consists of recording all samples in the replay buffer observed during train-\n5\ning until the policy reaches the \u201cmedium\u201d level of performance. Datasets similar to these three have\nbeen used in prior work, but in order to evaluate algorithms on mixtures of policies, we further intro-\nduce a \u201cmedium-expert\u201d dataset by mixing equal amounts of expert demonstrations and suboptimal\ndata, generated via a partially trained policy or by unrolling a uniform-at-random policy.\nAdroit.\n(Non-representable policies, narrow data distributions, sparse rewards, realistic) The\nAdroit domain (Rajeswaran et al., 2018) (pictured left) involves controlling a 24-DoF simulated\nShadow Hand robot tasked with hammering a nail, opening a door, twirling a pen, or picking up and\nmoving a ball. This domain was selected to measure the effect of a narrow expert data distributions\nand human demonstrations on a sparse reward, high-dimensional robotic manipulation task.\nWhile Rajeswaran et al. (2018) propose utilizing human demonstra-\ntions, in conjunction with online RL \ufb01ne-tuning, our benchmark\nadapts these tasks for evaluating the fully of\ufb02ine RL setting. We\ninclude three types of datasets for each task, two of which are in-\ncluded from the original paper: a small amount of demonstration\ndata from a human (\u201chuman\u201d) (25 trajectories per task) and a large\namount of expert data from a \ufb01ne-tuned RL policy (\u201cexpert\u201d). To\nmimic the use-case where a practitioner collects a small amount of\nadditional data from a policy trained on the demonstrations, we in-\ntroduce a third dataset generated by training an imitation policy on\nthe demonstrations, running the policy, and mixing data at a 50-50 ratio with the demonstrations,\nreferred to as \u201ccloned.\u201d The Adroit domain has several unique properties that make it qualitatively\ndifferent from the Gym MuJoCo tasks. First, the data is collected in from human demonstrators.\nSecond, each task is dif\ufb01cult to solve with online RL, due to sparse rewards and exploration chal-\nlenges, which make cloning and online RL alone insuf\ufb01cient. Lastly, the tasks are high dimensional,\npresenting a representation learning challenge.\nFrankaKitchen.\n(Undirected and multitask data, realistic) The\nFranka Kitchen domain, \ufb01rst proposed by Gupta et al. (2019), in-\nvolves controlling a 9-DoF Franka robot in a kitchen environment\ncontaining several common household items: a microwave, a kettle,\nan overhead light, cabinets, and an oven. The goal of each task is\nto interact with the items in order to reach a desired goal con\ufb01gu-\nration. For example, one such state is to have the microwave and\nsliding cabinet door open with the kettle on the top burner and the\noverhead light on. This domain benchmarks the effect of multitask\nbehavior on a realistic, non-navigation environment in which the \u201cstitching\u201d challenge is non-trivial\nbecause the collected trajectories are complex paths through the state space. As a result, algorithms\nmust rely on generalization to unseen states in order to solve the task, rather than relying purely on\ntrajectories seen during training.\nIn order to study the effect of \u201cstitching\u201d and generalization, we use 3 datasets of human demon-\nstrations, originally proposed by Gupta et al. (2019). The \u201ccomplete\u201d dataset consists of the robot\nperforming all of the desired tasks in order. This provides data that is easy for an imitation learning\nmethod to solve. The \u201cpartial\u201d and \u201cmixed\u201d datasets consist of undirected data, where the robot\nperforms subtasks that are not necessarily related to the goal con\ufb01guration. In the \u201cpartial\u201d dataset,\na subset of the dataset is guaranteed to solve the task, meaning an imitation learning agent may\nlearn by selectively choosing the right subsets of the data. The \u201cmixed\u201d dataset contains no tra-\njectories which solve the task completely, and the RL agent must learn to assemble the relevant\nsub-trajectories. This dataset requires the highest degree of generalization in order to succeed.\nFlow.\n(Non-representable policies,\nrealistic) The Flow\nbenchmark (Vinitsky et al., 2018) is a framework for study-\ning traf\ufb01c control using deep reinforcement learning. We use\ntwo tasks in the Flow benchmark which involve controlling\nautonomous vehicles to maximize the \ufb02ow of traf\ufb01c through a\nring or merge road con\ufb01guration (left).\n6\nWe use the Flow domain in order to provide a task that simulates real-world traf\ufb01c dynamics. A\nlarge challenge in autonomous driving is to be able to directly learn from human behavior. Thus, we\ninclude \u201chuman\u201d data from agents controlled by the intelligent driver model (IDM) (Treiber et al.,\n2000), a hand-designed model of human driving behavior. In order to provide data with a wider\ndistribution as a reference, we also include \u201crandom\u201d data generated from an agent that commands\nrandom vehicle accelerations.\nOf\ufb02ine CARLA.\n(Partial observability, non-representable policies, undirected and multitask\ndata, realistic) CARLA (Dosovitskiy et al., 2017) is a high-\ufb01delity autonomous driving simulator\nthat has previously been used with imitation learning approaches (Rhinehart et al., 2018; Codevilla\net al., 2018) from large, static datasets. The agent controls the throttle (gas pedal), the steering, and\nthe break pedal for the car, and receives 48x48 RGB images from the driver\u2019s perspective as obser-\nvations. We propose two tasks for of\ufb02ine RL: lane following within a \ufb01gure eight path (shown to the\nright, top picture), and navigation within a small town (bottom picture). The principle challenge of\nthe CARLA domain is partial observability and visual complexity, as all observations are provided\nas \ufb01rst-person RGB images.\nThe datasets in both tasks are generated via hand-designed controllers\nmeant to emulate human driving - the lane-following task uses simple\nheuristics to avoid cars and keep the car within lane boundaries, whereas the\nnavigation task layers an additional high-level controller on top that takes\nturns randomly at intersections. Similarly to the Maze2D and AntMaze do-\nmains, this dataset consists of undirected navigation data in order to test\nthe \u201cstitching\u201d property, however, it is in a more perceptually challenging\ndomain.\nEvaluation protocol.\nPrevious work tunes hyperparameters with online evaluation inside the sim-\nulator, and as Wu et al. (2019) show, the hyperparameters have a large impact on performance.\nUnfortunately, extensive online evaluation is not practical in real-world applications and this leads\nto over optimistic performance expectations when the system is deployed in a truly of\ufb02ine setting.\nTo rectify this problem, we designate a subset of tasks in each domain as \u201ctraining\u201d tasks, where hy-\nperparameter tuning is allowed, and another subset as \u201cevaluation\u201d tasks on which \ufb01nal performance\nis measured (See Appendix D Table 5).\nTo facilitate comparison across tasks, we normalize scores for each environment roughly to the range\nbetween 0 and 100, by computing normalized score = 100 \u2217\nscore\u2212random score\nexpert score\u2212random score. A\nnormalized score of 0 corresponds to the average returns (over 100 episodes) of an agent taking ac-\ntions uniformly at random across the action space. A score of 100 corresponds to the average returns\nof a domain-speci\ufb01c expert. For Maze2D, and Flow domains, this corresponds to the performance\nof the hand-designed controller used to collect data. For CARLA, AntMaze, and FrankaKitchen, we\nused an estimate of the maximum score possible. For Adroit, this corresponds to a policy trained\nwith behavioral cloning on human-demonstrations and \ufb01ne-tuned with RL. For Gym-MuJoCo, this\ncorresponds to a soft-actor critic (Haarnoja et al., 2018b) agent.\n6\nBENCHMARKING PRIOR METHODS\nWe evaluated recently proposed of\ufb02ine RL algo-\nrithms and several baselines on our of\ufb02ine RL\nbenchmarks. This evaluation (1) provides base-\nlines as a reference for future work, and (2) iden-\nti\ufb01es shortcomings in existing of\ufb02ine RL algo-\nrithms in order to guide future research.\nThe\naverage normalized performance for all tasks is\nplotted in the \ufb01gure to the right.\nIt is clear\nthat as we move beyond simple tasks and data\ncollection strategies, differences between algo-\nrithms are exacerbated and de\ufb01ciencies in all al-\ngorithms are revealed. See Appendix Table 2 for\n7\nnormalized results for all tasks, Appendix Table 3 for unnormalized scores, and Appendix E for\nexperimental details.\nWe evaluated behavioral cloning (BC), online and of\ufb02ine soft actor-critic (SAC) (Haarnoja et al.,\n2018b), bootstrapping error reduction (BEAR) (Kumar et al., 2019), and behavior-regularized actor-\ncritic (BRAC) (Wu et al., 2019), advantage-weighted regression (AWR) (Peng et al., 2019), batch-\nconstrained Q-learning (BCQ) (Fujimoto et al., 2018a), continuous action random ensemble mix-\ntures (cREM) (Agarwal et al., 2019b), and AlgaeDICE (Nachum et al., 2019). We note that REM\nwas originally designed for discrete action spaces, and the continuous action version has not been\ndeveloped extensively. In most domains, we expect online SAC to outperform of\ufb02ine algorithms\nwhen given the same amount of data because this baseline is able to collect on-policy data. There\nare a few exceptions, such as for environments where exploration challenges make it dif\ufb01cult to \ufb01nd\nhigh-reward states, such as the Adroit and maze domains.\nOverall, the benchmarked algorithms obtained the most success on datasets generated from an RL-\ntrained policy, such as in the Adroit and Gym-MuJoCo domains. In these domains, of\ufb02ine RL\nalgorithms are able to match the behavior policy when given expert data, and outperform when\ngiven suboptimal data. This positive result is expected, as it is the predominant setting in which\nthese prior algorithms have been benchmarked in past work.\nAnother positive result comes in the form of sparse reward tasks. In particular, many methods were\nable to outperform the baseline online SAC method on the Adroit and AntMaze domains. This\nindicates that of\ufb02ine RL is a promising paradigm for overcoming exploration challenges, which has\nalso been con\ufb01rmed in recent work (Nair et al., 2020). We also \ufb01nd that conservative methods that\nconstrain the policy to the dataset, such as BEAR, AWR, CQL, and BCQ, are able to handle biased\nand narrow data distributions well on domains such as Flow and Gym-MuJoCo.\nTasks with undirected data, such as the Maze2D, FrankaKitchen, CARLA and AntMaze domains,\nare challenging for existing methods. Even in the simpler Maze2D domain, the large maze provides a\nsurprising challenge for most methods. However, the smaller instances of Maze2D and AntMaze are\nvery much within reach of current algorithms. Mixture distributions (a form of non-representable\npolicies) were also challenging for all algorithms we evaluated. For MuJoCo, even though the\nmedium-expert data contains expert data, the algorithms performed roughly on-par with medium\ndatasets, except for hopper. The same pattern was found in the cloned datasets for Adroit, where the\nalgorithms mostly performed on-par with the limited demonstration dataset, even though they had\naccess to additional data.\nWe \ufb01nd that many algorithms were able to succeed to some extent on tasks with controller-generated\ndata, such as Flow and carla-lane. We also note that tasks with limited data, such as human demon-\nstrations in Adroit and FrankaKitchen, remain challenging. This potentially points to the need for\nmore sample-ef\ufb01cient methods, as big datasets may not be always be available.\n7\nDISCUSSION\nWe have proposed an open-source benchmark for of\ufb02ine RL. The choice of tasks and datasets were\nmotivated by properties re\ufb02ected in real-world applications, such as narrow data distributions and\nundirected, logged behavior. Existing benchmarks have largely concentrated on robotic control\nusing data produced by policies trained with RL (Fujimoto et al., 2018a; Kumar et al., 2019; Wu\net al., 2019; Gulcehre et al., 2020; Dulac-Arnold et al., 2020). This can give a misleading sense of\nprogress, as we have shown in our experiments that many of the more challenging properties that we\nexpect real-world datasets to have appear to result in a substantial challenge for existing methods.\nWe believe that of\ufb02ine RL holds great promise as a potential paradigm to leverage vast amounts of\nexisting sequential data within the \ufb02exible decision making framework of reinforcement learning.\nWe hope that providing a benchmark that is representative of potential problems in of\ufb02ine RL, but\nthat still can be accessibly evaluated in simulation, will greatly accelerate progress in this \ufb01eld and\ncreate new opportunities to apply RL in many real-world application areas.\nThere are several important properties exhibited in some real-world applications of RL that are not\nexplored in-depth in our benchmark. One is stochasticity of the environment, which can occur in\nsystems such as \ufb01nancial markets, healthcare, or advertisement. And while we explore domains\n8\nwith large observation spaces, some domains such recommender systems can exhibit large action\nspaces. Finally, our tasks are predominantly in the robotics, autonomous driving, and traf\ufb01c control.\nThere are several other areas (e.g. in \ufb01nance, industrial, or operations research) which could provide\nsimulated, yet still challenging, benchmarks.\nUltimately, we would like to see of\ufb02ine RL applications move from simulated domains to real-world\ndomains, using real-world datasets. This includes exciting areas such as recommender systems,\nwhere user behavior can be easily logged, and medicine, where doctors must keep complete medical\nrecords for each patient. A key challenge in these domains is that reliable evaluation must be done\nin a real system or using off-policy evaluation (OPE) methods. We believe that both reliable OPE\nmethods and real-world benchmarks which are standardized across different research groups, will\nbe important to establish for future benchmarks in of\ufb02ine RL.\nACKNOWLEDGEMENTS\nWe would like to thank Abhishek Gupta, Aravind Rajeswaran, Eugene Vinitsky, and Rowan McAl-\nlister for providing implementations and assistance in setting up tasks, Michael Janner for informa-\ntive discussions, and Kelvin Xu and Yinlam Chow for feedback on an earlier draft of this paper. We\nwould like to thank NVIDIA, Google Cloud Platform and Amazon Web Services for providing com-\nputational resources. This research was supported by the Of\ufb01ce of Naval Research and the DARPA\nAssured Autonomy Program.\n9\nREFERENCES\nAlekh Agarwal, Sham M Kakade, Jason D Lee, and Gaurav Mahajan. Optimality and approximation\nwith policy gradient methods in markov decision processes. arXiv preprint arXiv:1908.00261,\n2019a.\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. Striving for simplicity in off-policy\ndeep reinforcement learning. CoRR, abs/1907.04543, 2019b. URL http://arxiv.org/\nabs/1907.04543.\nGreg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang, and\nWojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\nSerkan Cabi, Sergio G\u00b4omez Colmenarejo, Alexander Novikov, Ksenia Konyushkova, Scott Reed,\nRae Jeong, Konrad \u02d9Zo\u0142na, Yusuf Aytar, David Budden, Mel Vecerik, et al. A framework for\ndata-driven robotics. arXiv preprint arXiv:1909.12200, 2019.\nFelipe Codevilla, Matthias Miiller, Antonio L\u00b4opez, Vladlen Koltun, and Alexey Dosovitskiy. End-\nto-end driving via conditional imitation learning. In 2018 IEEE International Conference on\nRobotics and Automation (ICRA), pp. 1\u20139. IEEE, 2018.\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. CARLA:\nAn open urban driving simulator. In Proceedings of the 1st Annual Conference on Robot Learning,\npp. 1\u201316, 2017.\nSimon S. Du, Sham M. Kakade, Ruosong Wang, and Lin F. Yang. Is a good representation suf-\n\ufb01cient for sample ef\ufb01cient reinforcement learning?\nIn International Conference on Learning\nRepresentations, 2020. URL https://openreview.net/forum?id=r1genAVKPB.\nGabriel Dulac-Arnold, Daniel Mankowitz, and Todd Hester. Challenges of real-world reinforcement\nlearning. arXiv preprint arXiv:1904.12901, 2019.\nGabriel Dulac-Arnold, Nir Levine, Daniel J Mankowitz, Jerry Li, Cosmin Paduraru, Sven Gowal,\nand Todd Hester. An empirical investigation of the challenges of real-world reinforcement learn-\ning. arXiv preprint arXiv:2003.11881, 2020.\nAmir-massoud Farahmand, Csaba Szepesv\u00b4ari, and R\u00b4emi Munos. Error propagation for approximate\npolicy and value iteration. In Advances in Neural Information Processing Systems, pp. 568\u2013576,\n2010.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. arXiv preprint arXiv:1812.02900, 2018a.\nScott Fujimoto, Herke van Hoof, and David Meger. Addressing function approximation error in\nactor-critic methods. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th In-\nternational Conference on Machine Learning, volume 80 of Proceedings of Machine Learning\nResearch, pp. 1587\u20131596. PMLR, 2018b.\nScott Fujimoto, Edoardo Conti, Mohammad Ghavamzadeh, and Joelle Pineau. Benchmarking batch\ndeep reinforcement learning algorithms. arXiv preprint arXiv:1910.01708, 2019.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\nShixiang Gu, Ethan Holly, Timothy Lillicrap, and Sergey Levine. Deep reinforcement learning for\nrobotic manipulation with asynchronous off-policy updates. In 2017 IEEE international confer-\nence on robotics and automation (ICRA), pp. 3389\u20133396. IEEE, 2017.\nCaglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio G\u00b4omez Colmenarejo, Kon-\nrad Zolna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel Dulac-\nArnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, O\ufb01r Nachum, George Tucker, Nicolas\nHeess, and Nando de Freitas. Rl unplugged: Benchmarks for of\ufb02ine reinforcement learning,\n2020.\n10\nAbhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy\nlearning: Solving long-horizon tasks via imitation and reinforcement learning. arXiv preprint\narXiv:1910.11956, 2019.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. CoRR, abs/1801.01290,\n2018a. URL http://arxiv.org/abs/1801.01290.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine.\nSoft actor-critic:\nOff-\npolicy maximum entropy deep reinforcement learning with a stochastic actor. arXiv preprint\narXiv:1801.01290, 2018b.\nDaniel Hein, Steffen Udluft, Michel Tokic, Alexander Hentschel, Thomas A Runkler, and Volkmar\nSterzing. Batch reinforcement learning on the industrial benchmark: First experiences. In 2017\nInternational Joint Conference on Neural Networks (IJCNN), pp. 4214\u20134221. IEEE, 2017.\nJames Henderson, Oliver Lemon, and Kallirroi Georgila. Hybrid reinforcement/supervised learning\nof dialogue policies from \ufb01xed data sets. Computational Linguistics, 34(4):487\u2013511, 2008.\nTodd Hester, Michael Quinlan, and Peter Stone. A real-time model-based reinforcement learning\narchitecture for robot control. arXiv preprint arXiv:1105.1749, 2011.\nEugene Ie, Chih-wei Hsu, Martin Mladenov, Vihan Jain, Sanmit Narvekar, Jing Wang, Rui Wu, and\nCraig Boutilier. Recsim: A con\ufb01gurable simulation platform for recommender systems. arXiv\npreprint arXiv:1909.04847, 2019.\nNatasha Jaques, Asma Ghandeharioun, Judy Hanwen Shen, Craig Ferguson, `Agata Lapedriza, Noah\nJones, Shixiang Gu, and Rosalind W. Picard. Way off-policy batch deep reinforcement learning\nof implicit human preferences in dialog. CoRR, abs/1907.00456, 2019. URL http://arxiv.\norg/abs/1907.00456.\nDmitry Kalashnikov, Alex Irpan, Peter Pastor, Julian Ibarz, Alexander Herzog, Eric Jang, Deirdre\nQuillen, Ethan Holly, Mrinal Kalakrishnan, Vincent Vanhoucke, et al. Scalable deep reinforce-\nment learning for vision-based robotic manipulation. In Conference on Robot Learning, pp. 651\u2013\n673, 2018.\nAviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning via\nbootstrapping error reduction. In Advances in Neural Information Processing Systems, 2019.\nURL http://arxiv.org/abs/1906.00949.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for of\ufb02ine\nreinforcement learning. arXiv preprint arXiv:2006.04779, 2020.\nSascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-\nment learning, pp. 45\u201373. Springer, 2012.\nRomain Laroche, Paul Trichelair, and Remi Tachet Des Combes. Safe policy improvement with\nbaseline bootstrapping. In International Conference on Machine Learning (ICML), 2019.\nSergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Of\ufb02ine reinforcement learning: Tuto-\nrial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\nLihong Li, Wei Chu, John Langford, and Robert E Schapire.\nA contextual-bandit approach to\npersonalized news article recommendation. In Proceedings of the 19th international conference\non World wide web, pp. 661\u2013670, 2010.\nLong-Ji Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.\nMachine learning, 8(3-4):293\u2013321, 1992.\nVolodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-\nmare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-\nstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.\nNature, 518(7540):529\u2013533, February 2015. ISSN 00280836.\n11\nKaichun Mo, Haoxiang Li, Zhe Lin, and Joon-Young Lee. The adobeindoornav dataset: Towards\ndeep reinforcement learning based real-world indoor robot visual navigation. 2018.\nR\u00b4emi Munos.\nError bounds for approximate policy iteration.\nIn Proceedings of the Twentieth\nInternational Conference on International Conference on Machine Learning, pp. 560\u2013567. AAAI\nPress, 2003.\nO\ufb01r Nachum, Bo Dai, Ilya Kostrikov, Yinlam Chow, Lihong Li, and Dale Schuurmans. Algaedice:\nPolicy gradient from arbitrary experience. arXiv preprint arXiv:1912.02074, 2019.\nAshvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online reinforcement\nlearning with of\ufb02ine datasets. arXiv preprint arXiv:2006.09359, 2020.\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\nOlivier Pietquin, Matthieu Geist, Senthilkumar Chandramohan, and Herv\u00b4e Frezza-Buet. Sample-\nef\ufb01cient batch reinforcement learning for dialogue management optimization. ACM Transactions\non Speech and Language Processing (TSLP), 7(3):1\u201321, 2011.\nDoina Precup, Richard S Sutton, and Satinder Singh. Eligibility traces for off-policy policy evalua-\ntion. In ICML\u201900 Proceedings of the Seventeenth International Conference on Machine Learning,\n2000.\nAravind Rajeswaran, Vikash Kumar, Abhishek Gupta, Giulia Vezzani, John Schulman, Emanuel\nTodorov, and Sergey Levine. Learning complex dexterous manipulation with deep reinforcement\nlearning and demonstrations. In Robotics: Science and Systems, 2018.\nNicholas Rhinehart, Rowan McAllister, and Sergey Levine.\nDeep imitative models for \ufb02exible\ninference, planning, and control. arXiv preprint arXiv:1810.06544, 2018.\nAlex Strehl, John Langford, Lihong Li, and Sham M Kakade. Learning from logged implicit explo-\nration data. In Advances in neural information processing systems, pp. 2217\u20132225, 2010.\nPhilip S Thomas, Georgios Theocharous, Mohammad Ghavamzadeh, Ishan Durugkar, and Emma\nBrunskill. Predictive off-policy policy evaluation for nonstationary decision problems, with ap-\nplications to digital marketing. In AAAI, pp. 4740\u20134745, 2017.\nEmanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control.\nIn 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pp. 5026\u20135033.\nIEEE, 2012.\nMartin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traf\ufb01c states in empirical observa-\ntions and microscopic simulations. Physical review E, 62(2):1805, 2000.\nEugene Vinitsky, Aboudy Kreidieh, Luc Le Flem, Nishant Kheterpal, Kathy Jang, Cathy Wu,\nFangyu Wu, Richard Liaw, Eric Liang, and Alexandre M Bayen. Benchmarks for reinforcement\nlearning in mixed-autonomy traf\ufb01c. In Conference on Robot Learning, pp. 399\u2013409, 2018.\nChristopher J.C.H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8:279\u2013292, 1992.\nCathy Wu, Aboudy Kreidieh, Kanaad Parvate, Eugene Vinitsky, and Alexandre M Bayen. Flow:\nArchitecture and benchmarking for reinforcement learning in traf\ufb01c control.\narXiv preprint\narXiv:1710.05465, pp. 10, 2017.\nYifan Wu, George Tucker, and O\ufb01r Nachum. Behavior regularized of\ufb02ine reinforcement learning.\narXiv preprint arXiv:1911.11361, 2019.\n12\nAppendices\nA\nTASK PROPERTIES\nThe following is a full list of task properties and dataset statistics for all tasks in the benchmark. Note\nthat the full dataset for \u201ccarla-town\u201d requires over 30GB of memory to store, so we also provide a\nsubsampled version of the dataset which we used in our experiments.\nDomain\nTask Name\nController Type\n# Samples\nMaze2D\nmaze2d-umaze\nPlanner\n106\nmaze2d-medium\nPlanner\n2 \u2217106\nmaze2d-large\nPlanner\n4 \u2217106\nAntMaze\nantmaze-umaze\nPlanner\n106\nantmaze-umaze-diverse\nPlanner\n106\nantmaze-medium-play\nPlanner\n106\nantmaze-medium-diverse\nPlanner\n106\nantmaze-large-play\nPlanner\n106\nantmaze-large-diverse\nPlanner\n106\nGym-MuJoCo\nhopper-random\nPolicy\n106\nhopper-medium\nPolicy\n106\nhopper-medium-replay\nPolicy\n200920\nhopper-medium-expert\nPolicy\n2 \u00d7 106\nhalfcheetah-random\nPolicy\n106\nhalfcheetah-medium\nPolicy\n106\nhalfcheetah-medium-replay\nPolicy\n101000\nhalfcheetah-medium-expert\nPolicy\n2 \u00d7 106\nwalker2d-random\nPolicy\n106\nwalker2d-medium\nPolicy\n106\nwalker2d-medium-replay\nPolicy\n100930\nwalker2d-medium-expert\nPolicy\n2 \u00d7 106\nAdroit\npen-human\nHuman\n5000\npen-cloned\nPolicy\n5 \u2217105\npen-expert\nPolicy\n5 \u2217105\nhammer-human\nHuman\n11310\nhammer-cloned\nPolicy\n106\nhammer-expert\nPolicy\n106\ndoor-human\nHuman\n6729\ndoor-cloned\nPolicy\n106\ndoor-expert\nPolicy\n106\nrelocate-human\nHuman\n9942\nrelocate-cloned\nPolicy\n106\nrelocate-expert\nPolicy\n106\nFlow\n\ufb02ow-ring-random\nPolicy\n106\n\ufb02ow-ring-controller\nPolicy\n106\n\ufb02ow-merge-random\nPolicy\n106\n\ufb02ow-merge-controller\nPolicy\n106\nFrankaKitchen\nkitchen-complete\nPolicy\n3680\nkitchen-partial\nPolicy\n136950\nkitchen-mixed\nPolicy\n136950\nCARLA\ncarla-lane\nPlanner\n105\ncarla-town\nPlanner\n2 \u2217106 full\n105 subsampled\nTable 1: Statistics for each task in the benchmark. For the controller type, \u201cplanner\u201d refers to a\nhand-designed navigation planner, \u201chuman\u201d refers to human demonstrations, and \u201cpolicy\u201d refers to\nrandom or neural network policies. The number of samples refers to the number of environment\ntransitions recorded in the dataset.\n13\nB\nRESULTS BY DOMAIN\nThe following tables summarize performance for each domain (excluding CARLA, due to all algo-\nrithms performing poorly), sorted by the best performing algorithm to the worst.\n14\nTask Name\nSAC\nBC\nSAC-off\nBEAR\nBRAC-p\nBRAC-v\nAWR\nBCQ\naDICE\nCQL\nMaze 2D\nmaze2d-umaze\n62.7\n3.8\n88.2\n3.4\n4.7\n-16.0\n1.0\n12.8\n-15.7\n5.7\nmaze2d-medium\n21.3\n30.3\n26.1\n29.0\n32.4\n33.8\n7.6\n8.3\n10.0\n5.0\nmaze2d-large\n2.7\n5.0\n-1.9\n4.6\n10.4\n40.6\n23.7\n6.2\n-0.1\n12.5\nAntMaze\nantmaze-umaze\n0.0\n65.0\n0.0\n73.0\n50.0\n70.0\n56.0\n78.9\n0.0\n74.0\nantmaze-umaze-diverse\n0.0\n55.0\n0.0\n61.0\n40.0\n70.0\n70.3\n55.0\n0.0\n84.0\nantmaze-medium-play\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n61.2\nantmaze-medium-diverse\n0.0\n0.0\n0.0\n8.0\n0.0\n0.0\n0.0\n0.0\n0.0\n53.7\nantmaze-large-play\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n6.7\n0.0\n15.8\nantmaze-large-diverse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.2\n0.0\n14.9\nGym\nhalfcheetah-random\n100.0\n2.1\n30.5\n25.1\n24.1\n31.2\n2.5\n2.2\n-0.3\n35.4\nwalker2d-random\n100.0\n1.6\n4.1\n7.3\n-0.2\n1.9\n1.5\n4.9\n0.5\n7.0\nhopper-random\n100.0\n9.8\n11.3\n11.4\n11.0\n12.2\n10.2\n10.6\n0.9\n10.8\nhalfcheetah-medium\n100.0\n36.1\n-4.3\n41.7\n43.8\n46.3\n37.4\n40.7\n-2.2\n44.4\nwalker2d-medium\n100.0\n6.6\n0.9\n59.1\n77.5\n81.1\n17.4\n53.1\n0.3\n79.2\nhopper-medium\n100.0\n29.0\n0.8\n52.1\n32.7\n31.1\n35.9\n54.5\n1.2\n58.0\nhalfcheetah-medium-replay\n100.0\n38.4\n-2.4\n38.6\n45.4\n47.7\n40.3\n38.2\n-2.1\n46.2\nwalker2d-medium-replay\n100.0\n11.3\n1.9\n19.2\n-0.3\n0.9\n15.5\n15.0\n0.6\n26.7\nhopper-medium-replay\n100.0\n11.8\n3.5\n33.7\n0.6\n0.6\n28.4\n33.1\n1.1\n48.6\nhalfcheetah-medium-expert\n100.0\n35.8\n1.8\n53.4\n44.2\n41.9\n52.7\n64.7\n-0.8\n62.4\nwalker2d-medium-expert\n100.0\n6.4\n-0.1\n40.1\n76.9\n81.6\n53.8\n57.5\n0.4\n111.0\nhopper-medium-expert\n100.0\n111.9\n1.6\n96.3\n1.9\n0.8\n27.1\n110.9\n1.1\n98.7\nAdroit\npen-human\n21.6\n34.4\n6.3\n-1.0\n8.1\n0.6\n12.3\n68.9\n-3.3\n37.5\nhammer-human\n0.2\n1.5\n0.5\n0.3\n0.3\n0.2\n1.2\n0.5\n0.3\n4.4\ndoor-human\n-0.2\n0.5\n3.9\n-0.3\n-0.3\n-0.3\n0.4\n-0.0\n-0.0\n9.9\nrelocate-human\n-0.2\n0.0\n0.0\n-0.3\n-0.3\n-0.3\n-0.0\n-0.1\n-0.1\n0.2\npen-cloned\n21.6\n56.9\n23.5\n26.5\n1.6\n-2.5\n28.0\n44.0\n-2.9\n39.2\nhammer-cloned\n0.2\n0.8\n0.2\n0.3\n0.3\n0.3\n0.4\n0.4\n0.3\n2.1\ndoor-cloned\n-0.2\n-0.1\n0.0\n-0.1\n-0.1\n-0.1\n0.0\n0.0\n0.0\n0.4\nrelocate-cloned\n-0.2\n-0.1\n-0.2\n-0.3\n-0.3\n-0.3\n-0.2\n-0.3\n-0.3\n-0.1\npen-expert\n21.6\n85.1\n6.1\n105.9\n-3.5\n-3.0\n111.0\n114.9\n-3.5\n107.0\nhammer-expert\n0.2\n125.6\n25.2\n127.3\n0.3\n0.3\n39.0\n107.2\n0.3\n86.7\ndoor-expert\n-0.2\n34.9\n7.5\n103.4\n-0.3\n-0.3\n102.9\n99.0\n0.0\n101.5\nrelocate-expert\n-0.2\n101.3\n-0.3\n98.6\n-0.3\n-0.4\n91.5\n41.6\n-0.1\n95.0\nFlow\n\ufb02ow-ring-controller\n100.7\n-57.0\n9.2\n62.7\n-12.3\n-91.2\n75.2\n76.2\n15.2\n52.0\n\ufb02ow-ring-random\n100.7\n94.9\n70.0\n103.5\n95.7\n78.6\n80.4\n94.6\n83.6\n87.9\n\ufb02ow-merge-controller\n121.5\n114.1\n111.6\n150.4\n129.8\n143.9\n152.7\n114.8\n196.4\n157.2\n\ufb02ow-merge-random\n121.5\n-17.1\n-40.1\n-20.6\n146.2\n27.3\n99.6\n28.2\n4.7\n40.6\nFranka Kitchen\nkitchen-complete\n0.0\n33.8\n15.0\n0.0\n0.0\n0.0\n0.0\n8.1\n0.0\n43.8\nkitchen-partial\n0.6\n33.8\n0.0\n13.1\n0.0\n0.0\n15.4\n18.9\n0.0\n49.8\nkitchen-mixed\n0.0\n47.5\n2.5\n47.2\n0.0\n0.0\n10.6\n8.1\n2.5\n51.0\nCARLA\ncarla-lane\n-0.8\n31.8\n0.1\n-0.2\n18.2\n19.6\n-0.4\n-0.1\n-1.2\n20.9\ncarla-town\n1.4\n-1.8\n-1.8\n-2.7\n-4.6\n-2.6\n1.9\n1.9\n-11.2\n-2.6\nTable 2: Normalized results comparing online & of\ufb02ine SAC (SAC, SAC-off), bootstrapping error reduction (BEAR), behavior-regularized actor critic with policy\n(BRAC-p) or value (BRAC-v) regularization, behavioral cloning (BC), advantage-weighted regression (AWR), batch-constrained Q-learning (BCQ), continuous\nrandom ensemble mixtures (cREM), and AlgaeDICE (aDICE). Average results are reported over 3 seeds, and normalized to a score between 0 (random) and 100\n(expert).\n15\nDomain\nTask Name\nSAC\nBC\nSAC-off\nBEAR\nBRAC-p\nBRAC-v\nAWR\nBCQ\nAlgaeDICE\nCQL\nMaze2D\nmaze2d-umaze\n110.4\n29.0\n145.6\n28.6\n30.4\n1.7\n25.2\n41.5\n2.2\n31.7\nmaze2d-medium\n69.5\n93.2\n82.0\n89.8\n98.8\n102.4\n33.2\n35.0\n39.6\n26.4\nmaze2d-large\n14.1\n20.1\n1.5\n19.0\n34.5\n115.2\n70.1\n23.2\n6.5\n40.0\nAntMaze\nantmaze-umaze\n0.0\n0.7\n0.0\n0.7\n0.5\n0.7\n0.6\n0.8\n0.0\n0.7\nantmaze-umaze-diverse\n0.0\n0.6\n0.0\n0.6\n0.4\n0.7\n0.7\n0.6\n0.0\n0.8\nantmaze-medium-play\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.6\nantmaze-medium-diverse\n0.0\n0.0\n0.0\n0.1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.5\nantmaze-large-play\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\n0.0\n0.2\nantmaze-large-diverse\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.1\nGym\nhalfcheetah-random\n12135.0\n-17.9\n3502.0\n2831.4\n2713.6\n3590.1\n36.3\n-1.3\n-318.0\n4114.8\nwalker2d-random\n4592.3\n73.0\n192.0\n336.3\n-7.2\n87.4\n71.5\n228.0\n26.5\n322.9\nhopper-random\n3234.3\n299.4\n347.7\n349.9\n337.5\n376.3\n312.4\n323.9\n10.0\n331.2\nhalfcheetah-medium\n12135.0\n4196.4\n-808.6\n4897.0\n5158.8\n5473.8\n4366.1\n4767.9\n-551.6\n5232.1\nwalker2d-medium\n4592.3\n304.8\n44.2\n2717.0\n3559.9\n3725.8\n800.7\n2441.0\n15.5\n2664.2\nhopper-medium\n3234.3\n923.5\n5.7\n1674.5\n1044.0\n990.4\n1149.5\n1752.4\n17.5\n2557.3\nhalfcheetah-medium-replay\n12135.0\n4492.1\n-581.3\n4517.9\n5350.8\n5640.6\n4727.4\n4463.9\n-540.8\n5455.6\nwalker2d-medium-replay\n4592.3\n518.6\n87.8\n883.8\n-11.5\n44.5\n712.5\n688.7\n31.0\n1227.3\nhopper-medium-replay\n3234.3\n364.4\n93.3\n1076.8\n0.5\n-0.8\n904.0\n1057.8\n14.0\n1227.3\nhalfcheetah-medium-expert\n12135.0\n4169.4\n-55.7\n6349.6\n5208.1\n4926.6\n6267.3\n7750.8\n-377.6\n7466.9\nwalker2d-medium-expert\n4592.3\n297.0\n-5.1\n1842.7\n3533.1\n3747.5\n2469.7\n2640.3\n19.7\n5097.3\nhopper-medium-expert\n3234.3\n3621.2\n32.9\n3113.5\n42.6\n5.1\n862.0\n3588.5\n15.5\n3192.0\nAdroit\npen-human\n739.3\n1121.9\n284.8\n66.3\n339.0\n114.7\n463.1\n2149.0\n-0.7\n1214.0\nhammer-human\n-248.7\n-82.4\n-214.2\n-242.0\n-239.7\n-243.8\n-115.3\n-210.5\n-234.8\n300.2\ndoor-human\n-61.8\n-41.7\n57.2\n-66.4\n-66.5\n-66.4\n-44.4\n-56.6\n-56.5\n234.3\nrelocate-human\n-13.7\n-5.6\n-4.5\n-18.9\n-19.7\n-19.7\n-7.2\n-8.6\n-10.8\n2.0\npen-cloned\n739.3\n1791.8\n797.6\n885.4\n143.4\n22.2\n931.3\n1407.8\n8.6\n1264.6\nhammer-cloned\n-248.7\n-175.1\n-244.1\n-241.1\n-236.7\n-236.9\n-226.9\n-224.4\n-233.1\n-0.41\ndoor-cloned\n-61.8\n-60.7\n-56.3\n-60.9\n-58.7\n-59.0\n-56.1\n-56.3\n-56.4\n-44.76\nrelocate-cloned\n-13.7\n-10.1\n-16.1\n-17.6\n-19.8\n-19.4\n-16.6\n-17.5\n-18.8\n-10.66\npen-expert\n739.3\n2633.7\n277.4\n3254.1\n-7.8\n6.4\n3406.0\n3521.3\n-6.9\n3286.2\nhammer-expert\n-248.7\n16140.8\n3019.5\n16359.7\n-241.4\n-241.1\n4822.9\n13731.5\n-235.2\n11062.4\ndoor-expert\n-61.8\n969.4\n163.8\n2980.1\n-66.4\n-66.6\n2964.5\n2850.7\n-56.5\n2926.8\nrelocate-expert\n-13.7\n4289.3\n-18.2\n4173.8\n-20.6\n-21.4\n3875.5\n1759.6\n-8.7\n4019.9\nFlow\n\ufb02ow-ring-controller\n25.8\n-273.3\n-147.7\n-46.3\n-188.5\n-338.2\n-22.6\n-20.7\n-136.3\n-66.5\n\ufb02ow-ring-random\n25.8\n14.7\n-32.4\n31.0\n16.2\n-16.2\n-12.7\n14.2\n-6.8\n15.1\n\ufb02ow-merge-controller\n375.4\n359.8\n354.6\n436.5\n392.9\n422.9\n441.4\n361.4\n533.9\n450.9\n\ufb02ow-merge-random\n375.4\n82.6\n33.9\n75.1\n427.6\n176.3\n329.3\n178.3\n128.7\n204.6\nFrankaKitchen\nkitchen-complete\n0.0\n1.4\n0.6\n0.0\n0.0\n0.0\n0.0\n0.3\n0.0\n1.8\nkitchen-partial\n0.0\n1.4\n0.0\n0.5\n0.0\n0.0\n0.6\n0.8\n0.0\n1.9\nkitchen-mixed\n0.0\n1.9\n0.1\n1.9\n0.0\n0.0\n0.4\n0.3\n0.1\n2.0\nOf\ufb02ine CARLA\ncarla-lane\n-8.6\n324.7\n-0.3\n-3.0\n186.1\n199.6\n-4.5\n-1.4\n-13.4\n213.2\ncarla-town\n-79.7\n-161.5\n-159.9\n-182.5\n-231.6\n-181.5\n-65.8\n-66.6\n-400.2\n-180.379\nTable 3: The raw, un-normalized scores for each task and algorithm are reported in the table below. These scores represent the undiscounted return obtained from\nexecuting a policy in the simulator, averaged over 3 random seeds.\n16\nC\nTASK AND DATASETS\nThe following table lists the tasks and dataset types included in the benchmark, including sources\nfor each.\nDomain\nSource\nDataset Types\nMaze2D\nN/A\nUMaze, Medium, Large\nAntMaze\nN/A\nUMaze, Diverse, Play\nGym-MuJoCo\nBrockman et al. (2016)\nExpert, Random, Medium (Various)\nTodorov et al. (2012)\nMedium-Expert, Medium-Replay\nAdroit\nRajeswaran et al. (2018)\nHuman, Expert (Rajeswaran et al., 2018)\nCloned\nFlow\nWu et al. (2017)\nRandom, Controller\nFranka Kitchen\nGupta et al. (2019)\nComplete, Partial, Mixed (Gupta et al., 2019)\nCARLA\nDosovitskiy et al. (2017)\nController\nTable 4:\nDomains and dataset types contained within our benchmark. Maze2D and AntMaze\nare new domains we propose. For each dataset, we also include references to the source if orig-\ninally proposed in another work. Datasets borrowed from prior work include MuJoCo (Expert,\nRandom, Medium), Adroit (Human, Expert), and FrankaKitchen (Complete, Partial, Mixed). All\nother datasets are datasets proposed by this work.\nD\nTRAINING AND EVALUATION TASK SPLIT\nThe following table lists our recommended protocol for hyperparameter tuning. Hyperparameters\nshould be tuned on the tasks listed on the left in the \u201cTraining\u201d column, and algorithms should be\nevaluated without tuning on the tasks in the right column labeled \u201cEvaluation\u201d.\nDomain\nTraining\nEvaluation\nMaze2D\nmaze2d-umaze\nmaze2d-eval-umaze\nmaze2d-medium\nmaze2d-eval-medium\nmaze2d-large\nmaze2d-eval-large\nAntMaze\nant-umaze\nant-eval-umaze\nant-umaze-diverse\nant-eval-umaze-diverse\nant-medium-play\nant-eval-medium-play\nant-medium-diverse\nant-eval-medium-diverse\nant-large-play\nant-eval-large-play\nant-large-diverse\nant-eval-large-diverse\nAdroit\npen-human\nhammer-human\npen-cloned\nhammer-cloned\npen-expert\nhammer-expert\ndoor-human\nrelocate-human\ndoor-cloned\nrelocate-cloned\ndoor-expert\nrelocate-expert\nGym\nhalfcheetah-random\nhopper-random\nhalfcheetah-medium\nhopper-medium\nhalfcheetah-mixed\nhopper-mixed\nhalfcheetah-medium-expert\nhopper-medium-expert\nwalker2d-random\nant-random\nwalker2d-medium\nant-medium\nwalker2d-mixed\nant-mixed\nwalker2d-medium-expert\nand-medium-expert\nTable 5: Our recommended partition of tasks into \u201ctraining\u201d tasks where hyperparameter tuning is\nallowed, and \u201cevaluation\u201d tasks where \ufb01nal algorithm performance should be reported.\n17\nE\nEXPERIMENT DETAILS\nFor all experiments, we used default hyperparameter settings and minimal modi\ufb01cations to public\nimplementations wherever possible, using 500K training iterations or gradient steps. The code bases\nwe used for evaluation are listed below. The most signi\ufb01cant deviation from original published\nalgorithms was that we used an unof\ufb01cial continuous-action implementation of REM (Agarwal et al.,\n2019b), which was originally implemented for discrete action spaces. We ran our experiments using\nGoogle cloud platform (GCP) on n1-standard-4 machines.\n\u2022 BRAC\nand\nAlgaeDICE:\nhttps://github.com/google-research/\ngoogle-research\n\u2022 AWR: https://github.com/xbpeng/awr\n\u2022 SAC: https://github.com/vitchyr/rlkit\n\u2022 BEAR: https://github.com/aviralkumar2907/BEAR\n\u2022 Continuous-action\nREM:\nhttps://github.com/theSparta/off_policy_\nmujoco\n\u2022 BCQ: https://github.com/sfujim/BCQ\n\u2022 CQL: https://github.com/aviralkumar2907/CQL\nF\nASSESSING THE FEASIBILITY OF HARD TASKS\n(a) carla-town\n(b) antmaze-large\n(c) maze2d-large\nFew prior methods were able to successfully solve carla-\ntown or the larger AntMaze tasks. While including tasks\nthat present a challenge for current methods is important\nto ensure that our benchmark has room for improvement,\nit is also important to provide some con\ufb01dence that the\ntasks are actually solvable. We veri\ufb01ed this in two ways.\nFirst, we ensured that the trajectories observed in these\ntasks have adequate coverage of the state space. An il-\nlustration of the trajectories in the CARLA and AntMaze\ntasks are shown below, where trajectories are shown as\ndifferent colored lines and the goal state is marked with a\nstar. We see that in carla-town and AntMaze, each lane or\ncorridor is traversed multiple times by the agent.\nSecond, the data in AntMaze was generated by having\nthe ant follow the same high-level planner in the maze as\nin Maze2D. Because Maze2D is solvable by most meth-\nods, we would expect this to mean that AntMaze is po-\ntentially solvable as well. This While the dynamics of the\nant itself are much more complex, its walking gait is a rel-\natively regular periodic motion, and since the high-level\nwaypoints are similar, we would expect the AntMaze data\nto provide similar coverage as in the 2D mazes, as shown\nin the \ufb01gures on the right. While the Ant has more erratic motion, both datasets cover the the ma-\njority of the maze thoroughly. A comparison of the state coverage between Maze2D and AntMaze\non all tasks is shown in the following Appendix section G.\n18\nG\nMAZE DOMAIN TRAJECTORIES\nIn this section, we visualized trajectories for the datasets in the Maze2D and AntMaze domains.\nEach image plots the states visited along each trajectory as a different colored line, overlaid on top\nof the maze. The goal state is marked as a white star.\nFigure 4: Trajectories visited in the Maze2D domain. From left-to-right: maze2d-umaze, maze2d-\nmedium, and maze2d-large.\nFigure 5: Trajectories visited in the AntMaze domain. Top row, from left-to-right: antmaze-umaze,\nantmaze-medium-play, and antmaze-large-play. Bottom row, from left-to-right: antmaze-umaze-\ndiverse, antmaze-medium-diverse, and antmaze-large-diverse.\n19\n",
    "2106.11810": "nuPlan: A closed-loop ML-based planning benchmark for autonomous vehicles\nHolger Caesar\nJuraj Kabzan\nKok Seang Tan\nWhye Kit Fong\nEric Wolff\nAlex Lang\nLuke Fletcher\nOscar Beijbom\nSammy Omari\nMotional\nAbstract\nIn this work, we propose the world\u2019s \ufb01rst closed-loop\nML-based planning benchmark for autonomous driving.\nWhile there is a growing body of ML-based motion plan-\nners, the lack of established datasets and metrics has\nlimited the progress in this area.\nExisting benchmarks\nfor autonomous vehicle motion prediction have focused\non short-term motion forecasting, rather than long-term\nplanning. This has led previous works to use open-loop\nevaluation with L2-based metrics, which are not suitable\nfor fairly evaluating long-term planning.\nOur bench-\nmark overcomes these limitations by introducing a large-\nscale driving dataset, lightweight closed-loop simulator,\nand motion-planning-speci\ufb01c metrics. We provide a high-\nquality dataset with 1500h of human driving data from 4\ncities across the US and Asia with widely varying traf\ufb01c pat-\nterns (Boston, Pittsburgh, Las Vegas and Singapore). We\nwill provide a closed-loop simulation framework with re-\nactive agents and provide a large set of both general and\nscenario-speci\ufb01c planning metrics. We plan to release the\ndataset at NeurIPS 2021 and organize benchmark chal-\nlenges starting in early 2022.\n1. Introduction\nLarge-scale human labeled datasets in combination with\ndeep Convolutional Neural Networks have led to an impres-\nsive performance increase in autonomous vehicle (AV) per-\nception over the last few years [9, 4]. In contrast, exist-\ning solutions for AV planning are still primarily based on\ncarefully engineered expert systems, that require signi\ufb01cant\namounts of engineering to adapt to new geographies and do\nnot scale with more training data. We believe that providing\nsuitable data and metrics will enable ML-based planning\nand pave the way towards a full \u201cSoftware 2.0\u201d stack.\nExisting real-world benchmarks are focused on short-\nterm motion forecasting, also known as prediction [6, 4,\n11, 8], rather than planning. This is evident in the lack\nof high-level goals, the choice of metrics, and the open-\nloop evaluation. Prediction focuses on the behavior of other\nagents, while planning relates to the ego vehicle behavior.\nFigure 1.\nWe show different driving scenarios to emphasize the\nlimitations of existing benchmarks. The observed driving route\nof the ego vehicle in shown in white and the hypothetical planner\nroute in red. (a) The absence of a goal leads to ambiguity at in-\ntersections. (b) Displacement metrics do not take into account the\nmulti-modal nature of driving. (c) open-loop evaluation does not\ntake into account agent interaction.\nPrediction is typically multi-modal, which means that for\neach agent we predict the N most likely trajectories. In\ncontrast, planning is typically uni-modal (except for con-\ntingency planning) and we predict a single trajectory. As\nan example, in Fig. 1a, turning left or right at an intersec-\ntion are equally likely options. Prediction datasets lack a\nbaseline navigation route to indicate the high-level goals of\nthe agents. In Fig. 1b, the options of merging immediately\nor later are both equally valid, but the commonly used L2\ndistance-based metrics (minADE, minFDE, and miss rate)\npenalize the option that was not observed in the data. In-\ntuitively, the distance between the predicted trajectory and\nthe observed trajectory is not a suitable indicator in a multi-\nmodal scenario. In Fig. 1c, the decision whether to continue\nto overtake or get back into the lane should be based on the\nconsecutive actions of all agent vehicles, which is not possi-\nble in open-loop evaluation. Lack of closed-loop evaluation\nleads to systematic drift, making it dif\ufb01cult to evaluate be-\nyond a short time horizon (3-8s).\nWe instead provide a planning benchmark to address\nthese shortcomings. Our main contributions are:\n\u2022 The largest existing public real-world dataset for au-\ntonomous driving with high quality autolabeled tracks\nfrom 4 cities.\n\u2022 Planning metrics related to traf\ufb01c rule violation, human\ndriving similarity, vehicle dynamics, goal achievement,\nas well as scenario-based.\n\u2022 The \ufb01rst public benchmark for real-world data with a\nclosed-loop planner evaluation protocol.\n1\narXiv:2106.11810v4  [cs.CV]  4 Feb 2022\nDataset\nData\nCities\nSensor Data\nType\nEvaluation\nArgoverse\n320h\n2\nPred\nOL\nnuPredict\n5h\n2\n\u2713\nPred\nOL\nLyft\n1118h\n1\nPred\nOL\nWaymo\n570h\n6\nPred\nOL\nnuPlan\n1500h\n4\n\u2713\nPlan.\nOL+CL\nTable 1. A comparison of leading datasets for motion prediction\n(Pred) and planning (Plan). We show the dataset size, number of\ncities, availability of sensor data, dataset type, and whether it uses\nopen-loop (OL) or closed-loop (CL) evaluation. nuPredict refers\nto the prediction challenge of the nuScenes [4] dataset.\n2. Related Work\nWe review the relevant literature for prediction and plan-\nning datasets, simulation, and ML-based planning.\nPrediction datasets.\nTable 1 shows a comparison be-\ntween our dataset and relevant prediction datasets. Argo-\nverse Motion Forecasting [6] was the \ufb01rst large-scale pre-\ndiction dataset. With 320h of driving data, it was unprece-\ndented in size and provides simple semantic maps with cen-\nterlines and driveable area annotations. However, the auto-\nlabeled trajectories in the dataset are of lower quality due to\nthe state of object detection \ufb01eld at the time and the insuf\ufb01-\ncient amount of human-labeled training data (113 scenes).\nThe nuScenes prediction [4] challenge consists of 850\nhuman-labeled scenes from the nuScenes dataset. While the\nannotations are high quality and sensor data is provided, the\nsmall scale limits the number of driving variations. The Lyft\nLevel 5 Prediction Dataset [11] contains 1118h of data from\na single route of 6.8 miles. It features detailed semantic\nmaps, aerial maps, and dynamic traf\ufb01c light status. While\nthe scale is unprecedented, the autolabeled tracks are often\nnoisy and geographic diversity is limited. The Waymo Open\nMotion Dataset [8] focuses speci\ufb01cally on the interactions\nbetween agents, but does so using open-loop evaluation.\nWhile the dataset size is smaller than existing datasets at\n570h, the autolabeled tracks are of high quality [17]. They\nprovide semantic maps and dynamic traf\ufb01c light status.\nThese datasets focus on prediction, rather than planning.\nIn this work we aim to overcome this limitation by using\nplanning metrics and closed-loop evaluation. We are the\n\ufb01rst large-scale dataset to provide sensor data.\nPlanning datasets.\nCommonRoad [1] provides a \ufb01rst of\nits kind planning benchmark, that is composed of differ-\nent vehicle models, cost functions and scenarios (including\ngoals and constraints). There are both pre-recorded and in-\nteractive scenarios. With 5700 scenarios in total, the scale\nof the dataset does not support training modern deep learn-\ning based methods. All scenarios lack sensor data.\nSimulation.\nSimulators have enabled breakthroughs in\nplanning and reinforcement learning with their ability to\nsimulate physics, agents, and environmental conditions in\na closed-loop environment.\nAirSim [19] is a high-\ufb01delty simulator for AVs, such as\ndrones and cars. It includes a physics engine that can op-\nerate at a high frequency for real-time hardware-in-the-loop\nsimulation. CARLA [7] supports the training and validation\nof autonomous urban driving systems. It allows for \ufb02exible\nspeci\ufb01cation of sensor suites and environmental conditions.\nIn the CARLA Autonomous Driving Challenge1 the goal is\nto navigate a set of waypoints using different combinations\nof sensor data and HD maps. Alternatively, users can use\nscene abstraction to omit the perception task and focus on\nplanning and control aspects. This challenge is conceptu-\nally similar to what we propose, but does not use real world\ndata and provides less detailed planning metrics.\nSim-to-real transfer is an active research area for diverse\ntasks such as localization, perception, prediction, planning\nand control. [21] show that the domain gap between sim-\nulated and real-world data remains an issue, by transfer-\nring a synthetically trained tracking model to the KITTI [9]\ndataset. To overcome the domain gap, they jointly train their\nmodel using real-world data for visible and simulation data\nfor occluded objects. [3] learn how to drive by transferring\na vision-based lane following driving policy from simula-\ntion to the real world without any real-world labels. [14]\nuse reinforcement learning in simulation to obtain a driving\nsystem controlling a full-size real-world vehicle. They use\nmostly synthetic data, with labelled real-world data appear-\ning only in the training of the segmentation network.\nHowever, all simulations have fundamental limits since\nthey introduce systematic biases. More work is required\nto plausibly emulate real-world sensors, e.g. to generate\nphoto-realistic camera images.\nML-based planning.\nA new emerging research \ufb01eld is\nML-based planning for AVs using real-world data. How-\never, the \ufb01eld has yet to converge on a common input/output\nspace, dataset, or metrics. A jointly learnable behavior and\ntrajectory planner is proposed in [18]. An interpretable cost\nfunction is learned on top of models for perception, pre-\ndiction and vehicle dynamics, and evaluated in open-loop\non two unpublished datasets. An end-to-end interpretable\nneural motion planner [24] takes raw lidar point clouds and\ndynamic map data as inputs and predicts a cost map for\nplanning. They evaluate in open-loop on an unpublished\ndataset, with a planning horizon of only 3s.\nChauffeur-\nNet [2] \ufb01nds that standard behavior cloning is insuf\ufb01cient\nfor handling complex driving scenarios, even when using as\nmany as 30 million examples. They propose exposing the\nlearner to synthesized data in the form of perturbations to\nthe expert\u2019s driving and augment the imitation loss with ad-\nditional losses that penalize undesirable events and encour-\n1See carlachallenge.org\n2\nage progress. Their unpublished dataset contains 26 million\nexamples which correspond to 60 days of continuous driv-\ning. The method is evaluated in a closed-loop and an open-\nloop setup, as well as in the real world. They also show\nthat open-loop evaluation can be misleading compared to\nclosed-loop. MP3 [5] proposes an end-to-end approach to\nmapless driving, where the input is raw lidar data and a\nhigh-level navigation goal.\nThey evaluate on an unpub-\nlished dataset in open and closed-loop. Multi-modal meth-\nods have also been explored in recent works [16, 20, 13].\nThese approaches explore different strategies for fusing var-\nious modality representations in order to predict future way-\npoints or control commands.\nNeural planners were also\nused in [15, 10] to evaluate an object detector using the KL\ndivergence of the planned trajectory and the observed route.\nExisting works evaluate on different metrics which are\ninconsistent across the literature. TransFuser [16] evalu-\nates its method on the number of infractions, the percentage\nof the route distance completed, and the route completion\nweighted by an infraction multiplier. Infractions include\ncollisions with other agents, and running red lights. [20]\nevaluates its planner using off-road time, off-lane time and\nnumber of crashes, while [13, 22] report the success rate\nof reaching a given destination within a \ufb01xed time window.\n[13] also introduces another metric which measures the av-\nerage percentage of distance travelled to the goal.\nWhile ML-based planning has been studied in great de-\ntail, the lack of published datasets and a standard set of met-\nrics that provide a common framework for closed-loop eval-\nuation has limited the progress in this area. We aim to \ufb01ll\nthis gap by providing an ML-based planning dataset and\nmetrics.\n3. Dataset\nOverview.\nWe plan to release 1500 hours of data from\nLas Vegas, Boston, Pittsburgh, and Singapore. Each city\nprovides its unique driving challenges. For example, Las\nVegas includes bustling casino pick-up and drop-off points\n(PUDOs) with complex interactions and busy intersections\nwith up to 8 parallel driving lanes per direction, Boston\nroutes include drivers who love to double park, Pittsburgh\nhas its own custom precedence pattern for left turns at in-\ntersections, and Singapore features left hand traf\ufb01c. For\neach city we provide semantic maps and an API for ef\ufb01-\ncient map queries. The dataset includes lidar point clouds,\ncamera images, localization information and steering in-\nputs. While we release autolabeled agent trajectories on\nthe entire dataset, we make only a subset of the sensor data\navailable due to the vast scale of the dataset (200+ TB).\nAutolabeling.\nWe use an of\ufb02ine perception system to la-\nbel the large-scale dataset at high accuracy, without the real-\ntime constraints imposed on the online perception system\nof an AV. We use PointPillars [12] with CenterPoint [23],\na modi\ufb01ed version multi-view fusion (MVF++) [17], and\nnon-causal tracking to achieve near-human labeling perfor-\nmance.\nScenarios.\nTo enable scenario-based metrics, we auto-\nmatically annotate intervals with tags for complex scenar-\nios. These scenarios include merges, lane changes, pro-\ntected or unprotected left or right turns, interaction with\ncyclists, interaction with pedestrians at crosswalks or else-\nwhere, interactions with close proximity or high accelera-\ntion, double parked vehicles, stop controlled intersections\nand driving in construction zones.\n4. Benchmarks\nTo further the state of the art in ML-based planning, we\norganize benchmark challenges with the tasks and metrics\ndescribed below.\n4.1. Overview\nTo evaluate a proposed method against the benchmark\ndataset, users submit ML-based planning code to our eval-\nuation server. The code must follow a provided template.\nContrary to most benchmarks, the code is containerized for\nportability in order to enable closed-loop evaluation on a\nsecret test set. The planner operates either on the autola-\nbeled trajectories or, for end-to-end open-loop approaches,\ndirectly on the raw sensor data. When queried for a partic-\nular timestep, the planner returns the planned position and\nheading of the ego vehicle. A provided controller will then\ndrive a vehicle while closely tracking the planned trajectory.\nWe use a prede\ufb01ned motion model to simulate the ego vehi-\ncle motion in order to approximate a real system. The \ufb01nal\ndriven trajectory is then scored against the metrics de\ufb01ned\nin Sec 4.2.\n4.2. Tasks\nWe present the three different tasks for our dataset with\nincreasing dif\ufb01culty.\nOpen-loop.\nIn the \ufb01rst challenge, we task the planning\nsystem to mimic a human driver. For every timestep, the\ntrajectory is scored based on prede\ufb01ned metrics. It is not\nused to control the vehicle. In this case, no interactions are\nconsidered.\nClosed-loop.\nIn the closed-loop setup the planner out-\nputs a planned trajectory using the information available\nat each timestep, similar to the previous case. However,\nthe proposed trajectory is used as a reference for a con-\ntroller, and thus, the planning system is gradually corrected\nat each timestep with the new state of the vehicle. While\nthe new state of the vehicle may not coincide with that of\n3\nthe recorded state, leading to different camera views or lidar\npoint clouds, we will not perform any sensor data warping\nor novel view synthesis. In this set, we distinguish between\ntwo tasks. In the Non-reactive closed-loop task we do not\nmake any assumptions on other agents behavior and simply\nuse the observed agent trajectories. As shown in [11], the\nvast majority of interventions in closed-loop simulation is\ndue to the non-reactive nature, e.g. vehicles naively collid-\ning with the ego vehicle. In the reactive closed-loop task\nwe provide a planning model for all other agents that are\ntracked like the ego vehicle.\n4.3. Metrics\nWe split the metrics into two categories, common met-\nrics, which are computed for every scenario and scenario-\nbased metrics, which are tailored to prede\ufb01ned scenarios.\nCommon metrics.\n\u2022 Traf\ufb01c rule violation is used to measure compliance with\ncommon traf\ufb01c rules. We compute the rate of collisions\nwith other agents, rate of off-road trajectories, the time\ngap to lead agents, time to collision and the relative ve-\nlocity while passing an agents as a function of the passing\ndistance.\n\u2022 Human driving similarity is used to quantify a maneuver\nsatisfaction in comparison to a human, e.g. longitudinal\nvelocity error, longitudinal stop position error and lateral\nposition error. In addition, the resulting jerk/acceleration\nis compared to the human-level jerk/acceleration.\n\u2022 Vehicle dynamics quantify rider comfort and feasibility of\na trajectory. Rider comfort is measured by jerk, acceler-\nation, steering rate and vehicle oscillation. Feasibility is\nmeasured by violation of prede\ufb01ned limits of the same\ncriteria.\n\u2022 Goal achievement measures the route progress towards a\ngoal waypoint on the map using L2 distance.\nScenario-based metrics.\nBased on the scenario tags from\nSec. 3, we use additional metrics for challenging maneu-\nvers. For lane change, time to collision and time gap to\nlead/rear agent on the target lane is measured and scored.\nFor pedestrian/cyclist interaction, we quantify the passing\nrelative velocity while differentiating their location. Fur-\nthermore, we compare the agreement between decisions\nmade by a planner and human for crosswalks and unpro-\ntected turns (right of way).\nCommunity feedback.\nNote that the metrics shown here\nare an initial proposal and do not form an exhaustive list.\nWe will work closely with the community to add novel sce-\nnarios and metrics to achieve consensus across the commu-\nnity. Likewise, for the main challenge metric we see multi-\nple options, such as a weighted sum of metrics, a weighted\nsum of metric violations above a prede\ufb01ned threshold or a\nhierarchy of metrics. We invite the community to collab-\norate with us to de\ufb01ne the metrics that will drive this \ufb01eld\nforward.\n5. Conclusion\nIn this work we proposed the \ufb01rst ML-based planning\nbenchmark for AVs. Contrary to existing forecasting bench-\nmarks, we focus on goal-based planning, planning metrics\nand closed-loop evaluation. We hope that by providing a\ncommon benchmark, we will pave a path towards progress\nin ML-based planning, which is one of the \ufb01nal frontiers in\nautonomous driving.\nReferences\n[1] Matthias Althoff, Markus Koschi, and Stefanie Manzinger.\nCommonRoad: Composable benchmarks for motion plan-\nning on roads. In Proc. of the IEEE Intelligent Vehicles Sym-\nposium, 2017. 2\n[2] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauf-\nfeurnet: Learning to drive by imitating the best and synthe-\nsizing the worst. In RSS, 2019. 2\n[3] Alex Bewley, Jessica Rigley, Yuxuan Liu, Jeffrey Hawke,\nRichard Shen, Vinh-Dieu Lam, and Alex Kendall. Learning\nto drive from simulation without real world labels. In ICRA,\n2019. 2\n[4] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In CVPR, 2020. 1,\n2\n[5] Sergio Casas, Abbas Sadat, and Raquel Urtasun. MP3: A\nuni\ufb01ed model to map, perceive, predict and plan. In CVPR,\n2021. 3\n[6] Ming-Fang Chang, John W Lambert, Patsorn Sangkloy, Jag-\njeet Singh, Slawomir Bak, Andrew Hartnett, De Wang, Peter\nCarr, Simon Lucey, Deva Ramanan, and James Hays. Argo-\nverse: 3d tracking and forecasting with rich maps. In CVPR,\n2019. 1, 2\n[7] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio\nLopez, and Vladlen Koltun. CARLA: An open urban driving\nsimulator. CoRR, 2017. 2\n[8] Scott Ettinger, Shuyang Cheng, and Benjamin Caine et al.\nLarge scale interactive motion forecasting for autonomous\ndriving: The Waymo Open Motion Dataset. arXiv preprint\narXiv:2104.10133, 2021. 1, 2\n[9] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The KITTI dataset. IJRR,\n32(11):1231\u20131237, 2013. 1, 2\n[10] Yiluan Guo, Holger Caesar, Oscar Beijbom, Jonah Philion,\nand Sanja Fidler. The ef\ufb01cacy of neural planning metrics: A\n4\nmeta-analysis of PKL on nuscenes. In IROS Workshop on\nBenchmarking Progress in Autonomous Driving, 2020. 3\n[11] John Houston, Guido Zuidhof, and Luca Bergamini et al.\nOne thousand and one hours: Self-driving motion prediction\ndataset. arXiv preprint arXiv:2006.14480, 2020. 1, 2, 4\n[12] Alex H. Lang, Sourabh Vora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In CVPR, 2019. 3\n[13] Eraqi Hesham M., Mohamed N. Moustafa, and Jens Honer.\nConditional imitation learning driving considering camera\nand lidar fusion. In NeurIPS, 2020. 3\n[14] Blazej Osinski, Adam Jakubowski, Pawel Ziecina, Piotr\nMilos, Christopher Galias, Silviu Homoceanu, and Henryk\nMichalewski. Simulation-based reinforcement learning for\nreal-world autonomous driving. In ICRA, 2020. 2\n[15] Jonah Philion, Amlan Kar, and Sanja Fidler.\nLearning to\nevaluate perception models using planner-centric metrics. In\nCVPR, 2020. 3\n[16] Aditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-\nmodal fusion transformer for end-to-end autonomous driv-\ning. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), 2021. 3\n[17] Charles R. Qi, Yin Zhou, Mahyar Najibi, Pei Sun, Khoa Vo,\nBoyang Deng, and Dragomir Anguelov.\nOffboard 3d ob-\nject detection from point cloud sequences. arXiv preprint\narXiv:2103.05073, 2021. 2, 3\n[18] Abbas Sadat, Mengye Ren, Andrei Pokrovsky, Yen-Chen\nLin, Ersin Yumer, and Raquel Urtasun. Jointly learnable be-\nhavior and trajectory planning for self-driving vehicles. In\nIROS, 2019. 2\n[19] Shital Shah, Debadeepta Dey, Chris Lovett, and Ashish\nKapoor. AirSim: High-\ufb01delity visual and physical simula-\ntion for autonomous vehicles. In Field and Service Robotics,\n2017. 2\n[20] Ibrahim Sobh, Loay Amin, Sherif Abdelkarim, Khaled\nElmadawy, Mahmoud Saeed, Omar Abdeltawab, Mostafa\nGamal, and Ahmad El Sallab. End-to-end multi-modal sen-\nsors fusion system for urban automated driving. In NeurIPS,\n2018. 3\n[21] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien\nGaidon. Learning to track with object permanence. arXiv\npreprint arXiv:2103.14258, 2021. 2\n[22] Yi Xiao, Felipe Codevilla, Akhil Gurram, Onay Urfalioglu,\nand Antonio M. L\u00b4opez. Multimodal end-to-end autonomous\ndriving. arXiv preprint arXiv:1906.03199, 2019. 3\n[23] Tianwei Yin, Xingyi Zhou, and Philipp Kr\u00a8ahenb\u00a8uhl. Center-\nbased 3d object detection and tracking.\narXiv preprint\narXiv:2006.11275, 2020. 3\n[24] Wenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin\nYang, Sergio Casas, and Raquel Urtasun. End-to-end inter-\npretable neural motion planner. In CVPR, 2021. 2\n5\n",
    "1910.03088": "INTERACTION Dataset:\nAn INTERnational, Adversarial and Cooperative moTION Dataset\nin Interactive Driving Scenarios with Semantic Maps\nWei Zhan1, Liting Sun1, Di Wang2,\u22c6, Haojie Shi3,\u22c6, Aubrey Clausse4, Maximilian Naumann5,\u22c6,\nJulius K\u00a8ummerle5, Hendrik K\u00a8onigshof5, Christoph Stiller5, Arnaud de La Fortelle4 and Masayoshi Tomizuka1\nAbstract\u2014Interactive motion datasets of road participants are\nvital to the development of autonomous vehicles in both industry\nand academia. Research areas such as motion prediction, motion\nplanning, representation learning, imitation learning, behavior\nmodeling, behavior generation, and algorithm testing, require\nsupport from high-quality motion datasets containing interactive\ndriving scenarios with different driving cultures. In this paper,\nwe present an INTERnational, Adversarial and Cooperative\nmoTION dataset (INTERACTION dataset) in interactive driving\nscenarios with semantic maps.\nFive features of the dataset are highlighted. 1) The interactive\ndriving scenarios are diverse, including urban/highway/ramp\nmerging and lane changes, roundabouts with yield/stop signs, sig-\nnalized intersections, intersections with one/two/all-way stops, etc.\n2) Motion data from different countries and different continents\nare collected so that driving preferences and styles in different\ncultures are naturally included. 3) The driving behavior is highly\ninteractive and complex with adversarial and cooperative motions\nof various traf\ufb01c participants. Highly complex behavior such\nas negotiations, aggressive/irrational decisions and traf\ufb01c rule\nviolations are densely contained in the dataset, while regular\nbehavior can also be found from cautious car-following, stop,\nleft/right/U-turn to rational lane-change and cycling and pedes-\ntrian crossing, etc. 4) The levels of criticality span wide, from\nregular safe operations to dangerous, near-collision maneuvers.\nReal collision, although relatively slight, is also included. 5) Maps\nwith complete semantic information are provided with physical\nlayers, reference lines, lanelet connections and traf\ufb01c rules.\nThe data is recorded from drones and traf\ufb01c cameras, and the\nprocessing pipelines for both are brie\ufb02y described. Statistics of\nthe dataset in terms of number of entities and interaction density\nare also provided, along with some utilization examples in the\nareas of motion prediction, imitation learning, decision-making\nand planing, representation learning, interaction extraction and\nsocial behavior generation. The dataset can be downloaded via\nhttps://interaction-dataset.com.\nI. INTRODUCTION\nIn order to enable fully autonomous driving in complex sce-\nnarios, comprehensive understanding and accurate prediction\n1W.\nZhan,\nL.\nSun,\nand\nM.\nTomizuka\nare\nwith\nthe\nMechanical\nSystems Control (MSC) Laboratory, Department of Mechanical Engi-\nneering, University of California, Berkeley, CA 94720 USA. (e-mail:\nwzhan@berkeley.edu).\n2D. Wang is with Xi\u2019an Jiaotong University, Xi\u2019an, P.R. China.\n3H. Shi is with Harbin Institute of Technology, Harbin, China.\n4A. Clausse and A. de La Fortelle are with MINES ParisTech, Paris, France.\n5M. Naumann, J. K\u00a8ummerle, H. K\u00a8onigshof and C. Stiller are with FZI\nResearch Center for Information Technology and Karlsruhe Institute of\nTechnology, Karlsruhe, Germany.\n\u22c6The work was conducted during their visit to the MSC Lab at University\nof California, Berkeley.\nFig. 1: Examples of the detection and tracking results in highly\ninteractive driving scenarios in the dataset.\nof the behavior and motion of other road users are required.\nMoreover, autonomous vehicles need to behave like vehicles\nwith human drivers to make themselves more predictable\nto others and thus, facilitate cooperation. These are two of\nthe major challenges in the \ufb01eld of autonomous driving. To\novercome these challenges, considerable amount of research\nefforts have been devoted to: i) predicting the future intention\nand motion of other road users [1]\u2013[3], ii) modeling and\nanalyzing driving behavior [4], [5], iii) clustering the motion\nand \ufb01nding representation of the motion primitives [6], [7],\niv) cloning and imitating human and expert behavior [8], [9],\nand v) generating human-like and social behavior and motion\n[10]\u2013[12].\nAll the aforementioned research areas require interactive\nvehicle motion data from real-world driving scenarios, which\nis the most fundamental and indispensable asset. NGSIM\ndataset [13] is the most popular one used in the aforementioned\nareas, such as prediction [14]\u2013[16], behavior modeling [5],\nsocial behavior generation and planning [10], and represen-\ntation learning [6], since it is publicly available with decent\nscale and quality. The recently released highD dataset [17]\narXiv:1910.03088v1  [cs.RO]  30 Sep 2019\nalso greatly assists behavior-related research such as prediction\n[18]. Public motion datasets such as NGSIM and highD\nfacilitated, but also restricted behavior-related research due to\nlimited diversity, complexity and criticality of the scenarios\nand behavior. Also, the importance of map information and\ncompleteness of interaction entities were under-addressed in\nmost of the existing datasets. However, these missing points\nare crucial for behavior-related research, which will be dis-\ncussed in the following.\n1)\nDiversity\nof\ninteractive\ndriving\nscenarios:\nRecent\nbehavior-related research using public datasets was mostly\nrestricted to highway scenarios due to the data availability.\nThere are many more highly interactive driving scenarios to\nexplore, such as roundabouts with yield/stop signs, (unsignal-\nized) one/two/all-way stop intersections (shown in Fig. 1), sig-\nnalized intersections with unprotected left turn, zipper merge\nin cities, etc.\n2) International diving cultures: Most of the existing\ndatasets only contain driving data in one speci\ufb01c country.\nHowever, driving cultures in different countries and different\ncontinents can be distinct for very similar scenarios. Without\nmotion data in similar scenarios from different countries, it is\nnot possible to incorporate the impact of driving cultures in\ndifferent countries, such as driving styles, preferences, risk\ntolerance, understanding of traf\ufb01c rules, etc., for behavior\nmodeling and analysis as well as the design of adaptive\nprediction and planning algorithms in different countries.\n3) Complexity of the scenarios and behavior: Most of the\nscenarios in the existing public datasets are relatively simple\nand structured with explicit right-of-way. The behavior of the\ndrivers is only occasionally impacted by others. There is very\nlittle social pressure (such as several vehicles waiting behind\nand even honking) on the drivers, so that their behavior is\ncautious without aggressive and irrational decisions. A motion\ndataset with much more complex and interactive behavior and\nscenarios is expected to facilitate the research tackling real\nand challenging problems.\n4) Criticality of the situations: Critical situations (such as\nnear-collision cases) are much more challenging and valuable\nthan others for behavior-related research areas. For instance,\n[15] proposed a fatality-aware prediction benchmark empha-\nsizing prediction inaccuracies in critical situations. However,\ncritical situations are too sparse in existing motion datasets,\nand can hardly be identi\ufb01ed. Therefore, a motion dataset with\ndenser critical situations is necessary to facilitate the research\nefforts on those dif\ufb01cult problems.\n5) Map information: Map information with references and\nsemantics such as lanelet connections and traf\ufb01c rules, are\ncrucial for behavior-related research areas such as motion\nplanning and prediction. It provides key information on input\n(features), such as route and goal point [9], distance to the\nmerging point [14], [15], lateral position within the lane [10],\netc., and makes the algorithms generalizable to other scenarios.\nSuch semantic maps are currently missing for most of the\nexisting public motion datasets.\n6) Completeness of interaction entities: In order to ac-\ncurately model, predict and imitate the interactive vehicle\nbehavior, it is crucial to provide motions of all surrounding\nentities which may impact their behavior in the dataset. This\nrequirement was often overlooked when using motion data\ncollected by onboard sensors due to occlusions and limited\n\ufb01eld of view of the sensors. Although existing motion datasets\ncollected from onboard sensors contain data collected from\na wide range of areas for long time periods, complete and\nmeaningful interaction pairs are relatively sparse.\nIn this paper, we will emphasize all the aforementioned\naspects to construct an international motion dataset collected\nby drones and traf\ufb01c cameras.\n\u2022 Diverse and international: It contains a variety of highly\ninteractive driving scenarios from different countries,\nsuch as roundabouts, signalized/unsignalized intersec-\ntions, as well as highway/urban merging and lane change.\n\u2022 Complex and critical: Part of the scenarios are relatively\nunstructured with inexplicit right-of-way. The driving be-\nhavior in the dataset are highly impacted by other drivers,\nwhose behavior can be aggressive or irrational due to the\nsocial pressure. Near-collision or slight-collision scenes\nare contained in the dataset to facilitate the research for\ncritical situations.\n\u2022 Semantic map and complete information: HD maps with\nsemantics are provided to generated key features in the\ncontext. Motions of all entities which may in\ufb02uence the\ndriving behavior are included in the dataset.\nThe proposed dataset can signi\ufb01cantly facilitate behavior-\nrelated research such as motion prediction, imitation learning,\ndecision-making and planning, representation learning, inter-\naction extraction and social behavior generation. Results from\nexemplar methods in all these areas are provided utilizing the\nproposed dataset.\nII. RELATED WORK\nA. Datasets from Bird\u2019s Eye View\nAs mentioned in Section I, NGSIM dataset [13] is the most\npopular vehicle motion dataset among the behavior-related\nresearch communities. The raw data was collected by cameras\nmounted on buildings and processed automatically [20]. The\naccuracy of the dataset is mostly acceptable. However, there\nmay be steady errors, and the image projection can signi\ufb01-\ncantly enlarge the size of the vehicles. Researchers proposed\nmethods [21] to rectify the errors, but it can only improve the\nquality of a small part of the dataset. In view of the problems\nin NGSIM, highD dataset [17] was constructed by using a\ndrone with more accurate vehicle motions and larger amount of\nhigh way driving data than NGSIM. Other datasets [22], [23]\nfrom bird\u2019s eye view are more focused on pedestrian behavior\nwithout strong vehicle interactions.\nThe driving scenarios presented in NGSIM and highD are\nquite limited. NGSIM contains highway driving (including\nramp merging and double lane change) and signalized inter-\nsection scenarios. In fact, signalized intersections are mostly\ncontrolled by the traf\ufb01c lights and interactions are very rare\nTABLE I: Comparison with existing motion datasets\nhighly interactive\nscenarios\ncomplexity of\nscenarios\ndensity of\naggressive\nbehavior\nnear-collision\nsituations\nand collisions\nHD maps\nwith semantics\ncompleteness of\ninteraction entities\n& viewpoint\nNGSIM [13]\nramp merging,\n(double) lane change\nstructured roads,\nexplicit right-of-way\nlow\nvery few\nnear-collision\nno\nyes, bird\u2019s-eye-view\nfrom a building\nhighD [17]\nlane change\nstructured roads,\nexplicit right-of-way\nlow\nvery few\nnear-collision\nno\nyes, bird\u2019s-eye-view\nfrom a drone\nArgoverse [19]\nunsignalized intersections,\npedestrian crossing\nunstructured roads,\ninexplicit right-of-way\nlow\nno\nyes,\nbut partially\nonly for the ego\ndata-collection vehicle\nINTERACTION\nroundabouts, ramp merging,\ndouble lane change\nunsignalized intersections\nunstructured roads,\ninexplicit right-of-way\nhigh\nyes\nyes\nyes, bird\u2019s-eye-view\nfrom a drone\nand slight. A small amount of lane changes are interactive, but\nmost of them are neither interactive nor critical. Ramp merging\nand double lane change can be highly interactive when the\ntraf\ufb01c is relatively dense, but the amount of interaction is still\nrelatively limited in NGSIM. HighD only contains highway\ndriving scenarios with car following and lane change. Urban\nscenarios which contain densely and highly interactive behav-\nior, such as roundabouts and unsignalized intersections are not\nincluded in either of the two public datasets of vehicle motions.\nB. Datasets from Onboard Sensors\nIn addition to the bird\u2019s-eye-view motion datasets, two types\nof onboard-sensor-based ones are also publicly available. One\nincludes motion data of surrounding entities from onboard\nLiDARs and front-view cameras, such as Argoverse [19] and\nHDD dataset [24]. The other only contains motions of many\ndata-collection vehicles from onboard GPS, such as 100-car\nstudy [25].\nThere are two major advantages for datasets from onboard\nsensors. One is that a variety of driving scenarios with\nrelatively long data recording time are usually included in\nthose datasets, such as urban driving at signalized/unsignalized\nintersections and highway driving with ramp merging, etc.\nThe other is that the occlusions of LiDARs and cameras are\nrecorded so that the actual occlusions from perspective of the\nego vehicle can be partially recovered.\nCompleteness of interaction entities is a major problem\nwhen using datasets from onboard sensors for behavior-related\nresearch. For motion datasets with GPS-based \ufb02eets, it is\nhard to determine whether the vehicles in an \u201dinteractive\u201d\nmotion segment was actually interacting with each other since\nthere is no motion recording of other surrounding vehicles (or\neven pedestrians) without GPS devices installed. For motion\ndatasets constructed from onboard LiDARs and cameras, it is\nhard to guarantee that all the surrounding objects impacting\nthe behavior of other vehicles are included in the dataset\nwhen predicting the motions of others. Therefore, complete\ninteractions are relatively sparse in such kind of datasets. If\nthe sensors cannot cover the full \ufb01eld of view, it will be even\nimpossible to guarantee the completeness of information for\nthe surrounding entities of the ego data collection vehicle.\nAlso, the data collected in a large area may lead to very\nfew repetitions at the same location. It is hard to learn multi-\nmodal driving behavior for prediction or planning since only\none sequence of motions can be found with similar features\nat the same location.\nMap information is also missing in most of the motion\ndatasets. To the best of our knowledge, Argoverse is the\nonly motion dataset providing relatively rich map information.\nPhysical layer (locations of curbs, road markings, etc.) is\ncontained and semantic information (lane bounds and turn\ndirections, etc.) required by prediction and planning is partially\nincluded.\nTable I provides a comparison of the three most useful\npublic vehicle motion datasets as well as the one presented in\nthis article. The proposed dataset contains much more diverse,\ncomplex and critical scenarios and vehicle motions comparing\nto the other three. In addition, HD maps with full semantic\ninformation are provided, and the completeness of interaction\nentities is superior to datasets from onboard sensors.\nIII. FEATURES OF THE DATASET\nIn this section, we will illustrate the features of the proposed\ndataset by highlighting the diversity, internationality, complex-\nity, criticality, and semantic map.\nA. Diversity\nFig. 2 illustrates a variety of highly interactive driving\nscenarios from traf\ufb01c cameras and drones in our dataset,\nincluding zipper merging in a city (Fig. 2 (a)), ramp merging\nand lane change on a highway (Fig. 2 (b)), \ufb01ve roundabouts\nwith yield and stop signs (Fig. 2 (c) - (g)), several unsignalized\nintersections with one/two/all-way stops (Fig. 2 (h) - (j)), and\nunprotected left turn at a signalized intersection (Fig. 2 (k)). In\nFig. 2, the \ufb01rst two letters of the names represent the sources\nof the data (drone as DR and traf\ufb01c camera as TC), while next\nthree letters represent the corresponding country and the last\ntwo represent the scenario code in the dataset. The numbers\nin circles denote the branch ID for each scenario.\nFig. 2 (b) contains several subscenarios. The subscenario\nwith the upper two lanes (that merge into one \ufb01nally) is a\nzipper merging which is similar to the urban counterpart in\nFig. 2 (a), where vehicles strongly interact with each other. It\nis also a ramp for the middle two lanes. The subscenario with\nthe lower three lanes (that merge into two \ufb01nally) is a forced\nmerging and vehicles have to change their lanes.\nThe roundabout in Fig. 2 (f) is an extremely busy 7-way\nroundabout with one \u201cyield\u201d branch and six \u201cstop\u201d branches.\n(a) DR_DEU_Merging_MT\n(b) DR_CHN_Merging_ZS\n(d) DR_CHN_Roundabout_LN\n(e) DR_DEU_Roundabout_OF\n(f) DR_USA_Roundabout_FT\n(g) DR_USA_Roundabout_EP\n(h) DR_USA_Intersection_EP\n(i) DR_USA_Intersection_MA\n(j) DR_USA_Intersection_GL\n!\n!\n!\n!\n!\n\"\n\"\n\"\n\"\n\"\n#\n#\n#\n#\n!\n#\n\"\n#\n$\n$\n$\n$\n(c) DR_USA_Roundabout_SR\n!\n#\n\"\n$\n%\n&\n$\n%\n&\n'\n(k) TC_BGR_Intersection_VA\n!\n\"\n#\n$\n!\n\"\n#\n$\n%\n&\n'\nFig. 2: A variety of highly interactive driving scenarios recorded by drones in the dataset, including: (a) urban merging, (b) highway\nramp merging and lane change, (c)-(g) \ufb01ve roundabouts, and (h)-(j) unsignalized intersections, and (k) unprotected left turn at a signalized\nintersection.\nLots of vehicles enter the roundabout at the same time with\nintensive interactions and relatively high speeds. The branches\nof the roundabouts in Fig. 2 (c)-(e) are controlled by yield\nsigns, while all branches of the roundabout in Fig. 2 (g) are\ncontrolled by stop signs.\nFigure 2 (i) shows an extremely busy all-way-stop intersec-\ntion with 9 lanes controlled by stop signs. Multiple vehicles are\ninteractively inching to compete. The scenario shown in Fig. 2\n(j) contains three branches (Branch 1, 2, 5) controlled by stop\nsigns, while vehicles from Branch 3 and 6 have the right-of-\nway (RoW). Lots of vehicles are entering the intersections\nfrom all branches (except Branch 4), and vehicles holding\nRoW on the straight road are with relatively high speed. A\nbusy all-way-stop T-intersection is shown in Fig. 2 (h), while\nthree other branches (Branch 4-6) are also controlled by stop\nsigns.\nB. Internationality\nThe motion data was collected from three continents (North\nAmerica, Asia and Europe). Motion data collected by drones\nare from four countries, namely, the US, China, Germany\nand Bulgaria, as indicated in the names of the scenarios\n(USA/CHN/DEU/BGR). Vehicles in all these countries are\ndriven on the right-hand side of the road. However, driving\nculture in these countries is with remarkable distinctions.\nWe provide motion data from three roundabouts with similar\ntraf\ufb01c rules, namely, SR from the US, OF from Germany and\nLN from China. All the three roundabouts do not have stop\nsigns, and the nominal traf\ufb01c rule is that the vehicles entering\nthe roundabout should yield the ones which is already in the\nroundabout.\nWe also provide motion data from two zipper merging\nscenarios, those are, MT from Germany and ZS from China\n(the upper two lanes in Fig. 2 (b)). Although MT is urban road\nand ZS is the entrance of highway, the \u201czipper\u201d rule remains\nthe same, and the speeds are similar when the traf\ufb01c is heavy.\nC. Complexity\nIn addition to regular driving behavior such as car-\nfollowing, lane change, stop and left/right/U-turn, our dataset\nemphasizes highly interactive and complex driving behavior\nwith cooperative and adversarial motions of the vehicles.\nBy carefully choosing the locations and corresponding rush\nhours for the data collection, we were able to gather large\namounts of strong interactions within relative short period of\ntime. Strongly interactive pairs of vehicles can even appear\nevery few seconds from time to time for scenarios such as\nthe ramp in ZS, the entrance branches in FT, the all-way-\nstop intersections in EP and MA as well as the two-way-stop\nintersection in GL.\nt = 0\nt = 1 s\nt = 3.2 s\nV0\nV0\nV0\nV1\nV1\nV1\nV2\nV2\nV2\nFig. 3: A sequence of images of a dangerous insertion in GL in the\nproposed dataset.\nAlso, scenarios in FT and GL are relatively unstructured\nsince there is no explicit lane restrictions in the roundabout or\nintersections. Vehicles can exploit the space to achieve their\ngoals, sometimes showing irrational and highly dangerous\nbehavior. For instance, Fig. 3 shows a dangerous insertion of\nV0 between two vehicles (V1 and V2) stopping and making\nleft turns from Branch 3 to Branch 5 in GL (refer to Fig. 2).\nThe driver of V0 intended to drive from Branch 1 to Branch\n4 but there was no explicit road structure for the driver.\nMoreover, aggressive or irrational behavior can often be\nfound due to inexplicit nominal or practical RoW. Vehicles\nmay arrive at the stop bars almost at the same time and drivers\nmay negotiate with each other by inching or even accelerating\nin MA and EP. The traf\ufb01c in FT and GL can be very busy and\nt = 0\nt = 1 s\nt = 2 s\nV0\nV0\nV0\nV1\nV1\nV1\nFig. 4: A sequence of images of a violation for the right-of-way in\na roundabout in the proposed dataset.\nit may take even minutes for the vehicle without nominal RoW\nto enter and pass, making the driver impatient. Also, there may\nbe a queue of vehicles waiting behind and even honking to\nput social pressures to the one in the front of queue. Although\nthere are explicit traf\ufb01c rules on who goes \ufb01rst for roundabouts\nor 2-way-stop intersections, vehicles without nominal RoW\nmay be aggressive, and vehicles with nominal RoW are mostly\naware of such potential violations and are ready to react. For\nexample, V0 in Fig. 4 was entering the roundabout in FT from\nBranch 3, while V1 was in the roundabout holding the RoW.\nHowever, V0 violated the rule and forced V1 to stop and yield.\nThose factors signi\ufb01cantly increase the complexity of the\nmotions in the dataset and bring forward lots of challenging\nbut valuable research topics for the community.\nD. Criticality\nAs discussed in Section III-C, vehicles holding the nominal\nRoW (in the roundabout of FT or on the straight road of GL)\nmay often encounter slight violations from vehicles without\nnominal RoW (entering the roundabout or intersection from\nbranches controlled by stop signs). Moreover, the vehicles\nholding the RoW may have relatively high speed (40 km/h\nor even higher). Therefore, critical situations can be observed\nin the dataset where time-to-collision-point (TTCP) can be\nextremely low. A slight collision can even be found in the\ndataset.\nt = 0\nt = 0.2 s\nt = 0.4 s\nV0\nV0\nV0\nV1\nV1\nV1\nFig. 5: A sequence of images of a near-collision case in the proposed\ndataset.\nFig. 5 shows a near-collision case in GL. V0 was making a\nleft turn from Branch 5 (with a stop sign) to Branch 6, while\nV1 (with the RoW) was going straight forward from Branch\n6 to Branch 3 with a relatively high speed. V1 had to execute\nemergency swerve to avoid the collision with V0, which was\nvery dangerous.\nBesides the critical, near-collision cases, a slight collision\nshown in Fig. 6 can also be found in the dataset in GL. V0\nwas making a right turn from Branch 5 (with a stop sign) to\nt = 0\nt = 0.17 s\nt = 0.27 s\nV0\nV0\nV0\nV1\nV1\nV1\nFig. 6: A sequence of images of a slight collision in the proposed\ndataset.\nBranch 3, while V1 (with the RoW) was making a right turn\nfrom Branch 6 to Branch 4. In this situation, the driver of V0\nmight have predicted that V1 was going straight to Branch 3,\nso that V0 could accelerate in advance.\nE. Semantic Map\nMap information is crucial for behavior-related research\nareas. The information required is twofold. The basic require-\nment is the physical layer containing a set of points or curves\nrepresenting curbs, road markings (lane markings, stop bars,\netc.) and other key features. In addition to the physical layer,\nsemantic information is also necessary, which includes but is\nnot limited to, 1) reference paths, 2) lanelets as well as their\nconnections and turn directions, 3) traf\ufb01c rules and RoW asso-\nciated, etc. Moreover, such information needs to be organized\nwith consistent format and toolkit to facilitate the users when\nutilizing the map. All the aforementioned requirements are\nmet in our dataset, and more detailed information on map\nconstruction can be found in Section IV-C.\nIV. CONSTRUCTION OF MOTION DATA AND MAPS\nIn this section, we will discuss the pipeline for constructing\nthe motion data from both drones and traf\ufb01c cameras, as well\nas the corresponding semantic maps.\nA. Motions from Drone Data\nWe used drones such as DJI Mavic 2 and DJI Phantom\n4 to collect the raw video data. The raw videos were 4K\n(3840x2160) by 30 Hz. We downsampled the video to 10 Hz\nand process the data. The processed results are partially\nillustrated in Fig. 1. The bounding boxes are very accurate\nand the paths are smooth after going through out processing\npipeline with the following three steps.\n\u2022 Video stabilization and alignment: Due to gradual or sud-\nden drift and rotation of drones, the collected videos need\nto be stabilized via video stabilization algorithms with\ntransformation estimator. Also, similarity transformation\nis applied to project all the frames to the \ufb01rst one and\naligned with the map.\n\u2022 Detection: In order to obtain accurate bounding boxes\nof the moving obstacles, Faster R-CNN [26] is applied.\nThe boxes are highly accurate, and very few inaccurate\ndetections are recti\ufb01ed manually.\n\u2022 Data association, tracking and smoothing: Kalman \ufb01lter\nis applied for data association and tracking. To obtain\nsmooth motions of the vehicles, a Rauch-Tung-Striebel\n(RTS) smoother [27] is also incorporated.\nFig. 7: An exemplary physical layer of a lanelet2 map [34].\nB. Motions from Traf\ufb01c Camera Data\nThe data processing pipeline for motions from traf\ufb01c camera\ndata mainly contains the following steps, and more details,\nincluding the camera parameter estimation, can be found in\n[28].\n\u2022 Detection: To detect vehicles and pedestrians in each\nframe, we use a state-of-the-art object detector [29],\nwhich provides detections with 2D bounding box, in-\nstance mask and instance type.\n\u2022 Data association: Detections are grouped into tracks\nusing a combination of an Intersection-over-Union [30]\ntracker which associates detections with high mask over-\nlap in successive frames, and a visual tracker [31] to\ncompensate for miss detections.\n\u2022 Tracking and smoothing: Once detections are grouped\ninto tracks, trajectories on the ground plane are estimated\nusing a RTS smoother. For the observation model, we use\na pin-hole camera model [32]. This allows to incorporate\nmeasurements and uncertainty directly in pixels, captur-\ning the uncertainty due to the resolution, position and\norientation of the camera. For vehicles, the RTS smoother\nuses a bicycle model [33] as process model, allowing to\ncapture the kinematics constraints of vehicles.\nC. Construction of the High De\ufb01nition Maps\nAs public roads are structured environments, the particular\nroad layout of a certain area strongly affects the motion of\nall traf\ufb01c participants. The structure for vehicles mostly starts\nby subdividing the road into lanes, and later combining them\nto create junctions, roundabouts, on ramps and so on. Further,\nmovement within this structured area is guided by traf\ufb01c rules,\nsuch as speed limits or prioritizing one road over another. In\norder to model such coherence, simply mapping center-lines\nof all lanes is not suf\ufb01cient anymore.\nTABLE II: Summary of the dataset.\nScenarios\nLocations\nVideo length (min)\nnumber\nof vehicles\nTotal video\nlength (min)\nTotal number\nof vehicles\nroundabout\nUSA Roundabout SR\n40.90\n965\n365.1\n10479\nCHN Roundabout LN\n24.24\n227\nDEU Roundabout OF\n55.04\n1083\nUSA Roundabout FT\n207.62\n7496\nUSA Roundabout EP\n37.30\n708\nunsignalized intersection\nUSA Intersection EP\n66.53\n1367\n433.33\n14867\nUSA Intersection MA\n107.37\n2982\nUSA Intersection GL\n259.43\n10518\nmerging and\nlane change\nDEU Merging MT\n37.93\n574\n132.55\n10933\nCHN Merging ZS\n94.62\n10359\nsignalized intersection\nTC Intersection VA\n60\n3775\n60\n3775\nThus, in order to allow for a thorough analysis of the\nrecorded trajectories, we provide centimeter-accurate high\nde\ufb01nition maps in the lanelet2 format [34]. Within lanelet2, the\nphysical layer of the road network, such as road borders, lane\nmarkings and traf\ufb01c signs is stored. An exemplary physical\nlayer is visualized in Figure 7. From this layer, atomic lane\nelements, called lanelets, are created. They describe the course\nof the lane and form the basis for so called regulatory\nelements, which determine traf\ufb01c regulations such as the right\nof way or the speed limit.\nWhen used alongside the recorded trajectories, these\nlanelet2 maps facilitate the reasoning about why some vehicles\ndecelerate while approaching a junction, or why others do not,\ndepending on the right of way but also on the presence of other\ntraf\ufb01c participants that potentially interact.\nV. STATISTICS OF THE DATASET\nA. Scenarios and Vehicle Density\nThe dataset contains motion data collected in four categories\nof scenarios: roundabout, unsignalized intersection, signalized\nintersection, merging and lane change, as shown in Fig. 2.\nA detailed summary of the dataset is listed in Table II. In\nthe roundabout scenarios, 10479 trajectories of vehicles from\n\ufb01ve different locations were recorded for around 365 minutes.\nSimilarly, in the unsignalized intersection scenarios, three\nlocations were included and 14867 trajectories were collected\nfor around 433 minutes. In the merging and lane change\nscenarios, 10933 trajectories were recorded at two locations\nfor around 133 minutes. Finally, one location was selected for\nthe signalized intersection, which provided 3775 trajectories\nfor around 60 minutes.\nB. Metrics for Interactive Behavior Identi\ufb01cation\nTo represent the density of the interactive behavior of the\nproposed dataset, we use the metric - number of interaction\npairs per vehicle (IPV) as in proposed in [35]. To calculate\nthe IPV, a set of rules were proposed in [35] to extract the\ninteractive behavior under different spatial representations of\nvehicle paths. The set of rules and metric are brie\ufb02y reviewed\nbelow.\n1) Minimum\ntime-to-con\ufb02ict-point\ndifference\n(\u25b3TTCPmin): \u25b3TTCPmin is a metric to describe the\nrelative states of two moving vehicles in a scenario\nwhere the paths of the two vehicles share a con\ufb02ict\npoint but without any forced stop. As shown in Fig. 8,\nsuch vehicle paths include two categories: (1) paths with\nstatic crossing or merging points such as intersections\n(Fig. 8 (a)-(b)), and (2) paths with dynamic crossing\nor merging points such as ramping and lane-changing,\nas shown in Fig. 8 (c)-(d). In such scenarios, merging\ncan happen anywhere in the shaded area. We de\ufb01ne\n\u25b3TTCPmin as\n\u25b3TTCPmin =\nmin\nt\u2208[Tstart,Tend] \u25b3TTCP t\n=\nmin\nt\u2208[Tstart,Tend](TTCP t\n1 \u2212TTCP t\n2) (1)\nwhere TTCP t\ni = \u25b3dt\ni/vt\ni, i = 1, 2 is the traveling time\nto the con\ufb02ict point of each vehicle in the interactive\npairs. vt\ni and \u25b3dt\ni are, respectively, the speed of the i-th\nvehicle and its distance to the con\ufb02ict point along the\npath at time t. For the scenarios with dynamic merging\npoints, we use the actual merging points of the vehicle\ntrajectories as the con\ufb02ict points. In (1), Tstart and Tend\nare set to be long enough to cover the interaction period\nbetween vehicles. If TTCPmin \u22643 s, then it is de\ufb01ned\nthat interaction exists.\n2) Waiting Period (WP): WP is a metric for vehicles\nwith forced stops along their paths. In [35], the default\nwaiting period at stops was set as 3 s, and the behavior\ndeviation from the default one was used as an indicator\nof the interactivity, i.e., interaction exists when WP >3 s.\nC. Distribution of Interactivity\nBased on the set of rules, there are 13375 interactive pairs\nof vehicles in the proposed dataset. We compare the inter-\nactivity among three datasets: the proposed INTERACTION\ndataset, the highD dataset, and the NGSIM dataset. Results\nare shown in Fig. 9, where the x-axis represents the length\nof \u25b3TTCPmin in seconds, and the y-axis are the number\nof vehicles (Fig. 9 (a)) and the density of vehicles1 (Fig. 9\n(b)), respectively. We can see that the INTERACTION dataset\ncontains more intensive interactions with \u25b3TTCPmin \u22641s.\n1The density is given by:\ndensity = number of vehicles with particular \u25b3TTCPmin\ntotal number of vehicles in the dataset\n.\n(a) static crossing/merging points\n(b) dynamic crossing/merging points\nFig. 8: Geometry of different interactive paths. In (a), the cross-\ning/merging points between two paths are static and \ufb01xed, while in\n(b), the crossing/merging points are dynamic.\nWe also summarized the distributions of \u25b3TTCPmin and\nWP of all vehicles in the dataset over different driving sce-\nnarios. The results are shown in Fig. 10. Similarly, the x-axis\nrepresents the length of \u25b3TTCPmin and WP in seconds, and\nthe y-axis is the density of vehicles in each scenario. We can\nsee that the dataset contains highly interactive trajectories with\na high density of \u25b3TTCPmin \u22641 s, and WP greater than 3 s.\nVI. UTILIZATION EXAMPLES\nThe proposed dataset is intended to facilitate researches\nrelated to driving behavior, as mentioned in Section I. In this\nsection, we provide several utilization examples of the pro-\nposed dataset, including motion/trajectory prediction, imitation\nlearning, motion planning and validation, motion clustering\nand representation, interaction extraction and human-like be-\nhavior generation.\nA. Motion Prediction and Behavior Analysis\nMotion/trajectory prediction is of vital importance for au-\ntonomous vehicles, particularly in situations where intensive\ninteraction happens. To obtain an accurate probabilistic pre-\ndiction model of vehicle motion, both learning- and planning-\nbased approaches have been extensively explored. By pro-\nviding high-density interactive trajectories along with HD\nsemantic maps, the proposed dataset can be used for both\napproaches.\nFor instance, [36] proposed a deep latent variable model\nbased on Wasserstein auto-encoder (WAE) to improve the\ninterpretability. It incorporated the structure of recurrent neural\nnetwork with vehicle kinematic model such that the output can\nbe constrained. The motion data in FT was utilized to train\nand test the model in comparison with other state-of-the-art\nmodels such as variational auto-encoder (VAE), auto-encoder,\nand generative adversarial network (GAN). Quantitative results\nshown in Section VI-A demonstrated that the proposed WAE-\nbased method can outperform other state-of-the-art models,\nTABLE III: Comparisons of prediction accuracy from [36].\nMethods\nfeatures\nRMSE\nMAE\nWAE-based\napproach\nx\n0.013/0.011\n0.046/0.035\ny\n0.006/0.014\n0.019/0.041\n\u03c8\n0.006/0.008\n0.018/0.042\nVAE\nx\n0.018/0.016\n0.25/0.22\ny\n0.006/0.003\n0.14/0.22\n\u03c8\n0.006/0.008\n0.13/0.21\nAuto-encoder\nx\n0.315/0.044\n1.026/0.315\ny\n0.057/0.141\n0.182/0.479\n\u03c8\n0.011/0.066\n0.037/0.078\nGAN\nx\n0.024/0.020\n0.324/0.273\ny\n0.007/0.017\n0.188/0.241\n\u03c8\n0.005/0.048\n0.107/0.286\nwhen comparing the root mean square error (RMSE) and mean\nabsolute error (MAE) of the prediction for position and yaw\nangle.\nOn the other hand, [37] took advantage of the HD semantic\nmaps and combined the learning-based and the planning-\nbased prediction methods. A deep learning model based on\nconditional variational auto-encoder (CVAE) and an optimal\nplanning framework based on inverse reinforcement learning\nare dynamically combined to predict both irrational and ra-\ntional behavior of the vehicles. Bene\ufb01ting from the the HD\nsemantic information, features for the deep learning model\nwere de\ufb01ned in Frenet frame, which generated much better\nprediction performance in terms of generalization. Some ex-\nemplar results are given in Fig. 11.\nB. Imitation Learning\nThe driving behavior in the proposed dataset can also be\nused for imitation learning which directly imitates how human\ndrive in complicated scenarios. We extended the fast integrated\nlearning and control framework proposed in [38] in the FT\nroundabout scenario. As shown in Fig. 12, both the semantic\nHD map information and the states of surrounding vehicles\n(the red boxes) were included as the features. The grey box\nrepresents the current position of the ego vehicle. The green\nboxes and blue boxes, respectively, are the ground truth future\npositions and generated future positions of the ego vehicle via\nthe imitation network.\nC. Validation of Decision and Planning\nBesides motion prediction and imitation, the motion data\nand maps in the dataset can also be used for testing different\ndecision making and motion planning algorithms. The data-\nreplay motions in the dataset are more suitable to test the\nperformances of the decision-maker and planner when the\nmotions of surrounding entities are independent of the ego\nmotions. For example, the motion of the ego vehicle may not\neffect others when it does not have the RoW, or it has the\nRoW but others violate the rules or ignore the ego motion.\nThe environmental representation and motion planning\nmethods proposed in [39] were tested in the FT roundabout\nFig. 9: Distribution of the \u25b3TTCPmin in three vehicle motion datasets: the proposed INTERACTION dataset, the HighD dataset and the\nNGSIM dataset.\nFig. 10: Distribution of the \u25b3TTCPmin, and WP across different locations and scenarios in the dataset.\ncollision_rate = 0.3\ncollision_rate = 0.7\ncollision_rate = 0.7\n(c) pure learning-based method\n(d) the new method\n(b) satisfied samples\n(a) original samples\nFig. 11: Some exemplar prediction results from [37].\n(a)\n(b)\nFig. 12: Two examples of the imitation learning results by employing\nthe method in [38].\nscenario. Fig. 13 is a bird\u2019s-eye-view screen-shot of the simula-\ntion. The red rectangle represents the autonomous vehicle with\nthe planner in [39]. It was decelerating to avoid the collision\nwith a vehicle entering the roundabout although it has the\nRoW.\nWe also combined the integrated decision and planning\nframework proposed in [40] and the sample-based motion\nplanner proposed in [41] to design the decision-maker and\nplanner under uncertainty. The predictor was designed accord-\ning to [42] based on dynamic Bayesian network (DBN) to\nprovide the probabilities of the intentions of others.\nFigure 14 shows the results of the planned speed pro\ufb01le with\ncorresponding bird\u2019s-eye-view screenshots of the situations at\nspeci\ufb01c time steps. The host autonomous vehicle was entering\nthe FT roundabout, and the vehicle in the roundabout, retrieved\nfrom the proposed dataset, was exiting. When it was not clear\nwhether the target vehicle was going to exit or not, such as\nthe time step in Fig. 14 (a), the predictor returned P(exit)\nas 0.626. With the non-conservatively defensive strategy pro-\nposed in [40], the ego vehicle was able to keep accelerating to\nenter the roundabout as planned for the next 0.5 s, so that the\npotential threat with low probability (the target vehicle stays\nFig. 13: A screen-shot of simulation when testing the motion planner\nin [39] with the proposed dataset.\nin the roundabout) did not affect the ef\ufb01ciency and comfort\nof the ego vehicle. The long-term planning corresponding to\nyielding case (red curve in Fig. 14 (b)) guaranteed that the\nego vehicle was able to fully stop for the worst case.\nP(exit) = 0.626\nP(exit) = 1\n(a)\n(b)\n(c)\n(d)\nFig. 14: Screenshots of the situations and corresponding planned\nspeed pro\ufb01les by implementing decision and planning methods in\n[40], [41] with predictor in [42] by utilizing the proposed dataset.\nD. Motion Clustering and Representation Learning\nThe X-means algorithm [43] was employed to cluster the\ntrajectories and obtain motion patterns with results shown in\nFig. 15. We constructed a feature space with vehicle motions\nin Fren\u00b4et Frame based on map information. Fig. 15 (a) shows\nthe clustered trajectory segments in different colors with the\nmap. Fig. 15 (b) and (d) demonstrate the cluster results\nwith longitudinal positions and speeds of the two interacting\nvehicles as the coordinates. The clustering results with the\n\ufb01rst and second components of principle component analysis\n(PCA) for the feature space are shown in Fig. 15 (c). In\nthe \ufb01gures we can see that different interactive motions are\nseparated and similar ones are clustered, which are desirable\nresults to obtain motion patterns.\n(a)\n(b)\n(c)\n(d)\nFig. 15: Results of X-means [43] motion clustering using the pro-\nposed dataset.\nE. Extraction of Interactive Agents and Trajectories.\nThe proposed dataset can also be used to learn the in-\nteraction relationships between agents. We implemented the\nlearning method and network structure proposed in [44] to\nextract the interaction frames of two agents. Some example\nresults are given in Fig. 16, where Fig. 16 (a) and (b) provide\none exemplar pair of interacting cars in the FT scenario, while\nFig. 16 (c) and (d) represent another pair. In Fig. 16 (a) and\n(c), the paths of both of the interacting cars are provided,\nand in Fig. 16 (b) and (d), the trajectories along longitudinal\ndirections are shown. We can see that the extracted interaction\nframes (purple circle) align quite well with the ground truth\nframes (blue star).\nF. Human-like Decision and Behavior Generation\nWe can also learn decision-making models that generate\nhuman-like decisions and behaviors with the proposed dataset.\nIn [45], an interpretable human behavior model was proposed\nbased on the cumulative prospect theory (CPT). As a non-\nexpected utility theory, CPT can well explain some systemat-\nically biased or \u201cirrational\u201d behavior/decisions of human that\ncannot be explained by the expected utility theory. Parameters\nof three different models were learned and tested using the data\nin the FT roundabout scenario: a prede\ufb01ned model based on\ntime-to-collision-point (TTCP), a learning-based model based\non neural networks, and the proposed CPT-based model. The\nresults (Fig. 17) showed that the CPT-based model outper-\nformed the TTCP model and achieved similar performance\nas the learning-based model with much less training data and\nbetter interpretability.\nVII. CONCLUSION\nIn this paper, we presented a motion dataset in a variety\nof highly interactive driving scenarios from the US, Germany,\nChina and other countries, including signalized/unsignalized\nintersections, roundabouts, ramp merging and lane change\nfrom cities and highway. Complex interactive motions were\ncaptured, featuring inexplicit right-of-way, relatively unstruc-\ntured roads, as well as aggressive and irrational behavior\ncaused by impatience and social pressure. Critical (near-\ncollision and slight-collision) situations can be found in the\ndataset. We also included high-de\ufb01nition (HD) maps with\nsemantic information for all scenarios in our dataset. The\ndata was recorded from drones and traf\ufb01c cameras and the\ndata processing pipeline was brie\ufb02y described. Our map-\naided dataset with diversity, internationality, complexity and\ncriticality of scenarios and behavior can signi\ufb01cantly facilitate\ndriving-behavior-related research such as motion prediction,\nimitation learning, decision-making and planning, representa-\ntion learning, interaction extraction, and human-like behavior\ngeneration, etc. Results from various kinds of methods of\nthese research areas were demonstrated utilizing the proposed\ndataset.\nVIII. ACKNOWLEDGEMENT\nThe authors also would like to thank the Karlsruhe House\nof Young Scientists (KHYS) for their support of Maximilian\u2019s\nresearch visit at MSC Lab.\nREFERENCES\n[1] S. Lef`evre, D. Vasquez, and C. Laugier, \u201cA survey on motion prediction\nand risk assessment for intelligent vehicles,\u201d ROBOMECH Journal,\nvol. 1, no. 1, pp. 1\u201314, Jul. 2014.\n[2] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila, and\nK. O. Arras, \u201cHuman motion trajectory prediction: A survey,\u201d arXiv\npreprint arXiv:1905.06113, 2019.\n[3] W. Zhan, A. de La Fortelle, Y.-T. Chen, C.-Y. Chan, and M. Tomizuka,\n\u201cProbabilistic prediction from planning perspective: Problem formula-\ntion, representation simpli\ufb01cation and evaluation metric,\u201d in Intelligent\nVehicles Symposium (IV), 2018 IEEE, 2018, pp. 1150\u20131156.\n[4] H. Okuda, N. Ikami, T. Suzuki, Y. Tazaki, and K. Takeda, \u201cModeling\nand Analysis of Driving Behavior Based on a Probability-Weighted\nARX Model,\u201d IEEE Transactions on Intelligent Transportation Systems,\nvol. 14, no. 1, pp. 98\u2013112, Mar. 2013.\n[5] K. Driggs-Campbell, V. Govindarajan, and R. Bajcsy, \u201cIntegrating Intu-\nitive Driver Models in Autonomous Planning for Interactive Maneuvers,\u201d\nIEEE Transactions on Intelligent Transportation Systems, vol. 18, no. 12,\npp. 3461\u20133472, Dec. 2017.\n[6] Q. Lin, Y. Zhang, S. Verwer, and J. Wang, \u201cMOHA: A Multi-Mode\nHybrid Automaton Model for Learning Car-Following Behaviors,\u201d IEEE\nTransactions on Intelligent Transportation Systems, pp. 1\u20138, 2018.\n[7] W. Wang, W. Zhang, and D. Zhao, \u201cUnderstanding V2V Driving\nScenarios through Traf\ufb01c Primitives,\u201d arXiv:1807.10422 [cs, stat], Jul.\n2018, arXiv: 1807.10422.\n[8] M. Kelly, C. Sidrane, K. Driggs-Campbell, and M. J. Kochenderfer,\n\u201cHG-DAgger: Interactive Imitation Learning with Human Experts,\u201d to\nappear in IEEE International Conference on Robotics and Automation\n(ICRA), 2019.\n(a) Paths of two interacting cars\n(b) trajectories along longitudinal direc-\ntions\n(c) Paths of two interacting cars\n(d) trajectories along longitudinal direc-\ntions\nFig. 16: Two examples of the extracted interaction pairs by implementing the learning method and network structure in [44].\nFig. 17: Results of interpretable human behavior model based on the\ncumulative prospect theory (CPT) [45] using the proposed dataset.\n[9] N. Rhinehart, R. McAllister, and S. Levine, \u201cDeep Imitative Models for\nFlexible Inference, Planning, and Control,\u201d Oct. 2018.\n[10] L. Sun, W. Zhan, M. Tomizuka, and A. D. Dragan, \u201cCourteous au-\ntonomous cars,\u201d in 2018 IEEE/RSJ International Conference on Intelli-\ngent Robots and Systems (IROS).\nIEEE, 2018, pp. 663\u2013670.\n[11] C. Guo, K. Kidono, R. Terashima, and Y. Kojima, \u201cToward Human-like\nBehavior Generation in Urban Environment Based on Markov Decision\nProcess With Hybrid Potential Maps,\u201d in 2018 IEEE Intelligent Vehicles\nSymposium (IV), Jun. 2018, pp. 2209\u20132215.\n[12] M. Naumann, M. Lauer, and C. Stiller, \u201cGenerating Comfortable, Safe\nand Comprehensible Trajectories for Automated Vehicles in Mixed\nTraf\ufb01c,\u201d in Proc. IEEE Intl. Conf. Intelligent Transportation Systems,\nHawaii, USA, Nov 2018, pp. 575\u2013582.\n[13] V. Alexiadis, J. Colyar, J. Halkias, R. Hranac, and G. McHale, \u201cThe Next\nGeneration Simulation Program,\u201d Institute of Transportation Engineers.\nITE Journal; Washington, vol. 74, no. 8, pp. 22\u201326, Aug. 2004.\n[14] L. Sun, W. Zhan, and M. Tomizuka, \u201cProbabilistic Prediction of Interac-\ntive Driving Behavior via Hierarchical Inverse Reinforcement Learning,\u201d\nin 2018 21st International Conference on Intelligent Transportation\nSystems (ITSC), Nov. 2018, pp. 2111\u20132117.\n[15] W. Zhan, L. Sun, Y. Hu, J. Li, and M. Tomizuka, \u201cTowards a Fatality-\nAware Benchmark of Probabilistic Reaction Prediction in Highly In-\nteractive Driving Scenarios,\u201d in 2018 21st International Conference on\nIntelligent Transportation Systems (ITSC), Nov. 2018, pp. 3274\u20133280.\n[16] F. Altch\u00b4e and A. de La Fortelle, \u201cAn LSTM network for highway\ntrajectory prediction,\u201d in 2017 IEEE 20th International Conference on\nIntelligent Transportation Systems (ITSC), Oct. 2017, pp. 353\u2013359.\n[17] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, \u201cThe highD Dataset:\nA Drone Dataset of Naturalistic Vehicle Trajectories on German High-\nways for Validation of Highly Automated Driving Systems,\u201d in 2018 21st\nInternational Conference on Intelligent Transportation Systems (ITSC),\nNov. 2018, pp. 2118\u20132125.\n[18] K. Messaoud, I. Yahiaoui, A. Verroust-Blondet, and F. Nashashibi,\n\u201cRelational recurrent neural networks for vehicle trajectory prediction,\u201d\nin 2019 IEEE Intelligent Transportation Systems Conference (ITSC),\n2019.\n[19] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,\nD. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, \u201cArgoverse:\n3d Tracking and Forecasting With Rich Maps,\u201d in Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recognition, 2019,\npp. 8748\u20138757.\n[20] Z. Kim, G. Gomes, R. Hranac, and A. Skabardonis, \u201cA machine vision\nsystem for generating vehicle trajectories over extended freeway seg-\nments,\u201d in 12th World Congress on Intelligent Transportation Systems,\n2005.\n[21] B. Coifman and L. Li, \u201cA critical evaluation of the Next Generation Sim-\nulation (NGSIM) vehicle trajectory dataset,\u201d Transportation Research\nPart B: Methodological, vol. 105, pp. 362\u2013377, Nov. 2017.\n[22] D. Yang, L. Li, K. Redmill, and U. \u00a8Ozg\u00a8uner, \u201cTop-view Trajectories:\nA Pedestrian Dataset of Vehicle-Crowd Interaction from Controlled\nExperiments and Crowded Campus,\u201d arXiv:1902.00487 [cs], Feb. 2019,\narXiv: 1902.00487.\n[23] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, \u201cLearning Social\nEtiquette: Human Trajectory Understanding In Crowded Scenes,\u201d in\nECCV 2016.\nSpringer International Publishing, 2016, pp. 549\u2013565.\n[24] V. Ramanishka, Y.-T. Chen, T. Misu, and K. Saenko, \u201cToward Driving\nScene Understanding: A Dataset for Learning Driver Behavior and\nCausal Reasoning,\u201d in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018, pp. 7699\u20137707.\n[25] V. L. Neale, T. A. Dingus, S. G. Klauer, J. Sudweeks, and M. Goodman,\n\u201cAn overview of the 100-car naturalistic study and \ufb01ndings,\u201d National\nHighway Traf\ufb01c Safety Administration, Paper, vol. 5, p. 0400, 2005.\n[26] S. Ren, K. He, R. Girshick, and J. Sun, \u201cFaster R-CNN: Towards Real-\nTime Object Detection with Region Proposal Networks,\u201d in Advances\nin Neural Information Processing Systems, 2015, pp. 91\u201399.\n[27] D. Simon, Optimal State Estimation: Kalman, H In\ufb01nity, and Nonlinear\nApproaches.\nNew York, NY, USA: Wiley-Interscience, 2006.\n[28] A. Clausse, S. Benslimane, and A. De La Fortelle, \u201cLarge-scale extrac-\ntion of accurate vehicle trajectories for driving behavior learning,\u201d 30th\nIEEE Intelligent Vehicles Symposium (IV), 2019.\n[29] K. He, G. Gkioxari, P. Doll\u00b4ar, and R. B. Girshick, \u201cMask R-CNN,\u201d\n2017 IEEE International Conference on Computer Vision (ICCV), pp.\n2980\u20132988, 2017.\n[30] E. Bochinski, V. Eiselein, and T. Sikora, \u201cHigh-speed tracking-by-\ndetection without using image information,\u201d in International Workshop\non\nTraf\ufb01c\nand\nStreet\nSurveillance\nfor\nSafety\nand\nSecurity\nat\nIEEE AVSS 2017, Lecce, Italy, Aug. 2017. [Online]. Available:\nhttp://elvera.nue.tu-berlin.de/\ufb01les/1517Bochinski2017.pdf\n[31] A. Luke\u02c7zi\u02c7c, T. Voj\u2019i\u02c7r, L. \u02c7Cehovin Zajc, J. Matas, and M. Kristan, \u201cDis-\ncriminative correlation \ufb01lter tracker with channel and spatial reliability,\u201d\nInternational Journal of Computer Vision, 2018.\n[32] D. C. Brown, \u201cClose-range camera calibration,\u201d PHOTOGRAMMETRIC\nENGINEERING, vol. 37, no. 8, pp. 855\u2013866, 1971.\n[33] P. Polack, F. Altch\u00b4e, B. d\u2019Andr\u00b4ea-Novel, and A. de La Fortelle, \u201cThe\nkinematic bicycle model: A consistent model for planning feasible\ntrajectories for autonomous vehicles?\u201d in 2017 IEEE Intelligent Vehicles\nSymposium (IV), June 2017, pp. 812\u2013818.\n[34] F. Poggenhans, J. Pauls, J. Janosovits, S. Orf, M. Naumann, F. Kuhnt,\nand M. Mayr, \u201cLanelet2: A high-de\ufb01nition map framework for the\nfuture of automated driving,\u201d in 2018 21st International Conference on\nIntelligent Transportation Systems (ITSC), Nov. 2018, pp. 1672\u20131679.\n[35] W. Zhan, L. Sun, D. Wang, Y. Jin, and M. Tomizuka, \u201cConstructing a\nHighly Interactive Vehicle Motion Dataset,\u201d in 2019 IEEE/RSJ Interna-\ntional Conference on Intelligent Robots and Systems (IROS), 2019.\n[36] H. Ma, J. Li, W. Zhan, and M. Tomizuka, \u201cWasserstein Generative\nLearning with Kinematic Constraints for Probabilistic Prediction of\nInteractive Driving Behavior,\u201d in 2019 IEEE Intelligent Vehicles Sym-\nposium, 2019.\n[37] Y. Hu, L. Sun, and M. Tomizuka, \u201cGeneric prediction architecture\nconsidering both rational and irrational driving behaviors,\u201d in 2019 22st\nInternational Conference on Intelligent Transportation Systems (ITSC),\nto appear, 2019.\n[38] L. Sun, C. Peng, W. Zhan, and M. Tomizuka, \u201cA Fast Integrated\nPlanning and Control Framework for Autonomous Driving via Imitation\nLearning,\u201d in ASME 2018 Dynamic Systems and Control Conference.\nAmerican Society of Mechanical Engineers, Sep. 2018, pp. 1\u201311.\n[39] W. Zhan, J. Chen, C. Y. Chan, C. Liu, and M. Tomizuka, \u201cSpatially-\npartitioned environmental representation and planning architecture for\non-road autonomous driving,\u201d in 2017 IEEE Intelligent Vehicles Sympo-\nsium (IV), Jun. 2017, pp. 632\u2013639.\n[40] W. Zhan, C. Liu, C. Y. Chan, and M. Tomizuka, \u201cA non-conservatively\ndefensive strategy for urban autonomous driving,\u201d in 2016 IEEE 19th\nInternational Conference on Intelligent Transportation Systems (ITSC),\npp. 459\u2013464.\n[41] T. Gu, J. Atwood, C. Dong, J. M. Dolan, and J.-W. Lee, \u201cTunable\nand stable real-time trajectory planning for urban autonomous driving,\u201d\nin 2015 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS).\nIEEE, 2015, pp. 250\u2013256.\n[42] J. Schulz, C. Hubmann, J. Lchner, and D. Burschka, \u201cInteraction-Aware\nProbabilistic Behavior Prediction in Urban Environments,\u201d in 2018\nIEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), Oct. 2018, pp. 3999\u20134006.\n[43] D. Pelleg, A. W. Moore et al., \u201cX-means: Extending k-means with\nef\ufb01cient estimation of the number of clusters.\u201d in ICML, vol. 1, 2000,\npp. 727\u2013734.\n[44] T. Shu, Y. Peng, L. Fan, H. Lu, and S.-C. Zhu, \u201cPerception of\nhuman interaction based on motion trajectories: From aerial videos to\ndecontextualized animations,\u201d Topics in cognitive science, vol. 10, no. 1,\npp. 225\u2013241, 2018.\n[45] L. Sun, W. Zhan, Y. Hu, and M. Tomizuka, \u201cInterpretable modelling\nof driving behaviors in interactive driving scenarios based on cumula-\ntive prospect theory,\u201d in 2019 IEEE Intelligent Transportation Systems\nConference (ITSC), 2019.\n",
    "2403.06845": "DriveDreamer-2: LLM-Enhanced World Models\nfor Diverse Driving Video Generation\nGuosheng Zhao1\u2217, Xiaofeng Wang1\u2217, Zheng Zhu2\u2217B, Xinze Chen2,\nGuan Huang2, Xiaoyi Bao1, and Xingang Wang1B\n1 Institute of Automation, Chinese Academy of Sciences\n2 GigaAI\nProject Page: https://drivedreamer2.github.io\nOn a rainy day, \nthere is a car \ncutting in.\nDriveDreamer-2\nBack\nBack Left\nFrame 1\nFrame N\nFront Left\nFront\nFront Right\nBack Right\nFrame n+1\nFrame n\n(a) User-customized driving video generation.\nrelative +8.3%\nrelative +4.4%\n(b) Generated video quality comparison and improvement in the downstream task.\nFig. 1: DriveDreamer-2 demonstrates powerful capabilities in generating multi-view\ndriving videos. DriveDreamer-2 can produce driving videos based on user descriptions,\nwhich improves the diversity of the synthetic data. Besides, the generation quality\nof DriveDreamer-2 surpasses other state-of-the-art methods and effectively enhances\ndownstream tasks.\nAbstract. World models have demonstrated superiority in autonomous\ndriving, particularly in the generation of multi-view driving videos. How-\never, significant challenges still exist in generating customized driving\narXiv:2403.06845v2  [cs.CV]  11 Apr 2024\n2\nG. Zhao, X. Wang et al.\nvideos. In this paper, we propose DriveDreamer-2, which builds upon\nthe framework of DriveDreamer and incorporates a Large Language\nModel (LLM) to generate user-defined driving videos. Specifically, an\nLLM interface is initially incorporated to convert a user\u2019s query into\nagent trajectories. Subsequently, a HDMap, adhering to traffic regula-\ntions, is generated based on the trajectories. Ultimately, we propose the\nUnified Multi-View Model to enhance temporal and spatial coherence in\nthe generated driving videos. DriveDreamer-2 is the first world model to\ngenerate customized driving videos, it can generate uncommon driving\nvideos (e.g., vehicles abruptly cut in) in a user-friendly manner. Besides,\nexperimental results demonstrate that the generated videos enhance the\ntraining of driving perception methods (e.g., 3D detection and track-\ning). Furthermore, video generation quality of DriveDreamer-2 surpasses\nother state-of-the-art methods, showcasing FID and FVD scores of 11.2\nand 55.7, representing relative improvements of \u223c30% and \u223c50%.\nKeywords: World models \u00b7 Autonomous driving \u00b7 Video generation\n1\nIntroduction\nWorld models for autonomous driving [23,25,59,62] have drawn extensive atten-\ntion from both the industry and academia in recent years. Benefiting from their\nexcellent predictive capabilities, autonomous driving world models facilitate the\ngeneration of diverse driving videos, encompassing even long-tail scenarios. The\ngenerated driving videos can be utilized to enhance the training of various driv-\ning perception approaches, proving highly beneficial for practical applications in\nautonomous driving.\nWorld modeling in autonomous driving presents a formidable challenge due\nto its inherent complexity and large sampling space. Early approaches [8, 22]\nmitigate these problems by incorporating world modeling within the Bird\u2019s Eye\nView (BEV) semantic segmentation space. However, these methods primarily ex-\nplore world models in simulated autonomous driving environments. In the recent\nevolution of autonomous driving technologies, there has been a substantial leap\nforward in the development of world models. This progress has been propelled by\nthe utilization of cutting-edge diffusion models [5,17, 18, 39,40,46], exemplified\nby notable contributions such as DriveDreamer [59], Drive-WM [62], Magic-\nDrive [7], Panacea [64], and the integration of large language models like GAIA-\n1 [23], ADriver-I [25]. These sophisticated models have played a pivotal role in\npushing the boundaries of world modeling capabilities, enabling researchers and\nengineers to delve into increasingly intricate and realistic driving scenarios. How-\never, it is important to note that a majority of these methods rely heavily on\nstructured information (e.g., 3D boxes, HDMaps, and optical flow) or real-world\nimage frames as conditions. This dependence not only constrains interactivity\nbut also limits the diversity of generated videos.\nTo tackle the aforementioned challenges, we propose DriveDreamer-2, which\nis the first world model to generate diverse driving videos in a user-friendly man-\nner. In contrast to previous methods [7,59,62] that rely on structured conditions\nDriveDreamer-2\n3\neither from specific datasets or sophisticated annotations, DriveDreamer-2 em-\nphasizes generating customized driving videos by simulating various traffic con-\nditions with user-friendly text prompts. Specifically, the traffic simulation task\nhas been disentangled into the generation of foreground conditions (trajectories\nof the ego-car and other agents) and background conditions (HDMaps of lane\nboundary, lane divider, and pedestrian crossing). For foreground generation, a\nfunctional library is constructed to finetune a Large Language Model (LLM), en-\nabling it to generate agent trajectories based on user text input. For background\nconditions, we propose the HDMap generator that employs a diffusion model to\nsimulate road structures. In this process, the previously generated agent trajec-\ntories are involved as conditional inputs, which allows the HDMap generator to\nlearn the associations between foreground and background conditions in driving\nscenes. Building upon the generated traffic structured conditions, we employ the\nDriveDreamer [59] framework to generate multi-view driving videos. It is noted\nthat we introduce the Unified Multi-view Video Model (UniMVM) within the\nDriveDreamer framework, which is designed to unify both intra-view and cross-\nview spatial consistency, enhancing the overall temporal and spatial coherence\nin the generated driving videos.\nExtensive experiment results show that DriveDreamer-2 is capable of produc-\ning diverse user-customized videos, including uncommon scenarios where vehicles\nabruptly cut in (depicted in Fig. 1). Besides, DriveDreamer-2 can generate high-\nquality driving videos with an FID of 11.2 and FVD of 55.7, relatively improving\nprevious best-performing methods by \u223c30% and \u223c50%. Furthermore, experi-\nments are conducted to verify that driving videos generated by DriveDreamer-2\ncan enhance the training of various autonomous driving perception methods,\nwhere the performance of detection and tracking are relatively improved by\n\u223c4% and \u223c8%.\nThe main contributions of this paper can be summarized as follows:\n\u2013 We present DriveDreamer-2, which is the first world model to generate di-\nverse driving videos in a user-friendly manner.\n\u2013 We propose a traffic simulation pipeline employing only text prompts as\ninput, which can be utilized to generate diverse traffic conditions for driving\nvideo generation.\n\u2013 UniMVM is presented to seamlessly integrate intra-view and cross-view spa-\ntial consistency, elevating the overall temporal and spatial coherence within\nthe generated driving videos.\n\u2013 Extensive experiments are conducted to show that DriveDreamer-2 can craft\ndiverse customized driving videos. Besides, DriveDreamer-2 enhances the\nFID and FVD by \u223c30% and \u223c50% compared to previous best-performing\nmethods. Moreover, the driving videos generated by DriveDreamer-2 en-\nhance the training of various driving perception methods.\n4\nG. Zhao, X. Wang et al.\n2\nRelated Works\n2.1\nWorld Models\nThe primary objective of world methods is to establish dynamic environmen-\ntal models, endowing agents with predictive capabilities for the future. In the\nearly exploration, Variational Autoencoders (VAE) [31] and Long Short-Term\nMemory (LSTM) [19] are employed to capture transition dynamics and ren-\ndering functionality, showcasing remarkable success across diverse applications\n[9\u201313,13,29,36,48,65]. Constructing driving world models poses distinctive chal-\nlenges, primarily arising from the high sample complexity inherent in real-world\ndriving tasks [3]. To address these challenges, ISO-Dream [42] introduces an\nexplicit disentanglement of visual dynamics into controllable and uncontrollable\nstates. MILE [22] strategically incorporates world modeling within the Bird\u2019s Eye\nView (BEV) semantic segmentation space. Recently, DriveDreamer [59], GAIA-\n1 [23], ADriver-I [25], and Drive-WM [62] have explored the training of driving\nworld models in the real world, leveraging powerful diffusion models or natural\nlanguage models. However, most of these methods heavily depend on structured\ninformation (e.g., 3D boxes, HDMaps, and optical flow) as conditions. This de-\npendency not only constrains interactivity but also limits generation diversity.\n2.2\nVideo Generation\nVideo generation and prediction are pivotal techniques for understanding the vi-\nsual world. In the early stages of video generation, methods like Variational\nAutoencoders (VAEs) [4, 21], flow-based model [33], and Generative Adver-\nsarial Networks (GANs) [38, 47, 53, 56] are explored. Language models [20, 26,\n32, 45, 51, 55, 60, 63] are also employed for intricate visual dynamics model-\ning. Recent advancements have seen diffusion models [5, 17, 18, 39, 40, 46] ex-\ntending their influence to video generation. Notably, video diffusion models\n[1, 14, 16, 28, 49, 58, 67] exhibit superior capabilities in generating high-quality\nvideos with realistic frames and smooth transitions, offering enhanced control-\nlability. These models adapt seamlessly to various input conditions, including\ntext, canny, sketch, semantic maps, and depth maps. In the realm of autonomous\ndriving, DriveDreamer-2 leverages powerful diffusion models for learning visual\ndynamics.\n2.3\nTraffic Simulation\nDriving simulators stand as a cornerstone in self-driving development, aiming\nto offer a controlled environment to mimic real-world conditions. LCTGen [52]\nutilizes an LLM to encode detailed language descriptions to a vector and sub-\nsequently employs a generator to produce corresponding simulated scenarios.\nThis method requires highly detailed language descriptions, including informa-\ntion such as the speed and orientation of agents. TrafficGen [6] comprehends\nthe inherent relationships within traffic scenarios, enabling the generation of\nDriveDreamer-2\n5\n\u201cThere is a car cutting \nin on a rainy day.\u201d\nfunction library\nLLM\nPython script\nBEV trajectory \n\u201cA BEV HDMap for \nautonomous driving.\u201d\n\u201c r a i n y ,  a  r e a l i s t i c \nautonomous driving \ns c e n e ,  p a n o r a m i c \nvideos from different \nperspectives.\u201d\ncondition embedding\nunified multi-view \nstructural information\nEncoder\nCLIP\nHDMap \nGenerator\ncamera\n \nCLIP embedding\nGenerated Video\nBEV HDMap\nCustomized Traffic Simulation\nUniMVM\n \nUnified Multi-View\nVideo Generator\nFig. 2: The overall framework of DriveDreamer-2 involves initially generating agent\ntrajectories according to the user query, followed by producing a realistic HDMap, and\nfinally generating multi-view driving videos.\ndiverse and legitimate traffic flows within the same map. CTG [70] generates\ntraffic simulations by employing manually designed loss functions that adhere to\ntraffic constraints. CTG++ [69] further extends CTG by utilizing GPT-4 [41]\nto convert user language descriptions into a loss function, which guides the\nscene-level conditional diffusion model to generate the corresponding scenario.\nIn DriveDreamer-2, we construct a functional library to finetune the LLM to\nachieve a user-friendly text-to-traffic simulation, which eliminates intricate loss\ndesign or complex text prompt inputs.\n3\nDriveDreamer-2\nFig. 2 illustrates the overall framework of DriveDreamer-2. A customized traf-\nfic simulation is first proposed to generate foreground agent trajectories and\nbackground HDMaps. Specifically, DriveDreamer-2 utilizes a finetuned LLM to\ntranslate user prompts into agent trajectories, and the HDMap generator is then\nintroduced to simulate road structures using the generated trajectories as con-\nditions. Leveraging the customized traffic simulation pipeline, DriveDreamer-2\nis capable of generating diverse structured conditions for the subsequent video\ngeneration. Building upon the architecture of DriveDreamer [59], the UniMVM\nframework is proposed to unify both intra-view and cross-view spatial consis-\ntency, thereby enhancing the overall temporal and spatial coherence in the gen-\nerated driving videos. In the subsequent sections, we delve into the details of the\ncustomized traffic simulation and the UniMVM framework.\n3.1\nCustomized Traffic Simulation\nIn the proposed customized traffic simulation pipeline, a trajectory-generation\nfunction library is constructed to finetune the LLM, which facilitates transferring\nuser prompts into diverse agent trajectories, encompassing maneuvers such as\ncut-ins and U-turns. Additionally, the pipeline incorporates the HDMap genera-\ntor to simulate the background road structures. During this phase, the previously\n6\nG. Zhao, X. Wang et al.\nPython script.\"\nFunction Library\nagent\n#Generate a trajectory of cutting in.\ndef cut_in(obj_trajs,obj_vels,safe_dis,is_ego) \n \n#Generate a forward movement trajectory.\ndef forward(obj_trajs,obj_vels,safe_dis,is_ego) \n...\nutils\n#Set a random seed to generate  trajectories.\ndef set_random_seed(seed):\n#Save the trajectoies of agents.\ndef save_trajectories(ego_trajs, obj_trajs)\n...\npedestrian\n#Generate a trajectory of the pedestrian crossing \nroad.\ndef pedestrian_crossing()\n...\nInference\na vehicle cuts in.\nUser Query\nPrompt\nTemplate\nLLM\nTrajectory Array\nText-to-Python-Script Pairs\nUser Query:   a vehicle cuts in. \n#import specific libraries\nimport libs\n#set  a random seed\nutils.set_random_seed(seed = 3577)\n#generate a trajectory of cutting in\nobj_trajs, obj_vels = agent.cut_in(is_ego=False)\n#generate the corresponding ego car trajectory\nego_trajs = agent.forward(obj_trajs=obj_trajs, \nobj_vels=obj_vels, safe_dis=8, is_ego=True)\n#save the generated trajectories\nutils.save_trajectories(ego_trajs=ego_trajs, \nobj_trajs=obj_trajs)\nPython Script \nFig. 3: The overview of customized trajectory generation. Initially, we leverage the\nestablished function library to assemble Text-to-Python-Script pairs. Subsequently, the\nconstructed dataset is employed to finetune LLM. Finally, the customized trajectories\nis generated by LLM based on user query.\ngenerated agent trajectories serve as conditional inputs, ensuring that the result-\ning HDMap adheres to traffic constraints. In the following, we elaborate on the\nfinetuning process of the LLM and the framework of the HDMap generator.\nFinetuning LLM for Trajectory Generation Previous traffic simulation\nmethods [37,69,70] necessitate the intricate specification of parameters, involv-\ning details such as the agent\u2019s speed, position, acceleration, and mission goal.\nTo simplify this intricate process, we propose to finetune LLM with the con-\nstructed trajectory-generation function library, allowing for the efficient trans-\nformation of user-friendly language inputs into comprehensive traffic simulation\nscenarios. As depicted in Fig. 3, the constructed function library encompasses 18\nfunctions, including agent functions (steering, constant speed, acceleration, and\nbraking), pedestrian functions (walking direction and speed), and other utility\nfunctions such as saving trajectories. Building upon these functions, Text-to-\nPython-Scripts pairs are manually curated for finetuning LLM (GPT-3.5). The\nscripts include a range of fundamental scenarios such as lane-changing, overtak-\ning, following other vehicles, and executing U-turns. Additionally, we encompass\nmore uncommon scenarios like pedestrians abruptly crossing, and vehicles cut-\nting into the lane. Taking the user input a vehicle cuts in as an example, the\ncorresponding script involves the following steps: initially generating a trajectory\nof cutting in (agent.cut_in()), followed by generating the corresponding ego\ncar trajectory (agent.forward()), and ultimately utilizing the saving function\nfrom utilities to directly output the trajectory of the ego-car and other agents in\narray format. For additional details, please refer to the supplementary materials.\nIn the inference phase, we follow [37] to expand prompt inputs to a pre-defined\ntemplate, and the finetuned LLM can directly output the trajectory array.\nHDMap Generation A comprehensive traffic simulation not only entails the\ntrajectories of foreground agents but also necessitates the generation of back-\nDriveDreamer-2\n7\nBEV trajectory\nBEV HDMap 1\nBEV HDMap 2\nBEV HDMap 3\nFig. 4: The proposed HDMap generator can generate diverse BEV HDMaps based on\nthe same BEV trajectory input. The orange and yellow colors represent the motion\ntrajectories of the ego car and other vehicles, respectively. The red color indicates\nroad boundaries, the blue color represents lane dividers, and the green color signifies\npedestrian crossings.\nground HDMap elements such as lanes and pedestrian crosswalks. Therefore,\nthe HDMap generator is proposed to ensure the background elements do not\nconflict with the foreground trajectories. In the HDMap generator, we formulate\nthe background elements generation as a conditional image generation problem,\nwhere the conditional input is the BEV trajectory map Tb \u2208R3\u00d7Hb\u00d7Wb, and\nthe target is the BEV HDMap Hb \u2208R3\u00d7Hb\u00d7Wb. Different from previous condi-\ntional image generation approaches [35, 68] that predominantly rely on outline\nconditions (edges, depths, boxes, segmentation maps), the proposed HDMap\ngenerator explores the correlations between the foreground and background traf-\nfic elements. Specifically, the HDMap generator is constructed upon an image-\ngeneration diffusion model. To train the generator, we curate a trajectory-to-\nHDMap dataset D = {Tb, Hb}. In the trajectory map, distinct colors are assigned\nto represent different agent categories. Meanwhile, the target HDMap comprises\nthree channels, representing lane boundaries, lane dividers, and pedestrian cross-\nings, respectively. Within the HDMap generator, we employ stacks of 2D convo-\nlution layers to incorporate the trajectory map condition. The resulting feature\nmaps CT are then seamlessly integrated into the diffusion model using [68] (see\nsupplement for additional architectural details). In the training stage, the diffu-\nsion forward process gradually adds noise \u03f5 to the latent feature Z0, resulting\nin the noisy latent feature ZTb. Then we train \u03f5\u03b8 to predict the noise we added,\nand the HDMap generator \u03d5 is optimized via\n  \\\nm i n  _{\\phi  } {\\cal \nL\n} =  {\\mat hb b E}\n_\n{\n{\\cal Z}_0,\\epsilon \\sim {\\cal N}({\\textbf {0,I}}),t,c} \\left [\\Vert \\epsilon -\\epsilon _\\theta ({\\cal Z}_t,t,c)\\Vert _2^2\\right ], \n(1)\nwhere time step t is uniformly sampled from [1, Tb]. As shown in Fig. 4, leverag-\ning the proposed HDMap generator allows us to generate diverse HDMaps based\non the same trajectory conditions. It is noteworthy that the generated HDMaps\nnot only adhere to traffic constraints (lane boundaries positioned on either side\nof lane dividers, and pedestrian crossings at intersections) but also seamlessly\nintegrate with trajectories.\n8\nG. Zhao, X. Wang et al.\ncross-view \nmodule \ncross-frame \nmodule \ncross-view \nmodule \ncross-frame \nmodule \ncross-frame \nmodule \ncross-frame \nmodule \ncross-frame \nmodule \nDriveDreamer\nDrive-WM\nDriveDreamer-2\nw/o image condition\nDriveDreamer-2\nwith initial frame\nDriveDreamer-2\nwith front video\nConcated Image Input\nConcated Image Input\nConcated Image Input\nSeparate Image Input\nSeparate Image Input\nFig. 5: The comparison of multi-view video generation paradigms. All structural con-\nditions and text prompts are omitted here to emphasize the distinctions between our\nUniMVM and previous methods. By adjusting the mask, UniMVM can generate videos\nconditioned on the initial frame, front view video, and without image input.\n3.2\nUniMVM\nUtilizing structured information generated by the customized traffic simulation,\nmulti-view driving videos can be generated via the framework of DriveDreamer\n[59]. However, the view-wise attention introduced in previous methods [59, 66]\ncan not guarantee multi-view consistency. To mitigate this problem, [34,62,64]\nemploy image or video conditions to generate multi-view driving videos. While\nthis approach enhances consistency between different views, it comes at the\nexpense of reduced generation efficiency and diversity. In DriveDreamer-2, we\nintroduce the UniMVM within the DriveDreamer framework. The UniMVM is\ndesigned to unify the generation of multi-view driving videos both with and\nwithout adjacent view conditions, which ensures temporal and spatial coherence\nwithout compromising generation speed and diversity.\nFormulation In multi-view video dataset pdata, x \u2208RK\u00d7T \u00d73\u00d7H\u00d7W is a se-\nquence of T images with K views, with height H and width W. Let xi denote\nthe sample of i-th view, then the multi-view video joint distribution p(x1, ..., xK)\ncan be obtained by [62]:\n  \\la bel {eq 2 } p({\\rm x_1, ... , \\r m x _K}) =  p({\\ rm x}_1)p({\\rm x}_{2}|{\\rm x}_1)... p\\left (\\left . {\\rm x}_{K} \\right | {\\rm x}_{1},{\\rm x}_2...,{\\rm x}_{K-1}\\right ). \n(2)\nEq. 2 indicates that adjacent view videos can be expanded with multiple gener-\nation steps, which is inefficient. In the proposed UniMVM, we draw inspiration\nfrom the Eq. 2 to expand the view. However, unlike Drive-WM [62] which requires\nthe independent generation of views, UniMVM unifies multiple views as a com-\nplete patch. Specifically, we concatenate the multi-view video in the order of {FL,\nF, FR, BR, B, BL}1 to obtain the spatially unified image x\u2032 \u2208RT \u00d73\u00d7H\u00d7KW .\nThen we can obtain the multi-view driving video distribution p(x\u2032):\n  p({ \\ rm x } ') = p({ \\r m  x } '\\cd o t (1-m ) ,{ \\ rm x} ' \\cdot m)=p({\\rm x}'\\cdot m)p({\\rm x}'\\cdot (1-m)|{\\rm x}'\\cdot m), \n(3)\n1 F:Front, L: Left, R: Right, B: Back.\nDriveDreamer-2\n9\nwhere m represents the mask of one of the all views. As shown in Fig. 5, we\ncompare the paradigm of UniMVM with that of DriveDreamer [59] and Drive-\nWM [62]. In contrast to these counterparts, UniMVM unifies multiple views into\na complete patch for video generation without introducing cross-view parame-\nters. Furthermore, various driving video generation tasks can be accomplished\nvia adjusting the mask m. Specifically, when m is set to mask future T \u22121\nframes, UniMVM enables future video prediction based on the input of the first\nframe. Configuring m to mask {FL, FR, BR, B, BL} views empowers UniMVM\nto achieve multi-view video outpainting, leveraging a front-view video input.\nFurthermore, UniMVM can generate multi-view videos when m is set to mask\nall video frames, and both quantitative and qualitative experiments verify that\nUniMVM is capable of generating temporally and spatially coherent videos with\nenhanced efficiency and diversity.\nVideo Generation Based on the UniMVM formulation, driving videos can be\ngenerated within the framework of DriveDreamer [59]. Specifically, our approach\nfirst unify the traffic structured conditions, which results in sequences of HDMaps\n{Hi}N\u22121\ni=0\n\u2208RN\u00d73\u00d7H\u00d7KW and 3D boxes {Bi}N\u22121\ni=0\n\u2208RN\u00d7C\u00d7H\u00d7KW (N is the\nframe number of video clip, and C is the category number). Note that sequences\nof 3D boxes can be derived from agent trajectories, and the sizes of 3D boxes are\ndetermined based on the respective agent category. Unlike DriveDreamer, the\n3D box conditions in DriveDreamer-2 no longer rely on position embedding and\ncategory embedding. Instead, the boxes are directly projected onto the image\nplane, functioning as a control condition. This approach eliminates introducing\nadditional control parameters as in [59]. We adopt three encoders to embed\nHDMaps, 3D boxes, and image frames into latent space features yH, yB, and yI.\nThen we concatenate the spatially aligned conditions yH, yB with Zt to obtain\nthe feature input Zin, where Zt is the noisy latent feature generated from yI\nby the forward diffusion process. For the training of the video generator, all\nparameters are optimized via denoising score matching [27] (see supplement for\ndetails).\n4\nExperiment\n4.1\nExperiment Details\nDataset. The training dataset is derived from the nuScenes dataset [2], consist-\ning of 700 training videos and 150 validation videos. Each video encompasses\napproximately 20 seconds of recorded footage, captured by six surround-view\ncameras. With a frame rate of 12Hz, this accumulates to around 1 million\nvideo frames available for training. Following [59,61], we preprocess the nuScenes\ndataset to calculate 12Hz annotations. Specifically, HDMaps and 3D boxes are\ntransformed to BEV perspective to train HDMap generation, and these annota-\ntions are projected to pixel coordinate to train video generation.\nTraining. For agent trajectory generation, we employ GPT-3.5 as the LLM.\nSubsequently, we utilize the constructed text-to-script dataset to finetune GPT-\n3.5 into an LLM with specialized trajectory generation knowledge. The proposed\n10\nG. Zhao, X. Wang et al.\nFrame\nLane changing\nPedestrian crossing road\nFig. 6: User-customized driving videos generated by DriveDreamer-2. The top row\ndepicts a scene where the ego car changes lanes, while the bottom row shows an unex-\npected pedestrian crossing the road at night.\nHDMap generator is built upon SD2.1 [46] with the ControlNet parameters [68]\nbeing trainable. The HDMap generator undergoes 55K training iterations with a\nbatch size of 24 and a resolution of 512\u00d7512. For the video generator, we harness\nthe powerful video generation capabilities of SVD [1], and all the parameters are\nfinetuned. During the training of the video generator, the mode is trained for\n200K iterations with a batch size of 1, a video frame length of N = 8, a view\nnumber of K = 6, and a spatial size of 256 \u00d7 448. All the experiments are\nconducted on NVIDIA A800 (80GB) GPUs, and we use the AdamW optimizer\n[30] with a learning rate 5 \u00d7 10\u22125.\nEvaluation. Extensive qualitative and quantitative experiments are conducted\nto assess DriveDreamer-2. For qualitative experiments, we visualize customized\ndriving video generation to validate that DriveDreamer-2 can produce diverse\ndriving videos in a user-friendly manner. Additionally, visualization comparisons\nare conducted between UniMVM and other generative paradigms to demon-\nstrate that DriveDreamer-2 excels in generating temporally and spatially co-\nherent videos. For quantitative experiments, the frame-wise Fr\u00e9chet Inception\nDistance (FID) [43] and Fr\u00e9chet Video Distance (FVD) [54] are utilized as met-\nrics. Besides, StreamPETR [57], building upon a ResNet-50 [15] backbone, is\ntrained at the same resolution of 256 \u00d7 448 to evaluate the improvements of 3D\nobject detection and multi-object tracking achieved by our generated results.\nMore details can be found in the supplementary material.\n4.2\nUser-Customized Driving Video Generation\nDriveDreamer-2 offers a user-friendly interface for generating driving videos.\nAs depicted in Fig. 1a, users are only required to input a text prompt (e.g.,\non a rainy day, there is a car cut in). Then DriveDreamer-2 produces multi-\nview driving videos aligned with the text input. Fig. 6 illustrates another two\ncustomized driving videos. The upper one depicts the process of the ego car\nchanging lanes to the left during the daytime. The lower one showcases an un-\nexpected pedestrian crossing the road at night, prompting the ego car to brake\nDriveDreamer-2\n11\nto avoid the collision. Notably, the generated videos demonstrate an exceptional\nlevel of realism, where we can even observe the reflection of high beams on the\npedestrian.\nTable 1: Comparison of the generation quality on nuScenes validation set. \u2020 denotes\nthat the corresponding conditions are generated.\nMethod\nConditions\nFID\u2193\nFVD\u2193\nDriveDreamer [59]\n-\n26.8\n353.2\nDriveDreamer-2\n-\n25.0\n105.1\nDrive-WM [62]\n3-view videos\u2020\n15.8\n122.7\nDriveDreamer-2\n1-view video\n18.4\n74.9\nDriveDreamer [59]\n1st-frame multi-view image\n14.9\n340.8\nDrivingdiffusion [34]\n1st-frame multi-view image\u2020\n15.8\n332.0\nPanacea [64]\n1st-frame multi-view image\u2020\n16.9\n139.0\nDriveDreamer-2\n1st-frame multi-view image\n11.2\n55.7\n4.3\nQuality Evaluation of Generated Videos\nTo verify the video generation quality, we compare DriveDreamer-2 with vari-\nous driving video generation approaches on the nuScenes validation set. For a\nfair comparison, we conducted evaluations under three different experimental\nsettings\u2014without image condition, with video condition, and with first frame\nmulti-view image condition. Additionally, Drive-WM [62], DrivingDiffusion [34]\nand Panacea [64] adopt a two-stage pipeline, first generating visual conditions\nand then generating videos. The experimental results, as shown in Tab. 1, in-\ndicate that DriveDreamer-2 consistently achieves high-quality evaluation out-\ncomes across all three settings. Specifically, in the absence of the image condi-\ntion, DriveDreamer-2 attains an FID of 25.0 and an FVD of 105.1, showcasing\na significant improvement over DriveDreamer [59]. Moreover, despite being lim-\nited to a single-view video condition, DriveDreamer-2 exhibits a 39% relative\nimprovement in FVD compared to DriveWM [62], which utilizes a three-view\nvideo condition. Furthermore, when provided with the first-frame multi-view im-\nage condition, DriveDreamer-2 achieves an FID of 11.2 and an FVD of 55.7,\nsurpassing all previous methods by a considerable margin.\nTable 2: Comparison involving data augmentation using synthetic data on 3D object\ndetection.\nImage size\nInitial frame\nReal\nGenerated\nmAP\u2191\nmAOE\u2193\nmAVE\u2193\nNDS\u2191\n256\u00d7448\n-\n\u2713\n-\n31.7\n67.9\n33.0\n43.5\n\u2713\n\u2713\n\u2713\n32.6\n61.7\n29.7\n45.2\n-\n\u2713\n\u2713\n32.9\n61.5\n30.4\n45.4\n12\nG. Zhao, X. Wang et al.\nTable 3: Comparison involving data augmentation using synthetic data on multi-\nobject tracking.\nImage size\nInitial frame\nReal\nGenerated\nAMOTA\u2191\nAMOTP\u2193\nIDS\u2193\n256\u00d7448\n-\n\u2713\n-\n28.9\n1.419\n687\n\u2713\n\u2713\n\u2713\n31.2\n1.396\n542\n-\n\u2713\n\u2713\n31.3\n1.387\n593\nTo further validate the quality of the generated data, we employ the generated\ndriving videos to enhance the training of 3D object detection and multi-object\ntracking. Specifically, we employ all structured conditions in the nuScenes train-\ning set to generate driving videos, which are combined with real videos to train\nStreamPETR [57] on downstream tasks. The experiment results are in Tab. 2 and\nTab. 3. When the initial frame is used as a condition, the generated videos prove\nto be effective in enhancing the performance of downstream tasks. The 3D de-\ntection metrics, mAP and NDS, show a relative improvement of 2.8% and 3.9%,\nrespectively. Besides, the tracking metrics, AMOTA and AMOTP, exhibit a rela-\ntive enhancement of 8.0% and 1.6%. Furthermore, DriveDreamer-2 is capable of\ngenerating high-quality driving videos even in the absence of image conditions.\nRemoving the image condition leads to increased diversity in the generated con-\ntent, consequently further improving performance metrics for downstream tasks.\nSpecifically, the mAP and NDS for 3D detection show a relative improvement of\n3.8% and 4.4%, respectively, while the AMOTA and AMOTP for tracking tasks\nexhibit enhancements of 8.3% and 2.3%, compared to the baseline.\nDriveDreamer-2 w/o UniMVM\nDriveDreamer-2 w/o UniMVM\nDriveDreamer-2 w/o UniMVM\nDriveDreamer-2 w/o UniMVM\nDriveDreamer-2 wtih UniMVM\nDriveDreamer-2 wtih UniMVM\nDriveDreamer-2 wtih UniMVM\nDriveDreamer-2 wtih UniMVM\nFig. 7: Visualization comparison between DriveDreamer-2 generation with and with-\nout UniMVM. The upper part depicts generation without UniMVM, while the lower\npart illustrates generation with UniMVM. It is evident that the inclusion of UniMVM\nresults in higher multi-view consistency in the generated content.\nDriveDreamer-2\n13\nFront Left\nDriveDreamer-2 w/o image codnition\nDriveDreamer-2 with initial frame\nDriveDreamer-2 with front-view video\nFrame 2 ground truth \nFrame 1 ground truth \nBack\nBack Left\nFront\nFront Right\nBack Right\nFig. 8: Visualization comparison with different conditions. Under different image con-\nditions, DriveDreamer-2 produces videos with a high level of multi-view consistency.\nUsing the 1st frame image (row 1) as a condition, the diversity of the generated second\nframe (row 3) is constrained, which closely resembles the ground truth second frame\n(row 2). Employing the front-view video as a condition results in increased generation\ndiversity (row 4), only the front-view image aligns closely with the ground truth (row\n2). Strikingly, in the absence of image as a condition, DriveDreamer-2 produces the\nhighest diversity (row 5). The generated colors of the cars and the street backgrounds\ndiffer significantly from the ground truth (row 2).\n4.4\nAblation Study\nWe conduct an ablation study to investigate the effect of diffusion backbone and\nthe proposed UniMVM, and the results are in Tab. 4. Compared to SD1.4 [46]\nused in DriveDreamer, SVD [1] provides richer prior knowledge of videos, re-\nsulting in 17.2 FID and 94.6 FVD. The introduction of SVD results in an al-\nmost 70% improvement in FVD. Additionally, we also note that a slight de-\ncrease in FID, which we hypothesize is attributed to the introduction of the\ncross-view module, disrupting the SVD\u2019s ability to learn spatial features. To\nfully unleash the potential of SVD in multi-view video generation, we propose\nUniMVM, which unifies constraints on intra- and cross-view, achieving remark-\nable FID and FVD scores of 11.2 and 55.7, respectively. These represent relative\nimprovements of \u223c30% and \u223c80% compared to DriveDreamer. As depicted in\nFig. 7, the upper row in each compared pair is generated by DriveDreamer-2\nwithout UniMVM, while the lower row is generated by DriveDreamer-2 with\nUniMVM. In the absence of UniMVM, DriveDreamer-2 generates inconsistent\nresults between views, including foreground vehicles and background structures.\nThe introduction of UniMVM leads to significant improvements in generating\nmulti-view videos, both in foreground and background aspects. The qualita-\n14\nG. Zhao, X. Wang et al.\nTable 4: The ablation study on the backbone and UniMVM. Cross-view module de-\nnotes the cross-view attention used in previous methods [34,59,62,64].\nMethod\nBackbone\nCross-view module\nUniMVM\nFID\u2193\nFVD\u2193\nDriveDreamer [59]\nSD1.4\n\u2713\n-\n14.9\n340.8\nDriveDreamer-2\nSVD\n\u2713\n-\n17.2\n94.6\n-\n\u2713\n11.2\n55.7\nTable 5: The ablation study on different conditions.\nMethod\nConditions\nFID\u2193\nFVD\u2193\nDriveDreamer [59]\n-\n26.8\n353.2\n1st-frame multi-view image\n14.9\n340.8\nDriveDreamer-2\n-\n25.0\n105.1\n1-view video\n18.4\n74.9\n1-st frame multi-view image\n11.2\n55.7\ntive results demonstrate the impressive capability of our UniMVM in achieving\nmulti-view consistency.\nMoreover, we explore the influence of various conditions on driving video\ngeneration, as shown in Tab. 5 and Fig. 8. The first row in Fig. 8 illustrates\nthe ground truth (GT) of the first frame, representing the style of the GT video.\nMeanwhile, the second row displays the GT of the second frame, representing the\nGT of the generated multi-view frame. DriveDreamer-2 with the initial frame\ncan generate results that are highly similar to the GT video, achieving optimal\nresults in terms of FID and FVD, with scores of 11.2 and 55.7, respectively.\nThe video generated by DriveDreamer-2 with the front-view video retains some\naspects of the GT scene while also introducing some diversity, resulting in 17.2\nFID and 94.6 FVD. DriveDreamer-2 can also generate extremely competitive\nresults even without any image conditioning, achieving FID and FVD scores\nof 25.0 and 105.1, respectively. Notably, DriveDreamer-2 exhibits the highest\ndiversity in this setting, where the generated appearance of cars and the street\nbackgrounds differ significantly from the ground truth.\n5\nDiscussion and Conclusion\nThis paper introduces DriveDreamer-2, an innovative extension of the Drive-\nDreamer framework that pioneers the generation of user-customized driving\nvideos. Leveraging a Large Language Model, DriveDreamer-2 first transfers user\nqueries into foreground agent trajectories. Then the background traffic condi-\ntions can be generated using the proposed HDMap generator, with agent trajec-\ntories as conditions. The generated structured conditions can be utilized for video\ngeneration, and we propose UniMVM to enhance temporal and spatial coherence.\nWe conduct extensive experiments to verify that DriveDreamer-2 can generate\nuncommon driving videos, such as abrupt vehicle maneuvers. Importantly, ex-\nperimental results showcase the utility of the generated videos in enhancing the\ntraining of driving perception methods. Furthermore, DriveDreamer-2 demon-\nstrates superior video generation quality compared to state-of-the-art methods,\nDriveDreamer-2\n15\nachieving FID and FVD scores of 11.2 and 55.7, respectively. These scores rep-\nresent remarkable relative improvements of approximately 30% and 50%, affirm-\ning the efficacy and advancement of DriveDreamer-2 in multi-view driving video\ngeneration.\nReferences\n1. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D.,\nLevi, Y., English, Z., Voleti, V., Letts, A., et al.: Stable video diffusion: Scaling\nlatent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127\n(2023) 4, 10, 13\n2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,\nPan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous\ndriving. CVPR (2019) 9, 23\n3. Chen, L., Wu, P., Chitta, K., Jaeger, B., Geiger, A., Li, H.: End-to-end autonomous\ndriving: Challenges and frontiers. arXiv preprint arXiv:2306.16927 (2023) 4\n4. Denton, E., Fergus, R.: Stochastic video generation with a learned prior. In: ICML\n(2018) 4\n5. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. NeurIPS\n(2021) 2, 4, 21\n6. Feng, L., Li, Q., Peng, Z., Tan, S., Zhou, B.: Trafficgen: Learning to generate\ndiverse and realistic traffic scenarios. In: ICRA (2023) 4\n7. Gao, R., Chen, K., Xie, E., Hong, L., Li, Z., Yeung, D.Y., Xu, Q.: Magicdrive: Street\nview generation with diverse 3d geometry control. arXiv preprint arXiv:2310.02601\n(2023) 2\n8. Gao, Z., Mu, Y., Shen, R., Chen, C., Ren, Y., Chen, J., Li, S.E., Luo, P., Lu, Y.:\nEnhance sample efficiency and robustness of end-to-end urban autonomous driving\nvia semantic masked world model. arXiv preprint arXiv:2210.04017 (2022) 2\n9. Ha, D., Schmidhuber, J.: Recurrent world models facilitate policy evolution.\nNeurIPS (2018) 4\n10. Hafner, D., Lee, K.H., Fischer, I., Abbeel, P.: Deep hierarchical planning from\npixels. NeurIPS (2022) 4\n11. Hafner, D., Lillicrap, T., Ba, J., Norouzi, M.: Dream to control: Learning behaviors\nby latent imagination. arXiv preprint arXiv:1912.01603 (2019) 4\n12. Hafner, D., Lillicrap, T., Norouzi, M., Ba, J.: Mastering atari with discrete world\nmodels. arXiv preprint arXiv:2010.02193 (2020) 4\n13. Hafner, D., Pasukonis, J., Ba, J., Lillicrap, T.: Mastering diverse domains through\nworld models. arXiv preprint arXiv:2301.04104 (2023) 4\n14. Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., Wood, F.: Flexible diffu-\nsion modeling of long videos. NeurIPS (2022) 4\n15. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition.\nIn: CVPR (2016) 10\n16. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D.P.,\nPoole, B., Norouzi, M., Fleet, D.J., et al.: Imagen video: High definition video\ngeneration with diffusion models. arXiv preprint arXiv:2210.02303 (2022) 4\n17. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS\n(2020) 2, 4, 21\n18. Ho, J., Saharia, C., Chan, W., Fleet, D.J., Norouzi, M., Salimans, T.: Cascaded\ndiffusion models for high fidelity image generation. JMLR (2022) 2, 4, 21\n16\nG. Zhao, X. Wang et al.\n19. Hochreiter, S., Schmidhuber, J.: Long short-term memory. Neural computation\n(1997) 4\n20. Hong, W., Ding, M., Zheng, W., Liu, X., Tang, J.: Cogvideo: Large-scale pretrain-\ning for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868\n(2022) 4\n21. Hsieh, J.T., Liu, B., Huang, D.A., Fei-Fei, L.F., Niebles, J.C.: Learning to decom-\npose and disentangle representations for video prediction. NeurIPS (2018) 4\n22. Hu, A., Corrado, G., Griffiths, N., Murez, Z., Gurau, C., Yeo, H., Kendall, A.,\nCipolla, R., Shotton, J.: Model-based imitation learning for urban driving. NeurIPS\n(2022) 2, 4\n23. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J.,\nCorrado, G.: Gaia-1: A generative world model for autonomous driving. arXiv\npreprint arXiv:2309.17080 (2023) 2, 4\n24. Hyv\u00e4rinen, A., Dayan, P.: Estimation of non-normalized statistical models by score\nmatching. JMLR (2005) 22\n25. Jia, F., Mao, W., Liu, Y., Zhao, Y., Wen, Y., Zhang, C., Zhang, X., Wang,\nT.: Adriver-i: A general world model for autonomous driving. arXiv preprint\narXiv:2311.13549 (2023) 2, 4\n26. Kalchbrenner, N., Oord, A., Simonyan, K., Danihelka, I., Vinyals, O., Graves, A.,\nKavukcuoglu, K.: Video pixel networks. In: ICML (2017) 4\n27. Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusion-\nbased generative models. NeurIPS (2022) 9, 22\n28. Khachatryan, L., Movsisyan, A., Tadevosyan, V., Henschel, R., Wang, Z.,\nNavasardyan, S., Shi, H.: Text2video-zero: Text-to-image diffusion models are zero-\nshot video generators. arXiv preprint arXiv:2303.13439 (2023) 4\n29. Kim, S.W., Zhou, Y., Philion, J., Torralba, A., Fidler, S.: Learning to simulate\ndynamic environments with gamegan. In: CVPR (2020) 4\n30. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980 (2014) 10\n31. Kingma, D.P., Welling, M.: Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114 (2013) 4\n32. Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Hornung, R., Adam, H.,\nAkbari, H., Alon, Y., Birodkar, V., et al.: Videopoet: A large language model for\nzero-shot video generation. arXiv preprint arXiv:2312.14125 (2023) 4\n33. Kumar, M., Babaeizadeh, M., Erhan, D., Finn, C., Levine, S., Dinh, L.,\nKingma, D.: Videoflow: A flow-based generative model for video. arXiv preprint\narXiv:1903.01434 (2019) 4\n34. Li, X., Zhang, Y., Ye, X.: Drivingdiffusion: Layout-guided multi-view driving scene\nvideo generation with latent diffusion model. arXiv preprint arXiv:2310.07771\n(2023) 8, 11, 14\n35. Li, Y., Liu, H., Wu, Q., Mu, F., Yang, J., Gao, J., Li, C., Lee, Y.J.: Gligen: Open-set\ngrounded text-to-image generation. In: CVPR (2023) 7\n36. Lin, J., Du, Y., Watkins, O., Hafner, D., Abbeel, P., Klein, D., Dragan, A.: Learning\nto model the world with language. arXiv preprint arXiv:2308.01399 (2023) 4\n37. Mao, J., Qian, Y., Zhao, H., Wang, Y.: Gpt-driver: Learning to drive with gpt.\narXiv preprint arXiv:2310.01415 (2023) 6\n38. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond\nmean square error. arXiv preprint arXiv:1511.05440 (2015) 4\n39. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,\nSutskever, I., Chen, M.: Glide: Towards photorealistic image generation and edit-\nDriveDreamer-2\n17\ning with text-guided diffusion models. arXiv preprint arXiv:2112.10741 (2021) 2,\n4\n40. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In:\nICML (2021) 2, 4\n41. OpenAI, R.: Gpt-4 technical report. arxiv 2303.08774. View in Article (2023) 5\n42. Pan, M., Zhu, X., Wang, Y., Yang, X.: Iso-dream: Isolating and leveraging non-\ncontrollable visual dynamics in world models. NeurIPS (2022) 4\n43. Parmar, G., Zhang, R., Zhu, J.Y.: On aliased resizing and surprising subtleties in\ngan evaluation. In: CVPR (2022) 10\n44. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using\n2d diffusion. arXiv preprint arXiv:2209.14988 (2022) 21\n45. Ranzato, M., Szlam, A., Bruna, J., Mathieu, M., Collobert, R., Chopra, S.: Video\n(language) modeling: a baseline for generative models of natural videos. arXiv\npreprint arXiv:1412.6604 (2014) 4\n46. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022) 2, 4, 10, 13\n47. Saito, M., Matsumoto, E., Saito, S.: Temporal generative adversarial nets with\nsingular value clipping. In: ICCV (2017) 4\n48. Seo, Y., Hafner, D., Liu, H., Liu, F., James, S., Lee, K., Abbeel, P.: Masked world\nmodels for visual control. In: CoRL (2023) 4\n49. Singer, U., Polyak, A., Hayes, T., Yin, X., An, J., Zhang, S., Hu, Q., Yang, H.,\nAshual, O., Gafni, O., et al.: Make-a-video: Text-to-video generation without text-\nvideo data. arXiv preprint arXiv:2209.14792 (2022) 4\n50. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-\nbased generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 (2020) 21\n51. Srivastava, N., Mansimov, E., Salakhudinov, R.: Unsupervised learning of video\nrepresentations using lstms. In: ICML (2015) 4\n52. Tan, S., Ivanovic, B., Weng, X., Pavone, M., Kraehenbuehl, P.: Language condi-\ntioned traffic generation. arXiv preprint arXiv:2307.07947 (2023) 4\n53. Tulyakov, S., Liu, M.Y., Yang, X., Kautz, J.: Mocogan: Decomposing motion and\ncontent for video generation. In: CVPR (2018) 4\n54. Unterthiner, T., Van Steenkiste, S., Kurach, K., Marinier, R., Michalski, M., Gelly,\nS.: Towards accurate generative models of video: A new metric & challenges. arXiv\npreprint arXiv:1812.01717 (2018) 10\n55. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,\nM.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length video generation\nfrom open domain textual description. ICLR (2023) 4\n56. Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene dynamics.\nNeurIPS (2016) 4\n57. Wang, S., Liu, Y., Wang, T., Li, Y., Zhang, X.: Exploring object-centric tem-\nporal modeling for efficient multi-view 3d object detection. arXiv preprint\narXiv:2303.11926 (2023) 10, 12, 22\n58. Wang, X., Yuan, H., Zhang, S., Chen, D., Wang, J., Zhang, Y., Shen, Y., Zhao,\nD., Zhou, J.: Videocomposer: Compositional video synthesis with motion control-\nlability. arXiv preprint arXiv:2306.02018 (2023) 4\n59. Wang, X., Zhu, Z., Huang, G., Chen, X., Lu, J.: Drivedreamer: Towards real-world-\ndriven world models for autonomous driving. arXiv preprint arXiv:2309.09777\n(2023) 2, 3, 4, 5, 8, 9, 11, 14\n18\nG. Zhao, X. Wang et al.\n60. Wang, X., Zhu, Z., Huang, G., Wang, B., Chen, X., Lu, J.: Worlddreamer: Towards\ngeneral world models for video generation via predicting masked tokens. arXiv\npreprint arXiv:2401.09985 (2024) 4\n61. Wang, X., Zhu, Z., Zhang, Y., Huang, G., Ye, Y., Xu, W., Chen, Z., Wang, X.: Are\nwe ready for vision-centric driving streaming perception? the asap benchmark. In:\nCVPR (2023) 9\n62. Wang, Y., He, J., Fan, L., Li, H., Chen, Y., Zhang, Z.: Driving into the future: Mul-\ntiview visual forecasting and planning with world model for autonomous driving.\narXiv preprint arXiv:2311.17918 (2023) 2, 4, 8, 9, 11, 14\n63. Weissenborn, D., T\u00e4ckstr\u00f6m, O., Uszkoreit, J.: Scaling autoregressive video models.\narXiv preprint arXiv:1906.02634 (2019) 4\n64. Wen, Y., Zhao, Y., Liu, Y., Jia, F., Wang, Y., Luo, C., Zhang, C., Wang, T.,\nSun, X., Zhang, X.: Panacea: Panoramic and controllable video generation for\nautonomous driving. arXiv preprint arXiv:2311.16813 (2023) 2, 8, 11, 14\n65. Wu, P., Escontrela, A., Hafner, D., Abbeel, P., Goldberg, K.: Daydreamer: World\nmodels for physical robot learning. In: CoRL (2023) 4\n66. Yang, K., Ma, E., Peng, J., Guo, Q., Lin, D., Yu, K.: Bevcontrol: Accurately\ncontrolling street-view elements with multi-perspective consistency via bev sketch\nlayout. arXiv preprint arXiv:2308.01661 (2023) 8\n67. Yang, R., Srivastava, P., Mandt, S.: Diffusion probabilistic modeling for video\ngeneration. arXiv preprint arXiv:2203.09481 (2022) 4\n68. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023) 7, 10\n69. Zhong, Z., Rempe, D., Chen, Y., Ivanovic, B., Cao, Y., Xu, D., Pavone, M., Ray,\nB.: Language-guided traffic simulation via scene-level diffusion. arXiv preprint\narXiv:2306.06344 (2023) 5, 6\n70. Zhong, Z., Rempe, D., Xu, D., Chen, Y., Veer, S., Che, T., Ray, B., Pavone, M.:\nGuided conditional diffusion for controllable traffic simulation. In: ICRA (2023) 5,\n6\nDriveDreamer-2\n19\nIn the supplementary material, we begin by detailing the implementation of\ncustomized traffic simulation and the Unified Mulit-view Video Model (UniMVM).\nThis is followed by an overview of the training specifics for downstream tasks and\nevaluation details. Additionally, a collection of visualization results is provided\nfor further analysis.\n6\nImplement Details\nFunction Library Below is an example of a cut in trajectory generation func-\ntion. Firstly, we initialize a starting point coordinate, and then during the gener-\nation of the cut in trajectory, we check the distance between the agent and other\nagents to control direction and avoid collisions. To enhance the diversity of gen-\nerated trajectories, we introduce random disturbances to the agent\u2019s speed and\norientation, aiming to approximate real-world scenarios as closely as possible.\nMoreover, this function library can be further expanded to meet users\u2019 demands\nfor generating scenes. More details about the function library can be found in\nthe code that we will release later.\ndef cut_in(obj_trajs=None ,obj_vels=None ,\nsafe_dis=10 , is_ego=False):\n\u2019\u2019\u2019\nGenerate a trajectory\nfor a agent\ncutting in.\nParameters:\nReturns:\nobj_trajs: the\ncutting in trajectory of the agent.\nobj_vels: the\nvelocities of the agent.\n\u2019\u2019\u2019\nagent_trajs = np.zeros ((1,NUM_POINT ,3))\nagent_vels = np.zeros ((1,NUM_POINT ,1))\n#initialize\nthe start\npoint\nif not is_ego:\nxy0 = np.random.rand(2)\nxy0[0] = utils. get_random_value (xy0[0],[0,10])\nmulti_factor = 1 if random.random ()>0.5 else -1\nxy0[1] = utils. get_random_value (xy0[1],Y_RANGE)\\\n*multi_factor\ny_margin = xy0[1]*multi_factor\nagent_trajs[0,0,:2] = xy0\ntarget_y = 0\nelse:\ny_margin = abs(obj_trajs[0,0,1])\ntarget_y=obj_trajs[0,0,1]\nvel = utils. get_random_value (np.random.rand (),V_RANGE)\nagent_vels[0,0] = vel\nyaw = utils. get_random_value (np.random.rand (),\nFORWARD_RANGE )\nvel_x = vel*np.cos(yaw)\n20\nG. Zhao, X. Wang et al.\nvel_y = vel*np.sin(yaw)\nflag = True\n# generate\nthe cut in trajectory\nfor t in range(1,NUM_POINT):\nagent_trajs[0,t,0] = agent_trajs[0,t-1,0]\\\n+vel_x*T_INTER\nagent_trajs[0,t,1] = agent_trajs[0,t-1,1]\\\n+vel_y*T_INTER\n#check\nwhether\nthe agent\nreaches\nthe front of the\n#target\nvehicle\nif (agent_trajs[0,t-1,1]-y_margin)*\\\n(agent_trajs[0,t,1]-y_margin)<=0:\nflag = False\n# approach\nthe target\nvehicle\nif abs(agent_trajs[0,t,1]-target_y)\\\n>y_margin/2 and flag:\nyaw_range = [-0.1,0] if multi_factor ==1 \\\nelse [0,0.1]\nyaw_max = 0 if multi_factor==1 else 20*np.pi/180\nyaw_min = -20*np.pi/180 if multi_factor==1 else 0\n# gradually\nreturn to the\nforward\ndirection.\nelif abs(agent_trajs[0,t,1]-target_y)\\\n>0.5 and flag:\nyaw_range = [0,0.1] if multi_factor==1\\\nelse [-0.1,0]\nyaw_max = -10*np.pi/180 if multi_factor==1\\\nelse 20*np.pi/180\nyaw_min = -20*np.pi/180 if multi_factor==1\\\nelse 10*np.pi/180\n# move\nforward\nelse:\nyaw_range = [-0.3*np.pi/180 ,0] if yaw>=0 \\\nelse [0,0.3*np.pi/180]\nyaw_max = 1*np.pi/180\nyaw_min = -1*np.pi/180\n# finetune\nthe\ndirection to avoid\ncollision\nif obj_trajs is not None:\nyaw_range = utils.update_yaw(yaw ,safe_dis ,\nagent_trajs ,\nobj_trajs)\nyaw += utils. get_random_value (np.random.rand (),\nyaw_range)\nyaw = np.clip(yaw ,yaw_min ,yaw_max)\nvel += utils. get_random_value (np.random.rand (),[-2,2]\n)\n# guarentee\nthe\ngenerated\nvelocity\nsurpasses\nthe\n# other\nvehicle to relize\ncutting in\nif obj_vels is not None:\nv_range = [obj_vels[0,t,0]+0.5,V_RANGE[1]+0.5]\nelse:\nDriveDreamer-2\n21\nv_range = V_RANGE\nvel = np.clip(vel ,v_range[0],v_range[1])\nagent_vels[0,t] = vel\nvel_x = vel*np.cos(yaw)\nvel_y = vel*np.sin(yaw)\nreturn\nagent_trajs ,agent_vels\nPrompt Template Fig. 9 depicts the general prompt template designed for\nfinetuning LLM. It includes encapsulated trajectory generation functions, and\ninstruction. During usage, the user prompt integrates into the instruction, di-\nrectly guiding the finetuned LLM to generate the corresponding Python script.\ndef cut_in():\n    '''\n    Generate a trajectory for a agent cutting in.\n    Parameters:\n    Returns:\n    obj_trajs: the cutting in trajectory of the agent.\n    obj_vels: the velocities of the agent.\n    '''\nYou are an expert for generate trajectoris for ego car and other agents. Next you will see some \nPython functions. Given a user prompt, you are asked to use these functions to generate agent \ntajectories that matches the user prompt.\nNow follow the prompt  {USER QUERY} to write the corresponding the Python script.\"\n...\nThere are some of Python functions that you can call for generating trajectories. Note the docstring of \nthe functions to understand what they do.\ndef pedestrian_crossing():\n    '''\n    Generate a trajectory of the pedestrian crossing raod.\n    Parameters:\n    Returns:\n    obj_trajs: the unexpected crossing trajectory of the agent.\n    obj_vels: the velocities of the agent.\n    '''\nFig. 9: Our prompt template designed for finetuning LLM. It contains information\nabout functions, and instruction. The user query is inserted into the placeholder {USER\nQUERY}.\nBEV HDMap Post-Process Since the generated HDMap is in the BEV per-\nspective, it cannot be directly applied to UniMVM for video generation. Hence,\nwe need post-processing to project the BEV HDMap onto the image coordinate\nsystem. Firstly, binarization and skeleton extraction algorithms are employed to\nextract different types of lane markings (as depicted in Fig. 4 (in the main text),\nred for lane boundaries, blue for lane dividers, and green for pedestrian cross-\nings). Then, the extracted lane markings are transformed into pixel coordinates\nto serve as the condition for UniMVM.\nTraining UniMVM We assume the data distribution is pdata(yI), where yI\ndenotes the image latents, and let p(y; \u03c3) be the distribution obtained by adding\ni.i.d. \u03c32-variance Gaussian noise to the data. Note that or sufficiently large \u03c3max,\np(y; \u03c3max2) \u2248N(0, \u03c3max2). Diffusion models (DM) [5, 17, 18, 44] use this fact\nand, starting from high variance Gaussian noise yM \u223cN(0, \u03c3max2), sequentially\ndenoise towards \u03c30 = 0. In practice, this iterative refinement process can be\nimplemented through the numerical simulation of the Probability Flow ordinary\ndifferential equation (ODE) [50]\n  d x =-\\dot {\\si gma  }(t )\\sigma (t)\\nabla _y \\log p(y; \\sigma (t)) dt, \n(4)\n22\nG. Zhao, X. Wang et al.\nFront Left\nBack\nBack Left\nFront\nFront Right\nBack Right\nFrame\nFig. 10: Visualization of the generated multi-view video. Regions highlighted by yellow\nrectangles indicate that DriveDreamer-2 exhibits strong temporal consistency.\nFrame\nFrame\nFig. 11: A generated video of a rainy day scene. The movement of the windshield wipers\n(highlighted by yellow rectangles) demonstrates that as DriveDreamer-2 possesses a\nhigh level of scene understanding.\nwhere \u2207y log p(y; \u03c3) is the score function [24]. DM training reduces to learning a\nmodel s\u03b8(x; \u03c3) for the score function \u2207x log p(y; \u03c3). The model can, for example,\nbe parameterized as \u2207x log p(y; \u03c3) \u2248s\u03b8(y; \u03c3) = (D\u03b8(y; \u03c3) \u2212y)/\u03c32 [27], where\nD\u03b8 is a learnable denoiser that tries to predict the clean yI. The denoiser D\u03b8 is\ntrained via denoising score matching (DSM)\n  \\mathbb {E}_{(y_{\\cal I},c)\\si\nm\n p_{data } (y _{ \\c a l I}\n,\nc\n),(\\sigma ,n)\\sim p(\\sigma ,n)}\\left [\\lambda _\\sigma \\Vert D_\\theta (y_{\\cal I}+n;\\sigma ,c)-y_{\\cal I}\\Vert _2^2\\right ], \n(5)\nwhere p(\u03c3, n) = p(\u03c3)N(n; 0, \u03c32), p(\u03c3) can be a probability distribution or density\nover noise levels \u03c3. \u03bb\u03c3 = (1 + \u03c32)\u03c3\u22122 is a weighting function, and c is the\nconditional signal, which including the HDMap features yH, box conditions yB\nand text prompt embeddings cp. In this work, we follow the EDM preconditioning\nframework [27], parameterizing the learnable denoiser D\u03b8 as\n  D_\\ th e ta (y; \\s i gma ) = c_{skip}(\\ sigma )y + c_{out}(\\sigma )F_\\theta (c_{in}(\\sigma )y; c_{noise}(\\sigma )), \n(6)\nwhere F\u03b8 is the network to be trained, and cskip(\u03c3) = 1/(\u03c32 + 1), cout(\u03c3) =\n\u2212\u03c3/\n\u221a\n\u03c32 + 1, cin(\u03c3) = 1/\n\u221a\n\u03c32 + 1, cnoise(\u03c3) = 0.25 log \u03c3.\nTraining Downstream Tasks StreamPETR is retrained at a resolution of\n512\u00d7256 instead of the 704\u00d7256 in original baseline [57]. The augmented training\ndata is generated by utilizing the structured information from the training set.\nDriveDreamer-2\n23\nFrame 1\nFrame 10\nFrame 12\nFrame 11\nFront Left\nBack\nBack Left\nFront\nFront Right\nBack Right\nFig. 12: A long video generated by the prompt a person crosses the road on a rainy\nday. Regions highlighted by yellow rectangles indicate the movement of the pedestrian.\nEvaluation Details The FID and FVD calculations are performed on 150 val-\nidation videos from the nuScenes dataset [2]. A setting with a frame rate of 4Hz\nand 8 frames per clip is employed to generate data for evaluating FID and FVD.\nAnd we use the official UCF FVD evaluation code2.\n7\nVisualization\nFrame 1\nFrame 9\nFrame 15\nFrame 12\nFront Left\nBack\nBack Left\nFront\nFront Right\nBack Right\nFig. 13: A long video generated by the prompt the ego car changes lane during the\ndaytime.\nAs shown in Fig. 10, DriveDreamer-2 demonstrates strong consistency across\nviews and frames, allowing even the completely occluded car to reappear in sub-\nsequent frames (as highlighted by yellow rectangles). The more details can be\nseen in videos/occluded_car.mp4. Fig. 11 depicts a rainy scene video generated\nby DriveDreamer-2 (see videos/windshield_wiper.mp4 for more details). As il-\nlustrated in Fig. 11, the windshield wipers of the truck are continuously clearing\n2 https://github.com/SongweiGe/TATS/\n24\nG. Zhao, X. Wang et al.\nthe windshield. This showcases the powerful scene understanding capabilities of\nour DriveDreamer-2. It can not only manipulate macroscopic weather conditions\nbut also adjust the behaviors of generated agents within the scene according to\nthe weather.\nFrame 1\nFrame 6\nFrame 8\nFrame 7\nFront Left\nBack\nBack Left\nFront\nFront Right\nBack Right\nFig. 14: A long video generated by the prompt on a rainy day, a car cuts in.\nRegarding the generation of long videos, we can utilize DriveDreamer-2 with-\nout any image conditioning to first generate a clip. Subsequently, the last frame\nof this clip is employed as the initial image condition to generate subsequent\nclips. Below, we showcase the results of directly using text prompts to generate\na 29-frame multi-view video. Fig. 12 is a long video generated by the prompt\na person crosses the road on a rainy day. (see videos/cross_road.mp4 for more\ndetails). It depicts a pedestrian crosses the road in front of the ego vehicle on a\nrainy day. Fig. 13 is generated by a prompt the ego car changes lane during the\ndaytime. (see videos/change_lane.mp4 for more details). It showcases the ego\ncar changes lanes to the right side during the daytime. As shown in Fig. 14, a\nvehicle cuts in from the left to the front of the ego car (prompted by on a rainy\nday, a car cuts in and see more details in videos/cut_in.mp4).\n",
    "2306.04873": "Complexity-aware Large Scale Origin-Destination Network\nGeneration via Diffusion Model\nCan Rong\nTsinghua University\nBeijing, China\nrc20@mails.tsinghua.edu.cn\nJingtao Ding\nTsinghua University\nBeijing, China\ndingjingtao186@qq.com\nZhicheng Liu\nTsinghua University\nBeijing, China\nzhichengliu@tsinghua.edu.cn\nYong Li\nTsinghua University\nBeijing, China\nliyong07@tsinghua.edu.cn\nABSTRACT\nThe Origin-Destination (OD) networks provide an estimation of\nthe flow of people from every region to others in the city, which\nis an important research topic in transportation, urban simulation,\netc. Given structural regional urban features, generating the OD\nnetwork has become increasingly appealing to many researchers\nfrom diverse domains. However, existing works are limited in in-\ndependent generation of each OD pairs, i.e., flow of people from\none region to another, overlooking the relations within the overall\nnetwork. In this paper, we instead propose to generate the OD net-\nwork, and design a graph denoising diffusion method to learn the\nconditional joint probability distribution of the nodes and edges\nwithin the OD network given city characteristics at region level. To\novercome the learning difficulty of the OD networks covering over\nthousands of regions, we decompose the original one-shot genera-\ntive modeling of the diffusion model into two cascaded stages, cor-\nresponding to the generation of network topology and the weights\nof edges, respectively. To further reproduce important network\nproperties contained in the city-wide OD network, we design an\nelaborated graph denoising network structure including a node\nproperty augmentation module and a graph transformer backbone.\nEmpirical experiments on data collected in three large US cities\nhave verified that our method can generate OD matrices for new\ncities with network statistics remarkably similar with the ground\ntruth, further achieving superior outperformance over competitive\nbaselines in terms of the generation realism.\nKEYWORDS\nUrban mobility, origin-destination, diffusion models, complex net-\nwork\n1\nINTRODUCTION\nThe ability to model population mobility for traffic control [2],\nurban planning [38] and resource scheduling [8, 27] has critical im-\npacts on everyday functioning and sustainable development of our\ncities. The Origin-Destination (OD) matrix is a commonly used way\nof structuring mobility flow information in a city, which includes\nthe number of travelers between every two regions. However, it\ncosts a lot to gather the OD matrix of a new city through traditional\ntravel surveys. This has led to studies that generate the OD matrices\nfor new cities without any flow information based on city character-\nistics including geographic structure as well as the demographics\nand urban functions. Unlike OD matrix forecasting [30, 39], this\nproblem of city-wide OD matrix generation aims to generate the\nOD matrix for a city without any historical flow information.\nExisting works for the OD matrix generation can be divided\ninto two categories, i.e., physics-based methods [5, 32, 43] and ma-\nchine learning-based methods [20, 25, 26, 31]. The physics-based\nmethods draw an analogy between population and physical phe-\nnomena [32, 43] and model the flow of people between two regions\nthrough equations with several parameters. The machine learning-\nbased methods follow the data-driven paradigm and utilize com-\nplex models with many parameters trained on a large amount of\ndata to model the nonlinear dependencies between the mobility\nflow and features of the origin as well as the destination. Because\nof the complexity of population mobility, physics-based methods\nhave relatively poor performance, while machine learning-based\nmethods have recently achieved better results and wider applica-\ntions [1, 14, 16, 42] aided by the sophisticated model structure.\nDespite their capability of incorporating complex urban factors\nrelated to flow generation, existing works neglect the relations\nbetween elements in the city-wide OD matrix and instead generate\neach element, i.e., mobility flow, in an independent manner. In\nthis regard, previous studies [28, 41] have investigated urban travel\ndemand patterns in a network perspective and empirically observed\nthe scaling behaviour, i.e., power laws, in distributions of node flux\nand link weight, etc. This important network property of the city-\nwide OD matrix [4, 6, 41] is vital for its generation realism and\nthus calls for a graph generative modeling solution that has been\nuntouched in previous works.\nRecently, deep generative models [10, 12, 18, 34] are proposed\nto model complex joint distribution of data. Among them, the best-\nperforming work is diffusion models [12]. Diffusion models approxi-\nmate complex data distributions by gradually removing small noise\nfrom simple distributions [12]. The elaborate denoising process\nallows diffusion models to effectively fit complex distributions and\ngenerate high quality data. Therefore, we propose to solve the prob-\nlem of city-wide OD matrix generation based on city characteristics\nvia graph denoising diffusion methods.\nHowever, it is hard to directly model joint distribution of the\ncity-wide OD matrix via graph diffusion methods and generate the\nOD matrix for a new city. There are two challenges as follows.\n\u2022 How to overcome the difficulty of generating the city-\nwide OD matrix covering thousands of regions? The\narXiv:2306.04873v2  [cs.LG]  9 Jun 2023\nConference\u201917, July 2017, Washington, DC, USA\nCan Rong, Jingtao Ding, Zhicheng Liu, and Yong Li\ncity-wide OD matrix is equivalent to a mobility flow net-\nwork with thousands of nodes, while existing deep gener-\native graphic models mainly focus on the molecular struc-\nture [11, 15, 23, 37] and can only handle topologies with\ntens or hundreds of nodes. This means building continu-\nous connections at million-level, which greatly increases the\ncomplexity of the joint distribution.\n\u2022 How to achieve realistic reproduction of network prop-\nerties contained in the mobility flow pattern in gen-\neration? The mobility flow network/OD matrix exhibits\nsparsity and scaling behaviours. People prefer to travel a\nshort distance, which causes no flow between most regions.\nMoreover, the spatial distribution of mobility in a city ap-\npears highly heterogeneous with scaling behavior on node\nand edge level [28, 41]. Maintaining these properties in gen-\nerated city-wide OD matrices is a claim of realism, but very\nchallenging.\nSolving above two challenges calls for a decoupling of the topol-\nogy and weights of the mobility flow network and locally graphic\nmodeling from node and edge level in graph diffusion methods. We\npropose a cascaded graph denoising diffusion method for soling\nthe problem of city-wide OD matrix generation (DiffODGen). To\ngenerate the city-wide OD matrix and overcome the first challenge,\nwe dismember the generation process into two stages, and con-\nstruct a pipeline comprised by two diffusion models to deal with\nsub-tasks in each stage. In the first stage, our proposed method\ndetermines the existence of flow, i.e., topology structure of the mo-\nbility flow network with a topology diffusion model. Then, the flow\nvolumes of region pairs that have flows will be generated via the\nflow diffusion model. For better integration of the two stages, we\ndesign a collaborative training to eliminate the cascading errors. For\nmaintaining network properties of sparsity and scaling behaviors,\nwe utilize the discrete denoising process [3, 37] in the topology\ndiffusion model and apply a graph transformer-based network pa-\nrameterization to model the city-wide OD matrix from network\nperspective respectively. Besides, we specially design node prop-\nerties augmentation modules in both diffusion models to enhance\nthe denoising networks with the capability of fully modeling the\nscaling behaviors on node level.\nIn summary, the contributions can be summarized as follows.\n\u2022 We propose to leverage the idea of graph generative mod-\neling in learning the joint distribution of mobility flows for\ngenerating city-wide OD matrices in new cities. To the best\nof our knowledge, we are the first to resolve this important\nurban computing problem of OD matrix generation within\nthe diffusion model paradigm.\n\u2022 We design a cascaded graph denoising diffusion method and\ngraph transformer network parameterization with a node\nproperties augmentation module to generate city-wide OD\nmatrix for new cities with the network properties retained.\n\u2022 Experiment results on two real-world dataset collected from\ntwo large US cities demonstrate the superiority of our Dif-\nfODGen over state-of-the-art baselines in terms of generat-\ning realistic city-wide OD matrices. Importantly, the gener-\nated OD matrix exhibits excellent network statistics similari-\nties of sparsity and scaling behaviors in both node level and\nedge level with the real-world data.\n2\nRELATED WORK\nIn this section, we will give a comprehensive review of the related\nworks on OD Matrix generation and recent achievements of diffu-\nsion models in the field of graph learning.\n2.1\nOD Flow Generation\nResearch related to OD matrix generation has a very long history,\nwith a recent boom brought about by the rise of machine learning\nalgorithms. The methods utilized in these works are mainly in\ntwo categories. The first kind of methods are inspired by classic\nphysical laws. In 1946, Zipf [43] introduced Newton\u2019s physics law\nof Gravitation to model the mobility flow between two regions. The\npopulation of one region is considered as the mass while the flow\nbetween two regions is modeled as the universal gravitational force\nbetween objects. Simini et al. [32] compare the movement of people\nwithin urban space to the processes of emission and absorption of\nradiation in solid physics. Due to the limitation of only modeling\nthe complex population mobility based on simple physical models,\nthese physical methods perform poorly.\nWith the advances in machine learning, a growing spectrum\nof problems are being tackled in the data-driven paradigm, and\npopulation mobility modeling is no exception. Robinson et al. [26]\nshow that the tree-based machine learning methods perform much\nbetter than traditional physics-based methods, especially Gradient\nBased Regression Trees (GBRT). Pourebrahim et al. [25] compare\npopular machine learning models and conclude that random forest\nperforms best. Simini et al. [31] introduce deep neural networks\nto enhance the gravity model with richer urban features, granting\nimproved nonlinear modeling capabilities. Liu et al. [20] bring the\ngraph neural networks into the domain of OD flow prediction,\nusing multitask learning as a training strategy to learn embedding\nwith stronger representational power to improve the prediction.\nHowever, these models all only consider the features of one OD\npair while ignoring complicated dependencies between mobility\nflows. These models cannot model the network statistic properties\nof the city-wide mobility flow networks [28].\n2.2\nDiffusion Models\nDiffusion models, which consist of a diffusion (noise-adding) pro-\ncess without any learning parameters and a reverse denoising pro-\ncess converting the sampled noise into data following complicated\ndistribution with the help of denoising networks, are a kind of re-\ncent emerging generative models [12, 22, 33, 34]. Such models have\noutperformed other deep learning generative models such as Varia-\ntional Auto-encoders (VAEs) [18], Generative Adversarial Networks\n(GANs) [10] etc in many domains such as computer visions [12, 29],\naudio generations [19, 35] and the graph generation [15, 23, 37].\nComplexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model\nConference\u201917, July 2017, Washington, DC, USA\nThe most relevant works are those that apply diffusion models\nto graph generation task [15, 23, 37]. Niu et al. [23] are the first\nto utilize score-based diffusion models which adopt continuous\nGaussian noise to construct the diffusion process to generate the\nadjacency matrix with the capability of permutation invariance.\nFurther, Jo et al. [15] explore the possibility of jointly generating\ngraphs with node and edge features employing diffusion models.\nRecent works [11, 37] have explored the construction of discrete\ndenoising diffusion models with multinomial noise, enabling the\ngenerated graphs to retain sparsity and improve the generation\nquality. Our work explores the feasibility of applying graph diffu-\nsion models to generate the city-wide OD matrices (mobility flow\nnetworks), where not only the volumes of the mobility flow are\ngenerated by denoising continuous noise, but also the sparsity and\nscaling properties of OD matrices [28, 41] are ensured by graph\ndenoising diffusion models.\n3\nPRELIMINARIES\nIn this section, we will list important notations and definitions. And\na brief introduction to principles of diffusion models are presented.\n3.1\nDefinitions and Problem Formulation\nDefinition 3.1. Regions. The city is spatially partitioned into\nseveral non-overlapping regions denoted as R = {\ud835\udc5f\ud835\udc56|\ud835\udc56= 1, 2, ..., \ud835\udc41},\nwhere \ud835\udc41is the number of regions. The different regions are located\nin different parts of the city and perform different urban functions,\nwhich are reflected through the urban characteristics X\ud835\udc5f, such as\ndemographics and points of interests (POIs).\nDefinition 3.2. OD flow. The OD flow denotes the directed mo-\nbility flow F\ud835\udc5f\ud835\udc5c\ud835\udc5f\ud835\udc54,\ud835\udc5f\ud835\udc51\ud835\udc60\ud835\udc61between a specific region pair that departs\nfrom the origin \ud835\udc5f\ud835\udc5c\ud835\udc5f\ud835\udc54and moves to the destination \ud835\udc5f\ud835\udc51\ud835\udc60\ud835\udc61.\nDefinition 3.3. OD Matrix. The OD matrix F \u2208R\ud835\udc41\u00d7\ud835\udc41includes\nthe OD flows between every two regions in the city, where \ud835\udc39\ud835\udc56,\ud835\udc57\nstands for the mobility flow from region \ud835\udc5f\ud835\udc56to region \ud835\udc5f\ud835\udc57.\nProblem 1. OD Matrix Generation. Given the regional urban\ncharacteristics of the city {X\ud835\udc5f|\ud835\udc5f\u2208R}, generate the whole picture of\nmobility flow of that city, i.e., the OD matrix F.\nThe OD matrix of a city can also be regarded as a directed\nweighted network G = {V, E}, where the nodes V = {\ud835\udc63} represent\nregions R = {\ud835\udc5f} and the directed edges E = {\ud835\udc52} with weights {\ud835\udc64\ud835\udc52}\nrepresent mobility flows.\nDefinition 3.4. Mobility Flow Network. The OD matrix \ud835\udc39in\nthis paper has the same meaning as a mobility flow network G,\nexcept that one is in terms of data organization and the other is\nfrom the network perspective. The element \ud835\udc39\ud835\udc56\ud835\udc57is equivalent to \ud835\udc64\ud835\udc52\ud835\udc56\ud835\udc57,\nwhere \ud835\udc52\ud835\udc56\ud835\udc57denotes the directed edge from node \ud835\udc56to node \ud835\udc57.\nDefinition 3.5. Adjacency Matrix. A adjacency matrix \ud835\udc40is a\n0-1 binary matrix, where \ud835\udc40\ud835\udc56\ud835\udc57= 0 means no edge from node \ud835\udc56to\nnode \ud835\udc57, i.e., no mobility flow from region \ud835\udc5f\ud835\udc56to region \ud835\udc5f\ud835\udc57and \ud835\udc40\ud835\udc56\ud835\udc57= 1\nis the opposite. This indicates whether there are edges between\nnodes in the mobility flow network.\n3.2\nDiffusion Models\nThe diffusion models, which consist of a diffusion process and a\nreverse denoising process, aim to learn the sophisticated data dis-\ntribution \ud835\udc5d(\ud835\udc650) by removing noise from simple distributions using\na Markov process [12]. Through the forward process \ud835\udc5e(\ud835\udc65\ud835\udc61|\ud835\udc65\ud835\udc61\u22121),\nthe model produces a series of noisy data as hidden states {\ud835\udc65\ud835\udc61|\ud835\udc61=\n1, ...,\ud835\udc47} by adding a small noise based on the original raw data \ud835\udc650.\n\ud835\udc5e(\ud835\udc65\ud835\udc61|\ud835\udc65\ud835\udc61\u22121) = N (\ud835\udc65\ud835\udc61;\n\u221a\ufe01\n1 \u2212\ud835\udefd\ud835\udc61\ud835\udc65\ud835\udc61\u22121, \ud835\udefd\ud835\udc61I),\n\ud835\udc5e(\ud835\udc651, ...,\ud835\udc65\ud835\udc47|\ud835\udc650) =\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc5e(\ud835\udc65\ud835\udc61|\ud835\udc5e\ud835\udc61\u22121),\n(1)\nwhere \ud835\udc47is the diffusion steps and {\ud835\udefd\ud835\udc61|\ud835\udc61= 1, ...,\ud835\udc47} is variance\nschedule. When sampling data from learned distribution, we need\na neural network \ud835\udf03to complete the reverse denoising process by\npredicting hidden states\ud835\udc5d\ud835\udf03(\ud835\udc65\ud835\udc61\u22121|\ud835\udc65\ud835\udc61) without\ud835\udc650 by neural networks\nuntil the last step of the denoising process has been performed:\n\ud835\udc5d\ud835\udf03(\ud835\udc650, ...,\ud835\udc65\ud835\udc47\u22121) =\n\ud835\udc47\n\u00d6\n\ud835\udc61=1\n\ud835\udc5d\ud835\udf03(\ud835\udc65\ud835\udc61\u22121|\ud835\udc65\ud835\udc61),\n(2)\nDifferent diffusion processes utilize different kinds of noises (Gauss-\nian noise for continuous data [12] and multinomial noise for discrete\ncategorical data [3]), depending on the specific scenario.\n4\nMETHOD\nIn this section, we will detail the cascaded graph denoising dif-\nfusion method for city-wide OD matrix generation in new cities\n(DiffODGen). First, we will introduce the framework of the method\nand dive into concrete information about each part of it.\n4.1\nCascaded Graph Denoising Diffusion\n4.1.1\nOverall Framework. As shown in Figure 1, our method\nincludes two stages, which can be treated as separating the city-\nwide OD matrix generation task into two steps. In the first stage,\nas we can see in the upper part of Figure 1, the method determines\nwhether there is a mobility flow between regions \ud835\udc43\ud835\udf03\ud835\udc40(\ud835\udc40|\ud835\udc4b), i.e.,\ntopology structure, with the topology diffusion model. Then, the\nregion pairs without flow will be directly set to zero flow and\ndiscarded in the rest of the process. According to the sparsity of\nthe OD matrix, this will greatly reduce the scale of city-wide OD\nmatrix generation and thus solve the first challenge. In the second\nstage, we use a continuous denoising diffusion model to learn the\njoint probability distribution of OD flows between the remaining\nregion pairs \ud835\udc43\ud835\udf03\ud835\udc39(\ud835\udc39|\ud835\udc4b) for generation.\nThe diffusion models in both the two stages are in the form of\nclassifier-free conditional diffusion models [13], where pairwise\nregional attributes are element-wise assigned to the OD flows be-\ntween each two regions in the OD matrix. This modeling can make\nthe generated OD matrix not only globally conform to the overall\nproperties, but also retain the advantages of the pairwise predictive\nmodel by fully considering the features of OD pairs so that OD\nflows are also locally reasonable.\n4.1.2\nCollaborative Training. A noteworthy issue is that com-\nbining the two diffusion models directly has the problem of cascad-\ning errors, i.e., the errors generated in the topology generation stage\nConference\u201917, July 2017, Washington, DC, USA\nCan Rong, Jingtao Ding, Zhicheng Liu, and Yong Li\nFigure 1: The architecture of our cascaded graph denoising diffusion method for the city-wide OD matrix generation.\nare completely transferred to the flow generation stage, which leads\nto an amplification of the overall instability. To solve this issue, we\nadopt the strategy of teacher-force learning to collaborate the\ntraining of networks \ud835\udf03\ud835\udc40and \ud835\udf03\ud835\udc39. The training process is based on\ndata from the source city, where the true value of the OD matrix\nis known. Not the adjacency matrix \u02c6\ud835\udc40generated is applied for\ntraining the network parameters in the flow diffusion model, but\nthe union e\n\ud835\udc40of generated \u02c6\ud835\udc40and the ground truth \ud835\udc40instead, which\nis computed as follow,\ne\n\ud835\udc40\ud835\udc56\ud835\udc57= \u02c6\ud835\udc40\ud835\udc56\ud835\udc57\u2228\ud835\udc40\ud835\udc56\ud835\udc57\n\ud835\udc53\ud835\udc5c\ud835\udc5f\ud835\udc56= 1, ..., \ud835\udc41\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc57= 1, ..., \ud835\udc41,\n(3)\nwhere the \u2228denotes the logic operator of OR. e\n\ud835\udc40then serves as\na mask in the flow diffusion model, so that it focuses only on the\nelements in the OD matrix whose value are not zero. In this way, the\nflow diffusion model has the capability of tolerating errors arising\nin the topology diffusion model and remedy them by generating the\nmissing zero flows during the topology generation, thus improving\nthe overall robustness of the method.\n4.2\nTopology Diffusion Model\nNext, we will give a introduction to the topology diffusion model,\nincluding discrete denoising process and network parameterization.\n4.2.1\nDiscrete Denoising Process. To avoid destroying the spar-\nsity of the generated OD matrix, we utilize the reverse denoising\nprocess with discrete state spaces to generate the adjacency matrix\nin the topology diffusion model. We adopt the d3pm proposed by\nAustin et al [3], which works for graph generation task [37], to\napproximate the data distribution \ud835\udc5d\ud835\udc40(\ud835\udc40) with \ud835\udc5d\ud835\udf03\ud835\udc40(\ud835\udc400). In our\napproach, data \ud835\udc40\ud835\udc56\ud835\udc57\u2208R\ud835\udc51, which follows a binary distribution, is en-\ncoded with one-hot encoding, where \ud835\udc51equals two, i.e., the number\nof classes (zero/non-zero flow). For the convenience of represen-\ntation, we briefly refer to the element in adjacency matrix \ud835\udc40\ud835\udc56\ud835\udc57as\n\ud835\udc5ain the following. The noise-addition in the diffusion process is\nimplemented through the matrix product with a series of transition\nmatrix (Q1, ..., Q\ud835\udc47),\n\ud835\udc5e(\ud835\udc5a\ud835\udc61|\ud835\udc5a\ud835\udc61\u22121) = \ud835\udc5a\ud835\udc61\u22121Q\ud835\udc61,\n(4)\nsuch that Q\ud835\udc61\n\ud835\udc56\ud835\udc57denotes the probability of state transiting from class \ud835\udc56\nto class \ud835\udc57. It is worth noting that the noisy data state\ud835\udc5a\ud835\udc61at\ud835\udc61diffusion\nstep can be directly obtained by closed-form [3] calculations,\n\ud835\udc5e(\ud835\udc5a\ud835\udc61|\ud835\udc5a0) = \ud835\udc5a0 \u00afQ\ud835\udc61,\n(5)\nwhere \u00afQ\ud835\udc61= Q1Q2...Q\ud835\udc61. The transition matrix Q\ud835\udc61applied in our\nmethod introduce the uniform noise perturbation, which is calcu-\nlated as follows,\nQ\ud835\udc61= \ud835\udefc\ud835\udc61I + (1 \u2212\ud835\udefc\ud835\udc61)1\ud835\udc511\ud835\udc51\u22a4\n\ud835\udc51\n,\n(6)\nwhere \u22a4means the matrix transposition and \ud835\udefccontrols the degree\nof adding noise. Given \ud835\udc5a0, the reverse denoising process can also\nbe computed in closed-form based on Bayes rule [3],\n\ud835\udc5e(\ud835\udc5a\ud835\udc61\u22121|\ud835\udc5a\ud835\udc61,\ud835\udc5a0) = \ud835\udc5a\ud835\udc61Q\ud835\udc61\u22a4\u2299\ud835\udc5a0 \u00afQ\ud835\udc61\u22121\n\ud835\udc5a0 \u00afQ\ud835\udc5a\ud835\udc61\u22a4\n,\n(7)\nwhere \u2299means the element-wise product. Since \ud835\udc5a0 is unknown,\nwe construct neural networks to fit the probability of \ud835\udc5d\ud835\udf03( \u02c6\ud835\udc5a0|\ud835\udc40\ud835\udc61)\ngiven noisy topology matrix \ud835\udc40\ud835\udc61and realize the denoising process\nwith predicted \u02c6\ud835\udc5a0 when doing generation,\n\ud835\udc5d\ud835\udf03\ud835\udc40(\ud835\udc40\ud835\udc61\u22121|\ud835\udc40\ud835\udc61) \u221d\n\u2211\ufe01\n\u02c6\ud835\udc5a0\n\ud835\udc5e(\ud835\udc5a\ud835\udc61\u22121|\ud835\udc5a\ud835\udc61, \u02c6\ud835\udc5a0)\ud835\udc5d\ud835\udf03\ud835\udc40( \u02c6\ud835\udc5a0|\ud835\udc40\ud835\udc61).\n(8)\nFor generating the adjacency matrix based on specific city character-\nistics, we then extend the above denoising process into a conditional\ndenoising process to model conditional probability \ud835\udc5d\ud835\udf03\ud835\udc40(\ud835\udc400|X\ud835\udc5f\u2208R).\nThe conditional denoising process is as follows,\n\ud835\udc5d\ud835\udf03\ud835\udc40(\ud835\udc40\ud835\udc61\u22121|\ud835\udc40\ud835\udc61) \u221d\n\u2211\ufe01\n\u02c6\ud835\udc5a0\n\ud835\udc5e(\ud835\udc5a\ud835\udc61\u22121|\ud835\udc5a\ud835\udc61, \u02c6\ud835\udc5a0)\ud835\udc5d\ud835\udf03\ud835\udc40( \u02c6\ud835\udc5a0|\ud835\udc40\ud835\udc61, X\ud835\udc5f).\n(9)\nIn the procedure of generation, a pure noise \ud835\udc40\ud835\udc47\u2208R\ud835\udc41\u00d7\ud835\udc41\u00d7\ud835\udc51,\nwhich follows the uniform distribution, is first sampled. The data\nsamples that follow the original data distribution are then recov-\nered gradually from the \ud835\udc40\ud835\udc47by the iterative calculation of reverse\ndenoising process described by Eq. 9, where \ud835\udf03\ud835\udc40is the trained neu-\nral networks. It should be emphasized that the whole process of\nrecovering data through latent states is performed on a discrete\nspace, which can fully preserve the sparsity of the OD matrix [37].\n4.2.2\nBackbone of Network Parameterization. For better con-\nsidering the adjacency matrix, i.e., topology structure and fully\nmodeling the network properties [28], the network parameteriza-\ntion of topology diffusion model is designed to build with the graph\ntransformer proposed by Dwivedi et al [9]. Moreover, we make\ntwo improvements to the network, adapting it to classifier-free\nelement-wise conditional schema and integrating a node proper-\nties augmentation module to enhance the ability of modeling the\nnetwork properties of mobility flows [28].\nComplexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model\nConference\u201917, July 2017, Washington, DC, USA\nFigure 2: The architecture of network parameterization of the diffusion model.\nThe backbone of the graph transformer receives two inputs, node\nfeatures and edge features, as shown in Figure 2. The graph trans-\nformer consists of several layers. In each layer, all nodes form a\nposition-insensitive sequence. The attention weights between each\npair of nodes will be calculated through self-attention. It is worth\nnoting that the calculation of attention weights used for informa-\ntion propagation between nodes also incorporates the influence\nof edge features. Each node updates its own features by weighted\naggregation of neighbor information. Each edge updates itself by\nfusing its own features with the attention information between\nits end nodes. The calculation process of each layer in the graph\ntransformer is as follows,\n\u210e\ud835\udc59+1\n\ud835\udc56\n= \ud835\udc42\ud835\udc59\n\u210e\u2225\ud835\udc3e\n\ud835\udc58=1(\n\u2211\ufe01\n\ud835\udc5f\ud835\udc57\u2208N\ud835\udc5f\ud835\udc56\n\ud835\udefc\ud835\udc58,\ud835\udc59\n\ud835\udc56\ud835\udc57\ud835\udc49\ud835\udc58,\ud835\udc59\u210e\ud835\udc59\n\ud835\udc57),\n\ud835\udc52\ud835\udc59+1\n\ud835\udc56\ud835\udc57\n= \ud835\udc42\ud835\udc59\n\ud835\udc52\u2225\ud835\udc3e\n\ud835\udc58=1(\ud835\udc4e\ud835\udc58,\ud835\udc59\n\ud835\udc56\ud835\udc57),\n\ud835\udefc\ud835\udc58,\ud835\udc59\n\ud835\udc56\ud835\udc57= \ud835\udc60\ud835\udc5c\ud835\udc53\ud835\udc61\ud835\udc5a\ud835\udc4e\ud835\udc65\ud835\udc57(\ud835\udc4e\ud835\udc58,\ud835\udc59\n\ud835\udc56\ud835\udc57),\n\ud835\udc4e\ud835\udc58,\ud835\udc59\n\ud835\udc56\ud835\udc57= (\n\ud835\udc44\ud835\udc58,\ud835\udc59\u210e\ud835\udc59\n\ud835\udc56\u00b7 \ud835\udc3e\ud835\udc58,\ud835\udc59\u210e\ud835\udc59\n\ud835\udc57\n\u221a\ufe01\n\ud835\udc51\ud835\udc58\n) +\ud835\udc4a\ud835\udc58,\ud835\udc59\ud835\udc52\ud835\udc59\n\ud835\udc56\ud835\udc57,\n(10)\nwhere \u210e\ud835\udc59\n\ud835\udc56means the hidden states of node \ud835\udc56after \ud835\udc59layers, \ud835\udc52\ud835\udc59\n\ud835\udc56\ud835\udc57means\nthe hidden states of the edge from node \ud835\udc56to node \ud835\udc57after \ud835\udc59layers, \ud835\udc42\ud835\udc59\n\u210e,\n\ud835\udc42\ud835\udc59\ud835\udc52, \ud835\udc44\ud835\udc58,\ud835\udc59, \ud835\udc3e\ud835\udc58,\ud835\udc59,\ud835\udc49\ud835\udc58,\ud835\udc59and\ud835\udc4a\ud835\udc58,\ud835\udc59are the learnable parameters, \u2225denotes\nthe concatenation, \ud835\udc58= 1 to \ud835\udc3emeans the number of attention heads.\nThe output of each layer will be input to the next layer. After the\nlast layer, the output of node features \ud835\udc3b\ud835\udc3fis dropped and the output\nof edge features \ud835\udc38\ud835\udc3fis converted into a tensor of the same shape as\n\ud835\udc40\ud835\udc61by a fully-connected layer to predict \u02c6\ud835\udc400, as shown in Figure 2.\n4.2.3\nNode Properties Augmentation for Topology. Message-\npassing graph neural networks have a limitation of unable to cap-\nture some specific features [21, 40], such as the weighted aggrega-\ntion schema can hardly model the degree of nodes in a graph. How-\never, the network properties of mobility flow are largely reflected\nin the region level, i.e., node level in mobility flow network [28].\nTherefore, we augment the capability of our graph transformer with\na node properties augmentation to model the node level flow charac-\nteristics by statistically counting the properties of nodes into node\nfeatures based on the adjacency matrix. The statistic properties of\nnodes include, but are not limited to, the degree of the node and the\ncentrality, etc. With the help of node properties augmentation, the\nnetwork can comprehensively model the noisy topology structure\nfrom both node and edge level in each diffusion step.\n4.2.4\nClassifier-free Element-wise Conditions. As shown in\nFigure 2, we make an improvement by introduce a CondEmb module\non the neural network backbone to make it classifier-free element-\nwise conditional denoising networks, so that diffusion models can\ndetermine whether there is a mobility flow based on its characteris-\ntics of origin and destination. First, urban attributes for each region\n{X\ud835\udc5f|\ud835\udc5f\u2208R} are processed by MLPs to become regional condition\nembeddings, which have the same dimension as the node features\nthat will be input to the graph transformer, as shown in Figure 2.\nThen, regional condition embeddings are fused with node features\nand edge features respectively through cross attention \u2297as the\nfinal input of the transformer, where the node features are directly\nfused with the embeddings region by region, while the edge fea-\ntures are fused with the concatenation of the embeddings of origin\nand destination as well as the distance between them. In this way,\neach row and column in the adjacency matrix, and each element\nhas corresponding spatial urban features as its unique conditions\nthat affect its generation in the denoising process.\n4.2.5\nTraining of Topology Diffusion Model. The network\n\ud835\udf03\ud835\udc40introduced above is trained by reducing the cross entropy be-\ntween the prediction \u02c6\ud835\udc400\n\ud835\udc56\ud835\udc57and true value \ud835\udc400\n\ud835\udc56\ud835\udc57of each element to\nempower the denoising process represented by Eq. 9. The overall\ncross entropy loss of the neural networks is calculated as follows,\nL\ud835\udc40( \u02c6\ud835\udc400, \ud835\udc400) =\n1\n\ud835\udc412\n\u2211\ufe01\n1\u2264\ud835\udc56,\ud835\udc57\u2264\ud835\udc41\nLCE(\ud835\udc5d\ud835\udf03\ud835\udc40( \u02c6\ud835\udc400\n\ud835\udc56\ud835\udc57|\ud835\udc40\ud835\udc61), \ud835\udc40\ud835\udc56\ud835\udc57),\n(11)\nwhere LCE means cross entropy loss. The training algorithm of net-\nworks is shown as Algorithm 1. Once neural networks are trained,\nthey can be employed to generate topology structure of mobility\nflow networks for better city-wide OD matrix generation.\n4.3\nFlow Diffusion Model\nGiven the adjacency matrix \u02c6\ud835\udc400 generated by the topology diffusion\nmodel, the mask e\n\ud835\udc40that determines which region pairs the flows\nshould be generated is calculated by Eq. 3. The mask e\n\ud835\udc40is then used\nin the flow diffusion model to determine which elements of the\nOD matrix need flow generation. It is worth noting that since the\nConference\u201917, July 2017, Washington, DC, USA\nCan Rong, Jingtao Ding, Zhicheng Liu, and Yong Li\nAlgorithm 1: Training Algorithm of Topology Diffusion\nModel\nInput:\nCities characteristics of the training city:\n\u2022 Spatial structure of the training city: R\ud835\udc60\ud835\udc5f\ud835\udc50\n\u2022 Regional city characteristics: {X\ud835\udc5f|\ud835\udc5f\u2208R\ud835\udc60\ud835\udc5f\ud835\udc50}\nAdjacency matrix of the training city:\n{\ud835\udc40\ud835\udc56\ud835\udc57|\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5f\ud835\udc57\u2208R\ud835\udc60\ud835\udc5f\ud835\udc50}\nOutput:\nLearned discrete denoising neural networks \ud835\udf03\ud835\udc40.\n1 repeat\n2\n\ud835\udc400 = \ud835\udc40;\n3\nSample \ud835\udc61\u223cU(1, 2, ...,\ud835\udc47) ;\n4\nSample \ud835\udc40\ud835\udc61\u223c\ud835\udc400 \u00afQ\ud835\udc61;\n5\nPredict \u02c6\ud835\udc400 via \ud835\udf03\ud835\udc40given \ud835\udc40\ud835\udc61and {X\ud835\udc5f|\ud835\udc5f\u2208R\ud835\udc60\ud835\udc5f\ud835\udc50} ;\n6\n\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60\u21d0= L\ud835\udc40( \u02c6\ud835\udc400, \ud835\udc400) ;\n7\noptimizer.step(\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60) ;\n8 until Loss convergences.;\ngenerated adjacency matrix is not completely accurate, there are\nstill zero flows that will be generated by the flow diffusion model.\n4.3.1\nContinous Denoising Diffusion Models. Different from\ntopology generation, we adopt Gaussian noise to construct the\ndiffusion process since OD flows are continuous values. Since\nDDPM [12] was proposed, diffusion models in continuous latent\nspaces have been explored to a significant extent [22]. In this part,\nthe process of noise-addition diffusion of our diffusion models is per-\nformed on the image-like tensor, i.e., the OD matrix \ud835\udc39\u2208R\ud835\udc41\u00d7\ud835\udc41\u00d7\ud835\udc51,\nwhere \ud835\udc51equals 1. Differently, the reverse denoising process is per-\nformed only at the position, where the elements equal to 1, in the\nmask e\n\ud835\udc40, as shown in Figure1. Thanks to the sparsity of the OD\nmatrix, although the OD matrix is large, our denoising process only\nfocuses on the nonzero flow part, so we can effectively model it.\nThe diffusion process of the flow diffusion model is similar to\nthe classical continuous diffusion models and has no parameters,\nwhich is shown in Eq. 1 from right to left. So we will not cover\nit repeatedly, but rather introduce the denoising process in detail.\nThe computation of each denoising step is shown as follows,\n\ud835\udc5d\ud835\udf03\ud835\udc39(\ud835\udc39\ud835\udc61\u22121|\ud835\udc39\ud835\udc61, {X\ud835\udc5f}) = N (\ud835\udc39\ud835\udc61\u22121; \ud835\udf07\ud835\udf03\ud835\udc39(\ud835\udc4b\ud835\udc61,\ud835\udc61|{X\ud835\udc5f}, (1 \u2212\u00af\ud835\udefc\ud835\udc61)I),\n(12)\nwhere N means the Gaussian distribution, \ud835\udefcis the noise schedule\nand \u00af\ud835\udefc\ud835\udc61= \ud835\udefc1\ud835\udefc2...\ud835\udefc\ud835\udc61.\n4.3.2\nNetwork Parameterization. Similar to the topology diffu-\nsion model, we construct a conditional diffusion model by applying\ncity characteristics as conditions to generate OD flows between\nregions. The network parameterization is also based on the graph\ntransformer, except that we use the geo-contextual embedding\nlearning methods proposed in GMEL [20] as the CondEmb module\nshown in Figure 2 to enhance our condition features representa-\ntional capabilities. To be specific, urban attributes {X\ud835\udc5f|\ud835\udc5f\u2208R} for\neach region are processed by two GATs [36] to obtain the embed-\ndings for each region as the origin and the destination, respectively.\nThen, region embeddings that are fused with the node features\nAlgorithm 2: Training Algorithm in Flow Diffusion Model\nInput:\nCities characteristics of the training city:\n\u2022 Spatial structure of the training city: R\ud835\udc60\ud835\udc5f\ud835\udc50\n\u2022 Regional city characteristics: {X\ud835\udc5f|\ud835\udc5f\u2208R\ud835\udc60\ud835\udc5f\ud835\udc50}\n\u2022 Mask e\n\ud835\udc40from the topology diffusion model\nOD matrix of the training city: {\ud835\udc39\ud835\udc56\ud835\udc57|\ud835\udc5f\ud835\udc56\ud835\udc4e\ud835\udc5b\ud835\udc51\ud835\udc5f\ud835\udc57\u2208R\ud835\udc60\ud835\udc5f\ud835\udc50}\nOutput:\nLearned noise prediction neural networks \ud835\udf03\ud835\udc39.\n1 repeat\n2\n\ud835\udc390 = \ud835\udc39;\n3\nSample \ud835\udc61\u223cU(1, 2, ...,\ud835\udc47) ;\n4\nSample \ud835\udf16\u223cN (0, I) ;\n5\n\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60\u21d0=\n\r\r\r e\n\ud835\udc40\u00b7\nh\n\ud835\udf16\u2212\ud835\udf16\ud835\udf03\ud835\udc39(\n\u221a\n\u00af\ud835\udefc\ud835\udc61\ud835\udc390 +\n\u221a\n1 \u2212\u00af\ud835\udefc\ud835\udc61\ud835\udf16,\ud835\udc61, {X\ud835\udc5f})\ni\r\r\r\n2\n;\n6\noptimizer.step(\ud835\udc59\ud835\udc5c\ud835\udc60\ud835\udc60) ;\n7 until Loss convergences.;\ninput into the graph transformer are composed of the origin em-\nbedding and the destination embedding of the region. The edge\nembeddings consist of the origin embedding of the origin and the\ndestination embedding of the destination, as well as the distance\nbetween them. In addition, node properties augmentation is also\nused for flow generation to enhance the graph modeling capability\nfrom both node and edge level.\n4.3.3\nTraining of Flow Diffusion Model. Ho et al. [12] show\nthat the denoising process can be trained by predicting the added\nnoise in the diffusion process at the corresponding time step, i.e.,\noptimizing the neural networks through the following objective,\n\ud835\udc5a\ud835\udc56\ud835\udc5b\ud835\udf03\ud835\udc39E\ud835\udc390\u223c\ud835\udc5e(\ud835\udc390),\ud835\udf16\u223cN(0,I) ||\ud835\udf16\u2212\ud835\udf16\ud835\udf03F (\ud835\udc39\ud835\udc61,\ud835\udc61|{X\ud835\udc5f})||2\n2,\n(13)\nwhere \ud835\udf16denotes the noise added during the step \ud835\udc61in diffusion\nprocess and || \u00b7 ||2 means the \ud835\udc3f\u22122 norm. The training algorithm\nis referred to Algorithm 2. The e\n\ud835\udc40is used in collaborative training\nschema to reduce the effect of cascade errors of combining the two\nstages, whcih has been introduced in Sec. 4.1.2. Once the neural\nnetwork is trained, it can be used to generate the OD flows for\ngiven topology of mobility flow network. First, a pure noise is\nsampled from the standard Gaussian noise N (0, I). The noise \ud835\udf16\ud835\udf03\ud835\udc39\nis then sequentially predicted and removed from \ud835\udc39\ud835\udc61to obtain \ud835\udc39\ud835\udc61\u22121.\n\ud835\udc390 is finally generated via \ud835\udc47denoising step iteratively. It is worth\nemphasizing that elements with a mask of 0 in e\n\ud835\udc40are not involved\nin the training and generation process because of the mask e\n\ud835\udc40.\n5\nEXPERIMENTS\n5.1\nExperimental Setup\n5.1.1\nData Description. We choose three major US metropolises\nfor our experiments. The cities are divided into regions at the census\ntract level. The basic statistic information on the cities is presented\nin Table 1. The data for each city includes two parts, city character-\nistics and the OD matrix, which are introduced as follows.\nCity Characteristics. City characteristics depict the geographic\nstructure of an entire city and the spatially heterogeneous distri-\nbution of city functions. In detail, each region carries attributes\nComplexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model\nConference\u201917, July 2017, Washington, DC, USA\nTable 1: Basic statistics of the dataset of the cities (\u201cNZRflow\u201d\nstands for the rate of nonzero flows).\nCity\n#regions #commuters #population area(km2) NZRflow\nNYC\n1,296\n1,694,884\n3,004,606\n451\n24.9%\nCook\n1,319\n1,776,489\n1,974,181\n4,230\n25.2%\nSeattle\n721\n1,595,531\n3,495,493\n5,872\n41.9%\nabout demographics and urban functions, which are portrayed\nthrough American Community Survey Data collected by the U.S.\nCensus Bureau and POIs distribution from OpenStreetMap [24]\nrespectively. Each region aggregates all the POIs therein to obtain a\ndistribution over different categories. The distance between regions\nis determined in terms of the planar Euclidean distance between\nthe centroids of the regions. Based on the above data processing,\nwe extract features for each region, as a feature vector including\ndemographic information by age, gender, income, etc. and the num-\nber of different categories of POIs, which are normally used for OD\nmatrix generation as in previous works [20, 25, 26, 31].\nCommuting OD Matrix. The OD matrices are constructed\nbased on the commuting flow collected in Longitudinal Employer-\nHousehold Dynamics Origin-Destination Employment Statistics\n(LODES) dataset in 2018. The commuting flows are aggregated into\nthe regions. Each element in the OD matrices records the number\nof workers who are resident in one region and employed in another.\n5.1.2\nBaselines. We compare our proposed DiffODGen with the\nfollowing six baselines that can be categorized into three categories.\nThe first category contains two traditional methods.\nGravity Model (GM) [5]. Motivated by Newton\u2019s law of Gravi-\ntation, GM assumes that the mobility flow is positively related to\nthe populations of origin and destination, and negatively related to\nthe distance between them.\nRandom Forest (RF) [25, 26]. RF is a widely adopted tree-based\nmachine learning model thanks to its robustness, achieving rather\ncompetitive performance in the task of OD flow generation.\nThe second category contains two state-of-the-art (SOTA) deep-\nlearning methods developed for OD flow generation.\nDeep Gravity Model (DGM) [31]. DGM introduces deep neural\nnetworks into the modeling of gravity models to obtain flows by\npredicting the probability distribution to different regions when\nthe outflow is given. We adapt the model to predict volumes of OD\nflow directly, making it to generate the OD matrix for new cities.\nGeo-contextual Multitask Embedding Layer (GMEL) [20].\nGMEL uses graph neural networks (GNNs) to aggregate informa-\ntion from neighbors of each region (node) to capture its spatial\nfeatures in a city (graph), which helps learn better-quality regional\nembeddings to achieve better prediction accuracy.\nFinally, we additionally compare two deep generative models to\ncheck the validity of our design in terms of generating a realistic\nmobility flow network, i.e., the OD matrix.\nNetGAN [7]. NetGAN is a GAN-based model that mimics real\nnetworks by generating random walking sequences with the same\ndistribution as those sampled from the real networks. We adapt it\nto generate directed weighted networks, i.e., OD matrices.\nDenoising Diffusion Probabilistic Model (DDPM) [12]. We\nalso adopt the naive diffusion model that directly learns to generate\nthe OD matrix as a baseline. In this baseline, no distinguishing\ntreatment is assigned to zero elements and non-zero elements in\nthe OD matrix.\n5.1.3\nEvaluation Metrics. We use two kinds of evaluation met-\nrics to verify the generation realism, including three metrics related\nto flow value error and three metrics related to network statistics\nsimilarity. Specifically, the three error-based metrics are Root Mean\nSquare Error (RMSE), Normalized Root Mean Square Error (NRMSE)\nand the commonly adopted Common Part of Commuting (CPC).\n\ud835\udc45\ud835\udc40\ud835\udc46\ud835\udc38=\n\u221a\ufe021\n|F|\n\u2211\ufe01\n\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2208R ||F\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2212\u02c6F\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57||2\n2,\n(14)\n\ud835\udc41\ud835\udc45\ud835\udc40\ud835\udc46\ud835\udc38= \ud835\udc45\ud835\udc40\ud835\udc46\ud835\udc38/\n\u221a\ufe02\n1\n\ud835\udc412\n\u2211\ufe01\n\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2208R ||\ud835\udc39\ud835\udc56\ud835\udc57\u2212\u00af\ud835\udc39\ud835\udc56\ud835\udc57||2\n2,\n(15)\n\ud835\udc36\ud835\udc43\ud835\udc36= 2\n\u2211\ufe01\n\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2208R\nmin(F\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57, \u02c6F\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57)/(\n\u2211\ufe01\n\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2208R\nF\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57+\n\u2211\ufe01\n\ud835\udc5f\ud835\udc56,\ud835\udc5f\ud835\udc57\u2208R\n\u02c6F\ud835\udc56\ud835\udc57), (16)\nwhere the \u00afF denotes the mean. To calculate the statistical similarity\nbetween generated OD matrices and real OD matrices, we adopt\nJensen-Shannon Divergence (JSD) and use it to measure the distance\nbetween distributions of generated data and real data, with respect\nto three typical network statistics, i.e., inflow, outflow and OD flow.\nThe computation of JSD is shown as follow,\n\ud835\udc3d\ud835\udc46\ud835\udc37= KL(PF||P\u02c6F) + KL(P\u02c6F||PF)/2,\n(17)\nwhere KL means Kullback\u2013Leibler divergence and P denotes the\nempirical probability distribution. The inflow and outflow are cal-\nculated by summing all flows into and out of regions, respectively.\n5.1.4\nParameter Settings. The number of layers of graph trans-\nformer in the topology and flow generation phases are 2 and 3\nrespectively, while the numbers of channels are both 64. The dif-\nfusion steps of two diffusion models in our method are 1000, and\nboth use a cosine noise scheduler proposed by Nichol et al [22].\nThe denoising networks in two diffusion models are trained with\nAdam optimizer [17] with a learning rate of 3e-4.\nThe gravity model follows the way in [5] with 4 fitting param-\neters. The \ud835\udc5b_\ud835\udc52\ud835\udc60\ud835\udc61\ud835\udc56\ud835\udc5a\ud835\udc4e\ud835\udc61\ud835\udc5c\ud835\udc5f\ud835\udc60of random forest are 100. The number of\nlayers for DGM is 10. The number of layers in GNN-based models is\n3 and the number of channels is 64. The DDPM baseline follow the\nparameter settings with the flow diffusion model in our method.\n5.2\nOverall Performance\nPerformance comparison. In Table 2 we compare the perfor-\nmance of DiffODGen with baseline methods by training models on\ndata collected in Cook County (in Chicago) and reporting genera-\ntion results in two new cities, i.e., New York City and Seattle. From\nthe results, we have the following findings:\n\u2022 Our proposed cascaded graph denoising diffusion method,\ni.e., DiffODGen, steadily achieves the best performance.\nThe OD matrix generated by DiffODGen achieves the best re-\nalism in terms of both flow value error and network statistics\nsimilarity. Specifically, compared with the best baseline, the flow\nvalue error (RMSE/NRMSE) is reduced by over 3% and 13% in\nNew York City and Seattle, respectively. Moreover, the statistical\nConference\u201917, July 2017, Washington, DC, USA\nCan Rong, Jingtao Ding, Zhicheng Liu, and Yong Li\nTable 2: OD matrix generation results on test cities. The best results are in bold and the next-to-the-best results are underlined.\nTest City\nCook County (|R|=1.3\ud835\udc58) \u2212\u2192New York City (|R|=1.3\ud835\udc58)\nCook County (|R|=1.3\ud835\udc58) \u2212\u2192Seattle (|R|=0.72\ud835\udc58)\nModel\nFlow Value Error\nNetwork Statistics Similarity\nFlow Value Error\nNetwork Statistics Similarity\nRMSE\u2193NRMSE\u2193CPC\u2191JSDinflow\u2193JSDoutflow\u2193JSDODflow\u2193RMSE\u2193NRMSE\u2193CPC\u2191JSDinflow\u2193JSDoutflow\u2193JSDODflow\u2193\nGM\n4.806\n1.066\n0.196\n0.684\n0.399\n0.407\n12.709\n1.021\n0.217\n0.592\n0.370\n0.482\nRF\n5.717\n1.268\n0.361\n0.296\n0.188\n0.054\n22.886\n1.839\n0.390\n0.068\n0.154\n0.028\nDGM\n4.453\n0.987\n0.213\n0.511\n0.598\n0.047\n11.738\n0.943\n0.368\n0.040\n0.366\n0.076\nGMEL\n4.427\n0.981\n0.271\n0.450\n0.656\n0.040\n12.415\n0.997\n0.278\n0.416\n0.333\n0.514\nNetGAN\n4.546\n1.007\n0.290\n0.478\n0.396\n0.040\n12.617\n1.007\n0.297\n0.396\n0.315\n0.426\nDDPM\n4.939\n1.094\n0.290\n0.594\n0.355\n0.063\n13.001\n1.044\n0.379\n0.440\n0.085\n0.092\nDiffODGen\n4.288\n(+3.14%)\n0.950\n(+3.26%)\n0.363\n(+0.60%)\n0.051\n0.064\n0.020\n10.109\n(+13.9%)\n0.812\n(+13.9%)\n0.433\n(+11.0%)\n0.023\n0.029\n0.062\ndistribution similarity between the generated OD matrix and real\none is rather high, indicated by a near-zero value of JSD.\n\u2022 Current deep-learning-based OD flow generation methods\nperform poorly in terms of network statistics similarity.\nAided by the capability of modeling nonlinear relationships be-\ntween city characteristics and flows, deep-learning-based meth-\nods, i.e., DGM and GMEL, achieve next-to-the-best accuracy with\nrespect to flow value error. However, the results of the three JSD\nmetrics indicate that they cannot generate a realistic OD ma-\ntrix regarding its network statistics, due to their independent\nmodeling of each OD matrix element.\n\u2022 Deep generative methods cannot compete with traditional\nmethods without considering the unique characteristics\nof the OD matrix. By comparing NetGAN and DDPM, i.e., two\ngenerative models widely used in other domains, with RF, we\nobserve that the former results have much larger JSD values,\nsuggesting a less similar network generated by NetGAN and\nDDPM. For DDPM in particular, this naive diffusion model is\nhardly comparable to the rather simple GM due to the lack of the\ncascaded diffusion designed for handling city-wide OD matrix\nand the network property augmentation leveraged along the\ndenoising process.\n\u2022 Despite the large deviation between the train and the test\ncity, DiffODGen shows stable performance in terms of gen-\neration realism. From Table 1 we can observe that the deviation\nbetween Seattle and Cook is larger than that between New York\nCity. Nevertheless, DiffODGen exhibits much significant advan-\ntage when tested in Seattle, which is indeed more difficult. This\ndemonstrates its capability of capturing the underlying mech-\nanism behind the observed city-wide OD matrix, owing to its\ngenerative modeling from a network perspective.\nNetwork statistics analysis. We further investigate the realism\nof generated city-wide OD matrix from network perspective. Previ-\nous works on mobility flow networks have empirically observed\nscaling behavior in distributions, i.e., a pow-law-like distribution\n\ud835\udc43(\ud835\udc65) \u223c\ud835\udc65\u2212\ud835\udefc, of node-level inflow [28] and edge-level OD flow [41].\nCorrespondingly, in Figure 3 we calculate probability distributions\nof these two statistics in both generated and real data, finding that\nthe two distributions of inflow and OD flow show a remarkably\ngood match to its power-law fitting, indicated by high values of\n(a) Inflow dist., New York City (\ud835\udc452=0.89)\n(b) OD flow dist., NYC (\ud835\udc452=0.98)\n(c) Inflow dist., Seattle (\ud835\udc452=0.86)\n(d) OD flow dist., Seattle (\ud835\udc452=0.97)\nFigure 3: Distribution of two typical network statistics, i.e.,\ninflow and OD flow, for both generated and real OD matrices\nof New York City and Seattle (The dotted line indicates a\npow-law fitting, with \ud835\udc452 provided).\n\ud835\udc452\u223c0.86-0.98. Note that we normalize two metrics by their means\nfor better visualization. Moreover, as a heavy-tailed distribution like\nthe power law indicates population heterogeneity that is generally\nhard to capture, the results demonstrate that DiffODGen success-\nfully reproduces the network property contained in the mobility\nflow pattern by generating realistic city-wide OD matrices.\n5.3\nTopology Generation Performance\nTo further demonstrate the necessity of leveraging graph generative\nmodeling technique for city-wide OD matrix generation, we inves-\ntigate the network topology generation performance of DiffODGen\nby comparing its first-stage results with other baseline methods in\nboth New York City and Seattle.\nPerformance comparison. We report the performance compari-\nson results with respect to network topology similarity between\nComplexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model\nConference\u201917, July 2017, Washington, DC, USA\nTable 3: Performance comparison of topology generation\nresults in two test cities (NZRflow denotes the relative change\nratio with respect to the original rate of nonzero flows in two\ncities).\nTest City\nNew York City (|R|=1.3\ud835\udc58)\nSeattle (|R|=0.72\ud835\udc58)\nModel\nCPC0/1 NZRflow JSDdegree CPC0/1 NZRflow JSDdegree\nGM\n0.356\n+253.4%\n0.862\n0.585\n+139.0%\n0.815\nRF\n0.476\n+103.6%\n0.486\n0.647\n+43.4%\n0.064\nDGM\n0.334\n\u221272.3%\n0.645\n0.618\n+52.4%\n0.055\nGMEL\n0.395\n+300.0%\n0.842\n0.591\n+143.9%\n0.820\nNetGAN\n0.411\n+281.1%\n0.856\n0.597\n+126.1%\n0.792\nDiffODGen 0.501\n+32.5%\n0.135\n0.630\n-37.7%\n0.040\nTable 4: Further study of the network topology in generated\ncity-wide OD matrices (adjacency matrices).\nTest City\nNew York City (|R|=1.3\ud835\udc58)\nSeattle (|R|=0.72\ud835\udc58)\nModel\nAccuracy FN1\u21920 FP0\u21921 Accuracy FN1\u21920 FP0\u21921\nGM\n0.276\n0.183\n0.900\n0.419\n0.022\n0.984\nRF\n0.605\n0.271\n0.434\n0.644\n0.451\n0.222\nDGM\n0.790\n0.785\n0.022\n0.601\n0.521\n0.229\nGMEL\n0.249\n0.001\n0.995\n0.419\n0.000\n1.000\nNetGAN\n0.334\n0.011\n0.924\n0.422\n0.029\n0.978\nDiffODGen\n0.699\n0.321\n0.345\n0.738\n0.339\n0.370\ngenerated and real OD matrices (here we use the binary version,\ni.e., adjacency matrix) in Table 3. We choose the binary version of\nCPC (CPC0/1), the rate of nonzero flows (NZRflow) and degree distri-\nbution similarity (JSDdegree). First, results in New York City demon-\nstrate the superiority of DiffODGen in reconstructing the most\nrealistic network topology given city characteristics in a new city.\nThe generated mobility flow network achieves the highest CPC0/1\nindicating a good similarity to the real one, and they have close\nnetwork statistics in terms of sparsity (the smallest gap between\nNZRflow) and degree distribution (the lowest JSDdegree). Compara-\ntively, we observe that other baselines cannot reproduce the realistic\nnetwork topology. They tend to generate rather too sparse (DGM)\nor too dense (GMEL) networks. Second, DiffODGen still generates\nthe most realistic network topology in Seattle when evaluating\nthe overall three metrics. Note that RF performs slightly better in\nCPC0/1, which may be attributed to the smaller scale (|R|=0.72\ud835\udc58)\nof Seattle that can be easier for traditional modeling methods.\nFurther study of topology similarity and sparsity. Next we\ninvestigate further the topology similarity and sparsity of generated\nOD matrices, which is motivated by the observations of NZRflow in\nTable 3. We check three additional metrics, i.e., accuracy, false neg-\native rate (predict a real nonzero flow to be zero) and false positive\nrate (predict a real zero flow to be nonzero), and report results in Ta-\nble 4. Note that accuracy is a biased metric as it does not consider the\nunbalanced ratio between nonzero-zero flows in real data. Unlike\nbaselines that overestimate either nonzero flows (GMEL, near-zero\nFN and near-one FP) or zero flows (DGM in New York City, near-\none FN and near-zero FP), the proposed DiffODGen successfully\nachieves a trade-off, indicating the necessity of learning to generate\nrealistic network topology in city-wide OD matrix generation.\n(a) Real flows (NZRflow=24.9%)\n(b) Ours (NZRflow=33.0%)\n(c) DGM (NZRflow=6.9%)\n(d) GMEL (NZRflow=99.6%)\nFigure 4: Spatial flow visualizations of real OD matrix and\ngenerated OD matrices in New York City.\n(a) Flow value error\n(b) Network statistics similarity\nFigure 5: Ablation study.\nFlow visualization. Finally we visualize the spatial distribution of\nboth real OD flows and generated OD flows in New York City (Fig-\nure 4). The results indicate an excellent match to above analysis in\nTable 3 and Table 4. Compared to real flows, the flows generated\nby the proposed DiffODGen is with the highest similarity, while\nthose by DGM and GMEL are either too sparse or too dense.\n5.4\nAblation Study\nNode Properties Augmentation for Topology Generation\n(NAT). We test the effect of node properties augmentation mod-\nule utilized in the topology diffusion model, and the results show\nthat the accurate and realistic generation of topology structure of\nmobility flow network is instrumental. From Figure 5(a), this de-\nsign will bring up to 14.15% performance improvement according\nto CPC, which is the largest in magnitude. From Figure 5(b), the\nperformance of the model on network statistics similarity would\nalso be significantly worse, without this design.\nNode Properties Augmentation for Topology Generation\n(NAF). Accordingly, we examine the utility of node properties\naugmentation applied in the flow diffusion model. The experimental\nresults show that this design also greatly enhances the effect of OD\nmatrix generation with a margin at 9.67%, according to Figure 5(a).\nThis shows that the enhancement of graph transformer is useful\nfrom both topology and flow volume perspectives. From Figure 5(b),\nConference\u201917, July 2017, Washington, DC, USA\nCan Rong, Jingtao Ding, Zhicheng Liu, and Yong Li\nthis design also shows comparable behavior in terms of network\nstatistics similarity..\nCollabrative Training (CoT). By experimenting with just the\ngeneration of the topology diffusion model, we found that collabo-\nrative training can continue to slightly improve performance based\non existing designs according to CPC according to Figure 5.\nALL. This part of the experiment is intended to examine the total\nperformance improvement of our model designs compared to the\ndirect application of the cascaded graph denoising diffusion model.\nFrom the Figure 5, these model designs are very instrumental. In\ncontrast with naive diffusion models, cascading design can well\nsolve the problem of difficult generation of large-scale OD matrix\nand improve the effect with a 20.6% improvement on performance.\n6\nCONCLUSION\nIn this work, we propose to investigate the spatial distribution of\nthe OD matrix, i.e. mobility flows between every two regions, of a\ncity from the perspective of networks, and explore the feasibility of\nintroducing the graph denoising diffusion method to model the joint\ndistribution of all elements in the OD matrix. Specifically, we de-\nsigned a cascaded graph denoising diffusion method (DiffODGen)\nto generate the city-wide OD matrix for the new city by first gener-\nating a topology structure and then mobility flows. By validating in\ntwo real-world data scenarios, the DiffODGen can effectively model\nthe joint distribution of the city-wide OD matrix and generate the\nOD matrix with scaling network behaviours similar with real-world\ndata to improve the precision. This demonstrates the necessity of\nmodeling the joint distribution of all elements in the OD matrix\nand the feasibility of applying graph diffusion models to solve the\nproblem of OD matrix generation.\nREFERENCES\n[1] Muhammad Adnan, Francisco C Pereira, Carlos Miguel Lima Azevedo, Kakali\nBasak, Milan Lovric, Sebasti\u00e1n Raveau, Yi Zhu, Joseph Ferreira, Christopher\nZegras, and Moshe Ben-Akiva. 2016. Simmobility: A multi-scale integrated\nagent-based simulation platform. In 95th Annual Meeting of the Transportation\nResearch Board Forthcoming in Transportation Research Record, Vol. 2. The National\nAcademies of Sciences, Engineering, and Medicine Washington, DC.\n[2] Theo A Arentze and Harry JP Timmermans. 2004. A learning-based transporta-\ntion oriented simulation system. Transportation Research Part B: Methodological\n38, 7 (2004), 613\u2013633.\n[3] Jacob Austin, Daniel D Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den\nBerg. 2021. Structured denoising diffusion models in discrete state-spaces. Ad-\nvances in Neural Information Processing Systems 34 (2021), 17981\u201317993.\n[4] Duygu Balcan, Vittoria Colizza, Bruno Gon\u00e7alves, Hao Hu, Jos\u00e9 J Ramasco, and\nAlessandro Vespignani. 2009. Multiscale mobility networks and the spatial\nspreading of infectious diseases. Proceedings of the national academy of sciences\n106, 51 (2009), 21484\u201321489.\n[5] Hugo Barbosa, Marc Barthelemy, Gourab Ghoshal, Charlotte R James, Maxime\nLenormand, Thomas Louail, Ronaldo Menezes, Jos\u00e9 J Ramasco, Filippo Simini,\nand Marcello Tomasini. 2018. Human mobility: Models and applications. Physics\nReports 734 (2018), 1\u201374.\n[6] Marc Barth\u00e9lemy. 2011. Spatial networks. Physics reports 499, 1-3 (2011), 1\u2013101.\n[7] Aleksandar Bojchevski, Oleksandr Shchur, Daniel Z\u00fcgner, and Stephan G\u00fcn-\nnemann. 2018. Netgan: Generating graphs via random walks. In International\nconference on machine learning. PMLR, 610\u2013619.\n[8] Carlos Caminha and Vasco Furtado. 2017. Impact of human mobility on police\nallocation. In 2017 IEEE International Conference on Intelligence and Security\nInformatics (ISI). IEEE, 125\u2013127.\n[9] Vijay Prakash Dwivedi and Xavier Bresson. 2020. A generalization of transformer\nnetworks to graphs. arXiv preprint arXiv:2012.09699 (2020).\n[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial\nnetworks. Commun. ACM 63, 11 (2020), 139\u2013144.\n[11] Kilian Konstantin Haefeli, Karolis Martinkus, Nathana\u00ebl Perraudin, and Roger\nWattenhofer. 2022. Diffusion Models for Graphs Benefit From Discrete State\nSpaces. arXiv preprint arXiv:2210.01549 (2022).\n[12] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic\nmodels. Advances in Neural Information Processing Systems 33 (2020), 6840\u20136851.\n[13] Jonathan Ho and Tim Salimans. 2022. Classifier-free diffusion guidance. arXiv\npreprint arXiv:2207.12598 (2022).\n[14] Sebastian H\u00f6rl, Felix Becker, and Kay W Axhausen. 2021. Simulation of price,\ncustomer behaviour and system impact for a cost-covering automated taxi system\nin Zurich. Transportation Research Part C: Emerging Technologies 123 (2021),\n102974.\n[15] Jaehyeong Jo, Seul Lee, and Sung Ju Hwang. 2022. Score-based Generative\nModeling of Graphs via the System of Stochastic Differential Equations. arXiv\npreprint arXiv:2202.02514 (2022).\n[16] Hadi Karimi, Seyed-Nader Shetab-Boushehri, and Bahador Ghadirifaraz. 2019.\nSustainable approach to land development opportunities based on both origin-\ndestination matrix and transportation system constraints, case study: Central\nbusiness district of Isfahan, Iran. Sustainable cities and society 45 (2019), 499\u2013507.\n[17] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[18] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes.\narXiv preprint arXiv:1312.6114 (2013).\n[19] Zhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. 2020.\nDiffwave: A versatile diffusion model for audio synthesis.\narXiv preprint\narXiv:2009.09761 (2020).\n[20] Zhicheng Liu, Fabio Miranda, Weiting Xiong, Junyan Yang, Qiao Wang, and\nClaudio Silva. 2020. Learning geo-contextual embeddings for commuting flow\nprediction. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 34.\n808\u2013816.\n[21] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric\nLenssen, Gaurav Rattan, and Martin Grohe. 2019. Weisfeiler and leman go neural:\nHigher-order graph neural networks. In Proceedings of the AAAI conference on\nartificial intelligence, Vol. 33. 4602\u20134609.\n[22] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffu-\nsion probabilistic models. In International Conference on Machine Learning. PMLR,\n8162\u20138171.\n[23] Chenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and\nStefano Ermon. 2020. Permutation invariant graph generation via score-based\ngenerative modeling. In International Conference on Artificial Intelligence and\nStatistics. PMLR, 4474\u20134484.\n[24] OpenStreetMap\ncontributors.\n2017.\nPlanet\ndump\nretrieved\nfrom\nhttps://planet.osm.org . https://www.openstreetmap.org.\n[25] Nastaran Pourebrahim, Selima Sultana, Amirreza Niakanlahiji, and Jean-Claude\nThill. 2019. Trip distribution modeling with Twitter data. Computers, Environment\nand Urban Systems 77 (2019), 101354.\n[26] Caleb Robinson and Bistra Dilkina. 2018. A machine learning approach to\nmodeling human migration. In Proceedings of the 1st ACM SIGCAS Conference on\nComputing and Sustainable Societies. 1\u20138.\n[27] Sijie Ruan, Jie Bao, Yuxuan Liang, Ruiyuan Li, Tianfu He, Chuishi Meng, Yanhua\nLi, Yingcai Wu, and Yu Zheng. 2020. Dynamic public resource allocation based\non human mobility prediction. Proceedings of the ACM on interactive, mobile,\nwearable and ubiquitous technologies 4, 1 (2020), 1\u201322.\n[28] Meead Saberi, Hani S Mahmassani, Dirk Brockmann, and Amir Hosseini. 2017.\nA complex network perspective for characterizing urban travel demand patterns:\ngraph theoretical analysis of large-scale origin\u2013destination demand networks.\nTransportation 44, 6 (2017), 1383\u20131402.\n[29] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily\nDenton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi,\nRapha Gontijo Lopes, et al. 2022. Photorealistic Text-to-Image Diffusion Models\nwith Deep Language Understanding. arXiv preprint arXiv:2205.11487 (2022).\n[30] Hongzhi Shi, Quanming Yao, Qi Guo, Yaguang Li, Lingyu Zhang, Jieping Ye, Yong\nLi, and Yan Liu. 2020. Predicting origin-destination flow via multi-perspective\ngraph convolutional network. In 2020 IEEE 36th International conference on data\nengineering (ICDE). IEEE, 1818\u20131821.\n[31] Filippo Simini, Gianni Barlacchi, Massimilano Luca, and Luca Pappalardo. 2021.\nA Deep Gravity model for mobility flows generation. Nature communications 12,\n1 (2021), 1\u201313.\n[32] Filippo Simini, Marta C Gonz\u00e1lez, Amos Maritan, and Albert-L\u00e1szl\u00f3 Barab\u00e1si.\n2012. A universal model for mobility and migration patterns. Nature 484, 7392\n(2012), 96\u2013100.\n[33] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2020. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502 (2020).\n[34] Yang Song and Stefano Ermon. 2019. Generative modeling by estimating gradients\nof the data distribution. Advances in Neural Information Processing Systems 32\n(2019).\n[35] Yusuke Tashiro, Jiaming Song, Yang Song, and Stefano Ermon. 2021. CSDI:\nConditional score-based diffusion models for probabilistic time series imputation.\nAdvances in Neural Information Processing Systems 34 (2021), 24804\u201324816.\nComplexity-aware Large Scale Origin-Destination Network Generation via Diffusion Model\nConference\u201917, July 2017, Washington, DC, USA\n[36] Petar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017.\nGraph attention networks.\narXiv preprint\narXiv:1710.10903 (2017).\n[37] Clement Vignac, Igor Krawczuk, Antoine Siraudin, Bohan Wang, Volkan Cevher,\nand Pascal Frossard. 2022. DiGress: Discrete Denoising diffusion for graph\ngeneration. arXiv preprint arXiv:2209.14734 (2022).\n[38] Vukan R Vuchic. 2002. Urban public transportation systems. University of\nPennsylvania, Philadelphia, PA, USA 5 (2002), 2532\u20132558.\n[39] Yuandong Wang, Hongzhi Yin, Hongxu Chen, Tianyu Wo, Jie Xu, and Kai Zheng.\n2019. Origin-destination matrix prediction via graph convolution: a new per-\nspective of passenger demand modeling. In Proceedings of the 25th ACM SIGKDD\ninternational conference on knowledge discovery & data mining. 1227\u20131235.\n[40] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks? arXiv preprint arXiv:1810.00826 (2018).\n[41] Xiao-Yong Yan, Wen-Xu Wang, Zi-You Gao, and Ying-Cheng Lai. 2017. Universal\nmodel of individual and population mobility on diverse spatial scales. Nature\ncommunications 8, 1 (2017), 1639.\n[42] Jinwei Zeng, Guozhen Zhang, Can Rong, Jingtao Ding, Jian Yuan, and Yong\nLi. 2022. Causal Learning Empowered OD Prediction for Urban Planning. In\nProceedings of the 31st ACM International Conference on Information & Knowledge\nManagement. 2455\u20132464.\n[43] George Kingsley Zipf. 1946. The P 1 P 2/D hypothesis: on the intercity movement\nof persons. American sociological review 11, 6 (1946), 677\u2013686.\n",
    "2401.08119": "SpecSTG: A Fast Spectral Diffusion Framework\nfor Probabilistic Spatio-Temporal Traffic Forecasting\nLequan Lin1\u2217\u2020 , Dai Shi1\u2020 , Andi Han2 and Junbin Gao1\n1The University of Sydney\n2Riken AIP\n{lequan.lin, dai.shi, junbin.gao}@sydney.edu.au, andi.han@riken.jp\nAbstract\nTraffic forecasting, a crucial application of spatio-\ntemporal graph (STG) learning, has traditionally\nrelied on deterministic models for accurate point\nestimations. Yet, these models fall short of quan-\ntifying future uncertainties. Recently, many prob-\nabilistic methods, especially variants of diffusion\nmodels, have been proposed to fill this gap. How-\never, existing diffusion methods typically deal with\nindividual sensors separately when generating fu-\nture time series, resulting in limited usage of spatial\ninformation in the probabilistic learning process.\nIn this work, we propose SpecSTG, a novel spec-\ntral diffusion framework, to better leverage spatial\ndependencies and systematic patterns inherent in\ntraffic data. More specifically, our method gener-\nates the Fourier representation of future time series,\ntransforming the learning process into the spectral\ndomain enriched with spatial information. Addi-\ntionally, our approach incorporates a fast spectral\ngraph convolution designed for Fourier input, al-\nleviating the computational burden associated with\nexisting models. Compared with state-of-the-arts,\nSpecSTG achieves up to 8% improvements on\npoint estimations and up to 0.78% improvements\non quantifying future uncertainties. Furthermore,\nSpecSTG\u2019s training and validation speed is 3.33\u00d7\nof the most efficient existing diffusion method for\nSTG forecasting. The source code for SpecSTG\nis available at https://anonymous.4open.science/r/\nSpecSTG.\n1\nINTRODUCTION\nTraffic forecasting on road networks is an essential applica-\ntion domain of spatio-temporal graph (STG) learning [Yuan\nand Li(2021); Jin et al.(2023b); Jin et al.(2023a)].\nAs a\ncrucial component of the Intelligence Transportation System\n(ITS), accurate and informative prediction of future traffic dy-\nnamics provides essential guidelines to traffic authorities in\ndecision-making [Lana et al.(2018); Boukerche et al.(2020)].\n\u2217The corresponding author.\n\u2020Equal contributions.\nFigure 1: Illustrations: (a) an example of traffic STG; (b) traffic flow\nforecasting in future 60 minutes with GMAN (deterministic) and\nSpecSTG (probabilistic) on PEMS04.\nSince traffic data, such as vehicle speed and traffic flow, are\ncollected from sensors in the continuous space of road net-\nworks, they present strong spatio-temporal dependencies, es-\npecially at neighbouring locations and time windows [Guo\net al.(2019)]. This naturally leads to their representation as\nSTGs: the traffic network is modelled as a graph, in which\nnodes are sensors and edges are decided with some criteria\nsuch as geographic distances. Hence, temporal records are\nstored as graph signals, while spatial information is encapsu-\nlated in the graph structure. In Figure 1 (a), we provide a\nvisualized example of traffic STG.\nSTG traffic forecasting aims at predicting future values at\nall sensors based on past time series and spatial connections\nin the traffic network. This task has traditionally relied on\ndeterministic models such as DCRNN [Li et al.(2018)] and\nGMAN [Zheng et al.(2020)] to produce accurate point esti-\nmations. Nevertheless, these models may fall short in identi-\nfying unexpected variations that lead to consequential change\nin traffic regulations [Pal et al.(2021); Wen et al.(2023); Hu\net al.(2023)]. This limitation can be overcome by probabilis-\ntic methods, which alternatively approximate the distribution\nof future time series, thus leading to more uncertainty-aware\npredictions. For example, Figure 1 (b) shows the results of\ndeterministic and probabilistic models on the PEMS04 traf-\nfic flow forecasting task [Guo et al.(2019)]. The determinis-\ntic model can only provide point estimations (red), while the\nprobabilistic model is capable of generating both point es-\ntimations and the forecasting interval (blue) which captures\nsome abrupt fluctuations in traffic flow (black box).\narXiv:2401.08119v3  [cs.LG]  6 Aug 2024\nFigure 2: The overview of SpecSTG. Illustrations of TimeGrad and GCRDD are provided to show the novelty and advantage of our approach.\nAmong all the probabilistic models applicable to STG\ntraffic forecasting, we specifically focus on diffusion mod-\nels [Yang et al.(2023); Ho et al.(2020); Lin et al.(2023)].\nClassic diffusion models for time series forecasting, such as\nTimeGrad [Rasul et al.(2021a)], generally follow a forward-\nbackward training process: the generative target (i.e., future\ntime series) is first turned into white noise and then recov-\nered by a learnable backward kernel. The backward kernel is\na conditional distribution which is similar to traditional dif-\nfusion models except for the inclusion of encoded past tem-\nporal information in its conditions. For prediction, samples\nfrom the distribution of future time series are generated by de-\nnoising white noise with the learned backward kernel. Since\nsuch methods are incapable of handling spatial information\nin STGs, diffusion models for STG forecasting, such as Diff-\nSTG [Wen et al.(2023)] and GCRDD [Li et al.(2023)], in-\ncorporate graph structure in the backward kernel condition.\nMore specifically, the backward kernel is usually approxi-\nmated by a denoising network, which takes graph structure\nand other conditions as input to predict noises injected in the\nforward process.\nOur work considers two limitations of existing diffusion\nmethods for STG forecasting.\n(Limitation #1) Although\nthese methods emphasize on the importance of spatial infor-\nmation in STG forecasting, they only use spatial information\nin the backward kernel condition. Consequently, the involve-\nment of spatial information in the overall forward-backward\ndiffusion learning process is limited. (Limitation #2) The\ndenoising network of existing methods usually relies sub-\nstantially on graph convolution to encode spatial information.\nThis often introduces a complexity of O(N 2), quadratic in\nthe number of sensors N in a traffic STG (see Subsection 4.1\nfor more details). Thus the computational cost is high espe-\ncially for large traffic networks, leading to slow training and\nsampling.\nIn this work, we propose a novel spectral diffusion frame-\nwork (SpecSTG), which adopts the graph Fourier represen-\ntation of time series as the generative target. According to\nthe spectral graph theory, the graph Fourier representation is\na measurement of variations in graph signals guided by the\ngraph structure [Chung(1997); Kreuzer et al.(2021)]. In the\ncontext of STGs, we may treat time points as features, thereby\nthe graph Fourier representation can be considered as a new\ntime series of systematic fluctuations enriched by spatial in-\nformation. Hence, Limitation #1 is resolved by generating\nthe Fourier representation rather than the original time se-\nries, transforming the entire diffusion process into the spec-\ntral domain. This effectively leverages the graph structure\nto construct a more comprehensive diffusion base with addi-\ntional systematic and spatial patterns. Besides, with no loss\nof information, the generated data can be converted back to\nthe original domain via the inverse Fourier transform for pre-\ndiction. Limitation #2 is mitigated by replacing the graph\nconvolution with a light-complexity alternative, which only\nworks for the Fourier input (details in Subsection 4.1). An\noverview of SpecSTG can be found in Figure 2. We also pro-\nvide illustrations of TimeGrad and GCRDD for comparison.\nThe contributions of this paper are three-fold:\n(1) To our best knowledge, this is the first work that explores\nprobabilistic STG forecasting on the graph spectral do-\nmain;\n(2) SpecSTG achieves up to 8% improvements on point es-\ntimations and up to 0.78% improvements on generating\ncompatible forecasting intervals.\n(3) SpecSTG\u2019s training and validation speed is 3.33\u00d7 of the\nmost efficient existing diffusion method for STG fore-\ncasting. Additionally, SpecSTG significantly accelerates\nthe sampling process, particularly for large sample sizes.\n2\nPRELIMINARIES\n2.1\nSpatio-Temporal Graphs\nSTGs can be considered as a multidimensional graph repre-\nsentation of entities in a system with time series as graph sig-\nnals. In traffic forecasting, we model sensors as nodes and\nthen create edges based on some spatial relationships such as\ngeographic distances. The average traffic records in obser-\nvation periods are modelled as graph signals. For a traffic\nnetwork with N sensors, the corresponding STG can be de-\nnoted as G{V, E, A}, where V is the set of nodes/sensors, E\nis the set of edges, and A \u2208RN\u00d7N is the adjacency matrix.\nA is assumed to be undirected and can be either weighted\nor unweighted.\nThe graph signals are denoted as XG =\n{x1, x2, ..., xt, ...|xt \u2208RN\u00d7Dx}, where Dx is the number\nof variables. In traffic forecasting, it is common that only one\nvariable such as speed or flow is of interest [Li et al.(2018);\nGuo et al.(2019)], thus we have Dx = 1 such that xt \u2208RN.\n2.2\nSpatio-Temporal Graph Forecasting\nThe objective of STG traffic forecasting is to predict a future\ntime series window Xf = {xt0+1, xt0+2, ..., xt0+f} given\nthe past context window Xc = {xt0\u2212c+1, xt0\u2212c+2, ..., xt0},\nwhere f and c are the length of future and past windows. We\ndenote the combination of past and future time series as X =\n{xt0\u2212c+1, xt0\u2212c+2, ..., xt0+f}. Normally, the target distri-\nbution of generative models depends on the sampling meth-\nods: one-shot methods produce all future predictions together\nfrom the distribution q(Xf|Xc, A) [Wen et al.(2023); Liu\net al.(2023)], while autoregressive methods generate samples\nfrom q(xt|Xt0\u2212c+1:t\u22121, A) for t = t0 + 1, t0 + 2, ..., t0 + f\nsuccessively, where Xt0\u2212c+1:t\u22121 = {xt0\u2212c+1, ..., xt\u22121} [Li\net al.(2023)]. Autoregressive methods often capture the se-\nquential information in consecutive time points more closely.\nHowever, they are associated with higher time costs be-\ncause of the non-parallel step-by-step sampling process. Our\nmethod, SpecSTG, is formulated within the autoregressive\nframework but is equipped with a specially designed spectral\ngraph convolution to mitigate computational inefficiency.\n2.3\nDenoising Diffusion Probabilistic Model\nDenoising diffusion probabilistic models (DDPMs) learn\nhow to generate samples from the target distribution via a\npair of forward-backward Markov chains [Ho et al.(2020);\nYang et al.(2023)].\nAssuming that x0\n\u223cq(x0) is the\noriginal data, for diffusion step k = 0, 1, ..., K, the for-\nward chain injects Gaussian noises to xk until q(xK) :=\nR\nq(xK|x0)q(x0)dx0 \u2248N(xK; 0, I). As a special prop-\nerty, given a noise schedule \u03b2 = {\u03b21, \u03b22, ..., \u03b2K}, we may\ndirectly compute the disturbed data at step k as xk\n=\n\u221a\u02dc\u03b1kx0 + \u221a1 \u2212\u02dc\u03b1k\u03f5, where \u02dc\u03b1k\n=\nQk\ni=1(1 \u2212\u03b2i) and\n\u03f5 \u223cN(0, I). Next, the backward chain denoises from xK\nto recover p\u03b8(x0) through a probabilistic backward kernel\np\u03b8(xk\u22121|xk), where \u03b8 denotes all learnable parameters. In\npractice, the backward kernel is usually optimized with a de-\nnoising network \u03f5\u03b8 by minimizing the loss function\nLDDP M(\u03b8) = Ek,x0,\u03f5\n\r\r\u03f5 \u2212\u03f5\u03b8\n\u0000xk, k\n\u0001\r\r2 ,\n(1)\nwhere \u03f5 \u223cN(0, I) represents the noises injected in the for-\nward diffusion steps. Please refer to Appendix A for more\ndetails on DDPMs.\n3\nSPATIO-TEMPORAL GRAPH\nFOURIER TRANSFORM\nGiven a traffic STG G{V, E, A} with N sensors, the normal-\nized graph Laplacian is computed as L = IN \u2212D\u22121\n2 AD\u22121\n2 ,\nwhere IN\n\u2208\nRN\u00d7N is the identity matrix, and D\n\u2208\nRN\u00d7N is the diagonal degree matrix. We denote the eigen-\ndecomposition of the graph Laplacian as L = U\u039bU \u22ba, where\nU \u2208RN\u00d7N and \u039b \u2208RN\u00d7N are the corresponding eigen-\nvector and eigenvalue matrices, respectively. Considering the\nunivariate graph signal XG = {x1, x2, ..., xt, ...|xt \u2208RN},\nthe Fourier transform for each time point t is given by \u02dcxt =\nU \u22baxt, known as the Fourier representation of xt in the spec-\ntral domain. The Fourier reconstruction is xt = U \u02dcxt. The\northonormal U ensures a lossless reconstruction for the tem-\nporal information. We may compute in matrix form for all\ntime points as \u02dc\nX = U \u22baX and X = U \u02dc\nX. In Appendix B,\nwe also discuss how to naturally extend the method to multi-\nvariate traffic STGs with xt \u2208RN\u00d7Dx, Dx \u22652.\nThe graph Fourier transform can be understood as a pro-\njection of graph signals onto the spectral domain spanned by\nthe eigenvector basis of graph Laplacian. The operator U\nbrings rich positional information for graph signals and offers\na platform to investigate the variations among signals through\na global perspective on the graph. In particular, when the\ninput is a traffic STG, the Fourier representation measures\nhow graph signals (time series values) fluctuate across the\nnetwork. This effectively integrates spatial connectivity into\ntime series, leading to a spatial-aware forecasting paradigm.\n4\nTHE PROPOSED METHOD\nSpecSTG assumes that the Fourier representation of future\ntime series follows the distribution\nq( \u02dc\nX0\nf| \u02dc\nX0\nc , A) \u2248\nt0+f\nY\nt=t0+1\np\u03b8(\u02dcx0\nt|\u02dcht\u22121, A),\n(2)\nwhere \u02dc\nX0\nc and \u02dc\nX0\nf are the noise-free Fourier representations\nof past and future time series, respectively. \u02dcht\u22121 represents\npast spatio-temporal condition encoded by a spectral recur-\nrent encoder. For each time point t = t0 + 1, ..., t0 + f, the\ndiffusion process will learn the corresponding backward ker-\nnel p\u03b8(\u02dcxk\u22121\nt\n|\u02dcxk\nt , \u02dcht\u22121, A) with k = 1, ..., K. The objective\nfunction of SpecSTG is given by\nL(\u03b8) = Et,k,\u02dcx0\nt ,\u03f5t\n\r\r\r\u03f5t \u2212\u03f5\u03b8\n\u0010\n\u02dcxk\nt , k, \u02dcht\u22121, A\n\u0011\r\r\r\n2\n,\n(3)\nwhere t denotes future time points, k denotes diffusion steps,\n\u02dcxk\nt is the disturbed data at time point t and step k, and\n\u03f5t \u223cN(0, I). It is worth noting that t may also start with\nt0 \u2212c+1 instead of t0 +1 to facilitate the learning of autore-\ngressive dependencies by considering both past and future in-\nstances. Lastly, we employ a graph-modified WaveNet archi-\ntecture [van den Oord et al.(2016)] for the denoising network\n\u03f5\u03b8(\u00b7), which is specially designed for Fourier input. In the\nrest of this section, we will focus on details of SpecSTG com-\nponents, training, and inference. The visualization of Spec-\nSTG architecture can be found in Figure 2.\n4.1\nLight-Complexity Spectral Graph Convolution\nDiffusion models for STG forecasting usually rely on graph\nconvolution to encode spatial information in the condition\nof backward kernel [Li et al.(2023); Wen et al.(2023); Liu\net al.(2023)]. Straightforwardly, we adopt the spectral convo-\nlution, which typically transforms graph signals to the spec-\ntral domain and then processes the data via frequency filtering\nbefore they are converted back to the original domain [Def-\nferrard et al.(2016); Kipf and Welling(2016)]. However, with\nSpecSTG, the Fourier representation is already formed as in-\nput, so the transform is no longer required in the convolu-\ntion. In addition, to ensure that the model pipeline flows in\nthe spectral domain throughout the diffusion learning process,\nwe do not apply reconstruction either.\nWe\nchoose\nthe\nChebyshev\nconvolution\n[Defferrard\net al.(2016)], whose filters are formed with Chebyshev poly-\nnomials to accelerate computation as ChebConv(X)\n=\nU PJ\u22121\nj=0 \u03d5jTj( \u02dc\u039b)U \u22baX, for polynomial degrees up to J \u22121,\nwhere Tj( \u02dc\u039b) \u2208RN\u00d7N is the j-th order Chebyshev polyno-\nmial evaluated at \u02dc\u039b = 2\u039b/\u03bbmax \u2212IN with \u03d5j being a learn-\nable coefficient. With Fourier representation \u02dc\nX = U \u22baX as\ninput, we define the modified spectral graph convolution as\nSpecConv( \u02dc\nX) :=\nJ\u22121\nX\nj=0\n\u03d5jTj( \u02dc\u039b) \u02dc\nX.\n(4)\nRemark 1 (Complexity Analysis). Our spectral convolution\nhas only O(N) complexity, contrasting to the O(N 2) com-\nplexity of the classic Chebyshev convolution.\nThis sig-\nnificant improvement endows SpecSTG with the advantage\nof time efficiency over methods using Chebyshev convolu-\ntion, such as GCRDD. Other methods such as DiffSTG may\nuse the graph convolution in [Kipf and Welling(2016)] as\nGCNConv(X) = AX, whose complexity is O(|E|) where\n|E| is the number of edges. As we see in Section 5.1, |E| is\noften much larger than N in traffic STGs.\n4.2\nSpectral Recurrent Encoder\nWe design SG-GRU as the spectral version of Graph GRU\n[Seo et al.(2018)] to encode past time series and spatial in-\nformation in graph Fourier domain as follows.\nFor t =\nt0 \u2212c + 1, ..., t0:\nz = \u03c3(SpecConv(\u02dcx0\nt)Wz1 + SpecConv(\u02dcht\u22121)Wz2)\n(5)\nr = \u03c3(SpecConv(\u02dcx0\nt)Wr1 + SpecConv(\u02dcht\u22121)Wr2)\n(6)\n\u03b6 = tanh(SpecConv(\u02dcx0\nt)W\u03b61 + SpecConv(r \u2299\u02dcht)W\u03b62)\n(7)\n\u02dcht+1 = z \u2299\u02dcht+ (1 \u2212z) \u2299\u03b6,\n(8)\nwhere \u02dch0 \u2208RDh is a vector of zeors with Dh being the\nhidden size, Wz1, Wr1, W\u03b61 \u2208R1\u00d7Dh, Wz2, Wr2, W\u03b62 \u2208\nRDh\u00d7Dh are learnable weights included in \u03b8, and \u03c3 is the\nsigmoid activation function.\nz and r are known as up-\ndate gate and reset gate.\n\u03b6 is the candidate state storing\ncurrent information to be updated in the hidden state.\nIn\nthe implementation, we may also input time features \u0393 =\n{\u03b3t0\u2212c+1, \u03b3t0\u2212c+2, ..., \u03b3t0+f} such as day of week and week\nof month by concatenating them with \u02dc\nX.\n4.3\nDenoising Network: Spectral Graph WaveNet\nIn reminiscent of TimeGrad, we design Spectral Graph\nWaveNet (SG-Wave) as the \u03f5\u03b8 of SpecSTG (see details in\nFigure 3). Besides, we replace some Conv1d layers with\nfully connected linear layers for efficient training and sam-\npling. The network takes disturbed data \u02dcxk\nt , diffusion step k,\nFigure 3: The denoising network \u03f5\u03b8 is a modified WaveNet struc-\nture.\nAlgorithm 1 SpecSTG training\nInput: The distribution of training data after Fourier trans-\nform:\nq({ \u02dc\nXc, \u02dc\nXf}); hidden states:\n\u02dch; number of dif-\nfusion steps:\nK; noise schedule {\u03b21, \u03b22, ..., \u03b2K}, graph:\nG(V, E, A).\nOutput: Optimized denoising network \u03f5\u03b8.\n1: Sample { \u02dc\nXc, \u02dc\nXf} \u223cq({ \u02dc\nXc, \u02dc\nXf})\n2: while Not Convergence do\n3:\nfor t = t0 + 1, t0 + 2, ..., t0 + f do\n4:\nk \u223cUniform(1, K), \u03f5t \u223cN(0, I)\n5:\nCompute \u02dcxk\nt = \u221a\u02dc\u03b1k \u02dcx0\nt + \u221a1 \u2212\u02dc\u03b1k\u03f5t\n6:\nend for\n7:\nTake gradient step on and do gradient descent for \u03b8\n\u2207\u03b8 Et,k,\u02dcx0\nt ,\u03f5t\n\r\r\r\u03f5t \u2212\u03f5\u03b8\n\u0010\n\u02dcxk\nt , k, \u02dcht\u22121, A\n\u0011\r\r\r\n2\n8: end while\nhidden condition \u02dcht\u22121, and graph adjacency A as inputs, and\naims at predicting the noise \u03f5t \u223cN(0, I) at step k for time\npoint t.\n4.4\nTraining and Inference\nWe compute the Fourier representation of all training data\nbefore the training process. Next, we input randomly sam-\npled { \u02dc\nXc, \u02dc\nXf} to SG-GRU\u03b8 to obtain hidden states \u02dch =\n{\u02dcht0, ..., \u02dcht0+f\u22121}.\nDuring training, with a pre-specified\nnoise schedule {\u03b21, \u03b22, ..., \u03b2K}, we randomly sample noise\n\u03f5t \u223cN(0, I) and step k \u223cUniform(0, K) to compute dis-\nturbed data \u02dcxk\nt for t = t0 + 1, t0 + 2, ..., t0 + f . Finally,\nwe take a gradient step on the objective function in Equa-\ntion (3). The training algorithm is provided in Algorithm\n1. With the denoising network, we can generate samples and\nmake predictions for the forecasting task. The generation pro-\ncess adopts autoregressive sampling, which means we gener-\nate samples for each time point one by one. For example,\nafter we generate samples for t = t0 + 1, we feed the sample\nmean back to the SG-GRU module to compute \u02dcht0+1, and\nAlgorithm 2 SpecSTG sampling and prediction for x0\nt\nInput: Hidden state: \u02dcht\u22121; Fourier operator U; number of\nsamples: S; variance hyperparameter: \u03c3k.\nOutput: Prediction \u02c6x0\nt.\n1: Randomly generate S samples {\u02dcxK\nt,s}S\ns=1 \u223cN(0, I) and\ndo the follows in parallel for all s.\n2: for k = K, K \u22121, ..., 1 do\n3:\ne = 0 if k = 1 else e \u223cN(0, I)\n4:\nCompute and update \u02dcxk\u22121\nt,s\nas\n1\n\u221a1\u2212\u03b2k\n\u0000\u02dcxk\nt,s\u2212\n\u03b2k\n\u221a1\u2212\u02dc\u03b1k\n\u03f5\u03b8(\u02dcxk\nt,s, k, \u02dcht\u22121, A)\n\u0001\n+ \u03c3ke\n5: end for\n6: Take average on S samples \u02dcx0\nt = 1\nS\nPS\ns=1 \u02dcx0\nt,s\n7: Compute predictions \u02c6x0\nt = U \u02dcx0\nt\nthen use it to generate samples for t = t0 +2. Lastly, we con-\nvert the predictions back to the original domain via Fourier\nreconstruction. The sampling and prediction algorithm for a\none-time point x0\nt is presented in Algorithm 2.\n5\nEXPERIMENTS\n5.1\nDatasets and Baseline Models\nWe validate our model on two traffic datasets, PEMS04\nand PEMS08 [Guo et al.(2019)], collected by the California\nTransportation Agencies\u2019 (CalTrans) Performance Measure-\nment System (PEMS) [Chen et al.(2001)]. PEMS04 com-\nprises traffic records from 307 sensors in California\u2019s District\n04 from Jan 1st to Feb 28th, 2018, while PEMS08 includes\ndata from 170 sensors in District 08 from July 1st to 31st Au-\ngust 2018. Our experiments focus on traffic flow and speed\navailable in both datasets, denoted as \u201cF\u201d and \u201cS\u201d respec-\ntively. For instance, \u201cPEMS04F\u201d denotes traffic flow records\nin the PEMS04 dataset. Each time point covers 5 minutes,\nwith the corresponding value representing the average records\nduring that interval. The speed data are continuous, allow-\ning us to introduce random Gaussian noises. Although traffic\nflow (i.e., the number of vehicles) is a discrete variable, we\nstill treat it as continuous considering the fact that it contains\nnumerous unique values. More details about the datasets can\nbe found in Table 1.\nTable 1: Dataset details.\nDataset\nType\n#Nodes\n#Edges\n#Time points\nPEMS04F\nFlow\n307\n680\n16992\nPEMS04S\nSpeed\nPEMS08F\nFlow\n170\n548\n17856\nPEMS08S\nSpeed\nSix probabilistic baselines are considered in our exper-\niments, including four diffusion methods: TimeGrad [Ra-\nsul et al.(2021a)], GCRDD [Li et al.(2023)], DiffSTG [Wen\net al.(2023)], and PriSTI [Liu et al.(2023)]; and two non-\ndiffusion methods, DeepVAR [Salinas et al.(2020)] and\nTransNVP [Rasul et al.(2021b)]. All baselines are state-of-\nthe-art diffusion models proposed in recent years. For more\ndiscussions on the baseline models, please see Appendix C.\n5.2\nMetrics\nWe use three metrics to evaluate the performance of Spec-\nSTG, including two deterministic metrics, Root Mean Squred\nError (RMSE) and Mean Absolute Error (MAE), and one\nprobabilistic metric, Continuous Ranked Probability Score\n(CRPS). RMSE and MAE are adopted to measure the dis-\ntance between predictions and the ground truth. We use the\nmean of generated samples as predictions to calculate RMSE\nand MAE. Given predictions \u02c6\nXf at time t0 (after the Fourier\nreconstruction) and ground truth Xf of one future window,\nthe formulas can be written as:\nRMSE( \u02c6\nXf, Xf) =\nv\nu\nu\nt 1\nf\nt0+f\nX\nt=t0+1\n(xt \u2212\u02c6xt)2,\n(9)\nMAE( \u02c6\nXf, Xf) = 1\nf\nt0+f\nX\nt=t0+1\n|xt \u2212\u02c6xt|.\n(10)\nThe final results reported in our experiments are the averages\nfrom all available predictive windows in the test set. CRPS is\na probabilistic metric that measures the compatibility of the\nlearned probabilistic distribution at each observation [Math-\neson and Winkler(1976)]. Given the cumulative distribution\nfunction (CDF) F of the distribution estimated at observation\nx, CRPS is defined as\nCRPS(F \u22121, x) =\nZ 1\n0\n2\n\u0000\u03ba \u2212Ix<F \u22121(\u03ba)\n\u0001\u0000x \u2212F \u22121(\u03ba)\n\u0001\nd\u03ba,\n(11)\nwhere \u03ba \u2208[0, 1], F \u22121 is the quantile function, and Ix<F \u22121(\u03ba)\nis an indicator function which equals to 1 when x < F \u22121(\u03ba)\nand 0 otherwise. To calculate the integral, we use 100 sam-\nples generated at each time point and sensor/node to approx-\nimate the corresponding distribution and calculate CRPS fol-\nlowing the way defined in [Tashiro et al.(2021)]. For each\nfuture window, we may compute the normalized CRPS at\ntime point t = t0 + 1, ..., t0 + f as\nP\nn CRPS(F \u22121\nt,n,xt,n)\nP\nn |xt,n|\n,\nwhere n = 1, ..., N denotes each sensor/node, and xt,n is\nthe value of sensor/node n at time t. Likewise, the \u201cCRPS\nAvg.\u201d is computed as\nP\nt,n CRPS(F \u22121\nt,n,xt,n)\nP\nt,n |xt,n|\n. We do not adopt\nCRPSsum\n[Rasul et al.(2021a); Tashiro et al.(2021)] as a\nmetric because sensors are not regarded as features in our ex-\nperiments. The results reported for CRPS are also averages\nover all available predictive windows in the test set.\n5.3\nImplementation Details\nWe implement SpecSTG on a single NVIDIA 4090 GPU with\n64GB of memory. The model is trained with the Adam op-\ntimizer with a learning rate schedule from 5e \u22124 to 1e \u22122.\nThe maximum number of epochs is 300 for flow data and 50\nfor speed data with batch size 64. Validation loss is used for\nTable 2: The results of traffic forecasting experiments in a future window of 60 minutes. Average RMSE, MAE, CRPS, and their point values\nat 15/30/60 minutes are reported. Lower values indicate better forecasting performance. The best results are marked in bold and the second\nbest results are underlined. Improvements of SpecSTG on existing methods are shown in percentage.\nRMSE\nMAE\nCRPS\nModels\nAvg.\n15min\n30min\n60min\nAvg.\n15min\n30min\n60min\nAvg.\n15min\n30min\n60min\nPEMS04F\nDeepVAR\n50.59\n43.90\n48.76\n60.46\n37.74\n32.97\n36.72\n45.87\n0.2094\n0.1997\n0.2108\n0.2209\nTransNVP\n82.74\n68.26\n81.70\n99.81\n61.85\n53.06\n62.25\n73.49\n0.2359\n0.2008\n0.2377\n0.2819\nTimeGrad\n35.58\n33.22\n35.24\n38.95\n21.70\n20.26\n21.56\n24.04\n0.0801\n0.0747\n0.0795\n0.0887\nGCRDD\n36.28\n31.94\n45.31\n41.99\n22.16\n19.48\n21.87\n26.18\n0.0779\n0.0689\n0.0768\n0.0982\nDiffSTG\n37.62\n34.99\n36.68\n43.04\n24.90\n22.53\n24.65\n29.24\n0.0904\n0.0815\n0.0894\n0.1077\nPriSTI\n33.74\n33.56\n33.71\n37.31\n22.46\n21.65\n22.32\n25.19\n0.0772\n0.0751\n0.0764\n0.0870\nSpecSTG\n33.15\n30.07\n32.81\n37.29\n21.53\n19.29\n21.39\n23.29\n0.0766\n0.0683\n0.0761\n0.0866\nImprove.\n1.75%\n5.86%\n2.67%\n0.54%\n0.78%\n0.98%\n0.79%\n3.12%\n0.78%\n0.87%\n0.39%\n0.46%\nPEMS08F\nDeepVAR\n41.43\n35.83\n39.88\n49.41\n27.86\n23.19\n27.03\n34.89\n0.1291\n0.1219\n0.1269\n0.1412\nTransNVP\n67.69\n60.48\n68.48\n76.16\n51.37\n49.08\n52.31\n58.38\n0.1802\n0.1601\n0.1822\n0.2073\nTimeGrad\n33.09\n30.17\n32.53\n37.51\n20.47\n18.24\n20.06\n24.24\n0.0705\n0.0618\n0.0695\n0.0843\nGCRDD\n28.83\n23.91\n28.10\n35.68\n18.72\n15.52\n18.35\n23.99\n0.0626\n0.0517\n0.0617\n0.0833\nDiffSTG\n28.26\n25.04\n27.54\n34.32\n18.99\n16.66\n18.63\n23.68\n0.0692\n0.0609\n0.0679\n0.0872\nPriSTI\n26.35\n24.58\n26.93\n29.91\n17.30\n15.98\n17.32\n20.67\n0.0576\n0.0539\n0.0581\n0.0688\nSpecSTG\n25.59\n22.23\n24.77\n29.90\n17.06\n14.93\n16.70\n20.25\n0.0572\n0.0500\n0.0558\n0.0680\nImprove.\n2.88%\n7.03%\n8.02%\n0.03%\n1.39%\n3.80%\n3.58%\n2.03%\n0.69%\n3.29%\n3.96%\n1.16%\nPEMS04S\nDeepVAR\n6.23\n5.72\n6.11\n6.93\n2.76\n2.52\n2.72\n3.13\n0.0490\n0.0450\n0.0488\n0.0549\nTransNVP\n6.25\n5.73\n6.27\n6.98\n3.36\n3.12\n3.39\n3.74\n0.0408\n0.0382\n0.0412\n0.0446\nTimeGrad\n5.92\n5.62\n5.91\n6.35\n2.38\n2.19\n2.37\n2.66\n0.0307\n0.0282\n0.0308\n0.0345\nGCRDD\n4.33\n3.10\n4.30\n5.63\n1.94\n1.51\n1.97\n2.58\n0.0245\n0.0189\n0.0248\n0.0329\nDiffSTG\n4.46\n3.24\n4.46\n5.72\n2.15\n1.66\n2.20\n2.83\n0.0264\n0.0206\n0.0267\n0.0340\nPriSTI\n4.42\n3.31\n4.67\n5.60\n1.96\n1.54\n1.99\n2.62\n0.0252\n0.0198\n0.0258\n0.0329\nSpecSTG\n4.06\n3.01\n4.09\n5.15\n1.93\n1.50\n1.97\n2.51\n0.0245\n0.0192\n0.0253\n0.0319\nImprove.\n6.24%\n2.90%\n4.88%\n8.04%\n0.52%\n0.66%\n0.00%\n2.71%\n0.00%\n-\n-\n3.04%\nPEMS08S\nDeepVAR\n5.73\n5.55\n5.70\n6.05\n2.56\n2.42\n2.57\n2.79\n0.0544\n0.0534\n0.0543\n0.0558\nTransNVP\n5.41\n5.12\n5.54\n5.74\n2.76\n2.64\n2.83\n2.91\n0.0349\n0.0334\n0.0358\n0.0368\nTimeGrad\n4.98\n4.93\n4.97\n5.03\n1.98\n1.95\n1.97\n2.12\n0.0267\n0.0262\n0.0268\n0.0272\nGCRDD\n3.75\n2.74\n3.77\n4.89\n1.72\n1.35\n1.75\n2.32\n0.0223\n0.0171\n0.0226\n0.0301\nDiffSTG\n3.97\n3.20\n4.07\n4.82\n2.36\n1.91\n2.43\n3.04\n0.0325\n0.0259\n0.0335\n0.0423\nPriSTI\n4.22\n3.02\n4.39\n5.20\n1.70\n1.29\n1.80\n2.15\n0.0217\n0.0162\n0.0230\n0.0272\nSpecSTG\n3.45\n2.58\n3.46\n4.36\n1.63\n1.27\n1.67\n2.02\n0.0217\n0.0170\n0.0219\n0.0268\nImprove.\n8.00%\n5.84%\n8.22%\n9.54%\n4.12%\n1.55%\n4.57%\n4.72%\n0.00%\n-\n0.10%\n1.47%\nmodel selection. The hyperparameters specific to diffusion\nmodels are set as follows. We use the quadratic scheme for\nnoise level \u03b2k starting from \u03b21 = 1e \u22124 and tune \u03b2K in\n[0.1, 0.2, 0.3, 0.4]. The number of diffusion steps K is se-\nlected from [50, 100, 200]. The maximum polynomial order\nin SpecConv is set as 2. The hidden size Dh is tuned in\n[64, 96]. In SG-Wave, the number of residual blocks M = 8\nand the residual channel Dr = 8. Finally, the number of sam-\nples S is set as 100 for all models. For all experiments, we\nsplit datasets with 60%/20%/20% train/validation/test pro-\nportions and apply Z-score normalization before the Fourier\ntransform. The graph structure is constructed depending on\nthe distance between sensors following [Guo et al.(2019)].\nMore implementation details are presented in Appendix D.\n5.4\nExperiment Results\nTable 2 presents the results of our traffic forecasting experi-\nments, where the prediction task involves forecasting future\ntime series for a 60-minute horizon based on observations\nfrom the past 60 minutes. The table includes numerical values\nfor the average RMSE, MAE, and CRPS over the entire fore-\ncast window. Additionally, it provides corresponding point\nevaluations assessed at 15, 30, and 60 minutes such that we\ncan evaluate the short and long-term forecasting performance\nof the models.\nAnalysis of deterministic results SpecSTG consistently\nachieves top-tier deterministic results across various tasks.\nIn comparison to the second-best model, SpecSTG exhibits\nan 8.00% improvement in average RMSE for PEMS08S and\na 6.24% improvement for PEMS04S. Similarly, the aver-\nage MAE sees enhancements of 4.12% for PEMS08S and\n1.39% for PEMS08F. In addition, our method shows profi-\nciency in both short and long-term deterministic forecasting.\nParticularly, the RMSE improvement of SpecSTG achieves\n7.03% on PEMS08F for short-term 15-minute forecasting\nand 9.54% on PEMS08S for long-term 60-minute forecast-\ning. We also observe improvements in MAE for most re-\nsults in the table, especially for traffic flow tasks. The supe-\nrior deterministic performance stems from SpecSTG\u2019s unique\nprobabilistic learning process in the spectral domain, lever-\nFigure 4: Forecasting visualizations of TimeGrad (green), GCRDD (blue), and SpecSTG (red). (a) and (b) are results on speed data\n(PEMS04S), while (c) presents results on flow data (PEMS04F).\naging rich global spatial information, which is an acknowl-\nedged crucial component in deterministic traffic forecasting\n[Yu et al.(2018); Fang et al.(2019)].\nAnalysis of probabilistic results Regarding the probabilistic\nmetric CRPS, SpecSTG demonstrates an advantage in traf-\nfic flow forecasting but not in vehicle speed forecasting. The\naverage CRPS is improved by 0.78% and 0.69% with Spec-\nSTG on PEMS04F and PEMS08F, respectively. However,\nlittle improvement is observed on PEMS04S and PEMS08S.\nSince speed and flow datasets share the same graph structure\n(e.g., PEMS04S and PEMS04F), we suggest that the inferior\nprobabilistic performance is related to the time series data.\nA possible explanation is that flow data is often associated\nwith higher variations, thus the systematic variations mea-\nsured by the Fourier representation are more informative than\nthe speed data. For instance, the standard deviations of data in\nPEMS08S and PEMS08F are 6.65 and 146.22, respectively.\nWe will further investigate this observation with forecasting\nvisualizations in Subsection 6.1.\nComments on baseline models Non-diffusion models such\nas DeepVAR and TransNVP clearly show inferior perfor-\nmance compared to diffusion models. PriSTI excels in pre-\ndicting traffic flow, leveraging the attention mechanism to\nprocess temporal dynamics. On the other hand, TimeGrad\nand GCRDD exhibit comparably better performance in fore-\ncasting vehicle speeds, utilizing a recurrent structure that em-\nphasizes consecutive temporal connectivity. We suppose this\ndistinction is caused by the attention mechanism\u2019s effective-\nness in capturing global temporal variations and the recur-\nrent structure\u2019s aptness for learning highly correlated speed\npatterns. Our approach, with a recurrent encoder, combines\nthe strengths of both methodologies. It preserves the ability\nto learn continuous time patterns while enhancing the iden-\ntification of variations through the incorporation of spectral\nvariation measurements.\n6\nDISCUSSIONS\nIn this section, we will visualize SpecSTG\u2019s forecasting out-\ncomes compare with two baseline diffusion models. In ad-\ndition, we will provide analyses on time efficiency and sen-\nsitivity to hyperparameters to further show the advantage of\nSpecSTG. More supplementary discussions can be found in\nAppendices E, F, and G.\n6.1\nForecasting Visualizations\nRecall that in Subsection 5.4, SpecSTG performs better in\nprobabilistic forecasting on traffic flow data than speed data.\nHere we further explore this observation by visualizing the\nforecasting outcomes of TimeGrad, GCRDD, and SpecSTG\non PEMS04S and PMES04F (Figure 4). The figure displays\nthe mean and 95% confidence interval (adjusted by time) of\nestimated future distributions. Our primary objective is to\nassess whether the forecasting intervals produced by various\nmethods are compatible with the actual future time series. An\nappropriate interval should capture the future variations while\nremaining sufficiently narrow to provide meaningful insights.\nIn traffic speed forecasting, SpecSTG\u2019s mean estimation\nis closer to future time series, but the intervals generated by\nTimeGrad and GCRDD sometimes better fit the variations in\nfuture values. Upon closer examination of the data patterns,\nwe observe that this impact is particularly pronounced in win-\ndows with very small variations (Figure 4 (a)). In contrast,\ndistributions estimated by SpecSTG at sensors with larger\nvariations (Figure 4 (b)) exhibit a better ability to capture\nFigure 5: Time efficiency of training, validation, and sampling.\nfuture uncertainty compared to TimeGrad and GCRDD. This\nmatches our previous hypothesis in Subsection 5.4 that STG\nforecasting with larger variations benefits more from the\nspectral diffusion process. Because more systematic fluc-\ntuations exist in such data, and thus more information can be\ncaptured by the Fourier representation, and eventually learned\nby the diffusion process.\nAnalogously, in traffic flow forecasting on PEMS04F, a\ndataset characterized by high systematic variation, Spec-\nSTG demonstrates promising performance by generating\nboth more accurate deterministic predictions and more com-\npatible distributions (Figure 4 (c)). This observation reflects\nthe experiment results that SpecSTG achieves outstanding\nperformance with PEMS04F in terms of all metrics.\n6.2\nTime Efficiency\nIn Figure 5, we compare the training, validation, and sam-\npling time of SpecSTG with three diffusion models for STG\nforecasting, including GCRDD, DiffSTG, and PriSTI. The\ntraining and validation of SpecSTG are clearly faster than\nother diffusion baselines. Particularly, SpecSTG\u2019s training\nplus validation time is 3.33\u00d7 of GCRDD, the most efficient\nmethod among other existing state-of-the-arts. The valida-\ntion time of DiffSTG is significantly high because it requires\nsampling and prediction during validation.\nTo show sam-\npling efficiency, we plot the sampling time per observation\n(i.e., a future window of 60 minutes) when diffusion steps\nK = 50, 100, 200. We set the batch size of one-shot methods,\nDiffSTG and PriSTI, as 8 and 16, and report the best results.\nFor autoregressive methods, SpecSTG shows a notable time\nadvantage over GCRDD in sampling. Besides, their sampling\ntime does not vary much with the number of samples S. By\ncontrast, the time cost of one-shot methods increases rapidly\nwith the increase of S. Although DiffSTG and PriSTI are\nmore efficient when S is small, a small number of samples\noften cannot present a clear picture of future data distribution.\nFigure 6: Sensitivity analysis of SpecSTG on key hyperparameters:\ndiffusion steps K and the end of beta schedule \u03b2K.\n6.3\nSensitivity Analysis on Hyperparameters\nIn this sensitivity analysis, we focus on the combination of\ntwo very important diffusion hyperparameters: the number of\ndiffusion steps, K, and the end of noise schedule, \u03b2K. Theo-\nretically, the choices of K and \u03b2K are relevant to each other.\nSince the noise level gradually increases from \u03b21 = 1e \u22124\nto \u03b2K in K diffusion steps, these two hyperparameters con-\ntrol the changing speed and level of noises in the diffusion\nprocess, which is essential for the white noise assumption of\ndiffusion models. The same as the implementation of Spec-\nSTG in our experiments, we set the search spaces of \u03b2K and\nK as [0.1, 0.2, 0.3, 0.4] and [50, 100, 200], respectively. In\nFigure 6, we use heatmaps to show the change in model per-\nformance in terms of RMSE, MAE, and CRPS on PEMS04S\nwith different hyperparameter combinations. We observe that\nSpecSTG typically performs better with a larger \u03b2K (for in-\nstance 0.3 or 0.4). The best results appear when \u03b2K = 0.3\nand K = 50. It is also worth noting that the forecasting per-\nformance of SpecSTG does not vary dramatically with differ-\nent hyperparameter combinations. This means our method is\nnot very sensitive to hyperparameter selection, alleviating the\nburden of hyperparameter tuning.\n7\nRELATED WORKS\nGenerative Diffusion Models The initial idea of diffusion\nmodels was introduced by [Sohl-Dickstein et al.(2015)].\nThen, some improvements proposed by [Ho et al.(2020)] en-\ndowed them with remarkable practical value, contributing\nto their conspicuous popularity nowadays. In recent years,\ndiffusion models have demonstrated their power over many\nexisting generative techniques in various real-world applica-\ntions such as image synthesis [Austin et al.(2021); Dhari-\nwal and Nichol(2021); Ho et al.(2022a)], video generation\n[Harvey et al.(2022); Ho et al.(2022b); Yang et al.(2022)],\nnatural language processing [Li et al.(2022b); Nikolay\net al.(2022); Yu et al.(2022)], and time series predic-\ntion [Rasul et al.(2021a); Li et al.(2022a); Alcaraz and\nStrodthoff(2023)].\nDiffusion Models for Time Series and STGs Pioneering\ndiffusion models for time series such as TimeGrad [Rasul\net al.(2021a)] and TimeDiff [Shen and Kwok(2023)] were\noriginally tailored for multivariate time series forecasting, uti-\nlizing sensors as variables but only exploring general depen-\ndencies without incorporating graph structural information.\nRecently, several diffusion models were proposed specifi-\ncally for STG forecasting. DiffSTG [Wen et al.(2023)] in-\ncorporates graph structure into the backward kernel with\na graph-modified Unet [Ronneberger et al.(2015)] architec-\nture. GCRDD [Li et al.(2023)], designed in reminiscent of\nTimeGrad, adopts a graph-enhanced recurrent encoder to pro-\nduce hidden states from past time series as conditions. Addi-\ntionally, USTD [Hu et al.(2023)] introduces a pre-trained en-\ncoder that better captures deterministic patterns via an unsu-\npervised reconstruction task. DVGNN [Liang et al.(2023)] is\na deterministic model but with a diffusion module to generate\ndynamic adjacency matrices in its pre-training process. Fur-\nthermore, PriSTI [Liu et al.(2023)] was initially developed\nfrom CSDI [Tashiro et al.(2021)] for STG imputation, but\nwith potential for forecasting tasks by masking future data as\nmissing values. We highlight that SpecSTG\u2019s novelty lies in\nits unique spectral diffusion framework that generates graph\nFourier representation of future time series, which leverages\nsystematic fluctuations in time series data guided by graph\nstructure to boost forecasting accuracy.\nSpectral diffusion on graphs and time series The idea of\nspectral diffusion has been applied in generating graph struc-\nture and classic time series. GSDM [Luo et al.(2023)] ex-\nplores the generation of spectral graph structure, i.e., the\neigenvalues of graph adjacency matrices, to enhance graph\ngeneration quality. Besides, research has shown that generat-\ning classic time series in the Fourier domain facilitates diffu-\nsion models to better capture the training distribution [Crabb\u00b4e\net al.(2024)]. Our method, SpecSTG, is the first endeavour to\ninvestigate spectral diffusion in generating graph signals and\nspatio-temporal data.\n8\nCONCLUDING REMARK\nIn this paper, we proposed SpecSTG, a spectral diffusion ap-\nproach for fast probabilistic spatio-temporal traffic forecast-\ning. Our method transforms the entire diffusion learning pro-\ncess to the spectral domain by generating the Fourier repre-\nsentation of future time series instead of the original data.\nAlthough we have introduced the autoregressive architecture\nof SpecSTG, the idea of spectral diffusion can be straight-\nforwardly applied to one-shot methods as well by altering\nthe generative target and graph convolution. Hence, Spec-\nSTG can be regarded as an effective framework for STG fore-\ncasting. Experiment results confirm the superior performance\nof SpecSTG, demonstrating more efficient training and sam-\npling compared to state-of-the-art diffusion methods. Never-\ntheless, we highlight that SpecSTG may fall short of predict-\ning compatible future distributions when the data have low\nvariations, diminishing the efficacy of spectral measurements\nof systematic fluctuations.\nReferences\n[Alcaraz and Strodthoff(2023)] Juan Lopez Alcaraz and Nils\nStrodthoff. 2023. Diffusion-based Time Series Imputa-\ntion and Forecasting with Structured State Space Mod-\nels.\nTransactions on Machine Learning Research 3\n(2023).\n[Austin et al.(2021)] Jacob\nAustin,\nDaniel\nD\nJohnson,\nJonathan Ho, Daniel Tarlow, and Rianne van den Berg.\n2021. Structured denoising diffusion models in discrete\nstate-spaces. Advances in Neural Information Process-\ning Systems 34 (2021), 17981\u201317993.\n[Boukerche et al.(2020)] Azzedine Boukerche, Yanjie Tao,\nand Peng Sun. 2020. Artificial intelligence-based vehic-\nular traffic flow prediction methods for supporting intel-\nligent transportation systems. Computer Networks 182\n(2020), 107484.\n[Chen et al.(2001)] Chao Chen, Karl Petty, Alexander Sk-\nabardonis, Pravin Varaiya, and Zhanfeng Jia. 2001.\nFreeway performance measurement system:\nmining\nloop detector data.\nTransportation Research Record\n1748, 1 (2001), 96\u2013102.\n[Chung(1997)] Fan RK Chung. 1997. Spectral graph theory.\nVol. 92. American Mathematical Society.\n[Coletta et al.(2023)] A. Coletta, S. Gopalakrishan, D. Bor-\nrajo, and S. Vyetrenko. 2023. On the Constrained Time-\nSeries Generation Problem. arXiv preprint 2307.01717\n(2023).\n[Crabb\u00b4e et al.(2024)] Jonathan Crabb\u00b4e, Nicolas Huynh, Jan\nStanczuk, and Mihaela van der Schaar. 2024. Time se-\nries diffusion in the frequency domain. International\nConference on Machine Learning (ICML) (2024).\n[Defferrard et al.(2016)] Micha\u00a8el Defferrard, Xavier Bres-\nson, and Pierre Vandergheynst. 2016.\nConvolutional\nneural networks on graphs with fast localized spectral\nfiltering. In Advances in Neural Information Processing\nSystems (NeurIPS), Vol. 29.\n[Dhariwal and Nichol(2021)] Prafulla Dhariwal and Alexan-\nder Nichol. 2021. Diffusion models beat GANs on im-\nage synthesis. In Advances in Neural Information Pro-\ncessing Systems, Vol. 34. 8780\u20138794.\n[Dinh et al.(2017)] Laurent Dinh, Jascha Sohl-Dickstein,\nand Samy Bengio. 2017. Density estimation using Real\nNVP. In International Conference on Learning Repre-\nsentations (ICLR).\n[Fang et al.(2019)] Shen Fang, Qi Zhang, Gaofeng Meng,\nShiming Xiang, and Chunhong Pan. 2019.\nGSTNet:\nGlobal Spatial-Temporal Network for Traffic Flow Pre-\ndiction.. In International Joint Conference on Artificial\nIntelligence (IJCAI). 2286\u20132293.\n[Guo et al.(2019)] Shengnan Guo, Youfang Lin, Ning Feng,\nChao Song, and Huaiyu Wan. 2019. Attention based\nspatial-temporal graph convolutional networks for traf-\nfic flow forecasting. In Proceedings of the AAAI Confer-\nence on Artificial Intelligence, Vol. 33. 922\u2013929.\n[Harvey et al.(2022)] William Harvey, Saeid Naderiparizi,\nVaden Masrani, Christian Weilbach, and Frank Wood.\n2022. Flexible diffusion modeling of long videos. In\nAdvances in Neural Information Processing Systems,\nVol. 35. 27953\u201327965.\n[Ho et al.(2020)] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\n2020.\nDenoising diffusion probabilistic models. In\nAdvances in Neural Information Processing Systems\n(NeurIPS), Vol. 33. 6840\u20136851.\n[Ho et al.(2022a)] Jonathan Ho, Chitwan Saharia, William\nChan, David J Fleet, Mohammad Norouzi, and Tim Sal-\nimans. 2022a. Cascaded Diffusion Models for High Fi-\ndelity Image Generation. Journal of Machine Learning\nResearch 23, 47 (2022), 1\u201333.\n[Ho et al.(2022b)] Jonathan Ho,\nTim Salimans,\nAlexey\nGritsenko, William Chan, Mohammad Norouzi, and\nDavid J. Fleet. 2022b.\nVideo Diffusion Models. In\nAdvances in Neural Information Processing Systems,\nVol. 35. PMLR, 8633\u20138646.\n[Hu et al.(2023)] Junfeng Hu, Xu Liu, Zhencheng Fan, Yux-\nuan Liang, and Roger Zimmermann. 2023.\nTowards\nUnifying Diffusion Models for Probabilistic Spatio-\nTemporal Graph Learning. arXiv:2310.17360 (2023).\n[Jin et al.(2023b)] Guangyin Jin, Yuxuan Liang, Yuchen\nFang, Zezhi Shao, Jincai Huang, Junbo Zhang, and Yu\nZheng. 2023b. Spatio-temporal graph neural networks\nfor predictive learning in urban computing: A survey.\nIEEE Transactions on Knowledge and Data Engineer-\ning (2023).\n[Jin et al.(2023a)] Ming Jin, Huan Yee Koh, Qingsong Wen,\nDaniele Zambon, Cesare Alippi, Geoffrey I Webb, Irwin\nKing, and Shirui Pan. 2023a. A survey on graph neu-\nral networks for time series: Forecasting, classification,\nimputation, and anomaly detection. arXiv:2307.03759\n(2023).\n[Kipf and Welling(2016)] Thomas\nN.\nKipf\nand\nMax\nWelling. 2016.\nSemi-Supervised Classification with\nGraph\nConvolutional\nNetworks.\nIn\nInternational\nConference on Learning Representations (ICLR).\n[Kreuzer et al.(2021)] Devin Kreuzer, Dominique Beaini,\nWill Hamilton, Vincent L\u00b4etourneau, and Prudencio\nTossou. 2021. Rethinking graph transformers with spec-\ntral attention. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), Vol. 34. 21618\u201321629.\n[Lana et al.(2018)] Ibai Lana, Javier Del Ser, Manuel Velez,\nand Eleni I Vlahogianni. 2018. Road traffic forecast-\ning: Recent advances and new challenges. IEEE Intel-\nligent Transportation Systems Magazine 10, 2 (2018),\n93\u2013109.\n[Li et al.(2023)] Ruikun Li,\nXuliang Li,\nShiying Gao,\nST Boris Choy, and Junbin Gao. 2023. Graph convolu-\ntion recurrent denoising diffusion model for multivari-\nate probabilistic temporal forecasting. In International\nConference on Advanced Data Mining and Applications\n(ADMA). Springer, 661\u2013676.\n[Li et al.(2022b)] Xiang Li, John Thickstun, Ishaan Gulra-\njani, Percy S Liang, and Tatsunori B Hashimoto. 2022b.\nDiffusion-LM improves controllable text generation. In\nAdvances in Neural Information Processing Systems,\nVol. 35. 4328\u20134343.\n[Li et al.(2022a)] Yan Li, Xinjiang Lu, Yaqing Wang, and\nDejing Dou. 2022a. Generative Time Series Forecast-\ning with Diffusion, Denoise, and Disentanglement. In\nAdvances in Neural Information Processing Systems,\nVol. 35. 23009\u201323022.\n[Li et al.(2018)] Yaguang Li, Rose Yu, Cyrus Shahabi, and\nYan Liu. 2018. Diffusion convolutional recurrent neu-\nral network: Data-driven traffic forecasting. In Interna-\ntional Conference on Learning Representations (ICLR).\n1\u20138.\n[Liang et al.(2023)] Guojun Liang, Prayag Tiwari, S\u0142awomir\nNowaczyk, Stefan Byttner, and Fernando Alonso-\nFernandez. 2023.\nDynamic Causal Explanation\nBased Diffusion-Variational Graph Neural Network\nfor Spatio-temporal Forecasting.\narXiv:2305.09703\n(2023).\n[Lin et al.(2023)] Lequan Lin, Zhengkun Li, Ruikun Li, Xu-\nliang Li, and Junbin Gao. 2023. Diffusion models for\ntime-series applications: a survey. Frontiers of Infor-\nmation Technology and Electronic Engineering (2023),\n1\u201323.\n[Liu et al.(2023)] Mingzhe Liu, Han Huang, Hao Feng,\nLeilei Sun, Bowen Du, and Yanjie Fu. 2023. PriSTI:\nA Conditional Diffusion Framework for Spatiotemporal\nImputation. In International Conference on Data Engi-\nneering (ICDE). 1\u201310.\n[Luo et al.(2023)] Tianze\nLuo,\nZhanfeng\nMo,\nand\nSinno Jialin Pan. 2023.\nFast graph generation via\nspectral diffusion.\nIEEE Transactions on Pattern\nAnalysis and Machine Intelligence (2023).\n[Matheson and Winkler(1976)] James\nE\nMatheson\nand\nRobert L Winkler. 1976. Scoring rules for continuous\nprobability distributions. Management Science 22, 10\n(1976), 1087\u20131096.\n[Nikolay et al.(2022)] Savinov Nikolay, Chung Junyoung,\nBinkowski Mikolaj, Elsen Erich, and Oord A\u00a8aron van\nden. 2022. Step-unrolled Denoising Autoencoders for\nText Generation. In International Conference on Learn-\ning Representations. 1\u20138.\n[Pal et al.(2021)] Soumyasundar Pal, Liheng Ma, Yingxue\nZhang, and Mark Coates. 2021.\nRNN with particle\nflow for probabilistic spatio-temporal forecasting. In In-\nternational Conference on Machine Learning (ICML).\nPMLR, 8336\u20138348.\n[Papamakarios et al.(2017)] George\nPapamakarios,\nTheo\nPavlakou, and Iain Murray. 2017. Masked autoregres-\nsive flow for density estimation. Advances in Neural\nInformation Processing Systems (NeurIPS) 30 (2017).\n[Rasul et al.(2021a)] Kashif Rasul, Calvin Seward, Ingmar\nSchuster, and Roland Vollgraf. 2021a. Autoregressive\ndenoising diffusion models for multivariate probabilis-\ntic time series forecasting. In International Conference\non Machine Learning (ICML). 8857\u20138868.\n[Rasul et al.(2021b)] Kashif Rasul, Abdul-Saboor Sheikh,\nIngmar Schuster, Urs Bergmann, and Roland Vollgraf.\n2021b.\nMulti-variate Probabilistic Time Series Fore-\ncasting via Conditioned Normalizing Flows. In Interna-\ntional Conference on Learning Representations. 1\u20138.\n[Ronneberger et al.(2015)] Olaf Ronneberger, Philipp Fis-\ncher, and Thomas Brox. 2015. U-net: Convolutional\nnetworks for biomedical image segmentation. In Medi-\ncal Image Computing and Computer-Assisted Interven-\ntion. Springer, 234\u2013241.\n[Salinas et al.(2020)] David Salinas, Valentin Flunkert, Jan\nGasthaus, and Tim Januschowski. 2020.\nDeepAR:\nProbabilistic forecasting with autoregressive recurrent\nnetworks. International Journal of Forecasting 36, 3\n(2020), 1181\u20131191.\n[Seo et al.(2018)] Youngjoo Seo, Micha\u00a8el Defferrard, Pierre\nVandergheynst, and Xavier Bresson. 2018. Structured\nsequence modeling with graph convolutional recurrent\nnetworks. In International Conference on Neural Infor-\nmation Processing (ICONIP). Springer, 362\u2013373.\n[Shao et al.(2022)] Zezhi Shao, Zhao Zhang, Wei Wei, Fei\nWang, Yongjun Xu, Xin Cao, and Christian S. Jensen.\n2022. Decoupled dynamic spatial-temporal graph neu-\nral network for traffic forecasting. Proceedings of the\nVLDB Endowment 15, 11 (2022), 2733\u20132746.\nhttps:\n//doi.org/10.14778/3551793.3551827\n[Shen and Kwok(2023)] L. F. Shen and J. Kwok. 2023. Non-\nautoregressive Conditional Diffusion Models for Time\nSeries Prediction. In International Conference on Ma-\nchine Learning (ICML). PMLR.\n[Sohl-Dickstein et al.(2015)] Jascha\nSohl-Dickstein,\nEric\nWeiss, Niru Maheswaranathan, and Surya Ganguli.\n2015.\nDeep unsupervised learning using nonequilib-\nrium thermodynamics. In International Conference on\nMachine Learning (ICML). PMLR, 2256\u20132265.\n[Sutskever et al.(2014)] Ilya Sutskever, Oriol Vinyals, and\nQuoc V Le. 2014. Sequence to sequence learning with\nneural networks. Advances in Neural Information Pro-\ncessing Systems (NeurIPS) 27 (2014).\n[Tashiro et al.(2021)] Yusuke Tashiro, Jiaming Song, Yang\nSong, and Stefano Ermon. 2021.\nCSDI: Conditional\nscore-based diffusion models for probabilistic time se-\nries imputation. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), Vol. 34. 24804\u201324816.\n[van den Oord et al.(2016)] A\u00a8aron van den Oord, Sander\nDieleman, Heiga Zen, Karen Simonyan, Oriol Vinyals,\nAlex Graves, Nal Kalchbrenner, Andrew W. Senior, and\nKoray Kavukcuoglu. 2016.\nWaveNet: A Generative\nModel for Raw Audio. In The 9th ISCA Speech Synthe-\nsis Workshop. 125.\n[Vaswani et al.(2017)] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Advances in Neural Information Pro-\ncessing Systems (NeurIPS), Vol. 30. 6000\u20136010.\n[Wen et al.(2023)] Haomin Wen, Youfang Lin, Yutong Xia,\nHuaiyu Wan, Qingsong Wen, Roger Zimmermann, and\nYuxuan Liang. 2023.\nDiffSTG: Probabilistic spatio-\ntemporal graph forecasting with denoising diffusion\nmodels. In ACM International Conference on Advances\nin Geographic Information Systems (ACM SIGSPA-\nTIAL). 1\u201312.\n[Yang et al.(2023)] Ling Yang, Zhilong Zhang, Yang Song,\nShenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang,\nBin Cui, and Ming-Hsuan Yang. 2023. Diffusion mod-\nels: A comprehensive survey of methods and applica-\ntions. Comput. Surveys 56, 4 (2023), 1\u201339.\n[Yang et al.(2022)] Ruihan Yang, Prakhar Srivastava, and\nStephan Mandt. 2022. Diffusion probabilistic modeling\nfor video generation. arXiv:2203.09481 1 (2022).\n[Yu et al.(2018)] Bing Yu, Haoteng Yin, and Zhanxing Zhu.\n2018. Spatio-temporal graph convolutional networks:\nA deep learning framework for traffic forecasting. In-\nternational Joint Conference on Artificial Intelligence\n(IJCAI) (2018).\n[Yu et al.(2022)] Peiyu Yu, Sirui Xie, Xiaojian Ma, Baox-\niong Jia, Bo Pang, Ruigi Gao, Yixin Zhu, Song-\nChun Zhu, and Ying Nian Wu. 2022.\nLatent Diffu-\nsion Energy-Based Model for Interpretable Text Mod-\nelling. In International Conference on Machine Learn-\ning, Vol. 162. PMLR, 25702\u201325720.\n[Yuan and Li(2021)] Haitao Yuan and Guoliang Li. 2021. A\nsurvey of traffic prediction: from spatio-temporal data\nto intelligent transportation.\nData Science and Engi-\nneering 6 (2021), 63\u201385.\n[Zheng et al.(2020)] Chuanpan\nZheng,\nXiaoliang\nFan,\nCheng Wang, and Jianzhong Qi. 2020.\nGman:\nA\ngraph multi-attention network for traffic prediction.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, Vol. 34. 1234\u20131241.\n[Zivot and Wang(2006)] Eric Zivot and Jiahui Wang. 2006.\nVector autoregressive models for multivariate time se-\nries.\nModeling Financial Time Series with S-PLUS\u00ae\n(2006), 385\u2013429.\nAPPENDICES\nAppendix A. Denoising Diffusion Probabilistic\nModel\nDenoising diffusion probabilistic model (DDPM) is one of\nthe classic formulations of diffusion models [Sohl-Dickstein\net al.(2015); Ho et al.(2020); Yang et al.(2023)]. With its\nhigh flexibility in modelling target distributions and out-\nstanding capability of capturing complex generative patterns,\nDDPM is widely used in various time series tasks\n[Ra-\nsul et al.(2021a); Tashiro et al.(2021); Coletta et al.(2023);\nLin et al.(2023)]. Unlike traditional probabilistic models that\nlearn the target distribution q(x) explicitly to generate sam-\nples, DDPM learns how to generate samples directly via a\npair of forward-backward Markov chains without accessing\nthe actual target distribution function. Assuming that x0 is\nthe original data, for k = 0, 1, ..., K, the forward chain in-\njects Gaussian noises to the generative target with a proba-\nbilistic transition kernel\nq(xk|xk\u22121) = N\n\u0010\nxk;\np\n1 \u2212\u03b2kxk\u22121, \u03b2kI\n\u0011\n,\nwhere \u03b2k \u2208(0, 1) is a hyperparameter controlling the noise\nlevel at each forward step. The design of forward chain en-\nables us to derive the disturbed data at a particular step k di-\nrectly from x0 as\nq(xk|x0) = N\n\u0010\nxk;\np\n\u02dc\u03b1kx0, (1 \u2212\u02dc\u03b1k)I\n\u0011\n,\nwhere \u02dc\u03b1k := Qk\ni=1 (1 \u2212\u03b2i). This means xk = \u221a\u02dc\u03b1kx0 +\n\u221a1 \u2212\u02dc\u03b1k\u03f5 with \u03f5 \u223cN(0, I). To ensure that the forward\nchain transits eventually to white noises, one shall have \u02dc\u03b1K \u2248\n0 such that q(xK) :=\nR\nq(xK|x0)q(x0)dx0 \u2248N(xK; 0, I).\nNext, the backward chain recovers white noises to original\ndata through a Gaussian transition kernel p\u03b8(xk\u22121|xk) =\nN\n\u0000xk\u22121; \u00b5\u03b8(xk, k), \u03c3kI\n\u0001\n, where \u00b5\u03b8(xk, k) is usually pa-\nrameterized with a neural network with learnable parameters\n\u03b8, and \u03c3k is a variance hyperparameter. These parameters are\noptimized by minimizing the negative evidence lower-bound\n(ELBO):\nLE(\u03b8) = Eq(x0:K)\n\"\n\u2212log p(xK) \u2212\nK\nX\nk=1\nlog p\u03b8(xk\u22121|xk)\nq(xk|xk\u22121)\n#\n.\nThe learning process is simplified with DDPM by [Ho\net al.(2020)]. Instead of learning \u00b5\u03b8(xk, k), a denoising net-\nwork \u03f5\u03b8\n\u0000xk, k\n\u0001\nis learned with the following objective func-\ntion:\nLDDP M(\u03b8) = Ek,x0,\u03f5\n\r\r\u03f5 \u2212\u03f5\u03b8\n\u0000xk, k\n\u0001\r\r2 .\nThis denoising network \u03f5\u03b8(xk, k) takes noised data xk and\nstep index k as inputs to predict the noise injected at step\nk in the forward chain. Finally, DDPM generates samples\nby eliminating the noises in random white noise xK\n\u223c\nN(xK; 0, I). For backward step k = K, K \u22121, ..., 1, DDPM\nupdates the sample as\nxk\u22121 \u2190\n1\n\u221a1 \u2212\u03b2k\n\u0012\nxk \u2212\n\u03b2k\n\u221a1 \u2212\u02dc\u03b1k\n\u03f5\u03b8(xk, k)\n\u0013\n+ \u03c3ke\nwhere \u03c3k is a hyperparameter of variance in the backward\ntransition kernel with e = 0 for k = 1, and e \u223cN(0, I)\notherwise. Eventually, x0 will be a sample from the same\ndistribution of the generative target.\nAppendix B. Generalized Fourier Transform\nThe Fourier transform can be naturally generalized to mul-\ntivariate traffic STGs.\nAssume that we have a traffic\nSTG with graph signals XG\n= {x1, x2, ..., xt, ...|xt \u2208\nRN\u00d7Dx}, where Dx is the number of variables. The graph\nis a univariate STG when Dx\n= 1, and a multivariate\nSTG when Dx \u22652.\nFor a sampled past-future window\nX\n=\n{xt0\u2212c+1, ..., xt0+f}\n\u2208\nRN\u00d7Dx\u00d7(c+f), we first\nsplit it according to its variables and rewrite it as X =\n{x\u2032\n1, ..., x\u2032\nd, ..., x\u2032\nDx|x\u2032\nd\n\u2208\nRN\u00d7(c+f)}.\nThen, with the\nFourier operator U, we conduct the transform on each vari-\nable matrix x\u2032\nd as\n\u02dcx\u2032\nd = U \u22bax\u2032\nd.\nHence, the Fourier representation of the sampled window X\nis \u02dc\nX = {\u02dcx\u2032\n1, ..., \u02dcx\u2032\nd, ..., \u02dcx\u2032\nDx} \u2208RN\u00d7Dx\u00d7(c+f). To convert\nthe Fourier representation of a single variable matrix back to\nthe original domain, we may apply Fourier reconstruction as\nx\u2032\nd = U \u02dcx\u2032\nd,\nso X = {U \u02dcx\u2032\n1, ..., U \u02dcx\u2032\nd, ..., U \u02dcx\u2032\nDx} \u2208RN\u00d7Dx\u00d7(c+f).\nAppendix C. Baselines\nIn the main experiment, we compare SpecSTG with four\nstate-of-the-art diffusion baselines:\n\u2022 TimeGrad [Rasul et al.(2021a)]: an autoregressive dif-\nfusion model using long short-term memory (LSTM)\nor gated recurrent units (GRU) to encode temporal dy-\nnamics and a time-modified WavaNet [van den Oord\net al.(2016)] as \u03f5\u03b8.\n\u2022 GCRDD [Li et al.(2023)]: an autoregressive diffusion\nmodel for spatio-temporal forecasting, which adopts a\ngraph-modified GRU to encode past time series and\nspatial connectivity as conditions.\nDeveloped from\nTimeGrad, GCRDD also employs a WaveNet architec-\nture for its \u03f5\u03b8 but with graph convolution to process spa-\ntial information.\n\u2022 DiffSTG [Wen et al.(2023)]:\nan one-shot diffusion\nmodel for spatio-temporal forecasting with graph-\nmodified UNet [Ronneberger et al.(2015)] as \u03f5\u03b8.\n\u2022 PriSTI [Liu et al.(2023)]: a one-shot diffusion model\nfor spatio-temporal imputation. It is equipped with a\nfeature extraction mechanism to construct conditions.\nAn attention-based [Vaswani et al.(2017)] WaveNet is\nadopted as its \u03f5\u03b8.\nIn addition, we also compare SpecSTG with two non-\ndiffusion probabilistic methods, including\n\u2022 DeepVAR [Salinas et al.(2020)]:\nan autoregressive\nRNN-based model for multivariate time series forecast-\ning, known as the multivariate variant of DeepAR.\n\u2022 Transformer Normalizing Flow [Rasul et al.(2021b)]:\nan autoregressive model for multivariate time series\nforecasting that approximates the target distribution\nwith a normalizing flow such as Real-NVP [Dinh\net al.(2017)] and MAF [Papamakarios et al.(2017)].\nHere we choose Real-NVP, and we call this model\nTransNVP.\nIn addition, in Appendix E, we provide supplementary com-\nparisons with non-diffusion probabilistic methods, including\n\u2022 Historical Average (HA): a deterministic method that\nregards time series as periodic processes and predicts fu-\nture time series with weighted averages from past obser-\nvations.\n\u2022 Vector Auto-Regressive model (VAR) [Zivot and\nWang(2006)]: a deterministic model for multivariate\ntime series forecasting that assumes time series are sta-\ntionary and predicts with lagged observations.\n\u2022 FC-LSTM [Sutskever et al.(2014)]: an LSTM model\nwith fully connected (FC) hidden units that performs\nwell in capturing sequential temporal dependencies.\n\u2022 STGCN [Yu et al.(2018)]: a graph convolutional net-\nwork for spatio-temporal traffic forecasting, equipped\nwith gated temporal convolution and graph convolution\nto process temporal and spatial information.\n\u2022 DCRNN [Li et al.(2018)]: a recurrent graph neural net-\nwork that integrates graph diffusion and GRU under an\nencoder-decoder structure.\n\u2022 ASTGCN [Guo et al.(2019)]: an attention-based graph\nconvolutional neural network that adopts both graph at-\ntention and temporal attention to learn spatio-temporal\npatterns.\n\u2022 GMAN [Zheng et al.(2020)]: a graph neural network\nwith multiple spatio-temporal attention blocks in its\nencoder-decoder architecture to enhance the learning of\nspatio-temporal patterns.\nAppendix D. Implementation Details\nDiffusion baselines Diffusion models are implemented with\nsimilar diffusion process hyperparameters as SpecSTG. For\nall models that utilize WaveNet architecture in their denois-\ning networks, we fix the number of residual blocks and resid-\nual channels both as 8. Other model-specific implementation\ndetails follow their original papers along with the default set-\ntings in their codes.\nNon-diffusion probabilistic baselines DeepVAR is imple-\nmented with PyTorchTS1. We choose the LSTM with 2\nlayers as its recurrent structure. The size of hidden states\nis fixed at 1024, because a smaller hidden size leads to un-\nsatisfactory performance, and a larger hidden size will not\nimprove the results much. TransNVP is also implemented\nwith PyTorchTS. Given that details of TransNVP are not\nmentioned in its paper, we set its hyperparameters majorly\naccording to the settings of Transformer MAF, which is a sim-\nilar model but uses MAF to model the target distribution. We\n1Source: https://github.com/zalandoresearch/pytorch-ts\nTable 3: Comparison between SpecSTG and classic deterministic\nmodels on PEMS04F and PEMS08F. Partial results are retrieved\nfrom [Shao et al.(2022)]. The best results are marked in bold, and\nthe second best results are underlined.\nRMSE\nMAE\nModel\n15min\n30min\n60min\n15min\n30min\n60min\nPEMS04F\nHA\n42.69\n49.37\n67.43\n28.92\n33.73\n46.97\nVAR\n34.30\n36.58\n40.28\n21.94\n23.72\n26.76\nFC-LSTM\n33.37\n39.1\n50.73\n21.42\n25.83\n36.41\nSTGCN\n30.76\n34.43\n41.11\n19.35\n21.85\n26.97\nDCRNN\n31.94\n36.15\n44.81\n20.34\n23.21\n29.24\nASTGCN\n31.43\n34.34\n40.02\n20.15\n22.09\n26.03\nGMAN\n29.32\n30.77\n30.21\n18.28\n18.75\n19.95\nSpecSTG\n30.17\n32.81\n37.29\n19.29\n21.39\n23.29\nPEMS08F\nHA\n34.96\n40.89\n56.74\n23.52\n27.67\n39.28\nVAR\n29.73\n30.30\n38.97\n19.52\n22.25\n26.17\nFC-LSTM\n26.27\n34.53\n47.03\n17.38\n21.22\n30.69\nSTGCN\n25.03\n27.27\n34.21\n15.30\n17.69\n25.46\nDCRNN\n25.48\n27.63\n34.21\n15.64\n17.88\n22.51\nASTGCN\n25.09\n28.17\n33.68\n16.48\n18.66\n22.83\nGMAN\n22.88\n24.02\n25.96\n13.80\n14.62\n15.72\nSpecSTG\n22.23\n24.77\n29.90\n14.93\n16.70\n20.25\nalso tune the hyperparameters when needed to improve the\nperformance of TransNVP.\nDeterministic baselines The results of deterministic base-\nlines are retrieved directly from the experiments in [Shao\net al.(2022)], which adopts similar experiment settings (e.g.,\n60%/20%/20% data split) as our experiments.\nAppendix E. Comparison with Classic\nDeterministic Baselines\nTable 3 presents a comparison between SpecSTG and deter-\nministic methods. Notably, SpecSTG outperforms most clas-\nsic deterministic approaches, including traditional statistical\nmodels like VAR and graph neural networks such as DCRNN\nand ASTGCN. However, GMAN consistently demonstrates\nsuperior performance, particularly in long-term forecasting.\nHere are two plausible explanations. Firstly, being a deter-\nministic model, GMAN is trained with MAE loss, strengthen-\ning the generation of accurate deterministic predictions com-\npared to SpecSTG, which optimizes an implicit probabilistic\nobjective. Secondly, GMAN\u2019s sophisticated encoder-decoder\narchitecture and multiple spatio-temporal attention blocks ef-\nfectively integrate spatial and temporal information, while\nSpecSTG relies on a single graph-modified GRU encoder for\nprocessing spatio-temporal patterns. This observation could\nguide future work to enhance our model.\nAppendix F. Supplementary Forecasting\nVisualizations\nIn Figure 7, we provide more forecasting visualizations of\nTimeGrad, GCRDD, and SpecSTG. Consistent with our con-\nclusion in Subsection 6.1, SpecSTG generates more compat-\nible forecasting intervals for data with more fluctuations (see\nFigure 7 (a), (d), (e), and (f)). On the contrary, when data\nvariations are small, the method is less effective due to the\nFigure 7: Supplementary forecasting visualizations on traffic speed data ((a), (b), (c)) and traffic flow data ((d), (e), (f)).\nlack of systematic information in the Fourier representation\n(see Figure 7 (b) and (c)).\nAppendix G. Extended Sensitivity Analysis\nIn Figure 8, we show extended sensitivity analysis results\nfor point evaluation at 15/30/60 minutes. The implementa-\ntion details of this extended sensitivity analysis are consistent\nwith those described in Subsection 6.3. As previously ob-\nserved, SpecSTG demonstrates better performance with rela-\ntively larger values of \u03b2K. The number of diffusion steps K\nshows less impact on performance compared to \u03b2K. Addi-\ntionally, the forecasting outcomes exhibit minimal variation\nacross different hyperparameter configurations.\nThis indi-\ncates that SpecSTG is not highly sensitive to hyperparame-\nters, thereby simplifying the hyperparameter selection pro-\ncess during training.\nFigure 8: Extended sensitivity analysis on key hyperparameters: dif-\nfusion step K and the end of beta schedule \u03b2K (point evaluation re-\nsults at 15/30/60 minutes).\n",
    "2306.16927": "1\nEnd-to-end Autonomous Driving:\nChallenges and Frontiers\nLi Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger and Hongyang Li\nAbstract\u2014The autonomous driving community has witnessed a rapid growth in approaches that embrace an end-to-end algorithm\nframework, utilizing raw sensor input to generate vehicle motion plans, instead of concentrating on individual tasks such as detection\nand motion prediction. End-to-end systems, in comparison to modular pipelines, benefit from joint feature optimization for perception\nand planning. This field has flourished due to the availability of large-scale datasets, closed-loop evaluation, and the increasing need\nfor autonomous driving algorithms to perform effectively in challenging scenarios. In this survey, we provide a comprehensive analysis\nof more than 270 papers, covering the motivation, roadmap, methodology, challenges, and future trends in end-to-end autonomous\ndriving. We delve into several critical challenges, including multi-modality, interpretability, causal confusion, robustness, and world\nmodels, amongst others. Additionally, we discuss current advancements in foundation models and visual pre-training, as well as how\nto incorporate these techniques within the end-to-end driving framework. We maintain an active repository that contains up-to-date\nliterature and open-source projects at https://github.com/OpenDriveLab/End-to-end-Autonomous-Driving.\nIndex Terms\u2014Autonomous Driving, End-to-end System Design, Policy Learning, Simulation.\n\u2726\n1\nINTRODUCTION\nC\nONVENTIONAL autonomous driving systems adopt a\nmodular design strategy, wherein each functionality,\nsuch as perception, prediction, and planning, is individually\ndeveloped and integrated into onboard vehicles. The plan-\nning or control module, responsible for generating steering\nand acceleration outputs, plays a crucial role in determining\nthe driving experience. The most common approach for\nplanning in modular pipelines involves using sophisticated\nrule-based designs, which are often ineffective in addressing\nthe vast number of situations that occur on road. Therefore,\nthere is a growing trend to leverage large-scale data and to\nuse learning-based planning as a viable alternative.\nWe define end-to-end autonomous driving systems as\nfully differentiable programs that take raw sensor data as\ninput and produce a plan and/or low-level control actions\nas output. Fig. 1 (a)-(b) illustrates the difference between\nthe classical and end-to-end formulation. The conventional\napproach feeds the output of each component, such as\nbounding boxes and vehicle trajectories, directly into sub-\nsequent units (dashed arrows). In contrast, the end-to-end\nparadigm propagates feature representations across compo-\nnents (gray solid arrow). The optimized function is set to\nbe, for example, the planning performance, and the loss\nis minimized via back-propagation (red arrow). Tasks are\njointly and globally optimized in this process.\nIn this survey, we conduct an extensive review of this\nemerging topic. Fig. 1 provides an overview of our work. We\nbegin by discussing the motivation and roadmap for end-to-\nend autonomous driving systems. End-to-end approaches\ncan be broadly classified into imitation and reinforcement\n\u2022\nL. Chen and H. Li are with OpenDriveLab, Shanghai AI Lab, Shanghai,\nChina, and the University of Hong Kong, Hong Kong, China. P. Wu is\nwith OpenDriveLab, Shanghai AI Lab, Shanghai, China. K. Chitta, B.\nJaeger and A. Geiger are with University of T\u00fcbingen and T\u00fcbingen AI\nCenter, Germany. Primary contact: lihongyang@pjlab.org.cn\nlearning, and we give a brief review of these methodologies.\nWe cover datasets and benchmarks for both closed and\nopen-loop evaluation. We summarize a series of critical\nchallenges, including interpretability, generalization, world\nmodels, causal confusion, etc. We conclude by discussing\nfuture trends that we think should be embraced by the\ncommunity to incorporate the latest developments from\ndata engines, and large foundation models, amongst others.\nNote that this review is mainly orchestrated from a theoret-\nical perspective. Engineering efforts such as version control,\nunit testing, data servers, data cleaning, software-hardware\nco-design, etc., play crucial roles in deploying the end-to-\nend technology. Publicly available information regarding\nthe latest practices on these topics is limited. We invite the\ncommunity towards more openness in future discussions.\n1.1\nMotivation of an End-to-end System\nIn the classical pipeline, each model serves a standalone\ncomponent and corresponds to a specific task (e.g., traffic\nlight detection). Such a design is beneficial in terms of\ninterpretability and ease of debugging. However, since the\noptimization objectives across modules are different, with\ndetection pursuing mean average precision (mAP) while\nplanning aiming for driving safety and comfort, the entire\nsystem may not be aligned with a unified target, i.e., the\nultimate planning/control task. Errors from each module, as\nthe sequential procedure proceeds, could be compounded\nand result in an information loss. Moreover, compared to\none end-to-end neural network, the multi-task, multi-model\ndeployment which involves multiple encoders and message\ntransmission systems, may increase the computational bur-\nden and potentially lead to sub-optimal use of compute.\nIn contrast to its classical counterpart, an end-to-end\nautonomous system offers several advantages. (a) The most\napparent merit is its simplicity in combining perception,\narXiv:2306.16927v3  [cs.RO]  15 Aug 2024\n2\nPipeline\nSection 1\nBenchmarking\nSection 3\nImitation Learning\n- Behavior Cloning\nReinforcement \nLearning\nImitation Learning \n\u2013 Inverse Optimal \nControl\nOpen-loop\nReal-world\nChallenges\nSection 4\nFuture Trends\nSection 5\nFoundation \nModel\nData\nEngine\nModular End-to-end \nPlanning \nInput \nModality\nnuPlan\nZero/Few-Shot \nLearning\nInterpreta-\nbility\nPolicy \nDistillation\nCausal\nConfusion\nMulti-task \nLearning\nTask A\nTask B\nNet\nPolicy\nExpert\nMethods\nSection 2\nVisual \nAbstraction\nRobustness / \nGeneralization\nWorld \nModel\nPerception\nPrediction\nPlanning\nMapping\nModule X\n(a) Classical Approach \n(b) End-to-end Paradigm (This Survey)\nModule Y\nfeature\nPerception\nBounding box\nbackpropagation\nPrediction\nPlanning\nTrajectory\nSafety\nGuarantee\nClosed-loop\nFig. 1: Survey at A Glance. (a) Pipeline and Methods. We define end-to-end autonomous driving as a learning-based\nalgorithm framework with raw sensor input and planning/control output. We deepdive into 270+ papers and categorize\ninto imitation learning (IL) and reinforcement learning (RL). (b) Benchmarking. We group popular benchmarks into\nclosed-loop and open-loop evaluation, respectively. We cover various aspects of closed-loop simulation and the limitations\nof open-loop evaluation for this problem. (c) Challenges. This is the main section of our work. We list key challenges from a\nwide range of topics and extensively analyze why these concerns are crucial. Promising resolutions to these challenges are\ncovered as well. (d) Future Trends. We discuss how end-to-end paradigm could benefit by aid of the rapid development\nof foundation models, visual pre-training, etc. Partial photos by courtesy of online resources.\nprediction, and planning into a single model that can be\njointly trained. (b) The whole system, including its inter-\nmediate representations, is optimized towards the ultimate\ntask. (c) Shared backbones increase computational efficiency.\n(d) Data-driven optimization has the potential to improve\nthe system by simply scaling training resources.\nNote that the end-to-end paradigm does not necessarily\nindicate one black box with only planning/control outputs.\nIt could have intermediate representations and outputs\n(Fig. 1 (b)) as in classical approaches. In fact, several state-of-\nthe-art systems [1, 2] propose a modular design but optimize\nall components together to achieve superior performance.\n1.2\nRoadmap\nFig. 2 depicts a chronological roadmap of critical achieve-\nments in end-to-end autonomous driving, where each part\nindicates an essential paradigm shift or performance boost.\nThe history of end-to-end autonomous driving dates back to\n1988 with ALVINN [3], where the input was two \u201cretinas\u201c\nfrom a camera and a laser range finder, and a simple neu-\nral network generated steering output. NVIDIA designed\na prototype end-to-end CNN system, which reestablished\nthis idea in the new era of GPU computing [8]. Notable\nprogress has been achieved with the development of deep\nneural networks, both in imitation learning [15, 16] and\nreinforcement learning [4, 17, 18, 19]. The policy distilla-\ntion paradigm proposed in LBC [5] and related approaches\n[20, 21, 22, 23] has significantly improved closed-loop per-\nformance by mimicking a well-behaved expert. To enhance\ngeneralization ability due to the discrepancy between the\nexpert and learned policy, several papers [10, 24, 25] have\nproposed aggregating on-policy data [26] during training.\nA significant turning point occurred around 2021. With\ndiverse sensor configurations available within a reasonable\ncomputational budget, attention was focused on incorpo-\nrating more modalities and advanced architectures (e.g.,\nTransformers [27]) to capture global context and represen-\ntative features, as in TransFuser [6, 28] and many variants\n[29, 30, 31]. Combined with more insights about the simula-\ntion environment, these advanced designs resulted in a sub-\nstantial performance boost on the CARLA benchmark [13].\nTo improve the interpretability and safety of autonomous\nsystems, approaches [11, 32, 33] explicitly involve various\nauxiliary modules to better supervise the learning process\nor utilize attention visualization. Recent works prioritize\ngenerating safety-critical data [7, 34, 35], pre-training a\n3\nCritical\nStates\nOn-Policy\nData\nSample\n1988\nCommand\nLearned \npolicy\nExpert\nCritical \nScenario \nUniAD\n2021\n2022\n2023\nData \nGeneration\nModular End-to-end \nPlanning\nModality / \nAdvanced Structure\nNEAT\nKING \nTransfuser\nInterpretability\nPolicy \nPretraining\nCARLA\nDS: 24.98\nCARLA\nDS: 47.65\nCARLA v2 Launched \nDS: N/A\nCARLA\nDS: 79.95\nNMP, BDD-X, PlanT\nSelfD, ACO \nInterFuser, ThinkTwice\nAdvsim, L2C\n2019\n2020\nPolicy \nDistillation\nConditional \nIL\nLBC\nDrive in A Day\nCILRS\nWOR, Roach, TCP \nGeneralization\nReinforcement\nLearning (RL)\nCARLA Launched\nDS: 8.94\nDARB\nAgileAD, SafeDAgger\nCIL\nCIRL, MaRLn, GRI\n2016\n\u2026\nImitation \nLearning (IL)\nALVINN\nCNN E2E\nInput\nDirection\nDrive by \nwire \ninterface\nTracking\nMapping\nPlanner\nPolicy\nPretraining\nDownstream \nTask\nSteer\ninstance\naction\nPPGeo\n2024\nTransformer\nDownstream\nPolicy\nPretraining\nExpert\nPrivileged \nAgent\nSensorimotor \nAgent\nAgent / Reward\nnuPlan Launched\nScore: 0.90\nP3, MP3, ST-P3\nAttention\nQuery\nSegmentation\nOffsets\nBDDV\n[3]\n[4]\n[5]\n[6]\n[7]\n[8]\n[9]\n[10]\n[11]\n[12]\n[2]\nFig. 2: Roadmap of End-to-end Autonomous Driving. We present the key milestones chronologically, grouping similar\nworks under the same theme. The representative or first work is shown in bold with an illustration, while the date of\nthe rest of the literature in the same theme may vary. We also display the score for each year\u2019s top entry in the CARLA\nleaderboard [13] (DS, ranging from 0 to 100) and the recent nuPlan challenge [14] (Score ranging from 0 to 1).\nfoundation model or backbone curated for policy learning\n[12, 36, 37], and advocating a modular end-to-end planning\nphilosophy [1, 2, 38, 39]. Meanwhile, the new and challeng-\ning CARLA v2 [13] and nuPlan [14] benchmarks have been\nintroduced to facilitate research into this area.\n1.3\nComparison to Related Surveys\nWe would like to clarify the difference between our survey\nand previous related surveys [40, 41, 42, 43, 44, 45, 46, 47, 48].\nSome prior surveys [40, 41, 42, 43] cover content similar to\nours in the sense of an end-to-end system. However, they do\nnot cover new benchmarks and approaches that arose with\nthe significant recent transition in the field, and place a mi-\nnor emphasis on frontiers and challenges. The others focus\non specific topics in this domain, such as imitation learning\n[44, 45, 46] or reinforcement learning [47, 48]. In contrast,\nour survey provides up-to-date information on the latest\ndevelopments in this field, covering a wide span of topics\nand providing in-depth discussions of critical challenges.\n1.4\nContributions\nTo summarize, this survey has three key contributions:\n(a) We provide a comprehensive analysis of end-to-end\nautonomous driving for the first time, including high-level\nmotivation, methodologies, benchmarks, and more. Instead\nof optimizing a single block, we advocate for a philosophy\nto design the algorithm framework as a whole, with the\nultimate target of achieving safe and comfortable driving.\n(b) We extensively investigate the critical challenges that\nconcurrent approaches face. Out of the more than 270\npapers surveyed, we summarize major aspects and pro-\nvide in-depth analysis, including topics on generalizability,\nlanguage-guided learning, causal confusion, etc.\n(c) We cover the broader impact of how to embrace large\nfoundation models and data engines. We believe that this\nline of research and the large scale of high-quality data it\nprovides could significantly advance this field. To facilitate\nfuture research, we maintain an active repository updated\nwith new literature and open-source projects.\n2\nMETHODS\nThis section reviews fundamental principles behind most\nexisting end-to-end self-driving approaches. Sec. 2.1 dis-\ncusses methods using imitation learning and provides de-\ntails on the two most popular sub-categories, namely behav-\nior cloning and inverse optimal control. Sec. 2.2 summarizes\nmethods that follow the reinforcement learning paradigm.\n2.1\nImitation Learning\nImitation learning (IL), also referred to as learning from\ndemonstrations, trains an agent to learn the policy by imitat-\ning the behavior of an expert. IL requires a dataset D = {\u03bei}\ncontaining trajectories collected under the expert\u2019s policy\n\u03c0\u03b2, where each trajectory is a sequence of state-action pairs.\nThe goal of IL is to learn an agent policy \u03c0 that matches \u03c0\u03b2.\nThe policy \u03c0 can output planned trajectories or control\nsignals. Early works usually adopt control outputs, due\nto the ease of collection. However, predicting controls at\ndifferent steps could lead to discontinuous maneuvers and\nthe network inherently specializes to the vehicle dynamics\nwhich hinders generalization to other vehicles. Another\ngenre of works predicts waypoints. It considers a relatively\nlonger time horizon. Meanwhile, converting trajectories for\nvehicles to track into control signals needs additional con-\ntrollers, which is non-trivial and involves vehicle models\nand control algorithms. Since no clear performance gap has\nbeen observed between these two paradigms, we do not\ndifferentiate them explicitly in this survey. An interesting\nand more in-depth discussion can be found in [22].\nOne widely used category of IL is behavior cloning (BC)\n[49], which reduces the problem to supervised learning.\nInverse Optimal Control (IOC), also known as Inverse Rein-\nforcement Learning (IRL) [50] is another type of IL method\nthat utilizes expert demonstrations to learn a reward func-\ntion. We elaborate on these two categories below.\n2.1.1\nBehavior Cloning\nIn BC, matching the agent\u2019s policy with the expert\u2019s is\naccomplished by minimizing planning loss as supervised\n4\n!!\n!!\"#\nBehavior Cloning\nInverse Optimal Control\nReinforcement Learning\n\"\nupdate\n!$\n#\n$\u2217\nData buffer\nTrajectory set\nLearn cost\n!$\n!\n!\nData buffer\nLearn\nDeployment\nDeployment\n&!\"#\nFig. 3: Overview of methods in end-to-end autonomous driving. We illustrate three popular paradigms, including two\nimitation learning frameworks (behavior cloning and inverse optimal control), as well as online reinforcement learning.\nlearning over the collected dataset: E(s,a) \u2113(\u03c0\u03b8(s), a). Here,\n\u2113(\u03c0\u03b8(s), a) represents a loss function that measures the\ndistance between the agent action and the expert action.\nEarly applications of BC for driving [3, 8, 51] utilized an\nend-to-end neural network to generate control signals from\ncamera inputs. Further enhancements, such as multi-sensor\ninputs [6, 52], auxiliary tasks [16, 28], and improved expert\ndesign [21], have been proposed to enable BC-based end-to-\nend driving models to handle challenging urban scenarios.\nBC is advantageous due to its simplicity and efficiency,\nas it does not require hand-crafted reward design, which\nis crucial for RL. However, there are some common is-\nsues. During training, it treats each state as independently\nand identically distributed, resulting in an important prob-\nlem known as covariate shift. For general IL, several on-\npolicy methods have been proposed to address this issue\n[26, 53, 54, 55]. In the context of end-to-end autonomous\ndriving, DAgger [26] has been adopted in [5, 10, 25, 56]. An-\nother common problem with BC is causal confusion, where\nthe imitator exploits and relies on false correlations between\ncertain input components and output signals. This issue has\nbeen discussed in the context of end-to-end autonomous\ndriving in [57, 58, 59, 60]. These two challenging problems\nare further discussed in Sec. 4.9 and Sec. 4.8, respectively.\n2.1.2\nInverse Optimal Control\nTraditional IOC algorithms learn an unknown reward func-\ntion R(s, a) from expert demonstrations, where the expert\u2019s\nreward function can be represented as a linear combination\nof features [50, 61, 62, 63, 64]. However, in continuous, high-\ndimensional autonomous driving scenarios, the definition\nof the reward is implicit and difficult to optimize.\nGenerative adversarial imitation learning [65, 66, 67]\nis a specialized approach in IOC that designs the reward\nfunction as an adversarial objective to distinguish the expert\nand learned policies, similar to the concept of generative\nadversarial networks [68]. Recently, several works propose\noptimizing a cost volume or cost function with auxiliary\nperceptual tasks. Since a cost is an alternative representation\nof the reward, we classify these methods as belonging to\nthe IOC domain. We define the cost learning framework\nas follows: end-to-end approaches learn a reasonable cost\nc(\u00b7) and use algorithmic trajectory samplers to select the\ntrajectory \u03c4 \u2217with the minimum cost, as illustrated in Fig. 3.\nRegarding cost design, it has representations including a\nlearned cost volume in a bird\u2019s-eye-view (BEV) [32], joint\nenergy calculated from other agents\u2019 future motion [69],\nor a set of probabilistic semantic occupancy or freespace\nlayers [39, 70, 71]. On the other hand, trajectories are typ-\nically sampled from a fixed expert trajectory set [1, 72] or\nprocessed by parameter sampling with a kinematic model\n[32, 38, 39, 70]. Then, a max-margin loss is adopted as in\nclassic IOC methods to encourage the expert demonstration\nto have a minimal cost while others have high costs.\nSeveral challenges exist with cost learning approaches.\nIn particular, in order to generate more realistic costs, HD\nmaps, auxiliary perception tasks, and multiple sensors are\ntypically incorporated, which increases the difficulty of\nlearning and constructing datasets for multi-modal multi-\ntask frameworks. Nevertheless, the aforementioned cost\nlearning methods significantly enhance the safety and in-\nterpretability of decisions (see Sec. 4.6), and we believe that\nthe industry-inspired end-to-end system design is a viable\napproach for real-world applications.\n2.2\nReinforcement Learning\nReinforcement learning (RL) [73, 74] is a field of learning by\ntrial and error. The success of deep Q networks (DQN) [75]\nin achieving human-level control on the Atari benchmark\n[76] has popularized deep RL. DQN trains a neural network\ncalled the critic (or Q network), which takes as input the\ncurrent state and an action, and predicts the discounted\nreturn of that action. The policy is then implicitly defined\nby selecting the action with the highest predicted return.\nRL requires an environment that allows potentially un-\nsafe actions to be executed, to collect novel data (e.g., via ran-\ndom actions). Additionally, RL requires significantly more\ndata to train than IL. For this reason, modern RL methods of-\nten parallelize data collection across multiple environments\n[77]. Meeting these requirements in the real world presents\ngreat challenges. Therefore, almost all papers that use RL in\ndriving have only investigated the technique in simulation.\nMost use different extensions of DQN. The community has\nnot yet converged on a specific RL algorithm.\nRL has successfully learned lane following on a real car\non an empty street [4]. Despite this encouraging result, it\nmust be noted that a similar task was already accomplished\nby IL three decades prior [3]. To date, no report has shown\nresults for end-to-end training with RL that are competitive\nwith IL. The reason for this failure likely is that the gradients\nobtained via RL are insufficient to train deep perception\narchitectures (i.e., ResNet) required for driving. Models used\nin benchmarks like Atari, where RL succeeds, are relatively\nshallow, consisting of only a few layers [78].\nRL has been successfully applied in end-to-end driving\nwhen combined with supervised learning (SL). Implicit\n5\naffordances [18, 19] pre-train the CNN encoder using SL\nwith tasks like semantic segmentation. In the second stage,\nthis encoder is frozen, and a shallow policy head is trained\non the features from the frozen encoder with a modern\nversion of Q-learning [79]. RL can also be used to finetune\nfull networks that were pre-trained using IL [17, 80].\nRL can also been effectively applied, if the network has\naccess to privileged simulator information. [48, 81, 82]. Privi-\nleged RL agents can be used for dataset curation. Roach [21]\ntrains an RL agent on privileged BEV semantic maps and\nuses the policy to automatically collect a dataset with which\na downstream IL agent is trained. WoR [20] employs a\nQ-function and tabular dynamic programming to generate\nadditional or improved labels for a static dataset.\nA challenge in the field is to transfer the findings from\nsimulation to the real world. In RL, the objective is ex-\npressed as reward functions, and many algorithms require\nthem to be dense and provide feedback at each environment\nstep. Current works typically use simple objectives, such\nas progress and collision avoidance. These simplistic de-\nsigns potentially encourage risky behaviors [81]. Devising or\nlearning better reward functions remains an open problem.\nAnother direction would be to develop RL algorithms that\ncan handle sparse rewards, enabling the optimization of\nrelevant metrics directly. RL can be effectively combined\nwith world models [83, 84, 85], though this presents specific\nchallenges (See Sec. 4.3). Current RL solutions for driv-\ning rely heavily on low-dimensional representations of the\nscene, and this issue is further discussed in Sec. 4.2.2.\n3\nBENCHMARKING\nAutonomous driving systems require a comprehensive eval-\nuation to ensure safety. Researchers must benchmark these\nsystems using appropriate datasets, simulators, metrics, and\nhardware to accomplish this. This section delineates three\napproaches for benchmarking end-to-end autonomous driv-\ning systems: (1) real-world evaluation, (2) online or closed-\nloop evaluation in simulation, and (3) offline or open-loop\nevaluation on driving datasets. We focus on the scalable and\nprincipled online simulation setting and summarize real-\nworld and offline assessments for completeness.\n3.1\nReal-world Evaluation\nEarly efforts on benchmarking self-driving involved real-\nworld evaluation. Notably, DARPA initiated a series of\nraces. The first event offered $1M in prize money for au-\ntonomously navigating a 240km route through the Mojave\ndesert, which no team achieved [86]. The final series event,\ncalled the DARPA Urban Challenge, required vehicles to\nnavigate a 96km mock-up town course, adhering to traf-\nfic laws and avoiding obstacles [87]. These races fostered\nimportant developments in self-driving, such as LiDAR\nsensors. Following this spirit, the University of Michigan\nestablished MCity [88], a large controlled real-world envi-\nronment to facilitate testing autonomous vehicles. However,\nsuch academic ventures have not been widely employed for\nend-to-end systems due to a lack of data and vehicles. In\ncontrast, industries with the resources to deploy fleets of\ndriverless vehicles could rely on real-world evaluation to\nbenchmark improvements in their algorithms.\n3.2\nOnline/Closed-loop Simulation\nConducting tests of self-driving systems in the real world is\ncostly and risky. To address this challenge, simulation is a\nviable alternative [14, 89, 90, 91, 92, 93]. Simulators facilitate\nrapid prototyping and testing, enable the quick iteration of\nideas, and provide low-cost access to diverse scenarios for\nunit testing. In addition, simulators offer tools for measuring\nperformance accurately. However, their primary disadvan-\ntage is that the results obtained in a simulated environment\ndo not necessarily generalize to the real world (Sec. 4.9.3).\nClosed-loop evaluation involves building a simulated\nenvironment that closely mimics a real-world driving en-\nvironment. The evaluation entails deploying the driving\nsystem in simulation and measuring its performance. The\nsystem has to navigate safely through traffic while progress-\ning toward a designated goal location. There are four main\nsub-tasks involved in developing such simulators: parame-\nter initialization, traffic simulation, sensor simulation, and\nvehicle dynamics simulation. We briefly describe these sub-\ntasks below, followed by a summary of currently available\nopen-source simulators for closed-loop benchmarks.\n3.2.1\nParameter Initialization\nSimulation offers the benefit of a high degree of control\nover the environment, including weather, maps, 3D assets,\nand low-level attributes such as the arrangement of objects\nin a traffic scene. While powerful, the number of these\nparameters is substantial, resulting in a challenging design\nproblem. Current simulators tackle this in two ways:\nProcedural Generation: Traditionally, initial parameters\nare hand-tuned by 3D artists and engineers [89, 90, 91, 92].\nThis limits scalability. Recently, some of the simulation\nproperties can be sampled from a probabilistic distribution\nwith computer algorithms, which we refer to as procedural\ngeneration [94]. Procedural generation algorithms combine\nrules, heuristics, and randomization to create diverse road\nnetworks, traffic patterns, lighting conditions, and object\nplacements [95, 96]. Due to its efficiency compared to fully\nmanual design, it has become one of the most commonly\nused methods of initialization for video games and sim-\nulations. Nevertheless, the process still needs pre-defined\nparameters and algorithms to control generation reliability,\nwhich is time-consuming and requires a lot of expertise.\nData-Driven: Data-driven approaches for simulation ini-\ntialization aim to learn the required parameters. Arguably,\nthe simplest way is to sample from real-world driving\nlogs [14, 93], where parameters such as road maps or traffic\npatterns are directly extracted from pre-recorded datasets.\nThe advantage of log sampling is its ability to capture\nthe natural variability present in real-world data, leading\nto more realistic simulation scenarios. However, it may\nnot encompass rare situations that are critical for testing\nthe robustness of autonomous driving systems. The initial\nparameters can be optimized to increase the representation\nof such scenarios [7, 34, 35]. Another advanced data-driven\napproach to initialization is generative modeling, where ma-\nchine learning algorithms are utilized to learn the underly-\ning structure and distributions of real-world data. They can\nthen generate novel scenarios that resemble the real world\nbut were not included in the original data [97, 98, 99, 100].\n6\n3.2.2\nTraffic Simulation\nTraffic simulation involves generating and positioning vir-\ntual entities in the environment with realistic motion [98,\n101]. These entities often include vehicles (such as cars, mo-\ntorcycles, bicycles, etc.) and pedestrians. Traffic simulators\nmust account for the effects of speed, acceleration, braking,\nobstructions, and the behavior of other entities. Moreover,\ntraffic light states must be periodically updated to simulate\nrealistic city driving. There are two popular approaches for\ntraffic simulation, which we describe below.\nRule-Based:\nRule-based\ntraffic\nsimulators\nuse\npre-\ndefined rules to generate the motion of traffic entities. The\nmost prominent implementation of this concept is the In-\ntelligent Driver Model (IDM) [102]. IDM is a car-following\nmodel that computes acceleration for each vehicle based on\nits current speed, the speed of the leading vehicle, and a de-\nsired safety distance. Although widely used and straightfor-\nward, this approach may be inadequate to simulate realistic\nmotion and complex interactions in urban environments.\nData-Driven: Realistic human traffic behavior is highly\ninteractive and complex, including lane changing, merging,\nsudden stopping, etc. To model such behavior, data-driven\ntraffic simulation utilizes data collected from real-world\ndriving. These models can capture more nuanced, realistic\nbehavior but require significant amounts of labeled data for\ntraining. A wide variety of learning-based techniques have\nbeen proposed for this task [98, 99, 101, 103, 104, 105].\n3.2.3\nSensor Simulation\nSensor simulation is crucial for evaluating end-to-end self-\ndriving systems. This involves generating simulated raw\nsensor data, such as camera images or LiDAR scans that\nthe driving system would receive from different viewpoints\nin the simulator [106, 107, 108]. This process needs to take\ninto account noise and occlusions to realistically assess the\nautonomous system. There are two main branches of ideas\nconcerning sensor simulation, as described below.\nGraphics-Based: Recent computer graphics simulators\nuse 3D models of the environment, along with traffic entity\nmodels, to generate sensor data via approximations of phys-\nical rendering processes in the sensors [90, 91]. For example,\nthis can involve occlusions, shadows, and reflections present\nin real-world environments while simulating camera im-\nages. However, the realism of graphics-based simulation is\noften subpar or comes at the cost of heavy computation,\nmaking parallelization non-trivial [109]. It is closely tied to\nthe quality of the 3D models and the approximations used in\nmodeling the sensors. A comprehensive survey of graphics-\nbased rendering for driving data is provided in [110].\nData-Driven: Data-driven sensor simulation leverages\nreal-world sensor data to create the simulation where both\nthe ego vehicle and background traffic may move differently\nfrom the way they did in recordings [111, 112, 113]. Popular\nmethods are Neural Radiance Fields (NeRF) [114] and 3D\nGaussian Splatting [115], which can generate novel views of\na scene by learning an implicit representation of the scene\u2019s\ngeometry and appearance. These methods can produce\nmore realistic sensor data visually than graphics-based ap-\nproaches, but they have limitations such as high rendering\ntimes or requiring independent training for each scene being\nSimulator Benchmarks\nCARLA\nCoRL\n[91],\nnoCrash\n[124],\nTown05\n[6],\nLAV\n[52],\nRoach [21], Longest6 [28], Leaderboard v1 and v2 [13]\nnuPlan\nNAVSIM [125], Val14 [126], Leaderboard [14]\nTABLE 1: Open-source Simulators with active benchmarks\nfor closed-loop evaluation of autonomous driving.\nreconstructed [108, 116, 117, 118, 119]. Another approach to\ndata-driven sensor simulation is domain adaptation, which\naims to minimize the gap between real and graphics-based\nsimulated sensor data [120]. Deep learning techniques such\nas GANs can be employed to improve realism (Sec. 4.9.3).\n3.2.4\nVehicle Dynamics Simulation\nThe final aspect of driving simulation pertains to ensur-\ning that the simulated vehicle adheres to physically plau-\nsible motion. Most existing publicly available simulators\nuse highly simplified vehicle models, such as the unicycle\nmodel [121] or the bicycle model [122]. However, in order\nto facilitate seamless transfer of algorithms from simulation\nto the real world, it is essential to incorporate more accu-\nrate physical modeling of vehicle dynamics. For instance,\nCARLA adopts a multi-body system approach, representing\na vehicle as a collection of sprung masses on four wheels.\nFor a comprehensive review, please refer to [123].\n3.2.5\nBenchmarks\nWe give a succinct overview of end-to-end driving bench-\nmarks available up to date in Table 1. In 2019, the original\nbenchmark released with CARLA [91] was solved with near-\nperfect scores [5]. The subsequent NoCrash benchmark [124]\ninvolves training on a single CARLA town under specific\nweather conditions and testing generalization to another\ntown and set of weathers. Instead of a single town, the\nTown05 benchmark [6] involves training on all available\ntowns while withholding Town05 for testing. Similarly, the\nLAV benchmark trains on all towns except Town02 and\nTown05, which are both reserved for testing. Roach [21] uses\na setting with 3 test towns, albeit all seen during training,\nand without the safety-critical scenarios in Town05 and LAV.\nFinally, the Longest6 benchmark [28] uses 6 test towns. Two\nonline servers, the leaderboard (v1 and v2) [13], ensure\nfair comparisons by keeping evaluation routes confidential.\nLeaderboard v2 is highly challenging due to the long route\nlength (over 8km on average, as opposed to 1-2km on v1)\nand a wide variety of new traffic scenarios.\nThe nuPlan simulator is currently accessible for eval-\nuating end-to-end systems via the NAVSIM project [125].\nFurther, there are two benchmarks on which agents input\nmaps and object properties via the data-driven parameter\ninitialization for nuPlan (Sec. 3.2.1). Val14, proposed in [126],\nuses a validation split of nuPlan. The leaderboard, a submis-\nsion server with the private test set, was used in the 2023\nnuPlan challenge, but it is no longer public for submissions.\n3.3\nOffline/Open-loop Evaluation\nOpen-loop evaluation mainly assesses a system\u2019s perfor-\nmance against pre-recorded expert driving behavior. This\n7\nmethod requires evaluation datasets that include (1) sensor\nreadings, (2) goal locations, and (3) corresponding future\ndriving trajectories, usually obtained from human drivers.\nGiven sensor inputs and goal locations as inputs, perfor-\nmance is measured by comparing the system\u2019s predicted\nfuture trajectory against the trajectory in the driving log.\nSystems are evaluated based on how closely their trajec-\ntory predictions match the human ground truth, as well\nas auxiliary metrics such as the collision probability with\nother agents. The advantage of open-loop evaluation is that\nit is easy to implement using realistic traffic and sensor\ndata, as it does not require a simulator. However, the key\ndisadvantage is that it does not measure performance in\nthe actual test distribution encountered during deployment.\nDuring testing, the driving system may deviate from the\nexpert driving corridor, and it is essential to verify the\nsystem\u2019s ability to recover from such drift (Sec. 4.9.2).\nFurthermore, the distance between the predicted and the\nrecorded trajectories is not an ideal metric in a multi-modal\nscenario. For example, in the case of merging into a turning\nlane, both the options of merging immediately or later could\nbe valid, but open-loop evaluation penalizes the option that\nwas not observed in the data. Therefore, besides measuring\ncollision probability and prediction errors, a few metrics\nwere proposed to cover more comprehensive aspects such\nas traffic violations, progress, and driving comfort [126].\nThis approach requires comprehensive datasets of trajec-\ntories to draw from. The most popular datasets for this pur-\npose include nuScenes [127], Argoverse [128], Waymo [129],\nand nuPlan [14]. All of these datasets comprise a large num-\nber of real-world driving traversals with varying degrees\nof difficulty. However, open-loop results do not provide\nconclusive evidence of improved driving behavior in closed-\nloop, due to the aforementioned drawbacks [124, 126, 130,\n131]. Overall, a realistic closed-loop benchmarking, if avail-\nable and applicable, is recommended in future research.\n4\nCHALLENGES\nFollowing each topic illustrated in Fig. 1, we now walk\nthrough current challenges, related works or potential reso-\nlutions, risks, and opportunities. We start with challenges in\nhandling different input modalities in Sec. 4.1, followed by a\ndiscussion on visual abstraction for efficient policy learning\nin Sec. 4.2. Further, we introduce learning paradigms such\nas world model learning (Sec. 4.3), multi-task frameworks\n(Sec. 4.4), and policy distillation (Sec. 4.5). Finally, we dis-\ncuss general issues that impede safe and reliable end-to-end\nautonomous driving, including interpretability in Sec. 4.6,\nsafety guarantees in Sec. 4.7, causal confusion in Sec. 4.8,\nand robustness in Sec. 4.9.\n4.1\nDilemma over Sensing and Input Modalities\n4.1.1\nSensing and Multi-sensor Fusion\nSensing: Though early work [8] successfully achieved fol-\nlowing a lane with a monocular camera, this single input\nmodality cannot handle complex scenarios. Therefore, var-\nious sensors in Fig. 4 have been introduced for recent self-\ndriving vehicles. Particularly, RGB images from cameras\nreplicate how humans perceive the world, with abundant\nconcat /\nattention\nVisual\nSensors\nNavigation\nSignal\nLanguage\nInstruction\nHD Maps\nEarly Fusion\nMiddle Fusion\nLate Fusion\ncommand\nfused\ninput\n(a) Input modality\n(b) Fusion strategy\nVehicle\nStates\nfused\nfeature\nfused\noutput\nFig. 4: Examples of input modality and fusion strategy.\nDifferent modalities have distinct characteristics, leading to\nthe challenge of effective sensor fusion. We take point clouds\nand images as examples to depict various fusion strategies.\nsemantic details; LiDARs or stereo cameras provide accu-\nrate 3D spatial knowledge. Emerging sensors like mmWave\nradars and event cameras excel at capturing objects\u2019 relative\nmovement. Additionally, vehicle states from speedometers\nand IMUs, together with navigation commands, are other\nlines of input that guide the driving system. However,\nvarious sensors possess distinct perspectives, data distri-\nbutions, and huge price gaps, thereby posing challenges in\neffectively designing the sensory layout and fusing them to\ncomplement each other for autonomous driving.\nMulti-sensor fusion has predominantly been discussed\nin perception-related fields, e.g., object detection [132, 133]\nand semantic segmentation [134, 135], and is typically cate-\ngorized into three groups: early, mid, and late fusion. End-\nto-end autonomous driving algorithms explore similar fu-\nsion schemes. Early fusion combines sensory inputs before\nfeeding them into shared feature extractors, where concate-\nnation is a common way for fusion [32, 136, 137, 138, 139].\nTo resolve the view discrepancy, some works project point\nclouds on images [140] or vice versa (predicting semantic\nlabels for LiDAR points [52, 141]). On the other hand, late\nfusion combines multiple results from multi-modalities. It is\nless discussed due to its inferior performance [6, 142]. Con-\ntrary to these methods, middle fusion achieves multi-sensor\nfusion within the network by separately encoding inputs\nand then fusing them at the feature level. Naive concate-\nnation is also frequently adopted [15, 22, 30, 143, 144, 145,\n146, 147]. Recently, works have employed Transformers [27]\nto model interactions among features [6, 28, 29, 148, 149].\nThe attention mechanism in Transformers has demonstrated\ngreat effectiveness in aggregating the context of different\nsensor inputs and achieving safer end-to-end driving.\nInspired by the progress in perception, it is beneficial\nto model modalities in a unified space such as BEV [132,\n133]. End-to-end driving also requires identifying policy-\nrelated contexts and discarding irrelevant details. We dis-\ncuss perception-based representations in Sec. 4.2.1. Besides,\nthe self-attention layer, interconnecting all tokens freely, in-\n8\ncurs a significant computational cost and cannot guarantee\nuseful information extraction. Advanced Transformer-based\nfusion mechanisms in the perception field, such as [150, 151],\nhold promise for application to the end-to-end driving task.\n4.1.2\nLanguage as Input\nHumans drive using both visual perception and intrinsic\nknowledge which together form causal behaviors. In areas\nrelated to autonomous driving such as embodied AI, incor-\nporating natural language as fine-grained knowledge and\ninstructions to control the visuomotor agent has achieved\nnotable progress [152, 153, 154, 155]. However, compared\nto robotic applications, the driving task is more straight-\nforward without the need for task decomposition, and the\noutdoor environment is much more complex with highly\ndynamic agents but few distinctive anchors for grounding.\nTo incorporate linguistic knowledge into driving, a few\ndatasets are proposed to benchmark outdoor grounding\nand visual language navigation tasks [156, 157, 158, 159].\nHAD [160] takes human-to-vehicle advice and adds a visual\ngrounding task. Sriram et al. [161] translate natural language\ninstructions into high-level behaviors, while [162, 163] di-\nrectly ground the texts. CLIP-MC [164] and LM-Nav [165]\nutilize CLIP [166] to extract both linguistic knowledge from\ninstructions and visual features from images.\nRecently, observing the rapid development of large lan-\nguage models (LLMs) [167, 168], works encode the per-\nceived scene into tokens and prompt them to LLMs for con-\ntrol prediction and text-based explanations [169, 170, 171].\nResearchers also formulate the driving task as a question-\nanswering problem and construct corresponding bench-\nmarks [172, 173]. They highlight that LLMs offer opportu-\nnities to handle sophisticated instructions and generalize to\ndifferent data domains, which share similar advantages to\napplications in robotic areas [174]. However, LLMs for on-\nroad driving could be challenging at present, considering\ntheir long inference time, low quantitative accuracy, and\ninstability of outputs. Potential resolutions could be employ-\ning LLMs on the cloud specifically for complex scenarios\nand using them solely for high-level behavior prediction.\n4.2\nDependence on Visual Abstraction\nEnd-to-end autonomous driving systems roughly have two\nstages: encoding the state into a latent feature representa-\ntion, and then decoding the driving policy with intermedi-\nate features. In urban driving, the input state, i.e., the sur-\nrounding environment and ego state, is much more diverse\nand high-dimensional compared to common policy learning\nbenchmarks such as video games [18, 175], which might lead\nto the misalignment between representations and necessary\nattention areas for policy making. Hence, it is helpful to de-\nsign \u201cgood\u201d intermediate perception representations, or first\npre-train visual encoders using proxy tasks. This enables the\nnetwork to extract useful information for driving effectively,\nthus facilitating the subsequent policy stage. Furthermore,\nthis can improve the sample efficiency for RL methods.\n4.2.1\nRepresentation Design\nNaive representations are extracted with various backbones.\nClassic convolutional neural networks (CNNs) still domi-\nnate, with advantages in translation equivariance and high\nefficiency [176]. Depth-pre-trained CNNs [177] significantly\nboost perception and downstream performance. In con-\ntrast, Transformer-based feature extractors [178, 179] show\ngreat scalability in perception tasks while not being widely\nadopted for end-to-end driving yet. For driving-specific rep-\nresentations, researchers introduce the concept of bird\u2019s-eye-\nview (BEV), fusing different sensor modalities and temporal\ninformation within a unified 3D space [133, 180, 181]. It also\nfacilitates easy adaptions to downstream tasks [2, 30, 182,\n183]. In addition, grid-based 3D occupancy is developed to\ncapture irregular objects and used for collision avoidance\nin planning [184]. Nevertheless, the dense representation\nbrings huge computation costs compared to BEV methods.\nAnother unsettled problem is representations of the map.\nTraditional autonomous driving relies on HD Maps. Due to\nthe high cost of availability of HD Maps, online mapping\nmethods have been devised with different formulations,\nsuch as BEV segmentation [185], vectorized lanlines [186],\ncenterlines and their topology [187, 188], and lane seg-\nments [189]. However, the most suitable formulation for\nend-to-end systems remains unvalidated.\nThough various representation designs offer possibilities\nof how to design the subsequent decision-making process,\nthey also place challenges as co-designing both parts is\nnecessary for a whole framework. Besides, given the trends\nobserved in several simple yet effective approaches with\nscaling up training resources [22, 28], the ultimate necessity\nof explicit representations such as maps is uncertain.\n4.2.2\nRepresentation Learning\nRepresentation learning often incorporates certain inductive\nbiases or prior information. There inevitably exist possible\ninformation bottlenecks in the learned representation, and\nredundant context unrelated to decisions may be removed.\nSome early methods directly utilize semantic segmen-\ntation masks from off-the-shelf networks as the input\nrepresentation for subsequent policy training [190, 191].\nSESR [192] further encodes segmentation masks into class-\ndisentangled representations through a VAE [193]. In [194,\n195], predicted affordance indicators, such as traffic light\nstates, offset to the lane center, and distance to the leading\nvehicle, are used as representations for policy learning.\nObserving that results like segmentation as representa-\ntions can create bottlenecks defined by humans and result in\nloss of useful information, some have chosen intermediate\nfeatures from pre-training tasks as effective representations\nfor RL training [18, 19, 196, 197]. In [198], latent features\nin VAE are augmented by attention maps obtained from\nthe diffused boundary of segmentation and depth maps to\nhighlight important regions. TARP [199] utilizes data from\na series of previous tasks to perform different tasks-related\nprediction tasks to acquire useful representations. In [200],\nthe latent representation is learned by approximating the\n\u03c0-bisimulation metric, which is comprised of differences\nof rewards and outputs from the dynamics model. ACO\n[36] learns discriminative features by adding steering angle\ncategorization into the contrastive learning structure. Re-\ncently, PPGeo [12] proposes to learn effective representation\nthrough motion prediction together with depth estimation\nin a self-supervised way on uncalibrated driving videos.\nViDAR [201] utilizes the raw image-point cloud pairs and\n9\npretrains the visual encoder with a point cloud forecasting\npre-task. These works demonstrate that self-supervised rep-\nresentation learning from large-scale unlabeled data for pol-\nicy learning is promising and worthy of future exploration.\n4.3\nComplexity of World Modeling for Model-based RL\nBesides the ability to better abstract perceptual representa-\ntions, it is essential for end-to-end models to make reason-\nable predictions about the future to take safe maneuvers.\nIn this section, we mainly discuss the challenges of current\nmodel-based policy learning works, where a world model\nprovides explicit future predictions for the policy model.\nDeep RL typically suffers from the high sample complex-\nity, which is pronounced in autonomous driving. Model-\nbased reinforcement learning (MBRL) offers a promising\ndirection to improve sample efficiency by allowing agents to\ninteract with the learned world model instead of the actual\nenvironment. MBRL methods employ an explicit world\n(environment) model, which is composed of transition dy-\nnamics and reward functions. This is particularly helpful in\ndriving, as simulators like CARLA are relatively slow.\nHowever, modeling the highly dynamic environment is\na challenging task. To simplify the problem, Chen et al. [20]\nfactor the transition dynamics into a non-reactive world\nmodel and a simple kinematic bicycle model. In [138], a\nprobabilistic sequential latent model is used as the world\nmodel. To address the potential inaccuracy of the learned\nworld model, Henaff et al. [202] train the policy network\nwith dropout regularization to estimate the uncertainty\ncost. Another approach [203] uses an ensemble of multiple\nworld models to provide uncertainty estimation, based on\nwhich imaginary rollouts could be truncated and adjusted\naccordingly. Motivated by Dreamer [83], ISO-Dream [204]\ndecouples visual dynamics into controllable and uncontrol-\nlable states, and trains the policy on the disentangled states.\nIt is worth noting that learning world models in raw im-\nage space is non-trivial for autonomous driving. Important\nsmall details, such as traffic lights, would easily be missed in\npredicted images. To tackle this, a few works [205, 206, 207]\nemploy the prevailing diffusion technique [208]. MILE [209]\nincorporates the Dreamer-style world model learning in the\nBEV segmentation space as an auxiliary task besides imita-\ntion learning. SEM2 [137] also extends the Dreamer struc-\nture but with BEV map inputs, and uses RL for training.\nBesides directly using the learned world model for MBRL,\nDeRL [197] combines a model-free actor-critic framework\nwith the world model, by fusing self-assessments of the\naction or state from both models.\nWorld model learning for end-to-end autonomous driv-\ning is an emerging and promising direction as it greatly\nreduces the sample complexity for RL, and understanding\nthe world is helpful for driving. However, as the driving\nenvironment is highly complex and dynamic, further study\nis still needed to determine what needs to be modeled and\nhow to model the world effectively.\n4.4\nReliance on Multi-Task Learning\nMulti-task learning (MTL) involves jointly performing sev-\neral related tasks based on a shared representation through\nExpert\nPrivileged \nagent\nSensorimotor \nagent\nFeature\nDistillation\nStrong \nSupervision\nStrong \nSupervision\n(a) Privileged agent training\n(b) Sensorimotor agent training\nFig. 5: Policy distillation. (a) The privileged agent learns a\nrobust policy with access to privileged ground-truth infor-\nmation. The expert is labeled with dashed lines to indicate\nthat it is not mandatory if the privileged agent is trained\nvia RL. (b) The sensorimotor agent imitates the privileged\nagent through both feature distillation and output imitation.\nseparate heads. MTL provides advantages such as computa-\ntional cost reduction, the sharing of relevant domain knowl-\nedge, and the ability to exploit task relationships to improve\nmodel\u2019s generalization ability [210]. Consequently, MTL is\nwell-suited for end-to-end driving, where the ultimate pol-\nicy prediction requires a comprehensive understanding of\nthe environment. However, the optimal combination of aux-\niliary tasks and appropriate weighting of losses to achieve\nthe best performance presents a significant challenge.\nIn contrast to common vision tasks where dense predic-\ntions are closely correlated, end-to-end driving predicts a\nsparse signal. The sparse supervision increases the difficulty\nof extracting useful information for decision-making in the\nencoder. For image input, auxiliary tasks such as semantic\nsegmentation [28, 31, 140, 211, 212, 213] and depth estima-\ntion [28, 31, 211, 212, 213] are commonly adopted in end-\nto-end autonomous driving models. Semantic segmentation\nhelps the model gain a high-level understanding of the\nscene; depth estimation enables the model to capture the 3D\ngeometry of the environment and better estimate distances\nto critical objects. Besides auxiliary tasks on perspective im-\nages, 3D object detection [28, 31, 52] is also useful for LiDAR\nencoders. As BEV becomes a natural and popular represen-\ntation for autonomous driving, tasks such as BEV segmen-\ntation are included in models [11, 23, 28, 29, 30, 31, 52, 149]\nthat aggregate features in BEV space. Moreover, in addi-\ntion to these vision tasks, [29, 211, 214] also predict visual\naffordances including traffic light states, distances to oppo-\nsite lanes, etc. Nonetheless, constructing large-scale datasets\nwith multiple types of aligned and high-quality annotations\nis non-trivaial for real-world applications, which remain as\na great concern due to current models\u2019 reliance on MTL.\n4.5\nInefficient Experts and Policy Distillation\nAs imitation learning, or its predominant sub-category, be-\nhavior cloning, is simply supervised learning that mimics\nexpert behaviors, corresponding methods usually follow the\n\u201cTeacher-Student\u201d paradigm. There lie two main challenges:\n10\n(1) Teachers, such as the handcrafted expert autopilot pro-\nvided by CARLA, are not perfect drivers, though having\naccess to ground-truth states of surrounding agents and\nmaps. (2) Students are supervised by the recorded output\nwith sensor input only, requiring them to extract perceptual\nfeatures and learn policy from scratch simultaneously.\nA few studies propose to divide the learning process\ninto two stages, i.e., training a stronger teacher network and\nthen distilling the policy to the student. In particular, Chen\net al. [5, 52] first employ a privileged agent to learn how to\nact with access to the state of the environment, then let the\nsensorimotor agent (student) closely imitate the privileged\nagent with distillation at the output stage. More compact\nBEV representations as input for the privileged agent pro-\nvide stronger generalization abilities and supervision than\nthe original expert. The process is depicted in Fig. 5.\nApart from solely supervising planning results, several\nworks also distill knowledge at the feature level. For exam-\nple, FM-Net [215] employs segmentation and optical flow\nmodels as auxiliary teachers to guide feature training. SAM\n[216] adds L2 feature loss between teacher and student\nnetworks, while CaT [23] aligns features in BEV. WoR [20]\nlearns a model-based action-value function and then uses\nit to supervise the visuomotor policy. Roach [21] trains a\nstronger privileged expert with RL, eliminating the upper\nbound of BC. It incorporates multiple distillation targets,\ni.e., action distribution, values/rewards, and latent features.\nBy leveraging the powerful RL expert, TCP [22] achieves\na new state-of-the-art on the CARLA leaderboard with a\nsingle camera as visual input. DriveAdpater [182] learns\na perception-only student and adapters with the feature\nalignment objective. The decoupled paradigm fully enjoys\nthe teacher\u2019s knowledge and student\u2019s training efficiency.\nThough huge efforts have been devoted to designing a\nrobust expert and transferring knowledge at various levels,\nthe teacher-student paradigm still suffers from inefficient\ndistillation. For instance, the privileged agent has access to\nground-truth states of traffic lights, which are small objects\nin images and thus hard to distill corresponding features.\nAs a result, the visuomotor agents exhibit large performance\ngaps compared to their privileged agents. It may also lead\nto causal confusion for students (see Sec. 4.8). It is worth\nexploring how to draw more inspiration from general distil-\nlation methods in machine learning to minimize the gap.\n4.6\nLack of Interpretability\nInterpretability plays a critical role in autonomous driving\n[217]. It enables engineers to better debug the system, pro-\nvides performance guarantees from a societal perspective,\nand promotes public acceptance. Achieving interpretability\nfor end-to-end driving models, which are often referred to\nas \u201cblack boxes\u201d, is more essential and challenging.\nGiven trained models, some post-hoc X-AI (explainable\nAI) techniques could be applied to gain saliency maps [211,\n218, 219, 220, 221]. Saliency maps highlight specific regions\nin the visual input on which the model primarily relies for\nplanning. However, this approach provides limited infor-\nmation, and its effectiveness and validity are difficult to\nevaluate. Instead, we focus on end-to-end frameworks that\ndirectly enhance interpretability in their model design. We\nintroduce each category of interpretability in Fig. 6 below.\nInterpretability\nfor E2E AD\nAttention\nInterpretable \nTasks\nCost Learning\nNatural \nLanguage\nUncertainty \nModeling\nLearned Attention \nWeights\nTransformer\nDense Seg /\nDepth\nObject Det / Pred\nOccupancy\nMotion Fields\nAction Description\nAction Explanation\nAleatoric / Data \nUncertainty\nEpistemic / Model \nUncertainty\nThe car is driving \nforward as there is \nnoting to impede it.\nFig. 6: Summary of the different forms of interpretability.\nThey aid in human comprehension of decision-making pro-\ncesses of end-to-end models and the reliability of outputs.\nAttention Visualization: The attention mechanism pro-\nvides a certain degree of interpretability. In [33, 211, 214, 221,\n222], a learned attention weight is applied to aggregate im-\nportant features from intermediate feature maps. Attention\nweights can also adaptively combine ROI pooled features\nfrom different object regions [223] or a fixed grid [224].\nNEAT [11] iteratively aggregates features to predict atten-\ntion weights and refine the aggregated feature. Recently,\nTransformer attention blocks are employed to better fuse\ndifferent sensor inputs, and attention maps display impor-\ntant regions in the input for driving decisions [28, 29, 31, 148,\n225]. In PlanT [226], attention layers process features from\ndifferent vehicles, providing interpretable insights into the\ncorresponding action. Similar to post-hoc saliency methods,\nalthough attention maps offer straightforward clues about\nmodels\u2019 focus, their faithfulness and utility remain limited.\nInterpretable Tasks: Many IL-based works introduce in-\nterpretability by decoding the latent feature representations\ninto other meaningful information besides policy prediction,\nsuch as semantic segmentation [2, 11, 15, 28, 29, 31, 52, 140,\n164, 211, 212, 213, 227], depth estimation [15, 28, 31, 211,\n212], object detection [2, 28, 31, 52], affordance predictions\n[29, 211, 214], motion prediction [2, 52], and gaze map esti-\nmation [228]. Although these methods provide interpretable\ninformation, most of them only treat these predictions as\nauxiliary tasks [11, 15, 28, 31, 140, 211, 212, 214], with no\nexplicit impact on final driving decisions. Some [29, 52] do\nuse these outputs for final actions, but they are incorporated\nsolely for performing an additional safety check.\nRules Integration and Cost Learning: As discussed in\nSec. 2.1.2, cost learning-based methods share similarities\nwith traditional modular systems and thus exhibit a certain\nlevel of interpretability. NMP [32] and DSDNet [229] con-\nstruct the cost volume in conjunction with detection and mo-\ntion prediction results. P3 [39] combines predicted semantic\noccupancy maps with comfort and traffic rules constraints\nto construct the cost function. Various representations, such\nas probabilistic occupancy and temporal motion fields [1],\nemergent occupancy [71], and freespace [70], are employed\nto score sampled trajectories. In [38, 126, 183, 230], human\nexpertise and pre-defined rules including safety, comfort,\n11\ntraffic rules, and routes based on perception and prediction\noutputs are explicitly included to form the cost for trajectory\nscoring, demonstrating improved robustness and safety.\nLinguistic Explainability: As one aspect of interpretabil-\nity is to help humans understand the system, natural lan-\nguage is a suitable choice for this purpose. Kim et al. [33] and\nXu et al. [231] develop datasets pairing driving videos or im-\nages with descriptions and explanations, and propose end-\nto-end models with both control and explanation outputs.\nBEEF [232] fuses the predicted trajectory and the intermedi-\nate perception features to predict justifications for the deci-\nsion. ADAPT [233] proposes a Transformer-based network\nto jointly estimate action, narration, and reasoning. Recently,\n[170, 172, 173] resort to the progress of multi-modality and\nfoundation models, using LLMs/VLMs to provide decision-\nrelated explanations, as discussed in Sec. 4.1.2.\nUncertainty Modeling: Uncertainty is a quantitative\napproach for interpreting the dependability of deep learning\nmodel outputs [234, 235], which can be helpful for designers\nand users to identify uncertain cases for improvement or\nnecessary intervention. For deep learning, there are two\ntypes of uncertainty: aleatoric uncertainty and epistemic\nuncertainty. Aleatoric uncertainty is inherent to the task,\nwhile epistemic uncertainty is due to limited data or mod-\neling capacity. In [236], authors leverage certain stochastic\nregularizations in the model to perform multiple forward\npasses as samples to measure the uncertainty. However, the\nrequirement of multiple forward passes is not feasible in\nreal-time scenarios. Loquercio et al. [235] and Filos et al. [237]\npropose capturing epistemic uncertainty with an ensemble\nof expert likelihood models and aggregating the results\nto perform safe planning. Regarding methods modeling\naleatoric uncertainty, driving actions/planning and uncer-\ntainty (usually represented by variance) are explicitly pre-\ndicted in [147, 238, 239]. Such methods directly model and\nquantify the uncertainty at the action level as a variable for\nthe network to predict. The planner would generate the final\naction based on the predicted uncertainty, either choosing\nthe action with the lowest uncertainty from multiple actions\n[238] or generating a weighted combination of proposed\nactions based on the uncertainties [147]. Currently, predicted\nuncertainty is mainly utilized in combination with hard-\ncoded rules. Exploring better ways to model and utilize\nuncertainty for autonomous driving is necessary.\n4.7\nLack of Safety Guarantees\nEnsuring safety is of utmost importance when deploying\nautonomous driving systems in real-world scenarios. How-\never, the learning-based nature of end-to-end frameworks\ninherently lacks precise mathematical guarantees regarding\nsafety, unlike traditional rule-based approaches [240].\nNevertheless, it should be noted that modular driving\nstacks have already incorporated specific safety-related con-\nstraints or optimizations within their motion planning or\nspeed prediction modules to enforce safety [241, 242, 243].\nThese mechanisms can potentially be adapted for integra-\ntion into end-to-end models as post-process steps or safety\nchecks, thereby providing additional safety guarantees. Fur-\nthermore, the intermediate interpretability predictions, as\ndiscussed in Sec. 4.6, such as detection and motion predic-\ntion results, can be utilized in post-processing procedures.\nBrake?\nInput image \u2208\u211d!\u00d7#\u00d7$\nVelocity \u2208\u211d%\nCorrelation\nDimension\nCorrelation\nDimension\n- I am braking because \nI see a red light.\n- I am braking because \nmy speed is low.\nFig. 7: Causal Confusion. The current action of a car is\nstrongly correlated with low-dimensional spurious features\nsuch as the velocity or the car\u2019s past trajectory. End-to-End\nmodels may latch on to them leading to causal confusion.\n4.8\nCausal Confusion\nDriving is a task that exhibits temporal smoothness, which\nmakes past motion a reliable predictor of the next action.\nHowever, methods trained with multiple frames can be-\ncome overly reliant on this shortcut [244] and suffer from\ncatastrophic failure during deployment. This problem is\nreferred to as the copycat problem [57] in some works and\nis a manifestation of causal confusion [245], where access to\nmore information leads to worse performance.\nCausal confusion in imitation learning has been a per-\nsistent challenge for nearly two decades. One of the earliest\nreports of this effect was made by LeCun et al. [246]. They\nused a single input frame for steering prediction to avoid\nsuch extrapolation. Though simplistic, this is still a preferred\nsolution in current state-of-the-art IL methods [22, 28]. Un-\nfortunately, using a single frame makes it hard to extract\nthe motion of surrounding actors. Another source of causal\nconfusion is speed measurement [16]. Fig. 7 showcases an\nexample of a car waiting at a red light. The action of the car\ncould highly correlate with its speed because it has waited\nfor many frames where the speed is zero and the action is\nthe brake. Only when the traffic light changes from red to\ngreen does this correlation break down.\nThere are several approaches to combat the causal con-\nfusion problem when using multiple frames. In [57], the\nauthors attempt to remove spurious temporal correlations\nfrom the bottleneck representation by training an adver-\nsarial model that predicts the ego agent\u2019s past action.\nIntuitively, the resulting min-max optimization trains the\nnetwork to eliminate its past from intermediate layers. It\nworks well in MuJoCo but does not scale to complex vision-\nbased driving. OREO [59] maps images to discrete codes\nrepresenting semantic objects and applies random dropout\nmasks to units that share the same discrete code, which\nhelps in confounded Atari. In end-to-end driving, Chauf-\nfeurNet [247] addresses the causal confusion issue by using\nthe past ego-motion as intermediate BEV abstractions and\ndropping out it with a 50% probability during training. Wen\net al. [58] propose upweighting keyframes in the training\nloss, where a decision change occurs (and hence are not pre-\ndictable by extrapolating the past). PrimeNet [60] improves\nperformance compared to keyframes by using an ensemble,\n12\nCovariate Shift\n(a) Long-tailed Distribution\n(b) Covariate Shift\n(c) Domain Adaptation\nSimulator\nReal world\nLocation A\nLocation B\nDay\nWeather B\nWeather A\nNight\nSensor A\nSensor B\nSource Domain\nTarget Domain\nExpert Trajectory\nLearned Policy\nNo data on \nhow to recover\nCommon\nnon-critical cases\nVarious rare but \nsafety-critical cases\nFig. 8: Challenges in robustness. Three primary generalization issues arise in relation to dataset distribution discrepancies,\nnamely long-tailed and normal cases, expert demonstration and test scenarios, and domain shift in locations, weather, etc.\nwhere the prediction of a single-frame model is given as\nadditional input to a multi-frame model. Chuang et al. [248]\ndo the same but supervise the multi-frame network with\naction residuals instead of actions. In addition, the problem\nof causal confusion can be circumvented by using only\nLiDAR histories (with a single frame image) and realigning\npoint clouds into one coordinate system. This removes ego-\nmotion while retaining information about other vehicles\u2019\npast states. This technique has been used in multiple works\n[1, 32, 52], though it was not motivated this way.\nHowever, these studies have used environments that are\nmodified to simplify studying the causal confusion problem.\nShowing performance improvements in state-of-the-art set-\ntings as mentioned in Sec. 3.2.5 remains an open problem.\n4.9\nLack of Robustness\n4.9.1\nLong-tailed Distribution\nOne important aspect of the long-tailed distribution prob-\nlem is dataset imbalance, where a few classes make up the\nmajority, as shown in Fig. 8 (a). This poses a big challenge\nfor models to generalize to diverse environments. Various\nmethods mitigate this issue with data processing, including\nover-sampling [249, 250], under-sampling [251, 252], and\ndata augmentation [253, 254]. Besides, weighting-based ap-\nproaches [255, 256] are also commonly used.\nIn the context of end-to-end autonomous driving, the\nlong-tailed distribution issue is particularly severe. Most\ndrives are repetitive and uninteresting e.g., following a\nlane for many frames. Conversely, interesting safety-critical\nscenarios occur rarely but are diverse in nature, and hard to\nreplicate in the real world for safety reasons. To tackle this,\nsome works rely on handcrafted scenarios [13, 101, 257, 258,\n259] to generate more diverse data in simulation. LBC [5]\nleverages the privileged agent to create imaginary supervi-\nsions conditioned on different navigational commands. LAV\n[52] includes trajectories of non-ego agents for training to\npromote data diversity. In [260], a simulation framework is\nproposed to apply importance-sampling strategies to accel-\nerate the evaluation of rare-event probabilities.\nAnother line of research [7, 34, 35, 261, 262, 263] gen-\nerates safety-critical scenarios in a data-driven manner\nthrough adversarial attacks. In [261], Bayesian Optimization\nis employed to generate adversarial scenarios. Learning to\ncollide [35] represents driving scenarios as the joint distri-\nbution over building blocks and applies policy gradient RL\nmethods to generate risky scenarios. AdvSim [34] modifies\nagents\u2019 trajectories to cause failures, while still adhering to\nphysical plausibility. KING [7] proposes an optimization\nalgorithm for safety-critical perturbations using gradients\nthrough differentiable kinematics models.\nIn general, efficiently generating realistic safety-critical\nscenarios that cover the long-tailed distribution remains a\nsignificant challenge. While many works focus on adversar-\nial scenarios in simulators, it is also essential to better utilize\nreal-world data for critical scenario mining and potential\nadaptation to simulation. Besides, a systematic, rigorous,\ncomprehensive, and realistic testing framework is crucial for\nevaluating end-to-end autonomous driving methods under\nthese long-tailed distributed safety-critical scenarios.\n4.9.2\nCovariate Shift\nAs discussed in Sec. 2.1, one important challenge for behav-\nior cloning is covariate shift. The state distributions from\nthe expert\u2019s policy and those from the trained agent\u2019s policy\ndiffer, leading to compounding errors when the trained\nagent is deployed in unseen testing environments or when\nthe reactions from other agents differ from training time.\nThis could result in the trained agent being in a state that\nis outside the expert\u2019s distribution for training, leading to\nsevere failures. An illustration is presented in Fig. 8 (b).\nDAgger (Dataset Aggregation) [26] is a common solution\nfor this issue. DAgger is an iterative training process. The\ncurrent trained policy is rolled out in each iteration to collect\nnew data, and the expert is used to label the visited states.\nThis enriches the dataset by adding examples of how to\nrecover from suboptimal states that an imperfect policy\nmight visit. The policy is then trained on the augmented\ndataset, and the process repeats. However, one downside of\nDAgger is the need for an available expert to query online.\nFor end-to-end autonomous driving, DAgger is adopted\nin [24] with an MPC-based expert. To reduce the cost of\nconstantly querying the expert, SafeDAgger [25] extends\nthe original DAgger algorithm by learning a safety policy\nthat estimates the deviation between the current policy\nand the expert policy. The expert is only queried when\nthe deviation is large. MetaDAgger [56] uses meta-learning\nwith DAgger to aggregate data from multiple environments.\nLBC [5] adopts DAgger and resamples the data with higher\nloss more frequently. In DARB [10], to better utilize failure\nor safety-related samples, it proposes several mechanisms,\nincluding task-based, policy-based, and policy & expert-\nbased mechanisms, to sample such critical states.\n13\n4.9.3\nDomain Adaptation\nDomain adaptation (DA) is a type of transfer learning in\nwhich the target task is the same as the source task, but the\ndomains differ. Here we discuss scenarios where labels are\navailable for the source domain while there are no labels or\na limited amount of labels available for the target domain.\nAs shown in Fig. 8 (c), domain adaptation for au-\ntonomous driving tasks encompasses several cases [264]:\n\u2022 Sim-to-real: the large gap between simulators used for\ntraining and the real world used for deployment.\n\u2022 Geography-to-geography: different geographic loca-\ntions with varying environmental appearances.\n\u2022 Weather-to-weather: changes in sensor inputs caused\nby weather conditions such as rain, fog, and snow.\n\u2022 Day-to-night: illumination variations in visual inputs.\n\u2022 Sensor-to-sensor: possible differences in sensor charac-\nteristics, e.g., resolution and relative position.\nNote that the aforementioned cases often overlap.\nTypically, domain-invariant feature learning is achieved\nwith image translators and discriminators to map images\nfrom two domains into a common latent space or repre-\nsentations like segmentation maps [265, 266]. LUSR [267]\nand UAIL [238] adopt a Cycle-Consistent VAE and GAN,\nrespectively, to project images into a latent representation\ncomprised of a domain-specific part and a domain-general\npart. In SESR [192], class disentangled encodings are ex-\ntracted from a semantic segmentation mask to reduce the\nsim-to-real gap. Domain randomization [268, 269, 270] is\nalso a simple and effective sim-to-real technique for RL\npolicy learning, which is further adapted for end-to-end\nautonomous driving [190, 271]. It is realized by randomizing\nthe rendering and physical settings of the simulators to\ncover the variability of the real world during training.\nCurrently, sim-to-real adaptation through source target\nimage mapping or domain-invariant feature learning is the\nfocus. Other DA cases are handled by constructing a diverse\nand large-scale dataset. Given that current methods mainly\nconcentrate on the visual gap in images, and LiDAR has be-\ncome a popular input modality for driving, specific adapta-\ntion techniques tailored for LiDARs must also be designed.\nBesides, traffic agents\u2019 behavior gaps between the simulator\nand the real world should be noticed as well. Incorporating\nreal-world data into simulation through techniques such as\nNeRF [114] is another promising direction.\n5\nFUTURE TRENDS\nConsidering the challenges and opportunities discussed, we\nlist some crucial directions for future research that may have\na broader impact in this field.\n5.1\nZero-shot and Few-shot Learning\nIt is inevitable for autonomous driving models to eventually\nencounter real-world scenarios that lie beyond the training\ndata distribution. This raises the question of whether we can\nsuccessfully adapt the model to an unseen target domain\nwhere limited or no labeled data is available. Formalizing\nthis task for the end-to-end driving domain and incor-\nporating techniques from the zero-shot/few-shot learning\nliterature are the key steps toward achieving this [272, 273].\n5.2\nModular End-to-end Planning\nThe modular end-to-end planning framework optimizes\nmultiple modules while prioritizing the ultimate planning\ntask, which enjoys the advantages of interpretability as\nindicated in Sec. 4.6. This is advocated in recent literature\n[2, 274] and certain industry solutions (Tesla, Wayve, etc.)\nhave involved similar ideas. When designing these differen-\ntiable perception modules, several questions arise regarding\nthe choice of loss functions, such as the necessity of 3D\nbounding boxes for object detection, whether opting for BEV\nsegmentation over lane topology for static scene perception,\nor the training strategies with limited modules\u2019 data.\n5.3\nData Engine\nThe importance of large-scale and high-quality data for\nautonomous driving can never be emphasized enough [275].\nEstablishing a data engine with an automatic labeling\npipeline [276] could greatly facilitate the iterative devel-\nopment of both data and models. The data engine for au-\ntonomous driving, especially modular end-to-end planning\nsystems, needs to streamline the process of annotating high-\nquality perception labels with the aid of large perception\nmodels in an automatic way. It should also support mining\nhard/corner cases, scene generation, and editing to facili-\ntate the data-driven evaluations discussed in Sec. 3.2 and\npromote diversity of data and the generalization ability of\nmodels (Sec. 4.9). A data engine would enable autonomous\ndriving models to make consistent improvements.\n5.4\nFoundation Model\nRecent advancements in foundation models in both lan-\nguage [167, 168] and vision [276, 277] have proved that\nlarge-scale data and model capacity can unleash the im-\nmense potential of AI in high-level reasoning tasks. The\nparadigm of finetuning [278] or prompt learning [279], opti-\nmization in the form of self-supervised reconstruction [280]\nor contrastive pairs [166], etc., are all applicable to the\nend-to-end driving domain. However, we contend that the\ndirect adoption of LLMs for driving might be tricky. The\noutput of an autonomous agent requires steady and accurate\nmeasurements, whereas the generative output in language\nmodels aims to behave like humans, irrespective of its\naccuracy. A feasible solution to develop a \u201cfoundation\u201d\ndriving model is to train a world model that can forecast\nthe reasonable future of the environment, either in 2D, 3D,\nor latent space. To perform well on downstream tasks like\nplanning, the objective to be optimized for the model needs\nto be sophisticated enough, beyond frame-level perception.\n6\nCONCLUSION AND OUTLOOK\nIn this survey, we provide an overview of fundamental\nmethodologies and summarize various aspects of simula-\ntion and benchmarking. We thoroughly analyze the exten-\nsive literature to date, and highlight a wide range of critical\nchallenges and promising resolutions.\nOutlook: The industry has dedicated considerable effort\nover the years to develop advanced modular-based sys-\ntems capable of achieving self-driving on highways. How-\never, these systems face significant challenges when con-\nfronted with complex scenarios, e.g., inner-city streets and\n14\nintersections. Therefore, an increasing number of compa-\nnies have started exploring end-to-end autonomous driving\ntechniques specifically tailored for these environments. It is\nenvisioned that with extensive high-quality data collection,\nlarge-scale model training, and the establishment of reliable\nbenchmarks, the end-to-end approach will have enormous\npotential over modular stacks in terms of performance and\neffectiveness. In summary, end-to-end autonomous driving\nfaces great opportunities and challenges simultaneously,\nwith the ultimate goal of building generalist agents. In this\nera of emerging technologies, we hope this survey could\nserve as a starting point to shed new light on this domain.\nACKNOWLEDGMENTS\nThis project is partially supported by National Key R&D\nProgram of China (2022ZD0160104), NSFC (62206172),\nand\nShanghai\nCommittee\nof\nScience\nand\nTechnology\n(23YF1462000). Andreas Geiger and Bernhard Jaeger are\nsupported by the ERC Starting Grant LEGO-3D (850533),\nthe BMWi in the project KI Delta Learning (project number\n19A19013O), and the DFG EXC number 2064/1 - project\nnumber 390727645. Kashyap Chitta is supported by the Ger-\nman Federal Ministry of Education and Research (BMBF):\nT\u00fcbingen AI Center, FKZ: 01IS18039A. We thank the Inter-\nnational Max Planck Research School for Intelligent Systems\nfor supporting Bernhard Jaeger and Kashyap Chitta.\nREFERENCES\n[1]\nS. Casas, A. Sadat, and R. Urtasun, \u201cMp3: A unified\nmodel to map, perceive, predict and plan,\u201d in CVPR,\n2021.\n[2]\nY. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai,\nS. Du, T. Lin, W. Wang, L. Lu, X. Jia, Q. Liu, J. Dai, Y. Qiao,\nand H. Li, \u201cPlanning-oriented autonomous driving,\u201d in\nCVPR, 2023.\n[3]\nD. A. Pomerleau, \u201cAlvinn: An autonomous land vehicle\nin a neural network,\u201d in NeurIPS, 1988.\n[4]\nA. Kendall, J. Hawke, D. Janz, P. Mazur, D. Reda, J.-M.\nAllen, V.-D. Lam, A. Bewley, and A. Shah, \u201cLearning to\ndrive in a day,\u201d in ICRA, 2019.\n[5]\nD. Chen, B. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cLearn-\ning by cheating,\u201d in CoRL, 2020.\n[6]\nA. Prakash, K. Chitta, and A. Geiger, \u201cMulti-modal fu-\nsion transformer for end-to-end autonomous driving,\u201d in\nCVPR, 2021.\n[7]\nN. Hanselmann, K. Renz, K. Chitta, A. Bhattacharyya,\nand A. Geiger, \u201cKing: Generating safety-critical driving\nscenarios for robust imitation via kinematics gradients,\u201d\nin ECCV, 2022.\n[8]\nM. Bojarski, D. Del Testa, D. Dworakowski, B. Firner,\nB. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller,\nJ. Zhang, et al., \u201cEnd to end learning for self-driving cars,\u201d\narXiv.org, vol. 1604.07316, 2016.\n[9]\nF. Codevilla, M. M\u00fcller, A. L\u00f3pez, V. Koltun, and A. Doso-\nvitskiy, \u201cEnd-to-end driving via conditional imitation\nlearning,\u201d in ICRA, 2018.\n[10]\nA. Prakash, A. Behl, E. Ohn-Bar, K. Chitta, and A. Geiger,\n\u201cExploring data aggregation in policy learning for vision-\nbased urban autonomous driving,\u201d in CVPR, 2020.\n[11]\nK. Chitta, A. Prakash, and A. Geiger, \u201cNeat: Neural\nattention fields for end-to-end autonomous driving,\u201d in\nICCV, 2021.\n[12]\nP. Wu, L. Chen, H. Li, X. Jia, J. Yan, and Y. Qiao, \u201cPolicy\npre-training for autonomous driving via self-supervised\ngeometric modeling,\u201d in ICLR, 2023.\n[13]\nCARLA, \u201cCARLA autonomous driving leaderboard.\u201d\nhttps://leaderboard.carla.org/, 2022.\n[14]\nH. Caesar, J. Kabzan, K. S. Tan, W. K. Fong, E. Wolff,\nA. Lang, L. Fletcher, O. Beijbom, and S. Omari, \u201cNu-\nplan: A closed-loop ml-based planning benchmark for\nautonomous vehicles,\u201d in CVPR Workshops, 2021.\n[15]\nJ. Hawke, R. Shen, C. Gurau, S. Sharma, D. Reda,\nN. Nikolov, P. Mazur, S. Micklethwaite, N. Griffiths,\nA. Shah, et al., \u201cUrban driving with conditional imitation\nlearning,\u201d in ICRA, 2020.\n[16]\nF. Codevilla, E. Santana, A. M. L\u00f3pez, and A. Gaidon,\n\u201cExploring the limitations of behavior cloning for au-\ntonomous driving,\u201d in ICCV, 2019.\n[17]\nX. Liang, T. Wang, L. Yang, and E. Xing, \u201cCirl: Control-\nlable imitative reinforcement learning for vision-based\nself-driving,\u201d in ECCV, 2018.\n[18]\nM. Toromanoff, E. Wirbel, and F. Moutarde, \u201cEnd-to-\nend model-free reinforcement learning for urban driving\nusing implicit affordances,\u201d in CVPR, 2020.\n[19]\nR.\nChekroun,\nM.\nToromanoff,\nS.\nHornauer,\nand\nF. Moutarde, \u201cGri: General reinforced imitation and\nits application to vision-based autonomous driving,\u201d\nRobotics, 2023.\n[20]\nD. Chen, V. Koltun, and P. Kr\u00e4henb\u00fchl, \u201cLearning to\ndrive from a world on rails,\u201d in ICCV, 2021.\n[21]\nZ. Zhang, A. Liniger, D. Dai, F. Yu, and L. Van Gool,\n\u201cEnd-to-end urban driving by imitating a reinforcement\nlearning coach,\u201d in ICCV, 2021.\n[22]\nP. Wu, X. Jia, L. Chen, J. Yan, H. Li, and Y. Qiao,\n\u201cTrajectory-guided control prediction for end-to-end au-\ntonomous driving: A simple yet strong baseline,\u201d in\nNeurIPS, 2022.\n[23]\nJ. Zhang, Z. Huang, and E. Ohn-Bar, \u201cCoaching a teach-\nable student,\u201d in CVPR, 2023.\n[24]\nY. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. A.\nTheodorou, and B. Boots, \u201cAgile autonomous driving\nusing end-to-end deep imitation learning,\u201d in RSS, 2017.\n[25]\nJ. Zhang and K. Cho, \u201cQuery-efficient imitation learning\nfor end-to-end simulated driving,\u201d in AAAI, 2017.\n[26]\nS. Ross, G. Gordon, and D. Bagnell, \u201cA reduction of\nimitation learning and structured prediction to no-regret\nonline learning,\u201d in AISTATS, 2011.\n[27]\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is\nall you need,\u201d in NeurIPS, 2017.\n[28]\nK. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and\nA. Geiger, \u201cTransfuser: Imitation with transformer-based\nsensor fusion for autonomous driving,\u201d PAMI, 2022.\n[29]\nH. Shao, L. Wang, R. Chen, H. Li, and Y. Liu, \u201cSafety-\nenhanced autonomous driving using interpretable sensor\nfusion transformer,\u201d in CoRL, 2022.\n[30]\nX. Jia, P. Wu, L. Chen, J. Xie, C. He, J. Yan, and H. Li,\n\u201cThink twice before driving: Towards scalable decoders\nfor end-to-end autonomous driving,\u201d in CVPR, 2023.\n[31]\nB. Jaeger, K. Chitta, and A. Geiger, \u201cHidden biases of end-\nto-end driving models,\u201d in ICCV, 2023.\n[32]\nW. Zeng, W. Luo, S. Suo, A. Sadat, B. Yang, S. Casas,\nand R. Urtasun, \u201cEnd-to-end interpretable neural motion\nplanner,\u201d in CVPR, 2019.\n[33]\nJ. Kim, A. Rohrbach, T. Darrell, J. Canny, and Z. Akata,\n\u201cTextual explanations for self-driving vehicles,\u201d in ECCV,\n2018.\n[34]\nJ. Wang, A. Pun, J. Tu, S. Manivasagam, A. Sadat,\nS. Casas, M. Ren, and R. Urtasun, \u201cAdvsim: Generat-\ning safety-critical scenarios for self-driving vehicles,\u201d in\nCVPR, 2021.\n[35]\nW. Ding, B. Chen, M. Xu, and D. Zhao, \u201cLearning to\ncollide: An adaptive safety-critical scenarios generating\nmethod,\u201d in IROS, 2020.\n[36]\nQ. Zhang, Z. Peng, and B. Zhou, \u201cLearning to drive by\n15\nwatching youtube videos: Action-conditioned contrastive\npolicy pretraining,\u201d in ECCV, 2022.\n[37]\nJ. Zhang, R. Zhu, and E. Ohn-Bar, \u201cSelfd: Self-learning\nlarge-scale driving policies from the web,\u201d in CVPR, 2022.\n[38]\nS. Hu, L. Chen, P. Wu, H. Li, J. Yan, and D. Tao, \u201cSt-p3:\nEnd-to-end vision-based autonomous driving via spatial-\ntemporal feature learning,\u201d in ECCV, 2022.\n[39]\nA. Sadat, S. Casas, M. Ren, X. Wu, P. Dhawan, and R. Ur-\ntasun, \u201cPerceive, predict, and plan: Safe motion plan-\nning through interpretable semantic representations,\u201d in\nECCV, 2020.\n[40]\nJ. Janai, F. G\u00fcney, A. Behl, and A. Geiger, \u201cComputer\nvision for autonomous vehicles: Problems, datasets and\nstate-of-the-art,\u201d arXiv.org, vol. 1704.05519, 2017.\n[41]\nA. Tampuu, T. Matiisen, M. Semikin, D. Fishman, and\nN. Muhammad, \u201cA survey of end-to-end driving: Archi-\ntectures and training methods,\u201d TNNLS, 2020.\n[42]\nS. Teng, X. Hu, P. Deng, B. Li, Y. Li, D. Yang, Y. Ai,\nL. Li, L. Chen, Z. Xuanyuan, et al., \u201cMotion planning\nfor autonomous driving: The state of the art and future\nperspectives,\u201d TIV, 2023.\n[43]\nD. Coelho and M. Oliveira, \u201cA review of end-to-end au-\ntonomous driving in urban environments,\u201d IEEE Access,\n2022.\n[44]\nA. O. Ly and M. Akhloufi, \u201cLearning to drive by imita-\ntion: An overview of deep behavior cloning methods,\u201d\nTIV, 2020.\n[45]\nL. Le Mero, D. Yi, M. Dianati, and A. Mouzakitis, \u201cA\nsurvey on imitation learning techniques for end-to-end\nautonomous vehicles,\u201d TITS, 2022.\n[46]\nB. Zheng, S. Verma, J. Zhou, I. W. Tsang, and F. Chen, \u201cIm-\nitation learning: Progress, taxonomies and challenges,\u201d\nTNNLS, 2022.\n[47]\nZ. Zhu and H. Zhao, \u201cA survey of deep RL and IL for\nautonomous driving policy learning,\u201d TITS, 2021.\n[48]\nB. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. A.\nSallab, S. K. Yogamani, and P. P\u00e9rez, \u201cDeep reinforcement\nlearning for autonomous driving: A survey,\u201d TITS, 2021.\n[49]\nM. Bain and C. Sammut, \u201cA framework for behavioural\ncloning,\u201d in Machine Intelligence 15, 1995.\n[50]\nB. D. Ziebart, A. L. Maas, J. A. Bagnell, A. K. Dey, et al.,\n\u201cMaximum entropy inverse reinforcement learning,\u201d in\nAAAI, 2008.\n[51]\nY. Lecun, E. Cosatto, J. Ben, U. Muller, and B. Flepp,\n\u201cDave: Autonomous off-road vehicle control using end-\nto-end learning,\u201d Tech. Rep. DARPA-IPTO Final Report,\nCourant Institute/CBLL, 2004.\n[52]\nD. Chen and P. Kr\u00e4henb\u00fchl, \u201cLearning from all vehicles,\u201d\nin CVPR, 2022.\n[53]\nK. Judah, A. P. Fern, T. G. Dietterich, and P. Tadepalli,\n\u201cActive imitation learning: Formal and practical reduc-\ntions to iid learning,\u201d JMLR, 2014.\n[54]\nS. Ross and D. Bagnell, \u201cEfficient reductions for imitation\nlearning,\u201d in AISTATS, 2010.\n[55]\nS. Ross and J. A. Bagnell, \u201cReinforcement and imitation\nlearning via interactive no-regret learning,\u201d arXiv.org,\nvol. 1406.5979, 2014.\n[56]\nA. E. Sallab, M. Saeed, O. A. Tawab, and M. Ab-\ndou, \u201cMeta learning framework for automated driving,\u201d\narXiv.org, vol. 1706.04038, 2017.\n[57]\nC. Wen, J. Lin, T. Darrell, D. Jayaraman, and Y. Gao,\n\u201cFighting copycat agents in behavioral cloning from ob-\nservation histories,\u201d in NeurIPS, 2020.\n[58]\nC. Wen, J. Lin, J. Qian, Y. Gao, and D. Jayaraman,\n\u201cKeyframe-focused visual imitation learning,\u201d in ICML,\n2021.\n[59]\nJ. Park, Y. Seo, C. Liu, L. Zhao, T. Qin, J. Shin, and T.-Y.\nLiu, \u201cObject-aware regularization for addressing causal\nconfusion in imitation learning,\u201d in NeurIPS, 2021.\n[60]\nC. Wen, J. Qian, J. Lin, J. Teng, D. Jayaraman, and Y. Gao,\n\u201cFighting fire with fire: avoiding dnn shortcuts through\npriming,\u201d in ICML, 2022.\n[61]\nD. Brown, W. Goo, P. Nagarajan, and S. Niekum, \u201cExtrap-\nolating beyond suboptimal demonstrations via inverse\nreinforcement learning from observations,\u201d in ICML,\n2019.\n[62]\nC. Finn, S. Levine, and P. Abbeel, \u201cGuided cost learning:\nDeep inverse optimal control via policy optimization,\u201d in\nICML, 2016.\n[63]\nS. Reddy, A. D. Dragan, and S. Levine, \u201cSqil: Imita-\ntion learning via reinforcement learning with sparse re-\nwards,\u201d arXiv.org, vol. 1905.11108, 2019.\n[64]\nS. Luo, H. Kasaei, and L. Schomaker, \u201cSelf-imitation\nlearning by planning,\u201d in ICRA, 2021.\n[65]\nJ. Ho and S. Ermon, \u201cGenerative adversarial imitation\nlearning,\u201d in NeurIPS, 2016.\n[66]\nY. Li, J. Song, and S. Ermon, \u201cInfogail: Interpretable imi-\ntation learning from visual demonstrations,\u201d in NeurIPS,\n2017.\n[67]\nG. Lee, D. Kim, W. Oh, K. Lee, and S. Oh, \u201cMixgail:\nAutonomous driving using demonstrations with mixed\nqualities,\u201d in IROS, 2020.\n[68]\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n\u201cGenerative adversarial networks,\u201d ACM, 2020.\n[69]\nH. Wang, P. Cai, R. Fan, Y. Sun, and M. Liu, \u201cEnd-to-\nend interactive prediction and planning with optical flow\ndistillation for autonomous driving,\u201d in CVPR Workshops,\n2021.\n[70]\nP. Hu, A. Huang, J. Dolan, D. Held, and D. Ra-\nmanan, \u201cSafe local motion planning with self-supervised\nfreespace forecasting,\u201d in CVPR, 2021.\n[71]\nT. Khurana, P. Hu, A. Dave, J. Ziglar, D. Held, and D. Ra-\nmanan, \u201cDifferentiable raycasting for self-supervised oc-\ncupancy forecasting,\u201d in ECCV, 2022.\n[72]\nS. Chen, B. Jiang, H. Gao, B. Liao, Q. Xu, Q. Zhang,\nC. Huang, W. Liu, and X. Wang, \u201cVadv2: End-to-end vec-\ntorized autonomous driving via probabilistic planning,\u201d\narXiv.org, vol. 2402.13243, 2024.\n[73]\nR. S. Sutton and A. G. Barto, \u201cReinforcement learning: An\nintroduction,\u201d TNNLS, 1998.\n[74]\nB. Jaeger and A. Geiger, \u201cAn invitation to deep reinforce-\nment learning,\u201d arXiv.org, vol. 2312.08365, 2023.\n[75]\nV. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness,\nM. G. Bellemare, A. Graves, M. Riedmiller, A. K. Fidje-\nland, G. Ostrovski, et al., \u201cHuman-level control through\ndeep reinforcement learning,\u201d Nature, 2015.\n[76]\nM. G. Bellemare, Y. Naddaf, J. Veness, and M. Bowling,\n\u201cThe arcade learning environment: An evaluation plat-\nform for general agents,\u201d JAIR, 2013.\n[77]\nD. Horgan, J. Quan, D. Budden, G. Barth-Maron, M. Hes-\nsel, H. Van Hasselt, and D. Silver, \u201cDistributed prioritized\nexperience replay,\u201d arXiv.org, vol. 1803.00933, 2018.\n[78]\nJ. Bjorck, C. P. Gomes, and K. Q. Weinberger, \u201cTowards\ndeeper deep reinforcement learning with spectral nor-\nmalization,\u201d in NeurIPS, 2021.\n[79]\nM. Toromanoff, E. Wirbel, and F. Moutarde, \u201cIs deep rein-\nforcement learning really superhuman on atari? leveling\nthe playing field,\u201d arXiv.org, vol. 1908.04683, 2019.\n[80]\nE. Ohn-Bar, A. Prakash, A. Behl, K. Chitta, and A. Geiger,\n\u201cLearning situational driving,\u201d in CVPR, 2020.\n[81]\nW. B. Knox, A. Allievi, H. Banzhaf, F. Schmitt, and\nP. Stone, \u201cReward (mis)design for autonomous driving,\u201d\nAI, 2023.\n[82]\nC. Zhang, R. Guo, W. Zeng, Y. Xiong, B. Dai, R. Hu,\nM. Ren, and R. Urtasun, \u201cRethinking closed-loop training\nfor autonomous driving,\u201d in ECCV, 2022.\n[83]\nD. Hafner, T. Lillicrap, J. Ba, and M. Norouzi, \u201cDream\nto control: Learning behaviors by latent imagination,\u201d in\nICLR, 2020.\n16\n[84]\nD. Hafner, T. Lillicrap, M. Norouzi, and J. Ba, \u201cMastering\natari with discrete world models,\u201d in ICLR, 2021.\n[85]\nD. Ha and J. Schmidhuber, \u201cRecurrent world models\nfacilitate policy evolution,\u201d in NeurIPS, 2018.\n[86]\nM. Buehler, K. Iagnemma, and S. Singh, The 2005 DARPA\ngrand challenge: the great robot race, vol. 36. Springer, 2007.\n[87]\nM. Buehler, K. Iagnemma, and S. Singh, The DARPA\nurban challenge: autonomous vehicles in city traffic, vol. 56.\nSpringer Science & Business Media, 2009.\n[88]\nU.\nof\nMichigan,\n\u201cMcity.\u201d\nhttps://mcity.umich.edu/,\n2015.\n[89]\nT. Team, \u201cTorcs, the open racing car simulator.\u201d https:\n//sourceforge.net/projects/torcs/, 2000.\n[90]\nM.\nMartinez,\nC.\nSitawarin,\nK.\nFinch,\nL.\nMeincke,\nA. Yablonski, and A. Kornhauser, \u201cBeyond grand theft\nauto v for training, testing and enhancing deep learning\nin self driving cars,\u201d arXiv.org, vol. 1712.01397, 2017.\n[91]\nA. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and\nV. Koltun, \u201cCARLA: An open urban driving simulator,\u201d\nin CoRL, 2017.\n[92]\nD. Team, \u201cDeepdrive: a simulator that allows anyone\nwith a pc to push the state-of-the-art in self-driving.\u201d\nhttps://github.com/deepdrive/deepdrive, 2020.\n[93]\nQ. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou,\n\u201cMetadrive: Composing diverse driving scenarios for\ngeneralizable reinforcement learning,\u201d PAMI, 2022.\n[94]\nM. Hendrikx, S. Meijer, J. Van Der Velden, and A. Iosup,\n\u201cProcedural content generation for games: A survey,\u201d\nTOMM, 2013.\n[95]\nD. J. Fremont, T. Dreossi, S. Ghosh, X. Yue, A. L.\nSangiovanni-Vincentelli, and S. A. Seshia, \u201cScenic: a lan-\nguage for scenario specification and scene generation,\u201d in\nPLDI, 2019.\n[96]\nF. Hauer, T. Schmidt, B. Holzm\u00fcller, and A. Pretschner,\n\u201cDid we test all scenarios for automated and autonomous\ndriving systems?,\u201d in ITSC, 2019.\n[97]\nS. Tan, K. Wong, S. Wang, S. Manivasagam, M. Ren,\nand R. Urtasun, \u201cScenegen: Learning to generate realistic\ntraffic scenes,\u201d in CVPR, 2021.\n[98]\nL. Bergamini, Y. Ye, O. Scheel, L. Chen, C. Hu, L. D.\nPero, B. Osinski, H. Grimmett, and P. Ondruska, \u201cSim-\nnet: Learning reactive self-driving simulations from real-\nworld observations,\u201d in ICRA, 2021.\n[99]\nL. Feng, Q. Li, Z. Peng, S. Tan, and B. Zhou, \u201cTrafficgen:\nLearning to generate diverse and realistic traffic scenar-\nios,\u201d in ICRA, 2023.\n[100] K. Chitta, D. Dauner, and A. Geiger, \u201cSledge: Synthe-\nsizing simulation environments for driving agents with\ngenerative models,\u201d in ECCV, 2024.\n[101] S. Suo, S. Regalado, S. Casas, and R. Urtasun, \u201cTrafficsim:\nLearning to simulate realistic multi-agent behaviors,\u201d in\nCVPR, 2021.\n[102] M. Treiber, A. Hennecke, and D. Helbing, \u201cCongested\ntraffic states in empirical observations and microscopic\nsimulations,\u201d Physical review E, 2000.\n[103] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che,\nB. Ray, and M. Pavone, \u201cGuided conditional diffusion\nfor controllable traffic simulation,\u201d in ICRA, 2023.\n[104] D. Xu, Y. Chen, B. Ivanovic, and M. Pavone, \u201cBits: Bi-level\nimitation for traffic simulation,\u201d in ICRA, 2023.\n[105] Z. Zhang, A. Liniger, D. Dai, F. Yu, and L. Van Gool, \u201cTraf-\nficBots: Towards world models for autonomous driving\nsimulation and motion prediction,\u201d in ICRA, 2023.\n[106] S.\nManivasagam,\nS.\nWang,\nK.\nWong,\nW.\nZeng,\nM. Sazanovich, S. Tan, B. Yang, W. Ma, and R. Urtasun,\n\u201cLidarsim: Realistic lidar simulation by leveraging the\nreal world,\u201d in CVPR, 2020.\n[107] Y. Chen, F. Rong, S. Duggal, S. Wang, X. Yan, S. Mani-\nvasagam, S. Xue, E. Yumer, and R. Urtasun, \u201cGeosim: Re-\nalistic video simulation via geometry-aware composition\nfor self-driving,\u201d in CVPR, 2021.\n[108] Z. Yang, Y. Chen, J. Wang, S. Manivasagam, W.-C. Ma,\nA. J. Yang, and R. Urtasun, \u201cUnisim: A neural closed-\nloop sensor simulator,\u201d in CVPR, 2023.\n[109] A. Petrenko, E. Wijmans, B. Shacklett, and V. Koltun,\n\u201cMegaverse: Simulating embodied agents at one million\nexperiences per second,\u201d in ICML, 2021.\n[110] Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei,\nL. Peng, J. Hu, D. Yao, et al., \u201cSynthetic datasets for\nautonomous driving: A survey,\u201d TIV, 2024.\n[111] A. Amini, I. Gilitschenski, J. Phillips, J. Moseyko,\nR. Banerjee, S. Karaman, and D. Rus, \u201cLearning robust\ncontrol policies for end-to-end autonomous driving from\ndata-driven simulation,\u201d RA-L, 2020.\n[112] A. Amini, T.-H. Wang, I. Gilitschenski, W. Schwarting,\nZ. Liu, S. Han, S. Karaman, and D. Rus, \u201cVista 2.0: An\nopen, data-driven simulator for multimodal sensing and\npolicy learning for autonomous vehicles,\u201d in ICRA, 2022.\n[113] T.-H. Wang, A. Amini, W. Schwarting, I. Gilitschenski,\nS. Karaman, and D. Rus, \u201cLearning interactive driving\npolicies via data-driven simulation,\u201d in ICRA, 2022.\n[114] B. Mildenhall, P. P. Srinivasan, M. Tancik, J. T. Barron,\nR. Ramamoorthi, and R. Ng, \u201cNerf: Representing scenes\nas neural radiance fields for view synthesis,\u201d in ECCV,\n2020.\n[115] B. Kerbl, G. Kopanas, T. Leimk\u00fchler, and G. Drettakis,\n\u201c3d gaussian splatting for real-time radiance field ren-\ndering,\u201d ACM Trans. on Graphics, 2023.\n[116] M. Tancik, V. Casser, X. Yan, S. Pradhan, B. Milden-\nhall, P. P. Srinivasan, J. T. Barron, and H. Kretzschmar,\n\u201cBlock-nerf: Scalable large scene neural view synthesis,\u201d\nin CVPR, 2022.\n[117] H. Turki, D. Ramanan, and M. Satyanarayanan, \u201cMega-\nnerf: Scalable construction of large-scale nerfs for virtual\nfly-throughs,\u201d in CVPR, 2022.\n[118] A. Kundu, K. Genova, X. Yin, A. Fathi, C. Pantofaru,\nL. Guibas, A. Tagliasacchi, F. Dellaert, and T. Funkhouser,\n\u201cPanoptic Neural Fields: A Semantic Object-Aware Neu-\nral Scene Representation,\u201d in CVPR, 2022.\n[119] Y. Yang, Y. Yang, H. Guo, R. Xiong, Y. Wang, and Y. Liao,\n\u201cUrbangiraffe: Representing urban scenes as composi-\ntional generative neural feature fields,\u201d in ICCV, 2023.\n[120] S. R. Richter, H. A. Alhaija, and V. Koltun, \u201cEnhancing\nphotorealism enhancement,\u201d PAMI, 2023.\n[121] A. Schoonwinkel, Design and test of a computer-stabilized\nunicycle. Stanford University, 1988.\n[122] P.\nPolack,\nF.\nAltch\u00e9,\nB.\nd\u2019Andr\u00e9a\nNovel,\nand\nA. de La Fortelle, \u201cThe kinematic bicycle model: A\nconsistent model for planning feasible trajectories for\nautonomous vehicles?,\u201d in IV, 2017.\n[123] R. Rajamani, Vehicle dynamics and control. Springer Science\n& Business Media, 2011.\n[124] F. Codevilla, A. M. Lopez, V. Koltun, and A. Dosovitskiy,\n\u201cOn offline evaluation of vision-based driving models,\u201d\nin ECCV, 2018.\n[125] D. Dauner, M. Hallgarten, T. Li, X. Weng, Z. Huang,\nZ. Yang, H. Li, I. Gilitschenski, B. Ivanovic, M. Pavone,\net al., \u201cNavsim: Data-driven non-reactive autonomous\nvehicle\nsimulation\nand\nbenchmarking,\u201d\narXiv.org,\nvol. 2406.15349, 2024.\n[126] D. Dauner, M. Hallgarten, A. Geiger, and K. Chitta, \u201cPart-\ning with misconceptions about learning-based vehicle\nmotion planning,\u201d in CoRL, 2023.\n[127] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong,\nQ. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom,\n\u201cnuscenes: A multimodal dataset for autonomous driv-\ning,\u201d in CVPR, 2020.\n[128] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh,\nS. Khandelwal, B. Pan, R. Kumar, A. Hartnett, J. K.\nPontes, et al., \u201cArgoverse 2: Next generation datasets\n17\nfor self-driving perception and forecasting,\u201d in NeurIPS\nDatasets and Benchmarks, 2021.\n[129] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Pat-\nnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., \u201cScal-\nability in perception for autonomous driving: Waymo\nopen dataset,\u201d in CVPR, 2020.\n[130] J.-T. Zhai, Z. Feng, J. Du, Y. Mao, J.-J. Liu, Z. Tan, Y. Zhang,\nX. Ye, and J. Wang, \u201cRethinking the open-loop evalu-\nation of end-to-end autonomous driving in nuscenes,\u201d\narXiv.org, vol. 2305.10430, 2023.\n[131] Z. Li, Z. Yu, S. Lan, J. Li, J. Kautz, T. Lu, and J. M.\nAlvarez, \u201cIs ego status all you need for open-loop end-\nto-end autonomous driving?,\u201d in CVPR, 2024.\n[132] T. Liang, H. Xie, K. Yu, Z. Xia, Z. Lin, Y. Wang, T. Tang,\nB. Wang, and Z. Tang, \u201cBEVFusion: A simple and robust\nliDAR-camera fusion framework,\u201d in NeurIPS, 2022.\n[133] Z. Liu, H. Tang, A. Amini, X. Yang, H. Mao, D. Rus, and\nS. Han, \u201cBevfusion: Multi-task multi-sensor fusion with\nunified bird\u2019s-eye view representation,\u201d in ICRA, 2023.\n[134] R. Zhang, S. A. Candra, K. Vetter, and A. Zakhor, \u201cSensor\nfusion for semantic segmentation of urban scenes,\u201d in\nICRA, 2015.\n[135] G. P. Meyer, J. Charland, D. Hegde, A. Laddha, and\nC. Vallespi-Gonzalez, \u201cSensor fusion for joint 3d object\ndetection and semantic segmentation,\u201d in CVPR Work-\nshops, 2019.\n[136] B. Zhou, P. Kr\u00e4henb\u00fchl, and V. Koltun, \u201cDoes computer\nvision matter for action?,\u201d Science Robotics, 2019.\n[137] Z. Gao, Y. Mu, R. Shen, C. Chen, Y. Ren, J. Chen, S. E.\nLi, P. Luo, and Y. Lu, \u201cEnhance sample efficiency and\nrobustness of end-to-end urban autonomous driving via\nsemantic masked world model,\u201d TITS, 2024.\n[138] J. Chen, S. E. Li, and M. Tomizuka, \u201cInterpretable end-\nto-end urban autonomous driving with latent deep rein-\nforcement learning,\u201d TITS, 2022.\n[139] P. Cai, S. Wang, H. Wang, and M. Liu, \u201cCarl-lead: Lidar-\nbased end-to-end autonomous driving with contrastive\ndeep reinforcement learning,\u201d arXiv.org, vol. 2109.08473,\n2021.\n[140] Z. Huang, C. Lv, Y. Xing, and J. Wu, \u201cMulti-modal\nsensor fusion-based deep neural network for end-to-end\nautonomous driving with scene understanding,\u201d IEEE\nSensors Journal, 2020.\n[141] O. Natan and J. Miura, \u201cFully end-to-end autonomous\ndriving with semantic depth cloud mapping and multi-\nagent,\u201d in IV, 2022.\n[142] Y. Xiao, F. Codevilla, A. Gurram, O. Urfalioglu, and A. M.\nL\u00f3pez, \u201cMultimodal end-to-end autonomous driving,\u201d\nTITS, 2020.\n[143] I. Sobh, L. Amin, S. Abdelkarim, K. Elmadawy, M. Saeed,\nO. Abdeltawab, M. E. Gamal, and A. E. Sallab, \u201cEnd-to-\nend multi-modal sensors fusion system for urban auto-\nmated driving,\u201d in NeurIPS Workshops, 2018.\n[144] Y. Chen, J. Wang, J. Li, C. Lu, Z. Luo, H. Xue, and\nC. Wang, \u201cLidar-video driving dataset: Learning driving\npolicies effectively,\u201d in CVPR, 2018.\n[145] H. M. Eraqi, M. N. Moustafa, and J. Honer, \u201cDynamic\nconditional imitation learning for autonomous driving,\u201d\nTITS, 2022.\n[146] S. Chowdhuri, T. Pankaj, and K. Zipser, \u201cMultinet: Multi-\nmodal multi-task learning for autonomous driving,\u201d in\nWACV, 2019.\n[147] P. Cai, S. Wang, Y. Sun, and M. Liu, \u201cProbabilistic end-\nto-end vehicle navigation in complex dynamic environ-\nments with multimodal sensor fusion,\u201d RA-L, 2020.\n[148] Q. Zhang, M. Tang, R. Geng, F. Chen, R. Xin, and L. Wang,\n\u201cMmfn: Multi-modal-fusion-net for end-to-end driving,\u201d\nin IROS, 2022.\n[149] H. Shao, L. Wang, R. Chen, S. L. Waslander, H. Li, and\nY. Liu, \u201cReasonnet: End-to-end driving with temporal\nand global reasoning,\u201d in CVPR, 2023.\n[150] Y. Li, A. W. Yu, T. Meng, B. Caine, J. Ngiam, D. Peng,\nJ. Shen, Y. Lu, D. Zhou, Q. V. Le, et al., \u201cDeepfusion: Lidar-\ncamera deep fusion for multi-modal 3d object detection,\u201d\nin CVPR, 2022.\n[151] S. Borse, M. Klingner, V. R. Kumar, H. Cai, A. Almuzairee,\nS. Yogamani, and F. Porikli, \u201cX-align: Cross-modal cross-\nview alignment for bird\u2019s-eye-view segmentation,\u201d in\nWACV, 2023.\n[152] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson,\nN. S\u00fcnderhauf, I. Reid, S. Gould, and A. Van Den Hengel,\n\u201cVision-and-language navigation: Interpreting visually-\ngrounded navigation instructions in real environments,\u201d\nin CVPR, 2018.\n[153] M. Shridhar, L. Manuelli, and D. Fox, \u201cCliport: What\nand where pathways for robotic manipulation,\u201d in CoRL,\n2022.\n[154] J. Duan, S. Yu, H. L. Tan, H. Zhu, and C. Tan, \u201cA survey of\nembodied ai: From simulators to research tasks,\u201d TETCI,\n2022.\n[155] S. Vemprala, R. Bonatti, A. Bucker, and A. Kapoor, \u201cChat-\ngpt for robotics: Design principles and model abilities,\u201d\narXiv.org, vol. 2306.17582, 2023.\n[156] T. Deruyttere, S. Vandenhende, D. Grujicic, L. Van Gool,\nand M. F. Moens, \u201cTalk2car: Taking control of your self-\ndriving car,\u201d in EMNLP, 2019.\n[157] P. Mirowski, M. Grimes, M. Malinowski, K. M. Hermann,\nK. Anderson, D. Teplyashin, K. Simonyan, A. Zisserman,\nR. Hadsell, et al., \u201cLearning to navigate in cities without\na map,\u201d in NeurIPS, 2018.\n[158] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi,\n\u201cTouchdown: Natural language navigation and spatial\nreasoning in visual street environments,\u201d in CVPR, 2019.\n[159] R. Schumann and S. Riezler, \u201cGenerating landmark nav-\nigation instructions from maps as a graph-to-text prob-\nlem,\u201d in ACL, 2021.\n[160] J. Kim, T. Misu, Y.-T. Chen, A. Tawari, and J. Canny,\n\u201cGrounding human-to-vehicle advice for self-driving ve-\nhicles,\u201d in CVPR, 2019.\n[161] S. Narayanan, T. Maniar, J. Kalyanasundaram, V. Gandhi,\nB. Bhowmick, and K. M. Krishna, \u201cTalk to the vehicle:\nLanguage conditioned autonomous navigation of self\ndriving cars,\u201d in IROS, 2019.\n[162] J. Kim, S. Moon, A. Rohrbach, T. Darrell, and J. Canny,\n\u201cAdvisable learning for self-driving vehicles by internal-\nizing observation-to-action rules,\u201d in CVPR, 2020.\n[163] J. Roh, C. Paxton, A. Pronobis, A. Farhadi, and D. Fox,\n\u201cConditional driving from natural language instruc-\ntions,\u201d in CoRL, 2019.\n[164] K. Jain, V. Chhangani, A. Tiwari, K. M. Krishna, and\nV. Gandhi, \u201cGround then navigate: Language-guided\nnavigation in dynamic scenes,\u201d in ICRA, 2023.\n[165] D. Shah, B. Osi\u00b4nski, b. ichter, and S. Levine, \u201cLm-nav:\nRobotic navigation with large pre-trained models of lan-\nguage, vision, and action,\u201d in CoRL, 2023.\n[166] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh,\nS. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark,\net al., \u201cLearning transferable visual models from natural\nlanguage supervision,\u201d in ICML, 2021.\n[167] OpenAI,\n\u201cGPT-4\nTechnical\nReport,\u201d\narXiv.org,\nvol. 2303.08774, 2023.\n[168] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A.\nLachaux, T. Lacroix, B. Rozi\u00e8re, N. Goyal, E. Hambro,\nF. Azhar, et al., \u201cLlama: Open and efficient foundation\nlanguage models,\u201d arXiv.org, vol. 2302.13971, 2023.\n[169] J. Mao, Y. Qian, H. Zhao, and Y. Wang, \u201cGpt-driver:\nLearning to drive with gpt,\u201d arXiv.org, vol. 2310.01415,\n2023.\n[170] Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K. K. Wong, Z. Li,\nand H. Zhao, \u201cDrivegpt4: Interpretable end-to-end au-\n18\ntonomous driving via large language model,\u201d arXiv.org,\nvol. 2310.01412, 2023.\n[171] H. Shao, Y. Hu, L. Wang, S. L. Waslander, Y. Liu, and\nH. Li, \u201cLmdrive: Closed-loop end-to-end driving with\nlarge language models,\u201d in CVPR, 2024.\n[172] C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie,\nJ. Bei\u00dfwenger, P. Luo, A. Geiger, and H. Li, \u201cDrivelm:\nDriving with graph visual question answering,\u201d in ECCV,\n2024.\n[173] T. Qian, J. Chen, L. Zhuo, Y. Jiao, and Y.-G. Jiang,\n\u201cNuscenes-qa: A multi-modal visual question answering\nbenchmark for autonomous driving scenario,\u201d in AAAI,\n2024.\n[174] Z. Yang, X. Jia, H. Li, and J. Yan, \u201cA survey of large\nlanguage models for autonomous driving,\u201d arXiv.org,\nvol. 2311.01043, 2023.\n[175] B. Hilleli and R. El-Yaniv, \u201cToward deep reinforcement\nlearning without a simulator: An autonomous steering\nexample,\u201d in AAAI, 2018.\n[176] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual\nlearning for image recognition,\u201d in CVPR, 2016.\n[177] Y. Lee, J.-w. Hwang, S. Lee, Y. Bae, and J. Park, \u201cAn\nenergy and gpu-computation efficient backbone network\nfor real-time object detection,\u201d in CVPR Workshops, 2019.\n[178] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,\nG. Heigold, S. Gelly, et al., \u201cAn image is worth 16x16\nwords: Transformers for image recognition at scale,\u201d in\nICLR, 2021.\n[179] M. Dehghani, J. Djolonga, B. Mustafa, P. Padlewski,\nJ. Heek, J. Gilmer, A. P. Steiner, M. Caron, R. Geirhos,\nI. Alabdulmohsin, et al., \u201cScaling vision transformers to\n22 billion parameters,\u201d in ICML, 2023.\n[180] H. Li, C. Sima, J. Dai, W. Wang, L. Lu, H. Wang, J. Zeng,\nZ. Li, J. Yang, H. Deng, et al., \u201cDelving into the devils\nof bird\u2019s-eye-view perception: A review, evaluation and\nrecipe,\u201d PAMI, 2023.\n[181] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Y. Qiao,\nand J. Dai, \u201cBevformer: Learning bird\u2019s-eye-view repre-\nsentation from multi-camera images via spatiotemporal\ntransformers,\u201d in ECCV, 2022.\n[182] X. Jia, Y. Gao, L. Chen, J. Yan, P. L. Liu, and H. Li,\n\u201cDriveadapter: Breaking the coupling barrier of percep-\ntion and planning in end-to-end autonomous driving,\u201d in\nICCV, 2023.\n[183] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou,\nQ. Zhang, W. Liu, C. Huang, and X. Wang, \u201cVad: Vec-\ntorized scene representation for efficient autonomous\ndriving,\u201d in ICCV, 2023.\n[184] W. Tong, C. Sima, T. Wang, L. Chen, S. Wu, H. Deng,\nY. Gu, L. Lu, P. Luo, D. Lin, et al., \u201cScene as occupancy,\u201d\nin ICCV, 2023.\n[185] Q. Li, Y. Wang, Y. Wang, and H. Zhao, \u201cHdmapnet: An\nonline hd map construction and evaluation framework,\u201d\nin ICRA, 2022.\n[186] B. Liao, S. Chen, X. Wang, T. Cheng, Q. Zhang, W. Liu,\nand C. Huang, \u201cMapTR: Structured modeling and learn-\ning for online vectorized HD map construction,\u201d in ICLR,\n2023.\n[187] H. Wang, T. Li, Y. Li, L. Chen, C. Sima, Z. Liu, B. Wang,\nP. Jia, Y. Wang, S. Jiang, et al., \u201cOpenlane-v2: A topology\nreasoning benchmark for unified 3d hd mapping,\u201d in\nNeurIPS Datasets and Benchmarks, 2023.\n[188] T. Li, L. Chen, H. Wang, Y. Li, J. Yang, X. Geng, S. Jiang,\nY. Wang, H. Xu, C. Xu, et al., \u201cGraph-based topology\nreasoning for driving scenes,\u201d arXiv.org, vol. 2304.05277,\n2023.\n[189] T. Li, P. Jia, B. Wang, L. Chen, K. JIANG, J. Yan, and H. Li,\n\u201cLanesegnet: Map learning with lane segment perception\nfor autonomous driving,\u201d in ICLR, 2024.\n[190] G. Wang, H. Niu, D. Zhu, J. Hu, X. Zhan, and G. Zhou, \u201cA\nversatile and efficient reinforcement learning framework\nfor autonomous driving,\u201d arXiv.org, vol. 2110.11573, 2021.\n[191] A. Behl, K. Chitta, A. Prakash, E. Ohn-Bar, and A. Geiger,\n\u201cLabel efficient visual abstractions for autonomous driv-\ning,\u201d in IROS, 2020.\n[192] S.-H. Chung, S.-H. Kong, S. Cho, and I. M. A. Nahrendra,\n\u201cSegmented encoding for sim2real of rl-based end-to-end\nautonomous driving,\u201d in IV, 2022.\n[193] D. P. Kingma and M. Welling, \u201cAuto-encoding variational\nbayes,\u201d arXiv.org, vol. 1312.6114, 2013.\n[194] M. Ahmed, A. Abobakr, C. P. Lim, and S. Nahavandi,\n\u201cPolicy-based reinforcement learning for training au-\ntonomous driving agents in urban areas with affordance\nlearning,\u201d TITS, 2022.\n[195] A. Sauer, N. Savinov, and A. Geiger, \u201cConditional affor-\ndance learning for driving in urban environments,\u201d in\nCoRL, 2018.\n[196] X. Zhang, M. Wu, H. Ma, T. Hu, and J. Yuan, \u201cMulti-task\nlong-range urban driving based on hierarchical planning\nand reinforcement learning,\u201d in ITSC, 2021.\n[197] C. Huang, R. Zhang, M. Ouyang, P. Wei, J. Lin, J. Su,\nand L. Lin, \u201cDeductive reinforcement learning for visual\nautonomous urban driving navigation,\u201d TNNLS, 2021.\n[198] R. Cheng, C. Agia, F. Shkurti, D. Meger, and G. Dudek,\n\u201cLatent attention augmentation for robust autonomous\ndriving policies,\u201d in IROS, 2021.\n[199] J. Yamada, K. Pertsch, A. Gunjal, and J. J. Lim, \u201cTask-\ninduced representation learning,\u201d in ICLR, 2022.\n[200] J. Chen and S. Pan, \u201cLearning generalizable represen-\ntations for reinforcement learning via adaptive meta-\nlearner of behavioral similarities,\u201d in ICLR, 2022.\n[201] Z. Yang, L. Chen, Y. Sun, and H. Li, \u201cVisual point\ncloud forecasting enables scalable autonomous driving,\u201d\nin CVPR, 2024.\n[202] M. Henaff, A. Canziani, and Y. LeCun, \u201cModel-predictive\npolicy learning with uncertainty regularization for driv-\ning in dense traffic,\u201d in ICLR, 2019.\n[203] J. Wu, Z. Huang, and C. Lv, \u201cUncertainty-aware model-\nbased reinforcement learning: Methodology and applica-\ntion in autonomous driving,\u201d IV, 2022.\n[204] M. Pan, X. Zhu, Y. Wang, and X. Yang, \u201cIso-dream:\nIsolating and leveraging noncontrollable visual dynamics\nin world models,\u201d in NeurIPS, 2022.\n[205] J. Yang, S. Gao, Y. Qiu, L. Chen, T. Li, B. Dai, K. Chitta,\nP. Wu, J. Zeng, P. Luo, et al., \u201cGeneralized predictive\nmodel for autonomous driving,\u201d in CVPR, 2024.\n[206] S. Gao, J. Yang, L. Chen, K. Chitta, Y. Qiu, A. Geiger,\nJ. Zhang, and H. Li, \u201cVista: A generalizable driving world\nmodel with high fidelity and versatile controllability,\u201d\narXiv.org, vol. 2405.17398, 2024.\n[207] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang,\n\u201cDriving into the future: Multiview visual forecasting\nand planning with world model for autonomous driv-\ning,\u201d in CVPR, 2024.\n[208] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and\nB. Ommer, \u201cHigh-resolution image synthesis with latent\ndiffusion models,\u201d in CVPR, 2022.\n[209] A. Hu, G. Corrado, N. Griffiths, Z. Murez, C. Gurau,\nH. Yeo, A. Kendall, R. Cipolla, and J. Shotton, \u201cModel-\nbased imitation learning for urban driving,\u201d in NeurIPS,\n2022.\n[210] R. Caruana, \u201cMultitask learning,\u201d Machine Learning, 1997.\n[211] K. Ishihara, A. Kanervisto, J. Miura, and V. Hautamaki,\n\u201cMulti-task learning with attention for end-to-end au-\ntonomous driving,\u201d in CVPR Workshops, 2021.\n[212] Z. Li, T. Motoyoshi, K. Sasaki, T. Ogata, and S. Sug-\nano, \u201cRethinking self-driving: Multi-task knowledge for\nbetter generalization and accident explanation ability,\u201d\narXiv.org, vol. 1809.11100, 2018.\n19\n[213] H. Xu, Y. Gao, F. Yu, and T. Darrell, \u201cEnd-to-end learning\nof driving models from large-scale video datasets,\u201d in\nCVPR, 2017.\n[214] A. Mehta, A. Subramanian, and A. Subramanian, \u201cLearn-\ning end-to-end autonomous driving using guided auxil-\niary supervision,\u201d in ICVGIP, 2018.\n[215] Y. Hou, Z. Ma, C. Liu, and C. C. Loy, \u201cLearning to steer\nby mimicking features from heterogeneous auxiliary net-\nworks,\u201d in AAAI, 2019.\n[216] A. Zhao, T. He, Y. Liang, H. Huang, G. Van den Broeck,\nand S. Soatto, \u201cSam: Squeeze-and-mimic networks for\nconditional visual driving policy learning,\u201d in CoRL,\n2020.\n[217] \u00c9. Zablocki, H. Ben-Younes, P. P\u00e9rez, and M. Cord, \u201cEx-\nplainability of deep vision-based autonomous driving\nsystems: Review and challenges,\u201d IJCV, 2022.\n[218] M. Bojarski, P. Yeres, A. Choromanska, K. Choromanski,\nB. Firner, L. Jackel, and U. Muller, \u201cExplaining how a\ndeep neural network trained with end-to-end learning\nsteers a car,\u201d arXiv.org, vol. 1704.07911, 2017.\n[219] M. Bojarski, A. Choromanska, K. Choromanski, B. Firner,\nL. J. Ackel, U. Muller, P. Yeres, and K. Zieba, \u201cVisual-\nbackprop: Efficient visualization of cnns for autonomous\ndriving,\u201d in ICRA, 2018.\n[220] S. Mohseni, A. Jagadeesh, and Z. Wang, \u201cPredicting\nmodel failure using saliency maps in autonomous driv-\ning systems,\u201d arXiv.org, vol. 1905.07679, 2019.\n[221] J. Kim and J. Canny, \u201cInterpretable learning for self-\ndriving cars by visualizing causal attention,\u201d in ICCV,\n2017.\n[222] K. Mori, H. Fukui, T. Murase, T. Hirakawa, T. Yamashita,\nand H. Fujiyoshi, \u201cVisual explanation by attention branch\nnetwork for end-to-end learning-based self-driving,\u201d in\nIV, 2019.\n[223] D. Wang, C. Devin, Q.-Z. Cai, F. Yu, and T. Darrell, \u201cDeep\nobject-centric policies for autonomous driving,\u201d in ICRA,\n2019.\n[224] L. Cultrera, L. Seidenari, F. Becattini, P. Pala, and\nA. Del Bimbo, \u201cExplaining autonomous driving by learn-\ning end-to-end visual attention,\u201d in CVPR Workshops,\n2020.\n[225] Y. Xiao, F. Codevilla, D. P. Bustamante, and A. M. Lopez,\n\u201cScaling self-supervised end-to-end driving with multi-\nview attention learning,\u201d arXiv.org, vol. 2302.03198, 2023.\n[226] K. Renz, K. Chitta, O.-B. Mercea, A. S. Koepke, Z. Akata,\nand A. Geiger, \u201cPlant: Explainable planning transformers\nvia object-level representations,\u201d in CoRL, 2022.\n[227] Y. Sun, X. Wang, Y. Zhang, J. Tang, X. Tang, and J. Yao,\n\u201cInterpretable end-to-end driving model for implicit\nscene understanding,\u201d in ITSC, 2023.\n[228] C. Liu, Y. Chen, M. Liu, and B. E. Shi, \u201cUsing eye gaze to\nenhance generalization of imitation networks to unseen\nenvironments,\u201d TNNLS, 2020.\n[229] W. Zeng, S. Wang, R. Liao, Y. Chen, B. Yang, and R. Urta-\nsun, \u201cDsdnet: Deep structured self-driving network,\u201d in\nECCV, 2020.\n[230] A. Cui, S. Casas, A. Sadat, R. Liao, and R. Urtasun,\n\u201cLookout: Diverse multi-future prediction and planning\nfor self-driving,\u201d in ICCV, 2021.\n[231] Y. Xu, X. Yang, L. Gong, H.-C. Lin, T.-Y. Wu, Y. Li,\nand N. Vasconcelos, \u201cExplainable object-induced action\ndecision for autonomous vehicles,\u201d in CVPR, 2020.\n[232] H. Ben-Younes, \u00c9. Zablocki, P. P\u00e9rez, and M. Cord, \u201cDriv-\ning behavior explanation with multi-level fusion,\u201d Pattern\nRecognition, 2022.\n[233] B. Jin, X. Liu, Y. Zheng, P. Li, H. Zhao, T. Zhang, Y. Zheng,\nG. Zhou, and J. Liu, \u201cAdapt: Action-aware driving cap-\ntion transformer,\u201d in ICRA, 2023.\n[234] C. Guo, G. Pleiss, Y. Sun, and K. Q. Weinberger, \u201cOn\ncalibration of modern neural networks,\u201d in ICML, 2017.\n[235] A. Loquercio, M. Segu, and D. Scaramuzza, \u201cA general\nframework for uncertainty estimation in deep learning,\u201d\nRA-L, 2020.\n[236] R. Michelmore, M. Kwiatkowska, and Y. Gal, \u201cEvaluat-\ning uncertainty quantification in end-to-end autonomous\ndriving control,\u201d arXiv.org, vol. 1811.06817, 2018.\n[237] A. Filos, P. Tigkas, R. McAllister, N. Rhinehart, S. Levine,\nand Y. Gal, \u201cCan autonomous vehicles identify, recover\nfrom, and adapt to distribution shifts?,\u201d in ICML, 2020.\n[238] L. Tai, P. Yun, Y. Chen, C. Liu, H. Ye, and M. Liu, \u201cVisual-\nbased autonomous driving deployment from a stochastic\nand uncertainty-aware perspective,\u201d in IROS, 2019.\n[239] P. Cai, Y. Sun, H. Wang, and M. Liu, \u201cVtgnet: A vision-\nbased trajectory generation network for autonomous ve-\nhicles in urban environments,\u201d IV, 2020.\n[240] S. Shalev-Shwartz, S. Shammah, and A. Shashua, \u201cOn\na formal model of safe and scalable self-driving cars,\u201d\narXiv.org, vol. 1708.06374, 2017.\n[241] T. Br\u00fcdigam, M. Olbrich, D. Wollherr, and M. Leibold,\n\u201cStochastic model predictive control with a safety guar-\nantee for automated driving,\u201d IV, 2021.\n[242] Y. Lyu, W. Luo, and J. M. Dolan, \u201cProbabilistic safety-\nassured adaptive merging control for autonomous vehi-\ncles,\u201d in ICRA, 2021.\n[243] J. P. Allamaa, P. Patrinos, T. Ohtsuka, and T. D. Son, \u201cReal-\ntime mpc with control barrier functions for autonomous\ndriving using safety enhanced collocation,\u201d arXiv.org,\nvol. 2401.06648, 2024.\n[244] R. Geirhos, J. Jacobsen, C. Michaelis, R. S. Zemel, W. Bren-\ndel, M. Bethge, and F. A. Wichmann, \u201cShortcut learning in\ndeep neural networks,\u201d Nature Machine Intelligence, 2020.\n[245] P. de Haan, D. Jayaraman, and S. Levine, \u201cCausal confu-\nsion in imitation learning,\u201d in NeurIPS, 2019.\n[246] U. Muller, J. Ben, E. Cosatto, B. Flepp, and Y. LeCun, \u201cOff-\nroad obstacle avoidance through end-to-end learning,\u201d in\nNeurIPS, 2005.\n[247] M. Bansal, A. Krizhevsky, and A. S. Ogale, \u201cChauffeurnet:\nLearning to drive by imitating the best and synthesizing\nthe worst,\u201d in RSS, 2019.\n[248] C. Chuang, D. Yang, C. Wen, and Y. Gao, \u201cResolving\ncopycat problems in visual imitation learning via residual\naction prediction,\u201d in ECCV, 2022.\n[249] M. Buda, A. Maki, and M. A. Mazurowski, \u201cA systematic\nstudy of the class imbalance problem in convolutional\nneural networks,\u201d NN, 2018.\n[250] J. Byrd and Z. Lipton, \u201cWhat is the effect of importance\nweighting in deep learning?,\u201d in ICML, 2019.\n[251] I. Mani and I. Zhang, \u201cknn approach to unbalanced\ndata distributions: a case study involving information\nextraction,\u201d in ICML Workshops, 2003.\n[252] X.-Y. Liu, J. Wu, and Z.-H. Zhou, \u201cExploratory undersam-\npling for class-imbalance learning,\u201d TCYB, 2008.\n[253] S. Gidaris and N. Komodakis, \u201cDynamic few-shot visual\nlearning without forgetting,\u201d in CVPR, 2018.\n[254] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz,\n\u201cmixup: Beyond empirical risk minimization,\u201d in ICLR,\n2017.\n[255] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \u201cFocal\nloss for dense object detection,\u201d in ICCV, 2017.\n[256] Y. Cui, M. Jia, T.-Y. Lin, Y. Song, and S. Belongie, \u201cClass-\nbalanced loss based on effective number of samples,\u201d in\nCVPR, 2019.\n[257] S. Akhauri, L. Y. Zheng, and M. C. Lin, \u201cEnhanced\ntransfer learning for autonomous driving with systematic\naccident simulation,\u201d in IROS, 2020.\n[258] Q. Li, Z. Peng, Q. Zhang, C. Liu, and B. Zhou, \u201cImproving\nthe generalization of end-to-end driving through proce-\ndural generation,\u201d arXiv.org, vol. 2012.13681, 2020.\n[259] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P.\nFl\u00f6tter\u00f6d, R. Hilbrich, L. L\u00fccken, J. Rummel, P. Wagner,\n20\nand E. Wie\u00dfner, \u201cMicroscopic traffic simulation using\nsumo,\u201d in ITSC, 2018.\n[260] M. O\u2019Kelly, A. Sinha, H. Namkoong, R. Tedrake, and J. C.\nDuchi, \u201cScalable end-to-end autonomous vehicle testing\nvia rare-event simulation,\u201d in NeurIPS, 2018.\n[261] Y. Abeysirigoonawardena, F. Shkurti, and G. Dudek,\n\u201cGenerating adversarial driving scenarios in high-fidelity\nsimulators,\u201d in ICRA, 2019.\n[262] W. Ding, B. Chen, B. Li, K. J. Eun, and D. Zhao, \u201cMul-\ntimodal safety-critical scenarios generation for decision-\nmaking algorithms evaluation,\u201d RA-L, 2021.\n[263] L. Zhang, Z. Peng, Q. Li, and B. Zhou, \u201cCAT: Closed-\nloop adversarial training for safe end-to-end driving,\u201d in\nCoRL, 2023.\n[264] L. T. Triess, M. Dreissig, C. B. Rist, and J. M. Z\u00f6llner, \u201cA\nsurvey on deep domain adaptation for lidar perception,\u201d\nin IV Workshops, 2021.\n[265] Y. You, X. Pan, Z. Wang, and C. Lu, \u201cVirtual to real rein-\nforcement learning for autonomous driving,\u201d in BMVC,\n2017.\n[266] A. Bewley, J. Rigley, Y. Liu, J. Hawke, R. Shen, V.-D.\nLam, and A. Kendall, \u201cLearning to drive from simulation\nwithout real world labels,\u201d in ICRA, 2019.\n[267] J. Xing, T. Nagata, K. Chen, X. Zou, E. Neftci, and J. L.\nKrichmar, \u201cDomain adaptation in reinforcement learning\nvia latent unified state representation,\u201d in AAAI, 2021.\n[268] J. Tobin, R. Fong, A. Ray, J. Schneider, W. Zaremba, and\nP. Abbeel, \u201cDomain randomization for transferring deep\nneural networks from simulation to the real world,\u201d in\nIROS, 2017.\n[269] X.\nB.\nPeng,\nM.\nAndrychowicz,\nW.\nZaremba,\nand\nP. Abbeel, \u201cSim-to-real transfer of robotic control with\ndynamics randomization,\u201d in ICRA, 2018.\n[270] J. Matas, S. James, and A. J. Davison, \u201cSim-to-real rein-\nforcement learning for deformable object manipulation,\u201d\nin CoRL, 2018.\n[271] B. Osi\u00b4nski, A. Jakubowski, P. Zi\u02dbecina, P. Mi\u0142o\u00b4s, C. Galias,\nS. Homoceanu, and H. Michalewski, \u201cSimulation-based\nreinforcement learning for real-world autonomous driv-\ning,\u201d in ICRA, 2020.\n[272] R. Kirk, A. Zhang, E. Grefenstette, and T. Rockt\u00e4schel, \u201cA\nsurvey of zero-shot generalisation in deep reinforcement\nlearning,\u201d JAIR, 2023.\n[273] J. Snell, K. Swersky, and R. Zemel, \u201cPrototypical networks\nfor few-shot learning,\u201d in NeurIPS, 2017.\n[274] P. Karkus, B. Ivanovic, S. Mannor, and M. Pavone, \u201cDiff-\nstack: A differentiable and modular control stack for\nautonomous vehicles,\u201d in CoRL, 2022.\n[275] H. Li, Y. Li, H. Wang, J. Zeng, P. Cai, H. Xu, D. Lin, J. Yan,\nF. Xu, L. Xiong, et al., \u201cOpen-sourced data ecosystem in\nautonomous driving: the present and future,\u201d arXiv.org,\nvol. 2312.03408, 2023.\n[276] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland,\nL. Gustafson, T. Xiao, S. Whitehead, A. C. Berg, W.-Y. Lo,\net al., \u201cSegment anything,\u201d in ICCV, 2023.\n[277] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec,\nV. Khalidov, P. Fernandez, D. Haziza, F. Massa, A. El-\nNouby, et al., \u201cDinov2: Learning robust visual features\nwithout supervision,\u201d TMLR, 2024.\n[278] J.-B. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr,\nY. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds,\net al., \u201cFlamingo: a visual language model for few-shot\nlearning,\u201d in NeurIPS, 2022.\n[279] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright,\nP. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al.,\n\u201cTraining language models to follow instructions with\nhuman feedback,\u201d in NeurIPS, 2022.\n[280] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick,\n\u201cMasked autoencoders are scalable vision learners,\u201d in\nCVPR, 2022.\n",
    "2112.03126": "Published as a conference paper at ICLR 2022\nLABEL-EFFICIENT SEMANTIC SEGMENTATION WITH\nDIFFUSION MODELS\nDmitry Baranchuk, Ivan Rubachev, Andrey Voynov, Valentin Khrulkov, Artem Babenko\nYandex Research\nABSTRACT\nDenoising diffusion probabilistic models have recently received much research at-\ntention since they outperform alternative approaches, such as GANs, and currently\nprovide state-of-the-art generative performance. The superior performance of dif-\nfusion models has made them an appealing tool in several applications, including\ninpainting, super-resolution, and semantic editing. In this paper, we demonstrate\nthat diffusion models can also serve as an instrument for semantic segmentation,\nespecially in the setup when labeled data is scarce. In particular, for several pre-\ntrained diffusion models, we investigate the intermediate activations from the net-\nworks that perform the Markov step of the reverse diffusion process. We show that\nthese activations effectively capture the semantic information from an input image\nand appear to be excellent pixel-level representations for the segmentation prob-\nlem. Based on these observations, we describe a simple segmentation method,\nwhich can work even if only a few training images are provided. Our approach\nsigni\ufb01cantly outperforms the existing alternatives on several datasets for the same\namount of human supervision. The source code of the project is publicly available.\n1\nINTRODUCTION\nDenoising diffusion probabilistic models (DDPM) (Sohl-Dickstein et al., 2015; Ho et al., 2020) have\nrecently outperformed alternative approaches to model the distribution of natural images both in the\nrealism of individual samples and their diversity (Dhariwal & Nichol, 2021). These advantages of\nDDPM are successfully exploited in applications, such as colorization (Song et al., 2021), inpainting\n(Song et al., 2021), super-resolution (Saharia et al., 2021; Li et al., 2021b), and semantic editing\n(Meng et al., 2021), where DDPM often achieve more impressive results compared to GANs.\nSo far, however, DDPM were not exploited as a source of effective image representations for dis-\ncriminative computer vision problems. While the prior literature has demonstrated that various gen-\nerative paradigms, such as GANs (Donahue & Simonyan, 2019) or autoregressive models (Chen\net al., 2020a), can be used to extract the representations for common vision tasks, it is not clear if\nDDPM can also serve as representation learners. In this paper, we provide an af\ufb01rmative answer to\nthis question in the context of semantic segmentation.\nIn particular, we investigate the intermediate activations from the U-Net network that approximates\nthe Markov step of the reverse diffusion process in DDPM. Intuitively, this network learns to denoise\nits input, and it is not clear why the intermediate activations should capture semantic information\nneeded for high-level vision problems. Nevertheless, we show that on certain diffusion steps, these\nactivations do capture such information, and therefore, can potentially be used as image representa-\ntions for downstream tasks. Given these observations, we propose a simple semantic segmentation\nmethod, which exploits these representations and works successfully even if only a few labeled\nimages are provided. On several datasets, we show that our DDPM-based segmentation method\noutperforms the existing baselines for the same amount of supervision.\nTo sum up, the contributions of our paper are:\n1. We investigate the representations learned by the state-of-the-art DDPM and show that they\ncapture high-level semantic information valuable for downstream vision tasks.\n1\narXiv:2112.03126v3  [cs.CV]  16 Mar 2022\nPublished as a conference paper at ICLR 2022\n2. We design a simple semantic segmentation approach that exploits these representations and\noutperforms the alternatives in the few-shot operating point.\n3. We compare the DDPM-based representations with their GAN-based counterparts on the same\ndatasets and demonstrate the advantages of the former in the context of semantic segmentation.\n2\nRELATED WORK\nIn this section, we brie\ufb02y describe the existing lines of research relevant to our work.\nDiffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) are a class of generative models that\napproximate the distribution of real images by the endpoint of the Markov chain which originates\nfrom a simple parametric distribution, typically a standard Gaussian. Each Markov step is modeled\nby a deep neural network that effectively learns to invert the diffusion process with a known Gaus-\nsian kernel. Ho et al. highlighted the equivalence of diffusion models and score matching (Song &\nErmon, 2019; 2020), showing them to be two different perspectives on the gradual conversion of a\nsimple known distribution into a target distribution via the iterative denoising process. Very recent\nworks (Nichol, 2021; Dhariwal & Nichol, 2021) have developed more powerful model architectures\nas well as different advanced objectives, which led to the \u201cvictory\u201d of DDPM over GANs in terms\nof generative quality and diversity. DDPM have been widely used in several applications, including\nimage colorization (Song et al., 2021), super-resolution (Saharia et al., 2021; Li et al., 2021b), in-\npainting (Song et al., 2021), and semantic editing (Meng et al., 2021). In our work, we demonstrate\nthat one can also successfully use them for semantic segmentation.\nImage segmentation with generative models is an active research direction at the moment, how-\never, existing methods are primarily based on GANs. The \ufb01rst line of works (Voynov & Babenko,\n2020; Voynov et al., 2021; Melas-Kyriazi et al., 2021) is based on the evidence that the latent\nspaces of the state-of-the-art GANs have directions corresponding to effects that in\ufb02uence the fore-\nground/background pixels differently, which allows producing synthetic data to train segmentation\nmodels. However, these approaches are currently able to perform binary segmentation only, and it is\nnot clear if they can be used in the general setup of semantic segmentation. The second line of works\n(Zhang et al., 2021; Tritrong et al., 2021; Xu, 2021; Galeev et al., 2020) is more relevant to our study\nsince they are based on the intermediate representations obtained in GANs. In particular, the method\nproposed in (Zhang et al., 2021) trains a pixel class prediction model on these representations and\ncon\ufb01rms their label ef\ufb01ciency. In the experimental section, we compare the method from (Zhang\net al., 2021) to our DDPM-based one and demonstrate several distinctive advantages of our solution.\nRepresentations from generative models for discriminative tasks. The usage of generative mod-\nels, as representation learners, has been widely investigated for global prediction (Donahue & Si-\nmonyan, 2019; Chen et al., 2020a), and dense prediction problems (Zhang et al., 2021; Tritrong\net al., 2021; Xu, 2021; Xu et al., 2021). While previous works highlighted the practical advantages\nof these representations, such as out-of-distribution robustness (Li et al., 2021a), generative models\nas representation learners receive less attention compared to alternative unsupervised methods, e.g.,\nbased on contrastive learning (Chen et al., 2020b). The main reason is probably the dif\ufb01culty of\ntraining a high-quality generative model on a complex, diverse dataset. However, given the recent\nsuccess of DDPM on Imagenet (Deng et al., 2009), one can expect that this direction will attract\nmore attention in the future.\n3\nREPRESENTATIONS FROM DIFFUSION MODELS\nIn the following section, we investigate the image representations learned by diffusion models. First,\nwe provide a brief overview of the DDPM framework. Then, we describe how to extract features\nwith DDPM and investigate what kind of semantic information these features might capture.\nBackground. Diffusion models transform noise xT \u223cN(0, I) to the sample x0 by gradually denois-\ning xT to less noisy samples xt. Formally, we are given a forward diffusion process:\nq(xt|xt\u22121) := N(xt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI),\n(1)\nfor some \ufb01xed variance schedule \u03b21, . . . , \u03b2t.\n2\nPublished as a conference paper at ICLR 2022\nUpsample\n1\n1\n1\n1\n1\n1\n1\n1\nPixel representation\n1\n1\nVote\n\nFeature maps\nPixel classifiers\nUpsample\nUpsample\nLinear\nReLU\nBatchNorm\nLinear\nReLU\nBatchNorm\nLinear\nFigure 1: Overview of the proposed method. (1) x0 \u2212\u2192xt by adding noise according to q(xt|x0).\n(2) Extracting feature maps from a noise predictor \u03f5\u03b8(xt, t). (3) Collecting pixel-level representa-\ntions by upsampling the feature maps to the image resolution and concatenating them. (4) Using the\npixel-wise feature vectors to train an ensemble of MLPs to predict a class label for each pixel.\nImportantly, a noisy sample xt can be obtained directly from the data x0:\nq(xt|x0) := N(xt; \u221a\u00af\u03b1tx0, (1 \u2212\u00af\u03b1t)I),\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, \u03f5 \u223cN(0, 1),\n(2)\nwhere \u03b1t := 1 \u2212\u03b2t, \u00af\u03b1t := Qt\ns=1 \u03b1s.\nPretrained DDPM approximates a reverse process:\np\u03b8(xt\u22121|xt) := N(xt\u22121; \u00b5\u03b8(xt, t), \u03a3\u03b8(xt, t)).\n(3)\nIn practice, rather than predicting the mean of the distribution in Equation (3), the noise predictor\nnetwork \u03f5\u03b8(xt, t) predicts the noise component at the step t; the mean is then a linear combination\nof this noise component and xt. The covariance predictor \u03a3\u03b8(xt, t) can be either a \ufb01xed set of scalar\ncovariances or learned as well (the latter was shown to improve the model quality (Nichol, 2021)).\nThe denoising model \u03f5\u03b8(xt, t) is typically parameterized by different variants of the UNet archi-\ntecture (Ronneberger et al., 2015), and in our experiments we investigate the state-of-the-art one\nproposed in (Dhariwal & Nichol, 2021).\nExtracting representations. For a given real image x0 \u2208RH\u00d7W \u00d73, one can compute T sets of\nactivation tensors from the noise predictor network \u03f5\u03b8(xt, t). The overall scheme for a timestep t is\npresented in Figure 1. First, we corrupt x0 by adding Gaussian noise according to Equation (2). The\nnoisy xt is used as an input of \u03f5\u03b8(xt, t) parameterized by the UNet model. The UNet\u2019s intermediate\nactivations are then upsampled to H \u00d7 W with bilinear interpolation. This allows treating them as\npixel-level representations of x0.\n3.1\nREPRESENTATION ANALYSIS\nWe analyze the representations produced by the noise predictor \u03f5\u03b8(xt, t) for different t. We consider\nthe state-of-the-art DDPM checkpoints trained on the LSUN-Horse and FFHQ-256 datasets1.\nThe intermediate activations from the noise predictor capture semantic information. For this\nexperiment, we take a few images from the LSUN-Horse and FFHQ datasets and manually assign\neach pixel to one of the 21 and 34 semantic classes, respectively. Our goal is to understand whether\nthe pixel-level representations produced by DDPM effectively capture the information about seman-\ntics. To this end, we train a multi-layer perceptron (MLP) to predict the pixel semantic label from\nits features produced by one of the 18 UNet decoder blocks on a speci\ufb01c diffusion step t. Note\nthat we consider only the decoder activations because they also aggregate the encoder activations\nthrough the skip connections. MLPs are trained on 20 images and evaluated on 20 hold-out ones.\nThe predictive performance is measured in terms of mean IoU.\n1https://github.com/openai/guided-diffusion\n3\nPublished as a conference paper at ICLR 2022\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.1\n0.2\n0.3\n0.4\n0.5\nmIoU\nLSUN-Horse\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nmIoU\nFFHQ-256\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\nFigure 2: The evolution of predictive performance of DDPM-based pixel-wise representations for\ndifferent UNet decoder blocks and diffusion steps. The blocks are numbered from the deep to shal-\nlow ones. The most informative features typically correspond to the later steps of the reverse diffu-\nsion process and middle layers of the UNet decoder. The earlier steps correspond to uninformative\nrepresentations. The plots for other datasets are provided in Appendix A\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nmIoU\nLSUN-Horse | Small-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\nmIoU\nLSUN-Horse | Large-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\nFigure 3: The evolution of predictive performance of DDPM-based pixel-wise representations on\nthe LSUN-Horse dataset for classes with the smallest (Left) and largest (Right) average areas. The\npredictive performance for small-sized objects starts growing later in the reverse process. The deeper\nblocks are more informative for larger objects and the shallower blocks are more informative for\nsmaller objects. A similar evaluation for other datasets is provided in Appendix A.\nThe evolution of predictive performance across the different blocks and diffusion steps t is presented\nin Figure 2. The blocks are numbered from the deep to shallow ones. Figure 2 shows that the\ndiscriminability of the features produced by the noise predictor \u03f5\u03b8(xt, t) varies for different blocks\nand diffusion steps. In particular, the features corresponding to the later steps of the reverse diffusion\nprocess typically capture semantic information more effectively. In contrast, the ones corresponding\nto the early steps are generally uninformative. Across different blocks, the features produced by the\nlayers in the middle of the UNet decoder appear to be the most informative on all diffusion steps.\nAlso, we separately consider small-sized and large-sized semantic classes based on the average area\nin the annotated dataset. Then, we evaluate mean IoU for these classes independently across the\ndifferent UNet blocks and diffusion steps. The results on LSUN-Horse are in Figure 3. As expected,\nthe predictive performance for large-sized objects starts growing earlier in the reverse process. The\nshallower blocks are more informative for smaller objects, while the deeper blocks are more so for\nthe larger ones. In both cases, the most discriminative features still correspond to the middle blocks.\nFigure 2 implies that for certain UNet blocks and diffusion steps, similar DDPM-based representa-\ntions correspond to the pixels of the same semantics. Figure 4 shows the k-means clusters (k=5)\nformed by the features extracted by the FFHQ checkpoint from the blocks {6, 8, 10, 12} on the dif-\nfusion steps {50, 200, 400, 600, 800}, and con\ufb01rms that clusters can span coherent semantic objects\nand object-parts. In the block B=6, the features correspond to coarse semantic masks. At the other\nextreme, the features from B=12 can discriminate between \ufb01ne-grained face parts but exhibit less\nsemantic meaningness for coarse fragmentation. Across different diffusion steps, the most meaning-\nful features correspond to the later ones. We attribute this behavior to the fact that on the earlier steps\nof the reverse process, the global structure of a DDPM sample has not yet emerged, therefore, it is\nhardly possible to predict segmentation masks at this stage. This intuition is qualitatively con\ufb01rmed\nby the masks in Figure 4. For t=800, the masks poorly re\ufb02ect the content of actual images, while\nfor smaller values of t, the masks and images are semantically coherent.\n4\nPublished as a conference paper at ICLR 2022\nImage\nTimestep 50\nTimestep 200\nTimestep 400\nTimestep 600\nTimestep 800\nTimestep 50\nTimestep 200\nTimestep 400\nTimestep 600\nTimestep 800\nBlock 6 [512x32x32]\nBlock 8 [512x64x64]\nImage\nTimestep 50\nTimestep 200\nTimestep 400\nTimestep 600\nTimestep 800\nTimestep 50\nTimestep 200\nTimestep 400\nTimestep 600\nTimestep 800\nBlock 10 [512x64x64]\nBlock 12 [256x128x128]\nFigure 4: Examples of k-means clusters (k=5) formed by the features extracted from the UNet\ndecoder blocks {6, 8, 10, 12} on the diffusion steps {50, 200, 400, 600, 800}. The clusters from the\nmiddle blocks spatially span coherent semantic objects and parts.\n3.2\nDDPM-BASED REPRESENTATIONS FOR FEW-SHOT SEMANTIC SEGMENTATION\nThe potential effectiveness of the intermediate DDPM activations observed above implies their\nusage as image representations for dense prediction tasks. Figure 1 schematically presents our\noverall approach for image segmentation, which exploits the discriminability of these represen-\ntations.\nIn more detail, we consider a few-shot semi-supervised setup, when a large number\nof unlabeled images {X1, . . . , XN} \u2282RH\u00d7W \u00d73 from the particular domain are available, and\nonly for n training images {X1, . . . , Xn} \u2282RH\u00d7W \u00d73 the groundtruth K-class semantic masks\n{Y1, . . . , Yn} \u2282RH\u00d7W \u00d7{1,...,K} are provided.\nAs a \ufb01rst step, we train a diffusion model on the whole {X1, . . . , XN} in an unsupervised manner.\nThen, this diffusion model is used to extract the pixel-level representations of the labeled images\nusing the subset of the UNet blocks and diffusion steps t. In this work, we use the representations\nfrom the middle blocks B={5, 6, 7, 8, 12} of the UNet decoder and later steps t={50, 150, 250}\nof the reverse diffusion process. These blocks and time steps are motivated by the insights from\nSection 3.1 but intentionally not tuned for each dataset.\nWhile the feature extraction at the particular time step is stochastic, we \ufb01x the noise for all timesteps\nt and ablate this in Section 4.1. The extracted representations from all blocks B and steps t are\nupsampled to the image size and concatenated, forming the feature vectors for all pixels of the\ntraining images. The overall dimension of the pixel-level representations is 8448.\nThen, following (Zhang et al., 2021), we train an ensemble of independent multi-layer perceptrons\n(MLPs) on these feature vectors, which aim to predict a semantic label of each pixel available for\ntraining images. We adopt the ensemble con\ufb01guration and training settings from (Zhang et al.,\n2021) and exploit them across all other methods in our experiments, see Appendix C for details.\nTo segment a test image, we extract its DDPM-based pixel-wise representations and use them to\npredict the pixel labels by the ensemble. The \ufb01nal prediction is obtained by majority voting.\n4\nEXPERIMENTS\nThis section experimentally con\ufb01rms the advantage of the DDPM-based representations for the se-\nmantic segmentation problem. We start from a thorough comparison to the existing alternatives and\nthen dissect the reasons for the DDPM success by additional analysis.\nDatasets. In our evaluation, we mainly work with the \u201cbedroom\u201d, \u201ccat\u201d and \u201chorse\u201d categories\nfrom LSUN (Yu et al., 2015) and FFHQ-256 (Karras et al., 2019). As a training set for each dataset,\nwe consider several images for which the \ufb01ne-grained semantic masks are collected following the\nprotocol from (Zhang et al., 2021). For each dataset, a professional assessor was hired to annotate\ntrain and test samples. We denote the collected datasets as Bedroom-28, FFHQ-34, Cat-15, Horse-\n21, where the number corresponds to the number of semantic classes.\n5\nPublished as a conference paper at ICLR 2022\nDataset\nRealTrain\nRealTest\nGAN DDPM Total\nBedroom-28\n40\n20\n40\n40\n140\nFFHQ-34\n20\n20\n20\n20\n80\nCat-15\n30\n20\n30\n30\n110\nHorse-21\n30\n30\n30\n30\n120\nCelebA-19\n20\n500\n\u2014\n\u2014\n520\nADE-Bedroom-30\n50\n650\n\u2014\n\u2014\n700\nTable 1: Number of annotated images for each dataset used in our evaluation.\nAdditionally, we consider two datasets, which, in contrast to others, have publicly available annota-\ntions and sizable evaluation sets:\n\u2022 ADE-Bedroom-30 is a subset of the ADE20K dataset (Zhou et al., 2018), where we extract\nonly images of bedroom scenes with 30 most frequent classes. We resize each image to 256 for\nthe smaller side and then crop them to obtain the 256\u00d7256 samples.\n\u2022 CelebA-19 is a subset of the CelebAMask-HQ dataset (Lee et al., 2020), which provides the\nannotation for 19 facial attributes. All images are resized to 256 resolution.\nThe number of annotated images for each dataset are in Table 1. Other details are in Appendix E.\nMethods. In the evaluation, we compare our method (denoted as DDPM) to several prior ap-\nproaches which tackle the few-shot semantic segmentation setup. First, we describe the baselines\nthat produce a large set of annotated synthetic images to train a segmentation model:\n\u2022 DatasetGAN (Zhang et al., 2021) \u2014 this method exploits the discriminability of pixel-level\nfeatures produced by GANs. In more detail, assessors annotate a few GAN-produced images.\nThen, the latent codes of these images are used to obtain the intermediate generator activations,\nwhich are considered as pixel-level representations. Given these representations, a classi\ufb01er\nis trained to predict a semantic label for each pixel. This classi\ufb01er is then used to label new\nsynthetic GAN images, which, for their part, serve as a training set for the DeepLabV3 seg-\nmentation model (Chen et al., 2017). For each dataset, we increase the number of synthetic\nimages until the performance on the validation set is not saturated. According to (Zhang et al.,\n2021), we also remove 10% of synthetic samples with the most uncertain predictions.\n\u2022 DatasetDDPM mirrors the DatasetGAN baseline with the only difference being that GANs are\nreplaced with DDPMs. We include this baseline to compare the GAN-based and DDPM-based\nrepresentations in the same scenario.\nNote that our segmentation method described in Section 3.2 is more straightforward compared to\nDatasetGAN and DatasetDDPM since it does not require auxiliary steps of the synthetic dataset\ngeneration and training the segmentation model on it.\nThen, we consider a set of baselines that allow extracting intermediate activations from the real\nimages directly and use them as pixel-level representations similarly to our method. In contrast to\nDatasetGAN and DatasetDDPM, these methods can potentially be bene\ufb01cial due to the absence of\nthe domain gap between real and synthetic images.\n\u2022 MAE (He et al., 2021) \u2014 one of the state-of-the-art self-supervised methods, which learns a\ndenoising autoencoder to reconstruct missing patches. We use ViT-Large (Dosovitskiy et al.,\n2021) as a backbone model and reduce the patch size to 8\u00d78 to increase the spatial dimensions\nof the feature maps. We pretrain all models on the same datasets as DDPM using the of\ufb01cial\ncode2. The feature extraction for this method is described in Appendix F.\n\u2022 SwAV (Caron et al., 2020) \u2014 one more recent self-supervised approach. We consider a twice\nwider ResNet-50 model for evaluation. All models are pretrained on the same datasets as\nDDPM also using the of\ufb01cial source code3. The input image resolution is 256.\n\u2022 GAN Inversion employs the state-of-the-art method (Tov et al., 2021) to obtain the latent codes\nfor real images. We map the annotated real images to the GAN latent space, which allows\ncomputing the intermediate generator activations and using them as pixel-level representations.\n2https://github.com/facebookresearch/mae\n3https://github.com/facebookresearch/swav\n6\nPublished as a conference paper at ICLR 2022\nMethod\nBedroom-28\nFFHQ-34\nCat-15\nHorse-21\nCelebA-19\u2217\nADE Bedroom-30\u2217\nALAE\n20.0 \u00b1 1.0\n48.1 \u00b1 1.3\n\u2014\n\u2014\n49.7 \u00b1 0.7\n15.0 \u00b1 0.5\nVDVAE\n\u2014\n57.3 \u00b1 1.1\n\u2014\n\u2014\n54.1 \u00b1 1.0\n\u2014\nGAN Inversion\n13.9 \u00b1 0.6\n51.7 \u00b1 0.8 21.4 \u00b1 1.7\n17.7 \u00b1 0.4\n51.5 \u00b1 2.3\n11.1 \u00b1 0.2\nGAN Encoder\n22.4 \u00b1 1.6\n53.9 \u00b1 1.3 32.0 \u00b1 1.8 26.7 \u00b1 0.7\n53.9 \u00b1 0.8\n15.7 \u00b1 0.3\nSwAV\n42.4 \u00b1 1.7\n56.9 \u00b1 1.3\n45.1 \u00b1 2.1 54.0 \u00b1 0.9\n52.4 \u00b1 1.3\n30.6 \u00b1 1.6\nMAE\n45.0 \u00b1 2.0\n58.8 \u00b1 1.1 52.4 \u00b1 2.3\n63.4 \u00b1 1.4\n57.8 \u00b1 0.4\n31.7 \u00b1 1.8\nDatasetGAN\n31.3 \u00b1 2.3\n57.0 \u00b1 1.1 36.5 \u00b1 2.3\n45.4 \u00b1 1.4\n\u2014\n\u2014\nDatasetDDPM (Ours)\n47.9 \u00b1 2.9\n56.0 \u00b1 0.9 47.6 \u00b1 1.5 60.8 \u00b1 1.0\n\u2014\n\u2014\nDDPM (Ours)\n49.4 \u00b1 1.9\n59.1 \u00b1 1.4\n53.7 \u00b1 3.3 65.0 \u00b1 0.8\n59.9 \u00b1 1.0\n34.6 \u00b1 1.7\nTable 2: The comparison of the segmentation methods in terms of mean IoU. (*) On CelebA-19 and\nADE Bedroom-30, we evaluate models pretrained on FFHQ-256 and LSUN Bedroom, respectively.\n\u2022 GAN Encoder \u2014 while GAN Inversion struggles to reconstruct images from LSUN domains,\nwe also consider the activations of the pretrained GAN encoder used for GAN Inversion.\n\u2022 VDVAE (Child, 2021) \u2014 state-of-the-art autoencoder model. The intermediate activations are\nextracted from both encoder and decoder and concatenated. While there are no pretrained mod-\nels on the LSUN datasets, we evaluate this model only on the publicly available checkpoint4\non FFHQ-256. Note that VAEs are still signi\ufb01cantly inferior to GANs and DDPMs on LSUN.\n\u2022 ALAE (Pidhorskyi et al., 2020) adopts StyleGANv1 generator and adds an encoder network to\nthe adversarial training. We extract features from the encoder model. In our evaluation, we use\npublicly available models on LSUN-Bedroom and FFHQ-10245.\nGenerative pretrained models. In our experiments, we use the state-of-the-art StyleGAN2 (Karras\net al., 2020) models for the GAN-based baselines and the state-of-the-art pretrained ADMs (Dhari-\nwal & Nichol, 2021) for our DDPM-based method.\nSince there is not a pretrained model for\nFFHQ-256, we train it ourselves using the of\ufb01cial implementation6. For evaluation on the ADE-\nBedroom-30 dataset, we use the models (including the baselines) pretrained on LSUN-Bedroom.\nFor Celeba-19, we evaluate the models trained on FFHQ-256.\nMain results. The comparison of the methods in terms of the mean IoU measure is presented in\nTable 2. The results are averaged over 5 independent runs for different data splits. We also report per\nclass IoUs in Appendix D. Additionally, we provide several qualitative examples of segmentation\nwith our method in Figure 5. Below we highlight several key observations:\n\u2022 The proposed method based on the DDPM representations signi\ufb01cantly outperforms the alter-\nnatives on most datasets.\n\u2022 The MAE baseline is the strongest competitor to the DDPM-based segmentation and demon-\nstrates comparable results on the FFHQ-34 and Cat-15 datasets.\n\u2022 The SwAV baseline underperforms compared to the DDPM-based segmentation. We attribute\nthis behavior to the fact that this baseline is trained in the discriminative fashion and can sup-\npress the details, which are needed for \ufb01ne-grained semantic segmentation. This result is con-\nsistent with the recent \ufb01ndings in (Cole et al., 2021), which shows that the state-of-the-art\ncontrastive methods produce representations, which are suboptimal for \ufb01ne-grained problems.\n\u2022 DatasetDDPM outperforms its counterpart DatasetGAN against most benchmarks. Note that\nboth these methods use the DeepLabV3 network. We attribute this superiority to the higher\nquality of DDPM synthetics, therefore, a smaller domain gap between synthetic and real data.\n\u2022 On most datasets, DDPM outperforms the DatasetDDPM competitor. We provide an addi-\ntional experiment to investigate this in the discussion section below.\nOverall, the proposed DDPM-based segmentation outperforms the baselines that exploit alternative\ngenerative models and also the baselines trained in the self-supervised fashion. This result highlights\nthe potential of using the state-of-the-art DDPMs as strong unsupervised representation learners.\n4https://github.com/openai/vdvae\n5https://github.com/podgorskiy/ALAE\n6https://github.com/openai/guided-diffusion\n7\nPublished as a conference paper at ICLR 2022\nFFHQ\n34 classes\nCelebAMask\n19 classes\nLSUN-Bedroom\n28 classes\nADE-Bedroom\n30 classes\nLSUN-Cat\n15 classes\nImage\nLSUN-Horse\n21 classes\nGroundtruth\nDDPM\nImage\nGroundtruth\nDDPM\nImage\nGroundtruth\nDDPM\nImage\nGroundtruth\nDDPM\nImage\nGroundtruth\nDDPM\nFigure 5: The examples of segmentation masks predicted by our method on the test images along\nwith the groundtruth annotated masks.\nBedroom-28\nCat-15\nHorse-21\nTrain data\nReal\nDDPM\nGAN\nReal\nDDPM\nGAN\nReal\nDDPM\nGAN\nDatasetGAN\n\u2014\n\u2014\n31.3 \u00b1 2.3\n\u2014\n\u2014\n36.5 \u00b1 2.3\n\u2014\n\u2014\n45.4 \u00b1 1.4\nDatasetDDPM\n\u2014\n47.9 \u00b1 2.9\n\u2014\n\u2014\n47.6 \u00b1 1.5\n\u2014\n\u2014\n60.8 \u00b1 1.0\n\u2014\nDDPM\n49.4 \u00b1 1.9 48.7 \u00b1 2.6\n43.3 \u00b1 2.9\n53.7 \u00b1 3.3\n47.9 \u00b1 2.7\n41.1 \u00b1 2.2\n65.0 \u00b1 0.8\n62.4 \u00b1 1.0\n60.0 \u00b1 1.0\nTable 3: Performance of DDPM-based segmentation when trained on real and synthetic images.\nWhen trained on DDPM-produced data, DDPM demonstrates comparable performance to Dataset-\nDDPM. When trained on GAN-produced data, DDPM still signi\ufb01cantly outperforms DatasetGAN,\nbut the gap between them reduces.\n4.1\nDISCUSSION\nThe effect of training on real data. The proposed DDPM method is trained on annotated real\nimages, while DatasetDDPM and DatasetGAN are trained on synthetic ones, which are typically less\nnatural, diverse, and can lack objects of particular classes. Moreover, synthetic images are harder\nfor human annotation since they might have some distorted objects that are dif\ufb01cult to assign to a\nparticular class. In the following experiment, we quantify the performance drop caused by training\non real or synthetic data. Speci\ufb01cally, Table 3 reports the performance of the DDPM approach\ntrained on real, DDPM-produced and GAN-produced annotated images. As can be seen, training\non real images is very bene\ufb01cial on the domains where the \ufb01delity of generative models is still\nrelatively low, e.g., LSUN-Cat, which indicates that annotated real images are a more reliable source\nof supervision. Moreover, if the DDPM method is trained on synthetic images, its performance\nbecomes on par with DatasetDDPM. On the other hand, when trained on GAN-produced samples,\nDDPM signi\ufb01cantly outperforms DatasetGAN. We attribute this to the fact that DDPMs provide\nmore semantically-valuable pixel-wise representations compared to GANs.\nSample-ef\ufb01ciency. In this experiment, we evaluate the performance of our method when it utilizes\nless annotated data. We provide mIoU for four datasets in Table 4. Importantly, DDPM is still able\nto outperform most baselines in Table 2, using signi\ufb01cantly less supervision.\nThe effect of stochastic feature extraction. Here, we investigate whether our method can bene\ufb01t\nfrom the stochastic feature extraction described in Section 3.2. We consider the deterministic case,\nwhen the noise \u03f5\u223cN(0, I) is sampled once and used in (2) to obtain xt for all timesteps t during\nboth training and evaluation. Then, we compare it to the following stochastic options:\nFirst, different \u03f5t are sampled for different timesteps t and shared during the training and evaluation.\nSecond, one samples different noise for all timesteps at each training iteration; during the evaluation\nthe method also uses unseen noise samples.\n8\nPublished as a conference paper at ICLR 2022\nNo corruption\nWeak\nMedium\nStrong\nCorruption level\n10\n15\n20\n25\n30\n35\n40\n45\n50\nmIoU\nSwAV\nMAE\nDDPM\n(a) Bedroom-28\nNo corruption\nWeak\nMedium\nStrong\nCorruption level\n10\n20\n30\n40\n50\n60\n70\nmIoU\nSwAV\nMAE\nDDPM\n(b) Horse-21\nFigure 6: mIoU degradation for different image corruption levels on the Bedroom-28 and Horse-21\ndatasets. DDPM demonstrates higher robustness and preserves its advantage for all distortion levels.\nBedroom-28\nCat-15\nHorse-21\nMethod\n40\n20\n10\n30\n20\n10\n30\n20\n10\nDDPM\n49.4 \u00b1 1.9 46.2 \u00b1 3.6\n38.2 \u00b1 2.9\n53.7 \u00b1 3.3\n49.2 \u00b1 4.2\n42.0 \u00b1 4.8\n65.0 \u00b1 0.8\n63.8 \u00b1 0.7\n56.9 \u00b1 2.4\nTable 4: Evaluation of the proposed method with a different number of labeled training data. Even\nusing less annotated data, DDPM still outperforms most baselines in Table 2.\nShare Train/Test Share for t\nBedroom-28\nFFHQ-34\n+\n+\n49.3 \u00b1 1.9\n59.1 \u00b1 1.4\n+\n-\n49.1 \u00b1 2.2\n59.3 \u00b1 1.5\n-\n-\n48.9 \u00b1 1.6\n59.3 \u00b1 1.4\nTable 5: Performance of the DDPM-based method for different feature extraction variations. All\nconsidered stochastic options provide a similar mIoU to the determinstic one.\nThe results are provided in Table 5. As one can see, the difference in the performance is marginal.\nWe attribute this behavior to the following reasons:\n\u2022 Our method uses later t of the reverse diffusion process where the noise magnitude is low.\n\u2022 Since we exploit the deep layers of the UNet model, the noise might not affect the activations\nfrom these layers signi\ufb01cantly.\nRobustness to input corruptions. In this experiment, we investigate the robustness of DDPM-\nbased representations. First, we learn pixel classi\ufb01ers on the clean images using the DDPM, SwAV\nand MAE representations on the Bedroom-28 and Horse-21 datasets. Then, 18 diverse corruption\ntypes, adopted from (Hendrycks & Dietterich, 2019), are applied to test images. Each corruption\nhas \ufb01ve levels of severity. In Figure 6, we provide mean IoUs computed over all corruption types\nfor 1, 3, 5 levels of severity, denoted as \u201cweak\u201d, \u201cmedium\u201d and \u201cstrong\u201d, respectively.\nOne can observe that the proposed DDPM-based method demonstrates higher robustness and pre-\nserves its advantage over the SwAV and MAE models even for severe image distortions.\n5\nCONCLUSION\nThis paper demonstrates that DDPMs can serve as representation learners for discriminative com-\nputer vision problems. Compared to GANs, diffusion models allow for a straightforward computa-\ntion of these representations for real images, and one does not need to learn an additional encoder,\nwhich maps images to the latent space. This DDPM\u2019s advantage and superior generative quality pro-\nvide state-of-the-art performance in the few-shot semantic segmentation task. The notable restraint\nof the DDPM-based segmentation is a requirement of high-quality diffusion models trained on the\ndataset at hand, which can be challenging for complex domains, like ImageNet or MSCOCO. How-\never, given the rapid research progress on DDPM, we expect they will reach these milestones in the\nnearest future, thereby extending the range of applicability for the corresponding representations.\n9\nPublished as a conference paper at ICLR 2022\nREFERENCES\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\nUnsupervised learning of visual features by contrasting cluster assignments.\narXiv preprint\narXiv:2006.09882, 2020.\nLiang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous\nconvolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever.\nGenerative pretraining from pixels. In ICML, 2020a.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\ncontrastive learning of visual representations. In ICML, 2020b.\nRewon Child. Very deep {vae}s generalize autoregressive models and can outperform them on\nimages. In International Conference on Learning Representations, 2021.\nElijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge Belongie. When does\ncontrastive visual representation learning work? arXiv preprint arXiv:2105.05837, 2021.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hi-\nerarchical image database. In 2009 IEEE conference on computer vision and pattern recognition,\npp. 248\u2013255. Ieee, 2009.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. 2021.\nJeff Donahue and Karen Simonyan. Large scale adversarial representation learning. NeurIPS, 2019.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-\nreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at\nscale. ICLR, 2021.\nDanil Galeev, Konstantin So\ufb01iuk, Danila Rukhovich, Mikhail Romanov, Olga Barinova, and Anton\nKonushin. Learning high-resolution domain-speci\ufb01c representations with a gan generator. In\nS+SSPR, 2020.\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar, and Ross Girshick.\nMasked\nautoencoders are scalable vision learners. arXiv:2111.06377, 2021.\nDan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common cor-\nruptions and perturbations. In International Conference on Learning Representations, 2019.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. 2020.\nTero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pp. 4401\u20134410, 2019.\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Ana-\nlyzing and improving the image quality of stylegan. 2020 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pp. 8107\u20138116, 2020.\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In Yoshua Bengio\nand Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR 2015,\nSan Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015.\nCheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. Maskgan: Towards diverse and interactive\nfacial image manipulation. In IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 2020.\nDaiqing Li, Junlin Yang, Karsten Kreis, Antonio Torralba, and Sanja Fidler. Semantic segmentation\nwith generative models: Semi-supervised learning and strong out-of-domain generalization. In\nCVPR, 2021a.\n10\nPublished as a conference paper at ICLR 2022\nHaoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff:\nSingle image super-resolution with diffusion probabilistic models. 2021b.\nLuke Melas-Kyriazi, Christian Rupprecht, Iro Laina, and Andrea Vedaldi. Finding an unsupervised\nimage segmenter in each of your deep generative models. arXiv preprint arXiv:2105.08127, 2021.\nChenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit:\nImage synthesis and editing with stochastic differential equations. 2021.\nPrafulla Nichol, Alex & Dhariwal. Improved denoising diffusion probabilistic models. ICML, 2021.\nStanislav Pidhorskyi, Donald A Adjeroh, and Gianfranco Doretto. Adversarial latent autoencoders.\nIn Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recog-\nnition (CVPR), 2020.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation. In International Conference on Medical image computing and computer-\nassisted intervention, pp. 234\u2013241. Springer, 2015.\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\nNorouzi. Image super-resolution via iterative re\ufb01nement. 2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In ICML, 2015.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn NeurIPS, 2019.\nYang Song and Stefano Ermon. Improved techniques for training score-based generative models.\nNeurIPS, 2020.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. 2021.\nOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder\nfor stylegan image manipulation. arXiv preprint arXiv:2102.02766, 2021.\nNontawat Tritrong, Pitchaporn Rewatbowornwong, and Supasorn Suwajanakorn. Repurposing gans\nfor one-shot semantic part segmentation. In CVPR, 2021.\nAndrey Voynov and Artem Babenko. Unsupervised discovery of interpretable directions in the gan\nlatent space. In ICML, 2020.\nAndrey Voynov, Stanislav Morozov, and Artem Babenko. Object segmentation without labels with\nlarge-scale generative models. ICML, 2021.\nChangxi Xu, Jianjin & Zheng. Linear semantics in generative adversarial networks. In CVPR, 2021.\nYinghao Xu, Yujun Shen, Jiapeng Zhu, Ceyuan Yang, and Bolei Zhou. Generative hierarchical\nfeatures from synthesizing images. In CVPR, 2021.\nFisher Yu, Yinda Zhang, Shuran Song, Ari Seff, and Jianxiong Xiao.\nLsun: Construction of\na large-scale image dataset using deep learning with humans in the loop.\narXiv preprint\narXiv:1506.03365, 2015.\nYuxuan Zhang, Huan Ling, Jun Gao, Kangxue Yin, Jean-Francois La\ufb02eche, Adela Barriuso, Antonio\nTorralba, and Sanja Fidler. Datasetgan: Ef\ufb01cient labeled data factory with minimal human effort.\nIn CVPR, 2021.\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic\nunderstanding of scenes through the ade20k dataset. International Journal of Computer Vision,\n127:302\u2013321, 2018.\n11\nPublished as a conference paper at ICLR 2022\nAPPENDIX\nA\nEVOLUTION OF PREDICTIVE PERFORMANCE\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.1\n0.2\n0.3\n0.4\n0.5\nmIoU\nLSUN-Cat\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\nmIoU\nLSUN-Bedroom\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\nFigure 7: The evolution of predictive performance of DDPM-based pixel-wise representations for\ndifferent UNet blocks and diffusion steps on LSUN-Cat and LSUN-Bedroom. The blocks are num-\nbered from the deep to shallow ones.\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.1\n0.2\n0.3\n0.4\nmIoU\nFFHQ-256 | Small-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\nmIoU\nFFHQ-256 | Large-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.0\n0.1\n0.2\n0.3\n0.4\nmIoU\nLSUN-Cat | Small-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.1\n0.2\n0.3\n0.4\n0.5\nmIoU\nLSUN-Cat | Large-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nmIoU\nLSUN-Bedroom | Small-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\n0\n100\n200\n300\n400\n500\n600\n700\n800\n900\n1000\nTimesteps\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nmIoU\nLSUN-Bedroom | Large-sized Objects\nBlock 2\nBlock 4\nBlock 6\nBlock 8\nBlock 10\nBlock 12\nBlock 14\nFigure 8: The evolution of predictive performance of DDPM-based pixel-wise representations on\nthe FFHQ-256, LSUN-Cat and LSUN-Bedroom datasets for classes with the smallest (Left) and\nlargest (Right) average areas.\n12\nPublished as a conference paper at ICLR 2022\nB\nDATASETDDPM & DATASETGAN SATURATION\nDatasetDDPM\nDatasetGAN\nDataset\n10k\n20K\n30K\n40K\n50K\n10K\n20K\n30K\n40K\n50K\nBedroom-28 45.1 \u00b1 2.3 46.2 \u00b1 2.3 46.1 \u00b1 2.8 47.8 \u00b1 2.3 47.9 \u00b1 2.9 30.6 \u00b1 2.3 30.4 \u00b1 3.1 30.9 \u00b1 2.4 30.9 \u00b1 2.4 31.3 \u00b1 2.7\nFFHQ-34\n55.9 \u00b1 0.8 55.8 \u00b1 0.7 55.9 \u00b1 0.7 56.0 \u00b1 0.8 55.9 \u00b1 0.7 56.4 \u00b1 1.0 56.9 \u00b1 1.0 57.0 \u00b1 1.1 57.0 \u00b1 1.2 57.0 \u00b1 1.2\nCat-15\n43.6 \u00b1 3.0 46.4 \u00b1 1.7 46.2 \u00b1 1.9 47.4 \u00b1 1.7 47.6 \u00b1 1.5 34.7 \u00b1 2.8 34.8 \u00b1 2.9 36.3 \u00b1 2.3 35.8 \u00b1 2.5 36.5 \u00b1 2.3\nHorse-21\n57.0 \u00b1 1.2 59.5 \u00b1 0.5 59.0 \u00b1 2.0 60.4 \u00b1 1.1 60.8 \u00b1 0.9 41.6 \u00b1 2.0 43.1 \u00b1 1.8 45.4 \u00b1 1.4 44.5 \u00b1 1.2 44.6 \u00b1 1.4\nTable 6: Performance of DatasetDDPM and DatasetGAN for 10K\u221250K synthetic images in the\ntraining dataset. Mean IoU of both methods saturates at 30K\u221250K of synthetic data.\nC\nTRAINING SETUP\nThe ensemble of MLPs consists of 10 independent models. Each MLP is trained for \u223c4 epochs\nusing the Adam optimizer (Kingma & Ba, 2015) with 0.001 learning rate. The batch size is 64. This\nsetting is used for all methods and datasets.\nMLP architecture. We adopt the MLP architecture from (Zhang et al., 2021). Speci\ufb01cally, we use\nMLPs with two hidden layers with ReLU nonlinearity and batch normalization. The sizes of hidden\nlayers are 128 and 32 for datasets with a number of classes less than 30, and 256 and 128 for others.\nAlso, we evaluate the performance of the proposed method for twice wider / deeper MLPs on the\nBedroom-28 and FFHQ-34 datasets and do not observe any noticeable difference, see Table 7.\nMethod\nBedroom-28 FFHQ-34\nOriginal MLP\n49.4\n59.1\nWider MLP\n49.5\n59.1\nDeeper MLP\n49.3\n58.9\nTable 7: Performance of the proposed method for twice wider / deeper MLP architecture within the\nensemble. More expressive MLPs do not improve the performance.\nD\nPER CLASS IOUS\nbackground\nperson\nthigh\nleg\nmuzzle\nhead\nbarrel\ntail\near\nneck\nsaddle\nshoulder\nback\nmane\nchest\nforelock\nhoof\nleg protection\nbridle\nnostril\neye\n0\n20\n40\n60\n80\n100\nIoU\n(a) Horse-21\nbackground\nhead\near\neye\nnose\nleg\nback\npaw\ntail\nchest\nwhiskers\nbelly\ntongue\nmouth\nneck\n0\n20\n40\n60\n80\nIoU\nDatasetDDPM\nDDPM\nDatasetGAN\n(b) Cat-15\nbackground\nneck\nhair\nteeth\nforehead\ncheek\nnose tip\nchin\ninferior lip\neyebrow\nbottom lid\nala of nose\nphiltrum\nnose\near\nbridge\nmoustache\nhead\njaw\nsuperior lip\niris\nlobule\nsclera\nfrown\nhelix\nnostril\npupil\ntop lid\ntemple\ntear duct\nsideburns\neyelashes\noral commissure\nwrinkles\n0\n20\n40\n60\n80\nIoU\n(c) FFHQ-34\nbed\n\ufb02oor\nwall\nceiling\nwindow\nlamp shade\ncurtain\npicture\ncushion\npillow\nheadboard\ntable\ncarpet\nchandelier\npicture frame\ntable top\nfootboard\nplant\nlamp column\ntable staff\ndoor\nchair\nwindow frame\nplinth\ncurtain rod\npouf\nside rail\nwardrobe\n0\n20\n40\n60\n80\nIoU\nDatasetDDPM\nDDPM\nDatasetGAN\n(d) Bedroom-28\nFigure 9: Per class IoUs for DatasetGAN, DatasetDDPM and DDPM.\n13\nPublished as a conference paper at ICLR 2022\n(a) Bedroom-28\nbed\nwall\ntable\npillow\ntable top\nheadboard\n\ufb02oor\npicture\nlamp shade\nlamp column\nceiling\npicture frame\ncushion\ntable staff\nfootboard\ncurtain\nplinth\nside rail\nwindow\ncurtain rod\nwindow frame\nchandelier\nchair\ncarpet\ndoor\nplant\npouf\nwardrobe\n0\n10\n20\n30\n40\n# masks\nReal\nDDPM\nGAN\n(b) FFHQ-34\nbackground\nnose tip\nbridge\nala of nose\nnose\nneck\noral commissure\ninferior lip\njaw\nforehead\nfrown\nhead\ncheek\nchin\ntear duct\neyebrow\nbottom lid\nphiltrum\nsuperior lip\ntemple\nsclera\niris\npupil\nhair\ntop lid\near\nlobule\neyelashes\nhelix\nnostril\nsideburns\nteeth\nmoustache\nwrinkles\n0\n5\n10\n15\n20\n# masks\nbackground\nneck\nhead\nchest\nmuzzle\near\neye\nbarrel\nmane\nback\nnostril\nshoulder\nleg\nthigh\nhoof\nbridle\ntail\nperson\nsaddle\nforelock\nleg protection\n0\n5\n10\n15\n20\n25\n30\n# masks\nReal\nDDPM\nGAN\n(c) Horse-21\nbackground\nhead\neye\nnose\near\nchest\nleg\nbelly\nback\npaw\nwhiskers\nneck\nmouth\ntail\ntongue\n0\n5\n10\n15\n20\n25\n30\n# masks\nReal\nDDPM\nGAN\n(d) Cat-15\nFigure 10: Number of instances of each semantic class in the annotated real and synthetic train sets.\nE\nDATASET DETAILS\nE.1\nCLASS NAMES\nBedroom-28: [bed, footboard, headboard, side rail, carpet, ceiling, chandelier, curtain, cushion,\n\ufb02oor, table, table top, picture, pillow, lamp column, lamp shade, wall, window, curtain rod, window\nframe, chair, picture frame, plinth, door, pouf, wardrobe, plant, table staff]\nFFHQ-34: [background, head, cheek, chin, ear, helix, lobule, bottom lid, eyelashes, iris, pupil,\nsclera, tear duct, top lid, eyebrow, forehead, frown, hair, sideburns, jaw, moustache, inferior lip, oral\ncommissure, superior lip, teeth, neck, nose, ala of nose, bridge, nose tip, nostril, philtrum, temple,\nwrinkles]\nCat-15: [background, back, belly, chest, leg, paw, head, ear, eye, mouth, tongue, tail, nose, whiskers,\nneck]\nHorse-21: [background, person, back, barrel, bridle, chest, ear, eye, forelock, head, hoof, leg, mane,\nmuzzle, neck, nostril, tail, thigh, saddle, shoulder, leg protection]\nCelebA-19: [background, cloth, ear r, eye g, hair, hat, l brow, l ear, l eye, l lip, mouth, neck, neck l,\nnose, r brow, r ear, r eye, skin, u lip]\nADE-Bedroom-30: [wall, bed, \ufb02oor, table, lamp, ceiling, painting, windowpane, pillow, curtain,\ncushion, door, chair, cabinet, chest, mirror, rug, armchair, book, sconce, plant, wardrobe, clock,\nlight, \ufb02ower, vase, fan, box, shelf, television]\nE.2\nCLASS STATISTICS\nIn Figure 10, we report the statistics of classes computed over annotated real images as well as\nannotated synthetic images produced by GAN and DDPM.\n14\nPublished as a conference paper at ICLR 2022\nF\nEXTRACTING REPRESENTATIONS FROM MAE\nTo obtain pixelwise representations, we apply the model to a fully observed image (mask ratio=0)\nof resolution 256 and extract feature maps from the deepest 12 ViT-L blocks . The feature maps from\neach block have 1024\u00d732\u00d732 dimensions. Similarly to other methods, we upsample the extracted\nfeature maps to 256\u00d7256 and concatenate them. The overall dimension of the pixel representation\nis 12288.\nIn addition, we investigated other feature extraction strategies and got the following observations:\n1. Including activations from the decoder did not provide any noticeable gains;\n2. Extracting activations right after self-attention layers caused slightly inferior performance;\n3. Extracting activations from every second encoder block also provided a bit worse results.\n15\n",
    "2404.04629": "Diffusion Model for Robust Multi-Sensor Fusion\nin 3D Object Detection and BEV Segmentation\nDuy-Tho Le1 , Hengcan Shi2\u2020 , Jianfei Cai1 , and Hamid Rezatofighi1\n1Monash University, 2 Hunan University\ntho.le1@monash.edu hengcan.shi@gmail.com\nhttps://ldtho.github.io/DifFUSER/\nBEV Map Segmentation Predictions\nCar\nMotorcycle\nPedestrian\nBarrier\nGround truth\nWalkway\nLane Divider\nDrivable area\nStopline\n3D Object Detection\u00a0Predictions\nEgo vehicle\nFig. 1: We propose to use denoising diffusion process for multi-modal BEV features\nfusion, which are then used for 3D object detection and BEV map segmentation.\nAbstract. Diffusion models have recently gained prominence as pow-\nerful deep generative models, demonstrating unmatched performance\nacross various domains. However, their potential in multi-sensor fusion\nremains largely unexplored. In this work, we introduce \u201cDifFUSER\u201d, a\nnovel approach that leverages diffusion models for multi-modal fusion\nin 3D object detection and BEV map segmentation. Benefiting from\nthe inherent denoising property of diffusion, DifFUSER is able to re-\nfine or even synthesize sensor features in case of sensor malfunction,\nthereby improving the quality of the fused output. In terms of archi-\ntecture, our DifFUSER blocks are chained together in a hierarchical\nBiFPN fashion, termed cMini-BiFPN, offering an alternative architec-\nture for latent diffusion. We further introduce a Gated Self-conditioned\nModulated (GSM) latent diffusion module together with a Progressive\nSensor Dropout Training (PSDT) paradigm, designed to add stronger\nconditioning to the diffusion process and robustness to sensor failures.\nOur extensive evaluations on the Nuscenes dataset reveal that DifFUSER\nnot only achieves state-of-the-art performance with a 70.04% mIOU in\nBEV map segmentation tasks but also competes effectively with leading\ntransformer-based fusion techniques in 3D object detection.\n\u2020 Corresponding author\narXiv:2404.04629v2  [cs.CV]  24 Sep 2024\n2\nDT. Le et al.\nKeywords: 3D Object Detection \u00b7 BEV Map Segmentation \u00b7 Diffusion\n1\nIntroduction\n3D object detection and Bird\u2019s Eye View (BEV) map segmentation are funda-\nmental tasks in autonomous driving. The former involves detecting objects in\n3D space, while the latter segments BEV maps into semantic categories. A grow-\ning trend in this field is the fusion of features from multiple sensors, leveraging\ntheir complementary strengths. For example, 3D point clouds provide geometric\ndata for 3D object detection, but lack the rich color information crucial for BEV\nsemantic segmentation \u2014a gap effectively filled by 2D images.\nRecent transformer-based fusion methods [1,47,49] have set new benchmarks\nin 3D object detection by learning feature mappings from both sensors through\ncross-attention mechanisms. Nevertheless, these methods are less adaptable for\nextra tasks like BEV map segmentation. An alternative is to create unified BEV\nrepresentations from both sensors, as seen in LSS-based [31] approaches. How-\never, these often yield sub-optimal results due to: 1) inadequately designed fusion\nmodule architectures that fail to capture the intricate relationship between the\nsensors, and 2) the inherent noise between different modalities, causing the fused\nfeatures to be noisy and inaccurate.\nTo address these challenges, we explore the use of generative models, particu-\nlarly Diffusion Probabilistic Models (DPMs), for multi-modal fusion and denois-\ning. DPMs have shown promise in various applications, including image genera-\ntion [9,37] and object detection [3]. Yet, the potential of DPMs in multi-sensor\nfusion, especially between 3D point clouds and 2D multi-view images, remains\nunexplored. To this end, we propose DifFUSER, a conditional diffusion-based\ngenerative model with enhanced fusion architecture designed for multi-modal\nlearning in 3D perception. It processes features from both sensors to output a\nrefined BEV representation, which is subsequently directed to task-specific heads\nfor 3D object detection and BEV map segmentation. We also show DifFUSER\u2019s\nrobustness in synthesizing new features to compensate for missing modality, min-\nimising performance loss and ensuring reliability even with compromised sensor.\nIn terms of architecture, our DifFUSER blocks are chained together in a\nhierarchical BiFPN (Bidirectional Feature Pyramid Network) fashion [14, 39],\ntermed cMini-BiFPN, offering an alternative architecture for latent diffusion,\nparticularly good in handling detailed and multi-scales features from different\nsensors. This architectural change is complemented by a Gated Self-conditioned\nModulated (GSM) latent diffusion module, designed to strengthen the condition-\ning modulation during diffusion, leading to more precise denoising and feature\nenhancement. We train DifFUSER end-to-end using our proposed Progressive\nSensor Dropout Training (PSDT) paradigm, together with task-specific losses,\nwhich enhance the model\u2019s robustness and capability in denoising corrupted\nBEV features and generating more accurate outputs for downstream tasks.\nDifFUSER\n3\nCamera\nFeature\nLiDAR\nFeature\nOurs\n\u00a0DifFUSER\nBEVFusion\nBEV encoder\nBEVFusion\nOurs\nGround-truth\nFig. 2: Comparison of our DifFUSER fusion module\nwith BEV encoder of the baseline. Our fusion mod-\nule\u2019s output activation map is much more expressive\nthan the baseline\u2019s, resulting in much better perfor-\nmance in downstream tasks.\nFig. 2 presents the out-\nput activation maps from the\nBEV encoder of the baseline\nBEVFusion [23] and our Dif-\nFUSER module. The activa-\ntion map from our BEVFu-\nsion module is blurry, which\nmight hinder the downstream\ntask performance. In contrast,\nour fusion module, with bet-\nter architecture design and\nthe\ndenoising\nproperty\nof\nthe diffusion model, can ef-\nfectively mitigate the noises\nand intrinsic differences be-\ntween different sensors, no-\ntably improving the predic-\ntions and strong robustness\nagainst sensor malfunction-\ning. We conduct extensive ex-\nperiments on the large-scale\nNuscenes dataset [2] to eval-\nuate the performance and de-\nsign choice of DifFUSER. The\nresults show that DifFUSER\nachieves SOTA performance\n(70.04% mIOU) in the BEV map segmentation task and performs on par with\nSOTA transformer-based fusion methods in the 3D object detection task. We\nalso conduct ablation studies to show the effectiveness of each component in\nDifFUSER.\nIn summary, our contributions are listed as follows.\n\u2013 We are first to introduce a new DPM-based approach for multi-modal fu-\nsion in 3D perception. With a well-designed diffusion architecture (cMini-\nBiFPN) and training paradigm (PSDT), our method showcases enhanced\nmulti-modal feature fusion and denoising capabilities, as well as robustness\nto sensor failure by generatively synthesizing missing sensor data.\n\u2013 We propose a Gated Self-Conditioned Modulated (GSM) latent diffusion\nmechanism within the DifFUSER blocks, reinforcing the fusion process with\nstronger conditioning and shaping the diffusion trajectory towards high-\nquality features.\n\u2013 We conduct extensive experiments on the Nuscenes dataset [2] to evaluate\nthe performance of DifFUSER. The results show that DifFUSER surpasses\nthe current SOTA by 7.34% in BEV map segmentation tasks and performs\non par with SOTA transformer-based fusion methods in 3D object detection.\n4\nDT. Le et al.\n2\nRelated Work\nSingle-modality 3D perception. Single-modality 3D perception has been\nwidely studied in the past few years and mainly comprises two categories: 1)\nLidar-based and 2) camera-based methods. Camera-based detection evolved from\nearly dense prediction pipelines [42,43] to sophisticated methods like DETR3D\n[44], BEVFormer [17] and BEVDepth [16], which integrate Bird\u2019s Eye View\n(BEV) representations with transformer attention for multi-view fusion. In con-\ntrast, LiDAR-based detection relies on point cloud data, with pioneering archi-\ntectures like PointNet [34] processing raw point clouds and subsequent develop-\nments like VoxelNet [55] transforming into structured formats like voxels [6,7,48],\nrange images [8,38], and pillars [13,14,22]. These approaches reflect the ongoing\nevolution in object detection, leveraging the unique strengths of each modality.\nMulti-modality fusion approaches in 3D perception. Fusion-based ap-\nproaches have been widely adopted in 3D perception tasks and can be cate-\ngorised into 3 main groups: 1) early fusion, 2) late fusion, and 3) feature fusion.\nEarly fusion methods directly fuse the raw data from both sensors, either by\naugmenting the raw point cloud with image features [35,40,46] or leveraging the\nimage features to guide the point cloud feature extraction [28,33]. The fusion is\nperformed sequentially, introducing additional latency to the system and prone\nto error propagation. Late fusion methods [29,30] operate on the predictions\nfrom both sensors, either by fusing the predictions [30] or by fusing the fea-\ntures extracted from both sensors [29]. The fusion is performed at the end of the\npipeline, which is more efficient than early fusion methods but fails to capture the\nfeature-to-feature relationships between both sensors. Thus the fusion results are\noften noisy and sub-optimal. Feature fusion methods [1,23,47,49] have deeper\nfeature fusion, where the features from both sensors are fused at multiple levels\nof the network. Although the fused representations are more expressive and yield\nbetter performance, LiDAR and camera features are intrinsically different and\naligning their features is still a challenging task. Most current SOTA methods\nfall into this category. Particularly, transformer-based fusion methods [1,47,49]\nhave been proposed to fuse the features from both sensors, leveraging feature-to-\nfeature relationships captured by the transformer encoder-decoder architecture\nand achieving SOTA performance in 3D object detection. However, transformer-\nbased fusion is usually computationally demanding and is not easy to adapt to\nother perception tasks such as BEV map segmentation. Another way of fusing\nmulti-modalities is to construct an unified BEV representation [18,23] from both\nsensors using LSS-based [31] backbone to lift the 2D image features to 3D space,\nand then apply multiple task-specific heads to the fused representation. This ap-\nproach is more efficient and can be easily extended to multiple perception tasks.\nHowever, the fusion result is often sub-optimal due to the inherent noise and\nmisalignment between different sensors.\nOur DifFUSER belongs to the feature-fusion sub-category, we aim to improve\nthe fusion architecture and leverage the denoising property of generative models\nto mitigate the noises and intrinsical differences between different sensors, thus\nimproving downstream tasks\u2019 performance.\nDifFUSER\n5\nSampling\n+ noise\nSegmentation\nLoss\nDetection\nLoss\nDiffusion\nLoss\nSegmentation\u00a0\nDetection\u00a0\na) Input\nb) Backbones\nc) DifFUSER\nd) Task heads\ne) Losses\nImage backbone & View\ntransform\nLidar backbone & Flatten\ncondition\nPSDT\nConcat\nGSM are used as encoder blocks\nFig. 3: Our DifFUSER framework is structured to first process input data\u2014comprising\nboth point clouds and images\u2014through respective backbones to create initial latent\nfeatures. These features are then concatenated and fed into the DifFUSER blocks.\nWithin these blocks, the concatenated feature is used as condition (partially masked\nout) to iteratively denoise the corrupted features, enhancing its quality at each step.\nThe output feature is then used for downstream tasks.\nGenerative models for 3D perception. Generative models have been re-\ncently adopted in 3D perception tasks. However, most of them are for text-to-\n3D [19,32], image-to-3D [20], and 3D trajectory prediction [11]. We notice that\nthere are concurrent works [54, 56] that utilise diffusion models for 3D object\ndetection, but they only operate on single-modality (either camera or LiDAR)\nand do not utilise diffusion for multi-modal fusion.\nIn contrast, we aim to leverage the denoising property of generative models\nto mitigate the noises and intrinsic differences between different sensors, tackling\nsome of the most fundamental tasks in autonomous driving, such as 3D object\ndetection and BEV map segmentation. Our DifFUSER is the first to adapt\ndiffusion models for multi-modal fusion for 3D perception tasks. The generated\nBEV feature can be shared and optimized end-to-end with downstream tasks.\n3\nMethodology\nIn this section, we introduce DifFUSER, a conditional generative model with\nenhanced fusion architecture, designed for multi-modal and multi-task learning\nwithin the scope of 3D object detection and BEV map segmentation. Our Dif-\nFUSER features take the extracted features from lidar and camera sensors and\nprocess them in three ways: one used as the diffusion target, one infused with\nGaussian noise (xF\nt ) and one is partially masked by PSDT (\u02dcxF\n0 ). \u02dcxF\n0 and xF\nt are\nthen input into the cMini-BiFPN (with GSM as encoder blocks) module, where\nthey are denoised and refined. This enhanced representation is finally utilised by\ntask-specific heads for precise 3D object detection and BEV map segmentation.\n3.1\nFuser Architecture Design (cMini-BiFPN).\nIn autonomous driving, where fast processing is the top priority, the need to\nensure both the performance and efficiency of the diffusion module is impor-\n6\nDT. Le et al.\ntant. Thus, we introduce a conditional BiFPN-like diffusion architecture, which\nwe term as the conditional-Mini-BiFPN (cMini-BiFPN). Note that we use the\nterm cMini-BiFPN to indicate the bi-directional feature fusion alone, without\nthe GSM module. As depicted in Fig. 3, the initial three blocks F 1\nin, F 2\nin, F 3\nin\u2014the\nencoder blocks\u2014process both partially masked original \u02dcxF\n0 and perturbed BEV\nfeatures xF\nt , where \u02dcxF\n0 serves as a guide for the diffusion process, aiding in\nthe progressive refinement of the corrupted BEV feature. Subsequent blocks\nF 1\nout, F 2\nout, F 3\nout\u2014the decoder blocks\u2014accept the conditioned BEV representa-\ntion and apply iterative top-down and bottom-up bi-directional fusion to gener-\nate the final BEV feature. Particularly,\n  \n\\s c aleb\no\nx\n \n{0.\n9}{ $\nF_{it\n}^{2}=\\ope r\nato\nrn\name \n{Co\nnv}\\left (\\delta \\left (\\frac {\\left (w_{1}^{\\prime } \\cdot F_{i n}^{2}+w_{2}^{\\prime } \\cdot Resize\\left (F_{i n}^{3}\\right )\\right .}{w_{1}^{\\prime }+w_{2}^{\\prime }+\\varepsilon }\\right )\\right )$} \\label {eq:f2_up} \n(1)\n  \n\\sc a lebo\nx\n \n{\n0.9}\n{ $F _\n{out}^\n{ 2} =\n\\opera\nt orname {C o\nnv}\\\nlef\nt  (\\d\ne lta \n\\ le\nft (\\frac {\\left (w_{1}^{\\prime \\prime } \\cdot F_{in}^{2} +w_{2}^{\\prime \\prime } \\cdot F_{it}^{2}+ w_{3}^{\\prime \\prime } \\cdot Resize\\left (F_{out}^{1}\\right ) \\right .}{w_{1}^{\\prime \\prime }+w_{2}^{\\prime \\prime }+w_{3}^{\\prime \\prime } +\\varepsilon }\\right )\\right )$} \\label {eq:f2} \n(2)\nwhere \u03b4 denotes the Swish activation function [39], F 2\nit represents the inter-\nmediate fusion result of F 2\nin and F 3\nin, and F 2\nout is computed by blending F 2\nit,\nF 1\nout, and F 2\nin. The weights w\u2032\n1,2,3 and w\u2032\u2032\n1,2,3 are learnable and optimised during\ntraining. F 1\nout and F 3\nout are derived similarly to F 2\nout. The final feature map is\nthe concatenation of F 1\nout, F 2\nout, and F 3\nout, which is then processed through a\nTransfusion-L [1] detection head and a segmentation head [23], facilitating both\n3D object detection and BEV map segmentation. We believe that this stream-\nlined and efficient cMini-BiFPN architecture is well-suited for the demanding\ndiffusion processes in autonomous driving applications, balancing computational\nefficiency with the necessity for expeditious sampling.\n3.2\nProgressive Sensor Dropout Training (PSDT)\nSensor reliability in autonomous driving is crucial. Cameras and LiDAR sys-\ntems, the eyes of robotic agents, can occasionally be occluded or fail, potentially\ncompromising safety and operational efficiency. Recognising this, we propose\nthe Progressive Sensor Dropout Training (PSDT) method, which enhances our\nDifFUSER model\u2019s robustness and adaptability, particularly in scenarios where\nsensor inputs may be occluded or failed. It enables the model to generate or\nreconstruct missing features from camera or LiDAR inputs by leveraging the\nlearned distribution across both modalities. This approach ensures consistently\nhigh performance, even in cases of sensor malfunction or when sensor data is\npartially missing, closely simulating real-world operational challenges.\nSpecifically in PSDT, xF\n0 , the input for the whole fusion process, containing\nfeatures from both camera and LiDAR, is utilised in three key ways: as a training\ntarget, as a noisy input for the diffusion module, and as a diffusion condition\nwith randomly dropped camera or LiDAR features. To simulate the conditions\nof sensor dropout or malfunction, we progressively increase the dropout rate for\neither the camera or LiDAR inputs from 0% to a predefined maximum \u03b1 = 25\nDifFUSER\n7\nduring training. This is represented as:\n  \nF_{\\te x t  { masked}}^\nm  \n= F ^ m\n \n\\odot \\text {Bernoulli}\\left (\\frac {\\alpha }{100} \\cdot \\frac {e}{E}\\right ) \n(3)\nwhere e represents the current epoch out of the total E epochs, defining the\ndropout probability p_{\\text {dropout}}(e) that dictates the chance of each feature in modal-\nity F ^m being dropped. This strategic process, achieved by applying a binary\nmask generated from a Bernoulli distribution based on p_{\\text {dropout}}(e), not only\ntrains the model to effectively denoise and generate more expressive features\nbut also minimises its reliance on any single sensor, thereby enhance its capabil-\nity to handle incomplete sensor data with greater resilience.\n3.3\nGated Self-Conditioned Modulated (GSM) Diffusion Module\nDeparting from traditional diffusion models [9,37], which primarily use just t as\nthe condition, the GSM module leverages both the temporal (time step t) and\nthe sensor information contained within the partially masked latent sample (\\pr\notect \\tilde  {x}^F_0 )\nto guide the diffusion process. This requires stronger conditioning on the noisy\nlatent to shape the diffusion trajectory towards generating high-quality features.\nThis dual-conditioning approach, augmented by the PSDT paradigm and task-\nspecific losses, ensures the diffusion process focuses on feature enhancement and\nnot mere replication, encourage the synthesis of more expressive features.\nInput Feature\nConditioning\nConv2D\nNormalize\nConv Block\nGate\nActivation\nSummation\nSigmoid\nMultiplication\nShift\nScale\nFig. 4: GSM diffusion block.\nSpecifically, in DifFUSER,\nthe GSM module is used in\nencoder blocks F 1\nin, F 2\nin, F 3\nin\n(see Fig. 3). The GSM dif-\nfusion process is encapsulated\nby a FiLM-like (Feature-wise\nLinear\nModulation)\nopera-\ntion. This modulation com-\nputes\nparameters\nthat\ndy-\nnamically adjust the diffusion\ntrajectory based on the condi-\ntions \u02dcxF\n0 . The modulation for-\nmula is defined as:\n  \\\nb e gin\n { aligned} \\ti\nld e  {x} ^ F_0\n & = \\t\nil d e { x}^\nF _ 0 +\n \\ t ex t it \n{ t ime\n\\_e\nm b }( t ), \n\\ \\  \\ g amm a  _t &= \\sigma (W_{\\gamma } \\cdot \\tilde {x}^F_0 + b_{\\gamma }), \\\\ \\alpha _t &= W_{\\alpha } \\cdot \\tilde {x}^F_0 + b_{\\alpha }, \\\\ \\beta _t &= W_{\\beta } \\cdot \\tilde {x}^F_0 + b_{\\beta }, \\\\ \\hat {x}^F_0 &= \\gamma _t \\cdot (x^F_t \\cdot (1+\\alpha _t) + \\beta _t). \\end {aligned} \\label {eq:GSM} \n(4)\nHere, \\pr\notect \\tilde  {x}^F_0 is the version of x^\nF_0 that may be partially masked due to PSDT. The\nparameters \\alpha _t, \\beta _t, and \\gamma _t modulate the noisy input feature x^\nF_t via scaling and\nshifting, with \\gamma _t acting as a sigmoid-gated self-conditioning element to recalibrate\n8\nDT. Le et al.\nlayer activation based on the feature itself. These parameters are derived from the\ncondition \\pr\notect \\tilde  {x}^F_0 using a simple convolution block, we deliberately limit gradient flow\nwhen introducing noise to x^\nF_0 to prevent model memorisation. The architecture\nof our gated self-conditioned modulated diffusion mechanism is detailed in Fig. 4.\n3.4\nDifFUSER training and loss\nDifFUSER is trained end-to-end in either single-task or multi-task fashion, to-\ngether with task-specific losses, which encourage the module to not just denois-\ning but also generating more expressive BEV features serving the downstream\ntasks. In the forward noising process, Gaussian noise \u03b5 is gradually added to the\nextracted features xF\n0 via a discrete stochastic differential equation (SDE):\n  q(\nx ^ F_\nt ,  x ^F_0)\n =  \n\\\ns q rt {\\bar {\\alpha _t}}x^F_0 + \\varepsilon \\sqrt {1 - \\bar {\\alpha _t}} \n(5)\nwhere \u00af\u03b1t is a hyperparameter and \u03b5 \u223cN(0, I). If the time step t is large, xF\nt\nwould become Gaussian noise (see Appendix for more discussion).\nThe reverse denoising process is a discrete SDE that gradually maps a Gaus-\nsian noise into a sample. At each time step, given xF\nt , t, and \u02dcxF\n0 , it predicts the\nnext reverse step:\n  p^{\\l\neft ( \nt  \\ri\ng h t )\n}\n(\nx\n^F_{\nt -1}\n |  x^\nF _ t, \\ti\nl\nde  { x\n}^F_\n0, t) \n=  \\beg\ni n {\nc a ses\n}  \\ma\nthcal {N}\\left ( p^{\\left ( 1 \\right )}_\\theta (x^F_1, \\tilde {x}^F_0, t), \\sigma ^2I \\right ) & \\text {if } t = 1 \\\\ q(x^F_{t-1} | x^F_t, p^{\\left ( t \\right )}_\\theta (x^F_t,\\tilde {x}^F_0, t)) & \\text {otherwise.} \\end {cases} \n(6)\nDDPM sampler [9] is not considered a practical model due to its slow sampling\nspeed. Later, we explore and compare the potential of using training-free faster\ndiffusion samplers, such as the first-order sampler DDIM [37], or higher-order\nsamplers like DPM-Solver++ [26] and DEIS [52].\nDiffusion loss. Instead of using \u03b5-prediction from DDPM [9], we train our model\nto predict the denoised and unmasked sample xF\n0 from the noisy xF\nt at each time\nstep, and use a simple mean-squared error loss on the prediction, thanks to the\nparameterisation of \u00b5\u03b8 as a denoising network p\u03b8 :\n  \\label { e q:diffu sion_\nl oss\n} \\mathc\na l {\nL } _{ dif\nf u s io\nn\n} =\n \\mathbb {E}_{t \\sim [1,T], \\widetilde {x}^F_t \\sim q_t} \\left [ \\left \\| p^{\\left ( t \\right )}_\\theta (x^F_t,t, \\tilde {x}^F_0) - x^F_0 \\right \\|^2 \\right ] \n(7)\nwhere exF\n0 is the original partially masked feature from both sensors, xF\nt is\nthe noisy feature at time step t, qt is the distribution of xF\nt at time step t, and\np(t)\n\u03b8 (xF\nt , t, xF\n0 )) is the predicted \u02c6xF\n0 at time step t.\nSegmentation loss. We use focal loss Lseg for the segmentation head, as in [23].\nDetection loss. Similar to [1,23], the Hungarian set prediction loss is applied to\nthe detection head, which is a combination of the classification focal loss and the\nsmooth L1 regression loss. Top K predictions are selected and matched with the\nground truth boxes by Hungarian matching. The detection loss is then calculated\nas Ldetection = \u03bbclsLcls + \u03bbregLreg.\nDifFUSER\n9\nTotal loss. The total loss is the weighted sum of the diffusion loss, the detection\nloss, and the segmentation loss, depending on the training mode:\n  \\lab e l {eq:tota l _loss} \\ m athcal {L}_{total} = \\mathcal {L}_{diffusion} + \\lambda _{seg} \\mathcal {L}_{seg} + \\lambda _{det} \\mathcal {L}_{detection} \n(8)\nwhere \u03bbdiff, \u03bbdet, \u03bbseg are the weights for each loss.\n4\nExperiment\nOur experiments focus on two critical tasks in autonomous driving: 3D object\ndetection and Bird-Eye-View (BEV) map segmentation. 3D object detection is\nessential for accurately identifying and locating objects within a 3D space, such\nas vehicles, pedestrians, and road obstacles. BEV map segmentation, on the\nother hand, involves partitioning the bird\u2019s-eye view of the driving environment\ninto different semantic categories, aiding in path planning and navigation. Both\ntasks benefit significantly from the fusion of LiDAR and camera data, offering\ncomprehensive spatial and contextual understanding.\nDataset. We report DifFUSER\u2019s performance on the nuScenes [2] dataset, a\nchallenging large-scale benchmark with multiple multi-modal data annotations\nfor autonomous driving (3D bounding box and map segmentation), which is\nsuitable for our method. This dataset comprises roughly 1000 scenes, split into\n700 for training, 150 for validation, and another 150 for testing. Each scene in\nthe dataset represents a 20-second duration, recorded using six cameras and a\nLiDAR, along with additional data from five radars. The cameras, calibrated for\nprecision, capture RGB images at a rate of 12 FPS, encompassing a full 360-\ndegree horizontal field of view. Simultaneously, the 32-beam LiDAR scans the\nenvironment at 20 FPS. We do not use Radar data in our experiments.\nBEV map segmentation. BEV map segmentation, an integral part of au-\ntonomous driving, involves segmenting the bird\u2019s-eye view of the road into dis-\ntinct semantic categories. This task aids in path planning and navigation by\nproviding a comprehensive overview of the road layout and relevant features.\nWe adopt the training approach of the baseline BEVFusion [23], assessing IOUs\nof drivable areas, pedestrian crossings, walkways, stop lines, carparks, dividers,\nand class-averaged mean IoU (mIOU). Our evaluation is conducted in a 100m x\n100m area centered around the ego vehicle and reports mIOU.\n3D object detection. Next, we evaluate DifFUSER on 3D object detection.\nNuScenes [2] dataset provides 3D bounding boxes for 10 object classes, namely\ncar, truck, bus, trailer, construction vehicle, pedestrian, motorcycle, bicycle, traf-\nfic cone, barrier. Adhering to the dataset\u2019s established protocols, we transform\nthe data from the preceding nine frames into the current point cloud frame for\nboth training and evaluation. We use the official evaluation metrics for 3D ob-\nject detection, namely NDS and mAP. And mean Average Translation Error\n(mATE), mean Average Scale Error (mASE), mean Average Orientation Error\n(mAOE), mean Average Velocity Error (mAVE), and mean Average Attribute\nError (mAAE) are reported. Higher NDS and mAP indicate better performance.\n10\nDT. Le et al.\nTable 1: DifFUSER outperforms state-of-the-art multi-sensor fusion methods on BEV\nmap segmentation on nuScenes val with a 6.4% improvement across categories. Abbre-\nviations: Mod. = Modality, Driv. = Drivable, Ped. Cr. = Pedestrian Crossing, Wlk. =\nWalkway, Stp. Ln. = Stop Line, Cpk. = Carpark, Div. = Divider. Relative improve-\nments over the baseline are shown below. Bold, blue, and underlined values indicate\nthe best, second-best, and baseline performance, respectively.\nMethod\nMod.\u2191Driv.\u2191Ped. Cr.\u2191Wlk.\u2191Stp. Ln.\u2191Cpk.\u2191Div.\u2191Mean\u2191\nLSS [31]\nC\n75.4\n38.8\n46.3\n30.3\n39.1\n36.5\n44.4\nCVT [53]\nC\n74.3\n36.8\n39.9\n25.8\n35.0\n29.4\n40.2\nM2BEV [45]\nC\n77.2\n-\n-\n-\n-\n40.5\n-\nBEVFusion [23]\nC\n81.7\n54.8\n58.4\n47.4\n50.7\n46.4\n56.6\nPointPillars [13]\nL\n72.0\n43.1\n53.1\n29.7\n27.7\n37.5\n43.8\nCenterPoint [50]\nL\n75.6\n48.4\n57.5\n36.5\n31.7\n41.9\n48.6\nPointPainting [40]\nLC\n75.9\n48.5\n57.1\n36.9\n34.5\n41.9\n49.1\nMVP\nLC\n76.1\n48.7\n57.0\n36.9\n33.0\n42.2\n49.0\nBEVFusion [23]\nLC\n85.5\n60.5\n67.6\n52.0\n57.0\n53.7\n62.7\nDifFUSER (Ours)\nLC\n89.7\n69.0\n74.3\n61.9\n63.0\n62.3\n70.0\nImprovement\n\u21914.2\n\u21918.5\n\u21916.7\n\u21919.9\n\u21916.0\n\u21918.6\n\u21917.3\nTraining Details. Unless specified otherwise, all models are trained using\nPSDT, with a batch size of 3 on two NVIDIA 4090 GPUs. Following [23], we\nkeep Swin-T [21] and Voxelnet [55] as image and LiDAR backbones, respectively.\nWe reduce the resolution of camera images to 448\u00d7800 pixels and convert the\nLiDAR point cloud into voxels, using a size of 0.075 meters for detection and\n0.1 meters for segmentation. To accommodate different spatial requirements of\ndetection and segmentation, we use grid sampling with bilinear interpolation\nto adjust BEV feature maps before each specific task. For 3D object detection,\nwe first pretrain LiDAR only backbone for 15 epochs, then finetune the fusion\nmodule for 10 epochs with AdamW [25] optimizer with a learning rate of 3.75e-5\nand a Cosine Annealing scheduler [24]. For BEV Map Segmentation, we train\nthe model for 20 epochs with AdamW optimizer with a learning rate of 2e-5 and\na OneCycle scheduler [36]. We use the same training strategy in our ablation\nstudy for fair comparison. We also study joint 3D detection and segmentation\ntraining in our ablation study using a shorter training schedule for both tasks.\n5\nDicussion and Comparison with State-of-the-arts\nBEV Map Segmentation. Tab. 1 shows the comparison of BEV map seg-\nmentation performance on the NuScenes val set. DifFUSER exhibits a remark-\nable performance leap over the baseline BEVFusion [23]. While BEVFusion [23]\nachieves an mIOU score of 62.7, DifFUSER significantly elevates this to 70.04,\nmarking a substantial 7.3% improvement. This pronounced advancement is con-\nsistent across various categories, as in Tab. 1 all outperforming their respective\nscores in BEVFusion [23]. These results underscore DifFUSER\u2019s capability to\nenhance the quality of fused features, which is crucial in the segmentation task.\nDifFUSER\n11\nTable 2: Comparison of 3D object detection performance on the Nuscenes test dataset.\nMod. abbreviates Modality, where L and C denote LiDAR and Camera, respectively.\nRelative improvement over the baseline is shown in a separate row. Bold, blue, and un-\nderlined values indicate the best, second-best, and baseline performance, respectively.\nMethods\nMod.\u2191NDS\u2191mAP\u2191mATE\u2193mASE\u2193mAOE\u2193mAVE\u2193mAAE\u2193\nPointPainting [40]\nLC\n61.0\n54.1\n0.380\n0.260\n0.541\n0.293\n0.131\nPointAugmenting [41]\nLC\n71.1\n66.8\n0.253\n0.235\n0.354\n0.266\n0.123\nMVP [51]\nLC\n70.5\n66.4\n0.263\n0.238\n0.321\n0.313\n0.134\nFusionPainting [46]\nLC\n71.6\n68.1\n0.256\n0.236\n0.346\n0.274\n0.132\nUVTR [15]\nLC\n71.1\n67.1\n0.306\n0.245\n0.351\n0.225\n0.124\nTransFusion [1]\nLC\n71.7\n68.9\n0.259\n0.243\n0.359\n0.288\n0.127\nBEVFusion [18]\nLC\n71.8\n69.2\n0.260\n0.250\n0.370\n0.270\n0.130\nBEVFusion [23]\nLC\n72.9\n70.2\n0.261\n0.239\n0.329\n0.260\n0.134\nDeepInteraction [49]\nLC\n73.4\n70.8\n0.257\n0.240\n0.325\n0.245\n0.128\nMSMDFusion [12]\nLC\n73.0\n71.0\n0.250\n0.240\n0.330\n0.260\n0.140\nLinK [27]\nLC\n73.4\n69.8\n0.258\n0.229\n0.312\n0.234\n0.136\nFocalFormer3D [5]\nLC\n73.9\n71.6\n0.250\n0.240\n0.330\n0.220\n0.130\nLargeKernel-F [7]\nLC\n74.1\n71.1\n0.236\n0.228\n0.298\n0.241\n0.131\nCMT [47]\nLC\n74.1\n72.0\n0.279\n0.235\n0.308\n0.259\n0.112\nDifFUSER (ours)\nLC\n73.8\n71.3\n0.250\n0.241\n0.317\n0.244\n0.130\nImprovement\n\u21911.9\n\u21911.0\n\u21930.011\n\u21910.002\n\u21930.012\n\u21930.016\n\u21930.004\nTable 3: Results for 3D object detection\non NuScenes (val). Bold, blue, and un-\nderlined values indicate the best, second-\nbest, and baseline performance, respec-\ntively. Mod. stands for Modality.\nMethods\nMod. NDS\u2191mAP\u2191\nTransFusion [1]\nL\n70.1\n65.1\nFUTR3D [4]\nLC\n68.3\n64.5\nUVTR [15]\nLC\n70.2\n65.4\nTransFusion [1]\nLC\n71.3\n67.5\nBEVFusion [23]\nLC\n71.4\n68.5\nDeepInteration [49]\nLC\n72.6\n69.9\nCMT [47]\nLC\n72.9\n70.3\nDifFUSER\nLC\n72.6\n70.0\nImprovement\n\u21911.2\n\u21911.5\nFig. 5: Performance Comparison of Dif-\nFUSER with and without Sensor Drop (ei-\nther Lidar or Camera)\n1\n2\n4\n8\nSteps\n45\n50\n55\n60\n65\n70\nmIOU\n63.88\n66.65\n69.38\n70.04\n42.48\n45.63\n48.22\n48.01\n55.4\n58.05\n59.28\n61.15\nLidar & Camera\nLidar Only\nCamera Only\nLidar Baseline\nCamera Baseline\nFig. 6 shows the qualitative results of DifFUSER\u2019s BEV map segmentation on\nthe NuScenes val set. BEVFusion tends to produce noisy segmentation results,\nespecially at further distances, where the LiDAR point cloud is sparse and sen-\nsor misalignment is more pronounced. In contrast, DifFUSER produces more\naccurate segmentation results, with finer details and less noise.\n3D Object Detection. Tab. 2 shows the comparison of 3D object detection\nperformance on the NuScenes test set. DifFUSER demonstrates notable improve-\nment over the baseline BEVFusion [23]. While BEVFusion [23] already showed\n12\nDT. Le et al.\nGround-truth\nOurs\nBevfusion\nWalkway\nLane Divider\nDrivable area\nStopline\nCar park\nPedestrian Crossing\nFig. 6: Qualitative result on BEV map segmentation task on the Nuscenes val set,\ncompared to BEVFusion [23]. Green bounding boxes highlight the areas where the\nbaseline method fails to segment, while our method can segment correctly.\nimpressive results with an NDS of 72.9 and an mAP of 70.2, DifFUSER fur-\nther enhances these metrics to 73.8 (+0.9%) and 71.2 (+1%), competing closely\nwith CMT [47], which achieves an NDS of 74.1 and an mAP of 72.0. This im-\nprovement is attributed to the more effective multi-modal diffusion-based fusion.\nSpecifically, DifFUSER\u2019s denoising capability contributes to finer detail preser-\nvation and noise reduction in fused features, leading to better object detection\naccuracy. The improvement is also evident in other metrics, with DifFUSER\nachieving lower error rates (mATE, mASE, mAOE, mAVE, mAAE) compared\nto BEVFusion [23]. This suggests that DifFUSER not only enhances the detec-\ntion accuracy but also improves the overall reliability and precision of the object\ndetection task. The same trend is observed in the validation set, as in Tab. 3.\nRobustness to sensor failure. To highlight the robustness of DifFUSER under\nsensor failures, we present the segmentation performance comparison in Fig. 5.\nThe findings reveal that with a sufficient number of sampling steps, the model is\ncapable of generating features that effectively compensate for and act as replace-\nments for the missing sensor data. This capability is a significant advancement\nover existing models like CMT [47], which, under similar missing sensor condi-\ntions, can only match the performance of single sensor baselines. Our DifFUSER\nmodel distinguishes itself by not just matching but exceeding the baseline per-\nformance achieved with single sensor inputs thanks to the synthetic features it\ngenerates. The ability to generate and utilise synthetic features effectively miti-\ngates the reliance on any single sensor modality, ensuring the model\u2019s operational\nresilience in diverse and challenging environments.\nDifFUSER\n13\n6\nAblation Study\nGSM diffusion block design. We conducted experiments about the compo-\nnents of the GSM module, where all models are trained with a shorter training\nschedule (6 epochs). As shown in Tab. 7, by incrementally incorporating Scale,\nShift, and Gate operations, we observed a marked improvement in mean In-\ntersection over Union (mIOU), with the Gate operation significantly boosting\nthe mIOU to 62.17. This underscores the effectiveness of GSM\u2019s dynamic feature\nmodulation, especially its capacity for strong conditioning, which selectively em-\nphasises relevant features, enabling the model to generate meaningful outputs.\nContribution of each component. We investigate the contribution of each\ncomponent in DifFUSER to understand whether the improvement comes from\nthe enhanced cMini-BiFPN fusion architecture or the gated self-conditioned la-\ntent diffusion module. We conduct experiments on the NuScenes val set with\nthe same settings as in Sec. 4, using single-task models. It can be seen that by\nadding BiFPN-like feature fusion module, the performance of BEV map seg-\nmentation and 3D object detection are improved by 3.8% in mIOU and 0.4% in\nNDS, respectively. Further integrating the GSM module with a 4-step sampling\nimproves mIOU by an additional 2.3% and NDS by an additional 0.3%. Extend-\ning the diffusion sampling to 8 steps further raises mIOU to 70.04% and NDS\nto 72.6%, which is the final performance of DifFUSER.\nDiffusion ODE Solvers. We perform a small experiment to compare the per-\nformance of other second-order ODE solvers besides the de-facto standard first-\norder DDIM solver. We choose BEV map segmentation for this experiment and\nreport the segmentation mIOU results. We use the same settings as those in\nSec. 4, and compare the performance of DDIM [37], DPM-Solver++ [26] and\nDEIS [52] at different sampling steps. As shown in Tab. 4, the performance of\nDPM-Solver++ and DEIS are comparable to DDIM, and the performance of\nall three methods are improved as the number of sampling steps increases. It\nshows that all solvers can lead to good convergence of the fused features, and\nthe performance is not sensitive to the choice of solvers. For best performance,\nWe choose DDIM with 8 sampling steps as the default solver in our experiments\ndue to its simplicity and efficiency. However, in terms of efficiency, we believe\n4-step sampling is already sufficient.\nInference Time. Here we measure the inference time of DifFUSER on a single\nNVIDIA 4090 GPU. At 1-step sampling, Diffuser can reach an average speed of\n8 FPS (123.1 ms latency), the full break-down is shown in Tab. 8\nMulti-task joint training. Finally, we investigate the performance of Dif-\nFUSER in multi-task joint training. We use a shorter training schedule and\nre-scale the losses of each task to balance the training. As shown in Tab. 5, the\nperformance of both tasks decreases compared to single-task training, an issue\ncommonly known as negative transfer, which we leave for future investigation.\n1 Including BEV-Pooling, a non-DL operation without extra parameters.\n14\nDT. Le et al.\nTable 4: Performance comparison of differ-\nent sampling methods at varying steps\nSampling step\n1\n2\n4\n8\nDDIM [37]\n63.88 66.65 68.82 70.04\nDPM-Solver [26] 64.02 66.20 68.90 70.02\nDeIS [52]\n63.80 66.30 68.81 69.95\nTable 5: Multi-task joint training per-\nformance\nTask\nNDS mIOU\nDetection\n69.59\n-\nSegmentation\n-\n65.8\nJoint\n68.23\n61.36\nTable 6: Comparison of Detection and Seg-\nmentation Performance\nComponent\nmAP NDS mIOU\nBaseline**\n68.9\n71.4\n62.7\n+cMiniBiFPN**\n69.4\n71.8\n66.5\n+GSM (4-step) + PSDT* 69.7\n72.1\n68.82\n+GSM (8-step) + PSDT* 70.0 72.6\n70.04\n[*] With Ldiffusion, [**] Without Ldiffusion\nTable 7: GSM diffusion block design.\nScale Shift Gate mIOU\n\u2713\n59.92\n\u2713\n\u2713\n60.73\n\u2713\n\u2713\n\u2713\n62.17\nTable 8: Detailed breakdown of inference speeds, excluding the speed of point cloud\nvoxelisation module, following our baseline.\nModule\nLatency Parameters GFLOPs MACs\nCamera\n63ms1\n27.5M\n127.7\n63.8\nLiDAR\n25ms\n7.1M\n32.3\n16.2\nFuser\n24ms\n37.5M\n294.0\n147.0\nTask heads\n11ms\n4.7M\n189.0\n94.5\nTotal\n123ms\n76.8M\n643.0\n321.5\n7\nConclusion\nIn this work, we propose DifFUSER, a diffusion-based generative model for\nmulti-modal fusion in 3D perception. DifFUSER is designed to enhance the\nquality of fused features by improving the fusion architecture and leveraging\nthe denoising property of diffusion models. Central to our approach is the in-\ntroduction of a Gated Self-Conditioned Modulated (GSM) latent diffusion mod-\nule and PSDT paradigm, specifically engineered to improve the fusion output\nfeatures and robustness against sensor noise and failure. Additionally, cMini-\nBiFPN fusion architecture emerged as a promising multi-scale latent diffusion\nmodule. Extensive experiments on the NuScenes dataset show that DifFUSER\nachieves SOTA performance in BEV map segmentation tasks and can fairly com-\npete with SOTA transformer-based fusion methods in 3D object detection. This\nshows the potential of generative models to significantly enhance the capability\nof autonomous driving systems. Our approach opens a direction of using diffu-\nsion models for better accuracy and reliability in 3D perception tasks, promis-\ning further enhancements and broader applicability of diffusion-based models in\ncomplex, real-world environments.\nDifFUSER\n15\nAcknowledgments. This work has been partially funded by The Australian\nResearch Council Discovery Project (ARC DP2020102427). We acknowledge the\npartial sponsorship of our research by DARPA Assured Neuro Symbolic Learning\nand Reasoning (ANSR) program, under award number FA8750-23-2-1016.\nReferences\n1. Bai, X., Hu, Z., Zhu, X., Huang, Q., Chen, Y., Fu, H., Tai, C.L.: Transfusion:\nRobust lidar-camera fusion for 3d object detection with transformers. In: CVPR.\npp. 1090\u20131099 (2022)\n2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,\nPan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous\ndriving. In: CVPR. pp. 11621\u201311631 (2020)\n3. Chen, S., Sun, P., Song, Y., Luo, P.: Diffusiondet: Diffusion model for object de-\ntection. In: ICCV. pp. 19830\u201319843 (2023)\n4. Chen, X., Zhang, T., Wang, Y., Wang, Y., Zhao, H.: Futr3d: A unified sensor fusion\nframework for 3d detection. In: CVPR. pp. 172\u2013181 (2023)\n5. Chen, Y., Yu, Z., Chen, Y., Lan, S., Anandkumar, A., Jia, J., Alvarez, J.M.:\nFocalformer3d: focusing on hard instance for 3d object detection. In: Proceedings\nof the IEEE/CVF International Conference on Computer Vision. pp. 8394\u20138405\n(2023)\n6. Chen, Y., Li, Y., Zhang, X., Sun, J., Jia, J.: Focal sparse convolutional networks\nfor 3d object detection. In: CVPR. pp. 5428\u20135437 (2022)\n7. Chen, Y., Liu, J., Zhang, X., Qi, X., Jia, J.: Largekernel3d: Scaling up kernels in\n3d sparse cnns. In: CVPR. pp. 13488\u201313498 (2023)\n8. Fan, L., Xiong, X., Wang, F., Wang, N., Zhang, Z.: Rangedet: In defense of range\nview for lidar-based 3d object detection. In: ICCV. pp. 2918\u20132927 (2021)\n9. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. NeurIPS 33,\n6840\u20136851 (2020)\n10. Ho,\nJ.,\nSalimans,\nT.:\nClassifier-free\ndiffusion\nguidance.\narXiv\npreprint\narXiv:2207.12598 (2022)\n11. Jiang, C., Cornman, A., Park, C., Sapp, B., Zhou, Y., Anguelov, D., et al.: Mo-\ntiondiffuser: Controllable multi-agent motion prediction using diffusion. In: CVPR.\npp. 9644\u20139653 (2023)\n12. Jiao, Y., Jie, Z., Chen, S., Chen, J., Ma, L., Jiang, Y.G.: Msmdfusion: Fusing lidar\nand camera at multiple scales with multi-depth seeds for 3d object detection. In:\nProceedings of the IEEE/CVF conference on computer vision and pattern recog-\nnition. pp. 21643\u201321652 (2023)\n13. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars:\nFast encoders for object detection from point clouds. In: CVPR. pp. 12697\u201312705\n(2019)\n14. Le, D.T., Shi, H., Rezatofighi, H., Cai, J.: Accurate and real-time 3d pedestrian\ndetection using an efficient attentive pillar network. RA-L 8(2), 1159\u20131166 (2022)\n15. Li, Y., Chen, Y., Qi, X., Li, Z., Sun, J., Jia, J.: Unifying voxel-based representation\nwith transformer for 3d object detection. NeurIPS 35, 18442\u201318455 (2022)\n16. Li, Y., Ge, Z., Yu, G., Yang, J., Wang, Z., Shi, Y., Sun, J., Li, Z.: Bevdepth:\nAcquisition of reliable depth for multi-view 3d object detection. In: AAAI. vol. 37,\npp. 1477\u20131485 (2023)\n16\nDT. Le et al.\n17. Li, Z., Wang, W., Li, H., Xie, E., Sima, C., Lu, T., Qiao, Y., Dai, J.: Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera images via spatiotem-\nporal transformers. In: European conference on computer vision. pp. 1\u201318. Springer\n(2022)\n18. Liang, T., Xie, H., Yu, K., Xia, Z., Lin, Z., Wang, Y., Tang, T., Wang, B., Tang,\nZ.: Bevfusion: A simple and robust lidar-camera fusion framework. NeurIPS 35,\n10421\u201310434 (2022)\n19. Lin, C.H., Gao, J., Tang, L., Takikawa, T., Zeng, X., Huang, X., Kreis, K., Fidler,\nS., Liu, M.Y., Lin, T.Y.: Magic3d: High-resolution text-to-3d content creation. In:\nCVPR. pp. 300\u2013309 (2023)\n20. Liu, R., Wu, R., Van Hoorick, B., Tokmakov, P., Zakharov, S., Vondrick, C.: Zero-\n1-to-3: Zero-shot one image to 3d object. In: ICCV. pp. 9298\u20139309 (2023)\n21. Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., Lin, S., Guo, B.: Swin\ntransformer: Hierarchical vision transformer using shifted windows. In: ICCV. pp.\n10012\u201310022 (2021)\n22. Liu, Z., Zhao, X., Huang, T., Hu, R., Zhou, Y., Bai, X.: Tanet: Robust 3d object\ndetection from point clouds with triple attention. In: AAAI. vol. 34, pp. 11677\u2013\n11684 (2020)\n23. Liu, Z., Tang, H., Amini, A., Yang, X., Mao, H., Rus, D.L., Han, S.: Bevfusion:\nMulti-task multi-sensor fusion with unified bird\u2019s-eye view representation. In: 2023\nIEEE International Conference on Robotics and Automation (ICRA). pp. 2774\u2013\n2781. IEEE (2023)\n24. Loshchilov, I., Hutter, F.: Sgdr: Stochastic gradient descent with warm restarts.\narXiv preprint arXiv:1608.03983 (2016)\n25. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101 (2017)\n26. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver++: Fast solver for\nguided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095\n(2022)\n27. Lu, T., Ding, X., Liu, H., Wu, G., Wang, L.: Link: Linear kernel for lidar-based\n3d perception. In: Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition. pp. 1105\u20131115 (2023)\n28. Paigwar, A., Sierra-Gonzalez, D., Erkent, \u00d6., Laugier, C.: Frustum-pointpillars: A\nmulti-stage approach for 3d object detection using rgb camera and lidar. In: ICCV.\npp. 2926\u20132933 (2021)\n29. Pang, S., Morris, D., Radha, H.: Clocs: Camera-lidar object candidates fusion for\n3d object detection. In: IROS. pp. 10386\u201310393. IEEE (2020)\n30. Pang, S., Morris, D., Radha, H.: Fast-clocs: Fast camera-lidar object candidates\nfusion for 3d object detection. In: WACV. pp. 187\u2013196 (2022)\n31. Philion, J., Fidler, S.: Lift, splat, shoot: Encoding images from arbitrary camera\nrigs by implicitly unprojecting to 3d. In: Computer Vision\u2013ECCV 2020: 16th Eu-\nropean Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XIV 16.\npp. 194\u2013210. Springer (2020)\n32. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using\n2d diffusion. arXiv preprint arXiv:2209.14988 (2022)\n33. Qi, C.R., Liu, W., Wu, C., Su, H., Guibas, L.J.: Frustum pointnets for 3d object\ndetection from rgb-d data. In: CVPR. pp. 918\u2013927 (2018)\n34. Qi, C.R., Su, H., Mo, K., Guibas, L.J.: Pointnet: Deep learning on point sets for\n3d classification and segmentation. In: CVPR. pp. 652\u2013660 (2017)\nDifFUSER\n17\n35. Simon, M., Amende, K., Kraus, A., Honer, J., Samann, T., Kaulbersch, H., Milz,\nS., Michael Gross, H.: Complexer-yolo: Real-time 3d object detection and tracking\non semantic point clouds. In: CVPR Workshops. pp. 0\u20130 (2019)\n36. Smith, L.N., Topin, N.: Super-convergence: Very fast training of neural networks\nusing large learning rates. In: Artificial intelligence and machine learning for multi-\ndomain operations applications. vol. 11006, pp. 369\u2013386. SPIE (2019)\n37. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 (2020)\n38. Sun, P., Wang, W., Chai, Y., Elsayed, G., Bewley, A., Zhang, X., Sminchisescu,\nC., Anguelov, D.: Rsn: Range sparse net for efficient, accurate lidar 3d object\ndetection. In: CVPR. pp. 5725\u20135734 (2021)\n39. Tan, M., Pang, R., Le, Q.V.: Efficientdet: Scalable and efficient object detection.\nIn: CVPR. pp. 10781\u201310790 (2020)\n40. Vora, S., Lang, A.H., Helou, B., Beijbom, O.: Pointpainting: Sequential fusion for\n3d object detection. In: CVPR. pp. 4604\u20134612 (2020)\n41. Wang, C., Ma, C., Zhu, M., Yang, X.: Pointaugmenting: Cross-modal augmentation\nfor 3d object detection. In: CVPR. pp. 11794\u201311803 (2021)\n42. Wang, T., Xinge, Z., Pang, J., Lin, D.: Probabilistic and geometric depth: Detecting\nobjects in perspective. In: Conference on Robot Learning. pp. 1475\u20131485. PMLR\n(2022)\n43. Wang, T., Zhu, X., Pang, J., Lin, D.: Fcos3d: Fully convolutional one-stage monoc-\nular 3d object detection. In: ICCV. pp. 913\u2013922 (2021)\n44. Wang, Y., Guizilini, V.C., Zhang, T., Wang, Y., Zhao, H., Solomon, J.: Detr3d: 3d\nobject detection from multi-view images via 3d-to-2d queries. In: Conference on\nRobot Learning. pp. 180\u2013191. PMLR (2022)\n45. Xie, E., Yu, Z., Zhou, D., Philion, J., Anandkumar, A., Fidler, S., Luo, P., Alvarez,\nJ.M.: M2bev: Multi-camera joint 3d detection and segmentation with unified birds-\neye view representation. arXiv preprint arXiv:2204.05088 (2022)\n46. Xu, S., Zhou, D., Fang, J., Yin, J., Bin, Z., Zhang, L.: Fusionpainting: Multimodal\nfusion with adaptive attention for 3d object detection. In: ITSC. pp. 3047\u20133054.\nIEEE (2021)\n47. Yan, J., Liu, Y., Sun, J., Jia, F., Li, S., Wang, T., Zhang, X.: Cross modal trans-\nformer: Towards fast and robust 3d object detection. In: ICCV. pp. 18268\u201318278\n(2023)\n48. Yan, Y., Mao, Y., Li, B.: Second: Sparsely embedded convolutional detection. Sen-\nsors 18(10), 3337 (2018)\n49. Yang, Z., Chen, J., Miao, Z., Li, W., Zhu, X., Zhang, L.: Deepinteraction: 3d object\ndetection via modality interaction. NeurIPS 35, 1992\u20132005 (2022)\n50. Yin, T., Zhou, X., Krahenbuhl, P.: Center-based 3d object detection and tracking.\nIn: CVPR. pp. 11784\u201311793 (2021)\n51. Yin, T., Zhou, X., Kr\u00e4henb\u00fchl, P.: Multimodal virtual point 3d detection. NeurIPS\n34, 16494\u201316507 (2021)\n52. Zhang, Q., Chen, Y.: Fast sampling of diffusion models with exponential integrator.\narXiv preprint arXiv:2204.13902 (2022)\n53. Zhou, B., Kr\u00e4henb\u00fchl, P.: Cross-view transformers for real-time map-view semantic\nsegmentation. In: CVPR. pp. 13760\u201313769 (2022)\n54. Zhou, X., Hou, J., Yao, T., Liang, D., Liu, Z., Zou, Z., Ye, X., Cheng, J.,\nBai, X.: Diffusion-based 3d object detection with random boxes. arXiv preprint\narXiv:2309.02049 (2023)\n55. Zhou, Y., Tuzel, O.: Voxelnet: End-to-end learning for point cloud based 3d object\ndetection. In: CVPR. pp. 4490\u20134499 (2018)\n18\nDT. Le et al.\n56. Zou, J., Zhu, Z., Ye, Y., Wang, X.: Diffbev: Conditional diffusion model for bird\u2019s\neye view perception. arXiv preprint arXiv:2303.08333 (2023)\nDifFUSER\n1\nDiffusion Model for Robust Multi-Sensor Fusion in\n3D Object Detection and BEV Segmentation\nSupplementary Material\n8\nAdditional implementation details\n8.1\nFormulation of Diffusion Models\nHere, we review the formulation of diffusion models used in DiF-\nFUSER, which based on the frameworks established in [9, 10, 37].\nBeginning with an original latent features xF\n0 \u223cq(xF\n0 ), we apply a\nsequential noising mechanism q, which incrementally infuses Gaus-\nsian noise into latent features xF\n1 , xF\n2 , ..., xF\nT at each time t. Specif-\nically, the noise infusion at each stage is determined by a variance\nschedule \u03b2t \u2208(0, 1):\n  q(\n\\bm {x\n} ^ F\n_{\n1:T\n}|\\b\nm  {x\n}^F_\n0) \n&= \\\np rod\n ^T_ { t=1}\nq (\n\\\nb m  {x}\n^F_t  | \\bm {x}^F_{t-1}) \\\\ q(\\bm {x}^F_t | \\bm {x}^F_{t-1}) &= \\mathcal {N}(\\bm {x}^F_t; \\sqrt {1-\\beta _t}\\bm {x}^F_{t-1}, \\beta _t \\bm {I})\n(10)\nFollowing DDPM [9], instead of iteratively apply q, it is feasible\nto sample xF\nt at any chosen t directly:\n  q(\n\\ bm \n{ x } ^F_t\n| \\ bm {x}\n^ F _0 )  &= \\m\nathc\na l {N}(\n\\ b m\n \n{ x }^F_ t ;  \\sq rt {\\bar {\\alpha }_t}\\bm {x}^F_0, (1 - \\bar {\\alpha }_t)\\bm {I}) \\\\ &= \\sqrt {\\bar {\\alpha }_t} \\bm {x}^F_0 + \\epsilon \\sqrt {1 - \\bar {\\alpha }_t}, \\epsilon \\sim \\mathcal {N}(0, \\bm {I})\\label {eq:direct_sampling}\n(12)\nwhere \u00af\u03b1t = Qt\ns=0 \u03b1s and \u03b1t = 1 \u2212\u03b2t. In this context, \u00af\u03b1t is employed\nfor scheduling the noise. According to Bayes\u2019 theorem, the posterior\nq(xF\nt\u22121|xF\nt , xF\n0 ) is also Gaussian:\n  q(\n\\bm {x\n} ^ F_\n{ t - 1}|\\\nbm { x}\n^F_t\n,  \\b\nm  { x}\n^F_0) &= \\mathcal {N}(\\bm {x}^F_{t-1}; \\mu _t'(\\bm {x}^F_t, \\bm {x}^F_0), \\beta _t' \\mathbf {I})\\label {eq:reverse_posterior}\n(13)\nwhere the mean and variance, given by\n  \n\\mu \n_ t '(\n\\ b m\n {x}^F_t\n,  \\bm\n {\nx }\n^F_0)  &= \\fr\na c  {\\\nsq\nrt {\\bar {\\alpha }_{t-1}}\\beta _t}{1-\\bar {\\alpha }_t}\\bm {x}^F_0 + \\frac {\\sqrt {\\alpha _t}(1-\\bar {\\alpha }_{t-1})}{1-\\bar {\\alpha }_t} \\bm {x}^F_t \\label {eq:mean_tilde}\n(14)\nand\n  \n\\ b e t a _t'\n & = \\\nfrac {1-\\bar {\\alpha }_{t-1}}{1-\\bar {\\alpha }_t} \\beta _t \\label {eq:variance_tilde}\n(15)\n2\nDT. Le et al.\nrespectively, characterize this Gaussian distribution.\nTo generate a sample from q(xF\n0 ), we may sample from q(xF\nT )\nand iteratively apply the reverse transitions q(xF\nt\u22121|xF\nt ) back to xF\n0 .\nGiven a sufficiently extensive T and a suitably chosen \u03b2t series (\u03b2t \u2192\n0), the distribution q(xF\nT ) converges closely to an isotropic Gaus-\nsian. Specifically, our conditional diffusion model is parametrized\nthrough p\u03b8(xF\nt\u22121|xF\nt , t, \u02dcxF\n0 ). Because p\u03b8(xF\nt\u22121|xF\nt , t, \u02dcxF\n0 ) is not be easily\nestimated but could be approximated using a neural network ( i.e.\nDifFUSER), where the aim is to predict \u00b5\u03b8 and \u03a3\u03b8:\n  p_{\n\\theta\n } (\\ bm \n{ x }^ F_{t\n-1}| x^F_t\n,  t, \\ti\nl de  {x}^\nF _ 0)  &\\\nc oloneqq \\mathcal {N}(\\bm {x}^F_{t-1};\\mu _{\\theta }(x^F_t, t,\\tilde {x}^F_0), \\Sigma _{\\theta }(x^F_t, t,\\tilde {x}^F_0)) \\label {eq:ptheta}\n(16)\nHere, a neural network can also be optimized to predict either \u03f5\nor xF\n0 [10,37]. We opt for predicting xF\n0 in this paper.\nDifFUSER\n3\n8.2\nPseudo-code of DifFUSER training and sampling\nAlgorithm 1 DifFUSER Training\nfunction train_loss(images, pointcloud, gt_boxes, gt_map)\n\"\"\"\nInput:\nimages: [B, V, H, W, 3] # V: number of multi-view images\npointcloud: [B, N, 3] # N: number of points in pointcloud\ngt_boxes: [B, *, D] # D: number of box dimensions\ngt_map: [B, C, 200, 200] # C: number of segmentation classes\nOutput: total loss which is a measure of the prediction quality\n\"\"\"\n# Encode the image and LiDAR pointcloud features\ncam_feats \u2190image_encoder(images)\nlidar_feats \u2190lidar_encoder(pointcloud)\n# Concatenate extracted features forming initial latent\n# representation and set as diffusion target\nxF\n0 \u2190concatenate(cam_feats, lidar_feats)\ndiffusion_target \u2190xF\n0\n# Perturb the initial representation using PDST\n\u02dcxF\n0 \u2190PDST(xF\n0 )\n# Randomly select a time step in the diffusion process\nt \u2190randint(0, T)\nnoise \u2190normal(mean=0, std=1) # Generate noise\n# Apply the noise to the latent representation at time step t\nxF\nt\n\u2190sqrt(alpha_cumprod(t) * xF\n0 + sqrt(1-alpha_cumprod(t)) * noise\n# Predict the denoised features at time t using the DifFUSER model\n\u02c6xF\n0 \u2190DifFUSER(\u02dcxF\n0 , t)\n# Pass the enhanced features through the task-specific head\npred \u2190task_head(\u02c6xF\n0 )\n# Compute the total loss\nloss \u2190diffusion_loss(diffusion_target, \u02c6xF\n0 )\n+ task_loss(pred, gt_boxes, gt_map)\nreturn loss\nend function\n4\nDT. Le et al.\nAlgorithm 2 DifFUSER Sampling\nfunction train_loss(images, pointcloud, steps, T)\n\"\"\"\nInput:\nimages: [B, V, H, W, 3] # V: number of multi-view images\npointcloud: [B, N, 3] # N: number of points in pointcloud\nsteps: number of sampling steps\nT: number of time steps\nOutput: task-specific predictions\n\"\"\"\n# Encode the image and LiDAR pointcloud features\ncam_feats \u2190image_encoder(images)\nlidar_feats \u2190lidar_encoder(pointcloud)\n# Concatenate extracted features forming initial latent\n# representation and use as diffusion condition\nxF\n0 \u2190concatenate(cam_feats, lidar_feats)\n# Sampling step sizes [(T-1, T-2), (T-2, T-3), ..., (1, 0)]\ntimes \u2190reverse(linspace(-1, T - 1, steps + 1))\ntime_pairs \u2190list(zip(times[:-1], times[1:]))\nxF\nt\n\u2190normal(mean=0, std=1) # Sample xF\nt\nfrom noise\nfor (t_now, t_next) in time_pairs:\n\u02c6xF\n0 \u2190DifFUSER(xF\n0 , xF\nt , t_now)\n# estimate xF\nt\nat t_next, sampler can be [37], [52], or [26]\nxF\nt \u2190sampler_step(\u02c6xF\n0 , t_now, t_next)\n# Pass the predicted enhanced features through the task-specific\nhead\npred \u2190task_head(\u02c6xF\n0 )\nreturn pred\nend function\nDifFUSER\n5\n9\nAdditional Qualitative Results\n6\nDT. Le et al.\nMotorcycle\nPedestrian\nBarrier\nWalkway\nLane Divider\nDrivable area\nStopline\nCarpark Area\nPed Crossing\nCar\nTraffic Cone\nConstruction Veh\nBicycle\nBEV Map Segmentation:\n3D Object Detection:\nFig. 7: We provide additional qualitative results for challenging scenarios from the\nNuScenes val set. On the left, we show the projections of predicted 3D bounding boxes\non the camera images. On the right, we show the 3D prediction (in BEV) and BEV\nmap segmentation results. Best viewed in color and zoomed in.\n",
    "2312.11578": "Diffusion-Based Particle-DETR for BEV Perception\nAsen Nachkov1\nMartin Danelljan2\nDanda Pani Paudel1,2\nLuc Van Gool1,2\n1INSAIT, Sofia University\n2ETH Zurich\nAbstract\nThe Bird-Eye-View (BEV) is one of the most widely-used\nscene representations for visual perception in Autonomous\nVehicles (AVs) due to its well suited compatibility to down-\nstream tasks.\nFor the enhanced safety of AVs, modeling\nperception uncertainty in BEV is crucial. Recent diffusion-\nbased methods offer a promising approach to uncertainty\nmodeling for visual perception but fail to effectively de-\ntect small objects in the large coverage of the BEV. Such\ndegradation of performance can be attributed primarily to\nthe specific network architectures and the matching strategy\nused when training. Here, we address this problem by com-\nbining the diffusion paradigm with current state-of-the-art\n3D object detectors in BEV. We analyze the unique chal-\nlenges of this approach, which do not exist with determinis-\ntic detectors, and present a simple technique based on ob-\nject query interpolation that allows the model to learn po-\nsitional dependencies even in the presence of the diffusion\nnoise. Based on this, we present a diffusion-based DETR\nmodel for object detection that bears similarities to parti-\ncle methods. Abundant experimentation on the NuScenes\ndataset shows equal or better performance for our gener-\native approach, compared to deterministic state-of-the-art\nmethods. Our source code will be made publicly available.\n1. Introduction\n3D Object detection - the task of localizing and classify-\ning objects in a real-world 3D coordinate frame - is one of\nthe most important tasks in the pipeline of an autonomous\nvehicle. It is critical to safe self-driving since it informs\nthe subsequent prediction, planning, and actuation modules\nand, evidently, one needs to recognize an obstacle in order\nto avoid it. Estimating the object locations directly from\nthe camera views [2, 3, 42] faces difficulties related to per-\nspective warping and size-distance ambiguities. Instead, the\nbird-eye-view (BEV) has established itself as a useful repre-\nsentation to facilitate perception because it is ego-centered,\nmetrically-accurate, orthographic - thus avoiding perspec-\n0.34\n0.35\n0.36\n0.37\n0.38\n0.39\n0.40\n0.41\n0.42\nMean Average Precision (mAP)\n0.42\n0.44\n0.46\n0.48\n0.50\n0.52\nNuScenes Detection Score (NDS)\nPGD\nFCOS3D\nDETR3D\nBEVFormer\nParticle-DETR (ours)\nDiffusionDet\nFigure 1. Performance improvement. Our method outperforms\nthe baseline stochastic model DiffusionDet [9] and is compara-\nble in performance to deterministic models like BEVFormer [34].\nHere, the results of stochastic models are shown as circles and\nthose of deterministic models as triangles. The size of the circles\nis proportional to the number of search tokens used.\ntive distortion of shapes, and suffers less from occlusions\nand object deformations.\nRecently, it has been shown that diffusion models can be\nsuccessfully used for 2D object detection [9] - a completely\ndifferent setup than the generative tasks like text-to-image\nwhere they have been dominating [11, 25, 52, 53, 55]. In\nprinciple, one should then be able to apply diffusion-based\nobject detection also in the 2D BEV and predict 3D loca-\ntions, reaping all the diffusion benefits like incremental re-\nfinement and the ability to trade-off compute and accuracy?\nWe find that naively applying diffusion in BEV yields an\nalgorithm with insufficient performance. We attribute this\nto the challenging problem setting and the fact that the net-\nwork architecture is not tuned for the particular geometric\naspects of the BEV:\n1. Setting: Recent works [33, 34, 47, 51] represent the\nBEV as a set of spatially-correlated latent features cor-\nresponding to a (50 \u00d7 50) or even (100 \u00d7 100) meter\ngrid around the ego-vehicle. The detectable objects such\nas cars and pedestrians are naturally very small in rela-\ntion to the size of the whole BEV map, making detection\n1\narXiv:2312.11578v1  [cs.CV]  18 Dec 2023\nmore challenging compared to on common datasets used\nto benchmark 2D detection algorithms [16, 17, 35].\n2. Architecture: DiffusionDet [9], the representative dif-\nfusion algorithm, uses an ROI-based architecture [21]\nwhich aggregates BEV features only within the proposed\nboxes. This makes object features rather local, prevent-\ning extensive search on the BEV. Local features work\nwell in settings where the target boxes are larger and\nmore dense, but we believe in the BEV one needs a more\nspecialized architecture to better handle object sparsity.\nProblem statement and approach.\nSince object detec-\ntion is ultimately a search problem and smaller objects are\nharder to locate, some of the inherent challenges when using\ndiffusion to detect objects can be exacerbated in the BEV.\nThus, the research question we try to answer is: How should\nthe diffusion approach and network architecture be adjusted\nso as to ease the search process in the BEV? To that end, our\ninsights are that first, to search more effectively, one should\npool information across the search tokens used (boxes, an-\nchors, queries), and second, one should take measures to\nprevent the diffusion noise from overwhelming any posi-\ntional dependencies that exist in the data.\nTo pool information across the search tokens we need\nto have them communicate with each other. This can be\nachieved using self-attention which in turn points to a trans-\nformer method like DETR [5, 14, 32, 39, 67, 71]. These\nmodels utilize a number of fixed object queries which they\nlearn to regress into the predicted boxes. They further make\nincorporate cross-attention modules to look up relevant fea-\ntures from the image in a way that is independent across\nindividual queries. The combined architecture can utilize\nglobal features, which becomes increasingly more useful as\nthe objects\u2019 sizes decrease.\nRegarding positional dependencies, we show how the\ndiffusion noise affects the matchings between predicted and\ntarget boxes. In essence, most approaches like Deformable-\nDETR [71] start with fixed (x, y) reference points, look up\nthe image features in those locations, and output corrections\nwhich are subsequently applied to them. But when diffu-\nsion is applied on the initial reference points, they become\nno longer associated with the object queries, preventing the\nmodel from using positional information. To address this\nchallenge we introduce object query interpolation as a sim-\nple way to learn positional relations for DETR-like models\neven in the presence of noise over the references.\nThe resulting generative model can refine its predictions,\ntrade-off accuracy and compute, operate with a different\nnumber of search tokens at train and test time, and yields\nresults comparable or better than those of battle-tested de-\nterministic models. Furthermore, it has similarities to par-\nticle methods from which ideas like particle pruning and\nrefinement can be borrowed.\nDenoise\nDiffuse\nFigure 2. Diffusion in BEV. Our approach diffuses the ground-\ntruth object centers in BEV and learns to denoise them. At test\ntime, we start from random references corresponding to the box\ncenters and progressively refine them to their true locations.\nContributions.\nOur contributions are the following:\n1. In Section 3.3 we provide a novel view on the assign-\nment instability problem that is present in most DETR-\nlike models [5, 71] by showcasing how the stochasticity\nof the diffusion process affects the assignments.\n2. In Section 3.4 we showcase our module called query in-\nterpolation which allows the model to learn positional\ninformation even in the presence of diffusion noise.\n3. We integrate the proposed module into a deformable-\nDETR [71] variant, called Particle-DETR, which uses\ndiffusion to denoise box centers to their true positions.\nWe further provide a detailed analysis of the perfor-\nmance of the model on the realistic and large-scale\nNuScenes dataset [4].\n2. Related Work\nDiffusion-based object detection.\nUtilizing a diffusion\nmodel for exact tasks started with DiffusionDet [8], where\nthe model learns to denoise axis-aligned 2D boxes in the\nimage coordinate system. First, a backbone network, e.g.,\nResNet [23] or Swin transformer [40], extracts multi-scale\nimage features. At train time, random noise is added to the\nground-truth (GT) boxes according to a diffusion schedule,\nwhile at test time random boxes are sampled from a Gaus-\nsian distribution. Subsequently, a decoder with a region-\nof-interest-based (ROI) architecture [21, 24] aggregates the\nfeatures inside each box and produces corrections to the box\nparameters. The output boxes are then matched to the GT\nboxes for training.\nOther applications.\nInspired by DiffusionDet, the\nusage of diffusion models for other prediction tasks has\nincreased.\nIt has been applied to the denoising of BEV\nfeatures [72], to prediction of future discrete BEV tokens\n[68], to action segmentation in videos [38], to weakly su-\npervised object localisation (WSOL) [70], to human motion\nprediction [1] and pose estimation [18], to domain adaptive\nsemantic segmentation [45], to video anomaly detection\n[56], to camouflaged object detection [9], to text-video\nretrieval [30], and to open-world object detection [62].\n2\nMany-to-one\nset prediction loss\nBEV\nencoder\nBEV features\nQuery interpolation\nSA\nDetector\nas \nObject queries\nas \nRandom reference points \nGT boxes\nPredicted boxes\nGround truth boxes and centers\nto object\ncenters only\nPredicted boxes in BEV\nFixed object queries\nCA\nFigure 3. Diffusion in BEV. A feature extractor processes the images from the camera feed at the current timestamp. An encoder learns\nto project these features from the perspective of the car to the top-down orthographic bird-eye-view. At training time we add noise to\nthe ground-truth object centers according to a diffusion schedule, while at test time we directly sample the references. Using our query\ninterpolation module each reference position is associated with a spatial feature. Using these features, the decoder learns to denoise the\nreference points to their true locations. A set prediction loss is adopted for training.\nThe DETR family of models. Current object detection\nin BEV is dominated by DETR-variants [5, 7, 14, 32, 39, 66,\n67]. They utilize a transformer sequence where a fixed num-\nber of object queries look up the relevant image features us-\ning cross-attention and are transformed to a fixed number of\noutput boxes. Since the outputs and the GTs are unordered,\na set-matching step is needed in order to assign predictions\nto targets. This matching has been described as unstable,\nbecause of how one prediction can be matched to different\ntargets across multiple training iterations on the same im-\nage. Various approaches try to mitigate this issue by intro-\nducing query denoising [32], where some object queries are\nmatched to their target by index, and contrastive denoising\n[67] where both positive and negative examples are used in\neach query group.\nBEV perception. Transforming the camera features to\nBEV is an active area of research. It has been done us-\ning both traditional approaches where 3D voxels are pro-\njected onto the image plane and the image features within\nthe projection are average-pooled [51], or where a categor-\nical depth distribution is estimated for each image pixel, af-\nter which the features are \u201dlifted\u201d in 3D according to their\ndepths [47, 49]. Implicit projection, where depth is not esti-\nmated explicitly, can be achieved by utilizing self-attention\nto look up the past BEV and cross-attention to look up the\ncurrent image features [28, 29, 34, 48, 65, 69]. This is the\napproach we rely on in this work.\nOnce in BEV, mod-\nels may perform joint detection and trajectory prediction\n[15, 26, 27, 41, 63, 69], BEV segmentation [46], tracking\n[22], or agent interaction analysis [6, 12].\n3. Approach\nIn this section we motivate our method by considering the\nunique challenge arising when combining diffusion with\nperception in BEV, cf. Subsection 3.2 and how our method\nalleviates this challenge, cf. Subsection 3.4.\n3.1. Preliminaries\nDiffusion models.\nDiffusion models are a class of gen-\nerative models whose goal is to learn to sample from the\ndistribution over a sample space. To that end, as part of\nthe training procedure, a stochastic process adds noise to\neach input sample according to a predefined schedule. At\ntraining time, the model learns to predict the added noise,\nwhile at test time one generates random initial noise which\nthe model progressively denoises until a data point from the\ntraining distribution is formed.\nThe forward process, which adds noise to the sample at\ntraining time, is defined as\nq(ztd|z0) = N(ztd|\np\n\u00af\u03b1tdz0, (1 \u2212\u00af\u03b1td)I),\nwhere td is the time-index of the diffusion process (differ-\nent from the temporal frame index tf in the context of the\nBEV sequences), ztd is the noisy sample at that time, z0 is\nthe noise-less ground-truth sample, and \u00af\u03b1td = Qtd\ns=0 \u03b1s =\nQtd\ns=0(1 \u2212\u03b2s) is the corresponding parameter from the\nschedule controlling the variance of the noise.\nThe network output f\u03b8(ztd, td) is conditioned on the\nnoisy sample ztd, the diffusion time td and its parameters\n\u03b8 are optimized to minimize the loss\n3\nL = 1\n2\u2225f\u03b8(ztd, td) \u2212z0\u22252.\nSince this corresponds to a denoising process, at test time\nwe sample random noise zT and progressively refine it by\nfeeding the previous output as the subsequent input to the\nnetwork, i.e. z0 = f\u03b8(f\u03b8(...(f\u03b8(f\u03b8(zT , T), T \u22121))...), 0).\nVarious improvements exist to speed-up this process [10,\n43, 44, 55].\nSince the noise added to each data sample is independent\nacross all sample elements, we can use this process to gen-\nerate different objects like images [52], bounding boxes [8],\ncamera poses [58]. Here, the diffusion is applied over box\ncenters (cx, cy) in BEV, to which we refer as particles.\nDETR models. DETR models for object detection [5]\nrely on a transformer-based architecture. A feature extrac-\ntor, usually convolutional, extracts image features which are\nthen passed to a transformer encoder where each feature\npatch can attend to other feature patches. Subsequently, a\ntransformer decoder, relying on a fixed number N of la-\ntent vectors {q1, ..., qN} called object queries, looks up the\nfeatures from the encoder and outputs bounding boxes. A\none-to-one matching step using the Hungarian algorithm is\nrequired to assign predictions to box targets.\nThe object queries are learned using gradient descent and\nare fixed at test time. Since positional encodings for the ob-\nject queries are also used, the model can learn information\nrelated to the order of the object queries.\nAn important modification to the this setup is given by\nDeformableDETR [71] where {q1, ..., qN} are not only or-\ndered between them, but each object query qi is tied to a\nparticular 2D position ri, called a reference point within the\nimage coordinate frame. Since both the object queries and\nreference points are learned, the model can focus not only\non the content of the pixels, but also on the query positions.\nFixing the reference points {r1, ..., rN} makes training\neasier because query qi will always have the same relative\nlocation compared to query qj, j \u0338= i. In that case, the\ncross-attentions in the decoder learn only how to attend to\nthe surrounding features which makes learning more stable.\nThis fixed nature is crucial in relation to the stochasticity we\nwill introduce by the diffusion process.\n3.2. Adding diffusion to BEV\nOur setup is shown in Figure 3. A feature extractor [23]\nalong with a feature neck [36] processes all camera images\nfrom the current timestep tf, outputting multi-scale feature\nmaps for each camera view. A BEV encoder, in practice\nBEVFormer [34], projects these features around the ego-\nvehicle. In BEV, we add noise to the ground-truth object\ncenters and concatenate with additional random locations.\nThese are passed as reference points to the decoder which,\nBEV Features\nTarget 1\nNoisy reference\npoint 2\nPrediction 1\nScenario 1\nTarget 2\nBEV Features\nTarget 1\nNoisy\nreference\npoint 2\nScenario 2\nNoisy\nreference\npoint 1\nPrediction 1\nTarget 2\nPrediction 2\nNoisy reference\npoint 1\nFigure 4. Label ambiguity. A case where the random reference\npoints may produce different targets, depending on the matching.\nThe top row shows ambiguity when we match predictions to GTs\nby total distance (linear sum assignment). The columns show how\ndifferent samplings of the blue points may push the same feature\nlocation in BEV to different targets, thus confusing the model. In\nall cases the matching is done between the predictions and the tar-\ngets. The bottom row shows how the training loss depends on the\nnumber of references for a simple toy task (cf. suppl. materials).\nsimilar to DeformableDETR [71], refines some of them into\nthe GT positions.\nAt test time, we sample initial random box centers and\nrun them through the decoder. Since the model is trained\nto work with variable reference points, it can plug in the\npredicted box centers as input reference points in the next\ndenoising step.\nThis allows iterative refinement of our\npredictions - something that deterministic models like De-\nformableDETR [71] cannot do because they rely on object\nqueries which are fixed to particular positions.\nWe follow DETR [5] in applying auxiliary losses to each\ndecoder layer, instead of just the final one. We refer to each\ndecoder layer as a stage and to each pass through all decoder\nlayers as a single DDIM [55] step. By having multiple such\nsteps we can trade-off accuracy and compute. Each DDIM\nstep requires evaluating the only the decoder.\n3.3. Matching\nThe matching cost used in object detectors from the DETR\n[5] family typically considers both the predicted box dimen-\nsions and the predicted class logits. As a result, one cannot\nsay that predictions spatially closer to the GT box will al-\nways be matched and those farther away will not. Yet, deter-\nministic detectors do converge because even if the matching\nchanges across iterations, the static nature of the inputs al-\n4\nlows one to learn the spatial relationships in the image.\nLabel ambiguity.\nIn the diffusion case there exist specific\nsituations where learning is, in fact, impossible due to the\nsame BEV feature having different targets, depending on\nthe noisy sampling of the reference points. Figure 4 illus-\ntrates these conceptually.\nSuppose we use the Hungarian algorithm for matching\nand we sample the initial reference points as the blue points\non the top-left plot in Figure 4. Then the matching will be\nas shown by the arrows. However, if one of the points is\nsampled differently, as in the top-right plot, we may end\nup matching them differently. In reality, the BEV features\ncorresponding to the (x, y) position where noisy reference\npoint 1 is, will be pushed by the optimization in the first\ncase toward target 1 and in the second case toward target 2.\nThis creates label ambiguity arising specifically due to the\nrandom sampling of the starting locations.\nUsing more object queries than GT boxes reduces the\npossibility of this ambiguity to hinder the training. This is\nbecause having more predictions and matching them with\nany strategy that takes the distance into account (unlike e.g.\nmatching by index) will make the model produce smaller\nrefinements to the starting reference points. Thus, a high\namount of object queries is needed both to detect many ob-\njects, but also to help detect them accurately. Explanations\non a toy example can be found in Section A from the sup-\nplementary materials.\n3.4. Object query interpolation\nOur diffusion is applied on the reference points ri. As a\nfirst approach, we consider a DeformableDETR [71] archi-\ntecture with N learnable object query vectors which are as-\nsigned to their references by index. Thus, object query qi\nmay be placed in different (x, y) locations depending on the\nsampling. While this approach works fairly well in prac-\ntice, it clearly prohibits the model from learning positional\ninformation for query qi simply because its position keeps\nchanging during each training iteration.\nInstead, we propose to interpolate the learned queries at\nthe random sampled reference locations, as shown in Fig-\nure 5. We learn a grid of regularly-placed object queries,\nwhich we bilinearly interpolate at the reference points. This\nensures that sampling the same location r = (x, y) will\nalways yield the same object query q(x,y). Additionally,\nthis decouples the number of object queries at training time\nand at test time, since at training time one needs to learn\nN queries, but at test time they can be interpolated at Ntest\ndifferent locations.\nIn principle, one can also directly interpolate the BEV at\nthe sampled locations, avoiding the use of learned object\nqueries altogether.\nOur preliminary experiments showed\nthat learning becomes prohibitively difficult in this case,\nWidth \nHeight \nLatent\ndimension\nFixed object queries \nInterpolated object query \nReference point \n with\nlocation \nBEV features \nFigure 5. Query interpolation. Our method learns a number of\nregularly-placed object queries, shown in green, which are fixed at\ntest time. This allows the content of each object query to depend\non its position in BEV. To accommodate the stochasticity required\nby the diffusion process we interpolate the object queries at the\nnoisy locations, shown in red. This ensures that if we sample the\nsame reference point many times, we will always obtain the same\nobject query at that location.\nowing to the diversity and nature of the BEV features. It\nis much easier if the model looks up the BEV features using\ncross-attention instead of starting from the BEV features as\nobject queries.\n3.5. Additional method components\nLoss function.\nThe stochastic nature of the algorithm\nmakes training very slow and difficult if we match predic-\ntions and ground truths in a one-to-one manner. To alleviate\nthis, we employ many-to-one matching where many predic-\ntions are matched to each GT box. This speeds up training\ntremendously at the cost of having to post-process the pre-\ndictions using non-maximum suppression (NMS).\nOur loss function is given by\nL = \u03bbclsLcls + \u03bbregLreg,\nwhere Lcls is the focal loss between predicted and target\nclass probabilities [37] and Lreg is the \u21131 loss between the\npredicted and GT box parameters. We do not employ a gen-\neralized IoU loss [50]. The matching cost is the same as the\nloss function. For detection, the box parameters include the\nbox center and dimensions, orientation, and velocity in the\nbird-eye-view plane:\nb = (cx, cy, cz, w, h, l, sin \u03b8, cos \u03b8, vx, vy).\nParticle nature. The many-to-one matching is crucial\nfor our approach because it allows the model to learn gra-\ndient fields, or basins of attraction around each GT box.\nThis aspect, combined with the random reference points,\nallows us to look at this architecture as a particle DETR\nmodel where multiple particles, the references r1, ..., rN,\ncan move freely and are attracted around the GT boxes.\nThrough the self-attention layers, they can communicate\n5\nSetting\nmAP \u2191\nNDS \u2191\nmATE \u2193\nmASE \u2193\nmAOE \u2193\nmAVE \u2193\nmAAE \u2193\n1. DiffusionDet [8] - ROI arch., box tokens\n0.3846\n0.4580\n0.7420\n0.3382\n0.4475\n0.5935\n0.2222\n2. 1 + positional encodings\n0.3852\n0.4717\n0.7357\n0.2777\n0.5040\n0.5008\n0.1911\n3. DETR arch., random references\n0.3929\n0.4975\n0.7172\n0.2707\n0.3835\n0.4224\n0.1963\n4. 3 + simple n-to-1 matching & NMS\n0.4001\n0.5203\n0.6913\n0.2710\n0.3415\n0.3540\n0.1938\n5. 3 + simOTA matching & NMS\n0.3817\n0.5138\n0.6444\n0.2641\n0.3208\n0.3422\n0.1989\n6. 4 + radial suppression\n0.4082\n0.5203\n0.6456\n0.2704\n0.3528\n0.3739\n0.1956\n7. 4 + training with added fixed queries\n0.4077\n0.5215\n0.6453\n0.2696\n0.3405\n0.3747\n0.1935\n8. 7 + radial suppression\n0.4088\n0.5222\n0.6437\n0.2696\n0.3395\n0.3768\n0.1922\nFCOS3D [59]\n0.3430\n0.4150\n0.7250\n0.2630\n0.4220\n1.292\n0.153\nPGD [60]\n0.3690\n0.4280\n0.6830\n0.2600\n0.4390\n1.2680\n0.1850\nDETR3D [61]\n0.3460\n0.4250\n0.7730\n0.2680\n0.3830\n0.8420\n0.2160\nBEVFormer [34], permuted queries\n0.3976\n0.5073\n0.6809\n0.2744\n0.3722\n0.3908\n0.1962\nBEVFormer, random reference points\n0.2997\n0.4474\n0.735\n0.2765\n0.3974\n0.4179\n0.1975\nBEVFormer, deterministic\n0.4154\n0.5168\n0.6715\n0.2738\n0.3691\n0.3967\n0.1981\nBEVFormer-Enh (ours)\n0.4189\n0.5298\n0.6319\n0.2684\n0.3283\n0.3737\n0.1945\nTable 1. Model progression and results on the NuScenes val set. We showcase how the model components and different architectures\naffect performance. Models numbered 1-8 are all evaluated with 3 DDIM steps and 900 queries.\nsimilar to how the best location is globally shared in a par-\nticle swarm optimization [31]. The DDIM denoising [55]\nsteps then provide opportunities to refine, renew, or prune\nthe particles, based on their confidence. Additionally, the\nnumber of particles which end up on top of a target object\ncan provide a rudimentary measure about the uncertainty of\nour perceptions at that BEV location. We cannot refer to the\nsearch tokens of DETR models [5, 71] as dynamic because\nthey are fixed and do not allow for sequential refinement.\n4. Experiments\nNuScenes dataset.\nWe evaluate our approach on the\nlarge-scale NuScenes dataset [4], comprising almost 1.4\nmillion annotated 3D bounding boxes, across 1000 scenes.\nThere are 23 semantic classes of which 10 are evaluated.\nThe frequency of the images is 2 Hz. The main metrics of\ninterest are the Mean Average Precision (mAP) and, more\nimportantly, the NuScenes Detection Score (NDS).\nFor the mAP detections are calculated by greedily as-\nsigning predictions to targets only based on the distance be-\ntween the predicted and GT centers. There are four distance\nthresholds - 0.5, 1, 2, and 4 meters. The mAP is calcu-\nlated as the average precision over 100 recall percentiles\nand is further averaged over all 10 detectable classes and\nover these 4 distance thresholds.\nAt evaluation time, once the assignment between pre-\ndicted boxes and targets is completed, one can calculate var-\nious true positive metrics - translation error (mATE), scale\nerror (mASE), box orientation (mAOE), velocity (mAVE),\nattribute error (mAAE) - over the matched pairs. These are\nweighted together with the mAP to form the NDS metric.\nIt has been claimed that the NDS metric is more realistic in\nterms of real-life driving performance than the mAP [54].\n4.1. Comparison with baselines\nIn our experiments we compare against the following state-\nof-the-art models:\n1. DiffusionDet [8], which we modify minimally and uti-\nlize directly in BEV as our main baseline,\n2. DeformableDETR [71], as used in BEVFormer [34],\na state-of-the-art deterministic detector which already\ngreatly outperforms the vanilla DETR model [5].\nBaseline. Table 1 shows our main results. We rely on\nBEVFormer\u2019s encoder to project the images into the top-\ndown view. Since the original DiffusionDet works only on\naxis-aligned boxes, we modify it by adopting rotated ROI\npooling similar to [64]. The architecture follows a six stage\nRCNN [21] decoder where each stage takes the BEV fea-\ntures and a number of rotated boxes in BEV, parameterized\nas (cx, cy, w, h, \u03b8). The BEV features falling into the ro-\ntated box are aggregated and deformable convolutions [13]\nare applied to model instance interactions between differ-\nent boxes. Each stage outputs corrections which are applied\nto the current boxes to produce the subsequent-stage boxes.\nOverall, applying DiffusionDet directly in BEV yields good\nperformance compared to reference models [59\u201361] but in-\nferior compared to the deterministic BEVFormer.\nPositional encodings. It is common to encounter cer-\ntain classes more often in some positions relative to the ego-\nvehicle, e.g. pedestrians appear in front of the car less often\nthan at the side of the car. The ROI-based architecture does\nnot consider the absolute locations of the boxes in BEV,\n6\nwhich motivates us to use sinusoidal positional encodings\n[57], which we concatenate to the aggregated BEV features\nfor each box token. This improves performance but is still\ninsufficient compared to BEVFormer.\nGlobal features to address sparsity. ROI-based archi-\ntectures emphasize the local features inside each box. Such\na prior may be sufficient on datasets like COCO [35], but\nfor smaller objects we argue that more global features are\nneeded. This motivates us to consider a DETR-based ar-\nchitecture where instead of boxes and ROI-pooling we have\nobject queries and attention. Now, each stage first applies\nself-attention over the object queries, thereby considering\ntheir relative position and content, and then applies cross-\nattention over the BEV. This cross-attention has potentially\nunlimited view and can aggregate more global BEV features\nfor each token than the ROI architecture.\nMany-to-one matching.\nAt this point, even though\nglobal features and positions are considered, we found that\nwith random reference points r1, ..., rN, the supervisory\nsignal when matching in a one-to-one fashion is simply too\nweak. Thus, we experiment with two many-to-one match-\ning strategies. The first we call simple N-to-1 because it\nsimply repeats the GT boxes a number of times, stacking\nthem on top of each other, and then applies the linear sum\nassignment solver for matching. For the second strategy\nwe use the SimOTA [20] approximation to optimal trans-\nport assignment [19], which matches a variable number of\npredictions to each target.\nDetection accuracy. The results show that our diffusion-\nbased Particle-DETR achieves good performance and no-\nticeably outperforms the baseline DiffusionDet [9] on both\nmAP and NDS. Even more, its performance is comparable\nto that of deterministic approaches like BEVFormer [34].\nOur generative approach achieves higher NDS, showing\nthat once a detection has been established, the predicted box\ndimensions, orientation, and velocity are, on average, more\naccurate. Yet, a small gap of about 1.6 mAP points remains.\nEnhancement with static references. The random ref-\nerences allow the model to learn basins of attraction around\neach GT center. However, nothing prevents us from utiliz-\ning fixed references as well, which yield higher precision.\nThus, we further experiment with a setup in which we train\nwith two sets of references - one random, coming from the\ndiffusion process, and one static. In turn, the two reference\nsets result in two sets of queries - one where the queries\nare interpolated at the random locations (cf.\nSubsection\n3.4), and one where the queries are learned and fixed, as in\n[34, 71]. Since the decoder is shared, the joint training cap-\ntures any synergies between the random and fixed queries,\nimproving the performance of both. At test time, to keep the\nnumber of queries comparable to previous models, we can\nuse either only the diffusion queries or the static ones. Us-\ning the diffusion queries we obtain our final Particle-DETR\nmodel. Using the static ones we obtain an enhanced BEV-\nFormer which we call BEVFormer-Enh.\n4.2. Implementation details\nThe implementation of our Particle-DETR is straightfor-\nward and follows that of BEVFormer [34]. We train the\nmodel for the same number of iterations as BEVFormer and\nthe number of parameters is similar. The training hyper-\nparameters are shown in Table 3 and pseudocodes for the\ntrain-test behaviour can be found in Algorithms 1 and 2.\nGradient detachment. To further facilitate training, we\nequip each decoder layer with look forward twice updates\n[67], where the reference points for each decoder layer are\nnot detached from the computation graph when computing\nthe next-layer reference points during the forward pass.\nFiltering of predictions. At training time, the many-to-\none matching helps to learn the basins of attraction around\neach GT center. However, at test time it results in many\nfalse positives, as can be seen from Section C in the supple-\nmentary materials. Thus, we employ NMS and also utilize\na small score threshold which filters any predictions with\nconfidence below it.\nRadial suppression. We found that very small objects\nlike traffic cones do not overlap and are missed by NMS.\nFor that reason, we introduced radial suppression to further\nfilter out the boxes. In essence, we first order the predic-\ntions by decreasing confidence. Then for the most confident\nones, we sequentially replace them with weighted averages\nof their close-by boxes which, in turn, are filtered:\nbi =\nP\nk bk\u03c0k\nP\nk \u03c0k\n, \u2200k :\nq\n(cx,i \u2212cx,k)2 + (cy,i \u2212cy,k)2 < r.\nHere cx,k is the x-coordinate of the center of the k-th\nbox, and \u03c0k is the confidence for that box. We implement\nradial suppression independently for each semantic class.\n4.3. Additional properties\nFlexibility. The architecture of our Particle-DETR allows\nus to train it with one number of queries but evaluate with a\n400\n600\n800\n1000\n1200\n1400\nNum. references\n0.495\n0.500\n0.505\n0.510\n0.515\n0.520\n0.525\nNuScenes Detection Score (NDS)\nDDIM steps = 1\nDDIM steps = 3\nDDIM steps = 5\nDDIM steps = 7\nDDIM steps = 9\nFigure 6. Effect of number of references on NDS. Holding the\nnumber of DDIM steps fixed, NDS increases as the number of\nrandom references increases.\n7\ndifferent number. Additionally, the number of DDIM steps\n[55] allows us to further trade-off accuracy and compute.\nFigure 6 shows that both increasing the number of DDIM\nsteps and the number of particles used improves perfor-\nmance. With 900 references it only takes a single DDIM\nstep to outperform BEVFormer on NDS.\nStochastic nature of results. Since we rely on randomly\nsampled initial reference points, the outputs of our method\nare stochastic. Table 2 shows statistics over 10 test runs. We\nnote that performance is very consistent across the runs.\n4.4. Qualitative study\nHere we perform a qualitative comparison between our pre-\ndictions and those of BEVFormer [34].\nIn general, the\nhigher NDS which results from the diffusion process makes\nour detections more precise in terms of location, size, and\norientation, which can be particularly beneficial for very\nsmall objects (e.g. traffic cones) near the car. On some\nscenes our method recognizes even partially-occluded ob-\njects earlier and more confidently, as shown in Figure 7.\nIt is common for models to struggle with accurate es-\ntimation of the dimensions of large objects like buses and\ntrucks. This is because they obscure the camera\u2019s field of\nview considerably, making it hard to estimate where the ob-\nNot\ndetected\nDetected\nat\nconfidence\n0.05\nDetected\nat\nconfidence\n0.10\nDetected\nat\nconfidence\n0.20\nFrame\nFrame\nFrame\nFrame\nFigure 7. Sample predictions in BEV. Green boxes are ground-\ntruths, red are predicted by our Particle-DETR, and blue is pre-\ndicted by BEVFormer, compared to which we detect earlier in the\nframes and more confidently, even for less-common objects.\nMetric\nBEVFormer\nOurs (stoc.)\nOurs\nNDS \u2191\n0.5168\n0.5271 (0.0002)\n0.5287\nmAP \u2191\n0.4154\n0.4163 (0.0003)\n0.4184\nmATE \u2193\n0.6715\n0.6415 (0.0008)\n0.6386\nmASE \u2193\n0.2738\n0.2689 (0.0002)\n0.2686\nmAOE \u2193\n0.3691\n0.3390 (0.0009)\n0.3362\nmAVE \u2193\n0.4179\n0.3688 (0.0010)\n0.3688\nmAAE \u2193\n0.1981\n0.1920 (0.0006)\n0.1931\nTable 2. Performance statistics on the NuScenes val set. We\ncompare our stochastic Particle-DETR (col. 3), evaluated with\n1500 queries and 1 DDIM step, and the deterministic BEVFormer-\nEnh (col. 4) to the original BEVFormer. The standard deviations\nfor the random methods are shown in parentheses.\nject ends. We notice that in some scenes our method im-\nproves noticeably in this regard. Further visualizations and\nanalysis can be found in the supplementary materials.\n5. Discussion\nPrecision in generative models. Learning a distribution for\nthe bounding boxes given the BEV features stands in stark\ncontrast to text-to-image tasks. One should recognize that\neven for a very detailed text prompt, there are many corre-\nsponding valid images. Perturbing the pixel values will not\nchange the description significantly. Thus, text-to-image\ntasks tolerate a large amount of variation in the generated\nsamples. Object detection, however, requires precision in\nthe outputs. Hence, adjusting for number of queries, we\nfind a small performance gap in mAP with respect to deter-\nministic approaches natural, as the random reference inputs\nwill always induce a distribution on the outputs.\nUncertainty. One benefit of learning a distribution over\nthe boxes is that this provides a rudimentary way to un-\nderstand their uncertainty. Unfortunately, it is likely that\nit mixes epistemic uncertainty resulting from the estimated\nmodel parameters and aleatoric uncertainty related to the\nrandomness of the boxes themselves. Heatmaps showing\nestimated box distributions are available in the supplemen-\ntary materials, along with further discussion.\n6. Conclusion\nIn this work we explored diffusion-based generative models\nfor 3D object detection in BEV. We have shown that naively\nusing previous approaches yields a performance gap. To\nclose it, we adopt a transformer-based architecture and a\nspecific query interpolation module to facilitate the model\nin learning positional information even in the presence of\ndiffusion. We formulate the diffusion process as diffusion\nover particles, which yields a unique interpretation based on\nparticle methods. Our approach greatly improves on previ-\nous generative methods and achieves comparable results to\nstrong deterministic ones.\n8\n7. Aknowledgements\nThis research was partially funded by the Ministry of Edu-\ncation and Science of Bulgaria (support for INSAIT, part of\nthe Bulgarian National Roadmap for Research Infrastruc-\nture).\nReferences\n[1] German Barquero, Sergio Escalera, and Cristina Palmero.\nBelfusion: Latent diffusion for behavior-driven human mo-\ntion prediction. In Proceedings of the IEEE/CVF Interna-\ntional Conference on Computer Vision, pages 2317\u20132327,\n2023. 2\n[2] Garrick Brazil and Xiaoming Liu. M3d-rpn: Monocular 3d\nregion proposal network for object detection. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 9287\u20139296, 2019. 1\n[3] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt\nSchiele. Kinematic 3d object detection in monocular video.\nIn Computer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part\nXXIII 16, pages 135\u2013152. Springer, 2020. 1\n[4] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Gi-\nancarlo Baldan, and Oscar Beijbom.\nnuscenes: A multi-\nmodal dataset for autonomous driving. In Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 11621\u201311631, 2020. 2, 6\n[5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-\nend object detection with transformers. In European confer-\nence on computer vision, pages 213\u2013229. Springer, 2020. 2,\n3, 4, 6\n[6] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov.\nMultipath: Multiple probabilistic anchor tra-\njectory hypotheses for behavior prediction. arXiv preprint\narXiv:1910.05449, 2019. 3\n[7] Qiang Chen, Xiaokang Chen, Jian Wang, Shan Zhang, Kun\nYao, Haocheng Feng, Junyu Han, Errui Ding, Gang Zeng,\nand Jingdong Wang.\nGroup detr: Fast detr training with\ngroup-wise one-to-many assignment. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 6633\u20136642, 2023. 3\n[8] Shoufa Chen, Peize Sun, Yibing Song, and Ping Luo. Dif-\nfusiondet: Diffusion model for object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 19830\u201319843, 2023. 2, 4, 6\n[9] Zhennan Chen, Rongrong Gao, Tian-Zhu Xiang, and Fan\nLin.\nDiffusion model for camouflaged object detection.\narXiv preprint arXiv:2308.00303, 2023. 1, 2, 7, 4, 5\n[10] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye.\nCome-closer-diffuse-faster: Accelerating conditional diffu-\nsion models for inverse problems through stochastic contrac-\ntion. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, pages 12413\u201312422,\n2022. 4\n[11] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu,\nand Mubarak Shah. Diffusion models in vision: A survey.\nIEEE Transactions on Pattern Analysis and Machine Intelli-\ngence, 2023. 1\n[12] Alexander Cui, Sergio Casas, Kelvin Wong, Simon Suo, and\nRaquel Urtasun. Gorela: Go relative for viewpoint-invariant\nmotion forecasting.\nIn 2023 IEEE International Confer-\nence on Robotics and Automation (ICRA), pages 7801\u20137807.\nIEEE, 2023. 3\n[13] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong\nZhang, Han Hu, and Yichen Wei. Deformable convolutional\nnetworks. In Proceedings of the IEEE international confer-\nence on computer vision, pages 764\u2013773, 2017. 6\n[14] Xiyang Dai, Yinpeng Chen, Jianwei Yang, Pengchuan\nZhang, Lu Yuan, and Lei Zhang. Dynamic detr: End-to-\nend object detection with dynamic attention. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 2988\u20132997, 2021. 2, 3\n[15] Patrick Dendorfer, Vladimir Yugay, Aljosa Osep, and Laura\nLeal-Taix\u00b4e. Quo vadis: Is trajectory forecasting the key to-\nwards long-term multi-object tracking? Advances in Neural\nInformation Processing Systems, 35:15657\u201315671, 2022. 3\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and\npattern recognition, pages 248\u2013255. Ieee, 2009. 2\n[17] Mark Everingham, Luc Van Gool, Christopher KI Williams,\nJohn Winn, and Andrew Zisserman. The pascal visual object\nclasses (voc) challenge. International journal of computer\nvision, 88:303\u2013338, 2010. 2\n[18] Runyang Feng, Yixing Gao, Tze Ho Elden Tse, Xueqing\nMa, and Hyung Jin Chang. Diffpose: Spatiotemporal dif-\nfusion model for video-based human pose estimation.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 14861\u201314872, 2023. 2\n[19] Zheng Ge, Songtao Liu, Zeming Li, Osamu Yoshie, and Jian\nSun. Ota: Optimal transport assignment for object detection.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 303\u2013312, 2021. 7\n[20] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian\nSun. Yolox: Exceeding yolo series in 2021. arXiv preprint\narXiv:2107.08430, 2021. 7, 2, 4\n[21] Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra\nMalik. Rich feature hierarchies for accurate object detection\nand semantic segmentation. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition, pages\n580\u2013587, 2014. 2, 6\n[22] JunYoung Gwak, Silvio Savarese, and Jeannette Bohg.\nMinkowski tracker:\nA sparse spatio-temporal r-cnn for\njoint object detection and tracking.\narXiv preprint\narXiv:2208.10056, 2022. 3\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proceed-\nings of the IEEE conference on computer vision and pattern\nrecognition, pages 770\u2013778, 2016. 2, 4\n[24] Kaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Gir-\nshick. Mask r-cnn. In Proceedings of the IEEE international\nconference on computer vision, pages 2961\u20132969, 2017. 2\n9\n[25] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-\nfusion probabilistic models. Advances in neural information\nprocessing systems, 33:6840\u20136851, 2020. 1\n[26] Anthony Hu, Zak Murez, Nikhil Mohan, Sof\u00b4\u0131a Dudas, Jef-\nfrey Hawke, Vijay Badrinarayanan, Roberto Cipolla, and\nAlex Kendall. Fiery: Future instance prediction in bird\u2019s-\neye view from surround monocular cameras. In Proceedings\nof the IEEE/CVF International Conference on Computer Vi-\nsion, pages 15273\u201315282, 2021. 3\n[27] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi\nYan, and Dacheng Tao. St-p3: End-to-end vision-based au-\ntonomous driving via spatial-temporal feature learning. In\nEuropean Conference on Computer Vision, pages 533\u2013549.\nSpringer, 2022. 3\n[28] Junjie Huang and Guan Huang. Bevdet4d: Exploit tempo-\nral cues in multi-camera 3d object detection. arXiv preprint\narXiv:2203.17054, 2022. 3\n[29] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin\nGao, Weiming Hu, and Yu-Gang Jiang. Polarformer: Multi-\ncamera 3d object detection with polar transformer. In Pro-\nceedings of the AAAI Conference on Artificial Intelligence,\npages 1042\u20131050, 2023. 3\n[30] Peng Jin, Hao Li, Zesen Cheng, Kehan Li, Xiangyang Ji,\nChang Liu, Li Yuan, and Jie Chen. Diffusionret: Genera-\ntive text-video retrieval with diffusion model. arXiv preprint\narXiv:2303.09867, 2023. 2\n[31] James Kennedy and Russell Eberhart. Particle swarm opti-\nmization. In Proceedings of ICNN\u201995-international confer-\nence on neural networks, pages 1942\u20131948. IEEE, 1995. 6\n[32] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni,\nand Lei Zhang. Dn-detr: Accelerate detr training by intro-\nducing query denoising. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 13619\u201313627, 2022. 2, 3\n[33] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang,\nLewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng,\nHao Tian, et al. Delving into the devils of bird\u2019s-eye-view\nperception: A review, evaluation and recipe. arXiv preprint\narXiv:2209.05324, 2022. 1\n[34] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chong-\nhao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer:\nLearning bird\u2019s-eye-view representation from multi-camera\nimages via spatiotemporal transformers. In European con-\nference on computer vision, pages 1\u201318. Springer, 2022. 1,\n3, 4, 6, 7, 8, 2, 5\n[35] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays,\nPietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence\nZitnick. Microsoft coco: Common objects in context. In\nComputer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer, 2014. 2, 7\n[36] Tsung-Yi Lin, Piotr Doll\u00b4ar, Ross Girshick, Kaiming He,\nBharath Hariharan, and Serge Belongie.\nFeature pyra-\nmid networks for object detection.\nIn Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, pages 2117\u20132125, 2017. 4\n[37] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and\nPiotr Doll\u00b4ar. Focal loss for dense object detection. In Pro-\nceedings of the IEEE international conference on computer\nvision, pages 2980\u20132988, 2017. 5\n[38] Daochang Liu, Qiyue Li, Anh-Dung Dinh, Tingting Jiang,\nMubarak Shah, and Chang Xu. Diffusion action segmenta-\ntion. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 10139\u201310149, 2023. 2\n[39] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi,\nHang Su, Jun Zhu, and Lei Zhang.\nDab-detr: Dynamic\nanchor boxes are better queries for detr.\narXiv preprint\narXiv:2201.12329, 2022. 2, 3\n[40] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. In\nProceedings of the IEEE/CVF international conference on\ncomputer vision, pages 10012\u201310022, 2021. 2\n[41] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furi-\nous: Real time end-to-end 3d detection, tracking and motion\nforecasting with a single convolutional net. In Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition, pages 3569\u20133577, 2018. 3\n[42] Jiageng Mao, Shaoshuai Shi, Xiaogang Wang, and Hong-\nsheng Li. 3d object detection for autonomous driving: A\ncomprehensive survey. International Journal of Computer\nVision, pages 1\u201355, 2023. 1\n[43] Weibo Mao, Chenxin Xu, Qi Zhu, Siheng Chen, and Yan-\nfeng Wang. Leapfrog diffusion model for stochastic trajec-\ntory prediction. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 5517\u2013\n5526, 2023. 4\n[44] Alexander Quinn Nichol and Prafulla Dhariwal. Improved\ndenoising diffusion probabilistic models.\nIn International\nConference on Machine Learning, pages 8162\u20138171. PMLR,\n2021. 4\n[45] Duo Peng, Ping Hu, Qiuhong Ke, and Jun Liu. Diffusion-\nbased image translation with label guidance for domain\nadaptive semantic segmentation.\nIn Proceedings of the\nIEEE/CVF International Conference on Computer Vision,\npages 808\u2013820, 2023. 2\n[46] Lang Peng, Zhirong Chen, Zhangjie Fu, Pengpeng Liang,\nand Erkang Cheng. Bevsegformer: Bird\u2019s eye view semantic\nsegmentation from arbitrary camera rigs. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, pages 5935\u20135943, 2023. 3\n[47] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encod-\ning images from arbitrary camera rigs by implicitly unpro-\njecting to 3d. In Computer Vision\u2013ECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23\u201328, 2020, Pro-\nceedings, Part XIV 16, pages 194\u2013210. Springer, 2020. 1,\n3\n[48] Zequn Qin, Jingyu Chen, Chao Chen, Xiaozhi Chen, and\nXi Li.\nUnifusion: Unified multi-view fusion transformer\nfor spatial-temporal representation in bird\u2019s-eye-view.\nIn\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 8690\u20138699, 2023. 3\n[49] Cody Reading, Ali Harakeh, Julia Chae, and Steven L\nWaslander.\nCategorical depth distribution network for\nmonocular 3d object detection.\nIn Proceedings of the\n10\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8555\u20138564, 2021. 3\n[50] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir\nSadeghian, Ian Reid, and Silvio Savarese. Generalized in-\ntersection over union: A metric and a loss for bounding\nbox regression. In Proceedings of the IEEE/CVF conference\non computer vision and pattern recognition, pages 658\u2013666,\n2019. 5, 2\n[51] Thomas Roddick, Alex Kendall, and Roberto Cipolla. Ortho-\ngraphic feature transform for monocular 3d object detection.\narXiv preprint arXiv:1811.08188, 2018. 1, 3\n[52] Robin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00a8orn Ommer.\nHigh-resolution image\nsynthesis with latent diffusion models.\nIn Proceedings of\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, pages 10684\u201310695, 2022. 1, 4\n[53] Chitwan Saharia, William Chan, Saurabh Saxena, Lala\nLi, Jay Whang, Emily L Denton, Kamyar Ghasemipour,\nRaphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans,\net al. Photorealistic text-to-image diffusion models with deep\nlanguage understanding.\nAdvances in Neural Information\nProcessing Systems, 35:36479\u201336494, 2022. 1\n[54] Tim Schreier, Katrin Renz, Andreas Geiger, and Kashyap\nChitta. On offline evaluation of 3d object detection for au-\ntonomous driving. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 4084\u20134089,\n2023. 6\n[55] Jiaming\nSong,\nChenlin\nMeng,\nand\nStefano\nErmon.\nDenoising diffusion implicit models.\narXiv preprint\narXiv:2010.02502, 2020. 1, 4, 6, 8\n[56] Anil Osman Tur, Nicola Dall\u2019Asen, Cigdem Beyan, and\nElisa Ricci.\nExploring diffusion models for unsupervised\nvideo anomaly detection.\nIn 2023 IEEE International\nConference on Image Processing (ICIP), pages 2540\u20132544.\nIEEE, 2023. 2\n[57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. Advances in neural\ninformation processing systems, 30, 2017. 7\n[58] Jianyuan Wang, Christian Rupprecht, and David Novotny.\nPosediffusion: Solving pose estimation via diffusion-aided\nbundle adjustment. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 9773\u20139783,\n2023. 4\n[59] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin.\nFcos3d: Fully convolutional one-stage monocular 3d object\ndetection.\nIn Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 913\u2013922, 2021. 6\n[60] Tai Wang, ZHU Xinge, Jiangmiao Pang, and Dahua Lin.\nProbabilistic and geometric depth: Detecting objects in per-\nspective. In Conference on Robot Learning, pages 1475\u2013\n1485. PMLR, 2022. 6\n[61] Yue Wang, Vitor Campagnolo Guizilini, Tianyuan Zhang,\nYilun Wang, Hang Zhao, and Justin Solomon.\nDetr3d:\n3d object detection from multi-view images via 3d-to-2d\nqueries. In Conference on Robot Learning, pages 180\u2013191.\nPMLR, 2022. 6\n[62] Yanghao Wang, Zhongqi Yue, Xian-Sheng Hua, and Han-\nwang Zhang. Random boxes are open-world object detec-\ntors. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pages 6233\u20136243, 2023. 2\n[63] Pengxiang Wu, Siheng Chen, and Dimitris N Metaxas. Mo-\ntionnet:\nJoint perception and motion prediction for au-\ntonomous driving based on bird\u2019s eye view maps. In Pro-\nceedings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 11385\u201311395, 2020. 3\n[64] Xingxing Xie, Gong Cheng, Jiabao Wang, Xiwen Yao, and\nJunwei Han. Oriented r-cnn for object detection. In Proceed-\nings of the IEEE/CVF international conference on computer\nvision, pages 3520\u20133529, 2021. 6, 2\n[65] Chenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou\nZhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao,\nLewei Lu, et al.\nBevformer v2: Adapting modern image\nbackbones to bird\u2019s-eye-view recognition via perspective su-\npervision. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 17830\u2013\n17839, 2023. 3\n[66] Mingqiao Ye, Lei Ke, Siyuan Li, Yu-Wing Tai, Chi-Keung\nTang, Martin Danelljan, and Fisher Yu. Cascade-detr: Delv-\ning into high-quality universal object detection. In Proceed-\nings of the IEEE/CVF International Conference on Com-\nputer Vision, pages 6704\u20136714, 2023. 3\n[67] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun\nZhu, Lionel M Ni, and Heung-Yeung Shum.\nDino: Detr\nwith improved denoising anchor boxes for end-to-end object\ndetection. arXiv preprint arXiv:2203.03605, 2022. 2, 3, 7\n[68] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui\nHu, and Raquel Urtasun. Learning unsupervised world mod-\nels for autonomous driving via discrete diffusion.\narXiv\npreprint arXiv:2311.01017, 2023. 2\n[69] Yunpeng Zhang, Zheng Zhu, Wenzhao Zheng, Junjie Huang,\nGuan Huang, Jie Zhou, and Jiwen Lu. Beverse: Unified per-\nception and prediction in birds-eye-view for vision-centric\nautonomous driving.\narXiv preprint arXiv:2205.09743,\n2022. 3\n[70] Yuzhong Zhao, Qixiang Ye, Weijia Wu, Chunhua Shen, and\nFang Wan. Generative prompt model for weakly supervised\nobject localization. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 6351\u20136361,\n2023. 2\n[71] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection.\narXiv preprint\narXiv:2010.04159, 2020. 2, 4, 5, 6, 7, 1\n[72] Jiayu Zou, Zheng Zhu, Yun Ye, and Xingang Wang. Diffbev:\nConditional diffusion model for bird\u2019s eye view perception.\narXiv preprint arXiv:2303.08333, 2023. 2\n11\nDiffusion-Based Particle-DETR for BEV Perception\nSupplementary Material\nIn this document we provide additional reasoning, re-\nsults, and comprehensive details supporting our method.\nAppendix A. Label Ambiguity\nLabel ambiguity arises when the matching between predic-\ntions and targets depends on the sampling of the references.\nFigure 4 showed an example where we match using the to-\ntal distance. In Figure 8 we provide also a case where the\nmatching is by index and label ambiguity is present. We\nassume there are N targets y1, ..., yN and N predictions\nb1, ..., bN, and predictions bi is matched with target yi.\nBEV Features\nTarget 1\nNoisy reference\npoint 1\nPrediction 2\nNoisy reference\npoint 2\nBEV Features\nTarget 1\nNoisy reference\npoint 2\nPrediction 1\nNoisy reference\npoint 1\nTarget 2\nScenario 1\nScenario 2\nMatching by index\nTarget 2\nPrediction 2\nPrediction 1\nFigure 8. Label ambiguity when matching by index. Permut-\ning the initially sampled reference points causes the matching to\nchange which makes training unstable.\nWhen matching by index, suppose reference ri has BEV\ncoordinates (x, y). The model either looks up the BEV fea-\ntures at (x, y) or interpolates the queries at (x, y) and pro-\nduces a prediction which is matched to yi. However, if rj is\nsampled at location (x, y) the prediction will be the same,\nbut the target will be yj. This confuses the model because\nthe same features at (x, y) can have different targets.\nTo assess the impact that ambiguous targets may have\non the results, we study a simple toy task. We fix a single\nrandom image I \u2208RC,H,W and construct a network with\nthe following forward pass:\n\u2022 First, the image is passed through two convolution layers\nthat keep the output size the same as the initial size.\n\u2022 Then, we look up, i.e. interpolate, the features at a num-\nber of reference points, provided as an additional input.\n\u2022 The resulting features are processed by two linear layers\nafter which new 2D locations are returned.\nThus, our network takes in an image and some reference\npoints, and returns new 2D locations as output. The loss\nfunction is the simple \u21131 loss between the predictions and a\nnumber of fixed targets. Our experiments suggest that when\nwe sample the input reference points randomly, and there\nare only a few of them, the resulting label ambiguity is lim-\niting and prevents the network from overfitting, even on a\nsingle image. This is the case when there are fewer or equal\nrandom references than the number of targets. In Figure 4\nwe have 10 targets. With only 10 random references, the\nmodel does not converge, even if we let it run indefinitely.\nWith more references than targets, the model does man-\nage to converge, with the convergence speed depending on\nthe number of references. This is because only some pre-\ndictions are used in the loss function, which makes predic-\ntions more localized to where their reference points start\nfrom. This behaviour does not exist if the references are\nfixed across training iterations, in which case the model al-\nways converges to a loss of zero, irrespective of whether it\nfinds all targets or only some of them.\nIn a deterministic setup more references only speed up\ntraining. But when they are random, it becomes necessary\nto have many of them in order to converge.\nAppendix B. Implementation\nHere we provide additional information about the imple-\nmentation and the experiments. Table 3 contains the train-\ning hyperparameters, while Algorithms 1 and 2 provide\nPyTorch-like pseudocodes for the training and testing logic.\nMost function names are borrowed from [9].\nSetting\nValue\nNum. learnable object queries\n900\nNum. references at test time\nVariable\nBEV size, (H, W, C)\n(200, 200, 256)\nOptimizer\nAdamW\nSignal-to-noise ratio\n2\nDDIM steps [55]\n3\nNMS discard threshold\n0.1\nMin. confidence threshold\n0.02\nRadial suppression threshold\n0.5\nQuery formation\nInterpolation\nMatching type\nSimple many-to-one\nBEV encoder\nBEVFormer-Base [34]\nDecoder type\nDeformableDETR [71]\nTraining epochs\n24\nLR schedule\nCosine, 2 \u00d7 10\u22124 \u219210\u22126\nClip gradient 2-norm\n35\nBatch size\n1\nTable 3. Particle-DETR training hyperparameters. Additional\nhyperparameters follow BEVFormer-Base [34].\n1\nAlgorithm 1: Particle-DETR Training\n# Inputs\n# F: BEV features, (C, H, W)\n# fixed queries:\nqueries over which to\ninterpolate, (N, C)\n# GTs:\nground-truth boxes, (N, C)\n# scale:\nthe signal-to-noise ratio\n# Extract object centers in BEV\nGT centers = GTs[..., :2]\n# Pad references up to a desired number\nref points = pad refs(GT centers)\n# Scale and apply diffusion\nref points = (2 * ref points - 1) * scale\ndiff time = randint(0, T)\neps = normal(mean=0, std=1)\nref points = sqrt(alpha cumprod(diff time))\n* ref points + sqrt(1 -\nalpha cumprod(diff time)) * eps\n# Interpolate the queries\nqueries = grid sample(fixed queries,\nref points)\nkeys = F\nvalues = F\n# Call the decoder and compute loss\npred boxes = decoder(queries, keys, values,\nref points, diff time)\nloss = set prediction loss(GTs, pred boxes)\nFor the implementation, our codebase is based on that of\nBEVFormer [34]. We train all models for 24 epochs on the\nNuScenes dataset [4] on 8 NVidia A100 GPUs, while the\nevaluation is always performed on a single GPU. At both\ntraining and test time the batch size is set to 1.\nDiffusion box updates. The original DiffusionDet [9]\nonly works with axis-aligned boxes. For the baseline, we\nre-implement and modify it to use rotated boxes. Inspired\nby [64], each stage of the decoder outputs the elements\n(\u03b4cx, \u03b4cy, cz, \u03b4w, \u03b4h, l, \u03b4\u03b8, vx, vy), which are applied to the\ninput boxes (cx, cy, w, h, \u03b8) to produce the updated boxes\nat the current stage (c\u2032\nx, c\u2032\ny, w\u2032, h\u2032, \u03b8\u2032) as follows:\n\u00afw = w cos \u03b8 + h sin \u03b8\n\u00afh = w sin \u03b8 + h cos \u03b8\nc\u2032\nx = \u00afw\u03b4cx + cx\nc\u2032\ny = \u00afh\u03b4cy + cy\nw\u2032 = exp(\u03b4w)w\nh\u2032 = exp(\u03b4h)h\n\u03b8\u2032 = \u03b8 + \u03b4\u03b8.\nGIoU loss. Unlike the original DiffusionDet [9], in our\nimplementation we do not use the GIoU loss [50] because\nits implementation for rotated boxes with supported back-\nAlgorithm 2: Particle-DETR Inference\n# Inputs\n# F: BEV features, (C, H, W)\n# fixed queries:\nqueries over which to\ninterpolate, (N, C)\n# Prepare random references\neps = normal(mean=0, std=1)\nref points = normalize(eps)\n# DDIM times [(T-1, T-2), ..., (0, -1)]\ntimes = reversed(linespace(-1, T, steps))\ntime pairs = list(zip(times[:-1], times[1:])\n# Interpolate the queries\nqueries = grid sample(fixed queries,\nref points)\nkeys = F\nvalues = F\nall preds = []\nfor (t now, t next) in time pairs:\npred boxes, queries = decoder(queries,\nkeys, values, ref points,\nt now)\nall preds.append(pred boxes)\nref points = ddim step(ref points,\npred boxes, t now, t next)\nref points = ref renewal(ref points)\n# Filter predictions\npreds = nms(all preds)\npreds = radial suppression(preds)\npropagation is non-trivial.\nWe leave the investigation of\nhow similar metrics can be used as losses for future work.\nSimOTA. When evaluating the simOTA matching strat-\negy [20] we remove the cost matrix masking which is\npresent in DiffusionDet [9]. There, additional cost is added\nto those predictions whose center does not fall close enough\nto or within a target box, which effectively prevents these\npredictions from ever being matched.\nIn our setup, this\nmasking introduced a large amount of instability. Hence,\nwe utilize simOTA using the raw cost matrix, where all pre-\ndictions are considered as potential matchings to all targets.\nAppendix C. Additional Experiments\nHere we provide additional experimental results. All results\nand plots are from the NuScenes [4] validation dataset, un-\nless otherwise noted.\nDeterministic and random references. Our Particle-\nDETR provides rich opportunities to tweak the test-time\nperformance after training. Once the model is trained, one\ncan freely change the hyparparameters governing the infer-\nence behaviour. First, we assess how the joint training with\ntwo sets of references, fixed and random, affects perfor-\nmance. Once the model from this joint setup is trained, we\ncan evaluate with both sets of references or with either one\nof them. Table 6 shows the performance when evaluating\n2\nDDIM steps\nDiff. queries\nmAP \u2191\nNDS \u2191\n1\n300\n0.4192\n0.5301\n500\n0.4192\n0.5302\n700\n0.4202\n0.5306\n900\n0.4211\n0.5310\n1200\n0.4208\n0.5304\n1500\n0.4208\n0.5302\n3\n300\n0.4109\n0.5245\n500\n0.4155\n0.5275\n700\n0.4171\n0.5285\n900\n0.4188\n0.5289\n1200\n0.4184\n0.5285\n1500\n0.4192\n0.5290\n5\n300\n0.4164\n0.5275\n500\n0.4190\n0.5288\n700\n0.4201\n0.5295\n900\n0.4198\n0.5290\n1200\n0.4196\n0.5292\n1500\n0.4197\n0.5293\n7\n300\n0.4172\n0.5281\n500\n0.4194\n0.5289\n700\n0.4203\n0.5297\n900\n0.4199\n0.5290\n1200\n0.4201\n0.5292\n1500\n0.4189\n0.5287\n9\n300\n0.4179\n0.5280\n500\n0.4192\n0.5290\n700\n0.4198\n0.5294\n900\n0.4197\n0.5291\n1200\n0.4186\n0.5287\n1500\n0.4184\n0.5284\nTable 4.\nJoint effects of DDIM steps and diffusion object\nqueries when using both diffusion and fixed queries. With both\nquery sets, adding more random queries or DDIM steps does not\nhave any noticeable effect. In practice, one could use just a single\nDDIM step with a variable number of queries.\nwith both sets. The performance is statistically-significant\nand outperforms BEVFormer on all metrics.\nWe analyze how each reference set contributes the re-\nsults.\nTo do this, we look at the self-attention values\nthroughout the decoder layers. When evaluating with both\nquery sets, fixed queries from the first decoder layer spend\nabout 94% of their attention on other fixed queries, whereas\nthe diffusion queries, associated with the random refer-\nences, spend 82.5% of their attention on the fixed queries.\nThis imbalance is rectified in the subsequent decoder layers.\nParticularly, in the last decoder layer both query sets spend\nabout 50% of their attention onto the other query set.\nWhen evaluating the final model with both static and ran-\ndom references we find that increasing the number of DDIM\nsteps or the number of random queries does not have a sig-\nnificant effect, presumably because the queries correspond-\ning to the static references are more important. This is high-\nDDIM steps\nDiff. queries\nmAP \u2191\nNDS \u2191\n1\n300\n0.3540\n0.4936\n500\n0.3907\n0.5140\n700\n0.4046\n0.5208\n900\n0.4105\n0.5242\n1200\n0.4143\n0.5261\n1500\n0.4162\n0.5273\n3\n300\n0.3755\n0.5038\n500\n0.3944\n0.5141\n700\n0.4032\n0.5191\n900\n0.4076\n0.5208\n1200\n0.4114\n0.5232\n1500\n0.4127\n0.5242\n5\n300\n0.3930\n0.5122\n500\n0.4062\n0.5201\n700\n0.4111\n0.5234\n900\n0.4133\n0.5250\n1200\n0.4144\n0.5253\n1500\n0.4154\n0.5263\n7\n300\n0.3999\n0.5164\n500\n0.4096\n0.5224\n700\n0.4123\n0.5241\n900\n0.4144\n0.5257\n1200\n0.4148\n0.5259\n1500\n0.4154\n0.5265\n9\n300\n0.4019\n0.5175\n500\n0.4110\n0.5237\n700\n0.4134\n0.5249\n900\n0.4149\n0.5259\n1200\n0.4148\n0.5261\n1500\n0.4147\n0.5258\nTable 5. The joint effects of DDIM steps and diffusion object\nqueries when utilizing diffusion queries only. Here, we explore\nwhether adding more DDIM steps or more diffusion queries im-\nproves performance. When using only diffusion queries, more\nDDIM steps and more queries improve performance.\nlighted in Table 4. The mAP and NDS results are slightly\nhigher than if we are using the static queries only, as in Ta-\nble 2, showing that the additional queries corresponding to\nMetric\nBEVFormer-Base\nBEVFormer-Enh (ours)\nNDS \u2191\n0.5168\n0.5287 (0.0003)\nmAP \u2191\n0.4154\n0.4184 (0.0006)\nmATE \u2193\n0.6715\n0.6386 (0.0009)\nmASE \u2193\n0.2738\n0.2686 (0.0002)\nmAOE \u2193\n0.3691\n0.3362 (0.0009)\nmAVE \u2193\n0.4179\n0.3688 (0.0013)\nmAAE \u2193\n0.1981\n0.1931 (0.0007)\nTable 6.\nPerformance of BEVFormer-Enh.\nWe compare\nBEVFormer-Enh, evaluated with both static and random refer-\nences, to the fully-deterministic BEVFormer. For our method, we\nrepeat the evaluation 10 times and report the mean values. The\nstandard deviations are shown in parentheses.\n3\nDDIM steps\nDiff. queries\nmAP \u2191\nNDS \u2191\n1\n300\n0.3540\n0.4931\n500\n0.3899\n0.5125\n700\n0.4042\n0.5202\n900\n0.4108\n0.5239\n1200\n0.4142\n0.5260\n1500\n0.4164\n0.5272\n3\n300\n0.3746\n0.5031\n500\n0.3952\n0.5143\n700\n0.4034\n0.5187\n900\n0.4080\n0.5214\n1200\n0.4113\n0.5238\n1500\n0.4139\n0.5250\n5\n300\n0.3940\n0.5128\n500\n0.4070\n0.5210\n700\n0.4124\n0.5244\n900\n0.4146\n0.5257\n1200\n0.4159\n0.5267\n1500\n0.4169\n0.5269\n7\n300\n0.3999\n0.5162\n500\n0.4104\n0.5230\n700\n0.4133\n0.5244\n900\n0.4154\n0.5263\n1200\n0.4162\n0.5267\n1500\n0.4165\n0.5270\n9\n300\n0.4031\n0.5175\n500\n0.4120\n0.5240\n700\n0.4142\n0.5248\n900\n0.4156\n0.5263\n1200\n0.4161\n0.5270\n1500\n0.4165\n0.5272\nTable 7. The joint effects of DDIM steps and diffusion object\nqueries when utilizing radial suppression. When using only dif-\nfusion queries, more DDIM steps and more queries improve per-\nformance considerably.\nrandom references do increase performance.\nWhat happens if we use only the random references? In\nthat case the mAP and NDS metrics are naturally lower, be-\ncause the proposed corrections should all be relative to the\ncurrent reference location, which is random. There is a clear\ntrade-off between the usage of additional DDIM steps or ad-\nditional references, and performance. Tables 5 and 7 show\nthis with and without radial suppression. We highlight that\ngiven enough random references the Particle-DETR does\nmanage to beat BEVFormer on mAP. On NDS, it only takes\n900 references and a single DDIM step to beat it.\nFiltering. We also provide additional justification for\nwhy filtering is needed. Since the many-to-one matching\ncauses multiple predictions to be stacked on top of each\nother we found the usage of NMS necessary.\nThe best\nthreshold is 0.1, which we also combine with confidence-\nbased filtering, as shown in Table 8.\nOur radial suppression replaces a confident predicted\nNMS threshold\nConf. threshold\nmAP \u2191\nNDS \u2191\n0.1\n0.02\n0.3845\n0.5135\n0.05\n0.3818\n0.5138\nNone\n0.3847\n0.5132\n0.5\n0.02\n0.3876\n0.5129\n0.05\n0.3859\n0.5142\nNone\n0.3876\n0.5128\nNone\n0.02\n0.2264\n0.4305\n0.05\n0.2261\n0.4314\nNone\n0.2264\n0.4303\nTable 8. Filtering using NMS and confidence thresholds. We\nevaluate a Particle-DETR trained with SimOTA [20] matching\nwithout radial suppression. We set the DDIM steps to 3 and vary\nthe NMS threshold and the score threshold. The results show that\nNMS is needed. Furthermore, using SimOTA matching does not\nresult in better performance than simple many-to-one matching.\nbox with the weighted average of the predictions within a\nball neighborhood. We tune the radius of this neighbor-\nhood. For very small values no predictions are filtered. For\nlarge values predictions from multiple different objects are\nfiltered. The optimal occurs around 0.5 meters, as shown in\nTable 9. Data for the joint tuning of the radius and the NMS\ncan be found in Table 10.\nReference resampling. Following DiffusionDet [9] we\nresample the search tokens between DDIM steps. We ex-\nperimented with different strategies but found the basic one\nin [9] to work best, as shown in Table 11. Thus, between\neach DDIM step we simply resample the references corre-\nsponding to less confident predictions. Alternative strate-\ngies which we tested include resampling close to the con-\nfident predictions, resampling without applying the DDIM\nsteps, or no resampling altogether.\nModel characteristics. Our BEVFormer-Enh model has\nSetting\nmAP \u2191\nNDS \u2191\nr = 0.5 m\n0.4112\n0.5234\nr = 0.75 m\n0.4143\n0.5257\nr = 1 m\n0.4132\n0.5257\nr = 1.25 m\n0.4102\n0.5242\nr = 1.5 m\n0.4073\n0.5226\nr = 1.75 m\n0.4030\n0.5204\nr = 2 m\n0.3971\n0.5172\nr = 3 m\n0.3697\n0.5014\nr = 4 m\n0.3372\n0.4806\nTable 9. The effect of radial suppression. Here, we set the NMS\nthreshold to 0.5, the DDIM steps to 3, the score threshold to 0.02\nand only vary the radius of the radial suppression. The results sug-\ngest that a small radius is best because it keeps the averaging suffi-\nciently localized. Larger radii cause predictions corresponding to\ndifferent targets to be averaged, reducing detection accuracy.\n4\nthe same number of parameters, FLOPS, and FPS as BEV-\nFormer [34]. The Particle-DETR is similar but can use more\ncompute depending on how many DDIM steps one runs.\nThe compute in the query interpolation depends only on the\nnumber of reference points and not on the DDIM steps.\nSetting\nmAP \u2191\nNDS \u2191\nNMS threshold = 0.1\nr = 0.5 m\n0.4188\n0.5289\nr = 0.75 m\n0.4173\n0.5283\nr = 1 m\n0.4138\n0.5265\nr = 1.25 m\n0.4101\n0.5246\nr = 1.5 m\n0.4066\n0.5228\nNMS threshold = 0.5\nr = 0.5 m\n0.4112\n0.5237\nr = 0.75 m\n0.4130\n0.5251\nr = 1 m\n0.4123\n0.5250\nr = 1.25 m\n0.4096\n0.5234\nr = 1.5 m\n0.4067\n0.5222\nTable 10. The joint effects of radial suppression and NMS.\nHere, we set the DDIM steps to 3 and the score threshold to 0.02\nand only vary the NMS threshold and the radius of the radial sup-\npression. The best results are achieved with strong NMS suppres-\nsion and a relatively small radius.\nResampling strategy\nmAP \u2191\nNDS \u2191\nStandard normal resampling [9]\n0.4156\n0.5264\nResample near predictions\n0.4144\n0.5251\nNo DDIM, with resampling\n0.4146\n0.5252\nNo DDIM, no resampling\n0.3959\n0.5130\nTable 11. Different resampling strategies. The simple strategy\nof pruning the low-confident references are replacing them with\nnew random ones works best.\nMetric\nParticle-DETR\nBEVFormer-Enh\nNDS \u2191\n0.5283\n0.5323\nmAP \u2191\n0.4287\n0.4326\nmATE \u2193\n0.6011\n0.5904\nmASE \u2193\n0.2600\n0.2595\nmAOE \u2193\n0.4596\n0.4504\nmAVE \u2193\n0.4125\n0.4109\nmAAE \u2193\n0.1275\n0.1287\nTable 12. Nuscenes test set metrics. For the Particle-DETR we\nuse 1500 queries and 1 DDIM step, with radial suppression radius\nset to 0.5 and NMS threshold set to 0.1.\nAppendix D. Qualitative Analysis\nHere we provide additional visualizations of the predic-\ntions.\nFigure 9 shows an example where our Particle-\nDETR, using only the diffusion queries, detects very small\nobjects which are missed by BEVFormer. In fact, the AP\nat 0.5 meters for traffic cones is above 0.34, whereas that\nFigure 9. Sample predictions in BEV. Green boxes are ground-\ntruths, red are predicted by our Particle-DETR, and blue is pre-\ndicted by BEVFormer. The right figure is a zoomed-in version of\nthe left one, centered around the ego-vehicle.\nof BEVFormer is 0.28. Similar detection boosts can be ob-\nserved also for cars (+1 AP point), bicycles (+2.4), motorcy-\ncles (+2.8), pedestrians (+1.8), and barriers (+9 AP points).\nIn general, the increased NDS results mostly from more\naccurate translation, orientation, and velocity.\nIn Figure\n13 we show predictions from our Particle-DETR projected\nonto the camera images. We highlight diverse driving con-\nditions including bright sunshine, rain - where raindrops\ncreate localized blur in the images, and nighttime - where\npixel intensity noise due to the low exposure time is present.\nIn all these cases our method produces reasonably accurate\npredictions, while being a fully generative model.\nBasins of attraction. We visualize the transformation\nof the starting references in Figure 12. In general, each GT\nattracts the starting references around it. This gradient flow\nis learned by the model and the many-to-one matching is a\nnecessary condition for its existence. The basins of attrac-\nFigure 10. Metrics comparison. The improved NDS results from\nsignificantly lower orientation, translation, and velocity errors.\n5\nFigure 11. Uncertainty comparison. Our Particle-DETR produces meaningful heatmaps due to its many-to-one attractive nature.\ntion are well localized and separated. We considered adding\n\u21131 regularization between the predictions and the starting\nreferences to explicitly make predictions more localized but\nthis was not needed as such a localization property seems to\ndevelop naturally from the training setup.\nThe BEV is patched together from multiple camera\nviews. Logically, it is desirable to prohibit reference points\nstarting from one side of the ego-vehicle to be refined to the\nother side because a view from one side does not provide\ninformation about the opposite view. The attractive nature\nof the Particle-DETR more or less satisfies this constraint.\nThe references which start in problematic BEV regions,\nFigure 12. Reference dynamics. We plot the starting random\nreferences as faint blue circles. Predicted centers with sufficient\nconfidence are shown in red. The starting references transformed\nto those predictions are in brighter blue. The black lines show how\neach reference has been modified through the six decoder stages.\nBetter viewed zoomed.\nsuch as behind walls or outside of the road, are pushed to\nthe sides of the BEV as the features at those locations do not\ncorrespond to any visible scene elements. The confidence\nof the corresponding predictions is near zero.\nAppendix E. Additional Discussion\nStandard DETR models are fully-deterministic. To get an\nuncertainty estimate over the predictions, one usually needs\nto explicitly modify the model architecture, for example by\nadding additional outputs, which stand in for the standard\ndeviation of the box location. Here, our generative Particle-\nDETR is advantageous, in that a rudimentary form of un-\ncertainty may be readily available.\nFirst, we consider the baseline BEVFormer and we plot\nheatmaps over the predicted box centers, computed using\nkernel density estimation, as shown in Figure 11. The den-\nsity (and color) at each predicted center in this plot is de-\ntermined mainly by how close this point is to nearby pre-\ndictions.\nThe first heatmap shows the density over the\npredicted centers only, even without considering the con-\nfidence of each prediction. Since BEVFormer uses one-to-\none matching, most of the predictions are quite spread apart\nand few of them are attracted to the same GT box. If we\nweight the predictions by their confidence, we get the third\nheatmap, which is more reasonable.\nNow, we apply the same procedure to our Particle-\nDETR. In the second heatmap we plot the density of\nthe predicted centers only, before applying NMS. Due to\nthe many-to-one matching during training, the predictions\nstack, which prevents the density from being too spread\napart and keeps it relatively focused on the true objects. If\nwe further consider the predicted confidence as a weight to\neach point, we get the fourth heatmap where the density is\neven better localized.\nThus, the dynamics of how predictions are formed them-\nselves contain information. Though for object detection we\nonly use a few predictions, for uncertainty estimation many\ncan be useful. We leave it for future work to investigate how\nto reason more formally about this opportunity.\n6\nFigure 13. Particle-DETR predictions projected onto the camera images. We show three scenes, each with six cameras around the\nego-vehicle. Green boxes are ground-truths, red are predicted by our Particle-DETR. We only show predictions with confidence greater\nthan 0.2. The predictions are relatively accurate across diverse driving scenarios, including sunny, overcast, and night-time conditions.\n7\n",
    "2208.09801": "PointDP: Diffusion-driven Puri\ufb01cation against\nAdversarial Attacks on 3D Point Cloud Recognition\nJiachen Sun\u22171, Weili Nie2, Zhiding Yu2, Z. Morley Mao1, and Chaowei Xiao2,3\n1University of Michigan, 2NVIDIA, 3ASU\nAbstract\n3D Point cloud is becoming a critical data representation in many real-world\napplications like autonomous driving, robotics, and medical imaging. Although\nthe success of deep learning further accelerates the adoption of 3D point cloud in\nthe physical world, deep learning is notorious for its vulnerability to adversarial\nattacks. In this work, we \ufb01rst identify that the state-of-the-art empirical defense,\nadversarial training, has a major limitation in applying to 3D point cloud models due\nto gradient obfuscation. We further propose PointDP, a puri\ufb01cation strategy that\nleverages diffusion models to defend against 3D adversarial attacks. We extensively\nevaluate PointDP on six representative 3D point cloud architectures, and leverage\n10+ strong and adaptive attacks to demonstrate its lower-bound robustness. Our\nevaluation shows that PointDP achieves signi\ufb01cantly better robustness than state-\nof-the-art puri\ufb01cation methods under strong attacks. Results of certi\ufb01ed defenses\non randomized smoothing combined with PointDP will be included in the near\nfuture.\n1\nIntroduction\nPoint cloud data is emerging as one of the most broadly used representations in 3D computer vision. It\nis a versatile data format available from various sensors like LiDAR and stereo cameras and computer-\naided design (CAD) models, which depicts physical objects by a number of coordinates in the 3D\nspace. Many deep learning-based 3D perception models have been proposed [58, 34, 43, 59, 41, 9]\nand thus realized several safety-critical applications (e.g., autonomous driving) [78, 46, 45]. Although\ndeep learning models [41, 42] have exhibited performance boost on many challenging tasks, extensive\nstudies show that they are notoriously vulnerable to adversarial attacks [5, 48, 66], where attackers\nmanipulate the input in an imperceptible manner, which will lead to incorrect predictions of the target\nmodel. Because of the broad applications of 3D point clouds in safety-critical \ufb01elds, it is imperative\nto study the adversarial robustness of point cloud recognition models.\nThe manipulation space for 2D adversarial attacks is to change pixel-level numeric values of the\ninput images. Unlike adversarial examples in 2D applications, the \ufb02exible representation of 3D point\nclouds results in an arguably larger attack surface. For example, adversaries could shift and drop\nexisting points [85], add new points into the pristine point cloud [49], or even generate new point\nclouds [86] to launch attacks. To make attacks less perceptible, different strategies like limiting\nthe number of altered points and constraining the maximal magnitude of shifted points [49]. The\n\ufb02exibility of 3D point cloud data formats enables diverse attacks, thus hindering a practical and\nuniversal defense design.\nConsidering the safety criticalness involved in 3D point cloud applications, various studies have\nbeen devoted to advancing the robustness of 3D point cloud recognition models. DUP-Net [87] and\n\u2217jiachens@umich.edu\nPreprint. Under review.\narXiv:2208.09801v1  [cs.CV]  21 Aug 2022\nGvG-PointNet++ [14] pioneered to add statistical outlier removal (SOR) modules as a pre-processing\nand in-network block, respectively, as mitigation strategies. More lately, Sun et al. [50] broke the\nrobustness of DUP-Net and GvG-PointNet++ by speci\ufb01c adaptive attacks. Adversarial training\nhas been acknowledged as the most powerful defense to deliver empirical robustness on PointNet,\nDGCNN, and PCT [49]. Meanwhile, advanced puri\ufb01cation strategies like IF-Defense [64] and\nLPC [26] leverage more complex module to clean the adversarial point clouds. However, we for\nthe \ufb01rst time demonstrate that standard adversarial training suffers from gradient obfuscation in the\npoint cloud recognition models. We also extensively evaluate IF-Defense and LPC to show that their\npuri\ufb01cation strategies are both vulnerable to stronger attacks (\u00a7 4.3).\nIn this work, we further propose PointDP, an adversarial puri\ufb01cation method that leverages a diffusion\nmodel as a cleanser module to defend against 3D adversaries. Lately, diffusion models have been\nemerging as dominant generative models [23, 36, 12], which extend to the 3D space as well [31].\nDiffusion models have been proven to be effective in defending against attacks in the 2D space [37].\nDiffusion models take two steps to (i) diffuse the input data to noise gradually and (ii) reverse the\nnoised data to its origin step by step (\u00a7 2.1). Besides the high quality of generation, diffusion models\nadd randomness in every step of its process, which could help preventing adaptive adversaries from\nlaunching attacks. We rigorously evaluate PointDP with six representative point cloud models and\nsixteen attacks. PointDP on average achieves 75.9% robust accuracy while maintaining similar clean\naccuracy to the original models.\nIn a nutshell, our contributions are summarized as two-fold:\n\u2022 We for the \ufb01rst time demonstrate that standard adversarial training [33, 49], the most longstanding\ndefense in the 2D space, has a major limitation of application in 3D point cloud models due to\narchitecture designs. We leverage black-box attacks to demonstrate our claim that drop adversarially\ntrained models\u2019 robust accuracy to \u223c10%.\n\u2022 We propose PointDP that leverage diffusion models to purify adversarial examples. We conduct\nextensive and rigorous evaluation on six representative models with numerous attacks to compre-\nhensively understand the robustness of PointDP. Our evaluation shows that PointDP outperforms\nstate-of-the-arts puri\ufb01cation methods, IF-Defense [64] and LPC [26] by 12.6% and 40.3% on\naverage, respectively.\n2\nPointDP: Diffusion-driven Puri\ufb01cation against 3D Adversaries\nWe \ufb01rst introduce the preliminaries of diffusion models and then propose PointDP that \ufb01rst introduces\nnoise to the adversarial 3D point clouds, followed by the forward process of diffusion models to get\ndiffused point clouds. Puri\ufb01ed point clouds are recovered through the reverse process (\u00a7-2.2). Next,\nwe follow [37] to apply the adjoint method to backward propagate through SDE for ef\ufb01cient gradient\nevaluation with strong adaptive attacks (\u00a7 2.3).\n2.1\nPreliminaries\nIn this section, we brie\ufb02y review the background of diffusion models in 3D vision tasks. As mentioned\nin \u00a7 1, diffusion models involve the forward and reverse processes.\nGiven a clean point cloud sampled from the unknown data distribution x(0) \u223cq(x), the forward\nprocess of the diffusion model leverages a \ufb01xed Markov chain to gradually add Gaussian noise to the\nclean point cloud x0 over a pre-de\ufb01ned T time steps, resulting in a number of noisy point clouds\nx(1), x(2), ..., x(T). Mathematically, the forward process is de\ufb01ned as\nq(x(1 : T)|x(0)) :=\nT\nY\nt=1\nq(x(t)|x(t \u22121)),\nq(x(t)|x(t \u22121)) := N(x(t);\np\n1 \u2212\u03b2(t)x(t \u22121), \u03b2(t)I)\n(1)\nwhere \u03b2(t) is a scheduling function of the added Gaussian noise (e.g., \u03b2(t) = \u03b2min+(\u03b2max\u2212\u03b2min)t).\nThe reverse process, in contrast, is trained to recover the diffused point cloud in an iterative manner.\n3D Point clouds have less semantics than 2D images due to the lack of texture information. Therefore,\n2\nFigure 1: Illustration of PointDP. We leverage [31] as the diffusion model in our study.\npoint cloud diffusion models leverage a separate encoder e to as a latent feature zx = e(x) as a\ncondition to help recover the clean point cloud.\np\u03b8(x(0 : T)|z) := p(x(T))\nT\nY\nt=1\np\u03b8(x(t \u22121)|x(t), z),\np\u03b8(x(t \u22121)|x(t), z) := N(x(t \u22121)|\u00b5\u03b8(x(t), t, z), \u03b2(t)I)\n(2)\nwhere \u00b5\u03b8 denotes the approximated mean value parameterized by a neural network, and z = e(xa).\nThe training objective is to learn the variational bound of the negative log-likelihood [31]. In practice,\nwe jointly train the encoder e with \u00b5\u03b8. Essentially, the sampling process is similar to the DDPM\nmodel [12]:\nx(t \u22121) =\n1\np\n\u03b1(t)\n(x(t) \u2212\n1 \u2212\u03b1(t)\nq\n1 \u2212\u03b1(t)\n\u03f5\u03b8(x(t), t, z))\n(3)\nwhere \u03b1(t) = Qt\ns=1 \u03b1(s). Point cloud diffusion models have recently achieved SOTA performance\non generating and autoencoding 3D point clouds, which provides us with opportunities for adversarial\npoint cloud puri\ufb01cation.\n2.2\nDesign of PointDP\nFigure 1 illustrates the pipeline of PointDP. Nie et al. [37] have shown that diffusion-driven puri\ufb01-\ncation is able to remove the adversarial effect for 2D images. As mentioned in \u00a7 2.1, conditional\ndiffusion models were proposed in the 3D point cloud space. Speci\ufb01cally, we use the design in [31] as\nthe base model for the puri\ufb01cation process in our study. Note that we do not aiming at designing new\npoint cloud diffusion models, but instead propose a novel puri\ufb01cation pipeline along with rigorous\nevaluation as our main contributions.\nLet xa be an adversarial example w.r.t. the pristine classi\ufb01er f, we initialize the input of the forward\ndiffusion process as xa, i.e., x(t = 0) = xa. The forward diffusion process can be solved by\nEquation 4 from t = 0 to t = t\u2217\nx(t\u2217) =\np\n\u03b1(t\u2217)xa +\np\n1 \u2212\u03b1(t\u2217)\u03f5\n(4)\nwhere \u03b1(t) = e\u2212\nR t\n0 \u03b2(s)ds, \u03f5 \u223cN(0, I). We leverage Equation 3 to recover the clean point clouds.\nEquivalently, the reverse can be also solved by the SDE solver in [37], noted as: sdeint:\n\u02c6\nx(0) = sdeint(x(t\u2217), frev, grev, w, t\u2217, 0)\n(5)\nwhere the six inputs are initial value, drift coef\ufb01cient, diffusion coef\ufb01cient, Wiener process, initial\ntime, and end time [37].\nfrev(x, t) = \u22121\n2\u03b2(t)[x + 2s\u03b8(x, t)]\ngrev(t) =\np\n\u03b2(t)\n(6)\n3\nwhere score function s\u03b8 is derived from s\u03b8(x, t) = \u2212\n1\n\u221a\n1\u2212\u03b1(t)\u03f5\u03b8(x(t), t, z) [23].\nBesides, the hyper-parameter t\u2217and T trades off the denoising performance and ef\ufb01ciency. We\nempirically choose t\u2217= 10 and T = 200 in our study, which has shown satisfactory results in our\nevaluation (\u00a7 4).\n2.3\nAdpative Attacks on PointDP\nPointDP is a pre-processing module that puri\ufb01es the adversarial perturbations. [2] have shown that\ninput transformation-based methods can be broken by speci\ufb01cally designed attacks. Therefore, it\nis essential to model the adaptive attacks on PointDP to demonstrate its lower-bound adversarial\nrobustness. We thus formulate two types of adaptive attacks on PointDP.\nAttack on Latent Feature. As PointDP utilizes conditional diffusion models for adversarial puri\ufb01ca-\ntion, the latent feature z is a good candidate for adversaries to launch attacks. Concretely, adversaries\ncan set the goal to maximize some distance metric D between the latent feature of the optimized\nadversarial examples and the oracle latent feature of clean inputs zoracle. Without loss of generality,\nthe adaptive attacks can be formulated as:\nxs+1 = Projx+S(xs + \u03b1 \u00b7 norm(\u2207xsD(e(xs), zoracle))),\n(7)\nwhere xs denotes the adversarial examples from the s-th step, Proj is the function to project the\nadversarial examples to the pre-de\ufb01ned space S, and \u03b1 is the attack step size. We choose two distance\nmetrics in our study, where the \ufb01rst one is the KL divergence [17] and the other is the the \u21131 norm\ndistance. In our evaluation (\u00a7 4), we report the lowest accuracy achieved under attacks with two\ndistance metrics.\nAttack Using BPDA. We follow [37] to formulate the adaptive attack as an augmented SDE process.\nWe re-state the attack formulation as below. For the SDE in Equation 5, the augmented SDE that\ncomputes the gradient\n\u2202L\n\u2202x(t\u2217) of backward propagating through it is given by:\n \nx(t\u2217)\n\u2202L\n\u2202x(t\u2217)\n!\n=sdeint\n  \n\u02c6x(0)\n\u2202L\n\u2202\u02c6x(0)\n!\n, \u02dcf, \u02dcg, \u02dcw, 0, t\u2217\n!\n(8)\nwhere\n\u2202L\n\u2202\u02c6x(0) is the gradient of the objective L w.r.t. the output \u02c6x(0) of the SDE in Equatrion 5), and\n\u02dcf([x; z], t) =\n \nfrev(x, t)\n\u2202frev(x,t)\n\u2202x\nz\n!\n\u02dcg(t) =\n\u0012\n\u2212grev(t)1d\n0d\n\u0013\n\u02dcw(t) =\n\u0012\n\u2212w(1 \u2212t)\n\u2212w(1 \u2212t)\n\u0013\nwhere 1d and 0d denote the d-dimensional vectors of all ones and all zeros, respectively. Nie et\nal. [37] have demonstrated that such approximation align well with the true gradient value. Therefore,\nwe leverage this adaptive attack formulation for our evaluation.\n3\nRelated Work\nIn this section, we review the current progress of deep learning, adversarial attacks, and defenses for\n3D point cloud recognition tasks.\n3.1\nDeep Learning on 3D Point Cloud Recognition\n2D computer vision has achieved stellar progress on architectural designs of convolutional neural\nnetworks [22], followed by vision transformers [15]. However, there is currently no consensus on the\narchitecture of 3D perception models since there is no standard data format for 3D perception [52]. As\nraw data from both 3D scanners and triangular meshes can be ef\ufb01ciently transformed into point clouds,\n4\nthey are becoming the most often utilized data format in 3D perception. 3D networks at the early stage\nuse dense voxel grids for perception [58, 34, 47, 53], which discretize a point cloud to voxel cells\nfor classi\ufb01cation, segmentation, and object detection. PointNet pioneered to leverage global pooling\nhelp achieve memory-ef\ufb01cient permutation invariance in an end-to-end manner. PointNet++ [42] and\nDGCNN [60] followed up to add sophisticated local clustering operations to advance the performance.\nSparse tensors are the other direction in 3D network designs [19, 9] to use 3D convolutions to improve\n3D perception performance. PointCNN and RSCNN reformed the classic pyramid CNN to improve\nthe local feature generation [27, 30]. PointConv and KPConv designed new convolution operation for\npoint cloud learning [63, 54]. PointTransformer and PCT advanced self-attention blocks in the 3D\nspace and achieved good performance [84, 20]. Various novel local clustering operations [67, 32]\nalso show enhancements on the clean performance. In this work, we focus on PointNet, PointNet++,\nDGCNN, PCT, CurveNet, and PointMLP as our evaluation backbones since they are representative\nand widely used and achieve state-of-the-art results in point cloud recognition [1].\n3.2\nAdversarial Attacks and Defenses\nAdversarial attacks have become the main obstacle that hinder deep learning models from real-world\ndeployments, especially in safety-critical applications [16, 48, 5, 83, 82]. There are a lot of adversarial\nattacks proposed in the 2D space to break the various vision models [8, 70, 76, 73, 24, 25, 71, 51]. To\n\ufb01ll this gap between standard and robust accuracies, many mitigation solutions have been studied and\npresented to improve the robustness against adversarial attacks [77, 75, 4, 39, 35, 79, 69, 80, 68] and\n3D domains [14, 87, 50]. However, most of them including adding randomization [29, 13, 14], model\ndistillation [39], adversarial detection [35], and input transformation [77, 75, 38, 4, 87] have been\ncompromised by adaptive attacks [50, 55, 2]. Adversarial training (AT) [33, 18, 62, 44], in contrast,\ndelivered a more longstanding mitigation strategy [72, 74, 81]. However, the robust accuracy achieved\nby AT is still not satisfactory enough to be used in practice. Most recently, Nie et al. proposed\nDiffPure [37] that leverages diffusion models to defend against adversarial attacks, and following-up\nstudies to extend it to certi\ufb01ed defenses [7].\nAdversarial attacks and defenses also extend to 3D point clouds. Xiang et al. [70] \ufb01rst demonstrated\nthat point cloud recognition models are vulnerable to adversarial attacks. They also introduced\ndifferent threat models like point shifting and point adding attacks. Wen et al. [61] enhanced the loss\nfunction in C&W attack to achieve attacks with smaller perturbations and Hamdi et al. [21] presented\ntransferable black-box attacks on point cloud recognition. Zhou et al. [87] and Donget al. [14]\nproposed to purify the adversarial point clouds by input transformation and adversarial detection.\nHowever, these methods have been successfully by [50] through adaptive attacks. Moreover, Liu et\nal. [29] made a preliminary investigation on extending countermeasures in the 2D space to defend\nagainst simple attacks like FGSM [18] on point cloud data. Sun et al. [49] conducted a more\nthorough study on the application of self-supervised learning in adversarial training for 3D point\nclodu recognition. Besides adversarial training, advanced puri\ufb01cation methods IF-Defense [64]\nand LPC [26] were proposed to transform the adversarial examples to the clean manifold. In this\nwork, we present PointDP, that utilizes 3D diffusion models to purify adversarial point clouds. We\nalso demonstrate that standard adversarial training suffer from strong black-box attacks and SOTA\npuri\ufb01cation methods (i.e., IF-Defense and LPC) are vulnerable to PGD-styled adversaries (\u00a7 4.3).\n4\nExperiments and Results\nIn this section, we \ufb01rst introduce our experimental setups (\u00a7 4.1). We then present the standard\nrobustness evaluation of PointDP(\u00a7 4.2). We next show that how the SOTA adversarial training and\nadversarial puri\ufb01cation methods fail under various strong attacks (\u00a7 4.3). We \ufb01nally conduct stress\ntest on PointDP to show its actual robustness under various stronger adaptive attacks (\u00a7 4.4).\n4.1\nExperimental Setups\nDatasets and Network Architectures. We conduct all the experiments on the widely used Model-\nNet40 point cloud classi\ufb01cation benchmark [65], consisting of 12,311 CAD models from 40 arti\ufb01cial\nobject categories. We adopt the of\ufb01cial split with 9,843 samples for training and 2,468 for testing. We\nalso uniformly sample 1024 points from the surface of each object and normalize them into an edge-\nlength-2 cube, following most of the prior arts [41]. As mentioned before, there are various backbones\n5\nfor 3D point cloud recognition in the literature. To demonstrate the universality of PointDP, we\nselect six representative model architectures including PointNet [41], PointNet++ [42], DGCNN [60],\nPCT [20], CurveNet [67], and PointMLP [32]. These backbones either have representative designs\n(e.g., Transformer and MLP) or achieve SOTA performance on the ModelNet40 benchmark.\nAdversarial Attacks. As brie\ufb02y described in \u00a7 3.2, adversarial attacks could be roughly categorized\ninto C&W- and PGD-styled attacks. C&W attacks involves the perturbation magnitude into the\nobjective term of the optimization procedure, while PGD attacks set the perturbation magnitude as\na \ufb01rm constraint in the optimization procedure. Moreover, adversarial attacks by \u2113p norm as the\ndistance metric for the perturbation. Although a number of attacks measure Chamfer and Handoff\n\u201cdistances\u201d in 3D point cloud [66], they are not formal distance metrics as they do not satisfy the\ntriangular inequality. Therefore, we still leverage \u21132 and \u2113\u221e, following most defense studies in\nboth 2D and 3D vision tasks [8, 49]. We also have designed adaptive attacks on our proposed\nmethod \u00a7 2.3. Besides naive C&W and PGD attacks, we leverage speci\ufb01c attacks designed to\nbreak the robustness of point cloud recognition such as kNN [56] and AdvPC [21]. We also apply\nstrong adaptive AutoAttack [11] (i.e., APGD) in our evaluation. Moreover, we use SPSA [57] and\nNattack [28] as black-box adversaries, followed by the suggestion of Carlini et al. [6]. We also\nleverage EOT-AutoAttack. Point adding (PA) and dropping/detaching (PD) attacks are also evaluated\nin our study, followed by the setups in [49]. We set the attack steps to 200 to maximize the adversarial\ncapability and follow the settings in [49] for other attack parameters by default.\nEvaluation Metrics. We leverage two main metrics to evaluate the performance of our defense\nproposal, which are standard and robust accuracy. The standard accuracy measures the performance\nof the defense method on clean data, which is evaluated on the whole test set from ModelNet40. The\nrobust accuracy measures the performance on adversarial examples generated by different attacks.\nBecause of the high computational cost of applying adaptive and black-box attacks to our method,\nwe evaluate robust accuracy for our defense on a \ufb01xed subset of 128 point clouds randomly sampled\nfrom the test set. Notably, robust accuracies of most baselines do not change much on the sampled\nsubset, compared to the whole test set. We evaluate the robust accuracy on the whole test set for other\nadversarial attacks with acceptable overhead (e.g., C&W and PGD attacks).\nTable 1: Evaluation Results of Plain Model on PA and PD (Accuracy %). Models under other attacks\nmostly have 0% accuracy, so we do not present them here.\nPointNet\nPointNet++\nDGCNN\nPCT\nCurveNet\nPointMLP\nNone\n90.1\n92.8\n92.5\n92.8\n93.2\n93.5\nPA\n44.1\n19.9\n35.1\n20.8\n48.9\n7.2\nPD\n33.3\n69.8\n64.5\n53.0\n72.6\n71.1\nBaseline. Without any defense applied to the original recognition models, the robust accuracy is\nmostly 0 for all models under \u21132 and \u2113\u221ebased attacks. DGCNN exceptionally achieves 64% on\n\u21132-based PGD, AutoAttack, respectively, due to its dynamic clustering design, which adaptively\ndiscards outlier points. PA and PD are two weaker attacks and Table 1 presents the robust accuracy\nagainst these two attacks.\n4.2\nExperiment Results of PointDP\nIn this section, we \ufb01rst present the evaluation results of PointDPunder attacks on the plain models.\nWe train the diffusion and 3D point cloud recognition models in a sequential order. Table 2 presents\nthe detailed results of PointDP against attacks on six models. We \ufb01nd that PointDP overall achieves\nsatisfactory results across all models and attacks. The average robust accuracy against adversarial\nattacks is above 75%. We observe a drop on the clean accuracy for the chosen models, which is\nexpected. As mentioned before, diffusion models for 3D point cloud is a more dif\ufb01cult task than 2D\nimage diffusion, which may lead to partial semantic loss. The average drop of standard accuracy is\n4.9%. We \ufb01nd that DGCNN still achieves the best robustness combined with PointDP, which has a\n79.9% of robust accuracy. We further compare the performance of PointDPwith adversarial training,\nIF-Defense, and LPC in the next section.\n4.3\nFailure of State-of-the-Art Defenses\n6\nTable 2: Evaluation Results of Adversarial Attacks on PointDP (Accuracy %).\nPointNet\nPointNet++\nDGCNN\nPCT\nCurveNet\nPointMLP\nNone\n86.8\n87.9\n86.9\n87.0\n88.0\n88.2\n\u2113\u221e\n\u03f5 = 0.05\nC&W\n77.9\n78.6\n78.9\n76.8\n73.1\n76.2\nPGD\n78.1\n80.6\n80.3\n77.2\n74.8\n79.8\nAdvPC\n69.7\n76.6\n79.1\n79.4\n72.6\n75.2\nPA\n82.1\n85.1\n84.8\n85.5\n86.3\n85.8\n\u21132\n\u03f5 = 1.25\nC&W\n82.4\n82.9\n81.9\n80.9\n81.5\n82.6\nPGD\n80.1\n75.0\n74.6\n72.0\n71.7\n76.4\nAdvPC\n69.1\n76.3\n79.0\n74.2\n74.1\n75.6\nkNN\n83.5\n82.9\n83.3\n82.3\n81.5\n83.1\n\u21130\n\u03f5 = 200\nPD\n68.9\n74.1\n77.3\n76.3\n76.8\n77.4\nFigure 2: Compare among SOTA Adversarial Puri\ufb01cation Strategies (i.e., IF-Defense [64], LPC [26],\nand PointDP). The results of IF-Defense and PointDP are averaged from six models.\nIn this section, we demonstrate how lately proposed defense solutions fail when encountered with\nstronger (adaptive) adversarial attacks on 3D point cloud recognition models.\nTable 3: Evaluation Results of Standard Adversar-\nial Training (Accuracy %) \u2113\u221e\u03f5 = 0.05.\nPointNet\nDGCNN\nPCT\nNone\n87.8\n90.6\n89.7\nPGD\n52.1\n67.4\n51.3\nAutoAttack\n40.5\n56.4\n47.2\nSPSA\n56.7\n7.8\n11.4\nNattack\n55.1\n5.4\n6.5\nAdversarial training (AT) has been applied to\nPointNet, DGCNN, and PCT with the help of\nself-supervised learning [49] that achieves sat-\nisfactory robustness. Such observations are con-\nsistent with the performance of AT for 2D per-\nception models. However, we \ufb01nd that AT is, in\nfact, a weak defense solution in 3D perception\nmodels. First, as acknowledged by [49], point\ncloud models (e.g., PointNet++ and CurveNet)\noften leverage different sampling strategies to\nselect anchor points, like furthest point sampling\n(FPS). Such sampling involves high randomness. AT either cannot converge with different random\nseeds in each iteration or over\ufb01ts to a single random seed. Therefore, AT cannot \ufb01t these models.\nMoreover, we discover that the kNN layers will cause severe gradient obfuscation in point cloud\nmodels as well. Different from 2D models that are almost fully differentiable, except for the max\npooling layer. As shown in Figure 3, kNN essentially applies top-k. Therefore, gradient backward\npropagation through kNN layers is indexing, which is not non-smooth. The heavy usage of kNN\nlayers in DGCNN and PCT will drastically hinder the gradient \ufb02ow. As mentioned in \u00a7 4.1, we\nexploit black-box SPSA and Nattack to validate our \ufb01ndings. Table 3 presents the results of AT. SPSA\nand Nattack can greatly lower the average robust accuracy (7.8%) than white-box attacks (55.6%) on\nDGCNN and PCT, which con\ufb01rms the effect of gradient obfuscation. PointNet, however, achieves\nbetter robustness under black-box attacks because it only has one max pooling layer and does not\nemploy kNN layers.\nExisting puri\ufb01cation-based defenses against 3D adversarial point clouds mainly leverage C&W-\nstyled attacks in their evaluation. C&W attacks utilize the method of Lagrange multipliers to \ufb01nd\ntractable adversarial examples while minimizing the magnitudes of the perturbation. From the\nperspective of adversary, such attacks are desirable due to their stealthiness, while this does not hold\nfrom a defensive view. Defense methods should be evaluated against strong adaptive attacks [6].\nIF-Defense and LPC are the SOTA adversarial puri\ufb01cation methods for 3D point cloud models.\n7\n1 def knn(x, k):\n2\ninner =\n-2*torch.matmul(x.transpose (2, 1), x)\n3\nxx = torch.sum(x**2, dim=1, keepdim=True)\n4\npairwise_distance = -xx - inner - xx.transpose (2, 1)\n5\nidx = pairwise_distance .topk(k=k, dim=-1)[1]\n6\n# (batch_size , num_points , k)\n7\nreturn idx\n8\n9 def\nget_graph_feature (x, k):\n10\n# x\u2019s shape is (batch_size , num_dims , num_points)\n11\nidx = knn(x, k=k)\n# (batch_size , num_points , k)\n12\n...... # shape\ntransformation\nhere\n13\nfeature = x.view(batch_size*num_points ,\n-1)[idx , :]\n14\n# idx is used as index to select\nfeatures , leading to gradient\nobfuscation\n15\n......\n16\nreturn\nfeature\n17\n18 # forward\nfunction\nfor\nEdgeConv\n19 def\nforward(self , x):\n20\n......\n21\nx = get_graph_feature (x, k=self.k) # kNN\nhyperparameter\n22\nx = self.conv1(x) # convolution\n23\nx1 = x.max(dim=-1, keepdim=False)[0] # max\npooling\n24\n......\nFigure 3: PyTorch [40]-Style Code Snippet of EdgeConv [60] in Point Cloud Recognition Models.\nAdversarial training fails since the kNN layers leverage the top-k function where the gradient\npropagate to the index, resulting in gradient obfuscation.\nTable 4: Evaluation Results of Adversarial Attacks on IF-Defense (Accuracy %).\nPointNet\nPointNet++\nDGCNN\nPCT\nCurveNet\nPointMLP\nONet\nNone\n90.0\n92.8\n92.4\n92.8\n93.1\n93.5\n\u2113\u221e\n\u03f5 = 0.05\nPGD\n69.9\n74.0\n61.0\n54.1\n51.9\n61.6\nAdvPC\n69.4\n72.8\n61.6\n53.9\n53.6\n62.5\n\u21132\n\u03f5 = 1.25\nPGD\n74.2\n77.5\n70.5\n67.2\n68.7\n70.5\nAdvPC\n69.0\n72.9\n63.0\n64.5\n55.4\n67.9\nConvONet\nNone\n90.1\n92.8\n92.5\n92.8\n93.2\n93.5\n\u2113\u221e\n\u03f5 = 0.05\nPGD\n66.4\n73.2\n52.9\n46.8\n45.3\n55.7\nAdvPC\n63.7\n71.2\n55.5\n47.2\n46.7\n55.0\n\u21132\n\u03f5 = 1.25\nPGD\n72.2\n76.7\n69.8\n65.6\n62.7\n71.4\nAdvPC\n63.4\n74.3\n56.6\n59.8\n47.2\n71.0\nWe leverage PGD and AdvPC attacks, which assign constant adversarial budget in the adversarial\noptimization stage. We follow the original setups of IF-Defense and LPC in our study. Such\nevaluation is stronger than C&W attacks, while we note that they are not strict adaptive attacks since\nthe adversarial target is still the classi\ufb01er itself. Similar to PointDP , IF-Defense can be pre-pended to\nany point cloud classi\ufb01er, but LPC uses a speci\ufb01c backbone. Table 4 presents the detailed evaluation\nresults of IF-Defense under various settings and attacks. We \ufb01nd that PointDP achieves much better\nrobustness than IF-Defense, which is on average an 12.6% improvements. However, IF-Defense\nachieves slightly higher clean accuracy (4.9%). This is because IF-Defense leverages SOR to smooth\nthe point cloud [87]. However, such an operation has been demonstrated to be vulnerable [50]. With\nspeci\ufb01c adaptive attacks, there will be a even larger drop of robust accuracy for IF-Defense.\nFigure 2 shows the comparison among PointDP and existing methods. PointDP overall achieves the\nbest performance than prior arts, which are 12.6% and 40.3% improvements than IF-Defense and\nLPC, respectively. We \ufb01nd that even without adaptive attacks, adversaries with constant budgets can\nalready hurt the robust accuracy by a signi\ufb01cant margin. This suggests that IF-Defense and LPC\nfail to deliver strong robustness to 3D point cloud recognition models. Especially, LPC appears in\n8\nthe proceedings of CVPR 2022, but actually achieves trivial robustness, indicating that a rigorous\nbenchmarking is highly required in this community.\n4.4\nDefense against Adaptive Threats\nWe have so far illustrated that state-of-the-art defenses can be easily broken by (adaptive) adversarial\nattacks and PointDP consistently achieves the best robustness. In this section, we further extensively\nevaluate the robustness of PointDP on even stronger adaptive attacks to demonstrate the actual\nrobustness realized by PointDP. As mentions in \u00a7 4.1, we leverage two types of adaptive attacks\nin our study, and Table 5 presents their results. We also leverage black-box SPSA and Nattack to\nvalidate our results. We \ufb01nd that BPDA-PGD the strongest adaptive attacks, which align well with\nprevious study on 2D diffusion-driven puri\ufb01cation [37]. Even though with strong adaptive attacks,\nPointDP still achieves much better robustness. Besides, black-box attacks are much less effective.\nAlthough we admit that PointDP still relies on gradient obfuscation, the extremely high randomness\nwill hinder the black-box adversaries \ufb01nding correct gradients.\nTable 5: Evaluation Results of Strong Adaptive Attacks on PointDP (Accuracy %).\nPointNet\nPointNet++\nDGCNN\nPCT\nCurveNet\nPointMLP\nNone\n86.8\n87.9\n86.9\n87.0\n88.0\n88.2\n\u2113\u221e\n\u03f5 = 0.05\nBPDA-PGD\n77.1\n78.6\n79.2\n76.1\n73.9\n77.7\nEOT-AutoAttack\n78.0\n79.9\n79.1\n76.5\n75.9\n78.9\nPGD\n80.8\n80.7\n82.9\n82.5\n80.8\n79.9\nAdvPC\n69.9\n76.8\n79.4\n79.8\n72.9\n75.4\nSPSA\n76.6\n78.9\n74.9\n78.5\n76.4\n80.9\nNattack\n75.2\n77.9\n74.4\n78.0\n76.1\n78.9\nPA\n81.7\n84.7\n84.1\n84.5\n84.8\n85.2\n\u21132\n\u03f5 = 1.25\nBPDA-PGD\n78.9\n73.3\n73.3\n71.2\n70.7\n75.1\nEOT-AutoAttack\n79.6\n74.4\n74.2\n71.3\n71.3\n75.9\nPGD\n86.1\n87.5\n82.5\n86.3\n87.7\n87.8\nAdvPC\n69.1\n76.9\n79.2\n74.5\n74.3\n76.1\nSPSA\n76.1\n77.0\n74.4\n74.5\n77.0\n78.9\nNattack\n74.9\n76.5\n73.9\n74.0\n76.3\n77.2\n\u21130\n\u03f5 = 200\nPD\n61.3\n72.1\n73.5\n75.9\n74.1\n74.4\n5\nDiscussion\nAdversarial robustness has been well-established in 2D vision tasks, where Carlini et al. [6] and\nmany other researchers have devoted signi\ufb01cant efforts to set up a rigorous evaluation protocol. In\nthis study, we also emphasize that this evaluation protocol should be followed in 3D point cloud\nrobustness study. Counter-intuitively, we have demonstrated that standard adversarial training is\nnot a good candidate to deliver robustness against strong black-box adversaries because of gradient\nobfuscation. We propose PointDP as an adversarial puri\ufb01cation strategy to mitigate the robustness\nloss in the 3D space. We would like to clarify that almost all puri\ufb01cation methods (including PointDP)\nstill depend on gradient obfuscation. However, we argue that proper usage of gradient obfuscation\ncould still serve as a good defense, as long as the obfuscation is sophisticated enough. The multi-step\npuri\ufb01cation in diffusion models adds extremely high-level randomness that EOT [3] and BPDA [2]\nattacks are hard to model. Therefore, we believe our extensive evaluation reveals the actual robustness\nof PointDP.\nBroader Impacts and Limitations. Mitigation solutions to adversarial attacks are critical and\nessential for modern machine learning systems. Given that 3D point cloud is heavily adopted in\nsafety-critical applications, we believe our study is valuable in demonstrating the vulnerabilities\nof existing SOTA defenses. PointDP also. On the other hand, diffusion models needs multiple\nsteps in the reverse process to recover the point cloud and hinder adaptive attacks, which will\nincur additional computational overhead. PointDP also limits itself to empirical robustness without\ntheoretical guarantees. Therefore, we cannot exclude possibilities that PointDP could be broken by\nfuture stronger attacks. We plan to include certi\ufb01ed defense (i.e., randomized smoothing [10]) into\nour framework in the near future.\n9\n6\nConclusion\nIn this paper, we propose PointDP, an adversarial puri\ufb01cation method against attacks on 3D point\ncloud recognition. We have \ufb01rst demonstrated that adversarial training and prior puri\ufb01cation methods\nare actually vulnerable to strong attacks. We further leverage extensive evaluation to validate that\nPointDP outperforms existing SOTA methods by a signi\ufb01cant margin in robust accuracy.\nReferences\n[1] 3D Point Cloud Classi\ufb01cation Benchmark on ModelNet40. https://paperswithcode.com/\nsota/3d-point-cloud-classification-on-modelnet40, 2021.\n[2] A. Athalye, N. Carlini, and D. Wagner. Obfuscated gradients give a false sense of security:\nCircumventing defenses to adversarial examples. In International Conference on Machine\nLearning, pages 274\u2013283. PMLR, 2018.\n[3] A. Athalye, L. Engstrom, A. Ilyas, and K. Kwok. Synthesizing robust adversarial examples. In\nJ. Dy and A. Krause, editors, Proceedings of the 35th International Conference on Machine\nLearning, volume 80 of Proceedings of Machine Learning Research, pages 284\u2013293. PMLR,\n10\u201315 Jul 2018.\n[4] M. Bafna, J. Murtagh, and N. Vyas. Thwarting adversarial examples: An l_0-robustsparse\nfourier transform. arXiv preprint arXiv:1812.05013, 2018.\n[5] Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen, K. Fu, and Z. M. Mao.\nAdversarial sensor attack on lidar-based perception in autonomous driving. In Proceedings of the\n2019 ACM SIGSAC conference on computer and communications security, pages 2267\u20132281,\n2019.\n[6] N. Carlini, A. Athalye, N. Papernot, W. Brendel, J. Rauber, D. Tsipras, I. Goodfellow, A. Madry,\nand A. Kurakin. On evaluating adversarial robustness. arXiv preprint arXiv:1902.06705, 2019.\n[7] N. Carlini, F. Tramer, J. Z. Kolter, et al. (certi\ufb01ed!!) adversarial robustness for free! arXiv\npreprint arXiv:2206.10550, 2022.\n[8] N. Carlini and D. Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee\nsymposium on security and privacy (sp), pages 39\u201357. IEEE, 2017.\n[9] C. Choy, J. Gwak, and S. Savarese. 4d spatio-temporal convnets: Minkowski convolutional\nneural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 3075\u20133084, 2019.\n[10] J. Cohen, E. Rosenfeld, and Z. Kolter. Certi\ufb01ed adversarial robustness via randomized smooth-\ning. In International Conference on Machine Learning, pages 1310\u20131320. PMLR, 2019.\n[11] F. Croce and M. Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse\nparameter-free attacks. In International Conference on Machine Learning, pages 2206\u20132216.\nPMLR, 2020.\n[12] P. Dhariwal and A. Nichol. Diffusion models beat gans on image synthesis. Advances in Neural\nInformation Processing Systems, 34:8780\u20138794, 2021.\n[13] G. S. Dhillon, K. Azizzadenesheli, Z. C. Lipton, J. Bernstein, J. Kossai\ufb01, A. Khanna, and\nA. Anandkumar. Stochastic activation pruning for robust adversarial defense. arXiv preprint\narXiv:1803.01442, 2018.\n[14] X. Dong, D. Chen, H. Zhou, G. Hua, W. Zhang, and N. Yu. Self-robust 3d point recognition\nvia gather-vector guidance. In 2020 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11513\u201311521. IEEE, 2020.\n[15] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani,\nM. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[16] K. Eykholt, I. Evtimov, E. Fernandes, B. Li, A. Rahmati, C. Xiao, A. Prakash, T. Kohno, and\nD. Song. Robust physical-world attacks on deep learning visual classi\ufb01cation. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern Recognition, pages 1625\u20131634, 2018.\n10\n[17] J. Goldberger, S. Gordon, H. Greenspan, et al. An ef\ufb01cient image similarity measure based on\napproximations of kl-divergence between two gaussian mixtures. In ICCV, volume 3, pages\n487\u2013493, 2003.\n[18] I. J. Goodfellow, J. Shlens, and C. Szegedy. Explaining and harnessing adversarial examples.\narXiv preprint arXiv:1412.6572, 2014.\n[19] B. Graham and L. van der Maaten. Submanifold sparse convolutional networks. arXiv preprint\narXiv:1706.01307, 2017.\n[20] M.-H. Guo, J.-X. Cai, Z.-N. Liu, T.-J. Mu, R. R. Martin, and S.-M. Hu. Pct: Point cloud\ntransformer. arXiv preprint arXiv:2012.09688, 2020.\n[21] A. Hamdi, S. Rojas, A. Thabet, and B. Ghanem. Advpc: Transferable adversarial perturbations\non 3d point clouds. In European Conference on Computer Vision, pages 241\u2013257. Springer,\n2020.\n[22] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In\nProceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013\n778, 2016.\n[23] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural\nInformation Processing Systems, 33:6840\u20136851, 2020.\n[24] L. Huang, C. Gao, Y. Zhou, C. Xie, A. Yuille, C. Zou, and N. Liu. Universal physical camou\ufb02age\nattacks on object detectors, 2019.\n[25] L. Huang, C. Gao, Y. Zhou, C. Xie, A. L. Yuille, C. Zou, and N. Liu. Universal physical\ncamou\ufb02age attacks on object detectors. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 720\u2013729, 2020.\n[26] K. Li, Z. Zhang, C. Zhong, and G. Wang. Robust structured declarative classi\ufb01ers for 3d point\nclouds: Defending adversarial attacks with implicit gradients. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pages 15294\u201315304, 2022.\n[27] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, and B. Chen. Pointcnn: Convolution on x-transformed\npoints. Advances in neural information processing systems, 31:820\u2013830, 2018.\n[28] Y. Li, L. Li, L. Wang, T. Zhang, and B. Gong. Nattack: Learning the distributions of adver-\nsarial examples for an improved black-box attack on deep neural networks. In International\nConference on Machine Learning, pages 3866\u20133876. PMLR, 2019.\n[29] D. Liu, R. Yu, and H. Su. Extending adversarial attacks and defenses to deep 3d point cloud\nclassi\ufb01ers. In 2019 IEEE International Conference on Image Processing (ICIP), pages 2279\u2013\n2283. IEEE, 2019.\n[30] Y. Liu, B. Fan, S. Xiang, and C. Pan. Relation-shape convolutional neural network for point\ncloud analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 8895\u20138904, 2019.\n[31] S. Luo and W. Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings\nof the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2837\u20132845,\n2021.\n[32] X. Ma, C. Qin, H. You, H. Ran, and Y. Fu. Rethinking network design and local geometry in\npoint cloud: A simple residual mlp framework. arXiv preprint arXiv:2202.07123, 2022.\n[33] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu. Towards deep learning models\nresistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\n[34] D. Maturana and S. Scherer. Voxnet: A 3d convolutional neural network for real-time object\nrecognition. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems\n(IROS), pages 922\u2013928. IEEE, 2015.\n[35] D. Meng and H. Chen. Magnet: a two-pronged defense against adversarial examples. In\nProceedings of the 2017 ACM SIGSAC conference on computer and communications security,\npages 135\u2013147, 2017.\n[36] A. Q. Nichol and P. Dhariwal. Improved denoising diffusion probabilistic models. In Interna-\ntional Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\n[37] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar. Diffusion models for\nadversarial puri\ufb01cation. arXiv preprint arXiv:2205.07460, 2022.\n11\n[38] N. Papernot and P. McDaniel. Extending defensive distillation. arXiv preprint arXiv:1705.05264,\n2017.\n[39] N. Papernot, P. McDaniel, X. Wu, S. Jha, and A. Swami. Distillation as a defense to adversarial\nperturbations against deep neural networks. In 2016 IEEE symposium on security and privacy\n(SP), pages 582\u2013597. IEEE, 2016.\n[40] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,\nN. Gimelshein, L. Antiga, et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. Advances in neural information processing systems, 32, 2019.\n[41] C. R. Qi, H. Su, K. Mo, and L. J. Guibas. Pointnet: Deep learning on point sets for 3d\nclassi\ufb01cation and segmentation. In Proceedings of the IEEE conference on computer vision and\npattern recognition, pages 652\u2013660, 2017.\n[42] C. R. Qi, L. Yi, H. Su, and L. J. Guibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. arXiv preprint arXiv:1706.02413, 2017.\n[43] G. Riegler, A. O. Ulusoy, and A. Geiger. Octnet: Learning deep 3d representations at high reso-\nlutions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,\n2017.\n[44] A. Shafahi, M. Najibi, A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S. Davis, G. Taylor, and\nT. Goldstein. Adversarial training for free! arXiv preprint arXiv:1904.12843, 2019.\n[45] S. Shi, C. Guo, L. Jiang, Z. Wang, J. Shi, X. Wang, and H. Li. Pv-rcnn: Point-voxel feature set\nabstraction for 3d object detection. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10529\u201310538, 2020.\n[46] S. Shi, X. Wang, and H. Li. Pointrcnn: 3d object proposal generation and detection from\npoint cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pages 770\u2013779, 2019.\n[47] S. Song and J. Xiao. Deep Sliding Shapes for amodal 3D object detection in RGB-D images. In\nCVPR, 2016.\n[48] J. Sun, Y. Cao, Q. A. Chen, and Z. M. Mao. Towards robust lidar-based perception in autonomous\ndriving: General black-box adversarial sensor attack and countermeasures. In 29th USENIX\nSecurity Symposium (USENIX Security 20), pages 877\u2013894. USENIX Association, Aug. 2020.\n[49] J. Sun, Y. Cao, C. B. Choy, Z. Yu, A. Anandkumar, Z. M. Mao, and C. Xiao. Adversarially\nrobust 3d point cloud recognition using self-supervisions. Advances in Neural Information\nProcessing Systems, 34:15498\u201315512, 2021.\n[50] J. Sun, K. Koenig, Y. Cao, Q. A. Chen, and Z. M. Mao. On the adversarial robustness of 3d\npoint cloud classi\ufb01cation, 2020.\n[51] J. Sun, A. Mehra, B. Kailkhura, P.-Y. Chen, D. Hendrycks, J. Hamm, and Z. M. Mao. Certi\ufb01ed\nadversarial defenses meet out-of-distribution corruptions: Benchmarking robustness and simple\nbaselines. arXiv preprint arXiv:2112.00659, 2021.\n[52] J. Sun, Q. Zhang, B. Kailkhura, Z. Yu, C. Xiao, and Z. M. Mao. Benchmarking robustness of 3d\npoint cloud recognition against common corruptions. arXiv preprint arXiv:2201.12296, 2022.\n[53] L. P. Tchapmi, C. B. Choy, I. Armeni, J. Gwak, and S. Savarese. Segcloud: Semantic segmenta-\ntion of 3d point clouds. In International Conference on 3D Vision (3DV), 2017.\n[54] H. Thomas, C. R. Qi, J.-E. Deschaud, B. Marcotegui, F. Goulette, and L. J. Guibas. Kpconv:\nFlexible and deformable convolution for point clouds.\nIn Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 6411\u20136420, 2019.\n[55] F. Tramer, N. Carlini, W. Brendel, and A. Madry. On adaptive attacks to adversarial example\ndefenses. arXiv preprint arXiv:2002.08347, 2020.\n[56] T. Tsai, K. Yang, T.-Y. Ho, and Y. Jin. Robust adversarial objects against deep learning models.\nIn Proceedings of the AAAI Conference on Arti\ufb01cial Intelligence, volume 34, pages 954\u2013962,\n2020.\n[57] J. Uesato, B. O\u2019donoghue, P. Kohli, and A. Oord. Adversarial risk and the dangers of evaluating\nagainst weak attacks. In International Conference on Machine Learning, pages 5025\u20135034.\nPMLR, 2018.\n12\n[58] D. Z. Wang and I. Posner. Voting for voting in online point cloud object detection. In Robotics:\nScience and Systems, volume 1, pages 10\u201315607. Rome, Italy, 2015.\n[59] P.-S. Wang, Y. Liu, Y.-X. Guo, C.-Y. Sun, and X. Tong. O-cnn: Octree-based convolutional\nneural networks for 3d shape analysis. ACM Transactions on Graphics (TOG), 36(4):1\u201311,\n2017.\n[60] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. Solomon. Dynamic graph\ncnn for learning on point clouds. Acm Transactions On Graphics (tog), 38(5):1\u201312, 2019.\n[61] Y. Wen, J. Lin, K. Chen, and K. Jia. Geometry-aware generation of adversarial and cooperative\npoint clouds. 2019.\n[62] E. Wong, L. Rice, and J. Z. Kolter. Fast is better than free: Revisiting adversarial training. arXiv\npreprint arXiv:2001.03994, 2020.\n[63] W. Wu, Z. Qi, and L. Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. In\nProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\n9621\u20139630, 2019.\n[64] Z. Wu, Y. Duan, H. Wang, Q. Fan, and L. J. Guibas. If-defense: 3d adversarial point cloud\ndefense via implicit function based restoration. arXiv preprint arXiv:2010.05272, 2020.\n[65] Z. Wu, S. Song, A. Khosla, F. Yu, L. Zhang, X. Tang, and J. Xiao. 3d shapenets: A deep\nrepresentation for volumetric shapes. In Proceedings of the IEEE conference on computer vision\nand pattern recognition, pages 1912\u20131920, 2015.\n[66] C. Xiang, C. R. Qi, and B. Li. Generating 3d adversarial point clouds. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9136\u20139144, 2019.\n[67] T. Xiang, C. Zhang, Y. Song, J. Yu, and W. Cai. Walk in the cloud: Learning curves for point\nclouds shape analysis. In Proceedings of the IEEE/CVF International Conference on Computer\nVision, pages 915\u2013924, 2021.\n[68] C. Xiao, R. Deng, B. Li, T. Lee, B. Edwards, J. Yi, D. Song, M. Liu, and I. Molloy. Advit:\nAdversarial frames identi\ufb01er based on temporal consistency in videos. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 3968\u20133977, 2019.\n[69] C. Xiao, R. Deng, B. Li, F. Yu, M. Liu, and D. Song. Characterizing adversarial examples based\non spatial consistency information for semantic segmentation. In Proceedings of the European\nConference on Computer Vision (ECCV), pages 217\u2013234, 2018.\n[70] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song. Generating adversarial examples with\nadversarial networks. arXiv preprint arXiv:1801.02610, 2018.\n[71] C. Xiao, J.-Y. Zhu, B. Li, W. He, M. Liu, and D. Song. Spatially transformed adversarial\nexamples. arXiv preprint arXiv:1801.02612, 2018.\n[72] C. Xie, M. Tan, B. Gong, A. Yuille, and Q. V. Le. Smooth adversarial training. arXiv preprint\narXiv:2006.14536, 2020.\n[73] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille. Adversarial examples for semantic\nsegmentation and object detection. In International Conference on Computer Vision. IEEE,\n2017.\n[74] C. Xie and A. Yuille. Intriguing properties of adversarial training at scale. In International\nConference on Learning Representations, 2020.\n[75] W. Xu, D. Evans, and Y. Qi. Feature squeezing: Detecting adversarial examples in deep neural\nnetworks. arXiv preprint arXiv:1704.01155, 2017.\n[76] C. Yang, A. Kortylewski, C. Xie, Y. Cao, and A. Yuille. Patchattack: A black-box texture-\nbased attack with reinforcement learning. In European Conference on Computer Vision, pages\n681\u2013698. Springer, 2020.\n[77] Y. Yang, G. Zhang, D. Katabi, and Z. Xu. Me-net: Towards effective adversarial robustness\nwith matrix estimation. arXiv preprint arXiv:1905.11971, 2019.\n[78] T. Yin, X. Zhou, and P. Kr\u00e4henb\u00fchl. Center-based 3d object detection and tracking. CVPR,\n2021.\n13\n[79] H. Zhang, H. Chen, C. Xiao, S. Gowal, R. Stanforth, B. Li, D. Boning, and C.-J. Hsieh.\nTowards stable and ef\ufb01cient training of veri\ufb01ably robust neural networks. arXiv preprint\narXiv:1906.06316, 2019.\n[80] H. Zhang, H. Chen, C. Xiao, B. Li, D. S. Boning, and C.-J. Hsieh. Robust deep reinforcement\nlearning against adversarial perturbations on observations. 2020.\n[81] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan. Theoretically principled\ntrade-off between robustness and accuracy. In International Conference on Machine Learning,\npages 7472\u20137482. PMLR, 2019.\n[82] Q. Zhang, S. Hu, J. Sun, Q. A. Chen, and Z. M. Mao. On adversarial robustness of trajectory\nprediction for autonomous vehicles. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 15159\u201315168, June 2022.\n[83] X. Zhang, A. Zhang, J. Sun, X. Zhu, Y. E. Guo, F. Qian, and Z. M. Mao. Emp: Edge-assisted\nmulti-vehicle perception. In Proceedings of the 27th Annual International Conference on Mobile\nComputing and Networking, pages 545\u2013558, 2021.\n[84] H. Zhao, L. Jiang, J. Jia, P. H. Torr, and V. Koltun. Point transformer. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 16259\u201316268, 2021.\n[85] T. Zheng, C. Chen, J. Yuan, B. Li, and K. Ren. Pointcloud saliency maps. In Proceedings of the\nIEEE/CVF International Conference on Computer Vision, pages 1598\u20131606, 2019.\n[86] H. Zhou, D. Chen, J. Liao, K. Chen, X. Dong, K. Liu, W. Zhang, G. Hua, and N. Yu. Lg-gan:\nLabel guided adversarial network for \ufb02exible targeted attack of point cloud based deep networks.\nIn Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npages 10356\u201310365, 2020.\n[87] H. Zhou, K. Chen, W. Zhang, H. Fang, W. Zhou, and N. Yu. Dup-net: Denoiser and upsampler\nnetwork for 3d adversarial point clouds defense. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pages 1961\u20131970, 2019.\n14\n",
    "2302.10463": "Multimodal Trajectory Prediction: A Survey\nRenhao Huang , Hao Xue , Maurice Pagnucco , Flora Salim , Yang Song\nSchool of Computer Science and Engineering, University of New South Wales, Sydney, Australia\n{renhao.huang, hao.xue1, morri, \ufb02ora.salim, yang.song1}@unsw.edu.au\nAbstract\nTrajectory prediction is an important task to sup-\nport safe and intelligent behaviours in autonomous\nsystems.\nMany advanced approaches have been\nproposed over the years with improved spatial\nand temporal feature extraction. However, human\nbehaviour is naturally multimodal and uncertain:\ngiven the past trajectory and surrounding environ-\nment information, an agent can have multiple plau-\nsible trajectories in the future. To tackle this prob-\nlem, an essential task named multimodal trajec-\ntory prediction (MTP) has recently been studied,\nwhich aims to generate a diverse, acceptable and\nexplainable distribution of future predictions for\neach agent. In this paper, we present the \ufb01rst survey\nfor MTP with our unique taxonomies and compre-\nhensive analysis of frameworks, datasets and eval-\nuation metrics.\nIn addition, we discuss multiple\nfuture directions that can help researchers develop\nnovel multimodal trajectory prediction systems.\n1\nIntroduction\nTrajectory prediction has received great attention over the\nyears for autonomous systems such as social robotics and\nself-driving cars. It aims to predict the future trajectories for\nroad users such as vehicles, pedestrians and cyclists based on\ntheir past trajectories and surrounding environments includ-\ning static factors such as terrain and obstacles and dynamic\nfactors such as surrounding moving agents.\nTraditional trajectory prediction explores physical models\nto model human behaviours where future motions are pre-\ndicted using physical models, such as Social Force [Helbing\nand Moln\u00b4ar, 1995], which describe social behaviours such as\nclustering and collision avoidance as attractive and repulsive\nforces. However, such models cannot handle complex inter-\nactions to propose human-like future predictions. More re-\ncently, learning-based models have been proposed to learn\ncomplex spatial and temporal interactions from datasets us-\ning advanced modules such as pooling [Alahi et al., 2016],\nattention [Gupta et al., 2018] and graph neural networks [Mo-\nhamed et al., 2020; Huang et al., 2019].\nTrajectory prediction is typically formulated as determin-\nistic trajectory prediction (DTP) tasks where the model is ex-\nFigure 1: An example of Multimodal Trajectory Prediction. The\nblue, green and red lines are the observed, the ground truth and other\npossible paths respectively. The agent can have multiple plausible\ntrajectories including the ground truth given an observed path.\npected to provide only one prediction for each agent. How-\never, DTP suffers heavily from the uncertainty from limited\nsocial clues and the performance is usually capped. Taking\nFig. 1 as an example, assuming that the agent is going to en-\nter the building, the observed path indicates that it may also\ncross the road or follow the sidepath. Since all scenarios are\nplausible, it is unrealistic for DTP to predict a single trajec-\ntory without suf\ufb01cient cues, such as human intentions.\nTherefore, a recent task named multimodal trajectory pre-\ndiction (MTP) is proposed by Gupta et al. [2018], where the\nmodel is expected to provide multiple predictions to cover all\nmodalities, i.e., plausible paths, of future trajectories. MTP\nis expected to handle the uncertainty of predictions and has\nbecome a default setting in almost all recent studies. Some\nmethods have focused on improving the feature extraction\nmodules, while others try to generate more diverse and so-\ncially acceptable distributions using only one ground truth\nfuture trajectory.\nIn this paper, we present the \ufb01rst survey for multimodal\ntrajectory prediction. Existing trajectory prediction surveys\nfor pedestrians [Rudenko et al., 2020] and vehicles [Teeti et\nal., 2022] all build their taxonomy from the feature extraction\nperspective and MTP is brie\ufb02y introduced as auxiliary con-\ntent. As a more realistic scenario in trajectory prediction, we\nbelieve that deeper investigations and analysis are required.\nWe review and provide our taxonomies for the frameworks,\narXiv:2302.10463v1  [cs.RO]  21 Feb 2023\nMTP\nFrameworks\nNoise-based Framework\nGAN\nGAN: SGAN [Gupta et al., 2018];InfoGAN: S-Ways [Amirian et al., 2019];\nBicycleGAN: Social-BiGAT [Kosaraju et al., 2019]; MGAN:MG-GAN [Dendorfer et al., 2021]\nConditional CVAE\nDesire [Lee et al., 2017]; AgentFormer [Yuan et al., 2021]; DisDis [Chen et al., 2021];\nABC [Halawa et al., 2022]; SocialVAE [Xu et al., 2022]\nNormalising Flow\nHBAFlow [Bhattacharyya et al., 2021]; FloMo [Sch\u00a8oller and Knoll, 2021]; STGlow [Liang et al., 2022]\nDDPM\nMID [Gu et al., 2022]\nAnchor Conditioned Framework\nPredicted Endpoint Conditioned\nPECNet [Mangalam et al., 2020]; YNet [Mangalam et al., 2021];\nTNT [Zhao et al., 2021]; DenseTNT [Gu et al., 2021]\nPrototype Trajectory Conditioned\nCoverNet [Phan-Minh et al., 2020]; MultiPath [Chai et al., 2020];\nS-Anchor [Kothari et al., 2021]; SIT [Shi et al., 2022]\nGrid-based Framework\nBeam Search: Multiverse[Liang et al., 2020]; ST-MR [Li et al., 2022]; Gumbel Sampling: TDOR [Guo et al., 2022]\nBivariate Gaussian Outputs\nSTGCNN [Mohamed et al., 2020]; MultiPath [Chai et al., 2020]\nOther Techniques\nfor Improved MTP\nAdvanced Discriminators\nFSGAN [Parth and Alexandre, 2019]; SEGAN [Huang et al., 2020];\nSafeCritics [van der Heiden et al., 2019]; SGANv2 [Kothari and Alahi, 2022]\nAdvanced Sampling Tricks\nTTST [Mangalam et al., 2021]; LDS [Ma et al., 2021]; NPSN [Bae et al., 2022]\nFigure 2: An overview of the taxonomy of MTP frameworks.\ndatasets and evaluation metrics for MTP and analyse their\nadvantages and issues. Then, we discuss potential directions\nthat should be the focus of future research.\n2\nBackground\nAgent\nAn agent in trajectory prediction is a road user with\nself-cognition such as a pedestrian, a motorist or a cyclist.\nTrajectory\nA trajectory of an agent i in trajectory predic-\ntion is de\ufb01ned as a sequence of 2D real-world or pixel co-\nordinates: {XT\ni , Y \u03c4\ni }, where XT\ni\n= {Xt\ni|t \u2208[1, Tobs]} is\nthe observed trajectory with Tobs timesteps, Y \u03c4\ni\n= {Y t\ni |t \u2208\n(Tobs, Tobs + Tpred]} is the ground truth of the future path\nwith Tpred time steps and i is the index among N agents in a\nscene i \u2208[1, N]. Both Xt\ni and Y t\ni contain 2D coordinates.\nTrajectory Prediction\nThe goal of trajectory prediction\nis to optimise a model fT P to predict K future trajecto-\nries \u02c6Y \u03c4\ni,K = { \u02c6Y t\ni,k|k = 1, \u00b7 \u00b7 \u00b7 , K} using observed informa-\ntion XT\ni , XT\n1:N\\i, S as inputs: \u02c6Y \u03c4\ni,K = fT P (XT\ni , XT\n1:N\\i, S),\nwhere XT\n1:N\\i are the set of agents i\u2019s neighbours\u2019 observed\ntrajectories and S is the scene information which can be the\nLIDAR data, high-de\ufb01nition (HD) maps, scene images, etc.\nWhen K = 1, where only a single prediction is allowed\nfor each agent, the task is deterministic trajectory prediction\n(DTP) and expects a minimum prediction error compared to\nY \u03c4\ni . Otherwise, it becomes multimodal trajectory prediction\n(MTP) and aims to predict a distribution of all acceptable fu-\nture trajectories.\nStandard Framework for DTP\nThe framework for DTP\nusually follows the sequence-to-sequence structure as shown\nin Fig. 3a, where the past encoder extracts the spatial and\ntemporal information from the observed information and the\ndecoder predicts the future path. To build a DTP model, the\npast encoder can be (1) the combination of encoding mod-\nules for temporal, social and physical features [Xue et al.,\n2018; Sadeghian et al., 2019; Dendorfer et al., 2021]; (2)\na CNN-based encoding module on raster HD maps [Wang\net al., 2020] or heatmaps [Mangalam et al., 2021]; or (3) a\ngraph neural network-based encoding module on vectorised\nHD maps [Gao et al., 2020]. The decoder can be a recurrent\nnetwork-based autoregressive module or an MLP-based or a\nCNN-based non-autoregressive module. The reconstruction\nloss such as l1 or l2 loss is used to optimise the prediction\nwith the expectation of less error to the ground truth. MTP\nmodels can also use these past encoders and decoders in their\nframeworks, except that their decoders are executed repeat-\nedly with different feature inputs.\nMultimodality in Trajectory Prediction\nGiven the ob-\nserved information, there can be multiple plausible and so-\ncially acceptable future predictions for the agent. Therefore,\nit is different from the data modality in other multimodal\nlearning tasks. It is unlikely to expect a model to predict only\none future trajectory consistently and precisely to match the\nground truth due to limited clues that can be obtained from the\nenvironment and the inherent randomisation for each move-\nment. Therefore, MTP requires the model to provide multiple\nproposals that are acceptable to humans.\n3\nFrameworks for MTP\nA \u201cgood\u201d distribution predicted from an MTP model is ex-\npected to satisfy the following aspects: (1) Diversity, where\nthe predicted distribution should cover all possible solutions;\n(2) Social Acceptance, where the predicted paths should be\nrealistic with the past trajectory and follow social norms; (3)\nExplanablity/Controllability, where each prediction should\nfollow a reasonable intention or be controlled by understand-\nable conditions. It is challenging because optimal distribu-\ntions are estimated using only one ground truth which often\nleads to less diverse and unacceptable predictions. To tackle\nthis problem, many advanced frameworks are proposed. In\nthis section, we review the MTP frameworks with their tax-\nonomy in Fig. 2 and general pipelines in Fig. 3.\n3.1\nNoise-based MTP Framework\nThe simplest way to convert DTP to MTP is to inject random\nnoise into the model. In this section, we discuss the Noise-\nbased MTP framework introduced by Gupta et al. [2018]\n(a) DTP\n(b) GAN\n(c) CVAE\n(d) Normalising Flow\n(e) PEC\n(f) PTC\n(g) Grid-based\nFigure 3: General Pipelines of DTP and MTP frameworks. The block with multiple layers is executed repeatedly with K different inputs.\nDashed blue lines and blocks are executed for training only.\nwhere features from the past encoder are concatenated with\nGaussian noise vectors and sent together into the decoder for\ndiverse predictions. The prediction is optimised by variety\nloss using the minimum reconstruction error:\nLvariety( \u02c6Y \u03c4\nK,i, Y \u03c4\ni ) = min\nk<K Lrec( \u02c6Y \u03c4\nk,i, Y \u03c4\ni ).\n(1)\nAs mentioned in Gupta et al. [2018], variety loss can suc-\ncessfully alleviate the mode collapse problem brought from\nthe reconstruction loss. Thiede and Brahma [2019] explain\nthat a learner trained with variety loss can converge to the\nsquare root of the ground truth probability density function,\nwhich further illustrates the success of this loss function.\nThis framework can be simply integrated into any DTP\nmodel and has been widely used in trajectory prediction.\nHowever, this framework may generate unrealistic predic-\ntions and is dif\ufb01cult to control. To tackle this problem, many\nadvanced generative frameworks are proposed.\nGenerative Adversarial Network (GAN)\nTo encourage\nhigh-quality predictions, a special loss function is needed to\ndiscriminate between bad and good predictions, known as ad-\nversarial loss. Gupta et al. [2018] introduce the GAN-based\nMTP framework with its \ufb01rst model S-GAN. The general\npipeline is shown in Fig. 3b, where a discriminator is learned\nto classify whether the future trajectory is the ground truth\nor the prediction and the adversarial loss is its cross entropy\nresult. Further works aim to improve the performance us-\ning more advanced GAN frameworks. For example, Kosaraju\net al. [2019] propose Social-BiGAT which follows the Bicy-\ncleGAN to perform reversible transformations between each\ngeneration and its latent noise vector to further alleviate the\nmode collapse. Concurrently, Amirian et al. [2019] claim that\nthe reconstruction loss is the main factor that causes mode\ncollapse. Hence, their model S-Way depreciates this loss and\nfollows InfoGAN to use latent code to control the prediction.\nDendorfer et al. [2021] suggest that the manifold of the dis-\ntribution of future paths is discontinued and thus cannot be\nwell covered by other GAN-based methods. Therefore, they\npropose MG-GAN by using multiple decoders, each one han-\ndling a continuous sub-manifold.\nConditional Variational Autoencoder (CVAE)\nCVAE-\nbased trajectory prediction models [Lee et al., 2017; Yuan\net al., 2021; Chen et al., 2021] follow [Sohn et al., 2015]\nthat maximise the evidence lower-bound of feature distribu-\ntion as shown in Fig. 3c and is an alternative to encourage di-\nverse predictions. In addition, the latent distribution of CVAE\ncan be better controlled and enhanced. For instance, Dis-\nDis [Chen et al., 2021] and ABC [Halawa et al., 2022] pre-\ndict personalised and action-aware motion patterns by distin-\nguishing the feature distributions via contrastive learning. A\nrecent model named SocialVAE [Xu et al., 2022] uses a time-\nwise CVAE by applying CVAE with recurrent networks. We\nstrongly recommend [Ivanovic et al., 2021] for a comprehen-\nsive review of CVAE in trajectory prediction.\nNormalising Flow (NF)\nGAN or CVAE-based models are\ndif\ufb01cult to train due to the implicit distribution modelling.\nTherefore, the NF-based MTP framework is proposed to ex-\nplicitly learn the data distribution through an invertible net-\nwork shown in Fig. 3d, which can convert a complicated\ndistribution into a tractable form via invertible transforma-\ntions. For example, HBAFlow [Bhattacharyya et al., 2021]\nuses a Haar wavelets based block autoregressive model that\nsplits couplings to learn distributions for motion prediction\nwhile FloMo [Sch\u00a8oller and Knoll, 2021] utilises monotonic\nrational-quadratic splines for expressive and fast inversion.\nSTGlow [Liang et al., 2022] proposes the generative \ufb02ow\nwith pattern normalisation to learn the motion behaviour con-\nditioned on the representation of social interactions. How-\never, there is no NF-based model capable of handling discon-\ntinued manifolds. A plausible solution is to follow MG-GAN\nto use multiple invertible decoders.\nDenoise Defusion Probabilistic Model (DDPM)\nA new\nand interesting framework, motion indeterminacy diffusion\n(MID) [Gu et al., 2022] is proposed for predictions with\ncontrollable diversity. It follows DDPM [Ho et al., 2020]\nthat the latent vector zK is sampled from Gaussian distribu-\ntions with controlled stochasticity via parameterised Markov\nChain. The limitation is that this framework suffers from sig-\nni\ufb01cant time consumption during the reverse diffusion pro-\ncess due to the required number of steps.\n3.2\nAnchor Conditioned MTP Framework\nTo effectively guide the model to predict trajectories with\ncontrolled behaviours, it has been proposed that each predic-\ntion should be conditioned on a prior [Chai et al., 2020; Zhao\net al., 2021], also named anchors, explicit to each modality.\nWell-known anchors include endpoints, the \ufb01nal locations the\nagent may arrive at, or prototype trajectories, the basic mo-\ntions the agent may follow. Ideally, using anchors can ef-\nfectively alleviate the mode collapse problem and encourage\nmore robust and explainable predictions. We categorise the\nframework using anchors as anchor conditioned MTP frame-\nwork. This framework usually contains two sub-tasks: (1)\nanchor selection, which selects K plausible anchors from an\nanchor set and; (2) waypoint decoding, which predicts way-\npoints, the \ufb01nal prediction of future trajectory, based on the\ngiven anchor. The anchor selection can be performed via ran-\ndom sampling or top K ranking. Then the \u201cbest\u201d anchor is\nselected to optimise the waypoint decoding during training as\nteacher force [Williams and Zipser, 1989]. In this section, we\ndiscuss two derivative frameworks named the predicted end-\npoint conditioned (PEC) and prototype trajectory conditioned\n(PTC) framework that use endpoints and prototype trajecto-\nries as anchors respectively.\nPredicted Endpoint Conditioned (PEC) Framework\nIn-\ntuitively, agents can \ufb01rst decide the location they will arrive\nand then plan their future trajectories [Rehder and Kloeden,\n2015]. This introduces the PEC framework where the end-\npoints can be predicted as an anchor and waypoints are gener-\nated to reach those positions. As shown in Fig. 3e, this frame-\nwork \ufb01rstly predicts the endpoint distribution via an endpoint\ndecoder. Then, the waypoint decoder predicts the middle lo-\ncations given each selected endpoint. During training, the\nground truth endpoint is selected so that the relationship be-\ntween predicted waypoints and the conditioned endpoint is\nenhanced. During testing, the endpoint is selected via random\nor top K sampling from a heatmap. The PEC framework is\nwidely used in current trajectory prediction methods due to\nits simplicity and effectiveness.\nMangalam et al. [2020] \ufb01rst introduced the PEC frame-\nwork and propose PECNet, which uses a CVAE to generate\nmultiple endpoints. Further methods suggest that models can\nachieve better performance if predictions can be controlled\nby scene-compliant endpoints. For example, TNT [Zhao et\nal., 2021] and DenseTNT [Gu et al., 2021] predict vehicles\u2019\nendpoints by sampling positions on center lanes while YNet\n[Mangalam et al., 2021] and Goal-GAN [Dendorfer et al.,\n2020] directly predict an endpoint heatmap by integrating the\nobserved trajectory and the scene segmentation image. Mean-\nwhile, ExpertTraj [Zhao and Wildes, 2021] suggests that end-\npoints can be obtained with a training-free process by sam-\npling from existing trajectory repositories with minimal dy-\nnamic time-warping differences. Moreover, the PEC frame-\nwork can help the long-term prediction by conditioning on\nthe endpoint and middle waypoints [Mangalam et al., 2021;\nWang et al., 2022]. Tran et al. [2021] estimate the destina-\ntion of the entire journey where the agent leaves the observed\nregion to better control the future trajectory in a dynamic pre-\ndiction horizon. Future PEC models can focus on avoiding\nunreachable endpoints due to the middle barrier and leverag-\ning the multimodality of waypoints to the same endpoint.\nPrototype Trajectory Conditioned (PTC) Framework\nThe anchor set of the PTC framework are prototype trajecto-\nries, each one representing a modality and providing a basic\nmotion that the waypoints should follow with necessary re-\n\ufb01nement. As shown in Fig. 3f, the PTC framework learns to\nselect the candidate prototype trajectories from the anchor set\nand predict their residuals to the ground truth via the resid-\nual decoder during the waypoint decoding stage. To build\nthe anchor set with suf\ufb01cient diversity, prototypes can be col-\nlected by clustering trajectories in existing datasets using the\nk-means algorithm in MultiPath [Chai et al., 2020] and bag-\nging using a greedy approximation algorithm in CoverNet\n[Phan-Minh et al., 2020]. Moreover, S-Anchor [Kothari et\nal., 2021] constructs the set with different levels of speeds\nand directions via a discrete choice model [Antonini et al.,\n2006] to integrate social interactions. SIT [Shi et al., 2022]\nbuilds a tree-like route map and dynamically selects and re-\n\ufb01nes the path segments. Apparently, using prototype trajecto-\nries can simplify the training and achieve diversity. However,\ncurrent prototype trajectories are usually too simple to han-\ndle complex scenarios. Therefore, more advanced prototype\ntrajectories can be explored in future works.\n3.3\nGrid-based MTP Framwork\nGrid-based MTP framework is an alternative way by employ-\ning occupancy grid maps to indicate which location the agent\nwill go to in the next time step. As shown in Fig. 3g, the scene\nis divided into grid cells and the model predicts the occupancy\nprobability in each cell determined by observed information\nfor each timestep. Multimodal predictions can be obtained\nby sampling exact decision for the next location via Gumbel-\nSoftmax in TDOR [Guo et al., 2022] or beam search for tra-\njectories with top K accumulated log probabilities in Multi-\nverse [Liang et al., 2020] and ST-MR [Li et al., 2022]. The\nmain bene\ufb01t of the grid-based framwork is that they can be\nhighly compliant with scenes with advanced training strate-\ngies such as reinforcement learning or occupancy losses and\nare suitable for long-term prediction. However, it is rarely\nused due to signi\ufb01cant computation from the convolutional\noperations and high sensitivity to the resolution of the maps.\n3.4\nBivariate Gaussian for Outputs Representation\nInstead of regressing the exact 2D coordinates, some models\nhypothesise a bi-variate Gaussian distribution of locations at\neach time step. The objective of these models is to maximise\nthe likelihood of ground truth in the predicted distribution\nvia negative log-likelihood loss. This strategy was \ufb01rst used\nin S-LSTM [Alahi et al., 2016] for deterministic prediction\nbut was depreciated by GAN-based models due to its non-\ndifferentiable position sampling. Then, it is reused in Social-\nSTGCNN [Mohamed et al., 2020] and [Shi et al., 2021] for\nMTP where multiple trajectories can be obtained by sampling\nK futures positions from the predicted distributions.\nHowever, the output positions are sampled individually and\nmay not be temporally correlated, causing unrealistic predic-\ntions. One solution is to integrate it with recurrent neural net-\nworks to generate diverse predictions as a special grid-based\nframework. It can also be combined with anchor-based MTP\nframeworks to avoid the expectation-maximisation training\nprocedure and visualise the uncertainties under each time step\nfor better optimisation [Chai et al., 2020].\n3.5\nOther Techniques for Improved MTP\nAdvanced Discriminators\nAdvanced discriminators in\nGAN-based models have been proposed to improve the qual-\nity of generated trajectories. For example, Parth and Alexan-\ndre [2019] and Huang et al. [2020] propose improved dis-\ncriminators to simplify the adversarial training for recurrent\nneural networks. SC-GAN [Wang et al., 2020] enhances the\ndiscriminator to check scene compliance using differentiable\nrasterised maps and scene images while some methods pro-\npose an improved discriminator to ensure social acceptance\n[Kothari and Alahi, 2022; van der Heiden et al., 2019].\nAdvanced Sampling Tricks\nRandom sampling from an\nMTP model may not cover all modes due to limited sampling\nnumbers. Therefore, advanced sampling tricks are proposed\nto ensure the coverage of the distribution. For example, Ma\net al. [2021] propose post-hoc methods named Likelihood Di-\nverse Sampling (LDS) to enhance the quality and diversity for\n\ufb02ow-based methods by training a sampling model by balanc-\ning the likelihood of an individual trajectory and the spatial\nseparation among trajectories and can also be adapted to other\nframeworks. Mangalam et al. [2021] propose the Test Time\nSampling Trick to cluster sampled endpoints into K centres\nfor wider coverage of predicted endpoints. Bae et al. [2022]\npropose the Non-Probability Sampling Network via a Quasi-\nMonte Carlo method to generate non-biased samples.\n4\nDatasets and Benchmarks for MTP\nDatasets and Benchmarks for Trajectory Prediction\nEx-\nisting widely used benchmarks for both MTP and DTP\ninclude ETH & UCY [Lerner et al., 2007] and Stanford\nDrone Dataset [Robicquet et al., 2016] for pedestrians and\nNuScenes [Caesar et al., 2020], Argoverse [Chang et al.,\n2019] and Waymo [Ettinger et al., 2021] for vehicles. Each\nprovides annotated trajectories and the environment informa-\ntion presented by videos, reference images or high-de\ufb01nition\n(HD) maps.\nSynthetic Dataset for Toy Experiments\nThe distribution\nof trajectories in each dataset is implicit and hence it is hard\nto evaluate whether the model correctly \ufb01ts the distribution.\nTherefore, synthetic datasets have been proposed with simple\nand controllable distributions for evaluation. For example,\nAmirian et al. [2019] propose a toy dataset with six groups\nof trajectories, each group starting from one speci\ufb01c point\nand following three different ways to the endpoint. Chai et\nal. [2020] propose a 3-way intersection toy dataset with the\nprobability of choosing the left, middle or right path set. Ex-\nperiments using these datasets highlight the mode collapse\nproblem and socially-acceptance in current frameworks.\nForkingPath: A Special Benchmark for MTP\nLiang et\nal. [2020] suggest that current trajectory prediction bench-\nmarks all share the same problem that only one possible\nground truth trajectory is provided for each agent, which is\nnot suitable for MTP evaluation.\nTo tackle this problem,\nthey propose the ForkingPath dataset that provides multiple\nhuman-annotated ground truth trajectories for each agent so\nthat ground truth distribution is visible. Further studies [Den-\ndorfer et al., 2021; Ma et al., 2021] also use it to compare the\npredicted and ground truth distributions.\n5\nEvaluation Metrics\nIn DTP, the default evaluation metrics are average displace-\nment error (ADE) and \ufb01nal displacement error (FDE), which\nmeasure the l2 distances to ground truth trajectories through\nall future timesteps and the last future timestep respectively.\nIn addition, there are multiple metrics examining social ac-\nceptance such as collision rate, overlap rate and offroad rate\nwhich measure whether the predicted trajectory of the agent\ncollides with surrounding agents or enters inaccessible re-\ngions. Evaluation metrics for MTP need to take all predic-\ntions into consideration and thus are more challenging. In\nthis section, we review these MTP metrics summarised by\nthe taxonomy in Fig. 4 and their challenges.\n5.1\nLower-bound-based MTP Metrics\nLower-bound-based MTP metrics are simple and widely used\nin MTP. Given K predicted trajectories, each prediction is\ncompared with the ground truth and the best score is recorded\nwithout considering the exact con\ufb01dence. Therefore, these\nmetrics can be simply converted from those for DTP and are\nvalid for any models in trajectory prediction.\nMinimum-of-N (MoN)\nMoN is \ufb01rst proposed in [Gupta et\nal., 2018] and is the default metric for most MTP works. It\ncalculates the minimum error among all predictions:\nMoN = Ei,t\u2208\u03c4 min\nk<K DE( \u02c6Y t\ni,k, Y t\ni )\n(2)\nwhere DE can be any distance metrics used in DTP. Many\nworks adopt this strategy to adapt ADE and FDE for multi-\nmodal prediction, abbreviated as minADEK and minFDEK\nEvaluation Metrics\nLower-Bound based Metrics\nMinimum of N (MoN)\nminADEK; minFDEK\nMissing Rate (MR)\nMR in Argoverse [Chang et al., 2019]; Waymo [Ettinger et al., 2021];\nNuScenes [Caesar et al., 2020]\nProbability-aware Metrics\nMost-Likely (ML)\nML + ADE/FDE [Chen et al., 2021]; ML + Overlap Rate [Ettinger et al., 2021];\nmAP/Soft mAP [Ettinger et al., 2021]\nTopK\nPCMD [Chen et al., 2021]; MoN + Sampling Tricks\nGaussian-based Metrics\nKDE-NLL [Ivanovic and Pavone, 2019]; AMD/AMV [Mohamed et al., 2022]\nDistribution-aware Metrics\nCoverage\nEMD [Amirian et al., 2019]; Precision & Recall [Dendorfer et al., 2021]; PTU [Li et al., 2022]\nFigure 4: An overview of the taxonomy of MTP evaluation metrics.\nand they become the default metrics in all multimodal trajec-\ntory prediction methods and benchmarks.\nMiss Rate (MR)\nSome vehicle trajectory prediction bench-\nmarks such as Waymo, Argoverse and nuScenes use MR to\nindicate whether the ground truth can be covered by the pre-\ndictions. A prediction misses the ground truth if it is more\nthan d meters from the ground truth according to their dis-\nplacement error and hits otherwise. MR counts the scenarios\nthat all predictions miss the ground truth:\nMR = Ei,t\u2208\u03c4 sign((min\nk<K DE( \u02c6Y t\ni,k, Y t\ni )) \u2212d)\n(3)\nwhere FDE is used as the displacement metric in Argoverse\nand Waymo benchmarks and ADE is used in NuScenes. The\ndistance threshold d is 2 meters in Argoverse and NuScenes\nand is adjusted with speed in Waymo benchmark.\nChallenge: Information Leak\nLower-bound-based metrics are sensitive to randomisation\nand are insuf\ufb01cient indicators of the performance of mod-\nels. Information leaks happen during the testing since only\nthe best prediction is used for evaluation based on the dis-\ntances to the ground truth. This allows a distribution with\nhigh entropy for a lower error. For example, the constant ve-\nlocity model (CVM) [Sch\u00a8oller et al., 2020] may even \u201cout-\nperform\u201d deep learning-based models by adjusting the angles\nfor a wider spread distribution. This further results in an unre-\nliable indication of interaction handling. For example, a pre-\ndiction that violates the social rules can be generated from a\nDTP model without social interaction modules and neglected\nin MTP since the best one is selected.\n5.2\nProbability-aware MTP Metrics\nProbability-aware metrics measure how likely the ground\ntruth can be sampled from the predicted distribution. In con-\ntrast to lower-bound metrics, MTP models are required to as-\nsign the highest probability to the best prediction.\nMost-likely (ML) based Metrics\nThe simplest way is to\nselect the prediction with the highest probability to perform\nthe DTP evaluation. For example, the ML metric [Chen et\nal., 2021] simply selects the most likely prediction for ADE\nand FDE calculation, as well as the overlap rate calculation\nin the Waymo benchmark. Similarly, the mean average pre-\ncision (mAP) is used in the Waymo benchmark [Ettinger et\nal., 2021]. The most likely prediction is considered a true\npositive if it aligns with the ground truth; otherwise, it is a\nfalse positive. All other predictions are assigned a false pos-\nitive. Then, it computes the area under the precision-recall\ncurve. From 2022, Waymo benchmark uses Soft mAP, which\nis the same as mAP except that it ignores the penalty from the\npredictions other than the most likely one.\nTopK based Metrics\nChen et al. [2021] suggest that one\nprediction cannot represent the whole distribution. There-\nfore, we can select candidates with a probability larger than\na threshold \u03b3 among M \u226bK predictions for MoN evalua-\ntion, known as the probability cumulative minimum distance\n(PCMD):\nPCMD = MoN( \u02c6Yi,k|P( \u02c6Yi,k\u2032|XT\ni , k\u2032 < M) \u2265\u03b3)\n(4)\nThen, predictions with top K probabilities are selected. How-\never, it cannot be used if the probability of each prediction is\nnot provided. To tackle that, we can select K predictions us-\ning sampling tricks in Sec. 3.5.\nGaussian based Metrics\nIf no probability is provided, an\nalternative method is to \ufb01rst estimate a Gaussian distribution\ngiven K discrete predictions using a method such as kernel\ndensity estimation (KDE), by estimating the probability den-\nsity function given a sequence of independent random vari-\nables. In trajectory prediction, [Ivanovic and Pavone, 2019]\n\ufb01rstly introduces KDE-NLL as one of the evaluation met-\nrics in MTP, which computes the mean log-likelihood of the\nground truth trajectory for each future timestep:\nKDE-NLL = \u2212Ei,t\u2208\u03c4 log P(Y t\ni |KDE( \u02c6Y t\ni,K)),\n(5)\nand is further used in subsequent studies such as [Man-\ngalam et al., 2020, 2021].\nMohamed et al. [2022] fur-\nther improves KDE-NLL by proposing Average Mahalanobis\nDistance (AMD), which measures the distance between the\nground truth and the generated distribution, and Average\nMaximum Eigenvalue (AMV) to measure the con\ufb01dence of\nthe predictions.\nChallenge: Ground Truth May Not Be Most Likely\nProbability-aware metrics hypothesise that ground truth is ex-\npected to be sampled with the highest probability. However,\ndatasets are noisy and the predictions can be more reasonable\nthan the ground truth based on observed clues and thus should\nnot be penalised. For example, if an agent has a behaviour\nsuch as \u201czig zag\u201d in pedestrian datasets and \u201csudden cut in\u201d\nin vehicle datasets, how likely should this behaviour happen?\nIn these cases, we believe that lower-bound based metrics are\nmore suitable.\n5.3\nDistribution-aware Metric\nNone of the metrics above penalises the unacceptable predic-\ntions that are outside the ground truth distribution. The main\nbarrier is that only one ground truth is provided and its dis-\ntribution cannot be estimated. To address this issue, datasets\nsuch as ForkingPath provide multiple ground truth trajecto-\nries for each agent so that we can directly evaluate the cover-\nage of the predicted and ground truth distributions.\nCoverage-based Metrics\nAmirian et al. [2019] propose the\nearth-moving distance (EMD) by calculating the ADE results\nwith linear sum assignment between predicted and ground\ntruth samples. Dendorfer et al. [2021] propose the recall and\nprecision metrics for generative networks to measure the cov-\nerage. Given the predicted and real future trajectory set, re-\ncall counts how many predicted trajectories can \ufb01nd a ground\ntruth trajectory that lies inside a certain range d.\nRecall = Ek<KG( min\nk\u2032<KR || \u02c6Y t\ni,k \u2212Y t\ni,k\u2032||2) < d\n(6)\nwhere KG is the number of predictions and KR is the num-\nber of annotated ground truths for agent i. In other words, the\npredicted distribution should cover all ground truth trajecto-\nries. On the other hand, the precision, also named Pecentage\nof Trajectory Usage (PTU) in [Li et al., 2022], calculates the\nratio of generated samples in the support of the ground truth\ndistribution and penalise out-of-distribution predictions:\nPrecision = Ek<KR( min\nk\u2032<KG ||Y t\ni,k \u2212\u02c6Y t\ni,k\u2032||2) < d\n(7)\nChallenge: Heavy Labour Effort\nApparently, distribution-aware metrics require extra annota-\ntions and corrections by human experts on real-world datasets\nwhich is labour intensive. In addition, even human annotation\ncannot guarantee the coverage of all modalities. Although\nsynthetic datasets can alleviate this problem, they can only\nevaluate simple and unrealistic interactions. Therefore, these\nmetrics are not used in most benchmarks.\n6\nConclusions and Future Directions\nIn this survey, we provide a comprehensive review for MTP\nwith our taxonomies of frameworks, datasets and evaluation\nmetrics. Deep analysis and discussions are provided regard-\ning their bene\ufb01ts and problems. Finally, we suggest several\ndirections for future research as follows:\nBetter Evaluation Metrics\nEvaluation metrics are impor-\ntant to guide the design of the model architecture and loss\nfunctions. Unfortunately, current metrics described in Sec. 5\neither neglect unacceptable predictions or are constrained by\nthe datasets and frameworks. Therefore, it is essential to pro-\npose more comprehensive evaluation metrics which can cor-\nrectly indicate the performance of MTP models without being\nconstrained on datasets or frameworks.\nMotion Planning using Multimodal Predictions\nWe be-\nlieve that MTP will be \ufb01nally used for downstream tasks\nsuch as motion planning and control in autonomous systems.\nThe planning model can provide safe and collision-free routes\nbased on trajectory predictions with diverse modalities. How-\never, to our knowledge, motion planning and MTP are cur-\nrently developed independently. To connect these two \ufb01elds,\nwe \ufb01rst suggest exploring MTP-aware motion planning mod-\nels that can bene\ufb01t from multimodal predictions. Then, the\nperformance of these models can be used as an evaluation\nmetric and planning-aware MTP models can be investigated\nto help motion planning models achieve better results.\nLanguage Guided Explainable MTP\nTo build a trustable\nand safe autonomous system, it is essential to build explain-\nable MTP frameworks that provide human-understandable\ndecision-making for multimodal predictions. Currently, most\nMTP frameworks provide multiple predictions without ex-\nplaining their decisions [Kothari et al., 2021]. Although the\nanchor-based MTP framework alleviates this problem by con-\ntrolling the modality of each prediction using anchors, they\nare far from being explainable.\nRecently, Xue and Salim [2022] have proposed a new\nprompt-based learning paradigm named PromptCast, sug-\ngesting that human language can be a prompt to guide time\nseries forecasting with the form of questioning and answer-\ning. We hence suggest a language-guided MTP framework\ncan be a plausible solution for explainable MTP. We believe\nthat language can be used as prompts to guide predictions\nwith complex modalities and to provide human-readable ex-\nplanations for predicted future trajectories.\nLightweight MTP Frameworks\nIt is also bene\ufb01cial to pre-\ndict trajectories with a longer horizon which requires a larger\nK to cover all modalities. Moreover, some trajectory predic-\ntion models such as YNet [Mangalam et al., 2021] use convo-\nlutional decoders to generate heatmaps and encourage scene-\ncompliant predictions. However, decoders in MTP frame-\nworks are executed repeatedly as shown in Fig. 3 which re-\nsults in excessive time and memory consumption. Therefore,\nexploring the lightweight MTP frameworks can bene\ufb01t real-\ntime autonomous systems.\nMTP with Out-of-distribution (OOD) Modalities\nPre-\ndictions from current MTP models are expected to match ex-\nisting modalities in the dataset and hence the datasets are re-\nquired to cover all kinds of modalities. It is challenging to\npredict OOD modalities especially when the dataset is biased\nand thus is not robust in an unseen environment. It will be\ninteresting to propose a generalised framework to tackle this\nproblem. Future directions can be building more comprehen-\nsive datasets or considering approaches for the OOD problem\nin domain generalisation to help tackle this problem.\nUrban-wide MTP\nCurrent MTP focuses on the future\nmovement over short distances. However, we believe it can\nbe extended to an urban-wide location prediction: human mo-\nbility prediction (HMP), which is to predict the next place-\nof-interest (POI) based on previous locations and rich con-\ntext information such as the semantics of locations. HMP is\nnaturally multimodal since multiple POIs are acceptable with\ndifferent uncertainties and hence the advances in MTP can be\napplied on it as well. Besides, MobTCast [Xue et al., 2021]\nuses DTP to enhance the geographical contextual for HMP.\nTherefore, we believe that MTP can be an even stronger aux-\niliary task to consider the uncertainty of future POIs.\nReferences\nAlexandre Alahi,\nKratarth Goel,\nVignesh Ramanathan,\nAlexandre Robicquet, Li Fei-Fei, and Silvio Savarese.\nSocial LSTM: Human Trajectory Prediction in Crowded\nSpaces. In CVPR, pages 961\u2013971, 2016.\nJavad Amirian, Jean-Bernard Hayet, and Julien Pettre. Social\nWays: Learning Multi-Modal Distributions of Pedestrian\nTrajectories With GANs. In CVPR Workshops, 2019.\nGianluca Antonini, Michel Bierlaire, and Mats Weber. Dis-\ncrete choice models of pedestrian walking behavior. TR B,\npages 667\u2013687, 2006.\nInhwan Bae, Jin-Hwi Park, and Hae-Gon Jeon.\nNon-\nprobability sampling network for stochastic human trajec-\ntory prediction. In CVPR, pages 6477\u20136487, 2022.\nApratim Bhattacharyya, Christoph-Nikolas Straehle, Mario\nFritz, and Bernt Schiele. Haar Wavelet based Block Au-\ntoregressive Flows for Trajectories.\nIn DAGM GCPR,\npages 275\u2013288, 2021.\nHolger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora,\net al. NuScenes: A Multimodal Dataset for Autonomous\nDriving. In CVPR, pages 11621\u201311631, 2020.\nYuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir\nAnguelov. MultiPath: Multiple Probabilistic Anchor Tra-\njectory Hypotheses for Behavior Prediction.\nIn CoRL,\npages 86\u201399, 2020.\nMing-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet\nSingh, Slawomir Bak, et al. Argoverse: 3D Tracking and\nForecasting With Rich Maps. In CVPR, 2019.\nGuangyi Chen, Junlong Li, Nuoxing Zhou, Liangliang Ren,\nand Jiwen Lu. Personalized trajectory prediction via dis-\ntribution discrimination.\nIn ICCV, pages 15580\u201315589,\n2021.\nPatrick Dendorfer, Aljosa Osep, and Laura Leal-Taixe. Goal-\nGAN: Multimodal Trajectory Prediction Based on Goal\nPosition Estimation. In ACCV, 2020.\nPatrick Dendorfer, Sven El\ufb02ein, and Laura Leal-Taix\u00b4e.\nMG-GAN: A Multi-Generator Model Preventing Out-of-\nDistribution Samples in Pedestrian Trajectory Prediction.\nIn ICCV, pages 13158\u201313167, 2021.\nScott Ettinger, Shuyang Cheng, Benjamin Caine, Chenxi Liu,\nHang Zhao, et al. Large Scale Interactive Motion Forecast-\ning for Autonomous Driving: The Waymo Open Motion\nDataset. In ICCV, pages 9710\u20139719, 2021.\nJiyang Gao, Chen Sun, Hang Zhao, Yi Shen, Dragomir\nAnguelov, et al. Vectornet: Encoding HD maps and Agent\nDynamics from Vectorized Representation.\nIn CVPR,\npages 11525\u201311533, 2020.\nJunru Gu, Chen Sun, and Hang Zhao. DenseTNT: End-to-\nend trajectory prediction from dense goal sets. In ICCV,\npages 15303\u201315312, 2021.\nTianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yong-\nming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory\nprediction via motion indeterminacy diffusion. In CVPR,\npages 17113\u201317122, 2022.\nKe Guo, Wenxi Liu, and Jia Pan. End-to-End Trajectory Dis-\ntribution Prediction Based on Occupancy Grid Maps. In\nCVPR, pages 2242\u20132251, 2022.\nAgrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and\nAlexandre Alahi. Social GAN: Socially Acceptable Tra-\njectories with Generative Adversarial Networks. In CVPR,\npages 2255\u20132264, 2018.\nMarah Halawa, Olaf Hellwich, and Pia Bideau. Action-based\ncontrastive learning for trajectory prediction.\nIn ECCV,\npages 143\u2013159, 2022.\nHelbing and Moln\u00b4ar. Social Force Model for Pedestrian Dy-\nnamics. Physical Review E, pages 4282\u20134286, 1995.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-\nsion probabilistic models. NeurIPS, 33:6840\u20136851, 2020.\nYingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and\nZhaoqi Wang. STGAT: Modeling spatial-temporal inter-\nactions for human trajectory prediction. In ICCV, pages\n6272\u20136281, 2019.\nRenhao Huang, Yang Song, and Maurice Pagnucco. An Im-\nproved Discriminator for GAN-Based Trajectory Predic-\ntion Models. In DICTA, pages 1\u20133, 2020.\nB. Ivanovic and Marco Pavone. The Trajectron: Probabilis-\ntic Multi-Agent Trajectory Modeling With Dynamic Spa-\ntiotemporal Graphs. ICCV, pages 2375\u20132384, 2019.\nB. Ivanovic, Karen Leung, Edward Schmerling, and Marco\nPavone. Multimodal Deep Generative Models for Trajec-\ntory Prediction: A Conditional Variational Autoencoder\nApproach. RA-L, 6:295\u2013302, 2021.\nVineet Kosaraju, Amir Sadeghian, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nIan D. Reid, et al. Social-BiGAT: Multimodal Trajectory\nForecasting using Bicycle-GAN and Graph Attention Net-\nworks. In NeurIPS, 2019.\nParth Kothari and Alexandre Alahi. Safety-compliant genera-\ntive adversarial networks for human trajectory forecasting.\narXiv preprint arXiv:2209.12243, 2022.\nParth Kothari, Brian Sifringer, and Alexandre Alahi. Inter-\npretable Social Anchors for Human Trajectory Forecasting\nin Crowds. In CVPR, pages 15556\u201315566, 2021.\nNamhoon Lee, Wongun Choi, Paul Vernaza, Christopher B\nChoy, Philip HS Torr, and Manmohan Chandraker. Desire:\nDistant future prediction in dynamic scenes with interact-\ning agents. In CVPR, pages 336\u2013345, 2017.\nAlon Lerner, Yiorgos Chrysanthou, and Dani Lischinski.\nCrowds by Example. Comput. Graph. Forum, pages 655\u2013\n664, 2007.\nLihuan Li, Maurice Pagnucco, and Yang Song. Graph-Based\nSpatial Transformer With Memory Replay for Multi-\nFuture Pedestrian Trajectory Prediction. In CVPR, pages\n2231\u20132241, 2022.\nJunwei Liang, Lu Jiang, Kevin Murphy, Ting Yu, and Alexan-\nder Hauptmann. The Garden of Forking Paths: Towards\nMulti-Future Trajectory Prediction, 2020.\nRongqin Liang, Yuanman Li, Jiantao Zhou, and Xia Li.\nSTGlow: A Flow-based Generative Framework with Dual\nGraphormer for Pedestrian Trajectory Prediction. ArXiv,\n2022.\nYecheng Jason Ma, Jeevana Priya Inala, Dinesh Jayaraman,\nand Osbert Bastani. Likelihood-based diverse sampling for\ntrajectory forecasting. In ICCV, pages 13279\u201313288, 2021.\nKarttikeya Mangalam, Harshayu Girase, Shreyas Agarwal,\nKuan-Hui Lee, Ehsan Adeli, et al. It is Not the Journey\nbut the Destination: Endpoint Conditioned Trajectory Pre-\ndiction. In ECCV, pages 759\u2013776, 2020.\nKarttikeya Mangalam, Yang An, Harshayu Girase, and Jiten-\ndra Malik. From Goals, Waypoints & Paths to Long Term\nHuman Trajectory Forecasting.\nIn ICCV, pages 15233\u2013\n15242, 2021.\nAbduallah Mohamed, Kun Qian, Mohamed Elhoseiny, and\nChristian Claudel.\nSocial-STGCNN: A Social Spatio-\nTemporal Graph Convolutional Neural Network for Hu-\nman Trajectory Prediction. In CVPR, pages 14424\u201314432,\n2020.\nAbduallah Mohamed, Deyao Zhu, Warren Vu, et al. Social-\nImplicit: Rethinking Trajectory Prediction Evaluation and\nThe Effectiveness of Implicit Maximum Likelihood Esti-\nmation. In ECCV, pages 463\u2013479, 2022.\nKothari Parth and Alahi Alexandre. Adversarial loss for hu-\nman trajectory prediction. In hEART, 2019.\nTung Phan-Minh, Elena Corina Grigore, Freddy A. Boul-\nton, Oscar Beijbom, and Eric M. Wolff. CoverNet: Mul-\ntimodal Behavior Prediction Using Trajectory Sets.\nIn\nCVPR, pages 14062\u201314071, 2020.\nEike Rehder and Horst Kloeden.\nGoal-directed pedestrian\nprediction. In ICCV Workshops, pages 50\u201358, 2015.\nAlexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and\nSilvio Savarese. Learning social etiquette: Human trajec-\ntory understanding in crowded scenes.\nIn ECCV, pages\n549\u2013565, 2016.\nAndrey Rudenko, Luigi Palmieri, Michael Herman, Kris M\nKitani, Dariu M Gavrila, and Kai O Arras. Human Motion\nTrajectory Prediction: A Survey. IJRR, pages 895\u2013935,\n2020.\nAmir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki\nHirose, Hamid Rezato\ufb01ghi, and Silvio Savarese. SoPhie:\nAn Attentive GAN for Predicting Paths Compliant to So-\ncial and Physical Constraints. In CVPR, pages 1349\u20131358,\n2019.\nChristoph Sch\u00a8oller and Alois Knoll. Flomo: Tractable motion\nprediction with normalizing \ufb02ows. In IROS, pages 7977\u2013\n7984, 2021.\nChristoph Sch\u00a8oller, Vincent Aravantinos, Florian Samuel\nLay, and Alois Knoll. What the Constant Velocity Model\nCan Teach Us About Pedestrian Motion Prediction. RA-L,\npages 1696\u20131703, 2020.\nLiushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou,\nMo Zhou, et al. Sparse Graph Convolution Network for\nPedestrian Trajectory Prediction. In CVPR, pages 8994\u2013\n9003, 2021.\nLiushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou,\nFang Zheng, Nanning Zheng, and Gang Hua. Social In-\nterpretable Tree for Pedestrian Trajectory Prediction. In\nAAAI, pages 2235\u20132243, 2022.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan.\nLearning\nStructured Output Representation using Deep Conditional\nGenerative Models. In NeurIPS, 2015.\nIzzeddin Teeti, Salman Khan, Ajmal Shahbaz, Andrew\nBradley, and Fabio Cuzzolin. Vision-based Intention and\nTrajectory Prediction in Autonomous Vehicles: A Survey.\nIn IJCAI, pages 5630\u20135637, 2022.\nLuca Anthony Thiede and Pratik Prabhanjan Brahma. Ana-\nlyzing the variety loss in the context of probabilistic trajec-\ntory prediction. ICCV, pages 9953\u20139962, 2019.\nHung Tran, Vuong Le, and Truyen Tran. Goal-driven long-\nterm trajectory prediction. In WACV, pages 796\u2013805, 2021.\nTessa van der Heiden, Naveen Shankar Nagaraja, Christian\nWeiss, and Efstratios Gavves. SafeCritic: Collision-Aware\nTrajectory Prediction. ArXiv, 2019.\nEason Wang, Henggang Cui, Sai Yalamanchi, et al. Improv-\ning movement predictions of traf\ufb01c actors in bird\u2019s-eye\nview models using gans and differentiable trajectory ras-\nterization. In KDD, pages 2340\u20132348, 2020.\nChuhua Wang, Yuchen Wang, Mingze Xu, and David J. Cran-\ndall. Stepwise goal-driven networks for trajectory predic-\ntion. ICRA, 2022.\nRonald J. Williams and David Zipser. A Learning Algorithm\nfor Continually Running Fully Recurrent Neural Networks.\nNeural Computation, pages 270\u2013280, 1989.\nPei Xu, Jean-Bernard Hayet, and Ioannis Karamouzas. So-\ncialVAE: Human Trajectory Prediction using Timewise\nLatents. In ECCV, 2022.\nHao Xue and Flora Salim. Promptcast: A new prompt-based\nlearning paradigm for time series forecasting. 2022.\nHao Xue, Du Q. Huynh, and Mark Reynolds. SS-LSTM: A\nHierarchical LSTM Model for Pedestrian Trajectory Pre-\ndiction. In WACV, pages 1186\u20131194, 2018.\nHao Xue, Flora Salim, Yongli Ren, and Nuria Oliver. MobT-\nCast: Leveraging Auxiliary Trajectory Forecasting for Hu-\nman Mobility Prediction. NeurIPS, pages 30380\u201330391,\n2021.\nYe Yuan, Xinshuo Weng, Yanglan Ou, and Kris Ki-\ntani. Agentformer: Agent-aware transformers for socio-\ntemporal multi-agent forecasting. In ICCV, pages 9813\u2013\n9823, 2021.\nHe Zhao and Richard P. Wildes. Where Are You Heading?\nDynamic Trajectory Prediction With Expert Goal Exam-\nples. In ICCV, pages 7629\u20137638, 2021.\nHang Zhao, Jiyang Gao, Tian Lan, Chen Sun, Ben Sapp, Bal-\nakrishnan Varadarajan, Yue Shen, et al. TNT: Target-driven\nTrajectory Prediction. In CoRL, pages 895\u2013904, 2021.\n",
    "2401.02916": "UNCOVERING THE HUMAN MOTION PATTERN:\nPATTERN MEMORY-BASED DIFFUSION MODEL FOR TRAJECTORY PREDICTION\nYuxin Yang\u2020, Pengfei Zhu\u2020, Mengshi Qi\u2709, Huadong Ma\nBeijing Key Laboratory of Intelligent Telecommunications Software and Multimedia,\nBeijing University of Posts and Telecommunications\nABSTRACT\nHuman trajectory forecasting is a critical challenge in fields\nsuch as robotics and autonomous driving. Due to the inher-\nent uncertainty of human actions and intentions in real-world\nscenarios, various unexpected occurrences may arise. To un-\ncover latent motion patterns in human behavior, we introduce\na novel memory-based method, named Motion Pattern Priors\nMemory Network. Our method involves constructing a mem-\nory bank derived from clustered prior knowledge of motion\npatterns observed in the training set trajectories. We intro-\nduce an addressing mechanism to retrieve the matched pat-\ntern and the potential target distributions for each prediction\nfrom the memory bank, which enables the identification and\nretrieval of natural motion patterns exhibited by agents, sub-\nsequently using the target priors memory token to guide the\ndiffusion model to generate predictions. Extensive experi-\nments validate the effectiveness of our approach, achieving\nstate-of-the-art trajectory prediction accuracy. The code will\nbe made publicly available.\nIndex Terms\u2014 Trajectory Prediction, Memory Network,\nClustering, Diffusion Model\n1. INTRODUCTION\nHuman trajectory prediction plays a pivotal role in ensuring\nthe safety and efficiency of applications such as autonomous\ndriving systems [1] and social robots. However, predicting\nhuman trajectories is inherently challenging due to the unpre-\ndictable and subjective nature of human behavior.\nTo solve these challenges, significant strides have been\nachieved in research across different aspects, such as social\ninteractions modeling [2, 3]. To emphasize the capability of\naccurately forecasting a wide range of potential future trajec-\ntories in various situations, many generative models are uti-\nlized for stochastic trajectory prediction to model the distri-\nbution of future trajectories, including generative adversarial\nnetworks (GANs)-based[4], conditional variational autoen-\ncoder (CVAE)-based [5, 6, 7], and diffusion-based [8] meth-\nods. Recently, memory networks have been used in the task of\n\u2020 Equal contribution.\n\u2709Corresponding author.\ntrajectory prediction. Many approaches create memory repos-\nitories with different contents. Some methods [6, 9] store all\ntrajectories or clusters of trajectories in the memory bank, and\nthe selection for prediction simply relies on calculating cosine\nsimilarity between observed and stored trajectories. Other\nmethods [10] use memory mechanisms by storing starting and\nending points and training a scoring network to select appro-\npriate intentions.\nWhile notable advancements have been made, unnatural\ntrajectory generation and limited exploration of human mo-\ntion uncertainties remain challenging. Because existing gen-\nerative model-based methods lack reasonable guidance from\nmotion pattern priors, and current memory-based approaches\nlack a systematic strategy to harness inherent uncertainties\nin motion pattern behavior. For instance, unexpected occur-\nrences in real-world scenarios, like abrupt changes in human\nmotion trajectories, are not adequately addressed and taken\ninto consideration.\nTo solve these problems, we introduce a novel memory\napproach that combines the strengths of the aforementioned\nstochastic methods and memory approaches to fully lever-\nage the valuable probabilistic information and motion pattern\npriors. We employ a clustering method based on the motion\ntrend of trajectories to obtain the human motion pattern pri-\nors and store them in the memory bank, which facilitates the\nprediction of a novel agent\u2019s recurring motion pattern given\nits observed motion state [11]. Then we retrieve the matched\npattern and the potential target distribution to obtain the tar-\nget priors memory token, combined with the motion state as\na condition to guide the learning of the reverse diffusion pro-\ncess. We optimize the model using the evidence lower bound\nmaximization method during the training, and sample the tra-\njectories by denoising from a noise distribution during the in-\nference. Extensive experiments validate the effectiveness of\nour approach, achieving state-of-the-art results on ETH/UCY\nand Stanford Drone datasets.\nIn summary, the key contributions of our work can be\nsummarized as follows: 1) We introduce a motion pattern\nprior memory bank to refine prediction results. This marks\nthe first work of utilizing a clustering method to store hu-\nman motion patterns with uncertainties and target distribu-\ntion priors for prediction guidance. 2) We introduce a novel\narXiv:2401.02916v2  [cs.CV]  8 Jan 2024\nS-T\nEncoder\nmotion\nstate\nMotion Pattern Priors\n Memory Bank\nClustered \nMotion Patterns \nMemory\n\u03f5 ~ \ud835\udc41(0,1)\nMSE\nFC\nLoss\nSinusoidal Positional Encoding\nmotion\nstate\ns\nFC\nFC\n\ud835\udc4c\ud835\udc60\n...\nTargets Addressed\n\ud835\udc7b\ud835\udc82\ud835\udc93\ud835\udc88\ud835\udc86\ud835\udc95\ud835\udfcf\nPattern Matched\nTransformer\ns\nMLP\n...\nTarget \nDistribution \nMemory\nEncoder\nTarget-Guided Diffusion\n\ud835\udc7f\ud835\udfcf\n\ud835\udc7f\ud835\udfd0\n\ud835\udc7f\ud835\udfd1\nObserved Trajectory \n\ud835\udc80\ud835\udfcf\n\ud835\udc80\ud835\udfd0\n\ud835\udc80\ud835\udfd1\nFuture Trajectory \nFig. 1. The overview of our proposed MP2MNet method. It contains an encoder, the motion pattern priors memory bank, and\na Transformer-based decoder. The encoder captures information to obtain the motion state representation. S denotes the total\ndiffusion step and s denotes the sth step. Y s is corrupted s steps by adding noise variable to ground-truth Y 0. The decoder\nprocesses Y s along with motion state embedding, target priors memory token, and time embedding to generate the output. The\ntraining objective is to minimize the mean square error (MSE) loss between the output and the noise variable in the Gaussian\ndistribution. This is achieved through target-guided diffusion generation for each iteration s to optimize the network.\ntarget-guided diffusion model in Motion Pattern Priors Mem-\nory Network (MP2MNet). Using the matched motion pattern\nand target distribution, we can obtain the target priors mem-\nory token as guidance for the Transformer-based decoder in\nthe reverse diffusion process to guide the diffusion model to\ngenerate various reasonable prediction results. 3) We conduct\nextensive experiments on multiple benchmark datasets, show-\ncasing the superior performance of our method compared to\nstate-of-the-art methods from recent years.\n2. RELATED WORKS\nTrajectory prediction: Human trajectory prediction meth-\nods can be classified into single-modality and multi-modality\napproaches. In single-modality approaches, early trajectory\nprediction research primarily relied on deterministic mod-\nels like Markov processes [12] and recurrent neural net-\nworks [13].\nNevertheless, modern approaches emphasize\nmodeling complex social interactions among agents. For ex-\nample, Social-LSTM [2] introduced a social pooling layer\nto capture agent interactions, extended by Social-GAN [4].\nAdditionally, attention-based methods are essential to cap-\nture critical interactions in crowded environments [3]. Other\nworks have integrated scene understanding to extract global\ninformation, such as SS-LSTM [14]. Trajectron++ [5] estab-\nlished the connections between scene information and agent\nmotion using graph structures, and MANTRA [15] combined\nmemory mechanisms with scene images. MID [8] is notewor-\nthy as it introduced diffusion models to predict trajectories by\nmodeling the process of human motion variation from inde-\nterminate to determinate.\nMemory Networks: Memory networks are commonly\nused in question-answering tasks.\nIn recent years, mem-\nory networks have gained prominence in trajectory predic-\ntion tasks, demonstrating noteworthy advancements.\nSev-\neral methods have established memory repositories with di-\nverse contents. For example, Ma et al. [16] applied mem-\nory networks to trajectory prediction tasks, utilizing genera-\ntive memory for continuous trajectory prediction. Mangalam\net al. [6] utilized a memory network for single-agent trajec-\ntory prediction. Similarly, SHENet [9] employed a similar\napproach by clustering trajectories within the same scene, fil-\ntering trajectories in the memory repository based on similar-\nity, and utilizing multimodal information for human trajectory\nprediction. MemoNet [10] focuses on target points, utilizing\nmemory mechanisms to store starting and target points and\ntraining a scoring network for target point prediction.\nDifferent from these works, we propose a novel memory\napproach combining the strength of the diffusion model and\nusing motion pattern priors to guide the trajectory generation.\n3. OUR METHOD\n3.1. Problem Definition\nThe goal of human trajectory prediction is to forecast the fu-\nture trajectories of multiple agents based on their historical\nmotion paths. Mathematically, given a scenario where the\nobserved historical trajectories are denoted as Z, and where\nmultiple agents are present, each with their observed histor-\nical trajectory data represented as Xi = {xt\ni}0\nt=\u2212Tobs+1 \u2208\nRTobs\u00d72, here i \u2208[1, 2, . . . , N] represents the identifier of\nthe observed agents in the environment, and xt\ni \u2208R2 repre-\nsents the position of the ith agent in the scene at time t. Our\nmethod aims to predict the trajectories of multiple agents in\nthe future period, denoted as Yi = {yt\ni}Tpred\nt=1\n\u2208RTpred\u00d72, i \u2208\n[1, 2, . . . , N]. The predicted future trajectories should closely\nresemble the ground truth as much as possible. In subsec-\ntion 3.4, we denote X, Y without the agent index i for the\nobserved and predicted trajectory.\n3.2. Overall Network Architecture\nWe present our MP2MNet based on the diffusion model,\nwhich is a novel target-guided framework to formulate the\ntrajectory prediction task as a reverse process of diffusion. To\nbe specific, as depicted in Figure 1, our approach consists of\nthree parts: 1) an encoder network, 2) a motion pattern priors\nmemory bank, and 3) a Transformer-based decoder.\nThe encoder captures information from historical trajec-\ntories, producing the observed motion state embedding. Here\nwe apply the encoder of Trajectron++ [5]. Using our mo-\ntion pattern priors memory bank, we can retrieve the matched\npattern and target distributions to generate target priors mem-\nory tokens as guidance. It combines with other information\nsuch as motion state embedding to serve as a condition for\nthe Transformer-based decoder, designed to model Gaussian\ntransitions in a Markov chain, enabling the network to opti-\nmize and generate predictions through denoising.\n3.3. Motion Pattern Priors Memory\nFor each cluster, we utilize the Motion Pattern Priors Mem-\nory module to construct the memory bank Zbank, with clus-\ntered trajectories and uncertainty value attached. Note that,\nthe Motion Pattern Priors Memory module is used to refine\nprediction results, which means our generative predictions do\nnot solely rely on the memory bank. This module can be sum-\nmarized as three procedures, i.e., 1) constructing the motion\npattern priors memory bank, 2) retrieving the matched mo-\ntion pattern with target distributions, and 3) generating corre-\nsponding target priors memory token.\nMemory Bank Initialization.\nTo efficiently improve\nmemory usage, we utilize clustering to group similar trajec-\ntories based on their motion trends, forming clustered motion\npattern distributions. These distributions are then stored in the\nmemory bank along with their uncertainty values. To be spe-\ncific, for all trajectories in the training set, we represent their\ntrajectories as a set Z, with targets yTpred included. Then we\nuse K-means clustering to group similar trajectories consider-\ning the whole motion trend in the Z. Assume we can obtain K\nclusters, representing a total of K motion pattern distribution\npriors, formulated as {N}K\n0 , of which \u00b5i and \u03c3i2 represent\n2D trajectories and uncertainty value of agent i. Correspond-\ning target distributions of each cluster can be formulated as\n{\u03c1Tpred}K\n0 }. Finally, we can obtain the motion pattern priors\nmemory bank Zbank = {{N}K\n0 , {\u03c1Tpred}K\n0 }, which can be\nused then as priors memory.\nTrajectory Addressing. Given the past trajectory Xi of\nagent i, we first use Xi to obtain the matched motion pattern\npriors Ni with (\u00b5i, \u03c3i) from the memory bank by selecting\nthe one with minimal Gaussian negative log-likelihood score.\nFor each motion pattern Nj, assuming the provided tra-\njectory Xi follows a Gaussian distribution with mean \u00b5j and\nvariance \u03c3j2, we can calculate the Gaussian Negative Log-\nlikelihood (NLL) score as follows:\nSNLL = 1\n2\n\u0012\nlog(max( \u03c3j, \u03f5 )) + ( Xi \u2212\u00b5j )2\nmax( \u03c3j, \u03f5 )\n\u0013\n,\n(1)\nwhere Xi represents the provided past trajectory, \u00b5j corre-\nsponds to the trajectory associated with motion pattern Nj\nstored in the memory bank, and \u03c3j represents the uncertainty\nof \u00b5j. The parameter \u03f5 is a hyper-parameter introduced for\nstability.\nThe pattern Ni with \u00b5i and \u03c3i, which yields the minimum\nGaussian NLL score, is considered the optimal match for the\ncurrent agent i:\nNi = arg\nmin\n\u00b5i,\u03c3i\u2208N SNLL(Xi, \u00b5i, \u03c3i).\n(2)\nSubsequently, we can retrieve the potential target distribution\n\u03c1Tpred\ni\nassociated with the matched pattern from the memory\nbank, which is employed to guide the diffusion process for\ngenerating future trajectories.\nTarget Priors Memory Token Generation. Using the\nobtained potential target distribution \u03c1Tpred\ni\nof the agent i\nthrough the trajectory addressing mechanism, we use sinu-\nsoidal position encoding and MLP to convert 2D target posi-\ntions into target embedding, used as an additional target prior\nfor prediction refinement:\nh2j = f(sin\n\u0010\n\u03c1Tpred\ni\n/\u03bb2j/D\u0011\n),\nh2j+1 = f(cos\n\u0010\n\u03c1Tpred\ni\n/\u03bb2j/D\u0011\n),\n(3)\nwhere \u03c1Tpred\ni\nrepresents the target position of agent i, h \u2208RD\nrepresents the target priors memory token, j is the dimension\nindex, and D is the dimension of the embedding \u03c1Tpred\ni\n. \u03bb\nTable 1. Quantitative results on the Stanford Drone dataset with Best-of-20 strategy in ADE/FDE metric. \u2193represents that\nlower is better. The best results are highlighted in bold.\nTime\nSocial-LSTM [2]\nSocial-GAN [4]\nTrajectron++ [5]\nMANTRA [15]\nGroupNet+CVAE [7]\nMP2MNet\n4.8s\n31.19/56.97\n27.23/41.44\n19.30/32.70\n8.96/17.76\n9.31/16.11\n8.89/16.45\ndenotes the max period of the sinusoidal function. The wave-\nlengths form a geometric progression from 2\u03c0 to 10000 \u00b7 2\u03c0.\nAnd f(\u00b7) denotes MLP.\n3.4. Target-guided Diffusion Model\nAs depicted in Figure 1, we utilize the Transformer as the de-\ncoder following the previous work [8]. We denote that Ys is\ncorrupted by adding a noise variable for s times to the ground\ntruth trajectory Y0.\nThen we take Ys, the target priors memory token h com-\nbined with the motion state embedding denoted as G, and\ntime embedding as the input of the Transformer-based de-\ncoder for the reverse diffusion process.\nThe decoder is trained to generate trajectories from Gaus-\nsian noise conditioned on the information including G at each\nstep of the denoising process. The reverse diffusion process\nprogressively diminishes uncertainties across all accessible\nregions, ultimately leading to specific predictions using a pa-\nrameterized Markov chain.\nTarget-guided Diffusion. The reverse diffusion process\nis the joint distribution p\u03b8\n\u0000Y(0:S) | G\n\u0001\nconditioned on G,\ndefined as a Markov chain with learned Gaussian transitions\nthat begins with p\n\u0000YS\u0001\n= N\n\u0000YS; 0, I\n\u0001\n:\np\u03b8\n\u0010\nY(0:S)\u0011\n:= p\n\u0010\nYS\u0011\nS\nY\ns=1\np\u03b8\n\u0000Ys\u22121 | Ys, G\n\u0001\n,\n(4)\np\u03b8\n\u0000Ys\u22121 | Ys, G\n\u0001\n:= N\n\u0000Ys\u22121; \u00b5\u03b8 (Ys, s, G) , \u03a3\u03b8 (Ys, s)\n\u0001\n,\n(5)\nwhere s denotes the diffusion step, \u03b8 represents our target-\nguided diffusion model\u2019s parameter, and \u03a3\u03b8 (Ys, s) equals\nto \u03b2sI. \u03b2s is the variance at the denoising step s, controlling\nthe extent of added noise.\nThe forward diffusion process is a Markov chain that\ngradually adds Gaussian noise to raw trajectory data for S\nsteps according to a uniformly increasing variance schedule\n\u03b21,...,\u03b2S, which constrains the level of noise injection. We\ncan formulate the approximate posterior as:\nq\n\u0010\nY(1:S) | Y0\u0011\n:=\nS\nY\ns=1\nq\n\u0000Ys | Ys\u22121\u0001\n,\n(6)\nq\n\u0000Ys | Ys\u22121\u0001\n= N\n\u0010\nYs;\np\n1 \u2212\u03b2sYs\u22121, \u03b2sI\n\u0011\n.\n(7)\nUsing the notation \u03b1s := 1 \u2212\u03b2s and \u00af\u03b1s := Qs\nm=1 \u03b1m, we\ncan obtain:\nq\n\u0000Ys | Y0\u0001\n= N\n\u0000Ys; \u221a\u00af\u03b1sY0, (1 \u2212\u00af\u03b1s) I\n\u0001\n.\n(8)\nWhen the total number of denoising step S is large enough,\nq(YS)) approximates to N(YS; 0, I), where N is a Gaussian\ndistribution.\nTraining Objective. Our target-guided diffusion model\nperforms training by optimizing the variational lower bound.\nAs the exact log-likelihood is intractable, we use the evidence\nlower bound maximization method and minimize the KL di-\nvergence. We can use KL divergence to directly compare\np\u03b8\n\u0000Ys\u22121 | Ys, G\n\u0001\nagainst forward process posteriors:\nL = Eq,s\n\u0002\nDKL\n\u0000q\n\u0000Ys\u22121 | Ys, Y0\u0001\n\u2225p\u03b8\n\u0000Ys\u22121 | Ys, G\n\u0001\u0001\u0003\n= Eq,s\n\u0002\nDKL\n\u0000N\n\u0000Ys\u22121; \u02dc\u00b5s, \u03a3q(s)\u2225N\n\u0000Ys\u22121; \u00b5\u03b8, \u03a3(s)\n\u0001\u0001\u0003\n= Eq,s\nh\n\u2225\u00b5\u03b8 (Ys, s, G) \u2212\u02dc\u00b5s\u22252\n2\ni\n,\n(9)\nwhere \u00b5\u03b8 and \u02dc\u00b5s are calculated using the reparameterization\ntrick as:\n\u02dc\u00b5s\n\u0000Ys, Y0\u0001\n=\n\u221a\u00af\u03b1s\u22121\u03b2s\n1 \u2212\u00af\u03b1s Y0 +\n\u221a\u03b1s (1 \u2212\u00af\u03b1s\u22121)\n1 \u2212\u00af\u03b1s\nYs,\n\u02dc\u03b2s = 1 \u2212\u00af\u03b1s\u22121\n1 \u2212\u00af\u03b1s \u03b2sI,\n(10)\n\u00b5\u03b8 (Ys, s, G) =\n1\n\u221a\u03b1s\n\u0012\nYs \u2212\n\u03b2s\n\u221a1 \u2212\u00af\n\u03b1s\n\u03f5\u03b8 (Ys, s, G)\n\u0013\n,\n(11)\nwhere \u03f5\u03b8(\u00b7) denotes the noise predictor of our target-guided\ndiffusion model for trajectory generation. We optimize the\nnetwork through diffusion generation by performing mean\nsquare error (MSE) loss between the output and a noise vari-\nable in standard Gaussian distribution for the current iteration\ns, following the work [8]:\nLMSE(\u03b8, \u03d5) = Ez,Y0,s\n\r\rz \u2212\u03f5(\u03b8,\u03d5) (Ys, s, G)\n\r\r ,\n(12)\nwhere \u03b8 and \u03d5 are parameters of the target-guided diffusion\nmodel and encoder respectively, and z \u223c((0, I)).\nInference. During the reverse process, with the reparam-\neterization, we use the DDPM sampling technique to repeat-\nedly denoise Y S to Y 0 by using the equation below for S\nsteps as:\nYs\u22121 =\n1\n\u221a\u03b1s\n\u0012\nYs \u2212\n\u03b2s\n\u221a1 \u2212\u00af\u03b1s\n\u03f5\u03b8 (Ys, s, G)\n\u0013\n+\np\n\u03b2sz.\n(13)\n4. EXPERIMENTS\n4.1. Experimental Setup\nDatasets. We evaluate the effectiveness of our MP2MNet\non two commonly used benchmarks for human trajectory\nTable 2. Comparison of state-of-the-art methods on ETH/UCY datasets. \u2193represents that lower is better. We report ADE and\nFDE for predicting future 12 frames in meters. \u2020 represents our reproduction results. \u2018AVG\u2019 means the average result over 5\nsubsets. The best-of-20 is adopted for evaluation. The best results are highlighted in bold while the second-best results are\nunderlined.\nMethods\nEvaluation metrics: ADE\u2193/ FDE\u2193(in meters)\nETH\nHOTEL\nUNIV\nZARA1\nZARA2\nAVG\nLSTM\n1.01 / 1.94\n0.60 / 1.34\n0.71 / 1.52\n0.41 / 0.89\n0.31 / 0.68\n0.61 / 1.27\nS-LSTM [2]\n0.75 / 1.38\n0.61 / 1.40\n0.58 / 1.03\n0.42 / 0.70\n0.43 / 0.71\n0.56 / 1.05\nSGAN [4]\n0.87 / 1.62\n0.67 / 1.37\n0.76 / 1.52\n0.35 / 0.68\n0.42 / 0.84\n0.61 / 1.21\nSTGAT [3]\n0.68 / 1.33\n0.38 / 0.72\n0.56 / 1.21\n0.34 / 0.69\n0.29 / 0.60\n0.45 / 0.91\nMANTRA [15]\n0.70 / 1.76\n0.28 / 0.68\n0.51 / 1.26\n0.25 / 0.67\n0.20 / 0.54\n0.39 / 0.98\nTPNMS [17]\n0.52 / 0.89\n0.22 / 0.39\n0.55 / 1.13\n0.35 / 0.70\n0.27 / 0.56\n0.38 / 0.73\nSocial-Implicit [18]\n0.66 / 1.44\n0.20 / 0.36\n0.31 / 0.60\n0.25 / 0.50\n0.22 / 0.43\n0.33 / 0.67\nGroupNet [7]\n0.46 / 0.73\n0.15 / 0.25\n0.26 / 0.49\n0.21 / 0.39\n0.17 / 0.33\n0.25 / 0.44\nMID\u2020 [8]\n0.42 / 0.75\n0.18 / 0.32\n0.23 / 0.46\n0.20 / 0.40\n0.17 / 0.34\n0.25 / 0.46\nOurs (w/o memory)\n0.44 / 0.76\n0.18 / 0.33\n0.23 / 0.47\n0.25 / 0.52\n0.19 / 0.41\n0.26 / 0.50\nOurs (w/ memory)\n0.39 / 0.69\n0.16 / 0.27\n0.21 / 0.43\n0.23 / 0.48\n0.16 / 0.32\n0.23 / 0.44\nprediction tasks: ETH/UCY [19, 20], and Stanford Drone\nDataset(SDD) [21]. ETH/UCY includes positions of pedes-\ntrians in the world coordinates from 5 scenes: ETH, HOTEL,\nUNIV, ZARA1, and ZARA2. We evaluate our model on this\ndataset using the same approach as in previous works [4, 3],\nemploying the leave-one-out method. SDD is captured in a\nuniversity campus environment from a bird\u2019s-eye view. For\nboth datasets, we use the data from the past 8 frames (3.2 sec-\nonds) to predict the trajectory for the future 12 frames (4.8\nseconds).\nEvaluation Metrics. Evaluation metrics include ADE and\nFDE, which are commonly used in prior works: Average Dis-\nplacement Error (ADE) measures the average L2 distance be-\ntween the ground truth and prediction results over all specified\nprediction time steps, while Final Displacement Error (FDE)\nrepresents the distance between the predicted destination and\nthe true destination at time-step Tpred.\nImplementation Details. The training was performed with\nAdam Optimizer with a learning rate of 0.001 and a batch size\nof 256. We set diffusion steps S as 100. The Transformer has\nthree layers with 512 dimensions and four attention heads.\nThe final trajectory prediction is obtained by downsampling\nthe Transformer output sequence through three fully con-\nnected layers (from 512d to 256d and then to 2d).\n4.2. Results and Analysis\nValidation Results on Pedestrian Datasests. We conduct\na comprehensive quantitative comparison of our MP2MNet\nmethod with a diverse set of contemporary approaches. We\nadopt a best-of-20 evaluation strategy, consistent with previ-\nous methods [4, 3, 7, 8] for fair comparison.\nTable 1 provides a comparison between our method and\ntypical existing approaches on the Stanford Drone Dataset.\nOur method achieves a leading average ADE of 8.89 in pixel\ncoordinates, surpassing all prevalent approaches. Meanwhile,\nour method achieved the best mean ADE/FDE (0.23/0.44)\nfor the ETH/UCY dataset, outperforming all other trajectory\nprediction methods. Detailed quantitative results comparing\nour proposed method with state-of-the-art methods are pre-\nsented in Table 2. Our method exhibits superior performance,\ndemonstrating an 8% reduction in ADE compared to the state-\nof-the-art GroupNet.\nCompared to MID [8], our method\nachieves improvements in both ADE and FDE in the ETH\nscene, with reductions of 7% and 8% respectively. Notably,\nour model exhibits significant advancements over previous\nmethods, particularly in the ETH, UNIV, and ZARA2.\nAblation Study. Furthermore, we explore the impact of\nour motion pattern priors memory bank. We substitute the\ntarget priors memory token to the raw predicted target em-\nbedding for comparison. Comparisons of the last two rows\nof Table 2 and the visualization results in Fig. 2 (b) and (c)\nreveal that our memory-based approach enhances prediction\nperformance. Our method with motion pattern priors mem-\nory outperforms the method without this memory by reducing\nmean ADE by 11.5% and mean FDE by 12%.\nVisualization Results. Visualization results for compar-\nison are tested on scenes in the ETH/UCY dataset, as shown\nin Fig. 2. To validate the effectiveness of our memory ap-\nproach, we compare the prediction results of our method\nand two other baseline methods including MID [8] and our\nmethod without motion pattern priors memory. Our method\nFig. 2. Visualization comparison on the ETH/UCY datasets. We compare the best-of-20 predictions generated by our approach\nwith those from two baseline methods: the previous MID method [8] and our method without the motion pattern priors memory.\nGround truths are in red solid lines, past trajectories in dark blue solid lines, and prediction results in light blue lines. We\nvisualize 20 predictions for each agent with light blue dashed lines and corresponding targets are marked with stars. The result\nshows significant improvements by utilizing our memory-based method.\nhas been proven to be effective based on the qualitative eval-\nuation results. The best-of-20 prediction results are closest to\nthe ground truth, and the predictions fall within a reasonable\ndiverse range, which retains stochastic varieties of human\nmovements and considers diverse human motion patterns.\n5. CONCLUSION\nIn this paper, we propose a novel memory approach to effi-\nciently leverage motion pattern priors from the training set.\nWe introduce a cluster-based memory bank to store human\nmotion patterns with target distributions. We adopt an ad-\ndressing mechanism to retrieve the matched pattern and the\ntarget distribution and generate the target priors memory to-\nken which in turn guides the diffusion model to generate tra-\njectories. Extensive experiments and the ablation study vali-\ndate the effectiveness of our method.\n6. REFERENCES\n[1] Jesse Levinson, Jake Askeland, Jan Becker, Jennifer Dolson, David\nHeld, Soeren Kammel, J Zico Kolter, Dirk Langer, Oliver Pink,\nVaughan Pratt, et al., \u201cTowards fully autonomous driving: Systems\nand algorithms,\u201d in 2011 IEEE intelligent vehicles symposium (IV).\nIEEE, 2011, pp. 163\u2013168.\n[2] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Ro-\nbicquet, Li Fei-Fei, and Silvio Savarese, \u201cSocial lstm: Human trajec-\ntory prediction in crowded spaces,\u201d in CVPR, 2016, pp. 961\u2013971.\n[3] Yingfan Huang, Huikun Bi, Zhaoxin Li, Tianlu Mao, and Zhaoqi Wang,\n\u201cStgat: Modeling spatial-temporal interactions for human trajectory\nprediction,\u201d in CVPR, 2019, pp. 6272\u20136281.\n[4] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexan-\ndre Alahi, \u201cSocial gan: Socially acceptable trajectories with generative\nadversarial networks,\u201d in CVPR, 2018, pp. 2255\u20132264.\n[5] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco\nPavone,\n\u201cTrajectron++: Dynamically-feasible trajectory forecasting\nwith heterogeneous data,\u201d in ECCV. Springer, 2020, pp. 683\u2013700.\n[6] Karttikeya Mangalam, Harshayu Girase, Shreyas Agarwal, Kuan-Hui\nLee, Ehsan Adeli, Jitendra Malik, and Adrien Gaidon, \u201cIt is not the\njourney but the destination: Endpoint conditioned trajectory predic-\ntion,\u201d in ECCV. Springer, 2020, pp. 759\u2013776.\n[7] Chenxin Xu, Maosen Li, Zhenyang Ni, Ya Zhang, and Siheng Chen,\n\u201cGroupnet: Multiscale hypergraph neural networks for trajectory pre-\ndiction with relational reasoning,\u201d in CVPR, 2022, pp. 6498\u20136507.\n[8] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao,\nJie Zhou, and Jiwen Lu, \u201cStochastic trajectory prediction via motion\nindeterminacy diffusion,\u201d in CVPR, 2022, pp. 17113\u201317122.\n[9] Mancheng Meng, Ziyan Wu, Terrence Chen, Xiran Cai, Xiang Zhou,\nFan Yang, and Dinggang Shen, \u201cForecasting human trajectory from\nscene history,\u201d NeurIPS, vol. 35, pp. 24920\u201324933, 2022.\n[10] Chenxin Xu, Weibo Mao, Wenjun Zhang, and Siheng Chen, \u201cRemem-\nber intentions: retrospective-memory-based trajectory prediction,\u201d in\nCVPR, 2022, pp. 6488\u20136497.\n[11] Judith A Ouellette and Wendy Wood, \u201cHabit and intention in every-\nday life: The multiple processes by which past behavior predicts future\nbehavior.,\u201d Psychological bulletin, vol. 124, no. 1, pp. 54, 1998.\n[12] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Brad-\nbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard\nSocher,\n\u201cAsk me anything: Dynamic memory networks for natural\nlanguage processing,\u201d in ICML. PMLR, 2016, pp. 1378\u20131387.\n[13] Jeremy Morton, Tim A Wheeler, and Mykel J Kochenderfer, \u201cAnal-\nysis of recurrent neural networks for probabilistic modeling of driver\nbehavior,\u201d TITS, vol. 18, no. 5, pp. 1289\u20131298, 2016.\n[14] Hao Xue, Du Q Huynh, and Mark Reynolds, \u201cSs-lstm: A hierarchical\nlstm model for pedestrian trajectory prediction,\u201d in WACV. IEEE, 2018,\npp. 1186\u20131194.\n[15] Francesco Marchetti, Federico Becattini, Lorenzo Seidenari, and Al-\nberto Del Bimbo, \u201cMantra: Memory augmented networks for multiple\ntrajectory prediction,\u201d in CVPR, 2020, pp. 7143\u20137152.\n[16] Hengbo Ma, Yaofeng Sun, Jiachen Li, Masayoshi Tomizuka, and Chiho\nChoi, \u201cContinual multi-agent interaction behavior prediction with con-\nditional generative memory,\u201d RAL, vol. 6, no. 4, pp. 8410\u20138417, 2021.\n[17] Rongqin Liang, Yuanman Li, Xia Li, Yi Tang, Jiantao Zhou, and Wen-\nbin Zou, \u201cTemporal pyramid network for pedestrian trajectory predic-\ntion with multi-supervision,\u201d in AAAI, 2021, vol. 35, pp. 2029\u20132037.\n[18] Abduallah Mohamed, Deyao Zhu, Warren Vu, Mohamed Elhoseiny,\nand Christian Claudel, \u201cSocial-implicit: Rethinking trajectory predic-\ntion evaluation and the effectiveness of implicit maximum likelihood\nestimation,\u201d in ECCV. Springer, 2022, pp. 463\u2013479.\n[19] Stefano Pellegrini, Andreas Ess, and Luc Van Gool, \u201cImproving data\nassociation by joint modeling of pedestrian trajectories and groupings,\u201d\nin ECCV. Springer, 2010, pp. 452\u2013465.\n[20] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski, \u201cCrowds by\nexample,\u201d in Computer graphics forum. Wiley Online Library, 2007,\nvol. 26, pp. 655\u2013664.\n[21] Alexandre Robicquet, Amir Sadeghian, Alexandre Alahi, and Silvio\nSavarese, \u201cLearning social etiquette: Human trajectory understanding\nin crowded scenes,\u201d in ECCV. Springer, 2016, pp. 549\u2013565.\n",
    "2403.11643": "Diffusion-Based Environment-Aware\nTrajectory Prediction\nTheodor Westny1 , Bj\u00f6rn Olofsson2,1 , and Erik Frisk1\n1 Department of Electrical Engineering, Link\u00f6ping University\n{theodor.westny, bjorn.olofsson, erik.frisk}@liu.se\n2 Department of Automatic Control, Lund University\nbjorn.olofsson@control.lth.se\nAbstract. The ability to predict the future trajectories of traffic partici-\npants is crucial for the safe and efficient operation of autonomous vehicles.\nIn this paper, a diffusion-based generative model for multi-agent trajectory\nprediction is proposed. The model is capable of capturing the complex\ninteractions between traffic participants and the environment, accurately\nlearning the multimodal nature of the data. The effectiveness of the\napproach is assessed on large-scale datasets of real-world traffic scenarios,\nshowing that our model outperforms several well-established methods in\nterms of prediction accuracy. By the incorporation of differential motion\nconstraints on the model output, we illustrate that our model is capable\nof generating a diverse set of realistic future trajectories. Through the use\nof an interaction-aware guidance signal, we further demonstrate that the\nmodel can be adapted to predict the behavior of less cooperative agents,\nemphasizing its practical applicability under uncertain traffic conditions.\nKeywords: Trajectory Prediction \u00b7 Generative Modeling \u00b7 Autonomous\nDriving\n1\nIntroduction\nAn important property of self-driving vehicles is their ability to accurately forecast\nthe behaviors (motion and intentions) of surrounding road users. However, merely\nmaking accurate predictions is not sufficient in practical applications. To enable\nsafe and robust decision-making processes, it is crucial that these methods also\neffectively handle the multimodal uncertainty in the forecasted behaviors. As\na consequence, probabilistic modeling has become a cornerstone in devising\nrobust trajectory prediction methods [28]. With the emerging field of generative\nmodeling offering promising new directions for research and development, we seek\nto explore the potential of conditional diffusion models for trajectory prediction.\nDiffusion models [23,32,64,66], a type of generative model, have seen a surge\nin popularity in recent years. These models have made a significant impact in\nthe field of image synthesis [23,56,59,66]; and more recently, been innovatively\napplied in other domains, including molecule generation [25], temporal data\nmodeling [2,57], traffic scenario generation [54], and more [79]. However, their\nadaptation and use for trajectory prediction applications are still limited.\narXiv:2403.11643v1  [cs.CV]  18 Mar 2024\n2\nT. Westny et al.\nFig. 1: The directed graphical model considered in this work. The goal of diffusion\nmodels is to learn the process of transforming noise into samples that are representative\nof the true data distribution. In this work, the task of the proposed model is to generate\nrealistic and physically feasible future trajectories for road users.\nAn important aspect of trajectory prediction, specifically multi-agent methods,\nis the modeling of inter-agent interactions. Using graph neural networks (GNNs)\nfor these purposes has emerged as one fundamental technique [28, 55]. Their\nscalability and flexibility in handling geometric data provide a distinctive inductive\nbias, ideal for modeling the complex interactions that are inherent in many real-\nworld traffic scenarios. While the modeling of inter-agent interactions has become\na cornerstone in behavior prediction methods [1,13,41,44,48,71,76], incorporating\nmap-based information and road-agent interactions have likewise attracted the\ninterest of several researchers [14,27,39,42,47,62,83,85]. Unifying these approaches\nas conditionals in a diffusion model is the focus of this work.\nIn motion forecasting tasks, a notable drawback of black-box (neural) models is\ntheir tendency to compute outputs that either defy physical feasibility or lack the\ncharacteristics of natural motion. To mitigate these issues, there have been several\nproposals for incorporating differential motion constraints within trajectory\nprediction models [11, 39, 62, 73, 75, 76, 81]. Still, their direct inclusion in the\ndiffusion framework is not appropriate given the original denoising objective [23].\nHowever, by a simple modification of the model output and learning objective, we\ncan directly enforce physical properties in the learnable model, enabling realistic\nand physically feasible trajectory predictions.\n1.1\nContributions\nThe primary contributions of this paper are:\n\u27a4A diffusion-based multi-agent trajectory prediction model that conditions pre-\ndictions on inter-agent interactions and map-based information. It generates\nphysically feasible predictions by the use of differential motion constraints.\n\u27a4A comprehensive evaluation and comparison of the proposed model on two\nlarge-scale real-world datasets, showcasing its effectiveness in handling multi-\nmodality and uncertainty in trajectory predictions.\n\u27a4An investigation into guided sampling based on inter-agent connectivity and\nits effect on the prediction process.\nDiffusion-Based Environment-Aware Trajectory Prediction\n3\nInvestigations were conducted using the highD [36] and rounD [37] datasets.\nImplementations will be made available online.\n2\nRelated Work\nIn this section, we review works that are close to our proposal. For more general\noverviews of trajectory prediction methods, we refer to several extensive surveys\non the topic, e.g., [16,28].\n2.1\nProbabilistic Modeling\nConsidering the complex dynamics of traffic and the multiple possible decisions of\nindividual road users, it is no coincidence that several researchers have employed\napproaches with probabilistic elements toward the solution of the behavior\nprediction problem. One common method is to model future trajectories as a\ndistribution over possible outcomes [13,26,47,48,76,85] using mixture density\nnetworks (MDNs) [5]. Contrasting these works are those that employ generative\napproaches, such as generative adversarial networks (GANs) [19] to implicitly\nencode multimodality [3,21,61,83] or using conditional variational autoencoders\n(CVAEs) [65] to explicitly do so [39,62,78,80].\nGiven the recent success of diffusion models in multiple domains [79], it\nis natural to consider their potential for trajectory prediction. Although there\nare notable examples of using diffusion-based models in temporal data model-\ning [2,57], employing them specifically for motion generation has largely been\nfocused on computer graphics applications [33,58,68,82]. While these works offer\nimportant insights, they are typically single-agent methods and therefore lack\nthe interaction-aware mechanisms that have been the recipe for success in many\nrecent (multi-agent) trajectory prediction approaches. Of particular interest are\ntherefore diffusion models that in addition to modeling temporal mechanisms\nalso incorporate spatial components, e.g., GNNs, such as in [40,43,72].\nDespite recent developments, the research on diffusion-based models tailored\nfor trajectory prediction remains an open area of research, with only a few works\nexploring their potential [7,20,58]. In [20], the authors proposed a Transformer-\nbased diffusion model for pedestrian trajectory prediction. Showing promising\nresults across several pedestrian datasets, they specifically highlight the ability of\nthe model to capture multimodal behavior. Despite these promising results, the\nauthors note the time cost of the reversal process, which is a known issue with\ndiffusion models [32,66] (this was later addressed in the public release of the code).\nIn [7], the authors proposed a diffusion-based model for the prediction of vehicle\ntrajectories on highway driving datasets. The model combines a transformer with\nrecurrent mechanisms and a GNN to capture temporal and spatial dependencies.\nThe authors show that the model outperforms several baselines on short-term\nprediction tasks, but struggles to accurately predict longer trajectories. In [58], a\ndiffusion-based model was proposed for the prediction of pedestrian trajectories\nfor character animation. In the work, the authors proposed a two-stage approach,\n4\nT. Westny et al.\nwhere the first stage incorporates a diffusion-based model that predicts the future\ntrajectory, subsequently used as a plan for a physics-based controller in the\nsecond stage. The key contribution of the work is the added controllability which\nenables guidance at test time.\n2.2\nEnvironment-Aware Modeling\nSince the seminal work in [1], several researchers have targeted the problem of de-\nveloping interaction-aware models with various choices of architectures, including\npooling techniques [1,13,21], convolutional neural networks (CNNs) [12,13,83],\nand attention mechanisms [3,29,44,48,51]. However, the natural representation\nof trajectories as temporal graphs, where nodes represent entities or key locations,\nand edges represent their relationships has enabled GNNs to emerge as the most\ncommonly used technique. In [15], one of the first works to use GNNs for motion\nprediction, the authors investigated selected architectures, presenting promising\nresults in terms of interaction-aware modeling. In response, there have been several\nproposals that include GNNs within the prediction model [31,39,41,62,71,76] with\ngraph convolutional network (GCN) [34] and graph attention network (GAT) [70]\nas some of the most commonly used architectures.\nIncorporating map-based information in trajectory prediction methods, similar\nto inter-agent interactions, can be done in several ways. Many works employ\nCNNs to extract scene features, using rasterized maps [12,47], or high-definition\n(HD) semantic maps [39, 51, 62, 83, 85] as inputs. With the availability of HD\nmaps increasing, there have been several proposals to make effective use of them,\ne.g., representing them as lane graphs [14,18,42,85]. Lane graphs consist of a\nset of nodes representing the scene with additional semantic information, such\nas lane types, lane directions, and lane connectivity. The benefit of using lane\ngraphs is their natural inclusion into the GNN framework\u2014assuring that data\nmodalities are consistently handled. In this work, lane graphs are used as a means\nof incorporating map-based information into the model.\n2.3\nMotion Constraints\nA potential issue with black-box models tasked with motion forecasting is that\nmodel outputs can be physically infeasible or lack the general characteristics of\nnatural motion. Recently, there have been several proposals for incorporating\ndifferential constraints within trajectory prediction models [11,39,62,73,75,76,81],\nrevealing two distinct approaches: one in which the motion constraints are pre-\ndefined [11,39,62] and based on kinematic (or dynamic) single-track models [35]\n[38, p. 613], the second being that the constraints are learned [73,76,81], e.g., using\nneural ODEs [8]. The architecture in [11], one of the first to explicitly incorporate\nkinematic constraints as part of a deep (CNN-based) trajectory prediction model,\nillustrated the potential of the approach. The effect of incorporating differential\nconstraints was investigated in detail in [76], revealing that their inclusion can\nsignificantly stabilize learning and improve prediction performance, most notably\nin terms of extrapolation. This was continued in [75], where the authors evaluated\nDiffusion-Based Environment-Aware Trajectory Prediction\n5\nvarious motion models in the context of learning-based trajectory prediction,\nconcluding that simple dynamic models are preferable over more complex ones,\nand best combined with a 2nd order numerical solver. Importantly, none of the\ndiffusion-based trajectory prediction methods mentioned previously [7, 20, 58]\nexplicitly incorporate physical constraints as part of the learnable model.\nIn the computer-generated motion domain, generative networks are often\nregularized using geometric losses [53,63,68] to encourage natural and coherent\nmotion. Of particular interest here is the diffusion model presented in [68] as they\nmodify the typical denoising objective [23] by not predicting the noise directly but\ninstead the signal itself, meaning they can directly penalize the lack of physical\ncharacteristics in the model output. Adopting a similar adjustment enables the\ninclusion of differential constraints directly in the model, offering the possibility\nof enforcing physically feasible trajectories.\n3\nTrajectory Prediction Model\nThe proposed model, illustrated in Fig. 2, consists of a GNN-based diffusion\nmodel, adapted for multi-agent trajectory prediction. The model should synthesize\nfuture trajectories x1:N\n0\nof length N for all agents \u03bd \u2208V currently in the scene\nat the prediction time instant given the condition c. The condition contains\ninformation about the agents\u2019 past trajectories, inter-agent interactions, and\nmap-based information. For brevity, xt is here on used to denote the full sequence\nat diffusion step t.\n3.1\nEnviroment-Aware Modeling\nInter-agent relationships over n time steps are modeled as sequences G1, . . . , Gn of\ngraphs centered around a vehicle \u03bd0 and each node represents a traffic participant.\nLet Gi = (Vi, Ei) be a graph at time i, where Vi is the node set and Ei is the edge\nset. Additionally, Let f \u03bd\ni \u2208Rdf be the features of node \u03bd \u2208Vi and e\u03bd,\u03c4\ni\n\u2208Rde\nbe the features of edge (\u03bd, \u03c4) \u2208Ei, where the features contain information about\nthe agents\u2019 states, such as position and velocity. It is important to note that\nthe cardinality of the graphs within the observation window h may vary as a\nresult of the arrival and departure of agents. The exact observation history H of\nlength h can be summarized as H = {G1\u2212h, . . . G\u22121, G0}. We note that the exact\ngraph structure is unknown during the decoding process, i.e., over the prediction\nhorizon. During this stage, the model utilizes the last observed graph G0 as a\nproxy for the future graph structures.\nIn addition to inter-agent interactions, we draw inspiration from [14,42] and\nmodel road-agent relationships using lane graphs, denoted by J = (\u02dcV, \u02dcE). In\ncontrast to the inter-agent graphs, the lane graphs are not temporal by nature\nbut instead contain a spatial snapshot of the scene at the prediction time instant.\nThe important distinction, however, is that the lane graphs additionally include\nenvironmental nodes that hold information about road boundaries and lane\nlines. The lane graphs are constructed based on the geometric constraints of\n6\nT. Westny et al.\nFig. 2: Schematic illustration of the proposed model. The model takes as input the\ncondition c = {H, J }, the current latent state xt, and diffusion step t to predict agent\ntrajectories x0. The state history H up until the prediction time instant and the lane\ngraph J are encoded using two GNN modules. The diffusion step t is passed through a\nFourier feature encoder [67] and then summed with the embedded latent state. Next,\nthe sum is passed through two multi-head attention (MHA) mechanisms [69], one for\neach encoded condition part, and the resulting representations are concatenated and\nfused using an MLP to create a combined context encoding. The context encoding\nis then input into a Graph-GRU decoder (together with the last hidden state of the\nencoder) to compute the motion control inputs u. Using the computed control action\nand the agent states xinit\n0\nat the prediction time instant, the predicted trajectories are\nsolved for using numerical integration.\nthe current road topology and connect environmental nodes to agents through\ndirected edges based on a nearest-neighbor criterion. Together, the lane graph\nand the inter-agent graphs form the condition c = {H, J }.\nGraph Attention Network. Both H and J are encoded using GNNs based\non the improved GAT architecture (GATv2) [6]. GAT layers use an attention\nmechanism to compute a set of aggregation weights over the inclusive neighbor-\nhood \u02dcN(\u03bd) = N(\u03bd) \u222a{\u03bd} for each node \u03bd, enabling the GNN to focus more on\nspecific neighbors in the graph. The attention coefficients are computed as\n  \\g a\nttw\n \n{\\tau } = \\ frac {  \\ex p  \\left \n(\n\\\nattn ^\\ tra nspose {} \\l relu \\ left  ( \\weightind {1} \\hidden {}^\\agent + \\weightind {2} \\hidden ^\\neighbor + \\weightind {3} \\ew {\\agent }{\\neighbor } \\right )\\right ) }{ \\sum _{\\upsilon \\in \\ineigh {\\agent {}}} \\exp \\left (\\attn ^\\transpose {} \\lrelu \\left ( \\weightind {1} \\hidden {}^\\agent + \\weightind {2} \\hidden ^\\upsilon + \\weightind {3} \\ew {\\agent }{\\upsilon } \\right )\\right ) }, \n(1)\nwhere a and W(\u00b7) are learnable matrices. The node update rule using the attention\nweights is then given by\n  \\ u p d atedrep {\n}\n = \\bm\n {b} + \\gattw {\\agent }\\weightind {1} \\hidden {}^\\agent {} + \\smashoperator {\\sum _{\\tau \\in \\neigh {\\agent {}}}} \\gattw {\\tau } \\weightind {2} \\hidden {}^\\tau , \n(2)\nDiffusion-Based Environment-Aware Trajectory Prediction\n7\nwhere h\u03bd is the current embedding of node \u03bd, and b is a learnable bias term.\nThe employed GNN, is a small extension of GATv2 where an additional weight\nmatrix W4 is introduced only for the center node in the update step:\n  \\ u p d atedrep  {} = \\\nb\nm {b} \n+ (\\gattw {\\agent }\\weightind {1} + \\weightind {4}) \\hidden {}^\\agent {} + \\smashoperator {\\sum _{\\tau \\in \\neigh {\\agent {}}}} \\gattw {\\tau } \\weightind {2} \\hidden {}^\\tau , \n(3)\nthereby introducing additional flexibility in how the representation of the center\nnode is used.\nGraph-Gated Recurrent Unit. To capture the temporal aspect of the data,\ngraph-gated recurrent units (Graph-GRUs) are adopted. The Graph-GRU is\na recurrent unit that enables spatio-temporal interactions to be captured by\nreplacing the linear mappings in the conventional gated recurrent unit (GRU) [9]\nwith GNN components [49, 84]. The GNNs take as input the representations\nfor the specific node \u03bd and the information of its neighborhood. Intermediate\nrepresentations are computed by two GNNs as\n \n\\l\nabel {\neq:gnn\n_in\nt\ne rm_f\n}\n \\\nl e ft  \n[ \\grur\ne\npx {\nr\n} \n\\| \\gr\nurepx \n{z}\n \n\\ | \\g\nr\nur\nepx \n{\nh}\n \\r\ni\nght \n]\n &= \\gnnf {i}\\\\ \\label {eq:gnn_interm_h} \\left [\\grureph {r} \\| \\grureph {z} \\| \\grureph {h} \\right ] &= \\gnnh {i-1},\n(4b)\nwhere \u2225is the concatenation operation. These are then used to compute the\nembedded state h\u03bd\ni for time step i as\n  \n\\ l abel\n {e q :f\null _ gru\n_upd\nat\ne }  \\bm\n {r } _i\n^\\a g ent\n &= \n\\s\ni g ma (\\gr\nure p x \n{ r } \n+ \\ g rur\neph \n{r\n}  + \\ bm\n { b }_\nr )  \\\n\\  \\b\nm {z}_i^\\agent &= \\sigma (\\grurepx {z} + \\grureph {z} + \\bm {b}_z) \\\\ \\bm {n}_i^\\agent &= \\text {tanh}(\\grurepx {n} + \\bm {r}_i^\\agent \\odot \\grureph {n} + \\bm {b}_{n})\\\\ \\encrep {i} &= (\\bm {1} - \\bm {z}_i^\\agent ) \\odot \\bm {n}_i^\\agent {} + \\bm {z}_i^\\agent \\odot \\encrep {i-1},\n(5d)\nwhere the bias terms br, bz, bn are additional learnable parameters, \u2299the\nHadamard product, and \u03c3 is the sigmoid function. Graph-GRUs are used to\nencode the history H and to decode the motion control inputs u.\n3.2\nDiffusion-Based Forecasting\nThe underlying principle of diffusion models is to progressively perturb the\nobserved data with a forward (diffusion) process, then recover the original data\nthrough a reverse process [23,32,64,66]. These processes are commonly modeled\nthrough two Markov chains (see Fig. 1 for an example of how the process is\nmodeled in this work). To learn the reverse transition, a continuous-time forward\ntransition from x0 to xt is first defined as in [30]:\n  \\\nl\nabel { e\nq\n: f orward-process} \\latent {\\dstep } = \\sqrt {\\schedule (\\dstep )} \\latent {0} + \\sqrt {1 - \\schedule (\\dstep )} \\noise , \n(6)\n8\nT. Westny et al.\nwhere \u03f5 \u223cN(0, I), t \u223cU(0, 1) and \u03b3(t) is a monotonically decreasing noise\nscheduling function. The transition function given by Eq. (6) allows for the\ndirect sampling of xt at any arbitrary diffusion step t without the need for\ncomputation of the forward process step by step. Selecting a sufficiently large\nnumber of diffusion steps T and progressing in the diffusion direction ensures\nthat x0 becomes fully corrupted, i.e., x1 \u223cN(0, I). However, in the context of\nmulti-agent trajectory prediction, it is more effective to instead draw samples\nfrom a distribution that is specific to each agent. To that end, we propose to\nadjust the forward process as\n  \\\nl\nabel { e\nq\n: f\no\nrwar\nd\n-process-mod} \\latent {\\dstep } = \\sqrt {\\schedule (\\dstep )} \\latent {0} + \\left (1 - \\sqrt {\\schedule (\\dstep )}\\right ) \\latent {0}^{\\text {init}} + \\sqrt {1 - \\schedule (\\dstep )} \\noise , t\n(7)\nwhere xinit\n0\nrefers to the agent states at the prediction time instant, the effect\nbeing that x1 \u223cN(xinit\n0\n, I). This method allows the network to tailor the reversal\nof the diffusion process according to the unique characteristics and states of each\nagent.\nObjective. Instead of predicting \u03f5t as formulated in [23,66], the model learns to\npredict the clean signal, i.e., \u02c6x0 = M\u03b8(xt, t, c) using the simple objective [23],\n  \\ma\nt\nhcal {L} = \\min _\\theta \\mathb b  {E}_{ \\l aten\nt {0} \\sim \\posterior (\\latent {0}), \\noise \\sim \\normal (\\zero , \\eye ), \\dstep \\sim \\uniform (0,1)} \\lVert \\latent {0} - \\model (\\latent {\\dstep }, \\dstep , \\cond ) \\rVert ^2_2 \n(8)\nThis adaptation is inspired by the arguments made in [68], where the addition\nof geometric losses is used to enforce physical properties in a neural motion\ngeneration model. Adopting this simple adjustment enables the inclusion of\ndifferential constraints by having the learnable parts of the model M\u03b8 compute\nthe inputs to a motion model, a modification that has been an important part in\nseveral recent trajectory prediction models [11,39,62,75,76].\nReverse Process. To generate samples from a learned model, we follow a series\nof (reverse) state transitions x1 \u2192x1\u2212\u2206\u2192\u00b7 \u00b7 \u00b7 \u2192x0. This is done by iteratively\napplying the denoising function M\u03b8 on each latent state xt and predicting the\nclean signal \u02c6x0. The current prediction is then diffused to xt\u2212\u2206for usage in the\nnext iteration of the reverse process; a procedure which is repeated for T steps\nuntil t = \u03b5, where \u03b5 is a small positive number (typically, \u03b5 \u2208O(10\u22123) [32]).\nThe sampling procedure can be conducted using various transition rules, e.g.,\nDDPM [23], DDIM [66], or EDM [32]. Importantly, we note that the diffusion\nsteps are not necessarily equidistant in time, i.e., \u2206is not constant. In practice,\nwe follow [32] where the time steps are distributed according to\n  \\l a\nb\ne\nl\n {eq :\nt\ni m e-s\nt\ne\np} t _\n{\ni < \\\nds\nt\nep s  } = \\left (\\schedule _{\\rm max}^\\frac {1}{\\rho } + \\frac {i}{\\dsteps - 1}(\\schedule _{\\rm min}^\\frac {1}{\\rho } - \\schedule _{\\rm max}^\\frac {1}{\\rho }) \\right )^\\rho , \\quad t_\\dsteps = 0, \n(9)\nand set \u03c1 = 3, meaning we are taking smaller steps as i approaches T. This\nmodification allows for more fine-grained control of the reversal process near the\nend of the diffusion process.\nDiffusion-Based Environment-Aware Trajectory Prediction\n9\nInteraction-Aware Guidance. An important attribute of trajectory prediction\nmodels is their interaction-aware capabilities. In this work, such interactions\nare modeled through the use of graphs, where node connectivity is described by\nthe edge set E. Removing elements from the edge set is a simple way to disable\nmessage-passing between nodes and offers an interesting approach to investigating\nthe effect of inter-agent interactions on the prediction outcome during test time.\nLet E\u2032 = {(\u03bd, \u03bd) | \u03bd \u2208V} denote the set of all self-loop edges in the graph G and\nc\u2032 = (H\u2032, J ) the corresponding condition where each element in H\u2032 has E\u2032 as\nits edge set. Inspired by classifier-free guidance [24], we seek to investigate how\nnode connectivity can be used to guide the denoising process by modifying the\nprediction rule according to\n  \\ l ab e l { eq:gui da nce - s i gnal} \\p redlatent {0} = (1 - w) \\cdot \\model (\\latent {\\dstep }, \\dstep , \\cond ') + w \\cdot \\model (\\latent {\\dstep }, \\dstep , \\cond ), \n(10)\nwhere w \u2208[0, 1] is a weighting factor that controls the influence of inter-agent\ninteractions on the predicted trajectories. To prevent the model from becoming\noverly dependent on the guidance signal, edges are randomly removed during\ntraining, thus enabling a transition to a guidance-free model during inference.\n3.3\nDifferentially Constrained Predictions\nThe differential constraints are enforced by having the learnable parts of the\nmodel M\u03b8 compute the 2-DOF control inputs u = [ux, uy] to a motion model\nf with continuous-time dynamics. The output trajectory is then obtained by\nsolving an initial value problem using Heun\u2019s 2nd order method [4, p. 78]. For\nour purposes, we mainly employ two different motion models depending on the\nagent under investigation. Wheeled vehicles (cars, trucks, motorcycles, etc.) are\nmodeled as point masses, with longitudinal and lateral acceleration as inputs\n v\\ b eg\nin { sp\nli\nt }  &\n \\ dot {v}_x = u_x \\\\ & \\dot {v}_y = u_y \\\\ & u_x^2 + u_y^2 \\leq (\\mu g)^2, \\end {split}\n(11)\nwhere \u00b5 = 0.7 is the coefficient of road adhesion (slightly below dry asphalt) [77, p.\n25], and g = 9.81 m/s2 is the acceleration due to gravity. The inputs are elliptically\nconstrained in a manner related to the tire friction ellipse [77, p. 45], thereby\naccounting for vehicle capability constraints. For pedestrians, which generally\nhave more complex motion patterns, we apply first-order neural ODEs [8]:\n  \\ begin  {s\npl i t} \\d ot {x} = f_x(\\bm {x}, u_x) \\\\ \\dot {y} = f_y(\\bm {x}, u_y), \\end {split} \\label {eq:pedestrian-ode}\n(12)\nwhere x = [x, y] is the state vector, and fx and fy are MLPs with learnable\nparameters where we decouple the state-transition equations to reduce the model\ncomplexity. Notably, the inputs of Eq. (12) are trained to be close to the ground\ntruth velocities to improve the interpretation of their physical meaning.\n10\nT. Westny et al.\n4\nEvaluations & Results\nThe proposed model is evaluated on two real-world datasets and assessed both\nquantitatively and qualitatively. The metrics used for the evaluation are:\n1. Average Displacement Error (ADE): The average Euclidean distance in meters\nbetween the predicted and ground truth trajectories.\n2. Final Displacement Error (FDE): The Euclidean distance in meters between\nthe predicted and ground truth trajectories at the final time step.\n3. Miss Rate (MR): The percentage of predictions where the predicted trajectory\nat the final time step is not within 2.0 m from the ground truth.\nSince our model has no explicit modes, we take the mean of 6 samples to calculate\nthe metrics in order to make a fair comparison with the baselines. Apart from\nthe proposed model, we evaluate two additional variants:\n(a) The proposed model but without the motion model f (denoted \u2018\u2019\\f\u2018\u2019 in\ntables), i.e., the decoder computes the predicted trajectory directly.\n(b) Like (a) with a closed-loop refinement step (denoted \u2018\u2019\u27f2\u2018\u2019 in tables). Using\nthe output of the diffusion model as a reference trajectory, we apply a simple\nclosed-loop control scheme based on the pure-pursuit algorithm [10] with a\nspeed-dependent lookahead horizon to refine the predicted trajectory (see\nAppendix A.3 for more details).\nDatasets. Experiments are conducted using the highD [36] and rounD [37]\ndatasets. The datasets contain trajectories from various locations in Germany\ncaptured at a frequency of 25 Hz. The data is downsampled by a factor of 5,\nresulting in a sampling time of 0.2 s. The maximum length of the observation\nwindow is set to 3 s (h = 15), while the prediction horizon is 5 s (N = 25). The\npreprocessed highD and rounD datasets consist of 100 404 and 29 248 samples,\nrespectively. We allocate 80% of the total samples for training, 10% for validation,\nand 10% for testing. Contrary to the rounD dataset, the highD dataset does not\ncontain any explicit lane-graph information. In those cases, the model only uses\nthe trajectory information. This also provides an opportunity to investigate the\npractical significance of the map encoder.\nImplementation Details. The model was implemented using the PyTorch\nlibrary [52] and trained using the AdamW optimizer [46] with a learning rate of\n0.0005 and a batch size of 512 on a single NVIDIA GeForce RTX 3090 GPU. The\nlearning rate was decayed using a cosine annealing learning rate scheduler [45].\nThe diffusion process is modeled using a linear scheduling function \u03b3(t) = 1 \u2212t.\nSample generation was conducted using the 1st order method from [32], which\nwe found worked best in practice. Importantly, we only require T = 2 diffusion\nsteps to obtain highly competitive samples which we use for the evaluation.\nDiffusion-Based Environment-Aware Trajectory Prediction\n11\nTable 1: Quantitative performance on the highD [36] (a) and rounD [37] (b) datasets,\nbased on the metrics in [76]. The best results are in bold, and the second best are\nunderlined.\n(a) Highway\nMethod\nADE \u2193\nFDE \u2193\nMR \u2193\nConstant Acc.\n0.78\n2.63\n0.55\nS-LSTM [1]\n0.41\n1.49\n0.22\nCS-LSTM [13]\n0.39\n1.38\n0.19\nGRIP++ [41]\n0.38\n1.49\n0.18\nmmTransformer [44]\n0.39\n1.13\n0.15\nTrajectron++ [62]\n0.44\n1.62\n0.23\nMTP-GO [76]\n0.30\n1.07\n0.13\nOurs (w = 0.0)\n0.39\n1.43\n0.23\nOurs (w = 0.4)\n0.29\n1.16\n0.15\nOurs (w = 0.8)\n0.29\n1.01\n0.12\nOurs (w = 1.0)\n0.28\n0.99\n0.11\nOurs\\f (w = 1.0)\n0.46\n1.27\n0.16\nOurs\u27f2(w = 1.0)\n0.36\n1.25\n0.17\n(b) Roundabout\nMethod\nADE \u2193\nFDE \u2193\nMR \u2193\nConstant Acc.\n4.83\n16.2\n0.95\nS-LSTM [1]\n1.20\n3.47\n0.56\nCS-LSTM [13]\n1.19\n3.57\n0.60\nGRIP++ [41]\n1.11\n3.19\n0.52\nmmTransformer [44]\n1.29\n3.50\n0.59\nTrajectron++ [62]\n1.09\n3.53\n0.54\nMTP-GO [76]\n0.96\n2.95\n0.46\nOurs (w = 0.0)\n1.23\n3.99\n0.64\nOurs (w = 0.4)\n1.01\n3.31\n0.57\nOurs (w = 0.8)\n0.86\n2.82\n0.49\nOurs (w = 1.0)\n0.84\n2.73\n0.46\nOurs\\f (w = 1.0)\n0.96\n3.10\n0.52\nOurs\u27f2(w = 1.0)\n0.96\n2.99\n0.49\n4.1\nQuantitative Results\nThe quantitative results are presented in Tab. 1. Performance is reported for\ndifferent values of the weighting factor w from Eq. (10), used to highlight the\nimportance of including inter-agent interactions.\nThe proposed model outperforms several established methods on both datasets,\nachieving the lowest ADE, FDE, and MR scores. Interestingly, the model performs\nwell on the highD dataset, despite the lack of map information. Compared to the\nperformance reported in [7] that struggled with making accurate predictions on\nlong-term trajectories using a diffusion-based model, our method does not share\nthe same limitations. This is likely a result of the incorporation of differential\nconstraints as this helps guide the model to generate more realistic and high-\nquality samples. This is further corroborated by the results of the model that\nexcludes the motion model f, which performs much worse than its counterpart.\nInspired by the diffusion model in [58], we also include a variant of our model\nthat uses a closed-loop refinement step. The results show that the refinement\nstep has a slight positive effect on the prediction accuracy compared to the model\nwithout f, which is more pronounced on the highway dataset, but still not enough\nto outperform the proposed architecture. A notable drawback of this approach\nis the added complexity of adding a closed-loop control scheme, which requires\nadditional tuning and testing depending on the specific application.\nIt is worth noting that the model outperforms MTP-GO [76] on both datasets,\nwhich is a strong baseline for trajectory prediction and shares many architectural\nsimilarities with our model. We hypothesize that this could be owing to our\nchoice of motion model, which has a more physically grounded objective.\n12\nT. Westny et al.\nFig. 3: Three example predictions on the highD [36] test set. The vehicles in the\nplots are used to represent the agents in the scene at the prediction time instant. The\npredicted trajectories are shown with the same colored scatter plots as the agent under\ninvestigation, while the ground truth is shown with a solid black line. For visual clarity,\nwe only show the predicted trajectories for vehicles performing a lane change (the model\npredicts lane-keeping maneuvers with equal accuracy).\nFig. 4: Three example predictions on the rounD [37] test set. The predicted trajectories\nare shown with the same colored scatter plots as the agent under investigation, while\nthe ground truth is shown with a solid black line. What is interesting to note is the\naccurate prediction of the pedestrian in the rightmost plot. Since the state-transition\ndynamics are parametrized using neural ODEs, it illustrates that the model has learned\na motion model that is representative of the real-world data.\n4.2\nQualitative Assessment\nGiven the encouraging performance shown in Sec. 4.1, it is interesting to investi-\ngate how predictions look visually. In Fig. 3, we present example predictions using\nthe highway data. Even though the average predicted distance is 200\u2013300 m ahead\nof the observed time instant, the predictions are highly accurate and closely in line\nwith the ground truth trajectories. Interestingly, although the dataset contains a\nlarge number of vehicles performing lane-keeping maneuvers, the model predicts\nlane changes with equal accuracy. In Fig. 4, we present representative predictions\nusing the roundabout data. The model is mostly accurate in predicting future\ntrajectories, even when various agents are involved.\nDiffusion-Based Environment-Aware Trajectory Prediction\n13\nFig. 5: Three example predictions on the rounD [37] test set. The multiple predicted\ntrajectories for a specific agent are shown with same-colored scatter plots. The scenarios\nwere chosen to illustrate the model\u2019s ability to predict a diverse set of future trajectories.\nEach prediction is based on a unique sample from the diffusion process, thereby\nshowcasing that the model has implicitly learned to capture the multimodal nature of\nthe data.\nMultimodality. In Fig. 5, three example predictions using roundabout data\nare presented\u2014showcasing the model\u2019s ability to provide multimodal predictions.\nAlthough the model has no explicit representation of modes, it is interesting to see\nthat the model can still generate multiple candidate trajectories given the same\ncondition. We note that to generate a diverse set of predictions, the model needs\nto be sampled multiple times following the reverse process outlined in Sec. 3.2,\nwhich is a limitation that might need to be addressed in future work. However,\nour model still only requires very few diffusion steps to generate high-quality\nsamples. It should also be noted that the multimodal nature of the predictions\nwas found to be more pronounced in the roundabout scenarios. This is likely due\nto the increased complexity of the scenarios and the larger number of possible\nfuture trajectories given the same condition.\nInteraction-Aware Guidance. Originally, guidance-based sampling [24] was\nproposed to enable diffusion models that could generate both unconditional\nand conditional samples based on the availability of a guidance signal (typically\nimage labels). In this work, we investigate how node connectivity can guide the\nprediction process. When w = 0.0, the model only uses graphs with self-loops,\neffectively computing predictions based on the individual agent\u2019s trajectory. When\nw = 1.0, the model instead considers the original graph structure. The test results\nshow that using w < 1.0 worsens prediction performance, arguably due to its\nusefulness in modeling inter-agent interactions. Still, we found that removing edges\nfrom the graphs during training improved validation accuracy, a generalization\neffect that has been observed in other graph-based models [60]. However, when\nobserving the trajectories for w < 1.0, we found; perhaps unsurprisingly, that\nthe model generates predictions that more so reflect a behavior that is more\ncentered around the individual goal of the agent in question, as is illustrated in\nFig. 6. We hypothesize that this could be used for predicting the behavior of less\ncooperative agents or drivers with a more aggressive driving style\u2014potentially of\ninterest in the context of robust motion planning.\n14\nT. Westny et al.\nFig. 6: Example predictions on the rounD [36] dataset for different values of interaction-\naware guidance signal strength w, where w = 0.0 removes all node-to-node message\npassing, while w = 1.0 preserves the original graph structure. As was discussed in\nSec. 4.1, letting w < 1.0 has limited practical use when assessing overall prediction\naccuracy. However, as the circles highlight in the figure, removing inter-agent interactions\ngives rise to more egocentric predictions: agents that typically slow down to yield to\nother agents in the scene, now enter into the roundabout without any regard for other\nagents. Vehicles that are already in the roundabout are seemingly unaffected by this\nmodification which is supported by the traffic rules, something the model has learned.\n5\nConclusions\nIn this paper, we have proposed a generative model for trajectory prediction\nbased on the diffusion process. The model is capable of capturing the complex\ninteractions between traffic participants and the environment, accurately learning\nthe multimodal nature of the data. The effectiveness of the approach was assessed\non large-scale datasets of real-world traffic scenarios, showing that our model\noutperforms several well-established methods in terms of prediction accuracy.\nBy the incorporation of differential constraints, we illustrate that our model\nis capable of generating a diverse set of realistic future trajectories, which is\nessential for the safe and efficient operation of autonomous vehicles. Finally,\nUsing a guidance signal based on node connectivity, we demonstrated that the\nmodel can be steered to generate predictions that are more or less influenced\nby the inter-agent interaction\u2014an attribute we hypothesize to be beneficial in\nvarious real-world applications.\nAcknowledgements\nThis research was supported by the Strategic Research Area at Link\u00f6ping-Lund in\nInformation Technology (ELLIIT) and the Wallenberg AI, Autonomous Systems\nand Software Program (WASP) funded by the Knut and Alice Wallenberg\nFoundation. Computations were enabled by the Berzelius resource provided\nby the Knut and Alice Wallenberg Foundation at the National Supercomputer\nCentre.\nDiffusion-Based Environment-Aware Trajectory Prediction\n15\nReferences\n1. Alahi, A., Goel, K., Ramanathan, V., Robicquet, A., Fei-Fei, L., Savarese, S.:\nSocial LSTM: Human trajectory prediction in crowded spaces. In: Conference on\nComputer Vision and Pattern Recognition (CVPR). pp. 961\u2013971. IEEE (2016)\n2. Alcaraz, J.M.L., Strodthoff, N.: Diffusion-based time series imputation and fore-\ncasting with structured state space models. Transactions on Machine Learning\nResearch (2023)\n3. Amirian, J., Hayet, J.B., Pettr\u00e9, J.: Social ways: Learning multi-modal distributions\nof pedestrian trajectories with GANs. In: Conference on Computer Vision and\nPattern Recognition (CVPR) Workshops. IEEE/CVF (2019)\n4. Ascher, U.M., Petzold, L.R.: Computer methods for ordinary differential equations\nand differential-algebraic equations, vol. 61. SIAM (1998)\n5. Bishop, C.M.: Mixture density networks. Tech. rep., Aston University (1994)\n6. Brody, S., Alon, U., Yahav, E.: How attentive are graph attention networks? In:\nInternational Conference on Learning Representations (ICLR) (2022)\n7. Chen, K., Chen, X., Yu, Z., Zhu, M., Yang, H.: Equidiff: A conditional equivariant\ndiffusion model for trajectory prediction. In: 26th International Conference on\nIntelligent Transportation Systems (ITSC). pp. 746\u2013751. IEEE (2023)\n8. Chen, R.T.Q., Rubanova, Y., Bettencourt, J., Duvenaud, D.K.: Neural ordinary\ndifferential equations. In: Advances in Neural Information Processing Systems\n(NeurIPS) (2018)\n9. Cho, K., van Merri\u00ebnboer, B., Bahdanau, D., Bengio, Y.: On the properties of\nneural machine translation: Encoder\u2013decoder approaches. In: Proceedings of SSST-\n8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation.\npp. 103\u2013111 (2014)\n10. Coulter, R.C.: Implementation of the pure pursuit path tracking algorithm. Tech.\nrep., Carnegie-Mellon UNIV Pittsburgh PA Robotics INST (1992)\n11. Cui, H., Nguyen, T., Chou, F.C., Lin, T.H., Schneider, J., Bradley, D., Djuric, N.:\nDeep kinematic models for kinematically feasible vehicle trajectory predictions. In:\nInternational Conference on Robotics and Automation (ICRA). pp. 10563\u201310569.\nIEEE (2020)\n12. Cui, H., Radosavljevic, V., Chou, F.C., Lin, T.H., Nguyen, T., Huang, T.K.,\nSchneider, J., Djuric, N.: Multimodal trajectory predictions for autonomous driving\nusing deep convolutional networks. In: International Conference on Robotics and\nAutomation (ICRA). pp. 2090\u20132096. IEEE (2019)\n13. Deo, N., Trivedi, M.M.: Convolutional social pooling for vehicle trajectory prediction.\nIn: Conference on Computer Vision and Pattern Recognition (CVPR) Workshops.\npp. 1468\u20131476. IEEE (2018)\n14. Deo, N., Wolff, E., Beijbom, O.: Multimodal trajectory prediction conditioned\non lane-graph traversals. In: Conference on Robot Learning. pp. 203\u2013212. PMLR\n(2022)\n15. Diehl, F., Brunner, T., Le, M.T., Knoll, A.: Graph neural networks for modelling\ntraffic participant interaction. In: Intelligent Vehicles Symposium (IV). pp. 695\u2013701.\nIEEE (2019)\n16. Fang, W., Wang, F., Shen, P., Zheng, Z., Xue, J., Chua, T.s.: Behavioral intention\nprediction in driving scenes: A survey. arXiv preprint arXiv:2211.00385 (2022)\n17. Fey, M., Lenssen, J.E.: Fast graph representation learning with PyTorch Geometric.\nIn: ICLR Workshop on Representation Learning on Graphs and Manifolds (2019)\n16\nT. Westny et al.\n18. Gao, X., Jia, X., Li, Y., Xiong, H.: Dynamic scenario representation learning for\nmotion forecasting with heterogeneous graph convolutional recurrent networks.\nIEEE Robotics and Automation Letters 8(5), 2946\u20132953 (2023)\n19. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,\nS., Courville, A., Bengio, Y.: Generative adversarial nets. In: Advances in neural\ninformation processing systems (NeurIPS) (2014)\n20. Gu, T., Chen, G., Li, J., Lin, C., Rao, Y., Zhou, J., Lu, J.: Stochastic trajectory\nprediction via motion indeterminacy diffusion. In: Conference on Computer Vision\nand Pattern Recognition (CVPR). pp. 17113\u201317122. IEEE/CVF (2022)\n21. Gupta, A., Johnson, J., Fei-Fei, L., Savarese, S., Alahi, A.: Social GAN: Socially\nacceptable trajectories with generative adversarial networks. In: Conference on\nComputer Vision and Pattern Recognition (CVPR). IEEE (2018)\n22. Hendrycks, D., Gimpel, K.: Gaussian error linear units (GELUs). arXiv preprint\narXiv:1606.08415 (2016)\n23. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: Advances\nin Neural Information Processing Systems (NeurIPS). pp. 6840\u20136851 (2020)\n24. Ho, J., Salimans, T.: Classifier-free diffusion guidance. In: NeurIPS Workshop on\nDeep Generative Models and Downstream Applications (2021)\n25. Hoogeboom, E., Satorras, V.G., Vignac, C., Welling, M.: Equivariant diffusion\nfor molecule generation in 3D. In: International conference on machine learning\n(ICML). pp. 8867\u20138887. PMLR (2022)\n26. Hu, Y., Zhan, W., Tomizuka, M.: Probabilistic prediction of vehicle semantic\nintention and motion. In: Intelligent Vehicles Symposium (IV). pp. 307\u2013313. IEEE\n(2018)\n27. Hu, Y., Zhan, W., Tomizuka, M.: Scenario-transferable semantic graph reasoning\nfor interaction-aware probabilistic prediction. IEEE Transactions on Intelligent\nTransportation Systems 23(12), 23212\u201323230 (2022)\n28. Huang, Y., Du, J., Yang, Z., Zhou, Z., Zhang, L., Chen, H.: A survey on trajectory-\nprediction methods for autonomous driving. IEEE Transactions on Intelligent\nVehicles (2022)\n29. Huang, Z., Mo, X., Lv, C.: Multi-modal motion prediction with transformer-based\nneural network for autonomous driving. In: International Conference on Robotics\nand Automation (ICRA). pp. 2605\u20132611. IEEE (2022)\n30. Jabri, A., Fleet, D., Chen, T.: Scalable adaptive computation for iterative generation.\nIn: International Conference on Machine Learning (ICML). PMLR (2023)\n31. Jeon, H., Choi, J., Kum, D.: SCALE-Net: Scalable vehicle trajectory prediction\nnetwork under random number of interacting vehicles via edge-enhanced graph\nconvolutional neural network. In: International Conference on Intelligent Robots\nand Systems (IROS). pp. 2095\u20132102. IEEE/RSJ (2020)\n32. Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusion-\nbased generative models. In: Advances in Neural Information Processing Systems\n(NeurIPS). pp. 26565\u201326577 (2022)\n33. Kim, J., Kim, J., Choi, S.: FLAME: Free-form language-based motion synthesis\n& editing. In: AAAI Conference on Artificial Intelligence. vol. 37, pp. 8255\u20138263\n(2023)\n34. Kipf, T.N., Welling, M.: Semi-supervised classification with graph convolutional\nnetworks. In: International Conference on Learning Representations (ICLR) (2017)\n35. Kong, J., Pfeiffer, M., Schildbach, G., Borrelli, F.: Kinematic and dynamic vehicle\nmodels for autonomous driving control design. In: Intelligent Vehicles Symposium\n(IV). pp. 1094\u20131099. IEEE (2015)\nDiffusion-Based Environment-Aware Trajectory Prediction\n17\n36. Krajewski, R., Bock, J., Kloeker, L., Eckstein, L.: The highD dataset: A drone\ndataset of naturalistic vehicle trajectories on german highways for validation of\nhighly automated driving systems. In: 21st International Conference on Intelligent\nTransportation Systems (ITSC). pp. 2118\u20132125. IEEE (2018)\n37. Krajewski, R., Moers, T., Bock, J., Vater, L., Eckstein, L.: The rounD dataset:\nA drone dataset of road user trajectories at roundabouts in germany. In: 23rd\nInternational Conference on Intelligent Transportation Systems (ITSC). pp. 1\u20136.\nIEEE (2020)\n38. Lavalle, S.M.: Planning algorithms. Cambridge university press (2006)\n39. Li, J., Ma, H., Zhang, Z., Li, J., Tomizuka, M.: Spatio-temporal graph dual-attention\nnetwork for multi-agent prediction and tracking. IEEE Transactions on Intelligent\nTransportation Systems 23(8), 10556\u201310569 (2022)\n40. Li, R., Li, X., Gao, S., Choy, S.B., Gao, J.: Graph convolution recurrent denoising\ndiffusion model for multivariate probabilistic temporal forecasting. In: International\nConference on Advanced Data Mining and Applications. pp. 661\u2013676. Springer\n(2023)\n41. Li, X., Ying, X., Chuah, M.C.: GRIP++: Enhanced graph-based interaction-aware\ntrajectory prediction for autonomous driving. arXiv preprint arXiv:1907.07792\n(2019)\n42. Liang, M., Yang, B., Hu, R., Chen, Y., Liao, R., Feng, S., Urtasun, R.: Learning\nlane graph representations for motion forecasting. In: European Conference on\nComputer Vision (ECCV). pp. 541\u2013556. Springer (2020)\n43. Liu, M., Huang, H., Feng, H., Sun, L., Du, B., Fu, Y.: PriSTI: A conditional\ndiffusion framework for spatiotemporal imputation. In: International Conference on\nData Engineering (ICDE). vol. 39, pp. 1927\u20131939. IEEE Computer Society (2023)\n44. Liu, Y., Zhang, J., Fang, L., Jiang, Q., Zhou, B.: Multimodal motion prediction with\nstacked transformers. In: Conference on Computer Vision and Pattern Recognition\n(CVPR). pp. 7577\u20137586. IEEE/CVF (2021)\n45. Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm restarts.\nIn: International Conference on Learning Representations (ICLR) (2017)\n46. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. In: International\nConference on Learning Representations (ICLR) (2019)\n47. Messaoud, K., Deo, N., Trivedi, M.M., Nashashibi, F.: Trajectory prediction for\nautonomous driving based on multi-head attention with joint agent-map represen-\ntation. In: Intelligent Vehicles Symposium (IV). pp. 165\u2013170. IEEE (2021)\n48. Messaoud, K., Yahiaoui, I., Verroust-Blondet, A., Nashashibi, F.: Attention based\nvehicle trajectory prediction. IEEE Transactions on Intelligent Vehicles 6(1), 175\u2013\n185 (2021)\n49. Oskarsson, J., Sid\u00e9n, P., Lindsten, F.: Temporal graph neural networks for irregular\ndata. In: International Conference on Artificial Intelligence and Statistics (AISTATS)\n(2023)\n50. Paden, B., \u010c\u00e1p, M., Yong, S.Z., Yershov, D., Frazzoli, E.: A survey of motion\nplanning and control techniques for self-driving urban vehicles. IEEE Transactions\non Intelligent Vehicles 1(1), 33\u201355 (2016)\n51. Park, S.H., Lee, G., Seo, J., Bhat, M., Kang, M., Francis, J., Jadhav, A., Liang, P.P.,\nMorency, L.P.: Diverse and admissible trajectory forecasting through multimodal\ncontext understanding. In: European Conference on Computer Vision (ECCV). pp.\n282\u2013298. Springer (2020)\n52. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T.,\nLin, Z., Gimelshein, N., Antiga, L., et al.: PyTorch: An imperative style, high-\n18\nT. Westny et al.\nperformance deep learning library. In: Advances in neural information processing\nsystems (NeurIPS) (2019)\n53. Petrovich, M., Black, M.J., Varol, G.: Action-conditioned 3D human motion synthe-\nsis with transformer VAE. In: IEEE/CVF International Conference on Computer\nVision (ICCV). pp. 10985\u201310995 (2021)\n54. Pronovost, E., Ganesina, M.R., Hendy, N., Wang, Z., Morales, A., Wang, K., Roy,\nN.: Scenario Diffusion: Controllable driving scenario generation with diffusion. In:\nAdvances in Neural Information Processing Systems (NeurIPS) (2023)\n55. Rahmani, S., Baghbani, A., Bouguila, N., Patterson, Z.: Graph neural networks\nfor intelligent transportation systems: A survey. IEEE Transactions on Intelligent\nTransportation Systems 24(8), 8846\u20138885 (2023)\n56. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. arXiv preprint arXiv:2204.06125\n(2022)\n57. Rasul, K., Seward, C., Schuster, I., Vollgraf, R.: Autoregressive denoising diffu-\nsion models for multivariate probabilistic time series forecasting. In: International\nConference on Machine Learning (ICML). pp. 8857\u20138868. PMLR (2021)\n58. Rempe, D., Luo, Z., Bin Peng, X., Yuan, Y., Kitani, K., Kreis, K., Fidler, S.,\nLitany, O.: Trace and pace: Controllable pedestrian animation via guided trajectory\ndiffusion. In: Conference on Computer Vision and Pattern Recognition (CVPR).\npp. 13756\u201313766. IEEE/CVF (2023)\n59. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution im-\nage synthesis with latent diffusion models. In: IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR). pp. 10684\u201310695 (2022)\n60. Rong, Y., Huang, W., Xu, T., Huang, J.: DropEdge: Towards deep graph convolu-\ntional networks on node classification. In: International Conference on Learning\nRepresentations (ICLR) (2020)\n61. Sadeghian, A., Kosaraju, V., Sadeghian, A., Hirose, N., Rezatofighi, H., Savarese,\nS.: SoPhie: An attentive GAN for predicting paths compliant to social and physical\nconstraints. In: Conference on Computer Vision and Pattern Recognition (CVPR).\npp. 1349\u20131358. IEEE/CVF (2019)\n62. Salzmann, T., Ivanovic, B., Chakravarty, P., Pavone, M.: Trajectron++:\nDynamically-feasible trajectory forecasting with heterogeneous data. In: European\nConference on Computer Vision (ECCV). pp. 683\u2013700. Springer (2020)\n63. Shi, M., Aberman, K., Aristidou, A., Komura, T., Lischinski, D., Cohen-Or, D.,\nChen, B.: MotioNet: 3D human motion reconstruction from monocular video with\nskeleton consistency. ACM Transactions on Graphics (TOG) 40(1), 1\u201315 (2020)\n64. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsupervised\nlearning using nonequilibrium thermodynamics. In: International Conference on\nMachine Learning (ICML). pp. 2256\u20132265. PMLR (2015)\n65. Sohn, K., Lee, H., Yan, X.: Learning structured output representation using deep\nconditional generative models. In: Advances in neural information processing systems\n(NeurIPS) (2015)\n66. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: International\nConference on Learning Representations (ICLR) (2021)\n67. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-\nbased generative modeling through stochastic differential equations. In: International\nConference on Learning Representations (ICLR) (2021)\n68. Tevet, G., Raab, S., Gordon, B., Shafir, Y., Cohen-or, D., Bermano, A.H.: Human\nmotion diffusion model. In: International Conference on Learning Representations\n(ICLR) (2023)\nDiffusion-Based Environment-Aware Trajectory Prediction\n19\n69. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\nL.u., Polosukhin, I.: Attention is all you need. In: Advances in Neural Information\nProcessing Systems (NeurIPS) (2017)\n70. Veli\u010dkovi\u0107, P., Cucurull, G., Casanova, A., Romero, A., Li\u00f2, P., Bengio, Y.: Graph\nattention networks. In: International Conference on Learning Representations\n(ICLR) (2018)\n71. Wang, Z., Zhang, J., Chen, J., Zhang, H.: Spatio-temporal context graph trans-\nformer design for map-free multi-agent trajectory prediction. IEEE Transactions\non Intelligent Vehicles (2023)\n72. Wen, H., Lin, Y., Xia, Y., Wan, H., Zimmermann, R., Liang, Y.: DiffSTG: Prob-\nabilistic spatio-temporal graph forecasting with denoising diffusion models. In:\nInternational Conference on Advances in Geographic Information Systems (2023)\n73. Wen, S., Wang, H., Metaxas, D.: Social ODE: Multi-agent trajectory forecasting\nwith neural ordinary differential equations. In: European Conference on Computer\nVision (ECCV). pp. 217\u2013233. Springer (2022)\n74. Westny, T., Frisk, E., Olofsson, B.: Vehicle behavior prediction and generalization\nusing imbalanced learning techniques. In: International Conference on Intelligent\nTransportation Systems (ITSC). pp. 2003\u20132010. IEEE (2021)\n75. Westny, T., Oskarsson, J., Olofsson, B., Frisk, E.: Evaluation of differentially\nconstrained motion models for graph-based trajectory prediction. In: Intelligent\nVehicles Symposium (IV). IEEE (2023)\n76. Westny, T., Oskarsson, J., Olofsson, B., Frisk, E.: MTP-GO: Graph-based proba-\nbilistic multi-agent trajectory prediction with neural ODEs. IEEE Transactions on\nIntelligent Vehicles 8(9), 4223\u20134236 (2023)\n77. Wong, J.Y.: Theory of ground vehicles. John Wiley & Sons, 5th edn. (2022)\n78. Xu, P., Hayet, J.B., Karamouzas, I.: SocialVAE: Human trajectory prediction\nusing timewise latents. In: European Conference on Computer Vision (ECCV). pp.\n511\u2013528. Springer (2022)\n79. Yang, L., Zhang, Z., Song, Y., Hong, S., Xu, R., Zhao, Y., Zhang, W., Cui, B.,\nYang, M.H.: Diffusion models: A comprehensive survey of methods and applications.\nACM Computing Surveys 56(4), 1\u201339 (2023)\n80. Yuan, Y., Weng, X., Ou, Y., Kitani, K.M.: Agentformer: Agent-aware transform-\ners for socio-temporal multi-agent forecasting. In: International Conference on\nComputer Vision (ICCV). pp. 9813\u20139823. IEEE/CVF (2021)\n81. Yue, J., Manocha, D., Wang, H.: Human trajectory prediction via neural social\nphysics. In: European Conference on Computer Vision (ECCV). pp. 376\u2013394.\nSpringer (2022)\n82. Zhang, M., Cai, Z., Pan, L., Hong, F., Guo, X., Yang, L., Liu, Z.: MotionDif-\nfuse: Text-driven human motion generation with diffusion model. arXiv preprint\narXiv:2208.15001 (2022)\n83. Zhao, T., Xu, Y., Monfort, M., Choi, W., Baker, C., Zhao, Y., Wang, Y., Wu, Y.N.:\nMulti-agent tensor fusion for contextual trajectory prediction. In: Conference on\nComputer Vision and Pattern Recognition (CVPR). pp. 12126\u201312134. IEEE/CVF\n(2019)\n84. Zhao, X., Chen, F., Cho, J.H.: Deep learning for predicting dynamic uncertain\nopinions in network data. In: International Conference on Big Data (Big Data). pp.\n1150\u20131155. IEEE (2018)\n85. Zhou, Z., Wang, J., Li, Y.H., Huang, Y.K.: Query-centric trajectory prediction. In:\nConference on Computer Vision and Pattern Recognition (CVPR). pp. 17863\u201317873.\nIEEE/CVF (2023)\n20\nT. Westny et al.\nA\nExtended Implementation Details\nAs noted in the main paper, we evaluated the proposed model on two large-scale\nreal-world datasets: highD [36] and rounD [37]. The proposed method and its\nvariants were implemented using the PyTorch library [52] and trained using the\nAdamW optimizer [46] with a learning rate of 0.0005 and a batch size of 512\non a single NVIDIA GeForce RTX 3090 GPU. The learning rate was decayed\nusing a cosine annealing learning rate scheduler [45]. Graph neural network layers\n(GATv2 [6]) were implemented using the PyTorch Geometric library [17]. Detailed\nparameter configurations for the model are provided in Tab. 2. The (diffusion)\ntime embedding module consists of a Fourier feature encoder [67] followed by a\n2-layered MLP and the latent state embedding consists of a single linear layer.\nThe context fusion encoder consists of a 2-layered MLP. The hidden dimension\nof all MLPs (linear layers) is the same as the encoder/decoder hidden dimension.\nWhenever applicable, we used the SiLU (Swish) activation function [22].\nTable 2: Model hyperparameters.\n(a) Highway\nSpecification\nEncoder\nDecoder\nLane\nMHA\nfped\nDim.\n512\n512\n\u2014\n512\n\u2014\nLayers\n1\n1\n\u2014\n\u2014\n\u2014\nHeads\n1\n1\n\u2014\n4\n\u2014\n(b) Roundabout\nSpecification\nEncoder\nDecoder\nLane\nMHA\nfped\nDim.\n128\n128\n128\n128\n32\nLayers\n1\n1\n3\n\u2014\n2\nHeads\n1\n1\n4\n4\n\u2014\nTable 3: Node features used in the experiments. Edge weights were also included in\nthe form of Euclidean distance between connected nodes.\nFeature\nDescription\nUnit\nx\nLongitudinal coordinate\nm\ny\nLateral coordinate\nm\nvx\nInstantaneous longitudinal velocity\nm/s\nvy\nInstantaneous lateral velocity\nm/s\nax\nInstantaneous longitudinal acceleration\nm/s2\nay\nInstantaneous lateral acceleration\nm/s2\n\u03c8\nYaw angle\nrad\nHighway specific [74]\ndy\nLateral deviation from the current lane centerline\n[\u22121, 1]\ndr\nLateral deviation from the road center\n[\u22121, 1]\nRoundabout specific [76]\nr\nEuclidean distance from the roundabout center\nm\n\u03b8\nAngle relative to the roundabout center\nrad\nLane specific\nxJ\nLongitudinal coordinate\nm\nyJ\nLateral coordinate\nm\nDiffusion-Based Environment-Aware Trajectory Prediction\n21\nA.1\nMethods Compared\nThe following models were included in the comparative study:\n\u2725Constant Acc.: Open-loop model assuming constant acceleration.\n\u2725S-LSTM [1]: Uses an encoder\u2013decoder network based on LSTM for trajectory\nprediction. Interactions are encoded using social pooling tensors.\n\u2725CS-LSTM [13]: Similar to S-LSTM, but instead applies CNN to learn inter-\nactions from the pooling tensors.\n\u2725GRIP++ [41]: Encodes interactions using a graph network and generates\ntrajectories with an RNN-based encoder\u2013decoder.\n\u2725mmTransformer [44]: Transformer-based model for multimodal trajectory\nprediction. Interactions are encoded using multiple stacked Transformers.\nGenerates several candidate proposals and selects the most likely one using a\nlearned scoring function.\n\u2725Trajectron++ [62]: GNN-based recurrent model. Performs trajectory predic-\ntion by a CVAE-based model together with hard-coded kinematic constraints.\n\u2725MTP-GO [76]: GNN-based recurrent model. Computes trajectories with an\nMDN-based decoder and learned differential motion constraints.\nAll results were obtained from the extended study and experiments conducted\nin [76] where all methods were modified to make use of the same input features\n(see Tab. 3), including edge weights for graph-based methods. To make a fair\ncomparison, our proposed method did not use the roundabout-specific features\nbut instead the lane graph, with lane specific features as node features.\nA.2\nModeling Practicalities\nRecall that wheeled vehicles (cars, trucks, motorcycles, etc.) in this work are\nmodeled as point masses, with longitudinal and lateral acceleration as inputs\n v\\ b eg\nin { sp\nli\nt }  &\n \\ dot {v}_x = u_x \\\\ & \\dot {v}_y = u_y \\\\ & u_x^2 + u_y^2 \\leq (\\mu g)^2. \\end {split}\n(13)\nIn practice, computed inputs are updated based on a conditional statement that\nutilizes the fact that input vector u can be expressed in a polar coordinate frame\n  \n\\\nrho  &= \n\\sqrt\n { u_x^2 \n+ u\n_y\n^\n2} \\\\ \\phi &= \\arctan \\left (\\frac {u_y}{u_x}\\right ).\n(14b)\nSince the upper bound on the input signals only affects the magnitude \u03c1 of the\ninput vector, the constraint can be enforced by updating it according to\n  \\r\nh\no\n '  \\\nco\nloneqq \\begin {cases} \\rho \\quad &\\rho \\leq \\mu g \\\\ \\mu g \\quad &\\text {otherwise} \\end {cases} \n(15)\n22\nT. Westny et al.\nwhich in practice is achieved by the use of a HardTanh activation function with\nan upper bound of \u00b5g. The (possibly) scaled magnitude is then used to recompute\nthe input signals to the simulation model as\n  u _x  &= \\r\nho ' \n\\c o s (\\phi ) \\\\ u_y &= \\rho ' \\sin (\\phi ),\n(16b)\nthus retaining the direction of the original input vector.\nA.3\nPure-Pursuit Controller\nOne proposed variant of our method incorporates a closed-loop refinement step\nwhere the output of the diffusion model is used as a reference trajectory. The\nclosed-loop control scheme is based on the pure-pursuit algorithm [10] and includes\na speed-dependent lookahead horizon. The method has a long history of success\nin the robotics community, much attributed to its simplicity and satisfactory\nperformance [50]. The algorithm is based on fitting a semi-circle from the vehicle\u2019s\ncurrent configuration to a (pursuit) point on the reference path ahead of the\nvehicle by the lookahead distance l. The curvature c of the circle is given by\n  \\c\nu r\nv a\nture =  \n\\fra c  \n{2  p}{\\lo\nokahea\nd }, \\quad p = \\begin {bmatrix} x_{\\text {ref}} - x \\\\ y_{\\text {ref}} - y \\end {bmatrix}^T \\begin {bmatrix} \\sin (\\psi ) \\\\ \\cos (\\psi ) \\end {bmatrix} \n(17)\nwhere \u03c8 is the yaw angle. In order to adapt the control law to different scenarios,\na variable lookahead that depends on the vehicle\u2019s current speed v was used:\n  \\ l o o k ahead _v = \\lookahead + \\lookaheadconstant \\cdot |v|, \\label {eq:lookahead} v\n(18)\nwhere a is a constant that determines the rate of increase of the lookahead\ndistance with speed v. Using the computed control action, the vehicles\u2019 states\nare simulated forward using a curvature-input unicycle model:\n  \\ be gin {sp\nli t }v \\dot {x\n} & =v\\hat {v} \\cos (\\psi ),\\\\ \\dot {y} &= \\hat {v} \\sin (\\psi ),\\\\ \\dot {\\psi } &= \\hat {v} \\curvature , \\end {split}\n(19)\nwhere \u02c6v is the vehicle\u2019s predicted speed (taken as the norm of the velocities in\n\u02c6x0). The values of l and a were chosen based on a grid search over the validation\nset, taken as 1.5 and 0.5, respectively.\nDiffusion-Based Environment-Aware Trajectory Prediction\n23\nB\nDiffusion Model Transition Rules\nThe formulation of diffusion models can be done differently depending on how the\nforward and reverse processes are modeled: Either through a Markovian chain [23],\na non-Markovian process [66], a stochastic differential equation (SDE) [67], or as\na (deterministic) probability flow ODE [67]. Regardless of the formulation, all\nmodels can be trained using the same forward transition from x0 to xt:\n  \\\nl\natent {\n\\\nd s\nt\nep }\n \n= \\sqrt {\\schedule (\\dstep )} \\latent {0} + \\textcolor {eccvblue}{\\left (1 - \\sqrt {\\schedule (\\dstep )}\\right ) \\latent {0}^{\\text {init}}} + \\sqrt {1 - \\schedule (\\dstep )} \\noise , t\n(20)\nwhere \u03f5 \u223cN(0, I), t \u223cU(0, 1) and \u03b3(t) is a monotonically decreasing noise\nscheduling function. As noted in the main text, we follow a series of (reverse)\nstate transitions x1 \u2192x1\u2212\u2206\u2192\u00b7 \u00b7 \u00b7 \u2192x0 to generate samples from the learned\nmodel. This is done by iteratively applying the denoising function M\u03b8 on each\nlatent state xt and predicting the clean signal \u02c6x0. The sampling procedure can\nbe conducted using various transition rules, e.g., DDPM [23], DDIM [66], or\nEDM [32]. For completeness, a brief overview of these transition rules is provided.\nIn order to express all transition rules according to their original proposal using\nthe same variables, we first define the following quantities:\n  \\ al phab\nart \n{\\ ds\ntep\n } &\\\ncolo\nne qq  \\ schedule (\\dstep ) \\\\ \\alphat {\\dstep } &\\coloneqq \\frac {\\alphabart {\\dstep }}{\\alphabart {\\dstep -\\ddiff }} \\\\ \\betat {\\dstep } &\\coloneqq 1 - \\alphat {\\dstep }.\n(23)\nB.1\nDDPM\nDiffusing \u02c6x0 to retrieve xt\u2212\u2206is achieved by sampling from the posterior distri-\nbution when conditioned on x0,\n  \\posteri or ( \\latent  {\\dst ep - \\d\nd iff } | \\latent {\\dstep }, \\latent {0}) = \\normal (\\latent {\\dstep -\\ddiff }; \\posteriormu (\\latent {\\dstep }, \\latent {0}), \\posteriorvar \\eye ), \n(24)\nwhere the mean and variance of the posterior is given by\n  \\pos ter i\normu (\\l\na t ent\n { \\\ndstep  }, \\la\nt e nt \n{0}\n) \n=  \\ f rac {\n\\ s qrt\n {\\alphabart {\\dstep -\\ddiff }}\\betat {\\dstep }}{1 - \\alphabart {\\dstep }}\\latent {0} + \\frac {\\sqrt {\\alphat {\\dstep }}(1 -\\alphabart {\\dstep -\\ddiff })}{1 - \\alphabart {\\dstep }}\\latent {\\dstep }, \\qquad \\posteriorvar = \\frac {1 - \\alphabart {\\dstep -\\ddiff }}{1 - \\alphabart {\\dstep }} \\betat {\\dstep }. \n(25)\nA notable limitation of the DDPM approach is the prolonged sampling time,\nattributed to the number of reversal steps T needed to produce high-quality sam-\nples. Additionally, given its Markovian nature, time steps are assumed equidistant,\nwhich can lead to a suboptimal reversal process.\nB.2\nDDIM\nTo address the limitations of the DDPM, the DDIM model was proposed [66].\nThe DDIM model introduces a non-Markovian process, allowing for more flexible\ntime steps and an accelerated sampling procedure,\n  \\posteri or ( \\latent  {\\dste p -\\ ddi\nf f } | \\latent {\\dstep }, \\latent {0}) = \\normal (\\latent {\\dstep -\\ddiff }; \\varposteriormu (\\latent {\\dstep }, \\latent {0}), \\varposteriorvar \\eye ), \n(26)\n24\nT. Westny et al.\nwhere\n  \\begi n { s\np\nlit} \\v a\nr\np o steri o rmu\n ( \\l a tent {\n\\d s tep\n },\n \\ la t e nt {0\n} )  &=\n \n\\ s\nqrt\n {\\al\np\nh\na b art  {\\dstep -\\ddiff }}\\latent {0} + \\sqrt {1 - \\alphabart {\\dstep -\\ddiff } - \\varposteriorvar } \\cdot \\frac {\\latent {\\dstep } - \\sqrt {\\alphabart {\\dstep }}\\latent {0}}{\\sqrt {1 - \\alphabart {\\dstep }}} \\\\ \\varposteriorvar &= \\eta ^2 \\frac {1 - \\alphabart {\\dstep -\\ddiff }}{1 - \\alphabart {\\dstep }} \\left (1 -\\frac {\\alphabart {\\dstep }}{\\alphabart {\\dstep -\\ddiff }}\\right ), \\qquad \\eta \\in [0,1] \\end {split}\n(27)\nwhere \u03b7 is a constant used to control the stochasticity of the reverse process\n(setting it to 1 recovers the DDPM).\nB.3\nEDM\nThe EDM [32] sampling procedure relies on the SDEs of Song et al. [67]:\n  \\ b egin {split}  \\t ext { d}\\la te\nn\nt \n{\n\\pm } &= \\u nder bra\nc e {-\\dot {\\ sch edule  }(\\d st e\np\n )\\schedu le \n(\n\\d\ns\ntep ) \\n abla _{\\l atent {}}\\log \\joint (\\latent {} ; \\schedule (\\dstep )) \\ \\text {d}t}_{\\text {probability flow ODE}} \\\\ &\\pm \\underbrace {\\lambda (\\dstep )\\schedule (\\dstep )^2 \\nabla _{\\latent {}}\\log \\joint (\\latent {} ; \\schedule (\\dstep )) \\ \\text {d}t + \\sqrt {2 \\lambda (\\dstep )} \\schedule (\\dstep ) \\ \\text {d}w_{\\dstep }}_{\\text {Langevin diffusion SDE}} \\end {split}\n(28)\nwhere wt is the standard Wiener process and \u03bb(t) expresses the rate at which\nexisting noise is replaced with new noise [32]. The \u00b1 sign in the equation illustrates\nwhich SDE to solve for the forward or reverse process. In [32], a stochastic 2nd\norder solver was proposed, shown to be more efficient than the transition rules\nused in DDPM and DDIM for image generation benchmarks. Although the EDM\nsampler was used in this work to generate trajectories from the learned model,\nthe 2nd order correction step was omitted, a simplification that we found to be\nsufficient for our purposes.\nExperimenting with the three sampling methods outlined above, it was found\nthat using the EDM sampler gave the best prediction performance. Still, the\nDDIM sampler was not far behind (for the same number of reversal steps).\n",
    "2311.01223": "Diffusion Models for Reinforcement Learning: A Survey\nZhengbang Zhu1 , Hanye Zhao1 , Haoran He1 , Yichao Zhong1 ,\nShenyu Zhang1 , Haoquan Guo2 , Tingting Chen2 , Weinan Zhang1\u2217\n1Shanghai Jiao Tong University\n2Shanghai Marine Electronic Equipment Research Institute\n{zhengbangzhu, wnzhang}@sjtu.edu.cn\nAbstract\nDiffusion models surpass previous generative mod-\nels in sample quality and training stability. Recent\nworks have shown the advantages of diffusion mod-\nels in improving reinforcement learning (RL) so-\nlutions. This survey aims to provide an overview\nof this emerging field and hopes to inspire new av-\nenues of research. First, we examine several chal-\nlenges encountered by RL algorithms. Then, we\npresent a taxonomy of existing methods based on\nthe roles of diffusion models in RL and explore how\nthe preceding challenges are addressed. We further\noutline successful applications of diffusion models\nin various RL-related tasks. Finally, we conclude\nthe survey and offer insights into future research\ndirections. We are actively maintaining a GitHub\nrepository for papers and other related resources in\nutilizing diffusion models in RL 1.\n1\nIntroduction\nDiffusion models have emerged as a powerful class of gener-\native models, garnering significant attention in recent years.\nThese models employ a denoising framework that can effec-\ntively reverse a multistep noising process to generate new\ndata [Song et al., 2021].\nIn contrast to earlier generative\nmodels such as Variational Autoencoders (VAE) [Kingma\nand Welling, 2013] and Generative Adversarial Networks\n(GAN) [Goodfellow et al., 2014], diffusion models ex-\nhibit superior capabilities in generating high-quality sam-\nples and demonstrate enhanced training stability.\nConse-\nquently, they have made remarkable strides and achieved\nsubstantial success in diverse domains including computer\nvision [Ho et al., 2020; Lugmayr et al., 2022; Luo and\nHu, 2021], natural language processing [Austin et al., 2021;\nLi et al., 2022], audio generation [Lee and Han, 2021;\nKong et al., 2020], and drug discovery [Xu et al., 2022;\nSchneuing et al., 2022], etc.\nReinforcement learning (RL) [Sutton and Barto, 2018] fo-\ncuses on training agents to solve sequential decision-making\ntasks by maximizing cumulative rewards.\nWhile RL has\n\u2217Corresponding author.\n1https://github.com/apexrl/Diff4RLSurvey\nachieved remarkable successes in various domains [Kober\net al., 2013; Kiran et al., 2021], there are some long-\nstanding challenges.\nSpecifically, despite the considerable\nattention garnered by offline RL for overcoming low sam-\nple efficiency issue in online RL [Kumar et al., 2020; Fu-\njimoto and Gu, 2021], conventional Gaussian policies may\nfail to fit the datasets with complex distributions for their\nrestricted expressiveness. Meanwhile, although experience\nreplay is used to improve sample efficiency [Mnih et al.,\n2013], there is still data scarcity problem in environments\nwith high-dimensional state spaces and complex interaction\npatterns. A common usage of learned dynamic models in\nmodel-based RL is planning in them [Nagabandi et al., 2018;\nSchrittwieser et al., 2020; Zhu et al., 2021], but the per-\nstep autoregressive planning approaches suffer from the com-\npounding error problem [Xiao et al., 2019]. An ideal RL\nalgorithm should be able to learn a single policy to perform\nmultiple tasks and generalize to new environments [Vithay-\nathil Varghese and Mahmoud, 2020; Beck et al., 2023]. How-\never, existing works still struggle in multitask generalizations.\nRecently, there has been a series of works applying diffu-\nsion models in sequential decision-making tasks, with a par-\nticular focus on offline RL. As a representative work, Dif-\nfuser [Janner et al., 2022] fits a diffusion model for trajec-\ntory generation on the offline dataset, and plans desired fu-\nture trajectories by guided sampling. There have been many\nfollowing works where diffusion models behave as differ-\nent modules in the RL pipeline, e.g., replacing conventional\nGaussian policies [Wang et al., 2023], augmenting experi-\nence dataset [Lu et al., 2023b], extracting latent skills [Venka-\ntraman et al., 2023], among others. We also observe that plan-\nning and decision-making algorithms facilitated by diffusion\nmodels perform well in broader applications such as multi-\ntask RL [He et al., 2023a], imitation learning [Hegde et al.,\n2023], and trajectory generation [Zhang et al., 2022]. More\nimportantly, diffusion models have already shed light on re-\nsolving those long-standing challenges in RL owing to their\npowerful and flexible distributional modeling ability.\nThis survey centers its attention on the utilization of dif-\nfusion models in RL, with additional consideration given to\nmethods incorporating diffusion models in the contexts of tra-\njectory generation and imitation learning, primarily due to the\nevident interrelations between these fields. Section 2 elab-\norates on the aforementioned RL challenges, and discusses\narXiv:2311.01223v4  [cs.LG]  23 Feb 2024\nhow diffusion models can help solve each challenge. Sec-\ntion 3 provides a background on the foundations of diffusion\nmodels and also covers two class of methods that are partic-\nularly important in RL-related applications: guided sampling\nand fast sampling. Section 4 illustrates what roles diffusion\nmodels play in RL among existing works. Section 5 discusses\nthe contribution of diffusion models on different RL-related\napplications. Section 6 summarizes the survey with a discus-\nsion on emerging new topics.\n2\nChallenges in Reinforcement Learning\nIn this section, we list four challenges in RL algorithms and\nbriefly discuss why diffusion models can address them.\n2.1\nRestricted Expressiveness in Offline Learning\nDue to the low-sample efficiency, online RL [Sutton and\nBarto, 2018] is challenging to be applied in real-world sce-\nnarios. Offline RL [Fujimoto et al., 2019; Kumar et al., 2020]\nlearns optimal policies from pre-collected datasets without\nenvironmental interaction, which can significantly improve\nsample efficiency. Directly applying off-policy RL to offline\nlearning causes severe extrapolation errors [Fujimoto et al.,\n2019]. Existing works penalize value predictions on out-of-\ndistribution samples [Kumar et al., 2020] or limit the learning\npolicy to be close to the data collecting policy [Kostrikov et\nal., 2021]. However, penalties imposed on the value func-\ntion may result in an overly conservative policy [Lyu et al.,\n2022]; when imposing policy constraints on commonly used\nunimodal Gaussian parameterization, the restricted expres-\nsiveness makes it difficult to fit the possibly diverse dataset.\nReinforcement learning via supervised learning framework\n(RvS) [Schmidhuber, 2019] is another important paradigm in\noffline RL, which eliminates Q-learning thus free of extrapo-\nlation errors. RvS learns a policy conditioned on the observed\nreturns via supervised learning and then conditions it on a\nhigh return to generate desired behaviors [Chen et al., 2021].\nSimilar to policy constraining, RvS requires fitting the en-\ntire dataset. Therefore, the expressiveness of parameterized\npolicies also matters in RvS. Diffusion models have the ca-\npability to represent any normalizable distribution [Neal and\nothers, 2011], with the potential to effectively improve the\nperformance of policy constraining and RvS algorithms on\ncomplex datasets.\n2.2\nData Scarcity in Experience Replay\nOff-policy and offline RL methods use different levels of ex-\nperience replay to improve sample efficiency. Note that ex-\nperience replay in some literature only refers to data reuse\nin off-policy RL. Here, the term broadly refers to updating\nthe current model with rollout data from other policies. Al-\nthough all previous experiences can be used for policy learn-\ning in off-policy RL, the limitation of simulation speed and\nthe potentially huge state and action spaces may still hinder\npolicy optimization. In offline RL, policy learning is more\nlimited by the quality and coverage of the dataset as no fur-\nther interactions are allowed. Inspired by the success of data\naugmentation in computer vision, some works adopt similar\naugmentation in RL to reduce data scarcity. RAD [Laskin et\nal., 2020] uses image augmentation such as random cropping\nor rotation to improve learning efficiency in vision-based RL.\nImre [2021] and Cho et al. [2022] use generative models,\nVAE [Kingma and Welling, 2013] and GAN [Goodfellow et\nal., 2014], to augment the real dataset with generated syn-\nthetic data. However, existing works either lack fidelity when\nusing random augmentation or are limited to simple environ-\nments due to insufficient modeling ability of particular gen-\nerative models, making them difficult to be applied to more\ncomplex tasks. Diffusion models have demonstrated notable\nperformances surpassing previous generative models in high-\nresolution image synthesis [Ho et al., 2020]. When applied\nto RL data, diffusion models are better suited for enhancing\ncomplex interactions.\n2.3\nCompounding Error in Model-based Planning\nMBRL [Luo et al., 2022] fits a dynamic model from online\nrollout data or offline datasets to facilitate decision-making.\nCommon dynamic models mimic single-step state transitions\nand rewards in the dataset. When predicting with a neural\ndynamic model, there could be single-step errors due to the\nlimited data support and stochastic environment transitions.\nCumulative single-step errors can make planned states devi-\nate from the dataset distribution, which causes the compound-\ning error problem when using the model for multistep plan-\nning [Xiao et al., 2019]. In contrast, diffusion models with\npowerful modeling ability of joint distributions can operate\non the trajectory level and plan for multiple time steps si-\nmultaneously, improving temporal consistency and reducing\ncompounding errors.\n2.4\nGeneralization in Multitask Learning\nNormal RL algorithms lack generalization abilities at the\ntask level [Beck et al., 2023]. Even in the same environ-\nment, changing the reward function requires retraining a pol-\nicy from scratch. Existing online multitask RL [Liu et al.,\n2021] works attempt to learn the same policy in different\ntask environments, suffering from conflicting gradients across\nmultiple tasks and low sample efficiency due to pure online\nlearning. Recently, it has been a popular research direction\nto train a high-capacity model on multitask offline datasets\nand then deploy it on new tasks with or without online fine-\ntuning [Taiga et al., 2022]. Transformer-based pre-training\ndecision models like Gato [Reed et al., 2022] excel at mul-\ntitask policy learning. However, they typically require high-\nquality datasets, large parameter sizes, and high training and\ninference costs. In multitask RL, designing an algorithm that\ncan fit mixed-quality multitask datasets and generalize to new\ntasks emerges as a vital issue. As a powerful class of gen-\nerative models, diffusion models can deal with multimodal\ndistributions in multitask datasets, and adapt to new tasks by\nestimating the task distribution.\n3\nFoundations of Diffusion Models\nWe introduce the foundations of diffusion models, including\ntwo prominent formulations and several sampling techniques\nthat are particularly important in RL tasks.\nGaussian Policy\nRandom\u00a0Augmentation\nDiffusion\u00a0Augmentation\nDiffusion Planning\nDiffusion\nModel\n...\n...\n...\n...\n...\nAugmentation\nModel Training\nData\nData\nRandom\nPertubations\nInteract\nData\nCollect\nTrain\nAgent\nReplay\nBuffer\nEnvironment\n...\n...\n...\n...\nAutoregressive\nPlanning\n...\n...\n...\nDiffusion Policy\n...\nFigure 1: An illustration of how diffusion models play a different role in the classic Agent-Environment-Buffer cycle compared to previous\nsolutions. (1) When used as a planner, diffusion models optimize the whole trajectory at each denoising step, whereas the autoregressive\nmodels generate the next-step output only based on previously planned partial subsequences. (2) When used as a policy, diffusion models\ncan model arbitrary action distributions, whereas Gaussian policies can only fit the possibly diversified dataset distribution with unimodal\ndistributions. (3) When used as a data synthesizer, diffusion models augment the dataset with generated data sampled from the learned dataset\ndistribution, whereas augmentation with random perturbations might generate samples that deviate from data samples.\n3.1\nDenoising Diffusion Probabilistic Model\nAssuming that the real data x0 are sampled from an under-\nlying distribution q(x0), DDPM [Ho et al., 2020] utilizes\na parameterized diffusion process, represented as p\u03b8(x0) =\nR\np(xT ) QT\nt=1 p\u03b8(xt\u22121|xt) dx1:T , to model how the pure\nnoise xT = N(0, I) is denoised into real data x0.\nEach\nstep of the diffusion process is represented by xt, with T in-\ndicating the total number of steps. Note that both the dif-\nfusion process and RL involve time steps; thus, we denote\ndiffusion steps as superscripts and RL time steps as sub-\nscripts. The sequence xT :0 is defined as a Markov chain with\nlearned Gaussian transitions characterized by p\u03b8(xt\u22121|xt) =\nN(\u00b5\u03b8(xt, t), \u03a3(xt, t)).\nIf the process is reversed as x0:T ,\neach step is defined by the forward transition q(xt|xt\u22121),\nwhich is formulated as adding Gaussian noise to the data ac-\ncording to a variance schedule \u03b21:T :\nxt =\n\u221a\n\u03b1txt\u22121 +\n\u221a\n1 \u2212\u03b1t\u03f5t ,\n(1)\nwhere \u03b1t = 1 \u2212\u03b2t, \u03f5t \u223cN(0, I). From Eq. (1), we can\nderive a direct mapping from x0 to xt:\nxt =\n\u221a\n\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5(xt, t) ,\nwhere \u00af\u03b1t = Qt\n1 \u03b1i. From Bayes theorem and relation be-\ntween xt and x0, we have\nq(xt\u22121|xt, x0) = N( 1\n\u221a\n\u03b1t (xt \u2212\n\u03b2t\n\u221a\n1 \u2212\u00af\u03b1t \u03f5(xt, t)), \u03b2tI) .\n(2)\nEq. (2) allows us to sample xT from Gaussian noise and de-\nnoise step by step until we obtain x0. However, the noise\n\u03f5(xt, t) is unknown. To address this, a parameterized network\n\u03f5\u03b8 is employed to predict the noise. Ho et al. [2020] propose\nthe following simplified loss function for learning \u03f5\u03b8, which\nis a weighted version of the evidence lower bound (ELBO):\nL(\u03b8) = Ex0,\u03f5,t[\u2225\u03f5 \u2212\u03f5\u03b8(\n\u221a\n\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, t)\u22252] ,\n(3)\nwhere \u03f5 is sampled from N(0, I).\n3.2\nScore-based Generative Models\nSong et al. [2021] extend DDPM to continuous-time diffu-\nsion processes, and the sequence x0, x1, . . . , xT is replaced\nwith a continuous function xt, t \u2208[0, T]. The forwarding\nprocess can be described as a Stochastic Differential Equa-\ntion (SDE):\ndx = f(x, t) dt + g(t) dw ,\nwhere f(x, t) and g(t) are pre-defined functions, and dw is\nthe Brownian motion. According to Langevin dynamics, the\nreverse of the forwarding process is described by a reverse-\ntime SDE:\ndx = [f(x, t) \u2212g2(t)\u2207x log pt(x)] dt + g(t) d \u00afw ,\nwhere \u00afw is the reverse Brown motion, pt(x) is the probability\ndensity of xt, and s(x) = \u2207x log pt(x) is called the score\nfunction of pt(x). In practice, a parameterized score model\ns\u03b8 is adopted to estimate the score function, which can be\ntrained by minimizing\nL(\u03b8) = Ex0,t,xt \u0002\n\u2225s\u03b8(xt, t) \u2212\u2207xt log p(xt|x0)\u22252\n2\n\u0003\n.\n3.3\nGuided Sampling Methods\nGuided sampling from diffusion models considers sampling\nfrom the conditional distribution p(x|y), where y is the de-\nsired attribute of generated samples. Two main categories of\nguidance are classifier-guidance and classifier-free guidance.\nClassifier guidance.\nClassifier guided sampling relies on a\ndifferentiable classifier model p\u03d5(y|x). Specifically, since the\nguidance needs to be performed on each denoising step, the\nclassifier model p(y|xt) is trained on noisy samples of x and\ncorresponding attribute y. The conditional reverse process\ncan be written as\np\u03b8,\u03d5(xt\u22121|xt, y) = Zp\u03b8(xt\u22121|xt)p\u03d5(y|xt\u22121) ,\n(4)\nDiffusion Model\n...\nCondition on Current Step\nGuidance\n...\nIterations\n(a) Planner\nDiffusion\nModel\nIterations\nQ Function(s)\nQ\nLearning\nPolicy\nOptimization\n(b) Policy\nDataset\nModel Training\nDiffusion\nModel\nSynthetic\nDataset\nAgent(s)\nAugmented\nDataset\nTrain\n(c) Data Synthesizer\nFigure 2: Different roles of diffusion models in RL. (a) Diffusion models as the planner. The sampling target is a part of trajectories whose\ncomponents may vary from specific tasks. (b) Diffusion models as the policy. The sampling target is the action conditioned on the state,\nusually guided by the Q-function via policy gradient-style guidance or directly subtracting it from the training objective. (c) Diffusion\nmodels as the data synthesizer. The sampling target is also the trajectory, and both real and synthetic data are used for downstream policy\nimprovement. For better visualizations, we omit the arrows for N denoising iterations in (c) and only show generated synthetic data from\nrandomly sampled noise. Note that there are other roles that are less explored, and we introduce them in Section 4.4.\nwhere Z is the normalization factor. Dhariwal and Nichol\n[2021] approximate Eq. (4) by another Gaussian distribution:\np\u03b8,\u03d5(xt\u22121|xt, y) = N(\u00b5t + w\u03a3tg, \u03a3t) ,\n(5)\nwhere g = \u2207xt log p\u03d5(y|xt)|xt=\u00b5t and w is the guidance\nscale to control the strength of conditions. \u00b5t and \u03a3t are the\nmean and the covariance matrix in Eq. (2), respectively.\nClassifier-free guidance.\nClassifier-free sampling relies on\nan extra conditional noise model \u03f5\u03b8(xt, y, t). In practice, the\nconditional and unconditional models share the same set of\nparameters, and the unconditional model is represented by\nsetting y as a dummy value \u2205.\nHo and Salimans [2022]\nstate that the noise learning target in Eq. (3) is a scaled\nscore function of p(xt), i.e., \u03f5(xt, t) = \u2212\u03c3t\u2207xt log p(xt) and\n\u03c3t = \u221a\u03b2t. By using Bayes theorem, we have\n\u2207xt log p(y|xt) = \u22121/\u03c3t(\u03f5(xt, y, t) \u2212\u03f5(xt, t)) .\nAccording to Eq. (5), we can derive the guided noise predictor\nas \u00af\u03f5\u03b8(xt, y, t) = \u03f5\u03b8(xt, t) \u2212w\u03c3t\u2207xt log p(y|xt). Replacing\nthe score function with the noise model predictions, the noise\nused in classifier-guided sampling can be written as\n\u02c6\u03f5w(xt, y, t) = w\u03f5\u03b8(xt, y, t) + (1 \u2212w)\u03f5\u03b8(xt, t) .\n3.4\nFast Sampling Methods\nVarious fast sampling methods are proposed to overcome the\nprolonged iterative sampling time of diffusion models. We\nsummarize these methods into two categories: those that do\nnot involve learning and those that require extra learning, and\ndescribe representative works in each category.\nLearning-free methods.\nDDIM [Song et al., 2022] is one\nof the seminal works on sampling acceleration. It extends\nDDPM to a non-Markovian formulation by learning another\nMarkov chain q\u03b8(xt\u22121|xt, x0). Some high-order solvers are\nproposed for diffusion sampling, such as DPM-solver [Lu\net al., 2022], which provides an excellent trade-off between\nsample quality and sampling speed. With DDIM as its first-\norder version, DPM-solver boosts the efficiency of solving\nPF-ODE, outperforming common numerical ODE solvers.\nLearning-based methods.\nLearning-based sampling meth-\nods require extra training to obtain a higher sampling effi-\nciency at a slight expense of sampling quality. A recent work,\nTruncate Diffusion Probabilistic Model (TDPM) [Zheng et\nal., 2023], demonstrates that both the noising and denoising\nprocess can be early terminated to reduce the iterative steps.\nMoreover, Watson et al. [2021] propose a strategy to select\nthe best K time steps to maximize the training objective for\nthe DDPMs, which also decreases the denoising steps.\n4\nThe Roles of Diffusion Models in RL\nFig. 1 illustrates how diffusion models play a different role\nin RL compared to previous solutions. Current works apply-\ning diffusion models on RL mainly fall into three categories:\nas planners, as policies, and as data synthesizers. It is essen-\ntial to note that we include methods that generate action-only\nsequences as planners, even though some of the representa-\ntive works have \u201cpolicy\u201d in their names, e.g., Diffusion Pol-\nicy [Chi et al., 2023]. Generating multi-step action sequences\ncan be viewed as planning in action space, and the use of dif-\nfusion models to ensure temporal consistency is similar to\nother planning-based diffusion methods. The following sub-\nsections will illustrate overall frameworks and representative\npapers for each category.\n4.1\nPlanner\nPlanning in RL refers to using a dynamic model to make\ndecisions imaginarily and selecting the appropriate action to\nmaximize cumulative rewards. This process usually explores\nvarious sequences of actions and states, thus improving deci-\nsions over a longer horizon. Planning is commonly used in\nthe MBRL framework with a learned dynamic model. How-\never, the planning sequences are usually simulated autore-\ngressively, which may lead to severe compounding errors,\nespecially in the offline setting due to limited data support.\nDiffusion models offer a promising alternative as they can\ngenerate multi-step planning sequences simultaneously.\nA general framework of diffusion planners is shown in\nFig. 2(a). Diffusion models are designed to generate clips of\nthe trajectory \u03c4 = (s1, a1, r1, . . . , sH, aH, rH), denoted as\nx(\u03c4) = (e1, e2, . . . , eH). H is the planning horizon. Here et\nrepresents the selected elements from (st, at, rt), where vari-\nous choices can be made as et = (st, at) [Janner et al., 2022;\nLiang et al., 2023a; He et al., 2023a], et = (st, at, rt) [He\net al., 2023a; Hu et al., 2023], et = st [Ajay et al., 2023;\nZhu et al., 2023], or et = at [Chi et al., 2023; Li et al.,\n2023b]. RL datasets often contain trajectory data of vary-\ning quality. In order to make the diffusion planner generate\nhigh rewarded trajectories during evaluations, guided sam-\npling techniques are widely adopted. The guidance can be\neither injected in the sampling stage following the classifier\nguided sampling or in both the training and sampling stages\nfollowing the classifier-free sampling.\nWhen deploying the trained diffusion planner for online\nevaluations, fast sampling methods are usually adopted to re-\nduce the inference time. Besides, to ensure the planned trajec-\ntory is congruous with the agent\u2019s current state, before each\ndenoising step, the first h (h \u22651) steps of the noisy trajectory\nare substituted with the h steps of historical states observed\nby the agent. Here, h is a hyperparameter where a larger h\ncan better handle partially observable and non-Markovian set-\ntings but increases the modeling complexity.\n4.2\nPolicy\nBased on whether they rely on a dynamic model for making\ndecisions, RL algorithms can be categorized into MBRL and\nmodel-free RL. Under such classification criteria, using diffu-\nsion models as planners is more akin to MBRL, as the gener-\nated trajectories encapsulate dynamics information. Another\nperspective is that diffusion planner can be seen as a com-\nbination of policy and dynamic model [Janner et al., 2022].\nIn contrast, using diffusion models as policies focuses on im-\nproving existing model-free RL solutions. Section 2.1 states\nthe main drawbacks of current offline policy learning meth-\nods: over-conservatism and lack of expressiveness. Many\nworks use diffusion models as the policy class in model-free\nRL to tackle these problems.\nDiffusion-QL [Wang et al., 2023] first combines the diffu-\nsion policy with the Q learning framework and finds that it\ncan perfectly fit on datasets collected by strong multi-modal\nbehavior policies, where previous distance-based policy reg-\nularization approaches fail. Compared with using diffusion\nmodels as planners, the diffusion target of the diffusion pol-\nicy is simply the action given the current state, as shown in\nFig. 2(b). Suppose the noise predictor is \u03f5\u03b8(ak, k, s) parame-\nterized by \u03b8, and the derived action mean is \u00b5\u03b8(a|s). To guide\nthe model sampling actions that can lead to high returns, it is\nnecessary to take Q(s, a) into consideration. Diffusion-QL\nincludes a weighted Q maximization term into the diffusion\ntraining loss as\nL(\u03b8) = Ek,\u03f5,(s,a)\u223cD[\u2225\u03f5 \u2212\u03f5\u03b8(ak, s, k)\u22252\n2]\n\u2212\n\u03b7\nE(s,a)\u223cD[Q(s, a)] \u00b7 Es\u223cD,a0\u223c\u03c0\u03b8(\u00b7|s)[Q(s, a0)] ,\nwhere \u03b7 is a hyperparameter, and D is the offline dataset.\nSome works [Chen et al., 2023a; Lu et al., 2023a; Hansen-\nEstruch et al., 2023; Kang et al., 2023] construct the policy\nby (advantage) weighted regression as\n\u03c0\u03b8(a|s) \u221d\u00b5\u03b8(a|s) exp(\u03b1Q(s, a)) ,\nwhere \u03b1 is the temperature hyperparameter. Following this,\nChen et al. [2023a] decouple the policy learning into be-\nhavior learning and action evaluation, which allows more\nfreedom in the choice of guidance. They also propose in-\nsample planning for Q-learning, avoiding extrapolation errors\nin previous offline RL methods. CEP [Lu et al., 2023a] fur-\nther extends this framework to sample from the more general\nenergy-guided distribution p(x) \u221dq(x) exp(\u2212\u03b2E(x)). Here,\nE(x) is an energy function, and in the RL setting, it is trained\nvia contrastive learning to match the in-sample softmax of Q\nfunctions. Since there are off-the-shelf Q functions or energy\nfunctions after training, some methods use those functions\nto further improve the sampling quality during evaluations.\nThey first sample multiple candidate actions for a given state\nand use Q or energy values to perform weighted sampling or\njust select the best candidate.\nFast reaction is crucial when deploying policies in online\nenvironments. Therefore, almost all diffusion policies use\nsmaller diffusion steps during sampling, usually about 15\nsteps. ODE solvers such as the DPM-solver [Lu et al., 2022]\nare also used to accelerate sampling [Chen et al., 2023a;\nLu et al., 2023a; Kang et al., 2023; Li et al., 2023c]. Kang\net al. [2023] introduce action approximation, which allows\none-step action sampling in the training stage.\n4.3\nData Synthesizer\nIn addition to fitting multi-modal distributions, a simple and\ncommon use of diffusion models is to create synthetic data,\nwhich has been widely applied in computer vision. There-\nfore, the diffusion model is a natural data synthesizer for RL\ndatasets because data scarcity is a practical concern. To en-\nsure consistency of synthetic data to the environment dynam-\nics, previous data augmentation approaches in RL usually add\nminor perturbations to states and actions [Sinha et al., 2021].\nIn contrast, Fig. 2(c) illustrates that diffusion models generate\ndiverse and consistent data by learning the data distribution\nfrom the entire dataset Dreal. The diffusion model first learns\nthe parameterized data distribution \u03c1\u03b8(\u03c4) from the real data\nDreal, and generates desired synthetic data by\nDsyn = {\u03c4 \u223c\u03c1\u03b8(\u03c4)} .\nThen, real and synthetic data are combined together as\nD = Dreal \u222aDsyn ,\nand D is used for policy learning. In online settings, the pol-\nicy interacts with the environment, collects more real data\ninto Dreal, and updates the diffusion model. As a result, the\ndiffusion model and the policy are updated alternately.\n4.4\nOthers\nBesides the primary directions discussed above, other ways of\nimproving RL with diffusion models are less explored. Ma-\nzoure et al. [2023] estimate value functions with diffusion\nmodels by learning the discounted state occupancy, combined\nwith a learned reward estimator. Then, the value function can\nbe directly computed by definition, where future states are\nsampled from the diffusion model. Venkatraman et al. [2023]\nfirst encode the high-level trajectories into semantically rich\nlatent representations, then apply diffusion models to learn\nthe latent distribution. Conditioning on latent representations\nimproves the capability of Q-functions and policies without\nsignificant extrapolation errors. Rigter et al. [2023] use a dif-\nfusion dynamic model and allow an online RL policy to col-\nlect synthetic trajectories on it. The interplay of the diffusion\ndynamic model and RL policy is done by alternating between\nstate denoising and Langevin dynamics of policy actions.\n5\nApplications of Diffusion Models\nIn this section, we conduct a complete review of existing\nworks. We divide them into five groups based on the tasks\nthey are applied: offline RL, online RL, imitation learning,\ntrajectory generation, and data augmentation. For each group,\nwe provide a detailed explanation of how each method uses\ndiffusion models to handle the task.\n5.1\nOffline RL\nOffline RL aims to learn a policy from previously collected\ndatasets without online interaction. Assuming there is a static\ndataset D collected by some (unknown) behavior policy \u03c0\u03b2,\noffline RL requires the learning algorithm to derive a policy\n\u03c0(a|s) that attains the most cumulative reward:\n\u03c0\u2217= arg max\n\u03c0\nE\u03c4\u223cp\u03c0(\u03c4)\nh H\nX\nt=0\n\u03b3tr(st, at)\ni\n.\nThe fundamental challenge in offline RL is the distributional\nshift. This refers to the discrepancy between the dataset dis-\ntribution used to train the function approximators (e.g., poli-\ncies and value functions) and the distribution on which the\npolicy is evaluated. This mismatch often results in subpar on-\nline performance. High-dimensional and expressive function\napproximation generally exacerbates this issue.\nSeveral methods use diffusion models to help tackle or\navoid the above challenges. Janner et al. [2022] first propose\nto generate optimal trajectories through iterative denoising\nwith classifier-guided sampling. Subsequent works [Wang\net al., 2023; Chen et al., 2023a; He et al., 2023b; Ada et\nal., 2023; Brehmer et al., 2023; Hansen-Estruch et al., 2023]\nrepresent the policy as a diffusion model to capture multi-\nmodal distributions and enhance the expressiveness of the\npolicy class, which is beneficial to relieve the approxima-\ntion error between the cloned behavior policy and true behav-\nior policy. Ajay et al. [2023] alleviate the distribution shift\nproblem by generating state sequences with conditional diffu-\nsion models followed by inverse dynamic functions to derive\nexecutable actions, which propose a novel approach to use\nclassifier-free guidance with low-temperature sampling to de-\nnoise out return-maximizing trajectories. LatentDiffuser [Li,\n2023] performs diffusion planning over a learned latent space\nwith separate decoders to recover raw trajectories. Benefit-\ning from a more compact planning space, it achieves superior\nperformances on long-horizon and high-dimensional tasks. In\norder to improve the generation ability of diffusion models for\nRL, Lu et al. [2023a] propose a new guidance method named\ncontrastive energy prediction and Hu et al. [2023] capture\nmore temporal conditions. By incorporating control-theoretic\ninvariance into the diffusion dynamics, SafeDiffuser [Xiao et\nal., 2023] guarantees the safe generation of planning trajecto-\nries. HDMI [Li et al., 2023a] adopts a hierarchical structure\nto tackle long-horizon decision-making problems, which uses\na reward-conditional model to discover sub-goals and a goal-\nconditional model to generate actions. Dong et al. [2023]\ncondition the diffusion planner on diverse behavior attributes\nand learn from human preferences to generate trajectories that\ncan match user-customized behaviors. CPQL [Chen et al.,\n2023d] leverages consistency models as the policy class for\nfast training and sampling, while EDP [Kang et al., 2023]\nachieves speed-up during training by using single-step model\npredictions as action approximations. Diffusion models are\nalso used as the value function [Mazoure et al., 2023] and\nrepresentation model [Venkatraman et al., 2023] to facilitate\ntraining of normal RL policies. Recent research has made\nprogress in using diffusion models to improve the perfor-\nmance of policies in multitask and multi-agent offline RL.\nMultitask offline RL.\nDiffusion models are verified to\nhave the potential to address the challenge of multi-task gen-\neralization in offline RL. He et al. [2023a] first extend the\nconditional diffusion model to be capable of solving multi-\ntask decision-making problems and synthesizing useful data\nfor downstream tasks.\nLCD [Zhang et al., 2023a] lever-\nages a hierarchical structure to achieve long-horizon multi-\ntask control. MetaDiffuser [Ni et al., 2023] demonstrates that\nincorporating the conditional diffusion model into the con-\ntext of task inference outperforms previous meta-RL meth-\nods. AdaptDiffuser [Liang et al., 2023a] combines bootstrap-\nping and diffusion-based generative modeling together to en-\nable the model to adapt to unseen tasks.\nMulti-agent offline RL.\nUsing diffusion models in multi-\nagent RL helps model discrepant behaviors among agents\nand reduces approximation error. MADiff [Zhu et al., 2023]\nuses an attention-based diffusion model to model the com-\nplex coordination among behaviors of multiple agents, which\nis well-suited to learning complex multi-agent interactions.\nDOM2 [Li et al., 2023c] incorporates the diffusion model into\nthe policy classes to enhance learning and makes it possible\nto generalize to shifted environments well.\n5.2\nOnline RL\nRecently, there have been some works showing that diffusion\nmodels can also boost online RL training. Value estimations\nin online RL are noisier and change with the current policy,\nwhich poses additional challenges on training a multistep dif-\nfusion model. DIPO [Yang et al., 2023a] proposes an action\nrelabeling strategy to perform policy improvement at the data\nlevel, bypassing the potentially unstable value-guided train-\ning. Actions in the online rollout dataset are updated with gra-\ndient ascent, and the diffusion training objective is just super-\nvised learning on the relabeled dataset. Chen et al. [2023d]\nconduct experiments to verify that consistency models with\none-step sampling can naturally serve as online RL policies\nand achieve a balance between exploration and exploitation.\nInstead of using diffusion models as policies, Rigter et al.\n[2023] build a diffusion dynamic model to generate synthetic\ntrajectories that are consistent with online RL policies. Ap-\nplications of diffusion models in online RL are less explored\nTable 1: Summary of papers on diffusion models for RL.\nModel & Paper\nRole of Diffusion Models\nKeyword(s)\nGuidance\nDiffuser [Janner et al., 2022]\nPlanner\nOffline\nClassifier\nAdaptDiffuser [Liang et al., 2023a]\nOffline\nClassifier\nEDGI [Brehmer et al., 2023]\nOffline\nClassifier\nTCD [Hu et al., 2023]\nOffline\nClassifier-free\nLatentDiffuser [Li, 2023]\nOffline\nEnergy Function\nHDMI [Li et al., 2023a]\nOffline; Hierarchical\nClassifier-free\nSafeDiffuser [Xiao et al., 2023]\nOffline; Safe\nNone\nMADiff [Zhu et al., 2023]\nOffline; Multi-agent\nClassifier-free\nMTDiff-p [He et al., 2023a]\nOffline; Multitask\nClassifier-free\nMetaDiffuser [Ni et al., 2023]\nOffline; Multitask\nClassifier-free\nDiffusion Policy [Chi et al., 2023]\nImitation; Robotics\nNone\nCrossway Diffusion [Li et al., 2023b]\nImitation; Robotics\nNone\nAVDC [Ko et al., 2023]\nImitation; Robotics\nNone\nSkillDiffuser [Liang et al., 2023b]\nImitation; Multitask; Hierarchical\nClassifier-free\nMLD [Chen et al., 2022]\nTrajectory Generation\nClassifier-free\nMDM [Tevet et al., 2022]\nTrajectory Generation\nClassifier-free\nUniSim [Yang et al., 2023b]\nTrajectory Generation\nClassifier-free\nReMoDiffuse [Zhang et al., 2023b]\nTrajectory Generation\nClassifier-free\nSinMDM [Raab et al., 2023]\nTrajectory Generation\nNone\nEquiDiff [Chen et al., 2023b]\nTrajectory Generation\nNone\nMoFusion [Dabral et al., 2022]\nTrajectory Generation\nNone\nMotionDiffuse [Zhang et al., 2022]\nTrajectory Generation\nNone\nMPD [Carvalho et al., 2023]\nTrajectory Generation; Robotics\nClassifier\nMotionDiffuser [Jiang et al., 2023]\nTrajectory Generation; Multi-agent\nClassifier\nAlignDiff [Dong et al., 2023]\nRLHF\nClassifier-free\nDiffusion-QL [Wang et al., 2023]\nPolicy\nOffline\nQ-loss\nSRDP [Ada et al., 2023]\nOffline\nQ-loss\nEDP [Kang et al., 2023]\nOffline\nQ-loss\nSfBC [Chen et al., 2023a]\nOffline\nSample & Reweight\nIDQL [Hansen-Estruch et al., 2023]\nOffline\nSample & Reweight\nDiffCPS [He et al., 2023b]\nOffline\nConvex Optimization\nCPQL [Chen et al., 2023d]\nOffline; Online\nQ-loss\nCEP [Lu et al., 2023a]\nOffline; Image Synthesis\nEnergy Function\nDOM2 [Li et al., 2023c]\nOffline; Multi-agent\nQ-loss\nNoMaD [Sridhar et al., 2023]\nImitation; Robotics\nNone\nBESO [Reuss et al., 2023]\nImitation; Goal-conditioned\nClassifier-free\nPearce et al. [2023]\nImitation\nClassifier-free\nYoneda et al. [2023]\nImitation; Robotics\nNone\nPlayFusion [Chen et al., 2023c]\nImitation; Robotics\nNone\nXSkill [Xu et al., 2023]\nImitation; Robotics\nNone\nCoDP [Ng et al., 2023]\nHuman-in-the-loop\nNone\nGenAug [Chen et al., 2023e]\nData Synthesizer\nRobotics\nNone\nROSIE [Yu et al., 2023]\nRobotics\nNone\nSynthER [Lu et al., 2023b]\nOffline; Online\nNone\nMTDiff-s [He et al., 2023a]\nOffline; Multitask\nClassifier-free\nLDCQ [Venkatraman et al., 2023]\nLatent Representation\nOffline\nClassifier-free\nDVF [Mazoure et al., 2023]\nValue Function\nOffline\nNone\nPolyGRAD [Rigter et al., 2023]\nDynamic Model\nOnline\nNone\ncompared to offline settings and merit further investigation.\n5.3\nImitation Learning\nThe goal of imitation learning (IL) is to reproduce behavior\nsimilar to experts in the environment by extracting knowledge\nfrom expert demonstrations. Recently, many works [Hegde\net al., 2023; Ng et al., 2023; Chen et al., 2023c; Kapelyukh\net al., 2022] have demonstrated the efficacy of representing\npolicies as diffusion models to capture multi-modal behav-\nior. Pearce et al. [2023] apply diffusion models to imitate\nhuman behavior in sequential environments, in which diffu-\nsion models are compared with other generative models and\nviable approaches are developed to improve the quality of be-\nhavior sampled from diffusion models. Chi et al.; Xian et\nal.\n[2023; 2023] generate the robot\u2019s behavior via a con-\nditional denoising diffusion process on robot action space.\nExperiment results show that Diffusion models are good at\npredicting closed-loop action sequences while guaranteeing\ntemporal consistency [Chi et al., 2023]. Li et al. [2023b]\nimprove the models in Chi et al. [2023] by incorporating an\nauxiliary reconstruction loss on intermediate representations\nof the reverse diffusion process. Beneficial from its power-\nful generation ability, leveraging diffusion models to acquire\ndiverse skills to handle multiple manipulation tasks is promis-\ning [Chen et al., 2023c; Mishra et al., 2023; Xu et al., 2023;\nHa et al., 2023]. Diffusion models are already applied to\ngoal-conditioned IL: Reuss et al.\n[2023] use a decoupled\nscore-based diffusion model to learn an expressive goal-\nconditional policy. In contrast, Sridhar et al. [2023] build a\nunified diffusion policy to solve both goal-directed navigation\nand goal-agnostic exploration problems. Liang et al. [2023b]\nadopt a hierarchical structure where the high-level skills are\ndetermined by the current visual observation and language in-\nstructions. Therefore the low-level skill-conditioned planner\ncan satisfy the user-specified multitask instructions.\n5.4\nTrajectory Generation\nTrajectory generation aims to produce a dynamically feasible\npath that satisfies a set of constraints. We focus on using dif-\nfusion models to generate human pose and robot interaction\nsequences, which are more related to the decision-making\nscenario. Many works [Zhang et al., 2022; Jiang et al., 2023;\nTevet et al., 2022; Zhang et al., 2023b; Chen et al., 2022;\nDabral et al., 2022] have remarked that the conditional dif-\nfusion models perform better than traditional methods which\nuse GAN or Transformer. Employing a denoising-diffusion-\nbased framework, they achieve diverse and fine-grained mo-\ntion generation with various conditioning contexts [Chen et\nal., 2023b; Carvalho et al., 2023].\nRecent works [Du et\nal., 2023b; Ko et al., 2023; Du et al., 2023a] harness dif-\nfusion models to synthesize a set of future frames depicting\nits planned actions in the future, after which control actions\nare extracted from the generated video. This approach makes\nit possible to train policies solely on RGB videos and deploy\nlearned policies to various robotic tasks with varying dynam-\nics [Black et al., 2023; Gao et al., 2023]. UniSim [Yang et\nal., 2023b] uses diffusion models to build a universal simu-\nlator of real-world interaction by learning through combined\ndiverse datasets. It can be used to train both high-level vision-\nlanguage planners and low-level RL policies, demonstrating\npowerful emulation ability.\n5.5\nData Augmentation\nSince diffusion models perform well in learning over mul-\ntimodal or even noisy distributions, they can model original\ndata distribution precisely. What is more, they are capable of\ngenerating diverse data points to expand original distribution\nwhile maintaining dynamic accuracy. Recent works [Yu et\nal., 2023; Chen et al., 2023e] consider augmenting the obser-\nvations of robotic control using a text-guided diffusion model\nwhile maintaining the same action. The recently proposed\nSynthER [Lu et al., 2023b] and MTDiff-s [He et al., 2023a]\ngenerate complete transitions of trained tasks via a diffusion\nmodel. Lu et al. [2023b] directly train the diffusion model\nfrom the offline dataset or the online replay buffer and then\ngenerate samples for policy improvement. Analysis shows\nthat both diversity and accuracy of data generated by diffu-\nsion models are higher than those generated by prior data aug-\nmentation methods. He et al. [2023a] deploy diffusion syn-\nthesizer on multi-task offline datasets and achieve better per-\nformance than that on single-task datasets. They claim that\nfitting on multiple tasks may enable implicit knowledge shar-\ning across tasks, which also benefits from the multi-modal\nproperty of diffusion models. These works demonstrate that\ndiffusion augmentation can bring significant improvement in\npolicy learning for IL, online RL, and offline RL.\n6\nSummary and Future Prospects\nThis survey offers a comprehensive overview of contempo-\nrary research endeavors concerning the application of dif-\nfusion models in the realm of RL. According to the roles\nplayed by diffusion models, we categorize existing methods\ninto using diffusion models as planners, policies, data syn-\nthesizers, and less popular roles such as value functions, rep-\nresentation models, etc. By comparing each class of meth-\nods to traditional solutions, we can see how the diffusion\nmodel addresses some of the longstanding challenges in RL,\ni.e., restricted expressiveness, data scarcity, compounding er-\nror, and multitask generalization. It is worth emphasizing\nthat the incorporation of diffusion models into RL remains\nan emerging field, and there are many research topics worth\nexploring. Here, we outline four prospective research direc-\ntions, namely, generative simulation, integrating safety con-\nstraints, retrieval-augmented generation, and composing dif-\nferent skills.\nGenerative simulation.\nAs shown in Fig. 1, existing works\nuse diffusion models to overcome certain limitations of pre-\nvious solutions in both the agent and buffer parts. However,\nthere has been a scarcity of research focused on using diffu-\nsion modeling to improve the environment. Gen2Sim [Katara\net al., 2023] uses text-to-image diffusion models to generate\ndiverse objects in simulation environments, where RL poli-\ncies are trained to learn robot manipulation skills. Besides\ngenerating objects in the scene, diffusion models have the po-\ntential for broader applications in generative simulation, such\nas the generation of various possible dynamics functions, re-\nward functions, or opponents in multi-agent learning.\nIntegrating safety constraints.\nMaking decisions in real\ntasks often necessitates compliance with various safety con-\nstraints. Several safe RL methods transform a constrained\nRL problem to its unconstrained equivalent [Achiam et al.,\n2017], which is then solved by generic RL algorithms. Poli-\ncies acquired through these methods remain tailored to the\nspecific constraint threshold specified during training. A re-\ncent research [Liu et al., 2023] has extended the applicability\nof decision transformers to the context of safety-constrained\nsettings, thereby enabling a single model to adapt to diverse\nthresholds by adjusting the input cost-to-go. Similarly, dif-\nfusion models have the potential to be deployed in safe RL\nby viewing safety constraints as sampling conditions. Ajay\net al. [2023] demonstrate that a diffusion-based planner can\ncombine different movement skills to produce new behav-\niors. Also, classifier-guided sampling can include new con-\nditions by learning additional classifiers, while the parame-\nters of the diffusion model remain unchanged [Dhariwal and\nNichol, 2021]. This makes the diffusion model promising for\nscenarios with new safety requirements after model training.\nRetrieval-enhanced generation.\nRetrieval techniques are\nemployed in various domains such as recommender sys-\ntems [Qin et al., 2020] and large language models [Kandpal\net al., 2023] to enhance the model capacity and handle long-\ntail distributed datasets. Some works utilize retrieved data\nto boost text-to-image and text-to-motion diffusion genera-\ntion [Sheynin et al., 2022; Zhang et al., 2023b], promoting\nbetter coverage of uncommon condition signals. During on-\nline interactions, RL agents may also encounter states that\nare rare in the training dataset. By retrieving relevant states\nas model inputs, the performance of diffusion-based decision\nmodels can be improved in these states. Also, if the retrieval\ndataset is constantly updated, diffusion models have the po-\ntential to generate new behaviors without retraining.\nComposing different skills.\nFrom the perspective of skill-\nbased RL, it is promising to break down complex tasks into\nsmaller, more manageable sub-skills. Diffusion models ex-\ncel in modeling multi-modal distributions, and since multiple\nsub-skills can be viewed as distinct modes within the distri-\nbution of possible behaviors, they offer a natural fit for this\ntask. Combining with classifier guidance or classifier-free\nguidance, diffusion models are possible to generate proper\nskills to complete the facing task.\nExperiments in offline\nRL also suggest that diffusion models can share knowledge\nacross skills and combine them up [Ajay et al., 2023], thus\nhaving the potential for zero-shot adaptation or continuous\nRL by composing different skills.\nAcknowledgments\nThe work is partially supported by National Key R&D Pro-\ngram of China (2022ZD0114804) and National Natural Sci-\nence Foundation of China (62076161). We thank Minghuan\nLiu, Xihuai Wang, Jingxiao Chen and Mingcheng Chen for\nvaluable suggestions and discussions.\nReferences\n[Achiam et al., 2017] Joshua Achiam, David Held, Aviv\nTamar, and Pieter Abbeel. Constrained policy optimiza-\ntion.\nIn International conference on machine learning,\npages 22\u201331. PMLR, 2017.\n[Ada et al., 2023] Suzan Ece Ada, Erhan Oztop, and Emre\nUgur. Diffusion policies for out-of-distribution general-\nization in offline reinforcement learning, 2023.\n[Ajay et al., 2023] Anurag Ajay, Yilun Du, Abhi Gupta,\nJoshua B. Tenenbaum, T. Jaakkola, and Pulkit Agrawal. Is\nconditional generative modeling all you need for decision-\nmaking? 2023.\n[Austin et al., 2021] Jacob\nAustin,\nDaniel\nD\nJohnson,\nJonathan Ho, Daniel Tarlow, and Rianne Van Den Berg.\nStructured denoising diffusion models in discrete state-\nspaces. Advances in Neural Information Processing Sys-\ntems, 34:17981\u201317993, 2021.\n[Beck et al., 2023] Jacob Beck, Risto Vuorio, Evan Zheran\nLiu, Zheng Xiong, Luisa Zintgraf, Chelsea Finn, and Shi-\nmon Whiteson. A survey of meta-reinforcement learning.\narXiv preprint arXiv:2301.08028, 2023.\n[Black et al., 2023] Kevin\nBlack,\nMitsuhiko\nNakamoto,\nPranav Atreya, Homer Walke, Chelsea Finn, Aviral Ku-\nmar, and Sergey Levine. Zero-shot robotic manipulation\nwith pretrained image-editing diffusion models, 2023.\n[Brehmer et al., 2023] Johann Brehmer, Joey Bose, Pim\nde Haan, and Taco Cohen. Edgi: Equivariant diffusion\nfor planning with embodied agents, 2023.\n[Carvalho et al., 2023] Joao Carvalho, An T. Le, Mark\nBaierl, Dorothea Koert, and Jan Peters. Motion planning\ndiffusion: Learning and planning of robot motions with\ndiffusion models, 2023.\n[Chen et al., 2021] Lili Chen,\nKevin Lu,\nAravind Ra-\njeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter\nAbbeel, Aravind Srinivas, and Igor Mordatch. Decision\ntransformer: Reinforcement learning via sequence model-\ning. Advances in neural information processing systems,\n34:15084\u201315097, 2021.\n[Chen et al., 2022] Xin Chen, Biao Jiang, Wen Liu, Zilong\nHuang, Bin Fu, Tao Chen, Jingyi Yu, and Gang Yu. Ex-\necuting your commands via motion diffusion in latent\nspace. 2023 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 18000\u201318010,\n2022.\n[Chen et al., 2023a] Huayu Chen, Cheng Lu, Chengyang\nYing, Hang Su, and Jun Zhu. Offline reinforcement learn-\ning via high-fidelity generative behavior modeling. In The\nEleventh International Conference on Learning Represen-\ntations, 2023.\n[Chen et al., 2023b] Kehua Chen, Xianda Chen, Zihan Yu,\nMeixin Zhu, and Hai Yang. Equidiff: A conditional equiv-\nariant diffusion model for trajectory prediction, 2023.\n[Chen et al., 2023c] Lili Chen, Shikhar Bahl, and Deepak\nPathak. Playfusion: Skill acquisition via diffusion from\nlanguage-annotated play.\nIn 7th Annual Conference on\nRobot Learning, 2023.\n[Chen et al., 2023d] Yuhui Chen, Haoran Li, and Dongbin\nZhao. Boosting continuous control with consistency pol-\nicy, 2023.\n[Chen et al., 2023e] Zoey Chen,\nSho Kiami,\nAbhishek\nGupta, and Vikash Kumar. Genaug: Retargeting behaviors\nto unseen situations via generative augmentation. arXiv\npreprint arXiv:2302.06671, 2023.\n[Chi et al., 2023] Cheng Chi, Siyuan Feng, Yilun Du, Zhen-\njia Xu, Eric Cousineau, Benjamin Burchfiel, and Shuran\nSong. Diffusion policy: Visuomotor policy learning via\naction diffusion, 2023.\n[Cho et al., 2022] Daesol Cho, Dongseok Shim, and H Jin\nKim. S2p: State-conditioned image synthesis for data aug-\nmentation in offline reinforcement learning. Advances in\nNeural Information Processing Systems, 35:11534\u201311546,\n2022.\n[Dabral et al., 2022] Rishabh Dabral, Muhammad Hamza\nMughal, Vladislav Golyanik, and Christian Theobalt. Mo-\nfusion: A framework for denoising-diffusion-based mo-\ntion synthesis. 2023 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), pages 9760\u2013\n9770, 2022.\n[Dhariwal and Nichol, 2021] Prafulla Dhariwal and Alexan-\nder Nichol. Diffusion models beat gans on image synthe-\nsis. Advances in neural information processing systems,\n34:8780\u20138794, 2021.\n[Dong et al., 2023] Zibin Dong, Yifu Yuan, Jianye Hao, Fei\nNi, Yao Mu, Yan Zheng, Yujing Hu, Tangjie Lv, Changjie\nFan, and Zhipeng Hu. Aligndiff: Aligning diverse human\npreferences via behavior-customisable diffusion model,\n2023.\n[Du et al., 2023a] Yilun Du, Mengjiao Yang, Bo Dai, Han-\njun Dai, Ofir Nachum, Joshua B. Tenenbaum, Dale Schu-\nurmans, and Pieter Abbeel. Learning universal policies via\ntext-guided video generation, 2023.\n[Du et al., 2023b] Yilun Du, Mengjiao Yang, Pete Florence,\nFei Xia, Ayzaan Wahid, Brian Ichter, Pierre Sermanet,\nTianhe Yu, Pieter Abbeel, Joshua B. Tenenbaum, Leslie\nKaelbling, Andy Zeng, and Jonathan Tompson. Video lan-\nguage planning, 2023.\n[Fujimoto and Gu, 2021] Scott\nFujimoto\nand\nShixi-\nang Shane Gu.\nA minimalist approach to offline\nreinforcement learning. Advances in neural information\nprocessing systems, 34:20132\u201320145, 2021.\n[Fujimoto et al., 2019] Scott Fujimoto, David Meger, and\nDoina Precup.\nOff-policy deep reinforcement learning\nwithout exploration. In International conference on ma-\nchine learning, pages 2052\u20132062. PMLR, 2019.\n[Gao et al., 2023] Jialu Gao, Kaizhe Hu, Guowei Xu, and\nHuazhe Xu. Can pre-trained text-to-image models gen-\nerate visual goals for reinforcement learning?, 2023.\n[Goodfellow et al., 2014] Ian\nGoodfellow,\nJean\nPouget-\nAbadie, Mehdi Mirza, Bing Xu, David Warde-Farley,\nSherjil Ozair, Aaron Courville, and Yoshua Bengio. Gen-\nerative adversarial nets. Advances in neural information\nprocessing systems, 27, 2014.\n[Ha et al., 2023] Huy Ha, Pete Florence, and Shuran Song.\nScaling up and distilling down: Language-guided robot\nskill acquisition.\nIn 7th Annual Conference on Robot\nLearning, 2023.\n[Hansen-Estruch et al., 2023] Philippe Hansen-Estruch, Ilya\nKostrikov, Michael Janner, Jakub Grudzien Kuba, and\nSergey Levine. Idql: Implicit q-learning as an actor-critic\nmethod with diffusion policies, 2023.\n[He et al., 2023a] Haoran He, Chenjia Bai, Kang Xu, Zhuo-\nran Yang, Weinan Zhang, Dong Wang, Bin Zhao, and Xue-\nlong Li. Diffusion model is an effective planner and data\nsynthesizer for multi-task reinforcement learning. 2023.\n[He et al., 2023b] Longxiang He, Linrui Zhang, Junbo Tan,\nand Xueqian Wang. Diffcps: Diffusion model based con-\nstrained policy search for offline reinforcement learning,\n2023.\n[Hegde et al., 2023] Shashank Hegde, Sumeet Batra, K. R.\nZentner, and Gaurav S. Sukhatme.\nGenerating behav-\niorally diverse policies with latent diffusion models, 2023.\n[Ho and Salimans, 2022] Jonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance, 2022.\n[Ho et al., 2020] Jonathan Ho, Ajay Jain, and Pieter Abbeel.\nDenoising diffusion probabilistic models, 2020.\n[Hu et al., 2023] Jifeng Hu,\nYanchao Sun,\nSili Huang,\nSiYuan Guo, Hechang Chen, Li Shen, Lichao Sun,\nYi Chang, and Dacheng Tao. Instructed diffuser with tem-\nporal condition guidance for offline reinforcement learn-\ning, 2023.\n[Imre, 2021] Baris Imre. An investigation of generative re-\nplay in deep reinforcement learning. B.S. thesis, Univer-\nsity of Twente, 2021.\n[Janner et al., 2022] Michael Janner, Yilun Du, Joshua B.\nTenenbaum, and Sergey Levine. Planning with diffusion\nfor flexible behavior synthesis. In International Confer-\nence on Machine Learning, 2022.\n[Jiang et al., 2023] Chiyu Max Jiang, Andre Cornman, Che-\nolho Park, Ben Sapp, Yin Zhou, and Dragomir Anguelov.\nMotiondiffuser: Controllable multi-agent motion predic-\ntion using diffusion, 2023.\n[Kandpal et al., 2023] Nikhil\nKandpal,\nHaikang\nDeng,\nAdam Roberts, Eric Wallace, and Colin Raffel.\nLarge\nlanguage models struggle to learn long-tail knowledge.\nIn International Conference on Machine Learning, pages\n15696\u201315707. PMLR, 2023.\n[Kang et al., 2023] Bingyi Kang, Xiao Ma, Chao Du, Tianyu\nPang, and Shuicheng Yan. Efficient diffusion policies for\noffline reinforcement learning, 2023.\n[Kapelyukh et al., 2022] Ivan Kapelyukh, Vitalis Vosylius,\nand Edward Johns. Dall-e-bot: Introducing web-scale dif-\nfusion models to robotics. IEEE Robotics and Automation\nLetters, 8:3956\u20133963, 2022.\n[Katara et al., 2023] Pushkal Katara, Zhou Xian, and Ka-\nterina Fragkiadaki.\nGen2sim: Scaling up robot learn-\ning in simulation with generative models. arXiv preprint\narXiv:2310.18308, 2023.\n[Kingma and Welling, 2013] Diederik P Kingma and Max\nWelling. Auto-encoding variational bayes. arXiv preprint\narXiv:1312.6114, 2013.\n[Kiran et al., 2021] B Ravi Kiran, Ibrahim Sobh, Victor Tal-\npaert, Patrick Mannion, Ahmad A Al Sallab, Senthil Yoga-\nmani, and Patrick P\u00b4erez. Deep reinforcement learning for\nautonomous driving: A survey. IEEE Transactions on In-\ntelligent Transportation Systems, 23(6):4909\u20134926, 2021.\n[Ko et al., 2023] Po-Chen Ko, Jiayuan Mao, Yilun Du, Shao-\nHua Sun, and Joshua B. Tenenbaum. Learning to act from\nactionless videos through dense correspondences, 2023.\n[Kober et al., 2013] Jens Kober, J Andrew Bagnell, and Jan\nPeters. Reinforcement learning in robotics: A survey. The\nInternational Journal of Robotics Research, 32(11):1238\u2013\n1274, 2013.\n[Kong et al., 2020] Zhifeng Kong, Wei Ping, Jiaji Huang,\nKexin Zhao, and Bryan Catanzaro. Diffwave: A versa-\ntile diffusion model for audio synthesis. arXiv preprint\narXiv:2009.09761, 2020.\n[Kostrikov et al., 2021] Ilya\nKostrikov,\nRob\nFergus,\nJonathan Tompson, and Ofir Nachum. Offline reinforce-\nment learning with fisher divergence critic regularization.\nIn International Conference on Machine Learning, pages\n5774\u20135783. PMLR, 2021.\n[Kumar et al., 2020] Aviral Kumar, Aurick Zhou, George\nTucker, and Sergey Levine. Conservative q-learning for\noffline reinforcement learning. Advances in Neural Infor-\nmation Processing Systems, 33:1179\u20131191, 2020.\n[Laskin et al., 2020] Misha\nLaskin,\nKimin\nLee,\nAdam\nStooke, Lerrel Pinto, Pieter Abbeel, and Aravind Srinivas.\nReinforcement learning with augmented data. Advances in\nneural information processing systems, 33:19884\u201319895,\n2020.\n[Lee and Han, 2021] Junhyeok Lee and Seungu Han.\nNu-\nwave: A diffusion probabilistic model for neural audio up-\nsampling. arXiv preprint arXiv:2104.02321, 2021.\n[Li et al., 2022] Xiang Li, John Thickstun, Ishaan Gulrajani,\nPercy S Liang, and Tatsunori B Hashimoto. Diffusion-lm\nimproves controllable text generation. Advances in Neural\nInformation Processing Systems, 35:4328\u20134343, 2022.\n[Li et al., 2023a] Wenhao Li, Xiangfeng Wang, Bo Jin, and\nHongyuan Zha. Hierarchical diffusion for offline decision\nmaking. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett, editors, Proceedings of the 40th International\nConference on Machine Learning, volume 202 of Pro-\nceedings of Machine Learning Research, pages 20035\u2013\n20064. PMLR, 23\u201329 Jul 2023.\n[Li et al., 2023b] Xiang Li, Varun Belagali, Jinghuan Shang,\nand Michael S. Ryoo.\nCrossway diffusion:\nImprov-\ning diffusion-based visuomotor policy via self-supervised\nlearning, 2023.\n[Li et al., 2023c] Zhuoran Li, Ling Pan, and Longbo Huang.\nBeyond conservatism: Diffusion policies in offline multi-\nagent reinforcement learning, 2023.\n[Li, 2023] Wenhao Li. Efficient planning with latent diffu-\nsion, 2023.\n[Liang et al., 2023a] Zhixuan Liang, Yao Mu, Mingyu Ding,\nFei Ni, Masayoshi Tomizuka, and Ping Luo. AdaptDif-\nfuser: Diffusion models as adaptive self-evolving plan-\nners. In Andreas Krause, Emma Brunskill, Kyunghyun\nCho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett, editors, Proceedings of the 40th International\nConference on Machine Learning, volume 202 of Pro-\nceedings of Machine Learning Research, pages 20725\u2013\n20745. PMLR, 23\u201329 Jul 2023.\n[Liang et al., 2023b] Zhixuan Liang, Yao Mu, Hengbo Ma,\nMasayoshi Tomizuka, Mingyu Ding, and Ping Luo.\nSkilldiffuser: Interpretable hierarchical planning via skill\nabstractions in diffusion-based task execution.\narXiv\npreprint arXiv:2312.11598, 2023.\n[Liu et al., 2021] Bo Liu, Xingchao Liu, Xiaojie Jin, Peter\nStone, and Qiang Liu.\nConflict-averse gradient descent\nfor multi-task learning. Advances in Neural Information\nProcessing Systems, 34:18878\u201318890, 2021.\n[Liu et al., 2023] Zuxin Liu, Zijian Guo, Yihang Yao, Zhep-\neng Cen, Wenhao Yu, Tingnan Zhang, and Ding Zhao.\nConstrained decision transformer for offline safe rein-\nforcement learning.\narXiv preprint arXiv:2302.07351,\n2023.\n[Lu et al., 2022] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei\nChen, Chongxuan Li, and Jun Zhu. Dpm-solver: A fast\node solver for diffusion probabilistic model sampling in\naround 10 steps, 2022.\n[Lu et al., 2023a] Cheng Lu, Huayu Chen, Jianfei Chen,\nHang Su, Chongxuan Li, and Jun Zhu. Contrastive energy\nprediction for exact energy-guided diffusion sampling in\noffline reinforcement learning, 2023.\n[Lu et al., 2023b] Cong Lu, Philip J. Ball, and Jack Parker-\nHolder. Synthetic experience replay. In Workshop on Rein-\ncarnating Reinforcement Learning at ICLR 2023, 2023.\n[Lugmayr et al., 2022] Andreas Lugmayr, Martin Danell-\njan, Andres Romero, Fisher Yu, Radu Timofte, and Luc\nVan Gool. Repaint: Inpainting using denoising diffusion\nprobabilistic models.\nIn Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 11461\u201311471, 2022.\n[Luo and Hu, 2021] Shitong Luo and Wei Hu.\nDiffusion\nprobabilistic models for 3d point cloud generation. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pages 2837\u20132845, 2021.\n[Luo et al., 2022] Fan-Ming Luo,\nTian Xu,\nHang Lai,\nXiong-Hui Chen, Weinan Zhang, and Yang Yu. A survey\non model-based reinforcement learning.\narXiv preprint\narXiv:2206.09328, 2022.\n[Lyu et al., 2022] Jiafei Lyu, Xiaoteng Ma, Xiu Li, and\nZongqing Lu. Mildly conservative q-learning for offline\nreinforcement learning. Advances in Neural Information\nProcessing Systems, 35:1711\u20131724, 2022.\n[Mazoure et al., 2023] Bogdan Mazoure,\nWalter Talbott,\nMiguel Angel Bautista, Devon Hjelm, Alexander Toshev,\nand Josh Susskind. Value function estimation using con-\nditional diffusion models for control, 2023.\n[Mishra et al., 2023] Utkarsh Aashu Mishra, Shangjie Xue,\nYongxin Chen, and Danfei Xu. Generative skill chaining:\nLong-horizon skill planning with diffusion models. In 7th\nAnnual Conference on Robot Learning, 2023.\n[Mnih et al., 2013] Volodymyr Mnih, Koray Kavukcuoglu,\nDavid Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep\nreinforcement learning. arXiv preprint arXiv:1312.5602,\n2013.\n[Nagabandi et al., 2018] Anusha Nagabandi, Gregory Kahn,\nRonald S Fearing, and Sergey Levine. Neural network dy-\nnamics for model-based deep reinforcement learning with\nmodel-free fine-tuning. In 2018 IEEE international con-\nference on robotics and automation (ICRA), pages 7559\u2013\n7566. IEEE, 2018.\n[Neal and others, 2011] Radford M Neal et al. Mcmc using\nhamiltonian dynamics. Handbook of markov chain monte\ncarlo, 2(11):2, 2011.\n[Ng et al., 2023] Eley\nNg,\nZiang\nLiu,\nand\nMonroe\nKennedy III au2.\nDiffusion co-policy for synergistic\nhuman-robot collaborative tasks, 2023.\n[Ni et al., 2023] Fei Ni, Jianye Hao, Yao Mu, Yifu Yuan, Yan\nZheng, Bin Wang, and Zhixuan Liang. Metadiffuser: Dif-\nfusion model as conditional planner for offline meta-rl,\n2023.\n[Pearce et al., 2023] Tim Pearce, Tabish Rashid, Anssi Kan-\nervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu,\nSergio Valcarcel Macua, Shan Zheng Tan, Ida Momenne-\njad, Katja Hofmann, and Sam Devlin. Imitating human\nbehaviour with diffusion models. In The Eleventh Interna-\ntional Conference on Learning Representations, 2023.\n[Qin et al., 2020] Jiarui Qin, Weinan Zhang, Xin Wu, Jiarui\nJin, Yuchen Fang, and Yong Yu. User behavior retrieval\nfor click-through rate prediction. In Proceedings of the\n43rd International ACM SIGIR Conference on Research\nand Development in Information Retrieval, pages 2347\u2013\n2356, 2020.\n[Raab et al., 2023] Sigal Raab, Inbal Leibovitch, Guy Tevet,\nMoab Arar, Amit H. Bermano, and Daniel Cohen-Or. Sin-\ngle motion diffusion, 2023.\n[Reed et al., 2022] Scott\nReed,\nKonrad\nZolna,\nEmilio\nParisotto,\nSergio\nGomez\nColmenarejo,\nAlexander\nNovikov, Gabriel Barth-Maron, Mai Gimenez, Yury\nSulsky, Jackie Kay, Jost Tobias Springenberg, et al.\nA\ngeneralist agent. arXiv preprint arXiv:2205.06175, 2022.\n[Reuss et al., 2023] Moritz Reuss, Maximilian Li, Xiaogang\nJia, and Rudolf Lioutikov.\nGoal-conditioned imitation\nlearning using score-based diffusion policies, 2023.\n[Rigter et al., 2023] Marc Rigter, Jun Yamada, and Ingmar\nPosner. World models via policy-guided trajectory diffu-\nsion. arXiv preprint arXiv:2312.08533, 2023.\n[Schmidhuber, 2019] Juergen Schmidhuber. Reinforcement\nlearning upside down: Don\u2019t predict rewards\u2013just map\nthem to actions. arXiv preprint arXiv:1912.02875, 2019.\n[Schneuing et al., 2022] Arne\nSchneuing,\nYuanqi\nDu,\nCharles Harris, Arian Jamasb, Ilia Igashov, Weitao Du,\nTom Blundell, Pietro Li\u00b4o, Carla Gomes, Max Welling,\net al. Structure-based drug design with dvariant diffusion\nmodels. arXiv preprint arXiv:2210.13695, 2022.\n[Schrittwieser et al., 2020] Julian\nSchrittwieser,\nIoannis\nAntonoglou, Thomas Hubert, Karen Simonyan, Laurent\nSifre, Simon Schmitt, Arthur Guez, Edward Lockhart,\nDemis Hassabis, Thore Graepel, et al.\nMastering atari,\ngo, chess and shogi by planning with a learned model.\nNature, 588(7839):604\u2013609, 2020.\n[Sheynin et al., 2022] Shelly Sheynin, Oron Ashual, Adam\nPolyak, Uriel Singer, Oran Gafni, Eliya Nachmani, and\nYaniv Taigman.\nKnn-diffusion: Image generation via\nlarge-scale retrieval.\narXiv preprint arXiv:2204.02849,\n2022.\n[Sinha et al., 2021] Samarth Sinha, Ajay Mandlekar, and\nAnimesh Garg. S4rl: Surprisingly simple self-supervision\nfor offline reinforcement learning, 2021.\n[Song et al., 2021] Yang\nSong,\nJascha\nSohl-Dickstein,\nDiederik P. Kingma, Abhishek Kumar, Stefano Ermon,\nand Ben Poole. Score-based generative modeling through\nstochastic differential equations, 2021.\n[Song et al., 2022] Jiaming Song, Chenlin Meng, and Ste-\nfano Ermon. Denoising diffusion implicit models, 2022.\n[Sridhar et al., 2023] Ajay Sridhar, Dhruv Shah, Catherine\nGlossop, and Sergey Levine. Nomad: Goal masked dif-\nfusion policies for navigation and exploration, 2023.\n[Sutton and Barto, 2018] Richard S Sutton and Andrew G\nBarto.\nReinforcement learning: An introduction.\nMIT\npress, 2018.\n[Taiga et al., 2022] Adrien Ali Taiga, Rishabh Agarwal,\nJesse Farebrother, Aaron Courville, and Marc G Belle-\nmare. Investigating multi-task pretraining and generaliza-\ntion in reinforcement learning. In The Eleventh Interna-\ntional Conference on Learning Representations, 2022.\n[Tevet et al., 2022] Guy Tevet, Sigal Raab, Brian Gordon,\nYonatan Shafir, Daniel Cohen-Or, and Amit H. Bermano.\nHuman motion diffusion model, 2022.\n[Venkatraman et al., 2023] Siddarth Venkatraman, Shivesh\nKhaitan, Ravi Tej Akella, John Dolan, Jeff Schneider, and\nGlen Berseth. Reasoning with latent diffusion in offline\nreinforcement learning, 2023.\n[Vithayathil Varghese and Mahmoud, 2020] Nelson Vithay-\nathil Varghese and Qusay H Mahmoud. A survey of multi-\ntask deep reinforcement learning. Electronics, 9(9):1363,\n2020.\n[Wang et al., 2023] Zhendong Wang, Jonathan J Hunt, and\nMingyuan Zhou. Diffusion policies as an expressive policy\nclass for offline reinforcement learning. In The Eleventh\nInternational Conference on Learning Representations,\n2023.\n[Watson et al., 2021] Daniel Watson, Jonathan Ho, Moham-\nmad Norouzi, and William Chan. Learning to efficiently\nsample from diffusion probabilistic models, 2021.\n[Xian et al., 2023] Zhou\nXian,\nNikolaos\nGkanatsios,\nTheophile Gervet, Tsung-Wei Ke, and Katerina Fragki-\nadaki.\nChaineddiffuser:\nUnifying trajectory diffusion\nand keypose prediction for robotic manipulation. In 7th\nAnnual Conference on Robot Learning, 2023.\n[Xiao et al., 2019] Chenjun Xiao, Yifan Wu, Chen Ma, Dale\nSchuurmans, and Martin M\u00a8uller.\nLearning to combat\ncompounding-error in model-based reinforcement learn-\ning. arXiv preprint arXiv:1912.11206, 2019.\n[Xiao et al., 2023] Wei Xiao, Tsun-Hsuan Wang, Chuang\nGan, and Daniela Rus. Safediffuser: Safe planning with\ndiffusion probabilistic models, 2023.\n[Xu et al., 2022] Minkai Xu, Lantao Yu, Yang Song, Chence\nShi, Stefano Ermon, and Jian Tang. Geodiff: A geometric\ndiffusion model for molecular conformation generation.\narXiv preprint arXiv:2203.02923, 2022.\n[Xu et al., 2023] Mengda Xu,\nZhenjia Xu, Cheng Chi,\nManuela Veloso, and Shuran Song. Xskill: Cross embod-\niment skill discovery, 2023.\n[Yang et al., 2023a] Long Yang, Zhixiong Huang, Fenghao\nLei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting\nWen, Binbin Zhou, and Zhouchen Lin. Policy represen-\ntation via diffusion probability model for reinforcement\nlearning. arXiv preprint arXiv:2305.13122, 2023.\n[Yang et al., 2023b] Mengjiao Yang, Yilun Du, Kamyar\nGhasemipour, Jonathan Tompson, Dale Schuurmans, and\nPieter Abbeel. Learning interactive real-world simulators,\n2023.\n[Yoneda et al., 2023] Takuma\nYoneda,\nLuzhe\nSun,\n,\nGe Yang, Bradly Stadie, and Matthew Walter. To the noise\nand back: Diffusion for shared autonomy, 2023.\n[Yu et al., 2023] Tianhe\nYu,\nTed\nXiao,\nAustin\nStone,\nJonathan Tompson, Anthony Brohan, Su Wang, Jaspiar\nSingh, Clayton Tan, Jodilyn Peralta, Brian Ichter, et al.\nScaling robot learning with semantically imagined experi-\nence. arXiv preprint arXiv:2302.11550, 2023.\n[Zhang et al., 2022] Mingyuan Zhang, Zhongang Cai, Liang\nPan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei\nLiu. Motiondiffuse: Text-driven human motion generation\nwith diffusion model. arXiv preprint arXiv:2208.15001,\n2022.\n[Zhang et al., 2023a] Edwin Zhang,\nYujie Lu,\nWilliam\nWang, and Amy Zhang. Lad: Language control diffusion:\nefficiently scaling through space, time, and tasks. arXiv\npreprint arXiv:2210.15629, 2023.\n[Zhang et al., 2023b] Mingyuan Zhang, Xinying Guo, Liang\nPan, Zhongang Cai, Fangzhou Hong, Huirong Li, Lei\nYang, and Ziwei Liu. Remodiffuse: Retrieval-augmented\nmotion diffusion model, 2023.\n[Zheng et al., 2023] Huangjie\nZheng,\nPengcheng\nHe,\nWeizhu Chen, and Mingyuan Zhou. Truncated diffusion\nprobabilistic\nmodels\nand\ndiffusion-based\nadversarial\nauto-encoders, 2023.\n[Zhu et al., 2021] Menghui Zhu, Minghuan Liu, Jian Shen,\nZhicheng Zhang, Sheng Chen, Weinan Zhang, Deheng\nYe, Yong Yu, Qiang Fu, and Wei Yang. Mapgo: Model-\nassisted policy optimization for goal-oriented tasks. arXiv\npreprint arXiv:2105.06350, 2021.\n[Zhu et al., 2023] Zhengbang Zhu, Minghuan Liu, Liyuan\nMao, Bingyi Kang, Minkai Xu, Yong Yu, Stefano Ermon,\nand Weinan Zhang. Madiff: Offline multi-agent learning\nwith diffusion models, 2023.\n",
    "2208.06193": "Published as a conference paper at ICLR 2023\nDIFFUSION POLICIES\nAS\nAN EXPRESSIVE POLICY\nCLASS FOR OFFLINE REINFORCEMENT LEARNING\nZhendong Wang1,\u2217, Jonathan J Hunt2,\u2020 , Mingyuan Zhou1,\u2020\n1The University of Texas at Austin, 2 Twitter\nzhendong.wang@utexas.edu, jhunt@twitter.com\nmingyuan.zhou@mccombs.utexas.edu\nABSTRACT\nOffline reinforcement learning (RL), which aims to learn an optimal policy using\na previously collected static dataset, is an important paradigm of RL. Standard RL\nmethods often perform poorly in this regime due to the function approximation\nerrors on out-of-distribution actions. While a variety of regularization methods\nhave been proposed to mitigate this issue, they are often constrained by policy\nclasses with limited expressiveness that can lead to highly suboptimal solutions.\nIn this paper, we propose representing the policy as a diffusion model, a recent\nclass of highly-expressive deep generative models. We introduce Diffusion Q-\nlearning (Diffusion-QL) that utilizes a conditional diffusion model to represent\nthe policy. In our approach, we learn an action-value function and we add a term\nmaximizing action-values into the training loss of the conditional diffusion model,\nwhich results in a loss that seeks optimal actions that are near the behavior policy.\nWe show the expressiveness of the diffusion model-based policy, and the coupling\nof the behavior cloning and policy improvement under the diffusion model both\ncontribute to the outstanding performance of Diffusion-QL. We illustrate the su-\nperiority of our method compared to prior works in a simple 2D bandit example\nwith a multimodal behavior policy. We then show that our method can achieve\nstate-of-the-art performance on the majority of the D4RL benchmark tasks.\n1\nINTRODUCTION\nOffline reinforcement learning (RL), also known as batch RL, aims at learning effective policies\nentirely from previously collected data without interacting with the environment (Lange et al., 2012;\nFujimoto et al., 2019). Eliminating the need for online interaction with the environment makes\noffline RL attractive for a wide array of real-world applications, such as autonomous driving and\npatient treatment planning, where real-world exploration with an untrained policy is risky, expensive,\nor time-consuming. Instead of relying on real-world exploration, offline RL emphasizes the use of\nprior data, such as human demonstration, that is often available at a much lower cost than online\ninteractions. However, relying only on previously collected data makes offline RL a challenging\ntask. Applying standard policy improvement approaches to an offline dataset typically leads to\nrelying on evaluating actions that have not been seen in the dataset, and therefore their values are\nunlikely to be estimated accurately. For this reason, naive approaches to offline RL typically learn\npoor policies that prefer out-of-distribution actions whose values have been overestimated, resulting\nin unsatisfactory performance (Fujimoto et al., 2019).\nPrevious work on offline RL generally addressed this problem in one of four ways: 1) regularizing\nhow far the policy can deviate from the behavior policy (Fujimoto et al., 2019; Fujimoto & Gu, 2021;\nKumar et al., 2019; Wu et al., 2019; Nair et al., 2020; Lyu et al., 2022); 2) constraining the learned\nvalue function to assign low values to out-of-distribution actions (Kostrikov et al., 2021a; Kumar\net al., 2020); 3) introducing model-based methods, which learn a model of the environment dy-\nnamics and perform pessimistic planning in the learned Markov decision process (MDP) (Kidambi\n\u2217The work was done in part during a summer internship at Twitter.\n\u2020Joint senior authors; order determined by flipping a coin.\n1\narXiv:2208.06193v3  [cs.LG]  25 Aug 2023\nPublished as a conference paper at ICLR 2023\net al., 2020; Yu et al., 2021); 4) treating offline RL as a problem of sequence prediction with return\nguidance (Chen et al., 2021; Janner et al., 2021; 2022). Our approach falls into the first category.\nEmpirically, the performance of policy-regularized offline RL methods is typically slightly worse\nthan that of other approaches, and here we show that this is largely because the policy regularization\nmethods perform poorly due to their limited ability to accurately represent the behavior policy. This\nresults in the regularization adversely affecting the policy improvement. For example, the policy\nregularization may limit the exploration space of the agent to a small region with only suboptimal\nactions and then the Q-learning will be induced to converge towards a suboptimal policy.\nThe inaccurate policy regularization occurs for two main reasons: 1) policy classes are not ex-\npressive enough; 2) the regularization methods are improper. In most prior work, the policy is a\nGaussian distribution with mean and diagonal covariance specified by the output of a neural net-\nwork. However, as offline datasets are often collected by a mixture of policies, the true behavior\npolicy may exhibit strong multi-modalities, skewness, or dependencies between different action di-\nmensions, which cannot be well modeled by diagonal Gaussian policies (Shafiullah et al., 2022). In\na particularly extreme, but not uncommon example, a Gaussian policy is used to fit bimodal training\ndata by minimizing the Kullback\u2013Leibler (KL) divergence from the data distribution to the policy\ndistribution. This will result in the policy exhibiting mode-covering behavior and placing high den-\nsity in the middle area of the two modes, which is actually the low-density region of the training\ndata. In such cases, regularizing a new policy towards the behavior-cloned policy is likely to make\nthe policy learning substantially worse. Second, the regularization, such as the KL divergence and\nmaximum mean discrepancy (MMD) (Kumar et al., 2019), is often not well suited for offline RL.\nThe KL divergence needs access to explicit density values and MMD needs multiple action sam-\nples at each state for optimization. These methods require an extra step by first learning a behavior\ncloned policy to provide density values for KL optimization or random action samples for MMD\noptimization. Regularizing the current policy towards the behavior cloned policy can further induce\napproximation errors, since the cloned behavior policy may not model the true behavior policy well,\ndue to limitations in the expressiveness of the policy class. We conduct a simple bandit experiment\nin Section 4, which illustrates these issues can occur even on a simple bandit task.\nIn this work, we propose a method to perform policy regularization using diffusion (or score-based)\nmodels (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020). Specifically, we use\na multilayer perceptron (MLP) based denoising diffusion probabilistic model (DDPM) (Ho et al.,\n2020) as our policy. We construct an objective for the diffusion loss which contains two terms: 1)\na behavior-cloning term that encourages the diffusion model to sample actions in the same distribu-\ntion as the training set, and 2) a policy improvement term that attempts to sample high-value actions\n(according to a learned Q-value). Our diffusion model is a conditional model with states as the con-\ndition and actions as the outputs. Applying a diffusion model here has several appealing properties.\nFirst, diffusion models are very expressive and can well capture multi-modal distributions. Second,\nthe diffusion model loss constitutes a strong distribution matching technique and hence it could be\nseen as a powerful sample-based policy regularization method without the need for extra behavior\ncloning. Third, diffusion models perform generation via iterative refinement, and the guidance from\nmaximizing the Q-value function can be added at each reverse diffusion step.\nIn summary, our contribution is Diffusion-QL, a new offline RL algorithm that leverages diffu-\nsion models to do precise policy regularization and successfully injects the Q-learning guidance\ninto the reverse diffusion chain to seek optimal actions.\nWe test Diffusion-QL on the D4RL\nbenchmark tasks for offline RL and show this method outperforms prior methods on the ma-\njority of tasks.\nWe also visualize the method on a simple bandit task to illustrate why it can\noutperform prior methods. Code is available at https://github.com/Zhendong-Wang/\nDiffusion-Policies-for-Offline-RL.\n2\nPRELIMINARIES AND RELATED WORK\nOffline RL. The environment in RL is typically defined by a Markov decision process (MDP):\nM = {S, A, P, R, \u03b3, d0}, with state space S, action space A, environment dynamics P(s\u2032 | s, a) :\nS \u00d7 S \u00d7 A \u2192[0, 1], reward function R : S \u00d7 A \u2192R, discount factor \u03b3 \u2208[0, 1), and initial state\ndistribution d0 (Sutton & Barto, 2018). The goal is to learn policy \u03c0\u03b8(a | s), parameterized by \u03b8, that\nmaximizes the cumulative discounted reward E [P\u221e\nt=0 \u03b3tr(st, at)]. The action-value or Q-value of\n2\nPublished as a conference paper at ICLR 2023\na policy \u03c0 is defined as Q\u03c0(st, at) = Eat+1,at+2,...\u223c\u03c0 [P\u221e\nt=0 \u03b3tr(st, at)]. In the offline setting (Fu\net al., 2020), instead of the environment, a static dataset D \u225c{(s, a, r, s\u2032)}, collected by a behavior\npolicy \u03c0b, is provided. Offline RL algorithms learn a policy entirely from this static offline dataset\nD, without online interactions with the environment.\nDiffusion Model. Diffusion-based generative models (Ho et al., 2020; Sohl-Dickstein et al., 2015;\nSong & Ermon, 2019) assume p\u03b8(x0) :=\nR\np\u03b8(x0:T )dx1:T , where x1, . . . , xT are latent variables\nof the same dimensionality as the data x0 \u223cp(x0). A forward diffusion chain gradually adds noise\nto the data x0 \u223cq(x0) in T steps with a pre-defined variance schedule \u03b2i, expressed as\nq(x1:T | x0) := QT\nt=1 q(xt | xt\u22121),\nq(xt | xt\u22121) := N(xt; \u221a1 \u2212\u03b2txt\u22121, \u03b2tI).\nA reverse diffusion chain, constructed as p\u03b8(x0:T ) := N(xT ; 0, I) QT\nt=1 p\u03b8(xt\u22121 | xt), is then\noptimized by maximizing the evidence lower bound defined as Eq[ln\np\u03b8(x0:T )\nq(x1:T | x0)] (Jordan et al.,\n1999; Blei et al., 2017). After training, sampling from the diffusion model consists of sampling\nxT \u223cp(xT ) and running the reverse diffusion chain to go from t = T to t = 0. Diffusion models\ncan be straightforwardly extended to conditional models by conditioning p\u03b8(xt\u22121 | xt, c).\nRelated Work: Policy Regularization. Most prior methods for offline RL in the class of regular-\nized policies rely on behavior cloning for policy regularization: BCQ (Fujimoto et al., 2019) con-\nstructs the policy as a learnable and maximum-value-constrained deviation from a separately learned\nConditional-VAE (CVAE, Sohn et al. (2015)) behavior-cloning model; BEAR (Kumar et al., 2019)\nadds a weighted behavior-cloning loss via minimizing MMD into the policy improvement step;\nTD3+BC (Fujimoto & Gu, 2021) applies the same trick as BEAR via maximum likelihood estima-\ntion (MLE); BRAC (Wu et al., 2019) evaluates multiple methods for behavior-cloning regularization,\nsuch as the KL divergence, MMD, and Wasserstein dual form; IQL (Kostrikov et al., 2021b) is an\nadvantage weighted behavior-cloning method with \u201cin-sample\u201d learned Q-value functions. Goo &\nNiekum (2022) emphasize the necessity of conducting explicit behavioral cloning in offline RL,\nwhile Ajay et al. (2022) admit the power of conditional generative models for decision making.\nRelated Work: Diffusion Models in RL. Pearce et al. (2023) propose to better imitate human\nbehaviors via diffusion models which are expressive and stable. Diffuser (Janner et al., 2022) applies\na diffusion model as a trajectory generator. The full trajectory of state-action pairs form a single\nsample for the diffusion model. A separate return model is learned to predict the cumulative rewards\nof each trajectory sample. The guidance of the return model is then injected into the reverse sampling\nstage. This approach is similar to Decision Transformer (Chen et al., 2021), which also learns a\ntrajectory generator through GPT2 (Radford et al., 2019) with the help of the true trajectory returns.\nWhen used online, sequence models can no longer predict actions from states autoregressively (since\nthe states are an outcome of the environment). Thus, in the evaluation stage, a whole trajectory is\npredicted for each state while only the first action is applied, which incurs a large computational\ncost. Our approach employs diffusion models for RL in a distinct manner.\nWe apply the diffusion model to the action space and we form it as a conditional diffusion model\nwith states as the condition. This approach is model-free and the diffusion model is sampling a\nsingle action at a time. Further, our Q-value function guidance is injected during training, which\nprovides good empirical performance in our case. While both Diffuser (Janner et al., 2022) and our\nwork apply diffusion models in Offline RL, Diffuser is from the model-based trajectory-planning\nperspective while our method is from the offline model-free policy-optimization perspective.\n3\nDIFFUSION Q-LEARNING\nBelow we explain how we apply a conditional diffusion model as an expressive policy for behavior\ncloning. Then, we introduce how we add Q-learning guidance into the learning of our diffusion\nmodel in the training stage with the behavior cloning term acting as a form of policy regularization.\n3.1\nDIFFUSION POLICY\nNotation: Since there are two different types of timesteps in this work, one for the diffusion process\nand one for reinforcement learning we use superscripts i \u2208{1, . . . , N} to denote diffusion timestep\nand subscripts t \u2208{1, . . . , T} to denote trajectory timestep.\n3\nPublished as a conference paper at ICLR 2023\nWe represent our RL policy via the reverse process of a conditional diffusion model as\n\u03c0\u03b8(a | s) = p\u03b8(a0:N | s) = N(aN; 0, I) QN\ni=1 p\u03b8(ai\u22121 | ai, s)\nwhere the end sample of the reverse chain, a0, is the action used for RL evaluation. Generally,\np\u03b8(ai\u22121 | ai, s) could be modeled as a Gaussian distribution N(ai\u22121; \u00b5\u03b8(ai, s, i), \u03a3\u03b8(ai, s, i)).\nWe follow Ho et al. (2020) to parameterize p\u03b8(ai\u22121 | ai, s) as a noise prediction model with the\ncovariance matrix fixed as \u03a3\u03b8(ai, s, i) = \u03b2iI and mean constructed as\n\u00b5\u03b8(ai, s, i) =\n1\n\u221a\u03b1i\n\u0000ai \u2212\n\u03b2i\n\u221a1\u2212\u00af\u03b1i \u03f5\u03b8(ai, s, i)\n\u0001\n.\nWe first sample aN \u223cN(0, I) and then from the reverse diffusion chain parameterized by \u03b8 as\nai\u22121 | ai =\nai\n\u221a\u03b1i \u2212\n\u03b2i\n\u221a\n\u03b1i(1\u2212\u00af\u03b1i)\u03f5\u03b8(ai, s, i) + \u221a\u03b2i\u03f5, \u03f5 \u223cN(0, I), for i = N, . . . , 1.\n(1)\nFollowing DDPM (Ho et al., 2020), when i = 1, \u03f5 is set as 0 to improve the sampling quality.\nWe mimic the simplified objective proposed by Ho et al. (2020) to train our conditional \u03f5-model via\nLd(\u03b8) = Ei\u223cU,\u03f5\u223cN (0,I),(s,a)\u223cD\n\u0002\n||\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1ia +\n\u221a\n1 \u2212\u00af\u03b1i\u03f5, s, i)||2\u0003\n,\n(2)\nwhere U is a uniform distribution over the discrete set as {1, . . . , N} and D denotes the offline\ndataset, collected by behavior policy \u03c0b. This diffusion model loss Ld(\u03b8) is a behavior-cloning\nloss, which aims to learn the behavior policy \u03c0b(a | s) (i.e. it seeks to sample actions from the same\ndistribution as the training data). Note the marginal of the reverse diffusion chain provides an im-\nplicit, expressive distribution that can capture complex distribution properties, such as skewness and\nmulti-modality, exhibited by the offline datasets. In addition, the regularization is sampling-based\nthat only requires taking random samples from both D and the current policy (i.e. this method does\nnot require us to know the behavior policy, which may be infeasible when the dataset is collected\nby human demonstrations). Different from the usual two-step strategy, our strategy provides a clean\nand effective way of applying regularization on a flexible policy.\nLd(\u03b8) can efficiently be optimized by sampling a single diffusion step i for each data point, but the\nreverse sampling in Equation (1), which requires iteratively computing \u03f5\u03b8 networks N times, can\nbecome a bottleneck for the running time. Thus we may want to limit N to a relatively small value.\nTo work with small N, with \u03b2min = 0.1 and \u03b2max = 10.0, we follow Xiao et al. (2021) to define\n\u03b2i = 1 \u2212\u03b1i = 1 \u2212e\u2212\u03b2min( 1\nN )\u22120.5(\u03b2max\u2212\u03b2min) 2i\u22121\nN2 ,\nwhich is a noise schedule obtained under the variance preserving SDE of Song et al. (2021).\n3.2\nQ-LEARNING\nThe policy-regularization loss Ld(\u03b8) is a behavior-cloning term, but would not result in learning\na policy that can outperform the behavior policy that generated the training data. To improve the\npolicy, we inject Q-value function guidance into the reverse diffusion chain in the training stage in\norder to learn to preferentially sample actions with high values. The final policy-learning objective\nis a linear combination of policy regularization and policy improvement:\n\u03c0 = arg min\n\u03c0\u03b8\nL(\u03b8) = Ld(\u03b8) + Lq(\u03b8) = Ld(\u03b8) \u2212\u03b1 \u00b7 Es\u223cD,a0\u223c\u03c0\u03b8\n\u0002\nQ\u03d5(s, a0)\n\u0003\n.\n(3)\nNote that a0 is reparameterized by Equation (1) and hence the gradient of the Q-value function with\nrespect to the action is backpropagated through the whole diffusion chain.\nAs the scale of the Q-value function varies in different offline datasets, to normalize it, we follow\nFujimoto & Gu (2021) to set \u03b1 as \u03b1 =\n\u03b7\nE(s,a)\u223cD[|Q\u03d5(s,a)|], where \u03b7 is a hyperparameter that balances\nthe two loss terms and the Q in the denominator is for normalization only and not differentiated over.\nThe Q-value function itself is learned in a conventional way, minimizing the Bellman operator (Lil-\nlicrap et al., 2015; Fujimoto et al., 2019) with the double Q-learning trick (Hasselt, 2010). We\nbuilt two Q-networks, Q\u03d51, Q\u03d52, and target networks Q\u03d5\u2032\n1, Q\u03d5\u2032\n2 and \u03c0\u03b8\u2032. We then optimize \u03d5i for\ni = {1, 2} by minimizing the objective,\nE(st,at,st+1)\u223cD,a0\nt+1\u223c\u03c0\u03b8\u2032\n\u0014\f\f\f\n\f\f\f\n\u0000r(st, at) + \u03b3 mini=1,2 Q\u03d5\u2032\ni(st+1, a0\nt+1)\n\u0001\n\u2212Q\u03d5i(st, at)\n\f\f\f\n\f\f\f\n2\u0015\n.\n(4)\nWe conduct extensive experiments in Sections 4 and 5 and show that Ld and Lq work together to\nachieve the best performance. We summarize our implementation in Algorithm 1.\n4\nPublished as a conference paper at ICLR 2023\nAlgorithm 1 Diffusion Q-learning\nInitialize policy network \u03c0\u03b8, critic networks Q\u03d51 and Q\u03d52, and target networks \u03c0\u03b8\u2032, Q\u03d5\u2032\n1 and Q\u03d5\u2032\n2\nfor each iteration do\nSample transition mini-batch B = {(st, at, rt, st+1)} \u223cD.\n# Q-value function learning\nSample a0\nt+1 \u223c\u03c0\u03b8\u2032(at+1 | st+1) by Equation (1).\nUpdate Q\u03d51 and Q\u03d52 by Equation (4). (max Q backup by Kumar et al. (2020) could be added)\n# Policy learning\nSample a0\nt \u223c\u03c0\u03b8(at | st) by Equation (1).\nUpdate policy by minimizing Equation (3).\n# Update target networks\n\u03b8\u2032 = \u03c1\u03b8\u2032 + (1 \u2212\u03c1)\u03b8, \u03d5\u2032\ni = \u03c1\u03d5\u2032\ni + (1 \u2212\u03c1)\u03d5i for i = {1, 2}.\nend for\n4\nPOLICY REGULARIZATION\nIn this section, we illustrate how the previous policy regularization methods work compared to our\nconditional diffusion-based approach on a simple bandit task with a 2D continuous action space.\nBelow we first provide a brief review of prior methods.\nBC-MLE. The policy \u03c0\u03b8(a | s) is modeled by a Gaussian distribution N(a; \u00b5\u03b8(st), \u03a3\u03b8(s)), where\nusually \u00b5\u03b8 and \u03a3\u03b8 are parameterized by multi-layer perceptrons (MLPs) and for simplicity \u03a3\u03b8 is\nassumed to be a diagonal matrix. The policy is optimized by maximizing E(s,a)\u223cD[log \u03c0\u03b8(a | s)].\nTD3+BC (Fujimoto & Gu, 2021) directly add the behavior-cloning (BC) loss as an additional term in\npolicy learning, while IQL (Kostrikov et al., 2021b) proposes using \u201cin-sample\u201d learned advantage\nfunctions to reweigh the log term inside the expectation.\nBC-CVAE. The policy \u03c0\u03b8(a | s) is modeled by a CVAE model with an encoder network q\u03b8(z | s, a)\nand a decoder network p\u03b8(a | s, z). The two networks are optimized by maximizing the evidence\nlower bound E(s,a)\u223cD[Ez\u223cq(\u00b7 | s,a)[log p(a | s, z)] \u2212KL(q(z | s, a)||p(z))], where p(z) is a prior\ndistribution that is usually set as standard Gaussian. BCQ (Fujimoto et al., 2019) trains a CVAE\nmodel as an approximation of the behavior policy and trains another deviation model to guide the\nactions drawn from the CVAE approximation towards the regions with high learned Q-values.\nBC-MMD. BEAR (Kumar et al., 2019) also mimics the behavior policy via a CVAE model and\nproposes to limit the current policy \u03c0\u03b8(a | s) to be close to the cloned behavior policy via MMD\nminimization. A Tanh-Gaussian policy is used, which is a Gaussian network with a Tanh activation\nfunction at the output layer.\nBCQ and BEAR can be seen as a two-step regularization: First, an approximation of the behavior\npolicy is learned (behavior cloning), and then the policy learned by policy improvement is regular-\nized towards the cloned behavior policy. However, such a two-step approach means that the efficacy\nof the second-step policy regularization heavily depends on the cloning quality, and an inaccurate\nregularization could misguide the subsequent policy improvement step. We illustrate this weakness\nin a 2D continuous action space bandit example.\nExample. We consider a simple bandit task with real-valued actions in a 2D space, a \u2208[\u22121, 1]2.\nWe construct an offline dataset D = {(aj)}M\nj=1 with M = 10000 action examples, where the\nactions are collected from an equal mixture of four Gaussian distributions with centers \u00b5 \u2208\n{(0.0, 0.8), (0.8, 0.0), (0.0, \u22120.8), (\u22120.8, 0.0)} and standard deviations \u03c3d = (0.05, 0.05), as de-\npicted in the first panel of Figure 1. Note that this example exhibits strong multi-modality in the\nbehavior policy distribution, which is often the case when the dataset is collected by different poli-\ncies. For example, if multiple humans are involved, some may be experts and choose actions from a\ndifferent mode from amateur demonstrators.\nTo evaluate the strength of prior regularization methods, we first compare them to our diffusion-\nbased approach on a behavior-cloning task, where the goal is to just clone the behavior policy that\ngenerated the data, not improve on it. As shown in the first row of Figure 1, we observe that the\ndiffusion model captures all the four density modes of the behavior policy. The policy of BC-MLE\nis limited to a single mode and hence exhibits a strong mode-covering behavior. It fits the four den-\n5\nPublished as a conference paper at ICLR 2023\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nGround Truth\n2\n1\n0\n1\n2\nx\n2\n1\n0\n1\n2\ny\nBC-MLE\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-CVAE\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-MMD\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nAdd Reward\nr\nN(3.0, 0.5)\nr\nN(0.0, 0.5)\nr\nN(1.5, 0.5)\nr\nN(5.0, 0.5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBCQ\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBEAR-MMD\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL\nFigure 1: Offline RL experiments on a simple bandit task. The first row shows the comparison of behavior\ncloning between our method (BC-Diffusion, N = 50) and prior methods. Prior methods struggle to capture\nthe multi-modal behavior policy (ground truth). The second row shows the comparison results when policy\nimprovement is added (first figure shows the rewards). The policy regularization of prior methods results in a\npoor policy, since the behavior-cloning step fails to capture the multi-modal behavior policy, while our method\n(Diffusion-QL) correctly identifies the high reward behavior mode.\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion(N=2)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion(N=5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion(N=10)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion(N=20)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion(N=50)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=2)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=10)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=20)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=50)\nFigure 2: Experiment examining the effect of varying the number of diffusion steps N on the simple bandit\ntalks. The first row shows the ablation study of N for BC-Diffusion. The second row shows the ablation study\nof N for Diffusion-QL. Large N results in a better fit to the data distribution, but leads to a higher computational\ncost. By combining the behavior cloning and Q-value losses during training, we are able to learn near-optimal\npolicies with fewer diffusion steps.\nsity modes by one Gaussian distribution with a large standard deviation, whose high-density regions\nare actually the low-density regions of the true behavior policy. The CVAE model is shown to ex-\nhibit mode-covering behavior even though it itself is an implicit model with enough expressiveness:\nWe see that CVAE captures the four modes but high densities are assigned between them to cover\nall the modes. Note we also observe the CVAE model sometimes fails to capture all four modes\nwith different random seeds. The Tanh-Gaussian policy optimized under MMD learns to align the\ndensities near the boundary lines and, due to its limited policy expressiveness, fails to capture the\ntrue distribution. These observed failures on behavior cloning in this simple example illustrate the\nlimitations of prior regularization methods in the common case of multi-modal behavior policies.\nNext, we investigate how the policy improvement will be impacted by the corresponding policy\nregularization. We assign each data point a reward sampled from a Gaussian distribution, whose\nmean is determined by the data center and standard deviation is fixed as 0.5, as shown in the second\nrow of Figure 1. Note here we mimic the offline RL setting, under which the underlying reward\nfunction is unknown and needs to be learned. We compare Diffusion Q-learning (QL) with prior\nmethods, including TD3+BC, BCQ, and BEAR-MMD. We train all methods with 1000 epochs to\nensure convergence. Due to the strong policy constraint applied by each method, we observe that\nthe policy improvement of prior methods is constrained to suboptimal or even wrong exploration\nregions, induced by the corresponding behavior-cloning regularization. TD3+BC cannot converge\nto the optimal mode since the behavior policy places most density in the region where no offline data\nexists. BCQ learns to place major actions on the four diagonal corners discovered by its CVAE-based\nbehavior cloning. The policy of BEAR-MMD learns to place actions randomly since the exploration\nregion is constrained by inaccurate policy regularization. We observe that the prior regularizations\ntypically push the policy to converge to sub-optimal solutions, such as BCQ, or prevent the policy\n6\nPublished as a conference paper at ICLR 2023\nfrom being concentrated on the optimal corner, such as TD3+BC and BEAR-MMD. By contrast,\nthe policy of Diffusion-QL successfully converges to the optimal bottom corner. This is because\n1) diffusion policy is expressive enough to recover the behavior policy, which covers all modes for\nfurther exploration; 2) the Q-learning guidance, directly through linearly combined loss functions\nin Equation (3), helps diffusion policy seek optimal actions in the region. The two components are\nworking together to produce good performance.\nDiffusions steps. We further investigated how the diffusion policy performs as the number of dif-\nfusion timesteps N is varied. As expected, the first row of Figure 2 shows that as N increases, the\ndiffusion model becomes more expressive and learns more details about the underlying data distri-\nbution. When N is increased to 50, the true data distribution is accurately recovered. The second\nrow shows that with Q-learning applied, a moderately small N is able to deliver good performance\ndue to our loss coupling defined in Equation (3). However, we can see with a larger N the policy\nregularization imposed by the diffusion model-based cloning becomes stronger. For example, when\nN = 2, there are still a few action points sampled near regions with no training data, whereas when\nN = 50, the policy is constrained in the correct data region for further exploration. The num-\nber of timesteps N serves as a trade-off between policy expressiveness and computational cost for\nDiffusion-QL. We found N = 5 performs well on D4RL (Fu et al., 2020) datasets, which is also a\nsmall enough value for cost-effective training and deployment.\n5\nEXPERIMENTS\nWe evaluate our method on the popular D4RL (Fu et al., 2020) benchmark. Further, we conduct\nempirical studies on the number of timesteps required for our diffusion model and also perform an\nablation study for analyzing the contribution of the two main components of Diffusion-QL.\nDatasets. We consider four different domains of tasks in D4RL benchmark: Gym, AntMaze, Adroit,\nand Kitchen. The Gym-MuJoCo locomotion tasks are the most commonly used standard tasks for\nevaluation and are relatively easy, since they usually include a significant fraction of near-optimal\ntrajectories in the dataset and the reward function is quite smooth. AntMaze consists of more chal-\nlenging tasks, which have sparse rewards and explicitly need the agent to stitch various sub-optimal\ntrajectories to find a path towards the goal of the maze (Fu et al., 2020). Adroit datasets are mostly\ncollected by human behavior and the state-action region reflected by the offline data is often very\nnarrow, so strong policy regularization is needed to ensure that the agent stays in the expected re-\ngion. The Kitchen environment requires the agent to complete 4 target subtasks in order to reach a\ndesired state configuration, and hence long-term value optimization is important for it.\nBaselines. We consider different classes of baselines that perform well in each domain of tasks. For\npolicy regularization-based methods, we include the classic BC, BEAR (Kumar et al., 2019), BRAC\n(Wu et al., 2019), BCQ (Fujimoto et al., 2019), TD3+BC (Fujimoto & Gu, 2021), AWR (Peng et al.,\n2019), AWAC (Nair et al., 2020), and IQL (Kostrikov et al., 2021b), along with the Onestep RL\n(Brandfonbrener et al., 2021), which is based on single-step improvement. For Q-value constraint\nmethods, we include REM (Agarwal et al., 2020) and CQL (Kumar et al., 2020). For model-based\noffline RL, we consider MoRel (Kidambi et al., 2020). For sequence modelling approaches, we\ncompare with Decision Transformer (DT) (Chen et al., 2021) and Diffuser (Janner et al., 2022). We\nreport the performance of baseline methods either using the best results reported from their own\npaper, Fu et al. (2020) or Kostrikov et al. (2021b).\nExperimental details. We train for 1000 epochs (2000 for Gym tasks). Each epoch consists of\n1000 gradient steps with batch size 256. The training is usually quite stable as shown in Figure 3\nexcept that we observe the training for AntMaze tasks has variations due to its sparse reward setting\nand lack of optimal trajectories in the offline datasets. Hence, we save multiple model checkpoints\nduring training and use a completely offline method, as described in Appendix D, to select the best\ncheckpoint for performance evaluation. Using Ld loss as a lagging indicator of online performance,\nwe perform early stopping and select the checkpoint with the second or third lowest Ld value. The\nresults in our main paper are all based on the offline model selection. When a small amount of online\ninteraction can be used for model selection, we can achieve even better results (Appendix Table 4).\nEffect of hyperparameter N. We conduct an empirical study on the effect of the number of\ntimesteps N of our diffusion model on real tasks. As shown in Figure 3, empirically we find as\n7\nPublished as a conference paper at ICLR 2023\n0\n10\n20\n30\n40\nTraining Progress (# 50k steps)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nhalfcheetah-medium-expert-v2\nN=2\nN=5\nN=10\nN=20\n0\n10\n20\n30\n40\nTraining Progress (# 50k steps)\n0.2\n0.4\n0.6\n0.8\n1.0\nNormalized Score\nhopper-medium-expert-v2\nN=2\nN=5\nN=10\nN=20\n0\n10\n20\n30\n40\nTraining Progress (# 50k steps)\n0.00\n0.25\n0.50\n0.75\n1.00\n1.25\nNormalized Score\nwalker2d-medium-expert-v2\nN=2\nN=5\nN=10\nN=20\nFigure 3: Ablation study of N on selected Gym tasks. We consider a N grid [2, 5, 10, 20] and we find that\nN = 5 is good enough for the selected tasks.\nTable 1: The performance of Diffusion-QL and SOTA baselines on D4RL Gym, AntMaze, Adroit, and Kitchen\ntasks. Results for Diffusion-QL correspond to the mean and standard errors of normalized scores over 50\nrandom rollouts (5 independently trained models and 10 trajectories per model) for Gym tasks, which generally\nexhibit low variance in performance, and over 500 random rollouts (5 independently trained models and 100\ntrajectories per model) for the other tasks. Note the standard error of AntMaze is usually large since the return\nof trajectories is binomial (1 for success while 0 for failure). Our method outperforms all prior methods by a\nclear margin on all domain, even on the challenging AntMaze, for which behavior cloning methods would fail\nand some form of policy improvement is essential.\nGym Tasks\nBC\nAWAC\nDiffuser\nMoRel\nOnestep RL\nTD3+BC\nDT\nCQL\nIQL\nDiffusion-QL\nhalfcheetah-medium-v2\n42.6\n43.5\n44.2\n42.1\n48.4\n48.3\n42.6\n44.0\n47.4\n51.1 \u00b1 0.5\nhopper-medium-v2\n52.9\n57.0\n58.5\n95.4\n59.6\n59.3\n67.6\n58.5\n66.3\n90.5 \u00b1 4.6\nwalker2d-medium-v2\n75.3\n72.4\n79.7\n77.8\n81.8\n83.7\n74.0\n72.5\n78.3\n87.0 \u00b1 0.9\nhalfcheetah-medium-replay-v2\n36.6\n40.5\n42.2\n40.2\n38.1\n44.6\n36.6\n45.5\n44.2\n47.8 \u00b1 0.3\nhopper-medium-replay-v2\n18.1\n37.2\n96.8\n93.6\n97.5\n60.9\n82.7\n95.0\n94.7\n101.3 \u00b1 0.6\nwalker2d-medium-replay-v2\n26.0\n27.0\n61.2\n49.8\n49.5\n81.8\n66.6\n77.2\n73.9\n95.5 \u00b1 1.5\nhalfcheetah-medium-expert-v2\n55.2\n42.8\n79.8\n53.3\n93.4\n90.7\n86.8\n91.6\n86.7\n96.8 \u00b1 0.3\nhopper-medium-expert-v2\n52.5\n55.8\n107.2\n108.7\n103.3\n98.0\n107.6\n105.4\n91.5\n111.1 \u00b1 1.3\nwalker2d-medium-expert-v2\n107.5\n74.5\n108.4\n95.6\n113.0\n110.1\n108.1\n108.8\n109.6\n110.1 \u00b1 0.3\nAverage\n51.9\n50.1\n75.3\n72.9\n76.1\n75.3\n74.7\n77.6\n77.0\n88.0\nAntMaze Tasks\nBC\nAWAC\nBCQ\nBEAR\nOnestep RL\nTD3+BC\nDT\nCQL\nIQL\nDiffusion-QL\nantmaze-umaze-v0\n54.6\n56.7\n78.9\n73.0\n64.3\n78.6\n59.2\n74.0\n87.5\n93.4 \u00b1 3.4\nantmaze-umaze-diverse-v0\n45.6\n49.3\n55.0\n61.0\n60.7\n71.4\n53.0\n84.0\n62.2\n66.2 \u00b1 8.6\nantmaze-medium-play-v0\n0.0\n0.0\n0.0\n0.0\n0.3\n10.6\n0.0\n61.2\n71.2\n76.6 \u00b1 10.8\nantmaze-medium-diverse-v0\n0.0\n0.7\n0.0\n8.0\n0.0\n3.0\n0.0\n53.7\n70.0\n78.6 \u00b1 10.3\nantmaze-large-play-v0\n0.0\n0.0\n6.7\n0.0\n0.0\n0.2\n0.0\n15.8\n39.6\n46.4 \u00b1 8.3\nantmaze-large-diverse-v0\n0.0\n1.0\n2.2\n0.0\n0.0\n0.0\n0.0\n14.9\n47.5\n56.6 \u00b1 7.6\nAverage\n16.7\n18.0\n23.8\n23.7\n20.9\n27.3\n18.7\n50.6\n63.0\n69.6\nAdroit Tasks\nBC\nSAC\nBCQ\nBEAR\nBRAC-p\nBRAC-v\nREM\nCQL\nIQL\nDiffusion-QL\npen-human-v1\n25.8\n4.3\n68.9\n-1.0\n8.1\n0.6\n5.4\n35.2\n71.5\n72.8 \u00b1 9.6\npen-cloned-v1\n38.3\n-0.8\n44.0\n26.5\n1.6\n-2.5\n-1.0\n27.2\n37.3\n57.3 \u00b1 11.9\nAverage\n32.1\n1.8\n56.5\n12.8\n4.9\n-1.0\n2.2\n31.2\n54.4\n65.1\nKitchen Tasks\nBC\nSAC\nBCQ\nBEAR\nBRAC-p\nBRAC-v\nAWR\nCQL\nIQL\nDiffusion-QL\nkitchen-complete-v0\n33.8\n15.0\n8.1\n0.0\n0.0\n0.0\n0.0\n43.8\n62.5\n84.0 \u00b1 7.4\nkitchen-partial-v0\n33.8\n0.0\n18.9\n13.1\n0.0\n0.0\n15.4\n49.8\n46.3\n60.5 \u00b1 6.9\nkitchen-mixed-v0\n47.5\n2.5\n8.1\n47.2\n0.0\n0.0\n10.6\n51.0\n51.0\n62.6 \u00b1 5.1\nAverage\n38.4\n5.8\n11.7\n20.1\n0.0\n0.0\n8.7\n48.2\n53.3\n69.0\nthe N increases, the model converges faster and the performance becomes more stable. In the fol-\nlowing D4RL tasks, we set a moderate value, N = 5, to balance the performance and computational\ncost. With N = 5, the training time of our method is similar to that of CQL (Kumar et al., 2020).\nThe other hyperparameters, such as the learning rate and \u03b7, are provided in Appendix E.\n5.1\nCOMPARISON TO OTHER METHODS\nWe compare our Diffusion-QL with the baselines on four domains of tasks and report the results in\nTable 1. We give the analysis based on each specific domain.\nResults for Gym Domain. We can see while most baselines already work well on the Gym tasks,\nDiffusion-QL can often further improve their performance by a clear margin, especially in \u2018medium\u2019\n8\nPublished as a conference paper at ICLR 2023\nTable 2: Ablation study. We conduct an ablation study to compare our diffusion model with CVAE models, and\nour Q-learning method with BCQ policy improvement.\nGym Tasks\nBC-CVAE\nBC-Diffusion\nBCQ-CVAE\nBCQ-Diffusion\nCVAE-QL\nDiffusion-QL\nhalfcheetah-medium-expert-v2\n67.3 \u00b1 7.4\n76.6 \u00b1 7.0\n96.1 \u00b1 0.5\n94.6 \u00b1 1.0\n70.3 \u00b1 5.5\n97.2 \u00b1 0.4\nhopper-medium-expert-v2\n69.9 \u00b1 8.6\n78.0 \u00b1 8.9\n108.5 \u00b1 0.6\n109.3 \u00b1 0.9\n109.2 \u00b1 4.6\n112.3 \u00b1 0.8\nwalker2d-medium-expert-v2\n102.5 \u00b1 4.4\n103.1 \u00b1 4.4\n110.7 \u00b1 0.2\n114.0 \u00b1 0.5\n50.5 \u00b1 38.2\n111.2 \u00b1 0.9\nAverage\n79.9\n85.9\n105.1\n106.0\n76.6\n106.9\nand \u2018medium-replay\u2019 tasks. Note the \u2018medium\u2019 datasets include the trajectories collected by an\nonline SAC (Haarnoja et al., 2018) agent trained to approximately 1/3 the performance of the expert.\nHence, the Tanh-Gaussian policy at that time is usually exploratory and not concentrated, which\nmakes the collected data distribution hard to learn. As shown in Section 4, the diffusion model\nhas the expressivity to mimic the behavior policy even in complicated cases and then the policy\nimprovement term will guide the policy to converge to the optimal actions in the subset of explored\naction space. These two components are key to the good empirical performance of Diffusion-QL.\nResults for AntMaze Domain. The sparse reward and large amount of sub-optimal trajectories\nmake the AntMaze tasks especially difficult. Strong and stable Q-learning is required to achieve\ngood performance. For example, BC-based methods could easily fail on \u2018medium\u2019 and \u2018large\u2019 tasks.\nWe show that the proposed Q-learning guidance added during the training of a conditional diffusion\nmodel is stable and effective. Empirically, with a proper \u03b7, we find Diffusion-QL outperforms the\nprior methods by a clear margin, especially in harder tasks, such as \u2018large-diverse\u2019.\nResults for Adroit and Kitchen Domain. We find the Adroit domain needs strong policy reg-\nularization to overcome the extrapolation error (Fujimoto et al., 2019) in offline RL, due to the\nnarrowness of the human demonstrations. With a smaller \u03b7, Diffusion-QL easily beats the other\nbaselines by its reverse diffusion-based policy, which has high expressiveness and better policy reg-\nularization. Moreover, long-term value optimization is required for the Kitchen tasks, and we find\nDiffusion-QL also performs very well in this domain.\n5.2\nABLATION STUDY\nIn this section, we analyze why Diffusion-QL outperforms the other policy-constraint based meth-\nods quantitatively on D4RL tasks. We conduct an ablation study on the two main components of\nDiffusion-QL: use of a diffusion model as an expressive policy and Q-learning guidance. For the\npolicy part, we compare our diffusion model with the popular CVAE model for behavior cloning.\nFor the Q-learning component, we compare with the BCQ approach on policy improvement.\nAs shown in Table 2, BC-Diffusion model outperforms BC-CVAE, validating that our diffusion-\nbased policy is more expressive and better at capturing the data distributions (as we would expect\nfrom the results in figure 1). Under the same BCQ framework, which explicitly limits how far the ac-\ntion samples from polices could deviate from the cloned action samples, BCQ-Diffusion still works\nbetter than BCQ-CVAE. The hard and physical value constraints on the deviation by BCQ actually\nlimits the policy improvement, as we can see that, Diffusion-QL further boosts the performance.\nNote Diffusion-QL applies the diffusion model learning itself as a soft policy regularization and\nguides the policy optimization via additive Q-learning. The weak expressiveness and poor cloning\nbehavior of CVAE makes it fail when coupling with a free Q-learning guidance, as shown by the re-\nsults of CVAE-QL. In a nutshell, the ablation study shows that the two components of Diffusion-QL\nare working together to produce good performance.\n6\nCONCLUSION\nIn this work, we present Diffusion-QL, which is a conditional diffusion-based offline RL algorithm.\nFirst, its policy is built by the reverse chain of a conditional diffusion model, which allows for\na highly expressive policy class and whose learning itself acts as a strong policy regularization\nmethod. Second, Q-learning guidance through a jointly learned Q-value function is injected in the\nlearning of the diffusion policy, which guides the denoising sampling towards the optimal region in\nits exploration area. The two key components contribute to its state-of-the-art performance across\nall tasks in the D4RL benchmark.\n9\nPublished as a conference paper at ICLR 2023\nACKNOWLEDGEMENTS\nZ. Wang and M. Zhou acknowledge the support of NSF-IIS 2212418 and the Texas Advanced Com-\nputing Center (TACC) for providing HPC resources that have contributed to the research results\nreported within this paper.\nREFERENCES\nRishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on offline\nreinforcement learning. In International Conference on Machine Learning, pp. 104\u2013114. PMLR,\n2020.\nAnurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal.\nIs conditional generative modeling all you need for decision-making?\narXiv preprint\narXiv:2211.15657, 2022.\nChristopher M Bishop. Mixture density networks. 1994.\nDavid M Blei, Alp Kucukelbir, and Jon D McAuliffe. Variational inference: A review for statisti-\ncians. Journal of the American statistical Association, 112(518):859\u2013877, 2017.\nDavid Brandfonbrener, Will Whitney, Rajesh Ranganath, and Joan Bruna. Offline RL without off-\npolicy evaluation. Advances in Neural Information Processing Systems, 34:4933\u20134946, 2021.\nLili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel,\nAravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence\nmodeling. Advances in neural information processing systems, 34, 2021.\nJustin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4RL: Datasets for deep\ndata-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\nScott Fujimoto and Shixiang Shane Gu. A minimalist approach to offline reinforcement learning.\nAdvances in Neural Information Processing Systems, 34, 2021.\nScott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning without\nexploration. In International Conference on Machine Learning, pp. 2052\u20132062. PMLR, 2019.\nWonjoon Goo and Scott Niekum. Know your boundaries: The necessity of explicit behavioral\ncloning in offline rl. arXiv preprint arXiv:2206.00695, 2022.\nTuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy\nmaximum entropy deep reinforcement learning with a stochastic actor. In International confer-\nence on machine learning, pp. 1861\u20131870. PMLR, 2018.\nHado Hasselt. Double Q-learning. Advances in neural information processing systems, 23, 2010.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in\nNeural Information Processing Systems, 33:6840\u20136851, 2020.\nMichael Janner, Qiyang Li, and Sergey Levine. Offline reinforcement learning as one big sequence\nmodeling problem. Advances in neural information processing systems, 34, 2021.\nMichael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for\nflexible behavior synthesis. arXiv preprint arXiv:2205.09991, 2022.\nMichael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction\nto variational methods for graphical models. Machine learning, 37(2):183\u2013233, 1999.\nRahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel: Model-\nbased offline reinforcement learning. Advances in neural information processing systems, 33:\n21810\u201321823, 2020.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\n10\nPublished as a conference paper at ICLR 2023\nIlya Kostrikov, Rob Fergus, Jonathan Tompson, and Ofir Nachum. Offline reinforcement learning\nwith fisher divergence critic regularization. In International Conference on Machine Learning,\npp. 5774\u20135783. PMLR, 2021a.\nIlya Kostrikov, Ashvin Nair, and Sergey Levine. Offline reinforcement learning with implicit Q-\nlearning. arXiv preprint arXiv:2110.06169, 2021b.\nAviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy Q-\nlearning via bootstrapping error reduction. Advances in Neural Information Processing Systems,\n32, 2019.\nAviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative Q-learning for offline\nreinforcement learning.\nAdvances in Neural Information Processing Systems, 33:1179\u20131191,\n2020.\nSascha Lange, Thomas Gabel, and Martin Riedmiller. Batch reinforcement learning. In Reinforce-\nment learning, pp. 45\u201373. Springer, 2012.\nTimothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa,\nDavid Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. arXiv\npreprint arXiv:1509.02971, 2015.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu.\nDpm-solver: A\nfast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\nJiafei Lyu, Xiaoteng Ma, Xiu Li, and Zongqing Lu. Mildly conservative Q-learning for offline\nreinforcement learning. arXiv preprint arXiv:2206.04745, 2022.\nAshvin Nair, Abhishek Gupta, Murtaza Dalal, and Sergey Levine. Awac: Accelerating online rein-\nforcement learning with offline datasets. arXiv preprint arXiv:2006.09359, 2020.\nTim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Ser-\ngio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human\nbehaviour with diffusion models. arXiv preprint arXiv:2301.10677, 2023.\nXue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nTim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv\npreprint arXiv:2202.00512, 2022.\nNur Muhammad Mahi Shafiullah, Zichen Jeff Cui, Ariuntuya Altanzaya, and Lerrel Pinto. Behavior\ntransformers: Cloning k modes with one stone. arXiv preprint arXiv:2206.11251, 2022.\nJascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsuper-\nvised learning using nonequilibrium thermodynamics. ArXiv, abs/1503.03585, 2015.\nKihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep\nconditional generative models. Advances in neural information processing systems, 28, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn Advances in Neural Information Processing Systems, pp. 11918\u201311930, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In Interna-\ntional Conference on Learning Representations, 2021. URL https://openreview.net/\nforum?id=PxTIG12RRHS.\n11\nPublished as a conference paper at ICLR 2023\nRichard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.\nZhendong Wang, Huangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Diffusion-\nGAN: Training gans with diffusion. arXiv preprint arXiv:2206.02262, 2022.\nYifan Wu, George Tucker, and Ofir Nachum. Behavior regularized offline reinforcement learning.\narXiv preprint arXiv:1911.11361, 2019.\nZhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with\ndenoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021.\nTianhe Yu, Aviral Kumar, Rafael Rafailov, Aravind Rajeswaran, Sergey Levine, and Chelsea Finn.\nCombo: Conservative offline model-based policy optimization. Advances in neural information\nprocessing systems, 34:28954\u201328967, 2021.\nHuangjie Zheng, Pengcheng He, Weizhu Chen, and Mingyuan Zhou. Truncated diffusion proba-\nbilistic models and diffusion-based adversarial auto-encoders. arXiv preprint arXiv:2202.09671,\n2022.\n12\nPublished as a conference paper at ICLR 2023\nAppendix\nA\nMORE TOY EXPERIMENTS\nHere we describe an additional toy experiment on a bandit task.\nActions are again in\na real-valued 2D space, a\n\u2208\n[\u22121, 1]2.\nThe offline data D\n=\n{(ai)}10000\ni=1\nare col-\nlected by sampling actions equally from four Gaussian distributions with centers \u00b5\n\u2208\n{(\u22120.8, 0.8), (0.8, 0.8), (0.8, \u22120.8), (\u22120.8, \u22120.8)} and standard deviations \u03c3d = (0.05, 0.05), as\ndepicted in the first panel of Figure 4. We conduct the same experiments as the ones in our main\npaper (see figure 1) and show the performance in Figure 4. The only difference in this experiment is\nthat the samples are now in the corners of the ation space.\nFor behavior-cloning experiments, we observe that only our diffusion model could recover the orig-\ninal data distribution while the prior regularization methods fail in some way. For example, CVAE\ncould only capture the two diagonal modes and place density between them, while MMD tends to\nalign density around the boundaries because of its Tanh-Gaussian policy. For Q-learning experi-\nments, we observe that the prior regularization methods typically push the policy to converge to\nsub-optimal solutions in BCQ and BEAR while preventing the policy of TD3+BC from being con-\ncentrated on the right corner. However, the policy of Diffusion-QL successfully converges to the\noptimal bottom corner. The ablation study experiments are consistent with our conclusion in the\nmain paper.\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nGround Truth\n2\n1\n0\n1\n2\nx\n2\n1\n0\n1\n2\ny\nBC-MLE\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-CVAE\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-MMD\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBC-Diffusion\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nAdd Reward\nr\nN(3.0, 0.5)\nr\nN(0.0, 0.5)\nr\nN(1.5, 0.5)\nr\nN(5.0, 0.5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBCQ\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nBEAR-MMD\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nN=2\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nN=5\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nN=10\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nN=20\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nN=50\n1.0\n0.5\n0.0\n0.5\n1.0\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nLearned Reward Function\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=10)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=20)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nDiffusion-QL(N=50)\n0\n1\n2\n3\n4\nFigure 4: Bandit experiment with a strong multi-modal behavior policy. The first row shows the comparison\nof behavior-cloning results between our method and prior methods. The second row shows the comparison\nresults with Q-learning involved. The third row shows the ablation study of N for BC-Diffusion. The fourth\nrow shows the learned reward function and the ablation study of N for Diffusion-QL.\n13\nPublished as a conference paper at ICLR 2023\nB\nIMPLEMENTATION DETAILS\nDiffusion policy. We build our policy as an MLP-based conditional diffusion model. Following the\nparameterization of Ho et al. (2020), the model itself is a residual model, \u03f5\u03b8(ai, s, i), where the i is\nthe last timestep and s is the state condition. We model \u03f5\u03b8 as a 3-layer MLPs with Mish activations\nand we use 256 hidden units for all networks. The input of \u03f5\u03b8 is the concatenation of the last step\naction vector, the current state vector, and the sinusoidal positional embedding of timestep i. The\noutput of \u03f5\u03b8 is the predicted residual at diffusion timestep i.\nQ networks. We build two Q networks with the same MLP setting as our diffusion policy, which\nhas 3-layer MLPs with Mish activations and 256 hidden units for all networks.\nWe use the Adam (Kingma & Ba, 2014) optimizer for the training of both Diffusion policy and Q\nnetworks.\nC\nEXPERIMENTAL DETAILS\nWe train our algorithm with 2000 epochs for Gym tasks and 1000 epochs for the other tasks, where\neach epoch consists of 1000 gradient steps. For the Gym locomotion tasks, we average mean returns\nover 6 independently trained models and 10 trajectories per mode. For the other tasks, we average\nover 6 independently trained models and 100 evaluation trajectories.\nWe investigate two model selection methods, online and offline model selection. For online model\nselection, following the convention of supervised learning, the best-performed model is saved during\ntraining and used for final evaluation. For offline case, we select the model based on our lagging\nindicator Ld and more details are in Appendix D.\nWe use the original task rewards from MuJoCo Gym and Kitchen tasks. We standardize Adroit task\nrewards for training stability. We modify the rewards according to the suggestion of CQL (Kumar\net al., 2020) for the AntMaze datasets.\nD\nOFFLINE MODEL SELECTION\nFor reducing the training cost and picking the best model during training without any interaction\nwith the real environment, we provide a way to properly conduct early stopping for Diffusion-\nQL. Empirically, we found that Ld loss is a lagging indicator of online performance. Note Ld is\nthe behavior cloning loss of our diffusion model-based policy and measures how close the current\npolicy is to the behavior policy. For offline RL, we want Ld to be small to get rid of the OOD\nissue but we also don\u2019t expect Ld to be the smallest since our goal is policy learning while not\nbehavior cloning. Hence, we monitor the Ld and save multiple model checkpoints during training.\nWe stop the training whenever Ld increases in our evaluation stage for early stopping. When the\ntraining stopped, we picked the checkpoint according to our metric, the second or third lowest Ld\nvalue. Note our model selection part is totally offline and only based on Ld without any access\nto the environment. The selection of the Ld is not very sensitive. Always using the 2nd smallest\ncheckpoints doesn\u2019t impact the performance much. For example, with 2nd checkpoints selected, for\nthe Gym domain, the average score is 87.6, and for the AntMaze domain, the average score is 69.1.\nE\nHYPERPARAMETERS\nFor Diffusion-QL, we consider three hyperparameters in total: learning rate, Q-learning weight \u03b7,\nand whether to use max Q backup from CQL (Kumar et al., 2020). For the learning rate of Adam,\nwe consider values in the grid {1 \u00d7 10\u22123, 3 \u00d7 10\u22124, 3 \u00d7 10\u22125} for the policy, while we use a fixed\nlearning rate, 3 \u00d7 10\u22124, for Q-networks. For \u03b7, we consider values according to the characteristics\nof different domains, as we mentioned in the description of datasets that the Adroit and Kitchen\ntasks require more policy regularization and the AntMaze tasks require more Q-learning. For max\nQ backup, we only apply it on the AntMaze tasks. Based on these considerations, we provide our\nhyperparameter setting in Table 3.\n14\nPublished as a conference paper at ICLR 2023\nTable 3: Hyperparameter settings of all selected tasks.\nTasks\nlearning rate\n\u03b7\nmax Q backup\nhalfcheetah-medium-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nhopper-medium-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nwalker2d-medium-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nhalfcheetah-medium-replay-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nhopper-medium-replay-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nwalker2d-medium-replay-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nhalfcheetah-medium-expert-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nhopper-medium-expert-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nwalker2d-medium-expert-v2\n3 \u00d7 10\u22124\n1.0\nFalse\nantmaze-umaze-v0\n3 \u00d7 10\u22124\n0.5\nFalse\nantmaze-umaze-diverse-v0\n3 \u00d7 10\u22124\n2.0\nTrue\nantmaze-medium-play-v0\n1 \u00d7 10\u22123\n2.0\nTrue\nantmaze-medium-diverse-v0\n3 \u00d7 10\u22124\n3.0\nTrue\nantmaze-large-play-v0\n3 \u00d7 10\u22124\n4.5\nTrue\nantmaze-large-diverse-v0\n3 \u00d7 10\u22124\n3.5\nTrue\npen-human-v1\n3 \u00d7 10\u22125\n0.15\nFalse\npen-cloned-v1\n3 \u00d7 10\u22125\n0.1\nFalse\nkitchen-complete-v0\n3 \u00d7 10\u22124\n0.005\nFalse\nkitchen-partial-v0\n3 \u00d7 10\u22124\n0.005\nFalse\nkitchen-mixed-v0\n3 \u00d7 10\u22124\n0.005\nFalse\nF\nOPTIMAL RESULTS\nIf a small amount of online experience is provided during the evaluation stage for model selection,\nwe can pick the best models during training via online evaluations (similar to early stopping in\nsupervised learning). This regime provides a further boost in the performance of Diffusion-QL as\nshown in Table 4.\nG\nLIMITATIONS AND FUTURE WORK\nDiffusion policies are highly expressive and hence they can capture multi-modal distributions well.\nWe have shown this results in learning better policies in offlineRL. However, at the inference time,\nthe reverse sampling defined in Equation (1) requires iteratively computing \u03f5\u03b8 networks N times, and\nthis can become a bottleneck for the running time. In our case, we couple the learning of diffusion\npolicies with Q-learning, and achieve good performance with small N = 5. Diffusion policies\nwith N = 5 could be four to five times slower in action inference compared to previous one-step\nfeedforward policies, and hence the inference cost could prevent the approach from deployment in\nsome real-world scenarios, where fast response is necessary.\nThis motivates potential future works. There have been many recent works focusing on improving\nthe sampling speed of diffusion models (Lu et al., 2022; Wang et al., 2022; Xiao et al., 2021; Sal-\nimans & Ho, 2022; Song et al., 2020; Zheng et al., 2022), which could be applied to improve the\nsampling efficiency of Diffusion-QL. For example, the diffusion policy may be able to be distilled\ninto a simpler feedforward policy after training.\nH\nGAUSSIAN MIXTURE POLICY\nWe test a Gaussian Mixture policy as a classic policy class that can capture multi-modal distributions\nas an additional baseline. We modify TD3+BC (Fujimoto & Gu, 2021) by replacing the original\ndeterministic actor with a mixture density network (Bishop, 1994), where each mixture component\nis a Gaussian. Since a Gaussian mixture policy is applied, we replaced minimizing the L2 loss (from\nTD3+BC) between predicted actions and real actions, with maximizing the likelihood estimate of\nGaussian mixtures on real state-action pairs. We keep all the other parts the same as TD3+BC.\n15\nPublished as a conference paper at ICLR 2023\nTable 4: Performance comparison with online model selection and offline model selection.\nGym Tasks\nDiffusion-QL (Offline)\nDiffusion-QL (Online)\nhalfcheetah-medium-v2\n51.1 \u00b1 0.5\n51.5 \u00b1 0.3\nhopper-medium-v2\n90.5 \u00b1 4.6\n96.6 \u00b1 3.4\nwalker2d-medium-v2\n87.0 \u00b1 0.9\n87.3 \u00b1 0.5\nhalfcheetah-medium-replay-v2\n47.8 \u00b1 0.3\n48.3 \u00b1 0.2\nhopper-medium-replay-v2\n101.3 \u00b1 0.6\n102.0 \u00b1 0.4\nwalker2d-medium-replay-v2\n95.5 \u00b1 1.5\n98.0 \u00b1 0.5\nhalfcheetah-medium-expert-v2\n96.8 \u00b1 0.3\n97.2 \u00b1 0.4\nhopper-medium-expert-v2\n111.1 \u00b1 1.3\n112.3 \u00b1 0.8\nwalker2d-medium-expert-v2\n110.1 \u00b1 0.3\n111.2 \u00b1 0.9\nAverage\n88.0\n89.3\nAntMaze Tasks\nDiffusion-QL (Offline)\nDiffusion-QL (Online)\nantmaze-umaze-v0\n93.4 \u00b1 3.4\n96.0 \u00b1 3.3\nantmaze-umaze-diverse-v0\n66.2 \u00b1 8.6\n84.0 \u00b1 10.1\nantmaze-medium-play-v0\n76.6 \u00b1 10.8\n79.8 \u00b1 8.7\nantmaze-medium-diverse-v0\n78.6 \u00b1 10.3\n82.0 \u00b1 9.5\nantmaze-large-play-v0\n46.4 \u00b1 8.3\n49.0 \u00b1 9.4\nantmaze-large-diverse-v0\n56.6 \u00b1 7.6\n61.7 \u00b1 8.2\nAverage\n69.6\n75.4\nAdroit Tasks\nDiffusion-QL (Offline)\nDiffusion-QL (Online)\npen-human-v1\n72.8 \u00b1 9.6\n75.7 \u00b1 9.0\npen-cloned-v1\n57.3 \u00b1 11.9\n60.8 \u00b1 11.8\nAverage\n65.1\n68.3\nKitchen Tasks\nDiffusion-QL (Offline)\nDiffusion-QL (Online)\nkitchen-complete-v0\n84.0 \u00b1 7.4\n84.5 \u00b1 6.1\nkitchen-partial-v0\n60.5 \u00b1 6.9\n63.7 \u00b1 5.2\nkitchen-mixed-v0\n62.6 \u00b1 5.1\n66.6 \u00b1 3.3\nAverage\n69.0\n71.6\nTable 5: Performance comparison for Gaussian Mixture ablation study.\nGym Tasks\nTD3+BC\nTD3+BC-GM\nDiffusion-QL\nhalfcheetah-medium-expert-v2\n90.4\n49.0\n96.8\nhopper-medium-expert-v2\n98.0\n40.0\n111.1\nwalker2d-medium-expert-v2\n110.1\n66.5\n110.1\nWe evaluated the model (we call it TD3+BC-GM) on both our bandit toy experiment and selected\nD4RL tasks. As shown in Figure 5, with a properly selected number of mixtures, the Gaussian\nmixture could capture the multi-modal distribution in our behavior cloning experiment. However,\nin the Q-learning experiment, it fails to converge to the optimal target location, and always places\nsome density on the suboptimal modes, resulting in a suboptimal policy with multi-modes. For\nD4RL experiments, we set the number of mixtures to be 3, and seen in Table 5, we observed that\nTD3+BC-GM does not perform well on the three D4RL tasks, which is consistent with our previous\nobservation that TD3+BC-GM is prone to have suboptimal actions.\nOne reason for the poor performance could be that Gaussian mixture models are challenging to fit\nwell, particularly in higher dimensional spaces. We are also forced to choose the number of mixture\ncomponents, which for the D4RL experiments we don\u2019t know a priori how many components there\nare. Moreover, each mixture component is often parameterized with a diagonal Gaussian that has\nlimited ability in capturing the dependence between different dimensions.\n16\nPublished as a conference paper at ICLR 2023\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nGround Truth\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=2)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=3)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=4)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nAdd Reward\nr\nN(3.0, 0.5)\nr\nN(0.0, 0.5)\nr\nN(1.5, 0.5)\nr\nN(5.0, 0.5)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=2)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=3)\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\nx\n1.00\n0.75\n0.50\n0.25\n0.00\n0.25\n0.50\n0.75\n1.00\ny\nTD3+BC-GM (K=4)\nFigure 5: Gaussian Mixture ablation study. The first row shows the behavior cloning experiment and the second\nrow shows the Q-learning experiment, which are the same set of experiments described in Section 4. Here, K\nis the number of Gaussian mixtures.\n17\n",
    "2401.03629": "DDM-Lag : A Diffusion-based Decision-making Model for\nAutonomous Vehicles with Lagrangian Safety Enhancement\nJiaqi Liu, Student Member, IEEE, Peng Hang, Member, IEEE, Xiaocong Zhao, Jianqiang Wang, and Jian Sun\nAbstract\u2014Decision-making stands as a pivotal component in\nthe realm of autonomous vehicles (AVs), playing a crucial role\nin navigating the intricacies of autonomous driving. Amidst\nthe evolving landscape of data-driven methodologies, enhancing\ndecision-making performance in complex scenarios has emerged\nas a prominent research focus. Despite considerable advance-\nments, current learning-based decision-making approaches ex-\nhibit potential for refinement, particularly in aspects of policy\narticulation and safety assurance. To address these challenges,\nwe introduce DDM-Lag, a Diffusion Decision Model, augmented\nwith Lagrangian-based safety enhancements. This work con-\nceptualizes the sequential decision-making challenge inherent in\nautonomous driving as a problem of generative modeling, adopt-\ning diffusion models as the medium for assimilating patterns of\ndecision-making. We introduce a hybrid policy update strategy\nfor diffusion models, amalgamating the principles of behavior\ncloning and Q-learning, alongside the formulation of an Actor-\nCritic architecture for the facilitation of updates. To augment the\nmodel\u2019s exploration process with a layer of safety, we incorporate\nadditional safety constraints, employing a sophisticated policy\noptimization technique predicated on Lagrangian relaxation to\nrefine the policy learning endeavor comprehensively. Empirical\nevaluation of our proposed decision-making methodology was\nconducted across a spectrum of driving tasks, distinguished by\ntheir varying degrees of complexity and environmental contexts.\nThe comparative analysis with established baseline methodologies\nelucidates our model\u2019s superior performance, particularly in\ndimensions of safety and holistic efficacy.\nImpact Statement\u2014In an era where autonomous vehicles (AVs)\nsymbolize the cutting edge of transportation innovation, ensuring\nthe safety and reliability of their decision-making systems re-\nmains a pivotal challenge. Existing artificial intelligence methods,\nsuch as reinforcement learning, still lack sufficient progress in\ntheir large-scale safe application on autonomous vehicles, with\nkey issues including the safety of data-driven approaches and the\nadaptability of their strategies. Our research introduces DDM-\nLag, an advanced diffusion-based decision-making model with\nintegrated Lagrangian safety enhancements, specifically designed\nfor AVs. This method not only elevates the state of AV decision-\nmaking through the use of generative modeling and sophisticated\noptimization techniques but also significantly enhances safety and\nadaptability in dynamic environments. DDM-Lag contributes to\nelevating the intelligence level of decision-making in autonomous\nvehicles and provides a blueprint for applying similar methodolo-\ngies in other domains requiring reliable decision-making under\nThis work was supported in part by the National Natural Science Foun-\ndation of China (52125208, 52232015), the Belt and Road Cooperation\nProgram under the 2023 Shanghai Action Plan for Science, Technology and\nInnovation (23210750500), the Shanghai Scientific Innovation Foundation\n(23DZ1203400) and the Fundamental Research Funds for the Central Uni-\nversities.\nJiaqi Liu, Peng Hang, Xiaocong Zhao, and Jian Sun are with the Department\nof Traffic Engineering and Key Laboratory of Road and Traffic Engineering,\nMinistry of Education, Tongji University, Shanghai 201804, China. (e-mail:\n{liujiaqi13, hangpeng, zhaoxc, sunjian}@tongji.edu.cn)\nJianqiang Wang is with State Key Laboratory of Automotive Safety and\nEnergy School of Vehicle and Mobility, Tsinghua University, Beijing 100084,\nChina.(e-mail: wjqlws@tsinghua.edu.cn)\nCorresponding author: Jian Sun\nuncertainty. This study underscores the potential of merging\nadvanced computational models with safety-centric optimizations\nto enhance the safe operation of intelligent systems.\nIndex Terms\u2014Autonomous Vehicle; Diffusion model; La-\ngrangian method; Decision-making\nI. INTRODUCTION\nThe advent of autonomous driving technology is poised to\ntransform the global transportation system fundamentally [1],\n[2]. Decision-making, a pivotal element within autonomous\ndriving systems, plays an essential role in ensuring the safety,\nstability, and efficiency of autonomous vehicles (AVs) [3].\nDespite significant advancements, the decision-making capa-\nbilities of AVs in complex scenarios are yet to reach their\nfull potential. Concurrently, the evolution of deep learning\nhas propelled learning-based methodologies to the forefront,\nwith reinforcement learning (RL) being a particularly notable\nmethod. RL approaches sequential decision-making by opti-\nmizing the cumulative rewards of a trained agent. This method\nhas demonstrated superior performance over human capabili-\nties in various complex decision scenarios [4]. However, RL\nstill faces challenges in decision safety, sampling efficiency,\npolicy articulation, and training stability [5]\u2013[7].\nIn a different vein, diffusion models, a class of advanced\ngenerative models, have recently achieved remarkable success,\nespecially in image generation [8]. Functioning as probabilistic\nmodels, they incorporate noise into data in a forward pro-\ncess and then iteratively remove the noise to recover the\noriginal data, following a Markov chain framework. Com-\npared to other generative models, diffusion models excel in\nsampling efficiency, data fidelity, training consistency, and\ncontrollability [8], [9]. Moreover, these models have been\nvalidated as effective tools for enhancing decision-making in\nreinforcement learning strategies [10]. While diffusion models\nhave seen applications in game strategy and robotic motion\nplanning [11], [12], their potential in autonomous driving\ndecision-making is still largely untapped. This exploration\nis imperative, especially considering the black-box nature\nof neural networks, which complicates the direct assurance\nof decision-making safety\u2014a critical requirement for highly\nsafety-conscious autonomous vehicles.\nTo address these challenges, we introduce DDM-Lag, a\nnovel Diffusion Decision Model augmented with Lagrangian-\nbased safety enhancements, specifically tailored for improving\ndecision-making in autonomous driving.\nWe commence by delineating the sequential decision-\nmaking quandary in autonomous driving as a Constrained\nMarkov Decision Process (CMDP), adopting a perspective\nrooted in generative modeling. We integrate diffusion mod-\nels for resolving the generative decision-making conundrum,\narXiv:2401.03629v2  [cs.RO]  5 Apr 2024\nwhere models ingest environmental perception information\nas input and output vehicle control variables, learning and\nupdating through a generative autoregressive methodology.\nFor the stable and efficient updating of the diffusion model,\nwe propose a hybrid policy update method that amalgamates\nbehavior cloning and Q-learning within a diffusion model\nframework, and correspondingly design an Actor-Critic archi-\ntecture to facilitate this updating process. Within this update\nframework, our diffusion model\u2019s updating objectives include\ntwo components: 1) a behavior cloning term, encouraging\nthe diffusion model to sample behaviors from a distribution\nanalogous to the training set; and 2) a policy improvement\nterm, aimed at sampling higher-value actions. Concurrently,\nadditional safety constraints are integrated within the model\u2019s\nexploration constraints to ensure the safety of action explo-\nration, employing a policy optimization methodology predi-\ncated on Lagrangian relaxation to refine the entire policy learn-\ning trajectory comprehensively. The model update phase incor-\nporates a Proportional-Integral-Derivative (PID) controller for\nthe adjustment of \u03bb, ensuring a stable update trajectory. Fig.1\ndelineates the entire workflow of our initiative, including the\ndevelopment of an offline expert data collection module. This\nmodule is pivotal for training multiple reinforcement learning\nagents and amassing data from diverse scenarios.\nFinally, the efficacy of the DDM-Lag approach undergoes\nevaluation in various driving tasks, each differing in com-\nplexity and environmental context. When juxtaposed with\nestablished baseline methods, our model exhibits superior\nperformance, particularly in aspects of decision-making safety\nand comprehensive performance.\nOur contributions can be summarized as follows:\n\u2022 We model the autonomous driving decision-making pro-\ncess as a generative diffusion process, proposing a hybrid\nPolicy update method that integrates behavior cloning\nwith Q-learning, facilitated by an Actor-Critic framework\nfor policy updates.\n\u2022 A Lagrangian relaxation-based policy optimization ap-\nproach is adopted, enhancing the safety of the decision-\nmaking process.\n\u2022 The proposed method is subjected to testing in a variety\nof driving scenarios, demonstrating advantages in safety\nand comprehensive performance.\nThe rest of the paper is organized as follows. Section II\nsummarizes the recent related works. The preliminaries and the\nproblem formulation are described in section III. In section IV,\nthe framework we proposed is described. The simulation\nenvironment and comprehensive experiments are introduced\nand the results are analyzed in section V. Finally, this paper\nis concluded in section VI.\nII. RELATED WORKS\nA. Decision-Making of Autonomous Vehicles\nThe decision-making process is a cornerstone in the\nfunctionality of autonomous vehicles (AVs). Traditional ap-\nproaches in this realm encompass rule-based, game theory-\nbased, and learning-based methodologies. Significantly, with\nEnvironment\nEvaluation\nData Collection\nTraining\nExpert Data Buffer\n\u2026\nRL Expert\nAutonomous Vehicle\nDiffusion Actor \nCritic \nEntropy \nTD Error \nSafety Enhancement\nFig. 1: The overall procedure of our work.\nthe advent and rapid progression of deep learning and arti-\nficial intelligence, learning-based approaches have garnered\nincreasing interest. These methods, particularly reinforcement\nlearning (RL) [13], [14] and large language models (LLM)\n[15], [16], exhibit formidable and extraordinary learning ca-\npacities. They adeptly handle intricate and dynamic environ-\nments where conventional rule-based decision-making systems\nmay falter or respond sluggishly [17].\nSaxena et al. [13] introduced a pioneering model-free RL\nstrategy, enabling the derivation of a continuous control policy\nacross the AVs\u2019 action spectrum, thus significantly enhancing\nsafety in dense traffic situations. Liu et al. [18] developed a\ntransformer-based model addressing the multi-task decision-\nmaking challenges at unregulated intersections. Concurrently,\nLLMs have also emerged as a focal point in AV decision-\nmaking. The Dilu framework [15], integrating a Reasoning\nand Reflection module, facilitates decision-making grounded\nin common-sense knowledge and allows for continuous system\nevolution.\nB. Reinforcement Learning in Safe Decision-Making\nIn contemporary research, reinforcement learning is recog-\nnized as an efficacious learning-based method for sequential\ndecision-making [19]. By optimizing the cumulative reward,\nRL identifies the optimal action strategy for agents [4], finding\napplications in domains such as robotic control, gaming, and\nautonomous vehicles [20]. However, the \u2019black box\u2019 nature\nof RL makes the safety of its policy outputs challenging to\nguarantee [6].\nTo augment the safety aspect in RL decision-making,\nvarious methodologies have been proposed to incorporate\nsafety layers or regulate the agents\u2019 exploratory processes\nduring training [6], [21]. Safe RL is often conceptualized\nas a Constrained Markov Decision Process (CMDP) [22],\nincorporating constraints to mitigate unsafe exploration by the\nagent. Borkar [23] proposed an actor-critic RL approach for\nCMDP, employing the envelope theorem from mathematical\neconomics and analyzing primal-dual optimization through a\nthree-time scale process. Berkenkamp et al. [24] developed\na safe model-based RL algorithm using Lyapunov functions,\nensuring stability under the assumption of a Gaussian process\nprior.\nC. Applications of the Diffusion Model\nThe diffusion model has emerged as a potent generative\ndeep-learning tool, utilizing a denoising framework for data\ngeneration, with notable success in image generation and\ndata synthesis [8]. Recent studies have applied the diffusion\nmodel to sequential decision-making challenges, functioning\nas a planner [25], [26], policy network [10], [12], and data\nsynthesizer [11], [12].\nDiffuser [25] employs the diffusion model for trajectory\ngeneration, leveraging offline dataset learning and guided\nsampling for future trajectory planning. Wang et al. [10]\ndemonstrated the superior performance of a diffusion-model-\nbased policy over traditional Gaussian policies in Q-learning,\nparticularly for offline reinforcement learning. Additionally, to\nenhance dataset robustness, Lu et al. [11] utilized the diffusion\nmodel for data synthesis, learning from both offline and online\ndatasets.\nIn our study, we posit the diffusion model as a central actor\nin the decision-making process of autonomous agents, aiming\nto augment the flexibility and diversity in AV decision-making\nstrategies.\nIII. PRELIMINARIES\nA. Constrained Markov Decision Process (CMDP)\nA CMDP is characterized by the tuple (S, A, R, C, \u03b3, \u00b5),\nwhere S represents the state space, A denotes the action space,\nR : S \u00d7 A \u2192R is the reward function, and C : S \u00d7 A \u2192\nR signifies the corresponding single-stage cost function. The\ndiscount factor is denoted by \u03b3, and \u00b5 indicates the initial state\ndistribution. We define a policy \u03c0 as a map to from states to\na probability distribution over actions, and \u03c0(a|s) represents\nthe probability of action a based on state s.\nOur study focuses on a class of stationary policies \u03c0\u03b8,\nparameterized by \u03b8. The objective function is framed in\nterms of the infinite horizon discounted reward criterion:\nE [P\u221e\nt=0 \u03b3tR(st, at) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t]. Similarly, the con-\nstraint function is expressed via the infinite horizon dis-\ncounted cost: E [P\u221e\nt=0 \u03b3tC(st, at) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t]. The\noptimization problem with constraints can be formulated as\nfollows:\nmax\n\u03b8\nE\n\" \u221e\nX\nt=0\n\u03b3tR(st, at) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t\n#\n.\ns.t. E\n\" \u221e\nX\nt=0\n\u03b3tC(st, at) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t\n#\n\u2264d.\n(1)\nB. Lagrangian Methods\nIn the context of the constrained optimization problem as\ndelineated in equation 1, the associated Lagrangian can be\narticulated as:\nL(\u03b8, \u03bb) = JR(\u03c0\u03b8) \u2212\u03bb(JC(\u03c0\u03b8) \u2212d)\n(2)\nwhere \u03bb \u2208R+ denotes the Lagrange multiplier, JR(\u03c0\u03b8) =\nE [P\u221e\nt=0 \u03b3tR(st, at, st+1) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t], JC(\u03c0\u03b8)\n=\nE [P\u221e\nt=0 \u03b3tC(st, at, st+1) | s0 \u223c\u00b5, at \u223c\u03c0\u03b8, \u2200t]. The objec-\ntive is to identify a tuple (\u03b8\u2217, \u03bb\u2217) that represents both the\npolicy and the Lagrange parameter, fulfilling the condition:\nL(\u03b8\u2217, \u03bb\u2217) = max\n\u03b8\nmin\n\u03bb L(\u03b8, \u03bb).\n(3)\nResolving the max-min problem is tantamount to locating a\nglobal optimal saddle point (\u03b8\u2217, \u03bb\u2217) such that for all (\u03b8, \u03bb),\nthe following inequality is maintained:\nL(\u03b8\u2217, \u03bb) \u2265L(\u03b8\u2217, \u03bb\u2217) \u2265L(\u03b8, \u03bb\u2217).\n(4)\nGiven that \u03b8 is associated with the parameters of a Deep\nNeural Network, identifying such a globally optimal saddle\npoint is computationally challenging. Thus, our objective shifts\nto finding a locally optimal saddle point, satisfying equation 4\nwithin a defined local neighbourhood H\u03f51,\u03f52:\nH\u03f51,\u03f52\n\u25b3= {(\u03b8, \u03bb)| \u2225\u03b8 \u2212\u03b8\u2217\u2225\u2264\u03f51, \u2225\u03bb \u2212\u03bb\u2217\u2225\u2264\u03f52}\n(5)\nfor some \u03f51, \u03f52 > 0. Assuming L(\u03b8, \u03bb) is determinable for\nevery (\u03b8, \u03bb) tuple, the gradient search algorithm will be used\nas to identify a local (\u03b8\u2217, \u03bb\u2217) pair [27]:\n\u03b8n+1\n=\n\u03b8n \u2212\u03b71(n)\u2207\u03b8n(\u2212L(\u03b8n, \u03bbn))\n(6)\n=\n\u03b8n + \u03b71(n)[\u2207\u03b8nJR(\u03c0\u03b8) \u2212\u03bbn\u2207\u03b8nJC(\u03c0\u03b8)] (7)\n\u03bbn+1\n=\n[\u03bbn + \u03b72(n)\u2207\u03bbn(\u2212L(\u03b8n, \u03bbn))]+\n(8)\n=\n[\u03bbn \u2212\u03b72(n)(JC(\u03c0\u03b8) \u2212d)]+.\n(9)\nHere, [x]+ represents max(0, x), ensuring the Lagrange\nmultiplier remains non-negative post-update. In equation 7-\nequation 9, \u03b71(n), \u03b72(n) > 0 \u2200n are the predefined step-\nsize schedules, adhering to standard step-size conditions. For\ni = 1, 2, the conditions\nX\nk\n\u03b7i(n) = \u221e,\nX\nk\n\u03b72\ni (n) < \u221eare\nsatisfied.\nC. Problem Formulation\nThe decision-making conundrum is conceptualized within\nthe framework of a Constrained Markov Decision Process\n(CMDP). Commencing from an initial state s0, the AV iterates\nthrough transitions from one state st \u2208S to a subsequent state\nst+1 \u2208S at each timestep t = 0, 1, . . . , T. This progression\ninvolves the execution of an action at \u2208A, consequent to\nwhich a reward rt \u2208R is accrued within the environmental\ncontext.\n1) State Space: In our study, the state input S of the AV\ncomprises four main components. The first component is the\nego vehicle\u2019s own state, including its position [xego, yego],\nvelocity [vxego, vyego], steering angle [heading], and distance\nfrom the road boundary [disbound]. The second component is\nnavigation information, for which we calculate a route from\nthe origin to the destination and generate a set of checkpoints\nalong the route at predetermined intervals, providing the rela-\ntive distance and direction to the next checkpoint as navigation\ndata. The third component consists of a 240-dimensional\nvector, characterizing the vehicle\u2019s surrounding environment\nin a manner akin to LiDAR point clouds. The LiDAR sensor\nscans the environment in a 360-degree horizontal field of view\nusing 240 lasers, with a maximum detection radius of 50\nForward Process\nAction \nProbability\n\ud835\udf0b\ud835\udf0b\ud835\udf03\ud835\udf03(\ud835\udc4e\ud835\udc4e|\ud835\udc60\ud835\udc60)\nSoftmax\n\ud835\udcb6\ud835\udcb60\n\ud835\udcb6\ud835\udcb61\n\ud835\udcb6\ud835\udcb6\ud835\udc56\ud835\udc56\u22121\n\ud835\udcb6\ud835\udcb6\ud835\udc56\ud835\udc56\n\ud835\udcb6\ud835\udcb6\ud835\udc41\ud835\udc41\nMLP\nMLP\nMLP\nMLP\nConditions\n( \ud835\udcae\ud835\udcae\ud835\udc61\ud835\udc61, 1, \ud835\udcb6\ud835\udcb6\ud835\udc61\ud835\udc61\n1)\n( \ud835\udcae\ud835\udcae\ud835\udc61\ud835\udc61, \ud835\udc56\ud835\udc56\u22121, \ud835\udcb6\ud835\udcb6\ud835\udc61\ud835\udc61\n\ud835\udc56\ud835\udc56\u22121)\n( \ud835\udcae\ud835\udcae\ud835\udc61\ud835\udc61, \ud835\udc56\ud835\udc56, \ud835\udcb6\ud835\udcb6\ud835\udc61\ud835\udc61\n\ud835\udc56\ud835\udc56)\n( \ud835\udcae\ud835\udcae\ud835\udc61\ud835\udc61, \ud835\udc41\ud835\udc41, \ud835\udcb6\ud835\udcb6\ud835\udc61\ud835\udc61\nN)\nGaussian \nNoise\nOptimal \nAction\nReverse Process\nEnvironment\nFig. 2: The conditioned diffusion process for our method.\nmeters and a horizontal resolution of 1.5 degrees. The final\ncomponent includes the state of surrounding vehicles, such as\ntheir position and velocity information, acquired through V2X\ncommunication.\n2) Action Space: We utilize two normalized actions to\ncontrol the lateral and longitudinal motion of the target vehicle,\ndenoted as A = [a1, a2]T \u2208(0, 1). These normalized actions\nare subsequently translated into low-level continuous control\ncommands: steering us, acceleration ua, and brake signal ub\nas follows:\nus = Smax \u00b7 a1\nua = Fmax \u00b7 max{0, a2}\nub = \u2212Bmax \u00b7 min{0, a2}\n(10)\nwhere Smax represents the maximum steering angle, Fmax the\nmaximum engine force, and Bmax the maximum braking force.\n3) Reward Function: The reward function plays a crucial\nrole in optimizing the agent\u2019s performance. In our study, we\ndefine the reward function as:\nR = \u03c91rdis + \u03c92rv + \u03c93rs\n(11)\nThe components include rdis for the reward based on distance\ncovered, rv for the speed reward, and rs for the terminal\nreward.\nIV. METHODOLOGY\nIn this section, we provide a comprehensive exposition of\nthe design of our DDM-Lag model. Initially, we develop\na diffusion-based optimizer designed to generate solutions\nfor continuous vehicle control decisions. Subsequently, we\nintroduce a hybrid policy update method for diffusion models,\nintegrating behavior cloning with Q-learning, and devise an\nActor-Critic architecture to guide the update of the diffusion\nmodel. Finally, to augment the model\u2019s safety performance,\nwe incorporate a safety loss as an optimization constraint,\nemploying the Lagrangian relaxation method to address the\nconstrained optimization problem.\nA. Diffusion Policy\nAt any given moment, an AV makes decisions a based\non the current input environmental state s, with the AV\u2019s\npolicy denoted as \u03c0\u03b8(a|s). We model this policy using the\nreverse process of a conditional diffusion model. According\nto the diffusion model, an optimal decision solution under\nthe current environment can progressively increase in noise\nuntil it conforms to a Gaussian distribution, a process termed\nas the forward process of probability noising. Subsequently,\nduring the reverse process of probability inference, the optimal\ndecision generation network, denoted as \u03c0\u03b8(\u00b7), functions as a\ndenoiser that initiates with Gaussian noise and reconstructs\nthe optimal decision solution, denoted as a0, based on the\nenvironmental condition, s. An illustration of this diffusion\nprocess is depicted in Fig.2. In the following subsections, we\nfirst elucidate the forward process and then employ the reverse\nprocess of Diffusion to model the policy.\nNotation:This paper differentiates between two categories\nof timesteps: diffusion timesteps, indicated by superscripts\ni \u2208{1, . . . , N}, and trajectory timesteps, denoted by sub-\nscripts t \u2208{1, . . . , T}. Within this framework, the terms\nautonomous vehicles and agents are used interchangeably\nwithout distinction.\n1) The Forward Process of Probability Noising: The deci-\nsion output a0 = \u03c0\u03b8(a|s) \u223cR|A| represents the likelihood\nof selecting each decision under the environmental state s.\nWe denote the distribution\u2019s vector at step i in the forward\nprocess as ai, maintaining the same dimensionality as a0.\nTo evolve the initial probability distribution a0 towards in-\ncreased uncertainty, we sequentially introduce Gaussian noise\nat each step, yielding a1, a2, . . . , aN. The progression from\nai\u22121 to ai follows a Gaussian distribution with a mean of\n\u221a1 \u2212\u03b2iai\u22121 and a variance of \u03b2iI, as described by:\nq(ai|ai\u22121) = N(ai;\np\n1 \u2212\u03b2iai\u22121, \u03b2iI),\n(12)\nwhere i = 1, . . . , N, and \u03b2i is calculated based on a variance\nscheduling method, controlling the noise level throughout the\nActor\nReverse Diffusion Chain\nOptimal Action\nGaussian Noise\nDiffusion Network\n\u2026\n\u2026\n\ud835\udc43\ud835\udc43\ud835\udc47\ud835\udc47\n\ud835\udc43\ud835\udc43\ud835\udc61\ud835\udc61\n\ud835\udc43\ud835\udc430\nExperience Data Buffer\nEnvironment\nProbability\nSampling\nAction\nExecute Action\nSoft Update\nOptimizer\nDiffusion Network\nCritic Network\nDiffusion \nNetwork\nUpdate\nCritic Loss\nTarget Actor\nTarget Critic\nCritic\nActor\n\ud835\udc44\ud835\udc44\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\ud835\udc61\n\ud835\udc44\ud835\udc44\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\ud835\udc52\nTrajectory Collection\nReward\ud835\udc5f\ud835\udc5f\ud835\udc61\ud835\udc61\nObservation \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61+1\nObservation \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61\n(\ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61, \ud835\udc4e\ud835\udc4e\ud835\udc59\ud835\udc59, \ud835\udc60\ud835\udc60\ud835\udc61\ud835\udc61+1, \ud835\udc5f\ud835\udc5f\ud835\udc59\ud835\udc59)\nData Batch\nCritic \nNetwork\nOnline Inferring\nOffline Training\nSafety Constraints\nFig. 3: The framework of DDM-Lag algorithm.\nforward process:\n\u03b2i = 1 \u2212e\u2212\u03b2min\nN\n\u22122i\u22121\n2N2 (\u03b2max\u2212\u03b2min)\n(13)\nGiven that ai is solely dependent on ai\u22121, the forward\nprocess exhibits Markovian properties. The distribution of aN,\nstarting from a0, is the cumulative result of all transitions\nq(ai|ai\u22121):\nq(aN|a0) =\nT\nY\nt=1\nq(ai|ai\u22121).\n(14)\nAlthough not implemented practically, the forward process\nprovides a theoretical basis to understand the connection\nbetween a0 and any ai:\nai = \u221a\u00af\u03b1ia0 +\n\u221a\n1 \u2212\u00af\u03b1i\u03f5,\n(15)\nwhere \u03b1i = 1 \u2212\u03b2i, \u00af\u03b1i = Qt\nk=1 \u03b1k signifies the cumulative\nproduct up to step t, and \u03f5 \u223cN(0, I) represents standard\nGaussian noise. As t advances, aN becomes entirely noise,\nadhering to the distribution N(0, I).\n2) The Reverse Process of Probability Inference: The re-\nverse process, also called the sampling process, aims to infer\nthe target a0 from a noise sample aN \u223cN(0, I) by removing\nnoise from it. In our method, the purpose is to infer the\noptimal decision action from the noise sample. We model\nthe diffusion policy using the reverse process of conditional\ndiffusion models:\n\u03c0\u03b8(a | s) = p\u03b8(a0:N | s) = N(aN; 0, I) QN\ni=1 p\u03b8(ai\u22121 | ai, s)\n(16)\nwhere a0, the end sample of the reverse chain, represents the\naction executed and evaluated. The conditional distribution\np\u03b8(ai\u22121 | ai, s) can be modeled as a Gaussian distribution\nN(ai\u22121; \u00b5\u03b8(ai, s, i), \u03a3\u03b8(ai, s, i)). Following [28], we param-\neterize p\u03b8(ai\u22121 | ai, s) as a noise prediction model, fixing the\ncovariance matrix as \u03a3\u03b8(ai, s, i) = \u03b2iI, and constructing the\nmean as:\n\u00b5\u03b8(ai, s, i) =\n1\n\u221a\u03b1i\n\u0000ai \u2212\n\u03b2i\n\u221a1 \u2212\u00af\u03b1i\n\u03f5\u03b8(ai, s, i)\n\u0001\n(17)\nThe reverse diffusion chain, parameterized by \u03b8, is sampled\nas:\nai\u22121 | ai =\nai\n\u221a\u03b1i \u2212\n\u03b2i\n\u221a\n\u03b1i(1\u2212\u00af\u03b1i)\u03f5\u03b8(ai, s, i) + \u221a\u03b2i\u03f5,\n\u03f5 \u223cN(0, I),\nfor i = N, . . . , 1.\n(18)\nWhen i = 1, \u03f5 is set to 0 to enhance sampling quality. We\nadopt the simplified objective proposed by [28] to train our\nconditional \u03f5-model via:\nLd(\u03b8) = Ei\u223cU,\u03f5\u223cN (0,I),(s,a)\u223cD\n\u0002\n||\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1ia +\n\u221a\n1 \u2212\u00af\u03b1i\u03f5, s, i)||2\u0003\n(19)\nwhere U is a uniform distribution over the discrete set\n{1, . . . , N} and D denotes the offline dataset.\nB. Reinforcement-Guided Diffusion Policy Learning\nThe policy regularization loss in equation 19, Ld(\u03b8), that\nwe employ is a behavior cloning term capable of effectively\nlearning the behavioral patterns from expert data. However,\nthis still falls short of bridging the gap to the complex environ-\nments and decision-making behaviors encountered in the real\nworld, making it challenging for the model to learn strategies\nthat surpass those in the training data. To improve the policy,\nwe introduce guidance from the Q-value function into the\nbackward diffusion chain during the training phase, facilitating\nthe learning of actions with higher values through priority\nsampling. For this purpose, we have designed an Actor-Critic\nframework to guide the diffusion model in parameter updates.\nAs depicted in Fig.3, this framework utilizes two neural\nnetworks: the policy network, \u03c0\u03b8, for decision-making, and\nthe Q network, Q\u03d5, for policy evaluation. The diffusion model\nis defined as the actor, with its policy network denoted as \u03c0\u03b8,\nand we employ a Multilayer Perceptron (MLP) as the Critic.\nDuring training, the Actor-Critic framework alternates be-\ntween policy evaluation and policy improvement in each\niteration, dynamically updating the Critic Q\u03d5 and the Actor\n\u03c0\u03b8. In the policy evaluation phase, we update the estimated Q-\nfunction by minimizing the L2 norm of the entropy-regularized\nTemporal Difference (TD) error:\ny(rt, st+1) = rt + \u03b3Eat+1\u223c\u03c0\u03b8(\u00b7|st+1) [Q\u03d5\u2032(st+1, at+1)]\n\u2212\u03b1 log \u03c0\u03b8(at+1|st+1)] (20)\nLQ(\u03d5) = 1\n2E(st,at,rt,st+1)\u223cD[y(rt, st+1)\u2212Q\u03d5(st, at)]2. (21)\nwhere \u03d5\u2032 is the parameter of the target network Q\u03d5\u2032, and \u03b1 is\nthe temperature parameter.\nIn the policy improvement phase, for the diffusion-based\nactor \u03c0\u03b8, our update target function comprises two parts: the\npolicy regularization loss term Ld(\u03b8) and the policy improve-\nment objective term Lq(\u03b8). The policy regularization loss term\nLd(\u03b8), equivalent to behavior cloning loss, is utilized for\nlearning expert prior knowledge from human expert demon-\nstration data. However, as it is challenging to surpass expert\nperformance solely with this, we introduce a Q-function-based\npolicy improvement objective term Lq(\u03b8) during training,\nguiding the diffusion model to prioritize sampling high-value\nactions. Consequently, our policy learning objective function\nis expressed as:\n\u03c0 = arg min\n\u03c0\u03b8\nL(\u03b8) = Ld(\u03b8) + Lq(\u03b8)\n= Ld(\u03b8) \u2212\u03b1 \u00b7 Es\u223cD,a0\u223c\u03c0\u03b8\n\u0002\nQ\u03d5(s, a0)\n\u0003\n.\n(22)\na0 is reparameterized by equation 18, allowing the Q-value\nfunction gradient with respect to the action to propagate\nbackward through the entire diffusion chain.\nC. Safety Enhancement with Constrained Optimization\nDue to the inherent randomness in the Actor\u2019s exploratory\nprocess, ensuring the safety of its actions is a significant chal-\nlenge, particularly in autonomous vehicle decision-making. To\nenhance safety in the learning and policy updating processes,\nwe incorporate safety constraints into the policy update pro-\ncess, treating the original problem as a constrained optimiza-\ntion issue. The safety cost function is defined as follows:\nC = \u03c9\u2032\n1c1 + \u03c9\u2032\n2c2 + \u03c9\u2032\n3c3\n(23)\nwhere c1, c2 and c3 are the penalty for the condition: out of\nroad, crashing with other vehicles, crashing with other objects,\nrespectively.\nAlgorithm 1: Reinforcement Guided Diffusion Deci-\nsion Model with Safe Enhancement\nInputs : Expert Dataset D\nOutputs: Updated policy and critics: \u03b8\u2032,\u03d5\u2032,\u03c8\u2032\n1 Initialize policy network \u03c0\u03b8; critic networks Q\u03d5;cost\ncritic networks QC\n\u03c8; and their target networks \u03c0\u03b8\u2032,\nQ\u03d5\u2032, QC\n\u03c8\u2032; Lagrange parameter \u03bb0 \u22650; Integral\nI \u21900;\n2 Choose tuning parameters: KP , KI, KD \u22650; Previous\nCost: JC,prev \u21900;\n3 for Epoch = 1 to M do\n4\nSample transition mini-batch\nB = {(st, at, rt, st+1)} \u223cD;\n5\nInitialize a random normal distribution\naN \u223cN(0, I);\n6\nfor the denoising step t = T to 1 do\n7\nInfer and scale a denoising distribution using a\ndeep neural network;\n8\nCalculate the mean \u00b5\u03b8 of the reverse transition\ndistribution p\u03b8\n\u0000ai\u22121|ai\n\u0001\nby equation 17;\n9\nCalculate the distribution ai\u22121 by equation 18 ;\n10\nend\n11\nSample the action a0\nt;\n12\nUpdate lagrange multiplier \u03bb:\n13\nReceive cost JC = E\u03c4[P\nt=0 \u02c6ct];\n14\n\u2206\u2190JC \u2212d;\n15\n\u2202\u2190(JC \u2212JC,prev)+;\n16\nI \u2190(I + \u2206)+;\n17\n\u03bb \u2190(KP \u2206+ KII + KD\u2202)+;\n18\nJC,prev \u2190JC;\n19\nPolicy learning:\n20\nUpdate diffusion policy parameter \u03b8:\n21\n\u03b8n+1 =\n\u03b8n + \u03b71(n)\n\u0002\n\u2207\u03b8n[Ld(\u03b8) + Lq(\u03b8)] \u2212\u03bbn\u2207\u03b8nL\u03bb\nq (\u03b8)\n\u0003\n;\n22\nUpdate Q\u03d5 and Q\u03c8 by equation 21;\n23\nUpdate target networks:\n24\n\u03b8\u2032 \u2190\u03c1\u03b8\u2032 + (1 \u2212\u03c1)\u03b8, \u03d5\u2032 \u2190\u03c1\u03d5\u2032 + (1 \u2212\u03c1)\u03d5,\n25\n\u03c8\u2032 \u2190\u03c1\u03c8\u2032 + (1 \u2212\u03c1)\u03c8,\n26 end\nThen we apply the Lagrangian method to this optimization\nprocess. The entire optimization problem becomes:\n\u03b8\u2217= arg max\n\u03b8\nmin\n\u03bb\u22650 E{(\nX\nt=0\n\u03b3tRt)\u2212\u03bb[(\nX\nt=0\n\u03b3t \u02c6Ct)\u2212C]} (24)\nHere, \u03b8 and \u03bb are updated through policy gradient ascent and\nstochastic gradient descent (SGD) [29]. We then introduce a\nsafety-assessing Critic QC\n\u03c8 to estimate the cumulative safety\nconstraint value P\nt\u2032=t \u03b3(t\u2212t\u2032)Ct. With the reward replaced by\nthe safety constrain, the safety-critic network can be optimized\nby equation 21. For the actor \u03c0\u03b8, the safety constraint violation\nminimization objective can be written as:\nL\u03bb\nq (\u03b8) = Est\u223cD,at\u223c\u03c0\u03b8(\u00b7|st)[QC\n\u03c8(st, at) \u2212C]\n(25)\nNow, by combining the original policy improvement objective\nequation 22 and the safety constraint minimization optimiza-\n(a)\n(b)\n(c)\nFig. 4: The three kinds of mixed scenario used for traning and testing our method, (a) scenario 1, (b) scenario 2, (c) mixed long-distance\nscenario 3.\ntion objective equation 25, we derive our final policy update\nobjective:\n\u03c0 = arg min\n\u03c0\u03b8\nL(\u03b8) = Ld(\u03b8) + Lq(\u03b8) \u2212\u03bbL\u03bb\nq (\u03b8)\n(26)\nHowever, directly optimizing the Lagrangian dual during\npolicy updates can lead to oscillation and overshooting, thus\naffecting the stability of policy updates. From a control theory\nperspective, the multiplier update represents an integral con-\ntrol. Therefore, following [30], we introduce proportional and\nderivative control to update the Lagrangian multiplier, which\ncan reduce oscillation and lower the degree of action violation.\nSpecifically, we use a PID controller to update \u03bb, as follows:\n\u03bb \u2190Kp\u03b4 + Ki\nZ k\ni=1\n\u03b4di + Kd\n\u03b4\ndi,\n\u03b4 = E\u03c4[\nX\nt=0\n\u02c6ct] \u2212C\n(27)\nwhere we denote the training iteration as i, and Kp, Ki, Kd are\nthe hyper-parameters. Optimizing \u03bb with equation 24 reduces\nto the proportional term in\nequation 27, while the integral\nand derivative terms compensate for the accumulated error and\novershoot in the intervention occurrence. The whole training\nprocedure of DDM-Lag is summarized in Algorithm 1.\nV. EXPERIMENT AND EVALUATION\nIn this section, the detailed information of the simulation\nenvironment and our models will be introduced. Sequently,\nthe experiments results are analyzed.\nA. Experiments and Baselines\nEnvironments. We conducted our experiments, including\ndata collection, model training, and testing, in the MetaDrive\nSimulator [31], which is based on OpenAI Gym Environ-\nment and allows for the creation of various traffic scenarios.\nTo comprehensively assess the performance of our method,\nwe established three distinct composite traffic scenarios for\ntraining and testing our algorithm, as depicted in Fig.4. The\nfirst two scenarios are designed for short-distance tests, while\nthe third scenario is for long-distance testing. The first traffic\nscenario includes straight roads and a segment of curved road\nto evaluate the basic driving capabilities of AVs on urban\nroads. In the second scenario, we set up a mixed scenario\ncomprising unsignalized intersections and roundabouts, where\nthe autonomous vehicle is required to execute challenging\ndriving maneuvers such as unprotected left turns. The third\nscenario is designed to evaluate the performance of au-\ntonomous vehicles over longer driving distances, featuring\nmultiple complex intersections and roundabouts. Within each\nscenario, we manipulate the background traffic flow density\nto control the complexity of the scenario, specifically setting\na traffic density of 0.1 as low density and 0.2 as high density.\nDataset Collection. An offline expert dataset is indispens-\nable for training the diffusion model. As illustrated in Figure\n1, we utilize the Soft Actor-Critic (SAC) with the Lagrangian\nalgorithm [32] and human expertise to collect experience\ndata. This expert experience data encompasses trajectories\nand rewards from diverse environments, which are stored in\nthe data buffer and constitute the offline dataset. For every\nscenario, differing in difficulty and traffic density, 10,000\ntrajectories are respectively collected with the expert agent.\nBaselines. We selected several competitive baseline algo-\nrithms that have been widely employed in recent years for\nsafe reinforcement learning and autonomous driving decision\ncontrol. These include the classical Behavior Cloning algo-\nrithm (BC), TD3+BC [33], Decision Transformer (DT) [34],\nIQL [35], Deep Q-learning with a diffusion policy (Diffu-\nsion Q-learning), and Q learning with CAVE policy network\n(CAVE-QL). These methods encompass offline RL, online\nRL, sequence modeling-based RL, and decision modeling\nmethods based on other generative approaches, providing a\ncomprehensive benchmark for evaluating the performance of\nour method.\nTABLE I: The hyperparameter setting of our work\nSymbol\nDefinition\nValue\nBs\nBatch Size\n256\nla\nactor learning rate\n0.001\nlc\ncritic learning rate\n0.99\n\u03bb\nWeight of the Lagrangian term\n0.75\n\u03b3\nDiscount factor for the reward\n0.99\n\u03c4\nSoft update coefficient of the target network\n0.005\nB. Implementation Details\nFor each model,the training timesteps is 20K , batch size is\n512 and the cost uplimit C for the cost function equation 23\nis set as 10. Our diffusion model is built based on a 3-layer\nMLP with 256 hidden units for all networks. As for BCQ\nand Diffusion Q-learning baseline, the policy network is set\nas same as our diffusion actor. For the diffusion model, the\nnumber of diffusion steps N is set as 5. The Kp, Ki and Kd\nfor the PID controller are set as 0.1, 0.003, 0.001, respectively.\nWe set the following values for the reward function: rdis = 1,\nrv = 0.1, rs = 10, rc1 = 5.0, rc2 = 5.0 and rc3 = 5.0. The\ncoefficient of each reward term is set as 1. In the safety cost\nfunction, c1 = 1.0, c2 = 5.0 and c3 = 5.0. The coefficient of\neach cost function term is set as 1. Other parameters are shown\nin Tab.I. All experiments are conducted in a computation\nplatform with Intel Xeon Silver 4214R CPU and NVIDIA\nGeForce RTX 3090 GPU.\nC. Results Analysis\n1) Basic Performance Analysis: Our method was evalu-\nated alongside several baseline algorithms across a variety\nof scenarios with different traffic densities. Our evaluation\nmetrics include Mean Reward, Safety Cost, and Safe Run-\nning Length. Mean Reward is calculated using equation 11\nand assesses the average performance of different algorithms\nacross multiple evaluation iterations; Safety Cost, determined\nby equation 23, quantifies the incidence of safety violations\nduring autonomous driving; Safe Running Length evaluates\nthe average duration an autonomous vehicle can drive safely\nacross different distances.\nFor each test, if the vehicle exhibits any hazardous driving\nbehavior, such as collisions, lane departures, or leaving the\nlane, the record is reset to zero. Overall, an algorithm that\nachieves a higher Mean Reward, lower Safety Cost, and longer\nSafe Running Length demonstrates superior comprehensive\nperformance. Animations demonstrating cases from DDM-Lag\ncan be accessed at the site.1\nSafety\nAnalysis.\nIn\nautonomous\ndriving,\nsafety\nis\nparamount. The safety cost for different algorithms under\nvarious scenarios is presented in Tab.II, where a lower safety\ncost signifies fewer risky decisions and higher algorithmic\nsafety. It is observed that Diffusion QL and TD3+BC exhibit\nthe highest average safety cost across the three scenarios.\nDT and IQL algorithms perform slightly better than other\nbaselines, while our method, owing to the explicit inclusion\n1See\nhttps://drive.google.com/drive/folders/\n1SypbnDVqn4xD85s-UjRkxkcI0Rtd2OXs\n(a)\n(b)\nFig. 5: Interaction performance evaluation results for different sce-\nnarios, (a) scenario 1, (b) scenario 2.\nof safety constraints, significantly reduces safety violations\ncompared to other baselines, achieving the lowest safety cost.\nAdditionally, across different scenarios with varying densities,\nour method maintains safety cost within a controlled range,\ndemonstrating adequate reliability and stability. Across the\nscenarios, it is notable that most algorithms exhibit better\nsafety performance in low-density situations, as high-density\nscenarios may introduce more potential collisions and hazards.\nComprehensive Performance Analysis. Given that safety\ncost is related to the running length of autonomous vehi-\ncles in scenarios, the comprehensive performance of different\nalgorithms requires further analysis. We assess the overall\nperformance of algorithms in test scenarios using average\nreward and Safe Running Length, with results presented in\nTab.III and Tab.IV. The tables reveal that BC and DiffusionQL\nhave the lowest average rewards, but it is noteworthy that these\ntwo algorithms exhibit higher Average Safe Running Length\ncompared to other baselines. This indicates that during testing,\nthese algorithms spend more time on decision-making infer-\nence but achieve poorer performance, suggesting suboptimal\ndecision efficiency and overall performance. The remaining\nfour baselines have closely matched Average Reward perfor-\nmances, with IQL performing slightly better. In comparison,\nour method demonstrates the best performance across different\nscenarios.\n2) Interaction Ability Analysis: Beyond basic decision-\nmaking performance, the ability of algorithms to interact with\nhuman-driven vehicles (HVs) in complex scenarios warrants\nattention. Algorithms that perform well on evaluation metrics\nmay not necessarily be suitable for interaction with human\ndrivers on the road due to potentially aggressive or overly\nTABLE II: The Average Safety Cost of Different Algorithms in Testing.\nTask\nBC\nDiffusion QL\nCVAE-QL\nTD3+BC\nDT\nIQL\nDDM-Lag\nSce.1-Den.1\n13.654\n8.919\n18.043\n8.564\n12.610\n3.865\n0.720\nSce.1-Den.2\n30.283\n31.146\n17.293\n34.604\n8.953\n17.958\n0.874\nSce.2-Den.1\n18.625\n22.825\n24.121\n30.713\n14.147\n17.536\n0.789\nSce.2-Den.2\n22.712\n30.096\n31.054\n33.135\n23.789\n24.100\n0.980\nSce.3-Den.1\n57.955\n75.110\n42.952\n81.654\n26.546\n50.041\n2.103\nSce.3-Den.2\n60.049\n86.197\n80.586\n57.479\n55.393\n64.472\n1.954\nAverage\n33.880\n42.382\n35.675\n41.025\n23.573\n29.662\n1.237\nTABLE III: The Mean Reward of Different Algorithms in Testing.\nTask\nBC\nDiffusion QL\nCVAE-QL\nTD3+BC\nDT\nIQL\nDDM-Lag\nSce.1-Den.1\n105.6\n132.8\n188.6\n210.5\n184.4\n209.5\n230.6\nSce.1-Den.2\n107.9\n47.3\n114.3\n120.8\n137.0\n140.0\n205.7\nSce.2-Den.1\n118.5\n127.3\n173.1\n197.7\n175.9\n183.3\n206.7\nSce.2-Den.2\n92.3\n92.4\n142.0\n149.2\n126.5\n148.9\n171.5\nSce.3-Den.1\n277.7\n260.9\n467.5\n281.8\n392.7\n446.5\n491.3\nSce.3-Den.2\n240.4\n275.0\n269.8\n370.3\n322.7\n329.8\n456.6\nAverage\n157.1\n156.0\n225.9\n221.7\n223.2\n243.0\n293.7\nTABLE IV: The Average Safe Running Length of Different Algorithms in Testing.\nTask\nBC\nDiffusionQL\nCVAE-QL\nTD3+BC\nDT\nIQL\nDDM-Lag\nSce.1-Den.1\n313.7\n370.8\n546.8\n601.0\n515.6\n588.0\n643.6\nSce.1-Den.2\n288.7\n153.7\n322.4\n325.8\n371.4\n360.2\n522.5\nSce.2-Den.1\n413.0\n437.0\n640.8\n687.2\n602.6\n624.5\n712.3\nSce.2-Den.2\n362.7\n401.5\n579.9\n602.6\n508.6\n600.2\n668.9\nSce.3-Den.1\n1093.0\n1190.2\n1833.5\n1164.7\n1575.4\n1793.9\n1915.8\nSce.3-Den.2\n955.3\n1107.2\n1142.6\n1504.6\n1215.2\n1383.1\n1780.5\nAverage\n571.0\n610.1\n844.3\n814.3\n798.1\n891.7\n1040.6\nconservative behaviors that are difficult for humans to com-\nprehend. We employ two of the most widely used indicators\nin traffic safety engineering and driving interaction evaluation:\nTime to Collision (TTC) and Post Encroachment Time (PET).\nTime to Collision refers to the estimated time remaining\nbefore a collision would occur between the subject vehicle\nand a target vehicle, assuming no change in their speeds or\ndirections. Post-Encroachment Time (PET) is an indicator of\nconflict severity that measures the time difference between one\nvehicle leaving and another vehicle entering a common area\nof potential conflict, making it particularly apt for assessing\nvehicular interactions in intersection scenarios.\nGiven their applicability for analyzing micro-interaction be-\nhaviors, we focus our analysis on two short-distance scenarios,\nScenario 1 and Scenario 2, with TTC applicable to Scenario 1\nand PET to Scenario 2. We calculated the average minimum\nTTC and average PET for our method and two baseline\nalgorithms, along with the corresponding driving speeds, as\nshown in Fig.5.\nIn Scenario 1, the Diffusion QL algorithm exhibits the\nlowest TTC and highest average speed, suggesting a ten-\ndency towards more aggressive and risky driving behaviors.\nConversely, the BC algorithm shows a lower average speed,\nindicating a possible conservative bias in the learned decisions.\nThis demonstrates that while BC and Diffusion QL may excel\nin evaluation metrics, they might not be directly applicable\nto real-world traffic decision-making due to their extreme\nbehavioral tendencies. In contrast, DDM-Lag manages to\nmaintain a larger TTC while balancing throughput efficiency,\nindicating a certain advantage.\nIn Scenario 2, the TD3+BC algorithm shows the smallest\nPET and highest average speed, implying a higher interaction\nrisk at intersections. Similarly, the BC algorithm exhibits\nthe largest PET but the slowest average speed, mirroring\nits conservative performance in Scenario 1. Our DDM-Lag\nalgorithm, however, performs better by maintaining a good\nbalance between safety and efficiency.\nD. Ablation Study\nIn this ablation study, we conducted experiments on data\nfrom the most complex scenarios to explore the impact of\nemploying diffusion models as a policy representation and\nthe addition of the Lagrangian safety enhancement module\non the overall method. We compared four algorithms for the\nablation study: Advantage Actor Critic (A2C,without the dif-\nfusion module and Lagrangian safety module), Diffusion-A2C\n(without the Lagrangian safety module), A2C-Lag (without the\ndiffusion module), and Diffusion-BC. The comparsion results\nare shown in Tab.V.\nIt is evident that our method, in comparison to both A2C\nand A2C-Lag, significantly enhances the model\u2019s stability and\naverage reward through the incorporation of the diffusion\nmodule, resulting in superior average performance. Moreover,\nwhen comparing the A2C+diffusion and BC+Diffusion algo-\nrithms, the addition of the Lagrangian module to our method\nfurther elevates the safety performance of the model. The\nablation experiments substantiate that the various components\nof our proposed method synergistically operate to yield com-\nmendable performance.\nTABLE V: Ablation study. We conduct an ablation study to compare our DDM-Lag model with different methods.\nTasks\nA2C\nBC-Diffusion\nA2C-Lag\nA2C-Diffusion\nDDM-Lag\nSce.1-Den.1\n166.8\n181.7\n228.0\n224.4\n230.6\nSce.1-Den.2\n148.8\n162.1\n203.4\n200.2\n205.7\nSce.2-Den.1\n201.0\n143.5\n192.7\n201.1\n206.7\nSce.2-Den.2\n166.8\n119.1\n165.7\n167.0\n171.5\nSce.3-Den.1\n217.6\n444.3\n477.1\n479.2\n491.3\nSce.3-Den.2\n202.3\n412.9\n443.4\n445.4\n456.5\nAverage\n183.9\n244.0\n285.0\n286.2\n293.7\nVI. CONCLUSION\nDecision-making processes are fundamental to the oper-\national integrity and safety of autonomous vehicles (AVs).\nContemporary data-driven decision-making algorithms in this\ndomain exhibit a discernible potential for enhancements.\nIn this study, we introduce DDM-Lag, a diffusion-based\ndecision-making model for AVs, distinctively augmented with\na safety optimization constraint. A key point in our approach\ninvolves the integration of safety constraints within CMDP to\nensure a secure action exploration framework. Furthermore,\nwe employ a policy optimization method based on Lagrangian\nrelaxation to facilitate comprehensive updates of the policy\nlearning process. The efficacy of the DDM-Lag model is\nevaluated in different driving tasks. Comparative analysis\nwith baseline methods reveals that our model demonstrates\nenhanced performance, particularly in the aspects of safety\nand comprehensive operational effectiveness.\nLooking ahead, we aim to further refine the inference effi-\nciency of the DDM-Lag model by fine-tuning its hyperparam-\neters. We also plan to explore and integrate additional safety\nenhancement methodologies to elevate the safety performance\nof our model. Moreover, the adaptability and robustness of\nour model will be subjected to further scrutiny through its\napplication in an expanded array of scenarios and tasks.\nREFERENCES\n[1] Z. Wang, C. Lv, and F.-Y. Wang, \u201cA new era of intelligent vehicles\nand intelligent transportation systems: Digital twins and parallel intelli-\ngence,\u201d IEEE Transactions on Intelligent Vehicles, 2023.\n[2] P. Hang, Y. Zhang, N. de Boer, and C. Lv, \u201cConflict resolution for\nconnected automated vehicles at unsignalized roundabouts considering\npersonalized driving behaviours,\u201d Green Energy and Intelligent Trans-\nportation, vol. 1, no. 1, p. 100003, 2022.\n[3] J. Liu, D. Zhou, P. Hang, Y. Ni, and J. Sun, \u201cTowards socially responsive\nautonomous vehicles: A reinforcement learning framework with driving\npriors and coordination awareness,\u201d IEEE Transactions on Intelligent\nVehicles, 2023.\n[4] X. He and C. Lv, \u201cRobotic control in adversarial and sparse reward envi-\nronments: A robust goal-conditioned reinforcement learning approach,\u201d\nIEEE Transactions on Artificial Intelligence, 2023.\n[5] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yo-\ngamani, and P. P\u00b4erez, \u201cDeep reinforcement learning for autonomous\ndriving: A survey,\u201d IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 4909\u20134926, 2021.\n[6] A. B. Jeddi, N. L. Dehghani, and A. Shafieezadeh, \u201cMemory-augmented\nlyapunov-based safe reinforcement learning: end-to-end safety under\nuncertainty,\u201d IEEE Transactions on Artificial Intelligence, 2023.\n[7] I. Ilahi, M. Usama, J. Qadir, M. U. Janjua, A. Al-Fuqaha, D. T.\nHoang, and D. Niyato, \u201cChallenges and countermeasures for adversarial\nattacks on deep reinforcement learning,\u201d IEEE Transactions on Artificial\nIntelligence, vol. 3, no. 2, pp. 90\u2013109, 2021.\n[8] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, \u201cDiffusion\nmodels in vision: A survey,\u201d IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023.\n[9] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive survey\nof methods and applications,\u201d ACM Computing Surveys, vol. 56, no. 4,\npp. 1\u201339, 2023.\n[10] Z. Wang, J. J. Hunt, and M. Zhou, \u201cDiffusion policies as an expres-\nsive policy class for offline reinforcement learning,\u201d arXiv preprint\narXiv:2208.06193, 2022.\n[11] C. Lu, P. J. Ball, and J. Parker-Holder, \u201cSynthetic experience replay,\u201d\narXiv preprint arXiv:2303.06614, 2023.\n[12] H. He, C. Bai, K. Xu, Z. Yang, W. Zhang, D. Wang, B. Zhao, and X. Li,\n\u201cDiffusion model is an effective planner and data synthesizer for multi-\ntask reinforcement learning,\u201d arXiv preprint arXiv:2305.18459, 2023.\n[13] D. M. Saxena, S. Bae, A. Nakhaei, K. Fujimura, and M. Likhachev,\n\u201cDriving in dense traffic with model-free reinforcement learning,\u201d in\n2020 IEEE International Conference on Robotics and Automation\n(ICRA).\nIEEE, 2020, pp. 5385\u20135392.\n[14] G. Li, S. Li, S. Li, Y. Qin, D. Cao, X. Qu, and B. Cheng, \u201cDeep\nreinforcement learning enabled decision-making for autonomous driving\nat intersections,\u201d Automotive Innovation, vol. 3, pp. 374\u2013385, 2020.\n[15] L. Wen, D. Fu, X. Li, X. Cai, T. Ma, P. Cai, M. Dou, B. Shi, L. He, and\nY. Qiao, \u201cDilu: A knowledge-driven approach to autonomous driving\nwith large language models,\u201d arXiv preprint arXiv:2309.16292, 2023.\n[16] L. Chen, O. Sinavski, J. H\u00a8unermann, A. Karnsund, A. J. Willmott,\nD. Birch, D. Maund, and J. Shotton, \u201cDriving with llms: Fusing\nobject-level vector modality for explainable autonomous driving,\u201d arXiv\npreprint arXiv:2310.01957, 2023.\n[17] W. Schwarting, J. Alonso-Mora, and D. Rus, \u201cPlanning and decision-\nmaking for autonomous vehicles,\u201d Annual Review of Control, Robotics,\nand Autonomous Systems, vol. 1, pp. 187\u2013210, 2018.\n[18] J. Liu, P. Hang, X. Qi, J. Wang, and J. Sun, \u201cMtd-gpt: A multi-\ntask decision-making gpt model for autonomous driving at unsignalized\nintersections,\u201d in 2023 IEEE 26th International Conference on Intelligent\nTransportation Systems (ITSC).\nIEEE, 2023, pp. 5154\u20135161.\n[19] M. Liu, Y. Zhu, Y. Chen, and D. Zhao, \u201cEnhancing reinforcement\nlearning via transformer-based state predictive representations,\u201d IEEE\nTransactions on Artificial Intelligence, 2024.\n[20] H.-n. Wang, N. Liu, Y.-y. Zhang, D.-w. Feng, F. Huang, D.-s. Li, and\nY.-m. Zhang, \u201cDeep reinforcement learning: a survey,\u201d Frontiers of\nInformation Technology & Electronic Engineering, vol. 21, no. 12, pp.\n1726\u20131744, 2020.\n[21] S. Gu, L. Yang, Y. Du, G. Chen, F. Walter, J. Wang, Y. Yang, and\nA. Knoll, \u201cA review of safe reinforcement learning: Methods, theory\nand applications,\u201d arXiv preprint arXiv:2205.10330, 2022.\n[22] E. Altman, Constrained Markov decision processes.\nRoutledge, 2021.\n[23] V. S. Borkar, \u201cAn actor-critic algorithm for constrained markov decision\nprocesses,\u201d Systems & control letters, vol. 54, no. 3, pp. 207\u2013213, 2005.\n[24] F. Berkenkamp, M. Turchetta, A. Schoellig, and A. Krause, \u201cSafe model-\nbased reinforcement learning with stability guarantees,\u201d Advances in\nneural information processing systems, vol. 30, 2017.\n[25] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine, \u201cPlanning with diffu-\nsion for flexible behavior synthesis,\u201d arXiv preprint arXiv:2205.09991,\n2022.\n[26] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal,\n\u201cIs conditional generative modeling all you need for decision-making?\u201d\narXiv preprint arXiv:2211.15657, 2022.\n[27] M. Andrychowicz, M. Denil, S. Gomez, M. W. Hoffman, D. Pfau,\nT. Schaul, B. Shillingford, and N. De Freitas, \u201cLearning to learn by\ngradient descent by gradient descent,\u201d Advances in neural information\nprocessing systems, vol. 29, 2016.\n[28] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d\nAdvances in neural information processing systems, vol. 33, pp. 6840\u2013\n6851, 2020.\n[29] S.-i. Amari, \u201cBackpropagation and stochastic gradient descent method,\u201d\nNeurocomputing, vol. 5, no. 4-5, pp. 185\u2013196, 1993.\n[30] A. Stooke, J. Achiam, and P. Abbeel, \u201cResponsive safety in reinforce-\nment learning by pid lagrangian methods,\u201d in International Conference\non Machine Learning.\nPMLR, 2020, pp. 9133\u20139143.\n[31] Q. Li, Z. Peng, L. Feng, Q. Zhang, Z. Xue, and B. Zhou, \u201cMetadrive:\nComposing diverse driving scenarios for generalizable reinforcement\nlearning,\u201d IEEE transactions on pattern analysis and machine intelli-\ngence, vol. 45, no. 3, pp. 3461\u20133475, 2022.\n[32] S. Ha, P. Xu, Z. Tan, S. Levine, and J. Tan, \u201cLearning to walk in the real\nworld with minimal human effort,\u201d arXiv preprint arXiv:2002.08550,\n2020.\n[33] S. Fujimoto and S. S. Gu, \u201cA minimalist approach to offline reinforce-\nment learning,\u201d Advances in neural information processing systems,\nvol. 34, pp. 20 132\u201320 145, 2021.\n[34] L. Chen, K. Lu, A. Rajeswaran, K. Lee, A. Grover, M. Laskin, P. Abbeel,\nA. Srinivas, and I. Mordatch, \u201cDecision transformer: Reinforcement\nlearning via sequence modeling,\u201d Advances in neural information pro-\ncessing systems, vol. 34, pp. 15 084\u201315 097, 2021.\n[35] I. Kostrikov, A. Nair, and S. Levine, \u201cOffline reinforcement learning\nwith implicit q-learning,\u201d arXiv preprint arXiv:2110.06169, 2021.\n",
    "2404.02524": "Versatile Scene-Consistent Traffic Scenario\nGeneration as Optimization with Diffusion\nZhiyu Huang1\u22c6, Zixu Zhang2\u22c6, Ameya Vaidya2, Yuxiao Chen3,\nChen Lv1, and Jaime Fern\u00e1ndez Fisac2\n1 Nanyang Technological University zhiyu001@e.ntu.edu.sg\n2 Princeton University {zixuz, jfisac}@princeton.edu\n3 NVIDIA Research\nAbstract. Generating realistic and controllable agent behaviors in traf-\nfic simulation is crucial for the development of autonomous vehicles. This\nproblem is often formulated as imitation learning (IL) from real-world\ndriving data by either directly predicting future trajectories or inferring\ncost functions with inverse optimal control. In this paper, we draw a\nconceptual connection between IL and diffusion-based generative mod-\neling and introduce a novel framework Versatile Behavior Diffusion\n(VBD) to simulate interactive scenarios with multiple traffic participants.\nOur model not only generates scene-consistent multi-agent interactions\nbut also enables scenario editing through multi-step guidance and refine-\nment. Experimental evaluations show that VBD achieves state-of-the-art\nperformance on the Waymo Sim Agents benchmark. In addition, we illus-\ntrate the versatility of our model by adapting it to various applications.\nVBD is capable of producing scenarios conditioning on priors, integrating\nwith model-based optimization, sampling multi-modal scene-consistent\nscenarios by fusing marginal predictions, and generating safety-critical\nscenarios when combined with a game-theoretic solver. Project website:\nhttps://sites.google.com/view/versatile-behavior-diffusion\nKeywords: Traffic Simulation \u00b7 Scene Generation \u00b7 Multi-agent Inter-\naction \u00b7 Diffusion Model \u00b7 Behavior Prediction\n1\nIntroduction\nSimulation plays a crucial role in validating the performance of autonomous\ndriving systems. A primary challenge is generating diverse, realistic, and in-\nteractive traffic behaviors in a scalable and human-like manner. Conventional\nmodel or rule-based methods [33,59] are inadequate to deliver realism and cap-\nture human interactions in the real world. Leveraging readily available driving\nlogs [2, 39, 63], many studies have turned to data-driven methods and apply\nimitation learning (IL) techniques to model more realistic behaviors for traffic\n\u22c6Equal contribution\narXiv:2404.02524v1  [cs.RO]  3 Apr 2024\n2\nZ. Huang et al.\nControlled agent\nTraffic light\nStop sign\nSpeed [m/s]\nSpeed bump\nFig. 1: Performance of our VBD model on the Waymo Sim Agents task. The multi-\nagent diffusion policy is capable of controlling a large number of agents in an interactive\nand map-adherent manner for traffic simulation.\nagents [16,30,57,66,69]. However, these methods mostly focus on a single agent\nor employ a shared policy across all traffic participants, leading to a lack of\nscene consistency and causing collisions when deployed in a highly interactive\nscenario [7]. To improve consistency, one can formulate the multi-agent behav-\nior generation as a joint trajectory optimization problem, and explicitly learn\na model to rationalize traffic interaction. Modeling optimization objective is a\nstandard inverse optimal control (IOC) or inverse reinforcement learning (IRL)\nproblem [41,73]. Strategies vary from directly training a mapping from scenario\nto cost [28,48,61], or learning the cost weights for a set of handcraft heuristics\nthrough differentiable optimization layers [12, 24, 26]. However, these methods\nstruggle to scale to general scenes or require cumbersome cost function design.\nOur objective is to leverage diffusion models (a.k.a score-based models) [23,51,\n53, 56], a class of generative modeling methods that gradually recover struc-\ntured data from random noise, for traffic scenario generation. Diffusion models\nenable effective behavior modeling for multi-agent joint futures and allow for it-\nerative refinement. A critical aspect of diffusion models is controllability, which\nenables generating scenarios or editing agent behaviors to meet specific user re-\nquirements (e.g., cooperative or adversarial). Although diffusion models have\nbeen increasingly employed for generating agent behaviors [5, 20, 32, 64, 70, 71],\ntraining and controlling diffusion models for traffic agent interaction modeling\nremain challenging, and its connection with the classic formulation of this task\nVersatile Scene-Consistent Scenario Generation with Diffusion\n3\nunder imitation learning has been overlooked. Therefore, we aim to bridge the\nconceptual gap between scenario generation and diffusion and develop a practi-\ncal framework to model scene-consistent interactive traffic scenarios and enable\nuser-specified behavior generation through structured guidance.\nIn this paper, we propose the Versatile Behavior Diffusion (VBD), which\nutilizes both the map and historical states of agents as conditional inputs to\ngenerate realistic and controllable traffic scenarios. VBD consists of three main\ncomponents. First, we employ a query-centric Transformer-based [50, 72] scene\ncontext encoder, which encodes the states of agents and map polylines in their\nlocal coordinates and preserves relative information in attention, thus enhancing\nthe multi-agent modeling performance. Second, we introduce a Transformer-\nbased denoiser to generate scene-level joint behaviors of agents from noise. Third,\nwe incorporate a Transformer-based multi-modal trajectory predictor to forecast\nthe individual agents\u2019 intentions as behavior priors. While this predictor is not\nrequired for scenario generation, we find it improves training stability and can be\nincorporated with the denoiser to sample diverse scene-consistent scenarios. The\nversatility of our model can be realized through directly predicting scenarios\nthrough the denoiser in one step, controllable sampling of various tasks with\nguidance, and further improving the generation quality by fusing the behavior\npriors. The primary contributions of this paper are summarized as follows:\n1. We draw conceptual insights to connect diffusion generative modeling with\nthe classic imitation learning formulation of traffic scenario generation.\n2. We propose the Versatile Behavior Diffusion model that integrates joint\nmulti-agent diffusion policy and marginal multi-modal behavior prediction\nto facilitate realistic and controllable traffic simulation.\n3. We demonstrate the model\u2019s state-of-the-art performance of multi-agent in-\nteraction modeling on the Waymo Sim Agents Benchmark.\n4. We propose various straightforward guidance strategies for the diffusion pol-\nicy, including cost function, goal priors, and game theory structure, to gen-\nerate diverse, realistic, and specified traffic scenarios.\n2\nRelated Work\nTraffic Simulation. There has been a growing shift towards learning-based\nmethods to enhance the realism and interactivity of traffic simulations [58].\nBITS [65] employs imitation learning to simulate agent behaviors by infer-\nring high-level intent and replicating low-level driving actions. The socially-\ncontrollable behavior generation model proposed in [6] focuses on simulating\nsocial and interactive behaviors. Symphony [30] integrates learning-based poli-\ncies with parallel beam search to further enhance realism. Trajeglish [44] intro-\nduces a multi-agent sequence of motion tokens using a GPT-like encoder-decoder\narchitecture, achieving state-of-the-art realism. Another line of research focuses\non generating safety-critical or adversarial scenarios to test the robustness of\ndriving systems. STRIVE [47] generates challenging scenarios that can induce\ncollisions with the ego planner through optimization in latent space. Similarly,\n4\nZ. Huang et al.\nAdvDO [3] and KING [21] utilize optimization-based methods to generate adver-\nsarial trajectories for robust planning. TrafficBots [69] introduces a multi-agent\npolicy conditioned on specific goals to generate configurable behaviors, though\nit faces challenges in goal formation. CAT [67] chooses conflicting trajectories\nfrom the predicted behavior distribution of agents. However, those methods can-\nnot produce interactions during the safety-critical scenario, as the trajectory of\nthe agent under attack are often assumed to be fixed or known to the adversar-\nial counterpart. Moreover, current simulation models often lack versatility since\nthey are trained for either maximum likelihood (realistic) behaviors or adver-\nsarial scenarios. We aim to build a unified framework for both tasks to obtain\nrealistic and controllable traffic simulation.\nMulti-modal Behavior Prediction. Behavior prediction is closely related\nto traffic simulation or behavior cloning tasks [16, 57, 69]. Recent advances in\nlearning-based behavior prediction models have significantly increased the accu-\nracy of both agent-wise motion prediction [27,40,72] and scene-level multi-agent\njoint prediction [25,38,50]. Leveraging a large amount of real-world data, they are\ncapable of generating accurate multi-modal distributions of possible behaviors\nfor multiple agents in a scene. In addition, diffusion models have been applied\nin behavior prediction and generation tasks [10,32,43], demonstrating superior\nresults in multi-agent motion prediction. Our proposed model integrates multi-\nmodal behavior prediction as action priors or feasible high-level intentions for\ntraffic agents, which aligns with realistic distributions and can be used in guided\ndiffusion to generate specific agent behavior.\nDiffusion Models. Score-based model, a.k.a Diffusion models [23, 51\u201353, 56],\nhave gained widespread popularity in various generative tasks, including im-\nage [68], audio [34], and video [14] generation. Recently, diffusion models have\nshown great potential in traffic scenario generation due to their diversity and\ncontrollability. SceneDM [20] utilizes a diffusion model to generate joint and\nconsistent future motions of all agents in a scene. MotionDiffuser [32] employs\na diffusion-based representation for joint multi-agent motion prediction and in-\ntroduces a constrained sampling framework for controlled trajectory sampling.\nCTG [71] combines diffusion modeling and STL rules to enforce traffic rules\non generated trajectories. CTG++ [70] leverages Large Language Models to\ntranslate user queries into loss functions, guiding the diffusion model toward\ngenerating query-compliant scenarios. TRACE [46] proposes a guided diffusion\nmodel to generate future trajectories for pedestrians, employing analytical loss\nfunctions to impose trajectory constraints. DiffScene [64] and [5] utilizes guided\ndiffusion with adversarial optimization objectives to simulate safety-critical sce-\nnarios. However, a conceptual understanding of Diffusion models under traffic\nsimulation settings has been overlooked in previous works. We aim to explore the\noptimal training strategies for traffic behavior modeling using diffusion model\nand propose various sampling strategies, to enhance realism and versatility.\nVersatile Scene-Consistent Scenario Generation with Diffusion\n5\n3\nProblem Formulation\nTraffic Scenario Generation as Optimization. Consider a traffic scenario\nS = (x, u, c) with episode length T containing a tensor of A agents\u2019 trajecto-\nries x = (x1, . . . , xA) \u2208RA\u00d7T \u00d7Dx and control sequences u = (u1, \u00b7 \u00b7 \u00b7 , uA) \u2208\nRA\u00d7T \u00d7Du. The context of the scene c \u2208RDc includes information regarding the\nroad map, the status of traffic lights, the initial joint state of all agents x0, etc.\nGiven an optimization objective J\u03b8(x, u; c), we formulate scenario generation as\na finite-horizon optimal control problem by:\n  \\\nunders et {\\ctr ls eq \\in \\r e als  ^{\\ n agent s \\t im e s \\ t h o r i z on \\times \\nc } }{\\min }~\\cost _\\hparam (\\traj ,\\ctrlseq ;\\condition ), ~\\text {s.t.}~ \\traj _0 = \\state _0, ~\\traj _{\\tdisc +1} = \\dyn (\\traj _{\\tdisc }, \\ctrlseq _{\\tdisc }), ~ \\forall \\tdisc \\in \\{0,\\ldots ,\\thorizon -1\\},\\label {eqn: opt_prob} (1)\nwhere f represents the discrete-time joint dynamics. If our goal is to generate\nrealistic (statistically representative) scenarios, the objective J\u03b8 should be de-\nsigned to incentivize real-world driving behaviors, whether through a statistical\nloss metric (behavior cloning) or by using inverse reinforcement learning (IRL).\nOnce J\u03b8 is established, we need to tractably find an optimal joint control se-\nquence u, either through numerical optimization or reinforcement learning.\nGenerative Modeling as Trajectory Optimization. Instead of solving the\naforementioned IL problem in two steps, prior works [18,22] have shown a strong\nconnection between IL and generative modeling under a generative-adversarial\ntraining framework. Extending the analysis of [8, 13, 36], we show that synthe-\nsizing a diffusion generative model in this IL setting can be viewed as learning\nthe gradient descent step of a particular optimal control solver.\nConsider a dataset D with scenario triplets sampled independently from an un-\nknown distribution p. Since we are interested in scenario generation given a scene\ncontext and the recorded trajectory x is a rollout of control u with known dy-\nnamics f, we can factorize the probability density function as p(S) = p(u|c)p(c).\nUnder Maximum Entropy IRL [73] formulation, we aim to approximate p(u|c)\nas the Boltzmann distribution of an optimization objective:\n  \\dis t rdata ( \\c t\nrl\nseq |\\conditi on  )\\approx \\distr _\\theta (\\ctrlseq |\\condition ):=\\frac {1}{Z_\\theta }\\exp (-\\cost _\\hparam (\\traj (\\ctrlseq ), \\ctrlseq ;\\condition )), \\label {eqn: ebm} \n(2)\nwhere Z\u03b8 is the partition function. Eq. (2) resembles the Energy-Based Models\n(EBM) [35,55]. Specifically, we want to learn the parameter \u03b8 of the optimization\nobjective that maximizes the conditional log-likelihood of the dataset D:\n  \\th eta\n =\n \\argmax  _{\\hat \\theta }\\expectation _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\theta }(\\ctrlseq |\\condition )]. \\label {eqn: cmle} \n(3)\nIdeally, we can employ score-matching [29,53,54,60] to directly learn the gradient\nof J\u03b8 w.r.t the control (our random variable of interest) as the score function:\n  \\sm all \\n a bla _{\\ ct rl seq  } \\log  \\distrdata (\\ ct rlseq\n \n|\\\nco ndi tion )\\approx \\score _\\theta (\\ctrlseq |\\condition ) := \\nabla _{\\ctrlseq } \\log \\distr _\\theta (\\ctrlseq |\\condition ) = -\\nabla _{\\ctrlseq } \\cost _\\hparam (\\traj (\\ctrlseq ),\\ctrlseq ;\\condition ) - \\cancelto {0}{\\nabla _\\ctrl \\log Z}. \\label {eqn:score} \n(4)\nIf \u2207uJ\u03b8 was obtained over the entire action space, we could use it for gradient\ndescent. However, since the dataset contains mostly near-optimal scenarios, the\n6\nZ. Huang et al.\ngradient estimation in suboptimal regions of the action space (away from demon-\nstration data) may be inaccurate or not well-defined. To overcome this issue, a\nclass of approaches [13, 23, 52, 54, 56] utilize a stochastic process to gradually\ndiffuse p into noised distributions pk for k steps until it becomes a known distri-\nbution pK = \u03c0. These methods are commonly known as Diffusion models [23,52]\nand are later generalized as score-based models by [56]. Specifically, we train a\nstep-conditioned score function s\u03b8(\u02dcu|c, k) to approximate the gradient of the log\nnoised distribution \u2207\u02dcu log pk(\u02dcu) by:\n  \\th eta\n =\n \\argmax _{\\hat \\theta }\n\\\nexpectat ion  _{\\scen a rio \\sim \\da\nt\naset ,k\\sim \\uniform (0,K)}\\expectation _{\\tilde {\\ctrlseq }\\sim \\distrdata _k(\\cdot |\\ctrlseq )} \\left [ \\lambda (k)\\lVert \\nabla _{\\tilde {\\ctrlseq }}\\log \\distrdata _k(\\tilde {\\ctrlseq }|\\ctrlseq )-\\score _{\\hat \\theta }(\\tilde {\\ctrlseq }|\\condition , k)\\rVert \\right ], \\label {eqn: score_loss} (5)\nwhere \u03bb(k) is a positive weighting function. At inference time, we can generate\nscenarios by first randomly selecting \u02dcu from the known distribution \u03c0 and sam-\npling through the reverse diffusion process.\nConnecting this formulation of generative modeling with trajectory optimization,\nwe can view the forward diffusion as uplifting original data distribution into a\nhigher-dimensional space augmented by diffusion step k. By injecting noise, we\nachieve good coverage over the entire action space in the final step K so that\ns\u03b8(\u02dcu|c, K) are well defined for random \u02dcu. Sampling through reverse diffusion can\nbe interpreted as stochastic gradient descent towards high-probability regions\nwith a fixed descent direction along the diffusion step, analogous to the direct\nshooting method in optimal control. We note that at low noise level k, as pk is\nclose to the original data distribution p, s\u03b8(u|c, k \u21920) \u2248\u2212\u2207uJ\u03b8(x, u; c), which\nis the gradient we originally try to model. Therefore, the generative modeling\nof scenarios can be viewed as an explicit solution of IL by learning the gradient\nsteps of trajectory optimization and solving the optimal control problem through\nreverse diffusion sampling.\nIn the remainder of the paper, without loss of generality, we consider a specific\nform of Diffusion model, diffusion-denoising probabilistic models (DDPM) [23],\nwhich is also known as the discrete-time variance-preserving score-based SDE\n(VP-SDE) [56]. The equivalence between the original DDPM training objective\nand the score-matching loss (Eq. (5)) has been shown in [37].\nControllable and Compositional Generation. In many applications, we\nwant to generate scenarios that satisfy a specific user requirement y without\nretraining the model. For example, y can be defined as the goal or the reference\npath for individual agents, or it can describe a soft constraint, such as obeying\nthe speed limit or avoiding collisions. From the perspective of optimal control\n(Eq. (1)), we modify the optimization objective to: J\u03b8(x, u; c)+Jy(x, u; c). Plug-\nging into the EBM representation, we obtain a new conditional distribution:\np(u|c, y) \u221dp(u|c)p(y|u, c), where p(u|c) is the data distribution we approxi-\nmated through generative modeling and p(y|u, c) is the likelihood of y. This\nimmediately resembles the compositionality in EBM [13] and Classifier Guid-\nance in diffusion model [11]. Specifically, we can sample the reverse process with\nVersatile Scene-Consistent Scenario Generation with Diffusion\n7\na conditional score function:\n  \\ nab la _{\\ti ld e  \\ctrlse q } \\ log \\dis tr d ata  _k (\\tilde \\ctrlseq |\\condition , y)\\approx \\score _\\theta (\\tilde {\\ctrlseq }|\\condition ,y,k) = \\score _\\theta (\\tilde {\\ctrlseq }|\\condition ,k) + \\nabla _{\\tilde \\ctrlseq } \\log \\distrdata _k(y|\\tilde \\ctrlseq , \\condition ), \n(6)\nwhere pk(y|\u02dcu, c) is the likelihood of y given the noised action \u02dcu at step k. It is\nimportant to note that pk(y|\u02dcu, c) is not equivalent to the likelihood of y in the\ndata distribution p(y|u, c), therefore it is typically required to train a separate\nmodel [11, 31]. Prior works [32, 70, 71] proposed practical approximation to the\ngradient of noised likelihood with the gradient of Jy for guidance, which en-\nables flexible composition and controllability with additional objective without\ntraining. We analyze these guidance methods in Sec. 4.\n4\nVersatile Behavior Diffusion Model\nModel Structure. The Versatile Behavior Diffusion model consists of three\nmain components as illustrated in Fig. 2. The scene encoder E\u03d5 : c 7\u2192\u02c6c encodes\nthe scene context c into its latent representation \u02c6c using query-centric attention\nTransformers [50]. Leveraging rich scene context information from encoder, the\ndenoiser D\u03b8 : (\u02c6c, \u02dcu, k) 7\u2192\u02c6u directly predict a joint control sequence \u02c6u from \u02c6c and\nnoised control \u02dcu at step k. This allows our model to perform one-step genera-\ntion, while still maintaining the capability of iterative sampling and refinement.\nThe behavior predictor P\u03c8 : (\u02c6c, {\u03b6i}M\ni=1) 7\u2192{CatM(\u02c6ua, \u02c6\u03c9a)}A\na=1 predicts an M-\nmode marginal categorical trajectory distribution of each agent from \u02c6c with the\nhelp of a set of representative static end-point anchors {\u03b6i}M\ni=1 extracted from\ndata [50]. All three modules utilize a stack of query-centric self-attention and\ncross-attention blocks for flexibility and scalability. Details regarding the model\narchitecture can be found in the supplementary materials.\nModel Training. We implement a multi-task learning framework that concur-\nrently trains the encoder, denoiser, and predictor components of our model. To\ntrain the denoiser, we aim to minimize the denoising loss:\n  \\ m athcal {L}_{\\denoiser } = \\expectation _{\\ sce nar i o \\ sim \\distrdata ,k\\sim \\uniform (0,K)}\\expectation _{\\tilde {\\ctrlseq }\\sim \\distrdata _k(\\cdot |\\ctrlseq )} \\left [ \\lambda (k)\\mathcal {SL}_1( \\hat {\\traj }(\\denoiser (\\hat \\condition , \\tilde \\ctrlseq , k))-\\traj ) \\right ], \\label {eqn: denoise_loss} \n(7)\nwhich is defined as the the Smooth L1 loss between ground-truth trajectories x\nand the trajectories \u02c6x rollout from \u02c6u.\nTraining a denoiser with the scene encoder directly can be unstable, partially\nbecause the denoiser focuses on structured data from the noisy trajectories rather\nthan the information from context encoding. To address this issue, we suggest\nincorporating an additional task in the model to predict multi-modal trajectories,\nwhich can more effectively attend to the context encoding. This setting not\nonly stabilizes training and enhances the overall learning performance but also\nprovides behavior priors for individual agents. To train the behavior predictor\nP\u03c8, we first select the mode m\u2217that most closely matches the ground truth\ntrajectory of each agent and minimize the predictor loss defined as:\n \\s m all \n\\ m\na\nthc\nal \n{\nL}_{\\predi c tor\n } = \\ex pect\na\ntion _{\\scenario \\sim \\distrdata } \\left [ \\sum _{a=1}^{A}\\mathcal {SL}_1 \\left (\\hat \\traj (\\hat \\ctrlseq ^{a,m^*}) - \\traj ^a\\right ) + \\beta {CE} (m^*, \\hat {\\omega }^a) \\right ], \n(8)\n8\nZ. Huang et al.\nScene Encoder\nDenoiser\nAgents\nMap Polylines\nStatic End-goal Anchors\nScene-Consistent\nJoint Plans\nAgent-wise Marginal \nTrajectories\nBehavior Predictor\nNoise Level\nQuery-centric \nAttention\nQuery-centric \nAttention\nQuery-centric \nAttention\nSelf \nAttention\nCross \nAttention\nCross \nAttention\nCross \nAttention\nOptional \nGuidance\nScene Context\nCondition\nNoisy Trajectories\nEncoder\nPosition \nEmbed\nCost\nFig. 2: Overview of the proposed VBD model. The input scenario tokens are encoded\nthrough a query-centric Transformer-based scenario encoder. The behavior predictor\ngenerates marginal multi-modal trajectories. The denoiser predicts the joint multi-\nagent future trajectories while attending to themselves and the condition tokens. Dur-\ning inference, the predicted behavior priors or user-defined model-based objectives J\ncan be used to guide the denoising process to generate desired scenarios.\nwhich penalizes the smooth L1 difference between the ground truth trajectory of\neach agent and the trajectory from the best mode m\u2217, and encourages a higher\nprobability to be assigned on this mode through a Cross-Entropy loss.\nScenario Generation with VBD. To employ the VBD model for scenario\ngeneration or behavior modeling, we can use the following implementations.\n\u2013 One-step generation. We directly query the denoiser with a randomly sam-\npled noised \u02dcu. Since the encoder provides rich scene context information, we\nempirically find one-step generation can sample high-quality joint scenarios.\nHowever, due to strong conditioning on the scene context, one-step sampling\noften collapses to a single mode of scenario.\n\u2013 Multi-step sampling. To improve sample diversity, we utilize a standard\nDDPM sampling approach by iteratively querying the denoiser and updating\nthe noised \u02dcu.\n\u2013 Guided Sampling. To impose constraints or targets on specific agents, we\nuse the classifier guidance method [11] by alternatively updating the noised\n\u02dcu with denoiser output and \u2207\u02dcu log pk(y|\u02dcu, c). We evaluate two approaches\nfor approximation: 1. CTG [70, 71] directly approximates log pk(y|\u02dcu, c) \u2248\nlog p(y|\u02dcu, c) = \u2212Jy(x(\u02dcu), \u02dcu; c); 2. MotionDiffuser (MD) [32] approximates\nlog pk(y|\u02dcu, c) \u2248Jy(x(D\u03b8(\u02dcu)), D\u03b8(\u02dcu); c), where D\u03b8(\u02dcu) is the one-step gener-\nation result from denoiser.\nDiverse Scene-consistent Scenario Generation. Sampling diverse outputs\nfrom a conditional diffusion model is challenging, especially when the denois-\ning strongly relies on the context information [49]. On the other hand, behavior\nVersatile Scene-Consistent Scenario Generation with Diffusion\n9\npredictors capture the multi-modal trajectories of individual agents but will re-\nsult in scene inconsistency if marginal trajectories are naively combined, because\nthe predictor alone cannot ensure the collective coherence necessary for realistic\nmulti-agent scenario generation. VBD can used as an effective scenario optimizer\nand produce diverse and scene-consistent scenarios by first sampling goal posi-\ntions from the behavior predictor (or any other models) and generating joint\ntrajectories matching individual goals using guided sampling with the denoiser.\nGame-theoretic Safety-critical Scenario Generation. It is essential to ex-\npose AVs to a variety of simulated safety-critical scenarios to stress-test and\nimprove their planning ability. To generate interactive safety-critical scenarios,\nwe model the two-agent interaction as a map-constrained pursuit-evasion game,\nwhere the pursuer aims to cause a collision with the evader, while the evader\nattempts to avoid it. One approach to solving such a game is iterative best re-\nsponse (IBR), such as gradient descent\u2013ascent (GDA). Specifically, we can apply\n\u03c4-GDA to guarantee local convergence to a stable minimax equilibrium of the\npursuit-evasion game [17]. Generating a scenario with a game-theoretic solver\nalone often leads to unrealistic results that disregard the scene context. Instead,\nleveraging realistic traffic behavior modeled by VBD, we propose a game-guided\ndiffusion scheme by alternative denoising, performing gradient descent and as-\ncent for agents, and updating the noised \u02dcu until convergence. The algorithm for\nthe game-theoretic sampling can be found in the Supplementary Materials.\n5\nExperiments\nPlatform. We conduct the experiments on the large-scale Waymo Open Mo-\ntion Dataset (WOMD) [15], which includes 486,995 9-second logged real-world\nscenarios for training and 44,097 scenarios for validation. The dataset provides\ntracks of all agents and corresponding vectorized maps in each scenario. We\nemploy the Waymax simulator [19] as the interface for closed-loop traffic sim-\nulation. In the Waymo Sim Agents benchmark [39], we evaluate our model on\n44,920 testing scenarios. To improve closed-loop rollout performance and stabil-\nity [4], trajectories are replanned in the receding horizon fashion.\nImplementation Details. During training, we consider A = 32 agents, 256\nmap polylines (each containing 30 waypoints), and 16 traffic lights in the scene.\nVBD generates T = 80 steps of future control sequences with step size 0.1s based\non (up to) 11 steps of past trajectories. The scene encoder contains 6 query-\ncentric-attention Transformer layers, and the embedding dimension is 256. The\nbehavior predictor comprises 4 cross-attention Transformer layers and generates\nM = 64 possible trajectories for each agent along with respective probability es-\ntimates. The denoiser includes two decoding blocks with a total of 4 Transformer\nlayers. A cosine variance schedule is adopted in the diffusion process, employing\nK = 10 diffusion steps, and the maximum value of \u03b2(k) is set to 0.999. The pre-\ndicted raw actions are standardized during the diffusion process, with the mean\n10\nZ. Huang et al.\nand standard deviation of actions set to 0 and 1. The scalability of Transformer\nblocks allows VBD to be adapted to any number of agents during inference.\nEvaluation Metrics. We follow the official evaluation metrics of Waymo Sim\nAgents benchmark [39] encompassing kinematic, interactive, and map-based fea-\ntures, and a meta realism metric is calculated as a weighted sum of these features.\nFor the assessment of controllable scenario generation, we utilize a subset of 500\nWOMD interactive validation scenarios selected by prior work [67]. We employ\nmetrics provided by the Waymax simulator [19], such as off-road incidents, colli-\nsions, wrong-way, kinematic infeasibility, and average displacement error (ADE)\nof the rollout trajectories w.r.t. the ground-truth ones (log divergence).\nResults on Sim Agents Benchmark. The Waymo Sim Agents benchmark re-\nquires 32 independent rollouts of simulation from one scenario, and each rollout\ncontains the x/y/z coordinates and headings for up to 128 agents over an 8-\nsecond future horizon. We evaluate the performance of scenario generation from\nVBD\u2019s joint diffusion policy by initiating the denoising process sampling from\nunit Gaussian noise. As summarized in Tab. 1, we demonstrate that the behav-\nior generation performance from VBD closely matches state-of-the-art models.\nWe present a selection of qualitative simulation results in Fig. 1, showcasing the\nmodel\u2019s ability to generate diverse and realistic traffic scenarios. Further analysis\nsuggests VBD is capable of facilitating realistic agent interactions through its\njoint multi-agent diffusion policy.\nTable 1: Testing Results on the Waymo Sim Agents Benchmark\nMethod\nRealism Meta Kinematic Interactive Map-based minADE\nVBD (Ours)\n0.6342\n0.4212\n0.7256\n0.8200\n1.3509\nTrajeglish [44]\n0.6437\n0.4157\n0.7646\n0.8104\n1.6146\nMVTA [62]\n0.6361\n0.4175\n0.7390\n0.8139\n1.8698\nMTR+++ [45]\n0.6077\n0.3597\n0.7172\n0.8151\n1.6817\nSceneDM [20]\n0.5821\n0.4244\n0.6675\n0.7000\n2.4186\nCAD [9]\n0.5314\n0.3357\n0.5638\n0.7688\n2.3146\nJoint-Multipath++ [39]\n0.4766\n0.1792\n0.6322\n0.6833\n2.0517\nScene-consistent Scenario Optimization. In this experiment, we evaluate\nthe effectiveness of VBD as a scenario optimization tool to generate scene-\nconsistent interactions from marginal behavior priors. Specifically, the baseline\n(marginal) method directly samples actions from the behavior predictor in a\nreceding horizon fashion with the replan frequency as 1 Hz. For each agent in\nthe scene, we select the final positions of agents\u2019 most likely predicted trajec-\ntories as the goal prior and apply guided sampling to minimize the mean L2\ndistance between the denoised results and goals. We compare their performance\nwith 500 scenarios of the WOMD interactive subset and test each method with\nVersatile Scene-Consistent Scenario Generation with Diffusion\n11\nthree different random seeds. The average L2 distance between the goals and\nrollout trajectories is 2.1833 m, which shows VBD can reach goal points better\nthan marginal samples. The results in Tab. 2 indicate VBD significantly reduces\nthe collision rate and our model better captures interactions between agents.\nMoreover, under the circumstance when priors were selected from suboptimal\nsamples, e.g. off-road or wrong-way, VBD can alleviate these cases and generate\nscenarios that adhere to scene context.\nTable 2: VBD Improves Scene-Consistency from Marginal Behavior Priors\nMethod\nCollision [%] \u2193Off-road [%] \u2193Wrong-way [%] \u2193Kin. [%] \u2193\nADE [m]\nMarginal\n5.61\u00b10.27\n6.19\u00b10.05\n0.86\u00b10.09\n0.31\u00b10.02 1.113\u00b10.012\nMarginal + VBD\n2.23\u00b10.15\n1.26\u00b10.10\n0.52\u00b10.11\n0.32\u00b10.01 0.974\u00b10.005\nAdditionally, we find that VBD is capable of generating interactive conditional\npredictions, when there is only one or a subset of agents are conditioned by\nmarginal priors. Here, a human user supervises the simulation process and man-\nually selects the target agent and its target goal. As seen in Fig. 3, by conditioning\non the behavior that vehicle 5 makes a left turn ignoring the stop sign, VBD\u2019s\ndiffusion policy can generate a scenario such that vehicle 1 is forced to yield for\nthe other vehicle. On the other hand, the nominal diffusion policy rollout with-\nout any guidance follows a completely different but still scene-consistent traffic\nordering. Such behaviors can be observed in a variety of scenes, and additional\nexamples can be found in the Supplementary Materials.\nTable 3: Cost-guided Sampling Improves Generation Quality\nMethod\nCollision [%] \u2193Off-road [%] \u2193Wrong-way [%] \u2193Kin. [%] \u2193\nADE [m]\nVBD\n2.47\u00b10.09\n1.21\u00b10.14\n0.57\u00b10.02\n0.24\u00b10.01 1.010\u00b10.007\nVBD + Collision (CTG)\n1.67\u00b10.58\n1.51\u00b10.18\n0.86\u00b10.08\n0.25\u00b10.01\n1.711\u00b10.020\nVBD + Collision (MD)\n1.11\u00b10.23\n1.15\u00b10.17\n0.60\u00b10.12\n0.25\u00b10.01\n1.113\u00b10.010\nComposition with Model-Based Objectives. The performance of VBD can\nbe further improved when combined with a model-based optimization objective.\nIn this experiment, we introduce a simple collision avoidance cost to maximize\neach agent\u2019s minimum distance from others along the horizon. We compare the\nperformance of different guidance methods including CTG guidance [71] and\nMotionDiffuser (MD) guidance [32]. As shown in Tab. 3, this simple objective\nfurther reduces the collision rate. We observe that under the same cost, CTG\nhas worse performance. One explanation is that noised \u02dcu sampled from u with\nlow Jy may have high costs, and assuming the pk is equivalent to the data\ndistribution p in CTG could lead to incorrect gradient at high noise level. The\nqualitative results in Fig. 4 also illustrate that, by composing with a model-based\noptimization objective, the model can generate collision-free interaction in the\nchallenging narrow-passage scene.\n12\nZ. Huang et al.\nW/o Condition\nWith Goal Condition\nInteractive Agent\nPrior Agent\nOther Agent\nSpeed [m/s]\nGoal\nFig. 3: Results of conditional generation. Top: Nominal VBD rollout without guidance\ngenerates a scene-consistent scenario, where Vehicle 5 (in purple) waits at the stop sign\nand then precedes. Bottom: Using goal-guided diffusion to minimize Vehicle 5\u2019s final\nposition w.r.t to a given goal, we enforce Vehicle 5 to run the stop sign. VBD model\ngenerates a scene-consistent scenario with Vehicle 1 yielding to Vehicle 5.\nAgent 0\nAgent 1\nOther Agent\nSpeed [m/s]\nFig. 4: Results of collision cost-guided generation. Two vehicles interact and coordinate\nin a narrow passage scenario with collision cost function guidance. (Note: Vehicles 3\nand 5 have been in collision since the initial step.)\nSafety-critical Scenario Generation. As illustrated in Fig. 5 and Fig. 6, our\nmodel is capable of generating safety-critical scenarios with the proposed game-\ntheoretic guided sampling framework in various scenes. Distinct from previous\nworks [5, 47, 67], which sample adversarial behaviors given fixed trajectories of\nvehicles being pursued, our guidance strategy actively optimizes the actions of\nboth the purser and evader and leads to highly-interactive scenarios. Our pro-\nposed method facilitates the generation of highly realistic scenarios, especially\nwith regard to adversarial behavior, by ensuring it remains proportionately ad-\nversarial and responsive to the ego vehicle\u2019s actions. This strategy overcomes\nthe shortcomings of previous methods, which tend to generate unrealistically\naggressive adversarial tactics and are less useful in stress-testing AVs. Detailed\nformulations can be found in the Supplementary Materials.\nVersatile Scene-Consistent Scenario Generation with Diffusion\n13\nAvoider\nAdversarial Pursuer\nOther Agent\nSpeed [m/s]\nFig. 5: Results of game theoretical guided generation. Top: The adversarial pursuer\nmerges in front of the evader, performs a brake check, and attempts to cause a rear-end\ncrash. The evader immediately switches its lane and avoids the collision. Bottom: The\nadversarial pursuer merges aggressively to the adjacent lane, and the evader yields to\nthe pursuer by slowing down.\nAblation Studies. We investigate the factors that influence the training of the\ndiffusion policy, focusing on the number of diffusion steps and the integration of\na prior predictor in a multi-task learning framework. We train the denoiser under\nvarious settings and test the performance of the learned policy across identical\ntesting scenarios. For the evaluation of diffusion steps, we only train the denoiser\nand fix the weights of the encoder, which are obtained from the same trained\nmodel. For evaluating the influence of the prior predictor during training, the\nnumber of diffusion steps is fixed at 10. The results are presented in Tab. 4.\nTable 4: Ablation Results on the Diffusion Policy\nDiffusion\nCollision [%] \u2193Off-road [%]\u2193Wrong-way [%] \u2193\nADE [m]\nGuidance\nCollision [%]\nstep=1\n1.95\u00b10.03\n1.24\u00b10.02\n0.67\u00b10.03\n0.823\u00b10.002\n\u2013\nstep=5\n2.18\u00b10.16\n1.23\u00b10.04\n0.64\u00b10.06\n0.961\u00b10.004\n1.30\nstep=10\n2.47\u00b10.09\n1.21\u00b10.14\n0.57\u00b10.02\n1.010\u00b10.007\n1.11\nstep=25\n2.68\u00b10.15\n1.52\u00b10.06\n0.64\u00b10.09\n1.061\u00b10.010\n1.27\nstep=50\n3.60\u00b10.45\n1.67\u00b10.17\n0.67\u00b10.01\n1.123\u00b10.012\n1.51\nstep=100\n3.68\u00b10.63\n1.81\u00b10.24\n0.69\u00b10.05\n1.163\u00b10.011\n1.50\nw/o predictor\n3.01\u00b10.14\n1.96\u00b10.09\n0.90\u00b10.10\n1.218\u00b10.007\n1.41\nThe results suggest that increasing the number of diffusion steps has a negative\neffect on simulation performance. This is likely because, different from image\ngeneration, behavior generation is strongly conditioned on the scene context,\n14\nZ. Huang et al.\nAvoider\nAdversarial Pursuer\nOther Agent\nSpeed [m/s]\nFig. 6: Results of game-guided safety-critical scenario generation. Top: The adversarial\npursuer suddenly pulls out and causes a rear-end crash with the evader. Bottom:\nStochastic sampling allows us to generate diverse safety-critical behaviors. The pursuer\nforces the green vehicle to drive to the opposite side of the road while causing a collision.\nwhich means that small variance gaps between steps are not necessary for refin-\ning predictions. Moreover, adding more diffusion steps can introduce excessive\nrandomness into the sampling process, leading to a decrease in behavior modeling\nperformance. The result indicates that using 10 steps of diffusion can balance\nthe performance of normal and guided sampling, which can also significantly\nimprove the inference speed. Regarding the effect of the behavior predictor, the\nresult reveals that it benefits the diffusion policy by improving the learning of\nthe encoder part of the model.\n6\nConclusions\nIn this paper, we introduce a scene-consistent scenario optimizer leveraging the\ndiffusion model, called Versatile Behavior Diffusion. This model integrates a de-\nnoiser for diffusion inference, which accounts for the joint futures of all agents,\nas well as a behavior predictor that provides the prior distribution of the agents\u2019\nmulti-modal trajectories. The experiment results demonstrate that our model\nis capable of generating diverse, realistic, and interactive scenarios, achieving\nstate-of-the-art performance on the Waymo Sim Agents benchmark. Moreover,\nwe show the model\u2019s versatility in various tasks employing different structured\nguidance strategies, such as model-based optimization objectives, prior-based\nscene editing or conditional generation, and game-theoretical guidance for craft-\ning safety-critical scenarios. Future research could aim to enhance the model\u2019s\nruntime efficiency and apply it to AV planning tests or replace handcraft guid-\nance objective design and heuristic prior selection with LLMs through proper\ngrounding to facilitate automated scenario generation.\nVersatile Scene-Consistent Scenario Generation with Diffusion\n15\nReferences\n1. Anderson, B.D.: Reverse-time diffusion equation models. Stochastic Processes and\ntheir Applications 12(3), 313\u2013326 (1982) 31\n2. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,\nPan, Y., Baldan, G., Beijbom, O.: nuscenes: A multimodal dataset for autonomous\ndriving. In: Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition. pp. 11621\u201311631 (2020) 1\n3. Cao, Y., Xiao, C., Anandkumar, A., Xu, D., Pavone, M.: Advdo: Realistic ad-\nversarial attacks for trajectory prediction. In: European Conference on Computer\nVision. pp. 36\u201352. Springer (2022) 4\n4. Chang, W.J., Hu, Y., Li, C., Zhan, W., Tomizuka, M.: Analyzing and enhancing\nclosed-loop stability in reactive simulation. In: 2022 IEEE 25th International Con-\nference on Intelligent Transportation Systems (ITSC). pp. 3665\u20133672. IEEE (2022)\n9\n5. Chang, W.J., Pittaluga, F., Tomizuka, M., Zhan, W., Chandraker, M.: Controllable\nsafety-critical closed-loop traffic simulation via guided diffusion. arXiv preprint\narXiv:2401.00391 (2023) 2, 4, 12\n6. Chang, W.J., Tang, C., Li, C., Hu, Y., Tomizuka, M., Zhan, W.: Editing driver\ncharacter: Socially-controllable behavior generation for interactive traffic simula-\ntion. arXiv preprint arXiv:2303.13830 (2023) 3\n7. Chen, Y., Ivanovic, B., Pavone, M.: Scept: Scene-consistent, policy-based trajec-\ntory predictions for planning. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition (CVPR). pp. 17103\u201317112 (June 2022)\n2\n8. Chi, C., Feng, S., Du, Y., Xu, Z., Cousineau, E., Burchfiel, B., Song, S.: Dif-\nfusion policy: Visuomotor policy learning via action diffusion. arXiv preprint\narXiv:2303.04137 (2023) 5, 30\n9. Chiu, H.k., Smith, S.F.: Collision avoidance detour for multi-agent trajectory fore-\ncasting. arXiv preprint arXiv:2306.11638 (2023) 10\n10. Choi, Y., Mercurius, R.C., Shabestary, S.M.A., Rasouli, A.: Dice: Diverse diffu-\nsion model with scoring for trajectory prediction. arXiv preprint arXiv:2310.14570\n(2023) 4\n11. Dhariwal, P., Nichol, A.: Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems 34, 8780\u20138794 (2021) 6, 7, 8\n12. Diehl, C., Klosek, T., Krueger, M., Murzyn, N., Osterburg, T., Bertram, T.:\nEnergy-based potential games for joint motion forecasting and control. In: 7th\nAnnual Conference on Robot Learning (2023) 2\n13. Du, Y., Mordatch, I.: Implicit generation and modeling with energy based models.\nAdvances in Neural Information Processing Systems 32 (2019) 5, 6, 30\n14. Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure\nand content-guided video synthesis with diffusion models. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision. pp. 7346\u20137356 (2023)\n4\n15. Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai, Y., Sapp,\nB., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V.,\nMcCauley, A., Shlens, J., Anguelov, D.: Large scale interactive motion forecasting\nfor autonomous driving: The waymo open motion dataset. In: Proceedings of the\nIEEE/CVF International Conference on Computer Vision (ICCV). pp. 9710\u20139719\n(2021) 9\n16\nZ. Huang et al.\n16. Feng, L., Li, Q., Peng, Z., Tan, S., Zhou, B.: Trafficgen: Learning to generate\ndiverse and realistic traffic scenarios. In: 2023 IEEE International Conference on\nRobotics and Automation (ICRA). pp. 3567\u20133575. IEEE (2023) 2, 4\n17. Fiez, T., Ratliff, L., Mazumdar, E., Faulkner, E., Narang, A.: Global convergence\nto local minmax equilibrium in classes of nonconvex zero-sum games. Advances in\nNeural Information Processing Systems 34, 29049\u201329063 (2021) 9, 26\n18. Finn, C., Christiano, P., Abbeel, P., Levine, S.: A connection between genera-\ntive adversarial networks, inverse reinforcement learning, and energy-based models.\narXiv preprint arXiv:1611.03852 (2016) 5\n19. Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X.,\nWang, Y., Chen, X., et al.: Waymax: An accelerated, data-driven simulator for\nlarge-scale autonomous driving research. arXiv preprint arXiv:2310.08710 (2023)\n9, 10\n20. Guo, Z., Gao, X., Zhou, J., Cai, X., Shi, B.: Scenedm: Scene-level multi-\nagent trajectory generation with consistent diffusion models. arXiv preprint\narXiv:2311.15736 (2023) 2, 4, 10\n21. Hanselmann, N., Renz, K., Chitta, K., Bhattacharyya, A., Geiger, A.: King: Gener-\nating safety-critical driving scenarios for robust imitation via kinematics gradients.\nIn: European Conference on Computer Vision. pp. 335\u2013352. Springer (2022) 4\n22. Ho, J., Ermon, S.: Generative adversarial imitation learning. Advances in neural\ninformation processing systems 29 (2016) 5\n23. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances in\nneural information processing systems 33, 6840\u20136851 (2020) 2, 4, 6, 31, 33\n24. Huang, Z., Karkus, P., Ivanovic, B., Chen, Y., Pavone, M., Lv, C.: Dtpp: Differen-\ntiable joint conditional prediction and cost evaluation for tree policy planning in\nautonomous driving. arXiv preprint arXiv:2310.05885 (2023) 2, 22\n25. Huang, Z., Liu, H., Lv, C.: Gameformer: Game-theoretic modeling and learning\nof transformer-based interactive prediction and planning for autonomous driving.\nIn: Proceedings of the IEEE/CVF International Conference on Computer Vision\n(ICCV). pp. 3903\u20133913 (October 2023) 4\n26. Huang, Z., Liu, H., Wu, J., Lv, C.: Differentiable integrated motion prediction and\nplanning with learnable cost function for autonomous driving. IEEE transactions\non neural networks and learning systems (2023) 2\n27. Huang, Z., Mo, X., Lv, C.: Multi-modal motion prediction with transformer-\nbased neural network for autonomous driving. In: 2022 International Conference\non Robotics and Automation (ICRA). pp. 2605\u20132611. IEEE (2022) 4\n28. Huang, Z., Wu, J., Lv, C.: Driving behavior modeling using naturalistic human\ndriving data with inverse reinforcement learning. IEEE transactions on intelligent\ntransportation systems 23(8), 10239\u201310251 (2021) 2\n29. Hyv\u00e4rinen, A., Dayan, P.: Estimation of non-normalized statistical models by score\nmatching. Journal of Machine Learning Research 6(4) (2005) 5, 31\n30. Igl, M., Kim, D., Kuefler, A., Mougin, P., Shah, P., Shiarlis, K., Anguelov, D.,\nPalatucci, M., White, B., Whiteson, S.: Symphony: Learning realistic and diverse\nagents for autonomous driving simulation. In: 2022 International Conference on\nRobotics and Automation (ICRA). pp. 2445\u20132451. IEEE (2022) 2, 3\n31. Janner, M., Du, Y., Tenenbaum, J., Levine, S.: Planning with diffusion for flexible\nbehavior synthesis. In: International Conference on Machine Learning. pp. 9902\u2013\n9915. PMLR (2022) 7\n32. Jiang, C., Cornman, A., Park, C., Sapp, B., Zhou, Y., Anguelov, D., et al.: Motion-\ndiffuser: Controllable multi-agent motion prediction using diffusion. In: Proceed-\nVersatile Scene-Consistent Scenario Generation with Diffusion\n17\nings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\npp. 9644\u20139653 (2023) 2, 4, 7, 8, 11, 33\n33. Kesting, A., Treiber, M., Helbing, D.: General lane-changing model mobil for car-\nfollowing models. Transportation Research Record 1999(1), 86\u201394 (2007) 1\n34. Kong, Z., Ping, W., Huang, J., Zhao, K., Catanzaro, B.: Diffwave: A versatile\ndiffusion model for audio synthesis. In: International Conference on Learning Rep-\nresentations (2020) 4\n35. LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., Huang, F.: A tutorial on energy-\nbased learning. Predicting structured data 1(0) (2006) 5, 30\n36. Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual gen-\neration with composable diffusion models. In: European Conference on Computer\nVision. pp. 423\u2013439. Springer (2022) 5, 30\n37. Luo, C.: Understanding diffusion models: A unified perspective. arXiv preprint\narXiv:2208.11970 (2022) 6, 33\n38. Mo, X., Huang, Z., Xing, Y., Lv, C.: Multi-agent trajectory prediction with hetero-\ngeneous edge-enhanced graph attention network. IEEE Transactions on Intelligent\nTransportation Systems (2022) 4\n39. Montali, N., Lambert, J., Mougin, P., Kuefler, A., Rhinehart, N., Li, M., Gulino, C.,\nEmrich, T., Yang, Z., Whiteson, S., et al.: The waymo open sim agents challenge.\narXiv preprint arXiv:2305.12032 (2023) 1, 9, 10\n40. Nayakanti, N., Al-Rfou, R., Zhou, A., Goel, K., Refaat, K.S., Sapp, B.: Wayformer:\nMotion forecasting via simple & efficient attention networks. In: 2023 IEEE Inter-\nnational Conference on Robotics and Automation (ICRA). pp. 2980\u20132987. IEEE\n(2023) 4\n41. Ng, A.Y., Russell, S., et al.: Algorithms for inverse reinforcement learning. In: Icml.\nvol. 1, p. 2 (2000) 2\n42. Nichol, A.Q., Dhariwal, P.: Improved denoising diffusion probabilistic models. In:\nInternational Conference on Machine Learning. pp. 8162\u20138171. PMLR (2021) 32\n43. Niedoba, M., Lavington, J.W., Liu, Y., Lioutas, V., Sefas, J., Liang, X., Green, D.,\nDabiri, S., Zwartsenberg, B., Scibior, A., et al.: A diffusion-model of joint interac-\ntive navigation. In: Thirty-seventh Conference on Neural Information Processing\nSystems (2023) 4\n44. Philion, J., Peng, X.B., Fidler, S.: Trajeglish: Learning the language of driving\nscenarios. arXiv preprint arXiv:2312.04535 (2023) 3, 10\n45. Qian, C., Xiu, D., Tian, M.: The 2nd place solution for 2023 waymo open sim\nagents challenge. arXiv preprint arXiv:2306.15914 (2023) 10\n46. Rempe, D., Luo, Z., Bin Peng, X., Yuan, Y., Kitani, K., Kreis, K., Fidler, S.,\nLitany, O.: Trace and pace: Controllable pedestrian animation via guided trajectory\ndiffusion. In: Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition. pp. 13756\u201313766 (2023) 4\n47. Rempe, D., Philion, J., Guibas, L.J., Fidler, S., Litany, O.: Generating useful\naccident-prone driving scenarios via a learned traffic prior. In: Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 17305\u2013\n17315 (2022) 3, 12\n48. Rosbach, S., James, V., Gro\u00dfjohann, S., Homoceanu, S., Roth, S.: Driving with\nstyle: Inverse reinforcement learning in general-purpose planning for automated\ndriving. In: 2019 IEEE/RSJ International Conference on Intelligent Robots and\nSystems (IROS). pp. 2658\u20132665. IEEE (2019) 2\n49. Sadat, S., Buhmann, J., Bradely, D., Hilliges, O., Weber, R.M.: Cads: Unleash-\ning the diversity of diffusion models through condition-annealed sampling. arXiv\npreprint arXiv:2310.17347 (2023) 8\n18\nZ. Huang et al.\n50. Shi, S., Jiang, L., Dai, D., Schiele, B.: Mtr++: Multi-agent motion prediction\nwith symmetric scene modeling and guided intention querying. arXiv preprint\narXiv:2306.17770 (2023) 3, 4, 7\n51. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-\nvised learning using nonequilibrium thermodynamics. In: International conference\non machine learning. pp. 2256\u20132265. PMLR (2015) 2, 4\n52. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. arXiv preprint\narXiv:2010.02502 (2020) 4, 6\n53. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data\ndistribution. Advances in neural information processing systems 32 (2019) 2, 4, 5,\n31\n54. Song, Y., Garg, S., Shi, J., Ermon, S.: Sliced score matching: A scalable approach to\ndensity and score estimation. In: Uncertainty in Artificial Intelligence. pp. 574\u2013584.\nPMLR (2020) 5, 6, 31\n55. Song, Y., Kingma, D.P.: How to train your energy-based models. arXiv preprint\narXiv:2101.03288 (2021) 5, 30\n56. Song, Y., Sohl-Dickstein, J., Kingma, D.P., Kumar, A., Ermon, S., Poole, B.: Score-\nbased generative modeling through stochastic differential equations. arXiv preprint\narXiv:2011.13456 (2020) 2, 4, 6, 31, 33\n57. Sun, S., Gu, Z., Sun, T., Sun, J., Yuan, C., Han, Y., Li, D., Ang Jr, M.H.:\nDrivescenegen: Generating diverse and realistic driving scenarios from scratch.\narXiv preprint arXiv:2309.14685 (2023) 2, 4\n58. Suo, S., Regalado, S., Casas, S., Urtasun, R.: Trafficsim: Learning to simulate\nrealistic multi-agent behaviors. In: Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition. pp. 10400\u201310409 (2021) 3\n59. Treiber, M., Hennecke, A., Helbing, D.: Congested traffic states in empirical ob-\nservations and microscopic simulations. Physical review E 62(2), 1805 (2000) 1\n60. Vincent, P.: A connection between score matching and denoising autoencoders.\nNeural computation 23(7), 1661\u20131674 (2011) 5, 31\n61. Wang, T., Dhiman, V., Atanasov, N.: Inverse reinforcement learning for au-\ntonomous navigation via differentiable semantic mapping and planning. Au-\ntonomous Robots 47(6), 809\u2013830 (2023) 2\n62. Wang, Y., Zhao, T., Yi, F.: Multiverse transformer: 1st place solution for waymo\nopen sim agents challenge 2023. arXiv preprint arXiv:2306.11868 (2023) 10\n63. Wilson, B., Qi, W., Agarwal, T., Lambert, J., Singh, J., Khandelwal, S., Pan, B.,\nKumar, R., Hartnett, A., Pontes, J.K., et al.: Argoverse 2: Next generation datasets\nfor self-driving perception and forecasting. arXiv preprint arXiv:2301.00493 (2023)\n1\n64. Xu, C., Zhao, D., Sangiovanni-Vincentelli, A., Li, B.: Diffscene: Diffusion-based\nsafety-critical scenario generation for autonomous vehicles. In: The Second Work-\nshop on New Frontiers in Adversarial Machine Learning (2023) 2, 4\n65. Xu, D., Chen, Y., Ivanovic, B., Pavone, M.: Bits: Bi-level imitation for traffic\nsimulation. In: 2023 IEEE International Conference on Robotics and Automation\n(ICRA). pp. 2929\u20132936. IEEE (2023) 3\n66. Zhang, C., Tu, J., Zhang, L., Wong, K., Suo, S., Urtasun, R.: Learning realistic\ntraffic agents in closed-loop. In: 7th Annual Conference on Robot Learning (2023)\n2\n67. Zhang, L., Peng, Z., Li, Q., Zhou, B.: Cat: Closed-loop adversarial training for\nsafe end-to-end driving. In: Conference on Robot Learning. pp. 2357\u20132372. PMLR\n(2023) 4, 10, 12\nVersatile Scene-Consistent Scenario Generation with Diffusion\n19\n68. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: Proceedings of the IEEE/CVF International Conference on\nComputer Vision. pp. 3836\u20133847 (2023) 4\n69. Zhang, Z., Liniger, A., Dai, D., Yu, F., Van Gool, L.: Trafficbots: Towards world\nmodels for autonomous driving simulation and motion prediction. arXiv preprint\narXiv:2303.04116 (2023) 2, 4\n70. Zhong, Z., Rempe, D., Chen, Y., Ivanovic, B., Cao, Y., Xu, D., Pavone, M., Ray,\nB.: Language-guided traffic simulation via scene-level diffusion. arXiv preprint\narXiv:2306.06344 (2023) 2, 4, 7, 8\n71. Zhong, Z., Rempe, D., Xu, D., Chen, Y., Veer, S., Che, T., Ray, B., Pavone,\nM.: Guided conditional diffusion for controllable traffic simulation. In: 2023 IEEE\nInternational Conference on Robotics and Automation (ICRA). pp. 3560\u20133566.\nIEEE (2023) 2, 4, 7, 8, 11, 33\n72. Zhou, Z., Wang, J., Li, Y.H., Huang, Y.K.: Query-centric trajectory prediction.\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition. pp. 17863\u201317873 (2023) 3, 4\n73. Ziebart, B.D., Maas, A.L., Bagnell, J.A., Dey, A.K., et al.: Maximum entropy\ninverse reinforcement learning. In: Aaai. vol. 8, pp. 1433\u20131438. Chicago, IL, USA\n(2008) 2, 5, 30\n20\nZ. Huang et al.\nSupplementary Material\nA\nVersatile Behavior Diffusion Architecture\nA.1\nSystem Dynamics\nThe diffusion model operates in the action space, and we assume that there is a\ndynamic function that can translate actions to physical states xt+1 = f(xt, ut).\nA unicycle dynamics function is utilized to transform agent actions into states,\nwhich is adopted to approximate the dynamics of all agent types, including\nvehicles, pedestrians, and cyclists. The current state of an agent is defined by\nits global coordinates (x, y), yaw angle \u03c8, and velocities vx, vy. Given the action\nof an agent, including acceleration \u02d9v and yaw rate \u02d9\u03c8, and the time length for\none step \u2206t, the next-step state of the agent is calculated using the following\nforward dynamics f, expressed as:\n  \\ b eg i n { split} x(t+1) & = x_t + {v_x}(t) \\Delta t, \\\ry(t+1) = y_t + {v_y}(t) \\Delta t, \\\\ \\psi (t+1) &= \\psi (t) + \\dot \\psi \\Delta t, \\\rv(t+1) = \\sqrt {{v_x}(t)^2 + {v_y}(t)^2} + \\dot v \\Delta t, \\\\ {v_x}(t+1) &= v(t+1) \\cos \\psi (t+1), \\ {v_y}(t+1) = v(t+1) \\sin \\psi (t+1). \\end {split} x\n(S1)\nSince each operation in the dynamics function is differentiable, it can be inte-\ngrated as a layer in the network to convert predicted actions into states.\nFurthermore, we employ the inverse dynamics function f \u22121 to calculate actions\nfrom ground-truth states, which is formulated as:\n  \\be g in { sp l it} \n\\d\no t v( t\n)\n &= \\frac {v(t+1) - v(t)}{\\Delta t}, \\ v(t) = \\sqrt {v_x(t)^2 + v_y(t)^2}, \\\\ \\dot \\psi (t) &= \\frac {\\psi (t+1) - \\psi (t)}{\\Delta t}. \\end {split} x\n(S2)\nA.2\nModel Structure\nScene Encoder. The encoder processes three main inputs: the agent history\ntensor ([A, Th, Da]), the map polyline tensor ([Ml, Mp, Dp]), and the traffic lights\ntensor ([Mt, Dt]). Here, Th denotes the number of historical steps, Ml the num-\nber of polylines, Mp the number of waypoints per polyline, Mt the number of\ntraffic lights, and Ml + Mt = M represents the combined count of map ele-\nments. The feature sizes for agents, polylines, and traffic lights are represented\nby Da, Dp, and Dt, respectively. The agent history tensor records each agent\u2019s\nhistorical state, including x, y coordinates, heading angle (\u03c8), velocities (vx, vy),\nand bounding box dimensions (l, w, h), along with the agent type. Each map\npolyline, comprising Mp = 30 waypoints, includes attributes like x, y coordi-\nnates, direction angle, the traffic light state controlling the lane, and lane type.\nThe traffic lights tensor encompasses the x, y coordinates of stop points and the\nstate of each traffic light. Before encoding, positional attributes of all elements\nVersatile Scene-Consistent Scenario Generation with Diffusion\n21\nare converted into their local coordinate systems; for agents, the reference point\nis their last recorded state, and for map polylines, it is the location of the first\nwaypoint. We choose A = 32 agents, Th = 11 historical steps, Ml = 256 map\npolylines, and Mt = 16 traffic lights.\nWe first encode the agent history tensor, utilizing a shared GRU network to\nproduce a tensor of shape [A, D], which is then combined with the agent type\nembedding. For map polylines, an MLP is employed for encoding, resulting in\na tensor of shape [Ml, Mp, D]. This is followed by max-pooling along the way-\npoint axis to produce a tensor of shape [Ml, D]. For traffic lights, we only encode\ntheir light status using an MLP, yielding a tensor of shape [Mt, D]. These ten-\nsors are then concatenated to form the initial scene encoding tensor with shape\n[A + M, D]. The initial scene encoding is further processed using query-centric\nTransformer layers to symmetrically encode the interrelationships among scene\ncomponents. In this approach, each scene element is translated into its local\ncoordinate system and encoded with query-centric features, and the relative po-\nsition of each pair of scene elements is calculated and encoded as edge attributes.\nFor example, for elements i and j, the relative position \u2206ij = [\u2206x, \u2206y, \u2206heading]\nis computed and then encoded into the edge attribute using an MLP, result-\ning in relation encoding tensor eij = MLP(\u2206ij). The query-centric attention\nmechanism for a query element qi operates as follows:\n  \\smal l QC A( Q ^i, K, \nV\n, \n\\\nm\nat\nhb f  {e}\n) = \\t\nex t  {\nso f tmax\n} \\lef\nt\n ( \\frac {\\mathbf {q}^i}{\\sqrt {D}} \\left [ \\left \\{ \\mathbf {k}^j + \\mathbf {e}^{ij} \\right \\}_{j \\in \\Omega (j)} \\right ]^T \\right ) \\left ( \\left \\{ \\mathbf {v}^{j} + \\mathbf {e}^{ij} \\right \\}_{j \\in \\Omega (j)} \\right ), \n(S3)\nwhere kj, vj represent the key and value elements respectively, each contain-\ning relevant element-centric information, and j \u2208\u2126(j) indicates the index of\nother tokens. Other standard operations in Transformers, such as multi-head at-\ntention, feed-forward networks, and layer normalization, remain unchanged. In\npractice, batch operations can be applied to implement the multi-head query-\ncentric attention mechanism efficiently. The encoder consists of 6 query-centric\nTransformer layers to process the initial scene encoding, the embedding dimen-\nsion D = 256, and the final output tensor retains the same shape as [A + M, D].\nDenoiser. The denoiser part processes three types of input: noised actions\n([A, Tf, 2]) derived from the ground-truth state and action trajectories, the noise\nlevel, and the scene encoding tensor. Each agent\u2019s action at every timestep\na = [ \u02d9v, \u02d9\u03c8]T consists of acceleration and yaw rate, while the state comprises coor-\ndinates, heading, and velocity (x, y, \u03c8, v). Here, actions are reduced to a shorter\nlength Tf from T, by replicating the same action over multiple timesteps, denoted\nas Ta. The noise level is encoded via an embedding layer to a tensor of shape\n[1, 1, D]. Noised actions are converted into noisy states using a forward dynamics\nmodel and subsequently encoded into a tensor of shape [A, Tf, D] using an MLP.\nThe encoded noisy states are combined with the noise level embedding and a\ntemporal embedding ([1, Tf, D]) to create the initial trajectory embedding. For\nthe denoising process, two decoding blocks, each comprising two Transformer\ndecoder layers, are applied to predict clean actions. Within the decoding block,\n22\nZ. Huang et al.\na self-attention Transformer module is employed to model the joint distribution\nof future plans across agents. To maintain closed-loop rollout causality, a causal\nrelationship mask [24] is used in the self-attention module, which ensures that\ninformation from future timesteps cannot be utilized at the current timestep.\nFurthermore, a cross-attention Transformer module is used to model the scene-\nconditional distribution, by relating the noisy trajectories to the encoded scene\nconditions. Since the elements are encoded in a query-centric manner, the decod-\ning layers still require relative positional information between elements, which\ncan be obtained from the encoder.\nFollowing the Transformer decoding stage, the resulting trajectory embedding\nis fed into an MLP to decode the clean actions tensor of shape [A, Tf, 2]. Sub-\nsequently, clean states ([A, T, 3], encompassing x, y, \u03c8) are deduced from these\npredicted actions using a differentiable dynamics model. In this work, we choose\nT = 80 and Ta = 2, resulting in Tf = 40 and thus significantly reducing compu-\ntational demands while maintaining high accuracy.\nBehavior predictor. The behavior predictor generates the marginal distribu-\ntions of possible behaviors for agents by directly decoding from the encoded\nscene conditions. To accurately predict the probabilities of possible goals, the\npredictor takes as input the static anchors for agents in local coordinates with\nshape [A, Mo, 2] as the modality query inputs. The anchors contain Mo = 64\ntypical x, y coordinates at T = 80 extracted from data using the K-means algo-\nrithm, and vary across different agent types such as vehicles, pedestrians, and\ncyclists. We utilize an MLP encoder to encode these anchors into a tensor of\nshape [A, Mo, 256]. This encoding is then combined with the agent encoding of\nshape [A, 1, 256], to form an initial query tensor with dimensions [A, Mo, 256].\nThen we employ four cross-attention Transformer layers where relative relation\nencoding is still used in the attention mechanism. The predictor iteratively re-\nfines its predictions through several decoding layers and finally, an MLP decoding\nhead is added to decode the possible action sequences for all agents, resulting\nin a tensor of [A, Mo, Tf, 2]. These action trajectories are transformed into state\ntrajectories of shape [A, Mo, T, 4] using the same differentiable dynamics model,\nand each waypoint in the trajectory contains the state (x, y, \u03c8, v). Another MLP\nlayer decodes the embedding after the Transformer layers to derive marginal\nscores (probabilities) of these predicted trajectories, with shape [A, Mo].\nA.3\nModel Training\nPredictor. The training of the predictor follows the multi-trajectory-prediction\n(MTP) loss setting. This involves selecting the best-predicted trajectories and\ncomputing the loss relative to the ground-truth trajectories. To determine the\nbest-predicted indices for an agent, the following criterion is applied:\n  m\n^\n* =  \\be gin {cases} \\arg \\min _i ||ac^i - x_T||, & \\text {if } x_T \\text { is valid}, \\\\ \\arg \\min _i || \\sum _t ({\\hat x}_t^i - x_t) ||, & \\text {otherwise}, \\end {cases} c\n(S4)\nVersatile Scene-Consistent Scenario Generation with Diffusion\n23\nwhere aci is the static anchor point, xt is the ground-truth point of the trajectory,\nand \u02c6xi\nt is the predicted trajectory point. This means that if the ground-truth\ntrajectory endpoint is invalid, the predicted trajectory with the smallest average\ndisplacement error is selected; otherwise, the trajectory corresponding to the\nclosest anchor point is selected.\nSubsequently, trajectories are chosen from the multi-modal predictions based on\nthe indices m\u2217, and the Smooth L1 loss is computed between these selected tra-\njectories and the ground-truth trajectories. For the training of the scoring head,\ncross-entropy loss is utilized, comparing the predicted logits with the given in-\ndices. Note that this loss function is computed marginally, and time steps lacking\nground-truth data or invalid agents are excluded from the loss calculation.\nDenoiser. The denoiser is trained to recover the clean trajectories under various\nnoise levels. At each training step, noise level k and Gaussian noise \u03f5 are sampled\nand applied to corrupt the ground-truth trajectories. The denoiser is optimized\nto predict the denoised trajectories from the corrupted trajectories. Since the\nmodel predicts scene-level joint trajectories, all agent trajectories are affected\nby the same noise level. The training procedure of the denoiser is described in\nAlgorithm 1.\nModel. The total loss function for the multi-task learning model is formulated\nas:\n  \\ma t hcal {L} = \\mathcal {L}_{\\denoiser } + \\gamma \\mathcal {L}_{\\predictor }, \n(S5)\nwhere \u03b3 is a hyperparameter to balance the importance of tasks.\nThe hyperparameters used in the total loss function is \u03b3 = 0.5, and \u03b2 = 0.05 is\nused in the predictor loss. The model is trained using an AdamW optimizer with\na weight decay of 0.01. The initial learning rate is set at 0.0002 and decays by\n0.02 every 1, 000 training steps, and a linear warm-up is employed for the first\n1, 000 steps. The total number of epochs for training is 16. Gradient clipping\nis implemented with a norm limit set to 1.0. The training of the model utilizes\nBFloat16 Mixed precision and is executed on four NVIDIA A100 GPUs, with a\nbatch size of 14 per GPU.\nB\nGuided Scenario Generation\nThe guided diffusion algorithm using a score function is illustrated in Algo-\nrithm 2. In practice, multiple gradient steps are employed to more effectively\nperturb the predicted means. The score function Jy varies with the structure y,\nwhich can be defined as the goal for individual agents or a soft constraint, such\nas obeying the speed limit or avoiding collisions.\nB.1\nPrior Guidance\nConsider a scenario where we need to specify desired behaviors for Na agents. We\ncan utilize the predicted behavior priors from the VBD model to heuristically\n24\nZ. Huang et al.\nAlgorithm 1 Training of denoiser\nRequire: Denoiser D\u03b8, dataset D, denoising steps K, dynamics function f, inverse\ndynamics function f \u22121\n1: for each training iteration do\n2:\nx, c \u223cD\n\\triangleright Sample from dataset\n3:\nGet action trajectory: u = f \u22121(x)\n4:\nk \u223cU(0, K), \u03f5 \u223cN(0, I)\n\\triangleright Sample noise level and Gaussian noise\n5:\nAdd noise to ground-truth: \u02dcuk = \u221a\u00af\u03b1ku + \u221a1 \u2212\u00af\u03b1k\u03f5\n6:\nPredict denoised trajectory: \u02c6u = D\u03b8(\u02dcuk, k, c, f), \u02c6x = f(\u02c6u)\n7:\nCompute loss: LD\u03b8 = SL1(\u02c6x \u2212x)\n\\triangleright Use smooth L1 loss\n8:\nUpdate denoiser parameters \u03b8\n9: end for\nAlgorithm 2 Guided diffusion with score function\nRequire: Denoiser D\u03b8, score function Jy, diffusion steps K, gradient steps Ng, scaling\nparameter \u03b1, standard deviation \u03c3k\n1: \u02dcuK \u223cN(0, I)\n\\triangleright Sample initial trajectory\n2: for k \u2190K to 1 do\n3:\n\u02c6u \u2190D\u03b8(\u02dcuk, k, c)\n\\triangleright Predict denoised control sequence\n4:\n\u02dc\u00b5k \u2190\n\u221a\u03b1k(1\u2212\u00af\u03b1k\u22121)\n1\u2212\u00af\u03b1k\n\u02dcuk +\n\u221a\n\u00af\u03b1k\u22121\u03b2k\n1\u2212\u00af\u03b1k\n\u02c6u\n\\triangleright Calculate unguided posterior \u02dc\u00b5k\n5:\nfor i \u21901 to Ng do\n6:\n\u02dc\u00b5k \u2190\u02dc\u00b5k + \u03b1\u03c3k\u2207\u02dc\u00b5kJy(D\u03b8(\u02dc\u00b5k))\n\\triangleright Guidance gradient step\n7:\nend for\n8:\n\u02dcuk\u22121 \u223cN(\u02dc\u00b5k, \u03c32\nkI)\n\\triangleright Sample previous-step noised control sequence\n9: end for\n10: Return: Final control sequence u \u2190\u02dcu0\nVersatile Scene-Consistent Scenario Generation with Diffusion\n25\ndetermine the target behaviors or goals for each target agent, represented by\n{gi}i=1:Na. Without sophisticated cost and feature design, the score function\nbased on behavior priors is significantly simplified as follows:\n  \\ma t h\nc\nal {J}\n_{goal }  =\n -\\sum _{i=1:N_a} \\mathcal {SL}_1 (g^{i} - x_T^{i}), \n(S6)\nwhere SL1 denotes the Smooth L1 loss, xi\nT is the state of an agent derived from\nactions using a differentiable dynamic function, and T is the planning horizon.\nThe other agents in the scene will not be directly influenced by the guidance.\nB.2\nCost Guidance\nAnother form of guidance is differentiable cost functions, which can be used to\nfurther improve the generation quality, such as collision avoidance and staying\non-road. The formulation of these cost functions is outlined as follows. Specifi-\ncally, the collision avoidance cost function is formulated as:\n  \\mathc a\nl\n \n{J}\n_\n{\nove\nrlap} = \\sum _{t = 1}^T \\sum _{i,j}^A d_{ij}(\\traj _t) \\mathds 1(d_{ij}(\\traj _t) < \\epsilon _d), \n(S7)\nwhere dij represents the Minkowski distance between the footrpints of agents i\nand j at time t. The parameter \u03f5d is the threshold for defining potential collision.\nThe on-road guidance function is designed to prevent agents from straying off\ndrivable roads. The on-road cost function is formulated as:\n  \\math c a\nl\n \n{J}\n_\n{\no\nnroa\nd\n} = -\n \\\ns\num _{t=1}^T \\sum _i^A \\text {relu} \\left ( d_r(\\traj _t^i) \\right ), \n(S8)\nwhere dr denotes the signed distance between the bounding box of an agent and\nthe nearest road edge. A negative distance indicates that the agent\u2019s position is\non the road, while a positive distance suggests off-road. It is important to note\nthat this cost is applied only to vehicles that are not off-road at the initial step.\nB.3\nGame-based Safety-critical Guidance\nTo generate interactive safety-critical scenarios, we propose a simple modifica-\ntion to the original guided diffusion by incorporating a gradient descent\u2013ascent\n(GDA) algorithm. Specifically, we model the two-agent interaction as a map-\nconstrained pursuit-evasion game. The pursuer aims to cause a collision with\nthe evader and stay within the road boundary. On the other hand, the evader\nattempts to avoid the collision while maintaining itself on the road. The main\nalgorithm for game-theoretic safety-critical scenario generation is presented in\nAlgorithm 3. In practice, we utilize the time-scale separation from \u03c4-GDA to\n26\nZ. Huang et al.\nguarantee local convergence to a stable Minimax equilibrium of the pursuit-\nevasion game [17]. We update the pursuer more frequently than the evader,\nproviding information advantage to the adversarial agents. In addition, we can\nfurther enhance the realism of the scenario with optional gradient masks Me and\nMp, which allow us to adjust the adversity of the pursuer and the responsiveness\nof the evader by only performing gradient updates on selected timesteps.\nAlgorithm 3 Game-theoretic-guided diffusion with gradient descent-ascent\nRequire: Denoiser D\u03b8, diffusion steps K, evader index e, pursuer index p, guidance\nsteps Ng, descent steps Ne, ascent steps Np, evader gradient mask Me, pursuer\ngradient mask Mp, scaling parameter \u03b1, standard deviation \u03c3k\n1: \u02dcuK \u223cN(0, I)\n\\triangleright Sample initial trajectory\n2: for k \u2190K to 1 do\n3:\n\u02c6u \u2190D\u03b8(\u02dcuk, k, c)\n\\triangleright Predict denoised control sequence\n4:\n\u02dc\u00b5k \u2190\n\u221a\u03b1k(1\u2212\u00af\u03b1k\u22121)\n1\u2212\u00af\u03b1k\n\u02dcuk +\n\u221a\n\u00af\u03b1k\u22121\u03b2k\n1\u2212\u00af\u03b1k\n\u02c6u\n\\triangleright Calculate unguided posterior \u02dc\u00b5k\n5:\nfor i \u21901 to Ng do\n6:\nfor id \u21901 to Ne do\n7:\n\u02c6x \u2190f(D\u03b8(\u02dc\u00b5k))\n8:\nJ \u2190mint de,p(\u02c6xt) + mint dr(\u02c6xe\nt)\n9:\n\u02dc\u00b5e\nk \u2190\u02dc\u00b5k \u2212\u03b1\u03c3kMe\u2207\u02dc\u00b5e\nkJ\n\\triangleright Gradient descent for evader\n10:\nend for\n11:\nfor id \u21901 to Np do\n12:\n\u02c6x \u2190f(D\u03b8(\u02dc\u00b5k))\n13:\nJ \u2190mint de,p(\u02c6xt) + mint dr(\u02c6xp\nt )\n14:\n\u02dc\u00b5p\nk \u2190\u02dc\u00b5k + \u03b1\u03c3kMp\u2207\u02dc\u00b5p\nkJ\n\\triangleright Gradient ascent for evader\n15:\nend for\n16:\nend for\n17:\n\u02dcuk\u22121 \u223cN(\u02dc\u00b5k, \u03c32\nkI)\n\\triangleright Sample previous-step noised control sequence\n18: end for\n19: Return: Final control sequence u \u2190\u02dcu0\nB.4\nPrior-based Safety-critical Guidance\nWe can also utilize prior guidance in the multi-agent diffusion policy to facili-\ntate the generation of safety-critical scenarios. The prior prediction significantly\nfacilitates the identification of potentially unsafe agents, which is very difficult\nfor existing methods, and augments the realism of the generated adversarial\nbehaviors. By using the behavior predictor in our model, we can obtain prior\ndistributions of the possible movements of surrounding agents. From this distri-\nbution, agents potentially conflicting with the ego vehicle\u2019s plans are identified,\nand these selected priors are used to guide diffusion policy to generate expected\nscenarios. The process of selecting adversarial agents is represented as follows:\n  g^ { adv } =\n \\a rg\n \n\\\nmax _{\ni,  j} \np\n_j^i \n\\ l eft\n \n[  c ol (g_j^i, g^{ego}_*) \\mathds {1}(p_j^i > \\epsilon _p) \\right ], \\ \\forall i \\neq ego, \n(S9)\nVersatile Scene-Consistent Scenario Generation with Diffusion\n27\nwhere gi\nj, pi\nj denote the trajectory and probability estimation for the prior mode\nj of agent i, and gego\n\u2217\nis the most-likely prior mode of the ego vehicle. col is a col-\nlision probability function, which decreases linearly with the temporal proximity\nto a collision, and \u03f5p is a threshold controlling the scenario\u2019s possibility.\nThe core idea of this strategy is that we select the highest posterior probability of\nother agents\u2019 behaviors that conflict with the ego agent\u2019s normal driving behav-\nior. The selected adversarial behaviors are then fed into the diffusion policy as\nguidance inputs. Meanwhile, the behaviors of other agents, including the ego ve-\nhicle, are controlled by the policy without guidance to respond to that situation.\nNote that utilizing prior guidance in the diffusion policy yields more realistic\nadversarial behavior compared to direct trajectory rollout, as well as robustness\nagainst wrongly selected priors that are less likely in real-world conditions. The\nproposed prior guidance strategy is illustrated in Fig. S1.\n(1)\n(2)\n(3)\nEgo Agent\nSim Agent\nAdv Agent\nEgo Trajectory\nAdv Trajectory\np=0.1\np=0.1\np=0.001\np=0.01\nPred Trajectory\nFig. S1: Illustration of prior-guided safety-critical scenario generation. (1) The behav-\nior predictor generates multi-modal trajectories for each sim agent; (2) From the prior\ndistribution, select the agent behavior with the highest probability of conflicting with\nthe normal ego plan; (3) Generate the expected scenario using the diffusion policy.\nC\nExperiment Setup and Additional Results\nC.1\nSim Agents\nWe evaluate the performance of both diffusion policy and behavior predictor\nin the Waymo Sim Agents benchmark. The testing of the behavior prediction\nmodel employs an open-loop planner, which means that the model directly gen-\nerates 8-second multi-agent trajectories and these trajectories are rolled out in\nthe Waymax simulator. Although this setting violates the closed-loop testing\nprotocol of the Sim Agents benchmark, it serves as a useful indicator of our\nmodel\u2019s ability to accurately generate behavior distributions for multiple agents.\nFor the diffusion policy, we directly roll out the policy\u2019s output in the simulator\nwithout replanning, primarily due to time constraints, as the benchmark requires\ntesting in a large number of scenarios. It is important to note that despite the\nabsence of replanning, the diffusion policy adheres to the benchmark\u2019s closed-\nloop protocol, because of the incorporation of a causal relationship mask in the\nattention-based decoding process. The decoding of an agent\u2019s state at a given\ntimestep relies only on the static map and the states of other agents in preceding\ntimesteps, without any sharing of information about their future intentions.\n28\nZ. Huang et al.\nDuring testing, we only let the policy/model control the first 32 agents in the\nscene, while the remaining agents follow a constant velocity policy. For the eval-\nuation of behavior priors, the model first generates marginal behavior distribu-\ntions for the agents. Subsequently, we sample 32 times from these distributions\nto obtain 32 different scenario rollouts. For the diffusion policy, a batch of Gaus-\nsian noise is directly sampled, allowing the generation of 32 varied scenarios\nsimultaneously, thereby expediting the sampling process.\nThe results in Tab. S1 indicate that both the diffusion policy and behavior pre-\ndiction perform very well. Notably, the joint diffusion policy outperforms the\nmarginal behavior prediction model in terms of interactive score, which high-\nlights the advantage of the diffusion policy for joint behavior modeling. These\nfindings suggest that our VBD model is reliable at generating accurate marginal\nbehavior priors for agents and can facilitate realistic agent interactions through\nits joint multi-agent diffusion policy.\nTable S1: Testing Results of the VBD model on the Waymo Sim Agents Benchmark\nMethod\nRealism Meta Kinematic Interactive Map-based minADE\nVBD-Diffusion\n0.6342\n0.4212\n0.7256\n0.8200\n1.3509\nVBD-BP\n0.6315\n0.4261\n0.7177\n0.8216\n1.3400\nC.2\nEvaluation Metrics\nIn addition to the Sim Agents benchmark evaluation metrics, we employ the fol-\nlowing metrics provided by the Waymax simulation platform for the controllable\ntraffic scenario generation testing.\nOff-road. A binary metric indicates if a vehicle drives off the road, based on\nits position relative to oriented roadgraph points. If a vehicle is on the left side\nof an oriented road edge, it is considered on the road; otherwise, the vehicle is\nconsidered off-road. This metric is averaged over all valid agents in the scene.\nCollision. A binary metric identifies whether an agent has collided with another\nagent. For each pair of objects, if their 2D bounding boxes overlap in the same\ntimestep, they are considered as collision. This metric is computed as an average\nacross all valid agents in the scene.\nWrong-way. A binary metric that measures whether a vehicle deviates from\nits intended driving direction. A wrong-way movement is flagged if the vehicle\u2019s\nheading angle deviates more than 90 degrees from its closest lane direction for a\nduration exceeding 1 second. The calculation of this metric is an average across\nall valid agents in the scene.\nKinematic infeasibility. A binary metric that computes whether a transition\nstep is kinematically feasible for the vehicle. The limit of acceleration magnitude\nis empirically set to be 6 m/s2 and the steering curvature magnitude to be 0.3\nm\u22121. The metric is averaged for all valid agents in the scene.\nVersatile Scene-Consistent Scenario Generation with Diffusion\n29\nLog divergence. This metric quantifies deviation from logged behavior using\nthe average displacement error (ADE), defined as the L2 distance between an\nagent\u2019s current and logged positions at each timestep. The metric is averaged\nacross all timesteps and all valid agents in the scene.\nC.3\nGuided Scenario Generation\nFor the guided diffusion testing, we employ Ng = 5 gradient steps and set the\nstrength parameter \u03b1 = 0.1, and guidance is applied throughout all diffusion\nsteps. We limit the simulation to a maximum of 32 agents to manage compu-\ntational resources, and any additional agents present in the original scenarios,\nmostly static vehicles, are excluded. The experiments are conducted on selected\n500 9-second scenarios from the WOMD validation interactive subset. The sim-\nulation starts at 1 second and extends over a horizon of 8 seconds, and the\nsimulation model\u2019s replanning frequency is 1 Hz. We show additional results on\nguided scenario generation in video format.\nC.4\nReactive Simulation\nThe primary requirement of simulation is to ensure that the agents respond\nrealistically to the actions of the ego vehicle. To assess our model\u2019s performance,\nwe conduct reactivity tests where the ego vehicle is decoupled from the VBD\nmodel and is instead controlled by another planner. We test the reactivity of our\nmodel across 500 scenarios where the labeled self-driving car (SDC) is controlled\nby a log-playback planner or an IDM-route planner. In this experiment, only the\nhistorical movements of the SDC are provided to the model, and thus the VBD\nmodel does not control the ego vehicle but rather coordinates the behaviors of\nthe remaining agents. The results are presented in Tab. S2, which indicate that\nthe reactivity of the diffusion policy is superior compared to marginal behavior\nprediction or behavior cloning method, as evidenced by a significant reduction in\nego vehicle collisions. Employing an IDM-route planner, which may not mimic\nhuman-like driving and deviate substantially from the actual trajectory, results\nin poorer performance. Conversely, using a log-playback planner with better\nhuman likeness enables our VBD model to generate reactive behaviors to the ego\nvehicle. Visualizations of the reactive simulation outcomes with the log-playback\nplanner are available in video format.\nTable S2: Testing Results of the VBD Model on Reactive Simulation\nModel\nEgo planner\nCollision w/ ego [%] Off-road [%] Log divergence [m]\nBP\nLog-playback\n10.60\n4.49\n0.979\nBP\nIDM-route\n13.20\n5.38\n1.070\nDiffusion Log-playback\n4.80\n1.43\n1.082\nDiffusion IDM-route\n8.40\n2.26\n1.107\n30\nZ. Huang et al.\nC.5\nSafety-critical Scenario Generation\nUsing the VBD model with the proposed game guidance or unsafe prior guidance,\nwe demonstrate additional results of safety-critical scenario generation in video\nformat.\nD\nOptimization with Diffusion\nD.1\nConnection Between Score-based Model and Optimization\nIn this subsection, we provide an extended analysis to show that one interpreta-\ntion of score-based or diffusion-based generative model under imitation learning\nsetting is learning the gradient descent step of a particular optimal control solver\nbased on previous results from [8,13,36].\nConsider a dataset D with scenario triplets S := (x, u, c) sampled independently\nfrom an unknown distribution p, whose probability density function can be fac-\ntorized the as p(S) = p(u|c)p(c). In particular, we are interested in modeling\nthe conditional probability p(u|c) of the joint control sequence u w.r.t. the scene\ncontext c by p\u03b8(u|c), from which we can sample (high probability) u and imple-\nment realistic trajectories x with known dynamics f.\nUnder Maximum Entropy IRL [73] formulation, p\u03b8(u|c) can be structured as the\nBoltzmann distribution of an optimization objective:\n  \\dis t rdata ( \\c t\nrl\nseq |\\conditi on  )\\approx \\distr _\\theta (\\ctrlseq |\\condition ):=\\frac {1}{Z_\\theta }\\exp (-\\cost _\\hparam (\\traj (\\ctrlseq ), \\ctrlseq ;\\condition )), \\label {eqn_s: ebm} \n(S10)\nwhere Z\u03b8 is the partition function (normalizing factor). Eq. (S10) resembles the\nEnergy-Based Models (EBM), which can be trained through the estimation of\nmaximum likelihood [35,55].\nCorollary 1. The maximum likelihood estimation of \u03b8 can be obtained by max-\nimizing conditional log-likelihood of the dataset D:\n  \\th eta\n =\n \\argmax  _{\\hat \\theta }\\expectation _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\theta }(\\ctrlseq |\\condition )]. \\label {eqn_s: cmle} \n(S11)\nProof. Assuming we want to model the unknown data distribution p by p\u03b8,\u03d5(S) =\np\u03b8(x, u|c)p\u03d5(c). To estimate both \u03b8 and \u03d5, we can maximize the log-likelihood\nas:\n  \\\nmax  _\n{\\hat \\t heta  ,\\ h at \n\\ph i \n}\\expect ation _{\\s c ena r io \\sim \\dataset }[\\log \\distr _{\\hat \\theta ,\\hat \\phi }] = \\max _{\\hat \\theta ,\\hat \\phi }\\expectation _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\theta }(\\traj ,\\ctrlseq |\\condition )+\\log \\distr _{\\hat \\phi }(\\condition )] \n  = \n\\ma x \n_{\\hat \\ theta ,\\hat  \\ph\ni } \\e\nxpectati o n _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\theta }(\\traj ,\\ctrlseq |\\condition )]+\\max _{\\hat \\theta ,\\hat \\phi }\\expectation _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\phi }(\\condition )] \n  = \n\\m\nax _{\\ha t \\the ta }\\ e xpe\nct\nation _{ \\ scenario \\sim \\dataset }[\\log \\distr _{\\hat \\theta }(\\traj ,\\ctrlseq |\\condition )]+\\max _{\\hat \\phi }\\expectation _{\\scenario \\sim \\dataset }[\\log \\distr _{\\hat \\phi }(\\condition )] \nTherefore, we can separate the problem and learn the parameter \u03b8 of conditional\ndistribution via maximizing log-likelihood independent of learning p\u03d5(c).\n\u2293\u2294\nVersatile Scene-Consistent Scenario Generation with Diffusion\n31\nIdeally, we can train a score function s\u03b8(u|c) by score-matching [29,53,54,60] to\napproximate the gradient of log p(u|c) w.r.t. the control sequence (our random\nvariable of interest):\n  \\na bla _{ \\ ctrlseq  }  \\ log  \\distr d ata (\\ctrls eq  | \\condi\nti\non  )\\ approx \\score _\\theta (\\ctrlseq |\\condition ) := \\nabla _{\\ctrlseq } \\log \\distr _\\theta (\\ctrlseq |\\condition ) = -\\nabla _{\\ctrlseq } \\cost _\\hparam (\\traj (\\ctrlseq ),\\ctrlseq ;\\condition ) - \\cancelto {0}{\\nabla _\\ctrl \\log Z}. \\label {eqn_s: score} \n(S12)\nWith the score function, \u2207uJ\u03b8 is naturally obtained and can be directly used\nfor gradient descent. However, since the dataset contains mostly near-optimal\nscenarios, the gradient estimation in suboptimal regions of the action space (away\nfrom demonstration data) may be inaccurate. Given a context c, if we initialize\nan arbitrary u far away from the minimizer of J , errors in the score function\nmay lead to suboptimal generation. To overcome this issue, we follow [56] to\ndiffuse the control u through a forward stochastic differential equation (SDE):\n  d  \\tild e {\\ c trlseq } = h(\\tilde {\\ctrlseq }, k) dk+g(k)d\\mathbf {w}, \\label {eqn_s: SDE} \n(S13)\nwhere w is the wiener process and k \u2208[0, K] is the continuous diffusion step.\nThis process allows p to gradually transform into noised distributions pk until\nit becomes a known distribution pK = \u03c0. While the probability density function\nof pk is unknown, we can sample a noised control \u02dcu \u223cpk by first sampling\nu \u223cp and perturbing with pk(\u02dcu|u) through SDE. This allows us to train a\nstep-conditioned score function s\u03b8(\u02dcu|c, k) to approximate \u2207\u02dcu log pk(\u02dcu) by:\n  \\th eta\n =\n \\argmax _{\\hat \\theta }\n\\\nexpectat ion  _{\\scen a rio \\sim \\di\ns\ntrdata ,k\\sim \\uniform (0,K)}\\expectation _{\\tilde {\\ctrlseq }\\sim \\distrdata _k(\\cdot |\\ctrlseq )} \\left [ \\lambda (k)\\lVert \\nabla _{\\tilde {\\ctrlseq }}\\log \\distrdata _k(\\tilde {\\ctrlseq }|\\ctrlseq )-\\score _{\\hat \\theta }(\\tilde {\\ctrlseq }|\\condition , k)\\rVert \\right ], \\label {eqn_s: score_loss} \n(S14)\nwhere \u03bb(k) is a positive weighting function. At inference time, we can generate\nscenarios by first sampling \u02dcu from the known distribution \u03c0 and iterating through\nthe reverse SDE [1]:\n  d \\\nt\nilde {\\ c trlseq }=\\lef t \n[\nh( \\ tilde {\\ctrlseq }, k) - g^2(k) \\score _\\theta (\\tilde {\\ctrlseq }|\\condition , k)\\right ]dk+g(k)d\\mathbf {\\bar {w}}. \n(S15)\nTo connect the score-based model with IRL and EBM, we can view the forward\ndiffusion as uplifting original data distribution into a higher-dimensional space.\nBy injecting noise, we achieve good coverage over the entire action space in\nthe final step K so that s\u03b8(\u02dcu|c, K) are well defined for random \u02dcu. Sampling\nthrough reverse SDE can be interpreted as stochastic gradient descent towards\nhigh-probability regions with a fixed descent direction along the diffusion step,\nanalogous to the direct shooting method in optimal control. We note that at\nlow noise level k, as pk is close to the original data distribution p, s\u03b8(u|c, k) \u2248\n\u2212\u2207uJ\u03b8(x, u; c), which recovers our learning objective. Therefore, the generative\nmodeling of scenarios can be viewed as an implicit solution of IRL by learning\nthe gradient steps of trajectory optimization and solving the optimal control\nproblem through sampling.\nD.2\nDDPM: Denoising Diffusion Probabilistic Model\nOne popular method that uses the idea of learning reverse diffusion process\nto construct data is the Diffusion Denoising Probabilistic Model (DDPM) [23].\n32\nZ. Huang et al.\nThe discrete-time formulation of the forward diffusion process of DDPM can be\ndescribed as [42]:\n  q(\\t i l d e  {\\ctr ls\ne\nq\n }_\n1, \\cdots , \\tilde {\\ctrlseq }_K | {\\ctrlseq }) := \\prod _{k=1}^K q(\\tilde {\\ctrlseq }_k | \\tilde {\\ctrlseq }_{k-1}), \n(S16)\n  q(\\tilde { \\c t\nr\nlseq\n \n} _ {k} | \\t ild\ne\n {\\ctrlseq }_{k-1}) := \\mathcal {N} \\left (\\tilde {\\ctrlseq }_{k}; \\sqrt {1-\\beta _k} \\tilde {\\ctrlseq }_{k-1}, \\beta _k \\mathbf {I} \\right ), \\label {eqn: diffusion_forward} \n(S17)\nwhere \u03b2k \u2208(0, 1) is the k-th noise scale from a predefined noise scheduling, u\nis the clean action sampled from data distribution, and \u02dcuk is the noisy action\nsamples at diffusion step k. In the final step K, the data distribution approaches\nan isotropic Gaussian distribution q(\u02dcuK) \u2248N(\u02dcuK; 0, I). Let \u03b1k = 1 \u2212\u03b2k and\n\u00af\u03b1k = Qk\ni=0 \u03b1i, we can directly sample \u02dcui from data u without iterative diffusion\nvia reparameterization trick:\n  \\ t ilde \\ct r\nl\ns e q _k =  \\sqrt  \n{\n\\ a lpha _ k }\\ti lde \\ctrlseq _{k-1} +\\sqrt {1-\\alpha _k}\\epsilon _{k-1} = \\sqrt {\\bar \\alpha _k}\\ctrlseq + \\sqrt {1-\\bar \\alpha _k}\\epsilon , \\ \\epsilon \\sim \\mathcal {N}(0, \\mathbf {I}). \\label {eqn: ddpm_forward_sample} \n(S18)\nThe generation process is accomplished by learning to reverse the forward diffu-\nsion process based on context information c. The reverse diffusion process starts\nwith an isotropic Gaussian noise q(\u02dcuK) and can be expressed as follows:\n  p_\\th e t a  (\\tild e {\\ctrl\ns\ne\nq }\n_0, \\cdots ,\\ tilde {\\ctrlseq }_K | \\condition ) := q(\\tilde {\\ctrlseq }_K) \\prod _{i=1}^K p_\\theta (\\tilde {\\ctrlseq }_{k-1} | \\tilde {\\ctrlseq }_{k}, \\condition ), \n(S19)\n  p_\\theta (\\ ti ld e\n {\\ctrl seq }_{ k-1} | \\t ilde  {\n\\c\nt\nrlseq }_{k}, \\condition ) := \\mathcal {N} \\left ( \\tilde {\\ctrlseq }_{k-1} ; \\mu _\\theta (\\tilde {\\ctrlseq }_k, \\denoiser (\\tilde \\ctrlseq _k, k, \\condition )), \\sigma ^2_k \\mathbf {I} \\right ), \n(S20)\nwhere \u00b5\u03b8 calculates the posterior mean of the noise at k \u22121 step from \u02dcuk and\nDDPM denoiser output D\u03b8(\u00b7), and \u03c3k is the standard deviation according to the\nfixed noise schedule.\nSpecifically, the denoiser D\u03b8(\u00b7) estimates the clean action trajectory sample \u02c6uk\nfrom the current noisy sample \u02dcuk, according to which the mean of the previous\nnoisy sample \u02dcuk\u22121 can be derived as follows:\n  \\l\nabel {me\na n } \\\nmu _\n{k} : =  \\frac\n { \\sq\nrt {\\bar \\alpha _{k-1}} \\beta _k}{1 - \\bar \\alpha _k} {\\hat {\\ctrlseq }_k} + \\frac {\\sqrt {\\alpha _k} (1 - \\bar \\alpha _{k-1})}{1 - \\bar \\alpha _k} \\tilde {\\ctrlseq }_{k}, \n(S21)\nA cosine schedule is employed for \u00af\u03b1k, which is formulated as:\n  \\ b ar \\a\nlpha _ k = \\ f rac {f_n(k)}{f_n(0)}, \\ f_n(k) = \\cos \\left ( \\frac {k/K + s}{1 + s} \\cdot \\frac {\\pi }{2} \\right )^2, s\n(S22)\nwhere K is the total number of diffusion steps, and s = 0.008 is a small offset\nto prevent \u03b2k from being too small near k = 0. The variances \u03b2k are calculated\nas \u03b2k = 1 \u2212\n\u00af\u03b1k\n\u00af\u03b1k\u22121 , with a maximum value capped at 0.999. Once the variance\nschedule is established, the values of \u03b1k, \u03b2k, and \u00af\u03b1k can be calculated.\nVersatile Scene-Consistent Scenario Generation with Diffusion\n33\nIn the reverse diffusion process, it is often necessary to incorporate constraints\nor objectives to guide the process towards desired outcomes, denoted as y. This\nguidance is implemented by subtly altering the predicted mean of the model at\neach denoising step, similar to the CTG method [71]. Therefore, the denoising\nstep is modified to impose guidance in the form of a score function Jy as:\n  \\ t il d e \\mu _k = \\mu _k + \\alpha \\sigma _k \\nabla _{\\mu _k} \\mathcal {J}_y(\\mu _k), \n(S23)\nwhere \u03b1 is a parameter that controls the strength of the guidance.\nHowever, calculating the gradient based on the mean of noisy actions is difficult\nand inaccurate, and thus manipulating the noise mean using the noisy gradient\ncan result in errors and instability. To address this, we propose an alternative\napproach similar to the MotionDiffuser method [32]: calculating the objective\nfunction using the one-step generation result from denoiser rather than the noisy\nmean. This modification is expressed as:\n  \\ t il d e \\mu _k = \\mu _k + \\alpha \\sigma _k \\nabla _{\\tilde {\\ctrlseq }_k} \\mathcal {J}_y(\\mathcal {D}_\\theta (\\tilde {\\ctrlseq }_k)), \n(S24)\nwhere the gradient \u2207\u02dcuk is calculated with respect to the noisy actions \u02dcuk and\nnecessitates differentiation through the denoiser D.\nD.3\nConnecting DDPM with Score-Based Model\nIn previous subsections, we consider a general form of score-based model with\ncontinuous-time SDE proposed by Song et al. [56]. To draw the connection,\nDDPM is a discrete-time variance-preserving SDE (VP-SDE) with\n  f( k, x ) \n= -\\fr\nac { 1\n}\n{2}\\beta (k),\\quad g(k) = \\sqrt {\\beta (k)}. \n(S25)\nwhose proof is stated in Appendix B. of [56]. i-th diffusion step in DDPM can\nbe viewed as discrete sampling along continuous time step k.\nTo train the denoiser, Ho et al. [23] used a simplified loss to minimize difference\nbetween sampled noise \u03f5 the predicted noise \u03f5\u03b8(\u00b7) from the denoiser by:\n  \\la b el {eqn_s: ddpm_\nl\nos s } \\begi n {spli\nt } \\mathcal {L}_\\\nt\nex t  {DDPM} &\n=\n \\ mathb b {E}_{\ni \\sim [1, N], \\scenario \\sim \\dataset , {\\epsilon }\\sim \\gaussian }\\Big [\\|{\\epsilon } - {\\epsilon }_\\theta (\\tilde \\ctrlseq _i, i, \\condition )\\|^2 \\Big ] \\\\ &= \\mathbb {E}_{i \\sim [1, N], \\scenario \\sim \\dataset , {\\epsilon }\\sim \\gaussian } \\Big [\\|{\\epsilon } - {\\epsilon }_\\theta (\\sqrt {\\bar {\\alpha }_i}\\ctrlseq + \\sqrt {1 - \\bar {\\alpha }_i}{\\epsilon }, i,\\condition )\\|^2 \\Big ]. \\end {split} \n(S26)\nThe equivalence between the DDPM loss function (Eq. (S26)) and score-function\nloss (Eq. (S14)) has also shown in Section 3.2 [37] with:\n  \\nabla _{\\t i l\nd\ne \\ ctr\nls\neq _i} \\ di s t\nr\n _ i ({\\\ntilde \\ ct rlseq _i}|\\condition ) = -\\frac {1}{\\sqrt {1-\\bar \\alpha _i}}\\epsilon , \\quad \\score (\\tilde \\ctrlseq _i|\\condition , i) = -\\frac {1}{\\sqrt {1-\\bar \\alpha _i}}\\epsilon _\\theta (\\tilde \\ctrlseq _i, \\condition , i). \n(S27)\n",
    "2404.02082": "1\nWcDT: World-centric Diffusion Transformer for\nTraffic Scene Generation\nChen Yang1, Aaron Xuxiang Tian2, Dong Chen3, Tianyu Shi4\u2217, Arsalan Heydarian5\nAbstract\u2014In this paper, we introduce a novel approach for\nautonomous driving trajectory generation by harnessing the\ncomplementary strengths of diffusion probabilistic models (a.k.a.,\ndiffusion models) and transformers. Our proposed framework,\ntermed the \u201cWorld-Centric Diffusion Transformer\u201d (WcDT),\noptimizes the entire trajectory generation process, from feature\nextraction to model inference. To enhance the scene diversity\nand stochasticity, the historical trajectory data is first prepro-\ncessed and encoded into latent space using Denoising Diffusion\nProbabilistic Models (DDPM) enhanced with Diffusion with\nTransformer (DiT) blocks. Then, the latent features, historical\ntrajectories, HD map features, and historical traffic signal in-\nformation are fused with various transformer-based encoders.\nThe encoded traffic scenes are then decoded by a trajectory de-\ncoder to generate multimodal future trajectories. Comprehensive\nexperimental results show that the proposed approach exhibits\nsuperior performance in generating both realistic and diverse\ntrajectories, showing its potential for integration into automatic\ndriving simulation systems.\nIndex Terms\u2014Traffic scene generation, diffusion probabilistic\nmodels, transformer, autonomous driving.\nI. INTRODUCTION\nA\nUtonomous driving represents a cutting-edge technology\nset to revolutionize transportation, freeing drivers from\nexhausting driving and mitigating traffic congestion by en-\nabling vehicles to operate with little or no human intervention\n[1]\u2013[4]. The development of autonomous driving algorithms\noften involves a process of trial and error, as researchers\nseek to optimize performance and enhance safety [5]\u2013[7].\nHowever, the validation of autonomous vehicles (AVs) through\nreal-world testing presents significant challenges. Beyond the\nsubstantial time investment required due to the rarity of critical\nincidents, real-world testing also involves complex safety,\nregulatory, and cost considerations [8], [9].\nAs the development of autonomous driving systems (ADS)\nprogresses, simulators play a pivotal role in the design and\nevaluation process [10]. These simulators offer cost-effective\n1Chen Yang is with the Department of Computer Science and Informatics,\nCardiff University, Cardiff, UK. Email: yc19970530@gmail.com.\n2Aaron\nXuxiang\nTian\nis\nwith\nthe\nInformation\nNetworking\nInsti-\ntute, Carnegie Mellon University, Pittsburgh, PA, 15213, USA. Email:\naarontian00@gmail.com.\n3Dong Chen is with Environmental Institute & Link Lab & Computer\nScience, University of Virginia, Charlottesville, VA, 22903, USA. Email:\ndqc4vv@virginia.edu.\n4Tianyu Shi is with Transportation Research Institute, University of\nToronto. Email: ty.shi@mail.utoronto.ca.\n5Arsalan Heydarian is with Link Lab & Engineering Systems and En-\nvironment, University of Virginia, Charlottesville, VA, 22903, USA. Email:\nheydarian@virginia.edu.\n\u2217Tianyu Shi is the corresponding author.\ntools and various traffic scenarios to test driving algorithms,\nproviding controllable environments for experimentation and\nrefinement [11], [12]. To ensure the effective development of\nAVs, these simulators should be highly realistic, accurately\nreplicating real-world traffic scenarios and driver behaviors\nto enable effective translation to real-world applications [13].\nHowever, a challenge arises as current driving simulators com-\nmonly generate agent behaviors through two primary methods:\nreplaying recorded driving logs or employing heuristic-based\ncontrollers [13], [14]. These approaches may restrict the\ndiversity and unpredictability of driving behaviors observed in\nreal-world scenarios, thereby affecting the comprehensiveness\nof the testing and validation processes for ADS [15].\nRecently, multimodal motion prediction approaches [16]\u2013\n[22] have demonstrated remarkable efficacy in traffic scene\ngeneration. These methods, often leveraging efficient fusion\nstrategies or employing encoder-decoder networks with Trans-\nformer architectures, have yielded promising results, particu-\nlarly notable in competitions such as the Waymo Open Dataset\nMotion Prediction Challenge [23]. However, these methods\nface limitations in generating actions for all agents based on\ncomprehensive global information and cannot spontaneously\ngenerate varied actions [16].\nOn the other hand, there has been a notable trend toward\nleveraging generative adversarial networks (GANs) [24]\u2013[26]\nand Variational Auto-Encoders (VAEs) [27], [28] to create\ntraffic scenes for ADS. However, these methods encounter\nseveral limitations. First, the behaviors generated by these\nmodels seem plausible as they primarily mirror the distribution\nof the training data and lack diversity [13]. Additionally, the\ntraining process for GANs can be unstable due to adversarial\nlearning [15]. Moreover, most current generative approaches\ndo not effectively utilize the local smoothness of agent tra-\njectories, leading to unrealistic outcomes [15]. Finally, these\nmethods tend to concentrate on predicting future paths for\nindividual vehicles [24]\u2013[26], neglecting the comprehensive\nrepresentation of all agent types within a scene. More recently,\ndiffusion probabilistic models also referred to as diffusion\nmodels, have emerged as a promising framework for gener-\nating realistic and diverse traffic scenarios [13], [15], [29],\n[30]. These approaches conceptualize traffic scene generation\nas an inverse diffusion process by removing noise from a\nrandom distribution. However, it is important to note that these\napproaches typically require input features to be transformed\ninto Frenet coordinates centered on each agent during the data\npreprocessing stage to accurately capture the motion dynamics\nof various agents within the scene [16]. In addition, a notable\nlimitation of these approaches is their tendency to output only\narXiv:2404.02082v1  [cs.CV]  2 Apr 2024\n2\nFigure 1: World-centric model and Agent-centric model: (a) The conventional trajectory prediction model is called the \u201cAgent-\ncentric model\u201d and the methods in the Sim Agents leaderboard are based on it. (b) In contrast, our method uses position\nembedding to replace the cumbersome coordinate transformation; it also implements multi-scenario multi-agent trajectory\ngeneration, which significantly improves reasoning efficiency.\none trajectory for each agent per inference time, which can\nlead to expensive execution latency [31].\nIn this paper, we introduce an innovative framework for\ngenerating traffic scenes tailored to autonomous driving appli-\ncations, harnessing the complementary capabilities of diffusion\nmodels and transformer-based encoder-decoder frameworks.\nOur proposed framework, named the \u201cWorld-Centric Diffusion\nTransformer\u201d (WcDT), optimizes the trajectory generation\npipeline from feature extraction to model inference. A dis-\ntinctive feature of WcDT is its ability to concurrently generate\ncoherent and joint future movements for a comprehensive set\nof agents, such as vehicles, bicycles, and pedestrians, within a\nsingle inferential procedure. The primary contributions of this\nresearch can be highlighted as follows:\n\u2022 We introduce a novel paradigm for traffic scene genera-\ntion that enables the simultaneous generation of consis-\ntent and joint future movements for all involved agents\nin a single inference pass.\n\u2022 We construct a Diffusion-Transformer module that sig-\nnificantly enriches scene diversity and the stochasticity\nof agent behaviors. This architecture stands out for its\nefficiency and its holistic integration of the world state.\n\u2022 Our model sets a new benchmark for realism and diversity\nin trajectory generation, as evidenced by its performance\non the open-sourced traffic generation dataset. The devel-\noped software and demos are available in our open-source\nrepository1.\nThe remainder of this paper is organized as follows. Sec-\ntion II provides a comprehensive literature review on existing\nwork for traffic scene generation. The problem formulation and\nthe proposed WcDT framework are introduced in Section III\n1https://github.com/yangchen1997/WcDT\nwhereas experiments, results, and discussions are presented\nin Section IV. Lastly, in Section V, we conclude the paper,\nsummarize our contributions, and suggest potential insights\nfor future research.\nII. RELATED WORK\nTraffic simulators play a crucial role in evaluating the\nefficiency and effectiveness of ADS, which can be broadly\nclassified into rule-based and learning-based approaches. Rule-\nbased approaches excel in accurately capturing high-level\ntraffic characteristics through the direct encoding of traffic\nrules [32]. Despite their precision in adhering to established\nregulations, these methods tend to exhibit a lack of flexibility,\noften leading to simulations that might not fully capture the\ncomplexity and variability of real-world driving behaviors\n[13], [19]. In contrast, learning-based approaches enhance\nthe realism of traffic simulations by analyzing and applying\npatterns derived from extensive datasets of actual driving\ntrajectories [13], offering a more nuanced and adaptable rep-\nresentation of vehicular dynamics. Therefore, in this study, we\nwill focus on the learning-based approaches.\nA. Motion prediction-based methods\nRecent advancements in traffic scene generation have seen\na shift towards employing motion prediction-based method-\nologies to create multimodal traffic scenarios [16]\u2013[22], which\nleverage sophisticated algorithms to forecast the movements of\nvarious traffic participants, thereby enriching the realism and\ncomplexity of simulated environments. For instance, Multi-\npath++ [17] enhances the trajectory prediction capabilities of\nits predecessor, Multipath [16], by incorporating a context-\naware fusion technique and integrates diverse inputs reflecting\n3\nthe current state of the world to generate multimodal trajecto-\nries using Gaussian mixture models, leading to more accurate\nand varied path predictions. Trafficsim [19] capitalizes on\nrecent breakthroughs in motion forecasting by adopting an im-\nplicit latent variable model to simulate multi-agent behaviors\nwithin traffic systems.\nThe exploration of transformer-based encoder-decoder ar-\nchitectures for motion prediction underscores a pivotal devel-\nopment in the field [18], [20]\u2013[22]. Specifically, the Scene\nTransformer [18] introduces a novel approach by utilizing\na global coordinate frame to encode the interactions among\nmultiple agents, thereby facilitating the joint prediction of\nbehaviors for all agents involved. Moreover, the Motion Trans-\nformer (MTR) [20] emerges as a leading example within\nthe transformer encoder-decoder framework for multimodal\nmotion prediction by concurrently optimizing for global in-\ntention localization and local movement refinement, securing\nthe top position on the leaderboards of the Waymo Open\nMotion Dataset [23] at the time of publication. Building\non these advancements, the Multiverse Transformer (MVTA)\nhas emerged as the current top performer in the Waymo\nOpen Sim Agents Challenge (WOSAC). This model intro-\nduces innovative training and sampling techniques, alongside\na receding horizon prediction mechanism, to further push the\nboundaries of accuracy and efficiency in motion prediction for\nautonomous driving applications. Despite these advancements,\nscene generation requires considering multimodal features\nof the entire environment (including surrounding agents and\nmaps), whereas trajectory prediction primarily concentrates\non the surrounding scene information of the predicted agent.\nHence, the key limitation in employing trajectory prediction\nmethods for scene generation lies in the inability to gener-\nate actions for all agents based on comprehensive \u201cglobal\u201d\ninformation. Furthermore, unlike our diffusion-based model\nwhich can generate varied actions at each inference, trajectory\nprediction methods cannot spontaneously generate actions.\nB. Generative model-based methods\nGenerative adversarial networks (GANs) [24]\u2013[26] and\nVariational Auto-Encoders (VAEs) [27], [28] have been ex-\nplored for generating traffic scenes. For instance, in [24],\na conditional generative neural system (CGNS) is proposed\nfor probabilistic trajectory generation to approximate the data\ndistribution. In [28], a conditional VAE is developed to ex-\ntensively perform multimodal and context-driven generative\ntasks for high-dimensional traffic scene generation problems.\nHowever, these methods often tend to generate unrealistic\ntrajectories as they primarily mirror the distribution of the\ntraining data and lack diversity [13]. Additionally, the training\nprocess for GANs can be unstable due to adversarial learning\n[15] and VAEs often have limited expressiveness of the latent\nspace as they often assume a simple Gaussian prior over the\nlatent variables, which can limit the expressiveness of the\nmodel.\nRecently, diffusion models have emerged as a compelling\nalternative to GANs and VAEs in the generation of realis-\ntic and diverse data [13], [29]. Notably, a classifier-guided\ndiffusion approach has been innovatively applied in [29] to\ntrajectory data alongside a probabilistic framework tailored for\nsynthesizing behaviors. In [13], a novel conditional diffusion\nmodel is introduced for controllable traffic generation (CTG),\nenabling users to specify desired trajectory properties during\ntest time\u2014such as achieving a specific goal or adhering to\nspeed limits\u2014while ensuring the outcomes remain realistic\nand physically plausible through dynamic constraints. How-\never, it is important to note that these methodologies are\nprimarily focused on generating behaviors for single agents.\nOn the other hand, the exploration into multi-agent tra-\njectory generation leveraging diffusion models has subse-\nquently garnered increasing attention [15], [30]. Specifically,\nSceneDM [15] proposes a novel framework based on diffusion\nmodels to generate future motions of all the agents, yielding\nstate-of-the-art results on the Waymo Sim Agents Benchmark.\nIn DJINN [30], a conditioned diffusion model is developed to\ngeneratively produce traffic scenarios over the joint states of\nall agents in the scene. However, a notable limitation of these\nmodels is their tendency to predict trajectories for individual\nagents per inference time. In contrast, our approach is designed\nto produce joint and coherent future trajectories for all agents\nsimultaneously.\nTo tackle the above challenges, we integrate diffusion\nmodels with transformer-based encoder-decoder architectures,\nenhancing the efficiency and effectiveness of the traffic scene\ngeneration process. This innovative approach leverages the\nstrengths of both technologies to generate more accurate and\ncohesive multi-agent trajectories. Details of this methodology\nwill be elaborated in the subsequent sections.\nIII. WCDT FOR TRAFFIC SCENE GENERATION\nIn this section, we detail our novel WcDT framework, de-\nsigned to represent and generate complex traffic scenes. First,\nwe illustrate our approach to modeling the intricacies of traffic\nenvironments, followed by a comprehensive introduction to\nour framework and its components.\nA. Traffic scene representation\nTraffic environments are characterized by multimodal data,\nincluding road layouts, traffic signal statuses, historical and\npredictive agent movements, pedestrian dynamics, and varying\nenvironmental conditions [33], [34]. To adeptly encode these\ndiverse elements within our WcDT framework, we utilize a\nunified approach that effectively captures the essence of traffic\nscenes, including predicted agents and environmental agents\n(i.e., world agents). Contrasting with existing methodologies,\nwhich require information to be transformed according to\nthe individual perspective or center of each agent [16], Our\napproach simplifies the encoding process by:\n\u2022 Transforming the positions of both predicted agents and\nworld agents into a unified Frenet coordinate system,\nfacilitating a standardized representation.\n\u2022 Representing the historical trajectories of predicted agents\nthrough movement statements rather than traditional co-\nordinate vectors, enhancing the clarity and interpretability\nof data.\n4\nFigure 2: Overview of Scene Encoder, which consists of the following modules: (a) Embedding blocks for encoding the position\nand features of multimodal scene information; (b) Spatial and temporal attention module for encoding the temporal and spatial\ninformation of predicted agents; (c) Multimodal features former for incorporating information from other agents, map points\nand traffic light features in the traffic scene; (d) Fusion blocks for fusing the previous features and generating trajectories.\nWithin our WcDT framework, we define key variables for\nsimulating traffic scenarios:\n\u2022 Aall, Ap, and Aw: The counts of all agents, predicted\nagents, and world agents, respectively.\n\u2022 Th and Tf: Historical and future prediction time steps.\n\u2022 L and P: The traffic lane lines and specific points on\nthem in the scenarios.\n\u2022 Stl: Traffic light states implicitly dictating agent flow.\n\u2022 D: Dimensionality of features for representing data.\nFor different objects in the traffic scenes, we represent them\nin different formats.\n\u2022 Agent move statement and features: To mitigate the im-\npact of varying agents\u2019 positions on historical and future\ntrajectories, we introduce the absolute states to represent\nthe past and prospective states of agents. Specifically,\nfor agent i at time step t, the state st is represented as\nsi\nt = [(xt \u2212xt\u22121), (yt \u2212yt\u22121), (\u03b8t \u2212\u03b8t\u22121), (vt \u2212vt\u22121)].\nHere, xt, yt, \u03b8t, and vt represent the longitudinal position,\nlateral position, heading angle, and velocity at time step\nt, respectively. The dimensional space for each agent\u2019s\nfeatures is [A, Th \u22121, D].\n\u2022 Traffic light feature: The dataset for traffic lights within\neach scenario, denoted as [Stl, Th, Dt], includes the po-\nsitions and operational statuses of traffic signals across\nhistorical intervals. For any given traffic signal point\nstl \u2208Stl, we represent this information through a one-\nhot encoding of the traffic light signals and the spatial\npositioning of traffic lights at each historical moment.\n\u2022 Map feature: The map features, denoted as [1, L, P, Dm],\ninclude essential lane information within a traffic sce-\nnario, such as lane positions and types. Each lane line\nlt \u2208L at the current time step is characterized by a\ndetailed representation, using one-hot encoding to specify\nthe positions of all points along the lane and to define the\nlane\u2019s type.\nFigure 2 shows an overview of our proposed WcDT\nframework for traffic scene generation, including three major\ncomponents: action diffusion, scene encoder, and trajectory\ndecoder, which are detailed in the following subsections.\nB. Action Diffusion\nTo enrich the diversity of generated trajectories in our\nWcDT framework, we encode the agent actions into latent\nspace which increases the uncertainty of agents\u2019 actions. The\ngenerated latent features are then used as one of the inputs to\nthe scene encoder. Our approach utilizes Denoising Diffusion\nProbabilistic Models (DDPM) [35] for the encoding of actions.\nTraditionally, DDPM relies on a convolutional U-Net architec-\nture. However, recent studies, including [36], suggest that the\nspecific inductive biases of U-Net are not essential for diffu-\nsion model performance. The advent of transformer networks\n[37] has revolutionized domain-specific architectures in fields\nranging from language and vision to reinforcement learning,\nshowing exceptional scalability with increases in model size,\ncomputational power, and data volume. Motivated by these\nadvancements, our work replaces the conventional UNet with\ntransformer modules within the conditional diffusion model\nframework, i.e., the \u201cDiffusion Transformers\u201d (DiTs) module\nto significantly boost the performance of our framework [36]\nand ensure more diverse trajectories for agents in simulated\nenvironments.\nFigure 3 shows the architecture of the conditional DiT\nblocks for the action latent encoding. The network takes\nrandom noise points, time steps, and historical trajectories\nof agents as input features and outputs \u201caction latent\u201d to the\nscene encoder. To refine the DiT network, the DDPM\u2019s loss\nfunction is meticulously designed to encourage the network\nto produce an \u201caction latent\u201d that adheres to the kinematic\nconstraints of the agents [35]. This optimization not only\naligns with the physical laws governing agent movements but\nalso enhances the variability of their actions due to the inherent\nunpredictability of the data generated by the diffusion model.\nThe loss function for updating the DiT module is denoted as\nfollows:\nLdiff = ||\u03f5 \u2212\u03f5\u03b8(\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, t)||2.\n(1)\nwhere\n\u00af\u03b1t are hyperparameters specified prior to diffusion\nmodel training. \u03f5\u03b8 is the diffusion model integrated with DiT\nblocks, and \u03b8 is the learnable parameter. \u03f5 is a set of standard\nnormally distributed random noise points.\n5\nFigure 3: Overview of the developed conditional DiT blocks\nfor action latent encoding: taking random noise points, time\nsteps, and historical trajectories of agents as input features and\noutputting \u201caction latent\u201d to scene encoder.\nC. Scene Encoder\nIn the traffic scene, various agents may present, including\npredicted agents (e.g., vehicles, pedestrians, and bicycles),\nworld agents (e.g., other vehicles, pedestrians, and bicycles),\nmap features, and traffic light information. To effectively\ngenerate realistic and diverse trajectories, we encode the agents\nwith different embedding blocks with different sizes and lay-\ners. These blocks globally encode the agent\u2019s characteristics,\nthereby avoiding the need to convert the coordinate system into\na Frenet coordinate system centered around individual agents\n\u2013 a common necessity in the existing trajectory prediction\nmodels. The Pose-Embedding encodes all positional attributes\npi into a unified one-dimensional matrix, while the Feature-\nEmbedding translates the agent\u2019s features (including height,\nwidth, and type, the latter in one-hot encoding format) into a\none-dimensional matrix as well. For a given predicted agent\ni:\nEp = \u03d5p[xi, yi],\n(2)\nwhere \u03d5p represents a linear transformation layer, and Ep de-\nnotes the positional embedding result. The feature embedding\noutcome Ef is denoted as:\nEf = \u03d5f[fw, fh, ftype],\n(3)\nwhere \u03d5f indicates a linear transformation layer, with fw and\nfh representing the agent\u2019s width and height, respectively, and\nftype specifying the agent\u2019s type after conversion to a one-\nhot encoded format. To synthesize these embedding into a\ncomprehensive agent representation, we apply:\nEA = ReLU(LayerNorm(Concat(Ep, Ef)))\n(4)\nwhere EA is the agent\u2019s final embedded representation,\nachieved by concatenating the positional and feature em-\nbeddings, followed by normalization and the application of\nthe ReLU activation function. This process ensures a robust\nand nuanced representation of each agent within the WcDT\nframework, enhancing the model\u2019s ability to accurately capture\ntraffic dynamics.\nTo accurately represent traffic scenarios, the encoding pro-\ncess also encompasses world agent Aw, map Am, and traffic\nlight Al features, ensuring these elements are seamlessly\nintegrated. First, an encoding layer processes each raw fea-\nture to yield embedded representations, e.g., Ew, Em, and\nEl. Subsequently, these representations are refined through\na series of advanced neural network blocks, including a\nmulti-head self-attention block for detailed feature analysis, a\ncross-attention block for inter-feature relationships, and fully\nconnected layers designed for the advanced extraction and\nintegration of features. Here attention layers are employed\ninstead of traditional CNNs due to their ability to dynamically\nfocus on relevant input features, efficiently capture long-\nrange dependencies, and offer enhanced interpretability and\nflexibility across diverse inputs. The multi-head self-attention\nlayers for the world agent encoding are denoted as:\nqAp = W Q\u00d7hEp, kAp = W K\u00d7hEp, vAp = W V \u00d7hEp,\n\u221dAp = Softmax\n \nqT\nAp\n\u221adk\nkAp\n!\n,\n(5)\nSelfAp =\u221dAp vAp,\n(6)\nwhere W Q, W K, W V are learnable parameters and EAp is\nthe result of the predicted agent after the embedding block.\nAfter the self-attention layer, the cross-attention encoding is\ndenoted as:\nqAp = W Q\u00d7hEAp, kAp = W K\u00d7hEAw, vAp = W V \u00d7hEAw,\n\u221dAo = Softmax\n \nqT\nAp\n\u221adk\nkAo\n!\n,\n(7)\nCrossAp =\u221dAp vAp,\n(8)\nwhere W Q, W K, W V are learnable parameters.\nCross Attention in Map Former:\nqAp = W Q\u00d7hEAp, km = W K\u00d7hEm, vm = W V \u00d7hEm,\n\u221dm = Softmax\n \nqT\nAp\n\u221adk\nkM\n!\n,\n(9)\nCrossm =\u221dAp vM,\n(10)\nwhere W Q, W K, W V are learnable parameters.\nCross Attention in Traffic Light Former:\nqAp = W Q\u00d7hEAp, kl = W K\u00d7hEl, vl = W V \u00d7hEl,\n\u221dm = Softmax\n \nql\nAp\n\u221adk\nkl\n!\n,\n(11)\nCrossl =\u221dAp vl,\n(12)\nwhere W Q, W K, and W V are learnable parameters.\nSpatial and Temporal Fusion. To effectively capture the\ndynamic and complex nature of traffic scenarios, we propose a\nTemporal Spatial Fusion Attention layer to distill the temporal\nand spatial characteristics of predicted agents\u2019 movements and\nintegrate multi-modal data, including predicted agents, world\nagents, map configurations, and traffic signals within traffic\n6\nenvironments. The ADV\u2019s features are augmented with the\n\u201caction latent\u201d generated from Eq. 4 for enhanced move-\nment descriptions. To distill critical insights from the spatial-\ntemporal data, multi-head self-attention blocks are employed.\nFor the broader multi-modal context of traffic scenes, these\nblocks identify key spatial and temporal details, which are then\nrestructured through MLP blocks to fit the trajectory decoder\u2019s\nrequirements, ensuring a thorough and accurate understanding\nof traffic dynamics.\nD. Trajectory Decoder\nThe trajectory decoder plays a crucial role in decoding the\nfused traffic features into the future trajectories of agents.\nIn this paper, the decoder is comprised of GRU blocks [38]\nand MLP blocks, specifically tailored to handle the temporal\nvariations in agent movements. Inspired by [39] on multimodal\ntrajectory prediction, which underscores its adaptability to\nagents exhibiting a range of behaviors over time, we intro-\nduce a multimodal output mechanism to effectively cater to\nagents with varied actions. To mitigate the impact of agents\u2019\ndiffering initial positions on the outcomes, our model outputs\ninclude agents\u2019 move statements alongside their likelihoods.\nThe trajectory for model M is formulated as:\nTrajm\na = Posa +\nTf\nX\ni=t\n[\u2206xm, \u2206ym, \u2206\u03b8m],\n(13)\nwhere Posa is the agent\u2019s position at the current time step,\nand Trajm\na\nrepresents the projected future trajectory points\nfor agents in model M. These points are calculated from\nthe agent\u2019s initial position Posa, adjusted by the incremental\ndisplacement [\u2206xm, \u2206ym, \u2206\u03b8m] for each subsequent time\nstep t. The speed is determined as:\nSpeedm\na = [\u2206xm, \u2206ym]\n\u2206t\n,\n(14)\nwith the speed Speedm\na being derived from the displacement\n[\u2206xm, \u2206ym] over the time interval \u2206t. This approach, based\non kinematic rules, calculates the agents\u2019 move statements,\nresulting in a multi-modal trajectory output formatted as\n[xm\na , ym\na , \u03b8m\na , vm\na ].\nE. Loss functions\nIn this paper, the objective is to guarantee that the generated\ntrajectories adhere to scene constraints and simultaneously pre-\nserve trajectory diversity. The trajectory from the multimodal\nset with the minimal loss is selected, and its deviation from\nthe ground truth is quantified using the Huber loss [40] as\nfollows.\nLreg = Huber(Trajp, Trajgt),\n(15)\nwhere Trajp represents the predicted trajectory for the modality\nwith the lowest loss, and Trajgt is the ground truth trajectory.\nIn addition, we introduce classification loss, which helps\nthe model to find out the closest modality to the ground truth.\nDuring training, we take the modality with the smallest AED\nas the truth of classification, the classification loss Lcls is\nformulated as:\nLcls = \u2212\nM\nX\ni=1\nyilog(pi),\n(16)\nHence, the overall loss function employed is the aggregate\nof diffusion loss, regression loss, and classification loss as:\nLtotal = Ldiff + Lreg + Lcls.\n(17)\nwhere Ldiff is the standard diffusion model loss function\nwhich is computed as the L2 loss of the predicted noise and\nthe original noise.\nIV. EXPERIMENTS AND RESULT ANALYSIS\nIn this section, we evaluate the performance of our WcDT\nmodel on a public traffic dataset, comparing it against leading\nmethods and conducting ablation studies to highlight its ef-\nfectiveness in traffic scene generation for autonomous driving\nsimulations.\nA. Experimental Setup\nDataset. We develop and assess our traffic scene generation\nmodel using the Waymo Motion Prediction dataset [23], which\noffers detailed motion trajectories of agents and high-definition\nmap data. The dataset includes 576,012 authentic driving\nscenarios and is divided into 486,995 scenarios for training,\n44,097 for validation, and 44,920 for testing [41]. Each traffic\nscenario is captured for 9 seconds, with sequences sampled\nat a frequency of 10 Hz. In testing scenarios, only the initial\n1-second segment of the trajectory is made publicly accessi-\nble and the developed algorithms are expected to accurately\ngenerate the motion trajectories of agents for the subsequent\n8 seconds, based on this limited initial data.\nMetrics. To assess the realism and diversity of the tra-\njectories generated by our model, we employ the evaluation\nmetrics aligned with those established in the literature [19],\n[42], [43], and utilize the Sim Agents Challenge metrics [42].\nThese metrics allow us to measure the alignment between\nour generated trajectories and the ground truth, focusing on\nthree essential aspects: kinematic metrics, object interaction\nmetrics, and map-based metrics. The detailed criteria of these\nevaluations are presented in Appendix A.\nIn this study, we refer to the Sim Agents Challenge metrics,\nwhich assess the similarity between generated trajectories and\nground truth along three dimensions: kinematic metrics, object\ninteraction metrics, and map-based metrics [19], [42], [43], as\nshown in Section A.1 of the Appendix. We adopt the approxi-\nmate negative log-likelihood (NLL) to evaluate similarity, the\nNLL we wish to minimize the following equation:\nNLL\u2217= \u22121\n|D|\n|D|\nX\ni=0\nLogqworld(o\u2265t,i|o<t,i),\n(18)\nwhere o<t,i denotes historical observations, including map\nobservations, traffic light observations, and agents\u2019 historical\nobservations and o\u2265t,i denotes future observations. where\nqworld is the \u201cworld model\u201d.\n7\nImplementation Details. We train our model over 128\nepochs utilizing two NVIDIA A100 GPUs, employing the\nAdam optimizer for optimization purposes [44]. The batch\nsize, initial learning rate, diffusion time steps, and dropout rate\nare set as 128, 2\u00d710\u22124, 100, and 0.1, respectively. To further\nenhance the stability of the training process, a cosine annealing\nscheduler [45] is employed to methodically reduce the learning\nrate as training progresses, ensuring gradual convergence to\noptimal solutions. The architecture of the model is composed\nof several components (see Section III): the backbone features\nan Action Diffusion mechanism with 2 DiT blocks; the Scene\nUnderstand Encoder, which includes 4 Other Agent Former\nblocks, 4 Map Former blocks, and 2 Traffic Light Former\nblocks; and the Trajectory Decoder, which is equipped with\n2 MLP blocks. Both Multi-Head Self-Attention and Multi-\nHead Cross-Attention mechanisms within the Scene Encoder\nutilized an identical configuration in terms of the number of\nheads and hidden units. To assess the impact of model scale\non its overall performance, our experiments encompass two\ndistinct model variants: the smaller WcDT-64, featuring 8\nattention heads and 64 hidden units, and the more expansive\nWcDT-128, with 8 attention heads and 128 hidden units. The\ncurrent location of the self-driving vehicle is designated as the\ncoordinate origin for the traffic scenario to ensure a consistent\nreference point for all trajectory predictions.\nB. Comparison with state-of-the-art methods\nIn this subsection, we conduct a comprehensive compar-\nison of our proposed method against a range of state-of-\nthe-art benchmarks that were submitted to the Sim Agent\nChallenge2 [42], including Random Agent [42], Constant\nVelocity [42], MTR+++ [46], WayFormer [33], MULTI-\nPATH++ [17], MVTA [22], and MVTE [22]. Among these\ncompetitors, MVTE [22], MVTA [22], and MTR+++ [46]\nare highlighted for their exceptional performance, demonstrat-\ning advanced capabilities in generating realistic and feasible\nmotion trajectories for autonomous vehicles within simulated\nenvironments.\nTable I shows the evaluation results on the testing datasets\nfor all considered algorithms. Notably, the Random Agent\nmethod [42] which generates trajectories randomly, exhibits\nthe worst performance, achieving the lowest composite metric\nscore of 0.163. This metric, aggregating the metrics across\nmultiple features (similarity metrics, kinematic metrics, object\ninteraction metrics, and map-based metrics), forms the overall\nevaluation for the overall ranking. Conversely, the Constant\nVelocity Agent [42] which predicts future trajectories based\non the last known heading and speed from the given context\nor historical data, performs marginally better. Despite its\nfailure to excel in the generative tasks, this method records a\nslightly higher composite metric of 0.238, alongside improved\nADE and MinADE scores compared to the Random Agent.\nThis enhancement is attributed to its utilization of kinematic\ninformation as input, underscoring the value of incorporating\nbasic physical principles into trajectory prediction models.\n2Sim Agent Leaderboard at 02-04-2024: https://waymo.com/open/challen\nges/2023/sim-agents/\nMarkedly, the MVTE algorithm [22] stands out with the\nhighest composite metric at 0.517, indicating the most robust\nperformance among the evaluated methods. Our proposed\nmodel, WcDT, closely trails with a composite metric of 0.511,\nyet surpasses MVTE in specific areas such as Linear Speed,\nLinear Acceleration, Angle Speed, Distance to Object, and\nDistance to Road Edge metrics, in addition to achieving\nbetter MinADE scores. This nuanced performance under-\nscores WcDT\u2019s specialized capabilities in rendering precise\nand contextually appropriate trajectory predictions, evidencing\nits advanced integration and interpretation of dynamic traffic\nenvironment variables.\nC. Impact of the number of agents\nIn this subsection, we evaluate how the number of agents\nwithin a scenario affects trajectory generation, specifically\nfocusing on the ADE metric. Figure 4 illustrates the distri-\nbution of agents in 10,000 random samples from the testing\ndataset, revealing that scenarios containing between 2 to 6\nagents constitute most of the dataset. Additionally, Figure 4\nhighlights a trend where an increase in the number of agents\ncorrelates with a decline in model performance. This observa-\ntion suggests that higher traffic densities, represented by more\nagents, introduce complexities in interactions that challenge\nthe model\u2019s generation accuracy.\n0.0\n2.5\n5.0\n7.5\nAgent Number\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\nProbability\nHistogram of\nAgent Number\n2\n4\n6\n8\nAgent Number\n0.6\n0.8\n1.0\n1.2\n1.4\n1.6\nADE\nImpact of Agent Number\non ADE\nFigure 4: Histogram of the number of agents and the effect of\nthe number of agents on ADE.\nD. Ablation studies on diffusion model\nIn this subsection, we assess the impact of the action latent\nencoding module when fed with random noise inputs, the\nefficacy of the Unet network structure, and the performance\nof our custom-designed DiT block. The results, presented in\nTable II, highlight that the DiT module outperforms others\nin achieving the lowest ADE and minADE scores, alongside\nthe highest composite score. This indicates that the diffusion\nmodel, underpinned by the DiT module, substantially enhances\nthe diversity of agent actions while maintaining trajectory\nplausibility.\n8\nMethod\nLinear\nSpeed\nLinear\nAccel\nAng\nSpeed\nAng\nAccel\nDist\nto Obj\nCollision\nTTC\nDist to\nRoad Edge\nOffroad\nComposite\nMetric\nADE\nMinADE\nRandom Agent [42]\n0.002\n0.044\n0.074\n0.120\n0.000\n0.006\n0.734\n0.178\n0.325\n0.163\n50.740\n50.707\nConstant Velocity [42]\n0.074\n0.058\n0.019\n0.035\n0.208\n0.202\n0.737\n0.454\n0.325\n0.238\n7.924\n7.924\nMTR+++ [46]\n0.412\n0.107\n0.484\n0.437\n0.346\n0.414\n0.797\n0.654\n0.577\n0.470\n2.129\n1.682\nWayFormer [33]\n0.408\n0.127\n0.473\n0.437\n0.358\n0.403\n0.810\n0.645\n0.589\n0.472\n2.588\n1.694\nMULTIPATH++ [17]\n0.432\n0.230\n0.515\n0.452\n0.344\n0.420\n0.813\n0.639\n0.583\n0.489\n5.308\n2.052\nMVTA [22]\n0.437\n0.220\n0.533\n0.481\n0.373\n0.436\n0.830\n0.654\n0.629\n0.509\n3.938\n1.870\nMVTE [22]\n0.443\n0.222\n0.535\n0.481\n0.382\n0.451\n0.832\n0.664\n0.641\n0.517\n3.873\n1.677\nWcDT (Ours)\n0.445\n0.248\n0.538\n0.460\n0.387\n0.403\n0.792\n0.668\n0.608\n0.511\n4.563\n1.669\nTable I: Quantitative results on the Sim Agents Leaderboard: We refer to the Sim Agents Challenge metrics, which contain\ntwo types of metrics: similarity metrics (10 metrics on the left) and distance error metrics (ADE and MinADE). There are\nthree types of similarity metrics, kinematic metrics, object interaction metrics, and map-based metrics and the composite metric\ncombines the above metrics. Higher values of these metrics represent better performance. ADE and MinADE are commonly\nused in trajectory prediction, with lower values indicating closer proximity to the ground truth.\nRandom Noise\nUnet\nDit Blocks\nADE\u2193\nminADE\u2193\nComposite\nMetric\n\u2191\n\u2713\n5.950\n2.715\n0.326\n\u2713\n5.184\n1.907\n0.480\n\u2713\n4.563\n1.669\n0.511\nTable II: Ablation Study on Diffusion Model: We evaluate the\ncontribution of the diffusion model and compare the impact\non the results of Dit Blocks and Unet as the backbone of the\ndiffusion model.\nE. Ablation studies on traffic scene encoder\nIn this subsection, we evaluate the traffic scene encoder\u2019s\nstructure by adding or removing its components, including the\nSpatial & Temporal Attention module, Other Agent Former,\nHD Map Former, and Traffic Light Former. ADE and Mi-\nnADE metrics are employed to assess how well the generated\ntrajectories match the ground truth.\nTable III presents the evaluation results, highlighting the HD\nMap Former as the crucial component for accurate trajectory\ngeneration. The model records its worst ADE and MinADE\nperformance in the absence of this module, indicating its key\nrole in ensuring generated trajectories adhere to lane con-\nstraints. Similarly, the Spatial & Temporal Attention blocks are\ncritical, markedly improving the scene encoder\u2019s understand-\ning of traffic scenarios. Their absence results in sub-optimal\nADE and MinADE scores of 4.803 and 1.973, respectively.\nThe Traffic Light Former also emerges as essential, its absence\nreflecting negatively on trajectory accuracy to a degree com-\nparable to missing the Spatial & Temporal Attention block.\nAdditionally, the Other Agent Former proves vital in incorpo-\nrating the historical motion of nearby agents, crucial for gen-\nerating safe trajectories. Incorporating all these modules, our\ntraffic scene encoder, WcDT, achieves the best performance,\nreflected in the lowest ADE and MinADE, demonstrating the\neffectiveness of our comprehensive approach in encoding the\nsurrounding environments.\nF. Ablation studies on trajectory decoder\nIn this subsection, we assess the architecture of the tra-\njectory decoder, focusing on the contributions of GRU (Gated\nRecurrent Unit) blocks, MLP (Multi-Layer Perceptron) blocks,\nand various network configurations. This evaluation aims to\nSpatial &Temporal\nAttention\nOther Agent\nFormer\nHD Map\nFormer\nTraffic Light\nFormer\nADE\u2193\nminADE\u2193\n\u2713\n\u2713\n\u2713\n4.803\n1.973\n\u2713\n\u2713\n\u2713\n4.591\n1.883\n\u2713\n\u2713\n\u2713\n5.083\n2.130\n\u2713\n\u2713\n\u2713\n4.734\n1.865\n\u2713\n\u2713\n\u2713\n\u2713\n4.563\n1.669\nTable III: Ablation Study on the Components of Scene-\nEncoder: We evaluate the importance of each module by\nadding and removing a module from the scene encoder, and\nthen use ADE and minADE to evaluate model performance.\ndetermine how each component and setup influences the\ndecoder\u2019s ability to accurately predict future trajectories, high-\nlighting the significance of these elements in enhancing the\nmodel\u2019s performance.\nTable IV shows the evaluation results, underlining the\ncritical role of MLP layers in the trajectory decoder\u2019s per-\nformance. The absence of MLP blocks leads to significantly\npoorer results, with the model yielding its highest ADE and\nMinADE scores of 5.290 and 2.475, respectively. This decline\nin performance is attributed to the MLP blocks\u2019 fundamental\nrole in refining the predictive capabilities of the decoder and\nprocessing and integrating complex features extracted from the\ninput data. Furthermore, the GRU blocks are underscored as\nvital for enabling the decoder to effectively leverage historical\ninformation, further contributing to the accurate prediction of\nfuture trajectories. Incorporating both MLP and GRU blocks,\nour WcDT model achieves optimal results, demonstrating su-\nperior trajectory generation with the lowest ADE and MinADE\nscores.\nMLP Blocks\nGRU Blocks\nDimension\nADE\u2193\nminADE\u2193\n\u2713\n64\n5.438\n2.532\n\u2713\n128\n5.290\n2.475\n\u2713\n64\n4.570\n1.805\n\u2713\n128\n4.568\n1.780\n\u2713\n\u2713\n64\n4.565\n1.735\n\u2713\n\u2713\n128\n4.563\n1.669\nTable IV: Ablation Study on the Trajectory Decoder: We\nevaluated the impact of different components in the trajectory\ndecoder on the performance of the model. Moreover, we also\nevaluated the effect of the dimensionality of the hidden units\nin the trajectory decoder on ADE and minADE.\n9\nG. Ablation studies on network structures\nIn this subsection, we examine how varying the number\nof modalities and attention heads influences the quality of\ngenerated trajectories across different model scales. Table V\nillustrates that the WcDT-128 model consistently outperforms\nthe WcDT-64 variant, suggesting that models equipped with\nmore attention layers tend to offer improved performance,\nhighlighting the beneficial impact of increased model com-\nplexity on the precision and reliability of trajectory predictions.\nFurthermore, comparing the single-modality with the multi-\nmodality configuration in the WcDT-64 model reveals that\nthe latter achieves superior performance. This is because of\nthe multi-modality trajectory decoder\u2019s ability to leverage a\nbroader spectrum of information, enabling a more nuanced\nand comprehensive understanding of the driving environment,\nthereby facilitating more accurate trajectory generation.\nMethod\nMultimodal\nAttention\nBlock Heads\nADE\u2193\nminADE\u2193\nWcDT-64\n1\n8\n5.331\n2.864\nWcDT-64\n10\n8\n5.082\n2.548\nWcDT-64\n10\n16\n4.997\n2.470\nWcDT-64\n30\n16\n4.872\n1.962\nWcDT-128\n10\n8\n4.662\n1.781\nWcDT-128\n30\n16\n4.563\n1.669\nTable V: Impact of multimodal trajectory decoders and dimen-\nsion of attention blocks on scene generation performance\nV. CONCLUSION\nIn this paper, a new traffic scene generation framework was\nproposed with an optimized trajectory generation process from\nfeature extraction to model inference with advanced Diffusion\nwith Transformer (DiT) blocks. The latent features, historical\ntrajectories, HD map features, and historical traffic signal in-\nformation were fused with various transformer-based encoders\nequipped with powerful attention modules. A novel trajectory\ndecoder was further proposed to generate multimodal future\ntrajectories. The experimental results showed that the proposed\napproach set a new benchmark for realism and diversity in\ntrajectory generation, as evidenced by the open-sourced traffic\ngeneration dataset. Future work will focus on proposing more\nrobust algorithms for accommodating complex urban traffic\nscenarios and more predicted agents.\nAPPENDIX\nA. Sim Agents Challenge metrics\nIn this paper, the Sim Agents Challenge metrics [42],\nincluding kinematic metrics, object interaction metrics, and\nmap-based metrics, are employed for evaluating the generated\ntrajectories.\nThe kinematic metrics utilized to assess the similarity be-\ntween the kinematic features of the generated trajectories and\nthe ground truth encompass linear speed, linear acceleration,\nangular speed, and angular acceleration. These metrics are\ncrucial for evaluating the dynamic aspects of the trajectories:\n\u2022 Linear Speed: Calculated using the 3-dimensional posi-\ntional data between successive time steps, the linear speed\nmetric is defined as:\nvt\nl = ||Post \u2212Post\u22121\n\u2206t\n||2,\n(19)\nwhere Post and Post\u22121 represent the positions at the\ncurrent and previous time steps, respectively, and \u2206t is\nthe time interval between these steps.\n\u2022 Linear Acceleration: This metric quantifies the rate of\nchange in linear speed over time and is expressed as:\nat\nl = vt\nl \u2212vt\u22121\nl\n\u2206t\n,\n(20)\nhighlighting how quickly an agent accelerates or decel-\nerates in the linear direction.\n\u2022 Angular Speed: Reflecting the rate at which an agent\nchanges its orientation, angular speed is calculated by:\nvt\n\u03b8 = |\u03b8t \u2212\u03b8t\u22121|\n\u2206t\n,\n(21)\nindicating how quickly an agent alters its heading be-\ntween two-time steps.\n\u2022 Angular\nAcceleration: This metric determines the\nchange in angular speed over time, given by:\nat\n\u03b8 = vt\n\u03b8 \u2212vt\u22121\n\u03b8\n\u2206t\n.\n(22)\nmeasuring the acceleration in an agent\u2019s rotational movement.\nThe object interaction metrics serve to assess the similarity\nof object interaction features between the generated trajecto-\nries and the ground truth, focusing on how agents navigate\nand maintain safety within their environment. These metrics\ninclude Time-to-Collision (TTC) and Distance to Nearest\nObject, which are crucial for evaluating spatial awareness and\ncollision avoidance capabilities:\n\u2022 Time-to-collision(TTC): This metric calculates the time\nremaining (in seconds) before a potential collision occurs\nbetween the current agent and any agent ahead of it\non its current trajectory. TTC is a critical safety metric,\nindicating the urgency of taking evasive action to prevent\na collision [47].\n\u2022 Distance to Nearest Object: This metric measures the\nclosest proximity (in meters) of an agent to any other\nobject or agent within the traffic scene. It provides a quan-\ntitative assessment of spatial positioning and is essential\nfor evaluating how well the model predicts safe distances\nbetween entities to avoid crashes or unsafe interactions.\nMap-based metrics play a crucial role in evaluating the in-\nteraction between generated trajectories and the road environ-\nment, comparing these interactions against the ground truth.\nThese metrics focus on road adherence and the maintenance\nof safe boundaries, including road departures and distance to\nroad edge:\n\u2022 Road Departures: This metric assesses if, and potentially\nwhen, an agent veers off the designated roadway in future\ntime steps. It\u2019s critical for determining the model\u2019s ability\nto generate trajectories that adhere to road boundaries and\nmaintain lane discipline.\n10\nFigure 5: Visualization results of the ground truth and WTSGM-generated trajectories in cruise scenarios.\n\u2022 Distance to Road Edge: Measures the shortest Euclidean\ndistance (in meters) from the agent to the nearest edge of\nthe road within the traffic scenario. This metric provides\ninsight into the spatial positioning of the agent relative to\nthe road infrastructure, evaluating the model\u2019s capability\nto keep agents within safe proximity to the roadway and\nprevent off-road excursions.\nB. Visualization Results\nFigures 5 and 6 provide visual representations of the gen-\nerated trajectories for randomly sampled scenarios from the\nWaymo dataset. The input for these scenarios includes map\nfeatures, depicted as black dotted lines, and the initial 1-\nsecond trajectories of various agents, illustrated with dots.\nTo distinguish between the trajectories of different agents,\neach is represented by a unique color. Notably, the color\nintensity of these trajectories deepens progressively with time,\nproviding a visual cue to the temporal progression of each\nagent\u2019s movement.\nFigure 5 presents selected scenarios that demonstrate lane-\nchanging maneuvers, showing the capability of our proposed\napproach to generate both diverse and accurate trajectories\nin comparison to ground truth for scenarios necessitating\ndynamic driving maneuvers. This visualization underscores the\nmodel\u2019s proficiency in accurately predicting and representing\nthe complex behaviors associated with lane changes, empha-\nsizing its effectiveness in handling intricate driving situations.\nMeanwhile, Figure 6 shows the model\u2019s performance in more\ncomplex traffic scenarios involving intersections, where our\nmodel impressively achieves high accuracy in trajectory gen-\neration, further demonstrating its robustness and adaptability\nto varied and challenging driving environments.\nREFERENCES\n[1] Y. Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, \u201cA\nsurvey on trajectory-prediction methods for autonomous driving,\u201d IEEE\nTransactions on Intelligent Vehicles, vol. 7, no. 3, pp. 652\u2013674, 2022.\n[2] S. Teng, X. Hu, P. Deng, B. Li, Y. Li, Y. Ai, D. Yang, L. Li, Z. Xuanyuan,\nF. Zhu et al., \u201cMotion planning for autonomous driving: The state of the\nart and future perspectives,\u201d IEEE Transactions on Intelligent Vehicles,\n2023.\n[3] B. Paden, M. \u02c7C\u00e1p, S. Z. Yong, D. Yershov, and E. Frazzoli, \u201cA survey of\nmotion planning and control techniques for self-driving urban vehicles,\u201d\nIEEE Transactions on intelligent vehicles, vol. 1, no. 1, pp. 33\u201355, 2016.\n11\nFigure 6: Visualization results of the ground truth and WTSGM-generated trajectories in intersection scenarios.\n[4] D. Chen, K. Zhang, Y. Wang, X. Yin, Z. Li, and D. Filev,\n\u201cCommunication-efficient\ndecentralized\nmulti-agent\nreinforcement\nlearning for cooperative adaptive cruise control,\u201d IEEE Transactions on\nIntelligent Vehicles, 2024.\n[5] L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li, Z. Hu, X. Na,\nZ. Li et al., \u201cMilestones in autonomous driving and intelligent vehicles:\nSurvey of surveys,\u201d IEEE Transactions on Intelligent Vehicles, vol. 8,\nno. 2, pp. 1046\u20131056, 2022.\n[6] W. Liu, M. Hua, Z. Deng, Z. Meng, Y. Huang, C. Hu, S. Song, L. Gao,\nC. Liu, B. Shuai et al., \u201cA systematic survey of control techniques and\napplications in connected and automated vehicles,\u201d IEEE Internet of\nThings Journal, 2023.\n[7] S. Ge, Y. Xie, K. Liu, Z. Ding, E. Hu, L. Chen, and F.-Y. Wang,\n\u201cThe use of intelligent vehicles and artificial intelligence in mining\noperations: Ethics, responsibility, and sustainability,\u201d IEEE Transactions\non Intelligent Vehicles, vol. 8, no. 2, pp. 1021\u20131024, 2023.\n[8] M. O\u2019Kelly, A. Sinha, H. Namkoong, R. Tedrake, and J. C. Duchi, \u201cScal-\nable end-to-end autonomous vehicle testing via rare-event simulation,\u201d\nAdvances in neural information processing systems, vol. 31, 2018.\n[9] X. Hu, S. Li, T. Huang, B. Tang, R. Huai, and L. Chen, \u201cHow simulation\nhelps autonomous driving: A survey of sim2real, digital twins, and\nparallel intelligence,\u201d IEEE Transactions on Intelligent Vehicles, 2023.\n[10] S. Grollius, M. Ligges, J. Ruskowski, and A. Grabmaier, \u201cConcept of\nan automotive lidar target simulator for direct time-of-flight lidar,\u201d IEEE\nTransactions on Intelligent Vehicles, 2021.\n[11] E. Weiss and J. C. Gerdes, \u201cHigh speed emulation in a vehicle-in-\nthe-loop driving simulator,\u201d IEEE Transactions on Intelligent Vehicles,\nvol. 8, no. 2, pp. 1826\u20131836, 2022.\n[12] C. Brogle, C. Zhang, K. L. Lim, and T. Br\u00e4unl, \u201cHardware-in-the-\nloop autonomous driving simulation without real-time constraints,\u201d IEEE\nTransactions on Intelligent Vehicles, vol. 4, no. 3, pp. 375\u2013384, 2019.\n[13] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray,\nand M. Pavone, \u201cGuided conditional diffusion for controllable traffic\nsimulation,\u201d in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2023, pp. 3560\u20133566.\n[14] M. Treiber, A. Hennecke, and D. Helbing, \u201cCongested traffic states in\nempirical observations and microscopic simulations,\u201d Physical review E,\nvol. 62, no. 2, p. 1805, 2000.\n[15] Z. Guo, X. Gao, J. Zhou, X. Cai, and B. Shi, \u201cScenedm: Scene-\nlevel multi-agent trajectory generation with consistent diffusion models,\u201d\narXiv preprint arXiv:2311.15736, 2023.\n[16] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, \u201cMultipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction,\u201d arXiv\npreprint arXiv:1910.05449, 2019.\n[17] B. Varadarajan, A. Hefny, A. Srivastava, K. S. Refaat, N. Nayakanti,\nA. Cornman, K. Chen, B. Douillard, C. P. Lam, D. Anguelov et al.,\n\u201cMultipath++: Efficient information fusion and trajectory aggregation\nfor behavior prediction,\u201d in 2022 International Conference on Robotics\nand Automation (ICRA).\nIEEE, 2022, pp. 7814\u20137821.\n[18] J. Ngiam, B. Caine, V. Vasudevan, Z. Zhang, H.-T. L. Chiang, J. Ling,\nR. Roelofs, A. Bewley, C. Liu, A. Venugopal et al., \u201cScene transformer:\nA unified architecture for predicting multiple agent trajectories,\u201d arXiv\npreprint arXiv:2106.08417, 2021.\n[19] S. Suo, S. Regalado, S. Casas, and R. Urtasun, \u201cTrafficsim: Learn-\ning\nto\nsimulate\nrealistic\nmulti-agent\nbehaviors,\u201d\narXiv\npreprint\narXiv:2101.06557, 2021.\n[20] S. Shi, L. Jiang, D. Dai, and B. Schiele, \u201cMotion transformer with\n12\nglobal intention localization and local movement refinement,\u201d Advances\nin Neural Information Processing Systems, 2022.\n[21] \u2014\u2014, \u201cMtr++: Multi-agent motion prediction with symmetric scene\nmodeling and guided intention querying,\u201d IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2024.\n[22] Y. Wang, T. Zhao, and F. Yi, \u201cMultiverse transformer: 1st place\nsolution for waymo open sim agents challenge 2023,\u201d arXiv preprint\narXiv:2306.11868, 2023.\n[23] Z. Sun, J. Wang, Y. Chen, J. Xu, X. Zhang, Y. Li, Y. Zhang, Z. Liu,\nJ. Guo, T. Huang et al., \u201cLarge scale interactive motion forecasting for\nautonomous driving: The waymo open motion dataset,\u201d in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2021,\npp. 11 271\u201311 281.\n[24] J. Li, H. Ma, and M. Tomizuka, \u201cConditional generative neural system\nfor probabilistic trajectory prediction,\u201d in 2019 IEEE/RSJ International\nConference on Intelligent Robots and Systems (IROS).\nIEEE, 2019, pp.\n6150\u20136156.\n[25] S. Choi, J. Kim, and H. Yeo, \u201cTrajgail: Generating urban vehicle tra-\njectories using generative adversarial imitation learning,\u201d Transportation\nResearch Part C: Emerging Technologies, vol. 128, p. 103091, 2021.\n[26] R. Bhattacharyya, B. Wulfe, D. J. Phillips, A. Kuefler, J. Morton,\nR. Senanayake, and M. J. Kochenderfer, \u201cModeling human driving\nbehavior through generative adversarial imitation learning,\u201d IEEE Trans-\nactions on Intelligent Transportation Systems, vol. 24, no. 3, pp. 2874\u2013\n2887, 2022.\n[27] W. Ding, W. Wang, and D. Zhao, \u201cMulti-vehicle trajectories generation\nfor vehicle-to-vehicle encounters,\u201d in 2019 IEEE International Confer-\nence on Robotics and Automation (ICRA), 2019.\n[28] G. Oh and H. Peng, \u201cCvae-h: Conditionalizing variational autoencoders\nvia hypernetworks and trajectory forecasting for autonomous driving,\u201d\narXiv preprint arXiv:2201.09874, 2022.\n[29] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine, \u201cPlanning with diffu-\nsion for flexible behavior synthesis,\u201d arXiv preprint arXiv:2205.09991,\n2022.\n[30] M. Niedoba, J. W. Lavington, Y. Liu, V. Lioutas, J. Sefas, X. Liang,\nD. Green, S. Dabiri, B. Zwartsenberg, A. Scibior et al., \u201cA diffusion-\nmodel of joint interactive navigation,\u201d arXiv preprint arXiv:2309.12508,\n2023.\n[31] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun,\n\u201cLearning lane graph representations for motion forecasting,\u201d in Com-\nputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23\u201328, 2020, Proceedings, Part II 16.\nSpringer, 2020, pp. 541\u2013\n556.\n[32] E. Brockfeld, R. D. K\u00fchne, A. Skabardonis, and P. Wagner, \u201cToward\nbenchmarking of microscopic traffic flow models,\u201d Transportation re-\nsearch record, vol. 1852, no. 1, pp. 124\u2013129, 2003.\n[33] N. Nayakanti, R. Al-Rfou, A. Zhou, K. Goel, K. S. Refaat, and\nB. Sapp, \u201cWayformer: Motion forecasting via simple & efficient at-\ntention networks,\u201d in 2023 IEEE International Conference on Robotics\nand Automation (ICRA).\nIEEE, 2023, pp. 2980\u20132987.\n[34] Y. Gao, Z. Chen, J. Wang, X. Zhang, and Y. Zhang, \u201cVectornet: Encoding\nhd maps and agent dynamics from vectorized representation,\u201d arXiv\npreprint arXiv:2006.05262, 2020.\n[35] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d\nAdvances in neural information processing systems, vol. 33, pp. 6840\u2013\n6851, 2020.\n[36] W. Peebles and S. Xie, \u201cScalable diffusion models with transformers,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2023, pp. 4195\u20134205.\n[37] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez,\nL. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in\nneural information processing systems, pp. 5998\u20136008, 2017.\n[38] K. Cho, B. Van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares,\nH. Schwenk, and Y. Bengio, \u201cLearning phrase representations using rnn\nencoder-decoder for statistical machine translation,\u201d Computer Science,\n2014.\n[39] P.-E. Casas, J. Hsu, J. Kuderer, and P. Abbeel, \u201cMultipath: Modelling\nmultiple future driving paths with anchor trajectories,\u201d arXiv preprint\narXiv:1904.01124, 2019.\n[40] J. H. Friedman, \u201cGreedy function approximation: A gradient boosting\nmachine,\u201d Annals of Statistics, vol. 29, no. 5, pp. 1189\u20131232, 2001.\n[41] Waymo, \u201cWaymo open dataset,\u201d https://waymo.com/open/data/motion/,\n2021.\n[42] N. Montali, J. Lambert, P. Mougin, A. Kuefler, N. Rhinehart, M. Li,\nC. Gulino, T. Emrich, Z. Yang, S. Whiteson et al., \u201cThe waymo open\nsim agents challenge,\u201d arXiv preprint arXiv:2305.12032, 2023.\n[43] J. Gil, L. Mart\u00edn, C. Montes, and A. Ortega, \u201cA fast procedure for\ncomputing the distance between complex objects in three-dimensional\nspace,\u201d Computer graphics forum, vol. 10, no. 4, pp. 331\u2013340, 1991.\n[44] D. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d\nComputer Science, 2014.\n[45] I. Loshchilov and F. Hutter, \u201cSgdr: Stochastic gradient descent with\nwarm restarts,\u201d arXiv e-prints, vol. abs/1608.03983, 2016.\n[46] S. Shi, L. Jiang, D. Dai, and B. Schiele, \u201cMtr++: Multi-agent motion\nprediction with symmetric scene modeling and guided intention query-\ning,\u201d arXiv preprint arXiv:2306.17770, 2023.\n[47] L. Jiang, D. Chen, Z. Li, and Y. Wang, \u201cRisk representation, perception,\nand propensity in an integrated human lane-change decision model,\u201d\nIEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 12,\npp. 23 474\u201323 487, 2022.\n",
    "2405.03520": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n1\nIs Sora a World Simulator? A Comprehensive\nSurvey on General World Models and Beyond\nZheng Zhu\u2217, Xiaofeng Wang\u2217, Wangbo Zhao\u2217, Chen Min\u2217, Nianchen Deng\u2217, Min Dou\u2217,\nYuqi Wang\u2217, Botian Shi\u2020, Kai Wang\u2020, Chi Zhang\u2020, Yang You\u2020, Zhaoxiang Zhang\u2020,\nDawei Zhao\u2020, Liang Xiao\u2020, Jian Zhao\u2020, Jiwen Lu\u2020, Guan Huang\u2020\nAbstract\u2014General world models represent a crucial pathway toward achieving Artificial General Intelligence (AGI), serving as the\ncornerstone for various applications ranging from virtual environments to decision-making systems. Recently, the emergence of the Sora\nmodel has attained significant attention due to its remarkable simulation capabilities, which exhibits an incipient comprehension of\nphysical laws. In this survey, we embark on a comprehensive exploration of the latest advancements in world models. Our analysis\nnavigates through the forefront of generative methodologies in video generation, where world models stand as pivotal constructs\nfacilitating the synthesis of highly realistic visual content. Additionally, we scrutinize the burgeoning field of autonomous-driving world\nmodels, meticulously delineating their indispensable role in reshaping transportation and urban mobility. Furthermore, we delve into the\nintricacies inherent in world models deployed within autonomous agents, shedding light on their profound significance in enabling\nintelligent interactions within dynamic environmental contexts. At last, we examine challenges and limitations of world models, and\ndiscuss their potential future directions. We hope this survey can serve as a foundational reference for the research community and\ninspire continued innovation. This survey will be regularly updated at: https://github.com/GigaAI-research/General-World-Models-Survey.\nIndex Terms\u2014World models, Generative models, Video generation, Autonomous driving, Autonomous agents\n\u2726\n1\nINTRODUCTION\nI\nN the pursuit of Artificial General Intelligence (AGI), the\ndevelopment of general world models stands as a funda-\nmental avenue. General world models seek to understand the\nworld through generative processes. Notably, the introduc-\ntion of the Sora model [21] has garnered significant attention.\nIts remarkable simulation capabilities not only demonstrate\nan initial comprehension of physical laws but also highlight\nthe promising advancements in world models. As we stand\nat the forefront of AI-driven innovation, it is crucial to delve\ndeeper into the realm of world models, unraveling their\ncomplexities, evaluating their current developmental stage,\nand contemplating the potential trajectories they may follow\nin the future.\nWorld models predict the future to grow comprehen-\nsion of the world. This predictive capacity holds immense\npromise for video generation, autonomous driving, and the\ndevelopment of autonomous agents, which represent three\nmainstream directions of development in world models. As\n\u2022\n\u2217indicates equal contributions. \u2020 indicates corresponding authors.\n\u2022\nZheng Zhu and Guan Huang are with GigaAI, Beijing, China.\n\u2022\nXiaofeng Wang, Yuqi Wang and Zhaoxiang Zhang are with Institute of\nAutomation, Chinese Academy of Sciences, Beijing, China.\n\u2022\nWangbo Zhao, Kai Wang and Yang You are with National University of\nSingapore, Singapore.\n\u2022\nChen Min is with Institute of Computing Technology, Beijing, China.\n\u2022\nNianchen Deng, Min Dou and Botian Shi are with Shanghai Artificial\nIntelligence Laboratory, Shanghai, China.\n\u2022\nChi Zhang is with Mach Drive, Beijing, China.\n\u2022\nDawei Zhao and Liang Xiao are with Defense Innovation Institute, Beijing,\nChina.\n\u2022\nJian Zhao is with EVOL Lab, Institute of AI, China Telecom, and\nNorthwestern Polytechnical University.\n\u2022\nJiwen Lu is with Tsinghua University, Beijing, China.\nshown in Figure 1, video generation world models encom-\npass the generation and editing of videos to understand and\nsimulate the world, which are valuable for media production\nand artistic expression. Autonomous driving world models,\naided by techniques of video generation, create driving sce-\nnarios and learn driving elements and policies from driving\nvideos. This knowledge assists in generating driving actions\ndirectly or training driving policy networks, aiding in end-\nto-end autonomous driving. Similarly, agent world models\nutilize video generation to establish intelligent interactions\nin dynamic environments. Unlike driving models, they build\npolicy networks applicable to various contexts, either virtual\n(e.g., programs in games or simulated environments) or\nphysical (e.g., robots).\nBuilding upon the foundation of comprehensive world\nmodeling, video generation methods unveil physical laws\nthrough visual synthesis. Initially, the focus of generative\nmodels was primarily on image generation [10], [33], [46],\n[66], [155], [168], [173], [177], [236] and editing [95], [129],\n[154], [245], laying the foundation for more sophisticated\nadvancements in synthesizing dynamic visual sequences.\nOver time, generative models [17], [18], [52], [63], [68], [84],\n[84], [111], [229], [243] have evolved to not only capture the\nstatic attributes of images, but also seamlessly string together\nsequences of frames. These models have developed some\nunderstanding of physics and motion, which represent early\nand limited forms of general world models [62]. Notably,\nat the forefront of this evolution stands the Sora model\n[21]. By harnessing the power of generative techniques, Sora\ndemonstrates a profound ability to generate intricate visual\nnarratives that adhere to the fundamental principles of the\nphysical world. The relationship between generative models\nand world modeling is symbiotic, with each informing\narXiv:2405.03520v1  [cs.CV]  6 May 2024\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n2\nand enriching the other. Generative models can construct\nvast amounts of data in a controlled environment, which\nalleviates the need for extensive real-world data collection,\nparticularly beneficial for training AI systems essential in\nreal-world applications. Moreover, the efficacy of generative\nmodels critically hinges upon the depth of comprehension\nprovided by world models. It is the comprehensive under-\nstanding of underlying environmental dynamics afforded\nby world models that empowers generative models to\nproduce visually compelling signals of superior quality\nwhile adhering to stringent physical constraints. Thereby\nenhancing their realism and utility in various domains.\nThe ability of world models to understand the envi-\nronment not only enhances video generation quality, but\nalso benefits real-world driving scenarios. By employing\npredictive techniques to comprehend driving environments,\nworld models are reshaping transportation and urban\nmobility by anticipating future driving scenarios, thereby\nenhancing safety and efficiency. World methods, aimed at\nestablishing dynamic models of environments, are crucial\nin autonomous driving, where precise predictions about\nthe future are essential for safe maneuvering. However,\nconstructing world models for autonomous driving presents\nunique challenges, primarily due to the sample complexity\ninherent in real-world driving scenarios. Early methods [60],\n[90], [159] attempt to address these challenges by reducing\nthe search space and incorporating explicit disentanglement\nof visual dynamics. Despite progress, a critical limitation\nlies in the predominant focus on simulation environments.\nRecent advances have seen autonomous driving world\nmodels leverage generative models to tackle real-world\nscenarios with larger search spaces. GAIA-1 [91] employs\na Transformer to predict the next visual token, effectively\nconstructing the driving world model. This approach enables\nanticipating multiple potential futures based on various\nprompts, such as weather conditions, scenes, traffic partic-\nipants, and vehicle actions. Similarly, methods like Drive-\nDreamer [209] and Panacea [218] leverage pre-trained diffu-\nsion models to learn driving world models from real-world\ndriving videos. These techniques harness the structured infor-\nmation inherent in driving scenes to controllably generate\nhigh-quality driving videos, which can even enhance training\nfor driving perception tasks. DriveDreamer2 [249], based\non DriveDreamer, further integrates large language models\nto enhance the performance of driving world models and\nuser interaction. It enables the generation of controllable\ndriving scene videos solely through natural language input,\nencompassing even rare scenarios like sudden overtaking\nmaneuvers. Furthermore, Drive-WM [212] demonstrates\nthe feasibility of directly training end-to-end driving using\ngenerated driving scene videos, significantly improving end-\nto-end driving performance. By anticipating future scenarios,\nthese models empower vehicles to make informed decisions,\nultimately leading to safer and more efficient navigation\non the roads. Moreover, this integration not only improves\ntransportation systems\u2019 safety and efficiency but also opens\nnew possibilities for urban planning and design.\nBeyond their established utility in driving scenarios,\nworld models have increasingly become integral to the\nfunctioning of autonomous agents, facilitating intelligent\ninteractions across a myriad of contexts. For instance, world\nmodels in game agents not only augment the gaming\nexperience but also propel the development of sophisticated\ngame algorithms. The Dreamer series [72], [73], [74] exem-\nplify this with its adept use of world models to predict\nfuture states within gaming environments. This capability\nenables game agents to learn in imagination, markedly\ndecreasing the necessary volume of interactions for effective\nlearning. In robotic systems, innovative approaches further\nunderscore the versatility and potential of world models.\nUniPi [50], for instance, reimagines the decision-making\nproblem in robotics as a text-to-video task. Its policy-as-\nvideo formulation fosters learning and generalization across\ndiverse robot manipulation tasks. Similarly, UniSim [232]\nintroduces a simulator of dynamic interactions through\ngenerative modeling, which can then be deployed in real-\nworld scenarios without prior exposure. RoboDreamer [255]\npushes the envelope by leveraging world models to propose\nplans involving combinations of actions and objects, thus\nsolving unprecedented tasks in novel robotic execution envi-\nronments. The multifaceted applications of world models\nextend beyond games and robotics. LeCun\u2019s proposal of\nthe Joint-Embedding Predictive Architecture (JEPA) [115]\nheralds a significant departure from traditional generative\nmodels. JEPA learns to map input data to predicted outputs\nwithin a higher-level representation space, which enables the\nmodel to concentrate on learning more semantic features,\nenriching its capability for understanding and predicting\nacross various modalities.\nBased on the comprehensive discussions presented above,\nit is evident that research on world models holds tremendous\npotential towards achieving AGI and has wide-ranging appli-\ncations across various domains. Therefore, world models war-\nrant significant attention from both academia and industry,\nrequiring sustained efforts over an extended period. In\ncomparison to recent surveys [36], [67], [136], [193] on world\nmodels, our survey offers broader coverage. It not only\nencompasses generative world models in video generation\nbut also delves into the applications of world models in\ndecision-making systems such as autonomous driving and\nrobotics. We envision this survey to offer valuable insights\nfor newcomers embarking on their journey into this field,\nwhile also stimulating critical thinking and discussion among\nestablished researchers in the community.\nThe main contributions of this survey can be summarized\nas follows: (1) We present a holistic examination of recent\nadvancements in world model research, encompassing pro-\nfound philosophical perspectives and detailed discussions.\n(2) Our analysis delves deeply into the literature surrounding\nworld models for video generation, autonomous driving,\nand autonomous agents, uncovering their applications in\nmedia production, artistic expression, end-to-end driving,\ngames, and robots. (3) We assess the existing challenges\nand limitations of world models and delve into prospective\navenues for future research, with the intention of steering\nand igniting further progress in world models.\n2\nVIDEO GENERATION AS A GENERAL WORLD\nMODEL\nThe video generation task aims to create various realistic\nvideos, requiring the model to understand and simulate\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n3\nWorld Models for\nVideo Generation\nWorld Models for\nAutonomous Driving\nWorld Models for\nAutonomous Agents\nBasic\nTasks\nApplication \nScenarios\nMedia Production/\nArtistic Expression/...\nAutonomous Driving\nGames/Robotics/...\nConditional Video Generation\nText-to-Video, Image-to-Video, \nVideo-to-Video, Flow-to-Video,\nSketch-to-Video, ...\nVideo Editting\nVideo Stylization, Local Editing,\nVideo Inpainting, Video Outpainting,\nVideo Prompt-to-Prompt, ... \nConditional Video Generation\nText-to-Video, Image-to-Video,\nVideo-to-Video, Action-to-Video,\nHD-Maps-to-Video, ...\nDriving Tasks\nOccupancy Prediction, \nDriving Action Prediction, \nSynthetic Data Augmentation, ... \nConditional Video Generation\nText-to-Video, Image-to-Video,\nVideo-to-Video, Action-to-Video,\n...\nAgent Tasks\nAgent Action Prediction, \nMultimodal Understanding,\nSynthetic Data Augmentation, ... \nGeneral World Models: Understanding the World via Predicting the Future\nFig. 1. This survey focuses on world models for video generation, world models for autonomous driving, and world models for autonomous agents.\nVideo generation world models specialize in conditional video generation and various video editing tasks. These video generation techniques aid in\nthe understanding of complex scenes and decision-making processes in autonomous driving and autonomous agents world models. The applications\nof these world models are broad, ranging from media production and artistic expression to action prediction in autonomous driving and agent systems.\nthe mechanism in the physical world, which aligns with\nthe objective of building a general world model. In this\nsection, we first introduce the technologies behind the video\ngeneration models in Section 2.1. Then, in Section 2.2, we\npresent and review the advanced video generation models\nemerging in recent years. Finally, we discuss Sora in Sec-\ntion 2.3, which is considered to be the largest breakthrough\nin video generation.\n2.1\nTechnologies behind Video Generation\nThe concept of video generation contains several different\ntasks based on the conditions, such as class, text, or image.\nThis survey mainly focuses on the scenario where the text\ncondition is given, known as text-to-video generation. In\nthis section, we first briefly introduce the visual foundation\nmodels, which are widely used in generation models. Then,\nwe present the text encoders for extracting text features\nfrom the text condition. Finally, we review the evolution of\ngeneration techniques.\n2.1.1\nVisual Foundation Models\nThe visual foundation models were originally proposed\nto tackle traditional computer vision tasks, for example,\nimage classification [42], whereas they also inspire the\ndevelopment of generation models. Based on the architecture,\nthey can be roughly categorized into convolution-based\nmodels and Transformer-based models, both of which can\nalso be extended to the video data.\nConvolution-based Models. The convolution-based models\nfor vision tasks have been fully explored in the last decades.\nStaring from LeNet [114], AlexNet [112], VGGNet [186],\nInceptionNet [194], ResNet [78], DenseNet [94] are gradually\nproposed to tackle the image recogntion problems. These\nmodels are adopted as a backbone model for other visual\ntasks [77], [174], [178]. Typically, U-Net [178] builds a U-\nshape architecture based on a backbone model for image\nsegmentation tasks. The U-shape architecture enables the\nmodel can leverage both the low-level and high-level features\nfrom the backbone, which significantly improves the pixel-\nwise prediction. Benefiting from the superiority of pixel-wise\nprediction, the U-shape architecture is also widely used in\nimage generation models [45], [85], [177].\nTransformer-based Models. The Transformer is proposed\nin [205] for machine translation tasks and applied to vision\nrecognition by ViT [48]. In ViT, images are divided into\npatches, then projected into tokens and finally processed\nby a series of multi-head self-attention and multi-layer\nperceptron blocks. Its ability to capture long-range dependen-\ncies in images enables its superiority in image recognition.\nAfter that, distillation [198], window-attention [137], and\nmask image modeling [11], [76] approaches are introduced\nto improve the training or inference efficiency of vision\nTransformers. Except for the success in image recognition,\nTransformer-based models also demonstrate superiority in\nvarious visual tasks, such as object detection [26], [235], [244],\n[259], semantic segmentation [191], [224], [251], and image\ngeneration [9], [75], [164]. Thanks to its good scalability\nproperty, the Transformer-based model DiT [164] has become\nthe main architecture of Sora.\nExtension to Video. The methods mentioned above are\nmainly designed for image data. Researchers further extend\nthese methods to solve problems in the video domain.\nConvolution-based models [27], [55], [56], [102], [201], [202],\n[230] usually introduce 3D convolution layers to building\nthe spatial-temporal relationships in video data. Transformer-\nbased methods [3], [15], [123], [138] extend and improve the\nmulti-head self-attention from spatial-only design to jointly\nmodeling spatial-temporal relationships. These methods also\ninspire the architecture design of text-to-video generation\nmodels, such as [111], [238], [239].\n2.1.2\nText Encoders\nThe text encoder is adopted to extract the text embedding for\na given text prompt in image or video generation. Existing\ngeneration methods usually employ the text encoder of a\nmulti-modal model or directly use a language model to\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n4\nconduct the embedding extraction. In the following, we\nwill briefly present representative multi-modal models and\nlanguage models.\nPre-trained Multi-modal Models. The pre-trained multi-\nmodal models, such as [121], [122], [169], align the rep-\nresentation of image and text in the embedding space. It\nusually consists of an image encoder as well as a text encoder,\nwhich naturally can be adapted to inject text information into\ngeneration models. CLIP [169] is a typical pre-trained multi-\nmodal model, which has been widely used in image/video\ngeneration models [17], [168], [173], [177]. It is pre-trained\nwith large-scale image-text pairs through contrastive learning\n[99] and demonstrates superior performance across various\ntasks. However, CLIP is pre-trained for image-text alignment\ninstead of comprehending complex text prompts. This draw-\nback may limit the generation performance when the given\nprompt is long and detailed.\nPre-trained Language Models. The pre-trained language\nmodels are usually pre-trained on the large-scale corpus, thus\nhaving transferable ability on various downstream language\ntasks. BERT [44] is an early attempt at language model pre-\ntraining, which designed several tasks to push the model\nlearning from unlabeled data. This paradigm also inspires\nfollow-up works, such as RoBERTa [135] and BART [119].\nWith the increasing model size and enlarging training dataset,\nthe pre-trained models demonstrate surprising abilities,\nwhich are usually named as larger language models (LLMs)\n[1], [22], [170], [171], [172], [199], [200]. T5 [172] and Llama-2\n[199] are two widely used LLMs in generation tasks [32],\n[92], [180], [223] since their superior performance and open\navaliablity. The LLMs provide a better understanding of long\ntext prompts than CLIP, thus helping the generation to follow\nthe instructions of humans.\n2.1.3\nGeneration Techniques\nIn this section, we review the development of generation\ntechniques in recent decades.\nGAN. Before the success of diffusion-based methods, GAN\nintroduced in [64] have always been the mainstream methods\nin image generation. It has a generator G and a discriminator\nD. The generator G is adapted to generate an output G(z)\nfrom a noised z sampled from a Gaussian distribution and\nthe discriminator D is employed to classifier the output is\nreal or fake.\nFrom the original definition of GAN [64], the generator\nG and the discriminator D are trained in an adversarial\nmanner. Specifically, we first train the discriminator D. We\ninput real data x sampled from a data distribution pdata(x)\nand generated output G(z) into the discriminator D and it\nlearns to improve the discrimination ability on real and fake\nsamples. This can be formulated as:\n\u2113D = Ex\u223cpdata (x)[log D(x)] + Ez\u223cpz(z)[log(1 \u2212D(G(z)))].\n(1)\nThe discriminator D should maximize the loss \u2113D. During\nthis process, the parameters in G are frozen. Then, we train\nthe generator G following:\n\u2113G = Ez\u223cpz(z)[log(1 \u2212D(G(z)))].\n(2)\nThe generator G is trained to minimize the loss \u2113G so that\nthe generated samples can approach the real data. The\nparameters in D are also not updated during this process.\nFollowing works apply GAN to various tasks related to\nimage generation, such as style transfer [20], [106], [162],\n[257], image editing [165], [207], [256], and image inpainting\n[40], [132].\nDiffusion. Diffusion-based methods have started to domi-\nnate image generation since the Denoising Diffusion Proba-\nbilistic Model (DDPM) [85], which learns a reverse process\nto generate an image from a Gaussian distribution N(0, I).\nIt has two processes: the diffusion process (also known as\na forward process) and the denoising process (also known\nas the reverse process). During the diffusion process, we\ngradually add small Gaussian noise to an image in T\ntimesteps. Given a image x0 from the data distribution, we\ncan obtain xT through the cumulative distribution of all\nprevious diffusion processes:\nq (x1:T | x0) :=\nT\nY\nt=1\nq (xt | xt\u22121) ,\n(3)\nwhere\nq (xt | xt\u22121) := N\n\u0010\nxt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI\n\u0011\n(4)\nT and [\u03b21, \u03b22, . . . , \u03b2T ] denote the diffusion steps and the\npre-defined noise schedule, respectively. We can also obtain\nthe output at the t timestep through\nq (xt | x0) := N\n\u0000xt; \u221a\u00af\u03b1tx0, (1 \u2212\u00af\u03b1t) I\n\u0001 ,\n(5)\nwhere \u03b1t := 1 \u2212\u03b2t and \u00af\u03b1t := Qt\ni=0 \u03b1i. Thus, we have\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5,\n\u03f5 \u223cN(0, 1).\n(6)\nThe denoising process is the reverse of the diffusion\nprocess, enabling us to obtain images from the Gaussian\nnoise. To achieve this, a denoising model \u03f5\u03b8 learns to predict\nthe noise \u03f5t added at the timestep t through a simplified loss\nfunction, which can be formulated as:\n\u2113simple\nt\n(\u03b8) = Ex0,t,\u03f5t \u2225\u03f5\u03b8 (xt, t) \u2212\u03f5\u22252\n2\n(7)\n= Ex0,t,\u03f5t\n\r\r\u03f5\u03b8\n\u0000\u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5, t\n\u0001 \u2212\u03f5\n\r\r2\n2\n(8)\nThen, we can denoise step-by-step through\nxt\u22121 =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212\u00af\u03b1t\n\u03f5\u03b8 (xt, t)\n\u0013\n+ \u03b2tz,\n(9)\nwhere z \u223cN(0, I). Although the generation quality of\nDDPM is satisfactory, its slow generation speed hinders\nits broader application. Following works attempt to solve\nthis problem by reducing the denoising steps [140], [182],\n[188], [189], [250] or accelerating the denoising model [53],\n[142], [161], [185].\nAutoregressive Modeling. Autoregressive modeling has\nbeen explored in both language generation methods [22],\n[170], [171] and image generation tasks [33], [116], [237], [241].\nGiven a sequence of tokens (x1, x2, . . . , xK), the probability\nof the k-th toeken xk only depends on tokens (x1, x2, xk\u22121).\nAn autoregressive model p\u03b8 is trained to maximize the\nlikelihood of the current token, which can be formulated\nas:\n\u2113=\nK\nX\nk\nlog p\u03b8 (xk | x1, x2, . . . , xk\u22121)\n(10)\nRecently, LVM [5] scales up the amount of training data\nto 420 billion tokens and model size to 3 billion parameters,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n5\ndemonstrating an ability for general visual reasoning as well\nas generation and directing a potential way towards the\nworld model.\nMasked Modeling. Masked modeling is first designed for\nself-supervised learning for language models [44], [104], [135]\nand image models [11], [76]. Given a sequence of tokens\n(x1, x2, . . . , xK), some tokens are randomly masked out.\nThen, the model is forced to predict the masked tokens\nand reconstruct the original representation. Noting the\nability of image reconstruction of masked modeling, some\nworks [28], [125], [126] directly generate images from mask\ntokens and find it also generalizes well in video generation\ntasks [238], [239]. Considering its simplicity and surprising\nperformance, it is also a promising direction for future\ngeneration techniques.\n2.2\nAdvanced Video Generation Models\nIn this section, we review the advanced video generation\nmodels proposed in recent years. Based on the given condi-\ntions (e.g. example, classes, audios, texts, images, or videos),\nduring generation, video generation tasks can be divided\ninto different categories. Here, we mainly focus on the text-\nto-video method, where the text description is available\nduring generation. These models aim to generate videos that\nare semantically aligned with given texts while maintaining\nconsistency between different frames. The methods for idea\ngeneration with other conditions can be modified from the\ntext-to-image models.\n2.2.1\nGAN-based Methods\nBesides the success of image generation, GAN-based models\nalso achieve remarkable performance for video generation\n[7], [43], [108], [120], [128], [130], [160]. Here, we select three\nrepresentative methods and review them briefly. We visualize\na general architecture of GAN-based methods from video\ngeneration in Figure 4 (a).\nTemporal GANs conditioning on captions (TGANs-C)\n[160] adopts a text encoder based on LSTM [87] to extract\na text embedding. This embedding is then combined with\na vector of random noise, which together form the input\nto the generator. The generator contains a series of spatio-\ntemporal convolutions to generate the frame sequence.\nUnlike the GANs-based models for image generation in\nSection 2.1.3, which typically has only one discriminator,\nTGANs-C designs three discriminators in video, frame, and\nmotion-levels, respectively. Benefiting from these discrimi-\nnators, the model is capable of producing videos that align\nwith the provided text and akin to authentic video footage.\nText-Filter conditioning Generative Adversarial Network\n(TFGAN) [7] adopts the text features extracted from the\ntext encoder to generate a series of filters in different frames.\nThen, these filters are employed as the convolutional filters in\nthe discriminator for each frame generation. This operation\nenhances the semantic association between the given text\nand the generated video.\nThe StroyGAN [128] aims to generate a sequence of\nframes based on a multi-sentence paragraph, where each\nsentence is responsible for one frame. It adopts a story\nencoder and a context encoder to extract the global rep-\nresentation of the multi-sentence paragraph and sentence\nfor the current frame, respectively. Then, the output from\nthe story encoder and context encoder are combined and\ninput to the generator to generate the current frame. It also\nemploys two discriminators to ensure the frame-level and\nvideo-level consistency with the given paragraph.\n2.2.2\nDiffusion-based Methods\nThe development of diffusion models for image generation\nalso facilitates the progress in video generation. We select\nfour representative approaches due to their effectiveness or\nefficiency. We summarize the framework of these methods in\nFigure 4 (b).\nImagen Video [84] proposes a cascaded sampling pipeline\nfor video generation. Starting from a base video generation\nmodel [86], which generates video with low resolution and\nlow frame rate, the authors cascade spatial and temporal\nsuper-resolution models to progressively improve the resolu-\ntion and frame rate of generated videos.\nStable video diffusion (SVD) [17] is built upon Stable\nDiffusion [177] by inserting temporal convolution and atten-\ntion layers after spatial convolution and attention blocks. To\nimprove the generation performance, the authors propose\nto disengage the training into three stages: pre-training on\ntext-to-image task, pre-training on text-to-video task, and\ntext-to-video finetuning with high-quality data. It proves the\nimportance of data curation for video diffusion models.\nLatte [143] is an early attempt to apply a Transformer-\nbased model in video generation. The model is built based\non DiT [164] and contains extra blocks for spatial-temporal\nmodeling. To ensure the efficiency in generation, the authors\nexplore four efficient designs for spatial and temporal\nmodeling, which is similar to the operations mentioned in\nSection 2.1.1. The architecture of the Latte is thought to be\nsimilar to the design of Sora.\nStreamingT2V [80] divides the text-to-video generation\ninto three steps, enabling to generation of long videos with\neven more than 1,200 frames. First, it employs pre-trained\ntext-to-video models to generate a short video e.g. with only\n16 frames. Then, it extends a video diffusion model with\nshort-term and long-term memory mechanisms to autore-\ngressively generate further frames. Finally, another high-\nresolution video generation model is adopted to enhance\ngenerated videos.\n2.2.3\nAutoregressive Modeling-based Methods\nAutoregressive modeling is also a popular technique in video\ngeneration [88], [144], [220], [229], [237] . We present its\narchitecture in Figure 4 (c).\nVideoGPT [229] is a representative autoregressive\nmodeling-based method. It first trains a VQ-VAE [204] to\nencode videos into latent tokens. Then, the authors leverage\na GPT-like framework [170] and train the model learning\nto predict the next token in the latent space. During the\ninference, a series of tokens is sampled from the latent space\nand the trained VideoGPT with VQ-VAE decodes it into\ngenerated videos.\nGODIVA [220] also generates videos in a similar way\nwhile emphasizing reducing the computation complexity of\nthe model. Specifically, the authors propose to replace an\noriginal self-attention layer with three sparse self-attention\nlayers, which only are conducted along the temporal, row,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n6\nEncoder\n(a) Compression model\nTransformer-\nbased model\nNoise\nT steps \ndenoising\nHuman \nprompt\nDetailed prompt\nLanguage \nmodel / CLIP\nGPTs\n(b) Generation model\n(c) Condition input\nDecoder\nFig. 2. An potential architecture of Sora. This architecture is inspired from [21], [136].\nBefore 2021\n2021\n2022\n2023\n2024\n\u25cf GAN-based\n\u25cf Diffusion-based \n\u25cf Autoregressive modeling-based\n\u25cf Masked modeling-based\n\u25cf Sora\n\u25cf Latte\n\u25cf WorldDreamer\n\u25cf MAGVIT-v2\n\u25cf SVD\n\u25cf Imagen Video\n\u25cf CogVideo\n\u25cf VideoGPT\n\u25cf GODIVA\n\u25cf TGANs-C\n\u25cf TFGAN\n\u25cf StoryGAN\n\u25cf IRC-GAN\n\u25cf StoryDALL-E\n\u25cf StreamingT2V\n\u25cf MAGVIT\n\u25cf VideoPoet\n\u25cf TiVGAN\nFig. 3. Chronological overview of video generation models. We present representative models proposed in recent years. Before 2021, GAN-based\nmodels dominate video generation. After that, autoregressive modeling-based, diffusion-based, and masked modeling-based models start to emerge\nand achieve surprising performance.\nand column dimensions of the latent features, respectively.\nThe effectiveness of this disentangling operation is also\nverified by models mentioned in Section 2.1.\nCogVideo [88] inherits the knowledge from the pre-\ntrained autoregressive model CogView2 [47] to reduce the\nburden of training from scratch. To improve the alignment\nbetween the given text and generated video, the authors pro-\npose a multi-frame-rate hierarchical generation framework,\nwhich first generates key frames in an autoregressive manner\nand then recursively interpolates frames with bidirectional\nattentions.\n2.2.4\nMasked Modeling-based Methods\nMasked modeling is also an emerging video generation\nmethod. Unlike autoregressive modeling, which suffers from\nsequential generation, the masked modeling method can\ndecode videos in parallel. We visualize its architecture in\nFigure 4 (d).\nMAGVIT [238] encodes videos into tokens through a\n3D-VQ tokenizer and leverages a masked token modeling\nparadigm to accelerate the training. Specifically, the target\ntokens are randomly replaced with conditional tokens and\nmasked tokens during training. Then, a bidirectional Trans-\nformer is trained to refine the conditional tokens, predict\nmasked tokens, and reconstruct target tokens. To improve\nthe generation quality, MAGVIT-v2 [239] is introduced to\nimprove the video tokenizer. The authors design a lookup-\nfree quantization method to build the codebook and propose\na joint image-video tokenization model, enabling it can tackle\nimage and video generation jointly. After that, VideoPoet\n[111] integrates MAGVIT-v2 [239] into a large language\nmodel to generate videos from various conditioning signals\nSimilarly, WorldDreamer [210] also trains to model to\nreconstruct masked tokens based on those unmasked tokens.\nTo facilitate the training process, they design a spatial-\ntemporal patchwise Transformer, which conducts attention\nwithin a spatial-temporal window. It adopts cross-attention\nlayers to inject information of given text descripction into\nthe model. The priority of parallel decoding enables it to\nachieve much faster video generation than diffusion-based\nand autoregressive-based methods.\n2.2.5\nDatasets and Evaluation Metrics\nTraining a text-to-video generation model requires large-scale\nvideo-text pairs. In Table 1, we present serveral popular\ndatasets. These datasets may also be employed to train\nmulti-modal models. Based on the technical report from\nSora, the data quality, for example the video-text alginment\nand the richness of captions, is essential to the generation\nperformance. Hence, we hope more large-scale high quality\ndataset can be open-sourced, prompting the prosperity of\nvideo generation and even the development of world models.\nThe metrics adopted to evaulate the video genera-\ntion performance varies in different papers. Fo example,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n7\nLatte [143] and VideoGPT [229] measure the performance\nthrough Fr\u00b4echet Video Distance (FVD) [203]. CLIP similarity\n(CLIPSim) [220] is also a common evualtion approach.\nHuman evaluation as complementary to these metrics is also\nwidely adopted in existing works. Since evaluation score are\nhighly related to the random seed, it is not easy to conduct\nfair comparison. Moreover, different mehtods may adopt\ndiffernt dataset to evalution performance, which further\naggravates this problem. Human preference annotations\nmay be a potential solution for video generation evaluation.\nRecently, some comprehensive benchmarks [97], [133], [134]\nare proposed for the comparison fairness.\n2.3\nTowards World Models: Sora\nSora is a closed-source text-to-video generation model devel-\noped by OpenAI. Besides being capable of generating a\nminute of high-fidelity video, it demonstrates some abilities\nto simulate the real world. It directs a way towards the\nworld model through video generation models. In this\nsection, we briefly introduce the techniques behind Sora.\nSince Sora is closed-source, all analyses here are mainly\nbased on its technical report [21] and may vary from its real\nimplementation.\n2.3.1\nFramework\nSora is thought to be a diffusion-based video generation\nmodel. It consists of three parts: 1. A compression model\nthat compresses a raw video both temporally and spatially\ninto latent representation and an asymmetrical model that\nmaps the latent representation back to the original video. 2.\nA Transformer-based diffusion model, similar to DiT [164],\nwhich is trained in the latent space. 3. A language model that\nencoders human instruction into embedding and injects it\ninto the generation model.\nCompression Model. The compression model usually con-\ntains an encoder and a decoder. The former is adopted to\nproject the video into a low-dimensional latent space, while\nthe latter maps the latent representation back to the video.\nBased on the technical report [21], the compression model\nis built based on VAE [109] or VQ-VAE [204]. Since the\narchitecture of the decoder is usually in symmetric to the\nencoder, we mainly focus on the architecture of the encoder\nin this review.\nGiven a raw video V \u2208RT \u00d7H\u00d7W \u00d7C, the encoder first\nprojects it into a sequence of tokens x \u2208Rnt\u00d7nh\u00d7nw\u00d7d.\nBased on the methods employed in visual foundation models\nmentioned in Section 2.1.1, there exist two options: spatial-\nonly compression and spatial-temporal compression. The\nspatial-only compression only compresses the video along\nthe spatial dimension. It extracts image patches of size h \u00d7 w\nfor each frame and adopts a 2D convolutional layer to project\nit into xi \u2208Rd. In this case, we have nt = T, nh = H/h,\nand nw = W/w. This operation is widely adopted in ViTs\n[48]. The spatial-temporal compression method compresses\nthe video along both the spatial and temporal dimensions,\nwhich provides a larger compression rate. Specifically, it\nextracts spatial-temporal tubes of size t \u00d7 h \u00d7 w from the\nvideo and adopts a 3D convolutional layer to project it into\nan embedding xi \u2208Rd. Thus, we have nt = T/t, nh = H/h,\nand nw = W/w. This operation is similar to the tubelet\nembedding technique in ViViT [3].\nAfter the tokenization, the encoder can further process\nthese tokens through Transformer blocks, convolutional\nblocks, or the combination of them and project them into\nz \u2208Rn\u2032\nt\u00d7n\u2032\nh\u00d7n\u2032\nw\u00d7d\u2032. We present the architecture of the\ncompression model in Figure 2 (a).\nGeneration Model. Based on the technical report, the gener-\nation model is built up on DiT [164]. Since the original DiT\nis designed for class-to-image generation, two modifications\nshould be conducted on it. First, since the self-attention\nblocks and MLP blocks in DiT are designed for spatial\nmodeling, extra blocks for temporal modeling should be\nadded. This could be achieved via extending the original self-\nattention to both spatial and temporal dimensions. Second,\nthe condition is changed from class to text, and blocks to\ninject the text information should be added. The text-to-\nimage cross-attention block is a potential solution, whose\neffectiveness has been proven in [32]. Based on this, one layer\nof the potential architecture can be formulated as:\nx\u2032 = x + STA(x),\n(11)\nx\u2032\u2032 = x\u2032 + CA(x\u2032, c),\n(12)\ny = x\u2032\u2032 + MLP(x\u2032\u2032),\n(13)\nwhere STA and CA denotes the spatial-temporal attention\nand text-to-image cross attention blocks, respectively. xg \u2208\nR(ng\nt \u00d7ng\nh\u00d7ng\nw)\u00d7dg denotes the input of this layer. The text\nembedding derived from a language model e.g. T5 [172] or\na multi-modal model e.g. CLIP [169] is denoted as c. We\nomit the injection of timestep information for brevity, which\ncan be achieved with adaptive layer norm blocks [166]. We\nalso present the potential architecture in Figure 2 (b). Finally,\nthe generation model is trained to predict noise added to\nthe latent representation z. More details can be found in\ndiffusion techniques mentioned in Section 2.1.3\n2.3.2\nTraining Data\nA large challenge to training Sora is collecting large-scale\nhigh-quality video-text pairs. Previous works [16], [32] have\nproven that generation performance is highly dependent\non the quality of data. Low-quality data, for example,\nnoisy video-text pairs or too simple video captions, results\nin generation models with pool instruction following. To\ntackle this problem, Sora adopts the re-captioning technique\nproposed in DALL-E 3 [16]. Specifically, a video captioner\nis trained with high-quality video-text pairs, where the text\nis well-aligned with the corresponding video and contains\ndiverse and descriptive information. The video captioner\ncould be a video version of multi-modal large language\nmodels, like GPT-4V [1], mPLUG [225], or InternVideo [214].\nThen, the pre-trained video captioner is employed to generate\nhigh-quality captions for the training data of Sora. This\nsimple method effectively improves the data quality.\nDuring inference, to solve the problem that users may\nprovide too simple prompts, Sora adopts GPT-4 [1] to rewrite\nthe prompts so that they are detailed. This enables Sora to\ngenerate high-quality videos.\n2.3.3\nTowards World Models\nBased on the claim from OpenAI, Sora can work as a world\nsimulator, since it can understand the result of an action. For\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n8\nEncoder\nDiffusion \nmodel\nDecoder\nRepeat T steps\nText\nEncoder\nAutoregressive\nTransformer\nEmpty token\nText\nDecoder\nText\nNoise\nGenerator\nDiscriminator (s)\nMasked tokens\nTransformer\nText\nRepeat \nN steps\nDecoder\n(a)\n(b)\n(c)\n(d)\nFig. 4. Video generation methods. (a) GAN-based (b) Diffusion-based (c) Autoregressive modeling-based (d) Masked modeling-based.\nan example from its technical report, Sora generates a video\nwhere a painter can leave new strokes along a canvas that\npersist over time. Another example is that a man can eat a\nburger and leave bite marks, which denotes that Sora can\npredict the results of eating. These two examples indicate\nthat Sora can understand the world and predict the results\nof an action. This capability is well-aligned with the target of\nworld models: understanding the world via predicting the\nfuture. Hence, we believe that the techniques behind Sora\ncan further inspire the exloration of world models.\nFirst, the training and inference strategies improve the\nperformance and efficiency in large generation models. For\nexample, Sora learning from videos with enative aspect ratios,\nwhich obviously improve the composition and framing of\ngenerated videos. This requires both technical and engi-\nneering optimization to enbale efficent training. Generating\nvideos with 1 minute length is a large chalenge and burden\nfor inference server, which still impede releasing Sora to\npublic until now. The OpenAI\u2019s soulution may be valubale\nfor the community of large models. More potential techniues\nadopted in Sora can be found in [136]. We believe these\ncontritbuons in Sora could also inpire building world models.\nSecond, Sora adopts Transformer-based generation with\nextensive parameters and large-scale training data, resuting\nin emergent abilities in video generation. This suggests that\nthere also exsiting scaling laws in the visual field and directs\na promising way to build large vision models or even world\nmodels.\nFinally, Sora emphasize the essentiality of training data\nfor good generation performance once again. Although\nOpenAI has not disclosed the sources and scale of data used\nin Sora, some guesses think extensive game videos may be\nintroduced during training. The game videos may contains\nrich physical information, helping Sora to understanding\nthe physical world. This indicates that the incorporating\nphysical engine may be a potential path towards building\nworld moels.\n3\nWORLD MODELS FOR AUTONOMOUS DRIVING\nDriving requires navigating uncertainty. It is crucial to\nunderstand the uncertainty inherent in autonomous driving\nto make safe decisions, where even a minor mistake could\nhave fatal consequences [89]. There are two primary forms of\nuncertainty: epistemic uncertainty, which stems from a deficit\nin knowledge or information, and aleatoric uncertainty,\nwhich is rooted in the inherent randomness of the real\nworld [57]. To ensure safe driving, it is imperative to leverage\npast experiences embedded in world models to effectively\nmitigate both aleatoric and epistemic uncertainty.\nWorld models are adept at representing an agent\u2019s spatio-\ntemporal knowledge about its environment through the\nprediction of future changes [115]. Two primary types of\nworld models exist within autonomous driving aimed at\nreducing driving uncertainty, i.e., world model for end-to-\nend driving and world model as neural driving simulator.\nIn the simulation environment, methods such as MILE [90]\nand TrafficBots [248] do not distinguish between epistemic\nand aleatoric uncertainties and incorporate them into the\nmodel based on reinforcement learning, enhancing their\ncapacity for decision-making and future prediction, thereby\npaving the way to end-to-end autonomous driving. In the\nreal environment, Tesla [156] and methods like GAIA-1 [91]\nand Copilot4D [246] involve utilizing generative models\nto construct neural driving simulators that produce 2D or\n3D future scenes to enhance predictive capabilities, thus\nreducing aleatoric uncertainty. Additionally, generating new\nsamples can mitigate epistemic uncertainty regarding rare\ninstances such as corner cases. Figure 5 illustrates these\ntwo types of world models in autonomous driving. The\nneural driving simulator can be further subdivided into two\ncategories: those generating 2D images and those simulating\n3D scenes.\n3.1\nEnd-to-end Driving\nIn the domain of autonomous driving, the development\nof world models assumes a crucial role as they strive\nto construct dynamic representations of environments.\nAccurate predictions about the future are imperative for\nensuring safe maneuvering in contexts. However, con-\nstructing world models for autonomous driving poses dis-\ntinct challenges, mainly originating from the intricate sample\ncomplexity in driving scenarios. end-to-end autonomous\ndriving methods [60], [90], [159] strive to tackle these\nchallenges by minimizing the search space and integrating\nexplicit disentanglement of visual dynamics on the CARLA\nsimulator [49]. The comparison of existing end-to-end\ndriving methods based on world models is illustrated in\nTable 2.\nIso-Dream [159] introduces a Model-Based Reinforce-\nment Learning (MBRL) framework, aimed at effectively\ndisentangling and utilizing controllable and noncontrollable\nstate transitions via reinforcement learning. Furthermore,\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n9\nTABLE 1\nDatasets for video generation. ASR: Automatic speech recognition. This table is reported by [34]\nDataset\nYear\nText\nDomain\n#Video\nAvg\nVideo len\nAvg text len\nResolution\nMSVD [30]\n2011\nManual Caption\nOpen\n1970\n9.7s\n5.3hr\n8.7 words\n-\nLSMDC [176]\n2015\nManual Caption\nMovie\n118K\n4.8s\n158hr\n7.0 words\n1080p\nMSR-VTT [226]\n2016\nManual Caption\nOpen\n10K\n15.0s\n40hr\n9.3 words\n240p\nDiDeMo [2]\n2017\nManual Caption\nFlickr\n27K\n6.9s\n87hr\n8.0 words\n-\nActivityNet [24]\n2017\nManual Caption\nAction\n100K\n36.0s\n849hr\n13.5 words\n-\nYouCook2 [253]\n2018\nManual Caption\nCooking\n14K\n19.6s\n176hr\n8.8 words\n-\nVATEX [208]\n2019\nManual Caption\nOpen\n41K\n10s\n115hr\n15.2 words\n-\nHowTo100M [147]\n2019\nASR\nOpen\n136M\n3.6s\n134.5Khr\n4.0 words\n240p\nACAV [118]\n2021\nASR\nOpen\n100M\n10.0s\n277.7Khr\n-\n-\nYT-Temporal-180M [242]\n2021\nASR\nOpen\n180M\n-\n-\n-\n-\nHD-VILA-100M [228]\n2021\nASR\nOpen\n103M\n13.4s\n371.5Khr\n32.5 words\n720p\nWebVid-10M [6]\n2021\nManual Caption\nOpen\n10M\n18.0s\n50Khr\n12.0 words\n-\nVimeo25M [211]\n2023\nAutomatic Caption\nOpen\n25M\n4.5s\n-\n10.0 words\n-\nInternVid [213]\n2023\nAutomatic Caption\nOpen\n234M\n11.7s\n760.3Khr\n17.6 words\n720P\nPanda-70M [34]\n2024\nAutomatic Caption\nOpen\n70.8M\n8.5s\n166.8Khr\n13.2 words\n720p\n\u25cfDriveDreamer-2\n\u25cfGenAD\n\u25cfSubjectDrive\n\u25cfLidarDM\n\u25cf\nDriving\nDiffusion\n\u25cf\nCopilot4D\n\u25cf\nADrive-I\n\u25cf\nMUVO\n\u25cf\nOccWorld\n\u25cf\nPanacea\n\u25cf\nDrive-WM\n\u25cf\nWoVoGen\n\u25cf\nViDAR\n\u25cf\nTrafficBots\nUniWorld\n\u25cf\nDriveDreamer\nGAIA-1\n\u25cf\n\u25cf\nIso-Dream\nMILE\n\u25cf\nSEM2\n\u25cf\n\u25cf\n2023\n2023.9\n2023.12\n2024\n\u25cf\nThink2Drive\n\u25cf\nDriveWorld\n\u25cfEnd-to-End Driving\n\u25cfNeural Driving Simulator (2D)\n\u25cfNeural Driving Simulator (3D)\n2023.11\nTesla\nWorld Model\n\u25cf\nFig. 5. Timeline of World Models in Autonomous Driving. End-to-end driving and neural driving simulator (both 2D and 3D) approaches are emerging\nsince 2023.\nTABLE 2\nSummary of end-to-end driving methods based on world models. Img, Act, Seg, and Dest stand for images, action, segmentation, and destination,\nrespectively.\nMethod\nType\nCore Structure\nReward\nInput\nOutput\nSimulator\nIso-Dream [159]\nReinforcement Learning\nRSSM [71]\n\u2713\nImg, Act\nImg, Act\nCARLA v1\nMILE [90]\nImitation Learning\nPGM [109]\n\u00d7\nImg, Act\nImg, Act, BEV Seg\nCARLA v1\nSEM2 [60]\nReinforcement Learning\nRSSM [71]\n\u2713\nImg, Act\nImg, Mask\nCARLA v1\nTrafficBots [248]\nReinforcement Learning\nCVAE [187]\n\u2713\nStatic Map, Traffic Lights, Act\nAct, Dest\nCARLA v1\nThink2Drive [124]\nReinforcement Learning\nRSSM [71]\n\u2713\nBox, HD-Map, Traffic Lights\nAct\nCARLA v2\nIso-Dream optimizes the agent\u2019s behavior based on the\nseparated latent imaginations of world models. In detail,\nIso-Dream projects non-controllable states into the future to\nestimate state values and links them with the current con-\ntrollable state. Iso-Dream enhances the agent\u2019s long-horizon\ndecision-making capabilities, exemplified in scenarios like\nautonomous vehicles proactively evade potential hazards by\nanticipating the movements of surrounding vehicles.\nIso-Dream learns the world model by mapping the\n2D image in front view to control signals, which is not\nsuitable for autonomous driving in 3D space. To address this\nissue, MILE [90] integrates the world model with imitation\nlearning in 3D space, i.e., Bird\u2019s Eye View (BEV) space.\nMILE uses 3D geometry as an inductive bias and creates\na latent space from expert driving videos. The training\noccurs using an offline dataset of urban driving, devoid\nof any necessity for online engagement with the scene.\nIn performance, it surpasses prior cutting-edge methods\nby a significant 31% margin in driving score on CARLA,\neven when operating in entirely new town and weather\nconditions. Moreover, MILE demonstrates its capability to\nexecute intricate driving maneuvers solely based on plans\ngenerated through imaginative processes.\nSimilar to MILE, SEM2 [60] also constructs a world\nmodel in 3D space. SEM2 employs a novel approach by\nincorporating a latent filter to isolate crucial task-specific\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n10\nfeatures and then utilizes these features to reconstruct\na semantic mask. Additionally, it utilizes a multi-source\nsampler during training, which merges standard data with\nvarious corner case data within a single batch, effectively\nensuring a balanced data distribution. Specifically, SEM2\ntakes camera and LiDAR as inputs, encoding them into a\nlatent state with deterministic and stochastic variables. The\ninitial latent state is subsequently employed to regenerate the\nobservation. Following this, the latent semantic filter isolates\ndriving-relevant features from the latent state, reconstructs\nthe semantic mask, and predicts the reward. Extensive\nexperiment conducted on the CARLA simulator showcases\nSEM2\u2019s adeptness in sample efficiency and robustness to\nvariations in input permutations.\nTrafficBots [248], another end-to-end driving method\nbased on world model, places emphasis on forecasting\nthe actions of individual agents within a given scenario.\nBy factoring in the destination of each agent, TrafficBots\nutilizes a Conditional Variational Autoencoder (CVAE) [187]\nto imbue individual agents with unique characteristics,\nenabling action anticipation from a BEV perspective. Traf-\nficBots offers quicker operational speeds and scalability to\nhandle larger numbers of agents. Experiments carried out on\nthe Waymo dataset illustrate TrafficBots\u2019 capacity to emulate\nrealistic multi-agent behaviors and attain promising results\nin motion prediction tasks.\nThe above methods [60], [90], [159], [248] were exper-\nimented in CARLA v1, but inherently face challenges\nregarding data inefficiency in CARLA v2. CARLA v2 offers a\nmore quasi-realistic testbed. Addressing the complexities of\nCARLA v2 scenarios, Think2Drive [124], a model-based rein-\nforcement learning method for autonomous driving, encour-\nages the planner to think within the learned latent space.\nThis approach significantly enhances training efficiency\nby utilizing a low-dimensional state space and leveraging\nparallel computing of tensors. Think2Drive achieves expert-\nlevel proficiency on CARLA v2 simulator after a mere 3-day\ntraining period utilizing a single A6000 GPU. Furthermore,\nThink2Drive introduces the CornerCase Repository, a novel\nbenchmark designed to assess driving models across diverse\nscenarios.\nDespite the advancements seen in world models for end-\nto-end driving using reinforcement learning, a significant\nlimitation remains: its primary emphasis on simulation\nenvironments. Next, we will delve into research on world\nmodels for autonomous driving in real-world scenarios.\n3.2\nNeural Driving Simulator\nHigh-quality data serves as the bedrock for training deep\nlearning models. While text and image data are readily\navailable at low costs, acquiring data in the realm of\nautonomous driving poses challenges owing to factors such\nas spatio-temporal complexities and concerns regarding pri-\nvacy. This is particularly true for addressing long-tail targets\nthat directly impact realistic driving safety. World models\nare pivotal for understanding and simulating the complex\nphysical world [91]. Some recent endeavors have introduced\ndiffusion models [85] into the domain of autonomous driving\nto build world models as neural simulators to generate\nrequisite autonomous 2D driving videos [91], [93], [209],\n[231]. Additionally, some methods employ world models\nto generate 3D occupancy grids or LiDAR point clouds\ndepicting future scenes [19], [152], [233], [246]. Table 3\nprovides an overview of these neural driving simulator\nmethods based on world models.\n3.2.1\n2D Scene Generation\nWorld models for driving video generation entail tackling\ntwo pivotal challenges: Consistency and Controllability. Con-\nsistency is crucial for maintaining temporal and cross-view\ncoherence between generated images, whereas controllability\nensures that generated images align with corresponding\nannotations [218]. The comparison of exiting 2D driving\nvideo generation methods based on world models are shown\nin Table 4.\nGAIA-1 [91] is a cutting-edge generative world model\ndesigned to produce lifelike driving videos, offering precise\nmanipulation of both ego-vehicle actions and environmental\nelements. GAIA-1 tackles the challenge of world modeling\nby leveraging video, text, and action inputs as sequences of\ntokens, predicting subsequent tokens in an unsupervised way.\nIts structure comprises two main elements: the world model\nand the video diffusion decoder. The world model, boasting\n6.5 billion parameters, underwent a 15-day training period\nutilizing 64 NVIDIA A100s, while the video decoder, with 2.6\nbillion parameters, was trained for the same duration using\n32 NVIDIA A100s. The world model meticulously examines\nthe elements and dynamics within the scene, whereas the\ndiffusion decoder transforms latent representations into\nhigh-fidelity videos imbued with intricate realism. GAIA-\n1\u2019s training corpus comprises 4,700 hours of driving videos\ncollected in London, spanning from 2019 to 2023. Notably,\nGAIA-1 demonstrates an understanding of 3D geometry and\ncan capture the complex interactions induced by road irreg-\nularities. Furthermore, GAIA-1 adheres to similar scaling\nlaws observed in Large Language Models (LLMs). With its\nlearned representations and control over scene elements,\nGAIA-1 opens new possibilities for enhancing embodied\nintelligence.\nWhile GAIA-1 can generate realistic autonomous driving\nscene videos, its controllability is limited to using only\ntext and action as conditions for video generation, whereas\nautonomous driving tasks require adherence to structured\ntraffic constraints. DriveDreamer [209], which excels in\ncontrollable driving video generation, seamlessly aligns with\ntext prompts and structured traffic constraints, including HD-\nMap and 3D box data. The training pipeline of DriveDreamer\ncomprises two stages: initially, DriveDreamer is trained with\ntraffic structural information as intermediate conditions,\nsignificantly improving sampling efficiency. In the subse-\nquent stage, the world model is developed through video\nprediction, where driving actions are iteratively utilized to\nupdate future traffic structural conditions. This enables Drive-\nDreamer to anticipate variations in the driving environment\nbased on different driving strategies. Through extensive\nexperiments on the challenging nuScenes [25] benchmark,\nDriveDreamer is confirmed to enable precise and controllable\nvideo generation, representing the structural constraints of\nreal-world traffic situations.\nTo further bolster the consistency and controllability\nof generated multi-view videos, DriveDreamer-2 [249] is\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n11\nTABLE 3\nSummary of model structure for neural driving simulator based on world models in autonomous driving. Img, Act, PC, Traj, Occ, HD, Flow, Lay, Obj,\nSeq, and Arg2 stand for images, action, point cloud, trajectory, occupancy, HD-Map, optical flow, layout, objects, sequence, and Argoverse2,\nrespectively.\nTask\nMethod\nData Source\nArchitecture\nEncoder\nDecoder\nInput\nOutput\n2D\nGAIA-1 [91]\nWayve [91]\nGPT\nVQ-VAE\nVideo Diffusion Decoder\nImg, Text, Act\nImg\nDriveDreamer [209]\nnuScenes [25]\nDiffusion\nVAE\nTask Specific Decoder\nImg, Act, Box, Text\nImg, Act\nDrivingDiffusion [127]\nnuScenes [25]\nDiffusion\nDiffusion Encoder\nDiffusion Decoder\nImg, Flow, Text, 3D Lay\nImg\nADriver-I [103]\nnuScenes [25]\nDiffusion\nCLIP-ViT\nVideo Diffusion Decoder\nImg, Act\nImg, Act\nPanacea [218]\nnuScenes [25]\nDiffusion\nDiffusion Encoder\nDiffusion Decoder\nImg, Text, BEV Seq\nImg\nDrive-WM [212]\nnuScenes [25]\nDiffusion\nVAE\nVAE Decoder\nImg, Act\nImg, Traj\nWoVoGen [139]\nnuScenes [25]\nDiffusion\n4D Volume Encoder\nDiffusion Decoder\nImg, Text, HD, Occ, Obj\nImg, HD, Occ\nDriveDreamer-2 [249]\nnuScenes [25]\nDiffusion\nVAE\nVideo Decoder\nImg, HD, Traj, Box, Text\nImg\nGenAD [231]\nOpenDV-2K [231], nuScenes [25]\nDiffusion\nDiffusion Encoder\nDiffusion Decoder\nImg, Text, Act\nImg\nSubjectDrive [93]\nnuScenes [25]\nDiffusion\nDiffusion Encoder\nDiffusion Decoder\nImg, Subject\nImg\n3D\nUniWorld [151]\nnuScenes [25]\nTransformer\nBEV Encoder\nTask Specific Decoder\nImg\nImg, Occ\nCopilot4D [246]\nnuScenes [25], KITTI [61], Arg2 [219]\nDiffusion\nVQ-VAE\nVQ-VAE Decoder\nPC, Act\nPC\nMUVO [19]\nCARLA v1 [49]\nGRU\nSensorFusion\nTask Specific Decoder\nImg, PC, Act\nImg, Act, Occ\nOccWorld [252]\nnuScenes [25]\nGPT\nVQ-VAE\nVQ-VAE Decoder\nOcc, Ego Poses\nOcc, Ego Poses\nViDAR [233]\nnuScenes [25]\nTransformer\nBEV Encoder\nLatent Render\nImg, Ego Poses\nPC\nLidarDM [261]\nKITTI-360 [131], Waymo [192]\nDiffusion\nDiffusion Encoder\nDiffusion Decoder\nImg, Traffic Lay\nPC\nDriveWorld [152]\nnuScenes [25], OpenScene [37]\nTransformer\nBEV Encoder\nTask Specific Decoder\nImg, Act, Ego Poses\nOcc, Act\nTABLE 4\nComparison of FVD and FID metrics with 2D driving video generation\nmethods based on world models on the validation set of the nuScenes\ndataset.\nMethod\nMulti-View\nMulti-Frame\nFVD\u2193\nFID\u2193\nDriveDreamer [209]\n\u2713\n452.0\n52.6\nDriveDreamer [209]\n\u2713\n\u2713\n340.8\n14.9\nADriver-I [103]\n\u2713\n97.0\n5.5\nWoVoGen [139]\n\u2713\n\u2713\n418.0\n27.6\nDrivingDiffusion [127]\n\u2713\n\u2713\n332.0\n15.8\nPanacea [218]\n\u2713\n\u2713\n139.0\n17.0\nSubjectDrive [93]\n\u2713\n\u2713\n124.0\n16.0\nGenAD-nus [231]\n\u2713\n\u2713\n244.0\n15.4\nGenAD-OpenDV [231]\n\u2713\n\u2713\n184.0\n15.4\nDrive-WM [212]\n\u2713\n\u2713\n122.7\n15.8\nDriveDreamer-2 [249]\n\u2713\n\u2713\n55.7\n11.2\nintroduced as an evolution of the DriveDreamer frame-\nwork. DriveDreamer-2 integrates a LLM to augment the\ncontrollability of video generation. Initially, DriveDreamer-\n2 integrates an LLM interface to interpret user queries\nand translate them into agent trajectories. Subsequently, it\ngenerates an HD-Map in accordance with traffic regulations\nbased on these trajectories. Additionally, DriveDreamer-2\nproposes the unified multi-miew model to improve temporal\nand spatial consistency to generate multi-view videos.\nDifferent from DriveDreamer-2 with LLM, ADriver-I [103]\nleverages Multimodal Large Language Models (MLLMs)\nto enhance the controllability of generating driving scene\nvideos. Inspired by the interleaved document approach in\nMLLMs, ADriver-I introduces interleaved vision-action pairs\nto establish a standardized format for visual features and\ntheir associated control signals. These vision-action pairs are\nutilized as inputs, and ADriver-I forecasts the control signal\nof the present frame in an autoregressive manner. ADriver-I\ncontinues this iterative process with the predicted next frame,\nenabling it to achieve autonomous driving in the synthesized\nenvironment. Its performance is rigorously assessed through\nextensive experimentation on datasets such as nuScenes [25]\nand sizable proprietary datasets.\nADriver-I is limited to generating single-view videos. To\ngenerate multi-view videos as DriveDreamer-2, Panacea [218]\nand DrivingDiffusion [127] are proposed. Panacea [218] is\nan innovative video generation system designed specifically\nfor panoramic and controllable driving scene synthesis. It\noperates in two stages: initially crafting realistic multi-view\ndriving scene images, then expanding these images along\nthe temporal axis to create video sequences. For panoramic\nvideo generation, Panacea introduces decomposed 4D atten-\ntion, enhancing both multi-view and temporal coherence.\nAdditionally, Panacea utilizes ControlNet to incorporate\nBEV sequences. Beyond these fundamental features, Panacea\nmaintains flexibility by enabling manipulation of global scene\nattributes through textual descriptions, including weather,\ntime, and scene details, providing a user-friendly interface\nfor generating specific samples. DrivingDiffusion [127] also\npresents a multi-stage approach for generating multi-view\nvideos. It involves several crucial stages: multi-view single-\nframe image generation, shared single-view video generation\nacross multiple cameras, and post-processing capable of\nhandling extended video generation. It also introduces\nlocal prompts to improve the quality of images effectively.\nSubsequent to the generation process, post-processing is\nemployed to enhance the coherence among different views\nin subsequent frames. Additionally, it utilizes a temporal\nsliding window algorithm to prolong the video duration.\nThe objective of the above methods is to generate realistic\ndriving scenario videos given certain conditions. Drive-\nWM [212] takes this a step further by utilizing predicted\nfuture scene videos for end-to-end planning applications to\nenhance driving safety. Drive-WM introduces multi-view\nand temporal modeling to generate multi-view frames.\nTo improve multi-view consistency, Drive-WM proposes\nfactorizing the joint modeling to predict intermediate views\nconditioned on adjacent views, significantly enhancing\nconsistency between views. Drive-WM also introduces a\nsimple yet effective unified condition interface, enabling\nflexible utilization of diverse conditions such as images, text,\n3D layouts, and actions, thereby simplifying conditional\ngeneration. Furthermore, by leveraging the multi-view world\nmodel, Drive-WM explores end-to-end planning applications\nto enhance autonomous driving safety. Specifically, at each\ntime step, Drive-WM utilizes the world model to generate\npredicted future scenarios for trajectory candidates sampled\nfrom the planner. These futures are evaluated using an image-\nbased reward function, and the optimal trajectory is selected\nto extend the planning tree. Testing on real-world driving\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n12\ndatasets validates Drive-WM\u2019s capability to produce top-\ntier, cohesive, and manageable multi-view driving videos,\nthereby unlocking avenues for real-world simulations and\nsafe planning.\nControl signals like bounding boxes or HD-Maps provide\na sparse representation of the driving scene. WoVoGen [139]\nenhances diffusion-based generative models by introducing\na 4D world volume. Initially, WoVoGen builds a 4D world\nvolume by merging a reference scene with a forthcoming\nvehicle control sequence. This volume then guides the\ngeneration of multi-view imagery. Within this 4D structure,\neach voxel is enriched with LiDAR semantic labels obtained\nvia the fusion of multi-frame point clouds, enhancing the\ndepth and complexity of environmental comprehension.\nSubjectDrive [93] has undertaken further research to\nexplore the effects of increasing the scale of generated videos\non the performance of perception models in autonomous\ndriving. Through their investigations, they have demon-\nstrated the efficacy of scaling generative data production in\ncontinuously enhancing autonomous driving applications.\nIt has pinpointed the pivotal significance of enhancing data\ndiversity in efficiently expanding generative data production.\nConsequently, SubjectDrive has developed an innovative\nmodel incorporating a subject control mechanism.\nThe above methods for generating driving videos have\nlargely been studied on relatively small datasets like\nnuScenes [25]. GAIA-1 [91] was trained on a dataset of\n4,700 hours, but the training dataset is not publicly avail-\nable. Recently, GenAD [231] has released the largest multi-\nmodal video dataset for autonomous driving, OpenDV-2K,\nexceeding the scale of the widely used nuScenes dataset\nby a multiplier of 374. OpenDV-2K contains 2,059 hours of\nvideo content accompanied by textual annotations, drawn\nfrom a combination of 1,747 hours sourced from YouTube\nand an additional 312 hours gathered from public datasets.\nAddressing common challenges such as causal confusion\nand handling large motions, GenAD utilizes causal temporal\nattention and decoupled spatial attention mechanisms to\neffectively capture the rapid spatio-temporal fluctuations\npresent in highly dynamic driving environments. This\narchitecture allows GenAD to generalize across diverse\nscenarios in a zero-shot way. This acquired understanding is\nfurther substantiated through the application of its learned\nknowledge to driving challenges, including planning and\nsimulation tasks.\n3.2.2\n3D Scene Generation\nIn addition to generating 2D videos for autonomous driving\nthrough world modeling, some methods delve into utilizing\nworld models to produce 3D LiDAR point clouds or 3D\noccupancy grids.\nCopilot4D [246] presents an innovative approach to\nworld modeling by first tokenizing LiDAR point cloud\nobservations with VQ-VAE [204], then predicting future\nLiDAR point clouds via discrete diffusion. To efficiently\ndecode and denoise tokens in parallel, Copilot4D modifies\nthe masked generative image Transformer to fit within\nthe discrete diffusion framework with slight adjustments,\nyielding significant improvements. When utilized for training\nworld models based on LiDAR point cloud observations,\nCopilot4D achieves a remarkable reduction of over 65% in\nChamfer distance for point cloud forecasting at 1s prediction\nand over 50% at 3s prediction across datasets such as\nnuScenes [25], Argoverse2 [219], and KITTI Odometry [61].\nCopilot4D utilizes unannotated LiDAR data to construct\nits world model, while OccWorld [252] delves into the 3D\noccupancy space for the representation of 3D scenes. Occ-\nWorld initiates its approach by employing a VQ-VAE [204]\nto refine high-level concepts and derive discrete 3D semantic\noccupancy scene tokens in a self-supervised manner. Sub-\nsequently, it customizes the GPT [22] architecture, intro-\nducing a spatial-temporal generative Transformer to forecast\nscene tokens and ego tokens. Through these advancements,\nOccWorld achieves significant results in 4D occupancy\nforecasting and planning.\nCopilot4D and OccWorld employ past LiDAR or 3D\noccupancy frames to generate future 3D scenes, whereas\nMUVO [19] adopts a more comprehensive strategy by\nleveraging raw camera and LiDAR data as input. MUVO\naims to acquire a sensor-agnostic geometric representation of\nthe environment and predicts future scenes in the forms\nof RGB images, 3D occupancy grids, and LiDAR point\nclouds. Initially, MUVO undertakes image and LiDAR\npoint cloud processing, encoding, and fusion utilizing a\nTransformer-based architecture. Subsequently, it inputs the\nlatent representations of the sensor data into a transition\nmodel to establish a probabilistic model of the current state.\nConcurrently, MUVO forecasts the probabilistic model of\nfuture states and generates samples from it.\nWhile Copilot4D, OccWorld, and MUVO generate 3D\nscenes without control, LidarDM [261] excels in producing\nlayout-aware LiDAR videos. LidarDM employs latent diffu-\nsion models to generate the 3D scene, integrating dynamic\nactors to establish the underlying 4D world, and subse-\nquently generating realistic sensory observations within this\nvirtual environment. Beginning with the input traffic layout\nat time t = 0, LidarDM initiates the generation process by\ncreating actors and the static scene. Subsequently, LidarDM\ngenerates the motion of the actors and the ego-car, composing\nthe underlying 4D world. Finally, a generative- and physics-\nbased simulation is utilized to produce realistic 4D sensor\ndata. The LiDAR videos generated by LidarDM are realistic,\nlayout-aware, physically plausible, and temporally coherent.\nThey demonstrate a minimal domain gap when tested with\nperception modules trained on real data.\nAs an abstract spatio-temporal representation of reality,\nthe world model possesses the capability to predict future\nstates based on the present. The training mechanism of\nworld models holds promise in establishing a foundational\npre-trained model for autonomous driving. UniWorld [151],\nViDAR [233], and DriveWorld [152] delve into the exploration\nof 4D pre-training based on world models, aiming to enhance\nvarious downstream tasks of autonomous driving, such as\nperception, prediction, and planning.\nUniWorld [151] introduces the concept of predicting\nfuture 3D occupancy as a pre-text task for autonomous\ndriving, leveraging extensive unlabeled image-LiDAR pairs\nfor 4D pre-training. It takes multi-view images as inputs,\ngenerating feature maps in a unified BEV space [215]. These\nBEV representations are then utilized by a world model\nhead to predict the occupancy of future frames. UniWorld\ndemonstrates improvements in intersection over union for\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n13\ntasks like semantic scene completion and motion prediction\ncompared to 3D pre-training methods [149], [150].\nWhile UniWorld has demonstrated the effectiveness of 4D\npre-training based on world models for autonomous driving,\nit predicts future scenes by adding a simple occupancy\nhead. ViDAR [233] proposes latent rendering operator with\ndifferentiable ray-casting for future scene prediction. ViDAR\nconsists of three main components: history encoder, latent\nrendering operator, and future decoder. The history encoder\nembeds visual sequences into BEV space. Subsequently, these\nBEV features undergo processing by the latent rendering\noperator, which significantly bolsters downstream perfor-\nmance. The future decoder, functioning as an autoregressive\nTransformer, utilizes historical BEV features to iteratively\nforecast future LiDAR point clouds for various timestamps.\nTo enhance 4D pre-training for autonomous driving by\nbetter capturing spatio-temporal dynamics, DriveWorld [152]\ntakes a further step by separately addressing temporal and\nspatial information. DriveWorld introduces the memory\nstate-space model to reduce uncertainty within autonomous\ndriving across both spatial and temporal dimensions. Firstly,\nto tackle aleatoric uncertainty, DriveWorld proposes the\ndynamic memory bank module, which learns temporal-\naware latent dynamics to predict future scenes. Secondly,\nto mitigate epistemic uncertainty, DriveWorld introduces\nthe static scene propagation module, which learns spatial-\naware latent statics to provide comprehensive scene context.\nMoreover, DriveWorld introduces the task prompt, utilizing\nsemantic cues as guidance to dynamically adjust the feature\nextraction process for various driving tasks.\n4\nWORLD MODELS FOR AUTONOMOUS AGENTS\nIn artificial intelligence, an autonomous agent refers to\na system that can perceive its surrounding environment\nthrough sensors (such as cameras) and act upon it through\nactuators to achieve specific goals [58]. These agents can be\nphysical, like robots, or virtual, such as software programs\nthat perform tasks in digital environments.\nGiven a goal, agents need to plan a sequence of actions.\nThere are already many successful algorithms for dynamic\nplanning in known environments. In most cases, however,\nthe environment is complex and stochastic, making it difficult\nto model by human experience explicitly. Therefore, this\nfield\u2019s core topic is how agents learn to plan in an unknown\nand complex environment. One way to solve this problem is\nto have the agent accumulate experience and learn behaviors\ndirectly from the interaction with the environment, without\nmodeling the state changes of the environment (the so-called\nmodel-free reinforcement learning). While this solution is\nsimple and flexible, the learning process relies on many\ninteractions with the environment, which may be extremely\nexpensive, even unacceptable.\nWorld Models [69] is the first work that introduces the\nconcept of the world model in the field of reinforcement\nlearning, modeling knowledge about the world from the\nagent\u2019s experience and gaining the ability to predict the\nfuture. This work demonstrates that even a simple RNN\nmodel can capture the dynamics of the environment and\nsupport the agent to learn and evolve policies in this model.\nThis learning paradigm is referred to as learning in imagination\n[72]. With world models, the cost of trials and failures can be\ngreatly reduced [222].\nIn this section, we introduce the world models for\nautonomous agents. We first describe the general framework\nof a world model-based agent, including the key components\nand the model structures widely used in world model-based\nagents in Section 4.1. Then, we introduce the agents serving a\nvariety of tasks, such as game agents and robotics, in Section\n4.2. Finally, we present the benchmarks that are commonly\nused to evaluate the performance of world model-based\nagents.\n4.1\nGeneral Framework of an Agent based on World\nModel\nMost works implement world model-based agents under a\nbasic framework originating from robotics. In the framework,\nthe world model is the core component. To model and predict\nthe surrounding environment, pioneers proposed several\neffective structures, which are widely used in later works. In\nthis section, we describe in detail the key components of the\nframework and the widely used structures of world models.\n4.1.1\nKey Components\nFrom the view of software engineering, an agent system can\nbe decomposed into four components [181]:\nSensor Sensors are the interface between an agent and its\nenvironment, providing the raw (or interpreted) information\nthe robot needs to understand its current context and make\ndecisions. Perception of the environment encompasses mul-\ntiple modalities, including vision through cameras, audition\nthrough microphones, touch through touch sensors, etc.\nAmong these modalities, vision is critical. Most research uses\nvision as the only way for agents to perceive the environment.\nActor. Actors are the mechanisms through which an agent\nexerts influence or effectuates changes in its environment.\nThey are the output devices that allow the agent to perform\nactions, such as motors for movement, robotic arms for\nmanipulation, and communication interfaces for interaction\nwith other systems or humans. The actions taken by the agent\nare determined by the decisions made within its planning\nsystem and are executed through the actuators.\nPlanning. Planning is the cognitive process that enables\nthe autonomous agent to determine a sequence of actions\nthat will lead to achieving its goals. It involves analyzing\nthe current state of the environment as perceived by the\nsensors, defining the desired end state, and selecting the\nmost appropriate actions to bridge the gap between the\ncurrent and desired states. The planning component must\nconsider the agent\u2019s capabilities, constraints, and the poten-\ntial consequences of its actions. Effective planning allows\nthe agent to act purposefully and adaptively, optimizing its\nbehavior to achieve its objectives efficiently and effectively.\nWorld Model. A world model is an internal representation of\nthe surrounding environment. This model is crucial for the\nagent\u2019s ability to understand the context in which it operates,\npredict the outcomes of its actions, and make informed\ndecisions. The world model interacts with the other three\ncomponents through tell and ask interfaces [181]. That is to\nsay, it receives information from other components to update\nits state and also responds to queries from other components.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n14\nSafeDreamer\n\u25cf\nGenie\n\u25cf\nV-JEPA\n\u25cf\nRoboDreamer\n\u25cf\nUniSim\n\u25cf\nDr.G\nSWIM\n\u25cf\nDreamerV3\n\u25cf\nHarmonyDream\n\u25cf\nDayDreamer\n\u25cf\nTWM\n\u25cf\nSTORM\n\u25cf\nMC-JEPA\n\u25cf\nA-JEPA\n\u25cf\nI-JEPA\n\u25cf\nDreamerPro\n\u25cf\nDreamingV2\n\u25cf\nTransDreamer\n\u25cf\nIRIS\n\u25cf\nJEPA\n\u25cf\nDreaming\n\u25cf\nDreamerV2\n\u25cf\nLEXA\n\u25cf\nPathDreamer\n\u25cf\nPlan2Explore\n\u25cf\nDreamerV1\n\u25cf\nSimPLe\n\u25cf\nPlaNet\n\u25cf\nWorld Models\n\u25cfRobot\nDreamPolicy\n\u25cf\nBefore 2020\n2020\n2021\n2022\n2023\n2024\n\u25cf\n\u25cfRSSM-based\n\u25cfTransformer-based\n\u25cfJEPA-based\n\u25cf\n\u25cfDiffusion-based\nOthers\nFig. 6. Chronological overview of video generation model. We present the world model-based autonomous agents proposed in recent years. The\ncolors show different structures of world models. The RSSM dominated these efforts while the Transformer, JEPA, and diffusion are gaining more and\nmore attention from 2022.\nA robust world model can reasonably predict the future\nstate when told with current perceptions and actions, thereby\nguiding the planning component to make wiser decisions.\n4.1.2\nWidely-used Model Structure\nA world model\u2019s key ability is predicting the environment\u2019s\nfuture state. Given the inherent randomness in most envi-\nronments, predictions should maintain a balance between\ndeterminism and uncertainty. Many researches have been\nconducted on this problem, proposing a variety of model\nstructures. Figure 6 shows the works in this field. Among\nthese works, the most widely-used structures are RSSM\n[41], [71], [158], [222], JEPA [4], [12], [13], [54], [115], and\nTransformer-based models [23], [29], [175], [210], [247].\nRecurrent State Space Model. The Recurrent State Space\nModel (RSSM) is the core structure of the Dreamer series.\nRSSM aims to facilitate prediction in latent spaces. It learns a\ndynamic model of the environment from pixel observations\nand selects actions by planning in the encoded latent space.\nBy decomposing the latent state into stochastic and deter-\nministic parts, this model considers both deterministic and\nstochastic factors of the environment. Due to its exceptional\nperformance in continuous control tasks for robots, many\nsubsequent works have expanded upon its foundation.\nJoint-Embedding\nPredictive\nArchitecture.\nThe\nJoint-\nEmbedding Predictive Architecture (JEPA) is proposed in a\npaper by LeCun [115] that laid out a conceptual framework\nfor future autonomous machine intelligence architecture. It\nlearns the mapping from input data to predicted output.\nThis model is different from traditional generative models\nas it does not directly generate pixel-level output, but makes\npredictions in a higher-level representation space, allowing\nthe model to focus on learning more semantic features.\nAnother core idea of JEPA is to train the network through\nself-supervised learning so that it can predict missing or\nhidden parts in the input data. Through self-supervised\nlearning, models can be pre-trained on a large number of\nunlabeled data and then fine-tuned on downstream tasks,\nthereby improving their performance on a variety of visual\n[4], [12], [13] and non-visual tasks [54].\nTransformer-based World Models. The Transformer [205]\noriginates from the natural language processing task. It\noperates on the principle of the attention mechanism, which\nenables the model to simultaneously focus on different parts\nof the input data. Transformers have been proven to be more\neffective than Recurrent Neural Networks (RNNs) in many\ndomains that require long-term dependencies and direct\nmemory access for memory-based reasoning [8], thus gaining\nincreasing attention in the field of reinforcement learning in\nrecent years. Since 2022, multiple works have attempted to\nconstruct world models based on the Transformer and its\nvariants [146], [175], [247], achieving better performance than\nthe RSSM model on some complex memory interaction tasks\n[29]. Among them, Google\u2019s Genie [23] has attracted consider-\nable attention. This work constructs a generative interactive\nenvironment based on the ST-Transformer [227], trained\nthrough self-supervised learning from a vast collection of\nunlabeled internet video data. Genie demonstrates a new\nparadigm for manipulable world models, offering a glimpse\ninto the immense potential for the future development of\nworld models.\n4.2\nAgents for Different Tasks\nMany researchers have explored the application of agents in\nvarious fields and tasks, such as gaming, robotics, navigation,\ntask planning, etc. Among the most widely studied tasks are\ngames and robotics.\n4.2.1\nGame Agent\nGetting AI systems to learn to play games has been an\ninteresting topic for a long time. The research on game agents\nnot only improves the game experience but more importantly,\nhelps people develop more advanced algorithms and models.\nWith the introduction of the Arcade Learning Envi-\nronment (ALE) [14], Atari games have gained a lot of\nemphasis as a benchmark for reinforcement learning. The\nAtari collection includes more than 500 games, covering\na wide variety of game types and challenges, making it\nideal for evaluating the capabilities of reinforcement learning\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n15\nalgorithms. Many studies have shown that reinforcement\nlearning can make agents play games at a level comparable to\nthat of human players [51], [82], [113], [183]. However, most\nof them require a huge amount of interaction steps with the\nenvironment. World models can predict future states of the\nenvironment, allowing agents to learn in imagination, thus\nsignificantly reducing the number of interactions required\nfor learning.\nRES [35] is an RNN-based environment simulator that\ncan predict the subsequent state of the environment based\non a series of actions and corresponding environmental\nobservations. Based on this capability, SimPLe [105] designs\na novel stochastic video prediction model, which achieves\nsignificant improvement in sample efficiency. Under the\nconstraint of 100K interactions, SimPLe has a much better\nperformance in Atari games compared to previous model-\nfree reinforcement learning methods.\nDreamerV2 [73] trains a game agent based on the RSSM\nmodel [71]. Unlike previous approaches that use continuous\nlatent representations, DreamerV2 uses discrete categorical\nvariables. This discretization method enables the model\nto capture the dynamic changes in the environment more\naccurately. DreamerV2 further uses the actor-critic algorithm\nto learn the behaviors purely from imagined sequences\ngenerated by the world model and achieves performance\ncomparable to human players on the Atari 200M benchmark\n[153].\nIRIS [146] is one of the pioneers that apply Transformer\n[205] in the world model. The agent learns its skill in\na world model based on the autoregressive Transformer.\nAs pointed out by Robine et al. [175], the autoregressive\nTransformer can model more complex dependencies by\nallowing the world model to directly access previous states,\nwhile previous works can only view a compressed recurrent\nstate. IRIS shows that the Transformer architecture is more\nefficient in sampling, outperforming humans in the Atari100k\nbenchmark [105] by only two hours of gameplay.\nTWM [175] proposes a Transformer-XL [38]-based world\nmodel. Transformer-XL solves the problem of capturing\nlong-distance dependencies in language modeling tasks\nby introducing the segment-level recurrence mechanism to\nextend the length of context. TWM migrates this capability\ninto the world model, enabling the capture of long-term\ndependencies between the states of the environment. To run\nmore efficiently, TWM further trains a model-free agent in\nthe latent imagination, avoiding a full inference of the world\nmodel in runtime.\nSTORM [247] sets a new record in no-resorting-to-\nlookahead-search methods on Atari100k benchmark [105] by\nstochastic Transformer. Inspired by the fact that introducing\nrandom noise into the world model helps enhance the\nrobustness and reduce cumulative errors in autoregressive\npredictions, STORM employs a categorical variational autoen-\ncoder [109], which inherently has a stochastic nature.\nGenie [23] is a novel generative environment developed\nby the DeepMind team. It learns the ability to generate\ninteractive 2D worlds by unsupervised learning from many\ninternet videos without labels. The most attractive point\nis that it can not only generate an entirely new virtual\nenvironment based on image or text prompts but also predict\ncoherent video sequences of that environment frame by\nframe based on user input actions. Genie enhances the\nefficiency of virtual content creation as well as provides\na rich interactive learning platform for the training of future\nAI agents. Although the current video quality and frame\nrate still need improvement, it has already demonstrated the\nimmense potential of generative AI in building future virtual\nworlds.\n4.2.2\nRobotics\nGetting an agent to learn to manipulate a robot is a long-term\nchallenge. Agents are desired to plan autonomously, make\ndecisions, and control actuators (e.g., robotic arms and legs)\nto complete complex interactions with the physical world.\nCommon basic tasks include walking, running, jumping,\ngrasping, carrying, and placing objects. Some of the more\ncomplicated tasks require a combination of several basic\ntasks, such as taking a specific item out of a drawer or\nmaking a cup of coffee.\nOne difference between a robot and a game agent is that\nthe goal of the robot is to interact with the real environment,\nwhich not only makes the environment dynamics more\ncomplex and stochastic but also greatly increases the cost\nof interacting with the environment during the training\nprocess. Therefore, it is particularly important to reduce\nthe number of interaction steps with the environment and\nenhance sampling efficiency in such scenarios. In addition,\nthe control of the actuators is in a continuous action space,\nwhich is also very different from the discrete action space in\nthe game environment.\nPrevious works of model-based planning [39], [59], [79]\nlearn low-dimensional environment dynamics by assuming\nthat access to the underlying state and the reward function\nis available. But in complex environments, this assumption\nis often untenable. Hafner et al. [71] suggested learning\nthe environment dynamics from pixels and planning in\nlatent spaces. They proposed RSSM, which is the base of\nthe later Dreamer-like world models. They achieved similar\nperformance to the state-of-art model-free methods within\nless than 1/100 episodes on six continuous control tasks of\nDeepMind Control Suite (DMC) [195], which proves that\nlearning latent dynamics of the environments in the image\ndomain is a promising approach.\nHowever, PlaNet [71] learns the behaviors by online\nplanning, i.e., considering only rewards within a fixed\nimagination horizon, which brings shortsighted behaviors. To\nsolve this problem, Hafner et al. further proposed DreamerV1\n[72], an agent that learns long-horizon behaviors purely from\nthe imagination of the RSSM-based world model. Predicting\nin latent space is memory efficient, thus allowing imagine\nthousands of trajectories in parallel. DreamerV1 uses a novel\nactor-critic algorithm to learn behaviors beyond the horizon.\nThe evaluation performed on visual control tasks of DMC\nshows that DreamerV1 exceeds previous model-based and\nmodel-free approaches in data efficiency, computation time,\nand final performance.\nSafeDreamer [96] aims to address safe reinforcement\nlearning, especially in complex scenarios such as vision-\nonly tasks. SafeDreamer employs an online safety-reward\nplanning algorithm for planning within world models to\nmeet the constraints of vision-based tasks. It also combines\nLagrangian methods with online and background planning\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n16\nwithin world models to balance long-term rewards and costs.\nSafeDreamer demonstrates nearly zero-cost performance\nacross low-dimensional and visual input tasks and outper-\nforms other reinforcement learning methods in the Safety-\nGymnasium benchmark [101], showcasing its effectiveness in\nbalancing performance and safety in reinforcement learning\ntasks.\nThe above works only learn and evaluate their perfor-\nmance in simple simulation environments, while the real\nenvironments often contain task-unrelated visual distractions\nsuch as complex backgrounds and varying lights. RSSM\nlearns the world model by reconstructing image observations,\nmaking it very sensitive to visual distractions in images and\ndifficult to capture small but important content. Therefore,\nbased on DreamerV1 [72], Dreaming [157] avoids the auto-\nencoding process by directly imagining and planning in\nthe latent space, and trains the world model by contrastive\nlearning, which does not rely on pixel-level reconstruction\nloss, so that the method is robust to visual distractions in\nthe environment. DreamingV2 [158] further explores how\nto apply contrastive learning to the discrete latent space\nof DreamerV2 [73]. Experimental results on 5 simulated\nrobot tasks with 3D space and photorealistic rendering show\nthat DreamingV2 can effectively handle complex visual\nobservations, and the performance is significantly better\nthan that of DreamerV2.\nSimilar efforts are made by DreamerPro [41] and Dr.G\n[70], both of which use a reconstruction-free approach to\naddress the visual sensitivity issue of RSSM. The difference is\nthat DreamerPro uses the prototype learning method to train\nthe prediction of the world model in the latent space, which\navoids the expensive computation caused by the large batch\nsize required for contrast learning. Dr.G, on the other hand,\nuses a self-supervised method of double-contrast learning\nto replace the reconstruction loss in DreamerV1 [72]. Both\nare evaluated in environments from DMC synthesized with\ncomplex background videos, verifying their robustness to\nvisual distractions.\nBesides those works involving simulated environments\nonly, some works are trying to train a robot in the real\nworld. The most difficult thing is that interaction with the\nreal world is expensive or even dangerous. Thus the ability\nof training in imagination is especially important in such\nscenarios. RobotDreamPolicy [167] learns a world model first\nand then learns the policy in the world model to reduce the\ninteractions with the real environment. During the training\nof the world model, the robot executes random actions\nin the environment, collecting pairs of the image before\naction, action, and the image after action as the training data.\nDayDreamer [222] applies DreamerV2 [73] to 4 real robots\nand directly trains the model online in real environments.\nThe authors found in experiments that the Dreamer model is\ncapable of online learning in the real world, and can master\na skill in a very short time. These works provide strong\nevidence that the sample efficiency of the world model\ncan help robots learn various skills efficiently with fewer\ninteractions.\n4.2.3\nDiverse Environments and Tasks\nBesides game and robotic tasks, some research works have\nlooked at other tasks such as navigation. PathDreamer [110]\napplies the idea of world model to indoor navigation tasks.\nThe world model is used to enhance environmental aware-\nness and predictive planning. Given one or more previous\nobservations, PathDreamer can predict plausible panoramic\nimages of the future, even for unseen rooms or regions\nbehind corners. Furthermore, PathDreamer innovatively\nuses 3D point clouds for environment representation, which\nsignificantly improves navigation success.\nThe JEPA series of work applies the architecture proposed\nby LeCun [115] to a variety of modal understanding and pre-\ndiction tasks. I-JEPA [4] is a non-generative self-supervised\nlearning method that learns highly semantic visual represen-\ntations by predicting the representations of different target\nblocks within the same image from a single context block.\nA-JEPA [54] proposes a self-supervised learning method\nbased on audio spectrograms, which effectively applies the\nsuccessfully masked modeling principle from the visual\ndomain to audio. A context encoder is used to predict and\nalign the representations of different target blocks from the\nsame audio spectrogram. MC-JEPA [13] is a self-supervised\nlearning method that simultaneously learns video content\nfeatures and motion features through JEPA, using a shared\nencoder to improve the accuracy of motion estimation and\nenrich the content features to include motion information.\nV-JEPA [12] extends I-JEPA to feature prediction in videos. It\npresents a suite of vision models that are exclusively trained\nbased on the objective of feature prediction. These models\nare developed without relying on supervisory signals such\nas pre-trained image encoders, negative examples, text, and\nreconstruction techniques.\nOther research efforts aim to study agents suitable for\ndiverse tasks. DreamerV3 [74] is a universal algorithm that\nrealizes cross-domain learning with fixed hyperparameters\nby signal amplitude transformation and robust normal-\nization. The authors evaluated multiple benchmark sets\nfrom Atari games, high/low dimensional continuous control\ntasks, survival tasks, spatial and temporal reasoning tasks,\netc. The results show that DreamerV3 can master different\ndomains only by relying on the same set of hyperparameters,\nand its performance is even better than some specialized\nalgorithms designed for specific domains. DreamerV3 is also\nthe first agent to successfully collect diamonds from scratch\nin Minecraft without providing any human experience.\nPlan2Explore [184] proposes a self-supervised two-stage\nlearning process. In the first stage, the agent explores the envi-\nronment in a self-supervised manner, gathers information\nabout the environment, and summarizes past experiences\nin the form of a parametric world model. It is worth noting\nthat no reward information is provided to the agent during\nthis phase, and the exploration is performed by the agent\nautonomously. Then the agent learns behaviors in the trained\nworld model for specific tasks. This stage can be done with\nlittle or no interaction with the environment. The two-stage\nlearning process allows the agent to obtain a more universal\nworld model, making the agent learn downstream tasks more\nefficiently.\nSWIM [145] aims to solve the learning of complex\nand general skills in the real world. SWIM claims that\nan agent must utilize internet-scale human video data to\nunderstand rich interactions carried out by humans and\ngain meaningful affordances. To this end, SWIM proposes\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n17\na high-level, structured, human-centric action space that is\napplicable for both humans and robots. The world model\nis first trained from a large dataset containing around 50K\negocentric videos. Then the world model is finetuned with\nrobot data to fit the robot domain. After that, behaviors for\nspecified tasks can be learned in the trained world model\nusing the standard cross-entropy method [179]. With the help\nof human action videos, SWIM achieves about two times\nhigher success than prior approaches while requiring less\nthan 30 minutes of real-world interaction data.\nHarmonyDream [141] identifies the world model as a\nmulti-task model consisting of observation modeling tasks\nand reward modeling tasks. HarmonyDream argues that\ntraditional world modeling methods, which tend to focus on\nobservation modeling, can become difficult and inefficient\ndue to the complexity of the environment and the limited\ncapacity of the model. HarmonyDream maintains a balance\nbetween observation modeling and reward modeling by\nautomatically adjusting the loss coefficient, which can be\nadapted to different types of tasks and avoid complicated\nhyperparameter adjustments.\nRoboDreamer [255] learns compositional world models\nto enhance robotic imagination. It decomposes the video\ngeneration process and leverages the inherent composition-\nality of natural language. In this way, it can synthesize\nvideo plans of unseen combinations of objects and actions.\nRoboDreamer dissects language instructions into a set of\nprimitives, which then serve as distinct conditions for a\nset of models to generate videos. This method not only\ndemonstrates strong zero-shot generalization capabilities but\nalso shows promising results in multimodal-instructional\nvideo generation and deployment on robotic manipulation\ntasks.\nUniSim [232] is a generative simulator for real-world\ninteractions. UniSim contains a unified generative framework\ntaking action as input that integrates diverse datasets across\ndifferent modulations. With this approach, UniSim can\nsimulate the visual outcomes of both high-level instructions\nand low-level controls. UniSim can be utilized for various\napplications, such as controllable game content creation and\nthe training of embodied agents in simulated environments,\nwhich can be directly deployed in the real world.\n4.3\nCommonly Used Benchmarks\nA variety of benchmarks are used to measure the perfor-\nmance of game agents and robotics. The evaluation method\nis usually to test the completion of several specific tasks or\nthe rewards obtained by the agent after a limited amount of\ninteractive learning in a specific environment.\nAtari100k [105] is the most commonly used benchmark\nfor game agents, which uses a subset of 26 Atari games from\nthe Arcade Learning Environment [14]. For each game, the\nagent is allowed to collect up to 100K interactions. With\n4 frames per interaction, this is equivalent to 400K frames\nor 114 minutes (at 60FPS). To normalize the scores across\ndifferent games, a metric called Normalized Human Score\n(NHS) [153] is proposed, which is defined as:\nNHS = scoreagent \u2212scorerandom\nscorehuman \u2212scorerandom\n(14)\nWhere scorehuman is the score achieved by the professional\nhuman player and scorerandom is the score achieved by an\nagent using purely random policy. This metric evaluates the\nperformance of agents compared to the professional human\nplayer. Table 5 collects the performance reported by the\nworld model-based game agents mentioned in this survey.\nOverall, recent methods have been able to outperform human\nplayers in about half of these 26 games with a constraint of\nonly 100K interactions, and in some games, several times\nover. At the same time, in other games such as Alien,\nAmidar, and Seaquest, they perform much worse than human\nplayers. This may be because the environment dynamics of\nthese games are more complex, and 100K interactions are\nnot enough for the agent to have a full understanding of\nthe environment. On the other hand, low-quality images\nmake some important elements easily ignored by image-\nreconstruction-based algorithms, resulting in an incorrect\nunderstanding of the environment.\nFor robotic tasks, there are several benchmarks adopted\nfor different tasks and environments. DMC [195] is the most\ncommonly used benchmark for robot learning. It contains a\nvirtual environment that supports research into how agents\nlearn complex physical tasks. This environment offers a\ndiverse set of control tasks, from simple object moving to\ncomplex manipulator operations, as well as navigation tasks\nin 3D space. These tasks are built on top of the MuJoCo\nphysics engine [197]. It also supports high-dimensional\nobserving spaces, including pixel-level visual inputs, which\nmakes it suitable for studying vision-driven reinforcement\nlearning algorithms. To increase the visual diversity, DMC\nRemaster [65] extends DMC with seven types of visual\nfactors, including the ground texture, the background, the\ncolor of robot, the color of target, the specular property,\nthe camera position, and the light, thus presents a greater\nchallenge to the visual robustness of the algorithm.\nAnother common benchmark is RoboSuite [260]. It is a\nrobot learning simulation framework powered by MuJoCo\nphysics engine that provides a standardized benchmark envi-\nronment for robot learning research. The RoboSuite includes\na variety of robot models, grippers, controller modes, and a\nstandardized set of benchmark tasks. In addition, it supports\ngenerating new environments programmatically with a\nmodular API design that allows researchers the flexibility to\ndesign new robotic simulation environments.\nOther benchmarks for robotic tasks include Meta-World\n[240] which contains 50 distinct robotic manipulation tasks\nfor meta-RL and multi-task learning, RLBench [100] which\ncontains 100 unique, hand-designed tasks, covering every-\nthing from simple goal-reaching and door opening to more\ncomplex multi-stage tasks.\nDue to the choice of different tasks and interaction\nconstraints in different works, the results of robotic research\nworks are difficult to align. DreamingV2 [158] evaluates\na relatively complete set of these works, which covers\ndiscrete/continuous latent space and with/without image\nreconstruction. We refer to their evaluation results in this\nsurvey, which are presented in Table 6. The experiment\nanalyzes the impact of two factors, the discreteness or\ncontinuity of the latent space and the presence or absence\nof image reconstruction, on the learning effectiveness of the\nagent.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n18\nTABLE 5\nGame scores and human-normalized scores (HNS in %) of world-model-based game agents on the 26 games in the the Atari [14] 100k benchmark\n[105]. The last two columns are the scores achieved by DeepMind human gamers and the scores achieved by a random agent. The highest score of\neach row is bolded.\nSimPLe\n[105]\nDreamerV3\n[74]\nIRIS\n[146]\nTWM\n[175]\nSTORM\n[247]\nHarmonyDreamer\n[141]\nHuman\nRandom\nAlien\n616.9 (5.6%)\n959.0 (10.6%)\n420.0 (2.8%)\n674.6 (6.5%)\n984.0 (11.0%)\n890.0 (9.6%)\n7127.7 (100.0%)\n227.8 (0.0%)\nAmidar\n74.3 (4.0%)\n139.0 (7.8%)\n143.0 (8.0%)\n121.8 (6.8%)\n205.0 (11.6%)\n141.0 (7.9%)\n1719.5 (100.0%)\n5.8 (0.0%)\nAssault\n527.2 (58.7%)\n706.0 (93.1%)\n1524.4 (250.6%)\n682.6 (88.6%)\n801.0 (111.4%)\n1003.0 (150.2%)\n742.0 (100.0%)\n222.4 (0.0%)\nAsterix\n1128.3 (11.1%)\n932.0 (8.7%)\n853.6 (7.8%)\n1116.6 (10.9%)\n1028.0 (9.9%)\n1140.0 (11.2%)\n8503.3 (100.0%)\n210.0 (0.0%)\nBankHeist\n34.2 (2.7%)\n649.0 (85.9%)\n53.1 (5.3%)\n466.7 (61.2%)\n641.0 (84.8%)\n1069.0 (142.8%)\n753.1 (100.0%)\n14.2 (0.0%)\nBattleZone\n4031.2 (4.8%)\n12250.0 (28.4%)\n13074.0 (30.8%)\n5068.0 (7.8%)\n13540.0 (32.1%)\n16456.0 (40.5%)\n37187.5 (100.0%)\n2360.0 (0.0%)\nBoxing\n7.8 (64.2%)\n78.0 (649.2%)\n70.1 (583.3%)\n77.5 (645.0%)\n80.0 (665.8%)\n80.0 (665.8%)\n12.1 (100.0%)\n0.1 (0.0%)\nBreakout\n16.4 (51.0%)\n31.0 (101.7%)\n83.7 (284.7%)\n20.0 (63.5%)\n16.0 (49.7%)\n53.0 (178.1%)\n30.5 (100.0%)\n1.7 (0.0%)\nChopperCommand\n979.4 (2.6%)\n420.0 (-5.9%)\n1565.0 (11.5%)\n1697.4 (13.5%)\n1888.0 (16.4%)\n1510.0 (10.6%)\n7387.8 (100.0%)\n811.0 (0.0%)\nCrazyClimber\n62583.6 (206.8%)\n97190.0 (345.0%)\n59324.2 (193.8%)\n71820.4 (243.7%)\n66776.0 (223.5%)\n82739.0 (287.3%)\n35829.4 (100.0%)\n10780.5 (0.0%)\nDemonAttack\n208.1 (3.1%)\n303.0 (8.3%)\n2034.4 (103.5%)\n350.2 (10.9%)\n165.0 (0.7%)\n203.0 (2.8%)\n1971.0 (100.0%)\n152.1 (0.0%)\nFreeway\n16.7 (56.4%)\n0.0 (0.0%)\n31.1 (105.1%)\n24.3 (82.1%)\n34.0 (114.9%)\n0.0 (0.0%)\n29.6 (100.0%)\n0.0 (0.0%)\nFrostbite\n236.9 (4.0%)\n909.0 (19.8%)\n259.1 (4.5%)\n1475.6 (33.0%)\n1316.0 (29.3%)\n679.0 (14.4%)\n4334.7 (100.0%)\n65.2 (0.0%)\nGopher\n596.8 (15.7%)\n3730.0 (161.1%)\n2236.1 (91.8%)\n1674.8 (65.8%)\n8240.0 (370.4%)\n13043.0 (593.3%)\n2412.5 (100.0%)\n257.6 (0.0%)\nHero\n2656.6 (5.5%)\n11161.0 (34.0%)\n7037.4 (20.2%)\n7254.0 (20.9%)\n11044.0 (33.6%)\n13378.0 (41.4%)\n30826.4 (100.0%)\n1027.0 (0.0%)\nJamesbond\n100.5 (26.1%)\n445.0 (151.9%)\n462.7 (158.4%)\n362.4 (121.8%)\n509.0 (175.3%)\n317.0 (105.2%)\n302.8 (100.0%)\n29.0 (0.0%)\nKangaroo\n51.2 (0.0%)\n4098.0 (135.6%)\n838.2 (26.4%)\n1240.0 (39.8%)\n4208.0 (139.3%)\n5118.0 (169.8%)\n3035.0 (100.0%)\n52.0 (0.0%)\nKrull\n2204.8 (56.8%)\n7782.0 (579.3%)\n6616.4 (470.1%)\n6349.2 (445.1%)\n8413.0 (638.4%)\n7754.0 (576.7%)\n2665.5 (100.0%)\n1598.0 (0.0%)\nKungFuMaster\n14862.5 (65.0%)\n21420.0 (94.1%)\n21759.8 (95.7%)\n24554.6 (108.1%)\n26182.0 (115.3%)\n22274.0 (97.9%)\n22736.3 (100.0%)\n258.5 (0.0%)\nMsPacman\n1480.0 (17.6%)\n1327.0 (15.3%)\n999.1 (10.4%)\n1588.4 (19.3%)\n2673.0 (35.6%)\n1681.0 (20.7%)\n6951.6 (100.0%)\n307.3 (0.0%)\nPong\n12.8 (94.9%)\n18.0 (109.6%)\n14.6 (100.0%)\n18.8 (111.9%)\n11.0 (89.8%)\n19.0 (112.5%)\n14.6 (100.0%)\n-20.7 (0.0%)\nPrivateEye\n35.0 (0.0%)\n882.0 (1.2%)\n100.0 (0.1%)\n86.6 (0.1%)\n7781.0 (11.2%)\n2932.0 (4.2%)\n69571.3 (100.0%)\n24.9 (0.0%)\nQbert\n1288.8 (8.5%)\n3405.0 (24.4%)\n745.7 (4.4%)\n3330.8 (23.8%)\n4522.0 (32.8%)\n3933.0 (28.4%)\n13455.0 (100.0%)\n163.9 (0.0%)\nRoadRunner\n5640.6 (71.9%)\n15565.0 (198.6%)\n9614.6 (122.6%)\n9109.0 (116.1%)\n17564.0 (224.1%)\n14646.0 (186.8%)\n7845.0 (100.0%)\n11.5 (0.0%)\nSeaquest\n683.3 (1.5%)\n618.0 (1.3%)\n661.3 (1.4%)\n774.4 (1.7%)\n525.0 (1.1%)\n665.0 (1.4%)\n42054.7 (100.0%)\n68.4 (0.0%)\nUpNDown\n3350.3 (25.2%)\n7667.0 (63.9%)\n3546.2 (27.0%)\n15981.7 (138.4%)\n7985.0 (66.8%)\n10874.0 (92.7%)\n11693.2 (100.0%)\n533.4 (0.0%)\nAvg. HNS\n33.2%\n112.4%\n104.6%\n95.6%\n126.7%\n136.6%\n100.0%\n0.0%\nTABLE 6\nThe performance (Episode Return) of some world-model-based agents\nin different robot tasks. This table is reported by Okada et al. [158].\nEpisode Return is defined as the sum of all rewards earned by the agent\nin a full episode. The highest value of each row is bolded.\nDreamer\n[72]\nDreamerV2\n[73]\nDreaming\n[157]\nDreamingV2\n[158]\nDreamerPro\n[41]\n3D Robot-arm tasks from Dreaming [157] and RoboSuite [260]\nUR5-reach\n701\u00b1223\n704\u00b1222\n752\u00b11178\n776\u00b1194\n668\u00b1252\nLift\n134\u00b146\n165\u00b1126\n174\u00b1107\n327\u00b1150\n138\u00b164\nDoor\n154\u00b132\n190\u00b1126\n319\u00b1173\n383\u00b1143\n111\u00b1110\nPegInHole\n354\u00b147\n376\u00b159\n353\u00b150\n436\u00b126\n327\u00b143\nDifficult pole-swingup tasks from DMC [195]\nAcrobot-swingup\n382\u00b1147\n309\u00b1131\n359\u00b1111\n470\u00b1129\n-\nCartpole-two-poles\n256\u00b165\n248\u00b1103\n273\u00b153\n308\u00b155\n-\n3D robot tasks from DMC [195]\nQuadruped-walk\n242\u00b1120\n350\u00b189\n379\u00b1189\n492\u00b1127\n-\nQuadruped-run\n269\u00b1114\n352\u00b168\n339\u00b1128\n385\u00b191\n-\nReach-duplo\n5\u00b111\n149\u00b162\n145\u00b161\n199\u00b143\n87\u00b176\n2D robot tasks from DMC [195]\nCheetah-run\n776\u00b1120\n811\u00b175\n542\u00b1132\n768\u00b124\n-\nWalker-walk\n906\u00b170\n951\u00b128\n518\u00b176\n857\u00b1115\n-\nReacher-easy\n658\u00b1429\n923\u00b1215\n947\u00b1100\n924\u00b1210\n-\nReacher-hard\n247\u00b1392\n175\u00b1340\n743\u00b1346\n598\u00b1447\n-\nFinger-turn-easy\n665\u00b1430\n498\u00b1469\n842\u00b1286\n434\u00b1469\n-\nFinger-turn-hard\n533\u00b1426\n600\u00b1417\n858\u00b1210\n484\u00b1434\n-\n5\nDISCUSSION\nDespite the recent surge in research on general world\nmodels [23], [69] and specific applications in areas like\nautonomous driving [91], [103], [139], [152], [209], [212],\n[231], [246], [252] and robotics [72], [73], [74], [222], numerous\nchallenges and opportunities await further exploration. In\nthis section, we delve into the intricate challenges faced by\ngeneral world models and their current technical constraint,\nalongside envisioning potential future directions for their\ndevelopment. Additionally, we explore the unique challenges\nand promising avenues in the fields of autonomous driving\nand autonomous agents. Furthermore, we reflect on the eth-\nical and safety considerations arising from the deployment\nof these models.\n5.1\nGeneral World Models\nGeneral world models aim to represent and simulate a wide\nrange of situations and interactions, like those encountered\nin the real world. Recent advancements in generative models\nhave greatly improved video generation quality. Notably,\nSora can create high-definition videos up to one minute in\nlength, closely mimicking the physical world, showing great\npotential for general world models. However, it\u2019s crucial to\naddress existing issues and challenges for future progress.\n5.1.1\nChallenges\nVideo generation is not synonymous with world models.\nWhile video generation may serve as one manifestation of\nworld models, it does not fully address the core challenges\ninherent to world models. We will discuss several challenges\nthat we deem important for world models in the following.\nCausal Reasoning. As a predictive model, the essence of\nworld modeling lies in its capacity for reasoning. The model\nshould be capable of inferring outcomes of decisions never\nencountered before, rather than solely making predictions\nwithin known data distributions. As discussed in [163] and\nillustrated in Figure 7, we expect world models to possess the\nability of counterfactual reasoning, whereby outcomes are\ninferred through rational imagining. This ability is inherently\nhuman but remains a challenging task for current AI systems.\nFor example, imagine an autonomous vehicle facing a\nsudden traffic accident or a robot in a new environment.\nA world model with counterfactual reasoning can simulate\ndifferent actions they could take, predict outcomes, and\nchoose the safest response\u2014even in new situations. This\nwould significantly improve autonomous systems\u2019 decision-\nmaking, helping them handle new and complex scenarios.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n19\nFig. 7. The three level hierarchy of intelligence [163]. World models are\nexpected to conduct counterfactual reasoning.\nPhysical Laws. Although Sora\u2019s video generation is impres-\nsive, it\u2019s argued to fall short as a world model because\nit doesn\u2019t fully comply with physical laws. Realism, seen\nin Sora\u2019s videos, isn\u2019t the same as reality, which demands\nstrict obedience to physical laws like gravity, light inter-\naction, and fluid dynamics. While Sora has improved in\nmodeling movement, including pedestrians and rigid body\nmovements [52], it still struggles with accurately simulating\nfluids and complex physical phenomena. Training Sora with\njust video-text pairs isn\u2019t enough to grasp these complexities.\nUnderstanding physical laws often requires specific observa-\ntions, suggesting that combining Sora with physics-driven\nsimulators could be beneficial. Although these simulators\nmight not achieve Sora\u2019s level of realism, they correctly\nfollow physical properties.\nGeneralization. Generalization capability is a crucial aspect\nof world models, emphasizing not only data interpolation\nbut, more importantly, data extrapolation. For instance, in\nautonomous driving, real-life accidents or abnormal driving\nbehaviors are rare occurrences. Therefore, can the learned\nworld model imagine these rare driving events? This requires\nthe model to move beyond simply memorizing the training\ndata and instead develop a robust understanding of the\nunderlying principles governing driving dynamics and road\nscenarios. By extrapolating from known data and simulating\na wide range of potential situations, the world model can\nbetter prepare autonomous vehicles to navigate safely in the\nreal world, even in unfamiliar or unexpected circumstances.\nComputational Efficiency. Efficiency in video generation\nis currently a significant limitation. To maintain consis-\ntency in video generation, autoregressive methods are often\nemployed, leading to a considerable increase in generation\ntime. Based on the news and anaylysis in the internet,\nSora may take over about one hour to generate a video\nwith one minute length. Although a series of distillation-\nbased methods [31], [140] have emerged in image generation,\nyielding significant acceleration in performance, research in\nthe field of video generation remains limited.\nEvaluation System. Current world models are predomi-\nnantly based on generative model research, with evaluation\nmetrics primarily focusing on the quality of generation, such\nas FID [83] and FVD [203]. Additionally, there are some\nworks proposing more comprehensive evaluation bench-\nmarks, such as CLIPScore [81], T2VScore [221], VBench [97],\nEvalCrafter [133], PEEKABOO [98], and others. However,\ngeneration metrics alone cannot reflect the predictive ratio-\nnality of world models. This highlights the need for human-\ncentric evaluation [36], which measures whether the gener-\nated videos meet users\u2019 expectations or align with human\nreasoning. By incorporating human feedback, evaluations\nbecome more comprehensive, considering realism, coherence,\nand relevance. This approach also offers insights into real-\nworld utility, guiding further development and refinement\nfor practical applications.\n5.1.2\nFuture Perspectives\nDespite the acclaimed success of recent world model studies,\nand considering some of the core challenges we discussed\nbefore, we believe that future research on world models can\nstep further in the following directions.\n3D World Simulator. Video generation has advanced signif-\nicantly in simulating various aspects of the world, but the\nworld exists fundamentally in three dimensions. Therefore,\nfuture world models should possess the capability to predict\nand comprehend 3D spatial environments. This involves not\nonly capturing the visual appearance of objects and scenes\nbut also encoding their spatial relationships, depth informa-\ntion, and volumetric properties. Extending world models\ninto three-dimensional space can enable more immersive\nand realistic simulations, facilitating applications in virtual\nreality [117], augmented reality, robotics, and autonomous\nsystems. Moreover, 3D world models can enhance the ability\nto interpret and interact with the physical world.\nWorld Models for Embodied Intelligence. World models for\nembodied intelligence [206] involve creating comprehensive\nrepresentations of the environment that an agent interacts\nwith. This implies that world models can serve as simulators\nto train embodied agents\u2019 decision-making processes, as\ndemonstrated by Drive-WM\u2019s [212] preliminary attempts\nin the field of autonomous driving. Moreover, integration\nwith embodied intelligence enriches their direct interaction\nwith the environment, significantly enhancing machines\u2019\nunderstanding of and adaptability to the physical world.\n5.2\nWorld Models for Autonomous Driving\nWhile extensive research has been conducted on world\nmodels in autonomous driving, the current state of world\nmodels remains rudimentary compared to the comprehen-\nsive mental world models possessed by a skilled human\ndriver. Significant challenges persist in areas like action con-\ntrollability, 3D consistency, and overcoming data limitations.\nNevertheless, we hold firm in the belief that the foundational\nmodel for autonomous driving will be based on world\nmodels, enabling effective interaction and comprehensive\nunderstanding of the physical world.\n5.2.1\nChallenges\nAction Controllability. In the realm of autonomous driving,\nthe emphasis is on action-conditioned generation rather\nthan text-conditioned video generation. While this area has\ngarnered attention, only a handful of studies have delved\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n20\ninto it. For instance, GAIA-1 [91] and DriveDreamer [209]\nfocus on steering and throttle conditioning, while Drive-\nWM [212] utilizes planning trajectories for better integration\nwith end-to-end driving systems. However, achieving fine-\ngrained control over actions remains highly challenging. For\ninstance, when attempting to control a vehicle to perform\nunconventional maneuvers such as high-speed turns or U-\nturns, the quality of generation noticeably deteriorates. This\nlimitation is also influenced by the distribution of normal\ndata. Actions, being continuous variables, pose difficulty\nin learning their latent space representations from limited\ndata samples. Current methods are only capable of achieving\ncoarse motion control, emphasizing the considerable gap that\nstill exists towards achieving fine-grained control.\n3D Consistency. 3D consistency is crucial for autonomous\ndriving. Although current video generation techniques may\nappear realistic, ensuring their 3D consistency is challenging,\nthus compromising the reliability of world model generation.\nHowever, if the world model is to be truly applied, the\nability to consistently generate 3D spaces must be further\nimproved. While the Sora team believes that scaling up\ncan enable models to learn 3D consistency from videos,\nthis implicit learning approach is obviously less secure for\nautonomous driving. Given the abundance of sensors in\nautonomous vehicles, world models can extend beyond mere\nvideo generation. For instance, conditioning on point clouds\nor occupancy grids can significantly enhance 3D consistency.\nData Limitations. Data plays a crucial role in training\nfoundation models. Unlike the readily available image and\ntext data on the internet, autonomous driving encounters\nsignificant challenges in data collection, making world\nmodel construction exceedingly difficult. Firstly, autonomous\ndriving data collection differs substantially from human\nlearning due to fixed sensor positions. Humans learn about\nthe world\u2019s physics through passive observation and active\ninteraction, while autonomous vehicles lack this flexibility.\nUnderstanding the consequences of the ego-agent\u2019s actions\non the environment is vital for reasoning about inter-\nactions. However, such data is often scarce or hard to\nobtain, presenting a significant challenge in world model\nconstruction. Secondly, privacy concerns and commercial\ncompetition often deter automotive companies from sharing\ntheir autonomous driving data. This not only limits the scale\nof available data but also restricts its diversity. Lastly, data col-\nlection typically exhibits a long-tail distribution, emphasizing\nthe importance of rare scenarios that are nonetheless crucial\nfor autonomous driving. Therefore, the efficient selection\nof such data remains a challenging and unresolved issue.\nWhile GenAD [231] has explored training world models\nusing internet data, the effectiveness remains preliminary.\nAddressing these data limitation issues will facilitate research\non autonomous driving world models.\n5.2.2\nFuture Perspectives\nEnd-to-end Foundation Driving Models. The world model\nis crucial for building the end-to-end foundation model for\nautonomous driving. As a simulator of the real world, it can\nnot only provide high-quality data but also enable a closed-\nloop training environment for decision-making. Although\nthe driving domain is more restricted compared to general\nscenarios, it involves rich interactions and an understanding\nof spatial and temporal information, which are currently\nlacking in text-based video generation models. Currently,\nthe world model for autonomous driving is still far from\nachieving this goal. The best model, GAIA-1 [91], is trained\non 4,700 hours video data, akin to GPT predicting the next\ntoken. However, with a model size of 9B, it still falls far\nshort compared to large language models. However, the shift\ntowards big data-driven autonomous driving is undoubtedly\nan inevitable trend. Models will increasingly comprehend\nreality and grasp the rules and techniques of driving from\ndata, rather than relying solely on manually designed rules.\nIn this regard, Tesla\u2019s FSD beta 12.3 has demonstrated\namazing driving capabilities, offering a glimpse of hope\nfor the future end-to-end foundation model of driving.\nReal-world Driving Simulators. While many end-to-end\nautonomous driving methods are under research in the\nCARLA simulator, the inherent disparities between sim-\nulated and real-world environments present significant\nchallenges. This highlights the necessity of constructing more\nrealistic real-world driving simulations in the future. Lever-\naging the robust predictive capabilities of world models, we\ncan create even more realistic driving simulators that extend\nbeyond mere video generation. Such simulators must also\nfocus on aspects like scene layout control, lighting control,\nand object manipulation. Furthermore, world models can be\nseamlessly integrated with previous simulation [234] efforts\nbased on MVS [216], [217], NeRF [148], and 3D Gaussian\nSplatting [107], thereby enhancing the scene generalization\ncapabilities of existing methods. By utilizing more realistic\ndriving simulators for model training, it can greatly facilitate\nthe deployment of autonomous driving systems that perform\nreliably in practical settings [258].\n5.3\nWorld Models for Autonomous Agents\nAutonomous agents encompass both physical robots in the\nreal world and intelligent agents in digital environments.\nWorld models have the capability to simulate not only\nthe intricate complexities of the physical world but also\nthe nuances of digital environments. From the perspective\nof autonomous agents, world models present some new\nchallenges and opportunities.\n5.3.1\nChallenges\nUnderstand the Environment Dynamics. Agents need to\nunderstand their environments to function effectively. For\nphysical robots, this means grappling with the complex and\noften uncertain dynamics of the physical world, a task made\ndifficult by limited observations and the probabilistic nature\nof real-world changes. Unlike robots, humans navigate this\ncomplexity well thanks to multisensory perception, genetic\nknowledge, and the ability to learn from experience and\nshare knowledge. To enhance an agent\u2019s understanding\nof its environment, we can draw inspiration from human\ncapabilities in three ways: First, by enhancing multimodal\nperception, allowing agents to gather more comprehensive\ninformation through integrated models that encompass\nvision, sound, and touch. Examples of this approach include\nthe development of large language models like GPT-4V [1]\nand Gemini [196]. Second, leveraging extensive internet\ndata for unsupervised learning can aid agents in acquiring\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n21\nfundamental cognitive abilities. Lastly, advancing and dis-\nseminating sophisticated knowledge through systems like\nLeCun\u2019s multi-level knowledge induction [115] facilitates\nagents in rapidly attaining a deeper understanding of their\nenvironment.\nTask Generalization. Agents in real-world applications\nfrequently encounter a range of diverse tasks, necessitating\nworld models that can not only handle familiar tasks but\nalso generalize effectively to novel, unseen ones. This task\ngeneralization capability is crucial for agents, yet current\nrobots still face significant challenges in this regard. The\nmajority of robots today are specialized models, tailored to\nperform specific functions such as sweeping, transporting,\ncooking, and the like, limiting their adaptability and versa-\ntility in handling a wider range of tasks. This implies that\nlearning world models cannot solely rely on imitation and\ngeneration; rather, it is essential to abstract common sense\nfrom diverse tasks. Such common sense enables agents to\nmigrate and comprehend different tasks more easily. Mere\nreliance on big data learning is an inefficient and poorly\ngeneralizable approach. This is analogous to the concept of\nmeta-learning, where meta-learning methods train agents\nto learn how to learn, enabling them to quickly adapt to\nnew tasks. Additionally, multi-task learning frameworks\nempower agents to train on multiple tasks simultaneously,\nidentifying and leveraging the commonalities between them.\n5.3.2\nFuture Perspectives\nKnowledge Injection through Large Language Model. LLM\nhas demonstrated astonishing comprehension abilities over\nthe past two years. Through the language learning, the model\nhas acquired a certain amount of knowledge about the world.\nLeveraging this accumulated knowledge, the LLM can serve\nas a prior for world models, enabling the model to learn\ndifferent tasks more efficiently. Just like humans, world\nmodels initially envision scenarios based on their preexisting\nknowledge and subsequently refine their understanding\nthrough feedback obtained from the actual environment.\nWe believe that the integration of world models with large\nlanguage models represents one of the promising directions\nfor future development.\nReal-world Application. While the Dreamer series of algo-\nrithms [72], [73], [74] has shown promise in learning from\nlimited interaction through planning within simulated envi-\nronments and gaming scenarios, their application in real-\nworld robotics remains largely unexplored [222]. However,\nthe transition from simulation to reality is an inevitable\ndirection for future research. The real world introduces addi-\ntional uncertainties, including observation errors and control\nprecision, making it crucial to investigate the effectiveness of\nworld models for physical robots in real-world settings.\n5.4\nEthical and Safety Concerns\nThe main concerns surrounding tools like Sora revolve\naround their safety and ethical impacts.\nModel Accountability. As a powerful predictive model,\nensuring the reliability of world model predictions is a\ncritical concern. Accountability measures are indispensable\nto validate the accuracy and fairness of model outputs,\nparticularly considering their potential impact on decision-\nmaking processes. For instance, in autonomous driving,\nthe reliability of world model predictions is essential for\nensuring safety. Moreover, accountability measures should\nalso address issues of fairness, ensuring that world models\ndo not exhibit biases [254] that could disproportionately\nimpact certain groups or communities.\nDisinformation. Hyper-realistic videos generated by visual\ngenerative AI present an alarming threat, particularly in\ntheir potential to create emotionally manipulative content\nthat spreads misinformation, especially during critical events\nlike elections. The proliferation of fabricated videos depicting\npoliticians in fictitious scenarios significantly distorts public\nopinion. Furthermore, this misinformation seeps into edu-\ncation, making it harder to distinguish reality from false-\nhood. Tackling this issue demands collaborative action from\ngovernments, technology firms, media organizations, and\ncivil society to establish a reliable information dissemination\nsystem.\nData Privacy. The abundance of data undoubtedly propels\nthe rapid development of large foundation models, but it\nalso raises concerns about privacy protection. In a recent\nstudy [190], mainstream large language models like GPT-4,\nLlama-2, and Claude-2 were used to infer specific privacy\ndatasets. It was found that these large models could auto-\nmatically deduce various real privacy data hidden within\nthe text content analyzed from users\u2019 texts alone. Compared\nto textual data, the internet holds a vast amount of video\ndata and continues to update daily, which requires even\nmore attention to privacy concerns. Especially for large-scale\nmodels used in video generation, it is essential to disclose\nthe sources of training videos to prevent personal privacy\ndata from being unknowingly used for training. Additionally,\ncorresponding laws and policies should be established to\nclarify the protection and requirements of personal privacy\ndata.\n6\nCONCLUSION\nIn this survey, we conduct a comprehensive review of general\nworld models, underlining their pivotal importance in the\npursuit of AGI and their fundamental applications across a\nmyriad of domains, from immersive virtual environments\nto sophisticated decision-making systems. Through our\nexamination, the emergence of the Sora model is highlighted\nfor its unparalleled simulation capabilities and nascent\nunderstanding of physical principles, marking a significant\nmilestone in the evolution of world models. We delve\ndeeply into the current innovations, with a particular focus\non the application of world models for video generation,\nautonomous driving, and the operation of autonomous\nagents. Despite the progress and promising prospects, we\nalso critically evaluate the challenges and limitations facing\ncurrent world model methodologies, contemplating their\ncomplexity, ethical considerations, and scalability. This com-\nprehensive review not only showcases the current state\nand potential of world models but also illuminates the\npath toward their future development and application. We\nhope this survey can inspire the community toward novel\nsolutions, thereby broadening the horizon for world models\nand their applications in shaping the future of AGI.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n22\nREFERENCES\n[1]\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad,\nIlge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko\nAltenschmidt, Sam Altman, Shyamal Anadkat, et al.\nGpt-4\ntechnical report. arXiv preprint arXiv:2303.08774, 2023.\n[2]\nLisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic,\nTrevor Darrell, and Bryan Russell. Localizing moments in video\nwith natural language. In ICCV, 2017.\n[3]\nAnurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun,\nMario Lu\u02c7ci\u00b4c, and Cordelia Schmid. Vivit: A video vision trans-\nformer. In ICCV, 2021.\n[4]\nMahmoud Assran, Quentin Duval, Ishan Misra, Piotr Bojanowski,\nPascal Vincent, Michael Rabbat, Yann LeCun, and Nicolas Ballas.\nSelf-supervised learning from images with a joint-embedding\npredictive architecture. In CVPR, 2023.\n[5]\nYutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar,\nAlan Yuille, Trevor Darrell, Jitendra Malik, and Alexei A Efros.\nSequential modeling enables scalable learning for large vision\nmodels. arXiv preprint arXiv:2312.00785, 2023.\n[6]\nMax Bain, Arsha Nagrani, G\u00a8ul Varol, and Andrew Zisserman.\nFrozen in time: A joint video and image encoder for end-to-end\nretrieval. In ICCV, 2021.\n[7]\nYogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa,\nand Hans Peter Graf. Conditional gan with discriminative filter\ngeneration for text-to-video synthesis. In IJCAI, 2019.\n[8]\nAndrea Banino, Adri`a Puigdom`enech Badia, Raphael K\u00a8oster,\nMartin J. Chadwick, Vin\u00b4\u0131cius Flores Zambaldi, Demis Hassabis,\nCaswell Barry, Matthew M. Botvinick, Dharshan Kumaran, and\nCharles Blundell. MEMO: A deep network for flexible combina-\ntion of episodic memories. In ICLR, 2020.\n[9]\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su,\nand Jun Zhu. All are worth words: A vit backbone for diffusion\nmodels. In CVPR, 2023.\n[10]\nFan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi Pu, Yaole\nWang, Gang Yue, Yue Cao, Hang Su, and Jun Zhu.\nOne\ntransformer fits all distributions in multi-modal diffusion at scale.\nIn ICML, 2023.\n[11]\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-\ntraining of image transformers. arXiv preprint arXiv:2106.08254,\n2021.\n[12]\nAdrien Bardes, Quentin Garrido, Jean Ponce, Xinlei Chen, Michael\nRabbat, Yann LeCun, Mahmoud Assran, and Nicolas Ballas.\nRevisiting feature prediction for learning visual representations\nfrom video, 2024.\n[13]\nAdrien Bardes, Jean Ponce, and Yann LeCun. Mc-jepa: A joint-\nembedding predictive architecture for self-supervised learning of\nmotion and content features, 2023.\n[14]\nMarc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael\nBowling.\nThe arcade learning environment: An evaluation\nplatform for general agents. J. Artif. Int. Res., 2013.\n[15]\nGedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-\ntime attention all you need for video understanding? In ICML,\n2021.\n[16]\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang,\nLinjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo,\net al. Improving image generation with better captions. Computer\nScience, 2023.\n[17]\nAndreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel\nMendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion\nEnglish, Vikram Voleti, Adam Letts, et al. Stable video diffusion:\nScaling latent video diffusion models to large datasets. arXiv\npreprint arXiv:2311.15127, 2023.\n[18]\nAndreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn,\nSeung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your\nlatents: High-resolution video synthesis with latent diffusion\nmodels. In CVPR, 2023.\n[19]\nDaniel Bogdoll, Yitian Yang, and J Marius Z\u00a8ollner.\nMuvo: A\nmultimodal generative world model for autonomous driving with\ngeometric representations. arXiv preprint arXiv:2311.11762, 2023.\n[20]\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale\ngan training for high fidelity natural image synthesis.\narXiv\npreprint arXiv:1809.11096, 2018.\n[21]\nTim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo,\nLi Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman,\nClarence Ng, Ricky Wang, and Aditya Ramesh. Video generation\nmodels as world simulators. 2024.\n[22]\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah,\nJared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav\nShyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. NeurIPS, 2020.\n[23]\nJake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder,\nYuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie\nSteigerwald, Chris Apps, Yusuf Aytar, Sarah Bechtle, Feryal\nBehbahani, Stephanie Chan, Nicolas Heess, Lucy Gonzalez, Simon\nOsindero, Sherjil Ozair, Scott Reed, Jingwei Zhang, Konrad Zolna,\nJeff Clune, Nando de Freitas, Satinder Singh, and Tim Rockt\u00a8aschel.\nGenie: Generative interactive environments, 2024.\n[24]\nFabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan\nCarlos Niebles. Activitynet: A large-scale video benchmark for\nhuman activity understanding. In CVPR, 2015.\n[25]\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora,\nVenice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo\nBaldan, and Oscar Beijbom. nuscenes: A multimodal dataset for\nautonomous driving. In CVPR, 2020.\n[26]\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end\nobject detection with transformers. In ECCV, 2020.\n[27]\nJoao Carreira and Andrew Zisserman. Quo vadis, action recogni-\ntion? a new model and the kinetics dataset. In CVPR, 2017.\n[28]\nHuiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T\nFreeman. Maskgit: Masked generative image transformer. In\nCVPR, 2022.\n[29]\nChang Chen, Yi-Fu Wu, Jaesik Yoon, and Sungjin Ahn. Trans-\ndreamer: Reinforcement learning with transformer world models,\n2022.\n[30]\nDavid Chen and William B Dolan. Collecting highly parallel data\nfor paraphrase evaluation. In ACL, 2011.\n[31]\nJunsong Chen, Yue Wu, Simian Luo, Enze Xie, Sayak Paul, Ping\nLuo, Hang Zhao, and Zhenguo Li.\nPixart-{\\delta}: Fast and\ncontrollable image generation with latent consistency models.\narXiv preprint arXiv:2401.05252, 2024.\n[32]\nJunsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie,\nYue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu,\net al. Pixart-alpha: Fast training of diffusion transformer for pho-\ntorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426,\n2023.\n[33]\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo\nJun, David Luan, and Ilya Sutskever. Generative pretraining from\npixels. In ICML, 2020.\n[34]\nTsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekate-\nrina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang,\nHsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m:\nCaptioning 70m videos with multiple cross-modality teachers.\narXiv preprint arXiv:2402.19479, 2024.\n[35]\nSilvia Chiappa, S\u00b4ebastien Racani`ere, Daan Wierstra, and Shakir\nMohamed. Recurrent environment simulators. In ICLR, 2017.\n[36]\nJoseph Cho, Fachrina Dewi Puspitasari, Sheng Zheng, Jingyao\nZheng, Lik-Hang Lee, Tae-Ho Kim, Choong Seon Hong, and\nChaoning Zhang. Sora as an agi world model? a complete survey\non text-to-video generation. arXiv preprint arXiv:2403.05131, 2024.\n[37]\nOpenScene Contributors. Openscene: The largest up-to-date 3d\noccupancy prediction benchmark in autonomous driving. https:\n//github.com/OpenDriveLab/OpenScene, 2023.\n[38]\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le,\nand Ruslan Salakhutdinov. Transformer-XL: Attentive language\nmodels beyond a fixed-length context. In ACL, 2019.\n[39]\nMarc Peter Deisenroth and Carl Edward Rasmussen. Pilco: a\nmodel-based and data-efficient approach to policy search. In\nICML, 2011.\n[40]\nUgur Demir and Gozde Unal. Patch-based image inpainting with\ngenerative adversarial networks. arXiv preprint arXiv:1803.07422,\n2018.\n[41]\nFei Deng, Ingook Jang, and Sungjin Ahn.\nDreamerpro:\nReconstruction-free model-based reinforcement learning with\nprototypical representations. In ICML, 2022.\n[42]\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-\nFei. Imagenet: A large-scale hierarchical image database. In CVPR,\n2009.\n[43]\nKangle Deng, Tianyi Fei, Xin Huang, and Yuxin Peng.\nIrc-\ngan: Introspective recurrent convolutional gan for text-to-video\ngeneration. In IJCAI, 2019.\n[44]\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional transformers\nfor language understanding. arXiv preprint arXiv:1810.04805, 2018.\n[45]\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat\ngans on image synthesis. NeurIPS, 2021.\n[46]\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang\nZhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n23\nYang, et al. Cogview: Mastering text-to-image generation via\ntransformers. NeurIPS, 2021.\n[47]\nMing Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2:\nFaster and better text-to-image generation via hierarchical trans-\nformers. NeurIPS, 2022.\n[48]\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk\nWeissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa\nDehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly,\net al. An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv preprint arXiv:2010.11929, 2020.\n[49]\nAlexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez,\nand Vladlen Koltun. Carla: An open urban driving simulator. In\nCoRL, 2017.\n[50]\nYilun Du, Sherry Yang, Bo Dai, Hanjun Dai, Ofir Nachum, Josh\nTenenbaum, Dale Schuurmans, and Pieter Abbeel.\nLearning\nuniversal policies via text-guided video generation.\nNeurIPS,\n2024.\n[51]\nLasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan,\nVlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley,\nIain Dunning, Shane Legg, and Koray Kavukcuoglu. IMPALA:\nScalable distributed deep-RL with importance weighted actor-\nlearner architectures. In ICML, 2018.\n[52]\nPatrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan\nGranskog, and Anastasis Germanidis.\nStructure and content-\nguided video synthesis with diffusion models. In ICCV, 2023.\n[53]\nGongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning\nfor diffusion models. NeurIPS, 2024.\n[54]\nZhengcong Fei, Mingyuan Fan, and Junshi Huang. A-jepa: Joint-\nembedding predictive architecture can listen, 2024.\n[55]\nChristoph Feichtenhofer. X3d: Expanding architectures for effi-\ncient video recognition. In CVPR, 2020.\n[56]\nChristoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming\nHe. Slowfast networks for video recognition. In ICCV, 2019.\n[57]\nCraig R Fox and G\u00a8ulden \u00a8Ulk\u00a8umen. Distinguishing two dimen-\nsions of uncertainty. SSRN Electronic Journal, 2011.\n[58]\nStan Franklin and Art Graesser. Is it an agent, or just a program?:\nA taxonomy for autonomous agents. In Intelligent Agents III Agent\nTheories, Architectures, and Languages, 1997.\n[59]\nYarin Gal, Rowan McAllister, and Carl Edward Rasmussen.\nImproving PILCO with Bayesian neural network dynamics\nmodels. In Data-Efficient Machine Learning workshop, ICML, 2016.\n[60]\nZeyu Gao, Yao Mu, Ruoyan Shen, Chen Chen, Yangang Ren,\nJianyu Chen, Shengbo Eben Li, Ping Luo, and Yanfeng Lu.\nEnhance sample efficiency and robustness of end-to-end urban\nautonomous driving via semantic masked world model. arXiv\npreprint arXiv:2210.04017, 2022.\n[61]\nAndreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready\nfor autonomous driving? the kitti vision benchmark suite. In\nCVPR, 2012.\n[62]\nAnastasis Germanidis. Introducing general world models. 2023.\n[63]\nRohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval,\nSamaneh Azadi, Sai Saketh Rambhatla, Akbar Shah, Xi Yin,\nDevi Parikh, and Ishan Misra. Emu video: Factorizing text-to-\nvideo generation by explicit image conditioning. arXiv preprint\narXiv:2311.10709, 2023.\n[64]\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu,\nDavid Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua\nBengio. Generative adversarial nets. NeurIPS, 2014.\n[65]\nJake Grigsby and Yanjun Qi. Measuring visual generalization in\ncontinuous control from pixels, 2020.\n[66]\nShuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang,\nDongdong Chen, Lu Yuan, and Baining Guo. Vector quantized\ndiffusion model for text-to-image synthesis. In CVPR, 2022.\n[67]\nYanchen Guan, Haicheng Liao, Zhenning Li, Guohui Zhang, and\nChengzhong Xu. World models for autonomous driving: An\ninitial survey. arXiv preprint arXiv:2403.02622, 2024.\n[68]\nAgrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Li\nFei-Fei, Irfan Essa, Lu Jiang, and Jos\u00b4e Lezama. Photorealistic video\ngeneration with diffusion models. arXiv preprint arXiv:2312.06662,\n2023.\n[69]\nDavid Ha and J\u201durgen Schmidhuber. Recurrent world models\nfacilitate policy evolution. In NeurIPS, 2018.\n[70]\nJeongsoo Ha, Kyungsoo Kim, and Yusung Kim.\nDream to\ngeneralize: Zero-shot model-based reinforcement learning for\nunseen visual distractions. AAAI, 2023.\n[71]\nDanijar Hafner, Timothy Lillicrap, Ian Fischer, Ruben Villegas,\nDavid Ha, Honglak Lee, and James Davidson. Learning latent\ndynamics for planning from pixels. In ICML, 2019.\n[72]\nDanijar Hafner, Timothy P. Lillicrap, Jimmy Ba, and Mohammad\nNorouzi. Dream to control: Learning behaviors by latent imagina-\ntion. In ICLR, 2020.\n[73]\nDanijar Hafner, Timothy P. Lillicrap, Mohammad Norouzi, and\nJimmy Ba. Mastering atari with discrete world models. In ICLR,\n2021.\n[74]\nDanijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap.\nMastering diverse domains through world models, 2023.\n[75]\nAli Hatamizadeh, Jiaming Song, Guilin Liu, Jan Kautz, and Arash\nVahdat. Diffit: Diffusion vision transformers for image generation.\narXiv preprint arXiv:2312.02139, 2023.\n[76]\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00b4ar,\nand Ross Girshick.\nMasked autoencoders are scalable vision\nlearners. In CVPR, 2022.\n[77]\nKaiming He, Georgia Gkioxari, Piotr Doll\u00b4ar, and Ross Girshick.\nMask r-cnn. In ICCV, 2017.\n[78]\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep\nresidual learning for image recognition. In CVPR, 2016.\n[79]\nMikael Henaff, William F. Whitney, and Yann LeCun. Model-based\nplanning with discrete and continuous actions, 2018.\n[80]\nRoberto Henschel, Levon Khachatryan, Daniil Hayrapetyan,\nHayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant\nNavasardyan, and Humphrey Shi.\nStreamingt2v: Consistent,\ndynamic, and extendable long video generation from text. arXiv\npreprint arXiv:2403.14773, 2024.\n[81]\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and\nYejin Choi. Clipscore: A reference-free evaluation metric for image\ncaptioning. arXiv preprint arXiv:2104.08718, 2021.\n[82]\nMatteo Hessel, Joseph Modayil, Hado van Hasselt, Tom\nSchaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot,\nMohammad Azar, and David Silver.\nRainbow: combining\nimprovements in deep reinforcement learning. In AAAI, 2018.\n[83]\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard\nNessler, and Sepp Hochreiter. Gans trained by a two time-scale\nupdate rule converge to a local nash equilibrium. NeurIPS, 2017.\n[84]\nJonathan Ho, William Chan, Chitwan Saharia, Jay Whang,\nRuiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole,\nMohammad Norouzi, David J Fleet, et al. Imagen video: High\ndefinition video generation with diffusion models. arXiv preprint\narXiv:2210.02303, 2022.\n[85]\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion\nprobabilistic models. NeurIPS, 2020.\n[86]\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan,\nMohammad Norouzi, and David J Fleet. Video diffusion models.\nNeurIPS, 2022.\n[87]\nSepp Hochreiter and J\u00a8urgen Schmidhuber.\nLong short-term\nmemory. Neural computation, 1997.\n[88]\nWenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang.\nCogvideo: Large-scale pretraining for text-to-video generation via\ntransformers. arXiv preprint arXiv:2205.15868, 2022.\n[89]\nAnthony Hu. Neural World Models for Computer Vision. PhD thesis,\nUniversity of Cambridge, 2022.\n[90]\nAnthony Hu, Gianluca Corrado, Nicolas Griffiths, Zachary Murez,\nCorina Gurau, Hudson Yeo, Alex Kendall, Roberto Cipolla, and\nJamie Shotton. Model-based imitation learning for urban driving.\nNeurIPS, 2022.\n[91]\nAnthony Hu, Lloyd Russell, Hudson Yeo, Zak Murez, George\nFedoseev, Alex Kendall, Jamie Shotton, and Gianluca Corrado.\nGaia-1: A generative world model for autonomous driving. arXiv\npreprint arXiv:2309.17080, 2023.\n[92]\nXiwei Hu, Rui Wang, Yixiao Fang, Bin Fu, Pei Cheng, and Gang\nYu. Ella: Equip diffusion models with llm for enhanced semantic\nalignment. arXiv preprint arXiv:2403.05135, 2024.\n[93]\nBinyuan Huang, Yuqing Wen, Yucheng Zhao, Yaosi Hu, Yingfei\nLiu, Fan Jia, Weixin Mao, Tiancai Wang, Chi Zhang, Chang Wen\nChen, et al. Subjectdrive: Scaling generative data in autonomous\ndriving via subject control. arXiv preprint arXiv:2403.19438, 2024.\n[94]\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q\nWeinberger. Densely connected convolutional networks. In CVPR,\n2017.\n[95]\nLianghua Huang, Di Chen, Yu Liu, Yujun Shen, Deli Zhao, and Jin-\ngren Zhou. Composer: Creative and controllable image synthesis\nwith composable conditions. arXiv preprint arXiv:2302.09778, 2023.\n[96]\nWeidong Huang, Jiaming Ji, Borong Zhang, Chunhe Xia, and\nYaodong Yang. Safedreamer: Safe reinforcement learning with\nworld models. In ICLR, 2024.\n[97]\nZiqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si,\nYuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin,\nNattapol Chanpaisit, et al. Vbench: Comprehensive benchmark\nsuite for video generative models. arXiv preprint arXiv:2311.17982,\n2023.\n[98]\nYash Jain, Anshul Nasery, Vibhav Vineet, and Harkirat Behl.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n24\nPeekaboo: Interactive video generation via masked-diffusion.\narXiv preprint arXiv:2312.07509, 2023.\n[99]\nAshish Jaiswal, Ashwin Ramesh Babu, Mohammad Zaki Zadeh,\nDebapriya Banerjee, and Fillia Makedon. A survey on contrastive\nself-supervised learning. Technologies, 2020.\n[100] Stephen James, Zicong Ma, David Rovick Arrojo, and Andrew J.\nDavison. Rlbench: The robot learning benchmark & learning\nenvironment. RAL, 2020.\n[101] Jiaming Ji, Borong Zhang, Jiayi Zhou, Xuehai Pan, Weidong\nHuang, Ruiyang Sun, Yiran Geng, Yifan Zhong, Josef Dai, and\nYaodong Yang. Safety gymnasium: A unified safe reinforcement\nlearning benchmark. In NeurIPS, 2023.\n[102] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional\nneural networks for human action recognition. TPAMI, 2012.\n[103] Fan Jia, Weixin Mao, Yingfei Liu, Yucheng Zhao, Yuqing Wen,\nChi Zhang, Xiangyu Zhang, and Tiancai Wang.\nAdriver-i: A\ngeneral world model for autonomous driving.\narXiv preprint\narXiv:2311.13549, 2023.\n[104] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin\nLi, Fang Wang, and Qun Liu. Tinybert: Distilling bert for natural\nlanguage understanding. arXiv preprint arXiv:1909.10351, 2019.\n[105] Lukasz Kaiser, Mohammad Babaeizadeh, Piotr Milos, Blazej\nOsinski, Roy H. Campbell, Konrad Czechowski, Dumitru Erhan,\nChelsea Finn, Piotr Kozakowski, Sergey Levine, Afroz Mohiuddin,\nRyan Sepassi, George Tucker, and Henryk Michalewski. Model\nbased reinforcement learning for atari. In ICLR, 2020.\n[106] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator\narchitecture for generative adversarial networks. In CVPR, 2019.\n[107] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00a8uhler, and\nGeorge Drettakis. 3d gaussian splatting for real-time radiance\nfield rendering. ToG, 2023.\n[108] Doyeon Kim, Donggyu Joo, and Junmo Kim. Tivgan: Text to image\nto video generation with step-by-step evolutionary generator.\nIEEE Access, 2020.\n[109] Diederik P Kingma and Max Welling. Auto-encoding variational\nbayes. arXiv preprint arXiv:1312.6114, 2013.\n[110] Jing Yu Koh, Honglak Lee, Yinfei Yang, Jason Baldridge, and Peter\nAnderson. Pathdreamer: A world model for indoor navigation.\nIn ICCV, 2021.\n[111] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jos\u00b4e Lezama, Jonathan\nHuang, Rachel Hornung, Hartwig Adam, Hassan Akbari, Yair\nAlon, Vighnesh Birodkar, et al. Videopoet: A large language model\nfor zero-shot video generation. arXiv preprint arXiv:2312.14125,\n2023.\n[112] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet\nclassification with deep convolutional neural networks. NeurIPS,\n25, 2012.\n[113] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. CURL: Con-\ntrastive unsupervised representations for reinforcement learning.\nIn ICML, 2020.\n[114] Yann LeCun, L\u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner.\nGradient-based learning applied to document recognition. Pro-\nceedings of the IEEE, 1998.\n[115] Yann LeCun and Courant. A path towards autonomous machine\nintelligence version 0.9.2, 2022-06-27. 2022.\n[116] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-\nShin Han.\nAutoregressive image generation using residual\nquantization. In CVPR, 2022.\n[117] Lik-Hang Lee, Tristan Braud, Pengyuan Zhou, Lin Wang, Dianlei\nXu, Zijun Lin, Abhishek Kumar, Carlos Bermejo, and Pan Hui.\nAll one needs to know about metaverse: A complete survey on\ntechnological singularity, virtual ecosystem, and research agenda.\narXiv preprint arXiv:2110.05352, 2021.\n[118] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, Thomas\nBreuel, Gal Chechik, and Yale Song. Acav100m: Automatic cura-\ntion of large-scale datasets for audio-visual video representation\nlearning. In ICCV, 2021.\n[119] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad,\nAbdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke\nZettlemoyer. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension.\narXiv preprint arXiv:1910.13461, 2019.\n[120] Bowen Li. Word-level fine-grained story visualization. In ECCV,\n2022.\n[121] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip:\nBootstrapping language-image pre-training for unified vision-\nlanguage understanding and generation. In ICML, 2022.\n[122] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty,\nCaiming Xiong, and Steven Chu Hong Hoi. Align before fuse:\nVision and language representation learning with momentum\ndistillation. NeurIPS, 2021.\n[123] Kunchang Li, Yali Wang, Peng Gao, Guanglu Song, Yu Liu,\nHongsheng Li, and Yu Qiao. Uniformer: Unified transformer\nfor efficient spatiotemporal representation learning. arXiv preprint\narXiv:2201.04676, 2022.\n[124] Qifeng Li, Xiaosong Jia, Shaobo Wang, and Junchi Yan.\nThink2drive: Efficient reinforcement learning by thinking in latent\nworld model for quasi-realistic autonomous driving (in carla-v2).\narXiv preprint arXiv:2402.16720, 2024.\n[125] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina\nKatabi, and Dilip Krishnan. Mage: Masked generative encoder\nto unify representation learning and image synthesis. In CVPR,\n2023.\n[126] Tianhong Li, Dina Katabi, and Kaiming He. Self-conditioned\nimage generation via generating representations. arXiv preprint\narXiv:2312.03701, 2023.\n[127] Xiaofan Li, Yifu Zhang, and Xiaoqing Ye. Drivingdiffusion: Layout-\nguided multi-view driving scene video generation with latent\ndiffusion model. arXiv preprint arXiv:2310.07771, 2023.\n[128] Yitong Li, Zhe Gan, Yelong Shen, Jingjing Liu, Yu Cheng, Yuexin\nWu, Lawrence Carin, David Carlson, and Jianfeng Gao. Storygan:\nA sequential conditional gan for story visualization. In CVPR,\n2019.\n[129] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei\nYang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen:\nOpen-set grounded text-to-image generation. In ICCV, 2023.\n[130] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and\nLawrence Carin. Video generation from text. In AAAI, 2018.\n[131] Yiyi Liao, Jun Xie, and Andreas Geiger. Kitti-360: A novel dataset\nand benchmarks for urban scene understanding in 2d and 3d.\nTPAMI, 2022.\n[132] Hongyu Liu, Ziyu Wan, Wei Huang, Yibing Song, Xintong Han,\nand Jing Liao.\nPd-gan: Probabilistic diverse gan for image\ninpainting. In CVPR, 2021.\n[133] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang,\nHaoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying\nShan.\nEvalcrafter: Benchmarking and evaluating large video\ngeneration models. arXiv preprint arXiv:2310.11440, 2023.\n[134] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li,\nSishuo Chen, Xu Sun, and Lu Hou.\nFetv: A benchmark for\nfine-grained evaluation of open-domain text-to-video generation.\nNeurIPS, 2024.\n[135] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,\nDanqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and\nVeselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[136] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi\nChen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng\nGao, et al. Sora: A review on background, technology, limita-\ntions, and opportunities of large vision models. arXiv preprint\narXiv:2402.17177, 2024.\n[137] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang,\nStephen Lin, and Baining Guo. Swin transformer: Hierarchical\nvision transformer using shifted windows. In ICCV, 2021.\n[138] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Han Hu. Video swin transformer. In CVPR, 2022.\n[139] Jiachen Lu, Ze Huang, Jiahui Zhang, Zeyu Yang, and Li Zhang.\nWovogen: World volume-aware diffusion for controllable multi-\ncamera driving scene generation. arXiv preprint arXiv:2312.02934,\n2023.\n[140] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and Hang Zhao.\nLatent consistency models: Synthesizing high-resolution images\nwith few-step inference. arXiv preprint arXiv:2310.04378, 2023.\n[141] Haoyu Ma, Jialong Wu, Ningya Feng, Chenjun Xiao, Dong Li,\nJianye Hao, Jianmin Wang, and Mingsheng Long. Harmonydream:\nTask harmonization inside world models, 2024.\n[142] Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accel-\nerating diffusion models for free. arXiv preprint arXiv:2312.00858,\n2023.\n[143] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu,\nYuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion\ntransformer for video generation. arXiv preprint arXiv:2401.03048,\n2024.\n[144] Adyasha Maharana, Darryl Hannan, and Mohit Bansal. Storydall-\ne: Adapting pretrained text-to-image transformers for story\ncontinuation. In ECCV, 2022.\n[145] Russell Mendonca, Shikhar Bahl, and Deepak Pathak. Structured\nWorld Models from Human Videos. In RSS, 2023.\n[146] Vincent Micheli, Eloi Alonso, and Franc\u00b8ois Fleuret. Transformers\nare sample-efficient world models. In ICLR, 2023.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n25\n[147] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand\nTapaswi, Ivan Laptev, and Josef Sivic. Howto100m: Learning\na text-video embedding by watching hundred million narrated\nvideo clips. In ICCV, 2019.\n[148] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T\nBarron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing\nscenes as neural radiance fields for view synthesis. Communications\nof the ACM, 2021.\n[149] Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, and Bin\nDai.\nOccupancy-mae: Self-supervised pre-training large-scale\nlidar point clouds with masked occupancy autoencoders. IEEE\nTransactions on Intelligent Vehicles, 2023.\n[150] Chen Min, Liang Xiao, Dawei Zhao, Yiming Nie, and Bin Dai.\nMulti-camera unified pre-training via 3d scene reconstruction.\nRAL, 2024.\n[151] Chen Min, Dawei Zhao, Liang Xiao, Yiming Nie, and Bin Dai.\nUniworld: Autonomous driving pre-training via world models.\narXiv preprint arXiv:2308.07234, 2023.\n[152] Chen Min, Dawei Zhao, Liang Xiao, Jian Zhao, Xinli Xu, Zheng\nZhu, Lei Jin, Jianshu Li, Yulan Guo, Junliang Xing, Liping Jing,\nYiming Nie, and Bin Dai.\nDriveworld: 4d pre-trained scene\nunderstanding via world models for autonomous driving. In\nCVPR, 2024.\n[153] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A\nRusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin\nRiedmiller, Andreas K Fidjeland, Georg Ostrovski, Stig Petersen,\nCharles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King,\nDharshan Kumaran, Daan Wierstra, Shane Legg, and Demis\nHassabis.\nHuman-level control through deep reinforcement\nlearning. Nature, 2015.\n[154] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang,\nZhongang Qi, and Ying Shan. T2i-adapter: Learning adapters\nto dig out more controllable ability for text-to-image diffusion\nmodels. In AAAI, 2024.\n[155] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam,\nPamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen.\nGlide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741,\n2021.\n[156] World Model of Tesla. [online]. http://https://www.https://\nwww.youtube.com/watch?v=svgGsnBkl o.\n[157] Masashi Okada and Tadahiro Taniguchi.\nDreaming: Model-\nbased reinforcement learning by latent imagination without\nreconstruction. In ICRA, 2021.\n[158] Masashi Okada and Tadahiro Taniguchi. Dreamingv2: Reinforce-\nment learning with discrete world models without reconstruction.\nIn IROS, 2022.\n[159] Minting Pan, Xiangming Zhu, Yunbo Wang, and Xiaokang\nYang. Iso-dream: Isolating and leveraging noncontrollable visual\ndynamics in world models. NeurIPS, 2022.\n[160] Yingwei Pan, Zhaofan Qiu, Ting Yao, Houqiang Li, and Tao Mei.\nTo create what you tell: Generating videos from captions. In ACM\nICM, 2017.\n[161] Zizheng Pan, Bohan Zhuang, De-An Huang, Weili Nie, Zhiding\nYu, Chaowei Xiao, Jianfei Cai, and Anima Anandkumar.\nT-\nstitch: Accelerating sampling in pre-trained diffusion models with\ntrajectory stitching. arXiv preprint arXiv:2402.14167, 2024.\n[162] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and\nDani Lischinski. Styleclip: Text-driven manipulation of stylegan\nimagery. In ICCV, 2021.\n[163] Judea Pearl and Dana Mackenzie. The book of why: the new science\nof cause and effect. Basic books, 2018.\n[164] William Peebles and Saining Xie. Scalable diffusion models with\ntransformers. In ICCV, 2023.\n[165] Guim Perarnau, Joost Van De Weijer, Bogdan Raducanu, and\nJose M \u00b4Alvarez. Invertible conditional gans for image editing.\narXiv preprint arXiv:1611.06355, 2016.\n[166] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin,\nand Aaron Courville.\nFilm: Visual reasoning with a general\nconditioning layer. In AAAI, 2018.\n[167] Aj Piergiovanni, Alan Wu, and Michael S. Ryoo. Learning real-\nworld robot policies by dreaming. In IROS, 2019.\n[168] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann,\nTim Dockhorn, Jonas M\u00a8uller, Joe Penna, and Robin Rombach.\nSdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952, 2023.\n[169] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh,\nGabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell,\nPamela Mishkin, Jack Clark, et al. Learning transferable visual\nmodels from natural language supervision. In ICML, 2021.\n[170] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever,\net al.\nImproving language understanding by generative pre-\ntraining. 2018.\n[171] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario\nAmodei, Ilya Sutskever, et al. Language models are unsupervised\nmultitask learners. OpenAI blog, 2019.\n[172] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan\nNarang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu.\nExploring the limits of transfer learning with a unified text-to-text\ntransformer. JMLR, 2020.\n[173] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and\nMark Chen. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\n[174] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster\nr-cnn: Towards real-time object detection with region proposal\nnetworks. NeurIPS, 2015.\n[175] Jan Robine, Marc H\u201doftmann, Tobias Uelwer, and Stefan\nHarmeling. Transformer-based world models are happy with\n100k interactions. In ICLR, 2023.\n[176] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt\nSchiele. A dataset for movie description. In CVPR, 2015.\n[177] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick\nEsser, and Bj\u00a8orn Ommer. High-resolution image synthesis with\nlatent diffusion models. In CVPR, 2022.\n[178] Olaf Ronneberger, Philipp Fischer, and Thomas Brox.\nU-net:\nConvolutional networks for biomedical image segmentation. In\nMICCAI, 2015.\n[179] Reuven Rubinstein. The Cross-Entropy method for combinatorial\nand continuous optimization.\nMethodology And Computing In\nApplied Probability, 1999.\n[180] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay\nWhang, Emily L Denton, Kamyar Ghasemipour, Raphael Gon-\ntijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding.\nNeurIPS, 2022.\n[181] Ryo Sakagami, Florian S. Lay, Andreas D\u00a8omel, Martin J.\nSchuster, Alin Albu-Sch\u00a8affer, and Freek Stulp. Robotic world\nmodels\u2014conceptualization, review, and engineering best practices.\nFrontiers in Robotics and AI, 2023.\n[182] Tim Salimans and Jonathan Ho. Progressive distillation for fast\nsampling of diffusion models. arXiv preprint arXiv:2202.00512,\n2022.\n[183] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford,\nand Oleg Klimov. Proximal policy optimization algorithms, 2017.\n[184] Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel,\nDanijar Hafner, and Deepak Pathak. Planning to explore via\nself-supervised world models. In ICML, 2020.\n[185] Yuzhang Shang, Zhihang Yuan, Bin Xie, Bingzhe Wu, and Yan\nYan. Post-training quantization on diffusion models. In CVPR,\n2023.\n[186] Karen Simonyan and Andrew Zisserman. Very deep convolu-\ntional networks for large-scale image recognition. arXiv preprint\narXiv:1409.1556, 2014.\n[187] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured\noutput representation using deep conditional generative models.\nNeurIPS, 2015.\n[188] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising\ndiffusion implicit models. arXiv preprint arXiv:2010.02502, 2020.\n[189] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever.\nConsistency models. arXiv preprint arXiv:2303.01469, 2023.\n[190] Robin Staab, Mark Vero, Mislav Balunovi\u00b4c, and Martin Vechev.\nBeyond memorization: Violating privacy via inference with large\nlanguage models. arXiv preprint arXiv:2310.07298, 2023.\n[191] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid.\nSegmenter: Transformer for semantic segmentation. In ICCV, 2021.\n[192] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard,\nVijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai,\nBenjamin Caine, et al. Scalability in perception for autonomous\ndriving: Waymo open dataset. In CVPR, 2020.\n[193] Rui Sun, Yumin Zhang, Tejal Shah, Jiaohao Sun, Shuoying Zhang,\nWenqi Li, Haoran Duan, and Bo Wei. From sora what we can see:\nA survey of text-to-video generation.\n[194] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott\nReed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke,\nand Andrew Rabinovich. Going deeper with convolutions. In\nCVPR, 2015.\n[195] Yuval Tassa, Yotam Doron, Alistair Muldal, Tom Erez, Yazhe Li,\nDiego de Las Casas, David Budden, Abbas Abdolmaleki, Josh\nMerel, Andrew Lefrancq, Timothy Lillicrap, and Martin Riedmiller.\nDeepmind control suite, 2018.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n26\n[196] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu,\nJean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk,\nAndrew M Dai, Anja Hauth, et al. Gemini: a family of highly\ncapable multimodal models. arXiv preprint arXiv:2312.11805, 2023.\n[197] Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics\nengine for model-based control. In ICIRS, 2012.\n[198] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa,\nAlexandre Sablayrolles, and Herv\u00b4e J\u00b4egou. Training data-efficient\nimage transformers & distillation through attention. In ICML,\n2021.\n[199] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet,\nMarie-Anne Lachaux, Timoth\u00b4ee Lacroix, Baptiste Rozi`ere, Naman\nGoyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient\nfoundation language models.\narXiv preprint arXiv:2302.13971,\n2023.\n[200] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad\nAlmahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra,\nPrajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation\nand fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023.\n[201] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and\nManohar Paluri.\nLearning spatiotemporal features with 3d\nconvolutional networks. In ICCV, 2015.\n[202] Du Tran, Jamie Ray, Zheng Shou, Shih-Fu Chang, and Manohar\nPaluri. Convnet architecture search for spatiotemporal feature\nlearning. arXiv preprint arXiv:1708.05038, 2017.\n[203] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael\nMarinier, Marcin Michalski, and Sylvain Gelly. Towards accurate\ngenerative models of video: A new metric & challenges. arXiv\npreprint arXiv:1812.01717, 2018.\n[204] Aaron Van Den Oord, Oriol Vinyals, et al.\nNeural discrete\nrepresentation learning. NeurIPS, 2017.\n[205] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\nLlion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin.\nAttention is all you need. NeurIPS, 2017.\n[206] Sai H Vemprala, Rogerio Bonatti, Arthur Bucker, and Ashish\nKapoor.\nChatgpt for robotics: Design principles and model\nabilities. IEEE Access, 2024.\n[207] Tengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng\nChen. High-fidelity gan inversion for image attribute editing. In\nCVPR, 2022.\n[208] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang Wang,\nand William Yang Wang.\nVatex: A large-scale, high-quality\nmultilingual dataset for video-and-language research. In ICCV,\n2019.\n[209] Xiaofeng Wang, Zheng Zhu, Guan Huang, Xinze Chen, and Jiwen\nLu. Drivedreamer: Towards real-world-driven world models for\nautonomous driving. arXiv preprint arXiv:2309.09777, 2023.\n[210] Xiaofeng Wang, Zheng Zhu, Guan Huang, Boyuan Wang, Xinze\nChen, and Jiwen Lu.\nWorlddreamer: Towards general world\nmodels for video generation via predicting masked tokens, 2024.\n[211] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi\nHuang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing\nYang, et al. Lavie: High-quality video generation with cascaded\nlatent diffusion models. arXiv preprint arXiv:2309.15103, 2023.\n[212] Yuqi Wang, Jiawei He, Lue Fan, Hongxin Li, Yuntao Chen, and\nZhaoxiang Zhang.\nDriving into the future: Multiview visual\nforecasting and planning with world model for autonomous\ndriving. arXiv preprint arXiv:2311.17918, 2023.\n[213] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, Xin\nMa, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui Wang, et al.\nInternvid: A large-scale video-text dataset for multimodal under-\nstanding and generation. arXiv preprint arXiv:2307.06942, 2023.\n[214] Yi Wang, Kunchang Li, Xinhao Li, Jiashuo Yu, Yinan He, Guo\nChen, Baoqi Pei, Rongkun Zheng, Jilan Xu, Zun Wang, et al.\nInternvideo2: Scaling video foundation models for multimodal\nvideo understanding. arXiv preprint arXiv:2403.15377, 2024.\n[215] Zengran Wang, Chen Min, Zheng Ge, Yinhao Li, Zeming Li,\nHongyu Yang, and Di Huang. Sts: Surround-view temporal stereo\nfor multi-view 3d detection. arXiv preprint arXiv:2208.10145, 2022.\n[216] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and\nGuoping Wang. Aa-rmvsnet: Adaptive aggregation recurrent\nmulti-view stereo network. In ICCV, pages 6187\u20136196, 2021.\n[217] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and\nGuoping Wang. Bidirectional hybrid lstm based recurrent neural\nnetwork for multi-view stereo. TVCG, 2022.\n[218] Yuqing Wen, Yucheng Zhao, Yingfei Liu, Fan Jia, Yanhui Wang,\nChong Luo, Chi Zhang, Tiancai Wang, Xiaoyan Sun, and Xiangyu\nZhang. Panacea: Panoramic and controllable video generation for\nautonomous driving. arXiv preprint arXiv:2311.16813, 2023.\n[219] Benjamin Wilson, William Qi, Tanmay Agarwal, John Lambert,\nJagjeet Singh, Siddhesh Khandelwal, Bowen Pan, Ratnesh Kumar,\nAndrew Hartnett, Jhony Kaesemodel Pontes, et al. Argoverse\n2: Next generation datasets for self-driving perception and\nforecasting. arXiv preprint arXiv:2301.00493, 2023.\n[220] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan\nYang, Guillermo Sapiro, and Nan Duan. Godiva: Generating\nopen-domain videos from natural descriptions. arXiv preprint\narXiv:2104.14806, 2021.\n[221] Jay Zhangjie Wu, Guian Fang, Haoning Wu, Xintao Wang, Yixiao\nGe, Xiaodong Cun, David Junhao Zhang, Jia-Wei Liu, Yuchao\nGu, Rui Zhao, et al. Towards a better metric for text-to-video\ngeneration. arXiv preprint arXiv:2401.07781, 2024.\n[222] Philipp Wu, Alejandro Escontrela, Danijar Hafner, Pieter Abbeel,\nand Ken Goldberg. Daydreamer: World models for physical robot\nlearning. In CoRL, 2023.\n[223] Weijia Wu, Zhuang Li, Yefei He, Mike Zheng Shou, Chunhua Shen,\nLele Cheng, Yan Li, Tingting Gao, Di Zhang, and Zhongyuan\nWang. Paragraph-to-image generation with information-enriched\ndiffusion model. arXiv preprint arXiv:2311.14284, 2023.\n[224] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M\nAlvarez, and Ping Luo. Segformer: Simple and efficient design for\nsemantic segmentation with transformers. NeurIPS, 2021.\n[225] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye, Yuanhong\nXu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang, et al. mplug-2: A\nmodularized multi-modal foundation model across text, image\nand video. In ICML, 2023.\n[226] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video\ndescription dataset for bridging video and language. In CVPR,\n2016.\n[227] Mingxing Xu, Wenrui Dai, Chunmiao Liu, Xing Gao, Weiyao Lin,\nGuo-Jun Qi, and Hongkai Xiong. Spatial-temporal transformer\nnetworks for traffic flow forecasting, 2021.\n[228] Hongwei Xue, Tiankai Hang, Yanhong Zeng, Yuchong Sun, Bei\nLiu, Huan Yang, Jianlong Fu, and Baining Guo. Advancing high-\nresolution video-language representation with large-scale video\ntranscriptions. In CVPR, 2022.\n[229] Wilson Yan, Yunzhi Zhang, Pieter Abbeel, and Aravind Srinivas.\nVideogpt: Video generation using vq-vae and transformers. arXiv\npreprint arXiv:2104.10157, 2021.\n[230] Ceyuan Yang, Yinghao Xu, Jianping Shi, Bo Dai, and Bolei Zhou.\nTemporal pyramid network for action recognition. In CVPR, 2020.\n[231] Jiazhi Yang, Shenyuan Gao, Yihang Qiu, Li Chen, Tianyu Li, Bo\nDai, Kashyap Chitta, Penghao Wu, Jia Zeng, Ping Luo, et al.\nGeneralized predictive model for autonomous driving. arXiv\npreprint arXiv:2403.09630, 2024.\n[232] Sherry Yang, Yilun Du, Seyed Kamyar Seyed Ghasemipour,\nJonathan Tompson, Leslie Pack Kaelbling, Dale Schuurmans, and\nPieter Abbeel. Learning interactive real-world simulators. In ICLR,\n2024.\n[233] Zetong Yang, Li Chen, Yanan Sun, and Hongyang Li. Visual point\ncloud forecasting enables scalable autonomous driving. In CVPR,\n2024.\n[234] Ze Yang, Yun Chen, Jingkang Wang, Sivabalan Manivasagam,\nWei-Chiu Ma, Anqi Joyce Yang, and Raquel Urtasun. Unisim: A\nneural closed-loop sensor simulator. In CVPR, 2023.\n[235] Zhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient detr:\nimproving end-to-end object detector with dense prior. arXiv\npreprint arXiv:2104.01318, 2021.\n[236] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan\nBaid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei\nYang, Burcu Karagol Ayan, et al.\nScaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789, 2022.\n[237] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan\nBaid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei\nYang, Burcu Karagol Ayan, et al.\nScaling autoregressive\nmodels for content-rich text-to-image generation. arXiv preprint\narXiv:2206.10789, 2022.\n[238] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jos\u00b4e Lezama, Han Zhang,\nHuiwen Chang, Alexander G Hauptmann, Ming-Hsuan Yang,\nYuan Hao, Irfan Essa, et al. Magvit: Masked generative video\ntransformer. In CVPR, 2023.\n[239] Lijun Yu, Jos\u00b4e Lezama, Nitesh B Gundavarapu, Luca Versari,\nKihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye\nGu, Alexander G Hauptmann, et al.\nLanguage model beats\ndiffusion\u2013tokenizer is key to visual generation. arXiv preprint\narXiv:2310.05737, 2023.\n[240] Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol\nHausman, Chelsea Finn, and Sergey Levine.\nMeta-world: A\nbenchmark and evaluation for multi-task and meta reinforcement\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. XX, XXX 2024\n27\nlearning. In CoRL, 2020.\n[241] Yingchen Yu, Fangneng Zhan, Rongliang Wu, Jianxiong Pan,\nKaiwen Cui, Shijian Lu, Feiying Ma, Xuansong Xie, and Chunyan\nMiao. Diverse image inpainting with bidirectional and autoregres-\nsive transformers. In ACM ICM, 2021.\n[242] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung\nPark, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal\nneural script knowledge models. NeurIPS, 2021.\n[243] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei,\nYuchen Zhang, and Hang Li. Make pixels dance: High-dynamic\nvideo generation. arXiv preprint arXiv:2311.10982, 2023.\n[244] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu,\nLionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved\ndenoising anchor boxes for end-to-end object detection. arXiv\npreprint arXiv:2203.03605, 2022.\n[245] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala.\nAdding\nconditional control to text-to-image diffusion models, 2023.\n[246] Lunjun Zhang, Yuwen Xiong, Ze Yang, Sergio Casas, Rui Hu, and\nRaquel Urtasun. Copilot4d: Learning unsupervised world models\nfor autonomous driving via discrete diffusion. In ICLR, 2024.\n[247] Weipu Zhang, Gang Wang, Jian Sun, Yetian Yuan, and Gao Huang.\nStorm: Efficient stochastic transformer based world models for\nreinforcement learning. In NeurIPS, 2023.\n[248] Zhejun Zhang, Alexander Liniger, Dengxin Dai, Fisher Yu, and\nLuc Van Gool. Trafficbots: Towards world models for autonomous\ndriving simulation and motion prediction. In ICRA, 2023.\n[249] Guosheng Zhao, Xiaofeng Wang, Zheng Zhu, Xinze Chen, Guan\nHuang, Xiaoyi Bao, and Xingang Wang. Drivedreamer-2: Llm-\nenhanced world models for diverse driving video generation.\narXiv preprint arXiv:2403.06845, 2024.\n[250] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Azizzade-\nnesheli, and Anima Anandkumar. Fast sampling of diffusion\nmodels via operator learning. In ICML, 2023.\n[251] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun\nLuo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS\nTorr, et al. Rethinking semantic segmentation from a sequence-to-\nsequence perspective with transformers. In CVPR, 2021.\n[252] Wenzhao Zheng, Weiliang Chen, Yuanhui Huang, Borui Zhang,\nYueqi Duan, and Jiwen Lu.\nOccworld: Learning a 3d occu-\npancy world model for autonomous driving.\narXiv preprint\narXiv:2311.16038, 2023.\n[253] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic\nlearning of procedures from web instructional videos. In AAAI,\n2018.\n[254] Mi Zhou, Vibhanshu Abhishek, Timothy Derdenger, Jaymo Kim,\nand Kannan Srinivasan. Bias in generative ai. arXiv preprint\narXiv:2403.02726, 2024.\n[255] Siyuan Zhou, Yilun Du, Jiaben Chen, Yandong Li, Dit-Yan Yeung,\nand Chuang Gan. Robodreamer: Learning compositional world\nmodels for robot imagination, 2024.\n[256] Jiapeng Zhu, Yujun Shen, Deli Zhao, and Bolei Zhou. In-domain\ngan inversion for real image editing. In ECCV, 2020.\n[257] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros.\nUnpaired image-to-image translation using cycle-consistent adver-\nsarial networks. In ICCV, 2017.\n[258] Qingtian Zhu, Chen Min, Zizhuang Wei, Yisong Chen, and\nGuoping Wang. Deep learning for multi-view stereo via plane\nsweep: A survey. arXiv preprint arXiv:2106.15328, 2021.\n[259] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and\nJifeng Dai. Deformable detr: Deformable transformers for end-to-\nend object detection. arXiv preprint arXiv:2010.04159, 2020.\n[260] Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Mart\u00b4\u0131n-Mart\u00b4\u0131n,\nAbhishek Joshi, Soroush Nasiriany, and Yifeng Zhu. robosuite: A\nmodular simulation framework and benchmark for robot learning,\n2022.\n[261] Vlas Zyrianov, Henry Che, Zhijian Liu, and Shenlong Wang.\nLidardm: Generative lidar simulation in a generated world. arXiv\npreprint arXiv:2404.02903, 2024.\n",
    "2210.02303": "IMAGEN VIDEO: HIGH DEFINITION VIDEO\nGENERATION WITH DIFFUSION MODELS\nJonathan Ho\u2217, William Chan\u2217, Chitwan Saharia\u2217, Jay Whang\u2217, Ruiqi Gao, Alexey Gritsenko,\nDiederik P. Kingma, Ben Poole, Mohammad Norouzi, David J. Fleet, Tim Salimans\u2217\nGoogle Research, Brain Team\n{jonathanho,williamchan,sahariac,jwhang,ruiqig,agritsenko,\ndurk,pooleb,mnorouzi,davidfleet,salimans}@google.com\nABSTRACT\nWe present Imagen Video, a text-conditional video generation system based on a\ncascade of video diffusion models. Given a text prompt, Imagen Video generates\nhigh de\ufb01nition videos using a base video generation model and a sequence of in-\nterleaved spatial and temporal video super-resolution models. We describe how\nwe scale up the system as a high de\ufb01nition text-to-video model including design\ndecisions such as the choice of fully-convolutional temporal and spatial super-\nresolution models at certain resolutions, and the choice of the v-parameterization\nof diffusion models. In addition, we con\ufb01rm and transfer \ufb01ndings from previous\nwork on diffusion-based image generation to the video generation setting. Fi-\nnally, we apply progressive distillation to our video models with classi\ufb01er-free\nguidance for fast, high quality sampling. We \ufb01nd Imagen Video not only capable\nof generating videos of high \ufb01delity, but also having a high degree of controlla-\nbility and world knowledge, including the ability to generate diverse videos and\ntext animations in various artistic styles and with 3D object understanding. See\nimagen.research.google/video for samples.\nFigure 1: Imagen Video sample for the prompt: \u201cA bunch of autumn leaves falling on a calm lake to\nform the text \u2018Imagen Video\u2019. Smooth.\u201d The generated video is at 1280\u00d7768 resolution, 5.3 second\nduration and 24 frames per second.\n1\nINTRODUCTION\nGenerative modeling has made tremendous progress with recent text-to-image systems like\nDALL-E 2 (Ramesh et al., 2022), Imagen (Saharia et al., 2022b), Parti (Yu et al., 2022), CogView\n(Ding et al., 2021) and Latent Diffusion (Rombach et al., 2022). Diffusion models (Sohl-Dickstein\net al., 2015; Ho et al., 2020) in particular have found considerable success in multiple generative\nmodeling tasks (Nichol & Dhariwal, 2021; Ho et al., 2022a; Dhariwal & Nichol, 2022) including\ndensity estimation (Kingma et al., 2021), text-to-speech (Chen et al., 2021a; Kong et al., 2021;\nChen et al., 2021b), image-to-image (Saharia et al., 2022c;a; Whang et al., 2022), and text-to-image\n(Rombach et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022b).\n\u2217Equal contribution.\n1\narXiv:2210.02303v1  [cs.CV]  5 Oct 2022\nA colorful professional animated logo for \u2019Imagen Video\u2019 written using paint brush in cursive. Smooth animation.\nBlue \ufb02ame transforming into the text \u201cImagen\u201d. Smooth animation\nWooden \ufb01gurine sur\ufb01ng on a surfboard in space.\nBalloon full of water exploding in extreme slow motion.\nMelting pistachio ice cream dripping down the cone.\nA british shorthair jumping over a couch.\nCoffee pouring into a cup.\nFigure 2: Videos generated from various text prompts.\nImagen Video produces diverse and\ntemporally-coherent videos that are well-aligned with the given prompt.\n2\nA small hand-crafted wooden boat taking off to space.\nA person riding a bike in the sunset.\nDrone \ufb02ythrough interior of Sagrada Familia cathedral\nWooden \ufb01gurine walking on a treadmill made out of exercise mat.\nOrigami dancers in white paper, 3D render, ultra-detailed, on white background, studio shot, dancing modern dance.\nCamp\ufb01re at night in a snowy forest with starry sky in the background.\nAn astronaut riding a horse.\nFigure 3: Videos generated from various text prompts.\nImagen Video produces diverse and\ntemporally-coherent videos that are well-aligned with the given prompt.\n3\nA person riding a horse in the sunrise.\nA happy elephant wearing a birthday hat walking under the sea.\nStudio shot of minimal kinetic sculpture made from thin wire shaped like a bird on white background.\nA bunch of colorful candies falling into a tray in the shape of text \u2019Imagen Video\u2019. Smooth video.\nA group of people hiking in a forest.\nA goldendoodle playing in a park by a lake.\nIncredibly detailed science \ufb01ction scene set on an alien planet, view of a marketplace. Pixel art.\nFigure 4: Videos generated from various text prompts.\nImagen Video produces diverse and\ntemporally-coherent videos that are well-aligned with the given prompt.\n4\nA bunch of autumn leaves falling on a calm lake to form the text \u2019Imagen Video\u2019. Smooth.\nPouring latte art into a silver cup with a golden spoon next to it.\nShoveling snow.\nDrone \ufb02ythrough of a tropical jungle covered in snow\nA beautiful sunrise on mars, Curiosity rover. High de\ufb01nition, timelapse, dramatic colors\nA shark swimming in clear Carribean ocean.\nA hand lifts a cup.\nFigure 5: Videos generated from various text prompts.\nImagen Video produces diverse and\ntemporally-coherent videos that are well-aligned with the given prompt.\n5\nOur work aims to generate videos from text. Prior work on video generation has focused on more re-\nstricted datasets with autoregressive models (Ranzato et al., 2014; Shi et al., 2015; Finn et al., 2016;\nKalchbrenner et al., 2017; Babaeizadeh et al., 2021), latent-variable models with autoregressive pri-\nors (Mathieu et al., 2016; Vondrick et al., 2016; Babaeizadeh et al., 2018; Kumar et al., 2020), and\nmore recently non-autoregressive latent-variable approaches (Gupta et al., 2022). Diffusion models\nhave also shown promise for video generation (Ho et al., 2022b) at moderate resolution. Yang et al.\n(2022) showed autoregressive generation with a RNN-based model with conditional diffusion ob-\nservations. The concurrent work of Singer et al. (2022) also applied text-to-video modelling with\ndiffusion models, but built on a pretrained text-to-image model. Harvey et al. (2022) generates\nvideos up to 25 minutes in length with video diffusion models, however the domain is restricted.\nIn this work, we introduce Imagen Video, a text-to-video generation system based on video diffusion\nmodels (Ho et al., 2022b) that is capable of generating high de\ufb01nition videos with high frame \ufb01delity,\nstrong temporal consistency, and deep language understanding. Imagen Video scales from prior\nwork of 64-frame 128\u00d7128 videos at 24 frames per second to 128 frame 1280\u00d7768 high-de\ufb01nition\nvideo at 24 frames per second. Imagen Video has a simple architecture: The model consists of a\nfrozen T5 text encoder (Raffel et al., 2020), a base video diffusion model, and interleaved spatial\nand temporal super-resolution diffusion models. Our key contributions are as follows:\n1. We demonstrate the simplicity and effectiveness of cascaded diffusion video models for\nhigh de\ufb01nition video generation.\n2. We con\ufb01rm that recent \ufb01ndings in the text-to-image setting transfer to video generation,\nsuch as the effectiveness of frozen encoder text conditioning and classi\ufb01er-free guidance.\n3. We show new \ufb01ndings for video diffusion models that have implications for diffusion mod-\nels in general, such as the effectiveness of the v-prediction parameterization for sample\nquality and the effectiveness of progressive distillation of guided diffusion models for the\ntext-conditioned video generation setting.\n4. We demonstrate qualitative controllability in Imagen Video, such as 3D object understand-\ning, generation of text animations, and generation of videos in various artistic styles.\n2\nIMAGEN VIDEO\nOur model, Imagen Video, is a cascade of video diffusion models (Ho et al., 2022a;b). It consists\nof 7 sub-models which perform text-conditional video generation, spatial super-resolution, and tem-\nporal super-resolution. With the entire cascade, Imagen Video generates high de\ufb01nition 1280\u00d7768\n(width \u00d7 height) videos at 24 frames per second, for 128 frames (\u22485.3 seconds)\u2014approximately\n126 million pixels. We describe the components and techniques that constitute Imagen Video in the\nfollowing sections.\n2.1\nDIFFUSION MODELS\nImagen Video is built from diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho\net al., 2020) speci\ufb01ed in continuous time (Tzen & Raginsky, 2019; Song et al., 2021; Kingma et al.,\n2021). We use the formulation of Kingma et al. (2021): the model is a latent variable model with\nlatents z = {zt | t \u2208[0, 1]} following a forward process q(z|x) starting at data x \u223cp(x). The\nforward process is a Gaussian process that satis\ufb01es the Markovian structure:\nq(zt|x) = N(zt; \u03b1tx, \u03c32\nt I),\nq(zt|zs) = N(zt; (\u03b1t/\u03b1s)zs, \u03c32\nt|sI)\n(1)\nwhere 0 \u2264s < t \u22641, \u03c32\nt|s = (1 \u2212e\u03bbt\u2212\u03bbs)\u03c32\nt , and \u03b1t, \u03c3t specify a noise schedule whose log signal-\nto-noise-ratio \u03bbt = log[\u03b12\nt /\u03c32\nt ] decreases monotonically with t until q(z1) \u2248N(0, I). We use a\ncontinuous time version of the cosine noise schedule (Nichol & Dhariwal, 2021). The generative\nmodel is a learned model that matches this forward process in the reverse time direction, generating\nzt starting from t = 1 and ending at t = 0.\nLearning to reverse the forward process for generation can be reduced to learning to denoise zt \u223c\nq(zt|x) into an estimate \u02c6x\u03b8(zt, \u03bbt) \u2248x for all t. Like (Song & Ermon, 2019; Ho et al., 2020) and\nmost follow-up work, we optimize the model by minimizing a simple noise-prediction loss:\nL(x) = E\u03f5\u223cN (0,I),t\u223cU(0,1)\n\u0002\n\u2225\u02c6\u03f5\u03b8(zt, \u03bbt) \u2212\u03f5\u22252\n2\n\u0003\n(2)\n6\nwhere zt = \u03b1tx + \u03c3t\u03f5, and \u02c6\u03f5\u03b8(zt, \u03bbt) = \u03c3\u22121\nt\n(zt \u2212\u03b1t\u02c6x\u03b8(zt, \u03bbt)). We will drop the dependence on\n\u03bbt to simplify notation. In practice, we parameterize our models in terms of the v-parameterization\n(Salimans & Ho, 2022), rather than predicting \u03f5 or x directly; see Section 2.4.\nFor conditional generative modeling, we provide the conditioning information c drawn jointly with\nx to the model as \u02c6x\u03b8(zt, ct). We use these conditional diffusion models for spatial and temporal\nsuper-resolution in our pipeline of diffusion models: in these cases, c includes both the text and the\nprevious stage low resolution video as well as a signal \u03bb\u2032\nt that describes the strength of conditioning\naugmentation added to c. Saharia et al. (2022b) found it critical to condition all the super-resolution\nmodels with the text embedding, and we follow this approach.\nWe use the discrete time ancestral sampler (Ho et al., 2020), with sampling variances derived from\nlower and upper bounds on reverse process entropy (Sohl-Dickstein et al., 2015; Ho et al., 2020;\nNichol & Dhariwal, 2021). This sampler can be formulated by using a reversed description of the\nforward process as q(zs|zt, x) = N(zs; \u02dc\u00b5s|t(zt, x), \u02dc\u03c32\ns|tI) (noting s < t), where\n\u02dc\u00b5s|t(zt, x) = e\u03bbt\u2212\u03bbs(\u03b1s/\u03b1t) zt + (1 \u2212e\u03bbt\u2212\u03bbs)\u03b1sx\nand\n\u02dc\u03c32\ns|t = (1 \u2212e\u03bbt\u2212\u03bbs)\u03c32\ns.\n(3)\nStarting at z1 \u223cN(0, I), the ancestral sampler follows the rule\nzs = \u02dc\u00b5s|t(zt, \u02c6x\u03b8(zt)) +\nq\n(\u02dc\u03c32\ns|t)1\u2212\u03b3(\u03c32\nt|s)\u03b3\u03f5\n(4)\nwhere \u03f5 is standard Gaussian noise, \u03b3 is a hyperparameter that controls the stochasticity of the\nsampler (Nichol & Dhariwal, 2021), and s, t follow a uniformly spaced sequence from 1 to 0. See\nSection 3 for sampler hyperparameter settings.\nAlternatively, the deterministic DDIM sampler (Song et al., 2020) can be used for sampling. This\nsampler is a numerical integration rule for the probability \ufb02ow ODE (Song et al., 2021; Salimans\n& Ho, 2022), which describes how a sample from a standard normal distribution can be determin-\nistically transformed into a sample from the video data distribution using the denoising model. The\nDDIM sampler is useful for progressive distillation for fast sampling, as described in Section 2.7.\n2.2\nCASCADED DIFFUSION MODELS AND TEXT CONDITIONING\nCascaded Diffusion Models (Ho et al., 2022a) are an effective method for scaling diffusion models to\nhigh resolution outputs, \ufb01nding considerable success in both class-conditional ImageNet (Ho et al.,\n2022a) and text-to-image generation (Ramesh et al., 2022; Saharia et al., 2022b). Cascaded diffusion\nmodels generate an image or video at a low resolution, then sequentially increase the resolution\nof the image or video through a series of super-resolution diffusion models. Cascaded Diffusion\nModels can model very high dimensional problems while still keeping each sub-model relatively\nsimple. Imagen (Saharia et al., 2022b) also showed that by conditioning on text embeddings from a\nlarge frozen language model in conjunction with cascaded diffusion models, one can generate high\nquality 1024 \u00d7 1024 images from text descriptions. In this work we extend this approach to video\ngeneration.\nFigure 6 summarizes the entire cascading pipeline of Imagen Video. In total, we have 1 frozen text\nencoder, 1 base video diffusion model, 3 SSR (spatial super-resolution), and 3 TSR (temporal super-\nresolution) models \u2013 for a total of 7 video diffusion models, with a total of 11.6B diffusion model\nparameters. The data used to train these models is processed to the appropriate spatial and temporal\nresolutions by spatial resizing and frame skipping. At generation time, the SSR models increase\nspatial resolution for all input frames, whereas the TSR models increase temporal resolution by\n\ufb01lling in intermediate frames between input frames. All models generate an entire block of frames\nsimultaneously \u2013 so for instance, our SSR models do not suffer from obvious artifacts that would\noccur from naively running super-resolution on independent frames.\nOne bene\ufb01t of cascaded models is that each diffusion model can be trained independently, allowing\none to train all 7 models in parallel. Additionally, our super-resolution models are general purpose\nvideo super-resolution models, and they can be applied to real videos or samples from generative\nmodels other than the ones presented in this paper. This is similar to how Imagen\u2019s super-resolution\nmodels helped improve the \ufb01delity of the images generated by Parti (Yu et al., 2022), which is an\nautoregressive text-to-image model. We intend to explore hybrid pipelines of multiple model classes\nfurther in future work.\n7\nT5-XXL (4.6B)\nBase (5.6B)\n16x40x24 3fps\nTSR (1.7B)\n32x40x24 6fps\nTSR (780M)\n64x320x192 12fps\nTSR (630M)\n128x320x192 24fps\nSSR (1.4B)\n32x80x48 6fps\nSSR (340M)\n128x1280x768 24fps\nSSR (1.2B)\n32x320x192 6fps\nInput Text Prompt\nFigure 6: The cascaded sampling pipeline starting from a text prompt input to generating a 5.3-\nsecond, 1280\u00d7768 video at 24fps. \u201cSSR\u201d and \u201cTSR\u201d denote spatial and temporal super-resolution\nrespectively, and videos are labeled as frames\u00d7width\u00d7height. In practice, the text embeddings are\ninjected into all models, not just the base model.\nSpatial Conv\nSpatial Conv\nSpatial Conv\nSpatial Attention\nSpatial Attention\nTemporal Attention / Convolution\nSpatial Attention\nSpatial Conv\nSpatial Attention\nFrame 1\nFrame 2\nFrame 3\nFrame N\n...\nFigure 7: Video U-Net space-time separable block. Spatial operations are performed independently\nover frames with shared parameters, whereas the temporal operation mixes activations over frames.\nOur base model uses spatial convolutions, spatial self-attention and temporal self-attention. For\nmemory ef\ufb01ciency, our spatial and temporal super-resolution models use temporal convolutions\ninstead of attention, and our models at the highest spatial resolution do not have spatial attention.\nSimilar to Saharia et al. (2022b), we utilize contextual embeddings from a frozen T5-XXL text\nencoder (Raffel et al., 2020) for conditioning on the input text prompt. We \ufb01nd these embeddings\nto be critical for alignment between generated video and the text prompt. Similar to the \ufb01ndings\nof Saharia et al. (2022b), we observe evidence of deeper language understanding, enabling us to\ngenerate the videos displayed in Figs. 2 to 5.\n2.3\nVIDEO DIFFUSION ARCHITECTURES\nDiffusion models for image generation typically use a 2D U-Net architecture (Ronneberger et al.,\n2015; Salimans et al., 2017; Ho et al., 2020) to represent the denoising model \u02c6x\u03b8. This is a mul-\ntiscale model consisting of multiple layers of spatial attention and convolution at each resolution,\ncombined with shortcuts between layers at the same resolution. In earlier work on Video Diffusion\nModels, Ho et al. (2022b) introduced the Video U-Net , which generalizes the 2D diffusion model\narchitecture to 3D in a space-time separable fashion using temporal attention and convolution layers\ninterleaved within spatial attention and convolution layers to capture dependencies between video\nframes. Our work builds on the Video U-Net architecture: see Figure 7. Following Video Diffu-\nsion Models, each of our denoising models \u02c6x\u03b8 operate on multiple video frames simultaneously and\nthereby generate entire blocks of video frames at a time, which we \ufb01nd to be important to capture\nthe temporal coherence of the generated video compared to frame-autoregressive approaches. Our\nspatial super-resolution (SSR) and temporal super-resolution (TSR) models condition on their in-\nput videos by concatenating an upsampled conditioning input channelwise to the noisy data zt, the\nsame mechanism as SR3 (Saharia et al., 2022c) and Palette (Saharia et al., 2022a): spatial upsam-\npling before concatenation is performed using bilinear resizing, and temporal upsampling before\nconcatenation is performed by repeating frames or by \ufb01lling in blank frames.\n8\nOur base video model, which is the \ufb01rst model in the pipeline that generates data at the lowest frame\ncount and spatial resolution, uses temporal attention to mix information across time. Our SSR and\nTSR models, on the other hand, use temporal convolutions instead of temporal attention. The tem-\nporal attention in the base model enables Imagen Video to model long term temporal dependencies,\nwhile the temporal convolutions in the SSR and TSR models allow Imagen Video to maintain local\ntemporal consistency during upsampling. The use of temporal convolutions lowers memory and\ncomputation costs over temporal attention\u2014this is crucial because the very purpose of the TSR and\nSSR models is to operate at high frame rates and spatial resolutions. In our initial experiments,\nwe did not \ufb01nd any signi\ufb01cant improvements when using temporal attention over temporal con-\nvolutions in our SSR and TSR models, which we hypothesize is due to the signi\ufb01cant amount of\ntemporal correlation already present in the conditioning input to these models.\nOur models also use spatial attention and spatial convolutions. The base model and the \ufb01rst two\nspatial super-resolution models have spatial attention in addition to spatial convolutions. We found\nthis to improve sample \ufb01delity. However, as we move to higher resolutions, we switch to fully\nconvolutional architectures, like Saharia et al. (2022b), to minimize memory and compute costs in\norder to generate 1280\u00d7768 resolution data. The highest resolution SSR model in our pipeline is a\nfully convolutional model trained on random lower resolution spatial crops for training time memory\nef\ufb01ciency, and we \ufb01nd that the model easily generalizes to the full resolution during sampling time.\n2.4\nv-PREDICTION\nWe follow Salimans & Ho (2022) and use v-prediction parameterization (vt \u2261\u03b1t\u03f5\u2212\u03c3tx) for all our\nmodels. The v-parameterization is particularly useful for numerical stability throughout the diffu-\nsion process to enable progressive distillation for our models. For models that operate at higher res-\nolution in our pipeline, we also discovered that the v-parameterization avoids color shifting artifacts\nthat are known to affect high resolution diffusion models, and in the video setting it avoids temporal\ncolor shifting that sometimes appears with \u03f5-prediction models. Our use of v-parameterization also\nhas the bene\ufb01t of faster convergence of sample quality metrics: see Section 3.3.\n2.5\nCONDITIONING AUGMENTATION\nWe use noise conditioning augmentation (Ho et al., 2022a) for all our temporal and spatial super-\nresolution models. Noise conditioning augmentation has been found to be critical for cascaded\ndiffusion models for class-conditional generation (Ho et al., 2022a) as well as text-to-image models\n(Saharia et al., 2022b). In particular, it facilitates parallel training of different models in the cascade,\nas it reduces the sensitivity to domain gaps between the output of one stage of the cascade and the\ninputs used in training the subsequent stage.\nFollowing Ho et al. (2022a), we apply Gaussian noise augmentation with a random signal-to-noise\nratio to the conditioning input video during training, and this sampled signal-to-noise ratio is pro-\nvided to the model as well. At sampling time we use a \ufb01xed signal-to-noise ratio such as 3 or 5,\nrepresenting a small amount of augmentation that aids in removing artifacts in the samples from the\nprevious stage while preserving most of the structure.\n2.6\nVIDEO-IMAGE JOINT TRAINING\nWe follow Ho et al. (2022b) in jointly training all the models in the Imagen Video pipeline on images\nand videos. During training, individual images are treated as single frame videos. We achieve this\nby packing individual independent images into a sequence of the same length as a video, and bypass\nthe temporal convolution residual blocks by masking out their computation path. Similarly, we\ndisable cross-frame temporal attention by applying masking to the temporal attention maps. This\nstrategy allows us to use to train our video models on image-text datasets that are signi\ufb01cantly larger\nand more diverse than available video-text datasets. Consistent with Ho et al. (2022b), we observe\nthat joint training with images signi\ufb01cantly increases the overall quality of video samples. Another\ninteresting artifact of joint training is the knowledge transfer from images to videos. For instance,\nwhile training on natural video data only enables the model to learn dynamics in natural settings, the\nmodel can learn about different image styles (such as sketch, painting, etc.) by training on images.\n9\nAs a result, this joint training enables the model to generate interesting video dynamics in different\nstyles. See Fig. 8 for such examples.\n2.6.1\nCLASSIFIER FREE GUIDANCE\nWe found classi\ufb01er free guidance (Ho & Salimans, 2021) to be critical for generating high \ufb01delity\nsamples which respect a given text prompt. This is consistent with earlier results on text-to-image\nmodels (Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022b; Yu et al., 2022).\nIn the conditional generation setting, the data x is generated conditional on a signal c, which here\nrepresents a contextualized embedding of the text prompt, and a conditional diffusion model can\nbe trained by using this signal c as an additional input to the denoising model \u02c6x\u03b8(zt, c). After\ntraining, Ho & Salimans (2021) \ufb01nd that sample quality can be improved by adjusting the denoising\nprediction \u02c6x\u03b8(zt, c) using\n\u02dcx\u03b8(zt, c) = (1 + w)\u02c6x\u03b8(zt, c) \u2212w\u02c6x\u03b8(zt),\n(5)\nwhere w is the guidance strength, \u02c6x\u03b8(zt, c) is the conditional model, and \u02c6x\u03b8(zt) = \u02c6x\u03b8(zt, c = \u2205)\nis an unconditional model. The unconditional model is jointly trained with the conditional model\nby dropping out the conditioning input c. The predictions of the adjusted denoising model \u02dcx\u03b8(zt, c)\nare clipped to respect the range of possible pixel values, which we discuss in more detail in the next\nsection. Note that the linear transformation in Equation 5 can equivalently be performed in v-space\n(\u02dcv\u03b8(zt, c) = (1 + w) \u02c6v\u03b8(zt, c) \u2212w \u02c6v\u03b8(zt)) or \u03f5-space (\u02dc\u03f5\u03b8(zt, c) = (1 + w) \u02c6\u03f5\u03b8(zt, c) \u2212w \u02c6\u03f5\u03b8(zt)).\nFor w > 0 this adjustment has the effect of over-emphasizing the effect of conditioning on the signal\nc, which tends to produce samples of lower diversity but higher quality compared to sampling from\nthe regular conditional model (Ho & Salimans, 2021). The method can be interpreted as a way to\nguide the samples towards areas where an implicit classi\ufb01er p(c|zt) has high likelihood; as such, it\nis an adaptation of the explicit classi\ufb01er guidance method proposed by Dhariwal & Nichol (2022).\n2.6.2\nLARGE GUIDANCE WEIGHTS\nWhen using large guidance weights, the resulting \u02dcx\u03b8(zt, c) must be projected back to the pos-\nsible range of pixel values at every sampling step to prevent train-test mismatch. When using\nlarge guidance weights, the standard approach, i.e., clipping the values to the right range (e.g.,\nnp.clip(x, -1, 1)), leads to signi\ufb01cant saturation artifacts in the generated videos. A sim-\nilar effect was observed in Saharia et al. (2022b) for text-to-image generation.\nSaharia et al.\n(2022b) use dynamic thresholding to alleviate this saturation issue. Speci\ufb01cally, dynamic clipping\ninvolves clipping the image to a dynamically chosen threshold s followed by scaling by s (i.e.,\nnp.clip(x, -s, s) / s) (Saharia et al., 2022b).\nAlthough dynamic clipping can help with over-saturation, we did not \ufb01nd it suf\ufb01cient in initial ex-\nperiments. We therefore also experiment with letting w oscillate between a high and a low guidance\nweight at each alternating sampling step, which we \ufb01nd signi\ufb01cantly helps with these saturation\nissues. We call this sampling technique oscillating guidance. Speci\ufb01cally, we use a constant high\nguidance weight for a certain number of initial sampling steps, followed by oscillation between high\nand low guidance weights: this oscillation is implemented simply by alternating between a large\nweight (such as 15) and a small weight (such as 1) over the course of sampling. We hypothesize\nthat a constant high guidance weight at the start of sampling helps break modes with heavy empha-\nsis on text, while oscillating between high and low guidance weights helps maintain a strong text\nalignment (via high guidance sampling step) while limiting saturation artifacts (via low guidance\nsampling step). We however observed no improvement in sample \ufb01delity and more visual artifacts\nwhen applying oscillating guidance to models past the 80\u00d748 spatial resolution. Thus we only apply\noscillating guidance to the base and the \ufb01rst two SR models.\n2.7\nPROGRESSIVE DISTILLATION WITH GUIDANCE AND STOCHASTIC SAMPLERS\nSalimans & Ho (2022) proposed progressive distillation to enable fast sampling of diffusion models.\nThis method distills a trained deterministic DDIM sampler (Song et al., 2020) to a diffusion model\nthat takes many fewer sampling steps, without losing much perceptual quality. At each iteration\nof the distillation process, an N-step DDIM sampler is distilled to a new model with N/2-steps.\n10\nThis procedure is repeated by halving the required sampling steps each iteration. Meng et al. (2022)\nextend this approach to samplers with guidance, and propose a new stochastic sampler for use with\ndistilled models. Here we show that this approach also works very well for video generation.\nWe use a two-stage distillation approach to distill a DDIM sampler (Song et al., 2020) with classi\ufb01er-\nfree guidance. At the \ufb01rst stage, we learn a single diffusion model that matches the combined output\nfrom the jointly trained conditional and unconditional diffusion models, where the combination\ncoef\ufb01cients are determined by the guidance weight. Then we apply progressive distillation to that\nsingle model to produce models requiring fewer sampling steps at the second stage.\nAfter distillation, we use a stochastic N-step sampler: At each step, we \ufb01rst apply one deterministic\nDDIM update with twice the original step size (i.e., the same step size as a N/2-step sampler),\nand then we perform one stochastic step backward (i.e., perturbed with noise following the forward\ndiffusion process) with the original step size, inspired by Karras et al. (2022). See Meng et al. (2022)\nfor more details. Using this approach, we are able to distill all 7 video diffusion models down to just\n8 sampling steps per model without any noticeable loss in perceptual quality.\n3\nEXPERIMENTS\nWe train our models on a combination of an internal dataset consisting of 14 million video-text pairs\nand 60 million image-text pairs, and the publicly available LAION-400M image-text dataset. To\nprocess the data into a form suitable for training our cascading pipeline, we spatially resize images\nand videos using antialiased bilinear resizing, and we temporally resize videos by skipping frames.\nThroughout our model development process, we evaluated Imagen Video on several different met-\nrics, such as FID on individual frames (Heusel et al., 2017), FVD (Unterthiner et al., 2019) for\ntemporal consistency, and frame-wise CLIP scores (Hessel et al., 2021; Park et al., 2021) for video-\ntext alignment. Below, we explore the capabilities of our model and investigate its performance in\nregards to 1) scaling up the number of parameters in our model, 2) changing the parameterization\nof our model, and 3) distilling our models so that they are fast to sample from.\n3.1\nUNIQUE VIDEO GENERATION CAPABILITIES\nWe \ufb01nd that Imagen Video is capable of generating high \ufb01delity video, and that it possesses several\nunique capabilities that are not traditionally found in unstructured generative models learned purely\nfrom data. For example, Fig. 8 shows that our model is capable of generating videos with artistic\nstyles learned from image information, such as videos in the style of van Gogh paintings or water-\ncolor paintings. Fig. 9 shows that Imagen Video possesses an understanding of 3D structure, as it\nis capable of generating videos of objects rotating while roughly preserving structure. While the\n3D consistency over the course of rotation is not exact, we believe Imagen Video shows that video\nmodels can serve as effective priors for methods that do force 3D consistency. Fig. 10 shows that\nImagen Video is also reliably capable of generating text in a wide variety of animation styles, some\nof which would be dif\ufb01cult to animate using traditional tools. We see results such as these as an ex-\nciting indication of how general purpose generative models such as Imagen Video can signi\ufb01cantly\ndecrease the dif\ufb01culty of high quality content generation.\n3.2\nSCALING\nIn Figure 11 we show that our base video model strongly bene\ufb01ts from scaling up the parameter\ncount of the video U-Net. We performed this scaling by increasing the base channel count and depth\nof the network. This result is contrary to the text-to-image U-Net scaling results by Saharia et al.\n(2022b), which found limited bene\ufb01t from diffusion model scaling when measured by image-text\nsample quality scores. We conclude that video modeling is a harder task for which performance is\nnot yet saturated at current model sizes, implying future bene\ufb01ts to further model scaling for video\ngeneration.\n3.3\nCOMPARING PREDICTION PARAMETERIZATIONS\nIn early experiments we found that training \u03f5-prediction models (Ho et al., 2020) performed worse\nthan v-prediction (Salimans & Ho, 2022) especially at high resolutions. Speci\ufb01cally, for high res-\n11\nA cat eating food out of a bowl, in style of Van Gogh.\nA drone \ufb02ythrough over a watercolor painting of a forest.\nDrone \ufb02ythrough of a pixel art of futuristic city.\nFigure 8: Snapshots of frames from videos generated by Imagen Video demonstrating the ability of\nthe model to generate dynamics in different artistic styles.\nA 3D model of a 1800s victorian house. Studio lighting.\nA 3D model of a car made out of sushi. Studio lighting.\nA 3D model of an elephant origami. Studio lighting.\nFigure 9: Snapshots of frames from videos generated by Imagen Video demonstrating the model\u2019s\nunderstanding of 3D structures.\nA colorful professional animated logo for \u2019Diffusion\u2019 written using paint brush in cursive. Smooth animation.\nSprouts in the shape of text \u2019Imagen\u2019 coming out of a fairytale book.\nThousands of fast brush strokes slowly forming the text \u2019Imagen Video\u2019 on a light beige canvas. Smooth animation.\nFigure 10: Snapshots of frames from videos generated by Imagen Video demonstrating the ability\nof the model to render a variety of text with different style and dynamics.\n12\n0\n1\n2\n3\n4\n\u00b7105\n10\n15\n20\n25\nTraining Steps\nFVD\n500M Params\n1.6B Params\n5.6B Params\n(a) Scaling Comparison on FVD scores.\n0\n1\n2\n3\n4\n\u00b7105\n24\n24.5\nTraining Steps\nCLIP Score\n500M Params\n1.6B Params\n5.6B Params\n(b) Scaling Comparison on CLIP scores.\nFigure 11: Scaling Comparison for the base 16\u00d740\u00d724 video model on FVD and CLIP scores (on\n0-100 scale). Both FVD and CLIP scores are computed on 4096 video samples. We see clear signs\nof improvement on both metrics when scaling from 500M to 1.6B to 5.6B parameters.\n80\u00d748 input video frames\nSSR to 320\u00d7192 with \u03f5-prediction\nSSR to 320\u00d7192 with v-prediction\nFigure 12: Comparison between \u03f5-prediction (middle row) and v-prediction (bottom row) for a\n8\u00d780\u00d748\u21928\u00d7320\u00d7192 spatial super-resolution architecture at 200k training steps. The frames\nfrom the \u03f5-prediction model are generally worse, suffering from unnatural global color shifts across\nframes. The frames from the v-prediction model do not and are more consistent.\nolution SSR models, we observed that \u03f5-prediction converges relatively slowly in terms of sample\nquality metrics and suffers from color shift and color inconsistency across frames in the gener-\nated videos. Fig. 12 shows the comparison between \u03f5-prediction and v-prediction on a 80\u00d748 \u2192\n320\u00d7192 video spatial super-resolution task. It is clear that \u03f5-parameterization produces worse\ngenerations than v-parameterization. Fig. 13 shows the quantitative comparison between the two\nparameterizations as a function of training steps. We observe that v parameterization converges\nmuch more faster than \u03f5 parameterization.\n3.4\nPERCEPTUAL QUALITY AND DISTILLATION\nIn Table 1 we report perceptual quality metrics (CLIP score and CLIP R-Precision) for our model\nsamples, as well as for their distilled version. Samples are generated and evaluated at 192\u00d7320\nresolution for 128 frames at 24 frames per second. For CLIP score, we take the average score over\nall frames. For CLIP R-Precision (Park et al., 2021) we compute the top-1 accuracy (i.e. R = 1),\ntreating the frames of a video sample as images sharing the same text label (the prompt). We repeat\nthese over four different runs and report the mean and standard error.\n13\n0\n0.5\n1\n1.5\n2\n2.5\n\u00b7105\n10\n20\n30\nTraining Steps\nFirst Frame FID\n\u03f5-prediction\nv-prediction\nFigure 13: Comparison between 80\u00d748 \u2192320\u00d7192 SSR models trained with \u03f5- and v-prediction\nparameterizations. We report FID evaluated on the \ufb01rst upsampled frame; FVD score is excessively\nnoisy for the \u03f5-prediction model. We observe that the sample quality of the \u03f5-prediction model\nconverges much more slowly than that of the v-prediction model.\nWe \ufb01nd that distillation provides a very favorable trade-off between sampling time and perceptual\nquality: the distilled cascade is about 18\u00d7 faster, while producing videos of similar quality to the\nsamples from the original models. In terms of FLOPs, the distilled models are about 36\u00d7 more ef\ufb01-\ncient: The original cascade evaluates each model twice (in parallel) to apply classi\ufb01er-free guidance,\nwhile our distilled models do not, since they distilled the effect of guidance into a single model. We\nprovide samples from our original and distilled cascade in Figure 14 for illustration.\nGuidance w\nBase Steps\nSR Steps\nCLIP Score\nCLIP R-Precision\nSampling Time\nconstant=6\n256\n128\n25.19\u00b1.03\n92.12\u00b1.53\n618 sec\noscillate(15,1)\n256\n128\n25.02\u00b1.08\n89.91\u00b1.96\n618 sec\nconstant=6\n256\n8\n25.29\u00b1.05\n90.88\u00b1.50\n135 sec\noscillate(15,1)\n256\n8\n25.15\u00b1.09\n88.78\u00b1.69\n135 sec\nconstant=6\n8\n8\n25.03\u00b1.05\n89.68\u00b1.38\n35 sec\noscillate(15,1)\n8\n8\n25.12\u00b1.07\n90.97\u00b1.46\n35 sec\nground truth\n24.27\n86.18\nTable 1: CLIP scores and CLIP R-Precision (Park et al., 2021) values for generated samples and\nground truth videos on prompts from our test set. Cells highlighted in green represent distilled\nmodels. We compare three different combinations: original pipeline, distilled SR models on top\nof original base model, and fully distilled pipeline. The original base models use 256 sampling\nsteps, and original SR models use 128 steps. All distilled models use 8 sampling steps per stage.\nSampling from the original pipeline takes 618 seconds for one batch of samples, while sampling\nfrom the distilled pipeline takes 35 seconds, making the distilled pipeline about 18\u00d7 faster. We also\nexplored two different classi\ufb01er-free guidance settings for the base models: constant guidance with\nw = 6 and oscillating guidance which alternates between w = 15 and w = 1, following Saharia\net al. (2022b). When using oscillating guidance, the fully distilled pipeline performs the same as the\noriginal model, or even slightly better. When using \ufb01xed guidance, our fully distilled pipeline scores\nslightly lower than the original model, though the difference is minor. Combining the original base\nmodel with \ufb01xed guidance and distilled super-resolution models produced the highest CLIP score.\nFor all models, generated samples obtain better perceptual quality metrics than the original ground\ntruth data: By using classi\ufb01er-free guidance our models sample from a distribution tilted towards\nthese quality metrics, rather than from an accurate approximation of the original data distribution.\n4\nLIMITATIONS AND SOCIETAL IMPACT\nGenerative modeling has made tremendous progress, especially in recent text-to-image models (Sa-\nharia et al., 2022b; Ramesh et al., 2022; Rombach et al., 2022). Imagen Video is another step\nforward in generative modelling capabilities, advancing text-to-video AI systems. Video generative\nmodels can be used to positively impact society, for example by amplifying and augmenting human\ncreativity. However, these generative models may also be misused, for example to generate fake,\nhateful, explicit or harmful content. We have taken multiple steps to minimize these concerns, for\n14\nFigure 14: Frames from videos generated by Imagen Video for the text prompt \u201cA teddy bear wear-\ning sunglasses playing guitar next to a cactus.\u201d The samples on the left are produced by our original\nmodel cascade, while the samples on the right are from our distilled cascade with 8 sampling steps\nper stage. Both used constant guidance with w = 6 and static clipping.\nexample in internal trials, we apply input text prompt \ufb01ltering, and output video content \ufb01ltering.\nHowever, there are several important safety and ethical challenges remaining. Imagen Video and its\nfrozen T5-XXL text encoder were trained on problematic data (Bordia & Bowman, 2017; Birhane\net al., 2021; Bender et al., 2021). While our internal testing suggests much of explicit and violent\ncontent can be \ufb01ltered out, there still exists social biases and stereotypes which are challenging to\ndetect and \ufb01lter. We have decided not to release the Imagen Video model or its source code until\nthese concerns are mitigated.\n5\nCONCLUSION\nWe presented Imagen Video: a text-conditional video generation system based on a cascade of video\ndiffusion models. By extending the text-to-image diffusion models of Imagen (Saharia et al., 2022b)\nto the time domain, and training jointly on video and images, we obtained a model capable of gen-\nerating high \ufb01delity videos with good temporal consistency while maintaining the strong features\nof the original image system, such as the ability to accurately spell text. We transferred multiple\nmethods from the image domain to video, such as v-parameterization (Salimans & Ho, 2022), con-\nditioning augmentation (Ho et al., 2022a), and classi\ufb01er-free guidance (Ho & Salimans, 2021), and\nfound that these are also useful in the video setting. Video modeling is computationally demanding,\nand we found that progressive distillation (Salimans & Ho, 2022; Meng et al., 2022) is a valuable\ntechnique for speeding up video diffusion models at sampling time. Given the tremendous recent\nprogress in generative modeling, we believe there is ample scope for further improvements in video\ngeneration capabilities in future work.\n6\nACKNOWLEDGEMENTS\nWe give special thanks to Jordi Pont-Tuset and Shai Noy for engineering support. We also give\nthanks to our artist friends, Alexander Chen, Irina Blok, Ian Muldoon, Daniel Smith, and Pedro Ver-\ngani for helping us test Imagen Video and lending us their amazing creativity. We are extremely\ngrateful for the support from Erica Moreira for compute resources. Finally, we give thanks to\nElizabeth Adkison, James Bradbury, Nicole Brichtova, Tom Duerig, Douglas Eck, Dumitru Erhan,\nZoubin Ghahramani, Kamyar Ghasemipour, Victor Gomes, Blake Hechtman, Jonathan Heek, Yash\nKatariya, Sarah Laszlo, Sara Mahdavi, Anusha Ramesh, Tom Small, and Tris Warkentin for their\nsupport.\nREFERENCES\nLAION-400M. https://laion.ai/blog/laion-400-open-dataset/.\nMohammad Babaeizadeh, Chelsea Finn, D. Erhan, Roy H. Campbell, and Sergey Levine. Stochastic\nvariational video prediction. ArXiv, abs/1710.11252, 2018.\n15\nMohammad Babaeizadeh, Mohammad Taghi Saffar, Suraj Nair, Sergey Levine, Chelsea Finn, and\nDumitru Erhan. Fitvid: Over\ufb01tting in pixel-level video prediction, 2021. URL https://arxiv.\norg/abs/2106.13195.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. On the\ndangers of stochastic parrots: Can language models be too big?\n. In Proceedings of FAccT\n2021, 2021.\nAbeba Birhane, Vinay Uday Prabhu, and Emmanuel Kahembwe. Multimodal datasets: misogyny,\npornography, and malignant stereotypes. In arXiv:2110.01963, 2021.\nShikha Bordia and Samuel R. Bowman.\nIdentifying and Reducing Gender Bias in Word-Level\nLanguage Models. In NAACL, 2017.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, and William Chan. Wave-\nGrad: Estimating Gradients for Waveform Generation. In ICLR, 2021a.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, and William\nChan. WaveGrad 2: Iterative Re\ufb01nement for Text-to-Speech Synthesis . In INTERSPEECH,\n2021b.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis. In NeurIPS,\n2022.\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou,\nZhou Shao, Hongxia Yang, and Jie Tang.\nCogview: Mastering text-to-image generation via\ntransformers, 2021. URL https://arxiv.org/abs/2105.13290.\nChelsea Finn, Ian J. Goodfellow, and Sergey Levine. Unsupervised learning for physical interaction\nthrough video prediction. ArXiv, abs/1605.07157, 2016.\nAgrim Gupta, Stephen Tian, Yunzhi Zhang, Jiajun Wu, Roberto Mart\u2019in-Mart\u2019in, and Li Fei-Fei.\nMaskvit: Masked visual pre-training for video prediction. ArXiv, abs/2206.11894, 2022.\nWilliam Harvey, Saeid Naderiparizi, Vaden Masrani, Christian Weilbach, and Frank Wood. Flexible\ndiffusion modeling of long videos. ArXiv, abs/2205.11495, 2022.\nJack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi.\nClipscore: A\nreference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. arXiv preprint\narXiv:1706.08500, 2017.\nJonathan Ho and Tim Salimans. Classi\ufb01er-free diffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applications, 2021.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. NeurIPS,\n2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-\nmans. Cascaded diffusion models for high \ufb01delity image generation. JMLR, 2022a.\nJonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J.\nFleet. Video Diffusion Models. In arXiv:2204.03458, 2022b.\nNal Kalchbrenner, A\u00a8aron van den Oord, Karen Simonyan, Ivo Danihelka, Oriol Vinyals, Alex\nGraves, and Koray Kavukcuoglu. Video pixel networks. ArXiv, abs/1610.00527, 2017.\nTero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-\nbased generative models. arXiv preprint arXiv:2206.00364, 2022.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.\narXiv preprint arXiv:2107.00630, 2021.\n16\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro. DiffWave: A Versatile\nDiffusion Model for Audio Synthesis. In ICLR, 2021.\nManoj Kumar, Mohammad Babaeizadeh, D. Erhan, Chelsea Finn, Sergey Levine, Laurent Dinh,\nand Durk Kingma. Video\ufb02ow: A conditional \ufb02ow-based model for stochastic video generation.\narXiv: Computer Vision and Pattern Recognition, 2020.\nMicha\u00a8el Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond\nmean square error. CoRR, abs/1511.05440, 2016.\nChenlin Meng, Ruiqi Gao, Diederik P. Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans.\nOn distillation of guided diffusion models. arXiv preprint, 2022.\nAlex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In Interna-\ntional Conference on Machine Learning, pp. 8162\u20138171. PMLR, 2021.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Bob McGrew Pamela Mishkin,\nIlya Sutskever, and Mark Chen. GLIDE: Towards Photorealistic Image Generation and Editing\nwith Text-Guided Diffusion Models. In arXiv:2112.10741, 2021.\nDong Huk Park, Samaneh Azadi, Xihui Liu, Trevor Darrell, and Anna Rohrbach. Benchmark for\ncompositional text-to-image synthesis. In Thirty-\ufb01fth Conference on Neural Information Process-\ning Systems Datasets and Benchmarks Track (Round 1), 2021. URL https://openreview.\nnet/forum?id=bKBhQhPeKaF.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. Exploring the Limits of Transfer Learning with a Uni\ufb01ed Text-to-\nText Transformer. JMLR, 21(140), 2020.\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical Text-\nConditional Image Generation with CLIP Latents. In arXiv, 2022.\nMarc\u2019Aurelio Ranzato, Arthur D. Szlam, Joan Bruna, Micha\u00a8el Mathieu, Ronan Collobert, and Sumit\nChopra. Video (language) modeling: a baseline for generative models of natural videos. ArXiv,\nabs/1412.6604, 2014.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nResolution Image Synthesis with Latent Diffusion Models. In CVPR, 2022.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedi-\ncal image segmentation. In International Conference on Medical image computing and computer-\nassisted intervention, pp. 234\u2013241. Springer, 2015.\nChitwan Saharia, William Chan, Huiwen Chang, Chris A. Lee, Jonathan Ho, Tim Salimans, David J.\nFleet, and Mohammad Norouzi. Palette: Image-to-Image Diffusion Models. In SIGGRAPH,\n2022a.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kam-\nyar Seyed Ghasemipour, Burcu Karagol Ayan, S. Sara Mahdavi, Rapha Gontijo Lopes, Tim Sali-\nmans, Jonathan Ho, David J Fleet, and Mohammad Norouzi. Photorealistic Text-to-Image Diffu-\nsion Models with Deep Language Understanding. In NeurIPS, 2022b.\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\nNorouzi. Image super-resolution via iterative re\ufb01nement. IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2022c.\nTim Salimans and Jonathan Ho. Progressive Distillation for Fast Sampling of Diffusion Models. In\nICLR, 2022.\nTim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. PixelCNN++: Improving the\nPixelCNN with discretized logistic mixture likelihood and other modi\ufb01cations. In International\nConference on Learning Representations, 2017.\n17\nXingjian Shi, Zhourong Chen, Hao Wang, Dit-Yan Yeung, Wai-Kin Wong, and Wang chun Woo.\nConvolutional lstm network: A machine learning approach for precipitation nowcasting. In NIPS,\n2015.\nUriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry\nYang, Oron Ashual, Oran Gafni, Devi Parikh, Sonal Gupta, and Yaniv Taigman. Make-A-Video:\nText-to-Video Generation without Text-Video Data. In arXiv:2209.14792, 2022.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learn-\ning, pp. 2256\u20132265. PMLR, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv\npreprint arXiv:2010.02502, 2020.\nYang Song and Stefano Ermon. Generative Modeling by Estimating Gradients of the Data Distribu-\ntion. NeurIPS, 2019.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021.\nBelinda Tzen and Maxim Raginsky. Neural Stochastic Differential Equations: Deep Latent Gaussian\nModels in the Diffusion Limit. In arXiv:1905.09883, 2019.\nThomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Rapha\u00a8el Marinier, Marcin Michalski,\nand Sylvain Gelly. FVD: A new Metric for Video Generation. In ICLR 2022 Workshop: Deep\nGenerative Models for Highly Structured Data, 2019.\nCarl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics.\nArXiv, abs/1609.02612, 2016.\nJay Whang, Mauricio Delbracio, Hossein Talebi, Chitwan Saharia, Alexandros G. Dimakis, and\nPeyman Milanfar. Deblurring via Stochastic Re\ufb01nement. In CVPR, 2022.\nRuihan Yang, Prakhar Srivastava, and Stephan Mandt. Diffusion Probabilistic Modeling for Video\nGeneration. In arXiv:2203.09481, 2022.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin\nLi, Han Zhang, and Yonghui Wu Jason Baldridge. Scaling Autoregressive Models for Content-\nRich Text-to-Image Generation. In arXiv:2206.10789, 2022.\n18\n",
    "2310.01415": "Foundation Models for Decision Making Workshop at NeurIPS 2023\nGPT-DRIVER: LEARNING TO DRIVE WITH GPT\nJiageng Mao1\nYuxi Qian1\nJunjie Ye1\nHang Zhao2\nYue Wang1\n1University of Southern California\n2Tsinghua University\nABSTRACT\nWe present a simple yet effective approach that can transform the OpenAI GPT-3.5\nmodel into a reliable motion planner for autonomous vehicles. Motion planning is\na core challenge in autonomous driving, aiming to plan a driving trajectory that is\nsafe and comfortable. Existing motion planners predominantly leverage heuristic\nmethods to forecast driving trajectories, yet these approaches demonstrate insuffi-\ncient generalization capabilities in the face of novel and unseen driving scenarios.\nIn this paper, we propose a novel approach to motion planning that capitalizes\non the strong reasoning capabilities and generalization potential inherent to Large\nLanguage Models (LLMs). The fundamental insight of our approach is the refor-\nmulation of motion planning as a language modeling problem, a perspective not\npreviously explored. Specifically, we represent the planner inputs and outputs as\nlanguage tokens, and leverage the LLM to generate driving trajectories through\na language description of coordinate positions. Furthermore, we propose a novel\nprompting-reasoning-finetuning strategy to stimulate the numerical reasoning po-\ntential of the LLM. With this strategy, the LLM can describe highly precise trajec-\ntory coordinates and also its internal decision-making process in natural language.\nWe evaluate our approach on the large-scale nuScenes dataset, and extensive ex-\nperiments substantiate the effectiveness, generalization ability, and interpretability\nof our GPT-based motion planner. Code is now available here.\n1\nINTRODUCTION\nAutonomous driving stands as one of the most ambitious and challenging frontiers in modern tech-\nnology, aiming to revolutionize transportation systems globally. Central to this endeavor is the con-\ncept of motion planning, a cornerstone in autonomous driving technology that seeks to devise safe\nand comfortable driving trajectories for autonomous vehicles. The intricacies of motion planning\narise from its need to accommodate diverse driving scenarios and make reasonable driving decisions.\nAs autonomous vehicles interact with various environments and unpredictable human drivers, the\nrobustness and explainability of motion planners become essential for driving safety and reliability.\nExisting motion planning approaches generally fall into two categories. The rule-based methods\n(Treiber et al., 2000; Thrun et al., 2006; Bacha et al., 2008; Leonard et al., 2008; Urmson et al., 2008;\nChen et al., 2015; Sauer et al., 2018; Fan et al., 2018) designed explicit rules to determine driving\ntrajectories. These methods have clear interpretability but generally fail to handle extreme driving\nscenarios that are not covered by rules. Alternatively, the learning-based approaches (Bojarski et al.,\n2016; Codevilla et al., 2018; 2019; Rhinehart et al., 2019; Zeng et al., 2019; Sadat et al., 2020; Casas\net al., 2021; Hu et al., 2022; 2023; Dauner et al., 2023) resorted to a data-driven strategy and learned\ntheir models from large-scale human driving trajectories. While exhibiting good performance, these\napproaches sacrifice interpretability by viewing motion planning as a black-box forecasting problem.\nEssentially, both prevailing rule-based and learning-based approaches are devoid of the common\nsense reasoning ability innate to human drivers, which restricts their capabilities in tackling long-\ntailed driving scenarios.\nRecent advances in Large Language Models (LLMs) (Brown et al., 2020; Ouyang et al., 2022;\nOpenAI, 2023; Touvron et al., 2023a;b) have demonstrated great generalization power and common\nsense reasoning ability emerged from these language models, indicating their potential in addressing\nproblems in the realm of autonomous driving. An important question naturally arises: How can we\nleverage LLMs to resolve the motion planning problem? The major challenge is that motion planners\nare required to process heterogeneous inputs, e.g., ego-vehicle information, maps, and perception\n1\narXiv:2310.01415v3  [cs.CV]  5 Dec 2023\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nresults, and they need to predict high-precision waypoint coordinates that represent a future driving\ntrajectory. While LLMs excel at language understanding, they cannot directly handle heterogeneous\ndata. Moreover, it is yet to be established whether LLMs are capable of precise numerical reasoning,\ne.g. forecasting precise coordinate values that are demanded by motion planning.\nTo this end, we propose a novel approach that successfully unleashes the power of LLMs to address\nthe motion planning problem in autonomous driving. The critical insight is that we can reformulate\nmotion planning as a language modeling problem. Specifically, we propose to tackle the heteroge-\nneous planner inputs by transforming them into unified language tokens, and we instruct a GPT-3.5\nmodel to understand these tokens and then articulate the waypoint coordinates of a future driving tra-\njectory through natural language description. We further elucidate the essence of language modeling\nin motion planning from the perspective of tokenizers. Moreover, to stimulate the numerical reason-\ning potential of GPT-3.5, we propose a prompting-reasoning-finetuning strategy, where GPT-3.5 is\ninitially prompted in the context of autonomous driving, and then performs chain-of-thought reason-\ning to generate sensible outputs, and finally the model is fine-tuned with human driving trajectories\nto ensure alignments with human driving behaviors. With this strategy, GPT-3.5 is able to fore-\ncast highly precise waypoint coordinates with only a centimeter-level error. The chain-of-thought\nreasoning further enhances transparency in decision-making and makes our approach more inter-\npretable than other learning-based methods. Benefiting from the state-of-the-art GPT-3.5 model,\nour approach also exhibits good generalization and common sense reasoning ability.\nWe summarize our contributions as follows:\n\u00b7 We propose GPT-Driver, a GPT-based motion planner, innovatively transforming the motion plan-\nning task into a language modeling problem. We also provide an intuitive interpretation of language\nmodeling in motion planning through the lens of the GPT tokenizer.\n\u00b7 We propose a novel prompting-reasoning-finetuning strategy in the context of autonomous driving,\nwhich enables precise numerical reasoning and transparent decision-making of our approach.\n\u00b7 Our GPT-Driver demonstrates superior motion planning performance, few-shot generalization abil-\nity, and interpretability compared to the state-of-the-art motion planners on the nuScenes dataset.\n2\nRELATED WORKS\nMotion planning in autonomous driving. Motion planning aims to forecast safe and comfortable\ndriving routes for autonomous vehicles. Existing approaches can be divided into three categories:\nrule-based, optimization-based, and learning-based methods. The rule-based approaches (Treiber\net al., 2000; Thrun et al., 2006; Bacha et al., 2008; Leonard et al., 2008; Urmson et al., 2008; Chen\net al., 2015; Sauer et al., 2018; Fan et al., 2018; Dauner et al., 2023) resort to pre-defined rules\nto determine future driving trajectories. Intelligent Driver Model (Treiber et al., 2000) (IDM) is a\nseminal work that proposed a heuristic motion model to follow a leading vehicle in traffic while\nmaintaining a safe distance. Despite being simple and interpretable, IDM lacks sufficient capability\nto handle complicated driving behaviors such as U-turns. The optimization-based approaches (Li\net al., 2022; Liniger et al., 2015; Scheffe et al., 2022) formulate motion planning as an optimal\ncontrol problem. In contrast, the learning-based approaches (Bojarski et al., 2016; Codevilla et al.,\n2018; 2019; Rhinehart et al., 2019; Zeng et al., 2019; Sadat et al., 2020; Casas et al., 2021; Hu et al.,\n2022; 2023) proposed to handle complex driving scenarios by learning from large-scale human\ndriving data. Neural motion planner (Zeng et al., 2019) suggested using a learned cost volume to\nassess each feasible driving trajectory. P3 (Sadat et al., 2020), MP3 (Casas et al., 2021), ST-P3 (Hu\net al., 2022), and UniAD (Hu et al., 2023) proposed end-to-end learning of planning and other tasks\nin autonomous driving. These approaches rely on deep neural networks, while the decision-making\nprocess is implicitly encoded in neural networks and thus less interpretable.\nOur GPT-Driver is a learning-based motion planner. In contrast to other learning-based approaches,\nwe leverage the generalization and reasoning ability of the GPT-3.5 model, which enables our model\nto tackle those long-tailed driving scenarios that are generally challenging to other methods. Our\nmethod also has better interpretability thanks to the novel prompting-reasoning-finetuning strategy.\nLarge language models. Large Language Models (LLMs) are artificial intelligence systems trained\non Internet-scale data to understand and generate human-like text, showcasing remarkable abili-\n2\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nPerception and Prediction\nLanguage Descriptions\nPerception and Prediction:\n- Car at (12.05,4.12), moving to (11.98, 2.30)\n\u2026\nEgo-States:\n- Velocity: (0, 2.34)\n\u2026\n \nGPT as a \nMotion Planner\nLanguage Outputs\nNotable Objects:\n- Car at (2.34,19.08) \u2026\nPotential Effects:\n- Within the safety zone of the ego-vehicle \u2026\nPlanned Trajectory:\n[(0.12, 2.98), \u2026, (3.45,18.90)]\n \nMotion Planning Results\nFigure 1: Overview of GPT-Driver. We reformulate motion planning as a language modeling prob-\nlem. We convert observations and ego-states into language prompts, guiding the LLM to produce\na planned trajectory alongside its decision-making process in natural language. Subsequently, this\nplanned trajectory is reverted to the numerical format for motion planning.\nties in natural language processing. GPT (Brown et al., 2020) is a pioneering work that proposed\nthe Generative Pre-trained Transformer to tackle language understanding and generation problems.\nThe following versions GPT-3.5 and GPT-4 (OpenAI, 2023) demonstrated impressive chatting and\nreasoning ability. LLaMA and LLaMA 2 (Touvron et al., 2023a;b) are open-source foundation\nlanguage models. To better harness the capabilities of LLMs, InstructGPT (Ouyang et al., 2022)\nproposed to train LLMs to follow instructions with human feedback. (Wei et al., 2022) proposed\nchain-of-thought prompting to enhance the reasoning ability of LLMs. ReAct (Yao et al., 2022)\nexploited the synergy of reasoning and acting in LLMs. These methods have bolstered the language\nunderstanding and decision-making capabilities of LLMs. Despite the success of LLMs in language\nunderstanding, exploiting the power of LLMs in autonomous driving remains an open challenge, as\nthe inputs and outputs of autonomous systems are not language. In this paper, we tackle this chal-\nlenge by reformulating the traditional driving problem into a language modeling problem. Moreover,\nwe propose a novel prompting-reasoning-finetuning strategy tailored for autonomous driving, which\nis significantly different from the existing works (Yao et al., 2022; Wei et al., 2022) and amplifies\nthe reasoning capabilities of the LLM-based planner.\nThere is also a series of works (Ahn et al., 2022; Fu et al., 2023; Huang et al., 2022; Song et al.,\n2022) using LLMs for task-level planning, i.e., planning high-level actions for embodied agents. In\ncontrast, our method focuses on motion planning, i.e. planning waypoint-based low-level driving\ntrajectories for autonomous vehicles. Unlike the natural language descriptions used for high-level\nactions, trajectories are represented as sets of numerical coordinates, posing a greater challenge for\nLLMs. To the best of our knowledge, our work is the first to demonstrate GPT-3.5\u2019s capability for\ndetailed numerical reasoning in motion planning.\n3\nGPT-DRIVER\nIn this section, we present GPT-Driver, an LLM-based motion planner for autonomous driving.\nAn overview of our GPT-Driver is shown in Figure 1. We first introduce the basic concept and\nproblem definition of motion planning in the context of autonomous driving (Section 3.1). Then,\nwe demonstrate how to reformulate motion planning as a language modeling problem (Section 3.2).\nFinally, we introduce how to address this language modeling problem using a novel prompting-\nreasoning-finetuning strategy (Section 3.3).\n3.1\nPROBLEM DEFINITION\nThe objective of motion planning in autonomous driving is to plan a safe and comfortable driving\ntrajectory T with observations O and ego-states S as input. The motion planning process F can be\nformulated as:\nT = F(O, S).\n(1)\n3\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nA planned trajectory T can be represented as a set of waypoints of t timesteps: T \u2208Rt\u00d72:\nT = {(x1, y1), \u00b7 \u00b7 \u00b7 , (xt, yt)},\n(2)\nwhere (xi, yi) is a 2D waypoint coordinate that denotes the vehicle\u2019s anticipated location at the\ntimestep i. The ego-states S generally consist of a historical trajectory of this vehicle and its current\nstatus such as velocity and acceleration. The observations O contain the outputs of perception and\nprediction systems, e.g., detected object bounding boxes and their future motions.\nThe learning-based motion planners generally learn the trajectory T by imitating a human driver\u2019s\ndriving trajectory \u02c6T with L1 regression, where the loss function Lreg can be formulated as:\nLreg =\nT\nX\ni=1\n(|xi \u2212\u02c6xi| + |yi \u2212\u02c6yi|),\n(3)\nwhere (xi, yi) and (\u02c6xi, \u02c6yi) are waypoints of the planned trajectory T and the human trajectory T \u2032\nrespectively. Albeit simple, these approaches attempt to simultaneously regress waypoints across\ndifferent scales, e.g. coordinate values ranging from 0 to over 50, which generally results in impre-\ncise coordinate estimations of the more distant waypoints. To this end, we propose a novel approach\nthat supplants the traditional L1 trajectory regression with a language modeling framework.\n3.2\nMOTION PLANNING AS LANGUAGE MODELING\nThe crucial insight of this paper is to transform motion planning into a language modeling problem.\nGiven a driving trajectory T , we can represent it as a sequence of words that describe this trajectory:\nT = K({(x1, y1), \u00b7 \u00b7 \u00b7 , (xt, yt)}) = {w1, \u00b7 \u00b7 \u00b7 , wn},\n(4)\nwhere wi is the i-th word in this sequence. Please note that each coordinate value x or y in Equation\n2 can be freely transformed into a set of words {w} using a language tokenizer K. For instance, a\ncoordinate value 23.17 can be transformed into three words: \u201c23\u201d, \u201c.\u201d, and \u201c17\u201d using the GPT-3.5\ntokenizer. With this language representation, we can then reformulate the motion planning problem\nas a language modeling problem:\nLLM = \u2212\nN\nX\ni=1\nlog P( \u02c6wi|w1, \u00b7 \u00b7 \u00b7 , wi\u22121),\n(5)\nwhere w and \u02c6w are the words from the planned trajectory T and the human driving trajectory \u02c6T\nrespectively. By learning to maximize the occurrence probability P of the words \u02c6w derived from the\nhuman driving trajectory \u02c6T , motion planners can generate human-like driving trajectories.\nWe can derive a natural interpretation of how language modeling works in motion planning through\nthe lens of tokenization. Take the coordinate value 23.17 as an example. Through tokenization, it is\ndecomposed into \u201c23\u201d which is the integer part of this value, \u201c.\u201d, and \u201c17\u201d which is the decimal part\nof this value. Hence, the process of predicting this waypoint coordinate is essentially first estimating\na coarse location at the meter level (\u201c23\u201d here) and then estimating a fine-grained location at the\ncentimeter level (\u201c17\u201d here). Moreover, the estimations are established by classifications of the\ncorrect tokens in the vocabulary, rather than regression of their absolute values.\nWe note that language modeling has been employed in other tasks of computer vision and robotics,\nsuch as object detection (Chen et al., 2021; Xue et al., 2022; Wang et al., 2023) and robotic control\n(Brohan et al., 2023). However, these approaches heavily rely on specially designed tokens and to-\nkenizers, which makes their methods less intuitive and hard to generalize to other tasks. In contrast,\nour key observation is that a commonly used language tokenizer such as the GPT tokenizer already\nhas sufficient capability to estimate very precise numerical values for motion planning. This unique\nfinding makes our approach significantly simpler than prior methods, and also makes our approach\nmore generalizable and compatible with natural language.\n4\nFoundation Models for Decision Making Workshop at NeurIPS 2023\n**Autonomous Driving Planner**\nRole: You are the brain of an autonomous vehicle. Plan a safe 3-second driving trajectory. Avoid collisions with other objects.\nContext\n- Coordinates: X-axis is perpendicular, and Y-axis is parallel to the direction you're facing. You're at point (0,0).\n- Objective: Create a 3-second route using 6 waypoints, one every 0.5 seconds.\nInputs\n1. Perception & Prediction: Info about surrounding objects and their predicted movements.\n2. Ego-States: Your current state including velocity, heading angular velocity, can bus data, heading speed, and steering signal.\n3. Historical Trajectory: Your past 2-second route, given by 4 waypoints.\n4. Mission Goal: High-level goal for the next 3 seconds.\nTask\n- Thought Process: Note down critical objects and potential effects from your perceptions and predictions.\n- Action Plan: Detail your meta-actions based on your analysis.\n- Trajectory Planning: Develop a safe and feasible 3-second route using 6 new waypoints.\nOutput\n- Thoughts:\n  - Notable Objects\n    Potential Effects\n- Meta Action\n- Trajectory (MOST IMPORTANT):\n  - [(x1,y1), (x2,y2), ... , (x6,y6)]\nPerception and Prediction:                                                                                                                                                                                                                                 \n - animal at (-1.93,7.00), moving to (-2.31,10.89).                                                                                                                                                                                           \n - car at (-8.67,0.12), moving to (-8.50,-0.08).                                                                                                                                                                                                  \n - adult at (-1.21,6.78), moving to (-1.29,10.48).                                                                                                                                                                                              \nEgo-States:                                                                                                                                                                                                                                                   \n - Velocity (vx,vy): (0.00,1.46)                                                                                                                                                                                                                          \n - Heading Angular Velocity (v_yaw): (-0.00)                                                                                                                                                                                                   \n - Acceleration (ax,ay): (0.01,-0.15)                                                                                                                                                                                                                  \nHistorical Trajectory (last 2 seconds): [(-0.00,-6.74), (-0.03,-4.73), (-0.03,-3.07), (-0.02,-1.46)]                                                                                                                    \nMission Goal: RIGHT   \nFigure 2: An example of input prompts provided to the LLM. The upper text box offers a uni-\nversal context related to motion planning for every driving scenario. The lower text box provides a\nlanguage description of the observations and ego-states specific to this particular frame. Parameter-\nized inputs are highlighted in red.\n3.3\nPROMPTING-REASONING-FINETUNING\nDespite the potential of language modeling in motion planning, simply adopting (Wei et al., 2022;\nOuyang et al., 2022; Yao et al., 2022) and prompting GPT-3.5 to generate trajectories didn\u2019t work\nin practice (See Section 4.5). To this end, we introduce a novel prompting-reasoning-finetuning\nstrategy that stimulates the potential of language modeling to address the motion planning problem.\nSpecifically, we introduce a method that utilizes the GPT tokenizer K to convert observations O and\nego-states S into language prompts. These prompts are then fed into the GPT-3.5 model FGP T . We\ninstruct the model to articulate its decision-making process explicitly and produce planned trajecto-\nries T in natural language. Finally, we fine-tune the GPT model\u2019s outputs to ensure alignment with\nhuman driving trajectories. The prompting-reasoning-finetuning process can be formulated as\n{T , R} = FGP T (K(O, S)),\n(6)\nwhere T = {w1, \u00b7 \u00b7 \u00b7 , wn} is a language description of the trajectory in Equation 4, and R denotes\na language description of the chain-of-thought reasoning and decision-making process. In contrast\nto the traditional motion planning methods that solely generate planned trajectories, our approach\ngenerates both the trajectories T and the explicit reasoning process R, which makes our model\u2019s\ndecision-making process more transparent. Hence, our approach demonstrates better interpretability\nthan the existing methods.\nIn subsequent sections, we delve into details of the prompting, reasoning, and fine-tuning process.\n5\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nThoughts:                                                                                                                                                                                                                                                     \n - Notable Objects from Perception: animal at (-1.93,7.00)                                                                                                                                                                             \n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 1.0-second timestep                                                                                                 \n - Notable Objects from Perception: adult at (-1.21,6.78)                                                                                                                                                                                \n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 1.0-second timestep                                                                                                 \nMeta Action: TURN RIGHT WITH A CONSTANT SPEED                                                                                                                                                                       \nTrajectory:                                                                                                                                                                                                                                                   \n[(0.11,1.14), (0.45,2.28), (1.12,3.47), (2.18,4.54), (3.65,5.29), (5.49,5.58)]    \nFigure 3: An example of the expected outputs of the LLM. The chain-of-thought reasoning and\nthe planned trajectory are highlighted in red.\nPrompting. A key obstacle in using LLMs for motion planning is the disparity in data types: while\nmotion planners process heterogeneous inputs of observations and ego-states, LLMs are primarily\ndesigned to handle language inputs. To overcome the above limitations, we resort to the parame-\nterized representations of observations and ego-states and convert them into language descriptions.\nIn particular, we utilize detected objects that are parameterized by their class names and locations\nas perception results. For each object, we formulate a sentence capturing these attributes. These\nsentences collectively form the perception prompts. Similarly, we can craft prediction prompts by\nconverting the parameterized future trajectories of detected objects into natural language descrip-\ntions. We can also generate the prompts for ego-states by articulating the ego vehicle\u2019s current\nstatus such as velocity and heading. Furthermore, we provide general context information about\nmotion planning, such as the coordinate system, objective, etc. Finally, we rephrase these prompts\nin a more concise format using ChatGPT-4 and utilize them as the inputs to the GPT-3.5 model. An\nexample of prompts is shown in Figure 2.\nReasoning. A common weakness of current motion planners is their limited interpretability, since\nthese planners generate planned trajectories from black-box neural networks without elucidating the\nreasoning behind their decisions. To address this problem, we propose a novel chain-of-thought\nreasoning strategy specifically designed for autonomous driving. In particular, we summarize the\nchain-of-thought reasoning process in autonomous driving into 3 steps: First, from the perception\nresults, the motion planner needs to identify those critical objects that may affect its driving dynam-\nics. Second, by analyzing the future motions of these critical objects from the prediction results, the\nplanner should infer when, where, and how this critical object may influence the ego vehicle. Third,\non top of the insights gained from the previous analyses, the planner needs to draw a high-level\ndriving decision and then convert it into a planned trajectory. This three-step reasoning framework\noffers a more structured approach to motion planning and ensures greater transparency throughout\nthe planning procedure. An example is shown in Figure 3.\nFine-tuning. To align the LLM\u2019s outputs with human driving behaviors, we employ a simple fine-\ntuning strategy using the OpenAI fine-tuning API. Specifically, we collect human driving trajectories\n\u02c6T for each scenario from driving logs. To generate the ground truth guidance of chain-of-thought\nreasoning \u02c6R, we initially compute a hypothetical ego-trajectory based on the current velocity and\nacceleration of the ego vehicle, assuming there is no interference. Then, we identify the critical\nobjects and their potential effects by examining if any objects, based on their present positions\nand predicted future paths, overlap with the hypothetical ego-trajectory. We found this strategy\nworks well in practice, enabling us to bypass the tedious task of manually annotating the reasoning\nprocess. Finally, we can fine-tune the LLM\u2019s outputs {T , R} with the ground truth { \u02c6T , \u02c6R} using the\nlanguage modeling loss LLM defined in Equation 5. During inference, we transform the language\noutput of a planned trajectory back to its numerical format for evaluation.\n4\nEXPERIMENTS\nIn this section, we demonstrate the effectiveness, generalization ability, and interpretability of our\nGPT-Driver through extensive experiments on the large-scale and real-world nuScenes dataset (Cae-\nsar et al., 2020). We first introduce the experimental settings and evaluation metrics, and then com-\npare our approach against state-of-the-art motion planning methods on the nuScenes dataset. Finally,\nwe conduct studies to evaluate the generalization and interpretability of our approach.\n6\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nMethod\nL2 (m) \u2193\nCollision (%) \u2193\n1s\n2s\n3s\nAvg.\n1s\n2s\n3s\nAvg.\nST-P3 metrics\nST-P3 Hu et al. (2022)\n1.33\n2.11\n2.90\n2.11\n0.23\n0.62\n1.27\n0.71\nVAD Jiang et al. (2023)\n0.17\n0.34\n0.60\n0.37\n0.07\n0.10\n0.24\n0.14\nGPT-Driver (ours)\n0.20\n0.40\n0.70\n0.44\n0.04\n0.12\n0.36\n0.17\nUniAD metrics\nNMP Zeng et al. (2019)\n-\n-\n2.31\n-\n-\n-\n1.92\n-\nSA-NMP Zeng et al. (2019)\n-\n-\n2.05\n-\n-\n-\n1.59\n-\nFF Hu et al. (2021)\n0.55\n1.20\n2.54\n1.43\n0.06\n0.17\n1.07\n0.43\nEO Khurana et al. (2022)\n0.67\n1.36\n2.78\n1.60\n0.04\n0.09\n0.88\n0.33\nUniAD Hu et al. (2023)\n0.48\n0.96\n1.65\n1.03\n0.05\n0.17\n0.71\n0.31\nGPT-Driver (ours)\n0.27\n0.74\n1.52\n0.84\n0.07\n0.15\n1.10\n0.44\nTable 1: Motion planning performance. Our approach significantly outperforms prior works by a\nlarge margin in L2 and performs on par with the top methods in collision rate.\n4.1\nEXPERIMENTAL SETUP\nThe nuScenes dataset is a large-scale and real-world autonomous driving dataset. It contains 1000\ndriving scenarios and approximately 40000 key frames encompassing a diverse range of locations\nand weather conditions. We follow the general practice in prior works (Hu et al., 2022; 2023; Jiang\net al., 2023) and split the whole dataset into training, validation, and testing sets. We use the training\nset to fine-tune our model and evaluate our model\u2019s performance on the validation set, which ensures\na fair comparison with prior works.\nFor a fair comparison with other methods, we adopt the evaluation metrics in UniAD (Hu et al.,\n2023) to evaluate our planned trajectories. It contains two metrics: L2 error (in meters) and collision\nrate (in percentage). The average L2 error is computed by measuring each waypoint\u2019s distance in\nthe planned and ground-truth trajectories. It reflects the proximity of a planned trajectory to a human\ndriving trajectory. The collision rate is computed by placing an ego-vehicle box on each waypoint\nof the planned trajectory and then checking for collisions with the ground truth bounding boxes\nof other objects. It reflects the safety of a planned trajectory. We follow the common practice in\nprevious works and evaluate the motion planning result in the 3-second time horizon.\n4.2\nCOMPARISON AGAINST THE STATE-OF-THE-ART METHODS\nEnd-to-end driving approaches like UniAD (Hu et al., 2023) perform motion planning based on their\ninternal perception and prediction outputs. For a fair comparison with this work, we build our model\non top of the perception and prediction results from their model.\nTable 1 shows the motion planning performance of our GPT-Driver against the state-of-the-art meth-\nods. It is clear that our GPT-Driver significantly outperforms the prior works in the L2 metric by\na large margin, demonstrating the effectiveness of our approach in generating human-like driving\ntrajectories. L2 is a strong indicator of the imitation learning ability of motion planners. Our ap-\nproach surpasses the state-of-the-art approaches in L2, indicating that the fine-tuned LLM has a\nstronger imitation learning ability compared to MLP-based planners. The collision rate serves as a\nstrong indicator of the safety of motion planning. Our approach also aligns closely with the state-\nof-the-art methods in the collision metric, indicating our capability to plan safe driving trajectories.\nPlease note that other baseline methods heavily rely on tricks such as post-optimization to lower\nthe collision rate. By contrast, our approach doesn\u2019t rely on these tricks. It is worth noting that\nthese state-of-the-art planners (Hu et al., 2023) heavily rely on dense occupancy grids and maps, in\naddition to detection and prediction, which makes their systems intricate and time-consuming. In\ncontrast, our approach only takes language descriptions of detections and predictions as input obser-\nvations, which is much simpler than prior methods. Our method also has the potential to incorporate\nvectorized maps to further boost the performance.\n4.3\nFEW-SHOT MOTION PLANNING\nTo further validate the generalization ability of our GPT-Driver, we designed a few-shot motion\nplanning experiment. Specifically, we sampled 1%, 10%, 50% of the training scenarios and utilized\n7\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nMethod\nAvg. L2 (m) \u2193\nAvg. Collision (%) \u2193\n1%\n10%\n50%\n100%\n1%\n10%\n50%\n100%\nUniAD (Hu et al., 2023)\n5.37\n1.80\n1.42\n1.03\n6.86\n1.31\n0.49\n0.31\nGPT-Driver\n1.89\n1.20\n1.01\n0.84\n1.24\n0.95\n0.75\n0.44\nTable 2: Few-shot motion planning results. Our GPT-Driver performs significantly better than\nUniAD when the training data is limited and demonstrates better generalization ability.\nMethod\nL2 (m) \u2193\nCollision (%) \u2193\n1s\n2s\n3s\nAvg.\n1s\n2s\n3s\nAvg.\nGPT-Driver (in-context learning)\n2.41\n3.11\n4.00\n3.17\n4.20\n5.13\n6.58\n5.30\nGPT-Driver (fine-tuning)\n0.27\n0.74\n1.52\n0.84\n0.07\n0.15\n1.10\n0.44\nTable 3: Design choices of in-context learning and fine-tuning. The results indicate fine-tuning is\na more effective strategy for instructing the LLM in motion planning.\nthem for fine-tuning our model and training the state-of-the-art motion planner in UniAD. For a fair\ncomparison, both UniAD and our approach leverage the same pretrained detection and prediction\nmodules as inputs, and all other parameters remain the same. Table 2 illustrates the few-shot motion\nplanning results. Our approach attains decent motion planning results on the validation set when\nexposed to only 10% of the full training scenarios, while UniAD failed to obtain good performance\nwhen the training data is limited. In contrast to other learning-based planners that heavily rely on\nlarge amounts of data, our GPT-Driver fine-tuned on a few training scenarios could generalize well\nto the full validation set, which indicates its strong generalization and few-shot learning ability.\n4.4\nINTERPRETABILITY\nTo demonstrate the interpretability of our GPT-Driver, we visualized the reasoning outputs and the\nplanned trajectories of our model in Figure 4. From the figure, we can observe that our method is\nable to identify critical objects and assess their potential effects from all perception and prediction\ninputs, and then based on these observations it can generate a coherent high-level action as well as\na sensible driving trajectory. For example, in the first sub-figure, our GPT-Driver could identify all\nobstacles such as barriers and traffic cones, and further neglect the far-away white bus that has no\neffect on our driving route. Then it can generate a turn-right action with a deceleration to avoid\ncollisions with these obstacles. Finally, it plans a smooth and safe turning trajectory. In contrast\nto previous methods that only generate planned trajectories, our approach generates not only the\ntrajectories but also the reasoning process of how it predicts these trajectories. Thus our approach\ncan demonstrate better interpretability.\n4.5\nFINE-TUNING VS. IN-CONTEXT LEARNING\nIn-context learning and fine-tuning are two prevalent strategies to instruct an LLM for specific tasks.\nWhile our fine-tuning strategy works well in motion planning, it raises the question of whether in-\ncontext learning could achieve comparable results in this task. To answer this question, we designed\nan in-context learning experiment where we used both the inputs and the expected outputs in the\ntraining set as new exemplar inputs to instruct the LLM. The results in Table 3 suggest that fine-\ntuning performs significantly better than in-context learning. This is mainly because the model\u2019s\ncontext window is quite limited in in-context learning, e.g. GPT-3.5 can accommodate a maximum\nof only 5 exemplar inputs every time in our case. Hence, our fine-tuning strategy is indispensable.\n4.6\nLIMITATIONS\nDue to the limitations of the OpenAI APIs, we are unable to obtain the inference time of our model.\nThus it remains uncertain whether our approach can meet the real-time demands of commercial driv-\ning applications. Typically, the GPT-based planner would exhibit a longer inference time compared\n8\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nThoughts:\n - Notable Objects from Perception: barrier at (-2.63,4.13)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 0.0-second timestep\n - Notable Objects from Perception: trafficcone at (-1.85,5.22)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 0.5-second timestep\n - Notable Objects from Perception: trafficcone at (0.48,11.78)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 2.0-second timestep\n - Notable Objects from Perception: trafficcone at (-0.70,8.18)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 1.0-second timestep\n - Notable Objects from Perception: trafficcone at (-0.26,9.48)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 1.5-second timestep\nMeta Action: TURN RIGHT WITH A DECELERATION\nTrajectory:\n[(0.27,2.69), (0.88,5.08), (1.72,7.21), (2.65,9.14), (3.70,10.87), (4.46,12.03)]\nThoughts:\n - Notable Objects from Perception: adult at (-3.97,19.46)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 3.0-second timestep\n - Notable Objects from Perception: adult at (-2.58,18.30)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 3.0-second timestep\n - Notable Objects from Perception: adult at (-2.87,19.09)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 3.0-second timestep\nMeta Action: MOVE FORWARD WITH A DECELERATION TO ZERO\nTrajectory:\n[(0.02,0.43), (0.03,0.74), (0.03,0.94), (0.03,1.04), (0.04,1.04), (0.04,1.35)]\nThoughts:\n - Notable Objects from Perception: car at (2.43,17.44)\n   Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 2.0-second timestep\nMeta Action: MOVE FORWARD WITH A DECELERATION\nTrajectory:\n[(0.09,2.47), (0.16,4.73), (0.20,6.86), (0.18,8.90), (0.15,10.83), (0.12,12.59)]\nThoughts: \n- Notable Objects from Perception: motorcycle at (-1.43,17.61)\n  Potential Effects from Prediction: within the safety zone of the ego-vehicle at the 2.5-second timestep\nMeta Action: TURN RIGHT WITH A CONSTANT SPEED\nTrajectory:\n[(0.20,2.11), (0.64,4.02), (1.36,5.86), (2.42,7.74), (3.77,9.50), (5.30,11.06)]\nFigure 4: Visualization of the GPT-Driver\u2019s outputs (text boxes on the right) on the validation set.\nPlanned trajectories and notable objects are highlighted accordingly in red on the left images. Please\nnote that the images are only for illustration and are never used in our approach. The visualizations\nindicate that our method can effectively recognize critical objects and their potential impact from all\nperception and prediction inputs, and subsequently plan a sensible driving trajectory.\nto existing MLP-based planners. Nevertheless, we argue that there are many techniques that could\nresolve this problem, e.g. distilling a smaller LLM, etc. We leave this for future work.\nAnother limitation lies in the evaluation of motion planning. As open-loop motion planning doesn\u2019t\nfully emulate error accumulation in the driving process, recently close-loop motion planning has\nbecome increasingly popular to evaluate the performances of motion planners. We leave close-loop\nmotion planning of our GPT-Driver for future work.\n5\nCONCLUSION\nIn this paper, we introduce GPT-Driver, an innovative method that transforms the OpenAI GPT-3.5\nmodel into a dependable motion planner for autonomous driving. We reformulate motion planning\nas a language modeling problem, and we propose a novel prompting-reasoning-finetuning strategy to\ntackle this problem. Through extensive experiments on the large-scale autonomous driving dataset,\nour approach has demonstrated superior planning performance, generalization, and interpretability\ncompared to existing works. Future works include optimizing the inference time and involving more\nsensor observations such as high-definition maps in input prompts.\n9\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nREFERENCES\nMichael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea\nFinn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as i can, not as i say:\nGrounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022.\nAndrew Bacha, Cheryl Bauman, Ruel Faruque, Michael Fleming, Chris Terwelp, Charles Reinholtz,\nDennis Hong, Al Wicks, Thomas Alberi, David Anderson, et al. Odin: Team victortango\u2019s entry\nin the darpa urban challenge. Journal of field Robotics, 25(8):467\u2013492, 2008.\nMariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon\nGoyal, Lawrence D Jackel, Mathew Monfort, Urs Muller, Jiakai Zhang, et al. End to end learning\nfor self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\nAnthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choroman-\nski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action\nmodels transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\nHolger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush\nKrishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for\nautonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern\nrecognition, pp. 11621\u201311631, 2020.\nSergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A unified model to map, perceive, predict and\nplan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 14403\u201314412, 2021.\nChenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. Deepdriving: Learning affordance\nfor direct perception in autonomous driving. In Proceedings of the IEEE international conference\non computer vision, pp. 2722\u20132730, 2015.\nTing Chen, Saurabh Saxena, Lala Li, David J Fleet, and Geoffrey Hinton. Pix2seq: A language\nmodeling framework for object detection. arXiv preprint arXiv:2109.10852, 2021.\nFelipe Codevilla, Matthias M\u00a8uller, Antonio L\u00b4opez, Vladlen Koltun, and Alexey Dosovitskiy. End-\nto-end driving via conditional imitation learning.\nIn 2018 IEEE international conference on\nrobotics and automation (ICRA), pp. 4693\u20134700. IEEE, 2018.\nFelipe Codevilla, Eder Santana, Antonio M L\u00b4opez, and Adrien Gaidon. Exploring the limitations of\nbehavior cloning for autonomous driving. In Proceedings of the IEEE/CVF International Confer-\nence on Computer Vision, pp. 9329\u20139338, 2019.\nDaniel Dauner, Marcel Hallgarten, Andreas Geiger, and Kashyap Chitta. Parting with misconcep-\ntions about learning-based vehicle motion planning. arXiv preprint arXiv:2306.07962, 2023.\nHaoyang Fan, Fan Zhu, Changchun Liu, Liangliang Zhang, Li Zhuang, Dong Li, Weicheng Zhu,\nJiangtao Hu, Hongye Li, and Qi Kong.\nBaidu apollo em motion planner.\narXiv preprint\narXiv:1807.08048, 2018.\nDaocheng Fu, Xin Li, Licheng Wen, Min Dou, Pinlong Cai, Botian Shi, and Yu Qiao.\nDrive\nlike a human: Rethinking autonomous driving with large language models.\narXiv preprint\narXiv:2307.07162, 2023.\nPeiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion plan-\nning with self-supervised freespace forecasting. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 12732\u201312741, 2021.\nShengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-\nend vision-based autonomous driving via spatial-temporal feature learning. In European Confer-\nence on Computer Vision, pp. 533\u2013549. Springer, 2022.\n10\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nYihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du,\nTianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 17853\u201317862, 2023.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky Liang, Pete Florence, Andy Zeng, Jonathan\nTompson, Igor Mordatch, Yevgen Chebotar, et al. Inner monologue: Embodied reasoning through\nplanning with language models. arXiv preprint arXiv:2207.05608, 2022.\nBo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu\nLiu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient au-\ntonomous driving. arXiv preprint arXiv:2303.12077, 2023.\nTarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Dif-\nferentiable raycasting for self-supervised occupancy forecasting. In European Conference on\nComputer Vision, pp. 353\u2013369. Springer, 2022.\nJohn Leonard, Jonathan How, Seth Teller, Mitch Berger, Stefan Campbell, Gaston Fiore, Luke\nFletcher, Emilio Frazzoli, Albert Huang, Sertac Karaman, et al. A perception-driven autonomous\nurban vehicle. Journal of Field Robotics, 25(10):727\u2013774, 2008.\nBai Li, Yakun Ouyang, Li Li, and Youmin Zhang. Autonomous driving on curvy roads without\nreliance on frenet frame: A cartesian-based trajectory planning method. IEEE Transactions on\nIntelligent Transportation Systems, 23(9):15729\u201315741, 2022.\nAlexander Liniger, Alexander Domahidi, and Manfred Morari. Optimization-based autonomous\nracing of 1: 43 scale rc cars. Optimal Control Applications and Methods, 36(5):628\u2013647, 2015.\nOpenAI. Gpt-4 technical report. 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nNicholas Rhinehart, Rowan McAllister, Kris Kitani, and Sergey Levine. Precog: Prediction con-\nditioned on goals in visual multi-agent settings. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 2821\u20132830, 2019.\nAbbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Per-\nceive, predict, and plan: Safe motion planning through interpretable semantic representations. In\nComputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020,\nProceedings, Part XXIII 16, pp. 414\u2013430. Springer, 2020.\nAxel Sauer, Nikolay Savinov, and Andreas Geiger. Conditional affordance learning for driving in\nurban environments. In Conference on Robot Learning, pp. 237\u2013252. PMLR, 2018.\nPatrick Scheffe, Theodor Mario Henneken, Maximilian Kloock, and Bassam Alrifaee. Sequential\nconvex programming methods for real-time optimal trajectory planning in autonomous vehicle\nracing. IEEE Transactions on Intelligent Vehicles, 8(1):661\u2013672, 2022.\nChan Hee Song, Jiaman Wu, Clayton Washington, Brian M Sadler, Wei-Lun Chao, and Yu Su. Llm-\nplanner: Few-shot grounded planning for embodied agents with large language models. arXiv\npreprint arXiv:2212.04088, 2022.\nSebastian Thrun, Mike Montemerlo, Hendrik Dahlkamp, David Stavens, Andrei Aron, James\nDiebel, Philip Fong, John Gale, Morgan Halpenny, Gabriel Hoffmann, et al. Stanley: The robot\nthat won the darpa grand challenge. Journal of field Robotics, 23(9):661\u2013692, 2006.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.\n11\nFoundation Models for Decision Making Workshop at NeurIPS 2023\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Niko-\nlay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b.\nMartin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observa-\ntions and microscopic simulations. Physical review E, 62(2):1805, 2000.\nChris Urmson, Joshua Anhalt, Drew Bagnell, Christopher Baker, Robert Bittner, MN Clark, John\nDolan, Dave Duggins, Tugrul Galatali, Chris Geyer, et al. Autonomous driving in urban environ-\nments: Boss and the urban challenge. Journal of field Robotics, 25(8):425\u2013466, 2008.\nWenhai Wang, Zhe Chen, Xiaokang Chen, Jiannan Wu, Xizhou Zhu, Gang Zeng, Ping Luo, Tong\nLu, Jie Zhou, Yu Qiao, et al. Visionllm: Large language model is also an open-ended decoder for\nvision-centric tasks. arXiv preprint arXiv:2305.11175, 2023.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny\nZhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in\nNeural Information Processing Systems, 35:24824\u201324837, 2022.\nYujing Xue, Jiageng Mao, Minzhe Niu, Hang Xu, Michael Bi Mi, Wei Zhang, Xiaogang Wang, and\nXinchao Wang. Point2seq: Detecting 3d objects as sequences. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, pp. 8521\u20138530, 2022.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao.\nReact: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629,\n2022.\nWenyuan Zeng, Wenjie Luo, Simon Suo, Abbas Sadat, Bin Yang, Sergio Casas, and Raquel Urtasun.\nEnd-to-end interpretable neural motion planner. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 8660\u20138669, 2019.\n12\n",
    "2403.18344": "IEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n1\nLC-LLM: Explainable Lane-Change Intention and\nTrajectory Predictions with Large Language Models\nMingxing Peng, Xusen Guo, Xianda Chen, Meixin Zhu*, and Kehua Chen\nAbstract\u2014To ensure safe driving in dynamic environments,\nautonomous vehicles should possess the capability to accurately\npredict lane change intentions of surrounding vehicles in advance\nand forecast their future trajectories. Existing motion prediction\napproaches have ample room for improvement, particularly in\nterms of long-term prediction accuracy and interpretability. In\nthis paper, we address these challenges by proposing LC-LLM,\nan explainable lane change prediction model that leverages the\nstrong reasoning capabilities and self-explanation abilities of\nLarge Language Models (LLMs). Essentially, we reformulate the\nlane change prediction task as a language modeling problem,\nprocessing heterogeneous driving scenario information as natural\nlanguage prompts for LLMs and employing supervised fine-\ntuning to tailor LLMs specifically for lane change prediction task.\nAdditionally, we finetune the Chain-of-Thought (CoT) reasoning\nto improve prediction transparency and reliability, and include\nexplanatory requirements in the prompts during inference stage.\nTherefore, our LC-LLM model not only predicts lane change\nintentions and trajectories but also provides CoT reasoning and\nexplanations for its predictions, enhancing its interpretability.\nExtensive experiments based on the large-scale highD dataset\ndemonstrate the superior performance and interpretability of\nour LC-LLM in lane change prediction task. To the best of\nour knowledge, this is the first attempt to utilize LLMs for\npredicting lane change behavior. Our study shows that LLMs\ncan effectively encode comprehensive interaction information for\ndriving behavior understanding.\nIndex Terms\u2014Lane change, Large language models, Intention\nprediction, Trajectory prediction, Fine-tuning, Interpretability,\nAutonomous driving.\nI. INTRODUCTION\nL\nANE change prediction is a critical task for autonomous\ndriving systems to ensure safe and efficient navigation on\nthe road. Lane changing behavior, which refers to the action\nof a vehicle moving from one lane to another, is one of the\nmost complex driving behaviors due to intricate interactions\nwith other vehicles. Accurately forecasting the underlying\nlane change intentions and future trajectories of surrounding\nvehicles in advance is essential for autonomous vehicles to\nmake informed decisions and take appropriate actions.\nIn recent years, considerable advancements have been made\nin lane change predictions. Various approaches have been\nManuscript received XX June, 2024.\nCorresponding author is Meixin Zhu (E-mail: meixin@ust.hk).\nMingxing Peng, Xusen Guo and Xianda Chen are with the Systems Hub,\nThe Hong Kong University of Science and Technology (Guangzhou). Meixin\nZhu is with the Systems Hub, The Hong Kong University of Science and\nTechnology (Guangzhou) and Guangdong Provincial Key Lab of Integrated\nCommunication, Sensing and Computation for Ubiquitous Internet of Thing.\nKehua Chen is with the Division of Emerging Interdisciplinary Areas (EMIA),\nAcademy of Interdisciplinary Studies, The Hong Kong University of Science\nand Technology, Hong Kong, China.\nexplored to enhance performance in detecting lane change\nmaneuvers [1], [2], [3], [4]. However, at the early stage of a\nlane change maneuver when lateral displacement is minimal,\nit is essential to rely on the interaction information between\nthe target vehicle and its surrounding vehicles for accurate\npredictions. Therefore, the ability to effectively model interac-\ntion information among vehicles becomes paramount. Previous\nmethods exhibit limited capability in understanding interac-\ntive information, which results in weak long-term prediction\nperformance [1], [3]. Additionally, lane change trajectory\nprediction has witnessed notable progress with the adoption\nof deep learning techniques, such as Graph Neural Networks\n(GNN) and Transformers, leading to competitive results [4],\n[5], [6]. Despite these successes, these deep learning-based\napproaches frequently suffer from a lack of interpretability,\nas they generate predictions about future behaviors without\noffering substantial explanations for their results. In summary,\nenhancing the capability to model interactions among vehicles\nand augmenting the interpretability of prediction results are\nkey challenges in lane change prediction for autonomous\ndriving.\nIn recent years, advancements in Large Language Mod-\nels (LLMs) have provided new opportunities for addressing\nchallenges associated with lane change prediction. Notewor-\nthy progress in LLMs [7], [8], [9] has showcased their\nrobust information comprehension skills and powerful com-\nmon sense reasoning abilities. However, current researches\non lane change prediction have not yet incorporated LLMs.\nGiven the LLMs\u2019 profound understanding of complex driv-\ning scenarios and their extensive knowledge base, including\ncommon driving knowledge, it is rational to utilize LLMs in\nmodeling interactions between vehicles for the enhancement\nof future driving behavior predictions. In addition, LLMs have\ndemonstrated the capacity to leverage their vast knowledge\nbase to generate explanations for their predictions [10], [11].\nInspired by these studies, there is potential to leverage LLMs\nfor the explanation of predicted lane change intentions and\nfuture trajectories, thereby augmenting the interpretability of\npredictions in autonomous driving.\nIn this paper, we propose a novel approach that leverages\nLLMs\u2019 powerful reasoning and self-explanation capabilities to\naddress lane change prediction challenges mentioned above.\nIn essence, we reformulate the task of predicting lane change\nintentions and trajectories as a language modeling problem. To\nthis end, we integrate heterogeneous inputs, including target\nvehicle state, surrounding vehicle state, and map information,\nconverting them into structured prompts in natural language\nto input into the LLM. Then, we employ a supervised fine-\narXiv:2403.18344v2  [cs.AI]  5 Aug 2024\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n2\ntuning technique to tailor the LLM specifically for the lane\nchange prediction task. Additionally, in fine-tuning stage,\nwe perform Chain-of-Thought (CoT) reasoning to enhance\ntransparency and reliability in lane change predictions. Finally,\nwe integrate explanatory requirements into the prompts in the\ninference stage, thus enabling our fine-tuned model to generate\nexplanations for the lane change prediction results. Benefiting\nfrom the powerful capabilities of LLM, our fine-tuned model\ndemonstrates better performance and interpretability in lane\nchange prediction.\nThe main contributions of this paper include:\n\u2022 We propose LC-LLM, the first Large Language Model\nfor lane change prediction. It leverages the powerful\ncapabilities of LLMs to understand complex interactive\nscenarios, enhancing the performance of lane change\nprediction.\n\u2022 Our LC-LLM achieves explainable predictions. It not\nonly predicts lane change intentions and trajectories but\nalso generates explanations for the prediction results.\nAdditionally, we evaluate the accuracy of CoT reasoning\nto quantitatively evaluate the interpretability of our LC-\nLLM.\n\u2022 Extensive experiments on the highD dataset demon-\nstrate that our LC-LLM outperforms all baseline models,\nachieving a 17.7% improvement in lane change intention\nprediction, a 64.4% improvement in lateral trajectory\nprediction, and a 66.1% improvement in longitudinal\ntrajectory prediction, respectively.\nThe remainder of the paper is organized as follows. Section\nII reviews previous research related to our work. Section III\npresents the LLM-based lane change intentions and trajectory\nprediction approach. Section IV provides experimental results\ndemonstrating the performance of the proposed approach.\nFinally, the conclusions are drawn in Section V.\nII. RELATED WORK\nThe literature reviews [12], [13], [14] discuss various meth-\nods for predicting the future states of vehicles for autonomous\ndriving systems. These prediction studies can be classified into\nthe following two categories based on the output type: Inten-\ntion prediction and trajectory prediction. Concurrently, LLMs\nhave shown rapid progress in recent developments. Com-\nprehensive overviews of LLMs applications in autonomous\ndriving are discussed in [15].\nA. Intention Prediction\nLane change intention prediction is a critical component\nin the domain of autonomous driving and advanced driver\nassistance systems (ADAS). Research in this field focuses\non accurately predicting vehicles\u2019 intention to change lanes\nbefore the maneuver is executed, thereby enhancing road\nsafety and traffic flow. Various approaches have been ex-\nplored, including machine learning techniques, probabilistic\nmodeling, and deep learning. Mandalia et al. [16] first applied\nthe support vector machine (SVM) to identify lane change\nmaneuvers by incorporating features such as acceleration,\nsteering angle, distance, and so on. Lyu et al. [17] predict\nlane change intentions based on the support vector machine-\nrecursive feature elimination (SVM-RFE) model. He et al.\n[1] designed a Dynamic Bayesian Network (DBN) with the\npurpose of distinguishing between vehicle-following and lane-\nchange maneuvers. Further advancements were made by Hong\net al. [18] and Mozaffari et al. [2], who utilized a convolutional\nneural network (CNN) as feature extractors to fuse complex\ndriving environment context and effectively learn the driving\nbehavior. Zyner et al. [19], [20] employed Long Short-Term\nMemory Networks (LSTM) and Recurrent Neural Networks\n(RNN), respectively, as a sequence classifier for predicting\nvehicles\u2019 driving intentions. Xin et al. [3] implemented a\ndual-block LSTM architecture, where the first LSTM block\nprocesses sequential trajectory data to recognize driver in-\ntentions as an intermediate indicator. Izquierdo et al. [21]\nutilized a hybrid CNN-LSTM model to capture both local and\nglobal contextual features, as well as temporal information, to\nforecast vehicle lane change intentions. Gao et al. [4] proposed\na dual Transformer model, which includes a lane change\nintention prediction model and a trajectory prediction model.\nAlthough these approaches have been proven to have good\nperformance in detecting lane change maneuvers that have\nalready started, the capability of predicting driving intention\nin advance needs to be improved.\nB. Trajectory Prediction\nResearch on trajectory prediction for autonomous driving\nis a critical area of study, aiming to enhance the safety and\nefficiency of self-driving vehicles by forecasting the future\ntrajectory of surrounding entities such as cars, cyclists, and\npedestrians. Early approaches used bird\u2019s-eye view images as\nthe input and applied a CNN framework to process the ras-\nterized scene for trajectory prediction [22], [23]. Furthermore,\nseveral studies employed LSTM networks, utilizing one LSTM\nas an encoder to capture features from historical trajectories\nand another LSTM as a decoder to predict future trajectories\n[24], [25], [3]. More recent works represented scenes with\nvectorized data such as points and polylines, and processed\nthem with GNNs [26], [27] or Transformers [5], [4], [28] to\neffectively model the interactions between traffic participants\nand environment. In [27], Gao et al. proposed VectorNet\nwhich used GNNs to extract features from vectorized high-\ndefinition (HD) maps, thereby avoiding lossy rendering and\ncomputationally intensive CNN encoding. In [5], Shi et al.\nproposed the Motion transformer framework, which models\nmotion prediction as the joint optimization of global intention\nlocalization and local movement refinement. A recent study [6]\nrepresented continuous trajectories as sequences of discrete\nmotion tokens, framing multi-agent motion prediction as a\nlanguage modeling task. While these deep learning-based\napproaches achieve competitive results, their predictions often\nlack interpretability, which is not conducive to the devel-\nopment of safer and more transparent autonomous driving\nsystems.\nC. LLMs for Autonomous Driving\nRecent advancements in LLMs have demonstrated remark-\nable capabilities in extensive knowledge storage, logical rea-\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n3\nUser message\nSystem message\nLane Change Event\nPrompting\nLC-LLM Predicting\nResponse\n\u2713\nTask Description\n\u2713\nDomain Knowledge\n\u2713\nOutput Format\n\u2713\nMap Information\n\u2713\nTarget Vehicle Info\n\u2713\nHistorical Trajectory\n\u2713\nSurrounding Vehicle Info\n\u2713\nIntention\n       1: Left lane change\n\u2713\nFuture Trajectory\n \n[29.93, 1.41, \u2026, 125.22, 2.69]\nThought\nFinal Answer\nLLM\nLoRA\n\u2713\nNotable Features:\n \nThe speed of the ahead vehicle \u2026\n\u2713\nPotential behaviors:\n \nChange to the left lane for overtaking\nSupervised Fine-tuning\nInterpretability Inference\nFine-tuned \nmodel\n\u2026\n\u27a2Output Explanation\n\u2026\nFinal Answer\nThought\nExplanation\nFig. 1: The pipeline of our LC-LLM. The green trajectory in the Lane Change Event is the future trajectory of the target\nvehicle, which the model aims to predict. Observations are described using natural language prompts, which are input into the\nLLM. Supervised fine-tuning methods are then applied to fine-tune the LLM for accurate prediction of lane change intentions\nand trajectories, as well as the CoT reasoning. During the inference phase, prompts are designed to include explanatory\nrequirements. As a result, the fine-tuned model is capable of predicting the target vehicle\u2019s lane change intentions and future\ntrajectory in the current frame, while simultaneously providing the thought reasoning and explanations for its predictions, thus\nimproving the interpretability of the model\u2019s outputs.\nsoning, and question-answering. Considering these competen-\ncies, it is a natural extension to contemplate the application of\nLLMs to enhance the field of autonomous driving [15]. Wu\net al. [29] integrated LLMs with 3D detection and tracking\ntasks by using language prompts as semantic cues. Mao et\nal. [30] proposed a prompting-reasoning-finetuning strategy,\nenabling LLM to generate driving trajectories and showcased\nthe detailed numerical reasoning abilities of GPT-3.5 in motion\nplanning. Chen et al. [31] designed an object-level multimodal\nLLM architecture that combines vectorized numeric modalities\nwith a pre-trained LLM. Additionally, Xu et al. [32] presented\nan interpretable autonomous driving system employing LLMs\nand developed a visual instruction tuning dataset for inter-\npretable autonomous driving. While LLMs have been exten-\nsively explored in the field of autonomous driving perception\nand planning, their application in the domains of intention\nprediction and trajectory prediction remains unexplored.\nIII. METHODOLOGY\nIn this section, we introduce our LC-LLM, a model based on\nLLM designed for predicting lane change intentions and tra-\njectory in autonomous driving systems. The whole pipeline of\nour LC-LLM is depicted clearly in Fig. 1. We reconceptualize\nthe task of predicting intentions and trajectory as a language\nmodeling problem. To this end, we articulate observations\nusing natural language as prompts for input into the LLM\nand leverage supervised fine-tuning techniques to tailor the\nLLM to this specific task. During the inference stage, we\nincorporate explanatory requirements into the prompt. As a\nresult, our fine-tuned model, LC-LLM, not only forecasts the\nlane change intentions and future trajectories but also provides\nCoT reasoning and explanations for the predictions, thereby\nenhancing their interpretability.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n4\nLane change frame\nCurrent frame\nAdvanced prediction time \ud835\udc47\nFig. 2: A lane change scenario. The target vehicle is depicted\nin green and the surrounding vehicles are depicted in blue. The\norange line illustrates the trajectory of the target vehicle in the\nfuture t timesteps. The advanced prediction time T denotes\nthe temporal interval between the lane change frame and the\ncurrent frame.\nA. Problem Formulation\nIn this paper, we aim to develop a predictive model based\non LLM that is designed to simultaneously predict the lane\nchange intention and the trajectory of the target vehicle. A\nrepresentative lane change scenario is depicted in Fig. 2, where\nthe target vehicle is illustrated in green and the surrounding\nvehicles in blue. The objective is to forecast the position of the\ntarget vehicle\u2019s trajectory at t future timesteps, as well as to\ndetermine its lane change intentions within the same temporal\nwindow, as indicated by the orange line in Fig. 2.\nWe define the state of the target vehicle as tv, the states of\nthe surrounding vehicles as sv, and the map information as m,\nwhich serve as the inputs to our model. The outputs are the\nCoT reasoning C, the lane change intention I and the future\ntrajectory T . The process of the model F can be formulated\nas follows:\nC, I, T = F(tv, sv, m)\n(1)\nThe I \u2208{0, 1, 2}, where 0 denotes \u201ckeep lane\u201d, 1 denotes\n\u201cleft lane change\u201d, and 2 denotes \u201cright lane change\u201d. The\nfuture trajectory of prediction horizon t can be denoted by\nT\n= {(x1, y1), (x2, y2), ..., (xt, yt)}. And the corresponding\nCoT reasoning C is composed of notable features and potential\nbehavious. The target vehicle state tv consists of a historical\ntrajectory of this vehicle and its current velocity, type and so\non. The surrounding vehicle states m contain each vehicle\u2019s\ncurrent velocity, type, and their spatial relationship relative to\nthe target vehicle. The map information m includes details\nsuch as lane identifiers and lane markings.\nWe reframe the task of predicting intentions and trajectory\nas a language modeling problem. By utilizing natural language\nto describe both the input data and output results, we can\nrepresent them as a sequence of tokens. The input data which\ndescribe the current frame driving scenario can be denoted as\na sequence of tokens Ts:\nTs = K(tv, sv, m)\n(2)\nThe prediction result which describes the lane change inten-\ntions and trajectory can also be represented as a sequence of\ntokens {T1, T2, ..., Tn}:\n{T1, T2, ..., Tn} = K(C, I, T )\n(3)\nwhere the K is a language tokenizer used to transform input\ndata and output results into tokens. The Ti represents the i-\nth token in the sequence. By adopting this language-based\nrepresentation, we can redefine the prediction problem as a\nlanguage modeling problem, with the loss function resembling\nthat of language modeling [33]:\nL = \u2212\nn\nX\ni=1\nlog P(T \u2217\ni |T<i, Ts)\n(4)\nwhere T\u2217\ni denotes the ground truth of the next token given\nits historical tokens, and n represents the tokens number of\nground truth in one sample. By maximizing the conditional\nprobability P of the token T\u2217\ni , our model can effectively predict\nlane change intentions and future trajectory.\nB. Prompting\nGenerally, LLMs receive inputs in the form of natural lan-\nguage rather than unprocessed vectorized data. Consequently,\nthe formulation of effective prompts that describe the current\nobservations in natural language is critical. Several studies\nhave sought to harness the deep reasoning capabilities of the\nLLMs through clever prompt design [30], [34]. Building on\nthese prior efforts, we have crafted prompts that are clearer,\nmore intelligent, and better structured. Fig. 3 provides an\nexample of our input prompts.\nAs illustrated in Fig. 3, the input prompts consist of a\nsystem message displayed in the upper text block and a user\nmessage presented in the lower text block. The system mes-\nsage maintains consistency across diverse driving scenarios.\nIt delineates the designated role of the LLM, provides details\nof the coordinate system, and outlines the information and\nformat for the LLM\u2019s output. In our paper, the assigned role\nfor the LLM is that of a predictive model integrated within an\nautonomous driving system. The coordinate system in each\ndriving scenario is the vehicle coordinate system, which is\ncentered on the current position of the target vehicle. The\nexpected output includes predictions of lane change intentions\nand trajectory points over a future time horizon of four\nseconds, as well as includes thought reasoning which consists\nof notable features and potential behavior.\nThe user message provides a description of the observations\nspecific to the current frame, thus varying with each driving\nscenario. It includes information about the map, the state of\nthe target vehicle, the spatial relationships between the target\nvehicle and its surrounding vehicles. Map information in the\nprompts primarily denotes the number of lanes in the scenario\nand indicates whether the target vehicle is located in the left-\nmost, middle, or rightmost lane. Prompts related to the target\nvehicle\u2019s state are generated by detailing the target vehicle\u2019s\nhistorical trajectory over the past two seconds, its current\nvelocity, and its vehicle type. Given that most vehicles do not\nhave a large lateral displacement 4 seconds before changing\nlanes, the model mainly relies on the interaction information\nbetween the target vehicle and surrounding vehicles to predict\nlane change intentions in advance. Therefore, comprehending\nthe information pertaining to surrounding vehicles is crucial\nfor accurately predicting the lane change intentions of the\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n5\nRole: You are an expert driving prediction model of an autonomous driving system, that can predict the future driving \nintention and future 4-second driving trajectory for a given target vehicle, avoiding collision with other vehicles and \nobstacles on the road.\nContext: \n- Coordinates: Y-axis is perpendicular, and X-axis is parallel to the direction target vehicle is facing. target vehicle's \ncurrent position is (0,0). Positive values on the y-axis represent the left side of the target vehicle, and negative values on \nthe y-axis represent the right side of the vehicle.\nOutput: \n- Thought:\n  - Notable features\n  - Potential behaviors\n- Final Answer:\n  - Intention:\n  - 0: Keep lane; 1: Left lane change; 2: Right lane change. The final answer should be one of the three modes.\n  - Trajectory (MOST IMPORTANT): 4 points, one every 1 second\n  - [(x1,y1), (x2,y2), (x3,y3), (x4,y4)]\nThe target vehicle is driving on a three-lane highway, located at the rightmost lane.\nThe information of target vehicle is as follow:\n  - Velocity(km/h): vx=90.04, vy=-0.47\n  - Accelaration(m/s^2): ax=0.36, ay=0.07\n  - Type: Car, with width of 1.92 m and length of 4.24 m\n  - Historical position of the last 2 seconds (One point every 0.4s): [(-48.46,0.02), (-39.62,0.03), (-29.80,0.06), (-\n19.93,0.07), (-9.98,0.05), (0.0,0.0)]\nThe information of its surrounding vehicles (with a range of 200m) are listed as follow:\n  - Ahead: a Truck traveling at 86.90 km/h of X-axis, with a distance of 21 m.\n  - Behind: a Truck traveling at 91.55 km/h of X-axis, with a distance of 68 m.\n  - Left front: a Car traveling at 125.03 km/h of X-axis, with a distance of 26 m.\n  - Left rear: a Car traveling at 121.07 km/h of X-axis, with a distance of 13 m.\n  - Right front: a Truck traveling at 83.02 km/h of X-axis, with a distance of 41 m.\n  - Right rear: a Truck traveling at 79.56 km/h of X-axis, with a distance of 32 m.\nBase\nMap info\nSurrounding vehicle info\nFig. 3: An example of our input prompts in the fine-tuning stage. The input prompts comprise a system message displayed in\nthe upper text block and a user message presented in the lower text block. The user message mainly contains map information,\nthe target vehicle\u2019s state, the spatial relationships between the target vehicle and its surrounding vehicles.\ntarget vehicle. We denote the information of the nearest\nvehicles in eight directions surrounding the target vehicle\nas surrounding vehicle prompts. These directions include the\nahead, left front, right front, left side, right side, rear, left rear,\nand right rear. The surrounding vehicle information in each\ndirection encompasses details such as vehicle type, current\nspeed, and relative distance from the target vehicle. This\ncomprehensive surrounding vehicle information serves as a\ncritical prompt for the predictive model, enabling a thorough\nanalysis of the contextual interactions influencing the target\nvehicle\u2019s lane change intentions.\nC. Reasoning\nRecently, CoT reasoning has demonstrated remarkable ca-\npabilities in carrying out more intricate reasoning tasks [30],\n[35], [36], [37]. Drawing inspiration from GPT-Driver [30],\nwhich utilizes Chain-of-Thought reasoning strategy in motion\nplanning tasks to enhance transparency throughout the plan-\nning procedure, we employ CoT reasoning in lane change\nprediction task to improve reliability of predictions and ex-\nplanations. Unlike GPT-Driver, which only labels thoughts\nbased on whether the ego vehicle and surrounding vehicles\nwill collide in the future, we also consider domain knowledge\nin driving, traffic rules, and traditional lane change model rules\n[38], [39] for labeling CoT reasoning. In this way, our LC-\nLLM model can learn this knowledge and rules to provide\nmore reliable and accurate predictions and explanations. The\ndetails of CoT reasoning are shown in Fig. 4.\nOur CoT reasoning consists of notable features and potential\nbehaviors. The annotation of CoT reasonings are as follows:\n1) Labeling of Notable feature: Firstly, we labeled notable\nfeatures include significant lateral movement when lateral\nvelocity exceeds 1.5km/h, and high longitudinal acceleration\nwhen it surpasses 0.4m/s2. Secondly, the relative speeds of\nsurrounding vehicles compared to the target vehicle in the\ndirections ahead, left front, and right front are considered\nnotable features. Vehicles in these positions with a speed\nhigher than the target vehicle are labeled as free. Conversely,\nvehicles with a speed lower than the target vehicle are labeled\nas blocked, indicating potential congestion or the need for the\ntarget vehicle to overtake. At last, the presence of a truck ahead\nwithin 100 meters is also notable due to its impact on traffic\nflow and overtaking maneuvers [38]. Additionally, in right lane\nchange scenarios, the characteristic of the target vehicle being\na truck is also labeled as notable due to the traffic rule that\ntrucks typically travel in the right lane.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n6\nThought:\n  - Notable feature: The speed of the ahead vehicle is 86.90 km/h, slower than that of the target's, Ahead is block.\n  - Notable feature: The type of ahead vehicle is Truck.\n  - Notable feature: Left front is free.\n  - Notable feature: The speed of the right front vehicle is 83.02 km/h, slower than that of the target's.\n  - Potential behavior: Change to the left lane for overtaking.\nFinal Answer:\n  - Intention: \"1: Left lane change\"\n  - Trajectory: \"[(25.21,-0.06), (50.75,0.24), (76.69,1.49), (103.26,2.69)]\"\nFinal Answer\nThought\nFig. 4: An example of the output of our LC-LLM. The content in the yellow text block is the Chain-of-Thought reasoning.\nFig. 5: A data sample of Llama format. Special tokens, such\nas [INST], serve as delimiters distinguishing system messages,\nuser messages, and answer messages. The details of these\nmessages are elucidated in the Prompting section.\n2) Labeling of Potential behavior: Potential behaviors are\nclassified into eight categories: \u201cChange to the left lane for\novertaking\u201d occurs when encountering a slower vehicle ahead\nand occupying the rightmost or middle lane; \u201cChange left to\nthe fast lane\u201d corresponds to significant acceleration from the\ntarget vehicle; \u201cIrregular left lane change\u201d applies to scenarios\nin the highD dataset that involve a left lane change but do\nnot fit the first two categories; \u201cChange to the right lane for\novertaking\u201d is when the target vehicle occupies the leftmost or\nmiddle lane and the lane ahead is blocked; \u201cChange right to the\nslow lane\u201d is associated with significant deceleration from the\ntarget vehicle or when the target vehicle is a truck; \u201cIrregular\nright lane change\u201d covers right lane change scenarios in the\nhighD dataset that do not fit the first two rightward categories;\n\u201cFollowing and keep lane\u201d occurs when following a slower\nvehicle ahead; \u201cNormal keep lane\u201d is when the lane ahead is\nunobstructed and it is a lane keeping scenario in the highD\ndataset. These categories assist in predicting driving patterns\nand providing reliable explanations.\nD. Fine-tuning\nIn our work, we utilize an open-source foundational lan-\nguage model, Llama-2-13b-chat [9], as our pre-trained LLM.\nTo achieve parameter-efficient finetuning, we adopt the Low-\nRank Adaptation (LoRA) [40] strategy, which freezes the pre-\ntrained model weights and injects trainable rank decomposi-\ntion matrices into each layer of the Transformer architecture.\nAdditionally, we customize the LLM for our specific predic-\ntion task using supervised fine-tuning techniques. The raw\ndataset utilized in our work originates from the highD dataset\n[41], which captures human naturalistic vehicle trajectories\non German highways. Following the fine-tuning instructions\nprovided for Llama-2, each data sample is formatted to include\nan input prompt and a corresponding answer, separated by\na special token. This formatting is illustrated in the Fig. 5.\nThe answer for each sample is derived from the ground truth\nobtained from the highD dataset, encompassing programmat-\nically automatically labeled future trajectories, driving inten-\ntions and CoT reasoning. In summary, we process highD raw\ndataset with natural language, format each sample into the\nLlama format, and then feed it into LLM. Finally, we fine-\ntune the LLM by aligning the LLM\u2019s output {C, I, T } with\nthe corresponding ground truth labels {C\u2217, I\u2217, T \u2217}. This fine-\ntuning process is executed through the language modeling loss\nL as defined in Equation 4. During fine-tuning, we mask the\nloss for tokens within the input prompts, focusing backpropa-\ngation only on the tokens comprising the answers, in alignment\nwith the fine-tuning approach of Llama-2. By following this\napproach, our fine-tuned model, LC-LLM, can predict human\ndriving behaviors and corresponding CoT reasoning. This\ncapability is crucial for ensuring the safety of ego vehicles\nwithin an autonomous driving system.\nE. Interpretability of Prediction\nA prevalent limitation in contemporary autonomous driving\nprediction models lies in their constrained interpretability. This\nis attributed to the fact that these models generate predictions\nabout the future behaviors or trajectories of the target vehicle\nthrough black-box neural networks, offering little explanation\nbehind their prediction results. In our research, we address\nthis limitation in two distinct ways. First, we fine-tune the\nCoT reasoning capabilities of our LC-LLM. By doing so,\nour model learns the domain knowledge and rules in driving,\nwhich enhances transparency and reliability of lane change\npredictions, leading to more accurate and explainable predic-\ntions. Second, we incorporate explanatory requirements into\nthe input prompts in the inference stage, where the weights\nof our fine-tuned model are fixed. Benefiting from the self-\nexplanation [10], [11] capabilities of LLMs, our fine-tuned\nmodel, LC-LLM, not only predicts lane change intentions\nand future trajectories but also provides corresponding CoT\nreasoning and explanations for its prediction results, thereby\nenhancing the interpretability of the prediction results. This\napproach facilitates a more transparent and understandable\nprediction process, which is crucial for the practical appli-\ncation of autonomous driving systems. Some visual examples\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n7\nTABLE I: Hyperparameters for training our LC-LLM.\nHyper-parameter\nValue\nLearning rate\n5e-4\nBatch size\n8\nTrain epoch\n2\nLora r\n64\nLora alpha\n16\nGradient accumulation steps\n8\nWarmup steps\n600\nLoad in bits\n8\nof the interpretability of our LC-LLM model are shown in\nFig. 8.\nIV. EXPERIMENTAL RESULTS AND ANALYSIS\nThis section outlines the evaluations of our proposed LC-\nLLM model. First, we detail the dataset processing and the\nexperimental setup. Subsequently, we elucidate the evaluation\nmetrics. Next, we compare the proposed model with the base-\nline models and analyze the results quantitatively. Moreover,\nwe perform an ablation study on the key components of\nour proposed method. Finally, we conduct an interpretability\nstudy and a zero-shot study to evaluate the interpretability and\neffectiveness of our approach.\nA. Dataset Processing\nTo evaluate the performance of the model proposed in this\npaper, the highD dataset was employed for both training and\ntesting. The highD dataset is a large-scale natural vehicle\ntrajectory dataset collected on German highways. It encom-\npasses 16.5 hours of data from six distinct locations, involving\n110,000 vehicles, covering a total distance of 45,000 km,\nand capturing 5,600 documented complete lane changes. The\nhighD dataset comprises data extracted from 60 recordings.\nFor our study, we select data from the first 50 recordings\nfor training the model, while the remaining 10 recordings are\nreserved for testing. Furthermore, we adopt a target-centric\nstrategy that normalizes all position inputs to the coordinate\nsystem centered on the current frame position of the target\nvehicle.\nIn our work, the lane change frame Tlc is defined as the\nframe in which the lane id changes. The advanced prediction\ntime T denotes the temporal interval between the lane change\nframe and the current frame, and it can be expressed as\nT = Tlc\u2212Tcurrent, as illustrated in the Fig. 2. For the purpose\nof training and evaluating our proposed LC-LLM model, we\nextract both lane change (LC) and lane keeping (LK) scenarios\nfrom the highD dataset. The LK scenarios comprise data\nsamples wherein the lane id remains constant throughout,\nwhile the LC scenarios consist of data samples where the\nadvanced prediction time T falls within the range of [0, 4]\nseconds. Furthermore, in order to evaluate the performance\nof our LC-LLM in different T, we divide LC scenarios into\nfour parts according to the four intervals of T(T \u2208[0, 1], T \u2208\n(1, 2], T \u2208(2, 3], T \u2208(3, 4]). For the training dataset, we\nrandomly select 48000 lane keeping (LK) samples, 12000\nleft lane change (LLC) samples from each T interval, and\n12000 right lane change (RLC) samples from each T interval,\nresulting in a total of 144000 samples. Similarly, for the testing\ndataset, we randomly select 8000 LK samples, 2000 LLC\nsamples from each T interval, and 2000 RLC samples from\neach T interval, accumulating a total of 24000 samples.\nB. Experimental Setup\nWe utilize the training dataset extracted from the highD\ndataset to train our proposed LC-LLM model, as well as\nthe reimplemented baseline models: an LSTM-based model[3]\nand a Transformer-based model[4]. All the models are imple-\nmented using the PyTorch framework[42]. Additionally, we\nemploy the DeepSpeed [43] library for distributed training of\nour LC-LLM model. The training process is conducted on\neight A800 GPUs and takes 11 hours for the 13B model.\nThe hyperparameters used for model training are detailed in\nTable I. To ensure the stability of the experimental results,\neach experiment is repeated five times, and the average value\nis reported as the final result.\nC. Evaluation Metrics\nSimilar to previous works, we utilize precision, recall, F1\nscore, and macro average metrics to evaluate the performance\nof the intention prediction task. Additionally, we use the root\nmean square error (RMSE) metric to evaluate the performance\nof the trajectory prediction task.\n1) Evaluation Metrics of Intention Prediction:\n\u2022 Precision: The ratio of correctly predicted positive in-\nstances to the total predicted positives. It is calculated\nas\nPrecision =\nTrue Positives\nTrue Positives + False Positives.\n\u2022 Recall: The ratio of correctly predicted positive instances\nto the total actual positives. It is calculated as\nRecall =\nTrue Positives\nTrue Positives + False Negatives.\n\u2022 F1 Score: The harmonic mean of precision and recall. It\nis calculated as\nF1 Score = 2 \u00d7 Precision \u00d7 Recall\nPrecision + Recall.\n\u2022 Macro Avg: The average precision, recall, and F1 score\ncalculated across all classes. It is calculated as the arith-\nmetic mean of precision, recall, and F1 score.\n2) Evaluation Metrics of Trajectory Prediction:\n\u2022 RMSE (Root Mean Squared Error): A measure of the\naverage deviation between predicted and actual values. It\nis calculated as\nRMSE =\nrPn\ni=1(yi \u2212\u02c6yi)2\nn\n,\nwhere yi is the actual value, \u02c6yi is the predicted value,\nand n is the number of instances.\nIn trajectory prediction evaluation, we use RMSE (lat)\nto assess lateral prediction error and RMSE (lon) to assess\nlongitudinal prediction error.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n8\nTABLE II: Comparison of the intention prediction performance of the proposed LC-LLM model and baseline models on the\nhighD dataset.\nModel\nIntention\nT \u2208[0, 1]\nT \u2208(1, 2]\nT \u2208(2, 3]\nT \u2208(3, 4]\nAvg. (T \u2208[0, 4])\nP(%)\nR(%) F1(%)\nP(%)\nR(%) F1(%)\nP(%)\nR(%) F1(%)\nP(%)\nR(%) F1(%)\nP(%)\nR(%) F1(%)\nLSTM[3]\nLK\n84.6\n97.0\n90.4\n80.5\n97.7\n88.3\n69.6\n97.5\n81.2\n56.9\n97.4\n71.8\n71.2\n97.4\n82.3\nLLC\n97.8\n83.0\n89.8\n98.6\n79.3\n87.9\n97.1\n67.3\n79.5\n95.3\n49.9\n65.5\n97.4\n69.9\n81.4\nRLC\n98.9\n99.4\n99.1\n98.7\n97.0\n97.8\n98.8\n89.6\n94.0\n97.6\n74.7\n84.6\n98.5\n90.2\n94.2\nMacro avg.\n93.8\n93.1\n93.1\n92.6\n91.3\n91.3\n88.5\n84.8\n84.9\n83.3\n74.0\n74.0\n89.0\n85.8\n85.9\nTransformer[4]\nLK\n89.9\n91.7\n90.8\n85.5\n91.2\n88.4\n74.3\n91.1\n81.8\n63.4\n91.6\n75.0\n76.9\n91.4\n83.5\nLLC\n95.6\n91.1\n93.3\n94.6\n85.7\n89.9\n93.5\n73.8\n82.4\n92.7\n62.4\n74.6\n94.2\n78.2\n85.5\nRLC\n96.0\n98.5\n97.2\n95.4\n98.3\n96.8\n95.2\n93.7\n94.4\n93.2\n82.2\n87.3\n95.0\n93.1\n94.1\nMacro avg.\n93.8\n93.8\n93.8\n91.9\n91.7\n91.7\n87.6\n86.2\n86.2\n83.1\n78.7\n79.0\n88.7\n87.6\n87.7\nLC-LLM\nLK\n99.6\n96.4\n97.9\n99.9\n97.1\n98.5\n98.7\n96.7\n97.7\n85.9\n96.6\n90.9\n95.6\n96.7\n96.2\nLLC\n97.9\n99.4\n98.6\n98.6\n99.6\n99.1\n97.7\n98.7\n98.2\n97.1\n89.3\n93.0\n97.8\n96.7\n97.3\nRLC\n98.1\n99.8\n99.0\n98.2\n100.0\n99.1\n98.0\n99.0\n98.5\n97.2\n92.9\n95.0\n97.9\n97.9\n97.9\nMacro avg.\n98.5\n98.5\n98.5\n98.9\n98.9\n98.9\n98.1\n98.1\n98.1\n93.4\n92.9\n93.0\n97.1\n97.1\n97.1\n(a) Lateral RMSE \u2193\n(b) Longitudinal RMSE \u2193\nFig. 6: Comparison of the trajectory prediction performance of the proposed LC-LLM model and baseline models on the highD\ndataset.\nD. Overall Performance\nWe evaluate our proposed LC-LLM model through two\ntasks, one is intention prediction and the other is trajectory\nprediction. The results are as follows.\n1) Intention Prediction Results and Analysis:\nTable II shows the overall performance of the proposed LC-\nLLM model as well as baseline models in intention prediction\nwith four different advanced prediction time T. We can note\nthat our proposed LC-LLM significantly outperforms baseline\nmodels for all T, especially for the longer T. For example,\nwhen T \u2208(2, 3], the average F1 score of LC-LLM is 13.8%\nhigher than that of the Transformer model and 15.5% higher\nthan that of the LSTM. Additionally, when T \u2208(3, 4], the\naverage F1 score of LC-LLM surpasses that of the Transformer\nmodel by 17.7% and outperforms the LSTM model by 25.7%.\nThese results demonstrate that our LC-LLM performs well\neven at the early stage of a lane change maneuver when lateral\ndisplacement is minimal.\n2) Trajectory Prediction Results and Analysis:\nWe assessed the performance of our LC-LLM models in the\ntrajectory prediction task and compared them with baseline\nmodels. The results are depicted in Fig. 6. The left panel\nillustrates the trends of lateral RMSE for trajectory points\nacross different models and prediction horizons, while the\nright panel shows the trends of longitudinal RMSE. The\nresults indicate a notable improvement in both lateral and\nlongitudinal RMSE for our LC-LLM models in comparison\nto the baselines. Notably, with larger prediction horizons, our\nmodel demonstrates increasing advantages over the baselines\nin lateral RMSE and longitudinal RMSE, indicating better\nrobustness concerning long-term prediction horizons.\nThe results of lane change intention prediction task and\ntrajectory prediction task demonstrate that our LC-LLM out-\nperforms all baseline models in long-term (4 s) prediction,\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n9\nachieving a 17.7% improvement in lane change intention pre-\ndiction, a 64.4% improvement in lateral trajectory prediction,\nand a 66.1% improvement in longitudinal trajectory prediction,\nrespectively.\nFig. 7: Components ablation studies. The \u2018Base\u2019 refers to\nthe model results obtained using only the target vehicle\ninformation for prompting. The \u2018+Map info\u2019, \u2018+SV info\u2019, and\n\u2018+Thought\u2019 indicate the model results when map information,\nsurrounding vehicle information, and CoT reasoning are se-\nquentially incorporated, respectively.\nE. Ablation Study\n1) Components Ablation Studies:\nWe explored the impact of incorporating different com-\nponents on the trajectory prediction task, and the results\nare illustrated in Fig. 7. A series of ablation experiments\nwere conducted, progressively introducing different compo-\nnents\u2014Target Vehicle Information, Map Information, Sur-\nrounding Vehicle Information, and Chain-of-Thought reason-\ning. The comprehensive model, integrating all contextual\nprompts and CoT reasoning component, exhibited superior\nperformance compared to the base model with only Target\nVehicle Information, achieving a substantial 10.64% reduc-\ntion in lateral RMSE, and a remarkable 29.57% decrease in\nlongitudinal RMSE. Specifically, the addition of Surrounding\nVehicle Information proved crucial for the future trajectory\nforecasting. Furthermore, the incorporation of CoT reasoning\nyielded notable enhancements in trajectory prediction.\nTABLE III: Multi-task Ablation Study.\nTask\nIntention result\nTrajectory result\nF1 (avg)\nRMSE(lat)\nRMSE(lon)\nOnly Intention\n0.948\n\u2013\n\u2013\nOnly Trajectory\n\u2013\n0.257\n0.798\nIntention+Trajectory\n0.971\n0.210\n0.655\n2) Multi-task Ablation Study:\nWe further examined the efficacy of multi-task learning\nin concurrently predicting lane change intentions and future\ntrajectories, as depicted in Table III. The findings demonstrate\nsuperior performance in the multi-task setting compared to the\nisolated trajectory prediction task, emphasizing the benefits of\nintegrating high-level intention prediction for achieving more\naccurate trajectory prediction. Furthermore, in comparison\nto the standalone intention prediction task, adding trajectory\nprediction not only elevates the overall task complexity but\nalso enables the model to capture more nuanced details,\nconsequently, enhancing the accuracy of intention prediction\ntasks.\nF. Interpretability Study\n1) Visualization of Results: In order to demonstrate the\ninterpretability of our LC-LLM, we visualize several sample\nscenarios alongside the corresponding outputs from our model,\nas depicted in Fig. 8. The visualization illustrates that our\nLC-LLM is capable of forecasting lane change intentions and\nfuture trajectories of the target vehicle while also providing\ncorresponding CoT reasoning and explanations for the predic-\ntion results. For example, in the first case in the figure, with a\nslow-moving truck positioned ahead, the target vehicle need to\novertaking to gain some speed advantage. And given the high\nvelocity of the left front vehicle while the low velocity of right\nfront vehicle, the target vehicle has an opportunity to execute\na left lane change. The explanation given by our LC-LLM\nis consistent with this driving scenario. These explanations\nprovided by our model for these scenarios serve as evidence\nthat our LC-LLM possesses a deep understanding of the\ndriving scenario and makes reasonable and reliable predictions\nfor the target vehicle. In contrast to previous methodologies\nthat solely generate prediction results, our LC-LLM model\nexhibit better interpretability.\n2) Quantitative Evaluation: To quantitatively evaluate the\ninterpretability of our LC-LLM, we evaluate the accuracy of\nCoT reasoning. We randomly selected 100 samples from the\ntest dataset and manually compared the accuracy of the CoT\nreasoning output by our LC-LLM model with the ground truth\nin the dataset, scoring them accordingly. The scoring rule is\nthat 10 points will be deducted for each error, omission, or\naddition of a notable feature, and 50 points will be deducted\nfor a potential behavior error. Our quantitative evaluation result\nis that our model scores 97.2. This result demonstrates that our\nLC-LLM model excels in generating accurate and interpretable\npredictions, confirming the effectiveness of our methods in\nenhancing the transparency and reliability of autonomous\ndriving systems.\nG. Zero-shot Study\nTable IV presents a comparative analysis between zero-\nshot and fine-tuned experiments across two tasks. The fine-\ntuned model demonstrated substantial improvements, yielding\nmarkedly higher F1 scores and significantly lower RMSE com-\npared to the zero-shot experiment. These results underscore\nthe efficacy of model fine-tuning in injecting domain-specific\nknowledge into Large Language Models. This affirms the\nsuitability of fine-tuning as the preferred approach for enabling\nLarge Language Models (LLMs) to excel in specific domains\nin future applications.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n10\nThought:\n  - Notable feature: The speed of the ahead vehicle is 86.90 km/h, slower than that of the target's, Ahead is block.\n  - Notable feature: The type of ahead vehicle is Truck.\n  - Notable feature: Left front is free.\n  - Notable feature: The speed of the right front vehicle is 83.02 km/h, slower than that of the target's.\n  - Potential behavior: Change to the left lane for overtaking.\nFinal Answer:\n  - Intention: \"1: Left lane change\"\n  - Trajectory: \"[(25.21,-0.06), (50.75,0.24), (76.69,1.49), (103.26,2.69)]\u201c\nExplanation: Due to the slower speed of the truck ahead (86.90 km/h), the target vehicle needs to overtake. The \nleft front vehicle is moving faster, and the speed of the right front vehicle (83.02 km/h) is also slower. Therefore, \nthe target vehicle plans to change to the left lane.\nThought:\n  - Notable features: ax = 0.97\n  - Notable feature: Ahead is free.\n  - Notable feature: Left front is free.\n  - Potential behavior: Change left to the fast lane.\nFinal Answer:\n  - Intention: \"1: Left lane change\"\n  - Trajectory: \"[(23.93,0.39), (48.36,0.86), (73.66,1.36), (99.67,1.80)]\u201c\nExplanation: Given the notable feature that the acceleration is 0.97 and the road ahead is clear, along with a \nfaster-moving vehicle in the left front lane, the target vehicle plans to change to the left lane. \nThought:\n  - Notable feature: Ahead is free.\n  - Notable feature: Left front is free.\n  - Notable feature: Right front is free.\n  - Potential behavior: Irregular left lane changes.\nFinal Answer:\n  - Intention: \"1: Left lane change\"\n  - Trajectory: \"[(31.33,0.12), (62.96,0.63), (94.82,1.67), (126.92,2.72)]\u201c\nExplanation: With clear lanes ahead, to the left, and to the right, the target vehicle intends to make an irregular \nleft lane change. \nThought:\n  - Notable feature: The speed of the ahead vehicle is 100.87 km/h, slower than that of the target's, Ahead is block.\n  - Notable feature: Right front is free.\n  - Notable feature: leftmost lane\n  - Potential behavior: Change to the right lane for overtaking.\nFinal Answer:\n  - Intention: \"2: Right lane change\"\n  - Trajectory: \"[(29.17,-0.12), (58.55,-0.50), (87.96,-1.13), (117.76,-2.14)]\u201c\nExplanation: Since the speed of the vehicle ahead is 100.87 km/h, which is slower than the target's speed, and the \npath ahead is blocked, while the right front lane is clear, the target vehicle plans to change to the right lane for \novertaking. \nThought:\n  - Notable feature: Ahead is free.\n  - Notable feature: Left front is free.\n  - Notable feature: Right front is free.\n  - Notable feature: The type of target vehicle is Truck.\n  - Potential behavior: Change right to the slow lane.\nFinal Answer:\n  - Intention: \"2: Right lane change\"\n  - Trajectory: \"[(25.57,-0.13), (51.26,-0.47), (76.90,-1.20), (102.34,-2.20)]\u201c\nExplanation: The target vehicle, a truck, has clear lanes ahead, to the left, and to the right. Given these conditions, \nthe potential behavior is to change to the right, into the slow lane. \nThought:\n  - Notable feature: Ahead is free.\n  - Notable feature: Right front is free.\n  - Notable feature: leftmost lane\n  - Potential behavior: Irregular right lane changes.\nFinal Answer:\n  - Intention: \"2: Right lane change\"\n  - Trajectory: \"[(28.81,-0.36), (57.89,-0.91), (87.06,-1.48), (116.51,-2.13)]\u201c\nExplanation: With clear lanes ahead, to the left, and to the right, the target vehicle intends to make an irregular \nright lane change.\nThought:\n  - Notable feature: The speed of the ahead vehicle is 125.78 km/h, slower than that of the target's, Ahead is \nblock.\n  - Notable feature: The speed of the right front vehicle is 103.57 km/h, slower than that of the target's.\n  - Potential behavior: Following and Keep lane.\nFinal Answer:\n  - Intention: \"0: Keep lane\"\n  - Trajectory: \"[(35.19,-0.01), (70.66,-0.04), (106.12,-0.18), (141.35,-0.35)]\u201c\nExplanation: The speed of the vehicle ahead is 125.78 km/h, which is slower than the target vehicle's speed, \nindicating a block ahead. The speed of the right front vehicle is 103.57 km/h, also slower than the target \nvehicle's speed. The potential behavior is to follow and keep the lane.\nThought:\n  - Notable feature: Ahead is free.\n  - Notable feature: Left front is free.\n  - Notable feature: The speed of the right front vehicle is 88.52 km/h, slower than that of the target's.\n  - Potential behavior: Normal Keep lane.\nFinal Answer:\n  - Intention: \"0: Keep lane\"\n  - Trajectory: \"[(25.44,-0.06), (50.72,-0.06), (75.61,0.03), (100.63,0.19)]\u201c\nExplanation: The target vehicle will maintain its current lane because the vehicles ahead and to the left front are \nmoving faster than the target vehicle, while the vehicle in the right front is traveling slower at 88.52 km/h. \nFig. 8: Some visual examples demonstrating the interpretability of our LC-LLM model. In each example, the image on the\nleft depicts the driving scenario, and the text on the right presents the corresponding output of our model, which includes the\nprediction results and their reasoning and explanations.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n11\nTABLE IV: Zero-shot Study.\nMethod\nIntention Prediction\nTrajectory Prediction\nF1 score(avg)\nFailed cases\nLateral RMSE(4s)\nLongitudinal RMSE(4s)\nFailed cases\nZero-shot(Llama-13B-chat)\n0.210\n4\n1.405\n63.366\n4\nFine-tune(LC-LLM)\n0.971\n0\n0.210\n0.655\n0\na) Right front vehicle stops as the target vehicle changes lanes to the right.\nb) Right rear vehicle accelerates as the target vehicle changes lanes to the right.\nc) Left front vehicle decelerates as the target vehicle changes lanes to the left.\nd) Left rear vehicle accelerates as the target vehicle changes lanes to the left.\nFig. 9: Some visual examples evaluating the robustness of our\nLC-LLM. These test samples are from outside the distribution\nof the highD dataset.\nH. Robustness Evaluations\nAs mentioned in [44], safety-critical events are important\nto test the model\u2019s robustness. In this experiment, we evalu-\nated the robustness of our LC-LLM by creating four safety-\ncritical scenarios not included in the HighD datasets. The\nfirst scenario involves a left lane change with the left front\nvehicle emergency braking, with speeds of 0 to 50 km/h in\n10 km/h increments and relative distances of 10 to 100 meters\nin 10-meter increments, resulting in 60 samples. The second\nscenario tests a right lane change with the right front vehicle\nemergency braking, using the same speed and distance ranges,\nalso resulting in 60 samples. The third scenario involves a\nleft lane change with the left rear vehicle accelerating, with\nspeeds of 100 to 150 km/h in 10 km/h increments and the same\ndistance range, resulting in 60 samples. The fourth scenario\nexamines a right lane change with the right rear vehicle\naccelerating, using the same speed and distance ranges as the\nthird scenario, producing 60 samples.\nThe results, depicted in Fig. 9, highlight the robustness\nof our LC-LLM across these four challenging scenarios. In\nscenario (a), where the right front vehicle stops as the target\nvehicle changes lanes to the right, the LC-LLM accurately\npredicted the necessary adjustments to avoid a collision.\nScenario (b) shows the right rear vehicle accelerating as the\ntarget vehicle changes lanes to the right; here, the LC-LLM\nsuccessfully anticipated the increased speed of the rear vehicle\nand adjusted the trajectory accordingly. In scenario (c), the\nmodel handled the left front vehicle\u2019s deceleration effectively\nduring the target vehicle\u2019s left lane change, maintaining safe\ndistances and smooth transitions. Finally, in scenario (d), the\nLC-LLM demonstrated its ability to manage the left rear vehi-\ncle\u2019s acceleration during the target vehicle\u2019s left lane change,\nensuring a safe maneuver. These results affirm the model\u2019s\ncapability to handle out-of-distribution cases, enhancing its\napplicability in real-world autonomous driving scenarios.\nI. Limitations\nOur LC-LLM, while demonstrating significant improve-\nments in lane change intention and trajectory prediction accu-\nracy and interpretability, has limitations. Firstly, our model has\nbeen tested exclusively on the HighD dataset, which primarily\ncomprises highway scenarios. To ensure the robustness and\ngeneralizability of our model, it is essential to evaluate its\nperformance on more diverse datasets such as NuScenes\n[45] and Waymo [46], which encompass a wider variety of\ncomplex urban and suburban traffic scenarios. While incor-\nporating detailed map information from these complex urban\nscenarios into input prompts presents a considerable challenge.\nConsequently, the development of an appropriate prompt that\naccurately describe high-definition maps remains an objective\nfor our future work. Secondly, our LC-LLM exhibits slower\ninference speed compared to baseline models. Specifically,\nthe inference time for LC-LLM-13B is 26.72 seconds with\na batch size of 32, significantly longer than the 0.011 seconds\nand 0.014 seconds for the LSTM and Transformer models,\nrespectively. This slower inference time is partly due to the\nlarger model parameters, posing a challenge for real-time\napplications. Future work should focus on optimizing the\nmodel for faster inference and exploring techniques such as\nknowledge distillation to reduce the computational load.\nV. CONCLUSION\nIn this paper, we introduce LC-LLM, an explainable lane\nchange prediction model that not only forecasts lane change\nintentions and trajectories but also provides CoT reasoning and\nexplanations for its predictions. We reconceptualize the task\nof lane change prediction as a language modeling problem\nand employ a supervised fine-tuning technique to fine-tune\nthe LLM specifically for this task. In this way, we successfully\nleverage the strong common sense reasoning capabilities and\nself-explanation abilities of the LLM to address the challenge\nof lane change prediction. Our extensive experiments con-\nducted on highD dataset show that our LC-LLM improves the\naccuracy of predicting lane change intention and trajectory as\nwell as significantly augments the interpretability of the pre-\ndiction results. Future work includes extending our approach\nto urban driving scenarios, reducing inference times through\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n12\nknowledge distillation to improve model speed and efficiency,\nand developing methods to predict lane change intentions and\ntrajectories for multiple vehicles simultaneously.\nACKNOWLEDGMENTS\nThis work is supported by the National Natural Science\nFoundation of China under Grant 52302379, Guangzhou\nBasic and Applied Basic Research Projects under Grants\n2023A03J0106 and 2024A04J4290, Guangdong Province\nGeneral Universities Youth Innovative Talents Project under\nGrant 2023KQNCX100, and Guangzhou Municipal Science\nand Technology Project 2023A03J0011.\nREFERENCES\n[1] G. He, X. Li, Y. Lv, B. Gao, and H. Chen, \u201cProbabilistic intention pre-\ndiction and trajectory generation based on dynamic bayesian networks,\u201d\nin 2019 Chinese Automation Congress (CAC).\nIEEE, 2019, pp. 2646\u2013\n2651.\n[2] S. Mozaffari, E. Arnold, M. Dianati, and S. Fallah, \u201cEarly lane change\nprediction for automated driving systems using multi-task attention-\nbased convolutional neural networks,\u201d IEEE Transactions on Intelligent\nVehicles, vol. 7, no. 3, pp. 758\u2013770, 2022.\n[3] L. Xin, P. Wang, C.-Y. Chan, J. Chen, S. E. Li, and B. Cheng, \u201cIntention-\naware long horizon trajectory prediction of surrounding vehicles using\ndual lstm networks,\u201d in 2018 21st International Conference on Intelligent\nTransportation Systems (ITSC).\nIEEE, 2018, pp. 1441\u20131446.\n[4] K. Gao, X. Li, B. Chen, L. Hu, J. Liu, R. Du, and Y. Li, \u201cDual\ntransformer based prediction for lane change intentions and trajectories\nin mixed traffic environment,\u201d IEEE Transactions on Intelligent Trans-\nportation Systems, 2023.\n[5] S. Shi, L. Jiang, D. Dai, and B. Schiele, \u201cMotion transformer with global\nintention localization and local movement refinement,\u201d Advances in\nNeural Information Processing Systems, vol. 35, pp. 6531\u20136543, 2022.\n[6] A. Seff, B. Cera, D. Chen, M. Ng, A. Zhou, N. Nayakanti, K. S. Refaat,\nR. Al-Rfou, and B. Sapp, \u201cMotionlm: Multi-agent motion forecasting\nas language modeling,\u201d in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2023, pp. 8579\u20138590.\n[7] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman,\nD. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., \u201cGpt-4\ntechnical report,\u201d arXiv preprint arXiv:2303.08774, 2023.\n[8] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Ray et al., \u201cTraining language\nmodels to follow instructions with human feedback,\u201d Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27 730\u201327 744, 2022.\n[9] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosale et al., \u201cLlama\n2: Open foundation and fine-tuned chat models,\u201d arXiv preprint\narXiv:2307.09288, 2023.\n[10] N. F. Rajani, B. McCann, C. Xiong, and R. Socher, \u201cExplain yourself!\nleveraging language models for commonsense reasoning,\u201d arXiv preprint\narXiv:1906.02361, 2019.\n[11] S. Huang, S. Mamidanna, S. Jangam, Y. Zhou, and L. H. Gilpin, \u201cCan\nlarge language models explain themselves? a study of llm-generated\nself-explanations,\u201d arXiv preprint arXiv:2310.11207, 2023.\n[12] Y. Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, \u201cA\nsurvey on trajectory-prediction methods for autonomous driving,\u201d IEEE\nTransactions on Intelligent Vehicles, vol. 7, no. 3, pp. 652\u2013674, 2022.\n[13] S. Mozaffari, O. Y. Al-Jarrah, M. Dianati, P. Jennings, and A. Mouza-\nkitis, \u201cDeep learning-based vehicle behavior prediction for autonomous\ndriving applications: A review,\u201d IEEE Transactions on Intelligent Trans-\nportation Systems, vol. 23, no. 1, pp. 33\u201347, 2020.\n[14] L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li, Z. Hu, X. Na,\nZ. Li et al., \u201cMilestones in autonomous driving and intelligent vehicles:\nSurvey of surveys,\u201d IEEE Transactions on Intelligent Vehicles, vol. 8,\nno. 2, pp. 1046\u20131056, 2022.\n[15] Z. Yang, X. Jia, H. Li, and J. Yan, \u201cA survey of large language models\nfor autonomous driving,\u201d arXiv preprint arXiv:2311.01043, 2023.\n[16] H. M. Mandalia and M. D. D. Salvucci, \u201cUsing support vector machines\nfor lane-change detection,\u201d in Proceedings of the human factors and\nergonomics society annual meeting, vol. 49, no. 22. SAGE Publications\nSage CA: Los Angeles, CA, 2005, pp. 1965\u20131969.\n[17] N. Lyu, J. Wen, Z. Duan, and C. Wu, \u201cVehicle trajectory prediction and\ncut-in collision warning model in a connected vehicle environment,\u201d\nIEEE Transactions on Intelligent Transportation Systems, vol. 23, no. 2,\npp. 966\u2013981, 2020.\n[18] J. Hong, B. Sapp, and J. Philbin, \u201cRules of the road: Predicting\ndriving behavior with a convolutional model of semantic interactions,\u201d\nin Proceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2019, pp. 8454\u20138462.\n[19] A. Zyner, S. Worrall, J. Ward, and E. Nebot, \u201cLong short term memory\nfor driver intent prediction,\u201d in 2017 IEEE Intelligent Vehicles Sympo-\nsium (IV).\nIEEE, 2017, pp. 1484\u20131489.\n[20] A. Zyner, S. Worrall, and E. Nebot, \u201cA recurrent neural network\nsolution for predicting driver intention at unsignalized intersections,\u201d\nIEEE Robotics and Automation Letters, vol. 3, no. 3, pp. 1759\u20131764,\n2018.\n[21] R. Izquierdo, A. Quintanar, I. Parra, D. Fern\u00b4andez-Llorca, and M. Sotelo,\n\u201cExperimental validation of lane-change intention prediction methodolo-\ngies based on cnn and lstm,\u201d in 2019 IEEE Intelligent Transportation\nSystems Conference (ITSC).\nIEEE, 2019, pp. 3657\u20133662.\n[22] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, \u201cMultipath: Multiple\nprobabilistic anchor trajectory hypotheses for behavior prediction,\u201d arXiv\npreprint arXiv:1910.05449, 2019.\n[23] H. Cui, V. Radosavljevic, F.-C. Chou, T.-H. Lin, T. Nguyen, T.-K.\nHuang, J. Schneider, and N. Djuric, \u201cMultimodal trajectory predictions\nfor autonomous driving using deep convolutional networks,\u201d in 2019\nInternational Conference on Robotics and Automation (ICRA).\nIEEE,\n2019, pp. 2090\u20132096.\n[24] N. Deo and M. M. Trivedi, \u201cMulti-modal trajectory prediction of sur-\nrounding vehicles with maneuver based lstms,\u201d in 2018 IEEE intelligent\nvehicles symposium (IV).\nIEEE, 2018, pp. 1179\u20131184.\n[25] F. Altch\u00b4e and A. de La Fortelle, \u201cAn lstm network for highway trajectory\nprediction,\u201d in 2017 IEEE 20th international conference on intelligent\ntransportation systems (ITSC).\nIEEE, 2017, pp. 353\u2013359.\n[26] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun,\n\u201cLearning lane graph representations for motion forecasting,\u201d in Com-\nputer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK,\nAugust 23\u201328, 2020, Proceedings, Part II 16.\nSpringer, 2020, pp. 541\u2013\n556.\n[27] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid,\n\u201cVectornet: Encoding hd maps and agent dynamics from vectorized rep-\nresentation,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2020, pp. 11 525\u201311 533.\n[28] M. Zhu, S. S. Du, X. Wang, Z. Pu, Y. Wang et al., \u201cTransfollower:\nLong-sequence car-following trajectory prediction through transformer,\u201d\narXiv preprint arXiv:2202.03183, 2022.\n[29] D. Wu, W. Han, T. Wang, Y. Liu, X. Zhang, and J. Shen, \u201cLanguage\nprompt for autonomous driving,\u201d arXiv preprint arXiv:2309.04379,\n2023.\n[30] J. Mao, Y. Qian, H. Zhao, and Y. Wang, \u201cGpt-driver: Learning to drive\nwith gpt,\u201d arXiv preprint arXiv:2310.01415, 2023.\n[31] L. Chen, O. Sinavski, J. H\u00a8unermann, A. Karnsund, A. J. Willmott,\nD. Birch, D. Maund, and J. Shotton, \u201cDriving with llms: Fusing\nobject-level vector modality for explainable autonomous driving,\u201d arXiv\npreprint arXiv:2310.01957, 2023.\n[32] Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K. K. Wong, Z. Li, and\nH. Zhao, \u201cDrivegpt4: Interpretable end-to-end autonomous driving via\nlarge language model,\u201d arXiv preprint arXiv:2310.01412, 2023.\n[33] X. Qiu, T. Sun, Y. Xu, Y. Shao, N. Dai, and X. Huang, \u201cPre-trained\nmodels for natural language processing: A survey,\u201d Science China\nTechnological Sciences, vol. 63, no. 10, pp. 1872\u20131897, 2020.\n[34] C. Cui, Y. Ma, X. Cao, W. Ye, and Z. Wang, \u201cReceive, reason, and react:\nDrive as you say with large language models in autonomous vehicles,\u201d\narXiv preprint arXiv:2310.08034, 2023.\n[35] J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le,\nD. Zhou et al., \u201cChain-of-thought prompting elicits reasoning in large\nlanguage models,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 24 824\u201324 837, 2022.\n[36] T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, \u201cLarge lan-\nguage models are zero-shot reasoners,\u201d Advances in neural information\nprocessing systems, vol. 35, pp. 22 199\u201322 213, 2022.\n[37] D. Zhou, N. Sch\u00a8arli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schu-\nurmans, O. Bousquet, Q. Le, and E. Chi, \u201cLeast-to-most prompting\nenables complex reasoning in large language models,\u201d arXiv preprint\narXiv:2205.10625, 2022.\n[38] P. G. Gipps, \u201cA model for the structure of lane-changing decisions,\u201d\nTransportation Research Part B: Methodological, vol. 20, no. 5, pp.\n403\u2013414, 1986.\nIEEE TRANSACTIONS ON INTELLIGENT VEHICLES, VOL. XX, NO. X, XXX 2024\n13\n[39] P. Hidas, \u201cModelling lane changing and merging in microscopic traffic\nsimulation,\u201d Transportation Research Part C: Emerging Technologies,\nvol. 10, no. 5-6, pp. 351\u2013371, 2002.\n[40] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang,\nand W. Chen, \u201cLora: Low-rank adaptation of large language models,\u201d\narXiv preprint arXiv:2106.09685, 2021.\n[41] R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, \u201cThe highd dataset:\nA drone dataset of naturalistic vehicle trajectories on german highways\nfor validation of highly automated driving systems,\u201d in 2018 21st\ninternational conference on intelligent transportation systems (ITSC).\nIEEE, 2018, pp. 2118\u20132125.\n[42] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An\nimperative style, high-performance deep learning library,\u201d Advances in\nneural information processing systems, vol. 32, 2019.\n[43] J. Rasley, S. Rajbhandari, O. Ruwase, and Y. He, \u201cDeepspeed: System\noptimizations enable training deep learning models with over 100 billion\nparameters,\u201d in Proceedings of the 26th ACM SIGKDD International\nConference on Knowledge Discovery & Data Mining, 2020, pp. 3505\u2013\n3506.\n[44] S. Feng, H. Sun, X. Yan, H. Zhu, Z. Zou, S. Shen, and H. X. Liu, \u201cDense\nreinforcement learning for safety validation of autonomous vehicles,\u201d\nNature, vol. 615, no. 7953, pp. 620\u2013627, 2023.\n[45] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Kr-\nishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A multimodal\ndataset for autonomous driving,\u201d arXiv preprint arXiv:1903.11027, 2019.\n[46] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai,\nB. Sapp, C. R. Qi, Y. Zhou et al., \u201cLarge scale interactive motion\nforecasting for autonomous driving: The waymo open motion dataset,\u201d\nin Proceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 9710\u20139719.\n",
    "2404.02903": "LidarDM: Generative LiDAR Simulation in a\nGenerated World\nVlas Zyrianov1, Henry Che1, Zhijian Liu2, and Shenlong Wang1\n1 University of Illinois, Urbana-Champaign, IL, USA\n{vlasz2,hungdc2,shenlong}@illinois.edu\n2 Massachusetts Institute of Technology, MA, USA\nzhijian@mit.edu\nwww.zyrianov.org/lidardm\nAbstract. We present LidarDM, a novel LiDAR generative model ca-\npable of producing realistic, layout-aware, physically plausible, and tem-\nporally coherent LiDAR videos. LidarDM stands out with two unprece-\ndented capabilities in LiDAR generative modeling: (i) LiDAR generation\nguided by driving scenarios, offering significant potential for autonomous\ndriving simulations, and (ii) 4D LiDAR point cloud generation, enabling\nthe creation of realistic and temporally coherent sequences. At the heart\nof our model is a novel integrated 4D world generation framework. Specif-\nically, we employ latent diffusion models to generate the 3D scene, com-\nbine it with dynamic actors to form the underlying 4D world, and sub-\nsequently produce realistic sensory observations within this virtual envi-\nronment. Our experiments indicate that our approach outperforms com-\npeting algorithms in realism, temporal coherency, and layout consistency.\nWe additionally show that LidarDM can be used as a generative world\nmodel simulator for training and testing perception models.\nKeywords: LiDAR Generation \u00b7 Scene Generation \u00b7 Self-driving\n1\nIntroduction\nGenerative models have become notable in understanding data distributions and\ncontent creation, e.g. in image and video generation [10, 33, 52\u201355], 3D object\ngeneration [10,19,38,52], compression [5,29,68], and editing [37,47]. Generative\nmodels also show significant promise for simulation [6, 11, 18, 34, 46, 60, 64, 66,\n76,82], capable of creating realistic scenarios and their associated sensory data\nfor training and evaluating safety-critical embodied agents, such as in robotics\nand autonomous vehicles, without the need of expensive manual modeling of the\nreal world. These capabilities are crucial for applications that rely on extensive\nclosed-loop training or scenario testing.\nWhile advancements in conditional image and video generation [15,27,35,44]\nhave been remarkable, the specific task of generatively creating scenario-specific,\narXiv:2404.02903v1  [cs.CV]  3 Apr 2024\n2\nV. Zyrianov et al.\nRealistic\nTemporally Consistent\nDiverse\nLayout-aware\nTraffic Layout \nCondition\nFig. 1: We present LidarDM, a novel 4D LiDAR generative model. Our generated\nLiDAR videos simultaneously enjoy the benefits of being realistic, layout-conditioning,\nphysically plausible, diverse, and temporally coherent.\nrealistic LiDAR point cloud sequences for autonomous driving application re-\nmains under-explored. Current LiDAR generation methods fall into two broad\ncategories, each of which suffers from specific challenges:\n1. LiDAR generative modeling methods [8, 72, 79, 83] are currently limited to\nsingle-frame generation and do not provide the means for semantic control-\nlability and temporal consistency.\n2. LiDAR resimulation [14, 17, 46, 65, 67, 74] depends heavily on user-created\nor real-world collected assets. This induces a high operational cost, restricts\ndiversity, and limits broader applicability.\nTo address these challenges, we propose LidarDM (Lidar Diffusion Model),\nwhich creates realistic, layout-aware, physically plausible, and temporally coherent\nLiDAR videos. We explore two novel capabilities that have not been previously\naddressed: (i) LiDAR synthesis guided by driving scenarios, which holds im-\nmense potential for simulation in autonomous driving, and (ii) 4D LiDAR point\ncloud synthesis aimed at producing realistic and temporally coherent sequences\nof labeled LiDAR point clouds. Our key insight to achieving these goals lies in\nfirst generating and composing the underlying 4D world and then creating re-\nalistic sensory observations within this virtual environment. To achieve this, we\nintegrate existing 3D object generation approaches to create dynamic actors and\ndevelop a novel approach for large-scale 3D scene generation based on the latent\ndiffusion model. This method produces realistic and diverse 3D driving scenes\nfrom a coarse semantic layout, which to our knowledge, is one of the first of its\nkind. We apply trajectory generation to create dynamic effects while ensuring\nauthentic interactions among actors and between actors and the scene. Finally,\nwe compose the 3D world at each time step and perform stochastic raycasting\nsimulation to produce the final 4D LiDAR sequence. As shown in Fig. 1, our gen-\nerated results are diverse, align with the layout conditions, and are both realistic\nand temporally coherent.\nOur experimental results demonstrate that individual frames generated by\nLidarDM exhibit realism and diversity, with performance that is on-par with\nLidarDM\n3\nSwitch Lane\nTurn Left\nU-Turn\nAligned Map-Lidar\nt=40\nt=0\nt=20\nt=40\nt=0\nt=20\nt=40\nt=0\nt=20\nAligned Mesh-Lidar\nPre-train Perception Model\nGenerated Lidar\nMap + Free Labels\n(a) Generates lidar of Champs-\u00c9lys\u00e9es\n(b) Evaluates safety-critical scenarios by extending Waymax\n(c) Improves perception via pre-training\nFig. 2: Applications of LidarDM: (a) generating LiDAR that aligns well with the map\n(color boxes highlight the consistency between the lidar and the map) without 3D cap-\nturing or modeling; (b) providing sensor data for an existing traffic simulator (Way-\nmax [20]), enabling safety-critical scenarios evaluation from pure sensor data; (c) gen-\nerate large volume Lidar data with controllable obstacles locations (treated as ground-\ntruth labels, which are free to obtain) to improve perception models via pre-training\nwithout expensive data capturing and labelling.\nstate-of-the-art techniques in unconditional single-frame LiDAR point cloud gen-\neration. Moreover, we show that LidarDM can produce LiDAR videos that main-\ntain temporal coherency, outperforming a robust stable diffusion sensor genera-\ntion baseline. To our knowledge, this is the first LiDAR generative method\nwith this capability. We further demonstrate LidarDM\u2019s conditional generation\nby showing that the generated LiDAR matches well with ground-truth LiDAR\non matching map conditions. Lastly, we illustrate that the data generated by\nLidarDM exhibit a minimal domain gap when tested with perception modules\ntrained on real data and can also be used to augment training data to sig-\nnificantly boost the performance of 3D detectors. This gives premise for using\ngenerative LiDAR models to create realistic and controllable simulations for\ntraining and testing driving models (Please refer to Sec. 2 and Fig. 2 for detailed\napplications of LidarDM).\n2\nRelated Works\nLiDAR Simulation. Realistic LiDAR sensor simulation is crucial for robotics and\nself-driving vehicle training and testing. Traditional LiDAR generation methods\nuse raycasting-based physical approaches. Simulators like CARLA [14] and Air-\nSim [59] create environments with static (buildings, trees, street lights) and dy-\nnamic objects (cars, bicycles, buses). In these settings, virtual LiDAR sensors are\nplaced, casting rays to calculate depth through ray-triangle intersections. Such\napproaches are simple and easy to integrate, hence are widely used in robot\nsimulation [4,45]. Asset-based physical simulation methods for LiDAR face limi-\ntations in realism and scalability due to three key issues: the need for 3D assets,\n4\nV. Zyrianov et al.\nwhich are costly and limit variations; further, they also face challenges in closing\nthe sim2real gap for both asset design and physics simulation.\nRecent research attempts to address these shortcomings through data-driven\napproaches. LiDARSim [46] uses collected LiDAR sequences to reconstruct maps\nand dynamic assets. Subsequent works have improved asset reconstruction using\nneural fields-based approaches like NeRF [30,67] or neural feature fields [74], and\nwith automatic alignment [58], or with past LiDAR readings [30]. Additionally,\nrealistic physics effects such as ray dropping [49] and snow [21] have also been\nmodeled. However, constructing detailed 3D maps and objects from real-world\nsensor data is often costly and not scalable, typically requiring multiple passes\nover the same locations.\nOur approach, unlike traditional LiDAR simulations, is purely generative,\neliminating the need for man-made or reconstructed assets and allowing for\neasy creation of numerous virtual worlds. In Fig. 2 (a), we present a realistic\ngenerated LiDAR point cloud of Champs-\u00c9lys\u00e9es from only a hand-crafted map\nlayout (without any actual sensor data from France), which no re-simulation\nmethods can achieve.\nLiDAR Generation. Generative models provide a promising alternative for cre-\nating realistic LiDAR point clouds without reconstructing real-world environ-\nments. Early LiDAR generation work exploited the range image representation\nfor LiDAR generation. The pioneering work by Caccia et al. [8] focused on using\nGANs and VAEs for unconditional generation and performing reconstruction\ntasks for noisy LiDAR readings. LiDARGen [83] showed that using a score-\nbased diffusion model provides improved generative capability and can be used\nin downstream classifier-guided sampling tasks such as point cloud upsampling.\nThe range image representation offers the benefit of physically accurate render-\ning at the cost of being ego-centric, rather than scene-centric. Recently, UltraLi-\nDAR [72] proposed using a BEV voxel grid representation for LiDAR generation.\nSampling is performed with a VQVAE [50] in a learned discrete latent space,\nensuring a dense latent space. The BEV-centric representation provides the ben-\nefit of improved layout coherence and metrics, at the cost of not guaranteeing a\nphysically accurate LiDAR sample. However, despite significant advancements,\ncurrent generative models do not support conditioning on semantic layouts. This\nomission makes the generation process less controllable, reducing its practical ap-\nplications. Additionally, a model ensuring temporal consistency in LiDAR video\ngeneration remains to be developed.\nUnlike previous methods, our model addresses this challenge by (1) gener-\nalizing the task of LiDAR generation to geometry generation and data-driven\nray casting in a novel field representation; (2) guiding generation with a BEV\nHD Map layout; and (3) allowing complete control over dynamic scene composi-\ntion by adding generated 3D models and enabling free movement of the virtual\nLiDAR sensor within the frame. Fig. 2 (b) and (c), respectively, show that Li-\ndarDM\u2019s point cloud can provide realistic LiDAR information to an existing traf-\nfic simulator for autonomous driving\u2019s safety-critical case evaluation thanks to\nits temporal consistency, and can improve perception accuracy (Sec. 4.5) thanks\nLidarDM\n5\nScene Generation\nObject Generation\n4D World Composition\nTraffic Layout\n4D LiDAR Videos\nFig. 3: Overview of the LidarDM: Given the input traffic layout at time t = 0, Li-\ndarDM begins by generating actors and the static scene. We then generate the mo-\ntion of the actors and the egocar, and compose the underlying 4D world. Finally, a\ngenerative- and physics-based simulation is used to create realistic 4D sensor data.\nto its full controllability of scenarios, which means ground-truth detection labels\nare free to obtain. These two important benefits cannot be achieved with any\nother LiDAR generative methods.\nDiffusion Models. Our model builds upon the recent advancement in latent dif-\nfusion models. Directly applying diffusion models [24,62] on data can be burden-\nsome on the denoising network due to the issue of data sparsity. Latent diffusion\nalleviates this issue by performing the diffusion process on a dense latent space\nof an autoencoder. Models such as stable diffusion [54], stable diffusion XL [51],\nand MaskGIT [9] have championed latent diffusion as a highly effective technique\nfor generative modeling.\nSampling: A key challenge in diffusion-based modeling lies in selecting the\nsampling procedure. Early methods utilized Langevin dynamics or ancestral sam-\npling. Recent sampling quality and speed improvements have come from deter-\nministic non-Markovian techniques like DDIM [61] and PNDM [40]. Additionally,\nthe differential equation interpretation of diffusion models has led to the devel-\nopment of samplers like Euler [32] and DPM-Solver [43]. We empirically find\nEuler works well in practice for our model.\nConditioning: Various strategies have been devised to integrate conditions\ninto the diffusion model generation process. Classifier-based guidance (or pos-\nterior sampling) [47, 62, 83] utilizes the gradient of a classifier to enhance the\nmodel\u2019s score function. Classifier-free guidance [23] offers a method for training\nand sampling class-conditioned diffusion models. Controlnet [80] introduces a\ntechnique for adding controllability to a pre-trained diffusion model through a\nclass-conditioned hypernetwork. Our approach leverages classifier-free guidance.\nIn diffusion-based video generation [22,25,31,70,71,73], maintaining consis-\ntency is a central challenge. Various approaches have been proposed, leveraging\nmotion modeling, interpolation, or batch sampling to enhance consistency. Our\nmodel is the first to focus on LiDAR video generation. Unlike other methods,\nour technique capitalizes on LiDAR\u2019s unique attribute of underlying 3D world\nalignment, significantly improving temporal consistency.\n3\nLayout-Guided LiDAR Video Generation\nOur goal is to create a realistic, physically plausible, and temporally consis-\ntent LiDAR sequence that enables a free viewpoint based on a given bird\u2019s eye\n6\nV. Zyrianov et al.\nview semantic layout in a purely generative manner without relying on any pre-\ncollected assets like 3D maps. To our knowledge, this is the first solution of its\nkind, addressing layout-conditioned LiDAR generation and LiDAR video gen-\neration. The key to achieving this lies in first generating and composing the\nunderlying 3D world, followed by using generative simulation to create realistic\nsensory observations. We begin by formulating the generation as a joint 4D scene\ngeneration task (Sec. 3.1). Next, we discuss leveraging 3D diffusion models to\ncreate static and dynamic elements, ensuring their faithful interaction (Sec. 3.2).\nFinally, a sensor generation procedure is executed to produce the final LiDAR\nvideo (Sec. 3.3). Fig. 3 depicts the overview of our method.\n3.1\nProblem Formulation\nFormally, given an input layout \\ p rote ct \\mathcal  {I} \\in \\bbR ^{L\\times W \\times M} representing traffic elements from\na bird\u2019s eye view (where L, W, and M are length, width, and map classes,\nrespectively), our goal is to generate a LiDAR point cloud video \\ p rotect \\mathcal  {X} = \\{\\mathbf {x}_t\\}, with\neach \\p r otect \\mathbf  {x}_t \\in \\bbR ^{N\\times 3} being a point cloud at frame t with \\protect \\mathbf  {x}_0 matching the input\nlayout. This conditional generation setting offers full controllability, and hence\nlays the foundation for a practical asset-free simulator. Note that in the absence\nof a map, our approach defaults to unconditional generation.\n4D World Representation. Our key technical innovation to address the challenge\nlies in jointly modeling the generation of underlying 4D world together sensor\ngeneration. We define the world scene representation as \\ p rot ect \\\nmathcal  {W} = \\{ \\mathbf {s}, \\{\\mathbf {o}_i\\}_{i=0}^N\\}, where\n\\protect \\mathbf  {s} represents a static scene geometry and \\pr otec t \\mathbf  {o}_0, ..., \\mathbf {o}_N are dynamic objects. Both\nare represented in the form of an occupancy grid. To model dynamics, we addi-\ntionally consider the actions of these dynamic objects in the form of trajectories\n\\ p ro te ct \\ m a thcal  {P} = \\{ \\boldsymbol {\\tau }_{0},..., \\boldsymbol {\\tau }_{T} \\}, with \\ p r otect \\boldsy\nmbol  {\\tau }_t = \\{ \\boldsymbol {\\xi }_\\mathrm {ego}, \\{\\boldsymbol {\\xi }_{i, t}\\}_{i=0}^N \\} representing the pose of actor i at\ntime t as well as egocar pose \u03beego. The pose for rigid objects and the egocar lies\nin the \\protect \\mathbb  {SE}(3) space, while for articulated objects like pedestrians, it is represented\nas a kinematic chain. A composed scene represent the states of the world at t,\nincorporating the poses of the ego car and dynamic objects at time t, is denoted\nby \\p r otec t  \\mathcal  {W}_t = \\pi (\\mathcal {W}, \\boldsymbol {\\tau }_t), where \\pi is a composition operator applying transformations\nto each actor.\n4D World and LiDAR Generation. To ensure realism and consistency over time\nand between the world and sensory readings, we formulate the generation task\nas a sampling problem from the joint distribution p(\\m at hcal {X}, \\mathcal {P}, \\mathcal {W} | \\mathcal {I}). Directly mod-\neling and sampling the joint distribution, however, is challenging as it involves\nestimating a distribution across multiple data modalities (e.g., car trajectories,\nscene layouts, sensor noise, etc.). To tackle this, we factorize the joint distribution\np(\\m at hcal {X}, \\mathcal {P}, \\mathcal {W} | \\mathcal {I}) as follows:\n  \\lab e\nl\n \n{eq:joi\nn\nt}\n \n\\u nderb rac e {p(\\ mat\nh\nb\nf\n {s } |  \\m at hc\na\nl \n{\nI}) \\cdot \\pr\no\nd\n \n_i p(\\ ma th\nb\nf \n{\no}_i |  \\mathcal \n{I})}_{\\text {3D scene and object gen}} \\cdot \\underbrace {\\prod _t p(\\boldsymbol {\\tau }_t | \\boldsymbol {\\tau }_{<t}, \\mathcal {W}, \\mathcal {I})}_{\\text {trajectory gen}} \\cdot \\underbrace {\\prod _t p(\\mathbf {x}_t | \\boldsymbol {\\tau }_t, \\mathcal {W})}_{\\text {sensor simulation}}. \n(1)\nNext, we will discuss each individual task in detail.\nLidarDM\n7\nUNET\nUNET\n\u2026\n\ud835\udca9(0,1)\n\ud835\udca9(0,1)\nAccumulated Point Cloud \nNKSR Reconstruction\nObservation\nEncoder\nDecoder\nReconstruction\nLatent Diffusion\nTraining Data Preparation\nMap\nMap\nLatent Encoding\nGenerated Sample\nKL-Loss\nRecon-\nLoss\nB\nC\nA\nFig. 4: Our 3D scene generation pipeline. First, accumulated point clouds are used to\nreconstruct each ground truth mesh sample. Next, a variational autoencoder is trained\nto compress meshes into a latent code. Finally, a map-conditioned diffusion model is\ntrained to perform sampling within the latent space of the VAE, yielding novel samples.\n3.2\nScene, Object and Trajectory Generation\nWe decompose the world into a static background scene, constant over time, and\ndynamic foreground objects that move. This decomposition simplifies the chal-\nlenging 4D world generation into more manageable tasks: creating object geome-\ntries and generating dynamic effects. This modeling approach ensures temporal\nconsistency (e.g. keeping cars\u2019 shapes constant and walls and trees remain still\nover time) and physical plausibility (e.g. ensuring correct occlusion reasoning).\nScene Generation. The scene generation addresses the problem of sampling the\ngeometry of a scene from a given input layout \\protect \\mathcal  {I}: \\ p rotect \\mathbf  {s} \\sim p(\\mathbf {s} | \\mathcal {I} ). We parameterize\nthe 3D scene using a signed distance field, \\ p rote ct \\mathbf  {s} \\in \\mathbb {R}^{L \\times W \\times H}, where each entry s_j\nencodes the truncated signed distance to the surface,  - 1  \\ l eq s_j \\leq 1, with negative\nvalues indicating outside the mesh and positive values inside the surface.\nModel: We leverage the latent diffusion model [54,54] to tackle this challenge\nof modeling and sampling from p(s|I). We choose the latent diffusion model for\nits capacity to sample high-quality data while effectively incorporating strong\nconditional guidance. Specifically, our model encodes the high-dimensional SDF\nvolume s into a continuous latent representation z using an encoder-decoder\nstructure [54] with a scene encoder E\u03b8(s) = z and a scene decoder D\u03b8(z) =\n\u02dcs. This encoder-decoder structure efficiently compresses the input data into a\nlower-dimensional latent space, enabling more effective and efficient sampling.\nAdditionally, we encode our high-definition map layout \\protect \\mathcal  {I} into a latent space\n\\ p rotect \\mathbf  {c} = M_\\theta (\\mathcal {I}), allowing for more compact conditioning.\nSampling: We leverage a probabilistic denoising diffusion model [54,62] F\u03b8(z, c)\nto perform classifier-free guidance sampling [23]. Specifically for each diffusion\nstep k, the following Langevin dynamics step is performed to progressively de-\nnoise until a clean sample z0 is acquired:\n  \\m a th b f \n{ z}_ { k-1} = \\ ma t hbf {z}_ {\nk\n} + \\frac {\\lambda _k}{2} \\left [ (1+w)F_\\theta (\\mathbf {z}_k, \\mathbf {c}) - w F_\\theta (\\mathbf {z}_k) \\right ] + \\sqrt {\\lambda _k} \\boldsymbol {\\epsilon }_k \n8\nV. Zyrianov et al.\nRaydrop Prob\nRaydrop Mask\nFinal LiDAR Generation\nRange image\nRaycasting LiDAR\nMasked Range\nMasked LiDAR\nFig. 5: Stochastic raydrop networks for sensory noise simulation, further enhancing\nrealism. We highlighted the raydropped points in red on Masked Range and Masked\nLiDAR Images above.\nF\u03b8(zk, c) is the score function \u2207z log p(z|c) of the conditional distribution at zk\nand F\u03b8(zk, c = 0) is the the score function of the unconditional distribution\np\u03b8(z). w is the CFG guidance scale parameter, \u03bbk is an annealed noise schedule\nparameter, and \u03f5k \u223cN(0, I). Finally, a 3D scene sample s is recovered by\ndecode the reverse-diffused sampling latent code s = E\u03b8(z0). Fig. 4(c) depicts\nthe sampling procedure.\nTraining: We train our diffusion-based scene generation model using a dataset\nthat pairs scene geometry with map conditioning. Direct access to dense scene ge-\nometry is not available in practice. Instead, we use the state-of-the-art dense ge-\nometry reconstruction approach, neural kernel surface reconstruction (NKSR) [28],\nto recover a pseudo-GT from an input LiDAR sequence. Ground truth an-\nnotations are used to remove moving dynamic objects, ensuring our recon-\nstruction contains only the static scene and objects. We then train the auto-\nencoders for both scene geometry and map layout using reconstruction loss and\nKL divergence loss: \\qop name \\ r elax m{min}_\\theta \\mathcal {L}_\\mathrm {recon} + \\mathcal {L}_\\mathrm {KL} over real-world examples (Fig. 4 (a)).\nOur latent diffusion model is trained using the score matching loss function:\n\\pro t ect \\mathc\na\nl  {L}_\\m at hrm \n{\nLDM} = \\mathbb {E}_{(\\bz , \\bc ), \\boldsymbol {\\epsilon }, k} \\left [ \\| \\boldsymbol {\\epsilon } - F_\\theta (\\bz _k, k, \\bc )\\|_2^2\\right ]\n, where \\mathbf {z}_k is the forward diffused noisy\nsample at step k (Fig. 4 (b)).\nObject Generation. Inspired by the rapid advancements in 3D shape generation,\nwe employ two high-fidelity, 3D object generation frameworks, GET3D [19] and\nAvatarClip [26], to create dynamic traffic participants.\nFor each actor \\protect \\mathbf  {o}_i in a given layout \\protect \\mathcal  {I}, we sample a random variable \\ p rotect \\mathbf  {z} \\sim \\mathcal {N}\nand generate the corresponding actor mesh following \\p r otect \\mathbf  {o}_i = G(\\mathbf {z}), where G(\\cdot ) is\nthe Generator/Decoder of the chosen generative method. For cars, trucks, and\nother four-wheeled vehicles, we use GET3D [19], which has demonstrated state-\nof-the-art and diverse generative results for 3D shapes, including cars. The layout\nbox in \\protect \\mathcal  {I} is used to rescale the shape of each actor, ensuring that the sizes of\nthe generated shape and the input layout are consistent. AvatarClip [26] is used\nto generate pedestrians conditioned on a SMPL [41] pose and shape parameter\n\\ p rot ect \\bf  {p}=(\\boldsymbol {\\theta }, \\boldsymbol {\\beta }). Furthermore, each generated rigged model is animated with a walk-\ning animation from Mixamo [3], ensuring a realistic 3D human walking motion\nsequence over time.\nTogether, the generated static world \\protect \\mathbf  {s} and each actor \\protect \\mathbf  {o}_i define our 3D world\nscenario, denoted as \\protect \\mathcal  {W}. Fig. 3 depicts the composed scene.\nLidarDM\n9\nTrajectory Generation. To simulate a dynamic traffic scenario, we propose a\nretrieval-augmented generation coupled with a rejective sampling scheme to gen-\nerate realistic and physically plausible dynamics for each actor and the ego ve-\nhicle, turning our 3D scene representation W to 4D, denoting as Wt for a given\ntime t.\nFor the ego vehicle and each actor, jointly denoted as P = {\u03c4t}, we sampled\ntrajectories from a trajectory bank obtained from Waymo Open dataset [63] and\naugment them to the scene. This guarantees realistic dynamics as the dataset is\nobtained from a diverse set of real-world scenarios. To ensure the physical feasi-\nbility of the sampled trajectories with respect to our generated scene, we reject\nthose that violate rules of physics, such as collision with the static world, collision\nbetween actors, or hovering over the non-mesh area, while making sure to po-\nsition them correctly above the ground. Around 12.3% of sampled trajectories\nare retained. This rate is acceptable because resampling is trivial.\nAdditionally, for more realistic trajectory generation targeting for simula-\ntion use-cases, we extend Waymax [20], a data-driven 2D BEV traffic simulator,\nto control the behaviors of traffic actors in more systematic manners. Given a\nscenario from the WOMD Dataset [16], we use Waymax to replay ego-vehicle\u2019s\nand agent\u2019s real-world trajectories, with an additional reactive intelligent driving\nmodule [69] that updates each agent\u2019s acceleration to avoid collisions. Since the\ntrajectories are specific to the given real-world scenario, they are guaranteed to\nbe physically plausible, but less diverse than our approach above.\nEffectively, this approach renders our world generation to be completely\nasset-free, end-to-end generative, and thereby temporally consistent, allowing\nfor a realistic, generative, and physics-based simulation without the need for\nartist-curated [14] or pre-collected assets [46,74] as in previous lidar simulation\nmethod.\n3.3\nPhysics-Informed LiDAR Generation\nGiven the complete 4D world \\protect \\mathcal  {W} and the poses \\protect \\mathcal  {P}, our next step is to generate a\nrealistic LiDAR point cloud corresponding to these conditions. At a high level,\nwe use the poses to compose the scene and objects at each timestep, then perform\nphysics-informed ray casting to obtain purely physically simulated LiDAR as an\nintermediate result. As a final step, we leverage data-driven conditional sampling\nto generate the final LiDAR point cloud to simulate real-world LiDAR noises\nfrom the clean ray casting LiDAR.\nScene Composition. We use the Dual Marching Cube method [57] to obtain the\n3D mesh of the static world from the generated TSDF volume \\mathbf {s}. Then, with\nthe trajectories of the ego car and all actors at time t, \\tau _t, we transform the 3D\nmesh to the world coordinates and compose it with \\mathbf {s}, producing the full world\ngeometry at each time t: Wt = \u03c0(W, \u03c4 t) For each vehicle actor, \\pi applies a\nrigid body transformation. For pedestrians, besides the rigid body movement, \u03c0\nadditionally articulates the human body shape to simulate the animated nonrigid\nhuman movement with forward kinematics [7].\n10\nV. Zyrianov et al.\nPhysics-based Ray Casting. From the ego vehicle position \u03c4t, we perform ray-\ncasting by utilizing Open3D [81] to compute ray-triangle intersections against\nthe composed scene Wt, obtaining the raycast scan xt. To enhance realism of\nthe raycasting to real-world LiDAR, we closely match the LiDAR sensor config-\nuration (elevation angles, azimuth angles, field of view, etc.) with the real-world\nLidar, depending on the use-case. For single-frame generation (Sec. 4.3), we fol-\nlow the Velodyne HDL-64E manual [1] to match with KITTI-360 dataset [39].\nFor conditional multi-frame generation (Sec. 4.4), we use the calibration infor-\nmation provided with Waymo Open Dataset [63].\nStochastic Raydrop. Raycasted LiDAR from the generated world appears over-\nclean without real-world noises due to environmental and sensor noise factors. To\naddress this, inspired by LiDARSim, we have an additional stage that stochas-\ntically simulates \u201craydrop\u201d, where rays do not return to the sensor. For each\nraycast scan at time t, \\protect \\overline  {\\mathbf {x}}_t, we project it onto a 2D spherical range image using\npolar coordinates. This image uses azimuth and zenith angles to represent coor-\ndinates and encodes depth values for each pixel. We predict raydrop probability\nper pixel on this image using a U-Net architecture [48] supervised by real-world\nLiDAR scan raydrop masks. Our approach, unlike LiDARSim, requires only a\nrange map, eliminating the need for multiple additional metadata input channels\nthat are only available in real-world data. We also apply a Gumbel sigmoid for\nrandom sampling. The application of our stochastic raydrop method produces\nthe final LiDAR scan \\protect \\mathbf  {x}_t for each frame, concluding in our complete end-to-end\nLiDAR video generation process, as shown in Fig. 5.\n4\nExperiment\n4.1\nSetup\nDatasets. We evaluate our proposed LidarDM on KITTI-360 [39] and Waymo\nOpen [63] datasets. KITTI-360 is the de facto dataset for evaluating uncondi-\ntional LiDAR generation methods. The dataset contains nine driving sequences\n(76,715 samples), where the first sequence is used as a val sequence (11,518 sam-\nples) and the last eight are used for training (65,197 samples). The sequences\nwere collected across Karlsruhe, Germany with a 64 Beam LiDAR. However,\nKITTI-360 does not provide detailed BEV HD map information limiting its ap-\nplications in conditional models. Waymo Open [63] is a dataset containing 1048\nsequences with 158,000 training and 29,700 validation frames. A detailed HD\nmap allows us to train conditional diffusion models. The HD map is in a vector\nformat storing edges of map objects. We preprocess the map by centering it\non each LiDAR Frame and rasterizing it into a segmentation map. The dimen-\nsions of the map tensor are L\u00d7W\u00d7M (length\u00d7width\u00d7map classes). The map has\n5 classes (lane markings, road lines, edges, crosswalks, driveways).\nTraining Details. We train our models using four Nvidia A100 40GB GPUs. We\nuse the AdaM optimizer with a learning rate of 1e-4 for the VAE and 1e-5 for\nthe diffusion U-Net, with a cosine decay schedule.\nLidarDM\n11\nReal\nProjectedGAN\nLiDARGen\nUltraLiDAR\nOurs\nFig. 6: Real KITTI-360 samples vs unconditional samples from the competing meth-\nods. UltraLiDAR sample visualizations are directly acquired from their paper. Com-\npared to previous approaches, LidarDM generates samples that feature a greater quan-\ntity of more detailed salient objects (e.g., cars, pedestrians), sharper 3D structures\n(e.g., straight walls), and more realistic road layouts.\nModel Details. The diffusion UNet, which is conditioned on layout and SDF\nlatents, has 5 ResNet blocks with 2\u00d7 downsampling with channels of\n{128,128,256,512,512}. The SDF VAE has 4 ResNet blocks with channels of\n{448,640,896,1280}. The Map VAE has ResNet down/upsample blocks with\nchannels of {64,64,128,256,512}, and it is trained independently with X-Entropy\nand KL regularization.\n4.2\nBaselines\nUnconditional Generation. LiDARVAE, LiDARGAN, ProjectedGAN, and Li-\nDARGen are baselines that perform generation in the range image representa-\ntion. UltraLiDAR performs generation in the BEV voxel space. To provide a fair\ncomparison, we follow UltraLiDAR and evaluate MMD and JSD on a histogram\nof voxel occupancy instead of voxel density [72]. In addition, UltraLiDAR does\nnot provide samples so we use their reported numbers in their original paper.\nTemporal Coherency. We are the first to attempt the task of sequential LiDAR\ngeneration and thus no previous models exist for comparison. Nonetheless, we\nimplement a sequence diffusion baseline inspired by recent work in video gener-\nation. We believe this approach is the most straightforward initial approach.\nConcretely, we train a VAE to encode individual LiDAR frames. This has\nbeen shown previously [72] to be powerful and effective. Next, we train a diffusion\nmodel to directly denoise multiple (i.e., 5) LiDAR frames at once. Visually, this\napproach yields decent temporal consistency.\n12\nV. Zyrianov et al.\nMethod\nMMDBEV (\u2193)\nJSDBEV (\u2193)\nLiDAR VAE [8]\n8.53 \u00d7 10\u22124\n0.267\nLiDAR GAN [8]\n8.95 \u00d7 10\u22124\n0.243\nProjectedGAN [56]\n7.07 \u00d7 10\u22124\n0.201\nLidarGen [83]\n2.95 \u00d7 10\u22124\n0.136\nUltraLidar [72]\n9.67 \u00d7 10\u22125\n0.132\nLidarDM (Ours)\n1.67 \u00d7 10\u22124\n0.119\nTable 1: Qualitative results for unconditional generation on KITTI-360 dataset. (\nbest,\nsecond best,\nthird best)\nMethod\nTotal ICP\nEnergy [m] (\u2193)\nAverage\nICP Energy (\u2193)\nOutlier\nPercentage (\u2193)\nChamfer\nDistance [m] (\u2193)\nSequence Diffusion\n3616.58\n0.078\n20.56%\n0.39\nLidarDM (Ours)\n916.94\n0.014\n7.12%\n0.17\nTable 2: Temporal consistency. Outlier percentage uses distance threshold \u03c4 = 0.5m.\n4.3\nUnconditional Single-Frame Generation\nWe first validate our model architecture design and showcase our model\u2019s genera-\ntive capability by directly comparing against previous LiDAR generation models\nin unconditional generation on KITTI-360 [39].\nBased on the results in Table 1, BEV models (such as ours or UltraLiDAR)\nperform best in top-down layout quality (as reflected by MMD and JSD) com-\npared to range image models. The close performance gap compared to Ultra-\nLiDAR can be explained by the fact that UltraLiDAR was directly trained on\nthe task of modeling single LiDAR scans which the benchmark evaluates. In\naddition, the BEV voxel grid representation offers large flexibility in generating\nphysically implausible LiDAR readings that have the potential accurately match\nground truth data in histogram metrics. We also show qualitative comparisons\nagainst the baselines in Fig. 6.\n4.4\nMap-conditioned Multi-Frame Generation\nOur model is the first fully generative LiDAR model that can generate control-\nlable (through map conditioning), realistic, and temporally coherent synthetic\nLiDAR scans. We will then validate these properties in this section.\nConsistent Video Generation One of our key contributions is the tempo-\nral consistency of the sequential LiDAR generation. To evaluate this, we first\nuse ICP alignment to calculate a relative transformation between consecutive\ngenerated frames. We then exploit LiDAR\u2019s 3D nature and define an aver-\nage point-to-plane energy over a sequence of LiDAR scans as our quantita-\ntive metrics, following this equation: E = 1\nT\nPT\nt=1 point2plane(xt, xt\u22121) where\npoint2plane represents the point-to-plane distance [42], and xt indicates the\nLidarDM\n13\nAccumulated Lidar Point Clouds\nAccumulated Lidar Point Clouds\nt=0\nt=30\nt=60\nt=90\nt=0\nt=30\nt=60\nt=90\nAligned Map-Lidar\nAligned Map-Lidar\nCorresponding Lidar\nAligned Map-Lidar\nFig. 7: Qualitative results of map-conditioned sequence generation on 2 Waymax [20]\nmap sequences. We also showcase the corresponding accumulated point cloud to high-\ntlight the temporal consistency of LidarDM.\nLiDAR scan at time t. Intuitively, E is prone to higher values from dynamic\nobjects, but it is still a valuable metrics to determine if the general geometry of\nthe 3D scene is preserved over time as the majority of the environment is static.\nTo further evaluate the geometric consistency of the generative LiDAR sequence,\nwe also measure the outlier point ratio, defined as the percentage of points\nwith the point2plane distance larger than a certain threshold \u03c4. Table 2 shows\nour quantitative results, where we beat the baseline in both metrics by a notable\nmargin. These results clearly show that our LidarDM is capable of generating\ntemporally-consistent LiDAR sequences.\nmAP (%)\nmAP Agreement\nReal\n59.7\n81.1%\nLidarDM\n56.4\nTable\n3:\nReal2Sim:\nDetector\n[75]\ntrained on real data can be evaluated on\nLidarDM-generated data, showing strong\nagreement with its real counterpart, sug-\ngesting its potential for simulation.\nConfig\nmAP (%)\n35k Real\n58.2\n35k Real + 70k LidarDM\n61.3\nTable\n4:\nData\naugmentation:\nLidarDM-generated\ndata\ncan\nenhance\ntraining for detectors on real-world data,\nsuggesting its potential to improve the\nperception\nmodule\nwithout\nexpensive\ndata annotation.\nLayout-aware LiDAR Generation To ascertain the layout-awareness of our\nLidarDM, we use CenterPoint [75] trained on real-world LiDAR scans to validate\nwhether it can accurately detect objects from the LidarDM\u2019s LiDAR scan. As\nLidarDM currently does not generate intensity, we trained CenterPoint excluding\nintensity of the real-world LiDAR scans, hence the potential mismatch between\nour results and those obtained in the original paper [75].\nGiven an input layout I, we generate the corresponding LiDAR scan, run\nCenterPoint on it, and evaluate using mean average precision (mAP) for vehicles.\n14\nV. Zyrianov et al.\nWe compare this result with the mAP from the raw LiDAR. The result in Table\n3 indicates that CenterPoint\u2019s object detection on our generated LiDAR scan is\ncomparable to the ground truth. We also compute the mAP agreement between\nour generated LiDAR scan and raw LiDAR scan, indicating a strong agreement\nbetween the two and demonstrating our approach\u2019s map-awareness and realism.\nQualitative results We show qualitative results of our map-conditioned Li-\nDAR sequence generation in Fig. 7. Our generated results closely match the\nmap conditioning by adding flat surfaces and static vehicles that correspond to\nthe provided layout. The use of physical-based LiDAR sensor simulation guar-\nantees that the generated point clouds are properly occluded by obstacles and\nappear as a realistic LiDAR sweep pattern. We also showcase the accumulated\nLiDAR points over 90 frames in Waymax [20], highlighting LidarDM\u2019s temporal\nconsistency and map-awareness.\n4.5\nAugmenting Real Data with LidarDM\nLidarDM is the first LiDAR generative model capable of generating data condi-\ntioned on a given semantic layout. This capability offers the potential to augment\nthe training data for 3D perception models, thereby further boosting their perfor-\nmance. To evaluate this capability, we first use LidarDM to generate around 70k\nframes of simulation data based on the layout from Waymo Dataset [63]. After\nthat, we pre-train a LiDAR-based 3D object detection model, CenterPoint [75]\n(with PointPillars [36] as its backbone), on these generated LiDAR frames, paired\nwith the object labels from the dataset. We then train the same model on 35k\nframes of real data, both with and without the pre-training stage on the sim-\nulation data, to test the benefits of the LidarDM-generated data. According to\nTable 4, LidarDM can act as an effective generative data augmentation strategy,\noffering more than a 3% improvement in detection accuracy.\n5\nConclusion\nIn this paper, we introduced LidarDM, a novel layout-conditioned latent diffusion\nmodel for generating realistic LiDAR point clouds. Our approach frames the\nproblem as a joint 4D world creation and sensory data generation task and\ndevelops a novel latent diffusion model to create 3D scenes. The resulting point\ncloud videos are realistic, coherent, and layout-aware.\nLimitations So far, LidarDM relies on latent diffusion models, which are not\nreal-time. Recent progress in latent consistency models promises to accelerate\nthe generation process. We also leave LiDAR intensity modeling as future work.\nLidarDM\n15\nAcknowledgement This project is supported by the NCSA Faculty Fellowship,\nand NSF Awards #2331878, #2340254, and #2312102 and gifts from Intel, IBM,\nAmazon, and Meta. We greatly appreciate the NCSA for providing computing\nresources. Vlas Zyrianov is supported by the New Frontiers Fellowship. The\nauthors thank Scott Lathrop and Aaron D. Saxton for technical support with\nthe NCSA Delta cluster.\nReferences\n1. High definitian lidar hdl-64e. https://hypertech.co.il/wp-content/uploads/\n2015/12/HDL-64E-Data-Sheet.pdf\n2. Sketchfab. https://sketchfab.com/feed, accessed: 2021-11-26\n3. Adobe Inc.: Mixamo. https://www.mixamo.com/\n4. Afzal, A., Goues, C.L., Timperley, C.S.: Gzscenic: Automatic scene generation for\ngazebo simulator. arXiv preprint arXiv:2104.08625 (2021)\n5. Ball\u00e9, J., Laparra, V., Simoncelli, E.P.: End-to-end optimized image compression.\nIn: ICLR (2017)\n6. Blattmann, A., Rombach, R., Ling, H., Dockhorn, T., Kim, S.W., Fidler, S., Kreis,\nK.: Align your latents: High-resolution video synthesis with latent diffusion models.\nIn: CVPR (2023)\n7. Bogo, F., Kanazawa, A., Lassner, C., Gehler, P., Romero, J., Black, M.J.: Keep it\nsmpl: Automatic estimation of 3d human pose and shape from a single image. In:\nECCV (2016)\n8. Caccia, L., v. Hoof, H., Courville, A., Pineau, J.: Deep generative modeling of lidar\ndata. In: IROS (2019)\n9. Chang, H., Zhang, H., Jiang, L., Liu, C., Freeman, W.T.: Maskgit: Masked gener-\native image transformer. In: CVPR (2022)\n10. Chen, R., Chen, Y., Jiao, N., Jia, K.: Fantasia3d: Disentangling geometry and\nappearance for high-quality text-to-3d content creation. In: ICCV (2023)\n11. Chen, Y., Rong, F., Duggal, S., Wang, S., Yan, X., Manivasagam, S., Xue, S.,\nYumer, E., Urtasun, R.: Geosim: Realistic video simulation via geometry-aware\ncomposition for self-driving. In: CVPR (2021)\n12. Codevilla, F., Santana, E., L\u00f3pez, A.M., Gaidon, A.: Exploring the limitations of\nbehavior cloning for autonomous driving. ICCV (2019)\n13. Dauner, D., Hallgarten, M., Geiger, A., Chitta, K.: Parting with misconceptions\nabout learning-based vehicle motion planning. In: CoRL (2023)\n14. Dosovitskiy, A., Ros, G., Codevilla, F., Lopez, A., Koltun, V.: CARLA: An open\nurban driving simulator. In: CoRL (2017)\n15. Esser, P., Chiu, J., Atighehchian, P., Granskog, J., Germanidis, A.: Structure and\ncontent-guided video synthesis with diffusion models. In: ICCV (2023)\n16. Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai, Y., Sapp,\nB., Qi, C., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V.,\nMcCauley, A., Shlens, J., Anguelov, D.: Large scale interactive motion forecasting\nfor autonomous driving : The waymo open motion dataset. arXiv (2021)\n17. Fang, J., Zhou, D., Yan, F., Zhao, T., Zhang, F., Ma, Y., Wang, L., Yang, R.:\nAugmented lidar simulator for autonomous driving. RAL (2020)\n18. Feng, L., Li, Q., Peng, Z., Tan, S., Zhou, B.: Trafficgen: Learning to generate\ndiverse and realistic traffic scenarios. In: ICRA (2023)\n16\nV. Zyrianov et al.\n19. Gao, J., Shen, T., Wang, Z., Chen, W., Yin, K., Li, D., Litany, O., Gojcic, Z.,\nFidler, S.: Get3d: A generative model of high quality 3d textured shapes learned\nfrom images. In: Advances In Neural Information Processing Systems (2022)\n20. Gulino, C., Fu, J., Luo, W., Tucker, G., Bronstein, E., Lu, Y., Harb, J., Pan, X.,\nWang, Y., Chen, X., Co-Reyes, J.D., Agarwal, R., Roelofs, R., Lu, Y., Montali, N.,\nMougin, P., Yang, Z., White, B., Faust, A., McAllister, R., Anguelov, D., Sapp, B.:\nWaymax: An accelerated, data-driven simulator for large-scale autonomous driving\nresearch. In: NeurIPS (2023)\n21. Hahner, M., Sakaridis, C., Bijelic, M., Heide, F., Yu, F., Dai, D., Van Gool, L.:\nLiDAR Snowfall Simulation for Robust 3D Object Detection. In: CVPR (2022)\n22. Harvey, W., Naderiparizi, S., Masrani, V., Weilbach, C., Wood, F.: Flexible diffu-\nsion modeling of long videos (2022)\n23. Ho, J.: Classifier-free diffusion guidance. ArXiv (2022)\n24. Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. In: NeurIPS\n(2020)\n25. Ho, J., Salimans, T., Gritsenko, A., Chan, W., Norouzi, M., Fleet, D.J.: Video\ndiffusion models. arXiv:2204.03458 (2022)\n26. Hong, F., Zhang, M., Pan, L., Cai, Z., Yang, L., Liu, Z.: Avatarclip: Zero-shot text-\ndriven generation and animation of 3d avatars. ACM Transactions on Graphics\n(TOG) 41(4), 1\u201319 (2022)\n27. Hu, A., Russell, L., Yeo, H., Murez, Z., Fedoseev, G., Kendall, A., Shotton, J.,\nCorrado, G.: Gaia-1: A generative world model for autonomous driving (2023)\n28. Huang, J., Gojcic, Z., Atzmon, M., Litany, O., Fidler, S., Williams, F.: Neural\nkernel surface reconstruction. In: CVPR (2023)\n29. Huang, L., Wang, S., Wong, K., Liu, J., Urtasun, R.: Octsqueeze: Octree-structured\nentropy model for lidar compression. In: CVPR (2020)\n30. Huang, S., Gojcic, Z., Wang, Z., Williams, F., Kasten, Y., Fidler, S., Schindler, K.,\nLitany, O.: Neural lidar fields for novel view synthesis. In: ICCV (2023)\n31. H\u00f6ppe, T., Mehrjou, A., Bauer, S., Nielsen, D., Dittadi, A.: Diffusion models for\nvideo prediction and infilling (2022), https://arxiv.org/abs/2206.07696\n32. Karras, T., Aittala, M., Aila, T., Laine, S.: Elucidating the design space of diffusion-\nbased generative models. In: Proc. NeurIPS (2022)\n33. Karras, T., Laine, S., Aila, T.: A style-based generator architecture for generative\nadversarial networks. In: CVPR (2019)\n34. Kim, S.W., , Philion, J., Torralba, A., Fidler, S.: DriveGAN: Towards a Control-\nlable High-Quality Neural Simulation. In: CVPR (2021)\n35. Kim, S.W., Brown, B., Yin, K., Kreis, K., Schwarz, K., Li, D., Rombach, R.,\nTorralba, A., Fidler, S.: Neuralfield-ldm: Scene generation with hierarchical latent\ndiffusion models. In: CVPR (2023)\n36. Lang, A.H., Vora, S., Caesar, H., Zhou, L., Yang, J., Beijbom, O.: Pointpillars:\nFast encoders for object detection from point clouds (2019)\n37. Ledig, C., Theis, L., Huszar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken,\nA., Tejani, A., Totz, J., Wang, Z., Shi, W.: Photo-realistic single image super-\nresolution using a generative adversarial network. In: CVPR (2017)\n38. Li, M., Zhou, P., Liu, J.W., Keppo, J., Lin, M., Yan, S., Xu, X.: Instant3d: Instant\ntext-to-3d generation. arXiv preprint arXiv:2311.08403 (2023)\n39. Liao, Y., Xie, J., Geiger, A.: KITTI-360: A novel dataset and benchmarks for urban\nscene understanding in 2d and 3d. PAMI (2022)\n40. Liu, L., Ren, Y., Lin, Z., Zhao, Z.: Pseudo numerical methods for diffusion models\non manifolds. In: ICLR (2022)\nLidarDM\n17\n41. Loper, M., Mahmood, N., Romero, J., Pons-Moll, G., Black, M.J.: SMPL: A\nskinned multi-person linear model. ACM Trans. Graphics (Proc. SIGGRAPH Asia)\n34(6), 248:1\u2013248:16 (Oct 2015)\n42. Low, K.L.: Linear least-squares optimization for point-to-plane icp surface regis-\ntration (01 2004)\n43. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., Zhu, J.: Dpm-solver: A fast ode solver\nfor diffusion probabilistic model sampling in around 10 steps. In: NeurIPS (2022)\n44. Luo, S., Tan, Y., Huang, L., Li, J., Zhao, H.: Latent consistency mod-\nels: Synthesizing high-resolution images with few-step inference. arXiv preprint\narXiv:2310.04378 (2023)\n45. Macenski, S., Foote, T., Gerkey, B., Lalancette, C., Woodall, W.: Robot operating\nsystem 2: Design, architecture, and uses in the wild. Science Robotics (2022)\n46. Manivasagam, S., Wang, S., Wong, K., Zeng, W., Sazanovich, M., Tan, S., Yang,\nB., Ma, W.C., Urtasun, R.: Lidarsim: Realistic lidar simulation by leveraging the\nreal world. In: CVPR (2020)\n47. Meng, C., He, Y., Song, Y., Song, J., Wu, J., Zhu, J.Y., Ermon, S.: SDEdit: Guided\nimage synthesis and editing with stochastic differential equations. In: ICLR (2022)\n48. Milioto, A., Vizzo, I., Behley, J., Stachniss, C.: RangeNet++: Fast and Accurate\nLiDAR Semantic Segmentation. In: IEEE/RSJ Intl. Conf. on Intelligent Robots\nand Systems (IROS) (2019)\n49. Nakashima, K., Iwashita, Y., Kurazume, R.: Generative range imaging for learning\nscene priors of 3D lidar data. In: WACV (2023)\n50. van den Oord, A., Vinyals, O., Kavukcuoglu, K.: Neural discrete representation\nlearning (2018)\n51. Podell, D., English, Z., Lacey, K., Blattmann, A., Dockhorn, T., M\u00fcller, J., Penna,\nJ., Rombach, R.: Sdxl: Improving latent diffusion models for high-resolution image\nsynthesis. arXiv preprint arXiv:2307.01952 (2023)\n52. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using\n2d diffusion. arXiv (2022)\n53. Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., Chen, M.: Hierarchical text-\nconditional image generation with clip latents. ArXiv (2022)\n54. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution\nimage synthesis with latent diffusion models. In: CVPR (2022)\n55. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E.L., Ghasemipour,\nS.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J.,\nNorouzi, M.: Photorealistic text-to-image diffusion models with deep language un-\nderstanding. ArXiv (2022)\n56. Sauer, A., Chitta, K., M\u00fcller, J., Geiger, A.: Projected gans converge faster. In:\nNeurIPS (2021)\n57. Schaefer, S., Warren, J.: Dual marching cubes: primal contouring of dual grids. In:\n12th Pacific Conference on Computer Graphics and Applications, 2004. PG 2004.\nProceedings. pp. 70\u201376 (2004). https://doi.org/10.1109/PCCGA.2004.1348336\n58. Schmidt, J., Khan, Q., Cremers, D.: LiDAR View Synthesis for Robust Vehicle\nNavigation Without Expert Labels. In: ITSC (2023)\n59. Shah, S., Dey, D., Lovett, C., Kapoor, A.: Airsim: High-fidelity visual and physical\nsimulation for autonomous vehicles. In: FSR (2017)\n60. Shen, Y., Chandaka, B., Lin, Z.h., Zhai, A., Cui, H., Forsyth, D., Wang, S.: Sim-\non-wheels: Physical world in the loop simulation for self-driving. arXiv preprint\narXiv:2306.08807 (2023)\n61. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models. In: ICLR (2021)\n18\nV. Zyrianov et al.\n62. Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data\ndistribution. In: NeurIPS (2019)\n63. Sun, P., Kretzschmar, H., Dotiwalla, X., Chouard, A., Patnaik, V., Tsui, P., Guo,\nJ., Zhou, Y., Chai, Y., Caine, B., Vasudevan, V., Han, W., Ngiam, J., Zhao, H.,\nTimofeev, A., Ettinger, S., Krivokon, M., Gao, A., Joshi, A., Zhang, Y., Shlens, J.,\nChen, Z., Anguelov, D.: Scalability in perception for autonomous driving: Waymo\nopen dataset. In: CVPR (2020)\n64. Swerdlow, A., Xu, R., Zhou, B.: Street-view image generation from a bird\u2019s-eye\nview layout. arXiv preprint arXiv:2301.04634 (2023)\n65. Tallavajhula, A., Mericli, C., Kelly, A.: Off-road lidar simulation with data-driven\nterrain primitives. In: ICRA (2018)\n66. Tan, S., Wong, K., Wang, S., Manivasagam, S., Ren, M., Urtasun, R.: Scenegen:\nLearning to generate realistic traffic scenes. CVPR (2021)\n67. Tao, T., Gao, L., Wang, G., Lao, Y., Chen, P., Zhao, H., Hao, D., Liang, X.,\nSalzmann, M., Yu, K.: Lidar-nerf: Novel lidar view synthesis via neural radiance\nfields (2023)\n68. TechCrunch: Waveone aims to make video ai-native and turn streaming upside\ndown. https://techcrunch.com/2020/12/01/waveone-aims-to-make-video-\nai-native-and-turn-streaming-upside-down/, accessed: 2023-11-16\n69. Treiber, M., Hennecke, A., Helbing, D.: Congested traffic states in empirical obser-\nvations and microscopic simulations. Physical Review E 62, 1805\u20131824 (02 2000).\nhttps://doi.org/10.1103/PhysRevE.62.1805\n70. Villegas, R., Babaeizadeh, M., Kindermans, P.J., Moraldo, H., Zhang, H., Saffar,\nM.T., Castro, S., Kunze, J., Erhan, D.: Phenaki: Variable length video generation\nfrom open domain textual description. In: ICLR (2022)\n71. Voleti, V., Jolicoeur-Martineau, A., Pal, C.: Mcvd: Masked conditional video dif-\nfusion for prediction, generation, and interpolation. In: NeurIPS (2022)\n72. Xiong, Y., Ma, W.C., Wang, J., Urtasun, R.: Learning compact representations\nfor lidar completion and generation. In: CVPR (2023)\n73. Yang, R., Srivastava, P., Mandt, S.: Diffusion probabilistic modeling for video\ngeneration (2022)\n74. Yang, Z., Chen, Y., Wang, J., Manivasagam, S., Ma, W.C., Yang, A.J., Urtasun,\nR.: Unisim: A neural closed-loop sensor simulator. In: CVPR (2023)\n75. Yin, T., Zhou, X., Kr\u00e4henb\u00fchl, P.: Center-based 3d object detection and tracking.\nCVPR (2021)\n76. Yu, T., Xiao, T., Stone, A., Tompson, J., Brohan, A., Wang, S., Singh, J., Tan,\nC., Peralta, J., Ichter, B., et al.: Scaling robot learning with semantically imagined\nexperience. In: RSS (2023)\n77. Zeng, W., Luo, W., Suo, S., Sadat, A., Yang, B., Casas, S., Urtasun, R.: End-to-end\ninterpretable neural motion planner (2021)\n78. Zhai, J.T., Feng, Z., Du, J., Mao, Y., Liu, J.J., Tan, Z., Zhang, Y., Ye, X., Wang, J.:\nRethinking the open-loop evaluation of end-to-end autonomous driving in nuscenes.\narXiv preprint arXiv:2305.10430 (2023)\n79. Zhang, L., Xiong, Y., Yang, Z., Casas, S., Hu, R., Urtasun, R.: Learning unsuper-\nvised world models for autonomous driving via discrete diffusion. arXiv preprint\narXiv:2311.01017 (2023)\n80. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image\ndiffusion models. In: ICCV (2023)\n81. Zhou, Q.Y., Park, J., Koltun, V.: Open3D: A modern library for 3D data process-\ning. arXiv:1801.09847 (2018)\nLidarDM\n19\n82. Zhu, X., Zyrianov, V., Liu, Z., Wang, S.: Mapprior: Bird\u2019s-eye view perception\nwith generative models (2023)\n83. Zyrianov, V., Zhu, X., Wang, S.: Learning to generate realistic lidar point cloud.\nIn: ECCV (2022)\nLidarDM: Generative LiDAR Simulation in a\nGenerated World \u2013 Supplementary Materials\nAbstract. In the following supplementary material, we provide addi-\ntional model details and ablations (Sec. 1). In Sec. 2 we provide ad-\nditional qualitative LidarDM results, including a baseline comparison, a\nclose look at pedestrian details, and visualizations of map condition align-\nment. In Sec. 3, we provide additional downstream applications such as\ncrafting safety-critical and out-of-distribution scenarios for self driving.\nIn Sec. 3.3, we perform a Sim2Real data augmentation experiment on an\nend-to-end LiDAR-based planner.\n1\nExperimental Details\n1.1\nSequence Diffusion Baseline.\nThe sequential diffusion baseline follows a latent diffusion architecture similar\nto that of LidarDM. However, it uniquely adopts a BEV LiDAR representation,\nencoding the observed voxelized point cloud, in line with advanced LiDAR gen-\neration methods. This approach specifically employs a binary occupancy grid\ncentered around the ego sensor [72]. The training process is conducted in two\nstages. Initially, a VAE is trained using cross-entropy for reconstruction loss on\nthis data, supplemented by KL divergence regularization. In practice, we found\nUltralidar\u2019s discrete code does not perform as well as continuous latent space.\nFollowing this, a diffusion model is trained in the latent code space. This model\nis conditional and its training mirrors that of LidarDM; it utilizes the same map\nlatent code, which is occasionally omitted during training (20% of the time) [23].\nFor consistent generation, the model simultaneously denoises five latent codes,\neach corresponding to a LiDAR frame, through a latent code-concatenation tech-\nnique. This baseline is developed on four Nvidia A100 40GB GPUs. The AdaM\noptimizer is used, with a learning rate of 1e-4 for the VAE and 1e-5 for the\ndiffusion U-Net, following a cosine decay schedule.\nThis setup allows for an effective comparison between the baseline and our\napproach, demonstrating the efficacy of the core idea of jointly generating a 4D\nworld during sensor generation.\n1.2\nRaydrop Ablation\nWe perform an ablation study on various ray-dropping options in Table 1. The\ntable suggests that Gumbel Softmax generally yields better-calibrated LiDAR\nray drops than Softmax (the one used in Lidarsim), with lower JSD and MMD.\nAdditionally, although not using ray dropping yields a decent MMD score, it\n2\nsignificantly increases the number of points (doubling the ground truth number\nof points), making it unrealistic. For a fair comparison, all configurations are\nevaluated on the same subset of underlying validation 3D worlds (hence the\nnumbers might slightly differ from our reported test numbers).\nConfig\nMMDBEV (\u2193)\nJSDBEV (\u2193)\nNo Raydrop\n1.730 \u00d7 10\u22124\n0.1286\nSoftmax\n1.990 \u00d7 10\u22124\n0.1274\nGumbel (Ours)\n1.846 \u00d7 10\u22124\n0.1271\nTable 1: Raydrop Ablation. (\nbest,\nsecond best,\nthird best)\n2\nLidarDM Additional Results\n2.1\nSequential Diffusion Comparison.\nWe compare the layout-conditioned LiDAR video generation results of our method\nwith the strong sequential diffusion baseline, as described in Section 4.2 of the\nmain paper. Fig. 1 displays these results. From these, it is evident that our\napproach significantly outperforms the baseline in terms of realism and layout-\nawareness. Specifically, we would like the readers to pay attention to the dynamic\nobject\u2019s shape, the layout of walls and other static infrastructures, and missing\nobjects in the baseline.\n2.2\nPedestrian Motion.\nThanks to actor insertion, we can accurately model fine details such as walking\npedestrians compared to other pure generative results. Fig. 3 visualizes such\ncase, offering a view in both how the underlying pedestrian meshes behave, as\nwell as how the corresponding LiDAR points are affected.\n2.3\nMore Map-aligned Qualitative Results.\nFig. 2 presents more map-aligned visualization as well as accumulated point\ncloud, ensuring the map-awareness and temporal consistency of our approach.\nWe encourage the readers to watch our supplementary videos for better visual-\nization of qualitative results. Note: the black lines on the map indicates the road\nedge (which corresponds to a \"bump\" in LiDAR data as indicated in the color\nboxes), not structures (buildings, walls). Thus, our LiDAR points end outside of\nthose black edges (which corresponds to structures, such as buildings). This is\nconsistent with the real-world LiDAR.\nLidarDM\n3\n3\nLidarDM Downstream Tasks Results\n3.1\nSafety-Critical Scenarios\nWe argue that one of the many benefits of LidarDM is the ability to extend\nexisting traffic simulators with realistic and scenario-aware LiDAR data, allow-\ning for sensor-based critical safety scenario evaluation of autonomous system.\nMore specifically, we extend Waymax [20], a BEV traffic simulator, and show-\ncase how LidarDM can create realistic LiDAR for two types of safety-critical\nscenarios, ego-vehicle behavior manipulation (Fig. 4) and actors behavior ma-\nnipulation (Fig. 5).\n3.2\nOut of Distribution Object Insertion\nWe demonstrate the strong controllability and flexible nature of our generative\napproach by replicating dangerous scenarios in simulation. Similar to our ap-\nproach on vehicle or pedestrian actors (Sec. 3.2), we obtain the out-of-distribution\nmeshes from generative models (or even off-the-shelf assets on Sketchfab [2]). For\ntrajectories, we find it sufficient to sample from vehicles\u2019 trajectories from the\ntrajectory bank, or simply pre-define a series of road-crossing trajectories. In\nFig. 6, we show an example of inserting dangerous animals into the scene. These\nscenarios are not present in the Waymo dataset [63], but may occur in real life\ndue to escaped zoo animals or on roads that are near wildlife habitats.\n3.3\nLidarDM as Data Augmentation for Planning\nInspired by our perception data augmentation experiment, we performed a sec-\nond Sim2Real experiment to demonstrate that LidarDM-generated data can\naid in training a learning-based end-to-end motion planner. Model and infer-\nence: Drawing inspiration from the Neural Motion Planner [77], we developed\na learning-based motion planner that takes the five most recent LiDAR obser-\nvations (covering 0.5 seconds of past history) as input and generates a dense\ncost map of size W \u00d7 H \u00d7 T for the future, where T represents the number\nof future timestamps. In this case, T = 10, with a 0.3-second interval between\nconsecutive frames. During inference, we sample trajectories from a trajectory\nbank and select the one with the lowest overall cost as the final planned tra-\njectory. Training: During training, we employ a soft cross-entropy loss to train\nthe cost map and generate a trajectory bank from the Waymo dataset using K-\nMeans. Training and validation is done following the standard splits on Waymo\nOpen dataset. Remark: We want to highlight two key differences compared to\nthe vanilla NMP model: 1) our motion planner does not require privileged HD\nMaps as input, and 2) the motion planner does not explicitly incorporate the\nego car\u2019s past trajectory. The first design choice ensures that the model focuses\non leveraging sensor data; hence, the comparison concentrates on evaluating the\nquality of our generated LiDAR point clouds. The second choice is inspired by\nrecommended practices and findings from recent studies [12, 13, 78], suggesting\n4\nthat incorporating ego vehicle\u2019s past states results in short-cut effects and biases\nimitation learning for end-to-end driving.\nExperimental details: In order to show that LidarDM-generated data can\nhelp augment the motion planner, we first train a model on 92k snippets of\nLidarDM-generated sequences, then fine-tune it on 9.2k real data sequences.\nNote that we use trajectories of expert drivers as GT and use traffic layout\nconditions to generate our LidarDM samples. As a comparison, we also train the\nsame planner model using only the 9.2k real data sequences. To ensure fairness,\nwe train both the real-only model and the real+sim model for a total of 30 epochs\nuntil convergence and choose the best model based on minimal validation loss.\nExperimental results: We report two metrics: 1) the collision rate at a\n3-second future, which measures the safety and traffic awareness of the planner;\nand 2) the L2 distance at 1 second, 2 seconds, and 3 seconds, which measures\nthe accuracy for imitation. We present our results in Table 2. Using generative\npre-training improves the performance of the planner in a low-data regime. In\nparticular, our collision rate after 3 seconds has been reduced by 32% (relative)\nfrom 1.65% to 1.12%. To our knowledge, this is the first time conditional LiDAR\ngeneration has been shown to improve an end-to-end motion planner.\nConfig\nL2 (m)\n@ 1.0s\nL2 (m)\n@ 2.0s\nL2 (m)\n@ 3.0s\nCollision Rate (%)\n9.2k Real\n0.489\n1.374\n3.279\n1.65%\n9.2k Real + 92k LidarDM\n0.490\n1.341\n3.160\n1.12%\nTable 2: Planner data augmentation: LidarDM-generated data can enhance the\ntraining of a Neural Motion Planner [77]-inspired model on real-world data, suggesting\nits potential to improve the planning module without expensive data collection.(\nbest,\nsecond best)\nLidarDM\n5\nBird-Eye View\nSide View\nMap\nSeq Diffusion\nLidarDM (Ours)\nSeq Diffusion\nLidarDM (Ours)\nFig. 1: Comparison of Layout-Conditioned LiDAR Generation on Waymo dataset: Our\napproach significantly outperforms the strong latent-diffusion-based sequential gener-\nation baseline in terms of realism, physics plausibility, and coherence with the input\nlayout.\n6\nt=0\nt=30\nt=60\nt=90\nAccumulated\nFig. 2: More Map-Aligned Qualitative Results. We showcase 4 different frames of the\nsame sequence, with both map-aligned and LiDAR top-down view. We also show the\naccumulated point clouds, colored by their time index to showcase the temporal con-\nsistency.\nLidarDM\n7\nMap + View Angle\nUnderlying Mesh\nPoint Cloud\nt=0\nt=10\nt=20\nt=30\nt=40\nt=50\nFig. 3: Pedestrian motion captured with LidarDM: Thanks to our actor insertion ap-\nproach, we can capture high-fidelity pedestrian movement through LiDAR, which none\nother generative method can achieve.\n8\nRight Lane\nTurn Left\nU-Turn\nGo Straight\nTurn Left\nTurn Right\nt=0\nt=10\nt=20\nt=30\nt=40\nt=50\nFig. 4: Ego-Vehicle Behavior Manipulation: By extending Waymax, we can perform\nsafety-critical scenarios evaluation of autonomous system. We showcase that LidarDM\ncan produce realistic LiDAR for different simulated ego trajectories from 2 Waymax\nsequences.\nLidarDM\n9\nSharp U-Turn\nSwitch Lane\nt=0\nt=10\nt=20\nt=30\nt=40\nt=50\nFig. 5: Actor Behavior Manipulation: To create challenging situation that consitute a\nsafety-critical scenario, we showcase that LidarDM can also produce realistic LiDAR\ndata for when the actor\u2019s behavior is manipulated, as indicated in the purple boxes.\nUnderlying Mesh\nCorresponding Lidar\nFig. 6: Rare Scenario Simulation: The provided LidarDM approach is grounded in\nphysical simulation, suggesting that our generative method can be combined with stan-\ndard physics-based ray casting simulation to simulate out-of-distribution rare cases,\nsuch as a tiger crossing the street.\n",
    "1810.04805": "BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding\nJacob Devlin\nMing-Wei Chang\nKenton Lee\nKristina Toutanova\nGoogle AI Language\n{jacobdevlin,mingweichang,kentonl,kristout}@google.com\nAbstract\nWe introduce a new language representa-\ntion model called BERT, which stands for\nBidirectional Encoder Representations from\nTransformers. Unlike recent language repre-\nsentation models (Peters et al., 2018a; Rad-\nford et al., 2018), BERT is designed to pre-\ntrain deep bidirectional representations from\nunlabeled text by jointly conditioning on both\nleft and right context in all layers. As a re-\nsult, the pre-trained BERT model can be \ufb01ne-\ntuned with just one additional output layer\nto create state-of-the-art models for a wide\nrange of tasks, such as question answering and\nlanguage inference, without substantial task-\nspeci\ufb01c architecture modi\ufb01cations.\nBERT is conceptually simple and empirically\npowerful.\nIt obtains new state-of-the-art re-\nsults on eleven natural language processing\ntasks, including pushing the GLUE score to\n80.5% (7.7% point absolute improvement),\nMultiNLI accuracy to 86.7% (4.6% absolute\nimprovement), SQuAD v1.1 question answer-\ning Test F1 to 93.2 (1.5 point absolute im-\nprovement) and SQuAD v2.0 Test F1 to 83.1\n(5.1 point absolute improvement).\n1\nIntroduction\nLanguage model pre-training has been shown to\nbe effective for improving many natural language\nprocessing tasks (Dai and Le, 2015; Peters et al.,\n2018a; Radford et al., 2018; Howard and Ruder,\n2018). These include sentence-level tasks such as\nnatural language inference (Bowman et al., 2015;\nWilliams et al., 2018) and paraphrasing (Dolan\nand Brockett, 2005), which aim to predict the re-\nlationships between sentences by analyzing them\nholistically, as well as token-level tasks such as\nnamed entity recognition and question answering,\nwhere models are required to produce \ufb01ne-grained\noutput at the token level (Tjong Kim Sang and\nDe Meulder, 2003; Rajpurkar et al., 2016).\nThere are two existing strategies for apply-\ning pre-trained language representations to down-\nstream tasks: feature-based and \ufb01ne-tuning. The\nfeature-based approach, such as ELMo (Peters\net al., 2018a), uses task-speci\ufb01c architectures that\ninclude the pre-trained representations as addi-\ntional features. The \ufb01ne-tuning approach, such as\nthe Generative Pre-trained Transformer (OpenAI\nGPT) (Radford et al., 2018), introduces minimal\ntask-speci\ufb01c parameters, and is trained on the\ndownstream tasks by simply \ufb01ne-tuning all pre-\ntrained parameters. The two approaches share the\nsame objective function during pre-training, where\nthey use unidirectional language models to learn\ngeneral language representations.\nWe argue that current techniques restrict the\npower of the pre-trained representations, espe-\ncially for the \ufb01ne-tuning approaches.\nThe ma-\njor limitation is that standard language models are\nunidirectional, and this limits the choice of archi-\ntectures that can be used during pre-training. For\nexample, in OpenAI GPT, the authors use a left-to-\nright architecture, where every token can only at-\ntend to previous tokens in the self-attention layers\nof the Transformer (Vaswani et al., 2017). Such re-\nstrictions are sub-optimal for sentence-level tasks,\nand could be very harmful when applying \ufb01ne-\ntuning based approaches to token-level tasks such\nas question answering, where it is crucial to incor-\nporate context from both directions.\nIn this paper, we improve the \ufb01ne-tuning based\napproaches by proposing BERT: Bidirectional\nEncoder\nRepresentations\nfrom\nTransformers.\nBERT alleviates the previously mentioned unidi-\nrectionality constraint by using a \u201cmasked lan-\nguage model\u201d (MLM) pre-training objective, in-\nspired by the Cloze task (Taylor, 1953).\nThe\nmasked language model randomly masks some of\nthe tokens from the input, and the objective is to\npredict the original vocabulary id of the masked\narXiv:1810.04805v2  [cs.CL]  24 May 2019\nword based only on its context.\nUnlike left-to-\nright language model pre-training, the MLM ob-\njective enables the representation to fuse the left\nand the right context, which allows us to pre-\ntrain a deep bidirectional Transformer. In addi-\ntion to the masked language model, we also use\na \u201cnext sentence prediction\u201d task that jointly pre-\ntrains text-pair representations. The contributions\nof our paper are as follows:\n\u2022 We demonstrate the importance of bidirectional\npre-training for language representations. Un-\nlike Radford et al. (2018), which uses unidirec-\ntional language models for pre-training, BERT\nuses masked language models to enable pre-\ntrained deep bidirectional representations. This\nis also in contrast to Peters et al. (2018a), which\nuses a shallow concatenation of independently\ntrained left-to-right and right-to-left LMs.\n\u2022 We show that pre-trained representations reduce\nthe need for many heavily-engineered task-\nspeci\ufb01c architectures. BERT is the \ufb01rst \ufb01ne-\ntuning based representation model that achieves\nstate-of-the-art performance on a large suite\nof sentence-level and token-level tasks, outper-\nforming many task-speci\ufb01c architectures.\n\u2022 BERT advances the state of the art for eleven\nNLP tasks.\nThe code and pre-trained mod-\nels are available at https://github.com/\ngoogle-research/bert.\n2\nRelated Work\nThere is a long history of pre-training general lan-\nguage representations, and we brie\ufb02y review the\nmost widely-used approaches in this section.\n2.1\nUnsupervised Feature-based Approaches\nLearning widely applicable representations of\nwords has been an active area of research for\ndecades, including non-neural (Brown et al., 1992;\nAndo and Zhang, 2005; Blitzer et al., 2006) and\nneural (Mikolov et al., 2013; Pennington et al.,\n2014) methods.\nPre-trained word embeddings\nare an integral part of modern NLP systems, of-\nfering signi\ufb01cant improvements over embeddings\nlearned from scratch (Turian et al., 2010). To pre-\ntrain word embedding vectors, left-to-right lan-\nguage modeling objectives have been used (Mnih\nand Hinton, 2009), as well as objectives to dis-\ncriminate correct from incorrect words in left and\nright context (Mikolov et al., 2013).\nThese approaches have been generalized to\ncoarser granularities, such as sentence embed-\ndings (Kiros et al., 2015; Logeswaran and Lee,\n2018) or paragraph embeddings (Le and Mikolov,\n2014).\nTo train sentence representations, prior\nwork has used objectives to rank candidate next\nsentences (Jernite et al., 2017; Logeswaran and\nLee, 2018), left-to-right generation of next sen-\ntence words given a representation of the previous\nsentence (Kiros et al., 2015), or denoising auto-\nencoder derived objectives (Hill et al., 2016).\nELMo and its predecessor (Peters et al., 2017,\n2018a) generalize traditional word embedding re-\nsearch along a different dimension. They extract\ncontext-sensitive features from a left-to-right and a\nright-to-left language model. The contextual rep-\nresentation of each token is the concatenation of\nthe left-to-right and right-to-left representations.\nWhen integrating contextual word embeddings\nwith existing task-speci\ufb01c architectures, ELMo\nadvances the state of the art for several major NLP\nbenchmarks (Peters et al., 2018a) including ques-\ntion answering (Rajpurkar et al., 2016), sentiment\nanalysis (Socher et al., 2013), and named entity\nrecognition (Tjong Kim Sang and De Meulder,\n2003). Melamud et al. (2016) proposed learning\ncontextual representations through a task to pre-\ndict a single word from both left and right context\nusing LSTMs. Similar to ELMo, their model is\nfeature-based and not deeply bidirectional. Fedus\net al. (2018) shows that the cloze task can be used\nto improve the robustness of text generation mod-\nels.\n2.2\nUnsupervised Fine-tuning Approaches\nAs with the feature-based approaches, the \ufb01rst\nworks in this direction only pre-trained word em-\nbedding parameters from unlabeled text\n(Col-\nlobert and Weston, 2008).\nMore recently, sentence or document encoders\nwhich produce contextual token representations\nhave been pre-trained from unlabeled text and\n\ufb01ne-tuned for a supervised downstream task (Dai\nand Le, 2015; Howard and Ruder, 2018; Radford\net al., 2018). The advantage of these approaches\nis that few parameters need to be learned from\nscratch.\nAt least partly due to this advantage,\nOpenAI GPT (Radford et al., 2018) achieved pre-\nviously state-of-the-art results on many sentence-\nlevel tasks from the GLUE benchmark (Wang\net al., 2018a).\nLeft-to-right language model-\nBERT\nBERT\nE[CLS]\nE1\n E[SEP]\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\n[CLS]\nTok 1\n [SEP]\n...\nTok N\nTok 1\n...\nTokM\nQuestion\nParagraph\nStart/End Span\nBERT\nE[CLS]\nE1\n E[SEP]\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\n[CLS]\nTok 1\n [SEP]\n...\nTok N\nTok 1\n...\nTokM\nMasked Sentence A\nMasked Sentence B\nPre-training\nFine-Tuning\nNSP\nMask LM\nMask LM\nUnlabeled Sentence A and B Pair \nSQuAD\nQuestion Answer Pair\nNER\nMNLI\nFigure 1: Overall pre-training and \ufb01ne-tuning procedures for BERT. Apart from output layers, the same architec-\ntures are used in both pre-training and \ufb01ne-tuning. The same pre-trained model parameters are used to initialize\nmodels for different down-stream tasks. During \ufb01ne-tuning, all parameters are \ufb01ne-tuned. [CLS] is a special\nsymbol added in front of every input example, and [SEP] is a special separator token (e.g. separating ques-\ntions/answers).\ning and auto-encoder objectives have been used\nfor pre-training such models (Howard and Ruder,\n2018; Radford et al., 2018; Dai and Le, 2015).\n2.3\nTransfer Learning from Supervised Data\nThere has also been work showing effective trans-\nfer from supervised tasks with large datasets, such\nas natural language inference (Conneau et al.,\n2017) and machine translation (McCann et al.,\n2017). Computer vision research has also demon-\nstrated the importance of transfer learning from\nlarge pre-trained models, where an effective recipe\nis to \ufb01ne-tune models pre-trained with Ima-\ngeNet (Deng et al., 2009; Yosinski et al., 2014).\n3\nBERT\nWe introduce BERT and its detailed implementa-\ntion in this section. There are two steps in our\nframework: pre-training and \ufb01ne-tuning.\nDur-\ning pre-training, the model is trained on unlabeled\ndata over different pre-training tasks.\nFor \ufb01ne-\ntuning, the BERT model is \ufb01rst initialized with\nthe pre-trained parameters, and all of the param-\neters are \ufb01ne-tuned using labeled data from the\ndownstream tasks. Each downstream task has sep-\narate \ufb01ne-tuned models, even though they are ini-\ntialized with the same pre-trained parameters. The\nquestion-answering example in Figure 1 will serve\nas a running example for this section.\nA distinctive feature of BERT is its uni\ufb01ed ar-\nchitecture across different tasks. There is mini-\nmal difference between the pre-trained architec-\nture and the \ufb01nal downstream architecture.\nModel Architecture\nBERT\u2019s model architec-\nture is a multi-layer bidirectional Transformer en-\ncoder based on the original implementation de-\nscribed in Vaswani et al. (2017) and released in\nthe tensor2tensor library.1 Because the use\nof Transformers has become common and our im-\nplementation is almost identical to the original,\nwe will omit an exhaustive background descrip-\ntion of the model architecture and refer readers to\nVaswani et al. (2017) as well as excellent guides\nsuch as \u201cThe Annotated Transformer.\u201d2\nIn this work, we denote the number of layers\n(i.e., Transformer blocks) as L, the hidden size as\nH, and the number of self-attention heads as A.3\nWe primarily report results on two model sizes:\nBERTBASE (L=12, H=768, A=12, Total Param-\neters=110M) and BERTLARGE (L=24, H=1024,\nA=16, Total Parameters=340M).\nBERTBASE was chosen to have the same model\nsize as OpenAI GPT for comparison purposes.\nCritically, however, the BERT Transformer uses\nbidirectional self-attention, while the GPT Trans-\nformer uses constrained self-attention where every\ntoken can only attend to context to its left.4\n1https://github.com/tensor\ufb02ow/tensor2tensor\n2http://nlp.seas.harvard.edu/2018/04/03/attention.html\n3In all cases we set the feed-forward/\ufb01lter size to be 4H,\ni.e., 3072 for the H = 768 and 4096 for the H = 1024.\n4We note that in the literature the bidirectional Trans-\nInput/Output Representations\nTo make BERT\nhandle a variety of down-stream tasks, our input\nrepresentation is able to unambiguously represent\nboth a single sentence and a pair of sentences\n(e.g., \u27e8Question, Answer \u27e9) in one token sequence.\nThroughout this work, a \u201csentence\u201d can be an arbi-\ntrary span of contiguous text, rather than an actual\nlinguistic sentence. A \u201csequence\u201d refers to the in-\nput token sequence to BERT, which may be a sin-\ngle sentence or two sentences packed together.\nWe use WordPiece embeddings (Wu et al.,\n2016) with a 30,000 token vocabulary. The \ufb01rst\ntoken of every sequence is always a special clas-\nsi\ufb01cation token ([CLS]). The \ufb01nal hidden state\ncorresponding to this token is used as the ag-\ngregate sequence representation for classi\ufb01cation\ntasks. Sentence pairs are packed together into a\nsingle sequence. We differentiate the sentences in\ntwo ways. First, we separate them with a special\ntoken ([SEP]). Second, we add a learned embed-\nding to every token indicating whether it belongs\nto sentence A or sentence B. As shown in Figure 1,\nwe denote input embedding as E, the \ufb01nal hidden\nvector of the special [CLS] token as C \u2208RH,\nand the \ufb01nal hidden vector for the ith input token\nas Ti \u2208RH.\nFor a given token, its input representation is\nconstructed by summing the corresponding token,\nsegment, and position embeddings. A visualiza-\ntion of this construction can be seen in Figure 2.\n3.1\nPre-training BERT\nUnlike Peters et al. (2018a) and Radford et al.\n(2018), we do not use traditional left-to-right or\nright-to-left language models to pre-train BERT.\nInstead, we pre-train BERT using two unsuper-\nvised tasks, described in this section. This step\nis presented in the left part of Figure 1.\nTask #1: Masked LM\nIntuitively, it is reason-\nable to believe that a deep bidirectional model is\nstrictly more powerful than either a left-to-right\nmodel or the shallow concatenation of a left-to-\nright and a right-to-left model.\nUnfortunately,\nstandard conditional language models can only be\ntrained left-to-right or right-to-left, since bidirec-\ntional conditioning would allow each word to in-\ndirectly \u201csee itself\u201d, and the model could trivially\npredict the target word in a multi-layered context.\nformer is often referred to as a \u201cTransformer encoder\u201d while\nthe left-context-only version is referred to as a \u201cTransformer\ndecoder\u201d since it can be used for text generation.\nIn order to train a deep bidirectional representa-\ntion, we simply mask some percentage of the input\ntokens at random, and then predict those masked\ntokens. We refer to this procedure as a \u201cmasked\nLM\u201d (MLM), although it is often referred to as a\nCloze task in the literature (Taylor, 1953). In this\ncase, the \ufb01nal hidden vectors corresponding to the\nmask tokens are fed into an output softmax over\nthe vocabulary, as in a standard LM. In all of our\nexperiments, we mask 15% of all WordPiece to-\nkens in each sequence at random. In contrast to\ndenoising auto-encoders (Vincent et al., 2008), we\nonly predict the masked words rather than recon-\nstructing the entire input.\nAlthough this allows us to obtain a bidirec-\ntional pre-trained model, a downside is that we\nare creating a mismatch between pre-training and\n\ufb01ne-tuning, since the [MASK] token does not ap-\npear during \ufb01ne-tuning. To mitigate this, we do\nnot always replace \u201cmasked\u201d words with the ac-\ntual [MASK] token. The training data generator\nchooses 15% of the token positions at random for\nprediction. If the i-th token is chosen, we replace\nthe i-th token with (1) the [MASK] token 80% of\nthe time (2) a random token 10% of the time (3)\nthe unchanged i-th token 10% of the time. Then,\nTi will be used to predict the original token with\ncross entropy loss. We compare variations of this\nprocedure in Appendix C.2.\nTask #2:\nNext Sentence Prediction (NSP)\nMany important downstream tasks such as Ques-\ntion Answering (QA) and Natural Language Infer-\nence (NLI) are based on understanding the rela-\ntionship between two sentences, which is not di-\nrectly captured by language modeling. In order\nto train a model that understands sentence rela-\ntionships, we pre-train for a binarized next sen-\ntence prediction task that can be trivially gener-\nated from any monolingual corpus. Speci\ufb01cally,\nwhen choosing the sentences A and B for each pre-\ntraining example, 50% of the time B is the actual\nnext sentence that follows A (labeled as IsNext),\nand 50% of the time it is a random sentence from\nthe corpus (labeled as NotNext).\nAs we show\nin Figure 1, C is used for next sentence predic-\ntion (NSP).5 Despite its simplicity, we demon-\nstrate in Section 5.1 that pre-training towards this\ntask is very bene\ufb01cial to both QA and NLI. 6\n5The \ufb01nal model achieves 97%-98% accuracy on NSP.\n6The vector C is not a meaningful sentence representation\nwithout \ufb01ne-tuning, since it was trained with NSP.\n[CLS]\nhe\nlikes\nplay\n##ing\n[SEP]\nmy\ndog\nis\ncute\n[SEP]\nInput\nE[CLS]\nEhe\nElikes\nEplay\nE##ing\nE[SEP]\nEmy\nEdog\nEis\nEcute\nE[SEP]\nToken\nEmbeddings\nEA\nEB\nEB\nEB\nEB\nEB\nEA\nEA\nEA\nEA\nEA\nSegment\nEmbeddings\nE0\nE6\nE7\nE8\nE9\nE10\nE1\nE2\nE3\nE4\nE5\nPosition\nEmbeddings\nFigure 2: BERT input representation. The input embeddings are the sum of the token embeddings, the segmenta-\ntion embeddings and the position embeddings.\nThe NSP task is closely related to representation-\nlearning objectives used in Jernite et al. (2017) and\nLogeswaran and Lee (2018). However, in prior\nwork, only sentence embeddings are transferred to\ndown-stream tasks, where BERT transfers all pa-\nrameters to initialize end-task model parameters.\nPre-training data The pre-training procedure\nlargely follows the existing literature on language\nmodel pre-training. For the pre-training corpus we\nuse the BooksCorpus (800M words) (Zhu et al.,\n2015) and English Wikipedia (2,500M words).\nFor Wikipedia we extract only the text passages\nand ignore lists, tables, and headers. It is criti-\ncal to use a document-level corpus rather than a\nshuf\ufb02ed sentence-level corpus such as the Billion\nWord Benchmark (Chelba et al., 2013) in order to\nextract long contiguous sequences.\n3.2\nFine-tuning BERT\nFine-tuning is straightforward since the self-\nattention mechanism in the Transformer al-\nlows BERT to model many downstream tasks\u2014\nwhether they involve single text or text pairs\u2014by\nswapping out the appropriate inputs and outputs.\nFor applications involving text pairs, a common\npattern is to independently encode text pairs be-\nfore applying bidirectional cross attention, such\nas Parikh et al. (2016); Seo et al. (2017). BERT\ninstead uses the self-attention mechanism to unify\nthese two stages, as encoding a concatenated text\npair with self-attention effectively includes bidi-\nrectional cross attention between two sentences.\nFor each task, we simply plug in the task-\nspeci\ufb01c inputs and outputs into BERT and \ufb01ne-\ntune all the parameters end-to-end.\nAt the in-\nput, sentence A and sentence B from pre-training\nare analogous to (1) sentence pairs in paraphras-\ning, (2) hypothesis-premise pairs in entailment, (3)\nquestion-passage pairs in question answering, and\n(4) a degenerate text-\u2205pair in text classi\ufb01cation\nor sequence tagging. At the output, the token rep-\nresentations are fed into an output layer for token-\nlevel tasks, such as sequence tagging or question\nanswering, and the [CLS] representation is fed\ninto an output layer for classi\ufb01cation, such as en-\ntailment or sentiment analysis.\nCompared to pre-training, \ufb01ne-tuning is rela-\ntively inexpensive. All of the results in the pa-\nper can be replicated in at most 1 hour on a sin-\ngle Cloud TPU, or a few hours on a GPU, starting\nfrom the exact same pre-trained model.7 We de-\nscribe the task-speci\ufb01c details in the correspond-\ning subsections of Section 4. More details can be\nfound in Appendix A.5.\n4\nExperiments\nIn this section, we present BERT \ufb01ne-tuning re-\nsults on 11 NLP tasks.\n4.1\nGLUE\nThe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018a) is a col-\nlection of diverse natural language understanding\ntasks. Detailed descriptions of GLUE datasets are\nincluded in Appendix B.1.\nTo \ufb01ne-tune on GLUE, we represent the input\nsequence (for single sentence or sentence pairs)\nas described in Section 3, and use the \ufb01nal hid-\nden vector C \u2208RH corresponding to the \ufb01rst\ninput token ([CLS]) as the aggregate representa-\ntion. The only new parameters introduced during\n\ufb01ne-tuning are classi\ufb01cation layer weights W \u2208\nRK\u00d7H, where K is the number of labels. We com-\npute a standard classi\ufb01cation loss with C and W,\ni.e., log(softmax(CW T )).\n7For example, the BERT SQuAD model can be trained in\naround 30 minutes on a single Cloud TPU to achieve a Dev\nF1 score of 91.0%.\n8See (10) in https://gluebenchmark.com/faq.\nSystem\nMNLI-(m/mm)\nQQP\nQNLI\nSST-2\nCoLA\nSTS-B\nMRPC\nRTE\nAverage\n392k\n363k\n108k\n67k\n8.5k\n5.7k\n3.5k\n2.5k\n-\nPre-OpenAI SOTA\n80.6/80.1\n66.1\n82.3\n93.2\n35.0\n81.0\n86.0\n61.7\n74.0\nBiLSTM+ELMo+Attn\n76.4/76.1\n64.8\n79.8\n90.4\n36.0\n73.3\n84.9\n56.8\n71.0\nOpenAI GPT\n82.1/81.4\n70.3\n87.4\n91.3\n45.4\n80.0\n82.3\n56.0\n75.1\nBERTBASE\n84.6/83.4\n71.2\n90.5\n93.5\n52.1\n85.8\n88.9\n66.4\n79.6\nBERTLARGE\n86.7/85.9\n72.1\n92.7\n94.9\n60.5\n86.5\n89.3\n70.1\n82.1\nTable 1: GLUE Test results, scored by the evaluation server (https://gluebenchmark.com/leaderboard).\nThe number below each task denotes the number of training examples. The \u201cAverage\u201d column is slightly different\nthan the of\ufb01cial GLUE score, since we exclude the problematic WNLI set.8 BERT and OpenAI GPT are single-\nmodel, single task. F1 scores are reported for QQP and MRPC, Spearman correlations are reported for STS-B, and\naccuracy scores are reported for the other tasks. We exclude entries that use BERT as one of their components.\nWe use a batch size of 32 and \ufb01ne-tune for 3\nepochs over the data for all GLUE tasks. For each\ntask, we selected the best \ufb01ne-tuning learning rate\n(among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set.\nAdditionally, for BERTLARGE we found that \ufb01ne-\ntuning was sometimes unstable on small datasets,\nso we ran several random restarts and selected the\nbest model on the Dev set. With random restarts,\nwe use the same pre-trained checkpoint but per-\nform different \ufb01ne-tuning data shuf\ufb02ing and clas-\nsi\ufb01er layer initialization.9\nResults are presented in Table 1.\nBoth\nBERTBASE and BERTLARGE outperform all sys-\ntems on all tasks by a substantial margin, obtaining\n4.5% and 7.0% respective average accuracy im-\nprovement over the prior state of the art. Note that\nBERTBASE and OpenAI GPT are nearly identical\nin terms of model architecture apart from the at-\ntention masking. For the largest and most widely\nreported GLUE task, MNLI, BERT obtains a 4.6%\nabsolute accuracy improvement. On the of\ufb01cial\nGLUE leaderboard10, BERTLARGE obtains a score\nof 80.5, compared to OpenAI GPT, which obtains\n72.8 as of the date of writing.\nWe \ufb01nd that BERTLARGE signi\ufb01cantly outper-\nforms BERTBASE across all tasks, especially those\nwith very little training data. The effect of model\nsize is explored more thoroughly in Section 5.2.\n4.2\nSQuAD v1.1\nThe\nStanford\nQuestion\nAnswering\nDataset\n(SQuAD v1.1) is a collection of 100k crowd-\nsourced question/answer pairs (Rajpurkar et al.,\n2016).\nGiven a question and a passage from\n9The GLUE data set distribution does not include the Test\nlabels, and we only made a single GLUE evaluation server\nsubmission for each of BERTBASE and BERTLARGE.\n10https://gluebenchmark.com/leaderboard\nWikipedia containing the answer, the task is to\npredict the answer text span in the passage.\nAs shown in Figure 1, in the question answer-\ning task, we represent the input question and pas-\nsage as a single packed sequence, with the ques-\ntion using the A embedding and the passage using\nthe B embedding. We only introduce a start vec-\ntor S \u2208RH and an end vector E \u2208RH during\n\ufb01ne-tuning. The probability of word i being the\nstart of the answer span is computed as a dot prod-\nuct between Ti and S followed by a softmax over\nall of the words in the paragraph: Pi =\neS\u00b7Ti\nP\nj eS\u00b7Tj .\nThe analogous formula is used for the end of the\nanswer span. The score of a candidate span from\nposition i to position j is de\ufb01ned as S\u00b7Ti + E\u00b7Tj,\nand the maximum scoring span where j \u2265i is\nused as a prediction. The training objective is the\nsum of the log-likelihoods of the correct start and\nend positions. We \ufb01ne-tune for 3 epochs with a\nlearning rate of 5e-5 and a batch size of 32.\nTable 2 shows top leaderboard entries as well\nas results from top published systems (Seo et al.,\n2017; Clark and Gardner, 2018; Peters et al.,\n2018a; Hu et al., 2018). The top results from the\nSQuAD leaderboard do not have up-to-date public\nsystem descriptions available,11 and are allowed to\nuse any public data when training their systems.\nWe therefore use modest data augmentation in\nour system by \ufb01rst \ufb01ne-tuning on TriviaQA (Joshi\net al., 2017) befor \ufb01ne-tuning on SQuAD.\nOur best performing system outperforms the top\nleaderboard system by +1.5 F1 in ensembling and\n+1.3 F1 as a single system. In fact, our single\nBERT model outperforms the top ensemble sys-\ntem in terms of F1 score. Without TriviaQA \ufb01ne-\n11QANet is described in Yu et al. (2018), but the system\nhas improved substantially after publication.\nSystem\nDev\nTest\nEM\nF1\nEM\nF1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman\n-\n-\n82.3 91.2\n#1 Ensemble - nlnet\n-\n-\n86.0 91.7\n#2 Ensemble - QANet\n-\n-\n84.5 90.5\nPublished\nBiDAF+ELMo (Single)\n-\n85.6\n-\n85.8\nR.M. Reader (Ensemble)\n81.2 87.9 82.3 88.5\nOurs\nBERTBASE (Single)\n80.8 88.5\n-\n-\nBERTLARGE (Single)\n84.1 90.9\n-\n-\nBERTLARGE (Ensemble)\n85.8 91.8\n-\n-\nBERTLARGE (Sgl.+TriviaQA)\n84.2 91.1 85.1 91.8\nBERTLARGE (Ens.+TriviaQA) 86.2 92.2 87.4 93.2\nTable 2:\nSQuAD 1.1 results. The BERT ensemble\nis 7x systems which use different pre-training check-\npoints and \ufb01ne-tuning seeds.\nSystem\nDev\nTest\nEM\nF1\nEM\nF1\nTop Leaderboard Systems (Dec 10th, 2018)\nHuman\n86.3 89.0 86.9 89.5\n#1 Single - MIR-MRC (F-Net)\n-\n-\n74.8 78.0\n#2 Single - nlnet\n-\n-\n74.2 77.1\nPublished\nunet (Ensemble)\n-\n-\n71.4 74.9\nSLQA+ (Single)\n-\n71.4 74.4\nOurs\nBERTLARGE (Single)\n78.7 81.9 80.0 83.1\nTable 3: SQuAD 2.0 results. We exclude entries that\nuse BERT as one of their components.\ntuning data, we only lose 0.1-0.4 F1, still outper-\nforming all existing systems by a wide margin.12\n4.3\nSQuAD v2.0\nThe SQuAD 2.0 task extends the SQuAD 1.1\nproblem de\ufb01nition by allowing for the possibility\nthat no short answer exists in the provided para-\ngraph, making the problem more realistic.\nWe use a simple approach to extend the SQuAD\nv1.1 BERT model for this task. We treat ques-\ntions that do not have an answer as having an an-\nswer span with start and end at the [CLS] to-\nken. The probability space for the start and end\nanswer span positions is extended to include the\nposition of the [CLS] token. For prediction, we\ncompare the score of the no-answer span: snull =\nS\u00b7C + E\u00b7C to the score of the best non-null span\n12The TriviaQA data we used consists of paragraphs from\nTriviaQA-Wiki formed of the \ufb01rst 400 tokens in documents,\nthat contain at least one of the provided possible answers.\nSystem\nDev\nTest\nESIM+GloVe\n51.9 52.7\nESIM+ELMo\n59.1 59.2\nOpenAI GPT\n-\n78.0\nBERTBASE\n81.6\n-\nBERTLARGE\n86.6 86.3\nHuman (expert)\u2020\n-\n85.0\nHuman (5 annotations)\u2020\n-\n88.0\nTable 4: SWAG Dev and Test accuracies. \u2020Human per-\nformance is measured with 100 samples, as reported in\nthe SWAG paper.\n\u02c6\nsi,j = maxj\u2265iS\u00b7Ti + E\u00b7Tj. We predict a non-null\nanswer when \u02c6\nsi,j > snull + \u03c4, where the thresh-\nold \u03c4 is selected on the dev set to maximize F1.\nWe did not use TriviaQA data for this model. We\n\ufb01ne-tuned for 2 epochs with a learning rate of 5e-5\nand a batch size of 48.\nThe results compared to prior leaderboard en-\ntries and top published work (Sun et al., 2018;\nWang et al., 2018b) are shown in Table 3, exclud-\ning systems that use BERT as one of their com-\nponents. We observe a +5.1 F1 improvement over\nthe previous best system.\n4.4\nSWAG\nThe Situations With Adversarial Generations\n(SWAG) dataset contains 113k sentence-pair com-\npletion examples that evaluate grounded common-\nsense inference (Zellers et al., 2018). Given a sen-\ntence, the task is to choose the most plausible con-\ntinuation among four choices.\nWhen \ufb01ne-tuning on the SWAG dataset, we\nconstruct four input sequences, each containing\nthe concatenation of the given sentence (sentence\nA) and a possible continuation (sentence B). The\nonly task-speci\ufb01c parameters introduced is a vec-\ntor whose dot product with the [CLS] token rep-\nresentation C denotes a score for each choice\nwhich is normalized with a softmax layer.\nWe \ufb01ne-tune the model for 3 epochs with a\nlearning rate of 2e-5 and a batch size of 16. Re-\nsults are presented in Table 4. BERTLARGE out-\nperforms the authors\u2019 baseline ESIM+ELMo sys-\ntem by +27.1% and OpenAI GPT by 8.3%.\n5\nAblation Studies\nIn this section, we perform ablation experiments\nover a number of facets of BERT in order to better\nunderstand their relative importance. Additional\nDev Set\nTasks\nMNLI-m QNLI MRPC SST-2 SQuAD\n(Acc)\n(Acc)\n(Acc)\n(Acc)\n(F1)\nBERTBASE\n84.4\n88.4\n86.7\n92.7\n88.5\nNo NSP\n83.9\n84.9\n86.5\n92.6\n87.9\nLTR & No NSP\n82.1\n84.3\n77.5\n92.1\n77.8\n+ BiLSTM\n82.1\n84.1\n75.7\n91.6\n84.9\nTable 5: Ablation over the pre-training tasks using the\nBERTBASE architecture. \u201cNo NSP\u201d is trained without\nthe next sentence prediction task. \u201cLTR & No NSP\u201d is\ntrained as a left-to-right LM without the next sentence\nprediction, like OpenAI GPT. \u201c+ BiLSTM\u201d adds a ran-\ndomly initialized BiLSTM on top of the \u201cLTR + No\nNSP\u201d model during \ufb01ne-tuning.\nablation studies can be found in Appendix C.\n5.1\nEffect of Pre-training Tasks\nWe demonstrate the importance of the deep bidi-\nrectionality of BERT by evaluating two pre-\ntraining objectives using exactly the same pre-\ntraining data, \ufb01ne-tuning scheme, and hyperpa-\nrameters as BERTBASE:\nNo NSP: A bidirectional model which is trained\nusing the \u201cmasked LM\u201d (MLM) but without the\n\u201cnext sentence prediction\u201d (NSP) task.\nLTR & No NSP: A left-context-only model which\nis trained using a standard Left-to-Right (LTR)\nLM, rather than an MLM. The left-only constraint\nwas also applied at \ufb01ne-tuning, because removing\nit introduced a pre-train/\ufb01ne-tune mismatch that\ndegraded downstream performance. Additionally,\nthis model was pre-trained without the NSP task.\nThis is directly comparable to OpenAI GPT, but\nusing our larger training dataset, our input repre-\nsentation, and our \ufb01ne-tuning scheme.\nWe \ufb01rst examine the impact brought by the NSP\ntask.\nIn Table 5, we show that removing NSP\nhurts performance signi\ufb01cantly on QNLI, MNLI,\nand SQuAD 1.1. Next, we evaluate the impact\nof training bidirectional representations by com-\nparing \u201cNo NSP\u201d to \u201cLTR & No NSP\u201d. The LTR\nmodel performs worse than the MLM model on all\ntasks, with large drops on MRPC and SQuAD.\nFor SQuAD it is intuitively clear that a LTR\nmodel will perform poorly at token predictions,\nsince the token-level hidden states have no right-\nside context. In order to make a good faith at-\ntempt at strengthening the LTR system, we added\na randomly initialized BiLSTM on top. This does\nsigni\ufb01cantly improve results on SQuAD, but the\nresults are still far worse than those of the pre-\ntrained bidirectional models. The BiLSTM hurts\nperformance on the GLUE tasks.\nWe recognize that it would also be possible to\ntrain separate LTR and RTL models and represent\neach token as the concatenation of the two mod-\nels, as ELMo does. However: (a) this is twice as\nexpensive as a single bidirectional model; (b) this\nis non-intuitive for tasks like QA, since the RTL\nmodel would not be able to condition the answer\non the question; (c) this it is strictly less powerful\nthan a deep bidirectional model, since it can use\nboth left and right context at every layer.\n5.2\nEffect of Model Size\nIn this section, we explore the effect of model size\non \ufb01ne-tuning task accuracy. We trained a number\nof BERT models with a differing number of layers,\nhidden units, and attention heads, while otherwise\nusing the same hyperparameters and training pro-\ncedure as described previously.\nResults on selected GLUE tasks are shown in\nTable 6. In this table, we report the average Dev\nSet accuracy from 5 random restarts of \ufb01ne-tuning.\nWe can see that larger models lead to a strict ac-\ncuracy improvement across all four datasets, even\nfor MRPC which only has 3,600 labeled train-\ning examples, and is substantially different from\nthe pre-training tasks. It is also perhaps surpris-\ning that we are able to achieve such signi\ufb01cant\nimprovements on top of models which are al-\nready quite large relative to the existing literature.\nFor example, the largest Transformer explored in\nVaswani et al. (2017) is (L=6, H=1024, A=16)\nwith 100M parameters for the encoder, and the\nlargest Transformer we have found in the literature\nis (L=64, H=512, A=2) with 235M parameters\n(Al-Rfou et al., 2018). By contrast, BERTBASE\ncontains 110M parameters and BERTLARGE con-\ntains 340M parameters.\nIt has long been known that increasing the\nmodel size will lead to continual improvements\non large-scale tasks such as machine translation\nand language modeling, which is demonstrated\nby the LM perplexity of held-out training data\nshown in Table 6.\nHowever, we believe that\nthis is the \ufb01rst work to demonstrate convinc-\ningly that scaling to extreme model sizes also\nleads to large improvements on very small scale\ntasks, provided that the model has been suf\ufb01-\nciently pre-trained. Peters et al. (2018b) presented\nmixed results on the downstream task impact of\nincreasing the pre-trained bi-LM size from two\nto four layers and Melamud et al. (2016) men-\ntioned in passing that increasing hidden dimen-\nsion size from 200 to 600 helped, but increasing\nfurther to 1,000 did not bring further improve-\nments. Both of these prior works used a feature-\nbased approach \u2014 we hypothesize that when the\nmodel is \ufb01ne-tuned directly on the downstream\ntasks and uses only a very small number of ran-\ndomly initialized additional parameters, the task-\nspeci\ufb01c models can bene\ufb01t from the larger, more\nexpressive pre-trained representations even when\ndownstream task data is very small.\n5.3\nFeature-based Approach with BERT\nAll of the BERT results presented so far have used\nthe \ufb01ne-tuning approach, where a simple classi\ufb01-\ncation layer is added to the pre-trained model, and\nall parameters are jointly \ufb01ne-tuned on a down-\nstream task. However, the feature-based approach,\nwhere \ufb01xed features are extracted from the pre-\ntrained model, has certain advantages. First, not\nall tasks can be easily represented by a Trans-\nformer encoder architecture, and therefore require\na task-speci\ufb01c model architecture to be added.\nSecond, there are major computational bene\ufb01ts\nto pre-compute an expensive representation of the\ntraining data once and then run many experiments\nwith cheaper models on top of this representation.\nIn this section, we compare the two approaches\nby applying BERT to the CoNLL-2003 Named\nEntity Recognition (NER) task (Tjong Kim Sang\nand De Meulder, 2003). In the input to BERT, we\nuse a case-preserving WordPiece model, and we\ninclude the maximal document context provided\nby the data. Following standard practice, we for-\nmulate this as a tagging task but do not use a CRF\nHyperparams\nDev Set Accuracy\n#L\n#H #A LM (ppl) MNLI-m MRPC SST-2\n3\n768\n12\n5.84\n77.9\n79.8\n88.4\n6\n768\n3\n5.24\n80.6\n82.2\n90.7\n6\n768\n12\n4.68\n81.9\n84.8\n91.3\n12\n768\n12\n3.99\n84.4\n86.7\n92.9\n12 1024\n16\n3.54\n85.7\n86.9\n93.3\n24 1024\n16\n3.23\n86.6\n87.8\n93.7\nTable 6:\nAblation over BERT model size. #L = the\nnumber of layers; #H = hidden size; #A = number of at-\ntention heads. \u201cLM (ppl)\u201d is the masked LM perplexity\nof held-out training data.\nSystem\nDev F1 Test F1\nELMo (Peters et al., 2018a)\n95.7\n92.2\nCVT (Clark et al., 2018)\n-\n92.6\nCSE (Akbik et al., 2018)\n-\n93.1\nFine-tuning approach\nBERTLARGE\n96.6\n92.8\nBERTBASE\n96.4\n92.4\nFeature-based approach (BERTBASE)\nEmbeddings\n91.0\n-\nSecond-to-Last Hidden\n95.6\n-\nLast Hidden\n94.9\n-\nWeighted Sum Last Four Hidden\n95.9\n-\nConcat Last Four Hidden\n96.1\n-\nWeighted Sum All 12 Layers\n95.5\n-\nTable 7: CoNLL-2003 Named Entity Recognition re-\nsults. Hyperparameters were selected using the Dev\nset. The reported Dev and Test scores are averaged over\n5 random restarts using those hyperparameters.\nlayer in the output. We use the representation of\nthe \ufb01rst sub-token as the input to the token-level\nclassi\ufb01er over the NER label set.\nTo ablate the \ufb01ne-tuning approach, we apply the\nfeature-based approach by extracting the activa-\ntions from one or more layers without \ufb01ne-tuning\nany parameters of BERT. These contextual em-\nbeddings are used as input to a randomly initial-\nized two-layer 768-dimensional BiLSTM before\nthe classi\ufb01cation layer.\nResults are presented in Table 7. BERTLARGE\nperforms competitively with state-of-the-art meth-\nods. The best performing method concatenates the\ntoken representations from the top four hidden lay-\ners of the pre-trained Transformer, which is only\n0.3 F1 behind \ufb01ne-tuning the entire model. This\ndemonstrates that BERT is effective for both \ufb01ne-\ntuning and feature-based approaches.\n6\nConclusion\nRecent empirical improvements due to transfer\nlearning with language models have demonstrated\nthat rich, unsupervised pre-training is an integral\npart of many language understanding systems. In\nparticular, these results enable even low-resource\ntasks to bene\ufb01t from deep unidirectional architec-\ntures. Our major contribution is further general-\nizing these \ufb01ndings to deep bidirectional architec-\ntures, allowing the same pre-trained model to suc-\ncessfully tackle a broad set of NLP tasks.\nReferences\nAlan Akbik, Duncan Blythe, and Roland Vollgraf.\n2018. Contextual string embeddings for sequence\nlabeling. In Proceedings of the 27th International\nConference on Computational Linguistics, pages\n1638\u20131649.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018.\nCharacter-level lan-\nguage modeling with deeper self-attention.\narXiv\npreprint arXiv:1808.04444.\nRie Kubota Ando and Tong Zhang. 2005. A framework\nfor learning predictive structures from multiple tasks\nand unlabeled data. Journal of Machine Learning\nResearch, 6(Nov):1817\u20131853.\nLuisa Bentivogli,\nBernardo Magnini,\nIdo Dagan,\nHoa Trang Dang, and Danilo Giampiccolo. 2009.\nThe \ufb01fth PASCAL recognizing textual entailment\nchallenge. In TAC. NIST.\nJohn Blitzer, Ryan McDonald, and Fernando Pereira.\n2006. Domain adaptation with structural correspon-\ndence learning. In Proceedings of the 2006 confer-\nence on empirical methods in natural language pro-\ncessing, pages 120\u2013128. Association for Computa-\ntional Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP. Association for Computational Linguis-\ntics.\nPeter F Brown, Peter V Desouza, Robert L Mercer,\nVincent J Della Pietra, and Jenifer C Lai. 1992.\nClass-based n-gram models of natural language.\nComputational linguistics, 18(4):467\u2013479.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017.\nSemeval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation.\nIn Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1\u201314, Vancou-\nver, Canada. Association for Computational Lin-\nguistics.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nZ. Chen, H. Zhang, X. Zhang, and L. Zhao. 2018.\nQuora question pairs.\nChristopher Clark and Matt Gardner. 2018.\nSimple\nand effective multi-paragraph reading comprehen-\nsion. In ACL.\nKevin Clark, Minh-Thang Luong, Christopher D Man-\nning, and Quoc Le. 2018.\nSemi-supervised se-\nquence modeling with cross-view training. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1914\u2013\n1925.\nRonan Collobert and Jason Weston. 2008. A uni\ufb01ed\narchitecture for natural language processing: Deep\nneural networks with multitask learning.\nIn Pro-\nceedings of the 25th international conference on\nMachine learning, pages 160\u2013167. ACM.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo\u00a8\u0131c\nBarrault, and Antoine Bordes. 2017.\nSupervised\nlearning of universal sentence representations from\nnatural language inference data. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, pages 670\u2013680, Copen-\nhagen, Denmark. Association for Computational\nLinguistics.\nAndrew M Dai and Quoc V Le. 2015. Semi-supervised\nsequence learning. In Advances in neural informa-\ntion processing systems, pages 3079\u20133087.\nJ. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-\nFei. 2009. ImageNet: A Large-Scale Hierarchical\nImage Database. In CVPR09.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nWilliam Fedus, Ian Goodfellow, and Andrew M Dai.\n2018. Maskgan: Better text generation via \ufb01lling in\nthe . arXiv preprint arXiv:1801.07736.\nDan Hendrycks and Kevin Gimpel. 2016.\nBridging\nnonlinearities and stochastic regularizers with gaus-\nsian error linear units. CoRR, abs/1606.08415.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. 2016.\nLearning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies. Association for Computa-\ntional Linguistics.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model \ufb01ne-tuning for text classi\ufb01cation. In\nACL. Association for Computational Linguistics.\nMinghao Hu, Yuxing Peng, Zhen Huang, Xipeng Qiu,\nFuru Wei, and Ming Zhou. 2018.\nReinforced\nmnemonic reader for machine reading comprehen-\nsion. In IJCAI.\nYacine Jernite, Samuel R. Bowman, and David Son-\ntag. 2017. Discourse-based objectives for fast un-\nsupervised sentence representation learning. CoRR,\nabs/1705.00557.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke\nZettlemoyer. 2017. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In ACL.\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov,\nRichard Zemel, Raquel Urtasun, Antonio Torralba,\nand Sanja Fidler. 2015. Skip-thought vectors. In\nAdvances in neural information processing systems,\npages 3294\u20133302.\nQuoc Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Inter-\nnational Conference on Machine Learning, pages\n1188\u20131196.\nHector J Levesque, Ernest Davis, and Leora Morgen-\nstern. 2011. The winograd schema challenge. In\nAaai spring symposium: Logical formalizations of\ncommonsense reasoning, volume 46, page 47.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nef\ufb01cient framework for learning sentence represen-\ntations.\nIn International Conference on Learning\nRepresentations.\nBryan McCann, James Bradbury, Caiming Xiong, and\nRichard Socher. 2017. Learned in translation: Con-\ntextualized word vectors. In NIPS.\nOren Melamud, Jacob Goldberger, and Ido Dagan.\n2016. context2vec: Learning generic context em-\nbedding with bidirectional LSTM. In CoNLL.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in Neural Information Processing\nSystems 26, pages 3111\u20133119. Curran Associates,\nInc.\nAndriy Mnih and Geoffrey E Hinton. 2009. A scal-\nable hierarchical distributed language model.\nIn\nD. Koller, D. Schuurmans, Y. Bengio, and L. Bot-\ntou, editors, Advances in Neural Information Pro-\ncessing Systems 21, pages 1081\u20131088. Curran As-\nsociates, Inc.\nAnkur P Parikh, Oscar T\u00a8ackstr\u00a8om, Dipanjan Das, and\nJakob Uszkoreit. 2016. A decomposable attention\nmodel for natural language inference. In EMNLP.\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Nat-\nural Language Processing (EMNLP), pages 1532\u2013\n1543.\nMatthew Peters, Waleed Ammar, Chandra Bhagavat-\nula, and Russell Power. 2017. Semi-supervised se-\nquence tagging with bidirectional language models.\nIn ACL.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018a. Deep contextualized word rep-\nresentations. In NAACL.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018b.\nDissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n1499\u20131509.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018.\nImproving language under-\nstanding with unsupervised learning. Technical re-\nport, OpenAI.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Nat-\nural Language Processing, pages 2383\u20132392.\nMinjoon Seo, Aniruddha Kembhavi, Ali Farhadi, and\nHannaneh Hajishirzi. 2017. Bidirectional attention\n\ufb02ow for machine comprehension. In ICLR.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Ng, and\nChristopher Potts. 2013.\nRecursive deep models\nfor semantic compositionality over a sentiment tree-\nbank.\nIn Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631\u20131642.\nFu Sun, Linyang Li, Xipeng Qiu, and Yang Liu.\n2018.\nU-net:\nMachine reading comprehension\nwith unanswerable questions.\narXiv preprint\narXiv:1810.06638.\nWilson L Taylor. 1953.\nCloze procedure:\nA new\ntool for measuring readability. Journalism Bulletin,\n30(4):415\u2013433.\nErik F Tjong Kim Sang and Fien De Meulder.\n2003.\nIntroduction to the conll-2003 shared task:\nLanguage-independent named entity recognition. In\nCoNLL.\nJoseph Turian, Lev Ratinov, and Yoshua Bengio. 2010.\nWord representations: A simple and general method\nfor semi-supervised learning. In Proceedings of the\n48th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL \u201910, pages 384\u2013394.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000\u20136010.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and\nPierre-Antoine Manzagol. 2008.\nExtracting and\ncomposing robust features with denoising autoen-\ncoders.\nIn Proceedings of the 25th international\nconference on Machine learning, pages 1096\u20131103.\nACM.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018a.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. In Proceedings\nof the 2018 EMNLP Workshop BlackboxNLP: An-\nalyzing and Interpreting Neural Networks for NLP,\npages 353\u2013355.\nWei Wang, Ming Yan, and Chen Wu. 2018b. Multi-\ngranularity hierarchical attention fusion networks\nfor reading comprehension and question answering.\nIn Proceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers). Association for Computational Lin-\nguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2018.\nNeural network acceptability judg-\nments. arXiv preprint arXiv:1805.12471.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2018.\nA broad-coverage challenge corpus\nfor sentence understanding through inference.\nIn\nNAACL.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe,\nMohammad Norouzi,\nWolfgang Macherey,\nMaxim Krikun,\nYuan Cao,\nQin Gao,\nKlaus\nMacherey, et al. 2016.\nGoogle\u2019s neural ma-\nchine translation system: Bridging the gap between\nhuman and machine translation.\narXiv preprint\narXiv:1609.08144.\nJason Yosinski, Jeff Clune, Yoshua Bengio, and Hod\nLipson. 2014. How transferable are features in deep\nneural networks? In Advances in neural information\nprocessing systems, pages 3320\u20133328.\nAdams Wei Yu, David Dohan, Minh-Thang Luong, Rui\nZhao, Kai Chen, Mohammad Norouzi, and Quoc V\nLe. 2018.\nQANet: Combining local convolution\nwith global self-attention for reading comprehen-\nsion. In ICLR.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin\nChoi. 2018. Swag: A large-scale adversarial dataset\nfor grounded commonsense inference. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books.\nIn Proceedings of the IEEE\ninternational conference on computer vision, pages\n19\u201327.\nAppendix for \u201cBERT: Pre-training of\nDeep Bidirectional Transformers for\nLanguage Understanding\u201d\nWe organize the appendix into three sections:\n\u2022 Additional implementation details for BERT\nare presented in Appendix A;\n\u2022 Additional details for our experiments are\npresented in Appendix B; and\n\u2022 Additional ablation studies are presented in\nAppendix C.\nWe present additional ablation studies for\nBERT including:\n\u2013 Effect of Number of Training Steps; and\n\u2013 Ablation for Different Masking Proce-\ndures.\nA\nAdditional Details for BERT\nA.1\nIllustration of the Pre-training Tasks\nWe provide examples of the pre-training tasks in\nthe following.\nMasked LM and the Masking Procedure\nAs-\nsuming the unlabeled sentence is\nmy dog is\nhairy, and during the random masking procedure\nwe chose the 4-th token (which corresponding to\nhairy), our masking procedure can be further il-\nlustrated by\n\u2022 80% of the time: Replace the word with the\n[MASK] token, e.g., my dog is hairy \u2192\nmy dog is [MASK]\n\u2022 10% of the time: Replace the word with a\nrandom word, e.g., my dog is hairy \u2192my\ndog is apple\n\u2022 10% of the time:\nKeep the word un-\nchanged, e.g., my dog is hairy \u2192my dog\nis hairy. The purpose of this is to bias the\nrepresentation towards the actual observed\nword.\nThe advantage of this procedure is that the\nTransformer encoder does not know which words\nit will be asked to predict or which have been re-\nplaced by random words, so it is forced to keep\na distributional contextual representation of ev-\nery input token.\nAdditionally, because random\nreplacement only occurs for 1.5% of all tokens\n(i.e., 10% of 15%), this does not seem to harm\nthe model\u2019s language understanding capability. In\nSection C.2, we evaluate the impact this proce-\ndure.\nCompared to standard langauge model training,\nthe masked LM only make predictions on 15% of\ntokens in each batch, which suggests that more\npre-training steps may be required for the model\nBERT (Ours)\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n...\n...\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\n...\n...\nOpenAI GPT\nLstm\nELMo\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\nLstm\n T1\nT2\n TN\n...\n...\n...\n...\n...\n E1\nE2\n EN\n...\n T1\nT2\nTN\n...\n E1\nE2\n EN\n...\n T1\nT2\n TN\n...\n E1\nE2\n EN\n...\nFigure 3: Differences in pre-training model architectures. BERT uses a bidirectional Transformer. OpenAI GPT\nuses a left-to-right Transformer. ELMo uses the concatenation of independently trained left-to-right and right-to-\nleft LSTMs to generate features for downstream tasks. Among the three, only BERT representations are jointly\nconditioned on both left and right context in all layers. In addition to the architecture differences, BERT and\nOpenAI GPT are \ufb01ne-tuning approaches, while ELMo is a feature-based approach.\nto converge. In Section C.1 we demonstrate that\nMLM does converge marginally slower than a left-\nto-right model (which predicts every token), but\nthe empirical improvements of the MLM model\nfar outweigh the increased training cost.\nNext Sentence Prediction\nThe next sentence\nprediction task can be illustrated in the following\nexamples.\nInput = [CLS] the man went to [MASK] store [SEP]\nhe bought a gallon [MASK] milk [SEP]\nLabel = IsNext\nInput = [CLS] the man [MASK] to the store [SEP]\npenguin [MASK] are flight ##less birds [SEP]\nLabel = NotNext\nA.2\nPre-training Procedure\nTo generate each training input sequence, we sam-\nple two spans of text from the corpus, which we\nrefer to as \u201csentences\u201d even though they are typ-\nically much longer than single sentences (but can\nbe shorter also). The \ufb01rst sentence receives the A\nembedding and the second receives the B embed-\nding. 50% of the time B is the actual next sentence\nthat follows A and 50% of the time it is a random\nsentence, which is done for the \u201cnext sentence pre-\ndiction\u201d task. They are sampled such that the com-\nbined length is \u2264512 tokens. The LM masking is\napplied after WordPiece tokenization with a uni-\nform masking rate of 15%, and no special consid-\neration given to partial word pieces.\nWe train with batch size of 256 sequences (256\nsequences * 512 tokens = 128,000 tokens/batch)\nfor 1,000,000 steps, which is approximately 40\nepochs over the 3.3 billion word corpus.\nWe\nuse Adam with learning rate of 1e-4, \u03b21 = 0.9,\n\u03b22 = 0.999, L2 weight decay of 0.01, learning\nrate warmup over the \ufb01rst 10,000 steps, and linear\ndecay of the learning rate. We use a dropout prob-\nability of 0.1 on all layers. We use a gelu acti-\nvation (Hendrycks and Gimpel, 2016) rather than\nthe standard relu, following OpenAI GPT. The\ntraining loss is the sum of the mean masked LM\nlikelihood and the mean next sentence prediction\nlikelihood.\nTraining of BERTBASE was performed on 4\nCloud TPUs in Pod con\ufb01guration (16 TPU chips\ntotal).13 Training of BERTLARGE was performed\non 16 Cloud TPUs (64 TPU chips total). Each pre-\ntraining took 4 days to complete.\nLonger sequences are disproportionately expen-\nsive because attention is quadratic to the sequence\nlength. To speed up pretraing in our experiments,\nwe pre-train the model with sequence length of\n128 for 90% of the steps. Then, we train the rest\n10% of the steps of sequence of 512 to learn the\npositional embeddings.\nA.3\nFine-tuning Procedure\nFor \ufb01ne-tuning, most model hyperparameters are\nthe same as in pre-training, with the exception of\nthe batch size, learning rate, and number of train-\ning epochs. The dropout probability was always\nkept at 0.1. The optimal hyperparameter values\nare task-speci\ufb01c, but we found the following range\nof possible values to work well across all tasks:\n\u2022 Batch size: 16, 32\n13https://cloudplatform.googleblog.com/2018/06/Cloud-\nTPU-now-offers-preemptible-pricing-and-global-\navailability.html\n\u2022 Learning rate (Adam): 5e-5, 3e-5, 2e-5\n\u2022 Number of epochs: 2, 3, 4\nWe also observed that large data sets (e.g.,\n100k+ labeled training examples) were far less\nsensitive to hyperparameter choice than small data\nsets. Fine-tuning is typically very fast, so it is rea-\nsonable to simply run an exhaustive search over\nthe above parameters and choose the model that\nperforms best on the development set.\nA.4\nComparison of BERT, ELMo ,and\nOpenAI GPT\nHere we studies the differences in recent popular\nrepresentation learning models including ELMo,\nOpenAI GPT and BERT. The comparisons be-\ntween the model architectures are shown visually\nin Figure 3. Note that in addition to the architec-\nture differences, BERT and OpenAI GPT are \ufb01ne-\ntuning approaches, while ELMo is a feature-based\napproach.\nThe most comparable existing pre-training\nmethod to BERT is OpenAI GPT, which trains a\nleft-to-right Transformer LM on a large text cor-\npus. In fact, many of the design decisions in BERT\nwere intentionally made to make it as close to\nGPT as possible so that the two methods could be\nminimally compared. The core argument of this\nwork is that the bi-directionality and the two pre-\ntraining tasks presented in Section 3.1 account for\nthe majority of the empirical improvements, but\nwe do note that there are several other differences\nbetween how BERT and GPT were trained:\n\u2022 GPT is trained on the BooksCorpus (800M\nwords); BERT is trained on the BooksCor-\npus (800M words) and Wikipedia (2,500M\nwords).\n\u2022 GPT uses a sentence separator ([SEP]) and\nclassi\ufb01er token ([CLS]) which are only in-\ntroduced at \ufb01ne-tuning time; BERT learns\n[SEP], [CLS] and sentence A/B embed-\ndings during pre-training.\n\u2022 GPT was trained for 1M steps with a batch\nsize of 32,000 words; BERT was trained for\n1M steps with a batch size of 128,000 words.\n\u2022 GPT used the same learning rate of 5e-5 for\nall \ufb01ne-tuning experiments; BERT chooses a\ntask-speci\ufb01c \ufb01ne-tuning learning rate which\nperforms the best on the development set.\nTo isolate the effect of these differences, we per-\nform ablation experiments in Section 5.1 which\ndemonstrate that the majority of the improvements\nare in fact coming from the two pre-training tasks\nand the bidirectionality they enable.\nA.5\nIllustrations of Fine-tuning on Different\nTasks\nThe illustration of \ufb01ne-tuning BERT on different\ntasks can be seen in Figure 4. Our task-speci\ufb01c\nmodels are formed by incorporating BERT with\none additional output layer, so a minimal num-\nber of parameters need to be learned from scratch.\nAmong the tasks, (a) and (b) are sequence-level\ntasks while (c) and (d) are token-level tasks. In\nthe \ufb01gure, E represents the input embedding, Ti\nrepresents the contextual representation of token i,\n[CLS] is the special symbol for classi\ufb01cation out-\nput, and [SEP] is the special symbol to separate\nnon-consecutive token sequences.\nB\nDetailed Experimental Setup\nB.1\nDetailed Descriptions for the GLUE\nBenchmark Experiments.\nOur\nGLUE\nresults\nin\nTable1\nare\nobtained\nfrom\nhttps://gluebenchmark.com/\nleaderboard\nand\nhttps://blog.\nopenai.com/language-unsupervised.\nThe GLUE benchmark includes the following\ndatasets, the descriptions of which were originally\nsummarized in Wang et al. (2018a):\nMNLI\nMulti-Genre Natural Language Inference\nis a large-scale, crowdsourced entailment classi\ufb01-\ncation task (Williams et al., 2018). Given a pair of\nsentences, the goal is to predict whether the sec-\nond sentence is an entailment, contradiction, or\nneutral with respect to the \ufb01rst one.\nQQP\nQuora Question Pairs is a binary classi\ufb01-\ncation task where the goal is to determine if two\nquestions asked on Quora are semantically equiv-\nalent (Chen et al., 2018).\nQNLI\nQuestion Natural Language Inference is\na version of the Stanford Question Answering\nDataset (Rajpurkar et al., 2016) which has been\nconverted to a binary classi\ufb01cation task (Wang\net al., 2018a). The positive examples are (ques-\ntion, sentence) pairs which do contain the correct\nanswer, and the negative examples are (question,\nsentence) from the same paragraph which do not\ncontain the answer.\nBERT\nE[CLS]\nE1\n E[SEP]\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\n[CLS]\nTok \n1\n [SEP]\n...\nTok \nN\nTok \n1\n...\nTok\nM\nQuestion\nParagraph\nBERT\nE[CLS]\nE1\n E2\n EN\nC\nT1\n T2\n TN\nSingle Sentence \n...\n...\nBERT\nTok 1\n Tok 2\n Tok N\n...\n[CLS]\nE[CLS]\nE1\n E2\n EN\nC\nT1\n T2\n TN\nSingle Sentence \nB-PER\nO\nO\n...\n...\nE[CLS]\nE1\n E[SEP]\nClass \nLabel\n...\nEN\nE1\u2019\n...\nEM\u2019\nC\nT1\nT[SEP]\n...\nTN\nT1\u2019\n...\nTM\u2019\nStart/End Span\nClass \nLabel\nBERT\nTok 1\n Tok 2\n Tok N\n...\n[CLS]\nTok 1\n[CLS]\n[CLS]\nTok \n1\n [SEP]\n...\nTok \nN\nTok \n1\n...\nTok\nM\nSentence 1\n...\nSentence 2\nFigure 4: Illustrations of Fine-tuning BERT on Different Tasks.\nSST-2\nThe Stanford Sentiment Treebank is a\nbinary single-sentence classi\ufb01cation task consist-\ning of sentences extracted from movie reviews\nwith human annotations of their sentiment (Socher\net al., 2013).\nCoLA\nThe Corpus of Linguistic Acceptability is\na binary single-sentence classi\ufb01cation task, where\nthe goal is to predict whether an English sentence\nis linguistically \u201cacceptable\u201d or not (Warstadt\net al., 2018).\nSTS-B\nThe Semantic Textual Similarity Bench-\nmark is a collection of sentence pairs drawn from\nnews headlines and other sources (Cer et al.,\n2017). They were annotated with a score from 1\nto 5 denoting how similar the two sentences are in\nterms of semantic meaning.\nMRPC\nMicrosoft Research Paraphrase Corpus\nconsists of sentence pairs automatically extracted\nfrom online news sources, with human annotations\nfor whether the sentences in the pair are semanti-\ncally equivalent (Dolan and Brockett, 2005).\nRTE\nRecognizing Textual Entailment is a bi-\nnary entailment task similar to MNLI, but with\nmuch less training data (Bentivogli et al., 2009).14\nWNLI\nWinograd NLI is a small natural lan-\nguage inference dataset (Levesque et al., 2011).\nThe GLUE webpage notes that there are issues\nwith the construction of this dataset, 15 and every\ntrained system that\u2019s been submitted to GLUE has\nperformed worse than the 65.1 baseline accuracy\nof predicting the majority class. We therefore ex-\nclude this set to be fair to OpenAI GPT. For our\nGLUE submission, we always predicted the ma-\n14Note that we only report single-task \ufb01ne-tuning results\nin this paper. A multitask \ufb01ne-tuning approach could poten-\ntially push the performance even further. For example, we\ndid observe substantial improvements on RTE from multi-\ntask training with MNLI.\n15https://gluebenchmark.com/faq\njority class.\nC\nAdditional Ablation Studies\nC.1\nEffect of Number of Training Steps\nFigure 5 presents MNLI Dev accuracy after \ufb01ne-\ntuning from a checkpoint that has been pre-trained\nfor k steps. This allows us to answer the following\nquestions:\n1. Question:\nDoes BERT really need such\na large amount of pre-training (128,000\nwords/batch * 1,000,000 steps) to achieve\nhigh \ufb01ne-tuning accuracy?\nAnswer: Yes, BERTBASE achieves almost\n1.0% additional accuracy on MNLI when\ntrained on 1M steps compared to 500k steps.\n2. Question: Does MLM pre-training converge\nslower than LTR pre-training, since only 15%\nof words are predicted in each batch rather\nthan every word?\nAnswer: The MLM model does converge\nslightly slower than the LTR model. How-\never, in terms of absolute accuracy the MLM\nmodel begins to outperform the LTR model\nalmost immediately.\nC.2\nAblation for Different Masking\nProcedures\nIn Section 3.1, we mention that BERT uses a\nmixed strategy for masking the target tokens when\npre-training with the masked language model\n(MLM) objective. The following is an ablation\nstudy to evaluate the effect of different masking\nstrategies.\n200\n400\n600\n800\n1,000\n76\n78\n80\n82\n84\nPre-training Steps (Thousands)\nMNLI Dev Accuracy\nBERTBASE (Masked LM)\nBERTBASE (Left-to-Right)\nFigure 5: Ablation over number of training steps. This\nshows the MNLI accuracy after \ufb01ne-tuning, starting\nfrom model parameters that have been pre-trained for\nk steps. The x-axis is the value of k.\nNote that the purpose of the masking strategies\nis to reduce the mismatch between pre-training\nand \ufb01ne-tuning, as the [MASK] symbol never ap-\npears during the \ufb01ne-tuning stage. We report the\nDev results for both MNLI and NER. For NER,\nwe report both \ufb01ne-tuning and feature-based ap-\nproaches, as we expect the mismatch will be am-\npli\ufb01ed for the feature-based approach as the model\nwill not have the chance to adjust the representa-\ntions.\nMasking Rates\nDev Set Results\nMASK SAME\nRND\nMNLI\nNER\nFine-tune Fine-tune Feature-based\n80%\n10%\n10%\n84.2\n95.4\n94.9\n100%\n0%\n0%\n84.3\n94.9\n94.0\n80%\n0%\n20%\n84.1\n95.2\n94.6\n80%\n20%\n0%\n84.4\n95.2\n94.7\n0%\n20%\n80%\n83.7\n94.8\n94.6\n0%\n0% 100%\n83.6\n94.9\n94.6\nTable 8: Ablation over different masking strategies.\nThe results are presented in Table 8. In the table,\nMASK means that we replace the target token with\nthe [MASK] symbol for MLM; SAME means that\nwe keep the target token as is; RND means that\nwe replace the target token with another random\ntoken.\nThe numbers in the left part of the table repre-\nsent the probabilities of the speci\ufb01c strategies used\nduring MLM pre-training (BERT uses 80%, 10%,\n10%). The right part of the paper represents the\nDev set results. For the feature-based approach,\nwe concatenate the last 4 layers of BERT as the\nfeatures, which was shown to be the best approach\nin Section 5.3.\nFrom the table it can be seen that \ufb01ne-tuning is\nsurprisingly robust to different masking strategies.\nHowever, as expected, using only the MASK strat-\negy was problematic when applying the feature-\nbased approach to NER. Interestingly, using only\nthe RND strategy performs much worse than our\nstrategy as well.\n",
    "2310.02239": "MINIGPT-5: INTERLEAVED VISION-AND-LANGUAGE\nGENERATION VIA GENERATIVE VOKENS\nKaizhi Zheng\u2217, Xuehai He\u2217, and Xin Eric Wang\nUniversity of California, Santa Cruz\nhttps://github.com/eric-ai-lab/MiniGPT-5\nABSTRACT\nThe effectiveness of Multimodal Large Language Models (MLLMs) demonstrates\na profound capability in multimodal understanding. However, the simultaneous\ngeneration of images with coherent texts is still underdeveloped.\nAddressing\nthis, we introduce a novel interleaved vision-and-language generation method,\ncentered around the concept of \u201cgenerative vokens\u201d. These vokens serve as piv-\notal elements contributing to coherent image-text outputs. Our method is marked\nby a unique two-stage training strategy for description-free multimodal genera-\ntion, which does not necessitate extensive descriptions of images. We integrate\nclassifier-free guidance to enhance the alignment of generated images and texts,\nensuring more seamless and contextually relevant multimodal interactions. Our\nmodel, MiniGPT-5, exhibits substantial improvement over the baseline models on\nmultimodal generation datasets, including MMDialog and VIST. The human eval-\nuation shows MiniGPT-5 is better than the baseline model on more than 56% cases\nfor multimodal generation, highlighting its efficacy across diverse benchmarks.\n1\nINTRODUCTION\nThe development of large-scale vision-and-language models is significantly impacting a wide range\nof fields like automated dialogue systems and digital content creation. With the surge in research and\ndevelopment in this domain, the current state-of-the-art Large Language Models (LLMs) (OpenAI,\n2023; Chiang et al., 2023; Ouyang et al., 2022) and vision-and-language models such as (Wu et al.,\n2023a; Li et al., 2023c; Tsimpoukelli et al., 2021; Alayrac et al., 2022) fall short in generating\ncoherent multimodal outputs. This limitation becomes particularly evident in tasks that demand an\nintegrated handling of vision and language, essential for the next generation Large Language Models\n(LLMs).\nOur work, as illustrated in Fig. 1, seeks to address these shortcomings by enhancing the integration\nof text and image generation in LLMs. The challenges in developing a multimodal LLM capable\nof interleaved vision and language generation are manifold. First, LLMs typically lack mechanisms\nto directly produce images, prompting us to introduce \u201cgenerative vokens\u201d that bridge the gap be-\ntween textual and visual feature spaces. Second, the constraint of data scarcity, especially in vision-\nand-language tasks (Sharma et al., 2018) lacking extensive detailed descriptions of images (Huang\net al., 2016), is countered by our unique description-free training approach. Third, maintaining both\nimage-text and image-image consistency poses a significant challenge, which we address through\ndual-loss strategies. Finally, as we push forward the boundaries with LLMs, the large memory re-\nquirements urge us to devise more efficient end-to-end strategies and create an efficient training\npipeline accessible for the community, especially in downstream tasks.\nSpecifically, to overcome these challenges, we present MiniGPT-5, a novel approach for interleaved\nvision-and-language generation. By combing the Stable Diffusion with LLMs through special vi-\nsual tokens (Tan & Bansal, 2020) \u2013 \u201cgenerative vokens\u201d, we develop a new approach for multimodal\ngeneration. Our two-stage training methodology emphasizes a description-free foundational phase,\n\u2217These authors contributed equally to this work.\n1\narXiv:2310.02239v3  [cs.CV]  15 Mar 2024\nenabling effective model training even with limited caption-grounded images. This strategy, dis-\ntinct from existing works, pivots on generic stages free from image annotations. To ensure that the\ngenerated text and images are in harmony, our dual-loss strategy comes into play, further enhanced\nby our innovative generative voken approach and classifier-free guidance. Our parameter-efficient\nfine-tuning strategy optimizes training efficiency and addresses memory constraints.\nAs shown in Fig. 2, leveraging ViT (Vision Transformer) and Qformer (Li et al., 2023c), alongside\nLarge Language Models, we adapt multimodal inputs into generative vokens, seamlessly combined\nwith the high-resolution Stable Diffusion 2.1 model (Rombach et al., 2022b) for context-aware im-\nage generation. Incorporating images as auxiliary input with instruction tuning approaches and\npioneering both the text and image generation loss, we amplify the synergy between text and visu-\nals. We experiment on the CC3M (Sharma et al., 2018), VIST (Huang et al., 2016), and MMDi-\nalog (Feng et al., 2022) datasets. Notably, MiniGPT-5 shows superior performance across the two\nmultimodal generation datasets.\nIn summary, our contributions are primarily threefold:\n\u2022 We introduce a novel framework that leverages \u201cgenerative vokens\u201d to unify LLMs with\nStable Diffusion, facilitating interleaved vision-and-language generation without relying\non detailed image descriptions. We bridge the modality gap and improve the generation\nquality by using the loss of the latent diffusion model, the text generation loss, and the\ncaption alignment loss together during training.\n\u2022 We propose a new two-stage training strategy for description-free multimodal generation.\nThe first stage focuses on extracting high-quality text-aligned visual features from large\ntext-image pairs, while the second stage ensures optimal coordination between visual and\ntextual prompts during generation. The inclusion of classifier-free guidance during training\nenhances the overall generation quality.\n\u2022 MiniGPT-5 achieves significant improvements over baseline methods on interleaved vision-\nand-language datasets, including VIST and MMDialog, and comparable results to the state-\nof-the-art on the single text-image pair dataset, CC3M. The human evaluation further shows\nthat, compared with the two-stage baseline, MiniGPT-5 can provide better generation in\nperspectives of appropriate texts (55%), high-quality images (53%), and coherent multi-\nmodal outputs (56%).\n2\nRELATED WORK\nLarge Language Models As Large Language Models (LLMs) become increasingly impactful and\naccessible, a growing body of research has emerged to extend these pretrained LLMs into the realm\nof multimodal comprehension tasks (Zhu et al., 2023; Li et al., 2023c; Dai et al., 2023; OpenAI,\n2023; Li et al., 2023a; Alayrac et al., 2022; Li et al., 2023b). For example, to reproduce the impres-\nsive multimodal comprehension ability in GPT-4 (OpenAI, 2023), MiniGPT-4 (Zhu et al., 2023)\nproposes a projection layer to align pretrained vision component of BLIP-2 (Li et al., 2023c) with\nan advanced open-source large language model, Vicuna (Chiang et al., 2023). In our work, we utilize\nthe MiniGPT-4 as the base model and extend the model\u2019s capabilities to multimodal generation.\nText-to-Image Generation To transform textual descriptions into their corresponding visual repre-\nsentations, text-to-image models (Reed et al., 2016; Dhariwal & Nichol, 2021; Saharia et al., 2022;\nRombach et al., 2022b;a; Gu et al., 2023; Nichol et al., 2021; Ramesh et al., 2021; Yu et al., 2022;\nChang et al., 2023) employ complex architectures and sophisticated algorithms, bridging the gap be-\ntween textual information and visual content. These models are adept at interpreting the semantics\nof input text and translating them into coherent and pertinent images. A notable recent contribution\nin this field is Stable Diffusion V2 (Rombach et al., 2022b), which employs a diffusion process to\ngenerate conditional image features and subsequently reconstructs images from these features. Our\nresearch aims to leverage this pretrained model, enhancing its capabilities to accommodate both\nmultimodal input and output.\nMultimodal Generation with Large Language Models To augment the LLM\u2019s capabilities in\nseamlessly integrating vision and language generation, recent studies have introduced a variety of\n2\nMy sister arrived\nearly to help me\nwith the family\nbar bq.\nEvery one else\narrived soon\nafter.\nWhat should\u00a0\nhappen then?\nEveryone was\nhungry so we got\na lot of food.\nWe didn't realize\nthat there was\nmore to be done\nand everyone\nhad their roles.\nWe were glad\nwhen it was over\nand relaxed a\nlittle bit.\nMiniGPT-5\nMultimodal Input\nMultimodal Output\nFigure 1: MiniGPT-5 is a unified model for interleaved vision-and-language comprehension and\ngeneration. Besides the original multimodal comprehension and text generation abilities, MiniGPT-\n5 can provide appropriate, coherent multimodal outputs.\ninnovative methods (Ge et al., 2023; Sun et al., 2021; Koh et al., 2023; Sun et al., 2023b; Yu et al.,\n2023; Aiello et al., 2023; Wu et al., 2023c). For instance, CM3Leon (Yu et al., 2023) presents a\nretrieval-augmented, decoder-only architecture designed for both text-to-image and image-to-text\napplications. Similarly, Emu (Sun et al., 2023b) employs the pretrained EVA-CLIP (Sun et al.,\n2023a) model to convert images into one-dimensional features and fine-tunes the LLAMA (Touvron\net al., 2023) model to generate cohesive text and image features through autoregressive techniques.\nOn the other hand, NextGPT (Wu et al., 2023c), GILL (Koh et al., 2023) and SEED (Ge et al., 2023)\nexplore the concept of mapping vokens into the text feature space of a pretrained Stable Diffusion\nmodel; GILL and NextGPT employ an encoder-decoder framework, while SEED utilizes a trainable\nQ-Former structure. In contrast to these approaches, our model takes a more direct route by aligning\nvoken features with visual information. Additionally, we introduce several training strategies to\nenhance image quality and contextual coherence.\n3\nMETHOD\nIn order to endow Large Language Models with multimodal generation capabilities, we introduce\na new framework that integrates pretrained multimodal Large Language Models and text-to-image\ngeneration models. Central to our approach is the introduction of \u201cgenerative vokens\u201d, special visual\ntokens that effectively bridge the textual and visual domains during the training process. Addition-\nally, we implement a two-stage training method combined with a classifier-free guidance strategy to\nenhance the quality and coherence of generated outputs. Fig. 2 provides an overview of our model\nstructure. MiniGPT-5 primarily consists of two modules: the Integrated Vision-Language Encod-\ning Module, utilizing the pretrained multimodal large language model (MiniGPT-4) for handling\nmultimodal inputs, and the Multimodal Output Generation module, employing Stable Diffusion for\ngenerating visual outputs.\n3.1\nMULTIMODAL UNDERSTANDING MODULE\nRecent advancements in multimodal Large Language Models, such as MiniGPT-4 (Zhu et al., 2023),\nhave primarily concentrated on multimodal comprehension, enabling the processing of images as\nsequential input. The Integrated Vision-Language Encoding Module is designed to extend the capa-\nbilities of LLMs from mere comprehension to active generation in multimodal contexts. Generative\n3\n\"A discus got\nstuck up on the\nroof.\"\nImage\nEncoder\nLLM (Vicuna)\n\"Why not try getting it down with a soccer\nball? [IMG 1] ... [IMG n]\"\nOutput Hidden State\nLearnable Queries\nLinear Layer\nVoken Features\nTransformer\nDecoder\nTransformer\nEncoder\nFeature Mapper\nSD\nUnet\nSD Image\nEncoder\nZ\nNoise\nZt\nEstimated\nNoise\nVoken\nPositioning\nLoss\nVoken\nAlignment\nLoss\nGT Output Text\nMultimodal Input\nGT Output Image\nPEFT\nText\nTokenizer\nFigure 2: The overview structure of MiniGPT-5 pipeline. We leverage the pretrained multimodal\nlarge language model (MiniGPT-4) and text-to-image generation model (Stable Diffusion 2.1) to\ncreate a unified multimodal generation pipeline. The input image encoder includes a ViT, Qformer,\nand linear layer, pretrained by MiniGPT-4. The orange blocks include learnable parameters, while\nthe blue blocks are fixed during training. More details can be found in Section 3.\nvokens play a crucial role in this module, enabling the translation of raw visual inputs into a format\nthat LLMs can process and utilize for subsequent generation tasks.\nMultimodal Encoding Each text token is embedded into a vector etext \u2208Rd, while the pretrained\nvisual encoder transforms each input image into the feature eimg \u2208R32\u00d7d. These embeddings are\nconcatenated to create the input prompt features.\nGenerative Vokens Since the original LLM\u2019s V vocabulary only includes the textual tokens, we\nneed to construct a bridge between the LLM and the generative model. Therefore, we introduce a set\nof special tokens Vimg = {[IMG1], [IMG2], . . . , [IMGn]} (by default n = 8) as generative vokens\ninto the LLM\u2019s vocabulary V . The LLM\u2019s output hidden state for these vokens is harnessed for\nsubsequent image generation, and the positions of these vokens can represent the insertion of the in-\nterleaved images. With all pretrained weights \u03b8pretrained in MiniGPT-4 fixed, the trainable parameters\ninclude extra input embedding \u03b8voken input and output embedding \u03b8voken output.\nParameter-Efficient Fine-Tuning (PEFT) Parameter-efficient fine-tuning (PEFT) (Houlsby et al.,\n2019; Hu et al., 2021; Li & Liang, 2021) is critical in training Large Language Models (LLMs),\nemployed to adapt LLMs to downstream tasks without the need for extensive retraining. In PEFT,\nrather than updating all the parameters of a model, only a small subset of parameters is trained.\nThis subset typically includes task-specific components or lightweight layers added to the original\nmodel architecture (Zhang et al., 2021; Houlsby et al., 2019; Hu et al., 2021; Dettmers et al., 2023).\nWe apply PEFT to the MiniGPT-4 (Zhu et al., 2023) encoder, enhancing its ability to process and\ngenerate multimodal content based on given instructions or prompts. More specifically, this involves\nthe use of prefix tuning (Li & Liang, 2021) and LoRAHu et al. (2021) over the entire language\nencoder \u2013 Vicuna (Chiang et al., 2023) used in MiniGPT-4. Additionally, we implement learnable\nqueries at the input of the transformer decoder, a conventional approach in sequence-to-sequence\ntransformer architectures, to further improve the model\u2019s multimodal generation capabilities. We\nalso adopted learnable queries at the input of the transformer decoder as a conventional setting for\nsequence-to-sequence transformer architectures (Vaswani et al., 2017a). Learnable queries in the\ndecoder allow the model to have dynamic, adaptable representations for initiating the generation\nprocess. This is particularly useful when the model needs to generate outputs based on a mix of\nvisual and textual inputs. Combined with the instruction tuning (Ouyang et al., 2022), it notably\namplifies multimodal generation performance across various datasets.\n4\n3.2\nMUTIMODAL GENERATION MODULE\nTo accurately align the generative vokens with the text-to-image generation models, we formulate a\ncompact mapping module for dimension matching and incorporate several supervised losses, includ-\ning voken positioning loss and voken alignment loss. The voken positioning loss assists the model\nin learning the correct positioning of tokens, while the voken alignment loss directly aligns the vo-\nkens with the appropriate conditional generation features of the diffusion model. Since the gradients\nof generative vokens\u2019 features can be directly calculated from images, shown on the right side of\nFig. 2, our method does not need comprehensive descriptions of images, leading to description-free\nlearning.\nVoken Positioning We first jointly generate both text and vokens in the text space by follow-\ning next-word prediction in autoregressive language model (Vaswani et al., 2017b). During the\ntraining, we append the vokens Vimg to the positions of ground truth images and train the model\nto predict vokens within text generation.\nSpecifically, the generated tokens are represented as\nW = {w1, w2, . . . , wm}, where wi \u2208V \u222aVimg, and the causal language modeling loss is defined\nas:\nLtext := \u2212\nm\nX\ni=1\nlogp(wi|etext, eimg, w1, . . . , wi\u22121;\n\u03b8pretrained, \u03b8voken input, \u03b8voken output),\n(1)\nwhere wi \u2208V \u222aVimg\nVoken Alignment for Image Generation Next, we align the output hidden state hvoken, shown\nin Fig. 2, with the conditional feature space of the text-to-image generation model. To map the\nvoken feature hvoken to a feasible image generation conditional feature etext encoder \u2208RL\u00d7 \u02c6d (where\nL is the maximum input length of text-to-image generation text encoder, and \u02c6d is the dimension of\nencoder output feature in text-to-image generation model). We construct a feature mapper module,\nincluding a two-layer MLP model \u03b8MLP, a four-layer encoder-decoder transformer model \u03b8enc-dec,\nand a learnable decoder feature sequence q. The mapping feature \u02c6hvoken is then given by:\n\u02c6hvoken := \u03b8enc-dec(\u03b8MLP(hvoken), q) \u2208RL\u00d7 \u02c6d\n(2)\nTo generate appropriate images, the mapping feature \u02c6hvoken is used as a conditional input in the\ndenoising process. Intuitively, \u02c6hvoken should represent the corresponding conditional features that\nconduct the diffusion model to generate the ground truth image. We employ the latent diffusion\nmodel (LDM) loss as voken alignment loss for training the image generation module. During the\ntraining, the ground truth image is first converted to latent feature z0 through the pretrained VAE\n(Variational Autoencoder) (Kingma & Welling, 2013). Then, we obtain the noisy latent feature zt\nby adding noise \u03f5 to z0. A pretrained U-Net model \u03f5\u03b8 is used to calculate the conditional LDM loss\nas:\nLLDM := E\u03f5\u223cN (0,1),t\n\u0014\r\r\r\u03f5 \u2212\u03f5\u03b8\n\u0010\nzt, t, \u02c6hvoken\n\u0011\r\r\r\n2\n2\n\u0015\n(3)\nTo summarize, the voken positioning loss enables the model to learn the accurate placement of\ntokens. Without this component, the model lacks the essential capability to predict when vokens\nshould be generated during inference. Additionally, the voken alignment loss ensures the direct\ncorrespondence between vokens and the appropriate conditional generation characteristics of the\ndiffusion model. In the absence of this loss, the model is unable to learn semantic vokens from\nimages directly. This comprehensive approach ensures a coherent understanding and generation\nof both textual and visual elements, leveraging the capabilities of pretrained models, specialized\ntokens, and innovative training techniques.\n3.3\nTRAINING STRATEGY\nGiven the non-negligible domain shift between text and image domains, we observe that direct\ntraining on a limited interleaved text-and-image dataset can result in misaligning generated texts\n5\nand images and diminished image quality. Consequently, we adopt a two-stage training strategy: an\ninitial pretraining stage focusing on coarse feature alignment for unimodal generation, followed by a\nfine-tuning stage dedicated to intricate feature learning for multimodal generation. Furthermore, to\namplify the effectiveness of the generative tokens throughout the diffusion process, we incorporate\nthe idea of classifier-free guidance (Ho & Salimans, 2022) technique through the whole training\nprocess.\nTwo-stage Training Strategy Recognizing the non-trivial domain shift between pure-text gener-\nation and text-image generation, we propose a two-stage training strategy: Pretraining Stage and\nFine-tuning Stage. Initially, we align the voken feature with image generation features in single\ntext-image pair datasets, such as CC3M, where each data sample only contains one text and one im-\nage, and the text is usually the caption of the image. During this stage, we utilize captions as LLM\ninput, enabling LLM to generate vokens. Since these datasets include the image descriptive infor-\nmation, we also introduce an auxiliary loss to aid voken alignment, minimizing the distance between\nthe generative feature \u02c6hvoken and the caption feature from the text encoder \u03c4\u03b8 in the text-to-image\ngeneration model:\nLCAP := MSE(\u02c6hvoken, \u03c4\u03b8(c))\n(4)\nThe pretraining stage loss is expressed as LPretrain = \u03bb1\u2217Ltext+\u03bb2\u2217LLDM+\u03bb3\u2217LCAP, with selected\nvalues \u03bb1 = 0.01, \u03bb2 = 1, \u03bb3 = 0.1 to rescale the loss into a similar numerical range.\nAfter the pretraining stage, the model is capable of generating images for single text descriptions\nbut struggles with interleaved vision-and-language generation, which includes multiple text-image\npairs and requires complicated reasoning for both text and image generation. To address this, in\nthe fine-tuning stage, we further fine-tune our model with PEFT parameters by interleaved vision-\nand-language datasets, such as VIST, where the data sample has several steps with text-image and\ntexts are sequentially relevant. During this stage, we construct three types of tasks from the dataset,\nencompassing (1) text-only generation: given the next image, generating the related text; (2) image-\nonly generation: given the next text, generating the related image, and (3) multimodal generation:\ngenerating text-image pair by given context. The fine-tuning stage loss is given by LFine-tune =\n\u03bb1 \u2217Ltext + \u03bb2 \u2217LLDM. More implementation details can be found in Appendix A.\nClassifier-Free Guidance (CFG) To enhance the coherence between the generated text and im-\nages, we first leverage the idea of Classifier-free Guidance for multimodal generation. Classifier-\nfree guidance is introduced in the text-to-image diffusion process. This method observes that the\ngeneration model P\u03b8 can achieve improved conditional results by training on both conditional and\nunconditional generation with conditioning dropout. In our context, we want the model to focus\ndirectly on the output features hvoken from LLM. Instead of using original stable diffusion uncondi-\ntional distributions (dropping \u02c6hvoken), the whole feature mapper also needs to be included during the\nunconditional process. Therefore, our objective is to accentuate the trainable condition hvoken and\nthe generation model is fixed. During training, we replace hvoken with zero features h0 \u22080n\u00d7d with\na 10% probability, obtaining the unconditional feature \u02c6h0 = \u03b8enc-dec(\u03b8MLP(h0), q). During inference,\n\u02c6h0 serves as negative prompting, and the refined denoising process is:\nlog c\nP\u03b8\n\u0010\n\u03f5t | zt+1, \u02c6hvoken, \u02c6h0\n\u0011\n= log P\u03b8\n\u0010\n\u03f5t | zt+1, \u02c6h0\n\u0011\n+\n\u03b3\n\u0010\nlog P\u03b8\n\u0010\n\u03f5t | zt+1, \u02c6hvoken\n\u0011\n\u2212log P\u03b8\n\u0010\n\u03f5t | zt+1, \u02c6h0\n\u0011\u0011\n(5)\n4\nEXPERIMENTS\nTo assess the efficacy of our model, we conducted a series of evaluations across multiple bench-\nmarks. These experiments aim to address several key questions: (1) Can our model generate plausi-\nble images and reasonable texts? (2) How does our model compare with state-of-the-art models in\nboth single-turn and multi-turn interleaved vision-and-language generation tasks? (3) What impact\ndoes the design of each module have on overall performance? Below we will discuss the experimen-\ntal setup and present a comprehensive analysis of our model\u2019s performance. We use three datasets:\nCC3M (Sharma et al., 2018), VIST (Huang et al., 2016), and MMDialog (Feng et al., 2022). More\ndetails about datasets and data format can be found in Appendix B.\n6\nTable 1: Image generation on VIST. Given the\nhistorical context, models need to generate im-\nages for each step. FID scores evaluate the vi-\nsual diversities between generated and ground\ntruth images within each story sequence.\nModel\nCLIP-I (\u2191)\nFID (\u2193)\nSD 2.1 (Rombach et al., 2022b)\n0.59\n393.49\nFine-tuned SD 2.1\n0.61\n390.25\nTwo-stage Baseline\n0.57\n403.06\nGILL (Koh et al., 2023)\n0.60\n381.88\nMiniGPT-5 (Prefix Tuning)\n0.65\n381.55\nMiniGPT-5 (LoRA)\n0.66\n366.62\nTable 2: Narration Generation on VIST. We\nadded LoRA fine-tuning for GILL, MiniGPT-\n4, and MiniGPT-5 with the same LoRA config-\nuration. The results show that adding genera-\ntive vokens does not hurt the performance on the\nmultimodal comprehension tasks.\nModel\nS-BERT (\u2191)\nRouge-L (\u2191)\nMeteor (\u2191)\nGILL (Koh et al., 2023)\n0.3864\n0.1784\n0.1951\nMiniGPT-4 (Zhu et al., 2023)\n0.6273\n0.3401\n0.3296\nMiniGPT-5\n0.6315\n0.3373\n0.3263\n4.1\nEXPERIMENTAL SETUP\nBaselines For a comprehensive evaluation of our performance in multimodal generation, we con-\nducted comparative analyses with several prominent baseline models: the Fine-tuned Unimodal\nGeneration Models, Two-stage Baseline, GILL, and Divter.\n\u2022 Fine-tuned Unimodal Generation Models: To facilitate fair comparisons in both image\nand text generation, we fine-tuned two separate models, Stable Diffusion 2.1 and MiniGPT-\n4 (Zhu et al., 2023), utilizing the VIST dataset. Within the Stable Diffusion 2.1 (Rombach\net al., 2022b) model, the U-Net parameters were fine-tuned. For MiniGPT-4\u2019s LLM part,\nLoRA parameters were fine-tuned.\n\u2022 Two-stage Baseline: A common approach in multimodal generation involves first employ-\ning Large Language Models (LLMs) to create image captions, which are then fed into\ntext-to-image models for image generation (Wu et al., 2023b). We create such a two-stage\nbaseline for comparison with our end-to-end method by fine-tuning MiniGPT-4 for caption\ngeneration and Stable Diffusion 2.1 for text-to-image generation. Given the absence of\nimage descriptions in the VIST dataset, we incorporate a SOTA image captioning model,\nInstructBLIP-13B (Dai et al., 2023), to generate synthetic captions for supervision.\n\u2022 GILL1 (Koh et al., 2023): GILL is a recent innovation that allows the LLM to generate\nvokens using a pre-trained text-to-image generation model for single-image generation,\nwhere GILL minimizes the Mean Squared Error (MSE) loss between the text-to-image text\nencoding feature and voken features, similar to LCAP in our approach. For fine-tuning on\nmultimodal datasets, since GILL requires image captions for training, we use Descriptions\nof Images-in-Isolation (DII) (Huang et al., 2016) in the VIST fine-tuning and generate\ncaptions for MMDialog fine-tuning. Contrarily, MiniGPT-5 does not related on all caption\ndata during multimodal generation fine-tuning.\n\u2022 Divter (Sun et al., 2021): Divter is a state-of-the-art conversational agent developed for\nmultimodal dialogue contexts. It introduces a customized transformer structure for gener-\nating multimodal responses. Divter\u2019s methodology includes pretraining on a vast corpus\nof text-only dialogues and text-image pairs, followed by fine-tuning on a selected set of\nmultimodal response data. The MMDialog dataset regards Divter\u2019s method as the baseline.\nMetrics To comprehensively assess the model performance across image, text, and multimodal\ndimensions, we employ a diverse set of metrics. For evaluating the quality and diversity of generated\nimages, we utilize the Inception Score (IS) (Salimans et al., 2016), and Fr\u00b4echet Inception Distance\n(FID) (Heusel et al., 2017). Textual performance is gauged through metrics such as BLEU (Papineni\net al., 2002), Rouge-L (Lin, 2004), METEOR (Banerjee & Lavie, 2005), and Sentence-BERT (S-\nBERT) (Reimers & Gurevych, 2019) scores.\nFrom the multimodal perspective, we leverage CLIP-based metrics (Rombach et al., 2022b) to assess\nthe similarities between generated content and ground truth. CLIP-I evaluates the similarity between\n1Given the variations in the valid data within the CC3M dataset, we made adjustments to ensure fair com-\nparisons. Specifically, we retrained it on our specific CC3M data, following the guidelines in their official\nimplementation (https://github.com/kohjingyu/gill).\n7\nTable 3: Multimodal Story Generation. VIST Human Evaluation on 5,000 samples comparing\nMiniGPT-5 with both Two-stage Baseline and GILL, across Language Continuity, Image Quality,\nand Multimodal Coherence aspects. The results highlight the superiority of MiniGPT-5 in more than\nhalf cases.\nTwo-Stage Baseline\nGILL (Koh et al., 2023)\nModel\nMiniGPT-5\nBaseline\nTie\nMiniGPT-5\nBaseline\nTie\nLanguage Continuity (%)\n55.22\n34.89\n9.89\n54.18\n35.31\n10.51\nImage Quality (%)\n52.43\n37.79\n9.78\n54.25\n35.41\n10.34\nMultimodal Coherence (%)\n56.90\n28.88\n14.22\n55.32\n30.34\n14.33\ngenerated and ground-truth image features. To address potential misalignments in the multimodal\ngeneration, such as when the ground truth is text-only, but the output is multimodal, we utilize\nMM-Relevance (Feng et al., 2022). This metric calculates the F1 score based on CLIP similarities,\nproviding a nuanced evaluation of multimodal coherence.\nRecognizing that the generated multimodal output might be meaningful yet differ from the ground\ntruth, we also incorporate human evaluation to assess the model\u2019s performance. We examine the\nmodel\u2019s effectiveness from three perspectives: (1) Language Continuity: assessing if the produced\ntext aligns seamlessly with the provided context; (2) Image Quality: evaluating the clarity and rel-\nevance of the generated image; and (3) Multimodal Coherence: determining if the combined text-\nimage output is consistent with the initial context.\n4.2\nMAIN RESULTS\nIn this subsection, we present the performance of different models on the VIST (Huang et al., 2016)\nand MMDialg (Feng et al., 2022) datasets. Our evaluations span all vision, language, and multi-\nmodality domains to showcase the versatility and robustness of the proposed models.\nUnimodal Generation with Multimodal Input To evaluate the model performance on image gen-\neration and text generation, we systematically provide models with prior history context and subse-\nquently assess the generated images and narrations at each following step. Tables 1 and 2 outline\nthe results of these experiments on the VIST validation set, showing the performance in both image\nand language metrics, respectively. The findings demonstrate that MiniGPT-5 can generate coher-\nent, high-quality images utilizing long-horizontal multimodal input prompts across all data, without\ncompromising the original model\u2019s ability for multimodal comprehension, indicating the efficacy of\nour model in diverse settings.\nMultimodal Generation with Multimodal Input To assess the quality of multimodal generation,\nwe test both our model and the baselines on the VIST validation set by human evaluation. Given\na preceding multimodal sequence, models are tasked with producing the subsequent scenario for\neach task. We select a random sample of 5,000 sequences, with each requiring evaluation by two\nworkers. These evaluators are tasked with determining the superior multimodal output based on\nthree criteria: Language Continuity, Image Quality, and Multimodal Coherence. This assessment is\nfacilitated using Amazon Mechanical Turk (Crowston, 2012), with a representative example (Fig. 4)\nprovided in the Appendix. As depicted in Table 3, our model, MiniGPT-5, is found to generate\nmore fitting text narrations in around 55% of instances, deliver superior image quality in around\n53% of cases, and produce more coherent multimodal outputs in around 56% of the scenarios. This\ndata distinctly showcases its enhanced multimodal generation capabilities compared to the two-stage\nbaseline, which must generate intermediate image captions first.\nMultimodal Dialog Generation on MMDialog We conduct an evaluation of our method on the\nMMDialog dataset to determine the effectiveness of generating precise and appropriate multimodal\ninformation in multi-turn conversational scenarios. The model is required to generate either uni-\nmodal or multimodal responses based on the previous turns during the conversations. Our results, as\npresented in Table 4, demonstrate that MiniGPT-5 outperforms the baseline model Divter in terms\nof generating more accurate textual responses. While the image qualities of the generated responses\nare similar, MiniGPT-5 excels in MM-Relevance compared to the baselines. This indicates that our\n8\nTable 4: Multimodal generation results on MMDialog test set. In order to compare with their base-\nline, we use the same metrics reported in MMDialog (Feng et al., 2022).\nModel\nIS (\u2191)\nBLEU-1 (\u2191)\nBLEU-2 (\u2191)\nRouge-L (\u2191)\nMM-Relevance (\u2191)\nDivter (Sun et al., 2021)\n20.53\n0.0944\n0.0745\n0.1119\n0.62\nGILL (Koh et al., 2023)\n23.78\n0.2912\n0.1945\n0.1207\n0.64\nMiniGPT-5\n20.23\n0.3369\n0.2323\n0.1176\n0.67\nTable 5: Evaluation of different method designs for image generation qualities on the CC3M vali-\ndation set. The results show that all of our designs can help the model better align with the stable\ndiffusion model in the pertaining stage.\nModel\nCLIP-I (\u2191)\nCLIP-T (\u2191)\nIS (\u2191)\nFID (\u2193)\nMiniGPT-5\n0.61\n0.22\n28.09\n31.47\nMiniGPT-5 (w/o CFG)\n0.60\n0.22\n23.41\n33.73\nMiniGPT-5 (w/o LCAP )\n0.54\n0.16\n21.27\n40.24\nMiniGPT-5 (w/o LLDM)\n0.58\n0.20\n24.79\n34.65\nmodel can better learn how to position image generation and produce highly coherent multimodal\nresponses appropriately.\n4.3\nABLATION STUDIES\nTo further evaluate the effectiveness of our design, we conducted several ablation studies, and more\nablation studies can be found in Appendix C.\nText-to-Image Generation Qualities on CC3M\nEvaluation of Classifier-Free Guidance (CFG) To assess the effectiveness of the CFG strategy,\nwe trained our model without CFG dropoff. During inference, the model utilized the original CFG\ndenoising process, which utilized the empty caption feature from Stable Diffusion\u2019s text encoder as\nnegative prompt features. The results in Table 5 demonstrate that all metrics are worse without CFG,\nindicating that the CFG training strategy improves the image generation quality.\nEvaluation of Different Loss Guidance As described in Sec. 3.3, we introduced an auxiliary loss,\ndenoted as LCAP for CC3M training. To assess the impact of this loss and determine if the single\ncaption loss alone can generate high-quality images like GILL, we trained our model without the\ncaption loss LCAP (alignment between the mapped generative voken features and the caption fea-\ntures from stable diffusion text encoder) and the conditional latent diffusion loss LLDM (alignment\nbetween the mapped generative voken features and conditional features for latent diffusion process\nof ground truth images) separately. The results, as shown in Table 5, indicate that the caption loss\nsignificantly aids in generating better images, and the voken alignment loss further enhances coher-\nence and image quality performance.\nInfluence of Input Types for Image Generation To assess the impact of various types of input\ndata for image generation, models are tasked with generating the final-step images based on specific\nprompts and comparing them with ground truth images by CLIP-I metric. All models are fine-tuned\non data with full multimodal context and tested on various input types. As indicated in Table 6,\nthe MiniGPT-5 model exhibits exceptional proficiency in producing semantically precise images\ncompared to other models. Furthermore, we observed increased CLIP similarities when more in-\nformation was provided in the input, signifying the models\u2019 enhanced ability to process diverse,\nlong-horizon multimodal inputs.\nInstead of multimodal input, we also test single text-to-image generation qualities on the CC3M\nvalidation set, as displayed in Table 7. The results indicate that although our model can have better\ngeneration on multi-turn multimodal scenarios, Stable Diffusion 2 achieves the best outcomes across\n9\nI went to the\nnatural history\nmuseum today.\ntheir evolution display\nwas interesting.\nThey had many\ninteresting things on\ndisplay.\nMiniGPT-5\nTwo-Stage\nIt's a great place to\nspend some time in.\nMy favorite was this real\ncovered wagon from\n200 years ago.\nGT\nfirst, we went\nto the park.\nthen, we went\nswimming\ni looked coool\nin my glasses\nat the pool\nlater, we went\nto visit mommy\nwe played\ndress up.\nGILL\nSD 2\nTwo-stage\nMiniGPT-5\nGT\nID of perfect\nthough art:\ncomplementarity,\ncomplicity,\nsimplicity, security.\nNo, it's not a loan. It was found\nin the tomb of an 8th century\nMaya king and his wife at Tika!\nLoan, private\ncollection. Adding\ncolor to gallery!\nLovely depiction of\ntextiles, gestures.\nMMDialog -- Multimodal Dialog Generation\nMiniGPT-5\nGT\nVSIT -- Multimodal Generation\nVSIT -- Image Generation\nThey had an area for\ncryptozoology.\nThey also have a gift\nshop.\nI bought a book\nabout the history of\nthe museum\nGILL\nA Maya example,\ncouple coming\nsoon to 8th-\ncentury king and\nwife, lady from\nTikal, dancing.\nHow gracefull\ncoming? From a\nloan?\nGILL\nYes, from a loan.\nFigure 3: Qualitative examples from MiniGPT-5 and baselines on the VIST and MMDialog datasets.\nThe orange blocks indicate the input prompts, while the green blocks include model outputs. The\ncomparisons show that MiniGPT-5 can produce coherent and high-quality multimodal output. We\nwould like to emphasize that MiniGPT-5 does not use any caption data during fine-tuning on VIST\nand MMDialog, which obeys to our description-free settings. More qualitative examples can be\nfound in the Appendix D.\nall metrics for pure text-to-image generation. Since our model attempts to align with the pretrained\ntext encoder of Stable Diffusion 2 in this stage, there is a slight gap in performance due to the\nlimitation of data amount. Compared with the observations on the VIST dataset, we can conclude\nthat MiniGPT-5 is better at extracting features from long-horizontal multimodal information instead\n10\nTable 6: Influence of prompts for image generation on CLIP-I metrics on VIST. We establish four\ndistinct conditions for the final-step image generation: \u2018No Context\u2019 (solely the last step\u2019s narration),\n\u2018Text Context\u2019 (inclusive of historical textual narrations), \u2018Image Context\u2019 (inclusive of historical\nimages), and \u2018Image-Text Context\u2019 (inclusive of both historical images and narrations). From the\nresults, MiniGPT-5 can generate more coherent images.\nModel\nNo Context\nText Context\nImage Context\nImage-Text Context\nSD 2.1 (Rombach et al., 2022b)\n0.57\n0.59\n-\n-\nFine-tuned SD 2.1\n0.59\n0.61\n-\n-\nTwo-stage Baseline\n0.54\n0.56\n0.57\n0.58\nGILL (Koh et al., 2023)\n0.56\n0.59\n0.60\n0.60\nMiniGPT-5 (Prefix Tuning)\n0.60\n0.63\n0.68\n0.70\nMiniGPT-5 (LoRA)\n0.61\n0.64\n0.69\n0.70\nTable 7: Generation Qualities on CC3M and VIST. We find that MiniGPT-5 is better at extracting\nfeatures from long-horizontal multimodal information than single text input.\nCC3M\nVIST\nModel\nCLIP-I (\u2191)\nFID (\u2193)\nCLIP-I (\u2191)\nFID (\u2193)\nStable Diffusion 2.1 (Rombach et al., 2022b)\n0.64\n26.39\n0.59\n393.49\nGILL (Koh et al., 2023)\n0.57\n36.85\n0.60\n381.88\nMiniGPT-5\n0.61\n31.47\n0.66\n366.62\nof single text input. This indicates potential future directions on efficiently aligning LLMs with\ngenerative models. On the other hand, our model outperforms another state-of-the-art multimodal\ngeneration model, GILL, on all metrics, further validating the effectiveness of our design.\n5\nCONCLUSION\nIn this paper, we introduce MiniGPT-5, designed to augment the capabilities of LLMs for multi-\nmodal generation by aligning the LLM with a pretrained text-to-image generation model. Our ap-\nproach demonstrates substantial improvements, as evidenced by comprehensive experiments. There\nare still some limitations of MiniGPT-5. For example, we still find the object texture hard to main-\ntain in the new generation, and the generated image quality still has space to improve. Through\nthis work, we aspire to set a new benchmark for multimodal generative models, opening doors to\napplications previously deemed challenging due to the disjointed nature of existing image and text\nsynthesis paradigms.\nREFERENCES\nEmanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large\nautoregressive multimodal models. arXiv preprint arXiv:2309.15564, 2023.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural Information Processing Systems, 35:23716\u2013\n23736, 2022.\nSatanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved\ncorrelation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic\nevaluation measures for machine translation and/or summarization, pp. 65\u201372, 2005.\nHuiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan\nYang, Kevin Murphy, William T Freeman, Michael Rubinstein, et al. Muse: Text-to-image gen-\neration via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023.\n11\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng,\nSiyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An\nopen-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023.\nKevin Crowston. Amazon mechanical turk: A research tool for organizations and information sys-\ntems scholars. In Shaping the Future of ICT Research. Methods and Approaches: IFIP WG\n8.2, Working Conference, Tampa, FL, USA, December 13-14, 2012. Proceedings, pp. 210\u2013221.\nSpringer, 2012.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language\nmodels with instruction tuning, 2023.\nTim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. Qlora: Efficient finetuning\nof quantized llms. arXiv preprint arXiv:2305.14314, 2023.\nPrafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u20138794, 2021.\nJiazhan Feng, Qingfeng Sun, Can Xu, Pu Zhao, Yaming Yang, Chongyang Tao, Dongyan Zhao,\nand Qingwei Lin. Mmdialog: A large-scale multi-turn dialogue dataset towards multi-modal\nopen-domain conversation. arXiv preprint arXiv:2211.05719, 2022.\nYuying Ge, Yixiao Ge, Ziyun Zeng, Xintao Wang, and Ying Shan. Planting a seed of vision in large\nlanguage model. arXiv preprint arXiv:2307.08041, 2023.\nJing Gu, Yilin Wang, Nanxuan Zhao, Tsu-Jui Fu, Wei Xiong, Qing Liu, Zhifei Zhang, He Zhang,\nJianming Zhang, HyunJoon Jung, and Xin Eric Wang. Photoswap: Personalized subject swapping\nin images, 2023.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in\nneural information processing systems, 30, 2017.\nJonathan Ho and Tim Salimans.\nClassifier-free diffusion guidance.\narXiv preprint\narXiv:2207.12598, 2022.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp.\nIn International Conference on Machine Learning, pp. 2790\u20132799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen.\nLora: Low-rank adaptation of large language models.\narXiv preprint\narXiv:2106.09685, 2021.\nTing-Hao K. Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Jacob Devlin, Aishwarya\nAgrawal, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling.\nIn 15th Annual Conference of the North American Chapter of the Association for Computational\nLinguistics (NAACL 2016), 2016.\nDiederik P Kingma and Max Welling.\nAuto-encoding variational bayes.\narXiv preprint\narXiv:1312.6114, 2013.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216, 2023.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Jingkang Yang, and Ziwei Liu. Otter: A\nmulti-modal model with in-context instruction tuning. arXiv preprint arXiv:2305.03726, 2023a.\nDongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven C.H. Hoi. LAVIS:\nA one-stop library for language-vision intelligence. In Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 31\u2013\n41, Toronto, Canada, July 2023b. Association for Computational Linguistics. URL https:\n//aclanthology.org/2023.acl-demo.3.\n12\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\nBlip-2:\nBootstrapping language-\nimage pre-training with frozen image encoders and large language models.\narXiv preprint\narXiv:2301.12597, 2023c.\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. In Text summarization\nbranches out, pp. 74\u201381, 2004.\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew,\nIlya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with\ntext-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\nOpenAI. Gpt-4 technical report, 2023.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong\nZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow\ninstructions with human feedback.\nAdvances in Neural Information Processing Systems, 35:\n27730\u201327744, 2022.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: a method for automatic\nevaluation of machine translation. In Proceedings of the 40th annual meeting on association for\ncomputational linguistics, pp. 311\u2013318. Association for Computational Linguistics, 2002.\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen,\nand Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine\nLearning, pp. 8821\u20138831. PMLR, 2021.\nScott Reed, Zeynep Akata, Xinchen Yan, Lajanugen Logeswaran, Bernt Schiele, and Honglak Lee.\nGenerative adversarial text to image synthesis. In International conference on machine learning,\npp. 1060\u20131069. PMLR, 2016.\nNils Reimers and Iryna Gurevych.\nSentence-bert: Sentence embeddings using siamese bert-\nnetworks. arXiv preprint arXiv:1908.10084, 2019.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF confer-\nence on computer vision and pattern recognition, pp. 10684\u201310695, 2022a.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00a8orn Ommer. High-\nresolution image synthesis with latent diffusion models. In CVPR, 2022b.\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar\nGhasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic\ntext-to-image diffusion models with deep language understanding. Advances in Neural Informa-\ntion Processing Systems, 35:36479\u201336494, 2022.\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.\nImproved techniques for training gans. Advances in neural information processing systems, 29,\n2016.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp.\n2556\u20132565, 2018.\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng, Yaming Yang, Huang Hu, Fei Xu, Jessica\nZhang, Xiubo Geng, and Daxin Jiang. Multimodal dialogue response generation. arXiv preprint\narXiv:2110.08515, 2021.\nQuan Sun, Yuxin Fang, Ledell Wu, Xinlong Wang, and Yue Cao. Eva-clip: Improved training\ntechniques for clip at scale. arXiv preprint arXiv:2303.15389, 2023a.\n13\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang, Xiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. 2023b.\nHao Tan and Mohit Bansal. Vokenization: Improving language understanding with contextualized,\nvisual-grounded supervision. arXiv preprint arXiv:2010.06775, 2020.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00b4ee\nLacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and\nefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi, SM Eslami, Oriol Vinyals, and Felix Hill. Mul-\ntimodal few-shot learning with frozen language models. Advances in Neural Information Pro-\ncessing Systems, 34:200\u2013212, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, 30, 2017a.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\n\u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural informa-\ntion processing systems, pp. 5998\u20136008, 2017b.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-\nsual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint\narXiv:2303.04671, 2023a.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong Wang, Zecheng Tang, and Nan Duan. Vi-\nsual chatgpt: Talking, drawing and editing with visual foundation models.\narXiv preprint\narXiv:2303.04671, 2023b.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. Next-gpt: Any-to-any multi-\nmodal llm, 2023c.\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan,\nAlexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-\nrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022.\nLili Yu, Bowen Shi, Ramakanth Pasunuru, Benjamin Muller, Olga Golovneva, Tianlu Wang, Arun\nBabu, Binh Tang, Brian Karrer, Shelly Sheynin, et al. Scaling autoregressive multi-modal models:\nPretraining and instruction tuning. arXiv preprint arXiv:2309.02591, 2023.\nRenrui Zhang, Rongyao Fang, Peng Gao, Wei Zhang, Kunchang Li, Jifeng Dai, Yu Qiao, and Hong-\nsheng Li. Tip-adapter: Training-free clip-adapter for better vision-language modeling. arXiv\npreprint arXiv:2111.03930, 2021.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny.\nMinigpt-4: En-\nhancing vision-language understanding with advanced large language models. arXiv preprint\narXiv:2304.10592, 2023.\n14\nA\nIMPLEMENTATION DETAILS\nIn the pretraining stage, we introduce additional voken embeddings at both the input and out-\nput layers of the Vicuna-7B model, while keeping the embeddings of other tokens fixed. These\nnew embeddings \u2013 denoted as \u03b8voken input and \u03b8voken output \u2013 along with the feature mapper module\n(\u03b8MLP, \u03b8enc dec, q) are jointly trained on the CC3M dataset, which consists of single text-image pairs.\nTraining is conducted using the AdamW optimizer over two epochs, with a batch size of 48, amount-\ning to over 110,000 steps, and a learning rate of 2 \u00d7 10\u22124.\nIn the subsequent fine-tuning stage, we incorporate LoRA modules \u2013 denoted as \u03b8LoRA \u2013 into Vicuna\nfor the generation of both tokens and vokens. We keep the MLP model \u03b8MLP and decoder query q\nfixed. The model is then fine-tuned on interleaved vision-and-language datasets, like VIST and\nMMDialog. The trainable parameters for this stage are \u03b8 = {\u03b8voken input, \u03b8voken output, \u03b8LoRA, \u03b8enc dec}.\nTraining is carried out using the AdamW optimizer over four epochs, with a batch size of 32 and\na learning rate of 2 \u00d7 10\u22125. Trainable parameters are nearly 6.6 million, and all training can be\ncompleted on a server equipped with 4 A6000 GPUs.\nB\nEXPERIMENTAL SETTINGS\nB.1\nDATASETS\nCC3M (Sharma et al., 2018): Conceptual Captions (CC3M) dataset represents a remarkable col-\nlection of high-quality image captions, amassing approximately 3.3 million pairs of text and images\nfrom the internet. The CC3M dataset\u2019s diverse content, quality assurance, and support for multi-\nmodal learning make it a valuable asset for researchers and AI enthusiasts. Each dataset sample\nconsists of an image accompanied by a corresponding text description, reflecting the richness of\nhuman language and visual perception. However, after accounting for license restrictions and elim-\ninating invalid image links, the dataset comprises approximately 2.2 million data pairs suitable for\ntraining purposes and 10 thousand data pairs designated for validation.\nVIST (Huang et al., 2016): Visual Storytelling (VIST) dataset is an innovative compilation of\nvisual narratives. The VIST dataset\u2019s engaging content, narrative structure, and emphasis on se-\nquential understanding position it as an essential resource for researchers focusing on sequential\nimage understanding. Each sequence within this dataset consists of five images accompanied by\ncorresponding textual narratives, showcasing the intricate interplay between visual imagery and sto-\nrytelling. Designed to foster creativity and challenge conventional image-captioning models, the\ndataset provides a platform for training and validating algorithms capable of generating coherent\nand contextually relevant stories. After eliminating the invalid image links, we got over 65 thou-\nsand unique photos organized into more than 34 thousand storytelling sequences for training and 4\nthousand sequences with 8 thousand images for validation.\nMMDialog (Feng et al., 2022): Multi-Modal Dialogue (MMDialog) dataset stands as the largest\ncollection of multimodal conversation dialogues. The MMDialog dataset\u2019s extensive scale, real\nhuman-human chat content, and emphasis on multimodal open-domain conversations position it\nas an unparalleled asset for researchers and practitioners in artificial intelligence. Each dialogue\nwithin this dataset typically includes 2.59 images, integrated anywhere within the conversation,\nshowcasing the complex interplay between text and visual elements. Designed to mirror real-world\nconversational dynamics, the dataset is a robust platform for developing, training, and validating\nalgorithms capable of understanding and generating coherent dialogues that seamlessly blend textual\nand visual information.\nB.2\nDATA FORMAT\nPretraining Stage In the pretraining stage, we aim to synchronize the generative voken with the\ntext-to-image model\u2019s conditional feature, focusing on single-turn text-image pairs. To achieve this,\nwe utilize data from the CC3M dataset, constructing training samples by appending vokens as image\nplaceholders after the captions, such as \u201ca big black dog [IMG1] ... [IMGn].\u201d The Language Model\n(LLM) is then tasked with only generating these placeholders for text creation, and the correspond-\n15\ning output hidden features are further employed to compute the conditional generation loss with the\nground truth image.\nFine-tuning Stage In this stage, we utilize the VIST and MMDialog datasets, which contain\nmulti-turn multimodal data. During training, we integrate placeholders for input images, such as\n\u2019<Img><ImageHere></Img>\u2019, into the input text prompts when applicable. These prompts also\nencompass various instructions corresponding to different task types, with outputs manifesting as\npure-text, pure-voken, or text-voken combinations. Below, we present example templates in the\nVIST dataset to illustrate the different task types:\n\u2022 Text Generation: Input: \u201c<History Context> What happens in the next scene image:\n<Img><ImageHere></Img>\u201d; Output: \u201c<Text Description>\u201d\n\u2022 Image Generation: Input: \u201c<History Context> Generate an image with the scene de-\nscription: [Text Description]\u201d; Output: \u201c[IMG1]...[IMGn]\u201d\n\u2022 Text-Image Generation: Input: \u201c<History Context> What should happen then?\u201d; Output:\n\u201c<Text Description> [IMG1]...[IMGn]\u201d\nBy structuring the input and output in this manner, we create a flexible framework that accommo-\ndates various multimodal tasks, enhancing the model\u2019s ability to interpret and generate textual and\nvisual content. The history context in the VIST dataset includes all previous story steps with texts\nand images. In the MMDialog dataset, due to the limitation of computational resources, we only use\nup to one previous turn as the history context, and all data are formatted into the dialog.\nC\nMORE EXPERIMENTS\nC.1\nEVALUATION OF GUIDANCE SCALE\nSince our model incorporates CFG, evaluating how different guidance scales affect image generation\nis crucial. Therefore, we plotted several line charts in Fig 5 to depict the changes in metrics with\nvarying guidance scales. The figures reveal that the stable diffusion model and our model generate\nbetter images as the guidance scale increases. However, when the scale exceeds 10, the image\nsemantic coherence stabilizes while the image quality declines. This suggests that the guidance\nscale should be set within a reasonable range for optimal image generation.\nC.2\nEVALUATION OF VOKEN NUMBER\nThe voken features in our model are directly utilized as conditions in the text-to-image model,\nleading to the expectation that an increase in the number of vokens would enhance the model\u2019s\nrepresentative capabilities. To validate this hypothesis, we experimented by training the model with\nvarying numbers of vokens, ranging from 1 to 8. As illustrated in Fig 6, the model\u2019s performance\nconsistently improves with adding more vokens. This improvement is particularly noticeable when\nthe number of vokens is increased from 1 to 4, highlighting the significant role that vokens play in\nenhancing the model\u2019s effectiveness.\nC.3\nABLATION OF MODEL DESIGNS\nThis section explores alternatives to the transformer encoder/decoder architecture discussed in the\nmain paper.\nSpecifically, we experimented with two additional settings: Fixed Queries, and\nDecoder-Only model where learnable queries are fed into the transformer decoder. For the fixed\nqueries design, we initialize queries the same as learnable queries experiments in the main paper\nand keep them fixed during training. In the decoder-only approach, we utilize solely the transformer\ndecoder and apply padding to the decoder\u2019s output, ensuring that the token length reaches 77. This\nlength adjustment allows the output to be compatible with the Stable Diffusion encoder. The results\nof these experiments are detailed in Table 8. From the results of MiniGPT-5 with fixed queries, we\nfind there exists a slight trade-off between image-text coherence and image qualities, where fixed\nqueries can lead to higher image metrics (IS and FID) but lower CLIP similarities. Meanwhile,\nMiniGPT-5 consistently outperforms the Decoder-Only results in all four evaluation metrics, vali-\ndating the robustness and efficacy of MiniGPT-5\u2019s transformer encoder/decoder architecture design.\n16\nFigure 4: Screenshot for human evaluation interface on the Amazon Mechanical Turk crowdsource\nevaluation platform. Output 1 is generated by MiniGPT-5, while output 2 is generated by the two-\nstage baseline.\nModel\nCLIP-I (\u2191)\nCLIP-T (\u2191)\nIS (\u2191)\nFID (\u2193)\nMiniGPT-5\n0.61\n0.22\n28.09\n31.47\nMiniGPT-5 (Fixed Queries)\n0.60\n0.21\n28.55\n30.56\nMiniGPT-5 (Decoder-Only)\n0.58\n0.20\n24.74\n34.88\nTable 8: Evaluation of different model designs for image generation qualities on the CC3M valida-\ntion set.\nD\nMORE QUALITATIVE EXAMPLES\nIn this section, we provide additional qualitative examples to further demonstrate the capabilities of\nMiniGPT-5. Figures 7,8,9, and 10 showcase these examples across various datasets and settings.\nFigure 7 presents a comparative analysis on the VIST validation set, illustrating how MiniGPT-5\noutperforms baseline models in terms of image generation quality and alignment with multimodal\ninputs. The examples highlight the superiority of MiniGPT-5 in generating images that closely\nmatch the given text prompts.\n17\n(a) FID vs CFG Scale\n(b) IS vs CFG Scale\n(c) CLIP-T vs CFG Scale\n(d) CLIP-I vs CFG Scale\nFigure 5: Line charts for various metrics vs Classifier-free Guidance (CFG) scale on CC3M. The\nresults suggest that our CFG strategy can exhibit comparable effectiveness to the CFG strategy\nemployed in SD2, with the appropriate CFG scale significantly enhancing both image quality and\ncoherence.\n(a) FID vs nvoken\n(b) IS vs nvoken\n(c) CLIP-T vs nvoken\n(d) CLIP-I vs nvoken\nFigure 6: Line charts for various metrics vs the number of vokens on CC3M. As the number of\nvokens increases, the image quality and CLIP scores improve. In this work, our default voken\nnumber is 8.\nIn Figure 8, we focus on the performance of MiniGPT-5 in free multimodal generation scenarios.\nThe results clearly indicate an improvement over the Two-Stage baseline, emphasizing MiniGPT-5\u2019s\nability to perform consistent and creative multimodal generation.\n18\nFigure 9 showcases the application of MiniGPT-5 in the context of the MMDialog test set. Here, the\nemphasis is on free multimodal dialog generation, with MiniGPT-5 displaying a decent performance\nin generating coherent and contextually relevant multimodal dialogues.\nLastly, Figure 10 highlights MiniGPT-5\u2019s performance in single text-to-image generation tasks on\nthe CC3M validation set. The examples underline the model\u2019s proficiency in generating visually\naccurate and contextually appropriate images from textual descriptions, surpassing the performance\nof baseline models.\nEach figure includes a clear depiction of input prompts (indicated in orange blocks) and the corre-\nsponding model outputs (in green blocks), providing a comprehensive view of MiniGPT-5\u2019s capa-\nbilities across different multimodal generation tasks.\n19\ni had a photo\nsession with my\nfavorite doll.\nshe is so\nphilosophical\nsometimes.\nthe cat likes\nher too, they\nwere having a\ngood time.\nthe cat really likes\nher, he even gave\nher a kiss !\nshe finished\nthe session\nposing with\nher guitar,\nshe's such a\ngood\nmusician.\nGILL\nSD 2\nTwo-stage\nMiniGPT-5\nGT\nwe didn't expect\nsuch beauty\noutdoors.\nthe bridge was\nall i thought it\nwould be.\nthe view of the\nwater were\namazing.\nthe bridge was\nbreath taking.\nwe all agreed\nthe food was\nfantastic.\nGILL\nSD 2\nTwo-stage\nMiniGPT-5\nGT\ni took my wife out\nfor our anniversary\ndinner.\nour first course\nwas a light but\ndelicious salad.\nfollowing our\nsalad we had\nsquash bisque\nfor our main course\nwe had a beautifully\nplated salmon.\nto end our\nwonderful\nnight we had a\nparfait for\ndessert.\nGILL\nSD 2\nTwo-stage\nMiniGPT-5\nGT\nFigure 7: Comparative examples from MiniGPT-5 and baselines on the VIST validation set for\nimage generation with multimodal input. Orange blocks denote input prompts, while green blocks\nshow model outputs.\n20\nOn our class trip we all\nwore our school uniforms\nI got to read many\ndifferent book that I\nhad never read before.\nI really enjoyed being\non this trip\nMiniGPT-5\nTwo-Stage\nThis book was about\nanimals and it had lots of\npictures too.\nOne of my friends\nread us a story from\none of the books.\nGT\nAll of the kids were so\nexcited to read new\nbooks\nThe first book I read\nhad lots of cool\npictures in it.\nI really enjoyed\nreading the book.\nGILL\nCelebrating with all of\nour friends\nHer best friend even\ncame.\nEven my dad got in\non the act.\nMiniGPT-5\nTwo-Stage\nEven the guy behind us\nwas great and fun to be\naround.\nThe happy couple\nenjoying their\nengagement.\nGT\nThis is the crew right\nhere.\nEveryone was in\ngreate spirits\nWe had a great\ntime.\nGILL\nWe got to the town hall\nmeeting early and there\nwere a lot of people.\nHere's us watching\nthe introduction.\nEveryone had a lot of\nquestions and the\nmeeting was very long\nMiniGPT-5\nTwo-Stage\nWe all gathered to\ndiscuss the program\nMe and tanner taking\na selfie together after\nthe meeting.\nGT\nMe bondrit during\nintermission.\nJacob and his son.\nThe meeting was very\ninformative and we\nlearned a lot.\nGILL\nFigure 8: More qualitative examples from MiniGPT-5 and baselines on VIST validation set for free\nmultimodal generation.\n21\nWhat I find so funny\nis everyone has a\nstrong opinion of me\nand no one realises\nI'm actually a soppy,\nover dramatic bugger\nthat :growing_heart:\nHarry Potter\nI've read all the books at least 10\ntimes each!\nHarry Potter\nHaha he has the full box set\nand home and at his Nanna's\n:) he even tries to head butt\nhis lamp like dobby\n:face_with_tears_of_joy:\n:see-no-evil_monkey:\nMiniGPT-5\nGT\nYou would get on with\nmy 3 year old then he\nis obsessed with\nHarry potter haha\nSo cute!! I'm just\nabout to get into bed\nand finish off the\nGoblet of Fire for the\nmillionth time!\nHaha I know what you\nmean! I'm just about to\nfinish the last Harry Potter\nbook! I'm so excited for\nthe next one!\nGILL\nIt the final FlashbackFridayz of 2019\nand we are looking back with a theme\nof TravelFaves2019. Tag and retweet\nyour hosts and guest hosts; Share\nyous and tag you friends.\nTravelfaves2019 ours is the\ngorgeous waterfall in Costa Rica\nLuxurious views!\u00a0 Throwback to\nour trip to New Orleans last\nJanuary where we stopped by the\nTabasco Factory in Avery Island\nMiniGPT-5\nGT\nTravelfaves2019 we have seen\nquite a number of gorgeous\nAfrica\nOur travelfaves2019\nwhat's yours\nThe Greate Wall of China\nGILL\nFigure 9: More qualitative examples from MiniGPT-5 on MMDialog test set for free multimodal\ndialog generation.\n22\nMiniGPT-5\nSD 2\nGILL\nGT\nwomens hands\nsprinkle a dough\nwith flour close up\nMiniGPT-5\nSD 2\nGILL\nGT\nsunflowers have a\ndeep sentimental\nmeaning for me\nMiniGPT-5\nSD 2\nGILL\nGT\nwe all know\nsuperman , comic\nbook characters ,\nbut history is full of\nless impressive\nheroes\nMiniGPT-5\n\u00a0SD 2\nGILL\nGT\nboy looking in the\nencyclopedia\nthrough a\nmagnifying glass\nMiniGPT-5\nSD 2\nGILL\nGT\nhappy young\nbusinessman with a\nfolder running up a\ndrawn stairs along a\nconcrete wall\nFigure 10: More qualitative examples from MiniGPT-5 and baselines on CC3M validation set for\nsingle text-to-image generation.\n23\n",
    "2310.08949": "EasyGen: Easing Multimodal Generation with BiDiffuser and LLMs\nXiangyu Zhao, Bo Liu*, Qijiong Liu*, Guangyuan Shi*, Xiao-Ming Wu\f\nDepartment of Computing, The Hong Kong Polytechnic University\n{xiang-yu.zhao, bokelvin.liu, jyonn.liu, guang-yuan.shi}@connect.polyu.hk,\nxiao-ming.wu@polyu.edu.hk\nAbstract\nWe present EasyGen, an efficient model de-\nsigned to enhance multimodal understanding\nand generation by harnessing the capabilities\nof diffusion models and large language models\n(LLMs). Unlike existing multimodal models\nthat predominately depend on encoders like\nCLIP or ImageBind and need ample amounts\nof training data to bridge modalities, Easy-\nGen leverages BiDiffuser, a bidirectional condi-\ntional diffusion model, to foster more efficient\nmodality interactions. EasyGen achieves text\ngeneration by training a projection layer linking\nBiDiffuser and an LLM, and facilities image\ngeneration by training an adapter to align the\nLLM\u2019s text space with the BiDiffuser\u2019s image\nspace. Comprehensive quantitative and quali-\ntative experiments show that EasyGen excels\nin data-efficient training, high-quality image\ngeneration, and extendibility, effectively ad-\ndressing the challenges in multimodal gener-\nation. The source code is available at https:\n//github.com/zxy556677/EasyGen.\n1\nIntroduction\nIn recent years, remarkable progress has been made\nin the field of artificial intelligence generated con-\ntent (AIGC), notably in technologies like large lan-\nguage models (LLMs) (Chiang et al., 2023; Tou-\nvron et al., 2023; Brown et al., 2020; Chowdhery\net al., 2022; Zeng et al., 2022) for text generation\nand diffusion models (Rombach et al., 2022; Nichol\net al., 2022; Saharia et al., 2022) for visual gener-\nation. These breakthroughs have paved the way\nfor the development of multimodal large language\nmodels (MLLMs), sparking a recent trend of incor-\nporating extra visual modules into LLMs. Collabo-\nrative models, such as Visual ChatGPT (Wu et al.,\n2023a) and MM-REACT (Yang et al., 2023), strate-\ngically use externally pre-trained tools to translate\nvisual information into text descriptions and feed\n*Co-Second Author.\nthe data into LLMs. However, they are exclusively\ndependent on pre-trained tools for inference. Con-\ntrarily, end-to-end trained models including the\nBLIP series (Li et al., 2023b), LLaVA series (Liu\net al., 2023b,a), MiniGPT-4 (Zhu et al., 2023), and\nmPLUG-Owl (Ye et al., 2023) focus on mapping\nimage information to the text space of LLMs, en-\nabling LLMs to comprehend visual inputs.\nExisting end-to-end models are also not without\nlimitations. First, most of these multimodal models\nrely on either CLIP (Radford et al., 2021) or Image-\nBind (Girdhar et al., 2023) as their image encoder.\nWhile these encoders excel in learning unified rep-\nresentations that encompass both text and images,\nthey face challenges when it comes to transform-\ning between different modalities. This predica-\nment makes current vision-language models rely-\ning heavily on sizable data sets to align CLIP/Bind-\nencoded images with the language model, due to\nthe disparity between different modalities.\nMoreover, the majority of previous multimodal\nmodels have concentrated on comprehending multi-\nmodal content and lacked the capability to generate\nmultimodal responses, such as content beyond text.\nSeveral concurrent works, such as Emu (Sun et al.,\n2023) and NExT-GPT (Wu et al., 2023), have uti-\nlized diffusion models for multimodal generation.\nTypically, these methods involve training a projec-\ntion layer to align the output embedding space of\nthe LLM with the input embedding space of the\ndiffusion model (encoded by CLIP\u2019s text encoder)\nusing an MSE loss. However, this approach may\nlead to the underutilization of the semantic under-\nstanding and reasoning capabilities of the LLM,\nand may introduce information loss in the align-\nment process, ultimately leading to lower image\ngeneration quality compared to the original diffu-\nsion model, as elaborated in Sec. 5.6 and Tab. 6.\nIn this work, we propose EasyGen, an end-to-\nend model that facilitates multimodal generation\nwith a single bidirectional conditional diffusion\n1\narXiv:2310.08949v3  [cs.AI]  17 May 2024\nCould you please provide an introduction \nto this model in the picture?\nWhat other types of green energy \nirrigation systems are feasible?\nI\u2019m really intrigued by wind-powered \nirrigation systems. Could you please \nprovide a brief introduction and show \nme some pictures related to it?\nThe image features a solar irrigation system \nworking model that is displayed on a table. \nIt is made of various materials, such as \nplastic, and features various components, \nincluding solar panels, water tanks, a pump, \npiping, valves, hoses, faucets, plants, soil, \ndirt, pebbles, bottles, cups, spoons, straws, \nscrews and nuts, among others.\nThere are several other types of \ngreen energy irrigation systems apart \nfrom solar power. Some of them \ninclude wind-powered irrigation \nsystem, hydro-powered irrigation \nsystem, tidal-powered irrigation \nsystem \u2026\nA wind-powered irrigation system is a \ntype of green energy irrigation system \nthat utilizes wind energy to power the \nirrigation process. It involves the use of \nwind turbines, typically mounted on \ntowers, which capture the kinetic \nenergy of the wind and convert it into \nelectrical energy. The generated \nelectricity can be stored in batteries \u2026 \nHere are some possible pictures:\nFigure 1: Our model EasyGen can understand multimodal inputs and generate multimodal responses, as illustrated\nby model-generated speech bubbles in grey color, which include both text and images.\nmodel and LLMs, as illustrated in Figure 2. The\ndiffusion model, called BiDiffuser, is obtained by\nfine-tuning the UniDiffuser (Bao et al., 2023b),\nwith a specific focus on targeted image-to-text and\ntext-to-image tasks. This fine-tuning addresses Uni-\nDiffuser\u2019s limitation of attempting to fit all condi-\ntional distributions, including those based on noisy\ninputs, into a single model, which reduces its ef-\nfectiveness on specific tasks like conditional gen-\neration from noise-free inputs. BiDiffuser plays a\npivotal role for both text and image generation. In\nEasyGen, text generation is achieved by training\na projection layer that connects BiDiffuser and an\nLLM, while image generation is facilitated by train-\ning an adapter that infuses the text representation\nof the LLM into BiDiffuser. Figure 1 showcases\nEasyGen\u2019s ability to handle multimodal inputs and\ngenerate appropriate multimodal responses.\nEasyGen holds three significant advantages that\naddress the challenges in multimodal generation:\nFirst, EasyGen offers competitive performance\nin a data-efficient way compared to cutting-edge\nmodels, as shown in Tab. 3 (Sec. 5.5). This is due\nto BiDiffuser\u2019s ability to simplify the alignment of\nits embedding space with an LLM, which allows\nfor efficient training with less data for image-to-text\ntasks such as image captioning and VQA.\nSecond, EasyGen exhibits superior image gener-\nation quality, surpassing other end-to-end MLLMs,\nas shown in Tab. 6 (Sec. 5.6). This is attributed\nto the adapter\u2019s design (Sec. 4.2), which aligns the\nLLM\u2019s text space with the diffusion model\u2019s image\nspace, thereby utilizing the LLM\u2019s semantic under-\nstanding and reasoning capabilities. In contrast, the\nprojection layers in other MLLMs like NExT-GPT\nonly align the LLM\u2019s text space with the diffusion\nmodel\u2019s text space and are not trained by the image\ndenoising objective.\n   BiDiffuser\nProjection Layer\nText\nLLM\nEmbedding Layer\n   BiDiffuser\nLM-Head\ncaption\nText Response\nAdapter\nImage\nImage-to-text\nText-to-image\nInstruction\nTuning\nFigure 2: Overview of EasyGen.\nThird, EasyGen can be readily adapted to man-\nage complex vision-language tasks by incorpo-\nrating more advanced visual encoders or by inte-\ngrating BiDiffuser into contemporary sophisticated\nmultimodal LLMs like LLaVA to enhance perfor-\nmance, as shown in Tab. 5 (Sec. 5.7).\n2\nRelated Work\nMultimodal Language Models. Recent research\nhas witnessed a surge of interest in multimodal\nLLMs, including collaborative models (Wu et al.,\n2023a; Yang et al., 2023; Shen et al., 2023) and end-\nto-end methods (Alayrac et al., 2022; Guo et al.,\n2022; Li et al., 2022; Bao et al., 2021; Wang et al.,\n2022b,a,a). More recently, some works also ex-\nplore training LLMs with parameter-efficient tun-\ning (Li et al., 2023b; Zhang et al., 2023a) and in-\nstruction tuning (Dai et al., 2023; Liu et al., 2023b;\nYe et al., 2023; Zhu et al., 2023; Li et al., 2023a).\nDifferent from them, EasyGen is built upon BiDif-\nfuser, which promotes more efficient interactions\nbetween modalities.\nMultimodal Diffusion Models. Diffusion genera-\ntive models (Rombach et al., 2022; Ramesh et al.,\n2021; Nichol et al., 2022; Ruiz et al., 2023) have\nachieved strong results in text conditioned image\n2\ngeneration works. Specifically, Versatile Diffu-\nsion (Xu et al., 2023) employs the U-Net (Ron-\nneberger et al., 2015) architecture with a multi-\nflow design to tackle multiple modalities and tasks,\nwhile UniDiffuser (Bao et al., 2023b) adopts the\nU-ViT (Bao et al., 2023a) framework to treat both\nimage and text as sequential token streams for dif-\nfusion calculations. However, these models are\nunable to complete complex language tasks. Easy-\nGen combines the advantages of diffusion models\nand LLMs and achieves competitive performance\nin both image-to-text and text-to-image tasks.\nMultimodal Response Generation. Recent re-\nsearch has made significant advancements in multi-\nmodal response generation (Koh et al., 2023b; Tang\net al., 2023; Zhang et al., 2023b; Wu et al., 2023b;\nPan et al., 2023; Koh et al., 2023a; Sun et al., 2023;\nDong et al., 2023) using text-to-image models such\nas Stable Diffusion. However, the lack of semantic\nunderstanding capability in the CLIP text encoder\nmay result in low-quality generated images. Easy-\nGen addresses this issue by transferring knowledge\nfrom LLM to BiDiffuser via an adapter, enabling\nthe creation of high-quality textual semantic repre-\nsentations for text-to-image generation.\n3\nBasics of Diffusion Models\nUnconditional Generation. Given a data sample\ntaken from a real data distribution x0 \u223cq(x0),\ndiffusion models (Sohl-Dickstein et al., 2015; Ho\net al., 2020) first destruct the data by constructing\na Markov forward process and gradually injecting\nnoise to the data:\nq(x1:T |x0) =\nT\nY\nt=1\nq(xt|xt\u22121),\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI),\n(1)\nwhere \u03b2t \u2208(0, 1) is the variance added at diffusion\nstep t. Then, they learn to reverse the process:\np(x0:T ) = p(xT )\nT\nY\nt=1\np\u03b8(xt\u22121|xt),\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5t(xt, t), \u03c32\nt I),\n(2)\nwhere p(xT ) = N(xT ; 0, I) is the standard Gaus-\nsian distribution and \u00b5t(\u00b7) is the parameterization\nof the predicted mean. Diffusion models are trained\nto maximize the marginal likelihood of the data\nE[log p\u03b8(x0)], and the canonical objective is the\nvariational lower bound of log p\u03b8(x0). Denoising\ndiffusion probabilistic models (Ho et al., 2020) gen-\nerate samples xt \u223cq(xt|x0) by injecting noise\n\u03f5 \u223cN(0, I) to the data x0, and train a network\n\u03f5\u03b8(\u00b7) to predict the added noise \u03f5 using a standard\nmean squared error loss:\nL := Ex0,\u03f5,t[\u2225\u03f5 \u2212\u03f5\u03b8(xt, t)\u22252].\n(3)\nConditional Generation.\nFor conditional gen-\neration, a paired data (x0, y0) is given, and the\naim is to model the conditional data distribution\nq(x0|y0), where y0 can be image class or text\nprompt. Conditional generation includes classi-\nfier guidance (Dhariwal and Nichol, 2021) and\nclassifier-free guidance (Ho and Salimans, 2021).\nClassifier guidance requires training an extra clas-\nsifier on noisy data at inference time to improve\nsample quality. For classifier-free guidance, no\nclassifier needs to be trained. The denosing net-\nwork \u03f5\u03b8(xt|y0) simply conditions on the informa-\ntion encoded in y0. At inference time, with a guid-\nance scale s, the modified score estimate is further\nin the direction of \u03f5\u03b8(xt|y0) and away from the\nunconditional model \u03f5\u03b8(xt|\u2205) (\u2205is a null token):\n\u02c6\u03f5\u03b8(xt|y0) = \u03f5\u03b8(xt|\u2205) + s \u00b7 (\u03f5\u03b8(xt|y0) \u2212\u03f5\u03b8(xt|\u2205)).\n4\nProposed Model: EasyGen\nWe propose EasyGen, a model capable of process-\ning multimodal inputs and generating multimodal\noutputs. It achieves easy multimodal generation\nby leveraging a bidirectional conditional diffusion\nmodel to effectively bridge the gap between differ-\nent modalities and an LLM to comprehend mul-\ntimodal tasks and produce textual responses con-\ntaining cues for multimodal message creation. In\nthe subsequent section, we outline the multimodal\ngeneration process of EasyGen.\n4.1\nPre-training BiDiffuser: A Bidirectional\nConditional Diffusion Model\nSince the text space of LLMs is discrete, to min-\nimize the disparity between the output of a dif-\nfusion model and the input of LLMs, we lever-\nage Unidiffuser, a unified diffusion model capa-\nble of transforming images into the discrete text\nspace. During the training process, UniDiffuser\ninjects noise \u03f5x and \u03f5y to a set of paired image-text\ndata (x0, y0) and generates noisy data xtx and yty,\nwhere 0 \u2a7dtx, ty \u2a7dT represent two individual\ntimesteps (perturbation levels). It then trains a joint\ndenoising transformer U-ViT (Bao et al., 2023a)\n3\n\u2026\nCLIP Text\nLLM  \ntwo people \nstanding on top of \nsnowy mountain\nwith a ski pole\nAdapter\nAuto\nEncoder\n<textual noise>\nT\n0\n0\nT\nLever Tillman \nFromlet Fromlet \nFromlet ... \n\u2026\ntwo people standing on \ntop of snowy mountain \nwith a ski pole\nU-ViT\nU-ViT\n<visual noise>\nImage-to-Text\nText-to-Image\nX\nX\nY\nY\nFigure 3: The training of BiDiffuser involves finetuning\nthe denoising transformer U-ViT in UniDiffuser with a\njoint objective of image-to-text and text-to-image tasks.\n\u03f5\u03b8(xtx, yty, tx, ty) to predict the noise \u03f5x and \u03f5y\nby minimizing the mean squared error loss:\nE\u03f5x,\u03f5y,x0,y0[\u2225[\u03f5x, \u03f5y] \u2212\u03f5\u03b8(xtx, yty, tx, ty)\u22252],\nwhere the output of \u03f5\u03b8 is the concatenation of the\nestimated noise \u03f5x\n\u03b8 and \u03f5y\n\u03b8, i.e., \u03f5\u03b8 = [\u03f5x\n\u03b8, \u03f5y\n\u03b8].\nBy predicting \u03f5\u03b8(xtx, yty, tx, ty) for any tx and\nty, UniDiffuser learns all distributions related to\n(x0, y0) simultaneously. This includes all condi-\ntional distributions: q(x0|y0) for text-to-image\ngeneration, q(y0|x0) for image-to-text genera-\ntion, and those conditioned on noisy input, i.e.,\nq(x0|yty) and q(y0|xtx), for 0 < tx, ty \u2264T.\nLearning a conditional distribution q(x0|yty) or\nq(y0|xtx) can be seen as learning a distinct task.\nFrom a multitask learning perspective, due to lim-\nited network capacity, learning many tasks simul-\ntaneously (i.e., fitting all distributions to a single\nnetwork) may result in task competition or task con-\nflict, ultimately leading to suboptimal performance\nin particular tasks such as q(x0|y0) and q(y0|x0).\nTo resolve this issue and enhance the perfor-\nmance of both image-to-text and text-to-image gen-\neration tasks, we finetune UniDiffuser with exclu-\nsive emphasis on the two tasks:\nLd = E\u03f5x,\u03f5y,x0,y0[\u2225\u03f5x \u2212\u03f5x\n\u03b8(xtx, y0, tx, 0)\u22252+\n\u03b1\u2225\u03f5y \u2212\u03f5y\n\u03b8(x0, yty, 0, ty)\u22252].\nwhere \u03b1 is a hyperparameter to balance the learning\npaces of the two tasks. As depicted in Figure 3,\nour training objective entails predicting the text\ny0 based on the input image x0 and vice versa,\nwhere the input conditions for the model are noise-\nfree. We name the finetuned model \u201cBiDiffuser\u201d,\nsignifying its specialized ability in bidirectional\nconditional generation.\n4.2\nPre-training an Adapter to Enhance\nBiDiffuser\u2019s SUR Capability\nBiDiffuser uses the text encoder of CLIP, which is\ntrained with image-text contrastive learning, lim-\niting its semantic understanding and reasoning\n(SUR) ability for image generation. Drawing inspi-\nration from Zhong et al. (2023), we utilize LLMs to\nenhance the SUR capability of BiDiffuser. Specif-\nically, we design an adapter that employs the at-\ntention mechanism to integrate the semantic in-\nformation from LLM\u2019s last hidden state fLLM(\u00b7)\ninto the CLIP text encoder fCLIP(\u00b7). The adapter\nconsists of a projection layer MLP(\u00b7) and a cross-\nattention layer Att(\u00b7). Given a paired image-text\ndata (x0, y0), we can get ysur with enhanced SUR\nvia the adapter:\nysur = Att(fCLIP(y0)W Q, MLP(fLLM(y0))W K,\nMLP(fLLM(y0))W V ).\nThen, the semantic input to BiDiffuser is the com-\nbination of ysur and the CLIP text encoding of y0:\ny0 = \u03bb \u00b7 ysur + (1 \u2212\u03bb) \u00b7 fCLIP(y0),\n(4)\nwhere \u03bb is a balancing parameter. We train the\nadapter by freezing BiDiffuser and minimizing\nLada = E\u03f5y,x0[\u2225\u03f5x \u2212\u03f5x\n\u03b8(xtx, y0, tx)\u22252],\n(5)\nwhere \u03f5x\n\u03b8 is not updated as BiDiffuser is frozen.\n4.3\nImage-to-Text Generation\nBiDiffuser can convert images into vectors in the\ntext space, facilitating alignment with the vector\nspace of LLMs. In the following, we show how\nBiDiffuser can be integrated with LLMs to per-\nform image-to-text generation tasks such as image\ncaptioning and visual question answering (VQA).\n4.3.1\nAligning BiDiffuser with LLMs\nWe connect BiDiffuser and LLMs via a simple\nprojection layer, which maps text embeddings ob-\ntained from the output of the diffusion model to the\nembedding space of LLMs. As shown in Figure 4,\nthe alignment can take place either prior to the\nLLM (Pre-Align manner) or between its encoder\nand decoder components (Mid-Align manner).\nPre-Align Manner. As shown in Figure 4a, the\nprojection layer is placed before the LLM to map\nthe output of BiDiffuser (image representations)\nto the text embedding space of the LLM. The text\nembedding of the input image is then concatenated\n4\nTwo people standing on the top of a snowy mountain holding on to a ski pole.\nDecode\n<Textual Noise>\n<Image>\nBiDi\ufb00user\nLLM\nProjection Layer\nText Embedding\nText Embedding\nText Embedding\n### Human: \n<Query> ### Assistant: \nDescribe the image concisely.\n(a) Pre-Align manner.\nTwo people standing on the top of a snowy mountain holding on to a ski pole.\nDecode\n<Textual Noise>\n<Image>\nBiDi\ufb00user\nLLM Decoder\nProjection Layer\nText Embedding\nText Embedding\nText Embedding\n### Human: \nLLM Encoder\nLLM Encoder\n<Query> ### Assistant: \nDescribe the image concisely.\n(b) Mid-Align manner.\nFigure 4: Two different ways of aligning BiDiffuser with LLMs.\nwith the embeddings of the textual instructions and\nfed to the LLM for decoding. To synchronize the\ntext space of BiDiffuser with that of the LLM, we\npropose to use the image-grounded text generation\n(ITG) objective to drive the model to generate texts\nbased on the input image by computing the auto-\nregressive loss:\nLITG = \u22121\nL\nL\nX\nl=1\nlog p\u03d5(wg\nl |wg\n<l, I, TI),\n(6)\nwhere wg = (wg\n1, ..., wg\nL) represents the ground-\ntruth caption of image I with length L, TI is the\ntext instruction, and \u03d5 denotes the model parame-\nters, which include the parameters of the projection\nlayer and the LLM.\nMid-Align Manner. As shown in Figure 4b,\nthe projection layer is placed between the LLM\u2019s\nencoder and decoder, aiming to map the output of\nBiDiffuser to the embedding space of the text that\nis encoded by the LLM\u2019s encoder. Particularly, we\nargue that the output of BiDiffuser, once mapped\nby the projection layer and denoted as ddiff, should\nalign with the image caption that is encoded by\nthe LLM\u2019s encoder, denoted as dllm. Therefore, to\naccurately learn the alignment between the image\nand text representations, in addition to the ITG loss\nin Eq. 6, we also employ an image-text distance\nminimization (ITDM) loss:\nLITDM = 1\nN\nN\nX\ni=1\n\u2225ddiff \u2212dllm\u22252\n2,\nLmid = LITG + LITM.\n(7)\nwhere N is the batch size, and Lmid is the overall\nloss. In this manner, the model parameters \u03b8 only\ninclude the parameters of the projection layer.\nAfter the alignment, EasyGen gains the capabil-\nity of zero-shot image-to-text generation, including\ntasks such as image captioning and VQA.\n4.3.2\nInstruction-Tuning LLMs\nWhen aligning BiDiffuser with an LLM, we per-\nform instruction-tuning on the LLM to equip it\nwith the capability of understanding multimodal\ntasks. We designed different instructions for differ-\nent LLMs, as shown in Table 12. General instruc-\ntion template is denoted as follows:\nUSER: <Img><image></Img> + Instruction. As-\nsistant: <answer>.\nFor the <image> placeholder, we substitute it\nwith the output of BiDiffuser. To avoid over fitting\nto the specific task and counter the model\u2019s inclina-\ntion to generate excessively short outputs, we have\ndevised specific instructions (see Table 11), which\nenable the LLM to produce concise responses when\nnecessary. For different tasks, the distinct instruc-\ntion templates are as outlined in Appendix F.\n4.4\nText-to-Image Response Generation\nMost of existing multimodal models, including the\nBLIP series and LLaVA series are unable to pro-\nvide a multimodal response as they are primarily\ndesigned to generate only textual outputs. On the\nother hand, Emu (Sun et al., 2023) takes a unified\napproach to predict the subsequent visual or textual\ntoken in an auto-regressive manner, but it is heavily\nreliant on vast quantities of training data. Contrary\nto the limitations of these existing models, Easy-\nGen, by leveraging the bidirectional generation ca-\npability of BiDiffuser and the inference capability\nof LLMs, can produce accurate and high-quality\nvisual response with ease.\n5\nA: We went out to go to a \ngarage sale this morning. \nB: tat\u2019s awesome! Do \nyou play guitar?\n<img> An acoustic \nguitar with a \nheadstock on top of a \nbrick wall. </img>\nAutoKL \nDecoder\n<Dialogue Context>\nYeah, take a look. <Description>\nLLM\nBiDi\ufb00user\nText-to-Text Generation\nFigure 5: Text-to-image generation by EasyGen. LLM\ngenerates the response and description of the image.\nBiDiffuser generates images based on the description.\nTo tackle multimodal response generation tasks\nsuch as PhotoChat (Zang et al., 2021), we first\nleverage the MLLM to generate detailed image\ncaptions based on dialogue context. Then, we em-\nploy BiDiffuser to create the corresponding images\nwith the produced captions. Specifically, we re-\nplace the image featured in the dialogue with its\ncorresponding descriptive caption, encapsulating it\nwith task-specific tokens <Img>,</Img> and con-\nstructing the following instruction templates:\nUSER: Dialog history. Assistant: <response> +\n<Img><caption></Img>.\nWhen <caption> appears in response, it represents\nthe generated description of the image. So we\ncan use LLM\u2019s original auto-regressive training\nobjective. Specifically, we compute the probability\nof the target caption by:\nLt2t = \u22121\nL\nL\nX\nl=1\nlog p\u03d1(wc\nl |wc\n<l, H),\n(8)\nwhere wc = (wc\n1, ..., wc\nL) represents the caption of\nimage x0 with length L, H is the dialogue history,\nand \u03d1 denotes the LLM\u2019s parameters. Considering\nthe potential for alignment deviation in discrete\ntext alone, given the description of the image y0,\nwe utilize y0, which is the combination of the SUR\nadapter\u2019s output and the CLIP text encoder\u2019s out-\nput, as the conditional component of the diffusion\nmodel. This directly contributes to the denoising\nprocess. The loss function for the denoising pro-\ncess of a noisy image xtx is formulated in a way\nthat is similar to Eq. 5:\nLt2i = E\u03f5y,x0[\u2225\u03f5x \u2212\u03f5x\n\u03b8(xtx, y0, tx)\u22252],\n(9)\nwhere \u03f5x\n\u03b8 is not updated and we only train the pa-\nrameters of LLM and adapter. The overall loss for\ntext-to-image task is:\nLall = Lt2i + Lt2t.\n(10)\nTraining with the instruction data enables our\nmodel to not only produce text responses but also\nperform image intent classification and generate\nimage captions that BiDiffuser can interpret.\n5\nExperiments\n5.1\nExperimental Setup\nWe initialize encoder-decoder LLM from FlanT5-\nXL or decoder-only LLM from Vicuna-7B, along\nwith the utilization of the diffusion module from\nBiDiffuser. During the alignment process, we main-\ntain the frozen state of the BiDiffuser. The statis-\ntics of the datasets for pre-training, alignment and\ninstruction-tuning can be found in Appendix B. For\nthe image captioning task, EasyGen is evaluated\non both the MS-COCO (Lin et al., 2014) Karpa-\nthy test set and the NoCaps (Agrawal et al., 2019)\nvalidation set. For the VQA task, we evaluated on\nOK-VQA (Marino et al., 2019) validation set and\nGQA (Hudson and Manning, 2019) test-dev set.\nTo adapt the model for multimodal dialogue gen-\neration, we fine-tune the LLM and projection layers\non the PhotoChat dataset. We incorporate photo-\nsharing activities into the dialogue context by gen-\nerating <Img><caption></Img>, and utilize cross-\nentropy loss exclusively for fine-tuning the multi-\nmodal generation task. Given the limited expres-\nsiveness of image descriptions in the PhotoChat\ndataset, as evidenced by Figure 7 in Appendix I,\nwe regenerate image annotations in a text format\nsimilar to that used in MS-COCO.\n5.2\nEvaluation\nWe evaluate EasyGen on various vision-language\ntasks including image captioning (MS-COCO (Lin\net al., 2014), NoCaps (Agrawal et al., 2019)), vi-\nsual question answering (OK-VQA (Marino et al.,\n2019), GQA (Hudson and Manning, 2019)), and\nmultimodal dialog generation (PhotoChat (Zang\net al., 2021)).\nWe use BLIP (Li et al., 2022),\nFlamingo (Alayrac et al., 2022), BLIP-2 (Li et al.,\n2023b), InstructBlip (Dai et al., 2023), MiniGPT-\n4 (Zhu et al., 2023), and LLaVA (Liu et al.,\n2023b) as baselines for image-to-text tasks, and\nMaria (Liang et al., 2021) and Divter (Sun et al.,\n2021) as baselines for the multimodal response\ngeneration task. See details in Appendix C and E.\n5.3\nOverall Results\nTab. 1 presents the evaluation results for each base-\nline and our models on MS-COCO and VQA (zero-\n6\nModel\nDataset Size\nNoCaps (val)\nCOCO (Karpathy)\nOK-VQA\nGQA\nPT\nIT\nCIDEr\nSPICE\nBLEU@4\nCIDEr\nAccuracy\nAccuracy\nBLIP (Li et al., 2022)\n129M\n-\n113.2\n14.8\n40.4\n136.7\n-\n-\nFlamingo (Alayrac et al., 2022)\n1.8B\n-\n-\n-\n-\n138.1\n50.6\n-\nBLIP-2 OPT-6.7B (Li et al., 2023b)\n129M\n-\n121.0\n15.3\n43.5\n145.2\n36.4\n36.4\nBLIP-2 FlanT5XL (Li et al., 2023b)\n129M\n-\n121.6\n15.8\n42.4\n144.5\n39.4\n44.4\nInstructBlip 7B (Dai et al., 2023)\n129M\n1.2M\n123.1\n-\n40.8\n140.7\n61.0\u22c6\n49.2\u22c6\nMiniGPT-4 (Zhu et al., 2023)\n-\n5M\n42.4\n-\n-\n-\n37.5\n30.8\nLLaVA (Liu et al., 2023b)\n558K\n158K\n33.1\n-\n7.9\n30.0\n54.4\n41.3\nEasyGen FlanT5XL\n169K\n90K\n121.2\n15.5\n43.5\n145.7\n41.1\n37.2\nEasyGen Vicuna-7B\n169K\n90K\n121.8\n15.8\n42.4\n144.6\n45.2\n44.6\nTable 1: Evaluations of EasyGen and baselines on various image understanding tasks. PT, IT indicate sample sizes\nin the pretraining and instruction tuning stages respectively. EasyGen\u2019s results on NoCaps, OK-VQA and GQA\nwere obtained in a zero-shot setting. \u22c6denotes that the model was trained on other VQA datasets.\nModel\nResponse Generation\nImage\nBLEU-1/2\nPPL\u2193\nROUGE-L\nFID\u2193\nDivter Sun et al.\n6.5/1.7\n59.6\n5.69\n29.16\nMaria Liang et al.\n13.8/9.2\n48.7\n15.17\n-\nEasyGen FlanT5\n22.3/18.7\n13.3\n17.24\n10.30\nEasyGen Vicuan\n23.6/19.9\n11.3\n18.85\n9.72\n+ w/o adapter\n-\n-\n-\n10.16\nTable 2: Evaluation on the PhotoChat dataset.\nMLLM\nSample Size\nCosine Similarity \u2191\nMSE \u2193\nMiniGPT-4\n5M\n0.0016\n6.2031\nLLaVA v1.5\n558K\n-0.0026\n0.8433\nEmu\n2B\n0.0054\n0.4062\nEasyGen\n169K\n0.0128\n0.0338\nTable 3: Data efficiency. Avg. Cosine similarity and\nmean square error between the projected representa-\ntions and their respective captions embedded by LLM.\nshot) datasets. EasyGen outperforms most of the\nbaseline models on both the COCO test set and\nNoCaps validation set (zero-shot transfer). Despite\nbeing pre-trained on a small dataset (MS-COCO),\nEasyGen\u2019s performance on the image captioning\ngeneration task is comparable to models trained on\nlarger datasets. Additionally, on the OK-VQA and\nGQA datasets, EasyGen demonstrates improved\nperformance compared to other models of a sim-\nilar scale, achieving higher accuracy even with a\nsimple greedy search decoding method.\nIn Tab. 2, the evaluation results on the PhotoChat\ndataset are presented. Our method exhibits clear\nadvantages in terms of PPL, indicating strong per-\nformance on response generation task. Because\nof the image descriptions in the PhotoChat dataset\nare overly concise, we utilized EasyGen to regen-\nerate the image descriptions, which improved the\nperformance of our model on image generation\ncompared to other models. Additionally, with the\nadapter, EasyGen is capable of generating images\nof superior quality.\n5.4\nAblation Study\nIn Tab. 4, we examine the impact of freezing/tuning\nBiDiffuser and the LLM. It can be observed that\nfrozen Mid-Align method outperforms Pre-Align\nmethod in image captioning, which shows ITDM\nloss function is effective. However, the frozen Mid-\nAlign method exhibits inferior performance in the\nVQA task. We hypothesize that this is due to the\nintegration of mid-aligned target image features\nwith query information, and the projection layer\nis insensitive to instruction information. We con-\nduct instruction-tuning on Pre-Align T5 and Vi-\ncuna. Compared to models at the same scale, these\ninstruction-tuned models achieve superior results.\n5.5\nData Efficiency in Training\nIn Tab. 3, we examine the data efficiency of dif-\nferent image encoders for alignment with LLMs.\nEasyGen uses BiDiffuser, which maps images to\nthe text space, simplifying alignment with LLMs.\nTo assess the quality of visual representations, we\nmeasured the distance between the projected rep-\nresentations and their respective captions embed-\nded by an LLM. We randomly selected 1,000 im-\nages with their corresponding captions from the\nMSCOCO dataset.\nThe results show that our\nmodel, EasyGen, aligns significantly better with\nthe LLM compared to other CLIP-based MLLMs,\ndespite using less data for alignment. This indi-\ncates the effectiveness of our approach in achieving\nstrong alignment with LLMs.\n5.6\nImage Generation Quality\nTab. 6 evaluates the generated image\u2019s quality of\nMLLMs on MS-COCO validation set, using 30K\nrandomly selected prompts to compute the FID\nscore on generated images. To confirm the efficacy\nof our approach, we fine-tuned our method on a\n7\nLLM\nDiffusion\nModel\nAlignment\nNoCaps\nCOCO(Karpathy)\nOK-VQA\nCIDEr\nSPICE\nBLEU@4\nCIDEr\nAccuracy\nT5\nUniDiffuser\nPre-Align\n62.4\n18.0\n26.8\n90.7\n33.0\nT5\nBiDiffuser\nPre-Align\n119.1\n25.5\n42.6\n145.1\n41.1\nT5\nBiDiffuser\nMid-Align\n121.2\n25.1\n43.5\n145.7\n31.5\nT5\nBiDiffuser\nMid-Align\n121.5\n25.3\n43.6\n145.7\n36.4\nVicuna-7B\nBiDiffuser\nPre-Align\n121.8\n24.9\n42.4\n144.6\n45.2\nVicuna-7B\nBiDiffuser\nPre-Align\n119.0\n24.6\n40.3\n140.3\n42.7\nTable 4: Ablation study on image captioning and VQA tasks.\n/\ndenotes tuning/freezing the LLM.\nModel\nIT\nVQAv2 (test-dev)\nTextVQA\nMMBench (test)\nMiniGPT-4 (Zhu et al., 2023)\n5M\n-\n19.4\n23.0\nInstructBLIP Vicuna-7B (Dai et al., 2023)\n1.2M\n-\n50.1\n33.9\nLLaVA-1.5 Vicuna-7B (Liu et al., 2023a)\n665K\n78.5\n58.2\n65.2\nLLaVA-1.5 Vicuna-13B (Liu et al., 2023a)\n665K\n80.0\n61.3\n67.8\nEasyGen Vicuna-7B w/ ViT-L\n251K\n79.4\n57.9\n63.9\nLLaVA-1.5 Vicuna-7B w/ EasyGen\n665K\n80.2\n58.8\n66.1\nLLaVA-1.5 Vicuna-13B w/ EasyGen\n665K\n80.5\n61.5\n69.2\nTable 5: Evaluation of EasyGen variants and baselines on more complex VQA tasks and the latest MMBench\nbenchmark. \u201cw/ EasyGen\u201d means incorporating the core components of our model into existing models as depicted\nin Figure 6 in Appendix F. EasyGen variants rank among the top models on the leaderboard of MMBench.\nMM-Model\nFID \u2193\nDiffusion Model\nFID \u2193\nZero-Shot\nNExT-GPT\n11.28 (+0.07)\nSD\n11.21\nEmu\n11.66 (+1.73)\nSD v1.5\n9.93\nEasyGen\n9.16 (-0.55)\nUniDiffuser\n9.71\n+ w/o adapter\n9.52 (-0.19)\nUniDiffuser\n9.71\nFine-tuned on MS-COCO\nEasyGen\n7.68 (-0.44)\nUniDiffuser\n8.12\n+ w/o adapter\n7.89 (-0.23)\nUniDiffuser\n8.12\nTable 6: Comparing the image generation quality of\nend-to-end MLLMs and their corresponding diffusion\nmodels on the MS-COCO validation set (256 \u00d7 256).\nOur EasyGen surpasses the original diffusion model,\nwhile other MLLMs fall short in comparison.\nportion of the original data (LIAON-COCO) and\nthe MS-COCO train set, respectively. While other\nmodels resulted in a decrease in image generation\nperformance compared to the corresponding dif-\nfusion model, EasyGen outperformed the original\nUniDiffuser due to the fine-tuned BiDiffuser and\nthe adapter module. Furthermore, Tab. 7 provides\nCLIP-T scores from ImagenHub. We notice similar\ntrends to the results in Tab. 6 using the FID indica-\ntor. This suggests that our method can better align\nLLM with diffusion model\u2019s text space.\n5.7\nExtendability\nTab. 5 explores the extensibility of our method from\ntwo aspects. Firstly, we aim to enhance the perfor-\nmance of EasyGen on complex tasks such as VQA\nand OCR by integrating more powerful visual en-\ncoders. Considering the potential information dilu-\nMM-Model\nCLIP-T \u2191\nDiffusion Model\nCLIP-T \u2191\nNExT-GPT\n0.259 (-0.031)\nSD\n0.290\nEmu\n0.262 (-0.023)\nSD v1.5\n0.285\nEmu2\n0.266 (-0.023)\nSD XL\n0.289\nEasyGen\n9.16 (-0.55)\nUniDiffuser\n9.71\nTable 7: Comparing the CLIP-T score of end-to-end\nMLLMs and their corresponding diffusion models on\nthe ImagenHub.\ntion or omission when using BiDiffuser to convert\nimages to text space, we choose to integrate CLIP\nViT-L/14 as the image encoder (as depicted in Fig-\nure 6 in the Appendix). During this process, we\nfreeze CLIP and BiDiffuser while fine-tuning the\nparameters of the LLM and projection layers. The\nresults presented in Tab. 5 include performance on\ntraditional short QA and the modern benchmark\nMMBench (Liu et al., 2023c). With CLIP ViT-L,\nEasyGen\u2019s performance is better than LLaVA on\nthe VQAv2 dataset, demonstrating that BiDiffuser\ncan effectively assist LLM in understanding images.\nSecondly, we investigate the plug-and-play capabil-\nity of BiDiffuser, as it can also be integrated into\nother MLLMs (with the same LLMs) to improve\ntheir performance. The results demonstrate that\nwith BiDiffuser, LLaVA-1.5 could achieve better\nperformance. We speculate that BiDiffuser pro-\nvides guidance information to MLLMs, enabling\nthem to better understand the details of CLIP en-\ncoded images.\n8\n6\nConclusion\nWe have introduced EasyGen, a model that facil-\nitates multimodal understanding and generation.\nCompared to existing models, EasyGen offers a\nmore efficient solution by employing BiDiffuser, a\nbidirectional diffusion model. This allows for more\neffective modal interactions, handling both image-\nto-text and text-to-image generations by the fusion\nof BiDiffuser and LLMs. Additionally, EasyGen\ncan be easily integrated into existing advanced mul-\ntimodal LLMs to further boost their performance.\n7\nLimitations\nThis section aims to highlight the limitations of our\nwork and provide further insights into the research\nin this area. Our model relies on diffusion for multi-\nmodal interaction, which means that the text-to-\nimage and image-to-text processes may take longer.\nIn our experiments, we tested the performance of\nour model on one A100 (80G) GPU. During in-\nference, using 1000 image-caption pairs, EasyGen\ntook approximately 2.95 seconds for the caption\ngeneration task (with the diffusion module taking\nabout 2.41 seconds) and around 4.96 seconds to\ngenerate an image. We believe it would be bene-\nficial to explore more efficient sampling methods,\nsuch as DPM-Solver++ (Lu et al., 2022), to im-\nprove the overall efficiency of EasyGen.\nReferences\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. No-\ncaps: Novel object captioning at scale. In Proceed-\nings of the IEEE/CVF international conference on\ncomputer vision, pages 8948\u20138957.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc,\nAntoine Miech, Iain Barr, Yana Hasson, Karel\nLenc, Arthur Mensch, Katherine Millican, Malcolm\nReynolds, et al. 2022. Flamingo: a visual language\nmodel for few-shot learning. Advances in Neural\nInformation Processing Systems, 35:23716\u201323736.\nFan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan\nLi, Hang Su, and Jun Zhu. 2023a. All are worth\nwords: A vit backbone for diffusion models. In Pro-\nceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 22669\u201322679.\nFan Bao, Shen Nie, Kaiwen Xue, Chongxuan Li, Shi\nPu, Yaole Wang, Gang Yue, Yue Cao, Hang Su, and\nJun Zhu. 2023b. One transformer fits all distributions\nin multi-modal diffusion at scale. arXiv preprint\narXiv:2303.06555.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei.\n2021. Beit: Bert pre-training of image transformers.\nIn International Conference on Learning Representa-\ntions.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877\u20131901.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nWenliang Dai, Junnan Li, Dongxu Li, Anthony\nMeng Huat Tiong, Junqi Zhao, Weisheng Wang,\nBoyang Li, Pascale Fung, and Steven Hoi. 2023. In-\nstructblip: Towards general-purpose vision-language\nmodels with instruction tuning.\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffu-\nsion models beat gans on image synthesis. Advances\nin neural information processing systems, 34:8780\u2013\n8794.\nRunpei Dong, Chunrui Han, Yuang Peng, Zekun Qi,\nZheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun,\nHongyu Zhou, Haoran Wei, et al. 2023. Dreamllm:\nSynergistic multimodal comprehension and creation.\narXiv preprint arXiv:2309.11499.\nRohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Man-\nnat Singh, Kalyan Vasudev Alwala, Armand Joulin,\nand Ishan Misra. 2023.\nImagebind: One embed-\nding space to bind them all. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pat-\ntern Recognition, pages 15180\u201315190.\nYash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv\nBatra, and Devi Parikh. 2017. Making the v in vqa\nmatter: Elevating the role of image understanding\nin visual question answering. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 6904\u20136913.\nJiaxian Guo,\nJunnan Li,\nDongxu Li,\nAnthony\nMeng Huat Tiong, Boyang Li, Dacheng Tao, and\nSteven CH Hoi. 2022.\nFrom images to textual\nprompts: Zero-shot vqa with frozen large language\nmodels. arXiv preprint arXiv:2212.10846.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner,\nBernhard Nessler, and Sepp Hochreiter. 2017. Gans\ntrained by a two time-scale update rule converge to a\n9\nlocal nash equilibrium. Advances in neural informa-\ntion processing systems, 30.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. De-\nnoising diffusion probabilistic models. Advances\nin neural information processing systems, 33:6840\u2013\n6851.\nJonathan Ho and Tim Salimans. 2021. Classifier-free\ndiffusion guidance. In NeurIPS 2021 Workshop on\nDeep Generative Models and Downstream Applica-\ntions.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nEdward J Hu, Phillip Wallis, Zeyuan Allen-Zhu,\nYuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen,\net al. 2021. Lora: Low-rank adaptation of large lan-\nguage models. In International Conference on Learn-\ning Representations.\nDrew A Hudson and Christopher D Manning. 2019.\nGqa: A new dataset for real-world visual reasoning\nand compositional question answering. In Proceed-\nings of the IEEE/CVF conference on computer vision\nand pattern recognition, pages 6700\u20136709.\nJing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov.\n2023a. Generating images with multimodal language\nmodels. arXiv preprint arXiv:2305.17216.\nJing Yu Koh, Ruslan Salakhutdinov, and Daniel\nFried. 2023b. Grounding language models to im-\nages for multimodal generation.\narXiv preprint\narXiv:2301.13823.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin John-\nson, Kenji Hata, Joshua Kravitz, Stephanie Chen,\nYannis Kalantidis, Li-Jia Li, David A Shamma, et al.\n2017. Visual genome: Connecting language and vi-\nsion using crowdsourced dense image annotations.\nInternational journal of computer vision, 123:32\u201373.\nBo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang,\nJingkang Yang, and Ziwei Liu. 2023a.\nOtter: A\nmulti-modal model with in-context instruction tuning.\narXiv preprint arXiv:2305.03726.\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.\n2023b. Blip-2: Bootstrapping language-image pre-\ntraining with frozen image encoders and large lan-\nguage models. arXiv preprint arXiv:2301.12597.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In International Conference on Ma-\nchine Learning, pages 12888\u201312900. PMLR.\nZujie Liang, Huang Hu, Can Xu, Chongyang Tao, Xi-\nubo Geng, Yining Chen, Fan Liang, and Daxin Jiang.\n2021. Maria: A visual experience powered conver-\nsational agent. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5596\u20135611.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r,\nand C Lawrence Zitnick. 2014.\nMicrosoft coco:\nCommon objects in context. In Computer Vision\u2013\nECCV 2014: 13th European Conference, Zurich,\nSwitzerland, September 6-12, 2014, Proceedings,\nPart V 13, pages 740\u2013755. Springer.\nHaotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae\nLee. 2023a. Improved baselines with visual instruc-\ntion tuning. arXiv preprint arXiv:2310.03744.\nHaotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae\nLee. 2023b. Visual instruction tuning. arXiv preprint\narXiv:2304.08485.\nYuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li,\nSongyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi\nWang, Conghui He, Ziwei Liu, et al. 2023c. Mm-\nbench: Is your multi-modal model an all-around\nplayer? arXiv preprint arXiv:2307.06281.\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongx-\nuan Li, and Jun Zhu. 2022. Dpm-solver++: Fast\nsolver for guided sampling of diffusion probabilistic\nmodels. arXiv preprint arXiv:2211.01095.\nKenneth Marino, Mohammad Rastegari, Ali Farhadi,\nand Roozbeh Mottaghi. 2019. Ok-vqa: A visual ques-\ntion answering benchmark requiring external knowl-\nedge. In Proceedings of the IEEE/cvf conference\non computer vision and pattern recognition, pages\n3195\u20133204.\nVishvak Murahari, Prithvijit Chattopadhyay, Dhruv Ba-\ntra, Devi Parikh, and Abhishek Das. 2019. Improving\ngenerative visual dialog by answering diverse ques-\ntions. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya\nRamesh, Pranav Shyam, Pamela Mishkin, Bob Mc-\ngrew, Ilya Sutskever, and Mark Chen. 2022. Glide:\nTowards photorealistic image generation and edit-\ning with text-guided diffusion models.\nIn Inter-\nnational Conference on Machine Learning, pages\n16784\u201316804. PMLR.\nXichen Pan, Li Dong, Shaohan Huang, Zhiliang Peng,\nWenhu Chen, and Furu Wei. 2023. Kosmos-g: Gen-\nerating images in context with multimodal large lan-\nguage models. arXiv preprint arXiv:2310.02992.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In International confer-\nence on machine learning, pages 8748\u20138763. PMLR.\n10\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea Voss, Alec Radford, Mark Chen, and\nIlya Sutskever. 2021. Zero-shot text-to-image gen-\neration. In International Conference on Machine\nLearning, pages 8821\u20138831. PMLR.\nRobin Rombach, Andreas Blattmann, Dominik Lorenz,\nPatrick Esser, and Bj\u00f6rn Ommer. 2022.\nHigh-\nresolution image synthesis with latent diffusion mod-\nels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n10684\u201310695.\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox.\n2015. U-net: Convolutional networks for biomedical\nimage segmentation. In Medical Image Computing\nand Computer-Assisted Intervention\u2013MICCAI 2015:\n18th International Conference, Munich, Germany,\nOctober 5-9, 2015, Proceedings, Part III 18, pages\n234\u2013241. Springer.\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael\nPritch, Michael Rubinstein, and Kfir Aberman. 2023.\nDreambooth: Fine tuning text-to-image diffusion\nmodels for subject-driven generation. In Proceed-\nings of the IEEE/CVF Conference on Computer Vi-\nsion and Pattern Recognition, pages 22500\u201322510.\nChitwan Saharia, William Chan, Saurabh Saxena,\nLala Li, Jay Whang, Emily L Denton, Kam-\nyar Ghasemipour, Raphael Gontijo Lopes, Burcu\nKaragol Ayan, Tim Salimans, et al. 2022. Photo-\nrealistic text-to-image diffusion models with deep\nlanguage understanding. Advances in Neural Infor-\nmation Processing Systems, 35:36479\u201336494.\nDustin Schwenk, Apoorv Khandelwal, Christopher\nClark, Kenneth Marino, and Roozbeh Mottaghi. 2022.\nA-okvqa: A benchmark for visual question answer-\ning using world knowledge. In European Conference\non Computer Vision, pages 146\u2013162. Springer.\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li,\nWeiming Lu, and Yueting Zhuang. 2023. Hugging-\ngpt: Solving ai tasks with chatgpt and its friends in\nhuggingface. arXiv preprint arXiv:2303.17580.\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and\nAmanpreet Singh. 2020. Textcaps: a dataset for im-\nage captioning with reading comprehension. In Com-\nputer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceed-\nings, Part II 16, pages 742\u2013758. Springer.\nAmanpreet Singh,\nVivek Natarajan,\nMeet Shah,\nYu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh,\nand Marcus Rohrbach. 2019. Towards vqa models\nthat can read. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition\n(CVPR).\nJascha\nSohl-Dickstein,\nEric\nWeiss,\nNiru\nMah-\neswaranathan, and Surya Ganguli. 2015. Deep un-\nsupervised learning using nonequilibrium thermo-\ndynamics. In International conference on machine\nlearning, pages 2256\u20132265. PMLR.\nQingfeng Sun, Yujing Wang, Can Xu, Kai Zheng,\nYaming Yang, Huang Hu, Fei Xu, Jessica Zhang,\nXiubo Geng, and Daxin Jiang. 2021.\nMulti-\nmodal dialogue response generation. arXiv preprint\narXiv:2110.08515.\nQuan Sun, Qiying Yu, Yufeng Cui, Fan Zhang,\nXiaosong Zhang, Yueze Wang, Hongcheng Gao,\nJingjing Liu, Tiejun Huang, and Xinlong Wang.\n2023. Generative pretraining in multimodality. arXiv\npreprint arXiv:2307.05222.\nZineng Tang, Ziyi Yang, Chenguang Zhu, Michael\nZeng, and Mohit Bansal. 2023.\nAny-to-any gen-\neration via composable diffusion.\narXiv preprint\narXiv:2305.11846.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix,\nBaptiste Rozi\u00e8re, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai\nBai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren\nZhou, and Hongxia Yang. 2022a. Ofa: Unifying ar-\nchitectures, tasks, and modalities through a simple\nsequence-to-sequence learning framework. In Inter-\nnational Conference on Machine Learning, pages\n23318\u201323340. PMLR.\nWenhui Wang,\nHangbo Bao,\nLi Dong,\nJohan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022b. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nChenfei Wu, Shengming Yin, Weizhen Qi, Xiaodong\nWang, Zecheng Tang, and Nan Duan. 2023a.\nVisual chatgpt:\nTalking, drawing and editing\nwith visual foundation models.\narXiv preprint\narXiv:2303.04671.\nShengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and\nTat-Seng Chua. 2023b. Next-gpt: Any-to-any multi-\nmodal llm. arXiv preprint arXiv:2309.05519.\nXingqian Xu, Zhangyang Wang, Gong Zhang, Kai\nWang, and Humphrey Shi. 2023. Versatile diffusion:\nText, images and variations all in one diffusion model.\nIn Proceedings of the IEEE/CVF International Con-\nference on Computer Vision, pages 7754\u20137765.\nZhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin\nLin, Ehsan Azarnasab, Faisal Ahmed, Zicheng Liu,\nCe Liu, Michael Zeng, and Lijuan Wang. 2023. Mm-\nreact: Prompting chatgpt for multimodal reasoning\nand action. arXiv preprint arXiv:2303.11381.\nQinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye,\nMing Yan, Yiyang Zhou, Junyang Wang, An-\nwen Hu, Pengcheng Shi, Yaya Shi, et al. 2023.\nmplug-owl: Modularization empowers large lan-\nguage models with multimodality. arXiv preprint\narXiv:2304.14178.\n11\nXiaoxue Zang, Lijuan Liu, Maria Wang, Yang Song,\nHao Zhang, and Jindong Chen. 2021. Photochat: A\nhuman-human dialogue dataset with photo sharing\nbehavior for joint image-text modeling. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 6142\u20136152.\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang,\nHanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\nWendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An\nopen bilingual pre-trained model. In The Eleventh In-\nternational Conference on Learning Representations.\nRenrui Zhang, Jiaming Han, Aojun Zhou, Xiangfei Hu,\nShilin Yan, Pan Lu, Hongsheng Li, Peng Gao, and\nYu Qiao. 2023a. Llama-adapter: Efficient fine-tuning\nof language models with zero-init attention. arXiv\npreprint arXiv:2303.16199.\nYiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hong-\nsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu\nYue. 2023b.\nMeta-transformer: A unified frame-\nwork for multimodal learning.\narXiv preprint\narXiv:2307.10802.\nShanshan Zhong, Zhongzhan Huang, Weushao Wen,\nJinghui Qin, and Liang Lin. 2023. Sur-adapter: En-\nhancing text-to-image pre-trained diffusion models\nwith large language models. In Proceedings of the\n31st ACM International Conference on Multimedia,\npages 567\u2013578.\nDeyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and\nMohamed Elhoseiny. 2023. Minigpt-4: Enhancing\nvision-language understanding with advanced large\nlanguage models. arXiv preprint arXiv:2304.10592.\n12\nA\nEthics Statement\nWe adhere to the ACL Ethics Policy and have\nconducted our research using publicly available\nrepositories and datasets. Our primary focus is on\ninvestigating the integration of diffusion models\nand LLMs for multimodal generation. Therefore,\nthe results should be seen as AI-generated content.\nWhile we have not observed deliberate harmful\ncontent, the model has the potential to generate\nsuch content if triggered. We have taken steps to\nminimize this risk through fine-tuning on public\ndatasets, but caution is still exercised. In future, we\nwill prioritize improving downstream performance\nand exploring methods to enhance control over the\ngeneration process. To ensure reproducibility and\nsupport future research, we have made all resources\npublicly available and provided proper citations to\nprevious research within the code.\nB\nDatasets\nWe test the effectiveness of EasyGen by experi-\nmenting on different tasks including image caption-\ning, visual question answering (VQA), and multi-\nmodal dialogue tasks. Table 8 shows the statistics\nof the pre-training datasets for BiDiffuser, align-\nment and VQA tasks.\nWe use the MS-COCO (Lin et al., 2014) dataset\nfor image captioning. Following BLIP-2 (Li et al.,\n2023b), we fine-tune EasyGen on MS-COCO and\nevaluate its performance on the Karpathy test set\nand the NoCaps (Agrawal et al., 2019) validation\nset. In MS-COCO, each image typically has five\ncaptions that convey similar meanings. The train-\ning set consists of 82,783 images with 414,113\ncaptions, while the COCO Karpathy test set has\n5,000 images and the NoCaps validation set has\n4,500 images.\nFor multimodal dialogue, we utilize the Pho-\ntoChat (Zang et al., 2021) dataset, which is a high-\nquality dataset consisting of 10,917 images and\n12,286 dialogues. Each dialogue is associated with\na user image and its corresponding text descrip-\ntion. The dataset is divided into 10,286 training\ninstances, 1,000 development instances, and 1,000\ntesting instances. Moreover, PhotoChat includes\nphoto-sharing activities, defined as the process\nof creating <Img><caption></Img> in this study.\nEach conversation in PhotoChat is broken down\nand constructed into multiple samples so that each\nround of responses can be learned. Specifically,\nwe regard the first three turns as the dialog context,\nand the subsequent turns as the prediction targets.\nBy converting the dialogues of this dataset into the\nform mentioned in 4.4, we obtained 49,240 train,\n4,792 dev, and 4,836 test dialogue pairs.\nFor the VQA task, we conduct a quantitative\nevaluation on both the OK-VQA (Marino et al.,\n2019) validation set (5,046 questions) and the\nGQA (Hudson and Manning, 2019) test-dev set\n(12,578 questions). As shown in Table 4, for the\nfrozen LLM, following BLIP-2, we employ the\nlength penalty in beam search to encourage short\nanswer generation. On the contrary, for the tuned\nLLM, we use the VQA instructions (as shown in\nTable 10) to do instruction tuning during the align-\nment process. The data for instruction tuning is\nconstructed by randomly selecting 5K data from\nVQAv2 (Goyal et al., 2017) and 5K data from Vi-\nsual Dialog (Murahari et al., 2019) training set.\nC\nBaselines\nWe compare our proposed model with the follow-\ning state-of-the-art baselines:\nBLIP (Li et al., 2022) is a multimodal mixture\nof encoder-decoder. It can be used as an image-\nbased text encoder or decoder. We use it to perform\ncaption generation and VQA tasks.\nBLIP-2 (Li et al., 2023b) is pre-trained through\nbootstrapped learning from frozen visual encoder\nand LLMs using an efficient pre-training strategy.\nFlamingo (Alayrac et al., 2022) incorporates new\ncross-attention layers into Chinchilla language\nmodel (Hoffmann et al., 2022) to inject visual fea-\ntures, and pre-trains the new layers on billions of\nimage-text pairs. We use it to perform caption gen-\neration and VQA tasks.\nInstructBlip (Dai et al., 2023) is a vision-language\ninstruction tuning framework that is trained with\nBLIP-2 and capable of solving various visual lan-\nguage tasks.\nMiniGPT-4 (Zhu et al., 2023) utilizes a single pro-\njection layer to align visual information from a pre-\ntrained vision encoder with an LLM. It employed\nthe same visual encoder as used in BLIP-2.\nLLaVA (Liu et al., 2023b) employs a solitary pro-\njection layer to convert image features extracted\nfrom the pre-trained CLIP-ViT-L/14 visual encoder\ninto the language embedding space of Vicuna.\nMaria (Liang et al., 2021) is a neural conversation\nagent which can leverage visual world experiences\nsourced from a vast image index. It possesses the\nability to fetch a relevant image specific to the con-\n13\nData types\nDataset\nSize\nBiDiffuser\nAlignment\nFine-tuning\nCaption\nMS-COCO caption (Lin et al., 2014)\n83K\n\"\n\"\n%\nVisual Genome (Krishna et al., 2017)\n86K\n\"\n%\n%\nMultimodal instruction\nLLaVA dataset (Liu et al., 2023b)\n80K\n%\n\"\n\"\nVQA\nVQAv2 (Goyal et al., 2017)\n83K\n%\n-\n\"\nAOK-VQA (Schwenk et al., 2022)\n66K\n%\n%\n\"\nOCR-related tasks\nText Captions (Sidorov et al., 2020)\n22K\n%\n%\n\"\nTextVQA (Singh et al., 2019)\n%\n%\n\"\nTable 8: Description of datasets used in our alignment and VQA fine-tuning stages. Noting that in alignment process,\nwe used 5K images from VQAv2 dataset.\nDataset\nTask\nSplit\nMetric\nImage-to-Text\nMS-COCO (Lin et al., 2014)\nImage captioning\nTest\nCIDEr, BLEU, SPICE\nNoCaps (Agrawal et al., 2019)\nImage captioning\nVal\nCIDEr, SPICE\nOK-VQA (Marino et al., 2019)\nVQA\nVal\nAccuracy\nGQA (Hudson and Manning, 2019)\nVQA\nTest\nAccuracy\nMultimodal Generation\nPhotoChat Zang et al., 2021\nImage dialogue\nTest\nPPL, BLEU, ROUGE, FID\nTable 9: Summary of the evaluation datasets and metrics.\nversation and extract visual knowledge from it.\nDivter (Sun et al., 2021) focuses on exploring mul-\ntimodal dialogue generative models. Given the\ndialogue context, this model first generates a text\nresponse or image description and then generates\nan image according to the description.\n   BiDiffuser\nProjection Layer\n  CLIP ViT-L\nProjection Layer\nImage Embedding\nText Embedding\nText Embedding\nWhat might be the purpose \nof the buses in this \nlocation? ### Assistant:\nLLM\nEmbedding Layer\nFigure 6: Model\u2019s architecture for VQA finetuning. The\nmodule with blue background is referred to as BiDif-\nfuser, while the rest is the architecture of MLLM using\nCLIP as the image encoder (such as LLaVA).\nD\nEvaluation\nFor evaluating the quality of text generation, we\nutilize metrics such as BLEU, Rouge-L, Accu-\nracy, and PPL (Perplexity). Additionally, follow-\ning the approach of Vicuna (Chiang et al., 2023)\nand LLaVA (Liu et al., 2023b), we employ Chat-\nGPT to assess the generated responses from our\nmodel. Specifically, for the image captioning task,\nwe randomly select 30 images from the MS-COCO\nKarpathy split and then let ChatGPT score the re-\nsponses generated by EasyGen and the baseline\nmodels. ChatGPT evaluates the models\u2019 responses\nbased on relevance, details, and accuracy and as-\nsigns an overall score between 1 and 10, with a\nhigher score indicating better performance. To eval-\nuate the quality of image generation, we use the\nFrechet Inception Distance (FID) score (Heusel\net al., 2017), which measures the divergence be-\ntween two multivariate normal distributions.\nE\nImplementation Details\nLLM\nDuring the alignment process, we utilize\nthe AdamW optimizer with \u03b20 = 0.9, \u03b21 = 0.99,\nand weight decay of 0. The LLMs are trained with\na cosine learning rate of 2e-5 and a warmup rate\nof 0.03. We use a batch size of 96 for the frozen\nLLMs and 32 for the tuned LLMs. During training,\nwe convert the LLMs (FlanT5XL/Vicuna-7B) to\nBFloat16/FP16 and BiDiffuser to FP16. During the\nVQA tuning process, we use CLIP ViT-L/14 336px\nas additional image encoder. We finetune EasyGen\non mixture datasets for 1 epoch with a batch size\nof 32. We adopt the AdamW optimizer with \u03b2 =\n(0.9, 0.99) with the learning rate is 2e-5. We use\na cosine learning rate decay with a learning rate is\n2e-5 and warmup ration is 0.03.\nDiffusion Module\nWe inherit the settings from\nUniDiffuser and utilize pre-trained weights from\nits checkpoint for our text-to-image generator. The\nmodel is fine-tuned on the MS-COCO and VG\n14\nTask\nInstruction Template\nImage Captioning\nUSER: <image>+random[query] Assistant:\nLLaVA 80K\nUSER: Please answer question from this image: <image> Question: <question> Assistant:\nUSER: Image: <image> Question: <question> Assistant:\nUSER: Answer question <question> through the image <image> Assistant:\nMultimodal Dialogue\nUSER: Dialog history+<photo>+Dialogue history Assistant:\nVQA\nUSER: Image: <image> Question: <question> Short answer: Assistant:\nUSER: Image: <image> Question: <question> Answer the option\u2019s letter. Assistant:\nTable 10: Examples of task instruction templates. <image> represents the input image, <question> denotes the\nquestion in the VQA and LLaVA 80K dataset, and <photo> is the image description of the input image.\ndataset, which contains images with a resolution\nof 512 \u00d7 512, for 10 epochs with a batch size of\n312. For all of our sampling processes, we employ\nDPM-Solver with 50 steps.\n1. Describe the image concisely.\n2. Provide a brief description of the given image.\n3. Can you describe this image briefly?\n4. Provide a summary of visual elements depicted in the image.\n5. Give me the essential characteristics of the photograph in a\nconcise manner.\n6. Rephrase the image depicted in a concise manner.\n7. Describe the objects in this image no in detail.\n8. Please introduce the image for me briefly.\n9. Give me the image\u2019s short descriptions.\n10. Please provide a general depiction of the image presented.\nTable 11: For the image captioning task, a query instruc-\ntion is randomly selected.\nF\nInstruction Tuning\nWe list the instructions for different tasks in the\nmain paper in Table 10. Specifically, the queries\nused to describe image contents are presented in\nTable 11. Table 10 shows the templates used in\nVicuna, if the LLM is FlanT5, kindly use \u201cHuman\u201d\nto substitute \u201cUSER\u201d in the instruction templates.\nModel architecture for VQA finetuning is shown\nin Figure 6. EasyGen integrates the outputs of\nBiDiffuser with images encoded by CLIP ViT-L/14.\nWe freeze CLIP and BiDiffuser while only tuning\nthe parameters of the LLM and projection layers.\nG\nTraining Efficiency\nTable 13 summarizes the key factors in training\nEasyGen. The training process of EasyGen is com-\nputationally efficient, especially with the utiliza-\ntion of the parameter-efficient fine-tuning method\nLoRa (Hu et al., 2021). To enable multimodal\nresponse generation, we further train the aligned\nEasyGen. This process entails fine-tuning the LLM\n(FlanT5XL) on the PhotoChat dataset for 2 epochs,\nwhich typically requires approximately 4 A100\n(80G) GPU hours.\nDifferent Instruction Templates:\nCaption\nGeneration\nProvide a brief description of the given im-\nage. Assistant:\nResponse\nBuses parked with a snow mountain view be-\nhind them.\nVQA\nWhat numbers are displayed on the front of the\nbus on the right? Short answer: Assistant:\nResponse\n6044\nMultimodal\nDialogue\nWhat might be the purpose of the buses in this\nlocation? Assistant:\nResponse\nIt is plausible that they are in this location for\nmultiple reasons. Some possible reasons might\nbe: 1)...2)...\nTable 12: Examples of different instructions with dif-\nferent output formats. We use bold fonts to indicate\ndifferent instructions.\nH\nImpact of Alignment Manners\nIn Table 14, we investigate the impact of different\nalignment manners on EasyGen. After removing\nthe ITDM loss, the performance is slightly weaker\nthan the original model. It is evident that the MSE\nLoss can help to align the semantic spaces of the\ntwo models. Furthermore, the performance of the\nmodel will drop significantly after removing the\ncross-entropy loss, suggesting that constraints via\nthe language model play a key role.\nI\nMore Qualitative Results\nWe present several instances on PhotoChat dataset\nin Figure 7 and the image-captioning task in Fig-\nure 8. In Figure 9, 11, 10, we compare EasyGen\nwith state-of-the-art multimodal language models.\nThe responses of MiniGPT-4, LLaVA, mPLUG-\nowl and InstructBlip are obtained from their offi-\ncial demos. Morever, in Figure 12, 13, we show\nEasyGen\u2019s ability to accept multimodal inputs and\ngenerate multimodal responses.\n15\nModel\nTrainable Param.\nTraining Images\nTraining Cost\nPre-training\nBiDiffuser\n952M\n169K\n120 (A100 80GB) GPU hours\nAlignment\nProjection Layers +\nT5XL\n4M\n163K\n20 (RTX3090 24GB) GPU hours\nProjection Layers +\nT5XL\n3B\n173K\n20 (A100 80GB) GPU hours\nProjection Layers +\nVicuna 7B\n7B\n173K\n72 (A100 80GB) GPU hours\nProjection Layers +\nVicuna 7B(LoRa)\n610M\n173K\n20 (A100 80GB) GPU hours\nTable 13: EasyGen\u2019s trainable parameters, training data size, and training cost during alignment process.\nModel\nNoCaps (val)\nCOCO (Karpathy)\nOK-VQA\nGQA\nCIDEr\nSPICE\nSPICE\nBLEU@4\nCIDEr\nAccuracy\nAccuracy\nEasyGen Mid-Align FlanT5XL\n121.2\n15.5\n25.1\n43.5\n145.7\n31.5\n22.6\n+ w/o ITDM\n118.6\n15.3\n24.8\n42.2\n141.5\n-\n-\n+ w/o ITG\n93.2\n12.9\n23.0\n35.1\n127.6\n-\n-\nTable 14: Ablation studies on the instruction-tuning process and loss functions.\nA: I just got back from the pet store and you wouldn\u2019t believe what I saw? \nB: What you saw, can you tell me? \nA: I saw the cutest golden retriever ever\u2026 I couldn\u2019t stop looking at this animal\u2026 \nA: Check it out - I took a picture. <Img>\nA: What\u2019s onn your mind?              B: Good. tanks. I am thinking to make a cake. \nA: Sound yummy, what kind? \nB: I saw a chocolate cake topped with whipped cream. It looks so yummy.  \nB: I hope you can see the photo. <Img>\nObjects in the photo: Animal, \nGolden retriever, Dog, Carnivore.\nObjects in the photo: Baked goods, \nCake, Dairy\nB: Lovely and cute dude\nA: te cake looks beautiful, but \nshould not be hard to make.\nA golden retriever puppy peeking \nthrough a stu\ufb00ed animal\nA chocolate cake covered with \nwhipped cream\nB: What a cute golden retriever, I \nwant to play with him.\nA: Delicious! Do you have a recipe in \nmind for your chocolate cake with \nwhipped cream topping?\nGround Truth\nGround Truth\nEasyGen (Ours)\nEasyGen (Ours)\nDialogue \nContext\nImage \nDescription\nResponse\nImages\nFigure 7: Examples of the generated responses on PhotoChat dataset. The text highlighted in red indicates the\nobjects present in the image. The turns prefixed with A/B denote the given context.\nEasyGen: A giraffe eating \ngrass on a green grass field.\nEasyGen: A kitchen with a stove \nand oven in a fireplace.\nEasyGen: Two children are \nriding on skis with their parents \nin a snowy street.\nEasyGen: A snowboarder flying \nthrough the air on a frozen \nmountain slope.\nEasyGen: Two cats separated by the \nwindow are looking forward at the \nsame time.\nEasyGen: A long wooden bench \nwith pictures of the world.\nFigure 8: Examples of image captioning results by EasyGen.\n16\nMultiModal Chat Group\nPlease tell me the information about this picture.\nGenerating some pictures like this.\nThis painting is a classic representation of a young girl from the 17th century. The girl is depicted wearing a blue \nand white dress, with a pearl earring in her ear. The painting is a masterpiece of Dutch Golden Age art, a period of \ngreat cultural and economic prosperity in the Netherlands. The girl in the painting is likely depicted as a middle-\nclass citizen, and the painting may have been commissioned as a portrait or for a specific purpose, such as a \nfamily crest or a religious painting. The painting is known for its attention to detail, vivid colors, and beautifully \nrendered textures. It is a true masterpiece of Dutch art, and a must- see for anyone interested in the history of art.\nEasyGen (ours):\nEasyGen (ours):\nmPLUG-Owl:\nmPLUG-Owl:\nLLaVA:\nLLaVA:\nMiniGPT-4:\nMiniGPT-4:\nThe image is a painting depicting a young girl, likely a Dutch girl in a yellow dress, with her hair tied up in braids \nand a gold earring. She is looking directly into the viewer\u2019s eyes, creating an intimate connection with the observer. \nThe painting is set in an indoor scene with a table and chairs visible in the background. The girl is positioned in \nfront of the table, and there is an additional person standing to her left. There are also several other people in \nvarious positions around the room, suggesting a social gathering or a family setting.\nHere are a few examples of paintings that are similar in style to the one you described: 1. \u201cGirl with a Pearl Earring\u201d \nby Johannes Vermeer; 2. \u201cThe Procuress\u201d by Pieter de Hooch; 3. \u201cThe Artist\u2019s Mother\u201d by Gerard van Honthorst \u2026. \nThese paintings are all from the 17th century and depict women or groups of people in various settings, often with \na focus on everyday life. They are known for their attention to detail and realistic depiction of light and shadow.\nThe image is a portrait of a woman wearing a blue headscarf, a yellow dress, and a pearl earring. \nShe is looking to her left, and her outfit and accessories create a visually appealing contrast against \nthe background. The woman\u2019s attire and appearance suggest a sense of style and elegance.\nI\u2019m sorry, as an AI language model, I am not able to generate images. However, I \ncan suggest some ideas for paintings that share a similar style to the portrait of the \nwoman in the blue headscarf, yellow dress, and pearl earring by Johannes Vermeer:\nThe painting depicts a young woman with a pearl earring looking out into the distance. The woman is \nwearing a yellow dress with a blue and white headband. The background is a mix of blue and green, \nwith a few trees visible in the distance. The overall mood of the painting is peaceful and contemplative. \nThe painting is likely a representation of a woman reflecting on her thoughts or feelings.\nHere are some pictures that resemble the painting, featuring young girls with their hair up in braids and \nwearing gold earrings: A young girl in a yellow dress and braided hair, looking directly into the viewer\u2019s eyes. \nAnother young woman with her hair in similar braid styles, wears a white dress with gold accents, and has a \ngold necklace. Two girls in matching yellow outfits, with braiding and gold jewelry, standing together and \nlooking into each other\u2019 s eyes, creating a warm and inviting atmosphere.\nFigure 9: In this case study, for the first question, EasyGen can give an accurate answer including the background\ninformation of the image. With the image generation ability of BiDiffuser, EasyGen can generate visual responses.\n17\nMultiModal Chat Group\nWhere can I see this object appeared in this image?\nI plan to travel here, what should I pay attention to?\nEasyGen (ours):\nmPLUG-Owl:\nLLaVA:\nAs you\u2019re looking at a rendering of an iconic white and gold statue of a woman in a field, pay attention to the \ndetails of the sculpture, the sky, and the surroundings. The statue is situated in a green field, which provides a \npicturesque and serene atmosphere. The sky in the background is blue, suggesting a clear and sunny day. \nTo fully appreciate the beauty and significance of the statue, it\u2019s crucial to observe its detail, size, and the \noverall composition of the image. The statue may depict a woman, an important figure in history, or a \nsymbol of liberty and freedom. Additionally, pay attention to the field\u2019s surroundings, such as the grass, \nvegetation, and any other elements that contribute to the overall aesthetic.\nWhen visiting the Statue of Liberty, there are a few things you should keep in mind: \n1. The Statue of Liberty is a colossal neoclassical sculpture on Liberty Island in New York Harbor within New York City, \nUnited States. It was a gift from the people of France to the people of the United States and was dedicated on October 28, \n1886, to commemorate the centennial of the signing of the United States Declaration of Independence. \n2. The statue is 301 feet (92 meters) tall, including its pedestal and base, and is made of iron and steel. It was designed by \nthe French sculptor Frederic Auguste Bartholdi and the American architect Emmanuel Louis Masqueray. \n3. To get to the Statue of Liberty, you can take a ferry from Battery Park in Lower Manhattan. There are several ferry \nservices that offer tours to the statue, and it\u2019s important to plan ahead and book your tickets in advance to avoid long \nlines. Once you\u2019re on Liberty Island, you can take an elevator or stairs to the top of the pedestal, where you\u2019ll have a \nstunning view of \n4. New York Harbor and the surrounding city skyline. It\u2019s important to note that the pedestal is closed to visitors, but you \ncan still get a close-up view of the statue from the observation deck. \n5. Be sure to check the weather forecast before your visit, as the statue can be closed due to high winds or other inclement \nweather conditions. And don\u2019t forget to bring your camera to capture the stunning views and memorable moments!\nWhen planning a trip to visit the Statue of Liberty, there are several things you should pay attention to: \n1. Tickets: You will need to purchase tickets in advance to visit the Statue of Liberty and Ellis Island. You can \ndo this through the official Statue of Liberty National Monument website or through authorized ticket sellers. \n2. Accessibility: The Statue of Liberty is accessible via ferry from Battery Park in Lower Manhattan. There are \nalso guided tours available, which can provide more information about the history and significance of the \nstatue. \n3. Closed areas: Some areas of the Statue of Liberty, such as the pedestal and the crown, are closed to visitors \ndue to safety concerns. Make sure to check the official website for the most up-to-date information on \naccessibility and restrictions. \n4. Clothing and footwear: Wear comfortable shoes and dress appropriately for the weather, as you will be \nwalking and standing for extended periods of time.\nThe Statue of Liberty is located in New York City, United States. It is a famous landmark and \ntourist attraction, and you can see it in person by visiting Liberty Island in New York Harbor.\nEasyGen (ours):\nmPLUG-Owl:\nLLaVA:\nYou can see this blue and white 3D rendering of the Statue of Liberty located in the United States. \nThe image features an artistically detailed representation of the famous landmark, which stands tall \nand proud in the skyline. The statue represents the ideals of freedom, democracy, and human rights. \nThe scene is captured in a blue sky, emphasizing the beauty and grandeur of the iconic structure.\nYou can see the Statue of Liberty in the image, standing tall and proud in front of a blue sky.\nFigure 10: In this case study, for the first question, EasyGen can give an accurate answer, but the responses of the\nother two models are a bit biased. For the second question, EasyGen and LLaVA both give reasonable advice.\n18\nHow can the girl improve her working environment?\nIn the image, the young girl is sitting in a room with a laptop in front of her, which suggests that she \nmight be working or studying. To improve her working environment, she can consider several factors: \n1. Ergonomics: Ensure that the laptop is placed at eye level or slightly below, and the desk is at a \ncomfortable height for her to maintain a neutral posture. Adjustable furniture like a desk or a chair \ncan help achieve a comfortable working height. \n2. Lighting: Ensure that the room is well-lit, with natural light or artificial lighting to minimize eye \nstrain and improve focus. \n3. Organization: Keep the room clean and organized, with important items within easy reach. This \ncan help maintain focus and prevent distractions. \n4. Comfortable environment: Creating a comfortable environment, such as a dedicated workspace \nseparate from her bedroom, can help establish boundaries and promote productivity. \n5. Time management: Establish a routine and allocate specific time slots for studying or working on \nher laptop. This can help her stay on track and avoid procrastination. \nBy implementing these suggestions, the girl can enhance her working environment and achieve \nbetter focus, productivity, and overall comfort while working on her laptop.\nEasyGen (ours):\nmPLUG-Owl:\nMiniGPT-4:\nTo improve her working environment, the girl could consider several options: \n1. Organize her space: Ensure the area where she is working is clean, uncluttered, and well-lit. Remove \ndistractions such as toys or other items that might cause her to lose focus. Arrange her books and laptop \nin a manner that is easy to access and comfortable for her to work in. \n2. Create a designated work area: Set up a designated space for her to work, such as a desk, table, or a \nspecific corner in the living room. This helps her maintain her focus and minimize distractions. \n3. Provide a comfortable chair and proper lighting: Select a chair with good back support and proper \nergonomics to ensure she is comfortable while working. Lighting should be bright and evenly distributed \nto minimize shadows and glare. \n4. Set reasonable work hours: Set a reasonable time limit for her to work, and let her know when it\u2019s time \nto take a break. This can help her maintain focus and minimize fatigue. \nBy creating a well-organized, comfortable, and productive working environment, the girl can improve her \nfocus and efficiency while minimizing distractions during her time spent studying and doing homework.\nThere are several ways the girl can improve her working environment. One way is to make sure her desk and \nchair are comfortable and adjustable to her height. Another way is to have a clean and organized workspace, \nso she can easily find what she needs. Additionally, she could have a good lighting system to help her see her \nwork better. Finally, she could have a cushion or pillow to sit on to help reduce fatigue.\nInstructBlip:\nThe girl in the image can improve her working environment by organizing her surroundings and creating \na designated study area. To achieve this, she could separate the laptop from other items in the room and \ncreate a consistent workspace. This can help her maintain focus and reduce distractions, which can \nultimately enhance her productivity and creativity. Additionally, using a comfortable chair and positioning \nthe laptop at a suitable height can help prevent eye strain and promote better posture during long \nperiods of work. Using a calm atmosphere with adequate lighting and placing the laptop away from \nwindows or other sources of glare can also aid in ensuring a more conducive working environment.\nMultiModal Chat Group\nFigure 11: From this example, we can find that the response from EasyGen is more comprehensive and coherent.\nThis shows EasyGen can give reasonable suggestions based on the given image.\n19\nEasyGen\nI like the flowers in this picture. Please \nintroduce these flowers.\nShow me some close up pictures \nabout hydrangeas with pink petals.\nThe flowers in the picture are blue hydrangeas. \nHydrangeas are a popular flowering plant that \nare known for their large, showy flowers and \nclusters of small, delicate flowers in shades of \npink, blue, and white. The blue hydrangeas in the \npicture are likely the \u201cHydrangea macrophylla\u201d \nvariety, which is known for its large, cone-\nshaped flowers and blue or pink petals. These \nflowers are often used in floral arrangements \nand as a decorative element in landscap\nFigure 12: Example of multimodal response generation.\nEasyGen\nWhat is the object in the picture and who \ninvented it?\nWhat other things did he invent?\nWow, I am interested in phonograph, \nplease introduce it and show me some \npicture about it.\nThe object in the picture is an old \nincandescent light bulb, which was invented \nby Thomas Edison.\nIn addition to the incandescent light bulb, \nEdison also invented numerous other things, \nincluding the electric motor, the \nphonograph, and the first practical electric \nlighting system. He was recognized as one of \nthe most prolific inventors in history, with \nover 1,000 patents to his name.\nA phonograph, also called a record player, is \nan apparatus that plays recorded sound \nusing a disk or cylinder of grooved media, \ncalled a record or a cylinder. The record is \nplaced onto a turntable, which is connected \nto a motor that rotates the record at a \nspecific speed \u2026.\nHere are some possible pictures:\nFigure 13: Example of multimodal response generation.\n20\n",
    "2202.00512": "Published as a conference paper at ICLR 2022\nPROGRESSIVE DISTILLATION FOR FAST SAMPLING\nOF DIFFUSION MODELS\nTim Salimans & Jonathan Ho\nGoogle Research, Brain team\n{salimans,jonathanho}@google.com\nABSTRACT\nDiffusion models have recently shown great promise for generative modeling, out-\nperforming GANs on perceptual quality and autoregressive models at density es-\ntimation. A remaining downside is their slow sampling time: generating high\nquality samples takes many hundreds or thousands of model evaluations. Here\nwe make two contributions to help eliminate this downside: First, we present new\nparameterizations of diffusion models that provide increased stability when using\nfew sampling steps. Second, we present a method to distill a trained deterministic\ndiffusion sampler, using many steps, into a new diffusion model that takes half as\nmany sampling steps. We then keep progressively applying this distillation proce-\ndure to our model, halving the number of required sampling steps each time. On\nstandard image generation benchmarks like CIFAR-10, ImageNet, and LSUN, we\nstart out with state-of-the-art samplers taking as many as 8192 steps, and are able\nto distill down to models taking as few as 4 steps without losing much perceptual\nquality; achieving, for example, a FID of 3.0 on CIFAR-10 in 4 steps. Finally,\nwe show that the full progressive distillation procedure does not take more time\nthan it takes to train the original model, thus representing an ef\ufb01cient solution for\ngenerative modeling using diffusion at both train and test time.\n1\nINTRODUCTION\nDiffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020) are an emerg-\ning class of generative models that has recently delivered impressive results on many standard gen-\nerative modeling benchmarks. These models have achieved ImageNet generation results outper-\nforming BigGAN-deep and VQ-VAE-2 in terms of FID score and classi\ufb01cation accuracy score (Ho\net al., 2021; Dhariwal & Nichol, 2021), and they have achieved likelihoods outperforming autore-\ngressive image models (Kingma et al., 2021; Song et al., 2021b). They have also succeeded in image\nsuper-resolution (Saharia et al., 2021; Li et al., 2021) and image inpainting (Song et al., 2021c), and\nthere have been promising results in shape generation (Cai et al., 2020), graph generation (Niu et al.,\n2020), and text generation (Hoogeboom et al., 2021; Austin et al., 2021).\nA major barrier remains to practical adoption of diffusion models: sampling speed. While sam-\npling can be accomplished in relatively few steps in strongly conditioned settings, such as text-to-\nspeech (Chen et al., 2021) and image super-resolution (Saharia et al., 2021), or when guiding the\nsampler using an auxiliary classi\ufb01er (Dhariwal & Nichol, 2021), the situation is substantially differ-\nent in settings in which there is less conditioning information available. Examples of such settings\nare unconditional and standard class-conditional image generation, which currently require hundreds\nor thousands of steps using network evaluations that are not amenable to the caching optimizations\nof other types of generative models (Ramachandran et al., 2017).\nIn this paper, we reduce the sampling time of diffusion models by orders of magnitude in uncondi-\ntional and class-conditional image generation, which represent the setting in which diffusion models\nhave been slowest in previous work. We present a procedure to distill the behavior of a N-step DDIM\nsampler (Song et al., 2021a) for a pretrained diffusion model into a new model with N/2 steps, with\nlittle degradation in sample quality. In what we call progressive distillation, we repeat this distilla-\ntion procedure to produce models that generate in as few as 4 steps, still maintaining sample quality\ncompetitive with state-of-the-art models using thousands of steps.\n1\narXiv:2202.00512v2  [cs.LG]  7 Jun 2022\nPublished as a conference paper at ICLR 2022\nDistillation\nDistillation\nDistillation\nFigure 1: A visualization of two iterations of our proposed progressive distillation algorithm. A\nsampler f(z; \u03b7), mapping random noise \u03f5 to samples x in 4 deterministic steps, is distilled into a\nnew sampler f(z; \u03b8) taking only a single step. The original sampler is derived by approximately\nintegrating the probability \ufb02ow ODE for a learned diffusion model, and distillation can thus be\nunderstood as learning to integrate in fewer steps, or amortizing this integration into the new sampler.\n2\nBACKGROUND ON DIFFUSION MODELS\nWe consider diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020)\nspeci\ufb01ed in continuous time (Tzen & Raginsky, 2019a; Song et al., 2021c; Chen et al., 2021; Kingma\net al., 2021). We use x \u223cp(x) to denote training data. A diffusion model has latent variables\nz = {zt | t \u2208[0, 1]} and is speci\ufb01ed by a noise schedule comprising differentiable functions \u03b1t, \u03c3t\nsuch that \u03bbt = log[\u03b12\nt /\u03c32\nt ], the log signal-to-noise-ratio, decreases monotonically with t.\nThese ingredients de\ufb01ne the forward process q(z|x), a Gaussian process satisfying the following\nMarkovian structure:\nq(zt|x) = N(zt; \u03b1tx, \u03c32\nt I),\nq(zt|zs) = N(zt; (\u03b1t/\u03b1s)zs, \u03c32\nt|sI)\n(1)\nwhere 0 \u2264s < t \u22641 and \u03c32\nt|s = (1 \u2212e\u03bbt\u2212\u03bbs)\u03c32\nt .\nThe role of function approximation in the diffusion model is to denoise zt \u223cq(zt|x) into an estimate\n\u02c6x\u03b8(zt) \u2248x (the function approximator also receives \u03bbt as an input, but we omit this to keep our\nnotation clean). We train this denoising model \u02c6x\u03b8 using a weighted mean squared error loss\nE\u03f5,t\n\u0002\nw(\u03bbt)\u2225\u02c6x\u03b8(zt) \u2212x\u22252\n2\n\u0003\n(2)\nover uniformly sampled times t \u2208[0, 1]. This loss can be justi\ufb01ed as a weighted variational lower\nbound on the data log likelihood under the diffusion model (Kingma et al., 2021) or as a form of\ndenoising score matching (Vincent, 2011; Song & Ermon, 2019). We will discuss particular choices\nof weighting function w(\u03bbt) later on.\nSampling from a trained model can be performed in several ways. The most straightforward way is\ndiscrete time ancestral sampling (Ho et al., 2020). To de\ufb01ne this sampler, \ufb01rst note that the forward\nprocess can be described in reverse as q(zs|zt, x) = N(zs; \u02dc\u00b5s|t(zt, x), \u02dc\u03c32\ns|tI) (noting s < t), where\n\u02dc\u00b5s|t(zt, x) = e\u03bbt\u2212\u03bbs(\u03b1s/\u03b1t)zt + (1 \u2212e\u03bbt\u2212\u03bbs)\u03b1sx,\n\u02dc\u03c32\ns|t = (1 \u2212e\u03bbt\u2212\u03bbs)\u03c32\ns\n(3)\nWe use this reversed description of the forward process to de\ufb01ne the ancestral sampler. Starting at\nz1 \u223cN(0, I), the ancestral sampler follows the rule\nzs = \u02dc\u00b5s|t(zt, \u02c6x\u03b8(zt)) +\nq\n(\u02dc\u03c32\ns|t)1\u2212\u03b3(\u03c32\nt|s)\u03b3)\u03f5\n(4)\n= e\u03bbt\u2212\u03bbs(\u03b1s/\u03b1t)zt + (1 \u2212e\u03bbt\u2212\u03bbs)\u03b1s\u02c6x\u03b8(zt) +\nq\n(\u02dc\u03c32\ns|t)1\u2212\u03b3(\u03c32\nt|s)\u03b3)\u03f5,\n(5)\n2\nPublished as a conference paper at ICLR 2022\nwhere \u03f5 is standard Gaussian noise, and \u03b3 is a hyperparameter that controls how much noise is added\nduring sampling, following Nichol & Dhariwal (2021).\nAlternatively, Song et al. (2021c) show that our denoising model \u02c6x\u03b8(zt) can be used to determinis-\ntically map noise z1 \u223cN(0, I) to samples x by numerically solving the probability \ufb02ow ODE:\ndzt = [f(zt, t) \u22121\n2g2(t)\u2207z log \u02c6p\u03b8(zt)]dt,\n(6)\nwhere \u2207z log \u02c6p\u03b8(zt) = \u03b1t\u02c6x\u03b8(zt)\u2212zt\n\u03c32\nt\n. Following Kingma et al. (2021), we have f(zt, t) = d log \u03b1t\ndt\nzt\nand g2(t) = d\u03c32\nt\ndt \u22122 d log \u03b1t\ndt\n\u03c32\nt . Since \u02c6x\u03b8(zt) is parameterized by a neural network, this equation\nis a special case of a neural ODE (Chen et al., 2018), also called a continuous normalizing \ufb02ow\n(Grathwohl et al., 2018).\nSolving the ODE in Equation 6 numerically can be done with standard methods like the Euler\nrule or the Runge-Kutta method. The DDIM sampler proposed by Song et al. (2021a) can also\nbe understood as an integration rule for this ODE, as we show in Appendix B, even though it was\noriginally proposed with a different motivation. The update rule speci\ufb01ed by DDIM is\nzs = \u03b1s\u02c6x\u03b8(zt) + \u03c3s\nzt \u2212\u03b1t\u02c6x\u03b8(zt)\n\u03c3t\n(7)\n= e(\u03bbt\u2212\u03bbs)/2(\u03b1s/\u03b1t)zt + (1 \u2212e(\u03bbt\u2212\u03bbs)/2)\u03b1s\u02c6x\u03b8(zt),\n(8)\nand in practice this rule performs better than the aforementioned standard ODE integration rules in\nour case, as we show in Appendix C.\nIf \u02c6x\u03b8(zt) satis\ufb01es mild smoothness conditions, the error introduced by numerical integration of the\nprobability \ufb02ow ODE is guaranteed to vanish as the number of integration steps grows in\ufb01nitely\nlarge, i.e. N \u2192\u221e. This leads to a trade-off in practice between the accuracy of the numerical\nintegration, and hence the quality of the produced samples from our model, and the time needed to\nproduce these samples. So far, most models in the literature have needed hundreds or thousands of\nintegration steps to produce their highest quality samples, which is prohibitive for many practical\napplications of generative modeling. Here, we therefore propose a method to distill these accurate,\nbut slow, ODE integrators into much faster models that are still very accurate. This idea is visualized\nin Figure 1, and described in detail in the next section.\n3\nPROGRESSIVE DISTILLATION\nTo make diffusion models more ef\ufb01cient at sampling time, we propose progressive distillation: an\nalgorithm that iteratively halves the number of required sampling steps by distilling a slow teacher\ndiffusion model into a faster student model. Our implementation of progressive distillation stays\nvery close to the implementation for training the original diffusion model, as described by e.g.\nHo et al. (2020). Algorithm 1 and Algorithm 2 present diffusion model training and progressive\ndistillation side-by-side, with the relative changes in progressive distillation highlighted in green.\nWe start the progressive distillation procedure with a teacher diffusion model that is obtained by\ntraining in the standard way. At every iteration of progressive distillation, we then initialize the\nstudent model with a copy of the teacher, using both the same parameters and same model de\ufb01nition.\nLike in standard training, we then sample data from the training set and add noise to it, before\nforming the training loss by applying the student denoising model to this noisy data zt. The main\ndifference in progressive distillation is in how we set the target for the denoising model: instead\nof the original data x, we have the student model denoise towards a target \u02dcx that makes a single\nstudent DDIM step match 2 teacher DDIM steps. We calculate this target value by running 2 DDIM\nsampling steps using the teacher, starting from zt and ending at zt\u22121/N, with N being the number of\nstudent sampling steps. By inverting a single step of DDIM, we then calculate the value the student\nmodel would need to predict in order to move from zt to zt\u22121/N in a single step, as we show in\ndetail in Appendix G. The resulting target value \u02dcx(zt) is fully determined given the teacher model\nand starting point zt, which allows the student model to make a sharp prediction when evaluated at\nzt. In contrast, the original data point x is not fully determined given zt, since multiple different\ndata points x can produce the same noisy data zt: this means that the original denoising model is\n3\nPublished as a conference paper at ICLR 2022\npredicting a weighted average of possible x values, which produces a blurry prediction. By making\nsharper predictions, the student model can make faster progress during sampling.\nAfter running distillation to learn a student model taking N sampling steps, we can repeat the pro-\ncedure with N/2 steps: The student model then becomes the new teacher, and a new student model\nis initialized by making a copy of this model.\nUnlike our procedure for training the original model, we always run progressive distillation in dis-\ncrete time: we sample this discrete time such that the highest time index corresponds to a signal-to-\nnoise ratio of zero, i.e. \u03b11 = 0, which exactly matches the distribution of input noise z1 \u223cN(0, I)\nthat is used at test time. We found this to work slightly better than starting from a non-zero signal-\nto-noise ratio as used by e.g. Ho et al. (2020), both for training the original model as well as when\nperforming progressive distillation.\nAlgorithm 1 Standard diffusion training\nRequire: Model \u02c6x\u03b8(zt) to be trained\nRequire: Data set D\nRequire: Loss weight function w()\nwhile not converged do\nx \u223cD\n\u25b7Sample data\nt \u223cU[0, 1]\n\u25b7Sample time\n\u03f5 \u223cN(0, I)\n\u25b7Sample noise\nzt = \u03b1tx + \u03c3t\u03f5\n\u25b7Add noise to data\n\u02dcx = x\n\u25b7Clean data is target for \u02c6x\n\u03bbt = log[\u03b12\nt /\u03c32\nt ]\n\u25b7log-SNR\nL\u03b8 = w(\u03bbt)\u2225\u02dcx \u2212\u02c6x\u03b8(zt)\u22252\n2\n\u25b7Loss\n\u03b8 \u2190\u03b8 \u2212\u03b3\u2207\u03b8L\u03b8\n\u25b7Optimization\nend while\nAlgorithm 2 Progressive distillation\nRequire: Trained teacher model \u02c6x\u03b7(zt)\nRequire: Data set D\nRequire: Loss weight function w()\nRequire: Student sampling steps N\nfor K iterations do\n\u03b8 \u2190\u03b7\n\u25b7Init student from teacher\nwhile not converged do\nx \u223cD\nt = i/N, i \u223cCat[1, 2, . . . , N]\n\u03f5 \u223cN(0, I)\nzt = \u03b1tx + \u03c3t\u03f5\n# 2 steps of DDIM with teacher\nt\u2032 = t \u22120.5/N,\nt\u2032\u2032 = t \u22121/N\nzt\u2032 = \u03b1t\u2032 \u02c6x\u03b7(zt) + \u03c3t\u2032\n\u03c3t (zt \u2212\u03b1t\u02c6x\u03b7(zt))\nzt\u2032\u2032 = \u03b1t\u2032\u2032 \u02c6x\u03b7(zt\u2032) + \u03c3t\u2032\u2032\n\u03c3t\u2032 (zt\u2032 \u2212\u03b1t\u2032 \u02c6x\u03b7(zt\u2032))\n\u02dcx = zt\u2032\u2032\u2212(\u03c3t\u2032\u2032/\u03c3t)zt\n\u03b1t\u2032\u2032\u2212(\u03c3t\u2032\u2032/\u03c3t)\u03b1t\n\u25b7Teacher \u02c6x target\n\u03bbt = log[\u03b12\nt /\u03c32\nt ]\nL\u03b8 = w(\u03bbt)\u2225\u02dcx \u2212\u02c6x\u03b8(zt)\u22252\n2\n\u03b8 \u2190\u03b8 \u2212\u03b3\u2207\u03b8L\u03b8\nend while\n\u03b7 \u2190\u03b8\n\u25b7Student becomes next teacher\nN \u2190N/2 \u25b7Halve number of sampling steps\nend for\n4\nDIFFUSION MODEL PARAMETERIZATION AND TRAINING LOSS\nIn this section, we discuss how to parameterize the denoising model \u02c6x\u03b8, and how to specify the\nreconstruction loss weight w(\u03bbt). We assume a standard variance-preserving diffusion process for\nwhich \u03c32\nt = 1 \u2212\u03b12\nt . This is without loss of generalization, as shown by (Kingma et al., 2021,\nappendix G): different speci\ufb01cations of the diffusion process, such as the variance-exploding spec-\ni\ufb01cation, can be considered equivalent to this speci\ufb01cation, up to rescaling of the noisy latents zt.\nWe use a cosine schedule \u03b1t = cos(0.5\u03c0t), similar to that introduced by Nichol & Dhariwal (2021).\nHo et al. (2020) and much of the following work choose to parameterize the denoising model through\ndirectly predicting \u03f5 with a neural network \u02c6\u03f5\u03b8(zt), which implicitly sets \u02c6x\u03b8(zt) =\n1\n\u03b1t (zt\u2212\u03c3t\u02c6\u03f5\u03b8(zt)).\nIn this case, the training loss is also usually de\ufb01ned as mean squared error in the \u03f5-space:\nL\u03b8 = \u2225\u03f5 \u2212\u02c6\u03f5\u03b8(zt)\u22252\n2 =\n\r\r\r\r\n1\n\u03c3t\n(zt \u2212\u03b1tx) \u22121\n\u03c3t\n(zt \u2212\u03b1t\u02c6x\u03b8(zt))\n\r\r\r\r\n2\n2\n= \u03b12\nt\n\u03c32\nt\n\u2225x \u2212\u02c6x\u03b8(zt)\u22252\n2,\n(9)\nwhich can thus equivalently be seen as a weighted reconstruction loss in x-space, where the weight-\ning function is given by w(\u03bbt) = exp(\u03bbt), for log signal-to-noise ratio \u03bbt = log[\u03b12\nt /\u03c32\nt ].\n4\nPublished as a conference paper at ICLR 2022\nAlthough this standard speci\ufb01cation works well for training the original model, it is not well suited\nfor distillation: when training the original diffusion model, and at the start of progressive distillation,\nthe model is evaluated at a wide range of signal-to-noise ratios \u03b12\nt /\u03c32\nt , but as distillation progresses\nwe increasingly evaluate at lower and lower signal-to-noise ratios. As the signal-to-noise ratio goes\nto zero, the effect of small changes in the neural network output \u02c6\u03f5\u03b8(zt) on the implied prediction in\nx-space is increasingly ampli\ufb01ed, since \u02c6x\u03b8(zt) =\n1\n\u03b1t (zt \u2212\u03c3t\u02c6\u03f5\u03b8(zt)) divides by \u03b1t \u21920. This is not\nmuch of a problem when taking many steps, since the effect of early missteps is limited by clipping\nof the zt iterates, and later updates can correct any mistakes, but it becomes increasingly important\nas we decrease the number of sampling steps. Eventually, if we distill all the way down to a single\nsampling step, the input to the model is only pure noise \u03f5, which corresponds to a signal-to-noise\nratio of zero, i.e. \u03b1t = 0, \u03c3t = 1. At this extreme, the link between \u03f5-prediction and x-prediction\nbreaks down completely: observed data zt = \u03f5 is no longer informative of x and predictions \u02c6\u03f5\u03b8(zt)\nno longer implicitly predict x. Examining our reconstruction loss (equation 9), we see that the\nweighting function w(\u03bbt) gives zero weight to the reconstruction loss at this signal-to-noise ratio.\nFor distillation to work, we thus need to parameterize the diffusion model in a way for which the\nimplied prediction \u02c6x\u03b8(zt) remains stable as \u03bbt = log[\u03b12\nt /\u03c32\nt ] varies. We tried the following options,\nand found all to work well with progressive distillation:\n\u2022 Predicting x directly.\n\u2022 Predicting both x and \u03f5, via separate output channels {\u02dcx\u03b8(zt), \u02dc\u03f5\u03b8(zt)} of the neural net-\nwork, and then merging the predictions via \u02c6x = \u03c32\nt \u02dcx\u03b8(zt) + \u03b1t(zt \u2212\u03c3t\u02dc\u03f5\u03b8(zt)), thus\nsmoothly interpolating between predicting x directly and predicting via \u03f5.\n\u2022 Predicting v \u2261\u03b1t\u03f5 \u2212\u03c3tx, which gives \u02c6x = \u03b1tzt \u2212\u03c3t\u02c6v\u03b8(zt), as we show in Appendix D.\nIn Section 5.1 we test all three parameterizations on training an original diffusion model (no distil-\nlation), and \ufb01nd them to work well there also.\nIn addition to determining an appropriate parameterization, we also need to decide on a reconstruc-\ntion loss weighting w(\u03bbt). The setup of Ho et al. (2020) weights the reconstruction loss by the\nsignal-to-noise ratio, implicitly gives a weight of zero to data with zero SNR, and is therefore not a\nsuitable choice for distillation. We consider two alternative training loss weightings:\n\u2022 L\u03b8 = max(\u2225x \u2212\u02c6xt\u22252\n2, \u2225\u03f5 \u2212\u02c6\u03f5t\u22252\n2) = max( \u03b12\nt\n\u03c32\nt , 1)\u2225x \u2212\u02c6xt\u22252\n2; \u2018truncated SNR\u2019 weighting.\n\u2022 L\u03b8 = \u2225vt \u2212\u02c6vt\u22252\n2 = (1 + \u03b12\nt\n\u03c32\nt )\u2225x \u2212\u02c6xt\u22252\n2; \u2018SNR+1\u2019 weighting.\nWe examine both choices in our ablation study in Section 5.1, and \ufb01nd both to be good choices for\ntraining diffusion models. In practice, the choice of loss weighting also has to take into account\nhow \u03b1t, \u03c3t are sampled during training, as this sampling distribution strongly determines the weight\nthe expected loss gives to each signal-to-noise ratio. Our results are for a cosine schedule \u03b1t =\ncos(0.5\u03c0t), where time is sampled uniformly from [0, 1]. In Figure 2 we visualize the resulting loss\nweightings, both including and excluding the effect of the cosine schedule.\n5\nEXPERIMENTS\nIn this section we empirically validate the progressive distillation algorithm proposed in Section 3,\nas well as the parameterizations and loss weightings considered in Section 4. We consider various\nimage generation benchmarks, with resolution varying from 32 \u00d7 32 to 128 \u00d7 128. All experiments\nuse the cosine schedule \u03b1t = cos(0.5\u03c0t), and all models use a U-Net architecture similar to that\nintroduced by Ho et al. (2020), but with BigGAN-style up- and downsampling (Brock et al., 2019),\nas used in the diffusion modeling setting by Nichol & Dhariwal (2021); Song et al. (2021c). Our\ntraining setup closely matches the open source code by Ho et al. (2020). Exact details are given in\nAppendix E.\n5.1\nMODEL PARAMETERIZATION AND TRAINING LOSS\nAs explained in Section 4, the standard method of having our model predict \u03f5, and minimizing mean\nsquared error in the \u03f5-space (Ho et al., 2020), is not appropriate for use with progressive distillation.\n5\nPublished as a conference paper at ICLR 2022\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n\u22125\n0\n5\nlog SNR\nlog weight (excluding schedule)\nSNR weight\ntruncated SNR\nSNR+1 weight\n\u22126\n\u22124\n\u22122\n0\n2\n4\n6\n0\n0.1\n0.2\n0.3\n0.4\nlog SNR\nweight (including schedule)\nSNR\ntruncated SNR\nSNR+1\nFigure 2: Left: Log weight assigned to reconstruction loss \u2225x \u2212\u02c6x\u03bb\u22252\n2 as a function of the log-SNR\n\u03bb = log[\u03b12/\u03c32], for each of our considered training loss weightings, excluding the in\ufb02uence of\nthe \u03b1t, \u03c3t schedule. Right: Weights assigned to the reconstruction loss including the effect of the\ncosine schedule \u03b1t = cos(0.5\u03c0t), with t \u223cU[0, 1]. The weights are only de\ufb01ned up to a constant,\nand we have adjusted these constants to \ufb01t this graph.\nNetwork Output\nLoss Weighting\nStochastic sampler\nDDIM sampler\n(x, \u03f5) combined\nSNR\n2.54/9.88\n2.78/9.56\nTruncated SNR\n2.47/9.85\n2.76/9.49\nSNR+1\n2.52/9.79\n2.87/9.45\nx\nSNR\n2.65/9.80\n2.75/9.56\nTruncated SNR\n2.53/9.92\n2.51/9.58\nSNR+1\n2.56/9.84\n2.65/9.52\n\u03f5\nSNR\n2.59/9.84\n2.91/9.52\nTruncated SNR\nN/A\nN/A\nSNR+1\n2.56/9.77\n3.27/9.41\nv\nSNR\n2.65/9.86\n3.05/9.56\nTruncated SNR\n2.45/9.80\n2.75/9.52\nSNR+1\n2.49/9.77\n2.87/9.43\nTable 1: Generated sample quality as measured by FID and Inception Score (FID/IS) on uncondi-\ntional CIFAR-10, training the original model (no distillation), and comparing different parameteri-\nzations and loss weightings discussed in Section 4. All reported results are averages over 3 random\nseeds of the best metrics obtained over 2 million training steps; nevertheless we \ufb01nd results are still\n\u00b10.1 due to the noise inherent in training our models. Taking the neural network output to represent\na prediction of \u03f5 in combination with the Truncated SNR loss weighting leads to divergence.\nWe therefore proposed various alternative parameterizations of the denoising diffusion model that\nare stable under the progressive distillation procedure, as well as various weighting functions for\nthe reconstruction error in x-space. Here, we perform a complete ablation experiment of all pa-\nrameterizations and loss weightings considered in Section 4. For computational ef\ufb01ciency, and for\ncomparisons to established methods in the literature, we use unconditional CIFAR-10 as the bench-\nmark. We measure performance of undistilled models trained from scratch, to avoid introducing too\nmany factors of variation into our analysis.\nTable 1 lists the results of the ablation study. Overall results are fairly close across different pa-\nrameterizations and loss weights. All proposed stable model speci\ufb01cations achieve excellent perfor-\nmance, with the exception of the combination of outputting \u03f5 with the neural network and weighting\nthe loss with the truncated SNR, which we \ufb01nd to be unstable. Both predicting x directly, as well\nas predicting v, or the combination (\u03f5, x), could thus be recommended for speci\ufb01cation of diffusion\n6\nPublished as a conference paper at ICLR 2022\nmodels. Here, predicting v is the most stable option, as it has the unique property of making DDIM\nstep-sizes independent of the SNR (see Appendix D), but predicting x gives slightly better empirical\nresults in this ablation study.\n5.2\nPROGRESSIVE DISTILLATION\nWe evaluate our proposed progressive distillation algorithm on 4 data sets: CIFAR-10, 64 \u00d7 64\ndownsampled ImageNet, 128 \u00d7 128 LSUN bedrooms, and 128 \u00d7 128 LSUN Church-Outdoor. For\neach data set we start by training a baseline model, after which we start the progressive distillation\nprocedure. For CIFAR-10 we start progressive distillation from a teacher model taking 8192 steps.\nFor the bigger data sets we start at 1024 steps. At every iteration of distillation we train for 50\nthousand parameter updates, except for the distillation to 2 and 1 sampling steps, for which we use\n100 thousand updates. We report FID results obtained after each iteration of the algorithm. Using\nthese settings, the computational cost of progressive distillation to 4 sampling steps is comparable\nor less than for training the original model. In Appendix I we show that this computational cost can\nbe reduce much further still, at a small cost in performance.\nIn Figure 4 we plot the resulting FID scores (Heusel et al., 2017) obtained for each number of\nsampling steps. We compare against the undistilled DDIM sampler, as well as to a highly optimized\nstochastic baseline sampler. For all four data sets, progressive distillation produces near optimal\nresults up to 4 or 8 sampling steps. At 2 or 1 sampling steps, the sample quality degrades relatively\nmore quickly. In contrast, the quality of the DDIM and stochastic samplers degrades very sharply\nafter reducing the number of sampling steps below 128. Overall, we conclude that progressive\ndistillation is thus an attractive solution for computational budgets that allow less than or equal to\n128 sampling steps. Although our distillation procedure is designed for use with the DDIM sampler,\nthe resulting distilled models can in principle also be used with stochastic sampling: we investigate\nthis in Appendix F, and \ufb01nd that it achieves performance that falls in between the distilled DDIM\nsampler and the undistilled stochastic sampler.\nTable 2 shows some of our results on CIFAR-10, and compares against other fast sampling methods\nin the literature: Our method compares favorably and attains higher sampling quality in fewer steps\nthan most of the alternative methods. Figure 3 shows some random samples from our model obtained\nat different phases of the distillation process. Additional samples are provided in Appendix H.\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 3: Random samples from our distilled 64 \u00d7 64 ImageNet models, conditioned on the \u2018mala-\nmute\u2019 class, for \ufb01xed random seed and for varying number of sampling steps. The mapping from\ninput noise to output image is well preserved as the number of sampling steps is reduced.\n6\nRELATED WORK ON FAST SAMPLING\nOur proposed method is closest to the work of Luhman & Luhman (2021), who perform distillation\nof DDIM teacher models into one-step student models. A possible downside of their method is\n7\nPublished as a conference paper at ICLR 2022\n512\n256\n128\n64\n32\n16\n8\n4\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\nCIFAR-10\nDistilled\nDDIM\nStochastic\n512\n256\n128\n64\n32\n16\n8\n4\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\n64x64 ImageNet\nDistilled\nDDIM\nStochastic\n512\n256\n128\n64\n32\n16\n8\n4\n2\n1\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\n128x128 LSUN Bedrooms\nDistilled\nDDIM\nStochastic\n512\n256\n128\n64\n32\n16\n8\n4\n2\n1\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\n128x128 LSUN Church-Outdoor\nDistilled\nDDIM\nStochastic\nFigure 4: Sample quality results as measured by FID for our distilled model on unconditional\nCIFAR-10, class-conditional 64x64 ImageNet, 128x128 LSUN bedrooms, and 128x128 LSUN\nchurch-outdoor. We compare against the DDIM sampler and against an optimized stochastic sam-\npler, each evaluated using the same models that were used to initialize the progressive distillation\nprocedure. For CIFAR-10 we report an average over 4 random seeds. For the other data sets we\nonly use a single run because of their computational demand. For the stochastic sampler we set the\nvariance as a log-scale interpolation between an upper and lower bound on the variance, follow-\ning Nichol & Dhariwal (2021), but we use a single interpolation coef\ufb01cient rather than a learned\ncoef\ufb01cient. We then tune this interpolation coef\ufb01cient separately for each number of sampling steps\nand report only the best result for that number of steps: this way we obtained better results than with\nthe learned interpolation.\nthat it requires constructing a large data set by running the original model at its full number of\nsampling steps: their cost of distillation thus scales linearly with this number of steps, which can\nbe prohibitive. In contrast, our method never needs to run the original model at the full number\nof sampling steps: at every iteration of progressive distillation, the number of model evaluations\nis independent of the number of teacher sampling steps, allowing our method to scale up to large\nnumbers of teacher steps at a logarithmic cost in total distillation time.\nDDIM (Song et al., 2021a) was originally shown to be effective for few-step sampling, as was the\nprobability \ufb02ow sampler (Song et al., 2021c). Jolicoeur-Martineau et al. (2021) study fast SDE\nintegrators for reverse diffusion processes, and Tzen & Raginsky (2019b) study unbiased samplers\nwhich may be useful for fast, high quality sampling as well.\n8\nPublished as a conference paper at ICLR 2022\nOther work on fast sampling can be viewed as manual or automated methods to adjust samplers or\ndiffusion processes for fast generation. Nichol & Dhariwal (2021); Kong & Ping (2021) describe\nmethods to adjust a discrete time diffusion model trained on many timesteps into models that can\nsample in few timesteps. Watson et al. (2021) describe a dynamic programming algorithm to reduce\nthe number of timesteps for a diffusion model in a way that is optimal for log likelihood. Chen et al.\n(2021); Saharia et al. (2021); Ho et al. (2021) train diffusion models over continuous noise levels and\ntune samplers post training by adjusting the noise levels of a few-step discrete time reverse diffusion\nprocess. Their method is effective in highly conditioned settings such as text-to-speech and image\nsuper-resolution. San-Roman et al. (2021) train a new network to estimate the noise level of noisy\ndata and show how to use this estimate to speed up sampling.\nAlternative speci\ufb01cations of the diffusion model can also lend themselves to fast sampling, such\nas modi\ufb01ed forward and reverse processes (Nachmani et al., 2021; Lam et al., 2021) and training\ndiffusion models in latent space (Vahdat et al., 2021).\nMethod\nModel evaluations\nFID\nProgressive Distillation (ours)\n1\n9.12\n2\n4.51\n4\n3.00\n8\n2.57\nKnowledge distillation (Luhman & Luhman, 2021)\n1\n9.36\nDDIM (Song et al., 2021a)\n10\n13.36\n20\n6.84\n50\n4.67\n100\n4.16\nDynamic step-size extrapolation + VP-deep\n48\n82.42\n(Jolicoeur-Martineau et al., 2021)\n151\n2.73\n180\n2.44\n274\n2.60\n330\n2.56\nFastDPM (Kong & Ping, 2021)\n10\n9.90\n20\n5.05\n50\n3.20\n100\n2.86\nImproved DDPM respacing\n25\n7.53\n(Nichol & Dhariwal, 2021), our reimplementation\n50\n4.99\nLSGM (Vahdat et al., 2021)\n138\n2.10\nTable 2: Comparison of fast sampling results on CIFAR-10 for diffusion models in the literature.\n7\nDISCUSSION\nWe have presented progressive distillation, a method to drastically reduce the number of sampling\nsteps required for high quality generation of images, and potentially other data, using diffusion\nmodels with deterministic samplers like DDIM (Song et al., 2020). By making these models cheaper\nto run at test time, we hope to increase their usefulness for practical applications, for which running\ntime and computational requirements often represent important constraints.\nIn the current work we limited ourselves to setups where the student model has the same architecture\nand number of parameters as the teacher model: in future work we hope to relax this constraint\nand explore settings where the student model is smaller, potentially enabling further gains in test\ntime computational requirements. In addition, we hope to move past the generation of images and\nalso explore progressive distillation of diffusion models for different data modalities such as e.g.\naudio (Chen et al., 2021).\nIn addition to the proposed distillation procedure, some of our progress was realized through differ-\nent parameterizations of the diffusion model and its training loss. We expect to see more progress in\nthis direction as the community further explores this model class.\n9\nPublished as a conference paper at ICLR 2022\nREPRODUCIBILITY STATEMENT\nWe provide full details on model architectures, training procedures, and hyperparameters in Ap-\npendix E, in addition to our discussion in Section 5. In Algorithm 2 we provide fairly detailed pseu-\ndocode that closely matches our actual implementation, which is available in open source at https:\n//github.com/google-research/google-research/tree/master/diffusion_distillation.\nETHICS STATEMENT\nIn general, generative models can have unethical uses, such as fake content generation, and they\ncan suffer from bias if applied to data sets that are not carefully curated. The focus of this paper\nspeci\ufb01cally is on speeding up generative models at test time in order to reduce their computational\ndemands; we do not have speci\ufb01c concerns with regards to this contribution.\nREFERENCES\nJacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. Structured\ndenoising diffusion models in discrete state-spaces. CoRR, abs/2107.03006, 2021.\nAndrew Brock, Jeff Donahue, and Karen Simonyan. Large scale GAN training for high \ufb01delity\nnatural image synthesis. In International Conference on Learning Representations, 2019.\nRuojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely,\nand Bharath Hariharan.\nLearning gradient \ufb01elds for shape generation.\narXiv preprint\narXiv:2008.06520, 2020.\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J Weiss, Mohammad Norouzi, and William Chan. Wave-\nGrad: Estimating gradients for waveform generation. International Conference on Learning Rep-\nresentations, 2021.\nTian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differ-\nential equations. In Advances in Neural Information Processing Systems, pp. 6571\u20136583, 2018.\nPrafulla Dhariwal and Alex Nichol. Diffusion models beat GANs on image synthesis. arXiv preprint\narXiv:2105.05233, 2021.\nWill Grathwohl, Ricky TQ Chen, Jesse Bettencourt, Ilya Sutskever, and David Duvenaud. Ffjord:\nFree-form continuous dynamics for scalable reversible generative models.\narXiv preprint\narXiv:1810.01367, 2018.\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.\nGANs trained by a two time-scale update rule converge to a local Nash equilibrium. In Advances\nin Neural Information Processing Systems, pp. 6626\u20136637, 2017.\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances\nin Neural Information Processing Systems, pp. 6840\u20136851, 2020.\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim\nSalimans.\nCascaded diffusion models for high \ufb01delity image generation.\narXiv preprint\narXiv:2106.15282, 2021.\nEmiel Hoogeboom, Didrik Nielsen, Priyank Jaini, Patrick Forr\u00e9, and Max Welling.\nArgmax\n\ufb02ows and multinomial diffusion: Towards non-autoregressive language models. arXiv preprint\narXiv:2102.05379, 2021.\nAlexia Jolicoeur-Martineau, Ke Li, R\u00e9mi Pich\u00e9-Taillefer, Tal Kachman, and Ioannis Mitliagkas.\nGotta go fast when generating data with score-based models. arXiv preprint arXiv:2105.14080,\n2021.\nDiederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models.\narXiv preprint arXiv:2107.00630, 2021.\n10\nPublished as a conference paper at ICLR 2022\nZhifeng Kong and Wei Ping. On fast sampling of diffusion probabilistic models. arXiv preprint\narXiv:2106.00132, 2021.\nMax WY Lam, Jun Wang, Rongjie Huang, Dan Su, and Dong Yu. Bilateral denoising diffusion\nmodels. arXiv preprint arXiv:2108.11514, 2021.\nHaoying Li, Yifan Yang, Meng Chang, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen.\nSrdiff:\nSingle image super-resolution with diffusion probabilistic models.\narXiv preprint\narXiv:2104.14951, 2021.\nIlya Loshchilov and Frank Hutter.\nDecoupled weight decay regularization.\narXiv preprint\narXiv:1711.05101, 2017.\nEric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved\nsampling speed. arXiv preprint arXiv:2101.02388, 2021.\nEliya Nachmani, Robin San Roman, and Lior Wolf. Non gaussian denoising diffusion models. arXiv\npreprint arXiv:2106.07582, 2021.\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models.\nIn Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on\nMachine Learning, ICML, 2021.\nChenhao Niu, Yang Song, Jiaming Song, Shengjia Zhao, Aditya Grover, and Stefano Ermon. Per-\nmutation invariant graph generation via score-based generative modeling. In International Con-\nference on Arti\ufb01cial Intelligence and Statistics, pp. 4474\u20134484. PMLR, 2020.\nPrajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang,\nYang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast genera-\ntion for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017.\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad\nNorouzi. Image super-resolution via iterative re\ufb01nement. arXiv preprint arXiv:2104.07636, 2021.\nRobin San-Roman, Eliya Nachmani, and Lior Wolf. Noise estimation for generative diffusion mod-\nels. arXiv preprint arXiv:2104.02600, 2021.\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised\nlearning using nonequilibrium thermodynamics. In International Conference on Machine Learn-\ning, pp. 2256\u20132265, 2015.\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. Interna-\ntional Conference on Learning Representations, 2021a.\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution.\nIn Advances in Neural Information Processing Systems, pp. 11895\u201311907, 2019.\nYang Song and Stefano Ermon. Improved techniques for training score-based generative. Advances\nin Neural Information Processing Systems, 2020.\nYang Song, Conor Durkan, Iain Murray, and Stefano Ermon. Maximum likelihood training of score-\nbased diffusion models. arXiv e-prints, pp. arXiv\u20132101, 2021b.\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben\nPoole. Score-based generative modeling through stochastic differential equations. International\nConference on Learning Representations, 2021c.\nYuxuan Song, Qiwei Ye, Minkai Xu, and Tie-Yan Liu.\nDiscriminator contrastive divergence:\nSemi-amortized generative modeling by exploring energy of the discriminator. arXiv preprint\narXiv:2004.01704, 2020.\nBelinda Tzen and Maxim Raginsky. Neural stochastic differential equations: Deep latent gaussian\nmodels in the diffusion limit. arXiv preprint arXiv:1905.09883, 2019a.\n11\nPublished as a conference paper at ICLR 2022\nBelinda Tzen and Maxim Raginsky. Theoretical guarantees for sampling and inference in generative\nmodels with latent diffusions. In Conference on Learning Theory, pp. 3084\u20133114. PMLR, 2019b.\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. arXiv\npreprint arXiv:2106.05931, 2021.\nPascal Vincent. A connection between score matching and denoising autoencoders. Neural Compu-\ntation, 23(7):1661\u20131674, 2011.\nDaniel Watson, Jonathan Ho, Mohammad Norouzi, and William Chan. Learning to ef\ufb01ciently sam-\nple from diffusion probabilistic models. arXiv preprint arXiv:2106.03802, 2021.\n12\nPublished as a conference paper at ICLR 2022\nA\nPROBABILITY FLOW ODE IN TERMS OF LOG-SNR\nSong et al. (2021c) formulate the forward diffusion process in terms of an SDE of the form\ndz = f(z, t)dt + g(t)dW,\n(10)\nand show that samples from this diffusion process can be generated by solving the associated prob-\nability \ufb02ow ODE:\ndz = [f(z, t) \u22121\n2g2(t)\u2207z log pt(z)]dt,\n(11)\nwhere in practice \u2207z log pt(z) is approximated by a learned denoising model using\n\u2207z log pt(z) \u2248\u03b1t\u02c6x\u03b8(zt) \u2212zt\n\u03c32\nt\n.\n(12)\nFollowing Kingma et al. (2021) we have f(z, t) = d log \u03b1t\ndt\nzt and g2(t) = d\u03c32\nt\ndt \u22122 d log \u03b1t\ndt\n\u03c32\nt . Assum-\ning a variance preserving diffusion process with \u03b12\nt = 1 \u2212\u03c32\nt = sigmoid(\u03bbt) for \u03bbt = log[\u03b12\nt /\u03c32\nt ]\n(without loss of generality, see Kingma et al. (2021)), we get\nf(z, t) = d log \u03b1t\ndt\nzt = 1\n2\nd log \u03b12\n\u03bb\nd\u03bb\nd\u03bb\ndt zt = 1\n2(1 \u2212\u03b12\nt )d\u03bb\ndt zt = 1\n2\u03c32\nt\nd\u03bb\ndt zt.\n(13)\nSimilarly, we get\ng2(t) = d\u03c32\nt\ndt \u22122d log \u03b1t\ndt\n\u03c32\nt = d\u03c32\n\u03bb\nd\u03bb\nd\u03bb\ndt \u2212\u03c34\nt\nd\u03bb\ndt = (\u03c34\nt \u2212\u03c32\nt )d\u03bb\ndt \u2212\u03c34\nt\nd\u03bb\ndt = \u2212\u03c32\nt\nd\u03bb\ndt .\n(14)\nPlugging these into the probability \ufb02ow ODE then gives\ndz = [f(z, t) \u22121\n2g2(t)\u2207z log pt(z)]dt\n(15)\n= 1\n2\u03c32\n\u03bb[z\u03bb + \u2207z log p\u03bb(z)]d\u03bb.\n(16)\nPlugging in our function approximation from Equation 12 gives\ndz = 1\n2\u03c32\n\u03bb\n\u0014\nz\u03bb +\n\u0012\u03b1\u03bb\u02c6x\u03b8(z\u03bb) \u2212z\u03bb\n\u03c32\n\u03bb\n\u0013\u0015\nd\u03bb\n(17)\n= 1\n2[\u03b1\u03bb\u02c6x\u03b8(z\u03bb) + (\u03c32\n\u03bb \u22121)z\u03bb]d\u03bb\n(18)\n= 1\n2[\u03b1\u03bb\u02c6x\u03b8(z\u03bb) \u2212\u03b12\n\u03bbz\u03bb]d\u03bb.\n(19)\nB\nDDIM IS AN INTEGRATOR OF THE PROBABILITY FLOW ODE\nThe DDIM update rule (Song & Ermon, 2020) is given by\nzs = \u03c3s\n\u03c3t\n[zt \u2212\u03b1t\u02c6x\u03b8(zt)] + \u03b1s\u02c6x\u03b8(zt),\n(20)\nfor s < t. Taking the derivative of this expression with respect to \u03bbs, assuming again a variance\npreserving diffusion process, and using d\u03b1\u03bb\nd\u03bb = 1\n2\u03b1\u03bb\u03c32\n\u03bb and d\u03c3\u03bb\nd\u03bb = \u22121\n2\u03c3\u03bb\u03b12\n\u03bb, gives\nz\u03bbs\nd\u03bbs\n= d\u03c3\u03bbs\nd\u03bbs\n1\n\u03c3t\n[zt \u2212\u03b1t\u02c6x\u03b8(zt)] + d\u03b1\u03bbs\nd\u03bbs\n\u02c6x\u03b8(zt)\n(21)\n= \u22121\n2\u03b12\ns\n\u03c3s\n\u03c3t\n[zt \u2212\u03b1t\u02c6x\u03b8(zt)] + 1\n2\u03b1s\u03c32\ns \u02c6x\u03b8(zt).\n(22)\nEvaluating this derivative at s = t then gives\nz\u03bbs\nd\u03bbs\n|s=t = \u22121\n2\u03b12\n\u03bb[z\u03bb \u2212\u03b1\u03bb\u02c6x\u03b8(z\u03bb)] + 1\n2\u03b1\u03bb\u03c32\n\u03bb\u02c6x\u03b8(z\u03bb)\n(23)\n= \u22121\n2\u03b12\n\u03bb[z\u03bb \u2212\u03b1\u03bb\u02c6x\u03b8(z\u03bb)] + 1\n2\u03b1\u03bb(1 \u2212\u03b12\n\u03bb)\u02c6x\u03b8(z\u03bb)\n(24)\n= 1\n2[\u03b1\u03bb\u02c6x\u03b8(z\u03bb) \u2212\u03b12\n\u03bbz\u03bb].\n(25)\nComparison with Equation 19 now shows that DDIM follows the probability \ufb02ow ODE up to \ufb01rst\norder, and can thus be considered as an integration rule for this ODE.\n13\nPublished as a conference paper at ICLR 2022\nC\nEVALUATION OF INTEGRATORS OF THE PROBABILITY FLOW ODE\nIn a preliminary investigation we tried several numerical integrators for the probability \ufb02ow ODE.\nAs our model we used a pre-trained class-conditional 128x128 ImageNet model following the de-\nscription in Ho et al. (2020). We tried a simple Euler integrator, RK4 (the \u201cclassic\u201d 4th order\nRunge\u2013Kutta integrator), and DDIM (Song et al., 2021a). In addition we compared to a Gaussian\nsampler with variance equal to the lower bound given by Ho et al. (2020). We calculated FID scores\non just 5000 samples, hence our results in this experiment are not comparable to results reported in\nthe literature. This preliminary investigation gave the results listed in Table 3 and identi\ufb01ed DDIM\nas the best integrator in terms of resulting sample quality.\nSampler\nNumber of steps\nFID\nStochastic\n1000\n13.35\nEuler\n1000\n16.5\nRK4\n1000\n16.33\nDDIM\n1000\n15.98\nStochastic\n100\n18.44\nEuler\n100\n23.67\nRK4\n100\n18.94\nDDIM\n100\n16.35\nTable 3: Preliminary FID scores on 128 \u00d7 128 ImageNet for various integrators of the probability\n\ufb02ow ODE, and compared against a stochastic sampler. Model speci\ufb01cation and noise schedule\nfollow Ho et al. (2020).\nD\nEXPRESSION OF DDIM IN ANGULAR PARAMETERIZATION\nWe can simplify the DDIM update rule by expressing it in terms of \u03c6t = arctan(\u03c3t/\u03b1t), rather than\nin terms of time t or log-SNR \u03bbt, as we show here.\nGiven our de\ufb01nition of \u03c6, and assuming a variance preserving diffusion process, we have \u03b1\u03c6 =\ncos(\u03c6), \u03c3\u03c6 = sin(\u03c6), and hence z\u03c6 = cos(\u03c6)x + sin(\u03c6)\u03f5. We can now de\ufb01ne the velocity of z\u03c6 as\nv\u03c6 \u2261dz\u03c6\nd\u03c6 = d cos(\u03c6)\nd\u03c6\nx + d sin(\u03c6)\nd\u03c6\n\u03f5 = cos(\u03c6)\u03f5 \u2212sin(\u03c6)x.\n(26)\nRearranging \u03f5, x, v, we then get\nsin(\u03c6)x = cos(\u03c6)\u03f5 \u2212v\u03c6\n(27)\n= cos(\u03c6)\nsin(\u03c6) (z \u2212cos(\u03c6)x) \u2212v\u03c6\n(28)\nsin2(\u03c6)x = cos(\u03c6)z \u2212cos2(\u03c6)x \u2212sin(\u03c6)v\u03c6\n(29)\n(sin2(\u03c6) + cos2(\u03c6))x = x = cos(\u03c6)z \u2212sin(\u03c6)v\u03c6,\n(30)\nand similarly we get \u03f5 = sin(\u03c6)z\u03c6 + cos(\u03c6)v\u03c6.\nFurthermore, we de\ufb01ne the predicted velocity as\n\u02c6v\u03b8(z\u03c6) \u2261cos(\u03c6)\u02c6\u03f5\u03b8(z\u03c6) \u2212sin(\u03c6)\u02c6x\u03b8(z\u03c6),\n(31)\nwhere \u02c6\u03f5\u03b8(z\u03c6) = (z\u03c6 \u2212cos(\u03c6)\u02c6x\u03b8(z\u03c6))/ sin(\u03c6).\nRewriting the DDIM update rule in the introduced terms then gives\nz\u03c6s = cos(\u03c6s)\u02c6x\u03b8(z\u03c6t) + sin(\u03c6s)\u02c6\u03f5\u03b8(z\u03c6t)\n(32)\n= cos(\u03c6s)(cos(\u03c6t)z\u03c6t \u2212sin(\u03c6t)\u02c6v\u03b8(z\u03c6t)) + sin(\u03c6s)(sin(\u03c6t)z\u03c6t + cos(\u03c6t)\u02c6v\u03b8(z\u03c6t))\n(33)\n= [cos(\u03c6s) cos(\u03c6t) \u2212sin(\u03c6s) sin(\u03c6t)]z\u03c6t + [sin(\u03c6s) cos(\u03c6t) \u2212cos(\u03c6s) sin(\u03c6t)]\u02c6v\u03b8(z\u03c6t).\n(34)\n14\nPublished as a conference paper at ICLR 2022\nFinally, we use the trigonometric identities\ncos(\u03c6s) sin(\u03c6t) \u2212sin(\u03c6s) cos(\u03c6t) = cos(\u03c6s \u2212\u03c6t)\n(35)\nsin(\u03c6s) cos(\u03c6t) \u2212cos(\u03c6s) sin(\u03c6t) = sin(\u03c6s \u2212\u03c6t),\n(36)\nto \ufb01nd that\nz\u03c6s = cos(\u03c6s \u2212\u03c6t)z\u03c6t + sin(\u03c6s \u2212\u03c6t)\u02c6v\u03b8(z\u03c6t).\n(37)\nor equivalently\nz\u03c6t\u2212\u03b4 = cos(\u03b4)z\u03c6t \u2212sin(\u03b4)\u02c6v\u03b8(z\u03c6t).\n(38)\nViewed from this perspective, DDIM thus evolves z\u03c6s by moving it on a circle in the (z\u03c6t, \u02c6v\u03c6t) ba-\nsis, along the \u2212\u02c6v\u03c6t direction. The relationship between z\u03c6t, vt, \u03b1t, \u03c3t, x, \u03f5 is visualized in Figure 5.\nFigure 5: Visualization of reparameterizing the diffusion process in terms of \u03c6 and v\u03c6.\nE\nSETTINGS USED IN EXPERIMENTS\nOur model architectures closely follow those described by Dhariwal & Nichol (2021). For 64 \u00d7 64\nImageNet we use their model exactly, with 192 channels at the highest resolution. All other models\nare slight variations with different hyperparameters.\nFor CIFAR-10 we use an architecture with a \ufb01xed number of channels at all resolutions of 256. The\nmodel consists of a UNet that internally downsamples the data twice, to 16 \u00d7 16 and to 8 \u00d7 8. At\neach resolution we apply 3 residual blocks, like described by Dhariwal & Nichol (2021). We use\nsingle-headed attention, and only apply this at the 16 \u00d7 16 and 8 \u00d7 8 resolutions. We use dropout of\n0.2 when training the original model. No dropout is used during distillation.\nFor LSUN we use a model similar to that for ImageNet, but with a reduced number of 128 channels\nat the 64 \u00d7 64 resolution. Compared to ImageNet we have an additional level in the UNet, corre-\nsponding to the input resolution of 128 \u00d7 128, which we process using 3 residual blocks with 64\nchannels. We only use attention layers for the resolutions of 32 \u00d7 32 and lower.\nFor CIFAR-10 we take the output of the model to represent a prediction of x directly, as discussed\nin Section 4. For the other data sets we used the combined prediction of (x, \u03f5) like described in\nthat section also. All original models are trained with Adam with standard settings (learning rate of\n3\u221710\u22124), using a parameter moving average with constant 0.9999 and very slight decoupled weight\ndecay (Loshchilov & Hutter, 2017) with a constant of 0.001. We clip the norm of gradients to a\nglobal norm of 1 before calculating parameter updates. For CIFAR-10 we train for 800k parameter\nupdates, for ImageNet we use 550k updates, and for LSUN we use 400k updates. During distillation\nwe train for 50k updates per iteration, except for the distillation to 2 and 1 sampling steps, for which\nwe use 100k updates. We linearly anneal the learning rate from 10\u22124 to zero during each iteration.\n15\nPublished as a conference paper at ICLR 2022\nWe use a batch size of 128 for CIFAR-10 and 2048 for the other data sets. We run our experiments\non TPUv4, using 8 TPU chips for CIFAR-10, and 64 chips for the other data sets. The total time\nrequired to \ufb01rst train and then distill a model varies from about a day for CIFAR-10, to about 5 days\nfor ImageNet.\nF\nSTOCHASTIC SAMPLING WITH DISTILLED MODELS\nOur progressive distillation procedure was designed to be used with the DDIM sampler, but the\nresulting distilled model could in principle also be used with a stochastic sampler. Here we evaluate a\ndistilled model for 64x64 ImageNet using the optimized stochastic sampler also used in Section 5.2.\nThe results are presented in Figure 6.\n256\n128\n64\n32\n16\n8\n4\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\n64x64 ImageNet\nDistilled DDIM\nDistilled Stochastic\nUndistilled Stochastic\nFigure 6: FID of generated samples from distilled and undistilled models, using DDIM or stochastic\nsampling. For the stochastic sampling results we present the best FID obtained by a grid-search over\n11 possible noise levels, spaced log-uniformly between the upper and lower bound on the variance\nas derived by Ho et al. (2020). The performance of the distilled model with stochastic sampling\nis found to lie in between the undistilled original model with stochastic sampling and the distilled\nDDIM sampler: For small numbers of sampling steps the DDIM sampler performs better with the\ndistilled model, for large numbers of steps the stochastic sampler performs better.\nG\nDERIVATION OF THE DISTILLATION TARGET\nThe key difference between our progressive distillation algorithm proposed in Section 3 and the\nstandard diffusion training procedure is in how we determine the target value for our denoising\nmodel. In standard diffusion training, the target for denoising is the clean data x. In progressive\ndistillation it is the value \u02dcx the student denoising model would need to predict in order to match the\nteacher model when sampling. Here we derive what this target needs to be.\nUsing notation t\u2032 = t \u22120.5/N and t\u2032\u2032 = t \u22121/N, when training a student with N sampling steps,\nwe have that the teacher model samples the next set of noisy data zt\u2032\u2032 given the current noisy data zt\nby taking two steps of DDIM. The student tries to sample the same value in only one step of DDIM.\nDenoting the student denoising prediction by \u02dcx, and its one-step sample by \u02dczt\u2032\u2032, application of the\nDDIM sampler (see equation 8), gives:\n\u02dczt\u2032\u2032 = \u03b1t\u2032\u2032 \u02dcx + \u03c3t\u2032\u2032\n\u03c3t\n(zt \u2212\u03b1t\u02dcx).\n(39)\n16\nPublished as a conference paper at ICLR 2022\nIn order for the student sampler to match the teacher sampler, we must set \u02dczt\u2032\u2032 equal to zt\u2032\u2032. This\ngives\n\u02dczt\u2032\u2032 = \u03b1t\u2032\u2032 \u02dcx + \u03c3t\u2032\u2032\n\u03c3t\n(zt \u2212\u03b1t\u02dcx) = zt\u2032\u2032\n(40)\n=\n\u0012\n\u03b1t\u2032\u2032 \u2212\u03c3t\u2032\u2032\n\u03c3t\n\u03b1t\n\u0013\n\u02dcx + \u03c3t\u2032\u2032\n\u03c3t\nzt = zt\u2032\u2032\n(41)\n\u0012\n\u03b1t\u2032\u2032 \u2212\u03c3t\u2032\u2032\n\u03c3t\n\u03b1t\n\u0013\n\u02dcx = zt\u2032\u2032 \u2212\u03c3t\u2032\u2032\n\u03c3t\nzt\n(42)\n\u02dcx =\nzt\u2032\u2032 \u2212\u03c3t\u2032\u2032\n\u03c3t zt\n\u03b1t\u2032\u2032 \u2212\u03c3t\u2032\u2032\n\u03c3t \u03b1t\n(43)\nIn other words, if our student denoising model exactly predicts \u02dcx as de\ufb01ned in equation 43 above,\nthen the one-step student sample \u02dczt\u2032\u2032 is identical to the two-step teacher sample zt\u2032\u2032. In order to have\nour student model approximate this ideal outcome, we thus train it to predict \u02dcx from zt as well as\npossible, using the standard squared error denoising loss (see Equation 9).\nNote that this possibility of matching the two-step teacher model with a one-step student model is\nunique to deterministic samplers like DDIM: the composition of two standard stochastic DDPM\nsampling steps (Equation 5) forms a non-Gaussian distribution that falls outside the family of Gaus-\nsian distributions that can be modelled by a single DDPM student step: A multi-step stochastic\nDDPM sampler can thus not be distilled into a few-step sampler without some loss in \ufb01delity. This\nis in contrast with the deterministic DDIM sampler: here both the two-step DDIM teacher update\nand the one-step DDIM student update represent deterministic mappings implemented by a neural\nnet, which is why the student is able to accurately match the teacher.\nFinally, note that we do lose something during the progressive distillation process: while the original\nmodel was trained to denoise zt for any given continuous time t, the distilled student models are\nonly ever evaluated on a small discrete set of times t. The student models thus lose generality as\ndistillation progresses. At the same time, it\u2019s this loss of generality that allows the student models\nto free up enough modeling capacity to accurately match the teacher model without increasing their\nmodel size.\nH\nADDITIONAL RANDOM SAMPLES\nIn this section we present additional random samples from our diffusion models obtained through\nprogressive distillation. We show samples for distilled models taking 256, 4, and 1 sampling steps.\nAll samples are uncurated.\nAs explained in Section 3, our distilled samplers implement a deterministic mapping from input\nnoise to output samples (also see Appendix G). To facilitate comparison of this mapping for varying\nnumbers of sampling steps, we generate all samples using the same random input noise, and we\npresent the samples side-by-side. As these samples show, the mapping is mostly preserved when\nmoving from many steps to a single step: The same input noise is mapped to the same output image,\nwith a slight loss in image quality, as the number of steps is reduced. Since the mapping is preserved\nwhile reducing the number of steps, our distilled models also preserve the excellent sample diversity\nof diffusion models (see e.g. Kingma et al. (2021)).\n17\nPublished as a conference paper at ICLR 2022\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 7: Random samples from our distilled CIFAR-10 models, for \ufb01xed random seed and for\nvarying number of sampling steps.\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 8: Random samples from our distilled 64 \u00d7 64 ImageNet models, conditioned on the \u2018coral\nreef\u2019 class, for \ufb01xed random seed and for varying number of sampling steps.\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 9: Random samples from our distilled 64 \u00d7 64 ImageNet models, conditioned on the \u2018sports\ncar\u2019 class, for \ufb01xed random seed and for varying number of sampling steps.\n18\nPublished as a conference paper at ICLR 2022\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 10: Random samples from our distilled LSUN bedrooms models, for \ufb01xed random seed and\nfor varying number of sampling steps.\n(a) 256 sampling steps\n(b) 4 sampling steps\n(c) 1 sampling step\nFigure 11: Random samples from our distilled LSUN church-outdoor models, for \ufb01xed random seed\nand for varying number of sampling steps.\n19\nPublished as a conference paper at ICLR 2022\nI\nABLATION WITH FASTER DISTILLATION SCHEDULES\nIn order to further reduce the computational requirements for our progressive distillation approach,\nwe perform an ablation study on CIFAR-10, where we decrease the number of parameter updates\nwe use to train each new student model. In Figure 12 we present results for taking 25 thousand, 10\nthousand, or 5 thousand optimization steps, instead of the 50 thousand we suggested in Section 3.\nAs the results show, we can drastically decrease the number of optimization steps taken, and still get\nvery good performance when using \u22654 sampling steps. When taking very few sampling steps, the\nloss in performance becomes more pronounced when training the student for only a short time.\nIn addition to just decreasing the number of parameter updates, we also experiment with a schedule\nwhere we train each student on 4 times fewer sampling steps than its teacher, rather than the 2 times\nwe propose in Section 3. Here the denoising target is still derived from taking 2 DDIM steps with\nthe teacher model as usual, since taking 4 teacher steps would negate most of the computational\nsavings. As Figure 12 shows, this does not work as well: if the computational budget is limited,\nit\u2019s better to take fewer parameter updates per halving of the number of sampling steps then to skip\ndistillation iterations altogether.\nIn Figure 13 we show the results achieved with a faster schedule for the ImageNet and LSUN\ndatasets. Here also, we achieve excellent results with a faster distillation schedule.\n512\n256\n128\n64\n32\n16\n8\n4\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\nCIFAR-10\nDistilled 50k\nDistilled 25k\nDistilled 10k\nDistilled 5k\nDistilled 4\u00d7\nFigure 12: Comparing our proposed schedule for progressive distillation taking 50k parameter up-\ndates to train a new student every time the number of steps is halved, versus fast sampling schedules\ntaking fewer parameter updates (25k, 10k, 5k), and a fast schedule dividing the number of steps by\n4 for every new student instead of by 2. All reported numbers are averages over 4 random seeds.\nFor each schedule we selected the optimal learning rate from [5e\u22125, 1e\u22124, 2e\u22124, 3e\u22124].\n20\nPublished as a conference paper at ICLR 2022\n256\n128\n64\n32\n16\n8\n4\n2\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\nFID\n64x64 ImageNet\n50k updates\n10k updates\n256\n128\n64\n32\n16\n8\n4\n2\n1\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\n128x128 LSUN Bedrooms\n50k updates\n10k updates\n256\n128\n64\n32\n16\n8\n4\n2\n1\n3\n4\n5\n6\n7\n8\n9\n10\n20\nsampling steps\n128x128 LSUN Church-Outdoor\n50k updates\n10k updates\nFigure 13: Comparing our proposed schedule for progressive distillation taking 50k parameter up-\ndates to train a new student every time the number of steps is halved, versus a fast sampling schedule\ntaking 10k parameter updates. For each reported number of steps we selected the optimal learning\nrate from [5e\u22125, 1e\u22124, 2e\u22124, 3e\u22124]. Results are for a single random seed.\n21\n",
    "2303.01469": "Consistency Models\nYang Song 1 Prafulla Dhariwal 1 Mark Chen 1 Ilya Sutskever 1\nAbstract\nDiffusion models have significantly advanced the\nfields of image, audio, and video generation, but\nthey depend on an iterative sampling process that\ncauses slow generation. To overcome this limita-\ntion, we propose consistency models, a new fam-\nily of models that generate high quality samples\nby directly mapping noise to data. They support\nfast one-step generation by design, while still al-\nlowing multistep sampling to trade compute for\nsample quality. They also support zero-shot data\nediting, such as image inpainting, colorization,\nand super-resolution, without requiring explicit\ntraining on these tasks. Consistency models can\nbe trained either by distilling pre-trained diffu-\nsion models, or as standalone generative models\naltogether. Through extensive experiments, we\ndemonstrate that they outperform existing distilla-\ntion techniques for diffusion models in one- and\nfew-step sampling, achieving the new state-of-\nthe-art FID of 3.55 on CIFAR-10 and 6.20 on\nImageNet 64 \u02c6 64 for one-step generation. When\ntrained in isolation, consistency models become a\nnew family of generative models that can outper-\nform existing one-step, non-adversarial generative\nmodels on standard benchmarks such as CIFAR-\n10, ImageNet 64 \u02c6 64 and LSUN 256 \u02c6 256.\n1. Introduction\nDiffusion models (Sohl-Dickstein et al., 2015; Song & Er-\nmon, 2019; 2020; Ho et al., 2020; Song et al., 2021), also\nknown as score-based generative models, have achieved\nunprecedented success across multiple fields, including im-\nage generation (Dhariwal & Nichol, 2021; Nichol et al.,\n2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach\net al., 2022), audio synthesis (Kong et al., 2020; Chen et al.,\n2021; Popov et al., 2021), and video generation (Ho et al.,\n1OpenAI, San Francisco, CA 94110, USA. Correspondence to:\nYang Song <songyang@openai.com>.\nProceedings of the 40 th International Conference on Machine\nLearning, Honolulu, Hawaii, USA. PMLR 202, 2023. Copyright\n2023 by the author(s).\nFigure 1: Given a Probability Flow (PF) ODE that smoothly\nconverts data to noise, we learn to map any point (e.g., xt,\nxt1, and xT ) on the ODE trajectory to its origin (e.g., x0)\nfor generative modeling. Models of these mappings are\ncalled consistency models, as their outputs are trained to be\nconsistent for points on the same trajectory.\n2022b;a). A key feature of diffusion models is the iterative\nsampling process which progressively removes noise from\nrandom initial vectors. This iterative process provides a\nflexible trade-off of compute and sample quality, as using\nextra compute for more iterations usually yields samples\nof better quality. It is also the crux of many zero-shot data\nediting capabilities of diffusion models, enabling them to\nsolve challenging inverse problems ranging from image\ninpainting, colorization, stroke-guided image editing, to\nComputed Tomography and Magnetic Resonance Imaging\n(Song & Ermon, 2019; Song et al., 2021; 2022; 2023; Kawar\net al., 2021; 2022; Chung et al., 2023; Meng et al., 2021).\nHowever, compared to single-step generative models like\nGANs (Goodfellow et al., 2014), VAEs (Kingma & Welling,\n2014; Rezende et al., 2014), or normalizing flows (Dinh\net al., 2015; 2017; Kingma & Dhariwal, 2018), the iterative\ngeneration procedure of diffusion models typically requires\n10\u20132000 times more compute for sample generation (Song\n& Ermon, 2020; Ho et al., 2020; Song et al., 2021; Zhang\n& Chen, 2022; Lu et al., 2022), causing slow inference and\nlimited real-time applications.\nOur objective is to create generative models that facilitate ef-\nficient, single-step generation without sacrificing important\nadvantages of iterative sampling, such as trading compute\nfor sample quality when necessary, as well as performing\nzero-shot data editing tasks. As illustrated in Fig. 1, we\nbuild on top of the probability flow (PF) ordinary differen-\ntial equation (ODE) in continuous-time diffusion models\n(Song et al., 2021), whose trajectories smoothly transition\n1\narXiv:2303.01469v2  [cs.LG]  31 May 2023\nConsistency Models\nthe data distribution into a tractable noise distribution. We\npropose to learn a model that maps any point at any time\nstep to the trajectory\u2019s starting point. A notable property\nof our model is self-consistency: points on the same tra-\njectory map to the same initial point. We therefore refer to\nsuch models as consistency models. Consistency models\nallow us to generate data samples (initial points of ODE\ntrajectories, e.g., x0 in Fig. 1) by converting random noise\nvectors (endpoints of ODE trajectories, e.g., xT in Fig. 1)\nwith only one network evaluation. Importantly, by chaining\nthe outputs of consistency models at multiple time steps,\nwe can improve sample quality and perform zero-shot data\nediting at the cost of more compute, similar to what iterative\nsampling enables for diffusion models.\nTo train a consistency model, we offer two methods based\non enforcing the self-consistency property. The first method\nrelies on using numerical ODE solvers and a pre-trained\ndiffusion model to generate pairs of adjacent points on a\nPF ODE trajectory. By minimizing the difference between\nmodel outputs for these pairs, we can effectively distill a\ndiffusion model into a consistency model, which allows gen-\nerating high-quality samples with one network evaluation.\nBy contrast, our second method eliminates the need for a\npre-trained diffusion model altogether, allowing us to train\na consistency model in isolation. This approach situates\nconsistency models as an independent family of generative\nmodels. Importantly, neither approach necessitates adver-\nsarial training, and they both place minor constraints on the\narchitecture, allowing the use of flexible neural networks\nfor parameterizing consistency models.\nWe demonstrate the efficacy of consistency models on sev-\neral image datasets, including CIFAR-10 (Krizhevsky et al.,\n2009), ImageNet 64 \u02c6 64 (Deng et al., 2009), and LSUN\n256 \u02c6 256 (Yu et al., 2015). Empirically, we observe that\nas a distillation approach, consistency models outperform\nexisting diffusion distillation methods like progressive dis-\ntillation (Salimans & Ho, 2022) across a variety of datasets\nin few-step generation: On CIFAR-10, consistency models\nreach new state-of-the-art FIDs of 3.55 and 2.93 for one-step\nand two-step generation; on ImageNet 64 \u02c6 64, it achieves\nrecord-breaking FIDs of 6.20 and 4.70 with one and two net-\nwork evaluations respectively. When trained as standalone\ngenerative models, consistency models can match or surpass\nthe quality of one-step samples from progressive distillation,\ndespite having no access to pre-trained diffusion models.\nThey are also able to outperform many GANs, and exist-\ning non-adversarial, single-step generative models across\nmultiple datasets. Furthermore, we show that consistency\nmodels can be used to perform a wide range of zero-shot\ndata editing tasks, including image denoising, interpolation,\ninpainting, colorization, super-resolution, and stroke-guided\nimage editing (SDEdit, Meng et al. (2021)).\n2. Diffusion Models\nConsistency models are heavily inspired by the theory of\ncontinuous-time diffusion models (Song et al., 2021; Karras\net al., 2022). Diffusion models generate data by progres-\nsively perturbing data to noise via Gaussian perturbations,\nthen creating samples from noise via sequential denoising\nsteps. Let pdatapxq denote the data distribution. Diffusion\nmodels start by diffusing pdatapxq with a stochastic differen-\ntial equation (SDE) (Song et al., 2021)\ndxt \u201c \u00b5pxt, tq dt ` \u03c3ptq dwt,\n(1)\nwhere t P r0, Ts, T \u0105 0 is a fixed constant, \u00b5p\u00a8, \u00a8q and\n\u03c3p\u00a8q are the drift and diffusion coefficients respectively,\nand twtutPr0,T s denotes the standard Brownian motion.\nWe denote the distribution of xt as ptpxq and as a result\np0pxq \u201d pdatapxq. A remarkable property of this SDE is\nthe existence of an ordinary differential equation (ODE),\ndubbed the Probability Flow (PF) ODE by Song et al.\n(2021), whose solution trajectories sampled at t are dis-\ntributed according to ptpxq:\ndxt \u201c\n\u201e\n\u00b5pxt, tq \u00b4 1\n2\u03c3ptq2\u2207log ptpxtq\n\u0237\ndt.\n(2)\nHere \u2207log ptpxq is the score function of ptpxq; hence dif-\nfusion models are also known as score-based generative\nmodels (Song & Ermon, 2019; 2020; Song et al., 2021).\nTypically, the SDE in Eq. (1) is designed such that pT pxq\nis close to a tractable Gaussian distribution \u03c0pxq.\nWe\nhereafter adopt the settings in Karras et al. (2022), where\n\u00b5px, tq \u201c 0 and \u03c3ptq \u201c\n?\n2t.\nIn this case, we have\nptpxq \u201c pdatapxq b Np0, t2Iq, where b denotes the convo-\nlution operation, and \u03c0pxq \u201c Np0, T 2Iq. For sampling, we\nfirst train a score model s\u03d5px, tq \u00ab \u2207log ptpxq via score\nmatching (Hyv\u00a8arinen & Dayan, 2005; Vincent, 2011; Song\net al., 2019; Song & Ermon, 2019; Ho et al., 2020), then\nplug it into Eq. (2) to obtain an empirical estimate of the PF\nODE, which takes the form of\ndxt\ndt \u201c \u00b4ts\u03d5pxt, tq.\n(3)\nWe call Eq. (3) the empirical PF ODE. Next, we sample\n\u02c6xT \u201e \u03c0 \u201c Np0, T 2Iq to initialize the empirical PF ODE\nand solve it backwards in time with any numerical ODE\nsolver, such as Euler (Song et al., 2020; 2021) and Heun\nsolvers (Karras et al., 2022), to obtain the solution trajectory\nt\u02c6xtutPr0,T s. The resulting \u02c6x0 can then be viewed as an\napproximate sample from the data distribution pdatapxq. To\navoid numerical instability, one typically stops the solver\nat t \u201c \u03f5, where \u03f5 is a fixed small positive number, and\naccepts \u02c6x\u03f5 as the approximate sample. Following Karras\net al. (2022), we rescale image pixel values to r\u00b41, 1s, and\nset T \u201c 80, \u03f5 \u201c 0.002.\n2\nConsistency Models\nFigure 2: Consistency models are trained to map points on\nany trajectory of the PF ODE to the trajectory\u2019s origin.\nDiffusion models are bottlenecked by their slow sampling\nspeed. Clearly, using ODE solvers for sampling requires\niterative evaluations of the score model s\u03d5px, tq, which is\ncomputationally costly. Existing methods for fast sampling\ninclude faster numerical ODE solvers (Song et al., 2020;\nZhang & Chen, 2022; Lu et al., 2022; Dockhorn et al., 2022),\nand distillation techniques (Luhman & Luhman, 2021; Sali-\nmans & Ho, 2022; Meng et al., 2022; Zheng et al., 2022).\nHowever, ODE solvers still need more than 10 evaluation\nsteps to generate competitive samples. Most distillation\nmethods like Luhman & Luhman (2021) and Zheng et al.\n(2022) rely on collecting a large dataset of samples from\nthe diffusion model prior to distillation, which itself is com-\nputationally expensive. To our best knowledge, the only\ndistillation approach that does not suffer from this drawback\nis progressive distillation (PD, Salimans & Ho (2022)), with\nwhich we compare consistency models extensively in our\nexperiments.\n3. Consistency Models\nWe propose consistency models, a new type of models that\nsupport single-step generation at the core of its design, while\nstill allowing iterative generation for trade-offs between sam-\nple quality and compute, and zero-shot data editing. Consis-\ntency models can be trained in either the distillation mode or\nthe isolation mode. In the former case, consistency models\ndistill the knowledge of pre-trained diffusion models into a\nsingle-step sampler, significantly improving other distilla-\ntion approaches in sample quality, while allowing zero-shot\nimage editing applications. In the latter case, consistency\nmodels are trained in isolation, with no dependence on pre-\ntrained diffusion models. This makes them an independent\nnew class of generative models.\nBelow we introduce the definition, parameterization, and\nsampling of consistency models, plus a brief discussion on\ntheir applications to zero-shot data editing.\nDefinition Given a solution trajectory txtutPr\u03f5,T s of the\nPF ODE in Eq. (2), we define the consistency function as\nf : pxt, tq \u00de\u00d1 x\u03f5. A consistency function has the property\nof self-consistency: its outputs are consistent for arbitrary\npairs of pxt, tq that belong to the same PF ODE trajectory,\ni.e., fpxt, tq \u201c fpxt1, t1q for all t, t1 P r\u03f5, Ts. As illustrated\nin Fig. 2, the goal of a consistency model, symbolized as\nf\u03b8, is to estimate this consistency function f from data by\nlearning to enforce the self-consistency property (details\nin Sections 4 and 5). Note that a similar definition is used\nfor neural flows (Bilo\u02c7s et al., 2021) in the context of neural\nODEs (Chen et al., 2018). Compared to neural flows, how-\never, we do not enforce consistency models to be invertible.\nParameterization For any consistency function fp\u00a8, \u00a8q, we\nhave fpx\u03f5, \u03f5q \u201c x\u03f5, i.e., fp\u00a8, \u03f5q is an identity function. We\ncall this constraint the boundary condition. All consistency\nmodels have to meet this boundary condition, as it plays a\ncrucial role in the successful training of consistency models.\nThis boundary condition is also the most confining archi-\ntectural constraint on consistency models. For consistency\nmodels based on deep neural networks, we discuss two\nways to implement this boundary condition almost for free.\nSuppose we have a free-form deep neural network F\u03b8px, tq\nwhose output has the same dimensionality as x. The first\nway is to simply parameterize the consistency model as\nf\u03b8px, tq \u201c\n#\nx\nt \u201c \u03f5\nF\u03b8px, tq\nt P p\u03f5, Ts .\n(4)\nThe second method is to parameterize the consistency model\nusing skip connections, that is,\nf\u03b8px, tq \u201c cskipptqx ` coutptqF\u03b8px, tq,\n(5)\nwhere cskipptq and coutptq are differentiable functions\nsuch that cskipp\u03f5q \u201c 1, and coutp\u03f5q \u201c 0.\nThis way,\nthe consistency model is differentiable at t\n\u201c\n\u03f5 if\nF\u03b8px, tq, cskipptq, coutptq are all differentiable, which is criti-\ncal for training continuous-time consistency models (Appen-\ndices B.1 and B.2). The parameterization in Eq. (5) bears\nstrong resemblance to many successful diffusion models\n(Karras et al., 2022; Balaji et al., 2022), making it easier to\nborrow powerful diffusion model architectures for construct-\ning consistency models. We therefore follow the second\nparameterization in all experiments.\nSampling With a well-trained consistency model f\u03b8p\u00a8, \u00a8q,\nwe can generate samples by sampling from the initial dis-\ntribution \u02c6xT \u201e Np0, T 2Iq and then evaluating the consis-\ntency model for \u02c6x\u03f5 \u201c f\u03b8p\u02c6xT , Tq. This involves only one\nforward pass through the consistency model and therefore\ngenerates samples in a single step. Importantly, one can\nalso evaluate the consistency model multiple times by al-\nternating denoising and noise injection steps for improved\nsample quality. Summarized in Algorithm 1, this multistep\nsampling procedure provides the flexibility to trade com-\npute for sample quality. It also has important applications\nin zero-shot data editing. In practice, we find time points\n3\nConsistency Models\nAlgorithm 1 Multistep Consistency Sampling\nInput: Consistency model f\u03b8p\u00a8, \u00a8q, sequence of time\npoints \u03c41 \u0105 \u03c42 \u0105 \u00a8 \u00a8 \u00a8 \u0105 \u03c4N\u00b41, initial noise \u02c6xT\nx \u00d0 f\u03b8p\u02c6xT , Tq\nfor n \u201c 1 to N \u00b4 1 do\nSample z \u201e Np0, Iq\n\u02c6x\u03c4n \u00d0 x `\na\n\u03c4 2n \u00b4 \u03f52z\nx \u00d0 f\u03b8p\u02c6x\u03c4n, \u03c4nq\nend for\nOutput: x\nt\u03c41, \u03c42, \u00a8 \u00a8 \u00a8 , \u03c4N\u00b41u in Algorithm 1 with a greedy algorithm,\nwhere the time points are pinpointed one at a time using\nternary search to optimize the FID of samples obtained from\nAlgorithm 1. This assumes that given prior time points, the\nFID is a unimodal function of the next time point. We find\nthis assumption to hold empirically in our experiments, and\nleave the exploration of better strategies as future work.\nZero-Shot Data Editing Similar to diffusion models, con-\nsistency models enable various data editing and manipu-\nlation applications in zero shot; they do not require ex-\nplicit training to perform these tasks. For example, consis-\ntency models define a one-to-one mapping from a Gaussian\nnoise vector to a data sample. Similar to latent variable\nmodels like GANs, VAEs, and normalizing flows, consis-\ntency models can easily interpolate between samples by\ntraversing the latent space (Fig. 11). As consistency models\nare trained to recover x\u03f5 from any noisy input xt where\nt P r\u03f5, Ts, they can perform denoising for various noise\nlevels (Fig. 12). Moreover, the multistep generation pro-\ncedure in Algorithm 1 is useful for solving certain inverse\nproblems in zero shot by using an iterative replacement pro-\ncedure similar to that of diffusion models (Song & Ermon,\n2019; Song et al., 2021; Ho et al., 2022b). This enables\nmany applications in the context of image editing, including\ninpainting (Fig. 10), colorization (Fig. 8), super-resolution\n(Fig. 6b) and stroke-guided image editing (Fig. 13) as in\nSDEdit (Meng et al., 2021). In Section 6.3, we empiri-\ncally demonstrate the power of consistency models on many\nzero-shot image editing tasks.\n4. Training Consistency Models via Distillation\nWe present our first method for training consistency mod-\nels based on distilling a pre-trained score model s\u03d5px, tq.\nOur discussion revolves around the empirical PF ODE in\nEq. (3), obtained by plugging the score model s\u03d5px, tq\ninto the PF ODE. Consider discretizing the time horizon\nr\u03f5, Ts into N \u00b4 1 sub-intervals, with boundaries t1 \u201c \u03f5 \u0103\nt2 \u0103 \u00a8 \u00a8 \u00a8 \u0103 tN \u201c T.\nIn practice, we follow Karras\net al. (2022) to determine the boundaries with the formula\nti \u201c p\u03f51{\u03c1 ` i\u00b41{N\u00b41pT 1{\u03c1 \u00b4 \u03f51{\u03c1qq\u03c1, where \u03c1 \u201c 7. When\nN is sufficiently large, we can obtain an accurate estimate\nof xtn from xtn`1 by running one discretization step of a\nnumerical ODE solver. This estimate, which we denote as\n\u02c6x\u03d5\ntn, is defined by\n\u02c6x\u03d5\ntn :\u201c xtn`1 ` ptn \u00b4 tn`1q\u03a6pxtn`1, tn`1; \u03d5q,\n(6)\nwhere \u03a6p\u00a8 \u00a8 \u00a8 ; \u03d5q represents the update function of a one-\nstep ODE solver applied to the empirical PF ODE. For\nexample, when using the Euler solver, we have \u03a6px, t; \u03d5q \u201c\n\u00b4ts\u03d5px, tq which corresponds to the following update rule\n\u02c6x\u03d5\ntn \u201c xtn`1 \u00b4 ptn \u00b4 tn`1qtn`1s\u03d5pxtn`1, tn`1q.\nFor simplicity, we only consider one-step ODE solvers in\nthis work. It is straightforward to generalize our framework\nto multistep ODE solvers and we leave it as future work.\nDue to the connection between the PF ODE in Eq. (2) and\nthe SDE in Eq. (1) (see Section 2), one can sample along the\ndistribution of ODE trajectories by first sampling x \u201e pdata,\nthen adding Gaussian noise to x. Specifically, given a data\npoint x, we can generate a pair of adjacent data points\np\u02c6x\u03d5\ntn, xtn`1q on the PF ODE trajectory efficiently by sam-\npling x from the dataset, followed by sampling xtn`1 from\nthe transition density of the SDE Npx, t2\nn`1Iq, and then\ncomputing \u02c6x\u03d5\ntn using one discretization step of the numeri-\ncal ODE solver according to Eq. (6). Afterwards, we train\nthe consistency model by minimizing its output differences\non the pair p\u02c6x\u03d5\ntn, xtn`1q. This motivates our following con-\nsistency distillation loss for training consistency models.\nDefinition 1. The consistency distillation loss is defined as\nLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q :\u201c\nEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs,\n(7)\nwhere the expectation is taken with respect to x \u201e pdata, n \u201e\nUJ1, N \u00b41K, and xtn`1 \u201e Npx; t2\nn`1Iq. Here UJ1, N \u00b41K\ndenotes the uniform distribution over t1, 2, \u00a8 \u00a8 \u00a8 , N \u00b4 1u,\n\u03bbp\u00a8q P R` is a positive weighting function, \u02c6x\u03d5\ntn is given by\nEq. (6), \u03b8\u00b4 denotes a running average of the past values of\n\u03b8 during the course of optimization, and dp\u00a8, \u00a8q is a metric\nfunction that satisfies @x, y : dpx, yq \u011b 0 and dpx, yq \u201c 0\nif and only if x \u201c y.\nUnless otherwise stated, we adopt the notations in Defi-\nnition 1 throughout this paper, and use Er\u00a8s to denote the\nexpectation over all random variables. In our experiments,\nwe consider the squared \u21132 distance dpx, yq \u201c }x \u00b4 y}2\n2, \u21131\ndistance dpx, yq \u201c }x \u00b4 y}1, and the Learned Perceptual\nImage Patch Similarity (LPIPS, Zhang et al. (2018)). We\nfind \u03bbptnq \u201d 1 performs well across all tasks and datasets.\nIn practice, we minimize the objective by stochastic gradient\ndescent on the model parameters \u03b8, while updating \u03b8\u00b4 with\nexponential moving average (EMA). That is, given a decay\n4\nConsistency Models\nAlgorithm 2 Consistency Distillation (CD)\nInput: dataset D, initial model parameter \u03b8, learning rate\n\u03b7, ODE solver \u03a6p\u00a8, \u00a8; \u03d5q, dp\u00a8, \u00a8q, \u03bbp\u00a8q, and \u00b5\n\u03b8\u00b4 \u00d0 \u03b8\nrepeat\nSample x \u201e D and n \u201e UJ1, N \u00b4 1K\nSample xtn`1 \u201e Npx; t2\nn`1Iq\n\u02c6x\u03d5\ntn \u00d0 xtn`1 ` ptn \u00b4 tn`1q\u03a6pxtn`1, tn`1; \u03d5q\nLp\u03b8, \u03b8\u00b4; \u03d5q \u00d0\n\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u03b8 \u00d0 \u03b8 \u00b4 \u03b7\u2207\u03b8Lp\u03b8, \u03b8\u00b4; \u03d5q\n\u03b8\u00b4 \u00d0 stopgradp\u00b5\u03b8\u00b4 ` p1 \u00b4 \u00b5q\u03b8)\nuntil convergence\nrate 0 \u010f \u00b5 \u0103 1, we perform the following update after each\noptimization step:\n\u03b8\u00b4 \u00d0 stopgradp\u00b5\u03b8\u00b4 ` p1 \u00b4 \u00b5q\u03b8q.\n(8)\nThe overall training procedure is summarized in Algo-\nrithm 2. In alignment with the convention in deep reinforce-\nment learning (Mnih et al., 2013; 2015; Lillicrap et al., 2015)\nand momentum based contrastive learning (Grill et al., 2020;\nHe et al., 2020), we refer to f\u03b8\u00b4 as the \u201ctarget network\u201d,\nand f\u03b8 as the \u201conline network\u201d. We find that compared to\nsimply setting \u03b8\u00b4 \u201c \u03b8, the EMA update and \u201cstopgrad\u201d\noperator in Eq. (8) can greatly stabilize the training process\nand improve the final performance of the consistency model.\nBelow we provide a theoretical justification for consistency\ndistillation based on asymptotic analysis.\nTheorem 1. Let \u2206t :\u201c maxnPJ1,N\u00b41Kt|tn`1 \u00b4 tn|u, and\nfp\u00a8, \u00a8; \u03d5q be the consistency function of the empirical PF\nODE in Eq. (3). Assume f\u03b8 satisfies the Lipschitz condition:\nthere exists L \u0105 0 such that for all t P r\u03f5, Ts, x, and y,\nwe have \u2225f\u03b8px, tq \u00b4 f\u03b8py, tq\u22252 \u010f L \u2225x \u00b4 y\u22252. Assume\nfurther that for all n P J1, N \u00b4 1K, the ODE solver called\nat tn`1 has local error uniformly bounded by Opptn`1 \u00b4\ntnqp`1q with p \u011b 1. Then, if LN\nCDp\u03b8, \u03b8; \u03d5q \u201c 0, we have\nsup\nn,x }f\u03b8px, tnq \u00b4 fpx, tn; \u03d5q}2 \u201c Opp\u2206tqpq.\nProof. The proof is based on induction and parallels the\nclassic proof of global error bounds for numerical ODE\nsolvers (S\u00a8uli & Mayers, 2003). We provide the full proof in\nAppendix A.2.\nSince \u03b8\u00b4 is a running average of the history of \u03b8, we have\n\u03b8\u00b4 \u201c \u03b8 when the optimization of Algorithm 2 converges.\nThat is, the target and online consistency models will eventu-\nally match each other. If the consistency model additionally\nachieves zero consistency distillation loss, then Theorem 1\nAlgorithm 3 Consistency Training (CT)\nInput: dataset D, initial model parameter \u03b8, learning rate\n\u03b7, step schedule Np\u00a8q, EMA decay rate schedule \u00b5p\u00a8q,\ndp\u00a8, \u00a8q, and \u03bbp\u00a8q\n\u03b8\u00b4 \u00d0 \u03b8 and k \u00d0 0\nrepeat\nSample x \u201e D, and n \u201e UJ1, Npkq \u00b4 1K\nSample z \u201e Np0, Iq\nLp\u03b8, \u03b8\u00b4q \u00d0\n\u03bbptnqdpf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4px ` tnz, tnqq\n\u03b8 \u00d0 \u03b8 \u00b4 \u03b7\u2207\u03b8Lp\u03b8, \u03b8\u00b4q\n\u03b8\u00b4 \u00d0 stopgradp\u00b5pkq\u03b8\u00b4 ` p1 \u00b4 \u00b5pkqq\u03b8q\nk \u00d0 k ` 1\nuntil convergence\nimplies that, under some regularity conditions, the estimated\nconsistency model can become arbitrarily accurate, as long\nas the step size of the ODE solver is sufficiently small. Im-\nportantly, our boundary condition f\u03b8px, \u03f5q \u201d x precludes\nthe trivial solution f\u03b8px, tq \u201d 0 from arising in consistency\nmodel training.\nThe consistency distillation loss LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q can be ex-\ntended to hold for infinitely many time steps (N \u00d1 8) if\n\u03b8\u00b4 \u201c \u03b8 or \u03b8\u00b4 \u201c stopgradp\u03b8q. The resulting continuous-\ntime loss functions do not require specifying N nor the time\nsteps tt1, t2, \u00a8 \u00a8 \u00a8 , tNu. Nonetheless, they involve Jacobian-\nvector products and require forward-mode automatic dif-\nferentiation for efficient implementation, which may not\nbe well-supported in some deep learning frameworks. We\nprovide these continuous-time distillation loss functions in\nTheorems 3 to 5, and relegate details to Appendix B.1.\n5. Training Consistency Models in Isolation\nConsistency models can be trained without relying on any\npre-trained diffusion models. This differs from existing\ndiffusion distillation techniques, making consistency models\na new independent family of generative models.\nRecall that in consistency distillation, we rely on a pre-\ntrained score model s\u03d5px, tq to approximate the ground\ntruth score function \u2207log ptpxq. It turns out that we can\navoid this pre-trained score model altogether by leveraging\nthe following unbiased estimator (Lemma 1 in Appendix A):\n\u2207log ptpxtq \u201c \u00b4E\n\u201ext \u00b4 x\nt2\n\u02c7\u02c7\u02c7\u02c7 xt\n\u0237\n,\nwhere x \u201e pdata and xt \u201e Npx; t2Iq. That is, given x and\nxt, we can estimate \u2207log ptpxtq with \u00b4pxt \u00b4 xq{t2.\nThis unbiased estimate suffices to replace the pre-trained\ndiffusion model in consistency distillation when using the\nEuler method as the ODE solver in the limit of N \u00d1 8, as\n5\nConsistency Models\njustified by the following result.\nTheorem 2. Let \u2206t :\u201c maxnPJ1,N\u00b41Kt|tn`1 \u00b4 tn|u. As-\nsume d and f\u03b8\u00b4 are both twice continuously differentiable\nwith bounded second derivatives, the weighting function\n\u03bbp\u00a8q is bounded, and Er\u2225\u2207log ptnpxtnq\u22252\n2s \u0103 8.\nAs-\nsume further that we use the Euler ODE solver, and the\npre-trained score model matches the ground truth, i.e.,\n@t P r\u03f5, Ts : s\u03d5px, tq \u201d \u2207log ptpxq. Then,\nLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c LN\nCTp\u03b8, \u03b8\u00b4q ` op\u2206tq,\n(9)\nwhere the expectation is taken with respect to x \u201e pdata, n \u201e\nUJ1, N \u00b4 1K, and xtn`1 \u201e Npx; t2\nn`1Iq. The consistency\ntraining objective, denoted by LN\nCTp\u03b8, \u03b8\u00b4q, is defined as\nEr\u03bbptnqdpf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4px ` tnz, tnqqs, (10)\nwhere z \u201e Np0, Iq. Moreover, LN\nCTp\u03b8, \u03b8\u00b4q \u011b Op\u2206tq if\ninfN LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u0105 0.\nProof. The proof is based on Taylor series expansion and\nproperties of score functions (Lemma 1). A complete proof\nis provided in Appendix A.3.\nWe refer to Eq. (10) as the consistency training (CT) loss.\nCrucially, Lp\u03b8, \u03b8\u00b4q only depends on the online network\nf\u03b8, and the target network f\u03b8\u00b4, while being completely\nagnostic to diffusion model parameters \u03d5. The loss function\nLp\u03b8, \u03b8\u00b4q \u011b Op\u2206tq decreases at a slower rate than the\nremainder op\u2206tq and thus will dominate the loss in Eq. (9)\nas N \u00d1 8 and \u2206t \u00d1 0.\nFor improved practical performance, we propose to progres-\nsively increase N during training according to a schedule\nfunction Np\u00a8q. The intuition (cf., Fig. 3d) is that the consis-\ntency training loss has less \u201cvariance\u201d but more \u201cbias\u201d with\nrespect to the underlying consistency distillation loss (i.e.,\nthe left-hand side of Eq. (9)) when N is small (i.e., \u2206t is\nlarge), which facilitates faster convergence at the beginning\nof training. On the contrary, it has more \u201cvariance\u201d but less\n\u201cbias\u201d when N is large (i.e., \u2206t is small), which is desirable\nwhen closer to the end of training. For best performance,\nwe also find that \u00b5 should change along with N, according\nto a schedule function \u00b5p\u00a8q. The full algorithm of consis-\ntency training is provided in Algorithm 3, and the schedule\nfunctions used in our experiments are given in Appendix C.\nSimilar to consistency distillation, the consistency training\nloss LN\nCTp\u03b8, \u03b8\u00b4q can be extended to hold in continuous time\n(i.e., N \u00d1 8) if \u03b8\u00b4 \u201c stopgradp\u03b8q, as shown in Theo-\nrem 6. This continuous-time loss function does not require\nschedule functions for N or \u00b5, but requires forward-mode\nautomatic differentiation for efficient implementation. Un-\nlike the discrete-time CT loss, there is no undesirable \u201cbias\u201d\nassociated with the continuous-time objective, as we effec-\ntively take \u2206t \u00d1 0 in Theorem 2. We relegate more details\nto Appendix B.2.\n6. Experiments\nWe employ consistency distillation and consistency train-\ning to learn consistency models on real image datasets,\nincluding CIFAR-10 (Krizhevsky et al., 2009), ImageNet\n64 \u02c6 64 (Deng et al., 2009), LSUN Bedroom 256 \u02c6 256,\nand LSUN Cat 256 \u02c6 256 (Yu et al., 2015). Results are\ncompared according to Fr\u00b4echet Inception Distance (FID,\nHeusel et al. (2017), lower is better), Inception Score (IS,\nSalimans et al. (2016), higher is better), Precision (Prec.,\nKynk\u00a8a\u00a8anniemi et al. (2019), higher is better), and Recall\n(Rec., Kynk\u00a8a\u00a8anniemi et al. (2019), higher is better). Addi-\ntional experimental details are provided in Appendix C.\n6.1. Training Consistency Models\nWe perform a series of experiments on CIFAR-10 to under-\nstand the effect of various hyperparameters on the perfor-\nmance of consistency models trained by consistency distil-\nlation (CD) and consistency training (CT). We first focus on\nthe effect of the metric function dp\u00a8, \u00a8q, the ODE solver, and\nthe number of discretization steps N in CD, then investigate\nthe effect of the schedule functions Np\u00a8q and \u00b5p\u00a8q in CT.\nTo set up our experiments for CD, we consider the squared\n\u21132 distance dpx, yq \u201c }x \u00b4 y}2\n2, \u21131 distance dpx, yq \u201c\n}x \u00b4 y}1, and the Learned Perceptual Image Patch Simi-\nlarity (LPIPS, Zhang et al. (2018)) as the metric function.\nFor the ODE solver, we compare Euler\u2019s forward method\nand Heun\u2019s second order method as detailed in Karras et al.\n(2022). For the number of discretization steps N, we com-\npare N P t9, 12, 18, 36, 50, 60, 80, 120u. All consistency\nmodels trained by CD in our experiments are initialized with\nthe corresponding pre-trained diffusion models, whereas\nmodels trained by CT are randomly initialized.\nAs visualized in Fig. 3a, the optimal metric for CD is LPIPS,\nwhich outperforms both \u21131 and \u21132 by a large margin over\nall training iterations. This is expected as the outputs of\nconsistency models are images on CIFAR-10, and LPIPS is\nspecifically designed for measuring the similarity between\nnatural images. Next, we investigate which ODE solver and\nwhich discretization step N work the best for CD. As shown\nin Figs. 3b and 3c, Heun ODE solver and N \u201c 18 are the\nbest choices. Both are in line with the recommendation\nof Karras et al. (2022) despite the fact that we are train-\ning consistency models, not diffusion models. Moreover,\nFig. 3b shows that with the same N, Heun\u2019s second order\nsolver uniformly outperforms Euler\u2019s first order solver. This\ncorroborates with Theorem 1, which states that the optimal\nconsistency models trained by higher order ODE solvers\nhave smaller estimation errors with the same N. The results\nof Fig. 3c also indicate that once N is sufficiently large, the\nperformance of CD becomes insensitive to N. Given these\ninsights, we hereafter use LPIPS and Heun ODE solver for\nCD unless otherwise stated. For N in CD, we follow the\n6\nConsistency Models\n(a) Metric functions in CD.\n(b) Solvers and N in CD.\n(c) N with Heun solver in CD.\n(d) Adaptive N and \u00b5 in CT.\nFigure 3: Various factors that affect consistency distillation (CD) and consistency training (CT) on CIFAR-10. The best\nconfiguration for CD is LPIPS, Heun ODE solver, and N \u201c 18. Our adaptive schedule functions for N and \u00b5 make CT\nconverge significantly faster than fixing them to be constants during the course of optimization.\n(a) CIFAR-10\n(b) ImageNet 64 \u02c6 64\n(c) Bedroom 256 \u02c6 256\n(d) Cat 256 \u02c6 256\nFigure 4: Multistep image generation with consistency distillation (CD). CD outperforms progressive distillation (PD)\nacross all datasets and sampling steps. The only exception is single-step generation on Bedroom 256 \u02c6 256.\nsuggestions in Karras et al. (2022) on CIFAR-10 and Im-\nageNet 64 \u02c6 64. We tune N separately on other datasets\n(details in Appendix C).\nDue to the strong connection between CD and CT, we adopt\nLPIPS for our CT experiments throughout this paper. Unlike\nCD, there is no need for using Heun\u2019s second order solver\nin CT as the loss function does not rely on any particular\nnumerical ODE solver. As demonstrated in Fig. 3d, the con-\nvergence of CT is highly sensitive to N\u2014smaller N leads\nto faster convergence but worse samples, whereas larger\nN leads to slower convergence but better samples upon\nconvergence. This matches our analysis in Section 5, and\nmotivates our practical choice of progressively growing N\nand \u00b5 for CT to balance the trade-off between convergence\nspeed and sample quality. As shown in Fig. 3d, adaptive\nschedules of N and \u00b5 significantly improve the convergence\nspeed and sample quality of CT. In our experiments, we\ntune the schedules Np\u00a8q and \u00b5p\u00a8q separately for images of\ndifferent resolutions, with more details in Appendix C.\n6.2. Few-Step Image Generation\nDistillation In current literature, the most directly compara-\nble approach to our consistency distillation (CD) is progres-\nsive distillation (PD, Salimans & Ho (2022)); both are thus\nfar the only distillation approaches that do not construct\nsynthetic data before distillation. In stark contrast, other dis-\ntillation techniques, such as knowledge distillation (Luhman\n& Luhman, 2021) and DFNO (Zheng et al., 2022), have to\nprepare a large synthetic dataset by generating numerous\nsamples from the diffusion model with expensive numerical\nODE/SDE solvers. We perform comprehensive comparison\nfor PD and CD on CIFAR-10, ImageNet 64\u02c664, and LSUN\n256 \u02c6 256, with all results reported in Fig. 4. All methods\ndistill from an EDM (Karras et al., 2022) model that we pre-\ntrained in-house. We note that across all sampling iterations,\nusing the LPIPS metric uniformly improves PD compared\nto the squared \u21132 distance in the original paper of Salimans\n& Ho (2022). Both PD and CD improve as we take more\nsampling steps. We find that CD uniformly outperforms\nPD across all datasets, sampling steps, and metric functions\nconsidered, except for single-step generation on Bedroom\n256 \u02c6 256, where CD with \u21132 slightly underperforms PD\nwith \u21132. As shown in Table 1, CD even outperforms distilla-\ntion approaches that require synthetic dataset construction,\nsuch as Knowledge Distillation (Luhman & Luhman, 2021)\nand DFNO (Zheng et al., 2022).\nDirect Generation In Tables 1 and 2, we compare the\nsample quality of consistency training (CT) with other gen-\nerative models using one-step and two-step generation. We\nalso include PD and CD results for reference. Both tables re-\nport PD results obtained from the \u21132 metric function, as this\nis the default setting used in the original paper of Salimans\n7\nConsistency Models\nTable 1: Sample quality on CIFAR-10. \u02daMethods that require\nsynthetic data construction for distillation.\nMETHOD\nNFE (\u00d3)\nFID (\u00d3)\nIS (\u00d2)\nDiffusion + Samplers\nDDIM (Song et al., 2020)\n50\n4.67\nDDIM (Song et al., 2020)\n20\n6.84\nDDIM (Song et al., 2020)\n10\n8.23\nDPM-solver-2 (Lu et al., 2022)\n10\n5.94\nDPM-solver-fast (Lu et al., 2022)\n10\n4.70\n3-DEIS (Zhang & Chen, 2022)\n10\n4.17\nDiffusion + Distillation\nKnowledge Distillation\u02da (Luhman & Luhman, 2021)\n1\n9.36\nDFNO\u02da (Zheng et al., 2022)\n1\n4.12\n1-Rectified Flow (+distill)\u02da (Liu et al., 2022)\n1\n6.18\n9.08\n2-Rectified Flow (+distill)\u02da (Liu et al., 2022)\n1\n4.85\n9.01\n3-Rectified Flow (+distill)\u02da (Liu et al., 2022)\n1\n5.21\n8.79\nPD (Salimans & Ho, 2022)\n1\n8.34\n8.69\nCD\n1\n3.55\n9.48\nPD (Salimans & Ho, 2022)\n2\n5.58\n9.05\nCD\n2\n2.93\n9.75\nDirect Generation\nBigGAN (Brock et al., 2019)\n1\n14.7\n9.22\nDiffusion GAN (Xiao et al., 2022)\n1\n14.6\n8.93\nAutoGAN (Gong et al., 2019)\n1\n12.4\n8.55\nE2GAN (Tian et al., 2020)\n1\n11.3\n8.51\nViTGAN (Lee et al., 2021)\n1\n6.66\n9.30\nTransGAN (Jiang et al., 2021)\n1\n9.26\n9.05\nStyleGAN2-ADA (Karras et al., 2020)\n1\n2.92\n9.83\nStyleGAN-XL (Sauer et al., 2022)\n1\n1.85\nScore SDE (Song et al., 2021)\n2000\n2.20\n9.89\nDDPM (Ho et al., 2020)\n1000\n3.17\n9.46\nLSGM (Vahdat et al., 2021)\n147\n2.10\nPFGM (Xu et al., 2022)\n110\n2.35\n9.68\nEDM (Karras et al., 2022)\n35\n2.04\n9.84\n1-Rectified Flow (Liu et al., 2022)\n1\n378\n1.13\nGlow (Kingma & Dhariwal, 2018)\n1\n48.9\n3.92\nResidual Flow (Chen et al., 2019)\n1\n46.4\nGLFlow (Xiao et al., 2019)\n1\n44.6\nDenseFlow (Grci\u00b4c et al., 2021)\n1\n34.9\nDC-VAE (Parmar et al., 2021)\n1\n17.9\n8.20\nCT\n1\n8.70\n8.49\nCT\n2\n5.83\n8.85\nTable 2: Sample quality on ImageNet 64 \u02c6 64, and LSUN\nBedroom & Cat 256 \u02c6 256. :Distillation techniques.\nMETHOD\nNFE (\u00d3)\nFID (\u00d3)\nPrec. (\u00d2)\nRec. (\u00d2)\nImageNet 64 \u02c6 64\nPD: (Salimans & Ho, 2022)\n1\n15.39\n0.59\n0.62\nDFNO: (Zheng et al., 2022)\n1\n8.35\nCD:\n1\n6.20\n0.68\n0.63\nPD: (Salimans & Ho, 2022)\n2\n8.95\n0.63\n0.65\nCD:\n2\n4.70\n0.69\n0.64\nADM (Dhariwal & Nichol, 2021)\n250\n2.07\n0.74\n0.63\nEDM (Karras et al., 2022)\n79\n2.44\n0.71\n0.67\nBigGAN-deep (Brock et al., 2019)\n1\n4.06\n0.79\n0.48\nCT\n1\n13.0\n0.71\n0.47\nCT\n2\n11.1\n0.69\n0.56\nLSUN Bedroom 256 \u02c6 256\nPD: (Salimans & Ho, 2022)\n1\n16.92\n0.47\n0.27\nPD: (Salimans & Ho, 2022)\n2\n8.47\n0.56\n0.39\nCD:\n1\n7.80\n0.66\n0.34\nCD:\n2\n5.22\n0.68\n0.39\nDDPM (Ho et al., 2020)\n1000\n4.89\n0.60\n0.45\nADM (Dhariwal & Nichol, 2021)\n1000\n1.90\n0.66\n0.51\nEDM (Karras et al., 2022)\n79\n3.57\n0.66\n0.45\nPGGAN (Karras et al., 2018)\n1\n8.34\nPG-SWGAN (Wu et al., 2019)\n1\n8.0\nTDPM (GAN) (Zheng et al., 2023)\n1\n5.24\nStyleGAN2 (Karras et al., 2020)\n1\n2.35\n0.59\n0.48\nCT\n1\n16.0\n0.60\n0.17\nCT\n2\n7.85\n0.68\n0.33\nLSUN Cat 256 \u02c6 256\nPD: (Salimans & Ho, 2022)\n1\n29.6\n0.51\n0.25\nPD: (Salimans & Ho, 2022)\n2\n15.5\n0.59\n0.36\nCD:\n1\n11.0\n0.65\n0.36\nCD:\n2\n8.84\n0.66\n0.40\nDDPM (Ho et al., 2020)\n1000\n17.1\n0.53\n0.48\nADM (Dhariwal & Nichol, 2021)\n1000\n5.57\n0.63\n0.52\nEDM (Karras et al., 2022)\n79\n6.69\n0.70\n0.43\nPGGAN (Karras et al., 2018)\n1\n37.5\nStyleGAN2 (Karras et al., 2020)\n1\n7.25\n0.58\n0.43\nCT\n1\n20.7\n0.56\n0.23\nCT\n2\n11.7\n0.63\n0.36\nFigure 5: Samples generated by EDM (top), CT + single-step generation (middle), and CT + 2-step generation (Bottom). All\ncorresponding images are generated from the same initial noise.\n8\nConsistency Models\n(a) Left: The gray-scale image. Middle: Colorized images. Right: The ground-truth image.\n(b) Left: The downsampled image (32 \u02c6 32). Middle: Full resolution images (256 \u02c6 256). Right: The ground-truth image (256 \u02c6 256).\n(c) Left: A stroke input provided by users. Right: Stroke-guided image generation.\nFigure 6: Zero-shot image editing with a consistency model trained by consistency distillation on LSUN Bedroom 256\u02c6256.\n& Ho (2022). For fair comparison, we ensure PD and CD\ndistill the same EDM models. In Tables 1 and 2, we observe\nthat CT outperforms existing single-step, non-adversarial\ngenerative models, i.e., VAEs and normalizing flows, by a\nsignificant margin on CIFAR-10. Moreover, CT achieves\ncomparable quality to one-step samples from PD without\nrelying on distillation. In Fig. 5, we provide EDM samples\n(top), single-step CT samples (middle), and two-step CT\nsamples (bottom). In Appendix E, we show additional sam-\nples for both CD and CT in Figs. 14 to 21. Importantly, all\nsamples obtained from the same initial noise vector share\nsignificant structural similarity, even though CT and EDM\nmodels are trained independently from one another. This\nindicates that CT is less likely to suffer from mode collapse,\nas EDMs do not.\n6.3. Zero-Shot Image Editing\nSimilar to diffusion models, consistency models allow zero-\nshot image editing by modifying the multistep sampling\nprocess in Algorithm 1. We demonstrate this capability\nwith a consistency model trained on the LSUN bedroom\ndataset using consistency distillation. In Fig. 6a, we show\nsuch a consistency model can colorize gray-scale bedroom\nimages at test time, even though it has never been trained\non colorization tasks. In Fig. 6b, we show the same con-\nsistency model can generate high-resolution images from\nlow-resolution inputs. In Fig. 6c, we additionally demon-\nstrate that it can generate images based on stroke inputs cre-\nated by humans, as in SDEdit for diffusion models (Meng\net al., 2021). Again, this editing capability is zero-shot,\nas the model has not been trained on stroke inputs. In\nAppendix D, we additionally demonstrate the zero-shot\ncapability of consistency models on inpainting (Fig. 10),\ninterpolation (Fig. 11) and denoising (Fig. 12), with more\nexamples on colorization (Fig. 8), super-resolution (Fig. 9)\nand stroke-guided image generation (Fig. 13).\n7. Conclusion\nWe have introduced consistency models, a type of generative\nmodels that are specifically designed to support one-step\nand few-step generation. We have empirically demonstrated\nthat our consistency distillation method outshines the exist-\ning distillation techniques for diffusion models on multiple\nimage benchmarks and small sampling iterations. Further-\nmore, as a standalone generative model, consistency models\ngenerate better samples than existing single-step genera-\ntion models except for GANs. Similar to diffusion models,\nthey also allow zero-shot image editing applications such as\ninpainting, colorization, super-resolution, denoising, inter-\npolation, and stroke-guided image generation.\nIn addition, consistency models share striking similarities\nwith techniques employed in other fields, including deep\nQ-learning (Mnih et al., 2015) and momentum-based con-\ntrastive learning (Grill et al., 2020; He et al., 2020). This\noffers exciting prospects for cross-pollination of ideas and\nmethods among these diverse fields.\nAcknowledgements\nWe thank Alex Nichol for reviewing the manuscript and\nproviding valuable feedback, Chenlin Meng for providing\nstroke inputs needed in our stroke-guided image generation\nexperiments, and the OpenAI Algorithms team.\n9\nConsistency Models\nReferences\nBalaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Kreis,\nK., Aittala, M., Aila, T., Laine, S., Catanzaro, B., Kar-\nras, T., and Liu, M.-Y. ediff-i: Text-to-image diffusion\nmodels with ensemble of expert denoisers. arXiv preprint\narXiv:2211.01324, 2022.\nBilo\u02c7s, M., Sommer, J., Rangapuram, S. S., Januschowski, T.,\nand G\u00a8unnemann, S. Neural flows: Efficient alternative to\nneural odes. Advances in Neural Information Processing\nSystems, 34:21325\u201321337, 2021.\nBrock, A., Donahue, J., and Simonyan, K. Large scale\nGAN training for high fidelity natural image synthesis. In\nInternational Conference on Learning Representations,\n2019. URL https://openreview.net/forum?\nid=B1xsqj09Fm.\nChen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and\nChan, W. Wavegrad: Estimating gradients for waveform\ngeneration. In International Conference on Learning\nRepresentations (ICLR), 2021.\nChen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud,\nD. K. Neural Ordinary Differential Equations. In Ad-\nvances in neural information processing systems, pp.\n6571\u20136583, 2018.\nChen, R. T., Behrmann, J., Duvenaud, D. K., and Jacobsen,\nJ.-H. Residual flows for invertible generative modeling.\nIn Advances in Neural Information Processing Systems,\npp. 9916\u20139926, 2019.\nChung, H., Kim, J., Mccann, M. T., Klasky, M. L., and Ye,\nJ. C. Diffusion posterior sampling for general noisy in-\nverse problems. In International Conference on Learning\nRepresentations, 2023. URL https://openreview.\nnet/forum?id=OnD9zGAGT0k.\nDeng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei,\nL. Imagenet: A large-scale hierarchical image database.\nIn 2009 IEEE conference on computer vision and pattern\nrecognition, pp. 248\u2013255. Ieee, 2009.\nDhariwal, P. and Nichol, A. Diffusion models beat gans\non image synthesis. Advances in Neural Information\nProcessing Systems (NeurIPS), 2021.\nDinh, L., Krueger, D., and Bengio, Y. NICE: Non-linear\nindependent components estimation. International Con-\nference in Learning Representations Workshop Track,\n2015.\nDinh, L., Sohl-Dickstein, J., and Bengio, S. Density es-\ntimation using real NVP. In 5th International Confer-\nence on Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Proceedings.\nOpenReview.net, 2017. URL https://openreview.\nnet/forum?id=HkpbnH9lx.\nDockhorn, T., Vahdat, A., and Kreis, K. Genie: Higher-\norder denoising diffusion solvers.\narXiv preprint\narXiv:2210.05475, 2022.\nGong, X., Chang, S., Jiang, Y., and Wang, Z. Autogan:\nNeural architecture search for generative adversarial net-\nworks. In Proceedings of the IEEE/CVF International\nConference on Computer Vision, pp. 3224\u20133234, 2019.\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B.,\nWarde-Farley, D., Ozair, S., Courville, A., and Bengio,\nY. Generative adversarial nets. In Advances in neural\ninformation processing systems, pp. 2672\u20132680, 2014.\nGrci\u00b4c, M., Grubi\u02c7si\u00b4c, I., and \u02c7Segvi\u00b4c, S. Densely connected\nnormalizing flows. Advances in Neural Information Pro-\ncessing Systems, 34:23968\u201323982, 2021.\nGrill, J.-B., Strub, F., Altch\u00b4e, F., Tallec, C., Richemond, P.,\nBuchatskaya, E., Doersch, C., Avila Pires, B., Guo, Z.,\nGheshlaghi Azar, M., et al. Bootstrap your own latent-a\nnew approach to self-supervised learning. Advances in\nneural information processing systems, 33:21271\u201321284,\n2020.\nHe, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. Mo-\nmentum contrast for unsupervised visual representation\nlearning. In Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, pp. 9729\u20139738,\n2020.\nHeusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and\nHochreiter, S. GANs trained by a two time-scale update\nrule converge to a local Nash equilibrium. In Advances in\nNeural Information Processing Systems, pp. 6626\u20136637,\n2017.\nHo, J., Jain, A., and Abbeel, P. Denoising Diffusion Proba-\nbilistic Models. Advances in Neural Information Process-\ning Systems, 33, 2020.\nHo, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko,\nA., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J.,\net al. Imagen video: High definition video generation\nwith diffusion models. arXiv preprint arXiv:2210.02303,\n2022a.\nHo, J., Salimans, T., Gritsenko, A. A., Chan, W., Norouzi,\nM., and Fleet, D. J. Video diffusion models. In ICLR\nWorkshop on Deep Generative Models for Highly Struc-\ntured Data, 2022b.\nURL https://openreview.\nnet/forum?id=BBelR2NdDZ5.\nHyv\u00a8arinen, A. and Dayan, P. Estimation of non-normalized\nstatistical models by score matching. Journal of Machine\nLearning Research (JMLR), 6(4), 2005.\n10\nConsistency Models\nJiang, Y., Chang, S., and Wang, Z. Transgan: Two pure\ntransformers can make one strong gan, and that can scale\nup. Advances in Neural Information Processing Systems,\n34:14745\u201314758, 2021.\nKarras, T., Aila, T., Laine, S., and Lehtinen, J. Progres-\nsive growing of GANs for improved quality, stability,\nand variation. In International Conference on Learning\nRepresentations, 2018. URL https://openreview.\nnet/forum?id=Hk99zCeAb.\nKarras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J.,\nand Aila, T. Analyzing and improving the image quality\nof stylegan. 2020.\nKarras, T., Aittala, M., Aila, T., and Laine, S. Elucidating\nthe design space of diffusion-based generative models. In\nProc. NeurIPS, 2022.\nKawar, B., Vaksman, G., and Elad, M. Snips: Solving\nnoisy inverse problems stochastically. arXiv preprint\narXiv:2105.14951, 2021.\nKawar, B., Elad, M., Ermon, S., and Song, J. Denoising\ndiffusion restoration models. In Advances in Neural In-\nformation Processing Systems, 2022.\nKingma, D. P. and Dhariwal, P. Glow: Generative flow\nwith invertible 1x1 convolutions. In Bengio, S., Wal-\nlach, H., Larochelle, H., Grauman, K., Cesa-Bianchi, N.,\nand Garnett, R. (eds.), Advances in Neural Information\nProcessing Systems 31, pp. 10215\u201310224. 2018.\nKingma, D. P. and Welling, M. Auto-encoding variational\nbayes. In International Conference on Learning Repre-\nsentations, 2014.\nKong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro,\nB. DiffWave: A Versatile Diffusion Model for Audio\nSynthesis. arXiv preprint arXiv:2009.09761, 2020.\nKrizhevsky, A., Hinton, G., et al. Learning multiple layers\nof features from tiny images. 2009.\nKynk\u00a8a\u00a8anniemi, T., Karras, T., Laine, S., Lehtinen, J., and\nAila, T. Improved precision and recall metric for assess-\ning generative models. Advances in Neural Information\nProcessing Systems, 32, 2019.\nLee, K., Chang, H., Jiang, L., Zhang, H., Tu, Z., and Liu,\nC. Vitgan: Training gans with vision transformers. arXiv\npreprint arXiv:2107.04589, 2021.\nLillicrap, T. P., Hunt, J. J., Pritzel, A., Heess, N., Erez,\nT., Tassa, Y., Silver, D., and Wierstra, D. Continuous\ncontrol with deep reinforcement learning. arXiv preprint\narXiv:1509.02971, 2015.\nLiu, L., Jiang, H., He, P., Chen, W., Liu, X., Gao, J., and\nHan, J. On the variance of the adaptive learning rate and\nbeyond. arXiv preprint arXiv:1908.03265, 2019.\nLiu, X., Gong, C., and Liu, Q.\nFlow straight and fast:\nLearning to generate and transfer data with rectified flow.\narXiv preprint arXiv:2209.03003, 2022.\nLu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J.\nDpm-solver: A fast ode solver for diffusion probabilis-\ntic model sampling in around 10 steps. arXiv preprint\narXiv:2206.00927, 2022.\nLuhman, E. and Luhman, T. Knowledge distillation in\niterative generative models for improved sampling speed.\narXiv preprint arXiv:2101.02388, 2021.\nMeng, C., Song, Y., Song, J., Wu, J., Zhu, J.-Y., and Ermon,\nS. Sdedit: Image synthesis and editing with stochastic\ndifferential equations. arXiv preprint arXiv:2108.01073,\n2021.\nMeng, C., Gao, R., Kingma, D. P., Ermon, S., Ho, J., and\nSalimans, T. On distillation of guided diffusion models.\narXiv preprint arXiv:2210.03142, 2022.\nMnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,\nAntonoglou, I., Wierstra, D., and Riedmiller, M. Playing\natari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\nMnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,\nJ., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-\nland, A. K., Ostrovski, G., et al. Human-level control\nthrough deep reinforcement learning. nature, 518(7540):\n529\u2013533, 2015.\nNichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,\nP., McGrew, B., Sutskever, I., and Chen, M.\nGlide:\nTowards photorealistic image generation and editing\nwith text-guided diffusion models.\narXiv preprint\narXiv:2112.10741, 2021.\nParmar, G., Li, D., Lee, K., and Tu, Z. Dual contradistinctive\ngenerative autoencoder. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npp. 823\u2013832, 2021.\nPopov, V., Vovk, I., Gogoryan, V., Sadekova, T., and Kudi-\nnov, M. Grad-TTS: A diffusion probabilistic model for\ntext-to-speech. arXiv preprint arXiv:2105.06337, 2021.\nRamesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,\nM. Hierarchical text-conditional image generation with\nclip latents. arXiv preprint arXiv:2204.06125, 2022.\nRezende, D. J., Mohamed, S., and Wierstra, D. Stochastic\nbackpropagation and approximate inference in deep gen-\nerative models. In Proceedings of the 31st International\nConference on Machine Learning, pp. 1278\u20131286, 2014.\n11\nConsistency Models\nRombach, R., Blattmann, A., Lorenz, D., Esser, P., and\nOmmer, B. High-resolution image synthesis with latent\ndiffusion models. In Proceedings of the IEEE/CVF Con-\nference on Computer Vision and Pattern Recognition, pp.\n10684\u201310695, 2022.\nSaharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton,\nE., Ghasemipour, S. K. S., Ayan, B. K., Mahdavi, S. S.,\nLopes, R. G., et al. Photorealistic text-to-image diffusion\nmodels with deep language understanding. arXiv preprint\narXiv:2205.11487, 2022.\nSalimans, T. and Ho, J. Progressive distillation for fast\nsampling of diffusion models. In International Confer-\nence on Learning Representations, 2022. URL https:\n//openreview.net/forum?id=TIdIXIpzhoI.\nSalimans, T., Goodfellow, I., Zaremba, W., Cheung, V.,\nRadford, A., and Chen, X. Improved techniques for train-\ning gans. In Advances in neural information processing\nsystems, pp. 2234\u20132242, 2016.\nSauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling\nstylegan to large diverse datasets. In ACM SIGGRAPH\n2022 conference proceedings, pp. 1\u201310, 2022.\nSohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and\nGanguli, S. Deep Unsupervised Learning Using Nonequi-\nlibrium Thermodynamics. In International Conference\non Machine Learning, pp. 2256\u20132265, 2015.\nSong, J., Meng, C., and Ermon, S. Denoising diffusion\nimplicit models. arXiv preprint arXiv:2010.02502, 2020.\nSong, J., Vahdat, A., Mardani, M., and Kautz, J.\nPseudoinverse-guided diffusion models for inverse prob-\nlems. In International Conference on Learning Represen-\ntations, 2023. URL https://openreview.net/\nforum?id=9_gsMA8MRKQ.\nSong, Y. and Ermon, S. Generative Modeling by Estimating\nGradients of the Data Distribution. In Advances in Neural\nInformation Processing Systems, pp. 11918\u201311930, 2019.\nSong, Y. and Ermon, S. Improved Techniques for Training\nScore-Based Generative Models. Advances in Neural\nInformation Processing Systems, 33, 2020.\nSong, Y., Garg, S., Shi, J., and Ermon, S. Sliced score\nmatching: A scalable approach to density and score esti-\nmation. In Proceedings of the Thirty-Fifth Conference on\nUncertainty in Artificial Intelligence, UAI 2019, Tel Aviv,\nIsrael, July 22-25, 2019, pp. 204, 2019.\nSong, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A.,\nErmon, S., and Poole, B. Score-based generative mod-\neling through stochastic differential equations. In In-\nternational Conference on Learning Representations,\n2021. URL https://openreview.net/forum?\nid=PxTIG12RRHS.\nSong, Y., Shen, L., Xing, L., and Ermon, S. Solving inverse\nproblems in medical imaging with score-based genera-\ntive models. In International Conference on Learning\nRepresentations, 2022. URL https://openreview.\nnet/forum?id=vaRCHVj0uGI.\nS\u00a8uli, E. and Mayers, D. F. An introduction to numerical\nanalysis. Cambridge university press, 2003.\nTian, Y., Wang, Q., Huang, Z., Li, W., Dai, D., Yang, M.,\nWang, J., and Fink, O. Off-policy reinforcement learn-\ning for efficient and effective gan architecture search. In\nComputer Vision\u2013ECCV 2020: 16th European Confer-\nence, Glasgow, UK, August 23\u201328, 2020, Proceedings,\nPart VII 16, pp. 175\u2013192. Springer, 2020.\nVahdat, A., Kreis, K., and Kautz, J. Score-based generative\nmodeling in latent space. Advances in Neural Information\nProcessing Systems, 34:11287\u201311302, 2021.\nVincent, P. A Connection Between Score Matching and\nDenoising Autoencoders. Neural Computation, 23(7):\n1661\u20131674, 2011.\nWu, J., Huang, Z., Acharya, D., Li, W., Thoma, J., Paudel,\nD. P., and Gool, L. V. Sliced wasserstein generative\nmodels. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pp. 3713\u2013\n3722, 2019.\nXiao, Z., Yan, Q., and Amit, Y. Generative latent flow. arXiv\npreprint arXiv:1905.10485, 2019.\nXiao, Z., Kreis, K., and Vahdat, A. Tackling the generative\nlearning trilemma with denoising diffusion GANs. In\nInternational Conference on Learning Representations,\n2022. URL https://openreview.net/forum?\nid=JprM0p-q0Co.\nXu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. S. Pois-\nson flow generative models. In Oh, A. H., Agarwal, A.,\nBelgrave, D., and Cho, K. (eds.), Advances in Neural\nInformation Processing Systems, 2022. URL https:\n//openreview.net/forum?id=voV_TRqcWh.\nYu, F., Seff, A., Zhang, Y., Song, S., Funkhouser, T., and\nXiao, J. Lsun: Construction of a large-scale image dataset\nusing deep learning with humans in the loop.\narXiv\npreprint arXiv:1506.03365, 2015.\nZhang, Q. and Chen, Y.\nFast sampling of diffusion\nmodels with exponential integrator.\narXiv preprint\narXiv:2204.13902, 2022.\n12\nConsistency Models\nZhang, R., Isola, P., Efros, A. A., Shechtman, E., and Wang,\nO. The unreasonable effectiveness of deep features as a\nperceptual metric. In CVPR, 2018.\nZheng, H., Nie, W., Vahdat, A., Azizzadenesheli, K., and\nAnandkumar, A.\nFast sampling of diffusion models\nvia operator learning. arXiv preprint arXiv:2211.13449,\n2022.\nZheng, H., He, P., Chen, W., and Zhou, M. Truncated diffu-\nsion probabilistic models and diffusion-based adversarial\nauto-encoders. In The Eleventh International Confer-\nence on Learning Representations, 2023. URL https:\n//openreview.net/forum?id=HDxgaKk956l.\n13\nConsistency Models\nContents\n1\nIntroduction\n1\n2\nDiffusion Models\n2\n3\nConsistency Models\n3\n4\nTraining Consistency Models via Distillation\n4\n5\nTraining Consistency Models in Isolation\n5\n6\nExperiments\n6\n6.1\nTraining Consistency Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n6\n6.2\nFew-Step Image Generation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n7\n6.3\nZero-Shot Image Editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n9\n7\nConclusion\n9\nAppendices\n15\nAppendix A Proofs\n15\nA.1\nNotations\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.2\nConsistency Distillation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n15\nA.3\nConsistency Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n16\nAppendix B\nContinuous-Time Extensions\n18\nB.1\nConsistency Distillation in Continuous Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n18\nB.2\nConsistency Training in Continuous Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n22\nB.3\nExperimental Verifications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n24\nAppendix C Additional Experimental Details\n25\nModel Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nParameterization for Consistency Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n25\nSchedule Functions for Consistency Training . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nTraining Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n26\nAppendix D Additional Results on Zero-Shot Image Editing\n26\nInpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nColorization\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n27\nSuper-resolution\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\n14\nConsistency Models\nStroke-guided image generation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nDenoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nInterpolation\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n28\nAppendix E Additional Samples from Consistency Models\n28\nAppendices\nA. Proofs\nA.1. Notations\nWe use f\u03b8px, tq to denote a consistency model parameterized by \u03b8, and fpx, t; \u03d5q the consistency function of the empirical\nPF ODE in Eq. (3). Here \u03d5 symbolizes its dependency on the pre-trained score model s\u03d5px, tq. For the consistency function\nof the PF ODE in Eq. (2), we denote it as fpx, tq. Given a multi-variate function hpx, yq, we let B1hpx, yq denote the\nJacobian of h over x, and analogously B2hpx, yq denote the Jacobian of h over y. Unless otherwise stated, x is supposed to\nbe a random variable sampled from the data distribution pdatapxq, n is sampled uniformly at random from J1, N \u00b4 1K, and\nxtn is sampled from Npx; t2\nnIq. Here J1, N \u00b4 1K represents the set of integers t1, 2, \u00a8 \u00a8 \u00a8 , N \u00b4 1u. Furthermore, recall that\nwe define\n\u02c6x\u03d5\ntn :\u201c xtn`1 ` ptn \u00b4 tn`1q\u03a6pxtn`1, tn`1; \u03d5q,\nwhere \u03a6p\u00a8 \u00a8 \u00a8 ; \u03d5q denotes the update function of a one-step ODE solver for the empirical PF ODE defined by the score\nmodel s\u03d5px, tq. By default, Er\u00a8s denotes the expectation over all relevant random variables in the expression.\nA.2. Consistency Distillation\nTheorem 1. Let \u2206t :\u201c maxnPJ1,N\u00b41Kt|tn`1 \u00b4 tn|u, and fp\u00a8, \u00a8; \u03d5q be the consistency function of the empirical PF ODE\nin Eq. (3). Assume f\u03b8 satisfies the Lipschitz condition: there exists L \u0105 0 such that for all t P r\u03f5, Ts, x, and y, we have\n\u2225f\u03b8px, tq \u00b4 f\u03b8py, tq\u22252 \u010f L \u2225x \u00b4 y\u22252. Assume further that for all n P J1, N \u00b4 1K, the ODE solver called at tn`1 has local\nerror uniformly bounded by Opptn`1 \u00b4 tnqp`1q with p \u011b 1. Then, if LN\nCDp\u03b8, \u03b8; \u03d5q \u201c 0, we have\nsup\nn,x }f\u03b8px, tnq \u00b4 fpx, tn; \u03d5q}2 \u201c Opp\u2206tqpq.\nProof. From LN\nCDp\u03b8, \u03b8; \u03d5q \u201c 0, we have\nLN\nCDp\u03b8, \u03b8; \u03d5q \u201c Er\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8p\u02c6x\u03d5\ntn, tnqqs \u201c 0.\n(11)\nAccording to the definition, we have ptnpxtnq \u201c pdatapxq b Np0, t2\nnIq where tn \u011b \u03f5 \u0105 0. It follows that ptnpxtnq \u0105 0 for\nevery xtn and 1 \u010f n \u010f N. Therefore, Eq. (11) entails\n\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8p\u02c6x\u03d5\ntn, tnqq \u201d 0.\n(12)\nBecause \u03bbp\u00a8q \u0105 0 and dpx, yq \u201c 0 \u00f4 x \u201c y, this further implies that\nf\u03b8pxtn`1, tn`1q \u201d f\u03b8p\u02c6x\u03d5\ntn, tnq.\n(13)\nNow let en represent the error vector at tn, which is defined as\nen :\u201c f\u03b8pxtn, tnq \u00b4 fpxtn, tn; \u03d5q.\nWe can easily derive the following recursion relation\nen`1 \u201c f\u03b8pxtn`1, tn`1q \u00b4 fpxtn`1, tn`1; \u03d5q\n15\nConsistency Models\npiq\n\u201c f\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 fpxtn, tn; \u03d5q\n\u201c f\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 f\u03b8pxtn, tnq ` f\u03b8pxtn, tnq \u00b4 fpxtn, tn; \u03d5q\n\u201c f\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 f\u03b8pxtn, tnq ` en,\n(14)\nwhere (i) is due to Eq. (13) and fpxtn`1, tn`1; \u03d5q \u201c fpxtn, tn; \u03d5q. Because f\u03b8p\u00a8, tnq has Lipschitz constant L, we have\n\u2225en`1\u22252 \u010f \u2225en\u22252 ` L\n\r\r\r\u02c6x\u03d5\ntn \u00b4 xtn\n\r\r\r\n2\npiq\n\u201c \u2225en\u22252 ` L \u00a8 Opptn`1 \u00b4 tnqp`1q\n\u201c \u2225en\u22252 ` Opptn`1 \u00b4 tnqp`1q,\nwhere (i) holds because the ODE solver has local error bounded by Opptn`1 \u00b4 tnqp`1q. In addition, we observe that e1 \u201c 0,\nbecause\ne1 \u201c f\u03b8pxt1, t1q \u00b4 fpxt1, t1; \u03d5q\npiq\n\u201c xt1 \u00b4 fpxt1, t1; \u03d5q\npiiq\n\u201c xt1 \u00b4 xt1\n\u201c 0.\nHere (i) is true because the consistency model is parameterized such that fpxt1, t1; \u03d5q \u201c xt1 and (ii) is entailed by the\ndefinition of fp\u00a8, \u00a8; \u03d5q. This allows us to perform induction on the recursion formula Eq. (14) to obtain\n\u2225en\u22252 \u010f \u2225e1\u22252 `\nn\u00b41\n\u00ff\nk\u201c1\nOpptk`1 \u00b4 tkqp`1q\n\u201c\nn\u00b41\n\u00ff\nk\u201c1\nOpptk`1 \u00b4 tkqp`1q\n\u201c\nn\u00b41\n\u00ff\nk\u201c1\nptk`1 \u00b4 tkqOpptk`1 \u00b4 tkqpq\n\u010f\nn\u00b41\n\u00ff\nk\u201c1\nptk`1 \u00b4 tkqOpp\u2206tqpq\n\u201c Opp\u2206tqpq\nn\u00b41\n\u00ff\nk\u201c1\nptk`1 \u00b4 tkq\n\u201c Opp\u2206tqpqptn \u00b4 t1q\n\u010f Opp\u2206tqpqpT \u00b4 \u03f5q\n\u201c Opp\u2206tqpq,\nwhich completes the proof.\nA.3. Consistency Training\nThe following lemma provides an unbiased estimator for the score function, which is crucial to our proof for Theorem 2.\nLemma 1. Let x \u201e pdatapxq, xt \u201e Npx; t2Iq, and ptpxtq \u201c pdatapxqbNp0, t2Iq. We have \u2207log ptpxq \u201c \u00b4Er xt\u00b4x\nt2\n| xts.\nProof. According to the definition of ptpxtq, we have \u2207log ptpxtq \u201c \u2207xt log\n\u015f\npdatapxqppxt | xq dx, where ppxt | xq \u201c\nNpxt; x, t2Iq. This expression can be further simplified to yield\n\u2207log ptpxtq \u201c\n\u015f\npdatapxq\u2207xtppxt | xq dx\n\u015f\npdatapxqppxt | xq dx\n16\nConsistency Models\n\u201c\n\u015f\npdatapxqppxt | xq\u2207xt log ppxt | xq dx\n\u015f\npdatapxqppxt | xq dx\n\u201c\n\u015f\npdatapxqppxt | xq\u2207xt log ppxt | xq dx\nptpxtq\n\u201c\n\u017c pdatapxqppxt | xq\nptpxtq\n\u2207xt log ppxt | xq dx\npiq\n\u201c\n\u017c\nppx | xtq\u2207xt log ppxt | xq dx\n\u201c Er\u2207xt log ppxt | xq | xts\n\u201c \u00b4E\n\u201ext \u00b4 x\nt2\n| xt\n\u0237\n,\nwhere (i) is due to Bayes\u2019 rule.\nTheorem 2. Let \u2206t :\u201c maxnPJ1,N\u00b41Kt|tn`1 \u00b4 tn|u. Assume d and f\u03b8\u00b4 are both twice continuously differentiable with\nbounded second derivatives, the weighting function \u03bbp\u00a8q is bounded, and Er\u2225\u2207log ptnpxtnq\u22252\n2s \u0103 8. Assume further that\nwe use the Euler ODE solver, and the pre-trained score model matches the ground truth, i.e., @t P r\u03f5, Ts : s\u03d5px, tq \u201d\n\u2207log ptpxq. Then,\nLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c LN\nCTp\u03b8, \u03b8\u00b4q ` op\u2206tq,\nwhere the expectation is taken with respect to x \u201e pdata, n \u201e UJ1, N \u00b4 1K, and xtn`1 \u201e Npx; t2\nn`1Iq. The consistency\ntraining objective, denoted by LN\nCTp\u03b8, \u03b8\u00b4q, is defined as\nEr\u03bbptnqdpf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4px ` tnz, tnqqs,\nwhere z \u201e Np0, Iq. Moreover, LN\nCTp\u03b8, \u03b8\u00b4q \u011b Op\u2206tq if infN LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u0105 0.\nProof. With Taylor expansion, we have\nLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c Er\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqs\n\u201cEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1 ` ptn`1 \u00b4 tnqtn`1\u2207log ptn`1pxtn`1q, tnqqs\n\u201cEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1q ` B1f\u03b8\u00b4pxtn`1, tn`1qptn`1 \u00b4 tnqtn`1\u2207log ptn`1pxtn`1q\n` B2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1q ` op|tn`1 \u00b4 tn|qqs\n\u201cEt\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qq ` \u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqr\nB1f\u03b8\u00b4pxtn`1, tn`1qptn`1 \u00b4 tnqtn`1\u2207log ptn`1pxtn`1q ` B2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1q ` op|tn`1 \u00b4 tn|qsu\n\u201cEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqs\n` Et\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqrB1f\u03b8\u00b4pxtn`1, tn`1qptn`1 \u00b4 tnqtn`1\u2207log ptn`1pxtn`1qsu\n` Et\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqrB2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qsu ` Erop|tn`1 \u00b4 tn|qs.\n(15)\nThen, we apply Lemma 1 to Eq. (15) and use Taylor expansion in the reverse direction to obtain\nLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q\n\u201cEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqs\n` E\n\"\n\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qq\n\u201e\nB1f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qtn`1E\n\u201extn`1 \u00b4 x\nt2\nn`1\n\u02c7\u02c7\u02c7xtn`1\n\u0237\u0237*\n` Et\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqrB2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qsu ` Erop|tn`1 \u00b4 tn|qs\npiq\n\u201cEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqs\n` E\n\"\n\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qq\n\u201e\nB1f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qtn`1\n\u02c6xtn`1 \u00b4 x\nt2\nn`1\n\u02d9\u0237*\n17\nConsistency Models\n` Et\u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqrB2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qsu ` Erop|tn`1 \u00b4 tn|qs\n\u201cE\n\u201e\n\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qq\n` \u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qq\n\u201e\nB1f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qtn`1\n\u02c6xtn`1 \u00b4 x\nt2\nn`1\n\u02d9\u0237\n` \u03bbptnqB2dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4pxtn`1, tn`1qqrB2f\u03b8\u00b4pxtn`1, tn`1qptn \u00b4 tn`1qs ` op|tn`1 \u00b4 tn|q\n\u0237\n` Erop|tn`1 \u00b4 tn|qs\n\u201cE\n\u201e\n\u03bbptnqd\n\u02c6\nf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4\n\u02c6\nxtn`1 ` ptn \u00b4 tn`1qtn`1\nxtn`1 \u00b4 x\nt2\nn`1\n, tn\n\u02d9\u02d9\u0237\n` Erop|tn`1 \u00b4 tn|qs\n\u201cE\n\u201e\n\u03bbptnqd\n\u02c6\nf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4\n\u02c6\nxtn`1 ` ptn \u00b4 tn`1qxtn`1 \u00b4 x\ntn`1\n, tn\n\u02d9\u02d9\u0237\n` Erop|tn`1 \u00b4 tn|qs\n\u201cE r\u03bbptnqd pf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4 px ` tn`1z ` ptn \u00b4 tn`1qz, tnqqs ` Erop|tn`1 \u00b4 tn|qs\n\u201cE r\u03bbptnqd pf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4 px ` tnz, tnqqs ` Erop|tn`1 \u00b4 tn|qs\n\u201cE r\u03bbptnqd pf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4 px ` tnz, tnqqs ` Erop\u2206tqs\n\u201cE r\u03bbptnqd pf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4 px ` tnz, tnqqs ` op\u2206tq\n\u201cLN\nCTp\u03b8, \u03b8\u00b4q ` op\u2206tq,\n(16)\nwhere (i) is due to the law of total expectation, and z :\u201c\nxtn`1\u00b4x\ntn`1\n\u201e Np0, Iq.\nThis implies LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\nLN\nCTp\u03b8, \u03b8\u00b4q ` op\u2206tq and thus completes the proof for Eq. (9). Moreover, we have LN\nCTp\u03b8, \u03b8\u00b4q \u011b Op\u2206tq whenever\ninfN LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u0105 0. Otherwise, LN\nCTp\u03b8, \u03b8\u00b4q \u0103 Op\u2206tq and thus lim\u2206t\u00d10 LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c 0, which is a clear\ncontradiction to infN LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u0105 0.\nRemark 1. When the condition LN\nCTp\u03b8, \u03b8\u00b4q \u011b Op\u2206tq is not satisfied, such as in the case where \u03b8\u00b4 \u201c stopgradp\u03b8q, the\nvalidity of LN\nCTp\u03b8, \u03b8\u00b4q as a training objective for consistency models can still be justified by referencing the result provided\nin Theorem 6.\nB. Continuous-Time Extensions\nThe consistency distillation and consistency training objectives can be generalized to hold for infinite time steps (N \u00d1 8)\nunder suitable conditions.\nB.1. Consistency Distillation in Continuous Time\nDepending on whether \u03b8\u00b4 \u201c \u03b8 or \u03b8\u00b4 \u201c stopgradp\u03b8q (same as setting \u00b5 \u201c 0), there are two possible continuous-time\nextensions for the consistency distillation objective LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q. Given a twice continuously differentiable metric function\ndpx, yq, we define Gpxq as a matrix, whose pi, jq-th entry is given by\nrGpxqsij :\u201c B2dpx, yq\nByiByj\n\u02c7\u02c7\u02c7\u02c7\ny\u201cx\n.\nSimilarly, we define Hpxq as\nrHpxqsij :\u201c B2dpy, xq\nByiByj\n\u02c7\u02c7\u02c7\u02c7\ny\u201cx\n.\nThe matrices G and H play a crucial role in forming continuous-time objectives for consistency distillation. Additionally,\nwe denote the Jacobian of f\u03b8px, tq with respect to x as Bf\u03b8px,tq\nBx\n.\nWhen \u03b8\u00b4 \u201c \u03b8 (with no stopgrad operator), we have the following theoretical result.\nTheorem 3. Let tn \u201c \u03c4p n\u00b41\nN\u00b41q, where n P J1, NK, and \u03c4p\u00a8q is a strictly monotonic function with \u03c4p0q \u201c \u03f5 and \u03c4p1q \u201c T.\nAssume \u03c4 is continuously differentiable in r0, 1s, d is three times continuously differentiable with bounded third derivatives,\n18\nConsistency Models\nand f\u03b8 is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunction \u03bbp\u00a8q is bounded, and supx,tPr\u03f5,T s \u2225s\u03d5px, tq\u22252 \u0103 8. Then with the Euler solver in consistency distillation, we have\nlim\nN\u00d18pN \u00b4 1q2LN\nCDp\u03b8, \u03b8; \u03d5q \u201c L8\nCDp\u03b8, \u03b8; \u03d5q,\n(17)\nwhere L8\nCDp\u03b8, \u03b8; \u03d5q is defined as\n1\n2E\n\u00ab\n\u03bbptq\nrp\u03c4 \u00b41q1ptqs2\n\u02c6Bf\u03b8pxt, tq\nBt\n\u00b4 tBf\u03b8pxt, tq\nBxt\ns\u03d5pxt, tq\n\u02d9T\nGpf\u03b8pxt, tqq\n\u02c6Bf\u03b8pxt, tq\nBt\n\u00b4 tBf\u03b8pxt, tq\nBxt\ns\u03d5pxt, tq\n\u02d9ff\n. (18)\nHere the expectation above is taken over x \u201e pdata, u \u201e Ur0, 1s, t \u201c \u03c4puq, and xt \u201e Npx, t2Iq.\nProof. Let \u2206u \u201c\n1\nN\u00b41 and un \u201c n\u00b41\nN\u00b41. First, we can derive the following equation with Taylor expansion:\nf\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 f\u03b8pxtn`1, tn`1q \u201c f\u03b8pxtn`1 ` tn`1s\u03d5pxtn`1, tn`1q\u03c4 1punq\u2206u, tnq \u00b4 f\u03b8pxtn`1, tn`1q\n\u201ctn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\u03c4 1punq\u2206u \u00b4 Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u03c4 1punq\u2206u ` Opp\u2206uq2q,\n(19)\nNote that \u03c4 1punq \u201c\n1\n\u03c4 \u00b41ptn`1q. Then, we apply Taylor expansion to the consistency distillation loss, which gives\npN \u00b4 1q2LN\nCDp\u03b8, \u03b8; \u03d5q \u201c\n1\np\u2206uq2 LN\nCDp\u03b8, \u03b8; \u03d5q \u201c\n1\np\u2206uq2 Er\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8p\u02c6x\u03d5\ntn, tnqs\npiq\n\u201c\n1\n2p\u2206uq2\n\u02c6\nEt\u03bbptnq\u03c4 1punq2rf\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 f\u03b8pxtn`1, tn`1qsTGpf\u03b8pxtn`1, tn`1qq\n\u00a8 rf\u03b8p\u02c6x\u03d5\ntn, tnq \u00b4 f\u03b8pxtn`1, tn`1qsu ` ErOp|\u2206u|3qs\n\u02d9\npiiq\n\u201c 1\n2E\n\u201e\n\u03bbptnq\u03c4 1punq2\n\u02c6Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u00b4 tn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\n\u02d9T\nGpf\u03b8pxtn`1, tn`1qq\n\u00a8\n\u02c6Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u00b4 tn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\n\u02d9\u0237\n` ErOp|\u2206u|qs\n\u201c1\n2E\n\u201e\n\u03bbptnq\nrp\u03c4 \u00b41q1ptnqs2\n\u02c6Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u00b4 tn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\n\u02d9T\nGpf\u03b8pxtn`1, tn`1qq\n\u00a8\n\u02c6Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u00b4 tn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\n\u02d9\u0237\n` ErOp|\u2206u|qs\n(20)\nwhere we obtain (i) by expanding dpf\u03b8pxtn`1, tn`1q, \u00a8q to second order and observing dpx, xq \u201d 0 and \u2207ydpx, yq|y\u201cx \u201d 0.\nWe obtain (ii) using Eq. (19). By taking the limit for both sides of Eq. (20) as \u2206u \u00d1 0 or equivalently N \u00d1 8, we arrive at\nEq. (17), which completes the proof.\nRemark 2. Although Theorem 3 assumes the Euler ODE solver for technical simplicity, we believe an analogous result can\nbe derived for more general solvers, since all ODE solvers should perform similarly as N \u00d1 8. We leave a more general\nversion of Theorem 3 as future work.\nRemark 3. Theorem 3 implies that consistency models can be trained by minimizing L8\nCDp\u03b8, \u03b8; \u03d5q. In particular, when\ndpx, yq \u201c \u2225x \u00b4 y\u22252\n2, we have\nL8\nCDp\u03b8, \u03b8; \u03d5q \u201c E\n\u00ab\n\u03bbptq\nrp\u03c4 \u00b41q1ptqs2\n\r\r\r\r\nBf\u03b8pxt, tq\nBt\n\u00b4 tBf\u03b8pxt, tq\nBxt\ns\u03d5pxt, tq\n\r\r\r\r\n2\n2\nff\n.\n(21)\nHowever, this continuous-time objective requires computing Jacobian-vector products as a subroutine to evaluate the loss\nfunction, which can be slow and laborious to implement in deep learning frameworks that do not support forward-mode\nautomatic differentiation.\n19\nConsistency Models\nRemark 4. If f\u03b8px, tq matches the ground truth consistency function for the empirical PF ODE of s\u03d5px, tq, then\nBf\u03b8px, tq\nBt\n\u00b4 tBf\u03b8px, tq\nBx\ns\u03d5px, tq \u201d 0\nand therefore L8\nCDp\u03b8, \u03b8; \u03d5q \u201c 0. This can be proved by noting that f\u03b8pxt, tq \u201d x\u03f5 for all t P r\u03f5, Ts, and then taking the\ntime-derivative of this identity:\nf\u03b8pxt, tq \u201d x\u03f5\n\u00f0\u00f1Bf\u03b8pxt, tq\nBxt\ndxt\ndt ` Bf\u03b8pxt, tq\nBt\n\u201d 0\n\u00f0\u00f1Bf\u03b8pxt, tq\nBxt\nr\u00b4ts\u03d5pxt, tqs ` Bf\u03b8pxt, tq\nBt\n\u201d 0\n\u00f0\u00f1Bf\u03b8pxt, tq\nBt\n\u00b4 tBf\u03b8pxt, tq\nBxt\ns\u03d5pxt, tq \u201d 0.\nThe above observation provides another motivation for L8\nCDp\u03b8, \u03b8; \u03d5q, as it is minimized if and only if the consistency model\nmatches the ground truth consistency function.\nFor some metric functions, such as the \u21131 norm, the Hessian Gpxq is zero so Theorem 3 is vacuous. Below we show that a\nnon-vacuous statement holds for the \u21131 norm with just a small modification of the proof for Theorem 3.\nTheorem 4. Let tn \u201c \u03c4p n\u00b41\nN\u00b41q, where n P J1, NK, and \u03c4p\u00a8q is a strictly monotonic function with \u03c4p0q \u201c \u03f5 and \u03c4p1q \u201c T.\nAssume \u03c4 is continuously differentiable in r0, 1s, and f\u03b8 is twice continuously differentiable with bounded first and second\nderivatives. Assume further that the weighting function \u03bbp\u00a8q is bounded, and supx,tPr\u03f5,T s \u2225s\u03d5px, tq\u22252 \u0103 8. Suppose we use\nthe Euler ODE solver, and set dpx, yq \u201c \u2225x \u00b4 y\u22251 in consistency distillation. Then we have\nlim\nN\u00d18pN \u00b4 1qLN\nCDp\u03b8, \u03b8; \u03d5q \u201c L8\nCD, \u21131p\u03b8, \u03b8; \u03d5q,\n(22)\nwhere\nL8\nCD, \u21131p\u03b8, \u03b8; \u03d5q :\u201c E\n\u201e\n\u03bbptq\np\u03c4 \u00b41q1ptq\n\r\r\r\rtBf\u03b8pxt, tq\nBxt\ns\u03d5pxt, tq \u00b4 Bf\u03b8pxt, tq\nBt\n\r\r\r\r\n1\n\u0237\nwhere the expectation above is taken over x \u201e pdata, u \u201e Ur0, 1s, t \u201c \u03c4puq, and xt \u201e Npx, t2Iq.\nProof. Let \u2206u \u201c\n1\nN\u00b41 and un \u201c n\u00b41\nN\u00b41. We have\npN \u00b4 1qLN\nCDp\u03b8, \u03b8; \u03d5q \u201c\n1\n\u2206uLN\nCDp\u03b8, \u03b8; \u03d5q \u201c\n1\n\u2206uEr\u03bbptnq}f\u03b8pxtn`1, tn`1q \u00b4 f\u03b8p\u02c6x\u03d5\ntn, tnq}1s\npiq\n\u201c 1\n\u2206uE\n\u201e\n\u03bbptnq\n\r\r\r\rtn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\u03c4 1punq \u00b4 Bf\u03b8pxtn`1, tn`1q\nBtn`1\n\u03c4 1punq ` Opp\u2206uq2q\n\r\r\r\r\n1\n\u0237\n\u201cE\n\u201e\n\u03bbptnq\u03c4 1punq\n\r\r\r\rtn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q \u00b4 Bf\u03b8pxtn`1, tn`1q\nBtn`1\n` Op\u2206uq\n\r\r\r\r\n1\n\u0237\n\u201cE\n\u201e\n\u03bbptnq\np\u03c4 \u00b41q1ptnq\n\r\r\r\rtn`1\nBf\u03b8pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q \u00b4 Bf\u03b8pxtn`1, tn`1q\nBtn`1\n` Op\u2206uq\n\r\r\r\r\n1\n\u0237\n(23)\nwhere (i) is obtained by plugging Eq. (19) into the previous equation. Taking the limit for both sides of Eq. (23) as \u2206u \u00d1 0\nor equivalently N \u00d1 8 leads to Eq. (22), which completes the proof.\nRemark 5. According to Theorem 4, consistency models can be trained by minimizing L8\nCD, \u21131p\u03b8, \u03b8; \u03d5q. Moreover, the same\nreasoning in Remark 4 can be applied to show that L8\nCD, \u21131p\u03b8, \u03b8; \u03d5q \u201c 0 if and only if f\u03b8pxt, tq \u201c x\u03f5 for all xt P Rd and\nt P r\u03f5, Ts.\nIn the second case where \u03b8\u00b4 \u201c stopgradp\u03b8q, we can derive a so-called \u201cpseudo-objective\u201d whose gradient matches the\ngradient of LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q in the limit of N \u00d1 8. Minimizing this pseudo-objective with gradient descent gives another\nway to train consistency models via distillation. This pseudo-objective is provided by the theorem below.\n20\nConsistency Models\nTheorem 5. Let tn \u201c \u03c4p n\u00b41\nN\u00b41q, where n P J1, NK, and \u03c4p\u00a8q is a strictly monotonic function with \u03c4p0q \u201c \u03f5 and \u03c4p1q \u201c T.\nAssume \u03c4 is continuously differentiable in r0, 1s, d is three times continuously differentiable with bounded third derivatives,\nand f\u03b8 is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunction \u03bbp\u00a8q is bounded, supx,tPr\u03f5,T s \u2225s\u03d5px, tq\u22252 \u0103 8, and supx,tPr\u03f5,T s \u2225\u2207\u03b8f\u03b8px, tq\u22252 \u0103 8. Suppose we use the Euler\nODE solver, and \u03b8\u00b4 \u201c stopgradp\u03b8q in consistency distillation. Then,\nlim\nN\u00d18pN \u00b4 1q\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c \u2207\u03b8L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q,\n(24)\nwhere\nL8\nCDp\u03b8, \u03b8\u00b4; \u03d5q :\u201c E\n\u201e\n\u03bbptq\np\u03c4 \u00b41q1ptqf\u03b8pxt, tqTHpf\u03b8\u00b4pxt, tqq\n\u02c6Bf\u03b8\u00b4pxt, tq\nBt\n\u00b4 tBf\u03b8\u00b4pxt, tq\nBxt\ns\u03d5pxt, tq\n\u02d9\u0237\n.\n(25)\nHere the expectation above is taken over x \u201e pdata, u \u201e Ur0, 1s, t \u201c \u03c4puq, and xt \u201e Npx, t2Iq.\nProof. We denote \u2206u \u201c\n1\nN\u00b41 and un \u201c n\u00b41\nN\u00b41. First, we leverage Taylor series expansion to obtain\npN \u00b4 1qLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\n1\n\u2206uLN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\n1\n\u2206uEr\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqs\npiq\n\u201c\n1\n2\u2206u\n\u02c6\nEt\u03bbptnqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u00a8 rf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsu ` ErOp|\u2206u|3qs\n\u02d9\n\u201c\n1\n2\u2206uEt\u03bbptnqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsu ` ErOp|\u2206u|2qs\n(26)\nwhere (i) is derived by expanding dp\u00a8, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq to second order and leveraging dpx, xq \u201d 0 and \u2207ydpy, xq|y\u201cx \u201d 0.\nNext, we compute the gradient of Eq. (26) with respect to \u03b8 and simplify the result to obtain\npN \u00b4 1q\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\n1\n\u2206u\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q\n\u201c\n1\n2\u2206u\u2207\u03b8Et\u03bbptnqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsu ` ErOp|\u2206u|2qs\npiq\n\u201c 1\n\u2206uEt\u03bbptnqr\u2207\u03b8f\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqrf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqsu ` ErOp|\u2206u|2qs\npiiq\n\u201c 1\n\u2206uE\n\"\n\u03bbptnqr\u2207\u03b8f\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u201e\ntn`1\nBf\u03b8\u00b4pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\u03c4 1punq\u2206u\n\u00b4 Bf\u03b8\u00b4pxtn`1, tn`1q\nBtn`1\n\u03c4 1punq\u2206u\n\u0237*\n` ErOp|\u2206u|qs\n\u201cE\n\"\n\u03bbptnqr\u2207\u03b8f\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u201e\ntn`1\nBf\u03b8\u00b4pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\u03c4 1punq\n\u00b4 Bf\u03b8\u00b4pxtn`1, tn`1q\nBtn`1\n\u03c4 1punq\n\u0237*\n` ErOp|\u2206u|qs\n\u201c\u2207\u03b8E\n\"\n\u03bbptnqrf\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u201e\ntn`1\nBf\u03b8\u00b4pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\u03c4 1punq\n\u00b4 Bf\u03b8\u00b4pxtn`1, tn`1q\nBtn`1\n\u03c4 1punq\n\u0237*\n` ErOp|\u2206u|qs\n(27)\n\u201c\u2207\u03b8E\n\"\n\u03bbptnq\np\u03c4 \u00b41q1ptnqrf\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n\u201e\ntn`1\nBf\u03b8\u00b4pxtn`1, tn`1q\nBxtn`1\ns\u03d5pxtn`1, tn`1q\n\u00b4 Bf\u03b8\u00b4pxtn`1, tn`1q\nBtn`1\n\u0237*\n` ErOp|\u2206u|qs\n21\nConsistency Models\nHere (i) results from the chain rule, and (ii) follows from Eq. (19) and f\u03b8px, tq \u201d f\u03b8\u00b4px, tq, since \u03b8\u00b4 \u201c stopgradp\u03b8q.\nTaking the limit for both sides of Eq. (28) as \u2206u \u00d1 0 (or N \u00d1 8) yields Eq. (24), which completes the proof.\nRemark 6. When dpx, yq \u201c \u2225x \u00b4 y\u22252\n2, the pseudo-objective L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q can be simplified to\nL8\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c 2E\n\u201e\n\u03bbptq\np\u03c4 \u00b41q1ptqf\u03b8pxt, tqT\n\u02c6Bf\u03b8\u00b4pxt, tq\nBt\n\u00b4 tBf\u03b8\u00b4pxt, tq\nBxt\ns\u03d5pxt, tq\n\u02d9\u0237\n.\n(28)\nRemark 7. The objective L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q defined in Theorem 5 is only meaningful in terms of its gradient\u2014one cannot\nmeasure the progress of training by tracking the value of L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q, but can still apply gradient descent to this objective\nto distill consistency models from pre-trained diffusion models. Because this objective is not a typical loss function, we refer\nto it as the \u201cpseudo-objective\u201d for consistency distillation.\nRemark 8. Following the same reasoning in Remark 4, we can easily derive that L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q\n\u201c\n0 and\n\u2207\u03b8L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c 0 if f\u03b8px, tq matches the ground truth consistency function for the empirical PF ODE that in-\nvolves s\u03d5px, tq. However, the converse does not hold true in general. This distinguishes L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q from L8\nCDp\u03b8, \u03b8; \u03d5q,\nthe latter of which is a true loss function.\nB.2. Consistency Training in Continuous Time\nA remarkable observation is that the pseudo-objective in Theorem 5 can be estimated without any pre-trained diffusion\nmodels, which enables direct consistency training of consistency models. More precisely, we have the following result.\nTheorem 6. Let tn \u201c \u03c4p n\u00b41\nN\u00b41q, where n P J1, NK, and \u03c4p\u00a8q is a strictly monotonic function with \u03c4p0q \u201c \u03f5 and \u03c4p1q \u201c T.\nAssume \u03c4 is continuously differentiable in r0, 1s, d is three times continuously differentiable with bounded third derivatives,\nand f\u03b8 is twice continuously differentiable with bounded first and second derivatives. Assume further that the weighting\nfunction \u03bbp\u00a8q is bounded, Er\u2225\u2207log ptnpxtnq\u22252\n2s \u0103 8, supx,tPr\u03f5,T s \u2225\u2207\u03b8f\u03b8px, tq\u22252 \u0103 8, and \u03d5 represents diffusion model\nparameters that satisfy s\u03d5px, tq \u201d \u2207log ptpxq. Then if \u03b8\u00b4 \u201c stopgradp\u03b8q, we have\nlim\nN\u00d18pN \u00b4 1q\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c lim\nN\u00d18pN \u00b4 1q\u2207\u03b8LN\nCTp\u03b8, \u03b8\u00b4q \u201c \u2207\u03b8L8\nCTp\u03b8, \u03b8\u00b4q,\n(29)\nwhere LN\nCD uses the Euler ODE solver, and\nL8\nCTp\u03b8, \u03b8\u00b4q :\u201c E\n\u201e\n\u03bbptq\np\u03c4 \u00b41q1ptqf\u03b8pxt, tqTHpf\u03b8\u00b4pxt, tqq\n\u02c6Bf\u03b8\u00b4pxt, tq\nBt\n` Bf\u03b8\u00b4pxt, tq\nBxt\n\u00a8 xt \u00b4 x\nt\n\u02d9\u0237\n.\n(30)\nHere the expectation above is taken over x \u201e pdata, u \u201e Ur0, 1s, t \u201c \u03c4puq, and xt \u201e Npx, t2Iq.\nProof. The proof mostly follows that of Theorem 5. First, we leverage Taylor series expansion to obtain\npN \u00b4 1qLN\nCTp\u03b8, \u03b8\u00b4q \u201c\n1\n\u2206uLN\nCTp\u03b8, \u03b8\u00b4q \u201c\n1\n\u2206uEr\u03bbptnqdpf\u03b8px ` tn`1z, tn`1q, f\u03b8\u00b4px ` tnz, tnqqs\npiq\n\u201c\n1\n2\u2206u\n\u02c6\nEt\u03bbptnqrf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u00a8 rf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsu ` ErOp|\u2206u|3qs\n\u02d9\n\u201c\n1\n2\u2206uEt\u03bbptnqrf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u00a8 rf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsu ` ErOp|\u2206u|2qs\n(31)\nwhere z \u201e Np0, Iq, (i) is derived by first expanding dp\u00a8, f\u03b8\u00b4px`tnz, tnqq to second order, and then noting that dpx, xq \u201d 0\nand \u2207ydpy, xq|y\u201cx \u201d 0. Next, we compute the gradient of Eq. (31) with respect to \u03b8 and simplify the result to obtain\npN \u00b4 1q\u2207\u03b8LN\nCTp\u03b8, \u03b8\u00b4q \u201c\n1\n\u2206u\u2207\u03b8LN\nCTp\u03b8, \u03b8\u00b4q\n\u201c\n1\n2\u2206u\u2207\u03b8Et\u03bbptnqrf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u00a8 rf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsu ` ErOp|\u2206u|2qs\n22\nConsistency Models\npiq\n\u201c 1\n\u2206uEt\u03bbptnqr\u2207\u03b8f\u03b8px ` tn`1z, tn`1qsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u00a8 rf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsu ` ErOp|\u2206u|2qs\n(32)\npiiq\n\u201c 1\n\u2206uE\n\"\n\u03bbptnqr\u2207\u03b8f\u03b8px ` tn`1z, tn`1qsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u201e\n\u03c4 1punq\u2206uB1f\u03b8\u00b4px ` tnz, tnqz\n` B2f\u03b8\u00b4px ` tnz, tnq\u03c4 1punq\u2206u\n\u0237*\n` ErOp|\u2206u|qs\n\u201cE\n\"\n\u03bbptnq\u03c4 1punqr\u2207\u03b8f\u03b8px ` tn`1z, tn`1qsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u201e\nB1f\u03b8\u00b4px ` tnz, tnqz\n` B2f\u03b8\u00b4px ` tnz, tnq\n\u0237*\n` ErOp|\u2206u|qs\n\u201c\u2207\u03b8E\n\"\n\u03bbptnq\u03c4 1punqrf\u03b8px ` tn`1z, tn`1qsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u201e\nB1f\u03b8\u00b4px ` tnz, tnqz\n` B2f\u03b8\u00b4px ` tnz, tnq\n\u0237*\n` ErOp|\u2206u|qs\n\u201c\u2207\u03b8E\n\"\n\u03bbptnq\u03c4 1punqrf\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4pxtn, tnqq\n\u201e\nB1f\u03b8\u00b4pxtn, tnqxtn \u00b4 x\ntn\n` B2f\u03b8\u00b4pxtn, tnq\n\u0237*\n` ErOp|\u2206u|qs\n\u201c\u2207\u03b8E\n\"\n\u03bbptnq\np\u03c4 \u00b41q1ptnqrf\u03b8pxtn`1, tn`1qsTHpf\u03b8\u00b4pxtn, tnqq\n\u201e\nB1f\u03b8\u00b4pxtn, tnqxtn \u00b4 x\ntn\n` B2f\u03b8\u00b4pxtn, tnq\n\u0237*\n` ErOp|\u2206u|qs\n(33)\nHere (i) results from the chain rule, and (ii) follows from Taylor expansion. Taking the limit for both sides of Eq. (33) as\n\u2206u \u00d1 0 or N \u00d1 8 yields the second equality in Eq. (29).\nNow we prove the first equality. Applying Taylor expansion again, we obtain\npN \u00b4 1q\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\n1\n\u2206u\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c\n1\n\u2206u\u2207\u03b8Er\u03bbptnqdpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs\n\u201c 1\n\u2206uEr\u03bbptnq\u2207\u03b8dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs\n\u201c 1\n\u2206uEr\u03bbptnq\u2207\u03b8f\u03b8pxtn`1, tn`1qTB1dpf\u03b8pxtn`1, tn`1q, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs\n\u201c 1\n\u2206uE\n\"\n\u03bbptnq\u2207\u03b8f\u03b8pxtn`1, tn`1qT\n\u201e\nB1dpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnq, f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq\n` Hpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqpf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqq ` Op|\u2206u|2q\n\u0237*\n\u201c 1\n\u2206uEt\u03bbptnq\u2207\u03b8f\u03b8pxtn`1, tn`1qTrHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqpf\u03b8pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs ` Op|\u2206u|2qu\n\u201c 1\n\u2206uEt\u03bbptnq\u2207\u03b8f\u03b8pxtn`1, tn`1qTrHpf\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqpf\u03b8\u00b4pxtn`1, tn`1q \u00b4 f\u03b8\u00b4p\u02c6x\u03d5\ntn, tnqqs ` Op|\u2206u|2qu\npiq\n\u201c 1\n\u2206uEt\u03bbptnqr\u2207\u03b8f\u03b8px ` tn`1z, tn`1qsTHpf\u03b8\u00b4px ` tnz, tnqq\n\u00a8 rf\u03b8px ` tn`1z, tn`1q \u00b4 f\u03b8\u00b4px ` tnz, tnqsu ` ErOp|\u2206u|2qs\nwhere (i) holds because xtn`1 \u201c x ` tn`1z and \u02c6x\u03d5\ntn \u201c xtn`1 \u00b4 ptn \u00b4 tn`1qtn`1\n\u00b4pxtn`1\u00b4xq\nt2\nn`1\n\u201c xtn`1 ` ptn \u00b4 tn`1qz \u201c\nx ` tnz. Because (i) matches Eq. (32), we can use the same reasoning procedure from Eq. (32) to Eq. (33) to conclude\nlimN\u00d18pN \u00b4 1q\u2207\u03b8LN\nCDp\u03b8, \u03b8\u00b4; \u03d5q \u201c limN\u00d18pN \u00b4 1q\u2207\u03b8LN\nCTp\u03b8, \u03b8\u00b4q, completing the proof.\nRemark 9. Note that L8\nCTp\u03b8, \u03b8\u00b4q does not depend on the diffusion model parameter \u03d5 and hence can be optimized without\nany pre-trained diffusion models.\n23\nConsistency Models\n(a) Consistency Distillation\n(b) Consistency Training\nFigure 7: Comparing discrete consistency distillation/training algorithms with continuous counterparts.\nRemark 10. When dpx, yq \u201c \u2225x \u00b4 y\u22252\n2, the continuous-time consistency training objective becomes\nL8\nCTp\u03b8, \u03b8\u00b4q \u201c 2E\n\u201e\n\u03bbptq\np\u03c4 \u00b41q1ptqf\u03b8pxt, tqT\n\u02c6Bf\u03b8\u00b4pxt, tq\nBt\n` Bf\u03b8\u00b4pxt, tq\nBxt\n\u00a8 xt \u00b4 x\nt\n\u02d9\u0237\n.\n(34)\nRemark 11. Similar to L8\nCDp\u03b8, \u03b8\u00b4; \u03d5q in Theorem 5, L8\nCTp\u03b8, \u03b8\u00b4q is a pseudo-objective; one cannot track training by\nmonitoring the value of L8\nCTp\u03b8, \u03b8\u00b4q, but can still apply gradient descent on this loss function to train a consistency\nmodel f\u03b8px, tq directly from data. Moreover, the same observation in Remark 8 holds true: L8\nCTp\u03b8, \u03b8\u00b4q \u201c 0 and\n\u2207\u03b8L8\nCTp\u03b8, \u03b8\u00b4q \u201c 0 if f\u03b8px, tq matches the ground truth consistency function for the PF ODE.\nB.3. Experimental Verifications\nTo experimentally verify the efficacy of our continuous-time CD and CT objectives, we train consistency models with a\nvariety of loss functions on CIFAR-10. All results are provided in Fig. 7. We set \u03bbptq \u201c p\u03c4 \u00b41q1ptq for all continuous-time\nexperiments. Other hyperparameters are the same as in Table 3. We occasionally modify some hyperparameters for improved\nperformance. For distillation, we compare the following objectives:\n\u2022 CD p\u21132q: Consistency distillation LN\nCD with N \u201c 18 and the \u21132 metric.\n\u2022 CD p\u21131q: Consistency distillation LN\nCD with N \u201c 18 and the \u21131 metric. We set the learning rate to 2e-4.\n\u2022 CD (LPIPS): Consistency distillation LN\nCD with N \u201c 18 and the LPIPS metric.\n\u2022 CD8 p\u21132q: Consistency distillation L8\nCD in Theorem 3 with the \u21132 metric. We set the learning rate to 1e-3 and dropout\nto 0.13.\n\u2022 CD8 p\u21131q: Consistency distillation L8\nCD in Theorem 4 with the \u21131 metric. We set the learning rate to 1e-3 and dropout\nto 0.3.\n\u2022 CD8 (stopgrad, \u21132): Consistency distillation L8\nCD in Theorem 5 with the \u21132 metric. We set the learning rate to 5e-6.\n\u2022 CD8 (stopgrad, LPIPS): Consistency distillation L8\nCD in Theorem 5 with the LPIPS metric. We set the learning rate to\n5e-6.\nWe did not investigate using the LPIPS metric in Theorem 3 because minimizing the resulting objective would require\nback-propagating through second order derivatives of the VGG network used in LPIPS, which is computationally expensive\nand prone to numerical instability. As revealed by Fig. 7a, the stopgrad version of continuous-time distillation (Theorem 5)\nworks better than the non-stopgrad version (Theorem 3) for both the LPIPS and \u21132 metrics, and the LPIPS metric works\nthe best for all distillation approaches. Additionally, discrete-time consistency distillation outperforms continuous-time\n24\nConsistency Models\nTable 3: Hyperparameters used for training CD and CT models\nHyperparameter\nCIFAR-10\nImageNet 64 \u02c6 64\nLSUN 256 \u02c6 256\nCD\nCT\nCD\nCT\nCD\nCT\nLearning rate\n4e-4\n4e-4\n8e-6\n8e-6\n1e-5\n1e-5\nBatch size\n512\n512\n2048\n2048\n2048\n2048\n\u00b5\n0\n0.95\n0.95\n\u00b50\n0.9\n0.95\n0.95\ns0\n2\n2\n2\ns1\n150\n200\n150\nN\n18\n40\n40\nODE solver\nHeun\nHeun\nHeun\nEMA decay rate\n0.9999\n0.9999\n0.999943\n0.999943\n0.999943\n0.999943\nTraining iterations\n800k\n800k\n600k\n800k\n600k\n1000k\nMixed-Precision (FP16)\nNo\nNo\nYes\nYes\nYes\nYes\nDropout probability\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\nNumber of GPUs\n8\n8\n64\n64\n64\n64\nconsistency distillation, possibly due to the larger variance in continuous-time objectives, and the fact that one can use\neffective higher-order ODE solvers in discrete-time objectives.\nFor consistency training (CT), we find it important to initialize consistency models from a pre-trained EDM model in order\nto stabilize training when using continuous-time objectives. We hypothesize that this is caused by the large variance in our\ncontinuous-time loss functions. For fair comparison, we thus initialize all consistency models from the same pre-trained\nEDM model on CIFAR-10 for both discrete-time and continuous-time CT, even though the former works well with random\ninitialization. We leave variance reduction techniques for continuous-time CT to future research.\nWe empirically compare the following objectives:\n\u2022 CT (LPIPS): Consistency training LN\nCT with N \u201c 120 and the LPIPS metric. We set the learning rate to 4e-4, and the\nEMA decay rate for the target network to 0.99. We do not use the schedule functions for N and \u00b5 here because they\ncause slower learning when the consistency model is initialized from a pre-trained EDM model.\n\u2022 CT8 p\u21132q: Consistency training L8\nCT with the \u21132 metric. We set the learning rate to 5e-6.\n\u2022 CT8 (LPIPS): Consistency training L8\nCT with the LPIPS metric. We set the learning rate to 5e-6.\nAs shown in Fig. 7b, the LPIPS metric leads to improved performance for continuous-time CT. We also find that continuous-\ntime CT outperforms discrete-time CT with the same LPIPS metric. This is likely due to the bias in discrete-time CT, as\n\u2206t \u0105 0 in Theorem 2 for discrete-time objectives, whereas continuous-time CT has no bias since it implicitly drives \u2206t to 0.\nC. Additional Experimental Details\nModel Architectures\nWe follow Song et al. (2021); Dhariwal & Nichol (2021) for model architectures. Specifically, we\nuse the NCSN++ architecture in Song et al. (2021) for all CIFAR-10 experiments, and take the corresponding network\narchitectures from Dhariwal & Nichol (2021) when performing experiments on ImageNet 64 \u02c6 64, LSUN Bedroom\n256 \u02c6 256 and LSUN Cat 256 \u02c6 256.\nParameterization for Consistency Models\nWe use the same architectures for consistency models as those used for\nEDMs. The only difference is we slightly modify the skip connections in EDM to ensure the boundary condition holds for\nconsistency models. Recall that in Section 3 we propose to parameterize a consistency model in the following form:\nf\u03b8px, tq \u201c cskipptqx ` coutptqF\u03b8px, tq.\nIn EDM (Karras et al., 2022), authors choose\ncskipptq \u201c\n\u03c32\ndata\nt2 ` \u03c32\ndata\n,\ncoutptq \u201c\n\u03c3datat\na\n\u03c32\ndata ` t2 ,\n25\nConsistency Models\nwhere \u03c3data \u201c 0.5. However, this choice of cskip and cout does not satisfy the boundary condition when the smallest time\ninstant \u03f5 \u2030 0. To remedy this issue, we modify them to\ncskipptq \u201c\n\u03c32\ndata\npt \u00b4 \u03f5q2 ` \u03c32\ndata\n,\ncoutptq \u201c \u03c3datapt \u00b4 \u03f5q\na\n\u03c32\ndata ` t2 ,\nwhich clearly satisfies cskipp\u03f5q \u201c 1 and coutp\u03f5q \u201c 0.\nSchedule Functions for Consistency Training\nAs discussed in Section 5, consistency generation requires specifying\nschedule functions Np\u00a8q and \u00b5p\u00a8q for best performance. Throughout our experiments, we use schedule functions that take\nthe form below:\nNpkq \u201c\nSc\nk\nK pps1 ` 1q2 \u00b4 s2\n0q ` s2\n0 \u00b4 1\nW\n` 1\n\u00b5pkq \u201c exp\n\u02c6s0 log \u00b50\nNpkq\n\u02d9\n,\nwhere K denotes the total number of training iterations, s0 denotes the initial discretization steps, s1 \u0105 s0 denotes the target\ndiscretization steps at the end of training, and \u00b50 \u0105 0 denotes the EMA decay rate at the beginning of model training.\nTraining Details\nIn both consistency distillation and progressive distillation, we distill EDMs (Karras et al., 2022). We\ntrained these EDMs ourselves according to the specifications given in Karras et al. (2022). The original EDM paper did\nnot provide hyperparameters for the LSUN Bedroom 256 \u02c6 256 and Cat 256 \u02c6 256 datasets, so we mostly used the same\nhyperparameters as those for the ImageNet 64 \u02c6 64 dataset. The difference is that we trained for 600k and 300k iterations\nfor the LSUN Bedroom and Cat datasets respectively, and reduced the batch size from 4096 to 2048.\nWe used the same EMA decay rate for LSUN 256 \u02c6 256 datasets as for the ImageNet 64 \u02c6 64 dataset. For progressive\ndistillation, we used the same training settings as those described in Salimans & Ho (2022) for CIFAR-10 and ImageNet\n64 \u02c6 64. Although the original paper did not test on LSUN 256 \u02c6 256 datasets, we used the same settings for ImageNet\n64 \u02c6 64 and found them to work well.\nIn all distillation experiments, we initialized the consistency model with pre-trained EDM weights. For consistency training,\nwe initialized the model randomly, just as we did for training the EDMs. We trained all consistency models with the\nRectified Adam optimizer (Liu et al., 2019), with no learning rate decay or warm-up, and no weight decay. We also applied\nEMA to the weights of the online consistency models in both consistency distillation and consistency training, as well as\nto the weights of the training online consistency models according to Karras et al. (2022). For LSUN 256 \u02c6 256 datasets,\nwe chose the EMA decay rate to be the same as that for ImageNet 64 \u02c6 64, except for consistency distillation on LSUN\nBedroom 256 \u02c6 256, where we found that using zero EMA worked better.\nWhen using the LPIPS metric on CIFAR-10 and ImageNet 64 \u02c6 64, we rescale images to resolution 224 \u02c6 224 with bilinear\nupsampling before feeding them to the LPIPS network. For LSUN 256 \u02c6 256, we evaluated LPIPS without rescaling inputs.\nIn addition, we performed horizontal flips for data augmentation for all models and on all datasets. We trained all models on\na cluster of Nvidia A100 GPUs. Additional hyperparameters for consistency training and distillation are listed in Table 3.\nD. Additional Results on Zero-Shot Image Editing\nWith consistency models, we can perform a variety of zero-shot image editing tasks. As an example, we present additional\nresults on colorization (Fig. 8), super-resolution (Fig. 9), inpainting (Fig. 10), interpolation (Fig. 11), denoising (Fig. 12),\nand stroke-guided image generation (SDEdit, Meng et al. (2021), Fig. 13). The consistency model used here is trained via\nconsistency distillation on the LSUN Bedroom 256 \u02c6 256.\nAll these image editing tasks, except for image interpolation and denoising, can be performed via a small modification to the\nmultistep sampling algorithm in Algorithm 1. The resulting pseudocode is provided in Algorithm 4. Here y is a reference\nimage that guides sample generation, \u2126is a binary mask, d computes element-wise products, and A is an invertible linear\ntransformation that maps images into a latent space where the conditional information in y is infused into the iterative\n26\nConsistency Models\nAlgorithm 4 Zero-Shot Image Editing\n1: Input: Consistency model f\u03b8p\u00a8, \u00a8q, sequence of time points t1 \u0105 t2 \u0105 \u00a8 \u00a8 \u00a8 \u0105 tN, reference image y, invertible linear\ntransformation A, and binary image mask \u2126\n2: y \u00d0 A\u00b41rpAyq d p1 \u00b4 \u2126q ` 0 d \u2126s\n3: Sample x \u201e Npy, t2\n1Iq\n4: x \u00d0 f\u03b8px, t1q\n5: x \u00d0 A\u00b41rpAyq d p1 \u00b4 \u2126q ` pAxq d \u2126s\n6: for n \u201c 2 to N do\n7:\nSample x \u201e Npx, pt2\nn \u00b4 \u03f52qIq\n8:\nx \u00d0 f\u03b8px, tnq\n9:\nx \u00d0 A\u00b41rpAyq d p1 \u00b4 \u2126q ` pAxq d \u2126s\n10: end for\n11: Output: x\ngeneration procedure by masking with \u2126. Unless otherwise stated, we choose\nti \u201c\n\u02c6\nT 1{\u03c1 ` i \u00b4 1\nN \u00b4 1p\u03f51{\u03c1 \u00b4 T 1{\u03c1q\n\u02d9\u03c1\nin our experiments, where N \u201c 40 for LSUN Bedroom 256 \u02c6 256.\nBelow we describe how to perform each task using Algorithm 4.\nInpainting\nWhen using Algorithm 4 for inpainting, we let y be an image where missing pixels are masked out, \u2126be a\nbinary mask where 1 indicates the missing pixels, and A be the identity transformation.\nColorization\nThe algorithm for image colorization is similar, as colorization becomes a special case of inpainting once we\ntransform data into a decoupled space. Specifically, let y P Rh\u02c6w\u02c63 be a gray-scale image that we aim to colorize, where\nall channels of y are assumed to be the same, i.e., yr:, :, 0s \u201c yr:, :, 1s \u201c yr:, :, 2s in NumPy notation. In our experiments,\neach channel of this gray scale image is obtained from a colorful image by averaging the RGB channels with\n0.2989R ` 0.5870G ` 0.1140B.\nWe define \u2126P t0, 1uh\u02c6w\u02c63 to be a binary mask such that\n\u2126ri, j, ks \u201c\n#\n1,\nk \u201c 1 or 2\n0,\nk \u201c 0\n.\nLet Q P R3\u02c63 be an orthogonal matrix whose first column is proportional to the vector p0.2989, 0.5870, 0.1140q. This\northogonal matrix can be obtained easily via QR decomposition, and we use the following in our experiments\nQ \u201c\n\u00a8\n\u02dd\n0.4471\n\u00b40.8204\n0.3563\n0.8780\n0.4785\n0\n0.1705\n\u00b40.3129\n\u00b40.9343\n\u02db\n\u201a.\nWe then define the linear transformation A : x P Rh\u02c6w\u02c63 \u00de\u00d1 y P Rh\u02c6w\u02c63, where\nyri, j, ks \u201c\n2\u00ff\nl\u201c0\nxri, j, lsQrl, ks.\nBecause Q is orthogonal, the inversion A\u00b41 : y P Rh\u02c6w \u00de\u00d1 x P Rh\u02c6w\u02c63 is easy to compute, where\nxri, j, ks \u201c\n2\u00ff\nl\u201c0\nyri, j, lsQrk, ls.\nWith A and \u2126defined as above, we can now use Algorithm 4 for image colorization.\n27\nConsistency Models\nSuper-resolution\nWith a similar strategy, we employ Algorithm 4 for image super-resolution. For simplicity, we assume\nthat the down-sampled image is obtained by averaging non-overlapping patches of size p \u02c6 p. Suppose the shape of full\nresolution images is h \u02c6 w \u02c6 3. Let y P Rh\u02c6w\u02c63 denote a low-resolution image naively up-sampled to full resolution,\nwhere pixels in each non-overlapping patch share the same value. Additionally, let \u2126P t0, 1uh{p\u02c6w{p\u02c6p2\u02c63 be a binary\nmask such that\n\u2126ri, j, k, ls \u201c\n#\n1,\nk \u011b 1\n0,\nk \u201c 0 .\nSimilar to image colorization, super-resolution requires an orthogonal matrix Q P Rp2\u02c6p2 whose first column is\np1{p, 1{p, \u00a8 \u00a8 \u00a8 , 1{pq. This orthogonal matrix can be obtained with QR decomposition. To perform super-resolution, we\ndefine the linear transformation A : x P Rh\u02c6w\u02c63 \u00de\u00d1 y P Rh{p\u02c6w{p\u02c6p2\u02c63, where\nyri, j, k, ls \u201c\np2\u00b41\n\u00ff\nm\u201c0\nxri \u02c6 p ` pm \u00b4 m mod pq{p, j \u02c6 p ` m mod p, lsQrm, ks.\nThe inverse transformation A\u00b41 : y P Rh{p\u02c6w{p\u02c6p2\u02c63 \u00de\u00d1 x P Rh\u02c6w\u02c63 is easy to derive, with\nxri, j, k, ls \u201c\np2\u00b41\n\u00ff\nm\u201c0\nyri \u02c6 p ` pm \u00b4 m mod pq{p, j \u02c6 p ` m mod p, lsQrk, ms.\nAbove definitions of A and \u2126allow us to use Algorithm 4 for image super-resolution.\nStroke-guided image generation\nWe can also use Algorithm 4 for stroke-guided image generation as introduced in\nSDEdit (Meng et al., 2021). Specifically, we let y P Rh\u02c6w\u02c63 be a stroke painting. We set A \u201c I, and define \u2126P Rh\u02c6w\u02c63\nas a matrix of ones. In our experiments, we set t1 \u201c 5.38 and t2 \u201c 2.24, with N \u201c 2.\nDenoising\nIt is possible to denoise images perturbed with various scales of Gaussian noise using a single consistency\nmodel. Suppose the input image x is perturbed with Np0; \u03c32Iq. As long as \u03c3 P r\u03f5, Ts, we can evaluate f\u03b8px, \u03c3q to produce\nthe denoised image.\nInterpolation\nWe can interpolate between two images generated by consistency models. Suppose the first sample x1 is\nproduced by noise vector z1, and the second sample x2 is produced by noise vector z2. In other words, x1 \u201c f\u03b8pz1, Tq and\nx2 \u201c f\u03b8pz2, Tq. To interpolate between x1 and x2, we first use spherical linear interpolation to get\nz \u201c sinrp1 \u00b4 \u03b1q\u03c8s\nsinp\u03c8q\nz1 ` sinp\u03b1\u03c8q\nsinp\u03c8q z2,\nwhere \u03b1 P r0, 1s and \u03c8 \u201c arccosp\nzT\n1z2\n\u2225z1\u22252\u2225z2\u22252 q, then evaluate f\u03b8pz, Tq to produce the interpolated image.\nE. Additional Samples from Consistency Models\nWe provide additional samples from consistency distillation (CD) and consistency training (CT) on CIFAR-10 (Figs. 14\nand 18), ImageNet 64 \u02c6 64 (Figs. 15 and 19), LSUN Bedroom 256 \u02c6 256 (Figs. 16 and 20) and LSUN Cat 256 \u02c6 256\n(Figs. 17 and 21).\n28\nConsistency Models\nFigure 8: Gray-scale images (left), colorized images by a consistency model (middle), and ground truth (right).\n29\nConsistency Models\nFigure 9: Downsampled images of resolution 32 \u02c6 32 (left), full resolution (256 \u02c6 256) images generated by a consistency\nmodel (middle), and ground truth images of resolution 256 \u02c6 256 (right).\n30\nConsistency Models\nFigure 10: Masked images (left), imputed images by a consistency model (middle), and ground truth (right).\n31\nConsistency Models\nFigure 11: Interpolating between leftmost and rightmost images with spherical linear interpolation. All samples are generated\nby a consistency model trained on LSUN Bedroom 256 \u02c6 256.\n32\nConsistency Models\nFigure 12: Single-step denoising with a consistency model. The leftmost images are ground truth. For every two rows, the\ntop row shows noisy images with different noise levels, while the bottom row gives denoised images.\n33\nConsistency Models\nFigure 13: SDEdit with a consistency model. The leftmost images are stroke painting inputs. Images on the right side are\nthe results of stroke-guided image generation (SDEdit).\n34\nConsistency Models\n(a) EDM (FID=2.04)\n(b) CD with single-step generation (FID=3.55)\n(c) CD with two-step generation (FID=2.93)\nFigure 14: Uncurated samples from CIFAR-10 32 \u02c6 32. All corresponding samples use the same initial noise.\n35\nConsistency Models\n(a) EDM (FID=2.44)\n(b) CD with single-step generation (FID=6.20)\n(c) CD with two-step generation (FID=4.70)\nFigure 15: Uncurated samples from ImageNet 64 \u02c6 64. All corresponding samples use the same initial noise.\n36\nConsistency Models\n(a) EDM (FID=3.57)\n(b) CD with single-step generation (FID=7.80)\n(c) CD with two-step generation (FID=5.22)\nFigure 16: Uncurated samples from LSUN Bedroom 256 \u02c6 256. All corresponding samples use the same initial noise.\n37\nConsistency Models\n(a) EDM (FID=6.69)\n(b) CD with single-step generation (FID=10.99)\n(c) CD with two-step generation (FID=8.84)\nFigure 17: Uncurated samples from LSUN Cat 256 \u02c6 256. All corresponding samples use the same initial noise.\n38\nConsistency Models\n(a) EDM (FID=2.04)\n(b) CT with single-step generation (FID=8.73)\n(c) CT with two-step generation (FID=5.83)\nFigure 18: Uncurated samples from CIFAR-10 32 \u02c6 32. All corresponding samples use the same initial noise.\n39\nConsistency Models\n(a) EDM (FID=2.44)\n(b) CT with single-step generation (FID=12.96)\n(c) CT with two-step generation (FID=11.12)\nFigure 19: Uncurated samples from ImageNet 64 \u02c6 64. All corresponding samples use the same initial noise.\n40\nConsistency Models\n(a) EDM (FID=3.57)\n(b) CT with single-step generation (FID=16.00)\n(c) CT with two-step generation (FID=7.80)\nFigure 20: Uncurated samples from LSUN Bedroom 256 \u02c6 256. All corresponding samples use the same initial noise.\n41\nConsistency Models\n(a) EDM (FID=6.69)\n(b) CT with single-step generation (FID=20.70)\n(c) CT with two-step generation (FID=11.76)\nFigure 21: Uncurated samples from LSUN Cat 256 \u02c6 256. All corresponding samples use the same initial noise.\n42\n"
}