{
    "id": "2409.15816",
    "title": "Diffusion Models for Intelligent Transportation Systems: A Survey",
    "abstract": "Intelligent Transportation Systems (ITS) are vital in modern traffic\nmanagement and optimization, significantly enhancing traffic efficiency and\nsafety. Recently, diffusion models have emerged as transformative tools for\naddressing complex challenges within ITS. In this paper, we present a\ncomprehensive survey of diffusion models for ITS, covering both theoretical and\npractical aspects. First, we introduce the theoretical foundations of diffusion\nmodels and their key variants, including conditional diffusion models and\nlatent diffusion models, highlighting their suitability for modeling complex,\nmulti-modal traffic data and enabling controllable generation. Second, we\noutline the primary challenges in ITS and the corresponding advantages of\ndiffusion models, providing readers with a deeper understanding of the\nintersection between ITS and diffusion models. Third, we offer a\nmulti-perspective investigation of current applications of diffusion models in\nITS domains, including autonomous driving, traffic simulation, trajectory\nprediction, and traffic safety. Finally, we discuss state-of-the-art diffusion\nmodel techniques and highlight key ITS research directions that warrant further\ninvestigation. Through this structured overview, we aim to provide researchers\nwith a comprehensive understanding of diffusion models for ITS, thereby\nadvancing their future applications in the transportation domain.",
    "date": "2024-09-24T07:27:00+00:00",
    "fulltext": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n1\nDiffusion Models for Intelligent Transportation\nSystems: A Survey\nMingxing Peng, Kehua Chen, Xusen Guo, Qiming Zhang, Hongliang Lu, Hui Zhong, Di Chen, Meixin Zhu*, and\nHai Yang\nAbstract\u2014Intelligent Transportation Systems (ITS) are vital\nin modern traffic management and optimization, significantly\nenhancing traffic efficiency and safety. Recently, diffusion models\nhave emerged as transformative tools for addressing complex\nchallenges within ITS. In this paper, we present a comprehensive\nsurvey of diffusion models for ITS, covering both theoretical and\npractical aspects. First, we introduce the theoretical foundations\nof diffusion models and their key variants, including conditional\ndiffusion models and latent diffusion models, highlighting their\nsuitability for modeling complex, multi-modal traffic data and\nenabling controllable generation. Second, we outline the primary\nchallenges in ITS and the corresponding advantages of diffusion\nmodels, providing readers with a deeper understanding of the\nintersection between ITS and diffusion models. Third, we offer\na multi-perspective investigation of current applications of diffu-\nsion models in ITS domains, including autonomous driving, traf-\nfic simulation, trajectory prediction, and traffic safety. Finally, we\ndiscuss state-of-the-art diffusion model techniques and highlight\nkey ITS research directions that warrant further investigation.\nThrough this structured overview, we aim to provide researchers\nwith a comprehensive understanding of diffusion models for ITS,\nthereby advancing their future applications in the transportation\ndomain.\nIndex Terms\u2014Intelligent Transportation Systems, Diffusion\nModels, Autonomous Driving, Traffic Simulation, Traffic Fore-\ncasting, Traffic Safety.\nI. INTRODUCTION\nA\nS urbanization accelerates and populations grow, the de-\nmand for public transportation services increases along-\nside a steep rise in vehicle numbers. These trends have\ngradually revealed several issues in current transportation sys-\ntems, such as traffic congestion and accidents. With advance-\nments in computer technologies and transportation systems,\nmany cities are increasingly focused on developing intelligent\ntransportation systems (ITS) [1], which leverage cutting-edge\ntechnologies and extensive traffic data to enable efficient, high-\nquality, and safe traffic management. ITS encompasses sev-\nManuscript received XX September, 2024.\nCorresponding author is Meixin Zhu (E-mail: meixin@ust.hk).\nMingxing Peng, Xusen Guo, Qiming Zhang, Hongliang Lu, and Hui Zhong\nare with the Systems Hub, The Hong Kong University of Science and\nTechnology (Guangzhou). Meixin Zhu is with the Systems Hub, The Hong\nKong University of Science and Technology (Guangzhou) and Guangdong\nProvincial Key Lab of Integrated Communication, Sensing and Computation\nfor Ubiquitous Internet of Thing. Kehua Chen is with the Division of Emerging\nInterdisciplinary Areas (EMIA), Academy of Interdisciplinary Studies, The\nHong Kong University of Science and Technology, Hong Kong, China. Di\nChen is with the Department of Electrical and Electronic Engineering of The\nHong Kong Polytechnic University, Hong Kong, China. Hai Yang is with\nthe Department of Civil and Environmental Engineering, the Hong Kong\nUniversity of Science and Technology, Clear Water Bay, Kowloon, Hong\nKong, China.\neral domains, including autonomous driving, which enhances\ntraffic safety and efficiency; traffic simulation, which enables\nmodeling, analysis, and testing of various strategies; traffic\nforecasting, which aims to reduce congestion and optimize\nservices; and traffic safety, which seeks to minimize accidents\nand improve overall safety.\nTraffic data are inherently heterogeneous and multi-modal,\nincluding vehicle and pedestrian trajectories, driving images or\nvideos, spatial-temporal graphs derived from GPS positions,\nand textual data such as traffic rules and accident reports.\nThese data often exhibit complex spatial-temporal dependen-\ncies and uncertainties. Additionally, the data may be noisy,\nincomplete, or difficult to obtain, with privacy concerns par-\nticularly affecting personal GPS data collection. Consequently,\nprocessing these multi-modal, complex, and often imperfect\ndatasets presents a significant challenge for ITS.\nIn the past few decades, researchers have employed various\napproaches to address the challenges of ITS. For example,\nRecurrent Neural Networks (RNNs) are often used to model\ntemporal relationships, while Convolutional neural networks\n(CNNs) are commonly utilized to capture spatial structure\n[2]. And graph-based approaches have demonstrated superior\ncapabilities in extracting spatial correlations within traffic\nnetworks [3], [4]. However, these approaches often exhibit lim-\nitations when handling noisy or incomplete data. In contrast,\ngenerative models such as Generative Adversarial Networks\n(GANs) and Variational Autoencoders (VAEs) have proven\neffective for traffic data generation and imputation tasks [5],\n[6]. However, GANs suffer from unstable training, and VAE\nhas the limitation of low-quality output. Diffusion models,\nas a powerful class of generative models, offer advantages\nsuch as ease of training, enhanced generative performance,\ncontrollable generation, and multi-modal capabilities. To date,\ndiffusion models have been applied across a wide range of\nvision tasks [7], with promising applications such as Sora\n[8]. Inspired by these developments, an increasing number of\nresearchers in the ITS domain have begun to adopt diffusion\nmodels to address various challenges in ITS. Therefore, orig-\ninating in image processing and computer vision, diffusion\nmodels are now being applied across various traffic tasks, from\nautonomous driving and traffic simulation to traffic forecasting\nand traffic safety. As illustrated in Fig. 1, diffusion models are\nsuitable for processing various traffic data, and are capable\nof addressing a wide range of various traffic tasks based on\ntask-specific conditions or unconditional methods.\nThere have been numerous surveys on ITS [2], [9], as\nwell as specific technologies within the ITS domain [3], [4],\narXiv:2409.15816v1  [eess.SY]  24 Sep 2024\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n2\nTraffic Data\nTraffic Tasks\n\u2026\nVarious conditions or unconditional\n\u2026\nDiffusion Model\n\u2026\nForward Process   \ud835\udc2a(\ud835\udc99\ud835\udc95|\ud835\udc99\ud835\udc95\u2212\ud835\udfcf)\nAdd Noise\n\ud835\udc650\n\ud835\udc652\n\ud835\udc651\n\ud835\udc65\ud835\udc47\nReverse Process   \ud835\udc91\ud835\udf3d(\ud835\udc99\ud835\udc95\u2212\ud835\udfcf|\ud835\udc99\ud835\udc95)\nTrajectories\nTraffic Images \nSpatial-Temporal Graphs\nTraffic Texts\nAutonomous Driving\nTraffic Simulation \nTraffic Forecasting\nTraffic Safety\nDomain Knowledge\nTraffic Rules\nAccident Documents\n\u2026\nDenoise\nFig. 1: Overview of applying diffusion models to traffic tasks using various traffic data types, including trajectories, traffic\nimages, spatial-temporal graphs, and traffic-related texts.\n[10]. Similarly, several reviews have focused on diffusion\nmodels [11], [12], [13] and their applications in areas such\nas computer vision [7] and medical imaging [14]. However,\nthere is currently no comprehensive review of diffusion models\nwithin the ITS domain.\nTo address this gap, this paper presents a detailed literature\nreview on diffusion models in ITS. First, we outline how diffu-\nsion models have emerged as powerful tools for various traffic\ntasks. Specifically, we introduce the theoretical foundations\nof diffusion models, along with conditional diffusion models\nand latent diffusion models, which extend their applicability\nto more specific tasks within ITS. Second, we examine the\nkey challenges in ITS and the corresponding advantages of\ndiffusion models. Third, we investigate the applications of\ndiffusion models in areas such as autonomous driving, traffic\nsimulation, traffic forecasting, and traffic safety within ITS, as\nshown in Fig. 6. In particular, we review these applications\nbased on criteria such as task, denoising condition, or model\narchitecture, as illustrated in Table. I. Finally, we provide an\noutlook on potential future directions for diffusion models\nin ITS. Our goal is to bridge the gap between the diffusion\nmodel and transportation research communities, fostering in-\nterdisciplinary collaboration and advancing the application of\ndiffusion models in transportation.\nIn summary, the main contributions of this paper include:\n\u2022 To the best of our knowledge, this is the first compre-\nhensive literature review focused on the application of\ndiffusion models in ITS.\n\u2022 We systematically introduce how diffusion models have\nbecome powerful approaches for various traffic tasks by\nprocessing multi-modal and complex traffic data. Addi-\ntionally, we explore the key challenges in ITS and the\ncorresponding advantages of diffusion models. This anal-\nysis offers readers deeper insights into the intersection of\nITS and diffusion models.\n\u2022 We present a comprehensive and up-to-date literature\nreview of diffusion models in the ITS domain, focusing\non applications in autonomous driving, traffic simulation,\ntraffic forecasting, and traffic safety. By analyzing these\napplications through multiple perspectives, we aim to\noffer researchers from various ITS subfields a clear and\nefficient overview of the latest advancements in diffusion\nmodels.\n\u2022 We discuss the cutting-edge techniques in diffusion mod-\nels, and highlight key research directions for diffusion\nmodels in ITS that are worthy of further exploration.\nThe remainder of the paper is organized as follows: Sec. II\npresents theoretical foundations of diffusion models and their\nkey variants. Sec. III outline the key challenges in ITS and\nthe corresponding advantages of diffusion models. Sec. IV-\nSec. VII explores the diverse applications of diffusion models\nwithin ITS, including autonomous driving, traffic simulation,\ntraffic forecasting, and traffic safety. Sec. VIII discusses\nseveral promising directions for future research. Finally, the\nconclusions are drawn in Sec. IX.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n3\nII. THEORY\nDiffusion models have emerged as transformative tools in\nthe field of ITS. This section outlines how diffusion models\nhave become powerful and flexible methods for addressing\nvarious traffic-related challenges. First, we explore the the-\noretical foundations of diffusion models, which lies in their\nability to learn the underlying data distribution through a\nprocess of noise injection and subsequent denoising. This\nmakes them highly effective for modeling complex traffic\ndynamics. Next, we introduce key variants of diffusion models,\nparticularly conditional diffusion models and latent diffusion\nmodels, which extend their applicability to more specific\nand challenging tasks within ITS. By incorporating domain-\nspecific conditions and leveraging latent spaces, diffusion\nmodels can be applied to multi-modal traffic data, offering\nsolutions to a wide range of traffic-related tasks.\nA. Foundations of Diffusion Models\nDiffusion models are a powerful class of probabilistic gen-\nerative models that gradually perturb data by adding Guassian\nnoise to data, and then learn to reverse this process to generate\nnew data. During training, the model learns to denoise the\ndata at each step, effectively transforming random noise into\ncoherent and realistic outputs.\nThis section provides an overview of three predominant\nformulations in diffusion models: Denoising Diffusion Prob-\nabilistic Models (DDPMs), which utilize discrete steps to\nincrementally add and remove noise; Noise Conditioned Score\nNetworks (NCSNs), which estimate the gradient of the log-\ndensity of the data distribution to guide sample generation;\nand Stochastic Differential Equations (SDEs), which offer\na continuous-time perspective that unifies and generalizes\nboth DDPMs and NCSNs under a common mathematical\nframework.\n1) Denoising Diffusion Probabilistic Models (DDPMs):\nDDPMs [15], [16] utilize two Markov chains: a forward\n(diffusion) process that gradually adds Gaussian noise to\ndata, transforming it into pure noise over multiple steps,\nand a reverse (denoising) process, learned through neural\nnetworks\u2014typically based on a U-Net architecture [17]\u2014that\nprogressively removes the noise to reconstruct the original\ndata.\nForward (Diffusion) Process. The forward (diffusion) pro-\ncess incrementally corrupts the data by adding Gaussian noise\nin a series of T steps. Given a data distribution x0 \u223cq(x0), the\nforward process starts with the original data x0 and generates a\nsequence of latent variables x1, x2, . . . , xT through different\ndiffusion steps. The process is defined by a Markov chain\nwhere each state xt depends only on the previous state xt\u22121:\nq(xt|xt\u22121) = N(xt;\np\n1 \u2212\u03b2txt\u22121, \u03b2tI), \u2200t \u2208{1, . . . , T}\n(1)\nwhere \u03b2t \u2208(0, 1) is a hyperparameter representing the noise\nvariance schedule that controls the amount of noise added\nat each step. I denotes the identity matrix, and N(x; \u00b5, \u03c3)\nrepresents a normal distribution with mean \u00b5 and covariance\n\u03c3.\nThe entire forward process can be expressed directly in\nterms of the original data x0 using the reparameterization trick:\nxt = \u221a\u00af\u03b1tx0 +\n\u221a\n1 \u2212\u00af\u03b1t\u03f5,\n\u03f5 \u223cN(0, I)\n(2)\nwhere \u03b1t = 1 \u2212\u03b2t and \u00af\u03b1t = Qt\ns=1 \u03b1s.\nReverse (Denoising) Process. The goal of DDPMs is to\nlearn the reverse of this diffusion process, where the model\nstarts with Gaussian noise and progressively removes the noise\nto generate new data. The reverse process is also modeled as\na Markov chain, but it is parameterized by a neural network\np\u03b8(xt\u22121|xt) that generates p\u03b8(x0) in a step-by-step manner:\np\u03b8(xt\u22121|xt) = N(xt\u22121; \u00b5\u03b8(xt, t), \u03c32I)\n(3)\nIn the DDPM [16], the covariance \u03c32 is fixed to a constant\nvalue, and the mean \u00b5\u03b8(xt, t) is reformulated as:\n\u00b5\u03b8(xt, t) =\n1\n\u221a\u03b1t\n\u0012\nxt \u2212\n\u03b2t\n\u221a1 \u2212\u00af\u03b1t\n\u03f5\u03b8(xt, t)\n\u0013\n(4)\nwhere \u03f5\u03b8(xt, t) represents the neural network\u2019s prediction\nof the noise component at step t.\nThe objective of training a DDPM is to minimize the\nvariational bound on the negative log-likelihood, which can be\nsimplified to a mean squared error loss between the predicted\nnoise and the actual noise [16]:\nL(\u03b8) = Et,x0,\u03f5\n\u0002\n\u2225\u03f5 \u2212\u03f5\u03b8(xt, t)\u22252\u0003\n(5)\nwhere \u03f5 \u223cN(0, I) is the Gaussian noise, and xt is the noisy\ndata generated during the forward process.\n2) Noise Conditioned Score Networks (NCSNs):\nNCSNs [18] are a class of score-based generative models\nthat focus on estimating the score function of the data dis-\ntribution. Instead of explicitly modeling the reverse diffusion\nprocess, NCSNs learn the gradient of the log-density of the\ndata distribution at various noise levels via score matching\n[19], and subsequently generate samples via Langevin dynam-\nics [20], [21].\nScore Matching. Given an unknown data distribution\npdata(x), the score function of the data density p(x) is defined\nas \u2207x log p(x). The score network s\u03b8, a neural network\nparameterized by \u03b8, is trained to estimate the score function\n\u2207x log p(x). When the data distribution is unknown, score\nestimation can be performed using sliced score matching [22]\nor denoising score matching [23]. In NCSNs [18], denoising\nscore matching is adopted, wherein data are perturbed with\nmultiple levels of Gaussian noise. Specifically, the noise\ndistribution is pre-specified as q\u03c3(\u02dcx|x) = N(\u02dcx|x, \u03c32I), and\nthe gradient of the log-likelihood with respect to the noisy\ndata is given by \u2207\u02dcx log q\u03c3(\u02dcx|x) = \u2212(\u02dcx \u2212x)/\u03c32. Given a\nsequence noise scales \u03c31 < \u03c32 < ... < \u03c3L, the denoising\nscore matching objective for all \u03c3 \u2208{\u03c3i}L\ni=1 is defined as:\nL = 1\nL\nL\nX\ni=1\n\u03bb(\u03c3i)Ep(x)E\u02dcx\u223cq\u03c3i(\u02dcx|x)\n\"\r\r\r\rs\u03b8(\u02dcx, \u03c3i) + \u02dcx \u2212x\n\u03c32\ni\n\r\r\r\r\n2\n2\n#\n(6)\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n4\nwhere \u02dcx is a noised version of x, and \u03bb(\u03c3i) is a weighting\nfunction depending on \u03c3i.\nLangevin Dynamics. To generate samples, NCSNs employ\nannealed Langevin dynamics, starting with large noise levels\nand gradually annealing down to lower noise levels. At each\nnoise level, Langevin dynamics is iteratively applied using the\nlearned score function to recover the original data distribution\nprogressively. The update rule for Langevin dynamics is given\nby:\n\u02dcxt = \u02dcxt\u22121 + \u03b1i\n2 s\u03b8(\u02dcxt\u22121, \u03c3i) + \u221a\u03b1iN(0, I)\n(7)\nwhere \u03b1i = \u03f5 \u00b7 \u03c32\ni /\u03c32\nL, and t \u2208[1, T]. When \u03b1i \u21920 and\nT \u2192\u221e, the final generated sample converges to the original\ndata distribution pdata(x).\n3) Stochastic Differential Equations (SDEs):\nSDEs [24] provide a continuous-time framework that unifies\nthe concepts of DDPMs and NCSNs. Specifically, both the for-\nward and reverse processes in these models are formulated as\nsolutions to stochastic differential equations, with the reverse\nprocess requiring the estimation of score functions for noisy\ndata distributions.\nForward Process. In the SDEs [24], the forward process\ncan be represented as the solution to an It\u02c6o SDE [25]:\ndx = f(x, t)dt + g(t)dw\n(8)\nwhere f(\u00b7, t) denotes the drift coefficient of x(t), g(\u00b7) repre-\nsents the diffusion coefficient of x(t), and w is a Brownian\nmotion.\nThe forward processes in DDPMs and NCSNs can be\nregarded as discretizations of two different SDEs [24]. For\nDDPMs, the corresponding SDE is:\ndx = \u22121\n2\u03b2(t)xdt +\np\n\u03b2tdw\n(9)\nwhereas for NCSNs, the corresponding SDE is expressed\nas:\ndx =\nr\nd[\u03c32(t)]\ndt\ndw\n(10)\nReverse Process. To generate samples, starting from sam-\nples of the standard Gaussian distribution x(T) and reversing\nthe process, the reverse-time SDE is solved [26]:\ndx = [f(x, t) \u2212g(t)2\u2207x log pt(x)]dt + g(t)d\u00afw\n(11)\nwhere \u00afw is a Brownian motion with time flows backwards\nfrom T to 0, and dt is an infinitesimal negative timestep.\nSimilar\nto\nNCSNs,\nto\nestimate\nthe\nscore\nfunction\n\u2207x log pt(x), we train a time-dependent score model s\u03b8(xt, t)\nby generalizing the score matching objective to continuous\ntime. The objective function is given by:\nL=Et\nn\n\u03bb(t)Ex(0)Ex(t)|x(0)\nh\r\rs\u03b8(x(t), t)\u2212\u2207x(t)log p(x(t)|x(0))\n\r\r2\n2\nio\n(12)\nwhere t is uniformly sampled over the interval [0, T], and \u03bb(t)\nis a positive weighting function.\nB. Variants of Diffusion Models\nIn this section, we introduce key variants of diffusion\nmodels, including conditional diffusion models and latent\ndiffusion models (LDMs), which have significantly advanced\nthe field of intelligent transportation systems. These mod-\nels not only enhance the ability to generate realistic traffic\ndata but also offer flexibility and controllability in model-\ning complex traffic environments. By incorporating domain-\nspecific information, such as historical data, traffic layouts,\nor external semantic features, conditional diffusion models\nenable the generation of more accurate and diverse traffic\nscenarios that reflect real-world conditions. Meanwhile, LDMs\noperate in a lower-dimensional latent space, facilitating faster\ntraining and inference times while maintaining the fidelity\nof generated outputs. Additionally, LDMs allows multi-modal\nconditions within the latent space. These capabilities make\nLDMs particularly useful for image-based, video-based, or\ntext-involved traffic tasks. Collectively, these advanced models\ndemonstrate the potential of diffusion models to revolutionize\nintelligent transportation systems, providing powerful tools for\ntraffic simulating, forecasting, and optimization in increasingly\ndynamic urban environments.\n1) Conditional Diffusion Models:\nThe three types of standard diffusion models introduced\nabove are unconditional, where the inputs are limited to\nthe perturbed data xt and the diffusion step t. Conditional\ndiffusion models, on the other hand, incorporate conditional\ninformation as an extra input, allowing for control over the\ngeneration process according to specific requirements. This ca-\npability makes them highly adaptable for various applications\nin intelligent transportation systems. Below, we focus on four\nprimary conditioning mechanisms: concatenation-based, cross-\nattention-based, classifier-based, and classifier-free-based ap-\nproaches. Concatenation-based methods are simple to im-\nplement but may struggle to capture complex relationships\nbetween the data and conditions. Cross-attention-based meth-\nods excel at modeling long-range dependencies and complex\ninteractions, and enable multi-modal conditioning, but they do\nnot offer control over the strength of the conditions. Classifier-\nbased approaches provide adjustable guidance through external\nclassifiers but can be limited by the accuracy and generaliza-\ntion capability of the classifier. Classifier-free-based methods\nare flexible and do not require additional classifiers, but they\noften come with increased training costs. The visualization of\nthese four conditioning mechanisms is shown in Fig. 2.\nConcatenation-based. In concatenation-based mechanisms,\nthe conditioning information is directly concatenated with the\nperturbed data xt or the diffusion step t, and then fed into\nthe model for sample generation. This method is simple and\neffective, allowing the model to leverage the conditioning\ninformation throughout the denoising process. For example,\nin the field of intelligent transportation systems, conditioning\non historical data [27], [28], [29] or map feature [30] has been\nemployed for generating traffic trajectories. Similarly, image\nfeatures [31], [32] or traffic layout [33] have been directly\nconcatenated with the noise data vector for generating traffic\nscenarios. Additionally, conditioning on trip regions [34], road\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n5\n\ud835\udc65\ud835\udc61\n\ud835\udc61\nConditions\nHistorical info\nMap\n\u2026\nc\n\ud835\udc65\ud835\udc61\u22121\nDenoising Network\n1) Concatenation-based Condition Mechanism.\n\ud835\udc65\ud835\udc61\n\ud835\udc61\nConditions\nText\nExternal features\n\u2026\n\ud835\udc65\ud835\udc61\u22121\nDenoising Network\n2) Cross-attention-based Condition Mechanism.\n\ud835\udc44\n\ud835\udc3e\ud835\udc49\n\ud835\udc44\n\ud835\udc3e\ud835\udc49\n\ud835\udc44\n\ud835\udc3e\ud835\udc49\n\ud835\udc44\n\ud835\udc3e\ud835\udc49\n\ud835\udc65\ud835\udc61\u22121\n3) Classifier-based Condition Mechanism.\nClassifier\nReinforcement learning\nLanguage guidance\n\u2026\n\u2207\n\ud835\udc65\ud835\udc61\n\ud835\udc61\nDenoising Network\nCost function\n\ud835\udc65\ud835\udc61\u22121\n\ud835\udc65\ud835\udc61\n\ud835\udc61\nDenoising Network\nDenoising Network\n\u2205\n\ud835\udc65\ud835\udc61\n\ud835\udc61\nConditions\n\ud835\udc64\n1 \u2212\ud835\udc64\n4) Classifier-free-based Condition Mechanism.\nShared Parameters\nFig. 2: Different condition mechanisms for diffusion models. (1) Concatenation-based mechanism incorporates conditions such\nas historical data and maps directly into the input. (2) Cross-attention-based mechanism integrates conditions like text and\nexternal features through cross-attention layers. (3) Classifier-based mechanism uses an external classifier to guide denoising\nbased on conditions such as reinforcement learning or cost functions. (4) Classifier-free mechanism combines conditional and\nunconditional denoising models, balancing both with a weight parameter.\nnetwork [35], or graph structure [36] has been applied in traffic\nflow generation. These examples emphasize the effectiveness\nof concatenation-based mechanisms in diverse transportation\napplications.\nCross-attention-based. Cross-attention-based conditional\ndiffusion models integrate the cross-attention layers [37] into\nthe denoising networks, enabling effective fusion of condition-\ning information during the denoising process and guiding the\nnetwork to generate outputs aligned with the conditions. The\ncross-attention mechanism plays an important role in facilitat-\ning the interaction between the conditioning information and\nthe noisy data, especially in scenarios where their relationship\nis complex or involves different modalities, such as text and\nimages. Stable Diffusion [38] introduced a general-purpose\nconditioning mechanism based on cross-attention, enabling\nmulti-modal conditional inputs, making diffusion models into\npowerful and flexible generators. Building on this foundational\nwork, numerous studies have applied this cross-attention-based\nconditioning mechanism in the field of intelligent transporta-\ntion systems. For example, conditioning on text [39], [40],\n[32], [33], [41], [42], drive actions [30], external features\nand semantic features [43], origin-destination-departure time\n(ODT) feature [44], or bounding boxes [42] has been used for\nvarious traffic-related tasks.\nClassifier-based. The classifier-based mechanism incorpo-\nrates conditions by using a task-related classifier to guide the\ndiffusion sampling process, enabling controllable generation.\nDhariwal and Nicho [45] proposed a classifier-guidance ap-\nproach, where an additional classifier p\u03d5(y|xt, t) is trained\non noisy data xt and the diffusion step t. The gradients of\nthe guidance \u2207xt log p\u03d5(y|xt, t) are then used to guide the\ndiffusion sampling process towards a specified class label\ny. Given a pre-trained diffusion model p\u03b8(xt, t) and a pre-\ntrained classifier p\u03d5(y|xt, t), the diffusion sampling process is\nas follows:\nxt\u22121 = N(\u00b5\u03b8(xt, t) + w\u2207xt log p\u03d5(y|xt, t), \u03c32I)\n(13)\nwhere w is a hyperparameter controlling the strength of the\nguidance; as w increases, the generated samples more closely\nadhere to the specified conditions.\nFollowing this work, many studies related to traffic trajec-\ntory generation and motion planning have designed various\nclassifiers to controllably generate traffic scenarios that comply\nwith traffic rules and ensure trajectory smoothness. For ex-\nample, the cumulative rewards learned through reinforcement\nlearning [46], motion planning cost function [47], STL formu-\nlas based on traffic rules [48], language-based loss function\n[49], and driving behavior classes [50] have been designed as\nclassifier to generate task-conditioned samples.\nClassifier-free-based. The classifier-free mechanism com-\nbines unconditional and conditional diffusion models, achiev-\ning a balance between fidelity and diversity without the need\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n6\n\ud835\udc65\n\u04a7\ud835\udc65\nLatent Space\nForward Process \n\ud835\udf00\n\ud835\udc9f\n\u2026\nReverse Process \n\ud835\udc4b0\nAdd Noise \nDenoise \n\ud835\udc4b\ud835\udc47\n\ud835\udc4b\ud835\udc47\n\ud835\udc4b0\n\ud835\udc4b1\n\ud835\udc4b\ud835\udc47\u22121\nFig. 3: Illustration of latent diffusion models. Compared\nto standard diffusion models, they incorporate a pre-trained\nencoder E and decoder D, with the diffusion and denoising\nprocesses operating in latent space rather than pixel or data\nspace.\nto train a separate classifier. Additionally, it should be noted\nthat the conditional diffusion model can employ either a\nconcatenation mechanism or a cross-attention mechanism. In\nclassifier-free diffusion guidance [51], the authors jointly train\na conditional and an unconditional diffusion model, setting the\ncondition c to \u2205for the unconditional model. Then, a weighted\naverage of the conditional and unconditional scores is used to\nestimate the score function:\n\u02dc\u03f5t = w \u03f5\u03b8(xt, t, c) + (1 \u2212w) \u03f5\u03b8(xt, t, \u2205)\n(14)\nwhere w is also a guidance scale.\nFor many traffic-related generation tasks, researchers have\nemployed the classifier-free guidance mechanism to regulate\nthe diversity of the generated outputs [52], [50], [53], [54],\n[55], [34]. This approach prevents the outputs from following\nthe conditional guidance too closely or being constrained too\ntightly.\n2) Latent Diffusion Models:\nThe latent diffusion models (LDMs) [38] incorporate pre-\ntrained perceptual compression models, VQGAN [56], which\nconsist of an encoder E and a decoder D, as illustrated\nin the Fig. 3. This approach enables diffusion models to\nleverage a lower-dimensional latent space, thereby reducing\nthe computational burden during training and speeding up\ninference while maintaining high fidelity in generated outputs.\nFollowing this work, Blattmann et al. [57] extended LDM\nto the video latent diffusion model (VLDM) by introducing\ntemporal layers and finetuning the autoencoder of pre-trained\nLDM using video data.\nLDMs have gained attention in intelligent transportation\nsystems due to their ability to model complex traffic patterns\nand generate realistic traffic scenarios. This approach has\nproven particularly useful in simulating traffic flows [41], pre-\ndicting vehicle trajectories [28], [58], [59], [30], and enhancing\nautonomous driving systems through the generation of diverse\nand realistic traffic scenario data [60], [61], [62], [32], [63],\n[64].\nChallenges in ITS\nAbsence of Quality \nData\nPrivacy Issues\nLack of Rare Events\nDifficult to Model Complex \nTraffic Dynamics\nWeak Scalability and \nGeneralization\nLack of User-friendly \nInteraction\nFig. 4: The challenges in intelligent transportation systems.\nIII. CHALLENGES AND TECHNIQUES\nThis section discusses key challenges in ITS and high-\nlights why diffusion models, as a state-of-the-art generative\napproach, offer innovative solutions to these challenges. The\ncomplexity of traffic systems, combined with the inherent\nuncertainty and variability in traffic data, presents signifi-\ncant challenges for the development of robust models. These\nchallenges are further compounded by issues such as poor\ndata quality, privacy concerns, and the need for scalable\nsolutions that generalize effectively across different regions\nand traffic conditions. While various techniques have been\ndeveloped to address these challenges, diffusion models have\nemerged as a promising approach due to their advantages:\nhigh-fidelity generation, controllable generation, strong flex-\nibility, probabilistic modeling, and multi-modal capabilities.\nThese strengths enhance the accuracy and robustness of ITS\nmodels, improving their applicability across diverse scenarios\nwithin the ITS field. As illustrated in Fig. 4 and Fig. 5, the\nkey challenges in ITS and the corresponding advantages of\ndiffusion models are highlighted.\nA. Challenges in Intelligent Transportation Systems\nITS is a sophisticated system that integrates advanced tech-\nnologies and data analytics into transportation infrastructure\nand management to enhance the efficiency and safety of\ntransportation networks [2], [9]. ITS encompasses a broad\nrange of applications, including traffic prediction, autonomous\ndriving, traffic simulation, and so on, all aimed at improving\ntransportation services by using large-scale traffic data and\nautomated systems. However, several challenges affect the\neffectiveness and implementation of ITS:\n\u2022 Absence of Quality Data. High-quality data are crucial\nfor training reliable models, particularly in supervised\nlearning approaches. However, real-world traffic data\ncollected from traffic sensors, vehicle sensors, or GPS\ndevices are often noisy, incomplete, or insufficient, lim-\niting the ability to accurately predict and simulate traffic\nconditions.\n\u2022 Privacy Issues. The collection of real-world traffic data\nfrom various sources, such as vehicle sensors, GPS de-\nvices, and surveillance cameras, raises significant privacy\nconcerns. In particular, obtaining GPS data for traffic\nflow-related tasks is often challenging due to the need\nto protect personal and location information.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n7\nAdvantages \nof DMs\nHigh-fidelity \nGeneration\nControllable \nGeneration\nMulti-modal \nCapabilities\nStrong \nFlexibility\nProbabilistic \nModeling\nFig. 5: The advantages of diffusion models.\n\u2022 Lack of Rare Events. Rare but critical events, such as\naccidents, sudden weather changes, or unexpected road\nblockages, are difficult to model due to their infrequency.\nThis scarcity of data on such events makes it challeng-\ning to develop systems that can effectively handle and\nrespond to these situations.\n\u2022 Difficult to Model Complex Traffic Dynamics. Traffic\nsystems are inherently complex, involving spatial and\ntemporal dynamics at various scales, along with various\nexternal factors such as holidays, weathers conditions,\nand local events. Accurately modeling these dynamics\nand capturing the intricate relationships between different\nelements in the transportation network remains a chal-\nlenge.\n\u2022 Weak Scalability and Generalization. Many ITS so-\nlutions struggle to scale effectively or generalize across\ndifferent regions and traffic conditions. Solutions that\nwork well in one location may not perform as effectively\nin another due to variations in traffic patterns, and other\nlocal factors.\n\u2022 Lack of User-friendly Interaction. Many current ITS\ninterfaces and tools are difficult for users to navigate\nand use effectively. Improving user-friendly interaction is\nessential to ensure that users can easily understand and\nutilize the benefits of ITS technologies.\nB. Advantages of Diffusion Models\nIn ITS, various deep learning methods have been employed\nto address key challenges in various traffic tasks. For ex-\nample, RNNs [65], [66] have proven effective in modeling\ntemporal relationships in traffic data, and Transformers [37]\nare widely employed for multi-timestep traffic forecasting.\nAdditionally, graph-based techniques such as Graph Neural\nNetworks (GNNs) [67] and Graph Convolutional Networks\n(GCNs) [68] have emerged as powerful tools for modeling\ntraffic as graph structures, effectively capturing spatial inter-\nactions in transportation networks. However, these approaches\noften require large amounts of labeled data and tend to perform\npoorly with noise or incomplete data.\nIn contrast, generative models serve as flexible frameworks\nthat can not only incorporate architectures such as CNNs,\nRNNs, and GNNs, enhancing their representational capacity,\nbut are also effective at traffic data generation and imputation.\nHowever, generative models such as GANs [69], [70] often\nsuffer from issues like mode collapse and unstable training,\nwhile another type of generative models, VAEs [71], [72],\nfrequently produces lower-quality outputs and exhibits limited\nexpressiveness in their latent spaces.\nRecently, diffusion models have emerged as a promising\nclass of generative models, offering several unique advantages\nthat make them particularly well-suited for ITS applications:\n\u2022 High-fidelity\nGeneration.\nDiffusion\nmodels\nhave\ndemonstrated the ability to generate high-quality and\ndiverse outputs in traffic-related tasks. Compared to\nGANs and VAEs, diffusion models exhibit greater ease\nof training and superior generative capabilities. [45].\n\u2022 Controllable Generation. By incorporating task-related\nconditions, such as traffic layout, external factors, or task-\nrequirements text, conditional diffusion models enable\ncontrollable outputs. This capability is particularly useful\nfor a wide range of traffic-related applications, such\nas generating accident data for safety-critical testing or\ntraining accident detection models.\n\u2022 Strong Flexibility. Diffusion models can be flexibly\ncombined with other methods, including GNNs, rein-\nforcement learning, and even other generative models\nsuch as GANs and VAEs. This adaptability allows them\nto handle complex spatial-temporal dependency in traffic\ndata, improve overall model performance, or improve\nsampling efficiency.\n\u2022 Probabilistic Modeling. The inherent probabilistic nature\nof diffusion models provides a robust framework for\nhandling uncertainties and variations in traffic data, which\nis essential for predicting real-world, variable traffic sit-\nuations.\n\u2022 Multi-modal Capabilities. Traffic data is inherently\nmulti-modal,\nincluding\ntrajectories,\nimages,\nspatial-\ntemporal graphs, and textual information. LDMs enable\nmulti-modal input training, making them highly suitable\nfor various traffic tasks. Moreover, LDMs conditioned on\nuser-specific text can provide a user-friendly, language-\nbased interface.\nIn the following sections, we will explore specific applica-\ntions of diffusion models in the field of intelligent transporta-\ntion systems, including autonomous driving, traffic simulation,\ntraffic forecasting, and traffic safety. These applications will\ndemonstrate how the advantages of diffusion models support\ntheir practical implementation in real-world traffic scenarios.\nIV. DIFFUSION MODELS FOR AUTONOMOUS DRIVING\nAutonomous driving represents one of the most transfor-\nmative aspects of ITS. The integration of autonomous ve-\nhicles (AVs) into ITS can drastically reduce traffic conges-\ntion, enhance safety, and improve the overall efficiency of\ntransportation networks. However, achieving full autonomy in\ndriving poses significant challenges due to the complex and\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n8\nPerception\n- Object detection\n- Semantic segmentation\n- Object tracking\n- Data generation\nPlanning and Decision Making\n- Robotics\n- Autonomous vehicle\nTrajectory Prediction\n- Human trajectory prediction\n- Vehicle trajectory prediction\nAutonomous Driving\nTrajectory Generation\nScenario Generation\n- Image-based generation\n- Point cloud-based generation\nTraffic Flow Generation\nTraffic Simulation\nDiffusion Model\nDDPM, DDIM,\nLDM, ADM, VLDM\nFDM, VDM\nTraffic Flow Forecasting\nTravel Time Estimation\nTraffic Forecasting\nTraffic Anomaly Detection\nTraffic Accident Prevention\nTraffic Safety\nFig. 6: Overview of the application of diffusion models in various domains of intelligent transportation systems.\ndynamic nature of real-world driving environments, which are\ncharacterized by unpredictable events, diverse road conditions,\nand varying traffic behaviors [130], [131], [132]. Addressing\nthese challenges requires advanced models capable of handling\nuncertainty, learning from vast amounts of data, and making\nreal-time decisions in a safe and reliable manner. Diffusion\nmodels, with their ability to model complex distributions,\nrefine data, and generate high-quality predictions, play a cru-\ncial role in advancing the capabilities of autonomous driving.\nHowever, their computational inefficiency poses challenges.\nThus, many research works focus on accelerating these models\nto meet real-time requirements.\nThis section explores the application of diffusion models to\nvarious aspects of autonomous driving, including perception,\ntrajectory prediction, and planning. By leveraging the strengths\nof diffusion models, researchers aim to improve the overall\nperformance and safety of AVs, making them more adept at\nnavigating the complexities of modern roadways.\nA. Perception\nPerception in autonomous driving systems refers to the\ntechnologies that enable self-driving vehicles to sense and\nunderstand their environment [133]. However, sensor data are\noften affected by intemperate weather, light conditions, and\nother factors, which introduce noises and pose challenges for\nperception [134]. With the rapid development of diffusion\nmodels in the field of computer vision [7], [38], [135],\nmany researchers are now focusing on their applications in\nautonomous driving perception. The increasing interest in\ndiffusion models is attributed to their ability to enhance the\nclarity and quality of sensor data under diverse conditions\n[83], [136], [137], as well as their proficiency in modeling\nuncertainty in perception [138], [31]. By leveraging these\nstrengths, researchers aim to enhance perception tasks such as\nobject detection, semantic segmentation, and object tracking,\nthereby contributing to safer and more reliable autonomous\nvehicles. In the following part, we present a review of the\ncurrent advancements in the application of diffusion models\nfor these perception tasks.\n1) Object Detection:\nObject detection involves locating and sizing objects within\nan image[134]. Specifically, it entails determining the presence\nof objects and their positions by drawing bounding boxes\naround them. Recent advancements have introduced diffusion\nmodels to enhance detection accuracy. For example, Chen et\nal. [79] first redefined 2D object detection as a denoising\ndiffusion process conditioned on the corresponding image,\ntransforming noisy bounding boxes into precise object boxes.\nNotably, their model demonstrates superior flexibility, enabling\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n9\nTABLE I: Applications of diffusion models in intelligent transportation systems. To classify existing models, three key criteria\nare considered: the task, the denoising condition, and the architecture. Additionally, the datasets and open source are provided.\nThe following abbreviations are used to denote the architectures: DDPM (Denoising Diffusion Probabilistic Model) [16], DDIM\n(Denoising Diffusion Implicit Model) [73], ADM (Ablated Diffusion Model) [45], LDM (Latent Diffusion Model) [38], LED\n(LEapfrog Diffusion Model) [74], VLDM (Video Latent Diffusion Model) [57], EDM (Elucidating Diffusion Model) [75],\nFDM (Flexible Diffusion Model) [76], D3PM (Discrete Denoising Diffusion Probabilistic Model) [77], CARD (Classification\nand Regression Diffusion Model) [78].\nPaper\nTask\nDenoising Condition\nArchitecture Datasets\nYear\nOpen Source\nDiffusionDet [79]\n2D object detection\nconditioned on image feature\nDDIM\nCrowdHuman [80]\nCOCO [81]\n2023 ICCV\nDiffusionDet\nDetDiffusion [82]\n2D object detection\nconditioned\non\nperception-\naware attributes\nLDM\nCOCO [81]\n2024 CVPR\n\u2014\u2014\nDiffBEV [83]\nBEV semantic segmentation\n3D object detection\nconditioned on BEV feature\nDDPM\nnuScenes [84]\n2024 AAAI\nDiffBEV\nDDP [31]\nBEV map segmentation\nsemantic segmentation\ndepth estimation\nconditioned on image feature\nDDIM\nADE20K [85]\nNYU-DepthV2 [86]\nKITTI [87] et al.\n2023 ICCV\nDDP\nVPD [88]\nsemantic segmentation\nimage segmentation\ndepth estimation\nconditioned on text\nLDM\nADE20K [85]\nRefCOCO [89]\nNYU-DepthV2 [86]\n2023 ICCV\nVPD\nChen et al. [90]\nmulti-object tracking\nconditioned on text\nLDM\nMOT20 [91] et al.\n2024 CVPR\nLtD-MOT\nLuo et al. [92]\nmulti-object tracking\nconditioned on two adjacent\nraw images\nDDPM\nMOT20 [91] et al.\n2024 AAAI\nDiffusionTrack\nXie et al. [93]\nobject tracking\nunconditional\nDDIM\nGOT-10k [94]\nLaSOT [95]\n2024 CVPR\nDiffusionTrack\nLuo et al. [96]\n3D point cloud generation\nconditioned on shape latent\n[97]\nDDPM\nShapeNet\n2021 CVPR\nDPC\nDiffuMask [39]\nsemantic segmentation\nperception data augmentation\nconditioned on text\nLDM\nVOC [98]\nADE20K [85]\nCityscapes [99]\n2023 ICCV\nDiffuMask\nDatasetDM [40]\nperception data augmentation\nconditioned on text\nLDM\nCOCO [81] et al.\n2023 NIPS\nDatasetDM\nMID [27]\nhuman trajectory prediction\nconditioned on observed tra-\njectories\nDDPM\nSDD [100]\nETH [101]\nUCY [102]\n2022 CVPR\nMID\nLED [74]\nhuman trajectory prediction\nspeed up\nconditioned on observed tra-\njectories\nLED\nSDD [100] et al.\n2023 CVPR\nLED\nSingularTrajectory\n[103]\nhuman trajectory prediction\nspeed up\nconditioned on observed scene DDIM\nETH [101] et al.\n2024 CVPR\nSingularTrajectory\nIDM [104]\nhuman trajectory prediction\nspeed up\nconditioned on observed tra-\njectories\nconditioned on endpoint\nDDPM\nSDD [100] et al.\n2024 arxiv\n\u2014\u2014\nLADM [105]\nhuman trajectory prediction\nspeed up\nconditioned on coarse future\ntrajectory\nVAE\nDDPM\nETH [101] et al.\n2024 TIM\n\u2014\u2014\nBCDiff [106]\nhuman trajectory prediction\ninstantaneous trajectory pre-\ndiction\nconditioned on gate\nDDPM\nSDD [100] et al.\n2024 NIPS\n\u2014\u2014\nMotionDiffuser\n[28]\nmulti-agent prediction\nconditioned on observed scene\n, constraints\nclassifier guidance\nLDM\nWOMD [107]\n2023 CVPR\n\u2014\u2014\nSceneDiffusion\n[58]\nmulti-agent prediction\nconditioned\non\nobserved\nscene, interval time\nunconditional\nLDM\nArgoverse [108]\n2023 ITSC\n\u2014\u2014\nEquidiff [29]\nvehicle trajectory prediction\nconditioned on observed tra-\njectories, interactions\nDDPM\nNGSIM [109]\n2023 ITSC\n\u2014\u2014\nYao et al. [110]\nvehicle trajectory prediction\nconditioned on observed tra-\njectories, map\nDDPM\nArgoverse2 [111]\n2023 CSIS-IAC \u2014\u2014\nDiffuser [46]\nbehavior planning\nunconditional\nclassifier guidance\nADM\nD4RL [112]\n2022 ICML\ndiffuser\nDecision\nDiffuser\n[52]\ndecision making\nbehavior planning\nconditioned on rewards, con-\nstraints, skills\nclassifier-free guidance\nADM\nD4RL [112]\n2023 ICLR\n\u2014\u2014\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n10\nMPD [47]\nmotion planning\nunconditional\nclassifier guidance\nDDPM\nPointMass2D\n2023 IROS\nmpd\nDiffusion-ES [113] motion planning\nunconditional\ntruncated\nDDPM\nnuPlan [114]\n2024 CVPR\ndiffusion-es\nDrive-WM [60]\nmotion planning\nmultiview video generation\nconditioned on adjacent views VLDM\nnuScenes [84]\n2024 CVPR\nDrive-WM\nGenAD [61]\nmotion planning\nmultiview video generation\nconditioned on past frame, text VLDM\nWOMD [107] et al.\n2024 CVPR\nDriveAGI\nCTG [48]\nvehicle trajectory generation\nconditioned on observed scene\nSTL-based guidance\nADM\nnuScenes [84]\n2023 ICRA\nCTG\nCTG++ [49]\nmulti-agent trajectory genera-\ntion\nconditioned on observed scene\nlanguage-based guidance\nADM\nnuScenes [84]\n2023 CoRL\nCTG++\nDragtraffic [59]\nmulti-agent trajectory genera-\ntion\nconditioned on initial scene,\ntext\nLED\nWOMD [107]\n2024 IROS\nDragtraffic\nDJINN [50]\nmulti-agent trajectory genera-\ntion\nconditioned on arbitrary state\nclassifier-free guidance\nbehavior classes guidance\nEDM\nArgoverse [108]\nINTERACTION\n[115]\n2024 NIPS\n\u2014\u2014\nPronovost\net\nal.\n[30]\nmulti-agent trajectory genera-\ntion\nconditioned on map, tokens\nEDM\nLDM\nArgoverse2 [111]\n2023 NIPS\n\u2014\u2014\nRempe et al. [53]\nhuman trajectory generation\nconditioned on observed scene\nclassifier-free guidance\nADM\nETH [101] et al.\nnuScenes [84]\n2023 CVPR\ntrace pacer\nFDM [76]\nimage-based driving scenario\ngeneration\nconditioned\non\npreviously\nsampled frames\nFDM\nCarla [116]\n2022 NIPS\n\u2014\u2014\nGAIA-1 [54]\nimage-based driving scenario\ngeneration\nconditioned\non\npast\nimage,\ntext, action tokens\nclassifier-free guidance\nVDM\nFDM\nreal-world dataset\n2023 arxiv\n\u2014\u2014\nDriveDreamer [62] image-based driving scenario\ngeneration\nconditioned on image, road\nstructure, text\nLDM\nVLDM\nnuScenes\n2023 arxiv\nDriveDreamer\nDriveDreamer-2\n[117]\nimage-based driving scenario\ngeneration\nconditioned on structured info\nby LLMs, text\nEDM\nnuScenes [84]\n2024 arxiv\nDriveDreamer2\nPanacea [32]\nimage-based driving scenario\ngeneration\nconditioned on image, text,\nBEV sequence\nLDM\nDDIM\nnuScenes [84]\n2024 CVPR\npanacea\nDrivingDiffusion\n[33]\nimage-based driving scenario\ngeneration\nconditioned on key-frame, op-\ntical flow prior, text, 3D layout\nVDM\nLDM\nnuScenes [84]\n2023 arxiv\nDrivingDiffusion\nWoVoGen [63]\nimage-based driving scenario\ngeneration\nconditioned on past world vol-\numes, actions, text, 2D image\nfeature\nLDM\nnuScenes [84]\n2023 arxiv\nWoVoGen\nLiDMs [64]\npoint cloud-based driving sce-\nnario generation\nunconditional\nconditioned on arbitrary data\nLDM\nnuScenes [84]\nKITTI-360 [118]\n2024 CVPR\nLiDAR-Diffusion\nCopilot4D [55]\npoint cloud-based driving sce-\nnario generation\nconditioned on past observa-\ntions, actions\nclassifier-free guidance\nD3PM\nADM\nnuScenes [84] et al.\n2024 ICLR\n\u2014\u2014\nKSTDiff [119]\ntraffic flow generation\nconditioned on urban knowl-\nedge\ngraph,\nregion\nfeature,\nvolume estimator\nCARD\nreal-world dataset\n2023 SIGSPA-\nTIAL\nKSTDiff\nDiffTraj [34]\nGPS trajectory generation\nconditioned on trip region, de-\nparture time et al.\nclassifier-free guidance\nDDIM,\nADM\nreal-world dataset\n2023 NIPS\nDiffTraj\nDiff-RNTraj [35]\nGPS trajectory generation\nconditioned on road network\nDDPM\nreal-world dataset\n2024 arxiv\n\u2014\u2014\nChatTraffic [41]\ntraffic flow generation\nconditioned on text\nLDM\ntext-traffic\npairs\ndataset\n2024 arxiv\nChatTraffic\nRong et al. [120]\norigin-destination flow genera-\ntion\nconditioned on node feature,\nedge feature\nDDPM,\nADM\nreal-world dataset\n2023 arxiv\n\u2014\u2014\nDiffSTG [36]\ntraffic flow forecasting\nConditioned on past graph sig-\nnals, graph structure\nDDIM\nPEMS [121] et al.\n2023 GIS\nDiffSTG\nSpecSTG [122]\ntraffic flow forecasting\ntraffic speed forecasting\nconditioned on past graph sig-\nnals feature, adjacency matrix\nDDPM\nPEMS [121] et al.\n2024 arxiv\nSpecSTG\nDiffUFlow [43]\ntraffic flow forecasting\nconditioned on pass feature\nmap, coarse-grained flow map,\nsemantic features\nDDPM\nreal-world dataset\n2023 CIKM\n\u2014\u2014\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n11\nXu et al. [123]\ntraffic flow forecasting\nunconditional\nDDPM\nreal-world dataset\n2023 ICASSP\n\u2014\u2014\nST-SSPD [124]\ntraffic flow forecasting\nconditioned\non\npast\ndata\npoints,\ntemporal\nencoding,\nnode identifier\nDDPM\nMETR-LA [125] et\nal.\n2023 MobiArch \u2014\u2014\nDifforecast [126]\ntraffic flow forecasting\nimage generation\nconditioned on past S-T image DDPM\nreal-world dataset\n2023 BigData\n\u2014\u2014\nLin et al. [44]\norigin-destination travel time\nestimation\nconditioned on origin, destina-\ntion, departure time\nDDPM\nreal-world dataset\n2023 MOD\n\u2014\u2014\nDiffTAD [127]\ntrajectory anomaly detection\nunconditional\nDDIM\nNGSIM [109]\n2024 KBS\n\u2014\u2014\nVAD [128]\nvideo anomaly detection\nunconditional\nconditioned on original fea-\ntures\nLDM,\nDDIM\nCUHK Avenue [129]\net al.\n2023 ICCV\n\u2014\u2014\nAdVersa-SD [42]\naccident video understanding\naccident preventing\nconditioned on text, bounding\nboxes\nLDM\nMM-AU [42]\n2024 CVPR\nMM-AU\na dynamic number of boxes and iterative evaluation during\ninference. Additionally, Wang et al. [82] presented a pioneer-\ning framework that integrates diffusion models and perceptive\nmodels to enhance data generation quality and perception\ncapabilities. The framework leverages perception-aware at-\ntributes as conditions and employs perception-aware loss as\na form of supervision during the image generation process.\nThis conditional approach enables the generation of images\ntailored to specific perceptual criteria, thereby improving the\nperformance of downstream tasks such as object detection.\n2) Semantic Segmentation:\nSemantic segmentation involves classifying each pixel in\nan image into a predefined category [134]. Bird\u2019s Eye View\n(BEV) perception holds significant importance in the domain\nof autonomous driving perception, especially for semantic\nsegmentation. Recent works have utilized the diffusion model\nto enhance BEV perception [83], [138], [136]. Notably, Zhou\net al. [83] first applied conditional diffusion models to denoise\nand refine BEV features, addressing noise and distortions from\ncamera parameters and LiDAR scans, significantly improving\nBEV semantic segmentation and 3D object detection. In detail,\nthree BEV features serve as conditions for the diffusion model,\nenabling progressive denoising and enhancing fine-granularity\ndetails such as object boundaries and shapes.\nBeyond BEV feature conditioning [83], image features\n[31] and text [88] have also been employed as conditions\nin semantic segmentation tasks. Ji et al. [31] introduced\nDDP, a noise-to-map method that progressively removes noise\nfrom a Gaussian distribution, guided by image features, to\nproduce visual perception. DDP stands out for its dynamic\ninference capabilities, and natural awareness of the perception\nuncertainty. Additionally, DDP is easy to generalize to most\ndense visual perception tasks without needing task-specific\ndesigns. Motivated by the compelling generative semantic\nof a text-to-image diffusion model [38], Zhao et al. [88]\nproposed VPD, a framework utilizing pre-trained text-to-image\ndiffusion models for visual perception tasks. By prompting\nthe denoising decoder with textual inputs and refining text\nfeatures with an adapter, VPD aligns visual content with text\nprompts and leverages cross-attention maps for guidance. This\nwork suggests that pre-trained text-to-image diffusion models\ncan efficiently adapt to downstream visual perception tasks,\nbridging generative models and visual perception.\n3) Object Tracking:\nObject tracking involves locating an object or multiple\nobjects in a video, maintaining their identities, and tracking\ntheir trajectories over time [139]. Chen et al. [90] addressed\nthe challenge of trajectory length imbalance in multiple object\ntracking (MOT) datasets by proposing Stationary and Dynamic\nCamera View Data Augmentation (SVA and DVA) and a\nGroup Softmax module. Specifically, the DVA employs a con-\nditional diffusion model to alter scene backgrounds, helping\nthe network focus more on pedestrian features. This approach\neffectively alleviate the impact of long-tail distribution, en-\nhancing tracking system effectiveness. Additionally, Luo et al.\n[92] proposed a noise-to-tracking framework, which formu-\nlates object detection and association jointly as a consistent\ndenoising diffusion process from paired noise boxes to paired\nground-truth boxes, enabling consistency between detection\nand tracking. In contrast, Xie et al. [93] introduced a novel\nnoise-to-target tracking paradigm, employing a point set-based\ndenoising diffusion process for dynamic and precise target\nlocalization, offering superior self-correction and appearance\nvariation handling capabilities. This method also simplifies the\npost-processing, enabling real-time tracking capabilities.\n4) Perception Data Generation:\nRecent advancements [39], [40] have highlighted the ef-\nficacy of diffusion models in synthesizing images and their\ncorresponding annotations. Specifically, Wu et al. [39] have\nconcentrated on semantic segmentation, utilizing a text-guided\npre-trained diffusion model to generate synthetic images with\npixel-level semantic mask annotations. Building upon this\nwork, Wu et al. [40] presented a dataset generation model\nthat also leverages the knowledge learned by pre-trained\ndiffusion models to produce diverse perception annotations. It\nemphasizes a unified perception decoder, which can be trained\nwith minimal human-labeled data, to generate extensive high-\nfidelity images paired with various perception annotations\nincluding depth, segmentation, and human pose estimation.\n3D point cloud data, another form of perception data, has\nalso seen significant progress in generative modeling. Several\nstudies have applied diffusion models for the generation of\n3D point clouds. Luo et al. [96] introduced a novel generative\nmodel by treating 3D point cloud generation as a reverse\ndiffusion process. The model conditions on a shape latent,\nand demonstrates flexibility and robustness in generating high-\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n12\nquality, realistic 3D point clouds. Following this work, Sun\net al. [140] addressed the vulnerability of 3D point cloud\nrecognition models to adversarial attacks by leveraging the\ndiffusion model designed in [96] as the base model for the\nadversarial point cloud purification.\nB. Trajectory Prediction\nTrajectory prediction in autonomous driving systems in-\nvolves using past states of traffic participants in a given scene\nto forecast their future states [141]. The primary challenges\ninclude the uncertainty and multi-modality of future behavior,\nthe complex interactions between traffic participants, and\nenvironmental influences like road geometry [141], [142]. In\nrecent years, diffusion models have emerged as a promising\napproach for trajectory prediction due to their ability to\ncapture the inherent uncertainty and multi-modality of human\nbehavior and driving behavior. Additionally, diffusion models\ncan flexibly integrate map information, constraints, and other\nrelevant factors.\n1) Human Trajectory Prediction:\nTo address the challenges of unstable training and unnatural\ntrajectories in human trajectory prediction, Gu et al. [27]\nproposed Motion Indeterminacy Diffusion (MID). This method\nfirst leverages a diffusion model to transform trajectory pre-\ndiction into a reverse diffusion process, achieving a balance\nbetween prediction diversity and determinacy by adjusting the\nlength of a parameterized Markov chain. However, despite\nits promising performance, MID\u2019s 17-second runtime for 100\ndiffusion steps is impractical for real-time applications in\nautonomous driving systems. Following this pioneering work,\nmany subsequent studies have focused on the application of\ndiffusion models in trajectory prediction [74], [103], [104],\n[105], [143], [144]. Notably, to address the time-consuming\nproblem, Mao et al. [74] introduced a trainable leapfrog ini-\ntializer to bypass multiple denoising steps, enabling real-time\nprediction. Specifically, they employed a two-stage training\nstrategy: the first stage trains a denoising module similar to\nMID [27], while the second stage optimizes the leapfrog ini-\ntializer using the frozen denoising module. During inference,\nthe leapfrog initializer allows denoising to start directly from\nthe last few steps, significantly reducing computational time.\nLater, Bae et al. [103] proposed a unified model, Singular-\nTrajectory, introducing an adaptive anchor mechanism and\nleveraging a diffusion-based predictor to enhance prototype\npaths through a cascaded denoising process. Moreover, the\nadaptive anchor functions as a good initializer similar to\n[74], to speed up the denoising process. Additionally, Liu\net al. [104] decoupled trajectory prediction uncertainty into\nintention uncertainty and action uncertainties through two\ndiffusion processes. They also introduced a PriorNet mod-\nule for estimating prior noise distribution, reducing diffusion\nsteps and consequently cutting inference time by two-thirds.\nAnother study is LADM [105], which integrates the VAEs\nwith diffusion models. This combination enables the diffusion\nmodels to refine future trajectories generated by the VAE in\na low-dimensional space, enhancing prediction accuracy and\nsupporting real-time inference.\nInstantaneous trajectory prediction presents another chal-\nlenge in human trajectory prediction due to the need for\naccurate predictions based on very limited observational data\n[145]. Li et al. [106] addressed this challenge by utilizing\nbidirectional diffusion models to generate unobserved his-\ntorical trajectories and future trajectories step-by-step, effec-\ntively leveraging complementary information between them.\nFurthermore, they proposed a gate mechanism to balance the\ncontributions between the observed and future trajectories.\n2) Vehicle Trajectory Prediction:\nVehicle trajectories are often governed by physical rules and\nconstraints. Several works have incorporated these constraints\nas classifiers [28] or conditions [58] into diffusion models,\nthereby enabling physically feasible trajectory predictions.\nJiang et al. [28] utilized a compressed trajectory representation\nusing PCA-base latent diffusion models for multi-agent joint\nmotion prediction. Additionally, they introduced constrained\nsampling, enabling controlled predictions based on differ-\nentiable cost functions as a classifier. Similarly, Westny et\nal. [146] integrated differential motion constraints into the\ndiffusion model output, generating realistic future trajectories.\nAnother work by Balasubramanian et al. [58] employed con-\nditional latent diffusion models with temporal constraints to\npredict the motion of vehicles in a traffic scenario, while also\nproviding an unconditional mode as a scene initializer.\nIn addition to these advancements, other works have com-\nbined diffusion models with other network architectures. For\nexample, Chen et al. [29] noticed that previous works did\nnot fully exploit the geometric properties of trajectory. They\ncombined the diffusion models and equivariant transformer\nas an SO(2)-equivariant diffusion model for vehicle trajectory\nprediction, thereby fully utilizing the geometric properties of\nlocation coordinates. Moreover, they utilized Recurrent Neural\nNetworks and Graph Attention Networks to capture social\ninteractions among vehicles. Additionally, Yao et al. [110] ex-\ntended the MID model [27] for vehicle trajectory prediction by\nusing Graph Neural Networks to capture interactions between\nagents and road elements.\nC. Planning and Decision-making\nIn autonomous driving systems, planning and decision-\nmaking are crucial components. Planning entails generating a\nsafe and comfortable trajectory based on the vehicle\u2019s current\nstate, and environmental information [132]. Decision-making\ninvolves selecting the optimal high-level action by considering\nthe final goal, the environment, traffic rules, and ensuring\nsafety [147]. Diffusion models have shown promise potential\nin enhancing these components, particularly in improving\ngeneralization and flexibly integrating with other algorithms.\nDiffusion models exhibit robust generalization to new envi-\nronments with unseen obstacles [47], [60], which is essential\nfor dynamic environments. Additionally, diffusion models can\nflexibly integrate with other algorithms, enhancing their effec-\ntiveness. Since the autonomous vehicle is a specialized form\nof robotics, we examine the topic within both the robotics and\nautonomous driving fields.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n13\n1) Planning and Decision-making in Robotics:\nDiffusion models can flexibly combine with motion-\nplanning approaches, such as reinforcement learning (RL) [46]\nor trajectory optimization algorithms [47]. Specifically, Janner\net al. [46] proposed the Diffuser model, which combines\nRL with classifier-guided diffusion models [45] to improve\nplanning and decision-making processes. The Diffuser itera-\ntively denoises trajectories to generate plans, with the sam-\npling process guided by gradients of the cumulative rewards\nlearned through RL. In contrast, the follow-up work, Decision\nDiffuser [52], employed classifier-free diffusion guidance [51]\nto generate a sequence of future states, conditioning on re-\nwards, various constraints, and behavior skills. This approach\ndoesn\u2019t require a separately trained classifier but learns both a\nconditional and an unconditional model for the noise. While\nthe Decision Diffuser [52] demonstrates that classifier-free\nguidance performs better than classifier guidance in practice,\nthe Diffuser [46] enables planning for new rewards without\nretraining. A different approach was presented by Carvalho\net al. [47], who utilized learned diffusion priors to initialize\nan optimization-based motion planner. This method not only\nimproves and accelerates the planning process but also fosters\ngreater diversity in trajectory planning.\n2) Planning and Decision-making in AVs:\nDiffusion models have been employed to optimize the\nplanning process in autonomous driving. Yang et al. [113]\nfirst combined gradient-free evolutionary search with diffusion\nmodels to enhance planning for autonomous driving. Unlike\nconventional methods that use naive Gaussian perturbations,\nthis approach leverages a truncated diffusion-denoising process\nto mutate trajectories in the evolutionary search process,\nensuring that the resulting mutations remain within the data\nmanifold.\nAdditionally, several studies have leveraged diffusion mod-\nels to generate out-of-distribution driving scenarios, thereby\nimproving planning performance. For example, Wang et al.\n[60] leveraged diffusion models to generate multi-view future\nstate videos, enabling the prediction of future events and\nrisk assessment through these videos, thereby enhancing the\nsafety of end-to-end planning. Furthermore, evaluations on\ncounterfactual events demonstrate that their model improves\ngeneralization capabilities in out-of-distribution scenarios. An-\nother video generative model for motion planning is GenAD\n[61], which has the ability to generalize across diverse and\nunseen driving datasets in a zero-shot manner. Moreover,\nGenAD can be adapted for various tasks, including language-\nconditioned prediction, action-conditioned prediction, and mo-\ntion planning.\nRL has seen widespread application in planning and\ndecision-making for autonomous driving [148], [149]. Recent\nadvancements have incorporated diffusion models to improve\nthe performance and sampling efficiency of RL algorithms\n[150]. For example, Wang et al. [151] introduced Diffusion-\nQL, which integrates a conditional diffusion model as the\npolicy and combines it with Q-learning. Subsequently, Liu et\nal. [152] employed conditional diffusion models as the actor in\nan Actor-Critic decision-making framework, facilitating policy\nexploration and learning.\nV. DIFFUSION MODELS FOR TRAFFIC SIMULATION\nTraffic simulation is a critical tool for developing and\ntesting intelligent transportation systems, allowing researchers\nand engineers to model, analyze, and simulate the behav-\nior, interactions or movement of traffic participants within a\ntransportation network [153], [10]. Universal methods, such\nas rule-based or data-driven models often struggle to capture\nthe complexity and variability of real-world traffic dynamics\n[154]. These methods also lack the controllability to generate\ndiverse and customizable scenarios, which are essential for\nsafety-critical testing [155]. Furthermore, traffic data are often\nunavailable or suffer from privacy concerns, posing additional\nchallenges for data-driven traffic simulations.\nDiffusion models, a type of generative model, have recently\nemerged as a promising solution for overcoming these chal-\nlenges in traffic simulation. They are particularly effective\nat learning the distributions of traffic patterns, enabling the\ngeneration of high-fidelity simulations that closely mimic real-\nworld situations. Moreover, diffusion models offer signifi-\ncant advantages in terms of controllability, allowing users to\ncustomize generated traffic scenarios, trajectories, and flows\naccording to specific conditions or guidance.\nThis section explores the applications of diffusion models\nin traffic simulation, with a focus on their roles in traffic\ntrajectory generation, traffic scenario generation, and traffic\nflow generation. We also examine recent advancements in this\nfield and discuss how diffusion models are being integrated\nwith other technologies to enhance their effectiveness and\napplicability in intelligent transportation systems.\nA. Traffic Trajectory Generation\nTraffic trajectory generation, which focuses on creating\nrealistic and compliant paths for vehicles and pedestrians in\ntraffic simulations, is essential for the development and testing\nof intelligent transportation systems. Traditional heuristic-\nbased models [156] enable vehicles to adhere to specific\ntrajectories and traffic rules, but they often struggle to capture\nthe complexity of real-world driving behaviors. In contrast,\ndata-driven approaches can produce more realistic and human-\nlike behaviors [154], but they often lack the controllability to\ngenerate user-defined trajectories. Diffusion models stand out\nfor their ability to model real-world traffic data effectively\nby capturing the complexity and variability of traffic patterns.\nAdditionally, guidance-based diffusion models enhance con-\ntrollability and flexibility during the inference stage. These\nstrengths of diffusion models make them highly suitable for\ngenerating both realistic and controllable trajectories. Recent\nresearch has increasingly utilized these advanced diffusion\nmodels to improve the realism and controllability of agent\ntrajectory generation, offering significant advancements in\ntraffic simulation.\nSeveral studies have utilized classifiers to enhance con-\ntrollability during sampling, such as using Signal Temporal\nLogic (STL) rules classifiers [48] or language-based classifiers\n[49]. Zhong et al. [48] proposed a classifier-guided conditional\ndiffusion model to produce realism and controllable driving\ntrajectories. Unlike the approach of training a reward function\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n14\nas guidance [46], they utilized STL [157] to guide sampling\nto generate trajectories that are both physically feasible and\ncompliant with rules. Building on this, Zhong et al. [49] further\nadvanced their model by incorporating language instructions\nto guide the trajectory sampling process, thereby enhancing\nuser-friendliness. Specifically, they employed a large language\nmodel (LLM) to convert user language instructions into a\nguidance loss, replacing the STL-based guidance loss used\nin their earlier work [48]. In contrast, wang et al. [59]\nenhanced user-friendliness and controllability by introducing\nuser-defined context through the cross-attention mechanism.\nAdditionally, they utilized a regression model for initial scene\ncreation to enhance realism.\nMeanwhile, recent research has increasingly focused on\nmulti-agent joint trajectories generation, aiming to generate\nmore interactive trajectories [48], [59], [50], [30], [158], [159].\nNotably, Niedoba et al. [50] employed both classifier guidance\nand classifier-free guidance diffusion models to generate joint\ntrajectories for all agents in a traffic scene. They trained a\nbehavior classifier as guidance for conditional sampling, and\ncontrolled the strength of conditioning through classifier-free\nguidance, thereby enabling the flexible sampling of diverse\nbehavior modes. Additionally, Pronovost et al. [30] integrated\nlatent diffusion with object detection and trajectory regression\nto simultaneously generate poses and trajectories for all agents,\nconditioned on a map and scenario tokens.\nSome research has focused on human trajectory simulation.\nFor example, Rempe et al. [53] introduced a controllable\npedestrian simulation system that integrates a trajectory dif-\nfusion model (TRACE) for generating pedestrian paths and\na physics-based humanoid controller (PACER) to establish a\nclosed-loop system. Furthermore, the guided TRACE model\nallows users to constrain trajectories based on target way-\npoints, desired speeds, specified social groups, and other\nfactors.\nB. Traffic Scenario Generation\nTraffic scenario generation involves creating a temporal\nsequence of traffic scene elements that simulate the actions,\ninteractions, and events of the participating agents within a\ndriving environment [160], [155]. It plays a significant role\nin enhancing the efficiency and safety of intelligent trans-\nportation systems, as it enables the creation of diverse and\nsafety-critical scenarios. However, traffic scenario generation\nfaces two critical challenges: Consistency and Controllability\n[161], [32]. Consistency ensures that the generated scenarios\nare temporally and multi-view coherent, maintaining logical\nrelationships across time and from different viewpoints within\nthe scene. Controllability refers to the ability to guide the gen-\nerated scenarios to align with specific annotations, conditions,\nor objectives. Diffusion models have emerged as a powerful\ntool to address these challenges. Fundamentally, they can\neffectively model complex data distributions, achieving high\nlevels of realism. Additionally, diffusion models can be flexi-\nbly combined with various approaches, such as cross-view and\ncross-frame attention mechanisms, post-processing techniques,\nand multi-stage generation processes, to ensure both temporal\nand multi-view consistency. Moreover, controllable diffusion\nmodels, like ControlNet [162], can incorporate multimodal\nconditioning controls, including layout, text, segmentation, and\nother inputs, to fine-tune large diffusion models like Stable\nDiffusion [38], thereby enhancing the controllability of driving\nscenario generation.\nWith the rapid development of diffusion models in image\n[38] generation, video generation [57], [163], [164], and\nworld models [165], [161], diffusion models offer a powerful\nframework for generating high-quality, consistent, and control-\nlable traffic scenarios. In the following part, we will explore\nthe current advancements in traffic scenario generation from\ntwo different perspectives: image-based and point cloud-based\napproaches.\n1) Image-based Driving Scenario Generation:\nRecent advancements in diffusion models have led to signif-\nicant progress in generating realistic and controllable image-\nbased driving scenarios. For example, Harvey et al. [76]\nproposed a Flexible Diffusion Model (FDM), that enables\nthe model to sample any arbitrary subset of video frames\nconditioned on others, thereby optimizing frame sampling\nschedules and effectively handling long-range temporal de-\npendencies. Building on this foundational work in temporal\nsequence modeling, Hu et al. [54] integrated a video diffusion\ndecoder with a world model to create high-fidelity and long-\nterm driving scenarios. The world model [165] facilitates the\nunderstanding of the environment and the prediction of reason-\nable object interactions, while the diffusion decoder translates\nlatent representations into high-quality videos with realistic\ndetail. Additionally, it offers fine-grained control over the sim-\nulation environment through action and language conditioning.\nSimilarly, DriveDreamer [62] focused on generating high-\nquality, controllable driving videos and policies that align with\nreal-world traffic structures. Building upon the DriveDreamer\nfoundation, Zhao et al. [117] proposed the DriveDreamer-\n2 framework, which leverages the power of finetuned-LLMs\n[166], [167] to translate user descriptions into agent trajecto-\nries. Additionally, it employs an HDMap generator to produce\nhigh-definition (HD) maps. These trajectories and HD maps\nare then used as structured conditions to ultimately generate\nmulti-view driving scenes.\nNext, we discuss the multi-view driving video generation.\nWen et al. [32] integrated a pre-trained diffusion model and\na decomposed 4D attention mechanism within a two-stage\ngeneration pipeline to generate multi-view driving scenario\nvideos with temporal consistency. The first stage trains a multi-\nview image generator, while the second stage expands these\nimages along the temporal axis to create video sequences. Li\net al. [33] proposed DrivingDiffusion for generating spatially\nand temporally consistent multi-view videos of complex urban\ndriving scenes. Another important work is WoVoGen [63],\nwhich leverages 4D world volumes as foundational elements\nfor multi-camera street-view video generation, addressing key\nchallenges in ensuring intra-world consistency and inter-sensor\ncoherence. Furthermore, these approaches [32], [33], [63]\nemployed the ControlNet [162] framework to achieve Fine-\ngrained control, conditioned on the BEV sequences or 3D\nlayout or world volume-aware 2D image feature.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n15\n2) Point Cloud-based Driving Scenario Generation:\nMeanwhile, the generation of realistic driving scenarios\nfrom point cloud data has gained significant attention due to its\nimportance in traffic simulation [64], [55], [168]. Notably, Ran\net al. [64] concentrated on generating realistic LiDAR driving\nscenes from a latent space that incorporates geometric priors to\ncapture realism, enhancing pattern realism, geometry realism,\nand object realism. Furthermore, their approach leverages a\npre-trained model, CLIP [169], to enable controllability under\narbitrary conditions, including text prompts, semantic maps,\nand camera views. Zhang et al. [55] proposed Copilot4D for\nbuilding unsupervised world models. This approach leverages\nVQVAE [170] to tokenize point cloud observations, and com-\nbines MaskGIT [171] with discrete diffusion models [77] to\nefficiently decode and denoise tokens in parallel, enhancing\npoint cloud-based driving scene forecasting.\nC. Traffic Flow Generation\nTraffic flow generation involves creating synthetic data that\nmodels the movement of vehicles or pedestrians across specific\nregions within a transportation network [10]. These synthetic\ndata are crucial for macroscopic simulations [153], as model-\ning real-world human mobility trajectories often suffers from\nprivacy concerns. However, traffic flow generation presents\nseveral challenges. Firstly, the non-independent and identically\ndistributed nature of trajectories between different areas and\nthe inherent stochasticity of human behavior make traffic pat-\ntern modeling complicated. Secondly, traffic flow influenced\nby external factors such as traffic conditions, departure times,\nand local events, adds further complexity. Diffusion models are\nadept at handling stochasticity and uncertainty, making them\nparticularly well-suited for traffic flow generation. Further-\nmore, diffusion models can be flexibly combined with various\napproaches, such as GCNs, RNNs, and attention mechanisms,\nto model the spatiotemporal dependencies of traffic data.\nAdditionally, diffusion models enable conditional generation\nbased on text, road networks, external factors, and other inputs,\nallowing for the generation of customized traffic flow patterns.\nTo explore how diffusion models have been applied in traffic\nflow generation, we review several notable advancements in\nthe field.\nEffectively capturing spatiotemporal dependencies is crucial\nin traffic flow generation, given that traffic flow data typi-\ncally involves spatiotemporal information. Recently, DiffSTG\n[36] and ChatTraffic [41] introduced a GCN-based architec-\nture to effectively model spatiotemporal dependencies, while\nTimeGrad [172] employs an RNN, and both CSDI [173] and\nSTPP [174] utilize attention mechanisms for this purpose. In\ncontrast, Zhou et al. [119] proposed the KSTDiff model, which\nleverages an urban knowledge graph (UKG) to capture the\nspatiotemporal dependencies of urban flow. Additionally, they\ndeveloped a volume estimator that integrates region-specific\nfeatures to guide the diffusion model\u2019s sampling process,\nenabling the accurate generation of urban flow across different\nregions. Notably, ChatTraffic [41] also presented the first text-\nto-traffic generation framework. This approach incorporates\nBERT [175], a pre-trained text encoder, to extract text em-\nbedding, which serves as conditions to guide the generation\nof traffic flow.\nMany researchers have focused on GPS trajectory gener-\nation [34], [35] due to the ability of GPS trajectory data to\nreflect traffic flow, which is crucial in ITS. Specifically, Zhu et\nal. [34] proposed a Traj-UNet structure within diffusion mod-\nels for spatial-temporal modeling and embedding conditional\ninformation such as the trip region and departure time, thereby\nenabling controlled GPS trajectory generation. Subsequently,\nDiff-RNTraj [35] generates trajectories conditioned on the\nroad network, with these trajectories represented in a hybrid\nformat where each point is defined by a discrete road segment\nand a continuous moving rate.\nAdditionally, Rong et al. [120] proposed a cascaded graph\ndenoising diffusion method to capture the joint distribution of\nnodes and edges within the origin-destination (OD) network.\nThis method generates region-level OD flow for a new city\nby first generating the topology structure and then the corre-\nsponding mobility flows.\nVI. DIFFUSION MODELS FOR TRAFFIC FORECASTING\nTraffic forecasting is a critical component of intelligent\ntransportation systems, facilitating the optimization of traffic\nflow, the reduction of congestion, and the enhancement of\noverall transportation efficiency. It involves predicting future\ntraffic conditions, such as traffic flow rates and travel times, by\nanalyzing historical data. However, traffic forecasting presents\nsignificant challenges due to the inherent complexities of\ntransportation networks and concerns regarding the quality of\ntraffic data [176].\nRecent advancements in traffic forecasting have increas-\ningly focused on leveraging diffusion models to address these\nchallenges. Diffusion models have demonstrated significant\npromise in capturing the complex and dynamic nature of\ntraffic systems. By incorporating diffusion processes, these\nmodels effectively account for the uncertainties and noise\npresent in traffic data, making them particularly well-suited for\nhandling incomplete or imperfect traffic datasets. As a result,\nthe application of diffusion models in traffic forecasting is\ngaining momentum, especially in tasks such as traffic flow\nprediction and travel time estimation.\nA. Traffic Flow Forecasting\nTraffic flow forecasting entails predicting the future state of\ntraffic on transportation networks, including vehicle speeds,\ntraffic density, and flow rates, based on historical data and\nother relevant factors [176]. While significant progress has\nbeen made in this field, accurately forecasting traffic flow\nremains challenging due to the inherent uncertainties in flow\ndistributions and the complex external factors that impact\nforecasting performance. Additionally, the collected urban\nflow data is often unreliable, noisy, and sometimes incomplete,\nfurther complicating the prediction task. Recent advancements\nhave focused on addressing these challenges by leveraging\ndiffusion models, which have shown promise in recovering\ntraffic data [177], capturing the intricate spatial-temporal de-\npendencies and handling the uncertainties associated with\ntraffic flow data [36].\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n16\nGraph-based approaches have proven effective in extracting\nspatial correlations in traffic networks [3], [4]. Naturally,\nintegrating graph-based networks with diffusion models can\nenhance the modeling of intricate spatial-temporal depen-\ndencies. Wen et al. [36] proposed a GCN-based network\ncalled UGnet, which effectively captures multi-scale tempo-\nral dependencies and spatial correlations, thus significantly\nadvancing traffic flow prediction tasks. However, GCN-based\nmethods are computationally expensive, particularly for large-\nscale traffic networks. To address this issue, Lin et al. [122]\nincorporated a fast spectral graph convolution, which alleviates\nthe computational inefficiencies of existing models.\nDiffusion models have also been leveraged for fine-grained\ntraffic flow inference from noisy and incomplete data. For\nexample, Zheng et al. [43] and Xu et al. [123] focused on\nleveraging diffusion models for fine-grained traffic flow in-\nference from noisy and incomplete coarse-grained traffic flow\nmaps. Specifically, Zheng et al. [43] developed a transformer-\nbased spatial-temporal feature extraction network along with\na semantic feature extraction network designed to capture\nexternal factors and land features. These two types of features,\nserving as conditions for conditional diffusion models, facil-\nitate the robust modeling of dynamic and long-range spatial-\ntemporal dependencies. In contrast, Xu et al. [123] employed\na relaxed structural constraint and a disentangled scheme for\nflow map and external factor learning. Additionally, Lablack\net al. [124] proposed a vectorized state space module to\ndecompose the historical signal of an ego-graph into the\nfrequency domain, thereby reducing the impact of noise and\ndata imperfections present in real-world traffic data.\nLastly, recent research has introduced novel approaches\nthat transform the traffic flow forecasting task into a new\ndomain. Chi et al. [126] introduced a novel concept of a space-\ntime image to incorporate physical meanings of traffic state\nvariables. They transformed the traffic flow forecasting task\ninto a conditional image generation problem by leveraging\ndiffusion models.\nB. Travel Time Estimation\nOrigin-Destination (OD) travel time estimation aims to\npredict the time required to travel between a specific start-\ning point (origin) and a destination within a transportation\nnetwork. This task is complex due to the variability in travel\ntimes for the same OD pair, influenced by factors such as\ntraffic conditions and route choices [178]. Multiple historical\ntrajectories with different travel times may connect an OD pair,\nand these trajectories can differ significantly, making accurate\nprediction challenging. To address this, it is crucial to mitigate\nthe impact of outlier trajectories. The conditional diffusion\nmodel provides a promising solution to this challenge. For\nexample, Lin et al. [44] proposed a conditional diffusion-based\nmodel for OD travel time estimation, which leverages his-\ntorical trajectories. The model employs a pixelated trajectory\nrepresentation and is conditioned on origin, destination, and\ndeparture time (ODT) queries to capture correlations between\nOD pairs and historical travel patterns, thereby aiding in the\nfiltering of outlier trajectories.\nVII. DIFFUSION MODELS FOR TRAFFIC SAFETY\nTraffic safety is a critical area of research within intelli-\ngent transportation systems, focusing on minimizing the risks\nassociated with vehicular travel and reducing the frequency\nand severity of traffic accidents [179]. Recent advancements\nin diffusion models have opened new avenues for enhancing\ntraffic safety. These models excel in generating high-quality\nsamples from complex distributions and producing customiz-\nable samples conditioned on text descriptions, addressing the\nchallenge of limited traffic accident or anomaly data. They\nhave been effectively applied to various aspects of traffic\nsafety, including traffic anomaly detection and accident pre-\nvention. The successful detection of traffic anomalies and the\nprevention of accidents are crucial for maintaining safe and\nefficient transportation systems.\nA. Traffic Anomaly Detection\nTraffic anomaly detection aims to identify irregular patterns\nin traffic data that deviate from normal behavior, such as\nunusual vehicle activity, accidents, or irregular traffic flow\npatterns. Detecting these anomalies is important for traffic\nmanagement and safety. However, this task faces significant\nchallenges due to the lack of large-scale labeled anomaly\ndata and the difficulty in precisely defining the boundary\nbetween normal and abnormal patterns [180], [181]. Diffusion\nmodels, known for their powerful generative capacity, offer a\npromising solution. These models are well-suited for traffic\nanomaly detection, as anomalous events often exhibit a level\nof randomness and uncertainty that are inherently similar to\nthe diffusion process. By leveraging diffusion models to recon-\nstruct normal traffic patterns from Gaussian noise, researchers\ncan effectively identify samples that deviate from these normal\npatterns, thereby flagging them as anomalies.\nBuilding on this idea, Li et al. [127] formalized the ve-\nhicle trajectory anomaly detection problem as a noisy-to-\nnormal paradigm, which leverages the generative capabilities\nof diffusion models to reconstruct near-normal trajectories and\neffectively identifies anomalies by comparing the difference\nbetween a query trajectory and its reconstruction. Similarly,\nYan et al. [128] utilized diffusion models to learn the dis-\ntribution of normal samples for video anomaly detection.\nSpecifically, they employed two denoising diffusion modules\nto learn motion and appearance features from normal samples,\nensuring the generative quality of the produced features.\nB. Traffic Accident Prevention\nTraffic accident prevention requires a deep understanding of\naccident causality and then designing strategies to reduce their\nlikelihood. A significant challenge in this field is the lack of a\nlarge-scale and long-tailed accident dataset [182], which limits\nthe ability to develop comprehensive and effective accident\nprevention. Diffusion models, with their powerful controllable\ngeneration capabilities, have emerged as a promising tool to\novercome these challenges.\nRecent advancements in diffusion models have enabled\nmore innovative applications in traffic accident analysis and\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n17\nprevention. For example, Fang et al. [42] leveraged an ab-\nductive CLIP model within an Object-Centric Video Diffu-\nsion (OAVD) method to discern accident cause-effect chains,\nthereby enhancing the understanding of accident causality\nand improving accident prevention strategies. Specifically, this\napproach leverages diffusion models to generate new video\nframes conditioned on text descriptions, such as accident rea-\nsons and prevention advice. This allows for the visualization of\nhow accidents might unfold based on these descriptions, aiding\nin understanding and potentially predicting accident outcomes,\nand contributing to better accident prevention.\nFuture Directions\nIntegrating LLMs with \nDiffusion Models\nTraffic Guidance with \nPrior Knowledge\nNetwork Architectures \nImprovements\nFine-Tuning \nDiffusion Models\nEnhancing Speed \nand Efficiency\nFig. 7: Future research directions for diffusion models in\nintelligent transportation systems.\nVIII. FUTURE DIRECTIONS\nAs diffusion models continue to evolve, their potential to ad-\ndress complex challenges in ITS becomes increasingly evident.\nHowever, several critical areas require further investigation\nand innovation to fully realize their capabilities. This section\noutlines key research directions for diffusion models in ITS\nthat are worthy of further exploration, as shown in Fig. 7.\nA. Integrating LLMs with Diffusion Models\nThe integration of large language models (LLMs) and\ndiffusion models represents a promising new direction in ITS.\nPrevious works, such as [32], [33], [64], have primarily relied\non pre-trained CLIP [169] to encode textual information and\ngenerate outputs conditioned on these text feature representa-\ntions. However, CLIP exhibits inherent limitations in process-\ning long and complex sentences, which can negatively impact\nthe quality of generated outputs. LLMs, with their strong\ncapabilities in language understanding and knowledge-based\nreasoning, combined with the generative power of diffusion\nmodels, offer a compelling opportunity for enhanced perfor-\nmance. Recent studies, including MiniGPT-5 [183], which\nutilizes \u201cgenerative vokens\u201d to bridge LLMs and diffusion\nmodels, and EasyGen [184], which integrates these models\nvia a projection layer, have demonstrated the potential for\nproducing more realistic and reasonable outputs. Building on\nthese advancements, integrating LLMs and diffusion models\nfor various ITS tasks holds significant promise. In particular,\nin the field of traffic simulation, the use of LLMs for semantic\ncomprehension, reasoning, and automated decision-making\ncould lead to the generation of more realistic and contextually\naccurate driving images and videos. Moreover, another benefit\nof combining LLMs with diffusion models is their potential\nas a user interface. The natural language capabilities of LLMs\ncan provide a more intuitive means for users to interact with\nthese systems, enabling users to describe complex scenarios\nand receive tailored outputs without needing deep technical\nknowledge. This enhances the accessibility and usability of\ndiffusion models in ITS applications.\nB. Traffic Guidance with Prior Knowledge\nTraffic-related tasks often require reasoning that integrates\nboth scenario-specific features and domain-specific knowl-\nedge. Rather than relying on a large, and computationally\nexpensive diffusion model, the development of more efficient\ntraffic guidance that incorporate prior knowledge about traffic\nsystems can significantly enhance the generative process.\nExisting research has primarily focused on designing guidance\nto guide sampling in autonomous driving contexts, particularly\nin planning and decision-making. These guidance are often\nbased on reinforcement learning techniques or cost functions\ngrounded in traffic rules [46], [47]. Beyond autonomous driv-\ning, other domains within ITS, such as traffic flow predic-\ntion and traffic safety analysis, also rely heavily on domain\nknowledge. For instance, factors like the relationship between\ntraffic flow, urban population density, public holidays, weather\nconditions, and landmark locations are critical for accurate\ntraffic forecasting. By leveraging this extensive domain knowl-\nedge, task-specific guidance can be developed to improve the\nprediction of traffic patterns and congestion levels. Future\nresearch could focus on creating guidance that more effectively\nmine and utilize relevant prior knowledge for specific traffic-\nrelated tasks, thereby advancing the performance of diffusion\nmodels in these domains.\nC. Network Architectures Improvements\nThe architectures of diffusion models present substantial\nopportunities for improvement. U-Net [17], while demonstrat-\ning remarkable performance as a denoising network backbone\nacross various traffic-related tasks and being combinable with\nmethods such as GCNs to model spatial-temporal dependen-\ncies [36], still has considerable potential for further optimiza-\ntion. Recent advancements in transformer-based denoising net-\nworks, such as DiT [185], U-ViT [186], and their applications\nin diffusion models like Sora [8] and Stable Diffusion 3\n[187], have gained significant attention. Transformer-based\narchitectures excel in capturing long-range spatial-temporal\nrelationships and offer greater scalability. Therefore, leverag-\ning or refining transformer-based denoising networks holds\nsignificant potential for enhancing spatial-temporal-related\ntraffic applications, such as traffic flow forecasting and traffic\ntrajectory prediction. Furthermore, designing novel network\narchitectures specifically tailored to particular tasks within\nintelligent transportation systems, as backbones for diffusion\nmodels, presents a promising direction for future research.\nD. Fine-Tuning Diffusion Models\nLarge diffusion models, such as Stable Diffusion [38], pre-\ntrained on extensive image datasets, have demonstrated con-\nsiderable promise across various domains. Fine-tuning these\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n18\nmodels on traffic-specific data or for traffic-related condition\ncontrol can further enhance their applicability within ITS.\nRecent research has explored methods to fine-tune large pre-\ntrained diffusion models for more fine-grained control. For\nexample, ControlNet [162] adds spatial conditioning controls\nto large and pre-trained diffusion models through efficient fine-\ntuning techniques. Similarly, T2I-Adapter [188] learns simple\nand lightweight adapters to align internal knowledge in large\ndiffusion models with external control signals. Building on\nthese advancements, developing effective fine-tuning methods\ntailored to traffic data or traffic scenes holds the potential\nto significantly enhance the flexibility and control of these\nmodels in generating traffic-related outputs. These approaches\npromise to improve the models\u2019 utility in various ITS applica-\ntions, particularly in traffic simulation and incident detection.\nE. Enhancing Speed and Efficiency\nAlthough diffusion models have demonstrated significant\npotential in generating high-quality results, their computational\ncost and slow inference speeds remain major bottlenecks.\nTo enable real-time applications in ITS, such as autonomous\ndriving, future research should improve the efficiency of these\nmodels. Although recent advancements, including sampling\nacceleration [73], [189], [190], network architecture opti-\nmization [74], [38], and approach improvements [191], have\ncontributed to mitigating these challenges, further innovation is\nnecessary. Future research should explore the development of\nmore adaptive and lightweight network architectures, as well\nas parallel sampling techniques. Additionally, hybrid models\nthat integrate the strengths of diffusion models with faster,\nmore deterministic approaches might also prove valuable for\nreal-time applications in ITS.\nIX. CONCLUSION\nIn this paper, we provide a comprehensive review of dif-\nfusion models in ITS. We outline the theoretical founda-\ntions of diffusion models, discuss their key variants, and\ndemonstrate how they can effectively address the complex\nchallenges of ITS. Our review also highlights the advantages\nof diffusion models, especially in handling multi-modal, noisy,\nand incomplete traffic data. By investigating their current\napplications in ITS domains, including autonomous driving,\ntraffic simulation, traffic forecasting, and traffic safety, we\nhighlight the versatility and potential of diffusion models in\nenhancing various aspects of ITS. Additionally, we summarize\nseveral key research directions that warrant further investi-\ngation, including the integration of other approaches and the\ndevelopment of more efficient and scalable diffusion models\ntailored to various traffic-related tasks. We hope this review\nencourages further interdisciplinary collaboration, paving the\nway for the continued evolution of diffusion models as a\npivotal tool in future ITS.\nACKNOWLEDGMENTS\nThis study is supported by the National Natural Sci-\nence Foundation of China under Grant 52302379, Guang-\ndong Provincial Natural Science Foundation-General Project\nwith Grant 2024A1515011790, Guangzhou Basic and Ap-\nplied Basic Research Projects under Grants 2023A03J0106\nand\n2024A04J4290,\nGuangdong\nProvince\nGeneral\nUni-\nversities\nYouth\nInnovative\nTalents\nProject\nunder\nGrant\n2023KQNCX100, Guangzhou Municipal Science and Tech-\nnology Project 2023A03J0011, Nansha District Key R&D\nProject 2023ZD006.\nREFERENCES\n[1] J. Wootton, A. Garcia-Ortiz, and S. Amin, \u201cIntelligent transportation\nsystems: a global perspective,\u201d Mathematical and computer modelling,\nvol. 22, no. 4-7, pp. 259\u2013268, 1995.\n[2] M. Veres and M. Moussa, \u201cDeep learning for intelligent transportation\nsystems: A survey of emerging trends,\u201d IEEE Transactions on Intelli-\ngent transportation systems, vol. 21, no. 8, pp. 3152\u20133168, 2019.\n[3] J. Ye, J. Zhao, K. Ye, and C. Xu, \u201cHow to build a graph-based deep\nlearning architecture in traffic domain: A survey,\u201d IEEE Transactions\non Intelligent Transportation Systems, vol. 23, no. 5, pp. 3904\u20133924,\n2020.\n[4] H. Li, Y. Zhao, Z. Mao, Y. Qin, Z. Xiao, J. Feng, Y. Gu, W. Ju, X. Luo,\nand M. Zhang, \u201cA survey on graph neural networks in intelligent\ntransportation systems,\u201d arXiv preprint arXiv:2401.00713, 2024.\n[5] H. Lin, Y. Liu, S. Li, and X. Qu, \u201cHow generative adversarial networks\npromote the development of intelligent transportation systems: A\nsurvey,\u201d IEEE/CAA journal of automatica sinica, 2023.\n[6] G. Boquet, A. Morell, J. Serrano, and J. L. Vicario, \u201cA variational\nautoencoder solution for road traffic forecasting systems: Missing\ndata imputation, dimension reduction, model selection and anomaly\ndetection,\u201d Transportation Research Part C: Emerging Technologies,\nvol. 115, p. 102622, 2020.\n[7] F.-A. Croitoru, V. Hondru, R. T. Ionescu, and M. Shah, \u201cDiffusion\nmodels in vision: A survey,\u201d IEEE Transactions on Pattern Analysis\nand Machine Intelligence, 2023.\n[8] T.\nBrooks,\nB.\nPeebles,\nC.\nHolmes,\nW.\nDePue,\nY.\nGuo,\nL.\nJing,\nD.\nSchnurr,\nJ.\nTaylor,\nT.\nLuhman,\nE.\nLuhman,\nC.\nNg,\nR.\nWang,\nand\nA.\nRamesh,\n\u201cVideo\ngeneration\nmodels\nas\nworld\nsimulators,\u201d\n2024.\n[Online].\nAvailable:\nhttps:\n//openai.com/research/video-generation-models-as-world-simulators\n[9] R. A. Khalil, Z. Safelnasr, N. Yemane, M. Kedir, A. Shafiqurrahman,\nand N. Saeed, \u201cAdvanced learning technologies for intelligent trans-\nportation systems: Prospects and challenges,\u201d IEEE Open Journal of\nVehicular Technology, 2024.\n[10] H. Yan and Y. Li, \u201cA survey of generative ai for intelligent transporta-\ntion systems,\u201d arXiv preprint arXiv:2312.08248, 2023.\n[11] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang,\nB. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive survey\nof methods and applications,\u201d ACM Computing Surveys, vol. 56, no. 4,\npp. 1\u201339, 2023.\n[12] R. Jiang, G.-C. Zheng, T. Li, T.-R. Yang, J.-D. Wang, and X. Li,\n\u201cA survey of multimodal controllable diffusion models,\u201d Journal of\nComputer Science and Technology, vol. 39, no. 3, pp. 509\u2013541, 2024.\n[13] Y. Yang, M. Jin, H. Wen, C. Zhang, Y. Liang, L. Ma, Y. Wang, C. Liu,\nB. Yang, Z. Xu et al., \u201cA survey on diffusion models for time series\nand spatio-temporal data,\u201d arXiv preprint arXiv:2404.18886, 2024.\n[14] A. Kazerouni, E. K. Aghdam, M. Heidari, R. Azad, M. Fayyaz,\nI. Hacihaliloglu, and D. Merhof, \u201cDiffusion models in medical imaging:\nA comprehensive survey,\u201d Medical Image Analysis, vol. 88, p. 102846,\n2023.\n[15] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli,\n\u201cDeep unsupervised learning using nonequilibrium thermodynamics,\u201d\nin International conference on machine learning.\nPMLR, 2015, pp.\n2256\u20132265.\n[16] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic\nmodels,\u201d Advances in neural information processing systems, vol. 33,\npp. 6840\u20136851, 2020.\n[17] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional net-\nworks for biomedical image segmentation,\u201d in Medical image comput-\ning and computer-assisted intervention\u2013MICCAI 2015: 18th interna-\ntional conference, Munich, Germany, October 5-9, 2015, proceedings,\npart III 18.\nSpringer, 2015, pp. 234\u2013241.\n[18] Y. Song and S. Ermon, \u201cGenerative modeling by estimating gradients\nof the data distribution,\u201d Advances in neural information processing\nsystems, vol. 32, 2019.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n19\n[19] A. Hyv\u00a8arinen and P. Dayan, \u201cEstimation of non-normalized statistical\nmodels by score matching.\u201d Journal of Machine Learning Research,\nvol. 6, no. 4, 2005.\n[20] R. M. Neal, \u201cMcmc using hamiltonian dynamics,\u201d arXiv preprint\narXiv:1206.1901, 2012.\n[21] G. O. Roberts and R. L. Tweedie, \u201cExponential convergence of langevin\ndistributions and their discrete approximations,\u201d 1996.\n[22] Y. Song, S. Garg, J. Shi, and S. Ermon, \u201cSliced score matching: A\nscalable approach to density and score estimation,\u201d in Uncertainty in\nArtificial Intelligence.\nPMLR, 2020, pp. 574\u2013584.\n[23] P. Vincent, \u201cA connection between score matching and denoising\nautoencoders,\u201d Neural computation, vol. 23, no. 7, pp. 1661\u20131674,\n2011.\n[24] Y. Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon,\nand B. Poole, \u201cScore-based generative modeling through stochastic\ndifferential equations,\u201d arXiv preprint arXiv:2011.13456, 2020.\n[25] K. It\u02c6o, On stochastic differential equations.\nAmerican Mathematical\nSoc., 1951, no. 4.\n[26] B. D. Anderson, \u201cReverse-time diffusion equation models,\u201d Stochastic\nProcesses and their Applications, vol. 12, no. 3, pp. 313\u2013326, 1982.\n[27] T. Gu, G. Chen, J. Li, C. Lin, Y. Rao, J. Zhou, and J. Lu, \u201cStochastic\ntrajectory prediction via motion indeterminacy diffusion,\u201d in Proceed-\nings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2022, pp. 17 113\u201317 122.\n[28] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov\net al., \u201cMotiondiffuser: Controllable multi-agent motion prediction\nusing diffusion,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2023, pp. 9644\u20139653.\n[29] K. Chen, X. Chen, Z. Yu, M. Zhu, and H. Yang, \u201cEquidiff: A\nconditional equivariant diffusion model for trajectory prediction,\u201d in\n2023 IEEE 26th International Conference on Intelligent Transportation\nSystems (ITSC).\nIEEE, 2023, pp. 746\u2013751.\n[30] E. Pronovost, M. R. Ganesina, N. Hendy, Z. Wang, A. Morales,\nK. Wang, and N. Roy, \u201cScenario diffusion: Controllable driving\nscenario generation with diffusion,\u201d Advances in Neural Information\nProcessing Systems, vol. 36, pp. 68 873\u201368 894, 2023.\n[31] Y. Ji, Z. Chen, E. Xie, L. Hong, X. Liu, Z. Liu, T. Lu, Z. Li, and P. Luo,\n\u201cDdp: Diffusion model for dense visual prediction,\u201d in Proceedings of\nthe IEEE/CVF International Conference on Computer Vision, 2023, pp.\n21 741\u201321 752.\n[32] Y. Wen, Y. Zhao, Y. Liu, F. Jia, Y. Wang, C. Luo, C. Zhang, T. Wang,\nX. Sun, and X. Zhang, \u201cPanacea: Panoramic and controllable video\ngeneration for autonomous driving,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2024, pp.\n6902\u20136912.\n[33] X. Li, Y. Zhang, and X. Ye, \u201cDrivingdiffusion: Layout-guided multi-\nview driving scene video generation with latent diffusion model,\u201d arXiv\npreprint arXiv:2310.07771, 2023.\n[34] Y. Zhu, Y. Ye, S. Zhang, X. Zhao, and J. Yu, \u201cDifftraj: Generating\ngps trajectory with diffusion probabilistic model,\u201d Advances in Neural\nInformation Processing Systems, vol. 36, pp. 65 168\u201365 188, 2023.\n[35] T. Wei, Y. Lin, S. Guo, Y. Lin, Y. Huang, C. Xiang, Y. Bai,\nM. Ya, and H. Wan, \u201cDiff-rntraj: A structure-aware diffusion model\nfor road network-constrained trajectory generation,\u201d arXiv preprint\narXiv:2402.07369, 2024.\n[36] H. Wen, Y. Lin, Y. Xia, H. Wan, Q. Wen, R. Zimmermann, and\nY. Liang, \u201cDiffstg: Probabilistic spatio-temporal graph forecasting with\ndenoising diffusion models,\u201d in Proceedings of the 31st ACM Interna-\ntional Conference on Advances in Geographic Information Systems,\n2023, pp. 1\u201312.\n[37] A. Vaswani, \u201cAttention is all you need,\u201d Advances in Neural Informa-\ntion Processing Systems, 2017.\n[38] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,\n\u201cHigh-resolution image synthesis with latent diffusion models,\u201d in\nProceedings of the IEEE/CVF conference on computer vision and\npattern recognition, 2022, pp. 10 684\u201310 695.\n[39] W. Wu, Y. Zhao, M. Z. Shou, H. Zhou, and C. Shen, \u201cDiffumask: Syn-\nthesizing images with pixel-level annotations for semantic segmentation\nusing diffusion models,\u201d in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2023, pp. 1206\u20131217.\n[40] W. Wu, Y. Zhao, H. Chen, Y. Gu, R. Zhao, Y. He, H. Zhou, M. Z.\nShou, and C. Shen, \u201cDatasetdm: Synthesizing data with perception\nannotations using diffusion models,\u201d Advances in Neural Information\nProcessing Systems, vol. 36, pp. 54 683\u201354 695, 2023.\n[41] C. Zhang, Y. Zhang, Q. Shao, B. Li, Y. Lv, X. Piao, and B. Yin, \u201cChat-\ntraffc: Text-to-traffic generation via diffusion model,\u201d arXiv preprint\narXiv:2311.16203, 2023.\n[42] J. Fang, L.-l. Li, J. Zhou, J. Xiao, H. Yu, C. Lv, J. Xue, and T.-\nS. Chua, \u201cAbductive ego-view accident video understanding for safe\ndriving perception,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 22 030\u201322 040.\n[43] Y. Zheng, L. Zhong, S. Wang, Y. Yang, W. Gu, J. Zhang, and J. Wang,\n\u201cDiffuflow: Robust fine-grained urban flow inference with denoising\ndiffusion model,\u201d in Proceedings of the 32nd ACM International\nConference on Information and Knowledge Management, 2023, pp.\n3505\u20133513.\n[44] Y. Lin, H. Wan, J. Hu, S. Guo, B. Yang, Y. Lin, and C. S. Jensen,\n\u201cOrigin-destination travel time oracle for map-based services,\u201d Pro-\nceedings of the ACM on Management of Data, vol. 1, no. 3, pp. 1\u201327,\n2023.\n[45] P. Dhariwal and A. Nichol, \u201cDiffusion models beat gans on image\nsynthesis,\u201d Advances in neural information processing systems, vol. 34,\npp. 8780\u20138794, 2021.\n[46] M.\nJanner,\nY.\nDu,\nJ.\nB.\nTenenbaum,\nand\nS.\nLevine,\n\u201cPlan-\nning with diffusion for flexible behavior synthesis,\u201d arXiv preprint\narXiv:2205.09991, 2022.\n[47] J. Carvalho, A. T. Le, M. Baierl, D. Koert, and J. Peters, \u201cMotion\nplanning diffusion: Learning and planning of robot motions with\ndiffusion models,\u201d in 2023 IEEE/RSJ International Conference on\nIntelligent Robots and Systems (IROS).\nIEEE, 2023, pp. 1916\u20131923.\n[48] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray,\nand M. Pavone, \u201cGuided conditional diffusion for controllable traffic\nsimulation,\u201d in 2023 IEEE International Conference on Robotics and\nAutomation (ICRA).\nIEEE, 2023, pp. 3560\u20133566.\n[49] Z. Zhong, D. Rempe, Y. Chen, B. Ivanovic, Y. Cao, D. Xu, M. Pavone,\nand B. Ray, \u201cLanguage-guided traffic simulation via scene-level diffu-\nsion,\u201d in Conference on Robot Learning.\nPMLR, 2023, pp. 144\u2013177.\n[50] M. Niedoba, J. Lavington, Y. Liu, V. Lioutas, J. Sefas, X. Liang,\nD. Green, S. Dabiri, B. Zwartsenberg, A. Scibior et al., \u201cA diffusion-\nmodel of joint interactive navigation,\u201d Advances in Neural Information\nProcessing Systems, vol. 36, 2024.\n[51] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d arXiv\npreprint arXiv:2207.12598, 2022.\n[52] A. Ajay, Y. Du, A. Gupta, J. Tenenbaum, T. Jaakkola, and P. Agrawal,\n\u201cIs conditional generative modeling all you need for decision-making?\u201d\narXiv preprint arXiv:2211.15657, 2022.\n[53] D. Rempe, Z. Luo, X. Bin Peng, Y. Yuan, K. Kitani, K. Kreis, S. Fidler,\nand O. Litany, \u201cTrace and pace: Controllable pedestrian animation\nvia guided trajectory diffusion,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2023, pp.\n13 756\u201313 766.\n[54] A. Hu, L. Russell, H. Yeo, Z. Murez, G. Fedoseev, A. Kendall,\nJ. Shotton, and G. Corrado, \u201cGaia-1: A generative world model for\nautonomous driving,\u201d arXiv preprint arXiv:2309.17080, 2023.\n[55] L. Zhang, Y. Xiong, Z. Yang, S. Casas, R. Hu, and R. Urtasun,\n\u201cLearning unsupervised world models for autonomous driving via\ndiscrete diffusion,\u201d arXiv preprint arXiv:2311.01017, 2023.\n[56] P. Esser, R. Rombach, and B. Ommer, \u201cTaming transformers for\nhigh-resolution image synthesis,\u201d in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2021, pp.\n12 873\u201312 883.\n[57] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler,\nand K. Kreis, \u201cAlign your latents: High-resolution video synthesis with\nlatent diffusion models,\u201d in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2023, pp. 22 563\u201322 575.\n[58] L. Balasubramanian, J. Wurst, R. Egolf, M. Botsch, W. Utschick, and\nK. Deng, \u201cScenediffusion: Conditioned latent diffusion models for\ntraffic scene prediction,\u201d in 2023 IEEE 26th International Conference\non Intelligent Transportation Systems (ITSC).\nIEEE, 2023, pp. 3914\u2013\n3921.\n[59] S. Wang, G. Sun, F. Ma, T. Hu, Y. Song, L. Zhu, and M. Liu,\n\u201cDragtraffic: A non-expert interactive and point-based controllable\ntraffic scene generation framework,\u201d arXiv preprint arXiv:2404.12624,\n2024.\n[60] Y. Wang, J. He, L. Fan, H. Li, Y. Chen, and Z. Zhang, \u201cDriving into the\nfuture: Multiview visual forecasting and planning with world model for\nautonomous driving,\u201d in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2024, pp. 14 749\u201314 759.\n[61] J. Yang, S. Gao, Y. Qiu, L. Chen, T. Li, B. Dai, K. Chitta, P. Wu,\nJ. Zeng, P. Luo et al., \u201cGeneralized predictive model for autonomous\ndriving,\u201d in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 2024, pp. 14 662\u201314 672.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n20\n[62] X. Wang, Z. Zhu, G. Huang, X. Chen, and J. Lu, \u201cDrivedreamer:\nTowards real-world-driven world models for autonomous driving,\u201d\narXiv preprint arXiv:2309.09777, 2023.\n[63] J. Lu, Z. Huang, J. Zhang, Z. Yang, and L. Zhang, \u201cWovogen: World\nvolume-aware diffusion for controllable multi-camera driving scene\ngeneration,\u201d arXiv preprint arXiv:2312.02934, 2023.\n[64] H. Ran, V. Guizilini, and Y. Wang, \u201cTowards realistic scene gener-\nation with lidar diffusion models,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2024, pp.\n14 738\u201314 748.\n[65] J. L. Elman, \u201cFinding structure in time,\u201d Cognitive science, vol. 14,\nno. 2, pp. 179\u2013211, 1990.\n[66] S. Hochreiter, \u201cLong short-term memory,\u201d Neural Computation MIT-\nPress, 1997.\n[67] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,\n\u201cThe graph neural network model,\u201d IEEE transactions on neural\nnetworks, vol. 20, no. 1, pp. 61\u201380, 2008.\n[68] T. N. Kipf and M. Welling, \u201cSemi-supervised classification with graph\nconvolutional networks,\u201d arXiv preprint arXiv:1609.02907, 2016.\n[69] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,\nS. Ozair, A. Courville, and Y. Bengio, \u201cGenerative adversarial nets,\u201d\nAdvances in neural information processing systems, vol. 27, 2014.\n[70] A. Creswell, T. White, V. Dumoulin, K. Arulkumaran, B. Sengupta,\nand A. A. Bharath, \u201cGenerative adversarial networks: An overview,\u201d\nIEEE signal processing magazine, vol. 35, no. 1, pp. 53\u201365, 2018.\n[71] D. P. Kingma, \u201cAuto-encoding variational bayes,\u201d arXiv preprint\narXiv:1312.6114, 2013.\n[72] D. J. Rezende, S. Mohamed, and D. Wierstra, \u201cStochastic backprop-\nagation and approximate inference in deep generative models,\u201d in\nInternational conference on machine learning.\nPMLR, 2014, pp.\n1278\u20131286.\n[73] J. Song, C. Meng, and S. Ermon, \u201cDenoising diffusion implicit mod-\nels,\u201d arXiv preprint arXiv:2010.02502, 2020.\n[74] W. Mao, C. Xu, Q. Zhu, S. Chen, and Y. Wang, \u201cLeapfrog diffusion\nmodel for stochastic trajectory prediction,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2023, pp. 5517\u20135526.\n[75] T. Karras, M. Aittala, T. Aila, and S. Laine, \u201cElucidating the design\nspace of diffusion-based generative models,\u201d Advances in neural infor-\nmation processing systems, vol. 35, pp. 26 565\u201326 577, 2022.\n[76] W. Harvey, S. Naderiparizi, V. Masrani, C. Weilbach, and F. Wood,\n\u201cFlexible diffusion modeling of long videos,\u201d Advances in Neural\nInformation Processing Systems, vol. 35, pp. 27 953\u201327 965, 2022.\n[77] J. Austin, D. D. Johnson, J. Ho, D. Tarlow, and R. Van Den Berg,\n\u201cStructured denoising diffusion models in discrete state-spaces,\u201d Ad-\nvances in Neural Information Processing Systems, vol. 34, pp. 17 981\u2013\n17 993, 2021.\n[78] X. Han, H. Zheng, and M. Zhou, \u201cCard: Classification and regression\ndiffusion models,\u201d Advances in Neural Information Processing Systems,\nvol. 35, pp. 18 100\u201318 115, 2022.\n[79] S. Chen, P. Sun, Y. Song, and P. Luo, \u201cDiffusiondet: Diffusion model\nfor object detection,\u201d in Proceedings of the IEEE/CVF international\nconference on computer vision, 2023, pp. 19 830\u201319 843.\n[80] S. Shao, Z. Zhao, B. Li, T. Xiao, G. Yu, X. Zhang, and J. Sun,\n\u201cCrowdhuman: A benchmark for detecting human in a crowd,\u201d arXiv\npreprint arXiv:1805.00123, 2018.\n[81] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan,\nP. Doll\u00b4ar, and C. L. Zitnick, \u201cMicrosoft coco: Common objects in\ncontext,\u201d in Computer Vision\u2013ECCV 2014: 13th European Conference,\nZurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13.\nSpringer, 2014, pp. 740\u2013755.\n[82] Y. Wang, R. Gao, K. Chen, K. Zhou, Y. Cai, L. Hong, Z. Li, L. Jiang,\nD.-Y. Yeung, Q. Xu et al., \u201cDetdiffusion: Synergizing generative and\nperceptive models for enhanced data generation and perception,\u201d in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2024, pp. 7246\u20137255.\n[83] J. Zou, K. Tian, Z. Zhu, Y. Ye, and X. Wang, \u201cDiffbev: Conditional\ndiffusion model for bird\u2019s eye view perception,\u201d in Proceedings of the\nAAAI Conference on Artificial Intelligence, vol. 38, no. 7, 2024, pp.\n7846\u20137854.\n[84] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,\nA. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, \u201cnuscenes: A\nmultimodal dataset for autonomous driving,\u201d in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition,\n2020, pp. 11 621\u201311 631.\n[85] B. Zhou, H. Zhao, X. Puig, S. Fidler, A. Barriuso, and A. Torralba,\n\u201cScene parsing through ade20k dataset,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2017, pp. 633\u2013\n641.\n[86] N. Silberman, D. Hoiem, P. Kohli, and R. Fergus, \u201cIndoor segmentation\nand support inference from rgbd images,\u201d in Computer Vision\u2013ECCV\n2012: 12th European Conference on Computer Vision, Florence, Italy,\nOctober 7-13, 2012, Proceedings, Part V 12.\nSpringer, 2012, pp.\n746\u2013760.\n[87] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, \u201cVision meets robotics:\nThe kitti dataset,\u201d The International Journal of Robotics Research,\nvol. 32, no. 11, pp. 1231\u20131237, 2013.\n[88] W. Zhao, Y. Rao, Z. Liu, B. Liu, J. Zhou, and J. Lu, \u201cUnleashing\ntext-to-image diffusion models for visual perception,\u201d in Proceedings\nof the IEEE/CVF International Conference on Computer Vision, 2023,\npp. 5729\u20135739.\n[89] L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg, \u201cModeling\ncontext in referring expressions,\u201d in Computer Vision\u2013ECCV 2016:\n14th European Conference, Amsterdam, The Netherlands, October 11-\n14, 2016, Proceedings, Part II 14.\nSpringer, 2016, pp. 69\u201385.\n[90] S. Chen, E. Yu, J. Li, and W. Tao, \u201cDelving into the trajectory long-tail\ndistribution for muti-object tracking,\u201d in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition, 2024, pp.\n19 341\u201319 351.\n[91] P. Dendorfer, \u201cMot20: A benchmark for multi object tracking in\ncrowded scenes,\u201d arXiv preprint arXiv:2003.09003, 2020.\n[92] R. Luo, Z. Song, L. Ma, J. Wei, W. Yang, and M. Yang, \u201cDiffusiontrack:\nDiffusion model for multi-object tracking,\u201d in Proceedings of the AAAI\nConference on Artificial Intelligence, vol. 38, no. 5, 2024, pp. 3991\u2013\n3999.\n[93] F. Xie, Z. Wang, and C. Ma, \u201cDiffusiontrack: Point set diffusion model\nfor visual object tracking,\u201d in Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, 2024, pp. 19 113\u201319 124.\n[94] L. Huang, X. Zhao, and K. Huang, \u201cGot-10k: A large high-diversity\nbenchmark for generic object tracking in the wild,\u201d IEEE transactions\non pattern analysis and machine intelligence, vol. 43, no. 5, pp. 1562\u2013\n1577, 2019.\n[95] H. Fan, L. Lin, F. Yang, P. Chu, G. Deng, S. Yu, H. Bai, Y. Xu,\nC. Liao, and H. Ling, \u201cLasot: A high-quality benchmark for large-scale\nsingle object tracking,\u201d in Proceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2019, pp. 5374\u20135383.\n[96] S. Luo and W. Hu, \u201cDiffusion probabilistic models for 3d point cloud\ngeneration,\u201d in Proceedings of the IEEE/CVF conference on computer\nvision and pattern recognition, 2021, pp. 2837\u20132845.\n[97] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang,\nZ. Li, S. Savarese, M. Savva, S. Song, H. Su et al., \u201cShapenet:\nAn\ninformation-rich\n3d\nmodel\nrepository,\u201d\narXiv\npreprint\narXiv:1512.03012, 2015.\n[98] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisser-\nman, \u201cThe pascal visual object classes (voc) challenge,\u201d International\njournal of computer vision, vol. 88, pp. 303\u2013338, 2010.\n[99] M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Be-\nnenson, U. Franke, S. Roth, and B. Schiele, \u201cThe cityscapes dataset\nfor semantic urban scene understanding,\u201d in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2016, pp.\n3213\u20133223.\n[100] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, \u201cLearning\nsocial etiquette: Human trajectory understanding in crowded scenes,\u201d in\nComputer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam,\nThe Netherlands, October 11-14, 2016, Proceedings, Part VIII 14.\nSpringer, 2016, pp. 549\u2013565.\n[101] S. Pellegrini, A. Ess, and L. Van Gool, \u201cImproving data association by\njoint modeling of pedestrian trajectories and groupings,\u201d in Computer\nVision\u2013ECCV 2010: 11th European Conference on Computer Vision,\nHeraklion, Crete, Greece, September 5-11, 2010, Proceedings, Part I\n11.\nSpringer, 2010, pp. 452\u2013465.\n[102] A. Lerner, Y. Chrysanthou, and D. Lischinski, \u201cCrowds by example,\u201d\nin Computer graphics forum, vol. 26, no. 3.\nWiley Online Library,\n2007, pp. 655\u2013664.\n[103] I. Bae, Y.-J. Park, and H.-G. Jeon, \u201cSingulartrajectory: Universal\ntrajectory predictor using diffusion model,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 17 890\u201317 901.\n[104] C. Liu, S. He, H. Liu, and J. Chen, \u201cIntention-aware denoising diffusion\nmodel for trajectory prediction,\u201d arXiv preprint arXiv:2403.09190,\n2024.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n21\n[105] K. Lv, L. Yuan, and X. Ni, \u201cLearning autoencoder diffusion models\nof pedestrian group relationships for multimodal trajectory prediction,\u201d\nIEEE Transactions on Instrumentation and Measurement, 2024.\n[106] R. Li, C. Li, D. Ren, G. Chen, Y. Yuan, and G. Wang, \u201cBcdiff: Bidi-\nrectional consistent diffusion for instantaneous trajectory prediction,\u201d\nAdvances in Neural Information Processing Systems, vol. 36, 2024.\n[107] S. Ettinger, S. Cheng, B. Caine, C. Liu, H. Zhao, S. Pradhan, Y. Chai,\nB. Sapp, C. R. Qi, Y. Zhou et al., \u201cLarge scale interactive motion fore-\ncasting for autonomous driving: The waymo open motion dataset,\u201d in\nProceedings of the IEEE/CVF International Conference on Computer\nVision, 2021, pp. 9710\u20139719.\n[108] M.-F. Chang, J. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,\nD. Wang, P. Carr, S. Lucey, D. Ramanan et al., \u201cArgoverse: 3d tracking\nand forecasting with rich maps,\u201d in Proceedings of the IEEE/CVF\nconference on computer vision and pattern recognition, 2019, pp.\n8748\u20138757.\n[109] U.S. Department of Transportation Federal Highway Administration,\n\u201cNext Generation Simulation (NGSIM) Vehicle Trajectories and Sup-\nporting Data,\u201d 2016, [Dataset]. Provided by ITS DataHub through\nData.transportation.gov. Accessed: 2024-05-21.\n[110] Y. Yao, Y. Liu, X. Dai, S. Chen, and Y. Lv, \u201cA graph-based scene\nencoder for vehicle trajectory prediction using the diffusion model,\u201d\nin 2023 International Annual Conference on Complex Systems and\nIntelligent Science (CSIS-IAC).\nIEEE, 2023, pp. 981\u2013986.\n[111] B. Wilson, W. Qi, T. Agarwal, J. Lambert, J. Singh, S. Khandelwal,\nB. Pan, R. Kumar, A. Hartnett, J. K. Pontes et al., \u201cArgoverse 2: Next\ngeneration datasets for self-driving perception and forecasting,\u201d arXiv\npreprint arXiv:2301.00493, 2023.\n[112] J. Fu, A. Kumar, O. Nachum, G. Tucker, and S. Levine, \u201cD4rl:\nDatasets for deep data-driven reinforcement learning,\u201d arXiv preprint\narXiv:2004.07219, 2020.\n[113] B. Yang, H. Su, N. Gkanatsios, T.-W. Ke, A. Jain, J. Schneider, and\nK. Fragkiadaki, \u201cDiffusion-es: Gradient-free planning with diffusion\nfor autonomous and instruction-guided driving,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 15 342\u201315 353.\n[114] H. Caesar, J. Kabzan, K. S. Tan, W. K. Fong, E. Wolff, A. Lang,\nL. Fletcher, O. Beijbom, and S. Omari, \u201cnuplan: A closed-loop ml-\nbased planning benchmark for autonomous vehicles,\u201d arXiv preprint\narXiv:2106.11810, 2021.\n[115] W. Zhan, L. Sun, D. Wang, H. Shi, A. Clausse, M. Naumann, J. Kum-\nmerle, H. Konigshof, C. Stiller, A. de La Fortelle et al., \u201cInteraction\ndataset: An international, adversarial and cooperative motion dataset\nin interactive driving scenarios with semantic maps,\u201d arXiv preprint\narXiv:1910.03088, 2019.\n[116] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, \u201cCarla:\nAn open urban driving simulator,\u201d in Conference on robot learning.\nPMLR, 2017, pp. 1\u201316.\n[117] G. Zhao, X. Wang, Z. Zhu, X. Chen, G. Huang, X. Bao, and X. Wang,\n\u201cDrivedreamer-2: Llm-enhanced world models for diverse driving\nvideo generation,\u201d arXiv preprint arXiv:2403.06845, 2024.\n[118] Y. Liao, J. Xie, and A. Geiger, \u201cKitti-360: A novel dataset and\nbenchmarks for urban scene understanding in 2d and 3d,\u201d IEEE\nTransactions on Pattern Analysis and Machine Intelligence, vol. 45,\nno. 3, pp. 3292\u20133310, 2022.\n[119] Z. Zhou, J. Ding, Y. Liu, D. Jin, and Y. Li, \u201cTowards generative mod-\neling of urban flow through knowledge-enhanced denoising diffusion,\u201d\nin Proceedings of the 31st ACM International Conference on Advances\nin Geographic Information Systems, 2023, pp. 1\u201312.\n[120] C. Rong, J. Ding, Z. Liu, and Y. Li, \u201cComplexity-aware large scale\norigin-destination network generation via diffusion model,\u201d arXiv\npreprint arXiv:2306.04873, 2023.\n[121] C. Chen, K. Petty, A. Skabardonis, P. Varaiya, and Z. Jia, \u201cFreeway\nperformance measurement system: mining loop detector data,\u201d Trans-\nportation research record, vol. 1748, no. 1, pp. 96\u2013102, 2001.\n[122] L. Lin, D. Shi, A. Han, and J. Gao, \u201cSpecstg: A fast spectral diffusion\nframework for probabilistic spatio-temporal traffic forecasting,\u201d arXiv\npreprint arXiv:2401.08119, 2024.\n[123] X. Xu, Y. Wei, P. Wang, X. Luo, F. Zhou, and G. Trajcevski, \u201cDiffusion\nprobabilistic modeling for fine-grained urban traffic flow inference with\nrelaxed structural constraint,\u201d in ICASSP 2023-2023 IEEE International\nConference on Acoustics, Speech and Signal Processing (ICASSP).\nIEEE, 2023, pp. 1\u20135.\n[124] M. Lablack, S. Yu, S. Xu, and Y. Shen, \u201cLong-sequence model for\ntraffic forecasting in suboptimal situation,\u201d in Proceedings of the 18th\nWorkshop on Mobility in the Evolving Internet Architecture, 2023, pp.\n25\u201330.\n[125] R. Jiang, D. Yin, Z. Wang, Y. Wang, J. Deng, H. Liu, Z. Cai,\nJ. Deng, X. Song, and R. Shibasaki, \u201cDl-traff: Survey and benchmark\nof deep learning models for urban traffic prediction,\u201d in Proceedings of\nthe 30th ACM international conference on information & knowledge\nmanagement, 2021, pp. 4515\u20134525.\n[126] P. Chi and X. Ma, \u201cDifforecast: Image generation based highway\ntraffic forecasting with diffusion model,\u201d in 2023 IEEE International\nConference on Big Data (BigData).\nIEEE, 2023, pp. 608\u2013615.\n[127] C. Li, G. Feng, Y. Li, R. Liu, Q. Miao, and L. Chang, \u201cDifftad:\nDenoising diffusion probabilistic models for vehicle trajectory anomaly\ndetection,\u201d Knowledge-Based Systems, vol. 286, p. 111387, 2024.\n[128] C. Yan, S. Zhang, Y. Liu, G. Pang, and W. Wang, \u201cFeature prediction\ndiffusion model for video anomaly detection,\u201d in Proceedings of the\nIEEE/CVF International Conference on Computer Vision, 2023, pp.\n5527\u20135537.\n[129] C. Lu, J. Shi, and J. Jia, \u201cAbnormal event detection at 150 fps\nin matlab,\u201d in Proceedings of the IEEE international conference on\ncomputer vision, 2013, pp. 2720\u20132727.\n[130] J. Zhao, W. Zhao, B. Deng, Z. Wang, F. Zhang, W. Zheng, W. Cao,\nJ. Nan, Y. Lian, and A. F. Burke, \u201cAutonomous driving system: A\ncomprehensive survey,\u201d Expert Systems with Applications, p. 122836,\n2023.\n[131] L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, \u201cEnd-\nto-end autonomous driving: Challenges and frontiers,\u201d arXiv preprint\narXiv:2306.16927, 2023.\n[132] S. Teng, X. Hu, P. Deng, B. Li, Y. Li, Y. Ai, D. Yang, L. Li,\nZ. Xuanyuan, F. Zhu et al., \u201cMotion planning for autonomous driving:\nThe state of the art and future perspectives,\u201d IEEE Transactions on\nIntelligent Vehicles, vol. 8, no. 6, pp. 3692\u20133711, 2023.\n[133] S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu, \u201cA survey\nof deep learning techniques for autonomous driving,\u201d Journal of field\nrobotics, vol. 37, no. 3, pp. 362\u2013386, 2020.\n[134] E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda, \u201cA survey of\nautonomous driving: Common practices and emerging technologies,\u201d\nIEEE access, vol. 8, pp. 58 443\u201358 469, 2020.\n[135] D. Baranchuk, I. Rubachev, A. Voynov, V. Khrulkov, and A. Babenko,\n\u201cLabel-efficient semantic segmentation with diffusion models,\u201d arXiv\npreprint arXiv:2112.03126, 2021.\n[136] D.-T. Le, H. Shi, J. Cai, and H. Rezatofighi, \u201cDiffuser: Diffusion\nmodel for robust multi-sensor fusion in 3d object detection and bev\nsegmentation,\u201d arXiv preprint arXiv:2404.04629, 2024.\n[137] J. Li, B. Li, Z. Tu, X. Liu, Q. Guo, F. Juefei-Xu, R. Xu, and H. Yu,\n\u201cLight the night: A multi-condition diffusion framework for unpaired\nlow-light enhancement in autonomous driving,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2024, pp. 15 205\u201315 215.\n[138] A. Nachkov, M. Danelljan, D. P. Paudel, and L. Van Gool,\n\u201cDiffusion-based particle-detr for bev perception,\u201d arXiv preprint\narXiv:2312.11578, 2023.\n[139] W. Luo, J. Xing, A. Milan, X. Zhang, W. Liu, and T.-K. Kim, \u201cMultiple\nobject tracking: A literature review,\u201d Artificial intelligence, vol. 293, p.\n103448, 2021.\n[140] J. Sun, W. Nie, Z. Yu, Z. M. Mao, and C. Xiao, \u201cPointdp: Diffusion-\ndriven purification against adversarial attacks on 3d point cloud recog-\nnition,\u201d arXiv preprint arXiv:2208.09801, 2022.\n[141] Y. Huang, J. Du, Z. Yang, Z. Zhou, L. Zhang, and H. Chen, \u201cA\nsurvey on trajectory-prediction methods for autonomous driving,\u201d IEEE\nTransactions on Intelligent Vehicles, vol. 7, no. 3, pp. 652\u2013674, 2022.\n[142] R. Huang, H. Xue, M. Pagnucco, F. Salim, and Y. Song, \u201cMultimodal\ntrajectory prediction: A survey,\u201d arXiv preprint arXiv:2302.10463,\n2023.\n[143] Y. Tang, H. He, Y. Wang, and Y. Wu, \u201cUtilizing a diffusion model\nfor pedestrian trajectory prediction in semi-open autonomous driving\nenvironments,\u201d IEEE Sensors Journal, 2024.\n[144] Y. Yang, P. Zhu, M. Qi, and H. Ma, \u201cUncovering the human motion\npattern: Pattern memory-based diffusion model for trajectory predic-\ntion,\u201d arXiv preprint arXiv:2401.02916, 2024.\n[145] J. Sun, Y. Li, L. Chai, H.-S. Fang, Y.-L. Li, and C. Lu, \u201cHuman tra-\njectory prediction with momentary observation,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 6467\u20136476.\n[146] T. Westny, B. Olofsson, and E. Frisk, \u201cDiffusion-based environment-\naware trajectory prediction,\u201d arXiv preprint arXiv:2403.11643, 2024.\n[147] C. Badue, R. Guidolini, R. V. Carneiro, P. Azevedo, V. B. Cardoso,\nA. Forechi, L. Jesus, R. Berriel, T. M. Paixao, F. Mutz et al., \u201cSelf-\ndriving cars: A survey,\u201d Expert systems with applications, vol. 165, p.\n113816, 2021.\nIEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE, VOL. XX, NO. X, XXX 2024\n22\n[148] B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yo-\ngamani, and P. P\u00b4erez, \u201cDeep reinforcement learning for autonomous\ndriving: A survey,\u201d IEEE Transactions on Intelligent Transportation\nSystems, vol. 23, no. 6, pp. 4909\u20134926, 2021.\n[149] S. Aradi, \u201cSurvey of deep reinforcement learning for motion planning\nof autonomous vehicles,\u201d IEEE Transactions on Intelligent Transporta-\ntion Systems, vol. 23, no. 2, pp. 740\u2013759, 2020.\n[150] Z. Zhu, H. Zhao, H. He, Y. Zhong, S. Zhang, Y. Yu, and W. Zhang,\n\u201cDiffusion models for reinforcement learning: A survey,\u201d arXiv preprint\narXiv:2311.01223, 2023.\n[151] Z. Wang, J. J. Hunt, and M. Zhou, \u201cDiffusion policies as an expres-\nsive policy class for offline reinforcement learning,\u201d arXiv preprint\narXiv:2208.06193, 2022.\n[152] J. Liu, P. Hang, X. Zhao, J. Wang, and J. Sun, \u201cDdm-lag: A diffusion-\nbased decision-making model for autonomous vehicles with lagrangian\nsafety enhancement,\u201d arXiv preprint arXiv:2401.03629, 2024.\n[153] J. Nguyen, S. T. Powers, N. Urquhart, T. Farrenkopf, and M. Guck-\nert, \u201cAn overview of agent-based traffic simulators,\u201d Transportation\nresearch interdisciplinary perspectives, vol. 12, p. 100486, 2021.\n[154] D. Chen, M. Zhu, H. Yang, X. Wang, and Y. Wang, \u201cData-driven traffic\nsimulation: A comprehensive review,\u201d IEEE Transactions on Intelligent\nVehicles, 2024.\n[155] W. Ding, C. Xu, M. Arief, H. Lin, B. Li, and D. Zhao, \u201cA survey on\nsafety-critical driving scenario generation\u2014a methodological perspec-\ntive,\u201d IEEE Transactions on Intelligent Transportation Systems, vol. 24,\nno. 7, pp. 6971\u20136988, 2023.\n[156] P. A. Lopez, M. Behrisch, L. Bieker-Walz, J. Erdmann, Y.-P. Fl\u00a8otter\u00a8od,\nR. Hilbrich, L. L\u00a8ucken, J. Rummel, P. Wagner, and E. Wie\u00dfner,\n\u201cMicroscopic traffic simulation using sumo,\u201d in 2018 21st international\nconference on intelligent transportation systems (ITSC).\nIEEE, 2018,\npp. 2575\u20132582.\n[157] O. Maler and D. Nickovic, \u201cMonitoring temporal properties of con-\ntinuous signals,\u201d in International symposium on formal techniques in\nreal-time and fault-tolerant systems.\nSpringer, 2004, pp. 152\u2013166.\n[158] Z. Huang, Z. Zhang, A. Vaidya, Y. Chen, C. Lv, and J. F. Fisac,\n\u201cVersatile scene-consistent traffic scenario generation as optimization\nwith diffusion,\u201d arXiv preprint arXiv:2404.02524, 2024.\n[159] C. Yang, A. X. Tian, D. Chen, T. Shi, and A. Heydarian, \u201cWcdt:\nWorld-centric diffusion transformer for traffic scene generation,\u201d arXiv\npreprint arXiv:2404.02082, 2024.\n[160] S. Riedmaier, T. Ponn, D. Ludwig, B. Schick, and F. Diermeyer,\n\u201cSurvey on scenario-based safety assessment of automated vehicles,\u201d\nIEEE access, vol. 8, pp. 87 456\u201387 477, 2020.\n[161] Z. Zhu, X. Wang, W. Zhao, C. Min, N. Deng, M. Dou, Y. Wang,\nB. Shi, K. Wang, C. Zhang et al., \u201cIs sora a world simulator? a\ncomprehensive survey on general world models and beyond,\u201d arXiv\npreprint arXiv:2405.03520, 2024.\n[162] L. Zhang, A. Rao, and M. Agrawala, \u201cAdding conditional control\nto text-to-image diffusion models,\u201d in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2023, pp. 3836\u20133847.\n[163] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet,\n\u201cVideo diffusion models,\u201d Advances in Neural Information Processing\nSystems, vol. 35, pp. 8633\u20138646, 2022.\n[164] J. Ho, W. Chan, C. Saharia, J. Whang, R. Gao, A. Gritsenko, D. P.\nKingma, B. Poole, M. Norouzi, D. J. Fleet et al., \u201cImagen video:\nHigh definition video generation with diffusion models,\u201d arXiv preprint\narXiv:2210.02303, 2022.\n[165] D. Ha and J. Schmidhuber, \u201cRecurrent world models facilitate policy\nevolution,\u201d Advances in neural information processing systems, vol. 31,\n2018.\n[166] J. Mao, Y. Qian, H. Zhao, and Y. Wang, \u201cGpt-driver: Learning to drive\nwith gpt,\u201d arXiv preprint arXiv:2310.01415, 2023.\n[167] M. Peng, X. Guo, X. Chen, M. Zhu, K. Chen, X. Wang, Y. Wang et al.,\n\u201cLc-llm: Explainable lane-change intention and trajectory predictions\nwith large language models,\u201d arXiv preprint arXiv:2403.18344, 2024.\n[168] V. Zyrianov, H. Che, Z. Liu, and S. Wang, \u201cLidardm: Generative lidar\nsimulation in a generated world,\u201d arXiv preprint arXiv:2404.02903,\n2024.\n[169] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable\nvisual models from natural language supervision,\u201d in International\nconference on machine learning.\nPMLR, 2021, pp. 8748\u20138763.\n[170] A. Van Den Oord, O. Vinyals et al., \u201cNeural discrete representation\nlearning,\u201d Advances in neural information processing systems, vol. 30,\n2017.\n[171] H. Chang, H. Zhang, L. Jiang, C. Liu, and W. T. Freeman,\n\u201cMaskgit: Masked generative image transformer,\u201d in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition,\n2022, pp. 11 315\u201311 325.\n[172] K. Rasul, C. Seward, I. Schuster, and R. Vollgraf, \u201cAutoregressive\ndenoising diffusion models for multivariate probabilistic time se-\nries forecasting,\u201d in International Conference on Machine Learning.\nPMLR, 2021, pp. 8857\u20138868.\n[173] Y. Tashiro, J. Song, Y. Song, and S. Ermon, \u201cCsdi: Conditional\nscore-based diffusion models for probabilistic time series imputation,\u201d\nAdvances in Neural Information Processing Systems, vol. 34, pp.\n24 804\u201324 816, 2021.\n[174] Y. Yuan, J. Ding, C. Shao, D. Jin, and Y. Li, \u201cSpatio-temporal diffusion\npoint processes,\u201d in Proceedings of the 29th ACM SIGKDD Conference\non Knowledge Discovery and Data Mining, 2023, pp. 3173\u20133184.\n[175] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training\nof deep bidirectional transformers for language understanding,\u201d arXiv\npreprint arXiv:1810.04805, 2018.\n[176] W. Jiang and J. Luo, \u201cGraph neural network for traffic forecasting: A\nsurvey,\u201d Expert systems with applications, vol. 207, p. 117921, 2022.\n[177] Z. Zheng, Z. Wang, Z. Hu, Z. Wan, and W. Ma, \u201cRecovering traffic\ndata from the corrupted noise: A doubly physics-regularized denoising\ndiffusion model,\u201d Transportation Research Part C: Emerging Technolo-\ngies, vol. 160, p. 104513, 2024.\n[178] U. Mori, A. Mendiburu, M. \u00b4Alvarez, and J. A. Lozano, \u201cA review of\ntravel time estimation and forecasting for advanced traveller informa-\ntion systems,\u201d Transportmetrica A: Transport Science, vol. 11, no. 2,\npp. 119\u2013157, 2015.\n[179] K. Goniewicz, M. Goniewicz, W. Paw\u0142owski, and P. Fiedor, \u201cRoad\naccident rates: strategies and programmes for improving road traffic\nsafety,\u201d European journal of trauma and emergency surgery, vol. 42,\npp. 433\u2013438, 2016.\n[180] K. K. Santhosh, D. P. Dogra, and P. P. Roy, \u201cAnomaly detection in road\ntraffic using visual surveillance: A survey,\u201d ACM Computing Surveys\n(CSUR), vol. 53, no. 6, pp. 1\u201326, 2020.\n[181] X. Kong, J. Wang, Z. Hu, Y. He, X. Zhao, and G. Shen, \u201cMobile\ntrajectory anomaly detection: Taxonomy, methodology, challenges, and\ndirections,\u201d IEEE Internet of Things Journal, 2024.\n[182] J. Fang, J. Qiao, J. Xue, and Z. Li, \u201cVision-based traffic accident\ndetection and anticipation: A survey,\u201d IEEE Transactions on Circuits\nand Systems for Video Technology, 2023.\n[183] K. Zheng, X. He, and X. E. Wang, \u201cMinigpt-5: Interleaved vision-\nand-language\ngeneration\nvia\ngenerative\nvokens,\u201d\narXiv\npreprint\narXiv:2310.02239, 2023.\n[184] X. Zhao, B. Liu, Q. Liu, G. Shi, and X.-M. Wu, \u201cMaking multimodal\ngeneration easier: When diffusion models meet llms,\u201d arXiv preprint\narXiv:2310.08949, 2023.\n[185] W. Peebles and S. Xie, \u201cScalable diffusion models with transformers,\u201d\nin Proceedings of the IEEE/CVF International Conference on Com-\nputer Vision, 2023, pp. 4195\u20134205.\n[186] F. Bao, S. Nie, K. Xue, Y. Cao, C. Li, H. Su, and J. Zhu, \u201cAll are\nworth words: A vit backbone for diffusion models,\u201d in Proceedings of\nthe IEEE/CVF conference on computer vision and pattern recognition,\n2023, pp. 22 669\u201322 679.\n[187] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. M\u00a8uller, H. Saini,\nY. Levi, D. Lorenz, A. Sauer, F. Boesel et al., \u201cScaling rectified\nflow transformers for high-resolution image synthesis,\u201d in Forty-first\nInternational Conference on Machine Learning, 2024.\n[188] C. Mou, X. Wang, L. Xie, Y. Wu, J. Zhang, Z. Qi, and Y. Shan, \u201cT2i-\nadapter: Learning adapters to dig out more controllable ability for text-\nto-image diffusion models,\u201d in Proceedings of the AAAI Conference on\nArtificial Intelligence, vol. 38, no. 5, 2024, pp. 4296\u20134304.\n[189] T. Salimans and J. Ho, \u201cProgressive distillation for fast sampling of\ndiffusion models,\u201d arXiv preprint arXiv:2202.00512, 2022.\n[190] Y. Song, P. Dhariwal, M. Chen, and I. Sutskever, \u201cConsistency models,\u201d\narXiv preprint arXiv:2303.01469, 2023.\n[191] S. Xue, Z. Liu, F. Chen, S. Zhang, T. Hu, E. Xie, and Z. Li,\n\u201cAccelerating diffusion sampling with optimized time steps,\u201d in Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 2024, pp. 8292\u20138301.\n",
    "ref": [
        "2401.00713",
        "2312.08248",
        "2404.18886",
        "2011.13456",
        "2310.07771",
        "2402.07369",
        "2311.16203",
        "2205.09991",
        "2207.12598",
        "2211.15657",
        "2309.17080",
        "2311.01017",
        "2404.12624",
        "2309.09777",
        "2312.02934",
        "1609.02907",
        "2010.02502",
        "1805.00123",
        "2003.09003",
        "1512.03012",
        "2403.09190",
        "2301.00493",
        "2004.07219",
        "2106.11810",
        "1910.03088",
        "2403.06845",
        "2306.04873",
        "2401.08119",
        "2306.16927",
        "2112.03126",
        "2404.04629",
        "2312.11578",
        "2208.09801",
        "2302.10463",
        "2401.02916",
        "2403.11643",
        "2311.01223",
        "2208.06193",
        "2401.03629",
        "2404.02524",
        "2404.02082",
        "2405.03520",
        "2210.02303",
        "2310.01415",
        "2403.18344",
        "2404.02903",
        "1810.04805",
        "2310.02239",
        "2310.08949",
        "2202.00512",
        "2303.01469"
    ]
}